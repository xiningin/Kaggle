{"cell_type":{"b1156131":"code","d2387746":"code","b8bb577c":"code","976d084d":"code","298a8afe":"code","f4b1d8f9":"code","4c3672d2":"code","a0defb88":"code","522533e1":"code","42319343":"code","d47a5d55":"code","5512a9be":"markdown"},"source":{"b1156131":"import math\nimport numpy as np\n\nimport keras\nimport keras.backend as K\nfrom keras.layers import Dropout\nfrom keras.layers import Layer, Conv1D, Dropout, Add, Input\nfrom keras.initializers import Ones, Zeros\n\nimport tensorflow as tf","d2387746":"py_any = K.py_any\nndim = K.ndim","b8bb577c":"def batch_dot(x, y, axes=None):\n    \"\"\"Batchwise dot product.\n\n    `batch_dot` is used to compute dot product of `x` and `y` when\n    `x` and `y` are data in batch, i.e. in a shape of\n    `(batch_size, :)`.\n    `batch_dot` results in a tensor or variable with less dimensions\n    than the input. If the number of dimensions is reduced to 1,\n    we use `expand_dims` to make sure that ndim is at least 2.\n\n    # Arguments\n        x: Keras tensor or variable with `ndim >= 2`.\n        y: Keras tensor or variable with `ndim >= 2`.\n        axes: list of (or single) int with target dimensions.\n            The lengths of `axes[0]` and `axes[1]` should be the same.\n\n    # Returns\n        A tensor with shape equal to the concatenation of `x`'s shape\n        (less the dimension that was summed over) and `y`'s shape\n        (less the batch dimension and the dimension that was summed over).\n        If the final rank is 1, we reshape it to `(batch_size, 1)`.\n\n    # Examples\n        Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n        `batch_dot(x, y, axes=1) = [[17], [53]]` which is the main diagonal\n        of `x.dot(y.T)`, although we never have to calculate the off-diagonal\n        elements.\n\n        Shape inference:\n        Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n        If `axes` is (1, 2), to find the output shape of resultant tensor,\n            loop through each dimension in `x`'s shape and `y`'s shape:\n\n        * `x.shape[0]` : 100 : append to output shape\n        * `x.shape[1]` : 20 : do not append to output shape,\n            dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)\n        * `y.shape[0]` : 100 : do not append to output shape,\n            always ignore first dimension of `y`\n        * `y.shape[1]` : 30 : append to output shape\n        * `y.shape[2]` : 20 : do not append to output shape,\n            dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)\n        `output_shape` = `(100, 30)`\n\n    ```python\n        >>> x_batch = K.ones(shape=(32, 20, 1))\n        >>> y_batch = K.ones(shape=(32, 30, 20))\n        >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n        >>> K.int_shape(xy_batch_dot)\n        (32, 1, 30)\n    ```\n    \"\"\"\n    if isinstance(axes, int):\n        axes = (axes, axes)\n    x_ndim = ndim(x)\n    y_ndim = ndim(y)\n    if axes is None:\n        # behaves like tf.batch_matmul as default\n        axes = [x_ndim - 1, y_ndim - 2]\n    if py_any([isinstance(a, (list, tuple)) for a in axes]):\n        raise ValueError('Multiple target dimensions are not supported. ' +\n                         'Expected: None, int, (int, int), ' +\n                         'Provided: ' + str(axes))\n    if x_ndim > y_ndim:\n        diff = x_ndim - y_ndim\n        y = tf.reshape(y, tf.concat([tf.shape(y), [1] * (diff)], axis=0))\n    elif y_ndim > x_ndim:\n        diff = y_ndim - x_ndim\n        x = tf.reshape(x, tf.concat([tf.shape(x), [1] * (diff)], axis=0))\n    else:\n        diff = 0\n    if ndim(x) == 2 and ndim(y) == 2:\n        if axes[0] == axes[1]:\n            out = tf.reduce_sum(tf.multiply(x, y), axes[0])\n        else:\n            out = tf.reduce_sum(tf.multiply(tf.transpose(x, [1, 0]), y), axes[1])\n    else:\n        if axes is not None:\n            adj_x = None if axes[0] == ndim(x) - 1 else True\n            adj_y = True if axes[1] == ndim(y) - 1 else None\n        else:\n            adj_x = None\n            adj_y = None\n        out = tf.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n    if diff:\n        if x_ndim > y_ndim:\n            idx = x_ndim + y_ndim - 3\n        else:\n            idx = x_ndim - 1\n        out = tf.squeeze(out, list(range(idx, idx + diff)))\n    if ndim(out) == 1:\n        out = expand_dims(out, 1)\n    return out","976d084d":"def shape_list(x):\n    if K.backend() != 'theano':\n        tmp = K.int_shape(x)\n    else:\n        tmp = x.shape\n    tmp = list(tmp)\n    tmp[0] = -1\n    return tmp\n\n\ndef split_heads(x, n: int, k: bool = False):  # B, L, C\n    x_shape = shape_list(x)\n    m = x_shape[-1]\n    new_x_shape = x_shape[:-1] + [n, m \/\/ n]\n    new_x = K.reshape(x, new_x_shape)\n    return K.permute_dimensions(new_x, [0, 2, 3, 1] if k else [0, 2, 1, 3])\n\n\ndef merge_heads(x):\n    new_x = K.permute_dimensions(x, [0, 2, 1, 3])\n    x_shape = shape_list(new_x)\n    new_x_shape = x_shape[:-2] + [np.prod(x_shape[-2:])]\n    return K.reshape(new_x, new_x_shape)\n\n\n# q and v are B, H, L, C\/\/H ; k is B, H, C\/\/H, L ; mask is B, 1, L, L\ndef scaled_dot_product_attention_tf(q, k, v, attn_mask, attention_dropout: float, neg_inf: float):\n    w = batch_dot(q, k)  # w is B, H, L, L\n    w = w \/ K.sqrt(K.cast(shape_list(v)[-1], K.floatx()))\n    if attn_mask is not None:\n        w = attn_mask * w + (1.0 - attn_mask) * neg_inf\n    w = K.softmax(w)\n    w = Dropout(attention_dropout)(w)\n    return batch_dot(w, v)  # it is B, H, L, C\/\/H [like v]\n\n\ndef scaled_dot_product_attention_th(q, k, v, attn_mask, attention_dropout: float, neg_inf: float):\n    w = theano_matmul(q, k)\n    w = w \/ K.sqrt(K.cast(shape_list(v)[-1], K.floatx()))\n    if attn_mask is not None:\n        attn_mask = K.repeat_elements(attn_mask, shape_list(v)[1], 1)\n        w = attn_mask * w + (1.0 - attn_mask) * neg_inf\n    w = K.T.exp(w - w.max()) \/ K.T.exp(w - w.max()).sum(axis=-1, keepdims=True)\n    w = Dropout(attention_dropout)(w)\n    return theano_matmul(w, v)\n\n\ndef multihead_attention(x, attn_mask, n_head: int, n_state: int, attention_dropout: float, neg_inf: float):\n    _q, _k, _v = x[:, :, :n_state], x[:, :, n_state:2 * n_state], x[:, :, -n_state:]\n    q = split_heads(_q, n_head)  # B, H, L, C\/\/H\n    k = split_heads(_k, n_head, k=True)  # B, H, C\/\/H, L\n    v = split_heads(_v, n_head)  # B, H, L, C\/\/H\n    if K.backend() == 'tensorflow':\n        a = scaled_dot_product_attention_tf(q, k, v, attn_mask, attention_dropout, neg_inf)\n    else:\n        a = scaled_dot_product_attention_th(q, k, v, attn_mask, attention_dropout, neg_inf)\n    return merge_heads(a)\n\n\ndef gelu(x):\n    return 0.5 * x * (1 + K.tanh(math.sqrt(2 \/ math.pi) * (x + 0.044715 * K.pow(x, 3))))\n\n\n# https:\/\/stackoverflow.com\/a\/42194662\/2796084\ndef theano_matmul(a, b, _left=False):\n    assert a.ndim == b.ndim\n    ndim = a.ndim\n    assert ndim >= 2\n    if _left:\n        b, a = a, b\n    if ndim == 2:\n        return K.T.dot(a, b)\n    else:\n        # If a is broadcastable but b is not.\n        if a.broadcastable[0] and not b.broadcastable[0]:\n            # Scan b, but hold a steady.\n            # Because b will be passed in as a, we need to left multiply to maintain\n            #  matrix orientation.\n            output, _ = K.theano.scan(theano_matmul, sequences=[b], non_sequences=[a[0], 1])\n        # If b is broadcastable but a is not.\n        elif b.broadcastable[0] and not a.broadcastable[0]:\n            # Scan a, but hold b steady.\n            output, _ = K.theano.scan(theano_matmul, sequences=[a], non_sequences=[b[0]])\n        # If neither dimension is broadcastable or they both are.\n        else:\n            # Scan through the sequences, assuming the shape for this dimension is equal.\n            output, _ = K.theano.scan(theano_matmul, sequences=[a, b])\n        return output","298a8afe":"def _get_pos_encoding_matrix(max_len: int, d_emb: int) -> np.array:\n    pos_enc = np.array(\n        [[pos \/ np.power(10000, 2 * (j \/\/ 2) \/ d_emb) for j in range(d_emb)] if pos != 0 else np.zeros(d_emb) for pos in\n         range(max_len)], dtype=np.float32)\n    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n    return pos_enc\n\n\nclass BertEmbedding(keras.layers.Layer):\n    def __init__(self, output_dim: int = 768, dropout: float = 0.1, vocab_size: int = 30000,\n                 max_len: int = 512, trainable_pos_embedding: bool = True, use_one_dropout: bool = False,\n                 use_embedding_layer_norm: bool = False, layer_norm_epsilon: float = 1e-5, **kwargs):\n        super().__init__(**kwargs)\n        self.max_len = max_len\n        self.use_one_dropout = use_one_dropout\n        self.output_dim = output_dim\n        self.dropout = dropout\n        self.vocab_size = vocab_size\n\n        # Bert keras uses two segments for next-sentence classification task\n        self.segment_emb = keras.layers.Embedding(2, output_dim, input_length=max_len,\n                                                  name='SegmentEmbedding')\n\n        self.trainable_pos_embedding = trainable_pos_embedding\n        if not trainable_pos_embedding:\n            self.pos_emb = keras.layers.Embedding(max_len, output_dim, trainable=False, input_length=max_len,\n                                                  name='PositionEmbedding',\n                                                  weights=[_get_pos_encoding_matrix(max_len, output_dim)])\n        else:\n            self.pos_emb = keras.layers.Embedding(max_len, output_dim, input_length=max_len, name='PositionEmbedding')\n\n        self.token_emb = keras.layers.Embedding(vocab_size, output_dim, input_length=max_len, name='TokenEmbedding')\n        self.embedding_dropout = keras.layers.Dropout(dropout, name='EmbeddingDropOut')\n        self.add_embeddings = keras.layers.Add(name='AddEmbeddings')\n        self.use_embedding_layer_norm = use_embedding_layer_norm\n        if self.use_embedding_layer_norm:\n            self.embedding_layer_norm = LayerNormalization(layer_norm_epsilon)\n        else:\n            self.embedding_layer_norm = None\n        self.layer_norm_epsilon = layer_norm_epsilon\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0][0], input_shape[0][1], self.output_dim\n\n    def get_config(self):\n        config = {\n            'max_len': self.max_len,\n            'use_one_dropout': self.use_one_dropout,\n            'output_dim': self.output_dim,\n            'dropout': self.dropout,\n            'vocab_size': self.vocab_size,\n            'trainable_pos_embedding': self.trainable_pos_embedding,\n            'embedding_layer_norm': self.use_embedding_layer_norm,\n            'layer_norm_epsilon': self.layer_norm_epsilon\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def __call__(self, inputs, **kwargs):\n        tokens, segment_ids, pos_ids = inputs\n        segment_embedding = self.segment_emb(segment_ids)\n        pos_embedding = self.pos_emb(pos_ids)\n        token_embedding = self.token_emb(tokens)\n        if self.use_one_dropout:\n            summation = self.add_embeddings([segment_embedding, pos_embedding, token_embedding])\n            if self.embedding_layer_norm:\n                summation = self.embedding_layer_norm(summation)\n            return self.embedding_dropout(summation)\n        summation = self.add_embeddings(\n            [self.embedding_dropout(segment_embedding), self.embedding_dropout(pos_embedding),\n             self.embedding_dropout(token_embedding)])\n        if self.embedding_layer_norm:\n            summation = self.embedding_layer_norm(summation)\n        return summation","f4b1d8f9":"class MultiHeadAttention(Layer):\n    def __init__(self, n_head: int, n_state: int, attention_dropout: float, use_attn_mask: bool, neg_inf: float,\n                 **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.n_head = n_head\n        self.n_state = n_state\n        self.attention_dropout = attention_dropout\n        self.use_attn_mask = use_attn_mask\n        self.neg_inf = neg_inf\n\n    def compute_output_shape(self, input_shape):\n        x = input_shape[0] if self.use_attn_mask else input_shape\n        return x[0], x[1], x[2] \/\/ 3\n\n    def call(self, inputs, **kwargs):\n        x = inputs[0] if self.use_attn_mask else inputs\n        attn_mask = inputs[1] if self.use_attn_mask else None\n        return multihead_attention(x, attn_mask, self.n_head, self.n_state, self.attention_dropout, self.neg_inf)\n\n    def get_config(self):\n        config = {\n            'n_head': self.n_head,\n            'n_state': self.n_state,\n            'attention_dropout': self.attention_dropout,\n            'use_attn_mask': self.use_attn_mask,\n            'neg_inf': self.neg_inf,\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LayerNormalization(Layer):\n    def __init__(self, eps: float = 1e-5, **kwargs) -> None:\n        self.eps = eps\n        super().__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:], initializer=Ones(), trainable=True)\n        self.beta = self.add_weight(name='beta', shape=input_shape[-1:], initializer=Zeros(), trainable=True)\n        super().build(input_shape)\n\n    def call(self, x, **kwargs):\n        u = K.mean(x, axis=-1, keepdims=True)\n        s = K.mean(K.square(x - u), axis=-1, keepdims=True)\n        z = (x - u) \/ K.sqrt(s + self.eps)\n        return self.gamma * z + self.beta\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def get_config(self):\n        config = {\n            'eps': self.eps,\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass Gelu(Layer):\n    def __init__(self, accurate: bool = False, **kwargs):\n        super().__init__(**kwargs)\n        self.accurate = accurate\n\n    def call(self, inputs, **kwargs):\n        if not self.accurate:\n            return gelu(inputs)\n        if K.backend() == 'tensorflow':\n            erf = K.tf.erf\n        else:\n            erf = K.T.erf\n        return inputs * 0.5 * (1.0 + erf(inputs \/ math.sqrt(2.0)))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def get_config(self):\n        config = {\n            'accurate': self.accurate,\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))","4c3672d2":"class MultiHeadSelfAttention:\n    def __init__(self, n_state: int, n_head: int, attention_dropout: float,\n                 use_attn_mask: bool, layer_id: int, neg_inf: float) -> None:\n        assert n_state % n_head == 0\n        self.c_attn = Conv1D(3 * n_state, 1, name='layer_{}\/c_attn'.format(layer_id))\n        self.attn = MultiHeadAttention(n_head, n_state, attention_dropout, use_attn_mask,\n                                       neg_inf, name='layer_{}\/self_attention'.format(layer_id))\n        self.c_attn_proj = Conv1D(n_state, 1, name='layer_{}\/c_attn_proj'.format(layer_id))\n\n    def __call__(self, x, mask):\n        output = self.c_attn(x)\n        output = self.attn(output) if mask is None else self.attn([output, mask])\n        return self.c_attn_proj(output)\n\n\nclass PositionWiseFF:\n    def __init__(self, n_state: int, d_hid: int, layer_id: int, accurate_gelu: bool) -> None:\n        self.c_fc = Conv1D(d_hid, 1, name='layer_{}\/c_fc'.format(layer_id))\n        self.activation = Gelu(accurate=accurate_gelu, name='layer_{}\/gelu'.format(layer_id))\n        self.c_ffn_proj = Conv1D(n_state, 1, name='layer_{}\/c_ffn_proj'.format(layer_id))\n\n    def __call__(self, x):\n        output = self.activation(self.c_fc(x))\n        return self.c_ffn_proj(output)\n\n\nclass EncoderLayer:\n    def __init__(self, n_state: int, n_head: int, d_hid: int, residual_dropout: float, attention_dropout: float,\n                 use_attn_mask: bool, layer_id: int, neg_inf: float, ln_epsilon: float, accurate_gelu: bool) -> None:\n        self.attention = MultiHeadSelfAttention(n_state, n_head, attention_dropout, use_attn_mask, layer_id, neg_inf)\n        self.drop1 = Dropout(residual_dropout, name='layer_{}\/ln_1_drop'.format(layer_id))\n        self.add1 = Add(name='layer_{}\/ln_1_add'.format(layer_id))\n        self.ln1 = LayerNormalization(ln_epsilon, name='layer_{}\/ln_1'.format(layer_id))\n        self.ffn = PositionWiseFF(n_state, d_hid, layer_id, accurate_gelu)\n        self.drop2 = Dropout(residual_dropout, name='layer_{}\/ln_2_drop'.format(layer_id))\n        self.add2 = Add(name='layer_{}\/ln_2_add'.format(layer_id))\n        self.ln2 = LayerNormalization(ln_epsilon, name='layer_{}\/ln_2'.format(layer_id))\n\n    def __call__(self, x, mask):\n        a = self.attention(x, mask)\n        n = self.ln1(self.add1([x, self.drop1(a)]))\n        f = self.ffn(n)\n        return self.ln2(self.add2([n, self.drop2(f)]))\n\n\ndef create_transformer(embedding_dim: int = 768, embedding_dropout: float = 0.1, vocab_size: int = 30000,\n                       max_len: int = 512, trainable_pos_embedding: bool = True, num_heads: int = 12,\n                       num_layers: int = 12, attention_dropout: float = 0.1, use_one_embedding_dropout: bool = False,\n                       d_hid: int = 768 * 4, residual_dropout: float = 0.1, use_attn_mask: bool = True,\n                       embedding_layer_norm: bool = False, neg_inf: float = -1e9, layer_norm_epsilon: float = 1e-5,\n                       accurate_gelu: bool = False) -> keras.Model:\n    tokens = Input(batch_shape=(None, max_len), name='token_input', dtype='int32')\n    segment_ids = Input(batch_shape=(None, max_len), name='segment_input', dtype='int32')\n    pos_ids = Input(batch_shape=(None, max_len), name='position_input', dtype='int32')\n    attn_mask = Input(batch_shape=(None, 1, max_len, max_len), name='attention_mask_input',\n                      dtype=K.floatx()) if use_attn_mask else None\n    inputs = [tokens, segment_ids, pos_ids]\n    embedding_layer = BertEmbedding(embedding_dim, embedding_dropout, vocab_size, max_len, trainable_pos_embedding,\n                                    use_one_embedding_dropout, embedding_layer_norm, layer_norm_epsilon)\n    x = embedding_layer(inputs)\n    for i in range(num_layers):\n        x = EncoderLayer(embedding_dim, num_heads, d_hid, residual_dropout,\n                         attention_dropout, use_attn_mask, i, neg_inf, layer_norm_epsilon, accurate_gelu)(x, attn_mask)\n    if use_attn_mask:\n        inputs.append(attn_mask)\n    return keras.Model(inputs=inputs, outputs=[x], name='Transformer')","a0defb88":"# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport json\nimport six\nimport tensorflow as tf\n\n\nclass BertConfig(object):\n  \"\"\"Configuration for `BertModel`.\"\"\"\n\n  def __init__(self,\n               vocab_size,\n               hidden_size=768,\n               num_hidden_layers=12,\n               num_attention_heads=12,\n               intermediate_size=3072,\n               hidden_act=\"gelu\",\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings=512,\n               type_vocab_size=16,\n               initializer_range=0.02):\n    \"\"\"Constructs BertConfig.\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probability for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    \"\"\"\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings = max_position_embeddings\n    self.type_vocab_size = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file):\n    \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n    with tf.gfile.GFile(json_file, \"r\") as reader:\n      text = reader.read()\n    return cls.from_dict(json.loads(text))\n\n  def to_dict(self):\n    \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    \"\"\"Serializes this instance to a JSON string.\"\"\"\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"","522533e1":"def get_bert_weights_for_keras_model(check_point, max_len, model, tf_var_names):\n    keras_weights = [np.zeros(w.shape) for w in model.weights]\n    keras_weights_set = []\n\n    for var_name, _ in tf_var_names:\n        qkv, unsqueeze, w_id = _get_tf2keras_weights_name_mapping(var_name)\n        if w_id is None:\n            print('not mapped: ', var_name)  # TODO pooler, cls\/predictions, cls\/seq_relationship\n        else:\n            print(var_name, ' -> ', model.weights[w_id].name)\n            keras_weights_set.append(w_id)\n            keras_weight = keras_weights[w_id]\n            tensorflow_weight = check_point.get_tensor(var_name)\n            keras_weights[w_id] = _set_keras_weight_from_tf_weight(max_len, tensorflow_weight, keras_weight, qkv, unsqueeze, w_id)\n\n    keras_layer_not_set = set(list(range(len(keras_weights)))) - set(keras_weights_set)\n    assert len(keras_layer_not_set) == 0, 'Some weights were not set!'\n\n    return keras_weights\n\n\ndef _set_keras_weight_from_tf_weight(max_len, tensorflow_weight, keras_weight, qkv, unsqueeze, w_id):\n    if qkv is None:\n        if w_id == 1:  # pos embedding\n            keras_weight[:max_len, :] = tensorflow_weight[:max_len, :] if not unsqueeze else tensorflow_weight[None, :max_len, :]\n\n        elif w_id == 2:  # word embedding\n            keras_weight = tensorflow_weight\n        else:\n            keras_weight[:] = tensorflow_weight if not unsqueeze else tensorflow_weight[None, ...]\n    else:\n        p = {'q': 0, 'k': 1, 'v': 2}[qkv]\n        if keras_weight.ndim == 3:\n            dim_size = keras_weight.shape[1]\n            keras_weight[0, :, p * dim_size:(p + 1) * dim_size] = tensorflow_weight if not unsqueeze else tensorflow_weight[None, ...]\n        else:\n            dim_size = keras_weight.shape[0] \/\/ 3\n            keras_weight[p * dim_size:(p + 1) * dim_size] = tensorflow_weight\n\n    return keras_weight\n\n\ndef _get_tf2keras_weights_name_mapping(var_name):\n    w_id = None\n    qkv = None\n    unsqueeze = False\n\n    var_name_splitted = var_name.split('\/')\n    if var_name_splitted[1] == 'embeddings':\n        w_id = _get_embeddings_name(var_name_splitted)\n\n    elif var_name_splitted[2].startswith('layer_'):\n        qkv, unsqueeze, w_id = _get_layers_name(var_name_splitted)\n\n    return qkv, unsqueeze, w_id\n\n\ndef _get_layers_name(var_name_splitted):\n    first_vars_size = 5\n    w_id = None\n    qkv = None\n    unsqueeze = False\n\n    layer_number = int(var_name_splitted[2][len('layer_'):])\n    if var_name_splitted[3] == 'attention':\n        if var_name_splitted[-1] == 'beta':\n            w_id = first_vars_size + layer_number * 12 + 5\n        elif var_name_splitted[-1] == 'gamma':\n            w_id = first_vars_size + layer_number * 12 + 4\n        elif var_name_splitted[-2] == 'dense':\n            if var_name_splitted[-1] == 'bias':\n                w_id = first_vars_size + layer_number * 12 + 3\n            elif var_name_splitted[-1] == 'kernel':\n                w_id = first_vars_size + layer_number * 12 + 2\n                unsqueeze = True\n            else:\n                raise ValueError()\n        elif var_name_splitted[-2] == 'key' or var_name_splitted[-2] == 'query' or var_name_splitted[-2] == 'value':\n            w_id = first_vars_size + layer_number * 12 + (0 if var_name_splitted[-1] == 'kernel' else 1)\n            unsqueeze = var_name_splitted[-1] == 'kernel'\n            qkv = var_name_splitted[-2][0]\n        else:\n            raise ValueError()\n    elif var_name_splitted[3] == 'intermediate':\n        if var_name_splitted[-1] == 'bias':\n            w_id = first_vars_size + layer_number * 12 + 7\n        elif var_name_splitted[-1] == 'kernel':\n            w_id = first_vars_size + layer_number * 12 + 6\n            unsqueeze = True\n        else:\n            raise ValueError()\n    elif var_name_splitted[3] == 'output':\n        if var_name_splitted[-1] == 'beta':\n            w_id = first_vars_size + layer_number * 12 + 11\n        elif var_name_splitted[-1] == 'gamma':\n            w_id = first_vars_size + layer_number * 12 + 10\n        elif var_name_splitted[-1] == 'bias':\n            w_id = first_vars_size + layer_number * 12 + 9\n        elif var_name_splitted[-1] == 'kernel':\n            w_id = first_vars_size + layer_number * 12 + 8\n            unsqueeze = True\n        else:\n            raise ValueError()\n    return qkv, unsqueeze, w_id\n\n\ndef _get_embeddings_name(parts):\n    n = parts[-1]\n    if n == 'token_type_embeddings':\n        w_id = 0\n    elif n == 'position_embeddings':\n        w_id = 1\n    elif n == 'word_embeddings':\n        w_id = 2\n    elif n == 'gamma':\n        w_id = 3\n    elif n == 'beta':\n        w_id = 4\n    else:\n        raise ValueError()\n    return w_id","42319343":"def load_google_bert(base_location: str = '..\/input\/uncased_L-12_H-768_A-12\/',\n                     use_attn_mask: bool = True, max_len: int = 512) -> keras.Model:\n    bert_config = BertConfig.from_json_file(base_location + 'bert_config.json')\n    print(bert_config.__dict__)\n    init_checkpoint = base_location + 'bert_model.ckpt'\n    var_names = tf.train.list_variables(init_checkpoint)\n    check_point = tf.train.load_checkpoint(init_checkpoint)\n    model = create_transformer(embedding_layer_norm=True, neg_inf=-10000.0, use_attn_mask=use_attn_mask,\n                               vocab_size=bert_config.vocab_size, accurate_gelu=True, layer_norm_epsilon=1e-12, max_len=max_len,\n                               use_one_embedding_dropout=True, d_hid=bert_config.intermediate_size,\n                               embedding_dim=bert_config.hidden_size, num_layers=bert_config.num_hidden_layers,\n                               num_heads=bert_config.num_attention_heads,\n                               residual_dropout=bert_config.hidden_dropout_prob,\n                               attention_dropout=bert_config.attention_probs_dropout_prob)\n    weights = get_bert_weights_for_keras_model(check_point, max_len, model, var_names)\n    model.set_weights(weights)\n    return model","d47a5d55":"BERT_PRETRAINED_DIR = '..\/input\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12\/'\ng_bert = load_google_bert(base_location=BERT_PRETRAINED_DIR, use_attn_mask=False)\ng_bert.summary()\ng_bert.save('keras_bert_uncased_L-12_H-768_A-12.hdf5')","5512a9be":"This notebook shows how to load the tensorflow pretrained weights into a keras model.\nThe following code is based on https:\/\/github.com\/Separius\/BERT-keras with some modifications (the same vocab ordering as in the usual tf model is used, which is not the case in the reference implmentation)."}}