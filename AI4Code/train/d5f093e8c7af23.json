{"cell_type":{"fc577753":"code","a579751c":"code","583c2d1f":"code","35290212":"markdown"},"source":{"fc577753":"import gym\nenv = gym.make('Blackjack-v0')\ndiscount = 1\nreturns = {}\nv = {} # change v to q\n\ndef print_dict(a_dict):\n    key_list = sorted(a_dict.keys())\n    for key in key_list:\n        print(key,':',a_dict[key])\n\n#print(env.action_space)\n#print(env.observation_space)\n\n#run through 1 or more episodes\nfor i in range(100):\n    #start a new episode\n    state = env.reset()\n    episode_state = []   #combie state and action \n    episode_action = []  #combie state and action \n    episode_reward = []\n    episode_state.append(state) # no need this because we need action\n    #print('\\nstate',state)\n    for t in range(100):\n        #get a random action from the environment\n        action = env.action_space.sample()\n        #print('action',action)\n        #send the action to the environment and get back\n        #the next state, reward, whether or not the episode is over,\n        #and additional information which may or may not be\n        #defined in certain environments.\n        state, reward, done, info = env.step(action)\n        episode_action.append(action) #combie state and action \n        episode_reward.append(reward)\n        episode_state.append(state) #combie state and action \n        #print('state',state)\n        #print('reward',reward)\n        #print('done',done)\n        #print('info',info)\n        #check for the completion of the episode\n        if done:\n            #print('Episode finished after {} timesteps'.format(t+1))\n            break\n    #print(episode_state,'\\n',episode_action,'\\n',episode_reward)\n    g = 0\n    for tg in range(len(episode_reward)-1,-1,-1):\n        g = discount*g + episode_reward[tg]\n        if episode_state.index(episode_state[tg]) == tg:\n            if episode_state[tg] in returns:\n                returns[episode_state[tg]].append(g)\n            else:\n                returns[episode_state[tg]] = [g]\n            v[episode_state[tg]] = sum(returns[episode_state[tg]])\/\\\n                                   len(returns[episode_state[tg]])\n#print('\\nreturns')\n#print_dict(returns)\nprint(\"\\nv\")\nprint_dict(v)\n            \n#close the environment\nenv.close()\n\n","a579751c":"Change v to q","583c2d1f":"import gym\nenv = gym.make('Blackjack-v0')\ndiscount = 1\nreturns = {}\nq = {} # change v to q\n\ndef print_dict(a_dict):\n    key_list = sorted(a_dict.keys())\n    for key in key_list:\n        print(key,':',a_dict[key])\n\n#print(env.action_space)\n#print(env.observation_space)\n\n#run through 1 or more episodes\nfor i in range(10000):\n    #start a new episode\n    state = env.reset()\n    #combie state and action \n    episode_StateAction = []\n    episode_reward = []\n    #print('\\nstate',state)\n    for t in range(100):\n        #get a random action from the environment\n        action = env.action_space.sample()\n        #print('action',action)\n        #send the action to the environment and get back\n        #the next state, reward, whether or not the episode is over,\n        #and additional information which may or may not be\n        #defined in certain environments.\n        episode_StateAction.append((state,action))\n        state, reward, done, info = env.step(action)\n        episode_reward.append(reward)\n        #print('state',state)\n        #print('reward',reward)\n        #print('done',done)\n        #print('info',info)\n        #check for the completion of the episode\n        if done:\n            #print('Episode finished after {} timesteps'.format(t+1))\n            break\n    #print(episode_state,'\\n',episode_action,'\\n',episode_reward)\n    g = 0\n    for tg in range(len(episode_reward)-1,-1,-1):\n        g = discount*g + episode_reward[tg]\n        if episode_StateAction.index(episode_StateAction[tg]) == tg:\n            if episode_StateAction[tg] in returns:\n                returns[episode_StateAction[tg]].append(g)\n            else:\n                returns[episode_StateAction[tg]] = [g]\n            q[episode_StateAction[tg]] = sum(returns[episode_StateAction[tg]])\/\\\n                                   len(returns[episode_StateAction[tg]])\n#print('\\nreturns')\n#print_dict(returns)\nprint(\"\\nv\")\nprint_dict(q)\n            \n#close the environment\nenv.close()\n\n","35290212":"Orginal file and changed mark"}}