{"cell_type":{"b8ca9141":"code","59d498f3":"code","f219a838":"code","ab8dd943":"code","0fad89f6":"code","2ec878f1":"code","a8341d65":"code","cd2cac57":"code","c068f95b":"code","9ceb168f":"code","a941ff35":"code","89f30825":"markdown","716f2924":"markdown","b7982898":"markdown","e91975a5":"markdown","d5df6c51":"markdown","cd2c87df":"markdown","bcdffc22":"markdown","c90dbf7c":"markdown","21c20a35":"markdown","ed13ba41":"markdown","f75dd19d":"markdown","54c5a737":"markdown"},"source":{"b8ca9141":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","59d498f3":"data_path = '..\/input\/Admission_Predict.csv'\ndf = pd.read_csv(data_path)\ndf.head()","f219a838":"print(len(df.columns),\"columns are:\")\nfor x in df.columns:\n    print(x+ ',')","ab8dd943":"df=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})","0fad89f6":"print(df.info ())","2ec878f1":"import seaborn as sb\nimport matplotlib.pyplot as plt\n\nfig,ax = plt.subplots(figsize=(10, 10))\nsb.heatmap(df.corr(), ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\")\nplt.show()","a8341d65":"y = df['Chance of Admit']\nfeatures = ['GRE Score', 'TOEFL Score', 'CGPA']\nX = df[features]\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size = 0.3, random_state=1)","cd2cac57":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(train_X, train_y)\nlr_predicts = lr.predict(val_X)\nfrom sklearn.metrics import mean_squared_error\nlr_mse = mean_squared_error(lr_predicts, val_y)\nprint(lr_mse)\n\n","c068f95b":"from sklearn.linear_model import LogisticRegression\ntrain_y_01 = [1 if each > 0.5 else 0 for each in train_y]\nlogr = LogisticRegression()\nlogr.fit(train_X, train_y_01)\nlogr_predicts = logr.predict(val_X)\nlogr_mse = mean_squared_error(logr_predicts, val_y)\nprint(logr_mse)","9ceb168f":"from sklearn.tree import DecisionTreeRegressor\ntree = DecisionTreeRegressor(random_state=0)\ntree.fit(train_X, train_y)\ntree_predicts = tree.predict(val_X)\ntree_mse = mean_squared_error(val_y, tree_predicts)\nprint(tree_mse)","a941ff35":"test_data_path = '..\/input\/Admission_Predict_Ver1.1.csv'\ntest_data = pd.read_csv(test_data_path)\ntest_X = test_data[features]\ntest_preds = lr.predict(test_X)\noutput = pd.DataFrame({'Serial No.': test_data['Serial No.'],\n                       'Chance of Admit': test_preds})\noutput.to_csv('submission.csv', index=False)","89f30825":"### Small model: train and test samples","716f2924":"### Linear Regression","b7982898":"DecisionTreeRegressor is better than LogisticRegression, but worse than the LinearRegression.","e91975a5":"### Logistic Regression","d5df6c51":"Better to change the name of the last column - it's gonna be used.","cd2c87df":"From the linear regression: MSE=0.004301 .\nLet's try logistic regression instead.","bcdffc22":"### Brief Visualisation","c90dbf7c":"400 observations, no missing values.","21c20a35":"So, most important features for admission: CGPA, GRE, TOEFL.","ed13ba41":"From the logistic regression: MSE = 0.1024575 (large than with the simple linear regression).\nObviously, we lost a lot of info when recoded y into a binary variable.\n\nNext, let's try DecisionTreeRegressor.\n\n### DecisionTreeRegressor","f75dd19d":"### Preliminary setup","54c5a737":"### Data import"}}