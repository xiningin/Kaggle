{"cell_type":{"1fd3e363":"code","1456a3ce":"code","9a86bc15":"code","cba68636":"code","3fc751e0":"code","38824cb9":"code","c83af278":"code","4d3eb59e":"code","beeab6e3":"code","b32d3053":"code","07dfa98e":"code","4ca59f85":"code","b0d41ce5":"code","b8ffee5b":"code","6ec032a1":"code","e82c8b98":"code","c10e0be7":"code","262212be":"code","87c4089a":"code","f08c286f":"code","8b3cd182":"code","90d84e6b":"code","03cd6fb1":"code","b9ed9700":"code","72ded5fc":"markdown","e676af1f":"markdown","423e1633":"markdown","fb9bd65e":"markdown","11cacb7a":"markdown","6252c598":"markdown","e5ade6dd":"markdown"},"source":{"1fd3e363":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1456a3ce":"#import numpy as np\n#import pandas as pd\nfrom pandas import DataFrame, Series\n\nfrom category_encoders import OrdinalEncoder, TargetEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.model_selection import StratifiedKFold\n\"\"\"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization,Activation,Dropout,Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, ReduceLROnPlateau\nfrom tensorflow.keras.applications import VGG16\"\"\"\nimport lightgbm as lgb\nfrom sklearn.neural_network import MLPRegressor\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import roc_auc_score\ndef cal_smape(x,y):\n    return np.mean( np.abs(x-y) \/ ((x+y)\/2) ) * 100\ndef cal_mape(x,y):\n    return np.mean( np.abs((x-y) \/ x) ) * 100\n\n\"\"\"def mlp(num_cols):\n    model = Sequential()\n    \n    model.add(Dense(units=512, input_shape = (len(num_cols),), \n                    kernel_initializer='he_normal',activation='relu'))    \n    model.add(Dropout(0.3))\n    \n    model.add(Dense(units=64,kernel_initializer='he_normal',activation='relu'))\n    model.add(Dropout(0.3))\n    \n    #model.add(Dense(units=128,kernel_initializer='he_normal',activation='relu'))\n    #model.add(Dropout(0.3))\n\n    model.add(Dense(units=32,kernel_initializer='he_normal', activation='relu'))     \n    model.add(Dropout(0.3))\n    \n    model.add(Dense(1, activation='linear'))\n    model.compile(loss='mape', optimizer='adam', metrics=['mape']) \n    return model\"\"\"","9a86bc15":"## setting\ntarget_name = \"units_sold\"\nrm_cols = [\"title\",\"tags\"]","cba68636":"df_train = pd.read_csv('\/kaggle\/input\/sales-prediction-of-clothes-in-e-commerce\/train.csv', index_col=0)\ndf_test = pd.read_csv('\/kaggle\/input\/sales-prediction-of-clothes-in-e-commerce\/test.csv', index_col=0)\nprint(df_train.shape)\nprint(df_test.shape)","3fc751e0":"def add_discount_rate(df):\n    df[\"discount_rate\"] = (df[\"retail_price\"]-df[\"price\"])\/df[\"price\"]\n    return df\n\ndef add_shipping_price_rate(df):\n    df[\"shipping_price_rate\"] = df[\"shipping_option_price\"]\/df[\"price\"]\n    return df\n\ndef add_sum_count(df):\n    df[\"sum_count\"] = df[\"rating_five_count\"] \\\n                     + df[\"rating_four_count\"] \\\n                     + df[\"rating_three_count\"] \\\n                     + df[\"rating_two_count\"] \\\n                     + df[\"rating_one_count\"]\n    return df\n\ndef add_rating(df):\n    df[\"rating\"] = df[\"rating_five_count\"]*5 \\\n                 + df[\"rating_four_count\"]*4 \\\n                 + df[\"rating_three_count\"]*3 \\\n                 + df[\"rating_two_count\"]*2 \\\n                 + df[\"rating_one_count\"]*1\n    df[\"rating\"] \/= df[\"sum_count\"]\n    return df\n\ndef add_4_5(df):\n    df[\"four_five_count\"] = df[\"rating_five_count\"] \\\n                         + df[\"rating_four_count\"]\n    return df\n    \n\ndf_train = add_discount_rate(df_train)\ndf_train = add_shipping_price_rate(df_train)\ndf_train = add_sum_count(df_train)\ndf_train = add_rating(df_train)\n\ndf_test = add_discount_rate(df_test)\ndf_test = add_shipping_price_rate(df_test)\ndf_test = add_sum_count(df_test)\ndf_test = add_rating(df_test)\n\ndf_train = add_4_5(df_train)\ndf_test  = add_4_5(df_test)","38824cb9":"\"\"\"def drop_2_4(df):\n    df = df.drop([\"rating_four_count\"],axis=1)\n    df = df.drop([\"rating_three_count\"],axis=1)\n    df = df.drop([\"rating_two_count\"],axis=1)\n    return df\n\ndf_train = drop_2_4(df_train)\ndf_test = drop_2_4(df_test)\"\"\"","c83af278":"\"\"\"def drop_1_5(df):\n    df = df.drop([\"rating_five_count\"],axis=1)\n    df = df.drop([\"rating_one_count\"],axis=1)\n    return df\n\ndf_train = drop_1_5(df_train)\ndf_test = drop_1_5(df_test)\"\"\"","4d3eb59e":"train_txt = []\ntest_txt  = []\n# rm_cols\u306e\u9664\u5916\nfor i, col in enumerate(rm_cols):\n    train_txt.append(df_train[col])\n    test_txt.append(df_test[col])\n    #cats.remove(col_txt)\n    df_train = df_train.drop([col], axis=1)\n    df_test = df_test.drop([col], axis=1)","beeab6e3":"# \u5b66\u7fd2\u30c7\u30fc\u30bf\ny_train = df_train[target_name]\nX_train = df_train.drop([target_name], axis=1)\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\nX_test = df_test.copy()","b32d3053":"# Object\u578b\u306eColumns\ncats  = []\nncats = []\nfor col in X_train.columns:\n    if X_train[col].dtype == 'object':\n        cats.append(col)\n    else:\n        ncats.append(col)","07dfa98e":"# \u6a19\u6e96\u5316\nscaler = StandardScaler()\nscaler.fit(X_train[ncats])\n\nX_train[ncats] = scaler.transform(X_train[ncats])\nX_test[ncats]  = scaler.transform(X_test[ncats]) ","4ca59f85":"# \u6b20\u640d\u5024\u306e\u88dc\u5b8c\nX_train[ncats] = X_train[ncats].fillna(-9999)\nX_test[ncats] = X_test[ncats].fillna(-9999)","b0d41ce5":"X_train = X_train.fillna(X_train.median())\nX_test  = X_test.fillna(X_test.median())","b8ffee5b":"# TargetEncoding\ntarget = target_name\nX_temp = pd.concat([X_train, y_train], axis=1)\n\nfor col in cats:\n    # X_test\u306fX_train\u5168\u4f53\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n    summary = X_temp.groupby([col])[target].mean()\n    X_test[col] = X_test[col].map(summary) \n\n    # X_train\u306foof\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n    skf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True)\n    enc_train = Series(np.zeros(len(X_train)), index=X_train.index)\n\n    for i, (train_ix, val_ix) in enumerate((skf.split(X_train, y_train))):\n        X_train_, _ = X_temp.iloc[train_ix], y_train.iloc[train_ix]\n        X_val, _ = X_temp.iloc[val_ix], y_train.iloc[val_ix]\n\n        summary = X_train_.groupby([col])[target].mean()\n        enc_train.iloc[val_ix] = X_val[col].map(summary)\n        \n    X_train[col]  = enc_train\n    \n# \u6b20\u640d\u3092\u4e2d\u592e\u5024\u3067\u88dc\u5b8c\nX_train = X_train.fillna(X_train.median())\nX_test  = X_test.fillna(X_train.median())","6ec032a1":"df_train_txt = pd.DataFrame()\ndf_test_txt = pd.DataFrame()\n\nfor i in range(len(rm_cols)):\n    # \u30d9\u30af\u30c8\u30eb\u5316\u3059\u308b\u6587\u5b57\u5217\n    train_txt[i] = train_txt[i].fillna(\"#\")\n    test_txt[i]  = test_txt[i].fillna(\"#\")\n    sample = pd.concat([train_txt[0],test_txt[0]], axis=0)\n\n    # TfidfVectorizer\n    vec_tfidf = TfidfVectorizer(max_features=5)\n\n    # \u30d9\u30af\u30c8\u30eb\u5316\n    X = vec_tfidf.fit_transform(sample)\n\n    # CSR -> DataFrame\n    X = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())\n\n    # merge\n    df_train_txt = pd.concat([df_train_txt, X[:len(df_train)]], axis=1)\n    df_test_txt  = pd.concat([df_test_txt,  X[len(df_train):].reset_index(drop=True)], axis=1)\n    \nX_train = pd.concat([X_train,df_train_txt],axis=1)\nX_test  = pd.concat([X_test ,df_test_txt ],axis=1)","e82c8b98":"\"\"\"scores = []\ntrain_preds = [] \npredictions = []\ncv_num = 5\nseed_num = 5\n\n#for seed in range(seed_num):\nskf = StratifiedKFold(n_splits=cv_num, random_state=seed, shuffle=True)\nfor i, (train_ix, test_ix) in enumerate(skf.split(df_train_txt, y_train)):\n    X_train_, y_train_ = df_train_txt.values[train_ix], y_train.values[train_ix]\n    X_val, y_val = df_train_txt.values[test_ix], y_train.values[test_ix]\n\n    model_txt = lgb.LGBMRegressor(boosting_type=\"gbdt\",\n                                  num_leaves=100,\n                                  max_depth=10,\n                                  reg_alpha=0.1,\n                                  reg_lambda=0.1,\n                                  random_state=seed,\n                                  #min_data_in_leaf=1,\n                                  #num_iteration=100,\n                                  verbose=None,\n                                  )\n    model_txt.fit(X_train_, y_train_,\n                  eval_set=(X_val,y_val),\n                  eval_metric=\"mape\", \n                  early_stopping_rounds=200,\n                  verbose=None,\n                  )\n    y_pred = model_txt.predict(X_val)\n    score = cal_mape(y_val, y_pred)\n    scores.append(score)\n    train_preds.append(model_txt.predict(df_train_txt))\n    predictions.append(model_txt.predict(df_test_txt))\n# \u5e73\u5747\u30b9\u30b3\u30a2\u3092\u7b97\u51fa\nprint(seed, \"Average Score is %f\" % np.array(scores).mean())\n    \n    \ntrain_pred = np.zeros_like(train_preds[0])\nfor i in range(cv_num*seed): train_pred += train_preds[i]\ntrain_pred \/= cv_num*seed\n\ntest_pred = np.zeros_like(predictions[0])\nfor i in range(cv_num*seed): test_pred += predictions[i]\ntest_pred \/= cv_num*seed\n\n# \u5217\u306e\u8ffd\u52a0\nX_train[\"text_pred\"] = train_pred\nX_test[\"text_pred\"] = test_pred\n\n# \u6a19\u6e96\u5316\ncol = [\"text_pred\"]\nscaler = StandardScaler()\nscaler.fit(X_train[col])\n\nX_train[col] = scaler.transform(X_train[col])\nX_test[col]  = scaler.transform(X_test[col])\"\"\"","c10e0be7":"# \u6a19\u6e96\u5316\ncols = X_train.columns\nscaler = StandardScaler()\nscaler.fit(X_train[cols])\nX_train[cols] = scaler.transform(X_train[cols])\nX_test[cols]  = scaler.transform(X_test[cols])","262212be":"\"\"\"def post_process(num):\n    #getNearestValue\n    list = [100,200,500,1000,5000,10000,20000]\n    # \u30ea\u30b9\u30c8\u8981\u7d20\u3068\u5bfe\u8c61\u5024\u306e\u5dee\u5206\u3092\u8a08\u7b97\u3057\u6700\u5c0f\u5024\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u53d6\u5f97\n    idx = np.abs(np.asarray(list) - num).argmin()\n    return list[idx]\n\ndef post_process_v2(num):\n    #getNearestValue\n    list = [100,200,500,1000,5000,10000,20000]\n    # \u30ea\u30b9\u30c8\u8981\u7d20\u3068\u5bfe\u8c61\u5024\u306e\u5dee\u5206\u3092\u8a08\u7b97\u3057\u6700\u5c0f\u5024\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u53d6\u5f97\n    idx = np.abs(np.asarray(list) - num).argmin()\n    if num > 7500:\n        return list[idx]\n    else:\n        return num\n\ndef predict_and_post_process(model,x):\n    y = model.predict(x)\n    #y = list(map(lambda x: post_process(x),y))\n    y = list(map(lambda x: post_process_v2(x),y))\n    y = np.array(y,dtype=np.float64)\n    return y\"\"\"","87c4089a":"\"\"\"        clf = MLPRegressor(hidden_layer_sizes = (100,10,10,), \n                            #max_iter=1000, \n                            random_state=seed,\n                            learning_rate=\"adaptive\",\n                            solver=\"sgd\")\n        clf.fit(X_train_, y_train_)\n        y_pred_mlp = clf.predict(X_val)  \n        score_mlp = cal_mape(y_val, y_pred_mlp)\n        scores_mlp.append(score_mlp)\n        predictions_mlp.append(clf.predict(X_test))\n\"\"\"        ","f08c286f":"#scores_mlp = []\n#predictions_mlp = []\nscores = []\npredictions = []\ncv_num = 5\nseed_num = 20\n\nfor seed in range(seed_num):\n    skf = StratifiedKFold(n_splits=cv_num, random_state=seed, shuffle=True)\n    for i, (train_ix, test_ix) in enumerate(skf.split(X_train, y_train)):\n        X_train_, y_train_ = X_train.values[train_ix], y_train.values[train_ix]\n        X_val, y_val = X_train.values[test_ix], y_train.values[test_ix]\n\n        model = lgb.LGBMRegressor(boosting_type=\"gbdt\",\n                                  num_leaves=100,\n                                  max_depth=10,\n                                  reg_alpha=0.05,\n                                  reg_lambda=0.05,\n                                  random_state=seed,\n                                  #min_data_in_leaf=1,\n                                  #num_iteration=100,\n                                  verbose=None,\n                                  )\n        model.fit(X_train_, y_train_,\n                  eval_set=(X_val,y_val),\n                  eval_metric=\"mape\", \n                  early_stopping_rounds=200,\n                  verbose=None,\n                  )\n        y_pred = model.predict(X_val)\n        score = cal_mape(y_val, y_pred)\n        scores.append(score)\n\n        #print('CV Score of Fold_%d is %f' % (i, score))\n        #predictions.append(predict_and_post_process(model,X_test))\n        predictions.append(model.predict(X_test))\n\n    # \u5e73\u5747\u30b9\u30b3\u30a2\u3092\u7b97\u51fa\n    print(\"seed : \", seed, \", Average Score is %f\" % np.array(scores).mean())","8b3cd182":"pred = np.zeros_like(predictions[0])\nfor i in range(cv_num*seed): pred += predictions[i]\npred \/= cv_num*seed\n\n# sample submission\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001\u4e88\u6e2c\u5024\u3092\u4ee3\u5165\u306e\u5f8c\u3001\u4fdd\u5b58\u3059\u308b\nsubmission = pd.read_csv('\/kaggle\/input\/sales-prediction-of-clothes-in-e-commerce\/sample_submission.csv', index_col=0)\n\nsubmission[target] = pred # \u4e88\u6e2c\u5024\u3092\u63d0\u51fa\u7528\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306b\u4ee3\u5165\u3057\u307e\u3059\u3002\nsubmission.to_csv('.\/submission.csv')\nsubmission","90d84e6b":"import matplotlib.pyplot as plt\nplt.figure(figsize=(8,8))\nax = plt.subplot(111)\nax.scatter(y_val, y_pred)\nax.set_xlabel('True Value', fontsize=15)\nax.set_ylabel('Pred Value', fontsize=15)\nx = np.linspace(y_pred.min().min()-0.1, y_pred.max().max()+0.1, 2)\ny = x\nax.plot(x,y,'r-')","03cd6fb1":"lgb.plot_importance(model, height=0.5, figsize=(8,16))","b9ed9700":"for i, col in enumerate(X_train.columns):\n    print(i, col)","72ded5fc":"## CV\uff08\u4ea4\u5dee\u691c\u5b9a\uff09","e676af1f":"## \u63d0\u51fa","423e1633":"### Numeric\u578b\u306e\u7279\u5fb4\u91cf","fb9bd65e":"## X\uff08\u7279\u5fb4\u91cf\uff09\u3068 y\uff08\u30bf\u30fc\u30b2\u30c3\u30c8\uff09\u306b\u30c7\u30fc\u30bf\u3092\u5206\u5272","11cacb7a":"## \u7279\u5fb4\u91cf\u306e\u8ffd\u52a0\u30fb\u524a\u9664","6252c598":"### TXT\u578b\u306e\u7279\u5fb4\u91cf","e5ade6dd":"### Object\u578b\u306e\u7279\u5fb4\u91cf"}}