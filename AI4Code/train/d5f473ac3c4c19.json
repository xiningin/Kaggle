{"cell_type":{"4dd1d7f7":"code","8e5ef426":"code","88ab5f53":"code","dc2c42a1":"code","97d63d7c":"code","1937d320":"code","8dab3a7d":"code","fbcc260b":"code","7a9cfbb1":"code","fde6a5a6":"code","a62d06e4":"code","306dcb40":"code","24de9bf4":"code","871e22af":"code","19800e8f":"code","ae8a98c4":"code","43330ae2":"code","72b9e04d":"code","c592082d":"code","4100f3c1":"code","a9b400a0":"code","2ae99def":"code","26bd719d":"code","9f54a325":"code","db9db639":"code","6e2b7f34":"code","3cf1cbc9":"code","d149dd88":"code","635ea785":"code","5d70c953":"code","5bd681e9":"code","e81109ac":"code","e562bdd2":"code","f72b3ca1":"code","a5ab340b":"code","a2c85d7b":"code","484ecbce":"code","3a497d66":"code","a934641a":"code","53d745bb":"markdown","0aadad54":"markdown","13311346":"markdown","97d9b567":"markdown","bd91aa7b":"markdown","bb5b6d8a":"markdown","ec96ba7c":"markdown","029a3252":"markdown","ab332ec6":"markdown"},"source":{"4dd1d7f7":"!apt install aptitude -y\n!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n!pip install mecab-python3==0.996.6rc2\n\n!pip install unidic-lite\n!pip install neologdn\n\n!pip install python-Levenshtein","8e5ef426":"import json\nimport gc\nimport os\nimport re\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport MeCab\n\nimport pickle\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport tqdm\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom transformers import *\n\nimport neologdn\nimport Levenshtein\n\nm = MeCab.Tagger (\"-Ochasen\")","88ab5f53":"class config:\n    OUTPUT = \"\/kaggle\/working\"\n    MAX_LEN = 512\n    TOKENIZER = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku\/bert-base-japanese-whole-word-masking\")\n\nlen(config.TOKENIZER)","dc2c42a1":"!wget https:\/\/jaqket.s3-ap-northeast-1.amazonaws.com\/data\/train_questions.json 2> \/dev\/null\n!wget https:\/\/jaqket.s3-ap-northeast-1.amazonaws.com\/data\/dev1_questions.json 2> \/dev\/null\n!wget https:\/\/jaqket.s3-ap-northeast-1.amazonaws.com\/data\/dev2_questions.json 2> \/dev\/null\n!wget https:\/\/jaqket.s3-ap-northeast-1.amazonaws.com\/data\/candidate_entities.json.gz 2> \/dev\/null\n#!wget https:\/\/jaqket.s3-ap-northeast-1.amazonaws.com\/data\/all_entities.json.gz\n!wget --no-check-certificate https:\/\/www.nlp.ecei.tohoku.ac.jp\/projects\/AIP-LB\/static\/aio_leaderboard.json\n!gzip -d candidate_entities.json.gz\n#!gzip -d all_entities.json.gz","97d63d7c":"df_train_questions = pd.read_json('train_questions.json', orient='records', lines=True)\ndf_dev1_questions = pd.read_json('dev1_questions.json', orient='records', lines=True)\ndf_dev2_questions = pd.read_json('dev2_questions.json', orient='records', lines=True)\ndf_candidate_entities = pd.read_json('candidate_entities.json', orient='records', lines=True)\ndf_aio_leaderboard = pd.read_json(\"aio_leaderboard.json\", orient='records', lines=True)","1937d320":"dfs = []\nfor answer_entity, df in df_train_questions.groupby(\"answer_entity\"):\n    df = df.reset_index(drop=False)\n    if len(df) == 1:\n        df[\"min_levenshtein\"] = 9999\n        dfs.append(df)\n        continue\n    leven_d = []\n    for idx1 in range(len(df)):\n        q1 = df.iloc[idx1][\"question\"]\n        d_lst = []\n        for idx2 in range(len(df)):\n            if idx1 == idx2:\n                continue\n            q2 = df.iloc[idx2][\"question\"]\n            d = Levenshtein.distance(q1, q2)\n            d_lst.append(d)\n        min_d = min(d_lst)\n        leven_d.append(min_d)\n    df[\"min_levenshtein\"] = leven_d\n    dfs.append(df)\ndf_add_min_levenshtein = pd.concat(dfs).reset_index(drop=True)\n#df_add_min_levenshtein[\"loss_w\"] = df_add_min_levenshtein[\"min_levenshtein\"].map(lambda x: 1.0 if x > 20 else x\/20)","8dab3a7d":"duplicate_q_text_ids = []\nfor answer_entity, df in df_add_min_levenshtein.query(\"min_levenshtein<2\").groupby(\"answer_entity\"):\n    duplicate_q_text_ids += df[\"qid\"].values[1:].tolist()\nlen(duplicate_q_text_ids)","fbcc260b":"print(\"before:\", len(df_train_questions))\ndf_train_questions = df_train_questions.query(f\"qid not in {duplicate_q_text_ids}\").reset_index(drop=True)\nprint(\"after:\", len(df_train_questions))","7a9cfbb1":"def get_yomi(text):\n    wakati = [w.split(\"\\t\") for w in m.parse (text).split(\"\\n\")[:-2]]\n    yomi = \"\".join([w[1] for w in wakati])\n    return yomi\n\ndef fix_kana(text):\n    text = text.replace(\"\u30cb\u30e5\", \"\u30cc\")\n    text = text.replace(\"\u30c1\u30e5\u30a2\", \"\u30c1\u30e3\")\n    text = text.replace(\"\u30c7\u30a3\", \"\u30b8\")\n    text = text.replace(\"\u30f4\u30a1\", \"\u30d0\")\n    text = text.replace(\"\u30f4\u30a3\", \"\u30d3\")\n    text = text.replace(\"\u30f4\", \"\u30d6\")\n    text = text.replace(\"\u30f4\u30a7\", \"\u3079\")\n    text = text.replace(\"\u30f4\u30a9\", \"\u30dc\")\n    text = text.replace(\"\u30ba\", \"\u30b9\")\n    text = text.replace(\"\u30c3\", \"\")\n    text = text.replace(\"\u30fc\", \"\")\n    text = text.replace(\"\u30fb\", \"\")\n    return text\n\ndef split_kakkko(text):\n    kakko1 = re.findall(r\"\uff08.*?\uff09\", text)\n    kakko2 = re.findall(r\"\\(.*?\\)\", text)\n    kakko = kakko1 + kakko2\n    for w in kakko:\n        text = text.replace(w, \"\")\n    kakko_w = [k.replace(\"\uff08\", \"\").replace(\"\uff09\", \"\") for k in kakko]\n    sp_kakko = [text] + kakko_w\n    return sp_kakko\n\ndef del_kakko(text):\n    return text.replace(\")\", \"\").replace(\"()\", \"\").replace(\"\uff08\", \"\").replace(\"\uff09\", \"\")\n\ndef text_norm(text):\n    text = neologdn.normalize(text)\n    text = fix_kana(text)\n    text = get_yomi(text)\n    return text\n\ndef exist_sep_dot(answer_entity, original_answer):\n    sep_ans = answer_entity.split(\"\u30fb\")\n    sep_ans = [fix_kana(s) for s in sep_ans]\n    original_answer = fix_kana(original_answer)\n    sep_ans = [s in original_answer for s in sep_ans]\n    return sum(sep_ans) == len(sep_ans)","fde6a5a6":"# \u30ce\u30fc\u30de\u30e9\u30a4\u30ba\u3057\u305f\u30c6\u30ad\u30b9\u30c8\n_df_train_questions = df_train_questions[df_train_questions.apply(\n    lambda x: neologdn.normalize(x[\"answer_entity\"]) not in neologdn.normalize(x[\"original_answer\"]) and \n    neologdn.normalize(x[\"original_answer\"]) not in neologdn.normalize(x[\"answer_entity\"]), axis=1)].reset_index(drop=True)\n_df_train_questions.shape","a62d06e4":"# \u8aad\u307f\u306e\u307f\n_df_train_questions = _df_train_questions[_df_train_questions.apply(\n    lambda x: get_yomi(x[\"answer_entity\"]) not in get_yomi(x[\"original_answer\"]) and \n    get_yomi(x[\"original_answer\"]) not in get_yomi(x[\"answer_entity\"]), axis=1)].reset_index(drop=True)\n_df_train_questions.shape","306dcb40":"# \u30ab\u30ca\u3092\u4fee\u6b63\u3057\u305f\u3082\u306e\n_df_train_questions = _df_train_questions[_df_train_questions.apply(lambda x:\n                          fix_kana(x[\"answer_entity\"]) not in fix_kana(x[\"original_answer\"]) and\n                          fix_kana(x[\"original_answer\"]) not in fix_kana(x[\"answer_entity\"]), axis=1)].reset_index(drop=True)\n_df_train_questions.shape","24de9bf4":"# \u30c6\u30ad\u30b9\u30c8\u6b63\u898f\u5316\u3092\u307e\u3068\u3081\n_df_train_questions = _df_train_questions[_df_train_questions.apply(lambda x:\n                          text_norm(x[\"answer_entity\"]) not in text_norm(x[\"original_answer\"]) and\n                          text_norm(x[\"original_answer\"]) not in text_norm(x[\"answer_entity\"]), axis=1)].reset_index(drop=True)\n_df_train_questions.shape","871e22af":"# \u30ab\u30c3\u30b3\u306a\u3044\u3092\u8003\u616e\n_df_train_questions = _df_train_questions[_df_train_questions.apply(\n    lambda x: len(set(split_kakkko(x[\"answer_entity\"])) & set(split_kakkko(x[\"original_answer\"]))) == 0, axis=1)].reset_index(drop=True)\n_df_train_questions.shape","19800e8f":"# \u30ab\u30c3\u30b3\u3092\u6d88\u3059\n_df_train_questions = _df_train_questions[_df_train_questions.apply(lambda x:\n                          del_kakko(x[\"answer_entity\"]) not in del_kakko(x[\"original_answer\"]) and\n                          del_kakko(x[\"original_answer\"]) not in del_kakko(x[\"answer_entity\"]), axis=1)].reset_index(drop=True)\n_df_train_questions.shape","ae8a98c4":"# \u554f\u984c\u6587\u306b\u542b\u307e\u308c\u308b\nignore_qid = _df_train_questions[_df_train_questions.apply(lambda x: x[\"answer_entity\"] in x[\"question\"], axis=1)][\"qid\"].values\n\n_df_train_questions = _df_train_questions[_df_train_questions.apply(\n    lambda x: x[\"answer_entity\"] not in x[\"question\"], axis=1)].reset_index(drop=True)\n_df_train_questions.shape","43330ae2":"# \u7de8\u96c6\u8ddd\u96e2\u4e0b\u9650\n_df_train_questions[\"LevenshteinDist\"] = _df_train_questions.apply(lambda x:\n                          Levenshtein.distance(text_norm(x[\"answer_entity\"]), text_norm(x[\"original_answer\"])), axis=1)\n_df_train_questions = _df_train_questions[_df_train_questions[\"LevenshteinDist\"] > 1].reset_index(drop=True)\n_df_train_questions.shape","72b9e04d":"# \u7de8\u96c6\u8ddd\u96e2\u4e0a\u9650\n_df_train_questions[\"LevenshteinDist\"] = _df_train_questions.apply(lambda x:\n                          Levenshtein.distance(text_norm(x[\"answer_entity\"]), text_norm(x[\"original_answer\"])), axis=1)\n_df_train_questions = _df_train_questions[_df_train_questions[\"LevenshteinDist\"] < 50].reset_index(drop=True)\n_df_train_questions.shape","c592082d":"# \u30fb\u3067\u5206\u3051\u305f\u540d\u524d\n_df_train_questions = _df_train_questions[_df_train_questions.apply(\n    lambda x: not exist_sep_dot(x[\"answer_entity\"], x[\"original_answer\"]), axis=1)].reset_index(drop=True)\n_df_train_questions = _df_train_questions[_df_train_questions.apply(\n    lambda x: not exist_sep_dot(x[\"original_answer\"], x[\"answer_entity\"]), axis=1)].reset_index(drop=True)\n_df_train_questions.shape","4100f3c1":"#\u8a00\u3044\u63db\u3048\u8868\u73fe\n# \u4f8b\n# \u7cd6\u3092\u5206\u89e3\u3059\u308b\u50cd\u304d\u304c\u3042\u308b\u3001\u81b5\u81d3\u306e\u30e9\u30f3\u30b2\u30eb\u30cf\u30f3\u30b9\u5cf6\u03b2\u7d30\u80de\u304b\u3089\u5206\u6ccc\u3055\u308c\u308b\u30db\u30eb\u30e2\u30f3\u306f\u4f55\u3067\u3057\u3087\u3046?\n# \u30a4\u30f3\u30b9\u30ea\u30f3\uff08\u30a4\u30f3\u30b7\u30e5\u30ea\u30f3\u3001insulin\uff09\u306f\u3001\u81b5\u81d3\u306b\u5b58\u5728\u3059\u308b\u30e9\u30f3\u30b2\u30eb\u30cf\u30f3\u30b9\u5cf6\uff08\u81b5\u5cf6\uff09\u306e\u03b2\u7d30\u80de\u304b\u3089\u5206\u6ccc\u3055\u308c\u308b\u30da\u30d7\u30c1\u30c9\u30db\u30eb\u30e2\u30f3\u306e\u4e00\u7a2e\n# \u30a4\u30f3\u30b9\u30ea\u30f3 \u30a4\u30f3\u30b7\u30e5\u30ea\u30f3\n\nlst = []\nfor _, row in _df_train_questions.iterrows():\n    wiki = df_candidate_entities[df_candidate_entities[\"title\"] == row[\"answer_entity\"]][\"text\"].iloc[0]\n    if row[\"original_answer\"] not in wiki:\n        lst.append(row)\n        continue\n        \n    h_text = wiki.split(row[\"original_answer\"])[0]\n    t_text = wiki.split(row[\"original_answer\"])[1]\n    if len(h_text) == 0:\n        continue\n    if h_text[-1] == \"\u300c\" and  t_text[0] == \"\u300d\":\n        continue\n    if h_text[-1] == \"\u300e\" and  t_text[0] == \"\u300f\":\n        continue\n    if t_text[0] in [\"(\", \"\uff08\"]:\n        continue\n    if t_text[0] == \"\u3068\":\n        # \u3068\u547c\u3070\u308c\u3001\u3068\u8a00\u308f\u308c\u3001\u306a\u3069\n        continue\n    if t_text[0] in [\" \", \"\u3001\", \")\", \"\uff09\"]:\n        continue\n    #if h_text[-1] in [\"\u3001\"]:\n    #    print(row[\"question\"])\n    #    print(row[\"original_answer\"], row[\"answer_entity\"])\n    #    print(f\"{h_text[-10:]}<WORD>{h_text[:10]}\")\n    #    print()\n    lst.append(row)\n\n_df_train_questions = pd.DataFrame(lst).reset_index(drop=True)\n_df_train_questions.shape","a9b400a0":"ignore_qid_candidates = _df_train_questions[\"qid\"].values\n\nprint(\"ignore:\", len(ignore_qid))\nprint(\"candidate:\", len(ignore_qid_candidates))","2ae99def":"#train_preds = np.load(\"\/kaggle\/input\/aio-train-prediction-check\/train_preds.npy\")\n#df_train_questions[\"predict_idx\"] = train_preds\n#correct_qid = df_train_questions.query(\"predict_idx==0\")[\"qid\"].values\n#correct_data = list(set(ignore_qid_candidates) & set(correct_qid))\n#_df_train_questions.query(f\"qid not in {correct_data}\")","26bd719d":"not_good_data = list(ignore_qid) + list(ignore_qid_candidates)\nlen(not_good_data)","9f54a325":"use_index = df_train_questions.query(f\"qid not in {not_good_data}\").index.values\nnp.save(\"use_index\", use_index)","db9db639":"print(\"bofore:\", len(df_train_questions))\ndf_train_questions = df_train_questions.query(f\"qid not in {not_good_data}\").reset_index(drop=True)\nprint(\"after:\", len(df_train_questions))","6e2b7f34":"def extract_kakko(text):\n    re_text1 = re.findall(r\"\u300c.*?\u300d\", text)\n    re_text2 = re.findall(r\"\u300e.*?\u300f\", text)\n    re_text1 = [t.replace(\"\u300c\", \"\").replace(\"\u300d\", \"\") for t in re_text1]\n    re_text2 = [t.replace(\"\u300e\", \"\").replace(\"\u300f\", \"\") for t in re_text2]\n    return re_text1 + re_text2\n\nIGNORE_WORDS = [\n    \"\u3042\u308b\", \"\u4f55\", \"?\", \"\u305f\u3081\", \"\u3053\u3068\", \"\u3089\u308c\", \"\u3044\", \"\u3059\u308b\", \n    \"\u3044\u3046\", \"\u3064\u304f\", \"\u306e\", \"\u3046\u3061\", \"\u3067\u304d\u308b\", \"\u884c\u3046\", \"\u8868\u3059\"\n]\ndef wakati_mecab(text):\n    wakati = [w.split(\"\\t\") for w in m.parse (text).split(\"\\n\")[:-2]]\n    \n    pos_filter = [w[3].split(\"-\")[0] in [\"\u540d\u8a5e\", \"\u52d5\u8a5e\", \"\u5f62\u5bb9\u8a5e\"] for w in wakati]\n    verb_filter = [w[3] not in [\"\u52d5\u8a5e-\u63a5\u5c3e\", \"\u52d5\u8a5e-\u975e\u81ea\u7acb\"] for w in wakati]\n    is_filter = np.array(verb_filter).astype(int) + np.array(pos_filter).astype(int) == 2\n    wakati = np.array(wakati)[is_filter]\n    \n    words1 = [w[2] for w in wakati if w[2] not in IGNORE_WORDS]  # \u8868\u5c64=2\n    words2 = [w[1] for w in wakati if w[2] not in IGNORE_WORDS]  # \u8aad\u307f=1\n    words = list(set(words1 + words2))\n    return words\n\ndef split_question(question):\n    question_norm = neologdn.normalize(question)\n    key_phrase = extract_kakko(question_norm)\n    _question_norm = question_norm\n    for w in key_phrase:\n        _question_norm = _question_norm.replace(w, \"\")\n    sep_question = wakati_mecab(_question_norm)\n    return key_phrase, sep_question","3cf1cbc9":"def split_contents(contents, n_char=64, duplicate=5):\n    n_roop = len(contents)\n    lst = []\n    head_i = 0\n    for idx in range(n_roop):\n        tail_i = head_i + n_char\n        line = contents[head_i:tail_i]\n        if len(line) == 0:\n            break\n        lst.append(line)\n        head_i += n_char - duplicate\n    return lst\n\nurl_pattern = r'(http|https):\/\/([-\\w]+\\.)+[-\\w]+(\/[-\\w.\/?%&=]*)?'\ndef extract_contents_df(question, contents):\n    key_phrase, sep_question = split_question(question)\n    \n    lst = []\n    #lines = re.split(\"\u3002|\u3001|\\^\", contents)\n    lines = split_contents(contents)\n    for line in lines:\n        if len(line) == 0:\n            continue\n        if line[0] == \" \":\n            line = line[1:]\n        line = re.sub(url_pattern, \"\", line)\n        line = neologdn.normalize(line)\n        if len(line) == 0:\n            continue\n\n        _key_phrase= extract_kakko(line)\n        words = [w for w in key_phrase if w in line]\n        sp_line = wakati_mecab(line)\n        common_words = list(set(sep_question) & set(sp_line))\n\n        #if len(common_words)+len(words) == 0:\n        #    continue\n            \n        d = (len(words), len(common_words), line, words, common_words)\n\n        lst.append(d)\n    contents_df = pd.DataFrame(lst, columns=[\"n_keyword\", \"n_common\", \"text\", \"keyword\", \"common\"])\n    contents_df = contents_df.sort_values([\"n_keyword\", \"n_common\"], ascending=False).reset_index(drop=True)\n    return contents_df","d149dd88":"def extract_contents(contents_df):\n    exist_words = []\n    texts = []\n    sub_texts = []\n    for _, row in contents_df.iterrows():\n        words = row[\"keyword\"] + row[\"common\"]\n        diff_words = set(words) - set(exist_words)\n        if len(diff_words) > 0:\n            exist_words = list(set(words) | set(exist_words))\n            texts.append(row[\"text\"])\n        else:\n            sub_texts.append(row[\"text\"])\n    extracted_content = \"\u3002\".join(texts+sub_texts)\n    return extracted_content","635ea785":"def process_but(text):\n    sp_q = text.split(\"\u3067\u3059\u304c\u3001\")\n    tail_text = sp_q[-1]\n    sp_head = sp_q[0].split(\"\u3001\")\n    if len(sp_head) > 1:\n        text = sp_head[0] + \"\u3001\" + tail_text\n    else:\n        text = tail_text\n    return text","5d70c953":"def __get_question_tag(text):\n    if text[-1] == \"\u4f55\":\n        tag = \"[WHAT]\"\n    elif text[-1] == \"\u8ab0\":\n        tag = \"[WHO]\"\n    elif text[-2:] == \"\u3069\u3053\":\n        tag = \"[WHERE]\"\n    elif text[-2:] == \"\u3069\u308c\" or text[-3:] == \"\u3069\u3061\u3089\":\n        tag = \"[WHICH]\"\n    elif text[-3:] == \"\u3068\u3044\u3046\":\n        tag = \"[WHAT SAY]\"\n    else:\n        tag = \"[OTHER]\"\n    return tag\n\ndef get_question_tag(question):\n    head_sp = question.replace(\"?\", \"\").split(\"\u3067\u3057\u3087\u3046\")[0]\n    tag = __get_question_tag(head_sp)\n    if tag != \"[OTHER]\":\n        return tag\n    if head_sp[-3:] == \"\u306e\u3053\u3068\":\n        tag = get_question_tag(head_sp[:-3])\n        if tag != \"[OTHER]\":\n            return tag\n    if \"\u4f55\u3068\u3044\u3046\" in head_sp or \"\u4f55\u3068\u8a00\u3046\" in head_sp or \"\u3068\u547c\u3076\" in head_sp: \n        tag = \"[WHAT SAY]\"\n    if \"\u3069\u3053\u306e\" in head_sp:\n        tag = \"[WHERE]\"\n    if \"\u3069\u3093\u306a\" in head_sp or \"\u3069\u306e\u3088\u3046\u306a\" in head_sp:\n        tag = \"[LIKE WHAT]\"\n    if \"\u3044\u304f\u3064\" in head_sp:\n        tag = \"[HOW MANY]\"\n    if \"\u3060\u308c\" in head_sp or \"\u8ab0\" in head_sp:\n        tag = \"[WHO]\"\n    if head_sp.split(\"\u4f55\")[-1] in [\"\u770c\", \"\u5e02\", \"\u5dde\", \"\u5ddd\", \"\u6d77\u5ce1\", \"\u99c5\"]:\n        tag = \"[WHERE]\"\n    if head_sp.split(\"\u4f55\")[-1] in [\"\u65e5\", \"\u5e74\", \"\u6708\"]:\n        tag = \"[WHEN]\"\n    return tag","5bd681e9":"tag_count = df_train_questions[\"question\"].map(get_question_tag).value_counts()\ntag_count","e81109ac":"#for tag in tag_count.index:\n#    config.TOKENIZER.add_tokens(tag)\n\nprint(len(config.TOKENIZER))\nprint(config.TOKENIZER.added_tokens_encoder)","e562bdd2":"df = df_train_questions.sample(1, random_state=7)\n#df = df_dev1_questions.sample(1, random_state=1003)\nquestion, answer_candidates = df.values[0][[1, 3]]\nprint(question)\nprint(\",\".join(answer_candidates))\n\nc_idx = 0\n\ncandidate = answer_candidates[c_idx]\ncontents = df_candidate_entities[df_candidate_entities[\"title\"] == candidate][\"text\"].iloc[0]\nprint(candidate)\n\ncontents_df = extract_contents_df(question, contents)\ncontents_df","f72b3ca1":"#extract_contents(contents_df)","a5ab340b":"def tokenize_dataframe(input_df):\n    inputs = []\n    for _, row in tqdm.notebook.tqdm(input_df.iterrows(), total=len(input_df)):\n        question = row[\"question\"]\n        if \"\u3067\u3059\u304c\u3001\" in question:\n            question = process_but(question)\n        #tag = get_question_tag(question)\n        tag = \"[SEP]\"\n\n        _ids, _mask, _id_type = [], [], [] \n        for candidate in row[\"answer_candidates\"]:\n            contents = df_candidate_entities[df_candidate_entities[\"title\"] == candidate][\"text\"].iloc[0]\n            contents_df = extract_contents_df(question, contents)\n            extracted_content = extract_contents(contents_df)\n            #if extracted_content == \"\":\n            #    extracted_content = contents\n            \n            extracted_content = candidate + tag + extracted_content\n\n            tok = config.TOKENIZER.encode_plus(extracted_content,\n                                               question,\n                                               add_special_tokens=True,\n                                               max_length=config.MAX_LEN,\n                                               truncation_strategy=\"only_first\",\n                                               pad_to_max_length=True)\n            _ids.append(tok[\"input_ids\"])\n            _mask.append(tok[\"attention_mask\"])\n            _id_type.append(tok[\"token_type_ids\"])\n\n        try:\n            label = row[\"answer_candidates\"].index(row[\"answer_entity\"])\n        except ValueError:\n            label = 999  # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u30e9\u30d9\u30eb\u306f\u3068\u308a\u3042\u3048\u305a999\u3068\u3057\u3066\u304a\u304f\n\n        d = {\n            \"input_ids\": _ids,\n            \"attention_mask\": _mask,\n            \"token_type_ids\": _id_type, \n            \"label\": label\n        }    \n        inputs.append(d)\n    return inputs","a2c85d7b":"dev1 = tokenize_dataframe(df_dev1_questions)\n\ndel df_dev1_questions\ngc.collect()\n\nwith open(f\"{config.OUTPUT}\/dev1.pkl\", \"wb\") as f:\n    pickle.dump(dev1, f)","484ecbce":"dev2 = tokenize_dataframe(df_dev2_questions)\n\ndel df_dev2_questions \ngc.collect()\n\nwith open(f\"{config.OUTPUT}\/dev2.pkl\", \"wb\") as f:\n    pickle.dump(dev2, f)","3a497d66":"test = tokenize_dataframe(df_aio_leaderboard)\n\ndel df_aio_leaderboard \ngc.collect()\n\nwith open(f\"{config.OUTPUT}\/test.pkl\", \"wb\") as f:\n    pickle.dump(test, f)","a934641a":"train = tokenize_dataframe(df_train_questions)\n\ndel df_train_questions, df_candidate_entities\ngc.collect()\n\nwith open(f\"{config.OUTPUT}\/train.pkl\", \"wb\") as f:\n  pickle.dump(train, f)","53d745bb":"# \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c","0aadad54":"# \u524d\u51e6\u7406","13311346":"## \u540c\u3058\u554f\u984c\u306b\u3064\u3044\u3066","97d9b567":"## qid\u306e\u30ea\u30b9\u30c8\u4f5c\u6210","bd91aa7b":"# \u30af\u30a4\u30baAI\u738b","bb5b6d8a":"# train\u30c7\u30fc\u30bf\u306e\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0","ec96ba7c":"# \u30c8\u30fc\u30af\u30ca\u30a4\u30ba","029a3252":"## \u30bf\u30b0\u30c8\u30fc\u30af\u30f3\u306e\u8ffd\u52a0","ab332ec6":"# \u7121\u8996\u3059\u308b\u554f\u984c\u3092\u62bd\u51fa"}}