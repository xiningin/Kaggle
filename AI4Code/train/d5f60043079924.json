{"cell_type":{"e3c17615":"code","f65e94cc":"code","88b65709":"code","a30c9e84":"code","a2747bb5":"code","fb444b5c":"code","0e3fd833":"code","6517e23e":"code","ca68c44d":"code","6487e723":"code","edbbfdd2":"code","4658c1ae":"code","199bd401":"code","3279f84e":"code","d40d467c":"code","8a3aa794":"code","32dfe29d":"code","89022b96":"code","29c23649":"code","d55d1abd":"code","3df134c7":"code","82e61d40":"code","0e571776":"code","01852db6":"code","a4c75fdf":"code","8e227f48":"code","670695f4":"code","5bf0bd5d":"code","2c65e9de":"code","6751542e":"code","af978b42":"code","a7d5c81e":"code","2355b539":"code","edd256dc":"code","bd1e23f8":"code","2fe5b210":"code","98e5c06a":"code","f6381c03":"code","4bcabbb6":"code","c4d6ab1c":"code","10f179ac":"code","fffbb2eb":"code","fe892cb5":"code","2e627e21":"code","93ef60f5":"code","0c0744e3":"code","c10f3915":"code","0008d633":"code","2339bb5b":"code","b7ef88f6":"code","e3522efd":"code","cc00827e":"markdown","9d000e35":"markdown","a139b31d":"markdown","f6c85f01":"markdown","264f0e88":"markdown","4e0b795a":"markdown","dd18dd21":"markdown","69a3a76d":"markdown","c89ea596":"markdown","28bf9a48":"markdown","b1cbe3fc":"markdown","6e82191d":"markdown","c170baec":"markdown","35c945c7":"markdown","e7f8973f":"markdown","08070040":"markdown","885c3888":"markdown","bf233db8":"markdown","bd4e09b9":"markdown","0091ca2c":"markdown","efa4ef85":"markdown","fba3dba5":"markdown","72992239":"markdown","132d8c64":"markdown","9c896963":"markdown","80000255":"markdown","bd2714d1":"markdown","6a383ea1":"markdown","1ab163eb":"markdown","4300b549":"markdown","09b9506b":"markdown","6ccfb9ec":"markdown","845e5818":"markdown","70faf880":"markdown","a6263b7b":"markdown","c20e4ecf":"markdown","c719dd31":"markdown"},"source":{"e3c17615":"# Basic essential libraries >>>\nimport numpy as np\nimport pandas as pd\nimport os \nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\nimport time\nimport sys","f65e94cc":"# Accessing the data >>> \ntrain = train = pd.read_csv(\"..\/input\/train-costa-rica\/train.csv\")\nprint('Shape : ',train.shape)\npd.options.display.max_columns = None\ntrain.head(2)","88b65709":"Null_Df = pd.DataFrame({'Null_Count':train.isnull().sum(),\n                        'Null_Perc %':round(train.isnull().sum()*100\/train.shape[0],2)})\nNull_Df[Null_Df.Null_Count!=0]","a30c9e84":"print('Mean -->',train.v2a1_num.mean())\nprint('Median -->',train.v2a1_num.median())\nprint('Std -->',train.v2a1_num.std())\nsns.distplot(train[train.v2a1_num.notnull()].v2a1_num)\nplt.show()","a2747bb5":"# As we can see that all the data is centred at on point only hence we can replace the missing values with average values.\nprint('Mean BEFORE Change : ',train.v2a1_num.mean())\ntrain.v2a1_num.fillna(train.v2a1_num.mean(),inplace=True)\nprint('Mean AFTER Change : ',train.v2a1_num.mean())\nprint('\\nMissing Values :')\ntrain.v2a1_num.isnull().value_counts()","fb444b5c":"print('Mean -->',train.v18q1_num.mean())\nprint('Median -->',train.v18q1_num.median())\nprint('Std -->',train.v18q1_num.std())\n\nsns.distplot(train[train.v18q1_num.notnull()].v18q1_num)\nplt.show()","0e3fd833":"train.v18q1_num.fillna(train.v18q1_num.median(),inplace=True)\nprint('\\nMissing Values :')\ntrain.v18q1_num.isnull().value_counts()","6517e23e":"print('Mean -->',train.rez_esc_num.mean())\nprint('Median -->',train.rez_esc_num.median())\nprint('Std -->',train.rez_esc_num.std())\n\nsns.distplot(train[train.rez_esc_num.notnull()].rez_esc_num)\nplt.show()","ca68c44d":"# \"Unlike\" v18q1_num, this number can be in decimals hence we will replace the missing values with mean value >>>  \nprint('Mean BEFORE Change : ',train.rez_esc_num.mean())\ntrain.rez_esc_num.fillna(train.rez_esc_num.mean(),inplace=True)\nprint('Mean AFTER Change : ',train.rez_esc_num.mean())\nprint('\\nMissing Values :')\ntrain.rez_esc_num.isnull().value_counts()","6487e723":"train.meaneduc_num.fillna(method='ffill',inplace=True)\ntrain.SQBmeaned_num.fillna(method='ffill',inplace=True)","edbbfdd2":"Null_Df = pd.DataFrame({'Null_Count':train.isnull().sum(),\n                        'Null_Perc %':round(train.isnull().sum()*100\/train.shape[0],2)})\nNull_Df[Null_Df.Null_Count!=0]","4658c1ae":"train.head(2)","199bd401":"# Below code is to generate the separate lists of numerical and categorical columns.  \n# This is required as in this dataset, categorical and numerical columns have same type of values i.e. int or float instead of str value for...\n# ... categorical values which is usual for other datasets. Hence in those datasets, it is easy to filter the numerical and categorical columns...\n# ... using dtype. Hence for this dataset, I renamed the column names manually in excel by checking the column description given on the project's website. \n\nnumerical = []\ncategorical = []\nj = 0\nfor i in train.columns:\n    if train.columns.str.contains('_num',regex=True)[j]:\n        numerical.append(i)\n    else:\n        categorical.append(i)\n    j+=1\nprint('Numerical Columns : %s\\n'%len(numerical))\nprint('Categorical Columns : %s\\n'%len(categorical))\nprint('Total Columns :%s'%train.shape[1])","3279f84e":"temp_numerical = train.loc[:,numerical]\ntemp_categorical = train.loc[:,categorical]","d40d467c":"obj_num = []\nfor i in temp_numerical.columns:\n    if str(temp_numerical[i].dtype) =='object':\n        obj_num.append(i)\nprint('Numerical columns with str values : ',obj_num)        \nobj_cat = []\nfor i in temp_categorical.columns:\n    if str(temp_categorical[i].dtype) =='object':\n        obj_cat.append(i)\nprint('Categorical columns with str values : ',obj_cat) ","8a3aa794":"temp_categorical.drop(['Id', 'idhogar'],axis=1,inplace=True)","32dfe29d":"# Run below to know what are the str values >>> \n#test_temp_numerical.dependency_num.value_counts().index\n#temp_numerical.edjefe_num.value_counts().index\n#temp_numerical.edjefa_num.value_counts().index","89022b96":"plt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nsns.distplot(temp_numerical[(temp_numerical.edjefe_num != 'yes')& (temp_numerical.edjefe_num != 'no')].edjefe_num.astype('int'))\n\nplt.subplot(1,3,2)\nsns.distplot(temp_numerical[(temp_numerical.edjefa_num != 'yes')& (temp_numerical.edjefa_num != 'no')].edjefa_num.astype('int'))\n\nplt.subplot(1,3,3)\nsns.distplot(temp_numerical[(temp_numerical.dependency_num != 'yes')& (temp_numerical.dependency_num != 'no')].dependency_num.astype('float'))\n\nplt.show()","29c23649":"t1 = time.time()\nmean_edjefe = temp_numerical[(temp_numerical.edjefe_num != 'yes')& (temp_numerical.edjefe_num != 'no')].edjefe_num.astype('int').mean()\nmean_edjefa = temp_numerical[(temp_numerical.edjefa_num != 'yes')& (temp_numerical.edjefa_num != 'no')].edjefa_num.astype('int').mean()\nmean_dependency = temp_numerical[(temp_numerical.dependency_num != 'yes')& (temp_numerical.dependency_num != 'no')].dependency_num.astype('float').mean()\nedjefe = []\nedjefa = []\ndependency = []\nfor i in range(temp_numerical.shape[0]):\n    if temp_numerical.edjefe_num[i] in ['yes','no']:\n        edjefe.append(mean_edjefe)\n    else:\n        edjefe.append(temp_numerical.edjefe_num[i])\n    \n    if temp_numerical.edjefa_num[i] in ['yes','no']:\n        edjefa.append(mean_edjefa)\n    else:\n        edjefa.append(temp_numerical.edjefa_num[i])\n        \n    if temp_numerical.dependency_num[i] in ['yes','no']:\n        dependency.append(mean_dependency)\n    else:\n        dependency.append(temp_numerical.dependency_num[i])\n    \n    if i%100==0:        # Small patch just to keep track of progress of loop. \n        print('Rows Processed --> %s'%i,end='\\r')\n\ntemp_numerical.edjefe_num = edjefe\ntemp_numerical.edjefa_num = edjefa\ntemp_numerical.dependency_num = dependency\n\n# Changing the type of the columns >>> \n\ntemp_numerical.edjefe_num = temp_numerical.edjefe_num.astype('int64')\ntemp_numerical.edjefa_num = temp_numerical.edjefa_num.astype('int64')\ntemp_numerical.dependency_num = temp_numerical.dependency_num.astype('float64')\n\nt2 = time.time()\nprint('Time Elapsed : %s'%(t2-t1))","d55d1abd":"plt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nsns.distplot(temp_numerical.edjefe_num)\nplt.subplot(1,3,2)\nsns.distplot(temp_numerical.edjefa_num)\nplt.subplot(1,3,3)\nsns.distplot(temp_numerical.dependency_num)\nplt.show()","3df134c7":"# Making the copies of the above datasets >>> \n# WHY making copies ? Because we have processed the two types of data separately. Hence now if we make any changes to ...\n# ...the copies and want fresh data again then we can just run this cell >>> \n\nx_train_numerical = temp_numerical.copy()\nx_train_categorical = temp_categorical.copy()\n\n# This data would have the dependent variable as well hence we need to drop that >>> \nx_train_categorical.drop('Target',axis=1,inplace=True)\n\ny_train_full = train.loc[:,'Target']","82e61d40":"# Indicator var >>> Run This Cell >>>\nstd_scal = 0\nMinMax =0\n\n# Hence after running any of the scaling method's cell, run \"# Scaling method Used >>> \" cell as well to mention which...\n# ... method being used for scaling. ","0e571776":"# StandardScaler >>> \n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nscaled = sc.fit_transform(x_train_numerical)\n\n# Converting array type back to dataframe >>> \nx_train_scaled = pd.DataFrame(scaled)\nx_train_scaled.columns = x_train_numerical.columns\nstd_scal+=1","01852db6":"# MinMaxScaler >>> \n\nfrom sklearn.preprocessing import MinMaxScaler\n\nsc = MinMaxScaler()\nscaled = sc.fit_transform(x_train_numerical)\n\n# Converting array type back to dataframe >>> \nx_train_scaled = pd.DataFrame(scaled)\nx_train_scaled.columns = x_train_numerical.columns\nMinMax+=1","a4c75fdf":"# Scaling method Used >>> \nif MinMax >= std_scal:\n    print('Scaling Method Utilized : \"MinMaxScaler\"')\nelse:\n    print('Scaling Method Utilized : \"StandardScaler\"')","8e227f48":"print('Shape of Numerical : %s'%x_train_numerical.shape[1])\nprint('Shape of Categorical : %s'%x_train_categorical.shape[1])\nx_train_full = x_train_scaled.merge(x_train_categorical,left_index=True,right_index=True)\nprint('Shape of Combined : %s'%x_train_full.shape[1])","670695f4":"t1 = time.time()\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Creating the copies of data to be inserted >>> \nx_train = x_train_full.copy()\ny_train = y_train_full.copy()\n\nx_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.2,random_state=123)\n\n# Find optimal neigbors >>> \nA = []\nB = []\nfor i in range(11,500,40):\n    classifier = KNeighborsClassifier(n_neighbors=i,metric='euclidean').fit(x_train,y_train)\n    y_pred = classifier.predict(x_test)\n    A.append(i)\n    B.append(round(accuracy_score(y_test,y_pred)*100,3))\n    \ntable = pd.DataFrame({'Neighbors':A,'Accuracy':B})\nplt.figure(figsize=(15,5))\nplt.plot(table.Neighbors,table.Accuracy,marker='^',mec='black',mfc='r',ms=10,color='b')\nplt.xticks(np.arange(11,500,40))\nplt.grid()\nplt.show()\n\nt2 = time.time()\nprint('Time Elapsed : %s'%(t2-t1))","5bf0bd5d":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Creating the copies of data to be inserted >>> \nx_train = x_train_full.copy()\ny_train = y_train_full.copy()\n\nx_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.2,random_state=123)\n\n# Run KNN >>> \nclassifier = KNeighborsClassifier(n_neighbors=10,metric='euclidean').fit(x_train,y_train)\ny_pred = classifier.predict(x_test)\n\naccuracy_knn = round(accuracy_score(y_test,y_pred)*100,3)\nprint('Accuracy via KNN : ',accuracy_knn)","2c65e9de":"t1 = time.time()\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Creating the copies of data to be inserted >>> \nx_train = x_train_full.copy()\ny_train = y_train_full.copy()\n\nx_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.2,random_state=123)\n\n# Find optimal number of trees  >>> \nA = []\nB = []\nfor i in range(11,500,40):\n    classifier = RandomForestClassifier(n_estimators=i,criterion='entropy',random_state=123).fit(x_train,y_train)\n    y_pred = classifier.predict(x_test)\n    A.append(i)\n    B.append(round(accuracy_score(y_test,y_pred)*100,3))\n    \n    print('Processing Estimators : %s'%i,end='\\r')\n    \ntable = pd.DataFrame({'Neighbors':A,'Accuracy':B})\nplt.figure(figsize=(15,5))\nplt.plot(table.Neighbors,table.Accuracy,marker='^',mec='black',mfc='b',ms=10,color='r',linestyle='--')\nplt.xticks(np.arange(11,500,40))\nplt.grid()\nplt.show()\n\nt2 = time.time()\nprint('Time Elapsed : %s'%(t2-t1))","6751542e":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Creating the copies of data to be inserted >>> \nx_train = x_train_full.copy()\ny_train = y_train_full.copy()\n\nx_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.2,random_state=123)\n\n# Run RandomForest >>> \nclassifier = classifier = RandomForestClassifier(n_estimators=211,criterion='entropy',random_state=123).fit(x_train,y_train)\ny_pred = classifier.predict(x_test)\n\naccuracy = round(accuracy_score(y_test,y_pred)*100,3)\nprint('Accuracy via RandomForest : ',accuracy)","af978b42":"# Importing essential libraries >>> \n\nfrom sklearn.model_selection import cross_val_score\n\n# run K-Fold >>>\naccuracies = cross_val_score(estimator = classifier,X = x_train,y=y_train,cv=10)\n\n# Results >>>\n# -------------\nprint('Actual Accuracy : %s'%round((accuracies.mean()*100),2)+' %')\nprint('Standard Deviation of Accuracy : %s'%round((accuracies.std()*100),2)+' %')\nprint('RandomFt Accuracy :%s'%accuracy+' %')","a7d5c81e":"# Accessing the test data >>> \ntest = pd.read_csv(\"..\/input\/costa-rican-household-poverty-prediction\/test.csv\")\nprint('Shape : ',test.shape)\ntest.head(2)","2355b539":"test.drop(['Id','idhogar'],axis=1,inplace=True)\nprint('Shape : ',test.shape)\ntest.head(2)","edd256dc":"# Instead of manually renaming the columns, for test data I will try to add suffix \"_num\" to the numerical columns based on their values >>> \nnumerical = []\ncategorical = []\ncolumns = test.columns\nfor i in columns:\n    if len(test[i].unique()) >2:\n        test.rename(columns={i:i+'_num'},inplace=True)\n        numerical.append(i+'_num')\n    else:\n        categorical.append(i)\n        \nprint('Numerical Columns : %s\\n'%len(numerical))\nprint('Categorical Columns : %s\\n'%len(categorical))\nprint('Total Columns :%s'%test.shape[1])","bd1e23f8":"Null_Df = pd.DataFrame({'Null_Count':test.isnull().sum(),\n                        'Null_Perc %':round(test.isnull().sum()*100\/test.shape[0],2)})\nNull_Df[Null_Df.Null_Count!=0]","2fe5b210":"test.v2a1_num.fillna(test.v2a1_num.mean(),inplace=True)\ntest.v18q1_num.fillna(test.v18q1_num.median(),inplace=True)\ntest.rez_esc_num.fillna(test.rez_esc_num.mean(),inplace=True)\ntest.meaneduc_num.fillna(method='ffill',inplace=True)\ntest.SQBmeaned_num.fillna(method='ffill',inplace=True)","98e5c06a":"Null_Df = pd.DataFrame({'Null_Count':test.isnull().sum(),\n                        'Null_Perc %':round(test.isnull().sum()*100\/test.shape[0],2)})\nNull_Df[Null_Df.Null_Count!=0]","f6381c03":"test_temp_numerical = test.loc[:,numerical]\ntest_temp_categorical = test.loc[:,categorical]","4bcabbb6":"columns = temp_numerical.columns\nnum = []\nfor i in test_temp_numerical.columns:\n    if i not in columns:\n        num.append(i)\nprint('Column not matching : ',num)\n        \ncat=[]\ncolumns = temp_categorical.columns\nfor i in test_temp_categorical.columns:\n    if i not in columns:\n        cat.append(i)\nprint('Column not matching : ',cat)","c4d6ab1c":"obj_num = []\nfor i in test_temp_numerical.columns:\n    if str(test_temp_numerical[i].dtype) =='object':\n        obj_num.append(i)\nprint('Numerical columns with str values : ',obj_num)        \nobj_cat = []\nfor i in test_temp_categorical.columns:\n    if str(test_temp_categorical[i].dtype) =='object':\n        obj_cat.append(i)\nprint('Categorical columns with str values : ',obj_cat) ","10f179ac":"# Run below to know what are the str values >>> \n#test_temp_numerical.dependency_num.value_counts().index\n#test_temp_numerical.edjefe_num.value_counts().index\n#test_temp_numerical.edjefa_num.value_counts().index","fffbb2eb":"plt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nsns.distplot(test_temp_numerical[(test_temp_numerical.edjefe_num != 'yes')& (test_temp_numerical.edjefe_num != 'no')].edjefe_num.astype('int'))\n\nplt.subplot(1,3,2)\nsns.distplot(test_temp_numerical[(test_temp_numerical.edjefa_num != 'yes')& (test_temp_numerical.edjefa_num != 'no')].edjefa_num.astype('int'))\n\nplt.subplot(1,3,3)\nsns.distplot(test_temp_numerical[(test_temp_numerical.dependency_num != 'yes')& (test_temp_numerical.dependency_num != 'no')].dependency_num.astype('float'))\n\nplt.show()","fe892cb5":"t1 = time.time()\nmean_edjefe = test_temp_numerical[(test_temp_numerical.edjefe_num != 'yes')& (test_temp_numerical.edjefe_num != 'no')].edjefe_num.astype('int').mean()\nmean_edjefa = test_temp_numerical[(test_temp_numerical.edjefa_num != 'yes')& (test_temp_numerical.edjefa_num != 'no')].edjefa_num.astype('int').mean()\nmean_dependency = test_temp_numerical[(test_temp_numerical.dependency_num != 'yes')& (test_temp_numerical.dependency_num != 'no')].dependency_num.astype('float').mean()\nedjefe = []\nedjefa = []\ndependency = []\nfor i in range(test_temp_numerical.shape[0]):\n    if test_temp_numerical.edjefe_num[i] in ['yes','no']:\n        edjefe.append(mean_edjefe)\n    else:\n        edjefe.append(test_temp_numerical.edjefe_num[i])\n    \n    if test_temp_numerical.edjefa_num[i] in ['yes','no']:\n        edjefa.append(mean_edjefa)\n    else:\n        edjefa.append(test_temp_numerical.edjefa_num[i])\n        \n    if test_temp_numerical.dependency_num[i] in ['yes','no']:\n        dependency.append(mean_dependency)\n    else:\n        dependency.append(test_temp_numerical.dependency_num[i])\n    \n    if i%100==0:        # Small patch just to keep track of progress of loop. \n        print('Rows Processed --> %s'%i,end='\\r')\n\ntest_temp_numerical.edjefe_num = edjefe\ntest_temp_numerical.edjefa_num = edjefa\ntest_temp_numerical.dependency_num = dependency\n\n# Changing the type of the columns >>> \n\ntest_temp_numerical.edjefe_num = test_temp_numerical.edjefe_num.astype('int64')\ntest_temp_numerical.edjefa_num = test_temp_numerical.edjefa_num.astype('int64')\ntest_temp_numerical.dependency_num = test_temp_numerical.dependency_num.astype('float64')\n\nt2 = time.time()\nprint('Time Elapsed : %s'%(t2-t1))","2e627e21":"# Making the copies of the above datasets >>> \n# WHY making copies ? Because we have processed the two types of data separately. Hence now if we make any changes to ...\n# ...the copies and want fresh data again then we can just run this cell >>> \n\nx_test_numerical = test_temp_numerical.copy()\nx_test_categorical = test_temp_categorical.copy()","93ef60f5":"# MinMaxScaler >>> \n\nfrom sklearn.preprocessing import MinMaxScaler\n\nsc = MinMaxScaler()\nscaled = sc.fit_transform(x_test_numerical)\n\n# Converting array type back to dataframe >>> \nx_test_scaled = pd.DataFrame(scaled)\nx_test_scaled.columns = x_test_numerical.columns","0c0744e3":"print('Shape of Numerical : %s'%x_test_numerical.shape[1])\nprint('Shape of Categorical : %s'%x_test_categorical.shape[1])\nx_test_full = x_test_scaled.merge(x_test_categorical,left_index=True,right_index=True)\nprint('Shape of Combined : %s'%x_test_full.shape[1])","c10f3915":"y_test_pred = classifier.predict(x_test_full)","0008d633":"data_test = pd.read_csv(\"..\/input\/costa-rican-household-poverty-prediction\/test.csv\") \n# Accessing raw test again to get the Id column which is dropped in the original data. ","2339bb5b":"submission_file = pd.DataFrame({'Id':data_test.Id,'Target':y_test_pred})\nsubmission_file.Target.value_counts()","b7ef88f6":"submission_file.head()","e3522efd":"submission_file.to_csv('submission.csv',index=False)","cc00827e":"**<font color=red>Handling missing values :**`v2a1_num --> Monthly rent payment`","9d000e35":"**Tagging of \"Numerical\" columns**","a139b31d":"**Validating if all the train data columns match test data columns**","f6c85f01":"**`Cleaning Categorical`** : Well we really would not include the two columns found with str in categorical category as those columns can not contribute in the training of model because they are just IDs\/Unique Values hence can not predict any pattern. ","264f0e88":"## <font color = limegreen> Algorithm : KNN","4e0b795a":"**<font color=red>Handling missing values :**`v18q1_num  --> number of tablets household owns`","dd18dd21":"<font color=red>***So No mismatched columns!!!***","69a3a76d":"**<font color=royalblue>With above run, we can conclude that the highest accuracy we can achieve with neighbors = 11**","c89ea596":"**Scaling the Numerical Data**","28bf9a48":"# <font color = darkorange> Part - 1 : Data Analysis and Preprocessing ","b1cbe3fc":"### <font color = blue>Brief Description : \n**We need to classify the persons as 1,2,3 or 4 class based on the Costa Rican household characteristics in this dataset.**","6e82191d":"## <font color=fuchsia>Algorithm : RandomForest\nLets run RandomForest Algorithm and see how it performs","c170baec":"## <font color=blueviolet>Model Validation : K-Fold","35c945c7":"<font color = dodgerblue>**Handing Missing Values**","e7f8973f":"<font color=red>***So NO Null Values !!!***","08070040":"**<font color=red>Handling Missing values :**  `\"meaneduc_num\" and \"SQBmeaned_num\"`\n\nAs the number of values missing are very less hence we will use method of 'ffill' which means the data from just previous cell will be copied to the next missing cells.","885c3888":"# <font color=royalblue> Plan Of Attack:\n    \n1. Divide the train data into Numerical, Categorical and Dependent var\n2. Categorical : We don't need to encode categorical columns as the columns are already encoded as 0 or 1\n3. Numerical : We have to scale them. But we will scale them using both methods \"StandardScaler\" and \"MinMaxScaler\" so that we can use any of the scaled data as per requirement to increase accuracy. \n4. After scaling, we will combine both Categorical and Numerical\n5. Next we will split this combined data and dependent data into train and test datasets as we need to calculate the accuracy of our prediction model. \n6. Then we will try to apply differnt methodology like KNN, RandomForest and lastly ANN (if required)\n7. We will also try to use the Validation method K-Fold and dimension reduction method PCA (if required)\n8. Finally after the model is prepared, we will apply on test data provided with this project and will generate the submission file. ","bf233db8":"### <font color =darkblue> Scaling the Numerical data\n    \nWe will scale using both methods in separate cells hence whichever type is required, just run that cell !!!","bd4e09b9":"**Finding Missing Data (if any)**","0091ca2c":"### <font color = royalblue> Remarks for K-Fold :\n<font color = red>**From above, it is clear that the model accuracy is stable as the mean accuracy doesn't have high difference from the accuracy calculated by RandomForest model directly.**\n<font color = red\\>  \n    \n<font color =black>Now we will apply this model on the test data\n---\n    ","efa4ef85":"**<font color=red>Handling missing values :** `rez_esc_num  --> Years behind in school`\n","fba3dba5":"**<font color=royalblue>Lets create the model using the estimators = 211**","72992239":"**`Cleaning Numerical`** : For this, lets plot the histogram and see how the numerical values are spread so that we can decide how to replace the str values. ","132d8c64":"**<font color = steelblue>Just like the train data, we will handle str values for test data**","9c896963":"<font color=red>***So No Null values***","80000255":"### <font color = tomato>Combining Data : Numerical - Scaled + Categorical","bd2714d1":"**`Important :`** As we can see that all the data is centred at one point however we can not take average in this case because this column represents \"Number of tablets per household\" and if we replace by average it will be in decimals but a physical object can not be in decimals. \n\nHence we will replace the missing values by **`median`**","6a383ea1":"# <font color = darkblue> Part - 2 : Creating Prediction Model ","1ab163eb":"**Clearing Data : Handling Mixed Values**","4300b549":"After replacing the str values with numerical, the data is quite skewed to one point for all three columns however there was no other option but to replace the values or otherwise just remove the whole columns themselves. \n\nWe will try to remove the columns if we do not get good results. (good accuracy in the model). Lets move further for the scaling exercise >>> ","09b9506b":"## Fitting Model & Making Prediction ","6ccfb9ec":"**Combining Numerical + Categorical**","845e5818":"**<font color = steelblue>Data above is spread almost in a similar way. So we will replace the values with method : Mean of values :-)**","70faf880":"**Hanlding the missing values same as train data**","a6263b7b":"### <font color =darkmagenta>Split the data - Numerical, Categorical and Dependent","c20e4ecf":"### <font color=green> Cleaning the data\nThe data contains mixed values means along with the numerical values, the data contains string values within the same columns. Hence we have to clean this data","c719dd31":"**Making Copies of the datasets**"}}