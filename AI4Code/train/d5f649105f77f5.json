{"cell_type":{"c7701408":"code","fd1c3843":"code","b867a412":"code","efb54dfa":"code","96a94227":"code","537272b0":"code","dc2cf029":"code","361d393e":"code","15fc9c1d":"code","039f9709":"code","4e8deba3":"code","c6ed4869":"code","79709b6d":"code","615b5853":"code","60b4cb59":"code","6450906c":"code","e8365a1e":"code","59dbc6ed":"code","f03de868":"code","ef6b1d00":"code","2cefa5a2":"code","0f06935d":"code","2321e494":"code","c88fe2c9":"code","10c6bdf7":"code","f7dac61b":"code","7c26c2d4":"code","1972ec3e":"code","181fbc0c":"code","c461b98c":"code","1c4a4b8a":"code","3575f068":"code","8563d2d1":"code","7fe6eb3b":"code","84d30038":"code","159c2656":"code","bf977a2f":"code","75bd2827":"code","6b79a61c":"code","7b599ed4":"code","00b094f8":"code","f5af60f3":"code","b5649c34":"code","dc8af910":"code","5462e7e6":"code","269ca50e":"code","495dfc2e":"code","330f0083":"code","c49b5e5b":"code","07a5c53b":"code","43c114f9":"code","981db668":"code","21376e2d":"code","84432924":"code","2232c920":"code","be2cc145":"code","8d8743aa":"code","3d178bbc":"code","21e997ed":"code","2cbe70d0":"code","4be24df7":"code","5996d5d1":"code","ac06127b":"code","e8301e4e":"code","029029fa":"code","30ed906a":"code","b68b06f7":"code","11184468":"code","fe3f797a":"code","afe758b5":"code","07188e69":"code","bec42a9f":"code","a74ac434":"code","230bef56":"code","cf16e72f":"code","9a4e8e27":"code","3dc39bb0":"code","4dc90047":"code","1533c5b8":"markdown","ca3881f0":"markdown","757f2545":"markdown","890e1240":"markdown","05c48880":"markdown","d75f9851":"markdown","aba43a76":"markdown","3c6c53f9":"markdown","dd4e7a03":"markdown","becc1755":"markdown","b48d766b":"markdown","04987f9d":"markdown","9ffe0975":"markdown","b309de6c":"markdown","f6417d9a":"markdown","35c509e6":"markdown","ad41efcd":"markdown","ed7748f4":"markdown","112cf923":"markdown","8a9c06b3":"markdown","fed087b6":"markdown","d79b6899":"markdown","ff235566":"markdown","f2ac73f4":"markdown","0bca9565":"markdown","dbf9e1fb":"markdown","999b6619":"markdown","9699d39c":"markdown","0264ee40":"markdown","287f22dd":"markdown","af0a4b7e":"markdown","c692a2fd":"markdown","e7805332":"markdown","9351af3b":"markdown","4a2b18ba":"markdown","ebdd66ea":"markdown","6a486b6f":"markdown","8b883818":"markdown","c9004364":"markdown","315ab9e9":"markdown","03c4e872":"markdown","e0383755":"markdown","14074b54":"markdown","2eeb19ca":"markdown","0c70667d":"markdown","725bf26a":"markdown","dceee14f":"markdown","280a2859":"markdown","6251d1ed":"markdown","5611ebde":"markdown","05eeee3e":"markdown","9c77937e":"markdown","ef9bf74a":"markdown","3ae4ae6a":"markdown","b26e3f4b":"markdown","91323b72":"markdown","8a0fc88f":"markdown","bc7ab32b":"markdown"},"source":{"c7701408":"import time\nimport warnings\nimport os\nimport re\nimport gc\nimport random\nimport glob\nimport optuna\nimport sklearn.exceptions\nimport plotly","fd1c3843":"import pandas               as pd\nimport numpy                as np\nimport matplotlib.pyplot    as plt \nimport seaborn              as sns\nimport joblib               as jb","b867a412":"#!conda install catboost","efb54dfa":"import torch.nn             as nn","96a94227":"import lightgbm             as lgb\nimport catboost             as ctb\nimport xgboost              as xgb","537272b0":"from sklearn.model_selection       import train_test_split,StratifiedKFold, RepeatedStratifiedKFold\nfrom sklearn.preprocessing         import QuantileTransformer\nfrom sklearn                       import metrics\nfrom sklearn.ensemble              import RandomForestClassifier","dc2cf029":"from optuna.samplers               import TPESampler\nfrom optuna.visualization          import plot_edf\nfrom optuna.visualization          import plot_optimization_history\nfrom optuna.visualization          import plot_parallel_coordinate\nfrom optuna.visualization          import plot_param_importances\nfrom optuna.visualization          import plot_slice\nfrom optuna.visualization          import plot_intermediate_values\nfrom optuna.visualization          import plot_contour","361d393e":"!mkdir Data\n!mkdir Data\/pkl\n!mkdir Data\/sumbmission\n!mkdir model\n!mkdir model\/preds\n!mkdir model\/optuna","15fc9c1d":"def jupyter_setting():\n    \n    %matplotlib inline\n     \n    pd.options.display.max_columns = None\n    \n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n      \n    warnings.filterwarnings(action='ignore')\n    warnings.simplefilter('ignore')\n    warnings.filterwarnings('ignore')\n    warnings.filterwarnings('ignore', category=DeprecationWarning)\n    warnings.filterwarnings('ignore', category=FutureWarning)\n    warnings.filterwarnings('ignore', category=RuntimeWarning)\n    warnings.filterwarnings('ignore', category=UserWarning)\n    warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n    #pd.set_option('display.max_rows', 150)\n    pd.set_option('display.max_columns', 500)\n    pd.set_option('display.max_colwidth', None)\n\n    icecream = [\"#00008b\", \"#960018\",\"#008b00\", \"#00468b\", \"#8b4500\", \"#582c00\"]\n    #sns.palplot(sns.color_palette(icecream))\n    \n    return icecream\n\nicecream = jupyter_setting()","039f9709":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","4e8deba3":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.rcParams['font.size'] = 12\n    plt.title('Precision Recall vs threshold')\n    plt.xlabel('Threshold')\n    plt.legend(loc=\"lower left\")\n    \n    plt.grid(True)\n","c6ed4869":"def plot_precision_vs_recall(precisions, recalls):\n    plt.plot(recalls[:-1], precisions[:-1], \"b-\", label=\"Precision\")\n    \n    plt.rcParams['font.size'] = 12\n    plt.title('Precision vs recall')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    # plt.legend(loc=\"lower left\")\n    \n    plt.grid(True)","79709b6d":"\ndef plot_roc_curve(fpr, tpr, label=None):\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, \"r-\", label=label)\n    ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('XGBR ROC curve for TPS 09')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)","615b5853":"def get_df_cv(path):\n\n    name_file_pkl     = glob.glob(path + '*.pkl.z')\n    dic_preds_mdl_pkl = dict()\n\n    for p_name in name_file_pkl:    \n        y_model_pkl_name_col  = p_name.replace(path, '').replace('.pkl.z','') \n        y_model_pkl           = jb.load(p_name)   \n\n        dic_preds_mdl_pkl[y_model_pkl_name_col] = y_model_pkl\n\n    return pd.DataFrame(dic_preds_mdl_pkl)","60b4cb59":"def graf_corr(df):\n    \n    df = df.corr().round(5)\n\n    # M\u00e1scara para ocultar a parte superior direita do gr\u00e1fico, pois \u00e9 uma duplicata\n    mask = np.zeros_like(df)\n    mask[np.triu_indices_from(mask)] = True\n\n    # Making a plot\n    plt.figure(figsize=(16,16))\n    ax = sns.heatmap(df, annot=True, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\n\n    ax.set_title(\"Mapa de calor de correla\u00e7\u00e3o das vari\u00e1vel\", fontsize=17)\n\n    plt.setp(ax.get_xticklabels(), \n             rotation      = 90, \n             ha            = \"right\",\n             rotation_mode = \"anchor\", \n             weight        = \"normal\")\n\n    plt.setp(ax.get_yticklabels(), \n             weight        = \"normal\",\n             rotation_mode = \"anchor\", \n             rotation      = 0, \n             ha            = \"right\");\n","6450906c":"def correlation(dataset, threshold):\n\n    col_corr    = set()  # Conjunto de todos os nomes de colunas correlacionadas\n    corr_matrix = dataset.corr()\n    \n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # estamos interessados no valor coeficiente absoluto\n                colname = corr_matrix.columns[i]        # obtendo o nome da coluna\n                col_corr.add(colname)\n    \n    return col_corr","e8365a1e":"path = '\/content\/drive\/MyDrive\/kaggle\/07. Tabular Playground Series - Ago 2021\/'\npath = '..\/input\/tpsset21-001\/'","59dbc6ed":"df3_train     = jb.load(path + 'df3_train.pkl.z')\ndf3_test      = jb.load(path + 'df3_test.pkl.z')\ndf_submission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\n\ndf3_train.shape, df3_test.shape","f03de868":"df3_train.head()","ef6b1d00":"df3_train.info() , df3_test.info()  ","2cefa5a2":"df3_train = reduce_memory_usage(df3_train)\ndf3_test  = reduce_memory_usage(df3_test)\n\ndf3_train.shape, df3_test.shape","0f06935d":"df3_train.info(), df3_test.info()","2321e494":"X      = df3_train.drop(['claim'], axis=1)    \ny      = df3_train[\"claim\"].copy()\nX_test = df3_test\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, \n                                                      y, \n                                                      test_size    = 0.2, \n                                                      shuffle      = True, \n                                                      stratify     = y,\n                                                      random_state = 12359)\n\n\n\nprint(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X.shape, y.shape, X_test.shape) ","c88fe2c9":"scaler       = QuantileTransformer(output_distribution='normal' , random_state=0)\nX_test_sc_qt = pd.DataFrame(scaler.fit_transform(X_test), columns = X_test.columns)\n\nX_test_sc_qt.shape","10c6bdf7":"path = ''\n\nclass TunningModels(nn.Module):\n\n    def __init__(self, name_model, X_trn_, y_trn_, X_ts_, feature_=None, seed_=12359):\n        \n        super(TunningModels,self).__init__() \n        \n        self.name_clf  = name_model\n        self.X_trn     = X_trn_\n        self.y_trn     = y_trn_\n        self.X_ts      = X_ts_         \n        self.feature   = feature_\n        self.seed      = seed_\n        \n    def recover_prediction_first_level():\n        \n        preds_train1 = glob.glob(\"model\/train\/*.pkl.z\")\n        preds_test   = glob.glob(\"model\/test\/*.pkl.z\")\n        preds_val1   = glob.glob(\"model\/valid\/*.pkl.z\")\n\n        df_train1     = []\n        scores_traint = dict()\n\n        for p_name in preds_train1:    \n            p    = jb.load(p_name)\n            p_df = pd.DataFrame(p, columns=[p_name.replace('model\/train\\\\', '')])    \n            df_train1.append(p_df)    \n            scores_traint[p_name] = f1_score(y_train1, (p_df>.5))\n\n        df_val1     = [] \n        scores_val1 = dict()\n        for p_name in preds_val1:    \n            p    = jb.load(p_name)\n            p_df = pd.DataFrame(p, columns=[p_name.replace('model\/valid\\\\', '')])    \n            df_val1.append(p_df)    \n            scores_val1[p_name] = f1_score(y_val1, (p_df>.5))\n\n        df_test     = [] \n        scores_test = dict()\n        for p_name in preds_test:    \n            p         = jb.load(p_name)\n            p_df_test = pd.DataFrame(p, columns=[p_name.replace('model\/test\\\\', '')])    \n            df_test.append(p_df_test)\n\n        df_train1 = pd.concat(df_train1, axis=1)\n        df_val1   = pd.concat(df_val1, axis=1)\n        df_test   = pd.concat(df_test, axis=1)\n\n        return df_train1, df_val1, df_test.shape\n        \n    def delete_files(namefile):\n\n        path = ['model\/train', 'model\/test', 'model\/valid', 'model\/params', 'model\/score',\n                'model\/test_f', 'model\/cv_model', 'model\/preds', 'model\/optuna']\n\n        for path_ in path:\n            for raiz, diretorios, arquivos in os.walk(path_):\n                for arquivo in arquivos:\n                    if arquivo.startswith(namefile):\n                        os.remove(os.path.join(raiz, arquivo))\n \n    def logging_callback(study, frozen_trail):\n        prev_best = study.user_attrs.get('prev_best', None)\n        if prev_best != study.best_value:\n            study.set_user_attr('prev_best', study.best_value)\n            print(f\"Trail {frozen_trail.number} finished with best value {frozen_trail.value}\")\n\n    def cross_valid(model, model_name_, X_, y_, X_test_, type_model=1, \n                    feature=None, seed=None, tunning=1):\n    \n        n_splits    = 5\n        n_repeats   = 1\n        y_hat_test  = None\n        auc         = []\n        lloss       = []\n        f1          = []\n\n        if feature!=None:         \n            X_      = X_[feature]\n            X_test_ = X_test_[feature]\n        \n        kf = RepeatedStratifiedKFold(n_splits     = n_splits, \n                                     n_repeats    = n_repeats,                                   \n                                     random_state = seed)\n        \n        # https:\/\/stackoverflow.com\/questions\/65318931\/stratifiedkfold-vs-kfold-in-scikit-learn\n        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n            \n\n        clf_name = model.__class__.__name__\n\n        print('='*70)\n        print('Training model: {}'.format(clf_name))\n        print('='*70)\n        \n        scaler = QuantileTransformer(output_distribution='normal', random_state=seed)\n        \n        cols_model_base = ['fe_f50_zero', 'fe_f5_zero', 'fe_n_missing', 'cluster', \n                           'fe_std', 'f105', 'f102', 'f22', 'f79', 'f106', 'f71', \n                           'f77', 'f69']\n\n        for fold, (train_idx, valid_idx) in enumerate(kf.split(X_, y_, groups=y_)): \n\n            # Obtenha os dados de treino e \n            # valida\u00e7\u00e3o atrav\u00e9s de indice\n            X_trn, y_trn = X_.iloc[train_idx], y_.iloc[train_idx]\n            X_val, y_val = X_.iloc[valid_idx], y_.iloc[valid_idx]\n\n            # Scaler \n            X_trn  = pd.DataFrame(scaler.fit_transform(X_trn), columns=X_trn.columns)\n            X_val  = pd.DataFrame(scaler.fit_transform(X_val), columns=X_val.columns)\n\n            # Treinar o modelo baseline\n                        \n            #params_md_baseline = {'random_state': 0,          \n            #                      'predictor'   : 'gpu_predictor',\n            #                      'tree_method' : 'gpu_hist',\n            #                      'eval_metric' : 'auc'}\n            #\n            #model_base = xgb.XGBClassifier(**params_md_baseline)\n            #model_base.fit(X_trn[cols_model_base], y_trn)\n#\n            #threshold = .5\n#\n            #y_hat_prob_tr_mb = (model_base.predict_proba(X_trn[cols_model_base])[:, 1]>threshold).astype(int) \n            #y_hat_prob_vl_mb = (model_base.predict_proba(X_val[cols_model_base])[:, 1]>threshold).astype(int) \n            #y_hat_prob_ts_mb = (model_base.predict_proba(X_test_[cols_model_base])[:, 1]>threshold).astype(int) \n#\n            #X_trn['fe_md_baseline']   = y_hat_prob_tr_mb \n            #X_val['fe_md_baseline']   = y_hat_prob_vl_mb\n            #X_test_['fe_md_baseline'] = y_hat_prob_ts_mb\n\n            # Crie e ajuste um novo modelo \n            # usando os melhores par\u00e2metros.        \n            if type_model==1:\n                model.fit(X_trn, y_trn,\n                          eval_set              = [(X_val, y_val)],                          \n                          early_stopping_rounds = 100 ,\n                          verbose               = False)\n                \n            if type_model==2:\n                model.fit(X_trn, y_trn)\n\n            # Predi\u00e7\u00e3o na valida\u00e7\u00e3o\n            y_hat_val_prob = model.predict_proba(X_val)[:, 1]\n            y_hat_val      = (y_hat_val_prob >.5).astype(int) \n\n            # M\u00e9tricas \n            auc_      = metrics.roc_auc_score(y_val, y_hat_val_prob)                    \n            f1_score_ = metrics.f1_score(y_val, y_hat_val)        \n            log_loss_ = metrics.log_loss(y_val, y_hat_val_prob)\n            \n            # Salvar as m\u00e9tricas\n            auc.append(auc_)\n            f1.append(f1_score_)\n            lloss.append(log_loss_)\n\n            # Use o modelo treinado para \n            # 1 \/ n_splits das previs\u00f5es de sa\u00edda.\n            if y_hat_test is None:        \n                y_hat_test  = model.predict_proba(X_test_)[:, 1]        \n            else:        \n                y_hat_test  += model.predict_proba(X_test_)[:, 1]\n\n            msg = 'Fold {}\/{} AUC: {:2.5f} - F1: {:2.5f} - L. loss: {:2.5f}'\n            print(msg.format(fold+1, n_splits*n_repeats, auc_, f1_score_, log_loss_))\n\n            gc.collect()\n\n        y_hat_test \/= (n_splits*n_repeats)\n        auc_mean    = np.mean(auc)\n        auc_std     = np.std(auc)\n        lloss_mean  = np.mean(lloss)\n        f1_mean     = np.mean(f1)\n        msg         = '[Mean Fold] AUC: {:.5f}(Std:{:.5f}) - F1: {:.5f} - L. Loss: {:.5f}'\n        \n        # \n        if tunning==1:\n            path_name = path + 'model\/preds\/' + model_name_.format(auc_mean, seed)                        \n            jb.dump(y_hat_test, path_name)\n            score = auc_mean\n        else: \n            score = y_hat_test\n            \n        gc.collect()\n\n        print('-'*70)\n        print(msg.format(auc_mean, auc_std, f1_mean, lloss_mean))\n        print('='*70)\n        print('')\n                    \n        return score \n        \n    def xgb(self, trial):\n                \n        params = {'n_estimators'      : trial.suggest_int('n_estimators', 1000, 10000),\n                  'max_depth'         : trial.suggest_int('max_depth', 3, 10),            \n                  'min_child_weight'  : trial.suggest_int('min_child_weight', 10, 250),\n                  'subsample'         : trial.suggest_uniform('subsample', .65, .90),\n                  'colsample_bynode'  : trial.suggest_uniform('colsample_bynode', .65, .8),                              \n                  'learning_rate'     : trial.suggest_uniform('learning_rate', 0.01, 0.3),\n                  'colsample_bytree'  : trial.suggest_uniform('colsample_bytree', 0.3, 0.85),\n                  'reg_lambda'        : trial.suggest_int('reg_lambda', 2, 100),             \n                  'reg_alpha'         : trial.suggest_int('reg_alpha', 1, 50),             \n                  'eta'               : trial.suggest_uniform('eta', 0.01, 0.13),\n                  'alpha'             : trial.suggest_uniform('alpha', .3, .7), \n                  'gamma'             : trial.suggest_uniform('alpha', 10.7, 15.5), \n                 }\n                \n                        \n        mdl = xgb.XGBClassifier(**params, \n                                objective         = 'binary:logistic',                  \n                                predictor         = 'gpu_predictor',\n                                tree_method       = 'gpu_hist',\n                                eval_metric       = 'auc', \n                                random_state      = self.seed  \n                               )\n        \n        random     = str(np.random.rand(1)[0]).replace('.','')\n        name_model = self.name_clf + '_auc_{:2.5f}_{}_' + random + '.pkl.z'  \n        \n        score = TunningModels.cross_valid(model         = mdl, \n                                          model_name_   = name_model,\n                                          X_            = self.X_trn, \n                                          y_            = self.y_trn,  \n                                          X_test_       = self.X_ts,\n                                          type_model    = 2, \n                                          feature       = self.feature,\n                                          seed          = self.seed)\n             \n        return score\n   \n    def lgb(self, trial):\n        \n        params = {'learning_rate'     : trial.suggest_float('learning_rate', 1e-5, 0.1),               \n                  'max_depth'         : trial.suggest_int('max_depth', 3, 12),\n                  'n_estimators'      : trial.suggest_int('n_estimators', 500, 10000),\n                  'num_leaves'        : trial.suggest_int('num_leaves', 8, 356),\n                  'min_child_samples' : trial.suggest_int('min_child_samples', 2, 3000),\n                  'feature_fraction'  : trial.suggest_uniform('feature_fraction', 0.75, 0.95),\n                  'bagging_fraction'  : trial.suggest_uniform('bagging_fraction', 0.75, 0.95),\n                  'bagging_freq'      : trial.suggest_int('bagging_freq', 3, 7),\n                  'reg_alpha'         : trial.suggest_int('reg_alpha', .1, .90),\n                  'reg_lambda'        : trial.suggest_int('reg_lambda', 1e-5, 1e-2),                   \n                  'lambda_l1'         : trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n                  'lambda_l2'         : trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),           \n                }\n        \n        # Add a callback for pruning.\n        pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\")\n        \n        mdl = lgb.LGBMClassifier(**params, \n                                 objective     = 'binary',                   \n                                 metric        = 'auc',\n                                 verbosity     = -1,\n                                 boosting_type = 'gbdt',\n                                 class_weight  = 'balanced',\n                                 #callbacks     = [pruning_callback],\n                                 random_state  = self.seed, \n                                 n_jobs        = -1,\n                                 device       = 'gpu',                                 \n                                )\n        \n        random     = str(np.random.rand(1)[0]).replace('.','')\n        name_model = self.name_clf + '_auc_{:2.5f}_{}_' + random + '.pkl.z'  \n        \n        score = TunningModels.cross_valid(model         = mdl, \n                                          model_name_   = name_model,\n                                          X_            = self.X_trn, \n                                          y_            = self.y_trn,  \n                                          X_test_       = self.X_ts,\n                                          type_model    = 1, \n                                          feature       = self.feature,\n                                          seed          = self.seed)\n        \n        return score\n    \n    def ctb(self, trial):\n                \n        params = {\n                  'n_estimators'              : trial.suggest_int('n_estimators', 1000, 7000),\n                  'max_depth'                  : trial.suggest_int('max_depth', 2, 12),\n                  'iterations'                : trial.suggest_int('iterations', 1000, 7000),\n                  'od_wait'                   : trial.suggest_int('od_wait', 500, 2000),\n                  'learning_rate'             : trial.suggest_uniform('learning_rate', 1e-5, 0.1),\n                  'reg_lambda'                : trial.suggest_uniform('reg_lambda',1e-5, 1e-2),\n                  'subsample'                 : trial.suggest_uniform('subsample', 0, 1),\n                  'random_strength'           : trial.suggest_uniform('random_strength', 10, 50),\n                  'depth'                     : trial.suggest_int('depth', 3, 12),\n                  'min_data_in_leaf'          : trial.suggest_int('min_data_in_leaf', 1, 30),\n                  'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 15),\n                }\n        \n        mdl = ctb.CatBoostClassifier(**params, \n                                     eval_metric  = 'AUC',\n                                     task_type    = 'GPU',\n                                    )\n              \n        random     = str(np.random.rand(1)[0]).replace('.','')\n        name_model = self.name_clf + '_auc_{:2.5f}_{}_' + random + '.pkl.z'  \n        \n        score = TunningModels.cross_valid(model         = mdl, \n                                          model_name_   = name_model,\n                                          X_            = self.X_trn, \n                                          y_            = self.y_trn,  \n                                          X_test_       = self.X_ts,\n                                          type_model    = 1, \n                                          feature       = self.feature,\n                                          seed          = self.seed)\n        \n        return score\n    \n    \n    def rf(self, trial):\n                \n        params = {'max_depth'           : trial.suggest_int('max_depth', 4, 12),\n                  'n_estimators'        : trial.suggest_int('n_estimators', 10, 1000),\n                  'max_samples'         : trial.suggest_int('max_samples', 50 , 1200), \n                  #'min_sample_split'    : trial.suggest_int('min_sample_split', 100 , 1000),\n                  #'max_terminal_nodes'  : trial.suggest_float('max_terminal_nodes', .01 , .8),                  \n                  #'min_samples_lea'     : trial.suggest_int('min_samples_lea', 100 , 1000),\n                  'max_features'        : trial.suggest_float('max_features', .65 , .9), \n                }\n     \n            \n        mdl = rf_model = RandomForestClassifier(**params, n_jobs=-1)\n                      \n        random     = str(np.random.rand(1)[0]).replace('.','')\n        name_model = self.name_clf + '_auc_{:2.5f}_{}_' + random + '.pkl.z'  \n        \n        score = TunningModels.cross_valid(model         = mdl, \n                                          model_name_   = name_model,\n                                          X_            = self.X_trn, \n                                          y_            = self.y_trn,  \n                                          X_test_       = self.X_ts,\n                                          type_model    = 2, \n                                          feature       = self.feature,\n                                          seed          = self.seed)\n        \n        return score\n    \n     \n    def knn(self, trial):\n                \n        params = {'n_neighbors' : trial.suggest_int('n_neighbors', 4, 500),\n                  #'metric'      : trial.suggest_categorical ('max_samples', ['euclidean', 'minkowski','seuclidean'])\n                }\n        \n        mdl = KNeighborsClassifier(**params)\n        \n        random     = str(np.random.rand(1)[0]).replace('.','')\n        name_model = self.name_clf + '_auc_{:2.5f}_{}_' + random + '.pkl.z'  \n        \n        feature= ['fe_n_missing', 'fe_f5_zero', 'fe_f50_zero', 'fe_mean',\n                  'fe_median', 'fe_std', 'fe_skew', 'cluster']\n        \n        score = TunningModels.cross_valid(model         = mdl, \n                                          model_name_   = name_model,\n                                          X_            = self.X_trn, \n                                          y_            = self.y_trn,  \n                                          X_test_       = self.X_ts,\n                                          type_model    = 2, \n                                          feature       = feature,\n                                          seed          = self.seed)\n        \n        return score","f7dac61b":"# %%time\n# \n# SEED = 12359 \n# \n# name_model_xgb = 'xgb_001_tun'\n# TunningModels.delete_files(name_model_xgb)\n#     \n# # Inicialize a classe do modelo de otimiza\u00e7\u00e3o\n# modelOpt = TunningModels(name_model = name_model_xgb, \n#                          X_trn_     = X, \n#                          y_trn_     = y, \n#                          X_ts_      = X_test_sc_qt,                                     \n#                          feature_   = None)\n# \n# study_xgb = optuna.create_study(direction = 'maximize',\n#                                 sampler   = optuna.samplers.TPESampler(seed=SEED),\n#                                 pruner    = optuna.pruners.MedianPruner(n_warmup_steps=10),\n#                                 study_name= 'xgb_tuning'\n#                                ) \n# \n# study_xgb.optimize(modelOpt.xgb, \n#                    n_trials  = 10, \n#                   )\n# \n# auc_best_xgb = study_xgb.best_value \n# params_xgb   = study_xgb.best_params \n# path_name    = path + 'model\/optuna\/' + name_model_xgb + '_{:2.5f}.pkl.z'.format(auc_best_xgb) \n# \n# print('')\n# print('Best average accuracy: {:2.5f}'.format(auc_best_xgb))\n# print('Best parameters: {}'.format(params_xgb))\n# \n# jb.dump(study_xgb, path_name)","7c26c2d4":"#params_xgb","1972ec3e":"#plot_optimization_history(study_xgb)","181fbc0c":"#plot_intermediate_values(study_xgb)","c461b98c":"#plot_parallel_coordinate(study_xgb)","1c4a4b8a":"#plot_parallel_coordinate(study_xgb, params=[\"max_depth\", \"n_estimators\", 'reg_lambda'])","3575f068":"#plot_contour(study_xgb)","8563d2d1":"#plot_contour(study_xgb, params=[\"max_depth\", \"n_estimators\", 'reg_lambda'])","7fe6eb3b":"#plot_slice(study_xgb)","84d30038":"#plot_slice(study_xgb, params=[\"max_depth\", \"n_estimators\", 'reg_lambda'])","159c2656":"#plot_param_importances(study_xgb)","bf977a2f":"#optuna.visualization.plot_param_importances(study_xgb, \n#                                            target=lambda t: t.duration.total_seconds(), \n#                                            target_name=\"duration\")","75bd2827":"#plot_edf(study_xgb)","6b79a61c":"#%%time\n#model_xgb_001 = xgb.XGBClassifier(**params_xgb, \n#                                  objective         = 'binary:logistic',                  \n#                                  predictor         = 'gpu_predictor',\n#                                  tree_method       = 'gpu_hist',\n#                                  eval_metric       = 'auc', \n#                                  random_state      = SEED\n#                                 )\n#\n#y_hat_test_xgb_001 = TunningModels.cross_valid(model       = model_xgb_001, \n#                                               model_name_ = name_model_xgb, \n#                                               X_          = X_train, \n#                                               y_          = y_train, \n#                                               X_test_     = X_test_sc_qt, \n#                                               type_model  = 1, \n#                                               feature     = None,\n#                                               seed        = SEED, \n#                                               tunning     = 0\n#                                               )\n#\n#model_xgb = model_xgb_001.fit(X_train, y_train)\n#y_hat_xgb = model_xgb.predict(X_valid)\n#\n#print(metrics.classification_report(y_valid, y_hat_xgb))","7b599ed4":"#metrics.plot_confusion_matrix(model_xgb, \n#                              X_valid, \n#                              y_valid,\n#                              cmap   = 'inferno')\n#plt.title('Confusion matrix')\n#plt.grid(False)\n#plt.show()","00b094f8":"#y_pred = y_hat_xgb\n#y_true = y_valid\n#precisions, recalls, thresholds = metrics.precision_recall_curve(y_true, y_pred)\n#\n#plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n#plt.show()","f5af60f3":"#plot_precision_vs_recall(precisions, recalls)\n#plt.show()","b5649c34":"#y_true=y_valid\n#y_pred=y_hat_xgb\n#\n#fpr, tpr, thresholds = metrics. roc_curve(y_true, y_pred)\n#\n#plot_roc_curve(fpr, tpr, label=\"XGB\")\n#plt.show()\n","dc8af910":"#submission = pd.DataFrame({'id': df_submission.id, 'loss': y_hat_test_xgb_001})\n#submission.to_csv(path + 'Data\/sumbmission\/001_xgx_submission_tunning.csv', index=False)","5462e7e6":"# %%time\n# SEED = 12359 \n# \n# name_model_lgb = 'lgb_001_tun'\n# TunningModels.delete_files(name_model_lgb)\n#     \n# # Inicialize a classe do modelo de otimiza\u00e7\u00e3o\n# modelOpt = TunningModels(name_model = name_model_lgb, \n#                          X_trn_     = X, \n#                          y_trn_     = y, \n#                          X_ts_      = X_test_sc_qt,                                     \n#                          feature_   = None)\n# \n# study_lgb = optuna.create_study(direction = 'maximize',\n#                                 sampler   = optuna.samplers.TPESampler(seed=SEED),\n#                                 pruner    = optuna.pruners.MedianPruner(n_warmup_steps=10),\n#                                 study_name= 'lgbm_tuning'\n#                                ) \n# \n# study_lgb.optimize(modelOpt.lgb,n_trials=10)\n# \n# auc_best_lgb = study_lgb.best_value \n# params_lgb   = study_lgb.best_params \n# path_name    = path + 'model\/optuna\/' + name_model_lgb + '_{:2.5f}.pkl.z'.format(auc_best_lgb) \n# \n# print('')\n# print('Best average accuracy: {:2.5f}'.format(auc_best_lgb))\n# print('Best parameters: {}'.format(params_lgb))\n# \n#jb.dump(study_lgb, path_name)","269ca50e":"#plot_optimization_history(study_lgb)","495dfc2e":"#plot_parallel_coordinate(study_lgb)","330f0083":"#plot_parallel_coordinate(study_lgb, params=['max_depth','learning_rate', 'num_leaves'])","c49b5e5b":"#plot_slice(study_lgb)","07a5c53b":"#plot_slice(study_lgb)","43c114f9":"#plot_slice(study_lgb, params=['max_depth','learning_rate', 'num_leaves'])","981db668":"#optuna.visualization.plot_param_importances(study_lgb, \n#                                            target=lambda t: t.duration.total_seconds(), \n#                                            target_name=\"duration\"\n#                                           )","21376e2d":"#plot_param_importances(study_lgb, \n#                       target      = lambda t: t.duration.total_seconds(),\n#                       target_name = \"duration\")","84432924":"#plot_edf(study_lgb)","2232c920":"#%%time\n#model_lgb_001      = lgb.LGBMClassifier(**params_lgb,\n#                                        objective     = 'binary',                   \n#                                        metric        = 'auc',\n#                                        verbosity     = -1,\n#                                        boosting_type = 'gbdt',\n#                                        class_weight  = 'balanced',                                        \n#                                        random_state  = SEED, \n#                                        n_jobs        = -1,\n#                                        device       = 'gpu',                                 \n#                                        )\n#                                        \n#y_hat_test_lgb_001 = TunningModels.cross_valid(model       = model_lgb_001, \n#                                               model_name_ = name_model_lgb, \n#                                               X_          = X_train, \n#                                               y_          = y_train, \n#                                               X_test_     = X_test_sc_qt, \n#                                               type_model  = 1, \n#                                               feature     = None,\n#                                               seed        = SEED, \n#                                               tunning     = 0\n#                                               )\n#\n#model_lgb = model_lgb_001.fit(X_train, y_train)\n#y_hat_lgb = model_lgb.predict(X_valid)\n\n#print(metrics.classification_report(y_valid, y_hat_lgb))","be2cc145":"#metrics.plot_confusion_matrix(model_lgb, \n#                              X_valid, \n#                              y_valid,\n#                              cmap   = 'inferno')\n#plt.title('Confusion matrix')\n#plt.grid(False)\n#plt.show()","8d8743aa":"#submission = pd.DataFrame({'id': df_submission.id, 'loss': y_hat_test_lgb_001})\n#submission.to_csv(path + 'Data\/sumbmission\/001_lgb_submission_tunning.csv', index=False)","3d178bbc":"#%%time\n#SEED = 12359 \n#\n#name_model_rf = 'rf_001_tun'\n#TunningModels.delete_files(name_model_rf)\n#    \n#modelOpt = TunningModels(name_model = name_model_rf, \n#                         X_trn_     = X, \n#                         y_trn_     = y, \n#                         X_ts_      = X_test_sc_qt,                                     \n#                         feature_   = None)\n#\n#study_rf = optuna.create_study(direction = 'maximize',\n#                            sampler   = optuna.samplers.TPESampler(seed=SEED),\n#                            pruner    = optuna.pruners.MedianPruner(n_warmup_steps=10),\n#                            study_name= 'rf_tuning'\n#                           ) \n#\n#study_rf.optimize(modelOpt.rf, n_trials=10)\n#\n#auc_best_rf = study_rf.best_value \n#params_rf   = study_rf.best_params \n#path_name    = path + 'model\/optuna\/' + name_model_rf + '_{:2.5f}.pkl.z'.format(auc_best_rf) \n#\n#print('')\n#print('Best average accuracy: {:2.5f}'.format(auc_best_rf))\n#print('Best parameters: {}'.format(params_rf))\n#print('')\n\n#jb.dump(study_rf, path_name)","21e997ed":"#model_rf_001      = RandomForestClassifier(**params_rf)\n#y_hat_test_rf_001 = TunningModels.cross_valid(model       = model_rf_001, \n#                                               model_name_ = name_model_rf, \n#                                               X_          = X_train, \n#                                               y_          = y_train, \n#                                               X_test_     = X_test_sc_qt, \n#                                               type_model  = 2, \n#                                               feature     = None,\n#                                               seed        = SEED, \n#                                               tunning     = 0\n#                                               )\n#\n#model_rf = model_rf_001.fit(X_train, y_train)\n#y_hat_rf = model_rf.predict(X_valid)\n#\n#print(metrics.classification_report(y_valid, y_hat_rf))","2cbe70d0":"#metrics.plot_confusion_matrix(model_rf, \n#                              X_valid, \n#                              y_valid,\n#                              cmap   = 'inferno')\n#plt.title('Confusion matrix')\n#plt.grid(False)\n#plt.show()","4be24df7":"#submission = pd.DataFrame({'id': df_submission.id, 'loss': y_hat_test_rf_001})\n#submission.to_csv(path + 'Data\/sumbmission\/001_rf_submission_tunning.csv', index=False)","5996d5d1":"#df_sub_lgb = pd.read_csv('..\/input\/tpsset21003\/001_lgb_submission_tunning.csv')\n#df_sub_xgb  = pd.read_csv('..\/input\/tpsset21003\/001_xgx_submission_tunning.csv')\n#\n#df_sub_lgb.shape, df_sub_xgb.shape","ac06127b":"#submission = pd.DataFrame({'id': df_submission.id, 'claim': df_sub_lgb.claim})\n#submission.to_csv(path + 'Data\/sumbmission\/001_lgb_submission_01.csv', index=False)","e8301e4e":"#y_pred_ensable_01 = df_sub_lgb.claim * 0.8 + df_sub_xgb.claim * .2\n#y_pred_ensable_01\n\n# score kaggle: 0.81359 com .5\n# scare kagle: 0.81631 ","029029fa":"#submission = pd.DataFrame({'id': df_submission.id, 'claim': y_pred_ensable_01})\n#submission.to_csv(path + 'Data\/sumbmission\/001_ensable_submission_02.csv', index=False)","30ed906a":"df_cv_1 = get_df_cv('..\/input\/tpsset21002\/')\ndf_cv_2 = get_df_cv('..\/input\/psset21003\/')\ndf_cv_1.shape, df_cv_2.shape","b68b06f7":"\ndf_cv = pd.concat([df_cv_1, df_cv_2], axis=1)\ndf_cv.shape","11184468":"df_cv.head()","fe3f797a":"df_cv.describe()","afe758b5":"df_cv.mean(axis=1)","07188e69":"submission = pd.DataFrame({'id': df_submission.id, 'claim': df_cv.mean(axis=1) })\nsubmission.to_csv(path + 'Data\/sumbmission\/001_ensable_submission_05.csv', index=False) ","bec42a9f":"graf_corr(df_cv)","a74ac434":"corr_features = correlation(df_cv, 0.999)\nlen(set(corr_features))","230bef56":"print(corr_features)","cf16e72f":"df_cv.drop(labels=corr_features, axis=1, inplace=True)\n\ngraf_corr(df_cv)\n","9a4e8e27":"df_cv.columns","3dc39bb0":"submission = pd.DataFrame({'id': df_submission.id, 'claim': df_cv.mean(axis=1)})\nsubmission.to_csv(path + 'Data\/sumbmission\/001_ensable_submission_06.csv', index=False) \n\n# submiss\u00e3o no kaggle: 0.81154 ","4dc90047":"submission = pd.DataFrame({'id': df_submission.id, 'claim': df_cv.filter(regex=r'auc_0.81', axis=1).mean(axis=1) })\nsubmission.to_csv(path + 'Data\/sumbmission\/001_ensable_submission_07.csv', index=False) \n\n# submiss\u00e3o no kaggle: 0.81154  ","1533c5b8":"- Visualize the learning curves of the trials","ca3881f0":"### 1.3.3. Modelo Final","757f2545":"## 15.1. Recuparar as submissions  ","890e1240":"https:\/\/www.kaggle.com\/vishwas21\/tps-sep-21-tuning-xgb-catb-lgbm-and-stacking#Tuning-Light-GBM-Classifier-using-Optuna","05c48880":"# <div class=\"alert alert-success\">  0. IMPORTA\u00c7\u00d5ES <\/div>","d75f9851":"## 0.1. Bibliotecas","aba43a76":"**- Visualize relacionamentos de par\u00e2metros de alta dimens\u00e3o.**","3c6c53f9":"**- Visualize o hist\u00f3rico de otimiza\u00e7\u00e3o.**","dd4e7a03":"### 1.2.2. Analise de hiperpar\u00e2metros","becc1755":"### 15.4.1. Gerar Submission","b48d766b":"<h1 div class='alert alert-success'><center> TPS-Set: Tunning Hyperparameters (XGB, Catb and LGBM )\n <\/center><\/h1>\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26480\/logos\/header.png?t=2021-04-09-00-57-05)","04987f9d":"### 1.1.3. Modelo Final","9ffe0975":"### 1.4.3. Gerar Submission","b309de6c":"### 15.1.2. Predi\u00e7\u00e3o com a m\u00e9dia ponderada","f6417d9a":"- Selecione os par\u00e2metros para visualizar.","35c509e6":"# <div class=\"alert alert-success\"> 1.  TUNNING <\/div>","ad41efcd":"Como podemos observar, temos 22 previs\u00f5es realizadas na valida\u00e7\u00e3o cruzada, \u00e9 a mesma coisa que dizer que temos 22 modelos, vamos dar uma olhada no dataset.","ed7748f4":"- Selecione os par\u00e2metros para visualizar.","112cf923":"Temos muitas previs\u00f5es autocorrelacionadas, vamos fazer a exclus\u00e3o de algumas. ","8a9c06b3":"### 1.3.3. Gerar Submission","fed087b6":"## 1.4. RF","d79b6899":"**- Saiba quais hiperpar\u00e2metros est\u00e3o afetando a dura\u00e7\u00e3o do ensaio com a import\u00e2ncia dos hiperpar\u00e2metros.**","ff235566":"# 15. Ensable ","f2ac73f4":"## 1.3. LGBM","0bca9565":"- Visualize a fun\u00e7\u00e3o de distribui\u00e7\u00e3o emp\u00edrica","dbf9e1fb":"ROC curve\n","999b6619":"### 1.3.2. Analise","9699d39c":"### 0.3.1. Reduzindo mem\u00f3ria\ntaken from: https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro","0264ee40":"Agora vamos recuperar os melhores parametros do tunning para treinar um modelo.","287f22dd":"### 15.2.3. Correla\u00e7\u00e3o","af0a4b7e":"- Quais hiperpar\u00e2metros est\u00e3o afetando a dura\u00e7\u00e3o do ensaio com a import\u00e2ncia dos hiperpar\u00e2metros.","c692a2fd":"## 1.1. Classe Tunning ","e7805332":"## 1.2. XGB","9351af3b":"### 1.4.1. Tunning ","4a2b18ba":"- Visualize relacionamentos de hiperpar\u00e2metros.","ebdd66ea":"- Visualize hiperpar\u00e2metros individuais como gr\u00e1fico de fatias.","6a486b6f":"## 0.3. Carregar Dados","8b883818":"### 1.3.2. Analise de hiperpar\u00e2metros\n- refer\u00eancia: https:\/\/optuna.readthedocs.io\/en\/latest\/tutorial\/10_key_features\/005_visualization.html","c9004364":"### 1.1.4. Gerar Submission","315ab9e9":"**- Selecione os par\u00e2metros para visualizar.**","03c4e872":"### 1.3.1. Tunning ","e0383755":"## 0.2. Fun\u00e7\u00f5es","14074b54":"Para ensable vamos utilizar as previs\u00f5es geradas nos processos anteriores, que foram comentados, pois na vers\u00e3o anterior no notebook n\u00e3o foi processado at\u00e9 o final, como quero saber o resultado e ter uma ideia como estou estou na competi\u00e3o vamos gerar as submiss\u00f5es. ","2eeb19ca":"**- Visualize a fun\u00e7\u00e3o de distribui\u00e7\u00e3o emp\u00edrica.**","0c70667d":"**- Visualize hiperpar\u00e2metros individuais como gr\u00e1fico de fatias.**","725bf26a":"- Selecione os par\u00e2metros para visualizar.","dceee14f":"### 1.4.2. Analise","280a2859":"- isualize relacionamentos de par\u00e2metros de alta dimens\u00e3o. ","6251d1ed":"**- Visualize as import\u00e2ncias dos par\u00e2metros.**","5611ebde":"Vamos aplicar o scaler nos dados de teste. ","05eeee3e":"### 15.3.1. Gerar Submission","9c77937e":"- Visualize as import\u00e2ncias dos par\u00e2metros.  ","ef9bf74a":"### 1.2.1. Tunning ","3ae4ae6a":"### 1.4.2. Modelo Final","b26e3f4b":"## 15.2. Recuparar dataset\nAqui vamos recupar todas a previs\u00f5es realizadas na valida\u00e7\u00e3o cruzada para gerar uma nova submission.","91323b72":"Precision Recall versus the decision threshold","8a0fc88f":"### 15.2.1. Descritiva ","bc7ab32b":"- Visualize the optimization history"}}