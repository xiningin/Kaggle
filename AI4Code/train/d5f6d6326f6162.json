{"cell_type":{"702d05d5":"code","475cdf2d":"code","3387d956":"markdown"},"source":{"702d05d5":"# libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport numpy as np\nimport time\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nimport math\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GroupKFold, RepeatedStratifiedKFold, cross_validate, StratifiedShuffleSplit\nfrom sklearn import metrics\n\n# DATA LOADING\nfrom sklearn.neural_network import MLPClassifier\n\npath = '\/kaggle\/input\/stock-market-prediction\/'\npathOutput = '.\/'\ncompletoTrain='dataset_train_validation.csv'\ncompletoTest='dataset_test.csv'\n\n# INPUT\ntrain = pd.read_csv(os.path.join(path, completoTrain))\ntest = pd.read_csv(os.path.join(path, completoTest))\n\n# Se separan los resultados reales con los que comparar\ncolumns =  [col for col in test.columns if col not in ['company', 'age', 'market', 'TARGET']]\nsubmission = test[columns]\nsolucion = test['TARGET']\n\n####################### ELIMINACION de filas con null en la columna TARGET\ndef filter_rows_by_values(df, col, values):\n    return df[~df[col].isin(values)]\n\ntrain = filter_rows_by_values(train, \"TARGET\", [\"null\"])\n\n########################## NUEVAS FEATURES ###########################\n######33###### RSI ###################\ndef relative_strength_idx(df, n=14):\n    close = df['close']\n    delta = close.diff()\n    delta = delta[1:]\n    pricesUp = delta.copy()\n    pricesDown = delta.copy()\n    pricesUp[pricesUp < 0] = 0\n    pricesDown[pricesDown > 0] = 0\n    rollUp = pricesUp.rolling(n).mean()\n    rollDown = pricesDown.abs().rolling(n).mean()\n    rs = rollUp \/ rollDown\n    rsi = 100.0 - (100.0 \/ (1.0 + rs))\n    return rsi\n\n################################# NUEVAS FEATURES TRAIN\ntrain['close_lag'] = train['close'].shift(1)\ntrain['RSI'] = relative_strength_idx(train).fillna(0)\ntrain = train.fillna(0)\n\n################################## NUEVAS FEATURES TEST\n# all the same for the test data\ntest['close_lag'] = test['close'].shift(1)\ntest['RSI'] = relative_strength_idx(test).fillna(0)\ntest = test.fillna(0)\n\n#########################################\n# Se fraccionan los datos de train en: train + validaci\u00f3n\nfraccion_train = 0.7  # Fracci\u00f3n de datos usada para entrenar\nfraccion_valid = 1.00 - fraccion_train\ntrain_aleatorio = train.sample(frac=1)\ntrain = train_aleatorio.iloc[:int(fraccion_train * len(train)), :]\nvalidacion = train_aleatorio.iloc[int(fraccion_train * len(train)):, :]\n\n################# Se separa en features y target\ntrain_X = train[columns]\ntrain_y = train['TARGET']\nvalid_X = validacion[columns]\nvalid_y = validacion['TARGET']\n\n# MODEL TRAINING\n###################### MODELO LGBM ######################\nfolds = GroupKFold(n_splits=5)\nparams = {'objective': 'binary',\n          'learning_rate': 0.02,\n          \"boosting_type\": \"gbdt\",\n          \"metric\": 'precision',\n          'n_jobs': -1,\n          'min_data_in_leaf': 32,\n          'num_leaves': 1024,\n          }\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(train_X, train_y, groups=train['company'])):\n    print(f'Fold {fold_n} started at {time.ctime()}')\n    X_train, X_valid = train_X[columns].iloc[train_index], train_X[columns].iloc[valid_index]\n    y_train, y_valid = train_y.iloc[train_index], train_y.iloc[valid_index]\n\n    model = lgb.LGBMClassifier(**params, n_estimators=50)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)])\n#####################################################\n\n#################### SE DIBUJAN LAS FEATURES POR IMPORTANCIA #################\nfeature_importance = pd.DataFrame()\nfold_importance = pd.DataFrame()\nfold_importance[\"feature\"] = columns\nfold_importance[\"importance\"] = model.feature_importances_\nfold_importance[\"fold\"] = fold_n + 1\nfeature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\nfeature_importance[\"importance\"] \/= 5\n# Se pintan las primeras 50 features\ncols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\n\nbest_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\nplt.figure(figsize=(16, 12));\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\nplt.title('LGB Features (avg over folds)');\n# plt.show(block=False)\n# plt.pause(5)\nplt.savefig(pathOutput + \"BOLSA_feature_importances.png\")\n# plt.close()\n###################\n\n##################### VALIDACI\u00d3N ###################\nprint(\"COMIENZO DE VALIDACI\u00d3N\")\nscore = metrics.mean_absolute_error(valid_y, model.predict(valid_X))\nprint('CV score: {0:.4f}.'.format(score))\nprint(\"FIN DE VALIDACI\u00d3N\")\n###############################################\n","475cdf2d":"# PREDICCI\u00d3N independiente\n\n# Eliminaci\u00f3n de filas a null\ntest = filter_rows_by_values(test, \"TARGET\", [\"null\"])\n\nsubmission = test[columns]\nsolucion = test['TARGET']\n\n# S\u00f3lo aplicable si hay sigmoid en la \u00faltima capa\nprediccion = (model.predict(submission) > 0.5).astype(\"int32\")\nsubmission['TARGET']=prediccion\n\n# Se a\u00f1ade la empresa en la primera columna y se guarda en Excel\nsubmission=submission.join(test['company'])\nsubmission.set_index(submission.pop('company'), inplace=True)\nsubmission.reset_index(inplace=True)\nsubmission.to_csv(os.path.join(pathOutput, 'Bolsa_DL_submission.csv'), index=False)\n\n# PRECISION independiente\na=solucion\nb=prediccion\nTP=sum(1 for x,y in zip(a,b) if (x == y and y == 1))\nTPandFP=sum(b)\nprecision= TP \/ TPandFP\nprint(\"TP: \", TP)\nprint(\"TP + FP: \", TPandFP)\nprint(\"---->>>>>> PRECISION (TP\/(TP+FP)) FOR TEST DATASET: {0:.2f}% <<<<<<------\".format(precision * 100))\n\n# Tasa de desbalanceo\nift_mayoritaria = test[test.TARGET == False]\nift_minoritaria = test[test.TARGET == True]\ntasaDesbalanceo = round(ift_mayoritaria.shape[0] \/ ift_minoritaria.shape[0], 2)\nprint(\"Tasa de desbalanceo = \" + str(ift_mayoritaria.shape[0]) + \"\/\" + str(\n        ift_minoritaria.shape[0]) + \" = \" + str(tasaDesbalanceo))\n\nprint(\"Tasa de mejora de precisi\u00f3n respecto a random: \",\n              round(precision \/ (1\/(1+tasaDesbalanceo)), 2))\n\nfrom sklearn.metrics import classification_report\nprint(\"Informe de metricas:\")\nprint(classification_report(a, b))\n\nprint(\"END\")","3387d956":"\n\n# **Independent prediction:**\n\n"}}