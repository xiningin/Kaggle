{"cell_type":{"0427aec8":"code","02737464":"code","293288a3":"code","93696b80":"code","0d1713f9":"code","cb5aaa83":"code","f1db2851":"code","07275ff0":"code","dd3ebd98":"code","4db17ae5":"code","c819eeba":"code","152d65d3":"code","92a82a6c":"code","44adb55a":"code","3e52f44d":"code","d99d4079":"code","5867222e":"code","34332d32":"code","4d3b1184":"code","0ca08e2f":"code","607c213e":"code","8079f812":"code","d4a18867":"code","d4c64994":"code","dd252ee2":"code","db3481f1":"code","ba68fada":"markdown","6e3ceede":"markdown","b47db267":"markdown","58d1baee":"markdown","8572cd29":"markdown","cf0eec5e":"markdown","bcdd8bc6":"markdown","9f2865da":"markdown","b189dc77":"markdown","e08e922a":"markdown","be8ff180":"markdown","21a5c08d":"markdown","c3ad4a92":"markdown","93273848":"markdown","82ad2fd4":"markdown","0ad976bf":"markdown","d5a06781":"markdown","4aeeee6e":"markdown","3f055e52":"markdown","4cce0323":"markdown","b742ec63":"markdown","ea3fdec2":"markdown","fdebc8ea":"markdown","65e6d374":"markdown","4fd4abe9":"markdown","e3ca2e02":"markdown","05234487":"markdown"},"source":{"0427aec8":"#Importing the libraries used\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score","02737464":"df = pd.read_csv('..\/input\/diamonds\/diamonds.csv') #importing the dataframe from the appropriate folder\ndf = df.drop(['Unnamed: 0'], axis = 1) #removing the column of default indexes: the change was not necessary, only\n#for the sake of appearance","293288a3":"df","93696b80":"df.info()","0d1713f9":"df.isna().sum()","cb5aaa83":"print('Cut:\\n', df['cut'].value_counts(), '\\n')\nprint('Color:\\n', df['color'].value_counts(), '\\n')\nprint('Clarity:\\n', df['clarity'].value_counts(), '\\n')","f1db2851":"df['cut'] = df['cut'].replace({'Fair': 0, 'Good': 1, 'Very Good': 2, 'Premium': 3, 'Ideal': 4})\ndf['color'] = df['color'].replace({'J': 0, 'I': 1, 'H': 2, 'G': 3, 'F': 4, 'E': 5, 'D': 6})\ndf['clarity'] = df['clarity'].replace({'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1': 6, 'IF': 7})","07275ff0":"print(df.groupby('x').get_group(0), '\\n')\nprint(df.groupby('y').get_group(0), '\\n')","dd3ebd98":"print('Total number of records to be dropped:', len(df.groupby('z').get_group(0)))\ndf.groupby('z').get_group(0)","4db17ae5":"zeroes = df.groupby('z').get_group(0).index\ndf = df.drop(zeroes)","c819eeba":"plt.figure(figsize=(7,5))\nsns.heatmap(df.corr(), cmap = 'Blues', annot=True, fmt=\".2f\")","152d65d3":"df['boxVolume'] = df['x'] * df['y'] * df['z']\ndf = df.drop(['x', 'y', 'z'], axis = 1)\ndf","92a82a6c":"df.hist(bins=50, figsize=(20,15))","44adb55a":"plt.figure(figsize=(7,5))\nsns.heatmap(df.corr(), cmap = 'Blues', annot=True, fmt=\".2f\")","3e52f44d":"plt.scatter(df['carat'], df['boxVolume'], alpha = 0.1)","d99d4079":"df = df.drop(['boxVolume'], axis = 1)","5867222e":"plt.scatter(df['carat'], df['price'], alpha = 0.1)\nplt.xlabel('Weight (carat)')\nplt.ylabel('Price')\nplt.title('Weight vs price correlation')","34332d32":"x = df.drop(['price'], axis = 1)\ny = df['price']","4d3b1184":"z = x.values #returns a numpy array\nmin_max_scaler = MinMaxScaler()\nz_scaled = min_max_scaler.fit_transform(z)\nx = pd.DataFrame(z_scaled)\nx","0ca08e2f":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)","607c213e":"dummyModel = DummyRegressor(strategy = \"median\")\ndummyModel.fit(X_train, y_train)\npredictionsDummy = dummyModel.predict(X_test)\nr2ScoreDummy = dummyModel.score(X_test, y_test)","8079f812":"modelLinReg = LinearRegression()\nmodelLinReg.fit(X_train, y_train)\npredictionsLinReg = modelLinReg.predict(X_test)\npredictionsLinReg","d4a18867":"final_mse = mean_squared_error(y_test, predictionsLinReg)\nfinal_rmse = np.sqrt(final_mse)\n\nr2ScoreLinReg = r2_score(y_test, predictionsLinReg)\nprint('RMSE =', final_rmse, '\\nR2 =', r2ScoreLinReg)","d4c64994":"def chooseKFold():\n    maxi = 1 #saving the index of the highest score\n    max = 0 #saving the value of the highest score\n    for i in range(2,20):\n        accuracy = cross_val_score(modelLinReg, X_train, y_train, cv = i).mean()\n        if (accuracy > max):\n            maxi = i\n            max = accuracy\n    print('Best index:', maxi, \"\\ncross_val_score of index\", maxi, ':', max)\nchooseKFold()","dd252ee2":"fig = plt.figure()\nr2Scores = [r2ScoreDummy, r2ScoreLinReg]\nr2ScoresSize = np.arange(len(r2Scores))\naxes = fig.add_axes([0,0,1,1])\naxes.bar(['Dummy', 'Linear Regression'], r2Scores)\naxes.set_ylabel(\"Accuracy Score\")\naxes.set_title(\"Accuracy Scores of the different models\")","db3481f1":"modelLinReg.coef_ ","ba68fada":"We can see from the correlations dataframe that the x, y and z attributes have an extremely high linear correlation with one another, and from a logical point of view we can understand that as the length grows (for example), so do the width and the depth. As the 3 attributes are closely related to one another, we can try to think of a way to combine the 3, in order to maximize the future model's effectiveness. The best solution in my point of view is to multiply the 3 columns by one another, and putting the returned values into a new column, thus creating the column \"volume\". It has to be noted, that that the volume calculated is not of the diamond itself, but the \"box\" enclosing it on all size. The value, of course, is greater than the true volume of the diamond, but hte true volume of the diamond is not the purpose of this notebook. The proportions remain true, and that is what is needed for the models ahead.<br>\nSo we create a new column as was described, and drop the used values of x, y and z.","6e3ceede":"Now we can calculate the RMSE (Root Mean Square Error - The standard deviation of residuals) and the r2 score (described above).<br>\nThe RMSE is calculated by:\n$$ RMSE = \\sqrt{\\frac{\\sum_{i=1}^{N} (x_{i} - \\hat{x}_{i})^{2}}{N}} $$\nFrom the values recieved we can understand:\n- The model is quite good - the R2 value is 0.9, which is close to 1, thus meaning that the variance of the model is quite low.\n- The RMSE isrelatively low, considering that the prices range from almost 0 to 18000 (approximately)","b47db267":"First, we have to import the dataset .csv file into the notebook. Kaggle adds an index to the data, but upon import, the notebook also gives an index to the data, rendering Kaggle's index (named \"Unnamed: 0\") useless, so we drop it.","58d1baee":"Below are the histogams of every attribute and the with the attribute values as the x axis and the amount of records with said value in said attribute as the y axis. For our purposes, it is of almost no significance, except to visually understand the approximate distributions of values over the dataframe. This is needed in turn to understand that the data is not \"capped\" on either end (for example), meaning that the actual values were not rounded to a certain value if they were above that value, and likewise below. Another use for the histograms is seing that there are suffficient records for most if not all values of every attribute, for example if all reviewers were men, there will be no point of that attribute for the model's purposes.","8572cd29":"Now that we have split our data, we can get down to the models. Before choosing and applying the model to the data, we need a baseline model, to compare all others to it: if the models thereafter are worse than the base model, then the model we are evaluating is not good, to say the least. If the model is better than the base model, on the other hand, it means that the model works, and is able to be better at predicting than a \"dummy\" model. The base model we will use is \"Dummy Regressor\", which will, regardless of the input, put in the the median of all values in y_train. It puts in the most median value due to the strategy we have put into the model, although we could choose differently, but this strategy seems to return the best results.\n\nafter using the model.fit() and model.predict() methods and saving the predictions for later use if needed, we can use the model.score() method to test a see the r2 value of the results. in our case, the r2 turns out ot be negative, which is not very good at all as we strive for the r2 to return a value as close to 1 as possible. From the r2 score we can understand that the model does not represent the data in any way.<br>\nThe r2 equation is as follows:\n$$ R^{2} = \\frac{variance_{mean} - variance_{model}}{variance_{mean}} $$","cf0eec5e":"As promised, here we can see explicitly that there are no NaN values to deal with in this dataset.","bcdd8bc6":"### Dummy Classifier","9f2865da":"Below we calculate the best cv parameter (K-folds) value for the cross validation score. We run from 2 to 20. The cv value in cross validation score means the number of \"folds\" the training set is divided into.<br>\nCross validation is when we divide the training set into a certain number of parts of identical length. There are made this certain anount of runs, when for each run, a different one of the divided sets is set aside. The model runs as in the usual trainig set, then another \"piece\" of the training set is used. Their performances are recorded, and the mean value is displayed.<br><br>\nWe have identified after these runs that 5 is the best number of k-folds for this data (out of the given range at least). As we were hoping, the r2 score matches (almost) the r2 score of the original model, meaning the model works as expected and most probably the model is neither underfitted or overfitted.","b189dc77":"Next, we use the fuction min_max_scaler.fit_transform() method to achive normalization of the data.","e08e922a":"Again, we can see an extremely high linear correlation between the \"boxVolume\" and the \"carat\" weight of the diamond. Following a similar logic as before, we are going to drop the created \"boxVolume\" attribute.","be8ff180":"One last thing we have to do before we split the data into train and test sets, we have to normalize the values. As all are values are now numerical, and there are no NaN values, we can do that without too much trouble. The normalization will only be done on the values which will be given to us in the future for prediction (all data except the attribute which we have decided to predict: the overall satisfaction of the reviewer). In this case, the overall satisfaction is already normalized as it comprises of values 0 and 1 only, but in general it is not necessary to normalize the attribute for prediction.<br>\nWe choose normalizaiton and not standartization due to standartization giving less weight to the outliers of the dataframe.<br><br>\nTo normalize the data, we first split it to x and y, and mentioned above.","21a5c08d":"## Data Cleaning\nPresenting the recieved dataset at the end for review.","c3ad4a92":"The following dataset represents diamonds of every sort with multiple features describing each one. There are close to 54000 entries in this dataset. The diamonds are identified by:<br>\n- Carat: weight (measured in carat, which is 0.2g)\n- Cut: cut quality (in increasing order Fair, Good, Very Good, Premium, Ideal)\n- Color: color of the diamond (in increasing quality J, I, H, G, F, E, D), with D being the best and J the worst\n- Clarity: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n- Depth: calculated by the height of a diamond, measured from the culet to the table, divided by its average girdle diameter (the image below gives interpretations to all the \"diamond-wise\" vocabulary).\n- Table: The diamond's table size, expressed as a percentage of its diameter.\n- Price: the diamond's price. This is the value we wish to predict using a model later on in the notebook\n- X: the length of the diamond in mm.\n- Y: the width of the diamond in mm.\n- Z: the depth of the diamond in mm.\n<br><br>\nAs Mentioned before, our purpose in this notebook is to determine the price of a diamond (not necessarily from this dataset), given every other feature of a diamond except its price.<br>\nLink to the dataset on kaggle: https:\/\/www.kaggle.com\/shivam2503\/diamonds","93273848":"Due to there being a high correlation between the diamond weight (\"carat\") and the diamond price, we shall plot a scatter graph of diamond weight vs price. We can see that much of the data is in roughly a parabolic (exponential) to linear shape, which suggests that the model of linear regression might be of use in this dataframe.<br>\nfurthermore, there are multiple outliers to the data. If their number was small (int the units), it might have been a good idea to remove them from consideration as they could be \"anomalies\", or the data was input wrong, to name but 2 options. Since it is not the case with our data, I choose not to ignore the outliers, but rather include them with all the rest for consideration of the model.","82ad2fd4":"Now that we know what each string attribute consists of, we can encode the values appropriately. This is done using the replace() function, which replaces the listed values by others which we specify. For example, wherever the word \"Ideal\" in the \"cut\" attribute appears, it is changed to the number \"4\". In a similar way, all other values are treated. The values are chosen to be 0 for the worst of the list and going upwards through the integers from it.","0ad976bf":"Below, the info() function is run, showing the attributes of the dataset, as well as the number of values each attribute has (telling us in a roundabout way that there are no NaN values, although we will see this explicitly later in just a second), and the data type of each attribute. As we can see that several of the attributes are of the type \"object\" and from a quick review we can see that they are of type \"string\", which we will have to encode to numerical values before applying any machine learning models.","d5a06781":"Below we create a dataframe of correlations between every attribute with one another. The dataframe represents the linear correlation between the attributes (non linear relations have very low values as the functions considers only linear relations). The values range between -1 and 1, where positive values represent positive relations (\"as X increases, Y increases\"), while negative value represent negative relations (\"as X increases, Y decreases\"). All attribute has an exact linear relation with itself (1.0000). A final thing to not is that the table presented has many duplicates (we could remove the upper right triangle (if the table was cut diagonally from the upper-left corner to the lower-right corner), and not lose any data).<br>\nUsing this data, we plot a heatmap using seaborn, to better and easier see the higher values.","4aeeee6e":"## Models\nNow that we are finished with data cleaning, we have a dataset, where there are no NaN values, all attributes are numerical and normalized, we can split the data using the train_test_split() method. The method splits the data according to a random state (an integer), so that over multiple reruns, the resulting dataframes from the method remain the same. The test size is set to the standard 0.2 (20%), which results in close to 11000 records (which is enough in my opinion).","3f055e52":"# Diamond Prices - Regression","4cce0323":"Co cocnlude, we have found a models that works very well with our dataset. It is able to predict the diamond's price based on the features of the diamond with an r2 score of 0.903. We checked and can say with some certainty that the model is not overfitted and not underfitted.","b742ec63":"# to see again\nAgain, We present a correclations dataframe of the diamonds after the changes we have done.","ea3fdec2":"Now that we know what a bad model looks like, lets try to find a better one. We will use Linear Regression. We define the model, run the fit() method to fit the training data, and the predict() method to use the model to predict values and save them to a new variable.","fdebc8ea":"In order to encode the strings from the dataset as numerical values, we first have to understand the values they consist of. Although we were told at the beginning of the notebook the values, it is always good practice to double check, to avoid all kinds of bugs and inconvieniences. This is done below.","65e6d374":"We can notice that some of the values are not as they should be. We can see that some of the widths (\"y\"), lengths (\"x\") and depths (\"z\") of the records (diamonds) are zeroes. This is, of course, impossible. We can also notice, that wherever the \"x\" or \"y\" value is 0, the \"z\" value is also 0. Because there are only 20 such records with zeroes in the \"z\" attribute, it seems more appropriate to drop these records, as it seems wrong to put some value instead (mean or median).<br>\nThe following two cells of code displays these values, where the first cell presents the \"x\" and \"y\" records with zeroes, whereas the second cell shows the \"z\" counterpart as well as the total number of records to be dropped.","4fd4abe9":"![kfolds.png](attachment:kfolds.png)","e3ca2e02":"![diamond_anatomy%20%281%29.jpg](attachment:diamond_anatomy%20%281%29.jpg)","05234487":"The following code cell drops said values, by first selecting them and putting the indexes of those records (as a series) into a variable. Then, dropping the indexes from the original dataset. "}}