{"cell_type":{"b844234a":"code","85eb569e":"code","935cae6c":"code","44e8186a":"code","7aaf2c73":"code","9813b9ba":"code","bff1ba20":"code","8b8e1afc":"code","55cf08fb":"code","3e326ee3":"code","6c20c6c8":"code","fadbcc0f":"code","23ef5843":"code","0a797ff6":"code","09630e6a":"code","b6835bc3":"code","0b39ed11":"code","83801b6c":"code","29050480":"code","8277995f":"code","7398fed2":"code","3c9b6903":"code","3d08d467":"code","d01d7976":"code","20389df4":"code","a2702b92":"code","20aa284d":"code","bbcc2d61":"code","4a4e3582":"code","e9f5597a":"code","fdf7ec7e":"code","4fdb8ce4":"code","b30561fd":"markdown","d01d7ad6":"markdown","a2fecb2c":"markdown","fe657744":"markdown","fc94de89":"markdown","e7aaa5b5":"markdown","71d71725":"markdown","5a452670":"markdown","adef22be":"markdown","4c565753":"markdown","13188e84":"markdown","071c3b14":"markdown","55dbcdf7":"markdown","48c6b01f":"markdown","712c8831":"markdown"},"source":{"b844234a":"from statsmodels.api import OLS, add_constant\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import make_regression\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.preprocessing import normalize","85eb569e":"data=pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","935cae6c":"data.head()","44e8186a":"data.isnull().sum()","7aaf2c73":"y=data.quality\nX=data.drop(['quality'],axis=1)\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42)","9813b9ba":"ols_model=OLS(y_train,X_train)\nresults=ols_model.fit()","bff1ba20":"results.summary()","8b8e1afc":"X.drop(['fixed acidity','citric acid','residual sugar'],axis=1,inplace=True)","55cf08fb":"X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42)","3e326ee3":"X_train=add_constant(X_train)\nX_test=add_constant(X_test)\nols_model=OLS(y_train,X_train)\nresults=ols_model.fit()\nresults.summary()","6c20c6c8":"results.predict(X_test)[0:5]","fadbcc0f":"def rounder(array):\n    rounded_array=[]\n    for i in array:\n        rounded_array.append(int(i))\n    return np.array(rounded_array)","23ef5843":"ols_pred=results.predict(X_test)\nrounded_pred_ols=rounder(ols_pred)\nprint(f'The RMSE of sklearn model: {np.sqrt(mean_squared_error(rounded_pred_ols,y_test))}')","0a797ff6":"lm=LinearRegression()\nlm.fit(X_train,y_train)","09630e6a":"print('Coefficients: ', np.round(lm.coef_,2))\nprint('Intercept: ', np.round(lm.intercept_,2))","b6835bc3":"preds=lm.predict(X_test)\nrounded_pred_lm=rounder(preds)\nprint(f'The RMSE of sklearn model: {np.sqrt(mean_squared_error(y_test,rounded_pred_lm))}')","0b39ed11":"tensor_lm=Sequential()\ntensor_lm.add(Dense(units=1,input_shape=[9,]))#single output","83801b6c":"tensor_lm.summary()","29050480":"tensor_lm.compile(optimizer='Adam',\n                 loss='mean_squared_error')","8277995f":"X_train.shape","7398fed2":"hist=tensor_lm.fit(X_train,y_train,epochs=100,validation_split=0.2,verbose=0)","3c9b6903":"history_df=pd.DataFrame(hist.history)","3d08d467":"history_df['epoch']=hist.epoch","d01d7976":"history_df.head()","20389df4":"test_preds=tensor_lm.predict(X_test).flatten()","a2702b92":"rounded_array_tensor=rounder(test_preds)","20aa284d":"print(f'The RMSE of tensorflow model: {np.sqrt(mean_squared_error(rounded_array_tensor,y_test))}')","bbcc2d61":"tensor_lm.layers[0].kernel","4a4e3582":"tensor_lm_n=Sequential()\ntensor_lm_n.add(Dense(units=64,input_shape=(9,), activation='relu'))\ntensor_lm_n.add(Dense(units=64,activation='relu'))\ntensor_lm_n.add(Dense(units=1))","e9f5597a":"tensor_lm_n.compile(optimizer='adam',loss='mean_squared_error')","fdf7ec7e":"hist=tensor_lm_n.fit(X_train,y_train,epochs=100,verbose=0, validation_split=0.2)","4fdb8ce4":"pred=tensor_lm_n.predict(X_test)\npred=rounder(pred)\nprint(f'RMSE for DNN Model: {np.sqrt(mean_squared_error(y_test,pred))}')","b30561fd":"We should also add a constant to the model. Even this decreases the R-squared value, it makes our model more realistic.\n\n *An intercept is not included by default and should be added by the user.* (From the description of OLS)\n \nSo, for this purpose, we can use \"add_constant()\" from statsmodels.api. ","d01d7ad6":"**Question:** Can we boost the accuracy of the deep learning linear regression model?","a2fecb2c":"Thanks...","fe657744":"The \"m\" we discussed above is,","fc94de89":"So there are lots of variables here to explain. Let's start one by one.\n\n**R-squared:** This is a metric that tells us how successful the independent variable (or variables) expresses the dependent variable.   \n\n**Adj. R-squared:** Since R-squared is about explainability, whenever we add information R-squared increases. The problem is the added information is not always informative. Adjusted R-squared is resistant to this type of information so we can say that it is a more reliable metric while evaluating the model.\n\n**F-statistic:** This shows how \"meaningful\" the model is. \n\n**Prob (F-statistic):** It is the \"P\" value of F-statistic. Since P<0.05, we can say that the model is meaningful.\n\n**AIC and BIC:** These are used for model comparison.\n\n**coef:** Coefficients of each independent variable.\n\n**std.err:** Standard error of these coefficients.\n\n**t:** t values of the coefficients.\n\n**P>|t|:** P values for t values. We can understand whether containing any independent variable is meaningful to explain the model or not. Variables having P>0.05, should be removed from the model. (fixed acidity, citric acid, and residual sugar)\n\n**[0.025 0.975]:** Confidence intervals","e7aaa5b5":"# Linear Regression\nIn linear regression, we try to find a correlation coefficient for each independent variable and sometimes a constant to represent the dependant variable as a function of the independent variable.,\n\nOrdinary Least Squares Method is a method that is used for optimizing the regression equation for the dataset. The goal is to minimize the sum of the distances between data samples and the line of the regression equation.\n\nFor a linear equation, it looks something like this, \n\n![image.png](attachment:image.png)","71d71725":"This notebook we will cover,\n1. What Linear Regression is\n2. What Ordinary Least Squares is\n3. 3 different ways of Linear Regression applications for multiple inputs.\n4. Create a DNN model and see whether it is superior.","5a452670":"# DNN Regression\nIn DNN Regression, we use several additional dense layers before the final one. Let's see whether it increases the accuracy of our model.\n\nNote: Please keep in mind that adding additional layers will not always boost the accuracy. I have already proven that [here](https:\/\/www.kaggle.com\/egemenuurdalg\/ml-model-vs-ann-vs-deeper-ann) for instance.","adef22be":"We can also see the coefficients and intercepts with .coef_ and .intercept_ keywords.","4c565753":"# Importing and Preprocessing the Dataset","13188e84":"The predictions are float numbers however, the values in the dataset are integers. So we have to convert them to the same format before evaluating the models.","071c3b14":"# Regression Model with Scikit-Learn","55dbcdf7":"# Regression Model with Tensorflow\nWe will use a dense layer for linear regression. It will produce an equation y=mx+b where m is a matrix with dimensions (number of features,1)","48c6b01f":"# Regression Model with statsmodels.api","712c8831":"Let's see how to create a linear regression model by using ordinary least squares. But first, we have to import the dataset."}}