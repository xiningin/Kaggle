{"cell_type":{"6facfe0d":"code","9d40ea22":"code","a0fb3837":"code","d89070fd":"code","5d4b235a":"code","24331f07":"code","23c27a88":"code","118634c9":"code","f813c8f3":"code","0c5db904":"code","457d3d2b":"code","386e57fa":"code","9c9fc64e":"code","0394e964":"code","3892f669":"code","cb8d088d":"code","8be82237":"markdown","d5eaca16":"markdown","30349b49":"markdown","edf578ad":"markdown","f42d74d6":"markdown","ba8b28ee":"markdown","45e93392":"markdown","2e3ffc0c":"markdown","32d8e1bf":"markdown","e11624b0":"markdown","10df0dea":"markdown","be4a5af1":"markdown","7d04a12c":"markdown","a91c5db5":"markdown","1ecbedef":"markdown","a319848c":"markdown","de6b2c34":"markdown"},"source":{"6facfe0d":"\nimport numpy as np # forlinear algebra\nimport matplotlib.pyplot as plt #for plotting things\nimport os\nfrom PIL import Image\nprint(os.listdir(\"..\/input\"))\n\n# Keras Libraries\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom sklearn.metrics import classification_report, confusion_matrix","9d40ea22":"mainDIR = os.listdir('..\/input\/chest_xray\/chest_xray')\nprint(mainDIR)","a0fb3837":"train_folder= '..\/input\/chest_xray\/chest_xray\/train\/'\nval_folder = '..\/input\/chest_xray\/chest_xray\/val\/'\ntest_folder = '..\/input\/chest_xray\/chest_xray\/test\/'\n\n","d89070fd":"# train \nos.listdir(train_folder)\ntrain_n = train_folder+'NORMAL\/'\ntrain_p = train_folder+'PNEUMONIA\/'\n","5d4b235a":"#Normal pic \nprint(len(os.listdir(train_n)))\nrand_norm= np.random.randint(0,len(os.listdir(train_n)))\nnorm_pic = os.listdir(train_n)[rand_norm]\nprint('normal picture title: ',norm_pic)\n\nnorm_pic_address = train_n+norm_pic\n\n#Pneumonia\nrand_p = np.random.randint(0,len(os.listdir(train_p)))\n\nsic_pic =  os.listdir(train_p)[rand_norm]\nsic_address = train_p+sic_pic\nprint('pneumonia picture title:', sic_pic)\n\n# Load the images\nnorm_load = Image.open(norm_pic_address)\nsic_load = Image.open(sic_address)\n\n#Let's plt these images\nf = plt.figure(figsize= (10,6))\na1 = f.add_subplot(1,2,1)\nimg_plot = plt.imshow(norm_load)\na1.set_title('Normal')\n\na2 = f.add_subplot(1, 2, 2)\nimg_plot = plt.imshow(sic_load)\na2.set_title('Pneumonia')","24331f07":"# let's build the CNN model\n\ncnn = Sequential()\n\n#Convolution\ncnn.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(64, 64, 3)))\n\n#Pooling\ncnn.add(MaxPooling2D(pool_size = (2, 2)))\n\n# 2nd Convolution\ncnn.add(Conv2D(32, (3, 3), activation=\"relu\"))\n\n# 2nd Pooling layer\ncnn.add(MaxPooling2D(pool_size = (2, 2)))\n\n# Flatten the layer\ncnn.add(Flatten())\n\n# Fully Connected Layers\ncnn.add(Dense(activation = 'relu', units = 128))\ncnn.add(Dense(activation = 'sigmoid', units = 1))\n\n# Compile the Neural network\ncnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n\n\n\n","23c27a88":"num_of_test_samples = 600\nbatch_size = 32","118634c9":"# Fitting the CNN to the images\n# The function ImageDataGenerator augments your image by iterating through image as your CNN is getting ready to process that image\n\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)  #Image normalization.\n\ntraining_set = train_datagen.flow_from_directory('..\/input\/chest_xray\/chest_xray\/train',\n                                                 target_size = (64, 64),\n                                                 batch_size = 32,\n                                                 class_mode = 'binary')\n\nvalidation_generator = test_datagen.flow_from_directory('..\/input\/chest_xray\/chest_xray\/val\/',\n    target_size=(64, 64),\n    batch_size=32,\n    class_mode='binary')\n\ntest_set = test_datagen.flow_from_directory('..\/input\/chest_xray\/chest_xray\/test',\n                                            target_size = (64, 64),\n                                            batch_size = 32,\n                                            class_mode = 'binary')\n","f813c8f3":"cnn.summary()","0c5db904":"cnn_model = cnn.fit_generator(training_set,\n                         steps_per_epoch = 163,\n                         epochs = 1,\n                         validation_data = validation_generator,\n                         validation_steps = 624)","457d3d2b":"test_accu = cnn.evaluate_generator(test_set,steps=624)\n","386e57fa":"print('The testing accuracy is :',test_accu[1]*100, '%')","9c9fc64e":"Y_pred = cnn.predict_generator(test_set, 100)\ny_pred = np.argmax(Y_pred, axis=1)\n# confusion_matrix(validation_generator.classes, y_pred)\n","0394e964":"max(y_pred)","3892f669":"# Accuracy \nplt.plot(cnn_model.history['acc'])\nplt.plot(cnn_model.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training set', 'Validation set'], loc='upper left')\nplt.show()\n","cb8d088d":"# Loss \n\nplt.plot(cnn_model.history['val_loss'])\nplt.plot(cnn_model.history['loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training set', 'Test set'], loc='upper left')\nplt.show()","8be82237":"# <font color='red'> What this means? <\/font>\nThe world of medical imaging is ripe for a revolution in terms of deploying CNN based technologies. There is no need for a doctor or a health care provider to ponder these images to gauge things. This is not a shot at doctors or their jobs! The task of reading these is incredibly menial and repititive. Those are two things that AI technologies are great at. I will be working with some more bio-medical imaging dataset just to get more practice and probably create some web applications online for atleast one of them!!","d5eaca16":"# <font color='purple'> Let's take a look at the Confusion matrix and the Classification Report. <\/font>","30349b49":"# <font color='purple'>The purpose of this exercise is to see how accurate of a Neural Network we can create to classify X-Ray scans from patients with pneumonia. <\/font>\n\n![https:\/\/media.giphy.com\/media\/l0MYtoPNAQ0YCgNVe\/giphy.gif](https:\/\/media.giphy.com\/media\/l0MYtoPNAQ0YCgNVe\/giphy.gif)","edf578ad":"# <font color='purple'>CONVOLUTIONAL NEURAL NETWORKS TO THE RESCUE<\/font>\nA Convolutional Neural Network is a special type of an Artificial Intelligence implementation which uses a  special mathematical matrix manipulation called the convolution operation to process data from the images.\n\n* A **convolution** does this by multiplying two matrices and yielding a third, smaller matrix. \n* The Network takes an input image, and uses a filter **(or kernel)**  to create a **feature map** describing the image. \n* In the convolution operation, we take a filter (usually 2x2 or 3x3 matrix ) and **slide** it over the image matrix. The coresponding numbers in both matrices are multiplied and and added to yield a single number describing that input space. This process is repeated all over the image.This can be seen in the following animation\n![https:\/\/cdn-images-1.medium.com\/max\/800\/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif](https:\/\/cdn-images-1.medium.com\/max\/800\/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif)                                     \n\n-------> This is a 2-D representation of calculations happening in 3 dimensions. This is what is actually happening \n![Source: https:\/\/cdn-images-1.medium.com\/max\/800\/1*EuSjHyyDRPAQUdKCKLTgIQ.png](https:\/\/cdn-images-1.medium.com\/max\/800\/1*EuSjHyyDRPAQUdKCKLTgIQ.png)      \n\n* We use different filters to pass over our inputs, and take all the feature maps, put them together as the final output of the convolutional layer.\n* We then pass the output of this layer through a non-linear activation function. The most commonly used one is ReLU. \n* The next step of our process involves further reducing the dimensionality of the data which will lower the computation power required for training this model. This is      achieved by using a **Pooling Layer.** The most commonly used one is **max pooling** which takes the maximum value in the window created by a filter. This              significantly reduces the training time and preserves significant information. \n![https:\/\/cdn-images-1.medium.com\/max\/800\/1*vbfPq-HvBCkAcZhiSTZybg.png](https:\/\/cdn-images-1.medium.com\/max\/800\/1*vbfPq-HvBCkAcZhiSTZybg.png)\n\nTwo of the most commonly associated words with CNN are **stride** and **padding **\n\n**STRIDE:** Stride just means the amount a filter moves during a covolution operation. So, a stride of 1 means that the filter will slide 1 pixel after each covolution operation as shown in this animation.\n![https:\/\/cdn-images-1.medium.com\/max\/800\/0*iqNdZWyNeCr5tCkc.](https:\/\/cdn-images-1.medium.com\/max\/800\/0*iqNdZWyNeCr5tCkc.)\n\n**PADDING:** Padding is just zero value pixels that surround the input image. This protects the loss of any valuable information since the feature map is ever shrinking. \n\n*This section was inspired by this post: https:\/\/medium.freecodecamp.org\/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050*\n\n*The gifs\/animations are from this post: https:\/\/towardsdatascience.com\/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2*\n","f42d74d6":"## Let's take a look at some of the pictures.\n\n","ba8b28ee":" ## <font color='purple'>  Our data is located in three folders:<\/font>\n   \n1. train= contains the training data\/images for teaching our model.\n2.  val= contains images which we will use to validate our model. The purpose of this data set is to prevent our model from **Overfitting**. Overfitting is when your model gets a little too comofortable with the training data and can't handle data it hasn't see....too well.\n3. test = this contains the data that we use to test the model once it has learned the relationships between the images and their label (Pneumonia\/Not-Pneumonia)\n\n       \n    \n    ","45e93392":"# <font color='green'>PHEW!!! So we just built a CNN that can predict penumonia from X-Ray scans !!! WOOOOO<\/font>\n\n\n![https:\/\/media.giphy.com\/media\/3ov9jG4eqz9k3XXsU8\/giphy.gif](https:\/\/media.giphy.com\/media\/3ov9jG4eqz9k3XXsU8\/giphy.gif)","2e3ffc0c":"# This summary is a great way for us to see how our CNN is being set up","32d8e1bf":"**The above pictures are being generated randomly from the dataset. There are slight differences in the pictures, but  let's see if we can't create a Convolutional Neural Network which can create a more objective measure of quantifying and classifying these differences. **","e11624b0":"# <font color ='purple'>Convolutional Neural Networks are easy to build and run because of the deep learning libraries such as Keras<\/font>\n* **Keras** is a high level deep learning library. \n* It runs on top of more low level libraries such as Tensorflow which is why you will usually see that message. 'Using TensorFlow backend.' display when you call Keras.\n* I am going to build a CNN that has 2 Convolution layers. After each convolution, I will add a Pooling layer using max-pooling. \n* After the second convolutional layer, we will add 2 Fully Connected layers which will take out feature map of an image and make predictions about it. The second FC  \n   has only 1 unit becuase it needs to predict if someone has pneumonia. \n * When we compile this CNN, we will use the Adam optimization fuction which optimizes how fast our model learns the correct classification of the image.                         Optimization functions are far too complex for me to get into in this post. \n","10df0dea":"# <font color ='purple'>Let's start by Importing the required libraries <\/font>","be4a5af1":"## Let's set up the training and testing folders.\n","7d04a12c":"# <font color='purple'>Now, we are going to fit the model to our training dataset and we will keep out testing dataset seperate <\/font>","a91c5db5":"# <font color='purple'>Now, let's see how accuracy changed over our training and Validation Set and also how our Loss function changed!!<\/font>","1ecbedef":"# <font color='red'> Have you ever wondered HOW and WHAT a computer sees?<\/font>\n\nThe answer to that lies in mammilian physiology. The way we mammals and almost any other animal with two eyes and a developed brain sees is something as follows:\n1. Rays of lights reflected off an object hit your retina.\n2. The sensory input is transported to your brain.\n3. The brain, after interpreting the input, tells you what it sees. \n\nAlso, recall that your family was there to teach you what things are. You know something is an umbrella because you look at it and someone told you that this specific object is called an umbrella. Your brain, then stores that association and you don't have to be taught this again. \n\n**Computers,** learn\/see in not such a different way from us. They will, however, need to look and analyze thousands upon thousands of images before they can generalize and say that a yellow umbrella falls under the same category as a black umbrella. This is because what they see are not pictures, but numerical representaions of pixels describing these pictures. So while we see **'things** in pictures, a computer sees this\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*cot55wd6gdoJlovlCw0AAQ.png)\n\n\n![https:\/\/media.giphy.com\/media\/ChzfTLSi47FYc\/giphy.gif](https:\/\/media.giphy.com\/media\/ChzfTLSi47FYc\/giphy.gif)\n\n\n","a319848c":"# <font color ='purple'> Alright.... I don't know about you, but I am no X-Ray reading wizard, but these don't look too off to me. <\/font>\n\nThere are some obvious differences in the rib cage shape of the patient who has pneumonia vs someone who is normal. However, other than that I can't tell anything else. \n\nThere are two ways, we can resolve this problem:\n\n1. Go to Med-school and learn the skills that are necessary to read these figures and make educated guesses as to what they mean. \n\n2. Not go $250,000 in debt and train a Convolutional Nerual Network to tell me the differnces...\n\n# Soooo.....\n![https:\/\/media.giphy.com\/media\/l396ZJAsqbN5VX3O0\/giphy.gif](http:\/\/media.giphy.com\/media\/l396ZJAsqbN5VX3O0\/giphy.gif)\n\n\n","de6b2c34":"# <font color='purple'> There you go....all caught up. No need to be scared. If this looks intimidating don't worry, it is pretty easy to implement. <\/font>\n\n![https:\/\/media.giphy.com\/media\/l3q2UyW34cT2rcgko\/giphy.gif](https:\/\/media.giphy.com\/media\/l3q2UyW34cT2rcgko\/giphy.gif)\n\n"}}