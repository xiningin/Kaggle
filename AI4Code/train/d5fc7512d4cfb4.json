{"cell_type":{"d89bdc9a":"code","3a99d1ad":"code","7a29d0c0":"code","f255eea7":"code","8fedac84":"code","61557d88":"code","d1300cf6":"code","2871e320":"code","6520aeb6":"code","e82f247f":"code","0bd90e8f":"code","7612e884":"code","3b2110e9":"code","8ae0f723":"code","8cb7b2ca":"code","d08a6e70":"code","c79c27d6":"code","98ba18d3":"code","2c0e2e7a":"code","63e67abc":"code","34c00d40":"code","a1c9688e":"code","ae96e756":"code","81c002a7":"code","9c543505":"code","9423b5ea":"code","6e0de526":"code","818a022d":"code","1e83222b":"code","1b3cf037":"code","cdbd8c7c":"markdown","5f75664b":"markdown","dc6402b4":"markdown","ac9a30c8":"markdown","7904b646":"markdown","a095205b":"markdown","74b6f74b":"markdown","65c06039":"markdown","2d77fade":"markdown","458d9ebe":"markdown","6e246c8b":"markdown","483b067c":"markdown","e27c3c06":"markdown","5229c483":"markdown","380a9a10":"markdown","a159c52f":"markdown","a654232a":"markdown","dd66f8da":"markdown","45f415bd":"markdown","fc160527":"markdown","55de96e3":"markdown","0bfcbd5c":"markdown","79a77a69":"markdown","2ddd3f37":"markdown","c5dbe6da":"markdown","27129b8b":"markdown","bf21d1a3":"markdown","68f9e90c":"markdown","fdb05ad6":"markdown","9f8b76dc":"markdown","eb9c2f34":"markdown","e6b3a178":"markdown","7cdde5ca":"markdown"},"source":{"d89bdc9a":"! pip install scipy==1.1.0","3a99d1ad":"# from google.colab import drive\n# drive.mount('\/content\/gdrive\/')","7a29d0c0":"import sys\nimport os\nprefix = '\/content\/gdrive\/My Drive\/'\n# modify \"customized_path_to_your_homework\" here to where you uploaded your homework\ncustomized_path_to_your_homework = 'ECE 5424 ML\/homework_spring\/HW4'\nsys_path = os.path.join(prefix, customized_path_to_your_homework)\nsys.path.append(sys_path)","f255eea7":"#Switching to GPU\nimport torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","8fedac84":"import torch.nn as nn\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random Tensors to hold inputs and outputs\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# Use the nn package to define our model as a sequence of layers. nn.Sequential\n# is a Module which contains other Modules, and applies them in sequence to\n# produce its output. Each Linear Module computes output from input using a\n# linear function, and holds internal Tensors for its weight and bias.\nmodel = nn.Sequential(\n    nn.Linear(D_in, H),\n    nn.ReLU(),\n    nn.Linear(H, D_out),\n)\n\n# The nn package also contains definitions of popular loss functions; in this\n# case we will use Mean Squared Error (MSE) as our loss function.\nloss_fn = nn.MSELoss(reduction='sum')\n\nlearning_rate = 1e-4\nfor t in range(500):\n    # Forward pass: compute predicted y by passing x to the model. Module objects\n    # override the __call__ operator so you can call them like functions. When\n    # doing so you pass a Tensor of input data to the Module and it produces\n    # a Tensor of output data.\n    y_pred = model(x)\n\n    # Compute and print loss. We pass Tensors containing the predicted and true\n    # values of y, and the loss function returns a Tensor containing the\n    # loss.\n    loss = loss_fn(y_pred, y)\n    if t % 100 == 99:\n        print(f'iteration {t}: {loss.item()}')\n\n    # Zero the gradients before running the backward pass.\n    model.zero_grad()\n\n    # Backward pass: compute gradient of the loss with respect to all the learnable\n    # parameters of the model. Internally, the parameters of each Module are stored\n    # in Tensors with requires_grad=True, so this call will compute gradients for\n    # all learnable parameters in the model.\n    loss.backward()\n\n    # Update the weights using gradient descent. Each parameter is a Tensor, so\n    # we can access its gradients like we did before.\n    with torch.no_grad():\n        for param in model.parameters():\n            param -= learning_rate * param.grad","61557d88":"N, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random Tensors to hold inputs and outputs\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# Use the nn package to define our model and loss function.\nmodel = nn.Sequential(\n    nn.Linear(D_in, H),\n    nn.ReLU(),\n    nn.Linear(H, D_out),\n)\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\n# Use the optim package to define an Optimizer that will update the weights of\n# the model for us. \nlearning_rate = 1e-4\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\nfor t in range(500):\n    # Forward pass: compute predicted y by passing x to the model.\n    y_pred = model(x)\n\n    # Compute and print loss.\n    loss = loss_fn(y_pred, y)\n    if t % 100 == 99:\n        print(f'iteration {t}: {loss.item()}')\n\n    # Before the backward pass, use the optimizer object to zero all of the\n    # gradients for the variables it will update (which are the learnable\n    # weights of the model). This is because by default, gradients are\n    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n    # is called. Checkout docs of torch.autograd.backward for more details.\n    optimizer.zero_grad()\n\n    # Backward pass: compute gradient of the loss with respect to model\n    # parameters\n    loss.backward()\n\n    # Calling the step function on an Optimizer makes an update to its\n    # parameters\n    optimizer.step()","d1300cf6":"class TwoLayerNet(nn.Module):\n    def __init__(self, D_in, H, D_out):\n        \"\"\"\n        In the constructor we instantiate two nn.Linear modules and assign them as\n        member variables.\n        \"\"\"\n        super(TwoLayerNet, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Tensor of input data and we must return\n        a Tensor of output data. We can use Modules defined in the constructor as\n        well as arbitrary operators on Tensors.\n        \"\"\"\n        h_relu = self.linear1(x).clamp(min=0)\n        y_pred = self.linear2(h_relu)\n        return y_pred\n\n\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random Tensors to hold inputs and outputs\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# Construct our model by instantiating the class defined above\nmodel = TwoLayerNet(D_in, H, D_out)\n\n# Construct our loss function and an Optimizer. The call to model.parameters()\n# in the SGD constructor will contain the learnable parameters of the two\n# nn.Linear modules which are members of the model.\ncriterion = nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\nfor t in range(500):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y)\n    if t % 100 == 99:\n        print(f'iteration {t}: {loss.item()}')\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()","2871e320":"import numpy as np\nimport matplotlib.pyplot as plt\n\ncurrent_path = '..\/input\/assignmentds'\nX_train = np.loadtxt(current_path + '\/X1_train.csv', delimiter=',')\nX_test = np.loadtxt(current_path + '\/X1_test.csv', delimiter=',')\ny_train = np.loadtxt(current_path + '\/y1_train.csv', delimiter=',')\ny_test = np.loadtxt(current_path + '\/y1_test.csv', delimiter=',')\n\n# Plot it to see why is it called two-moon dataset\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train);","6520aeb6":"from torch.utils.data import TensorDataset, DataLoader\nbatch_size = 64 # mini-batch size\nnum_workers = 4 # how many parallel workers are we gonna use for reading data\nshuffle = True # shuffle the dataset\n\n# Convert numpy array import torch tensor\nX_val = torch.FloatTensor(X_train[500:])\nX_train = torch.FloatTensor(X_train[:500])\nX_test = torch.FloatTensor(X_test)\ny_val = torch.LongTensor(y_train.reshape(-1, 1)[500:])\ny_train = torch.LongTensor(y_train.reshape(-1, 1)[:500])\ny_test = torch.LongTensor(y_test.reshape(-1, 1))\n\n# First, create a dataset from torch tensor. A dataset define how to read data\n# and process data for creating mini-batches.\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, \n                          num_workers=num_workers, shuffle=shuffle)","e82f247f":"epoch = 5 # an epoch means looping through all the data in the datasets\nlr = 1e-1\n\n# create a simple model that is probably not gonna work well\nmodel = nn.Linear(X_train.size(1), 1)\nmodel.to(device)\noptim = torch.optim.SGD(model.parameters(), lr=lr)\n\nfor e in range(epoch):\n    loss_epoch = 0\n    # loop through train loader to get x and y\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optim.zero_grad()\n        y_pred = model(x)\n        # !!WARNING!!\n        # THIS IS A CLASSIFICATION TASK, SO YOU SHOULD NOT\n        # USE THIS LOSS FUNCTION. \n        loss = (y_pred - y.float()).abs().mean()\n        loss.backward()\n        optim.step()\n        loss_epoch += loss.item()\n    print(f'Epcoh {e}: {loss_epoch}')","0bd90e8f":"class SimpleNN(nn.Module):\n    \n    def __init__(self, D_in, H, D_out):\n        super(SimpleNN, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, H)\n        self.linear3 = nn.Linear(H, D_out) \n        \n        \n        ################################################################################\n        # TODO:                                                                        #\n        # Construct your small feedforward NN here.                                    #\n        ################################################################################\n        \n        ################################################################################\n        #                                 END OF YOUR CODE                             #\n        ################################################################################\n        \n    def forward(self, x):\n        a1 = nn.ReLU()\n        h1 = a1(self.linear1(x))\n        a2 =  nn.ReLU()\n        h2 = a2(self.linear2(h1))\n        a3 = nn.Sigmoid()\n        y_pred = a3(self.linear3(h2))\n        return y_pred\n        ################################################################################\n        # TODO:                                                                        #\n        # feed the input to your network, and output the predictions.                  #\n        ################################################################################\n        \n        ################################################################################\n        #                                 END OF YOUR CODE                             #\n        ################################################################################","7612e884":"# helper function for computing accuracy\ndef get_acc(pred, y):\n    pred = pred.float()\n    y = y.float()\n    return (y==pred).sum().float()\/y.size(0)*100.","3b2110e9":"def Solver(model, train_loader, optim, criterion, epoch=501, lr=1e-1, print_every=20):\n    '''\n    The solver function for training your model\n\n    model: your designed model\n    train_loader: data loader for training data\n    optim: SGD optimizer\n    criterion: criterion for calculating loss, i.e. nn.BCELoss\n    epoch: number of training epochs, an epoch means looping through all the data in the datasets\n    lr: training learning rate\n    print_every: number of epochs to print out loss and accuracies\n      '''\n    lossd = nn.BCELoss()\n    for e in range(epoch):\n        loss_epoch = 0\n        for x, y in train_loader:\n            optim.zero_grad()\n            y_pred = model(x)\n#             print(y_pred, y)\n            loss = lossd(y_pred, y.float())\n            loss.backward()\n            optim.step()\n            loss_epoch += loss.item()\n    ################################################################################\n    # TODO:                                                                        #\n    # Loop through the dataloader and train your model with nn.BCELoss.            #\n    ################################################################################\n\n    ################################################################################\n    #                                 END OF YOUR CODE                             #\n    ################################################################################\n        if e % print_every == 0:\n            y_pred = (model(X_train) > 0.5)\n            train_acc = get_acc(y_pred, y_train)\n            y_val_pred = (model(X_val) > 0.5)\n            val_acc = get_acc(y_val_pred, y_val)  \n            print(f'Epcoh {e}: {loss_epoch}, Training accuracy: {train_acc}, Validation accuracy: {val_acc}')\n    return model","8ae0f723":"# create a simple model that is probably not gonna work well\nepoch=20\nlr=1e-1\n\n\n\n################################################################################\n# TODO:                                                                        #\n# Initialize your model and SGD optimizer here.  \nmodel = SimpleNN(X_train.size(1),20, 1)\noptim = torch.optim.SGD(model.parameters(), lr=lr)\n#################################################################################\n\n################################################################################\n#                                 END OF YOUR CODE                             #\n################################################################################\n\n# train your model\nmodel = Solver(model, train_loader, optim, criterion, epoch=epoch, lr=lr, print_every=2)","8cb7b2ca":"y_pred = (model(X_train) > 0.5)\ntrain_acc = get_acc(y_pred, y_train)\n\ny_val_pred = (model(X_val) > 0.5)\nval_acc = get_acc(y_val_pred, y_val)  \n\ny_pred = (model(X_test) > 0.5)\ntest_acc = get_acc(y_pred, y_test)\nprint(f'Training accuracy: {train_acc}, Validation accuracy: {val_acc}, Testing accuracy: {test_acc}')","d08a6e70":"import torchvision\nimport torchvision.transforms as transforms\n\n# Preprocessing steps on the training\/testing data. You can define your own data augmentation\n# here, and PyTorch's API will do the rest for you.\ntransform_train = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ncurrent_path = prefix + customized_path_to_your_homework\n# This will automatically download the dataset for you\ntrainset = torchvision.datasets.CIFAR10(root=current_path , train=True, download=True, transform=transform_train)\ntestset = torchvision.datasets.CIFAR10(root=current_path , train=False, download=True, transform=transform_test)","c79c27d6":"class SimpleNN(nn.Module):\n    \n    def __init__(self, D_in, D_out):\n        super().__init__()\n        self.linear1 = nn.Linear(D_in, 100)\n        self.linear2 = nn.Linear(100, 100)\n        self.linear3 = nn.Linear(100, D_out) \n        ################################################################################\n        # TODO:                                                                        #\n        # Construct your small feedforward NN here.                                    #\n        ################################################################################\n        \n        ################################################################################\n        #                                 END OF YOUR CODE                             #\n        ################################################################################\n        \n    def forward(self, x):\n        # note that: here, the data is of the shape (B, C, H, W)\n        # where B is the batch size, C is color channels, and H\n        # and W is height and width.\n        # To feed it into the linear layer, we need to reshape it\n        # with .view() function.\n        batch_size = x.size(0)\n        x = x.view(batch_size, -1) # reshape the data from (B, C, H, W) to (B, C*H*W)\n#         print(x.shape)\n        a1 = nn.ReLU()\n        h1 = a1(self.linear1(x))\n        a2 =  nn.ReLU()\n        h2 = a2(self.linear2(h1))\n        a3 = nn.Softmax()\n        y_pred = a3(self.linear3(h2))\n        return y_pred\n        ################################################################################\n        # TODO:                                                                        #\n        # Forward pass, output the prediction score.                                   #\n        ################################################################################\n        \n        ################################################################################\n        #                                 END OF YOUR CODE                             #\n        ################################################################################","98ba18d3":"epoch = 15\nlr = 1e-2\nn_input = 3072\nn_classes = 10\n\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=50, shuffle=False, num_workers=2)\n\n\ndef ohe(x):\n    ans = [[0]*10 for _ in range(x.shape[0])]    \n    for i in range(len(x)):\n        ans[i][x[i]] = 1\n    return ans\n        \n################################################################################\n# TODO:                                                                        #\n# Initialize your model and SGD optimizer here.  \nmodel = SimpleNN(n_input,n_classes)\nmodel.to(device)\noptim = torch.optim.SGD(model.parameters(), lr=lr)\n#################################################################################\n\n################################################################################\n#                                 END OF YOUR CODE                             #\n################################################################################\n\n# train your model\n# model = Solver(model, train_loader, optim, criterion, epoch=epoch, lr=lr, print_every=2)\nlossd = nn.BCELoss()\nfor e in range(epoch):\n    loss_epoch = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n#         print(y)\n        optim.zero_grad()\n        y_pred = model(x)\n        y = torch.Tensor(ohe(y)).to(device)\n#         print(y.is_cuda, y_pred.is_cuda)\n#         print(y)\n#             print(y_pred, y)\n#         Y_train_enc = ohe.transform(y)\n        loss = lossd(y_pred, y)\n        loss.backward()\n        optim.step()\n        loss_epoch += loss.item()\n    print(loss_epoch)\n\n\n\n\n################################################################################\n# TODO:                                                                        #\n# Your training code here. #\n\n\n\n################################################################################\n    \n################################################################################\n#                                 END OF YOUR CODE                             #\n################################################################################\n","2c0e2e7a":"def get_model_acc(model, loader):\n    ys = []\n    y_preds = []\n    for x, y in loader:\n        x,y = x.to(device), y.to(device)\n        ys.append(y)\n        # set the prediction to the one that has highest value\n        # Note that the the output size of model(x) is (B, 10)\n        y_preds.append(torch.argmax(model(x), dim=1))\n    y = torch.cat(ys, dim=0)\n    y_pred = torch.cat(y_preds, dim=0)\n    print((y == y_pred).sum())\n    return get_acc(y_pred, y)","63e67abc":"train_acc = get_model_acc(model, train_loader)\ntest_acc = get_model_acc(model, test_loader)\nprint(f'Training accuracy: {train_acc}, Testing accuracy: {test_acc}')","34c00d40":"import torch.nn.functional as F\nclass CNNClassifier(nn.Module):\n    \n    def __init__(self, D_in, D_out):\n        super(CNNClassifier, self).__init__()\n        self.c1 = nn.Conv2d(D_in, 100,3)\n        self.c2 = nn.Conv2d(100, 100,3)\n        self.c3 = nn.Conv2d(100, 100,3)\n        self.l1 = nn.Linear(100*2*2, D_out)\n                \n    def forward(self, x):\n        a1 = nn.ReLU()\n        a2 = nn.ReLU()\n        a3 = nn.ReLU()\n        a4 = nn.Softmax()\n\n        v1 = F.max_pool2d(a1(self.c1(x)),2)\n        v2 = F.max_pool2d(a2(self.c2(v1)),2)\n        v3 = F.max_pool2d(a3(self.c3(v2)),2)\n        sha = v3.shape[1]*v3.shape[2]*v3.shape[3]\n        v4 = a4(self.l1(v3.view(x.shape[0], sha)))\n        \n        return v4","a1c9688e":"# You can tune these hyperparameters as you like.\nepoch = 10\nlr = 1e-1\nn_input = 3072\nn_classes = 10\nbatch_size = 64\nnum_workers = num_workers\n\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n\n\nmodel = CNNClassifier(3,n_classes)\nmodel.to(device)\noptim = torch.optim.Adam(model.parameters(), lr=0.001)\n#################################################################################\n\n################################################################################\n#                                 END OF YOUR CODE                             #\n################################################################################\n\n# train your model\n# model = Solver(model, train_loader, optim, criterion, epoch=epoch, lr=lr, print_every=2)\nlossd = nn.BCELoss()\nfor e in range(epoch):\n    loss_epoch = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n#         print(y)\n        optim.zero_grad()\n        y_pred = model(x)\n        y = torch.FloatTensor(ohe(y)).to(device)\n#         print(y)\n#         print(y_pred, y)\n#         Y_train_enc = ohe.transform(y)\n        loss = lossd(y_pred,  y)\n        loss.backward()\n        optim.step()\n        loss_epoch += loss.item()\n    print(loss_epoch)","ae96e756":"# turn on evaluation mode. This is crucial when you have BatchNorm in your network,\n# as you want to use the running mean\/std you obtain durining training time to normalize\n# your input data. Rememeber to call .train() function after evaluation\nmodel.eval()\ntrain_acc = get_model_acc(model, train_loader)\ntest_acc = get_model_acc(model, test_loader)\nprint(f'Training accuracy: {train_acc}, Testing accuracy: {test_acc}')","81c002a7":"import torch.nn.functional as F\nimport time\n\ntime_d = {} \n\nclass CNNClassifier(nn.Module):\n    \n    def __init__(self, D_in, D_out, l):\n        super(CNNClassifier, self).__init__()\n        self.layers = l\n        self.c1 = nn.Conv2d(D_in, 100,3)\n        self.c2 = nn.Conv2d(100, 100,3)\n        self.c3 = nn.Conv2d(100, 100,3)\n        self.c4 = nn.Conv2d(100, 100,3)\n        if(l == 1):self.l1 = nn.Linear(100*30*30, D_out)\n        if(l == 2):self.l1 = nn.Linear(100*28*28, D_out)\n        if(l == 3):self.l1 = nn.Linear(100*26*26, D_out)\n        if(l == 4):self.l1 = nn.Linear(100*24*24, D_out)\n        \n        ################################################################################\n        # TODO:                                                                        #\n        # Construct a CNN with 2 or 3 convolutional layers and 1 linear layer for      #\n        # outputing class prediction. You are free to pick the hyperparameters         #\n        ################################################################################\n            \n        # Enter your code here.                                                        #\n\n\n        # End of your code.                                                            #\n        ################################################################################\n        \n        \n    def forward(self, x):\n        \n        \n        a1 = nn.ReLU()\n        a2 = nn.ReLU()\n        a3 = nn.ReLU()\n        a4 = nn.ReLU()\n        a4 = nn.Softmax()\n        if(self.layers>=1):\n            v1 =a1(self.c1(x))\n#             print(v1.shape)\n        if(self.layers>=2):\n            v1 = a2(self.c2(v1))\n#             print(v1.shape)\n        if(self.layers>=3):\n            v1= a3(self.c3(v1))\n#             print(v1.shape)\n        if(self.layers>=4):\n            v1 = a4(self.c4(v1))\n#             print(v1.shape)\n        sha = v1.shape[1]*v1.shape[2]*v1.shape[3]\n        v4 = a4(self.l1(v1.view(x.shape[0], sha)))\n        \n        return v4\n    \n    \n    \n# You can tune these hyperparameters as you like.\nepoch = 10\nlr = 1e-1\nn_input = 3072\nn_classes = 10\nbatch_size = 64\nnum_workers = num_workers\n\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n\n################################################################################\n# TODO:                                                                        #\n# Enter your code here.                                                        #\n\n\n# End of your code.                                                            #\n################################################################################\n\n\n\nacc_train = {}\nacc_test = {}\nfor layers in range(1,5):\n    start_time = time.time()\n    \n\n    model = CNNClassifier(3,n_classes, layers)\n    model.to(device)\n    optim = torch.optim.SGD(model.parameters(), lr=lr)\n    #################################################################################\n\n    ################################################################################\n    #                                 END OF YOUR CODE                             #\n    ################################################################################\n\n    # train your model\n    # model = Solver(model, train_loader, optim, criterion, epoch=epoch, lr=lr, print_every=2)\n    lossd = nn.BCELoss()\n    for e in range(epoch):\n        loss_epoch = 0\n        for x, y in train_loader:\n            x, y = x.to(device), y.to(device)\n    #         print(y)\n            optim.zero_grad()\n            y_pred = model(x)\n            y = torch.FloatTensor(ohe(y)).to(device)\n    #         print(y)\n    #         print(y_pred, y)\n    #         Y_train_enc = ohe.transform(y)\n            loss = lossd(y_pred,  y)\n            loss.backward()\n            optim.step()\n            loss_epoch += loss.item()\n    time_d[layers]= time.time() - start_time\n    model.eval()\n    train_acc = get_model_acc(model, train_loader)\n    test_acc = get_model_acc(model, test_loader)\n    acc_train[layers] = train_acc\n    acc_test[layers] = test_acc\n    \n    \nimport matplotlib.pyplot as plt\nlists = sorted(acc_train.items()) # sorted by key, return a list of tuples\nx1, y1 = zip(*lists) \nplt.plot(x1, y1, label = \"Accuracy_train\")\nlists2 = sorted(acc_test.items())\nx2, y2 = zip(*lists2)\nplt.plot(x2, y2, label = \"Accuracy_test\")\nplt.xlabel('Number of convolutional layers')\nplt.ylabel('Accuracy')\nplt.title('Comparision of 4 network architectures with different numbers of convolutional layers')\nplt.xticks([1,2,3,4])\nplt.legend()\nplt.show()","9c543505":"print(\"Time for each layer\", time_d)","9423b5ea":"import torch.nn.functional as F\nclass CNNClassifier(nn.Module):\n    \n    def __init__(self, D_in, D_out):\n        super(CNNClassifier, self).__init__()\n        self.c1 = nn.Conv2d(D_in, 100,3)\n        self.c2 = nn.Conv2d(100, 100,3)\n        self.c3 = nn.Conv2d(100, 100,3)\n        self.l1 = nn.Linear(100*2*2, D_out)\n                \n    def forward(self, x):\n        a1 = nn.ReLU()\n        a2 = nn.ReLU()\n        a3 = nn.ReLU()\n        a4 = nn.Softmax()\n\n        v1 = F.max_pool2d(a1(self.c1(x)),2)\n        v2 = F.max_pool2d(a2(self.c2(v1)),2)\n        v3 = F.max_pool2d(a3(self.c3(v2)),2)\n        sha = v3.shape[1]*v3.shape[2]*v3.shape[3]\n        v4 = a4(self.l1(v3.view(x.shape[0], sha)))\n        \n        return v4\n    \n    \n# You can tune these hyperparameters as you like.\nepoch = 10\nlr = 0.001\nn_input = 3072\nn_classes = 10\nbatch_size = 64\nnum_workers = num_workers\n\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n\n\n\n#################################################################################\n\n################################################################################\n#                                 END OF YOUR CODE                             #\n################################################################################\n\n# train your model\n# model = Solver(model, train_loader, optim, criterion, epoch=epoch, lr=lr, print_every=2)\nlossd = nn.BCELoss()\noptim_list = [torch.optim.RMSprop , torch.optim.Adam]\noptim_list_string = ['RMSprop', 'Adam']\nfor i in range(len(optim_list)):\n    model = CNNClassifier(3,n_classes)\n    model.to(device)\n    optim = torch.optim.RMSprop(model.parameters(), lr=0.001)\n    for e in range(epoch):\n        loss_epoch = 0\n        for x, y in train_loader:\n            x, y = x.to(device), y.to(device)\n    #         print(y)\n            optim.zero_grad()\n            y_pred = model(x)\n            y = torch.FloatTensor(ohe(y)).to(device)\n    #         print(y)\n    #         print(y_pred, y)\n    #         Y_train_enc = ohe.transform(y)\n            loss = lossd(y_pred,  y)\n            loss.backward()\n            optim.step()\n            loss_epoch += loss.item()\n    model.eval()\n    train_acc = get_model_acc(model, train_loader)\n    test_acc = get_model_acc(model, test_loader)\n    #     acc_train[layers] = train_acc\n    #     acc_test[layers] = test_acc\n    print('For',optim_list_string[i], 'Training accuracy = ', train_acc.item(), 'Testing accuracy',test_acc.item() )   \n","6e0de526":"class Best(nn.Module):   \n    def __init__(self):\n        super(Net, self).__init__()\n        self.c1 = Conv2d(1, 4, kernel_size=3, stride=1, padding=1)\n        self.b1 = BatchNorm2d(4)\n        self.c2 = Conv2d(4, 4, kernel_size=3, stride=1, padding=1)\n        self.b2 = BatchNorm2d(4)\n        self.l1 = nn.Linear(4 * 7 * 7, 10)\n\n    # Defining the forward pass    \n    def forward(self, x):\n        x = self.c1(x)\n        x = self.b1(x)\n        x = nn.ReLU(x)\n        x = F.max_pool2d(x, 2)\n        x = self.c2(x)\n        x = self.b2(x)\n        x = nn.ReLU(x)\n        x = F.max_pool2d(x, 2)\n        x = self.cnn_layers(x)\n        x = x.view(x.size(0), -1)\n        x = self.l1(x)\n        return x\n# You can tune these hyperparameters as you like.\nepoch = 15\nlr = 1e-1\nn_input = 3072\nn_classes = 10\nbatch_size = 64\nnum_workers = num_workers\n\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n\n\nmodel = CNNClassifier(3,n_classes)\nmodel.to(device)\noptim = torch.optim.Adam(model.parameters(), lr=0.001)\n\n\n# train your model\n# model = Solver(model, train_loader, optim, criterion, epoch=epoch, lr=lr, print_every=2)\nlossd = nn.BCELoss()\nfor e in range(epoch):\n    loss_epoch = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n#         print(y)\n        optim.zero_grad()\n        y_pred = model(x)\n        y = torch.FloatTensor(ohe(y)).to(device)\n#         print(y)\n#         print(y_pred, y)\n#         Y_train_enc = ohe.transform(y)\n        loss = lossd(y_pred,  y)\n        loss.backward()\n        optim.step()\n        loss_epoch += loss.item()\n    print(loss_epoch)","818a022d":"# turn on evaluation mode. This is crucial when you have BatchNorm in your network,\n# as you want to use the running mean\/std you obtain durining training time to normalize\n# your input data. Rememeber to call .train() function after evaluation\nmodel.eval()\ntrain_acc = get_model_acc(model, train_loader)\ntest_acc = get_model_acc(model, test_loader)\nprint(f'Training accuracy: {train_acc}, Testing accuracy: {test_acc}')","1e83222b":"# #Run below two lines (in google colab), installation steps to get .pdf of the notebook\n\n# !apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n# !pip install pypandoc\n\n# # (optional) After installation, comment above two lines and run again to remove installation comments from the notebook.","1b3cf037":"# # Find path to your notebook file in drive and enter in below line\n# !jupyter nbconvert --to PDF \"\/content\/gdrive\/MyDrive\/ECE 5424 ML\/homework_spring\/HW4\/HW4.ipynb\"\n# #Example: \"\/content\/gdrive\/MyDrive\/ECE 5424 ML\/homework_spring\/HW4\/HW4.ipynb\"","cdbd8c7c":"# Save as PDF","5f75664b":"### 3.2.4 Improve Your Model [10 pts]\n\nAgain, we want you to play with your model a bit harder, and improve it. \n**Get at least 70% accuracy on testing set. [10 pts]**\n\n* You are free to use everything you can find in the documents (`BatchNorm`, `SeLU`, etc), as long as it is not a **predefined network architectures in PyTorch package**. \n* You can also implement some famous network architectures to push the performance. \n\n(A simple network with 5-6 `nn.Conv2d` can give you at least 70% accuracy on testing set).","dc6402b4":"# Section 1 Pytorch basics","ac9a30c8":"Append the directory to your python path using sys.\n\nPlease do modify the `customized_path_to_your_homework` to where you uploaded your homework in the Google Drive","7904b646":"# ECE-5424 \/ CS-5824 Advanced Machine Learning\n# Assignment 4: Fully-Connected Networks and Convoluntional Neural Networks [85 pts]","a095205b":"Now, let's create a PyTorch `DataLoader`:","74b6f74b":"<span style=\"color:red\">**Briefly explain what you have observed in three or four sentences. Does stacking layers always give you better results? How about the computational time?:**<\/span> **[2 pts]**","65c06039":"<span style=\"color:red\">**Explain your design and hyperparameter choice in three or four sentences:**<\/span> **[2 pts]**","2d77fade":"# Section 0 I have used jupyter notebook instead of google colab","458d9ebe":"### Please Write Your VT PID Here: dhirajsrivastava","6e246c8b":"Sometimes you will want to specify models that are more complex than a sequence of existing Modules; for these cases you can define your own Modules by subclassing nn.Module and defining a forward which receives input Tensors and produces output Tensors using other modules or other autograd operations on Tensors.\n\nFor example, we can implement our 2-layer simple NN as the following:","483b067c":"### 3.2.3 Optimizer? Optimizer! [10 pts]\nSo far, we only use SGD as our optimizer. **Now, pick two other optimizers, train your CNN models, and compare the performance you get. [8 pts]**","e27c3c06":"## 2.2 Your Simple NN [20 pts]\nNow, it is time for you to implement your own model for this classification task. Your job here is to:\n1. Complete the SimpleNN class. It should be a 2- or 3-layer NN with proper non-linearity. **[10 pts]**\n2. Train your model with SGD optimizer. **[5 pts]**\n3. Tune your model a bit so you can achieve at least 80% accuracy both on training set and test set. **[5 pts]**\nHint: you might want to look up `nn.ReLU`, `nn.Sigmoid`, `nn.BCELoss` in the [official document](https:\/\/pytorch.org\/docs\/stable\/). You are allowed to freely pick the hyperparameters of your model.","5229c483":"# Section 3. Image Classification with CNN [65 pts]\nNow, we are back to the image classification problem. In this section, our goal is to, again, train models on CIFAR-10 to perform image classification. Your tasks here are to:\n1. Build and Train a simple feed-forward Neural Network (consists of only nn.Linear layer with activation function) for the classification task\n2. Build and Train a **Convolutional** Neural Network (CNN) for the classification task\n3. Try different settings for training your CNN\n4. Reproduce","380a9a10":"Below, we provide a simple example on how to train your model with this dataloader:","a159c52f":"## Submission guideline\n\n1. Click the Save button at the top of the Jupyter Notebook.\n2. Please make sure to have entered your Virginia Tech PID below.\n3. Select Edit -> Clear All Output. This will clear all the outputs from all cells (but will keep the content of the cells).\n4. Select Runtime -> Restart and Run All. This will run all the cells in order.\n5. Once you've rerun everything, follow the instruction at the end of this notebook to save it as a PDF.\n6. Look at the PDF file and make sure all your solutions are there, displayed correctly. \n7. Upload the PDF file and this notebook **INDEPENDENTLY**.\n8. Please **DO NOT** upload any data.","a654232a":"Evaluate your accuracy:","dd66f8da":"<span style=\"color:red\">**What did you see? Which optimizer is your favorite? Describe:**<\/span> **[2 pts]**","45f415bd":"# Section 2 Fully-Connected Networks [20 pts]","fc160527":"## 2.1 Load Two-moon datasets\nNow, let's use PyTorch to solve some synthetic datasets. In previous assignment, we have to write some codes to create training batches. Again, this can also be done with PyTorch `DataLoader`. The `DataLoader` utilizes parallel workers to read and prepare batches for you, which can greatly speedup the code when your time bottleneck is on file I\/O.\n\nHere, we show a simple example that can create a dataloader from numpy data:","55de96e3":"In the following cell, we provide the code for creating a CIFAR10 dataloader. As you can see, PyTorch's `torchvision` package actually has an interface for the CIFAR10 dataset: ","0bfcbd5c":"### 3.2.2 STACK MORE LAYERS [15 pts]\n\n1. **Try at least 4 network architectures with different numbers of convolutional layers. [8pts]**\n2. Train these settings with `optim.SGD` \n3. **Plot the accuracy as a fuction of convolutional layers and describe what you have observed (running time, performance, etc). [5 pts]**","79a77a69":"Train your network. **[5 pts]**","2ddd3f37":"## 3.2 Convolutional Neural Network (CNN) [45 pts]\nConvolutional layer has been proven to be extremely useful for vision-based task. As mentioned in the lecture, this speical layer allows the model to learn filters that capture crucial visual features. \n\n","c5dbe6da":"### 3.2.1 Implement and Evaluate CNN [10 pts]\nIn this section, you will need to construct a CNN for classifying CIFAR-10 image. Specifically, you need to:\n1. build a `CNNClassifier` with `nn.Conv2d`, `nn.Maxpool2d` and activation functions that you think are appropriate. \n2. You would need to flatten the output of your convolutional networks with `view()`, and feed it into a `nn.Linear` layer to predict the class labels of the input. \n\nOnce you are done with your module, train it with `optim.SGD`, and evaluate it. You should get an accuracy around **55%** on training set and **53%** on testing set. **[8 pts]**\n\nHint: You might want to look up `nn.Conv2d`, `nn.Maxpool2d`, `nn.CrossEntropyLoss()`, `view()` and `size()`.","27129b8b":"In this homework, you will need to use **GPU** to accelerate your training process. Thus, please set up the **runtime type**.\n\n**Runtime -> Change Runtime type -> Hardware Accelerator -> GPU**","bf21d1a3":"## 1.1. `nn` Module\nComputational graphs and autograd are a very powerful paradigm for defining complex operators and automatically taking derivatives; however for large neural networks raw autograd can be a bit too low-level.\n\nWhen building neural networks we frequently think of arranging the computation into layers, some of which have learnable parameters which will be optimized during learning.\n\nIn PyTorch, the nn package serves this purpose. The nn package defines a set of Modules, which are roughly equivalent to neural network layers. A Module receives input Tensors and computes output Tensors, but may also hold internal state such as Tensors containing learnable parameters. The nn package also defines a set of useful loss functions that are commonly used when training neural networks.\n\nNow, let's see how our simple NN could be implemented using the nn module.","68f9e90c":"## 3.1 Simple NN [20 pts]\nImplement a simple feed-forward neural network, and train it on the CIFAR-10 training set. Here's some specific requirements:\n1. The network should only consists of `nn.Linear` layers and the activation functions of your choices (e.g. `nn.Tanh`, `nn.ReLU`, `nn.Sigmoid`, etc). \n2. Train your model with `torch.optim.SGD` with the hyperparameters you like the most. \n\nComplete SimpleNN() below. **[10 pts]** ","fdb05ad6":"### 3.1.1 Evaluate NN [5 pts]\nEvaluate your NN. You should get an accuracy around **50%** on training set and **49%** on testing set.","9f8b76dc":"Mount your google drive in google colab","eb9c2f34":"Simply put, PyTorch is a **Tensor** library like Numpy. These two libraries similarly provide useful and efficient APIs for you to deal with your tensor data. What really differentiate PyTorch from Numpy are the following two features:\n1. Numerical operations that can **run on GPUs** (more than 10x speedup)\n2. Automatic differentiation for building and training neural networks\n\nIn this section, we will walk through some simple example, and see how the automatic differentiation functionality can make your life much easier.","e6b3a178":"Now evaluate your model with the helper function:","7cdde5ca":"So far, we have been updating the model parameters manually with `torch.no_grad()`. However, if we want to use optimization algorithms other than SGD, it might get a bit nasty to do it manually. Instead of manually doing this, we can use `optim` pacakge to help optimize our model: "}}