{"cell_type":{"e9beb14d":"code","6495576e":"code","0bb7d3f3":"code","cfd8171c":"code","40ad9797":"code","62f7e014":"code","361b062d":"code","69bf76b9":"code","098fde36":"code","327d3a4f":"code","c9b881ad":"code","97344660":"code","a6b4cc5d":"code","ed1d6ee0":"code","852fb413":"code","9fec322d":"code","5d685549":"code","7778589e":"code","27bd2772":"code","765a3113":"code","0c1468f0":"code","eb95a637":"markdown","6df1504f":"markdown","03127ad5":"markdown","3a9fc3d4":"markdown","5cec0c85":"markdown","6683c316":"markdown","c4f14cd4":"markdown","7d87e112":"markdown","28cbd9a7":"markdown","41617836":"markdown","afcdafd9":"markdown","52953072":"markdown","3b8a161e":"markdown","aa715984":"markdown","d61d7d5f":"markdown","1e881925":"markdown","111d6c7b":"markdown","0560016f":"markdown","8206bcd7":"markdown","9e63601c":"markdown","1d7def97":"markdown","79305d25":"markdown","f4ce6eed":"markdown","ea536e16":"markdown","6b4be758":"markdown","87f2b036":"markdown","5464003b":"markdown","d351643e":"markdown"},"source":{"e9beb14d":"#  Import Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n#  maps\nimport folium\nfrom folium.plugins import MarkerCluster\n\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 80)\n\n#  Kaggle directories\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","6495576e":"#  bottle.csv contains information on ocean conditions\n#  cast.csv   contains information on collecting stations\ndf = pd.read_csv('..\/input\/bottle.csv')","0bb7d3f3":"df.shape","cfd8171c":"df.describe()   #  NUMERICAL DATA","40ad9797":"df.describe(include=['O'])   #  CATEGORICAL DATA","62f7e014":"nulls = df.isnull().sum().sort_values(ascending = False)\nprcet = round(nulls\/len(df)*100,2)\n\ndf_null = pd.DataFrame(columns =  ['Attr','Total','Percent'])\ndf_null.Attr  = nulls.index\ndf_null.Total = nulls.values\ndf_null.Percent = prcet.values\nprint(df_null.head(20))","361b062d":"for i in df_null.Attr[df_null['Percent'] > 30]:\n    df = df.drop([i], axis=1)\n    #print(df.shape,i)","69bf76b9":"for i in df.columns:\n    if df[i].isnull().sum() > 0:\n        df[i].fillna(df[i].mode().mean(), inplace=True)\n        #print('filled',i)","098fde36":"nulls = df.isnull().sum().sort_values(ascending = False)\nprcet = round(nulls\/len(df)*100,2)\n\ndf_null = pd.DataFrame(columns =  ['Attr','Total','Percent'])\ndf_null.Attr  = nulls.index\ndf_null.Total = nulls.values\ndf_null.Percent = prcet.values\nprint(df_null.head())","327d3a4f":"print('COUNT OF DUPLICATES:  {}'.format(df.duplicated().sum()))","c9b881ad":"#  Depth_ID = [Century]-[YY][MM][ShipCode]-etc\n#  19-4903CR-HY-060-0930-05400560-0020A-7\ndf['Year'] = (df['Depth_ID'].str.split('-', expand=True)[0] + \\\n                df['Depth_ID'].str.split('-', expand=True)[1]). \\\n                map(lambda x: str(x)[:4])\ndf['Month'] = (df['Depth_ID'].str.split('-', expand=True)[1]). \\\n                 map(lambda x: str(x)[2:4])\n                 \ndf[['Depth_ID','Year','Month']].head(10)","97344660":"drop_cols = ['Cst_Cnt', 'Btl_Cnt', 'Sta_ID', 'Depth_ID', 'Depthm','Year','Month']\ndf_norm = df.drop(drop_cols, axis=1)  #  data for normalization\ndf_scale = df_norm.copy(deep=True)    #  backup data","a6b4cc5d":"df_scale = StandardScaler().fit_transform(df_scale)\n\n#  create dataframe\ndf_norm = pd.DataFrame(df_scale, index=df_norm.index, columns=df_norm.columns)","ed1d6ee0":"df_norm.corr()\n\n#  Drop columns with mode = \"0.0\".  No impact on correlation\nfor i in df_norm.columns.tolist():\n    if (df_norm[i].mode()[0] == 0.0):\n        print(' - ',i,df_norm[i].mode()[0])\n        df_norm = df_norm.drop(i,axis=1)\n\n#  Create correlation dataframe\ndf_corr = pd.DataFrame(columns=['Attributes','Correlation'])\ndf_corr.Attributes = df_norm.corr()['Salnty'].sort_values(ascending=False).index\ndf_corr.Correlation = df_norm.corr()['Salnty'].sort_values(ascending=False).values\nprint(df_corr)","852fb413":"#  observations for plotting\nplot_attr = ['R_DYNHT', 'R_SIGMA', 'R_Depth', 'RecInd', 'NH3q',  'T_prec', 'T_degC', 'R_POTEMP', 'O2ml_L']\n\nfor i in plot_attr:\n    if plot_attr[0] == i:\n        df_plot = df_corr[df_corr.Attributes == i]\n    else:\n        df_plot = df_plot.append(df_corr[df_corr.Attributes == i])\nprint(df_plot)\n\n#  take sample of data for plotting\ndf_sample = df_norm.sample(n=int(round(len(df)*.002,0)), random_state=0)\nprint('\\n\\nPlotting data shape: {}'.format(df_sample.shape))","9fec322d":"#  Salinity distribution\nplt.figure(figsize=(8,6))\nplt.xlim([32, 36])#  Salinity distribution\nplt.title('Salinity Distribution (g\/Kg)', fontsize=14)\nsns.distplot(df['Salnty'], color='darkgreen')","5d685549":"#  Yearly change in Salinity\nfig = plt.figure(figsize=(12,6))\nfig.autofmt_xdate()\nfig.add_subplot(121)\nplt.title('Yearly Change in Salinity (g\/Kg)', fontsize=14)\nsns.scatterplot(data=df, x='Year', y='Salnty', color='darkgreen')\n\n#  Seasonal change in Salinity\nfig.add_subplot(122)\nplt.title('Seasonal Change in Salinity (g\/Kg)', fontsize=14)\nsns.scatterplot(data=df, x='Month', y='Salnty', color='darkgreen')\nplt.show()","7778589e":"fig = plt.figure(figsize=(14,60))\ncol = 3\nrow  = int(len(df_corr.Attributes)\/col)\ncount = 1\n\nfor i, j in zip(df_plot.Attributes,df_plot.Correlation):\n    fig.add_subplot(row, col, count)\n    plt.title('Salinity vs {} (corr = {:.4})\\nnormalized distribution'.format(i,j))\n    plt.xlim(-4,4)\n    sns.distplot(df_sample.Salnty)\n    sns.distplot(df_sample[i])\n    count = count + 1\n\nplt.show()","27bd2772":"fig = plt.figure(figsize=(14,60))\ncol = 3\nrow  = int(len(df_corr.Attributes)\/col)\ncount = 1\n\nfor i, j in zip(df_plot.Attributes,df_plot.Correlation):\n    fig.add_subplot(row, col, count)\n    plt.title('Salinity vs {} (corr = {:.4f})\\nnormalized distribution'.format(i,j))\n    sns.regplot(x=df_sample[i],y=\"Salnty\",data=df_sample,order=2, scatter_kws={'alpha':0.25},color='green');\n    count = count + 1\n\nplt.show()","765a3113":"#  Load the Dataset\ndfLOC = pd.read_csv('..\/input\/cast.csv')","0c1468f0":"#  select location points\ndfLOC = dfLOC[['Lat_Dec', 'Lon_Dec','Date']]\ndfLOC = dfLOC.tail(1000)\ndfLOC = dfLOC.reset_index(drop=True)  # reset index after tail\n\n#  create folium map\nsalinity_map   = folium.Map(location=[dfLOC.Lat_Dec.mean(),dfLOC.Lon_Dec.mean()], zoom_start=6)\nmarker_cluster = MarkerCluster().add_to(salinity_map)\n\nfor i in range(len(dfLOC)):\n    folium.Marker(location=[dfLOC.Lat_Dec[i],dfLOC.Lon_Dec[i]],\n            popup = (dfLOC.Date[i]),         # dates in popups\n            icon = folium.Icon(color='green')  # green popup icon\n    ).add_to(marker_cluster)\n\nsalinity_map.add_child(marker_cluster)\nsalinity_map         #  display map","eb95a637":"<a id=\"top\"><\/a> \n# Salinity CalCOFI: Data Clean, Correlation, Visualizations and Folium Map\n\nThe CalCOFI data set represents the longest (1949-present) and most complete (more than 50,000 sampling stations) time series of oceanographic and larval fish data in the world. CalCOFI research drew world attention to the biological response to the dramatic Pacific-warming event in 1957-58 and introduced the term \u201cEl Ni\u00f1o\u201d into the scientific literature. \n\nThis analysis involves CalCOFI data cleaning, correlation, visualization and folium map.\n\n\n### Table Of Content\n1.  [Data Collection](#coll)<br>\n\n2.  [Understanding the Data](#data)<br>\n2.1  [Data Types](#data_info)<br>\n2.2  [Statistical Summary](#data_summ)<br>\n\n3.  [Data Cleaning](#prep)<br>\n3.1  [Check for NULLs\/Duplicates](#prep_nulls)<br>\n3.2  [Extract Month\/Year from Depth_ID](#prep_extract)<br>\n3.3  [Drop columns that cannot be Normalized](#prep_drop)<br>\n\n4.  [Correlations](#corr)<br>\n4.1  [Normalization](#corr_norm)<br>\n4.2  [Correlation - Salinity](#corr_saln)<br>\n\n5. [Data Visualization](#eda)<br>\n5.1  [Salinity Plots](#eda_saln)<br>\n5.2  [Distribution Plots - Correlation](#eda_dist)<br>\n5.3  [Regression Plots - Correlation](#eda_regr)<br>\n\n6.  [Map - Collection Station Locations](#map)<br>   \n","6df1504f":"###  Load the Datasets","03127ad5":"##  5.1  Salinity Plots <a id=\"eda_saln\"><\/a>\nSalinity plots will use the complete dataset.","3a9fc3d4":"[go to top of document](#top)     \n\n---\n#  6.  Map - Collection Station Locations<a id=\"map\"><\/a>","5cec0c85":"[go to top of document](#top)     \n\n---\n#  5.  Data Visualization <a id=\"eda\"><\/a>\nSome of the attribute values will result in similar plots, i.e. R_O2Sat and O2Sat.  In that case, only one observation will be selected for plotting the distribution and regression plots.  For this project, nine observations were selected.\n\n**df_sample** - dataframe size will be sampled (reduced) in order to have cleaner plots.","6683c316":"##  5.2  Distribution Plots for Correlations <a id=\"eda_dist\"><\/a>","c4f14cd4":"---\nPlease upvote if you found this helpful :-)\n###  END\n[go to top of document](#top)","7d87e112":"##  5.3  Regression Plots - Correlation <a id=\"eda_regr\"><\/a>","28cbd9a7":"##  2.1  Data Types<a id=\"data_info\"><\/a>\n-  **Categorical data** - collection station information - for map\n-  **Numerical data** - scientific data\n\n\nThe collection station locations will be used for the Folium map, but the main attributes for this analysis will be:\n\n*  **Depth_ID**  - extract months and years\n*  **Salnty**  - Salinity in g of salt per kg of water (g\/kg).  _Target_ variable","41617836":"[go to top of document](#top)     \n\n---\n#  3.  Data Cleaning <a id=\"prep\"><\/a>\nClean the data before begining any type of analysis.","afcdafd9":"##  2.2  Statistical Summary <a id=\"data_summ\"><\/a>\nSummarize descriptive statistics of the dataset for *numerical* and *categorical* features. ","52953072":"###  3.1.1  Check for NULL percentages","3b8a161e":"## 3.2  Extract Month\/Year from Depth_ID <a id=\"prep_extract\"><\/a>\n_cast.csv_ file has the month\/year data, but due to the size of the _bottle.csv_, it's be easier to extract it from here.","aa715984":"###  5.1.2  Plot of Salinity over Time","d61d7d5f":"[go to top of document](#top)     \n\n---\n# 4.  Correlation<a id=\"corr\"><\/a>\nCorrelation is a statistical metric for measuring to what extent different variables are interdependent.  In order to perform correlation, we need to first normalize the data.","1e881925":"###  3.1.4  Re-check NULL Percentages\nShows attributes for that can be used for EDA and Correlation.","111d6c7b":"**Statistical Summary - NUMERICAL DATA**   \nSummarize the central tendency, dispersion and shape of numeric features, excluding categorical and NaN values.","0560016f":"[go to top of document](#top)     \n\n---\n#  2.  Understanding the Data <a id=\"data\"><\/a>","8206bcd7":"###  5.1.1  Salinity Distribution","9e63601c":"## 4.1  Normalization<a id=\"corr_norm\"><\/a>\nNormalization is a rescaling of the data from the original range so that all values are within a certain range, typically between 0 and 1. Normalized data is essential in machine learning. Correlation and models will not produce good results if the scales are not standardized.\n\nData in **df_corr** will be normalized and the **df** data frame will be updated with the encoded and normalized data.","1d7def97":"## 3.3  Drop columns that cannot be Normalized <a id=\"prep_drop\"><\/a>\nDropping columns that cannot be normalized.\n   - Cst_Cnt   Auto-numbered Cast Count\n   - Btl_Cnt   Auto-numbered Bottle count\n   - Sta_ID    CalCOFI Line and Station\n   - Depth_ID  [Century]-[YY][MM][ShipCode]","79305d25":"###  3.1.2  Drop attributes with more than 30% data missing","f4ce6eed":"###  3.1.3  Fill remaining NULLs with **mode** values\nSome attributes have more than one mode.  Take mean of the multiple modes for the 'fillna' value.","ea536e16":"---\n#  1.  Data Collection <a id=\"coll\"><\/a>\n###  Import Python Libraries","6b4be758":"**Statistical Summary - CATEGORICAL DATA**   \nSummarize the count, uniqueness and frequency of categorical features, excluding numerical values.","87f2b036":"###  3.1.5  Check for Duplicated values","5464003b":"## 4.2  Correlation - Salinity<a id=\"corr_saln\"><\/a>","d351643e":"## 3.1  Check for NULLs\/Duplicates <a id=\"prep_nulls\"><\/a>\nCleaning up the NULL and duplicate values in the dataset:\n\n*  3.1.1  Check for NULL percentages\n*  3.1.2  Drop attributes with more than 30% data missing\n*  3.1.3  Fill remaining NULLs with **mean** values\n*  3.1.4  Re-check NULL Percentages\n*  3.1.5  Check for duplicated"}}