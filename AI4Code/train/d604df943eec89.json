{"cell_type":{"1f36c617":"code","e2647f50":"code","09e4604b":"code","bd45d819":"code","3386df68":"code","e4e2777b":"code","9d4f09ac":"code","ff4e2c1b":"code","0a6cc90c":"code","0223f113":"code","24f89be3":"code","d740137f":"code","0524f246":"code","efc898ee":"code","5b166f73":"code","06d92e23":"code","6a689b7d":"code","0048d510":"code","43849c67":"code","db202074":"code","363cdcc8":"code","f862e6e0":"code","497b418a":"code","2ece10cc":"code","4e368406":"code","0d35f696":"code","c54b8e81":"code","64de4d41":"code","dbbe1c0b":"code","8244d801":"code","5f209326":"code","ffdad433":"code","5341f1d4":"code","6a5836d1":"code","cdb11443":"code","abc4be52":"code","5739c5b3":"code","d8d677c3":"code","69359e07":"code","1d6cb657":"code","a591d405":"code","22f13b58":"code","395dd183":"code","f66c749e":"code","85bd314e":"code","2331c178":"code","7713b242":"code","1579e520":"code","90daa8c6":"code","bd0bbe86":"code","492009ca":"code","c0c9effe":"code","5ba5f605":"code","0dcf13f1":"code","8836272e":"code","758141ab":"code","65209bb0":"code","3dd32219":"code","fde5c984":"code","ddec8620":"code","0ffc08c0":"code","2e5f4f8f":"code","0713a791":"code","847c960d":"code","d11b888b":"code","6a2e4ad7":"code","9b7cb648":"code","c6b7776c":"code","75c63e5b":"code","7e0b99d1":"code","55c41ab3":"code","c982a60a":"code","6be1f828":"code","9f61ca0c":"code","345f3de6":"code","86a5a876":"code","e563de45":"code","73b55716":"code","cf98481d":"code","9f74e6f4":"code","d44d37a7":"code","89ebd543":"code","05a80bc3":"code","4192ad0a":"code","ae2c25af":"code","c99e9a8d":"code","0ee5160d":"code","c8a65034":"code","baa46729":"code","26c6f329":"code","a148bfee":"code","b8daf0bf":"code","4da65566":"code","d1338f1a":"code","108af1c8":"code","6580d14a":"code","3d22d2d9":"code","516b1934":"code","b1e5cafe":"code","5a04fc52":"code","2ffa9367":"code","397be2f4":"code","8b1400d5":"code","5b35a6ed":"code","1bc8c210":"code","7124eb26":"code","5030c6e6":"code","c46dac0b":"code","bfef6dae":"code","25b15a5c":"code","52687d5c":"code","804c66d4":"code","8e7bea83":"code","a9e14f29":"code","d29ced0e":"code","4dc900d6":"code","bb3c69ae":"code","6f4c181e":"code","c1ef1c4f":"code","027b1e8a":"code","2d61efb2":"code","a236eb6d":"code","ae991bf0":"code","7a8d66c4":"code","c124ceb1":"code","e4faa2cf":"code","2a7c95e1":"code","6e1cfb5d":"code","ded10d72":"code","18f6cd03":"code","e5fd445d":"code","a8a641f2":"code","492bca21":"markdown","0b4d69e7":"markdown","390f37d7":"markdown","9e6fa07b":"markdown","584a4f0f":"markdown","5c5d3eb0":"markdown","893b58de":"markdown","4f242966":"markdown","f16bd568":"markdown","62c1fcd3":"markdown","5436df24":"markdown","6b32801c":"markdown","13c68733":"markdown","9eb8c061":"markdown","427a9f2a":"markdown","2c9286ff":"markdown","a4fa87a3":"markdown","e43874a1":"markdown","67f8abff":"markdown","030814a3":"markdown","2a9bb3ae":"markdown","ca9c9d83":"markdown","74c26955":"markdown","51742b5a":"markdown","d6a05387":"markdown","28dc777c":"markdown","a86d31d6":"markdown","97b0e238":"markdown","a6e82828":"markdown","43f097a5":"markdown","7bd6e4d7":"markdown","403cab8a":"markdown","c42a0fd1":"markdown","8aba1bec":"markdown","d08ff1a4":"markdown","2884477d":"markdown","a6df3f8c":"markdown","e59c2a2a":"markdown","66ae26b2":"markdown","27cfb55a":"markdown","7bd074e1":"markdown","075f073e":"markdown","bf4c674f":"markdown","2a7d1b2f":"markdown","c926d494":"markdown","17a0444e":"markdown","623b4a89":"markdown","214f9c11":"markdown","460f5029":"markdown","b2094b48":"markdown","05d2f645":"markdown","d11ca27f":"markdown","c01fda56":"markdown","2309ead7":"markdown","635b5b53":"markdown","5cb40e4b":"markdown","b2ab7b88":"markdown","6210d20c":"markdown","a34ebc5c":"markdown","0b49994f":"markdown","d2149c53":"markdown","acb6d657":"markdown","2860d774":"markdown","7861de52":"markdown"},"source":{"1f36c617":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e2647f50":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(\"ignore\")\nplt.style.use('ggplot')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]","09e4604b":"train_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')\ntest_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')\ntrain_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv')\ntest_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv')","bd45d819":"train_transaction.shape, test_transaction.shape, train_identity.shape,  test_identity.shape","3386df68":"train_transaction.head()","e4e2777b":"test_transaction.head()","9d4f09ac":"train_transaction.info(), test_transaction.info()","ff4e2c1b":"train_identity.head()","0a6cc90c":"test_identity.head()","0223f113":"train_transaction.isnull().sum()","24f89be3":"train_identity.isnull().sum()","d740137f":"test_transaction.isnull().sum()","0524f246":"test_identity.isnull().sum()","efc898ee":"train_transaction['TransactionID'].value_counts()","5b166f73":"train_identity['TransactionID'].value_counts()","06d92e23":"test_transaction['TransactionID'].value_counts()","6a689b7d":"train_identity['TransactionID'].value_counts()","0048d510":"print(np.sum(train_transaction['TransactionID'].isin(train_identity['TransactionID'].unique())))\nprint(np.sum(test_transaction['TransactionID'].isin(test_identity['TransactionID'].unique())))","43849c67":"train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","db202074":"train.head()","363cdcc8":"test.head()","f862e6e0":"import plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\n\nx = train_transaction['isFraud'].value_counts().index\ny = train_transaction['isFraud'].value_counts().values\n\ntrace2 = go.Bar(\n     x=x ,\n     y=y,\n     marker=dict(\n         color='blue',\n         colorscale = 'Viridis',\n         reversescale = True\n     ),\n     name=\"Imbalance\",    \n )\nlayout = dict(\n     title=\"Data imbalance - isFraud\",\n     width = 600, height = 400,\n     xaxis=go.layout.XAxis(\n     automargin=True),\n     yaxis=dict(\n         showgrid=False,\n         showline=False,\n         showticklabels=True,\n #         domain=[0, 0.85],\n     ), \n)\nfig1 = go.Figure(data=[trace2], layout=layout)\niplot(fig1)","497b418a":"import datetime\nstartdate = datetime.datetime.strptime('2017-12-01', '%Y-%m-%d')\ntrain['TransactionDT'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\ntest['TransactionDT'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))","2ece10cc":"fig, axes = plt.subplots(1, 1, figsize=(16, 6))\ntrain.set_index('TransactionDT').resample('D').mean()['isFraud'].plot(ax=axes, color='blue').set_ylabel('isFraud mean', fontsize=14);\naxes.set_title('Mean of isFraud by day', fontsize=16);","4e368406":"fig, axes = plt.subplots(1, 1, figsize=(16, 6))\ntrain['TransactionDT'].dt.floor('d').value_counts().sort_index().plot(ax=axes, color='blue').set_xlabel('Date', fontsize=14);\ntest['TransactionDT'].dt.floor('d').value_counts().sort_index().plot(ax=axes, color='tab:orange').set_ylabel('Number of training examples', fontsize=14);\naxes.set_title('Number of training examples by day', fontsize=16);\naxes.legend(['Train', 'Test']);","0d35f696":"fig, ax1 = plt.subplots(figsize=(16, 6))\ntrain.set_index('TransactionDT').resample('D').mean()['isFraud'].plot(ax=ax1, color='blue')\nax1.tick_params(axis='y', labelcolor='blue')\nax1.set_ylabel('isFraud mean', color='blue', fontsize=14)\nax2 = ax1.twinx()\ntrain['TransactionDT'].dt.floor('d').value_counts().sort_index().plot(ax=ax2, color='tab:orange');\nax2.tick_params(axis='y', labelcolor='tab:orange');\nax2.set_ylabel('Number of training examples', color='tab:orange', fontsize=14);\nax2.grid(False)","c54b8e81":"ax = train.plot(x='TransactionDT',\n                       y='TransactionAmt',\n                       kind='scatter',\n                       alpha=0.01,\n                       label='TransactionAmt-train',\n                       title='Train and test Transaction Ammounts by Time (TransactionDT)',\n                       color='blue',\n                       ylim=(0, 5000),\n                       figsize=(15, 5))\ntest.plot(x='TransactionDT',\n                      y='TransactionAmt',\n                      kind='scatter',\n                      label='TransactionAmt-test',\n                      alpha=0.01,\n                      color='tab:orange',\n                       ylim=(0, 5000),\n                      ax=ax)\n\ntrain.loc[train_transaction['isFraud'] == 1] \\\n    .plot(x='TransactionDT',\n         y='TransactionAmt',\n         kind='scatter',\n         alpha=0.01,\n         label='TransactionAmt-train',\n         title='Train and test Transaction Ammounts by Time (TransactionDT)',\n         ylim=(0, 5000),\n         color='yellow',\n         figsize=(15, 5),\n         ax=ax)\nplt.show()","64de4d41":"fig, ax = plt.subplots(1, 2, figsize=(18,6))\n\ntime_val = train['TransactionAmt'].values\n\nsns.distplot(time_val, ax=ax[0], color='blue')\nax[0].set_title('Distribution of TransactionAmt', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\nsns.distplot(np.log(time_val), ax=ax[1], color='tab:orange')\nax[1].set_title('Distribution of LOG TransactionAmt', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\nplt.show()","dbbe1c0b":"fig, ax = plt.subplots(1, 2, figsize=(18,6))\n\ntime_val = train_transaction.loc[train_transaction['isFraud'] == 1]['TransactionAmt'].values\n\nsns.distplot(np.log(time_val), ax=ax[0], color='blue')\nax[0].set_title('Distribution of LOG TransactionAmt, isFraud=1', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\ntime_val = train_transaction.loc[train_transaction['isFraud'] == 0]['TransactionAmt'].values\n\nsns.distplot(np.log(time_val), ax=ax[1], color='tab:orange')\nax[1].set_title('Distribution of LOG TransactionAmt, isFraud=0', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\n\nplt.show()","8244d801":"fig, ax = plt.subplots(1, 2, figsize=(20,5))\n\nsns.countplot(x=\"ProductCD\", ax=ax[0], hue = \"isFraud\", data=train)\nax[0].set_title('ProductCD train', fontsize=14)\nsns.countplot(x=\"ProductCD\", ax=ax[1], data=test)\nax[1].set_title('ProductCD test', fontsize=14)\nplt.show()","5f209326":"def calcul(val):\n    X = train[train['ProductCD'] == val]['ProductCD'].value_counts()\n    Y = train[(train['ProductCD'] == val) & (train['isFraud'] == 1)]['ProductCD'].value_counts()\n    return np.around(Y\/X * 100)","ffdad433":"calcul('W')","5341f1d4":"calcul('C')","6a5836d1":"train['card1'].isnull().sum()","cdb11443":"train['card1'].nunique()","abc4be52":"plt.figure(figsize=(14, 6))\nsns.kdeplot(train[train['isFraud']==1]['card1'], label='isFraud 1', color = 'blue');\nsns.kdeplot(train[train['isFraud']==0]['card1'], label='isFraud 0', color = 'tab:orange');","5739c5b3":"train['card2'].isnull().sum()","d8d677c3":"train['card2'].nunique()","69359e07":"plt.figure(figsize=(14, 6))\nsns.kdeplot(train[train['isFraud']==1]['card2'], label='isFraud 1', color = 'blue');\nsns.kdeplot(train[train['isFraud']==0]['card2'], label='isFraud 0', color = 'tab:orange');","1d6cb657":"train['card3'].isnull().sum()","a591d405":"train['card3'].nunique()","22f13b58":"plt.figure(figsize=(14, 6))\nsns.kdeplot(train[train['isFraud']==1]['card3'], label='isFraud 1', color = 'blue');\nsns.kdeplot(train[train['isFraud']==0]['card3'], label='isFraud 0', color = 'tab:orange');","395dd183":"train.loc[train.card3.isin(train.card3.value_counts()[train.card3.value_counts() < 200].index), 'card3'] = \"Others\"\ntrain.loc[train.card5.isin(train.card5.value_counts()[train.card5.value_counts() < 300].index), 'card5'] = \"Others\"","f66c749e":"tmp = pd.crosstab(train['card3'], train['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\ntotal = len(train_transaction)\n\nplt.figure(figsize=(18, 6))\ng2 = sns.countplot(x = 'card3', data=train, order=list(tmp.card3.values))\ng22 = g2.twinx()\ngg2 = sns.pointplot(x='card3', y='Fraud', data=tmp, \n                    color='black', order=list(tmp.card3.values))\ngg2.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\ng2.set_title(\"Card 3 Values Distribution and % of Transaction Frauds\", fontsize=20)\ng2.set_xlabel(\"Card 3 Values\", fontsize=18)\ng2.set_ylabel(\"Count\", fontsize=18)\nfor p in g2.patches:\n    height = p.get_height()\n    g2.text(p.get_x()+p.get_width()\/2.,\n            height + 25,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\") \nplt.show()","85bd314e":"train['card4'].isnull().sum()","2331c178":"train['card4'].nunique()","7713b242":"fig, ax = plt.subplots(1, 2, figsize=(25,10))\n\nsns.countplot(x=\"card4\", ax=ax[0], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[0].set_title('card4 isFraud=0', fontsize=14)\nsns.countplot(x=\"card4\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('card4 isFraud=1', fontsize=14)","1579e520":"train['card5'].isnull().sum()","90daa8c6":"train['card5'].nunique()","bd0bbe86":"tmp2 = pd.crosstab(train['card5'], train['isFraud'], normalize='index') * 100\ntmp2 = tmp2.reset_index()\ntmp2.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\ntotal = len(train_transaction)\n\nplt.figure(figsize=(18, 6))\ng3 = sns.countplot(x='card5', data=train, order=list(tmp2.card5.values))\ng3t = g3.twinx()\ng3t = sns.pointplot(x='card5', y='Fraud', data=tmp2, \n                    color='black', order=list(tmp2.card5.values))\ng3t.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\ng3.set_title(\"Card 5 Values Distribution and % of Transaction Frauds\", fontsize=20)\ng3.set_xticklabels(g3.get_xticklabels(),rotation=90)\ng3.set_xlabel(\"Card 5 Values\", fontsize=18)\ng3.set_ylabel(\"Count\", fontsize=18)\nfor p in g3.patches:\n    height = p.get_height()\n    g3.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\",fontsize=11) \n    \nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\n\nplt.show()","492009ca":"train['card6'].isnull().sum()","c0c9effe":"train['card6'].nunique()","5ba5f605":"fig, ax = plt.subplots(1, 2, figsize=(25,10))\n\nsns.countplot(x=\"card6\", ax=ax[0], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[0].set_title('card6 isFraud=0', fontsize=14)\nsns.countplot(x=\"card6\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('card6 isFraud=1', fontsize=14)","0dcf13f1":"addr_cols = [a for a in train.columns if 'addr' in a]\ntrain[addr_cols].head()","8836272e":"train['addr1'].plot(kind='hist',\n                                bins=50,\n                                figsize=(15, 2),\n                                title='addr1',\n                                color = 'blue')\nplt.show()\ntrain['addr2'].plot(kind='hist',\n                                bins=50,\n                                figsize=(15, 2),\n                                title='addr2',\n                                color = 'blue')\nplt.show()","758141ab":"train.loc[train.addr1.isin(train.addr1.value_counts()[train.addr1.value_counts() <= 5000 ].index), 'addr1'] = \"Others\"\ntrain.loc[train.addr2.isin(train.addr2.value_counts()[train.addr2.value_counts() <= 50 ].index), 'addr2'] = \"Others\"","65209bb0":"total_amt = train.groupby(['isFraud'])['TransactionAmt'].sum().sum()\n\ndef ploting_cnt_amt(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    \n    plt.figure(figsize=(16,14))    \n    plt.suptitle(f'{col} Distributions ', fontsize=24)\n    \n    plt.subplot(211)\n    g = sns.countplot( x=col,  data=df, order=list(tmp[col].values))\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,tmp['Fraud'].max()*1.1)\n    gt.set_ylabel(\"%Fraud Transactions\", fontsize=16)\n    g.set_title(f\"Most Frequent {col} values and % Fraud Transactions\", fontsize=20)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\",fontsize=12) \n        \n    g.set_ylim(0,max(sizes)*1.15)\n    \n    #########################################################################\n    perc_amt = (df.groupby(['isFraud',col])['TransactionAmt'].sum() \\\n                \/ df.groupby([col])['TransactionAmt'].sum() * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    amt = df.groupby([col])['TransactionAmt'].sum().reset_index()\n    perc_amt = perc_amt.fillna(0)\n    plt.subplot(212)\n    g1 = sns.barplot(x=col, y='TransactionAmt', \n                       data=amt, \n                       order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, \n                        order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,perc_amt['Fraud'].max()*1.1)\n    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    g1.set_title(f\"{col} by Transactions Total + %of total and %Fraud Transactions\", fontsize=20)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Transaction Total Amount(U$)\", fontsize=16)\n    g1.set_xticklabels(g.get_xticklabels(),rotation=45)    \n    \n    for p in g1.patches:\n        height = p.get_height()\n        g1.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total_amt*100),\n                ha=\"center\",fontsize=12) \n        \n    plt.subplots_adjust(hspace=.4, top = 0.9)\n    plt.show()\n    \nploting_cnt_amt(train, 'addr1')","3dd32219":"ploting_cnt_amt(train, 'addr2')","fde5c984":"dist_cols = [d for d in train.columns if 'dist' in d]\ntrain[dist_cols].head()","ddec8620":"train['dist1'].isna().sum(), train['dist2'].isna().sum()","0ffc08c0":"train['dist1'].plot(kind='hist',\n                                bins=5000,\n                                figsize=(15, 2),\n                                title='dist1 distribution',\n                                color='blue',\n                                logx=True)\nplt.show()\ntrain['dist2'].plot(kind='hist',\n                                bins=5000,\n                                figsize=(15, 2),\n                                title='dist2 distribution',\n                                color= 'blue',\n                                logx=True)\nplt.show()\n","2e5f4f8f":"train.loc[train['P_emaildomain'].isin(['gmail.com', 'gmail']),'P_emaildomain'] = 'Google'\n\ntrain.loc[train['P_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                         'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                         'yahoo.es']), 'P_emaildomain'] = 'Yahoo Mail'\ntrain.loc[train['P_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                         'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                         'outlook.es', 'live.com', 'live.fr',\n                                         'hotmail.fr']), 'P_emaildomain'] = 'Microsoft'\ntrain.loc[train.P_emaildomain.isin(train.P_emaildomain\\\n                                         .value_counts()[train.P_emaildomain.value_counts() <= 500 ]\\\n                                         .index), 'P_emaildomain'] = \"Others\"\ntrain.P_emaildomain.fillna(\"NoInf\", inplace=True)","0713a791":"ploting_cnt_amt(train, 'P_emaildomain')","847c960d":"train.loc[train['R_emaildomain'].isin(['gmail.com', 'gmail']),'R_emaildomain'] = 'Google'\n\ntrain.loc[train['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                             'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                             'yahoo.es']), 'R_emaildomain'] = 'Yahoo Mail'\ntrain.loc[train['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                             'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                             'outlook.es', 'live.com', 'live.fr',\n                                             'hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\ntrain.loc[train.R_emaildomain.isin(train.R_emaildomain\\\n                                         .value_counts()[train.R_emaildomain.value_counts() <= 300 ]\\\n                                         .index), 'R_emaildomain'] = \"Others\"\ntrain.R_emaildomain.fillna(\"NoInf\", inplace=True)","d11b888b":"ploting_cnt_amt(train, 'R_emaildomain')","6a2e4ad7":"from scipy import stats\n\ndef resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary","9b7cb648":"c_cols = [c for c in train if c[0] == 'C']\ntrain[c_cols].head()","c6b7776c":"resumetable(train[c_cols])","75c63e5b":"train[c_cols].describe()","7e0b99d1":"train.loc[train.C1.isin(train.C1\\\n                              .value_counts()[train.C1.value_counts() <= 400 ]\\\n                              .index), 'C1'] = \"Others\"","55c41ab3":"ploting_cnt_amt(train, 'C1')","c982a60a":"train.loc[train.C2.isin(train.C2\\\n                              .value_counts()[train.C2.value_counts() <= 350 ]\\\n                              .index), 'C2'] = \"Others\"","6be1f828":"ploting_cnt_amt(train, 'C2')","9f61ca0c":"train.loc[train.C4.isin(train.C4\\\n                              .value_counts()[train.C4.value_counts() <= 400 ]\\\n                              .index), 'C4'] = \"Others\"","345f3de6":"ploting_cnt_amt(train, 'C4')","86a5a876":"d_cols = [c for c in train if (c[0] == 'D') and (c[1] != 'e')]\ntrain[d_cols].head()","e563de45":"resumetable(train[d_cols])","73b55716":"train[d_cols].describe()","cf98481d":"train['D1'].value_counts()","9f74e6f4":"train.loc[train.D1.isin(train.D1.value_counts()[train.D1.value_counts() <= 2000 ].index), 'D1'] = \"Others\"","d44d37a7":"ploting_cnt_amt(train, 'D1')","89ebd543":"train.D2.value_counts()","05a80bc3":"train.loc[train.D2.isin(train.D2.value_counts()[train.D2.value_counts() <= 2000 ].index), 'D2'] = \"Others\"","4192ad0a":"ploting_cnt_amt(train, 'D2')","ae2c25af":"train.D8.value_counts()","c99e9a8d":"train.loc[train.D8.isin(train.D8.value_counts()[train.D8.value_counts() <= 250 ].index), 'D8'] = \"Others\"","0ee5160d":"ploting_cnt_amt(train, 'D8')","c8a65034":"train['D8'].unique()","baa46729":"m_cols = [c for c in train if c[0] == 'M']\ntrain[m_cols].head()","26c6f329":"resumetable(train[m_cols])","a148bfee":"train[m_cols].describe()","b8daf0bf":"train.M1.value_counts()","4da65566":"train.M4.value_counts()","d1338f1a":"for col in m_cols:\n    train[col] = train[col].fillna(\"Miss\")\n    \ndef ploting_dist_ratio(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.figure(figsize=(20,5))\n    plt.suptitle(f'{col} Distributions ', fontsize=22)\n\n    plt.subplot(121)\n    g = sns.countplot(x=col, data=df, order=list(tmp[col].values))\n    # plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n    g.set_title(f\"{col} Distribution\\nCound and %Fraud by each category\", fontsize=18)\n    g.set_ylim(0,400000)\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,20)\n    gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    for p in gt.patches:\n        height = p.get_height()\n        gt.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\",fontsize=14) \n        \n    perc_amt = (train.groupby(['isFraud',col])['TransactionAmt'].sum() \/ total_amt * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.subplot(122)\n    g1 = sns.boxplot(x=col, y='TransactionAmt', hue='isFraud', \n                     data=df[df['TransactionAmt'] <= lim], order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,5)\n    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    g1.set_title(f\"{col} by Transactions dist\", fontsize=18)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Transaction Amount(U$)\", fontsize=16)\n        \n    plt.subplots_adjust(hspace=.4, wspace = 0.35, top = 0.80)\n    \n    plt.show()","108af1c8":"for col in m_cols:\n    ploting_dist_ratio(train, col, lim=2500)","6580d14a":"v_cols = [c for c in train if c[0] == 'V']\ntrain[v_cols].head()","3d22d2d9":"resumetable(train[v_cols])","516b1934":"train[v_cols].describe()","b1e5cafe":"len(train.isFraud[train.isFraud==1])\/len(train)","5a04fc52":"# Helper functions\n# 1. For calculating % na values in  columns\ndef percent_na(df):\n    percent_missing = df.isnull().sum() * 100 \/ len(df)\n    missing_value_df = pd.DataFrame({'column_groups': percent_missing.index,\n                                 'percent_missing': percent_missing.values})\n    return missing_value_df\n# 2. For plotting grouped histograms \ndef sephist(col):\n    yes = train_transaction[train_transaction['isFraud'] == 1][col]\n    no = train_transaction[train_transaction['isFraud'] == 0][col]\n    return yes, no","2ffa9367":"pd.options.display.max_colwidth =300\nVcols=train_transaction.columns[train_transaction.columns.str.startswith('V')]\ntrain_transaction_vcol_na = percent_na(train_transaction[Vcols])\ntrain_transaction_vcol_na_group= train_transaction_vcol_na.groupby('percent_missing')['column_groups'].unique().reset_index()\nnum_values_per =[]\nfor i in range(len(train_transaction_vcol_na_group)):\n    num_values_per.append(len(train_transaction_vcol_na_group['column_groups'][i]))\ntrain_transaction_vcol_na_group['num_columns_group'] = num_values_per\ntrain_transaction_vcol_na_group","397be2f4":"pd.options.display.max_colwidth =300\nVcols=test_transaction.columns[test_transaction.columns.str.startswith('V')]\ntest_transaction_vcol_na = percent_na(test_transaction[Vcols])\ntest_transaction_vcol_na_group= test_transaction_vcol_na.groupby('percent_missing')['column_groups'].unique().reset_index()\nnum_values_per =[]\nfor i in range(len(test_transaction_vcol_na_group)):\n    num_values_per.append(len(test_transaction_vcol_na_group['column_groups'][i]))\ntest_transaction_vcol_na_group['num_columns_group'] = num_values_per\ntest_transaction_vcol_na_group","8b1400d5":"def column_value_freq(sel_col,cum_per):\n    dfpercount = pd.DataFrame(columns=['col_name','num_values_'+str(round(cum_per,2))])\n    for col in sel_col:\n        col_value = train_transaction[col].value_counts(normalize=True)\n        colpercount = pd.DataFrame({'value' : col_value.index,'per_count' : col_value.values})\n        colpercount['cum_per_count'] = colpercount['per_count'].cumsum()\n        if len(colpercount.loc[colpercount['cum_per_count'] < cum_per,] ) < 2:\n            num_col_99 = len(colpercount.loc[colpercount['per_count'] > (1- cum_per),])\n        else:\n            num_col_99 = len(colpercount.loc[colpercount['cum_per_count']< cum_per,] )\n        dfpercount=dfpercount.append({'col_name': col,'num_values_'+str(round(cum_per,2)): num_col_99},ignore_index = True)\n    dfpercount['unique_values'] = train_transaction[sel_col].nunique().values\n    dfpercount['unique_value_to_num_values'+str(round(cum_per,2))+'_ratio'] = 100 * (dfpercount['num_values_'+str(round(cum_per,2))]\/dfpercount.unique_values)\n    dfpercount['percent_missing'] = percent_na(train_transaction[sel_col])['percent_missing'].round(3).values\n    return dfpercount\n\ndef column_value_details(sel_col,cum_per):\n    dfpercount = pd.DataFrame(columns=['col_name','values_'+str(round(cum_per,2)),'values_'+str(round(1-cum_per,2))])\n    for col in sel_col:\n        col_value = train_transaction[col].value_counts(normalize=True)\n        colpercount = pd.DataFrame({'value' : col_value.index,'per_count' : col_value.values})\n        colpercount['cum_per_count'] = colpercount['per_count'].cumsum()\n        if len(colpercount.loc[colpercount['cum_per_count'] < cum_per,] ) < 2:\n            values_freq = colpercount.loc[colpercount['per_count'] > (1- cum_per),'value'].tolist()\n        else:\n            values_freq = colpercount.loc[colpercount['cum_per_count']< cum_per,'value'].tolist() \n        values_less_freq =  [item for item in colpercount['value'] if item not in values_freq]\n        dfpercount=dfpercount.append({'col_name': col,'values_'+str(round(cum_per,2)) : values_freq ,'values_'+str(round(1-cum_per,2)): values_less_freq},ignore_index = True)\n    num_values_per =[]\n    for i in range(len(dfpercount)):\n        num_values_per.append(len(dfpercount['values_'+str(round(cum_per,2))][i]))\n    dfpercount['num_values_per'] = num_values_per\n    return dfpercount","5b35a6ed":"def vcol_multiplot(col,cum_per,ax1):\n    col_freq = column_value_freq(col,cum_per)      \n    plot1=col_freq.plot(x='col_name',y=['unique_values','num_values_'+str(round(cum_per,2))],kind='bar',rot=90,ax = ax1)\n    for p in plot1.patches[1:]:\n        h = p.get_height()\n        x = p.get_x()+p.get_width()\/2.\n        if h != 0:\n            plot1.annotate(\"%g\" % p.get_height(), xy=(x,h), xytext=(0,4), rotation=90, \n                   textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\n    plot1.set(ylabel='Count')\n    plot1= plot1.set(title='Data Details  in each V columns with ' + str(round(col_freq.percent_missing.mean(),4)) +'% missing values')\n    \ndef vcol_plot(col,cum_per):\n    col_freq = column_value_freq(col,cum_per)      \n    plot1=col_freq.plot(x='col_name',y=['unique_values','num_values_'+str(round(cum_per,2))],kind='bar',rot=90)\n    for p in plot1.patches[1:]:\n        h = p.get_height()\n        x = p.get_x()+p.get_width()\/2.\n        if h != 0:\n            plot1.annotate(\"%g\" % p.get_height(), xy=(x,h), xytext=(0,4), rotation=90, \n                   textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\n    plot1.set(ylabel='Count')\n    plot1= plot1.set(title='Data Details  in each V columns with ' + str(round(col_freq.percent_missing.mean(),4)) +'% missing values')","1bc8c210":"cum_per = 0.965\nfig, axs = plt.subplots(2,1, figsize=(15, 16), facecolor='w', edgecolor='k',squeeze=False)\naxs=axs.ravel()\nvcol_multiplot(train_transaction_vcol_na_group.column_groups[0],cum_per,axs[0])\nvcol_multiplot(train_transaction_vcol_na_group.column_groups[1],cum_per,axs[1])","7124eb26":"fig, axs = plt.subplots(4,2, figsize=(15,16), facecolor='w', edgecolor='k',squeeze=False)\n#fig.subplots_adjust(hspace = 0.75, wspace=.001)\naxs = axs.ravel()\nfor i in range(2,10):\n    vcol_multiplot(train_transaction_vcol_na_group.column_groups[i],cum_per,axs[i-2])\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","5030c6e6":"fig, axs = plt.subplots(5,1, figsize=(15,16), facecolor='w', edgecolor='k',squeeze=False)\naxs=axs.ravel()\nvcol_multiplot(train_transaction_vcol_na_group.column_groups[10],cum_per,axs[0])\nvcol_multiplot(train_transaction_vcol_na_group.column_groups[11],cum_per,axs[1])\nvcol_multiplot(train_transaction_vcol_na_group.column_groups[12],cum_per,axs[2])\nvcol_multiplot(train_transaction_vcol_na_group.column_groups[13],cum_per,axs[3])\nvcol_multiplot(train_transaction_vcol_na_group.column_groups[14],cum_per,axs[4])\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","c46dac0b":"colfreq=column_value_freq(Vcols,cum_per)\ncolfreqbool = colfreq[colfreq.unique_values==2]\nif len(colfreqbool)%3 == 0:\n    nrow = len(colfreqbool)\/3\nelse:\n    nrow = len(colfreqbool) \/\/ 3 + 1 \nsns.set(rc={'figure.figsize':(14,16)})\nfor num, alpha in enumerate(colfreqbool.col_name):\n    plt.subplot(nrow, 3, num+1)\n    plot1= sns.countplot(data=train_transaction,x=alpha,hue='isFraud')\n    for p in plot1.patches[1:]:\n        h = p.get_height()\n        x = p.get_x()+p.get_width()\/2.\n        if h != 0:\n            plot1.annotate(\"%g\" % p.get_height(), xy=(x,h), xytext=(0,4), rotation=90, \n                   textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\n    plt.legend(title='isFraud',loc='upper right')\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","bfef6dae":"def cum_value_count(col):\n    col_value = train_transaction[col].value_counts(normalize=True)\n    colpercount = pd.DataFrame({'value' : col_value.index,'per_count' : col_value.values})\n    colpercount['cum_per_count'] = colpercount['per_count'].cumsum()\n    return colpercount","25b15a5c":"def V_doublecat_plot(cols,cum_per,limit):\n    Vcol_details=column_value_details(cols,cum_per)\n    V_cat = Vcol_details[Vcol_details['num_values_per'] <= limit].reset_index()\n    sns.set(rc={'figure.figsize':(14,len(V_cat)*2)})\n    x=1\n    for num, alpha in enumerate(V_cat.col_name):\n        plt.subplot(len(V_cat),2,x)\n        sns.countplot(data=train_transaction[train_transaction[alpha].isin (V_cat['values_'+str(round(cum_per,2))][num])],y=alpha,hue='isFraud')\n        plt.legend(loc='lower right')\n        plt.title('Count of unique values which make '+str(round(cum_per*100,3))+'% of data in column ' + str(alpha) )\n        plt.subplot(len(V_cat),2,x+1)\n        sns.countplot(data=train_transaction[train_transaction[alpha].isin (V_cat['values_'+str(round(1-cum_per,2))][num])],y=alpha,hue='isFraud')\n        plt.legend(loc='lower right')\n        plt.title('Count of unique values which make only '+str(round((1-cum_per)*100,3))+'% of data in column ' + str(alpha) )\n        x= x+2\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","52687d5c":"def V_cat_plot(cols,cum_per,limit):\n    Vcol_details=column_value_details(cols,cum_per)\n    V_cat = Vcol_details[Vcol_details['num_values_per'] <= limit].reset_index()\n    sns.set(rc={'figure.figsize':(14,len(V_cat)*2)})\n    x=1\n    for num, alpha in enumerate(V_cat.col_name):\n        plt.subplot(len(V_cat),2,x)\n        sns.countplot(data=train_transaction[train_transaction[alpha].isin (V_cat['values_'+str(round(cum_per,2))][num])],y=alpha,hue='isFraud')\n        plt.legend(loc='lower right')\n        plt.title('Count of unique values which make '+str(round(cum_per*100,3))+'% of data in column ' + str(alpha) )\n        plt.subplot(len(V_cat),2,x+1)\n        yes = train_transaction[(train_transaction['isFraud'] == 1) & (train_transaction[alpha].isin (V_cat['values_'+str(round(1-cum_per,2))][num]))][alpha]\n        no = train_transaction[(train_transaction['isFraud'] == 0) & (train_transaction[alpha].isin (V_cat['values_'+str(round(1-cum_per,2))][num]))][alpha]\n        plt.hist(yes, alpha=0.75, label='Fraud', color='r')\n        plt.hist(no, alpha=0.25, label='Not Fraud', color='g')\n        plt.legend(loc='upper right')\n        plt.title('Histogram of values which make '+str(round((1-cum_per)*100,3))+'% of data in column ' + str(alpha) )\n        x= x+2\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","804c66d4":"def V_num_plot(cols,cum_per,limit):\n    Vcol_details=column_value_details(cols,cum_per)\n    V_num = Vcol_details[Vcol_details['num_values_per'] > limit].reset_index()\n    sns.set(rc={'figure.figsize':(14,len(V_num)*2)})\n    x=1\n    for num, alpha in enumerate(V_num.col_name):\n        plt.subplot(len(V_num),2,x)\n        yes = train_transaction[(train_transaction['isFraud'] == 1) & (train_transaction[alpha].isin (V_num['values_'+str(round(cum_per,2))][num]))][alpha]\n        no = train_transaction[(train_transaction['isFraud'] == 0) & (train_transaction[alpha].isin (V_num['values_'+str(round(cum_per,2))][num]))][alpha]\n        plt.hist(yes, alpha=0.75, label='Fraud', color='r')\n        plt.hist(no, alpha=0.25, label='Not Fraud', color='g')\n        plt.legend(loc='upper right')\n        plt.title('Histogram of  values which make '+str(round(cum_per*100,3))+'% of data in column ' + str(alpha) )\n        plt.subplot(len(V_num),2,x+1)\n        yes = train_transaction[(train_transaction['isFraud'] == 1) & (train_transaction[alpha].isin (V_num['values_'+str(round(1-cum_per,2))][num]))][alpha]\n        no = train_transaction[(train_transaction['isFraud'] == 0) & (train_transaction[alpha].isin (V_num['values_'+str(round(1-cum_per,2))][num]))][alpha]\n        plt.hist(yes, alpha=0.75, label='Fraud', color='r')\n        plt.hist(no, alpha=0.25, label='Not Fraud', color='g')\n        plt.legend(loc='upper right')\n        plt.title('Histogram of values which make '+str(round((1-cum_per)*100,3))+'% of data in column ' + str(alpha) )\n        x= x+2\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n","8e7bea83":"colfreqpseudobool = colfreq[(colfreq.unique_values !=2) & (colfreq['num_values_'+str(round(cum_per,2))] <= 2)]","a9e14f29":"pseudoboolcat = colfreqpseudobool[colfreqpseudobool.unique_values <=15]['col_name'].values\nV_doublecat_plot(pseudoboolcat,cum_per,15)","d29ced0e":"pseudoboolnum = colfreqpseudobool[colfreqpseudobool.unique_values >15]['col_name'].values\n","4dc900d6":"V_cat_plot(pseudoboolnum,cum_per,15)\n","bb3c69ae":"colfreqcat = colfreq[(colfreq.unique_values <=15) & (colfreq['num_values_'+str(round(cum_per,2))] > 2)]\ncolfreqcat","6f4c181e":"colfreqpseudocat = colfreq[(colfreq.unique_values >15) & (colfreq['num_values_'+str(round(cum_per,2))] <= 15) & (colfreq['num_values_'+str(round(cum_per,2))]> 2)]","c1ef1c4f":"V_cat_plot(colfreqpseudocat.col_name,cum_per,15)","027b1e8a":"colfreqnum = colfreq[colfreq['num_values_'+str(round(cum_per,2))]>15]","2d61efb2":"V_num_plot(colfreqnum.col_name,cum_per,15)","a236eb6d":"id_cols = [c for c in train if c[0] == 'i']\ntrain[id_cols].head()","ae991bf0":"train[id_cols].describe(include = 'all')","7a8d66c4":"train.groupby('DeviceType') \\\n    .mean()['isFraud'] \\\n    .sort_values() \\\n    .plot(kind='barh',\n          figsize=(15, 5),\n          title='Percentage of Fraud by Device Type')\nplt.show()","c124ceb1":"train.groupby('DeviceInfo') \\\n    .count()['TransactionID'] \\\n    .sort_values(ascending=False) \\\n    .head(20) \\\n    .plot(kind='barh', figsize=(15, 5), title='Top 20 Devices in Train')\nplt.show()","e4faa2cf":"def cat_feat_ploting(df, col):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.figure(figsize=(14,10))\n    plt.suptitle(f'{col} Distributions', fontsize=22)\n\n    plt.subplot(221)\n    g = sns.countplot(x=col, data=df, order=tmp[col].values)\n    # plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n\n    g.set_title(f\"{col} Distribution\", fontsize=19)\n    g.set_xlabel(f\"{col} Name\", fontsize=17)\n    g.set_ylabel(\"Count\", fontsize=17)\n    # g.set_ylim(0,500000)\n    for p in g.patches:\n        height = p.get_height()\n        g.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\", fontsize=14) \n\n    plt.subplot(222)\n    g1 = sns.countplot(x=col, hue='isFraud', data=df, order=tmp[col].values)\n    plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\n    gt = g1.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, color='black', order=tmp[col].values, legend=False)\n    gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n\n    g1.set_title(f\"{col} by Target(isFraud)\", fontsize=19)\n    g1.set_xlabel(f\"{col} Name\", fontsize=17)\n    g1.set_ylabel(\"Count\", fontsize=17)\n\n    plt.subplot(212)\n    g3 = sns.boxenplot(x=col, y='TransactionAmt', hue='isFraud', \n                       data=df[df['TransactionAmt'] <= 2000], order=tmp[col].values )\n    g3.set_title(\"Transaction Amount Distribuition by ProductCD and Target\", fontsize=20)\n    g3.set_xlabel(\"ProductCD Name\", fontsize=17)\n    g3.set_ylabel(\"Transaction Values\", fontsize=17)\n\n    plt.subplots_adjust(hspace = 0.4, top = 0.85)\n\n    plt.show()","2a7c95e1":"for col in ['id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29']:\n    train[col] = train[col].fillna('NaN')\n    cat_feat_ploting(train, col)","6e1cfb5d":"train.loc[train['id_30'].str.contains('Windows', na=False), 'id_30'] = 'Windows'\ntrain.loc[train['id_30'].str.contains('iOS', na=False), 'id_30'] = 'iOS'\ntrain.loc[train['id_30'].str.contains('Mac OS', na=False), 'id_30'] = 'Mac'\ntrain.loc[train['id_30'].str.contains('Android', na=False), 'id_30'] = 'Android'\ntrain['id_30'].fillna(\"NaN\", inplace=True)","ded10d72":"ploting_cnt_amt(train, 'id_30')","18f6cd03":"train.loc[train['id_31'].str.contains('chrome', na=False), 'id_31'] = 'Chrome'\ntrain.loc[train['id_31'].str.contains('firefox', na=False), 'id_31'] = 'Firefox'\ntrain.loc[train['id_31'].str.contains('safari', na=False), 'id_31'] = 'Safari'\ntrain.loc[train['id_31'].str.contains('edge', na=False), 'id_31'] = 'Edge'\ntrain.loc[train['id_31'].str.contains('ie', na=False), 'id_31'] = 'IE'\ntrain.loc[train['id_31'].str.contains('samsung', na=False), 'id_31'] = 'Samsung'\ntrain.loc[train['id_31'].str.contains('opera', na=False), 'id_31'] = 'Opera'\ntrain['id_31'].fillna(\"NaN\", inplace=True)\ntrain.loc[train.id_31.isin(train.id_31.value_counts()[train.id_31.value_counts() < 200].index), 'id_31'] = \"Others\"","e5fd445d":"ploting_cnt_amt(train, 'id_31')","a8a641f2":"train['id_31']","492bca21":"**Reference**\n\ncrosstab : https:\/\/twinstarinfo.blogspot.com\/2018\/10\/python-pandascrosstab.html \\\ntwinx : https:\/\/www.delftstack.com\/ko\/howto\/matplotlib\/how-to-add-y-axis-label-to-secondary-y-axis-in-matplotlib\/","0b4d69e7":"Identity CSVs - These will be merged onto the transactions to create additional features","390f37d7":"* **Transaction DT**\n\nAccording to official time, TransactionDT feature is a timedelta from a given reference datetime, not a real time stamp. and Most people estimate the start time as December 1, 2017.","9e6fa07b":"* **D1~D5**","584a4f0f":"**card5**","5c5d3eb0":"**Pseudo Booleans**","893b58de":"**id_30**","4f242966":"**Reference**\n\nsubplots_adjust : https:\/\/www.delftstack.com\/ko\/howto\/matplotlib\/how-to-improve-subplot-size-or-spacing-with-many-subplots-in-matplotlib\/","f16bd568":"Fraudulent charges appear to have a higher average transaction ammount","62c1fcd3":"The most fraudulent transactions were detected on the visa card, with the least American express.\n","5436df24":"**card3**","6b32801c":"* **V1~V339**\n\nEach of the 339 V columns has a low importance and is usually eliminated. To make V columns more useful, understand this column.","13c68733":"There were no fraudulent transaction data except debit and credit cards, and debit cards had more normal transactions than credit cards, but there were also more fraudulent transactions.","9eb8c061":"* **ProductCD**","427a9f2a":"Ploting columns with few unique id values","2c9286ff":"As you can see, Card1 column is given as Categorical but it is behaving like Continuous Data. It has 13553 unique values.","a4fa87a3":"**card1**","e43874a1":"* **C1~C14**","67f8abff":"# **Introduction**\n\n* **Why We Should Care About Payment Fraud?**\n\nPayment card fraud is a serious and long-term threat to society with an economic impact forecast to be $416bn in 2017.\n\nBesides financial losses, it has been identified that criminal enterprises and Organised Crime Groups (OCGs) use payment card fraud to fund their activities including arms, drugs and terrorism. The activities of these criminals include violence and murder--individual acts of fraud have a human cost.\n\nFraud is increasing dramatically with the progression of modern technology and global communication. As a result, fighting fraud has become an important issue to be explored. As presented in the following figure, the detection and prevention mechanisms are used mostly to combat fraud.\n\n\n\n* **What is Fraud Detection ?**\n\nFraud detection tries to discover and identify fraudulent activities as they enter the systems and report them to a system administrator\n\n\n* **Credit card fraud detection**\n\nMostly, the strategy of credit card fraud detection is pattern recognition by analyzing user spending behavior automatically. Customer spending behavior contains information about the transaction amount, time gap since last purchase, day of the week, item category, customer address, etc. Anomaly based fraud detection is mostly used for credit card fraud detection system in which the cardholder's profile is made up by analyzing the cardholder spending behavior pattern. In doing so, any incoming transaction that is inconsistent with the cardholder's profile would be considered as suspicious\n\n\n* **From the competition overview**\n\nIn this competition, you\u2019ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.","030814a3":"Let's take a look at how each V column creates 96.5% of the data in a non-fraud transaction ratio.","2a9bb3ae":"Missing values do not exist in this data.","ca9c9d83":"In some of these columns a higher proportion of fraud cases are seen for values which form less than 3.5% of the column data","74c26955":"The histograms of values less than 3.5% of the column data shows a higher proportion of fraud transactions.","51742b5a":"in Card 3, as we have many values with low frequencies, I decided to set value to \"Others\". Also, in Card 3 I set the % of Fraud ratio in yaxis2\n","d6a05387":"* **addr**\n\nAccording to the name of the feature we can assume that it contains some kind of users address, but in an encoded way. and The data description states that these are categorical even though they look numeric. ","28dc777c":"It looks like the Pseudo Boolean and Pseudo Categorical columns are important as in both tpes there is a higher proportion of fraud cases when the values fall with less than 3.5% of column data unique values","a86d31d6":"**id_31**","97b0e238":"The first grouping is based on the percentage of missing values in the columns . The columns can be divided into 15 groups as below.","a6e82828":"**Distributions**\n\nI will group all e-mail domains by the respective enterprises.\nI will set as \"Others\" all values with less than 300 entries.","43f097a5":"* **emaildomain**","7bd6e4d7":"Test data and train data have **393 columns** excluding isFraud data. Based on the TransactionDT, the test data is assumed to have been created after the train data.","403cab8a":"* **dist**\n\nPerhaps this could be the distance between the cardholder's home\/work address and the transaction.","c42a0fd1":"The most transactions were made through gmail, and fraudulent transaction detection was also the most common in gmail. What's unusual is that iCloud has a high value.","8aba1bec":"It is the value of T, F, or NaN except for M4.","d08ff1a4":"Based on the data distribution columns can be divided into 5 types.\n\n 1. **Boolean** - columns with only two unique values\n\n 2. **Pseudo- Boolean** - columns with 96.5% data covered by maximum two unique values. Within this there are two types.\n \n    Pseudo-Boolean-categorical - Columns with 15 or less unique values but 96.5% data covered by  maximum two unique values\\\n    Pseudo-Boolean-numerical - Columns with more than 15 unique values but 96.5% data covered by  maximum two unique values\n    \n 3. **Pseudo-Categorical** - Columns with 96.5% data covered by 15 or less unique values\n\n 4. **Numerical** - All Other columns","2884477d":"In Card3 we can see that 150 and 185 are the most common values in the column.\nWe have 9.54% of Frauds in 185. The values with highest Fraud Transactions are 185, 119 and 144.","a6df3f8c":"# **EDA for CIS Fraud Detection**","e59c2a2a":"* **M1~M9**","66ae26b2":"As you can see, Most transaction data is non-fraud. Fraud transaction is 3.5%, which is unbalanced. Therefore, attention should be paid to overfitting problems during the analysis process.","27cfb55a":"**Boolean Columns**","7bd074e1":"Almost all entries in Addr2 are in the same value.\nInterestingly in the value 65 , the percent of frauds are almost 60%\nAltought the value 87 has 88% of total entries, it has 96% of Total Transaction Amounts","075f073e":"2. **Data road**","bf4c674f":"* **Card**","2a7d1b2f":"**Reference**\n\nisin : https:\/\/3months.tistory.com\/283 \\\nunique, value_counts : https:\/\/rfriend.tistory.com\/267","c926d494":"It can be seen that dist1-2 contains many missing values.\n","17a0444e":"With the exception of V305, it has values of 0 and 1, with most values of 1.","623b4a89":"24.4% of TransactionIDs in train (144233 \/ 590540) have an associated train_identity.  \\\n28.0% of TransactionIDs in test (141907 \/ 506691) have an associated train_identity.","214f9c11":"**card2**","460f5029":"* **TransactionAmt**\n\nData representing the amount of transactions. To avoid skwness of the transaction distribution, it is represented using log transformations Because of the log transfrom, any values between 0 and 1 will appear to be negative.","b2094b48":"In Card5 the most frequent values are 226, 224, 166 that represents 73% of data. Also is posible to see high % of frauds in 137, 147, 141 that has few entries for values.","05d2f645":"**DeviceType**","d11ca27f":"Except for M4, the remaining M values show a high fraud detection rate in missing values.","c01fda56":"**P emaildomain**\n\nI will group all e-mail domains by the respective enterprises.\nAlso, I will set as \"Others\" all values with less than 500 entries.","2309ead7":"# **Reference**\n\n* Introduction\n\n 1. https:\/\/www.kaggle.com\/robikscube\/ieee-fraud-detection-first-look-and-eda\n 2. https:\/\/www.kaggle.com\/haataa\/complete-eda-with-background-knowledge\n \n \n* EDA\n\n 1. https:\/\/www.kaggle.com\/robikscube\/ieee-fraud-detection-first-look-and-eda\n 2. https:\/\/www.kaggle.com\/artgor\/eda-and-models \n 3. https:\/\/www.kaggle.com\/nroman\/eda-for-cis-fraud-detection\n 4. https:\/\/www.kaggle.com\/jesucristo\/fraud-complete-eda\n 5. https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt\n 6. https:\/\/www.kaggle.com\/rajeshcv\/understanding-v-columns","635b5b53":"Use logx to plot the distribution better.","5cb40e4b":"* **id_1 ~ id_38**\n\nid data including customer ID information.\n\n  * Categorical Features\n  * DeviceType\n  * DeviceInfo\n  * id_12 - id_38","b2ab7b88":"All datasets have missing values, which are interpreted as common in the real world.","6210d20c":"1. **Import necessary librairies for EDA**","a34ebc5c":"For now we don't know exactly what these values represent.\n\nW has the most number of observations, S the least.\n\nProductCD C has the most fraud with >12%\n\nProductCD W has the least with ~2%","0b49994f":"It can be seen that some unique values belong to 3.5%.","d2149c53":"**card4**","acb6d657":"* **Data loading and overview**\n\n\nIn the competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n\nData is separated into two datasets: information about the identity of the customer and transaction information. Not all transactions belong to identities, which are available. Maybe it would be possible to use additional transactions to generate new features.","2860d774":"**Reference**\n\nset_index : https:\/\/kongdols-room.tistory.com\/123 \\\nresample : https:\/\/rfriend.tistory.com\/494","7861de52":"Since many NaN values exist, we visualize them by replacing them with Miss."}}