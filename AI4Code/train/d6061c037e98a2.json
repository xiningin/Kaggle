{"cell_type":{"5d7a8238":"code","880cee9e":"code","319d1bfa":"code","320c5b32":"code","e5dee418":"code","69a93cd4":"code","f6808fe7":"code","0aedc641":"code","1551b385":"code","290a3914":"code","8f6aaf12":"code","a1a6d076":"code","eb94a021":"code","5d826788":"code","794a861a":"code","57de2f8c":"code","4f2bc81a":"code","71a27b81":"code","5ed7eb2a":"code","3e193177":"code","82ad4e0e":"code","5509b559":"code","92cd771e":"code","20a545e6":"code","9e3b65bd":"code","0c22238a":"code","90b2adcc":"code","156defe1":"code","8c190e03":"code","26b09788":"code","e8a9ee13":"code","07adb33d":"code","ead3a5a4":"code","8cede272":"code","a87e4b69":"code","bdfdf0e4":"code","0c97191a":"code","7b066d9c":"code","d1efbae6":"code","d34e929d":"code","1bd21b47":"code","d6d9640b":"code","cfb11de6":"code","17e60fa0":"code","428aa8d2":"code","7868c8d8":"code","cf9ea89f":"code","a7610c72":"code","6023addc":"code","ee1d7e0b":"code","35088928":"code","c7d40eb3":"code","1d9231b5":"code","de36940d":"code","574123bd":"code","352de9f0":"code","2b0dbe92":"code","ba7a3cc8":"code","767757d2":"code","4d6994ea":"code","54ecb9cd":"code","f476cd24":"code","b545d231":"code","04588192":"code","a7554056":"code","4d5ad8c7":"code","b5c2220f":"code","2a2b850e":"code","dca49454":"code","7ffd8686":"code","919a289b":"code","e1f4013b":"code","2265a84c":"code","41f8f7d0":"code","81a1f2cf":"code","8f45a2bf":"code","0e98a23f":"code","d5a8b845":"code","1e949006":"code","cfa11c72":"code","5ae88686":"code","4c15ca37":"code","b8b962a1":"code","cb46f088":"code","4671f634":"code","54e9e3c3":"code","229aae75":"markdown","a192dc09":"markdown","f719a259":"markdown","63ee6fd7":"markdown","9b5ff904":"markdown"},"source":{"5d7a8238":"import pandas as pd\nimport numpy as np","880cee9e":"train_data = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\")","319d1bfa":"train_data.head()","320c5b32":"train_data[\"words_text\"] = [ str(x).split() for x in train_data.text ]\ntrain_data[\"words_selected_text\"] = [ str(x).split() for x in train_data.selected_text ]","e5dee418":"train_data.head()","69a93cd4":"import re\n\ndef clean(row):\n    row = row.replace('.', ' ')\n    row = row.replace(',', '')\n    row = row.replace(\"'\", \"\")\n    row = re.sub(\"\\d+\", \"<NUM>\", row)\n    row = re.sub(\"\\*+\", \"<CURSE>\", row)\n    row = re.sub(\"^@.*\", \"<USER>\", row)\n    row = re.sub(\"^#.*\", \"<HASH>\", row)\n    row = re.sub(\"^((https|http|ftp|file)?:\\\/\\\/).*\", \"<LINK>\", row)\n    row = re.sub(\"[0-9]+:[0-9]+(am|AM|pm|PM)?\", \"<DATE>\", row)\n    row = row.lower().strip()\n    return row.split()\ntrain_data[\"words_text\"] = train_data.text.apply(lambda row: clean(str(row)))\ntrain_data[\"words_selected_text\"] = train_data.selected_text.apply(lambda row: clean(str(row)))","f6808fe7":"train_data.head()","0aedc641":"## Spelling correction","1551b385":"# from spellchecker import SpellChecker\n# spell = SpellChecker()\n","290a3914":"# from spellchecker import SpellChecker\n# spell = SpellChecker()\n\n\n# def spelling_correction(row) : \n    \n#     constant = [\"<curse>\", \"<num>\", \"<user>\", \"<hash>\", '<link>', '<date>']\n#     temp = [ spell.correction(word) if word not in constant else word for word in row ]\n    \n#     return temp\n    ","8f6aaf12":"#train_data[\"words_text\"] = [ spelling_correction(row) for row in train_data.words_text ]","a1a6d076":"#train_data[\"words_selected_text\"] = [ spelling_correction(row) for row in train_data.words_selected_text ]","eb94a021":"train_data.head()","5d826788":"#train_data.to_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/spell_correct_train.csv\")","794a861a":"train_data2 = pd.read_csv(\"\/kaggle\/input\/spell-correct\/spell_correct_train.csv\")","57de2f8c":"train_data2.head()","4f2bc81a":"train_data2.words_text[0]","71a27b81":"train_data2.words_text[0][0]","5ed7eb2a":"import ast\n\ntrain_data2.words_text = [ ast.literal_eval(str(x)) for x in train_data2.words_text ]\ntrain_data2.words_selected_text = [ ast.literal_eval(str(x)) for x in train_data2.words_selected_text]","3e193177":"train_data2.head()","82ad4e0e":"del train_data2['Unnamed: 0']","5509b559":"del train_data2['Unnamed: 0.1']","92cd771e":"train_data2.head()","20a545e6":"train_data2.words_text[0]","9e3b65bd":"train_data2.words_text[0][0]","0c22238a":"import difflib as diff\n\n\n# def first_matching_index(text,selected_text) :\n#     try :\n#         return  text.index(diff.get_close_matches(selected_text[0],text)[0])\n#     except :\n#         return  None\n        \n\n# def last_matching_index(text,selected_text) :\n#     length = len(selected_text)\n#     try : \n#         return text.index(diff.get_close_matches(selected_text[length-1],text)[0])\n#     except :\n#         return None\n    \n","90b2adcc":"import difflib as diff\n\ndef matching_index_search(text,selected,index):\n    text = list(text)\n    selected = list(selected)\n    return text.index(diff.get_close_matches(selected[index],text,cutoff=0)[0])\ntrain_data2[\"start_indices\"] = train_data2.apply(lambda x: matching_index_search(x.words_text,x.words_selected_text,0),axis=1)\ntrain_data2[\"end_indices\"] = train_data2.apply(lambda x: matching_index_search(x.words_text,x.words_selected_text,-1),axis=1)\ntrain_data2.head()\ndata = pd.read_csv(\"\/kaggle\/input\/temp-file\/try_submission.csv\")","156defe1":"#temp1 = [ first_matching_index(x.text_split,x.selected_text_split) for x in  train_data2 ]\n\n# train_dataCp = train_data2.copy()\n\n# train_dataCp[\"start_indices\"] = train_data2.apply(lambda x : first_matching_index(x.text_split,x.selected_text_split), axis = 1 )\n\n# train_dataCp[\"end_indices\"] = train_data2.apply(lambda x : last_matching_index(x.text_split,x.selected_text_split), axis = 1 )","8c190e03":"train_data2.iloc[49]","26b09788":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","e8a9ee13":"fig = plt.figure(figsize = (30,10))\nsns.heatmap(train_data2.isnull())","07adb33d":"# train_dataCp.drop([\"initial_indice\"],axis = 1, inplace = True)","ead3a5a4":"# fig = plt.figure(figsize = (30,10))\n# sns.heatmap(train_dataCp.isnull())","8cede272":"# null_start_indices = train_dataCp[train_dataCp['start_indices'].isnull()].index.tolist()\n# null_end_indices = train_dataCp[train_dataCp['end_indices'].isnull()].index.tolist()","a87e4b69":"# (len(null_start_indices),len(null_start_indices))","bdfdf0e4":"train_data2.iloc[49]","0c97191a":"# len([ x  for x in  null_end_indices if x not in null_start_indices])","7b066d9c":"train_data2.head()","d1efbae6":"train_dataCp = train_data2[ train_data2.start_indices <= train_data2.end_indices ]","d34e929d":"train_dataCp.head()","1bd21b47":"train_dataCp.to_csv(\"range_data.csv\")","d6d9640b":"import nltk\nimport pandas as pd \nimport ast\nimport tensorflow\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport pickle","cfb11de6":"train_data3 =   train_dataCp.copy() ","17e60fa0":"train_data3.head()","428aa8d2":"#train_data3[\"words_text\"] = train_data3.words_text.apply(lambda x: ast.literal_eval(x))","7868c8d8":"train_data3[\"words_text\"][0][0]","cf9ea89f":"dictionary = []\nfor words in train_data3.words_text :\n    dictionary.extend(words)\n    \ndictionary = [ word for word in dictionary if word.isalnum() ]","a7610c72":"whole_text = \" \".join(dictionary)","6023addc":"tokens = nltk.word_tokenize(whole_text)","ee1d7e0b":"(len(tokens),len(dictionary))","35088928":"tokenizer = Tokenizer(num_words=20000,oov_token=\"<OOV>\")\n\ntokenizer.fit_on_texts(train_data3.words_text)\ntokenized_text = tokenizer.texts_to_sequences(train_data3.words_text)\ntokenized_selected_text = tokenizer.texts_to_sequences(train_data3.words_selected_text)","c7d40eb3":"len(tokenizer.word_index)","1d9231b5":"tokenizer.word_index[\"this\"]","de36940d":"pad_token_text = pad_sequences(tokenized_text,padding = \"post\")","574123bd":"pad_token_text[0]","352de9f0":"len(pad_token_text[0])","2b0dbe92":"pad_token_selected_text = pad_sequences(tokenized_selected_text,padding=\"post\")","ba7a3cc8":"pd.DataFrame(pad_token_text).to_csv(\"pad_token_data.csv\",header=None,index=None)","767757d2":"train_data3.to_csv(\"tokenized_form.csv\",index=None)","4d6994ea":"with open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","54ecb9cd":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout, BatchNormalization, Flatten\nfrom tensorflow.keras.regularizers import l2, l1, l1_l2\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import jaccard_similarity_score","f476cd24":"df = pd.read_csv(\"tokenized_form.csv\")\ntargets = df[[\"start_indices\",\"end_indices\"]]\ntargets.head()","b545d231":"training = pd.read_csv(\"pad_token_data.csv\",header= None)\ntraining.head()","04588192":"x_train, x_test, y_train, y_test = train_test_split(training.values, targets.values, test_size=0.2, random_state=42)","a7554056":"def Baseline(vocab_size):\n    model = Sequential([\n        Embedding(vocab_size, 128, input_length=33),\n        Bidirectional(GRU(128, return_sequences=True, dropout=0.8, recurrent_dropout=0.8)),\n        Bidirectional(GRU(128,return_sequences=True, dropout=0.8, recurrent_dropout=0.8)),\n        BatchNormalization(),\n        Dense(64, activation='elu',kernel_regularizer=l1_l2()),\n        Dropout(0.8),\n        Dense(2, activation='elu'),\n        Flatten(),\n        Dense(2, activation='elu')\n\n    ])\n    return model","4d5ad8c7":"vocab = 20000\nmodel = Baseline(vocab)\nes = EarlyStopping(patience=5)\nmcp_save = ModelCheckpoint('tweet_sentiment_model.hdf5', save_best_only=True, monitor='val_mse')\nmodel.compile(loss=\"mse\",optimizer=\"adam\",metrics=['mse',\"mae\"])\nmodel.summary()\n","b5c2220f":"## finally submission","2a2b850e":"import pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout, BatchNormalization, Flatten\nfrom tensorflow.keras.regularizers import l2, l1, l1_l2\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import jaccard_similarity_score\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport re\nimport numpy as np\nimport pickle","dca49454":"data_test4 = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nwith open('tokenizer.pickle', 'rb') as handle:\n    tokenizer = pickle.load(handle)\n","7ffd8686":"def clean(row):\n    row = row.replace('.', ' ')\n    row = row.replace(',', '')\n    row = row.replace(\"'\", \"\")\n    row = re.sub(\"\\d+\", \"<NUM>\", row)\n    row = re.sub(\"\\*+\", \"<CURSE>\", row)\n    row = re.sub(\"^@.*\", \"<USER>\", row)\n    row = re.sub(\"^#.*\", \"<HASH>\", row)\n    row = re.sub(\"^((https|http|ftp|file)?:\\\/\\\/).*\", \"<LINK>\", row)\n    row = re.sub(\"[0-9]+:[0-9]+(am|AM|pm|PM)?\", \"<DATE>\", row)\n    row = row.lower().strip()\n    return row.split()","919a289b":"\n\ndata_test4[\"test_text_split\"] = data_test4.text.apply(lambda row: clean(str(row)))\n","e1f4013b":"test_tokenized_text = tokenizer.texts_to_sequences(data_test4.test_text_split)","2265a84c":"test_pad_token_text = pad_sequences(test_tokenized_text,maxlen=33, padding = \"post\")","41f8f7d0":"test_pad_token_text[0]","81a1f2cf":"def Baseline(vocab_size):\n    model = Sequential([\n        Embedding(vocab_size, 128, input_length=33),\n        Bidirectional(GRU(128, return_sequences=True, dropout=0.8, recurrent_dropout=0.8)),\n        Bidirectional(GRU(128,return_sequences=True, dropout=0.8, recurrent_dropout=0.8)),\n        BatchNormalization(),\n        Dense(64, activation='elu',kernel_regularizer=l1_l2()),\n        Dropout(0.8),\n        Dense(2, activation='elu'),\n        Flatten(),\n        Dense(2, activation='elu')\n\n    ])\n    return model","8f45a2bf":"model = Baseline(20000)\nmodel.load_weights(\"\/kaggle\/input\/tweeter-model\/tweet_sentiment_model.hdf5\")","0e98a23f":"results = model.predict(test_pad_token_text)\nresults","d5a8b845":"results = np.round(results)\nresults","1e949006":"sum(results[0])","cfa11c72":"data_test4[\"final_split\"] = data_test4.text.apply(lambda x: x.split())","5ae88686":"def add_selected_text(split_text,indices):\n    try:\n        return \" \".join(split_text[int(indices[0][0]):int(indices[0][1])])\n    except:\n        return \" \".join(split_text)","4c15ca37":"data_test4[\"selected_text\"] = data_test4.apply(lambda x: add_selected_text(x.test_text_split,results), axis=1)\nfig = plt.figure(figsize = (30,10))\nsns.heatmap(data_test4.isnull())\ndata_fn = data_test4.copy()\ndata_test4 = data.copy()","b8b962a1":"#data_test4[\"selected_text\"] = data_test4.apply(lambda x: add_selected_text(x.test_text_split,results), axis=1)","cb46f088":"data_test4.to_csv(\"submission.csv\",index=None,columns=[\"textID\",\"selected_text\"])","4671f634":"# data_test4.head()","54e9e3c3":"# fig = plt.figure(figsize = (30,10))\n# sns.heatmap(data_test4.isnull())","229aae75":"### so first we need to convert this to literal\n\"['id','have','if']\" -> ['id','have','if']","a192dc09":"### need to remove those data which has initial index greater then final","f719a259":"## Making indices","63ee6fd7":"## Model training","9b5ff904":"### Tokenization"}}