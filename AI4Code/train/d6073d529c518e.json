{"cell_type":{"e9a3329b":"code","cad21d9a":"code","a90dad3b":"code","c1787b10":"code","b67ac662":"code","340752b8":"code","b96e55aa":"code","7146f4f6":"code","415f85d8":"code","096f3cb1":"code","69a2b433":"code","6d895e2e":"code","7351e757":"code","25fb2a7f":"code","c0b5256a":"code","6980da8d":"code","8880beb2":"code","3128e961":"code","61f2d195":"code","90a087a0":"code","68f2a9f0":"code","59a923b8":"code","32876bf6":"code","f38f6938":"code","e2cdd48d":"code","8288f498":"markdown","70b7e4b5":"markdown","e2761356":"markdown","da3d9a8a":"markdown"},"source":{"e9a3329b":"import os\nimport glob\nfrom joblib import Parallel, delayed\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)\nimport pickle\nimport matplotlib.pyplot as plt\nimport seaborn as sns","cad21d9a":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn.functional as F\n\nimport random","a90dad3b":"SEED = 2021\n\ndef random_seed(SEED):\n    \n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(SEED)\n        torch.cuda.manual_seed_all(SEED)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nrandom_seed(SEED)","c1787b10":"!pip install ..\/input\/pytorchtabnet\/pytorch_tabnet-3.1.1-py3-none-any.whl","b67ac662":"from pytorch_tabnet.pretraining import TabNetPretrainer\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.metrics import Metric\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\n\nimport pandas as pd\nimport numpy as np\nnp.random.seed(0)\n\nimport os\nfrom pathlib import Path","340752b8":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x \/ y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std]\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df","b96e55aa":"for fold in range(5):\n    !cp -r ..\/input\/optiver-tabnet-50\/tabnet_model_test_{str(fold)}\/* .\n    !zip tabnet_model_test_{str(fold)}.zip model_params.json network.pt\n    \nmodelpath = [os.path.join(\".\/\",s) for s in os.listdir(\".\/\") if (\"zip\" in s)]    ","7146f4f6":"train = pd.read_pickle(\"..\/input\/optiver-lgbm-model\/lgbm_train.pkl\")  \n\nfor col in train.columns.to_list()[4:]:\n    train[col] = train[col].fillna(train[col].mean())\n    \nscales = train.drop(['row_id', 'target', 'time_id',\"stock_id\"], axis = 1).columns.to_list()    \n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(train[scales])\n\nle=LabelEncoder()\nle.fit(train[\"stock_id\"])","415f85d8":"# Read train and test\n_, test = read_train_test()\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntest = get_time_stock(test)\n\n## fillna for test data ##\nfor col in train.columns.to_list()[4:]:\n    test[col] = test[col].fillna(train[col].mean())\n\nx_test = test.drop(['row_id', 'time_id',\"stock_id\"], axis = 1).values\n\n# Transform stock id to a numeric value\nx_test = scaler.transform(x_test)\nX_testdf = pd.DataFrame(x_test)\n\nX_testdf[\"stock_id\"]=test[\"stock_id\"]\n\n# Label encoding\nX_testdf[\"stock_id\"] = le.transform(X_testdf[\"stock_id\"])\n\nx_test = X_testdf.values","096f3cb1":"tabnet_params = dict(\n    n_d = 32,\n    n_a = 32,\n    n_steps = 3,\n    gamma = 1.3,\n    lambda_sparse = 0,\n    optimizer_fn = optim.Adam,\n    optimizer_params = dict(lr = 1e-2, weight_decay = 1e-5),\n    mask_type = \"entmax\",\n    scheduler_params = dict(\n        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n    scheduler_fn = ReduceLROnPlateau,\n    seed = 42,\n    #verbose = 5,\n    cat_dims=[len(le.classes_)], cat_emb_dim=[10], cat_idxs=[-1] # define categorical features\n)\n\nclf = TabNetRegressor(**tabnet_params)","69a2b433":"preds=[]\nfor path in modelpath:\n    \n    clf.load_model(path)\n    preds.append(clf.predict(x_test).squeeze(-1))\n    \nmodel1_predictions = np.mean(preds,axis=0)","6d895e2e":"import gc\nimport joblib","7351e757":"MODEL_DIR = '..\/input\/optiver-lgbm-model\/'\n\ntrain = pd.read_pickle(MODEL_DIR + 'lgbm_train.pkl')\n\ntrain.shape","25fb2a7f":"# Function to read our base train and test set\ndef read_test():\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    \n    # Create a key to merge with book and trade data\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    \n    return test\n\n# Read test\ntest = read_test()\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\ntest = get_time_stock(test)","c0b5256a":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef evaluate(train, test):\n    \n    # Split features and target\n    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n    y = train['target']\n    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n    \n    # Transform stock id to a numeric value\n    x['stock_id'] = x['stock_id'].astype(int)\n    x_test['stock_id'] = x_test['stock_id'].astype(int)\n    \n    # Create out of folds array\n    oof_predictions = np.zeros(x.shape[0])\n    \n    # Create test array to store predictions\n    test_predictions = np.zeros(x_test.shape[0])\n    \n    SEEDS = [42, 66]\n    \n    TEST_PREDICTIONS = np.zeros(x_test.shape[0])\n    \n    for seed in SEEDS:\n        \n        # Create out of folds array\n        oof_predictions = np.zeros(x.shape[0])\n\n        # Create test array to store predictions\n        test_predictions = np.zeros(x_test.shape[0])  \n        \n        num_folds = 5\n        \n        # Create a KFold object\n        kfold = KFold(n_splits = num_folds, random_state = 66, shuffle = True)\n    \n        # Iterate through each fold\n        for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n            \n            print(f'Evaluating_seed_{seed}_fold_{fold + 1}')\n            \n            x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n            y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n\n            model = joblib.load(MODEL_DIR + f'lgbm_seed_{seed}_model_{fold+1}.pkl')\n\n            # Add predictions to the out of folds array\n            oof_predictions[val_ind] = model.predict(x_val)\n\n            # Predict the test set\n            test_predictions += model.predict(x_test) \/ num_folds\n\n            del model\n            gc.collect()\n\n        rmspe_score = rmspe(y, oof_predictions)\n        print(f'\\nOur out of folds RMSPE for seed {seed} is {rmspe_score}\\n')\n        \n        TEST_PREDICTIONS += test_predictions \/ len(SEEDS)\n        \n    # Return test predictions\n    return TEST_PREDICTIONS","6980da8d":"# Traing and evaluate\nmodel2_predictions = evaluate(train, test)","8880beb2":"import pandas as pd\nfrom catboost import Pool, CatBoostRegressor","3128e961":"DATA_DIR = '..\/input\/optiver-lgbm-model\/'\nMODEL_DIR = '..\/input\/optiver-cb\/'\n\ntrain = pd.read_pickle(DATA_DIR + 'lgbm_train.pkl')\n\n# Function to read our base train and test set\ndef read_test():\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    \n    # Create a key to merge with book and trade data\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    \n    return test\n\n# Read test\ntest = read_test()\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\ntest = get_time_stock(test)","61f2d195":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef evaluate(train, test):\n    \n    # Split features and target\n    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n    y = train['target']\n    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n    \n    # Transform stock id to a numeric value\n    x['stock_id'] = x['stock_id'].astype(int)\n    x_test['stock_id'] = x_test['stock_id'].astype(int)\n    \n    # Create out of folds array\n    oof_predictions = np.zeros(x.shape[0])\n    \n    # Create test array to store predictions\n    test_predictions = np.zeros(x_test.shape[0])\n    \n    SEEDS = [42]\n    \n    TEST_PREDICTIONS = np.zeros(x_test.shape[0])\n    \n    for seed in SEEDS:\n        \n        # Create out of folds array\n        oof_predictions = np.zeros(x.shape[0])\n\n        # Create test array to store predictions\n        test_predictions = np.zeros(x_test.shape[0])  \n        \n        num_folds = 5\n        \n        # Create a KFold object\n        kfold = KFold(n_splits = num_folds, random_state = 66, shuffle = True)\n    \n        # Iterate through each fold\n        for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n            \n            print(f'Evaluating_seed_{seed}_fold_{fold + 1}')\n            \n            x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n            y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n            \n            # Root mean squared percentage error weights\n            train_pool = Pool(x_train, y_train)\n            val_pool = Pool(x_val, y_val)\n            test_pool = Pool(x_test) \n\n            model = joblib.load(MODEL_DIR + f'cb_model_{fold+1}.pkl')\n\n            # Add predictions to the out of folds array\n            oof_predictions[val_ind] = model.predict(val_pool)\n\n            # Predict the test set\n            test_predictions += model.predict(test_pool) \/ num_folds\n\n            del model\n            gc.collect()\n\n        rmspe_score = rmspe(y, oof_predictions)\n        print(f'\\nOur out of folds RMSPE for seed {seed} is {rmspe_score}\\n')\n        \n        TEST_PREDICTIONS += test_predictions \/ len(SEEDS)\n        \n    # Return test predictions\n    return TEST_PREDICTIONS","90a087a0":"# Traing and evaluate\nmodel3_predictions = evaluate(train, test)","68f2a9f0":"pd.DataFrame(np.vstack((model1_predictions, model2_predictions, model3_predictions)).transpose(), columns=['model1','model2','model3'])","59a923b8":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\nfrom numpy.random import seed\nseed(2021)\nimport tensorflow as tf\ntf.random.set_seed(2021)\nfrom tensorflow import keras\nimport numpy as np\nfrom keras import backend as K\n\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x \/ y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n#     df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n#     df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n#     df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n#     df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n#     df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n#     df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n#     df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n#     df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n#     df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n#     df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n#     vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility',\n#                 'log_return1_realized_volatility_600', 'log_return2_realized_volatility_600', \n#                 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400',\n# #                 'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', \n#                 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200',\n# #                 'log_return1_realized_volatility_100', 'log_return2_realized_volatility_100', \n#                 'trade_log_return_realized_volatility',\n#                 'trade_log_return_realized_volatility_600', \n#                 'trade_log_return_realized_volatility_400',\n# #                 'trade_log_return_realized_volatility_300',\n# #                 'trade_log_return_realized_volatility_100',\n#                 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n# Read train and test\ntrain, test = read_train_test()\n\n# Get unique stock ids \ntrain_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)\nout_train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\nout_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n\n#out_train[out_train.isna().any(axis=1)]\nout_train = out_train.fillna(out_train.mean())\nout_train.head()\n\n# code to add the just the read data after first execution\n\n# data separation based on knn ++\nnfolds = 5 # number of folds\nindex = []\ntotDist = []\nvalues = []\n# generates a matriz with the values of \nmat = out_train.values\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\nmat = scaler.fit_transform(mat)\n\nnind = int(mat.shape[0]\/nfolds) # number of individuals\n\n# adds index in the last column\nmat = np.c_[mat,np.arange(mat.shape[0])]\n\n\nlineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n\nlineNumber = np.sort(lineNumber)[::-1]\n\nfor n in range(nfolds):\n    totDist.append(np.zeros(mat.shape[0]-nfolds))\n\n# saves index\nfor n in range(nfolds):\n    \n    values.append([lineNumber[n]])    \n\n\ns=[]\nfor n in range(nfolds):\n    s.append(mat[lineNumber[n],:])\n    \n    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\nfor n in range(nind-1):    \n\n    luck = np.random.uniform(0,1,nfolds)\n    \n    for cycle in range(nfolds):\n         # saves the values of index           \n\n        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n\n        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n        totDist[cycle] += sumDist        \n                \n        # probabilities\n        f = totDist[cycle]\/np.sum(totDist[cycle]) # normalizing the totdist\n        j = 0\n        kn = 0\n        for val in f:\n            j += val        \n            if (j > luck[cycle]): # the column was selected\n                break\n            kn +=1\n        lineNumber[cycle] = kn\n        \n        # delete line of the value added    \n        for n_iter in range(nfolds):\n            \n            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n            j= 0\n        \n        s[cycle] = mat[lineNumber[cycle],:]\n        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\n\nfor n_mod in range(nfolds):\n    values[n_mod] = out_train.index[values[n_mod]]\ndef root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)\/ y_true )))\n    \nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=20, verbose=0,\n    mode='min',restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n    mode='min')\ncolNames = list(train)\n\ncolNames.remove('time_id')\ncolNames.remove('target')\ncolNames.remove('row_id')\ncolNames.remove('stock_id')\n\n\ntrain.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\nqt_train = []\n\nfor col in colNames:\n    #print(col)\n    qt = QuantileTransformer(random_state=21,n_quantiles=200, output_distribution='normal')\n    train[col] = qt.fit_transform(train[[col]])\n    test[col] = qt.transform(test[[col]])    \n    qt_train.append(qt)\nfrom keras.backend import sigmoid\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nget_custom_objects().update({'swish': Activation(swish)})\nhidden_units = (128,64,32)\nstock_embedding_size = 24\n\ncat_data = train['stock_id']\n\ndef base_model():\n    \n    # Each instance will consist of two inputs: a single user id, and a single movie id\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(264,), name='num_data')\n\n\n    #embedding, flatenning and concatenating\n    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    out = keras.layers.Concatenate()([stock_flattened, num_input])\n    \n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n        \n\n    #out = keras.layers.Concatenate()([out, num_input])\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n    \n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out,\n    )\n    \n    return model\nmodel_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 5\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = list(train)\n\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\nfeatures_to_consider.remove('row_id')\n#features_to_consider.remove('pred_NN')\n\n\ntrain[features_to_consider] = train[features_to_consider].fillna(train[features_to_consider].mean())\ntest[features_to_consider] = test[features_to_consider].fillna(train[features_to_consider].mean())\n\ntrain[pred_name] = 0\ntest['target'] = 0\n\n\nfor n_count in range(n_folds):\n    print('CV {}\/{}'.format(counter, n_folds))\n    \n    indexes = np.arange(nfolds).astype(int)    \n    indexes = np.delete(indexes,obj=n_count, axis=0) \n    \n    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n    \n    X_train = train.loc[train.time_id.isin(indexes), features_to_consider]\n    y_train = train.loc[train.time_id.isin(indexes), target_name]\n    X_test = train.loc[train.time_id.isin(values[n_count]), features_to_consider]\n    y_test = train.loc[train.time_id.isin(values[n_count]), target_name]\n    \n    #############################################################################################\n    # NN\n    #############################################################################################\n    \n    model = base_model()\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.005),\n        loss=root_mean_squared_per_error\n    )\n    \n    try:\n        features_to_consider.remove('stock_id')\n    except:\n        pass\n    \n    num_data = X_train[features_to_consider]\n    \n    scaler = MinMaxScaler(feature_range=(-1, 1))         \n    num_data = scaler.fit_transform(num_data.values)    \n    \n    cat_data = X_train['stock_id']    \n    target =  y_train\n    \n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n    cat_data_test = X_test['stock_id']\n\n    model.fit([cat_data, num_data], \n              target,               \n              batch_size=1024,\n              epochs=1000,\n              validation_data=([cat_data_test, num_data_test], y_test),\n              callbacks=[es, plateau],\n              validation_batch_size=len(y_test),\n              shuffle=True,\n             verbose = 1)\n\n    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    \n    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    \n    tt =scaler.transform(test[features_to_consider].values)\n    test[target_name] += model.predict([test['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n       \n    counter += 1\n    features_to_consider.append('stock_id')\ntest[target_name] = test[target_name]\/n_folds\nnn_pre = test[target_name]\nscore = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\nprint('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\ndef train_and_evaluate(train, test):\n    # Hyperparammeters (optimized)\n    seed = 2021\n    params = {\n        'learning_rate': 0.1,        \n        'lambda_l1': 2,\n        'lambda_l2': 7,\n        'num_leaves': 800,\n        'min_sum_hessian_in_leaf': 20,\n        'feature_fraction': 0.8,\n        'feature_fraction_bynode': 0.8,\n        'bagging_fraction': 0.9,\n        'bagging_freq': 42,\n        'min_data_in_leaf': 700,\n        'max_depth': 4,\n        'seed': seed,\n        'feature_fraction_seed': seed,\n        'bagging_seed': seed,\n        'drop_seed': seed,\n        'data_random_seed': seed,\n        'objective': 'rmse',\n        'boosting': 'gbdt',\n        'verbosity': -1,\n        'n_jobs': -1,\n    }   \n    \n    # Split features and target\n    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n    y = train['target']\n    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n    # Transform stock id to a numeric value\n    x['stock_id'] = x['stock_id'].astype(int)\n    x_test['stock_id'] = x_test['stock_id'].astype(int)\n    \n    # Create out of folds array\n    oof_predictions = np.zeros(x.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(x_test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = 10, random_state = 1111, shuffle = True)\n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n        model = joblib.load('..\/input\/stock-embedding-ffnn-lgbm-training\/'+f'model_fold{fold}.pkl')\n        plt.figure(figsize=(12,6))\n        lgb.plot_importance(model, max_num_features=10)\n        plt.title(\"Feature importance\")\n        plt.show()\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val)\n        # Predict the test set\n        test_predictions += model.predict(x_test) \/ 10\n        \n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    # Return test predictions\n    return test_predictions\nfeatures_to_consider.append('row_id' )\nfeatures_to_consider.append('time_id')\n\ntest_predictions = train_and_evaluate(train.loc[:, train.columns != 'pred_NN'], test.loc[:, test.columns != 'target'])\n# Save test predictions\ntest['target2'] = test_predictions","32876bf6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport pathlib\nfrom tqdm.auto import tqdm\nimport json\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport requests as re\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta, FR\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport glob\nimport os\nfrom sklearn import model_selection\nimport joblib\nimport lightgbm as lgb\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nfrom matplotlib_venn import venn2, venn3\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('seaborn-colorblind')\n\nimport warnings\nwarnings.simplefilter('ignore')\n\npd.get_option(\"display.max_columns\")\nDEBUG = False\n# MODE = 'TRAIN'\nMODE = 'INFERENCE'\nMODEL_DIR = '..\/input\/optiver-lgb-and-te-baseline'\nclass CFG:\n    INPUT_DIR = '..\/input\/optiver-realized-volatility-prediction'\n    OUTPUT_DIR = '.\/'\ndef init_logger(log_file='train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nlogger = init_logger(log_file=f'{CFG.OUTPUT_DIR}\/baseline.log')\nlogger.info(f'Start Logging...')\ntrain = pd.read_csv(os.path.join(CFG.INPUT_DIR, 'train.csv'))\n\nlogger.info('Train data: {}'.format(train.shape))\ntest = pd.read_csv(os.path.join(CFG.INPUT_DIR, 'test.csv'))\n\nlogger.info('Test data: {}'.format(test.shape))\nss = pd.read_csv(os.path.join(CFG.INPUT_DIR, 'sample_submission.csv'))\n\nlogger.info('Sample submission: {}'.format(ss.shape))\ntrain_book_stocks = os.listdir(os.path.join(CFG.INPUT_DIR, 'book_train.parquet'))\n\nif DEBUG:\n    logger.info('Debug mode: using 3 stocks only')\n    train_book_stocks = train_book_stocks[:3]\n\nlogger.info('{:,} train book stocks: {}'.format(len(train_book_stocks), train_book_stocks))\n# load stock_id=0\ndef load_book(stock_id=0, data_type='train'):\n    \"\"\"\n    load parquest book data for given stock_id\n    \"\"\"\n    book_df = pd.read_parquet(os.path.join(CFG.INPUT_DIR, f'book_{data_type}.parquet\/stock_id={stock_id}'))\n    book_df['stock_id'] = stock_id\n    book_df['stock_id'] = book_df['stock_id'].astype(np.int8)\n    \n    return book_df\n\ndef load_trade(stock_id=0, data_type='train'):\n    \"\"\"\n    load parquest trade data for given stock_id\n    \"\"\"\n    trade_df = pd.read_parquet(os.path.join(CFG.INPUT_DIR, f'trade_{data_type}.parquet\/stock_id={stock_id}'))\n    trade_df['stock_id'] = stock_id\n    trade_df['stock_id'] = trade_df['stock_id'].astype(np.int8)\n    \n    return trade_df\n\nbook0 = load_book(0)\nlogger.info('Book data of stock id = 1: {}'.format(book0.shape))\ntrade0 = load_trade(0)\nlogger.info('Book data of stock id = 1: {}'.format(trade0.shape))\nbook_df = book0.merge(\n    trade0\n    , how='outer'\n    , on=['time_id', 'stock_id', 'seconds_in_bucket']\n)\n\ndef fix_jsonerr(df):\n    \"\"\"\n    fix json column error for lightgbm\n    \"\"\"\n    df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df.columns]\n    return df\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    series_log_return = log_return(series_log_return)\n    return np.sqrt(np.sum(series_log_return ** 2))\n\ndef fe_row(book):\n    \"\"\"\n    Feature engineering (just volatility for now) for each row\n    \"\"\"\n    \n    # volatility\n    for i in [1, 2, ]:  \n        # wap\n        book[f'book_wap{i}'] = (book[f'bid_price{i}'] * book[f'ask_size{i}'] +\n                        book[f'ask_price{i}'] * book[f'bid_size{i}']) \/ (\n                               book[f'bid_size{i}']+ book[f'ask_size{i}'])\n        \n    # mean wap\n    book['book_wap_mean'] = (book['book_wap1'] + book['book_wap2']) \/ 2\n    \n    # wap diff\n    book['book_wap_diff'] = book['book_wap1'] - book['book_wap2']\n    \n    # other orderbook features\n    book['book_price_spread'] = (book['ask_price1'] - book['bid_price1']) \/ (book['ask_price1'] + book['bid_price1'])\n    book['book_bid_spread'] = book['bid_price1'] - book['bid_price2']\n    book['book_ask_spread'] = book['ask_price1'] - book['ask_price2']\n    book['book_total_volume'] = book['ask_size1'] + book['ask_size2'] + book['bid_size1'] + book['bid_size2']\n    book['book_volume_imbalance'] = (book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2'])\n    \n    return book    \n\ndef fe_agg(book_df):\n    \"\"\"\n    feature engineering (aggregation by stock_id x time_id)   \n    \"\"\" \n            \n    # features\n    book_feats = book_df.columns[book_df.columns.str.startswith('book_')].values.tolist()\n    trade_feats = ['price', 'size', 'order_count', 'seconds_in_bucket']\n        \n    # agg trade features\n    trade_df = book_df.groupby(['time_id', 'stock_id'])[trade_feats].agg([\n        'sum', 'mean', 'std', 'max', 'min'\n    ]).reset_index()\n    \n    # agg volatility features\n    fe_df = book_df.groupby(['time_id', 'stock_id'])[book_feats].agg([\n        realized_volatility\n    ]).reset_index()\n    fe_df.columns = [\" \".join(col).strip() for col in fe_df.columns.values]\n    \n    # merge\n    fe_df = fe_df.merge(\n        trade_df\n        , how='left'\n        , on=['time_id', 'stock_id']\n    )\n    \n    return fe_df\n    \ndef fe_all(book_df):\n    \"\"\"\n    perform feature engineerings\n    \"\"\"\n      \n    # row-wise feature engineering\n    book_df = fe_row(book_df)\n    \n    # feature engineering agg by stock_id x time_id \n    fe_df = fe_agg(book_df)\n    \n    return fe_df\n    \ndef book_fe_by_stock(stock_id=0):\n    \"\"\"\n    load orderbook and trade data for the given stock_id and merge\n    \n    \"\"\"\n    # load data\n    book_df = load_book(stock_id, 'train')\n    trade_df = load_trade(stock_id, 'train')\n    book_feats = book_df.columns.values.tolist()\n    \n    # merge\n    book_df = book_df.merge(\n        trade_df\n        , how='outer'\n        , on=['time_id', 'seconds_in_bucket', 'stock_id']\n    )\n    \n    # sort by time\n    book_df = book_df.sort_values(by=['time_id', 'seconds_in_bucket'])\n    \n    # fillna for book_df\n    book_df[book_feats] = book_df[book_feats].fillna(method='ffill')\n    \n    # feature engineering\n    fe_df = fe_all(book_df)\n    return fe_df\n\ndef book_fe_by_stock_test(stock_id=0):\n    \"\"\"\n    same function but for the test\n    \n    \"\"\"\n    # load data\n    book_df = load_book(stock_id, 'test')\n    trade_df = load_trade(stock_id, 'test')\n    book_feats = book_df.columns.values.tolist()\n    \n    # merge\n    book_df = book_df.merge(\n        trade_df\n        , how='outer'\n        , on=['time_id', 'seconds_in_bucket', 'stock_id']\n    )\n    \n    # sort by time\n    book_df = book_df.sort_values(by=['time_id', 'seconds_in_bucket'])\n    \n    # fillna for book_df\n    book_df[book_feats] = book_df[book_feats].fillna(method='ffill')\n    \n    # feature engineering\n    fe_df = fe_all(book_df)\n    return fe_df\n    \ndef book_fe_all(stock_ids, data_type='train'): \n    \"\"\"\n    Feature engineering with multithread processing\n    \"\"\"\n    # feature engineering agg by stock_id x time_id\n    with Pool(cpu_count()) as p:\n        if data_type == 'train':\n            feature_dfs = list(tqdm(p.imap(book_fe_by_stock, stock_ids), total=len(stock_ids)))\n        elif data_type == 'test':\n            feature_dfs = list(tqdm(p.imap(book_fe_by_stock_test, stock_ids), total=len(stock_ids)))      \n        \n    fe_df = pd.concat(feature_dfs)\n    \n    # feature engineering agg by stock_id\n    vol_feats = [f for f in fe_df.columns if ('realized' in f) & ('wap' in f)]\n    if data_type == 'train':\n        # agg\n        stock_df = fe_df.groupby('stock_id')[vol_feats].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n        \n        # fix column names\n        stock_df.columns = ['stock_id'] + [f'{f}_stock' for f in stock_df.columns.values.tolist()[1:]]        \n        stock_df = fix_jsonerr(stock_df)\n    \n    # feature engineering agg by time_id\n    time_df = fe_df.groupby('time_id')[vol_feats].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    time_df.columns = ['time_id'] + [f'{f}_time' for f in time_df.columns.values.tolist()[1:]]\n    \n    # merge\n    fe_df = fe_df.merge(\n        time_df\n        , how='left'\n        , on='time_id'\n    )\n    \n    # make sure to fix json error for lighgbm\n    fe_df = fix_jsonerr(fe_df)\n    \n    # out\n    if data_type == 'train':\n        return fe_df, stock_df\n    elif data_type == 'test':\n        return fe_df\nif MODE == 'TRAIN':\n    # all book data feature engineering\n    stock_ids = [int(i.split('=')[-1]) for i in train_book_stocks]\n    book_df, stock_df = book_fe_all(stock_ids, data_type='train')\n\n    assert book_df['stock_id'].nunique() > 2\n    assert book_df['time_id'].nunique() > 2\n    \n    # save stock_df for the test\n    stock_df.to_pickle('train_stock_df.pkl')\n    logger.info('train stock df saved!')\n    \n    # merge\n    book_df = book_df.merge(\n        stock_df\n        , how='left'\n        , on='stock_id'\n    ).merge(\n        train\n        , how='left'\n        , on=['stock_id', 'time_id']\n    ).replace([np.inf, -np.inf], np.nan).fillna(method='ffill')\n\n    # make row_id\n    book_df['row_id'] = book_df['stock_id'].astype(str) + '-' + book_df['time_id'].astype(str)\n\nfrom tqdm import tqdm\n# test\ntest_book_stocks = os.listdir(os.path.join(CFG.INPUT_DIR, 'book_test.parquet'))\n\nlogger.info('{:,} test book stocks: {}'.format(len(test_book_stocks), test_book_stocks))\n\n# all book data feature engineering\ntest_stock_ids = [int(i.split('=')[-1]) for i in test_book_stocks]\ntest_book_df = book_fe_all(test_stock_ids, data_type='test')\n\n# load stock_df, if inference\nif MODE == 'INFERENCE':\n    stock_df = pd.read_pickle(f'{MODEL_DIR}\/train_stock_df.pkl')\n    \n# merge\ntest_book_df = test.merge(\n    stock_df\n    , how='left'\n    , on='stock_id'\n).merge(\n    test_book_df\n    , how='left'\n    , on=['stock_id', 'time_id']\n).replace([np.inf, -np.inf], np.nan).fillna(method='ffill')\n\n# make row_id\ntest_book_df['row_id'] = test_book_df['stock_id'].astype(str) + '-' + test_book_df['time_id'].astype(str)\ntarget = 'target'\ndrops = [target, 'row_id', 'time_id']\nfeatures = [f for f in test_book_df.columns.values.tolist() if f not in drops]\ncats = ['stock_id', ]\n\nlogger.info('{:,} features ({:,} categorical): {}'.format(len(features), len(cats), features))\n# evaluation metric\ndef RMSPEMetric(XGBoost=False):\n\n    def RMSPE(yhat, dtrain, XGBoost=XGBoost):\n\n        y = dtrain.get_label()\n        elements = ((y - yhat) \/ y) ** 2\n        if XGBoost:\n            return 'RMSPE', float(np.sqrt(np.sum(elements) \/ len(y)))\n        else:\n            return 'RMSPE', float(np.sqrt(np.sum(elements) \/ len(y))), False\n\n    return RMSPE\n# LightGBM parameters\nparams = {\n    'n_estimators': 20000,\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'learning_rate': 0.01,\n    'subsample': 0.75,\n    'subsample_freq': 4,\n    'feature_fraction': 0.8,\n    'seed':2021,\n    'early_stopping_rounds': 500,\n    'verbose': -1\n} \ndef fit_model(params, X_train, y_train, X_test, features=features, cats=[], era='stock_id', fold_type='kfold', n_fold=5, seed=2021):\n    \"\"\"\n    fit model with cross validation\n    \"\"\"\n    \n    models = []\n    oof_df = X_train[['time_id', 'stock_id', target]].copy()\n    oof_df['pred'] = np.nan\n    y_preds = np.zeros((len(X_test),))\n    \n    if fold_type == 'stratifiedshuffle':\n        cv = model_selection.StratifiedShuffleSplit(n_splits=n_fold, random_state=seed)\n        kf = cv.split(X_train, X_train[era])\n    elif fold_type == 'kfold':\n        cv = model_selection.KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n        kf = cv.split(X_train, y_train)      \n    \n    fi_df = pd.DataFrame()\n    fi_df['features'] = features\n    fi_df['importance'] = 0\n        \n    for fold_id, (train_index, valid_index) in tqdm(enumerate(kf)):\n        # split\n        X_tr = X_train.loc[train_index, features]\n        X_val = X_train.loc[valid_index, features]\n        y_tr = y_train.loc[train_index]\n        y_val = y_train.loc[valid_index]\n        \n        # model (note inverse weighting)\n        train_set = lgb.Dataset(X_tr, y_tr, categorical_feature=cats, weight=1\/np.power(y_tr, 2))\n        val_set = lgb.Dataset(X_val, y_val, categorical_feature=cats, weight=1\/np.power(y_val, 2))\n        model = lgb.train(\n            params\n            , train_set\n            , valid_sets=[train_set, val_set]\n            , feval=RMSPEMetric()\n            , verbose_eval=250\n        )\n        \n        # feature importance\n        fi_df[f'importance_fold{fold_id}'] = model.feature_importance(importance_type=\"gain\")\n        fi_df['importance'] += fi_df[f'importance_fold{fold_id}'].values\n        \n        # save model\n        joblib.dump(model, f'model_fold{fold_id}.pkl')\n        logger.debug('model saved!')\n\n        # predict\n        oof_df['pred'].iloc[valid_index] = model.predict(X_val)\n        y_pred = model.predict(X_test[features])\n        y_preds += y_pred \/ n_fold\n        models.append(model)\n        \n    return oof_df, y_preds, models, fi_df\n\nif MODE == 'TRAIN':\n    oof_df, y_preds, models, fi_df = fit_model(params, \n                                          book_df, \n                                          book_df[target], \n                                          test_book_df, \n                                          features=features, \n                                          cats=cats,\n                                          era=None,\n                                          fold_type='kfold', \n                                          n_fold=5, \n                                          seed=2021\n                                              )\nfrom sklearn.metrics import r2_score\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\nif MODE == 'TRAIN':\n    oof_df.dropna(inplace=True)\n    y_true = oof_df[target].values\n    y_pred = oof_df['pred'].values\n    \n    oof_df[target].hist(bins=100)\n    oof_df['pred'].hist(bins=100)\n    \n    R2 = round(r2_score(y_true, y_pred), 3)\n    RMSPE = round(rmspe(y_true, y_pred), 3)\n    logger.info(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}')\n# performance by stock_id\nif MODE == 'TRAIN':\n    for stock_id in oof_df['stock_id'].unique():\n        y_true = oof_df.query('stock_id == @stock_id')[target].values\n        y_pred = oof_df.query('stock_id == @stock_id')['pred'].values\n        \n        R2 = round(r2_score(y_true, y_pred), 3)\n        RMSPE = round(rmspe(y_true, y_pred), 3)\n        logger.info(f'Performance by stock_id={stock_id}: R2 score: {R2}, RMSPE: {RMSPE}')\nif MODE == 'INFERENCE':\n    \"\"\"\n    used for inference kernel only\n    \"\"\"\n    y_preds = np.zeros(len(test_book_df))\n    files = glob.glob(f'{MODEL_DIR}\/*model*.pkl')\n    assert len(files) > 0\n    for i, f in enumerate(files):\n        model = joblib.load(f)\n        y_preds += model.predict(test_book_df[features])\n    y_preds \/= (i+1)\n    \ntest_book_df[target] = y_preds","f38f6938":"final_predictions = (model1_predictions * 0.23 + model2_predictions * 0.27 + model3_predictions * 0.27+y_preds*0.23)*0.4+0.6*((nn_pre + test_predictions)\/2)\nsubmit = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/sample_submission.csv')\n\nsubmit.target = final_predictions\nsubmit","e2cdd48d":"submit.to_csv('submission.csv',index = False)","8288f498":"## Model 2: LGBM\nInspired from: https:\/\/www.kaggle.com\/felipefonte99\/optiver-lgb-with-optimized-params    ","70b7e4b5":"# Another LGBM","e2761356":"## Model 3: Catboost\n    \nInspired from: https:\/\/www.kaggle.com\/ramikhreas\/catboost-with-optimized-params?scriptVersionId=71165561","da3d9a8a":"## Model 1: TabNet\nInspired from: https:\/\/www.kaggle.com\/chumajin\/optiver-realized-tabnet-baseline"}}