{"cell_type":{"a9c8c8a7":"code","16721c5d":"code","8419b37a":"code","64bd43b1":"code","b972b18e":"code","7b1759a7":"code","573a0f08":"code","01fc1edc":"code","5772efe2":"code","deebf6a3":"code","8cb3cbae":"code","1b1f61b8":"code","6e84272f":"code","04165812":"code","7794d66d":"code","2af86832":"code","38a7f8cd":"code","c946c40f":"code","0c3e8e1d":"code","c2436f3b":"code","583d31f0":"code","688345c0":"code","14181dca":"code","f1118899":"code","b7d657ad":"code","ef001e05":"code","388fb1be":"code","97c94154":"code","2952d531":"code","2da24dd4":"code","96efd33a":"code","83884892":"code","8d07e4f3":"code","021d370e":"code","b20b6adc":"code","a19a2a45":"code","8773a63f":"markdown","be224986":"markdown","0325288b":"markdown","d2845a14":"markdown","251852f4":"markdown","061e2ca4":"markdown","e1718cc7":"markdown"},"source":{"a9c8c8a7":"pip install ctgan","16721c5d":"pip install scipy","8419b37a":"conda update --force-reinstall pandas","64bd43b1":"from sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import RobustScaler\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nfrom ctgan import CTGANSynthesizer\nimport gc\nimport numpy as np \nimport pandas as pd\nimport itertools\nimport gc\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom datetime import datetime\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer, LabelEncoder, minmax_scale,MinMaxScaler,RobustScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Dropout, Input, InputLayer, Flatten, LayerNormalization, BatchNormalization\n","b972b18e":"gc.collect()","7b1759a7":"pd.set_option(\"display.max_columns\", None)\n\ntrain_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\npseudolabels_df = pd.read_csv(\"..\/input\/tps-dec-pseudolabels-v1\/tps-dec-pseudolabels.csv\")\nsub_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\n\npseudolabels_df.head()","573a0f08":"print(\"The imbalanced class distribution:\")\nprint((train_df.groupby('Cover_Type').Id.nunique() \/ len(train_df)).apply(lambda p: f\"{p:.3%}\"))\nprint(\"The imbalanced class number:\")\nprint(train_df.groupby('Cover_Type').Id.nunique())","01fc1edc":"train_df = train_df[train_df.Cover_Type != 5]\ntrain_df = pd.concat([train_df, pseudolabels_df], axis=0)","5772efe2":"del pseudolabels_df\ngc.collect()","deebf6a3":"print(\"The imbalanced class distribution:\")\nprint((train_df.groupby('Cover_Type').Id.nunique() \/ len(train_df)).apply(lambda p: f\"{p:.3%}\"))\nprint(\"The imbalanced class number:\")\nprint(train_df.groupby('Cover_Type').Id.nunique())","8cb3cbae":"train_df.drop(\"Id\", axis=1, inplace=True)\ntest_df.drop(\"Id\", axis=1, inplace=True)","1b1f61b8":"train_df_4=train_df[train_df.Cover_Type==4]","6e84272f":"feature_cols = train_df.columns.tolist()\ncnt_features = []\ncat_features = []\n\nfor col in feature_cols:\n    if train_df[col].dtype=='float64':\n        cnt_features.append(col)\n    else:\n        cat_features.append(col)","04165812":"ctgan = CTGANSynthesizer(epochs=50)\nctgan.fit(train_df_4, cat_features)","7794d66d":"# Synthetic copy\nsamples = ctgan.sample(700)","2af86832":"frames = [train_df,samples]\n\ntrain_df = pd.concat(frames)","38a7f8cd":"del samples, ctgan , cnt_features , cat_features , train_df_4 , frames\ngc.collect()","c946c40f":"print(\"The imbalanced class number:\")\nprint(train_df.groupby('Cover_Type').size())","0c3e8e1d":"cols = [\"Soil_Type7\", \"Soil_Type15\"]\n\ntrain_df.drop(cols, axis=1, inplace=True)\ntest_df.drop(cols, axis=1, inplace=True)","c2436f3b":"idx = train_df[train_df[\"Cover_Type\"] == 5].index\ntrain_df.drop(idx, axis=0, inplace=True)","583d31f0":"new_names = {\n    \"Horizontal_Distance_To_Hydrology\": \"x_dist_hydrlgy\",\n    \"Vertical_Distance_To_Hydrology\": \"y_dist_hydrlgy\",\n    \"Horizontal_Distance_To_Roadways\": \"x_dist_rdwys\",\n    \"Horizontal_Distance_To_Fire_Points\": \"x_dist_firepts\"\n}\n\ntrain_df.rename(new_names, axis=1, inplace=True)\ntest_df.rename(new_names, axis=1, inplace=True)","688345c0":"le = LabelEncoder()\ntarget = le.fit_transform(train_df[\"Cover_Type\"])","14181dca":"train_df[\"Aspect\"][train_df[\"Aspect\"] < 0] += 360\ntrain_df[\"Aspect\"][train_df[\"Aspect\"] > 359] -= 360\n\ntest_df[\"Aspect\"][test_df[\"Aspect\"] < 0] += 360\ntest_df[\"Aspect\"][test_df[\"Aspect\"] > 359] -= 360","f1118899":"# Manhhattan distance to Hydrology\ntrain_df[\"mnhttn_dist_hydrlgy\"] = np.abs(train_df[\"x_dist_hydrlgy\"]) + np.abs(train_df[\"y_dist_hydrlgy\"])\ntest_df[\"mnhttn_dist_hydrlgy\"] = np.abs(test_df[\"x_dist_hydrlgy\"]) + np.abs(test_df[\"y_dist_hydrlgy\"])\n\n# Euclidean distance to Hydrology\ntrain_df[\"ecldn_dist_hydrlgy\"] = (train_df[\"x_dist_hydrlgy\"]**2 + train_df[\"y_dist_hydrlgy\"]**2)**0.5\ntest_df[\"ecldn_dist_hydrlgy\"] = (test_df[\"x_dist_hydrlgy\"]**2 + test_df[\"y_dist_hydrlgy\"]**2)**0.5","b7d657ad":"soil_features = [x for x in train_df.columns if x.startswith(\"Soil_Type\")]\ntrain_df[\"soil_type_count\"] = train_df[soil_features].sum(axis=1)\ntest_df[\"soil_type_count\"] = test_df[soil_features].sum(axis=1)\n\nwilderness_features = [x for x in train_df.columns if x.startswith(\"Wilderness_Area\")]\ntrain_df[\"wilderness_area_count\"] = train_df[wilderness_features].sum(axis=1)\ntest_df[\"wilderness_area_count\"] = test_df[wilderness_features].sum(axis=1)","ef001e05":"features_Hillshade = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n\ntrain_df[\"Hillshade_mean\"] = train_df[features_Hillshade].mean(axis=1)\ntrain_df['amp_Hillshade'] = train_df[features_Hillshade].max(axis=1) - train_df[features_Hillshade].min(axis=1) \n\ntest_df[\"Hillshade_mean\"] = test_df[features_Hillshade].mean(axis=1)\ntest_df['amp_Hillshade'] = test_df[features_Hillshade].max(axis=1) - test_df[features_Hillshade].min(axis=1) ","388fb1be":"train_df.loc[train_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ntest_df.loc[test_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ntest_df.loc[test_df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ntest_df.loc[test_df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ntest_df.loc[test_df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\ntrain_df.loc[train_df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ntest_df.loc[test_df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\ntrain_df.loc[train_df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ntest_df.loc[test_df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","97c94154":"cols = [\n    \"Elevation\",\n    \"Aspect\",\n    \"mnhttn_dist_hydrlgy\",\n    \"ecldn_dist_hydrlgy\",\n    \"soil_type_count\",\n    \"wilderness_area_count\",\n    \"Slope\",\n    \"x_dist_hydrlgy\",\n    \"y_dist_hydrlgy\",\n    \"x_dist_rdwys\",\n    \"Hillshade_9am\",\n    \"Hillshade_Noon\",\n    \"Hillshade_3pm\",\n    \"x_dist_firepts\",\n    \"soil_type_count\",\n    \"wilderness_area_count\"\n]\n\nscaler = RobustScaler()\ntrain_df[cols] = scaler.fit_transform(train_df[cols])\ntest_df[cols] = scaler.transform(test_df[cols])","2952d531":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n \n    return df","2da24dd4":"train_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","96efd33a":"gc.collect()","83884892":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tf_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"Running on TPU:\", tpu.master())\nexcept:\n    tf_strategy = tf.distribute.get_strategy()\n    print(f\"Running on {tf_strategy.num_replicas_in_sync} replicas\")\n    print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","8d07e4f3":"# Plot training history\ndef plot_history(history, *, n_epochs=None, plot_lr=False, plot_acc=True, title=None, bottom=None, top=None):\n    \"\"\"Plot (the last unique n_epochs epochs of) the training history\"\"\"\n    plt.figure(figsize=(15, 6))\n    from_epoch = 0 if n_epochs is None else len(history['loss']) - n_epochs\n    \n    # Plot training and validation losses\n    plt.plot(np.arange(from_epoch, len(history['loss'])), history['loss'][from_epoch:], label='training loss')\n    try:\n        plt.plot(np.arange(from_epoch, len(history['loss'])), history['val_loss'][from_epoch:], label='Validation loss')\n        best_epoch = np.argmin(np.array(history['val_loss']))\n        best_val_loss = history['val_loss'][best_epoch]\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_loss], c='r', label=f'Best val_loss = {best_val_loss:.5f}')\n        if best_epoch > 0:\n            almost_epoch = np.argmin(np.array(history['val_loss'])[:best_epoch])\n            almost_val_loss = history['val_loss'][almost_epoch]\n            if almost_epoch >= from_epoch:\n                plt.scatter([almost_epoch], [almost_val_loss], c='orange', label='Second best val_loss')\n    except KeyError:\n        pass\n    if bottom is not None: plt.ylim(bottom=bottom)\n    if top is not None: plt.ylim(top=top)\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='lower left')\n    if title is not None: plt.title(title)\n        \n    # Plot validation metrics\n    if plot_acc:\n        best_epoch = np.argmax(np.array(history['val_acc']))\n        best_val_acc = history['val_acc'][best_epoch]\n        ax2 = plt.gca().twinx()\n        ax2.plot(np.arange(from_epoch, len(history['loss'])), np.array(history['val_acc'][from_epoch:]), color='r', label='Validation accuracy')\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_acc], c='r', label=f'Best val_acc = {best_val_acc:.5f}')\n        ax2.set_ylabel('Accuracy')\n        ax2.legend(loc='center right')\n        \n    # Plot learning rate\n    if plot_lr:\n        ax2 = plt.gca().twinx()\n        ax2.plot(np.arange(from_epoch, len(history['loss'])), np.array(history['lr'][from_epoch:]), color='g', label='Learning rate')\n        ax2.set_ylabel('Learning rate')\n        ax2.legend(loc='upper right')\n        \n    plt.show()\n    ","021d370e":"features = [f for f in test_df.columns if f != 'Id' and f != 'Cover_Type']","b20b6adc":"%%time\nEPOCHS = 200# increase the number of epochs if the training curve indicates that a better result is possible\nVERBOSE = 0 # set to 0 for less output, or to 2 for more output\nSINGLE_FOLD = False # set to True for a quick experiment and to False for full cross-validation\nRUNS = 1 # should be 1. increase the number of runs only if you want see how the result depends on the random seed\nBATCH_SIZE = 1024 # if you set this too high, the notebook will crash (out of memory)\nFOLDS =20\n\ndef my_model(X):\n    \n    \n    \"\"\"Return a compiled Keras model\"\"\"\n    model = Sequential()\n    model.add(InputLayer(input_shape=(X.shape[-1])))\n\n    model.add(Dense(512, activation='selu'))\n    model.add(Dense(512, activation='selu'))\n    model.add(Dropout(rate=0.1))\n    model.add(BatchNormalization())\n    \n    model.add(Dense(256, activation='selu'))\n    model.add(Dense(256, activation='selu'))\n    model.add(BatchNormalization())\n    \n    model.add(Dense(128, activation='selu'))\n    model.add(Dense(128, activation='selu'))\n    model.add(BatchNormalization())\n    \n    model.add(Dense(64, activation='selu'))\n    model.add(Dropout(rate=0.1))\n    model.add(Dense(64, activation='selu'))\n        \n    # Add the final layer with the correct activation function\n    # Adding kernel_regularizer=tf.keras.regularizers.l2(l2=0.03) didn't make a difference\n    model.add(Dense(len(le.classes_), activation='softmax'))\n    \n    \n    # Compile the model\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='sparse_categorical_crossentropy', metrics=['acc'])\n    return model\n\n# Make the results reproducible\nnp.random.seed(2021)\ntf.random.set_seed(2021)\n\ntotal_start_time = datetime.now()\nscore_list, test_pred_list1, history_list = [], [], []\noof_list = [np.full((len(train_df), len(le.classes_)), -1.0, dtype='float32') for run in range(RUNS)]\nfor run in range(RUNS):\n    kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=1)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, y=train_df.Cover_Type)):\n        print(f\"Fold {run}.{fold}\")\n        start_time = datetime.now()\n        X_tr = train_df.iloc[train_idx]\n        X_va = train_df.iloc[val_idx]\n        y_tr = target[train_idx]\n        y_va = target[val_idx]\n        X_tr = X_tr[features]\n        X_va = X_va[features]\n\n        # train\n        preproc = StandardScaler() \n        X_tr = preproc.fit_transform(X_tr)\n        X_va = preproc.transform(X_va)\n        \n        model = my_model(X_tr)\n\n        # Define two callbacks: ReduceLROnPlateau, EarlyStopping\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, \n                               patience=5, verbose=VERBOSE)\n\n        es = EarlyStopping(monitor=\"val_acc\", patience=20, \n                           verbose=VERBOSE, mode=\"max\", \n                           restore_best_weights=True)\n        \n\n        # train and save the model\n        history = model.fit(X_tr, y_tr, \n                            validation_data=(X_va, y_va), \n                            epochs=EPOCHS,\n                            verbose=VERBOSE,\n                            batch_size=BATCH_SIZE, \n                            validation_batch_size=len(X_va),\n                            shuffle=True,\n                            callbacks=[lr, es])\n        history_list.append(history.history)\n        model.save(f\"model1\/model{run}.{fold}\")\n        \n        # Inference for validation after last epoch of fold\n        y_va_pred = model.predict(X_va, batch_size=len(X_va))\n        oof_list[run][val_idx] = y_va_pred\n        y_va_pred = np.argmax(y_va_pred, axis=1)\n\n        # Evaluation\n        accuracy = accuracy_score(y_va, y_va_pred)\n        score_list.append((accuracy, datetime.now() - start_time))\n        print(f\"Fold {run}.{fold} | {str(datetime.now() - start_time)[-12:-7]} | Epochs: {len(history_list[-1]['loss'])} | Accuracy: {accuracy:.5f}\")\n        if run == 0: plot_history(history_list[-1], title=f\"Accuracy: {accuracy:.5f}\")\n\n        # Inference for test: keep the predicted probabilities\n        test_pred_list1.append(model.predict(preproc.transform(test_df[features]), batch_size=BATCH_SIZE))\n        \n        # Clean up the memory (it seems that Keras doesn't clean up everything at keyboard interrupts)\n        del model, y_va_pred\n        gc.collect()\n        \n        if SINGLE_FOLD: break","a19a2a45":"# Create the submission file\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nsub = test_df[['Id']].copy()\nsub['Cover_Type'] = le.inverse_transform(np.argmax(sum(test_pred_list1), axis=1)) # soft voting by adding the probabilities of all models in the ensemble\nsub.to_csv('submission.csv', index=False)\n\n# Plot the distribution of the test predictions\nplt.figure(figsize=(10,3))\nplt.hist(train_df['Cover_Type'], bins=np.linspace(0.5, 7.5, 8), density=True, label='train labels')\nplt.hist(sub['Cover_Type'], bins=np.linspace(0.5, 7.5, 8), density=True, rwidth=0.7, label='Test predictions')\nplt.xlabel('Cover_Type')\nplt.ylabel('Frequency')\nplt.gca().yaxis.set_major_formatter(PercentFormatter())\nplt.legend()\nplt.show()\n\nsub.head()","8773a63f":"# imports","be224986":"concatenate dataframes","0325288b":"# Modeling ","d2845a14":"# Feature Engineering","251852f4":"# Read Data","061e2ca4":"Renaming some columns with long names","e1718cc7":"## Generate data for cover_type == 4"}}