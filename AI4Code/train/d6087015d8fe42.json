{"cell_type":{"8038aa42":"code","cc90bcd8":"code","d5abe0bb":"code","a4b9c1dc":"code","63840881":"code","d77b75db":"code","ddbd6d4d":"code","7146c230":"code","f72420a8":"code","0ae5bf4e":"code","1441cdb8":"code","ef7570c2":"code","7ef91c77":"code","f63620b3":"code","e3a4f459":"code","a251ea31":"code","3e03b308":"code","c9b64bf7":"code","cca5259c":"code","b534943b":"code","42db2bf4":"markdown","e6de6c60":"markdown","362d0a94":"markdown"},"source":{"8038aa42":"import re\nimport requests\nfrom bs4 import BeautifulSoup as bs","cc90bcd8":"html_doc = requests.get('https:\/\/realpython.github.io\/fake-jobs\/').text\nhtml_doc[:500]","d5abe0bb":"html_parsed = bs(html_doc)\nprint(html_parsed.prettify()[:500])","a4b9c1dc":"html_parsed.find('div',attrs={'class':'card-content'})\n\n# Below is the example of 1 div\n# We require only details,\n# Title (stored in h2 tag)\n# Subtitle (stored in h3 tag)\n# Location (stored in p tag)\n# Date Posted (stored in time tag)\n# Job Link (stored in a tag)","63840881":"job_data = [[i.h2.string,i.h3.string,re.sub(r'[^\\w,]','',i.p.string),i.time.string,str(i.select('a:nth-of-type(2)')[0])]\n            for i in html_parsed.find_all('div',attrs={'class':'card-content'})]\n\n# Extracting required details","d77b75db":"job_data[0]\n\n# Below data contains a tag, we will extract only URL from it","ddbd6d4d":"# Extracting only url and storing it again in variable\n\nfor index,val in enumerate(job_data):\n  job_data[index][4] = re.split(r'\"',val[4])[3]","7146c230":"job_data[0]","f72420a8":"bs(requests.get(job_data[0][4]).text).find('div',attrs={'class':'content'}).p.string\n\n# Example of 1 job description, we will use this logic in loop as below and description on the list","0ae5bf4e":"for index,job_details in enumerate(job_data):\n  job_description = bs(requests.get(job_details[4]).text).find('div',attrs={'class':'content'}).p.string\n  job_data[index].append(job_description)","1441cdb8":"job_data[0]\n\n# We have scrapped description of each job from their url ","ef7570c2":"import pandas as pd","7ef91c77":"job_details_data = pd.DataFrame(columns=['Title','Subtitle','Location','Date_Posted','Job_Link','Description'])\n\n# Creating a dataframe with default columns","f63620b3":"# Inserting job data into dataframe\n\nfor index,job_details in enumerate(job_data):\n  job_details_data.loc[index,'Title'] = job_details[0]\n  job_details_data.loc[index,'Subtitle'] = job_details[1]\n  job_details_data.loc[index,'Location'] = job_details[2]\n  job_details_data.loc[index,'Date_Posted'] = job_details[3]\n  job_details_data.loc[index,'Job_Link'] = job_details[4]\n  job_details_data.loc[index,'Description'] = job_details[5]\n","e3a4f459":"job_details_data.head()","a251ea31":"def make_clickable(link,text):\n    return f'<a href=\"{link}\">{text}<\/a>'\n\n# Making the links clickable","3e03b308":"job_details_data['Job_Link'] = job_details_data.apply(lambda x: make_clickable(x['Job_Link'], x['Title']), axis=1)","c9b64bf7":"job_details_data.head()","cca5259c":"from IPython.display import HTML","b534943b":"HTML(job_details_data.head().to_html(escape=False))","42db2bf4":"# Dataset creation\n\nWe will scrap [Fake Python](https:\/\/realpython.github.io\/fake-jobs\/) website and create a csv file with jobs and their details.\n\n<img src='https:\/\/drive.google.com\/uc?export=view&id=1Cr2uaUGP_WCX73_-geibVB25PH7J5YTK' height=500>\n","e6de6c60":"We have extracted required details, but only description of each job is missing, we will get the description of each job from their URL as below,\n\n<img src='https:\/\/drive.google.com\/uc?export=view&id=1uNkmRMKwpGn5yY6TQAC6otEp6-gOzoq_' height=400>\n","362d0a94":"## References\n\n1. [Real Python web scrapping](https:\/\/realpython.com\/beautiful-soup-web-scraper-python\/)"}}