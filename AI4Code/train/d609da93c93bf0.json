{"cell_type":{"efadedb0":"code","5d19340e":"code","d9ad1cf1":"code","670faf6c":"code","9cee7762":"code","beb19f61":"code","7dccabb9":"code","f28a5d72":"code","6203a42d":"code","25c13a94":"code","ed338d36":"code","bcc8633f":"code","21843a24":"code","b2f7e61b":"code","75b34e1d":"code","d98c9d1f":"code","06d71667":"code","f92bd4c9":"code","f89e2c49":"code","6f7666c4":"code","39b6b9e0":"code","2f0a02d9":"code","12e15512":"code","6d88f5d8":"code","dd826766":"code","f8840da9":"code","c3c39162":"code","a87d98ae":"code","6a095744":"code","bc18f0c0":"markdown","09826c15":"markdown","9e9b28b2":"markdown","5ca577c0":"markdown","75270ceb":"markdown","1a93c1db":"markdown","3605032b":"markdown","7d138698":"markdown","29163b00":"markdown","df58c310":"markdown","87c64b8f":"markdown","4d1c9056":"markdown","3160790f":"markdown","5e94168c":"markdown"},"source":{"efadedb0":"import numpy as np\nimport pandas as pd\nimport re # regular expression is used to find a specific text or letter\/ word in a sentence or paragraph\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords # these are basically the words which don't convey much meaning like a the an etc.\nfrom nltk.stem.porter import PorterStemmer # this is used to stem the word like for eg if we have loved --> love!\nfrom sklearn.feature_extraction.text import CountVectorizer #to vectorize the words into a vector of frequent words count!\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","5d19340e":"print(stopwords.words('english')) # all the words in stopwords that don't really add much value","d9ad1cf1":"news_dataset = pd.read_csv('..\/input\/fake-news\/train.csv')","670faf6c":"news_dataset.shape","9cee7762":"news_dataset.head()","beb19f61":"news_dataset.isnull().sum()","7dccabb9":"news_dataset = news_dataset.fillna('')","f28a5d72":"news_dataset['content'] = news_dataset['author'] + \" \" + news_dataset['title']\nprint(news_dataset['content'])","6203a42d":"stem = PorterStemmer() # basically creating an object for stemming! Stemming is basically getting the root word, for eg: loved --> love! ","25c13a94":"# now let's create a function to preprocess a cell and then apply it to the entire feature!\ndef stemming(content):\n    stemmed_content = re.sub('[^a-zA-Z]', ' ',content) # this basically replaces everything other than lower a-z & upper A-Z with a ' ', for eg apple,bananna --> apple bananna\n    stemmed_content = stemmed_content.lower() # to make all text lower case\n    stemmed_content = stemmed_content.split() # this basically splits the line into words with delimiter as ' '\n    stemmed_content = [stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')] # basically remove all the stopwords and apply stemming to the final data\n    stemmed_content = ' '.join(stemmed_content) # this basically joins back and returns the cleaned sentence\n    return stemmed_content","ed338d36":"# let's apply the function on our feature content\nnews_dataset['content'] = news_dataset['content'].apply(stemming)","bcc8633f":"print(news_dataset['content'])","21843a24":"X = news_dataset['content'].values\ny = news_dataset['label'].values","b2f7e61b":"print(X)","75b34e1d":"vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(X)","d98c9d1f":"classifier = LogisticRegression(C = 100, penalty = 'l2', solver= 'newton-cg')\nclassifier.fit(X, y)","06d71667":"# accuracy score on training data\ny_pred_train = classifier.predict(X)\naccuracy_train = accuracy_score(y,y_pred_train)\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier,X = X,y= y , cv = 10)\n\n\nprint(\"-------------------------------\")\nprint(\"Accuracy score on training data: \", accuracy_train)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f}\".format(accuracies.std()*100))","f92bd4c9":"# grid search to find better hyper parameters\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{'solver': ['newton-cg','liblinear'], 'penalty': ['l2'],'C': [100, 10, 1.0, 0.1, 0.01]}]\ngrid_search = GridSearchCV(estimator=classifier,\n                          param_grid=parameters,\n                          scoring='accuracy',\n                          cv=10)\ngrid_search.fit(X,y)\nprint(\"Best Accuracy: {:.2f} %\".format(grid_search.best_score_*100))\nprint(\"Best Parameters: \", grid_search.best_params_)","f89e2c49":"news_dataset_test = pd.read_csv('..\/input\/fake-news\/test.csv')","6f7666c4":"# checking for null values\nnews_dataset_test.isnull().sum()","39b6b9e0":"# replacing nulls with ''\nnews_dataset_test = news_dataset_test.fillna('')","2f0a02d9":"# creating content column and applying stemming from the function we created above !\n\nnews_dataset_test['content'] = news_dataset_test['author'] + \" \" + news_dataset_test['title']\nnews_dataset_test['content'] = news_dataset_test['content'].apply(stemming)","12e15512":"# seperating the data and vectorizing to predict the labels from the model we made!\n\nX_test = news_dataset_test['content']\nX_test = vectorizer.transform(X_test)","6d88f5d8":"# now to predict the labels from the model!\n\ny_pred_final = classifier.predict(X_test)\nprint(y_pred_final)","dd826766":"y_pred_final.shape","f8840da9":"news_dataset_test['id'].shape","c3c39162":"results = pd.DataFrame(news_dataset_test['id'], columns=['id'])\nresults.head()","a87d98ae":"# adding the y_pred_finals to the data frame!\nresults['label'] = y_pred_final\nresults.shape","6a095744":"results.to_csv('Results.csv',index=False)","bc18f0c0":"Now that we have X and y let's train our model on a classifier model!","09826c15":"Let's load the dataset now! It's a dataset from kaggle's inclass competition fake-news!\n[https:\/\/www.kaggle.com\/c\/fake-news\/data](http:\/\/)","9e9b28b2":"Let's create the submission file!","5ca577c0":"Let's seperate the data into the data and labels we need! **content** and **label**","75270ceb":"* nltk - It is a famous library for natual language processing!\n* re - regular expressions are basically used in python to extract a piece of data from large data, you can have a read about it if you find it confusing or interesting! And perhaps try making a python project on web scraping to implement that knowledge! ","1a93c1db":"Data cleaning and stemming!","3605032b":"Now we have to convert the cleanned text into numbers rather in a vector that the model will understand and learn from. Basically it creates a vector with the count of the words!","7d138698":"Now let's analyse our model and evaluate accuracy! We have got a **excellent accuracy** using **logistic classifier** but to not leave it to chance that we got lucky on the train set let's check the accuracy using **K-Fold cross validation** and **tune the hyper parameters** if possible!","29163b00":"Let's check if we have any missing values","df58c310":"Let's start by importing the required libraries!","87c64b8f":"Since we have a large dataset, it's okay to drop the missing value! here I'm gonna replace the missing values with an empty string. And we're gonna be using the title and author combined cause the text feature is too large and may even be a paragraph but it's good to use the text for training! But for now let's check the accuracy with the title and author feature!","4d1c9056":"# Fake news prediction \nIn this project we're gonna be looking at some labeled data and try to classify the news if it is real or fake!\nWe're gonna achieve this using **natural language processing** and I'll try to specify every step we take!\nLet's get started!\nI got a accuracy score of **99.8%** on the test data using logistic classifier! \nHappy learning!","3160790f":"After tuning hyper parameters we get much higher results! **99%**\nLet's now get the result for test set and check accuracy in the competition!","5e94168c":"Let's check the stopwords!"}}