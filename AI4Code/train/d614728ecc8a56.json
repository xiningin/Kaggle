{"cell_type":{"59d3ada4":"code","ab86dc8b":"code","9893c1db":"code","d74206a2":"code","19977de8":"code","40a04d1d":"code","ef1ab0dd":"code","ae44151a":"code","8a73d1cb":"code","41d56729":"code","f7f4a26d":"code","67acb4f0":"code","69559bac":"code","91899db0":"code","80c423b6":"code","027e6afc":"code","afdfd27c":"code","2e5bdfc0":"code","9fe41c54":"code","a9801c3a":"code","f984d6b3":"code","8d5091ee":"code","32233006":"code","32bd8837":"code","eb8b2c1a":"code","8fcd3be7":"code","e774c664":"code","8242f3d0":"code","cd284f95":"code","dfaf5e29":"markdown","95216b31":"markdown","97212ef4":"markdown","901f4fd7":"markdown","69cb24fe":"markdown","f8964a2c":"markdown","f9c74724":"markdown","98343167":"markdown","5923fed5":"markdown","6e8bf5da":"markdown","a695641b":"markdown","c07d51c5":"markdown","0773f96d":"markdown","5796c8ca":"markdown","e136058d":"markdown","eae3c7b5":"markdown","b9f74e04":"markdown","db6c9b4e":"markdown","f3e9daf4":"markdown","f075cf36":"markdown","4aab643f":"markdown","37ab2349":"markdown","e5df1d12":"markdown","7a644256":"markdown","ac483126":"markdown","25a9045e":"markdown"},"source":{"59d3ada4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ab86dc8b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline ","9893c1db":"df = pd.read_csv(\"..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\", engine = \"python\")\ndf.head()","d74206a2":"df.info()","19977de8":"df.isnull().sum()","40a04d1d":"#Replacing all the null values with zero\ndf['salary'].fillna(0, inplace = True)","ef1ab0dd":"data = df\n\nstatus = {'Placed': 1,'Not Placed': 0} \ndata['status'] = [status[item] for item in data['status']] ","ae44151a":"df.describe()","8a73d1cb":"def myplot(data,x,y):\n    plt.Figure(figsize =(12,12))\n    sns.boxplot(x = data[x],y= data[y])\n    g = sns.FacetGrid(data, row = y)\n    g = g.map(plt.hist,x)\n    plt.show()","41d56729":"sns.set_style(\"ticks\")\nmyplot(data,\"salary\",\"ssc_b\")","f7f4a26d":"sns.set_style(\"ticks\")\nmyplot(data, \"salary\", \"gender\")\n","67acb4f0":"sns.countplot(data['status'],hue=data['gender'])","69559bac":"fig, axes = plt.subplots(2,3, figsize=(12,12))\nsns.barplot(x=\"hsc_s\", y=\"status\", data=data, ax = axes[(0,0)] )\nsns.barplot(x=\"hsc_s\", y=\"hsc_p\", data=data, ax = axes[(0,1)])\nsns.barplot(x=\"degree_t\", y=\"status\", data=data, ax = axes[(0,2)])\nsns.barplot(x=\"status\", y=\"degree_p\", data=data, ax = axes[(1,0)])\nsns.barplot(x=\"degree_t\", y=\"degree_p\", data=data, ax = axes[(1,1)])\nplt.tight_layout(pad = 3)","91899db0":"fig, axes = plt.subplots(2,2, figsize=(12,12))\nsns.barplot(x=\"workex\", y=\"status\", data=data, ax = axes[(0,0)])\nsns.barplot(x=\"status\", y=\"etest_p\", data=data, ax = axes[(0,1)])\nsns.barplot(x=\"specialisation\", y=\"status\", data=data, ax = axes[(1,0)])\nsns.barplot(x=\"status\", y=\"mba_p\", data=data, ax = axes[(1,1)])\nplt.tight_layout(pad = 3)","80c423b6":"# encoding for the features\n\nimport category_encoders as ce\nencoder = ce.BackwardDifferenceEncoder(cols=['ssc_b', \"hsc_b\", \"hsc_s\", \"degree_t\", \"workex\", \"specialisation\", \"gender\"])\ndata_new = encoder.fit_transform(data)\n\ndata_new.head()\n","027e6afc":"data_new.drop(['intercept','sl_no'], axis=1, inplace=True)\ndata_new.head()","afdfd27c":"x = data_new[\"salary\"]\nlabels = data_new['status']\nfeatures = data_new.iloc[:, :-2 ]\nfeatures = pd.concat([features, x], axis=1, join='inner')\nfeatures.head()\n","2e5bdfc0":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n# feature extraction\nmodel = LogisticRegression(solver='lbfgs')\nrfe = RFE(model, 10)\nfit = rfe.fit(features, labels)\nprint(\"Num Features: %d\" % fit.n_features_)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)","9fe41c54":"#dropping the features\nfeatures.drop([\"gender_0\", \"ssc_b_0\", \"hsc_b_0\", \"degree_t_0\", \"specialisation_0\"], axis=1, inplace=True)\nfeatures.head()","a9801c3a":"df_final = features\ndf_final.to_numpy()\ndistinct_labels = list(set(labels))","f984d6b3":"from sklearn.manifold import TSNE\nimport matplotlib.patheffects as PathEffects\n\ny = labels\nX_raw = df_final\ny_raw = np.array(y, dtype = 'int')\ntsne = TSNE(n_components=2, random_state=0, perplexity = 50, n_iter = 5000)\nX_2d = tsne.fit_transform(X_raw)\nX1 = X_2d[:,0:1]\nY1 = X_2d[:,1:2]\n\nsns.set_style('ticks')\nsns.set_palette('muted')\nsns.set_context(\"notebook\", font_scale=1.5,\n                rc={\"lines.linewidth\": 2.5})\n\n\ncategory_to_color = {0: 'red', 1: 'blue'}\ncategory_to_label = {0: 'Unplaced', 1:\"Placed\"}\n\nfig, ax = plt.subplots(1,1)\nfor category, color in category_to_color.items():\n    mask = y == category\n    ax.plot(X_2d[mask, 0], X_2d[mask, 1], 'o',\n            color=color, label=category_to_label[category], ms = 6)\n\nax.legend(loc='best')\nax.axis('on')\nax.axis('tight')\nplt.xlabel('Dimension1')\nplt.ylabel('Dimension2')\nplt.title(' t-SNE plot')","8d5091ee":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler()\nx = scaler.fit_transform(X_raw)\ny = y_raw","32233006":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.18)","32bd8837":"model1 = RandomForestClassifier()\nmodel1.fit(x_train,y_train)\nmodel1.score(x_test,y_test)\npredictions1 = model1.predict(x_test)\nprint(confusion_matrix(y_test,predictions1))\nprint(classification_report(y_test,predictions1))","eb8b2c1a":"model2 = DecisionTreeClassifier()\nmodel2.fit(x_train,y_train)\n\npredictions2 = model2.predict(x_test)\nprint(confusion_matrix(y_test,predictions2))\nprint(classification_report(y_test,predictions2))","8fcd3be7":"model3 = KNeighborsClassifier()\nmodel3.fit(x_train,y_train)\n\npredictions3 = model3.predict(x_test)\nprint(confusion_matrix(y_test,predictions3))\nprint(classification_report(y_test,predictions3))","e774c664":"model4 = XGBClassifier()\nmodel4.fit(x_train,y_train)\npredictions4 = model4.predict(x_test)\nprint(confusion_matrix(y_test,predictions4))\nprint(classification_report(y_test,predictions4))","8242f3d0":"model5 = GaussianNB()\nmodel5.fit(x_train,y_train)\npredictions5 = model5.predict(x_test)\nprint(confusion_matrix(y_test,predictions5))\nprint(classification_report(y_test,predictions5))","cd284f95":"#neural network classifier\nmodel6 = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\nmodel6.fit(x_train,y_train)\npredictions6 = model6.predict(x_test)\nprint(confusion_matrix(y_test,predictions6))\nprint(classification_report(y_test,predictions6))","dfaf5e29":"From info of the data, we can see the salary has only 148 entries and thus 67 entries are null value. This is due to the fact htat a guy is placed or not, thus for data cleansing ,we will remove the null values with some values helpful for us.(here zero)","95216b31":"From above plots it is clear that number of jobs available for commerce and science students is greater than that of students for other streams. Also it can be seen, the scores of class 12th do effect abit, although the impact is not that much high. Also the score of degree affects the placement, however maintaining an average score is enough to land in good jobs.","97212ef4":"As we can see the number of placed students population of boys is higher than girls, although less number of girls did sit for the placement.This shows boys have slight better chance of getting hired than girls.","901f4fd7":"As from the boxplot we can see median salary for boys is greater than girls and so is the highest package. From the histogram, the number of unplaced girls and boys are 30 and 40 respectively. Althoug the number of unplaed girls seems less, but compared to the strngth of males and females, it is relatively high. ","69cb24fe":"## Encoding the data\n","f8964a2c":"The first graph gives clear understanding that a work experience is preferred for getting into a good job,so one must go for good internships during college days. The employement test scores, doesn't matter much,and the scores of mba too, however a decent average score must be maintained. The marketing and finance student have more job opprtunities than H.R. for mba degree.","f9c74724":"## Extracting important features\n","98343167":"# Data Exploration","5923fed5":"# Conclusion and Discussion\n1. We can say that for classification model, the results are pretty well and if the student has some decent credentials in college and better 12th score, there might be chances of earning good salary job.\n2. The gender is taken into factor for placement, however it is very less.\n3. Co- curriculars do affects as it adds to the holistic personality of the student.\n4. Most important point is work experience and internships do effects the placements and obviously one must go for it during college days.\n5. Another key factor is stream choice as science and commerce have better jobs than arts, however only chosing a stream won't help to earn decent job,rather one must take into account above mentioned factors.","6e8bf5da":"Campus placement is becoming highly competitive and there is immense load on colleges. This puts great pressures on the students if they are studying in some reputed college as the fear of not getting placed is constantly haunting them due to shallow fall in economy of country due to COVID-19. Here I will try to address the key dependencies of credentials earned from class 10th to current degree that would affect the chances of placement. Some key points undertaken are: -\n\n1. Choice of board in class 10th and 12th to get placed.\n\n2. Does gender effects the placements stats?\n\n3. Work Experience, and internships effects.\n\n4. What factors are responsible for not getting placed?\n\n5. How does stream effects placement?\n\n***\n\n**At the end a model will be trained to perform predictive analysis**\n\n***","a695641b":"## Standardizing the data","c07d51c5":"## Applying t-SNE and using it for getting insight of features\n","0773f96d":"<a href = \"https:\/\/seaborn.pydata.org\/generated\/seaborn.FacetGrid.html\" target=\"_blank\"> reference<\/a> code for above plots","5796c8ca":"Now we will be replacing the string values with integer values for our understanding, such as status will be rplaced by 1 for placed and 0 for unplaced","e136058d":"# Data Preprocessing and Feature Engineering","eae3c7b5":"Looking at the confusion matrix, it appears that all the models are performing exceptionally well. Some reasons which I can think of is: - \n1. Since addressing the case of placed and unplaced becomes binary classification problem, so most of the models are able to perform well as the dataset size is too small.\n2. The t-SNE plot shows that both the classes are ar too apart and thus the features are searable, due to which complexity has almost gone down.\n3. Feature selection step extracts only the most important features, due to which the data becomes quite clean for training purpose.\n4. Also the presence of outlier is not there or is minimal as we can see from the plots above.","b9f74e04":"# Training different classifiers and measuring the accuracy values\n","db6c9b4e":"1. Backward difference encoding of the categorical features and features containing texts.\n2. Dropping the features not relevant or which puts less impact on the model.\n3. Extracting important features.\n4. Using PCA\/t-SNE for distribution visualization.\n5. Standardizing the dataset before training.","f3e9daf4":"**Thus the key factor would be internship. Mostly the scores are discarded but they actually have some importance for the placement. Also chosing the stream puts huge impact for future growth of carrier**","f075cf36":"# Campus Recruitment","4aab643f":"The code for backward difference encoding is taken from the mentioned <a href=\"https:\/\/www.datacamp.com\/community\/tutorials\/categorical-data\" target=\"_blank\">source <\/a> .\nIn dataset as shown above we have encoded the string categorical data with some encoded values, so that it becomes\nrelevant for models to learn. One hot encoding may suffer with the problem of curse of dimensionality, thus backward difference encoding is used.","37ab2349":"Looking at the above t-SNE plot it is very easy to conclude that features are separable, also the two classes are shown. Now we will move further to train models and do predictive analysis and verify models by using different metrices for our classifiers or regressor models. Before this there is one last step of normalizing the dataset so that all the values lies in a particular range for the robustness of our model.","e5df1d12":"# Importing important libraries and reading data","7a644256":"We will use these features for training our ML algorithm and further look over the results. I chose 10 features important for the training purpose, although one is free to chose of its own.The important features for placement stats are: **[\"ssc_p\", \"hsc_p\", \"hsc_s_0\", \"hsc_s_1\", \"degree_p\", \"degree_t_1\", \"workex_0\", \"etest_p\", \"mba_p\", \"salary\"]** To have some insights about features we will use dimensionality reductin in below setion and have look how much separability in the features is present.\nThe code snippet for feature selection is taken from https:\/\/machinelearningmastery.com\/feature-selection-machine-learning-python\/.\n\nSome other sources from where help is taken for feature selection is https:\/\/www.datacamp.com\/community\/tutorials\/feature-selection-python\n\nWe will keep only the 10 most important features for working further.","ac483126":"## Dropping unneccessary features\n","25a9045e":"Although the median salary for both central and state board remains same, the highest package given is higher\nfor students of central board. This happens because co-curricular activities provided to students are far\nbetter than those of state board. Prominently the stage fear and hesitation cause is removed for students coming\nfrom central board schools. From the histogram, it is seen that unplaced students are more for central schools.However overall placement stats remains same, thus choice of board for class 10th doesn't affects much for the placement."}}