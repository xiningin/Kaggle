{"cell_type":{"f08407aa":"code","20544d5e":"code","3bd7ce91":"code","40dd2696":"code","aec97390":"code","8435cdd1":"code","e9e7eb00":"code","f06e3bbb":"code","9d9fffa3":"code","454f0885":"code","624a7f06":"code","87699695":"code","87d06ad5":"code","a007fb03":"markdown","8d83e673":"markdown","11bd2973":"markdown"},"source":{"f08407aa":"from IPython.display import Markdown, display","20544d5e":"def content(text):\n    \"\"\" This function allows you to output content \"\"\"\n    my_response = \"<div style= 'background-color:rgb(247, 247, 247); border:1px solid rgb(207,207,207); border-color:rgb(107,107,107); padding: 10px'> \\\n    <span style='color: black;  font-family: medium-content-serif-font, Georgia, Cambria, 'Times New Roman', Times, serif; \\\n    font-weight: 400; letter-spacing: -0.004em; line-height: 1.58; '>\"+ text + \"<\/span><\/div>\"\n    display(Markdown(my_response))","3bd7ce91":"content(\n    \"In statistics, isotonic regression or monotonic regression is the technique of fitting a free-form line to a sequence\" +\n    \" of observations such that the fitted line is non-decreasing (or non-increasing) everywhere, and lies as close to \" +\n    \"the observations as possible. It has applications in statistical inference. \" +\n    \"One might use it to fit an isotonic curve to the means of some set of experimental results when an increase in those means according to some particular \" + \"\\n\\n\" +\n    \"ordering is expected. A benefit of isotonic regression is that it is not constrained by any functional form, such as\" +\n    \" the linearity imposed by linear regression, as long as the function is monotonic increasing.\"+\n    \"https:\/\/en.wikipedia.org\/wiki\/Isotonic_regression\"\n)","40dd2696":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aec97390":"df = pd.read_csv('..\/input\/10k-statement-data-for-us-biotech-companies\/biotech_timeSeries.csv', encoding='utf8')\ndf.head()","8435cdd1":"# Lets first handle numerical features with nan value\nnumerical_nan = [feature for feature in df.columns if df[feature].isna().sum()>1 and df[feature].dtypes!='O']\nnumerical_nan","e9e7eb00":"df[numerical_nan].isna().sum()","f06e3bbb":"## Replacing the numerical Missing Values\n\nfor feature in numerical_nan:\n    ## We will replace by using median since there are outliers\n    median_value=df[feature].median()\n    \n    df[feature].fillna(median_value,inplace=True)\n    \ndf[numerical_nan].isnull().sum()","9d9fffa3":"from scipy import linalg\nimport matplotlib","454f0885":"def isotonic_regression(y, weight=None, y_min=None, y_max=None, callback=None):\n    \"\"\"Solve the isotonic regression model::\n\n        min sum w[i] (y[i] - y_[i]) ** 2\n\n        subject to y_min = y_[1] <= y_[2] ... <= y_[n] = y_max\n\n    where:\n        - y[i] are inputs (real numbers)\n        - y_[i] are fitted\n        - w[i] are optional strictly positive weights (default to 1.0)\n\n    Parameters\n    ----------\n    y : iterable of floating-point values\n        The data.\n\n    weight : iterable of floating-point values, optional, default: None\n        Weights on each point of the regression.\n        If None, weight is set to 1 (equal weights).\n\n    y_min : optional, default: None\n        If not None, set the lowest value of the fit to y_min.\n\n    y_max : optional, default: None\n        If not None, set the highest value of the fit to y_max.\n\n    Returns\n    -------\n    `y_` : list of floating-point values\n        Isotonic fit of y.\n\n    References\n    ----------\n    \"Active set algorithms for isotonic regression; A unifying framework\"\n    by Michael J. Best and Nilotpal Chakravarti, section 3.\n    \"\"\"\n    if weight is None:\n        weight = np.ones(len(y), dtype=y.dtype)\n    if y_min is not None or y_max is not None:\n        y = np.copy(y)\n        weight = np.copy(weight)\n        C = np.dot(weight, y * y) * 10  # upper bound on the cost function\n        if y_min is not None:\n            y[0] = y_min\n            weight[0] = C\n        if y_max is not None:\n            y[-1] = y_max\n            weight[-1] = C\n\n    active_set = [(weight[i] * y[i], weight[i], [i, ])\n                  for i in range(len(y))]\n    current = 0\n    counter = 0\n    while current < len(active_set) - 1:\n        value0, value1, value2 = 0, 0, np.inf\n        weight0, weight1, weight2 = 1, 1, 1\n        while value0 * weight1 <= value1 * weight0 and \\\n                        current < len(active_set) - 1:\n            value0, weight0, idx0 = active_set[current]\n            value1, weight1, idx1 = active_set[current + 1]\n            if value0 * weight1 <= value1 * weight0:\n                current += 1\n\n            if callback is not None:\n                callback(y, active_set, counter, idx1)\n                counter += 1\n\n        if current == len(active_set) - 1:\n            break\n\n        # merge two groups\n        value0, weight0, idx0 = active_set.pop(current)\n        value1, weight1, idx1 = active_set.pop(current)\n        active_set.insert(current,\n                          (value0 + value1,\n                           weight0 + weight1, idx0 + idx1))\n        while value2 * weight0 > value0 * weight2 and current > 0:\n            value0, weight0, idx0 = active_set[current]\n            value2, weight2, idx2 = active_set[current - 1]\n            if weight0 * value2 >= weight2 * value0:\n                active_set.pop(current)\n                active_set[current - 1] = (value0 + value2, weight0 + weight2,\n                                           idx0 + idx2)\n                current -= 1\n\n    solution = np.empty(len(y))\n    if callback is not None:\n        callback(y, active_set, counter+1, idx1)\n        callback(y, active_set, counter+2, idx1)\n    for value, weight, idx in active_set:\n        solution[idx] = value \/ weight\n    return solution","624a7f06":"import numpy as np\nimport pylab as pl\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.utils import check_random_state\n\n\ndef cb(y, active_set, counter, current):\n    solution = np.empty(len(y))\n    for value, weight, idx in active_set:\n        solution[idx] = value \/ weight\n    fig = matplotlib.pyplot.gcf()\n    fig.set_size_inches(9.5,6.5)\n\n    color = y.copy()\n    pl.scatter(np.arange(len(y)), solution, s=50, cmap=pl.cm.Spectral, vmin=50, c=color)\n    pl.scatter([np.arange(len(y))[current]], [solution[current]], s=200, marker='+', color='red')\n    pl.xlim((0, 40))\n    pl.ylim((50, 300))\n    pl.savefig('isotonic_%03d.png' % counter)\n    pl.show()\n\nn = 40\nx = np.arange(n)\nrs = check_random_state(0)\ny = rs.randint(-50, 50, size=(n,)) + 50. * np.log(1 + np.arange(n))\n\n###############################################################################\n# Fit IsotonicRegression and LinearRegression models\n\n\ny_ = isotonic_regression(y, callback=cb)","87699695":"import pylab as pl\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.utils import check_random_state\n\nn = 100\ny = np.array([0]*50+[1]*50)\nrs = check_random_state(0)\nx = np.random.random(size=(n,)) #you can interpret it as the outputs of the SVM or any other model\n\nres = sorted(list(zip(x,y)), key = lambda x: x[0]) \nx = []\ny = []\nfor i,j in res:\n    x.append(i)\n    y.append(j)\nx= np.array(x)\ny= np.array(y)\n###############################################################################\n# Fit IsotonicRegression and LinearRegression models\n\nir = IsotonicRegression()\ny_ = ir.fit_transform(x, y)\n\nlr = LinearRegression()\nlr.fit(x[:, np.newaxis], y)  # x needs to be 2d for LinearRegression\n\n###############################################################################\n# plot result\n\nsegments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]\nlc = LineCollection(segments, zorder=0)\nlc.set_array(np.ones(len(y)))\nlc.set_linewidths(0.5 * np.ones(n))\n\nfig = pl.figure()\npl.plot(x, y, 'r.', markersize=12)\npl.plot(x, y_, 'g.-', markersize=12)\npl.plot(x, lr.predict(x[:, np.newaxis]), 'b-')\npl.gca().add_collection(lc)\npl.legend(('Data', 'Isotonic Fit', 'Linear Fit'), loc='lower right')\npl.title('Isotonic regression')\n\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(9.5,6.5)\npl.savefig('inverse_isotonic.png')\npl.show()","87d06ad5":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thanks Kundan Jha for the Isotonic Regression. @mpwolke was Here.' )","a007fb03":"![](https:\/\/image3.slideserve.com\/5743965\/isotonic-regression1-l.jpg)slideserve.com","8d83e673":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Isotonic or Monotonic Regression<\/h3>","11bd2973":"#Codes by Kundan Jha https:\/\/www.kaggle.com\/kundnjha\/isotonic-regression"}}