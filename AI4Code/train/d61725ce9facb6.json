{"cell_type":{"5840bbfb":"code","e640e274":"code","05038996":"code","a9fb2054":"code","b8d7079c":"code","3f5c81cf":"code","3c2014c1":"code","71ec6878":"code","7c88f31f":"code","1ef40f03":"code","e53e13c2":"code","3da01de2":"code","5f698209":"code","1d89636f":"code","12d0c8cd":"code","d4ee1b1a":"code","99b0915e":"code","ad48b329":"code","bccda79a":"code","44a8f58a":"code","8866158b":"code","15cc889c":"code","92509f51":"code","b61ed907":"code","20c43418":"code","2a936a7c":"code","dd980e9a":"code","d1855eff":"code","69562ba7":"code","2e0a77a2":"code","460b8bbd":"code","479cb8bd":"code","60f00c1d":"code","7f97b4e5":"code","3b55578f":"code","c3291b26":"code","d2c2a235":"code","bf4dc14c":"code","88421965":"code","6f815755":"code","2c3029fc":"code","6b68f915":"code","af0ec114":"code","32dd80c5":"code","225a8ef0":"code","b3ea9744":"code","ae3295c6":"code","f0b3d330":"code","785add20":"code","8f58ac99":"code","9271bfbe":"code","da00faac":"code","ea7ac548":"code","afbcb177":"code","56bf0e84":"code","89480570":"code","560ae56a":"code","80f25b49":"code","66b5bca9":"code","140b1473":"code","32384a00":"code","2ccbc9c3":"code","e4741aa2":"code","59e4cb83":"code","c27f5be1":"code","4a547b81":"code","204b77a7":"code","8c42b0d6":"code","93bdb22d":"code","045619fe":"code","a7254117":"code","150e6c6d":"code","48d81d30":"code","fc25148e":"code","72e039ba":"markdown","706da0da":"markdown","2eeabd23":"markdown","95054a24":"markdown","d75abcf3":"markdown","f9224442":"markdown","f6c4f2a8":"markdown","414ba162":"markdown","982f4e46":"markdown","5aafa547":"markdown","63f19b55":"markdown","81a000c2":"markdown","08b8addd":"markdown","9d0fa7f5":"markdown","9aca929c":"markdown","91d51e32":"markdown","08bfa488":"markdown","b4a5fefa":"markdown","f5114159":"markdown","defe3c71":"markdown","0d66dbc3":"markdown","a55e0e4f":"markdown","9e7b7eb8":"markdown","fb6e5108":"markdown","a102367c":"markdown","f1316c38":"markdown","f559fde0":"markdown","ffb66115":"markdown","fd54f85f":"markdown","e5aaa528":"markdown","17ee3377":"markdown","1f50f435":"markdown","b052cee1":"markdown","64d04e0b":"markdown","405770cd":"markdown","3c747e23":"markdown","99d064c0":"markdown","4cafbfe9":"markdown","e5aea962":"markdown","09b4d2e1":"markdown","1726524d":"markdown","38f1de6e":"markdown","13e896c1":"markdown","09b73642":"markdown","f0f66948":"markdown","2a9bd91d":"markdown","23ac2e9c":"markdown","b87f4254":"markdown","46987d8f":"markdown","b4c9500e":"markdown","907a8a5e":"markdown","822982b4":"markdown","0c1525a3":"markdown","1ccd86c2":"markdown","8bf2e82f":"markdown","f013da80":"markdown","6aa29cda":"markdown","4d23c279":"markdown","b281eb10":"markdown","fdfb130e":"markdown","5f731bab":"markdown","d7505280":"markdown","49b0586e":"markdown","9b47243a":"markdown","a0bf8eef":"markdown","2f2881ee":"markdown","97d1a5d5":"markdown","92b79f6f":"markdown","d9c8f963":"markdown","5483f6d1":"markdown","1e80fdc1":"markdown","4fe0b2f7":"markdown","643c30e2":"markdown","b78001b8":"markdown"},"source":{"5840bbfb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.simplefilter('ignore')\nimport operator\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity\nfrom sklearn.cluster import KMeans\n\nimport re\nimport string \nfrom collections import Counter\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport plotly.plotly as py\nfrom plotly import tools\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e640e274":"from nltk.corpus import stopwords\nstop = stopwords.words('english')","05038996":"emails = pd.read_csv('..\/input\/emails.csv')\nquestions = pd.read_csv('..\/input\/questions.csv')\nprofessionals = pd.read_csv('..\/input\/professionals.csv')\ncomments = pd.read_csv('..\/input\/comments.csv')\ntag_users = pd.read_csv('..\/input\/tag_users.csv')\ngroup_memberships = pd.read_csv('..\/input\/group_memberships.csv')\ntags = pd.read_csv('..\/input\/tags.csv')\nstudents = pd.read_csv('..\/input\/students.csv')\ngroups = pd.read_csv('..\/input\/groups.csv')\ntag_questions = pd.read_csv('..\/input\/tag_questions.csv')\nmatches = pd.read_csv('..\/input\/matches.csv')\nanswers = pd.read_csv('..\/input\/answers.csv')\nschool_memberships = pd.read_csv('..\/input\/school_memberships.csv')\nquestion_score = pd.read_csv('..\/input\/question_scores.csv')\nAnswer_score = pd.read_csv('..\/input\/answer_scores.csv')","a9fb2054":"print(students.shape)\nstudents.head(5)","b8d7079c":"students=students.dropna(subset = ['students_id','students_location', 'students_date_joined'])\nprint(students.shape)\nstudents.head(5)","3f5c81cf":"students['students_location'].nunique()","3c2014c1":"top10_regions_stud = students['students_location'].value_counts().head(10)\nax = top10_regions_stud.plot.bar(x=top10_regions_stud.index, y=top10_regions_stud.values)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nax.set_title('students count by location')\n#ax.set_facecolor(\"black\")","71ec6878":"date=students['students_date_joined'].str.split('-').values\nyears=[]\nfor i in range(0,len(date)):\n    years.append(date[i][0])\n\nstud_join_years = pd.Series(years)\nstud_join_years_counts=stud_join_years.value_counts()\nstud_join_years_counts.sort_index(inplace=True)\nax = stud_join_years_counts.plot.bar(x=stud_join_years_counts.index, y=stud_join_years_counts.values)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nax.set_title('number of student over years')","7c88f31f":"print(questions.shape)\nquestions.head(5)","1ef40f03":"questions.isnull().any()","e53e13c2":"print(questions.iloc[0]['questions_body'])\nprint(questions.iloc[30]['questions_body'])\nprint(questions.iloc[120]['questions_body'])\nprint(questions.iloc[300]['questions_body'])","3da01de2":"all_quest_titles=questions['questions_title'].str.cat(sep=' ')\n\nwordcloud = WordCloud(width=1500, height=1500).generate(all_quest_titles)\n\nplt.figure(figsize=(20, 7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","5f698209":"question_date=questions['questions_date_added'].str.split('-').values\nyears_questions=[]\n#months_questions=[]\nfor i in range(0,len(question_date)):\n    years_questions.append(question_date[i][0])\n    #months_questions.append(question_date[i][1])\n\n    \nquestions_date_added = pd.Series(years_questions)\nquestions_date_added_counts=questions_date_added.value_counts()\n\nquestions_date_added_counts.sort_index(inplace=True)\nax = questions_date_added_counts.plot.bar(x=questions_date_added_counts.index, y=questions_date_added_counts.values)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nax.set_title('number of questions over years')\n","1d89636f":"students_questions = pd.merge(students, questions, left_on='students_id',right_on='questions_author_id', how='inner')\nstudents_questions.head()","12d0c8cd":"no_questions_Bystudent=students_questions.groupby('students_id').size()\ntop10_activeStudents=no_questions_Bystudent.sort_values(ascending=False).head(10)\nax = top10_activeStudents.plot.bar(x=top10_activeStudents.index, y=top10_activeStudents.values)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nax.set_title('number of questions asked by students')","d4ee1b1a":"print(professionals.shape)\nprofessionals.head(5)\n\n","99b0915e":"professionals.isnull().any()","ad48b329":"professionals=professionals.dropna(subset = ['professionals_location', 'professionals_industry', 'professionals_headline'])\nprint(professionals.shape)\nprofessionals.head(5)","bccda79a":"professionals['professionals_industry'].value_counts()","44a8f58a":"top10_prof_industries = professionals['professionals_industry'].value_counts().head(10)\nax = top10_prof_industries.plot.bar(x=top10_prof_industries.index, y=top10_prof_industries.values)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nax.set_title('count professionals by Industry')","8866158b":"professionals['professionals_location'].nunique()","15cc889c":"top10_prof_regions = professionals['professionals_location'].value_counts().head(10)\nax = top10_prof_regions.plot.bar(x=top10_prof_regions.index, y=top10_prof_regions.values)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nax.set_title('count professionals by location')","92509f51":"professionals['professionals_headline'].value_counts().sort_values(ascending=False).head(10)\n","b61ed907":"professionals=professionals[professionals['professionals_headline'] != '--']\nprofessionals.shape","20c43418":"top10_profHeadlines = professionals['professionals_headline'].value_counts().sort_values(ascending=False).head(10)\nax = top10_profHeadlines.plot.bar(x=top10_profHeadlines.index, y=top10_profHeadlines.values)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nax.set_title('count professionals by headlines')","2a936a7c":"comments.head(5)","dd980e9a":"top10_number_comments =comments['comments_parent_content_id'].value_counts().head(10)\nax = top10_number_comments.plot.bar(x=top10_number_comments.index, y=top10_number_comments.values)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nax.set_title('count comments number for questions')\nax.set_xlabel(\"question id\")","d1855eff":"comments_profs = pd.merge(comments, professionals, left_on='comments_author_id',right_on='professionals_id',how='inner')\ncomments_profs                          ","69562ba7":"top10_profID_inComments = comments_profs.groupby('professionals_id').size().sort_values(ascending=False).head(10)\nax = top10_profID_inComments.plot.bar(x=top10_profID_inComments.index, y=top10_profID_inComments.values)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nax.set_title('Top10 professionals make comments')","2e0a77a2":"top10_prof_inComments_df = professionals.loc[professionals['professionals_id'].isin(top10_profID_inComments.index)]\ntop10_prof_inComments_df['professionals_industry']","460b8bbd":"top10_industry_comments=comments_profs.groupby('professionals_industry').size().sort_values(ascending=False).head(10)\n#ax = top10_industry_comments.plot.bar(x=top10_industry_comments.index, y=top10_industry_comments.values)\n#print(top10_industry_comments)\n\nprof_in_top10Industry_df = professionals.loc[professionals['professionals_industry'].isin(top10_industry_comments.index)]\ntop10_prof_comments=prof_in_top10Industry_df.groupby('professionals_industry').size()                                         \n\n#print(top10_prof_comments)\n\ncommentsVsProf_inIndustry_df=pd.concat([top10_industry_comments.rename('no.comments'), top10_prof_comments.rename('no.profs')], axis=1 )\nax = commentsVsProf_inIndustry_df.plot.bar()\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nax.set_title('no.comments , no.profs VS Industry')\nax.set_xlabel(\"Industry name\")","479cb8bd":"print(answers.shape)\nanswers.head(5)","60f00c1d":"questions.head(5)","7f97b4e5":"answers.head(5)","3b55578f":"questions_Answers = pd.merge(questions, answers, left_on='questions_id',right_on='answers_question_id', how='inner')\nquestions_Answers.head(5)","c3291b26":"questions_Answers['questions_date_added'] = pd.to_datetime(questions_Answers['questions_date_added'])\nquestions_Answers['answers_date_added'] = pd.to_datetime(questions_Answers['answers_date_added'])\nquestions_Answers['respond_time'] = (questions_Answers['answers_date_added']-questions_Answers['questions_date_added']).dt.days  \n#questions_Answers['respond_time'].head(10)\nax=questions_Answers['respond_time'].plot()\nax.set_xlim([0,2000])\nax.set_title('questions respond time')\nax.set_xlabel(\"respond time in days\")","d2c2a235":"questions_Answers['number_words_inQuestion'] = questions_Answers['questions_body'].apply(lambda x: len(x.split(' ')))\nquestions_Answers.loc[(questions_Answers['respond_time'] <= 7), 'week'] = 1\nquestions_Answers.loc[(questions_Answers['respond_time'] > 7) & (questions_Answers['respond_time'] <= 14), 'week'] = 2\nquestions_Answers.loc[(questions_Answers['respond_time'] > 14) & (questions_Answers['respond_time'] <= 21), 'week'] = 3\nquestions_Answers.loc[(questions_Answers['respond_time'] > 21) & (questions_Answers['respond_time'] <= 28), 'week'] = 4\nquestions_Answers.loc[(questions_Answers['respond_time'] > 28), 'week'] = 5\n\n\nax = sns.countplot(x=\"week\", data=questions_Answers)\nax.set_title('respond time duration in week')\n\nprint(questions_Answers.groupby('week').size())","bf4dc14c":"corr=questions_Answers.plot.scatter(x='number_words_inQuestion',y='respond_time')\ncorr.set_title('response time VS number_words_inQuestion')","88421965":"fast_response_words = questions_Answers[ questions_Answers['week']==1]['number_words_inQuestion']\nslow_response_words = questions_Answers[ questions_Answers['week']==5]['number_words_inQuestion']\n\npal = sns.color_palette()\n\nplt.figure(figsize=(18, 8))\nplt.hist(fast_response_words, bins=40, range=[0, 80], color=pal[9], label='fast')\nplt.hist(slow_response_words, bins=40, range=[0, 80], color=pal[3], alpha=0.4, label='slow')\nplt.legend()\nplt.xlabel('Number of words', fontsize=15)\nplt.ylabel('Number of responses', fontsize=15)\n","6f815755":"tags.head(5)","2c3029fc":"tags.nunique()","6b68f915":"tag_users.head(5)","af0ec114":"tags_tagUsers = pd.merge(tags, tag_users, left_on='tags_tag_id',right_on='tag_users_tag_id', how='inner')\ntags_tagUsers.head(5)","32dd80c5":"all_tags_followed=tags_tagUsers['tags_tag_name'].str.cat(sep=' ')\n\nwordcloud = WordCloud(width=1500, height=800).generate(all_tags_followed)\n\nplt.figure(figsize=(20, 7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","225a8ef0":"emails.head(5)","b3ea9744":"emails.isnull().any()","ae3295c6":"emails_frequency=emails['emails_frequency_level'].value_counts()\nprint(emails_frequency)\nax = emails_frequency.plot.bar(x=emails_frequency.index, y=emails_frequency.values)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nax.set_title('Email frequency level count')\nax.set_xlabel(\"Email freq.Level\")","f0b3d330":"matches.head(5)","785add20":"top10_mails = matches.groupby('matches_email_id').size().sort_values(ascending=False).head(10)\nax = top10_mails.plot.bar(x=top10_mails.index, y=top10_mails.values)\nax.set_title('Top10 mails containing questions')","8f58ac99":"X_train, X_test= train_test_split(questions, test_size=0.05, random_state=42)\nquestions_Answers = pd.merge(X_train, answers, left_on='questions_id',right_on='answers_question_id', how='inner')\nquestions_Answers_test = pd.merge(X_test, answers, left_on='questions_id',right_on='answers_question_id', how='inner')\nt = pd.merge(tags, tag_questions, left_on='tags_tag_id', right_on='tag_questions_tag_id')\nt_test = pd.merge(t,questions_Answers_test,left_on='tag_questions_question_id',right_on='answers_question_id')","9271bfbe":"k=questions_Answers_test['answers_author_id'].isin(questions_Answers['answers_author_id'])\nf=questions_Answers_test[k]\nf=f.reset_index()\nUQ=f['questions_body'].unique()","da00faac":"def process_text(df, col):\n    df[col] = df[col].str.replace('[^\\w\\s]','') # replacing punctuations\n    df[col] = df[col].str.replace('-',' ') # replacing dashes\n    df[col] = df[col].str.replace('\\d+','') # replacing digits\n    df[col] = df[col].str.lower().str.split() # convert all str to lowercase    \n    df[col] = df[col].apply(lambda x: [item for item in x if item not in stop]) # remove stopwords    \n    df[col] = df[col].apply(' '.join) # convert list to str\n    return df","ea7ac548":"def merging(df1, df2, left, right):\n    return df1.merge(df2, how=\"inner\", left_on=left, right_on=right)","afbcb177":"def combine_authors(df):\n    c = df.groupby('questions_id')['answers_author_id'].apply(list)\n    df_z = merging(df, pd.DataFrame(c), 'questions_id', 'questions_id')\n    df_z.drop('answers_author_id_x', axis=1, inplace=True)\n    df_z['answers_author_id_y'] = df_z['answers_author_id_y'].apply(', '.join)\n    df_z.drop_duplicates(inplace=True)\n    return df_z","56bf0e84":"Ques_Ans_sub = questions_Answers[['questions_title', 'questions_body', 'answers_author_id', 'questions_id']].copy()\n\nUniqueQues = combine_authors(Ques_Ans_sub)\n\nQues_Prof = UniqueQues[['questions_id', 'answers_author_id_y']].copy()\n\nUniqueQues.drop('answers_author_id_y', axis=1, inplace=True)\nUniquesQues = process_text(UniqueQues, \"questions_title\") \nUniqueQues = process_text(UniqueQues, \"questions_body\")","89480570":"tf = TfidfVectorizer(analyzer='word',\n                         ngram_range=(1,2),\n                         min_df=3,\n                         max_df=0.9,\n                         stop_words='english')\nlst=UniqueQues['questions_body']\ntfidf_matrix = tf.fit(lst)\ntfidfTrans= tfidf_matrix.transform(lst)","560ae56a":"def CosSim(QuesBody):\n    TargetQues= tfidf_matrix.transform([QuesBody])\n    tfidfTrans.shape\n    cosine_sim = linear_kernel(TargetQues, tfidfTrans)\n    q_titles = UniqueQues['questions_title']\n    q_ids = UniqueQues['questions_id']\n    indices = pd.Series(UniqueQues.index, UniqueQues['questions_title'])\n    return cosine_sim,indices,q_titles,q_ids","80f25b49":"def get_recommendations_idx(df):\n    sim_scores = list(enumerate(cosine_sim[0]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[0:21]\n    q_indices = [i[0] for i in sim_scores]\n    scores= [i[1] for i in sim_scores]\n    df['scores']=scores\n    return q_indices\n\ndef get_recommendations(df):\n    return q_titles.iloc[get_recommendations_idx(df)]\n    \ndef get_questions_id(df):\n    ls=q_ids.iloc[get_recommendations_idx(df)] \n    df['Ques_ID']=ls.tolist()\n    return df  ","66b5bca9":"df=pd.DataFrame()\ntest=f['questions_body'][5]\ncosine_sim,indices,q_titles,q_ids=CosSim(test)\nget_questions_id(df)","140b1473":"y=[i for i, value in enumerate(questions_Answers_test['questions_body']) if value in test]\nExpectedProf=questions_Answers_test['answers_author_id'][y]\nExpectedProf=ExpectedProf.reset_index()\nExpectedProf","32384a00":"z=[i for i, value in enumerate(questions_Answers['answers_author_id']) if value in ExpectedProf['answers_author_id'][0]]\nm=questions_Answers.iloc[z]\ntemp5=m['answers_question_id'].isin(df['Ques_ID'])\nRecomProfs5=m[temp5]\nRecomProfs5","2ccbc9c3":"ExpectedProf['answers_author_id'][0]","e4741aa2":"temp=answers['answers_question_id'].isin(df['Ques_ID'])\nRecomProfs=answers[temp]\ntemp2=comments_profs['professionals_id'].isin(RecomProfs['answers_author_id'])\nActiveProfs=comments_profs[temp2]\nActiveProfs=ActiveProfs[['professionals_id', 'comments_date_added' ]].copy()\nActiveProfs['comments_date_added'] =pd.to_datetime(ActiveProfs.comments_date_added)\nActiveProfs.sort_values(by=['comments_date_added'], inplace=True, ascending=False)\nActiveProfs.drop_duplicates('professionals_id',inplace=True)\nActiveProfs['Active']=ActiveProfs['comments_date_added']>'2018-01-01 00:00:00 UTC+0000'","59e4cb83":"RecomProfsk=pd.merge(RecomProfs,ActiveProfs,how='left',left_on='answers_author_id',right_on='professionals_id')\nRecomProfs2=RecomProfsk[['answers_question_id','answers_author_id','Active']][:]\nRecomProfs2.nunique()","c27f5be1":"temp3=answers['answers_author_id'].isin(RecomProfs2['answers_author_id'].unique())\nActiveAns=answers[temp3]\nActiveAns['answers_date_added'] =pd.to_datetime(ActiveAns.answers_date_added)\nActiveAns.sort_values(by=['answers_date_added'], inplace=True, ascending=False)\nActiveAns.drop_duplicates('answers_author_id',inplace=True)\nActiveAns['Active']=ActiveAns['answers_date_added']>'2018-01-01 00:00:00'\nActiveAns=ActiveAns[['answers_author_id','answers_question_id','Active']][:]","4a547b81":"RecomProfsx=pd.merge(RecomProfs2,ActiveAns,how='left',left_on='answers_author_id',right_on='answers_author_id')\nRecomProfsy=RecomProfsx[['answers_question_id_x','answers_author_id','Active_x','Active_y']][:]\nRecomProfsy.fillna(False,inplace=True)\nRecomProfsy['Active']=RecomProfsy['Active_x'] | RecomProfsy['Active_y']\nRecomProfsy.drop(['Active_x','Active_y'], axis=1, inplace=True)\nRecomProfsy.rename(columns={'answers_question_id_x' :'questions_id'},inplace=True)\n\nRecomProfsy","204b77a7":"Recommended_Prof=pd.merge(RecomProfsy,df,how='inner',left_on='questions_id',right_on='Ques_ID')\nRecommended_Prof.drop_duplicates(inplace=True)\nRecommended_Prof['FinalWeight']=(5*Recommended_Prof['scores'])+(2*Recommended_Prof['Active'])\nRecommended_Prof2=Recommended_Prof.groupby(['answers_author_id'],as_index=False).max()\nRecommended_Prof2.sort_values(by=['FinalWeight'], inplace=True, ascending=False)\nToCollaborative=Recommended_Prof2['answers_author_id'].values\nToCollaborative","8c42b0d6":"def getProfessionalTages(t,id):\n    indices1 = [i for i, value in enumerate(t['answers_author_id']) if value == id]\n    p = t['tags_tag_name'][indices1]\n    comm=np.unique(list(p))\n    return comm","93bdb22d":"#elbow method to chosse the best number of clusters\ndef elbow(df1):\n    distortions = []\n    k = 50\n    K = []\n    while k < 101:\n        print(k)\n        K.append(k)\n        kmeanModel = KMeans(n_clusters=k).fit(df1)\n        kmeanModel.fit(df1)\n        distortions.append(sum(np.min(cdist(df1, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) \/ df1.shape[0])\n        k += 2\n\n    # # Plot the elbow\n    plt.plot(K, distortions, 'bx-')\n    plt.xlabel('k')\n    plt.ylabel('Distortion')\n    plt.title('The Elbow Method showing the optimal k')\n    plt.show()\n    return","045619fe":"#merge the data\nt = pd.merge(tags,tag_questions, left_on='tags_tag_id', right_on='tag_questions_tag_id')\nt = pd.merge(t,questions_Answers,left_on='tag_questions_question_id',right_on='answers_question_id')\ntags=t['tags_tag_name']\n\n#get features form the question using tfidf vectirization\nvectorizer = TfidfVectorizer(max_df=0.5, max_features=100, min_df=2, stop_words='english',use_idf=True)\nvec = vectorizer.fit(tags)\nx=vec.transform(tags)\ndf1=x.toarray()","a7254117":"#cluster the data using k means\nkm = KMeans(n_clusters=62, init='k-means++', max_iter=100, n_init=1).fit(x)\nlabels = km.labels_.tolist()\n\n#now predict the new professional\nids=ToCollaborative\nmost_common=[]\nfor id in range(len(ids)):\n    out=getProfessionalTages(t,ids[id])\n    out = vec.transform(out)\n    out=out.toarray()\n    pred=km.predict(out)\n    indices = [i for i, value in enumerate(labels) if value in pred]\n    #print(indices)\n    p=t['answers_author_id'][indices]\n    comm=Counter(p)\n    n=10\n    if len(comm)<10:\n        n=len(comm)\n    arr=[]\n    comm=comm.most_common(n)\n    for a,b in comm:\n        arr.append(a)\n    most_common.append(arr)","150e6c6d":"most_common","48d81d30":"output=dict()\nfor i in range(len(most_common)):\n    for j in range (len(most_common[i])):\n        if most_common[i][j] in output:\n            value=output.get(str(most_common[i][j]))+1\n            key=str(most_common[i][j])\n            output.update({key: value})\n        else:\n            output.setdefault(str(most_common[i][j]), 1)\nsorted_output = sorted(output.items(), key=operator.itemgetter(1),reverse=True)\nn=10\nif len(comm) < 10:\n    n = len(sorted_output)","fc25148e":"sorted_output[0:n]","72e039ba":"The id's of recommended professionals by content based is **ToCollaborative**","706da0da":"What are the most regions the professionals come from ?","2eeabd23":"<font size=\"5\">Proposed solution<\/font>","95054a24":"### Let's begin with content based recommender<a id='cb'><\/a>","d75abcf3":"response time of questions vary , But what make question be answered faster than the other ? if questions in domains with small number of professionals ? is the length of question effects ? \nWhat make professionals late in the response ?!!!","f9224442":"### Let's explore emails<a id='em'><\/a>","f6c4f2a8":"most of the questions' title about College , Career and jobs","414ba162":"### collaborative filtering<a id='cf'><\/a>","982f4e46":"The U.S. has almost 500 students for every guidance counselor. Underserved youth lack the network to find their career role models, making CareerVillage.org the only option for millions of young people in America and around the globe with nowhere else to turn.\n\nTo date, 25,000 volunteers have created profiles and opted in to receive emails when a career question is a good fit for them. This is where your skills come in. To help students get the advice they need, the team at CareerVillage.org needs to be able to send the right questions to the right volunteers. The notifications sent to volunteers seem to have the greatest impact on how many questions are answered,\n\nSo the objective of this Kernel is to develop a method to recommend relevant questions to the professionals who are most likely to answer them.","5aafa547":"Which tags have the largest numbers of followers ?","63f19b55":"Assurance Associate at PwC and Software Engineer are the most two frequent headlines of professionals ","81a000c2":"are all the questions take the same time to be answered ?","08b8addd":"seems there is no correlation at general view?!! let's also check differences between questions answered during first week and those which are answered after more than 1 month","9d0fa7f5":"### Let's explore professionals<a id='prof'><\/a>","9aca929c":"The top active professional in comments from Design industry","91d51e32":"what makes 3,140 questions took more than 1 month to answered ? ","08bfa488":"### Let's explore questions<a id='quest'><\/a>","b4a5fefa":"2033 rows are deleted ... let's continue exploring :D","f5114159":"#### final output<a id='final'><\/a>","defe3c71":"### Let's explore comments<a id='comm'><\/a> ","0d66dbc3":"** What is the most region the students comes from ?!**","a55e0e4f":"From the fact that the professionals will supposed to answer similar questions they previously answered , so getting a similar questions to the asked question will help a lot ,\nso the supposed recommender consists of two parts :\n1.  In the first module the recommendations are based on the similarities between the question asked by a student and the pre-answered questions by the professionals in the system , where the recommended professionals from this part are the professionals who anweser similar questions and ranked by their activity status (the most recent active one has higher score than the less active one) .\n2. In this module the recommendations are based on the idea that professionals similar to another professional can be used to predict how much this professional will answer a particular question even if he didn\u2019t answer a similar question before.\n\n","9e7b7eb8":"<font size=\"5\">**Summary**<\/font>","fb6e5108":"Let's explore comments with professionals and see the most interactive professionals through comments ..","a102367c":"Let's see distribution of number of students over years","f1316c38":"# Kernel content\n   ## 1. [EDA](#eda)\n   ## 2. [Content based filtering](#cb)\n   ## 3. [Collaborative filtering](#cf)\n  ## 4. [Testing whole model](#tst)\n  ## 5. [How to use the model](#use)\n","f559fde0":"The recommendation system is a hybrid of content based filtering and collaborative filtering , so the output of content based which named by **ToCollaborative** is passed from content based filtering to collaborative filtering and the final output is sorted_output","ffb66115":"Seems that the community became most popular in 2016 ","fd54f85f":"New York is the largest region the students come from ...","e5aaa528":"1. write the asked question body in the variable [test](#test)  \n\n2.  the final ids of recommended professionals are in the variable [sorted_output](#final)","17ee3377":"### Let's explore matches<a id='mat'><\/a>","1f50f435":"the final recommended professionals' ids is **sorted_output**","b052cee1":"what is the top 10 mails containing questions ?","64d04e0b":"Telecommunications , Information technology and college are most followed hashtags","405770cd":"176 rows are removed ... Let's continue exploration","3c747e23":"### First split data into train and test ","99d064c0":"What are the most frequent headlines of professionals?","4cafbfe9":"Let's see distribution of questions over years","e5aea962":"Recommending professionals by giving weight=5 to the cosine similarity and 2 for the Active status","09b4d2e1":"## Let's build the model","1726524d":"Great there is no NAN values .. let's continue exploring","38f1de6e":"5248 rows are deleted ... let's continue exploration","13e896c1":"There are 176 professionals without headlines ... Let's remove these rows and continue exploring","09b73642":"Let's see the hottest questions with the largest number of comments ...","f0f66948":"is the lenght of question body significantly effect in the response time ? Let's see the correlation between response time and number of words in the question body !","2a9bd91d":"#### note : test is the input question body (asked question)\nto input your test just comment this line : test=f['questions_body'][5]\nand put your input question body \n\n**For ex :**\ntest=\"What is the best road to learn data science ?\"","23ac2e9c":"What are the most question's titles about?","b87f4254":"## Read All csv's","46987d8f":"There are NAN values ... Let's clean it and continue exploring :D","b4c9500e":"Information Technology and Services has the largest number of professionals","907a8a5e":"First let's merge students and questions tables","822982b4":"### Let's explore students with questions<a id='st_qt'><\/a>","0c1525a3":"### Let's test content based recommender<a id='test'><\/a>","1ccd86c2":"Who are the most active students ?","8bf2e82f":"Let's combine Questions and Answers","f013da80":"<font size=\"5\">**Problem description**<\/font>","6aa29cda":"let's see if the question body need some cleaning","4d23c279":"most subscribers have daily email notification","b281eb10":"### Let's explore Questions with answers<a id='qt_ans'><\/a>","fdfb130e":"2016 is the year with largest number of questions ... seems that the community becomes more famous from year 2016","5f731bab":"Is there a relations between number of professionals in an industry and number of comments in this industry ?","d7505280":"The largest number of heros from New York","49b0586e":"### Let's explore Answers<a id='ans'><\/a>","9b47243a":"great it doesn't need cleaning .. it is in good english format","a0bf8eef":"### Let's Explore tags with tag_users<a id='tag_tgU'><\/a>","2f2881ee":"unexpectedly there is no relation between number of comments and professionals in the same industry","97d1a5d5":"There are 16269 unique tags","92b79f6f":"## EDA<a id='eda'><\/a> \n### 1. [Exploring students](#stud)\n### 2. [Exploring questions](#quest)\n### 3. [Exploring students with questions](#st_qt)\n### 4. [Exploring Professionals](#prof)\n### 5. [Exploring comments](#comm)\n### 6. [Exploring professionals with comments](#prof_comm)\n### 7. [Exploring Answers](#ans)\n### 8. [Exploring questions with answers](#qt_ans)\n### 9. [Exploring tags](#tag)\n### 10.[Exploring tags with tag_users](#tag_tgU)\n### 11.[Exploring Emails](#em)\n### 12.[Exploring matches](#mat)","d9c8f963":"### Exploring professionals with comments<a id='prof_comm'><\/a>","5483f6d1":"Discovering the last date of professionals's answers","1e80fdc1":"### Let's explore tags<a id='tag'><\/a> ","4fe0b2f7":"seems that shorter questions are answered faster than the long ones ......\nBut this is by not \nseems it is not the question body length that affects these slow response !!\nAre they belongs to industries with small number of proffesionals ? Let's see..","643c30e2":"### Let's explore students<a id='stud'><\/a>","b78001b8":"### How to use the model<a id='use'><\/a>"}}