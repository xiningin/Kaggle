{"cell_type":{"69c4909a":"code","a35368c3":"code","0d690c15":"code","ee491347":"code","247194a3":"code","8a793133":"code","47e52a3e":"code","2cc92051":"code","e95177c4":"code","d11701e4":"code","cd0b2fbf":"code","f9a6aded":"code","41e9b241":"code","832d66e7":"code","2b223882":"code","6f790f8f":"code","8a905299":"code","bef46755":"code","ce3df8b9":"code","3c09258e":"code","232bfffc":"code","6645c477":"code","e7d9ef79":"code","5840d050":"code","b8b378aa":"code","33b22bc0":"code","d83c2470":"code","46f53841":"code","f58b9cac":"code","1dd2415e":"code","329193f2":"code","495417f4":"code","fbabef99":"code","db21bec0":"code","439a44b9":"code","d9736fcc":"code","1e5cb8ea":"code","7381873c":"code","26fd9ffa":"code","f852b3e6":"code","1923c21d":"code","544ae213":"code","f8a6487a":"code","156c3172":"code","d019fcd4":"code","c6d84d6a":"code","9f11a83f":"code","0f271983":"code","1bccccf0":"code","3bafed1b":"code","69203275":"code","98198bcf":"code","fa3ce541":"code","f6061cbd":"code","9fdc0065":"code","2ca202ec":"code","e40930af":"code","f1742c9f":"code","a3e80a66":"code","5147dcd2":"code","5af915b3":"markdown","3e8f2dc5":"markdown","85a71c95":"markdown","62ed16fd":"markdown","baddf2d7":"markdown","704315bd":"markdown","6a42a4ec":"markdown","016750d0":"markdown","ccadaa59":"markdown","a9d64782":"markdown","ab1d4f81":"markdown","5926bbb4":"markdown","30fea0ed":"markdown","e4767a0e":"markdown","93a62a11":"markdown","c7923dec":"markdown","a2ba6c65":"markdown","ac6030b4":"markdown","ef737818":"markdown","07368c5f":"markdown","4f05eea8":"markdown","e42e74d8":"markdown","ac8e3e58":"markdown","e20cf940":"markdown","c03df831":"markdown","18562172":"markdown","d01142b3":"markdown","4a7ab5d7":"markdown","89adfea9":"markdown"},"source":{"69c4909a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a35368c3":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom lightgbm.sklearn import LGBMRegressor\nimport lightgbm as lgb \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport gc\ngc.enable()\n\n#json requirements\nimport json\nfrom pandas.io.json import json_normalize\n\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","0d690c15":"def load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df\n\ntrain = load_df(\"..\/input\/train.csv\")\ntest = load_df(\"..\/input\/test.csv\")","ee491347":"train[:3]","247194a3":"#target variable\n#Since we are predicting the natural log of sum of all transactions of the user\n#let us sum up the transaction revenue at user level and take a log and then do a scatter plot.\ntrain[\"totals.transactionRevenue\"] = train[\"totals.transactionRevenue\"].astype('float')\ngdf = train.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()\n\nplt.figure(figsize=(8,6))\nplt.scatter(range(gdf.shape[0]), np.sort(np.log1p(gdf[\"totals.transactionRevenue\"].values)))\nplt.xlabel('index', fontsize=12) \nplt.ylabel('TransactionRevenue', fontsize=12)\nplt.show()","8a793133":"#transforming target column\ntrain['totals.transactionRevenue'] = train['totals.transactionRevenue'].fillna(0)\ntrain['totals.transactionRevenue'] = np.log1p(train['totals.transactionRevenue'])","47e52a3e":"train[:3]","2cc92051":"const_cols = [c for c in train.columns if train[c].nunique(dropna=False)==1 ]\nconst_cols\n","e95177c4":"cols_to_drop = const_cols  #drop constant columns\n\ntrain = train.drop(cols_to_drop , axis=1)\ntest = test.drop(cols_to_drop, axis=1)","d11701e4":"(set(train.columns).difference(set(test.columns))) #variable which are not common in both test and train","cd0b2fbf":"train = train.drop(['trafficSource.campaignCode'] , axis=1)","f9a6aded":"train.shape #(903653, 34)\ntest.shape #(804684, 33)","41e9b241":"train.head()\n\npercent = (100 * train.isnull().sum() \/ train.shape[0]).sort_values(ascending=False)\n\npercent[:20]","832d66e7":"percent = (100 * test.isnull().sum() \/ test.shape[0]).sort_values(ascending=False)\npercent[:10]","2b223882":"drop_cols_na = ['trafficSource.adContent', 'trafficSource.adwordsClickInfo.slot', 'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.isVideoAd',\n            'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adwordsClickInfo.gclId','trafficSource.referralPath']\n\ntrain.drop(drop_cols_na, axis=1, inplace=True)\ntest.drop(drop_cols_na, axis=1, inplace=True)","6f790f8f":"print(train.shape)\nprint(test.shape)","8a905299":"test.info()","bef46755":"def find_missing(data):\n    # number of missing values\n    count_missing = data.isnull().sum().values\n    # total records\n    total = data.shape[0]\n    # percentage of missing\n    ratio_missing = count_missing\/total\n    # return a dataframe to show: feature name, # of missing and % of missing\n    return pd.DataFrame(data={'missing_count':count_missing, 'missing_ratio':ratio_missing}, index=data.columns.values)\ntrain_missing = find_missing(train)\ntest_missing = find_missing(test)","ce3df8b9":"train_missing.reset_index()[['index', 'missing_ratio']]\\\n    .merge(test_missing.reset_index()[['index', 'missing_ratio']], on='index', how='left')\\\n    .rename(columns={'index':'columns', 'missing_ratio_x':'train_missing_ratio', 'missing_ratio_y':'test_missing_ratio'})\\\n    .sort_values(['train_missing_ratio', 'test_missing_ratio'], ascending=False)\\\n    .query('train_missing_ratio>0')","3c09258e":"miss_per = {}\nfor k, v in dict(train.isna().sum(axis=0)).items():\n    if v == 0:\n        continue\n    miss_per[k] = 100 * float(v) \/ len(train)\n    \nimport operator \nsorted_x = sorted(miss_per.items(), key=operator.itemgetter(1), reverse=True)\nprint (\"There are \" + str(len(miss_per)) + \" columns with missing values\")\n","232bfffc":"train.info()","6645c477":"# assuming missing values mean it is False, we fill NAs with false\ntrain['trafficSource.isTrueDirect'].fillna(False, inplace=True)\ntest['trafficSource.isTrueDirect'].fillna(False, inplace=True)","e7d9ef79":"train['trafficSource.keyword'].fillna('nan', inplace=True)\ntest['trafficSource.keyword'].fillna('nan', inplace=True)","5840d050":"train['totals.bounces'] = train['totals.bounces'].astype('float')\ntrain['totals.bounces'] = train['totals.bounces'].fillna(0)\ntest['totals.bounces'] = test['totals.bounces'].astype('float')\ntest['totals.bounces'] = test['totals.bounces'].fillna(0)","b8b378aa":"train['totals.newVisits'] = train['totals.newVisits'].astype('float')\ntrain['totals.newVisits'] = train['totals.newVisits'].fillna(0)\ntest['totals.newVisits'] = test['totals.newVisits'].astype('float')\ntest['totals.newVisits'] = test['totals.newVisits'].fillna(0)","33b22bc0":"train['totals.pageviews'] = train['totals.pageviews'].astype('float')\ntrain['totals.pageviews'] = train['totals.pageviews'].fillna(0)\ntest['totals.pageviews'] = test['totals.pageviews'].astype('float')\ntest['totals.pageviews'] = test['totals.pageviews'].fillna(0)","d83c2470":"train.info()","46f53841":"#totals.hits\ntrain['totals.hits'] = train['totals.hits'].astype('float')\ntest['totals.hits'] = test['totals.hits'].astype('float')\n#visitNumber\ntrain['visitNumber'] = train['visitNumber'].astype('float')\ntest['visitNumber'] = test['visitNumber'].astype('float')","f58b9cac":"plt.figure(figsize=(10,10))\nsns.heatmap(train.corr(),annot=True)","1dd2415e":"# Date-based features\n\n## on train data\n# transforming date\ntrain['date'] = train['date'].astype(str)\ntrain[\"date\"] = train[\"date\"].apply(lambda x : x[:4] + \"-\" + x[4:6] + \"-\" + x[6:])\ntrain[\"date\"] = pd.to_datetime(train[\"date\"])  \n\n#extracting features\ntrain['month'] = train['date'].dt.month\ntrain['day'] = train['date'].dt.day\ntrain['weekday'] = train['date'].dt.weekday\ntrain['weekofyear'] = train['date'].dt.weekofyear\n\n## similarly on test\n# transforming date\ntest['date'] = test['date'].astype(str)\ntest[\"date\"] = test[\"date\"].apply(lambda x : x[:4] + \"-\" + x[4:6] + \"-\" + x[6:])\ntest[\"date\"] = pd.to_datetime(test[\"date\"])  \n\n#extracting features\ntest['month'] = test['date'].dt.month\ntest['day'] = test['date'].dt.day\ntest['weekday'] = test['date'].dt.weekday\ntest['weekofyear'] = test['date'].dt.weekofyear","329193f2":"# using above features to create user count features\n\n## on train\ntrain['month_unique_user_count'] = train.groupby('month')['fullVisitorId'].transform('nunique')\ntrain['day_unique_user_count'] = train.groupby('day')['fullVisitorId'].transform('nunique')\ntrain['weekday_unique_user_count'] = train.groupby('weekday')['fullVisitorId'].transform('nunique')\ntrain['weekofyear_unique_user_count'] = train.groupby('weekofyear')['fullVisitorId'].transform('nunique')\n\n## on test\ntest['month_unique_user_count'] = test.groupby('month')['fullVisitorId'].transform('nunique')\ntest['day_unique_user_count'] = test.groupby('day')['fullVisitorId'].transform('nunique')\ntest['weekday_unique_user_count'] = test.groupby('weekday')['fullVisitorId'].transform('nunique')\ntest['weekofyear_unique_user_count'] = test.groupby('weekofyear')['fullVisitorId'].transform('nunique')","495417f4":"train[:3]","fbabef99":"# device based features\n\ntrain['browser_category'] = train['device.browser'] + '_' + train['device.deviceCategory']\ntrain['browser_operatingSystem'] = train['device.browser'] + '_' + train['device.operatingSystem']\n\ntest['browser_category'] = test['device.browser'] + '_' + test['device.deviceCategory']\ntest['browser_operatingSystem'] = test['device.browser'] + '_' + test['device.operatingSystem']\n\ntrain['source_country'] = train['trafficSource.source'] + '_' + train['geoNetwork.country']\ntest['source_country'] = test['trafficSource.source'] + '_' + test['geoNetwork.country']","db21bec0":"train['totals.hits'] = np.log1p(train['totals.hits'])\ntest['totals.hits'] = np.log1p(test['totals.hits'].astype(int))\n\ntrain['sum_pageviews_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\ntrain['count_pageviews_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\ntrain['mean_pageviews_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\ntrain['sum_hits_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\ntrain['count_hits_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\ntrain['mean_hits_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n\ntest['sum_pageviews_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\ntest['count_pageviews_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\ntest['mean_pageviews_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\ntest['sum_hits_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\ntest['count_hits_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\ntest['mean_hits_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n\ntrain['mean_hits_per_day'] = train.groupby(['day'])['totals.hits'].transform('mean')\ntrain['sum_hits_per_day'] = train.groupby(['day'])['totals.hits'].transform('sum')\ntest['mean_hits_per_day'] = test.groupby(['day'])['totals.hits'].transform('mean')\ntest['sum_hits_per_day'] = test.groupby(['day'])['totals.hits'].transform('sum')","439a44b9":"train['sum_pageviews_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\ntrain['count_pageviews_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\ntrain['mean_pageviews_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\n\ntrain['sum_pageviews_per_region'] = train.groupby('geoNetwork.region')['totals.pageviews'].transform('sum')\ntrain['count_pageviews_per_region'] = train.groupby('geoNetwork.region')['totals.pageviews'].transform('count')\ntrain['mean_pageviews_per_region'] = train.groupby('geoNetwork.region')['totals.pageviews'].transform('mean')\n\ntest['sum_pageviews_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\ntest['count_pageviews_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\ntest['mean_pageviews_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\n\ntest['sum_pageviews_per_region'] = test.groupby('geoNetwork.region')['totals.pageviews'].transform('sum')\ntest['count_pageviews_per_region'] = test.groupby('geoNetwork.region')['totals.pageviews'].transform('count')\ntest['mean_pageviews_per_region'] = test.groupby('geoNetwork.region')['totals.pageviews'].transform('mean')","d9736fcc":"train['sum_hits_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\ntrain['count_hits_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\ntrain['mean_hits_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n\ntrain['sum_hits_per_region'] = train.groupby('geoNetwork.region')['totals.hits'].transform('sum')\ntrain['count_hits_per_region'] = train.groupby('geoNetwork.region')['totals.hits'].transform('count')\ntrain['mean_hits_per_region'] = train.groupby('geoNetwork.region')['totals.hits'].transform('mean')\n\ntrain['sum_hits_per_country'] = train.groupby('geoNetwork.country')['totals.hits'].transform('sum')\ntrain['count_hits_per_country'] = train.groupby('geoNetwork.country')['totals.hits'].transform('count')\ntrain['mean_hits_per_country'] = train.groupby('geoNetwork.country')['totals.hits'].transform('mean')\n\ntest['sum_hits_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\ntest['count_hits_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\ntest['mean_hits_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n\ntest['sum_hits_per_region'] = test.groupby('geoNetwork.region')['totals.hits'].transform('sum')\ntest['count_hits_per_region'] = test.groupby('geoNetwork.region')['totals.hits'].transform('count')\ntest['mean_hits_per_region'] = test.groupby('geoNetwork.region')['totals.hits'].transform('mean')\n\ntest['sum_hits_per_country'] = test.groupby('geoNetwork.country')['totals.hits'].transform('sum')\ntest['count_hits_per_country'] = test.groupby('geoNetwork.country')['totals.hits'].transform('count')\ntest['mean_hits_per_country'] = test.groupby('geoNetwork.country')['totals.hits'].transform('mean')","1e5cb8ea":"train['user_pageviews_sum'] = train.groupby('fullVisitorId')['totals.pageviews'].transform('sum')\ntrain['user_hits_sum'] = train.groupby('fullVisitorId')['totals.hits'].transform('sum')\ntest['user_pageviews_sum'] = test.groupby('fullVisitorId')['totals.pageviews'].transform('sum')\ntest['user_hits_sum'] = test.groupby('fullVisitorId')['totals.hits'].transform('sum')\n\ntrain['user_pageviews_count'] = train.groupby('fullVisitorId')['totals.pageviews'].transform('count')\ntrain['user_hits_count'] = train.groupby('fullVisitorId')['totals.hits'].transform('count')\ntest['user_pageviews_count'] = test.groupby('fullVisitorId')['totals.pageviews'].transform('count')\ntest['user_hits_count'] = test.groupby('fullVisitorId')['totals.hits'].transform('count')\n\ntrain['user_pageviews_sum_to_mean'] = train['user_pageviews_sum'] \/ train['user_pageviews_sum'].mean()\ntrain['user_hits_sum_to_mean'] = train['user_hits_sum'] \/ train['user_hits_sum'].mean()\ntest['user_pageviews_sum_to_mean'] = test['user_pageviews_sum'] \/ test['user_pageviews_sum'].mean()\ntest['user_hits_sum_to_mean'] = test['user_hits_sum'] \/ test['user_hits_sum'].mean()","7381873c":"train['user_pageviews_to_region'] = train['user_pageviews_sum'] \/ train['mean_pageviews_per_region']\ntrain['user_hits_to_region'] = train['user_hits_sum'] \/ train['mean_hits_per_region']\n\ntest['user_pageviews_to_region'] = test['user_pageviews_sum'] \/ test['mean_pageviews_per_region']\ntest['user_hits_to_region'] = test['user_hits_sum'] \/ test['mean_hits_per_region']","26fd9ffa":"train['trafficSource.keyword'].value_counts()\ndef kw_category(x):\n    x = str(x).lower()\n    if x == 'nan':\n        return 'nan'\n    \n    x = ''.join(x.split())\n    \n    if 'youtube' in x or 'you' in x or 'yo' in x or 'tub' in x:\n        return 'youtube'\n    elif 'google' in x or 'goo' in x or 'gle' in x:\n        return 'google'\n    elif 'content' in x or 'targe' in x or 'cont' in x or 'target' in x or 'remarket' in x:\n        return 'content_target'\n    elif 'auto' in x or 'match' in x or 'tching' in x:\n        return 'auto_matching'\n    elif 'art' in x or 'entertainment' in x or 'game' in x or 'gamin' in x or 'play' in x:\n        return 'art_entertainment_play'\n    else:\n        return 'other'\n    \ntrain['KW_category'] = train['trafficSource.keyword'].apply(kw_category)\ntest['KW_category'] = test['trafficSource.keyword'].apply(kw_category)    ","f852b3e6":"train['KW_category'].value_counts()\ntrain['trafficSource.keyword'].value_counts()","1923c21d":"# generating squared columns\ncol_x = ['sum_pageviews_per_network_domain', 'sum_hits_per_network_domain',\n       'mean_hits_per_network_domain',\n       'mean_pageviews_per_network_domain', 'totals.hits',\n       'totals.pageviews']\n\nfor i in col_x:\n    train[i+'_square'] = train[i] ** 2\n    test[i+'_square'] = test[i] ** 2","544ae213":"#generating root columns\nfor i in col_x:\n    train[i+'_root'] = train[i] ** 0.5\n    test[i+'_root'] = test[i] ** 2","f8a6487a":"train.info()","156c3172":"train[:3]","d019fcd4":"train_fullVisitorId = train['fullVisitorId']\ntrain_sessionId = train['sessionId']\ntrain_visitId = train['visitId']\n\ntest_fullVisitorId = test['fullVisitorId']\ntest_id= test_id = test_fullVisitorId.values\ntest_sessionId = test['sessionId']\ntest_visitId = test['visitId']\n\nid_cols_to_drop= [\"sessionId\",\"fullVisitorId\",\"visitId\"]\nvisit_traits= [\"date\",\"visitNumber\",\"visitStartTime\"]\nbase_cat_cols_to_drop = ['trafficSource.keyword']\n\nall_cols_to_drop= id_cols_to_drop + visit_traits + base_cat_cols_to_drop\n#dropping these columns from train and test\ntrain.drop(all_cols_to_drop, axis=1, inplace=True)\ntest.drop(all_cols_to_drop, axis=1, inplace=True)","c6d84d6a":"corr_matrix = train.corr().abs()\ncorr_matrix","9f11a83f":"# remove columns due to correlation\n# removing one of totals.hits, totals.bounces and totals.pageviews\ncorr_columns_to_remove= [\"totals.pageviews\"]\n\n#removing them from train and test\ntrain.drop(corr_columns_to_remove, axis=1, inplace=True)\ntest.drop(corr_columns_to_remove, axis=1, inplace=True)","0f271983":"train[:3]","1bccccf0":"train.info()","3bafed1b":"num_cols = ['totals.bounces',\n             'totals.hits',\n             'totals.newVisits',\n             'month_unique_user_count',\n             'day_unique_user_count',\n             'weekday_unique_user_count',\n             'weekofyear_unique_user_count',\n             'sum_pageviews_per_network_domain',\n             'count_pageviews_per_network_domain',\n             'mean_pageviews_per_network_domain',\n             'sum_hits_per_network_domain',\n             'count_hits_per_network_domain',\n             'mean_hits_per_network_domain',\n             'mean_hits_per_day',\n             'sum_hits_per_day',\n             'sum_pageviews_per_region',\n             'count_pageviews_per_region',\n             'mean_pageviews_per_region',\n             'sum_hits_per_region',\n             'count_hits_per_region',\n             'mean_hits_per_region',\n             'sum_hits_per_country',\n             'count_hits_per_country',\n             'mean_hits_per_country',\n             'user_pageviews_sum',\n             'user_hits_sum',\n             'user_pageviews_count',\n             'user_hits_count',\n             'user_pageviews_sum_to_mean',\n             'user_hits_sum_to_mean',\n             'user_pageviews_to_region',\n             'user_hits_to_region',\n             'sum_pageviews_per_network_domain_square',\n             'sum_hits_per_network_domain_square',\n             'mean_hits_per_network_domain_square',\n             'mean_pageviews_per_network_domain_square',\n             'totals.hits_square',\n             'totals.pageviews_square',\n             'sum_pageviews_per_network_domain_root',\n             'sum_hits_per_network_domain_root',\n             'mean_hits_per_network_domain_root',\n             'mean_pageviews_per_network_domain_root',\n             'totals.hits_root',\n             'totals.pageviews_root']\n\ntarget= ['totals.transactionRevenue']\n\ncat_cols= set(list(train)) - set(target) - set(num_cols)","69203275":"y = train['totals.transactionRevenue']\ntrain = train.drop('totals.transactionRevenue', axis=1)","98198bcf":"train","fa3ce541":"test","f6061cbd":"cat_cols","9fdc0065":"# label encoder\nle = preprocessing.LabelEncoder()\n\nfor c in cat_cols:\n    le.fit(train[c])\n    train[c] = le.transform(train[c])","2ca202ec":"#label encoding for test\nfor c in cat_cols:\n    le.fit(test[c])\n    test[c] = le.transform(test[c])","e40930af":"train_x, val_x, train_y, val_y = train_test_split(train, y, test_size=0.25, random_state=20)\n\nlgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\",\n              \"num_leaves\" : 36, \"learning_rate\" : 0.05, \"bagging_fraction\" : 0.75, \"feature_fraction\" : 0.6, \"bagging_frequency\" : 7}\n    \nlgb_train = lgb.Dataset(train_x, label=train_y)\nlgb_val = lgb.Dataset(val_x, label=val_y)\nmodel = lgb.train(lgb_params, lgb_train, 300, valid_sets=[lgb_val], early_stopping_rounds=50, verbose_eval=100)","f1742c9f":"preds = model.predict(test, num_iteration=model.best_iteration)\ntest[\"PredictedLogRevenue\"] = np.expm1(preds)","a3e80a66":"sub_df = pd.DataFrame({\"fullVisitorId\":test_id})","5147dcd2":"sub_df[\"PredictedLogRevenue\"] = np.expm1(preds)\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"predictions_test_lgb.csv\", index=False)","5af915b3":"## Further data treatment","3e8f2dc5":"### Treating values in each column","85a71c95":"### Transforming features","62ed16fd":"#### \ttotals.pageviews","baddf2d7":"#### trafficSource.keyword","704315bd":"### Feature removal on the basis of correlation","6a42a4ec":"We see that totals.hits and totals.pageviews are highly correlated. Removing one of them. Since total pageViews had  nulls. We shall remove totals.pageviews-- Maybe after feature engineering\n\nWe also see a high correlation between visitid and visitstarttime. We will remove one of them too.\n\nVisitid also is highly correlated with date. We will also remove multicollinearity from there.","016750d0":"## Dimension Reduction (heuristic)","ccadaa59":"### Check if any any extra columns present in test or train","a9d64782":"## Null Value Treatment","ab1d4f81":"### Identifying columns with nulls to be treated","5926bbb4":"## Investigating target variable","30fea0ed":"## Model Building","e4767a0e":"## Load dataset","93a62a11":"### create and train model","c7923dec":"### Check null data- drop columns with nulls>70%","a2ba6c65":"#### trafficSource.isTrueDirect","ac6030b4":"This proves: The 80\/20 rule has proven true for many businesses\u2013only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies.","ef737818":"### Feature removal on the basis of instincts and variable meaning","07368c5f":"#### totals.newVisits","4f05eea8":"### Changing data types","e42e74d8":"### Check correlation","ac8e3e58":"## Feature Engineering and Processing","e20cf940":"### Drop constant columns and Session ID","c03df831":"## Final Feature Selection","18562172":"### predictions","d01142b3":"### Separating target variable","4a7ab5d7":"#### totals.bounces","89adfea9":"Here we will focus on:\n1. correcting datatypes "}}