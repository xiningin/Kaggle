{"cell_type":{"724fe8de":"code","20c8c9d1":"code","43bfd4d3":"code","ae318cb3":"code","483f2478":"code","4da1dfaf":"code","5515f1f4":"code","237b8517":"code","f1db3ca1":"code","fc2986b3":"code","3358b494":"code","ee28d73d":"code","777757ae":"code","12cf78bf":"code","00e4c6df":"code","0ff13d15":"code","376ed2e8":"code","e396ba55":"code","c8cab6a3":"code","8a6d9e15":"code","0c86d347":"code","459bf4a4":"code","b3e8b7d7":"code","7f96feac":"code","0a2e9c3b":"code","59c246ef":"markdown","71fd3c4f":"markdown","efec06fa":"markdown","d915d5c7":"markdown","0572d1f5":"markdown","8c9c262d":"markdown","f2058f0c":"markdown","7767d2d8":"markdown","f1d1cf14":"markdown","68084f35":"markdown","ce9fe478":"markdown","e8013772":"markdown","15405422":"markdown","77f084cb":"markdown","bd0acb01":"markdown","db211365":"markdown"},"source":{"724fe8de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","20c8c9d1":"#all the imports\nfrom keras.layers import Input,Dense\nfrom keras.models import Model,Sequential\nfrom keras import regularizers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nnp.random.seed(203)\n","43bfd4d3":"data = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndata[\"Time\"] = data[\"Time\"].apply(lambda x: x\/3600 % 24)\ndata.head()","ae318cb3":"vc = data[\"Class\"].value_counts().to_frame().reset_index()\nvc[\"percent\"] = vc[\"Class\"].apply(lambda x: round(100*float(x)\/len(data),2))\nvc= vc.rename(columns={\"index\":\"Target\",\"Class\":\"Count\"})\nvc","483f2478":"non_fraud = data[data[\"Class\"] == 0].sample(1000)\nfraud = data[data[\"Class\"] == 1]\n\ndf = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\nX = df.drop([\"Class\"],axis = 1).values\ny = df[\"Class\"].values","4da1dfaf":"def tsne_plot(x1,y1,name=\"graph.png\"):\n    tsne = TSNE(n_components = 2,random_state=0)\n    X_t = tsne.fit_transform(x1)\n    \n    plt.figure(figsize=(12,8))\n    plt.scatter(X_t[np.where(y1==0),0], X_t[np.where(y1==0),1],marker=\"o\",color=\"g\",linewidth =\"1\",alpha = 0.8,label = \"Non Fraud\")\n    plt.scatter(X_t[np.where(y1==1),0], X_t[np.where(y1==1),1],marker=\"o\",color=\"r\",linewidth =\"1\",alpha = 0.8,label = \"Fraud\")\n    \n    plt.legend(loc=\"best\");\n    plt.savefig(name);\n    plt.show();\n    \ntsne_plot(X,y,name=\"original.png\")","5515f1f4":"#input layer\ninput_layer = Input(shape = (X.shape[1],))\n\n#encoding part\nencoded = Dense(100,activation = 'tanh',activity_regularizer=regularizers.l1(10e-5))(input_layer)\nencoded = Dense(50,activation=\"relu\")(encoded)\n\n\n#decoding part\ndecoded = Dense(50,activation = \"tanh\")(encoded)\ndecoded = Dense(50,activation = \"tanh\")(decoded)\n\n\n## output layer\noutput_layer = Dense(X.shape[1],activation = \"relu\")(decoded)\nprint(output_layer)","237b8517":"autoencoder = Model(input_layer,output_layer)\nautoencoder.compile(optimizer=\"adadelta\",loss=\"mse\")","f1db3ca1":"x = data.drop([\"Class\"],axis=1)\ny = data[\"Class\"].values\n\nx_scale = preprocessing.MinMaxScaler().fit_transform(x.values)\nx_norm,x_fraud = x_scale[y==0],x_scale[y==1]","fc2986b3":"autoencoder.fit(x_norm[0:2000],x_norm[0:2000],\n               batch_size = 256,epochs = 10,\n               shuffle = True,validation_split = 0.20);","3358b494":"hidden_representation = Sequential()\nhidden_representation.add(autoencoder.layers[0])\nhidden_representation.add(autoencoder.layers[1])\nhidden_representation.add(autoencoder.layers[2])","ee28d73d":"norm_hid_rep = hidden_representation.predict(x_norm[:3000])\nfraud_hid_rep = hidden_representation.predict(x_fraud)","777757ae":"rep_x = np.append(norm_hid_rep,fraud_hid_rep,axis=0)\ny_n = np.zeros(norm_hid_rep.shape[0])\ny_f = np.ones(fraud_hid_rep.shape[0])\nrep_y = np.append(y_n,y_f)\ntsne_plot(rep_x,rep_y,\"latent_representation.png\")","12cf78bf":"train_x, val_x, train_y,val_y = train_test_split(rep_x,rep_y,test_size=0.25)\nclf = LogisticRegression(solver = \"lbfgs\").fit(train_x,train_y)\npred_y = clf.predict(val_x)\nprint(\"\")\nprint(\"Classification report\")\nprint(classification_report(val_y,pred_y))\n\nprint(\"\")\nprint(\"Accuracy_score: \",accuracy_score(val_y,pred_y))","00e4c6df":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","0ff13d15":"import re \nfull_data = [train, test]\n\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)","376ed2e8":"for dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1","e396ba55":"for dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)","c8cab6a3":"def get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)    \n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare']         = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare']         = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    dataset.loc[ dataset['Age'] <= 16, 'Age']        = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\n\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","8a6d9e15":"X = train.drop([\"Survived\"],axis = 1)\ny = train[\"Survived\"]\ny = y.values","0c86d347":"#define the modl\ninput_layer = Input(shape=(X.shape[1],))\nencoded = Dense(100,activation=\"tanh\",activity_regularizer=regularizers.l1(10e-5))(input_layer)\nencoded = Dense(50,activation=\"relu\")(encoded)\n\ndecoded = Dense(50,activation=\"tanh\")(encoded)\ndecoded = Dense(100,activation=\"tanh\")(decoded)\noutput_layer = Dense(X.shape[1],activation=\"relu\")(decoded)\n\nautoencoder = Model(input_layer,output_layer)\nautoencoder.compile(optimizer=\"adadelta\",loss = \"mse\")","459bf4a4":"scaler = preprocessing.MinMaxScaler()\nscaler.fit(X.values)\nX_scale = scaler.transform(X.values)\ntest_x_scale = scaler.transform(test.values)\n\nx_perished, x_survived = X_scale[y == 0], X_scale[y == 1]\nautoencoder.fit(x_perished, x_perished, epochs = 20, shuffle = True, validation_split = 0.25)","b3e8b7d7":"hidden_representation = Sequential()\nhidden_representation.add(autoencoder.layers[0])\nhidden_representation.add(autoencoder.layers[1])\nhidden_representation.add(autoencoder.layers[2])\n","7f96feac":"perished_hid_rep = hidden_representation.predict(x_perished)\nsurvived_hid_rep = hidden_representation.predict(x_survived)\n\nrep_x = np.append(perished_hid_rep, survived_hid_rep, axis = 0)\ny_n = np.zeros(perished_hid_rep.shape[0])\ny_f = np.ones(survived_hid_rep.shape[0])\nrep_y = np.append(y_n, y_f)","0a2e9c3b":"train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.25)\nclf = LogisticRegression().fit(train_x, train_y)\npred_y = clf.predict(val_x)\n\nprint (classification_report(val_y, pred_y))\nprint (accuracy_score(val_y, pred_y))","59c246ef":"Train the model\n","71fd3c4f":"The beauty of this approach is we do not need too many samples of data for learning the good representations, we will use only 2000 rows of non-fraud cases to train the autoencoder. We also need not run this model for a large number of epochs. \n\n**Explanation** : The choice of small samples from the original datasets is based on the intuition that one class characteristics (non-fraud) will differe from that of the other(fraud). To distinguish these characteristics we need to show the autoencoders only one class of data. This is because the autoencoder will try to learn only one class and automatically distinguish the other class","efec06fa":"train the classifier","d915d5c7":"Now we can just train  a simple linear classifier on the dataset\n\n### Simple Linear Classifier","0572d1f5":"we can see that the dataset is imbalanced. The advantage of representation learning approach is that it can handle the imbalance of such problems, take only 1000 non-fraud transactions","8c9c262d":"generate the hidden representations of 2 classe: non-fraud and fraud by predicting the raw inputs using the above model\n\n","f2058f0c":"The dataset contains 28 anonymized variables, 1 amount variable, 1 time variable and 1 target variable\n\nLooking at the distribution of the target","7767d2d8":"From the above graph we can observe that there are many non_fraud transactions which are very close to fraud transactions, therefore it is difficult to classify from model\n\n## Autoencoders\nAutoencoders are special type of neural n\/w architecture in which the output is the same as input. Autoencoders are trained in an unsupervised manner in order to learn the extremely low level representations of the input data. These low level features are then deformed back to project the actual data. An autoencoder is a regression task where the network is asked to predict its input. These n\/ws have a tight bottleneck of a few neurons in the middle forcing them to create effective representations that compress the input into a low dimensional code that can be used by the decoder to reproduce the original input.\n\n\nWe will create an autoencoder model in which we will model only the non-fraud cases. the model will try to learn the best representation of non-fraud cases. The same model will be used to generate the representations of fraud cases and we expect them to be different from non-fraud ones.\n\nCreate a n\/w with 1 input layer and 1 output later having identical dimensions i.e. the shape of non-fraud cases. We will use keras package\n","f1d1cf14":"### Visualize the latent representations: fraud vs non-fraud","68084f35":"before training let's perform min max scaling","ce9fe478":"### Applying to a different dataset: titanic","e8013772":"create the model architecture by compiling input and output layers. Also add the optimizer and loss function. Here \"adadelta\" is used as an optimizer and mse as the loss function","15405422":"Obtain the hidden representation","77f084cb":"### Visualize fraud vs non-fraud transactions\n\nusing T-SNE( t-Distributed Stochastic Neighbor Embedding) is a dataset decomposition technique which reduces the dimensions of data and produces only top n components with maximum information\n\n\nEvery dot in the following represents a transaction. Non fraud is green while fraud is red","bd0acb01":"define the autoencoder model","db211365":"### Obtain the latent representations\nnow the model is trained. We are interested in obtaining the **latent representaion of the input** learned by the model. this can be accessed by the weights of the trained model. we will create another network containing sequential layers are we will only add the trained weights till the third layer where latent representation exists."}}