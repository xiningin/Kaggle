{"cell_type":{"d466b706":"code","eb465e9d":"code","51238b11":"code","cabca2df":"code","fb19bf58":"code","89222814":"code","05c73f09":"code","fd463607":"code","1e681f0e":"code","2e87fa25":"code","44f3fccb":"code","d3469c40":"code","6a090741":"code","cd788524":"code","c3c943bb":"code","f0c7088e":"code","ba721c5a":"code","8e102a02":"markdown","315769d2":"markdown","90ff75a2":"markdown","4d6f719a":"markdown","18bd5592":"markdown","062a2134":"markdown","dd5bedc0":"markdown","d5963021":"markdown","5043b290":"markdown","632a5d78":"markdown","d7965185":"markdown","10c2f48d":"markdown","89a3d3b2":"markdown","53163b52":"markdown","51980c83":"markdown"},"source":{"d466b706":"import tensorflow as tf\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\nexcept ValueError:\n    tpu = None\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()","eb465e9d":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 1.9 --apt-packages libomp5 libopenblas-dev","51238b11":"import os\n\nos.environ[\"XLA_USE_BF16\"] = \"1\"\nos.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\"\n\nimport torch\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\nimport gc\n\nfrom tqdm import tqdm\nfrom collections import OrderedDict, namedtuple\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport joblib\nfrom joblib import Parallel, delayed\n\nimport torch_xla.utils.serialization as xser\n\nimport time\n\nimport collections\nimport logging\nimport transformers\nfrom transformers import (\n    AdamW,\n    get_linear_schedule_with_warmup,\n    get_constant_schedule,\n    AutoTokenizer,\n    AutoModel,\n    AutoConfig,\n    get_cosine_schedule_with_warmup,\n)\nimport sys\nfrom sklearn import metrics, model_selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm.notebook import tqdm\n\nfrom random import shuffle\nimport random\n\nimport re\n\nimport warnings\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n","cabca2df":"!pip install -U wandb -qq\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"wandb\")\n\nimport wandb\n\nwandb.login(key=wandb_key)","fb19bf58":"class CFG:\n    \n    # wandb\n    project = \"chaii-qa\"\n    run_name = \"tpu-xlm-r-large\"\n    \n    # model\n    model_type = 'xlm_roberta'\n    model_name_or_path = \"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2\"\n    config_name = \"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2\"\n\n    # tokenizer\n    tokenizer_name = \"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2\"\n    max_seq_length = 384\n    doc_stride = 128\n\n    # train\n    epochs = 1\n    per_device_train_batch_size = 8\n    per_device_eval_batch_size = 16\n    n_folds = 5\n\n    # optimizer\n    optimizer_type = 'AdamW'\n    learning_rate = 2e-5\n    weight_decay = 1e-2\n    epsilon = 1e-8\n    max_grad_norm = 1.0\n\n    # scheduler\n    decay_name = 'linear-warmup'\n    warmup_ratio = 0.1\n\n    # logging\n    logging_steps = 10\n\n    # evaluate\n    output_dir = 'output'\n    seed = 2021","89222814":"train = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv\")\ntest = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv\")\nexternal_mlqa = pd.read_csv(\"..\/input\/mlqa-hindi-processed\/mlqa_hindi.csv\")\nexternal_xquad = pd.read_csv(\"..\/input\/mlqa-hindi-processed\/xquad.csv\")\nexternal_train = pd.concat([external_mlqa, external_xquad])\n\n\ndef create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n    for fold_num, (t_, v_) in enumerate(kf.split(X=data, y=data.language.values)):\n        data.loc[v_, \"kfold\"] = fold_num\n    return data\n\n\ntrain = create_folds(train, num_splits=CFG.n_folds)\nexternal_train[\"kfold\"] = -1\nexternal_train[\"id\"] = list(np.arange(1, len(external_train) + 1))\ntrain = pd.concat([train, external_train]).reset_index(drop=True)\n\n\ndef convert_answers(row):\n    return {\"answer_start\": [row[0]], \"text\": [row[1]]}\n\n\ntrain[\"answers\"] = train[[\"answer_start\", \"answer_text\"]].apply(convert_answers, axis=1)\n\ndel external_mlqa\ndel external_xquad\ndel external_train\ngc.collect();","05c73f09":"def prepare_train_features(args, example, tokenizer):\n    example[\"question\"] = example[\"question\"].lstrip()\n    tokenized_example = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=\"only_second\",\n        max_length=args.max_seq_length,\n        stride=args.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n\n    features = []\n    for i, offsets in enumerate(offset_mapping):\n        feature = {}\n\n        input_ids = tokenized_example[\"input_ids\"][i]\n        attention_mask = tokenized_example[\"attention_mask\"][i]\n\n        feature[\"input_ids\"] = input_ids\n        feature[\"attention_mask\"] = attention_mask\n        feature[\"offset_mapping\"] = offsets\n\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_example.sequence_ids(i)\n\n        sample_index = sample_mapping[i]\n        answers = example[\"answers\"]\n\n        if len(answers[\"answer_start\"]) == 0:\n            feature[\"start_position\"] = cls_index\n            feature[\"end_position\"] = cls_index\n        else:\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            if not (\n                offsets[token_start_index][0] <= start_char\n                and offsets[token_end_index][1] >= end_char\n            ):\n                feature[\"start_position\"] = cls_index\n                feature[\"end_position\"] = cls_index\n            else:\n                while (\n                    token_start_index < len(offsets)\n                    and offsets[token_start_index][0] <= start_char\n                ):\n                    token_start_index += 1\n                feature[\"start_position\"] = token_start_index - 1\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                feature[\"end_position\"] = token_end_index + 1\n\n        features.append(feature)\n    return features","fd463607":"class ChaiiDataset(torch.utils.data.Dataset):\n    def __init__(self, features, mode=\"train\"):\n        super().__init__()\n        self.features = features\n        self.mode = mode\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, item):\n        feature = self.features[item]\n        if self.mode == \"train\":\n            return {\n                \"input_ids\": torch.tensor(feature[\"input_ids\"], dtype=torch.long),\n                \"attention_mask\": torch.tensor(\n                    feature[\"attention_mask\"], dtype=torch.long\n                ),\n                \"offset_mapping\": torch.tensor(\n                    feature[\"offset_mapping\"], dtype=torch.long\n                ),\n                \"start_position\": torch.tensor(\n                    feature[\"start_position\"], dtype=torch.long\n                ),\n                \"end_position\": torch.tensor(feature[\"end_position\"], dtype=torch.long),\n            }\n        else:\n            return {\n                \"input_ids\": torch.tensor(feature[\"input_ids\"], dtype=torch.long),\n                \"attention_mask\": torch.tensor(\n                    feature[\"attention_mask\"], dtype=torch.long\n                ),\n                \"offset_mapping\": feature[\"offset_mapping\"],\n                \"sequence_ids\": feature[\"sequence_ids\"],\n                \"id\": feature[\"example_id\"],\n                \"context\": feature[\"context\"],\n                \"question\": feature[\"question\"],\n            }","1e681f0e":"class Model(nn.Module):\n    def __init__(self, modelname_or_path, config):\n        super(Model, self).__init__()\n        self.config = config\n        self.model = AutoModel.from_pretrained(modelname_or_path, config=config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def _reinit_layers(self, num_layers):\n        encoder = self.model.encoder\n        for layer in encoder.layer[-num_layers:]:\n            for module in layer.modules():\n                if isinstance(module, nn.Linear):\n                    module.weight.data.normal_(mean=0.0, std=encoder.config.initializer_range)\n                    if module.bias is not None:\n                        module.bias.data.zero_()\n                elif isinstance(module, nn.Embedding):\n                    module.weight.data.normal_(mean=0.0, std=encoder.config.initializer_range)\n                    if module.padding_idx is not None:\n                        module.weight.data[module.padding_idx].zero_()\n                elif isinstance(module, nn.LayerNorm):\n                    module.bias.data.zero_()\n                    module.weight.data.fill_(1.0)\n                    \n                \n    def forward(\n        self, \n        input_ids, \n        attention_mask=None, \n        # token_type_ids=None\n    ):\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n\n        sequence_output = outputs[0]\n        pooled_output = outputs[1]\n        \n        # sequence_output = self.dropout(sequence_output)\n        qa_logits = self.qa_outputs(sequence_output)\n        \n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n    \n        return start_logits, end_logits\n    ","2e87fa25":"def loss_fn(preds, labels):\n    start_preds, end_preds = preds\n    start_labels, end_labels = labels\n    \n    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n    total_loss = (start_loss + end_loss) \/ 2\n    return total_loss","44f3fccb":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.max = 0\n        self.min = 1e5\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n        if val > self.max:\n            self.max = val\n        if val < self.min:\n            self.min = val","d3469c40":"def train_loop_fn(data_loader, model, optimizer, device, num_batches, scheduler=None):\n\n    model.train()\n\n    losses = AverageMeter()\n    \n    tk0 = tqdm(data_loader, total=num_batches, desc=\"Training\", disable=not xm.is_master_ordinal())\n    start_time = time.time()\n\n    for bi, d in enumerate(tk0):\n\n        input_ids = d['input_ids']\n        attention_mask = d['attention_mask']\n        targets_start = d['start_position']\n        targets_end = d['end_position']\n\n\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        targets_start = targets_start.to(device)\n        targets_end = targets_start.to(device)\n\n        optimizer.zero_grad()\n\n        outputs_start, outputs_end = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n        )\n\n        loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n\n        loss.backward()\n        xm.optimizer_step(optimizer)\n\n        loss = loss.detach().item()\n\n        losses.update(loss, input_ids.size(0))\n\n        if scheduler is not None:\n            scheduler.step()\n        \n        print_loss = xm.mesh_reduce('loss_reduce', loss, reduce_fn)\n        tk0.set_postfix(loss=print_loss)\n        \n        if xm.is_master_ordinal():\n            wandb.log({\"train_loss\": print_loss})\n        \n        if bi % 10 == 0:\n            xm.master_print(\n                f\"bi={bi}, {time.time()-start_time:<2.2f} - loss:{print_loss}\"\n            )\n\n        \n    del loss\n    del losses\n    del print_loss\n    \n    gc.collect()","6a090741":"def eval_loop_fn(data_loader, model, device):\n\n    model.eval()\n\n    losses = AverageMeter()\n\n    with torch.no_grad():\n        for bi, d in enumerate(data_loader):\n\n            input_ids = d['input_ids']\n            attention_mask = d['attention_mask']\n            targets_start = d['start_position']\n            targets_end = d['end_position']\n            \n\n\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            targets_start = targets_start.to(device)\n            targets_end = targets_start.to(device)\n\n            outputs_start, outputs_end = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n            )\n\n            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n            losses.update(loss.item(), input_ids.size(0))\n            \n\n    if xm.is_master_ordinal():\n        wandb.log({\"eval_loss\": losses.avg})\n    xm.master_print(f\"EVAL loss={losses.avg}\")\n\n    return None","cd788524":"def reduce_fn(vals):\n    return sum(vals) \/ len(vals)\n\n\ndef _run(fold):\n\n    xm.master_print(f\"Starting fold {fold}\")\n    device = xm.xla_device()\n    model = mx.to(device)\n    \n    if xm.is_master_ordinal():\n        wandb.init(\n            project=CFG.project,\n            config={x:getattr(CFG, x) for x in dir(CFG) if \"__\" not in x},\n            name=CFG.run_name+f\"-fold-{k}\"\n                  )\n\n    \n    train_dataset = ChaiiDataset(sum([fts for i, fts in enumerate(all_features) if i!=fold], []))\n    valid_dataset = ChaiiDataset(all_features[fold])\n    \n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=True,\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=CFG.per_device_train_batch_size,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=2,\n    )\n\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n        valid_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False,\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=CFG.per_device_eval_batch_size,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=0,\n    )\n\n    num_train_steps = int(\n        len(train_dataset) \/ CFG.per_device_train_batch_size \/ xm.xrt_world_size() * CFG.epochs\n    ) \n\n    model_params = model.named_parameters()\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {\n            \"params\": [\n                p for n, p in model_params if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": CFG.weight_decay,\n        },\n        {\n            \"params\": [\n                p for n, p in model_params if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n\n    optimizer = AdamW(optimizer_parameters, lr=CFG.learning_rate * xm.xrt_world_size())\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n        num_training_steps=num_train_steps,\n    )\n\n    xm.master_print(\n        f\"num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}\"\n    )\n    \n    num_batches = int(len(train_dataset) \/ (CFG.per_device_train_batch_size * xm.xrt_world_size()))\n\n    for epoch in range(CFG.epochs):\n\n        xm.master_print(f\"Starting epoch {epoch}\")\n        mp_device_loader = pl.MpDeviceLoader(train_data_loader, device)\n        train_loop_fn(\n            mp_device_loader,\n            model,\n            optimizer,\n            device,\n            num_batches,\n            scheduler,\n        )\n\n        del mp_device_loader\n        gc.collect()\n\n        mp_device_loader = pl.MpDeviceLoader(valid_data_loader, device)\n        eval_loop_fn(mp_device_loader, model, device)\n        \n        del mp_device_loader\n        gc.collect()\n    \n    if xm.is_master_ordinal():\n        wandb.finish()\n\n    return","c3c943bb":"def _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = _run(flags[\"fold\"])","f0c7088e":"config = AutoConfig.from_pretrained(CFG.config_name)\ntokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_name)\n\nall_features = [None]*CFG.n_folds\n\nfor k in range(CFG.n_folds):\n    print('Preparing fold', k)\n    fold_data = train[train[\"kfold\"]==k]\n    all_features[k] = [features for row in fold_data.itertuples() for features in prepare_train_features(CFG, row._asdict(), tokenizer)]","ba721c5a":"for k in range(CFG.n_folds):  \n    \n    \n    # use model wrapper for reducing memory usage across TPU cores\n    mx = xmp.MpModelWrapper(Model(CFG.model_name_or_path, config))\n  \n    FLAGS={}\n    FLAGS[\"fold\"] = k\n    xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')\n    \n    save_dir = f\"fold-{k}\"\n    \n    tokenizer.save_pretrained(save_dir)    \n    config.save_pretrained(save_dir)    \n    torch.save(mx._model.state_dict(), f\"{save_dir}\/pytorch_model.bin\")","8e102a02":"# Train","315769d2":"# Import Packages \ud83d\udce6","90ff75a2":"# Dataset","4d6f719a":"# Model ","18bd5592":"# Eval","062a2134":"# Loss function","dd5bedc0":"# Setup Data \ud83d\udcca","d5963021":"# Chaii-QA: PyTorch XLM-R Large TPU \u26a1\n\n### This notebook is made referencing the following notebooks: \ud83d\ude4f\n- https:\/\/www.kaggle.com\/abhishek\/roberta-on-steroids-pytorch-tpu-training\n- https:\/\/www.kaggle.com\/philippsinger\/xlm-roberta-large-pytorch-pytorch-tpu\n- https:\/\/www.kaggle.com\/rhtsingh\/chaii-qa-5-fold-xlmroberta-torch-fit\n\n### If you find this notebook helpful, then please consider upvoting the notebooks it was made from. \ud83d\udd3c\n\n### I am by no means a TPU expert, so if you have recommendations, I am all ears! \ud83d\udc42\n\n### To Do \u2705:\n#### 1. Add jaccard score \ud83d\udcaf\n#### 2. Incorporate Weights and Biases for logging \u2696\ufe0f\n#### 3. ??? Leave a suggestion in the comments \ud83d\udcad \n#### 4. Add section to show how to do inference on GPU\n#### 5. Do full 5-fold training","5043b290":"# Configuration \ud83d\udcdd","632a5d78":"# Install XLA \ud83d\udc68\u200d\ud83d\udcbb","d7965185":"# Set up Weights and Biases \u2696\ufe0f","10c2f48d":"# Connect to TPU \ud83d\udd17","89a3d3b2":"# Loss Recorder","53163b52":"# Prepare features","51980c83":"# Run"}}