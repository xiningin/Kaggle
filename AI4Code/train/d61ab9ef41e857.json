{"cell_type":{"4343e45e":"code","a8b8780c":"code","42a30a61":"code","b8ba53be":"code","4dc66684":"code","dca2f86c":"code","65e10a92":"code","6610080f":"code","0ba54786":"code","f15621fa":"code","8414bfa4":"code","6738c901":"code","ca29886b":"code","c8d89c48":"code","d26ce6f9":"code","4171384b":"code","8befa76e":"code","0d756bce":"code","6526015a":"markdown","ef3ef202":"markdown","b7025a81":"markdown","f8f97772":"markdown","805e7621":"markdown","f8b00357":"markdown","7f783b4f":"markdown"},"source":{"4343e45e":"!pip install segmentation-models-pytorch","a8b8780c":"import os.path as osp\nfrom glob import glob\nimport json\nimport random\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.utils.data as data\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport segmentation_models_pytorch as smp\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn","42a30a61":"def fix_seed(seed):\n    # random\n    random.seed(seed)\n    # Numpy\n    np.random.seed(seed)\n    # Pytorch\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True","b8ba53be":"# Seed\u5024\u306e\u56fa\u5b9a\nSEED = 42\nfix_seed(SEED)\n\n# \u30d1\u30b9\u306e\u5b9a\u7fa9\nrootpath = \"..\/input\/driving-segmentation-inclass-competition\/\"\ntrain_dir = osp.join(rootpath, \"train\")\ntest_dir = osp.join(rootpath, \"test\")\nconfig_path = osp.join(rootpath, \"config.json\")\nwith open(config_path) as f:\n    config = json.load(f)\n\n# \u5b9a\u6570\u3092\u5b9a\u7fa9\nclass_num = len(config['labels'])\nimg_size = (1920, 1080)","4dc66684":"# id\u3092\u30ad\u30fc\u3068\u3057\u3066label\u306e\u60c5\u5831\u3092\u683c\u7d0d\u3059\u308bdict\u3092\u4f5c\u6210\nlabels_map = {}\nfor label_dict in config['labels']:\n    labels_map[label_dict['id']] = label_dict\n    del label_dict['id']\nlabels_map","dca2f86c":"img_list, anno_list = sorted(glob(osp.join(train_dir, 'frames\/*'))), sorted(glob(osp.join(train_dir, 'labels\/*')))\nlen(img_list), len(anno_list)","65e10a92":"# (RGB)\u306e\u8272\u306e\u5e73\u5747\u5024\u3068\u6a19\u6e96\u504f\u5dee\ninput_size = 512\nencoder = 'resnet34'\n\n# pytorch-segmentation\u306e\u524d\u51e6\u7406\u95a2\u6570\u3092\u53d6\u5f97\npreprocess_input = get_preprocessing_fn(encoder, pretrained='imagenet')\n\n# \u8a13\u7df4\u7528\ntrain_transform = A.Compose([\n    A.Rotate((-10, 10)),  # \u56de\u8ee2\n    A.HorizontalFlip(p=0.5),  # \u30e9\u30f3\u30c0\u30e0\u30df\u30e9\u30fc\n    A.Resize(input_size, input_size),  # \u30ea\u30b5\u30a4\u30ba(input_size)\n    A.Lambda(image=preprocess_input), # \u524d\u51e6\u7406\u306e\u5b9f\u884c\n    ToTensorV2(),\n])\n\n# \u691c\u8a3c\u7528\nval_transform = A.Compose([\n    A.Resize(input_size, input_size),  # \u30ea\u30b5\u30a4\u30ba(input_size)\n    A.Lambda(image=preprocess_input), # \u524d\u51e6\u7406\u306e\u5b9f\u884c\n    ToTensorV2(),\n])","6610080f":"class Dataset(data.Dataset):\n\n    def __init__(self, img_list, anno_list=None, transform=None):\n        self.img_list = img_list\n        self.anno_list = anno_list\n        self.transform = transform\n\n    def __len__(self):\n        '''\u753b\u50cf\u306e\u679a\u6570\u3092\u8fd4\u3059'''\n        return len(self.img_list)\n\n    def __getitem__(self, index):\n        '''\n        \u524d\u51e6\u7406\u3092\u3057\u305f\u753b\u50cf\u306eTensor\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u3068\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u53d6\u5f97\n        '''\n        # 1. \u753b\u50cf\u8aad\u307f\u8fbc\u307f\n        image_file_path = self.img_list[index]\n        img = cv2.imread(image_file_path)   # [\u9ad8\u3055][\u5e45][\u8272RGB]\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # 2. \u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u753b\u50cf\u8aad\u307f\u8fbc\u307f\n        anno_class_img = torch.tensor(float('nan'))\n        if self.anno_list is not None:\n            anno_file_path = self.anno_list[index]\n            anno_class_img = cv2.imread(anno_file_path, cv2.IMREAD_GRAYSCALE)  # [\u9ad8\u3055][\u5e45]\n\n        # 3. \u524d\u51e6\u7406\u3092\u5b9f\u65bd\n        if self.transform is not None:\n            if self.anno_list is not None:\n                transformed = self.transform(image=img, mask=anno_class_img)\n                img, anno_class_img = transformed['image'], transformed['mask']\n            else:\n                transformed = self.transform(image=img)\n                img = transformed['image']\n        return img.float(), anno_class_img.float()","0ba54786":"# \u52d5\u4f5c\u78ba\u8a8d\n\n# \u30c7\u30fc\u30bf\u5206\u5272\ntrain_img_list, val_img_list, train_anno_list, val_anno_list = train_test_split(img_list, anno_list, random_state=SEED)\n\n# \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210\ntrain_dataset = Dataset(train_img_list, train_anno_list, transform=train_transform)\nval_dataset = Dataset(val_img_list, val_anno_list, transform=val_transform)\n\n# \u30c7\u30fc\u30bf\u306e\u53d6\u308a\u51fa\u3057\u4f8b\nimg, anno_class_img = val_dataset[0]\nprint(img.shape)\nprint(anno_class_img.shape)\n\n# \u753b\u50cf\u306e\u63cf\u753b\nimg_val = img.numpy().transpose((1, 2, 0))\nplt.imshow(img_val)\nplt.show()\n\n# \u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u753b\u50cf\u306e\u8868\u793a\nanno_class_img_val = anno_class_img.numpy()\nplt.imshow(anno_class_img_val)\nplt.show()","f15621fa":"# \u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc\u306e\u4f5c\u6210\nbatch_size = 32\n\ntrain_dataloader = data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True)\n\nval_dataloader = data.DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False)","8414bfa4":"# \u52d5\u4f5c\u306e\u78ba\u8a8d\nbatch_iterator = iter(train_dataloader)  # \u30a4\u30bf\u30ec\u30fc\u30bf\u306b\u5909\u63db\nimges, anno_class_imges = next(batch_iterator)  # 1\u756a\u76ee\u306e\u8981\u7d20\u3092\u53d6\u308a\u51fa\u3059\nprint(imges.size())  # torch.Size([8, 3, 512, 512])\nprint(anno_class_imges.size())  # torch.Size([8, 512, 512])","6738c901":"model = smp.Unet(\n    encoder_name=encoder,   \n    encoder_weights=\"imagenet\", \n    in_channels=3,               \n    classes=class_num+1,\n)","ca29886b":"num_epochs = 5\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()","c8d89c48":"# GPU\u304c\u4f7f\u3048\u308b\u304b\u78ba\u8a8d\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"\u4f7f\u7528\u30c7\u30d0\u30a4\u30b9\uff1a\", device)\n\nmodel = model.to(device)\nbest_val_loss = float('inf')\n\nfor epoch in range(num_epochs):\n\n    epoch_train_loss = 0.0  # epoch\u306e\u640d\u5931\u548c\n    epoch_val_loss = 0.0  # epoch\u306e\u640d\u5931\u548c\n\n    print('-------------')\n    print(f'epoch {epoch+1}\/{num_epochs}')\n    print('-------------')\n    \n    # train\n    model.train()\n    for imges, anno_class_imges in tqdm(train_dataloader):\n        imges = imges.to(device)\n        anno_class_imges = anno_class_imges.to(device)\n        \n        outputs = model(imges)\n        loss = criterion(outputs, anno_class_imges.long())\n        \n        loss.backward()  # \u52fe\u914d\u306e\u8a08\u7b97\n        optimizer.step() # \u52fe\u914d\u306e\u66f4\u65b0\n        optimizer.zero_grad() # \u52fe\u914d\u306e\u521d\u671f\u5316\n        \n        epoch_train_loss += loss.item()\n    \n    \n    # val\n    model.eval()\n    for imges, anno_class_imges in tqdm(val_dataloader):\n        imges = imges.to(device)\n        anno_class_imges = anno_class_imges.to(device)\n        \n        with torch.no_grad():\n            outputs = model(imges)\n            loss = criterion(outputs, anno_class_imges.long())\n        epoch_val_loss += loss.item()\n    \n    if epoch_val_loss < best_val_loss:\n        best_val_loss = epoch_val_loss\n        # \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4fdd\u5b58\u3059\u308b\n        torch.save(model.state_dict(), f'best.pth')\n    \n    print(f'epoch {epoch+1} || epoch_train_loss:{epoch_train_loss:.4f} || epoch_val_loss:{epoch_val_loss:.4f}')\n    \n","d26ce6f9":"model.load_state_dict(torch.load('best.pth'))\n\ntest_img_list, test_anno_list = sorted(glob(osp.join(test_dir, 'frames\/*'))), sorted(glob(osp.join(test_dir, 'labels\/*')))\ntest_dataset = Dataset(test_img_list, anno_list=None, transform=val_transform)\ntest_dataloader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","4171384b":"# test\npreds = []\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.eval()\nfor imges, anno_class_imges in tqdm(test_dataloader):\n    imges = imges.to(device)\n    anno_class_imges = anno_class_imges.to(device)\n\n    with torch.no_grad():\n        outputs = model(imges)\n        outputs = outputs.argmax(dim=1).detach().cpu().numpy().astype('uint8')\n    \n    preds += [outputs]\n    \npreds = np.concatenate(preds)","8befa76e":"# \u30e9\u30f3\u30ec\u30f3\u30b0\u30b9\u7b26\u53f7\u3078\u30a8\u30f3\u30b3\u30fc\u30c9\u3059\u308b\u95a2\u6570\ndef rle_encode(img):\n    \"\"\" TBD\n    \n    Args:\n        img (np.array): \n            - 1 indicating mask\n            - 0 indicating background\n    \n    Returns: \n        run length as string formated\n    \"\"\"\n    \n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","0d756bce":"result = pd.read_csv(osp.join(rootpath, 'sample_submission.csv'))\nencoded_pixels = []\nfor pred in tqdm(preds):\n    pred = cv2.resize(pred, dsize=img_size, interpolation=cv2.INTER_NEAREST)\n    for label_id in range(1, class_num+1):\n        rle = rle_encode(np.where(pred==label_id, 1, 0))\n        encoded_pixels += [rle]\nresult['EncodedPixels'] = encoded_pixels\nresult.to_csv('result.csv', index=False)","6526015a":"# \u4e8b\u524d\u6e96\u5099","ef3ef202":"# \u30e2\u30c7\u30eb\u5b9a\u7fa9","b7025a81":"# Dataset\u306e\u4f5c\u6210","f8f97772":"# \u63a8\u8ad6","805e7621":"# \u63d0\u51fa\u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210","f8b00357":"# Dataloader\u306e\u4f5c\u6210","7f783b4f":"# \u5b66\u7fd2"}}