{"cell_type":{"3c94698a":"code","8c572100":"code","ee01a58a":"code","0a318356":"code","f0afdd73":"code","3e5d0f3a":"code","0d35de99":"code","e22b83a9":"code","b2669a83":"code","4e18325b":"code","07739f67":"code","e5d0a095":"code","cb02ec43":"code","12fe6b0f":"code","769de977":"code","d424344e":"code","e85d49a1":"code","224ce2e9":"code","cdb3b72a":"code","6a552b1d":"code","4180d051":"code","ad3eb3d3":"code","99a7b62d":"code","4ced945e":"code","2e79a749":"code","cf409c42":"code","960787e0":"code","f6e5b9ba":"code","35e7f37f":"code","e21aaf3b":"code","dc280243":"code","ec1a437e":"code","65cb8b13":"code","af4f1d9b":"code","c131a17b":"code","4aa2c6d5":"code","5244309e":"code","ec6d7b20":"code","b0326d3d":"code","9edd5f40":"code","23bae101":"code","0d5f6d76":"code","acd60998":"code","2c9692fc":"code","41d044bc":"markdown","0775fd94":"markdown","ba677dfb":"markdown","99e27244":"markdown","0763c036":"markdown","e6929ba6":"markdown","7e8fa071":"markdown","f5f8e02b":"markdown","5aa1f0c3":"markdown","8ad91931":"markdown","625f50f0":"markdown","14065355":"markdown","e4c810d5":"markdown","5c2b709f":"markdown","d0cef883":"markdown","91dd4b4e":"markdown"},"source":{"3c94698a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8c572100":"import warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import metrics","ee01a58a":"#Read the datasets\ntrain = pd.read_csv(r'\/kaggle\/input\/train.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/test.csv')","0a318356":"train.head()","f0afdd73":"train.info()","3e5d0f3a":"#Since for cabin there are only 204 rows that have values we can say that this coloum is better dropped\ntrain.drop('Cabin',axis=1,inplace=True)\ntest.drop('Cabin',axis=1,inplace=True)","0d35de99":"#For Embarked there are 2 rows that are having null values\ntrain[train.Embarked.isnull()]","e22b83a9":"#Since we can see that Both are form Pclass 1 we can just impute the most Embarked values from Pclass\ntrain[['Pclass','Embarked']][train.Pclass==1].groupby('Embarked').count()","b2669a83":"#So Changing NaN to 'S'\ntrain.Embarked = train.Embarked.fillna('S')","4e18325b":"#Now Age has 19% of missing values, so the logic is impute it with median by grouping Parch,Sex\ntrain.Age = train[['Parch','Sex','Age']].groupby(['Parch','Sex']).transform(lambda group: group.fillna(group.median()))\ntest.Age = test[['Parch','Sex','Age']].groupby(['Parch','Sex']).transform(lambda group: group.fillna(group.median()))","07739f67":"#Since we cant impute the age dropping the rows having missing values\ntrain.dropna(subset=['Age'],inplace=True)","e5d0a095":"#Converting binary to numeric\ntrain.Sex = train.Sex.map({'male':1,'female':0})\ntest.Sex = test.Sex.map({'male':1,'female':0})","cb02ec43":"#One-hot encoding for Embarked\nEmbark = pd.get_dummies(train.Embarked)\nEmbark.drop('S',axis=1,inplace=True)\ntrain = pd.concat([train,Embark],axis=1)\nEmbark1 = pd.get_dummies(test.Embarked)\nEmbark1.drop('S',axis=1,inplace=True)\ntest = pd.concat([test,Embark1],axis=1)","12fe6b0f":"train.describe(percentiles=[.25,.5,.75,.9,.95,.98])","769de977":"#Plotting the scatter plot\nplt.scatter(train.Fare,train.Fare)","d424344e":"#Hence deleting the coloum\ntrain = train[train.Fare!=512.329200]","e85d49a1":"#Dropping the unrequired colums\ntrainbkp = train[['PassengerId']]\ntrain.drop(['Name','Ticket','Embarked','PassengerId'],axis=1,inplace=True)\ntestbkp = test[['PassengerId']]\ntest.drop(['Embarked','PassengerId','Name','Ticket'],axis=1,inplace=True)","224ce2e9":"y = train.pop('Survived')\nx = train\nx_test = test","cdb3b72a":"plt.figure(figsize=(10,5))\nsns.heatmap(x.corr(),annot=True)","6a552b1d":"x.drop('Parch',axis=1,inplace=True)\nx_test.drop('Parch',axis=1,inplace=True)","4180d051":"#checking the corelation matix once again\nsns.heatmap(x.corr(),annot=True)","ad3eb3d3":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nx[['Pclass','Age','SibSp','Fare']] = scaler.fit_transform(x[['Pclass','Age','SibSp','Fare']])\nx_test[['Pclass','Age','SibSp','Fare']] = scaler.transform(x_test[['Pclass','Age','SibSp','Fare']])","99a7b62d":"x.describe()","4ced945e":"lm1 = sm.GLM(y,(sm.add_constant(x)),family= sm.families.Binomial())\nlm1.fit().summary()","2e79a749":"lgrg = LogisticRegression()\nrfe =RFE(lgrg,5)","cf409c42":"rfe = rfe.fit(x,y)","960787e0":"rfe.support_","f6e5b9ba":"list(zip(x.columns,rfe.support_))","35e7f37f":"#Hence considering the colums\ncols = x.columns[rfe.support_]\nx_sm = sm.add_constant(x[cols])","e21aaf3b":"lm2 = sm.GLM(y,x_sm,family=sm.families.Binomial())\nmd1 = lm2.fit()","dc280243":"y_pred = md1.predict(x_sm)\ny_pred = y_pred.values.reshape(-1)","ec1a437e":"#Crearting the dataframe with survied,predicated,passenger\nanswer = pd.DataFrame({'PassengerID':trainbkp.PassengerId,'Survived':y,'Predicted':y_pred})\nanswer['Predicted5'] = answer['Predicted'].map(lambda x:1 if x>0.5 else 0)","65cb8b13":"#checking the VIF\nvif = pd.DataFrame()\nvif['Features'] = x_sm.columns\nvif['VIF'] = [variance_inflation_factor(x_sm.values,i)for i in range(x_sm.shape[1])]\nvif['VIF'] = round(vif['VIF'],2)\nvif = vif.sort_values(by='VIF',ascending=False)\nvif","af4f1d9b":"metrics.accuracy_score(answer.Survived,answer.Predicted5)","c131a17b":"numbers = [float (x)\/10 for x in range(10)]\nfor i in numbers:\n        answer[i] = answer.Predicted.map(lambda x: 1 if x>i else 0)","4aa2c6d5":"\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(answer['Survived'], answer[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","5244309e":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","ec6d7b20":"asnwer = answer[['PassengerID','Survived',0.4]]","b0326d3d":"metrics.accuracy_score(answer['Survived'],answer[0.4])","9edd5f40":"x_test = x_test[cols]","23bae101":"x_test = sm.add_constant(x_test)\ny_test = md1.predict(x_test)\ny_test = y_test.values.reshape(-1)","0d5f6d76":"sol = pd.DataFrame()\nsol['PassengerID'] = testbkp['PassengerId']\nsol['y_test'] = y_test\nsol['Survived'] = sol['y_test'].map(lambda x: 1 if x>0.4 else 0)\nsol.drop('y_test',axis=1,inplace=True)","acd60998":"sol.columns = ['PassengerId','Survived']","2c9692fc":"sol.to_csv(r'gender_submission.csv')","41d044bc":"*Converting the data to numeric*","0775fd94":"*Checking for outliers*","ba677dfb":"#### Step1: Reading the required dataframe","99e27244":"#### Step6:Scaling the data","0763c036":"Note:- Here 512 for fare seems a outlier","e6929ba6":"#### Step8: Feature slection","7e8fa071":"#### Step4:Splitting the predictor variable","f5f8e02b":"#### Step9: Working on test dataset","5aa1f0c3":"#### Importing the required libraries","8ad91931":"#### Step3:Cleaning the data","625f50f0":"#### Step2:Getting the feel for dataframe","14065355":"#### Step7:Model Bulding","e4c810d5":"#### Step5: Looking for any co-relations","5c2b709f":"Note:- So here there are null coloums,'Age','cabin','Embarked' needs address","d0cef883":"Note: from above 0.4 is optimal cure","91dd4b4e":"Observations:- Fare and Pclass are negatively corelated, dropping Pfare coloum"}}