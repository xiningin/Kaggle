{"cell_type":{"f4c66b44":"code","b9f532e8":"code","aef971d3":"code","6b6ed14a":"code","888b7f27":"code","993a5fbf":"code","262e29b3":"code","6dcbd1f9":"code","531c311a":"code","aecef259":"code","53b5b1c9":"code","37f6bbfc":"markdown","4237de7e":"markdown","d9c32d60":"markdown","c4e27556":"markdown","5b90658e":"markdown","8b96b835":"markdown","201a9766":"markdown"},"source":{"f4c66b44":"import numpy as np\nimport pandas as pd \nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nimport warnings\nwarnings.filterwarnings('ignore')","b9f532e8":"data = '..\/input\/tabular-playground-series-oct-2021\/train.csv'\ndf = pd.read_csv(data)\ntest=pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')","aef971d3":"test","6b6ed14a":"X = df.drop(['loss','id'], axis=1)\ntest=test.drop('id',axis=1)\ny = df['loss']\n","888b7f27":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 0)","993a5fbf":"space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n        'gamma': hp.uniform ('gamma', 1,9),\n        'reg_alpha' : hp.quniform('reg_alpha', 50,150,1),\n        'reg_lambda' : hp.quniform('reg_lambda', 40,100,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'n_estimators': hp.quniform('n_estimators',5000,10000,1000),\n        'tree_method':'gpu_hist',\n        'subsample': hp.uniform('subsample', 0,1),\n        'learning_rate': hp.uniform('learning_rate',0.000001,1),\n        'seed': 0\n    }\n\n","262e29b3":"from sklearn.metrics import mean_squared_error\ndef objective(space):\n    clf=xgb.XGBRegressor(\n                    n_estimators =int(space['n_estimators']), \n                    max_depth = int(space['max_depth']), \n                    gamma = space['gamma'],\n                    reg_alpha = int(space['reg_alpha']),\n                    reg_lambda =int(space['reg_lambda']),\n                    min_child_weight=int(space['min_child_weight']),\n                    colsample_bytree=int(space['colsample_bytree']))\n    \n    evaluation = [( X_train, y_train), ( X_test, y_test)]\n    \n    clf.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"rmse\",\n            early_stopping_rounds=10,verbose=False)\n    \n\n    pred = clf.predict(X_test)\n    accuracy = mean_squared_error(y_test,pred)\n    print (\"SCORE:\", accuracy)\n    return {'loss': -accuracy, 'status': STATUS_OK }","6dcbd1f9":"\nimport warnings\nwarnings.filterwarnings('ignore')\ntrials = Trials()\n\nbest_hyperparams = fmin(fn = objective,\n                        space = space,\n                        algo = tpe.suggest,\n                        max_evals = 10,\n                        trials = trials)","531c311a":"best_hyperparams","aecef259":"from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\nparams = {'colsample_bytree': 0.824275003154649,\n 'gamma': 7.565239251710113,\n 'learning_rate': 0.035137857511338515,\n 'max_depth': 11,\n 'min_child_weight': 10,\n 'n_estimators': 6000,\n 'reg_alpha': 54,\n 'reg_lambda': 54,\n 'subsample': 0.7862419047343154}\nparams['tree_method'] = 'gpu_hist'\n\n\nsplits = 12\nstf = StratifiedKFold(n_splits=splits, shuffle=True)\noof= np.zeros((X.shape[0],))\nprediction = 0\nmodel_fi = 0\ntotal_mean_rmse = 0\n\nfor num, (train_id, valid_id) in enumerate(stf.split(X, y)):\n    X_train, X_valid = X.loc[train_id], X.loc[valid_id]\n    y_train, y_valid = y.loc[train_id], y.loc[valid_id]\n    \n    model = xgb.XGBRegressor(**params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",verbose=0)\n    \n    prediction += model.predict(test) \/ splits\n    oof[valid_id] = model.predict(X_valid)\n    oof[oof < 0] = 0\n\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof[valid_id]))\n    print(f\"Fold {num} RMSE: {fold_rmse}\")\n\nsub=pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')\nsub['loss']=prediction","53b5b1c9":"sub.to_csv('submission.csv',index=False)","37f6bbfc":"# Learning Task Parameters \n\n1. **objective [default=reg:squarederror]**\n\nIt defines the loss function to be minimized. Most commonly used values are given below -\n\n* reg:squarederror : regression with squared loss.\n\n* reg:squaredlogerror: regression with squared log loss 1\/2[log(pred+1)\u2212log(label+1)]2. - All input labels are required to be greater than -1.\n\n* reg:logistic : logistic regression\n\n* binary:logistic : logistic regression for binary classification, output probability\n\n* binary:logitraw: logistic regression for binary classification, output score before logistic transformation\n\n* binary:hinge : hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.\n\n* multi:softmax : set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)\n\n* multi:softprob : same as softmax, but output a vector of ndata nclass, which can be further reshaped to ndata nclass matrix. The result contains predicted probability of each data point belonging to each class.\n\n2. **eval_metric [default according to objective]**\n* The metric to be used for validation data.\n* The default values are rmse for regression, error for classification and mean average precision for ranking.\n* We can add multiple evaluation metrics.\n* Python users must pass the metrices as list of parameters pairs instead of map.\n* The most common values are given below -\n\n * rmse : root mean square error\n * mae : mean absolute error\n * logloss : negative log-likelihood\n * error : Binary classification error rate (0.5 threshold). \n * merror : Multiclass classification error rate.\n * mlogloss : Multiclass logloss\n * auc: Area under the curve\n * aucpr : Area under the PR curve","4237de7e":"# Parameters for Tree Booster\n1. **eta [default=0.3, ]**\n* alias: learning_rate\n* Step size shrinkage used in update to prevents overfitting.\n* After each boosting step, we can directly get the weights of new features\n* It makes the model more robust by shrinking the weights on each step.\n* range: [0,1]\n\n2. **gamma [default=0]**\n* Minimum loss reduction required to make a further partition on a leaf node of the tree. \n* The larger gamma is, the more conservative the algorithm will be.\n* range: [0,\u221e]\n\n3. **max_depth [default=6]**\n\n* Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. \n* 0 is only accepted in lossguided growing policy when tree_method is set as hist or gpu_hist and it indicates no limit on depth. \n* Beware that XGBoost aggressively consumes memory when training a deep tree.\n* range: [0,\u221e] )\n\n4. **min_child_weight [default=1]**\n\n* its Minimum sum of instance weight (hessian) needed in a child. I\n* if the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. \n* In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. \n* The larger min_child_weight is, the more conservative the algorithm will be.\n* range: [0,\u221e]\n\n5. **max_delta_step [default=0]**\n\n* Maximum delta step we allow each leaf output to be. \n* If the value is set to 0, it means there is no constraint. \n* If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update.\n* range: [0,\u221e]\n\n6. **subsample [default=1]**\n\n* It denotes the fraction of observations to be randomly samples for each tree.\n* Subsample ratio of the training instances.\n* Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. - This will prevent overfitting.\n* Subsampling will occur once in every boosting iteration.\n* Lower values make the algorithm more conservative and prevents overfitting but too small alues might lead to under-fitting.\n* typical values: 0.5-1\n* range: (0,1]\n\n\n\n7. **sampling_method [default= uniform]**\n\n* The method to use to sample the training instances.\n* **uniform:** each training instance has an equal probability of being selected. Typically set subsample >= 0.5 for good results.\n* **gradient_based:** the selection probability for each training instance is proportional to the regularized absolute value of gradients \n\n8. **colsample_bytree, colsample_bylevel, colsample_bynode [default=1]**\n\n**This is a family of parameters for subsampling of columns.**\n\n**All colsample_by** parameters have a range of (0, 1], the default value of 1, and specify the fraction of columns to be subsampled.\n\n**lsample_bytree**s the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n\n**colsample_bylevel** is the subsample ratio of columns for each level. Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.\n\n**colsample_bynode** is the subsample ratio of columns for each node (split). Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level.\n\n**colsample_by*** parameters work cumulatively. For instance, the combination **{'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5}** with 64 features will leave 8 features to choose from at each split.\n\n9. **lambda [default=1]**\n* alias: reg_lambda\n* L2 regularization term on weights. \n* Increasing this value will make model more conservative.\n\n10. **alpha [default=0]**\n* alias: reg_alpha\n* L1 regularization term on weights.\n* Increasing this value will make model more conservative.\n\n11. **grow_policy [default= depthwise]**\n* Controls a way new nodes are added to the tree.\n* Currently supported only if tree_method is set to hist or gpu_hist.\n* **Choices:** depthwise, lossguide\n* **depthwise:** split at nodes closest to the root.\n* **lossguide:** split at nodes with highest loss change.\n\n12. **max_leaves [default=0]**\n* Maximum number of nodes to be added. \n* Only relevant when grow_policy=lossguide is set.\n\n[for more see this](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html)","d9c32d60":"# XGBoost Parameters\u00b6\n\nBefore running XGBoost, we must set three types of parameters: general parameters, booster parameters and task parameters.\n\n1. **General parameters** relate to which booster we are using to do boosting, commonly tree or linear model\n\n2. **Booster parameter**s depend on which booster you have chosen\n\n3. **Learning task parameters** decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.","c4e27556":"# Relevent General Parameters\u00b6\n\n1. **booster** [default= gbtree ] \n  * Which booster to use. Can be gbtree, gblinear or dart; \n  * gbtree and dart use tree based models while gblinear uses linear functions.\n\n2. **verbosity** [default=1]\n  * Verbosity of printing messages. Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug). \n  * Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message. \n  * If there\u2019s unexpected behaviour, please try to increase value of verbosity.\n\n3. **nthread** [default to maximum number of threads available if not set]\n  * Number of parallel threads used to run XGBoost. When choosing it, please keep thread contention and hyperthreading in mind.\n\n\n","5b90658e":"# A Guide on XGBoost hyperparameters tuning\n\n* XGBoost algorithm provides large range of hyperparameters. We should know how to tune these hyperparameters to improve and take full advantage of the XGBoost model.\n\n* Model training typically starts with parameters being initialized to some values (random values or set to zeros). As training\/learning progresses the initial values are updated using an optimization algorithm (e.g. gradient descent). The learning algorithm is continuously updating the parameter values as learning progress but hyperparameter values set by the model designer remain unchanged.\n\n* In Machine learning you choose and set hyperparameter values that your learning algorithm will use before the training of the model even begins\n\n","8b96b835":"public score 7.87\n\n{'n_estimators':5000,\n          'learning_rate': 0.02,\n          'subsample': 0.5,\n          'colsample_bytree': 0.7,\n          'max_depth': 6,\n          'booster': 'gbtree',\n          'tree_method': 'gpu_hist',\n          'reg_lambda': 60,\n          'reg_alpha': 60,\n           'n_jobs': 4}\n   \n           ","201a9766":"# The available hyperopt optimization algorithms are -\n\n* hp.choice(label, options) \u2014 Returns one of the options, which should be a list or tuple.\n\n* hp.randint(label, upper) \u2014 Returns a random integer between the range [0, upper).\n\n* hp.uniform(label, low, high) \u2014 Returns a value uniformly between low and high.\n\n* hp.quniform(label, low, high, q) \u2014 Returns a value round(uniform(low, high) \/ q) * q,\n\n* hp.normal(label, mean, std) \u2014 Returns a real value that\u2019s normally-distributed with mean and standard deviation sigma."}}