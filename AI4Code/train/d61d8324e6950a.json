{"cell_type":{"aeb6a87e":"code","ad714049":"code","0eae03e9":"code","121b0c94":"code","ca87427c":"code","54e09d2c":"code","7a14956c":"code","da62d91a":"code","0c0d3ebe":"code","b946e33e":"code","3060608a":"code","2e980a72":"code","7ee3b749":"code","78a12d11":"code","fe384bc6":"code","728a0d25":"code","f69d373e":"code","a359eddd":"code","9e12d531":"code","9ecfaae6":"code","ddea2554":"code","a143c0df":"code","8e4e2d5a":"code","34de6911":"code","5dbb81d5":"code","4d3adb3c":"code","efb6c46b":"code","88bd30b1":"code","bfa3e148":"code","3b08b330":"code","4bdfe34c":"code","9f5dc933":"code","259bb67f":"code","6e404363":"code","5dbab6be":"code","8de9835e":"code","36000ef2":"code","0da41283":"code","502be8d7":"code","3bebd36e":"code","a1d48b13":"code","670b26aa":"code","e086b859":"code","602d2cdd":"code","7e225596":"code","8bba38e0":"code","2007849b":"code","4fdddcec":"code","737096be":"code","cff528b2":"code","3e3d8985":"code","58aa8917":"code","e14a6095":"code","d78d6427":"code","140cde4a":"code","ca2ff4db":"code","029020f2":"code","d657f590":"code","4b39a54b":"markdown","a2e2cba7":"markdown","bfdafc4d":"markdown","501bf46b":"markdown","3012357e":"markdown","f4a6af64":"markdown","5049e78e":"markdown","323c85da":"markdown","d4581efd":"markdown","5baefffc":"markdown","a60e5741":"markdown","2e2ba024":"markdown","b0d601d6":"markdown","7a496d62":"markdown","ddd02829":"markdown","cb8a2a44":"markdown","af65337f":"markdown","e692f5b0":"markdown","0ad18403":"markdown","5e3c9b1a":"markdown","ef5aa8d3":"markdown"},"source":{"aeb6a87e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style('whitegrid')\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","ad714049":"df = pd.read_csv('..\/input\/adult-census-income\/adult.csv')","0eae03e9":"df.head()","121b0c94":"df.info()","ca87427c":"df.describe()","54e09d2c":"df.shape","7a14956c":"df.select_dtypes(exclude=np.number).columns","da62d91a":"df.select_dtypes(include=np.number).columns","0c0d3ebe":"# Checking for null values, if any\n\ndf.isnull().sum()","b946e33e":"# Checking for class imbalance\n\ndf['income'].value_counts()","3060608a":"# Converting the same into percentage, for better understanding\n\ndf['income'].value_counts(normalize=True)*100","2e980a72":"plt.figure(figsize=(15,8))\nsns.countplot(df['workclass'], hue=df['income']);","7ee3b749":"plt.figure(figsize=(15,8))\nax = sns.countplot(df['education'], hue=df['income']);\n\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.tight_layout()\nplt.show()","78a12d11":"plt.figure(figsize=(15,8))\nsns.countplot(df['marital.status'], hue=df['income']);","fe384bc6":"plt.figure(figsize=(15,8))\nax = sns.countplot(df['occupation'], hue=df['income']);\n\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.tight_layout()\nplt.show()","728a0d25":"plt.figure(figsize=(15,8))\nsns.countplot(df['relationship'], hue=df['income']);","f69d373e":"plt.figure(figsize=(15,8))\nsns.countplot(df['race'], hue=df['income']);","a359eddd":"sns.countplot(df['sex'], hue=df['income']);","9e12d531":"plt.figure(figsize=(15,8))\nax = sns.countplot(df['native.country']);\n\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.tight_layout()\nplt.show()","9ecfaae6":"plt.figure(figsize=(15,8))\nsns.distplot(df['age']);","ddea2554":"plt.figure(figsize=(15,8))\nsns.distplot(df['fnlwgt']);","a143c0df":"plt.figure(figsize=(15,8))\nsns.distplot(df['education.num']);","8e4e2d5a":"plt.figure(figsize=(15,8))\nsns.distplot(df['capital.gain'], kde=False);","34de6911":"plt.figure(figsize=(15,8))\nsns.distplot(df['capital.loss'], kde=False);","5dbb81d5":"plt.figure(figsize=(15,8))\nsns.distplot(df['hours.per.week']);","4d3adb3c":"# Checking correlation\n\nsns.heatmap(df.corr(), annot=True, cmap='viridis');","efb6c46b":"# Number of '?' in the dataset\n\nfor col in df.columns:\n    print(col,':', df[df[col] == '?'][col].count())","88bd30b1":"for cols in df.select_dtypes(exclude=np.number).columns:\n    df[cols] = df[cols].str.replace('?', 'Unknown')","bfa3e148":"# Unique values in each categorical feature\n\nfor cols in df.select_dtypes(exclude=np.number).columns:\n    print(cols, ':', df[cols].unique(), end='\\n\\n')","3b08b330":"# Checking for correlation between columns 'education' and 'education-num'\n\npd.crosstab(df['education.num'],df['education'])","4bdfe34c":"df['native.country'].value_counts(normalize=True)*100","9f5dc933":"df.drop(['fnlwgt', 'capital.gain', 'capital.loss', 'native.country', 'education'], axis=1, inplace=True)","259bb67f":"# Dropping rows with hours.per.week = 99\n\ndf.drop(df[df['hours.per.week'] == 99].index, inplace=True)","6e404363":"# Converting values in target column to numbers\n\ndf['income'] = df['income'].map({'<=50K':0, '>50K':1})","5dbab6be":"# Encoding categorical features\n\ncategorical_columns = df.select_dtypes(exclude=np.number).columns\nnew_df = pd.get_dummies(data=df, prefix=categorical_columns, drop_first=True)","8de9835e":"new_df.shape","36000ef2":"pd.set_option('max_columns', 50)\nnew_df.head()","0da41283":"X = new_df.drop('income', axis=1)\ny = new_df['income']","502be8d7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","3bebd36e":"# Hyperparameter tuning of Logistic Regression\n\nparam_grid = {'penalty':['l1', 'l2', 'elasticnet'], 'C':[0.001, 0.01, 0.1, 1, 10, 100],\n             'solver':['lbfgs', 'liblinear'], 'l1_ratio':[0.001, 0.01, 0.1]}\n\ngrid = GridSearchCV(LogisticRegression(), param_grid=param_grid, verbose=3)\n\ngrid.fit(X, y)","a1d48b13":"grid.best_params_","670b26aa":"grid.best_score_","e086b859":"log_reg = LogisticRegression(C=1, l1_ratio=0.001, solver='lbfgs', penalty='l2')","602d2cdd":"# Hyperparameter tuning of Random Forest\n\nparam_grid = {'criterion':['gini', 'entropy'], 'max_depth':[2, 4, 5, 7, 9, 10], 'n_estimators':[100, 200, 300, 400, 500]}\n\ngrid = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, verbose=3)\n\ngrid.fit(X, y)","7e225596":"grid.best_params_","8bba38e0":"grid.best_score_","2007849b":"rfc = RandomForestClassifier(max_depth=10, n_estimators=100, criterion='gini')","4fdddcec":"# Hyperparameter tuning of XGBoost\n\nparam_grid = {'max_depth':[2, 4, 5, 7, 9, 10], 'learning_rate':[0.001, 0.01, 0.1, 0.2, 0.3], 'min_child_weight':[2, 4, 5, 6, 7]}\n\ngrid = GridSearchCV(XGBClassifier(), param_grid=param_grid, verbose=3)\n\ngrid.fit(X, y)","737096be":"grid.best_params_","cff528b2":"grid.best_score_","3e3d8985":"xgb = XGBClassifier(learning_rate=0.2, max_depth=4, min_child_weight=2)","58aa8917":"# Hyperparameter tuning of CatBoost\n\nparam_grid = {'depth':[2, 4, 5, 7, 9, 10], 'learning_rate':[0.001, 0.01, 0.1, 0.2, 0.3], 'iterations':[30, 50, 100]}\n\ngrid = GridSearchCV(CatBoostClassifier(), param_grid, verbose=3)\n\ngrid.fit(X, y)","e14a6095":"grid.best_params_","d78d6427":"grid.best_score_","140cde4a":"cb = CatBoostClassifier(iterations=100, depth=10, learning_rate=0.1, verbose=False)","ca2ff4db":"classifiers = [log_reg, rfc, xgb, cb]\n\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=11)\n\nscores_dict = {}\n\nfor train_index, valid_index in folds.split(X_train, y_train):\n    # Need to use iloc as it provides integer-location based indexing, regardless of index values.\n    X_train_fold, X_valid_fold = X.iloc[train_index], X.iloc[valid_index]\n    y_train_fold, y_valid_fold = y.iloc[train_index], y.iloc[valid_index]\n    \n    for classifier in classifiers:\n        name = classifier.__class__.__name__\n        classifier.fit(X_train_fold, y_train_fold)\n        training_predictions = classifier.predict_proba(X_valid_fold)\n        # roc_auc_score should be calculated on probabilities, hence using predict_proba\n        \n        scores = roc_auc_score(y_valid_fold, training_predictions[:, 1])\n        if name in scores_dict:\n            scores_dict[name] += scores\n        else:\n            scores_dict[name] = scores\n\n# Taking average of the scores\nfor classifier in scores_dict:\n    scores_dict[classifier] = scores_dict[classifier]\/folds.n_splits","029020f2":"scores_dict","d657f590":"final_predictions = xgb.predict_proba(X_test)\n\nprint(roc_auc_score(y_test, final_predictions[:, 1]))","4b39a54b":"# 7. Finalising the Model + Prediction","a2e2cba7":"First we will visualize categorical features. Then numeric.","bfdafc4d":"# 5. Feature Selection and Engineering","501bf46b":"# 1. Importing\/ Loading Relevant Libraries","3012357e":"# 4. Visualizations","f4a6af64":"Now, I can directly fit the model on training data, and then make predictions. However, I want to try different approach wherein I choose optimal hyperparameters, and train and predict on folds. Reason behind using this is, model\non each fold will be better and could give a better score when we blend them.\n\nBasically, training models on fold is done for two purposes:\n1. to calculate average models\n2. to train several models, predict with each of them and average their predictions. This makes the result more stable & robust\n\nThe below code is referenced from __[artgor's work](https:\/\/www.kaggle.com\/artgor\/bayesian-optimization-for-robots)__ (shoutout to him)\n","5049e78e":"Target variable : income","323c85da":"Let's drop columns. We will drop - \n1. fnlwgt - seems exactly like ID column, so basically useless\n2. native.country - almost 90% observations are from one country. Seems useless to me\n3. capital.gain - majority of the values are 0\n4. capital.loss - same as above\n5. education - as this can be described by education.num","d4581efd":"# 3. Analyzing Data","5baefffc":"#### Conclusions\n\n1. age, fnlwgt seems right skewed.\n2. education.num is multi-modal, which makes sense.\n3. Majority of the values in capital.gain, and capital.loss are 0.\n4. hours_per_week seems like normally distributed (although multi-modal), with majority of the people working around 40 hours a week.","a60e5741":"#### Conclusions\n\n1. workclass, occupations, and native.country have missing values denoted as '?'\n2. Need to check if education is correlated to numeric feature 'education.num'\n3. Will need to see if native.country has any predictive power","2e2ba024":"Let's have a look at all the categorical columns present in our dataset","b0d601d6":"Clearly from the above scores dictionary, we can see that XGBoost fares better than the rest. Hence we will use the same, and predict our data","7a496d62":"We can clearly see that categorical feature 'education' can perfectly be described numeric feature 'education.num'. Hence, we can drop one column.","ddd02829":"Instead of dropping the rows with seemingly missing values '?', I'll just rename it to 'Unknown', that way, if there is unseen data which the model sees with '?', it can help predict with better accuracy.","cb8a2a44":"# Introduction\n\nIn this Notebook, I am working through the Income Prediction problem associated with the Adult Income Census dataset. The prediction task is to determine whether a person makes over $50K a year.\n\nFollowing steps are used:\n\n1. Load Libraries\n2. Load Data\n3. Very Basic Data Analysis\n4. Very Basic Visualizations\n5. Feature Selection + Engineering\n6. Modeling + Algorithm Tuning (will use Logistic Regression, Random Forest, XGBoost & CatBoost)\n7. Finalizing the Model + Prediction\n\n### Evaluation Metric\nI will be using roc_auc_score for evaluation.\n\nThis is my first Notebook, and hence, I am pretty sure there is a lot of room to improve and add ons. Please feel free to leave me any comments with regards to how I can improve.","af65337f":"Let's have a look at all the numerical columns present in our dataset","e692f5b0":"Majority of the values in native.country seem to be USA. Let's find out percentage","0ad18403":"# 2. Loading Data","5e3c9b1a":"##### Conclusions from basic analysis of data\n\n1. We can clearly see right off the bat that although no null values are present, there are some missing values as '?'\n2. Something doesn't seem right with captial.gain, and capital.loss\n3. fnlwgt seems similar to like an ID column\n4. There is class imbalance. Almost 76% observations are earning less than, or equal to 50K","ef5aa8d3":"# 6. Modelling + Algorithm Tuning"}}