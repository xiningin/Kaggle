{"cell_type":{"3622cd15":"code","1395e41b":"code","3ed6bb86":"code","e58f3682":"code","5ae19765":"code","0f518b12":"code","397db207":"code","e4bd6895":"code","2783086d":"code","1ae214fc":"code","f66bf1ac":"code","cac695f6":"code","8a4650e9":"code","0f2903c2":"code","c31b76cc":"code","7cfa92cf":"code","6d521486":"code","a4035ac9":"code","fa3ec24c":"code","59bf50b9":"code","9dfc6709":"code","95523a4f":"code","ad287d14":"code","79854331":"code","49e848c4":"markdown","bfd985dd":"markdown","d1aaae41":"markdown","7a09a730":"markdown","9c086243":"markdown","9bf1ce2b":"markdown","6f62fbe7":"markdown","b2c1274f":"markdown","073f7338":"markdown","5d2531c3":"markdown","c53d0a81":"markdown","5a17349f":"markdown","34534a18":"markdown","8ec2e144":"markdown","445818a0":"markdown","0dffcb03":"markdown","10c1d0f3":"markdown","b0d0b3bf":"markdown"},"source":{"3622cd15":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pylab import bone,pcolor,colorbar,plot,show\nimport tensorflow as tf","1395e41b":"#you might need to install MiniSom\n!pip install MiniSom","3ed6bb86":"#importing dataset\nds = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\nds.head()","e58f3682":"#checking for null values\nds.isnull().sum()","5ae19765":"#visualizing class vector\nsns.countplot(x = 'Class', data = ds)","0f518b12":"sns.boxplot(x = 'Class', y = 'Time', data = ds)","397db207":"sns.barplot(x = 'Class', y = 'Amount', data = ds)","e4bd6895":"ds['Class'].value_counts()","2783086d":"# Since our classes are highly imbalanced we should make them equivalent in order to have a normal distribution of the classes.\n\n#shuffling data first\nds = ds.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud = ds.loc[ds['Class'] == 1]\nnon_fraud = ds.loc[ds['Class'] == 0][:492]\n\nnew_ds = pd.concat([fraud, non_fraud])\n\n# Shuffle dataframe rows\nnew_ds = new_ds.sample(frac=1, random_state = 0)\n\n#resetting index\nnew_ds.reset_index(drop = True,inplace = True)\nnew_ds.head()","1ae214fc":"sns.countplot(x = 'Class', data = new_ds)","f66bf1ac":"new_ds.describe()","cac695f6":"#checking correlation\nplt.figure(figsize = (10,8))\nsns.heatmap(new_ds.corr(),cmap='coolwarm_r')","8a4650e9":"#creating a new column giving an unique id to customer\nnew_ds.insert(0, 'Customer', new_ds.index + 1)\nnew_ds.head()","0f2903c2":"#splitting our dataset in dependent and independent variables\nx = new_ds.iloc[:, :-1].values \ny = new_ds.iloc[:, -1].values","c31b76cc":"#now normal distribution is required in the dataset so that all values come between (0,1)\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range = (0,1))\nx = sc.fit_transform(x)\n","7cfa92cf":"#Creating MiniSom\nfrom minisom import MiniSom\n\n#map of dimentions 15x15\nsom = MiniSom(x = 15, y = 15, input_len = 31, sigma = 1.0, learning_rate = 0.6)\n\n#initializing some random weights to the nodes\nsom.random_weights_init(x)\n\n#training SOM using 100 iterations\nsom.train_random(data = x, num_iteration = 100)","6d521486":"#plotting SOM\nplt.figure(figsize=(10,8))\nbone()\n\n#distance_map returns matrix of Mean Interneuron Distance(MID)\npcolor(som.distance_map().T,cmap= 'hot')\ncolorbar()\nmarkers = ['o', 's']\ncolors = ['r', 'g']\nfor i, j in enumerate(x):\n    w = som.winner(j)\n    plot(w[0] + 0.5,\n         w[1] + 0.5,\n         markers[y[i]],\n         markeredgecolor = colors[y[i]],\n         markerfacecolor = 'None',\n         markersize = 12,\n         markeredgewidth = 2)\nshow()","a4035ac9":"#now storing winning nodes in mappings\nmappings = som.win_map(x)\nfrauds = np.concatenate((mappings[(1,4)], mappings[(2,1)], mappings[(5,13)], mappings[(9,5)], mappings[(9,6)], mappings[(13,12)]), axis = 0)\nfrauds = sc.inverse_transform(frauds)","fa3ec24c":"#printing IDs of Potential FRAUDS\nprint('Fraud Customer IDs')\nfor i in frauds[:, 0]:\n    print(int(i))","59bf50b9":"#creating dependent variable Except 'Customers' vector because it has no role in training\nX = new_ds.iloc[:, 1:].values","9dfc6709":"#CREATING OUR INDEPENDENT VARIABLE\n\n#array of zeroes is created with length equal to the dataset\nis_fraud = np.zeros(len(new_ds))\n\n#now updating Values of is_fraud to 1 for the IDs which were present in ouput of SOM\nfor i in range(len(new_ds)):\n    if new_ds.iloc[i,0] in frauds:\n        is_fraud[i] = 1","95523a4f":"#applying feature scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)","ad287d14":"#MODEL CREATION\nann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units=2, activation='relu'))\nann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nann.fit(X, is_fraud, batch_size = 32, epochs = 10)","79854331":"#storing predictions\ny_pred = ann.predict(customers)\n\n#concatinating ids with probability of frauds\ny_pred = np.concatenate((new_ds.iloc[:, 0:1].values, y_pred), axis = 1)\n\n#sorting in order\ny_pred = y_pred[y_pred[:, 1].argsort()]\nprint(y_pred)","49e848c4":"**NOTE : AS MOST OF THE DATA IS REMOVED WE HAVE INCREASED THE CHANCES OF OUR MODEL NOT PERFORMING WELL.**","bfd985dd":"# What are Self Organizing Maps\nA self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.\n\nSource: https:\/\/en.wikipedia.org\/wiki\/Self-organizing_map","d1aaae41":"**SOME POINTS ABOUT SOM :**\n1. Weights updation is done using concept of BMU(Best Matching Unit) .\n2. SOM retains the TOPOLOGY of the input set.\n3. It reveals correlations that are hard to find.\n4. It classifies data without supervision.\n5. There is no target vector, This means no back propogation.\n6. Their is no Lateral Connection between output nodes.","7a09a730":"**HERE 0 MEANS NO FRAUD AND 1 MEANS FRAUD**","9c086243":"\n**THIS IS AN EXAMPLE OF IMBALANCED DATASET AND THIS NEEDS TO BE TAKEN CARE OFF**\n\n**SOLUTION :** Creating sub-samples for our dataset(equal frauds and non-frauds)\n\n**WHY TO MAKE SUB-SAMPLES ?**\n* **Overfitting:** Our models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs.\n* **Wrong Correlations:** Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features.\n\n**RANDOM UNDER-SAMPLING:**\n1. Number of frauds would be taken.\n2. Equal amount of RANDOM non frauds would be taken from a shuffled dataset.\n3. Concatenation would be done for non fraud and fraud arrays.\n4. Shuffling would be done again.\n","9bf1ce2b":"**AS YOU CAN SEE ABOVE, FRAUD CASES ARE ALMOST NEGLIGIBLE AS COMPARED TO NON FRAUD**\n","6f62fbe7":"**PERFECTLY BALANCED(AS ALL THINGS SHOULD BE)**","b2c1274f":"**AS WE CAN SEE-**\n* **NON FRAUDS HAVE AN AVERAGE TIME BETWEEN (75000 - 100000)**\n* **FRAUDS HAVE AN AVERAGE OF AROUND 75000**","073f7338":"**NOTE:** There are high correlations among the vectors, Which means there are outliers among those columns but we will not remove outliers bcause outliers means they are frauds and we will detect outliers in the dataset.\n\n**Detecting outliers are same as detecting frauds.**\n\n**OUTLIERS** will be distinguished using MID(Mean Interneuron Distance) which is the distance of a node from its consecutive nodes, higher the value of MID higher are the chances that the node is an outlier and hence is a **FRAUD**","5d2531c3":"**NOTE : In X we have taken the independent variable of new_ds as well because in this case it is not acting as an independent variable, Our independent variable would be the output of SOM (i.e. array frauds), and hence the last column would be included(it may provide some relevant information)**","c53d0a81":"NOTE : The higher the value of MID (higher are the chances of it being an outlier and thus a FRAUD)","5a17349f":"**NOTE : SOM doesn't recognizes all the frauds.**","34534a18":"**NOTE :** In mappings I've take coordinates of TOP 6 MID values, but you can take more by taking THRESHOLD values low.","8ec2e144":"# **SHIFTING TO SUPERVISED LEARNING**","445818a0":"**NO NULL VALUES**","0dffcb03":"**CONCLUSION -**\n* **NON FRAUD CUSTOMERS HAVE LESS AMOUNT VALUES**\n* **FRAUDS HAVE MORE AMOUNT VALUES**","10c1d0f3":"**NOTE : HERE THE OUTPUT IS THE ARRAY OF IDs FROM OUR SUB-SAMPLED DATASET WITH PROBABILITIES OF IT BEING A FRAUD**","b0d0b3bf":"**In this Notebook I have implemented Self Organizing Maps (SOM) using MiniSom for detecting FRAUDS and then used the output SOM(potential Frauds) as an input for ANN.**\n\n**Further Using ANN i have trained my model to get fraud probabilities.** \n\n**NOTE : It is an HYBRID Deep Learning Model - that is shifting from Unsupervised Learning (SOM) to Supervised Learning(ANN)**"}}