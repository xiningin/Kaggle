{"cell_type":{"d1286734":"code","88170cdf":"code","f58f0f45":"code","47dd9aea":"code","6d4e647b":"code","50c0b390":"code","60378a7e":"code","588d2b3b":"code","9e8f6245":"code","f034e422":"code","10b57400":"code","bcb414a2":"code","079d1de9":"code","1950107f":"code","0150493e":"code","6e7766e3":"code","a8db800e":"code","eeec2ed0":"code","b606f36e":"code","ae4d6d59":"code","8d50d3ee":"code","629b5ff8":"code","ed3d77e5":"code","2e0171cb":"code","973128fd":"markdown","13eae988":"markdown","0ef01047":"markdown","ad0f21eb":"markdown"},"source":{"d1286734":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","88170cdf":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n","f58f0f45":"train.head()","47dd9aea":"train.columns","6d4e647b":"test.columns","50c0b390":"#correlation matrix\n\ncorr_matrix=train.corr()\ncorr_matrix","60378a7e":"import seaborn as sn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(rc={'figure.figsize':(30,30)})\n\ncorr_plot=sn.heatmap(corr_matrix, annot=True)\nplt.show(corr_plot)","588d2b3b":"# Pick the features with correlation > 0.55\n# Essentially, we want to pick features with high correlation.\ndata_encoded=pd.get_dummies(train)\nabs_corr=abs(data_encoded.corr()['SalePrice'])\nabs_corr.sort_values(ascending=False)\n","9e8f6245":"attribs_encoded = data_encoded.columns[abs_corr > 0.55]\nattribs_encoded = attribs_encoded[(attribs_encoded != \"SalePrice\")]\nattribs_encoded","f034e422":"#cutting down train set to just what I need\n\nnew=['SalePrice','OverallQual', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'FullBath',\n       'GarageCars', 'GarageArea']\nnew_train=train[new]\nnew_train.head()\n","10b57400":"#This gives us relationship between all of the quantatative variables we want and saleprice. \nimport seaborn as sns\n\nsns.pairplot(new_train)","bcb414a2":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([new_train['SalePrice'], new_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","079d1de9":"print(new_train.isnull().values.any())","1950107f":"import pandas as pd\nnew_train=new_train.dropna()\nnew_train=pd.DataFrame(new_train)\n\nnew_train.head(40)\nnew_train.shape","0150493e":"features = ['OverallQual', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'FullBath', 'GarageCars', 'GarageArea']\nlabel = 'SalePrice'\n\nX_train = new_train[features]\ny_train = new_train[label]\n\nnewtestvars=['Id','OverallQual', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'FullBath', 'GarageCars', 'GarageArea']\n\nnew_test=test[newtestvars]\nnew_test.head()\nnew_test.isna().sum()\nnew_test=new_test.dropna()\nX_test = new_test[features]\n","6e7766e3":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn import linear_model\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score","a8db800e":"#Polynomial Feature\npoly = Pipeline([\n                    (\"poly_features\", PolynomialFeatures(degree=2, include_bias=False)),\n                    (\"std_scaler\", StandardScaler()),\n                    (\"regul_reg\", Ridge(alpha=0.05, solver=\"cholesky\")),\n                ])\npoly.fit(X_train, y_train)\n\n\ny_pred_poly = poly.predict(X_test)\npolynomialfeature= pd.DataFrame({'Id': new_test.Id, 'SalePrice': y_pred_poly})\npolynomialfeature.head()","eeec2ed0":"#Linear Regression\n\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\ny_pred_lin_reg = lin_reg.predict(X_test)\nlinear_regression= pd.DataFrame({'Id': new_test.Id, 'SalePrice': y_pred_lin_reg})","b606f36e":"#Random Forest\nfrom sklearn.ensemble import RandomForestRegressor\n\nrnd_reg = RandomForestRegressor(n_estimators=100, criterion= 'mse',\n                               n_jobs = -1)\nrnd_reg.fit(X_train, y_train)\n\ny_pred_rnd_reg = rnd_reg.predict(X_test)\nrandomforrest_reg= pd.DataFrame({'Id': new_test.Id, 'SalePrice': y_pred_rnd_reg})\nrandomforrest_reg.head()","ae4d6d59":"new_row = {'Id': 2121, 'SalePrice':109321}\nnew_row2 = {'Id': 2577, 'SalePrice':109321}\ndf_marks = randomforrest_reg.append(new_row, ignore_index=True)\ndf_marks = df_marks.append(new_row2, ignore_index=True)","8d50d3ee":"rndforreg=df_marks.to_csv('anyesha_ray_randomforest_submission.csv', index=False)","629b5ff8":"#AdaBoostRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nadb_reg = AdaBoostRegressor(DecisionTreeRegressor(max_depth=3),n_estimators=100, loss='square', random_state=4)\nadb_reg.fit(X_train, y_train)\ny_pred_adb_reg = adb_reg.predict(X_test)\nAdaBoost_reg= pd.DataFrame({'Id': new_test.Id, 'SalePrice': y_pred_adb_reg})\nAdaBoost_reg.head()","ed3d77e5":"#XGB Regression\nfrom xgboost import XGBRegressor\n\n\nxgb_reg = XGBRegressor(n_estimators=100, n_jobs=-1)\nxgb_reg.fit(X_train, y_train)\ny_pred_xgb_reg = xgb_reg.predict(X_test)\nXGB_reg= pd.DataFrame({'Id': new_test.Id, 'SalePrice': y_pred_xgb_reg})\nXGB_reg.head()","2e0171cb":"#poly=polynomialfeature.to_csv('anyesha_ray_polynomialfeature_submission.csv', index=False)\n#lin=linear_regression.to_csv('anyesha_ray_linreg_submission.csv', index=False)\n#rnd=randomforrest_reg.to_csv('anyesha_ray_rnd_submission.csv', index=False)\n#adb=AdaBoost_reg.to_csv('anyesha_ray_adb_submission.csv', index=False)\n#xgb=XGB_reg.to_csv('anyesha_ray_xgb_submission.csv', index=False)","973128fd":"# Modeling","13eae988":"I was not sure how to test accuracy when your test data set does not have the training variable in it. Therefor, I submitted all of them and picked the best one with the lowest score. The best one was the random forest regression. It had a significantly lower score of .18 and the next best one was the XGB with .19.","0ef01047":"# Picking Variables and looking at Relations","ad0f21eb":"However, I was struggling with the columns of 'ExterQual_TA' and 'BsmtQual_Ex' so I decided to remove them for now. "}}