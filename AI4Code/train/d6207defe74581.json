{"cell_type":{"fcc90726":"code","ec9d7550":"code","78688983":"code","2568b51b":"code","696bc160":"code","61348088":"code","39c3b754":"code","e733da0f":"code","6f2bfdb1":"code","6d04b75f":"code","de795bf2":"code","67b8894b":"code","54a44afc":"code","04c3cba6":"code","005f0e91":"code","bc2b2524":"code","cf0040c1":"code","dba5a09e":"code","8f2afb33":"code","042534f8":"code","82021efd":"code","e10c9c09":"code","953c87aa":"code","d2b960d8":"markdown","4f06f773":"markdown","e144be8a":"markdown","1869498d":"markdown","8ff9323d":"markdown","d814e9af":"markdown","cc899402":"markdown","374c0201":"markdown","56e49473":"markdown","aaa60079":"markdown"},"source":{"fcc90726":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec9d7550":"import matplotlib.pyplot as plt\nfrom datetime import datetime\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n%matplotlib inline","78688983":"df = pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/train.csv')\ndf","2568b51b":"df.describe()","696bc160":"df.info()","61348088":"df.duplicated().sum()","39c3b754":"df.isnull().sum()","e733da0f":"sns.histplot(df['target'])","6f2bfdb1":"df.info()","6d04b75f":"X=df.copy()\nY=(df['target']).astype(int)\nX.drop(['target','id'], axis=1,inplace=True)\n\n","de795bf2":"from sklearn.preprocessing import MinMaxScaler\nminmax = MinMaxScaler()\ndf1 = minmax.fit_transform(X)\nX = pd.DataFrame(df1)","67b8894b":"X_train, X_valid, y_train, y_valid = train_test_split(X,Y, train_size=0.8, test_size=0.2,random_state=0)\ny_valid","54a44afc":"from sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import NearestCentroid\nmodels=[RandomForestClassifier(),LogisticRegression(),SGDClassifier(),tree.DecisionTreeClassifier(),NearestCentroid(),SVC()]\nmodel_names=['RandomForestclassifier','LogisticRegression','SGDClassifier','DecisionTreeClassifier','NearestCentroid','SVC']\nroc=[]\nd={}\n\nfor model in range (len(models)):\n    clf=models[model]\n    clf.fit(X_train,y_train)\n    print(\"model_name : \",model_names[model])\n    print(clf.get_params())\n    test_pred=clf.predict(X_valid)\n    #print(test_pred)\n    roc.append(roc_auc_score(y_valid,test_pred))\n    \nd={'Modelling Algo':model_names,'Roc':roc}   \nd","04c3cba6":"roc_frame=pd.DataFrame(d)\nroc_frame","005f0e91":"sns.factorplot(y='Modelling Algo',x='Roc',data=roc_frame,kind='bar',size=5,aspect=2)","bc2b2524":"def scoring_roc_auc(y, y_pred):\n    try:\n        return roc_auc_score(y, y_pred)\n    except:\n        return 0.5\nfrom sklearn.metrics import make_scorer\nroc_auc = make_scorer(scoring_roc_auc)","cf0040c1":"\nparam_grid =  {\n        'C': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1],\n        'tol': [0.00009, 0.0001, 0.00011],\n        'max_iter': [int(x) for x in np.linspace(start = 100, stop = 10000, num = 32)],\n        'penalty': ['l1', 'l2', 'elasticnet'],\n        'solver': ['liblinear','sag']\n    }\n\nbest = GridSearchCV(estimator=LogisticRegression(random_state=42, class_weight='balanced'), param_grid=param_grid, scoring=roc_auc, cv=20, n_jobs=-1,verbose=3)\nbest.fit(X_train, y_train)\nprint(best.best_score_,best.best_estimator_,best.best_params_)\n\n","dba5a09e":"pred=best.predict(X_valid)\nscore=roc_auc_score(y_valid,pred)\nscore","8f2afb33":"test=pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/test.csv')\ntest\n","042534f8":"test2=test['id']\ntest.drop('id',axis=1,inplace=True)\ntest_final = minmax.fit_transform(test)\ntest_final = pd.DataFrame(test_final)\ntest_final","82021efd":"pred=best.predict(test_final)","e10c9c09":"predictions = pd.DataFrame({'id':test2,\n                       'target': pred})","953c87aa":"predictions.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","d2b960d8":"## What is Overfitting\n   It means that the model is memorizing the train data so well that it do really bad on the test data it's like that the model is not generalized enough to understand any unseen data\n## How to know that you overfitted\n   When the train score is really high and the test score is really low \n## How to fix it :\n* Cross validation\n* Early stopping\n* Regulrization\n* Train with more data\n* Remove Features\n* Ensembling\n\nFor more Info check this article [overfitting-in-machine-learning](https:\/\/elitedatascience.com\/overfitting-in-machine-learning)","4f06f773":"# Importing Libraries","e144be8a":"# Loading Data","1869498d":"# Spliting and Scaling The Data","8ff9323d":"# \ud83d\ude04 Generating Submission File","d814e9af":"# Model","cc899402":"# Grid Search","374c0201":"*NO duplicates*","56e49473":"*No Missing values*","aaa60079":"# Testing"}}