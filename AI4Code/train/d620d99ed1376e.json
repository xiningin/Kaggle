{"cell_type":{"d374b675":"code","fab06637":"code","39d563d3":"code","f86c485d":"code","55f6b354":"code","a9c9b612":"code","d46e1776":"code","9a526d58":"code","be03bb6c":"code","ac8cc373":"code","d0631443":"code","fa7bd018":"code","6ab24b9c":"code","812f2bed":"code","e48c4a95":"code","bb5d3dcd":"code","d55bcad6":"code","ac07f43d":"code","f8a1b5d6":"code","13fdcdef":"code","cbec7013":"code","5f5e3e88":"code","a5f9bfa0":"code","95de4ebd":"code","5ebc9984":"code","f9fa5e61":"code","c57744b9":"code","86d3a192":"code","ff4abcfb":"code","4fb2dbe9":"code","5ac5341a":"code","b096f6fa":"code","08318864":"code","323a6f10":"code","e7704630":"code","b7727e49":"code","917ff3a7":"code","e418aeb9":"code","d2fba010":"code","a0370040":"code","a6744ac9":"code","344fb9ce":"code","a529b611":"code","2e2b4601":"code","0d768234":"code","8eb67dd7":"code","3bb90bdb":"code","a866d4b5":"code","2b77d910":"code","39330a77":"code","cae74fc3":"code","5698f8f6":"code","978743db":"code","2a0ffcf2":"code","91b6780a":"code","c0dfdc80":"code","9cb24ed9":"code","4b3dab8a":"code","68b6ba26":"code","14a9b6cb":"code","5de1a829":"code","2997ed72":"code","c6c4422c":"code","84e24702":"code","dbaaf93f":"code","45ec7ad0":"code","9f7158b7":"code","2cb74c0f":"code","29281486":"code","4e7780a0":"code","a18acb85":"code","62ae7253":"code","0ef725d5":"code","ef6fd3e6":"code","43c58ab9":"code","6e9aca5c":"code","06c487a9":"code","290561e0":"code","e6552237":"code","4006a122":"code","e38e4269":"code","fdedd62b":"code","aa2f3ea1":"code","8fc948e7":"code","8d7213ef":"code","1415f5fe":"code","94ffbcf0":"code","9db5a247":"code","6cb5e13b":"code","17d03686":"code","1fa3600e":"code","187e78ee":"markdown","0d44a3a7":"markdown","3326b222":"markdown","4acb0e93":"markdown","287aefb5":"markdown","088dd99a":"markdown","3cb10002":"markdown","46373f41":"markdown","e37809e1":"markdown","c081d4ec":"markdown","5dc3ddd2":"markdown","87431153":"markdown","10f027a7":"markdown","c73434b4":"markdown","debd561b":"markdown","13d8b118":"markdown","8c7b8466":"markdown","00cbd7ed":"markdown","1719c99a":"markdown","f621b46d":"markdown","94f1d651":"markdown","745fffb5":"markdown","f7803a0e":"markdown","44a4e524":"markdown","f150bbdf":"markdown","140a15b5":"markdown","93e7aaef":"markdown","f12e28df":"markdown","0ad1067f":"markdown","dbc76b59":"markdown","e98b15b5":"markdown","661185eb":"markdown","10b164b4":"markdown","4818921a":"markdown","bf666f03":"markdown","b1c550d3":"markdown","df3f3150":"markdown","2d17234b":"markdown","e209ab39":"markdown","4a8cde1c":"markdown","0341e056":"markdown","10674588":"markdown","fe92a45c":"markdown","b873b3ea":"markdown","ae75e910":"markdown","27d44e1e":"markdown","fce8ec8a":"markdown"},"source":{"d374b675":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fab06637":"data_path = '\/kaggle\/input\/SFSalaries.csv'\ndf = pd.read_csv(data_path)\nprint(df.shape)\ndf.head(5)","39d563d3":"df.shape\nprint('Total records:', df.shape[0])\nprint('Total columns:',df.shape[1])","f86c485d":"df.info()","55f6b354":"df = df.drop(columns=['Id'])","a9c9b612":"df['JobTitle'] = df['JobTitle'].map(lambda x: x.lower())","d46e1776":"for column in ['BasePay', 'OvertimePay', 'Benefits', 'OtherPay']:\n    df[column] = pd.to_numeric(df[column], errors='coerce')\n    df[column].fillna(value=np.float64(0))","9a526d58":"def exploring_stats(df_input):\n    total_rows = df_input.shape[0]\n    total_columns = df_input.shape[1]\n    # check data type\n    name = []\n    sub_type = []\n    for n, t in df_input.dtypes.iteritems():\n        name.append(n)\n        sub_type.append(t)\n\n    # check distinct\n    # cname is column name\n    check_ndist = []\n    for cname in df_input.columns:\n        ndist = df_input[cname].nunique()\n        pct_dist = ndist * 100.0 \/ total_rows\n        check_ndist.append(\"{} ({:0.2f}%)\".format(ndist, pct_dist))\n    # check missing\n    check_miss = []\n    for cname in df_input.columns:\n        nmiss = df_input[cname].isnull().sum()\n        pct_miss = nmiss * 100.0 \/ total_rows\n        check_miss.append(\"{} ({:0.2f}%)\".format(nmiss, pct_miss))\n    # check zeros\n    check_zeros = []\n    for cname in df_input.columns:\n        try:\n            nzeros = (df_input[cname] == 0).sum()\n            pct_zeros = nzeros * 100.0 \/ total_rows\n            check_zeros.append(\"{} ({:0.2f}%)\".format(nzeros, pct_zeros))\n        except:\n            check_zeros.append(\"{} ({:0.2f}%)\".format(0, 0))\n            continue\n    # check negative\n    check_negative = []\n    for cname in df_input.columns:\n        try:\n            nneg = (df_input[cname].astype(\"float\") < 0).sum()\n            pct_neg = nneg * 100.0 \/ total_rows\n            check_negative.append(\"{} ({:0.2f}%)\".format(nneg, pct_neg))\n        except:\n            check_negative.append(\"{} ({:0.2f}%)\".format(0, 0))\n            continue\n    data = {\"column_name\": name, \"data_type\": sub_type, \"n_distinct\": check_ndist, \"n_miss\": check_miss, \"n_zeros\": check_zeros,\n            \"n_negative\": check_negative, }\n    # check stats\n    df_stats = df_input.describe().transpose()\n    check_stats = []\n    for stat in df_stats.columns:\n        data[stat] = []\n        for cname in df_input.columns:\n            try:\n                data[stat].append(df_stats.loc[cname, stat])\n            except:\n                data[stat].append(0.0)\n    # col_ordered = [\"name\", \"sub_type\", \"n_distinct\", \"n_miss\", \"n_negative\", \"n_zeros\",\n    #                \"25%\", \"50%\", \"75%\", \"count\", \"max\", \"mean\", \"min\", \"std\"]  # + list(pdf_sample.columns)\n    df_data = pd.DataFrame(data)\n    # df_data = pd.concat([df_data, df_sample], axis=1)\n    # df_data = df_data[col_ordered]\n    return df_data","be03bb6c":"exploring_stats(df)","ac8cc373":"condition = (df['BasePay']<0) | (df['OvertimePay']<0) | (df['TotalPay']<0) |(df['TotalPayBenefits']<0)","d0631443":"df = df[~condition]\ndf.info()","fa7bd018":"df = df[~df.JobTitle.isin(['not provided'])]","6ab24b9c":"def job_type(row):\n  for key, value in job_type_dict.items():\n    for val in value:\n      if val in row.lower():\n        return key\n  return 'Other'\n\ndef job_level(row):\n  for key, value in level_dict.items():\n    for val in value:\n      if val in row.lower():\n        return key\n  return 'Other'","812f2bed":"job_type_dict = dict({\n    'Management' : ['department head', 'manager', 'deputy director', 'human resources', 'employee relations', 'project director','superintendent'],\n    'Fire': ['fire'],\n    'Police': ['police', 'sherif', 'probation', 'sergeant', 'officer', 'lieutenant', 'safetycomm',\n               'public defender', 'incident', 'forensic', 'criminalist'],\n    'Transit': ['mta', 'transit', 'truck', 'transportation', 'railway'],\n    'Medical': ['anesth', 'medical', 'nurs', 'health', 'physician', 'diagnostic imaging tech', 'psychologist',\n             'orthopedic', 'health', 'pharm', 'care', 'dentist', 'therapist'],\n    'Airport': ['airport'],\n    'Animal': ['animal'],\n    'Architecture': ['architect', 'construction'],\n    'Court': ['court', 'legal'],\n    'Automotive': ['mechanic', 'automotive', 'transmission line'],\n    'Engineering': ['engineer', 'engr', 'eng', 'program', 'information technology', 'power generation', 'radiologic technologist',],\n    'General Laborer': ['general laborer', 'painter', 'inspector', 'machinist', 'administrative',\n                        'carpenter', 'electrician', 'plumber', 'maintenance', 'dispatcher',\n                        'custodian', 'garden', 'guard',  'cleaner', 'sewer repair',\n                        'gardener', 'social worker', 'public works', 'parking', 'technician', 'worker'],\n    'Service': ['food serv', 'public service', 'service'],\n    'Admin': ['admin', 'aide', 'assistant', 'secretary', 'elections clerk', 'executive contract employee',\n              'attendant', 'librar', 'public svc aide-public works'],\n    'Data': ['analyst', 'data'],\n    'Accounting': ['accountant', 'account', 'account clerk','treasurer'],\n    'Government': ['mayoral', 'mayor', 'assessor'], \n    'Recreation': ['recreation'], \n    'Lawyer': ['attorney', 'lawyer'],\n    'Education' : ['instructor', 'counselor', 'employment & training'],\n    'Shipping': ['port','porter', 'train controller'],\n    'Sales' : ['senior clerk', 'junior clerk', 'clerk'],\n    'NA':['not provide']\n})","e48c4a95":"level_dict = dict({\n    'Top Level Management' : ['chief' , 'senior management', 'senior deputy', 'director', 'mayor', 'superintendent'],\n    'Middle-Level Management': ['manager',  'deputy', 'department head' , 'commander'],\n    'First-Level Management' : ['leader', 'supervisor', 'sergeant', 'captain', 'administrator', 'head park',\n                                'lieutenant', 'sheriff', 'management assistant'], \n    'Expert' : ['prof', 'specialist', 'counselor', 'attorney', 'pharmacist', 'forensic', 'dentist','criminalist' , 'therapist', 'psychologist', \n                'board secretary', 'employment & training'],\n    'Experienced Level': ['senior', 'special', 'registered', 'anesthetist', 'paramedic', 'investigator', 'marine engineer', 'engineer',\n                          'pr administrative analyst', 'assessor', 'treasurer', 'clerk', 'management assistant', 'architect',\n                          'commissioner',\n                          'stationary eng', 'public defender', 'executive contract employee', 'court executive officer', 'pilot of fire boats',\n                          'instructor','patient care assistant', 'principal', 'account clerk', 'assistant medical examiner'], \n    'Entry Level': ['trainee', 'junior', 'practitioner', 'associate', 'physician assistant', 'library page', 'medical evaluations assistant',\n                    'asst engr', \n                    'nursing assistant', 'licensed vocational', 'nurse midwife', 'administrative'],\n    'Technician' : ['technician', 'operator', 'firefighter', 'inspector', 'diagnostic imaging tech','electrical transit system mech', 'clerk typist',\n                    'library assistant', 'train controller', 'safetycomm',\n                    'police officer', 'firefighter', 'mechanic', 'librarian', 'machinist', 'dispatcher', 'elections clerk', 'tech'],\n    'General' : ['worker', 'guard', 'custodian',  'gardener', 'plumber', 'porter', 'public svc aide-public works',\n                 'parking', 'driver', 'painter', 'cleaner', 'electrician', 'carpenter', 'general laborer'],\n    'NA':['not provide']\n})","bb5d3dcd":"df['JobType'] = df['JobTitle'].map(job_type)\ndf['Level'] = df['JobTitle'].map(job_level)","d55bcad6":"# plt.figure(figsize=(15, 8))\n# ax = sns.boxplot(x=\"Level\", y=\"BasePay\", hue=\"Status\", data=df)   \n# ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n# plt.show()","ac07f43d":"is_null = df.isnull().sum()\nis_null = is_null[is_null>0]\nis_null.sort_values(inplace=True, ascending=False)\n\n#missing data\ntotal = is_null\npercent = is_null\/len(df) * 100\n\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n# plot missing data percent again\nprint(missing_data.index)\nplt.figure(figsize=(8, 8))\nsns.set(style='whitegrid')\ng = sns.barplot(x=missing_data.index, y='Percent', data=missing_data)\nplt.xticks(rotation = 90)\nplt.title(\"missing values percentage.\")\nplt.xticks(rotation=45)\n\n#plot value on top of bar\nfor p in range(len(missing_data)):\n  value = missing_data.iloc[p, 1]\n  g.text(p, value, f'{value:1.2f}%', color='black', ha=\"center\")\n\nplt.show()","f8a1b5d6":"df_drop_columns = df.drop(columns=['Notes', 'Agency'])\ndf_drop_columns=df_drop_columns[~df_drop_columns['JobTitle'].isin(['not provided'])]\ndf_drop_columns['Benefits'] = df_drop_columns.groupby(['JobType', 'Level'])['Benefits'].transform(lambda x: x.fillna(x.median()))\ndf_drop_columns = df_drop_columns.dropna(subset=['Benefits'])\ndf_drop_columns['BasePay'] = df_drop_columns.groupby(['JobType', 'Level'])['BasePay'].transform(lambda x: x.fillna(x.median()))\ndf_drop_columns['OvertimePay'] = df_drop_columns['OvertimePay'].fillna(0)\ndf_drop_columns['Status'] = df_drop_columns['Status'].fillna('NA')","13fdcdef":"df_drop_columns['OtherPay'] = df_drop_columns['OtherPay'].astype(float)\ndf_drop_columns['Benefits'] = df_drop_columns['Benefits'].astype(float)","cbec7013":"df_no_missing = df_drop_columns[(df_drop_columns['OtherPay']>=0) & (df_drop_columns['Benefits']>=0)]","5f5e3e88":"# exploring_stats(df_no_missing)","a5f9bfa0":"# for column in ['OtherPay', 'Benefits']:\n#     df_no_mising.info()[column] = pd.to_numeric(df_no_mising.info()[column], errors='coerce')\n#     df_no_mising.info()[column].fillna(value=np.float64(0))","95de4ebd":"# df_drop_columns['BasePay'] = df_drop_columns['BasePay'].apply(lambda x: np.nan if x <= 0.00 else x)","5ebc9984":"exploring_stats(df_no_missing)","f9fa5e61":"# df_drop_columns['BasePay'] = df_drop_columns.groupby(['JobTitle'])['BasePay'].transform(lambda x: x.fillna(x.median()))","c57744b9":"df_no_missing['TotalPay']  = df_no_missing['BasePay'] + df_no_missing['OvertimePay'] + df_no_missing['OtherPay'] \ndf_no_missing['TotalPayBenefits'] = df_no_missing['TotalPay'] + df_no_missing['Benefits'] ","86d3a192":"exploring_stats(df_no_missing)","ff4abcfb":"# df_no_missing[df_no_missing.BasePay == 0]","4fb2dbe9":"plt.figure(figsize=(15,8))\ndf_no_missing.boxplot(column=['BasePay', 'OvertimePay', 'TotalPay', 'TotalPayBenefits', 'OtherPay', 'Benefits'])\nplt.show()","5ac5341a":"def remove_outlier(df_in, col_name):\n    q1 = df_in[col_name].quantile(0.25)\n    q3 = df_in[col_name].quantile(0.75)\n    iqr = q3-q1 #Interquartile range\n    fence_low  = q1-1.5*iqr\n    fence_high = q3+1.5*iqr\n    df_out = df_in.loc[(df_in[col_name] >= fence_low) & (df_in[col_name] <= fence_high)]\n#     df_out_outliers = df_in.loc[(df_in[col_name] < fence_low) | (df_in[col_name] > fence_high)]\n    return df_out","b096f6fa":"# def outlier(df_in, col_name):\n#     q1 = df_in[col_name].quantile(0.25)\n#     q3 = df_in[col_name].quantile(0.75)\n#     iqr = q3-q1 #Interquartile range\n#     fence_low  = q1-1.5*iqr\n#     fence_high = q3+1.5*iqr\n#     df_out = df_in.loc[(df_in[col_name] < fence_low) | (df_in[col_name] > fence_high)]\n#     return df_out","08318864":"df_out = df_no_missing\nfor col_name in df_no_missing._get_numeric_data().columns:\n  df_out = remove_outlier(df_out, col_name)\n  ","323a6f10":"df_out.shape","e7704630":"plt.figure(figsize=(15,8))\nbp_dict = df_out.boxplot(column=['BasePay', 'OvertimePay', 'TotalPay', 'TotalPayBenefits', 'OtherPay', 'Benefits'], return_type='both',\n    patch_artist = True)\n\nplt.show()","b7727e49":"# plt.figure(figsize=(15,8))\n# df_out.boxplot(column=['BasePay', 'OvertimePay', 'TotalPay', 'TotalPayBenefits', 'OtherPay', 'Benefits'])\n# plt.show()","917ff3a7":"plt.figure(figsize=(5,6))\nsns.boxplot(x='Status', y='BasePay', data=df_out, showfliers=False)\nplt.show()","e418aeb9":"mean_pt = df_out[df_out.Status.isin(['PT'])]['BasePay']\nmean_ft =df_out[df_out.Status.isin(['FT'])]['BasePay']\nf_val, p_val = stats.f_oneway(mean_pt, \n                              mean_ft)  \nprint('PT std is ', mean_pt.std())\nprint('FT std is ', mean_ft.std())\nprint( \"ANOVA results: F=\", f_val, \", P =\", p_val )\n# Approach using p-value\nif p_val > 0.05:\n    print('Failed to reject the null hypothesis')\nelse:\n    print('Reject the null hypothesis (Null hypothesis: the means of all groups are identical )')","d2fba010":"pay_columns = ['BasePay', 'OvertimePay', 'OtherPay', 'TotalPay', 'Benefits', 'TotalPayBenefits']\npays_arrangement = list(zip(*(iter(pay_columns),) * 3))\npays_arrangement","a0370040":"#     2x3 array of axes\nfig, axes = plt.subplots(2,3)\n\n# set the figure height\nfig.set_figheight(6)\nfig.set_figwidth(18)\n\nfor i in range(len(pays_arrangement)):\n    for j in range(len(pays_arrangement[i])):\n        # pass in axes to pandas hist\n        df_out[pays_arrangement[i][j]].hist(ax=axes[i,j], color = 'g')\n        axes[i,j].set_title('Histogram of '+ pays_arrangement[i][j])\n#         axes[i,j].set_xlim(0, df_no_missing[pays_arrangement[i][j]].max())\n        \n# add a row of emptiness between the two rows\nplt.subplots_adjust(hspace=1)\n# add a row of emptiness between the cols\nplt.subplots_adjust(wspace=1)\nplt.show()","a6744ac9":"t, pvalue = stats.ttest_rel(df_out['TotalPay'], df_out['BasePay'])\nprint(\"ttest is \", t, \", and pvalue is \", pvalue)\n# Approach using p-value\nif pvalue > 0.05:\n    print('Failed to reject the null hypothesis')\nelse:\n    print('Reject the null hypothesis')","344fb9ce":"import seaborn as sns\nplt.figure(figsize=(10,7))\nsns.distplot(df_out['TotalPay'], rug=True, hist=False, label='TotalPay')\nsns.distplot(df_out['BasePay'], rug=True, hist=False, label='BasePay')","a529b611":"t, pvalue = stats.shapiro(df_no_missing['TotalPay'])\nprint(pvalue)\n# Approach using p-value\nif pvalue > 0.05:\n    print('Failed to reject the null hypothesis')\nelse:\n    print('Reject the null hypothesis (null hypothesis is normally distributed)')","2e2b4601":"import seaborn as sns\ntrain_df = df_out\nplt.figure(figsize=(16,8))\nsns.countplot('JobType', data = train_df, hue = 'Year')\nplt.xticks(rotation = 30)\nplt.xlabel('Job Type')\nplt.ylabel('Number of Labors')\nplt.title('Number of labors for each job type of each year')\nplt.tight_layout()","0d768234":"import seaborn as sns\nplt.figure(figsize=(15,7))\nsns.countplot('Level', data = train_df, hue = 'Year')\nplt.xticks(rotation = 30)\nplt.xlabel('Level')\nplt.ylabel('Number of Labors')\nplt.title('Labours Distribution of each years by level')\nplt.tight_layout()","8eb67dd7":"df_data1 = train_df[~train_df.Status.isin(['NA'])][['JobType', 'Status', 'TotalPay']]\ndf_data = df_data1.pivot_table(values='TotalPay', index='JobType', columns='Status', aggfunc='count').fillna(0)\ndf_data\nstatus_percents = df_data.div(df_data.sum(1), axis=0)\nplt.figure(figsize=(20,8))\n# and plot the bar graph with a stacked argument.  \nax = status_percents.plot(kind='barh', stacked=True, rot=0, figsize=(5,8), color ='gy')\nplt.ylabel('Job Type')\nplt.xlabel('Percentage')\nplt.xlim(0,1)\nplt.tight_layout()\nplt.show()","3bb90bdb":"plt.figure(figsize=(12,6))\nsns.barplot(data= train_df.groupby('JobType')['TotalPay'].agg('mean').reset_index(), x ='JobType', y = 'TotalPay')\nplt.xticks(rotation=90)\nplt.title('Average TotalPay in each Job type')\nplt.tight_layout()","a866d4b5":"grouped_jobtype_anavo=train_df[['JobType', 'TotalPay']].groupby(['JobType'])\nf_val, p_val = stats.f_oneway(grouped_jobtype_anavo.get_group('Lawyer')['TotalPay'], \n                              grouped_jobtype_anavo.get_group('Data')['TotalPay'], \n                              grouped_jobtype_anavo.get_group('Architecture')['TotalPay'])  \nprint('Lawyer std is ', grouped_jobtype_anavo.get_group('Lawyer')['TotalPay'].std())\nprint('Data std is ', grouped_jobtype_anavo.get_group('Data')['TotalPay'].std())\nprint('Architecture std is ', grouped_jobtype_anavo.get_group('Architecture')['TotalPay'].std())\nprint( \"ANOVA results: F=\", f_val, \", P =\", p_val )\n# Approach using p-value\nif p_val > 0.05:\n    print('Failed to reject the null hypothesis')\nelse:\n    print('Reject the null hypothesis (the means of all groups are identical )')","2b77d910":"plt.figure(figsize=(15,6))\nsns.barplot(data= train_df.groupby('Level')['TotalPay'].agg('mean').reset_index(), x = 'TotalPay',  y ='Level', order=['General','Other','Technician','Entry Level','Experienced Level','Expert','First-Level Management','Middle-Level Management','Top Level Management'])\nplt.xticks(rotation=0)\nplt.title('Average TotalPay in each Level')\nplt.tight_layout()","39330a77":"df_data2 = train_df[~train_df.Status.isin(['NA'])][['Level', 'Status', 'TotalPay']]\ndf_dataa = df_data2.pivot_table(values='TotalPay', index='Level', columns='Status', aggfunc='count').fillna(0)\nstatus_percents = df_dataa.div(df_dataa.sum(1), axis=0)\nplt.figure(figsize=(20,8))\n# and plot the bar graph with a stacked argument.  \nax = status_percents.plot(kind='barh', stacked=True, rot=0, figsize=(5,5), color ='gy')\nplt.ylabel('Level')\nplt.xlabel('Percentage')\nplt.xlim(0,1)\nplt.tight_layout()\nplt.show()","cae74fc3":"plt.figure(figsize=(15,6))\nax = sns.boxplot(y=\"Level\", x=\"BasePay\", data=train_df, order=['General','Other','Technician','Entry Level','Experienced Level','Expert','First-Level Management','Middle-Level Management','Top Level Management'])\nplt.show()","5698f8f6":"plt.figure(figsize=(15,8))\nax = sns.boxplot(y=\"Level\", x=\"TotalPay\", data=train_df, order=['General','Other','Technician','Entry Level','Experienced Level','Expert','First-Level Management','Middle-Level Management','Top Level Management'])\nplt.show()","978743db":"top_16_occupations = train_df.JobType.value_counts().sort_values(ascending=False).head(16).index\nsalaries_averages_by_occupation = (train_df[train_df.JobType.isin(top_16_occupations)].\n                                   groupby('JobType')[['BasePay', 'Benefits', 'OvertimePay', 'OtherPay']].\n                                   aggregate('mean'))\n\nax = salaries_averages_by_occupation.plot(kind='bar', figsize=(20,8))\nax.set_title('Average salaries by job type (top 16 job type popular)')\nax.set_xlabel('Mean Compensation ')","2a0ffcf2":"salary_percents = salaries_averages_by_occupation.div(salaries_averages_by_occupation.sum(1), axis=0)\nplt.figure(figsize=(20,8))\n# and plot the bar graph with a stacked argument.  \nax = salary_percents.plot(kind='bar', stacked=True, rot=90, figsize=(15,8))\nplt.xlabel('Job Type')\nplt.ylabel('Percentage')\nplt.tight_layout()\nplt.show()","91b6780a":"level = train_df.Level.value_counts().sort_values(ascending=False).head(13).index\nsalaries_averages_by_level = (train_df[train_df.Level.isin(level)]\n                                   .groupby('Level')[['BasePay', 'Benefits', 'OvertimePay', 'OtherPay']]\n                                   .aggregate('mean')\n)\n\nax = salaries_averages_by_level.plot(kind='bar', figsize=(10,8))\nax.set_title('Average salaries by level')\nax.set_ylabel('Value')\nax.set_xlabel('Level')","c0dfdc80":"salary_percents_by_level = salaries_averages_by_level.div(salaries_averages_by_level.sum(1), axis=0)\nplt.figure(figsize=(25,15))\n# and plot the bar graph with a stacked argument.  \nax = salary_percents_by_level.plot(kind='bar', stacked=True, rot=90, figsize=(10,8), color='gyrb')\nplt.xlabel('Level')\nplt.ylabel('Percentage')\nplt.tight_layout()\nplt.show()","9cb24ed9":"corr_matrix = df_out._get_numeric_data().sample(frac=0.1).corr()\nf, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(corr_matrix, vmax=1, annot=True, square=True);\nplt.show()","4b3dab8a":"# plt.figure(figsize=(6,6))\n# plt.scatter(df_no_missing['TotalPayBenefits'], df_no_missing['TotalPay'])\n# plt.xlabel(\"TotalPayBenefits\")\n# plt.ylabel('TotalPay')\n# plt.title(\"Relationship between TotalPayBenefits and TotalPay\")\n# plt.show()","68b6ba26":"# # Create correlation matrix\n# corr_matrix = corr_matrix.abs()\n# # Select upper triangle of correlation matrix\n# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# # Find index of feature columns with correlation greater than 0.95\n# to_drop_corr = [column for column in upper.columns if any(upper[column] > 0.9)]","14a9b6cb":"# train_df = df_out.drop(columns=to_drop_corr)","5de1a829":"# train_df.columns","2997ed72":"train_df.columns","c6c4422c":"train_df['LastNameLength'] = (train_df.EmployeeName.str.split().apply(lambda a: len(a[-1])))","84e24702":"fig = plt.figure()\nax = fig.add_subplot(1,1,1)\ntrain_df.LastNameLength.hist(bins=20, ax=ax, alpha=0.3)\nplt.title('Histograme of last name length')","dbaaf93f":"\nt, pvalue = stats.shapiro(train_df['LastNameLength'])\nprint(pvalue)\n# Approach using p-value\nif pvalue > 0.05:\n    print('Failed to reject the null hypothesis')\nelse:\n    print('Reject the null hypothesis')","45ec7ad0":"plt.figure(figsize=(6, 6))\nplt.scatter(train_df['LastNameLength'], train_df['TotalPay'], color='g')\nplt.xlim(0,)\nplt.ylim(0,)\nplt.show()","9f7158b7":"pearson_coef, p_value = stats.pearsonr(train_df['LastNameLength'], train_df['TotalPay'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a p-value of p =\", p_value)  ","2cb74c0f":"plt.figure(figsize=(8, 8))\nsns.scatterplot(train_df['TotalPay'], train_df['Benefits'], color ='g')\nplt.xlabel('TotalPay')\nplt.ylabel('Benefits')\nplt.title('Relationship between TotalPay and Benefits')\nplt.show()","29281486":"# train_df['bnf_bp_ration'] = train_df['Benefits']\/train_df['BasePay']\n# train_df[train_df.JobTitle =='senior deputy sheriff']","4e7780a0":"from sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\ndata = train_df.drop(columns='Year')._get_numeric_data()\nx = data['TotalPay']\ny= data['Benefits']\n\n# create new plot and data\nplt.plot()\ncolors = ['b', 'g', 'r']\nmarkers = ['o', 'v', 's']\n\n# k means determine k\ndistortions = []\nK = range(1,10)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k).fit(data)\n    kmeanModel.fit(data)\n    distortions.append(sum(np.min(cdist(data, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) \/ data.shape[0])\n\n# Plot the elbow\nplt.plot(K, distortions, 'o--')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('Elbow Method for optimal k')\nplt.show()","a18acb85":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\n# scaler = MinMaxScaler()\n# normData = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(data)\ncentroids = np.round(kmeans.cluster_centers_,2)\nclustering = kmeans.predict(data)\nprint(centroids)","62ae7253":"# from sklearn.cluster import KMeans\n# from sklearn.preprocessing import MinMaxScaler\n# scaler = MinMaxScaler()\n# normData = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n# kmeans = KMeans(n_clusters=4)\n# kmeans.fit(normData)\n# centroids = np.round(kmeans.cluster_centers_,2)\n# clustering = kmeans.predict(normData)\n# print(centroids)","0ef725d5":"output = []\nfor x in clustering:\n    if x not in output:\n        output.append(x)\nprint(output)","ef6fd3e6":"# normData.head()","43c58ab9":"train_df['cluster'] = pd.Series(clustering, index=data.index)","6e9aca5c":"train_df.head()","06c487a9":"train_df.columns","290561e0":"print(centroids)","e6552237":"pd.crosstab(train_df.JobType, train_df.cluster)","4006a122":"level_df = pd.crosstab(train_df.Level, train_df.cluster)\nlevel_df","e38e4269":"from sklearn.cluster import KMeans\nX=data.values[1:,:]\nkm = KMeans(\n    n_clusters=3, init='random',\n    n_init=10, max_iter=300, \n    tol=1e-04, random_state=0\n)\ny_km = km.fit_predict(X)","fdedd62b":"y_km","aa2f3ea1":"X","8fc948e7":"#X=data\n# plot the 3 clusters\nplt.scatter(\n    X[y_km == 0, 0], X[y_km == 0, 1],\n    s=50, c='lightgreen',\n    marker='s', edgecolor='black',\n    label='cluster 1'\n)\n\nplt.scatter(\n    X[y_km == 1, 0], X[y_km == 1, 1],\n    s=50, c='orange',\n    marker='o', edgecolor='black',\n    label='cluster 2'\n)\n\nplt.scatter(\n    X[y_km == 2, 0], X[y_km == 2, 1],\n    s=50, c='lightblue',\n    marker='v', edgecolor='black',\n    label='cluster 3'\n)\n\n# plot the centroids\nplt.scatter(\n    km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n    s=250, marker='*',\n    c='red', edgecolor='black',\n    label='centroids'\n)\nplt.legend(scatterpoints=1)\nplt.xlabel('TotalPay')\nplt.ylabel('Benefits')\nplt.grid()\nplt.show()","8d7213ef":"# # Feature Importance\n# from sklearn import datasets\n# from sklearn import metrics\n# from sklearn.ensemble import RandomForestRegressor\n\n# # fit an Extra Trees model to the data\n# model = RandomForestRegressor()\n# model.fit(train_df.drop(columns=['TotalPay']), train_df.TotalPay)\n# feature_importances = pd.DataFrame(model.feature_importances_,\n#                                    index = train_df.drop(columns=['TotalPay']).columns,\n#                                    columns=['importance']).sort_values('importance', ascending=False)","1415f5fe":"# ft = list(feature_importances[:20].index)\n# print(ft)","94ffbcf0":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_df[['LastNameLength']], train_df.TotalPay, test_size=0.2, random_state=42)\n","9db5a247":"from sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score","6cb5e13b":"# Create linear regression object\nregr = linear_model.LinearRegression()\n# Train the model using the training sets\nregr.fit(X_train, y_train)","17d03686":"y_pred = regr.predict(X_test)","1fa3600e":"print('R-squared = ', r2_score(y_test, y_pred))\nprint('MSE       = ', mean_squared_error(y_test, y_pred))","187e78ee":" # III.Univariate analysis","0d44a3a7":"Zero BasePay handling","3326b222":"Salary structure by Level and JobTitle","4acb0e93":"# VI. Building machine learning models\n## 1. The relationship between lastname length and salary","287aefb5":"Number of labors as Expert or top level management ","088dd99a":"Deeply look on dataset","3cb10002":"The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution.","46373f41":"<p> ANAVO test for comparing average paid in each jobtype","e37809e1":"Correct TotalPay and TotalPayBenefits after handling missing","c081d4ec":"## 2. The benefits\/basepay ratio and salary","5dc3ddd2":"# Linear regression -- scikit-learn","87431153":" ## 2. Outliers\n ### a. Outliers","10f027a7":"## 1. Construct data set description\n\n","c73434b4":"## 3. JobTitle processing","debd561b":"*** atfer handling missing value, now can convert feature OtherPay and Benefits into float (correct data type), then drop negative values**","13d8b118":"Test for mean of each Status (just for PT and FT)","8c7b8466":" *** Remove negative rows (mis-typo)**","00cbd7ed":"### b. Handling Outliers","1719c99a":"Here we see that the distribution of BasePay between 2 status categories, fulltime and parttime, are distinct enough to take status as a potential good predictor of price. The BasePay of FT is much higher than PT. But this feature was missing a lot","f621b46d":" # IV.Bivariate analysis\n ## 1. Correlation\n <h4>Continuous numerical variables:<\/h4> ","94f1d651":"Next part is detecting and handling with outliers","745fffb5":"### 2.2. Kmeans by Level","f7803a0e":"** Correct data types, salary should be float data type => correct. However, otherpay and benefits contain string => not yet converted => converted data type later","44a4e524":"p-value is  >  0.05 and < 0.1: there is weak evidence that the correlation is significant, And the linear relationship is extremely weak, seems as no relationship","f150bbdf":"# Training\/Test splitting","140a15b5":"# II.Data Cleaning\n ## 1. Missing value\n  ### a. Detect missing value\n\n\n","93e7aaef":"### b. Handling missing value\n<p> 1. Fill missing value for feature BasePay and Benefits by group mean based on JobType and Level\n <p> 2. Fill missing value with 0 for OvertimePay (no benefits means no money) and NA for status as not decalred\n <p> 3. Notes contains almost nan value ==> drop those columns\n<p> 4. Agency only has 1 value \"San Francisco\" ==> drop too\n\n","f12e28df":"Lawyer has a quite high paid and benefits while transit and police work overtime more than other groups","0ad1067f":"this dataset has 148k rows and 13 columns, but the column id has no meaning => drop it by drop function","dbc76b59":"TotalPay was not draw from a normal distribution","e98b15b5":"Define function name exploring_stats: check distinct values in each column, check missing, check zero, negative, and some stats","661185eb":"## 3. Kmeans model","10b164b4":"for detecting outlier, i use box plot to visualize data and remove outliers as well, there are too much outliers. As you may know, the distribution of salaries are right skewed, a lot of data points beyond Q3 and no data points under Q1","4818921a":"3 groups: lawyer, data and architecture have average basepay been different","bf666f03":"As a first look, the number of labors in General laborers and admin grew up through 4 years while other \njob type were not significant growth in 4 years","b1c550d3":"Lawyer had a high paid and much distincted with other groups, following was \narchitecture\n","df3f3150":"job title contains 1637 unique values => too much to investigate => consider categorizing jobtitle into jobtype and level. Then found if there is any relationship between jobtype, level and salary ","2d17234b":"Firstly, we need to read data file by function pd.read_csv in python","e209ab39":"# I.Data Importing","4a8cde1c":"Just a quick look, education, medical, police, service and transit work overtime, especially transit","0341e056":"there are 23 jobtypes basis and 9 level","10674588":"just keep all rows have salary been not negative","fe92a45c":"## 2. Construct Attribute description","b873b3ea":"Technician gets more overtimepay than other levels","ae75e910":"income almost comes from BasePay and Benefits, hence 2 variables should be choosen and investigate are TotalPay and Benefits","27d44e1e":"There are 5 columns, containing missing values: basepay, overtime, benefits, notes and status\n<p> salary but contains negative => consider drop them => maybe mis-typo\n<p> salary but contains zero a lot","fce8ec8a":"Note: The distributions of income are right skewed"}}