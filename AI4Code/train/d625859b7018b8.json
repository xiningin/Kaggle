{"cell_type":{"475fe567":"code","83d5662e":"code","0306e1d2":"code","ee32181e":"code","68965854":"code","9f8a816a":"code","d76d9cd3":"code","b1c8f650":"code","cd78a192":"code","2a503199":"code","58f35211":"code","129b1507":"code","d1931845":"code","9ba2b794":"code","b9f2b1b1":"code","47fc7897":"code","5285ba7a":"code","1969db58":"code","52574f0a":"code","9645898c":"code","b022ef88":"code","3b1f8246":"code","5ae2ef31":"code","1433530d":"code","93b2d656":"code","12c4149e":"code","3e72abf2":"code","d6030654":"code","9864ab35":"code","d18bb79f":"code","21e5ac59":"code","5d8afb17":"code","62a0c92d":"code","149935fa":"code","1655031e":"code","f0ae8e5c":"code","ce942a33":"code","b3d67ca6":"code","28fe0f07":"code","4e2dbe57":"code","f127b19c":"code","5426ab3d":"code","9fa50790":"code","0c4ea663":"markdown","aab00d40":"markdown","49cae308":"markdown","f4eeb3aa":"markdown","bdcd50e3":"markdown","6fb9ae5c":"markdown","f95000a5":"markdown","52a18998":"markdown","6a81f31a":"markdown","f524566f":"markdown","7c96ac67":"markdown","20e38433":"markdown","d28e2383":"markdown","35e39fc7":"markdown","f6c7e367":"markdown","76423773":"markdown","6759dc48":"markdown","5ca21d30":"markdown","abe037a0":"markdown","61e499dd":"markdown","a9134885":"markdown","877a2e79":"markdown","736ee230":"markdown","7cd2b930":"markdown","9fae2145":"markdown"},"source":{"475fe567":"import scipy\nimport numpy as np\n\nfrom matplotlib import pyplot\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","83d5662e":"# cook data\nnp.random.seed(42)\n\nx1 = np.random.randn(100)\ny = 2 * x1 + 10  # one unit change in x1 changes 2 units of y in same direction\n\nx1 = x1 + 0.25 * np.random.randn(100)\nx2 = -x1 # one unit change in x2 changes 2 units of y but in opposite direction","0306e1d2":"# plot cooked data\nfig = pyplot.figure(1, (16,4))\n\nax1 = pyplot.subplot(1,3,1)\nax1.scatter(x1, y)\nax1.set_title('x1 v\/s y')\n\nax2 = pyplot.subplot(1,3,2)\nax2.scatter(x2, y)\nax2.set_title('x2 v\/s y')\n\nax3 = pyplot.subplot(1,3,3)\nax3.scatter(x1, x2)\nax3.set_title('x1 v\/s x2')\n\npyplot.tight_layout()\npyplot.show()","ee32181e":"# quantitative look at correlations\nprint(scipy.stats.pearsonr(x1, y))\nprint(scipy.stats.pearsonr(x2, y))\nprint(scipy.stats.pearsonr(x1, x2)) # -1 value means perfect correlation which we expect","68965854":"X = np.hstack((x1.reshape(-1,1),x2.reshape(-1,1)))\nX.shape","9f8a816a":"lr = LinearRegression().fit(X[:,:1], y) # fit x1 only\nlr.coef_, lr.intercept_","d76d9cd3":"lr = LinearRegression().fit(X[:,1:], y) # fit x2 only\nlr.coef_, lr.intercept_","b1c8f650":"lr = LinearRegression().fit(X, y) # fit both x1 and x2\nlr.coef_, lr.intercept_","cd78a192":"# cook data\nnp.random.seed(42)\nx1 = np.random.randn(100)\ny1 = 2 * x1 + 4\n\nnp.random.seed(142)\nx2 = np.random.randn(100)\ny2 = 2 * x2 + 4\n\ny = y1 + y2","2a503199":"# plot cooked data\nfig = pyplot.figure(1, (16,4))\n\nax1 = pyplot.subplot(1,3,1)\nax1.scatter(x1, y)\nax1.set_title('x1 v\/s y')\n\nax2 = pyplot.subplot(1,3,2)\nax2.scatter(x2, y)\nax2.set_title('x2 v\/s y')\n\nax3 = pyplot.subplot(1,3,3)\nax3.scatter(x1, x2)\nax3.set_title('x1 v\/s x2')\n\npyplot.tight_layout()\npyplot.show()","58f35211":"# quantitative look at correlations\nprint(scipy.stats.pearsonr(x1, y))\nprint(scipy.stats.pearsonr(x2, y))\nprint(scipy.stats.pearsonr(x1, x2))","129b1507":"X = np.hstack((x1.reshape(-1,1),x2.reshape(-1,1)))\nX.shape","d1931845":"lr = LinearRegression().fit(X[:,:1], y) # fit x1 only\nlr.coef_, lr.intercept_","9ba2b794":"lr = LinearRegression().fit(X[:,1:], y) # fit x2 only\nlr.coef_, lr.intercept_","b9f2b1b1":"lr = LinearRegression().fit(X, y) # fit both x1 and x2\nlr.coef_, lr.intercept_","47fc7897":"# cook data\nnp.random.seed(42)\nx1 = np.random.randn(100)\ny1 = 2 * x1 + 4 # one unit change in x1 changes 2 units of y in same direction\n\nnp.random.seed(142)\nx2 = np.random.randn(100)\ny2 = 2 * x2 + 4 # one unit change in x2 changes 2 units of y in same direction\n\nx3 = -x1 # one unit change in x3 changes 2 units of y in opposite direction\n\ny = y1 + y2","5285ba7a":"# plot cooked data\nfig = pyplot.figure(1, (18,8))\n\nax = pyplot.subplot(2,3,1)\npyplot.scatter(x1, y)\nax.set_title('x1 v\/s y')\n\nax = pyplot.subplot(2,3,2)\npyplot.scatter(x2, y)\nax.set_title('x2 v\/s y')\n\nax = pyplot.subplot(2,3,3)\npyplot.scatter(x3, y)\nax.set_title('x3 v\/s y')\n\nax = pyplot.subplot(2,3,4)\npyplot.scatter(x1, x2)\nax.set_title('x1 v\/s x2')\n\nax = pyplot.subplot(2,3,5)\npyplot.scatter(x1, x3)\nax.set_title('x1 v\/s x3')\n\nax = pyplot.subplot(2,3,6)\npyplot.scatter(x2, x3)\nax.set_title('x2 v\/s x3')\n\npyplot.tight_layout()\npyplot.show()","1969db58":"# quantitative look at correlations\nprint(scipy.stats.pearsonr(x1, y))\nprint(scipy.stats.pearsonr(x2, y))\nprint(scipy.stats.pearsonr(x3, y))\nprint(scipy.stats.pearsonr(x1, x2))\nprint(scipy.stats.pearsonr(x1, x3))\nprint(scipy.stats.pearsonr(x2, x3))","52574f0a":"X = np.hstack((x1.reshape(-1,1), x2.reshape(-1,1), x3.reshape(-1,1)))\nX.shape","9645898c":"lr = LinearRegression().fit(X[:,:1], y) # fit x1 only\nlr.coef_, lr.intercept_","b022ef88":"lr = LinearRegression().fit(X[:,1:2], y) # fit x2 only\nlr.coef_, lr.intercept_","3b1f8246":"lr = LinearRegression().fit(X[:,2:], y) # fit x3 only\nlr.coef_, lr.intercept_","5ae2ef31":"lr = LinearRegression().fit(X[:,:2], y) # fit x1 & x2\nlr.coef_, lr.intercept_","1433530d":"lr = LinearRegression().fit(X[:,[0,2]], y) # fit x1 & x3\nlr.coef_, lr.intercept_","93b2d656":"lr = LinearRegression().fit(X[:,1:], y) # fit x2 & x3\nlr.coef_, lr.intercept_","12c4149e":"lr = LinearRegression().fit(X, y) # fit all x1,x2,x3\nlr.coef_, lr.intercept_","3e72abf2":"# we cook the same experiment_1 data\nnp.random.seed(42)\n\nx1 = np.random.randn(100)\ny = 2 * x1 + 10  # one unit change in x1 changes 2 units of y in same direction\n\nx1 = x1 + 0.25 * np.random.randn(100)\nx2 = -x1 # one unit change in x2 changes 2 units of y in opposite direction\n\nX = np.hstack((x1.reshape(-1,1),x2.reshape(-1,1)))\nX.shape","d6030654":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=42)\nX_train.shape, X_test.shape","9864ab35":"lr = LinearRegression().fit(X_train[:, :1], y_train) # fit x1 only\n\nprint('train score: ', lr.score(X_train[:, :1], y_train))\nprint('test score: ', lr.score(X_test[:, :1], y_test))\nprint('estimated params: ', *lr.coef_, lr.intercept_)","d18bb79f":"lr = LinearRegression().fit(X_train, y_train) # fit both x1 & x2\n\nprint('train score: ', lr.score(X_train, y_train))\nprint('test score: ', lr.score(X_test, y_test))\nprint('estimated params: ', *lr.coef_, lr.intercept_)","21e5ac59":"# we cook the same exp3 data\nnp.random.seed(42)\nx1 = np.random.randn(100)\ny1 = 2 * x1 + 4 # one unit change in x1 changes 2 units of y in same direction\n\nnp.random.seed(142)\nx2 = np.random.randn(100)\ny2 = 2 * x2 + 4 # one unit change in x2 changes 2 units of y in same direction\n\nx3 = -x1 # one unit change in x3 changes 2 units of y in opposite direction\n\ny = y1 + y2\n\nX = np.hstack((x1.reshape(-1,1), x2.reshape(-1,1), x3.reshape(-1,1)))\nX.shape","5d8afb17":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=42)\nX_train.shape, X_test.shape","62a0c92d":"lr = LinearRegression().fit(X_train[:, :2], y_train) # fit x1 & x2 only\n\nprint('train score: ', lr.score(X_train[:, :2], y_train))\nprint('test score: ', lr.score(X_test[:, :2], y_test))\nprint('estimated params: ', *lr.coef_, lr.intercept_)","149935fa":"lr = LinearRegression().fit(X_train, y_train) # fit all x1,x2 & x3\n\nprint('train score: ', lr.score(X_train, y_train))\nprint('test score: ', lr.score(X_test, y_test))\nprint('estimated params: ', *lr.coef_, lr.intercept_)","1655031e":"def get_vif(X_design: np.ndarray) -> list:\n    vif = []\n    for i in range(X_design.shape[1]-1):\n        vif.append(variance_inflation_factor(X_design, i+1))\n\n    return vif","f0ae8e5c":"# we cook the same exp3 data\nnp.random.seed(42)\nx1 = np.random.randn(100)\ny1 = 2 * x1 + 4 # one unit change in x1 changes 2 units of y in same direction\n\nnp.random.seed(142)\nx2 = np.random.randn(100)\ny2 = 2 * x2 + 4 # one unit change in x2 changes 2 units of y in same direction\n\nx3 = -x1 # one unit change in x3 changes 2 units of y in opposite direction\n\ny = y1 + y2\n\nX = np.hstack((x1.reshape(-1,1), x2.reshape(-1,1), x3.reshape(-1,1)))\nX.shape","ce942a33":"X_design = sm.add_constant(X[:,:2]) # the input of ols reuires a design matrix\nols = sm.OLS(y, X_design).fit()\nprint(ols.summary())","b3d67ca6":"print(f'\\nVIF of predictors: {get_vif(X_design)}')","28fe0f07":"X_design = sm.add_constant(X)\nols = sm.OLS(y, X_design).fit()\nprint(ols.summary())","4e2dbe57":"print(f'\\nVIF of predictors: {get_vif(X_design)}')","f127b19c":"ols = sm.regression.linear_model.OLS(y, X_design).fit_regularized(method='elastic_net', L1_wt=1.) # lasso\nols.params","5426ab3d":"ols = sm.regression.linear_model.OLS(y, X_design).fit_regularized(method='elastic_net', L1_wt=0.) # ridge\nols.params","9fa50790":"ols = sm.regression.linear_model.OLS(y, X_design).fit_regularized(method='elastic_net', L1_wt=.5) # elastic-net\nols.params","0c4ea663":"    Again as expected no effect.","aab00d40":"This is all for now. Do comment if you find anything I missed or miss-interpreted, I would love to know. Thankyou for reading this notebook :)\n\n<span style=\"color:red\">**If you like this notebooks then please upvote and also share with others.**<span>\n    \nAlso, have a read of my other notebooks which includes variety of topics including [statistical analysis in python](https:\/\/www.kaggle.com\/gauravsharma99\/statistical-analysis-on-mpg-data), [facial emotion recognition](https:\/\/www.kaggle.com\/gauravsharma99\/facial-emotion-recognition), and [many more](https:\/\/www.kaggle.com\/gauravsharma99\/notebooks).","49cae308":"### References\n\n* [identifying-important-independent-variables](https:\/\/statisticsbyjim.com\/regression\/identifying-important-independent-variables\/)\n\n* [classical assumptions for linear regression](https:\/\/statisticsbyjim.com\/regression\/ols-linear-regression-assumptions\/)","f4eeb3aa":"### Experiment 4\nEffect of correlated attributes on model's prediction.","bdcd50e3":"We can see that due to very less multicollinearity both the estimated coefficients are not affected a lot and even the estimates are improved after taking both x1 & x2 because y is y1+y2 hence depending on both x1 & x2, therefore using both the predictors which impact y we get better and more accurate estimates.","6fb9ae5c":"So multicollinearity has no significant effect on the model's performance and it's kind of make sense because individual estimates are effecting each other only. You can think of this like if **two correlated variables are just two entangled variables**(where strength of entanglement is proportional to the strength of collinearity b\/w them) such that if 1 changes other also changes making the end result same.\n\nLet's try this one more time with data used in exp3.","f95000a5":"Before starting we should clear some basics.\n\n* The regression coefficient value associated with an attribute represents the **mean change of the dependent variable given a one-unit shift in an independent variable**.\n\n* We cannot compare two regression coeff. because they can be on different scale, for eg. coeff of x1 is 2 and coeff of x2 is 4, we cannot directly say that x2 is more important than x1 because they can be on different scales. Like x1 is distance in km and x2 is weight in grams.\n\n* We can compare two standardized regression coeff.\n\nThroughout the notebook I will directly compare regression coeff. because they are on the same scale as I cooked the data artificially.\n\n**Note:** Whenever I say **estimate of x**, I mean **estimate of coeff. of x**.","52a18998":"So all the estimated parameters are same as the true parameters. Also VIF of both x1 & x2 is almost 1 indicating they are not correlated with each other hence **every estimates and statistics calculated by OLS are correct and trustable in this case.**","6a81f31a":"### Experiment 2\nEffect of no\/less correlated attributes on their respective coefficients.","f524566f":"### Experiment 1\nEffect of highly correlated attributes on their respective coefficients.","7c96ac67":"### Conclusions\n\nFrom these experiments we conclude following results\n* If model has coupled\/collinear features than their estimated coefficients are effected and are not trustable. Also the effects depends on degree\/severity of collinearity. Less collinearity have negligible effect and high collinearity have huge effect. (**proved from exp1 & exp2**)\n* In a model the estimated coefficients are effected only for those features which are involved in collinearity with other. (**proved from exp3**)\n* Multicollinearity doesn't affect the predictive power of the model. (**proved from exp4**)\n\n### Long Story Short\nIf model's predictive performance is your only goal then no need to worry about multicollinearity but interpretation of fitted model is your primary goal or as important as performance then you should not rely\/make your decisions on the bases of fitted model if their is presence of reasonable multicollinearity because in that estimated parameters are untrustable and same with p-values.","20e38433":"### Some techniques to deal with multicollinearity\n\n* Stepwise removal of predictors having large VIF i.e., regress-remove-regress cycle until all VIF's are in satisfying range.\n* Both ridge regression and lasso regression are addressed to deal with multicollinearity or a mixture of these i.e., elastic-net regression.\n* Apply PCR(principal component regessor) but this will result in lossing model interpretation as principal components are hard to interpret and if that's the case then you won't apply it because if interpretation is not the need then you can apply LR as the predictive power is not affected much due to multicollinearity.","d28e2383":"So all the estimated parameters are not same as the true parameters. x1 & x3 have wrong estimates due to perfect correlation but x2 has correct estimate due to no correlation. Also VIF of both x1 & x3 is $\\infty $ indicating that they are perfectly correlated with each other hence **their estimates and statistics calculated by OLS are incorrect and can't be trusted.** But x2 has no impact and we can trust all it's estimates and statistics.\n\n    Also note the warning [2], OLS automatically find such an high multicollinearity in the design matrix.","35e39fc7":"**Although the estimated parameters are quite reasonable but there are some errors in them and that's not just due to the noise in data but also due to the fact that y depends on both x1 and x2 (as y=y1+y2)**","f6c7e367":"The low pearson coeff b\/w x1 & x2 suggests that there is very less correlation between them, and can even be rejected under significance level of 5%.\n\n    So due to less multicollinearity estimated coeff. are more reliable now.","76423773":"#### Interpretation of results\n\nThese estimated coeff. means one unit change in x1 changes $\\approx 1$ units of y in same direction and one unit change in x2 changes $\\approx 1$ units of y in opposite direction. Which we know is not true as we did different settings while cooking the data.\n\nThe reason for this is because the two features are highly correlated(perfect correlation in this case) their contribution is divided equally in changing the response. The coeff. of x1 no more interpreted as the change in y on 1 unit change in x1 but it's now interpreted as **change in y on 1 unit change in x1 given 1 unit change of x2**, i.e., it is **conditioned** now. This is because if x1 & x2 would have 0 correlation between them then we can keep x2 constant and change x1 by one unit and detect the change in y but now because of such a high correlation b\/w them we can't keep x2 constant on changing x1, if we change x1 a bit x2 also changes depending on it's correlation with x1 (if 0 correlation than no change and for $\\pm 1$ compelete change).\n\nSo long story short, if now someone looks at these coeff. then they interpret the results as one unit change in x1 changes $\\approx 1$ units of y in same direction and one unit change in x2 changes $\\approx 1$ units of y in opposite direction. Such bad estimates are very dangerous because our interpretation of model results in critical decision making and if results are wrong then interpretation is wrong and hence decisions are wrong. This may have critical effects in medical field as these two features could be some components of drugs and we may be interested in it's effect on some disease(i.e., y in this case).\n\n    So when 2 or more highly correlated features are feeded to the linear regression their estimates are very bad and can't be trusted. Hence, multicollinearity results in less reliable estimates and this reliability is inversely proportional with level multicollinearity i.e., more multicollinearity means less reliable coeff.","6759dc48":"So, if you followed the notebook then this result is not unexpected for you.\n\nBecause x1 & x3 are perfectly correlated hence their coeff. estimates are affected a lot and in this case get exactly halfed fue to -1 corr. But you may have noticed that coeff. of x2 is not effected at all and even get improved. This is due to the fact that x2 has very low correlation with both x1 & x3, hence multicollinearity between x1 & x3 has no effect on it's estimates. And also the estimates are improved due to the fact that y depends on both x1 & x2 and we provided both.\n\n**Ground Truth**<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- one unit change in x1 changes 1 unit of y in same direction<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- one unit change in x2 changes 2 units of y in same direction<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- one unit change in x3 changes 1 unit of y in opposite direction\n\n**Interpretation from results**<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- one unit change in x1 changes 1 unit of y in same direction (which is absolutely False)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- one unit change in x2 changes 2 units of y in same direction (which is absolutely True)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- one unit change in x3 changes 1 unit of y in opposite direction(which is absolutely False)\n\nAlthough we see that multicollinearity affects the estimated parameters but **what about prediction?** because in some rare cases we may not be that much interested in interpreting the results as compared to getting high prediction accuracy (though such cases are very rare, but the idea is we are happy if we get good overall predictions even though individual estimates are wrong).","5ca21d30":"So basically x1 & x3 are perfectly correlated but (x1,x2) & (x2,x3) pairs have very less correlation","abe037a0":"So the next question that arise to you is that are we needed to find this multicollinearity manually and if so then what in case of large number of predictors. Isn't there a tool for that? And the answer is we are not required to do it manually. Most of the tools use **Variance Inflation Factor(VIF)** for finding the strength of multicollinearity among predictors.\n\n### Variance Inflation Factor\nVIF detects multicollinearity in regression analysis for every predictor by taking that predictor, and regressing it against every other predictor in the model giving a R-squared value and then substituting it in the VIF formula.\n\n$$\\large VIF = \\frac{1}{1 - R_{i}^{2}}$$\n\nwhere, $R_{i}^{2}$ is the R-squared value of $i^{th}$ predictor.\n\nThe value of VIF tells what percentage variance (i.e. the standard error squared) is inflated for each coefficient. For example, a VIF of 1.9 tells that the variance of a particular coefficient is 90% bigger than what you would expect if there was no multicollinearity \u2014 if there was no correlation with other predictors.\n\nInterpretation of VIF:<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1 is not correlated.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1-5 is moderately correlated.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;>5 is highly correlated.\n\n<sup>[source](https:\/\/www.statisticshowto.datasciencecentral.com\/variance-inflation-factor\/)<sup>","61e499dd":"This notebook is divided into following experiments -\n\n**Experiment 1 -** Effect of highly correlated attributes on their respective coefficients<br>\n**Experiment 2 -** Effect of no\/less correlated attributes on their respective coefficients<br>\n**Experiment 3 -** Effect of data with both high\/low correlated attributes on their respective coefficients<br>\n**Experiment 4 -** Effect of correlated attributes on model's prediction<br>","a9134885":"### Experiment 3\nEffect of data with both high\/low correlated attributes on their respective coefficients.","877a2e79":"#### Interpretation of results\n**These estimated coeff. means one unit change in x1 changes on average $\\approx 2$ units of y in same direction and one unit change in x2 changes on average $\\approx 2$ units of y in opposite direction.** Which we know is true as we did these settings while cooking the data. So, when these 2 highly correlated features are feeded independently to the linear regression their estimates are very close and good.\n\n**The slight errors in the estimated parameters is due to the noise in data as well, if we have clean data with no noise then the estimated parameters converges to true parameters.**","736ee230":"We can see from the plots that both x1 & x2 are +vely correlated with y but has very less correlation with each other and this is also verified by below pearson's correlation","7cd2b930":"The estimates of coeff. of x1, x2, x3 are pretty close to true parameters but bit noisy and the reason is already explained above. And also **due to high multicollinearity estimates get highly effected when y is regressed on x1 & x3 combined.** But the estimates are improved when y is regressed on both x2 & x3 due to the fact that y is composed of both (x1,x2) and x3 is nothing but -ve x2, hence regressor get everything that is needed to estimate true behaviour.","9fae2145":"    We see that this regularized regressors moves one correlated variable close to zero resulting in better estimate of the other variable. This is because in this case one of the x1 & x2 are redundant i.e., only one is needed hence it's good to make one of them close to or equal to zero resulting in better estimates of the other one."}}