{"cell_type":{"f00df677":"code","df8eed71":"code","6828bbbe":"code","9630f57a":"code","de34ed73":"code","4ed2d133":"code","46c42848":"code","2f416869":"code","2e9131da":"code","3321ad00":"code","6748273e":"code","093a56a6":"code","bf392b8c":"code","c8fcfef1":"code","1ddbbab3":"code","0131f619":"code","d49ba5a1":"code","b93df2ec":"code","078f4a6e":"markdown","7b037752":"markdown","757862b1":"markdown"},"source":{"f00df677":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","df8eed71":"from keras.models import Sequential, load_model\nfrom keras.layers import LSTM, GRU\nfrom keras.layers import Dense, Embedding, Bidirectional, Dropout, Flatten\nfrom keras.optimizers import Adam, SGD\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences","6828bbbe":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","9630f57a":"train.head()","de34ed73":"print('Training Dataset contain {} samples'.format(train.shape[0]))\nprint('Testing Dataset contain {} samples'.format(test.shape[0]))","4ed2d133":"train = train.drop(['id', 'keyword', 'location'], axis=1)\ntest = test.drop(['id', 'keyword', 'location'], axis=1)","46c42848":"y_train =  train['target'].values\nX_train = train.drop(['target'], axis=1).values.reshape(len(train),)\nX_test = test['text'].values.reshape(len(test),)","2f416869":"total_tweets = np.concatenate((X_train, X_test))\nprint('Total tweets : ', len(total_tweets))","2e9131da":"\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(total_tweets)\n\n# Vocbvulary Size\nvocab_size = len(tokenizer.word_index) + 1\nprint('Size of Vocabulary : ', vocab_size)","3321ad00":"# Maximum length for padding sequence\nmaxlen = max(len(x.split()) for x in total_tweets)\nprint('Maximum length of tweet : ', maxlen)","6748273e":"X_train_token = tokenizer.texts_to_sequences(X_train)\nX_test_token = tokenizer.texts_to_sequences(X_test)\n\nprint('Text before tokenized')\nprint(X_train[0])\nprint('\\nText after tokenized')\nprint(X_train_token[0])","093a56a6":"X_train_pad = pad_sequences(X_train_token, maxlen=maxlen, padding='post')\nX_test_pad = pad_sequences(X_test_token, maxlen=maxlen, padding='post')\n\nprint('Tokenized text before padding')\nprint(X_train_token[0])\nprint('\\nTokenized text after padding')\nprint(X_train_pad[0])","bf392b8c":"hidden_units = 128\nembed_units = 100\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embed_units, input_length = maxlen))\nmodel.add(Bidirectional(LSTM(hidden_units)))\nmodel.add(Dropout(0.2))\n#model.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","c8fcfef1":"learning_rate = 0.0001\n\nmodel.compile(loss = 'binary_crossentropy',\n              optimizer = 'adam',\n              metrics = ['accuracy'])","1ddbbab3":"batch_size = 512\nnum_itr = 5\n\nmodel_history = model.fit(X_train_pad, y_train, \n                          batch_size=batch_size, \n                          epochs=num_itr, \n                          validation_split=0.2)","0131f619":"pred = model.predict(X_test_pad)","d49ba5a1":"sub = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsub[\"target\"] = pred\nsub[\"target\"] = sub[\"target\"].apply(lambda x : 0 if x<=.5 else 1)","b93df2ec":"sub.to_csv(\"submit_1.csv\", index=False)","078f4a6e":"**Preprocessing**","7b037752":"**Dataset Load**","757862b1":"We will drop 'id', 'keyword' and 'location columns as we are going to train an RNN only on text data."}}