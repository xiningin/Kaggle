{"cell_type":{"6e0da07d":"code","dfac257b":"code","e03d831d":"code","462336cf":"code","204e1160":"code","7e26eb2e":"code","fa06ef7e":"code","fee988dd":"code","1ebc79dc":"code","8544e4c4":"code","2805666c":"code","e63eebc6":"code","0b676220":"code","935f4593":"code","23f349e2":"code","e7c42094":"code","168c6897":"code","f7b75ae5":"code","1730dc57":"code","9ded0978":"code","a0c10349":"code","2d9e7bd7":"code","da410238":"code","a5a703f3":"code","ebb9d06d":"code","146fa8bb":"code","604fbc16":"code","bb21b85c":"code","7253ade1":"code","fe678a4c":"code","76c1828b":"code","fc90eeaa":"code","3bd7090b":"code","4b1b63e9":"code","b7c6206a":"code","12a8bf0d":"code","00f5ecbc":"code","7b6bc819":"code","b8a2daf0":"code","2855f032":"code","74a24fa9":"code","4c4abc6a":"code","9039bc01":"code","bbb6b51d":"code","9d082ee1":"code","8c8719d6":"code","d51583c7":"code","376f477d":"code","9f6823b0":"code","85283dd9":"code","21b1509a":"code","dfa33436":"code","ee75e343":"code","62fa71d0":"code","268f38b3":"code","9491c379":"code","a672e114":"code","6aead910":"code","a36c2d80":"code","557980ef":"markdown","4eba6eb8":"markdown"},"source":{"6e0da07d":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 15,15\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV ,RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier , ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_auc_score\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","dfac257b":"train_data = pd.read_csv('\/kaggle\/input\/Train\/Train\/Train.csv')\npatient_profile = pd.read_csv('\/kaggle\/input\/Train\/Train\/Patient_Profile.csv')\nhealth_camp_detail = pd.read_csv('\/kaggle\/input\/Train\/Train\/Health_Camp_Detail.csv')\nfirst_health_camp = pd.read_csv('\/kaggle\/input\/Train\/Train\/First_Health_Camp_Attended.csv')\nsecond_health_camp = pd.read_csv('\/kaggle\/input\/Train\/Train\/Second_Health_Camp_Attended.csv')\nthird_health_camp = pd.read_csv('\/kaggle\/input\/Train\/Train\/Third_Health_Camp_Attended.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/test.csv')","e03d831d":"print(train_data.shape)\ntrain_data.head()","462336cf":"print(test_data.shape)\ntest_data.head()","204e1160":"health_camp_detail.tail()","7e26eb2e":"first_health_camp.head()","fa06ef7e":"first_health_camp.drop(columns=['Unnamed: 4'],inplace=True)\nsns.distplot(first_health_camp['Health_Score'])","fee988dd":"second_health_camp.head()","1ebc79dc":"sns.distplot(second_health_camp['Health Score'])","8544e4c4":"third_health_camp.head()","2805666c":"visits = third_health_camp.Number_of_stall_visited.value_counts()\nplt.xlabel(\"No.of visits\")\nplt.ylabel('Count')\nsns.barplot(visits.index , visits.values)","e63eebc6":"def nullColumns(train_data):\n    list_of_nullcolumns =[]\n    for column in train_data.columns:\n        total= train_data[column].isna().sum()\n        try:\n            if total !=0:\n                print('Total Na values is {0} for column {1}' .format(total, column))\n                list_of_nullcolumns.append(column)\n        except:\n            print(column,\"-----\",total)\n    print('\\n')\n    return list_of_nullcolumns\n\n\ndef percentMissingFeature(data):\n    data_na = (data.isnull().sum() \/ len(data)) * 100\n    data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)[:30]\n    missing_data = pd.DataFrame({'Missing Ratio' :data_na})\n    print(missing_data.head(20))\n    return data_na\n\n\ndef plotMissingFeature(data_na):\n    f, ax = plt.subplots(figsize=(15, 12))\n    plt.xticks(rotation='90')\n    if(data_na.empty ==False):\n        sns.barplot(x=data_na.index, y=data_na)\n        plt.xlabel('Features', fontsize=15)\n        plt.ylabel('Percent of missing values', fontsize=15)\n        plt.title('Percent missing data by feature', fontsize=15)","0b676220":"print('train data')\nprint(nullColumns(train_data))\nprint('\\n')\nprint('test_data')\nprint(nullColumns(test_data))\nprint('\\n')\nprint('first_health_camp')\nprint(nullColumns(first_health_camp))\nprint('\\n')\nprint('second_health_camp')\nprint(nullColumns(second_health_camp))\nprint('\\n')\nprint('third_health_camp')\nprint(nullColumns(third_health_camp))\nprint('\\n')\nprint('patient_profile')\nprint(nullColumns(patient_profile))\nprint('\\n')\nprint('health_camp_detail')\nprint(nullColumns(health_camp_detail))\nprint('\\n')","935f4593":"combined_data = pd.concat([train_data,test_data],axis = 0)\ncombined_data = combined_data.reset_index(drop =  True)\ncombined_data.head()","23f349e2":"Number_Of_Prior_Registration = []\ncombined_data['Number_Of_Prior_Registration'] = 0\nfor idx, row in combined_data.iterrows():\n    patient_id = row['Patient_ID']\n    Number_Of_Prior_Registration.append(combined_data.iloc[:idx,:].loc[(combined_data['Patient_ID']==patient_id),:].shape[0])","e7c42094":"combined_data['Number_Of_Prior_Registration'] = Number_Of_Prior_Registration","168c6897":"#first_occurence = []\n#for val in combined_data['Patient_ID'].unique():\n#    first_occurence.append(combined_data[combined_data.Patient_ID == val].first_valid_index())","f7b75ae5":"#combined_data['Old_Patient_ID'] = 1\n#combined_data.loc[first_occurence,'Old_Patient_ID'] = 0","1730dc57":"combined_data = pd.merge(combined_data,health_camp_detail,on='Health_Camp_ID',how= 'left')","9ded0978":"first_health_camp['F_O'] = 1\ncombined_data = pd.merge(combined_data,first_health_camp[['Patient_ID','Health_Camp_ID','F_O']],on=['Patient_ID','Health_Camp_ID'],how = 'left')","a0c10349":"second_health_camp['S_O'] = 1\ncombined_data = pd.merge(combined_data,second_health_camp[['Patient_ID','Health_Camp_ID','S_O']],on=['Patient_ID','Health_Camp_ID'],how = 'left')","2d9e7bd7":"third_health_camp['T_O'] = third_health_camp['Number_of_stall_visited'].apply(lambda x : 1 if x>0 else 0)\ncombined_data = pd.merge(combined_data,third_health_camp[['Patient_ID','Health_Camp_ID','T_O']],on=['Patient_ID','Health_Camp_ID'],how = 'left')","da410238":"combined_data = pd.merge(combined_data,patient_profile,on=['Patient_ID'],how = 'left')","a5a703f3":"combined_data['F_O'] = combined_data['F_O'].fillna(0)\ncombined_data['S_O'] = combined_data['S_O'].fillna(0)\ncombined_data['T_O'] = combined_data['T_O'].fillna(0)\ncombined_data['Outcome'] = combined_data['F_O'] +combined_data['S_O'] +combined_data['T_O']\ncombined_data['Outcome'] =combined_data['Outcome'].astype('int64')\n#combined_data['Health_Camp_Type'] = np.where(combined_data['Health_Camp_ID'].isin(first_health_camp.Health_Camp_ID),1\n#                                             ,np.where(combined_data['Health_Camp_ID'].isin(second_health_camp.Health_Camp_ID),2\n #                                                     ,np.where(combined_data['Health_Camp_ID'].isin(third_health_camp.Health_Camp_ID),3,0)))","ebb9d06d":"outcome = combined_data.loc[:75278,\"Outcome\"].value_counts()\nprint(outcome)\nplt.xlabel(\"outcome\")\nplt.ylabel('count')\nsns.barplot(outcome.index , outcome.values)","146fa8bb":"nullColumns(combined_data)","604fbc16":"combined_data['Registration_Date'] = combined_data.apply(\n    lambda row: row['Camp_Start_Date'] if row['Registration_Date'] != row['Registration_Date'] else row['Registration_Date'],\n    axis=1\n)\n\ncombined_data['City_Type'] = combined_data['City_Type'].fillna(combined_data['City_Type'].mode()[0])\n\ncombined_data['Employer_Category'] = combined_data['Employer_Category'].fillna(combined_data['Employer_Category'].mode()[0])\n\ncombined_data['Income'] = combined_data['Income'].apply(lambda x : '0' if x == 'None' else x)\n\ncombined_data['Education_Score'] = combined_data['Education_Score'].apply(lambda x : '0' if x == 'None' else x)\n\nages = []\n\ncombined_data['Age'].apply(lambda x : ages.append(float(x)) if x!= 'None' else 0).mean()\n\nage_mean = sum(ages)\/len(ages)\n\ncombined_data['Age'] = combined_data['Age'].apply(lambda x : age_mean if x == 'None' else float(x))\n\ncombined_data['Age'] = combined_data['Age'].astype('int64')","bb21b85c":"def generate_date_features(calendar,colname,prefix):\n    \n    df = pd.DataFrame()\n    \n    df[prefix+'_Year'] = pd.to_datetime(calendar[colname]).dt.year\n\n    df[prefix+'_Month'] = pd.to_datetime(calendar[colname]).dt.month\n\n    df[prefix+'_Day'] = pd.to_datetime(calendar[colname]).dt.day\n\n    df[prefix+'_Dayofweek'] = pd.to_datetime(calendar[colname]).dt.dayofweek\n\n    df[prefix+'_DayOfyear'] = pd.to_datetime(calendar[colname]).dt.dayofyear\n\n    df[prefix+'_Week'] = pd.to_datetime(calendar[colname]).dt.week\n\n    #df['Quarter'] = pd.to_datetime(calendar[colname]).dt.quarter \n\n    #df['Is_month_start'] = pd.to_datetime(calendar[colname]).dt.is_month_start\n\n    #df['Is_month_end'] = pd.to_datetime(calendar[colname]).dt.is_month_end\n\n    #df['Is_quarter_start'] = pd.to_datetime(calendar[colname]).dt.is_quarter_start\n\n    #df['Is_quarter_end'] = pd.to_datetime(calendar[colname]).dt.is_quarter_end\n\n    #df['Is_year_start'] = pd.to_datetime(calendar[colname]).dt.is_year_start\n\n    #df['Is_year_end'] = pd.to_datetime(calendar[colname]).dt.is_year_end\n\n    #df['Semester'] = np.where(df['Quarter'].isin([1,2]),1,2)\n\n    #df['Is_weekend'] = np.where(df['Dayofweek'].isin([5,6]),1,0)\n\n    #df['Is_weekday'] = np.where(df['Dayofweek'].isin([0,1,2,3,4]),1,0)\n\n    return df","7253ade1":"date_features1 = generate_date_features(combined_data,'Camp_End_Date','Camp_End')\ndate_features2 = generate_date_features(combined_data,'Camp_Start_Date','Camp_Start')\n#date_features3 = generate_date_features(combined_data,'Registration_Date','Registration')","fe678a4c":"combined_data = pd.concat([date_features1,\n                           date_features2,\n                           #date_features3,\n                           combined_data],axis =1)","76c1828b":"combined_data['Camp_duration'] = (pd.to_datetime(combined_data['Camp_End_Date'])-pd.to_datetime(combined_data['Camp_Start_Date'])).dt.days\n#combined_data['Approach'] = (pd.to_datetime(combined_data['First_Interaction'])-pd.to_datetime(combined_data['Camp_Start_Date'])).dt.days\ncombined_data['Delay'] = (pd.to_datetime(combined_data['Camp_End_Date'])-pd.to_datetime(combined_data['First_Interaction'])).dt.days\ncombined_data['Eagernes'] = (pd.to_datetime(combined_data['Camp_Start_Date'])-pd.to_datetime(combined_data['Registration_Date'])).dt.days","fc90eeaa":"le = LabelEncoder()\nfor col in combined_data.columns:\n    #print(col)\n    if combined_data[col].dtype == 'O':\n        combined_data[col] = le.fit_transform(combined_data[col])\n        ","3bd7090b":"combined_data['Enthusiasm'] = combined_data.apply(lambda x : 1 if x.LinkedIn_Shared == 1 or x.Facebook_Shared == 1 or x.Twitter_Shared == 1 else 0,axis =1)","4b1b63e9":"#combined_data['Social_Media'] = combined_data['LinkedIn_Shared'] +combined_data['Facebook_Shared'] + combined_data['Twitter_Shared']\n#combined_data = combined_data.drop(columns= ['LinkedIn_Shared','Facebook_Shared','Twitter_Shared'])","b7c6206a":"#combined_data['Income_Age1'] = combined_data['Income']\/combined_data['Age']\n#combined_data['Income_Age2'] = combined_data['Income']*combined_data['Age']","12a8bf0d":"#combined_data['Education_Age1'] = combined_data['Education_Score']\/ combined_data['Age']\n#combined_data['Education_Age2'] = combined_data['Education_Score'] * combined_data['Age']","00f5ecbc":"#combined_data['Education_Income1'] = combined_data['Education_Score']\/ combined_data['Income']\n#combined_data['Education_Income1'] = combined_data['Education_Income1'].fillna(0)\n#combined_data['Education_Income2'] = combined_data['Education_Score'] * combined_data['Income']","7b6bc819":"#combined_data['Category1_squared'] =combined_data['Category1']*combined_data['Category1']","b8a2daf0":"combined_data = combined_data.drop(columns= ['LinkedIn_Shared'\n                                             ,'Facebook_Shared'\n                                             ,'Twitter_Shared'\n                                            #,'Online_Follower'\n                                            ])\ncombined_data = combined_data.drop(columns = ['Patient_ID'\n                                              ,'Health_Camp_ID'\n                                              ,'Var3'\n                                              ,'Var4'\n                                              ,'F_O'\n                                              ,'S_O'\n                                              ,'T_O'\n                                              ,'Employer_Category'\n                                             #,'Category3'\n                                             ])\n\ncombined_data = combined_data.drop(columns= ['First_Interaction'\n                                             ,'Camp_End_Date'\n                                             ,'Camp_Start_Date'\n                                             ,'Registration_Date'\n                                             #,'Income'\n                                             #,'Education_Score'\n                                             #,'Age'\n                                            ])","2855f032":"target = combined_data['Outcome']\ncombined_data = combined_data.drop(columns = ['Outcome'])","74a24fa9":"def create_submission_file(model_list):\n    preds = 0\n    submission = pd.read_csv('\/kaggle\/input\/sample_submission.csv')\n    for model in model_list:\n        preds = preds + (model.predict_proba(combined_data.iloc[75278:,:])[:,-1])\n        #preds = preds + (model.predict_proba(combined_data[75278:,:])[:,-1])\n    submission.loc[:,'Outcome'] = preds\/len(model_list)\n    !rm '.\/submission.csv'\n    submission.to_csv('submission.csv', index = False, header = True)\n    print(submission.head())","4c4abc6a":"def auc_cv(model,X,y):\n    rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n    auc = cross_val_score(model, X, y, scoring='roc_auc', cv = rskf)\n    return(auc)","9039bc01":"X_train = combined_data.iloc[:55000,:]\nX_val = combined_data.iloc[55000:75278,:]\ny_train = target[:55000]\ny_val =target[55000:75278]","bbb6b51d":"from imblearn.under_sampling import TomekLinks\n\ntl = TomekLinks(n_jobs = -1)\nX_train, y_train = tl.fit_sample(X_train, y_train)","9d082ee1":"def feature_importance(model, X_train):\n\n    print(model.feature_importances_)\n    names = X_train.columns.values\n    ticks = [i for i in range(len(names))]\n    plt.bar(ticks, model.feature_importances_)\n    plt.xticks(ticks, names,rotation =90)\n    plt.show()","8c8719d6":"weight = float(y_train.value_counts()[0]\/y_train.value_counts()[1])\nprint(\"Count Majority Class : {0}\".format(y_train.value_counts()[0]))\nprint(\"Count Minority Class : {0}\".format(y_train.value_counts()[1]))\nprint('weight : {0}'.format(weight))","d51583c7":"model_xgb = xgb.XGBClassifier(scale_pos_weight= weight, \n                              colsample_bytree=0.8, gamma=0.045, \n                             learning_rate=0.1, max_depth=10, \n                             n_estimators=1000,\n                             reg_alpha=0.45, reg_lambda=0.8,\n                             subsample=0.5,\n                             random_state =7, nthread = -1,seed=42,n_jobs = -1)\n###tooooooo slow\n\n#score = auc_cv(model_xgb,X_train,y_train)\n#print('mean : {0} std: {1}'.format(score.mean(),score.std())) \n\nmodel_xgb.fit(X_train,y_train,eval_set=[(X_train, y_train), (X_val, y_val)],\n        eval_metric=['auc','logloss'],\n        early_stopping_rounds = 50,\n        verbose=2)","376f477d":"feature_importance(model_xgb,X_train)","9f6823b0":"create_submission_file([model_xgb])","85283dd9":"model_lgb = lgb.LGBMClassifier(scale_pos_weight= weight, bagging_frequency=4, boosting_type='gbdt'\n                               ,colsample_bytree=0.8, feature_fraction=0.5,\n                               importance_type='split', learning_rate=0.1, max_depth=10,\n                               min_split_gain=0.0001, n_estimators=1000, n_jobs=-1,random_state=101, reg_alpha=0.45,\n                               reg_lambda=0.8, subsample=1.0)\n\n##Cross-Validation\n\n#score = auc_cv(model_lgb,X_train,y_train)\n#print('mean : {0} std: {1}'.format(score.mean(),score.std()))\n\nmodel_lgb.fit(X_train, y_train,eval_set=[(X_train, y_train), (X_val, y_val)],\n        eval_metric=['auc','logloss'],\n        early_stopping_rounds = 100,\n        verbose=2)","21b1509a":"print(model_lgb.best_score_['valid_1'])\nfeature_importance(model_lgb,X_train)","dfa33436":"create_submission_file([model_lgb])","ee75e343":"rfc = RandomForestClassifier(n_estimators=500 ,\n                             max_depth=10, min_samples_split=2, \n                             min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n                             n_jobs=-1, random_state=123, verbose=3, \n                             class_weight='balanced')\n\n##Cross-Validation\n#score = auc_cv(rfc,combined_data.iloc[:75278,:],target[:75278])\n#print('mean : {0} std: {1}'.format(score.mean(),score.std()))\n","62fa71d0":"rfc.fit(X_train,y_train)","268f38b3":"feature_importance(rfc,X_train)","9491c379":"create_submission_file([rfc,model_lgb])","a672e114":"etc = ExtraTreesClassifier(n_estimators=500 ,\n                             max_depth=6, min_samples_split=2, \n                             min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n                             max_leaf_nodes=None, min_impurity_decrease=0.0,\n                             bootstrap=True, oob_score=False, n_jobs=-1, random_state=123, verbose=3, \n                             class_weight='balanced')\n##Cross-Validation\n\n#score = auc_cv(etc,combined_data.iloc[:75278,:],target[:75278])\n#print('mean : {0} std: {1}'.format(score.mean(),score.std()))","6aead910":"etc.fit(X_train,y_train)","a36c2d80":"create_submission_file([etc,rfc,model_lgb,model_xgb])","557980ef":"# Dealing with class imbalance -- Under-sampling using Tomek links\n* Tomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/tomek.png?v=2)\n\n","4eba6eb8":"# There exists a class imbalance"}}