{"cell_type":{"984a4f89":"code","e5a40f6a":"code","08c5375f":"code","e1de2455":"markdown","fc163489":"markdown","735b1042":"markdown","2efd1379":"markdown","434c9fae":"markdown"},"source":{"984a4f89":"import numpy as np\n\ndef predict(X, w, b):\n    \"\"\"Make a prediction according to the equation number 7.\n    \n    Args:\n        X: a features matrix.\n        w: weights (a column vector).\n        b: a bias.\n      \n    Returns:\n        vector: a prediction with the same dimensions as a target column vector (n by 1).\n    \"\"\"\n    \n    # .dot() is a matrix multiplication in Numpy.\n    # We can ommit all-ones vector because Numpy can add a scalar to a vector directly.\n    return X.dot(w) + b\n\ndef J(y_hat, y):\n    \"\"\"Calculate a cost of this solution (equation 8).\n    \n    Args:\n        y_hat: a prediction vector.\n        y: a target vector.\n    \n    Returns:\n        scalar: a cost of this solution.\n    \"\"\"\n    # **2 - raise to the power of two.\n    # .mean() - calculate a mean value of vector elements.\n    return ((y_hat - y)**2).mean()\n\ndef dw(X, y_hat, y):\n    \"\"\"Calculate a partial derivative of J with respect to w (equation 9).\n    \n    Args:\n        X: a features matrix.\n        y_hat: a prediction vector.\n        y: a target vector.\n      \n    Returns:\n        vector: a partial derivative of J with respect to w.\n    \"\"\"\n    # .transpose() - transpose matrix.\n    return 2 * X.transpose().dot(y_hat - y) \/ len(y)\n\ndef db(y_hat, y):\n    \"\"\"Calculate a partial derivative of J with respect to b (equation 10).\n    \n    Args:\n        y_hat: a prediction vector.\n        y: a target vector.\n    \n    Returns:\n        vector: a partial derivative of J with respect to b.\n    \"\"\"\n    return 2 * (y_hat - y).mean()","e5a40f6a":"# A features matrix.\nX = np.array([\n                 [4, 7],\n                 [1, 8],\n                 [-5, -6],\n                 [3, -1],\n                 [0, 9]\n             ])\n\n# A target column vector.\ny = np.array([\n                 [37],\n                 [24],\n                 [-34], \n                 [16],\n                 [21]\n             ])\n\n# Initialize weights and bias with zeros.\nw = np.zeros((X.shape[1], 1))\nb = 0\n\n# How much gradient descent steps we will perform.\nnum_epochs = 50\n\n# A learning rate.\nalpha = 0.01\n\n# Here will be stored J for each epoch.\nJ_array = []\n\nfor epoch in range(num_epochs):\n    # Equation 7.\n    y_hat = predict(X, w, b)\n\n    # Equation 8.\n    J_array.append(J(y_hat, y))\n    \n    # Equation 11.\n    w = w - alpha * dw(X, y_hat, y)\n    \n    # Equation 12.\n    # b converges slower than w, so we increased alpha for it by a factor of 10. It's not mandatory though.\n    b = b - alpha * db(y_hat, y) * 10","08c5375f":"import matplotlib.pyplot as plt\n\nplt.plot(J_array)\nplt.xlabel('epoch')\nplt.ylabel('J')\nplt.show()\n\n# {:.3} - round to three significant figures for f-string.\nprint(f\"w1 = {w[0][0]:.3}\")\nprint(f\"w2 = {w[1][0]:.3}\")\nprint(f\"b = {b:.3}\")\nprint(f\"J = {J_array[-1]:.3}\")","e1de2455":"# Linear Regression\n\nHere I will present an implementation of a gradient descent for a linear regression.\n\n## Notation\n\nItalic lowercase $a$ will denote scalars  \nVectors will be formatted as bold lowercase: $\\textbf{a}$  \nBold uppercase font such as this $\\textbf{A}$ will be used for matrices\n\nThe following variables will be used below:  \n$m$ - number of features (columns)  \n$n$ - number of samples (rows)  \n$^{(i)}$ - $i$-th sample, where $1 \\leq i \\leq n$  \n$y$ - target (true values)  \n$\\hat{y}$ - prediction  \n$w_j$ - weight for $j$-th feature  \n$b$ - bias  \n$\\textbf{X}$ - features matrix of all samples  \n$\\textbf{y}$ - target vector  \n$\\hat{\\textbf{y}}$ - predicted vector  \n$\\textbf{w}$ - weights vector  \n$\\textbf{1}$ - all-ones vector of length n  \n$^T$ - [transpose](https:\/\/en.wikipedia.org\/wiki\/Transpose) operation\n\n## Unvectorized form\n\nThe general equation of a linear regression is following:\n\n\\begin{align*}\n\\hat{y}^{(i)} = x_{1}^{(i)}w_1 + \\ldots + x_{m}^{(i)}w_m + b \\tag 1\n\\end{align*}\n\nLet's define a cost function, we will use a [mean squared error](https:\/\/en.wikipedia.org\/wiki\/Mean_squared_error):\n\n\\begin{align*}\nJ(w_1,\\ldots,w_m,b) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} ( \\hat{y}^{(i)} - y^{(i)} )^{2} = \\frac{1}{n} \\sum\\limits_{i=1}^{n} ( x_{1}^{(i)} w_1 + \\ldots + x_{m}^{(i)} w_m + b - y^{(i)} )^{2} \\tag 2\n\\end{align*}\n\nNow we need to find a gradient of function $J$. It's a vector of the steapest ascent direction of the function and basically is just partial derivatives for weights and a bias of the previous formula:\n\n\\begin{align*}\n\\frac{\\partial J}{\\partial w_j} = \\frac{2}{n} \\sum\\limits_{i=1}^{n} ( x_{1}^{(i)} w_1 + \\ldots + x_{m}^{(i)} w_m + b - y^{(i)} ) \\cdot x_{j}^{(i)} = \\frac{2}{n} \\sum\\limits_{i=1}^{n} ( \\hat{y}^{(i)} - y^{(i)} ) \\cdot x_{j}^{(i)} \\tag 3\n\\end{align*}\n\n\\begin{align*}\n\\frac{\\partial J}{\\partial b} = \\frac{2}{n} \\sum\\limits_{i=1}^{n} ( x_{1}^{(i)} w_1 + \\ldots + x_{m}^{(i)} w_m + b - y^{(i)} ) = \\frac{2}{n} \\sum\\limits_{i=1}^{n} ( \\hat{y}^{(i)} - y^{(i)} )\\tag 4\n\\end{align*}\n\nNow the final step are rules updation for a gradient descent. We should minimize the cost function, so we need to move in an opposite direction from a gradient and we are subtracting our derivatives:\n\n\\begin{align*}\nw_j \\leftarrow w_j - \\alpha \\frac{\\partial J}{\\partial w_j} \\tag 5\n\\end{align*}\n\n\\begin{align*}\nb \\leftarrow b - \\alpha \\frac{\\partial J}{\\partial b} \\tag 6\n\\end{align*}\n\nThe symbol $\\leftarrow$ means that the left-hand side is updated to take the value on the right-hand side; the constant $\\alpha$ is known as a learning rate. The larger it is, the larger a step we take. If we chose the small learning rate and perform this update operations many times then we will find an optimal solution of weights and the bias.\n\n## Vectorization\n\nNow we will rewrite these equations in a vector form. Equation 1 in a vectorized form will become:\n\n\\begin{align*}\n\\hat{\\textbf{y}} = \\textbf{Xw} + b\\textbf{1} \\tag 7\n\\end{align*}\n\nCost function (see equation 2):\n\\begin{align*}\nJ(\\textbf{w},b) = \\frac{1}{n} {\\left\\lVert \\textbf{Xw} + b \\textbf{1} - \\textbf{y}\\right\\rVert}^2 = \\frac{1}{n} {\\left\\lVert \\hat{\\textbf{y}} - \\textbf{y}\\right\\rVert}^2 \\tag 8\n\\end{align*}\n\nTwo vertical bars mean a [$L^2$ norm](http:\/\/mathworld.wolfram.com\/L2-Norm.html) operation which is basically a square root of the sum of squared elements of the vector. If we raise it to the power of two then we will get just the sum of squared elements of the vector. And that is what a sum in equation 2 does.\n\nPartial derivatives (see equations 3 and 4):\n\n\\begin{align*}\n\\frac{\\partial J}{\\partial \\textbf{w}} = \\frac{2}{n} \\textbf{X}^{T} (\\textbf{Xw} + b \\textbf{1} - \\textbf{y}) = \\frac{2}{n} \\textbf{X}^{T} (\\hat{\\textbf{y}} - \\textbf{y}) \\tag 9\n\\end{align*}\n\n\\begin{align*}\n\\frac{\\partial J}{\\partial b} = \\frac{2}{n} (\\textbf{Xw} + b \\textbf{1} - \\textbf{y}) = \\frac{2}{n} (\\hat{\\textbf{y}} - \\textbf{y}) \\tag{10}\n\\end{align*}\n\nUpdate rules (see equations 5 and 6):\n\n\\begin{align*}\n\\textbf{w} \\leftarrow \\textbf{w} - \\alpha \\frac{\\partial J}{\\partial \\textbf{w}} \\tag{11}\n\\end{align*}\n\n\\begin{align*}\nb \\leftarrow b - \\alpha \\frac{\\partial J}{\\partial b} \\tag{12}\n\\end{align*}\n\n**Check dimensions for every equation and rewrite them in a long form to be sure that they are correct representations of unvectorized formulas.**\n\n## Example\n\nFor example, we have the following five samples with two features $x_1$ and $x_2$, target $y$ and we want to find the best fitting linear function for this data:\n\n| $x_1$       | $x_2$        | $y$         |\n|-------------|--------------|-------------|\n| 4           | 7            | 37          |\n| 1           | 8            | 24          |\n| -5          | -6           | -34         |\n| 3           | -1           | 16          |\n| 0           | 9            | 21          |","fc163489":"You can check that the answer is $w_1 = 5$, $w_2 = 2$ and $b = 3$. If you multiply the first column by $w_1$, the second by $w_2$, sum them together and add $b$, you will get exactly our target column. Let's show how to find this solution using python and vecorized formulas above.\n\nWe will implement functions for formulas 7, 8, 9, and 10.","735b1042":"Visualize how the cost drops after each epoch.","2efd1379":"WIth cost $J$ approaching zero the results are almost the same as we stated before: $w_1 = 5$, $w_2 = 2$ and $b = 3$.  \nIf you increase a number of epochs you will get even closer to the true values.","434c9fae":"Now let's run gradient descent for our example."}}