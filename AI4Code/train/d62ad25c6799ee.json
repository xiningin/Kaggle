{"cell_type":{"b02daf56":"code","8fc480b4":"code","69108424":"code","6af063e8":"code","a09ec415":"code","d13c772b":"code","36e4d0b0":"code","3a44df23":"code","f0626975":"code","3f75c6a6":"code","8facb762":"code","39955f49":"code","3a06db55":"code","1c3dcfb9":"code","0cf69cf7":"code","b71bf5b5":"markdown","94229a94":"markdown","206ff4cf":"markdown","5c8d4c4a":"markdown","2b7c3858":"markdown","2f7aaa66":"markdown","535d8b69":"markdown","4a923f0b":"markdown","cf991bb8":"markdown"},"source":{"b02daf56":"!pip install tweepy","8fc480b4":"#Importing Libraries\nimport tweepy\nfrom textblob import TextBlob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud\nimport json\nfrom collections import Counter","69108424":"#Part-1: Authorization and Search tweets\n#Getting authorization\nconsumer_key = 'p6Fg6WjZeOIQN1FvrsnvCfcbo'\nconsumer_key_secret = '7sgTYd66QCOoaEDuOXSoH5TkoTJrzmWvjEDnVdIaO7tqZgeGhg'\naccess_token = '132425188-3pEgCTglpzvYC605OMINgGkn3fO5pTFMv2NfWY61'\naccess_token_secret = 'Qkzccwwbf6J4Mrpkem4LHVefHxBT13E3C28x0F8gufIyc'\nauth = tweepy.OAuthHandler(consumer_key, consumer_key_secret)\nauth.set_access_token(access_token, access_token_secret)\napi = tweepy.API(auth, wait_on_rate_limit=True)","6af063e8":"#Defining Search keyword and number of tweets and searching tweets\nquery = 'COVID-19'\nmax_tweets = 2000\nsearched_tweets = [status for status in tweepy.Cursor(api.search, q=query).items(max_tweets)]","a09ec415":"#Part-2: Sentiment Analysis Report\n\n#Finding sentiment analysis (+ve, -ve and neutral)\npos = 0\nneg = 0\nneu = 0\nfor tweet in searched_tweets:\n    analysis = TextBlob(tweet.text)\n    if analysis.sentiment[0]>0:\n       pos = pos +1\n    elif analysis.sentiment[0]<0:\n       neg = neg + 1\n    else:\n       neu = neu + 1\nprint(\"Total Positive = \", pos)\nprint(\"Total Negative = \", neg)\nprint(\"Total Neutral = \", neu)","d13c772b":"#Plotting sentiments\nlabels = 'Positive', 'Negative', 'Neutral'\nsizes = [257, 223, 520]\ncolors = ['gold', 'yellowgreen', 'lightcoral']\nexplode = (0.1, 0, 0)  # explode 1st slice\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\nplt.axis('equal')\nplt.show()","36e4d0b0":"#Part-3: Creating Dataframe of Tweets\n\n#Cleaning searched tweets and converting into Dataframe\nmy_list_of_dicts = []\nfor each_json_tweet in searched_tweets:\n    my_list_of_dicts.append(each_json_tweet._json)\n    \nwith open('tweet_json_Data.txt', 'w') as file:\n        file.write(json.dumps(my_list_of_dicts, indent=4))\n        \nmy_demo_list = []\nwith open('tweet_json_Data.txt', encoding='utf-8') as json_file:  \n    all_data = json.load(json_file)\n    for each_dictionary in all_data:\n        tweet_id = each_dictionary['id']\n        text = each_dictionary['text']\n        favorite_count = each_dictionary['favorite_count']\n        retweet_count = each_dictionary['retweet_count']\n        created_at = each_dictionary['created_at']\n        my_demo_list.append({'tweet_id': str(tweet_id),\n                             'text': str(text),\n                             'favorite_count': int(favorite_count),\n                             'retweet_count': int(retweet_count),\n                             'created_at': created_at,\n                            })\n        \n        tweet_dataset = pd.DataFrame(my_demo_list, columns = \n                                  ['tweet_id', 'text', \n                                   'favorite_count', 'retweet_count', \n                                   'created_at'])\n    \n#Writing tweet dataset ti csv file for future reference\ntweet_dataset.to_csv('tweet_data.csv')","3a44df23":"tweet_dataset.shape","f0626975":"tweet_dataset.head()","3f75c6a6":"#part 4 - Cleaning Data\n\n#Removing @ handle\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt \n\ntweet_dataset['text'] = np.vectorize(remove_pattern)(tweet_dataset['text'], \"@[\\w]*\")","8facb762":"tweet_dataset.head()","39955f49":"tweet_dataset['text'].head(10)","3a06db55":"#Cleaning Tweets\ncorpus = []\nfor i in range(0, 1000):\n    tweet = re.sub('[^a-zA-Z0-9]', ' ', tweet_dataset['text'][i])\n    tweet = tweet.lower()\n    tweet = re.sub('rt', '', tweet)\n    tweet = re.sub('http', '', tweet)\n    tweet = re.sub('https', '', tweet)\n    tweet = tweet.split()\n    ps = PorterStemmer()\n    tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n    tweet = ' '.join(tweet)\n    corpus.append(tweet)","1c3dcfb9":"#Part-5: Visualization\n\n#Word Cloud\nall_words = ' '.join([text for text in corpus])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","0cf69cf7":"#Term Freuency - TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(tweet_dataset['text'])\n#Count Most Frequent Words\nCounter = Counter(corpus)\nmost_occur = Counter.most_common(10) \nprint(most_occur)","b71bf5b5":"You can pass the keyword of your interst here and maximum number of tweets to be downloaded through the tweepy API.","94229a94":"As we are ready now with the tweet data set, we will analyze our dataset and clean this data in the following segments.","206ff4cf":"Now, after performning the NLP operations, we visualize the most frequent words in the tweets through a word clound and using the terf frequency.","5c8d4c4a":"To use the 'tweepy' API, you need to create an account with Twitter Developer. After creating the account, go to 'Get Started' option and navigate to the 'Create an app' option. After you create the app, not down the below required credintials from there.","2b7c3858":"Here, we will create a dataframe of all the tweet data that we have downloaded. Later all the processed data will be saved to a CSV file in the local system. Through this way, we can utilize this tweet data for other experimental purpose. ","2f7aaa66":"Here, as we are ready with the clean tweet data, we will perform NLP operations on the tweet texts including taking only alphabets, converting all to lower cases, tokenization and stemming. As retweets, hypertexts etc. are present in the tweets, we need to remove all those unneccessary information.","535d8b69":"The above al the most frequent terms appeared in the tweet data. \nSo, this is the way how we can download the tweets, clean those tweets, convert them into dataframes, save them inti csv file and finally, analyze those tweets.","4a923f0b":"We will now analyze the sentiments of tweets that we have downloaded and visualize them here.","cf991bb8":"Twitter is a big source of news nowadays because it is the most comprehensive source of public conversations around the world. A latest news which may not be available on news channels or websites but it may be trending on the twitter among public coonversations. Any information, despit of being cnstructive or destructive can be easily spread across the twitter network skipping the editorial cuts or regulations. \n\nBecause of this nature of the twitter, it is alos being popular among data analysts who want to gather some trending information and perform analytics. In this article, we will learn to download and analyze the twitter data. We will learn how to get tweets related to an interesting keyword, how to clean, analyze, visualize those tweets and finally how to covert it into a dataframe and save it into a csv file.\n\nFirst of all, to fetch tweets from twitter, we need to install the tweepy library."}}