{"cell_type":{"b0ee50d2":"code","0c71560c":"code","48254426":"code","392370d5":"code","e8fa3987":"code","4dff5e79":"code","aaacd16f":"code","7eb1227b":"code","48e49083":"code","1510e1ef":"code","0952648f":"code","9d2f01cf":"code","913f8c47":"code","e59929d9":"code","1ae12dcb":"markdown","ddd8996b":"markdown","6dd98926":"markdown","45851329":"markdown","73e43694":"markdown"},"source":{"b0ee50d2":"import os\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n#print(os.listdir(\"..\/input\"))\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib\n\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor, cv, DMatrix, train\n\nimport lightgbm as lgb\n\nfrom sklearn.base import BaseEstimator, RegressorMixin\n\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(round(rmse.mean(), 5))","0c71560c":"#load data\ndf_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\n#df_train = pd.read_csv(\"data\/train.csv\")\n#df_test = pd.read_csv(\"data\/test.csv\")\n\n#drop outliers how described in [1]\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)\n\ndf_train_test = pd.concat((df_train.loc[:,'MSSubClass':'SaleCondition'],\n                      df_test.loc[:,'MSSubClass':'SaleCondition']))\n\nTRAIN_SIZE = df_train.shape[0]","48254426":"#log transform the target because of (R)MSLE metric:\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n\n#MSSubClass to str\ndf_train_test['MSSubClass'] = df_train_test['MSSubClass'].apply(lambda x: str(x))\n\n#log transform skewed numeric features:\nnumeric_feats = df_train_test.dtypes[df_train_test.dtypes != \"object\"].index\n\n#find skewed features and log tranform of them to normalize\nskewed_feats = df_train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\ndf_train_test[skewed_feats] = np.log1p(df_train_test[skewed_feats])","392370d5":"#drop features with one large class (>98%)\ndf_train_test.drop(columns=['Street'], inplace=True)\ndf_train_test.drop(columns=['Utilities'], inplace=True)\ndf_train_test.drop(columns=['Condition2'], inplace=True)\ndf_train_test.drop(columns=['RoofMatl'], inplace=True)\ndf_train_test.drop(columns=['Heating'], inplace=True)\n\n#new feature - total area of house as a sum of all floors area\ndf_train_test['TotalSF'] = df_train_test['TotalBsmtSF'] + df_train_test['1stFlrSF'] + df_train_test['2ndFlrSF']\n\n#price per feet engineering:\n#cost of 1 square feet of living area per house by Neighborhood groups\n#Its a kind of mean encoding (or likehood encoding)\ndf_train['DolPerFeetLiv'] = df_train['SalePrice']\/df_train['GrLivArea']\ndata = pd.concat([df_train['Neighborhood'], df_train['DolPerFeetLiv']], axis=1)\ncost_per_district = data.groupby('Neighborhood')['DolPerFeetLiv'].mean()\ndf_train_test['DolPerFeetNeigLiv'] = df_train_test['Neighborhood'].apply(lambda x: cost_per_district[x])\n\n#cost of 1 square feet of lot area per house by Neighborhood groups\ndf_train['DolPerFeetLot'] = df_train['SalePrice']\/df_train['LotArea']\ndata = pd.concat([df_train['Neighborhood'], df_train['DolPerFeetLot']], axis=1)\ncost_per_district = data.groupby('Neighborhood')['DolPerFeetLot'].mean()\ndf_train_test['DolPerFeetNeigLot'] = df_train_test['Neighborhood'].apply(lambda x: cost_per_district[x])","e8fa3987":"#dummy - encoding\ndf_train_test_dummies = pd.get_dummies(df_train_test)\n\n#filling NA's with the mean of the column:\ndf_train_test_dummies = df_train_test_dummies.fillna(df_train_test_dummies.mean())\n\n#train test splitting\nX = df_train_test_dummies[:TRAIN_SIZE]\nX_test = df_train_test_dummies[TRAIN_SIZE:]\ny = df_train.SalePrice\n\nprint('shape for Lasso: ', X.shape)","4dff5e79":"#Lasso tune hyperparams\n#107 vars, rsmse = 0.11004\nmodel_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X, y)\nlasso_rmse = rmse_cv(model_lasso)\ncoef = pd.Series(model_lasso.coef_, index = X.columns)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n\nprint(rmse_cv(model_lasso).mean())\n\nmodel_lasso.fit(X, y)\ny_lasso = model_lasso.predict(X_test)\ny_lasso = np.expm1(y_lasso)","aaacd16f":"#feature engineering for boosting\ndef Qual_to_num(x):\n    if x == 'Ex':\n        return 4\n    elif x == 'Gd':\n        return 3\n    elif x == 'TA':\n        return 2\n    else:\n        return 1  \n\ndf_train_test['KitchenQualNum'] = df_train_test.KitchenQual.apply(Qual_to_num)\ndf_train_test['HeatingQCNum'] = df_train_test.HeatingQC.apply(Qual_to_num)\n\ndf_train_test_dummies = pd.get_dummies(df_train_test)\n\n#filling NA's with the mean of the column:\ndf_train_test_dummies = df_train_test_dummies.fillna(df_train_test_dummies.mean())\n\ndf_train_test_dummies = pd.get_dummies(df_train_test_dummies, columns=['MoSold'])\n\n#train test splitting\nX = df_train_test_dummies[:TRAIN_SIZE]\nX_test = df_train_test_dummies[TRAIN_SIZE:]\ny = df_train.SalePrice\n\nprint('shape for XGBoost: ', X.shape)","7eb1227b":"#rmse 0.11683\nmodel_xgb = XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\n\nprint(rmse_cv(model_xgb).mean())\n\nmodel_xgb.fit(X, y)\ny_xgb = model_xgb.predict(X_test)\ny_xgb = np.expm1(y_xgb)","48e49083":"#LilghtGBM rmse 0.11582\nmodel_lgb = lgb.LGBMRegressor(objective='regression',\n                              num_leaves=4,\n                              learning_rate=0.05, \n                              n_estimators=1000,\n                              max_bin=75, \n                              bagging_fraction=0.8,\n                              bagging_freq=5, \n                              feature_fraction=0.2319,\n                              feature_fraction_seed=9, \n                              bagging_seed=9,\n                              min_data_in_leaf=6, \n                              min_sum_hessian_in_leaf=11)\nprint(rmse_cv(model_lgb).mean())\n\nmodel_lgb.fit(X, y)\ny_lgb = model_lgb.predict(X_test)\ny_lgb = np.expm1(y_lgb)","1510e1ef":"#The pairplot for predeictions of LGBM and Lasso\npredictions = pd.DataFrame({\"lgb\":y_lgb, \"lasso\":y_lasso})\npredictions.plot(x = \"lgb\", y = \"lasso\", kind = \"scatter\")","0952648f":"#The pairplot for predeictions of LGBM and XGB\npredictions = pd.DataFrame({\"xgb\":y_xgb, \"lgb\":y_lgb})\npredictions.plot(x = \"xgb\", y = \"lgb\", kind = \"scatter\")","9d2f01cf":"#mixing version of several classifiers\nclass MeanRegressor(BaseEstimator, RegressorMixin):  \n\n    def __init__(self, regressor1=None, regressor2=None, regressor3=None, r1=0.33, r2=0.33):\n        \n        self.regressor1 = regressor1\n        self.regressor2 = regressor2\n        self.regressor3 = regressor3\n        self.r1 = r1\n        self.r2 = r2\n        \n\n    def fit(self, X, y=None):\n       \n        self.regressor1.fit(X, y)\n        self.regressor2.fit(X, y)\n        self.regressor3.fit(X, y)\n        \n        return self\n\n    def predict(self, X, y=None):\n        return self.r1 * self.regressor1.predict(X) + self.r2 * self.regressor2.predict(X) + (1 - self.r1 - self.r2)*self.regressor3.predict(X)","913f8c47":"#0.10843\nmr = MeanRegressor(model_lasso, model_lgb, model_xgb, 0.55, 0.39)\nrmse_cv(mr)","e59929d9":"#0.5 0.4 0.1 - best, because xgb make worse\npreds = 0.55*y_lasso + 0.39*y_lgb + 0.06*y_xgb\npreds = preds\nsolution = pd.DataFrame({\"id\":df_test.Id, \"SalePrice\":preds})\n\nsolution.to_csv(\"solution.csv\", index = False)","1ae12dcb":"As in [2] kernel let's use Lasso regression and tune it's hyperparams.\nLasso is a strong regularization model which choose the most important feature. \nIn our case there are 107 the most important ones.","ddd8996b":"I've defined my own estimator class by calculating mean value of several estimators. It's pretty simple!","6dd98926":"Let's use two best gradient boosting regressors: XGBoost and LightGBM. Estimate score each of them by cross-validation:","45851329":"Note that total list of features differs for Boosting model. For boosting algorithm I use additional features: information about range of some properties,","73e43694":"Hi everyone!\nI was inspired by these two kernels: \n\nhttps:\/\/www.kaggle.com\/ammar111\/house-price-prediction-bagging-xgboost-top-8 [1]\n\nhttps:\/\/www.kaggle.com\/apapiu\/regularized-linear-models [2]\n\nand tried to use the most useful techiques from both of them. \nI also used some tricks like feature engineering and mixing several estimators (by mean their predictions).\n\nThe total score of this kernel is about 10.9 on cross-validation and about 11.5 on public leader board,"}}