{"cell_type":{"682fadad":"code","fdf2e6fa":"code","dd8eb2c3":"code","4a3c729b":"code","c0919b25":"code","707798da":"code","c9e4901e":"code","4e102f09":"code","7fb719c6":"code","67e68935":"code","69cee681":"code","a4ff5c1e":"code","ea45f659":"code","0f025f5d":"code","a628dc43":"code","9f006787":"code","99b437b9":"code","4eeeb61e":"code","0e2cb025":"code","77842cff":"code","076cd74c":"code","012f1e45":"code","e70dd59a":"code","943a2e82":"code","296e652a":"code","9daf9e91":"code","71a5e656":"code","44047387":"markdown","a127c4e4":"markdown","4189592f":"markdown","bfd61d4f":"markdown","b7436a43":"markdown","947c9ab0":"markdown","5748fd26":"markdown","22953898":"markdown","85f235dd":"markdown","17237063":"markdown","1d7ed452":"markdown","5bf12478":"markdown","9ce42d67":"markdown","7334d299":"markdown","3abce4e3":"markdown","79503296":"markdown","4270b4c6":"markdown","7a35109c":"markdown","0e84d66f":"markdown","15a41dbe":"markdown","84225fdf":"markdown","6f4d3246":"markdown","b15270e2":"markdown","66ff88ee":"markdown","f37356e3":"markdown","3d5de088":"markdown"},"source":{"682fadad":"%matplotlib inline\nimport shap\nimport lime\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.externals.six import StringIO\nfrom sklearn import preprocessing, metrics, model_selection\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder, LabelBinarizer \nfrom sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, recall_score, precision_score, f1_score, precision_recall_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, train_test_split\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom datetime import datetime, date, timezone, timedelta\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, gc\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nsns.set()","fdf2e6fa":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndisplay(df_train.info())\ndisplay(df_train.head())","dd8eb2c3":"sns.distplot(df_train['Age'].dropna(),bins=range(0,100,10),kde=False)","4a3c729b":"sns.pairplot(df_train)","c0919b25":"print('Train columns with null values:\\n', df_train.isnull().sum())","707798da":"display(df_train[\"Embarked\"].value_counts())","c9e4901e":"display(df_train[\"Cabin\"].str[0].value_counts())","4e102f09":"display(df_train[\"Fare\"].describe())\nplt.hist(df_train[\"Fare\"])\nplt.show()\nplt.hist(df_train[\"Fare\"][df_train[\"Fare\"]<=100])","7fb719c6":"def feature_engineering(df):\n    # Null Value Handling\n    df[\"Age\"].fillna(df[\"Age\"].median(),inplace=True)\n    df[\"Embarked\"].fillna(df['Embarked'].mode()[0], inplace = True)\n    df = df.fillna(-1)\n    \n    # Feature Encoding\n    df[\"Sex\"] = df[\"Sex\"].map({'male':1,'female':0}).fillna(-1).astype(int)\n    df[\"Embarked\"] = df[\"Embarked\"].map({'S':0,'C':1,'Q':2}).astype(int)\n    df[\"Cabin\"] = df[\"Cabin\"].str[0].map({'T':0,'G':1,'F':2,'E':3,'D':4,'C':5,'B':6,'A':7}).fillna(-1).astype(int)\n    \n    # Binning\n    bins_age = np.linspace(0, 100, 10)\n    df[\"AgeBin\"] = np.digitize(df[\"Age\"], bins=bins_age)\n    \n    df[\"FareBin\"] = 0\n    df[\"FareBin\"][(df[\"Fare\"]>=0)&(df[\"Fare\"]<10)] = 1\n    df[\"FareBin\"][(df[\"Fare\"]>=10)&(df[\"Fare\"]<20)] = 2\n    df[\"FareBin\"][(df[\"Fare\"]>=20)&(df[\"Fare\"]<30)] = 3\n    df[\"FareBin\"][(df[\"Fare\"]>=30)&(df[\"Fare\"]<40)] = 4\n    df[\"FareBin\"][(df[\"Fare\"]>=40)&(df[\"Fare\"]<50)] = 5\n    df[\"FareBin\"][(df[\"Fare\"]>=50)&(df[\"Fare\"]<100)] = 6\n    df[\"FareBin\"][(df[\"Fare\"]>=100)] = 7\n\n    # Create New Features (Optional)\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n    df['Title'] = -1\n    df['Title'][df[\"Name\"].str.contains(\"Mr\")] = 0\n    df['Title'][df[\"Name\"].str.contains(\"Master\")] = 1\n    df['Title'][df[\"Name\"].str.contains(\"Miss\")] = 2\n    df['Title'][df[\"Name\"].str.contains(\"Mrs\")] = 3\n    \n    # Drop unsed columns\n    del df[\"Age\"]\n    del df[\"Fare\"]\n    del df[\"Ticket\"]\n    \n    return df","67e68935":"df_train_fe = feature_engineering(df_train)\ndf_test_fe = feature_engineering(df_test)\n\ndisplay(df_train_fe.head())","69cee681":"display(df_train_fe[\"FareBin\"].value_counts())","a4ff5c1e":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(df_train_fe)","ea45f659":"exclude_columns = [\n    'Name',\n    'Ticket',\n    'PassengerId',\n    'Survived'\n]\n\nevals_result = {}\nfeatures = [c for c in df_train_fe.columns if c not in exclude_columns]\ntarget = df_train_fe['Survived']\nprint(len(target))\n\ngc.collect()\n\nX_train, X_test, y_train, y_test = train_test_split(df_train_fe[features], target, test_size=0.2, random_state=440)\n\nparam = {   \n    'boost': 'gbdt',\n    'learning_rate': 0.008,\n    'feature_fraction':0.20,\n    'bagging_freq':1,\n    'bagging_fraction':1,\n    'max_depth': -1,\n    'num_leaves':17,\n    'lambda_l2': 0.9,\n    'lambda_l1': 0.9,\n    'max_bin':200,\n    'metric':{'auc','binary_logloss'},\n#    'metric':{'binary_logloss'},\n    'tree_learner': 'serial',\n    'objective': 'binary',\n    'verbosity': 1,\n}\n\noof = np.zeros(len(df_train_fe))\npredictions = np.zeros(len(df_test_fe))\nfeature_importance_train = pd.DataFrame()\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_valid = lgb.Dataset(X_test, y_test)\nnum_round = 10000\nclf = lgb.train(param, lgb_train, num_round, valid_sets = [lgb_train, lgb_valid],\n      verbose_eval=100, early_stopping_rounds = 1000, evals_result = evals_result)\noof = clf.predict(X_test, num_iteration=clf.best_iteration)\n\n## Prediction\npredictions = clf.predict(df_test_fe[features], num_iteration=clf.best_iteration)\n\n# Visualize Metrics\naxL = lgb.plot_metric(evals_result, metric='auc')\naxL.set_title('AUC')\naxL.set_xlabel('Iterations')\naxL.set_ylim(0,1.1)\naxR = lgb.plot_metric(evals_result, metric='binary_logloss')        \naxR.set_title('Binary_Logloss')\naxR.set_xlabel('Iterations')\nplt.show()\n\n# Importance\nfold_importance_train = pd.DataFrame()\nfold_importance_train[\"feature\"] = features\nfold_importance_train[\"importance\"] = clf.feature_importance()\nfold_importance_train[\"fold\"] = 1\nfeature_importance_train = pd.concat([feature_importance_train, fold_importance_train], axis=0)\n\nprecisions, recalls, thresholds = precision_recall_curve(y_test, oof)\n\nfig = plt.figure(figsize=(14,4))\n\n### Threshold vs Precision\/Recall\nax = fig.add_subplot(1,2,1)\nax.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\nax.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\nax.set_title(\"Threshold vs Precision\/Recall\")\nax.set_xlabel(\"Threshold\")\nax.legend(loc=\"center left\")\nax.set_ylim([0,1.1])\nax.grid()\nfig.show()\n\n### Precision-Recall Curve\nax = fig.add_subplot(1,2,2)\nax.step(recalls, precisions, color='b', alpha=0.2, where='post')\nax.fill_between(recalls, precisions, step='post', alpha=0.2, color='b')\nax.set_title('Precision-Recall Curve')\nax.set_xlabel('Recall')\nax.set_ylabel('Precision')\nax.set_ylim([0.0, 1.05])\nax.set_xlim([0.0, 1.0])\nax.grid()\nfig.show()\n\n### \u30b9\u30b3\u30a2\u5206\u5e03\ndf_train_result = pd.DataFrame()\ndf_train_result['Actual Result'] = y_test\ndf_train_result['Prediction Score'] = oof\ndf_train_result = df_train_result.sort_values('Prediction Score',ascending=False).sort_index(ascending=False)\ndf_target = df_train_result[df_train_result['Actual Result']==1]\ndf_nontarget = df_train_result[df_train_result['Actual Result']==0]\n\n### Show Importance\ncols = (feature_importance_train[[\"feature\",\"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_train.loc[feature_importance_train.feature.isin(cols)]\n\nplt.figure(figsize=(14,5))\nsns.barplot(x=\"importance\", y=\"feature\", \n            data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\n","0f025f5d":"submit = pd.DataFrame({ 'PassengerId' : df_test_fe[\"PassengerId\"], 'Survived': np.int32(predictions >= 0.5) })\nsubmit.to_csv('submission.csv', index=False)","a628dc43":"shap.initjs()\nexplainer = shap.TreeExplainer(clf)\nshap_values = explainer.shap_values(X_train)\nshap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X_train.iloc[0,:])","9f006787":"shap.force_plot(explainer.expected_value[1], shap_values[1][1,:], X_train.iloc[1,:])","99b437b9":"shap.force_plot(explainer.expected_value[1], shap_values[1][2,:], X_train.iloc[2,:])","4eeeb61e":"shap.force_plot(base_value=explainer.expected_value[1], shap_values=shap_values[1], features=X_train.columns)","0e2cb025":"shap.decision_plot(explainer.expected_value[1], shap_values[1][0,:], X_train.iloc[0,:])","77842cff":"shap.summary_plot(shap_values, X_train)\nshap.summary_plot(shap_values, X_train, plot_type='bar')","076cd74c":"shap.dependence_plot(\"AgeBin\",shap_values[1], X_train)","012f1e45":"shap.dependence_plot(\"Sex\",shap_values[1], X_train)","e70dd59a":"shap.dependence_plot(\"Pclass\",shap_values[1], X_train)","943a2e82":"shap.dependence_plot(\"FareBin\",shap_values[1], X_train)","296e652a":"shap.dependence_plot(\"Title\",shap_values[1], X_train)","9daf9e91":"import lime\nimport lime.lime_tabular\n\ndef predict_fn(x):\n    preds = clf.predict(x, num_iteration=clf.best_iteration).reshape(-1,1)\n    p0 = 1 - preds\n    return np.hstack((p0, preds))\n\nexplainerLime = lime.lime_tabular.LimeTabularExplainer(\n    X_train.values,\n    mode='classification',\n    feature_names=features,\n   class_names=[\"NotSurvived\", \"Survived\"],\n   verbose=True\n    )\n\nnp.random.seed(1)\ni = 0\nexp = explainerLime.explain_instance(X_train[features].values[i], predict_fn, num_features=10)\nexp.show_in_notebook(show_all=True)","71a5e656":"shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X_train.iloc[0,:])","44047387":"From this plot, we can understand the yonger, the higher survive change. (1 : Age 0~10, 8 : 70~80)","a127c4e4":"By using this Dependency Plot, we can visualize the affection between variables to the predictions score.\nWhen you specify 1 feature, This Dependenc_Plot function will automatically pick up another feature which have the strongest dependency with the selected feature. This is useful!","4189592f":"## Check Value Variation","bfd61d4f":"## Summary Plot","b7436a43":"# Model Interpretation with Titanic Data\n\nHi, I am very new to Data Science...\nNow I'm studying model interpretation methodologies like SHAP, LIME.\nI have created this kernel to pracice using those libraries.\nPlease advice if you found any mistakes in the code.\n\nThank you so much in advance!","947c9ab0":"## Check Value Distribution for Numeric Fields","5748fd26":"## Force Plot (Overall Data)","22953898":"##### Objective of this plot seems same as Force Plot (understanding the reason why that specific record are judged as \"Survived\/Nonsurvived\".","85f235dd":"This Order is different from Feature Importance.\nHow can we choose Feature Importance and SHAP in the usual business scenario?\nIf SHAP value is more intuitive for human than Feature Importance, is there any meaning to use Feature Importance?","17237063":"From this plot, we can understand how much each feature impacted to the prediction score.\nFor example, for this passenger, Sex = 0 (Femail) attributed to the suvival the most.","1d7ed452":"Use same First Training Data as that was used for SHAP Force Plot Visualization.","5bf12478":"## Check Pearson Correlation","9ce42d67":"# Model Interpretation by SHAP","7334d299":"# Simple EDA\nAs main purpose of this Kernel is to practice SHAP library, I have done minimum EDA...","3abce4e3":"In thi plot,passengers are sorted by the similarity. So similar passengers (which means having close feature value from the view point of data) are gathered. From this plot ,we can roughly understand what kind of passenger groups are there and its volume.","79503296":"# Model Interpretation by LIME","4270b4c6":"# Simple Feature Engineering\nReferrence : https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy","7a35109c":"# Train Model and Predict with LightGBM","0e84d66f":"# Load Data","15a41dbe":"## Decision Plot","84225fdf":"## Force Plot (Individual Data)","6f4d3246":"##### This interpretation seems almost same as that was interpreted by SHAP. (Show below again)\n##### \u30fbTitle & Sex higher the survive rate the most.\n##### \u30fbCabin & Pclass & Pclass comes after those.\n##### \u30fbParch & Embarked lower the survive rate a little.","b15270e2":"## NULL Value Check","66ff88ee":"## Dependency Plot","f37356e3":"From this plot, we can understand Sex = Women (Sex = 0) have raised chance of survive.","3d5de088":"Next, I tried to utilize LIME library. As SHAP('17) is suggeted after LIME('16) and it unified Additive Feature Attribution Methodology including LIME , I don't know how much motivation is remaining to use LIME now but let me try to use this. Let me know if you know about this point."}}