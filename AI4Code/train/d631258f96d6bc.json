{"cell_type":{"835ba8a6":"code","fe539e6e":"code","bc3966fd":"code","70166b15":"code","5d39fcba":"code","b48275db":"code","1a85d13d":"code","cd139a98":"code","e5949dcb":"code","42ccb372":"code","ec2548fe":"code","fd8757da":"code","28d7be35":"code","840e95ea":"code","dcd9acae":"code","3e09f672":"code","d1d6f742":"code","73259777":"code","6d3f4ca0":"code","8b2fc4b6":"code","ccdff82f":"code","8b082e12":"code","b3e2eb3b":"markdown","37d32c44":"markdown","9d771901":"markdown","c893f274":"markdown","28977e8a":"markdown","aab756e0":"markdown","8352ff6e":"markdown","ba38af3d":"markdown","6e8c26e8":"markdown","ebc505f7":"markdown","e9e8c7d7":"markdown","8bc2c54a":"markdown","b4920004":"markdown","26bd1a81":"markdown","3eea1a68":"markdown","38109579":"markdown","b6539702":"markdown","0d09f143":"markdown","27b2c925":"markdown","bf607958":"markdown","cebd6b71":"markdown","411e9634":"markdown","c9000908":"markdown","ecc7ee67":"markdown","d97d63c8":"markdown","da0ae765":"markdown","af1a102a":"markdown","b1d92191":"markdown","44ad4fdc":"markdown","3747281a":"markdown","9f1e2412":"markdown"},"source":{"835ba8a6":"!pip install -Uqq fastbook","fe539e6e":"# basic libraris\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n        \n# fastai\nfrom fastai import *\nfrom fastbook import *\nfrom fastai.vision import *\nfrom fastai.imports import *\nfrom fastai.vision.all import *\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# widgets\nimport ipywidgets as widgets","bc3966fd":"gv('''\ninit->predict->loss->gradient->step->stop\nstep->predict[label=repeat]\n''')","70166b15":"def f(x): return x**2\nplot_function(f, 'x', 'x**2')","5d39fcba":"plot_function(f, 'x', 'x**2')\nplt.scatter(-1.5, f(-1.5), color='red');","b48275db":"xt = tensor(3.).requires_grad_()","1a85d13d":"yt = f(xt)\nyt","cd139a98":"yt.backward()\nxt.grad","e5949dcb":"time = torch.arange(0,20).float(); \ntime","42ccb372":"speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1\nplt.scatter(time,speed);","ec2548fe":"def f(t, params): \n    a,b,c = params\n    return a*(t**2) + (b*t) + c","fd8757da":"def mse(preds, targets): return ((preds-targets)**2).mean()","28d7be35":"params = torch.randn(3).requires_grad_()\nparams","840e95ea":"preds = f(time, params)","dcd9acae":"def show_preds(preds, ax=None):\n    if ax is None: ax = plt.subplots()[1]\n    ax.scatter(time,speed)\n    ax.scatter(time, to_np(preds),color='red')\n    ax.set_ylim(-300,100)\nshow_preds(preds)","3e09f672":"loss = mse(preds, speed)\nloss","d1d6f742":"loss.backward()\nparams.grad","73259777":"lr = 1e-5\nparams.data -= lr * params.grad.data\nparams.grad = None","6d3f4ca0":"preds = f(time,params)\nmse(preds, speed)","8b2fc4b6":"def apply_step(params, prn = True):\n    preds = f(time, params)\n    loss = mse(preds, speed)\n    loss.backward()\n    params.data -= lr * params.grad.data\n    params.grad = None\n    if prn: print(loss.item())\n    return preds","ccdff82f":"for i in range(10): apply_step(params)","8b082e12":"_, axs = plt.subplots(1,4, figsize = (12,3))\nfor ax in axs: show_preds(apply_step(params, False),ax)\nplt.tight_layout()","b3e2eb3b":"The plot will obvisouly be different from the book since the parameters are randomly defined. The book doesn't mention the seed and so the exact result isn't reproducible.","37d32c44":"#### 3.1 Basic example <a id=4><\/a>\n\n[back to top](#100)\n\nLet's take a very basic example","9d771901":"#### 3.3 Stepping with a Learning Rate <a id=6><\/a>\n\n[back to top](#100)\n\nDeciding how to change our parameters based on the values of the gradients is an important part of the deep learning process. Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the *learning rate* (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything.\n\n$$\nw -= w.grad * lr\n$$\n\nThis is known as *stepping* your parameters, using an *optimization* step. If you pick a learning rate that\u2019s too low, it can mean having to do a lot of steps.\n\n<center>\n<img src = \"https:\/\/github.com\/fastai\/fastbook\/raw\/780b76bef3127ce5b64f8230fce60e915a7e0735\/images\/chapter2_small.svg\">\n<\/center>\n\nBut picking a learning rate that's too high is even worse\u2014it can actually result in the loss getting worse.\n\n<center>\n<img src = \"https:\/\/github.com\/fastai\/fastbook\/raw\/780b76bef3127ce5b64f8230fce60e915a7e0735\/images\/chapter2_div.svg\">\n<\/center>\n\nIf the learning rate is too high, it may also \"bounce\" around, rather than actually diverging.\n\n<center>\n<img src = \"https:\/\/github.com\/fastai\/fastbook\/raw\/780b76bef3127ce5b64f8230fce60e915a7e0735\/images\/chapter2_bouncy.svg\">\n<\/center>","c893f274":"<center><h4 class=\"alert alert-danger\">Seven step process<\/h4> <a id=3><\/a><\/center>","28977e8a":"<h3 class=\"alert alert-info\">3. Introduction<\/h3> <a id=3><\/a>\n\n[back to top](#100)","aab756e0":"<h3 class=\"alert alert-info\">2. Packages<\/h3> <a id=2><\/a>\n\n[back to top](#100)","8352ff6e":"<center><h1 class=\"alert alert-warning\">Gradient Descent 101<\/h1><\/center>","ba38af3d":"##### 7. Stop\n\nWe just decided to stop after 10 epochs arbitrarily. In practice, we would watch the training and validation losses and our metrics to decide when to stop, as we\u2019ve discussed.","6e8c26e8":"We must do this a couple of times.","ebc505f7":"##### 1. Initialize the parameters","e9e8c7d7":"##### 5. Step the weights","8bc2c54a":"#### 3.2 Calculating gradients <a id=5><\/a>\n\n[back to top](#100)\n\nTo understand gradients in more details, you may check this out -> [PyTorch 101](https:\/\/www.kaggle.com\/namanmanchanda\/pytorch-101)\n\nCalculating gradients in PyTorch is quite easy.","b4920004":"Now we look to see what would happen if we increased or decreased our parameter by a little bit\u2014the adjustment. This is simply the slope at a particular point.\n\n<center>\n<img src = \"https:\/\/github.com\/fastai\/fastbook\/raw\/780b76bef3127ce5b64f8230fce60e915a7e0735\/images\/grad_illustration.svg\">\n<\/center>\n\nWe can change our weight by a little in the direction of the slope, calculate our loss and adjustment again, and repeat this a few times. Eventually, we will get to the lowest point on our curve.\n\n<center>\n<img src = \"https:\/\/github.com\/fastai\/fastbook\/raw\/780b76bef3127ce5b64f8230fce60e915a7e0735\/images\/chapter2_perfect.svg\">\n<\/center>","26bd1a81":"The sequence of steps described earlier starts with assigning random weights.","3eea1a68":"##### 3. Calculate the loss","38109579":"Random tensor for test","b6539702":"### If you like the notebook, consider giving an upvote. \u270c\ufe0f\n\n[back to top](#100)\n\nCheck out my other notebooks:-\n\n1. https:\/\/www.kaggle.com\/namanmanchanda\/cat-vs-dog-classifier-10-lines-of-code-fast-ai\n2. https:\/\/www.kaggle.com\/namanmanchanda\/star-wars-classifier\n3. https:\/\/www.kaggle.com\/namanmanchanda\/pima-indian-diabetes-eda-and-prediction","0d09f143":"Choosing a loss function.","27b2c925":"##### 4. Calculate the gradients","bf607958":"We want to distinguish clearly between the function\u2019s input and its parameters (the values that define which quadratic we\u2019re trying). So let\u2019s collect the parameters in one argument and thus separate the input, t, and the parameters, params, in the function\u2019s signature.","cebd6b71":"Calculate the loss again - it shows that it has been significantly reduced.","411e9634":"##### 6. Repeat the process","c9000908":"<h3 class=\"alert alert-info\">4. End-to-End gradient descent example<\/h3> <a id=7><\/a>\n\n[back to top](#100)","ecc7ee67":"***The following code and definitions are taken from [Deep Learning for Coders with Fastai and PyTorch](https:\/\/www.amazon.com\/Deep-Learning-Coders-fastai-PyTorch\/dp\/1492045527) book.***\n\nTable of Contents: <a id=100><\/a>\n1. [Downloads](#1)\n2. [Packages](#2)\n3. [Introduction](#3)\n    - 3.1 [Basic example](#4)\n    - 3.2 [Calculating gradiensts](#5)\n    - 3.3 [Stepping with a learning rate](#6)\n4. [End-to-End gradient descent example](#7)","d97d63c8":"`requires_grad_()` is used to tell PyTorch that we want to calculate gradients with respect to this variable at that value.It is essentially tagging the variable, so PyTorch will remember to keep track of how to compute gradients of the other direct calculations on it that you will ask for.","da0ae765":"##### 2. Calculate the predictions","af1a102a":"Machine Learning in words of Arthur Samuel \n>  *Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would \"learn\" from its experience.*\n\nLet's say we have an image `X`, representated as a vector with rank one i.e with all rows stacked up end to end. And we are assuming we have another vector `W` which consists of weights, that are related to our image. Now the aim is to predict the image correcly, but during the training, we must figure out a way to update the weights to make them a little better. With such an approach, we can repeat that step a number of times, making the weights better and better, until they are as good as we can make them. We want to find the specific values for `W` that cause the result of our classification function to be high for correclty classified images and low for incorreclty classified images.\n\nThe following steps can be followed:-\n1. *Initialize* the weights.\n2. For each image, use these weights to *predict* whether the image is correclty classified or not.\n3. Based on these predictions, calculate how good the model is (*its loss*).\n4. Calculate the *gradient*, which measures for each weight how changing that weight would change the loss.\n5. *Step* (that is, change) all the weights based on that calculation.\n6. Go back to step 2 and repeat the process.\n7. Iterate until you decide to *stop* the training process (for instance, because the model is good enough or you don\u2019t want to wait any longer).","b1d92191":"<h3 class=\"alert alert-info\">1. Downloads<\/h3> <a id=1><\/a>\n\n[back to top](#100)","44ad4fdc":"Let\u2019s create a little function to see how close our predictions are to our targets, and take a look.","3747281a":"Plotting the training process","9f1e2412":"Finally, we tell PyTorch to calculate the gradients for us."}}