{"cell_type":{"65fe9ee1":"code","8cdcae3d":"code","ff8d34ba":"code","db3bc603":"code","969a8f70":"code","79c3627d":"code","296b4578":"code","076e810a":"code","4902602e":"code","83bfe523":"code","8ae75a95":"code","7cb1d981":"code","8d8ccc09":"code","f165b980":"code","e71abbb1":"code","779db28b":"code","fd0389eb":"markdown","5d415efa":"markdown","b2adad3c":"markdown","22bc053a":"markdown","7caa226d":"markdown","5e8b3fe2":"markdown","bb2ead5e":"markdown","e5fcf606":"markdown","2d3d203a":"markdown","5105f322":"markdown","b6be22ac":"markdown"},"source":{"65fe9ee1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8cdcae3d":"#Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Feature Engineering\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Model Selection and Metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier, reset_parameter\nfrom sklearn.metrics import f1_score, recall_score, accuracy_score, roc_auc_score, precision_score, auc, roc_curve\n\n# Hyperparamter Tuning\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n","ff8d34ba":"train =pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/train.csv')","db3bc603":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\ntrain.head(500)","969a8f70":"sns.set(style=\"darkgrid\")\nplt.style.use('fivethirtyeight')\n\nplt.subplot(221)\ncolors = ['#1849CA', 'crimson']\nplt.title('Insurance Clients based on Gender',fontsize=15)\ncircle = plt.Circle((0, 0), 0.6, color = 'white')\ntrain['Gender'].value_counts().plot(kind='pie', figsize=(8, 8), rot=1, colors=colors, autopct = '%.2f%%')\np = plt.gcf()\np.gca().add_artist(circle)\nplt.axis('off')\nplt.legend()\n\nplt.subplot(222)\ncolors = ['lightblue', 'crimson', 'pink', '#1849CA']\nexplode = [0, 0.075, 0, 0.075]\nplt.title('Health Insuranced Clients',fontsize=15)\ncircle = plt.Circle((0, 0), 0.6, color = 'white')\nhealth = train[['Gender','Previously_Insured']].values.tolist()\nhealth = pd.DataFrame([h[0] + ' Insured' if h[1] == 1 else h[0] for h in health ],columns=['Gen_Ins'])\nhealth['Gen_Ins'].value_counts().plot(kind='pie', explode=explode, figsize=(16, 16), rot=1, colors=colors, autopct = '%.2f%%')\np = plt.gcf()\np.gca().add_artist(circle)\nplt.axis('off')\nplt.legend()\n\nplt.subplot(223)\ncolors = ['#1849CA', 'crimson']\nplt.title('Vehicle damage based on Gender',fontsize=15)\ncircle = plt.Circle((0, 0), 0.6, color = 'white')\ntrain['Vehicle_Damage'].value_counts().plot(kind='pie',figsize=(16, 16), rot=1, colors=colors, autopct = '%.2f%%')\np = plt.gcf()\np.gca().add_artist(circle)\nplt.axis('off')\nplt.legend()\n\nplt.subplot(224)\ncolors = ['#1849CA', 'pink', 'lightblue', 'crimson']\nexplode = [0.075, 0, 0, 0.075]\nplt.title('Vehicle Insuranced Clients',fontsize=15)\ncircle = plt.Circle((0, 0), 0.6, color = 'white')\nhealth = train[['Gender','Vehicle_Damage']].values.tolist()\nhealth = pd.DataFrame([h[0] + ' Insured' if h[1] == 'Yes' else h[0] for h in health ],columns=['Veh_Ins'])\nhealth['Veh_Ins'].value_counts().plot(kind='pie', explode=explode, figsize=(16, 16), rot=1, colors=colors, autopct = '%.2f%%')\np = plt.gcf()\np.gca().add_artist(circle)\nplt.axis('off')\nplt.legend()\n\n\nplt.show()","79c3627d":"plt.subplot(311)\nplt.title('Age Distribution',fontsize=15)\nmen = train[train['Gender']=='Male']\nwomen = train[train['Gender']=='Female']\na = sns.kdeplot(men['Age'], shade='True', legend='True', label='Male')\nb = sns.kdeplot(women['Age'], shade='True', legend='True', label='Female')\n\nplt.subplot(312)\nplt.title('Health Insured Clients Distribution',fontsize=15)\nhealth = train[['Gender','Previously_Insured','Age']]\nhealth = health[health['Previously_Insured'] == 1]\nmen = health[health['Gender']=='Male']\nwomen = health[health['Gender']=='Female']\na = sns.kdeplot(men['Age'], shade='True', legend='True', label='Male')\nb = sns.kdeplot(women['Age'], shade='True', legend='True', label='Female')\n\nplt.subplot(313)\nplt.title('Vehicle Insured Clients Distribution',fontsize=15)\nhealth = train[['Gender','Response','Age']]\nhealth = health[health['Response'] == 1]\nmen = health[health['Gender']=='Male']\nwomen = health[health['Gender']=='Female']\na = sns.kdeplot(men['Age'], shade='True', legend='True', label='Male')\nb = sns.kdeplot(women['Age'], shade='True', legend='True', label='Female')\n\nplt.tight_layout()\nplt.show()","296b4578":"ax = sns.catplot(data=train, x='Vehicle_Age', hue='Response', col='Vehicle_Damage', kind='count')","076e810a":"plt.subplot(211)\nplt.title('Effect of Annual Premium on Response',fontsize=15)\nax = sns.violinplot(data=train[train['Annual_Premium']<100000], y=\"Annual_Premium\", x=\"Response\")\nplt.subplot(212)\nplt.title('Effect of Vintage on Response',fontsize=15)\nbx = sns.violinplot(data=train, y=\"Vintage\", x=\"Response\")\nplt.tight_layout()\nplt.show()","4902602e":"train['Gender_Code'] = pd.CategoricalIndex(train['Gender']).codes\ntrain['Vehicle_Age_code'] = pd.CategoricalIndex(train['Vehicle_Age']).codes\ntrain['Vehicle_Damage_code'] = pd.CategoricalIndex(train['Vehicle_Damage']).codes ","83bfe523":"model_train = train[['Age', 'Driving_License', 'Region_Code', 'Previously_Insured',\n                   'Policy_Sales_Channel', 'Gender_Code',\n                   'Vehicle_Age_code', 'Vehicle_Damage_code']]\n\nscaler = StandardScaler()\n\nfor param in ['Age',\n              'Driving_License',\n              'Region_Code',\n              'Previously_Insured',\n              'Policy_Sales_Channel',\n              'Gender_Code',\n              'Vehicle_Age_code',\n              'Vehicle_Damage_code']:\n    model_train[param] = scaler.fit_transform(model_train[param].values.reshape(-1, 1))\n    \nX_train, X_test, y_train, y_test = train_test_split(model_train, train['Response'], test_size = 0.2, shuffle = True)","8ae75a95":"model = {\n    \"Decision Tree\": DecisionTreeClassifier(), \n    \"SGD\" : SGDClassifier(), \n    \"Random Forest\" : RandomForestClassifier(), \n    \"Gradient Boosting\" : GradientBoostingClassifier(),\n    \"XGBoost\" : XGBClassifier(),\n    \"CatBoost\" : CatBoostClassifier(),\n    \"LGBM\" : LGBMClassifier()\n        }\n","7cb1d981":"scores = []\nprob_score = {}\nfor mod in model:\n    classifier = model[mod]\n    classifier.fit(X_train, y_train)\n    pred = classifier.predict(X_test)\n    try:\n        score = classifier.predict_proba(X_test)[:,1]\n        roc = roc_auc_score(y_test, score, average='weighted')\n        prob_score[mod] = score\n    except:\n        roc = 0\n    scores.append([\n        mod,\n        accuracy_score(y_test, pred),\n        f1_score(y_test, pred, average='weighted'),\n        precision_score(y_test, pred, average='weighted'),\n        recall_score(y_test, pred, average='weighted'),\n        roc\n    ])","8d8ccc09":"def highlight_max(s):\n    is_max = s == s.max()\n    return ['background-color: yellow' if v else '' for v in is_max]\n\nscores_df  = pd.DataFrame(scores)\nindex_model = {count: s for count, s in enumerate(scores_df[0])}\ncol = {count+1: s for count, s in enumerate(['Accuracy','F1 Score','Precision','Recall','ROC AUC'])}\nscores_df = scores_df.drop(0, axis=1)\nscores_df = scores_df.rename(columns=col, index=index_model)\nscores_df.style.apply(highlight_max)","f165b980":"plt.title('ROC Curves of Classifiers')\nplt.xlabel('Precision')\nplt.ylabel('Recall')\n\nfor key in prob_score:\n    fpr, tpr, _ = roc_curve(y_test, prob_score[key])\n    plt.plot(fpr, tpr, label=key)\n\nplt.plot((0,1), ls='dashed',color='black')\nplt.legend()\nplt.show()","e71abbb1":"param_test ={'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n\nfit_params={\"early_stopping_rounds\":30, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_test,y_test)],\n            'eval_names': ['valid'],\n            'verbose': 100,\n            'categorical_feature': 'auto'}\n\n\n\nclf = LGBMClassifier(max_depth=-1, random_state=15, silent=True, metric='None', n_jobs=4, n_estimators=5000)\ngs = RandomizedSearchCV(\n    estimator=clf, param_distributions=param_test, \n    n_iter=100,\n    scoring='roc_auc',\n    cv=3,\n    refit=True,\n    random_state=15,\n    verbose=True)\n\n\n# Uncomment to perform Randomsearch\n# gs.fit(X_train, y_train, **fit_params)\n# print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))\n\nRandomsearch_params = {'colsample_bytree': 0.6261473679815167, 'min_child_samples': 237, 'min_child_weight': 0.001, 'num_leaves': 28, 'reg_alpha': 10, 'reg_lambda': 10, 'subsample': 0.7567691135431514} ","779db28b":"def learning_rate_010_decay_power_0995(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\n#set optimal parameters\nclf_sw = LGBMClassifier(**clf.get_params())\nclf_sw.set_params(**Randomsearch_params)\nclf_sw.fit(X_train,y_train, **fit_params, callbacks=[reset_parameter(learning_rate=learning_rate_010_decay_power_0995)])","fd0389eb":"# Conclusion\n\n## The ROC AUC score using LGBM is around 0.86\n\n","5d415efa":"### Vehicle Damage is clearly related to Response, clients whose vehicle never damaged didn't opt for Vehicle Insurance","b2adad3c":"<a id=\"section-four\"><\/a>\n# Hyperparameter Tuning\n\n### Hyperparameters are important because they directly control the behaviour of the training algorithm and have a significant impact on the performance of the model is being trained. \n\n\n> ### We select **LGBM** Model for hypertuning because it has highest ROC Score","22bc053a":"<center><h1> Vehicle Insurance Prediction \ud83d\ude98 \ud83d\ude96 <\/h1>\n    <h2> Ensemble Boosting Models <\/h2>\n<img src=\"https:\/\/doctorins.com\/wp-content\/uploads\/2017\/12\/auto-insurance-banner.jpg\" width=\"1000\" >\n<\/center>\n\n<br><br>\n\n\n<h3>Navigate to<h3>\n    \n* [Problem Description](#section-zero)\n* [EDA and Visualization](#section-one)\n* [Feature Engineering](#section-two)\n* [Model Selection](#section-three)\n* [Hyperparameter Tuning](#section-four)\n* [Model Training](#section-five)","7caa226d":"## Importing Necessary Libraries","5e8b3fe2":"<a id=\"section-three\"><\/a>\n# Model Selection \n\n\n    \n* Problem can be identified as Binary Classification (wheather customer opts for vehicle insurance or not)\n* Dataset has more than 300k records\n* cannot go with SVM Classifier as it takes more time to train as dataset increase\n* Idea is to start selection of models as:\n\n    1. Decision Tree\n    2. Random Forest\n    3. SGD\n    4. Gradient Boost\n    5. XG Boost\n    6. Cat Boost\n    7. LGBM\n    \n\n    \n    \n    \n## Boosting Models\n\n### Boosting is one of the techniques that uses the concept of ensemble learning. A boosting algorithm combines multiple simple models","bb2ead5e":"### Age is definitely affecting the Response as we can observe in below plots","e5fcf606":"<a id=\"section-two\"><\/a>\n# Feature Engineering\n\n* Need to encode categorical data to integers\n* We will Drop of Vintage and Annual Premium as we observe their impact on Response is not encouraging\n* Need to scale all parameters which makes easy for algorith to reach minima","2d3d203a":"<a id=\"section-one\"><\/a>\n# EDA and Visualization\n","5105f322":"### Annual Premium and Vintage effect is very less on Vehicle Insurance Response","b6be22ac":"<a id=\"section-five\"><\/a>\n# Model Training\n"}}