{"cell_type":{"cac59d41":"code","c547c768":"code","1cac2025":"code","cce42802":"code","fd8eed66":"code","562f0644":"code","975adcd7":"code","11359d90":"code","9c13bb51":"code","7e069fea":"code","d8d5848c":"code","710be9bc":"code","81aa8e59":"code","6468ed77":"code","52ac4e62":"code","2ee2c711":"code","2bc53f77":"code","0b054d0f":"code","11873017":"code","bdf2e8f9":"code","e0e731e6":"code","84ffbfd0":"code","021634c6":"code","fc4e54ef":"code","cd65921e":"code","42a0099e":"code","232d0ab8":"code","5559cf29":"code","5e48bffb":"code","dfe0956e":"code","55568c0a":"code","5635b517":"code","be3cfc73":"code","044f66d6":"code","73981d07":"markdown","af89917f":"markdown","f336f61a":"markdown","6bd36406":"markdown","afc05ebd":"markdown","c89063ca":"markdown","16042f20":"markdown","371480ad":"markdown","05f4439a":"markdown","ebcefe7d":"markdown","dbdfda6d":"markdown","b18dddcd":"markdown","b42d0b52":"markdown","55f82017":"markdown","f79e4f03":"markdown"},"source":{"cac59d41":"# top level params\n\n# Use partial data (e.g. to speed up testing)\n# value = 0 means use everything\nTRAIN_N = 0\nTEST_N = 0\n\n# Training parameter\nN_EPOCH = 100\nTIMEOUT = 8.5 * 3600 # 8.5 hours\nBATCH_SIZE = 256\n\n# train\/test split size\nTEST_SIZE = 0.1\n\n# number of models&cv in ensemble\nCV = 10\n\n# for reproducibility\nSEED = 17\n\n# optionally disable tqdm on long-running operations\n# since somehow it makes committing stuck ...\nDISABLE_TQDM=True\n\n# # Testing params\n# TRAIN_N = 30000\n# TEST_N = 5000\n# TIMEOUT = 15*60 # 15 minutes","c547c768":"import os\nimport random\nimport re\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport plotly.graph_objs as go\nimport plotly\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n\nset_seed(SEED)","1cac2025":"INPUT_PATH = '.'\n\n# check if inside kaggle kernels\nif os.path.exists('\/kaggle\/input\/scl-2021-ds'):\n    INPUT_PATH = '\/kaggle\/input\/scl-2021-ds'\nelse:\n    # check if inside colab\n    try:\n        from google.colab import drive\n        drive.mount('\/content\/drive')\n\n        %cd \"\/content\/drive\/MyDrive\/Workspace\/Shopee Code League 2021\/DS\/spacy-val-v6\"\n    except ModuleNotFoundError:\n        pass","cce42802":"!pip install -q spacy-lookups-data parse\n!python -m spacy validate","fd8eed66":"train = pd.read_csv(INPUT_PATH+'\/train.csv')\ntrain[['poi', 'street']] = train['POI\/street'].str.split('\/', 1, expand=True)\n\nif TRAIN_N > 0:\n    train = train[:TRAIN_N]\n\ntrain","562f0644":"import spacy\nfrom spacy.lang.char_classes import LIST_ELLIPSES, LIST_ICONS, LIST_PUNCT, LIST_QUOTES, LIST_CURRENCY\nfrom spacy.lang.char_classes import CURRENCY, PUNCT, CONCAT_QUOTES, ALPHA_LOWER, ALPHA_UPPER\nfrom spacy.symbols import ORTH\nfrom spacy.util import compile_infix_regex, compile_prefix_regex, compile_suffix_regex\n\n# From here:\n# https:\/\/github.com\/explosion\/spaCy\/blob\/master\/spacy\/lang\/punctuation.py\ndef create_tokenizer(nlp):\n    # Modify tokenizer patterns.\n    # Basically we want to break everything.\n    breaks = (\n        LIST_PUNCT\n        + LIST_ELLIPSES\n        + LIST_QUOTES\n        + LIST_CURRENCY\n        + LIST_ICONS\n        + [r'\\.', r'\\-', r'@', r'\\\/']\n    )\n\n    infixes = breaks\n\n    suffixes = (\n        LIST_PUNCT\n        + LIST_ELLIPSES\n        + LIST_QUOTES\n        + LIST_ICONS\n        + [\"'s\", \"'S\", \"\u2019s\", \"\u2019S\", \"\u2014\", \"\u2013\"]\n        + [\n            r\"(?<=[0-9])\\+\",\n            r\"(?<=\u00b0[FfCcKk])\\.\",\n            r\"(?<=[0-9])(?:{c})\".format(c=CURRENCY),\n            # r\"(?<=[0-9])(?:{u})\".format(u=UNITS),\n            r\"(?<=[0-9{al}{e}{p}(?:{q})])\\.\".format(\n                al=ALPHA_LOWER, e=r\"%\u00b2\\-\\+\", q=CONCAT_QUOTES, p=PUNCT\n            ),\n            r\"(?<=[{au}][{au}])\\.\".format(au=ALPHA_UPPER),\n        ]\n    )\n\n    prefixes = (\n        breaks\n        + [\n            # prefix noXXX, rtXXX, rwXXX\n            r\"(?:no|rt|rw)(?=[0-9])\",\n        ]\n    )\n\n    nlp.tokenizer.infix_finditer = compile_infix_regex(infixes).finditer\n    nlp.tokenizer.prefix_search = compile_prefix_regex(prefixes).search\n    nlp.tokenizer.suffix_search = compile_suffix_regex(suffixes).search\n    # token_match will match nothing\n    nlp.tokenizer.token_match = re.compile('a^').match\n    # url_match in v2.3, i.e. Kaggle Kernels\n    if spacy.__version__.startswith('2.3'):\n       nlp.tokenizer.url_match = re.compile('a^').match\n\n    nlp.tokenizer.rules = {}\n    nlp.tokenizer.add_special_case(\"'s\", [{ORTH: \"'s\"}])\n\n    return nlp.tokenizer\n\nnlp = spacy.blank('id')\n\ntokenizer = create_tokenizer(nlp)\n\ntokenizer.explain(\"jl. raya i. no 19 no19 nomor10 n19 no19a rt1 rw3 rt01\/03 t-sh t-shirt a.b. s.h. a...b 's marcy's place( di depan indomaret)kel. jati ab.cde unit20#02,jln .a @here 702t\")\n# tokenizer.explain('ant bank cent asia 702t')","975adcd7":"from spacy.tokens.doc import Doc\n\nNGRAMS_ALIGNMENT = (1, 5)\n\n# The following function returns inf if the original is not the prefix,\n# and returns the length difference otherwise\ndef diff_cost(orig, label):\n    if not label.startswith(orig):\n        return float('inf')\n    return len(label) - len(orig)\n\n# We find the alignment using DP similar to Levenhstein distance, but we only allow insertion from source\ndef find_alignment(text, label, full=False, ngrams=NGRAMS_ALIGNMENT, return_span_only=False, occupied=(0,0), verbose=False):\n    if len(label) == 0:\n        if return_span_only:\n            return (0, 0)\n        else:\n            return (text, [])\n\n    dt = nlp(text)\n    dl = nlp(label)\n\n    nt = len(dt)\n    nl = len(dl)\n\n    dp = np.zeros((nt+1, nl+1))\n    dps = np.zeros((nt+1, nl+1))\n    dpa = [[[] for i in range(nl+1)] for j in range(nt+1)]\n\n    for il in range(1, nl+1):\n        dp[0][il] = float('inf')\n    if full:\n        for it in range(1, nt+1):\n            dp[it][0] = float('inf')\n    \n    occupied_it = occupied\n    for it in range(nt):\n        if dt[it:it+1].start_char == occupied[0]:\n            occupied_it = (it, occupied_it[1])\n        if dt[it:it+1].end_char == occupied[1]:\n            occupied_it = (occupied_it[0], it+1)\n\n    for it in range(1, nt+1):\n        for il in range(1, nl+1):\n            # TODO: should I compare whitespace?\n            if dt[it-1].text == dl[il-1].text:\n                dp[it][il] = dp[it-1][il-1]\n                dps[it][il] = dps[it-1][il-1] if il > 1 else it\n                dpa[it][il] = dpa[it-1][il-1] + [il-1]\n            else:\n                dp[it][il] = float('inf')\n                dps[it][il] = -1\n\n                # First token must not be magically inserted\n                if il > 1:\n                    # penalize a full word insertion\n                    dp[it][il] = dp[it][il-1] + 10 * len(dl[il-1].text)\n                    dps[it][il] = dps[it][il-1]\n                    dpa[it][il] = dpa[it][il-1].copy()\n\n                cost_change = dp[it-1][il-1] + diff_cost(dt[it-1].text, dl[il-1].text)\n                if cost_change < dp[it][il]:\n                    dp[it][il] = cost_change\n                    dps[it][il] = dps[it-1][il-1] if il > 1 else it\n                    dpa[it][il] = dpa[it-1][il-1] + [il-1]\n\n    # treat only best alignments based on the starting positions\n    best_alignment_by_start = np.zeros(nt+1).astype('int')\n    best_alignment_by_start_cost = np.full((nt+1,), float('inf'))\n    for it in range(1, nt+1):\n        start = int(dps[it][nl])\n        cost = dp[it][nl]\n        if cost < best_alignment_by_start_cost[start]:\n            best_alignment_by_start_cost[start] = cost\n            best_alignment_by_start[start] = it\n\n    best_span = (0, 0)\n    best_cost_no_restriction = float('inf')\n    best_cost = float('inf')\n    best_alignment = []\n    for it in range(1, nt+1):\n        cost = best_alignment_by_start_cost[it]\n        if cost <= best_cost_no_restriction:\n            best_cost_no_restriction = cost\n            span = (it-1, best_alignment_by_start[it])\n            # only assign on non-conflicting spans\n            if span[1] <= occupied_it[0] or occupied_it[1] <= span[0]:\n                if cost < best_cost:\n                    best_cost = cost\n                    best_span = span\n                    best_alignment = dpa[span[1]][nl]\n\n    if full:\n        assert(dps[nt][nl] == 1)\n        best_cost = dp[nt][nl]\n        best_span = (0, nt)\n        best_alignment = dpa[nt][nl]\n\n    best_alignment += [nl]\n    \n    # assert(best_cost < 1000)\n    if best_cost > 1000:\n        if verbose:\n            print('cost > 1000!')\n            print('text:', text)\n            print('label:', label)\n            print('text tokens:', [t.text for t in dt])\n            print('label tokens:', [t.text for t in dl])\n            print('occupied:', occupied, ':', occupied_it)\n            print('best cost:', best_cost)\n            print('best span:', best_span)\n            print(dp)\n            print(dps)\n            print()\n\n        if return_span_only:\n            return (0, 0)\n\n        return text, []\n\n    if verbose:\n        print('text:', text)\n        print('label:', label)\n        print('text tokens:', [t.text for t in dt])\n        print('label tokens:', [t.text for t in dl])\n        print('occupied:', occupied, ':', occupied_it)\n        print('best cost:', best_cost)\n        print('best span:', best_span)\n        print('best alignment:', dt[best_span[0]:best_span[1]])\n        print('best alignment:', best_alignment)\n        print(dp)\n        print(dps)\n    \n    if return_span_only:\n        span = dt[best_span[0]:best_span[1]]\n        return span.start_char, span.end_char\n\n    words = [t.text for t in dt]\n    spaces = [t.whitespace_ != '' for t in dt]\n    for i in range(best_span[1]-best_span[0]):\n        words[best_span[0]+i] = dl[best_alignment[i]:best_alignment[i+1]].text\n\n    alignments = []\n    for ngram in range(ngrams[0], ngrams[1]+1):\n        for i in range(nt-ngram+1):\n            wt = dt[i:i+ngram].text\n            wl = Doc(nlp.vocab, words=words[i:i+ngram], spaces=spaces[i:i+ngram-1]+[False]).text\n\n            # We include alignments for matching tokens, for duplicates detection.\n            # But we do not allow alignments starting with non-alphanumeric,\n            # because the resulting mappings become a bit ... chaotic.\n            if re.match(\"^\\w\", wt) or full:\n                alignments.append((wt, wl))\n\n    if verbose:\n        print(alignments)\n        print()\n\n    aligned = Doc(nlp.vocab, words=words, spaces=spaces)\n\n    return (aligned.text, alignments)\n\n# raw_address = \"cluster visana @the savia blok k3 no. 1. nusaloka sektor 14, 6, jl. ambon, bsd city\"\n# label = \"the savia\"\n# find_alignment(raw_address, label, verbose=True)\n\n# # for applied mappings later on, we use full=True\n# find_alignment(\"batas sub karawang-p\", \"batas sub karawang-pamanukan\", full=True, ngrams=(1,1), verbose=True)\n\n# raw_address = \"thaitea nyot nyot prap, prap no 9\"\n# label = \"thaitea nyot nyot prapatan\/prap\".split(\"\/\")\n# raw_address = \"dadap no 2-45 dadap dadap kosambi\"\n# label = \"dadap\/dadap\".split(\"\/\")\n# raw_address = \"jalan sentanu iii gg. ii no. 22 ayani utara, peguyangan kaja\"\n# label = \"sentanu iii\/jalan sentanu iii\".split(\"\/\")\nraw_address = \"jl.be 0 rt 6 32 sukoharjo ngaglik\"\nlabel = \"\/jl.besi-j\".split(\"\/\")\n\npoi_span = find_alignment(raw_address, label[0], return_span_only=True, verbose=True)\nstreet_span = find_alignment(raw_address, label[1], return_span_only=True, verbose=True, occupied=poi_span)","11359d90":"def row_to_entities(row):\n    # We find POI first, as it is more often POI appear in front of street\n    if row.poi == '':\n        poi_span = (0, 0)\n    else:\n        poi_span = find_alignment(row.raw_address, row.poi, return_span_only=True)\n    if row.street == '':\n        street_span = (0, 0)\n    else:\n        street_span = find_alignment(row.raw_address, row.street, return_span_only=True, occupied=poi_span)\n        # this will return (0, 0) on conflicts. on that case, retry with not setting occupied.\n        if street_span == (0, 0):\n            street_span = find_alignment(row.raw_address, row.street, return_span_only=True)\n            # then we check if reversing the order is more appropriate\n            if row.poi != '':\n                secondary_poi_span = find_alignment(row.raw_address, row.poi, return_span_only=True, occupied=street_span)\n                if secondary_poi_span != (0, 0):\n                    poi_span = secondary_poi_span\n\n    entities = []\n    if poi_span != (0, 0):\n        entities.append(poi_span + (\"POI\",))\n    if street_span != (0, 0):\n        entities.append(street_span + (\"STREET\",))\n    # sort the order\n    if len(entities) == 2 and entities[1][0] < entities[0][0]:\n        entities.reverse()\n\n    return {'entities': entities}\n\ndef entities_overlap(ent):\n    ent = ent['entities']\n    if len(ent) < 2:\n        return False\n    if ent[0][1] <= ent[1][0] or ent[1][1] <= ent[0][0]:\n        return False\n    return True","9c13bb51":"import json\n\ndef save_train_entities(train=train):\n    df = train.copy()\n    df.entities = df.entities.apply(lambda x: json.dumps(x, separators=(',', ':')))\n    df.to_csv('train_entities.csv', index=False)\n\ndef json_to_entities(dump):\n    ent = json.loads(dump)\n    return {'entities': [tuple(item) for item in ent['entities']]}\n\ndef load_train_entities():\n    df = pd.read_csv('train_entities.csv').fillna('')\n    df.entities = df.entities.apply(json_to_entities)\n    return df","7e069fea":"%%time\n\ntrain['entities'] = train.apply(row_to_entities, axis=1)\ntrain['overlap'] = train.entities.apply(entities_overlap)\nsave_train_entities()\n!head train_entities.csv","d8d5848c":"train[train.overlap]","710be9bc":"from sklearn.model_selection import train_test_split\n\n# tuple (address, entities, label)\nTRAIN_DATA = train.apply(lambda row: (row.raw_address, row.entities, row['POI\/street']), axis=1)\n\n# Drop training data having overlapping entities\nnonoverlapping_index = TRAIN_DATA.apply(lambda item: not entities_overlap(item[1]))\nTRAIN_DATA = TRAIN_DATA[nonoverlapping_index]\nTRAIN_DATA.reset_index(drop=True, inplace=True)\n\n# for reverse lookup for OOF\ntrain_index = np.where(nonoverlapping_index)[0]\n\nprint('train:', len(TRAIN_DATA))","81aa8e59":"TRAIN_DATA[:10].values","6468ed77":"from spacy.util import minibatch, compounding, decaying\nfrom spacy.gold import GoldParse\nfrom spacy.scorer import Scorer\nfrom sklearn.metrics import accuracy_score\n\nmetrics_names = ['accuracy', 'accuracy_poi', 'accuracy_street',\n                 'raw_accuracy', 'raw_accuracy_poi', 'raw_accuracy_street',\n                 'precision', 'precision_poi', 'precision_street',\n                 'recall', 'recall_poi', 'recall_street',\n                 'f1', 'f1_poi', 'f1_street']\n\ndef print_evaluation_metrics(metrics, prefix=''):\n    print('{}Validation Accuracy (real) : {:.4f} (POI\/STREET: {:.4f}\/{:.4f})'.format(prefix, metrics['accuracy'], metrics['accuracy_poi'], metrics['accuracy_street']))\n    print('{}           Accuracy (raw)  : {:.4f} (POI\/STREET: {:.4f}\/{:.4f})'.format(prefix, metrics['raw_accuracy'], metrics['raw_accuracy_poi'], metrics['raw_accuracy_street']))\n    print('{}           Precision       : {:.4f} (POI\/STREET: {:.4f}\/{:.4f})'.format(prefix, metrics['precision'], metrics['precision_poi'], metrics['precision_street']))\n    print('{}           Recall          : {:.4f} (POI\/STREET: {:.4f}\/{:.4f})'.format(prefix, metrics['recall'], metrics['recall_poi'], metrics['recall_street']))\n    print('{}           F1-score        : {:.4f} (POI\/STREET: {:.4f}\/{:.4f})'.format(prefix, metrics['f1'], metrics['f1_poi'], metrics['f1_street']))\n\ndef doc_to_pred(doc):\n    street = ''\n    poi = ''\n    for ent in doc.ents:\n        if ent.label_ == \"POI\" and poi == '':\n            poi = ent.text\n        if ent.label_ == \"STREET\" and street == '':\n            street = ent.text\n    return \"{}\/{}\".format(poi, street)\n\ndef evaluate(model, valid_data, batch_size=BATCH_SIZE, prefix='', return_prediction=False):\n    texts = [t for t, _, l in valid_data]\n    labels = [l for t, _, l in valid_data]\n    preds = []\n\n    scorer = Scorer()\n\n    time.sleep(0.5)\n    pbar = tqdm(zip(model.pipe(texts, batch_size=batch_size), valid_data), total=len(valid_data), disable=DISABLE_TQDM)\n    for doc, item in pbar:\n        text, annotation, label = item\n\n        pred = model(text)\n\n        if not entities_overlap(annotation):\n            y = GoldParse(model.make_doc(text), entities=annotation['entities'])\n            scorer.score(pred, y)\n\n        preds.append(doc_to_pred(pred))\n\n        pbar.set_description('{}Valid eval'.format(prefix))\n    \n    labels_poi = []\n    labels_street = []\n    preds_poi = []\n    preds_street = []\n    for label, pred in zip(labels, preds):\n        label_poi, label_street = label.split('\/')\n        pred_poi, pred_street = pred.split('\/')\n        labels_poi.append(label_poi)\n        labels_street.append(label_street)\n        preds_poi.append(pred_poi)\n        preds_street.append(pred_street)\n\n    labels_poi_raw = []\n    labels_street_raw = []\n    labels_raw = []\n    for text, entities, _ in valid_data:\n        poi = ''\n        street = ''\n        for entity in entities['entities']:\n            if entity[2] == 'POI':\n                poi = text[entity[0]:entity[1]]\n            if entity[2] == 'STREET':\n                street = text[entity[0]:entity[1]]\n        labels_poi_raw.append(poi)\n        labels_street_raw.append(street)\n        labels_raw.append('{}\/{}'.format(poi, street))\n\n    accuracy = accuracy_score(labels, preds)\n    accuracy_poi = accuracy_score(labels_poi, preds_poi)\n    accuracy_street = accuracy_score(labels_street, preds_street)\n\n    raw_accuracy = accuracy_score(labels_raw, preds)\n    raw_accuracy_poi = accuracy_score(labels_poi_raw, preds_poi)\n    raw_accuracy_street = accuracy_score(labels_street_raw, preds_street)\n\n    precision = scorer.scores['ents_p'] \/ 100\n    precision_poi = scorer.scores['ents_per_type']['POI']['p'] \/ 100\n    precision_street = scorer.scores['ents_per_type']['STREET']['p'] \/ 100\n\n    recall = scorer.scores['ents_r'] \/ 100\n    recall_poi = scorer.scores['ents_per_type']['POI']['r'] \/ 100\n    recall_street = scorer.scores['ents_per_type']['STREET']['r'] \/ 100\n\n    f1 = scorer.scores['ents_f'] \/ 100\n    f1_poi = scorer.scores['ents_per_type']['POI']['f'] \/ 100\n    f1_street = scorer.scores['ents_per_type']['STREET']['f'] \/ 100\n\n    scores = {\n        'accuracy': accuracy,\n        'accuracy_poi': accuracy_poi,\n        'accuracy_street': accuracy_street,\n\n        'raw_accuracy': raw_accuracy,\n        'raw_accuracy_poi': raw_accuracy_poi,\n        'raw_accuracy_street': raw_accuracy_street,\n\n        'precision': precision,\n        'precision_poi': precision_poi,\n        'precision_street': precision_street,\n\n        'recall': recall,\n        'recall_poi': recall_poi,\n        'recall_street': recall_street,\n\n        'f1': f1,\n        'f1_poi': f1_poi,\n        'f1_street': f1_street,\n    }\n\n    print_evaluation_metrics(scores, prefix=prefix)\n\n    if return_prediction:\n         return preds, scores\n    else:\n        return scores\n\ndef plot_train_metrics(metrics):\n    fig = go.Figure()\n    for label, values in metrics.items():\n        if label == 'epoch':\n            continue\n        visible = label in ['accuracy', 'raw_accuracy', 'f1', 'loss']\n        fig.add_trace(go.Scatter(x=metrics['epoch'], y=values, mode='lines+markers', name=label, visible=True if visible else 'legendonly'))\n\n    fig.update_layout(xaxis_title='Epoch', hovermode='x')\n    fig.show()\n\ndef train_spacy(data,\n                iterations=10,\n                dropout=0.2,\n                valid_data=None,\n                valid_metrics='f1',\n                patience=5,\n                learn_rate=3e-4,\n                learn_rate_bootstrap=1e-3,\n                bootstrap_iterations=10,\n                learn_rate_patience=1,\n                learn_rate_decay=0.1**(1\/5), #0.1 every 5 epochs\n                min_learn_rate=3e-6,\n                timeout=0,\n                prev_iterations=0,\n                prev_model=None,\n                plot=False,\n                filename=None):\n    t_start = time.time()\n    \n    # at this point we can't continue without valid_data\n    assert(valid_data is not None)\n    assert(valid_metrics in metrics_names)\n\n    # drop label from data\n    data = data.apply(lambda row: (row[0], row[1]))\n\n    if prev_model is not None:\n        assert(prev_iterations > 0)\n        model = prev_model\n    else:\n        assert(prev_iterations == 0)\n        model = spacy.blank('id')\n        create_tokenizer(model)\n\n        ner = model.create_pipe('ner')\n        model.add_pipe(ner, last=True)\n\n        # add labels\n        for _, annotations in data:\n            for ent in annotations['entities']:\n                ner.add_label(ent[2])\n    \n    # dropout = decaying(0.8, 0.2, 1e-6) #minimum, max, decay rate\n    if prev_iterations > 0:\n        sizes = BATCH_SIZE\n    else:\n        sizes = compounding(16, BATCH_SIZE, 1.001)\n\n    best_epoch = -1\n    best_valid_metrics = {}\n    for metrics in metrics_names:\n        best_valid_metrics[metrics] = 0\n    best_model_name = ''\n\n    all_valid_metrics = {}\n    for metrics in metrics_names + ['loss', 'epoch']:\n        all_valid_metrics[metrics] = []\n\n    other_pipes = [pipe for pipe in model.pipe_names if pipe != 'ner']\n    with model.disable_pipes(*other_pipes):\n        if prev_iterations > 0:\n            print('Resuming training ({}+{} epochs, timeout={}s)'.format(prev_iterations, iterations, timeout))\n            optimizer = model.resume_training()\n        else:\n            print('Beginning training ({} epochs, timeout={}s)'.format(iterations, timeout))\n            optimizer = model.begin_training()\n\n        if prev_iterations < bootstrap_iterations:\n            optimizer.learn_rate = learn_rate_bootstrap\n            print('Learning rate: {:.1e} (bootstrap)'.format(optimizer.learn_rate))\n        else:\n            optimizer.learn_rate = learn_rate\n            print('Learning rate: {:.1e}'.format(optimizer.learn_rate))\n\n        print('Using validation metrics: {}'.format(valid_metrics))\n\n        for epoch in range(prev_iterations+1, prev_iterations + iterations+1):\n            t_epoch_start = time.time()\n            print() # spacing\n\n            if epoch > bootstrap_iterations and optimizer.learn_rate > learn_rate:\n                optimizer.learn_rate = learn_rate\n                print('(Epoch {}) Bootstrap done, changing learning rate to {:.1e}'.format(epoch, optimizer.learn_rate))\n\n            np.random.shuffle(data)\n            losses = {}\n\n            # update model for this epoch\n            batches = minibatch(data, size=sizes)\n            time.sleep(0.5)\n            pbar = tqdm(batches, disable=DISABLE_TQDM)\n            total_data_processed = 0\n            for batch in pbar:\n                texts, annotations = zip(*batch)\n                model.update(texts, annotations, sgd=optimizer, drop=dropout, losses=losses)\n\n                current_batch_size = len(texts)\n                total_data_processed += current_batch_size\n\n                pbar.total = pbar.n + 1 + (len(data)-total_data_processed + current_batch_size-1) \/\/ current_batch_size\n                pbar.set_description_str(\"(Epoch {}) Train loss: {:.4f}\".format(epoch, losses['ner']\/total_data_processed))\n            \n            assert(total_data_processed == len(data))\n\n            print('(Epoch {}) Training Loss: {:.4f}'.format(epoch, losses['ner']\/len(data)))\n\n            # evaluate validation\n            scores = evaluate(model, valid_data, prefix='(Epoch {}) '.format(epoch))\n\n            for metrics in metrics_names:\n                all_valid_metrics[metrics].append(scores[metrics])\n            all_valid_metrics['loss'].append(losses['ner']\/len(data))\n            all_valid_metrics['epoch'].append(epoch)\n\n            # save best model\n            if scores[valid_metrics] > best_valid_metrics[valid_metrics]:\n                best_epoch = epoch\n                best_valid_metrics = scores\n                best_valid_metrics['epoch'] = epoch\n\n                prev_best_model_name = best_model_name\n                best_model_name = '{}_epoch_{}_loss_{:.0f}_valid_{}_{:.4f}'.format(filename, best_epoch, losses['ner'], valid_metrics, best_valid_metrics[valid_metrics])\n                model.to_disk(best_model_name)\n                # delete previous saved model so disk won't be too polluted\n                if prev_best_model_name != '':\n                    !rm -rf $prev_best_model_name\n\n            print('(Epoch {}) Best validation {}: {:.4f} at epoch {}'.format(epoch, valid_metrics, best_valid_metrics[valid_metrics], best_epoch))\n\n            if epoch-best_epoch >= patience:\n                # early stop\n                print('(Epoch {}) validation {} not improved for {} epochs, early stopping'.format(epoch, valid_metrics, epoch-best_epoch))\n\n                if plot:\n                    plot_train_metrics(all_valid_metrics)\n                assert(best_model_name != '')\n                model = spacy.load(best_model_name)\n                return model, best_valid_metrics\n\n            # reduce rl on plateau\n            if epoch-best_epoch >= learn_rate_patience:\n                optimizer.learn_rate *= learn_rate_decay\n                if optimizer.learn_rate < min_learn_rate:\n                    optimizer.learn_rate = min_learn_rate\n                    print('(Epoch {}) Validation {} not improved for {} epochs, learn_rate already at min value of {:.1e}'.format(epoch, valid_metrics, epoch-best_epoch, optimizer.learn_rate))\n                else:\n                    print('(Epoch {}) Validation {} not improved for {} epochs, reducing learn rate to {:.1e}'.format(epoch, valid_metrics, epoch-best_epoch, optimizer.learn_rate))\n\n            # log training time\n            t_epoch_end = time.time()\n            t_epoch_elapsed = t_epoch_end - t_epoch_start\n            t_elapsed = t_epoch_end - t_start\n            print('(Epoch {}) Elapsed time: {:.1f}s (total), {:.1f}s (current epoch)'.format(epoch, t_elapsed, t_epoch_elapsed))\n\n            # stop when timeout\n            if timeout > 0:\n                if t_elapsed > timeout:\n                    print('(Epoch {}) Timeout of {} seconds has passed, stopping training'.format(epoch, timeout))\n\n                    if plot:\n                        plot_train_metrics(all_valid_metrics)\n                    assert(best_model_name != '')\n                    model = spacy.load(best_model_name)\n                    return model, best_valid_metrics\n\n    if plot:\n        plot_train_metrics(all_valid_metrics)\n    assert(best_model_name != '')\n    model = spacy.load(best_model_name)\n    return model, best_valid_metrics","52ac4e62":"%%time\n\nimport parse\nfrom sklearn.utils import Bunch\n\nmodels_meta = []\n\nfor root, dirs, files in os.walk(\"\/kaggle\/input\", topdown=False):\n    for dir in dirs:\n        parsed = parse.parse('spacy_v6_seed{}_fold{}_epoch_{}_loss_{}_valid_f1_{}', dir)\n        if parsed is not None:\n            models_meta.append(Bunch(path=os.path.join(root, dir),\n                                     seed=int(parsed[0]),\n                                     fold=int(parsed[1]),\n                                     epoch=int(parsed[2]),\n                                     loss=float(parsed[3]),\n                                     f1=float(parsed[4])))\n\nmodels_meta = sorted(models_meta, key=lambda x: x.fold)\n\nfor meta in models_meta:\n    print(meta)\nprint()\nprint('Found {} models (out of {} CV)'.format(len(models_meta), CV))\n\nmodels = []\n\nfor i, meta in enumerate(models_meta):\n    assert(meta.fold == i+1) # make sure that we got the correct fold\n\n    model = spacy.load(meta.path)\n    print('Model for fold ({}\/{}) loaded'.format(meta.fold, CV))\n    models.append(model)","2ee2c711":"%%time\n\nfrom sklearn.model_selection import KFold\n\ncv_seed = SEED\ncv_n_splits = CV\n\ncv = KFold(n_splits=10, random_state=cv_seed, shuffle=True)\n\nprint('Ensemble validation performance summary:')\n\ntrain['oof'] = np.full((len(train)), '')\n\nfor cv_fold, model, (train_cv_index, val_cv_index) in zip(range(10), models, cv.split(TRAIN_DATA)):\n    if cv_fold == cv_n_splits:\n        break\n\n    VALID_CV = TRAIN_DATA.iloc[val_cv_index].reset_index(drop=True)\n\n    print()\n    val_cv_preds, _ = evaluate(model, VALID_CV, prefix='(Fold {}\/{}) '.format(cv_fold+1, cv_n_splits), return_prediction=True)\n\n    train['oof'].iloc[train_index[val_cv_index]] = val_cv_preds","2bc53f77":"train[['oof_poi', 'oof_street']] = train['oof'].str.split('\/', 1, expand=True).fillna('')\n\noof_acc = accuracy_score(train['POI\/street'], train['oof'])\noof_poi_acc = accuracy_score(train['poi'], train['oof_poi'])\noof_street_acc = accuracy_score(train['street'], train['oof_street'])\n\nprint('Out-of-folds accuracy score: {:.4f} (POI\/STREET: {:.4f}\/{:.4f})'.format(oof_acc, oof_poi_acc, oof_street_acc))\n\n# save oof\ntrain.to_csv('oof.csv', index=False)","0b054d0f":"test = pd.read_csv(INPUT_PATH+'\/test.csv')\n\nif TEST_N > 0:\n    test = test[:TEST_N]\n\ntest.tail(10)","11873017":"from collections import Counter\nfrom sklearn.metrics import accuracy_score\n\ndef predict(texts, model, batch_size=BATCH_SIZE, title=''):\n    return [doc_to_pred(doc) for doc in tqdm(model.pipe(texts, batch_size=batch_size), total=len(texts), desc=title, disable=DISABLE_TQDM)]\n\ndef ensemble_predict(texts, models):\n    preds = pd.DataFrame()\n    for i, model in enumerate(models):\n        pred_label = 'pred_{}'.format(i+1)\n        pred_poi_label = 'pred_poi_{}'.format(i+1)\n        pred_street_label = 'pred_street_{}'.format(i+1)\n\n        time.sleep(0.5)\n        preds[pred_label] = predict(texts, model, title='({}\/{}) '.format(i+1, len(models)))\n        preds[[pred_poi_label, pred_street_label]] = preds[pred_label].str.split('\/', 1, expand=True)\n\n    poi_preds = []\n    poi_preds_count = []\n    street_preds = []\n    street_preds_count = []\n\n    for _, row in preds.iterrows():\n        poi_preds_row = [row['pred_poi_{}'.format(i+1)] for i in range(len(models))]\n        poi_counter = Counter(poi_preds_row)\n        poi_vote, poi_vote_count = poi_counter.most_common()[0]\n        poi_preds.append(poi_vote)\n        poi_preds_count.append(poi_vote_count)\n\n        street_preds_row = [row['pred_street_{}'.format(i+1)] for i in range(len(models))]\n        street_counter = Counter(street_preds_row)\n        street_vote, street_vote_count = street_counter.most_common()[0]\n        street_preds.append(street_vote)\n        street_preds_count.append(street_vote_count)\n\n    preds['pred_poi'] = poi_preds\n    preds['pred_poi_count'] = poi_preds_count\n    preds['pred_street'] = street_preds\n    preds['pred_street_count'] = street_preds_count\n    preds['pred'] = preds.apply(lambda row: '{}\/{}'.format(row.pred_poi, row.pred_street), axis=1)\n    \n    return preds","bdf2e8f9":"%%time\n\ntest_preds = ensemble_predict(test.raw_address, models)\n\ntest = test.join(test_preds)","e0e731e6":"test.tail(10)","84ffbfd0":"poi_vote_counts = test.groupby(by=['pred_poi_count']).count()[['pred_poi']].rename(columns={'pred_poi': 'count'})\npoi_vote_counts['ratio'] = poi_vote_counts['count'] \/ sum(poi_vote_counts['count'])\npoi_vote_counts","021634c6":"street_vote_counts = test.groupby(by=['pred_street_count']).count()[['pred_street']].rename(columns={'pred_street': 'count'})\nstreet_vote_counts['ratio'] = street_vote_counts['count'] \/ sum(street_vote_counts['count'])\nstreet_vote_counts","fc4e54ef":"def print_stat(label, num, total):\n    print('{}: {} ({:.2f}%)'.format(label, num, 100 * num \/ total))\n\ndef empty_stat(df, poi_column='poi', street_column='street', title=''):\n    total = len(df)\n\n    print_stat('{}Empty POI   '.format(title), len(df[df[poi_column] == \"\"]), total)\n    print_stat('{}Empty Street'.format(title), len(df[df[street_column] == \"\"]), total)\n    print_stat('{}Empty Both  '.format(title), len(df[(df[poi_column] == \"\") & (df[street_column] == \"\")]), total)","cd65921e":"print('Test Empty Stat:')\nempty_stat(test, poi_column='pred_poi', street_column='pred_street')\n\nprint()\nprint('Test Empty Stat Folds:')\nfor i in range(len(models)):\n    pred_label = 'pred_{}'.format(i+1)\n    pred_poi_label = 'pred_poi_{}'.format(i+1)\n    pred_street_label = 'pred_street_{}'.format(i+1)\n\n    empty_stat(test, poi_column=pred_poi_label, street_column=pred_street_label, title='(Fold {}\/{}) '.format(i+1, len(models)))\n    print()","42a0099e":"from collections import Counter\n\ndef create_mappings(texts, labels, ngrams=NGRAMS_ALIGNMENT):\n    mappings = []\n\n    for text, label in tqdm(zip(texts, labels), total=len(texts), disable=DISABLE_TQDM):\n        aligned, mapping = find_alignment(text, label, ngrams=ngrams, full=True)\n        for m in mapping:\n            mappings.append(m)\n\n    mappings_count = Counter(mappings).most_common()\n\n    df_mappings = pd.DataFrame({\n        'token': [m[0] for m, c in mappings_count],\n        'mapping': [m[1] for m, c in mappings_count],\n        'count': [c for m, c in mappings_count],\n        'ngram': [len(tokenizer(m[0])) for m, c in tqdm(mappings_count, disable=DISABLE_TQDM)],\n    })\n\n    return df_mappings\n\ndef get_raw_address_alignment(row, column, entity_label):\n    if row[column] == '':\n        return ''\n\n    span = None\n    for ent in row.entities['entities']:\n        if ent[2] == entity_label:\n            span = (ent[0], ent[1])\n    return row.raw_address[span[0]:span[1]]\n\ndef train_to_mappings(train):\n    poi_mappings = create_mappings(train.apply(lambda row: get_raw_address_alignment(row, 'poi', 'POI'), axis=1), train.poi)\n    street_mappings = create_mappings(train.apply(lambda row: get_raw_address_alignment(row, 'street', 'STREET'), axis=1), train.street)\n\n    return poi_mappings, street_mappings\n\ndef get_most_common_mappings(mappings):\n    mappings_count_map = {}\n    for _, row in mappings.iterrows():\n        if row.token not in mappings_count_map:\n            mappings_count_map[row.token] = {}\n        mappings_count_map[row.token][row.mapping] = row['count']\n\n    mappings_most_common = []\n    for token, counts in mappings_count_map.items():\n        most_common = Counter(counts).most_common(1)[0]\n        mappings_most_common.append((token,) + most_common)\n\n    return pd.DataFrame({\n        'token': [m[0] for m in mappings_most_common],\n        'mapping': [m[1] for m in mappings_most_common],\n        'count': [m[2] for m in mappings_most_common],\n    })","232d0ab8":"def create_mappings_m(df_mappings_unique, threshold=0):\n    m = {}\n\n    for _, row in tqdm(df_mappings_unique.iterrows(), total=len(df_mappings_unique), disable=DISABLE_TQDM):\n        assert(row.token not in m)\n        if row.token != row.mapping and row['count'] >= threshold:\n            # We need to know exactly how each tokens map to the others.\n            m[row.token] = find_alignment(row.token, row.mapping, full=True, ngrams=(1,1))[1]\n    \n    return m\n\ndef apply_mappings(text, m, verbose=False):\n    doc = nlp(text)\n    n = len(doc)\n    words = [d.text for d in doc]\n    spaces = [d.whitespace_ != '' for d in doc]\n    is_mapped = [False for d in doc]\n\n    # apply mappings from the longest phrases first\n    for ngram in reversed(range(NGRAMS_ALIGNMENT[0], NGRAMS_ALIGNMENT[1]+1)):\n        for i in range(n - ngram + 1):\n            # do not map already mapped words\n            if sum(is_mapped[i:i+ngram]) > 0:\n                continue\n            span = doc[i:i+ngram]\n            if span.text in m:\n                # assert(len(m[span.text]) == ngram)\n                if len(m[span.text]) != ngram:\n                    print('Token length mismatched. Skipping... :(')\n                    print('    raw:', text)\n                    print('   span:', span.text)\n                    print('mapping:', m[span.text])\n                    print()\n                    return text\n\n                words[i:i+ngram] = [d for _, d in m[span.text]]\n                is_mapped[i:i+ngram] = [True] * ngram\n\n    mapped = Doc(nlp.vocab, words=words, spaces=spaces).text\n\n    if verbose:\n        print('before:', text)\n        print(' after:', mapped)\n\n    return mapped","5559cf29":"%%time\n\npoi_mappings, street_mappings = train_to_mappings(train)\npoi_mappings_most_common = get_most_common_mappings(poi_mappings)\nstreet_mappings_most_common = get_most_common_mappings(street_mappings)\n\nm_poi = create_mappings_m(poi_mappings_most_common)\nm_street = create_mappings_m(street_mappings_most_common)","5e48bffb":"poi_mappings.to_csv('poi_mappings.csv', index=False)\nstreet_mappings.to_csv('street_mappings.csv', index=False)","dfe0956e":"poi_mappings_most_common","55568c0a":"street_mappings_most_common","5635b517":"%%time\n\ntest['pred_poi_mapped'] = test.pred_poi.apply(lambda x: apply_mappings(x, m_poi))\ntest['pred_street_mapped'] = test.pred_street.apply(lambda x: apply_mappings(x, m_street))\n\n# even though we did map street above, we won't use for the actual prediction.\n# it was only for raw outputs purposes.\ntest['POI\/street'] = test.apply(lambda row: '{}\/{}'.format(row.pred_poi_mapped, row.pred_street), axis=1)","be3cfc73":"test[test.pred_poi_mapped != test.pred_poi][['raw_address', 'pred_poi', 'pred_poi_mapped']]","044f66d6":"test.to_csv('test_raw_output.csv', index=False)\ntest[['id', 'POI\/street']].to_csv('submission.csv', index=False)","73981d07":"# Tokenize\n\nNow we attempt to not use any normalization, but to use a more flexible tokenizer instead. This tokenizer will basically break every tokens that are not alphanumeric, with a little exceptions\/special rules.","af89917f":"Empty stats. Should be:\n- empty poi: ~59.5%\n- empty street: ~23.4%\n- empty both: ~10.6%","f336f61a":"# Save Prediction to file","6bd36406":"# Finding Alignments\n\nPerhaps this is the single most interesting thing from this solution.\n\nWe will attempt to find alignments between the labels (`POI\/street`) with the `raw_address`. An alignment is valid if the substring from the `raw_address` can be transformed into `POI`\/`street` by expanding one or more of its tokens.\n\nThe algorithm is basically a slight modification of [Levehnstein Distance](https:\/\/en.wikipedia.org\/wiki\/Levenshtein_distance), but:\n- performed on tokens level instead of characters,\n- only suffix expansions are allowed,\n- one suffix may expand to multiple tokens (e.g. \"jl. anyer-\" to \"jl. anyer-panarukan\"),\n- with the cost = the length of added character, with more penalty on expansion to multiple tokens.\n\nThe complexity is $O(nm)$ where $n$ and $m$ are the number of tokens in `raw_address` and labels.\n\n## Example\n\n```\n(20955-th train data)\n\ntext: ked kand gg. vii kedungkandang kedungkandang\nlabel: ked kandang, gg. vii\n\ntext tokens: ['ked', 'kand', 'gg', '.', 'vii', 'kedungkandang', 'kedungkandang']\nlabel tokens: ['ked', 'kandang', ',', 'gg', '.', 'vii']\n\nbest cost: 13.0\nexplanation: \"kand\" -> \"kandang,\"\n              cost = 3 (adding \"ang\")\n                     + 10 (adding \",\" is more expensive\n                           because it is a new token))\nbest alignment: \"ked kand gg. vii\" -> \"ked kandang, gg. vii\"\n\ncost table:\n               ked kandang  ,   gg    .   vii\n       [[  0.  inf   inf   inf  inf  inf  inf]\nked     [  0.   0.   70.   80. 100. 110. 140.]\nkand    [  0.  inf    3.   13.  33.  43.  73.]\ngg      [  0.  inf   inf   inf  13.  23.  53.]\n.       [  0.  inf   inf   inf  inf  13.  43.]\nvii     [  0.  inf   inf   inf  inf  inf  13.]\nkedu... [  0.  inf   inf   inf  inf  inf  inf]\nkedu... [  0.  inf   inf   inf  inf  inf  inf]]\n \ntoken alignments: [('ked', 'ked'),\n                   ('kand', 'kandang,'),\n                   ('gg', 'gg'),\n                   ('vii', 'vii')]\n```\n\n## Performance\n\nThis algorithm successfully finds all alignments in all 300k training data. In addition to that, by marking a span of tokens to be \"occupied\", we can use it to find both the alignments of `POI` and `street`. We figured that there were only 216 rows (only 0.07%) of the data having overlapping POI and street.\n\nAnd it finds all the alignments in less than 5 minutes (<1ms\/row).\n\n## Usage\n\nThe `find_alignment` function is used twice:\n\n- First, as already mentioned, to find all alignments between `raw_address` and `POI` and `street`. Then, we use the alignment to tag the tokens in `raw_address` for training NER model. In other words, we use the `raw_address` as-is for training.\n- Second, to obtain all phrase (1-3 n-grams) mappings found in `POI` and `street`. We perform this as correction to the output of the said NER model in the postprocessing.","afc05ebd":"# Postprocessing: Apply Phrase Mapping\n\nFor duplicates, i.e. one token is mapped to different multiple values, we will take *the most common* of them all.\n\nThis increased our score significantly (0.63 -> 0.66) from our previous method of taking only mappings with no duplicates.","c89063ca":"Let's see how consistent are these models.","16042f20":"## Preparing Training Data","371480ad":"Let's see which rows we have differently now.","05f4439a":"# ~Training~ Load Saved Model","ebcefe7d":"## Load Saved Models\n\n\nWe already train each fold separately. Now we just have to load them.","dbdfda6d":"## ~Actually Training~","b18dddcd":"# Prediction","b42d0b52":"Only 216 rows!","55f82017":"We will only map POI, as previous validation results showed mapping street resulted in worse scores.","f79e4f03":"~We will split the training data into two parts: `TRAIN_DATA` and `TEST_DATA`. Later on, `TRAIN_DATA` will be split into `TRAIN` and `VALID` for each folds.~\n\n~This is to evaluate this method's performance. Later on, we will train on full training data.~\n\nNow we will use the whole training data for CV, with no test split."}}