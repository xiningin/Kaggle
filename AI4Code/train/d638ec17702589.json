{"cell_type":{"234e4315":"code","d1b84e28":"code","4c531de0":"code","18263784":"code","e73c3432":"code","d642b02e":"code","bc96babb":"code","2ddf183e":"code","6df95b2e":"code","7192507a":"code","99679ece":"code","2b9b0758":"code","8f09ea04":"code","42d7dec8":"markdown","d12cf932":"markdown","4d871326":"markdown","9478b8db":"markdown","e777fcc0":"markdown","2bff3fca":"markdown"},"source":{"234e4315":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\nfrom catboost import CatBoostClassifier\nfrom sklearn.preprocessing import StandardScaler","d1b84e28":"train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ntrain.set_index('Id', inplace=True)\n\n# code for lossless compression from GUILLAUME MARTIN:    https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ntrain = reduce_mem_usage(train)","4c531de0":"# sum of soil types\ntrain['sum_s'] = train.filter(regex='Soil').sum(axis=1)","18263784":"sns.boxenplot(x='Cover_Type', y='sum_s', data=train, palette='cool')\nplt.show()","e73c3432":"# new dataframe describing how often a certain soil-type occurs at the same location as a cover-type\nfor_feat = train[train['Cover_Type'] != 5].filter(regex='Soil')\nfor_feat['y'] = train['Cover_Type']\nfor_feat = for_feat.groupby('y').mean().transpose()\n# removing soil-types without any related cover-types\nfor_feat = for_feat[for_feat.sum(axis=1) != 0]","d642b02e":"for_feat.head()","bc96babb":"scl = StandardScaler()\nfor_feat = pd.DataFrame(scl.fit_transform(for_feat), index=for_feat.index, columns=for_feat.columns)","2ddf183e":"km = KMeans(n_clusters=5, random_state=69)\nfor_feat['Cluster'] = km.fit_predict(for_feat)","6df95b2e":"for c in for_feat.Cluster.unique():\n    train['c' + str(c) + '_sum'] = train[for_feat[for_feat['Cluster'] == c].index].sum(axis=1)","7192507a":"for_feat['Cluster'].value_counts()","99679ece":"pal = ['#02ABB7', '#674076', \"#F62E97\", '#E672E0', 'crimson']\n\nfig, axs = plt.subplots(2, 3, figsize=(19,15))\n\n# top ten industries by cluster\nsns.scatterplot(ax= axs[0,0], x=1, y=2, hue='Cluster', palette=pal, data=for_feat)\nsns.scatterplot(ax= axs[0,1], x=1, y=3, hue='Cluster', palette=pal, data=for_feat)\nsns.scatterplot(ax= axs[0,2], x=1, y=4, hue='Cluster', palette=pal, data=for_feat)\nsns.scatterplot(ax= axs[1,0], x=1, y=6, hue='Cluster', palette=pal, data=for_feat)\nsns.scatterplot(ax= axs[1,1], x=1, y=7, hue='Cluster', palette=pal, data=for_feat)\naxs[1,2].set_axis_off()\nplt.show()","2b9b0758":"y_train = train.pop('Cover_Type')\ncat = CatBoostClassifier(iterations = 3000, depth= 6, verbose=100, task_type= 'GPU')\ncat.fit(train, y_train)","8f09ea04":"feature_importance = pd.DataFrame()\nfeature_importance['f_importance'] = cat.get_feature_importance()\nfeature_importance['feature'] = train.columns\nfeature_importance = feature_importance.sort_values('f_importance', ascending=False)\n\nfig = plt.subplots(figsize=(10,15))\nsns.barplot(x=\"f_importance\", y=\"feature\", data=feature_importance, palette='cool')\nplt.tight_layout()\nplt.show()","42d7dec8":"# Grouping the Soil-Types","d12cf932":"Cluster 2 seems to be pointless as a new feature as it only contains one soil-type","4d871326":"# Feature Importance","9478b8db":"# Sum of Soil-Types","e777fcc0":"# Summary","2bff3fca":"In this notebook I want to present a few of the features I derived from the dataset. The first feature is simply the sum of soil-types at a location. This feature seems to have a high importance. I also grouped the soil-types by their similarity with regards to the cover-types. The resulting features describe how many soil-types of a certain group are present at a location. There seems to be some importance for these features."}}