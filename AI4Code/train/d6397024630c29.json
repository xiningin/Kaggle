{"cell_type":{"79abf016":"code","51316f87":"code","75bf527c":"code","8ac85719":"code","11956c00":"code","aaaade39":"code","559a4f81":"code","b259910e":"code","e407cba8":"code","e72e9da5":"code","c0f11114":"code","9ccc02b1":"code","052b1976":"code","8d2015c2":"code","91ec289a":"code","10fb0805":"code","57c61bc4":"code","8e1689aa":"code","885156f1":"code","ae0e5d4d":"code","6bf426ea":"code","ebc7ab99":"code","412cca08":"code","bdf950a6":"code","65a4b04e":"code","043f153b":"code","99bc7af9":"code","b4dddfa7":"code","8ca378ce":"code","d7c9699c":"code","ccd75f81":"code","52436610":"code","84cb7704":"code","2e46edf6":"code","a27ca17e":"code","798e42ab":"code","778b0d22":"code","bfbd2509":"code","a5109aaf":"code","f043f120":"code","682184d9":"code","05070735":"code","cd54ea84":"code","d3817460":"code","95621ad2":"code","98e1f98e":"code","3757ec4d":"code","36eea998":"code","f7df1dc3":"code","615e8289":"code","ffae7d04":"code","10c6fb1d":"code","69354d07":"code","cd6dd860":"code","5246baaa":"code","d3b83a5b":"code","0662841d":"code","093d153f":"code","12b2a088":"code","70aed870":"code","c0da2b36":"code","f4f186ce":"code","c86d3e0f":"code","2e4ee671":"code","75183c18":"code","b3efb6a4":"code","c3cd3329":"code","e2aaabf0":"code","71244b39":"code","7cf9a9d5":"code","38a77e95":"code","9d1948ae":"code","cd79775a":"code","5744bdbc":"code","76ff854b":"code","bdfd3849":"code","2ec75b32":"code","c0bea4a0":"code","2743cc89":"code","f36a7b9f":"code","a2fe7f85":"code","a015a706":"code","82760ac3":"code","1e74f4c5":"code","2138ff78":"code","a288f158":"code","6c070a63":"code","e95142a8":"code","2fa53014":"code","339d8fb1":"markdown","efe1afbd":"markdown","0fd79588":"markdown","8af7a304":"markdown","599e1d59":"markdown","75bb7447":"markdown","9d214804":"markdown","904adbb6":"markdown","4e857ee8":"markdown","96fd9d90":"markdown","f22b47e2":"markdown","c429d64b":"markdown","e4437829":"markdown","e5403ced":"markdown","ee403434":"markdown","5a91956c":"markdown","e0cb40a2":"markdown","6ce217d9":"markdown","3d186710":"markdown","fd06b3cf":"markdown","3024e64d":"markdown","d8f0546c":"markdown","3db4bea5":"markdown","50b606b4":"markdown","f1603e1b":"markdown","115e1bbb":"markdown","cabc5223":"markdown","2822036f":"markdown","ee0f56fb":"markdown","ada127a0":"markdown","c1e45cb5":"markdown","ab7c4a30":"markdown","68f4aaaa":"markdown","9ad492c2":"markdown","a480a01f":"markdown","35a62790":"markdown","9ea798aa":"markdown","ff649bb4":"markdown","d65c5947":"markdown","a6ce5035":"markdown","b4a2d284":"markdown","37450e78":"markdown","3b2f727b":"markdown","b52fe6f3":"markdown","52ac684b":"markdown","ec95b9b1":"markdown","126ab16d":"markdown","5452d0e6":"markdown","23a7a518":"markdown","31a894ac":"markdown","994d9f27":"markdown","c9419989":"markdown","cd99fb70":"markdown","2b4b518b":"markdown","7398e7b7":"markdown","3f257449":"markdown","6f035d9c":"markdown","6c6ca6eb":"markdown","65b463e0":"markdown","bac12464":"markdown","8a232be6":"markdown","aa2491d2":"markdown","f37b13bf":"markdown","b646c138":"markdown","a101e56a":"markdown","e36431bc":"markdown","284c23ba":"markdown","e6962b43":"markdown","f22391f5":"markdown","52545b4f":"markdown","20b8921e":"markdown","52c6a7bf":"markdown","e79fc7d3":"markdown"},"source":{"79abf016":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","51316f87":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","75bf527c":"df_train = df_train.drop(columns = 'Id')\ndf_test = df_test.drop(columns = 'Id')","8ac85719":"Check_years = df_train.columns[df_train.columns.str.contains(pat = 'Year|Yr')] ","11956c00":"df_train[Check_years.values].max().sort_values(ascending = False)","aaaade39":"df_test[Check_years.values].max().sort_values(ascending = False)","559a4f81":"Replace_year = df_test.loc[(df_test['GarageYrBlt'] > 2050), 'GarageYrBlt'].index.tolist()\ndf_test.loc[Replace_year, 'GarageYrBlt'] = df_test['GarageYrBlt'].mode()","b259910e":"train_missing = df_train.count().loc[df_train.count() < 1460].sort_values(ascending = False)","e407cba8":"sns.set_theme(rc = {'grid.linewidth': 0.6, 'grid.color': 'white',\n                    'axes.linewidth': 1, 'axes.facecolor': '#ECECEC', \n                    'axes.labelcolor': '#000000',\n                    'figure.facecolor': 'white',\n                    'xtick.color': '#000000', 'ytick.color': '#000000'})","e72e9da5":"with plt.rc_context(rc = {'figure.dpi': 120, 'axes.labelsize': 8.5, \n                          'xtick.labelsize': 6, 'ytick.labelsize': 6}): \n\n    fig, ax = plt.subplots(1, 1, figsize = (6, 4))\n\n    sns.barplot(x = train_missing.values, y = train_missing.index, palette = 'viridis')\n\n    plt.xlabel('Non-Na values')\n\n    plt.show()","c0f11114":"test_missing = df_test.count().loc[df_test.count() < 1459].sort_values(ascending = False)","9ccc02b1":"with plt.rc_context(rc = {'figure.dpi': 120, 'axes.labelsize': 8.5, \n                          'xtick.labelsize': 6, 'ytick.labelsize': 6}):\n    \n    fig, ax = plt.subplots(1, 1, figsize = (7, 6))\n\n    sns.barplot(x = test_missing.values, y = test_missing.index, palette = 'viridis')\n\n    plt.xlabel('Non-Na values')\n\n    plt.show()","052b1976":"None_category = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', \n                 'FireplaceQu', 'GarageCond', 'GarageQual', \n                 'GarageFinish', 'GarageType', 'BsmtCond', \n                 'BsmtExposure', 'BsmtQual', 'BsmtFinType1', \n                 'BsmtFinType2']","8d2015c2":"for column in None_category:\n    \n    df_train.loc[df_train[column].isnull(), column] = 'None'\n    df_test.loc[df_test[column].isnull(), column] = 'None'","91ec289a":"df_train.loc[:, df_train.isna().sum() > 0].isna().sum().sort_values(ascending = False)","10fb0805":"from sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler","57c61bc4":"cont_vars = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', \n             'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n             '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n             'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch']","8e1689aa":"knn_vars_train_cont = df_train[cont_vars].copy()\n\nScaler = RobustScaler()\n\nknn_vars_train_cont = pd.DataFrame(Scaler.fit_transform(knn_vars_train_cont), \n                                   columns = [\"col\" + str(i) for i in range(0, 15)])\n\ntrain_imp_cont = KNNImputer(n_neighbors = 5, weights = 'uniform', metric = 'nan_euclidean')","885156f1":"train_imp_cont_results = train_imp_cont.fit_transform(knn_vars_train_cont)\n\ntrain_imp_cont_results = pd.DataFrame(Scaler.inverse_transform(train_imp_cont_results), \n                                      columns = [\"col\" + str(i) for i in range(0, 15)])","ae0e5d4d":"df_train['LotFrontage'] = train_imp_cont_results['col0']\ndf_train['MasVnrArea'] = train_imp_cont_results['col2'].astype('float64')","6bf426ea":"for column in ['MasVnrType', 'Electrical']:\n    \n    df_train.loc[df_train[column].isnull(), column] = df_train[column].mode()[0]","ebc7ab99":"from sklearn.preprocessing import LabelEncoder","412cca08":"knn_vars_train_cat = df_train.drop(cont_vars, axis = 1)\nknn_vars_train_cat = knn_vars_train_cat.drop('SalePrice', axis = 1)\n\nobj_vars = knn_vars_train_cat.select_dtypes(include = ['object', 'category']).columns","bdf950a6":"for column in obj_vars:\n    \n    knn_vars_train_cat[column] = LabelEncoder().fit_transform(knn_vars_train_cat[column])","65a4b04e":"train_imp_cat = KNNImputer(n_neighbors = 5, weights = 'uniform', metric = 'nan_euclidean')\n\ntrain_imp_cat_results = train_imp_cat.fit_transform(knn_vars_train_cat)\n\ntrain_imp_cat_results = pd.DataFrame(train_imp_cat_results, \n                                     columns = [\"col\" + str(i) for i in range(0, 64)])","043f153b":"df_train['GarageYrBlt'] = train_imp_cat_results['col48']\ndf_train['GarageYrBlt'] = df_train['GarageYrBlt'].astype('int64')","99bc7af9":"knn_vars_test_cont = df_test[cont_vars].copy()\n\nScaler = StandardScaler()\n\nknn_vars_test_cont = pd.DataFrame(Scaler.fit_transform(knn_vars_test_cont), \n                                  columns = [\"col\" + str(i) for i in range(0, 15)])\n\ntest_imp_cont = KNNImputer(n_neighbors = 5, weights = 'uniform', metric = 'nan_euclidean')","b4dddfa7":"test_imp_cont_results = test_imp_cont.fit_transform(knn_vars_test_cont)\n\ntest_imp_cont_results = pd.DataFrame(Scaler.inverse_transform(test_imp_cont_results), \n                                     columns = [\"col\" + str(i) for i in range(0, 15)])","8ca378ce":"df_test['LotFrontage'] = test_imp_cont_results['col0']","d7c9699c":"for column in df_test.columns: \n    \n    if ((df_test[column].isnull().sum() <= 60) & (df_test[column].isnull().sum() > 0) & \n        ((df_test[column].dtypes == 'O') | (df_test[column].dtypes == 'float64')) & \n        (df_test[column].nunique() < 20)):\n        \n        df_test.loc[df_test[column].isnull(), column] = df_test[column].mode()[0]\n        \n    elif ((df_test[column].isnull().sum() <= 60) & (df_test[column].isnull().sum() > 0) & \n          (df_test[column].dtypes == 'float64') & (df_test[column].nunique() > 100)):\n        \n        df_test.loc[df_test[column].isnull(), column] = df_test[column].mean()\n        \n    else: pass","ccd75f81":"knn_vars_test_cat = df_test.drop(cont_vars, axis = 1)","52436610":"for column in knn_vars_test_cat:\n    \n    knn_vars_test_cat[column] = LabelEncoder().fit_transform(knn_vars_test_cat[column])","84cb7704":"test_imp_cat = KNNImputer(n_neighbors = 5, weights = 'uniform', metric = 'nan_euclidean')\n\ntest_imp_cat_results = test_imp_cat.fit_transform(knn_vars_test_cat)\n\ntest_imp_cat_results = pd.DataFrame(test_imp_cat_results, \n                                    columns = [\"col\" + str(i) for i in range(0, 64)])","2e46edf6":"df_test['GarageYrBlt'] = test_imp_cat_results['col48']\ndf_test['GarageYrBlt'] = df_test['GarageYrBlt'].astype('int64')","a27ca17e":"print(df_train.isna().sum().any(), df_test.isna().sum().any(), sep = '\\n')","798e42ab":"train_obj = df_train.select_dtypes(include = ['object', 'category']).columns\n\ntrain_int_float = df_train.select_dtypes(include = ['int64', 'float64'])\ncol_order = train_int_float.nunique().sort_values(ascending = False).index.tolist()\ntrain_int_float = train_int_float[col_order].columns","778b0d22":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 7, \n                          'xtick.labelsize': 5, 'ytick.labelsize': 5}): \n\n    fig, ax = plt.subplots(5, 5, figsize = (8.5, 10), sharey = True)\n\n    for idx, (column, axes) in list(enumerate(zip(train_int_float[0:22], ax.flatten()))):\n    \n        sns.scatterplot(ax = axes, x = df_train[column], \n                        y = np.log(df_train['SalePrice']), \n                        hue =  np.log(df_train['SalePrice']), \n                        palette = 'viridis', alpha = 0.7, s = 8)\n    \n        axes.legend([], [], frameon = False)\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n\n    plt.tight_layout()\n    plt.show()","bfbd2509":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 7, \n                          'xtick.labelsize': 5, 'ytick.labelsize': 5}): \n\n    fig, ax = plt.subplots(5, 4, figsize = (8.5, 9), sharey = True)\n\n    for idx, (column, axes) in list(enumerate(zip(train_int_float[22:], ax.flatten()))):\n    \n        sns.scatterplot(ax = axes, x = df_train[column], \n                        y = np.log(df_train['SalePrice']), \n                        hue =  np.log(df_train['SalePrice']), \n                        palette = 'viridis', alpha = 0.7, s = 8)\n    \n        axes.legend([], [], frameon = False)\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n    \n    plt.tight_layout()\n    plt.show()","a5109aaf":"train_cont_balanced = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', \n                       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n                       '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n                       'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch']\n\ntrain_cont_unbalanced = ['LowQualFinSF', '3SsnPorch' , 'PoolArea' , 'MiscVal']","f043f120":"train_cat = df_train.drop(train_cont_balanced, axis = 1).columns.tolist()\ntrain_cat.remove('SalePrice')","682184d9":"df_train[train_cat].loc[:, df_train.nunique() > 25].nunique().sort_values(ascending = False)","05070735":"train_high_cat = df_train[train_cat].loc[:, df_train.nunique() > 25].copy()","cd54ea84":"for column in train_high_cat.columns:\n    \n    train_high_cat[column] = train_high_cat[column].astype('category')","d3817460":"with plt.rc_context(rc = {'figure.dpi': 450, 'axes.labelsize': 5, \n                          'xtick.labelsize': 4, 'ytick.labelsize': 4}): \n\n    fig, ax = plt.subplots(1, 3, figsize = (6, 7.5))\n\n    for idx, (column, axes) in list(enumerate(zip(train_high_cat.columns, ax.flatten()))): \n    \n        sns.stripplot(ax = axes, x = np.log(df_train['SalePrice']), \n                      y = train_high_cat[column], \n                      palette = 'viridis', alpha = 0.95, size = 1.5)\n\n        sns.boxplot(ax = axes, x = np.log(df_train['SalePrice']), \n                    y = train_high_cat[column],\n                    showmeans = True, meanline = True, zorder = 10,\n                    meanprops = {'color': 'r', 'linestyle': '-', 'lw': 0.8},\n                    medianprops = {'visible': False},\n                    whiskerprops = {'visible': False},\n                    showfliers = False, showbox = False, showcaps = False)\n        \n        sns.pointplot(ax = axes, x = np.log(df_train['SalePrice']), \n                      y = train_high_cat[column],\n                      ci = None, color = 'r', scale = 0.15)\n    \n    else: \n    \n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n    \n    plt.tight_layout()\n    plt.show()","95621ad2":"train_norm_cat = df_train[train_cat].loc[:, df_train.nunique() <= 25].columns.tolist()","98e1f98e":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 7, \n                          'xtick.labelsize': 5.5, 'ytick.labelsize': 5.5}): \n\n    fig, ax = plt.subplots(5, 3, figsize = (8, 13), sharey = True)\n\n    for idx, (column, axes) in list(enumerate(zip(train_norm_cat[: 15], ax.flatten()))):\n    \n        order = df_train.groupby(column)['SalePrice'].mean().sort_values(ascending = True).index\n    \n        sns.violinplot(ax = axes, x = df_train[column], \n                       y = np.log(df_train['SalePrice']),\n                       order = order, scale = 'width',\n                       linewidth = 0.3, palette = 'viridis',\n                       saturation = 0.5, inner = None)\n    \n        plt.setp(axes.collections, alpha = 0.3)\n    \n        sns.stripplot(ax = axes, x = df_train[column], \n                      y = np.log(df_train['SalePrice']),\n                      palette = 'viridis', s = 1.3, alpha = 0.9,\n                      order = order)\n    \n        sns.boxplot(ax = axes, x = df_train[column], order = order,\n                    y = np.log(df_train['SalePrice']),\n                    showmeans = True, meanline = True, zorder = 10,\n                    meanprops = {'color': 'r', 'linestyle': '--', 'lw': 0.6},\n                    medianprops = {'visible': False},\n                    whiskerprops = {'visible': False},\n                    showfliers = False, showbox = False, showcaps = False)\n        \n        if df_train[column].nunique() > 5: \n        \n            plt.setp(axes.get_xticklabels(), rotation = 90)\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n\n    plt.tight_layout()\n    plt.show()","3757ec4d":"indx_final = [30, 462, 495, 523, 588, 632, 968, 1298, 1324]","36eea998":"df_train = df_train.drop(indx_final, axis = 0).reset_index(drop = True)","f7df1dc3":"##### Training a model #####\n\n# Lasso_outliers = linear_model.Lasso(alpha = 0.0005)\n\n# Lasso_fit = Lasso_outliers.fit(X_train, y)\n\n##### Getting outliers #####\n\n# rows_to_drop = (Lasso_fit.predict(X_train) - df_train['SalePrice'])**2\n# rows_to_drop[rows_to_drop > 0.2].index","615e8289":"df_train['TotalPorch'] = (df_train['ScreenPorch'] + df_train['EnclosedPorch'] + \n                          df_train['3SsnPorch'] + df_train['ScreenPorch'])\n\ndf_train['Rooms_kitchens'] = (df_train['TotRmsAbvGrd'] + df_train['BsmtFullBath'] + \n                              df_train['BsmtHalfBath'] + df_train['FullBath'] + \n                              df_train['HalfBath'])\n\ndf_train['Sqr_feet_per_room'] = ((df_train['1stFlrSF'] + \n                                  df_train['2ndFlrSF']) \/ df_train['TotRmsAbvGrd'])","ffae7d04":"train_cont_balanced.append('TotalPorch')\ntrain_cont_balanced.append('Sqr_feet_per_room')","10c6fb1d":"df_test['TotalPorch'] = (df_test['ScreenPorch'] + df_test['EnclosedPorch'] + \n                         df_test['3SsnPorch'] + df_test['ScreenPorch'])\n\ndf_test['Rooms_kitchens'] = (df_test['TotRmsAbvGrd'] + df_test['BsmtFullBath'] + \n                             df_test['BsmtHalfBath'] + df_test['FullBath'] + \n                             df_test['HalfBath'])\n\ndf_test['Sqr_feet_per_room'] = ((df_test['1stFlrSF'] + \n                                 df_test['2ndFlrSF']) \/ df_test['TotRmsAbvGrd'])","69354d07":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 7, \n                          'xtick.labelsize': 6, 'ytick.labelsize': 6,\n                          'legend.fontsize': 6, 'legend.title_fontsize': 6}): \n\n    fig, ax = plt.subplots(1, 4, figsize = (8, 3), sharey = True)\n\n    for idx, (column, axes) in list(enumerate(zip(train_cont_unbalanced, ax.flatten()))):\n    \n        sns.scatterplot(ax = axes, x = df_train[column], \n                        y = np.log(df_train['SalePrice']), \n                        hue = np.log(df_train['SalePrice']), \n                        palette = 'viridis', alpha = 0.8, s = 9)\n\n    axes_legend = ax.flatten()\n\n    axes_legend[0].legend(title = 'SalePrice', loc = 'lower right')\n    axes_legend[1].legend(title = 'SalePrice', loc = 'lower right')\n    axes_legend[3].legend(title = 'SalePrice', loc = 'lower right')\n    \n    plt.tight_layout()\n    plt.show()","cd6dd860":"for column in train_cont_unbalanced:\n    \n    df_train.loc[(df_train[column] == 0), column] = 'None' \n    \n    df_train.loc[(df_train[column] != 0) & (df_train[column] != 'None'), column] = 'Present'","5246baaa":"for column in train_cont_unbalanced:\n    \n    df_test.loc[(df_test[column] == 0), column] = 'None' \n    \n    df_test.loc[(df_test[column] != 0) & (df_test[column] != 'None'), column] = 'Present'","d3b83a5b":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 7, \n                          'xtick.labelsize': 5, 'ytick.labelsize': 5}):\n    \n    fig, ax = plt.subplots(5, 4, figsize = (8.5, 9))\n\n    for idx, (column, axes) in list(enumerate(zip(train_cont_balanced, ax.flatten()))):\n    \n        sns.kdeplot(ax = axes, x = df_train[column], \n                    fill = True, alpha = 0.2, color = '#006e7a',\n                    linewidth = 0.8)\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n    \n    plt.tight_layout()\n    plt.show()","0662841d":"df_train[train_cont_balanced] = np.log(df_train[train_cont_balanced] + 1)\ndf_test[train_cont_balanced] = np.log(df_test[train_cont_balanced] + 1)","093d153f":"from sklearn.model_selection import KFold","12b2a088":"def mean_encode(train_data, test_data, columns, target_col, alpha = 0, folds = 1):\n    encoded_cols = []\n    target_mean_global = train_data[target_col].mean()\n    for col in columns:\n        # Getting means for test data\n        nrows_cat = train_data.groupby(col)[target_col].count()\n        target_means_cats = train_data.groupby(col)[target_col].mean()\n        target_means_cats_adj = (target_means_cats*nrows_cat + \n                                 target_mean_global*alpha)\/(nrows_cat+alpha)\n        # Mapping means to test data\n        encoded_col_test = test_data[col].map(target_means_cats_adj)\n        # Getting a train encodings\n        kfold = KFold(folds, shuffle=True, random_state=1).split(train_data[target_col].values)\n        parts = []\n        \n        for tr_in, val_ind in kfold:\n            # divide data\n            df_for_estimation, df_estimated = train_data.iloc[tr_in], train_data.iloc[val_ind]\n            # getting means on data for estimation (all folds except estimated)\n            nrows_cat = df_for_estimation.groupby(col)[target_col].count()\n            target_means_cats = df_for_estimation.groupby(col)[target_col].mean()\n            target_means_cats_adj = (target_means_cats*nrows_cat + \n                                         target_mean_global*alpha)\/(nrows_cat+alpha)\n            # Mapping means to estimated fold\n            encoded_col_train_part = df_estimated[col].map(target_means_cats_adj)\n \n            # Saving estimated encodings for a fold\n            parts.append(encoded_col_train_part)\n            encoded_col_train = pd.concat(parts, axis = 0)\n            encoded_col_train.fillna(target_mean_global, inplace = True)\n\n        # Saving the column with means\n        encoded_col = pd.concat([encoded_col_train, encoded_col_test], axis = 0)\n        encoded_col[encoded_col.isnull()] = target_mean_global\n        encoded_cols.append(pd.DataFrame({'mean_'+ target_col + '_' + col:encoded_col}))\n    all_encoded = pd.concat(encoded_cols, axis = 1)\n    return (all_encoded.loc[train_data.index,:], \n            all_encoded.loc[test_data.index,:])","70aed870":"train_mean_encoding = df_train[list(train_high_cat.columns)].copy()\ntrain_mean_encoding['SalePrice'] = df_train['SalePrice']\n\ntarget_col = 'SalePrice'\ncolumns = train_mean_encoding.columns.tolist()\n\ncolumns_test = columns\ncolumns_test.remove('SalePrice')\ntest_mean_encoding = df_test[columns_test]\n\nindex_0 = list(range(0, 1459))\nindex_1 = list(range(1451, 2910))\n\ntest_mean_encoding = test_mean_encoding.rename(index = dict(zip(index_0, index_1)))","c0da2b36":"Mean_encoding = mean_encode(train_mean_encoding, test_mean_encoding, \n                            columns, target_col, alpha = 5, folds = 10)","f4f186ce":"train_high_cat_encoded = np.log(Mean_encoding[0].reset_index(drop = True))\ntest_high_cat_encoded = np.log(Mean_encoding[1].reset_index(drop = True))","c86d3e0f":"from sklearn.preprocessing import OneHotEncoder","2e4ee671":"train_test_norm_cat = pd.concat([df_train[train_norm_cat], \n                                 df_test[train_norm_cat]], \n                                 axis = 0, join = 'outer', \n                                 ignore_index = True)","75183c18":"OHE =  OneHotEncoder(sparse = False, handle_unknown = 'ignore')\n\ntrain_test_norm_cat_OHE = pd.DataFrame(pd.DataFrame(OHE.fit_transform(train_test_norm_cat)))\ntrain_test_norm_cat_OHE.columns = OHE.get_feature_names(train_test_norm_cat.columns.tolist())","b3efb6a4":"NULLS = pd.DataFrame({'%_nulls': train_test_norm_cat_OHE.isin([0]).mean()})\nNULLS = NULLS.reset_index().sort_values(ascending = False, by = '%_nulls')\nNULLS = NULLS.rename(columns = {'index': 'Variable'})\n\nDROP = NULLS.loc[((NULLS['%_nulls'] >= 0.99) | (NULLS['%_nulls'] <= 0.005)), 'Variable'].values","c3cd3329":"train_test_norm_cat_OHE = train_test_norm_cat_OHE.drop(DROP, axis = 1)","e2aaabf0":"train_norm_cat_OHE = train_test_norm_cat_OHE.iloc[:1451, ]\ntest_norm_cat_OHE = (train_test_norm_cat_OHE.iloc[1451:, ]).reset_index(drop = True)","71244b39":"train_ordinal = pd.DataFrame()\ntest_ordinal = pd.DataFrame()","7cf9a9d5":"train_ordinal['OverallQual'] = df_train['OverallQual']\ntrain_ordinal['OverallCond'] = df_train['OverallCond']","38a77e95":"test_ordinal['OverallQual'] = df_test['OverallQual']\ntest_ordinal['OverallCond'] = df_test['OverallCond']","9d1948ae":"train_cont_balanced_default = df_train[train_cont_balanced].copy()\ntest_cont_balanced_default = df_test[train_cont_balanced].copy()","cd79775a":"train_list = [train_high_cat_encoded, train_norm_cat_OHE,\n              train_cont_balanced_default, train_ordinal]","5744bdbc":"X_train = pd.concat(train_list, axis = 1)\ny = np.log(df_train['SalePrice'])","76ff854b":"test_list = [test_high_cat_encoded, test_norm_cat_OHE,\n             test_cont_balanced_default, test_ordinal]","bdfd3849":"X_test = pd.concat(test_list, axis = 1)","2ec75b32":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold\n\nCV = KFold(n_splits = 10, random_state = 999, shuffle = True)\nCV_rep = RepeatedKFold(n_splits = 10, n_repeats = 3, random_state = 999)\n\n# 1.1 Lasso\n\nfrom sklearn import linear_model\n\n###### Training a model ######\n\n# %%time\n\n# Lasso_model = linear_model.Lasso()\n\n# alpha = {'alpha': [x \/ 25000 for x in range(1, 50, 1)],\n#          'tol': [0.0000001], \n#          'max_iter': [3000]}\n\n# Lasso_grid = GridSearchCV(Lasso_model, alpha, verbose = True, \n#                           scoring = 'neg_root_mean_squared_error', \n#                           n_jobs = 7, cv = CV)\n\n# Lasso_fit = Lasso_grid.fit(X_train, y)\n\n###### Getting scores and parameters ######\n\n# round(-1*Lasso_fit.best_score_, 5)\n# Lasso_fit.best_params_\n\n###### Getting feature importance ######\n\n# FI_lasso = list(zip(abs(Lasso_fit.best_estimator_.coef_), X_train.columns))\n# FI_lasso = pd.DataFrame(FI_lasso, columns = ['Imp', 'Variable'])\n# FI_lasso = FI_lasso.sort_values(ascending = False, by = 'Imp')\n\n########################################\n\n# 1.2 Elastic Net\n\n###### Training a model ######\n\n# %%time\n\n# ElasticNet_model = linear_model.ElasticNet()\n\n# alpha_l1 = {'alpha': [x \/ 25000 for x in range(1, 25, 1)],\n#             'l1_ratio': [x \/ 100 for x in range(10, 100, 1)],\n#             'tol': [0.000001], \n#             'max_iter': [4000]}\n\n# ElasticNet_random = RandomizedSearchCV(ElasticNet_model, alpha_l1, verbose = True, \n#                                        scoring = 'neg_root_mean_squared_error', \n#                                        n_jobs = 7, cv = CV, n_iter = 50)\n\n# ElasticNet_fit = ElasticNet_random.fit(X_train, y)\n\n###### Getting scores and parameters ######\n\n# round(-1*ElasticNet_fit.best_score_, 5)\n# ElasticNet_fit.best_params_\n\n########################################\n\n# 1.3 XGBoost\n\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X_train, y, \n                                   test_size = 0.1, random_state = 999, \n                                   shuffle = True)\n\n###### Training a model ######\n\n# %%time\n\n# XGB_model = xgb.XGBRegressor(use_label_encoder = False, \n#                              eval_metric = 'rmse', \n#                              n_estimators = 10000)\n\n# XGB_param_Random = {'reg_alpha': [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 3],\n#                     'reg_lambda': [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 3],\n#                     'learning_rate': [x \/ 400 for x in range(1, 10, 1)],\n#                     'max_depth': list(range(2, 15, 1)),\n#                     'min_child_weight': list(range(2, 35, 1)),\n#                     'gamma': [x \/ 200 for x in range(0, 50, 1)],\n#                     'subsample': [0.5, 0.6, 0.7, 0.8, 0.9],\n#                     'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9]}\n\n# XGB_random_grid = RandomizedSearchCV(XGB_model, XGB_param_Random, cv = CV, \n#                                      verbose = False, n_jobs = 7, \n#                                      scoring = 'neg_root_mean_squared_error', \n#                                      n_iter = 65)\n\n# XGB_fit = XGB_random_grid.fit(x_train, y_train, \n#                               early_stopping_rounds = 200, \n#                               eval_set = [[x_test, y_test]], \n#                               eval_metric = 'rmse', verbose = False)\n\n###### Getting scores and parameters ######\n\n# round(-1*XGB_fit.best_score_, 5)\n# XGB_fit.best_params_\n\n########################################\n\n# 1.4 LGBM\n\nimport lightgbm as lgb\n\nfrom scipy.stats import randint\nfrom scipy.stats import uniform\n\n###### Training a model ######\n\n# %%time\n\n# LGBM_model = lgb.LGBMRegressor(n_estimators = 10000)\n\n# LGBM_param_Random = {'reg_lambda': [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2],\n#                      'reg_alpha': [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2],\n#                      'min_child_samples': randint(1, 100),\n#                      'subsample': [x \/ 10 for x in range(1, 10, 1)], # bagging_fraction\n#                      'subsample_freq': randint(1, 200), # bagging_freq\n#                      'num_leaves': randint(1, 200),\n#                      'max_depth': list(range(1, 15, 1)),\n#                      'max_bin': randint(1, 700),\n#                      'learning_rate': [x \/ 200 for x in range(1, 10, 1)],\n#                      'colsample_bytree': [x \/ 10 for x in range(1, 11, 1)]} # feature_fraction \n                        \n                    \n# LGBM_random_grid = RandomizedSearchCV(LGBM_model, LGBM_param_Random, cv = CV, \n#                                       verbose = False, n_jobs = 7, \n#                                       scoring = 'neg_root_mean_squared_error', n_iter = 100)\n\n# LGBM_fit = LGBM_random_grid.fit(x_train, y_train, early_stopping_rounds = 100, \n#                                 eval_set = [[x_test, y_test]], \n#                                 eval_metric = 'rmse', verbose = False)\n\n###### Getting scores and parameters ######\n\n# round(-1*LGBM_fit.best_score_, 5)\n# LGBM_fit.best_params_\n\n########################################\n\n# 1.5 SVR\n\n# Before using KNN or SVR, we have to scale data!\n\nfrom sklearn.preprocessing import RobustScaler\n\nvars_for_scaling = (train_high_cat_encoded.columns.tolist() + \n                   train_cont_balanced_default.columns.tolist())\n\nScaler = RobustScaler()\n\nX_train_scaled = X_train.copy()\nX_test_scaled = X_test.copy()\n\nfor column in vars_for_scaling:\n    \n    X_train_scaled[column] = Scaler.fit_transform(X_train[[column]])\n    X_test_scaled[column] = Scaler.fit_transform(X_test[[column]])\n    \nfrom sklearn.svm import SVR\n\n###### Training a model ######\n\n# %%time\n\n# SVR_model = SVR()\n\n# parameters = {'kernel' : ['rbf'],\n#               'C' : list(range(1, 100, 1)),\n#               'epsilon' : [x \/ 2000 for x in range(1, 50, 1)],\n#               'gamma' : [x \/ 10000 for x in range(1, 50, 1)]}\n\n# SVR_random_grid = RandomizedSearchCV(SVR_model, parameters, cv = CV, \n#                                      verbose = False, n_jobs = 7, \n#                                      scoring = 'neg_root_mean_squared_error', \n#                                      n_iter = 60)\n\n# SVR_fit = SVR_random_grid.fit(X_train_scaled, y)\n\n###### Getting scores and parameters ######\n\n# round(-1*SVR_fit.best_score_, 3)\n# SVR_fit.best_params_\n\n########################################\n\n# 1.6 KNN\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\n###### Training a model ######\n\n# %%time\n\n# KNN_model = KNeighborsRegressor()\n\n# KNN_param_Random = {'leaf_size': list(range(1, 50, 1)),\n#                     'n_neighbors': list(range(1, 50, 1)),\n#                     'p' : [1, 2], \n#                     'weights': ('uniform', 'distance'),\n#                     'metric': ('minkowski', 'chebyshev'), \n#                     'algorithm': ('ball_tree', 'kd_tree')}\n\n# KNN_random_grid = RandomizedSearchCV(KNN_model, KNN_param_Random, cv = CV_rep,  \n#                                      scoring = 'neg_root_mean_squared_error', \n#                                      verbose = True, n_jobs = 7, n_iter = 100)\n\n# KNN_fit = KNN_random_grid.fit(X_train_scaled, y)\n\n###### Getting scores and parameters ######\n\n# round(-1*KNN_fit.best_score_, 5)\n# KNN_fit.best_params_","c0bea4a0":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.pipeline import make_pipeline","2743cc89":"base_learners = [\n                 ('Lasso', linear_model.Lasso(tol = 1e-7, \n                           alpha = 0.00028, max_iter = 3000)),\n    \n                 ('El_Net', linear_model.ElasticNet(tol = 1e-6, \n                            alpha = 0.00044, l1_ratio = 0.61, max_iter = 4000)),\n    \n                 ('XGB', xgb.XGBRegressor(use_label_encoder = False, \n                         eval_metric = 'rmse',                   \n                         n_estimators = 5000,\n                         reg_alpha = 0.1,\n                         reg_lambda = 0.005,\n                         learning_rate = 0.0125,\n                         max_depth = 13,\n                         min_child_weight = 4,\n                         gamma = 0.04,\n                         subsample = 0.7,\n                         colsample_bytree = 0.6)),\n    \n                 ('LGBM', lgb.LGBMRegressor(\n                          n_estimators = 9000,\n                          reg_lambda = 1.8,\n                          reg_alpha = 0.01,\n                          min_child_samples = 13,\n                          subsample = 0.8,\n                          subsample_freq = 11,\n                          num_leaves = 101,\n                          max_depth = 3,\n                          max_bin = 160,\n                          learning_rate = 0.005,\n                          colsample_bytree = 0.1)),\n    \n                 ('KNN', make_pipeline(RobustScaler(), \n                         KNeighborsRegressor(\n                         leaf_size = 25,\n                         n_neighbors = 9,\n                         p = 1,\n                         weights = 'distance',\n                         metric = 'minkowski',\n                         algorithm = 'ball_tree'))),\n    \n                 ('SVR', make_pipeline(RobustScaler(), \n                         SVR(\n                         kernel = 'rbf',\n                         C =  10, \n                         epsilon =  0.017,\n                         gamma =  0.0007)))\n                ]","f36a7b9f":"Final_stack = StackingRegressor(estimators = base_learners, \n                                final_estimator = linear_model.Lasso(tol = 1e-7, \n                                alpha = 0.00028, max_iter = 3000), \n                                passthrough = True, verbose = False, \n                                cv = 5)","a2fe7f85":"Final_fit = Final_stack.fit(X_train, y)\n\ny_pred = Final_fit.predict(X_test)","a015a706":"submission = pd.DataFrame({'Id': list(range(1461, 2920)), 'SalePrice': np.exp(y_pred)})\nsubmission.to_csv('submission.csv', index = False)","82760ac3":"Corr_vars = abs(df_train.corr()['SalePrice']).sort_values(ascending = False)","1e74f4c5":"High_corr_vars = Corr_vars.loc[Corr_vars > 0.6].index.tolist()\nHigh_corr_vars.remove('SalePrice')","2138ff78":"High_corr_vars","a288f158":"from itertools import combinations","6c070a63":"train_cont_comb = pd.DataFrame()","e95142a8":"for c_1, c_2 in combinations(df_train[High_corr_vars], 2):\n    \n    train_cont_comb['{0}*{1}'.format(c_1, c_2)] = df_train[c_1] * df_train[c_2]","2fa53014":"train_cont_comb.head(3).round(1)","339d8fb1":"Here is an axample of how you can do it yourself:","efe1afbd":"For starters, we should separate variables by their type in order to figure out what columns are categorical and what are numeric, which is crucial for further analysis.","0fd79588":"For the sake of experimenting with <span style = \"color: #E85E40\"> matplotlib <\/span> and <span style = \"color: #E85E40\"> seaborn <\/span>, I plotted some categorical features. In my judgment, using stripplots with ordered by the target mean categories can be quite insightful. First and foremost, you can clearly see how many observations each category contains, which is vital if you want to isolate imbalanced features. On top of that, after ordering every category, a relationship (if present) of an independent variable with the target becomes evident.","8af7a304":"* [I. Getting started](#I)\n    * [1. Importing some basic libraries](#I_1)\n    * [2. Loading data](#I_2)\n    * [3. Preliminary cleaning](#I_2)\n\n\n* [II. Missing values](#II)\n    * [1. Visualising NaNs](#II_1)\n    * [2. Imputing NaN values. Training set](#II_2)\n    * [3. Imputing NaN values. Test set](#II_3)\n    \n   \n* [III. EDA](#III)\n    * [1. Visualising potential numeric variables](#III_1)\n    * [2. Visualising categorical variables](#III_2)\n\n\n* [IV. Feature engineering](#IV)\n    * [1. Dealing with outliers](#IV_1)\n    * [2. Adding some new variables](#IV_2)\n    * [3. Binning imbalanced features](#IV_3)\n    * [4. Transforming skewed variables](#IV_4)\n    * [5. Encoding variables](#IV_5)\n    * [6. Getting the final training and test sets](#IV_6)\n\n\n* [V. Building models](#V)\n    * [1. Tuning modelss](#V_1)\n    * [2. Stacking](#V_2)\n\n\n* [VI. Some techniques that could have been useful](#VI)\n    * [1. Feature interactions](#VI_1)","599e1d59":"### 2.1 High cardinality features","75bb7447":"Playing around with different encoding techniques, I found out that \"OverallQual\" and \"OverallCond\" significantly boosted CV scores when they were encoded ordinally. But I decided to do both: keep them ordinal and one-hot encode them, allowing models to make all difficult choices for themselves.","9d214804":"<div style = \"color: #000000;\n             display: fill;\n             padding: 8px;\n             border-radius: 5px;\n             border-style: solid;\n             border-color: #a63700;\n             background-color: rgba(235, 125, 66, 0.3)\">\n    \n<span style = \"font-size: 20px; font-weight: bold\">Note:<\/span> \n<span style=\"font-size: 15px\">This function works properly only if training and test sets have different indices.<\/span>\n<\/div>","904adbb6":"Remember, it is important to scale your data before using KNN because this algorithm is distance-based. Robust scaler was used since outliers had not been dealt with yet.","4e857ee8":"## 1. Visualising NaNs <a class=\"anchor\" id = \"II_1\"><\/a>","96fd9d90":"Before setting a threshold, try plotting residuals. It can help a lot.","f22b47e2":"### 2.1 \"LotFrontage\"","c429d64b":"High cardinality features were encoded via mean encoding with cross validation and regularisation, which are a must if you want to prevent overfitting.\n\nI used a piece of code from this excellent notebook:\n\nhttps:\/\/www.kaggle.com\/vprokopev\/mean-likelihood-encodings-a-comprehensive-study\n\nI can also recommend you two great videos that cover the idea of mean encoding and various regularisation techniques: \n\nhttps:\/\/www.coursera.org\/lecture\/competitive-data-science\/concept-of-mean-encoding-b5Gxv\n\nhttps:\/\/www.coursera.org\/lecture\/competitive-data-science\/regularization-LGYQ2","e4437829":"### 2.3 \"GarageYrBlt\"","e5403ced":"### 3.3 \"GarageYrBlt\"","ee403434":"## Thanks for reading!","5a91956c":"I want to delete \"Id\" columns, as they won't provide us with any useful information.","e0cb40a2":"<h1><center> IV. Feature engineering <\/center><\/h1> <a class=\"anchor\" id = \"IV\"><\/a>","6ce217d9":"Based on graphs like the ones above, we can easily determine what variables are actually continous. In addition, I kept imbalanced predictors away from balanced ones.","3d186710":"How did I determine what observations were outliers? I simply run a Lasso model and collected the largest residuals. I didn't do it in this segment because all columns needed to be properly encoded (remember actual analysis is not linear while writing a notebook is).","fd06b3cf":"Let's get only categorical features:","3024e64d":"Next, let's make sure that columns that have a limited range of values don't have any obviously incorrect observations. To do so we can simply utilise the following function:","d8f0546c":"## 1. Dealing with outliers <a class=\"anchor\" id = \"IV_1\"><\/a>","3db4bea5":"### 2.2 Variables with manageable cardinality","50b606b4":"<div style = \"color: #000000;\n             display: fill;\n             padding: 8px;\n             border-radius: 5px;\n             border-style: solid;\n             border-color: #a63700;\n             background-color: rgba(235, 125, 66, 0.3)\">\n    \n<span style = \"font-size: 20px; font-weight: bold\">Note:<\/span> \n<span style=\"font-size: 15px\">If you want to explore other feature engineering techniques and get utility functions, please refer to this <a href=\"https:\/\/www.kaggle.com\/suprematism\/advanced-feature-engineering-utility-functions\">notebook<\/a>.<\/span>\n<\/div>","f1603e1b":"Also, don't forget to inverse transform your data.","115e1bbb":"Before we can build any models or engineer some features, we have to deal with missing values. First of all, I created a visual representation of NaN values that helped me understand their structure.","cabc5223":"### 2.2 Other variables (few missing values)","2822036f":"Finally, you can train a model that has a built-in regularisation (Lasso, for instance) and see what variables are actually important.","ee0f56fb":"<h1><center> Content <\/center><\/h1>","ada127a0":"The rest of categorical variables were encoded with the help of one-hot encoding.","c1e45cb5":"### 5.1 Mean encoding","ab7c4a30":"I built 6 models: **Lasso**, **ElasticNet**, **XGB**, **LGBM**, **SVR** and **KNN**, and then stacked them using <span style = \"color: #E85E40\"> StackingRegressor <\/span>. Since tuning hyperparameters took me about 2 hours (XGB was quite slow), I run only the final (tuned) models in this notebook, but you can still see how they were tuned. I mostly relied on <span style = \"color: #E85E40\"> RandomizedSearchCV <\/span>.","68f4aaaa":"Imbalanced numeric variables were determined at the very beginning.","9ad492c2":"Setting some global parameters for all plots was done with the aid of <code style = \"background-color: #faedde\">sns.set_theme(rc = {})<\/code>.","a480a01f":"In this segment I included some feature engineering options that didn't really work in this case (based on CV scores) but might be valuable in other situations.","35a62790":"<h1><center> II. Missing values <\/center><\/h1> <a class=\"anchor\" id = \"II\"><\/a>","9ea798aa":"I added some variables that made sense to me. For instance, I calculated the total number of rooms (kitchens, bathrooms and other rooms).","ff649bb4":"## 1. Visualising potential numeric variables <a class=\"anchor\" id = \"III_1\"><\/a>","d65c5947":"I used KNN imputer when the number of missing values was relatively large; however, when there were only few NaNs, I thought that replacing them with the mode or mean of a respective column was a reasonable choice.","a6ce5035":"Based on data description, we can conclude that NaN values in some columns are actually a category, namely \"Not present\". So, instead of dropping these columns, we can make them \"clean\".","b4a2d284":"It is important to visualise variables with lots of categories, as you can observe whether they have some kind of relationship with the target. If yes, you should not drop them but encode properly.","37450e78":"## 3. Binning imbalanced features <a class=\"anchor\" id = \"IV_3\"><\/a>","3b2f727b":"## 1. Importing some basic libraries <a class=\"anchor\" id = \"I_1\"><\/a>","b52fe6f3":"<div style = \"color: #000000;\n             display: fill;\n             padding: 8px;\n             border-radius: 5px;\n             border-style: solid;\n             border-color: #a63700;\n             background-color: rgba(235, 125, 66, 0.3)\">\n    \n<span style = \"font-size: 20px; font-weight: bold\">Note:<\/span> \n<span style=\"font-size: 15px\">If you want to learn more about efficiently creating neat visualisations, please refer to this <a href=\"https:\/\/www.kaggle.com\/suprematism\/house-prices-advanced-visualisation\">notebook<\/a>.<\/span>\n<\/div>","52ac684b":"## 3. Preliminary cleaning <a class=\"anchor\" id = \"I_3\"><\/a>","ec95b9b1":"## 2. Visualising categorical variables <a class=\"anchor\" id = \"III_2\"><\/a>","126ab16d":"<h1><center> III. EDA <\/center><\/h1> <a class=\"anchor\" id = \"III\"><\/a>","5452d0e6":"## 2. Stacking <a class=\"anchor\" id = \"V_2\"><\/a>","23a7a518":"Finally, we can check the number of NaN values left:","31a894ac":"## 2. Imputing NaN values. Training set <a class=\"anchor\" id = \"II_2\"><\/a>","994d9f27":"<h1><center> V. Building models <\/center><\/h1> <a class=\"anchor\" id = \"V\"><\/a>","c9419989":"As far as I can tell, making features look more \"normal\" is not the number one priority, especially if we take into account that we are not doing statistical analysis. We care about how accurate our models are. Thus, I simply used log transformation.","cd99fb70":"## 2. Loading data <a class=\"anchor\" id = \"I_2\"><\/a>","2b4b518b":"## 1. Feature interactions <a class=\"anchor\" id = \"VI_1\"><\/a>","7398e7b7":"The year 2207 was replaced the mode of the column.","3f257449":"## 4. Transforming skewed variables <a class=\"anchor\" id = \"IV_4\"><\/a>","6f035d9c":"### 5.2 One-hot encoding","6c6ca6eb":"<h1><center> VI. Some techniques that could have been useful <\/center><\/h1> <a class=\"anchor\" id = \"VI\"><\/a>","65b463e0":"At this stage, you can drop columns that almost entirely consist of a single class.","bac12464":"## 2. Adding some new variables <a class=\"anchor\" id = \"IV_2\"><\/a>","8a232be6":"<h1><center> I. Getting started <\/center><\/h1> <a class=\"anchor\" id = \"I\"><\/a>","aa2491d2":"Following that, you should create combinations of the previously picked variables and multiply them, for example.","f37b13bf":"### 3.1 \"LotFrontage\"","b646c138":"Label encoder was used to transform categorical data before feeding it to KNN.","a101e56a":"Actually, separating variables by their type had been done before this step through analysing graphs; nevertheless, since EDA is never linear I decided to utilise some pieces of code that had been written beforehand.\n\nI also want to mention that I imputed NaN values in numeric columns using only numeric variables and categorical columns using only categorical variables.","e36431bc":"The process was exactly the same for the test set.","284c23ba":"## 3. Imputing NaN values. Test set <a class=\"anchor\" id = \"II_3\"><\/a>","e6962b43":"## 5. Encoding variables <a class=\"anchor\" id = \"IV_5\"><\/a>","f22391f5":"### 5.3 Ordinal encoding","52545b4f":"## 1. Tuning models <a class=\"anchor\" id = \"V_1\"><\/a>","20b8921e":"## 6. Getting the final training and test sets <a class=\"anchor\" id = \"IV_6\"><\/a>","52c6a7bf":"Sometimes interactions between variables may prove to be valuable. Instead of picking pairs of predictors by hand and crossing them via various mathematical operations, you can automate this process to some extent.\n\nAt first, define what variables you want to use. For instance, you can collect features that are highly correlated with your target:","e79fc7d3":"### 3.2 Other variables (few missing values)"}}