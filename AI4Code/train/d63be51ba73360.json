{"cell_type":{"9b8f1a70":"code","9e7b3cdb":"code","66786071":"code","705f8b1b":"code","83072d0a":"code","b29b81ac":"code","0fe9c4a9":"code","cbb4986c":"code","136784ef":"code","fde23309":"code","6cd5a95a":"code","df97bb7b":"code","c7bde3df":"code","06e100ec":"code","f9f9ea37":"code","edd79be2":"code","fc58f445":"code","286000a1":"code","59ccc482":"code","1ed4a7d0":"code","4020cb63":"code","c71f6c2c":"code","46c38c57":"code","216b0c4c":"code","ea87d810":"code","44e47924":"code","86827403":"code","d88de9ce":"code","76219c54":"code","a21482e0":"code","cbc7d688":"code","a3fc4e49":"code","1eb3bbad":"code","f3efeecf":"code","7414fe19":"code","8b167711":"code","c6809338":"code","ef96dcac":"code","7825d821":"code","8c3dc7b5":"code","f35ef96b":"code","42a04993":"code","c6d21835":"code","fdeb0b77":"code","321e6835":"code","7f9bc188":"code","2961cd58":"code","3f91a776":"code","fca6dcb0":"code","27348b8c":"code","21cfa978":"code","1145f00c":"code","540ce1eb":"code","5a462997":"code","c40c705e":"code","64fcdc93":"code","11953005":"code","23dbef18":"code","69ced8bc":"code","1fbb29b0":"code","c2df778a":"code","cc0acd71":"code","1b3af2f3":"code","539cbb03":"code","4e58d7d2":"code","b9bf441b":"code","3b5a980c":"code","38cbe6de":"code","c0380e43":"code","bf24537a":"code","938d888e":"code","e8f2e591":"code","8bb6066a":"code","6bf572b8":"code","2fb400e1":"code","5bd9fdcf":"code","7cd214a3":"code","6495a871":"code","21355594":"code","1c904fd0":"code","92b885f6":"code","319babdf":"code","3a30049a":"code","4d596a6b":"code","1dd0831c":"code","7a7b9cf7":"code","2b575f8d":"code","49b7e628":"code","525a60ae":"code","b5570420":"code","bdaadf15":"code","704ce3a9":"code","8d5efa4e":"code","d0f17f78":"code","3d277505":"code","11316fc0":"code","4976876d":"code","4f5c1400":"code","99eaea9c":"code","6b9400f5":"code","8caf751c":"code","c44adf29":"code","b4bd3fc7":"code","509f0c38":"code","3800549b":"code","9e595df4":"markdown","959b1618":"markdown","7ea45b41":"markdown","85d1670b":"markdown","cad701ab":"markdown","c8975e01":"markdown","95e674b1":"markdown","0c5eaec2":"markdown","7a0d960e":"markdown","1c8b9a45":"markdown","8c10903f":"markdown","ef2c5b65":"markdown","59d956f4":"markdown","d96b7174":"markdown","11df6c43":"markdown","1e016dc8":"markdown","9fce9b3f":"markdown","3d601ffe":"markdown","c6e7aa6c":"markdown","c9f6f260":"markdown","fdd36303":"markdown","55d1a0fd":"markdown","21d39d94":"markdown","fd3fa570":"markdown","b1e63605":"markdown","6a88b6d4":"markdown","e709f8c3":"markdown","36e888ea":"markdown","61bb74c1":"markdown","a2f15785":"markdown","88208e53":"markdown","a65e0d60":"markdown","e5a1ac2f":"markdown","53a0762f":"markdown","91ec2515":"markdown","0ce1738a":"markdown","aeb17e04":"markdown","c6c91b4e":"markdown","f1d785dc":"markdown","aa7c29f5":"markdown","1dadbde5":"markdown","b24372d4":"markdown","946d11f2":"markdown","f76b1aba":"markdown","1e9780e5":"markdown","a417b149":"markdown","dc83017e":"markdown","e59f41c4":"markdown","0d936624":"markdown","2bcfc012":"markdown","942a4946":"markdown","c0e0282b":"markdown","53791cc9":"markdown","e6d1e8e9":"markdown","80f59c2c":"markdown","e03f2632":"markdown","53534c61":"markdown","9be9ed23":"markdown","25b5d3ab":"markdown","c7a39bdd":"markdown","d9ec5f54":"markdown","13da713d":"markdown","cce5a1e5":"markdown","4e6d5e77":"markdown","1cea34e6":"markdown","e486891e":"markdown","1f1d538b":"markdown","68ef2f59":"markdown","8561dde3":"markdown","1ee2bf30":"markdown","531924d9":"markdown","9838eb06":"markdown","e0dcc5cc":"markdown"},"source":{"9b8f1a70":"# Import Libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport missingno as msno\nimport seaborn as sns\n\nimport os\nimport random\n\nimport random\n\n# Statistics\nfrom scipy.stats import norm\nfrom scipy import stats\n\n\n# Preprocessing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9e7b3cdb":"# Load Dataset\ntrain=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","66786071":"train.head()","705f8b1b":"train.columns","83072d0a":"sample.head()","b29b81ac":"# Description File\nwith open('..\/input\/house-prices-advanced-regression-techniques\/data_description.txt', encoding='utf8') as f:\n    for line in f:\n        print(line.strip())","0fe9c4a9":"train.describe()","cbb4986c":"train.shape","136784ef":"test.describe()","fde23309":"test.shape","6cd5a95a":"train.info()","df97bb7b":"# show only non-zero missing values\ntrain.isnull().sum()[train.isnull().sum()!=0]","c7bde3df":"test.isnull().sum()[test.isnull().sum()!=0]","06e100ec":"features=train.columns\ntotal_houses=train.shape[0]\n\nfull=pd.DataFrame()     # columns with no missing values\nremove=pd.DataFrame()    # columns with >50% missing values which we have to remove\nmedium=pd.DataFrame()    # columns with some missing values\n\nfor feature in features:\n    if train[feature].count() == total_houses:\n        full[feature]=train[feature]\n    elif train[feature].count() >0.5*total_houses:\n        medium[feature]=train[feature]\n    else:\n        remove[feature]=train[feature]\n    ","f9f9ea37":"remove   # Dataframe with feature containg missing values > 50%","edd79be2":"print('Number of Numerical feature: ',end=' ')\nprint(len(train.select_dtypes(include=['number']).columns))        # Return a subset of the DataFrame's columns based on the column dtypes\nprint('Numerical features:')\nprint(train.select_dtypes(include=['number']).columns.values)       #number- int +float","fc58f445":"train.describe(exclude=['O'])     # Describe dataframe excluding Object dtype(categorical variable)","286000a1":"print('Number of Numerical feature: ',end=' ')\nprint(len(train.select_dtypes(include=['O']).columns))        # Return a subset of the DataFrame's columns based on the column dtypes\nprint('Categorical  features:')\nprint(train.select_dtypes(include=['O']).columns.values)       #number- int +float","59ccc482":"plt.figure(figsize=(8,8))\nplt.hist(train['SalePrice'],bins=50)\nplt.title('Sale Prices')\nplt.show()","1ed4a7d0":"print(train['SalePrice'].describe())","4020cb63":"print('Min Sale of the House:',34900)\nprint('Max Sale of the House:',755000)","c71f6c2c":"train.drop(['Id'],axis=1,inplace=True)\n\ntrain.drop(columns=remove,axis=1,inplace=True)","46c38c57":"train","216b0c4c":"# Numerical Feature\ndf_num=train.select_dtypes(include='number')\ndf_num","ea87d810":"# Categorical feature\ndf_cat=train.select_dtypes(include='O')\ndf_cat","44e47924":"df_num.hist(figsize=(20, 25), bins=50, xlabelsize=8, ylabelsize=8); # \n","86827403":"train['PoolArea'].value_counts()","d88de9ce":"# find the rows in column GarageArea whose value is 0\ndf_num.loc[df_num['GarageArea'] == 0, 'GarageArea']","76219c54":"# we want to know the ratio of (values equals zero) \/ 1460\nfeature_zero_ratio = {feature:df_num.loc[df_num[feature] == 0, feature].count() \/ 1460 for feature in df_num.columns.values}\nfeature_zero_ratio","a21482e0":"for feature in df_num.columns.values:\n    if feature_zero_ratio[feature] > 0.40:\n        df_num.drop(columns=feature,axis=1,inplace=True)     # remove feature from df_num,train & medium\n        train = train.drop([feature], axis=1)\n        if feature in medium:\n            medium = medium.drop([feature], axis=1)\n            \nprint(train.shape)\nprint(df_num.shape)","cbc7d688":"df_num_corr = df_num.corr()['SalePrice'][:-1] # -1 because the latest row is SalePrice\ngolden_features_list = df_num_corr[abs(df_num_corr) > 0.5].sort_values(ascending=False)\nprint(\"There is {} strongly correlated values with SalePrice:\\n{}\".format(len(golden_features_list), golden_features_list))","a3fc4e49":"# Threshold for removing correlated variables\nthreshold = 0.80\n\ndef highlight(value):\n    if value > threshold:\n        style = 'background-color: pink'\n    else:\n        style = 'background-color: palegreen'\n    return style\n\n# Absolute value correlation matrix\ncorr_matrix=df_num.corr().abs().round(2)\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# print(upper)\nupper.style.format(\"{:.2f}\").applymap(highlight)","1eb3bbad":"# Select columns with correlations above threshold\ncollinear_feature=[column for column in upper.columns if any(upper[column]>threshold)]\ntrain.drop(columns=collinear_feature,inplace=True)\ndf_num.drop(columns=collinear_feature,inplace=True)\n","f3efeecf":"for i in range(0, len(df_num.columns), 5):\n    sns.pairplot(data=df_num, x_vars=df_num.columns[i:i+5], y_vars=['SalePrice'])","7414fe19":"print('Number of features left in numerical features:',len(df_num.columns))\nprint('Numerical Features left:')\nprint(df_num.columns.values)","8b167711":"df_num.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(df_num.corr(),annot=True, square=True, fmt='.2f', annot_kws={'size':10})","c6809338":"corr_with_price = df_num.corr()\ncorr_with_price = corr_with_price.sort_values(by= 'SalePrice', ascending=False)\ncorr_with_price['SalePrice']","ef96dcac":"numerical_have_missing = pd.DataFrame()\ncategorical_have_missing = pd.DataFrame()\n\n# Numerical\nfor feature in df_num.columns.values:\n    if feature in medium:\n        numerical_have_missing[feature] = df_num[feature]\n      \n    \n# Categorical\nfor feature in df_cat.columns.values:\n    if feature in medium:\n        categorical_have_missing[feature] = df_cat[feature]","7825d821":"print(numerical_have_missing.columns)\n","8c3dc7b5":"df_num['LotFrontage'].describe()","f35ef96b":"df_num['LotFrontage'].hist()","42a04993":"old_LotFrontage=list(numerical_have_missing['LotFrontage'].values)\nmissing_idx=list(numerical_have_missing.loc[ numerical_have_missing['LotFrontage'].isnull(),'LotFrontage'].index)\n\nrand_values=[random.randint(59,80) for i in range(len(missing_idx))]\n\nind=0\nfor idx in missing_idx:\n    old_LotFrontage[idx]=rand_values[ind]\n    ind+=1\n    \nnumerical_have_missing['LotFrontage'] = pd.Series(old_LotFrontage)\ntrain['LotFrontage'] = pd.Series(old_LotFrontage)\n\nprint(numerical_have_missing['LotFrontage'].count())\nprint(train['LotFrontage'].count())","c6d21835":"print(categorical_have_missing.columns.values)","fdeb0b77":"categorical_have_missing.isnull().sum()","321e6835":"print('Total categorical missing values:',len(categorical_have_missing.isnull().sum()))","7f9bc188":"fig, axes = plt.subplots(round(len(categorical_have_missing.columns) \/ 4), 4, figsize=(18, 10))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(categorical_have_missing.columns):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        sns.countplot(x=categorical_have_missing.columns[i], alpha=0.7, data=categorical_have_missing, ax=ax)\n\nfig.tight_layout()      # Adjust the padding between and around subplots.","2961cd58":"categorical_have_missing.drop(['FireplaceQu'],axis=1,inplace=True)\ntrain.drop(['FireplaceQu'],axis=1,inplace=True)","3f91a776":"imputer=SimpleImputer(missing_values=np.NaN,strategy='most_frequent')\nfor feature in categorical_have_missing.columns:\n    categorical_have_missing[feature]=imputer.fit_transform(categorical_have_missing[feature].values.reshape(-1,1))\n    train[feature]=imputer.fit_transform(train[feature].values.reshape(-1,1))\n    ","fca6dcb0":"train.isnull().sum()","27348b8c":"train.shape","21cfa978":"for i in range(0, len(df_num.columns), 5):\n    plt.figure(figsize=(15,15))\n    sns.pairplot(data=df_num, x_vars=df_num.columns[i:i+5], y_vars=['SalePrice'])","1145f00c":"outlier_indices=[]","540ce1eb":"outlier_indices.extend(list(train[train['LotFrontage']>250].index))","5a462997":"plt.scatter(train['LotArea'],train['SalePrice'])","c40c705e":"outlier_indices.extend(list(train[train['LotArea']>150000].index))","64fcdc93":"outlier_indices.extend(list(train[(train['BsmtFinSF1']>4000) | (train['TotalBsmtSF']>4000) | ((train['GrLivArea']>4000) & (train['SalePrice']<400000))].index))","11953005":"outlier_indices     ","23dbef18":"train.drop(train.index[outlier_indices],inplace=True)","69ced8bc":"train","1fbb29b0":"train.reset_index(drop=True,inplace=True)","c2df778a":"    sns.distplot(train['SalePrice'],fit=norm)\n    plt.figure()\n    res=stats.probplot(train['SalePrice'],plot=plt)","cc0acd71":"train['SalePrice']=np.log(train['SalePrice'])","1b3af2f3":"    sns.distplot(train['SalePrice'],fit=norm)\n    plt.figure()\n    res=stats.probplot(train['SalePrice'],plot=plt)","539cbb03":"#histogram and normal probability plot\nsns.distplot(train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['GrLivArea'], plot=plt)","4e58d7d2":"#data transformation\ntrain['GrLivArea'] = np.log(train['GrLivArea'])","b9bf441b":"#histogram and normal probability plot\nsns.distplot(train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['GrLivArea'], plot=plt)","3b5a980c":"from sklearn.preprocessing import LabelEncoder","38cbe6de":"df_cat=train.select_dtypes(include='O')","c0380e43":"le=LabelEncoder()\nfor feature in df_cat.columns.values:\n    df_cat[feature]=le.fit_transform(df_cat[feature])\n    train[feature]=le.fit_transform(train[feature])\n","bf24537a":"train.info()","938d888e":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor","e8f2e591":"target = train['SalePrice']\ntrain = train.drop(['SalePrice'], axis=1)\n\nX, y = train, target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","8bb6066a":"lr=LinearRegression()\nlr.fit(X_train,y_train)\ny_pred=lr.predict(X_test)\nlr.score(X_train,y_train)           # Return the coefficient of determination R^2 of the prediction","6bf572b8":"np.sqrt(mean_squared_log_error(y_pred, y_test))","2fb400e1":"y_test","5bd9fdcf":"plt.figure()\nplt.scatter(x=list(range(0, 291)),y= y_test, color='blue')         \nplt.scatter(x=list(range(0, 291)),y= y_pred, color='black')\nplt.show()","7cd214a3":"pip install bayesian-optimization","6495a871":"from bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import cross_val_score","21355594":"#Bayesian optimization\ndef bayesian_optimization(dataset, function, parameters):\n   X_train, y_train, X_test, y_test = dataset\n   n_iterations = 10\n   gp_params = {\"alpha\": 1e-4}\n\n   BO = BayesianOptimization(function, parameters)\n   BO.maximize(n_iter=n_iterations, **gp_params)\n\n   return BO.max\n","1c904fd0":"def rfc_optimization(cv_splits):\n    def function(n_estimators, max_depth, min_samples_split):\n        return cross_val_score(\n               RandomForestRegressor(\n                   n_estimators=int(max(n_estimators,0)),                                                               \n                   max_depth=int(max(max_depth,1)),\n                   min_samples_split=int(max(min_samples_split,2)), \n                   n_jobs=-1, \n                   random_state=42),  \n               X=X_train, \n               y=y_train, \n               cv=cv_splits,\n               scoring='neg_mean_squared_log_error',\n               n_jobs=-1).mean()\n\n    parameters = {\"n_estimators\": (10, 1000),\n                  \"max_depth\": (1, 150),\n                  \"min_samples_split\": (2, 20)}\n    \n    return function, parameters","92b885f6":"#Train model\ndef train_rf(X_train, y_train, X_test, y_test, function, parameters):\n    dataset = (X_train, y_train, X_test, y_test)\n#     cv_splits = 4\n    \n    best_solution = bayesian_optimization(dataset, function, parameters)      \n    params = best_solution[\"params\"]\n\n    model = RandomForestRegressor(\n             n_estimators=int(max(params[\"n_estimators\"], 0)),\n             max_depth=int(max(params[\"max_depth\"], 1)),\n             min_samples_split=int(max(params[\"min_samples_split\"], 2)), \n             n_jobs=-1, \n             random_state=42)\n\n    model.fit(X_train, y_train)\n    \n    return model","319babdf":"func_rf,param_rf=rfc_optimization(10)\nrf_bo=train_rf(X_train, y_train, X_test, y_test, func_rf, param_rf)","3a30049a":"rf_bo.get_params","4d596a6b":"y_pred=rf_bo.predict(X_test)","1dd0831c":"print(y_pred.shape)","7a7b9cf7":"print('Root Mean Squared Error in Random Forest:', np.sqrt(mean_squared_log_error(y_test, y_pred)))","2b575f8d":"pred=rf_bo.predict(X_test)","49b7e628":"print('Root Mean Squared Error in Random Forest:', np.sqrt(mean_squared_log_error(y_test, pred)))","525a60ae":"importances=rf_bo.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf_bo.estimators_],axis=0)\nindices = np.argsort(importances)","b5570420":"# Plot the feature importances of the forest\nplt.figure(figsize=(20,15))\nplt.title(\"Feature importances\")\nplt.barh(range(X_train.shape[1]), importances[indices],\n       color=\"r\", xerr=std[indices], align=\"center\")\n# If you want to define your own labels,\n# change indices to a list of labels on the following line.\nplt.yticks(range(X.shape[1]), indices)\nplt.ylim([-1, X.shape[1]])\nplt.show()","bdaadf15":"plt.figure(figsize=(20,15))\n(pd.Series(rf_bo.feature_importances_, index=X_train.columns).plot(kind='barh',color='red')) ","704ce3a9":"def xgb_optimization(cv_splits, eval_set):\n    def function(eta, gamma, max_depth,n_estimators):\n            return cross_val_score(\n                       XGBRegressor(\n                       objective=\"reg:squaredlogerror\",\n                       n_estimators=int(max(n_estimators,0)),\n                       learning_rate=max(eta, 0),\n                       gamma=max(gamma, 0),\n                       max_depth=int(max_depth),                                               \n                       seed=42,\n                       n_jobs=-1),  \n                   X=X_train, \n                   y=y_train, \n                   cv=cv_splits,\n                   scoring=\"neg_mean_squared_log_error\",\n#                    fit_params={\n#                         \"early_stopping_rounds\": 10, \n#                         \"eval_metric\":\"reg:squaredlogerror\", \n#                         \"eval_set\": eval_set},\n                   n_jobs=-1).mean()\n\n    parameters = {\"eta\": (0.001, 0.4),\n                  \"gamma\": (0, 20),\n                  \"max_depth\": (1, 2000),\n                  \"n_estimators\":(10,1000)}\n    \n    return function, parameters","8d5efa4e":"def train_xgb(X_train, y_train, X_test, y_test, function, parameters):\n    dataset = (X_train, y_train, X_test, y_test)\n#     cv_splits = 4\n    \n    best_solution = bayesian_optimization(dataset, function, parameters)      \n    params = best_solution[\"params\"]\n\n    model = XGBRegressor(\n             n_estimators=int(max(params[\"n_estimators\"], 0)),\n             max_depth=int(max(params[\"max_depth\"], 1)),\n             \n             n_jobs=-1, \n             random_state=42)\n\n    model.fit(X_train, y_train)\n    \n    return model","d0f17f78":"func_xgb,param_xgb=xgb_optimization(10,y_train)\nxgb_bo=train_xgb(X_train, y_train, X_test, y_test, func_xgb, param_xgb)","3d277505":"predXGB=xgb_bo.predict(X_test)","11316fc0":"print('Root Mean Squared Error in Random Forest:', np.sqrt(mean_squared_log_error(y_test, predXGB)))","4976876d":"print(test.info())","4f5c1400":"ids = test['Id']","99eaea9c":"test= test[train.columns.values]\n# Delete Id again\n# train= train.drop(['Id'], axis=1)","6b9400f5":"test","8caf751c":"# Let's code it\nold_LotFrontage = list(test['LotFrontage'].values)\nmissing_indices = list(test.loc[test['LotFrontage'].isnull(), 'LotFrontage'].index)\nrandom_values = [random.randint(60, 80) for _ in range( 1460 - test['LotFrontage'].count() ) ]\nrandom_values_idx = 0\n\nfor missing_idx in missing_indices:\n        \n    old_LotFrontage[missing_idx] = random_values[random_values_idx]\n    random_values_idx += 1\n        \n\ntest['LotFrontage'] = pd.Series(old_LotFrontage)\nprint(test['LotFrontage'].count())","c44adf29":" imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\nfor feature in test.drop(['LotFrontage'], axis=1):\n    \n    test[feature] = imputer.fit_transform(test[feature].values.reshape((-1, 1)))\n    test[feature] = imputer.fit_transform(test[feature].values.reshape((-1, 1)))\n    \nle=LabelEncoder()\nfor feature in df_cat.columns.values:\n    test[feature]=le.fit_transform(test[feature])\nprint(test.isnull().sum().max())\nprint(test.shape)","b4bd3fc7":"rest = set(X.columns.values) - set(list(test.columns.values))\nfor feature in list(rest):\n    test[feature] = 0","509f0c38":"\n# test = test.drop(['Id'], axis=1)\nourPred = xgb_bo.predict(test)\nsubmission = pd.DataFrame({\n        \"Id\": ids,\n        \"SalePrice\": ourPred\n    })\nsubmission.to_csv('submission.csv', index=False)","3800549b":"submission","9e595df4":"Remove some of the Varibales which are highly correlated.","959b1618":"# Handling Missing Data","7ea45b41":"TRAIN MISSING VALUES","85d1670b":"**Hurray! we have completely remove outliers.**","cad701ab":"Now, after doing Log Transformation Let's check again.","c8975e01":"# House Prices - Advanced Regression Techniques ","95e674b1":"### Missing Values","0c5eaec2":"Thanks to utility function bayesian optimization is much more efficient in tuning parameters of machine learning algorithms than grid or random search techniques. It can effectively balance \u201cexploration\u201d and \u201cexploitation\u201d in finding global optimum.\n\nTo present Bayesian optimization in action we use BayesianOptimization library written in Python to tune hyperparameters of Random Forest and XGBoost classification algorithms. We need to install it via pip:","7a0d960e":"* `GarageYrBlt` & `YearBuilt`, `TotalBsmtSF` & `1stFlrSF` , `GrLivArea` & `TotalRmsAbvGrd`, `GarageArea` & `Garagecars` all these Pairs are highly Correlated so we can remove one of the feature from these pairs.","1c8b9a45":"# Submission","8c10903f":"This shows Pool area of most of House=0, It means house doesn'y have any pool.","ef2c5b65":"So, Now we hae 2 dataframes numerical_having_missing_values, categorical_having_missing values which contain columns with missing value for numerical and categorical variable respecctively.","59d956f4":"### INFERENCE MADE TILL NOW","d96b7174":"**Now, Lets Come to categorical variable missing values**","11df6c43":" Who is 'SalePrice'?\n\nThe answer to this question lies in testing for the assumptions underlying the statistical bases for multivariate analysis. We already did some data cleaning and discovered a lot about 'SalePrice'. Now it's time to go deep and understand how 'SalePrice' complies with the statistical assumptions that enables us to apply multivariate techniques.\n\nAccording to Hair et al. (2013), four assumptions should be tested:\n\n* Normality - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.\n\n* Homoscedasticity - I just hope I wrote it right. Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' (Hair et al., 2013). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.\n\n* Linearity- The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.\n\n* Absence of correlated errors - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.\n\n","1e016dc8":"**Outliers** are of 2 types Univariate and Multivariate Outlier.\n* Univariate: These outliers can be found when we look at distribution of a single variable.(Boxplot)\n* Multivariate outliers- are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions.(scatterplot).\n","9fce9b3f":"***What's The Next Step?***","3d601ffe":"Since `FireplaceQu` has 690 missing value so it is better to remove this and for rest of the categorical variable fill the missing value with mode.","c6e7aa6c":"Now, we will use Random Forest regressor with Bayesian optimization for hyperparameter tuning.","c9f6f260":" Check Missing values","fdd36303":"So, from the above scatterplot of each feature with Sales we can see that most of the features are linearly Correlated with SalePrice.","55d1a0fd":"**Ways of imputing missing values:**\n\n* numerical\n1. Deletion of rows of missing value\n2. give it the value of the median \n3. give it the value of the mean\n4. give it a random value from (mean - std) to (mean + std).\n\n* categorical\n1. give it the value of the mode (the most appeared value)\n2. do some analysis more, then do step 1 to decrease the ratio of the wrong values\n\n* Some advanced methods of imputation\n1. Prediction Model - Make a ML Model to predict the values.\n2. KNN Imputation","21d39d94":"# XGBOOST MODEL","fd3fa570":"Let's drop the Id Feature & remove features","b1e63605":"Majority of the Sale Price of house lies between 100000 - 200000","6a88b6d4":"Perfect, we now have a list of strongly correlated values but this list is incomplete as we know `that correlation is affected by outliers`. So we could proceed as follow:\n\nPlot the numerical features and see which ones have very few or explainable outliers\nRemove the outliers from these features and see which one can have a good correlation without their outliers\nBtw, correlation by itself does not always explain the relationship between data so ploting them could even lead us to new insights and in the same manner, check that our correlated values have a linear relationship to the SalePrice.\n\nFor example, relationships such as curvilinear relationship cannot be guessed just by looking at the correlation value so lets take the features we excluded from our correlation table and plot them to see if they show some kind of pattern.","e709f8c3":"**Last thing from these graphs and heatmaps: we will remove the neutral features**\n\nNeutral Features : features have a relationship with 'SalePrice' in this range [-0.1, 0.2]\n","36e888ea":"* **remove**- contain the number of features to be removed from data frame\n* **full**- full contain number of features which is full have no missing values.\n* **medium**- medium contain features which have some missinf values (less then 50%).\n* We have seen there are total of 38 Numerical features and 43 Categorical Fetaures\n* **ID**- ID is unnecessary feature we Don't need it.\n","61bb74c1":"# Label Encoding (Change categorical variable into Label encoding)","a2f15785":"### Remove Features","88208e53":"Now, let's plot each category variable and try to impute the missing value.","a65e0d60":"Some of the output points are almost overlapping with the predicted outputs. Some are close but not overlapping.","e5a1ac2f":"* We have Explored the various types of Numerical feature by plotting it.\n* Remove the Numerical features in which Most of the values are Zero indicating the feature is not present in house.\n* Draw a correlation matrix and remove the one of the feature from the Pairs which are highy correlated with each other.\n* Reduce the Number of Numerical featueres from 38 to 18.","53a0762f":"# Outliers ","91ec2515":"![image.png](attachment:2e9d4cf6-561c-41ca-8a6f-c6a131728768.png)","0ce1738a":"# Some Statistics","aeb17e04":"As We can see from above scatter plots we have some outliers and since there are few we are gone a delete them.\n* Lotfrontage - 2 outliers\n* LotArea- Let's consider the rightmost 3 dots as outliers.\n* BsmtFinSF1 - 1 outlier\n* TotalBsmtSF - 1 outlier\n* GrLivArea- Righmost 2 dots with lower SalePrice are outliers.","c6c91b4e":"Right Skewed Sales","f1d785dc":"So let's fill missing value with random number number b\/w 25%(i.e 59) & 75%(i.e 80).","aa7c29f5":"`Remove features whose feature zero ratio>0.40 (only 40% zeros are allowed)`","1dadbde5":"It's time to deal with outliers.<br>\nNow, Question arises how?\n\n**Method of removing outlier**\n* Deleting observations\n* Transform and binning values- Transforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. \n* Imputing - Like imputing  missing values we can also impute outliers with mean, median & mode.\n* Treat separately: If there are significant number of outliers, we should treat them separately in 3 the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output.\n\n","b24372d4":"Similarly find for all featues","946d11f2":"* As you can see from above histograms some Features like Pool Area, MiscVal, ScreenPorch,3SsnPorof thech and many others have most of the values zero.\n* These feature whose most of the values is zero doesn't contribute much to training of the model(less important).\n* We remove those features who have >40% values is zero.","f76b1aba":"**According to Data Description Document text:** <br>\n**LotFrontage:** Linear feet of street connected to property\n","1e9780e5":"NOW, Each variable is converted to numeric type","a417b149":"I would like to thank [this Notebook](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) for this amazing statistical analysis.","dc83017e":"First, Lets also check How Each feature is correlated to `SalePrice`.","e59f41c4":"* Histogram - Kurtosis and skewness.\n* Normal probability plot - Data distribution should closely follow the diagonal that represents the normal distribution.","0d936624":"THANKS TO:\n* https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python#5.-Getting-hard-core\n* https:\/\/www.kaggle.com\/mostafaalaa123\/simple-house-prediction\/notebook#ML \n    ","2bcfc012":"If You like the notebook please **Upvote**","942a4946":"***Let's Plot all the Numerical Features***","c0e0282b":"### What we have Till Now \ud83e\udd14","53791cc9":"test.head()","e6d1e8e9":"# Multivariate Linear regressor","80f59c2c":"`Categorical`","e03f2632":"TEST MISSING VALUES","53534c61":"# Exploratory Data Analysis","9be9ed23":"### Plot the original y and the predicted output \u2018y_pred\u2019","25b5d3ab":"![image.png](attachment:be8c0367-b31e-49c5-bdb0-4de960a0bfcb.png)","c7a39bdd":"# Let's do Model Training","d9ec5f54":"We are going to do Exploratory Data Analysis on House Price Data.\n\nWhat You will learn?\n* Univariate Analysis\n* Bivariate analysis\n* Missing value Treatment\n* Outlier detecion and treatment\n* Dealing with categorical and Numerical variable.\n* Feature Selection","13da713d":"**Label Encoder**: It is used to transform non-numerical labels to numerical labels (or nominal categorical variables). Numerical labels are always between 0 and n_classes-1. ","cce5a1e5":"# Random Forest regressor","4e6d5e77":"Ok, 'SalePrice' is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line.\n\nBut everything's not lost. A simple data transformation can solve the problem. This is one of the awesome things you can learn in statistical books: in case of positive skewness, log transformations usually works well.","1cea34e6":"Now, there are lots of Features. Let's remove `unimportant feature(like ID)` and `Feature with missing values >50%`.","e486891e":"#### IF YOU GAIN SOME KNOWLEDGE PLEASE UPVOTE THE NOTEBOOK.","1f1d538b":"Now, Sale Price follow Normal distribution.","68ef2f59":"`NUMERICAL`","8561dde3":"No Missing values & removed unwanted features","1ee2bf30":" # Correlation ","531924d9":"#### Let's Look at the missing Values and try to impute it.","9838eb06":"* First we have to Explore each feature.\n* Look how the Sales is Distributed.\n* Find a Way to fill missing values & remove the features present in remove dataframe.\n* Perform EDA & take out the insight from the data.","e0dcc5cc":"### Numerical and Categorical feature"}}