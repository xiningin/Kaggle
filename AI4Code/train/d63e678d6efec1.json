{"cell_type":{"ef66cd0f":"code","a17e7836":"code","2f621367":"code","af209774":"code","836ad3c4":"code","dea04937":"code","33467877":"code","2151ab7b":"code","5e4c76cc":"code","0c49e270":"code","36e80e20":"code","d8045b5b":"code","64a497cb":"code","4ddc30b4":"code","368af6d3":"code","1aba7411":"code","1277e40a":"code","6044d12f":"code","0c20b72c":"code","6eb02f01":"code","47524e74":"code","b7f092d1":"code","8f1d37b2":"code","7e8cfe07":"code","e12babb1":"code","b4244445":"code","1a4a5326":"code","fb2560f5":"code","38fecf87":"code","503a06d7":"code","bb2bdbcb":"code","ad185e64":"code","84b7492e":"code","6dd5ddc1":"code","32a0a31d":"code","e6378089":"code","fe975a94":"code","cd9c184d":"code","aaebe21d":"code","293df1dc":"code","9c3114f1":"code","a103942f":"code","7d559efc":"code","03f3ec7f":"code","5290f1bd":"code","cd3b618d":"code","def23ff1":"code","d405e478":"markdown","4782ead1":"markdown","7122b14c":"markdown","eb394d12":"markdown","517e6f19":"markdown","cd460d86":"markdown","81a5991e":"markdown","2bf577d2":"markdown","fe0d36c7":"markdown","f02e0662":"markdown","af316bf0":"markdown","29637d7e":"markdown","161c7645":"markdown","8a36ef24":"markdown","e9e821bd":"markdown","d3a0c182":"markdown","ca5398f6":"markdown","0d725af8":"markdown","25a2b133":"markdown","70bb42ad":"markdown","ac8ef9d0":"markdown","35b19cd1":"markdown","9eb118e5":"markdown","21927726":"markdown","629dbb49":"markdown","bc1df8f1":"markdown","479ba91c":"markdown","6c312dbe":"markdown","3107b385":"markdown","dd0b6b6e":"markdown","cddc09b4":"markdown","17c09435":"markdown","204c75f4":"markdown","e2dc459a":"markdown","8544c3e9":"markdown","cfc79319":"markdown","67777bd6":"markdown","ae7c802a":"markdown","c0ddea95":"markdown","a50d44b8":"markdown","31a253b9":"markdown","827f1847":"markdown","68018c73":"markdown","5ae3bb2f":"markdown","800b4801":"markdown","ad2bd749":"markdown","c15cb59c":"markdown","313b84a5":"markdown","c0867898":"markdown","38b8856e":"markdown","d535323b":"markdown","d9d531e2":"markdown"},"source":{"ef66cd0f":"%matplotlib inline\n#%load_ext rpy2.ipython #loads ipython R Kernel\n#!pip install pandas-profiling[notebook] --upgrade #upgrade pandas profiling to stable version\n#from google.colab import drive\n#DRIVE_DIR = ''\n#drive.mount('\/content\/drive')","a17e7836":"#create subdirectories and load the dataset from kaggle \n#import os\n\nproject = 'RainAustralia\/'\nkaggle = '\/content\/drive\/MyDrive\/kaggle\/'\n#os.environ['KAGGLE_CONFIG_DIR'] = kaggle\n#!kaggle datasets download -d jsphyg\/weather-dataset-rattle-package \n","2f621367":"#import neccessary libraries and create the dataframe from source\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n#from pandas_profiling import ProfileReport\nfrom sklearn import tree\n#use liblinear svm for linear scaling complexity\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n#ignore SVC Convergence Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\n#profile = ProfileReport(df, title=\"Rain in Australia\", explorative=True)\n#profile.to_file('report.html')","af209774":"df.head()","836ad3c4":"#Convert data to more data type\ndf['Date'] = pd.to_datetime(df['Date'])\ndf = df.replace('Yes', 1)\ndf = df.replace('No', 0)\ndf = df[df['RainTomorrow'].notna()]\ndf['RainTomorrow'] = df['RainTomorrow'].astype('int')","dea04937":"df.isna().sum()","33467877":"#Get a dataset for every location and use the date as unique index\ndfs = {}\nfor line in set(df['Location'].values):\n   dfs[line] = df[df['Location'] == line]\n   dfs[line] = dfs[line].set_index('Date')\n   dfs[line] = dfs[line].drop('Location', axis=1)\nkeys = dfs.keys()","2151ab7b":"%matplotlib inline\nsns.set_theme(style=\"whitegrid\")\nsns.set(rc={'figure.figsize':(16,9)})\n\n#plot rain to no rain ratio by location\nrain_ratio = [dfs[line]['RainToday'].value_counts()[1] \/ (dfs[line]['RainToday'].value_counts()[0] + dfs[line]['RainToday'].value_counts()[1]) for line in keys ]\ndata = pd.DataFrame({'Percentage': rain_ratio, 'Location': keys})\ndata = data.sort_values(by=['Percentage'], ascending=False)\n\nsns.barplot(x='Percentage',y='Location', data=data).set_title('Rain to no rain ratio by location')","5e4c76cc":"#Plot missing values by location\nna_values = [ dfs[line].isna().sum().sum() for line in keys]\nvalues = [dfs[line].shape[0] * dfs[line].shape[1] * 100 for line in keys]\ndata = pd.DataFrame({'Location': keys, 'Count_NA' : na_values, 'Count' : values })\ndata['Percentage'] = data['Count_NA'] \/ data['Count']\ndata = data.sort_values(by=['Percentage'])\n\n\nplt.hlines(y=data['Location'], xmin = 0, xmax=data['Percentage'], color='skyblue')\nplt.plot(data['Percentage'], data['Location'], \"D\")\nplt.yticks(data['Location'])\nplt.title('Percentage of missing values by location')\nplt.show()\n","0c49e270":"sample = 'MelbourneAirport'\nstart = pd.Timestamp(2015,7,1)\nend = pd.Timestamp(2017,7,1)","36e80e20":"fig, axes = plt.subplots(2, 2, figsize=(20, 10))\nsns.scatterplot(data = dfs[sample].loc[dfs[sample].index > start] , x='Date', y='Temp3pm', hue='RainTomorrow', ax=axes[0,0])\nsns.scatterplot(data = dfs[sample].loc[dfs[sample].index > start] , x='Date', y='Pressure3pm', hue='RainTomorrow', ax=axes[0,1])\nsns.scatterplot(data = dfs[sample].loc[dfs[sample].index > start] , x='Date', y='Sunshine', hue='RainTomorrow', ax=axes[1,0])\nsns.scatterplot(data = dfs[sample].loc[dfs[sample].index > start] , x='Date', y='Humidity3pm', hue='RainTomorrow', ax=axes[1,1])\n","d8045b5b":"fig, axes = plt.subplots(1,2, figsize=(20, 10))\nsns.boxplot(data = dfs[sample].loc[dfs[sample].index > start] , y='Sunshine', x='RainTomorrow', ax=axes[0])\nsns.boxplot(data = dfs[sample].loc[dfs[sample].index > start] , y='Cloud3pm', x='RainTomorrow', ax=axes[1])","64a497cb":"fig, axes = plt.subplots(1,2, figsize=(20, 10))\nsns.boxplot(data = dfs[sample].loc[dfs[sample].index > start] , y='Humidity3pm', x='RainTomorrow', ax=axes[0])\nsns.boxplot(data = dfs[sample].loc[dfs[sample].index > start] , y='Pressure3pm', x='RainTomorrow', ax=axes[1])","4ddc30b4":"order = dfs[sample]['WindGustDir'].value_counts()\norder = order.keys()\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 9))\nsns.countplot(data = dfs[sample].loc[dfs[sample].index > start] , x='WindDir3pm', ax=axes[0], hue='RainTomorrow', order=order)\nsns.countplot(data = dfs[sample].loc[dfs[sample].index > start] , x='RainToday', hue='RainTomorrow', ax=axes[1])","368af6d3":"sns.pairplot(data = dfs[sample].loc[dfs[sample].index > start][['Temp9am', 'MaxTemp', 'Pressure3pm', 'Humidity3pm', 'Rainfall','RainTomorrow']], hue='RainTomorrow')","1aba7411":"#function to onehot encode every category\ndef get_Xy_oh(_df):\n  data = _df.dropna().copy()\n  data['RainToday'] = data['RainToday'].astype('int')\n  X = data.drop(['RainTomorrow','Date'], axis=1)\n  y = data['RainTomorrow']\n\n  for item in ['Location','WindGustDir','WindDir9am','WindDir3pm']:\n    temp = pd.get_dummies(X[item],prefix=item[4:])\n    X = X.join(temp)\n    X = X.drop(item, axis=1)\n\n  return train_test_split(X, y, test_size=0.2, random_state=42)","1277e40a":"#function to label encode every category\ndef get_Xy_le(_df):\n  le = LabelEncoder()\n  data = _df.dropna().copy()\n  data['RainToday'] = data['RainToday'].astype('int')\n  X = data.drop(['RainTomorrow','Date'], axis=1)\n  y = data['RainTomorrow']\n\n  for item in ['Location','WindGustDir','WindDir9am','WindDir3pm']:\n    X[item] = le.fit_transform(X[item])\n\n  return train_test_split(X, y, test_size=0.2, random_state=42)","6044d12f":"%%time\n#Onehot Encoding\nX_train, X_test, y_train, y_test = get_Xy_oh(df)\nscaler = StandardScaler()\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\nscore_oh_dct = [cross_val_score(tree.DecisionTreeClassifier(max_depth=i), X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean() for i in range(1,13)]\nscore_oh_svc = [cross_val_score(LinearSVC(C = C,), X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean() for C in [1,10,100] ]\nscore_oh_knn = [cross_val_score(KNeighborsClassifier(n_neighbors=n), X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean() for n in range(1,9,2) ]\n\n#Label Encoding\nX_train, X_test, y_train, y_test = get_Xy_le(df)\nscaler = StandardScaler()\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\nscore_le_dct = [cross_val_score(tree.DecisionTreeClassifier(max_depth=i), X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean() for i in range(1,13)]\nscore_le_svc = [cross_val_score(LinearSVC(C = C,), X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean() for C in [1,10,100] ]\nscore_le_knn = [cross_val_score(KNeighborsClassifier(n_neighbors=n), X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean() for n in range(1,9,2) ]","0c20b72c":"#plot everything\nfig, axes = plt.subplots(1,3)\n\naxes[0].plot(range(1,13), score_oh_dct, label=\"DCT OneHot\")\naxes[0].plot(range(1,13), score_le_dct, label=\"DCT LabelEnc\")\naxes[0].set_title('DCT Depth \/ Train Accuracy')\naxes[0].legend()\naxes[1].plot([1,10,100], score_oh_svc, label=\"SVC OneHot\")\naxes[1].plot([1,10,100], score_le_svc, label=\"SVC LabelEnc\")\naxes[1].set_title('SVC C Value \/ Train Accuracy')\naxes[1].legend()\naxes[2].plot(range(1,9,2),score_oh_knn, label=\"KNN OneHot\")\naxes[2].plot(range(1,9,2),score_le_knn, label=\"KNN LabelEnc\")\naxes[2].set_title('KNNC Neighbors \/ Train Accuracy')\naxes[2].legend()","6eb02f01":"#import graphviz \n#clf = tree.DecisionTreeClassifier(max_depth=5).fit(X_train, y_train)\n#dot_data = tree.export_graphviz(clf, out_file=None,filled=True, feature_names=X_train.columns, class_names=['NoRain','Rain']) \n#graph = graphviz.Source(dot_data, format='png') \n#graph.render(\"RainTomorrow\") ","47524e74":"%%time\n#Onehot encoding\nX_train, X_test, y_train, y_test = get_Xy_oh(df)\n\nscaler = StandardScaler()\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\nn_estimators = [10,100,500]\n\nscore_oh_rf = [cross_val_score(RandomForestClassifier(class_weight='balanced', n_estimators=i), X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean() for i in n_estimators]\nscore_oh_ada = [cross_val_score(AdaBoostClassifier(n_estimators = i), X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean() for i in n_estimators ]\n#if one has a gpu, the parameter tree_method='gpu_hist' can reduce computation time significantly\nscore_oh_xgb = [cross_val_score(XGBClassifier(use_label_encoder=False, tree_method='gpu_hist', n_estimators=i, n_jobs=-1, verbosity = 0), X_train, y_train, cv=5, scoring='accuracy').mean() for i in n_estimators]\n\n#Label encoding\nX_train, X_test, y_train, y_test = get_Xy_le(df)\n\nscaler = StandardScaler()\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\n\nscore_le_rf = [cross_val_score(RandomForestClassifier(class_weight='balanced', n_estimators=i), X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean() for i in n_estimators]\nscore_le_ada = [cross_val_score(AdaBoostClassifier(n_estimators = i), X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean() for i in n_estimators ]\nscore_le_xgb = [cross_val_score(XGBClassifier(use_label_encoder=False, tree_method='gpu_hist', n_estimators=i, n_jobs=-1, verbosity = 0), X_train, y_train, cv=5, scoring='accuracy').mean() for i in n_estimators]","b7f092d1":"fig, axes = plt.subplots(1,3)\n\naxes[0].plot(n_estimators, score_oh_rf, label=\"RF OneHot\")\naxes[0].plot(n_estimators, score_le_rf, label=\"RF LabelEnc\")\naxes[0].set_title('RF Estimators \/ Accuracy')\naxes[0].legend()\naxes[1].plot(n_estimators, score_oh_ada, label=\"ADA OneHot\")\naxes[1].plot(n_estimators, score_le_ada, label=\"ADA LabelEnc\")\naxes[1].set_title('ADA Estimators \/ Accuracy')\naxes[1].legend()\naxes[2].plot(n_estimators,score_oh_xgb, label=\"XGB OneHot\")\naxes[2].plot(n_estimators,score_le_xgb, label=\"XGB LabelEnc\")\naxes[2].set_title('XGB Estimators \/ Accuracy')\naxes[2].legend()","8f1d37b2":"#Onehot encoding\nX_train, X_test, y_train, y_test = get_Xy_oh(df)\n\nscaler = StandardScaler()\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\n\nbase = cross_val_score(XGBClassifier(use_label_encoder=False, tree_method='gpu_hist', n_estimators=500), X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean()","7e8cfe07":"base","e12babb1":"df_missing = df.copy()\ncols = df.columns\nnumerical = df_missing._get_numeric_data().columns\ncat = list(set(cols) - set(numerical))\n\nfor var in numerical:\n   df_missing[var]= df_missing[var].fillna(df_missing[var].mean()) \n\nfor var in cat:\n   df_missing[var]=df_missing[var].fillna(df_missing[var].mode()[0]) ","b4244445":"X_train, X_test, y_train, y_test = get_Xy_oh(df_missing)\nprint(base, cross_val_score(XGBClassifier(tree_method='gpu_hist', n_estimators=500, use_label_encoder=False, verbosity = 0), X_train, y_train, cv=5, scoring='accuracy').mean())","1a4a5326":"le = LabelEncoder()\ndata = df.copy()\ndata = data.drop('Date', axis=1)\n\nfor item in ['Location','WindGustDir','WindDir9am','WindDir3pm']:\n  data[item] = data[item].fillna('NaN')\n  data[item] = le.fit_transform(data[item])\n\nsample = data.sample(frac=0.1,random_state=42)\n\nimputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\nimputer.fit(sample)\ndata = imputer.transform(data)","fb2560f5":"data = pd.DataFrame(data)\ndata.columns = df.drop('Date',axis=1).columns\ndata['Date'] = df['Date']\ndata.head()","38fecf87":"X_train, X_test, y_train, y_test = get_Xy_oh(data)\nprint(base, cross_val_score(XGBClassifier(tree_method='gpu_hist', n_estimators=500, use_label_encoder=False, verbosity = 0), X_train, y_train, cv=5, scoring='accuracy').mean())","503a06d7":"def get_season(date):\n  month=date.month\n  day=date.day\n  if month in (1, 2, 3):\n    season = 2\n  elif month in (4, 5, 6):\n    season = 3\n  elif month in (7, 8, 9):\n    season = 4\n  else:\n    season = 1\n  if (month == 3) and (day > 19):\n    season = 3\n  elif (month == 6) and (day > 20):\n    season = 4\n  elif (month == 9) and (day > 21):\n    season = 1\n  elif (month == 12) and (day > 20):\n    season = 2\n  return(season)","bb2bdbcb":"df_season = df.dropna().copy()\ndf_season['Season'] = df_season['Date'].apply(get_season)\nX_train, X_test, y_train, y_test = get_Xy_oh(df_season)\nprint(base, cross_val_score(XGBClassifier(n_estimators=500, tree_method='gpu_hist', use_label_encoder=False, verbosity = 0), X_train, y_train, cv=5, scoring='accuracy').mean())","ad185e64":"df_norain = df.dropna().copy()\ndf_norain[\"NoRainFor\"] = df_norain[\"Date\"] - df_norain[\"Date\"].where(df_norain[\"RainToday\"].astype('boolean')).groupby(df_norain[\"Location\"]).ffill()\ndf_norain[\"NoRainFor\"] = df_norain[\"NoRainFor\"].fillna(pd.Timedelta(seconds=0))\ndf_norain[\"NoRainFor\"] = df_norain[\"NoRainFor\"].dt.days.astype(\"int\")","84b7492e":"X_train, X_test, y_train, y_test = get_Xy_oh(df_norain)\nprint(base , cross_val_score(XGBClassifier(n_estimators=500, tree_method='gpu_hist', use_label_encoder=False,verbosity = 0), X_train, y_train, cv=5, scoring='accuracy').mean())","6dd5ddc1":"data = df.dropna().copy()\ncnames = data.columns  \nX_train, X_test, y_train, y_test = get_Xy_le(data)\nclf = XGBClassifier(n_estimators=500, tree_method='gpu_hist', use_label_encoder=False, verbosity = 0).fit(X_train, y_train)\nimportances = clf.feature_importances_\nindices = np.argsort(importances)[::-1]\nfeature_names = X_train.columns\nfor i in range(X_train.shape[1]):\n    print(f\"{feature_names[i]} - {importances[indices[i]]}\")","32a0a31d":"data = data.drop(['Temp9am','Temp3pm','Pressure9am'],axis=1)","e6378089":"X_train, X_test, y_train, y_test = get_Xy_oh(data)\nprint(base, cross_val_score(XGBClassifier(n_estimators=500, tree_method='gpu_hist', use_label_encoder=False, verbosity = 0), X_train, y_train, cv=5, scoring='accuracy').mean())","fe975a94":"le = LabelEncoder()\ndf_os = df.dropna().copy()\nX_train, X_test, y_train, y_test = get_Xy_oh(df_os)\nos = SMOTE()\nX_train, y_train = os.fit_resample(X_train, y_train)\ncount = Counter(y_train)\nprint(count)\n","cd9c184d":"a = cross_val_score(XGBClassifier(use_label_encoder=False, tree_method='gpu_hist', n_estimators=500, verbosity = 0), X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean()","aaebe21d":"print(a)","293df1dc":"%%time\n\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [4, 5, 6, 7, 8],\n        'n_estimators': [100, 300,500,700,900]\n        }\n\ndf_os = df.dropna().copy()\nX_train, X_test, y_train, y_test = get_Xy_oh(df_os)\nxgb = XGBClassifier(use_label_encoder=False, verbosity = 0, tree_method='gpu_hist')\nxgb_random = RandomizedSearchCV(estimator = xgb, param_distributions = params, n_iter = 125, cv = 2, verbose=2, random_state=42, n_jobs = -1, scoring='accuracy')\n# Fit the random search model\nxgb_random.fit(X_train, y_train)","9c3114f1":"print(xgb_random.best_params_)","a103942f":"f = cross_val_score(XGBClassifier(n_estimators=700, tree_method='gpu_hist', use_label_encoder=False, verbosity = 0 , colsample_bytree=1, gamma=5, max_depth=5, min_child_weight=1, subsample=1), X_train, y_train, cv=5, scoring='accuracy').mean()\nprint(base, f)\nbase = f","7d559efc":"base_models = [\n    ('SVC'       , LinearSVC(C = 1)),\n    ('XGB'       , XGBClassifier(n_estimators=700, tree_method='gpu_hist', use_label_encoder=False, verbosity = 0 , colsample_bytree=1, gamma=5, max_depth=5, min_child_weight=1, subsample=1)),\n    ('RF'        , RandomForestClassifier(n_estimators=500))\n]","03f3ec7f":"%%time\nX_train, X_test, y_train, y_test = get_Xy_oh(df)\n\nscaler = StandardScaler()\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\n\nmeta_model = LogisticRegressionCV()\nstacking_model = StackingClassifier(estimators=base_models, \n                                    final_estimator=meta_model, \n                                    passthrough=True, \n                                    cv=3)\n\nprint(base, cross_val_score(stacking_model, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1).mean())","5290f1bd":"%%time\nX_train, X_test, y_train, y_test = get_Xy_oh(df)\nscaler = StandardScaler()\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\nclf = stacking_model.fit(X_train, y_train)\nX_test = scaler.transform(X_test)\n\nyhat = clf.predict(X_test)\nprint(\"Accuracy :\", accuracy_score(y_test, yhat))\ncm = confusion_matrix(y_test, yhat)\nprint(classification_report(y_test, yhat))\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","cd3b618d":"fpr, tpr, thresholds = roc_curve(y_test, yhat)\nplt.figure(figsize=(6,4))\nplt.plot(fpr, tpr, linewidth=2)\nplt.plot([0,1], [0,1], 'k--' )\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for RainTomorrow Meta learner')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","def23ff1":"#from sklearn.preprocessing import binarize\n#yhat = clf.predict_proba(X_test)[:,1]\n#yhat = binarize(yhat.reshape(-1,1), 0.5)\n#cm = confusion_matrix(y_test, yhat)\n#print(\"Accuracy :\", accuracy_score(y_test, yhat))\n#print('Confusion matrix\\n\\n', cm)\n#print(classification_report(y_test, yhat))\n#sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","d405e478":"### Missing Values","4782ead1":"### Base Models","7122b14c":"We derive from [[4]] to get the rain since n days feature. Regarding a cold start problem which caused every time series to start with a series of NaN for this feature till the very first RainToday was true, we just filled those nans with zeroes. \n\n\n[4]:https:\/\/stackoverflow.com\/questions\/49910451\/create-a-new-column-containing-time-since-last-event-in-pandas","eb394d12":"For exploration we focus on a smaller subset of the dataset. Therefore we create a dictionary of dataframes grouped by every locations data and its respective name as key. ","517e6f19":"We start with our continous features by examining several graphs to recognize any seasonal effects influencing the target variable. We will focus on the features with noticeable effects. Note that we don't show every possible graph as some features don't show seasonality or the seasonality is somewhat redundant (like Temp9am and Temp3pm).\nIt is to be expected that the -3pm features are more suitable at discriminating next days rainfall as they are closer in time to the next day than their -9am counterpart. It turns out that this effect is nevertheless smaller than expected.\n\n- Starting with the Temp3pm (top left)we can clearly figure out the seasonality effects on the temperature with their respective peak in a years january to february. Some outliers are noticeable: if in high temperature periods there is a significantly lower Temp3pm it tends to rain the next day.\n- Moving to Pressure3pm (top right) the graph is a little more scattered than Temp3pm but we can see some inverse behaviour to Temp3pm. It is noticeable that in lower than average Pressure3pm it tends to rain the next day.\n- Inspecting Sunshine (bottom left) we can notice same seasonal behaviour to Temp3pm in periods of high sunshine, allthough this is not transferable to lower sunshine days, as they seem to be much more indepent of seasonality. Nevertheless a higher population of RainTomorrow is noticeable at really low (0-2) sunshine level days.\n- Finally Humidity3pm has some sort of seasonality much like the Pressure3pm. We can notice that observations with a high humidity are more likely to cause next days rain.","cd460d86":"## Conclusion","81a5991e":"Next we can also conclude that high a Humidity and a low Pressure tend to determine a next days rain. ","2bf577d2":"Finally we check wether our model actually performs on the test data. ","fe0d36c7":"Moving to categorical features we can point out that:\n1. Southwind S is the most common direction with a low RainTomorrow rate.\n2. Also common are SE and SSE with an even lower RainTomorrow rate.\n3. N is the 2nd most common direction with a much increased rain ratio.\n4. if it has rained today, RainTomorrow is much more common","f02e0662":"### Ensembles","af316bf0":"### Parameter Tuning","29637d7e":"## Objectives\n\nThis notebook's goals are defined as the following:\n1. We want to visualize multivariate coherences that determine a next days rainfall\n2. Build a statistical model for classification of all locations and compare results\n  1. Note that target variable is imbalanced\n  2. Try different strategies for missing values\n  2. Implement feature engineering like days without rain and season\n  3. Test oversampling\n3. Draw a conclusion ","161c7645":"## Setup\nWe do a basic setup of the notebook to get the source data from Kaggle using Kaggles API. A google drive account with kaggle.json credentials file is required to be in \/kaggle directory of drive. We then load required libraries, create pandas dataframe from the source and generate a profilereport [[2]] \n\n[2]: https:\/\/github.com\/pandas-profiling\/pandas-profiling","8a36ef24":"A first attempt to improve accuracy score is by tackling the vast amount of missing values in the dataframe. The baseline model was trained with dropped na values which caused the sample size to go down from ~150k to ~53k. We explore the outcome of handling those missing values with two different strategies:\n* Missing values are filled using the mean for every numerical feature and the modal value for every categorical feature\n* Missing values will be imputed by their next neighbours using knn imputation.","e9e821bd":"Taking everything to a conclusion we accomplished a test accuracy of 86.7% which is higher than null accuracy of 77.5%. Using XGBoost to tweak model performance we can point out that optimizations had no big impact on model performance. We scored ~86.5% using a defaulted XGBoost. Against our asssumption handling the vast amount of missing values made performance worse. Our oversampling approach had a great impact on train accuracy, but falling short and making test accuracy worse. Only hyperparameter tuning and stacking multiple models enabled a small increase in accuracy. Future work could focus on further tweaking the forest and SVC and grid search hyperparameters for the meta learner, which is coomputationally expensive, or try deep learning methods e.g. lstm.   ","d3a0c182":"Looking at the model's feature importances we can identify Location feature as superior to any other feature. We can conclude that fitting a model for every Location in our dataframe would score better accuracy. This is also shown in [[5]] .\n\n[5]:https:\/\/www.kaggle.com\/amiromidvar\/prediction-all-location-avg-score-0-96-smote","ca5398f6":"We create a new subdirectory for the project, use it as working directory, download the dataset and finally unzip it.  ","0d725af8":"Next attempt is to use a random grid search to improve accuracy by hyperparameter tuning. ","25a2b133":"86.5% accuracy score is the base performance which we try further improve. ","70bb42ad":"Handling missing values we can conclude that both attempts slightly dragged down model performance. Which tells us that our model can't be further improved by adding more training data to our dataframe.","ac8ef9d0":"Furthemore we attempted removing highly correlated features Temp9am, Temp3am and Pressure 9am as suggested by [[6]]. Nevertheless our model performance didn't change significantly.\n\n[6]:https:\/\/www.kaggle.com\/purvitsharma\/rain-in-australia-90-9-accuracy","35b19cd1":"### Meta Learner","9eb118e5":"## Introduction\n\nAs of right now weather forecasting methods rely on observing the weather situation and collect vast amounts of data to derive a forecast. In short ranged forecasting the evolution of current weather systems is tracked to predict future movement based on the dynamics of the atmosphere. [[1]]\nFields that could benefit getting reliable next day rain forecasting are e.g. aerospace and agriculture to schedule tasks propperly. \n\n\n\n[1]:https:\/\/www.weather.gov\/car\/weatherforecasting","21927726":"### Feature Selection","629dbb49":"For the last setup step data science libraries are imported and a pandas dataframe is created using the csv source file","bc1df8f1":"### Feature Engineering","479ba91c":"### Oversampling","6c312dbe":"#### N-Days since last rain ","3107b385":"#### Season","dd0b6b6e":"Our model is not facing any over or underfitting issues, because we score even a little higher accuracy on the test data than we did in crossvalidation training","cddc09b4":"The distribution and characteristics of the Cloud3pm feature become much more visible if we remove the time axis and display using a boxplot. We can therefore identify that RainTomorrow is most likely determined by a cloud level greater or equal to 6. We use the same technique to dipict that RainTomorrow is more likely on days with lower sunshine levels.","17c09435":"#### Mean\/Modal Filling","204c75f4":"For allmost every model the label encoding works better allthough we achieved the best result using onehot encoding with a larger boosting tree. We will use XGboost as baseline model and try different strategies against this base.","e2dc459a":"We get a very first glance using the generated profile report 'report.html' in the working directory.\n\nOur dataset that describes 145.460 observations represents a time series of a vast of metreologically data for every of the 49 locations with most of them ranging from 2008-12-01 to 2017-07-01.\n\nThe target variable RainTomorrow is heavily unbalanced with only ~22.5% being true and ~77.5% being false. Therefore we have to expect to score superior to null accurracy of 0.775\n\nIf we exclude the Date, Location and the dependend target variable RainTomorrow there are 20 features to consider in our model. From the 20 features 16 are numerical, 3 categorical and 1 boolean. \n\nGiven that there are 10% missing cells in the dateset a strategy to handle those NaN values needs to be implemented. \nA special attention has to be put on the Evaporation, Sunshine, Cloud9am and Cloud3pm as they range from 38% to 48% of missing values. \n\nThe features Rainfall, Sunshine, WindSpeed9am, Cloud9am and Cloud3pm got zero values assigned. Features that highly correlate are MinTemp with the Temp9am, MaxTemp with the Temp3pm and Pressure9am with pressure3pm.\n\n","8544c3e9":"As for feature engineering we try out 2 attempts to improve model performance:\n* We use the date column to get a samples meteorological season. We try this because we detected seasonality in our exploratory analysis  \n* We create a feature which counts the days since the last rainfall occured. We try this to help the model detect heavy rain or dry intervals   ","cfc79319":"We adapted a baseline function of w3resource to get the season of a date and reversed it for our australian usecase. [[3]]\n\n[3]:https:\/\/www.w3resource.com\/python-exercises\/python-conditional-exercise-37.php","67777bd6":"Next we attempt oversampling using imblearns oversampling tool SMOTE, also suggested in [[5]] and [[6]] and can see a significant improvment of our model's training performance. Nevertheless this didn't turn out to improve test accuracy confirming again that more training data will not improve model performance any further.  \n\n[5]:https:\/\/www.kaggle.com\/amiromidvar\/prediction-all-location-avg-score-0-96-smote\n[6]:https:\/\/www.kaggle.com\/purvitsharma\/rain-in-australia-90-9-accuracy","ae7c802a":"## Data Preparation\nTo use proper datetime slicing the Date feature has to be converted to datetime first. Furthermore we replace the yes and no of RainToday and RainTomorrow with 1 and 0 respectively to finally convert them to boolean format. ","c0ddea95":"Using Graphviz we can plot the decision Tree.","a50d44b8":"## Data Understanding","31a253b9":"#### KNN Imputation","827f1847":"Finally we attempt to stack 3 of the best performing baseline models. As meta learner we choose a logistic regression. For the baseline models it is suggested that they are all taking different approaches [[7]]. Therefore we consider support vector classifier, random forest and our gradient boosting XGboost.\n\n[7]:https:\/\/www.statsoft.de\/glossary\/M\/MetaLearning.htm","68018c73":"## Data Analysis","5ae3bb2f":"We used feature engineering to create and evaluate the use of two new features. Neither of the two features had a positive impact on model performance. The model seems to already recognize seasonality effects considering the state of other features. ","800b4801":"# Using ML to reliably predict a next days rainfall \n**A time series analysis to uncover possible determinants and successfully predict a next days rainfall**\n\n![image.png](https:\/\/images.unsplash.com\/photo-1485797460056-2310c82d1213?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80)\n\n## Table of contents\n1. [Introduction](#Introduction)\n2. [Objectives](#Objectives)\n3. [Setup](#Setup)\n4. [Data Understanding](#Data-Understanding)\n5. [Data Preparation](#Data-Preparation)\n6. [Data Exploration](#Data-Exploration)\n7. [Data Analysis](#Data-Analysis)\n8. [Conclusion](#Conclusion)","ad2bd749":"Fitting the meta learner was computational expensive allthough we achieved a 0.03  higher accuracy score.","c15cb59c":"Using grid search to tune hyperparameters we have improved accuracy only at the 4th decimal.","313b84a5":"Next we take a look at the missing cells by dividing a locations number of missing cells by a locations number of overall cells. We can see that airports are locations with the least amount of missing cells. Given that Melbourne Airport has the least amount of missing cells and is also almost median Rain\/NoRain value we will use it as a sample for explorative analysis.","c0867898":"First we test the performance of label encoding against onehot encoding on the baseline classificators decision tree, support vector machine and knn classification. We use a 5 fold cross validation accuracy score and compare  different hyperparameters at the same time","38b8856e":"To reduce complexitiy we limit the sample timespan to 2 years.","d535323b":"## Data Exploration\nWithin the first plot we take a look at the balance of our target variable RainTomorrow. The graph dipicts location based differences in the distribution with Portland as the location with the most rain at a 37% ratio and Woomera as the location with the least rain at a 7% ratio. In the median there is a 22% rain ratio. We are able to tell our target variable is unbalanced and heavily location dependent.","d9d531e2":"After taking a loot at univariate behaviour we conclude the explorative analysis by conducting bivariate relationships with the help of a pairplot of some choosen features. A first interesting behaviour to point out is the correlation of MaxTemp to Temp9am (column 2, row 1) with the RainTomorrow being mostly true below of what would be a linear regression line."}}