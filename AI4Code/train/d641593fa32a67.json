{"cell_type":{"4358c8d7":"code","06b7aad5":"code","65152ee6":"code","456363a6":"code","dd5442e5":"code","2714a42b":"code","7a315c1c":"code","9e9b1cbf":"code","705a5560":"code","58518ded":"code","e6209b55":"code","97f08e2c":"code","8bb85334":"code","25e3683d":"code","e3c755ad":"code","8bc43dc3":"code","2b21391f":"code","2b66e43e":"code","0116eea2":"code","85939d42":"code","60c6edfc":"code","499ff5f6":"code","d02469dc":"code","439192ef":"code","34269a0c":"code","0d9f804f":"code","93be9287":"code","7cc2ff3b":"code","2bbedcff":"code","95ed1cfa":"code","992d2efd":"code","9db86e5f":"markdown","7632b044":"markdown","8cbe6ff9":"markdown","b1883b05":"markdown","64220e47":"markdown","26438139":"markdown","cc35db6f":"markdown","29a2f37f":"markdown","1cb1e151":"markdown","e055ca07":"markdown","e2f24f47":"markdown","33da94ab":"markdown","53c91cd0":"markdown","7f823be4":"markdown","bd944a69":"markdown","fd5e1c23":"markdown","467e44d0":"markdown","f98a956f":"markdown","f9bfb004":"markdown"},"source":{"4358c8d7":"#let's import the necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style('whitegrid')\n","06b7aad5":"df = pd.read_csv('..\/input\/breast-cancer-prediction-dataset\/Breast_cancer_data.csv')\ndf.head()","65152ee6":"df.shape","456363a6":"df.describe()","dd5442e5":"df.info()","2714a42b":"df.isnull().sum()","7a315c1c":"#let's look at the columns of the datasets\ndf.columns","9e9b1cbf":"# Look at numeric and categorical values separately\n\n\ndf.select_dtypes(include=[\"int\", \"float64\"]).columns","705a5560":"sns.pairplot(df, hue='diagnosis', vars=['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area',\n       'mean_smoothness'])","58518ded":"#correlation matrix to find out the most related features to the \"'\\nBenign_malignant_cancer'\" \ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, cmap=\"coolwarm\", vmax=.8, square=True, annot= True);","e6209b55":"plt.figure(figsize=(10, 8))\nsns.scatterplot(x = 'mean_radius', y = 'mean_area', hue = 'diagnosis', data = df)","97f08e2c":"plt.figure(figsize=(10, 8))\nsns.scatterplot(x = 'mean_radius', y = 'mean_perimeter', hue = 'diagnosis', data = df)","8bb85334":"df = df.drop(df[['mean_radius','mean_area']], axis = True)\n\ndf.head()","25e3683d":"sns.countplot(df['diagnosis'], label = \"Count\")","e3c755ad":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\ny = df['diagnosis']\nX = df.drop('diagnosis', axis=1)\n\n\nprint(f\"'X' shape: {X.shape}\")\nprint(f\"'y' shape: {y.shape}\")\n\npipeline = Pipeline([\n    ('min_max_scaler', MinMaxScaler()),\n    ('std_scaler', StandardScaler())\n])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","8bc43dc3":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","2b21391f":"from sklearn.svm import LinearSVC\n\nmodel = LinearSVC(loss='hinge', dual=True)\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)","2b66e43e":"from sklearn.svm import SVC\n\n# The hyperparameter coef0 controls how much the model is influenced by high degree ploynomials \nmodel = SVC(kernel='poly', degree=2, gamma='auto', coef0=1, C=5)\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)","0116eea2":"model = SVC(kernel='rbf', gamma=0.5, C=0.1)\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)","85939d42":"X_train = pipeline.fit_transform(X_train)\nX_test = pipeline.transform(X_test)","60c6edfc":"print(\"=======================Linear Kernel SVM==========================\")\nmodel = SVC(kernel='linear')\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)\n\nprint(\"=======================Polynomial Kernel SVM==========================\")\nfrom sklearn.svm import SVC\n\nmodel = SVC(kernel='poly', degree=2, gamma='auto')\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)\n\nprint(\"=======================Radial Kernel SVM==========================\")\nfrom sklearn.svm import SVC\n\nmodel = SVC(kernel='rbf', gamma=1)\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)","499ff5f6":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.01, 0.1, 0.5, 1, 10, 100], \n              'gamma': [1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.001], \n              'kernel': ['rbf', 'poly', 'linear']} \n\ngrid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1, cv=5, iid=True)\ngrid.fit(X_train, y_train)\n\nbest_params = grid.best_params_\nprint(f\"Best params: {best_params}\")\n\nsvm_clf = SVC(**best_params)\nsvm_clf.fit(X_train, y_train)\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=False)","d02469dc":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(X_train, y_train)","439192ef":"y_predicted = classifier.predict(X_test)\ny_predicted","34269a0c":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_predicted)\nprint(cm)","0d9f804f":"accuracy = (59+104)\/(59+4+4+104)\naccuracy","93be9287":"from sklearn.model_selection import cross_val_score\n\nmodel_aqccuracy = cross_val_score(estimator = classifier,X =  X_train, y = y_train, cv = 10 )\nprint(\"Accuracy: {:.2f} %\".format(model_aqccuracy.mean()* 100))\nprint(\"Standard Deviation: {:.2f} %\" .format(model_aqccuracy.std()* 100))","7cc2ff3b":"scaler = StandardScaler()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","2bbedcff":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\n\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","95ed1cfa":"plt.figure(figsize=(8,6))\nplt.scatter(X_train[:,0],X_train[:,1],c=y_train,cmap='plasma')\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","992d2efd":"param_grid = {'C': [0.01, 0.1, 0.5, 1, 10, 100], \n              'gamma': [1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.001], \n              'kernel': ['rbf', 'poly', 'linear']} \n\ngrid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1, cv=5, iid=True)\ngrid.fit(X_train, y_train)\nbest_params = grid.best_params_\nprint(f\"Best params: {best_params}\")\n\nsvm_clf = SVC(**best_params)\nsvm_clf.fit(X_train, y_train)\n\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=False)","9db86e5f":"#  MODEL TRAINING \n","7632b044":"We do not have missing values, Great!","8cbe6ff9":"#  Radial Kernel SVM\n","b1883b05":"# Linear Kernel SVM\n","64220e47":"# Summary\nSo far we have learned the below items:\n* Importing the datasets into the notebook\n* Data Understanding\n* Data Cleaning\n* Data Visualization\n* Evaluating the correlation between the features and the target\n* Feature Selection\n* Buidling the SVM Models(Linear, Polynomial, Radial) on Python\n* Evaluating the SVM models and getting the high accuracy score\n* Buidling the Logistic Regression Model on Python\n* Evaluating the Logistic Regression with K_Fold Cross Validation\n* Implementing the PCA Techniques to reduce the dimensions of the features\n","26438139":"#  Polynomial Kernel SVM\n","cc35db6f":"#  Principal Component Analysis\nDimension Reduction Techniques","29a2f37f":"# Support Vector Machine Hyperparameter tuning\n","1cb1e151":"# Feature Selection\/Feature Engineering ","e055ca07":"we can simply compute the accuracy of the Logistic Model by executing of the below code","e2f24f47":"As we've noticed before it is difficult to visualize high dimensional data, we can use PCA to find the first two principal components, and visualize the data in this new, two-dimensional space, with a single scatter-plot. Before we do this though, we'll need to scale our data so that each feature has a single unit variance.\n\n","33da94ab":"# Computing the accuracy with k_Fold Cross Validation","53c91cd0":"# Visualization ","7f823be4":"# Logistic Regression Predicition","bd944a69":"# Data Preparation for SVM\n","fd5e1c23":"We can see the high correlation between some features such as mean_radius& mean_area, therfore they are twins, we can easily remove one of them. becasue the mean_radius has a more correlation with the target varaibles (Diagnosis), we keep it\n\nAlso we can see another high correlation between two features (mean_perimeterm, mean_radius) and they are another twins.therfore we can remove the mean_radius as the same reason before.","467e44d0":"# Making the Confusion Matrix","f98a956f":"Clearly by using these two components we can easily separate these two classes.\n\n# Interpreting the components\nUnfortunately, with this great power of dimensionality reduction, comes the cost of being able to easily understand what these components represent.\n\nThe components correspond to combinations of the original features, the components themselves are stored as an attribute of the fitted PCA object:\n\nNote:\n\n   * Principal Component Analysis:\n\n* Used in exploratory data analysis (EDA)\n* Visualize genetic distance and relatedness between populations.\n\n* Method:\n\n   * Eigenvalue decomposition of a data covariance (or correlation) matrix\n   * Singular value decomposition of a data matrix (After mean centering \/ normalizing ) the data matrix for each attribute.\n   \n* Output\n  * Component scores, sometimes called factor scores (the transformed variable values)\n  * loadings (the weight)\n  \n* Data compression and information preservation\n* Visualization\n* Noise filtering\n* Feature extraction and engineering","f9bfb004":"# Predicting the Test set Results"}}