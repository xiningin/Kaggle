{"cell_type":{"8a3a03ce":"code","0f663638":"code","d62a74e9":"code","01224b6e":"code","76d962b7":"code","78877515":"code","70e4d1ea":"code","a1f79ed4":"code","e69b8d62":"code","f3d7ca3a":"code","97277635":"code","ce65d9d6":"code","033e3b8d":"code","6290fd8d":"code","5db7a7a4":"code","8c0f69aa":"code","95189689":"code","59fe7d18":"code","cc396f18":"code","c64fb919":"code","bf593f7c":"code","a9d03442":"code","99b591f8":"code","23fb58cb":"code","e72df963":"code","a41cdc0f":"code","c9e07a69":"code","6f6cc816":"code","f743b104":"code","74271c9a":"code","a7ab97b1":"code","79824605":"code","4d1ba5c2":"code","3c1e1645":"code","3cd22c74":"code","65e0763c":"code","b03cf507":"code","64a42c59":"code","4d32afd8":"code","ce1df0f5":"code","0f12fe23":"code","afb56fbd":"code","2a0ec3be":"code","e1d117b8":"code","af4cca71":"code","61a000e9":"code","cf35d648":"code","80ceac12":"code","3e9c101e":"code","ddc2c44e":"code","36ce0684":"code","e5296269":"code","c2262826":"code","668c210c":"code","4529ba95":"code","d3c8033a":"code","64976e34":"code","8a0657f5":"code","4dc1f21e":"code","85d4fbc5":"code","3c6afd86":"code","47e51618":"code","809d5147":"code","5825013d":"code","3afeeeb3":"code","0bf9e612":"code","3efb5782":"code","6492f442":"code","70cb8597":"code","929c1c68":"code","62fac5d0":"code","abf8b306":"code","7e8c2f2f":"code","8a2e00b7":"code","e61e2193":"code","cabf9593":"code","70a84a1d":"code","586454fb":"code","77ec48d0":"code","a6685ab1":"code","18348e7f":"code","b42a61b9":"code","45ec0d8b":"markdown"},"source":{"8a3a03ce":"!conda install -c conda-forge gdcm -y","0f663638":"from __future__ import print_function\n\nimport os\nfrom os import listdir\nimport IPython\nimport IPython.display\nimport copy\nimport pandas as pd\nimport numpy as np\nimport pydicom as dicom\nfrom pydicom import dcmread\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tqdm\nimport glob\nfrom typing import Dict\n\nfrom sklearn.preprocessing import RobustScaler\nfrom scipy import ndimage\nfrom scipy.ndimage.interpolation import zoom\n\nfrom skimage import measure, morphology, segmentation\nfrom skimage.measure import label, regionprops\nfrom skimage.morphology import binary_closing\nfrom skimage.segmentation import clear_border\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models","d62a74e9":"file_path = \"..\/input\/osic-pulmonary-fibrosis-progression\/\"\n\nlistdir(file_path)","01224b6e":"train_df = pd.read_csv(file_path + \"train.csv\")\ntest_df = pd.read_csv(file_path + \"test.csv\")\nsub_df = pd.read_csv(file_path + \"sample_submission.csv\")\n\ntrain_df.head()","76d962b7":"test_df.head()","78877515":"sub_df.head()","70e4d1ea":"duplicate_data = train_df[train_df.duplicated(subset=[\"Patient\", \"Weeks\"], keep=False)]\n\nduplicate_data","a1f79ed4":"train_df.drop_duplicates(subset=[\"Patient\", \"Weeks\"], keep=\"last\", inplace=True)\ntrain_df.info()","e69b8d62":"sub_df[[\"Patient\", \"Weeks\"]] = sub_df[\"Patient_Week\"].str.split(\"_\", expand=True)\nsub_df = sub_df[[\"Patient\", \"Weeks\", \"Patient_Week\"]]\nsub_df = sub_df.merge(test_df.drop(\"Weeks\", axis=1), on=\"Patient\")\n\ntrain_df[\"Source\"] = \"train\"\nsub_df[\"Source\"] = \"test\"\n\ndataset = train_df.append([sub_df])\ndataset.reset_index(drop=True, inplace=True)\ndataset.head()","f3d7ca3a":"dataset[\"FVC_ave\"] = (dataset[\"FVC\"] ) \/ dataset[\"Percent\"] * 100\ndataset.head(10)","97277635":"def baseline_week(df):\n    df = df.copy()\n    df[\"Weeks\"] = df[\"Weeks\"].astype(int)\n    df.loc[df[\"Source\"] == \"test\", \"min_weeks\"] = np.nan\n    df[\"min_weeks\"] = df.groupby(\"Patient\")[\"Weeks\"].transform(\"min\")\n    df[\"baseline_week\"] = df[\"Weeks\"] - df[\"min_weeks\"]\n    \n    return df","ce65d9d6":"dataset = baseline_week(dataset)\ndataset.head()","033e3b8d":"def get_baseline_fvc(df):\n    df = df.copy()\n    base = df.loc[df[\"Weeks\"] == df[\"min_weeks\"]].copy()\n    base = df[[\"Patient\", \"FVC\"]].copy()\n    base.columns = [\"Patient\", \"base_fvc\"]\n    base[\"no\"] = 1\n    base[\"no\"] = base.groupby(\"Patient\")[\"no\"].transform(\"cumsum\")\n    base = base[base.no == 1]\n    base.drop(\"no\", axis=1, inplace=True)\n    df = df.merge(base, on = \"Patient\", how = \"left\")\n    \n    return df","6290fd8d":"dataset = get_baseline_fvc(dataset)\ndataset.head()","5db7a7a4":"dataset[\"Sex\"] = pd.Categorical(dataset[\"Sex\"])\ndataset[\"Sex\"] = dataset.Sex.cat.codes\ndataset[\"SmokingStatus\"] = pd.Categorical(dataset[\"SmokingStatus\"])\ndataset[\"SmokingStatus\"] = dataset.SmokingStatus.cat.codes\n\ndataset.tail()","8c0f69aa":"true_pat_res = test_df.Patient.unique()\ntrue_pat_res.sort()\n\ntrue_result = train_df.loc[train_df[\"Patient\"].isin(true_pat_res)].copy()\ntrue_result.info()","95189689":"dataset.drop_duplicates(subset=[\"Patient\", \"Weeks\"], keep=\"last\", inplace=True)\n\ntrain_df = dataset.loc[dataset[\"Source\"] == \"train\"].copy()\n\ntest_df = dataset.loc[dataset[\"Source\"] == \"test\"].copy()\ntrain_df.drop(\"Source\", axis=1, inplace=True)\ntest_df.drop(\"Source\", axis=1, inplace=True)\n\n\ntrain_df.head()","59fe7d18":"train_df.drop([\"Patient_Week\", \"Percent\", \"min_weeks\"], axis=1, inplace=True)\n\ntrain_df.head()","cc396f18":"test_df.head()","c64fb919":"test_df.drop([\"Percent\", \"Patient_Week\", \"min_weeks\"], axis=1, inplace=True)\n\ntest_df","bf593f7c":"test_df.info()","a9d03442":"if file_path == \"..\/input\/osic-pulmonary-fibrosis-progression\/\":\n    train_df[\"dcm_path\"] = file_path + \"train\/\" + train_df.Patient + \"\/\"\nelse:\n    train_df[\"dcm_path\"] = file_path + \"train\/\" + train_df.StudyInstanceUID + \"\/\" + train_df.SeriesInstanceUID","99b591f8":"if file_path == \"..\/input\/osic-pulmonary-fibrosis-progression\/\":\n    test_df[\"dcm_path\"] = file_path + \"test\/\" + test_df.Patient + \"\/\"\nelse:\n    test_df[\"dcm_path\"] = file_path + \"test\/\" + test_df.StudyInstanceUID + \"\/\" + test_df.SeriesInstanceUID","23fb58cb":"patient_id = []\npatient_path = []\n\nif file_path == \"..\/input\/osic-pulmonary-fibrosis-progression\/\":\n    patients = train_df.Patient.unique()\nelse:\n    patients = train_df.StudyInstanceUID.unique()\n\nfor patient in patients:\n    patient_id.append(patient)\n    if file_path == \"..\/input\/osic-pulmonary-fibrosis-progression\/\":\n        path = train_df[train_df.Patient == patient].dcm_path.values[0]\n    else:\n        path = train_df[train_df.StudyInstanceUID == patient].dcm_path.values[0]\n    ex_dcm = listdir(path)[0]\n    patient_path.append(path)\n    ds = dcmread(path + \"\/\" + ex_dcm)\n\n\n\npatient_df = pd.DataFrame(data=patient_id, columns=[\"patient\"])\npatient_df.loc[:, \"patient_path\"] = patient_path\npatient_df.head()","e72df963":"def load_scan(dcm_path):\n    if file_path == \"\/..input\/osic-pulmonary-fibrosis-progression\/\":\n        files = listdir(dcm_path)\n        file_no = [np.int(files.split(\".\")[0]) for file in files]\n        sorted_files = np.sort(file_no)[::-1]\n        slices = [dcmread(dcm_path  + \"\/\" + str(file_no) + \".dcm\") for file_no in sorted_files]\n    else:\n            \n        slices = [dcmread(dcm_path + \"\/\" + s) for s in listdir(dcm_path)]\n        slices = [s for s in slices if \"SliceLocation\" in s]\n        slices.sort(key=lambda x: int(x.InstanceNumber))\n        try:\n            slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\n        except:\n            slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n            \n        for s in slices:\n            s.SliceThickness = slice_thickness\n            \n    return slices","a41cdc0f":"def convert_to_hu(slices):\n    image = np.stack([s.pixel_array for s in slices])\n    image = image.astype(np.int16)\n    \n    image[image == -2000] = 0\n    \n    intercept = scans[0].RescaleIntercept\n    slope = scans[0].RescaleSlope\n    \n    if slope != 1:\n        image = slope * image.astype(np.float64)\n        image = image.astype(np.int16)\n        \n    image += np.int16(intercept)\n    \n    return np.array(image, dtype=np.int16)","c9e07a69":"ex = train_df.dcm_path.values[0]\nscans = load_scan(ex)\nhu_scans = convert_to_hu(scans)\n\nplt.figure()\nplt.imshow(hu_scans[13], cmap=plt.cm.gray)\nplt.show()","6f6cc816":"def resample(image, scan, new_spacing=[1,1,1]):\n    spacing = np.array([scan[0].SliceThickness] + list(scan[0].PixelSpacing), dtype=np.float32)\n    resize_factor = spacing \/ new_spacing\n    new_real_shape = image.shape \/ resize_factor\n    new_shape = np.round(new_real_shape)\n    real_resize_factor = new_shape \/ image.shape\n    new_spacing = spacing \/ real_resize_factor\n    \n    image = ndimage.interpolation.zoom(image, real_resize_factor, mode=\"nearest\")\n    \n    return image, new_spacing","f743b104":"im_res, spacing = resample(hu_scans, scans, [1,1,1])\nhu_scans.shape, im_res.shape","74271c9a":"fig, ax = plt.subplots(1, 2, figsize=(11, 7))\nax[0].imshow(hu_scans[20], cmap=plt.cm.gray)\nax[1].imshow(im_res[1], cmap=plt.cm.gray)\nplt.show()","a7ab97b1":"def get_multival_vals(feature):\n    if type(feature) == dicom.multival.MultiValue:\n        return np.int(feature[0])\n    else:\n        return np.int(feature)","79824605":"def generate_markers(image):\n    marker_internal = image < -400\n    marker_internal = segmentation.clear_border(marker_internal)\n    marker_internal_labels = measure.label(marker_internal)\n    areas = [r.area for r in measure.regionprops(marker_internal_labels)]\n    areas.sort()\n    if len(areas) > 2:\n        for region in measure.regionprops(marker_internal_labels):\n            if region.area < areas[-2]:\n                for coordinates in region.coords:\n                    marker_internal_labels[coordinates[0], coordinates[1]] = 0\n    marker_internal = marker_internal_labels > 0\n    \n    external_a = ndimage.binary_dilation(marker_internal, iterations=10)\n    external_b = ndimage.binary_dilation(marker_internal, iterations=55)\n    marker_external = external_b ^ external_a\n    \n    marker_watershed = np.zeros((image.shape), dtype=np.int)\n    marker_watershed += marker_internal * 255\n    marker_watershed += marker_external * 128\n    \n    return marker_internal, marker_external, marker_watershed","4d1ba5c2":"patient_internal, patient_external, patient_watershed = generate_markers(hu_scans[13])\n\nfig, ax = plt.subplots(1, 3, figsize=(17, 6))\nax[0].set_title(\"Internel marker\")\nax[0].imshow(patient_internal, cmap=\"gray\")\nax[1].set_title(\"External Marker\")\nax[1].imshow(patient_external, cmap=\"gray\")\nax[2].set_title(\"watershed image\")\nax[2].imshow(patient_watershed, cmap=\"gray\")\n\nplt.show()","3c1e1645":"def separate_lungs(image):\n    marker_internal, marker_external, marker_watershed = generate_markers(image)\n    \n    sobel_filtered_dx = ndimage.sobel(image, 0)\n    sobel_filtered_dy = ndimage.sobel(image, 1)\n    sobel_gradient = np.hypot(sobel_filtered_dx, sobel_filtered_dy)\n    sobel_gradient *= 255.0 \/ np.max(sobel_gradient)\n    \n    watershed = morphology.watershed(sobel_gradient, marker_watershed)\n    \n    outline = ndimage.morphological_gradient(watershed, size=(3, 3))\n    outline = outline.astype(bool)\n    \n    blackhat_structure = [[0, 0, 1, 1, 1, 0, 0],\n                          [0, 1, 1, 1, 1, 1, 0],\n                          [1, 1, 1, 1, 1, 1, 1],\n                          [1, 1, 1, 1, 1, 1, 1],\n                          [1, 1, 1, 1, 1, 1, 1],\n                          [0, 1, 1, 1, 1, 1, 0],\n                          [0, 0, 1, 1, 1, 0, 0]]\n    \n    blackhat_structure = ndimage.iterate_structure(blackhat_structure, iterations=7)\n    outline += ndimage.black_tophat(outline, structure=blackhat_structure)\n    \n    lungfilter = np.bitwise_or(marker_internal, outline)\n    lungfilter = ndimage.morphology.binary_closing(lungfilter, structure=np.ones((5, 5)), iterations = 3)\n    \n    segmented = np.where(lungfilter == 1, image, -2000*np.ones((image.shape)))\n    \n    return segmented","3cd22c74":"train_segmented = separate_lungs(hu_scans[13])","65e0763c":"plt.figure(figsize=(7, 7))\nplt.title(\"Segmented Lung\")\nplt.imshow(train_segmented, cmap=plt.cm.gray)\n\n\nplt.show()","b03cf507":"def img_hu_processing(patient_df):\n    \n    for i, patient in enumerate(tqdm.tqdm(patient_df[\"patient\"].values)):\n        try:\n            path = patient_df.loc[patient_df[\"patient\"] == patient].patient_path.values[0]\n            scans = load_scan(path)\n            n = len(scans)\n            if n >= 30:\n                m = int(n\/10.0)\n                scans = scans[int(n*0.1):int(n*0.9):int(m)*2]\n                hu_scans = convert_to_hu(scans)\n            else:\n                hu_scans = convert_to_hu(scans)\n                \n            for patient in path:\n                b = hu_scans\n                np.savez(\"imgs.npz\", b)   \n        except Exception as e:\n            continue\n            ","64a42c59":"img_hu_processing(patient_df)","4d32afd8":"dict_a = np.load(\"imgs.npz\")\nprint(dict_a.keys())","ce1df0f5":"patient_df.head()","0f12fe23":"column_indices = {name: i for i, name in enumerate(train_df.columns)}\n\nn = len(train_df)\n\ntrain_ds = train_df[0:int(n*0.7)]\nval_ds = train_df[int(n*0.7):int(n*0.9)]\ntest_ds = train_df[int(n*0.9):]\n\nnum_features = train_df.shape[1]","afb56fbd":"train_df.describe().transpose()\n\ntrain_mean = train_ds.mean()\ntrain_std = train_ds.std()\n\ntrain_ds = (train_ds - train_mean) \/ train_std\nval_ds = (val_ds - train_mean) \/ train_std\ntest_ds = (test_ds - train_mean) \/ train_std","2a0ec3be":"class WindowGenerator():\n    def __init__(self, input_width, label_width, shift,\n                 train_ds=train_ds, val_ds=val_ds, test_ds=test_ds,\n                 label_columns=None):\n        \n        self.train_ds = train_ds\n        self.val_ds = val_ds\n        self.test_ds = test_ds\n        \n        self.label_columns = label_columns\n        if label_columns is not None:\n            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n            \n        self.column_indices = {name: i for i, name in enumerate(train_ds.columns)}\n        \n        self.input_width = input_width\n        self.label_width = label_width\n        self.shift = shift\n        \n        self.total_window_size = input_width + shift\n        \n        self.input_slice = slice(0, input_width)\n        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n        \n        self.label_start = self.total_window_size - self.label_width\n        self.labels_slice = slice(self.label_start, None)\n        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n        \n    def __repr__(self):\n        return \"\\n\".join([\n            f\"TotalWindowSpread: {self.total_window_size}\",\n            f\"Total Indices: {self.input_indices}\",\n            f\"Label Indices: {self.label_indices}\",\n            f\"Label Name: {self.label_columns}\"])","e1d117b8":"w1 = WindowGenerator(input_width=30, label_width=1, shift=30, label_columns=[\"FVC\"])\nw1","af4cca71":"w2 = WindowGenerator(input_width=5, label_width=1, shift=1, label_columns=[\"FVC\"])\nw2","61a000e9":"def split_window(self, features):\n    inputs = features[:, self.input_slice, :]\n    labels = features[:, self.labels_slice, :]\n    if self.label_columns is not None:\n        labels = tf.stack(\n            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n            axis=-1)\n        \n    inputs.set_shape([None, self.input_width, None])\n    labels.set_shape([None, self.label_width, None])\n    \n    return inputs, labels\n\nWindowGenerator.split_window = split_window","cf35d648":"ex_window = tf.stack([np.array(train_ds[:w2.total_window_size]),\n                      np.array(train_ds[:+w2.total_window_size]),\n                      np.array(train_ds[:+w2.total_window_size])])\n\n\nex_inputs, ex_labels = w2.split_window(ex_window)\n\nprint(f\"Window shape: {ex_window.shape}\")\nprint(f\"Inputs shape: {ex_inputs.shape}\")\nprint(f\"Labels shape: {ex_labels.shape}\")","80ceac12":"w2.example = ex_inputs, ex_labels","3e9c101e":"def plot(self, model=None, plot_column=\"FVC\", max_subplots=3):\n    inputs, labels = self.example\n    plt.figure(figsize=(11, 7))\n    plot_column_index = self.column_indices[plot_column]\n    max_n = min(max_subplots, len(inputs))\n    for n in range(max_n):\n        plt.subplot(3, 1, n+1)\n        plt.ylabel(f\"{plot_column} [normed]\")\n        plt.plot(self.input_indices, inputs[n, :, plot_column_index],\n                 label=\"Inputs\", marker=\".\", zorder=-10)\n        \n        if self.label_columns:\n            label_column_index = self.label_columns_indices.get(plot_column, None)\n        else:\n            label_column_index = plot_column_index\n            \n        if label_column_index is None:\n            continue\n            \n        plt.scatter(self.label_indices, labels[n, :, label_column_index],\n                    edgecolors=\"k\", label=\"Labels\", c=\"#2ca02c\", s=64)\n        if model is not None:\n            predictions = model(inputs)\n            plt.scatter(self.label_indices, predictions[n, :, label_column_index],\n                        marker=\"X\", edgecolors=\"k\", label=\"Predictions\",\n                        c=\"#ff7f0e\", s=64)\n        \n        if n == 0:\n            plt.legend()\n        \n    plt.xlabel(\"Weeks\")\n    \nWindowGenerator.plot = plot","ddc2c44e":"w2.plot()","36ce0684":"def create_dataset(self, data):\n    data = np.array(data, dtype=np.float32)\n    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n        data=data, targets=None, sequence_length=self.total_window_size,\n        sequence_stride=1, shuffle=True, batch_size=32)\n    \n    ds = ds.map(self.split_window)\n    \n    return ds\n\nWindowGenerator.create_dataset = create_dataset","e5296269":"@property\ndef train(self):\n    return self.create_dataset(self.train_ds)\n\n@property\ndef val(self):\n    return self.create_dataset(self.val_ds)\n\n@property\ndef test(self):\n    return self.create_dataset(self.test_ds)\n\n@property\ndef example(self):\n    result = getattr(self, \"_example\", None)\n    if result is None:\n        result = next(iter(self.train))\n        self._example = result\n    return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example","c2262826":"w2.train.element_spec","668c210c":"for example_inputs, example_labels in w2.train.take(1):\n    print(f\"exInput shape: {example_inputs.shape}\")\n    print(f\"exLabel shape: {example_labels.shape}\")\n    ","4529ba95":"singlestep_wind = WindowGenerator(input_width=1, label_width=1, shift=1, label_columns=[\"FVC\"])\nsinglestep_wind","d3c8033a":"for example_inputs, example_labels in singlestep_wind.train.take(1):\n    print(f\"Inputs: {example_inputs.shape}\")\n    print(f\"ouputs: {example_labels.shape}\")","64976e34":"class Baseline(tf.keras.Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n        \n    def call(self, inputs):\n        if self.label_index is None:\n            return inputs\n        \n        result = inputs[:, :, self.label_index]\n        return result[:, :, tf.newaxis]","8a0657f5":"max_epochs = 100\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n                                                  patience=2, \n                                                  mode=\"min\")\n\ndef compile_and_fit(model, window):\n    \n    model.compile(loss=tf.keras.losses.MeanSquaredError(),\n                  optimizer=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n                  metrics=[\"mae\", \"mse\"])\n    model.fit(window.train, epochs = max_epochs,\n              validation_data = window.val, \n              callbacks=[early_stopping])\n    history = model\n    return history","4dc1f21e":"baseline = Baseline(label_index=column_indices[\"FVC\"])\n\nbaseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n                 metrics=[\"mse\"])\n\nval_performance = {}\nperformance = {}\n\nval_performance[\"Baseline\"] = baseline.evaluate(singlestep_wind.val)\nperformance[\"Baseline\"] = baseline.evaluate(singlestep_wind.test, verbose=0)","85d4fbc5":"wide_wind = WindowGenerator(input_width=30, label_width=30, shift=1, label_columns=[\"FVC\"])\nwide_wind","3c6afd86":"print(\"Input shape:\", singlestep_wind.example[0].shape)\nprint(\"output shape:\", baseline(singlestep_wind.example[0]).shape)","47e51618":"wide_wind.plot(baseline)","809d5147":"lstm = tf.keras.models.Sequential([\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    tf.keras.layers.Dense(units=1)])","5825013d":"print(\"Lstm Input shape:\", wide_wind.example[0].shape)\nprint(\"Lstm Output shape:\", lstm(wide_wind.example[0]).shape)","3afeeeb3":"history = compile_and_fit(lstm, wide_wind)\n\nIPython.display.clear_output()\nval_performance[\"LSTM\"] = lstm.evaluate(wide_wind.val)\nperformance[\"LSTM\"] = lstm.evaluate(wide_wind.test, verbose=0)","0bf9e612":"wide_wind.plot(lstm)","3efb5782":"for name, value in performance.items():\n    print(f\"{name:12s}: {value[1]:0.4f}\")\n    ","6492f442":"output_steps = 140\nmulti_window = WindowGenerator(input_width=140,\n                               label_width=output_steps,\n                               shift=output_steps)\n\nmulti_window.plot()\nmulti_window","70cb8597":"class MultiStepLastBaseline(tf.keras.Model):\n    def call(self, inputs):\n        return tf.tile(inputs[:, -1:, :], [1, output_steps, 1])\n    \n\nlast_baseline = MultiStepLastBaseline()\nlast_baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n                      metrics=[\"mse\", \"mae\"])\n\n\nmulti_val_performance = {}\nmulti_performance = {}\n\nmulti_val_performance[\"Last\"] = last_baseline.evaluate(multi_window.val)\nmulti_performance[\"Last\"] = last_baseline.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(last_baseline)","929c1c68":"class RepeatBaseline(tf.keras.Model):\n    def call(self, inputs):\n        return inputs\n\n    \nrepeat_baseline = RepeatBaseline()\nrepeat_baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n                        metrics=[\"mse\", tf.keras.metrics.Accuracy()])\n\nmulti_val_performance[\"Repeat\"] = repeat_baseline.evaluate(multi_window.val)\nmulti_performance[\"Repeat\"] = repeat_baseline.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(repeat_baseline)","62fac5d0":"multi_lstm = tf.keras.models.Sequential([\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    tf.keras.layers.Dense(output_steps*num_features, kernel_initializer=tf.initializers.zeros),\n    tf.keras.layers.Reshape([output_steps, num_features])])\n\nhistory = compile_and_fit(multi_lstm, multi_window)\n\nIPython.display.clear_output()\n\n\nmulti_val_performance[\"LSTM\"] = multi_lstm.evaluate(multi_window.val)\nmulti_performance[\"LSTM\"] = multi_lstm.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_lstm)","abf8b306":"class FeedBack(tf.keras.Model):\n    def __init__(self, units, output_steps):\n        super().__init__()\n        self.output_steps = output_steps\n        self.units = units\n        self.lstm_cell = tf.keras.layers.LSTMCell(units)\n        self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n        self.dense = tf.keras.layers.Dense(num_features)","7e8c2f2f":"feedback_lstm = FeedBack(units=32, output_steps=output_steps)","8a2e00b7":"def warmup(self, inputs):\n    x, *state = self.lstm_rnn(inputs)\n    \n    prediction = self.dense(x)\n    \n    return prediction, state\n\nFeedBack.warmup = warmup","e61e2193":"prediction, state = feedback_lstm.warmup(multi_window.example[0])\nprediction.shape","cabf9593":"def call(self, inputs, training=None):\n    predictions = []\n    \n    prediction, state = self.warmup(inputs)\n    \n    predictions.append(prediction)\n    \n    for n in range(1, self.output_steps):\n        x = prediction\n        x, state = self.lstm_cell(x, states=state, training=training)\n        \n        prediction = self.dense(x)\n        predictions.append(prediction)\n        \n    predictions = tf.stack(predictions)\n    predictions = tf.transpose(predictions, [1, 0, 2])\n    return predictions\n    \nFeedBack.call = call","70a84a1d":"print(\"Batch, Time, Features:\", feedback_lstm(multi_window.example[0]).shape)","586454fb":"history = compile_and_fit(feedback_lstm, multi_window)\n\nIPython.display.clear_output()\n\nmulti_val_performance[\"AR LSTM\"] = feedback_lstm.evaluate(multi_window.val)\nmulti_performance[\"AR LSTM\"] = feedback_lstm.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(feedback_lstm)","77ec48d0":"for name, value in multi_performance.items():\n    print(f\"{name:12s}: {value[1]:0.4f}\")","a6685ab1":"test_df.head()","18348e7f":"sub_df.head()","b42a61b9":"def score(FVC_true, FVC_pred, sigma):\n    sigma_clipped = np.max(sigma, 70)\n    delta = np.abs(FVC_true - FVC_pred)\n    delta = np.min(delta, 1000)\n    sq_2 = np.sqrt(2)\n    metric = (delta \/ sigma_clipped) * sq_2 + (np.log(sigma_clipped * sq_2))\n    \n    return np.mean(metric)","45ec0d8b":"# Work in Progress\nOsic-pulmonary-fibrosis-progression\nData Preprocessing for Patients and their Images\nmodel selection and parameter settings"}}