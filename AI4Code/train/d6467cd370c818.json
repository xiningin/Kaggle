{"cell_type":{"07862dcb":"code","0925ac77":"code","ac273d3b":"code","4e574482":"code","0164e254":"code","cf8ddf2f":"code","2593b9b1":"code","7baebad1":"code","d94797a2":"code","ad6f814a":"code","d56e483d":"code","8a240bbe":"code","53882655":"code","5f25972a":"code","5cfd1297":"code","8f563945":"code","7174b608":"code","f77474d2":"code","81d57af0":"code","1fa3f858":"code","84854087":"code","c9f0c13f":"code","e3cbc578":"markdown","de74aaa2":"markdown","9683b6b8":"markdown","fa43e196":"markdown","ec4c4850":"markdown","7d2fef68":"markdown","7091568f":"markdown","2bd77747":"markdown","5e8f840c":"markdown","64f4d2ef":"markdown","a537d0f6":"markdown","ae565772":"markdown","ccc14f61":"markdown","a18ae2a9":"markdown","e115f237":"markdown","3a661e0d":"markdown","acc7ba84":"markdown","9d51edf9":"markdown","774e3d5e":"markdown","bb1907b0":"markdown","232f4967":"markdown","592b6ea1":"markdown","1ad817c8":"markdown"},"source":{"07862dcb":"import re\nimport numpy as np\nimport string\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\nstopwords = set(STOPWORDS)\ndata =\"\"\"We are about to study the idea of a computational process.\nComputational processes are abstract beings that inhabit computers.\nAs they evolve, processes manipulate other abstract things called data.\nThe evolution of a process is directed by a pattern of rules\ncalled a program. People create programs to direct processes. In effect,\nwe conjure the spirits of the computer with our spells.\"\"\"\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(data)\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(24, 24))\naxes[0].imshow(wordcloud)\naxes[0].axis('off')\naxes[1].imshow(wordcloud)\naxes[1].axis('off')\naxes[2].imshow(wordcloud)\naxes[2].axis('off')\nfig.tight_layout()","0925ac77":"sentences = \"\"\"We are about to study the idea of a computational process.\nComputational processes are abstract beings that inhabit computers.\nAs they evolve, processes manipulate other abstract things called data.\nThe evolution of a process is directed by a pattern of rules\ncalled a program. People create programs to direct processes. In effect,\nwe conjure the spirits of the computer with our spells.\"\"\"","ac273d3b":"# remove special characters\nsentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)\n\n# remove 1 letter words\nsentences = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentences).strip()\n\n# lower all characters\nsentences = sentences.lower()","4e574482":"words = sentences.split()\nvocab = set(words)","0164e254":"vocab_size = len(vocab)\nembed_dim = 10\ncontext_size = 2","cf8ddf2f":"word_to_ix = {word: i for i, word in enumerate(vocab)}\nix_to_word = {i: word for i, word in enumerate(vocab)}","2593b9b1":"# data - [(context), target]\n\ndata = []\nfor i in range(2, len(words) - 2):\n    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]\n    target = words[i]\n    data.append((context, target))\nprint(data[:5])","7baebad1":"embeddings =  np.random.random_sample((vocab_size, embed_dim))","d94797a2":"def linear(m, theta):\n    w = theta\n    return m.dot(w)","ad6f814a":"def log_softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return np.log(e_x \/ e_x.sum())","d56e483d":"def NLLLoss(logs, targets):\n    out = logs[range(len(targets)), targets]\n    return -out.sum()\/len(out)","8a240bbe":"def log_softmax_crossentropy_with_logits(logits,target):\n\n    out = np.zeros_like(logits)\n    out[np.arange(len(logits)),target] = 1\n    \n    softmax = np.exp(logits) \/ np.exp(logits).sum(axis=-1,keepdims=True)\n    \n    return (- out + softmax) \/ logits.shape[0]","53882655":"def forward(context_idxs, theta):\n    m = embeddings[context_idxs].reshape(1, -1)\n    n = linear(m, theta)\n    o = log_softmax(n)\n    \n    return m, n, o","5f25972a":"def backward(preds, theta, target_idxs):\n    m, n, o = preds\n    \n    dlog = log_softmax_crossentropy_with_logits(n, target_idxs)\n    dw = m.T.dot(dlog)\n    \n    return dw","5cfd1297":"def optimize(theta, grad, lr=0.03):\n    theta -= grad * lr\n    return theta","8f563945":"theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))","7174b608":"epoch_losses = {}\n\nfor epoch in range(80):\n\n    losses =  []\n\n    for context, target in data:\n        context_idxs = np.array([word_to_ix[w] for w in context])\n        preds = forward(context_idxs, theta)\n\n        target_idxs = np.array([word_to_ix[target]])\n        loss = NLLLoss(preds[-1], target_idxs)\n\n        losses.append(loss)\n\n        grad = backward(preds, theta, target_idxs)\n        theta = optimize(theta, grad, lr=0.03)\n        \n     \n    epoch_losses[epoch] = losses","f77474d2":"ix = np.arange(0,80)\n\nfig = plt.figure()\nfig.suptitle('Epoch\/Losses', fontsize=20)\nplt.plot(ix,[epoch_losses[i][0] for i in ix])\nplt.xlabel('Epochs', fontsize=12)\nplt.ylabel('Losses', fontsize=12)","81d57af0":"def predict(words):\n    context_idxs = np.array([word_to_ix[w] for w in words])\n    preds = forward(context_idxs, theta)\n    word = ix_to_word[np.argmax(preds[-1])]\n    \n    return word","1fa3f858":"# (['we', 'are', 'to', 'study'], 'about')\npredict(['we', 'are', 'to', 'study'])","84854087":"def accuracy():\n    wrong = 0\n\n    for context, target in data:\n        if(predict(context) != target):\n            wrong += 1\n            \n    return (1 - (wrong \/ len(data)))","c9f0c13f":"accuracy()","e3cbc578":"### Plot loss\/epoch","de74aaa2":"<h1 id=\"definition\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Definition\n        <a class=\"anchor-link\" href=\"#definition\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","9683b6b8":"### Dictionaries","fa43e196":"<h1 id=\"dataset\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","ec4c4850":"### Backward function","7d2fef68":"### Forward function","7091568f":"### Predict function","2bd77747":"<h1 id=\"architecture\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>CBOW Architecture\n        <a class=\"anchor-link\" href=\"#definition\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","5e8f840c":"### Data bags","64f4d2ef":"The CBOW architecture is pretty simple contains :<br>\n    - the word embeddings as inputs (idx)\n    - the linear model as the hidden layer\n    - the log_softmax as the output","a537d0f6":"<h1 id=\"analyze\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Analyze\n        <a class=\"anchor-link\" href=\"#analyze\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","ae565772":"### Log softmax + NLLloss = Cross Entropy","ccc14f61":"### Optimize function","a18ae2a9":"<h1 id=\"vocabulary\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Vocabulary\n        <a class=\"anchor-link\" href=\"#vocabulary\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","e115f237":"<h1 id=\"implementation\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Implementation\n        <a class=\"anchor-link\" href=\"#implementation\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","3a661e0d":"![cbow-nn.png](attachment:cbow-nn.png)","acc7ba84":"80 epochs and 100% accuracy<br>\n<br>\nIf you enjoyed this post, don't forget to up-vote!","9d51edf9":"### Embeddings","774e3d5e":"### Accuracy","bb1907b0":"**Word2vec**\n\nWord2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space.\n\n**CBOW**\n\nCBOW or Continous bag of words is to use embeddings in order to train a neural network where the context is represented by multiple words for a given target words.\n\nFor example, we could use \u201ccat\u201d and \u201ctree\u201d as context words for \u201cclimbed\u201d as the target word.<br>\nThis calls for a modification to the neural network architecture.<br>\nThe modification, shown below, consists of replicating the input to hidden layer connections C times, the number of context words, and adding a divide by C operation in the hidden layer neurons.","232f4967":"### Clean Data","592b6ea1":"### Linear Model","1ad817c8":"<h1 id=\"training\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}