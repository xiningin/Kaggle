{"cell_type":{"886b70fa":"code","46b9d1b0":"code","29404598":"code","ad98a8d2":"code","e182766c":"code","cd9b7d49":"code","510296ac":"code","d1e9af24":"code","937f6b2b":"code","ba888924":"code","7e9e8c2c":"code","87f40e35":"code","ba973df7":"code","7f17ad26":"code","af1b815d":"code","47c20260":"code","605083c2":"code","93d0f363":"code","7763cf06":"code","a255d045":"code","0d3cab8a":"code","ad94cc49":"code","3c972a97":"code","30ed9b70":"code","c16ca1ba":"code","685a1cb0":"code","7bff1626":"code","e5ef7921":"code","546b5c2f":"code","7d768527":"code","89badb2d":"code","221cd75a":"code","bc52e307":"markdown","fe74fee1":"markdown","a5c41ce8":"markdown","453e3f67":"markdown","1b315b95":"markdown","f6dd01b6":"markdown","b098ba05":"markdown","5b4cac5a":"markdown","b422b987":"markdown","3b04605c":"markdown","72dfc0ca":"markdown","ab3c7334":"markdown","b14f73e4":"markdown","6aa8d447":"markdown","bcdff7e8":"markdown","e23659fc":"markdown","7f59315b":"markdown","15b54633":"markdown","39b33c49":"markdown","19f28974":"markdown","fa54ebe4":"markdown","a4b4f48d":"markdown"},"source":{"886b70fa":"import numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport os\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\n\n\nfrom datetime import datetime\nfrom datetime import timedelta\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","46b9d1b0":"train_df = gpd.read_file(\"\/kaggle\/input\/enrichedcovid19week2\/enriched_covid_19_week_2.csv\")\ntrain_df[\"Country_Region\"] = [country_name.replace(\"'\",\"\") for country_name in train_df[\"Country_Region\"]]\ntrain_df[\"restrictions\"] = train_df[\"restrictions\"].astype(\"int\")\ntrain_df[\"quarantine\"] = train_df[\"quarantine\"].astype(\"int\")\ntrain_df[\"schools\"] = train_df[\"schools\"].astype(\"int\")\ntrain_df[\"total_pop\"] = train_df[\"total_pop\"].astype(\"float\")\ntrain_df[\"density\"] = train_df[\"density\"].astype(\"float\")\ntrain_df[\"hospibed\"] = train_df[\"hospibed\"].astype(\"float\")\ntrain_df[\"lung\"] = train_df[\"lung\"].astype(\"float\")\ntrain_df[\"total_pop\"] = train_df[\"total_pop\"]\/max(train_df[\"total_pop\"])\ntrain_df[\"density\"] = train_df[\"density\"]\/max(train_df[\"density\"])\ntrain_df[\"hospibed\"] = train_df[\"hospibed\"]\/max(train_df[\"hospibed\"])\ntrain_df[\"lung\"] = train_df[\"lung\"]\/max(train_df[\"lung\"])\ntrain_df.head()","29404598":"train_df.columns","ad98a8d2":"trend_df = pd.DataFrame(columns={\"infection_trend\",\"fatality_trend\",\"quarantine_trend\",\"school_trend\",\"total_population\",\"expected_cases\",\"expected_fatalities\"})","e182766c":"#Just getting rid of the first days to have a multiple of 14\n#Makes it easier to generate the sequences\ntrain_df = train_df.query(\"Date>'2020-01-22'and Date<='2020-03-18'\")\ndays_in_sequence = 14\n\ntrend_list = []\n\nwith tqdm(total=len(list(train_df.Country_Region.unique()))) as pbar:\n    for country in train_df.Country_Region.unique():\n        for province in train_df.query(f\"Country_Region=='{country}'\").Province_State.unique():\n            province_df = train_df.query(f\"Country_Region=='{country}' and Province_State=='{province}'\")\n            \n            #I added a quick hack to double the number of sequences\n            #Warning: This will later create a minor leakage from the \n            # training set into the validation set. TO FIX.\n            for i in range(0,len(province_df),int(days_in_sequence\/2)):\n                if i+days_in_sequence<=len(province_df):\n                    #prepare all the temporal inputs\n                    infection_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].ConfirmedCases.values]\n                    fatality_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].Fatalities.values]\n                    restriction_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].restrictions.values]\n                    quarantine_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].quarantine.values]\n                    school_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].schools.values]\n\n                    #preparing all the demographic inputs\n                    total_population = float(province_df.iloc[i].total_pop)\n                    density = float(province_df.iloc[i].density)\n                    hospibed = float(province_df.iloc[i].hospibed)\n                    lung = float(province_df.iloc[i].lung)\n\n                    expected_cases = float(province_df.iloc[i+days_in_sequence-1].ConfirmedCases)\n                    expected_fatalities = float(province_df.iloc[i+days_in_sequence-1].Fatalities)\n\n                    trend_list.append({\"infection_trend\":infection_trend,\n                                     \"fatality_trend\":fatality_trend,\n                                     \"restriction_trend\":restriction_trend,\n                                     \"quarantine_trend\":quarantine_trend,\n                                     \"school_trend\":school_trend,\n                                     \"demographic_inputs\":[total_population,density,hospibed,lung],\n                                     \"expected_cases\":expected_cases,\n                                     \"expected_fatalities\":expected_fatalities})\n        pbar.update(1)\ntrend_df = pd.DataFrame(trend_list)","cd9b7d49":"trend_df[\"temporal_inputs\"] = [np.asarray([trends[\"infection_trend\"],trends[\"fatality_trend\"],trends[\"restriction_trend\"],trends[\"quarantine_trend\"],trends[\"school_trend\"]]) for idx,trends in trend_df.iterrows()]\n\ntrend_df = shuffle(trend_df)","510296ac":"i=0\ny=0\ntemp_df = pd.DataFrame()\nfor idx,row in trend_df.iterrows():\n    if sum(row.infection_trend)>0:\n        temp_df = temp_df.append(row)\n    else:\n        if i<25:\n            temp_df = temp_df.append(row)\n            i+=1\ntrend_df = temp_df","d1e9af24":"trend_df.head()","937f6b2b":"sequence_length = 13\ntraining_percentage = 0.8","ba888924":"training_item_count = int(len(trend_df)*training_percentage)\nvalidation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\ntraining_df = trend_df[:training_item_count]\nvalidation_df = trend_df[training_item_count:]","7e9e8c2c":"X_temporal_train = np.asarray(np.reshape(np.asarray([np.asarray(x) for x in training_df[\"temporal_inputs\"].values]),(training_item_count,5,sequence_length))).astype(np.float32)\nX_demographic_train = np.asarray([np.asarray(x) for x in training_df[\"demographic_inputs\"]]).astype(np.float32)\nY_cases_train = np.asarray([np.asarray(x) for x in training_df[\"expected_cases\"]]).astype(np.float32)\nY_fatalities_train = np.asarray([np.asarray(x) for x in training_df[\"expected_fatalities\"]]).astype(np.float32)","87f40e35":"X_temporal_test = np.asarray(np.reshape(np.asarray([np.asarray(x) for x in validation_df[\"temporal_inputs\"]]),(validation_item_count,5,sequence_length))).astype(np.float32)\nX_demographic_test = np.asarray([np.asarray(x) for x in validation_df[\"demographic_inputs\"]]).astype(np.float32)\nY_cases_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_cases\"]]).astype(np.float32)\nY_fatalities_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_fatalities\"]]).astype(np.float32)","ba973df7":"#temporal input branch\ntemporal_input_layer = Input(shape=(5,sequence_length))\nmain_rnn_layer = layers.LSTM(128, return_sequences=True, recurrent_dropout=0.2)(temporal_input_layer)\n\n#demographic input branch\ndemographic_input_layer = Input(shape=(4))\ndemographic_dense = layers.Dense(16)(demographic_input_layer)\ndemographic_dropout = layers.Dropout(0.2)(demographic_dense)\n\n#cases output branch\nrnn_c = layers.LSTM(64)(main_rnn_layer)\nmerge_c = layers.Concatenate(axis=-1)([rnn_c,demographic_dropout])\ndense_c = layers.Dense(256)(merge_c)\ndropout_c = layers.Dropout(0.3)(dense_c)\ncases = layers.Dense(1, activation=\"relu\",name=\"cases\")(dropout_c)\n\n#fatality output branch\nrnn_f = layers.LSTM(64)(main_rnn_layer)\nmerge_f = layers.Concatenate(axis=-1)([rnn_f,demographic_dropout])\ndense_f = layers.Dense(256)(merge_f)\ndropout_f = layers.Dropout(0.3)(dense_f)\nfatalities = layers.Dense(1, activation=\"relu\", name=\"fatalities\")(dropout_f)\n\n\nmodel = Model([temporal_input_layer,demographic_input_layer], [cases,fatalities])\n\nmodel.summary()","7f17ad26":"callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=1, factor=0.6),\n             EarlyStopping(monitor='val_loss', patience=20),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\nmodel.compile(loss=[tf.keras.losses.MeanSquaredLogarithmicError(),tf.keras.losses.MeanSquaredLogarithmicError()], optimizer=\"adam\")","af1b815d":"history = model.fit([X_temporal_train,X_demographic_train], [Y_cases_train, Y_fatalities_train], \n          epochs = 200, \n          batch_size = 16, \n          validation_data=([X_temporal_test,X_demographic_test],  [Y_cases_test, Y_fatalities_test]), \n          callbacks=callbacks)","47c20260":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss over epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","605083c2":"plt.plot(history.history['cases_loss'])\nplt.plot(history.history['val_cases_loss'])\nplt.title('Loss over epochs for the number of cases')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","93d0f363":"plt.plot(history.history['fatalities_loss'])\nplt.plot(history.history['val_fatalities_loss'])\nplt.title('Loss over epochs for the number of fatalities')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","7763cf06":"model.load_weights(\"best_model.h5\")","a255d045":"predictions = model.predict([X_temporal_test,X_demographic_test])","0d3cab8a":"display_limit = 30\nfor inputs, pred_cases, exp_cases, pred_fatalities, exp_fatalities in zip(X_temporal_test,predictions[0][:display_limit], Y_cases_test[:display_limit], predictions[1][:display_limit], Y_fatalities_test[:display_limit]):\n    print(\"================================================\")\n    print(inputs)\n    print(\"Expected cases:\", exp_cases, \" Prediction:\", pred_cases[0], \"Expected fatalities:\", exp_fatalities, \" Prediction:\", pred_fatalities[0] )","ad94cc49":"#Will retrieve the number of cases and fatalities for the past 6 days from the given date\ndef build_inputs_for_date(country, province, date, df):\n    start_date = date - timedelta(days=13)\n    end_date = date - timedelta(days=1)\n    \n    str_start_date = start_date.strftime(\"%Y-%m-%d\")\n    str_end_date = end_date.strftime(\"%Y-%m-%d\")\n    df = df.query(\"Country_Region=='\"+country+\"' and Province_State=='\"+province+\"' and Date>='\"+str_start_date+\"' and Date<='\"+str_end_date+\"'\")\n    \n    #preparing the temporal inputs\n    temporal_input_data = np.reshape(np.asarray([df[\"ConfirmedCases\"],\n                                                 df[\"Fatalities\"],\n                                                 df[\"restrictions\"],\n                                                 df[\"quarantine\"],\n                                                 df[\"schools\"]]),\n                                     (5,sequence_length)).astype(np.float32)\n    \n    #preparing all the demographic inputs\n    total_population = float(province_df.iloc[i].total_pop)\n    density = float(province_df.iloc[i].density)\n    hospibed = float(province_df.iloc[i].hospibed)\n    lung = float(province_df.iloc[i].lung)\n    demographic_input_data = [total_population,density,hospibed,lung]\n    \n    return [np.array([temporal_input_data]), np.array([demographic_input_data])]","3c972a97":"#Take a dataframe in input, will do the predictions and return the dataframe with extra rows\n#containing the predictions\ndef predict_for_region(country, province, df):\n    begin_prediction = \"2020-03-19\"\n    start_date = datetime.strptime(begin_prediction,\"%Y-%m-%d\")\n    end_prediction = \"2020-04-30\"\n    end_date = datetime.strptime(end_prediction,\"%Y-%m-%d\")\n    \n    date_list = [start_date + timedelta(days=x) for x in range((end_date-start_date).days+1)]\n    for date in date_list:\n        input_data = build_inputs_for_date(country, province, date, df)\n        result = model.predict(input_data)\n        \n        #just ensuring that the outputs is\n        #higher than the previous counts\n        \n        result[0] = np.round(result[0])\n        if result[0]<input_data[0][0][0][-1]:\n            result[0]=np.array([[input_data[0][0][0][-1]]])\n        \n        result[1] = np.round(result[1])\n        if result[1]<input_data[0][0][1][-1]:\n            result[1]=np.array([[input_data[0][0][1][-1]]])\n        \n        #We assign the quarantine and school status\n        #depending on previous values\n        #e.g Once a country is locked, it will stay locked until the end\n        df = df.append({\"Country_Region\":country, \n                        \"Province_State\":province, \n                        \"Date\":date.strftime(\"%Y-%m-%d\"), \n                        \"restrictions\": 1 if any(input_data[0][0][2]) else 0,\n                        \"quarantine\": 1 if any(input_data[0][0][3]) else 0,\n                        \"schools\": 1 if any(input_data[0][0][4]) else 0,\n                        \"total_pop\": input_data[1][0],\n                        \"density\": input_data[1][0][1],\n                        \"hospibed\": input_data[1][0][2],\n                        \"lung\": input_data[1][0][3],\n                        \"ConfirmedCases\":round(result[0][0][0]),\t\n                        \"Fatalities\":round(result[1][0][0])},\n                       ignore_index=True)\n    return df","30ed9b70":"#The functions that are called here need to optimise, sorry about that!\ncopy_df = train_df\nwith tqdm(total=len(list(copy_df.Country_Region.unique()))) as pbar:\n    for country in copy_df.Country_Region.unique():\n        for province in copy_df.query(\"Country_Region=='\"+country+\"'\").Province_State.unique():\n            copy_df = predict_for_region(country, province, copy_df)\n        pbar.update(1)","c16ca1ba":"def display_comparison(region):\n    groundtruth_df = gpd.read_file(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv\")\n    groundtruth_df[\"ConfirmedCases\"] = groundtruth_df[\"ConfirmedCases\"].astype(\"float\")\n    groundtruth_df[\"Fatalities\"] = groundtruth_df[\"Fatalities\"].astype(\"float\")\n    groundtruth = groundtruth_df.query(\"Country_Region=='\"+region+\"' and Date>='2020-03-19' and Date<='2020-04-01'\")\n    prediction = copy_df.query(\"Country_Region=='\"+region+\"' and Date>='2020-03-19' and Date<='2020-04-01'\")\n    \n    plt.plot(groundtruth.ConfirmedCases.values)\n    plt.plot(prediction.ConfirmedCases.values)\n    plt.title(\"Comparison between the actual data and our predictions for the number of cases\")\n    plt.ylabel('Number of cases')\n    plt.xlabel('Date')\n    plt.xticks(range(len(prediction.Date.values)),prediction.Date.values,rotation='vertical')\n    plt.legend(['Groundtruth', 'Prediction'], loc='best')\n    plt.show()\n    \n    plt.plot(groundtruth.Fatalities.values)\n    plt.plot(prediction.Fatalities.values)\n    plt.title(\"Comparison between the actual data and our predictions for the number of fatalities\")\n    plt.ylabel('Number of fatalities')\n    plt.xlabel('Date')\n    plt.xticks(range(len(prediction.Date.values)),prediction.Date.values,rotation='vertical')\n    plt.legend(['Groundtruth', 'Prediction'], loc='best')\n    plt.show()","685a1cb0":"display_comparison(\"Kenya\")","7bff1626":"display_comparison(\"Germany\")","e5ef7921":"test_df = gpd.read_file(\"..\/input\/covid19-global-forecasting-week-2\/test.csv\")\n#The country_region got modifying in the enriched dataset by @optimo, \n# so we have to apply the same change to the test Dataframe.\ntest_df[\"Country_Region\"] = [ row.Country_Region if row.Province_State==\"\" else row.Country_Region+\"_\"+row.Province_State for idx,row in test_df.iterrows() ]\ntest_df.head()","546b5c2f":"submission_df = pd.DataFrame(columns=[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"])\nwith tqdm(total=len(test_df)) as pbar:\n    for idx, row in test_df.iterrows():\n        #Had to remove single quotes because of countries like Cote D'Ivoire for example\n        country_region = row.Country_Region.replace(\"'\",\"\").strip(\" \")\n        province_state = row.Province_State.replace(\"'\",\"\").strip(\" \")\n        item = copy_df.query(\"Country_Region=='\"+country_region+\"' and Province_State=='\"+province_state+\"' and Date=='\"+row.Date+\"'\")\n        submission_df = submission_df.append({\"ForecastId\":row.ForecastId,\n                                              \"ConfirmedCases\":int(item.ConfirmedCases.values[0]),\n                                              \"Fatalities\":int(item.Fatalities.values[0])},\n                                             ignore_index=True)\n        pbar.update(1)","7d768527":"submission_df.sample(20)","89badb2d":"submission_df.to_csv(\"submission.csv\",index=False)","221cd75a":"submission_df.describe(include='all')","bc52e307":"* **V5**: Submission pipeline fixed - *score: 3.50412*\n* **V6**: New RNN architecture with two separate branches for each output - *score: 2.94372*\n* **V8**: Add a post-processing step checking if the model's output is equal or greater the previous value - *score: 2.49449*\n* **V9**: Change the MSE losses to RMSLE - *score: 1.58228*\n* **V11**: Change the outputs' activation fucntions from linear to ReLU - *score: 1.41954*\n* **V12**: Use a 2-week period for predictions instead of 1. Replace the SimpleRNN layers with LSTM layers - *score: 1.17620*\n* **V13**: Fix bug in Cell 4 (flagged by [@jeremyoudin](https:\/\/www.kaggle.com\/jeremyoudin)) which made the dataset much larger due to duplicates and also created a leakage between the training and validation sets - *score: 1.25741*\n* **V15**: Start introducing new inputs from the [enriched dataset](https:\/\/www.kaggle.com\/optimo\/covid19-enriched-dataset-week-2) by [@optimo](https:\/\/www.kaggle.com\/optimo) and fix a bug which prevented the generation of 25% of the training data - *score: 0.87340*\n* **V16**: Add a new input to the model based on the coutries' total population. It is added as a separate input as it is not temporal like all previous inputs - *score: 0.86465*\n* **V19**: Use additional inputs using the enriched dataset. It makes the model more complex and harder to train but might be worth it! *score: 0.94491*\n* **V20**: Allow a 50% overlap between sequences when generating the dataset to double its size. It creates a minor leakage between the training and validation set so it will eventually needs to be fixed. I was curious to see how it would affect performances. Beside that, it appears that the model's training phase is not perfectly stable, with the \"fatality\" branch not always converging. *score:0.97666*\n* **V21**: Add new figures across the notebook and a function at the end of the notebook to visualise the predictions over the period of time defined for the Public Leaderboard.\n\n*PS: Please note that the scores may have change slightly. I assume that the evaluation process is updated as new data is coming in for the period from 19\/03\/2020 to 01\/04\/2020.*","fe74fee1":"I only display what I call \"temporal\" inputs as we're simply trying to have a feeling of how well our model is fitting the trends.","a5c41ce8":"# Covid-19 Forecasting using an RNN","453e3f67":"# Preparing the training data","1b315b95":"The model is very simple in terms of architecture. The only difference from what could traditionally be seen is that it has two outputs so we can have two different losses (one for the expected number of cases and for the expected number of fatalities).","f6dd01b6":"Preparing the inputs and shuffling the dataframe to make sure we have a bit of everything in our training and validation set.","b098ba05":"# Performance during training","5b4cac5a":"# Build the model","b422b987":"We can quickly check the quality of the predictions... One thing is clear, there is room for improvement!","3b04605c":"Splitting my dataset - 80% for training and 20% for validation","72dfc0ca":"# Apply the model to generate novel predictions","ab3c7334":"Only keeping 50 sequences where the number of cases stays at 0, as there were way too many of these samples in our dataset.","b14f73e4":"# Outputs: Observing the curves","6aa8d447":"## Generate predictions using the model","bcdff7e8":"I am using the original training CSV file instead of the enriched dataset as it is up-to-date with the latest stats from this week.","e23659fc":"The following functions will be used to get the 13 previous days from a given date and demographic information, predict the number of cases and fatalities, before iterating again. Therefore, it will use the prediction for the next day as part of the data for the one afterwards.","7f59315b":"The goal of this notebook is very simple: Generate additional features from the Covid19-global-forecasting dataset and feed it into an RNN. The RNN will take as inputs:\n*     number of cases for 13 days\n*     number of fatalities for 13 days\n*     restrictions applied for the area in the past 13 days\n*     quarantine applied for the area in the past 13 days\n*     school opened or closed for the area in the past 13 days\n*     additional information related to the area (population, density, number of hospital beds, lung measurement) \n\nas outputs:\n*    number of cases for the 14th day\n*    number of fatalities for the 14th day","15b54633":"I create a new dataframe where I will only store 13-day trends for each location with the resulting numbers on the 14th day. The time periods extracted do not overlap on purpose.","39b33c49":"![Diagram-Covid-model.png](attachment:Diagram-Covid-model.png)","19f28974":"# Generating the submission file","fa54ebe4":"### If you found this notebook helpful, please give it an upvote. It will be greatly appreciated!","a4b4f48d":"Just need to do this little trick to extract the relevant date and the forecastId and add that to the submission file."}}