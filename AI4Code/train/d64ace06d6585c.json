{"cell_type":{"9787096b":"code","db8ae2a5":"code","f2f5ca03":"code","d6f55288":"code","d717afeb":"code","7b72288c":"code","03ac0c3b":"code","2d6045c3":"code","9fef02ab":"code","ae434c2c":"code","2541cfc2":"code","cd048cba":"code","7a777aa6":"code","2d4c7db2":"code","09868bfa":"code","66177646":"code","45819132":"code","811398d5":"code","a998c3db":"code","3a97fa95":"code","ee0d1bab":"code","7aa25f0c":"code","9792bf6c":"code","d1e43d73":"code","0f437faf":"markdown","9fb68d48":"markdown","3d0f989c":"markdown","d1115ead":"markdown","53e680bf":"markdown","644d6696":"markdown","8aa4edb0":"markdown","f2532b27":"markdown","bfbd27b5":"markdown","13e25884":"markdown","a7acedfe":"markdown","77aeef88":"markdown","87da0c57":"markdown","ff278fde":"markdown"},"source":{"9787096b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams\nfrom matplotlib.cm import rainbow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier as dtc\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.pipeline import make_pipeline,Pipeline\nfrom sklearn.model_selection import GridSearchCV,cross_val_score\nfrom sklearn.metrics import classification_report","db8ae2a5":"data = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndata","f2f5ca03":"data.describe()","d6f55288":"data.info()","d717afeb":"rcParams['figure.figsize'] = 20, 14\nplt.matshow(data.corr())\nplt.yticks(np.arange(data.shape[1]), data.columns)\nplt.xticks(np.arange(data.shape[1]), data.columns)\nplt.colorbar()","7b72288c":"rcParams['figure.figsize'] = 6,4\nplt.bar(data['target'].unique(), data['target'].value_counts(), color = ['blue', 'black'])\nplt.xticks([0, 1])\nplt.xlabel('Target Classes')\nplt.ylabel('Count')\nplt.title('Count of each Target Class')","03ac0c3b":"for i in data:\n    print(\"Total unique values in \",i, \" column are :\", data[i].nunique())","2d6045c3":"data = pd.get_dummies(data, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])\ndata","9fef02ab":"scaler = StandardScaler()\ncols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndata[cols] = scaler.fit_transform(data[cols])\ndata","ae434c2c":"X = data.drop(['target'], axis = 1)\ny = data['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)","2541cfc2":"print(\"Total training examples are: \",X_train.shape[0])\nprint(\"Total testing examples are: \",X_test.shape[0])","cd048cba":"svc_scores = []\nkernel = ['rbf','linear','poly','sigmoid']\n\nfor i in range(len(kernel)):\n    model_svc = SVC(kernel = kernel[i])\n    model_svc.fit(X_train,y_train)\n    svc_scores.append(model_svc.score(X_test,y_test)*100)\n    ans = max(svc_scores)\nprint(\"Maximum Score by using various kernels of SVM model in percentage is:\",ans)\n\ncnt = 0\nfor i in svc_scores:\n    if i == ans:\n        main_kernel = kernel[cnt]\n    else:\n        cnt = cnt + 1","7a777aa6":"colors = rainbow(np.linspace(0, 1, len(kernel)))\nplt.bar(kernel, svc_scores, color = colors)\nfor i in range(len(kernel)):\n    plt.text(i, svc_scores[i], svc_scores[i])\nplt.xlabel('Kernels')\nplt.ylabel('Scores')\nplt.title('Support Vector Classifier scores for different kernels')","2d4c7db2":"model_svc = SVC(kernel = main_kernel)\nmodel_svc.fit(X_train,y_train)\ny_pred = model_svc.predict(X_test)\ntarget_names = ['class-0','class-1']\nprint(classification_report(y_test, y_pred, target_names=target_names))","09868bfa":"dtc_scores = []\nfor i in range(1,len(X.columns) + 1):\n    dtc_model = dtc(splitter='random',max_depth=8,max_features = i,random_state=0)\n    dtc_model.fit(X_train,y_train)\n    dtc_scores.append(dtc_model.score(X_test,y_test)*100)\n    ans = max(dtc_scores)\nprint(\"Maximum Score by using various features of Decision Tree model in percentage is:\",ans)\n\ncnt = 1\nfor i in dtc_scores:\n    if i == ans:\n        max_features = cnt\n    else:\n        cnt = cnt + 1","66177646":"plt.plot([i for i in range(1, len(X.columns) + 1)], dtc_scores, color = 'red')\nfor i in range(1, len(X.columns) + 1):\n    plt.text(i, dtc_scores[i-1], (i, dtc_scores[i-1]))\nplt.xticks([i for i in range(1, len(X.columns) + 1)])\nplt.xlabel('Maximum features')\nplt.ylabel('Scores')\nplt.title('Decision Tree Classifier scores for different number of maximum features')","45819132":"dtc_model = dtc(splitter='random',max_depth=8,max_features = max_features,random_state=0)\ndtc_model.fit(X_train,y_train)\ny_pred = dtc_model.predict(X_test)\ntarget_names = ['class-0','class-1']\nprint(classification_report(y_test, y_pred, target_names=target_names))","811398d5":"#Grid Search CV for finding best hyper-parameters for Decision Tree Classifier\n#sc = StandardScaler()\ndt = dtc()\npipe = Pipeline(steps=[('decisiontree', dt)])\ncriterion = ['gini', 'entropy']\nsplitter = ['best','random']\nmax_depth = [4,6,8,12]\nmax_features = list((range(1,len(X.columns) + 1)))\nparams = dict(decisiontree__criterion=criterion,\n              decisiontree__splitter=splitter,\n              decisiontree__max_depth=max_depth,\n              decisiontree__max_features=max_features)\nclf = GridSearchCV(pipe,params)\nclf.fit(X_train,y_train)\nprint('Best Criterion:',clf.best_estimator_.get_params()['decisiontree__criterion'])\nprint('Best Splitter:',clf.best_estimator_.get_params()['decisiontree__splitter'])\nprint('Best Depth:', clf.best_estimator_.get_params()['decisiontree__max_depth'])\nprint('Best Maximum Features:', clf.best_estimator_.get_params()['decisiontree__max_features'])\nprint(clf.best_estimator_.get_params()['decisiontree'])\nans = clf.score(X_test,y_test)*100\nprint(\"Best score using GridSearchCV on Decision Tree Model is:\",ans)","a998c3db":"rfc_scores = []\nn_estimators = [10,50,100,150,200,300,400,450,500,950,1000]\nfor i in n_estimators:\n    model_rfc = rfc(n_estimators = i,random_state=0)\n    model_rfc.fit(X_train,y_train)\n    rfc_scores.append(model_rfc.score(X_test,y_test)*100)\n    ans = max(rfc_scores)\nprint(\"Maximum Score by using various number of estimators of Random Forest model in percentage is: \",ans)\n\ncnt = 0\nfor i in rfc_scores:\n    if i == ans:\n        best_estimator = n_estimators[cnt]\n        break\n    else:\n        cnt = cnt + 1","3a97fa95":"colors = rainbow(np.linspace(0, 1, len(n_estimators)))\nplt.bar([i for i in range(len(n_estimators))], rfc_scores, color = colors, width = 0.8)\nfor i in range(len(n_estimators)):\n    plt.text(i, rfc_scores[i], rfc_scores[i])\nplt.xticks(ticks = [i for i in range(len(n_estimators))], labels = [str(estimator) for estimator in n_estimators])\nplt.xlabel('Number of estimators')\nplt.ylabel('Scores')\nplt.title('Random Forest Classifier scores for different number of estimators')","ee0d1bab":"model_rfc = rfc(n_estimators = best_estimator,random_state=0)\nmodel_rfc.fit(X_train,y_train)\ny_pred = model_rfc.predict(X_test)\ntarget_names = ['class-0','class-1']\nprint(classification_report(y_test, y_pred, target_names=target_names))","7aa25f0c":"model_params = {\n    'svm': {\n        'model': SVC(gamma='auto',probability=True),\n        'params': {\n            'svc__C': [1,10,100,1000],\n            'svc__kernel': ['rbf','linear','poly','sigmoid']\n        }  \n    },\n    'random_forest': {\n        'model': rfc(),\n        'params' : {\n            'randomforestclassifier__n_estimators': [10,50,100,150,200,300,400,450,500,950,1000]\n        }\n    }\n}","9792bf6c":"scores = []\nbest_estimators = {}\nfor algo, mp in model_params.items():\n    pipe = make_pipeline(StandardScaler(), mp['model'])\n    clf =  GridSearchCV(pipe, mp['params'], cv=5, return_train_score=False)\n    clf.fit(X_train, y_train)\n    scores.append({\n        'model': algo,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    best_estimators[algo] = clf.best_estimator_\n    \ndf = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf","d1e43d73":"print(\"Scores after applying GridSearchCV on SVM model: \",best_estimators['svm'].score(X_test,y_test)*100)\nprint(\"Scores after applying GridSearchCV on Random Forest model: \",best_estimators['random_forest'].score(X_test,y_test)*100)","0f437faf":"### Standardizing the values of some columns whose values are large and so they do not behave badly and look like standard normally distributed data.","9fb68d48":"###  Importing the basic libraries","3d0f989c":"### Converting the categorical column to dummy variables","d1115ead":"### Describing the data","53e680bf":"# Support Vector Classifier\n### In below block, SVC is used along with different kernels like 'rbf','linear','poly','sigmoid' and hence scores of all different kernels are calculated.The best score of kernel is selected for further classification report.","644d6696":"# Grid Search CV for SVM and Random Forest\n### Grid Search CV library is used here to select best parameters from both the SVM and Random Forest models and thereby calculating the score on test set by choosing those best parameters.","8aa4edb0":"### Reading the data from CSV file","f2532b27":"### Splitting into training and testing set","bfbd27b5":"# Random Forest Classifier\n### In below block of code,Random Forest Classifier is used with various number of estimators.These estimators are checked and the estimator which gives best score for model is further selected for classification report.","13e25884":"# Decision Tree Classifier\n### In the below block of code, Decision Tree Classifier is used along with some of the hyperparameters which are required to obtain good score.The best hyperparameters are selected which are further used in classification report.","a7acedfe":"### Visualizing the count of each target classes in order to check whether the dataset is imbalanced or not","77aeef88":"### Calculation of unique values in each column present in the dataset.","87da0c57":"### Visualizing the correlation among columns present in the dataset","ff278fde":"# Name: Jay Shah\n# Date: 09-07-2021\n# Heart Disease Analysis"}}