{"cell_type":{"306082fa":"code","cae1bd82":"code","f945ed79":"code","ee4591a6":"code","0e3941db":"code","2b7ac3d0":"code","754f4685":"code","f1770690":"code","5e92122d":"code","8756a2d3":"code","b9c95e6b":"code","ada82523":"code","d2a67d45":"code","aea3baaf":"code","82b4470a":"code","d1f50d89":"code","1f6dcfcb":"code","aa207bc4":"code","9b14b622":"code","925272a3":"code","b7e8376a":"code","dd496e27":"code","a37d955e":"code","20cd705a":"code","72000b5d":"code","4f89107f":"markdown","37836270":"markdown","07ea44a6":"markdown","6228573e":"markdown","642ac0ec":"markdown","3ababb84":"markdown","7302808f":"markdown","166fb3a5":"markdown","381e605a":"markdown","a4a4c0af":"markdown","fff840f9":"markdown","9a1a5aad":"markdown","86cd736a":"markdown","cf83d70a":"markdown","054b3e90":"markdown","0953f676":"markdown","9397be14":"markdown","cd13c623":"markdown"},"source":{"306082fa":"import os\nimport warnings\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom pandas.api.types import CategoricalDtype\n\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import VarianceThreshold, mutual_info_regression\n\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\n\nimport xgboost as xgb\nxgb.set_config(verbosity=0)\n\n# Mute warnings\nwarnings.filterwarnings('ignore')","cae1bd82":"data_dir = Path(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\")\ntrain_full = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")\ntest_full = pd.read_csv(data_dir \/ \"test.csv\", index_col=\"Id\")\ndf = pd.concat([train_full, test_full])\n\nX = df.copy()\ny = X.pop('SalePrice')","f945ed79":"X[\"Exterior2nd\"] = X[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n# Some values of GarageYrBlt are corrupt, so we'll replace them\n# with the year the house was built\nX[\"GarageYrBlt\"] = X[\"GarageYrBlt\"].where(X.GarageYrBlt <= 2010, X.YearBuilt)\n# Names beginning with numbers are awkward to work with\nX.rename(columns={ \"1stFlrSF\": \"FirstFlrSF\",\n                    \"2ndFlrSF\": \"SecondFlrSF\",\n                    \"3SsnPorch\": \"Threeseasonporch\",}, inplace=True,)","ee4591a6":"features_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \n                \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \n                \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \n                \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \n                \"CentralAir\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\nfor name in features_nom:\n    X[name] = X[name].astype(\"category\")\n    # Add a None category for missing values\n    if \"None\" not in X[name].cat.categories:\n        X[name].cat.add_categories(\"None\", inplace=True)","0e3941db":"five_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n# ten_levels = [str(i) for i in range(10)]\nten_levels = list(range(10))\nfeatures_ordered = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n# Add a None level for missing values\nfeatures_ordered = {key: [\"None\"] + value for key, value in features_ordered.items()}\n\nfor name, levels in features_ordered.items():\n    X[name] = X[name].astype(CategoricalDtype(levels, ordered=True))","2b7ac3d0":"X['OverallQual'] = X['OverallQual'].apply(str)\nX['OverallCond'] = X['OverallCond'].apply(str)","754f4685":"for name in X.select_dtypes('number'):\n    X[name] = X[name].fillna(0)\nfor name in X.select_dtypes('category'):\n    X[name] = X[name].fillna('None')","f1770690":"X_threshold = X.loc[train_full.index]\nfor colname in X_threshold.select_dtypes([\"object\", \"category\"]):\n    X_threshold[colname], _ = X_threshold[colname].factorize()","5e92122d":"varThres = VarianceThreshold(threshold=0.99*(1-0.99))\nvarThres.fit(X_threshold)\nX = X[X_threshold.columns[varThres.get_support()]]","8756a2d3":"X_scores = X.loc[train_full.index,:]\ny_scores = y.loc[train_full.index]\nfor colname in X_scores.select_dtypes([\"object\", \"category\"]):\n    X_scores[colname], _ = X_scores[colname].factorize()\n# All discrete features should now have integer dtypes\ndiscrete_features = [pd.api.types.is_integer_dtype(t) for t in X_scores.dtypes]\nmi_scores = mutual_info_regression(X_scores, y_scores, discrete_features=discrete_features, random_state=42)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X_scores.columns)\n\n# Plot MI Scores\nmi_scores = mi_scores.sort_values(ascending=True)\nwidth = np.arange(len(mi_scores))\nticks = list(mi_scores.index)\n\nplt.barh(width, mi_scores)\nplt.yticks(width, ticks)\nplt.title(\"Mutual Information Scores\")","b9c95e6b":"X = X.loc[:, mi_scores > 0.0]","ada82523":"# Relationships among numerical features are often expressed through mathematical formulas\ndef mathematical_transforms(df):\n    X = pd.DataFrame()  # dataframe to hold new features\n    X[\"LivLotRatio\"] = df.GrLivArea \/ df.LotArea\n    X[\"Spaciousness\"] = (df.FirstFlrSF + df.SecondFlrSF) \/ df.TotRmsAbvGrd\n    return X\n\n\n# Creat new columns from the original indexes\ndef interactions(df):\n    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\") \n    X = X.mul(df.GrLivArea, axis=0)\n    return X\n\n\n# Features describing the presence or absence of something often come in sets\ndef counts(df):\n    X = pd.DataFrame()\n    X[\"PorchTypes\"] = df[[\n        \"WoodDeckSF\",\n        \"OpenPorchSF\",\n        \"EnclosedPorch\",\n        \"Threeseasonporch\",\n        \"ScreenPorch\",\n    ]].gt(0.0).sum(axis=1)\n    return X\n\n\n# Break into simpler pieces\ndef break_down(df):\n    X = pd.DataFrame()\n    X[\"MSClass\"] = df.MSSubClass.str.split(\"_\", n=1, expand=True)[0]\n    return X\n\n\n# Aggregate informations across multiple rows grouped by some category.\ndef group_transforms(df):\n    X = pd.DataFrame()\n    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n    return X\n\nX = X.join(mathematical_transforms(X))\nX = X.join(interactions(X))\nX = X.join(counts(X))\nX = X.join(group_transforms(X))","d2a67d45":"# List out all features that are categorical\nnumerical_cols = X.select_dtypes(include= ['int64', 'float64']).columns\ncategorical_cols = X.select_dtypes(include = ['category']).columns","aea3baaf":"from sklearn.preprocessing import LabelBinarizer\nfrom sklearn.compose import make_column_selector as selector\n# Build Transformer pipeline\nnumerical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n                                         ])\ncategorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n                                          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n                                           ])\npreprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, selector(dtype_exclude='category')),\n                                              ('cat', categorical_transformer, selector(dtype_include='category')),\n                                              ])","82b4470a":"X_train, X_val, y_train, y_val = train_test_split(X.loc[train_full.index,:], y.loc[train_full.index], test_size=0.25, random_state=42)\nX_val_prep = preprocessor.fit_transform(X_val)\n\nX_test = X.loc[test_full.index,:]","d1f50d89":"# y_test = pd.read_csv('\/kaggle\/input\/ames-dataset\/result-with-best.csv').SalePrice","1f6dcfcb":"# xgb_reg = xgb.XGBRegressor(random_state=42)\n# ames_pipe = Pipeline(steps=[('preprocessor_transformers', preprocessor),\n#                             ('model', xgb_reg)])","aa207bc4":"# xgb_param_grid = {'model__subsample': np.arange(0.1, 1, 0.1),\n#                   'model__max_depth': np.arange(2,10,1),\n#                   'model__colsample_bytree': np.arange(0.1, 1.05, 0.05),\n#                   'model__eta': [0.001,0.01,0.1],\n#                   'model__min_child_weight' : np.arange(1,10,1),\n#                   'model__n_estimators' : [2000,3000],\n#                   'model__reg_alpha' : np.arange(0.1, 1, 0.1),\n#                  }\n# xgb_fit_params = {'model__eval_set':[(X_val, y_val)],\n#                   'model__early_stopping_rounds': 10\n#                  }\n# randomized_neg_mse = RandomizedSearchCV(estimator= ames_pipe,\n#                                         param_distributions=xgb_param_grid,\n#                                         scoring='neg_mean_squared_error',\n#                                         verbose=0,\n#                                         cv=10)\n# randomized_neg_mse.fit(X_train, y_train, \n#                        **xgb_fit_params\n#                       )","9b14b622":"# best_param = randomized_neg_mse.best_params_\n# best_param","925272a3":"# best_estimator = randomized_neg_mse.best_estimator_","b7e8376a":"# X_test1 = X.loc[test_full.index,:]\n# y_preds = best_estimator.predict(X_test)","dd496e27":"xgb_reg = xgb.XGBRegressor(subsample=0.7000000000000001,\n                           n_estimators=2000,\n                           min_child_weight=1,\n                           max_depth=6,\n                           eta=0.01,\n                           colsample_bytree=0.7,\n                           reg_alpha=0.5,\n                           reg_lambda=1.0,\n                           num_parallel_tree=1,\n                           random_state=42, verbosity=0,seed=123)\names_pipe = Pipeline(steps=[('preprocessor_transformers', preprocessor),\n                            ('model', xgb_reg)])\n\nxgb_fit_params = {'model__eval_set':[(X_val_prep, y_val)],\n                  'model__eval_metric':'rmse',\n#                   'model__early_stopping_rounds': 10\n                 }\names_pipe.fit(X_train, y_train,\n              **xgb_fit_params)","a37d955e":"y_val_preds = ames_pipe.predict(X_val)\nscore_val = np.sqrt(mean_squared_error(np.log(y_val), np.log(y_val_preds)))\nscore_val","20cd705a":"# Using new data to check the score\n# Only submition when get better result.\n# y_preds = ames_pipe.predict(X_test)\n# from sklearn.metrics import mean_squared_error\n# solution = pd.read_csv('\/kaggle\/input\/ames-dataset\/result-with-best.csv')\n# score = np.sqrt(mean_squared_error(np.log(solution.SalePrice), np.log(y_preds)))\n# score","72000b5d":"sample_sub = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ny_preds = ames_pipe.predict(X_test)\nfinal_data = {'Id': sample_sub.Id, 'SalePrice': y_preds}\n\nfinal_submission = pd.DataFrame(data=final_data).to_csv('submission_file.csv', \n                                                         index=False)","4f89107f":"# Fine-Tuning Model : Using RandomizedSearchCV","37836270":"# Data Preprocessing\n\n1.   Clean Data\n2.   Encode the Statistical Data Type\n3.   Handle Missing Values\n4.   Mutual Imformation (Feature Utility Scores)\n5.   Creature Feature \n6. OneHot Encoder (Pipeline)\n7. Standard Scaler(Pipeline","07ea44a6":"## Feature Selection\n    1. Remove features with low variance","6228573e":"### The ordered categorical features","642ac0ec":"### The unordered (nominative) categorical features","3ababb84":"# Load Data","7302808f":"These features are read as 'int' type, but is actually a nominative categorical.","166fb3a5":"## Apply Mi Scores to remove Uninformative Columns (mi_scores = 0.0)","381e605a":"## Encode the Statistical Data Type","a4a4c0af":"## Handle Missing Values","fff840f9":"## Clean Data","9a1a5aad":"### Make Mi Scores","86cd736a":"## Create Features","cf83d70a":"# Import packages and load dataset","054b3e90":"### Note: \n\n\n1.   We chose OHE(OneHot Encoder), not Label Encoder because Label Encoder is not fair with all Categories Variables. \n2.   A disadvantage of OHE is high cardinality, we can use PCA to reduce data dimension.","0953f676":"# Load Dattasets from Kaggle API","9397be14":"# Create Submission File","cd13c623":"## Feature Utility Scores"}}