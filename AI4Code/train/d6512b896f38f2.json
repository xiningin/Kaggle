{"cell_type":{"300895d9":"code","fbfe0a1b":"code","fa7f94f7":"code","96b16094":"code","c125d537":"code","14dcdeaa":"code","001f498e":"code","a7512a16":"code","deca0daa":"code","5c195158":"code","1b8d4ae9":"code","03621220":"code","7a9060be":"code","58d4c586":"code","58334697":"code","11a55129":"code","45c781e0":"code","406b2651":"code","3157d86d":"code","7b279d43":"code","1d766763":"code","8fa15628":"code","252f7bef":"code","d0913dc1":"code","b2a5a0c3":"code","5cdc2cc0":"code","4c8339e9":"code","b24995ea":"code","f353b28d":"code","cb916a45":"code","1eecced8":"code","f96a4a5b":"code","e6fb071a":"code","f75db8bd":"code","bc1cf90e":"code","bb8afb48":"code","e2197d68":"code","d2f0ee1a":"code","19fd636d":"code","70f5766e":"code","30abf3a6":"code","9c2c0830":"code","e8b614e5":"code","c2c792f5":"code","ae61bf55":"code","96de75e8":"code","83eebb75":"code","97956ea7":"code","94074f91":"code","4dc25ec8":"code","07e1d321":"code","5df8270f":"code","0ae83633":"code","a616e5ca":"code","2c8bad92":"code","8216c8aa":"code","f0d4cb2d":"code","ec7a8a68":"code","9f55cec9":"code","eda54dc1":"code","e53f2047":"code","ef907d33":"code","4a91d91f":"code","5304b900":"code","060c39a2":"code","d41c3e33":"code","c62b159f":"code","9f87fc0c":"code","95712bf8":"code","e3f70530":"code","1dd20a1c":"code","3b2bffa9":"code","0973e7bd":"markdown","bb9708ad":"markdown","51b78032":"markdown","60778cc2":"markdown","c08236b7":"markdown","8d7936d9":"markdown","f172e584":"markdown","92a63cef":"markdown","5c106273":"markdown","94d6b9be":"markdown","46ac1294":"markdown","da861a48":"markdown","176200f0":"markdown","6d2b03c9":"markdown","c657a52b":"markdown","18f66dc8":"markdown","8be9ffeb":"markdown","d58f9609":"markdown","701d2a27":"markdown","a7201584":"markdown","35e6036b":"markdown","103150ef":"markdown","ae5fc9f4":"markdown","a94f277d":"markdown","bd7b1f23":"markdown","b1285a8d":"markdown","aecf4461":"markdown","33ed8c71":"markdown","69dec48c":"markdown","d213b1da":"markdown","4a28363f":"markdown","3caada12":"markdown","98f3e70e":"markdown","47130586":"markdown","61e7bd75":"markdown","1988b514":"markdown","57644e12":"markdown","355c133d":"markdown","e703b6ef":"markdown","976fe283":"markdown","b6b1c6fa":"markdown","19cdc22b":"markdown","b74f4451":"markdown","2713ceb2":"markdown"},"source":{"300895d9":"import tensorflow\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n#from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.applications.densenet import DenseNet169\n\nimport os\n#from tensorflow.python.keras.applications.nasnet import preprocess_input\nfrom tqdm import tqdm","fbfe0a1b":"#model_prefix = 'MobileNetV2'\nmodel_prefix = 'DenseNet169'\n\nBATCH_SIZE = 32\n\nSIZE = 224 # MobileNetV2, ResNet50V2, MobileNet, RegNetX002, MobileNetV3Large, MobileNetV3Small, EfficientNetB7, DenseNet121, DenseNet169\n#SIZE = 331 # NASNetLarge\n#SIZE = 299 # InceptionResNetV2, InceptionV3, Xception\n\n# For mean-std measurements -> KFOLD = True\n# For model final testing -> KFOLD = False\nKFOLD = False\n\nEPOCHS = 30\nEPOCHS_KFOLD = 30\nEPOCHS_TEST = 1000","fa7f94f7":"models_dir = '.\/MODELS\/'\ndir_root = '..\/input\/lyme-clean-and-dirty\/Lyme_ver03\/'\ndir_original = dir_root + 'data\/'\ndir_augmented = dir_root + 'augmented_dataset\/'\ntest_df = pd.read_csv(dir_root + 'test_data.csv')\ntrain_df = pd.read_csv(dir_root + 'training_data.csv')\naugmented_df = pd.read_csv(dir_root + 'data_augmented.csv')","96b16094":"\"\"\"\nmodels_dir = '\/data1\/LYME\/MODELS\/'\ndir_root = '\/data1\/LYME\/ver_03\/'\ndir_original = dir_root + 'data\/'\ndir_augmented = dir_root + 'augmented_dataset\/'\ntest_df = pd.read_csv(dir_root + 'test_data.csv')\ntrain_df = pd.read_csv(dir_root + 'training_data.csv')\naugmented_df = pd.read_csv(dir_root + 'data_augmented.csv')\n\"\"\"","c125d537":"%%time\n\ndata = {}\n\ntarget_size = (SIZE, SIZE)\n\n\nfor i in range(len(test_df['image'])):\n    image_name = dir_original + test_df['image'][i]\n    image=tf.keras.preprocessing.image.load_img(image_name, color_mode='rgb', target_size = target_size)\n    image=np.array(image)\n    data.update({test_df['image'][i]: image})\n    \n\nfor i in range(len(train_df['image'])):\n    image_name = dir_original + train_df['image'][i]\n    image=tf.keras.preprocessing.image.load_img(image_name, color_mode='rgb', target_size = target_size)\n    image=np.array(image)\n    data.update({train_df['image'][i]: image})\n\n\nfor i in range(len(augmented_df['image'])):\n    image_name = dir_augmented + augmented_df['image'][i]\n    image=tf.keras.preprocessing.image.load_img(image_name, color_mode='rgb', target_size = target_size)\n    image=np.array(image)\n    data.update({augmented_df['image'][i]: image})\n\n\n","14dcdeaa":"print('train_df size: ',len(train_df))\nprint('test_df size: ',len(test_df))\n#data_df = pd.concat([test_df, train_df])\ndata_df = train_df\n\ndata_df.reset_index(drop=True, inplace=True)\ndata_df = data_df.sample(frac=1, random_state=123)\ndata_df.reset_index(drop=True, inplace=True)\n\nprint('data_original_df size: ',len(data_df))","001f498e":"def show_model_results(acc_per_fold, loss_per_fold, auc_per_fold):\n    # == Provide average scores ==\n    print('------------------------------------------------------------------------')\n    print('Score per fold')\n    for i in range(0, len(acc_per_fold)):\n      print('------------------------------------------------------------------------')\n      print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]} - AUC: {auc_per_fold[i]}')\n    print('------------------------------------------------------------------------')\n    print('Average scores for all folds:')\n    print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n    print(f'> AUC: {np.mean(auc_per_fold)} (+- {np.std(auc_per_fold)})')\n    print(f'> Loss: {np.mean(loss_per_fold)} (+- {np.std(loss_per_fold)})')","a7512a16":"def plot_mean_std(label, line_color, fill_color, epoch_list, mean_list, std_list, ax):\n  \n  ax.plot(\n    epoch_list,\n    mean_list,\n    color=line_color,\n    label=label, \n    lw=2,\n    alpha=0.8,\n  )\n\n  upper = (mean_list + std_list)\n  lower = (mean_list - std_list)\n\n  ax.fill_between(\n    epoch_list,\n    lower,\n    upper,\n    color=fill_color,\n    alpha=0.5,\n    label=r\"$\\pm$ 1 std. dev.\",\n  )\n\n\ndef plot_train_val_mean_std(metric_label, legend_location, metric_ylim, history_train_list_means, history_train_list_stds, history_val_list_means, history_val_list_stds, ax):\n\n\n  epochs_list = range(EPOCHS)\n\n  plot_mean_std('train', 'r', \"lightcoral\", epochs_list, history_train_list_means, history_train_list_stds, ax)\n  plot_mean_std('val', 'b', \"lightsteelblue\", epochs_list, history_val_list_means, history_val_list_stds, ax)\n\n  ax.set(\n    xlim = [0, EPOCHS-1],\n    ylim = metric_ylim,\n    title = metric_label + \" - History\",\n  )\n\n  ax.legend(loc=legend_location, ncol=2)\n\n  model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name    \n  figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n  os.makedirs(figures_dir, exist_ok=True)    \n  fig_filename = model_name + '_' + metric_label + '_history_' + add_title + '.png'\n  fig_file_path = os.path.join(figures_dir, fig_filename)\n  plt.savefig(fig_file_path, dpi=300)\n\n  plt.show()  \n","deca0daa":"class Base_Model(tf.keras.Model):\n\n  def __init__(self, target_size):\n    super().__init__()\n    self.target_size = target_size\n    self.base_model = DenseNet169(\n    #self.base_model = MobileNetV2(\n        include_top=False,\n        pooling='max', \n        weights=WEIGHTS, \n        input_shape = self.target_size)\n    \n    print(target_size)\n\n    # make the weights and biases of the base model non-trainable\n    # by \"freezing\" each layer of the BASE network\n    for layer in self.layers:\n        print(layer.name)\n        layer.trainable = TRAINABLE    \n    \n    self.flat_layer = tf.keras.layers.Flatten()\n    self.dense1_layer = tf.keras.layers.Dense(512, activation='relu')\n    self.dense2_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n\n  def call(self, inputs, training=False):\n    x = self.base_model(inputs)\n    x = self.flat_layer(x)\n    x = self.dense1_layer(x)\n\n\n    return self.dense2_layer(x)\n    ","5c195158":"class Complex_Model():\n\n  def __init__(self, result_data, result_label, test_data, test_label, \n               save_dir, save_best_model_path, epoch_num, \n               target_size = (SIZE, SIZE, 3), num_folds = 5):\n    \n    self.X = result_data\n    self.Y = result_label\n    self.x = test_data\n    self.y = test_label\n\n    self.target_size = target_size\n    self.save_dir = save_dir\n    self.save_best_model_path = save_best_model_path\n    self.epoch_num = epoch_num\n    self.num_folds = num_folds\n\n    self.acc_per_fold = []\n    self.loss_per_fold = []\n    self.auc_per_fold = []\n\n    self.history_acc_list = []\n    self.history_loss_list = []\n    self.history_auc_list = []\n    self.history_val_acc_list = []\n    self.history_val_loss_list = []\n    self.history_val_auc_list = []\n\n\n\n  def run(self):\n\n    kfold = StratifiedKFold(n_splits = self.num_folds)\n  \n\n    from tqdm import tqdm\n    for train, test in tqdm(kfold.split(self.X, self.Y)):\n\n      model = Base_Model(self.target_size)\n      #model = create_model()\n\n      #model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001), \n      model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001), \n                  loss = 'binary_crossentropy', \n                  metrics = [tf.keras.metrics.AUC(name='auc'),'acc'])\n\n\n      if DA == True:  \n        train_data_aug = ImageDataGenerator(\n          rescale=1.\/255,\n          featurewise_center=True, \n          samplewise_center=True, featurewise_std_normalization=True, samplewise_std_normalization=True,\n          zca_whitening=True, zca_epsilon=1e-06, rotation_range=90, \n          width_shift_range=1.0, height_shift_range=1.0, brightness_range=[0.5, 1.5], \n          shear_range=90.0, zoom_range=1.0, channel_shift_range=0.0, \n          fill_mode='nearest', horizontal_flip=False, vertical_flip=False\n        ).flow(self.X[train], self.Y[train])\n      else:\n        train_data_aug = ImageDataGenerator(rescale=1.\/255).flow(self.X[train], self.Y[train])\n    \n      test_data_aug = ImageDataGenerator(rescale=1.\/255).flow(self.X[test], self.Y[test])\n\n      checkpoint_AUC = ModelCheckpoint(save_dir, monitor='val_auc', verbose=2, save_best_only=True, \n                                      mode='max', save_weights_only=True)\n      \n      history = model.fit(\n        train_data_aug,\n        batch_size=BATCH_SIZE,\n        epochs=self.epoch_num,\n        verbose=2,\n        callbacks = [checkpoint_AUC],\n        validation_data=test_data_aug)\n        \n      model.summary()        \n      \n      model.load_weights(save_dir)\n      \n      scores = model.evaluate(test_data_aug, verbose=0)\n\n      self.loss_per_fold.append(scores[0])\n      self.auc_per_fold.append(scores[1])\n      self.acc_per_fold.append(scores[2])\n\n      \n      self.history_acc_list.append(history.history['acc'])\n      self.history_loss_list.append(history.history['loss'])\n      self.history_auc_list.append(history.history['auc'])\n      self.history_val_acc_list.append(history.history['val_acc'])\n      self.history_val_loss_list.append(history.history['val_loss'])\n      self.history_val_auc_list.append(history.history['val_auc'])\n\n\n  def run_full(self):\n    model = Base_Model(self.target_size)\n    #model = create_model()\n    \n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001), \n                  loss = 'binary_crossentropy', \n                  metrics = [tf.keras.metrics.AUC(name='auc'),'acc'])\n\n    if DA == True:  \n        train_data_aug = ImageDataGenerator(\n          rescale=1.\/255,\n          featurewise_center=True, \n          samplewise_center=True, featurewise_std_normalization=True, samplewise_std_normalization=True,\n          zca_whitening=True, zca_epsilon=1e-06, rotation_range=90, \n          width_shift_range=1.0, height_shift_range=1.0, brightness_range=[0.5, 1.5], \n          shear_range=90.0, zoom_range=1.0, channel_shift_range=0.0, \n          fill_mode='nearest', horizontal_flip=False, vertical_flip=False\n        ).flow(self.X, self.Y)\n    else:\n        train_data_aug = ImageDataGenerator(rescale=1.\/255).flow(self.X, self.Y)\n    \n    test_data_aug = ImageDataGenerator(rescale=1.\/255).flow(self.x, self.y)\n\n    checkpoint_AUC = ModelCheckpoint(save_dir, monitor='val_auc', verbose=2, save_best_only=True, \n                                      mode='max', save_weights_only=True)\n      \n    history = model.fit(\n        train_data_aug,\n        batch_size=BATCH_SIZE,\n        epochs=self.epoch_num,\n        verbose=2,\n        callbacks = [checkpoint_AUC],\n        validation_data=test_data_aug)\n        \n    model.summary()        \n      \n    model.load_weights(save_dir)\n      \n    scores = model.evaluate(test_data_aug, verbose=0)\n\n    print('**** Test BEST model ****')\n    Accuracy_string = 'Acc{acc_best:.3f}'.format(acc_best=scores[2])\n    print('Accuracy = {acc_best:.4f}'.format(acc_best=scores[2]))\n    AUC_string = 'AUC{AUC_best:.3f}'.format(AUC_best=scores[1])\n    print('AUC= {AUC_best:.4f}'.format(AUC_best=scores[1]))\n    Loss_string = 'Loss{Loss_best:.3f}'.format(Loss_best=scores[0])\n    print('Loss = {Loss_best:.4f}'.format(Loss_best=scores[0]))\n    \n    best_model_metrics_filename = self.save_best_model_path + '_' + Loss_string + '_' + Accuracy_string + '_' + AUC_string  + '_best.h5'\n    model.save_weights(best_model_metrics_filename) \n    \n    return model, best_model_metrics_filename, self.x, self.y\n    \n    \n  def show_results(self):\n    show_model_results(self.acc_per_fold, self.loss_per_fold, self.auc_per_fold)\n\n\n  def history_stat_metrics(self, history_list):\n    history_list_means = np.mean(np.asarray(history_list),axis=0)\n    history_list_stds = np.std(np.asarray(history_list),axis=0)\n    return(history_list_means, history_list_stds)\n\n\n  def plot_accuracy(self):\n\n    train_acc_list_means, train_acc_list_stds = self.history_stat_metrics(self.history_acc_list)\n    val_acc_list_means, val_acc_list_stds = self.history_stat_metrics(self.history_val_acc_list)\n\n    fig, ax = plt.subplots()\n\n    plot_train_val_mean_std('Accuracy', \"lower center\", [0.01, 1.05], train_acc_list_means, \n                        train_acc_list_stds, val_acc_list_means, val_acc_list_stds, ax)\n    \n   \n  def plot_auc(self):\n\n    train_auc_list_means, train_auc_list_stds = self.history_stat_metrics(self.history_auc_list)\n    val_auc_list_means, val_auc_list_stds = self.history_stat_metrics(self.history_val_auc_list)\n\n    fig, ax = plt.subplots()\n\n    plot_train_val_mean_std('AUC', \"lower center\", [0.01, 1.05], train_auc_list_means, train_auc_list_stds, \n                        val_auc_list_means, val_auc_list_stds, ax)\n\n\n\n  def plot_loss(self):\n\n    train_loss_list_means, train_loss_list_stds = self.history_stat_metrics(self.history_loss_list)\n    val_loss_list_means, val_loss_list_stds = self.history_stat_metrics(self.history_val_loss_list)\n\n    fig, ax = plt.subplots()\n\n    plot_train_val_mean_std('Loss', \"upper center\", [0.01, 6], train_loss_list_means, train_loss_list_stds, \n                        val_loss_list_means, val_loss_list_stds, ax) \n\n\n  def folds_mean_max_auc(self):\n    return np.mean(self.acc_per_fold), np.std(self.acc_per_fold), np.mean(self.auc_per_fold), np.std(self.auc_per_fold), np.mean(self.loss_per_fold), np.std(self.loss_per_fold)  ","1b8d4ae9":"def prepare_data_growing(data_df, noisy_part, limit):\n    result_label = []\n    result_data = []\n\n    dirty_pos_lyme_count = 0\n    dirty_no_lyme_count = 0\n\n    clean_pos_lyme_count = 0\n    clean_no_lyme_count = 0\n\n    clean_count = 0\n    dirty_count = 0\n    \n    dirty_limit = limit * noisy_part\n    print('dirty_limit: ', dirty_limit)\n    clean_limit = limit - dirty_limit\n    print('clean_limit: ', clean_limit)\n    \n    num_images = len(data_df['image'])\n    print('**************************')\n    print('Subset (images): ', num_images)\n    for i in range(num_images):\n        if (data_df['clean'][i] == 1) and (data_df['lyme_positive'][i] == 1) and (clean_pos_lyme_count < clean_limit):\n            result_label.append(data_df['lyme_positive'][i])\n            result_data.append(data[data_df['image'][i]])\n            clean_pos_lyme_count += 1\n            clean_count += 1\n\n        if (data_df['clean'][i] == 1) and (data_df['lyme_positive'][i] == 0) and (clean_no_lyme_count < clean_limit):\n            result_label.append(data_df['lyme_positive'][i])\n            result_data.append(data[data_df['image'][i]])\n            clean_no_lyme_count += 1\n            clean_count += 1\n       \n        if (data_df['clean'][i] == 0) and (data_df['lyme_positive'][i] == 1) and (dirty_pos_lyme_count < dirty_limit):\n            result_label.append(data_df['lyme_positive'][i])\n            result_data.append(data[data_df['image'][i]])\n            dirty_pos_lyme_count += 1\n            dirty_count += 1\n\n        if (data_df['clean'][i] == 0) and (data_df['lyme_positive'][i] == 0) and (dirty_no_lyme_count < dirty_limit):\n            result_label.append(data_df['lyme_positive'][i])\n            result_data.append(data[data_df['image'][i]])\n            dirty_no_lyme_count += 1       \n            dirty_count += 1       \n\n    print('dirty_pos_lyme_count:', dirty_pos_lyme_count)            \n    print('dirty_no_lyme_count:', dirty_no_lyme_count)\n    print('dirty_count:', dirty_count)\n\n    print('clean_pos_lyme_count:', clean_pos_lyme_count)            \n    print('clean_no_lyme_count:', clean_no_lyme_count)    \n    print('clean_count:', clean_count)\n  \n    result_data = np.asarray(result_data).astype(np.float32)\n    result_label = np.asarray(result_label).astype(np.float32)\n\n    print('**************************')\n    print('Final state of subset ...')\n    print('pos_lyme_count: ', dirty_pos_lyme_count + clean_pos_lyme_count)\n    print('no_lyme_count: ', dirty_no_lyme_count + clean_no_lyme_count)\n    print('Noise ratio: ', dirty_count\/(dirty_count + clean_count))\n    print('Growing subset (data) size: ',len(result_data))\n    print('Growing subset (label) size: ',len(result_label))\n    print('**************************')\n\n    return result_data, result_label","03621220":"def plot_noise_vs(x, y, x_label, y_label, plot_title):\n  plt.plot(x, y)\n  #plt.xticks(x)\n  plt.xlabel(x_label)\n  plt.ylabel(y_label)\n  plt.title(plot_title)\n\n  model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name    \n  figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n  os.makedirs(figures_dir, exist_ok=True)    \n  fig_filename = model_name + '_' + plot_title + '.png'\n  fig_file_path = os.path.join(figures_dir, fig_filename)\n  plt.savefig(fig_file_path, dpi=300)\n\n  plt.show()","7a9060be":"def plot_noise_mean_std(metric_label, legend_location, metric_ylim, x_list, list_means, list_stds, ax):\n\n    plot_mean_std('Mean', 'b', \"lightsteelblue\", x_list, list_means, list_stds, ax)\n\n    ax.set(\n        xlim = [0, len(x_list)+5],\n        ylim = metric_ylim,\n        title = metric_label + \" vs \" + x_label,\n        xlabel = x_label,\n        ylabel = metric_label\n    )\n\n    ax.legend(loc=legend_location, ncol=2)\n\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name    \n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)    \n    fig_filename = model_name + '_' + metric_label + \"_vs_\" + x_label_filename + '_FILL.png'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, dpi=300)\n\n    plt.show()  ","58d4c586":"def run_kfold():  \n    #save_dir = '.\/MODELS\/best'\n    #save_dir = os.path.dirname(checkpoint_path)\n    #os.makedirs(save_dir, exist_ok=True)\n    \n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    save_model_filename = models_dir + model_prefix + '\/' + model_name\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    \n    #num_list = [15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65]\n    num_list = list(range(5,46,1))\n\n    metrics_list = []\n    for n in tqdm(num_list):\n        result_data, result_label = prepare_data_growing(train_df, NOISE_PART, n)\n        add_title = 'noise' + str(NOISE_PART) + '_num' + str(n)\n        model = Complex_Model(result_data=result_data, result_label=result_label, \n                              test_data=[], test_label=[],\n                              save_dir=save_dir, save_best_model_path=save_model_filename, epoch_num = EPOCHS)\n    \n        model.run()\n        model.show_results()\n        model.plot_accuracy()\n        model.plot_loss()\n        model.plot_auc()\n    \n        metrics = model.folds_mean_max_auc()\n        metrics_list.append(metrics)   \n\n    acc_mean_list = []\n    acc_std_list = []\n    auc_mean_list = []\n    auc_std_list = []\n    loss_mean_list = []\n    loss_std_list = []\n    for n in range(len(num_list)):\n        acc_mean_list.append(metrics_list[n][0])\n        acc_std_list.append(metrics_list[n][1])\n        auc_mean_list.append(metrics_list[n][2])\n        auc_std_list.append(metrics_list[n][3])\n        loss_mean_list.append(metrics_list[n][4])\n        loss_std_list.append(metrics_list[n][5])    \n        \n    # Summary\n    summary = np.column_stack((np.array(num_list),acc_mean_list,acc_std_list,auc_mean_list,auc_std_list,loss_mean_list,loss_std_list))\n    summary_df = pd.DataFrame(summary)\n    summary_df.columns = ['num', 'acc_mean', 'acc_std', 'auc_mean', 'auc_std', 'loss_mean', 'loss_std']\n    summary_df.head()\n    summary_filename = model_name + '_summary.txt'\n    summary_file_path = os.path.join(figures_dir, summary_filename)\n    summary_df.to_csv(summary_file_path, index=False)\n        \n    plot_noise_vs(num_list, acc_mean_list, x_label, 'Accuracy (mean)', 'Accuracy_mean_vs_' + x_label_filename)\n    plot_noise_vs(num_list, acc_std_list, x_label, 'Accuracy (std)', 'Accuracy_std_vs_' + x_label_filename)\n    plot_noise_vs(num_list, auc_mean_list, x_label, 'AUC (mean)', 'AUC_mean_vs_' + x_label_filename)\n    plot_noise_vs(num_list, auc_std_list, x_label, 'AUC (std)', 'AUC_std_vs_' + x_label_filename)\n    plot_noise_vs(num_list, loss_mean_list, x_label, 'Loss (mean)', 'Loss_mean_vs_' + x_label_filename)\n    plot_noise_vs(num_list, loss_std_list, x_label, 'Loss (std)', 'Loss_std_vs_' + x_label_filename)\n    \n    fig, ax = plt.subplots()\n    plot_noise_mean_std('Accuracy', \"lower center\", [0.3, 0.9], num_list, np.array(acc_mean_list), np.array(acc_std_list), ax)   \n    fig, ax = plt.subplots()\n    plot_noise_mean_std('AUC', \"upper center\", [0.5, 1.1], num_list, np.array(auc_mean_list), np.array(auc_std_list), ax)    \n    fig, ax = plt.subplots()\n    plot_noise_mean_std('Loss', \"upper center\", [-0.2, 3.6], num_list, np.array(loss_mean_list), np.array(loss_std_list), ax)    ","58334697":"def run_final(test_data, test_label):  \n    \n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    save_model_filename = models_dir + model_prefix + '\/' + model_name\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    \n    print('**************************')\n    print('Training subset ...')\n    train_data, train_label = prepare_data_growing(train_df, NOISE_PART, 45)\n    print('**************************')    \n    print('Testing subset ...')\n    print('For all testing procedures use the same: test_data, test_label')    \n    #test_data, test_label = prepare_data_growing(test_df, NOISE_PART, 20)\n    \n    add_title = 'noise' + str(NOISE_PART) + '_num' + str(45)\n    model = Complex_Model(result_data=train_data, result_label=train_label, \n                          test_data=test_data, test_label=test_label,\n                          save_dir=save_dir, save_best_model_path=save_model_filename, \n                          epoch_num = EPOCHS)\n    \n    model, best_model_metrics_filename, test_data, test_label = model.run_full()\n        \n    return model, best_model_metrics_filename","11a55129":"import itertools\ndef plot_confusion_matrix(cm, \n                          #cm_std, # for k-fold only\n                          classes,\n                          normalize=False,\n                          title='Means',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm_sum = cm.sum(axis=1)[:, np.newaxis]\n        cm = cm.astype('float') \/ cm_sum\n#        cm_std = cm_std.astype('float') \/ cm_sum # for k-fold only\n        print(\"Confusion matrix, NORMALIZED\")\n    else:\n        print('Confusion matrix, WITHOUT normalization')\n\n    print('Mean values:')\n    print(cm)\n\n#    print('Standard deviation values:') # for k-fold only\n#    print(cm_std)\n\n    plt.title(title)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45, fontsize = 8)\n    plt.yticks(tick_marks, classes, fontsize = 8)\n\n    fmt = '.2f' if normalize else '.0f' # 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 #+ \"\\n\" + r'$\\pm$' + format(cm_std[i, j], fmt), # for k-fold only\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 7)\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    # Check folders\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    print('model_name: ', model_name)\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    print('figures_dir: ', figures_dir)    \n    \n    plt.title('')     \n    fig_filename = title + '_ConfusionMatrix.eps'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, bbox_inches='tight', dpi=300)\n    \n    plt.title(title)\n    fig_filename = title + '_ConfusionMatrix.png'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, bbox_inches='tight', dpi=300)","45c781e0":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\ndef test_model(model, best_model_metrics_filename, test_data, test_label):\n#def test_model(model, best_model_metrics_filename, test_data_aug, test_label):\n    \n    model.load_weights(best_model_metrics_filename)\n    \n    test_data_aug = ImageDataGenerator(rescale=1.\/255).flow(test_data, test_label)\n    scores = model.evaluate(test_data_aug, verbose=1)\n    \n    print('**** EXTENDED TEST *****')\n    print(' Scores for TEST subset:')\n    Loss_string = 'Loss{Loss_best:.3f}'.format(Loss_best=scores[0])\n    print('Loss = {Loss_best:.4f}'.format(Loss_best=scores[0]))\n    AUC_string = 'AUC{AUC_best:.3f}'.format(AUC_best=scores[1])\n    print('AUC= {AUC_best:.4f}'.format(AUC_best=scores[1]))\n    Accuracy_string = 'Acc{acc_best:.3f}'.format(acc_best=scores[2])\n    print('Accuracy = {acc_best:.4f}'.format(acc_best=scores[2]))    \n\n    # CM\n    print('**** CONFUSION MATRIX *****')\n    #x_test, y_test = test_data, test_label\n\n    #score = model.evaluate(x_test, y_test)\n    #print('score:', score)\n    #predicted_probs = model.predict(x_test)\n    #print('predicted_probs:', predicted_probs)\n    \n    #samples = test_data_aug.samples\n    #nb_samples = len(samples)\n    #predicted_probs = model.predict_generator(test_data_aug, steps = np.ceil(nb_samples\/32))\n    predicted_probs = model.predict(test_data\/255)\n    #print('predicted_probs:',predicted_probs)\n\n    df = pd.DataFrame(predicted_probs)\n    predicted_probs = df[0].values.tolist()\n    predicted = [round(x) for x in predicted_probs]\n    #predicted = predicted_probs #.argmax(axis=-1)\n    #print('predicted:', predicted)\n\n    expected = test_label\n    #expected = y_test\n    #expected = y_test.argmax(axis=-1)\n    #print('expected:', expected)\n\n    # Print test of confusion matrix\n    conf_matrix = confusion_matrix(expected, predicted)\n    print(conf_matrix)      \n    \n    # Check folders\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    print('model_name: ', model_name)\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    print('figures_dir: ', figures_dir)\n    title = model_name\n    print('title: ', title)\n\n    BINARY = True\n\n    #class_names = ['AZU','ONU', 'IZU', 'MYSLITE']\n    #class_names = subfolder_names\n    class_names = ['Lyme','NO']\n\n    # Compute confusion matrix\n    #cnf_matrix = confusion_matrix(expected, predicted)\n    np.set_printoptions(precision=4)\n\n    print('**************************')\n    # Plot non-normalized confusion matrix\n    if BINARY == True:\n        fig, ax = plt.subplots(dpi=300, figsize=(6, 3)) \n    else: \n        fig, ax = plt.subplots(dpi=300, figsize=(15, 15))\n    \n    plot_confusion_matrix(conf_matrix, \n                      #confusion_matrix_stds, # for k-fold only\n                      classes=class_names, normalize=False,\n                      #title = MODEL_NAME + '_' + TRAIN_TITLE + '_' + DA_TYPE + '_' + SAVED_MODEL + '_NOT_normalized')\n                      title = title + '_NOT_normalized')\n\n    print('**************************')\n    # Plot normalized confusion matrix\n    if BINARY == True:\n        fig, ax = plt.subplots(dpi=300, figsize=(6, 3)) \n    else: \n        fig, ax = plt.subplots(dpi=300, figsize=(15, 15))\n    plot_confusion_matrix(conf_matrix, \n                      #confusion_matrix_stds, # for k-fold only\n                      classes=class_names, normalize=True,\n                      #title = MODEL_NAME + '_' + TRAIN_TITLE + '_' + DA_TYPE + '_' + SAVED_MODEL + '_normalized')\n                      title = title + '_normalized')\n\n    plt.show()   \n    \n    TP = conf_matrix[0][0]\n    #print(TP)\n    FN = conf_matrix[0][1]\n    FP = conf_matrix[1][0]\n    TN = conf_matrix[1][1]    \n    \n    # ROC\n    from sklearn.metrics import roc_curve, auc\n\n    #fpr = dict()\n    #tpr = dict()\n    #roc_auc = dict()\n\n    fpr, tpr, _ = roc_curve(expected, predicted_probs)\n    roc_auc = auc(fpr, tpr)\n    #roc_auc_1 = roc_auc_score(expected, predicted_probs)\n\n    fig, ax = plt.subplots(figsize=(5, 5)) \n\n    ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Random, 0.5\", alpha=0.8)\n    ax.plot(fpr, tpr,\n         #label = \"AUC, %0.4f; , %0.4f\" % (roc_auc,roc_auc_1),\n         label = \"AUC, %0.4f\" % (roc_auc),\n         color='navy', linestyle='-', linewidth=4)\n\n    ax.legend(loc=\"lower right\", title = 'AUC')\n\n    plt.title('')     \n    fig_filename = title + '_ROC.eps'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, bbox_inches='tight', dpi=300)\n    \n    plt.title(title)\n    fig_filename = title + '_ROC.png'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, bbox_inches='tight', dpi=300)\n\n    plt.show()   \n    \n    # Save TEST results\n    \n    # Check folders\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    print('model_name: ', model_name)\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    print('figures_dir: ', figures_dir)\n\n    test_CM_AUC_filename = model_name + '_CM_AUC.csv'\n    print('test_CM_AUC_filename: ', test_CM_AUC_filename)\n    test_CM_AUC_file_path = os.path.join(figures_dir, test_CM_AUC_filename)\n\n    test_ROC_filename = model_name + '_ROC.csv'\n    print('test_results_filename: ', test_ROC_filename)\n    test_ROC_file_path = os.path.join(figures_dir, test_ROC_filename)\n\n    roc_df = pd.DataFrame(fpr.T, columns=['fpr'])\n    roc_df['tpr'] = pd.DataFrame(tpr)\n    roc_df.head()\n    roc_df.to_csv(test_ROC_file_path)\n\n    cm_auc_df = pd.DataFrame(columns=['Loss', 'AUC', 'Accuracy', 'ROC_AUC', 'TP', 'FN', 'FP', 'TN'], \n                         data=[[scores[0], scores[1], scores[2], roc_auc, TP, FN, FP, TN]])\n    cm_auc_df.head()\n    cm_auc_df.to_csv(test_CM_AUC_file_path)    ","406b2651":"def model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name):\n    # Check folders\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    print('model_name: ', model_name)\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    print('figures_dir: ', figures_dir)\n    return model_name, figures_dir","3157d86d":"# For all testing procedures: use the same test_data, test_label\nprint('Testing subset ...')\nNOISE_PART = 0.5\ntest_data, test_label = prepare_data_growing(test_df, NOISE_PART, 20)\ntest_data_aug = ImageDataGenerator(rescale=1.\/255).flow(test_data, test_label)","7b279d43":"# Train from ImageNet-trainable Weights\nWEIGHTS = 'imagenet'\nWEIGHTS_name = 'imagenet'\n# Train from scratch\n#WEIGHTS = None\n#WEIGHTS_name = 'Scratch'\n\nadd_title = ''\nx_label = 'Number of images'\nx_label_filename = 'Num'    \n\n#save_dir = ''\nsave_dir = models_dir + model_prefix + '\/'\nmodel_name = ''\nsave_model_filename = ''\nfigures_dir = ''","1d766763":"# Clean dataset \nNOISE_PART = 0.0\ndata_quality = 'CLEAN'\n\n# Dirty dataset \n#NOISE_PART = 1.0\n#data_quality = 'DIRTY'\n\n# Dirty & Clean dataset \n#NOISE_PART = 0.5\n#data_quality = 'DIRTY_CLEAN'","8fa15628":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\n#TRAINABLE = True\n#TL_name = 'NOTL'\n# Transfer Learning (TL) -> False\nTRAINABLE = False\nTL_name = 'TL'","252f7bef":"%%time\n# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","d0913dc1":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","b2a5a0c3":"%%time\n\n# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","5cdc2cc0":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","4c8339e9":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","b24995ea":"%%time\n# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","f353b28d":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","cb916a45":"%%time\n# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","1eecced8":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","f96a4a5b":"# Clean dataset \n#NOISE_PART = 0.0\n#data_quality = 'CLEAN'\n\n# Dirty dataset \nNOISE_PART = 1.0\ndata_quality = 'DIRTY'\n\n# Dirty & Clean dataset \n#NOISE_PART = 0.5\n#data_quality = 'DIRTY_CLEAN'","e6fb071a":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\n#TRAINABLE = True\n#TL_name = 'NOTL'\n# Transfer Learning (TL) -> False\nTRAINABLE = False\nTL_name = 'TL'","f75db8bd":"%%time\n# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","bc1cf90e":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","bb8afb48":"%%time\n# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","e2197d68":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","d2f0ee1a":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","19fd636d":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","70f5766e":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","30abf3a6":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","9c2c0830":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","e8b614e5":"# Clean dataset \n#NOISE_PART = 0.0\n#data_quality = 'CLEAN'\n\n# Dirty dataset \n#NOISE_PART = 1.0\n#data_quality = 'DIRTY'\n\n# Dirty & Clean dataset \nNOISE_PART = 0.5\ndata_quality = 'DIRTY_CLEAN'","c2c792f5":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\n#TRAINABLE = True\n#TL_name = 'NOTL'\n# Transfer Learning (TL) -> False\nTRAINABLE = False\nTL_name = 'TL'","ae61bf55":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","96de75e8":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","83eebb75":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","97956ea7":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","94074f91":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","4dc25ec8":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","07e1d321":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","5df8270f":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","0ae83633":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","a616e5ca":"# Train from ImageNet-trainable Weights\n#WEIGHTS = 'imagenet'\n#WEIGHTS_name = 'imagenet'\n# Train from scratch\nWEIGHTS = None\nWEIGHTS_name = 'random'","2c8bad92":"# Clean dataset \nNOISE_PART = 0.0\ndata_quality = 'CLEAN'\n\n# Dirty dataset \n#NOISE_PART = 1.0\n#data_quality = 'DIRTY'\n\n# Dirty & Clean dataset \n#NOISE_PART = 0.5\n#data_quality = 'DIRTY_CLEAN'","8216c8aa":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","f0d4cb2d":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","ec7a8a68":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","9f55cec9":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","eda54dc1":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","e53f2047":"# Clean dataset \n#NOISE_PART = 0.0\n#data_quality = 'CLEAN'\n\n# Dirty dataset \nNOISE_PART = 1.0\ndata_quality = 'DIRTY'\n\n# Dirty & Clean dataset \n#NOISE_PART = 0.5\n#data_quality = 'DIRTY_CLEAN'","ef907d33":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","4a91d91f":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","5304b900":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","060c39a2":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","d41c3e33":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","c62b159f":"# Clean dataset \n#NOISE_PART = 0.0\n#data_quality = 'CLEAN'\n\n# Dirty dataset \n#NOISE_PART = 1.0\n#data_quality = 'DIRTY'\n\n# Dirty & Clean dataset \nNOISE_PART = 0.5\ndata_quality = 'DIRTY_CLEAN'","9f87fc0c":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","95712bf8":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","e3f70530":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","1dd20a1c":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","3b2bffa9":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","0973e7bd":"# Prepare data","bb9708ad":"### Regime = DIRTY + CLEAN + TL","51b78032":"### Regime = DIRTY + TL","60778cc2":"### Regime = DIRTY + CLEAN + NOTL","c08236b7":"## Basic Parameters","8d7936d9":"# Plot functions","f172e584":"# Model_class","92a63cef":"#### Regime = DIRTY + CLEAN + TL + NODA","5c106273":"## Regime = DIRTY","94d6b9be":"#### Regime = CLEAN + NOTL + DA","46ac1294":"### Regime = DIRTY + NOTL","da861a48":"## Load from Kaggle","176200f0":"### Regime = CLEAN + TL","6d2b03c9":"#### TEST -> model","c657a52b":"# From random weights","18f66dc8":"#### Regime = DIRTY + TL + DA","8be9ffeb":"#### Regime = CLEAN + NOTL + NODA","d58f9609":"#### Regime = CLEAN + TL + DA","701d2a27":"## Regime = DIRTY + CLEAN","a7201584":"#### Regime = CLEAN + NOTL + NODA","35e6036b":"#### Regime = DIRTY + NOTL + NODA","103150ef":"#### Regime = DIRTY + CLEAN + NOTL + DA","ae5fc9f4":"#### Regime = DIRTY + NOTL + DA","a94f277d":"# Regimes","bd7b1f23":"# Regime = CLEAN","b1285a8d":"### Regime = CLEAN + NOTL","aecf4461":"#### Regime = DIRTY + CLEAN + NOTL + NODA","33ed8c71":"## Data PreProcessing ","69dec48c":"# Different number of images","d213b1da":"#### Regime = DIRTY + NOTL + NODA","4a28363f":"## Regime = DIRTY + CLEAN","3caada12":"#### Regime = DIRTY + CLEAN + NOTL + DA","98f3e70e":"### Regime = CLEAN + NOTL","47130586":"## Load from Local Storage","61e7bd75":"#### Regime = CLEAN + TL + NODA","1988b514":"#### Regime = DIRTY + TL + NODA","57644e12":"## Regime = CLEAN","355c133d":"## Regime = DIRTY","e703b6ef":"#### Regime = DIRTY + CLEAN + TL + DA","976fe283":"### Regime = DIRTY + NOTL","b6b1c6fa":"#### Regime = CLEAN + NOTL + DA","19cdc22b":"### Regime = DIRTY + CLEAN + NOTL","b74f4451":"#### Regime = DIRTY + NOTL + DA","2713ceb2":"#### Regime = DIRTY + CLEAN + NOTL + NODA"}}