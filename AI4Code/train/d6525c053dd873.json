{"cell_type":{"d6bb9aa5":"code","2e34d720":"code","95660c1e":"code","1c644c82":"code","dc918d56":"code","cedf533a":"code","acb2d457":"code","13fedcc3":"code","aa174bc6":"markdown","e2a1720c":"markdown","fb9a597e":"markdown"},"source":{"d6bb9aa5":"# Importing some libraries\nimport pandas as pd\nimport numpy as np\n\n# Import keras\nfrom keras.models import Sequential \nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n# Import from sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV","2e34d720":"url = \"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\"\npima_data = pd.read_csv(url)\npima_data.shape","95660c1e":"#Setting a random seed\nseed = 7\nnp.random.seed(seed)\n\n# Seperating predictors and response variable.\nX = pima_data.iloc[:,0:8]\ny = pima_data.iloc[:,8]","1c644c82":"# Create model \nmodel = Sequential()\n# First hidden layer with 12 neurons.\nmodel.add(Dense(12, input_dim = 8, kernel_initializer = 'uniform', activation = 'relu'))\n# Second hidden layer with 8 neurons\nmodel.add(Dense(8, kernel_initializer = 'uniform', activation = 'relu'))\n# Output layer with 11 neuron.\nmodel.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))","dc918d56":"# Compile model\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","cedf533a":"# Fit the model\nmodel.fit(X, y, validation_split = 0.33, epochs = 150, batch_size = 10)","acb2d457":"scores = model.evaluate(X,y)\nprint(\"%s %.2f%%\" %(model.metrics_names[1], scores[1] * 100))","13fedcc3":"#Setting a random seed\nseed = 7\nnp.random.seed(seed)\n\ndef create_model(kernel_initializer = 'glorot_uniform', optimizer = 'rmsprop'):\n    model = Sequential()\n    model.add(Dense(12, input_dim = 8, kernel_initializer = kernel_initializer, activation = 'relu'))\n    model.add(Dense(8, kernel_initializer = kernel_initializer, activation = 'relu'))\n    model.add(Dense(1, kernel_initializer = kernel_initializer, activation = 'sigmoid'))\n    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n    return model\n\n# Create a model from KerasClassifier\nmodel = KerasClassifier(build_fn = create_model, verbose = 0)\n\n# Grid Search epochs, batch_size and optimizer.\noptimizers = ['adam','rmsprop']\nkernel_initializer = ['glorot_uniform', 'normal','uniform']\nepochs = (50,100,150)\nbatches = (5,10,20)\n\nparam_grid = dict(optimizer = optimizers, kernel_initializer = kernel_initializer,\n                 nb_epoch = epochs, batch_size = batches)\ngrid = GridSearchCV(estimator = model, param_grid = param_grid)\ngrid_result = grid.fit(X,y)\n\nprint(\"Best search %f using %s\" %(grid_result.best_score_, grid_result.best_params_))","aa174bc6":"# Learning How to use Keras for Deep Learning","e2a1720c":"### Grid Search for Hyperparamter\n\nThe Keras library provides a convenient wrapper for deep learning models to be used as classification or regression estimators in scikit-learn.\n\nWe are trying to optimize the 4 hyperparameters :-\n1. Optimizer.\n2. weight intializer.\n3. No. of epochs.\n4. Mini batches.","fb9a597e":"Now we will create our model :-\n\n1. We have 8 input neurons corresponding to 8 input variables.\n2. weight intilization will be done using a uniform distribution.\n3. For the hidden layers we will use 'relu' activation function.\n4. For output layer we will use 'sigmoid' activation function.\n\nWe will also calculate our validation accuracy on 33% of the hold out data."}}