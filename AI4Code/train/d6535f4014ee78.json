{"cell_type":{"f8151d0d":"code","3e219285":"code","e5ba4589":"code","5b6c5453":"code","83e5615f":"code","1d564b5c":"code","8bafcbca":"code","4e8feab5":"code","3e0f088c":"code","f324b6df":"code","da39b3eb":"code","1a1daae4":"code","30a2c9a9":"code","0927fc87":"code","1cb53275":"code","c2e1687b":"code","9d9148cb":"markdown","76bb6dcf":"markdown"},"source":{"f8151d0d":"import numpy as np\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint \nfrom keras.utils import plot_model\nimport os\nfrom glob import glob\nfrom tqdm import tqdm\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","3e219285":"data = [\"Aishwarya_Rai\", \"Alia_Bhatt\", \"Amitabh_Bachchan\", \"Disha_Patani\", \"Hrithik_Roshan\", \"Jacqueline_Fernandez\", \"Salman_Khan\", \"Shah_Rukh_Khan\", \"Shahid_Kapoor\", \"Shraddha_Kapoor\", \"Varun_Dhawan\"]\nd1=r'..\/input\/bollywood-celeb-testing\/Bollywood_Training'\n\nINPUT_DIM = (64, 64,3) # Image dimension\nBATCH_SIZE = 64\nImg_size = 64\nZ_DIM = 200 # Dimension of the latent vector (z)\nIMAGES=[]\nfor i in data:\n    Train_dir = d1+'\/'+i+'\/'\n    print(Train_dir)\n    label=data.index(i)\n    for img in tqdm(os.listdir(Train_dir)):\n        path = os.path.join(Train_dir,img)\n    #print(path,label)\n        img = cv2.resize(cv2.imread(path,cv2.IMREAD_GRAYSCALE),(Img_size,Img_size))\n        IMAGES.append([np.array(img),np.array(label)])","e5ba4589":"no_of_images = int(str(len(IMAGES)))\nprint(\"Total number of images: \" +str(no_of_images))  ","5b6c5453":"data_flow = ImageDataGenerator(rescale=1.\/255).flow_from_directory(d1, \n                                                                   target_size = INPUT_DIM[:2],\n                                                                   batch_size = BATCH_SIZE,\n                                                                   shuffle = True,\n                                                                   class_mode = 'input',\n                                                                   subset = 'training'\n                                                                   )","83e5615f":"# ENCODER\ndef vae_encoder(input_dim, output_dim, conv_filters, conv_kernel_size, \n                  conv_strides, use_batch_norm = False, use_dropout = False):\n  \n  # Clear tensorflow session to reset layer index numbers to 0 for LeakyRelu, \n  # BatchNormalization and Dropout.\n  # Otherwise, the names of above mentioned layers in the model \n  # would be inconsistent\n  global K\n  K.clear_session()\n  \n  # Number of Conv layers\n  n_layers = len(conv_filters)\n\n  # Define model input\n  encoder_input = Input(shape = input_dim, name = 'encoder_input')\n  x = encoder_input\n\n  # Add convolutional layers\n  for i in range(n_layers):\n      x = Conv2D(filters = conv_filters[i], \n                  kernel_size = conv_kernel_size[i],\n                  strides = conv_strides[i], \n                  padding = 'same',\n                  name = 'encoder_conv_' + str(i)\n                  )(x)\n      if use_batch_norm:\n        x = BatchNormalization()(x)\n  \n      x = LeakyReLU()(x)\n\n      if use_dropout:\n        x = Dropout(rate=0.25)(x)\n\n  # Required for reshaping latent vector while building Decoder\n  shape_before_flattening = K.int_shape(x)[1:] \n  \n  x = Flatten()(x)\n  \n  z_mean = Dense(output_dim, name = 'mu')(x)\n  z_log_var = Dense(output_dim, name = 'z_log_var')(x)\n\n  # Defining a function for sampling\n  def sampling(args):\n    z_mean, z_z_log_var = args\n    epsilon = K.random_normal(shape=K.shape(z_mean), mean=0., stddev=1.) \n    return z_mean + K.exp(z_z_log_var\/2)*epsilon   \n  \n  # Using a Keras Lambda Layer to include the sampling function as a layer \n  # in the model\n  encoder_output = Lambda(sampling, name='encoder_output')([z_mean, z_log_var])\n\n  return encoder_input, encoder_output, z_mean, z_log_var, shape_before_flattening, Model(encoder_input, encoder_output)\n","1d564b5c":"vae_encoder_input, vae_encoder_output,  z_mean, z_log_var, vae_shape_before_flattening, vae_encoder  = vae_encoder(input_dim = INPUT_DIM,\n                                    output_dim = Z_DIM, \n                                    conv_filters = [32, 64, 64, 64],\n                                    conv_kernel_size = [3,3,3,3],\n                                    conv_strides = [2,2,2,2])\nvae_encoder.summary()","8bafcbca":"# Decoder\ndef vae_decoder(input_dim, shape_before_flattening, conv_filters, conv_kernel_size, \n                  conv_strides):\n\n  # Number of Conv layers\n  n_layers = len(conv_filters)\n\n  # Define model input\n  vae_decoder_input = Input(shape = (input_dim,) , name = 'vae_decoder_input')\n\n  # To get an exact mirror image of the encoder\n  x = Dense(np.prod(shape_before_flattening))(vae_decoder_input)\n  x = Reshape(shape_before_flattening)(x)\n\n  # Add convolutional layers\n  for i in range(n_layers):\n      x = Conv2DTranspose(filters = conv_filters[i], \n                  kernel_size = conv_kernel_size[i],\n                  strides = conv_strides[i], \n                  padding = 'same',\n                  name = 'decoder_conv_' + str(i)\n                  )(x)\n      \n      # Adding a sigmoid layer at the end to restrict the outputs \n      # between 0 and 1\n      if i < n_layers - 1:\n        x = LeakyReLU()(x)\n      else:\n        x = Activation('sigmoid')(x)\n\n  # Define model output\n  vae_decoder_output = x\n\n  return vae_decoder_input, vae_decoder_output, Model(vae_decoder_input, vae_decoder_output)","4e8feab5":"vae_decoder_input, vae_decoder_output, vae_decoder = vae_decoder(input_dim = Z_DIM,\n                                        shape_before_flattening = vae_shape_before_flattening,\n                                        conv_filters = [64,64,32,3],\n                                        conv_kernel_size = [3,3,3,3],\n                                        conv_strides = [2,2,2,2]\n                                        )\nvae_decoder.summary()\n","3e0f088c":"# The input to the model will be the image fed to the encoder.\nvae_input = vae_encoder_input\n\n# Output will be the output of the decoder. The term - decoder(encoder_output) \n# combines the model by passing the encoder output to the input of the decoder.\nvae_output = vae_decoder(vae_encoder_output)\n\n# Input to the combined model will be the input to the encoder.\n# Output of the combined model will be the output of the decoder.\nvae_model = Model(vae_input, vae_output)\n\nvae_model.summary()","f324b6df":"LEARNING_RATE = 0.0005\nN_EPOCHS = 600\nLOSS_FACTOR = 10000","da39b3eb":"def kl_loss(y_true, y_pred):\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    return kl_loss\n\ndef reconstruction_loss(y_true, y_pred):\n    return K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n\ndef compute_kernel(x, y):\n    x_size = tf.shape(x)[0]\n    y_size = tf.shape(y)[0]\n    dim = tf.shape(x)[1]\n    tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n    tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n    return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) \/ tf.cast(dim, tf.float32))\n\ndef compute_mmd(x, y, sigma_sqr=1.0):\n    x_kernel = compute_kernel(x, x)\n    y_kernel = compute_kernel(y, y)\n    xy_kernel = compute_kernel(x, y)\n    return tf.reduce_mean(x_kernel) + tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel)\n\ndef compute_mcd(x, y, sigma_sqr=1.0):\n    x_kernel = compute_kernel(x, x)\n    y_kernel = compute_kernel(y, y)\n    xy_kernel = compute_kernel(x, y)\n    yx_kernel = compute_kernel(y, x)\n    squared_x_kernel = tf.square(x_kernel)\n    squared_y_kernel = tf.square(y_kernel)\n    squared_xy_kernel = tf.square(xy_kernel)\n    #squared_yx_kernel = tf.square(yx_kernel)\n    return (tf.reduce_mean(squared_x_kernel) - 2 * tf.reduce_mean(tf.square(tf.reduce_mean(x_kernel)))\n                + tf.square(tf.reduce_mean(x_kernel)) - 2 * tf.reduce_mean(tf.square(squared_xy_kernel))\n                + 2 * tf.reduce_mean(tf.square(tf.reduce_mean(xy_kernel)))\n                + 2 * tf.reduce_mean(tf.square(tf.reduce_mean(yx_kernel)))\n                - 2 * tf.square(tf.reduce_mean(xy_kernel)) + tf.reduce_mean(squared_y_kernel)\n                - 2 * tf.reduce_mean(tf.square(tf.reduce_mean(y_kernel)))\n                + tf.square(tf.reduce_mean(y_kernel)))\n \ntrue_samples = tf.random.normal(tf.stack([200, Z_DIM]))\nloss_mmd = compute_mmd(true_samples, vae_encoder_output)\n\nloss_mcd = compute_mcd(true_samples, vae_encoder_output)\nbeta = 5  #ideal value of beta should be in between 0.001<beta<10. it is one of the hyperparameters which needs to be tuned\nloss_mmcd = loss_mmd + beta * loss_mcd\n\ndef total_loss(y_true, y_pred):\n    return LOSS_FACTOR*reconstruction_loss(y_true, y_pred) + loss_mmcd\n    #return LOSS_FACTOR*r_loss(y_true, y_pred) + kl_loss(y_true, y_pred)\n    #return LOSS_FACTOR*reconstruction_loss(y_true, y_pred) + loss_mmcd\n    \nadam_optimizer = Adam(lr = LEARNING_RATE)\n\nvae_model.compile(optimizer=adam_optimizer, loss = total_loss, metrics = [reconstruction_loss])\n\ncheckpoint_vae = ModelCheckpoint(os.path.join(r'.\/weightsmmcd.h5'), save_weights_only = True, verbose=1)\n","1a1daae4":"tf.config.experimental_run_functions_eagerly(True)\nvae_model.fit_generator(data_flow, \n                        shuffle=True, \n                        epochs = N_EPOCHS, \n                        initial_epoch = 0, \n                        steps_per_epoch=no_of_images \/ BATCH_SIZE,\n                        callbacks=[checkpoint_vae])","30a2c9a9":"\nexample_batch = next(data_flow)\nexample_batch = example_batch[0]\nexample_images = example_batch[:10]","0927fc87":"def plot_compare_vae(images=None):\n    \n  \n  if images is None:\n    example_batch = next(data_flow)\n    example_batch = example_batch[0]\n    images = example_batch[:10]\n\n  n_to_show = images.shape[0]\n  reconst_images = vae_model.predict(images)\n\n  fig = plt.figure(figsize=(15, 3))\n  fig.subplots_adjust(hspace=0.4, wspace=0.4)\n\n  for i in range(n_to_show):\n      img = images[i].squeeze()\n      sub = fig.add_subplot(2, n_to_show, i+1)\n      sub.axis('off')        \n      sub.imshow(img)\n      cv2.imwrite(r\".\/original.\"+ str(i)+\".jpeg\", 255*img)\n\n  for i in range(n_to_show):\n      img = reconst_images[i].squeeze()\n      sub = fig.add_subplot(2, n_to_show, i+n_to_show+1)\n      sub.axis('off')\n      sub.imshow(img)\n      cv2.imwrite(r\".\/mmcdvaegen.\"+ str(i)+\".jpeg\", 255*img)\n      ","1cb53275":"plot_compare_vae(images = example_images)","c2e1687b":"import matplotlib.pyplot as plt\nimport cv2\n\n# read images\n#img11 = cv2.imread(r\"C:\\Users\\sushilkumar.yadav\\Desktop\\vmware\\Personal\\Research\\Image_recognition_in_wild_using_Deep_Learning\\junichiro_koizumi.1.jpeg\")\n#img12 = cv2.imread(r\"C:\\Users\\sushilkumar.yadav\\Desktop\\vmware\\Personal\\Research\\Image_recognition_in_wild_using_Deep_Learning\\Database_FR\\New_VAE_Database\\junichiro_koizumi\\junichiro_koizumi.0.jpeg\")\n\nimg11 = cv2.imread(r\".\/original.5.jpeg\")\nimg11 = cv2.resize(img11,(64,64))\nimg12 = cv2.imread(r\".\/mmcdvaegen.5.jpeg\")\n\nimg1 = cv2.cvtColor(img11, cv2.COLOR_BGR2GRAY)\nimg2 = cv2.cvtColor(img12, cv2.COLOR_BGR2GRAY)\n#sift\nsift = cv2.SIFT_create()\n\nkeypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\nkeypoints_2, descriptors_2 = sift.detectAndCompute(img2,None)\n\nimg_1 = cv2.drawKeypoints(img1,keypoints_1,img11)\nplt.figure()\nplt.imshow(img_1)\n\nimg_2 = cv2.drawKeypoints(img2,keypoints_2,img12)\nplt.figure()\nplt.imshow(img_2)\nprint(len(keypoints_1), len(keypoints_2))\n#feature matching\nbf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n\nmatches = bf.match(descriptors_1,descriptors_2)\nmatches = sorted(matches, key = lambda x:x.distance)\n\nimg3 = cv2.drawMatches(img1, keypoints_1, img2, keypoints_2, matches[:50], img2, flags=2)\nplt.figure()\nplt.imshow(img3),plt.show()","9d9148cb":"**Maximum Mean and Covariance Discrepancy - Variational AutoEncoder**\n1. Improving Normal VAE results using Maximum Mean and Covariance discrepancy (MMCD) Technique\n2. Implementing New algorithm for Variational AutoEncoder (VAE) which reconstructs image better than    Normal VAE\n3. I have named this new version of VAE as MMCD_VAE\n4. I have used equations from the research paper: https:\/\/link.springer.com\/article\/10.1007\/s11063-019-10090-0\n5. few snippets of the code is taken from Internet\n6. Complete face database can be found at https:\/\/www.kaggle.com\/sushilyadav1998\/bollywood-celeb-localized-face-dataset\n","76bb6dcf":"**Conclusion**\nWe can see that that the generated image using MMCD_VAE contains more number of sift features and mostly most of the sift features are matching exactly. When compared the reconstructed samples of normal VAE (Trained for multi-class) with MMCD VAE (Trained for multi-class), it can be found that the image generated using MMCD_VAE is more clear and  contains more information. see the image for normal VAE generated image (SIFT Count for original image was 41 while the SIFT count for Normal VAE is 20)\n\n![image.png](attachment:image.png)\n\nThe generated images from MMCD_VAE model can be used in applications such as face recognition, object recognition, etc for improvement in the accuracy."}}