{"cell_type":{"44fff839":"code","826eaef3":"code","d2457589":"code","1bee68d3":"code","6c4dd07d":"code","0fa22fa1":"code","4b660dda":"code","ae464955":"code","1307f792":"code","788ce3bc":"code","3580cf0c":"code","e51b1794":"code","510a8d34":"code","a0b1b3e5":"code","8b58b16f":"code","167646b9":"code","5c12cde0":"code","f9e56fa8":"code","f5523f16":"code","2a1022a7":"code","79dc2c01":"code","300f659b":"code","31e1b60c":"code","cc75f334":"code","e74658f5":"code","23c53831":"code","73ee3ee1":"code","2ac93cfd":"code","49e7cc39":"code","71889f98":"code","e5e09079":"code","1415f526":"code","bfebda4d":"code","674aee22":"code","3c21bb49":"code","f40a4e1d":"code","cc00a0d7":"code","480e21d2":"code","3230a9fe":"code","01ddbdf1":"code","631005b6":"code","04d78c75":"code","8732653d":"code","5b82155a":"markdown","da6d3264":"markdown","8850ac1a":"markdown","3941a109":"markdown","799475d3":"markdown","8f227f13":"markdown","ae668368":"markdown","ac03636a":"markdown","25acac44":"markdown","d8b656e6":"markdown","0454ef40":"markdown","601773e8":"markdown","e48b833b":"markdown","b28e7439":"markdown","d9488ea1":"markdown","ed8ab233":"markdown","14d145a4":"markdown","0c724fed":"markdown","bf77a681":"markdown","ebc15be3":"markdown","31d30f84":"markdown","ffe3b7eb":"markdown","806e0f23":"markdown","3e712631":"markdown","97533e87":"markdown","38b2a513":"markdown","077306b4":"markdown","a5501fff":"markdown","61e2a140":"markdown","32a7d60b":"markdown","000ade9b":"markdown","c7286ee8":"markdown","0f912049":"markdown","632a9576":"markdown","439f80ed":"markdown","d8d323cb":"markdown","557124b6":"markdown","412a2237":"markdown","e165d158":"markdown","5c3a64f8":"markdown","25628a88":"markdown","207a86a2":"markdown","f2c221e1":"markdown"},"source":{"44fff839":"import numpy as np  #NumPy is a general-purpose array-processing package. It provides a high-performance multidimensional array object, and tools for working with these arrays.\nimport pandas as pd  #pandas is a popular Python-based data analysis toolkit. It presents a diverse range of utilities, ranging from parsing multiple file formats to converting an entire data table into a NumPy matrix array.\nimport matplotlib.pyplot as plt  #matplotlib.pyplot is a collection of command style functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc.\nimport seaborn as sns  #Seaborn is a library in Python predominantly used for making statistical graphics. Seaborn is a data visualization library built on top of matplotlib and closely integrated with pandas data structures in Python. Visualization is the central part of Seaborn which helps in exploration and understanding of data.\nimport warnings\nwarnings.filterwarnings(\"ignore\")","826eaef3":"netflix_dataset=pd.read_csv(\"\/kaggle\/input\/netflix-dataset\/netflix_dataset.csv\")\nnetflix_dataset.head()","d2457589":"netflix_dataset.info()","1bee68d3":"#Identify the unique values\ndict = {}\nfor i in list(netflix_dataset.columns):\n    dict[i] = netflix_dataset[i].value_counts().shape[0]\n    \nprint(pd.DataFrame(dict,index = [\"unique count\"]).transpose())","6c4dd07d":"# Missing values\nprint('Table of missing values: ')\nprint(netflix_dataset.isnull().sum())","0fa22fa1":"netflix_shows=netflix_dataset[netflix_dataset['type']=='TV Show']\nnetflix_movies=netflix_dataset[netflix_dataset['type']=='Movie']\n\nplt.figure(figsize=(10,7))\nsns.set(style=\"whitegrid\")\nax = sns.countplot(x=\"type\", data=netflix_dataset, palette=\"Set1\")\nax.set_title(\"TV Shows VS Movies\")","4b660dda":"netflix_date = netflix_shows[['date_added']].dropna()\nnetflix_date['year'] = netflix_date['date_added'].apply(lambda x : x.split(', ')[-1])\nnetflix_date['month'] = netflix_date['date_added'].apply(lambda x : x.lstrip().split(' ')[0])\n\nmonth_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'][::-1]\ndf = netflix_date.groupby('year')['month'].value_counts().unstack().fillna(0)[month_order].T\nplt.figure(figsize=(10, 7), dpi=200)\nplt.pcolor(df, cmap='afmhot_r', edgecolors='white', linewidths=2) # heatmap\nplt.xticks(np.arange(0.5, len(df.columns), 1), df.columns, fontsize=7, fontfamily='serif')\nplt.yticks(np.arange(0.5, len(df.index), 1), df.index, fontsize=7, fontfamily='serif')\n\nplt.title('Netflix Contents Update - HeatMap for Analysis', fontsize=12, fontfamily='calibri', fontweight='bold', position=(0.20, 1.0+0.02))\ncbar = plt.colorbar()\n\ncbar.ax.tick_params(labelsize=8) \ncbar.ax.minorticks_on()\nplt.show()","ae464955":"#Movie ratings analysis\nplt.figure(figsize=(12,10))\nsns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"rating\", data=netflix_dataset, palette=\"Set1\", order=netflix_dataset['rating'].value_counts().index[0:15])","1307f792":"imdb_ratings=pd.read_csv('\/kaggle\/input\/netflix-dataset\/IMDb ratings.csv', usecols=['weighted_average_vote'])\nimdb_titles=pd.read_csv('\/kaggle\/input\/netflix-dataset\/IMDb movies.csv', usecols=['title','year','genre'])\nratings = pd.DataFrame({'Title':imdb_titles.title,\n                    'Release Year':imdb_titles.year,\n                    'Rating': imdb_ratings.weighted_average_vote,\n                    'Genre':imdb_titles.genre})\nratings.drop_duplicates(subset=['Title','Release Year','Rating'], inplace=True)\nratings.shape","788ce3bc":"ratings.dropna()\njoint_data=ratings.merge(netflix_dataset,left_on='Title',right_on='title',how='inner')\njoint_data=joint_data.sort_values(by='Rating', ascending=False)","3580cf0c":"#Top rated 10 movies in Netflix are:\nimport plotly.express as px\ntop_rated=joint_data[0:10]\nfig =px.sunburst(\n    top_rated,\n    path=['title','country'],\n    values='Rating',\n    color='Rating')\nfig.show()","e51b1794":"#Top countries creating contents\ncountry_count=joint_data['country'].value_counts().sort_values(ascending=False)\ncountry_count=pd.DataFrame(country_count)\ntopcountries=country_count[0:11]\ntopcountries","510a8d34":"Last_fifteen_years = netflix_dataset[netflix_dataset['release_year']>2005 ]\nLast_fifteen_years.head()","a0b1b3e5":"#Year wise analysis\nplt.figure(figsize=(12,10))\nsns.set(style=\"darkgrid\")\nax = sns.countplot(y=\"release_year\", data=Last_fifteen_years, palette=\"Set1\", order=netflix_dataset['release_year'].value_counts().index[0:15])","8b58b16f":"#Analysis of TV Shows in Netflix\n\ncountries={}\nnetflix_shows['country']=netflix_shows['country'].fillna('Unknown')\ncou=list(netflix_shows['country'])\nfor i in cou:\n    #print(i)\n    i=list(i.split(','))\n    if len(i)==1:\n        if i in list(countries.keys()):\n            countries[i]+=1\n        else:\n            countries[i[0]]=1\n    else:\n        for j in i:\n            if j in list(countries.keys()):\n                countries[j]+=1\n            else:\n                countries[j]=1","167646b9":"countries_fin={}\nfor country,no in countries.items():\n    country=country.replace(' ','')\n    if country in list(countries_fin.keys()):\n        countries_fin[country]+=no\n    else:\n        countries_fin[country]=no\n        \ncountries_fin={k: v for k, v in sorted(countries_fin.items(), key=lambda item: item[1], reverse= True)}","5c12cde0":"# Top 10 TV shows creating countries.\n\nplt.figure(figsize=(8,8))\nax = sns.barplot(x=list(countries_fin.keys())[0:10],y=list(countries_fin.values())[0:10])\nax.set_xticklabels(list(countries_fin.keys())[0:10],rotation = 90)","f9e56fa8":"#Analysis of duration of movies\n\nnetflix_movies['duration']=netflix_movies['duration'].str.replace(' min','')\nnetflix_movies['duration']=netflix_movies['duration'].astype(str).astype(int)\nnetflix_movies['duration']","f5523f16":"sns.set(style=\"darkgrid\")\nax=sns.kdeplot(data=netflix_movies['duration'], shade=True)","2a1022a7":"#Analysis of duration of TV shows\n\nfeatures=['title','duration']\ndurations= netflix_shows[features]\n\ndurations['no_of_seasons']=durations['duration'].str.replace(' Season','')\n\n#durations['no_of_seasons']=durations['no_of_seasons'].astype(str).astype(int)\ndurations['no_of_seasons']=durations['no_of_seasons'].str.replace('s','')","79dc2c01":"durations['no_of_seasons']=durations['no_of_seasons'].astype(str).astype(int)","300f659b":"#TV shows with largest number of seasons\nt=['title','no_of_seasons']\ntop=durations[t]\n\ntop=top.sort_values(by='no_of_seasons', ascending=False)","31e1b60c":"top20=top[0:20]\ntop20.plot(kind='bar',x='title',y='no_of_seasons', color='blue')","cc75f334":"#Plot description based Recommender (Content Based Recommendations)\n\nnetflix_dataset['description'].head()","e74658f5":"#Recommedation System(Content Based)\n\n#Import TfIdfVectorizer from scikit-learn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a', etc.\ntfidf = TfidfVectorizer(stop_words='english')\n\n#Replace NaN with an empty string\nnetflix_dataset['description'] = netflix_dataset['description'].fillna('')\n\n#Construct the required TF-IDF matrix by fitting and transforming the data\ntfidf_matrix = tfidf.fit_transform(netflix_dataset['description'])\n\n#Output the shape of tfidf_matrix\ntfidf_matrix.shape","23c53831":"#Import linear_kernel\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","73ee3ee1":"#Construct a reverse map of indices and movie titles\nindices = pd.Series(netflix_dataset.index, index=netflix_dataset['title']).drop_duplicates()","2ac93cfd":"# Function that takes in movie title as input and outputs most similar movies\ndef get_recommendations(title, cosine_sim=cosine_sim):\n    # Get the index of the movie that matches the title\n    idx = indices[title]\n\n    # Get the pairwsie similarity scores of all movies with that movie\n    sim_scores = list(enumerate(cosine_sim[idx]))\n\n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:11]\n\n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n\n    # Return the top 10 most similar movies\n    return netflix_dataset['title'].iloc[movie_indices]","49e7cc39":"get_recommendations('Welcome')","71889f98":"get_recommendations('Avengers: Infinity War')","e5e09079":"get_recommendations('Dil Dhadakne Do')","1415f526":"#Filling null values with empty string.\nfilledna=netflix_dataset.fillna('')\nfilledna.head()","bfebda4d":"#Cleaning the data - making all the words lower case\ndef clean_data(x):\n        return str.lower(x.replace(\" \", \"\"))","674aee22":"#Identifying features on which the model is to be filtered.\nfeatures=['title','director','cast','listed_in','description']\nfilledna=filledna[features]","3c21bb49":"for feature in features:\n    filledna[feature] = filledna[feature].apply(clean_data)\n    \nfilledna.head()","f40a4e1d":"def create_soup(x):\n    return x['title']+ ' ' + x['director'] + ' ' + x['cast'] + ' ' +x['listed_in']+' '+ x['description']\n\nfilledna['soup'] = filledna.apply(create_soup, axis=1)","cc00a0d7":"# Import CountVectorizer and create the count matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english')\ncount_matrix = count.fit_transform(filledna['soup'])","480e21d2":"# Compute the Cosine Similarity matrix based on the count_matrix\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncosine_sim2 = cosine_similarity(count_matrix, count_matrix)","3230a9fe":"# Reset index of our main DataFrame and construct reverse mapping as before\nfilledna=filledna.reset_index()\nindices = pd.Series(filledna.index, index=filledna['title'])","01ddbdf1":"def get_recommendations_new(title, cosine_sim=cosine_sim):\n    title=title.replace(' ','').lower()\n    idx = indices[title]\n\n    # Get the pairwsie similarity scores of all movies with that movie\n    sim_scores = list(enumerate(cosine_sim[idx]))\n\n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:11]\n\n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n\n    # Return the top 10 most similar movies\n    return netflix_dataset['title'].iloc[movie_indices]","631005b6":"get_recommendations_new('Welcome', cosine_sim2)","04d78c75":"get_recommendations_new('Avengers: Infinity War', cosine_sim2)","8732653d":"get_recommendations_new('Dil Dhadakne Do', cosine_sim2)","5b82155a":"## **Analysis of duration of TV shows**","da6d3264":"United States has the most TV Shows contents that were created in netflix.","8850ac1a":"The next steps are the same as what we did with our plot description based recommender. \n\nOne important difference is that we use the CountVectorizer() instead of TF-IDF. This is because we do not want to down-weight the presence of an actor\/director if he or she has acted or directed in relatively more movies. It doesn't make much intuitive sense.","3941a109":"## Importing Libraries","799475d3":"show_id does represents the primary key of the datasets.\nThere are only two types of Netflix content type, where as others are distributed in wide range will need futher analysis with graphs.","8f227f13":"## **In which month, a producer can releases its content? (Month when least amount of content is added)**","ae668368":"Thus, 2018 was the year when most of the content were released.","ac03636a":"## **Top rated 10 movies in Netflix**","25acac44":"A recommender system is a type of information filtering system. By drawing from huge data sets, the system\u2019s algorithm can pinpoint accurate user preferences. Once you know what your users like, you can recommend them new, relevant content. And that\u2019s true for everything from movies and music, to romantic partners. \n \nThe rapid growth of data collection has led to a new era of information. Data is being used to create more efficient systems and this is where Recommendation Systems come into play. Recommendation Systems are a type of information filtering systems as they improve the quality of search results and provides items that are more relevant to the search item or are realted to the search history of the user.\n \nNetflix, YouTube, Tinder, and Amazon are all examples of recommender systems in use. The systems entice users with relevant suggestions based on the choices they make. Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on autoplay, and Facebook uses it to recommend pages to like and people to follow. Moreover, companies like Netflix and Spotify depend highly on the effectiveness of their recommendation engines for their successful businesses.","d8b656e6":"![image.png](attachment:966a71d4-b6dc-4b73-9891-6c0d22ba7c38.png)","0454ef40":"# **How Do Recommender Systems Work?**\n\n![image.png](attachment:0efa2f68-4c44-48f4-8c29-374089d94de2.png)","601773e8":"We see that over 17,900 different words were used to describe the 7787 movies in our dataset.","e48b833b":"### Content based filtering on multiple metrics\n\nContent based filtering on the following factors:\n\n* Title\n* Cast\n* Director\n* Listed in\n* Plot","b28e7439":"## **Analysis of TV Shows in Netflix** ","d9488ea1":"The next step would be to convert the names and keyword instances into lowercase and strip all the spaces between them. This is done so that our vectorizer doesn't count the Johnny of \"Johnny Depp\" and \"Johnny Galecki\" as the same","ed8ab233":"We are going to define a function that takes in a movie title as an input and outputs a list of the 10 most similar movies. Firstly, for this, we need a reverse mapping of movie titles and DataFrame indices. In other words, we need a mechanism to identify the index of a movie in our netflix DataFrame, given its title.","14d145a4":"For any of you who has done even a bit of text processing before knows we need to convert the word vector of each description.\nNow we'll compute Term Frequency-Inverse Document Frequency (TF-IDF) vectors for each description.\n\nThe TF-IDF(Term Frequency-Inverse Document Frequency (TF-IDF) ) score is the frequency of a word occurring in a document, down-weighted by the number of documents in which it occurs. This is done to reduce the importance of words that occur frequently in plot overviews and therefore, their significance in computing the final similarity score.\n\nNow if you are wondering what is **Term Frequency (TF)**, it is the relative frequency of a word in a document and is given as (term instances\/total instances). **Inverse Document Frequency (IDF)** is the relative count of documents containing the term is given as log(number of documents\/documents with term) The overall importance of each word to the documents in which they appear is equal to **TF * IDF**\n\nThis will give you a matrix where each column represents a word in the description vocabulary (all the words that appear in at least one document) and each row represents a movie, as before. This is done to reduce the importance of words that occur frequently in plot descriptions and therefore, their significance in computing the final similarity score.\n\nFortunately, scikit-learn gives you a built-in TfIdfVectorizer class that produces the TF-IDF matrix in a couple of lines.","0c724fed":"It is evident that there are more Movies on Netflix than TV shows.","bf77a681":"# **What is Recommendation System?**","ebc15be3":"If the year 2020 is considered, February and June were the months when comparatively less content was released. Therefore, these months may be a good choice for the success of a new release!","31d30f84":"While our system has done a decent job of finding movies with similar plot descriptions, the quality of recommendations is not that great. \"Welcome\" returns movies with similar description while it is more likely that the people who liked that movie are more inclined to enjoy other Akshay Kumar movies. This is something that cannot be captured by the present system.\n\nTherefore, more metrics are added to the model to improve performance.","ffe3b7eb":"# **Content-Based Recommendation System** \n\n## **Plot description based Recommender (Content Based Recommendations)**","806e0f23":"## **Top countries creating contents**","3e712631":"## Analysis of Movies vs TV Shows","97533e87":"So, a good amount of movies on Netflix are among the duration of 75-120 mins. It is acceptable considering the fact that a fair amount of the audience cannot watch a 3 hour movie in one sitting.","38b2a513":"# Exploratory Data Analysis(EDA)","077306b4":"We are now in a position to create our \"soup\" or a \"bag of words\" for all rows, which is a string that contains all the metadata that we want to feed to our vectorizer (namely actors, director and keywords).\n","a5501fff":"# Let's start with the code.","61e2a140":"## **Year wise analysis**","32a7d60b":"# **What Goes Into a Good Recommendation Engine?**\n\n![image.png](attachment:067c7448-0e54-487f-a897-aee4706cf3ea.png)","000ade9b":"We will compute pairwise similarity scores for all movies based on their plot descriptions and recommend movies based on that similarity score. The plot description is given in the **description feature** of our dataset. Let's take a look at the data...","c7286ee8":"## Loading the Dataset","0f912049":"With this matrix in hand, we can now compute a similarity score. There are several candidates for this; such as the euclidean, the Pearson and the cosine similarity scores. There is no right answer to which score is the best. Different scores work well in different scenarios and it is often a good idea to experiment with different metrics.\n\nWe will be using the cosine similarity to calculate a numeric quantity that denotes the similarity between two movies. We use the cosine similarity score since it is independent of magnitude and is relatively easy and fast to calculate. Mathematically, it is defined as follows:\n\n**similarity = cos(x, y) = x . y \/ ||x|| * ||y||**\n\nSince we have used the TF-IDF vectorizer, calculating the dot product will directly give us the cosine similarity score. \n\nTherefore, we will use **sklearn's linear_kernel()** instead of cosine_similarities() since it is faster..","632a9576":"# **Types of Recommendation Systems:**\n\n![image.png](attachment:3bdaa205-799e-463d-b1d6-3f10f83b6479.png)\n\n1. Content-based Filtering\n2. Collaborative Filtering\n3. Hybrid Recommendation System\n\n# **1. Content-based Filtering**\n\n* Content-based filtering methods are mainly based on the description of an item and a profile of the user\u2019s preferred choices. \n* In content-based filtering, keywords are used to describe the items, whereas a user profile is built to state the type of item this user likes.\n* For example, if a user likes to watch movies such as Mission Impossible, then the recommender system recommends movies of the action genre or movies of Tom Cruise.\n* The critical premise of content-based filtering is that if you like an item, you will also like a similar item. This approach has its roots mainly in information retrieval and information filtering research.\n\n# **2. Collaborative Filtering**\n\n* The collaborative filtering method is based on collecting and analyzing information based on behaviors, activities, or user preferences and predicting what they will like based on the similarity with other users. \n* The prediction is done using various predictive maintenance machine learning techniques. For example, the k-nearest neighbor (k-NN) approach and the Pearson Correlation.\n* For example, if user X likes Tennis, Badminton, and Golf while user Y likes Tennis, Badminton, and Hockey, they have similar interests. So, there is a high probability that X would like Hockey and Y would enjoy Golf. \n* One of the main advantages of the collaborative filtering approach is that it can recommend complex items accurately, such as movies, without requiring an understanding of the item itself as it does not depend on machine analyzable content.\n\n**Two types of collaborative filtering techniques are used:**\n> **1. User-User collaborative filtering:-**\n>* In this, the user vector includes all the items purchased by the user and rating given for each particular product. \n>* The similarity is calculated between users using an n*n matrix in which n is the number of users present. The similarity is calculated using the same cosine similarity formula. The recommending matrix is calculated. \n>* In this, the rating is multiplied by the similarity between the users who have bought this item and the user to which item has to be recommended. \n>* This value is calculated for all items that are new for that user and are sorted in descending order. Then the top items are recommended to that user.\n\n>**2. Item-Item collaborative filtering:-**\n>* In this, rather than considering similar users, similar items are considered. \n>* If the user \u2018A\u2019 loves \u2018Inception\u2019 he may like \u2018The Martian\u2019 as the lead actor is similar. Here, the recommendation matrix is m*m matrix where m is the number of items present.\n\n# **3. Hybrid Recommendation Systems**\n* Hybrid Recommendation engines are essentially the combination of diverse rating and sorting algorithms. For instance, a hybrid recommendation engine could use collaborative filtering and product-based filtering in tandem to recommend a broader range of products to customers with accurate precision.\n* Netflix is an excellent example of a hybrid recommendation system as they make recommendations by:\n>* Comparing the watching and searching habits of users and finding similar users on that platform, thus making use of collaborative filtering\n>* Recommending such shows\/movies which share common characteristics with the ones rated highly by the user. It is how they make use of content-based filtering.\n* Compared to pure collaborative and content-based methods, hybrid methods can provide more accurate recommendations. They can also overcome the common issues in recommendation systems such as cold start and the data paucity troubles.","439f80ed":"**We are now in a good position to define our recommendation function. These are the following steps we'll follow :-**\n\n1. Get the index of the movie given its title.\n2. Get the list of cosine similarity scores for that particular movie with all movies. Convert it into a list of tuples where the first element is its position and the second is the similarity score.\n3. Sort the aforementioned list of tuples based on the similarity scores; that is, the second element.\n4. Get the top 10 elements of this list. Ignore the first element as it refers to self (the movie most similar to a particular movie is the movie itself).\n5. Return the titles corresponding to the indices of the top elements.","d8d323cb":"## **Analysing IMDB ratings to get top rated movies on Netflix**","557124b6":"Performing inner join on the ratings dataset and netflix dataset to get the content that has both ratings on IMDB and are available on Netflix.","412a2237":"Thus, Grey's Anatomy, NCIS and Supernatural are amongst the tv series that have highest number of seasons.","e165d158":"**A) Understanding Relationships:-**\n* Relationships provide recommender systems with tremendous insight, as well as an understanding of customers. There are three main types that occur:\n\n>**1) User-Product Relationship**\n>* The user-product relationship occurs when some users have an affinity or preference towards specific products that they need.\n>\n>**2) Product-Product Relationship**\n>* Product-product relationships occur when items are similar in nature, either by appearance or description.\u00a0\n>\n>**3) User-User Relationship**\n>* User-user relationships occur when some customers have similar taste with respect to a particular product or service.\n\n\n**B) Data & Recommender Systems:-**\n* In addition to relationships, recommender systems utilize the following kinds of data:\n\n>**1) User Behavior Data**\n>* Users behavior data is useful information about the engagement of the user on the product. It can be collected from ratings, clicks and purchase history.\n>\n>**2) User Demographic Data**\n>* User demographic information is related to the user\u2019s personal information such as age, education, income and location.\n>\n>**3) Product Attribute Data**\n>* Product attribute data is information related to the product itself such as genre in case of books, cast in case of movies, cuisine in case of food.\n\n\n**C) How Do We Provide Data For Recommender Systems?**\n* Recommendation engines have three basic steps to make recommendations:\n\n>**1) Data Collection**\n>* Core of a recommendation engine is consumer data. These engines collect implicit and explicit data.\n>\n>**2) Data Storage**\n>* As the amount of data you store increases, you provide better recommendations for your customers.\n>\n>**3) Data Analysis and Recommendation**\n>* Recommendation engines analyze data by filtering it to extract relevant insights to make the final recommendations.","5c3a64f8":"## **Analysis of duration of movies** ","25628a88":"The largest count of movies are made with the 'TV-MA' rating. \"TV-MA\" is a rating assigned by the TV Parental Guidelines to a television program that was designed for mature audiences only.\n\nSecond largest is the 'TV-14' stands for content that may be inappropriate for children younger than 14 years of age.\n\nThird largest is the 'TV-PG' rating. Programs rated TV-PG may contain some material that parents or guardians may find inappropriate for younger children. Programs assigned a TV-PG rating may include infrequent coarse language, some sexual content, some suggestive dialogue, or moderate violence.","207a86a2":"## **Movie Ratings Analysis**","f2c221e1":"We see that our recommender has been successful in capturing more information due to more metadata and has given us (arguably) better recommendations. It is more likely that Marvel or DC comics fans will like the movies of the same production house. Therefore, to our features above we can add production_company . We can also increase the weight of the director, by adding the feature multiple times in the soup."}}