{"cell_type":{"bf83a803":"code","1007f26c":"code","b2b2e025":"code","ce987f00":"code","e93f6a7c":"code","ff720d8c":"code","c0aef27b":"code","d2a31d3d":"code","35d60785":"code","542bca99":"code","6cf7b615":"code","888fe4a4":"markdown","f194094a":"markdown","9e77ca5a":"markdown","6fe71a1a":"markdown","a9412283":"markdown","dae9b11c":"markdown","4700867f":"markdown","f3b00c31":"markdown","80150511":"markdown","d04aa2ae":"markdown","be9d24d7":"markdown","1ab8d004":"markdown","71f6964b":"markdown","7e14a4e3":"markdown","b89cdfdf":"markdown"},"source":{"bf83a803":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import tree\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1007f26c":"dataset = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')\n\ndataset.head(10)","b2b2e025":"dataset.info()","ce987f00":"for column in dataset.columns[1:] :\n    print(\"Unique values for column %s\" % column)\n    g = sns.countplot(data=dataset, x=column)\n    plt.show()\n    if '?' in dataset[column].unique() :\n        percentage = dataset[column].value_counts(normalize=True)['?']\n        print('Column %s has an unknown value with a percentage of %f' % (column, percentage))\n    print()","e93f6a7c":"dataset = dataset.drop(['veil-type', 'veil-color'], axis=1)","ff720d8c":"dataset['class'] = dataset['class'].map({'e' : 1, 'p' : 0})","c0aef27b":"for column in dataset.columns[1:]:\n    g = sns.barplot(data=dataset, x=column, y='class').set_ylabel('Edibility probability')\n    plt.show()","d2a31d3d":"columns = [ 'stalk-color-above-ring', 'odor','ring-type']\nfor column in columns:\n    g = sns.barplot(data=dataset, x=column, y='class').set_ylabel('Edibility probability')\n    plt.show()","35d60785":"X = dataset[columns]\nX = pd.get_dummies(X,drop_first=True)\ny = dataset['class']\n\nprint(\"The shape of our dataset is\", X.shape)","542bca99":"dtc = DecisionTreeClassifier()\n\n\nkfold = KFold(n_splits=10, random_state=10, shuffle=True)\ncv_results = cross_val_score(dtc, X_train, y_train, scoring='accuracy', cv=kfold)\nprint(cv_results.mean())","6cf7b615":"dtc = DecisionTreeClassifier()\ndtc.fit(X,y)\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(dtc, feature_names=X.columns)","888fe4a4":"We are now ready to train our model.","f194094a":"With 10 folds we get 99% accuracy ! Our visulization of the decision tree explains how the model works to determine the class of a mushroom. Explaining a model is very important for discussing with a client, research...etc so this is not a step you should put aside.\n\nIn this notebook, we went from 22 features to 3 (without dummies), while getting a very high accuracy. This is a very great example to show that you don't need that many features to get a model to work !\nOn the other hand, other models would have worked, but a decision tree is simple and explainable so this is enough in our case.\n\nThis is my first notebook on Kaggle, so I hope you liked it !","9e77ca5a":"# 1. Data visualization\n\nFirst, let's load our dataset and see what it looks like.","6fe71a1a":"## a. Class repartition","a9412283":"The current intuition for choosing a model is a decision tree, since the features are very selective. We are going to use cross validation to see if we are not overfitting.","dae9b11c":"# 3. Modelling\n","4700867f":"## b. Edibility probability by category\nOur target class is two-categorical. Let's convert it to a binary value. This will allow us to get the edibility probability depending on each category. Note that if the probability is 1, that means that this value is a discriminant for the class 0, and vice versa. ","f3b00c31":"# Conclusion\n","80150511":"Let's explain how the model decides what class a mushroom belongs to.","d04aa2ae":"We are actually dealing with a dataset full of categorical data. There are no NaN values here.\nWe have to get a closer look at what our categories look like for each column to see where to go next. For that we are using the Python library seaborn to understand the repartition for each category.","be9d24d7":"It seems like all values categorical, but we didn't take a look at all the features with this format. Let's see if it is true.","1ab8d004":"From these plots we can see that the column veil-type has only one value. We can therefore drop it. \nThe same goes for veil-color, which bears very little information. \n\nConcerning null-values, we can see that the column stalk-root has a ratio of 30% rows with value '?' which is basically a nan value. However, even with this percentage, we should keep that information for now because it can be helpful in the future.","71f6964b":"Since we only got categorical features, with only 3 of them, we can easily use dummies to transform them into binary features without getting a very large dataset.","7e14a4e3":"# Mushroom classification","b89cdfdf":"# 2.Feature selection\nWe have many columns. Some of them don't give us much information about whether a mushroom is edible or not. The above visualization shows us that there are features that are decisive in determining the class.\n\nFeatures are going to be selected based on the number of categories with either 0 or 1. \nThe columns stalk-color-above-ring, odor, and ring-type have a lot of discriminants. \n"}}