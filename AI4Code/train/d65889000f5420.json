{"cell_type":{"cd366d17":"code","18dc8507":"code","ce9425ef":"code","3c78076d":"code","ac28bc6d":"code","db12c056":"code","308c63eb":"code","afa2cb0e":"code","52b3f241":"code","c726d06f":"code","2e4ab7bd":"code","3a870094":"code","12111abe":"code","7318693e":"code","0fbcde99":"code","48739c6f":"code","d7891e96":"code","0886420e":"code","08d78ed8":"code","428d6eef":"code","c71de65d":"code","c304d083":"code","39e22b98":"code","3aecc642":"code","b4f6da1e":"code","0342997c":"code","f744e4db":"code","e85d7b81":"code","a95d70fc":"code","8e249cda":"code","0d79459b":"code","c924f5c4":"code","d2a01119":"code","a6f15aca":"code","b0735d0a":"code","a4e099a4":"code","53e276f0":"code","8ac9e35e":"code","5b52f85d":"code","c4548823":"code","c2a060cf":"code","c54e2328":"code","abf4d12a":"code","bff2600d":"code","d5095e70":"code","042a4dc0":"code","0ea3ec93":"code","ddd9cd0f":"code","87489d00":"code","140e9db7":"code","8ea7039a":"code","ff8f9c2b":"code","7d5f3f7a":"markdown","705fee59":"markdown","72192a5c":"markdown","ba78d8fe":"markdown","3699f810":"markdown","50dd7c32":"markdown","4a9f060c":"markdown","c00f43c5":"markdown","b4bead0c":"markdown","614524cf":"markdown","2f13d1c5":"markdown","29c411d0":"markdown","f1dd3cc3":"markdown","7c94cdb4":"markdown","c8fa2348":"markdown","4d0f776c":"markdown","2b3612a1":"markdown","a12d4ee1":"markdown","e52119d5":"markdown","5e0eaae2":"markdown","6aa00e5e":"markdown","69d546ee":"markdown","02185a40":"markdown","0ef883a5":"markdown","c1495640":"markdown","66ef7466":"markdown","d7205df6":"markdown","dbe92918":"markdown","9bae8653":"markdown","fff5a897":"markdown","87139841":"markdown","89014918":"markdown","f977d209":"markdown","a0cc86c3":"markdown","72a5f467":"markdown","3cf1b427":"markdown","4b303de6":"markdown","e9d585d3":"markdown","2436ff08":"markdown","89b550b3":"markdown","0913ad33":"markdown","1d9abf48":"markdown","107455aa":"markdown","4f9147ac":"markdown","4fdd521f":"markdown","8538f331":"markdown","70bde660":"markdown","a0d5fda2":"markdown","9058f0a2":"markdown","505ee09d":"markdown","5b009c50":"markdown","d7b0bcad":"markdown","f46f4aa4":"markdown","52ecb6c2":"markdown","f58fb1fe":"markdown"},"source":{"cd366d17":"import pandas as pd\nimport numpy as np\n# import re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\n\n# from sklearn.cross_validation import KFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\n# from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# for hyperparameter search\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV","18dc8507":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\ncombine = [train_df, test_df]","ce9425ef":"train_df.info()","3c78076d":"train_df.head()","ac28bc6d":"test_df.head()","db12c056":"print(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\nprint(\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)","308c63eb":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","afa2cb0e":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col', \n                                                 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","52b3f241":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","c726d06f":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","2e4ab7bd":"train_df.head()","3a870094":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","12111abe":"guess_ages = np.zeros((2,3))\nguess_ages","7318693e":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[(dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","0fbcde99":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","48739c6f":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()","d7891e96":"train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","0886420e":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","08d78ed8":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","428d6eef":"# train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n# test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntrain_df = train_df.drop(['Parch', 'SibSp'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp'], axis=1)\n\ncombine = [train_df, test_df]\n\ntrain_df.head()","c71de65d":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","c304d083":"freq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port","39e22b98":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","3aecc642":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","b4f6da1e":"# test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df['Fare'].fillna(test_df['Fare'].dropna().mode()[0], inplace=True)\ntest_df.head()","0342997c":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","f744e4db":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    ","e85d7b81":"train_df.head(10)","a95d70fc":"test_df.head(10)","8e249cda":"# Store our passenger ID for easy access\nPassengerId = test_df['PassengerId']","0d79459b":"# Some useful parameters which will come in handy later on\ntrain = train_df\ntest = test_df\n\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\n# kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\nkf = KFold(n_splits=NFOLDS, random_state=SEED)\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, has_random_state=1, params=None):\n        if(has_random_state==1):\n            params['random_state'] = seed\n        print(params)\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def predict_proba(self, x):\n        return self.clf.predict_proba(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)","c924f5c4":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,)) #record train set prediction result for each test_index\n    oof_test = np.zeros((ntest,)) #record test set prediction result for final prediction\n    oof_test_skf = np.empty((NFOLDS, ntest)) #record prediciton from each clf for all the x_test set\n\n#     for i, (train_index, test_index) in enumerate(kf):\n    for i, (train_index, test_index) in enumerate(kf.split(train)):    \n#         print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n#         oof_train[test_index] = clf.predict(x_te) #each clf has a validation result, so totally 5 meta features\n#         oof_test_skf[i, :] = clf.predict(x_test) #test prediction for different clf from different training set under CV\n#         print(clf.predict_proba(x_te)[:,1].shape)\n        oof_train[test_index] = clf.predict_proba(x_te)[:,1] #each clf has a validation result, so totally 5 meta features\n        oof_test_skf[i, :] = clf.predict_proba(x_test)[:,1] #test prediction for different clf from different training set under CV\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","d2a01119":"# Put in our parameters for said classifiers\n# Random Forest parameters\n# rf_params = {\n#     'n_jobs': -1,\n#     'n_estimators': 100,\n#      'warm_start': True, \n#     'max_depth': 6,\n#     'min_samples_leaf': 2,\n#     'max_features' : 'sqrt',\n#     'verbose': 0\n# }\n\n# RandomForest parameters we got from titanic-data-science-solution-jimmy-modified notebook\nrf_params = {'bootstrap': True, \n             'criterion': 'entropy',\n             'max_depth': 5, \n             'max_features': 'sqrt', \n             'min_samples_leaf': 1, \n             'min_samples_split': 5,\n             'min_weight_fraction_leaf': 0.0, \n             'n_estimators': 100, \n             'n_jobs': -1,\n             'verbose': 0,\n             'warm_start': False\n             }\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025,\n    'probability': True\n}\n\n# Logistic Regression\nlogreg_params = {    \n}\n\n# KNN\nknn_params ={\n    'n_neighbors': 3\n}\n","a6f15aca":"# Create 5 objects that represent our 5 models\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)\nlog = SklearnHelper(clf=LogisticRegression, seed=SEED, params=logreg_params)\nknn = SklearnHelper(clf=KNeighborsClassifier, seed=SEED, has_random_state=0, params=knn_params)","b0735d0a":"# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\ndrop_elements = ['PassengerId']\ntest  = test.drop(drop_elements, axis = 1)\nx_test = test.values # Creats an array of the test data","a4e099a4":"# Feature Scaling for non-tree based classifiers\n## We will be using standardscaler to transform\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\n## transforming \"train_x\"\nx_train = sc.fit_transform(x_train)\n\n## transforming \"The testset\"\nx_test = sc.transform(x_test)","53e276f0":"%%time\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\nlog_oof_train, log_oof_test = get_oof(log,x_train, y_train, x_test) # Logistic Regression\nknn_oof_train, knn_oof_test = get_oof(knn,x_train, y_train, x_test) # KNN\nprint(\"Training is complete\")","8ac9e35e":"rf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)","5b52f85d":"rf_features = [0.13796186, 0.28025103, 0.0400734,  0.06107588, 0.02110676, 0.26240436,\n 0.09555471, 0.01240719, 0.08916479]\net_features = [0.16660931, 0.4105351,  0.02365337, 0.06920249, 0.03212467, 0.18273988,\n 0.05908173, 0.02362692, 0.03242655]\nada_features = [0.058, 0.214, 0.064, 0.044, 0.016, 0.386, 0.06,  0.008, 0.15 ]\ngb_features = [0.15048707, 0.01816345, 0.02917081, 0.05736517, 0.02881883, 0.54593532,\n 0.12270411, 0.00780863, 0.03954661]","c4548823":"cols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })","c2a060cf":"feature_dataframe","c54e2328":"# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Extra Trees  feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Extra Trees  feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Extra Trees Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['AdaBoost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['AdaBoost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'AdaBoost Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Gradient Boost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Gradient Boost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Gradient Boosting Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","abf4d12a":"# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe","bff2600d":"y = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n            y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='bar-direct-labels')","d5095e70":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n     'GradientBoost': gb_oof_train.ravel(),\n     'SVC': svc_oof_train.ravel(),\n     'LogisticRegression': log_oof_train.ravel(),   \n     'KNN': knn_oof_train.ravel()                                   \n    })\nbase_predictions_train.head(10)","042a4dc0":"data = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')","0ea3ec93":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train, log_oof_train, knn_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test, log_oof_test, knn_oof_test), axis=1)","ddd9cd0f":"#Grid Search\ngbm_param_grid = {\n    'learning_rate': [0.05, 0.08, 0.1, 0.2],\n    'n_estimators': [100, 200, 500],\n    'max_depth': [3, 4, 5],\n    'gamma': [0.8, 0.9] \n}\ngbm = xgb.XGBClassifier()\ngrid_gbm = GridSearchCV(estimator=gbm,\n                       param_grid=gbm_param_grid,\n                       scoring='accuracy',\n                       cv=3,\n                       verbose=1,\n                       n_jobs=4)","87489d00":"%%time\ngrid_gbm.fit(x_train, y_train)\nprint(\"Best parameters found: \",grid_gbm.best_params_)","140e9db7":"gbm = xgb.XGBClassifier(\n learning_rate = 0.05,\n n_estimators= 100,\n max_depth= 4,\n gamma=0.9,                        \n n_jobs=4\n ).fit(x_train, y_train)\npredictions = gbm.predict(x_test)","8ea7039a":"acc_gbm = round(gbm.score(x_train, y_train) * 100, 2)\nacc_gbm","ff8f9c2b":"# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\n","7d5f3f7a":"Let us replace Age with ordinals based on these bands.","705fee59":"### Create new feature combining existing features\n\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.","72192a5c":"We can now create FareBand.","ba78d8fe":"**Feature importances generated from the different classifiers**\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such","3699f810":"Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.","50dd7c32":"Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.","4a9f060c":"## References\n\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\n\n- [introduction-to-ensembling-stacking-in-python](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python)\n- [data-science-framework-to-achieve-99-accuracy](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy)\n- [a-statistical-analysis-ml-workflow-of-titanic](https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic)\n- [Stacking\u200a\u2014\u200aA Super Learning Technique](https:\/\/medium.com\/@gurucharan_33981\/stacking-a-super-learning-technique-dbed06b1156d)","c00f43c5":"**Steps for Further Improvement**\n\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n\nSome additional steps that may be taken to improve one's score could be:\n\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.","b4bead0c":"**First-level output as new features**\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.","614524cf":"And the test dataset.","2f13d1c5":"**Correlation Heatmap of the Second Level Training set**","29c411d0":"**Producing the Submission file**\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:","f1dd3cc3":"## Some key points about model selection and construction \u6a21\u578b\u9009\u62e9\u4e0e\u6784\u5efa\u7684\u51e0\u4e2a\u5173\u952e\u95ee\u9898","7c94cdb4":"**Parameters**\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses all text while a value of 3 outputs the tree learning process at every iteration.\n\n Please check out the full description via the official Sklearn website. There you will find that there are a whole host of other useful parameters that you can play around with. ","c8fa2348":"**Creating NumPy arrays out of our train and test sets**\n\nGreat. Having prepared our first layer base models as such, we can now ready the training and test test data for input into our classifiers by generating NumPy arrays out of their original dataframes as follows:","4d0f776c":"# We are ready to model again!!!","2b3612a1":"Furthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.","a12d4ee1":"We can create another feature called IsAlone.","e52119d5":"## Model Stacking \u6a21\u578b\u5806\u53e0\nEnsemble Learning is a learning mechanism that uses multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms. Many of the popular modern machine learning algorithms are actually ensembles.In short ,this is a technique that take a collection of **weak learners** and **form a single, strong learner**.For example Random Forests(Bagging) and Gradient Boosting(Boosting)are both ensemble learners.Stacking is a technique that belongs to these family of learning.\n\nStacking-a Meta Modeling Technique is introduced by Wolpert in the year 1992.In Stacking there are two types of learners called **Base Learners** and a **Meta Learner**.Base Learners and Meta Learners are the normal machine learning algorithms like Random Forests, SVM, Perceptron etc. Base Learners try to fit the normal data sets where as Meta learner fit on the predictions of the base Learner.\n![](https:\/\/github.com\/icwangjimmy\/machine_learning_and_python_in_finance\/raw\/master\/pic\/stack1.png)\n\n### Stacking Technique involves the following Steps:-\n\n- 1.Split the training data into 2 disjoint sets\n- 2.Train several Base Learners on the first part\n- 3.Test the Base Learners on the second part and make predictions\n- 4.Using the predictions from (3) as inputs,the correct responses from the output, train the higher level learner or meta level Learner\n\nMeta Learner is kind of trying to find the optimal combination of base learners. Let us take an example of classification problem where we are trying to classify 4 classes and as a part of traditional paradigm we are testing various models and we find out that Logistic Regression is making better predictions on class 1 data and SVM is making better on class 2 and class 4 and KNN is doing better on class 3 and class 2.This performance is predicted because in general no model is perfect and has its own advantages and disadvantages. So, if we train a model on the predictions of these model can we get better results? This is the idea on which this entire concept is built upon. So if we train a Random Forest Classifier on these predictions of LR, SVM, KNN we get better results.\n![](https:\/\/github.com\/icwangjimmy\/machine_learning_and_python_in_finance\/raw\/master\/pic\/stack2.gif)\n\nLets Try to give shape to this technique:-\n\n## Set up the ensemble.\n- Specify a list of L base algorithms (with a specific set of model parameters).\n- Specify a meta learning algorithm.\n\n## Train the ensemble.\n- Train each of the L base algorithms on the training set.\n- Perform k-fold cross-validation on each of these learners and collect the cross-validated predicted values from each of the L algorithms.\n- Train the meta learning algorithm on the level-one data. The \u201censemble model\u201d consists of the L base learning models and the meta learning model, which can then be used to generate predictions on a test set.\n\n## Predict on new data.\n- To generate ensemble predictions, first generate predictions from the base learners.\n- Feed those predictions into the meta learner to generate the ensemble prediction.","5e0eaae2":"**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:","6aa00e5e":"### Converting categorical feature to numeric\n\nWe can now convert the EmbarkedFill feature by creating a new numeric Port feature.","69d546ee":"### Quick completing and converting a numeric feature\n\nWe can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We do this in a single line of code.\n\nNote that we are not creating an intermediate new feature or doing any further analysis for correlation to guess missing feature as we are replacing only a single value. The completion goal achieves desired requirement for model algorithm to operate on non-null values.\n\nWe may also want round off the fare to two decimals as it represents currency.","02185a40":"<a id=\"step2\"><\/a>\n# \u91cd\u590d\u4e00\u904d\u524d\u4e00\u4e2anotebook\u7684\u6570\u636e\u64cd\u4f5c\uff0c\u4e3a\u65b0\u7684model\u505a\u51c6\u5907\uff01\n# Acquire training and testing data\n","0ef883a5":"### Out-of-Fold Predictions\n\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\n\nBasically, you need to cut training data into two parts (train and validation), and train the base model with train data, then predict with validation data, so it can pass the validation data prediction as meta features to the meta model for second-level model training.\n![](https:\/\/github.com\/icwangjimmy\/machine_learning_and_python_in_finance\/raw\/master\/pic\/kfold.png)","c1495640":"**Output of the First level Predictions** \n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.","66ef7466":"### Helpers via Python Classes \u9762\u5411\u5bf9\u8c61\u7f16\u7a0b\u4e00\u77a5\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code\/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n![](https:\/\/github.com\/icwangjimmy\/machine_learning_and_python_in_finance\/raw\/master\/pic\/python_class.png)\n\nIn the section of code below, we essentially write a class SklearnHelper that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn classifiers. Therefore this cuts out redundancy as won't need to write the same methods five times if we wanted to invoke five different classifiers.","d7205df6":"Just a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting","dbe92918":"### Completing a categorical feature\n\nEmbarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance.","9bae8653":"### Completing a numerical continuous feature\n\nNow we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\n1. A simple way is to generate random numbers between mean and [standard deviation](https:\/\/en.wikipedia.org\/wiki\/Standard_deviation).\n\n**2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using [median](https:\/\/en.wikipedia.org\/wiki\/Median) values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...**\n\n3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2.","fff5a897":"<a id=\"step4\"><\/a>\n# Wrangle, prepare, cleanse the data\n- Wrangle data (Feature Engineering)","87139841":"# Second-Level Predictions from the First-level Output","89014918":"## Change Log\n- [20190131] fix a bug on family size calculation","f977d209":"## K-fold cross-validation K\u6298\u4ea4\u53c9\u786e\u8ba4\u6cd5\nIn k-fold cross-validation, we randomly split the training dataset into k folds without replacement, where k-1 folds are used for the model training and one fold is used for testing. This procedure is repeated k times so that we obtain k models and performance estimates.\n\nWe then calculate the average performance of the models based on the different, independent folds to obtain a performance estimate that is less sensitive to the subpartitioning of the training data compared to the holdout method. Typically, we use k-fold cross-validation for model tuning, that is, finding the optimal hyperparameter values that yield a satisfying generalization performance. Once we have found satisfactory hyperparameter values, we can retrain the model on the complete training set and obtain a final performance estimate using the independent test set.\n\n**Since k-fold cross-validation is a resampling technique without replacement, the advantage of this approach is that each sample point will be part of a training and test dataset exactly once, which yields a lower-variance estimate of the model performance than the holdout method.**\n![](https:\/\/github.com\/icwangjimmy\/machine_learning_and_python_in_finance\/raw\/master\/pic\/kfold1.png)","a0cc86c3":"# Generating our Base First-Level Models \n\nSo now let us prepare seven learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine\n 6. Logistic Regression\n 7. K-Nearest Neighbors","72a5f467":"We can now remove the AgeBand feature.","3cf1b427":"## The holdout method \u4fdd\u6301\u6cd5\nA classic and popular approach for estimating the generalization performance of machine learning models is holdout cross-validation. Using the holdout method, we **split our initial dataset into a separate training and test dataset\u2014the former is used for model training, and the latter is used to estimate its performance.** However, in typical machine learning applications, we are also interested in tuning and comparing different parameter settings to further improve the performance for making predictions on unseen data. This process is called model selection, where the term model selection refers to a given classification problem for which we want to select the optimal values of tuning parameters (also called hyperparameters). However, if we reuse the same test dataset over and over again during model selection, it will become part of our training data and thus the model will be more likely to overfit. Despite this issue, many people still use the test set for model selection, which is not a good machine learning practice.\n\nA better way of using the holdout method for model selection is to **separate the data into three parts: a training set, a validation set, and a test set.** The training set is used to fit the different models, and the performance on the validation set is then used for the model selection. The advantage of having a test set that the model hasn't seen before during the training and model selection steps is that we can obtain a less biased estimate of its ability to generalize to new data. The following figure illustrates the concept of holdout cross-validation where we use a validation set to repeatedly evaluate the performance of the model after training using different parameter values. Once we are satisfied with the tuning of parameter values, we estimate the models' generalization error on the test dataset:\n![](https:\/\/github.com\/icwangjimmy\/machine_learning_and_python_in_finance\/raw\/master\/pic\/holdout.png)\nA disadvantage of the holdout method is that the performance estimate is sensitive to how we partition the training set into the training and validation subsets; the estimate will vary for different samples of the data. In the next subsection, we will take a look at a more robust technique for performance estimation, k-fold cross-validation, where we repeat the holdout method k times on k subsets of the training data.","4b303de6":"Create a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.","e9d585d3":"### Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https:\/\/xgboost.readthedocs.io\/en\/latest\/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:","2436ff08":"**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:","89b550b3":"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.","0913ad33":"## Prepare Python Packages","1d9abf48":"We can convert the categorical titles to ordinal.","107455aa":"# Titanic Data Science Solution(2) \n## Introduction\nThis notebook is the second part of [titanic-data-science-solution-jimmy-modified](https:\/\/www.kaggle.com\/icwangjimmy\/titanic-data-science-solution-jimmy-modified). In this notebook, I reuse other experts' kernels and build a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level (meta) to predict the output from the earlier first-level predictions.\n\n","4f9147ac":"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.","4fdd521f":"We can also create an artificial feature combining Pclass and Age.","8538f331":"## Overfitting Discussion \u8fc7\u5ea6\u62df\u5408\u8ba8\u8bba\n![](https:\/\/github.com\/icwangjimmy\/machine_learning_and_python_in_finance\/raw\/master\/pic\/overfitting.png)\nAs you see in the chart above. Underfitting is when the model fails to capture important aspects of the data and therefore introduces more bias and performs poorly. On the other hand, Overfitting is when the model performs too well on the training data but does poorly in the validation set or test sets. This situation is also known as having less bias but more variation and perform poorly as well. Ideally we want to configure a model that performs well not only in the training data, but also in the test data. This is where bias-variance tradeoff comes in. When we have a model that overfits meaning more less biased and more possible chance of variance, we introduce some bais in exchance of having much less variance. ","70bde660":"Convert the Fare feature to ordinal values based on the FareBand.","a0d5fda2":"## N-Level Stacking \u53ef\u7ee7\u7eed\u6269\u5c55\u5230\u591a\u5c42\u5806\u53e0\nThe concept of Stacking can be extended to many Levels.\n![](https:\/\/github.com\/icwangjimmy\/machine_learning_and_python_in_finance\/raw\/master\/pic\/stack3.png)\n\n","9058f0a2":"### Converting a categorical feature\n\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0.","505ee09d":"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.","5b009c50":"\n## Ensembling & Stacking models\nFinally after repeating the feature engineering we have done before and preparing all the basic concepts of model selection and construction, we arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble! \u73b0\u5728\u7528\u4e0a\u9762\u63d0\u5230\u7684\u6982\u5ff5\u6765\u505a\u4e2a\u5806\u53e0\u6a21\u578b\u5427\uff01","d7b0bcad":"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.","f46f4aa4":"Let us create Age bands and determine correlations with Survived.","52ecb6c2":"Bear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n\ndef init : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\nThe rest of the code are simply methods of the class which simply call the corresponding methods already existing within the sklearn classifiers. Essentially, we have created a wrapper class to extend the various Sklearn classifiers so that this should help us reduce having to write the same code over and over when we implement multiple learners to our stacker.","f58fb1fe":"Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset."}}