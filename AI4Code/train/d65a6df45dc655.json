{"cell_type":{"861fd4db":"code","d8301e71":"code","15faa7cd":"code","cece001a":"code","63c0326d":"code","db11e188":"code","6260717f":"code","0e6c6f7e":"code","4641ec3c":"code","3c987609":"code","53d411d6":"code","b100e0a8":"code","a288bdd0":"code","45153e2e":"code","3470417d":"code","9805b91e":"code","b1bbcb65":"code","714d2399":"code","458e2b4f":"code","5dff49f4":"markdown","211ad267":"markdown","261514c0":"markdown","39751db9":"markdown","07329bc7":"markdown","4ff941e3":"markdown","e0820e75":"markdown","e487c962":"markdown","8ac43276":"markdown","2bb4a352":"markdown","ed6009ff":"markdown","18a89184":"markdown","1b8d3853":"markdown","62a74bc8":"markdown","c317aed1":"markdown","4a9b918d":"markdown","a4a4554e":"markdown","9af90aa3":"markdown","43cf017a":"markdown"},"source":{"861fd4db":"!pip install -q kagglerecipes","d8301e71":"import os\nimport ast\nimport mpire\nimport wandb\nimport timeit\nimport imageio\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# kagglerecipes based imports\nfrom kagglerecipes.preprocess import VoxelData\nfrom kagglerecipes.utils import (\n    get_patient_id,\n    get_all_BraTS21_dicom_meta,\n    get_patient_BraTS21ID_path,\n    get_image_plane,\n    KAGGLE_BRAINTUMOR_META_COLS\n)\nfrom kagglerecipes.wandb_utils import log_to_artifacts","15faa7cd":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=secret_value_0)\n    anony=None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","cece001a":"# This is an optional config used to show that you can pass dictionary to log your hyperparameters and othe required info. \nCONFIG = {'competition': 'rsna-miccai-brain', '_wandb_kernel': 'kr_voxel', 'use_wandb': True}","63c0326d":"DATA_PATH = Path('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/')\nTRAIN_PATH = DATA_PATH \/ 'train\/'\nSCAN_TYPES = ['FLAIR', 'T1w', 'T1wCE', 'T2w']","db11e188":"# Load as dataframe.\ntrain_df = pd.read_csv(DATA_PATH \/ 'train_labels.csv')\n# Exluding three cases\n# Refer: https:\/\/www.kaggle.com\/c\/rsna-miccai-brain-tumor-radiogenomic-classification\/discussion\/262046\ntrain_df = train_df[train_df.BraTS21ID != 109]\ntrain_df = train_df[train_df.BraTS21ID != 123]\ntrain_df = train_df[train_df.BraTS21ID != 709]\n\nprint(f'Number of BraTS21ID: {len(train_df)}')\n\n# Save the file as W&B artifacts. \nrun = wandb.init(project='rsna-miccai-brain-tumor', \n                 config=CONFIG,  # CONFIG is optional here\n                 job_type='log-dataset-labels',\n                 anonymous=anony) \n\nlog_to_artifacts(path_to_data=DATA_PATH\/'train_labels.csv', # \ud83d\udccc\n                artifact_name='raw_rsna_miccai_labels', \n                artifact_type='labels',\n                log='file')\nwandb.finish()","6260717f":"# add path to the df\ntrain_df['path'] = train_df.apply(lambda row: get_patient_BraTS21ID_path(row, TRAIN_PATH), axis=1) # \ud83d\udccc\n\ntrain_df.head(2)","0e6c6f7e":"# train_df = train_df.sample(100).reset_index(drop=True)","4641ec3c":"mpire.cpu_count()","3c987609":"#GPU count and name\n!nvidia-smi -L\n\n#cpu model name\n!lscpu |grep 'Model name'\n\n#no.of sockets i.e available slots for physical processors\n!lscpu | grep 'Socket(s):'\n\n#no.of cores each processor is having\n!lscpu | grep 'Core(s) per socket'\n\n#no.of threads each core is having\n!lscpu | grep 'Thread(s) per core'","53d411d6":"# Number of parallel processes to use for data processing\nn_jobs = 10\n\n# # Get DICOM metadata\ntrain_meta_df = get_all_BraTS21_dicom_meta(train_df, KAGGLE_BRAINTUMOR_META_COLS, \n                                           SCAN_TYPES, n_jobs, True) # \ud83d\udccc\n\n# Get orienation metadata\ntrain_meta_df['Orientation'] = train_meta_df.apply(get_image_plane, axis=1) # \ud83d\udccc\n\n# Save metadata\ntrain_meta_df.to_csv('train_meta_df.csv', index=False)\n\nprint(f'train_meta_df has {len(train_meta_df)} rows')\ntrain_meta_df.head()","b100e0a8":"# Save the file as W&B artifacts. \nrun = wandb.init(project='rsna-miccai-brain-tumor',\n                 config=CONFIG,\n                 job_type='create-dicom-metadata',\n                 anonymous=anony)\n\nlog_to_artifacts(path_to_data='train_meta_df.csv', # we saved the file in the last cell. # \ud83d\udccc\n                artifact_name='metadata',  # let's name it metadata. \n                artifact_type='meta-dataset', # let the type be meta-dataset\n                log='file') \n\nwandb.finish()","a288bdd0":"# Find the reference MRI sequence manually\ndftr2 = train_meta_df[(train_meta_df.Rows == '256') & \n                      (train_meta_df.Columns == '256') &\n                      (train_meta_df.Orientation == \"axial\") &\n                      (train_meta_df.SeriesDescription == \"T1w\")].groupby(['PatientID', 'Orientation', 'SeriesDescription']).size().reset_index(name='count') \n\ndftr2.loc[(dftr2['count'] < 50) & (dftr2['count'] >15)].reset_index(drop = True)","45153e2e":"REFERENCE_ID = '00147'","3470417d":" def save_voxel_data(\n     scan_types:list,\n     save_path, \n     reference_path,\n     patient_path,  # Path to the patient folder\n     BraTS21ID:int  # BraTS21ID\n ):\n    \"Returns a re-sampled image based on the reference path\"\n    \n    connect_voxel = VoxelData(reference_path) # \ud83d\udccc\n    \n    # Create folder to save to\n    save_dir = save_path \/ get_patient_id(BraTS21ID)\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Resample the dicom files and save the output images as numpy files\n    for scan in scan_types:\n        scan_path = os.path.join(patient_path, scan)\n        voxel_data = connect_voxel.get_voxel_data(scan_path) # \ud83d\udccc\n        np.save(save_dir \/ f'{scan}.npy', voxel_data)\n\n        \ndef save_all_voxel_data(\n    reference_path:str,  # Path to the dicom file to use as a template for resampling\n    save_path:str,  # Path to save voxel data to\n    df,  # Dataframe with path to patient folder and BraTS21ID\n    scan_types:list=['FLAIR', 'T1w', 'T1wCE', 'T2w'],  # The subfolders in the patient data to loop through\n):    \n    \"Resamples the dicom data based on reference template dicom and then saves that voxel data to save_path\"\n    \n    patient_path = df.path.values\n    BraTS21ID_ls = df.BraTS21ID.values\n    results = []\n    for i in tqdm(range(len(df))):\n        res = save_voxel_data(scan_types, save_path, reference_path, patient_path[i], BraTS21ID_ls[i])\n        results.append(res)","9805b91e":"REFERENCE_PATH = os.path.join(TRAIN_PATH \/ REFERENCE_ID, \"T1w\") # Note: The PatientID and the MRI Scan type selected as reference.\nSAVE_PATH = Path('..\/tmp\/') \n\nos.makedirs(SAVE_PATH, exist_ok=True)\n\n# Save all data\nsave_all_voxel_data(REFERENCE_PATH, SAVE_PATH, train_df)","b1bbcb65":"# Number of scans to viz\nNUM_SAMPLES = 32\n\n# Initialize a W&B run to log images\nrun = wandb.init(project='rsna-miccai-brain-tumor',\n                 config=CONFIG,\n                 name='viz-dataset-tables',\n                 anonymous=anony) # \ud83d\udccc W&B Code 1\n\ndata_at = wandb.Table(columns=['patient_id', 'target', 'FLAIR', 'T1w', 'T1wCE', 'T2w']) # \ud83d\udccc W&B Code 2\n\nfor i in tqdm(range(len(train_df))):\n    os.makedirs('tables-gif', exist_ok=True)\n    \n    row = train_df.loc[i]\n    patient_id = get_patient_id(row.BraTS21ID)\n    \n    for j, key in enumerate(SCAN_TYPES):\n        _frames = np.load(f'{SAVE_PATH}\/{patient_id}\/{key}.npy')\n        imageio.mimsave(f'tables-gif\/out_{patient_id}_{j}.gif', (_frames*255).astype('uint8'))\n    \n    data_at.add_data(patient_id,                                            \n                     row.MGMT_value,\n                     wandb.Image(f'tables-gif\/out_{patient_id}_0.gif'),\n                     wandb.Image(f'tables-gif\/out_{patient_id}_1.gif'),\n                     wandb.Image(f'tables-gif\/out_{patient_id}_2.gif'),\n                     wandb.Image(f'tables-gif\/out_{patient_id}_3.gif')) # \ud83d\udccc W&B Code 3\n    \n    if i == NUM_SAMPLES:\n        break\n\nwandb.log({'MRI Sequencing Dataset': data_at}) # \ud83d\udccc W&B Code 4\nwandb.finish() # \ud83d\udccc W&B Code 5","714d2399":"!zip -rq voxel.zip ..\/tmp","458e2b4f":"print('Done!')","5dff49f4":"## Save Metadata as W&B Artifacts\n\nAdditionally you can save the metadata as W&B Artifacts. Check out the [official documentation page](https:\/\/docs.wandb.ai\/guides\/artifacts) to learn more. ","211ad267":"### Visualize entire dataset interactively\n\n### [Check out the Tables](https:\/\/wandb.ai\/ayush-thakur\/brain-tumor-viz\/runs\/kb9nwx3a) \n\n![img](https:\/\/i.imgur.com\/4cGorA3.gif)","261514c0":"\u2b50 For demonstration purposes we are using only 100 patientIds sampled randomly. Comment the cell below to run on the entire dataset. Running on the entire dataset will take around ~90 minutes while using Kaggle kernel. ","39751db9":"# Create Dataset","07329bc7":"# Data Preprocessing\n\n### Numer of Cores to use - Try Check Your System Specs\nWe can use multiprocessing to handle some of our data processing. To get the best out of your multiprocessing, lets check how many cores are available to use","4ff941e3":"Add the path to the patient folders to `train_labels.csv`\n\n\ud83d\udccc Use `get_patient_BraTS21ID_path` to easily get the correct path. Check out the [documentation](https:\/\/ayulockin.github.io\/kagglerecipes\/utils.html#get_patient_BraTS21ID_path).","e0820e75":"# Create Voxel Dataset with KaggleRecipes\nHey fellow Kagglers, I have been working on a GitHub library called **[KaggleRecipes](https:\/\/github.com\/ayulockin\/kagglerecipes)** along with [Morgan](https:\/\/www.kaggle.com\/morganmcg), my colleague from Weights and Biases, these past few days. The idea for this repository is to package necessary utilities and provide baseline training and inference for the competitions, we are participating in. The repository is also instrumented with Weights and Biases and comes with convenient utilities to visualise your datasets and do efficient, effective model evaluation with W&B.\n\nThis is an **early release** and we are working to add new functionalities and features to abstract away a lot of boiler plate code for your Kaggle competitions. You can find the full documentation **[here](https:\/\/ayulockin.github.io\/kagglerecipes\/)**. \n\n# Created Dataset:\n\nThe following are the dataset created using KaggleRecipes using a single BraTS21ID as reference sequence. \n\n* [RSNA MICCAI Voxel BraTS21ID 143](https:\/\/www.kaggle.com\/ayuraj\/rsna-miccai-voxel-brats21id-143) - Uses BraTS21ID 143 as the reference sequence. \n\n\n## Multiprocessing with `mpire`\nMPIRE is a multiprocessing library recently released that abstracts away a lot of the multi-processing **and** and claims to be faster than `multiprocessing.Pool` and `concurrent.futures.ProcessPoolExecutor` and is on par with ray. This [blog post](https:\/\/towardsdatascience.com\/mpire-for-python-multiprocessing-is-really-easy-d2ae7999a3e9) introduces it and you can find the [mpire github repo here](https:\/\/github.com\/Slimmer-AI\/mpire\/issues\/11). \n\nWe've used it in `kagglerecipes` as its really enjoy able to work with (Morgan's favorite features is the simple flag to turn on a tqdm progress bar). We used it to cut down the dicom metadata extraction step in this notebook **from 44minutes to 12minutes**. \n\n## Credits\nThis kernel is possible because of awesome works done by these fellow Kagglers:\n* [Connecting voxel spaces](https:\/\/www.kaggle.com\/boojum\/connecting-voxel-spaces) by [Michael Beregov](https:\/\/www.kaggle.com\/boojum)\n* [Normalized Voxels: Align Planes and Crop](https:\/\/www.kaggle.com\/ren4yu\/normalized-voxels-align-planes-and-crop) by [yu4u](https:\/\/www.kaggle.com\/ren4yu)\n* [\ud83e\udde0 DICOM to 2D Resized Axial PNGs 256x256 [x36] \ud83e\udde0](https:\/\/www.kaggle.com\/smoschou55\/dicom-to-2d-resized-axial-pngs-256x256-x36) by [Sofia Moschou](https:\/\/www.kaggle.com\/smoschou55)\n\n## Previous Work\n\ud83e\uddd0 For a deeper EDA looking at individual mri slices see the [kernel here](https:\/\/www.kaggle.com\/ayuraj\/brain-tumor-eda-and-interactive-viz-with-w-b). <br>\n\ud83e\uddd0 You can use this voxel manipulated dataset with this [[Train] Brain Tumor as Video Classification + W&B](https:\/\/www.kaggle.com\/ayuraj\/train-brain-tumor-as-video-classification-w-b) kernel. \n","e487c962":"\u2b50Note: If you are using the entire dataset and you have decided on an ID that you want to use as reference, use Run and Save all. ","8ac43276":"# Visualize the Created Dataset as W&B Tables\n\nThis allows you to interactively check if the created dataset is correct. \n\n\ud83e\uddd0 For a deeper EDA looking at individual images see this **[kernel here](https:\/\/www.kaggle.com\/ayuraj\/brain-tumor-eda-and-interactive-viz-with-w-b)**\n\n\u26a0\ufe0f See this discussion post for more info on MGMT and the objective of this competition: **[[Self-Note] \"Brain tumor classification\" is misleading!](https:\/\/www.kaggle.com\/c\/rsna-miccai-brain-tumor-radiogenomic-classification\/discussion\/264861)**","2bb4a352":"In case you're more curious what specs the machine you're using has, [Tim Yee](https:\/\/www.kaggle.com\/teeyee314) has a nice kernel [here](https:\/\/www.kaggle.com\/teeyee314\/cpu-kernel-specs) that gives you system info.","ed6009ff":"![](https:\/\/i.imgur.com\/5xUFyIP.png)","18a89184":"Load the `train_labels.csv` dataset. \n\nLoad the dataset as W&B artifacts. This will enable data version control since there can be multiple possible dataset that you can create for this competition. Data version control goes a long way to ensure reproducible results, not mess up with a lot going on, share with team and most importantly have a bird eye view of everything. Maybe this [discussion post would be worth a read](https:\/\/www.kaggle.com\/c\/hpa-single-cell-image-classification\/discussion\/229586). \n\n\ud83d\udccc You can easily log any file or directory using our `log_to_artifacts` function. Check out the [documentation](https:\/\/ayulockin.github.io\/kagglerecipes\/wandb_utils.html#log_to_artifacts).","1b8d3853":"# Prepare dataset","62a74bc8":"The cell below have functions responsible to save the voxel manipulated dataset. \n\n\ud83d\udccc The `VoxelData` class contains the main logic for the dataset creation by manipulating the MRI sequences in the Voxel space. It takes `reference_path` which is the reference modality. We will be selecting PatientID `00102` with 23 slices. Check out the [documentation](https:\/\/ayulockin.github.io\/kagglerecipes\/preprocess.html#VoxelData).\n\n\ud83d\udccc The `VoxelData` has a method called`get_voxel_data` which takes in the path to the MRI sequence and returns the manipulated data. ","c317aed1":"# Imports and Setup","4a9b918d":"### [Check out the saved Artifacts page $\\rightarrow$](https:\/\/wandb.ai\/ayush-thakur\/brain-tumor-voxel-dataset\/artifacts\/meta-dataset\/metadata\/3debb76d8026375a39aa\/files)","a4a4554e":"You would want to save the created dataset as Kaggle Dataset which is not covered in this Kernel. But if you are pushing out the dataset as public Kaggle dataset if would be great if you can use the title something like: `RSNA-MICCAI Voxel Dataset BraTS21Id <PatientID>`\n\n**If you like the effort consider upvoting and using the library :)**","9af90aa3":"The code cell below performs the following steps:\n\n\ud83d\udccc It uses multiprocessing to quickly extract the associated metadata using our `get_all_BraTS21_dicom_meta` function. Check out the [documentation](https:\/\/ayulockin.github.io\/kagglerecipes\/utils.html#get_all_BraTS21_dicom_meta). <br>\n\ud83d\udccc It then extracts the orientation of each DICOM slice using `get_image_place` function. Check out the [documentation](https:\/\/ayulockin.github.io\/kagglerecipes\/utils.html#get_image_plane).\n\n","43cf017a":"In the cell below you can query the metadata table to find a reference image as the set criteria. \n\ncriteria:\n* Height (`Rows`) and Width (`Columns`)\n* Orientation (`Orientation`)\n* MRI Scan type (`SeriesDescription`)\n* Number of slices (`count`) per `PatientID`\n"}}