{"cell_type":{"70cc18fd":"code","403cf49f":"code","bbef064f":"code","30c446d4":"code","60dd1367":"code","6c41591c":"code","190c6145":"code","79ed07d4":"code","7b4b2a9e":"code","07a145e7":"code","cd41d5a7":"code","5a7c09bc":"code","d4285e8f":"code","d3924a79":"code","fdc6fb5a":"code","fc21ea59":"code","c24bb8e8":"code","e7b9041d":"code","a4e57733":"code","b4575605":"code","49d8c10e":"code","6d2f07f4":"code","5fc8c6fe":"code","db6171d9":"code","98651b46":"code","f0e752d9":"code","a8ba9174":"code","8b2ce51e":"code","6ebb32f0":"code","d53142ea":"code","046d7d66":"markdown"},"source":{"70cc18fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","403cf49f":"os.environ[\"WANDB_API_KEY\"] = \"0\"","bbef064f":"from transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","30c446d4":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","60dd1367":"df = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\ndf.info()","6c41591c":"df.premise.values[1]\n","190c6145":"df.hypothesis.values[1]","79ed07d4":"Languages=pd.DataFrame()\nLanguages['Type']=df.language.value_counts().index\nLanguages['Count']=df.language.value_counts().values\n","7b4b2a9e":"import seaborn as sns\n","07a145e7":"fig, ax = plt.subplots(figsize=(10,10))\nsns.barplot(Languages['Count'] ,Languages['Type'] , palette='Spectral' , ax = ax)","cd41d5a7":"sntmnt=pd.DataFrame()\nsntmnt['type']=df.label.value_counts().index\nsntmnt['count']=df.label.value_counts().values","5a7c09bc":"class_name = ['corresponding to entailment', 'neutral','contradiction']\nsns.countplot(df.label)\nplt.xlabel('Different Sentiment')\nplt.ylabel('Number of each Sentiments')","d4285e8f":"english_df = df[df['language'] == 'English']\nenglish_df","d3924a79":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","fdc6fb5a":"print(tokenizer.tokenize(\"This dataset is quiet complicated\"))","fc21ea59":"def encode_sentence(s):\n   tokens = list(tokenizer.tokenize(s))\n   tokens.append('[SEP]')\n   return tokenizer.convert_tokens_to_ids(tokens)","c24bb8e8":"encode_sentence(\"This dataset is quiet complicated\")\n","e7b9041d":"tokenizer.sep_token , tokenizer.sep_token_id","a4e57733":"tokenizer.cls_token , tokenizer.cls_token_id","b4575605":"def bert_encode(hypotheses, premises, tokenizer):\n    \n  num_examples = len(hypotheses)\n  \n  sentence1 = tf.ragged.constant([\n      encode_sentence(s)\n      for s in np.array(hypotheses)])\n  sentence2 = tf.ragged.constant([\n      encode_sentence(s)\n       for s in np.array(premises)])\n\n  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n\n  input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n  type_cls = tf.zeros_like(cls)\n  type_s1 = tf.zeros_like(sentence1)\n  type_s2 = tf.ones_like(sentence2)\n  input_type_ids = tf.concat(\n      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n  inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n  return inputs","49d8c10e":"train_input = bert_encode(df.premise.values, df.hypothesis.values, tokenizer)","6d2f07f4":"max_len = 50\n\ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","5fc8c6fe":"with strategy.scope():\n    model = build_model()\n    model.summary()","db6171d9":"model.fit(train_input, df.label.values, epochs = 2, verbose = 1, batch_size = 64, validation_split = 0.2)","98651b46":"test_data = pd.read_csv('..\/input\/contradictory-my-dear-watson\/test.csv')\ntest_data.head()\ntest_data.info()","f0e752d9":"test_input = bert_encode(test_data.premise.values, test_data.hypothesis.values, tokenizer)","a8ba9174":"output = [np.argmax(i) for i in model.predict(test_input)]","8b2ce51e":"submission = test_data.id.copy().to_frame()\nsubmission['prediction'] = output","6ebb32f0":"print(submission.head())","d53142ea":"submission.to_csv(\"submission.csv\", index = False)\nsubmission.prediction.value_counts()","046d7d66":"**#Task 1** :-\nVisualization of Data \n"}}