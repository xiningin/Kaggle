{"cell_type":{"decf4598":"code","0cc5e01d":"code","865c62f6":"code","a2a6655c":"code","b93fee97":"code","ac4be0c5":"code","6c2455d5":"code","afcd8147":"code","76a4ac7c":"code","fc94605c":"code","05c53a98":"code","ac704352":"code","bd3f80cc":"code","bf359a0d":"code","12af954c":"code","27a99b30":"code","c3b128f4":"code","8a5f0fb1":"code","ebe13156":"code","31c7019e":"code","cd74cbda":"code","2b338b2a":"code","6e90191e":"code","91da1604":"code","19c386c5":"code","12b99d02":"code","f6377d78":"code","8a4a3878":"code","e46fb1b2":"code","5649b311":"code","93ff0396":"code","c4ba65f6":"code","c988262b":"code","eb35c68d":"code","d5dad0d2":"code","337bc425":"code","d654a892":"code","9e99456a":"code","355afcc4":"markdown","398a64f1":"markdown","8a05e8ad":"markdown","433f2813":"markdown","c2ae0451":"markdown","73842925":"markdown","b16992e2":"markdown","8049b572":"markdown","1c4f2089":"markdown","cac50932":"markdown","862c90a6":"markdown","6ee22a52":"markdown","3a671c68":"markdown","e67392cf":"markdown","9e5c8369":"markdown"},"source":{"decf4598":"#import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport warnings\n\n%matplotlib inline\npd.set_option(\"display.max_rows\", None,\"display.max_columns\", None)\nwarnings.simplefilter(action='ignore')\nplt.style.use('seaborn')","0cc5e01d":"#load dataset\ndf_main = pd.read_csv('..\/input\/vehicle-dataset-from-cardekho\/car data.csv')","865c62f6":"df_main.head()","a2a6655c":"df_main.shape","b93fee97":"df_main.info()","ac4be0c5":"#numerical stats\ndf_main.describe()","6c2455d5":"#missing values\ndf_main.isna().sum()","afcd8147":"df_main['Age'] = 2020 - df_main['Year']\ndf_main.drop('Year',axis=1,inplace = True)","76a4ac7c":"df_main.rename(columns = {'Selling_Price':'Selling_Price(lacs)','Present_Price':'Present_Price(lacs)','Owner':'Past_Owners'},inplace = True)","fc94605c":"df_main.columns","05c53a98":"cat_cols = ['Fuel_Type','Seller_Type','Transmission','Past_Owners']\ni=0\nwhile i < 4:\n    fig = plt.figure(figsize=[10,4])\n    #ax1 = fig.add_subplot(121)\n    #ax2 = fig.add_subplot(122)\n    \n    #ax1.title.set_text(cat_cols[i])\n    plt.subplot(1,2,1)\n    sns.countplot(x=cat_cols[i], data=df_main)\n    i += 1\n    \n    #ax2.title.set_text(cat_cols[i])\n    plt.subplot(1,2,2)\n    sns.countplot(x=cat_cols[i], data=df_main)\n    i += 1\n    \n    plt.show()","ac704352":"num_cols = ['Selling_Price(lacs)','Present_Price(lacs)','Kms_Driven','Age']\ni=0\nwhile i < 4:\n    fig = plt.figure(figsize=[13,3])\n    #ax1 = fig.add_subplot(121)\n    #ax2 = fig.add_subplot(122)\n    \n    #ax1.title.set_text(num_cols[i])\n    plt.subplot(1,2,1)\n    sns.boxplot(x=num_cols[i], data=df_main)\n    i += 1\n    \n    #ax2.title.set_text(num_cols[i])\n    plt.subplot(1,2,2)\n    sns.boxplot(x=num_cols[i], data=df_main)\n    i += 1\n    \n    plt.show()","bd3f80cc":"df_main[df_main['Present_Price(lacs)'] > df_main['Present_Price(lacs)'].quantile(0.99)]","bf359a0d":"df_main[df_main['Selling_Price(lacs)'] > df_main['Selling_Price(lacs)'].quantile(0.99)]","12af954c":"df_main[df_main['Kms_Driven'] > df_main['Kms_Driven'].quantile(0.99)]","27a99b30":"sns.heatmap(df_main.corr(), annot=True, cmap=\"RdBu\")\nplt.show()","c3b128f4":"df_main.corr()['Selling_Price(lacs)']","8a5f0fb1":"df_main.pivot_table(values='Selling_Price(lacs)', index = 'Seller_Type', columns= 'Fuel_Type')","ebe13156":"df_main.pivot_table(values='Selling_Price(lacs)', index = 'Seller_Type', columns= 'Transmission')","31c7019e":"df_main.drop(labels='Car_Name',axis= 1, inplace = True)","cd74cbda":"df_main.head()","2b338b2a":"df_main = pd.get_dummies(data = df_main,drop_first=True) ","6e90191e":"df_main.head()","91da1604":"# Separating target variable and its features\ny = df_main['Selling_Price(lacs)']\nX = df_main.drop('Selling_Price(lacs)',axis=1)","19c386c5":"from sklearn.model_selection import train_test_split","12b99d02":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nprint(\"x train: \",X_train.shape)\nprint(\"x test: \",X_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","f6377d78":"from sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score","8a4a3878":"CV = []\nR2_train = []\nR2_test = []\n\ndef car_pred_model(model,model_name):\n    # Training model\n    model.fit(X_train,y_train)\n            \n    # R2 score of train set\n    y_pred_train = model.predict(X_train)\n    R2_train_model = r2_score(y_train,y_pred_train)\n    R2_train.append(round(R2_train_model,2))\n    \n    # R2 score of test set\n    y_pred_test = model.predict(X_test)\n    R2_test_model = r2_score(y_test,y_pred_test)\n    R2_test.append(round(R2_test_model,2))\n    \n    # R2 mean of train set using Cross validation\n    cross_val = cross_val_score(model ,X_train ,y_train ,cv=5)\n    cv_mean = cross_val.mean()\n    CV.append(round(cv_mean,2))\n    \n    # Printing results\n    print(\"Train R2-score :\",round(R2_train_model,2))\n    print(\"Test R2-score :\",round(R2_test_model,2))\n    print(\"Train CV scores :\",cross_val)\n    print(\"Train CV mean :\",round(cv_mean,2))\n    \n    # Plotting Graphs \n    # Residual Plot of train data\n    fig, ax = plt.subplots(1,2,figsize = (10,4))\n    ax[0].set_title('Residual Plot of Train samples')\n    sns.distplot((y_train-y_pred_train),hist = False,ax = ax[0])\n    ax[0].set_xlabel('y_train - y_pred_train')\n    \n    # Y_test vs Y_train scatter plot\n    ax[1].set_title('y_test vs y_pred_test')\n    ax[1].scatter(x = y_test, y = y_pred_test)\n    ax[1].set_xlabel('y_test')\n    ax[1].set_ylabel('y_pred_test')\n    \n    plt.show()","e46fb1b2":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\ncar_pred_model(lr,\"Linear_regressor.pkl\")","5649b311":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Creating Ridge model object\nrg = Ridge()\n# range of alpha \nalpha = np.logspace(-3,3,num=14)\n\n# Creating RandomizedSearchCV to find the best estimator of hyperparameter\nrg_rs = RandomizedSearchCV(estimator = rg, param_distributions = dict(alpha=alpha))\n\ncar_pred_model(rg_rs,\"ridge.pkl\")","93ff0396":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import RandomizedSearchCV\n\nls = Lasso()\nalpha = np.logspace(-3,3,num=14) # range for alpha\n\nls_rs = RandomizedSearchCV(estimator = ls, param_distributions = dict(alpha=alpha))","c4ba65f6":"car_pred_model(ls_rs,\"lasso.pkl\")","c988262b":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrf = RandomForestRegressor()\n\n# Number of trees in Random forest\nn_estimators=list(range(500,1000,100))\n# Maximum number of levels in a tree\nmax_depth=list(range(4,9,4))\n# Minimum number of samples required to split an internal node\nmin_samples_split=list(range(4,9,2))\n# Minimum number of samples required to be at a leaf node.\nmin_samples_leaf=[1,2,5,7]\n# Number of fearures to be considered at each split\nmax_features=['auto','sqrt']\n\n# Hyperparameters dict\nparam_grid = {\"n_estimators\":n_estimators,\n              \"max_depth\":max_depth,\n              \"min_samples_split\":min_samples_split,\n              \"min_samples_leaf\":min_samples_leaf,\n              \"max_features\":max_features}\n\nrf_rs = RandomizedSearchCV(estimator = rf, param_distributions = param_grid)","eb35c68d":"car_pred_model(rf_rs,'random_forest.pkl')","d5dad0d2":"print(rf_rs.best_estimator_)","337bc425":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\ngb = GradientBoostingRegressor()\n\n# Rate at which correcting is being made\nlearning_rate = [0.001, 0.01, 0.1, 0.2]\n# Number of trees in Gradient boosting\nn_estimators=list(range(500,1000,100))\n# Maximum number of levels in a tree\nmax_depth=list(range(4,9,4))\n# Minimum number of samples required to split an internal node\nmin_samples_split=list(range(4,9,2))\n# Minimum number of samples required to be at a leaf node.\nmin_samples_leaf=[1,2,5,7]\n# Number of fearures to be considered at each split\nmax_features=['auto','sqrt']\n\n# Hyperparameters dict\nparam_grid = {\"learning_rate\":learning_rate,\n              \"n_estimators\":n_estimators,\n              \"max_depth\":max_depth,\n              \"min_samples_split\":min_samples_split,\n              \"min_samples_leaf\":min_samples_leaf,\n              \"max_features\":max_features}\n\ngb_rs = RandomizedSearchCV(estimator = gb, param_distributions = param_grid)","d654a892":"car_pred_model(gb_rs,\"gradient_boosting.pkl\")","9e99456a":"Technique = [\"LinearRegression\",\"Ridge\",\"Lasso\",\"RandomForestRegressor\",\"GradientBoostingRegressor\"]\nresults=pd.DataFrame({'Model': Technique,'R Squared(Train)': R2_train,'R Squared(Test)': R2_test,'CV score mean(Train)': CV})\ndisplay(results)","355afcc4":"#### Random Forest","398a64f1":"### Exploratory Data Analysis (EDA)","8a05e8ad":"#### Lasso","433f2813":"#### Standard Linear Regression or Ordinary Least Squares","c2ae0451":"#### Bivariate\/Multi-Variate Analysis","73842925":"### Data Preparation","b16992e2":"#### Creating Dummies for Categorical Features","8049b572":"#### Ridge","1c4f2089":"### Model Creation\/Evaluation","cac50932":"### Data Preprocessing","862c90a6":"#### Gradient Boosting","6ee22a52":"### Train-Test Split","3a671c68":"#### Univariate Analysis","e67392cf":"#### Applying regression models\n1. Linear Regression \n2. Ridge Regression\n3. Lasso Regression\n4. Random Forest Regression\n5. Gradient Boosting regression","9e5c8369":"### Reading and Understanding the Dataset"}}