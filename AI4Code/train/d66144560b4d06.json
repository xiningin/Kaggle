{"cell_type":{"6090d927":"code","d67d3510":"markdown"},"source":{"6090d927":"import numpy as np\nimport pandas as pd\nimport scipy\nfrom sklearn.model_selection import train_test_split\n\n\nclass NaiveBayes:\n    def __init__(self, path, ratio, seed):\n        self.path = path\n        self.__RATIO = ratio\n        self.__SEED = seed\n\n    def import_data(self):\n        ds = pd.read_csv(self.path, skiprows=1, header=None)\n        self.x = ds.iloc[:, :-1]\n        self.y = ds.iloc[:, -1]\n        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x, self.y,\n                                                                                random_state=SEED, \n                                                                                train_size=RATIO)\n        self.y_hat = np.zeros(self.y_test.shape[0], dtype='int')\n\n    @staticmethod\n    def log_gaussian(x, mu, variance):\n        return np.lib.scimath.log(scipy.stats.norm(mu, variance).pdf(x))\n\n    @staticmethod\n    def train_model(x, y):\n        indices = [\n            np.repeat(np.unique(y), len(np.unique(y))),\n            np.array([\"Mean\", \"STD\"]*len(np.unique(y)))\n        ]\n        df = pd.DataFrame(np.empty((len(np.unique(y))*2, x.shape[1])), index=indices, columns=list(range(x.shape[1])))\n        for val in np.unique(y):\n            means = x[y == val].mean()\n            std = x[y == val].std()\n            df.loc[(val, 'Mean'), :] = means\n            df.loc[(val, 'STD'), :] = std\n        print(\"Data Summary for each Feature by Class:\")\n        print(df)\n        return df\n\n    def predict(self):\n        self.stats = self.train_model(self.x_train, self.y_train)\n        classes = np.log(self.y_train.value_counts()\/len(self.y_train))\n        predictions = pd.Series(np.zeros(len(np.unique(self.y_train))),\n                                index=np.unique(self.y_train))\n        for i in range(len(self.x_test)):\n            for val in np.unique(self.y_train):\n                temp = 0\n                for j in range(self.x_train.shape[1]):\n                    temp += self.log_gaussian(self.x_test.iloc[i, j],\n                                              self.stats.loc[(val, 'Mean'), j],\n                                              self.stats.loc[(val, 'STD'), j])\n                predictions[val] = classes[val] + temp\n            self.y_hat[i] = np.argmax(predictions)\n        accuracy = np.mean([1 if self.y_test.iloc[i] == self.y_hat[i] else 0 for i in range(self.y_test.shape[0])])\n        print(\"\\nNaive Bayes Accuracy:\", accuracy)\n        return accuracy\n\n\nif __name__ == '__main__':\n    RATIO = 0.75 # Train\/Test split ratio\n    SEED = 123 # Random Seed\n    PATH = '\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv' # File path\n    classifier = NaiveBayes(PATH, RATIO, SEED)\n    classifier.import_data()\n    classifier.predict()","d67d3510":"# Implementing Naive-Bayes Classifier by Using Pima-Diabetes Dataset\nIn this notebook I'll implement the Naive-Bayes algorithm for supervised learning classification tasks by using the famous Pima-Diabetes dataset.\n\nFor more info regarding the algorithm: https:\/\/towardsdatascience.com\/naive-bayes-classifier-explained-50f9723571ed"}}