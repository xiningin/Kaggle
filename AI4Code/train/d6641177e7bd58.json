{"cell_type":{"7e3d92e1":"code","09e2674a":"code","a9a17a68":"code","b6fc01e3":"code","3f1a04fa":"code","baa5c140":"code","be24c17c":"code","a168d051":"code","c4de4812":"code","f0637e8a":"markdown","199975e3":"markdown","54ce2f44":"markdown","f558575d":"markdown"},"source":{"7e3d92e1":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","09e2674a":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom transformers import TFBertModel, BertTokenizer\nfrom sklearn.model_selection import train_test_split\n","a9a17a68":"data = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\nlabels = data.is_sarcastic.values\nsentences = data.headline.values\ndata.head()","b6fc01e3":"PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME,do_lower_case = True)\n\ndef encoder(sentences):\n  ids = []\n  for sentence in sentences:\n    encoding = tokenizer.encode_plus(\n    sentence,\n    max_length=16,\n    truncation = True,\n    add_special_tokens=True,\n    return_token_type_ids=False,\n    pad_to_max_length=True,\n    return_attention_mask=False)\n    ids.append(encoding['input_ids'])\n  return ids\n\n#Train test split\ntrain_sents,test_sents, train_labels, test_labels  = train_test_split(sentences,labels,test_size=0.15)\n\ntrain_ids = encoder(train_sents)\ntest_ids = encoder(test_sents) ","3f1a04fa":"train_ids = tf.convert_to_tensor(train_ids)\ntest_ids = tf.convert_to_tensor(test_ids)\ntest_labels = tf.convert_to_tensor(test_labels)\ntrain_labels = tf.convert_to_tensor(train_labels)","baa5c140":"bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')\ninput_word_ids = tf.keras.Input(shape=(16,), dtype=tf.int32, name=\"input_word_ids\")  \nembedding = bert_encoder([input_word_ids])\ndense = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(embedding[0])\ndense = tf.keras.layers.Dense(128, activation='relu')(dense)\ndense = tf.keras.layers.Dropout(0.2)(dense)   \noutput = tf.keras.layers.Dense(1, activation='sigmoid')(dense)    \n\nmodel = tf.keras.Model(inputs=[input_word_ids], outputs=output)  \n","be24c17c":"model.compile(tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","a168d051":"history = model.fit(x = train_ids, y = train_labels, epochs = 3, verbose = 1, batch_size = 32, validation_data = (test_ids, test_labels))","c4de4812":"def plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","f0637e8a":"**Tokenizing all the sentences using pre-trained BERT model from Tranformers**","199975e3":"Converting arrays to tensors","54ce2f44":"Building Model from transformers using pre-trained bert-base-uncased (12-layer, 768-hidden, 12-heads, 110M parameters.)","f558575d":"# Sarcasm Detection using the Pre-Trained BERT model from Transformers "}}