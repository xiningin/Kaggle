{"cell_type":{"a184e621":"code","dfe9afc4":"code","78859b1b":"code","3920412f":"code","e9ef0508":"code","bb4f8b07":"code","3f7c3247":"code","9aac8759":"code","89d3e98e":"markdown","c2b5a2d9":"markdown","e063d342":"markdown","5f5ca56a":"markdown","7d0a149e":"markdown","f8c1cabb":"markdown","b7636d27":"markdown","87bb3cd8":"markdown","e71292d3":"markdown","5b740428":"markdown","5f960b9b":"markdown","587faaaf":"markdown","3befb3ff":"markdown","6da0f896":"markdown","ee1703ad":"markdown"},"source":{"a184e621":"import numpy as np\nimport pandas as pd \ndata = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nprint(data.isna().any())","dfe9afc4":"#removing categorical features for ease\ncols = ['Name', 'Sex', 'SibSp',\n       'Ticket']\ndata = data.drop(columns = cols)\n","78859b1b":"data1 = data.copy()\ndata1 = data1.dropna()\nprint('original data length ',len(data))\nprint('new data  length ',len(data1))","3920412f":"na_columns = data.columns[data.isnull().any()]\ndata2 = data.drop(columns = na_columns)\nprint(data2.columns)","e9ef0508":"### Let us take column age and see \n#Mean\ndata3 = data.copy()\nprint(data3)\ndata3['Age'] = data3.Age.fillna(data3.Age.mean())\nprint(\"-----After imputation------\")\nprint(data3)","bb4f8b07":"#Similarly for median, mode or  a constant value\n#Median\ndata3 = data.copy()\ndata3['Age'] = data3.Age.fillna(data3.Age.median())\n#Mode\ndata3 = data.copy()\ndata3['Age'] = data3.Age.fillna(data3.Age.mode())\n#Constant\ndata3 = data.copy()\ndata3['Age'] = data3.Age.fillna(20)\n","3f7c3247":"#### Doing this for Age column\ndata4 = data.copy()\n#drop columns having nan\ndata4 = data4.drop(columns = ['Cabin','Embarked'])\n\nx_train = data4.dropna()\ny_train = x_train['Age']\nx_train = x_train.drop(columns = ['Age'])\n\nx_predict = data[data['Age'].isnull()]\ntemp = x_predict[na_columns]\nx_predict = x_predict.drop(columns = na_columns)\n\n#Lets fit model\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 100 decision trees\nrf = RandomForestRegressor(n_estimators = 100, random_state = 22)\n# Train the model on training data\nrf.fit(x_train, pd.to_numeric(y_train))\n\n##predict\ny_predict = rf.predict(x_predict)\nprint(y_predict)","9aac8759":"x_temp = pd.concat([x_train,x_predict])\ny_temp = np.concatenate((y_train, y_predict))\ndata4 = x_temp.copy()\ndata4['Age'] = y_temp\nprint(data4)","89d3e98e":"> **2.\tDelete the columns having NA: **Another approach is to remove the columns that have NA. But if we do this with all columns, you may probably lose all your data. You can do this in some columns. But it is suggested that you go to this step only if your column has more than 80% of missing values. It also depends on the importance of the column in your use case.\n\n","c2b5a2d9":"This was a very introductory kernel\/post for handling missing values. Let me know if I should discuss them in more detail with more details.\n\nThank you.","e063d342":"***What to do if you don\u2019t handle them?***\n\n*Note: I do not suggest to go with this.*\n\nThere are many algorithms that have mechanisms to deal with missing values in the data. These algorithms are like Naive Bayes where you can choose what happens to the missing value. I would repeat again, handle your missing values beforehand and don\u2019t rely on the algorithm to handle it for you.\n","5f5ca56a":"***Methods of handling them.***\n\nLet us now discuss how to handle the missing values.\n","7d0a149e":"We are taking Titanic dataset. Let us check which column has missing values","f8c1cabb":"***Why handle missing values?***\n\nThe first problem is that many algorithms don\u2019t work if you input NAs (missing values) in them.\nOther than that missing values lead to bias in the data. It can sometimes even lead to bad results. That is why it is very important to handle them appropriately. The best thing is to avoid having missing data at the time it is being created. But in real scenario, you are not the one who collects data. So you must work on what has been collected and handle it on your end.\n\n\n","b7636d27":"As you can see, the size of data decreased drastically if we drop nas from our data. This step has lead to huge loss of data. Hence, we should do this only if nas are very less in number.","87bb3cd8":"**Handling Missing values \u2013 the first task when you start coding **","e71292d3":"You can see row 888 has NaN for Age column. We replaced it by mean","5b740428":"As we can see there are three columns having missing valures - Age, Cabin & Embarked.\n\nLet us see different methods.","5f960b9b":"**3.\tMissing value Imputation:** This is a widely used and very effective method when it comes to handling missing values. There are lots of options for imputation mainly \u2013 mean, median, mode or a constant value. No option is the best. It varies with dataset and use case. You can chose the value to be imputed based on your use case.","587faaaf":"***What is missing value?***\n\nDue to some reasons or sometimes maybe randomly, values do not get captured for all rows of certain columns in a dataset. These are termed as missing values. This is very common in real time datasets.","3befb3ff":"1.\t**Delete the rows having NA:** When you don\u2019t want to spend time thinking on what to do, the immediate solution that comes is mind is to just delete and remove the rows having NA. Neither will there be missing values nor will there be any problem. But this cannot be done every time. Removing rows lead to loss of data and we do not want that to happen. It is suggested that you go to this step only if your column has less than 5% of missing values. \n\n","6da0f896":"**4.\tAdvanced missing value imputation: ** For imputing missing values another option is to fill it using a predictive model. You can use simple regression or classification algorithms to perform this task.\n\n","ee1703ad":"Let us re-construct our data back"}}