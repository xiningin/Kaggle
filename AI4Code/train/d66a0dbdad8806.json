{"cell_type":{"43478c08":"code","a9395fac":"code","f9badc54":"code","d7dbcf83":"code","a92da488":"code","a8c14c9a":"code","6d204085":"code","672aa736":"code","4720c28a":"code","dbb2a740":"code","c4faa696":"code","eb0bda77":"code","f70026b8":"code","500e37e1":"code","be857dd4":"code","2589b384":"markdown","de9aa143":"markdown","242a1376":"markdown","17f9b52a":"markdown","d4e8a7d2":"markdown","b8a505b9":"markdown","170f03cf":"markdown"},"source":{"43478c08":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nimport nltk\nfrom sklearn.model_selection import train_test_split \nimport tensorflow as tf","a9395fac":"nltk.download(\"stopwords\")","f9badc54":"df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ndf.head()\n\n","d7dbcf83":"# Exclude stop words from text\ndef text_preprocessing(df, col): \n    stemmer = PorterStemmer()\n    stop_words = set(stopwords.words('english'))\n    for i in range(0, len(df)):\n        row = df.iloc[i, col]\n        words = row.lower().split()\n        text = ''\n    \n    for word in words:\n          if not word in stop_words:\n            word = stemmer.stem(word)\n            text = text + word + ' '\n\n    df.iloc[i, col] = text \n      \n    return df\n\n\ndf = text_preprocessing(df,3)","a92da488":"fig, ax = plt.subplots(tight_layout=True)\n\n# N is the count in each bin, bins is the lower-limit of the bin\nN, bins, patches = ax.hist(df.target, bins=50)\n\n# Color code by height\nfracs = df.target \/ df.target.max()\n\n# Normalize\nnorm = colors.Normalize(fracs.min(), fracs.max())\n\n# Set color in a loop\nfor thisfrac, thispatch in zip(fracs, patches):\n    color = plt.cm.viridis(norm(thisfrac))\n    thispatch.set_facecolor(color)","a8c14c9a":"#select input and output \nY = df.target.values.reshape(df.shape[0],1) #select the label (correct output) \ndf = df.drop('target', 1) #remove the label from input \nX = df.iloc[:,3].values \n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n                                         test_size=0.33, random_state=42)\n\n\ntraining_sentences = []\ntraining_labels = []\n\ntesting_sentences = []\ntesting_labels = []\n\n# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\nfor i in range(0,len(X_train)):\n    training_sentences.append(X_train[i])\n    training_labels.append(Y_train[i])\n    training_labels = [float(i) for i in training_labels]\n\nfor i in range(0,len(X_test)):\n    testing_sentences.append(X_test[i])\n    testing_labels.append(Y_test[i])\n    testing_labels = [float(i) for i in testing_labels]\n\ntraining_labels_final = np.array(training_labels)\ntesting_labels_final = np.array(testing_labels)\n","6d204085":"embedding_dim = 32\ntrunc_type='post'\noov_tok = \"<OOV>\"\n\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(training_sentences)\nmax_length = max(len(l) for l in sequences)\nvocab_size = len(word_index) + 1\n\npadded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)","672aa736":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score","4720c28a":"def create_model():\n    \n    model = keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n        tf.keras.layers.LSTM(embedding_dim),\n        layers.Dense(units=512, kernel_initializer='normal', activation='relu'),\n        layers.Dense(units=256, kernel_initializer='normal', activation='relu'),\n        layers.Dense(units=128, kernel_initializer='normal', activation='relu'),\n        # Regularization\n        layers.Dropout(0.4),\n        # the linear output layer \n        layers.Dense(units=1, kernel_initializer='normal', activation='linear'),\n    ])\n    \n    model.compile(optimizer = 'adam', loss='mean_squared_error')\n    \n    return model\n\nmodel = create_model()\nmodel.summary()","dbb2a740":"'''\n# Create a KerasClassifier\nmodel_KR = KerasRegressor(build_fn = create_model)\n\n# define the parameters to try out\nparams = {'batch_size':[16, 32, 128], 'epochs':[10, 20, 50]}\n\n# define RandomizedSearchCV\nrandom_searcher = RandomizedSearchCV(model_KR, param_distributions = params, cv = KFold(5))\n\n# fit the model\nrandom_searcher.fit(padded, training_labels_final)\n\n# take a look a the results\nprint(random_searcher.best_params_)\nprint(random_searcher.best_score_)\n\n# get the mean accuracy\nprint('The mean accuracy:', kfolds.mean())\n'''","c4faa696":"# Create a KerasClassifier with best parameters\nmodel_KR = KerasRegressor(build_fn = create_model, batch_size = 16, epochs = 10)\n\nmodel_KR.fit(padded, training_labels_final)","eb0bda77":"# Test set score calculation\nscore = model_KR.score(testing_padded, testing_labels_final)","f70026b8":"test = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\ntest = df = text_preprocessing(test,3)\n\nX_test = test.iloc[:,3].values \n\ntesting_sentences = []\n\nfor i in range(0,len(X_test)):\n    testing_sentences.append(X_test[i])\n\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)","500e37e1":"#model_KR.fit(padded, training_labels_final)\nprediction = model_KR.predict(testing_padded)\nprediction_list = [i for i in prediction]","be857dd4":"# create submission file\nsubmission = pd.DataFrame({'id' : test['id'], 'target' : prediction_list})\nsubmission.to_csv('submission.csv', index=False)\n","2589b384":"## Training the model with the best hyperparamets","de9aa143":"# Text preprocessing","242a1376":"# Exploring the Data\n\n### Distribution of the values for `target` (histogram)","17f9b52a":"# Data Preparation for Deep Learning","d4e8a7d2":"## Finding the best hyperparamets for the model","b8a505b9":"# **CommonLit Readability Prize *(A Kaggle competition)***\n### Rate the complexity of literary passages for grades 3-12 classroom use \n\n\n\n**Goal:**  To identify the appropriate reading level of a passage of text and to improve readability rating methods\n\n### Kaggle assingmet\nIn this competition, you\u2019ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains.If successful, you'll aid administrators, teachers, and students. Literacy curriculum developers and teachers who choose passages will be able to quickly and accurately evaluate works for their classrooms. Plus, these formulas will become more accessible for all. Perhaps most importantly, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills.","170f03cf":"# Deep Learning Model \n* `keras.Sequential` model with Embedding layer and LSTM layer "}}