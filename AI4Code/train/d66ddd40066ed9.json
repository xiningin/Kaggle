{"cell_type":{"085def49":"code","d9a20db5":"code","3e397a83":"code","06037cfe":"code","7a72d716":"code","e07e2a70":"code","d2536416":"code","ce5a0bfc":"code","d6a8d7dd":"code","5679e08d":"code","780ae9c0":"code","e45584cf":"code","a6d670e4":"code","97653c9e":"code","ad698cd5":"code","5beb3606":"code","eefe46aa":"code","35de5f64":"code","975ab54b":"code","664b1697":"code","f2389284":"code","347da9c5":"code","62709a58":"code","058e3bd5":"code","e3a00e1c":"code","587c225a":"code","4d825082":"code","4e6c7de5":"code","8f0e843b":"code","2cb0285b":"code","ceb03241":"code","8f8a755a":"code","ce473691":"code","f6af8b49":"code","766c8723":"code","bac0ebd2":"markdown","3d4c3486":"markdown","9a066ff4":"markdown","d1c20279":"markdown","9a42d3fe":"markdown","b93bf13e":"markdown","3dbcbd41":"markdown","0390c297":"markdown","367ebe63":"markdown","ed4c1539":"markdown","5b5aa169":"markdown","04efb9eb":"markdown","100fe48e":"markdown","ab4593cf":"markdown","1afad5a7":"markdown","280ec1c7":"markdown","5fc6c97e":"markdown","18b076ca":"markdown","035ae101":"markdown","73987914":"markdown","2b5ca9f9":"markdown","b3f4d37d":"markdown","2815dacc":"markdown"},"source":{"085def49":"#load packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold, ShuffleSplit\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\n#import warnings\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d9a20db5":"def concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set on axis 0\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n# Read the data\ndf_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)","3e397a83":"print(df_all.describe())","06037cfe":"print(df_train.info())\ndf_train.head()","7a72d716":"print(df_test.info())\ndf_test.head()","e07e2a70":"def display_missing(df):    \n    for col in df.columns.tolist():          \n        print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n    print('\\n')\n    \nfor df in dfs:\n    print('{}'.format(df.name))\n    display_missing(df)","d2536416":"df_train.Age = df_train.Age.fillna(df_train.Age.median())\ndf_test.Age = df_test.Age.fillna(df_test.Age.median())","ce5a0bfc":"df_train.Fare = df_train.Fare.fillna(df_train.Fare.mean())\ndf_test.Fare = df_test.Fare.fillna(df_test.Fare.mean())","d6a8d7dd":"df_train.Embarked = df_train.Embarked.fillna(df_train.Embarked.mode()[0])\ndf_test.Embarked = df_test.Embarked.fillna(df_test.Embarked.mode()[0])","5679e08d":"from sklearn.decomposition import PCA\ngrains = df_train[['Age', 'Parch']].to_numpy()\n\n# Make a scatter plot of the untransformed points\nplt.scatter(grains[:,0], grains[:,1])\n\n# Create a PCA instance: model\nmodel = PCA()\n\n# Fit model to points\nmodel.fit(grains)\n\n# Get the mean of the grain samples: mean\nmean = model.mean_\n\n# Get the first principal component: first_pc\nfirst_pc = model.components_[0,:]\n\n# Plot first_pc as an arrow, starting at mean\nplt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n\n# Keep axes on same scale\nplt.axis('equal')\nplt.show()","780ae9c0":"from sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\n\nsamples = df_train[['Age', 'Parch', 'SibSp']].to_numpy()\n\n# Create scaler: scaler\nscaler = StandardScaler()\n\n# Create a PCA instance: pca\npca = PCA()\n\n# Create pipeline: pipeline\npipeline = make_pipeline(scaler,pca)\n\n# Fit the pipeline to 'samples'\npipeline.fit(samples)\n\n# Plot the explained variances\nfeatures = range(pca.n_components_)\nplt.bar(features, pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.xticks(features)\nplt.show()\n","e45584cf":"fig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(df_all.corr(), square=True, cmap='RdYlGn')","a6d670e4":"cat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp']\n\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20, 20))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.countplot(x=feature, hue='Survived', data=df_train)\n    \n    plt.xlabel('{}'.format(feature), size=20, labelpad=15)\n    plt.ylabel('Passenger Count', size=20, labelpad=15)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\n    plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)\n\nplt.show()","97653c9e":"df_all['Fare'] = pd.qcut(df_all['Fare'], 3)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Fare', hue='Survived', data=df_all)\n\nplt.xlabel('Fare', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()\n","ad698cd5":"df_all['child'] = df_all.Age[df_all.Age <14]\ndf_all['child'].fillna(0, inplace=True)","5beb3606":"def IsChild(x):\n    if 0< x <= 14:\n        return 1\n    return 0","eefe46aa":"for df in dfs:\n    df['child'] = df.Age[df.Age <14]\n    df['child'].fillna(0, inplace=True)","35de5f64":"df_train.child.apply(IsChild)\ndf_test.child.apply(IsChild)","975ab54b":"df_train['Family_Size'] = df_train['SibSp'] + df_train['Parch'] + 1\ndf_test['Family_Size'] = df_test['SibSp'] + df_test['Parch'] + 1\ndf_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n\nfig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=df_all['Family_Size'].value_counts().index, y=df_all['Family_Size'].value_counts().values, ax=axs[0][0])\nsns.countplot(x='Family_Size', hue='Survived', data=df_all, ax=axs[0][1])\n\naxs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\naxs[0][1].set_title('Survival Counts in Family Size ', size=20, y=1.05)\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndf_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n\nfor df in [df_train, df_test]:\n    df['Family_Size_Grouped'] = df['Family_Size'].map(family_map)\n\nsns.barplot(x=df_all['Family_Size_Grouped'].value_counts().index, y=df_all['Family_Size_Grouped'].value_counts().values, ax=axs[1][0])\nsns.countplot(x='Family_Size_Grouped', hue='Survived', data=df_all, ax=axs[1][1])\n\naxs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)\naxs[1][1].set_title('Survival Counts in Family Size After Grouping', size=20, y=1.05)\n\nfor i in range(2):\n    axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n        axs[i][j].set_xlabel('')\n        axs[i][j].set_ylabel('')\n\nplt.show()","664b1697":"# Obtain target and predictors\ny = df_train['Survived']\nfeatures = ['Pclass', 'Sex', 'child', 'Family_Size_Grouped'] \n\nX = pd.get_dummies(df_train[features])\nX_test_full = pd.get_dummies(df_test[features])\nX_full = pd.get_dummies(df_all[features])","f2389284":"# Import necessary modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=505, stratify= y)\n\n# Create a k-NN classifier with 6 neighbors: knn\nknn = KNeighborsClassifier(n_neighbors = 6)\n\n# Fit the classifier to the training data\nknn.fit(X_train,y_train)\n\n# Print the accuracy\nprint(knn.score(X_test, y_test))","347da9c5":"# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train,y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nfig, axs = plt.subplots(figsize=(10, 5))\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","62709a58":"# Import necessary modules\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Predict the labels of the test data: y_pred\ny_pred = knn.predict(X_test)\n\n# Generate the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","058e3bd5":"# Import the necessary modules\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Setup the pipeline steps: steps\nsteps = [('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier())]\n        \n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nneighbors = np.arange(1, 9)\nparameters = {'knn__n_neighbors':neighbors}\n\n# Instantiate the GridSearchCV object: cv\nknn_cv = GridSearchCV(pipeline, param_grid= parameters, scoring='accuracy', cv=5)\n\n# Fit to the training set\nknn_cv.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = knn_cv.predict(X_test)\n\n# Instantiate and fit a k-NN classifier to the unscaled data\nknn_unscaled = GridSearchCV(KNeighborsClassifier(), param_grid= {'n_neighbors': neighbors}, scoring='accuracy', cv=5).fit(X_train, y_train)\n\n# Compute and print metrics\nprint(\"Accuracy: {}\".format(knn_cv.score(X_test, y_test)))\nprint('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Tuned Model Parameters: {}\".format(knn_cv.best_params_))","e3a00e1c":"# Setup the pipeline\nsteps = [('scaler', StandardScaler()),\n         ('SVM', SVC(random_state=505))]\n\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\n\n# Instantiate the GridSearchCV object: cv\ncv = GridSearchCV(pipeline, param_grid= parameters, scoring='accuracy', cv=5)\n\n# Fit to the training set\ncv.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = cv.predict(X_test)\n\n# Instantiate and fit a SVC classifier to the unscaled data\nsvc_unscaled = GridSearchCV(SVC(random_state=505), param_grid= {'C':[1, 10, 100], 'gamma':[0.1, 0.01]}, scoring='accuracy', cv=5).fit(X_train, y_train)\n\n# Compute and print metrics\nprint(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\nprint('Accuracy without Scaling: {}'.format(svc_unscaled.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))","587c225a":"# Import the necessary modules\nfrom sklearn.linear_model import LogisticRegression\n\n# Create the classifier: logreg\nlogreg = LogisticRegression()\n\n# Fit the classifier to the training data\nlogreg.fit(X_train,y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred_log = logreg.predict(X_test)\n\n# Compute and print the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred_log))\nprint(classification_report(y_test, y_pred_log))","4d825082":"# Import necessary modules\nfrom sklearn.metrics import roc_curve\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nfig, axs = plt.subplots(figsize=(10, 10))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Logistic Regression ROC Curve')\nplt.show()","4e6c7de5":"# Compute and print AUC score\nprint(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n\n# Compute cross-validated AUC scores: cv_auc\ncv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc', n_jobs=4)\n\n# Print list of AUC scores\nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\nprint(\"Average 5-Fold CV AUC Scores: {}\".format(np.mean(cv_auc)))","8f0e843b":"# Import necessary modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Setup the pipeline\nsteps = [('scaler', StandardScaler()),\n         ('logreg', LogisticRegression(solver='liblinear',random_state=505))]\n\npipeline = Pipeline(steps)\n\n# Setup the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'logreg__C': c_space}\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(pipeline, param_grid, scoring = 'accuracy', cv=5, n_jobs=4)\n\n# Fit it to the training data \nlogreg_cv.fit(X_train,y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = logreg_cv.predict(X_test)\n\n# Instantiate and fit a SVC classifier to the unscaled data\nlogreg_unscaled = GridSearchCV(LogisticRegression(solver='liblinear', random_state=505),\n                               param_grid= {'C':c_space},\n                               scoring='accuracy',\n                               cv=5).fit(X_train, y_train)\n\n# Compute and print metrics\nprint(\"Accuracy: {}\".format(logreg_cv.score(X_test, y_test)))\nprint('Accuracy without Scaling: {}'.format(logreg_unscaled.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Tuned Model Parameters: {}\".format(logreg_cv.best_params_))","2cb0285b":"# Setup the hyperparameter grid\nparam_grid= {'n_estimators':[10,100],\n             'max_depth':[3,6],\n             'criterion':['gini','entropy']}\n\n# Instantiate the GridSearchCV object: RandomForestClassifier\nrf_cv = GridSearchCV(RandomForestClassifier(random_state=105),\n                     param_grid,\n                     scoring='accuracy',\n                     cv=5)\n\n# Fit it to the training data \nrf_cv.fit(X_train,y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = rf_cv.predict(X_test)\n\n# Compute and print metrics\nprint(\"Accuracy: {}\".format(rf_cv.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Tuned Model Parameters: {}\".format(rf_cv.best_params_))","ceb03241":"# Setup the hyperparameter grid\nparam_grid = {\n    \"n_estimators\":[50,100],\n    \"learning_rate\":[0.1,0.01],\n    'max_depth': [3,6],\n }\n\n# Instantiate the GridSearchCV object with objective= 'binary:logistic'\nxgb_cv = GridSearchCV(XGBClassifier(objective= 'binary:logistic', random_state=505), \n                      param_grid, scoring = 'accuracy', cv=5, n_jobs=4)\n\n# Fit it to the training data \nxgb_cv.fit(X_train,y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = xgb_cv.predict(X_test)\n\n# Instantiate and fit a XGB classifier with objective= 'binary:hinge'\nxgb_hinge = GridSearchCV(XGBClassifier(objective= 'binary:hinge', random_state=505), param_grid, \n                            scoring='accuracy', cv=5).fit(X_train, y_train)\n\n# Compute and print metrics\nprint(\"Accuracy with binary logistic: {}\".format(xgb_cv.score(X_test, y_test)))\nprint('Accuracy with binary hinge: {}'.format(xgb_hinge.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Tuned Model Parameters: {}\".format(xgb_cv.best_params_))","8f8a755a":"from xgboost import plot_importance\n# fit model no training data\nmodel = XGBClassifier()\nmodel.fit(X, y)\n# feature importance\nprint(model.feature_importances_)\n# plot\nplot_importance(model)\n#plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\nplt.show()","ce473691":"# Import necessary modules\nfrom scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\n    \"n_estimators\":randint(10, 100),\n    \"learning_rate\":[0.5, 0.1, 0.01, 0.001],\n    'max_depth': randint(1, 9),\n }\n\n# Instantiate the RandomizedSearchCV object: tree_cv\nxgb_rs = RandomizedSearchCV(XGBClassifier(objective= 'binary:logistic', random_state=23),\n                            param_dist,\n                            cv=10,\n                            n_jobs=4,\n                           random_state=2,\n                           n_iter=20)\n\n# Fit it to the data\nxgb_rs.fit(X_train,y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = xgb_rs.predict(X_test)\n\n# Compute and print metrics\nprint(\"Accuracy: {}\".format(xgb_rs.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Tuned Model Parameters: {}\".format(xgb_rs.best_params_))","f6af8b49":"# Import necessary modules\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\n\nn_cols = X.shape[1]\n\npredictors = X.as_matrix()\n# Convert the target to categorical: target\ntarget = to_categorical(y)\n\n# Set up the model\nmodel = Sequential()\n\n# Add the desired number of layers\nmodel.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n\nmodel.add(Dense(50, activation='relu'))\n\nmodel.add(Dense(20, activation='relu'))\n\nmodel.add(Dense(10, activation='relu'))\n# Add the output layer\nmodel.add(Dense(2,activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nearly_stopping_monitor = EarlyStopping(patience=2)\n# Fit the model\nmodel_1 = model.fit(predictors, target, epochs=50, validation_split=0.2, callbacks=[early_stopping_monitor])\n\n# Create the plot\nplt.plot(model_1.history['val_loss'], 'r')\nplt.xlabel('Epochs')\nplt.ylabel('Validation score')\nplt.show()","766c8723":"# Tuned Model Parameters: {'criterion': 'gini', 'max_depth': 3, 'max_features': 8, 'min_samples_leaf': 6}\npredictors_test = X_test_full.as_matrix()\n\n# Generate test predictions\npred_test = model.predict(predictors_test)\npd.Series(pred_test[:,1]).round().astype(int)\n\noutput = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': pd.Series(pred_test[:,1]).round().astype(int)})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","bac0ebd2":"**KERAS**","3d4c3486":"**Feature Engineering**\n\nAccodring to https:\/\/www.anesi.com\/titanic.htm women, children from class 1, class 2 and crew have more than 80% survival rate. So let's keep this in mind.","9a066ff4":"**Logistic Regression**","d1c20279":"Try a naive out-of-the-box model **K- Nearest Neighbor** to start","9a42d3fe":"**MACHINE LEARNING MODELS**","b93bf13e":"> First we deal with the missing entries of the Age column. The lazy way to do this is to simply replace the missing value by the median of the Age column","3dbcbd41":"**SUPPORT VECTOR MACHINES**  \n*C-Support Vector Classification*","0390c297":"**Extreme Gradient Boosting with GridSearchCV**","367ebe63":"**FINAL MODEL**","ed4c1539":"> Let's check if there are any correlations between the features\/ columns ","5b5aa169":"> Parch and SibSp are correlated: Also, Fare and Survived. We explore this more later","04efb9eb":"Seems like rich people are highly likely to be saved! :p","100fe48e":"Due to **class imbalance**, i.e. there was ~30% surviving rate, we will look at the confusion matrix and classification report since this is good practice.","ab4593cf":"> The models below do not get affected by rescaling ","1afad5a7":"**Visualization**","280ec1c7":"We must save the young ones!","5fc6c97e":"**ANALYSING THE DATA**","18b076ca":"Below demonstrates the power of rescaling your data.","035ae101":"'Embarked'","73987914":"'Fare'","2b5ca9f9":"**Random Forest Classifier**","b3f4d37d":"**Extreme Gradient Boosting with RandomizedSearchCV**","2815dacc":"This is a personal notebook to practice machine learning models as a go through my DataCamp lessons."}}