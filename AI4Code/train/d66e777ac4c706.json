{"cell_type":{"5dbcbc88":"code","cf7ab2ed":"code","5a3a674f":"code","a94a6941":"code","9839e5c8":"code","c3549b99":"code","8d695e8c":"code","163d8194":"code","854de7bf":"code","2713c4ec":"code","7d7b4ba9":"code","93ead1a9":"code","0e67f4df":"code","635175c9":"code","f1069465":"code","4aa49a87":"code","bdd2014a":"code","836ab67c":"code","80771821":"code","92b60838":"code","c962a5fc":"code","9d650094":"code","a91e42f2":"code","3d26c6e5":"code","b9c750af":"code","bbee38ad":"code","9f6a4bb3":"code","4fd05355":"code","376d35b0":"code","f06f2fc2":"code","509b438e":"code","dbffa41a":"code","8b334f58":"code","112bf8c7":"code","be6cbf39":"code","5f97bd7e":"code","9008cfa3":"code","d2ec54c8":"code","243d262f":"code","c044bb6a":"code","c8a9cef8":"code","cab80ace":"code","b5d8c120":"code","c98204ff":"code","aa81e761":"code","1531f932":"code","7e13569b":"code","39256965":"markdown","914854b0":"markdown","b207b612":"markdown","76fa9a6e":"markdown","75d14f71":"markdown"},"source":{"5dbcbc88":"import pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","cf7ab2ed":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","5a3a674f":"data=pd.read_csv('\/kaggle\/input\/housing-prices-in-metropolitan-areas-of-india\/Hyderabad.csv')","a94a6941":"data.head(5)","9839e5c8":"data.describe() #Descriptive analysis","c3549b99":"data.info() ","8d695e8c":"plt.scatter(x=data['Area'],y=data['Price'],alpha=0.5)\nplt.ylabel('Rupees of Order 10 Crores')\nplt.xlabel('Area')\nplt.title('outlier points at area > 6000 & price at  16Cr')","163d8194":"sb.boxplot(x=data['Price'])","854de7bf":"plt.hist(data['Area'],bins=12)\nplt.title('Most of House areas in dataset are under 2000')","2713c4ec":"len(data['Location'].unique()) \n#There are 243 Unique Locations ","7d7b4ba9":"data.groupby('Location')['Price'].count().sort_values(ascending=False)[0:10]\n#Top 30 Locations of houses from the dataset","93ead1a9":"Location_text = \" \".join(data.Location)\nwordcloud = WordCloud(width=720, height=360,collocations=False).generate(text=Location_text)\nplt.figure(figsize=(30,18))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n#wc=WordCloud().generate(text=Location_text)","0e67f4df":"#Location_text","635175c9":"#from sklearn.impute import SimpleImputer\n#imp=SimpleImputer(missing_values=9,strategy=median)","f1069465":"# Replacing 9 with Null values\ndata[5:]=data[5:].replace(to_replace=9,value=np.nan)\ndata.isnull().sum()","4aa49a87":"len(data) #Before removing null values","bdd2014a":"data=data[5:].dropna(axis=0)\nlen(data)\ndata.isnull().sum()","836ab67c":"len(data) #After removing null values","80771821":"#To impute data rather than removing nulls\n\"\"\"\nfrom sklearn.impute import SimpleImputer\nimp=SimpleImputer(missing_values=9,strategy='most_frequent')\ncols=data.columns[5:]\ndata[cols]=imp.fit_transform(data[cols])\n\"\"\"","92b60838":"data.describe() #the data is clean","c962a5fc":"#data.to_csv('Hyderabad-vcr.csv')","9d650094":"q1=data['Price'].quantile(0.25)\nq3=data['Price'].quantile(0.75)\nirp=q3-q1\nlow=q1-1.5*irp\nupr=q3+1.5*irp\n#low=data['Price'].min() #since low was neg\nlow,upr","a91e42f2":"def imp(val):\n    if val>upr:\n        return upr\n    if val<low:\n        return low\n    else:\n        return val\ndata['Price']=data['Price'].apply(imp)","3d26c6e5":"plt.scatter(x=data['Area'],y=data['Price'],alpha=0.5)","b9c750af":"sb.boxplot(x=data['Price'])","bbee38ad":"plt.subplots(figsize = (25, 20))\ncolormap= sb.diverging_palette(220, 10, as_cmap = True)\nsb.heatmap(data.corr(), annot=True, cmap = colormap)","9f6a4bb3":"#For location it has many unique values and has alternate hypothesis so we need to bin the values to fewer groups\n#For that we are going to find the mean Price for each location and sorting them in ascending order\nLocation_table=data.groupby('Location').agg({'Price':'mean'}).sort_values('Price',ascending=True)","4fd05355":"Location_table.head()","376d35b0":"Location_table['Loc']=pd.cut(Location_table['Price'],bins=10,labels=['G0',\n                                                          'G1',\n                                                          'G2',\n                                                          'G3',\n                                                          'G4',\n                                                          'G5',\n                                                          'G6',\n                                                          'G7',\n                                                          'G8',\n                                                          'G9'],\n                           include_lowest=True)","f06f2fc2":"Location_table['Loc'].head()","509b438e":"Location_table=Location_table.drop(columns=\"Price\")\n#TO merge two tables we use \"merge\" function from pandas using zipcode as identifier\ndata=pd.merge(data,Location_table,\n                left_on='Location',\n                how='left',\n                right_index=True)\ndata.drop(columns='Location',inplace=True)","dbffa41a":"data.head()","8b334f58":"#now create dummies for Location\ndata=pd.get_dummies(data,columns=['Loc'],drop_first=True)\ndata.head()","112bf8c7":"#data.to_csv('Hyderabad-vtcr.csv')","be6cbf39":"from sklearn.preprocessing import StandardScaler as ss\n#Scaling the data set\nscalar=ss()\nY=data['Price']\n# Scaling\nX=scalar.fit_transform(data.drop(columns=['Price']))\n# Converting to pandas dataframe for easy manipulation\nX=pd.DataFrame(data=X,columns=data.drop(columns=['Price']).columns)\nX.head()","5f97bd7e":"# Checking multicollinearity\nk=X.corr()\nz=[[str(i),str(j)] for i in k.columns for j in k.columns if (k.loc[i,j]>abs(0.5)) & (i!=j)]\nz,len(z)","9008cfa3":"#Caluclating VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nVIF=pd.Series([vif(X.values,i) for i in range (X.shape[1])],index=X.columns)\nVIF","d2ec54c8":"# We need to remove vif > 5 and each time we remove one column the vif data changes so we remove,check vif,remove\ndef MC_rem(data):\n    VIF=pd.Series([vif(data.values,i) for i in range (data.shape[1])],index=data.columns)\n    if(VIF.max()>5):\n        data.drop(columns=[VIF[VIF==VIF.max()].index[0]],inplace=True)\n        print(VIF[VIF==VIF.max()].index[0],'has been removed from \"X_copy\"')\n        return data\n    else:\n        print('no multicollinearity')\n        return data","243d262f":"X_copy=X.copy()\nfor i in range(5):\n    X_copy=MC_rem(X_copy)\nX=X_copy\n###After Removing collinearity\nVIF=pd.Series([vif(X_copy.values,i) for i in range (X_copy.shape[1])],index=X_copy.columns)\nVIF","c044bb6a":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=101)\n\nX_train.shape,X_test.shape,Y_train.shape,Y_test.shape","c8a9cef8":"from sklearn.linear_model import LinearRegression \nlr=LinearRegression(normalize=True)\n# If norm=true intercept=0\nlr.fit(X_train,Y_train)","cab80ace":"predictions=lr.predict(X_test)\nlr.score(X_test,Y_test)","b5d8c120":"residuals=predictions-Y_test\nresidual_table=pd.DataFrame({'residual':residuals,\n                            'prediction':predictions})\nresidual_table=residual_table.sort_values(by='prediction')\nz=[i for i in range(int(residual_table['prediction'].max()))]\nk=[0 for i in range(int(residual_table['prediction'].max()))]","c98204ff":"plt.figure(dpi=130,figsize=(17,7))\nplt.scatter(residual_table['prediction'],residual_table['residual'],color='red',s=25)\nplt.plot(z,k,color='green',linewidth=3,label='Regression line')\nplt.ylim(-800000,800000)\nplt.xlabel('Fitted points(ordered by predictions)')\nplt.ylabel('Residuals')\nplt.title('residual plot')\nplt.legend()\nplt.show()","aa81e761":"# Plotting the distribution of errors\nplt.figure(dpi=100,figsize=(10,7))\nplt.hist(residual_table['residual'],color='red',bins=200)\nplt.xlabel('Residuals')\nplt.ylabel('frequency')\nplt.title('Distribution of residuals')\nplt.show()","1531f932":"coefftab=pd.DataFrame({\n    'column':X_train.columns,\n    'coeff':lr.coef_\n})\ncoefftab=coefftab.sort_values(by='coeff')","7e13569b":"plt.figure(figsize=(8,6),dpi=120)\nx=coefftab['column']\ny=coefftab['coeff']\nplt.barh(x,y)\nplt.xlabel('Coefficients')\nplt.ylabel('variables')\nplt.title('Normalised Coefficient Plot')\nplt.show()","39256965":"<p>The dataset has target variable <b>Price<\/b> and all other are independent variables<br>\nAll independent variables are boolean in nature except Area, No.of.Bedrooms and location also the null values have been represented as 9 in boolean columns, there are no blank or missing data from count<\/p>","914854b0":"## Implementing Linear Regression","b207b612":"Imputing Outliers","76fa9a6e":"<h2>Data Cleaning<\/h2>\nOutliers and Null Values (9 in this data set)","75d14f71":"## Transforming location variable "}}