{"cell_type":{"a0799f75":"code","222387ef":"code","9d8927bd":"code","f8471b0d":"code","456bbfa2":"code","c040b4e6":"code","f98334aa":"code","b8e39f0e":"code","d7dd8599":"code","78f7d8ad":"code","a78f021d":"code","197b8fe8":"code","00f636e5":"code","c166e49e":"code","2361ee30":"code","1d0d1573":"code","faaf0050":"code","150eee3d":"code","a568b27e":"code","38b27241":"code","a0b73ddb":"code","a2684b5a":"code","28417bac":"code","2cf10c19":"code","7f208581":"code","fa77c61d":"code","8b99b089":"code","07ec7eaf":"code","f559a1c4":"code","27e76bf0":"code","690438bc":"code","e9a048f7":"code","0adfd7d9":"code","ded51833":"code","001aa96a":"code","65aadb1b":"code","feaac651":"code","d265861c":"code","e70927a0":"code","36433b0f":"code","ccf05129":"code","ec211e31":"code","97aa8525":"code","0c71a6d2":"code","22a58e52":"code","90d5432a":"code","cdf8dd61":"code","3b28e663":"code","cc0509e2":"code","cb7dd26c":"code","82af31b7":"code","b7975787":"code","5df2d363":"code","abb5ecc6":"code","7bb23907":"code","1afd624e":"code","35801ae5":"code","c5b3fba5":"code","89ae1f46":"code","8d6bc2f7":"code","ec2202da":"code","440cc246":"code","0a726cef":"code","62cfd4c7":"code","1366d3e9":"code","70463790":"code","11fef36d":"code","0d9b7a4f":"code","965da2c4":"code","d40554f4":"code","cc1413d8":"code","d7731ac1":"code","78c88185":"code","3a333613":"code","afc0b156":"code","1faa822b":"code","e274f631":"code","5a2f943e":"code","66382fab":"code","052d29c6":"code","6b1ee6d5":"code","976642d1":"code","34db5fbe":"code","b82ed991":"code","09d74db2":"code","1b464067":"code","32e8caeb":"code","0f4c7fb0":"code","257ef5c1":"code","1e1a6bf6":"code","59aa1979":"code","760b3867":"code","a797e39b":"code","dc993be0":"code","0db5b8a5":"code","391e8f07":"code","50a2a941":"code","9b35c8ab":"code","eaf7597f":"code","3871a38e":"markdown","b5d3a68d":"markdown","aac7331c":"markdown","12955929":"markdown","4b009e22":"markdown","2f0c3392":"markdown","631a098e":"markdown","10a3d862":"markdown","288ea1de":"markdown","37bca7c8":"markdown","6fda2ad7":"markdown","d1949de1":"markdown","324854e4":"markdown","ad6b1b06":"markdown","d4f5af1a":"markdown","b23ca028":"markdown","f7b8878f":"markdown","6de82640":"markdown","5ed4786f":"markdown","6edf47ca":"markdown","cc2881a6":"markdown","41e2bbfd":"markdown","9c0b3533":"markdown","7e9e0017":"markdown","f01eae0c":"markdown","cb302b97":"markdown","11a8f9ad":"markdown","b45465a8":"markdown","600f41b7":"markdown"},"source":{"a0799f75":"'''\n    importing the required libraries here\n'''\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n'''\n    Importing the library for PCA\n'''\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","222387ef":"path =r'..\/input\/train.csv'\n\ndf = pd.read_csv(path)\n","9d8927bd":"df.info()","f8471b0d":"'''\n    this method will find the null percentage of all the columns in the dataframe passed to it\n'''\ndef find_null_per(df):\n    return round((df.isnull().sum()\/len(df))*100,2).sort_values(ascending=False)","456bbfa2":"'''\n    Checking for the null percentage in the dataframe df\n'''\nfind_null_per(df)","c040b4e6":"'''\n    This function will replace all the blank values for the passed dataframe to NA values. which will be acted as a string.\n'''\ndef replace_val(df):\n    for i in df.columns:\n        df[i] = df[i].fillna('NA')\n    return df\n","f98334aa":"'''\n    na_col_val variable has list of all those columns, which has null values, and all those null values should be considered\n    as a value of the column and not the nan value.\n'''\n\nna_col_val = ['Alley','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType',\n 'GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature']\ndf[na_col_val] = replace_val(df[na_col_val])","b8e39f0e":"df_copy = df.copy(deep=True)","d7dd8599":"'''\n    now we will check for all the columns where the data is skewed and then will remove all those columns, where the \n    one of the column value is greater than 80\n'''\n\ndef feature_removal(df,col,skew_val):\n    lst=[]\n    for i in col:\n        if round(df[i].value_counts()\/len(df)*100,2)[0] > skew_val:\n            df = df.drop(i, axis =1)\n            lst.append(i)        \n    print(\"removed columns are {val}\".format(val = lst))\n    return df       ","78f7d8ad":"df = feature_removal(df,na_col_val,skew_val=80)","a78f021d":"'''\n    list of removed columns are -:\n    removed columns are ['Alley', 'BsmtCond', 'BsmtFinType2', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n    \n    Here we are doing EDA on few columns. We will be checking if the columns which have been removed above were causing\n    biasness in the data or not.\n    \n    And from the below graph it is evident that the columns were biased towards one of the categories in the categorical \n    column.\n'''\n\n#in order to visualise a categorical variable we should use a box plot\nplt.figure(figsize=(5, 5))\n\nlst = ['Alley', 'BsmtCond', 'BsmtFinType2', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n\nl = len(lst)\/2\nfor i in lst:\n    sns.countplot(x = i, data = df_copy)\n    plt.show()\n\n#boxplot boundaries represents - 25%, median, 75 %","197b8fe8":"df.head()","00f636e5":"find_null_per(df)","c166e49e":"'''\n    Imputing the left over NA values, and checking what to do with the NA value. Whether to use mean, median or mode. \n    ]And how to decide that.\n'''\ndf['LotFrontage'].describe(percentiles = [0.25,0.50,0.75,0.90,0.95,0.99])","2361ee30":"'''\n    It seems, median is a good option to deal with this numeric column for now.\n'''\n\ndf['LotFrontage'] = df['LotFrontage'].fillna(value = df['LotFrontage'].median())\ndf['LotFrontage'].describe(percentiles = [0.25,0.50,0.75,0.90,0.95,0.99])","1d0d1573":"'''\n    Imputing the left over NA values, and checking what to do with the NA value.\n    Whether to use mean, median or mode. And how to decide that.\n'''\n\ndf['GarageYrBlt'].describe(percentiles = [0.25,0.50,0.75,0.90,0.95,0.99])","faaf0050":"'''\n    It seems, median is a good option to deal with this numeric year column also.\n    we are imputing this first, because with this we can change the year column easily\n'''\n\ndf['GarageYrBlt'] = df['GarageYrBlt'].fillna(value = df['GarageYrBlt'].median())\ndf['GarageYrBlt'].describe(percentiles = [0.25,0.50,0.75,0.90,0.95,0.99])","150eee3d":"'''\n    converting GarageYrBlt and YrSold to the age columns and will drop this columnn at the end.\n'''\n\ndf['GarageYrBlt_age'] = max(df['GarageYrBlt']) - df['GarageYrBlt']\ndf['YrSold_age'] = max(df['YrSold']) - df['YrSold']\ndf['YearBuilt_age'] = max(df['YearBuilt']) - df['YearBuilt']\ndf['YearRemodAdd_age'] = max(df['YearRemodAdd']) - df['YearRemodAdd']\n\ndf = df.drop('GarageYrBlt', axis =1)\ndf = df.drop('YrSold', axis =1)\ndf = df.drop('YearBuilt', axis =1)\ndf = df.drop('YearRemodAdd', axis =1)","a568b27e":"sns.pairplot(df)","38b27241":"'''\n    checking for the percentage of null values.\n'''\nfind_null_per(df)","a0b73ddb":"df.head()","a2684b5a":"df = df.drop('Id', axis = 1)","28417bac":"'''\n    Dropping the left over Null values as they are very less and would not impact much with the model creation.\n'''\n\ndf = df.dropna()\nfind_null_per(df)","2cf10c19":"'''\n    here i am checking if my target variable is normally distributed or not. And it seems the target variable is \n    skewed towards right. So, i will be using the logarithmic function to remove the skewness from the target\n    variable\n'''\n\n# Plot the histogram of the target variable\nfig = plt.figure()\nsns.distplot(df['SalePrice'], bins = 20)\nfig.suptitle('Sale Price', fontsize = 20)                  # Plot heading \nplt.xlabel('Sales', fontsize = 18)                         # X-label","7f208581":"'''\n    using the np.log function to normalize the target variable\n'''\nlog_func = np.log(df.SalePrice)\ndf['SalePrice'] = log_func\ndf.describe()","fa77c61d":"'''\n    Here, we can see that after using the logarithmic function, we have received the target variable as a normally\n    distributed variable\n'''\n# Plot the histogram of the target variable\nfig = plt.figure()\nsns.distplot(df['SalePrice'], bins = 20)\nfig.suptitle('Sale Price', fontsize = 20)                  # Plot heading \nplt.xlabel('Sales', fontsize = 18)                         # X-label","8b99b089":"'''\n    we will start the analysis with the heatmap which will tell us the correlations of the \n    columnms\n'''\nplt.figure(figsize = (20,16))\nsns.heatmap(df.corr(), annot=True)","07ec7eaf":"'''\n    Here we are creating two variables df_num which will have numerical variables and df_Rest which will have all the \n    variables except for the target variable.\n    \n    The df_temp created here will be used to find the multi collinearity of the numerical columns\n'''\ndf_temp = df.drop(['SalePrice'], axis = 1)\ndf_num = np.array(df_temp.select_dtypes(include=[np.number]).columns.values)\ndf_rest = set(df.columns) - set(df_num)","f559a1c4":"df_temp[df_num].head()\ndf_temp.columns","27e76bf0":"'''\n    I have created this method to automate the VIF process of feature removal. In this method, I am passing\n    a dataframe and the set of numerical columns to prcoeed with the removal of collinear columns.\n    For this assignment, the value for accepted coefficient is less than 5.\n    \n    Also, this method will print the order in which we are deleting the columns from the data frame\n'''\ndef calculate_vif(df,col):\n    vif = pd.DataFrame()\n    X = df[col]\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    if vif.iloc[0][1] < 5 :\n        return X\n    else:\n        Y = X.drop(vif.iloc[0][0], axis =1)\n        print(\"column dropped as part of VIF is  {val}\".format(val = vif.iloc[0][0]))\n        return(calculate_vif(X,Y.columns))","690438bc":"'''\n    calling calculate_vif which will iteratively remove the columns with more than 5 coefficient value. This deletes\n    one coefficient at a time.\n'''\ndf_vif = calculate_vif(df_temp,df_num)","e9a048f7":"'''\n    plotting the heatmap after removing most of the multicollinear columns\n'''\nplt.figure(figsize = (20,16))\nsns.heatmap(df_vif.corr(), annot=True)","0adfd7d9":"plt.figure(figsize=(16,8))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'LotArea', data = df_vif) \n\nplt.subplot(2,3,2)\nsns.boxplot(x = 'MasVnrArea', data = df_vif)  \n\nplt.subplot(2,3,3)\nsns.boxplot(x = 'BsmtFinSF1', data = df_vif)\n\nplt.subplot(2,3,4)\nsns.boxplot(x = 'BsmtFinSF2', data = df_vif)\n\nplt.subplot(2,3,5)\nsns.boxplot(x = 'BsmtUnfSF', data = df_vif)\n\nplt.subplot(2,3,6)\nsns.boxplot(x = '2ndFlrSF', data = df_vif)\n\nplt.show()","ded51833":"plt.figure(figsize=(16,8))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'LowQualFinSF', data = df_vif) \n\nplt.subplot(2,3,2)\nsns.boxplot(x = 'BsmtFullBath', data = df_vif)  \n\nplt.subplot(2,3,3)\nsns.boxplot(x = 'BsmtHalfBath', data = df_vif)\n\nplt.subplot(2,3,4)\nsns.boxplot(x = 'HalfBath', data = df_vif)\n\nplt.subplot(2,3,5)\nsns.boxplot(x = 'Fireplaces', data = df_vif)\n\nplt.subplot(2,3,6)\nsns.boxplot(x = 'WoodDeckSF', data = df_vif)\n\nplt.show()","001aa96a":"plt.figure(figsize=(16,8))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'OpenPorchSF', data = df_vif) \n\nplt.subplot(2,3,2)\nsns.boxplot(x = 'EnclosedPorch', data = df_vif)  \n\nplt.subplot(2,3,3)\nsns.boxplot(x = '3SsnPorch', data = df_vif)\n\nplt.subplot(2,3,4)\nsns.boxplot(x = 'ScreenPorch', data = df_vif)\n\nplt.subplot(2,3,5)\nsns.boxplot(x = 'PoolArea', data = df_vif)\n\nplt.subplot(2,3,6)\nsns.boxplot(x = 'MiscVal', data = df_vif)\n\nplt.show()","65aadb1b":"plt.figure(figsize=(16,8))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'GarageYrBlt_age', data = df_vif) \n\nplt.subplot(2,3,2)\nsns.boxplot(x = 'YrSold_age', data = df_vif)  \n\nplt.subplot(2,3,3)\nsns.boxplot(x = 'YearRemodAdd_age', data = df_vif)\n","feaac651":"df_vif.shape","d265861c":"'''\n    This function will help us in removing the outliers from a list of columns and a dataframe passed to this\n'''\ndef remove_outliers(df,col):\n    for i in col:\n        Q1 = df[i].quantile(0.05)\n        Q3 = df[i].quantile(0.97)\n        df = df[(df[i] >=Q1) &(df[i] <=Q3)]\n    return df","e70927a0":"df_no_outlier = remove_outliers(df_vif, list(df_vif.columns))\ndf_no_outlier.shape","36433b0f":"'''\n    concatenating the data recieved after removing the multicollinearity from the raw data.\n'''\n\ndf1 = df[df_vif.columns]\ndf3 = df[df_rest]\n\ndf_final = pd.concat([df1,df3] , axis = 1)\ndf_final.shape","ccf05129":"'''\n    there is one variable which is part of df_num showing numeric columns, where as it is a categorical column.\n    Hence changing the type to object.\n'''\ndf_final['MSSubClass'] = df_final['MSSubClass'].astype(object)","ec211e31":"'''\n    getting a list of columns which have data type as objects, to get the list of categorical columns.\n'''\n\nothers = ['object']\ndf_cat = np.array(df_final.select_dtypes(exclude=[np.number]).columns.values)\ndf_cat","97aa8525":"'''\n    generating the dummy variables from a list of category variables df_cat.\n'''\n\ndf_dummies = pd.get_dummies(df_final[df_cat], drop_first = True)\ndf_dummies.shape\n","0c71a6d2":"'''\n    printing the dummy columns, to verify if the dummy variables are generated fine or not.\n'''\n\ndf_dummies.columns","22a58e52":"'''\n    Here I am concatenating the original data frame with the dummy variables which were created. This is done to get the \n    entire dataframe in 1's and 0's for a Linear Regression.\n'''\n\ndf_final_1 = pd.concat([df_final, df_dummies], axis = 1)\ndf_final_1.shape","90d5432a":"'''\n    since the dummy variables are created and concatenated to the actual dataframe hence, i am removing the original columns\n    so that we donot have redundancy in the columns.\n'''\n\ndf_final_1 = df_final_1.drop(df_cat, axis = 1)\ndf_final_1.shape\ndf_final_1.columns","cdf8dd61":"df_copy.columns","3b28e663":"'''\n    Here i will be taking two columns at a time to do the EDA. TO understand the relation  of one column \n    with the target column.\n'''\nsns.pairplot(df_copy[['SalePrice','LotArea']])\nplt.show()","cc0509e2":"sns.pairplot(df_copy[['MasVnrArea','SalePrice']])\nplt.show()","cb7dd26c":"sns.pairplot(df_copy[['BsmtFinSF1','SalePrice']])\nplt.show()","82af31b7":"sns.pairplot(df_copy[['BsmtFinSF2','SalePrice']])\nplt.show()","b7975787":"#sns.pairplot(df_copy[['MSSubClass','SalePrice']])\nsns.catplot(x=\"MSSubClass\", y=\"SalePrice\", data=df_copy);\nplt.show()","5df2d363":"sns.catplot(x=\"SaleType\", y=\"SalePrice\", data=df_copy)\nplt.show()","abb5ecc6":"sns.catplot( x=\"LotShape\", y=\"SalePrice\", data=df_copy)\nplt.show()","7bb23907":"sns.catplot(x=\"SaleCondition\", y=\"SalePrice\", data=df_copy)\nplt.show()","1afd624e":"sns.catplot(x=\"RoofStyle\", y=\"SalePrice\", data=df_copy)\nplt.show()","35801ae5":"sns.catplot(x=\"GarageType\", y=\"SalePrice\", data=df_copy)\nplt.show()","c5b3fba5":"'''\n    Splitting the data into train and test data to start with the model creation.\n'''\n\ndf_train, df_test = train_test_split(df_final_1, train_size = 0.80, random_state = 100)\ndf_train.shape","89ae1f46":"df_test.shape","8d6bc2f7":"df_train.head()","ec2202da":"df_num = np.array(df_final.select_dtypes(include=[np.number]).columns.values)","440cc246":"'''\n    Scaling the test and the train data, using the Min Max scaler. So that everything is in the range 0 to 1\n'''\nscaler = MinMaxScaler()\ndf_train[df_num] = scaler.fit_transform(df_train[df_num])\ndf_train[df_num].describe()\ndf_test[df_num] = scaler.transform(df_test[df_num])","0a726cef":"'''\n    For creating the equation, we have separated the independent variable with the rest of the dependent variables.\n'''\ny_train = df_train.pop('SalePrice')\nX_train = df_train\n\ny_test = df_test.pop('SalePrice')\nX_test = df_test","62cfd4c7":"X_train.head()","1366d3e9":"y_train.head()","70463790":"df_test.describe()","11fef36d":"'''\n    I have created this function, so as to reduce the line of code \n    - for trying multiple paramter values of alpha\n    - create multiple models for both the ridge and the lasso regression\n    - evaluate the models graphically.\n    \n    Here i am passing 6 paramters which have the -:\n    - type of regression ridge\/lasso\n    - param - containing the alpha paramter value\n    - scr - 'neg_mean_absolute_error' or the scoring term to be used in ridge regression.\n    - fold - no of folds, for now we are using 5\n    - X_train\/y_train to train the models.\n'''\n\ndef lasso_ridge_model(estimator, param,scr,fold,X_train,y_train):\n    folds = fold\n    if estimator.lower() == \"ridge\":\n        estimator = Ridge()\n        model_cv = GridSearchCV(estimator = estimator,\n                            param_grid = param,\n                            scoring = 'neg_mean_absolute_error',\n                            cv = folds,\n                            return_train_score = True,\n                            verbose = 1                            \n                           )\n    else:\n        estimator = Lasso()\n        model_cv = GridSearchCV(estimator = estimator,\n                              param_grid = params,\n                              cv = fold,\n                              return_train_score = True,\n                              verbose =1)\n\n    model_cv.fit(X_train, y_train) \n    print(\"best alpha paramater value is {val}\".format(val = model_cv.best_params_))    \n    # plotting\n    cv_results = pd.DataFrame(model_cv.cv_results_)\n    plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n    plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n    plt.xlabel('alpha')\n    plt.ylabel('Negative Mean Absolute Error')\n    plt.title(\"Negative Mean Absolute Error and alpha\")\n    plt.legend(['train score', 'test score'], loc='upper left')\n    plt.show()\n    return model_cv","0d9b7a4f":"'''\n    Here we have created the alpha parameter and then used the lasso_ridge_model function\n'''\nparams ={'alpha':[0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000]}\nmodel_ridge_1 = lasso_ridge_model(\"ridge\", params,'neg_mean_absolute_error',5,X_train,y_train)","965da2c4":"'''\n    Here we have tried to tune the alpha parameter and then used the lasso_ridge_model function\n'''\nparams={'alpha':np.arange(0.2,1,0.01)}\nmodel_ridge_2 = lasso_ridge_model(\"ridge\",params,'neg_mean_absolute_error',5,X_train,y_train)","d40554f4":"'''\n    using the best alpha value to fit the ridge model\n'''\nalpha = 0.22\nridge = Ridge(alpha=alpha)\n\nridge.fit(X_train, y_train)\nlen(ridge.coef_)","cc1413d8":"'''\n    R2 score obtained for the ridge regression model with alpha = 0.22, even though Ridge does not zero out the coefficients\n    but still there are 4 coefficicnets whose values are so small that they are considered as 0\n\n'''\ny_train_pred_ridge = ridge.predict(X_train)\ny_test_pred_ridge = ridge.predict(X_test)\nprint(\"R2 score  Train {}\".format(r2_score(y_true=y_train,y_pred=y_train_pred_ridge)))\nprint(\"R2 score  Test {}\".format(r2_score(y_true=y_test,y_pred=y_test_pred_ridge)))\nprint(\"Number of non-zero coefficents {}\".format(np.sum(ridge.coef_!=0)))\nprint(\"RMSE train {}\".format(np.sqrt(mean_squared_error(y_train,y_train_pred_ridge))))\nprint(\"RMSE test {}\".format(np.sqrt(mean_squared_error(y_test,y_test_pred_ridge))))","d7731ac1":"'''\n    Here we have created the alpha parameter and then used the lasso_ridge_model function\n'''\nparams ={'alpha':[0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000]}\nmodel_lasso_1 = lasso_ridge_model(\"lasso\", params,'',5,X_train,y_train)","78c88185":"'''\n    Here we have tried to tune the alpha parameter and then used the lasso_ridge_model function\n'''\nparams = {'alpha' : np.arange(0,0.001,0.0001)}\nmodel_lasso_2 = lasso_ridge_model(\"lasso\", params,'',5,X_train,y_train)","3a333613":"'''\n    Here we have tried again to further tune the alpha parameter and then used the lasso_ridge_model function\n'''\nparams = {'alpha' : np.arange(0.0002,0.0005,0.00001)}\nmodel_lasso_3= lasso_ridge_model(\"lasso\", params,'',5,X_train,y_train)","afc0b156":"cv_results_lasso = pd.DataFrame(model_lasso_3.cv_results_)\ncv_results_lasso.head()","1faa822b":"'''\n    using the best alpha value to fit the lasso model\n'''\nalpha = 0.00023\nlasso = Lasso(alpha=alpha)\n\nlasso.fit(X_train, y_train)\nlen(lasso.coef_)","e274f631":"'''\n    Value of score obtained for the lasso regression and the non zero coefficicnet are 111\n'''\ny_train_pred_lasso = lasso.predict(X_train)\ny_test_pred_lasso = lasso.predict(X_test)\n\nprint(\"R2 score  Train {}\".format(r2_score(y_true=y_train,y_pred=y_train_pred_lasso)))\nprint(\"R2 score  Test {}\".format(r2_score(y_true=y_test,y_pred=y_test_pred_lasso)))\nprint(\"Number of non-zero coefficents {}\".format(np.sum(lasso.coef_!=0)))\nprint(\"RMSE train {}\".format(np.sqrt(mean_squared_error(y_train,y_train_pred_lasso))))\nprint(\"RMSE test {}\".format(np.sqrt(mean_squared_error(y_test,y_test_pred_lasso))))","5a2f943e":"'''\n    List of coefficients obtained after using the ridge regression\n'''\nmodel_parameter_ridge = list(ridge.coef_)\nmodel_parameter_ridge.insert(0,ridge.intercept_)\ncols = X_train.columns\ncols.insert(0,'constant')\nridge_coef = pd.DataFrame(list(zip(cols,model_parameter_ridge)))\nridge_coef.columns = ['Feaure','Coef']\nridge_coef.sort_values(by='Coef', ascending=False).head(10)","66382fab":"'''\n    List of coefficients obtained after using the lasso regression\n'''\nmodel_parameter_lasso = list(lasso.coef_)\nmodel_parameter_lasso.insert(0,lasso.intercept_)\ncols = X_train.columns\ncols.insert(0,'constant')\nlasso_coef = pd.DataFrame(list(zip(cols,model_parameter_lasso)))\nlasso_coef.columns = ['Feaure','Coef']\nlasso_coef.sort_values(by='Coef',ascending=False).head(10)","052d29c6":"'''\n    Splitting the data into train and test data to start with the model creation.\n'''\n\ndf_train, df_test = train_test_split(df_final_1, train_size = 0.70, random_state = 100)\ndf_train.shape","6b1ee6d5":"df_test.shape","976642d1":"df_train.head()","34db5fbe":"df_num = np.array(df_final.select_dtypes(include=[np.number]).columns.values)\ndf_num","b82ed991":"'''\n    Scaling the test and the train data, using the Min Max scaler. So that everything is in the range 0 to 1\n'''\nscaler = MinMaxScaler()\ndf_train[df_num] = scaler.fit_transform(df_train[df_num])\ndf_train[df_num].describe()\ndf_test[df_num] = scaler.transform(df_test[df_num])","09d74db2":"'''\n    For creating the equation, we have separated the independent variable with the rest of the dependent variables.\n'''\ny_train = df_train.pop('SalePrice')\nX_train = df_train\n\ny_test = df_test.pop('SalePrice')\nX_test = df_test","1b464067":"'''\n    Here we have created the alpha parameter and then used the lasso_ridge_model function\n'''\nparams ={'alpha':[0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000]}\nmodel_ridge_1 = lasso_ridge_model(\"ridge\", params,'neg_mean_absolute_error',5,X_train,y_train)","32e8caeb":"'''\n    Here we have tried to tune the alpha parameter and then used the lasso_ridge_model function\n'''\nparams={'alpha':np.arange(0.2,1,0.01)}\nmodel_ridge_2 = lasso_ridge_model(\"ridge\",params,'neg_mean_absolute_error',5,X_train,y_train)","0f4c7fb0":"'''\n    using the best alpha value to fit the ridge model\n'''\nalpha = 0.2\nridge = Ridge(alpha=alpha)\n\nridge.fit(X_train, y_train)\nlen(ridge.coef_)","257ef5c1":"'''\n    R2 score obtained for the ridge regression model with alpha = 0.22, even though Ridge does not zero out the coefficients\n    but still there are 4 coefficicnets whose values are so small that they are considered as 0\n\n'''\ny_train_pred_ridge = ridge.predict(X_train)\ny_test_pred_ridge = ridge.predict(X_test)\nprint(\"R2 score  Train {}\".format(r2_score(y_true=y_train,y_pred=y_train_pred_ridge)))\nprint(\"R2 score  Test {}\".format(r2_score(y_true=y_test,y_pred=y_test_pred_ridge)))\nprint(\"Number of non-zero coefficents {}\".format(np.sum(ridge.coef_!=0)))\nprint(\"RMSE train {}\".format(np.sqrt(mean_squared_error(y_train,y_train_pred_ridge))))\nprint(\"RMSE test {}\".format(np.sqrt(mean_squared_error(y_test,y_test_pred_ridge))))","1e1a6bf6":"'''\n    Here we have created the alpha parameter and then used the lasso_ridge_model function\n'''\nparams ={'alpha':[0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000]}\nmodel_lasso_1 = lasso_ridge_model(\"lasso\", params,'',5,X_train,y_train)","59aa1979":"'''\n    Here we have tried to tune the alpha parameter and then used the lasso_ridge_model function\n'''\nparams = {'alpha' : np.arange(0,0.001,0.0001)}\nmodel_lasso_2 = lasso_ridge_model(\"lasso\", params,'',5,X_train,y_train)","760b3867":"'''\n    Here we have tried again to further tune the alpha parameter and then used the lasso_ridge_model function\n'''\nparams = {'alpha' : np.arange(0.0002,0.0005,0.00001)}\nmodel_lasso_3= lasso_ridge_model(\"lasso\", params,'',5,X_train,y_train)","a797e39b":"cv_results_lasso = pd.DataFrame(model_lasso_3.cv_results_)\ncv_results_lasso.head()","dc993be0":"'''\n    using the best alpha value to fit the lasso model\n'''\nalpha = 0.00023\nlasso = Lasso(alpha=alpha)\n\nlasso.fit(X_train, y_train)\nlen(lasso.coef_)","0db5b8a5":"'''\n    Value of score obtained for the lasso regression and the non zero coefficicnet are 111\n'''\ny_train_pred_lasso = lasso.predict(X_train)\ny_test_pred_lasso = lasso.predict(X_test)\n\nprint(\"R2 score  Train {}\".format(r2_score(y_true=y_train,y_pred=y_train_pred_lasso)))\nprint(\"R2 score  Test {}\".format(r2_score(y_true=y_test,y_pred=y_test_pred_lasso)))\nprint(\"Number of non-zero coefficents {}\".format(np.sum(lasso.coef_!=0)))\nprint(\"RMSE train {}\".format(np.sqrt(mean_squared_error(y_train,y_train_pred_lasso))))\nprint(\"RMSE test {}\".format(np.sqrt(mean_squared_error(y_test,y_test_pred_lasso))))","391e8f07":"'''\n    List of coefficients obtained after using the ridge regression\n'''\nmodel_parameter_ridge = list(ridge.coef_)\nmodel_parameter_ridge.insert(0,ridge.intercept_)\ncols = X_train.columns\ncols.insert(0,'constant')\nridge_coef = pd.DataFrame(list(zip(cols,model_parameter_ridge)))\nridge_coef.columns = ['Feaure','Coef']\nval1 = ridge_coef.sort_values(by='Coef', ascending=False).head(5)\nval2 = ridge_coef.sort_values(by='Coef', ascending=True).head(5)\n# top 5 features from lasso\nval1","50a2a941":"# bottom 5 features from lasso\nval2","9b35c8ab":"'''\n    List of coefficients obtained after using the lasso regression\n'''\nmodel_parameter_lasso = list(lasso.coef_)\nmodel_parameter_lasso.insert(0,lasso.intercept_)\ncols = X_train.columns\ncols.insert(0,'constant')\nlasso_coef = pd.DataFrame(list(zip(cols,model_parameter_lasso)))\nlasso_coef.columns = ['Feaure','Coef']\nval1 = lasso_coef.sort_values(by='Coef',ascending=False).head(5)\nval2 = lasso_coef.sort_values(by='Coef',ascending=True).head(5)\n# top 5 features from lasso\nval1","eaf7597f":"'''\n    bottom 5 features from lasso\n'''\nval2","3871a38e":"## Data Cleaning and preparation","b5d3a68d":"> since we have got alpha = 0.2 as a best paramter for now, we will further try to tune it from 1 to 3 using np.arange","aac7331c":"@Author: Tushar","12955929":"# Problem -\n- A US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to     purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has \n  collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.\n\n- The company is looking at prospective properties to buy to enter the market. You are required to build a regression model using \n  regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\n\n- The company wants to know:\n  Which variables are significant in predicting the price of a house, and How well those variables describe the price of a house. \n  Also, determine the optimal value of lambda for ridge and lasso regression.\n\n- The solution is divided into the following sections:\n\n- Data understanding and exploration\n- Data cleaning\n- Data preparation\n- Model building and evaluation using ridge and lasso","4b009e22":"The above graph clearly shows how the increase in the basement are causes increase in the sale price","2f0c3392":"### Outlier Treatment on numerical columns with no multicollinearity","631a098e":"### Ridge Regression","10a3d862":"### Multicollinearity check","288ea1de":"### Univariate Analysis","37bca7c8":"- even with 0.05 to 0.97 we are removing almost 42% of the records. Hence after having very less data,\n- it does not make much sense to remove most of it. So, i will not be performing outlier analysis on this.","6fda2ad7":"> info shows -:\n\n- Alley with only 91 records\n- FireplaceQu with 770 records\n- PoolQC with 7 records\n- Fence with 281 records\n- MiscFeature with 54 records\n- Further checking this in the data dictionary, i found out that all these columns are having NA as category.\n- So, Now we will be cleaning the data and putting NA as a categorical value in the data","d1949de1":"## Lasso Regression","324854e4":"The rooftype Hip is highest in demand, Hence is helping with the Sale Price. The second best roof type seems to be Gable","ad6b1b06":"## Splitting the data now with 70\/30 and checking for stability of the model","d4f5af1a":"### Bivariate Analysis","b23ca028":"In the above graph we can see that for the subcategory 60 we have the highest Sale Price, and we have more sale for category 20","f7b8878f":"> here we can see that the same range of LotArea is sold for different prices. which gives us an idea on the type of house or plot \nit has built.","6de82640":"here we can see that with increase in the MasVnrArea, we have increase in the Sale Price","5ed4786f":"### Model Building and Evaluation","6edf47ca":"The category WD, New are highest sale type when checked in comparison to Sale Price","cc2881a6":"we can see that 8 columns where the data was skewed has been removed form the dataframe.","41e2bbfd":"The Sale Price is highest for the Normal and Partial sale condition. which shows people are looking more into buying the plot for immediate use and it does not account for future investment.","9c0b3533":"Here it shows with a BsmtFinSf1 type we have quite increased price for the same value.","7e9e0017":"## Ridge Regression","f01eae0c":"- I have build using both 70\/30 and 80\/20 train\/test data. And it seems that the model generated are very stable.\n- Especially Lasso is the best out of two where we have recevied good training and test score for both the models and there is very less difference between the training and test score.\n- the optimal value of alpha for -:\n    - Ridge is 0.2\n    - Lasso is 0.00023","cb302b97":"The Lot shape for Reg, ad LR1 has the highest sale amongst the other categories. Where IR1 has the highest among the 2.","11a8f9ad":"### Lasso Regression","b45465a8":"People seems to be looking for an attached Grage type and Builtin garage type. Hence it has an impact in the sale of the plot","600f41b7":"> since we have got alpha = 0.2 as a best paramter for now, we will further try to tune it from 1 to 3 using np.arange`"}}