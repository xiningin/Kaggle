{"cell_type":{"bded7301":"code","ea791bb8":"code","ea9d334e":"code","eeb86025":"code","8ad6d365":"code","770cf723":"code","bb26126e":"code","e66ac0ef":"code","2e6fa241":"code","800cd1eb":"code","2de6323e":"code","2b617c57":"code","920552f8":"code","af25bf6b":"code","9198ee20":"code","087f3103":"code","50002635":"code","bb109252":"code","eeed82c7":"code","71fc0551":"code","7751e6ce":"code","a0664580":"code","db341c98":"code","221f72e0":"code","e3b1dfc7":"code","b7fa46c5":"code","1ccf8cf7":"code","acb17102":"code","dc193300":"code","bae08ee1":"code","3e9de2fe":"code","4f598465":"code","553057e2":"code","5c1bb863":"code","fb16eb47":"code","933885be":"code","6382aa9b":"code","ac86509d":"code","3bb37f23":"code","783062d4":"code","96c42685":"code","d35ed0c7":"code","f2b49434":"code","8a24d025":"code","7a7b97b7":"code","5ec1e3dd":"code","aed64497":"code","9ed03f9e":"code","670cdf74":"code","b8086308":"code","5db59162":"code","77d1b371":"code","1cc4dcfc":"code","e19873c3":"code","02fecd93":"code","60f4d569":"code","74156d5d":"code","e0ba7c03":"code","26237edc":"code","c34c23d5":"code","cfb6da95":"code","243c1433":"code","c50311a8":"code","e24bf7be":"code","b4c8d03d":"code","f2b05b82":"code","702d1c9d":"code","994c8f23":"code","17735f81":"code","35e867b4":"code","8f6b4516":"code","c2814611":"markdown","3d8cffe8":"markdown","49173ea0":"markdown","5e7cf214":"markdown","a443dc24":"markdown","a89a5a34":"markdown","abcefec0":"markdown","5323428b":"markdown","9cde907a":"markdown","089bf201":"markdown","6c19a2be":"markdown","29128168":"markdown","89a038f0":"markdown","b9f48f40":"markdown","a6254389":"markdown"},"source":{"bded7301":"from sklearn import model_selection","ea791bb8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nDIR = '\/kaggle\/input'","ea9d334e":"!ls -lh \/kaggle\/input\/quora-question-pairs\/","eeb86025":"!unzip \/kaggle\/input\/quora-question-pairs\/sample_submission.csv.zip","8ad6d365":"!unzip \/kaggle\/input\/quora-question-pairs\/train.csv.zip","770cf723":"!unzip \/kaggle\/input\/quora-question-pairs\/test.csv.zip","bb26126e":"!ls -lh","e66ac0ef":"df_sub = pd.read_csv(\"sample_submission.csv\")","2e6fa241":"df_sub.shape","800cd1eb":"#df_sub[df_sub.test_id.isin([1046690, 1461432, 379205, 817520, 943911, 1270024,  2345796])]","2de6323e":"df_test = pd.read_csv(\"test.csv\")","2b617c57":"df_sub.shape, df_test.shape","920552f8":"import numpy as np\ndf_test = df_test.replace(np.nan, 'nan', regex=True)","af25bf6b":"df_test.question1.isna().sum(), df_test.question2.isna().sum()","9198ee20":"df = pd.read_csv(\"train.csv\")\ndf['kfold'] = -1\n\ndf = df.sample(frac=1.,random_state=2021).reset_index(drop=True)\n\nkf = model_selection.StratifiedKFold(n_splits=5, shuffle=False)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X=df, y = df.is_duplicate.values)):\n    print(len(train_idx), len(val_idx))\n    df.loc[val_idx, 'kfold'] = fold","087f3103":"df.shape","50002635":"df[df.question1.isna()]","bb109252":"df[df.question2.isna()]","eeed82c7":"df.dropna(inplace=True)","71fc0551":"df.question1.isna().sum(), df.question2.isna().sum(), df.question1.isnull().sum(), df.question2.isnull().sum()","7751e6ce":"df.to_csv(\"train_folds.csv\", index=False)","a0664580":"df_fold = pd.read_csv(\"train_folds.csv\")","db341c98":"import tensorflow_hub as hub","221f72e0":"embed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\")","e3b1dfc7":"embeddings = embed([\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"I am a sentence for which I would like to get its embedding\"])\n\nembeddings","b7fa46c5":"import torch","1ccf8cf7":"#Reproducing same results\nSEED = 2021\n\n#Torch\ntorch.manual_seed(SEED)\n\n#Cuda algorithms\ntorch.backends.cudnn.deterministic = True  ","acb17102":"import torch.nn as nn","dc193300":"BATCH_SIZE = 256","bae08ee1":"import pytorch_lightning as pl","3e9de2fe":"\nFOLD_MAPPPING = {\n    0: [1, 2, 3, 4],\n    1: [0, 2, 3, 4],\n    2: [0, 1, 3, 4],\n    3: [0, 1, 2, 4],\n    4: [0, 1, 2, 3]\n}","4f598465":"FOLD = 0","553057e2":"train_df = df_fold[df_fold.kfold.isin(FOLD_MAPPPING.get(FOLD))].reset_index(drop=True)\nvalid_df = df_fold[df_fold.kfold==FOLD].reset_index(drop=True)","5c1bb863":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","fb16eb47":"class IsDuplicateAdv(nn.Module):\n    def __init__(self, output_dim: int, emb_dim: int, hid_dim=512):\n        \"\"\"Non Linear model\n        \"\"\"\n        super().__init__()\n        #dense layer\n        \n        self.batchnorm1 = nn.BatchNorm1d(emb_dim * 2)\n        self.dropout = nn.Dropout(p=0.2)\n        self.nonlinear = nn.PReLU()\n        \n        self.fc1 = nn.Linear(emb_dim * 2, hid_dim)\n        self.batchnorm2 = nn.BatchNorm1d(hid_dim)\n        self.fc2 = nn.Linear(hid_dim, output_dim)\n        \n        #activation function\n        self.act = nn.Sigmoid()\n        \n    def forward(self, text1:[str], text2:[str]):\n        \"\"\"\n        text1: list of strings from question1, len: batch_size\n        text2: list of strings from question2, len: batch_size\n        \"\"\"\n        \n        emb1 = embed(text1)\n        e1 = torch.from_numpy(emb1.numpy())\n        \n        emb2 = embed(text2)\n        e2 = torch.from_numpy(emb2.numpy())\n        \n        # merged\n        x = torch.cat((e1, e2), dim = 1)\n        x = self.batchnorm1(x)\n        \n        \n        x=self.fc1(x)\n        x = self.nonlinear(x)\n        x = self.dropout(x)\n        x = self.batchnorm2(x)\n        \n        x=self.fc2(x)\n\n        #Final activation function\n        outputs=self.act(x)\n        \n        return outputs","933885be":"import torch.optim as optim\ncriterion = nn.BCELoss()","6382aa9b":"#define metric\ndef binary_accuracy(preds, y):\n    #round predictions to the closest integer\n    rounded_preds = torch.argmax(preds, dim=1)\n    \n    correct = (rounded_preds == y).float() \n    acc = correct.sum() \/ len(correct)\n    return acc","ac86509d":"class QuoraQPair(pl.LightningModule):\n    def __init__(self, model: IsDuplicateAdv):\n        super().__init__()\n        self.model = model\n        \n    def forward(self, text1:[str], text2:[str]):\n        return self.model(text1, text2)\n    \n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n        return optimizer\n    \n    def training_step(self, batch, batch_idx: int):\n        q1, q2, label = batch['q1'], batch['q2'], batch['label'] \n        label = label.float()\n        predictions = self.model(q1, q2)\n        loss = criterion(predictions[:,1], label) \n        acc = binary_accuracy(predictions, label) \n        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n        return {'loss': loss, 'acc': acc}\n\n    \n    def training_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        avg_acc = torch.stack([x['acc'] for x in outputs]).mean()\n        self.log('avg_train_loss', avg_loss, on_epoch=True, sync_dist=True, prog_bar=False,logger=True,on_step=False)\n        self.log('avg_train_acc', avg_acc, on_epoch=True, sync_dist=True, prog_bar=False,logger=True,on_step=False)\n    \n    def validation_step(self, batch, batch_idx: int):\n        q1, q2, label = batch['q1'], batch['q2'], batch['label'] \n        label = label.float()\n        predictions = self.model(q1, q2)\n        loss = criterion(predictions[:,1], label) \n        acc = binary_accuracy(predictions, label) \n        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        self.log('valid_acc', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        return {'loss': loss, 'acc': acc}\n    \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        avg_acc = torch.stack([x['acc'] for x in outputs]).mean()\n        self.log('avg_val_loss', avg_loss, on_epoch=True, sync_dist=True, prog_bar=False,logger=True,on_step=False)\n        self.log('avg_val_acc', avg_acc, on_epoch=True, sync_dist=True, prog_bar=False,logger=True,on_step=False)\n        \n        ","3bb37f23":"from torch.utils.data import DataLoader, Dataset","783062d4":"class QuoraTrainData(Dataset):\n    def __init__(self, df: pd.DataFrame):\n        self.df = df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        q1 = self.df.iloc[idx].question1\n        q2 = self.df.iloc[idx].question2\n        label = self.df.iloc[idx].is_duplicate\n        \n        return {\"q1\": q1, \"q2\": q2, \"label\": label}","96c42685":"class QuoraTestData(Dataset):\n    def __init__(self, df: pd.DataFrame):\n        self.df = df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        q1 = self.df.iloc[idx].question1\n        q2 = self.df.iloc[idx].question2\n        \n        return {\"q1\": q1, \"q2\": q2}","d35ed0c7":"class QuoraQPairDataModule(pl.LightningDataModule):\n    def __init__(self, train_df:pd.DataFrame, valid_df: pd.DataFrame, batch_size:int):\n        super().__init__()\n        self.batch_size = batch_size\n        self.train_df = train_df\n        self.valid_df = valid_df\n        \n    def prepare_data(self):\n        # any data downloading \/ preprocessing\n        pass\n    \n    def setup(self, stage=None):\n        # setup torch dataset\n        self.train_dataset = QuoraTrainData(self.train_df)\n        self.valid_dataset = QuoraTrainData(self.valid_df)\n    \n    def train_dataloader(self):\n        train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n        return train_loader\n    \n    def val_dataloader(self):\n        valid_loader = DataLoader(self.valid_dataset, batch_size=self.batch_size, num_workers=4)\n        return valid_loader\n    \n    def test_dataloader(self):\n        pass","f2b49434":"from pytorch_lightning import Callback\nclass MetricsCallback(Callback):\n    \"\"\"PyTorch Lightning metric callback.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.metrics = {\"train\": [], \"val\": []}\n\n    def on_validation_end(self, trainer, pl_module):\n        self.metrics[\"val\"].append(trainer.logged_metrics)\n    \n    def on_train_end(self, trainer, pl_module):\n        self.metrics[\"train\"].append(trainer.logged_metrics)","8a24d025":"net = IsDuplicateAdv(output_dim=2, emb_dim=512)\nmodel = QuoraQPair(net)\ndm = QuoraQPairDataModule(train_df, valid_df, BATCH_SIZE)","7a7b97b7":"from pytorch_lightning.loggers import CSVLogger","5ec1e3dd":"# logger\nimport os\ncsvlogger = CSVLogger(\n    save_dir=os.getcwd(),\n    name=\"exp_logs\"\n)\nos.getcwd()","aed64497":"metrics_callback = MetricsCallback()","9ed03f9e":"trainer = pl.Trainer(max_epochs=5,\n                     default_root_dir=os.getcwd(),\n                     logger=csvlogger,\n                     deterministic=True) # callbacks = [metrics_callback]","670cdf74":"trainer.fit(model, dm)","b8086308":"ls -lh exp_logs\/","5db59162":"df_metrics = pd.read_csv('exp_logs\/version_0\/metrics.csv')","77d1b371":"df_metrics","1cc4dcfc":"df_metrics_val = df_metrics[[\"avg_val_loss\", \"avg_val_acc\", \"epoch\"]].dropna()\ndf_metrics_train = df_metrics[[\"avg_train_loss\", \"avg_train_acc\", \"epoch\"]].dropna()","e19873c3":"df_metrics_val.avg_val_loss.values","02fecd93":"%matplotlib inline\nfrom matplotlib import pyplot as plt","60f4d569":"plt.plot(df_metrics_train.avg_train_loss.values, label=\"train\")\nplt.plot(df_metrics_val.avg_val_loss.values, label=\"val\")\nplt.title(\"Loss vs Epoch\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.grid(alpha=0.3)\nplt.legend()\nplt.show()","74156d5d":"plt.plot(df_metrics_train.avg_train_acc.values, label=\"train\")\nplt.plot(df_metrics_val.avg_val_acc.values, label=\"val\")\nplt.title(\"Accuracy vs Epochs\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.grid(alpha=0.3)\nplt.legend()\nplt.show()","e0ba7c03":"df_test.head()","26237edc":"test_dataset = QuoraTestData(df_test)\ntest_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=4)","c34c23d5":"predictions = np.zeros(df_test.shape[0])\nmodel_inference = model.model\nmodel_inference.eval()","cfb6da95":"test_iter = iter(test_loader)\ntres = test_iter.next()","243c1433":"df_test.head()","c50311a8":"temp = model_inference(tres['q1'], tres['q2'])[:,1].detach().cpu().numpy()\ntemp","e24bf7be":"predictions","b4c8d03d":"from tqdm import tqdm","f2b05b82":"for ind, batch in tqdm(enumerate(test_loader), total=len(test_loader)):\n    p = model_inference(batch['q1'], batch['q2'])[:,1].detach().cpu().numpy()\n    predictions[ind * 1024:(ind + 1) * 1024] = p","702d1c9d":"predictions.shape","994c8f23":"s = pd.DataFrame({'test_id': df_test['test_id'].values, 'is_duplicate': predictions})","17735f81":"s.head()","35e867b4":"s.to_csv(\"submission.csv\", index=False)","8f6b4516":"#s.shape\n#s[s.test_id.isin([1128118])]\n#df_sub.shape\n#df_sub.head()\n#df_sub[df_sub.test_id.isin([1128118,1128119 ])]","c2814611":"## Fix `nan` issue in `train` data","3d8cffe8":"# Load Universal Sentence Encode","49173ea0":"## Create validation dataset","5e7cf214":"## Data Cleaning and preparation","a443dc24":"# Quora Duplicate Questions Detection\n\n- This version is a refactored version of [this](https:\/\/www.kaggle.com\/sankarshan7\/quora-duplicate-question?scriptVersionId=51988650) original kernel\n- The [orignal](https:\/\/www.kaggle.com\/sankarshan7\/quora-duplicate-question?scriptVersionId=51988650) `pytorch` kernel is refactored to integrate with `PyTorchLightning` \n- Some `PytorchLightning` refactoring style has been taken from this kernel: [Lish-moa baseline approach by Adrew Lukyanenko](https:\/\/www.kaggle.com\/artgor\/lish-moa-baseline-approach\/notebook#Data-exploration)","a89a5a34":"# Data Module\n\n- [pl.DataModule Official Document](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/datamodules.html)\n- [How to use it in real case - Kaggle MoA Prediction by Andrew Lukyanenko](https:\/\/www.kaggle.com\/artgor\/lish-moa-baseline-approach)","abcefec0":"# How to log metrics properly using PyTorchLightning\n\n- [Why are losses different when logging from '_step' (with on_epoch=True) compared to logging from '_epoch_end'? #5539](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/5539)\n- [Understanding different values of training\/validation loss in callback_metrics dictionary](https:\/\/forums.pytorchlightning.ai\/t\/understanding-different-values-of-training-validation-loss-in-callback-metrics-dictionary\/568)","5323428b":"# Network Architecture\n\n- Before integrating with the `pytorchlightning`, let's desing the network in vanilla pytorch\n\n- The original architecture idea came from [here](https:\/\/www.linkedin.com\/pulse\/duplicate-quora-question-abhishek-thakur\/). But the original architecture is heavily simplified with the use of transfer learning using `Universal Sentence Encoder`\n\n<center>\n<img src='https:\/\/raw.githubusercontent.com\/msank00\/Kaggle_202101_Quora_Duplicate_Questions\/main\/images\/NN_Architecture.jpg' width='400'>    \n<\/center>","9cde907a":"# Prepare test data","089bf201":"**NOTE**\n\n- Set `on_step=False` for better logging ","6c19a2be":"# Training performance","29128168":"# Inference","89a038f0":"## Fix `nan` issue in Test data","b9f48f40":"## Wrap vanila `pytorch` network with `pytorchlightning`","a6254389":"## Set logger for accessing training history\n\n- [PyTorch CSVLOgger](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/generated\/pytorch_lightning.loggers.CSVLogger.html)"}}