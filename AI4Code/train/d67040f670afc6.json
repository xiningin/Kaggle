{"cell_type":{"416b57eb":"code","372e641d":"code","a2cce5e4":"code","6b6d00a0":"code","ac6996b5":"code","58f64f66":"code","aba422f0":"code","1533de3d":"code","fd83773b":"code","dc7634af":"code","a6458eba":"code","b68a2682":"code","4952254a":"code","7647b7b7":"code","3b798ecc":"code","2153eba5":"code","ac55dafa":"code","869d9f3d":"code","af6f9cbd":"code","e9fa07c9":"code","a1e04252":"code","af3982ed":"code","6928a2dc":"code","ecab280a":"code","9cf0af7e":"code","8b1dea41":"code","727e7ca6":"code","cef4dec8":"code","4c305838":"code","29eb245f":"code","af8a2a55":"code","18e09739":"code","7dbbd2d2":"code","78cc44e0":"code","5826ba0f":"code","e931100c":"code","f663d481":"code","d14d8977":"code","13790c07":"code","08799527":"code","210e23fa":"code","ca85f387":"code","ccc7eb66":"code","073783b9":"code","fa355ac2":"code","f9ab8e60":"code","160dfebd":"code","41249770":"code","52363a7a":"code","9b08a1c8":"code","d552abd0":"code","be19d684":"code","e64b48b8":"code","f512fae8":"code","199d1eae":"code","c440c78a":"code","3926cf94":"code","20761718":"code","c696cc3c":"markdown","0b23be7f":"markdown","f07def0a":"markdown","85bb48fe":"markdown","4e4741eb":"markdown","71324e96":"markdown","93bba4f4":"markdown","c9f68760":"markdown","d20fedf0":"markdown","be505f4f":"markdown","527e48a0":"markdown","93f5f579":"markdown","fb80f13d":"markdown","4570350b":"markdown","cf9325eb":"markdown","23e4a214":"markdown","4137891d":"markdown","38677647":"markdown","553caf40":"markdown","9764491d":"markdown","347bbb96":"markdown","ca930131":"markdown"},"source":{"416b57eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\n# remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')","372e641d":"train = pd.read_csv('..\/input\/train.csv',index_col='Id')\ntest  = pd.read_csv('..\/input\/test.csv',index_col='Id')","a2cce5e4":"print(train.shape)\ndisplay(train.head(1))\n\nprint(test.shape)\ndisplay(test.head(1))","6b6d00a0":"plt.style.use(style='ggplot')\nplt.rcParams['figure.figsize'] = (10, 6)","ac6996b5":"print (\"Skew is:\", train.SalePrice.skew())\nplt.hist(train.SalePrice, color='blue')\nplt.show()","58f64f66":"target = np.log(train.SalePrice)\nprint (\"Skew is:\", target.skew())\nplt.hist(target, color='blue')\nplt.show()","aba422f0":"numeric_features = train.select_dtypes(include=[np.number])#getting numeric columns","1533de3d":"corr = numeric_features.corr()\n\nprint (corr['SalePrice'].sort_values(ascending=False)[1:11], '\\n')\nprint (corr['SalePrice'].sort_values(ascending=False)[-10:])","fd83773b":"train.OverallQual.unique()#it means it is rating of some sort","dc7634af":"def pivotandplot(data, variable, onVariable, aggfunc):\n    pivot_var = data.pivot_table(index = variable,\n                                values = onVariable,\n                                aggfunc= aggfunc)\n    pivot_var.plot(kind='bar', color='blue')\n    plt.xlabel(variable)\n    plt.ylabel(onVariable)\n    plt.xticks(rotation=0)\n    plt.show()","a6458eba":"pivotandplot(train, 'OverallQual', 'SalePrice', np.median)\n#it shows a increasing trend ","b68a2682":"# It is a continous variable and hence lets look at the relationship of GrLivArea with SalePrice using a Regression plot\n\n_ = sns.regplot(train['GrLivArea'], train['SalePrice'])","4952254a":"train=train.drop(train[(train['GrLivArea']>4000)&(train['SalePrice']<300000)].index)\nsns.regplot(train['GrLivArea'], train['SalePrice'])","7647b7b7":"sns.regplot(train['GarageArea'], train['SalePrice'])","3b798ecc":"train = train[train['GarageArea'] < 1200]\nsns.regplot(train['GarageArea'], train['SalePrice'])","2153eba5":"train['log_SalePrice']=np.log(train['SalePrice']+1)\nsaleprices=train[['SalePrice','log_SalePrice']]#making of dataframe \n\nsaleprices.head(5)","ac55dafa":"train=train.drop(columns=['SalePrice','log_SalePrice'])","869d9f3d":"all_data = pd.concat((train, test))\nprint(all_data.shape)","af6f9cbd":"type(all_data.isnull().sum().sort_values(ascending = False))","e9fa07c9":"null_data = pd.DataFrame(all_data.isnull().sum().sort_values(ascending=False))[:34]#to use only first 34 rows as a dataframe\n\nnull_data.columns = ['Null Count']\nnull_data.index.name = 'Feature'\nnull_data","a1e04252":"len(all_data)","af3982ed":"(null_data\/len(all_data)) * 100","6928a2dc":"train.MiscFeature.unique()","ecab280a":"for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', \n            'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSSubClass'):\n    all_data[col] = all_data[col].fillna('None')","9cf0af7e":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n    all_data[col] = all_data[col].fillna(0)","8b1dea41":"for col in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Functional', 'Utilities'):\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])","727e7ca6":"figure, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\nfigure.set_size_inches(14,10)\n_ = sns.regplot(train['TotalBsmtSF'], saleprices['SalePrice'], ax=ax1)\n_ =sns.regplot(train['1stFlrSF'], saleprices['SalePrice'], ax=ax2)\n_ = sns.regplot(train['2ndFlrSF'], saleprices['SalePrice'], ax=ax3)\n_ = sns.regplot(train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF'], saleprices['SalePrice'], ax=ax4)","cef4dec8":"#Impute the entire data set\nall_data['TotalSF']=all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\n#Let's add two new variables for No 2nd floor and no basement\nall_data['No2ndFlr']=(all_data['2ndFlrSF']==0)\nall_data['NoBsmt']=(all_data['TotalBsmtSF']==0)","4c305838":"figure, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\nfigure.set_size_inches(14,10)\n_ = sns.barplot(train['BsmtFullBath'], saleprices['SalePrice'], ax=ax1)\n_ = sns.barplot(train['FullBath'], saleprices['SalePrice'], ax=ax2)\n_ = sns.barplot(train['BsmtHalfBath'], saleprices['SalePrice'], ax=ax3)\n_ = sns.barplot(train['BsmtFullBath'] + train['FullBath'] + train['BsmtHalfBath'] + train['HalfBath'], saleprices['SalePrice'], ax=ax4)","29eb245f":"all_data['TotalBath']=all_data['BsmtFullBath'] + all_data['FullBath'] + all_data['BsmtHalfBath'] + all_data['HalfBath']","af8a2a55":"all_data['YrBltAndRemod']=all_data['YearBuilt']+all_data['YearRemodAdd']","18e09739":"# treat some numeric values as str which is actually a categorical data\nall_data['MSSubClass']=all_data['MSSubClass'].astype(str)\nall_data['MoSold']=all_data['MoSold'].astype(str)\nall_data['YrSold']=all_data['YrSold'].astype(str)","7dbbd2d2":"all_data['NoLowQual']=(all_data['LowQualFinSF']==0)\nall_data['NoOpenPorch']=(all_data['OpenPorchSF']==0)\nall_data['NoWoodDeck']=(all_data['WoodDeckSF']==0)\nall_data['NoGarage']=(all_data['GarageArea']==0)\nall_data=all_data.drop(columns=['PoolArea','PoolQC']) # most of the houses has no pools. \nall_data=all_data.drop(columns=['MiscVal','MiscFeature']) # most of the houses has no misc feature.","78cc44e0":"all_data.shape","5826ba0f":"Basement = ['BsmtCond', 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', 'BsmtUnfSF','TotalBsmtSF']\nBsmt=all_data[Basement]\nBsmt.head()","e931100c":"from sklearn.preprocessing import LabelEncoder\ncond_encoder = LabelEncoder()\nBsmt['BsmtCond']=cond_encoder.fit_transform(Bsmt['BsmtCond'])\n\nexposure_encoder = LabelEncoder()\nBsmt['BsmtExposure'] = exposure_encoder.fit_transform(Bsmt['BsmtExposure'])\n\nfinTyp1_encoder = LabelEncoder()\nBsmt['BsmtFinType1'] = finTyp1_encoder.fit_transform(Bsmt['BsmtFinType1'])\n\nfinTyp2_encoder = LabelEncoder()\nBsmt['BsmtFinType2'] = finTyp2_encoder.fit_transform(Bsmt['BsmtFinType2'])\n\nqual_encoder = LabelEncoder()\nBsmt['BsmtQual'] = qual_encoder.fit_transform(Bsmt['BsmtQual'])","f663d481":"Bsmt.head(10)","d14d8977":"all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median()))","13790c07":"Bsmt['BsmtScore']= Bsmt['BsmtQual']  * Bsmt['BsmtCond'] * Bsmt['TotalBsmtSF']\nall_data['BsmtScore']=Bsmt['BsmtScore']","08799527":"Bsmt['BsmtFin'] = (Bsmt['BsmtFinSF1'] * Bsmt['BsmtFinType1']) + (Bsmt['BsmtFinSF2'] * Bsmt['BsmtFinType2'])\nall_data['BsmtFinScore']=Bsmt['BsmtFin']\nall_data['BsmtDNF']=(all_data['BsmtFinScore']==0)","210e23fa":"garage=['GarageArea','GarageCars','GarageCond','GarageFinish','GarageQual','GarageType','GarageYrBlt']\nGarage=all_data[garage]\n\ngarcond_encoder = LabelEncoder()\nGarage['GarageCond'] = garcond_encoder.fit_transform(Garage['GarageCond'])\n\ngarfin_encoder = LabelEncoder()\nGarage['GarageFinish'] = garfin_encoder.fit_transform(Garage['GarageFinish'])\n\ngarqual_encoder = LabelEncoder()\nGarage['GarageQual'] = garqual_encoder.fit_transform(Garage['GarageQual'])\n\ngartyp_encoder = LabelEncoder()\nGarage['GarageType'] = gartyp_encoder.fit_transform(Garage['GarageType'])","ca85f387":"Garage['GarageScore']=(Garage['GarageArea']) * (Garage['GarageCars']) * (Garage['GarageFinish'])*(Garage['GarageQual']) *(Garage['GarageType'])\nall_data['GarageScore']=Garage['GarageScore']","ccc7eb66":"non_numeric=all_data.select_dtypes(exclude=[np.number, bool])\nnon_numeric.head()","073783b9":"def onehot(col_list):\n    global all_data\n    while len(col_list) !=0:\n        col=col_list.pop(0)\n        data_encoded=pd.get_dummies(all_data[col], prefix=col)\n        all_data=pd.merge(all_data, data_encoded, on='Id')\n        all_data=all_data.drop(columns=col)\n    print(all_data.shape)","fa355ac2":"onehot(list(non_numeric))","f9ab8e60":"def log_transform(col_list):\n    transformed_col=[]\n    while len(col_list)!=0:\n        col=col_list.pop(0)\n        if all_data[col].skew() > 0.5:\n            all_data[col]=np.log(all_data[col]+1)\n            transformed_col.append(col)\n        else:\n            pass\n    print(f\"{len(transformed_col)} features had been tranformed\")\n    print(all_data.shape)","160dfebd":"numeric=all_data.select_dtypes(include=np.number)\nlog_transform(list(numeric))","41249770":"train=all_data[:len(train)]\ntest=all_data[len(train):]","52363a7a":"# loading pakages for model. \nfrom sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn import linear_model, model_selection, ensemble, preprocessing\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.neighbors import KNeighborsRegressor\nimport xgboost as xgb\n\n#Evaluation Metrics\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, r2_score,mean_absolute_error","9b08a1c8":"def rmse(predict, actual):\n    score =mean_squared_error(Ytrain, y_pred)**0.5\n    return score\nrmse_score = make_scorer(rmse)\nrmse_score","d552abd0":"feature_names=list(all_data)\nXtrain=train[feature_names]\nXtest=test[feature_names]\nYtrain=saleprices['log_SalePrice']","be19d684":"def score(model):\n    score = cross_val_score(model, Xtrain, Ytrain, cv=5, scoring=rmse_score).mean()\n    return score","e64b48b8":"scores = {}","f512fae8":"forest_reg = RandomForestRegressor(random_state=42)\nforest_reg.fit(Xtrain, Ytrain)\n\nforest_reg.fit(Xtrain,Ytrain)\ny_pred = forest_reg.predict(Xtrain)\n\nprint('')\nprint('####### RandomForest Regression #######')\nmeanCV = score(forest_reg)\nprint('Mean CV Score : %.4f' % meanCV)\n\n\nmse = mean_squared_error(Ytrain,y_pred)\nmae = mean_absolute_error(Ytrain, y_pred)\nrmse = mean_squared_error(Ytrain, y_pred)**0.5\nr2 = r2_score(Ytrain, y_pred)\nscores.update({'RandomForest':[meanCV,mse,mae,rmse,r2]})\n\nprint('')\nprint('MSE(RSS)    : %0.4f ' % mse)\nprint('MAE         : %0.4f ' % mae)\nprint('RMSE        : %0.4f ' % rmse)\nprint('R2          : %0.4f ' % r2)","199d1eae":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n  \n    {'n_estimators': [70,100], 'max_features': [150]},\n   \n    {'bootstrap': [True], 'n_estimators': [70,100], 'max_features': [150]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(Xtrain, Ytrain)\n\nprint('')\nprint('####### GridSearch RF Regression #######')\nmeanCV = score(grid_search)\nprint('Mean CV Score : %.4f' % meanCV)\n\n\nmse = mean_squared_error(Ytrain,y_pred)\nmae = mean_absolute_error(Ytrain, y_pred)\nrmse = mean_squared_error(Ytrain, y_pred)**0.5\nr2 = r2_score(Ytrain, y_pred)\nscores.update({'GridSearchRF':[meanCV,mse,mae,rmse,r2]})\n\nprint('')\nprint('MSE(RSS)    : %0.4f ' % mse)\nprint('MAE         : %0.4f ' % mae)\nprint('RMSE        : %0.4f ' % rmse)\nprint('R2          : %0.4f ' % r2)","c440c78a":"model_GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =42)\n\nmodel_GBoost.fit(Xtrain,Ytrain)\ny_pred = model_GBoost.predict(Xtrain)\n\nprint('')\nprint('####### GradientBoosting Regression #######')\nmeanCV = score(model_GBoost)\nprint('Mean CV Score : %.4f' % meanCV)\n\n\nmse = mean_squared_error(Ytrain,y_pred)\nmae = mean_absolute_error(Ytrain, y_pred)\nrmse = mean_squared_error(Ytrain, y_pred)**0.5\nr2 = r2_score(Ytrain, y_pred)\nscores.update({'GradientBoosting':[meanCV,mse,mae,rmse,r2]})\n\nprint('')\nprint('MSE(RSS)    : %0.4f ' % mse)\nprint('MAE         : %0.4f ' % mae)\nprint('RMSE        : %0.4f ' % rmse)\nprint('R2          : %0.4f ' % r2)","3926cf94":"GBoost_Predictions=np.exp(model_GBoost.predict(Xtest))-1\n","20761718":"output = pd.DataFrame({'Id': Xtest.index,\n                       'SalePrice': GBoost_Predictions})\noutput.to_csv('submission.csv', index=False)","c696cc3c":"Impute Categorical data for missing values and relace by 'None'**","0b23be7f":"Impute the numerical features and replace with a value of zero","f07def0a":"The regression plot clearly indicates that rices increase with increase in GrLivArea however there are some outliers where for GrLivArea above 4000 the prices are below 20k Let's Remove the outliers","85bb48fe":"Check the top 10 and Bottom 10 correlated and non-corelate features respectively","4e4741eb":"The BsmtFullBath ,FullBath, BsmtHalfBath can be combined for a TotalBath similar to TotalSF","71324e96":"## Analyzing the highest correlated feature\n## OVERLLQUAL","93bba4f4":"## GrLivArea\n1. Analyze the \n**GrLivArea: Above grade (ground) living area square feet** - Second highest correlation with SalePrice","c9f68760":"Some Outliers after GarageArea of 1200 Remove the outliers","d20fedf0":"## GarageArea","be505f4f":"Merge train and test data\nLet's find the percentage of missing values for the features","527e48a0":"## RandomForest Regressor","93f5f579":"TotalBsmtSF - Total Basement Square Feet\n1stFlrSF - First Floor Square Feet\n2ndFlrSF - Second Floor Square Feet\nAll the above three feature define area of the house and we can easily combine these to form TotalSF - Total Area in square feet","fb80f13d":"## Modelling","4570350b":"## Load The Data","cf9325eb":"## Gradient Boosting Regressor","23e4a214":"Let's check how the Overall Quality rating affects the median price of the house","4137891d":"             99% of Pool Quality Data is missing.In the case of PoolQC, the column refers to Pool Quality. Pool quality is NaN when PoolArea is 0, or there is no pool.\n    Similar is case for Garage column\n>     But what are the 96% missing Miscelleanous features ?","38677647":"## Data Preprocessesing","553caf40":"## New Features","9764491d":"## Grid Search for finding best params for RandomForest","347bbb96":" ## Impute the Data for missing values","ca930131":"By taking log of SalePrice the skew reduces to a larger extent"}}