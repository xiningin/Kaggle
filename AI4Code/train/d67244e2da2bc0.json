{"cell_type":{"395bcefe":"code","aaeaaee3":"code","1bf4204b":"code","0fd4c69e":"code","0fbb0263":"code","11070843":"code","0104ee26":"code","e70f0112":"code","89c6a82d":"code","2f51019c":"code","ee6eded2":"code","bd0ba1eb":"code","73c024e9":"code","e25c64de":"code","f1f46214":"code","e2b0d34c":"code","bcdac9f0":"code","4429325c":"code","06aae862":"code","d499fce0":"code","18678f9b":"code","a9cf7373":"code","730c4eb9":"code","eec4058c":"code","2638edcf":"code","365d1135":"code","a92e4070":"code","b2eabf05":"code","fe0b8ebf":"code","c0ff6395":"code","2a895bc8":"code","655e35c3":"code","2ffa86f8":"code","def9f28d":"code","ef95baa9":"markdown","6237766f":"markdown","73aa4cba":"markdown","38a095a5":"markdown","e74f26df":"markdown","969b9d80":"markdown","7a06b906":"markdown","8d9293f7":"markdown","9e5ac2be":"markdown","0ce85692":"markdown","60066c20":"markdown","ea5fe1e4":"markdown","21566ebb":"markdown","49303906":"markdown","dbf04ffc":"markdown","5a38d612":"markdown","a77ede03":"markdown","4052924b":"markdown"},"source":{"395bcefe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aaeaaee3":"data = pd.read_csv('\/kaggle\/input\/customer-segmentation\/Train.csv')\ndata.head()","1bf4204b":"data.info()","0fd4c69e":"plot_data = data.groupby('Segmentation')['Segmentation'].agg(['count']).reset_index()\n\nfig = px.pie(plot_data, values = plot_data['count'], names = plot_data['Segmentation'])\n\nfig.update_traces(textposition = 'inside', textinfo = 'percent + label', hole = 0.5, \n                  marker = dict(colors = ['#2A3132','#336B87'], line = dict(color = 'white', width = 2)))\n\nfig.update_layout(title_text = 'Customer<br>Segmentation', title_x = 0.5, title_y = 0.55, title_font_size = 26, \n                  title_font_family = 'Calibri', title_font_color = 'black', showlegend = False)\n                  \nfig.show()","0fbb0263":"def plot_category(feature, figsize=None):\n    A_count = data[data['Segmentation']=='A'].groupby([feature]).size()\n    B_count = data[data['Segmentation']=='B'].groupby([feature]).size()\n    C_count = data[data['Segmentation']=='C'].groupby([feature]).size()\n    D_count = data[data['Segmentation']=='D'].groupby([feature]).size()\n    labels = A_count.index\n\n    x = np.arange(len(labels)) # the label locations\n    width = 0.7  # the width of the bars\n\n    if figsize:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig, ax = plt.subplots()\n    rects1 = ax.bar(x-width\/3, round(A_count*100\/data.groupby([feature]).size(), 2), \n                    width\/5, label='A')\n    rects2 = ax.bar(x-width\/8, round(B_count*100\/data.groupby([feature]).size(), 2), \n                    width\/5, label='B')\n    rects3 = ax.bar(x+width\/8, round(C_count*100\/data.groupby([feature]).size(), 2), \n                    width\/5, label='C')\n    rects4 = ax.bar(x+width\/3, round(D_count*100\/data.groupby([feature]).size(), 2), \n                    width\/5, label='D')\n\n    ax.set_ylabel('Count')\n    ax.set_title('Based on %s'%feature)\n    ax.set_xticks(x)\n    ax.set_xticklabels(labels, rotation=80)\n    ax.legend(loc=0, bbox_to_anchor=(1, 1));\n\n    ax.bar_label(rects1, padding=1)\n    ax.bar_label(rects2, padding=1)\n    ax.bar_label(rects3, padding=1)\n    ax.bar_label(rects4, padding=1)\n\n    fig.tight_layout()\n    plt.show()\n    \ndef plot_numerical(feature, figsize=None):\n    fig = plt.figure(figsize=(10,6))\n\n    sns.kdeplot(data[data['Segmentation']=='A'][feature])\n    sns.kdeplot(data[data['Segmentation']=='B'][feature])\n    sns.kdeplot(data[data['Segmentation']=='C'][feature])\n    sns.kdeplot(data[data['Segmentation']=='D'][feature])\n\n    fig.legend(labels=['Segmentation A', 'Segmentation B', 'Segmentation C', 'Segmentation D'])\n    plt.title('Based on %s'%feature)\n    plt.show()\n    \ndef plot_pie(feature):\n    plot_data = data.groupby([feature, 'Segmentation'])[feature].agg({'count'}).reset_index()\n\n    fig = px.sunburst(plot_data, path = [feature, 'Segmentation'], values = 'count', color = feature, \n                      title = 'Affect of %s on Customer Segmentation'%feature, width = 600, height = 600)\n\n    fig.update_layout(plot_bgcolor = 'white', title_font_family = 'Calibri Black', title_font_color = '#221f1f', \n                      title_font_size = 22, title_x = 0.5)\n\n    fig.update_traces(textinfo = 'label + percent parent')\n    fig.show()","11070843":"for feature in ['Gender', 'Ever_Married', 'Graduated', 'Spending_Score']:\n    plot_pie(feature)","0104ee26":"for feature in ['Profession', 'Var_1']:\n    plot_pie(feature)","e70f0112":"for feature in ['Age', 'Work_Experience', 'Family_Size']:\n    plot_numerical(feature)","89c6a82d":"categorical_features = ['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score', 'Var_1']\nnumerical_features = ['Age', 'Work_Experience', 'Family_Size']\n\nto_drop = ['ID'] # contain unique values","2f51019c":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport os\nimport joblib","ee6eded2":"df = data.copy()\npath = '\/kaggle\/working'\nfor i, feature in enumerate(categorical_features):\n    le = LabelEncoder()\n\n    # create directory to save label encoding models\n    if not os.path.exists(os.path.join(path, \"TextEncoding\")):\n        os.makedirs(os.path.join(path, \"TextEncoding\"))\n\n    # perform label encoding\n    le.fit(df[feature])\n    \n    # save the encoder\n    joblib.dump(le, open(os.path.join(path, \"TextEncoding\/le_{}.sav\".format(feature)), 'wb'))\n    \n    # transfrom training data\n    df[feature] = le.transform(df[feature])\n\n    # get classes & remove first column to elude from dummy variable trap\n    columns = list(map(lambda x: feature+' '+str(x), list(le.classes_)))[1:]\n    \n    # save classes\n    joblib.dump(columns, \n                open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'wb'))","bd0ba1eb":"plt.figure(figsize=(5, 2))\nsns.heatmap(round(data[numerical_features].corr(), 2), annot=True,\n            mask=None, cmap='GnBu')\ncorr_mat = data[numerical_features].corr()\nplt.show()","73c024e9":"plt.figure(figsize=(10, 6))\nsns.heatmap(round(df[categorical_features+numerical_features].corr(method='spearman'), 2), annot=True,\n            mask=None, cmap='GnBu')\nplt.show()","e25c64de":"from statsmodels.stats.outliers_influence import variance_inflation_factor","f1f46214":"# Calculating VIF\nvif = pd.DataFrame()\ntemp = df.dropna()\nvif[\"variables\"] = [feature for feature in categorical_features+numerical_features if feature not in ['Age', 'Var_1']]\nvif[\"VIF\"] = [variance_inflation_factor(temp[vif['variables']].values, i) for i in range(len(vif[\"variables\"]))]\nprint(vif)","e2b0d34c":"missingValueFeatures = pd.DataFrame({'missing %': data.isnull().sum()*100\/len(data)})\nmissingValueFeatures[missingValueFeatures['missing %']>0]","bcdac9f0":"data_missing = data.dropna().reset_index(drop=True)","4429325c":"NumericData = data_missing[[feature for feature in numerical_features if feature not in []]]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","06aae862":"# Percentage of outliers present in each variable\noutlier_percentage = {}\nfor feature in numerical_features:\n    tempData = data_missing.sort_values(by=feature)[feature]\n    Q1, Q3 = tempData.quantile([0.25, 0.75])\n    IQR = Q3 - Q1\n    Lower_range = Q1 - (1.5 * IQR)\n    Upper_range = Q3 + (1.5 * IQR)\n    outlier_percentage[feature] = round((((tempData<(Q1 - 1.5 * IQR)) | (tempData>(Q3 + 1.5 * IQR))).sum()\/tempData.shape[0])*100,2)\noutlier_percentage","d499fce0":"df = data_missing.copy()\npath = '\/kaggle\/working'\nfor i, feature in enumerate(categorical_features):\n    \n    le = LabelEncoder()\n    ohe = OneHotEncoder(sparse=False)\n\n    # create directory to save label encoding models\n    if not os.path.exists(os.path.join(path, \"TextEncoding\")):\n        os.makedirs(os.path.join(path, \"TextEncoding\"))\n\n    # perform label encoding\n    le.fit(df[feature])\n    # save the encoder\n    joblib.dump(le, open(os.path.join(path, \"TextEncoding\/le_{}.sav\".format(feature)), 'wb'))\n    \n    # transfrom training data\n    df[feature] = le.transform(df[feature])\n\n    # get classes & remove first column to elude from dummy variable trap\n    columns = list(map(lambda x: feature+' '+str(x), list(le.classes_)))[1:]\n    \n    # save classes\n    joblib.dump(columns, \n                open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'wb'))\n    # load classes\n    columns = joblib.load(\n        open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'rb'))\n\n    if len(le.classes_)>2:\n        # perform one hot encoding\n        ohe.fit(df[[feature]])\n        # save the encoder\n        joblib.dump(ohe, open(os.path.join(path, \"TextEncoding\/ohe_{}.sav\".format(feature)), 'wb'))\n\n        # transfrom training data\n        # removing first column of encoded data to elude from dummy variable trap\n        tempData = ohe.transform(df[[feature]])[:, 1:]\n\n        # create Dataframe with columns as classes\n        tempData = pd.DataFrame(tempData, columns=columns)\n    else:\n        tempData = df[[feature]]\n    \n    # create dataframe with all the label encoded categorical features along with hot encoding\n    if i==0:\n        encodedData = pd.DataFrame(data=tempData, columns=tempData.columns.values.tolist())\n    else:\n        encodedData = pd.concat([encodedData, tempData], axis=1)","18678f9b":"# merge numerical features and categorical encoded features\ndf = df[numerical_features+['Segmentation']]\ndf = pd.concat([df, encodedData], axis=1)\ndf.info()","a9cf7373":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics, preprocessing\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom xgboost import XGBClassifier","730c4eb9":"train_data = df.copy()\nfeature_cols = [feature for feature in train_data.columns if feature not in(['Segmentation'])]\nprint('features used- ', feature_cols)\n\n''' Rescaling to [0,1] '''\nscaler = StandardScaler()\nscaler.fit(train_data[feature_cols])\ntrain_data[feature_cols] = scaler.transform(train_data[feature_cols])","eec4058c":"train_data['Segmentation'] = train_data['Segmentation'].map({'A':0, 'B':1, 'C':2, 'D':3})\nX = train_data[feature_cols]\ny = train_data['Segmentation']\n\nvalidation_size = 0.25\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=validation_size, \n                                                    random_state=0, stratify=y)","2638edcf":"model = LogisticRegression()\nmodel.fit(X_train, y_train)","365d1135":"y_pred = model.predict(X_train)\n\nprint('Train metrics...')\nprint(confusion_matrix(y_train, y_pred))\nprint(classification_report(y_train, y_pred))\n\ny_pred = model.predict(X_test)\n\nprint('Validation metrics...')\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","a92e4070":"''' metrics on original data '''\ny_pred = model.predict(train_data[feature_cols])\n\ndef make_cm(matrix, columns):\n    n = len(columns)\n    act = ['actual Segmentation'] * n\n    pred = ['predicted Segmentation'] * n\n\n    cm = pd.DataFrame(matrix, \n        columns=[pred, columns], index=[act, columns])\n    return cm\n\ndf_matrix=make_cm(\n    confusion_matrix(train_data['Segmentation'], y_pred),['A', 'B', 'C', 'D'])\n\ndisplay(df_matrix)\nprint(classification_report(train_data['Segmentation'], y_pred))","b2eabf05":"model = XGBClassifier(\n    learning_rate=0.05, \n    max_depth=3,\n    min_child_weight=5, \n    n_estimators=1000, \n    random_state=7, \n    reg_lambda=1.5,\n    reg_alpha=0.5,\n    use_label_encoder=False\n)\n\nmodel.fit(X_train, y_train,\n          eval_metric='mlogloss',\n          verbose=False)","fe0b8ebf":"y_pred = model.predict(X_train)\n\nprint('Train metrics...')\nprint(confusion_matrix(y_train, y_pred))\nprint(classification_report(y_train, y_pred))\n\ny_pred = model.predict(X_test)\n\nprint('Test metrics...')\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","c0ff6395":"''' metrics on original data '''\ny_pred = model.predict(train_data[feature_cols])\n\ndef make_cm(matrix, columns):\n    n = len(columns)\n    act = ['actual Segmentation'] * n\n    pred = ['predicted Segmentation'] * n\n\n    cm = pd.DataFrame(matrix, \n        columns=[pred, columns], index=[act, columns])\n    return cm\n\ndf_matrix=make_cm(\n    confusion_matrix(train_data['Segmentation'], y_pred),['A', 'B', 'C', 'D'])\n\ndisplay(df_matrix)\nprint(classification_report(train_data['Segmentation'], y_pred))","2a895bc8":"\nnp.random.seed(123)  # for reproducibility\n\nimport keras\nfrom keras.models import Sequential, Input, Model\nfrom keras.layers import Dense, Dropout","655e35c3":"# Initialize the constructor\nmodel = Sequential()\n# Add an input layer \n# arguemtns of dense: output shape, activation, input shape\n# activation(define the output function) = [relu', 'tanh']\nmodel.add(Dense(64, activation='relu', input_shape=(len(feature_cols),)))\nmodel.add(Dropout(0.3))\n# Add one hidden layer \n# after first layer no need to give input shape\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(8, activation='relu'))\n# Add an output layer \n# activation = [regression: 'linear', binary classification: 'sigmoid', multiclass: 'softmax']\n# threshold: >0 or <0\n# sigmoid: 1\/(1+e^-x), usually applied in output layer\n# rectifier: max(x,0), usually applied in hidden layer: relu\n# hyperbolic tangent tanh: (1-e^-2x)\/(1+e^-2x)\nmodel.add(Dense(4, activation='softmax'))\n\n# optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n# loss = [regression: 'mse', binary: 'binary_crossentropy', multi: 'categorical_crossentropy']\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='RMSprop',\n              metrics=['accuracy']\n         )\n\nprint(model.summary())","2ffa86f8":"fit = model.fit(X_train, pd.get_dummies(y_train), epochs=150, batch_size=500, verbose=1, validation_split=0.2)","def9f28d":"# Final evaluation of the model\nloss, accuracy = model.evaluate(X_test, pd.get_dummies(y_test), verbose=1)\n#print(\"Accuracy: \", accuracy*100, \"%\")\n\n# plot training vs validation for overfiiting\nplt.plot(fit.history['accuracy'], label='train accuracy')\nplt.plot(fit.history['val_accuracy'], label='validation accuracy')\nplt.plot(fit.history['loss'], label='train loss')\nplt.plot(fit.history['val_loss'], label='validation loss')\nplt.title('model_accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(loc=0, bbox_to_anchor=[1,1])\nplt.show()","ef95baa9":"# Analyzing features using VIF","6237766f":"|Variable|\tDefinition|\n|---|---|\n|ID\tUnique| ID|\n|Gender|\tGender of the customer|\n|Ever_Married|\tMarital status of the customer|\n|Age|\tAge of the customer|\n|Graduated|\tIs the customer a graduate?|\n|Profession|\tProfession of the customer|\n|Work_Experience|\tWork Experience in years|\n|Spending_Score|\tSpending score of the customer|\n|Family_Size|\tNumber of family members for the customer (including the customer)|\n|Var_1|\tAnonymised Category for the customer|\n|Segmentation|\t(target) Customer Segment of the customer|","73aa4cba":"# Model 2: XGB","38a095a5":"# Looking at Outliers","e74f26df":"As most of the features are uncorrelated, it is difficult to fill the NA values. Hence for the time being let's drop all NA.","969b9d80":"# CORRELATION","7a06b906":"# Model 3: Deep Learning","8d9293f7":"## Bivariate Analysis Correlation plot with the Numeric variables\n","9e5ac2be":"# EDA","0ce85692":"# Handling Categorical Features (Label and One Hot Encoding)","60066c20":"# Handling Missing Values","ea5fe1e4":"**Observations-**\n* Ever_Married - Spending_Score\n* Ever_Married - Age","21566ebb":"# Model 1: Logistic Regression","49303906":"Age and Var_1 have high VIF score","dbf04ffc":"**Observations-**\n* Ever_Married - UnMarried customers are usually in segment D while married are in segment A, B or C\n* Graduated - Graduated customers are usually in segment A, B or C while Ungraduated are in segment D\n* Profession - Customers in healthcare & marketing are mostly in segment D, Artist & engineers are usually in A, B or C\n* Spending_Score - Usually 'Low' spenders are in segment A or D while 'high' and 'average' spenders are in segment B or C\n* Age - <30 are in segment D, 30-40 or >70 are in segment A while 45-70 are in segment C\n* Work_Experience - <2 are in segment C while 6-11 are in segment A & D\n* Family_Size - <1 are in segment A, 1-3 are in Segment C and 4+ in segment D","5a38d612":"## Bivariate Analysis Correlation plot with the Categorical variables","a77ede03":"### Label encoding category features for correlation","4052924b":"# Training Model"}}