{"cell_type":{"b4b32a47":"code","c5bad7d2":"code","35c3a6d5":"code","ab8c7cf2":"code","59c2062f":"code","cab6edb8":"code","48da85a7":"code","6b434fce":"code","49a19ecc":"code","afc4936a":"code","aa00a1dd":"code","5b44bb04":"code","a589ffbe":"code","7aa2ecb6":"code","7b3fde40":"code","e1c20641":"code","a7c3e2ef":"code","b4966edf":"code","d07eff98":"code","74ca3266":"code","cf1e1bbd":"code","1cc16517":"code","6f39bd1b":"code","9f4d3313":"code","a8e8ceaa":"code","befd60b2":"code","3f6481b1":"code","e8263196":"code","b0be275e":"code","5e2dfa83":"code","5be66fd5":"code","8d8d95bf":"code","29878e9d":"code","d2106ba4":"code","a100ee83":"code","32a42f05":"code","e4b44646":"code","a88bb12b":"markdown","f77d498d":"markdown","cf541fac":"markdown","f9623467":"markdown","f860ed28":"markdown","a67832f1":"markdown"},"source":{"b4b32a47":"# Reloads required modules\n%reload_ext autoreload\n%autoreload 2\n\n#This will include the plots generated by matplotlib in your notebook\n%matplotlib inline","c5bad7d2":"#Imports everything needed\nfrom fastai.conv_learner import *","35c3a6d5":"#Change, if necessary. For this notebook, you don't have to!\nPATH = '..\/input\/planet-understanding-the-amazon-from-space\/'","ab8c7cf2":"ls {PATH}","59c2062f":"from fastai.plots import *","cab6edb8":"def get_1st(path, pattern): \n    return glob(f'{path}\/*{pattern}.*')[0]","48da85a7":"dc_path = \"..\/input\/dogs-vs-cats-redux-kernels-edition\/train\"\nlist_paths = [get_1st(f\"{dc_path}\", \"cat\"), get_1st(f\"{dc_path}\", \"dog\")]\nplots_from_files(list_paths, titles=[\"cat\", \"dog\"], maintitle=\"Single-label classification\")","6b434fce":"list_paths = [f\"{PATH}train-jpg\/train_0.jpg\", f\"{PATH}train-jpg\/train_1.jpg\"]\ntitles=[\"haze primary\", \"agriculture clear primary water\"]\nplots_from_files(list_paths, titles=titles, maintitle=\"Multi-label classification\")","49a19ecc":"# planet.py\n\nfrom fastai.imports import *\nfrom fastai.transforms import *\nfrom fastai.dataset import *\nfrom sklearn.metrics import fbeta_score\nimport warnings\n\ndef f2(preds, targs, start=0.17, end=0.24, step=0.01):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        return max([fbeta_score(targs, (preds>th), 2, average='samples')\n                    for th in np.arange(start,end,step)])","afc4936a":"#metrics and the model used\nmetrics=[f2]\nf_model = resnet34","aa00a1dd":"#The csv file where we have our labels\nlabel_csv = f'{PATH}train_v2.csv'\nn = len(list(open(label_csv)))-1\nval_idxs = get_cv_idxs(n)","5b44bb04":"'''\nTop down augmentation\nsz : data loader  size\n'''\ndef get_data(sz):\n    tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_top_down, max_zoom=1.05)\n    return ImageClassifierData.from_csv(PATH, 'train-jpg', label_csv, tfms=tfms,\n                    suffix='.jpg', val_idxs=val_idxs, test_name='test-jpg-v2')","a589ffbe":"#get image of size 64x64\ndata = get_data(64)","7aa2ecb6":"#data loader\nx,y = next(iter(data.val_dl))","7b3fde40":"print(np.shape(x))","e1c20641":"#labels\ny","a7c3e2ef":"list(zip(data.classes, y[0]))","b4966edf":"plt.imshow(data.val_ds.denorm(to_np(x))[1]);","d07eff98":"# Image looks washed out. So, we multiple the image with 1.4 that increases teh contrast\nplt.imshow(data.val_ds.denorm(to_np(x))[1]*1.4);","74ca3266":"data = data.resize(int(sz*1.3), '\/tmp')","cf1e1bbd":"TMP_PATH = \"\/tmp\/tmp\"\nMODEL_PATH = \"\/tmp\/model\/\"","1cc16517":"learn = ConvLearner.pretrained(f_model, data, metrics=metrics, tmp_name=TMP_PATH, models_name=MODEL_PATH)","6f39bd1b":"lrf=learn.lr_find()\nlearn.sched.plot()","9f4d3313":"lr = 0.2","a8e8ceaa":"learn.fit(lr, 3, cycle_len=1, cycle_mult=2)","befd60b2":"lrs = np.array([lr\/9,lr\/3,lr])","3f6481b1":"learn.unfreeze()\nlearn.fit(lrs, 3, cycle_len=1, cycle_mult=2)","e8263196":"learn.save(f'{sz}')","b0be275e":"learn.sched.plot_loss()","5e2dfa83":"sz=128","5be66fd5":"learn.set_data(get_data(sz))\nlearn.freeze()\nlearn.fit(lr, 3, cycle_len=1, cycle_mult=2)","8d8d95bf":"learn.unfreeze()\nlearn.fit(lrs, 3, cycle_len=1, cycle_mult=2)\nlearn.save(f'{sz}')","29878e9d":"sz=256","d2106ba4":"learn.set_data(get_data(sz))\nlearn.freeze()\nlearn.fit(lr, 3, cycle_len=1, cycle_mult=2)","a100ee83":"learn.unfreeze()\nlearn.fit(lrs, 3, cycle_len=1, cycle_mult=2)\nlearn.save(f'{sz}')","32a42f05":"multi_preds, y = learn.TTA()\npreds = np.mean(multi_preds, 0)","e4b44646":"f2(preds,y)","a88bb12b":"In multi-label classification each sample can belong to one or more clases. In the previous example, the first images belongs to two clases: *haze* and *primary*. The second image belongs to four clases: *agriculture*, *clear*, *primary* and  *water*.","f77d498d":"## Multi-label classification","cf541fac":"In single-label classification each sample belongs to one class. In the previous example, each image is either a *dog* or a *cat*.","f9623467":"## Multi-label models for Planet dataset","f860ed28":"## Multi-label versus single-label classification","a67832f1":"We use a different set of data augmentations for this dataset - we also allow vertical flips, since we don't expect vertical orientation of satellite images to change our classifications."}}