{"cell_type":{"d130b251":"code","b08cfe5f":"code","4e58fb01":"code","933baab8":"code","09b099ec":"code","d19db01a":"code","aafb85e5":"code","74151a25":"code","ccc31579":"code","ea56629b":"code","bdb92a7e":"code","d48c3c5e":"code","f21c6a12":"code","d5bb450d":"code","eb10d00c":"code","da4706d1":"code","98b04b98":"code","c175dc89":"code","6e4a91ee":"code","d44833da":"code","b794d386":"code","dcba3e16":"code","44b073fd":"code","4ec5525d":"code","2754d303":"code","72e31ea0":"code","cbfeb3b8":"code","cb841a49":"code","9fe9541e":"code","95bede4f":"code","fc82b2c8":"code","88c699da":"code","cbf48ead":"code","b99e46a9":"code","efea6a3a":"code","83148969":"code","a05ecf86":"code","18695d60":"code","ace1ab7d":"code","e24d724e":"code","4ab12673":"code","76301f27":"code","781e6a0b":"code","93eee1d2":"code","9e591758":"code","669ae472":"code","176a4e47":"code","1a430205":"code","8ce73d65":"code","3bc8f093":"code","40e108f8":"code","9e194d31":"code","570fca91":"code","e4b7ad65":"code","092feaeb":"code","c07a9b4f":"code","e9b41e5e":"code","cc5b5dd8":"code","5fb4dc3a":"code","d9e4d535":"code","10d4331c":"code","a5b9e18c":"markdown","5c132304":"markdown","671bff63":"markdown","443bfbe5":"markdown","c4215c64":"markdown","3ff65894":"markdown","27e7c64f":"markdown","c1ec6c8f":"markdown","43aee2f7":"markdown","e931ba17":"markdown","addee2a0":"markdown","4a56bd3d":"markdown","d0a323b5":"markdown","eaa926e9":"markdown","213be435":"markdown","6ce6fef6":"markdown","a1211a12":"markdown","3602e1b8":"markdown","496a8fe2":"markdown","c8c6fbdb":"markdown","8c908e8b":"markdown","856f6f90":"markdown","d173e3cc":"markdown","38642ca0":"markdown","6df33e14":"markdown","d0c38e19":"markdown","203ae347":"markdown","c75f8c93":"markdown","b6143332":"markdown","6db5c282":"markdown"},"source":{"d130b251":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn import preprocessing\n\n%matplotlib inline","b08cfe5f":"\ndef remove_outlier(col):\n    sorted(col)\n    Q1,Q3=np.percentile(col,[25,75])\n    IQR=Q3-Q1\n    lower_range= Q1-(1.5 * IQR)\n    upper_range= Q3+(1.5 * IQR)\n    return lower_range, upper_range","4e58fb01":"def scaleColumns(df, cols_to_scale):\n    scaler = preprocessing.MinMaxScaler()\n    for col in cols_to_scale:\n        df[col] = pd.DataFrame(scaler.fit_transform(pd.DataFrame(df[col])),columns=[col])\n    return df","933baab8":"experiment = {}","09b099ec":"df_original = pd.read_csv(\"..\/input\/diamonds\/diamonds.csv\")","d19db01a":"df_original.head()","aafb85e5":"df_original = df_original.drop(\"Unnamed: 0\", axis = 1)","74151a25":"df_original.isna().sum()","ccc31579":"df_original.describe()","ea56629b":"\n\ndf_original['color'].value_counts().plot(kind= 'bar')\n\n","bdb92a7e":"\n\ndf_original['cut'].value_counts().plot(kind = 'bar')\n\n","d48c3c5e":"df_original['clarity'].value_counts()","f21c6a12":"df_original.hist(figsize = (20,20), bins = 150)","d5bb450d":"\n\nplt.figure(figsize=(10,10))\nsns.heatmap(df_original.corr(), annot=True)\n\n","eb10d00c":"sns.pairplot(df_original)","da4706d1":"plt.figure(figsize =(15,10))\ndf_original.boxplot(vert = 0)","98b04b98":"df = df_original.copy()\ndf_sem_categ = df.drop([\"color\",\"cut\",\"clarity\"], axis = 1)\nscaleColumns(df_sem_categ,[\"carat\",\"depth\",\"table\",\"x\",\"y\",\"z\"])\ndf_sem_categ\n","c175dc89":"df_com_categ_transf = df_original.copy()\ndf_com_categ_transf['cut'] = pd.Categorical(df['cut']).codes\ndf_com_categ_transf['color'] = pd.Categorical(df['color']).codes\ndf_com_categ_transf['clarity'] = pd.Categorical(df['clarity']).codes\nscaleColumns(df_com_categ_transf,[\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"x\",\"y\",\"z\"])\ndf_com_categ_transf","6e4a91ee":"df = df_sem_categ.copy()\nx = df.drop(\"price\", axis = 1)\ny = df.pop(\"price\")\nknnmodel = KNeighborsRegressor(n_neighbors=3,metric='euclidean')\nx_train, x_test , y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 1)\n\nini = time.time()\nknnmodel.fit(x_train,y_train)\ny_pred = knnmodel.predict(x_test)\nfim = time.time()","d44833da":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","b794d386":"experiment['EXP-01'] = {'Regressor':'KNN','Categoria':'Sem','Outlier':'Com', 'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse, 'Tempo':fim-ini}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","dcba3e16":"\n\ndf = df_sem_categ.copy()\nx = df.drop(\"price\", axis = 1)\ny = df.pop(\"price\")\nlrmodel = LinearRegression()\nx_train, x_test , y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 1)\n\nini = time.time()\nlrmodel.fit(x_train,y_train)\ny_pred = lrmodel.predict(x_test)\nfim = time.time()","44b073fd":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","4ec5525d":"experiment['EXP-02'] = {'Regressor':'LR','Categoria':'Sem','Outlier':'Com', 'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse, 'Tempo':fim-ini}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","2754d303":"df = df_sem_categ.copy()\nfor column in df.columns:\n    lr,ur=remove_outlier(df[column])\n    df[column]=np.where(df[column]>ur,ur,df[column])\n    df[column]=np.where(df[column]<lr,lr,df[column])","72e31ea0":"plt.figure(figsize =(15,10))\ndf.boxplot(vert =0)","cbfeb3b8":"x = df.drop(\"price\", axis = 1)\ny = df.pop(\"price\")\nx_train, x_test , y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 1)\n\nini = time.time()\nlrmodel.fit(x_train,y_train)\ny_pred = lrmodel.predict(x_test)\nfim = time.time()","cb841a49":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","9fe9541e":"experiment['EXP-03'] = {'Regressor':'LR','Categoria':'Sem','Outlier':'Sem', 'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse, 'Tempo':fim-ini}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","95bede4f":"df = df_com_categ_transf.copy()\n\nx = df.drop(\"price\", axis = 1)\ny = df.pop(\"price\")\nx_train, x_test , y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 1)\n\nini = time.time()\nlrmodel.fit(x_train,y_train)\ny_pred = lrmodel.predict(x_test)\nfim = time.time()\n","fc82b2c8":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","88c699da":"experiment['EXP-04'] = {'Regressor':'LR','Categoria':'Com','Outlier':'Com', 'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse, 'Tempo':fim-ini}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","cbf48ead":"df = df_com_categ_transf.copy()\nfor column in df.columns:\n    lr,ur=remove_outlier(df[column])\n    df[column]=np.where(df[column]>ur,ur,df[column])\n    df[column]=np.where(df[column]<lr,lr,df[column])","b99e46a9":"x = df.drop(\"price\", axis = 1)\ny = df.pop(\"price\")\nx_train, x_test , y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 1)\n\nini = time.time()\nlrmodel.fit(x_train,y_train)\ny_pred = lrmodel.predict(x_test)\nfim = time.time()","efea6a3a":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","83148969":"experiment['EXP-05'] = {'Regressor':'LR','Categoria':'Com','Outlier':'Sem', 'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse, 'Tempo':fim-ini}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","a05ecf86":"df = df_sem_categ.copy()\nx = df.drop(\"price\", axis = 1)\ny = df.pop(\"price\")\nini = time.time()\nrfmodel = RandomForestRegressor(n_estimators=10, random_state=1)\nx_train, x_test , y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 1)\n\nini = time.time()\nrfmodel.fit(x_train,y_train)\ny_pred = rfmodel.predict(x_test)\nfim = time.time()\n\n","18695d60":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","ace1ab7d":"experiment['EXP-06'] = {'Regressor':'RF','Categoria':'Sem','Outlier':'Com', 'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse, 'Tempo':fim-ini}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","e24d724e":"df = df_sem_categ.copy()\nfor column in df.columns:\n    lr,ur=remove_outlier(df[column])\n    df[column]=np.where(df[column]>ur,ur,df[column])\n    df[column]=np.where(df[column]<lr,lr,df[column])","4ab12673":"x = df.drop(\"price\", axis = 1)\ny = df.pop(\"price\")\nini = time.time()\nrfmodel = RandomForestRegressor(n_estimators=10, random_state=1)\nx_train, x_test , y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 1)\nrfmodel.fit(x_train,y_train)\ny_pred = rfmodel.predict(x_test)\nfim = time.time()","76301f27":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","781e6a0b":"experiment['EXP-07'] = {'Regressor':'RF','Categoria':'Sem','Outlier':'Sem', 'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse, 'Tempo':fim-ini}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","93eee1d2":"df = df_com_categ_transf.copy()\nx = df.drop(\"price\", axis = 1)\ny = df.pop(\"price\")\nini = time.time()\nrfmodel = RandomForestRegressor(n_estimators=10, random_state=1)\nx_train, x_test , y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 1)\nrfmodel.fit(x_train,y_train)\ny_pred = rfmodel.predict(x_test)\nfim = time.time()\n","9e591758":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","669ae472":"experiment['EXP-08'] = {'Regressor':'RF','Categoria':'Com','Outlier':'Com', 'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse, 'Tempo':fim-ini}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","176a4e47":"df = df_com_categ_transf.copy()\nfor column in df.columns:\n    lr,ur=remove_outlier(df[column])\n    df[column]=np.where(df[column]>ur,ur,df[column])\n    df[column]=np.where(df[column]<lr,lr,df[column])","1a430205":"x = df.drop(\"price\", axis = 1)\ny = df.pop(\"price\")\nini = time.time()\nrfmodel = RandomForestRegressor(n_estimators=10, random_state=1)\nx_train, x_test , y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 1)\nrfmodel.fit(x_train,y_train)\ny_pred = rfmodel.predict(x_test)\nfim = time.time()","8ce73d65":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","3bc8f093":"experiment['EXP-09'] = {'Regressor':'RF','Categoria':'Com','Outlier':'Sem', 'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse, 'Tempo':fim-ini}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","40e108f8":"estimator = RandomForestRegressor()\npara_grids = {\"n_estimators\" : [10,50,100,150],\n              \"max_features\" : [\"auto\", \"log2\", \"sqrt\"],\n              \"bootstrap\"    : [True, False]}\n\n#para_grids = {\"n_estimators\" : [150]}\n\n\ngrid = GridSearchCV(estimator, para_grids)\n\nini = time.time()\ngrid.fit(x_train, y_train)\nbest = grid.best_estimator_\ny_pred = best.predict(x_test)\nfim = time.time()","9e194d31":"print ( grid.best_params_)","570fca91":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","e4b7ad65":"experiment['EXP-10'] = {'Regressor':'RF-GS','Categoria':'Com','Outlier':'Sem', 'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse, 'Tempo':fim-ini}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","092feaeb":"df = df_original.copy()\ndf['cut'] = pd.Categorical(df['cut']).codes\ndf['color'] = pd.Categorical(df['color']).codes\ndf['clarity'] = pd.Categorical(df['clarity']).codes\namostra = pd.DataFrame(df.loc[1250]).T\namostra","c07a9b4f":"anovosdados = np.array([['carat', 'cut','color','clarity','depth','table','x','y','z'], \n                       [0.72,2.0,4.0,6.0,62.5,57.0,5.7,5.73,3.57]])\n                   \ndfnovosdados = pd.DataFrame(anovosdados)\ndfnovosdados.columns = anovosdados[0]\ndfnovosdados = dfnovosdados.drop(0)\ndfnovosdados","e9b41e5e":"x = df.drop(\"price\", axis = 1)\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(x)\nx1 = scaler.transform(dfnovosdados)\nprint(x1)","cc5b5dd8":"predicao = best.predict(x1)","5fb4dc3a":"valor_real = amostra.iloc[0][\"price\"]\nvalor_previsto = predicao[0]\nprint('Valor do Diamante Real: ',valor_real)\nprint('Valor do Diamante Previsto: ',valor_previsto)","d9e4d535":"exp = pd.DataFrame(experiment).T\n\nexp.sort_index()","10d4331c":"plt.figure(figsize=(10,2))\nsns.barplot(x='R2' , y='Regressor' , data=exp)","a5b9e18c":"Selecionando uma amostra da base para compara\u00e7\u00e3o ","5c132304":"Data Frame com os campos categoricos <br>\nAplicaremos a transforma\u00e7\u00e3o de Categorico para Continuo. <br>\nN\u00e3o usaremos a t\u00e9cnica One Hot Encoding para manter os dados com um padr\u00e3o ordinal sendo :<br>\n\n**Cut**\n\nUma lapida\u00e7\u00e3o bem feita garante ao diamante um brilho que o faz se diferenciar das outras gemas. <br>\n\nCorte       O que significa ?<br>\nExcellent\tLapida\u00e7\u00e3o Excelente<br>\nVery Good\tLapida\u00e7\u00e3o Muito Boa<br>\nGood\t    Lapida\u00e7\u00e3o Boa<br>\nFair\t    Lapida\u00e7\u00e3o Aceit\u00e1vel<br>\nPoor\t    Lapida\u00e7\u00e3o Fraca <br>\n\n\n**Carat**\n\nO pre\u00e7o de um diamante diminui quanto mais cor ele apresentar.  <br>\n\nCor     O que significa?<br>\nD\t    Excepcionalmente incolor extra<br>\nE\t    Excepcionalmente incolor<br>\nF\t    Perfeitamente incolor <br>\nG\t    Nitidamente incolor<br>\nH\t    Incolor<br>\nI\t    Cor levemente percept\u00edvel<br>\nJ\t    Cor percept\u00edvel<br>\nK\t    Cor levemente vis\u00edvel<br>\nL\t    Cor vis\u00edvel<br>\nM - N\tCor levemente acentuada<br>\nO - Z\tCor acentuada<br>\n\n**Clarity**\n\nO grau de pureza do diamante se refere a presen\u00e7a (ou n\u00e3o) de inclus\u00f5es e manchas que possam diimuir seu valor.<br>\n\nGrau de Pureza\tO que significa ?<br>\nFLAWLESS \tInternamente e externamento puro<br>\nIF\t        Internamente livre de inclus\u00f5es<br>\nVVS1 e VVS2\tInclus\u00e3o ou inclus\u00f5es pequenin\u00edssimas, muito dificeis de serem visualizadas com lupa de 10x<br>\nVS1 e VS2\tInclus\u00f5es muito pequenas, dif\u00edceis de serem visualizadas com lupa de 10x<br>\nSI1 e SI2\tInclus\u00f5es pequenas, f\u00e1ceis de serem visualizadas com a lupa de 10x<br>\nI1\t        Inclus\u00f5es evidentes com lupa de 10x<br>\nI2\t        Uma inclus\u00e3o grande ou in\u00fameras inclus\u00f5es menores, f\u00e1ceis de serem visualizadas a olho nu<br>\nI3\t        Uma inclus\u00e3o grande ou in\u00fameras inclus\u00f5es menores, mito f\u00e1ceis de serem visualizadas a olho <br>\n\n","671bff63":"Gr\u00e1fico mostra coluna pre\u00e7o com valores outliers","443bfbe5":"# EXPERIMENTO 08\n## Random Forest Regressor  \n### Com Colunas Categ\u00f3ricas transformadas em continuas\n### Sem Tratamento de Outliers","c4215c64":"<h1 id=\"bibliotecas\">Bibliotecas<\/h1>","3ff65894":"<h1 id=\"experimentos\">Experimentos<\/h1>","27e7c64f":"# EXPERIMENTO 04\n## Linear Regressor  \n### Com Colunas Categ\u00f3ricas transformadas em continuas\n### Sem Tratamento de Outliers","c1ec6c8f":"# EXPERIMENTO 07\n## Random Forest Regressor  \n### Sem Colunas Categ\u00f3ricas transformadas em continuas\n### Com Tratamento de Outliers","43aee2f7":"Analisando colunas categ\u00f3ricas","e931ba17":"#  Regress\u00e3o - Pre\u00e7o de Venda de Diamantes\n\nEste notebook realiza um estudo, um conjunto de experimentos, de algoritmos de regress\u00e3o sobre o dataset Diamonds. Nosso objetivo \u00e9 predizer o valor de um diamante baseado nas suas caracter\u00edsticas.\n\n    \nConte\u00fado\n\n  \nO notebook est\u00e1 organizado como segue:\n\n\n\n<a href=\"#bibliotecas\">1 - Bibliotecas<\/a>\n\n<a href=\"#explora\">2 - An\u00e1lise Explorat\u00f3rio de dados<\/a>\n\n<a href=\"#experimentos\">3 - Experimentos<\/a>\n\n<a href=\"#hyperparameter\">4 - Hyperparameter Tuning<\/a>\n\n<a href=\"#prevendo\">5 - Prevendo o Valor de um diamante pelos atributos<\/a>\n\n<a href=\"#conclusao\">6 - Conclus\u00e3o<\/a>\n","addee2a0":"<h1 id=\"prevendo\">Prevendo o Valor de um diamante pelos atributos<\/h1>","4a56bd3d":"Fun\u00e7\u00e3o para ajustar outliers","d0a323b5":"# Experimento 01\n\n## knn Regressor - Modelo Base\n### Sem Colunas Categ\u00f3ricas \n### Sem Tratamento de Outliers","eaa926e9":"Criando um dataset para predizer","213be435":"# EXPERIMENTO 03\n## Linear Regressor  \n### Sem Colunas Categ\u00f3ricas \n### Com Tratamento de Outliers","6ce6fef6":"<h1 id=\"hyperparameter\">Hyperparameter Tuning<\/h1>","a1211a12":"# EXPERIMENTO 06\n## Random Forest Regressor  \n### Sem Colunas Categ\u00f3ricas transformadas em continuas\n### Sem Tratamento de Outliers","3602e1b8":"Verificando valores nulos","496a8fe2":"Gr\u00e1fico Mostra a coluna pre\u00e7o sem os outliers","c8c6fbdb":"Comparando valores (Real x Previsto pelos atributos)","8c908e8b":"Padronizando os dados ","856f6f90":"# EXPERIMENTO 02\n## Linear Regressor\n### Sem Colunas Categ\u00f3ricas \n### Sem Tratamento de Outliers","d173e3cc":"Vari\u00e1vel para armazenar as m\u00e9tricas das experi\u00eancias","38642ca0":"<h1 id=\"explora\">An\u00e1lise Explorat\u00f3ria de Dados<\/h1>","6df33e14":"fun\u00e7\u00e3o para normalizar dados","d0c38e19":"<h1 id=\"conclusao\">Conclus\u00e3o<\/h1>\n\n\u00c9 necess\u00e1rio um conhecimento do neg\u00f3cio para entendermos como devemos tratar as colunas. <br>\nOs algoritmos treinados com a inclus\u00e3o das colunas categoricas transformadas em valores continuos apresentaram melhor performance.<br>\n\nAlgoritmo RandomForestRegressor apresentou melhor peformance comparado ao LinearRegression e KNN\nmas a an\u00e1lise do tempo de execu\u00e7\u00e3o mostrou um custo computacional muito superior.<br>\n\nA exclus\u00e3o dos valores outliers melhoraram o processo de treinamento o que nos mostra a import\u00e3ncia da an\u00e1lise descritiva para a identifica\u00e7\u00e3o de problemas e a corre\u00e7\u00e3o dos dados apresentados ao algoritmos e que somente a escolha do algoritmo, mesmo o melhor algoritmo, n\u00e3o \u00e9 capaz de apresentar os melhores resultados poss\u00edveis. ","203ae347":"Importando os dados","c75f8c93":"# EXPERIMENTO 09\n## Random Forest Regressor  \n### Com Colunas Categ\u00f3ricas transformadas em continuas\n### Com Tratamento de Outliers","b6143332":"Visualizando a distribui\u00e7\u00e3o dos dados ","6db5c282":"# EXPERIMENTO 05\n## Linear Regressor  \n### Com Colunas Categ\u00f3ricas transformadas em continuas\n### Com Tratamento de Outliers"}}