{"cell_type":{"3896f112":"code","a71c08f1":"code","c7bd9e24":"code","7d71818f":"code","9106e0c9":"code","c1139b66":"code","8d44873d":"code","2cb36f7d":"code","a8baf38e":"code","df33809d":"code","6f24b2b4":"code","6464bc24":"markdown","9b0c29fe":"markdown","2d9bac46":"markdown","b19457f8":"markdown","5da174bf":"markdown","b4ce783f":"markdown","d7d54bcc":"markdown"},"source":{"3896f112":"import sys\nsys.path.append(\"..\/input\/timmeffnetv2\")","a71c08f1":"import platform\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport gc\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\n\nfrom albumentations.pytorch import ToTensorV2\n\nimport warnings\nwarnings.simplefilter('ignore')","c7bd9e24":"train_labels = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\ntrain_labels['path'] = train_labels['id'].apply(lambda x: f'..\/input\/seti-breakthrough-listen\/train\/{x[0]}\/{x}.npy')\ntrain_labels.head()","7d71818f":"class Config:\n    N_SPLITS = 3\n    model_name = 'vit_base_patch16_224_in21k'\n    resize = (224, 224)\n    TRAIN_BS = 32\n    VALID_BS = 16\n    num_workers = 8\n    NB_EPOCHS = 10","9106e0c9":"def plotFromFoldScores(score_dict, colors=None):\n    if colors is None:\n        colors = ['r', 'g', 'y', 'b', 'm']\n    \n    plt.figure(figsize=(10, 7))\n    for fold_num, scores in score_dict.items():\n        plt.plot(scores, f'{colors[fold_num]}o-', label=f'Fold {fold_num}')\n    \n    plt.ylabel(\"Validation ROC-AUC Score\")\n    plt.xlabel(\"Epochs\")\n    plt.title(\"Val ROC-AUC Scores in all Folds\")\n    plt.legend()\n    plt.show()","c1139b66":"class Trainer:\n    def __init__(self, model, optimizer, scheduler, train_dataloader, valid_dataloader, device):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.train_data = train_dataloader\n        self.valid_data = valid_dataloader\n        self.loss_fn = self.yield_loss\n        self.val_loss_fn = self.yield_loss\n        self.device = device\n        \n    def yield_loss(self, outputs, targets):\n        return nn.BCEWithLogitsLoss()(outputs, targets)\n    \n    def train_one_epoch(self):\n        prog_bar = tqdm(enumerate(self.train_data), total=len(self.train_data))\n        self.model.train()\n        avg_loss = 0\n        for idx, inputs in prog_bar:\n            image = inputs[0].to(self.device, dtype=torch.float)\n            targets = inputs[1].to(self.device, dtype=torch.float)\n\n            outputs = self.model(image).view(-1)\n\n            loss = self.loss_fn(outputs, targets)\n            prog_bar.set_description('loss: {:.2f}'.format(loss.item()))\n\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad(set_to_none=True)\n\n            avg_loss += loss.item()\n                \n        return avg_loss \/ len(self.train_data)\n    \n    def valid_one_epoch(self):\n        prog_bar = tqdm(enumerate(self.valid_data), total=len(self.valid_data))\n        self.model.eval()\n        all_targets = []\n        all_predictions = []\n        avg_loss = 0\n        with torch.no_grad():\n            for idx, inputs in prog_bar:\n                image = inputs[0].to(self.device, dtype=torch.float)\n                targets = inputs[1].to(self.device, dtype=torch.float)\n\n                outputs = self.model(image).view(-1)\n                \n                val_loss = self.val_loss_fn(outputs, targets)\n                prog_bar.set_description('val_loss: {:.2f}'.format(val_loss.item()))\n                \n                all_targets.extend(targets.cpu().detach().numpy().tolist())\n                all_predictions.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n                \n                avg_loss += val_loss.item()\n        val_roc_auc = roc_auc_score(all_targets, all_predictions)\n        return val_roc_auc, avg_loss \/ len(self.valid_data)\n    \n    def get_model(self):\n        return self.model","8d44873d":"class SETIData(Dataset):\n    def __init__(self, images, targets, is_test=False, augmentations=None): \n        self.images = images\n        self.targets = targets\n        self.is_test = is_test\n        self.augmentations = augmentations\n        \n    def __getitem__(self, index):\n        img, target = self.images[index], self.targets[index]\n        \n        img = np.load(img)\n        img = np.vstack(img)\n        img = img.transpose(1, 0)\n        img = img.astype(\"float\")[..., np.newaxis]\n        \n        if self.augmentations:\n            img = self.augmentations(image=img)['image']\n        \n        if self.is_test:\n            return img\n        \n        else:\n            target = self.targets[index]\n            return img, target\n    \n    def __len__(self):\n        return len(self.images)","2cb36f7d":"class EffNetV2(nn.Module):\n    def __init__(self, pretrained=True) -> None:\n        super(EffNetV2, self).__init__()\n        self.backbone = timm.create_model(Config.model_name, pretrained=pretrained, in_chans=1)\n        self.backbone.classifier = nn.Linear(self.backbone.classifier.in_features, 1)\n        \n    def forward(self, x) -> torch.Tensor:\n        out = self.backbone(x)\n        return out\n    \nclass VITModel(nn.Module):\n    \"\"\"\n    Model Class for VIT Model\n    \"\"\"\n    def __init__(self, model_name=Config.model_name, pretrained=True):\n        super(VITModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained, in_chans=1)\n        self.model.head = nn.Linear(self.model.head.in_features, 1)\n    \n    def forward(self, x):\n        x = self.model(x)\n        return x\n    \nclass MLPMixerModel(nn.Module):\n    \"\"\"\n    Model Class for MLP Mixer Model\n    \"\"\"\n    def __init__(self, model_name=Config.model_name, pretrained=True):\n        super(MLPMixerModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained, in_chans=1)\n        self.model.head = nn.Linear(self.model.head.in_features, 1)\n    \n    def forward(self, x):\n        x = self.model(x)\n        return x","a8baf38e":"class Augments:\n    \"\"\"\n    Contains Train, Validation Augments\n    \"\"\"\n    train_augments = Compose([\n        Resize(*Config.resize, p=1.0),\n        HorizontalFlip(p=0.5),\n        VerticalFlip(p=0.5),\n        ShiftScaleRotate(p=0.5, shift_limit=0.2, scale_limit=0.2, rotate_limit=20, border_mode=0, value=0, mask_value=0),\n        RandomResizedCrop(*Config.resize, p=1.0),\n        ToTensorV2(p=1.0),\n    ],p=1.)\n    \n    valid_augments = Compose([\n        Resize(*Config.resize, p=1.0),\n        ToTensorV2(p=1.0),\n    ], p=1.)","df33809d":"# Training Code\nif __name__ == '__main__':\n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n        DEVICE = torch.device('cuda:0')\n    else:\n        print(\"\\n[INFO] GPU not found. Using CPU: {}\\n\".format(platform.processor()))\n        DEVICE = torch.device('cpu')\n    \n    kfold = StratifiedKFold(n_splits=Config.N_SPLITS, shuffle=True, random_state=2021)\n    \n    fold_scores = {}\n    \n    for fold_, (trn_idx, val_idx) in enumerate(kfold.split(train_labels, train_labels['target'])):\n        print(f\"{'='*40} Fold: {fold_} {'='*40}\")\n        \n        train_data = train_labels.loc[trn_idx]\n        valid_data = train_labels.loc[val_idx]\n        \n        print(f\"[INFO] Training on {trn_idx.shape[0]} samples and validating on {valid_data.shape[0]} samples\")\n\n        # Make Training and Validation Datasets\n        training_set = SETIData(\n            images=train_data['path'].values,\n            targets=train_data['target'].values,\n            augmentations=Augments.train_augments\n        )\n\n        validation_set = SETIData(\n            images=valid_data['path'].values,\n            targets=valid_data['target'].values,\n            augmentations=Augments.valid_augments\n        )\n\n        train = DataLoader(\n            training_set,\n            batch_size=Config.TRAIN_BS,\n            shuffle=True,\n            num_workers=8,\n            pin_memory=True\n        )\n\n        valid = DataLoader(\n            validation_set,\n            batch_size=Config.VALID_BS,\n            shuffle=False,\n            num_workers=8\n        )\n\n        model = VITModel().to(DEVICE)\n        print(f\"[INFO] Training Model: {Config.model_name}\")\n        nb_train_steps = int(len(train_data) \/ Config.TRAIN_BS * Config.NB_EPOCHS)\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n\n        trainer = Trainer(model, optimizer, None, train, valid, DEVICE)\n        \n        per_fold_score = []\n        for epoch in range(1, Config.NB_EPOCHS+1):\n            print(f\"\\n{'--'*5} EPOCH: {epoch} {'--'*5}\\n\")\n\n            # Train for 1 epoch\n            tr_lss = trainer.train_one_epoch()\n            \n            # Validate for 1 epoch\n            current_roc, vl_lss = trainer.valid_one_epoch()\n            print(f\"Validation ROC-AUC: {current_roc:.4f}\")\n            \n            per_fold_score.append(current_roc)\n            torch.save(trainer.get_model().state_dict(), f\"{Config.model_name}_fold_{fold_}.pt\")\n        \n        fold_scores[fold_] = per_fold_score\n        del training_set, validation_set, train, valid, model, optimizer, trainer, current_roc\n        gc.collect()\n        torch.cuda.empty_cache()","6f24b2b4":"# Plot validation roc-auc scores for all 5 epochs from all 5 folds\nplotFromFoldScores(fold_scores)","6464bc24":"<h1 align='center' style='background: #1eeacf'>3. Custom Dataset - SETIData<\/h1>","9b0c29fe":"<h1 align='center' style='background: #1eeacf'>2. Trainer Class - Training and Validation Code<\/h1>","2d9bac46":"<h1 align='center' style='background: #1eeacf'>6. Main Training Code<\/h1>","b19457f8":"# PyTorch Trainer Script + Automatic Mixed Precision using Vision Transformers (ViT)!\n\n\n![](https:\/\/images.pexels.com\/photos\/5219462\/pexels-photo-5219462.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=350&w=500)\n\n\nThis notebooks features a PyTorch Modular Training Script which uses Automatic Mixed Precision and trains a model from the newly recently family of EfficientNetV2 and Vision Transformers (ViTs) and MLP Mixer\n\n<!-- This training notebook achieves about *0.91-0.92* ROC-AUC value on validation set when using `vit_base_patch16_224` model for training.\n -->\nIn my personal experiments, I have found models using patches (such as ViT and MLP Mixers) to perform much better than normal models like EfficientNets and ResNet family models.\n\nI would encourage you to fork my notebook and play around with the models and other parameters. ","5da174bf":"<h1 align='center' style='background: #1eeacf'>4. Model Classes - ViT and EffNetV2<\/h1>","b4ce783f":"<h1 align='center' style='background: #1eeacf'>1. Imports and Data Loading<\/h1>","d7d54bcc":"<h1 align='center' style='background: #1eeacf'>5. Augmentations<\/h1>"}}