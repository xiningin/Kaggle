{"cell_type":{"b0199f96":"code","4a262cd9":"code","7fb4ce97":"code","0a102e7b":"code","fd18c64f":"code","2c0712b7":"code","bad08365":"code","370458d9":"code","5aa22c36":"code","535a7f08":"code","fd49a948":"code","bb421340":"code","b58b3ec8":"code","3d41f269":"code","3598fad5":"code","86d0e802":"code","a6ebabec":"code","8c95d017":"code","21d0dfde":"code","c71132a5":"code","0a6566e2":"code","73e584ea":"code","cd9d872c":"code","cad4a14a":"code","e5ef57fb":"code","3acbc7bc":"code","07e1fb76":"code","df50b719":"code","04a4977e":"code","4421ab94":"code","5c669857":"code","29671e23":"code","cbff0240":"code","36cd7eb4":"code","91c2bf81":"code","a7b3e0e5":"code","bc9001e9":"code","de9d752b":"code","64091d51":"code","74a2c1dc":"code","c08506e7":"code","4d94b9ab":"code","0b4383e2":"code","bb0190fd":"code","780da417":"code","b4eacf97":"code","4dc363fb":"code","af5d6376":"code","9f8ec120":"code","da5f54bf":"code","825861a4":"code","3d38c315":"code","6a1ce3b1":"code","32cfe57e":"code","10ec54b6":"code","0dff90c7":"code","a997f0cb":"code","5c12a1c9":"code","851bd12a":"code","446b26f1":"code","a25b5d48":"code","378a93b0":"code","98dca8ed":"code","c31038a4":"code","8f475c25":"code","9795bdff":"code","8edc4c07":"code","d6d8eaeb":"code","ced943a4":"code","97f54d5e":"code","e5771a84":"markdown","ba348ba8":"markdown","2b20185d":"markdown","718876d6":"markdown","72f68489":"markdown","4eae3945":"markdown","06ad89b5":"markdown","c7772196":"markdown","11e3539e":"markdown","ce8d3691":"markdown","47996e4f":"markdown","a094e92a":"markdown","ce744c38":"markdown","7b539c2c":"markdown","526841d6":"markdown","24446ed0":"markdown","b0f6abba":"markdown","0ec7f951":"markdown","7da827c3":"markdown","3e639489":"markdown","a98ffef1":"markdown","bf2673d5":"markdown","66742b44":"markdown","11325aac":"markdown","d2992e61":"markdown","b0abf38e":"markdown","702397e2":"markdown","b858d084":"markdown","910435f9":"markdown","dea944f2":"markdown","e2292294":"markdown","f08490f8":"markdown","413f8aa6":"markdown","37a53a9d":"markdown","bf57f304":"markdown","62ab31a5":"markdown","22bf7790":"markdown","f276a2b0":"markdown","53cf34a5":"markdown","ec23c954":"markdown","94caafe1":"markdown","069ea8a9":"markdown","036440f1":"markdown","2853a2bf":"markdown","be56dc34":"markdown","0cb1aaaa":"markdown","733752e7":"markdown","6e8274d5":"markdown","67883791":"markdown","3fb047c8":"markdown","f21709e6":"markdown","ebf7cf0d":"markdown","a2a37a0d":"markdown","a6ab731d":"markdown","046ec66b":"markdown","c24e4af7":"markdown","c70ec960":"markdown","63422d7c":"markdown","0388a0ba":"markdown","092707ed":"markdown","2ae950b7":"markdown","fe507204":"markdown","90cb7336":"markdown","5cf59cfc":"markdown","ef4525d1":"markdown","28cbab3d":"markdown","8ed509e2":"markdown","c038b938":"markdown","efea244c":"markdown","9a8a0674":"markdown","5331e2e5":"markdown","a95f1088":"markdown","46640bc7":"markdown","a373f2c8":"markdown","129823ed":"markdown","c3a2a41a":"markdown","f3aab2c5":"markdown","09719c9a":"markdown","a674c7e5":"markdown","830a0477":"markdown","6397b51f":"markdown","494c95b8":"markdown","ac82784e":"markdown","0c4aa944":"markdown","42b7d813":"markdown","73ce087b":"markdown","0474e169":"markdown","d0b7a936":"markdown","29950538":"markdown","433c69e9":"markdown","0a7e0dd1":"markdown","2df1a0dd":"markdown","2066d225":"markdown","b3c39d63":"markdown","9e626931":"markdown","d35fb705":"markdown","63af21ce":"markdown","3e217cc0":"markdown"},"source":{"b0199f96":"# Packages and Modules\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport glob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime as dtt\nfrom datetime import date,timedelta","4a262cd9":"def get_yearMonth(path, str1):\n    \"\"\" Grabs Year & Month from\n        csv names\"\"\"\n    str1 = str1.replace(path+'\/','')\n    str1 = str1.split(\"-\")\n    str1 = str1[0]\n    str1 = str1[:4] + '-' + str1[4:]\n    dt = datetime.datetime.strptime(str1, \"%Y-%m\")\n    if len(str(dt.month)) == 1:\n        month = '0'+str(dt.month)\n    else:\n        month = str(dt.month)\n    return str(dt.year)+'-'+ month\n\n\ndef merge_bikeDatasets(path):\n    \"\"\"Merges bike data of the\n    last 12 mongths into one.\"\"\"\n    try:\n        path1 = os.path.join(path, \"*.csv\")\n        files =  glob.glob(path1)\n        df = pd.read_csv(files[0])\n        df[\"month\"] = get_yearMonth(files[0])\n        files1 = files[1:]\n        for file in files1:\n            tmp = pd.read_csv(file)\n            tmp[\"month\"] = get_yearMonth(file)\n            df = df.append(tmp, ignore_index=True)\n    except:\n        return False\n    else:\n        return df    ","7fb4ce97":"# Dataframe Created from csv file just read in!\nbike_df = pd.read_csv(\"..\/input\/biking\/bike_data_Oct2020_to_Sept2021_v01.csv\")\nbike_df.head()","0a102e7b":"rows, columns = bike_df.shape\nprint(f\"Rows: {rows}\\nColumns: {columns}\")","fd18c64f":"bike_df.columns","2c0712b7":"bike_df.info()","bad08365":"bike_df.isna().sum()","370458d9":"bike_cleaned_df = bike_df[bike_df.columns]","5aa22c36":"id(bike_cleaned_df ) == id(bike_df)","535a7f08":"cur_cols = set(list(bike_cleaned_df.columns))\ncols_tobe_removed =  set(['ride_id', 'start_station_id','end_station_id',\n                          'start_lat', 'start_lng', 'end_lat', 'end_lng'])\nnew_cols = cur_cols.difference(cols_tobe_removed)\nnew_cols = ['rideable_type', 'member_casual','started_at',  'month', \n            'start_station_name', 'ended_at', 'end_station_name']\n\nbike_cleaned_df = bike_cleaned_df[new_cols]\nbike_cleaned_df.head()","fd49a948":"set(bike_cleaned_df['rideable_type'].tolist())","bb421340":"bike_cleaned_df['rideable_type'] = bike_cleaned_df['rideable_type'].apply(\n    lambda x: 'classic_bike' if x == 'docked_bike' else x)\nbike_cleaned_df.head()","b58b3ec8":"set(bike_cleaned_df['rideable_type'].tolist())","3d41f269":"bike_cleaned_df['start_station_name'] = bike_cleaned_df['start_station_name'].apply(\n    lambda x: 'Halsted St & 18th St' if x == 'Halsted St & 18th St (Temp)' else x)","3598fad5":"bike_cleaned_df.isna().sum()","86d0e802":"bike_cleaned_df['start_station_name'] = bike_cleaned_df['start_station_name'].fillna(\"not_provided\")\nbike_cleaned_df['end_station_name'] = bike_cleaned_df['end_station_name'].fillna(\"not_provided\")\nbike_cleaned_df.isna().sum()","a6ebabec":"shape_bf = bike_cleaned_df.shape\nprint(shape_bf)","8c95d017":"bike_cleaned_df = bike_cleaned_df[bike_cleaned_df['start_station_name'] != 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)']\nshape_af = bike_cleaned_df.shape\nprint(shape_af)\nprint(f\"Number of data-points removed: {shape_bf[0]-shape_af[0]}\")","21d0dfde":"bike_cleaned_df['started_at'] =  pd.to_datetime(bike_cleaned_df['started_at'], format='%Y-%m-%d %H:%M:%S')\nbike_cleaned_df['ended_at'] =  pd.to_datetime(bike_cleaned_df['ended_at'], format='%Y-%m-%d %H:%M:%S')\n\nbike_cleaned_df['day_of_week'] = bike_cleaned_df['started_at'].dt.day_name()\n\nbike_cleaned_df[\"riding_duration\"] = (bike_cleaned_df['ended_at']-bike_cleaned_df['started_at']).astype('timedelta64[m]')\nbike_cleaned_df.head()","c71132a5":"sh_bf = bike_cleaned_df.shape\nprint(f\"Before negative durations were filtered out: {sh_bf}\")\nbike_cleaned_df = bike_cleaned_df[bike_cleaned_df[\"riding_duration\"]>=0]\nsh_af = bike_cleaned_df.shape\nprint(f\"After negative durations were filtered out: {sh_af}\")   \nprint(f\"Number of removed observations: {sh_bf[0] - sh_af[0]}\")   ","0a6566e2":"sh_bf9 = bike_cleaned_df.shape\nprint(f\"Before: {sh_bf9}\")\n\nbike_cleaned_df = bike_cleaned_df[\n    ((bike_cleaned_df[\"start_station_name\"] == \n     bike_cleaned_df[\"end_station_name\"]) & (bike_cleaned_df[\"riding_duration\"] <= 1)) == False]\n\nsh_af9 = bike_cleaned_df.shape\nprint(f\"After: {sh_af9}\")\nprint(f\"Number of dropped rows: {sh_bf9[0] - sh_af9[0]}\")","73e584ea":"bike_cleaned_df = bike_cleaned_df[bike_cleaned_df['riding_duration'].notna()]\nbike_cleaned_df.shape","cd9d872c":"bike_cleaned_df.rename(columns = {'rideable_type':'bicycle_type'}, inplace = True)\nbike_cleaned_df.rename(columns = {'member_casual':'client_type'}, inplace = True)\nbike_cleaned_df.head()","cad4a14a":"bike_cleaned_df['riding_duration'] = bike_cleaned_df['riding_duration'].round(2)\nprint(f\"Minimum Duration in minutes: {bike_cleaned_df['riding_duration'].min()}\")\nprint(f\"Average Duration in minutes: {round(bike_cleaned_df['riding_duration'].mean(),2)}\")\nprint(f\"Maximum Duration in minutes: {bike_cleaned_df['riding_duration'].max()}\")","e5ef57fb":"fig = plt.figure(figsize =(10, 7))\n \n# Creating plot\ndata = bike_cleaned_df['riding_duration'].tolist()\nplt.boxplot(data)\nplt.ylabel(\"Riding Lengths in Minutes\", color = 'blue')\nplt.title(\"Box Plot of the Riding Durations\", color = 'green')\n \n# show plot\nplt.grid()\nplt.show()","3acbc7bc":"Q1 = np.quantile(data, 0.25)\nQ2 = np.quantile(data, 0.5)\nQ3 = np.quantile(data, 0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5*IQR\nupper_bound = Q3 + 1.5*IQR\nI = [(Q1-1.5*IQR) if (Q1-1.5*IQR)>=0 else 0 , Q3+1.5*IQR]","07e1fb76":"print(f\"Normal\/non-skewing Range of Riding Durations in minutes: {I}\")","df50b719":"lesstn = bike_cleaned_df[bike_cleaned_df[\"riding_duration\"] < I[0]]\nlesstn.shape","04a4977e":"greatertn = bike_cleaned_df[bike_cleaned_df[\"riding_duration\"] > I[1]]\ngreatertn.shape","4421ab94":"rw_bf = bike_cleaned_df.shape\nprint(f\"Shape before filtering: {rw_bf}\")\nbike_cleaned_df = bike_cleaned_df[bike_cleaned_df[\"riding_duration\"] >= I[0]]\nbike_cleaned_df = bike_cleaned_df[bike_cleaned_df[\"riding_duration\"] <= I[1]]\nrw_af = bike_cleaned_df.shape\nprint(f\"Shape after filtering: {rw_af}\")\nprint(f\"Number of rows removed: {rw_bf[0]-rw_af[0]}\")","5c669857":"cleaned_bike_df =  bike_cleaned_df[bike_cleaned_df.columns]\ncleaned_bike_df.head()","29671e23":"cleaned_bike_df.shape","cbff0240":"cleaned_bike_df.columns","36cd7eb4":"cleaned_bike_df.head()","91c2bf81":"cleaned_bike_df.info()","a7b3e0e5":"tmp1 =  cleaned_bike_df[cleaned_bike_df.columns]\ntmp1['count']  = 1\ngp_cmn = tmp1.groupby([\"client_type\"]).count().reset_index()[['client_type', 'count']]\ngp_cmnv = gp_cmn.copy(deep=True)\ngp_cmn[\"percent\"] = ((gp_cmn[\"count\"]\/gp_cmn[\"count\"].sum()))\nformat_dict = {'percent': '{:.1%}'}\ngp_cmn.style.format(format_dict).hide_index()","bc9001e9":"sum_minutess = cleaned_bike_df.groupby(\n    ['client_type']).sum().reset_index()[['client_type', 'riding_duration']]\nsum_minutess","de9d752b":"daily_gp = cleaned_bike_df[['client_type','started_at','riding_duration']]\ndaily_gp =daily_gp.groupby(\n    [pd.Grouper(key='started_at', \n    freq='1D'),'client_type'])['riding_duration'].mean().reset_index()\ndaily_gp = daily_gp.groupby(['client_type']).mean().reset_index()\ndaily_gp = daily_gp.round(2)\ndaily_gpv = daily_gp.copy(deep = True)\ndaily_gp.head()","64091d51":"mnth_gp = cleaned_bike_df[['month', 'client_type','riding_duration']]\nmnth_gp = mnth_gp.groupby(['month','client_type']\n    )['riding_duration'].mean().reset_index()\nmnth_gp =mnth_gp.round(2)\nmnth_gpv = mnth_gp.copy(deep = True)\nmnth_gp.head()","74a2c1dc":"dw_gp = cleaned_bike_df[['day_of_week', 'client_type','riding_duration']]\ndw_gp = dw_gp.groupby(['day_of_week','client_type']\n    )['riding_duration'].mean().reset_index()\ndw_gp =dw_gp.round(2)\ndw_gpv = dw_gp.copy(deep = True)\ndw_gp.head()","c08506e7":"weekly_gp = cleaned_bike_df[['client_type','started_at','riding_duration']]\nweekly_gp =weekly_gp.groupby(\n    [pd.Grouper(key='started_at', \n    freq='1W'),'client_type'])['riding_duration'].mean().reset_index()\nweekly_gp = weekly_gp.round(2)\nweekly_gp.head()","4d94b9ab":"weekly_gp2 = weekly_gp.groupby(['client_type']).mean().reset_index()\nweekly_gp2 = weekly_gp2.round(2)\nweekly_gp2.head()","0b4383e2":"corr_wnr = cleaned_bike_df[['month','client_type','day_of_week','riding_duration']]\ndays_dict = {'Sunday':1, 'Monday':2, 'Tuesday':3, 'Wednesday':4, \n             'Thursday':5, 'Friday':6, 'Saturday':7}\ncorr_wnr['day_code'] = corr_wnr['day_of_week'].apply(lambda x: days_dict[x])\ncorr_wnr1 = corr_wnr[['day_code','riding_duration']]\ncorr_wnr1.corr()","bb0190fd":"corr_snr = cleaned_bike_df[['started_at']]\ncorr_snr['count'] =1\ncorr_snr['started_at'] = pd.to_datetime(corr_snr['started_at']).dt.date\ncorr_snr = corr_snr.groupby(['started_at']).sum().reset_index()\ncorr_snr['month'] = pd.DatetimeIndex(corr_snr['started_at']).month.astype(\"int\")\nmonth_codes = {'January':1, 'February':2,'March': 3,\n               'April':4,'May':5, 'June':6,\n            'July':7, 'August':8, 'September':9,\n            'October':10, 'November':11,'December':12}\nmonth_codes2 = dict([(value, key) for key, value in month_codes.items()])\n\ncorr_snr['month_name'] = corr_snr['month'].apply(lambda x: month_codes2[x])\nmonth_code = {'January':1, 'February':1,'March': 2,\n               'April':2,'May':2, 'June':3,\n            'July':3, 'August':3, 'September':4,\n            'October':4, 'November':4,'December':1}\ncorr_snr['season_code'] = corr_snr['month_name'].apply(lambda x: month_code[x])\n\ncorr_snr1 = corr_snr[['season_code', 'count']]\ncorr_snr1.corr()","780da417":"corr_srd = cleaned_bike_df[['started_at','riding_duration']]\ncorr_srd['started_at'] = pd.to_datetime(corr_srd['started_at']).dt.date\n#corr_srd = corr_srd.groupby(['started_at']).sum().reset_index()\ncorr_srd['month'] = pd.DatetimeIndex(corr_srd['started_at']).month.astype(\"int\")\ncorr_srd['month_name'] = corr_srd['month'].apply(lambda x: month_codes2[x])\ncorr_srd['season_code'] = corr_srd['month_name'].apply(lambda x: month_code[x])\n\ncorr_srd.head()","b4eacf97":"corr_srd1 = corr_srd[['season_code', 'riding_duration']]\ncorr_srd1.corr()","4dc363fb":"ms_df = cleaned_bike_df[['client_type','start_station_name', 'end_station_name']]\nms_df['count'] = 1\nms_df.head()","af5d6376":"cas_df = ms_df[ms_df[\"client_type\"] == \"casual\"]\ncas_df.head()","9f8ec120":"ssc_gp = cas_df.groupby(['start_station_name']).count().reset_index()[['start_station_name', 'count']]\nssc_gp.sort_values(by = ['count'], inplace = True, ascending = False)\nssc_gp.head()","da5f54bf":"esc_gp = cas_df.groupby(['end_station_name']).count().reset_index()[['end_station_name', 'count']]\nesc_gp.sort_values(by = ['count'], inplace = True, ascending = False)\nesc_gp.head()","825861a4":"mem_df = ms_df[ms_df[\"client_type\"] == \"member\"]\nmem_df.head()","3d38c315":"ssm_gp = mem_df.groupby(['start_station_name']).count().reset_index()[['start_station_name', 'count']]\nssm_gp.sort_values(by = ['count'], inplace = True, ascending = False)\nssm_gp.head()","6a1ce3b1":"esm_gp = mem_df.groupby(['end_station_name']).count().reset_index()[['end_station_name', 'count']]\nesm_gp.sort_values(by = ['count'], inplace = True, ascending = False)\nesm_gp.head()","32cfe57e":"fig = plt.figure(figsize =(12, 10))\ngp_cmnv.sort_values(by = ['count'], inplace = True, ascending = False)\nplots = sns.barplot(x=\"client_type\", y=\"count\", data=gp_cmnv)\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.2f'),\n    (bar.get_x() + bar.get_width() \/ 2,\n    bar.get_height()), ha='center', va='center',\n    size=15, xytext=(0, 8), color = \"green\",\n    textcoords='offset points')\n\n    plt.ylabel(\"Number of Riders\", color = 'black', rotation=90, fontsize=14, fontname='monospace')\nplt.xlabel(\"Client Types\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.suptitle(\"Numbers: Members vs Casual\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.title(\"The Cleaned Data Contains 57.3% Member Riders and 42.7% Casual riders\", \n             color = 'black', rotation=0, fontsize=12, fontname='monospace')\n# Finally showing the plot\nplt.grid()\nplt.show()","10ec54b6":"fig = plt.figure(figsize =(12, 10))\nsum_minutess.sort_values(by = ['riding_duration'], inplace = True, ascending = False)\nplots = sns.barplot(x=\"client_type\", y=\"riding_duration\", data=sum_minutess)\n\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.2f'),\n    (bar.get_x() + bar.get_width() \/ 2,\n    bar.get_height()), ha='center', va='center',\n    size=15, xytext=(0, 8), color = \"green\",\n    textcoords='offset points')\n    \nplt.ylabel(\"Riding Durations\", color = 'black', rotation=90, fontsize=14, fontname='monospace')\nplt.xlabel(\"Client Types\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.suptitle(\"Total Minutes Riden: Casual Riders vs  Member Riders\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.title(\"Despite numerical difference, both group of riders have similar total ridng minutes.\", \n             color = 'black', rotation=0, fontsize=12, fontname='monospace')\n\nplt.grid()\nplt.show()","0dff90c7":"fig = plt.figure(figsize =(12, 10))\ndaily_gpv.sort_values(by = ['riding_duration'], inplace = True, ascending = False)\nplots = sns.barplot(x=\"client_type\", y=\"riding_duration\", data=daily_gpv)\n\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.2f'),\n    (bar.get_x() + bar.get_width() \/ 2,\n    bar.get_height()), ha='center', va='center',\n    size=15, xytext=(0, 8), color = \"green\",\n    textcoords='offset points')\n    \nplt.ylabel(\"Riding Durations\", color = 'black', rotation=90, fontsize=14, fontname='monospace')\nplt.xlabel(\"Client Types\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.suptitle(\"Daily Average in Minutes: Casual Riders vs Member Riders\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.title(\"The Daily Average of Casual Riders is Higher than that of Member Riders!\", \n             color = 'black', rotation=0, fontsize=12, fontname='monospace')\n\nplt.grid()       \nplt.show()","a997f0cb":"fig = plt.figure(figsize =(12, 10))\nweekly_gp2.sort_values(by = ['riding_duration'], inplace = True, ascending = False)\nplots = sns.barplot(x=\"client_type\", y=\"riding_duration\", data=weekly_gp2)\n\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.2f'),\n    (bar.get_x() + bar.get_width() \/ 2,\n    bar.get_height()), ha='center', va='center',\n    size=15, xytext=(0, 8), color = \"green\",\n    textcoords='offset points')\n    \nplt.ylabel(\"Riding Durations\", color = 'black', rotation=90, fontsize=14, fontname='monospace')\nplt.xlabel(\"Client Types\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.suptitle(\"Overall Weekly Average in Minutes: Casual Riders vs Member Riders\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.title(\"The Overall Weekly Average of Casual Riders is Higher than that of Member Riders!\", \n             color = 'black', rotation=0, fontsize=12, fontname='monospace')\n\nplt.grid()       \nplt.show()","5c12a1c9":"fig = plt.figure(figsize =(16, 10))\n#weekly_gp.sort_values(by = ['riding_duration'], inplace = True, ascending = False)\nplots = sns.lineplot(x=\"started_at\", y=\"riding_duration\", data=weekly_gp)\n   \nplt.ylabel(\"Riding Durations\", color = 'black', rotation=90, fontsize=14, fontname='monospace')\nplt.xlabel(\"Client Types\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.suptitle(\"Weekly Averages in Minutes: Casual Riders vs Member Riders\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.title(\"Weekly Averages are proportional to season temperatures!\", \n             color = 'black', rotation=0, fontsize=12, fontname='monospace')\n\nplt.grid()       \nplt.show()","851bd12a":"mnth_gpv.head()","446b26f1":"fig = plt.figure(figsize =(20, 14))\n#weekly_gp2.sort_values(by = ['riding_duration'], inplace = True, ascending = False)\nplots = sns.barplot(x=\"month\", y=\"riding_duration\",hue=\"client_type\", data=mnth_gpv)\n\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.2f'),\n    (bar.get_x() + bar.get_width() \/ 2,\n    bar.get_height()), ha='center', va='center',\n    size=15, xytext=(0, 8), color = \"green\",\n    textcoords='offset points')\n    \nplt.ylabel(\"Riding Durations\", color = 'black', rotation=90, fontsize=14, fontname='monospace')\nplt.xlabel(\"Months\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.suptitle(\"Monthly Averages in Minutes: Casual Riders vs Member Riders\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.title(\"The Overall Weekly Average of Casual Riders is Higher than that of Member Riders!\", \n             color = 'black', rotation=0, fontsize=12, fontname='monospace')\n\nplt.grid()       \nplt.show()","a25b5d48":"fig = plt.figure(figsize =(16, 10))\n#weekly_gp.sort_values(by = ['riding_duration'], inplace = True, ascending = False)\nplots = sns.lineplot(x=\"month\", y=\"riding_duration\", hue = \"client_type\", data=mnth_gpv)\n   \nplt.ylabel(\"Riding Durations\", color = 'black', rotation=90, fontsize=14, fontname='monospace')\nplt.xlabel(\"Months\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.suptitle(\"Monthly Averages in Minutes: Casual Riders vs Member Riders\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.title(\"Monthly Averages are Higher during Spring and Summer and lowest During Winter!\", \n             color = 'black', rotation=0, fontsize=12, fontname='monospace')\n\nplt.grid()       \nplt.show()","378a93b0":"dw_gpv['day_code'] = dw_gpv['day_of_week'].apply(lambda x: days_dict[x])\ndw_gpv.sort_values(by = ['day_code'], inplace =True)\n\nfig = plt.figure(figsize =(20, 14))\n#weekly_gp2.sort_values(by = ['riding_duration'], inplace = True, ascending = False)\nplots = sns.barplot(x=\"day_of_week\", y=\"riding_duration\",hue=\"client_type\", data=dw_gpv)\n\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.2f'),\n    (bar.get_x() + bar.get_width() \/ 2,\n    bar.get_height()), ha='center', va='center',\n    size=15, xytext=(0, 8), color = \"green\",\n    textcoords='offset points')\n    \nplt.ylabel(\"Riding Durations Per Day of a Week\", color = 'black', rotation=90, fontsize=14, fontname='monospace')\nplt.xlabel(\"Days of the Week\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.suptitle(\"Averages Riding Minutes Per Day of a week: Casual Riders vs Member Riders\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.title(\"Daily Average Riding Durations are higher during the Weekend\", \n             color = 'black', rotation=0, fontsize=12, fontname='monospace')\n\nplt.grid()       \nplt.show()","98dca8ed":"fig = plt.figure(figsize =(16, 10))\n#weekly_gp.sort_values(by = ['riding_duration'], inplace = True, ascending = False)\nplots = sns.lineplot(x=\"day_of_week\", y=\"riding_duration\",hue=\"client_type\", data=dw_gpv)\n   \nplt.ylabel(\"Riding Durations Per Day of a Week\", color = 'black', rotation=90, fontsize=14, fontname='monospace')\nplt.xlabel(\"Days of the Week\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.suptitle(\"Averages Riding Minutes Per Day of a week: Casual Riders vs Member Riders\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.title(\"Daily Average Riding Durations are higher during the Weekend\", \n             color = 'black', rotation=0, fontsize=12, fontname='monospace')\n\nplt.grid()       \nplt.show()","c31038a4":"corr_wnr['count'] =1\ntmp = corr_wnr[[\"month\", 'day_of_week', 'count']]\ntmp = tmp.groupby([\"month\", 'day_of_week']).sum().reset_index()[[\"month\", 'day_of_week', 'count']]\n\nfig = plt.figure(figsize =(16, 10))\n#weekly_gp.sort_values(by = ['riding_duration'], inplace = True, ascending = False)\nplots = sns.scatterplot(x=\"month\", y=\"count\",hue=\"day_of_week\", data=tmp)\n   \nplt.ylabel(\"Number of Riders\", color = 'black', rotation=90, fontsize=14, fontname='monospace')\nplt.xlabel(\"Months\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.title(\"Correlation of Days of the Week and Number of Riders\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\n\nplt.grid()       \nplt.show()","8f475c25":"fig = plt.figure(figsize =(16, 10))\n#weekly_gp.sort_values(by = ['riding_duration'], inplace = True, ascending = False)\nplots = sns.scatterplot(x=\"month_name\", y=\"count\",hue=\"season_code\", data=corr_snr)\n   \nplt.ylabel(\"Number of Riders\", color = 'black', rotation=90, fontsize=14, fontname='monospace')\nplt.xlabel(\"Month Names\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.suptitle(\"Correlation of Seasons and Number of Riders\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.title(\"Strong Correlation Exists Between Seasons and Number of Riders\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\n\nplt.grid()       \nplt.show()","9795bdff":"fig = plt.figure(figsize =(16, 10))\n#weekly_gp.sort_values(by = ['riding_duration'], inplace = True, ascending = False)\nplots = sns.scatterplot(x=\"month_name\", y=\"riding_duration\",hue=\"season_code\", data=corr_srd)\n   \nplt.ylabel(\"Riding Durations\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.xlabel(\"Month Names\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.suptitle(\"Correlation of Seasons and Duration of Rides\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.title(\"No\/little Correlation Exists Between Seasons and Duration of Rides\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\n\nplt.grid()       \nplt.show()","8edc4c07":"top10ss = ssc_gp.head(10)\ntop10ss = top10ss[top10ss['start_station_name']!= 'not_provided']\ntop10ss","d6d8eaeb":"fig = plt.figure(figsize =(20, 12))\n#ssc_gp.sort_values(by = ['count'], inplace = True, ascending = False)\nplots = sns.barplot(x=\"start_station_name\", y=\"count\", data=top10ss)\n\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.2f'),\n    (bar.get_x() + bar.get_width() \/ 2,\n    bar.get_height()), ha='center', va='center',\n    size=15, xytext=(0, 8), color = \"green\",\n    textcoords='offset points')\n    \nplt.ylabel(\"Count of Most Visited Start Stations\", color = 'black', rotation=90, fontsize=14, fontname='monospace')\nplt.xlabel(\"Most Visited Start Stations\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.suptitle(\"Most visited Start Stations by Casual Riders\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.title(\"The most visited start stations are mostly closer to Recreational Centers!\", \n             color = 'black', rotation=0, fontsize=12, fontname='monospace')\nax = plt.gca()\nax.tick_params(axis='x', labelrotation = 45)\nplt.grid()       \nplt.show()","ced943a4":"top10es = esc_gp.head(10)\ntop10es = top10es[top10es['end_station_name']!= 'not_provided']\ntop10es","97f54d5e":"fig = plt.figure(figsize =(20, 12))\n#ssc_gp.sort_values(by = ['count'], inplace = True, ascending = False)\nplots = sns.barplot(x=\"end_station_name\", y=\"count\", data=top10es)\n\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.2f'),\n    (bar.get_x() + bar.get_width() \/ 2,\n    bar.get_height()), ha='center', va='center',\n    size=15, xytext=(0, 8), color = \"green\",\n    textcoords='offset points')\n    \nplt.ylabel(\"Count of Most Visited End Stations\", color = 'black', rotation=90, fontsize=14, fontname='monospace')\nplt.xlabel(\"Most Visited End Stations\", color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.suptitle(\"Most visited End Stations by Casual Riders\", \n             color = 'black', rotation=0, fontsize=14, fontname='monospace')\nplt.title(\"The most visited End Stations are mostly closer to Recreational Centers!\", \n             color = 'black', rotation=0, fontsize=12, fontname='monospace')\nax = plt.gca()\nax.tick_params(axis='x', labelrotation = 45)\nplt.grid()       \nplt.show()","e5771a84":"<div class=\"alert alert-block alert-warning\">","ba348ba8":"### 11. Most Visited Stations","2b20185d":"#### Structure of the Dataset","718876d6":"<div class=\"alert alert-block alert-warning\">","72f68489":"### Check for Missing Values","4eae3945":"### Summary of Data Preprocessing\n- Dimensions of the cleaned dataframe\n    - 20 rows werer removed due to 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)' under column name \"start_station_name\".\n    -  3, 304 Data-points\/obseravtions were removed because they have negative values under column name \"riding_duration\".\n    - 96, 261 datapoints were removed because they have the same starting and ending stations and a riding duration of less than or equal to 60 seocnds. \n    - 360, 298 data-points were removed because they fall on the outlier region. They have the potential to skew the result. \n    - 7 columns were removed in the process step.\n    - 2 new columns were added in the process step: riding_duration & day_of_week.\n    - Ended up with a cleaned dataframe of dimensions:    \n    $$Rows = 4, 676, 378\\ and\\ Columns = 9$$\n- 2 columns renamed: 'rideable_type' --> 'bicycle_type',  and  'member_casual' --> 'client_type'.","06ad89b5":"<div class=\"alert alert-block alert-warning\">","c7772196":"### A python code for putting the last 12 months' bike datasets together\n- The 12 datasets that belong to each month from October, 2020 to November, 2021 were put together into a single, big dataset using the following user-defined python fuctions.","11e3539e":"#### B10: Remove the rows that contain NA under column name \"riding_duration\" if any.","ce8d3691":"### 5. Monthly Averages (for each of the 12 months) Rides of the Rider Groups ","47996e4f":"### 2. Total Minutes Riden over the past 12 months by both Tpyes of Riders","a094e92a":"# <font color=\"blue\">Google Data Analytics Certificate - Case Study<\/font>","ce744c38":"![process2.png](attachment:bfc19fae-0e26-481f-935b-504d112b5323.png)","7b539c2c":"<div class=\"alert alert-block alert-warning\">","526841d6":"### 5. Monthly Average Rides of Casual Riders vs Member Riders in Minutes\n- As illustrated in the bar and line plots portrayed below, the monthly average rides in minute of the casual riders is higher than that of annual member riders throughout the entire period of observation.","24446ed0":"- Casual Riders' Obseravtions\/Datapoints with Unamed  End Stations, labeled as \"not_provided","b0f6abba":"- Number of observations whose riding_duration is greater than upper_bound:","0ec7f951":"Now that I have performed my analysis and gained some insights into my data, it is time to create visualizations to share my findings. Moreno has reminded me that they should be sophisticated and polished in order to effectively communicate to the executive team. I employed the following Case Study Roadmap as a guide.\n\n\n# Case Study Roadmap - Share\n### Guiding questions\n- Were I able to answer the question of how annual members and casual riders use Cyclistic bikes differently?\n- What story does my data tell?\n- How do my findings relate to my original question?\n- Who is my audience? What is the best way to communicate with them?\n- Can data visualization help me share my findings?\n- Is my presentation accessible to my audience?\n\n### Key tasks\n1. Determine the best way to share my findings.\n2. Create effective data visualizations.\n3. Present my findings.\n4. Ensure my work is accessible.\n\n### Deliverable\n- Supporting visualizations and key findings","7da827c3":"- Casual Riders' Obseravtions\/Datapoints with Unamed  Start Stations, labeled as \"not_provided","3e639489":"- Number of observations whose riding_duration is less than lower_bound:","a98ffef1":"![act2.png](attachment:42ece2ea-5968-4d3f-8d95-122df8147e76.png)","bf2673d5":"![analyze2.png](attachment:713ea7c7-f041-44e7-845e-ad9817b67d73.png)","66742b44":"- Averaged over all weeks in the obseravtion period","11325aac":"### 9. Correlation Between Riding Durations and Seasons\n- The seasons do not have a meaningful correlation with daily average of riding durations.","d2992e61":"         \"Wih Patience We Can Dissect An Ant and Reach Its Heart\"\n                                ~END~","b0abf38e":"- Member Riders' Obseravtions\/Datapoints with Unamed  Start Stations, labeled as \"not_provided","702397e2":"### 10. Correlation between Seasons and Riding durations\n- As demostrated below, the correlation between seasons and riding durations is 0.004, which is almost zero. Hence, there is no correlation between the seasons and riding durations.","b858d084":"### 7. Weekly Averages for members and casual riders","910435f9":"# Case Study Roadmap - Ask\n### Guiding questions\n#### Three questions will guide the future marketing program:\n1. How do annual members and casual riders use Cyclistic bikes differently?\n2. Why would casual riders buy Cyclistic annual memberships?\n3. How can Cyclistic use digital media to influence casual riders to become members?\n\n### My Assignment\nMoreno has assigned me the first question to answer: \n- How do annual members and casual riders use Cyclistic bikes differently? \n\n### Hypothesis\n- Annual members are much more profitable than casual riders; hence, converting casual riders into annual members would increase the company's profit.\n\n### Deliverable\nThen, I will produce a report with the following deliverables:\n1. A clear statement of the business task\n    - How do annual members and casual riders use Cyclistic bikes differently? \n        - Number of casual vs annual member riders?\n        - Averages of casual and member riders in terms of daily riding duration?\n        - What is the weekly and monthly average of riding durations for casual and annual-member riders?\n        - What are the correlations between seasons and number of rides of each group?\n        - what are the correlations between seasons and riding duration of each group?\n        - Overall correlations between seasons and riding durations?\n2. A description of all data sources used\n3. Documentation of any cleaning or manipulation of data\n4. A summary of my analysis\n5. Supporting visualizations and key findings\n6. My top three recommendations based on my analysis","dea944f2":"### 3. Daily Riding Average of Casual Riders vs Annual Member Riders ","e2292294":"- After Correction was made:","f08490f8":"<div class=\"alert alert-block alert-warning\">","413f8aa6":"#### B7: Add Two New columns named \"riding_duration\" and \"day_of_week\"\n- The unit of the new column \"riding_duration\" is minutes and that of \"day_of_week\" is day.\n\n            riding_duration <- (ended_at - started_at)\n            day_of_week <- day from started_at","37a53a9d":"#### B5: Replace missing names under \"start\/end_station_name\" with \"not_provided\"","bf57f304":"#### By Annual Members\n- 249, 862 rows have no named start stations for member riders\n- 250, 424 datapoints have no named end stations for member riders\n- In total, 500, 286 observations contain no named start or end stations for member riders","62ab31a5":"<div class=\"alert alert-block alert-warning\">","22bf7790":"Now that I have finished creating the pertinent visualizations, it is time to act on my findings. I have to prepare the deliverables that Morena asked me to create, including the three top recommendations based on my analysis. I used the following Case Study Roadmap as a guide.\n\n# Case Study Roadmap - Act\n### Guiding questions\n- What are my final conclusions based on my analysis?\n- How could my team and business apply my insights?\n- What next steps would I or my stakeholders take based on my findings?\n- Is there additional data I could use to expand on my findings?\n\n### Key tasks\n1. Creating my portfolio.\n2. Adding my case study.\n3. Practicing presenting my case study to a friend or family member.\n\n### Deliverable\n- My top three recommendations based on my analysis\n\n\n## A. conclusions\nBased on the insights uncovered\/mined from the data by the extensive analysis carried out, the following conclusions were drawn:\n1. In the data analyzed after apropos cleaning, 57.3% of the riders are annual member of the  Bike-share company in Chicago; whereas the remaining 42.7% are casual riders. Nonetheless, the member riders have only slightly less total annual riding duration (33, 130, 825 minutes) than that of casual riders (33, 714, 643 minutes).\n2. The casual riders have longer daily riding average (15.91 minutes) than the member riders (12.16 minutes) do have. Likewise, the casual riders have better weekly and monthly riding averages than the member riders through the whole observation period (12 months: October, 2020 to the end of September, 2021).\n3. Casual riders have better riding lengths on all days of the week compared to the member riders. In most cases, the number of riders of both types is much higher during the weekend (Sundays & Saturdays) than the weekdays.\n4. A strong correlation was uncovered between the seasons (Winter, Spring, Summer, & Fall vis-a-vis North America) and the number of riders. It was found out that the number of riders of both types start going up by the end of Winter and reaches its peak by Spring around May (stays almost around that peak until end of August), and conversely. Putting it another way, as the temperature increases, the number of riders increase sharply. Then, it starts going down at the start of Fall and plummets during the coldest season, Winter. Despite the  season-wise variation of rider\u2019s number, the number of casual riders is consistently higher than that of member riders under all scenarios. \n5. After thoroughly analyzing the locations of the top ten start or end stations most visited by casual riders in Chicago, it was discovered that almost all of them are closer to reactional centers like theatre, amphitheater, gymnasium,  et cetera.\n\n## B. Recommendations\nBased on the insights derived from the data, the following three marketing strategies or approaches are recommended in order to convert casual riders to annual member riders.\n1. Intensify promotional\/advertising works in reactional centers like theatre, amphitheater, gymnasium, cafeterias, restaurants, ... situated nearby most visited start or end stations.\n2. Introduce seasonal membership discounts (good discount during the Summer would be so encouraging) and weekend-based (especially on Sundays, Fridays, & Saturdays) riding price discount for members. \n3. Create partnership with service centers nearby most visited stations so as to reward newly joining members with free gymnasium, restaurant, or theatre tickets.","f276a2b0":"### Merged bike_dataset, read it in using Python\n- 5,136,261 rows and 14 columns\n- The column named \"month\" was added during the merging process\n- Before merging, the datasets appeared like what ensues:\n\n            ['202101-divvy-tripdata.csv',\n             '202010-divvy-tripdata.csv',\n             '202109-divvy-tripdata.csv',\n             '202012-divvy-tripdata.csv',\n             '202108-divvy-tripdata.csv',\n             '202106-divvy-tripdata.csv',\n             '202102-divvy-tripdata.csv',\n             '202104-divvy-tripdata.csv',\n             '202011-divvy-tripdata.csv',\n             '202107-divvy-tripdata.csv',\n             '202103-divvy-tripdata.csv',\n             '202105-divvy-tripdata.csv']","53cf34a5":"#### B8: Remove the values of the new column \"riding_duration\" which are negative\n- Under normal conditions, the start time can never be less than the end time; hence, any negative value when (ended_at - started_at) is computed is abnormal. It should be the result of a system fault or data entry problem. \n- Under normal scenario, the following condition must be met.\n\n            riding_duration <- (ended_at - started_at)\n            riding_duration >= 0\n- A shown in the cell below, there are 3304 rows where the value of riding_duration is negative! All these rows were removed from the dataframe.","ec23c954":"<div class=\"alert alert-block alert-warning\">","94caafe1":"<div class=\"alert alert-block alert-warning\">","069ea8a9":"# Case Study Roadmap - Prepare\n### Guiding questions and answers\n- Where is my data located?    \n    - Source URL: https:\/\/divvy-tripdata.s3.amazonaws.com\/index.html\n    \n- How is the data organized?\n    - I am using the last 12 months data of biking and they are stored on the site month wise. After downloading the most recent 12-month data (October, 2020 to September, 2021), they are mereged into a single, big dataset, named \"bike_data_Oct2020_to_Sept2021_v01.csv\". It is a structured data that comprises 5,136,261 rows and 14 columns.\n- Are there issues with bias or credibility in this data? Does your data ROCCC?\n    - There is sufficiently large data which is reliable (R) (proven fit for use and unbiased), original (O), comprehensive (C) (contains all critical information needed to answer the questions raised), current (C) and relevant to the task, and cited (C) by many.\n    - Hence, yes the data I have at hand ROCCCs!\n- How am I addressing licensing, privacy, security, and accessibility?\n    - Accessible with clearly declared use policy.\n    - The data has been made available by Motivate International Inc. under the license described in the page whose link is given below:\n        - https:\/\/www.divvybikes.com\/data-license-agreement\n- How did I verify the data\u2019s integrity?\n    -  By verifying its overall accuracy, completeness, and reliability\n- How does it help me answer my question?\n    - It contains information about the riding behavior of casual and annual member riders and it definitely helps us answer the question of how casual riders can be converted into member riders to make the company more profitable.\n- Are there any problems with the data?\n    - On the dataset, the \"start_station_id\" and \"end_station_id\" values are deliberately anonymized for the privacy of riders; hence, analysis based on these attributes cannot be made.    \n    - I also observed inconsistent station names and data-types (especially dates), missing stations, and inscosistent naming under the rideable_type ('electric_bike', 'classic_bike', and 'docked_bike'). The 'classic_bike', and 'docked_bike' mean the same thing. In older data (beyond December, 2020), it is writtent as \"docked_bike; however, this has been changed to \"classic_bike\" starting December, 2020. \n    - Cleaning, filtering, and column renaming are performed at the \"Process\" phase, just in the next step.\n\n### Key tasks\n1. Download data and store it appropriately.\n    - 12 datasets (October 2020 to September 2021) were downloaded and merged into a single big dataset, named \"bike_data_Oct2020_to_Sept2021_v01.csv\"\n2. Identify how it\u2019s organized.\n    - Structured and comprises 5,136,261 rows and 14 columns\n    - Renamed as \"bike_data_Oct2020_to_Sept2021_v01.csv\"\n3. Sort and filter the data.\n    - Unnecessary columns like start_lat, start_lng, end lat, and end_lng were filtered out!\n4. Determine the credibility of the data.\n    - Overall reliable, original, comprhensive, consistent, and cited datasets they are!\n\n### Deliverable\n- A description of all data sources used  \n    \n### Python packages and modules Neccessary for this project are Imported below:","036440f1":"![prepare2.png](attachment:233a3585-1fa1-41c5-baa8-84610e5b741b.png)","2853a2bf":"<div class=\"alert alert-block alert-warning\">","be56dc34":"- Normal\/non-skewing Range of Riding Durations","0cb1aaaa":"### 7. Correlation between Days of the Week and Number of Rides\n- In most cases, Saturdays and Sundays have the highest number of riders; especially during Spring and Summer the number is way higher. Friday has the next highest numbers of riders. \n- However, there is no meaningful correlation between the days of the week and number of rides. The conclusion here is that number of rides are higher during the weekend in most cases (rain or shine)","733752e7":"![ask2.png](attachment:755db52b-0ad5-4106-b5a9-7c3ac3464056.png)","6e8274d5":"#### B2: Perform Filtering\n-  Columns start_station_id, end_station_id, start_lat, start_lng, end_lat, end_lng, and ride_id are filtered out as follows!","67883791":"Now that my data is stored appropriately and has been prepared for analysis, it is time I start putting it to work. I have used the following Case Study Roadmap as a guide to successfully perform the analysis phase.\n\n# Case Study Roadmap - Analyze\n#### Guiding questions\n- How should the data be organized to perform analysis on it?\n- Has the data been properly formatted?\n- What surprises were discovered in the data?\n- What trends or relationships were found in the data?\n- How will these insights help answer the company's business questions?\n\n#### Key tasks\n1. Aggregate the data so it\u2019s useful and accessible.\n2. Organize and format the data.\n3. Perform calculations.\n4. Identify trends and relationships.\n\n### Deliverable\n- A summary of my analysis","3fb047c8":"\n## Analysis\n### 1. Overview of dataframe","f21709e6":"#### B12: Outliers Detection in the \"riding_duration\" column\n- 378674 outliers, they lie above the upper boundary, beyond $Q_3 + 1.5\\times IQR$\n- These outliers have the potential to skew the analysis result; hence, they are removed!\n\n1) Descriptive Statistics: using min(), mean(), and max()\n- As illustrated below, the maximum difference between start time and end time is way much farther from the average value than minimum difference is.","ebf7cf0d":"<div class=\"alert alert-block alert-warning\">","a2a37a0d":"# Case Study: How Does a Bike-Share Navigate Speedy Success?","a6ab731d":"### 1. Number of Casual Riders vs Annual Member Riders","046ec66b":"<div class=\"alert alert-block alert-warning\">","c24e4af7":"### Columns\/Attributes","c70ec960":"### 6. Average Minutes of rides on each day of the week for members and casual riders","63422d7c":"## Introduction\nWelcome to the Cyclistic bike-share analysis case study! In this case study, I will perform many real-world tasks of a data analyst. I will work for a fictional company, Cyclistic, and meet different characters and team members. In order to answer the key business questions, I will follow the steps of the data analysis process: **ask, prepare, process, analyze, share, and act**. I will borrow Google Analytics Certificate Course's icons\/pctures to represent each of these steps in what ensues. ","0388a0ba":"![me.jpg](attachment:f9bc6a45-3cce-46db-9cd7-142cd8bcfce3.jpg)","092707ed":"3) Determination of outliers using minimum and maximum bounds\n- Lower and upper bounds are computed by using the first and thrid quartiles along with 1.5 times of the inter quartile range ($IQR$).\n    - $IQR = Q3 - Q1$\n    - $lower\\_bound = Q1 - 1.5\\times IQR$\n    - $upper\\_bound = Q3 + 1.5\\times IQR$\n- In other words, all observations outside of the following interval ($I$) will be considered as potential outliers:\n\n$$I = [Q_1 - 1.5\\times IQR: Q_3 + 1.5\\times IQR]$$","2ae950b7":"## By: Alem Haddush Fitwi\n    91 Endicott Ave, Johnson City, NY 13790\n    Mobile Phone Number: (607) 232-1056  \n    E-mail: ahfitwi@gmail.com\/afitwi1@binghamton.edu","fe507204":"### 8. Correlation between Days of the Week and Number of Rides\n- Found no correlation; it is almost zero as shown below!","90cb7336":"- Averages of each week in the observation period","5cf59cfc":"#### B4: Fix values of the 'start_station_name' column\n- Make the following change because the '(Temp)' thing is causing some inconsistency:\n\n       'Halsted St & 18th St (Temp)' --> 'Halsted St & 18th St'","ef4525d1":"#### 2. Number of Anual-member riders vs Number of casual riders\n- Of all the riders in the cleaned data, 42.7% are casual members and 57.3% are annual members","28cbab3d":"### B. Cleaning dataframe \"bike_cleaned_df\" using Python-Tools\n#### B1: Create a dataframe, which is a separate  copy of the oginal one\n- Make deep copy of bike_df to bike_cleaned_df as ensues:\n\n        bike_cleaned_df = bike_df[bike_df.columns]       ","8ed509e2":"#### Dimensions: Rows and Columns","c038b938":"<div class=\"alert alert-block alert-warning\">","efea244c":"![share2.png](attachment:e80ae9ed-3571-48ff-9dd1-f8ed414da2c1.png)","9a8a0674":"### A. Dataset Cleaning Log\n- As observed from the structures of the dataset displayed in the previous section and thorough investigation, the dataset has some inconsistencies and it has to be cleaned before it can be employed for anlysis to answer the main question of this project.\n\n#### A0: Column \"month\" was Added During the Merging Process in previous step\n- While merging the 12 datasets, their respective month values were added as a new column.\n\n#### A1: Filtering\n- Filter out unnecessary columns and create a new dataframe with columns of interest only!\n    1. Delete the anonymized attributes\/columns, namely \"start_station_id\", and \"end_station_id\". They are of no use in this anlysis. \n    2. The columns named start_lat, start_lng, end lat, and end_lng offer no information useful for the analysis; as a result, they were filtered out. \n    3. The column named ride_id is also of no importance and removed from the dataset \n\n#### A2: Fixing the Values of column named \"rideable_type\"\n- This column has three values: 'electric_bike', 'classic_bike', and 'docked_bike'. However, they are supposed to be two. The 'classic_bike', and 'docked_bike' mean the same thing. The 'classic_bike' was introduced starting December, 2020; whereas the 'docked_bike' was in use before December 2020.\n- Hence, 'docked_bike' is changed to 'classic_bike' under this column.\n\n#### A3: Correcting Inconsistent Start Station Names (column: 'start_station_name') and Handling Missing Names\n- Under the 'start_station_name column, the following station names contain \"(Temp)\".\n\n            {'California Ave & Francis Pl (Temp)',\n             'Franklin St & Adams St (Temp)',\n             'Halsted St & 18th St (Temp)', --> Problem noticed here!\n             'Pulaski Rd & Eddy St (Temp)',\n             'Wentworth Ave & 24th St (Temp)',\n             'Wood St & Taylor St (Temp)'}\n- It causes inconsistency in 'Halsted St & 18th St'. The same station is named differently; hence, the substring \"(Temp)\" and trailing white spaces are cleaned for consistency's sake.\n             \n             'Halsted St & 18th St',\n             'Halsted St & 18th St (Temp)' --> 'Halsted St & 18th St'\n- Twenty (20) rows of the dataset contain {'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)'}. I feel like the staff of Divvy performed some testing. Even the name implies so. Hence, I opted to remove these 20 rows from the dataset.\n\n                ['HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)',\n                 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)']\n\n- Missing station names are filled with \"not_provided\"\n                 \n#### A4: Add New columns named \"riding_duration\" and \"day_of_week\"\n- No riding duration of each client (casual or member) is provided in the dataset. Hence, I opted to add this column for it is vital in the analysis process. It is the difference of the start and end times of every ride in the dataset.\n\n                riding_duration <- ended_at - started_at\n                \n- Besides, a column named \"day_of_week\" is added. Its values are extracted from the \"started_at\" column.\n\n                day_of_week <- day from started_at\n                \n#### A5: Removing Datapoints whose riding durations are in the outlier region\n- Multiple rows contain riding durations well above the upper boundary\n- Hence, these outliers removed to make sure the enalysis result is not skewed.\n                \n#### A6: Changing column names to something that makes more sense\n- Some of the attributes' names of the dataset don't make sense. I had struggled to make sense of the 'rideable_type' and 'member_casual' columns until I checked their contents. \n- Hence, for better and prompt understanding of the values they represent, I have changed these column names to bicycle_type and client_type, respectively, as depicted in what ensues.\n\n            'rideable_type' --> 'bicycle_type',  and \n            'member_casual' --> 'client_type' \n            \n#### A7: Cleaning was performed on the copied version of the original dataset \n- I made sure that the original and copy don't have the same memory addresses. They shouldn't indicate to the same memory location! \n\n            bike_cleaned_df <- bike_df","5331e2e5":"#### 8. Correlation between Season and Number of Rides\n- The scatter plot portrayed in the figure below shows an upward trend starting from the end of Winter to Summer.\n- There exists a strong correlation between the seasons and the number of rides. It is computed to be **0.58** in te analysis phase. \n- In conclusion, there is an uptrend correlation as we move from the end of winter to summer, and conversely","a95f1088":"- Member Riders' Obseravtions\/Datapoints with Unamed  End Stations, labeled as \"not_provided","46640bc7":"### 3. Total 12-month rides in minutes for each group\n- Member riders rode for **33, 130, 825.0** minutes over the period of the last 12 months\n- Casual riders rode for **33, 714, 643.0** minutes over the period of the last 12 months","a373f2c8":"#### B9: Remove the rows where the 'start_station_name' and 'end_station_name' indicate to the same place and the \"riding_duration\" is significantly small.\n- Probably a duration of one minute or less may mean that the riders changed their minds and dock the bicycle back. As a result, if the 'start_station_name' and 'end_station_name' indicate to the same place and the \"riding_duration\" is less than or equal to one minute (60 seconds), I opted to drop the corresponding data-point or observation from the dataset.\n- Example:\n            \n      classic_bike 2021-01-17 10:50:25 2021-01-17 10:50:33 State St & Pearson St   State St & Pearson St   member 2021-01 0.13           \n","129823ed":"### <font color=\"red\">Voila! A good beginning makes a good ending!<\/font>","c3a2a41a":"## Scenario\nI am data analyst working in the marketing analyst team at Cyclistic, a bike-share company in Chicago. The director of marketing believes the company\u2019s future success depends on maximizing the number of annual memberships. Therefore, my team wants to understand how **casual riders and annual members** use Cyclistic bikes differently. From these insights, my team will design a new marketing strategy to convert casual riders into annual members. But first, Cyclistic executives must approve our recommendations, so they must be backed up with compelling data insights and professional data visualizations.","f3aab2c5":"2) Using Histogram\n- Another basic way to detect outliers is to draw a histogram of the data.\n    - However, it gives no clear clue in this case\n    \n3) Boxplot\n- Boxplot is useful in detecting potential outliers.\n- A boxplot helps to visualize a quantitative variable by displaying five common location summary (minimum, median, first and third quartiles and maximum)","09719c9a":"- Make sure the original and the copied dataframes point to different memory locations. The following comparison has to produce a False result","a674c7e5":"### 4. Daily Average of the rider groups","830a0477":"<div class=\"alert alert-block alert-warning\">","6397b51f":"- Befre cleaning was made;","494c95b8":"<div class=\"alert alert-block alert-warning\">","ac82784e":"#### B3: Under \"rideable_type\" column, change 'docked_bike' to 'classic_bike'\n- Effectively changed to \"classic_bike\". As shown below, there are only 'electric_bike' and 'classic_bike' under this column after cleaning was performed.","0c4aa944":"### 6. Average Minutes of riding per day of the week: Member vs Casual Riders\n- As clearly depicted in the line graphs and barplots below, the daily average rides in minutes of the casual riders are consistently higher than that of annual member riders over the whole period of observation (last 12-months).\n- Besides, the average daily riding durations are much higher during the weekend.","42b7d813":"#### By Casual Riders\n- 226, 503 observations have no named start stations of casual riders\n- 255, 603 datapoints have no named end stations of casual riders\n- Totally, 482, 106 rows don't have named start or end stations of casual riders","73ce087b":"## Summary:\nA seasoned Engineer with a wealth of experience in Electrical & Computer Engineering projects, Computer Networking and Security Design & Installation\/Configuration, Software Development and Management, Business Intelligence, Data Analytics, Data Engineering, Data Science and Machine\/Deep Learning. I have excellent data processing skillsets and I tend more toward Data Science or Data Analytics using Python3, R, SQL, Machine Learning and PySpark (Spark SQL, Spark Streaming, Spark ML\/MLlib, ETL Pipelines, and Full Load and Replication on going) for faster data processing using cluster or parallel computing! Besides, I am very good at fullstack web and Dashboard developments using HTML, CSS, & JavaScript for frontend coding, Python3 for backend coding, & MySQL\/OracleDB\/PSQL for database programming and management. As described in what ensues, I have also successfully completed the Google Data analytics Certificate Course just to refresh and make myself more job-ready.\n\n## Professional & Project Experience:\n### Project Research Assistant  (May 2021 - Todate) \n    @ SUNY Research Foundation, Binghamton University\n- **Major Tasks Performed**:\n    - Developed and implemented a Distributed Network Communication Algorithm for the implementation of the \"Distributed Collaborative Orthogonal Matching Pursuit Thresholding Algorithm (DC-OMP-TA)\" based on a hybrid of peer-to-peer and client-server architectures. It was developed through the seamless integration of Python3 based communication application and a MATLAB implementation of the DC-OMP-TA algorithm. It was successfully tested on a single PC with multiple threads of CLI running on it to emulate multiple machines and in real distributed environemnt, on more than three distributedly connected machines. A report\/manual comprising more than 60 pages was generated, as well. \n    - Developed social distance and crowd density estimator using Deep Learning and Python3 for Technergetics. Human Detection, Distance and Area estimation algorithms were designed, developed and implemented. Out of it, a 12-page article was submitted for publication. The entire documentation is provided on my Github whose link is provided here: https:\/\/github.com\/ahfitwi\/SDProject2021\n    \n- **Tools Employed**:\n    - Python for video dataset processing, for building Deep-learning Models, for implementing the designed algorithms, for network programming, and file managemnet.\n    - MATLAB: for the implementation of DC-OMP-TA \n\n### Graduate Assistant-Data Analyst (August, 2017 -  May, 2021): \n        @ Dean's Office of Watson College of Engineering & Applied Science, Binghamton University\n- **Major Tasks Performed**:\n    - Industry grade dashboards and database designs and developments using Python and MySQL\n    - Business Intelligence (BI), and Content Analysis \n    - Data Science and Analysis on such input datasets as 15-year and semester-wise Student Opinion of Teaching (SOOT), Historical and current Admission Data, and 25 years research data. \n    - The size of datasets processed typically ranges from 1MB to 5GB. \n    - The outputs of the analysis were mainly actionable data and insightful reports (in CSV, EXCEL, SAS, PDF, HTML, and RTF\/WORD formats as requested) and graphical plots (line graphs, bar charts, pie charts, and scatter plots) generated using SAS, R, SQL, Python, and  Machine Learning\/Deep Learning algorithms. \n    - Preparing Surveys on Qualtrics, Performing Descriptive and Inferential Statistical Analysis using Python3 and R programming languages, Testing Hypothesis, and Determining confidence Intervals.\n    \n- **Tools employed**:\n    - PySpark (Spark SQL, Spark Streaming, Spark ML\/MLlib, ETL Pipelines, and Full Load and Replication on going)\n    - SAS for data analysis, visualization, generating aesthetic reports, and statistical analysis\n    - Python3 for backend development, dashboard development using dash and plotly, data analysis, machine learning based model building, & content analysis\n    - SQLite\/MySQL\/ Oracle SQL for designing, creating, & developing Databases and for creating queries and subqueries.\n    - CSS\/HTML\/js for the frontend development of a Dashboard\n    - Microsoft Excel: For cleaning and analyzing small datasets and generating visualization\n    - Google Sheets: For cleaning and analyzing small datasets and creating visualization.\n    - R for statistical analysis and visualization\n\n### Director of Information & Communication Technology Directorate (October 2012 - October 2015):\n    @ Mekelle University, Mekelle, Tigrai, Ethiopia\n- Datacenter: managed to build modern, tier-III datacenter for Mekelle University \n    - Very good at Designing Datacenter infrastructures, ERP\/CAN Networks, & Configurations\n- University ERP: managed to get the Mekelle University business processes automated \n    - Backend: Ruby On Rails, DB: MySQL, & Frondend: HTML, CSS, & Java Script\n\n### Mega Wind Farm Projects (Ashegoda and Adama-II): \n- successfully took part in two mega wind farm projects as Electrical & SCADA Engineer in the design, supervision, data analysis, and commissioning processes. \n- successfully developed data analysis and machine-learning-based forecasting models.\n- successfully worked as Electrical Engineer (September, 2012 to June, 2014) in a 120 MW Wind Farm project (Ashegoda, Mekelle, Ethiopia) and as a SCADA Engineer (July 01, 2013 to December 15, 2015) in another 153 MW Wind Farm Project (Adama, Ethiopia).\n\n###  Teaching Career (September 2006 - August 2017): \n    @ Mekelle University, Mekelle, Tigrai, Ethiopia\n- Worked as Graduate Assistant II, Assistant Lecturer, Lecturer, & Assistant professor.\n- Major Tasks: Teaching Computer Engineering Courses, Conducting Lab activities, carrying out research, giving community services, and software development.\n\n###  Research:\n- Areas: Privacy and Cyber Security, Video Surveillance, Lightweight Machine Learning\/Deep-learnig, Edge\/Fog\/Cloud Computing, Network Architecture, Distributed Computing, and IoTs \n- Tools & Hardwares: Statistical Methods, Machine\/Deep Learning Models, Scrambling\/Encryption Schemes, Python3, C++, SQLite, SQL, Edge and cloud servers, vSphere-based virtual machines, and Edge Devices (Raspberry PI 4, Cameras, Sensors, ...)\n- Publications: Authored and published more than 20 articles and conference papers and a bookchapter in repurable journals; not to mention the numerous unpublished teaching, training, and project materials (manuals, reports or documents) produced to date. \n\n### Leadership:\n- **Information & Communication Technology Director** (October, 2012 - October 26, 2015): managed the ICT directorate of Mekelle University (Ethiopia) for 3 years, responsible for running the entire university (has a population of more than 35, 000 students and staff) campus area network and ERP. The directorate had 6 departments and 106 IT staff members of various professions. \n- **Head and Program Coordinator** (2010 - 2012): Head of the Computer Engineering chair (11 staff members), and program coordinator of the department of Electrical and Computer Engineering (79 staff members), Mekelle University, for one year each. \n\n### Education:\n- Watson School of Engineering, Binghamton University, State University of New York (4400 Vestal Pkwy E, Binghamton, NY 13902, USA)\n    - PhD in Computer Engineering (Computer Engineering), Expected: December 2021 (Defended on November 17, 2021)\n       \n- Addis Ababa University (P.O.Box 1176, Addis Ababa, Ethiopia )\n    - M.Sc. in Computer Engigneering, Earned: September 2010\n\n- Bahir Dar University (P.O.Box 79, Bahir Dar, Ethiopia)\n    - B.Sc in Electrical & Computer Engineering, Earned: July 06, 2006\n\n### Technical and Programming Skills:\n1. **Excellent Data Processing Skill Set**: Super good in creating surveys on Qualtrics, Data Processing, Data Analyzing and Visualizing, Hypothesis Testing, and genrating reports helpful for informed decision making.\n2. **Advanced Python Programming + SQL + Machine Learning**: For both academic and professional purposes since 2014, mainly for Data Science and Analysis (Numpy, Pandas, Scipy, Scikit Learn, Seaborn,  Matplotlib, Plotly and Dash), for Deep Learning (Lightweight DNN for Conserving Privacy in Edge Devices as part of my PhD research), Network and Network Security Programming (Device Configurations, Extensive String Manipulations using RE, DNS Activity Monitoring, and Network Scan Detection projects), and industry grade dashboard developments.\n3. **PySpark (Spark SQL, Spark Streaming, Spark ML\/MLlib, ETL Pipelines, and Full Load and Replication on going)**: Excellent in PySpark for big data processing and real-time streaming applications. Very good data engineer.\n4. **R Programming**: Very good in R for data cleaning, organizing, analyzing, and static and dynamic(dashboard) visualization using a bunch of packages (tidyverse, dplyr, ggplot2, lubridate, data.table, tidyr, shiny, plotly, mlr3, ...)\n5. **Fullstack Web Development**: Very good at fullstack web development using HTML, CSS, & JavaScript for frontend coding, Python for backend coding, and MySQL (or Oracle DB, MangoDB, etc) for databases programming and management.\n6. **SAS Programming + SQL**: For data analysis (data preprocessing, organizing, transforming, mining, visualization, and reporting in xlsx, pdf, & rtf formats). I have used it since August  2017, week in, week out. \n7. **Spreadsheet or googlesheets**: Excellent in data cleaning, organizing, analysis, visualization, and reporting using Microsoft Excel and Googlesheets\n8. **Network Administration & Security**: Very good expertise and experience in IT Networks and Operational Networks (SCADA) design, installation, and administration. I had worked as Network Admin, SCADA Engineer, & ICT Director, and had been trained and certified in Network Administration, Data Center Infrastructures design and construction, and Network Security. I am also very good in network programming using Python.\n9. **MATLAB**: very good at programming using MATLAB. I have mostly used it for implementing works based on convex optimization in conjuction CVOPT, for implementation distributed collaborative orthogonal matching pursuit thresholding algorithm (DC-OMP-TA), drone flight simulations, and other arrays of complex and mathy projects and research tasks.\n10. **Other Programming Skills**:\n    - **Java**: I used to be an expert in it and I had developed many desktop and web apps using it. But I haven't used it since 2013.\n    - **PHP**: for backend web app development. I last used it in 2015.\n    - **C++**: I learnt this languange long ago. I still occasionally use it for research, in experimental activiteis where speed is of great essence.\n    - **VHDL**: I had frequently used it for academic and research purpose until 2015. Most recent use was in the Fall of 2018 for a VLSI class.\n    - **C**: the first programming language I learnt as a young boy. I was very good at it but I haven't used it for a while.","0474e169":"#### 9. Correlation between Season and Number of Rides\n- Seasons in relation to North America, specifically Chicago:\n    - Winter := {December, January, February}, season_code := 1\n    - Spring := {March, April, May}, season_code := 2\n    - Summer := {June, July, August}, season_code := 3\n    - Fall := {September, October, November}, season_code := 4\n- There exists a strong correlation between the seasons and the number of rides. It is computed to be **0.58** (illustrated below). As a result, there is an uptrend correlation as we move from the end of winter to summer, and conversely","d0b7a936":"#### Read\/load Dataset","29950538":"#### B11: Rename 'rideable_type' and 'member_casual' columns as follows:\n\n            'rideable_type' --> 'bicycle_type',  and \n            'member_casual' --> 'client_type' ","433c69e9":"#### B6: Remove Rows containing 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)' under column name \"start_station_name\"\n- Remove each row from bike_cleaned_df, which satisfies the following condition:\n\n        bike_cleaned_df['start_station_name'] == 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)'\n- These data-points appear to be testing data, not actual ones. 20 such rows\/obserations were removed as follows:","0a7e0dd1":"## Characters and teams\n- **Cyclistic**: A bike-share program that features more than 5,800 bicycles and 600 docking stations. Cyclistic sets itself apart by also offering reclining bikes, hand tricycles, and cargo bikes, making bike-share more inclusive to people with disabilities and riders who can\u2019t use a standard two-wheeled bike. The majority of riders opt for traditional bikes; about 8% of riders use the assistive options. Cyclistic users are more likely to ride for leisure, but about 30% use them to commute to work each day.\n- **Lily Moreno**: The director of marketing and my manager. Moreno is responsible for the development of campaigns and initiatives to promote the bike-share program. These may include email, social media, and other channels.\n- **Cyclistic marketing analytics team**: A team of data analysts who are responsible for collecting, analyzing, and reporting data that helps guide Cyclistic marketing strategy. I joined this team six months ago and have been busy learning about Cyclistic\u2019s mission and business goals \u2014 as well as how I, as a data analyst, can help Cyclistic achieve them.\n- **Cyclistic executive team**: The notoriously detail-oriented executive team will decide whether to approve the recommended marketing program.","2df1a0dd":"#### B13: Deep copy the cleaned dataframe to another one as follows:","2066d225":"- **Major Takeaways of the Analysis Phase**:\n    1. Casual Riders have better daily, weekly, and monthly average rides than member riders.\n    2. There is a strong correlation between seasons of the year and number of rides.\n    3. The number of rides is higher during the weekend than on any of the weekdays, almost throughout the entire observation period!","b3c39d63":"# Case Study Roadmap - Process\n### Guiding questions\n- What tools are you choosing and why?\n    - To clean, analyze, and visualize this big dataset with ease, I have chosen to use Python-programming which is an ideal tool when it comes to data wrangling, analysis, visualization, or other general purposes. It provides numerous packages that make data wrangling, analysis, and visualization so simple. The pandas and matplotlib packages are extensively employed in this project.\n- Have you ensured your data\u2019s integrity?\n    - Yes, the overall accuracy, completeness, and reliability of the dataset were verified\n- What steps have you taken to ensure that your data is clean?\n    - Through thorough investigation of the data, I identified the inconsistencies that exist in it and made all necessary cleaning on a copy of my dataset. Eventually, I made comparison between the editted copy and the original dataset to verify the effected changes.\n- How can you verify that your data is clean and ready to analyze?\n    - I made a manual and script-based comparison between the editted copy and the original dataset to verify the effected changes.\n    - All attributes of both datasets were compared and analyzed to make sure that only desirable changes were effected.\n- Have you documented your cleaning process so that you can review and share those results?\n    - Yes, I logged every cleaning made on the data. The cleaning process is described in ensuing subsections.\n\n### Key tasks\n1. Check the data for errors.\n    - Done! Erroneous parts were identified and cleaned appropriately \n2. Choose your tools.\n    - I chose Python-programming\n3. Transform the data so you can work with it effectively.\n    - Data was transformed into a form helpful for this analysis\n4. Document the cleaning process.\n    - Neatly documented \n    \n### Deliverable\n- Documentation of any cleaning or manipulation of data","9e626931":"<div class=\"alert alert-block alert-warning\">","d35fb705":"## About the company\nIn 2016, Cyclistic launched a successful bike-share offering. Since then, the program has grown to a fleet of 5,824 bicycles that are geotracked and locked into a network of 692 stations across Chicago. The bikes can be unlocked from one station and returned to any other station in the system anytime. \n\nUntil now, Cyclistic\u2019s marketing strategy relied on building general awareness and appealing to broad consumer segments. One approach that helped make these things possible was the flexibility of its pricing plans: single-ride passes, full-day passes, and annual memberships. Customers who purchase single-ride or full-day passes are referred to as casual riders. Customerswho purchase annual memberships are Cyclistic members.\n\nCyclistic\u2019s finance analysts have concluded that annual members are much more profitable than casual riders. Although the pricing flexibility helps Cyclistic attract more customers, Moreno believes that maximizing the number of annual members will be key to future growth. Rather than creating a marketing campaign that targets all-new customers, Moreno believes there is a very good chance to convert casual riders into members. She notes that casual riders are already aware of the Cyclistic program and have chosen Cyclistic for their mobility needs.\n\nMoreno has set a clear goal: Design marketing strategies aimed at converting casual riders into annual members. In order to do that, however, the marketing analyst team needs to better understand how annual members and casual riders differ, why casual riders would buy a membership, and how digital media could affect their marketing tactics. Moreno and her team are interested in analyzing the Cyclistic historical bike trip data to identify trends.","63af21ce":"### 4. Weekly Average Rides of Casual Riders vs Member Riders in Minutes\n- As illustrated in the barplot below, the monthly average rides in minute of the casual riders is higher than that of annual member riders throughout the period of observation.","3e217cc0":"### 10. Top Ten Stations Most Visited by Casual Riders\n- Unnamed start or end stations were counted out in this analysis"}}