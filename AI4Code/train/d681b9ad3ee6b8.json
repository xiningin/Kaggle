{"cell_type":{"8a126be9":"code","fbfc7861":"code","70f32b14":"code","cf691be9":"code","f66b331d":"code","c1dc33dc":"code","94b8a8b3":"code","8375e700":"code","5a193320":"code","4f3271eb":"code","2a122d94":"code","02be70a2":"code","ad6a1ed8":"code","54e0b792":"code","a657bf85":"code","e09d9c24":"code","cea76750":"code","d105b3a8":"code","bbb6d86c":"code","4eec025b":"code","cfd44736":"code","531e52d5":"code","abe4b87e":"code","68951d71":"code","0a7e1656":"code","7ef54390":"code","5b682447":"code","9d9b872b":"code","03fd7c75":"code","b6f83365":"code","6897fcc1":"markdown"},"source":{"8a126be9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fbfc7861":"import tensorflow as tf\nimport matplotlib.pyplot as plt","70f32b14":"from tensorflow.keras.datasets import imdb\nfrom tensorflow.keras import models, layers, optimizers","cf691be9":"(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) \n# most frequent words which occurs 10000 will be retrieve","f66b331d":"print('Words: ', train_data[0])\nprint('Labels: ', train_labels[0])\n\n# Tokenization performed that's why there are numbers","c1dc33dc":"word_index = imdb.get_word_index()\nprint(word_index)","94b8a8b3":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])","8375e700":"reverse_word_index[1]","5a193320":"review_at_0 = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\nprint(review_at_0)\n# 0, 1, 2 is related to space, margin","4f3271eb":"def vectorize_sequences(sequences, dimension = 10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.\n    return results","2a122d94":"x_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)","02be70a2":"x_train.shape","ad6a1ed8":"print(x_train[0])","54e0b792":"y_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(test_labels).astype('float32')","a657bf85":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid')) # sigmoid returns the value between 0 and 1","e09d9c24":"# there should be no bottleneck\n'''\nmeans first layer should have larger output\nand last layer should have smaller\n'''","cea76750":"x_val = x_train[:10000]\npartial_x_train = x_train[10000:]\n\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]","d105b3a8":"print(x_val.shape)\nprint(partial_x_train.shape)","bbb6d86c":"model.compile(\n    optimizer='rmsprop', # gradient descent is also known as rmsprop\n    loss='binary_crossentropy', # loss func is coorelated to it's problem\n    metrics=['acc']\n)\n\nhistory = model.fit(\n    partial_x_train,\n    partial_y_train,\n    epochs=20,\n    batch_size=512,\n    validation_data=(x_val, y_val)\n)","4eec025b":"history.params","cfd44736":"history.model","531e52d5":"history.history","abe4b87e":"history_dict = history.history","68951d71":"loss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label = 'Training Loss')\nplt.plot(epochs, val_loss_values, 'b', label = 'Validation Loss')\nplt.title(\"Training and Validation Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\nplt.clf()\n\nacc_values = history_dict['acc']\nval_acc_values = history_dict['val_acc']\nplt.plot(epochs, acc_values, 'bo', label = 'Training Accuracy')\nplt.plot(epochs, val_acc_values, 'b', label = 'Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","0a7e1656":"model.predict(x_test)","7ef54390":"# Validation Loss is getting increase after 4 epochs where Training Loss is getting decrease in every epochs\n\n# Training Accuracy is getting increase in every epoch","5b682447":"'''So the epoch value should be 4'''","9d9b872b":"history = model.fit(\n    partial_x_train,\n    partial_y_train,\n    epochs=4,\n    batch_size=512,\n    validation_data=(x_val, y_val)\n)","03fd7c75":"history_dict = history.history\n\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label = 'Training Loss')\nplt.plot(epochs, val_loss_values, 'b', label = 'Validation Loss')\nplt.title(\"Training and Validation Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\nplt.clf()\n\nacc_values = history_dict['acc']\nval_acc_values = history_dict['val_acc']\nplt.plot(epochs, acc_values, 'bo', label = 'Training Accuracy')\nplt.plot(epochs, val_acc_values, 'b', label = 'Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","b6f83365":"model.predict(x_test)","6897fcc1":"### Building Model:"}}