{"cell_type":{"e6a2941a":"code","b7a747a3":"code","434f2c84":"code","cf49a49a":"code","b5d59d09":"code","7058a8ed":"code","0f0627bc":"code","94973ed2":"code","1f579f06":"code","751e014b":"code","db1cee4b":"code","6c86c258":"code","0210b06f":"code","545267f9":"code","dfcb9697":"code","2af54da3":"code","daa7bfc5":"code","dbc5690d":"code","fc4d5482":"code","bca0de80":"code","b34e1a41":"code","2c9f6dd0":"code","7ec4f2e5":"code","118fa66e":"code","c7f5f2f8":"code","c37afd29":"code","9ab7b900":"code","37a8ebc6":"code","1a7e0820":"code","870e2da6":"code","8dd65e19":"code","cc820d54":"code","bfeb37f7":"code","3d565250":"code","f59c9ed6":"code","429cade4":"code","73f6f392":"code","4bba2a85":"code","cc110931":"code","bb00eb22":"code","270b2742":"code","ba32f9ca":"code","611f4506":"code","33072d39":"code","54c1e883":"code","978b2c80":"code","161bc563":"code","62341ef8":"code","fdb74a26":"code","3f4090d1":"code","ecbea3fb":"code","83fa46f4":"code","54fd17ac":"code","b904e984":"code","7c7a9faa":"code","03eebed9":"code","603347b5":"code","4c1f93e6":"code","f10c65f5":"code","b44e6ed4":"code","9b4f7b7d":"code","ee3bc34f":"code","3f23dc24":"code","f4c9bd2c":"code","8c8c9333":"code","56db579b":"code","1d177dc0":"code","5471abff":"code","6eb8dee3":"code","c2aaea37":"code","38459671":"markdown","f8464d5f":"markdown","b9c13c81":"markdown","51e0d37c":"markdown","7f62035a":"markdown","de7c8d71":"markdown","0c91ab34":"markdown","9feba4be":"markdown","6a193ae6":"markdown","b7b94825":"markdown","59bb7374":"markdown","eb5330b9":"markdown","b91729ee":"markdown","774e647a":"markdown","972ab4b6":"markdown","bbd96f9f":"markdown","ee0656cb":"markdown","695b6252":"markdown","e9f4e1ec":"markdown","691e46a6":"markdown","dbecbfb4":"markdown","fff60ef6":"markdown","4c3ac91f":"markdown","f550231f":"markdown","311b9b8b":"markdown","67e586f4":"markdown","09f5dc20":"markdown","89bcb9f4":"markdown","9dcab8c9":"markdown","7a5a7a81":"markdown","62071230":"markdown"},"source":{"e6a2941a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b7a747a3":"pd.set_option('max_columns', None)\npd.set_option('max_rows', None)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nimport pandas as pd\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.special import boxcox1p\n\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import Lasso\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","434f2c84":"\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","cf49a49a":"display(df_train.sample(5))\ndisplay(df_test.sample(5))","b5d59d09":"# Original training and test data shape\n\nprint(\"Training data shape : {}\".format(df_train.shape))\nprint(\"Test data shape     : {}\".format(df_test.shape))","7058a8ed":"df_train.describe()","0f0627bc":"# Takin a glance at all the features present\ndf_train.columns","94973ed2":"df_train.info(verbose = False);","1f579f06":"missing_data = df_train.isna().sum()\nmissing_data = missing_data[missing_data > 0]\nmissing_data_sorted = missing_data.sort_values(ascending = False)\n\npercent_missing = df_train.isna().mean().round(4)*100\npercent_missing = percent_missing[percent_missing>0]\npercent_missing_sorted = percent_missing.sort_values(ascending = False)\n\ndf_missing = pd.DataFrame()\ndf_missing['Missing_Data'] = missing_data_sorted\ndf_missing['Missing_Percent'] = percent_missing_sorted\ndisplay(df_missing)\n\nf, ax = plt.subplots(figsize=(8, 7))\n \npercent_missing_sorted.plot.bar(color=\"b\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Percent missing\")\nax.set(xlabel=\"Misssing features\")\nax.set(title=\"Missing percent Visual\")\nsns.despine(trim=True,left=True)","751e014b":"# Store  and drop the Id column just incase needed later\ntrain_id = df_train['Id']\ntest_id  = df_test['Id']\n\ndf_train.drop(['Id'], axis=1, inplace=True)\ndf_test.drop(['Id'], axis=1, inplace=True)","db1cee4b":"# Explore the target that we want to predict\n\nprint('\\033[1m' + \"Description of Sale Price:\")\ndisplay(df_train['SalePrice'].describe());\n\nprint('      ')\n\n# Skew and kurt\n\nprint('\\033[1m' + \"Skewness and Kurtosis of Sale Price:\")\n   \nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())\n\n","6c86c258":"\nf, ax = plt.subplots(figsize =(15,8))\nsns.distplot(df_train['SalePrice'], rug = True, fit = norm)\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency_SalePrice\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"Distribution of SalePrice\")\nsns.despine(trim=True, left=True)\nplt.show();","0210b06f":"#So target's (SalePrice) distribution is right skewed so have to normalize target first.\n\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n\ndf_train['SalePrice'].describe()\nprint('      ')\n\n# Skew and kurt\n\nprint('\\033[1m' + \"Skewness and Kurtosis of Sale Price after normalization:\")\n   \nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","545267f9":"f, ax = plt.subplots(figsize =(15,8))\nsns.distplot(df_train['SalePrice'], rug = True, fit = norm)\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency_SalePrice\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"Distribution of SalePrice\")\nsns.despine(trim=True, left=True)\nplt.show();","dfcb9697":"def CategoryFeaturePlot(columns):\n    fig = plt.figure(figsize=(30,55))\n    \n    for i, column in enumerate(columns):\n\n       \n        plt.subplot(16,5, i+1)\n        \n        sns.scatterplot(x = column, y = df_train['SalePrice'], data = df_train,s = 80)\n        \n        plt.xticks(rotation = 90,fontsize=10)\n        plt.tight_layout()\n\n    fig.show()","2af54da3":"CategoryFeaturePlot(df_train.columns)","daa7bfc5":"corrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(15, 12))\nsns.heatmap(corrmat, vmin = 0,vmax=1, square=True, cmap = 'RdBu', annot = True, fmt = '.1f', \n            linecolor = 'black', center = 0,annot_kws={\"size\": 7},);\n","dbc5690d":"#If plot the Heat map for features with correlation more than 75% we get below plot \n\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(corrmat, vmin = 0,vmax=1, square=True, cmap = 'RdBu', annot = True,mask= corrmat < 0.75, fmt = '.1f', \n            linecolor = 'black', center = 0,annot_kws={\"size\": 7},);","fc4d5482":"Pair1 = ['OverallQual', 'SalePrice']\nPair2 = ['YearBuilt', 'GarageYrBlt']\nPair3 = ['1stFlrSF', 'TotalBsmtSF']\nPair4 = ['GrLivArea' , 'TotRmsAbvGrd']\nPair5 = ['GarageCars', 'GarageArea']","bca0de80":"sns.pairplot(df_train, vars = Pair1, kind = 'reg',diag_kind = 'hist');","b34e1a41":"sns.pairplot(df_train, vars= Pair2, kind = 'reg',diag_kind = 'hist');","2c9f6dd0":"sns.pairplot(df_train, vars = Pair3, kind = 'reg',diag_kind = 'hist');","7ec4f2e5":"sns.pairplot(df_train, vars = Pair4, kind = 'reg',diag_kind = 'hist');","118fa66e":"sns.pairplot(df_train, vars = Pair5, kind = 'reg',diag_kind = 'hist');","c7f5f2f8":"CategoryFeaturePlot(['GrLivArea'])","c37afd29":"# Removing any records is very costly but this one may help the model so remove this outlier\ndf_train.drop(df_train[(df_train['GrLivArea']>4500) & (df_train['SalePrice']<300000)].index, inplace=True)\ndf_train.reset_index(drop=True, inplace=True)","9ab7b900":"train_labels = df_train['SalePrice'].reset_index(drop=True)\ntrain_set = df_train.drop(['SalePrice'], axis=1)\ntest_set = df_test\n\n# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\nall_data = pd.concat([train_set, test_set]).reset_index(drop=True)\nprint(\"Shape of all data : {}\".format(all_data.shape))","37a8ebc6":"\n# PoolQC is categorical with the values (Ex,Gd,TA,Fa,NA) so  will fill the missing with 'None'\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n# MiscFeature is categorical with the values (Elev,Gar2,Othr,Shed,TenC,NA) so  will fill the missing with 'None'\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n# Alley is categorical with the values (Grvl,Pave,NA) so  will fill the missing with 'None'\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n# Fence is categorical with the values (GdPrv,MnPrv,GdWo,MnWw,NA) so  will fill the missing with 'None'\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n# FireplaceQu is categorical with the values (Ex,Gd,TA,Fa,Po,NA) so  will fill the missing with 'None'\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n# Median of Lot frontage for the whole neighbourhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n# categorical 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond' fill with None\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n # If no garage then no car\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n# Fill in 0 for no basement\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n    \n # Categorical 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2' fill with None\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n# Categorical MasVnrType fill with None\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n# Fill 0 if no area\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n# For MSZoning 'RL' is predominant so will take that for missing ones\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n# Categorical Utilities fill with NOne\nall_data[\"Utilities\"] = all_data[\"Utilities\"].fillna(\"None\")\n# For Functional if no data that means typical\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n\n# For Electrical 'SBrkr' is predominant so will take that for missing ones\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n\n# For KitchenQual 'TA' is predominant so will take that for missing ones\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n\n# For Exterior1st and Exterior2nd since very few missing will take most predominant one\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n\n# For SaleType 'WD' is predominant so will take that for missing ones\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n\n# For categorical MSSubClass fill with None\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","1a7e0820":"missing_data = all_data.isna().sum()\nmissing_data = missing_data[missing_data > 0]\nmissing_data_sorted = missing_data.sort_values(ascending = False)\n\nprint(\"Missing data :\".format(missing_data_sorted))","870e2da6":"all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","8dd65e19":"cat_cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')","cc820d54":"#apply LabelEncoder to categorical features\nfor col in cat_cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[col].values)) \n    all_data[col] = lbl.transform(list(all_data[col].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","bfeb37f7":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\nall_data['YrBltRemod'] = all_data['YearBuilt'] + all_data['YearRemodAdd']\nall_data['TotalBathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) +\n                               all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))\nall_data['TotalPorchSf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] +\n                              all_data['EnclosedPorch'] + all_data['ScreenPorch'] +\n                              all_data['WoodDeckSF'])\n\nall_data[\"LivLotRatio\"] = all_data['GrLivArea']\/all_data['LotArea']\n\nall_data[\"TotalOutsideSF\"] = sum((all_data['WoodDeckSF'],all_data['OpenPorchSF'],all_data['EnclosedPorch'], all_data['ScreenPorch']))\n","3d565250":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"Skewed features :\\n\")\n\nskewness = pd.DataFrame()\nskewness['Skew_value'] = skewed_feats\nskewness.head(10)","f59c9ed6":"skewed_features = ['MiscVal','PoolArea','LotArea','LowQualFinSF','3SsnPorch','LandSlope','KitchenAbvGr','BsmtFinSF2','EnclosedPorch','ScreenPorch']","429cade4":"\ndef CategoryFeaturePlot(columns):\n    fig = plt.figure(figsize=(23,7))\n    for i, col in   enumerate(columns):\n        plt.subplot(2,5, i+1)\n        sns.distplot(all_data[col],fit=norm, kde=False)\n        plt.tight_layout()\n    fig.show()","73f6f392":"CategoryFeaturePlot(skewed_features)","4bba2a85":"skewness = skewness[abs(skewness) > 0.70]\n\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n  \n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    ","cc110931":"normalized_features = ['MiscVal','PoolArea','LotArea','LowQualFinSF','3SsnPorch','LandSlope','KitchenAbvGr','BsmtFinSF2','EnclosedPorch','ScreenPorch']","bb00eb22":"CategoryFeaturePlot(normalized_features)","270b2742":"all_data = pd.get_dummies(all_data)\nprint(\"Shape of all data : {}\".format(all_data.shape))","ba32f9ca":"all_data.head()","611f4506":"# Remove any duplicate column names\nall_data = all_data.loc[:,~all_data.columns.duplicated()]","33072d39":"X = all_data.iloc[:len(train_labels), :]\nX_test = all_data.iloc[len(train_labels):, :]\nX.shape, train_labels.shape, X_test.shape","54c1e883":"#((1458, 228), (1458,), (1459, 228))","978b2c80":"#Validation function\nkf = KFold(n_splits=12, random_state=42, shuffle=True)\n\ndef rmse_cv(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_mean_squared_error\", cv=kf))\n    return rmse","161bc563":"# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n","62341ef8":"best_alpha = 0.00099\nmodel_lasso = make_pipeline(RobustScaler(),Lasso(alpha=best_alpha, max_iter=50000))","fdb74a26":"model_lgb = lgb.LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       subsample=0.8 , \n                       subsample_freq=4,  \n                       bagging_seed=8,\n                       colsample_bytree=0.2, \n                       feature_fraction_seed=8,\n                       min_child_weight=0.001, \n                       verbose=-1,\n                       random_state=42)","3f4090d1":"# XGBoost Regressor\n\nmodel_xgb = xgb.XGBRegressor (learning_rate=0.05,\n                       n_estimators=7200,\n                       max_depth=6,\n                       min_child_weight=1.5,\n                       gamma=0.0,\n                       subsample=0.2,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.9,\n                       reg_lambda=0.6,\n                       random_state=42)  \n","ecbea3fb":"# Support Vector Regressor\nmodel_svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))","83fa46f4":"# Gradient Boosting Regressor\nmodel_gbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  ","54fd17ac":"# Random Forest Regressor\nmodel_rf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)","b904e984":"\nregressors = (model_lasso,model_lgb,model_xgb,  model_svr, model_gbr, model_rf)\n\nstacking_reg = StackingCVRegressor(regressors=regressors,\n                                meta_regressor= model_lasso,\n                                use_features_in_secondary=True)\n                             ","7c7a9faa":"scores = {}\n\nscore = rmse_cv(model_lasso)\nprint(\"model_lasso: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['model_lasso'] = (score.mean(), score.std())","03eebed9":"scores = {}\n\nscore = rmse_cv(model_lgb)\nprint(\"model_lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['model_lgb'] = (score.mean(), score.std())","603347b5":"score = rmse_cv(model_xgb)\nprint(\"model_xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['model_xgb'] = (score.mean(), score.std())","4c1f93e6":"score = rmse_cv(model_svr)\nprint(\"model_SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['model_svr'] = (score.mean(), score.std())","f10c65f5":"score = rmse_cv(model_rf)\nprint(\"model_rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['model_rf'] = (score.mean(), score.std())","b44e6ed4":"score = rmse_cv(model_gbr)\nprint(\"model_gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['model_gbr'] = (score.mean(), score.std())","9b4f7b7d":"\nstacked_reg_model_fit = stacking_reg.fit(np.array(X), np.array(train_labels))\nstacked_train_pred = stacked_reg_model_fit.predict(X)\nstacked_pred = np.floor(np.expm1(stacked_reg_model_fit.predict(X_test)))\nprint(rmsle(train_labels, stacked_train_pred))","ee3bc34f":"lasso_model_fit = model_lasso.fit(X, train_labels)\nlasso_train_pred = lasso_model_fit.predict(X)\nlasso_pred = np.floor(np.expm1(lasso_model_fit.predict(X_test)))\nprint(rmsle(train_labels, lasso_train_pred))","3f23dc24":"lgb_model_fit = model_lgb.fit(X, train_labels)\nlgb_train_pred = lgb_model_fit.predict(X)\nlgb_pred = np.floor(np.expm1(lgb_model_fit.predict(X_test)))\nprint(rmsle(train_labels, lgb_train_pred))","f4c9bd2c":"xgb_model_fit = model_xgb.fit(X, train_labels)\nxgb_train_pred = xgb_model_fit.predict(X)\nxgb_pred = np.floor(np.expm1(xgb_model_fit.predict(X_test)))\nprint(rmsle(train_labels, xgb_train_pred))","8c8c9333":"svr_model_fit = model_svr.fit(X, train_labels)\nsvr_train_pred = svr_model_fit.predict(X)\nsvr_pred = np.floor(np.expm1(svr_model_fit.predict(X_test)))\nprint(rmsle(train_labels, svr_train_pred))","56db579b":"\nrf_model_fit = model_rf.fit(X, train_labels)\nrf_train_pred = rf_model_fit.predict(X)\nrf_pred = np.floor(np.expm1(rf_model_fit.predict(X_test)))\nprint(rmsle(train_labels, rf_train_pred))","1d177dc0":"gbr_model_fit = model_gbr.fit(X, train_labels)\ngbr_train_pred = gbr_model_fit.predict(X)\ngbr_pred = np.floor(np.expm1(gbr_model_fit.predict(X_test)))\nprint(rmsle(train_labels, gbr_train_pred))","5471abff":"print('Blended rmsle score : {}'.format(rmsle(train_labels, (stacked_train_pred * 0.40) + (lgb_train_pred * 0.15) + (gbr_train_pred * 0.15)  + (rf_train_pred * .09) + \n            (svr_train_pred * .08) + (lasso_train_pred * .07) + (xgb_train_pred * .06) )))","6eb8dee3":"ensemble = ((stacked_pred * 0.30) + (lgb_pred * 0.30) + (gbr_pred * 0.25)  + (rf_pred * .05) + \n            (svr_pred * .04) + (lasso_pred * .03) + (xgb_pred * .03))\n","c2aaea37":"submission = pd.DataFrame({'Id': test_id, 'SalePrice': ensemble})\n\nsubmission.to_csv('submission.csv',index=False)\n\nprint(\"Submitted successfully!\")","38459671":"OverallQual <-> SalesPrice \nYearBuilt <-> GarageYrBlt \n1stFlrSF <-> TotalBsmtSF \nGrLivArea <-> TotRmsAbvGrd \nGarageCars <-> GarageArea","f8464d5f":"**Create New Features**","b9c13c81":"**These features have good relation:**","51e0d37c":"**Generic descriptive statistics of training data set**","7f62035a":"**Normalize skewed features with boxcox1p**","de7c8d71":"**Explore the Target - Sale Price and Visualize the data distribution**","0c91ab34":"**Training and Test Data split**","9feba4be":"**Model Stacking**","6a193ae6":"* **Data Description :**\n   Explore data and features and get to know it better\n* **Missing Data :**\n   Check if any missing value that needs to be taken care\n* **Explore Target :**\n    Check and explore what the values look like in target that we are going to predict and normalize if needed\n* **Feature Engineering and Feature Exploration :**\n    Exploring and taking closer look at all the Features\n* **Multicollinearity Check :**\n    Explore and check feature relation\n* **Handling Outliers :**\n    If there is any outlier that needs adjusting or if need to be removed\n* **Handling Missing values :**\n    Imputing missing records by evaluating the feature\n* **Label and Categorical Encoding :**\n    To get features in same scale for better model performance and early model convergence\n* **Creating new Features :**\n    New features for better model accuracy\n* **Handling Skewed Features :**\n    Normalizing the features for better model performance.\n    Visualizing skewed features and after normalizing the features\n* **Model Building and Prediction :**\n    Model building and tuning","b7b94825":"**Visualize the distributions after normalization**","59bb7374":"**There are few Numerical look a like Features but actually Categorical so transform them as Categorical**","eb5330b9":"# Handling Skewed Features","b91729ee":"**Verify if still any missing record**","774e647a":"**Target - Sale Price is Right Skewed. Used log1p to Normalize**","972ab4b6":"**Model fitting and Prediction**","bbd96f9f":"# Multicollinearity check","ee0656cb":"**Feature Exploration**","695b6252":"**Explore Outliers : If take a closer look at GrLivArea it is clearly visible some outliers**","e9f4e1ec":"**Missing Data Exploration**","691e46a6":"**Visualize the top 10 skewed features distribution with a histogram and maximum likelihood gaussian distribution fit:**","dbecbfb4":"**Encoding Categorical features with dummy encoding**","fff60ef6":"**Categorical Features and Label Encoding**","4c3ac91f":"**Correlation matrix to see how features are correlated with each other and Sale Price**","f550231f":"# Model Building and Tuning","311b9b8b":"**A quick look at feature data types and data type grouped counts**","67e586f4":"**Blended rmsle**","09f5dc20":"**Look and feel of train and test data set**","89bcb9f4":"**Although these features look to have close relation, they are not enough to be removed. Let's take a closer look on these Five Pairs - Pair Plot**","9dcab8c9":"**Target variable (Sale Price) plot after Normalization**","7a5a7a81":"**Missing value Imputation**","62071230":"**Feature Engineering and combining train and test data for better feature engineering**"}}