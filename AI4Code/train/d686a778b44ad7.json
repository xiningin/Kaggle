{"cell_type":{"3489e34b":"code","ee4d844d":"code","7240c084":"code","5de703bd":"code","6b86bb43":"code","3939d619":"code","6a893d1c":"code","f5f38e0d":"code","02be01d1":"code","af34dbee":"code","b3cd7b17":"code","0f354a4e":"code","a2eed013":"code","9e543d41":"code","699208db":"code","c2f8bb79":"code","491d0cc5":"code","ac8b06c1":"code","8542d5fa":"code","8973745e":"code","f3419626":"code","c2b547d5":"code","7dc699cd":"code","e318eea8":"code","93439f96":"code","86bdfed7":"code","3f6b6ec9":"code","9d044cde":"code","a9664ff3":"code","74ccfd44":"code","7157bbbd":"code","03352775":"code","084fd468":"code","0f7e4a36":"code","fa4e79c6":"code","cb0e69ed":"code","70529846":"code","6168ef50":"code","4824ac03":"code","e13e7e7b":"code","69722558":"code","9e38765b":"code","5f33cacd":"code","b2aad899":"code","75680eec":"code","cc2b74cd":"code","c2f54bad":"code","5c28455d":"code","9eaaf05c":"code","8ac64626":"code","26d179cb":"code","d303dc29":"code","a7fd5e02":"code","67c9d34e":"markdown","1e552e9d":"markdown","6b857d76":"markdown","10b03738":"markdown","df57b13c":"markdown","e5ae5156":"markdown","ebafe268":"markdown"},"source":{"3489e34b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ee4d844d":"import sqlite3\n\ncon = sqlite3.connect(\"..\/input\/wikibooks-dataset\/wikibooks.sqlite\")\ndf = pd.read_sql_query(\"SELECT * from en\", con)\n\ncon.close()","7240c084":"df.head()","5de703bd":"print(df[\"body_text\"][:2].value_counts())","6b86bb43":"import spacy\nfrom spacy.lemmatizer import Lemmatizer\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport gensim\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tqdm import tqdm","3939d619":"nlp = spacy.load(\"en_core_web_sm\",disable=['parser', 'ner'])","6a893d1c":"stop_list = ['|','\\n','\\n ','\\n\\n ','\\n\\n\\n ','   ','>','<','^']\nnlp.Defaults.stop_words.update(stop_list)","f5f38e0d":"for word in STOP_WORDS:\n    lexeme = nlp.vocab[word]\n    lexeme.is_stop = True","02be01d1":"def lemmatizer(doc):\n    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n    doc = u' '.join(doc)\n    return nlp.make_doc(doc)\n    \ndef remove_stopwords(doc):\n    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n    return doc","af34dbee":"nlp.add_pipe(lemmatizer,name='lemmatizer')\nnlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)","b3cd7b17":"text_data = df[\"body_text\"][:1000].str.encode('ascii', 'ignore').str.decode('ascii')","0f354a4e":"doc_list = []\nfor doc in tqdm(text_data):\n    art = nlp(doc)\n    doc_list.append(art)","a2eed013":"doc_list[1]","9e543d41":"import gensim.corpora as corpora","699208db":"clean_data = [' '.join(i) for i in doc_list]","c2f8bb79":"from sklearn.feature_extraction.text import CountVectorizer","491d0cc5":"vect = CountVectorizer(min_df=20, max_df=0.2, stop_words='english', \n                       token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')","ac8b06c1":"X = vect.fit_transform(clean_data)\ncorpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)","8542d5fa":"id_map = dict((v, k) for k, v in vect.vocabulary_.items())","8973745e":"unsup_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id_map, passes=10,\n                                               random_state=100, num_topics=30, workers=4)","f3419626":"for idx, topic in unsup_model.print_topics(-1):\n    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n    print(\"\\n\")","c2b547d5":"#measuring how good unsupervised model is. Lower the perplexity, better the model.\nprint('Perplexity: ', unsup_model.log_perplexity(corpus))","7dc699cd":"import pyLDAvis\nimport pyLDAvis.gensim","e318eea8":"pyLDAvis.enable_notebook()","93439f96":"words = corpora.Dictionary()\nid2word = dict((v, k) for k, v in vect.vocabulary_.items())\nword2id = dict((k, v) for k, v in vect.vocabulary_.items())\nwords.id2token = id2word\nwords.token2id = word2id","86bdfed7":"## displaying scatter plot visualization\nvis = pyLDAvis.gensim.prepare(unsup_model, corpus, words)\nvis","3f6b6ec9":"def topic_prediction(my_document):\n    string_input = [my_document]\n    X = vect.transform(string_input)\n    # Convert sparse matrix to gensim corpus.\n    corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n    output = list(unsup_model[corpus])[0]\n    topics = sorted(output,key=lambda x:x[1],reverse=True)\n    return topics[0][0]","9d044cde":"df_topic = pd.DataFrame()\ntp = []\nfor i in tqdm(df[\"body_text\"][:1000]):\n    tp.append(topic_prediction(i))\n\ndf_topic[\"Topic\"] = tp","a9664ff3":"df_topic[\"Text\"] = df[\"body_text\"][:1000].values\ndf_topic[\"clean_text\"] = clean_data\ndf_topic.columns = ['Topic', 'Text', 'Clean_Text']\n\ndf_topic.head()","74ccfd44":"df_topic.to_csv(\"topics.csv\",index=False)","7157bbbd":"topic_df = pd.read_csv(\".\/topics.csv\")\ntopic_df.head()","03352775":"topic_df[\"Clean_Text\"] = topic_df[\"Clean_Text\"].astype(str)","084fd468":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense,Flatten,Embedding, Dropout, LSTM\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.initializers import Constant","0f7e4a36":"from sklearn.model_selection import train_test_split","fa4e79c6":"X_train, X_test, y_train, y_test = train_test_split(topic_df[\"Clean_Text\"], topic_df[\"Topic\"], test_size=0.20, random_state=100)","cb0e69ed":"X_train.shape","70529846":"X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, random_state=100)","6168ef50":"X_test.shape","4824ac03":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nvocab_size = len(tokenizer.word_index) + 1\n# integer encode the documents\nencoded_docs = tokenizer.texts_to_sequences(X_train)","e13e7e7b":"print(len(max(encoded_docs)))","69722558":"max_length = 300\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","9e38765b":"!wget http:\/\/nlp.stanford.edu\/data\/glove.42B.300d.zip","5f33cacd":"!unzip .\/glove.42B.300d.zip","b2aad899":"embeddings_index = dict()\nf = open('.\/glove.42B.300d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","75680eec":"# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab_size, 300))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","cc2b74cd":"model=Sequential()\n\nembedding=Embedding(vocab_size, # number of unique tokens\n                    300,\n                    embeddings_initializer=Constant(embedding_matrix), # initialize \n                    input_length=300, \n                    trainable=False)\n\nmodel.add(embedding)\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","c2f54bad":"encoded_docs_test = tokenizer.texts_to_sequences(X_test)\npadded_docs_test = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n\nencoded_docs_valid = tokenizer.texts_to_sequences(X_valid)\npadded_docs_valid = pad_sequences(encoded_docs_valid, maxlen=max_length, padding='post')","5c28455d":"# fit the model\nmodel.fit(padded_docs, y_train, epochs=20, verbose=1,validation_data=(padded_docs_test,y_test))\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs_valid, y_valid, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","9eaaf05c":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score","8ac64626":"X = vect.fit_transform(topic_df[\"Clean_Text\"]).toarray()\ny = topic_df['Topic']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20,random_state=100)","26d179cb":"clf = XGBClassifier()\nclf.fit(X_train, y_train)","d303dc29":"y_pred = clf.predict(X_test)\npredictions = [round(value) for value in y_pred]","a7fd5e02":"accuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","67c9d34e":"# Supervised Classification","1e552e9d":"## Visualization","6b857d76":"## reading csv file","10b03738":"## Saving unsupervised labels to csv file","df57b13c":"## Cleaning Data","e5ae5156":"Since Deep learning model is not giving enough accuracy, trying out traditional ML model.","ebafe268":"## preparing csv"}}