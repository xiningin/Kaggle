{"cell_type":{"57f08e02":"code","7c04ae55":"code","8dac8c0b":"code","f8934e66":"code","4a9b19ed":"code","e10ec871":"code","203cf38e":"code","80bbe7d6":"code","c4b12b27":"code","8c181afb":"code","260eadeb":"code","cd56ae67":"code","872743a8":"code","1dfef575":"code","52055ec7":"code","4006280a":"code","a84910ff":"code","b1ec19a2":"code","17ae4319":"code","48c46236":"code","49f15d6d":"code","3680a3c4":"markdown","e006a8bc":"markdown","7a6594a2":"markdown","657cd76e":"markdown","695e18a5":"markdown","e5a3a3d1":"markdown","67d961ce":"markdown","d9916d7d":"markdown","9f6b0479":"markdown","5bc2acdc":"markdown","0c539bf2":"markdown","1ca3dd19":"markdown","04ee5459":"markdown"},"source":{"57f08e02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7c04ae55":"# Reading files from directory\nimport os\nimport pickle\n \n# Data manipulation & analysis\nimport pandas as pd\npd.set_option('display.max_columns',100)\npd.set_option('display.max_rows', 500)\nimport datetime as dt\n \nimport numpy as np\nimport scipy\n\n# Visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n \n # \u5b9f\u884c\u306b\u95a2\u4fc2\u306a\u3044\u8b66\u544a\u3092\u7121\u8996\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm import tqdm\nimport gc\nimport json\nimport math\n\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import accuracy_score,roc_auc_score,log_loss\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\n\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\n\nfrom lightgbm import LGBMClassifier\nimport lightgbm\n","8dac8c0b":"#\u5b9f\u884c\u74b0\u5883\u304c\u3069\u3053\u306e\u968e\u5c64\u306b\u3042\u308b\u304b\u3001\u30d1\u30b9\u3092\u691c\u7d22\nimport os\nprint(os.getcwd())","f8934e66":"train = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")","4a9b19ed":"display(train.head())\ndisplay(train.describe())\ndisplay(train.info())","e10ec871":"display(test.head())\ndisplay(test.describe())\ndisplay(test.info())","203cf38e":"train.columns","80bbe7d6":"#--------------------------------\n#\u6570\u5024\u306e\u7279\u5fb4\u91cf\u3000\u203b\u4e0a\u7d1a\u8005\u306f\u3001\u7279\u5fb4\u91cf\u306e\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\u3057\u3066\u3044\u308b\u3002\n#------------------------------\nfeatures_num = ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n       'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9',\n       'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14',\n       'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19',\n       'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24',\n       'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29',\n       'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34',\n       'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39',\n       'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44',\n       'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', ]\nfeatures_cat = ['target'] ","c4b12b27":"# Categorical features distribution \n\n# Params\nn_col = 1\nn_row = round(len(features_num) )\nsize = (n_col * 10, n_row * 4.5)\n\n#Create figure\nplt.subplots(n_row,n_col,figsize=size)\n\n# enumerate\u95a2\u6570\u3000for - loop \u30b3\u30fc\u30c9\u306e\u4ee3\u7528 \nfor  i ,feature  in enumerate(features_num , 1):\n    plt.subplot(n_row, n_col , i)\n    sns.countplot(x = feature, hue = 'target', data = train)\n    plt.xlabel(feature, fontsize=9); plt.legend()\nplt.show()\n","8c181afb":"train","260eadeb":"##--------------------------------------------\n#\u30ab\u30c6\u30b4\u30eafeature  \u3000\u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\n#---------------------------------------------\nfor feature in features_cat :\n    le = LabelEncoder()\n    le.fit(train[feature])\n    train[feature] = le.transform(train[feature])","cd56ae67":"# \u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u7279\u5fb4\u91cf\u3068\u76ee\u7684\u5909\u6570\u306b\u5206\u3051\u308b  \u6b63\u89e3\u30e9\u3079\u30eb\u3092\u8a18\u5165\n#train_x = train.drop([\"target\", \"target_num\"], axis=1)\ntrain_x = train.drop([\"target\"], axis=1)\ntrain_y = train['target']","872743a8":"# \u5909\u6570Id\u3092\u9664\u5916\u3059\u308b\ntrain_x = train_x.drop(['id'], axis=1)\ntest_x = test.drop(['id'], axis=1)","1dfef575":"from imblearn.over_sampling import KMeansSMOTE\nfrom sklearn.cluster import MiniBatchKMeans\nfor label, count in zip(*np.unique(train_y, return_counts=True)):\n    print('Class {} has {} samples'.format(label, count))\n\nkmeans_smote = KMeansSMOTE(\n    sampling_strategy = 'not majority',\n    random_state = 42,\n    k_neighbors = 10,\n    cluster_balance_threshold = 0.1,\n    kmeans_estimator = MiniBatchKMeans(n_clusters=100, random_state=42)\n    #kmeans_estimator = 100\n)\ntrain_xS, train_yS = kmeans_smote.fit_resample(train_x, train_y)\n\nfor label, count in zip(*np.unique(train_yS, return_counts=True)):\n    print('Class {} has {} samples after oversampling'.format(label, count))","52055ec7":"train_xS = pd.DataFrame(train_xS, columns=features_num , index=None)\ntrain_xS","4006280a":"train_x = pd.DataFrame(train_xS ) \ntest_x = pd.DataFrame(test_x ) \ntrain_x","a84910ff":"#import imblearn\n#from imblearn.over_sampling import SMOTE ,ADASYN\n\n#oversample = SMOTE() #ADASYN()\n#train_xS, train_yS = oversample.fit_resample(train_x, train_y)\n#np.sum(train_yS, axis = 0)","b1ec19a2":"test_x","17ae4319":"##############################\n#######     LGBMClassifier by tunner\n################################\n\ntrain_oof_smote_0 = np.zeros((len(train_xS), 4))\ntemp_test = np.zeros((len(test_x), 4))\n\n\n#lgbm_params = study.best_trial.params\nlgbm_params = {'reg_alpha': 13.777202060313781,\n               'reg_lambda': 0.010176661007259611,\n               'num_leaves': 109, 'learning_rate': 0.2,               \n               'max_depth': 3,\n               'n_estimators': 48126, \n               'min_child_samples': 27,\n               'min_child_weight': 0.22183890336004472,\n               'subsample': 0.4315725341793898, \n               'colsample_bytree': 0.1}\n\n\nNUM_FOLDS = 10\nkf =  StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=137)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train_xS, train_yS))):\n        print(f'Fold {f+1}')\n        train_df = train_xS.iloc[train_ind].reset_index(drop=True)\n        val_df = train_xS.iloc[val_ind].reset_index(drop=True)\n        train_target = train_yS.iloc[train_ind].reset_index(drop=True)\n        val_target = train_yS.iloc[val_ind].reset_index(drop=True)\n\n        model = LGBMClassifier(**lgbm_params)\n        model =  model.fit(train_df, train_target,eval_set=[(val_df,val_target)],early_stopping_rounds=100,verbose=False)\n        \n        temp_oof = model.predict_proba(val_df)\n        print(log_loss(val_target, temp_oof))\n        train_oof_smote_0[val_ind] = temp_oof\n\n        temp_test += model.predict_proba(test_x)\n        test_preds_smote_0 = temp_test\/NUM_FOLDS\n\n        \nprint('All_logloss',log_loss(train_yS, train_oof_smote_0))\nnp.save('train_oof_smote_0', train_oof_smote_0 ) #for validation\nnp.save('test_preds_smote_0',test_preds_smote_0 ) #for submission","48c46236":"# importance\u3092\u8868\u793a\u3059\u308b\nplt.rcParams[\"figure.figsize\"] = (10, 5)\nlightgbm.plot_importance(model,max_num_features = 25,height=.8)","49f15d6d":"##### \u63d0\u51fa\u7528\u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210 \u30d8\u30c3\u30c0\u30fc\u7121\u8a2d\u306e\u8a2d\u5b9a\n\n#test = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\")\n\nsubmission = pd.DataFrame( test_preds_smote_0)\nsubmission.columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\nsubmission['id'] = test['id']\nsubmission = submission[['id', 'Class_1', 'Class_2', 'Class_3', 'Class_4']]\n\nsubmission.to_csv(\"submission_smote_0.csv\", index=False)\ndisplay(submission.head(), submission.tail())","3680a3c4":"My strategy is  below.\n\n\u2460\u3000Optimazed LGBM,Then FIX parameter and feature eng.\n\n[https:\/\/www.kaggle.com\/hayahiko\/tps-may-easy-way-lgbm-optuna-step-by-step]\n\n\u2461  + Oversample code = SMOTE\n\n\u2462  + Oversampel code = ADSYN\n\nIn[21]  oversample = SMOTE()  or ADASYN() , Only switch!! another is same code. \n\nDigest result\n\n\u2460\u3000PB SCORE is 1.08644\n\n\u2461\u3000OOF All_logloss 0.953626756433771    \/ PB  1.33807\n\n\u2462\u3000OOF All_logloss 0.9713155172916581   \/  PB    1.31422\n\ntoo over-fit (-_-)\n\nI am studying now. If do you have more good study , please share me.","e006a8bc":"train.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","7a6594a2":"##--------------------------------------------\n#\u30ab\u30c6\u30b4\u30eafeature  \u30b9\u30b1\u30fc\u30e9\u30fc\n#---------------------------------------------\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\ntrain_x = sc.fit_transform(train_x)\ntest_x = sc.transform(test)","657cd76e":"Addtional more Advanced is ADASYN\n\n\n![ADASYN.PNG](attachment:3bf4c2b4-8dca-4af0-80e6-635dc271b709.PNG)","695e18a5":"Both train and test is similar to variance_inflation_factor.","e5a3a3d1":"#!pip install imbalanced-learn\nimport imblearn\nfrom imblearn.under_sampling import ClusterCentroids\nfrom imblearn.under_sampling import RandomUnderSampler\n \nundersample =  ClusterCentroids(random_state=0) # RandomUnderSampler(random_state=0)\ntrain_xS, train_yS = undersample.fit_resample(train_x, train_y)\nnp.sum(train_yS, axis = 0)\n\n","67d961ce":"label_dict = {val:idx for idx, val in enumerate(train['target'].unique())}\ntrain['target_num'] = train['target'].map(label_dict)","d9916d7d":"SMOTE\uff08Synthetic Minority Over-sampling Technique\uff09\u4f7f\u3063\u3066\u307f\u3066\u3069\u3046\u304b\n[https:\/\/imbalanced-learn.org\/stable\/over_sampling.html](http:\/\/)","9f6b0479":"![\u30ad\u30e3\u30d7\u30c1\u30e32.PNG](attachment:769cbada-c23f-4fc0-b785-a3e25f7e7b78.PNG)","5bc2acdc":"feat_cols = train.drop([\"target\", \"target_num\"], axis=1).columns\ntrain_unique_list= []\ntest_nunique_list = []\n\nfor col in feat_cols:\n    train_unique_list.append(train[col].nunique())\n    test_nunique_list.append(test[col].nunique())\n\nunique_df = pd.DataFrame(data=train_unique_list, index=feat_cols, columns=[\"train_nunique\"])\nunique_df[\"test_nunique\"] = test_nunique_list\n\nunique_df.style.background_gradient(cmap=\"Blues\")","0c539bf2":"I found Im-balance class of target by EDA .\n\nSo I tried re-balance impact study.\n\nSpecial thanks for \"SMOTE\" or not to \"SMOTE\" ... dealing with imbalanced dataset ...!!  by Mr.Remek Kinas\n\n[https:\/\/www.kaggle.com\/c\/tabular-playground-series-may-2021\/discussion\/235795](http:\/\/)\n\n[https:\/\/www.kaggle.com\/remekkinas\/tps-5-weighted-training-xgb-rf-lr-smote](http:\/\/)\n\nDon't forget to vote !!","1ca3dd19":"Frist , SMOTE  image is here .\nSMOTE\uff08Synthetic Minority Over-sampling Technique\uff09\n\n![smote.PNG](attachment:707956d5-7a0d-47e3-a352-48ca096acd8d.PNG)\n","04ee5459":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif[\"variables\"] = feat_cols\nvif[\"VIF_train\"] = [variance_inflation_factor(train[feat_cols].values, i)\\\n                    for i in range(train[feat_cols].shape[1])]\nvif[\"VIF_test\"] = [variance_inflation_factor(test[feat_cols].values, i)\\\n                   for i in range(test[feat_cols].shape[1])]\n\n#vif.style.background_gradient(cmap=\"cool\")\n\n\n#vif\u3092\u30b0\u30e9\u30d5\u5316\u3059\u308b\nplt.plot(vif[\"VIF_test\"])\nplt.plot(vif[\"VIF_train\"])"}}