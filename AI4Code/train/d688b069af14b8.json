{"cell_type":{"47953e30":"code","166b72cb":"code","82adbd94":"code","974c051f":"code","780fd75b":"code","6064d39b":"code","b553d59f":"code","f771f10f":"code","09d19822":"code","1eb5e0d2":"code","11f35ea0":"code","cf6676bd":"code","70493dd0":"code","3edd311d":"code","e12c6fe7":"code","e0a32a5c":"code","cb5d51c6":"code","8b2dde1b":"code","c7f3f78a":"code","ba6a6e96":"code","2fed3be8":"code","6d62c828":"code","2e5c1cd9":"code","ddc8824d":"code","c2d3a9e2":"code","7bbf3042":"code","02d75e33":"code","3aab72ce":"code","7e33e175":"code","fb789928":"markdown","65fca8fa":"markdown","48680df9":"markdown","0167558a":"markdown","c77cc76b":"markdown","d017e713":"markdown","d17aca28":"markdown","6edfe260":"markdown","fa031e56":"markdown","12f2cfcf":"markdown","b4133e43":"markdown","e479ddd9":"markdown","dde93071":"markdown","de988207":"markdown","5fbc6907":"markdown","9dbbdc55":"markdown","8aaa6f13":"markdown","40daf282":"markdown","17dfa1d5":"markdown","4b458493":"markdown","e06f0504":"markdown","351b6054":"markdown","eb5c4014":"markdown","f5a73cbd":"markdown","33df0984":"markdown","3a2b195c":"markdown","f35599ee":"markdown"},"source":{"47953e30":"!pip install scikit-learn -U","166b72cb":"# Intel\u00ae Extension for Scikit-learn installation:\n!pip install scikit-learn-intelex","82adbd94":"from sklearnex import patch_sklearn\npatch_sklearn()","974c051f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn","780fd75b":"# Loading train and test data\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\", parse_dates=['date'])\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\", parse_dates=['date'])","6064d39b":"train.dtypes","b553d59f":"# figuring out the theoretically possible level combination\ntime_series = ['country', 'store', 'product']\ncombinations = 1\nfor feat in time_series:\n    combinations *= train[feat].nunique()\n    \nprint(f\"There are {combinations} possible combinations\")","f771f10f":"time_series = ['country', 'store', 'product']\ncountry_store_product_train = train[time_series].drop_duplicates().sort_values(time_series)\ncountry_store_product_test =test[time_series].drop_duplicates().sort_values(time_series)\n\ncond_1 = len(country_store_product_train) == combinations\nprint(f\"Are all theoretical combinations present in train: {cond_1}\")\ncond_2 = (country_store_product_train == country_store_product_test).all().all()\nprint(f\"Are combinations the same in train and test: {cond_2}\")","09d19822":"train_dates = train.date.drop_duplicates().sort_values()\ntest_dates = test.date.drop_duplicates().sort_values()\n\nfig, ax = plt.subplots(1, 1, figsize = (11, 7))\ncmap_cv = plt.cm.coolwarm\n\ncolor_index = np.array([1] * len(train_dates) + [0] * len(test_dates))\n\nax.scatter(range(len(train_dates)), [.5] * len(train_dates),\n           c=color_index[:len(train_dates)], marker='_', lw=15, cmap=cmap_cv,\n           label='train', vmin=-.2, vmax=1.2)\n\nax.scatter(range(len(train_dates), len(train_dates) + len(test_dates)), [.55] * len(test_dates),\n           c=color_index[len(train_dates):], marker='_', lw=15, cmap=cmap_cv,\n           label='test', vmin=-.2, vmax=1.2)\n\ntick_locations = np.cumsum([0, 365, 366, 365, 365, 365])\nfor i in (tick_locations):\n    ax.vlines(i, 0, 2,linestyles='dotted', colors = 'grey')\n    \nax.set_xticks(tick_locations)\nax.set_xticklabels([2015, 2016, 2017, 2018, 2019, 2020], rotation = 0)\nax.set_yticklabels(labels=[])\nplt.ylim([0.45, 0.60])\nax.legend(loc=\"upper left\", title=\"data\")\n\nplt.show()","1eb5e0d2":"missing_train = pd.date_range(start=train_dates.min(), end=train_dates.max()).difference(train_dates)\nmissing_test = pd.date_range(start=test_dates.min(), end=test_dates.max()).difference(test_dates)\nprint(f\"missing dates in train: {len(missing_train)} and in test: {len(missing_test)}\")","11f35ea0":"# We create different time granularity\n\ndef process_time(df):\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['week'] = df['date'].dt.isocalendar().week\n    df['week'][df['week']>52] = 52\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['dayofyear'] = df['date'].dt.dayofyear\n    return df\n\ntrain = process_time(train)\ntest = process_time(test)","cf6676bd":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            selected.set_index('date').groupby('year')['num_sold'].mean().plot(ax=ax)\n            ax.set_title(f\"{country}:{store}\")\n    plt.show()","70493dd0":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('month')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{product} | {country}:{store}\")\n            ax.legend()\n    plt.show()","3edd311d":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('week')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","e12c6fe7":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('day')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","e0a32a5c":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('dayofweek')['num_sold'].sum().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","cb5d51c6":"festivities = pd.read_csv(\"..\/input\/festivities-in-finland-norway-sweden-tsp-0122\/nordic_holidays.csv\",\n                          parse_dates=['date'],\n                          usecols=['date', 'country', 'holiday'])","8b2dde1b":"gdp = pd.read_csv(\"..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv\")\ngdp = np.concatenate([gdp[['year', 'GDP_Finland']].values, \n                      gdp[['year', 'GDP_Norway']].values, \n                      gdp[['year', 'GDP_Sweden']].values])\ngdp = pd.DataFrame(gdp, columns=['year', 'gdp'])\ngdp['country'] = ['Finland']*5 + ['Norway']*5 +['Sweden']*5","c7f3f78a":"train['product'].unique(), train['store'].unique()","ba6a6e96":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\ndef process_data(df):\n    \n    processed = dict()\n    processed['row_id'] = df['row_id']\n    \n    print(\"creating dummies for main effects of time, country, store and product\")\n    to_dummies = ['country', 'store', 'product', 'month', 'week', 'day', 'dayofweek']\n    for feat in to_dummies:\n        tmp = pd.get_dummies(df[feat])\n        for col in tmp.columns:\n            processed[feat+'_'+str(col)] = tmp[col]\n    \n    print(\"creating dummies with 7 gg halo effect for Nordic holidays\")\n    tmp = pd.get_dummies(\n        df.merge(festivities, on=['date', 'country'], how='left').sort_values('row_id')['holiday'])\n    for col in tmp.columns:\n            peak = tmp[col].values + tmp[col].rolling(7).mean().fillna(0).values\n            processed['holiday_'+str(col)] =  peak\n    \n    print(\"creating interactions\")\n    high_lvl_interactions = [\n        ['country', 'product', 'month'],\n        ['country', 'product', 'week'],\n        ['country', 'store', 'week'],\n        ['country', 'product', 'month', 'day'],\n        ['country', 'product', 'month', 'dayofweek'],\n    ]\n    for sel in high_lvl_interactions:\n        tmp = pd.get_dummies(df[sel].apply(lambda row: '_'.join(row.values.astype(str)), axis=1))\n        for col in tmp.columns:\n            processed[col] = tmp[col]\n            \n    print(\"modelling time as continuous per each country\")\n    for country in ['Finland', 'Norway', 'Sweden']:\n        processed[country + '_prog'] = ((df.row_id \/\/ 18) + 1) * (df['country']==country).astype(int)\n        processed[country + '_prog^2'] = (processed[country + '_prog']**2)\n        processed[country + '_prog^3'] = (processed[country + '_prog']**3)\n    \n    print(\"adding sin\/cos of day of year\")\n    dayofyear = df.date.dt.dayofyear\n    steps = 4\n    for k in range(1, 32, steps):\n        processed[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * np.pi * k)\n        processed[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * np.pi * k)\n    \n    print(\"adding log(gdp)\")\n    gdp_countries = np.log1p(df.merge(gdp, on=['country', 'year'], how='left')['gdp'].values)\n    for country in ['Finland', 'Norway', 'Sweden']:\n        processed['gdp_'+ country] = gdp_countries * (df['country']==country).astype(int)\n            \n    print(f\"completed processing {len(processed)-1} features\")\n    \n    values = list()\n    columns = list()\n    for key, value in processed.items():\n        values.append(np.array(value))\n        columns.append(key)\n        \n    values = np.array(values).T        \n    processed = pd.DataFrame(values, columns=columns)\n    \n    print(\"resorting row ids\")\n    processed = processed.sort_values('row_id').set_index('row_id')\n    return processed\n\ndef process_target(df):\n    target = pd.DataFrame({'row_id':df['row_id'], 'num_sold':df['num_sold']})\n    target = target.sort_values('row_id').set_index('row_id')\n    return target\n\ntrain_test = process_data(train.append(test))\n\nprocessed_train = train_test.iloc[:len(train)].copy()\nprocessed_test = train_test.iloc[len(train):].copy()\n\ntarget = np.ravel(process_target(train))","2fed3be8":"def weighting(df, weights):\n    return df.year.replace(weights).values\n    \nweights = weighting(train, {2015:0.125, 2016:0.25, 2017:0.5, 2018:1})","6d62c828":"processed_train.shape, train.shape, processed_test.shape, test.shape","2e5c1cd9":"def SMAPE(y_true, y_pred):\n    # From https:\/\/www.kaggle.com\/cpmpml\/smape-weirdness\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\ndef SMAPE_exp(y_true, y_pred):\n    y_true = np.exp(y_true)\n    y_pred = np.exp(y_pred)\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\ndef SMAPE_err(y_true, y_pred):\n    # From https:\/\/www.kaggle.com\/cpmpml\/smape-weirdness\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return diff","ddc8824d":"# due to float calculations the computation is approximated\na = 5\nb = 7\nprint(a * b) # pure multiplicative\nprint(np.exp(np.log(a) + np.log(b))) # multiplicative made additive by log","c2d3a9e2":"from sklearn.linear_model import LinearRegression\n\naddictive_model = LinearRegression().fit(processed_train, target, sample_weight=weights)\naddictive_fit = addictive_model.predict(processed_train.values)\nprint(f\"addictive fit: {SMAPE(y_true=target, y_pred=addictive_fit): 0.3f}\")","7bbf3042":"multiplicative_model = LinearRegression().fit(processed_train, np.log(target), sample_weight=weights)\nmultiplicative_fit = np.exp(multiplicative_model.predict(processed_train.values))\nprint(f\"multiplicative fit: {SMAPE(y_true=target, y_pred=multiplicative_fit):0.3f}\")","02d75e33":"def selective_rounding(preds, lower=0.3, upper=0.7):\n    # selective rounding\n    dec = preds % 1\n    to_round = (dec<=lower)|(dec>=upper)\n    preds[to_round] = np.round(preds[to_round])\n    return preds","3aab72ce":"model = LinearRegression().fit(processed_train, np.log(target), sample_weight=weights)\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\")\npreds = np.exp(model.predict(processed_test.values))\n\n# rounding\npreds = selective_rounding(preds, lower=0.3, upper=0.7)\n\nsubmission.num_sold = preds\nsubmission.to_csv(\"submission.csv\", index=False)","7e33e175":"log_target = np.log(target)\ntrain_set = list(train.index[train.date<'2018-01-01'])\nval_set = list(train.index[train.date>='2018-01-01'])\nmodel = LinearRegression()\n\nmodel.fit(processed_train.iloc[train_set], log_target[train_set], sample_weight=weights[train_set])\npreds = np.exp(model.predict(processed_train.iloc[val_set].values))\n\n# rounding\npreds = selective_rounding(preds, lower=0.3, upper=0.7)\n\nsmape = SMAPE(y_true=target[val_set], y_pred=preds)\nprint(f\"hold-out smape: {smape}\")","fb789928":"##### We try modelling both an additive and a multiplicative predictor (for multiplicative you just need a log transoformation of the target because exponentiation of the sum of logs of the terms correspond to multiplications of the terms)","65fca8fa":"##### Having four complete years available allows various types of testing and modelling. In this EDA we will limit to use the last year available (2018) as an hold-out, setting our baseline model to be able to forecast an entire year in the future.","48680df9":"##### We then load the data, it is necessary paying attention to convert the date into datetime","0167558a":"##### As a second step let's visualize how time is split between train and test.","c77cc76b":"##### As a last check we verify that no date is missing from train and test:","d017e713":"##### We are ready to explore the data. In order to highlight the time series characteristics, we create panels of products x countries x shops. We start by aggregating at a year level.","d17aca28":"##### Friday and the week-end are the best days, but Sundays are not always at the same level as Saturdays (it depends on the year - why?).","6edfe260":"##### For testing purposes we refit the model only on years 2015-2017 using 2018 as an hold-out test.","fa031e56":"##### Before starting with EDA, it is important to check about the data structure. Apparently we have a combination of time series based on countries, stores and products. Let's first check if all the combinations appear in train and test.","12f2cfcf":"##### We prepare all the evaluation measures, both at an aggregate level, with exp transformation and at an individual cases level (for error analysis)","b4133e43":"##### The first series of panels points out that the country effect is kind of indipendent from store and product. There is an underlying country dynamic that replicates the same no matter the shop or the product sold by it. We also notice that shops differentiate only for the level of sales.","e479ddd9":"##### Based on the information we got we now proceed to feature engineering and to enrich the data (using festivities and GDP data).","dde93071":"#### In this notebook I start first from building a quick EDA revealing patters in the data, then I use EDA insights to build a simple linear model with only essential features (based on a out of time approach).","de988207":"##### Our next panels will explore seaasonality based on months:","5fbc6907":"##### It seems that the multiplicative model is better. We now use it to predict on the test set.","9dbbdc55":"* vers. 20 - introduced selective rounding\n* vers. 22 - adding sin\/cos processing of day of the year at different time lags","8aaa6f13":"##### The middle of the month usually presents less sales. The peak at the end may be influenced by seasonal peaks (end of year).","40daf282":"##### At a week level we see that differences are due to peaks. Peaks seem different in Spring. Probably is is Easter effect.","17dfa1d5":"##### Here we notice two important elements: seanality curves are different for each product and they also differ from year to year. Averaging the curves probably is safe bet for the future, as well as considering more relevant the recent years (thus weighting more the year 2018 for instance). For the sticker product, year 2017 seems particularly different from others.","4b458493":"### If you liked the notebook, consider to upvote, thank you and happy Kaggling!","e06f0504":"##### Having completed the checks, we process the datetime information, extracting it informative elements at different time granularities:","351b6054":"##### Since we will use Scikit-learn models, let's accelerate them first by using Intel Extension.","eb5c4014":"##### Since EDA demonstrated that there is some kind of yearly change, we overweight more recent observations.","f5a73cbd":"##### We now proceed to examine seasonality even more in detail at a week level:","33df0984":"##### We now process the data and scale it. Since EDA revealed how the different characteristics of the series are mostly main effects (country and store), we focus on finding the way to model the interaction between products and time.","3a2b195c":"##### And we completed by inspecting at a day of the week level:","f35599ee":"##### We now start obeserving recurrences at a monthly level:"}}