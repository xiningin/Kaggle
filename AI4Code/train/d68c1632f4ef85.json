{"cell_type":{"d41cac36":"code","a4c81f17":"code","6597eb6e":"code","45774fec":"code","9fa978a1":"code","9715b812":"code","fdf2c009":"code","ff520ad5":"code","9235034f":"code","b88450ff":"code","3d7ccef3":"code","c8641654":"code","2a71d50b":"code","8721fd70":"code","29df6ed3":"code","9dc2d7bb":"code","eb182b25":"code","0564dd82":"code","71a20aa7":"code","480fe28e":"code","dc1602c6":"code","1055a00a":"code","59e0b296":"code","28ab06b6":"code","20dd7569":"code","c35ca7f0":"code","99749930":"code","30ee078c":"code","535e9860":"code","505b6172":"code","d8501861":"code","cfa3f7e6":"code","170a4498":"code","1d4ffe7f":"code","b346028a":"code","e11d639b":"code","b9e3d400":"code","aed0bcd8":"code","aca4dfea":"code","a33849b3":"code","8847b50a":"code","bc2b008c":"code","293d8849":"code","2965a767":"code","7659e508":"code","a20f8765":"code","6a4f48a0":"code","202f98e9":"code","efcc3b28":"code","dfebf678":"code","b758c80d":"code","79dd510e":"code","8f86f736":"code","a20d57f5":"code","a66511d8":"code","90e182f4":"code","4c351d0c":"code","27dcea1f":"code","11cdb94d":"code","5cde2236":"code","dae0918c":"code","5043efb7":"markdown","9c237a6e":"markdown","1578aa4f":"markdown","c2aee299":"markdown","a09561e8":"markdown","66f0c3f3":"markdown","8b12b95f":"markdown","58bd814f":"markdown","2dce7849":"markdown","ee460535":"markdown","f666a223":"markdown","5bf35182":"markdown","66127afa":"markdown","3a5174c6":"markdown","d4178a5b":"markdown","102c1499":"markdown","45800662":"markdown","26766c6e":"markdown","351be38c":"markdown","c836f10b":"markdown","0b5c752a":"markdown","875469df":"markdown","f1ddc6c0":"markdown","afeca26c":"markdown","539a632c":"markdown","2091d86f":"markdown","eec9d21f":"markdown","63f9545e":"markdown","b13bdbb5":"markdown","a57541fe":"markdown","1d02e809":"markdown","a24560e0":"markdown","6bb0b05e":"markdown","06b3deb7":"markdown","2a739b66":"markdown","1468da23":"markdown","5d0601fe":"markdown","2a60ad3f":"markdown","c9cebc22":"markdown","390ee7c3":"markdown","1a6adc9d":"markdown","f44fee04":"markdown","ebd772e7":"markdown","c927a8c9":"markdown","1c69e33c":"markdown","344a66b8":"markdown","c228d7a4":"markdown","a82f7aeb":"markdown","241e91e4":"markdown","cb7bd2bb":"markdown","2727f625":"markdown"},"source":{"d41cac36":"import numpy as np\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\nfrom math import sqrt\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\n\nfrom matplotlib import style\nstyle.use('seaborn')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n%matplotlib inline\npd.options.display.float_format = '{:,.2f}'.format","a4c81f17":"PATH = '..\/input\/'","6597eb6e":"sales = pd.read_csv(PATH + 'sales_train.csv', low_memory=False); sales.tail()","45774fec":"sales.describe(include=\"all\")","9fa978a1":"items = pd.read_csv(PATH + 'items.csv', low_memory=False, index_col=1); items.head()","9715b812":"item_cat = pd.read_csv(PATH + 'item_categories.csv', low_memory=False, index_col=1); item_cat.head()","fdf2c009":"shops = pd.read_csv(PATH + 'shops.csv', low_memory=False, index_col=1); shops.head()","ff520ad5":"test = pd.read_csv(PATH + 'test.csv', low_memory=False, index_col=0); test.head()","9235034f":"def assign_key(df): df['key'] = df['item_id'].astype('int32').map(str) + \"_\" + df['shop_id'].astype('int32').map(str)\n    \nassign_key(sales)\nassign_key(test)","b88450ff":"test['key'][~test['key'].isin(sales['key'])].shape[0] \/ test['key'].shape[0]","3d7ccef3":"test['item_id'][~test['item_id'].isin(sales['item_id'])].drop_duplicates().shape[0]","c8641654":"test['shop_id'][~test['shop_id'].isin(sales['shop_id'])].shape[0]","2a71d50b":"sales[['date_block_num', 'item_cnt_day']].groupby('date_block_num').sum().plot(figsize=(9, 6))\nplt.show()","8721fd70":"df = sales.copy()\ntest_set = test.copy()","29df6ed3":"df = df.merge(items[['item_category_id']], how='left', on='item_id')\ntest_set = test_set.merge(items[['item_category_id']], how='left', on='item_id')\ntest_set.index.rename('ID', inplace=True)","9dc2d7bb":"sales.shape[0] == df.shape[0], test_set.shape[0] == test.shape[0]","eb182b25":"df.columns, test.columns","0564dd82":"df.isna().sum(), test.isna().sum()","71a20aa7":"df['date'] = pd.to_datetime(df['date'], format='%d.%m.%Y')","480fe28e":"attr = ['year', 'month', 'week', 'day']\n\nfor a in attr: df[a] = getattr(df['date'].dt, a)","dc1602c6":"df.columns","1055a00a":"d = df[['date_block_num', 'day']].groupby(['date_block_num']).max()\ndi = dict([(d.index[i], d.values[i][0]) for i in range(len(d))])\ndf['day_count'] = df['date_block_num'].map(di)","59e0b296":"df.loc[((df['week'] == 1) & (df['day'] > 7)), 'week'] = 53\ndf['week_block_num'] = (df['year'] - df['year'].min()) * 52 + df['week']\n\nwmax = df[['date_block_num', 'week_block_num']].groupby(['date_block_num']).max()\nwmin = df[['date_block_num', 'week_block_num']].groupby(['date_block_num']).min()\ndi = dict([(wmax.index[i], wmax.values[i][0] - wmin.values[i][0]) for i in range(len(wmax))])\ndf['week_count'] = df['date_block_num'].map(di)","28ab06b6":"df['qtr'] = df['month'].map(dict([[i, int(i \/\/ 3.1 + 1)] for i in df['month'].unique()]))","20dd7569":"df['x_mas'] = df['month'].map(dict([[i, i \/\/ 12] for i in df['month'].unique()]))","c35ca7f0":"df.columns","99749930":"to_group = ['date_block_num', 'shop_id', 'item_id', 'key', 'item_category_id',\n            'year', 'month', 'day_count', 'week_count', 'qtr', 'x_mas']","30ee078c":"to_mean = ['item_price']","535e9860":"to_sum = ['item_cnt_day']","505b6172":"da = df[to_group].copy()\nda = pd.concat([da,\n                df[to_group + to_mean].groupby(to_group).transform('mean'),\n                df[to_group + to_sum].groupby(to_group).transform('sum'),\n               ], axis=1).drop_duplicates()\nda.rename(columns={'item_cnt_day': 'item_cnt_month'}, inplace=True)\nda.head()","d8501861":"da.describe(include='all')","cfa3f7e6":"test_set.columns","170a4498":"avg_prices = pd.concat((da[['date_block_num', 'item_id']], da[['date_block_num', 'item_id','item_price']]\n                        .groupby(['date_block_num', 'item_id'])\n                        .transform('mean')), axis=1).sort_values(by=['date_block_num', 'item_id']).drop_duplicates().sort_values(by=['item_id', 'date_block_num'], ascending=False)\n\nfor m in da['date_block_num'].unique():\n    mask = ~test_set['key'].isin(da['key'][da['date_block_num'] == m])\n    mask2 = (da['date_block_num'] == m)\n    \n    md = test_set[['key', 'shop_id', 'item_id', 'item_category_id']][mask]\n    md['date_block_num'] = m\n    for col in ['year', 'month', 'day_count', 'week_count', 'qtr', 'x_mas']: md[col] = da[col][mask2].max()\n    \n    avg = avg_prices[avg_prices['date_block_num'] <= m].drop(columns='date_block_num').groupby('item_id').mean()\n    md['item_price'] = md['item_id'].map(dict(zip(avg.index, avg.values[:, 0]))).fillna(0)\n    md['item_cnt_month'] = 0\n    \n    da = da.append(md, ignore_index=True, sort=True)","1d4ffe7f":"s_ch = sales.copy()\ns_ch[\"orig_sales\"] = s_ch[\"item_price\"] * s_ch[\"item_cnt_day\"]\ns_ch[\"orig_item_count\"] = s_ch[\"item_cnt_day\"]\n\na_ch = da.copy()\na_ch[\"agg_sales\"] = a_ch[\"item_price\"] * a_ch[\"item_cnt_month\"]\na_ch[\"agg_item_count\"] = a_ch[\"item_cnt_month\"]\n\nfig = plt.figure(figsize=(15, 10))\nax1 = plt.subplot2grid((2, 2), (0, 0))\nax2 = plt.subplot2grid((2, 2), (0, 1))\n\ns_ch[[\"date_block_num\", \"orig_sales\"]].groupby(\"date_block_num\").agg([sum]).plot(legend=True, ax=ax1, title=\"Total Sales Value\")\ns_ch[[\"date_block_num\", \"orig_item_count\"]].groupby(\"date_block_num\").agg([sum]).plot(legend=True, ax=ax2, title=\"Items Sold\")\n\na_ch[[\"date_block_num\", \"agg_sales\"]].groupby(\"date_block_num\").agg([sum]).plot(legend=True, ax=ax1, color='r')\na_ch[[\"date_block_num\", \"agg_item_count\"]].groupby(\"date_block_num\").agg([sum]).plot(legend=True, color='r', ax=ax2)\n\n\nplt.tight_layout()\nplt.show()","b346028a":"da['prev_month'] = da['date_block_num'].map(dict([[i, i-1] if i > 0 else [i, np.nan] for i in df['date_block_num'].unique()]))\nda['prev_qtr'] = da['date_block_num'].map(dict([[i, i-3] if i > 2 else [i, np.nan] for i in df['date_block_num'].unique()]))\nda['prev_year'] = da['date_block_num'].map(dict([[i, i-12] if i > 11 else [i, np.nan] for i in df['date_block_num'].unique()]))","e11d639b":"di = dict(zip(da['date_block_num'].astype('float32').map(str) + da['key'].values, da['item_cnt_month']))\n\ndef prev_sales(prevsale, prevperiod): \n    da[prevsale] = (da[prevperiod].map(str) + da['key']).map(di).fillna(0)\n    da.drop(columns=prevperiod, inplace=True)\n\nprev_sales('item_cnt_prevm', 'prev_month')\nprev_sales('item_cnt_prevq', 'prev_qtr')\nprev_sales('item_cnt_prevy', 'prev_year')","b9e3d400":"da[['date_block_num','item_cnt_month', 'item_cnt_prevm', 'item_cnt_prevq', 'item_cnt_prevy']][da['date_block_num'] > 11].groupby(['date_block_num']).sum().plot(figsize=(10, 6))\nplt.show()","aed0bcd8":"da[['date_block_num','item_cnt_month', 'item_cnt_prevm', 'item_cnt_prevq', 'item_cnt_prevy']][(da['key'].isin(test['key'])) & (da['date_block_num'] > 11)].groupby(['date_block_num']).sum().plot(figsize=(10, 6))\nplt.show()","aca4dfea":"price_map = dict(da[['key', 'item_price']][da['date_block_num'] == da['date_block_num'].max()].values)\ntest_set['item_price'] = test_set['key'].map(price_map).fillna(0)\ntest_set['date_block_num'] = da['date_block_num'].max() + 1\ntest_set['year'] = 2015\ntest_set['month'] = 11\ntest_set['day_count'] = 30\ntest_set['week_count'] = 4\ntest_set['qtr'] = 4\ntest_set['x_mas'] = 0\nlags = [['item_cnt_prevm', 0], ['item_cnt_prevq', 2], ['item_cnt_prevy', 11]]\nfor l in lags: test_set[l[0]] = test_set['key'].map(dict(da[['key', 'item_cnt_month']][(da['date_block_num'] == da['date_block_num'].max() - l[1])].values))","a33849b3":"da[['year', 'month']][da['year']==da['year'].max()].max()","8847b50a":"to_val = (da['date_block_num'] == da['date_block_num'].max())\nto_train = (da['date_block_num'] > da['date_block_num'].max() - 12) & (da['date_block_num'] != da['date_block_num'].max())\n\nX_train = da[to_train].drop(columns=['item_cnt_month', 'date_block_num', 'key'])\nX_val = da[to_val].drop(columns=['item_cnt_month', 'date_block_num', 'key'])","bc2b008c":"da['item_cnt_month'].skew()","293d8849":"y_train = np.log1p(da['item_cnt_month'][to_train].clip(0., 20.))\ny_val = np.log1p(da['item_cnt_month'][to_val].clip(0., 20.))","2965a767":"X_train.shape, y_train.shape, X_val.shape, y_val.shape, da.shape","7659e508":"def RMSE(targ, pred): return np.sqrt(np.mean((np.expm1(targ) - np.expm1(pred))**2))","a20f8765":"from sklearn.ensemble import RandomForestRegressor","6a4f48a0":"def review(X_train, X_val):\n    m.fit(X_train, y_train)\n    preds = m.predict(X_val)\n    print(\"-\"*30, f'''\n    Training score: {m.score(X_train, y_train)*100:.2f}%\n    Validation score: {m.score(X_val, y_val)*100:.2f}%\n    Out-of-Bag score: {m.oob_score_*100:.2f}%\n    RMSE: {RMSE(y_val, preds):.4f}\n    ''')","202f98e9":"%%time\nm = RandomForestRegressor(n_estimators=50, max_features=0.85, min_samples_leaf=5,\n                          n_jobs=-1, oob_score=True)\n\nreview(X_train, X_val)","efcc3b28":"def f_i(X_train, X_val, use_RMSE=False):\n    global FI\n    accs = []\n    if use_RMSE:\n        targ = RMSE(m.predict(X_train), y_train)\n    else:\n        targ = m.score(X_train, y_train) \n    num_features = 15\n\n    for c in X_train.columns:\n        X = X_train.copy()\n        X[c] = X[[c]].sample(frac=1).set_index(X.index)[c]  # random shuffle of one column\n        if use_RMSE: \n            accs.append(RMSE(m.predict(X), y_train) - targ)\n        else: \n            accs.append(targ - m.score(X, y_train))\n\n\n    FI = sorted([[c, float(a)] for c, a in zip(X.columns, accs)], key=lambda x: x[1], reverse=True)[:num_features]\n    pd.DataFrame({'Score loss': [FI[i][1] for i in range(len(FI))], 'Features': [FI[i][0] for i in range(len(FI))]}).set_index('Features').sort_values(by='Score loss', ascending=True).plot.barh()\n    plt.show()","dfebf678":"f_i(X_train, X_val, use_RMSE=True)","b758c80d":"top = -2\nselected = [FI[i][0] for i in range(len(FI))][:top]\nXt = X_train[selected].copy()\nXv = X_val[selected].copy()","79dd510e":"%%time\nm = RandomForestRegressor(n_estimators=50, max_features=0.85, min_samples_leaf=5,\n                          n_jobs=-1, oob_score=True)\nreview(Xt, Xv)","8f86f736":"f_i(Xt, Xv, use_RMSE=True)","a20d57f5":"tp = da[['date_block_num', 'item_cnt_month']][to_train]\ntp['item_cnt_month'].clip(0., 20., inplace=True)\ntp['preds'] = np.expm1(m.predict(Xt))\npd.concat((tp['date_block_num'], tp.groupby('date_block_num').transform('sum')), axis=1).drop_duplicates().append(\n    {'date_block_num': 33,'item_cnt_month': da['item_cnt_month'][da['date_block_num'] == da['date_block_num'].max()]\n     .clip(0., 20.).sum(), 'preds': np.expm1(m.predict(Xv)).sum()}, ignore_index=True).groupby('date_block_num').sum().plot()\nplt.show()","a66511d8":"mask = (da['date_block_num'] > da['date_block_num'].max() - 12)\nX_train = da[selected][mask]\n# X_train = da.drop(columns=['item_cnt_month', 'date_block_num', 'key'])[mask]\ny_train = np.log1p(da['item_cnt_month'].clip(0., 20.))[mask]","90e182f4":"%%time\nm = RandomForestRegressor(n_estimators=50, max_features=0.85, min_samples_leaf=5,\n                          n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint(\"-\"*30, f'''\nTraining score: {m.score(X_train, y_train)*100:.2f}%\nRMSE: {RMSE(m.predict(X_train), y_train):.4f}\n''')","4c351d0c":"X_test = test_set[selected]\n# X_test = test_set.drop(columns=['date_block_num', 'key'])\ny_test = np.expm1(m.predict(X_test))","27dcea1f":"tp = da[['date_block_num']][mask]\ntp['preds'] = np.expm1(m.predict(X_train))\ntp = tp.groupby('date_block_num').transform('sum').drop_duplicates()\ntp.append({'preds': sum(y_test)}, ignore_index=True).plot()\nplt.show()","11cdb94d":"test_set['item_cnt_month'] = y_test","5cde2236":"name = 'submission_v4.csv'\nmy_submission = test_set[['item_cnt_month']].clip(0., 20.)\nmy_submission.to_csv(name)","dae0918c":"pd.read_csv(name).head()","5043efb7":"Finally, we want to summarise the sold units which are going to be our dependant variable in the model later on:","9c237a6e":"As for our dependant variable, we will train the model on log of monthly item count, rather than the level itself - this way are able to address the positive skew of the distribution that is mostly comprising  zeros.","1578aa4f":"As we expected, the plots match!\n\n","c2aee299":"Time to get the final results:","a09561e8":"Let's review our November predictions in comparison to the previous months for a sense check of our predictions:","66f0c3f3":"The previous month sales, item price and item category are among the most important features according to our model.\n\nLet's see if  we can get comparable predictions with just the main predictors as per the above chart:","8b12b95f":"For extra level of verification, let's compare the monthly number of items sold (`item_cnt_day`) and the monthly sales (`item_cnt_day` times `item_price`) per the original imported dataset and our aggregated set:","58bd814f":"**Submission**\n\nThe only step left now is to create the final submission:","2dce7849":"**Parsing the date**\n\nAs we've seen on the plot earlier, it is very likely seasonality is affecting the monthly sales. This means there is potentially valuable information in the date field - and that is what we are now going to extract into additional features using Pandas built in features:","ee460535":"## Modelling","f666a223":"Looks like 363 item id's vere never reported in our training data - again, something to take note of during feature engineering.","5bf35182":"We start off by defining the evaluation metric which, as per the competition rules, is RMSE (Root Mean Squared Error):","66127afa":"Seasonal spikes in December is something that is immediately apparent - most likely due to Christmas sales. This is certainly something to take note of for feature engineering.\n\nSpeaking of which...","3a5174c6":"## Problem Statement","d4178a5b":"Given the potential seasonality in the data, we will also add indicator variables for quarters of the year and Christmas time:","102c1499":"Only the `shop_id` and `item_id` are provided in the test set. As per the competition rules, we are looking for total number of items sold per shop. In order to obtain this data we will need to convert our daily training dataset into a monthly aggregate.\n\nIt's worth noting the combination of particular item and the shop it is being sold in defines a unique combination we are asked to forecast. Let's check if the training data we have allows us to infer the values for all unique combinations in the test set. To make the calculation easier, we'll define a unique key which will simply be a concatenation of the item and shop id's:","45800662":"**Missing item-shop combinations**\n\nAs we've seen previously, not all item-shop combinations that we are supposed to infer upon are available in our dataset. To allow our model to learn from these missing observations as well, we'll append the dataset to include them.\n\nSuch observations will also need an item price assigned. As an assumption, we will use the average item price up to that point in time:","26766c6e":"## Imports","351be38c":"We have three dimension tables supplied. Looking at them in turns, we note that:\n1. `shops` only contains the name of the shop which is irrelevant to our problem - we will therefore omit importing it into our dataset\n1. `items` has item names that are also irrelevant to the problem. However, it also holds the item category id which might prove to be relevant - that's something we definitely want to keep\n1. `item_cat` only holds category name - similar story as with `shops`\n\nThis means we only want to merge part of the `items` table with our dataset - and the test set too, using the occasion:","c836f10b":"The above dataset is an example of a [star schema](https:\/\/en.wikipedia.org\/wiki\/Star_schema), with **sales** being the *fact table* and the rest of our imported datasets being the *dimension tables*.","0b5c752a":"**Validation and Training sets**\n\nWe are dealing with a temporal problem here. This means random sampling for model validation purposes is not the best idea.\n\nAs a reminder, we are supposed to forecast the sales for the next month, which means November 2015, since the last month reported in our dataset is October 2015:","875469df":"**Missing values check**\n\nNow let's check for missing values in the combined dataset and the test set:","f1ddc6c0":"**Adding missing features to the test set**\n\nSince we have our additional features created for modelling purposes, it is worth creating them in the test set as well to allow forecasting from the model later on:","afeca26c":"Looks like everything went fine.","539a632c":"We note that the previous month sales fall beneath the the numbers we've actually observed in the past. This is because there are some shop-item combinations in the training set that are not present in the test set - we did not fill in the missing values for these. If we look at the same plot for only the combinations available in the test set, we see the expected output:","2091d86f":"## Modelling","eec9d21f":"Almost half of the combinations we are asked to forecast have never been observed in the training data - i.e. these shops never sold that particular item. This is an important realisation as we may need to amend our dataset to allow the model to correctly infer whenever this is the case.\n\nFor completeness, we check if all the shops and items from the test set are observed in the training set:","63f9545e":"This kernel focuses on solving a sales forecasting business problem as described by the [Predict Future Sales](https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales) competition. \n\nAs per the competition overview page:\n> *In this competition you will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.* \n\n>*We are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.*\n\nLet's get to it then!","b13bdbb5":"Our results seem to be underestimated, most likely due to log translation of clipped values. Otherwise, the model appears to be doing a reasonable job at forecasting October values.","a57541fe":"Looks reasonable, at least on a monthly aggregate level.","1d02e809":"**Dataset aggregation**\n\nNow it's time to group the dataset into monthly aggregates. Let's look at the current set of features:","a24560e0":"Now, time to make things happen!","6bb0b05e":"We will use a random forest regressor to deal with this problem:","06b3deb7":"**Training set**","2a739b66":" **Test set**\n\nLet's import the test set for the competition:","1468da23":"**Combining the dataset**\n\nSince this is a structured data problem, we need to follow some feature engineering steps before we apply a model to get our predictions. First of all, let's combine out star schema into a single table.","5d0601fe":"Now, let's add the number of weeks and number of days in a month. For this purpose, we will use the provided `date_block_num` which simply indicates the consecutive month number counting from the start of the analysed period:","2a60ad3f":"**Lagged Features**\n\nPast sales performance can often be a very good indicator of how good our business will be looking in the future. To verify that this is also the case for 1C shops, we will introduce some lagged features into the mix. \n\nIn an annual sales cycle, choosing last month, last quarter and last year sales sounds like a reasonable choice. Let us define these features then:","c9cebc22":"It's worth checking if we introduced any unwanted duplication in our dataset: ","390ee7c3":"**Initial glance at the data**\n\nTo have an idea of what we're up against, let's look at the monthly aggregates of number of items sold  (`item_cnt_day` - which will be our dependent variable in the modelling section) and plot it over the analysed months (`date_block_num`):","1a6adc9d":"So far so good!","f44fee04":"Therefore, setting October 2015 observations as our validation set for model training seems to be a good choice. \n\nAs for the data we will use for training, for computational purposes we will limit it to the last year (luckily, our lagged features will still hold some information on the previous periods).","ebd772e7":"In order to aggregate the dataset, we need to understand which features will need to be grouped:","c927a8c9":"**Final model**\n\nNow that we have satisfactory results on the validation set, we will retrain the model with the October data included. This will allow us to use the most up to date information when forecasting November.","1c69e33c":"We will also clip the item count values to range between 0 and 20, as per the competition rules:","344a66b8":"We also want to take the average price in a month in case there was a price change intra month:","c228d7a4":"**Feature Importance**\n\nLet's review which features have the biggest predictive power:","a82f7aeb":"## Feature Engineering","241e91e4":"We will be reducing the final dataset to monthly aggregates eventually but for now, we also extract information like week and day number to calculate the number of weeks and days in a month as these may also prove to have impact on the number of sales in a month:","cb7bd2bb":"## Getting the Data","2727f625":"RMSE has slightly improved as a result, we'll use this specification of the model for the final output then.\n\nLet's plot the results in a similar fashion as before:"}}