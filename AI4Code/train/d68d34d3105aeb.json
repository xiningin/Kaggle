{"cell_type":{"897bcab9":"code","8c4073c1":"code","47d1632b":"code","32d009f0":"code","a07f09b7":"code","9c414847":"code","dd0ed8f5":"code","53a647cc":"code","1f5a41d0":"code","f25c6c69":"code","9c23e833":"code","f5c71fef":"code","ad715f11":"code","e25603a7":"code","9fab8007":"code","e6d18090":"code","71200d56":"code","3a80ef67":"code","22b978e5":"code","9dbd2baf":"code","55fc189d":"code","510e4b02":"code","57de6ba7":"code","784d4c62":"code","94b9c6ce":"code","0be8adca":"code","710a3ab4":"code","230bc2a4":"code","4a07b327":"code","0ae08231":"code","622da3db":"code","7743babb":"code","e4f847a9":"code","69639f15":"code","1b6c31e6":"code","a6a74acf":"code","9892d8e0":"code","edeb1702":"code","2cf1258e":"code","d995ffd4":"code","43b1e337":"code","090371dd":"code","901ac15c":"code","a264b202":"code","ff423c35":"code","bae218a6":"markdown","8b387c5c":"markdown","8f466445":"markdown","a67a4808":"markdown","01f94166":"markdown","85f190f1":"markdown","78c7e65f":"markdown","7c5b8cf9":"markdown","97131afe":"markdown","0e06b48f":"markdown","b7c0373d":"markdown","84abaaf5":"markdown","04ccf457":"markdown"},"source":{"897bcab9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c4073c1":"df = pd.read_csv('\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv')\ndf['date'] = pd.to_datetime(df['date'])\ndf.head(5)","47d1632b":"df['CPM'] = (df['total_revenue'] \/ df['measurable_impressions']) * 100 * 1000\ndf.loc[df['measurable_impressions']==0, 'CPM'] = 0","32d009f0":"df[['revenue_share_percent', 'integration_type_id']].nunique()","a07f09b7":"df.drop(['revenue_share_percent', 'integration_type_id'], axis='columns', inplace=True)","9c414847":"split_mask = df['date'] < pd.to_datetime('2019-06-22')\ntrain_df, test_df = df[split_mask], df[~split_mask]\ntrain_df = train_df[train_df['CPM'] >= 0]\ntest_df = test_df[test_df['CPM'] >= 0]\n\ntrain_df = train_df[train_df['CPM'] <= np.percentile(train_df['CPM'], 95)]\ntest_df = test_df[test_df['CPM'] <= np.percentile(test_df['CPM'], 95)]\n\ntrain_df['sample'] = 'train'\ntest_df['sample'] = 'test'\n\ndf = pd.concat([train_df, test_df])","dd0ed8f5":"df.columns","53a647cc":"cat_cols = [\n    'site_id', 'ad_type_id', 'device_category_id', 'line_item_type_id', 'os_id',\n    'monetization_channel_id'\n]\nid_cols = [\n    'geo_id', 'advertiser_id', 'order_id', 'ad_unit_id'\n]\nall_discrete_cols = cat_cols + id_cols\n\nother_cols = [\n    'total_impressions', 'viewable_impressions', 'measurable_impressions'\n]\nother_cols_wo_targets = [\n    'total_impressions', 'viewable_impressions'\n]\n\ntarget_col = 'CPM'","1f5a41d0":"train_df[all_discrete_cols].nunique()","f25c6c69":"from sklearn.metrics import mean_squared_error","9c23e833":"mean_squared_error(test_df[target_col], np.zeros(test_df.shape[0]))","f5c71fef":"mean_squared_error(\n    test_df[target_col], \n    np.ones(test_df.shape[0]) * np.mean(train_df[target_col])\n)","ad715f11":"from sklearn.model_selection import TimeSeriesSplit, cross_validate\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.compose import ColumnTransformer","e25603a7":"train_df = train_df.sort_values('date')\ncv = TimeSeriesSplit(n_splits=5)","9fab8007":"baseline_model = Pipeline([\n    ('ohe', OneHotEncoder(handle_unknown='ignore')),\n    ('linreg', LinearRegression())\n])","e6d18090":"# baseline_cv = cross_validate(\n#     baseline_model, \n#     train_df[all_discrete_cols], train_df[target_col],\n#     cv=cv,\n#     scoring='neg_mean_squared_error',\n#     n_jobs=-1,\n# )\n\n# baseline_cv","71200d56":"baseline_model.fit(train_df[all_discrete_cols], train_df[target_col])\nmean_squared_error(baseline_model.predict(test_df[all_discrete_cols]), test_df[target_col])","3a80ef67":"from matplotlib import pyplot as plt\nfig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n\ntrain_df['CPM'].hist(bins=40, ax=ax1)\nax1.set_title('train')\n\ntest_df['CPM'].hist(bins=40, ax=ax2)\nax2.set_title('test');","22b978e5":"from sklearn.linear_model import LogisticRegression","9dbd2baf":"cpm_divider = Pipeline([\n    ('ohe', OneHotEncoder(handle_unknown='ignore')),\n    ('logreg', LogisticRegression(solver='sag', max_iter=500))\n])\n# cv_logreg = cross_validate(\n#     cpm_divider, \n#     train_df[all_discrete_cols], (train_df[target_col]>0), \n#     scoring=['roc_auc', 'precision', 'recall'],\n# )\n# cv_logreg","55fc189d":"from sklearn.base import BaseEstimator\nclass CustomPredictor(BaseEstimator):\n    def __init__(self, divider, predictor):\n        self.divider = divider\n        self.predictor = predictor\n        self.threshold = 0.5\n        \n    def fit(self, X, y):\n        cpm_over_zero = y > 0\n        self.divider.fit(X, cpm_over_zero)\n        self.predictor.fit(X[cpm_over_zero], y[cpm_over_zero])\n    \n    def predict(self, X):\n        divider_score = self.divider.predict_proba(X)[:, 1]\n        predicted_cpm = self.predictor.predict(X)\n        predicted_cpm[divider_score < self.threshold] = 0\n        return predicted_cpm","510e4b02":"cpm_divider = Pipeline([\n    ('ohe', OneHotEncoder(handle_unknown='ignore')),\n    ('logreg', LogisticRegression(solver='sag', max_iter=500))\n])\ncpm_predictor = Pipeline([\n    ('ohe', OneHotEncoder(handle_unknown='ignore')),\n    ('linreg', LinearRegression())\n])\n\ncustom = CustomPredictor(cpm_divider, cpm_predictor)","57de6ba7":"# custom_cv = cross_validate(\n#     custom, \n#     train_df[all_discrete_cols], train_df[target_col], \n#     scoring='neg_mean_squared_error',\n# )\n# custom_cv","784d4c62":"custom.fit(train_df[all_discrete_cols], train_df[target_col])\nmean_squared_error(test_df[target_col], custom.predict(test_df[all_discrete_cols]))","94b9c6ce":"df_first = df.copy()\ndf_first.sort_values('date', inplace=True)\ndf_first['view_to_total_impressions'] = df_first['viewable_impressions'] \/ df_first['total_impressions']\ndf_first.loc[df_first['total_impressions']==0, 'view_to_total_impressions'] = 0\n\ndf_first = df_first.drop('total_revenue',axis=1)","0be8adca":"from lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split\n\n# clip irrelevant predictions\ndef clip(x):\n    return np.clip(x, 0, None)\n","710a3ab4":"params = {\n    'alpha': 0.8,\n    'colsample_bytree': 0.6000000000000001,\n    'learning_rate': 0.01,\n    'n_estimators': 1950,\n    'num_leaves': 511,\n    'subsample': 0.6,\n    'objective': 'mse'\n}\nmodel_base = LGBMRegressor(random_state=17, **params)","230bc2a4":"is_train = df_first['sample'] == 'train'\nis_test = df_first['sample'] == 'test'\n\nnew_train, new_test = df_first[is_train], df_first[is_test]\ntrain_df, eval_df = train_test_split(new_train, test_size=0.2, random_state=17)\n\nmodel_base.fit(\n    train_df.drop(['CPM','date','sample'],axis=1), train_df['CPM'],\n    eval_set=(eval_df.drop(['CPM','date','sample'],axis=1), eval_df['CPM']),\n    early_stopping_rounds=20,\n    verbose=80,\n)\nmean_squared_error(new_test['CPM'], clip(model_base.predict(new_test.drop(['CPM','date','sample'],axis=1))))","4a07b327":"num_columns = [\n    'viewable_impressions',\n    'measurable_impressions',\n    'total_impressions', \n    'view_to_total_impressions'\n]","0ae08231":"aggs = ['mean', 'sum', 'median']\nagg_df = []\nfor agg in aggs:\n    agg_df.append(df_first.rolling(1)[num_columns].agg(agg).add_suffix(f'_{agg}'))\nagg_df = pd.concat(agg_df, axis='columns')\nagg_df.head(5)","622da3db":"from tqdm.auto import tqdm\nfrom collections import defaultdict\nfrom itertools import product\n\ndef compute_agg_by_cat_cols(df, cols_to_agg):\n    dates = df['date'].unique()\n    dates.sort()\n\n    aggs = ['sum', 'mean', 'median']\n\n    group_stats_df = defaultdict(lambda: defaultdict(list))\n    for agg, date, window_days in tqdm(list(product(aggs, dates, range(1, 8)))):\n        idx = np.searchsorted(dates, date)\n        window_dates = dates[idx - window_days:idx]\n\n        window_df = df[df['date'].isin(window_dates)]\n        if window_df.shape[0] == 0:\n            continue\n        stat_df = (\n            window_df\n            .groupby(id_cols)[cols_to_agg]\n            .agg(agg)\n            .add_suffix(f'_win_{window_days}_{agg}')\n        )\n        for id_col in id_cols:\n            grouped_by_col = (\n                window_df\n                .groupby(id_col)[cols_to_agg]\n                .agg(agg)\n                .add_suffix(f'_win_{window_days}_{agg}_{id_col}')\n            )\n            stat_df = stat_df.join(grouped_by_col)\n        stat_df['date'] = date\n        group_stats_df[agg][window_days].append(stat_df)\n    return group_stats_df\n\ndef compute_agg_by_id_cols(df, cols_to_agg):\n    dates = df['date'].unique()\n    dates.sort()\n    \n    aggs = ['sum', 'mean', 'median']\n    \n    group_stats_df_cat = defaultdict(lambda: defaultdict(list))\n    for agg, date, window_days in tqdm(list(product(aggs, dates, range(1, 8)))):\n        idx = np.searchsorted(dates, date)\n        window_dates = dates[idx - window_days:idx]\n\n        window_df = df[df['date'].isin(window_dates)][cat_cols + cols_to_agg]\n        stat_df = window_df[cat_cols].copy().drop_duplicates()\n        for id_col in cat_cols:\n            grouped_by_col = (\n                window_df\n                .groupby(id_col)[cols_to_agg]\n                .agg(agg)\n                .add_suffix(f'_win_{window_days}_{agg}_{id_col}')\n            )\n            stat_df = stat_df.join(grouped_by_col, on=id_col)\n        stat_df['date'] = date\n        group_stats_df_cat[agg][window_days].append(stat_df)\n    return group_stats_df_cat","7743babb":"cols_to_agg = num_columns\n\ngroup_stats_df = compute_agg_by_cat_cols(df_first, cols_to_agg)\ngroup_stats_df_cat = compute_agg_by_id_cols(df_first, cols_to_agg)","e4f847a9":"from itertools import chain\nfull_concated = df_first\n\njoin_cols = id_cols + ['date']\naggs, win_days = ['mean'], [7]\nfor agg, win_day in product(aggs, win_days):\n    stat_df = pd.concat(group_stats_df[agg][win_day]).reset_index().set_index(join_cols)\n    full_concated = full_concated.join(stat_df, on=join_cols)\n\njoin_cols = cat_cols + ['date']\naggs, win_days = ['mean'], [7]\nfor agg, win_day in product(aggs, win_days):\n    stat_df = pd.concat(group_stats_df_cat[agg][win_day]).set_index(join_cols)\n    \n    full_concated = full_concated.join(stat_df, on=join_cols, how='left')\nfull_concated = full_concated.join(agg_df)","69639f15":"is_train = full_concated['sample'] == 'train'\nis_test = full_concated['sample'] == 'test'\n\nnew_train, new_test = full_concated[is_train], full_concated[is_test]\ntrain_df, eval_df = train_test_split(new_train, test_size=0.2, random_state=17)","1b6c31e6":"model_hist = LGBMRegressor(random_state=17, **params)","a6a74acf":"model_hist.fit(\n    train_df.drop(['CPM','date','sample'],axis=1), train_df['CPM'],\n    eval_set=(eval_df.drop(['CPM','date','sample'],axis=1), eval_df['CPM']),\n    early_stopping_rounds=20,\n    verbose=80,\n)\nmean_squared_error(new_test['CPM'], clip(model_hist.predict(new_test.drop(['CPM','date','sample'],axis=1))))","9892d8e0":"cols_to_agg = num_columns + ['CPM']\n\ngroup_stats_cpm_df = compute_agg_by_cat_cols(df_first, cols_to_agg)\ngroup_stats_cpm_cat_df = compute_agg_by_id_cols(df_first, cols_to_agg)","edeb1702":"from itertools import chain\nfull_concated_cpm = df_first\n\njoin_cols = id_cols + ['date']\naggs, win_days = ['mean'], [1, 2, 7]\nfor agg, win_day in product(aggs, win_days):\n    stat_df = pd.concat(group_stats_cpm_df[agg][win_day]).reset_index().set_index(join_cols)\n    full_concated_cpm = full_concated_cpm.join(stat_df, on=join_cols)\n\njoin_cols = cat_cols + ['date']\naggs, win_days = ['mean'], [1, 2, 7]\nfor agg, win_day in product(aggs, win_days):\n    stat_df = pd.concat(group_stats_cpm_cat_df[agg][win_day]).set_index(join_cols)\n    \n    full_concated_cpm = full_concated_cpm.join(stat_df, on=join_cols, how='left')\nfull_concated_cpm = full_concated_cpm.join(agg_df)","2cf1258e":"is_train = full_concated_cpm['sample'] == 'train'\nis_test = full_concated_cpm['sample'] == 'test'\n\nnew_train_cpm, new_test_cpm = full_concated_cpm[is_train], full_concated_cpm[is_test]\ntrain_df, eval_df = train_test_split(new_train_cpm, test_size=0.2, random_state=17)","d995ffd4":"model_hist_cpm = LGBMRegressor(random_state=17, **params)\nmodel_hist_cpm.fit(\n    train_df.drop(['CPM','date','sample'],axis=1), train_df['CPM'],\n    eval_set=(eval_df.drop(['CPM','date','sample'],axis=1), eval_df['CPM']),\n    early_stopping_rounds=20,\n    verbose=80,\n)","43b1e337":"mean_squared_error(new_test_cpm['CPM'], clip(model_hist_cpm.predict(new_test_cpm.drop(['CPM','date','sample'],axis=1))))","090371dd":"cols_to_agg = ['CPM']\n\ngroup_stats_only_cpm_df = compute_agg_by_cat_cols(df_first, cols_to_agg)\ngroup_stats_only_cpm_df_cat = compute_agg_by_id_cols(df_first, cols_to_agg)","901ac15c":"from itertools import chain\nfull_concated = df_first\n\njoin_cols = id_cols + ['date']\naggs, win_days = ['mean'], [1, 5, 7]\nfor agg, win_day in product(aggs, win_days):\n    stat_df = pd.concat(group_stats_only_cpm_df[agg][win_day]).reset_index().set_index(join_cols)\n    full_concated = full_concated.join(stat_df, on=join_cols)\n\njoin_cols = cat_cols + ['date']\naggs, win_days = ['mean'], [1, 5, 7]\nfor agg, win_day in product(aggs, win_days):\n    stat_df = pd.concat(group_stats_only_cpm_df_cat[agg][win_day]).set_index(join_cols)\n    \n    full_concated = full_concated.join(stat_df, on=join_cols, how='left')\nfull_concated = full_concated.join(agg_df)","a264b202":"is_train = full_concated['sample'] == 'train'\nis_test = full_concated['sample'] == 'test'\n\nnew_train, new_test = full_concated[is_train], full_concated[is_test]\ntrain_df, eval_df = train_test_split(new_train, test_size=0.2, random_state=17)","ff423c35":"model = LGBMRegressor(random_state=17, **params)\nmodel.fit(\n    train_df.drop(['CPM','date','sample'],axis=1), train_df['CPM'],\n    eval_set=(eval_df.drop(['CPM','date','sample'],axis=1), eval_df['CPM']),\n    early_stopping_rounds=20,\n    verbose=80,\n)\nmean_squared_error(new_test['CPM'], clip(model.predict(new_test.drop(['CPM','date','sample'],axis=1))))","bae218a6":"### MSE 2720 > 2667 for model without historical features. May be we should add some smoothing or drop stat for smth categories","8b387c5c":"## MSE 2457","8f466445":"### MSE = 4234 without tuning threshold for logreg","a67a4808":"## Add historical features without CPM\n### Compute rolling stats","01f94166":"## Feature engineering and boosting","85f190f1":"### MSE 2667","78c7e65f":"## Baseline on OHE + LinReg","7c5b8cf9":"## Add historical CPM from previous dates with different window size \nIt's ok in production case, because on date[i] we already know CPM for date[j] when j < i","97131afe":"## try to classify CPM == 0","0e06b48f":"### Simple boosting","b7c0373d":"### MSE = 4569 :good-enough: :pepe-happy:","84abaaf5":"### MSE 2485","04ccf457":"## Add historical features based only on CPM"}}