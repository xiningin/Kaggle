{"cell_type":{"999efeeb":"code","7e9fc2d2":"code","1be802d3":"code","982717fb":"code","fe2f7ddd":"code","7f0e116a":"code","fed4015c":"code","28025152":"code","f496f88c":"code","5af1f923":"code","9fc38b6e":"code","23776e45":"code","6b9f1499":"code","73cb63e4":"code","3d7e54a2":"code","64ea78ac":"code","f88d1968":"code","f1a826c7":"code","7cb1cad2":"code","c72ef619":"code","eae5daba":"code","111e2966":"code","6e1f0492":"code","806b2344":"code","3f8fba1a":"code","44ceff12":"code","1ed3aed0":"code","a7068abf":"markdown","c42328b7":"markdown","b32a940f":"markdown","084a92d3":"markdown","4d3bdb5e":"markdown","1107c139":"markdown","94d23e12":"markdown","963103e4":"markdown","abda904e":"markdown","f9dab00c":"markdown","1129279e":"markdown"},"source":{"999efeeb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom scipy.interpolate import interp1d\nfrom datetime import date\nimport calendar","7e9fc2d2":"sb.set_style(\"whitegrid\")\n\n\"\"\"sns.set_style preset themes: darkgrid, whitegrid, \ndark, white, ticks\n\"\"\"\n\naptdata = pd.read_csv('..\/input\/KaggleV2-May-2016.csv')\naptdata=aptdata.rename(columns = {'Neighbourhood':'Neighborhood','Hipertension':'Hypertension','Handcap':'Handicap'})","1be802d3":"aptdata.info()\n'No Null Values'","982717fb":"aptdata.head()","fe2f7ddd":"aptdata['ScheduledDay']=pd.to_datetime(aptdata['ScheduledDay'])\naptdata['AppointmentDay']=pd.to_datetime(aptdata['AppointmentDay'])\naptdata['daystillapt']=(aptdata['AppointmentDay']-aptdata['ScheduledDay']).dt.days\naptdata.loc[aptdata['daystillapt']<0,'daystillapt']=0","7f0e116a":"aptdata['WeekdayNum']=aptdata['AppointmentDay'].dt.weekday\n\n'''edit data so that numerical analysis can be done'''\naptdata.loc[aptdata['Age']<0,'Age']=0\naptdata.loc[aptdata['No-show']=='No','Show']=1\naptdata.loc[aptdata['No-show']=='Yes','Show']=0\naptdata.loc[aptdata['Gender']=='F','Sex']=1\naptdata.loc[aptdata['Gender']=='M','Sex']=0\naptdata['Neighborhood'] = aptdata.Neighborhood.astype('category')\naptdata['Neighborhood'] = aptdata['Neighborhood'].cat.codes","fed4015c":"aptdata.head()","28025152":"plt.figure(figsize=(12,12))\nsb.heatmap(aptdata.iloc[:, 2:].corr(), annot=True, square=True, cmap='BuPu')\nplt.show()","f496f88c":"aptdata['No-show'].value_counts(normalize=True).plot.bar(figsize=(10,10), title= 'No-Shows')","5af1f923":"def probStatus(dataset,variable):\n    df=pd.crosstab(index=dataset[variable],columns=dataset['No-show'])\n    df['probShowUp']=df['No']\/(df['Yes']+df['No'])\n    df=df.reset_index()\n    return df","9fc38b6e":"df=probStatus(aptdata,'Age')\nx0=df['Age']\ny0=df['probShowUp']\nplt.plot(x0,y0,'o',label='Data')\nx=np.linspace(0,100,30)\noptions = ('slinear','cubic')\nfor o in options:\n    f=interp1d(x0,y0,kind=o)\n    plt.plot(x,f(x),label=o)\n\nplt.legend()\nplt.show()","23776e45":"df=probStatus(aptdata,'daystillapt')\nx0=df['daystillapt']\ny0=df['probShowUp']\nplt.plot(x0,y0,'o',label='Data')\nx=np.linspace(0,120,30)\noptions = ('slinear','quadratic',2)\nfor o in options:\n    f=interp1d(x0,y0,kind=o)\n    plt.plot(x,f(x),label=o)\n\nplt.legend()\nplt.show()","6b9f1499":"def probStatusVariable(variable):\n    rows=[]\n    for item in variable:\n        for level in aptdata[item].unique():\n            row = {'Condition': item}\n            total = len(aptdata[aptdata[item] == level])\n            n = len(aptdata[(aptdata[item] == level) & (aptdata.Show == 1)])\n            row.update({'Level': level, 'Probability': n\/total})\n            rows.append(row)\n    return pd.DataFrame(rows)","73cb63e4":"sb.barplot(data = probStatusVariable(['Diabetes', 'Hypertension']),\n            x = 'Condition', y = 'Probability', hue = 'Level', palette = 'Set2')\nplt.ylabel('Probability')\nplt.show()","3d7e54a2":"data = aptdata.drop(['PatientId','AppointmentID','Gender','ScheduledDay','AppointmentDay','No-show'],1)\ndata = np.log1p(data)\ny = data['Show']\ndata = data.drop('Show',1)\ndata = data.values\n\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train, y_test= train_test_split(data, y ,test_size = .3, random_state= 42)","64ea78ac":"X_train.shape ,X_test.shape ,y_train.shape , y_test.shape","f88d1968":"from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\nfrom sklearn import preprocessing\nlab_enc = preprocessing.LabelEncoder()\ny_train_encoded = lab_enc.fit_transform(y_train)\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train_encoded)\nrf_pred=rf.predict(X_train)\naccuracy_score(pd.DataFrame(rf.predict(X_train)),y_train_encoded)","f1a826c7":"predictions = rf.predict(X_test)\ny_test_encoded = lab_enc.fit_transform(y_test)\nrf_score= accuracy_score(pd.DataFrame(rf.predict(X_test)),y_test_encoded)\ny=predictions[:100]\nshow_test = aptdata.Show[100000:]\nshow_test=show_test[:100]\nshow_test=show_test.reset_index()\nplt.scatter(show_test['index'], show_test['Show'], c='b', label = 'data') #blue is true value\nplt.scatter(show_test['index'], y, c='r', label = 'prediction') #red is predicted value\nrf_score","7cb1cad2":"from sklearn.metrics import classification_report, confusion_matrix\nprint(\"=== Confusion Matrix ===\")\nprint(confusion_matrix(y_test_encoded, predictions))\nprint('\\n')\nprint(\"=== Classification Report ===\")\nprint(classification_report(y_test_encoded, predictions))\nprint('\\n')","c72ef619":"list(zip(aptdata.drop(['PatientId','AppointmentID','Gender','ScheduledDay','AppointmentDay','No-show', 'Show'],1), rf.feature_importances_))","eae5daba":"gb= GradientBoostingClassifier(learning_rate=0.1, n_estimators=120\n                            ,max_depth=10, min_samples_split= 8,\n                               max_features='sqrt', \n                                    subsample=0.8, random_state=42)\ngb.fit(X_train,y_train_encoded)\ngb_pre=gb.predict(X_train)\naccuracy_score(pd.DataFrame(gb.predict(X_train)),y_train_encoded)","111e2966":"gb.predict(X_test)\ngb_score=accuracy_score(pd.DataFrame(gb.predict(X_test)),y_test_encoded)\ngb_score","6e1f0492":"predictions = gb.predict(X_test)\ngb_score= accuracy_score(pd.DataFrame(gb.predict(X_test)),y_test_encoded)\ny=predictions[:100]\nshow_test = aptdata.Show[100000:]\nshow_test=show_test[:100]\nshow_test=show_test.reset_index()\nplt.scatter(show_test['index'], show_test['Show'], c='b', label = 'data') #blue is true value\nplt.scatter(show_test['index'], y, c='r', label = 'prediction') #red is predicted value\ngb_score","806b2344":"print(\"=== Confusion Matrix ===\")\nprint(confusion_matrix(y_test_encoded, predictions))\nprint('\\n')\nprint(\"=== Classification Report ===\")\nprint(classification_report(y_test_encoded, predictions))\nprint('\\n')","3f8fba1a":"list(zip(aptdata.drop(['PatientId','AppointmentID','Gender','ScheduledDay','AppointmentDay','No-show', 'Show'],1), rf.feature_importances_))","44ceff12":"algorithms = [rf_score,gb_score]\nnames= ['Random Forest','Gradient Boosting']\nfinal = pd.DataFrame([names,algorithms]).T\nfinal.columns =['Algorithms', 'Accuracy Score'] ","1ed3aed0":"final","a7068abf":"### Exploring Variables cont.\nAbout 80% of the participants showed up to their appointments and the other 20% of participants did not show up.","c42328b7":"# Exploring Variables\nI wanted to visually see how the data was correlated so I used a heat map to illustrate the correlation between each variable.  I wanted to observe the age, and pre-existing condition variables to see if there was a positive correlation between those variables and whether the patients came to their appointments. Although there was a positive correlation with these variables and whether the patients showed up to their appointments, the correlation coefficients were too close to zero to be statistically significant.","b32a940f":"# Random Forest Classification Model\nA Random Forest Classification model was used on this dataset to get an accuracy score of 75.6%. When doing further exploration on the results of the model, the precision, recall, and f1 scores showed that the model did a good job in classifying the participants that showed up, but did not do as well when classifying the participants that did not show up.  The random forest model also showed which features had the most predictive power.  These features in order of predictive power were Age, Neighborhood, and daystillapt. ","084a92d3":"I wanted to create a model that analyzed human behavior and also related to the field of public health & medicine, so I chose a dataset that could aid in answering the question, \"Why do 30% of patients miss their scheduled appointments?\"","4d3bdb5e":"# Medical Appointment No-Shows\n\n![steps.png](attachment:steps.png)\nThe goal for this project was to carry out the following steps: it was to first identify a problem. Next, obtain clean data. Then, explore the data and understand the variables, and finally utilize the knowledge and tools of mathematical modeling to ultimately create a model for predictive analytics.","1107c139":"The random forest does a great job in classifying the participants that showed up to their appointments but they falsely classify the participants that do not show up to their appointments.","94d23e12":"### Exploring Variables (Days until appointment)\nThe graph below shows the probability of showing up with respect to days until appointment.  We can see that when appointments are scheduled sooner, patients are more likely to show up.  Especially when patients schedule appointments on the day of, they show up to their appointment.  This can also reflect walk in appointments as well, but information on this assumption for the data is limited.  After 50 days, the data becomes very sporadic.","963103e4":"### Exploring Variables (Age)\nBecause mapping the correlation coefficients did not show us anything, I created a graph to show the probability of showing up with respect to age.  We can see that infants and elderly are more likely to show up to their appointments wheras adolescents to early adults are less likely to show up.","abda904e":"# Gradient Boosting Classification Model\nA Gradient Boosting Classification model was used on this dataset to get an accuracy score of 80%. Similar to the random forest model, the precision, recall, and f1 scores showed that the model did a good job in classifying the participants that showed up, but did not do as well when classifying the participants that did not show up. The recall and f1 score performed worse for the gradient boosting classifier than the random forest classifier. The important features had the most predictive power for this model were the same as the random forest classification model: Age, Neighborhood, and daystillapt.","f9dab00c":"### Exploring Variables (Diabetes & Hypertension)\nThis bar graph shows an analysis of the already existing health characteristics--diabetes and hypertension.  The green represents the participants who do not have diabetes or hypertension, and the peach\/orange represents the participants who do have diabetes or hypertension.  We can see that people with these health characteristics are slightly more likely to show up than people who do not have these health characteristics.","1129279e":"# The Data\nThe dataset that was pulled from kaggle contains over one hundred thousand medical appointments that were scheduled for 2016 in Vitoria, Espirito Santo, Brazil.  It contains over 15 variables such as gender, age, medical history, what day of the week the appointment was, and most importantly, if the patient showed up to the appointment or not.  Python was used for the analysis.  Some assumptions that were made were that all of the patients were visiting a primary care physican and not a specialist."}}