{"cell_type":{"330c8cb2":"code","49ba88fd":"code","61e3df83":"code","87b2a129":"code","5fd60222":"code","fe425479":"code","dd579bd9":"code","5edf511a":"code","3e12cd14":"code","d592d1dd":"code","a943ae87":"code","471aad11":"code","21ee171a":"code","bbbbfe62":"code","aa3b1a56":"code","7a17d0ed":"code","4fb16573":"code","0b5aea26":"code","364ea993":"code","d16510c0":"code","971ae7fb":"code","5c1c5453":"code","7f7441bd":"code","a0879719":"markdown","358d9987":"markdown","93df7629":"markdown","4b36ffc8":"markdown","628d5f2f":"markdown","8f073dc8":"markdown","e85202c0":"markdown","6091ea21":"markdown","697673fb":"markdown","6e5fc876":"markdown","92d8c4bb":"markdown","f0fb976f":"markdown","b2f6b29d":"markdown"},"source":{"330c8cb2":"from google.colab import files\nfiles.upload() #upload kaggle.json","49ba88fd":"!mkdir -p ~\/.kaggle\n!cp kaggle.json ~\/.kaggle\/\n\n!chmod 600 ~\/.kaggle\/kaggle.json\n!kaggle datasets download -d ashirwadsangwan\/imdb-dataset","61e3df83":"from zipfile import ZipFile\nfile_name = \"imdb-dataset.zip\"\n\nwith ZipFile(file_name, \"r\") as zip:\n  zip.extractall()\n  print(\"The dataset has been unzipped\")","87b2a129":"!ls","5fd60222":"!pip install pyspark","fe425479":"from pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import collect_set, col, count\nfrom pyspark.sql import SQLContext","dd579bd9":"conf = SparkConf().setAppName(\"MarketBasket\")\nconf = (conf.setMaster('local[*]')\n        .set('spark.executor.memory', '4G')\n        .set('spark.driver.memory', '45G')\n        .set('spark.driver.maxResultSize', '10G'))\nsc = SparkContext(conf=conf)","5edf511a":"sqlContext = SQLContext(sc)\n\ntitle_basics = 'title.basics.tsv.gz'\ntitle_basics = sqlContext.read.csv(title_basics, header=True, sep = '\\t')\ntitle_basics.show(5) # see what the table looks like originally\n\n# there are 'short' 'video' in titleType, keep 'movie' only\ntitle_basics = title_basics.filter((title_basics.titleType == \"movie\"))\ntitle_basics.show(5) # see what the table looks like after filtering","3e12cd14":"name_basics = 'name.basics.tsv.gz'\nname_basics = sqlContext.read.csv(name_basics, header=True, sep = '\\t')\nname_basics.show(5) # see what the table looks like ","d592d1dd":"title_principals = 'title.principals.tsv.gz'\ntitle_principals = sqlContext.read.csv(title_principals, header=True, sep = '\\t')\ntitle_principals.show(5) # see what the table looks like originally\n\ntitle_principals = title_principals.filter((title_principals.category == \"actor\") | (title_principals.category == \"actress\")) # keep only actor and actress\ntitle_principals = title_principals.select(col(\"tconst\"),col(\"nconst\")) # select movie id and actor id \ntitle_principals.show(5) # see what the table looks like after filtering\n","a943ae87":"title_principals = title_principals.join(title_basics, [\"tconst\"], 'leftsemi')\ntitle_principals.show(5) # print example","471aad11":"import itertools","21ee171a":"def sum(a,b):\n  return a+b\n\n# checks if a set of item is a subset of a basket.\ndef check_subset(rddlist, filt):\n  for item in filt:\n    if set(list(item)).issubset(set(rddlist)):\n      return ((item, 1))","bbbbfe62":"def apriori(rdd, threshold):\n  \n  flat_list  = rdd.flatMap(list) \n\n  singleton = flat_list.map(lambda item: (item , 1)) #add one for each actor appearence\n  singleton_summed = singleton.reduceByKey(sum) #sum of values by actor as key\n  singleton_filtered = singleton_summed.filter(lambda item: item[1] >= threshold ) #consider items that appear singularly a number of time larger than the threshold in the baskets.\n\n  #obtain a list of the codes of the items\n  frequent_actors = singleton_filtered.map(lambda item: (item[0]))\n\n  #Obtain all the pairs of frequent items:\n  pairs_list = list(itertools.combinations(frequent_actors.toLocalIterator(),2))\n\n  #Create the support table for the pairs of items by applying the filtering function previously created.\n  support_table_pairs = rdd.map(lambda x : check_subset(x, pairs_list)).filter(lambda x: x is not None).cache() #Apply filtering function to check if a pairs appear in the movies\n  support_table_pairs_summed = support_table_pairs.reduceByKey(sum) # sum of values by actor as key\n  support_table_pairs_filtered = support_table_pairs_summed.filter(lambda item: item[1] >= threshold) #consider just the actor that performed in more than support value\n\n  return (support_table_pairs_filtered)","aa3b1a56":"#create a list containing baskets: each basket is a movie and it is a list containing the actors that performed in that film\nbaskets = title_principals.groupBy(\"tconst\").agg(collect_set(\"nconst\").alias(\"actors\"))\n\n#list of baskets containing actors divided by movies\nbasket_list = baskets.select('actors').rdd.flatMap(list)\n\n#Parallelize the list in RDD\nbasket_list = sc.parallelize(basket_list.collect(),10)\n\nprint(basket_list.collect()[:2]) #print example","7a17d0ed":"minSupport = 100\nnumPartitions = basket_list.getNumPartitions()\nadjSupport = minSupport\/numPartitions\nadjSupport","4fb16573":"rdd_object = sc.parallelize([]) #initialize a RDD \n\nfor i in range(0, 2): # since the whole data is too large, I keep only the first two chunks \n  partition = sc.parallelize(basket_list.glom().collect()[i])\n  support_table_pairs_filtered = apriori(partition, adjSupport)\n  rdd_chunk = support_table_pairs_filtered.map(lambda item: (item[0],1))\n  rdd_object = rdd_object.union(rdd_chunk)","0b5aea26":"#Convert the RDD to dataframe\ncolumns = [\"actor_pair\",\"movies\"]\nrdd_sets = rdd_object.toDF(columns)\nrdd_sets.createOrReplaceTempView(\"rdd_sets\")\nsqlContext.sql(\"SELECT * FROM rdd_sets ORDER BY movies DESC\").show(truncate = False)","364ea993":"actors = rdd_object.map(lambda item : item[0])\nactors_list = actors.collect()","d16510c0":"rdd_check = basket_list.map(lambda x : check_subset(x, actors_list)).filter(lambda x: x is not None).cache()\nrdd_summed = rdd_check.reduceByKey(sum)\nrdd_filtered = rdd_summed.filter(lambda item: item[1] >= minSupport)\n\nprint(rdd_filtered.collect())","971ae7fb":"#convert the  RDD to dataframe\ncolumns = [\"actor_pair\",\"movies\"]\nresults = rdd_filtered.toDF(columns)\nresults.createOrReplaceTempView(\"results\")\nsqlContext.sql(\"SELECT * FROM results ORDER BY movies DESC\").show(truncate = False)","5c1c5453":"#list of all actors    \nflat_list  = basket_list.flatMap(list)\n\n#obtain the actors that performed in a number of movies larger than the threshold:\nsingleton = flat_list.map(lambda item: (item , 1)) #add one for each actor appearence\nsingleton_summed = singleton.reduceByKey(sum_actors) #sum of values by actor as key\nsingleton_filtered = singleton_summed.filter(lambda item: item[1] >= minSupport ) #consider just the actor that performed in more than support value\n\n#convert the first support RDD to dataframe\ncolumns = [\"actors\", \"movies\"]\nfirst_df = singleton_filtered.toDF(columns)\nfirst_df.createOrReplaceTempView(\"first_df\")\nsqlContext.sql(\"SELECT * FROM first_df ORDER BY movies DESC\").show(5, truncate = False)","7f7441bd":"name_basics.createOrReplaceTempView(\"name_basics\")\n\n#retrieve the name of the most frequent actors from the name_basics table\nsqlContext.sql(\"SELECT primaryName, actors, movies  FROM (first_df INNER JOIN name_basics ON actors = nconst) WHERE actors = 'nm2082516'\").show(truncate = False)\nsqlContext.sql(\"SELECT primaryName, actors, movies  FROM (first_df INNER JOIN name_basics ON actors = nconst) WHERE actors = 'nm0648803'\").show(truncate = False)\nsqlContext.sql(\"SELECT primaryName, actors, movies  FROM (first_df INNER JOIN name_basics ON actors = nconst) WHERE actors = 'nm0623427'\").show(truncate = False)\nsqlContext.sql(\"SELECT primaryName, actors, movies  FROM (first_df INNER JOIN name_basics ON actors = nconst) WHERE actors = 'nm0006982'\").show(truncate = False)","a0879719":"Setting Spark to deal with larger dataset","358d9987":"Combine A-prior and SON","93df7629":"[![Open In Colab](https:\/\/colab.research.google.com\/assets\/colab-badge.svg)](https:\/\/colab.research.google.com\/drive\/10I-UvzTNcjXDYDu18rEzPWem2vC5Jy0r#scrollTo=PGTL4CUt5h-o)","4b36ffc8":"Import dataset from Kaggle","628d5f2f":"Check what has been downloaded ","8f073dc8":"Create a sparkContext","e85202c0":"Result","6091ea21":"title_principals contains the principal cast\/crew for titles, which is necessary to extract movie and actors information.","697673fb":"SON algorithm","6e5fc876":"names.basic contains the actors information","92d8c4bb":"Now I have three tables: title_basics with only movie information, name_basics with only actor informaiton, title_principals with information of principal cast in a movie.","f0fb976f":"A-prior","b2f6b29d":"title_basics contains the movie information"}}