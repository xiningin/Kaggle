{"cell_type":{"5370f176":"code","cded8500":"code","b1c590b0":"code","47691733":"code","4a98c5a8":"code","f85cfdaf":"code","7c0f9e2a":"code","24b5f468":"code","a28fd5d8":"code","6ba13250":"code","9a841593":"code","df26510c":"code","9245dbbe":"code","10f9ecba":"code","93a0c162":"code","f4a75bac":"code","e6530292":"code","0aafb06a":"code","5193f7dd":"code","ba1fb564":"code","636339c8":"code","75d26a8c":"code","bbafb523":"code","5cdf28d0":"code","9aaff373":"code","0f62cd6a":"code","1bef2f50":"code","9558d20b":"code","91fbf427":"code","b8396fe6":"code","025ae62b":"code","41097954":"code","4f3cf7c3":"code","c5bd380d":"code","e27d6390":"code","365bfca0":"code","ba125e1a":"code","fa512d92":"code","6607b146":"code","d0dc2eb1":"code","0cd8b39b":"code","b5618876":"code","2bb503b2":"code","b94b7676":"code","31f7736e":"code","7c0ce725":"code","dc396cd0":"code","52c9c549":"markdown","1a1e6565":"markdown","8d75e92b":"markdown","7acc324b":"markdown","ac63e27c":"markdown","6e2eb8c7":"markdown","96ee3903":"markdown","ea4ec462":"markdown","0d4de083":"markdown","bb5ba456":"markdown","a19f1214":"markdown","8f144cdb":"markdown","49e75104":"markdown","14bc3fd5":"markdown","4a44c6b8":"markdown","013b6027":"markdown","7651559c":"markdown","d199c558":"markdown","d8e14a46":"markdown","973d3e1f":"markdown","6b60a15c":"markdown","1c8b9b0d":"markdown","82cfe075":"markdown","7e4643b1":"markdown","e76eb1e3":"markdown","01f09e77":"markdown","53d32ef2":"markdown","7923502d":"markdown","7ea2065a":"markdown","8233a17c":"markdown","9e015c92":"markdown","bafcd6bf":"markdown","a46ce971":"markdown"},"source":{"5370f176":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport random\nimport xgboost as xgb\nimport os\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import VotingClassifier","cded8500":"warnings.filterwarnings(\"ignore\")","b1c590b0":"DATA_FOLDER = os.path.join(\"..\", \"input\", \"gene-expression\/\")\nparams = {'legend.fontsize': 'x-large',\n          'figure.figsize': (15, 10),\n         'axes.labelsize': 'x-large',\n         'axes.titlesize':'x-large',\n         'xtick.labelsize':'x-large',\n         'ytick.labelsize':'x-large'}\nplt.rcParams.update(params)\nsns.set_theme(style=\"darkgrid\")\nnp.random.seed(42)","47691733":"le = LabelEncoder()","4a98c5a8":"train_data = pd.read_csv(f\"{DATA_FOLDER}data_set_ALL_AML_train.csv\")\ntest_data = pd.read_csv(f\"{DATA_FOLDER}data_set_ALL_AML_independent.csv\")\nlabels = pd.read_csv(f\"{DATA_FOLDER}actual.csv\")","f85cfdaf":"train_data.head()","7c0f9e2a":"test_data.head()","24b5f468":"print(f\"Train data has {train_data.isna().sum().sum()} NAs\")\nprint(f\"Test data has {test_data.isna().sum().sum()} NAs\")","a28fd5d8":"cols_train = [col for col in train_data.columns if \"call\" in col]\ncols_test = [col for col in test_data.columns if \"call\" in col]\ntrain_data.drop(cols_train, axis=1, inplace=True)\ntest_data.drop(cols_test, axis=1, inplace=True)","6ba13250":"train_data = train_data.T\ntest_data = test_data.T","9a841593":"train_data.head()","df26510c":"train_data.columns = test_data.iloc[1].values\ntrain_data.drop([\"Gene Description\", \"Gene Accession Number\"], axis=0, inplace=True)\ntest_data.columns = test_data.iloc[1].values\ntest_data.drop([\"Gene Description\", \"Gene Accession Number\"], axis=0, inplace=True)","9245dbbe":"train_data.head()","10f9ecba":"train_data[\"patient\"] = train_data.index.values\ntest_data[\"patient\"] = test_data.index.values","93a0c162":"train_data = train_data.astype(\"int32\")\ntest_data = test_data.astype(\"int32\")","f4a75bac":"labels[\"cancer\"] = le.fit_transform(labels[\"cancer\"])\ntrain_data = pd.merge(train_data, labels, on=\"patient\")\ntest_data = pd.merge(test_data, labels, on=\"patient\")","e6530292":"train_data[\"cancer\"].value_counts()","0aafb06a":"test_data[\"cancer\"].value_counts()","5193f7dd":"fig, axs = plt.subplots(1, 2)\nsns.countplot(x=\"cancer\", data=train_data, ax=axs[0])\naxs[0].set_title(\"Train data\", fontsize=24)\nsns.countplot(x=\"cancer\", data=test_data, ax=axs[1])\naxs[1].set_title(\"Test data\", fontsize=24)\nplt.show()","ba1fb564":"upsampled_data = random.sample(train_data.query(\"cancer == 1\")[\"patient\"].index.to_list(), k=8, )","636339c8":"upsampled_data","75d26a8c":"train_data_upsampled = pd.concat([train_data, train_data.iloc[upsampled_data, :]])","bbafb523":"fig, axs = plt.subplots(1, 2)\nsns.countplot(x=\"cancer\", data=train_data_upsampled, ax=axs[0])\naxs[0].set_title(\"Train data\", fontsize=24)\nsns.countplot(x=\"cancer\", data=test_data, ax=axs[1])\naxs[1].set_title(\"Test data\", fontsize=24)\nfig.suptitle(\"After upsampling\", fontsize=24)\nplt.show()","5cdf28d0":"X_train = train_data_upsampled.drop(columns=[\"patient\", \"cancer\"])\ny_train = train_data_upsampled[\"cancer\"]\nX_test = test_data.drop(columns=[\"patient\", \"cancer\"])\ny_test = test_data[\"cancer\"]","9aaff373":"# Features scaling\nsc = StandardScaler()\nX_train_scaled = sc.fit_transform(X_train)\nX_test_scaled = sc.transform(X_test)","0f62cd6a":"reduced_train = PCA().fit_transform(X_train_scaled)\nkmeans = KMeans(n_clusters=2, n_init=20)\nkmeans.fit(train_data_upsampled)","1bef2f50":"sns.scatterplot(x=reduced_train[0], y=reduced_train[1], hue=kmeans.labels_)\nplt.xticks(())\nplt.yticks(())\nplt.show()","9558d20b":"pca = PCA()\npca.fit_transform(X_train_scaled)\ntotal = sum(pca.explained_variance_)\nk = 0\ncurrent_variance = 0\nwhile current_variance \/ total < 0.90:\n    current_variance += pca.explained_variance_[k]\n    k = k + 1   \nprint(k, \" features explain around 90% of the variance. From 7129 features to \", k, sep='')\n\npca = PCA(n_components=k)\nX_train_pca = pca.fit(X_train_scaled)\nX_train_pca = pca.transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\n\nvar_exp = pca.explained_variance_ratio_.cumsum()\nvar_exp = var_exp*100\nplt.bar(range(1, k + 1), var_exp, color=\"brown\")\nplt.xlabel(\"Cumulutive explained variance\", fontsize=18)\nplt.ylabel(\"Number of components\", fontsize=18)\nplt.xlim((0.5, k + 1))\nplt.show()","91fbf427":"cancer_labels = train_data_upsampled[\"cancer\"].map({0: le.classes_[0], 1: le.classes_[1]}).values\npatient_labels = np.array(list(map(str, train_data_upsampled[\"patient\"].values))).astype(\"object\") + \"_\" + cancer_labels","b8396fe6":"link = linkage(X_train_scaled, 'ward', 'euclidean')","025ae62b":"dm = dendrogram(link, color_threshold=1250, labels=patient_labels)","41097954":"dist = link[:, 2]\ndist_rev = dist[::-1]\nidxs = range(1, len(dist) + 1)\nplt.plot(idxs, dist_rev, marker='o')\nplt.title('Distance between merged clusters')\nplt.xlabel('Step')\nplt.ylabel('Distance')\nplt.show()","4f3cf7c3":"rf_params = {\"bootstrap\": [False, True],\n             \"n_estimators\": [60, 70, 80, 90, 100],\n             \"max_features\": [0.6, 0.65, 0.7, 0.75, 0.8],\n             \"min_samples_leaf\": [8, 10, 12, 14],\n             \"min_samples_split\": [3, 5, 7]\n        }\n\nrf_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=rf_params, scoring=\"f1\")\nrf_search.fit(X_train_scaled, y_train)\nbest_rf = rf_search.best_estimator_","c5bd380d":"rf_prediction = best_rf.predict(X_test_scaled)\nf1_score = metrics.f1_score(y_test, rf_prediction)\nprint('Validation f1-score of RandomForest Classifier is', f1_score)\nprint (\"\\nClassification report :\\n\", metrics.classification_report(y_test, rf_prediction))\n\n# Confusion matrix\nplt.figure(figsize=(18, 14))\nplt.subplot(221)\nsns.heatmap(metrics.confusion_matrix(y_test, rf_prediction), annot=True, fmt = \"d\", linecolor=\"k\", linewidths=3)\nplt.title(\"CONFUSION MATRIX\", fontsize=20)\n\n# ROC curve\nrf_predicted_probs = best_rf.predict_proba(X_test_scaled)[:, 1]\nfpr, tpr, thresholds = metrics.roc_curve(y_test, rf_predicted_probs)\nplt.subplot(222)\nplt.plot(fpr, tpr, label = (\"Area_under the curve :\", metrics.auc(fpr, tpr)), color = \"r\")\nplt.plot([1,0], [1,0], linestyle = \"dashed\", color =\"k\")\nplt.legend(loc = \"best\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)\nplt.show()","e27d6390":"print(f\"Quantity of features with 0 importance: {(best_rf.feature_importances_ == 0).sum()}\")","365bfca0":"mask = (best_rf.feature_importances_ != 0)\nimportances = best_rf.feature_importances_[mask]\nfeature_names = train_data.columns.values[:7129][mask]","ba125e1a":"fig = plt.figure(figsize=(18, 12))\nax = fig.add_subplot(111)\nsns.barplot(x=feature_names, y=importances)\nplt.ylabel(\"Feature importance\")\nplt.xlabel(\"Feature name\")\nplt.title(\"Feature importance\", fontsize=28)\nax.set_xticklabels(feature_names, rotation = 45)\nplt.show()","fa512d92":"print(f\"Also accuracy score is: {metrics.accuracy_score(y_test, rf_prediction)}\")","6607b146":"knn_params = {\n    \"n_neighbors\": [i for i in range(1, 30, 5)],\n    \"weights\": [\"uniform\", \"distance\"],\n    \"algorithm\": [\"kd_tree\"],\n    \"leaf_size\": [1, 10, 20, 30],\n    \"p\": [1, 2]\n}\nknn_search = GridSearchCV(KNeighborsClassifier(), knn_params, n_jobs=-1, verbose=1, scoring=\"f1\")\nknn_search.fit(X_train_scaled, y_train)\nbest_knn = knn_search.best_estimator_","d0dc2eb1":"# xgb_params = {\"max_depth\": [3, 4, 5, 6, 7, 8, 10, 12],\n#               \"min_child_weight\": [1, 2, 4, 6, 8, 10, 12, 15],\n#               \"n_estimators\": [40, 50, 60, 70, 80, 90, 100, 110, 120, 130],\n#               \"learning_rate\": [0.001, 0.01, 0.05, 0.1, 0.2, 0.3]}\n# xgb_search = GridSearchCV(estimator=xgb.XGBClassifier(), param_grid=xgb_params, scoring=\"f1\")\n# xgb_search.fit(X_train_scaled, y_train)\n# best_xgb = xgb_search.best_estimator_","0cd8b39b":"nb_clf = GaussianNB()\nnb_clf.fit(X_train_scaled, y_train)","b5618876":"lr_params = {\"C\": [1e-03, 1e-2, 1e-1, 1, 10], \n      \"penalty\": [\"l1\", \"l2\"]}\nlog_refr_search = GridSearchCV(estimator=LogisticRegression(), param_grid=lr_params, scoring=\"f1\")\nlog_refr_search.fit(X_train_scaled, y_train)\nbest_lr = log_refr_search.best_estimator_","2bb503b2":"rf_f1_score = metrics.f1_score(y_test, best_rf.predict(X_test_scaled))\nknn_f1_score = metrics.f1_score(y_test, best_knn.predict(X_test_scaled))\nnb_clf_f1_score = metrics.f1_score(y_test, nb_clf.predict(X_test_scaled))\nlr_f1_score = metrics.f1_score(y_test, best_lr.predict(X_test_scaled))","b94b7676":"mean_f1_score = np.mean([rf_f1_score, knn_f1_score, nb_clf_f1_score, lr_f1_score])\nweight_rf = rf_f1_score \/ mean_f1_score\nweight_knn = knn_f1_score \/ mean_f1_score\nweight_nb = nb_clf_f1_score \/ mean_f1_score\nweight_lr = lr_f1_score \/ mean_f1_score","31f7736e":"ensemble = VotingClassifier(estimators=[(\"rf\", best_rf), (\"knn\", best_knn), (\"nb\", nb_clf), (\"lr\", best_lr)],\n                                        voting=\"soft\", weights=[weight_rf, weight_knn, weight_nb, weight_lr])","7c0ce725":"ensemble.fit(X_train_scaled, y_train)","dc396cd0":"ens_prediction = ensemble.predict(X_test_scaled)\nf1_score = metrics.f1_score(y_test, ens_prediction)\nprint('Validation f1-score of Ensemble Classifier is', f1_score)\nprint (\"\\nClassification report :\\n\", metrics.classification_report(y_test, ens_prediction))\n\n# Confusion matrix\nplt.figure(figsize=(18, 14))\nplt.subplot(221)\nsns.heatmap(metrics.confusion_matrix(y_test, ens_prediction), annot=True, fmt = \"d\", linecolor=\"k\", linewidths=3)\nplt.title(\"CONFUSION MATRIX\", fontsize=20)\n\n# ROC curve\nens_predicted_probs = ensemble.predict_proba(X_test_scaled)[:, 1]\nfpr, tpr, thresholds = metrics.roc_curve(y_test, ens_predicted_probs)\nplt.subplot(222)\nplt.plot(fpr, tpr, label = (\"Area_under the curve :\", metrics.auc(fpr, tpr)), color = \"r\")\nplt.plot([1,0], [1,0], linestyle = \"dashed\", color =\"k\")\nplt.legend(loc = \"best\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)\nplt.show()","52c9c549":"## Classification","1a1e6565":"## EDA and data preprocessing","8d75e92b":"#### KNN","7acc324b":"The dataset showes how new cases of cancer could be classified by gene expression monitoring (via DNA microarray) and thereby provided a general approach for identifying new cancer classes and assigning tumors to known classes. These data were used to classify patients with acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).","ac63e27c":"It seems like **call** columns have \"A\" almost everywhere, so I will drop it.","6e2eb8c7":"In test data we have $\\frac{ALL}{AML}$ ratio about $\\frac{20}{14}=1.43$ and in train $\\frac{27}{11}=2.45$. Lets use upsampling to combat class imbalance. I think here we can add about 8 additional random samples of **AML** class.","96ee3903":"I will start with **random forest**, esimate feature importance and then try to build ensemble using different classifiers.","ea4ec462":"As we can see there are also lots of features with pretty low importance (about e-5), but also we can admit 4 genes, wich have a big contribution in cancer classification. So genes **X95735**, **M55150**, and **M27891** contribute the most to the differences between **ALL** and **AML** cancer types in accordance with random forest classifier.","0d4de083":"Results:\n\n1. RandomForest have shown a nice results: f1-score was 0.896 and other metrics were good too\n2. Also we got the most important genes for cancer type distinguishing: X95735, M55150, X70297 and M27891\n3. Then we built Ensemble classifier and it have performed pretty cool results (f1-score was 1)","bb5ba456":"#### XGBOOST","a19f1214":"## Conclusion","8f144cdb":"It is a good model, but I am going to build ensemle based on different classifiers and hope it can show better performance.","49e75104":"#### Also it is interesting to see if hierarchial clusterisation give us 2 clusters based on cancer type.","14bc3fd5":"Here we have features in rows and patients in cols, so we need to transpose data.","4a44c6b8":"#### Naive Bayes","013b6027":"## Dimentionality reduction and clusterisation ","7651559c":"Here we can take approximately 27 PC for downstream analysis. May be we will use them later.","d199c558":"We can not to take right part of this plot into account (because it just a upsampled data). Dendrogram shows us that we can't extract two clusters based on cancer type, but the first large cluster contain almost only **ALL** and the second one contains both cancer types. Distance plot tells that it is a good way to distinguish 6 clusters. But it is just a preliminary analysis. Our main goal is build a classifier.","d8e14a46":"Seems like we can't define two clear clusters (at least by KMeans and using 2 PC for visualisation). It is not a problem now. Now I want to calculate PC so that they will explain at least 90% of variance and may be I can use them in downstream models.","973d3e1f":"#### Builing an ensemble","6b60a15c":"Scaling the data.","1c8b9b0d":"# Classification of two cancer types based on gene expression","82cfe075":"We have a lot of unimportant genes, so we can look only at non null features (with 0 importance).","7e4643b1":"#### Hierarchy","e76eb1e3":"It takes very long time on my machine, so I decided to interupt this.","01f09e77":"#### Logistic Regression","53d32ef2":"I want to build 5 different classifiers, evaluate some metrics (f1 score) and then try to create ensemble.","7923502d":"As we can see Ensemble have shown the best performance. All metrics are cool. Thanks for your attention.","7ea2065a":"### Load data","8233a17c":"### Random forest","9e015c92":"## Ensemble","bafcd6bf":"### Check NAs","a46ce971":"So now we have 4 different classifiers, some of them are good and the others are not and I want to use all of them to classify two cancer types. I will weigh my classifiers based on f1-score such that I will calculate mean f1-score and then divide f1-score of each model by mean, so the best estimator will have the higher weight."}}