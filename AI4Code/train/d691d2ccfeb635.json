{"cell_type":{"7d7abc26":"code","89b01f1b":"code","d65c01af":"code","5930d1b0":"code","283349b4":"code","063cb678":"code","fca36da0":"code","205192b4":"code","1c178860":"code","8101155b":"code","2c1c3e95":"code","51997d05":"code","60bcc9d0":"code","70db6b56":"code","8c93308b":"code","24a339f0":"code","e462bd49":"code","c906edb4":"code","f4c60bfa":"code","d885dbfa":"code","882981f3":"code","18352c6b":"code","49afbf85":"code","a77bc0c4":"code","19f69f60":"code","81f44835":"code","296214cf":"code","cc2b594b":"code","a711d72d":"code","7ba33b9d":"code","40aec1e1":"code","6ac951e1":"code","c65f33fd":"code","98175bab":"code","3f65cc50":"code","e266ff4c":"code","eda46592":"code","5fe39d75":"code","3fb2a6cf":"code","80f072f5":"code","f7a4c677":"code","a09e0066":"code","3aabcf98":"code","bd510a7b":"code","e83d9ad7":"code","3923206a":"code","80be3733":"code","27177c82":"markdown","e42cab37":"markdown","37b3819d":"markdown","804a5184":"markdown","35723f89":"markdown","6829f985":"markdown","085eb169":"markdown","21dafc59":"markdown","4477b07f":"markdown","2615592a":"markdown","34cc5005":"markdown","abf36c5d":"markdown","1387b934":"markdown","840ca440":"markdown","6568e795":"markdown","61381c78":"markdown","856417f7":"markdown","aa2b7026":"markdown","8aa1cfae":"markdown","425fe1d8":"markdown","a638ef4b":"markdown","1b4a06f5":"markdown","7fec0d20":"markdown","c64f2aa3":"markdown","1a772374":"markdown","e39e34b8":"markdown"},"source":{"7d7abc26":"#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","89b01f1b":"traindata =  pd.read_csv('C:\/Users\/3arrows\/Desktop\/ML Projects\/Learn Machine Learning ON Board Titanic - 17 Algorithms\/titanic\/train.csv')","d65c01af":"testdata =  pd.read_csv('C:\/Users\/3arrows\/Desktop\/ML Projects\/Learn Machine Learning ON Board Titanic - 17 Algorithms\/titanic\/test.csv')","5930d1b0":"traindata.head()","283349b4":"testdata.head()","063cb678":"print(traindata.shape)\nprint(testdata.shape)","fca36da0":"traindata.info()","205192b4":"testdata.info()","1c178860":"traindata.describe()","8101155b":"traindatacopy = traindata.copy()\ntestdatacopy = testdata.copy()","2c1c3e95":"print('Train columns with null values: {} \\n' .format(traindatacopy.isnull().sum()))\n\nprint('Test columns with null values: {}'.format(testdatacopy.isnull().sum()))","51997d05":"traindatacopy['Age'].fillna(traindatacopy['Age'].median(), inplace = True)\n\ntestdatacopy['Age'].fillna(testdatacopy['Age'].median(), inplace = True)\n\ndrop_column = ['Cabin']\n#drop_column = ['PassengerId', 'Ticket']\ntraindatacopy.drop(drop_column, axis=1, inplace = True)\ntestdatacopy.drop(drop_column, axis=1, inplace = True)","60bcc9d0":"print('Train columns with null values: {} \\n' .format(traindatacopy.isnull().sum()))\n\nprint('Test columns with null values: {}'.format(testdatacopy.isnull().sum()))","70db6b56":"traindatacopy.head()","8c93308b":"testdatacopy.head()","24a339f0":"alltables = [traindatacopy, testdatacopy]\n\nfor dataset in alltables:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes\/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\n","e462bd49":"traindatacopy.head()","c906edb4":"testdatacopy.head()","f4c60bfa":"sns.countplot(x=\"Survived\", data=traindatacopy) ","d885dbfa":"fig, saxis = plt.subplots(2, 2,figsize=(16,12))\n\nsns.countplot(x='Survived', hue=\"Embarked\", data=traindatacopy,ax = saxis[0,0])   \nsns.countplot(x='Survived', hue=\"IsAlone\", data=traindatacopy,ax = saxis[0,1])\nsns.countplot(x=\"Survived\", hue=\"Pclass\", data=traindatacopy, ax = saxis[1,0])\nsns.countplot(x=\"Survived\", hue=\"Sex\", data=traindatacopy, ax = saxis[1,1])\n","882981f3":"f,ax=plt.subplots(1,2,figsize=(16,7))\ntraindatacopy['Survived'][traindatacopy['Sex']=='male'].value_counts().plot.pie(explode=[0,0.2],autopct='%1.1f%%',ax=ax[0],shadow=True)\ntraindatacopy['Survived'][traindatacopy['Sex']=='female'].value_counts().plot.pie(explode=[0,0.2],autopct='%1.1f%%',ax=ax[1],shadow=True)\nax[0].set_title('Survived (male)')\nax[1].set_title('Survived (female)')","18352c6b":"a = sns.FacetGrid(traindatacopy, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , traindatacopy['Age'].max()))\na.add_legend()","49afbf85":"plt.subplots(figsize =(14, 12))\ncorrelation = traindatacopy.corr()\nsns.heatmap(correlation, annot=True,cmap='coolwarm')","a77bc0c4":"\n#define y variable aka target\/outcome\nTarget = ['Survived']\n\n#define x variables for original features aka feature selection\ndatatrain_x = ['Sex','Pclass', 'Embarked', 'SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name\/values for charts\ndatatrain_x_calc = ['Sex_Code','Pclass', 'SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\ndatatrain_xy =  Target + datatrain_x\nprint('Original X Y: ', datatrain_xy, '\\n')\n\n\n#define x variables for original w\/bin features to remove continuous variables\ndatatrain_x_bin = ['Pclass',  'FamilySize' ]\ndatatrain_xy_bin = Target + datatrain_x_bin\nprint('Bin X Y: ', datatrain_xy_bin, '\\n')\n\n\n#define x and y variables for dummy features original\ndatatrain_dummy = pd.get_dummies(traindatacopy[datatrain_x])\ndatatrain_x_dummy = datatrain_dummy.columns.tolist()\ndatatrain_xy_dummy = Target + datatrain_x_dummy\nprint('Dummy X Y: ', datatrain_xy_dummy, '\\n')\n\ndatatrain_dummy.head()","19f69f60":"#split train and test data with function defaults\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\n\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = train_test_split(traindatacopy[datatrain_x_calc], traindatacopy[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(traindatacopy[datatrain_x_bin], traindatacopy[Target] , random_state = 0)\n\nprint(\"DataTrain Shape: {}\".format(traindatacopy.shape))\nprint(\"Train1 Shape: {}\".format(train1_x_dummy.shape))\nprint(\"Test1 Shape: {}\".format(test1_x_dummy.shape))\n","81f44835":"train1_x_dummy.head()","296214cf":"from sklearn.neighbors import KNeighborsClassifier\n\nModel = KNeighborsClassifier(n_neighbors=5).fit(train1_x_dummy, train1_y_dummy)\n\ny_predKN = Model.predict(test1_x_dummy)\n\nprint('accuracy is',accuracy_score(y_predKN,test1_y_dummy))\n\nKNN = accuracy_score(y_predKN,test1_y_dummy)","cc2b594b":"from sklearn.preprocessing import MinMaxScaler\n\nscalar =  MinMaxScaler()\n\nx_scaled = scalar.fit_transform(train1_x_dummy)\ny_scaled = scalar.fit_transform(train1_y_dummy)\n\nModel1 = KNeighborsClassifier(n_neighbors=5).fit(x_scaled, train1_y_dummy)\n\npredKN = Model.predict(test1_x_dummy)\n\nprint('accuracy is',accuracy_score(predKN,test1_y_dummy))\n\n","a711d72d":"from sklearn.neighbors import  RadiusNeighborsClassifier\nModel=RadiusNeighborsClassifier(radius=148).fit(train1_x_dummy, train1_y_dummy)\ny_pred=Model.predict(test1_x_dummy)\n\nprint('accuracy is ', accuracy_score(test1_y_dummy,y_pred))\n\nRNC = accuracy_score(test1_y_dummy,y_pred)","7ba33b9d":"from sklearn.naive_bayes import GaussianNB\n\nModel = GaussianNB().fit(train1_x_dummy, train1_y_dummy)\n\ny_predN = Model.predict(test1_x_dummy)\n\nprint('accuracy is',accuracy_score(y_predN,test1_y_dummy))\n\nNBB = accuracy_score(y_predN,test1_y_dummy)","40aec1e1":"from sklearn.naive_bayes import BernoulliNB\nModel = BernoulliNB().fit(train1_x_dummy, train1_y_dummy)\n\ny_pred = Model.predict(test1_x_dummy)\n\nprint('accuracy is',accuracy_score(y_pred,test1_y_dummy))\n\nBer = accuracy_score(y_pred,test1_y_dummy)","6ac951e1":"from sklearn.svm import SVC\n\nModel = SVC().fit(train1_x_dummy, train1_y_dummy)\n\ny_predSVM = Model.predict(test1_x_dummy)\n\nprint('accuracy is',accuracy_score(y_predSVM,test1_y_dummy))\n\nSVMm = accuracy_score(y_predSVM,test1_y_dummy)","c65f33fd":"from sklearn.svm import SVC\n\nModel = SVC(kernel='rbf' , gamma=1).fit(train1_x_dummy, train1_y_dummy)\n\ny_predSVM = Model.predict(test1_x_dummy)\n\nprint('accuracy is',accuracy_score(y_predSVM,test1_y_dummy))\n\nSVMrbf = accuracy_score(y_predSVM,test1_y_dummy)","98175bab":"from sklearn.svm import LinearSVC\n\nModel = LinearSVC().fit(train1_x_dummy, train1_y_dummy)\n\ny_pred = Model.predict(test1_x_dummy)\n\nprint('accuracy is',accuracy_score(y_pred,test1_y_dummy))\n\nLSVM = accuracy_score(y_pred,test1_y_dummy)","3f65cc50":"from sklearn.svm import NuSVC\n\nModelNU = NuSVC().fit(train1_x_dummy, train1_y_dummy)\n\ny_predNu = Model.predict(test1_x_dummy)\n\nprint('accuracy is',accuracy_score(y_predNu,test1_y_dummy))\n\nNuS = accuracy_score(y_predNu,test1_y_dummy)","e266ff4c":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n\nModel = DecisionTreeClassifier().fit(train1_x_dummy, train1_y_dummy)\n\ny_predL = Model.predict(test1_x_dummy)\n\nprint('accuracy is',accuracy_score(y_predL,test1_y_dummy))\n\nDT = accuracy_score(y_predL,test1_y_dummy)","eda46592":"\nfrom sklearn.linear_model import LogisticRegression\n\nModel = LogisticRegression().fit(train1_x_dummy, train1_y_dummy)\n\ny_predLR = Model.predict(test1_x_dummy)\n\nprint('accuracy is',accuracy_score(y_predLR,test1_y_dummy))\n\nLR = accuracy_score(y_predLR,test1_y_dummy)","5fe39d75":"from sklearn.ensemble import RandomForestClassifier\nModel=RandomForestClassifier(max_depth=2).fit(train1_x_dummy, train1_y_dummy)\ny_predR=Model.predict(test1_x_dummy)\n\nprint('accuracy is ',accuracy_score(y_predR,test1_y_dummy))\n\nRT = accuracy_score(y_predR,test1_y_dummy)\n","3fb2a6cf":"from sklearn.tree import ExtraTreeClassifier\n\nModel = ExtraTreeClassifier().fit(train1_x_dummy, train1_y_dummy)\n\ny_pred = Model.predict(test1_x_dummy)\n\nprint('accuracy is',accuracy_score(y_pred,test1_y_dummy))\n\nETC = accuracy_score(y_pred,test1_y_dummy)","80f072f5":"from sklearn.ensemble import BaggingClassifier\n\nModel=BaggingClassifier().fit(train1_x_dummy, train1_y_dummy)\n\ny_pred=Model.predict(test1_x_dummy)\n\nprint('accuracy is ',accuracy_score(y_pred,test1_y_dummy))\n\nBCC = accuracy_score(y_pred,test1_y_dummy)","f7a4c677":"from sklearn.ensemble import AdaBoostClassifier\n\nModel=AdaBoostClassifier().fit(train1_x_dummy, train1_y_dummy)\n\ny_pred=Model.predict(test1_x_dummy)\n\nprint('accuracy is ',accuracy_score(y_pred,test1_y_dummy))\n\nAdaB = accuracy_score(y_pred,test1_y_dummy)","a09e0066":"from sklearn.ensemble import GradientBoostingClassifier\n\nModel=GradientBoostingClassifier().fit(train1_x_dummy, train1_y_dummy)\n\ny_predGR=Model.predict(test1_x_dummy)\n\n\nprint('accuracy is ',accuracy_score(y_predGR,test1_y_dummy))\n\nGBCC = accuracy_score(y_predGR,test1_y_dummy)","3aabcf98":"models = pd.DataFrame({\n    'Model': ['K-Nearest Neighbours','Radius Neighbors Classifier', 'Naive Bayes', 'BernoulliNB', 'Support Vector Machines',\n              'Linear Support Vector Classification', 'Nu-Support Vector Classification', 'Decision Tree', 'LogisticRegression',\n              'Random Forest', 'ExtraTreeClassifier', \"Bagging classifier \", \"AdaBoost classifier\", 'Gradient Boosting Classifier'],\n    'Score': [ KNN, RNC, NBB, Ber, SVMm, LSVM , NuS, DT, LR, RT,ETC, BCC, AdaB,  GBCC]})\nmodels.sort_values(by='Score', ascending=False)","bd510a7b":"plt.subplots(figsize =(14, 12))\n\nsns.barplot(x='Score', y = 'Model', data = models, palette=\"Set3\")\n\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","e83d9ad7":"submit_gbc = GradientBoostingClassifier().fit(tatrain_x_bin, Target)\n\nrr = submit_gbc.predict(datatrain_x_bin)","3923206a":"rr","80be3733":"submission = pd.DataFrame({\n        \"PassengerId\": testdata[\"PassengerId\"],\n        \"Survived\": rr\n    })\nsubmission.to_csv('titanic_submission.csv', index=False)\n\nsubmission.head(10)","27177c82":"<a id=\"28\"><\/a> <br>\n## References\n\nhttps:\/\/www.kaggle.com\/marcovasquez\/machine-learning-on-board-titanic-17-algothim\n\n\n","e42cab37":"<a id=\"7\"><\/a> <br>\n## Prepare Train and Test\n\nscikit-learn provides a helpful function for partitioning data, train_test_split, which splits out your data into a training set and a test set.\n\n- Training set for fitting the model\n- Test set for evaluation only","37b3819d":"<a id=\"8\"><\/a> <br>\n# Machine Learning Algorithms\n<img src=\"https:\/\/i.vas3k.ru\/7vx.jpg\">\nImage credit:vas3k.com \n\n","804a5184":"<a id=\"5\"><\/a> <br>\n## Load Data","35723f89":"## The Notebooks explores the basic use of Pandas and scikit-learn for Titanic: Machine Learning from Disaster using 14 ML Algorithms\n\nAhmed M.Kahlifa\n","6829f985":"<a id=\"16\"><\/a> <br>\n#### 7. Nu-Support Vector Classification\n\nSimilar to SVC but uses a parameter to control the number of support vectors.\n\nThe implementation is based on libsvm.","085eb169":" How many people survived from all passengers in traindatacopy?","21dafc59":"<a id=\"22\"><\/a> <br>\n#### 12. Bagging classifier \n\nBagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.","4477b07f":"<a id=\"23\"><\/a> <br>\n####  13. AdaBoost classifier\n\nAn AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.","2615592a":"<a id=\"24\"><\/a> <br>\n#### 14. Gradient Boosting Classifier\n\nGBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. Boosting is actually an ensemble of learning algorithms which combines the prediction of several base estimators in order to improve robustness over a single estimator. It combines multiple weak or average predictors to a build strong predictor.","34cc5005":"<a id=\"10\"><\/a> <br>\n#### 8. Decision Tree\n\nIt is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. In this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes\/ independent variables to make as distinct groups as possible. \n","abf36c5d":"<a id=\"12\"><\/a> <br>\n#### 9. Logistic Regression\n\nDon\u2019t get confused by its name! It is a classification not a regression algorithm. It is used to estimate discrete values ( Binary values like 0\/1, yes\/no, true\/false ) based on given set of independent variable(s). In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function. Hence, it is also known as logit regression. Since, it predicts the probability, its output values lies between 0 and 1 (as expected).","1387b934":"<a id=\"11\"><\/a> <br>\n#### 10. RandomForest\n\nRandom Forest is a trademark term for an ensemble of decision trees. In Random Forest, we\u2019ve collection of decision trees (so known as \u201cForest\u201d). To classify a new object based on attributes, each tree gives a classification and we say the tree \u201cvotes\u201d for that class. The forest chooses the classification having the most votes (over all the trees in the forest).","840ca440":"<a id=\"17\"><\/a> <br>\n#### 6. Linear Support Vector Classification [LSVM]\n\nSimilar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n\nThis class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.","6568e795":"<a id=\"3\"><\/a> <br>\n##  Import Libraries","61381c78":"## Submit","856417f7":"<a id=\"15\"><\/a> <br>\n#### 5. Support Vector Machines\n\nIt is a classification method. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate.\n\nFor example, if we only had two features like Height and Hair length of an individual, we\u2019d first plot these two variables in two dimensional space where each point has two co-ordinates (these co-ordinates are known as Support Vectors)","aa2b7026":"<a id=\"13\"><\/a> <br>\n#### 1. K-Nearest Neighbors\n\nIt can be used for both classification and regression problems. However, it is more widely used in classification problems in the industry. K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function.","8aa1cfae":"<a id=\"30\"><\/a> <br>\n###  Scores","425fe1d8":"<a id=\"29\"><\/a> <br>\n##  Data Cleaning \n\nThe process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting. \n\nFis","a638ef4b":"<a id=\"21\"><\/a> <br>\n#### 11.  ExtraTreeClassifier\n\nExtraTreesClassifier is an ensemble learning method fundamentally based on decision trees. ExtraTreesClassifier, like RandomForest, randomizes certain decisions and subsets of data to minimize over-learning from the data and overfitting.\nLet\u2019s look at some ensemble methods ordered from high to low variance, ending in ExtraTreesClassifier.","1b4a06f5":"<a id=\"18\"><\/a> <br>\n#### 2. Radius Neighbors Classifier\n\nIn scikit-learn RadiusNeighborsClassifier is very similar to KNeighborsClassifier with the exception of two parameters. First, in RadiusNeighborsClassifier we need to specify the radius of the fixed area used to determine if an observation is a neighbor using radius. Unless there is some substantive reason for setting radius to some value, it is best to treat it like any other hyperparameter and tune it during model selection. The second useful parameter is outlier_label, which indicates what label to give an observation that has no observations within the radius - which itself can often be a useful tool for identifying outliers.","7fec0d20":"<a id=\"14\"><\/a> <br>\n#### 3. Naive Bayes\n\nIt is a classification technique based on Bayes\u2019 theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.","c64f2aa3":"<a id=\"20\"><\/a> <br>\n#### 4. BernoulliNB\n\nLike MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary\/boolean features.","1a772374":"## Exploratory Data Analysis","e39e34b8":"<a id=\"6\"><\/a> <br>\n## Visualization\n"}}