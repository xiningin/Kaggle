{"cell_type":{"6dd2d571":"code","0b100242":"code","66812e53":"code","188bde96":"code","f4290d44":"code","5c1f41bf":"code","007722a7":"code","ae5222db":"code","9982db73":"code","cb078f0d":"code","e3da921e":"code","440b53ee":"code","9de51de2":"code","8c962798":"code","faa77471":"code","459ed27e":"code","9770e936":"code","d1964416":"code","6061b5a9":"code","8fa56a98":"code","0e036960":"code","4b180361":"code","015e959d":"code","f2955ea1":"code","f71f765e":"code","063dc291":"code","908482c0":"code","3346f576":"code","a01faa75":"code","a0f77552":"code","c08ff343":"code","2a34c037":"code","ba433b9f":"code","85b7ea66":"code","f0de4f98":"code","f8ff50ba":"code","acecd8cd":"code","51f7046a":"code","f1e84402":"code","f7534b9e":"code","0ef2c313":"code","4573e9df":"code","0f3493eb":"code","8c6ed8ab":"code","c4855603":"code","4048f3c1":"code","b334b3b5":"code","2b2eac30":"code","ef3caa74":"code","662b4daa":"code","753f509f":"code","d250f8ce":"code","c5d07bf1":"code","978a856d":"code","eaa6dc2b":"code","15f5120d":"code","c3c871f3":"markdown","34085eff":"markdown","fe4a953c":"markdown","702d98f2":"markdown","d78e8943":"markdown","b6061bda":"markdown","192f24d4":"markdown","d45dea76":"markdown","a25bbc5e":"markdown","3ecde841":"markdown","88beea09":"markdown","2a943442":"markdown","19b7e41b":"markdown","fee2dfe5":"markdown","b9ee2fab":"markdown","f9ba22be":"markdown","857fe9f2":"markdown","ca5c4715":"markdown","340387d2":"markdown","90ba2666":"markdown","f1e624e9":"markdown","da3573ac":"markdown","5cd88f31":"markdown","4327344d":"markdown","62e4aaaf":"markdown","a424eb82":"markdown","e2a727b7":"markdown","8401e1f8":"markdown","753d7503":"markdown","303ff8d7":"markdown","84cbb1f6":"markdown","3aa9fd9a":"markdown","6411e063":"markdown","396a18fe":"markdown","e08144d8":"markdown","76e81e7f":"markdown","b130e784":"markdown","7a4e0ccc":"markdown","06aae641":"markdown","84eefe68":"markdown","34a078ee":"markdown","bf63a2d1":"markdown","2212c23b":"markdown","f227f324":"markdown","cf1b6abe":"markdown","c6170ffb":"markdown","e8a314c1":"markdown","7cca7ca2":"markdown","82b8be84":"markdown","e84b358e":"markdown"},"source":{"6dd2d571":"import os\nimport numpy as np\nimport pandas as pd\nimport json\nfrom pandas.io.json import json_normalize\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\n\n\nfrom IPython.display import display, HTML","0b100242":"# Table printing large\nplt.rcParams['figure.figsize'] = (15, 7)\npd.set_option(\"display.max_columns\", 400)\npd.options.display.max_colwidth = 250\npd.set_option(\"display.max_rows\", 100)\n# High defition plots\n%config InlineBackend.figure_format = 'retina'\nsns.set()","66812e53":"base_path_data = \"..\/input\/\"\nprint(os.listdir(\"..\/input\"))","188bde96":"df_train = pd.read_csv(\"..\/input\/train\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test\/test.csv\")\n\nprint(f\"train.csv shape is {df_train.shape}\")\nprint(f\"test.csv shape is {df_test.shape}\")","f4290d44":"print(\"Basic statistics of the train set\")\ndisplay(df_train.describe(include=\"all\").T)\n\nprint(\"Basic statistics of the test set\")\ndisplay(df_test.describe(include=\"all\").T)","5c1f41bf":"df_train.sample(3)","007722a7":"df_breed_labels = pd.read_csv(os.path.join(base_path_data, \"breed_labels.csv\"))\ndf_color_labels = pd.read_csv(os.path.join(base_path_data, \"color_labels.csv\"))\ndf_state_labels = pd.read_csv(os.path.join(base_path_data, \"state_labels.csv\"))","ae5222db":"print(f\"breed_labels.csv shape is {df_breed_labels.shape}\")\ndf_breed_labels.sample(5)","9982db73":"print(f\"color_labels.csv shape is {df_color_labels.shape}\")\ndf_color_labels.sample(5)","cb078f0d":"print(f\"state_labels.csv shape is {df_state_labels.shape}\")\ndf_state_labels.sample(5)","e3da921e":"# sentiment\nwith open(os.path.join(base_path_data, \"train_sentiment\", \"048cd8bc0.json\")) as f:\n    data = json.load(f)\n\npprint(data)","440b53ee":"# metadata\nwith open(os.path.join(base_path_data, \"train_metadata\", \"000fb9572-6.json\")) as f:\n    data = json.load(f)\n\npprint(data)","9de51de2":"set(df_train.RescuerID.unique()).intersection(set(df_test.RescuerID.unique()))","8c962798":"common_names = list(set(df_train.Name.unique()).intersection(set(df_test.Name.unique())))\nlen(common_names)","faa77471":"common_names[:10]","459ed27e":"fig, axes = plt.subplots(8,3, figsize=(15, 20))\nimages_train = os.listdir(\"..\/input\/train_images\/\")\nfig.suptitle(\"24 random pet images\")\nimages_train = np.random.choice(images_train, 24)\nfor i, img in enumerate(images_train):\n    image = Image.open(\"..\/input\/train_images\/\" + img)\n    pet_id = img.split(\"-\")[0]\n    axes[i\/\/3, i%3].imshow(image)\n    axes[i\/\/3, i%3].grid(False)\n    axes[i\/\/3, i%3].set_axis_off()\n    axes[i\/\/3, i%3].set_title(\"Name: {}\\nAdoptionSpeed: {}\".format(*list(map(str, df_train[df_train.PetID==pet_id][[\"Name\", \"AdoptionSpeed\"]].values.tolist()[0]))))","9770e936":"100 * df_train.isnull().sum() \/ len(df_train)","d1964416":"100 * df_test.isnull().sum() \/ len(df_test)","6061b5a9":"df_train[df_train.duplicated(keep=False)].shape","8fa56a98":"cols = [col for col in df_train.columns.ravel() if col not in [\"PetID\", \"Name\", \"Description\", \"AdoptionSpeed\"]]","0e036960":"dups = df_train[df_train[cols].duplicated(keep=False)].sort_values(by=cols)\nprint(f\"Shape of matrix with duplicated rows not considering petid, name nor description {dups.shape}\")\ndups.head(2)","4b180361":"dups = df_test[df_test[cols].duplicated(keep=False)].sort_values(by=cols)\nprint(f\"Shape of matrix with duplicated rows not considering petid, name nor description {dups.shape}\")\ndups.head(2)","015e959d":"def plot_correlation_matrix(df):\n    corr = df.corr()\n\n    # Generate a mask for the upper triangle\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(20, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","f2955ea1":"plot_correlation_matrix(df_train)","f71f765e":"plot_correlation_matrix(df_test)","063dc291":"plot_correlation_matrix(df_train[df_train.Type==1].drop(columns=\"Type\"))","908482c0":"plot_correlation_matrix(df_test[df_test.Type==1].drop(columns=\"Type\"))","3346f576":"plot_correlation_matrix(df_test[df_test.Type==2].drop(columns=\"Type\"))","a01faa75":"plot_correlation_matrix(df_test[df_test.Type==2].drop(columns=\"Type\"))","a0f77552":"feats = ['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1',\n       'Color2', 'Color3', 'MaturitySize', 'FurLength', 'Vaccinated',\n       'Dewormed', 'Sterilized', 'Health', 'Quantity', 'Fee', 'State',\n       'VideoAmt', 'PhotoAmt', 'AdoptionSpeed']","c08ff343":"def plot_distribution(df, feat):\n    fig, ax = plt.subplots(figsize=(17,5))\n    sns.countplot(df_train[feat])\n    ax.xaxis.set_label_text(feat,fontdict= {'size':14})\n    ax.yaxis.set_label_text(\"Count\",fontdict= {'size':14})\n    plt.show()\n    print(f\"Total number of unique values for feature {feat} is {df[feat].nunique()}\")\n    print(100 * df_train[feat].value_counts(normalize=True, dropna=False))","2a34c037":"plot_distribution(df_train, \"AdoptionSpeed\")","ba433b9f":"for feat in feats:\n    print(f\"Univariate distribution of feature {feat}\")\n    plot_distribution(df_train, feat)","85b7ea66":"for cat_c in feats:\n    if cat_c == \"AdoptionSpeed\": continue\n    nunique = df_train[cat_c].nunique()\n    print(f'{cat_c}:')\n    print(f'{nunique} unique values')\n    if nunique < 50:\n        print(f'\\nValues:\\n{100 * df_train[cat_c].value_counts(normalize=True, dropna=False)}')\n      \n        # Countplot\n        fig, ax = plt.subplots(figsize=(12,4))\n        sns.countplot(x=cat_c, hue=\"AdoptionSpeed\", data=df_train, orient=\"h\")\n        #ax.text(5,5,\"Boxplot After removing outliers\", fontsize=18, color=\"r\", ha=\"center\", va=\"center\")\n        ax.xaxis.set_label_text(cat_c,fontdict= {'size':14})\n        ax.yaxis.set_label_text(\"Count\",fontdict= {'size':14})\n        plt.xticks(rotation=90)\n        plt.show()\n    else:\n        # Distplot to see the distribution after outliers have been removed\n        sns.set_style(\"whitegrid\")\n        fig, ax = plt.subplots(figsize=(12,4))\n        for aspeed in range(5):\n            sns.distplot(df_train[df_train.AdoptionSpeed == aspeed][cat_c].dropna(), hist=False, rug=False, label=\"AdoptionSpeed = {}\".format(aspeed))\n        ax.xaxis.set_label_text(cat_c,fontdict= {'size':14})\n        plt.xticks(rotation=90)\n        plt.legend()\n        plt.show()","f0de4f98":"c = df_train.corr().abs()\ns = c.unstack()\nso = s.sort_values(kind=\"quicksort\", ascending=False)\nso = pd.DataFrame(so[20:]).reset_index()\nso.columns = [\"var1\", \"var2\", \"corr\"]\nso = so[so[\"corr\"] > 0.3]\nso","f8ff50ba":"\nfor k, v in so.iterrows():\n    print(f'{v[\"var1\"]} vs {v[\"var2\"]} ({v[\"corr\"]})')\n    if (df_train[v[\"var1\"]].nunique() < 50) and (df_train[v[\"var2\"]].nunique() < 50):\n        fig, ax = plt.subplots(figsize=(12,4))\n        sns.countplot(x=v[\"var1\"], hue=v[\"var2\"], data=df_train, orient=\"h\")\n        #ax.text(5,5,\"Boxplot After removing outliers\", fontsize=18, color=\"r\", ha=\"center\", va=\"center\")\n        ax.xaxis.set_label_text(v[\"var1\"], fontdict= {'size':14})\n        ax.yaxis.set_label_text(\"Count\",fontdict= {'size':14})\n        plt.xticks(rotation=90)\n        plt.show()\n    else:\n        # Distplot to see the distribution after outliers have been removed\n        sns.set_style(\"whitegrid\")\n        fig, ax = plt.subplots(figsize=(12,4))\n        sns.scatterplot(x=v[\"var1\"], y=v[\"var2\"], data=df_train, hue=\"AdoptionSpeed\")\n        ax.xaxis.set_label_text(v[\"var1\"], fontdict= {'size':14})\n        #plt.xticks(rotation=90)\n        plt.show()","acecd8cd":"import numpy as np\nimport pandas as pd\nimport os\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nimport scipy as sp\nfrom sklearn import linear_model\nfrom functools import partial\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\nfrom collections import Counter\nimport json\nimport lightgbm as lgb","51f7046a":"# The following 3 functions have been taken from Ben Hamner's github repository\n# https:\/\/github.com\/benhamner\/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              \/ num_scored_items)\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)","f1e84402":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']","f7534b9e":"def rmse(actual, predicted):\n    return mean_squared_error(actual, predicted)**0.5","0ef2c313":"train_desc = df_train.Description.fillna(\"none\").values\ntest_desc = df_test.Description.fillna(\"none\").values\n\ntfv = TfidfVectorizer(min_df=3,  max_features=None,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words = 'english')\n    \n# Fit TFIDF\ntfv.fit(list(train_desc) + list(test_desc))\nX =  tfv.transform(train_desc)\nX_test = tfv.transform(test_desc)\n\n\nsvd = TruncatedSVD(n_components=180)\nsvd.fit(X)\nX = svd.transform(X)\n\nX_test = svd.transform(X_test)","4573e9df":"train_desc = df_train.Description.fillna(\"none\").values\ntest_desc = df_test.Description.fillna(\"none\").values\n\nsvd_n_components = 200\n\ntfv = TfidfVectorizer(min_df=2,  max_features=None,\n        strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        )\n    \n# Fit TFIDF\ntfv.fit(list(train_desc))\nX =  tfv.transform(train_desc)\nX_test = tfv.transform(test_desc)\n\nsvd = TruncatedSVD(n_components=svd_n_components)\nsvd.fit(X)\nprint(svd.explained_variance_ratio_.sum())\nprint(svd.explained_variance_ratio_)\nX = svd.transform(X)\nX = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(svd_n_components)])\ndf_train = pd.concat((df_train, X), axis=1)\nX_test = svd.transform(X_test)\nX_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(svd_n_components)])\ndf_test = pd.concat((df_test, X_test), axis=1)","0f3493eb":"# Thanks to beloruk1\n\ndef readFile(fn):\n    file = '..\/input\/train_sentiment\/'+fn['PetID']+'.json'\n    if os.path.exists(file):\n        with open(file) as data_file:    \n            data = json.load(data_file)  \n\n        df = json_normalize(data)\n        mag = df['documentSentiment.magnitude'].values[0]\n        score = df['documentSentiment.score'].values[0]\n        return pd.Series([mag,score],index=['mag','score']) \n    else:\n        return pd.Series([0,0],index=['mag','score'])\n    \ndef readTestFile(fn):\n    file = '..\/input\/test_sentiment\/' + fn['PetID'] + '.json'\n    if os.path.exists(file):\n        with open(file) as data_file:    \n            data = json.load(data_file)  \n\n        df = json_normalize(data)\n        mag = df['documentSentiment.magnitude'].values[0]\n        score = df['documentSentiment.score'].values[0]\n        return pd.Series([mag,score],index=['mag','score']) \n    else:\n        print(f'{file} does not exist')\n        return pd.Series([0,0],index=['mag','score'])\n    \ndf_train[['SentMagnitude', 'SentScore']] = df_train[['PetID']].apply(lambda x: readFile(x), axis=1)\ndf_test[['SentMagnitude', 'SentScore']] = df_test[['PetID']].apply(lambda x: readTestFile(x), axis=1)","8c6ed8ab":"# Not needed, as there's no overlap between RescuerID in train set and test set\n#lbl_enc = LabelEncoder()\n#lbl_enc.fit(df_train.RescuerID.values.tolist() + df_test.RescuerID.values.tolist())\n#df_train.RescuerID = lbl_enc.transform(df_train.RescuerID.values)\n#df_test.RescuerID = lbl_enc.transform(df_test.RescuerID.values)","c4855603":"y = df_train.AdoptionSpeed\ntrain = np.hstack((df_train.drop(['Name', 'Description', 'PetID', 'AdoptionSpeed', 'RescuerID'], axis=1).values, X))\ntest = np.hstack((df_test.drop(['Name', 'Description', 'PetID', 'RescuerID'], axis=1).values, X_test))","4048f3c1":"df_train.columns.ravel()","b334b3b5":"target = df_train['AdoptionSpeed']\ntrain_id = df_train['PetID']\ntest_id = df_test['PetID']\ndf_train.drop(['Name', 'Description', 'PetID', 'AdoptionSpeed', 'RescuerID'], axis=1, inplace=True, errors='ignore')\ndf_test.drop(['Name', 'Description', 'PetID', 'RescuerID'], axis=1, inplace=True, errors='ignore')","2b2eac30":"def run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n    kf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n    fold_splits = kf.split(train, target)\n    cv_scores = []\n    qwk_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0], 5))\n    all_coefficients = np.zeros((5, 4))\n    feature_importance_df = pd.DataFrame()\n    i = 1\n    for dev_index, val_index in fold_splits:\n        print('Started ' + label + ' fold ' + str(i) + '\/5')\n        if isinstance(train, pd.DataFrame):\n            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n        else:\n            dev_X, val_X = train[dev_index], train[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n        params2 = params.copy()\n        pred_val_y, pred_test_y, importances, coefficients, qwk = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        all_coefficients[i-1, :] = coefficients\n        if eval_fn is not None:\n            cv_score = eval_fn(val_y, pred_val_y)\n            cv_scores.append(cv_score)\n            qwk_scores.append(qwk)\n            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df['feature'] = train.columns.values\n        fold_importance_df['importance'] = importances\n        fold_importance_df['fold'] = i\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)        \n        i += 1\n    print('{} cv RMSE scores : {}'.format(label, cv_scores))\n    print('{} cv mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv std RMSE score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv QWK scores : {}'.format(label, qwk_scores))\n    print('{} cv mean QWK score : {}'.format(label, np.mean(qwk_scores)))\n    print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n    pred_full_test = pred_full_test \/ 5.0\n    results = {'label': label,\n               'train': pred_train, 'test': pred_full_test,\n                'cv': cv_scores, 'qwk': qwk_scores,\n               'importance': feature_importance_df,\n               'coefficients': all_coefficients}\n    return results\n\nparams = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 80,\n          'max_depth': 9,\n          'learning_rate': 0.01,\n          'bagging_fraction': 0.85,\n          'feature_fraction': 0.8,\n          'min_split_gain': 0.01,\n          'min_child_samples': 150,\n          'min_child_weight': 0.1,\n          'verbosity': -1,\n          'data_random_seed': 3,\n          'early_stop': 100,\n          'verbose_eval': 100,\n          'num_rounds': 10000}\n\ndef runLGB(train_X, train_y, test_X, test_y, test_X2, params):\n    print('Prep LGB')\n    d_train = lgb.Dataset(train_X, label=train_y)\n    d_valid = lgb.Dataset(test_X, label=test_y)\n    watchlist = [d_train, d_valid]\n    print('Train LGB')\n    num_rounds = params.pop('num_rounds')\n    verbose_eval = params.pop('verbose_eval')\n    early_stop = None\n    if params.get('early_stop'):\n        early_stop = params.pop('early_stop')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n    print('Predict 1\/2')\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    optR = OptimizedRounder()\n    optR.fit(pred_test_y, test_y)\n    coefficients = optR.coefficients()\n    pred_test_y_k = optR.predict(pred_test_y, coefficients)\n    print(\"Valid Counts = \", Counter(test_y))\n    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n    print(\"Coefficients = \", coefficients)\n    qwk = quadratic_weighted_kappa(test_y, pred_test_y_k)\n    print(\"QWK = \", qwk)\n    print('Predict 2\/2')\n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(), coefficients, qwk\n\nresults = run_cv_model(df_train, df_test, target, runLGB, params, rmse, 'lgb')","ef3caa74":"imports = results['importance'].groupby('feature')['feature', 'importance'].mean().reset_index()\nimports.sort_values('importance', ascending=False)","662b4daa":"optR = OptimizedRounder()\ncoefficients_ = np.mean(results['coefficients'], axis=0)\nprint(coefficients_)\ntrain_predictions = [r[0] for r in results['train']]\ntrain_predictions = optR.predict(train_predictions, coefficients_).astype(int)\nCounter(train_predictions)","753f509f":"optR = OptimizedRounder()\ntest_predictions = [r[0] for r in results['test']]\ntest_predictions = optR.predict(test_predictions, coefficients_).astype(int)\nCounter(test_predictions)","d250f8ce":"pd.DataFrame(sk_cmatrix(target, train_predictions), index=list(range(5)), columns=list(range(5)))","c5d07bf1":"quadratic_weighted_kappa(target, train_predictions)\n","978a856d":"rmse(target, [r[0] for r in results['train']])\n","eaa6dc2b":"submission = pd.DataFrame({'PetID': test_id, 'AdoptionSpeed': test_predictions})\nsubmission.head()","15f5120d":"submission.to_csv('submission.csv', index=False)","c3c871f3":" ## Bivariate with target","34085eff":"\n# Distribution","fe4a953c":"There are no full-duplicates in the data set. Let's see if there are duplicates once we remove name, Pet ID and description (we keep rescuer ID though)","702d98f2":"# PetFinder.my Adoption Prediction\n**Author**: Enrique Herreros  \n**First version date**: 29\/12\/2018  \n**Description**: A Kernel with an Exploratory Data Analysis to inspect superficially all the data that have been provided to us to predict adoption speed \n\n**Notes** \nIn this competition you will predict the speed at which a pet is adopted, based on the pet\u2019s listing information present on PetFinder.\nSometimes a profile represents a group of pets. In this case, the speed of adoption is determined by the speed at which all of the pets are adopted.\nThe data included open text, tabular, image data and results from running Google Vision API. See below for details.\nThis is a Kernels-only competition.\nAt the end of the competition, test data will be replaced in their entirety with new data of approximately the same size, and your kernels will be rerun on the new data.\n\n**File descriptions** \ntrain.csv - Tabular\/text data for the training set\ntest.csv - Tabular\/text data for the test set\nsample_submission.csv - A sample submission file in the correct format\nbreed_labels.csv - Contains Type, and BreedName for each BreedID. Type 1 is dog, 2 is cat.\ncolor_labels.csv - Contains ColorName for each ColorID\nstate_labels.csv - Contains StateName for each StateID\n\n# Table of contents\n1. Libraries and Data loading\n* Sneak peek\n  * Tabular\n  * Images\n* Null count\n* Duplicates\n* Correlation matrix\n * Dogs\n * Cats\n* Distribution\n * Univariate\n * Bivariate with target\n * Bivariate between correlated variables\n* Base Model\n* Submission\n","d78e8943":"Nope. What about common names?","b6061bda":"Percentage of null values in the train set per column","192f24d4":"Correlation matrix of cats part of the test set","d45dea76":"## Univariate\n","a25bbc5e":"Let's take a look at random images from the train set together with the pet's name and its adoption speed ","3ecde841":"And now for the rest of the features","88beea09":"# Null count","2a943442":"\n## Correlation matrix","19b7e41b":"Columns","fee2dfe5":"Features","b9ee2fab":"# Sneak peek","f9ba22be":"In the train set","857fe9f2":"## Bivariate between correlated variables","ca5c4715":"Example of output from Google Vision API","340387d2":"# Base model","90ba2666":"Are there any shared values of RescuerID between train and test?","f1e624e9":"Train and test data","da3573ac":"Libraries loading","5cd88f31":"I'm not sure if the classes predicted from the Google API would be of interest in this problem as the whole data set is about dogs and cats. I think that it would be more interesting to have aesthetic quality, blurriness or stuff related to quality of the picture, that could attract users towards certain pet or the other. We like it or not, humans feel attracted mainly by what they see.","4327344d":"Main data set","62e4aaaf":"# Libraries and Data loading","a424eb82":"Global variables","e2a727b7":"Now, let's adjust the coefficients to optimize the Quadratic Weighted Kappa","8401e1f8":"## Images\n","753d7503":"# Duplicates","303ff8d7":"Correlation matrix of whole test set","84cbb1f6":"## Only cats","3aa9fd9a":"## Only dogs","6411e063":"In the test set","396a18fe":"Breed, colors and states mapping values","e08144d8":"# Submission","76e81e7f":"The previous plots yield multiple things:\n1. Most of the features have very skewed distributions\n2. We should group together misrepresented values before one hot encoding them\n3. Not many publishings contain videos\n4. Most of the publishings have less than 5 pictures\n5. The target class is quite balanced (classes 1 to 4 have around 23% presence), apart from class 0 (less than 3% of the cases)\n6. Null encoding is sometimes represented with the value 0 = Not Specified in MaturitySize, FurLength or Health or value 3 = Not Sure in Vaccinated, Dewormed or Sterilized (as specified in the description)","b130e784":"Percentage of null values in the test set per column","7a4e0ccc":"First of all, let's see the target distribution","06aae641":"Correlation matrix of whole train set","84eefe68":"Correlation matrix of dogs part of the test set","34a078ee":"Visualization configuration","bf63a2d1":"TO-DO: same approach but normalizing by AdoptionSpeed frequency","2212c23b":"## Tabular","f227f324":"Wow, more than 800 names in common, let's see a few of them...","cf1b6abe":"Surprisingly, model considers of least importance the fact that the pet has ben dewormed, spayed \/ neutered, vaccinated. Gender is also not an important factor in this model. On the other hand, breed, number of photos, location, age and amount of pets to be adopted at once are the most important factors.\n\nI still want to explore many things:\n1. Reduce the amount of components in the SVD model\n2. Play with images. I will try to share with you a kernel where I validate the following hypothesis: visually similar pets \/ images have similar adoption speed\n3. Compare more single models: CATboost (of course :P), XGBoost, RF, etc\n4. ...","c6170ffb":"## All animals","e8a314c1":"Correlation matrix of dogs part of the train set","7cca7ca2":"In general, there are not many null values. There seems to be only null values in the Name and Description columns. Either PetFinder considers those 2 values as mandatory ones to get filled during pet card completition or the null values are encoded with a value, we will check later.","82b8be84":"Correlation matrix of cats part of the train set","e84b358e":"> **ALL VERY CUTE!**"}}