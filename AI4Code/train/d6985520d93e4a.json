{"cell_type":{"c3ff93c3":"code","4e3b5437":"code","c40a3556":"code","7a08de7c":"code","1a191d4e":"code","629524f3":"code","cb8369fb":"code","1b31d85d":"code","5f119d51":"code","9f1c1bb3":"code","406a3670":"code","4ca4587d":"code","544e502c":"code","98467aa2":"code","8136ef76":"code","a24e8318":"code","2e8b950c":"code","4f5fff7b":"code","4b5773d8":"code","fb9d9b9d":"markdown","3ebb3aab":"markdown","c2c9a791":"markdown","bdc2f455":"markdown","168da59c":"markdown"},"source":{"c3ff93c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e3b5437":"df = pd.read_csv('..\/input\/besedo\/train_trimmed_encod.csv', encoding='utf8')\npd.set_option('display.max_columns', None)\ndf.head()","c40a3556":"(1e2 * df.isnull().sum()\/len(df)).plot(kind='barh')\nplt.xlim(0, 10**2)\nplt.grid();","7a08de7c":"plt.figure(figsize=(6, 5))\nsns.heatmap(df.corr());","1a191d4e":"# Lets first handle numerical features with nan value\nnumerical_nan = [feature for feature in df.columns if df[feature].isna().sum()>1 and df[feature].dtypes!='O']\nnumerical_nan","629524f3":"df[numerical_nan].isna().sum()","cb8369fb":"## Replacing the numerical Missing Values\n\nfor feature in numerical_nan:\n    ## We will replace by using median since there are outliers\n    median_value=df[feature].median()\n    \n    df[feature].fillna(median_value,inplace=True)\n    \ndf[numerical_nan].isnull().sum()","1b31d85d":"from sklearn.preprocessing import LabelEncoder\n\n#fill in mean for floats\nfor c in df.columns:\n    if df[c].dtype=='float16' or  df[c].dtype=='float32' or  df[c].dtype=='float64':\n        df[c].fillna(df[c].mean())\n\n#fill in -999 for categoricals\ndf = df.fillna(-999)\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n        \nprint('Labelling done.')","5f119d51":"df = pd.get_dummies(df)","9f1c1bb3":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import (StandardScaler, MinMaxScaler, \n                                   OneHotEncoder, LabelEncoder)\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score","406a3670":"num_feat = df.drop('ad_customerSpecific_moderationDecision', axis=1).select_dtypes(include=np.number).columns\ncat_feat = df.drop('ad_customerSpecific_moderationDecision', axis=1).select_dtypes(include=['object']).columns\nX = df.drop('ad_customerSpecific_moderationDecision', axis=1)\ny = df['ad_customerSpecific_moderationDecision']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)","4ca4587d":"numeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)),\n    # ('imputer', KNNImputer()),\n    ('scaler', StandardScaler()),\n    # ('poly', PolynomialFeatures(2, include_bias=False)), \n    # ('scaler1', StandardScaler()),\n    # ('pca', PCA(n_components=100))\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('one_hot', OneHotEncoder(handle_unknown='ignore')),\n])","544e502c":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, num_feat),\n        ('cat', categorical_transformer, cat_feat)\n    ])","98467aa2":"pipe = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier',  LogisticRegression())\n])\n\nmodel = pipe.fit(X_train, y_train)\ntarget_names = y_test.unique().astype(str)\ny_pred = model.predict(X_test)","8136ef76":"print(classification_report(y_test, y_pred, target_names=target_names))","a24e8318":"print(round(pd.DataFrame(confusion_matrix(y_test, y_pred)\/len(y_test)*1e2)))","2e8b950c":"f1_score(y_true=y_test, y_pred=y_pred)","4f5fff7b":"from yellowbrick.classifier import DiscriminationThreshold\n\nfig, ax = plt.subplots(figsize=(6, 6))\nmodel_viz = DiscriminationThreshold(pipe)\nmodel_viz.fit(X_train, y_train)\nmodel_viz.poof();","4b5773d8":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Be patient. Mar\u00edlia Prata, @mpwolke was Here.')","fb9d9b9d":"#Probability of Default modeling\n\nProbability of default (PD) is a financial term describing the likelihood of a default over a particular time horizon. It provides an estimate of the likelihood that a borrower will be unable to meet its debt obligations.\nhttps:\/\/en.wikipedia.org\/wiki\/Probability_of_default\n\n\nThis Dataset is Not about Financial or credit analyses, Though I wanted to plot the DiscriminationThreshold(pipe) which I did.","3ebb3aab":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQqdcTTEGEG1NICznb52wLgamAaq8VjKe9Ngg&usqp=CAU)slideplayer.com","c2c9a791":"#Codes by Amol Deshmukh https:\/\/www.kaggle.com\/des137\/probability-of-default\/notebook","bdc2f455":"#Working Example","168da59c":"Where are the missing values from `ad_content_price_amount`, `customerSpecific_autoCarOrder` and `ad_location_customerSpecific_regionList_id`"}}