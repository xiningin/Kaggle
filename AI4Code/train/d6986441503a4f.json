{"cell_type":{"d8fbef3e":"code","2be29ca4":"code","73219fdb":"code","3866ee5c":"code","7a886edd":"code","d0bf8220":"code","2f51794c":"code","32bc1a0a":"code","14a44c82":"code","a83e80b4":"code","acb3ddf9":"code","a43da284":"code","65782284":"code","a049bb38":"code","5073abde":"code","101ccf77":"code","11a79eab":"code","4bcfa0a1":"code","c840d7e5":"code","fe62a6dc":"code","427f976c":"code","f2de4084":"markdown","6c5c59d8":"markdown","bb45ae75":"markdown","2b58ea51":"markdown","d27439a8":"markdown"},"source":{"d8fbef3e":"import torch\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re\nimport nltk\nimport string\nfrom collections import Counter\nfrom torch.utils.data import DataLoader\nimport numpy as np","2be29ca4":"cols = [\"sentiment\", \"review\"]\ntrain_orig = pd.read_csv(\"..\/input\/yelp-review-dataset\/yelp_review_polarity_csv\/train.csv\", names=cols)\ntest = pd.read_csv(\"..\/input\/yelp-review-dataset\/yelp_review_polarity_csv\/test.csv\", names=cols)","73219fdb":"# Reduce number of samples\ntrain_ = train_orig.groupby('sentiment',as_index=False).apply(lambda x: x.sample(frac=0.3))\ntrain_ = train_.reset_index( level = 0, drop=True)","3866ee5c":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n    #text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text) \n    return text","7a886edd":"train = train_.copy()\ntrain.review = train.review.apply(clean_text)\ntest.review = test.review.apply(clean_text)","d0bf8220":"X_train, X_valid, y_train, y_valid = train_test_split(train[\"review\"], train[\"sentiment\"], test_size=0.15, random_state=42, stratify=train[\"sentiment\"])\ntrain = pd.concat([X_train, y_train], axis=1)\nvalid = pd.concat([X_valid, y_valid], axis=1)","2f51794c":"train[\"split\"] = \"train\"\ntest[\"split\"] = \"test\"\nvalid[\"split\"] = \"val\"\nreviews_df = pd.concat([train, test, valid])\nreviews_df.rename(columns={\"sentiment\": \"rating\"}, inplace=True)\nreviews_df.head()","32bc1a0a":"reviews_df.to_csv(\".\/reviews.csv\")","14a44c82":"from torch.utils.data import Dataset\n\nclass ReviewDataset(Dataset):\n    def __init__(self, review_df, vectorizer):\n        \"\"\"\n        Args:\n            review_df (pandas.DataFrame): the dataset\n            vectorizer (ReviewVectorizer): vectorizer instantiated from dataset\n        \"\"\"\n        self.review_df = review_df\n        self._vectorizer = vectorizer\n\n        self.train_df = self.review_df[self.review_df.split=='train']\n        self.train_size = len(self.train_df)\n\n        self.val_df = self.review_df[self.review_df.split=='val']\n        self.validation_size = len(self.val_df)\n\n        self.test_df = self.review_df[self.review_df.split=='test']\n        self.test_size = len(self.test_df)\n\n        self._lookup_dict = {'train': (self.train_df, self.train_size),\n                             'val': (self.val_df, self.validation_size),\n                             'test': (self.test_df, self.test_size)}\n\n        self.set_split('train')\n\n    @classmethod\n    def load_dataset_and_make_vectorizer(cls, review_csv):\n        \"\"\"Load dataset and make a new vectorizer from scratch\n        \n        Args:\n            review_csv (str): location of the dataset\n        Returns:\n            an instance of ReviewDataset\n        \"\"\"\n        review_df = pd.read_csv(review_csv)\n        return cls(review_df, ReviewVectorizer.from_dataframe(review_df))\n\n    def get_vectorizer(self):\n        \"\"\" returns the vectorizer \"\"\"\n        return self._vectorizer\n\n    def set_split(self, split=\"train\"):\n        \"\"\" selects the splits in the dataset using a column in the dataframe \n        \n        Args:\n            split (str): one of \"train\", \"val\", or \"test\"\n        \"\"\"\n        self._target_split = split\n        self._target_df, self._target_size = self._lookup_dict[split]\n\n    def __len__(self):\n        return self._target_size\n\n    def __getitem__(self, index):\n        \"\"\"the primary entry point method for PyTorch datasets\n        \n        Args:\n            index (int): the index to the data point \n        Returns:\n            a dict of the data point's features (x_data) and label (y_target)\n        \"\"\"\n        row = self._target_df.iloc[index]\n\n        review_vector = \\\n            self._vectorizer.vectorize(row.review)\n\n        rating_index = \\\n            self._vectorizer.rating_vocab.lookup_token(row.rating)\n\n        return {'x_data': review_vector,\n                'y_target': rating_index}\n\n    def get_num_batches(self, batch_size):\n        \"\"\"Given a batch size, return the number of batches in the dataset\n        \n        Args:\n            batch_size (int)\n        Returns:\n            number of batches in the dataset\n        \"\"\"\n        return len(self) \/\/ batch_size","a83e80b4":"class Vocabulary(object):\n    \"\"\"Class to process text and extract Vocabulary for mapping\"\"\"\n\n    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n        \"\"\"\n        Args:\n            token_to_idx (dict): a pre-existing map of tokens to indices\n            add_unk (bool): a flag that indicates whether to add the UNK token\n            unk_token (str): the UNK token to add into the Vocabulary\n        \"\"\"\n\n        if token_to_idx is None:\n            token_to_idx = {}\n        self._token_to_idx = token_to_idx\n\n        self._idx_to_token = {idx: token \n                              for token, idx in self._token_to_idx.items()}\n\n        self._add_unk = add_unk\n        self._unk_token = unk_token\n        \n        self.unk_index = -1\n        if add_unk:\n            self.unk_index = self.add_token(unk_token) \n        \n        \n    def to_serializable(self):\n        \"\"\" returns a dictionary that can be serialized \"\"\"\n        return {'token_to_idx': self._token_to_idx, \n                'add_unk': self._add_unk, \n                'unk_token': self._unk_token}\n\n    @classmethod\n    def from_serializable(cls, contents):\n        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n        return cls(**contents)\n\n    def add_token(self, token):\n        \"\"\"Update mapping dicts based on the token.\n\n        Args:\n            token (str): the item to add into the Vocabulary\n        Returns:\n            index (int): the integer corresponding to the token\n        \"\"\"\n        if token in self._token_to_idx:\n            index = self._token_to_idx[token]\n        else:\n            index = len(self._token_to_idx)\n            self._token_to_idx[token] = index\n            self._idx_to_token[index] = token\n        return index\n\n    def lookup_token(self, token):\n        \"\"\"Retrieve the index associated with the token \n          or the UNK index if token isn't present.\n        \n        Args:\n            token (str): the token to look up \n        Returns:\n            index (int): the index corresponding to the token\n        Notes:\n            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n              for the UNK functionality \n        \"\"\"\n        if self._add_unk:\n            return self._token_to_idx.get(token, self.unk_index)\n        else:\n            return self._token_to_idx[token]\n\n    def lookup_index(self, index):\n        \"\"\"Return the token associated with the index\n        \n        Args: \n            index (int): the index to look up\n        Returns:\n            token (str): the token corresponding to the index\n        Raises:\n            KeyError: if the index is not in the Vocabulary\n        \"\"\"\n        if index not in self._idx_to_token:\n            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n        return self._idx_to_token[index]\n\n    def __str__(self):\n        return \"<Vocabulary(size=%d)>\" % len(self)\n\n    def __len__(self):\n        return len(self._token_to_idx)","acb3ddf9":"class ReviewVectorizer(object):\n    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n    def __init__(self, review_vocab, rating_vocab):\n        \"\"\"\n        Args:\n            review_vocab (Vocabulary): maps words to integers\n            rating_vocab (Vocabulary): maps class labels to integers\n        \"\"\"\n        self.review_vocab = review_vocab\n        self.rating_vocab = rating_vocab\n\n    def vectorize(self, review):\n        \"\"\"Create a collapsed one-hit vector for the review\n        \n        Args:\n            review (str): the review\n        Returns:\n            one_hot (np.ndarray): the collapsed one-hot encoding\n        \"\"\"\n        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n        \n        for token in review.split(\" \"):\n            if token not in string.punctuation:\n                one_hot[self.review_vocab.lookup_token(token)] = 1\n\n        return one_hot\n\n    @classmethod\n    def from_dataframe(cls, review_df, cutoff=25):\n        \"\"\"Instantiate the vectorizer from the dataset dataframe\n        \n        Args:\n            review_df (pandas.DataFrame): the review dataset\n            cutoff (int): the parameter for frequency-based filtering\n        Returns:\n            an instance of the ReviewVectorizer\n        \"\"\"\n        review_vocab = Vocabulary(add_unk=True)\n        rating_vocab = Vocabulary(add_unk=False)\n        \n        # Add ratings\n        for rating in sorted(set(review_df.rating)):\n            rating_vocab.add_token(rating)\n\n        # Add top words if count > provided count\n        word_counts = Counter()\n        for review in review_df.review:\n            for word in review.split(\" \"):\n                if word not in string.punctuation:\n                    word_counts[word] += 1\n               \n        for word, count in word_counts.items():\n            if count > cutoff:\n                review_vocab.add_token(word)\n\n        return cls(review_vocab, rating_vocab)\n\n    @classmethod\n    def from_serializable(cls, contents):\n        \"\"\"Intantiate a ReviewVectorizer from a serializable dictionary\n        \n        Args:\n            contents (dict): the serializable dictionary\n        Returns:\n            an instance of the ReviewVectorizer class\n        \"\"\"\n        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n        rating_vocab =  Vocabulary.from_serializable(contents['rating_vocab'])\n\n        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n\n    def to_serializable(self):\n        \"\"\"Create the serializable dictionary for caching\n        \n        Returns:\n            contents (dict): the serializable dictionary\n        \"\"\"\n        return {'review_vocab': self.review_vocab.to_serializable(),\n                'rating_vocab': self.rating_vocab.to_serializable()}","a43da284":"from torch.utils.data import DataLoader\n\ndef generate_batches(dataset, batch_size, shuffle=True,\n                     drop_last=True, device=\"cpu\"):\n    \"\"\"\n    A generator function which wraps the PyTorch DataLoader. It will \n      ensure each tensor is on the write device location.\n    \"\"\"\n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n                            shuffle=shuffle, drop_last=drop_last)\n\n    for data_dict in dataloader:\n        out_data_dict = {}\n        for name, tensor in data_dict.items():\n            out_data_dict[name] = data_dict[name].to(device)\n        yield out_data_dict","65782284":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass ReviewClassifier(nn.Module):\n    \"\"\" a simple perceptron-based classifier \"\"\"\n    def __init__(self, num_features):\n        \"\"\"\n        Args:\n            num_features (int): the size of the input feature vector\n        \"\"\"\n        super(ReviewClassifier, self).__init__()\n        self.fc1 = nn.Linear(in_features=num_features, \n                             out_features=1)\n\n    def forward(self, x_in, apply_sigmoid=False):\n        \"\"\"The forward pass of the classifier\n        \n        Args:\n            x_in (torch.Tensor): an input data tensor \n                x_in.shape should be (batch, num_features)\n            apply_sigmoid (bool): a flag for the sigmoid activation\n                should be false if used with the cross-entropy losses\n        Returns:\n            the resulting tensor. tensor.shape should be (batch,).\n        \"\"\"\n        y_out = self.fc1(x_in).squeeze()\n        if apply_sigmoid:\n            y_out = F.sigmoid(y_out)\n        return y_out","a049bb38":"from argparse import Namespace\n\nargs = Namespace(\n    # Data and path information\n    frequency_cutoff=25,\n    model_state_file='model.pth',\n    review_csv='.\/reviews.csv',\n    save_dir='.\/',\n    vectorizer_file='vectorizer.json',\n    # No model hyperparameters\n    # Training hyperparameters\n    batch_size=128,\n    early_stopping_criteria=5,\n    learning_rate=0.001,\n    num_epochs=100,\n    seed=1337,\n    # Runtime options omitted for space\n)\nargs","5073abde":"def compute_accuracy(y_pred, y_target):\n    y_target = y_target.cpu()\n    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n    return n_correct \/ len(y_pred_indices) * 100","101ccf77":"import torch.optim as optim \n\ndef make_train_state(args):\n    return {'epoch_index': 0,\n            'train_loss': [],\n            'train_acc': [],\n            'val_loss': [],\n            'val_acc': [],\n            'test_loss': -1,\n            'test_acc': -1}\ntrain_state = make_train_state(args)\n\nif not torch.cuda.is_available():\n    args.cuda = False\nargs.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n\n# dataset and vectorizer\ndataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\nvectorizer = dataset.get_vectorizer()\n\n# model\nclassifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\nclassifier = classifier.to(args.device)\n\n# loss and optimizer\nloss_func = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)","11a79eab":"for epoch_index in range(args.num_epochs):\n    train_state['epoch_index'] = epoch_index\n\n    # Iterate over training dataset\n\n    # setup: batch generator, set loss and acc to 0, set train mode on\n    dataset.set_split('train')\n    batch_generator = generate_batches(dataset, \n                                       batch_size=args.batch_size, \n                                       device=args.device)\n    running_loss = 0.0\n    running_acc = 0.0\n    classifier.train()\n    \n    for batch_index, batch_dict in enumerate(batch_generator):\n        # the training routine is 5 steps:\n\n        # step 1. zero the gradients\n        optimizer.zero_grad()\n\n        # step 2. compute the output\n        y_pred = classifier(x_in=batch_dict['x_data'].float())\n\n        # step 3. compute the loss\n        loss = loss_func(y_pred, batch_dict['y_target'].float())\n        loss_batch = loss.item()\n        running_loss += (loss_batch - running_loss) \/ (batch_index + 1)\n\n        # step 4. use loss to produce gradients\n        loss.backward()\n\n        # step 5. use optimizer to take gradient step\n        optimizer.step()\n\n        # -----------------------------------------\n        # compute the accuracy\n        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n        running_acc += (acc_batch - running_acc) \/ (batch_index + 1)\n\n    train_state['train_loss'].append(running_loss)\n    train_state['train_acc'].append(running_acc)\n    print(\"Accuracy: {} \\nLoss: {}\".format(running_acc, running_loss))\n    \n    # Iterate over val dataset\n\n    # setup: batch generator, set loss and acc to 0, set eval mode on\n    dataset.set_split('val')\n    batch_generator = generate_batches(dataset, \n                                       batch_size=args.batch_size, \n                                       device=args.device)\n    running_loss = 0.\n    running_acc = 0.\n    classifier.eval()\n\n    for batch_index, batch_dict in enumerate(batch_generator):\n\n        # step 1. compute the output\n        y_pred = classifier(x_in=batch_dict['x_data'].float())\n\n        # step 2. compute the loss\n        loss = loss_func(y_pred, batch_dict['y_target'].float())\n        loss_batch = loss.item()\n        running_loss += (loss_batch - running_loss) \/ (batch_index + 1)\n\n        # step 3. compute the accuracy\n        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n        running_acc += (acc_batch - running_acc) \/ (batch_index + 1)\n\n    train_state['val_loss'].append(running_loss)\n    train_state['val_acc'].append(running_acc)\n    ","4bcfa0a1":"dataset.set_split('test')\nbatch_generator = generate_batches(dataset, \n                                   batch_size=args.batch_size, \n                                   device=args.device)\nrunning_loss = 0.\nrunning_acc = 0.\nclassifier.eval()\n\nfor batch_index, batch_dict in enumerate(batch_generator):\n    # compute the output\n    y_pred = classifier(x_in=batch_dict['x_data'].float())\n\n    # compute the loss\n    loss = loss_func(y_pred, batch_dict['y_target'].float())\n    loss_batch = loss.item()\n    running_loss += (loss_batch - running_loss) \/ (batch_index + 1)\n\n    # compute the accuracy\n    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n    running_acc += (acc_batch - running_acc) \/ (batch_index + 1)\n\ntrain_state['test_loss'] = running_loss\ntrain_state['test_acc'] = running_acc","c840d7e5":"def predict_rating(review, classifier, vectorizer,\n                   decision_threshold=0.5):\n    \"\"\"Predict the rating of a review\n\n    Args:\n        review (str): the text of the review\n        classifier (ReviewClassifier): the trained model\n        vectorizer (ReviewVectorizer): the corresponding vectorizer\n        decision_threshold (float): The numerical boundary which\n            separates the rating classes\n    \"\"\"\n\n    review = preprocess_text(review)\n    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n    result = classifier(vectorized_review.view(1, -1))\n\n    probability_value = F.sigmoid(result).item()\n\n    index =  1\n    if probability_value < decision_threshold:\n        index = 0\n\n    return vectorizer.rating_vocab.lookup_index(index)\n\ntest_review = \"this is a pretty awesome book\"\nprediction = predict_rating(test_review, classifier, vectorizer)\nprint(\"{} -> {}\".format(test_review, prediction)","fe62a6dc":"# Sort weights\nfc1_weights = classifier.fc1.weight.detach()[0]\n_, indices = torch.sort(fc1_weights, dim=0, descending=True)\nindices = indices.numpy().tolist()\n\n# Top 20 words\nprint(\"Influential words in Positive Reviews:\")\nprint(\"--------------------------------------\")\nfor i in range(20):\n    print(vectorizer.review_vocab.lookup_index(indices[i]))","427f976c":"# Top 20 negative words\nprint(\"Influential words in Negative Reviews:\")\nprint(\"--------------------------------------\")\nindices.reverse()\nfor i in range(20):\n    print(vectorizer.review_vocab.lookup_index(indices[i]))","f2de4084":"# Model","6c5c59d8":"# Vectorizer","bb45ae75":"# Dataset class","2b58ea51":"# Dataloader","d27439a8":"# Vocabulary"}}