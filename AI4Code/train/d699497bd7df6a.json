{"cell_type":{"4a3cc383":"code","063b8372":"code","e3308183":"code","1e321b1e":"code","d9a97b33":"code","bbcf76a5":"code","2699d20b":"code","ad4d129e":"code","e512705e":"code","5497a05b":"code","110fd643":"code","14c97abd":"code","843d7aaf":"code","b83d55c1":"code","8b2ea7de":"code","add12961":"code","97d71b84":"code","b9e0123b":"code","bfac05ed":"code","14bb7a36":"code","5bbaa983":"code","84af5670":"code","a9b729da":"code","bcc9f8a4":"code","c03c6d6e":"code","4231d33d":"code","6500b1da":"code","6ac9ede1":"code","181bb3a8":"code","f8dc55fd":"code","51917694":"code","d36431bc":"code","d6fcdb3f":"code","4ed22fc8":"code","22709233":"code","3eead350":"code","25381549":"code","c39050c0":"code","b018c80f":"code","34bef6de":"code","2d032dc6":"code","277402d2":"code","3236447e":"code","fbee337e":"code","37377cd7":"code","1d9f358c":"code","ca9acd48":"code","edc523ed":"code","7712fe9f":"code","d27833a5":"code","f706f6d1":"code","a96cb3a8":"code","e6485434":"code","45f04980":"code","5013c1c3":"code","028c646f":"code","4d22becc":"code","e7441fbd":"code","e1ef9e74":"code","67946818":"code","f18c70e8":"code","110b912d":"code","3efc2a32":"code","75087998":"code","b83b7837":"code","6dc94ed0":"code","169e892a":"code","db0ea164":"code","0748e983":"code","0d61ac49":"code","026fdaec":"code","5a10abdc":"code","bd9d451b":"code","c0d0692a":"code","6deda6e8":"code","42d725f1":"code","94618d9c":"code","3e07a488":"markdown","2098ff89":"markdown","60e11f20":"markdown","dcd6f8bf":"markdown","2eff54d3":"markdown","50fc324b":"markdown","b0c75fa7":"markdown","e4ffde6b":"markdown","6d34e4f7":"markdown","b01f4473":"markdown","c0a39e56":"markdown","0b9c4fc6":"markdown","6057c63f":"markdown","130699f8":"markdown","8bc36a9b":"markdown","935622ae":"markdown","7f493e65":"markdown","b5351bd8":"markdown","37deccf6":"markdown","13a90dbe":"markdown","9f98f31c":"markdown","b12eadd2":"markdown","5a66de58":"markdown","aa49cf8e":"markdown","0cc3d0aa":"markdown","27216a42":"markdown","7cb41f36":"markdown","01bc1f28":"markdown","c5ff114a":"markdown","ed42c298":"markdown","5a71f358":"markdown","48c5a5a1":"markdown","5916e261":"markdown","c2265b42":"markdown","d6f5b3e7":"markdown","137a08bc":"markdown","6bad4463":"markdown","3bfa68b8":"markdown","47b25fae":"markdown","05010cf6":"markdown","146879f8":"markdown","bbad402b":"markdown","a7589c11":"markdown","2fd4580a":"markdown","40c98999":"markdown"},"source":{"4a3cc383":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","063b8372":"# Read the given CSV file, and view some sample records\n\nadvertising = pd.read_csv(\"\/kaggle\/input\/sales-advertisment\/advertising.csv\")\nadvertising.head()","e3308183":"#inspect the various aspects of our dataframe\nadvertising.shape","1e321b1e":"advertising.info()","d9a97b33":"advertising.describe()","bbcf76a5":"import matplotlib.pyplot as plt \nimport seaborn as sns","2699d20b":"sns.pairplot(advertising, x_vars=['TV', 'Newspaper', 'Radio'], y_vars='Sales', aspect=1, kind='scatter')\nplt.show()","ad4d129e":"sns.heatmap(advertising.corr(), cmap=\"YlGnBu\", annot = True)\nplt.show()","e512705e":"X = advertising['TV']\ny = advertising['Sales']","5497a05b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)","110fd643":"# Let's now take a look at the train dataset\n\nX_train.head()","14c97abd":"y_train.head()","843d7aaf":"import statsmodels.api as sm","b83d55c1":"# Add a constant to get an intercept\nX_train_sm = sm.add_constant(X_train)\n\n# Fit the resgression line using 'OLS'\nlr = sm.OLS(y_train, X_train_sm).fit()","8b2ea7de":"# Print the parameters, i.e. the intercept and the slope of the regression line fitted\nlr.params","add12961":"# Performing a summary operation lists out all the different parameters of the regression line fitted\nprint(lr.summary())","97d71b84":"plt.scatter(X_train, y_train)\nplt.plot(X_train, 6.9487 + 0.0545*X_train, 'r')\nplt.show()","b9e0123b":"y_train_pred = lr.predict(X_train_sm)\nres = (y_train - y_train_pred)","bfac05ed":"fig = plt.figure()\nsns.distplot(res, bins = 15)\nfig.suptitle('Error Terms', fontsize = 15)                  # Plot heading \nplt.xlabel('y_train - y_train_pred', fontsize = 15)         # X-label\nplt.show()","14bb7a36":"plt.scatter(X_train,res)\nplt.show()","5bbaa983":"# Add a constant to X_test\nX_test_sm = sm.add_constant(X_test)\n\n# Predict the y values corresponding to X_test_sm\ny_pred = lr.predict(X_test_sm)","84af5670":"y_pred.head()","a9b729da":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","bcc9f8a4":"#Returns the mean squared error; we'll take a square root\nnp.sqrt(mean_squared_error(y_test, y_pred))","c03c6d6e":"r_squared = r2_score(y_test, y_pred)\nr_squared","4231d33d":"plt.scatter(X_test, y_test)\nplt.plot(X_test, 6.948 + 0.054 * X_test, 'r')\nplt.show()","6500b1da":"from sklearn.model_selection import train_test_split\nX_train_lm, X_test_lm, y_train_lm, y_test_lm = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)","6ac9ede1":"X_train_lm.shape","181bb3a8":"X_train_lm = X_train_lm.values.reshape(-1,1)\nX_test_lm = X_test_lm.values.reshape(-1,1)","f8dc55fd":"print(X_train_lm.shape)\nprint(y_train_lm.shape)\nprint(X_test_lm.shape)\nprint(y_test_lm.shape)","51917694":"from sklearn.linear_model import LinearRegression\n\n# Representing LinearRegression as lr(Creating LinearRegression Object)\nlm = LinearRegression()\n\n# Fit the model using lr.fit()\nlm.fit(X_train_lm, y_train_lm)","d36431bc":"print(lm.intercept_)\nprint(lm.coef_)","d6fcdb3f":"sns.pairplot(advertising)\nplt.show()","4ed22fc8":"plt.figure(figsize=(20, 15))\nplt.subplot(2,2,1)\nsns.boxplot(y= advertising['TV'],   palette=\"Set1\"   )\nplt.subplot(2,2,2)\nsns.boxplot(y = advertising['Radio'],   palette=\"Set2\" )\nplt.subplot(2,2,3)\nsns.boxplot(y= advertising['Newspaper'],   palette=\"Set3\" )\nplt.subplot(2,2,4)\nsns.boxplot(y= advertising['Sales'],  palette=\"Set1\")\nplt.show()\n\nsns.boxplot(data=advertising.iloc[:,0:4])\nplt.show()","22709233":"from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\ndf_train, df_test = train_test_split(advertising, train_size = 0.7, test_size = 0.3, random_state = 100)","3eead350":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","25381549":"# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = ['TV','Radio','Newspaper','Sales']\n\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])","c39050c0":"df_train.head()","b018c80f":"df_train.describe()","34bef6de":"# Let's check the correlation coefficients to see which variables are highly correlated\n\nplt.figure(figsize = (6, 3))\nsns.heatmap(df_train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","2d032dc6":"y_train = df_train.pop('Sales')\nX_train = df_train","277402d2":"X_train.head()","3236447e":"import statsmodels.api as sm\n\n# Add a constant\nX_train_lm = sm.add_constant(X_train )\n\n# Create a first fitted model\nlr = sm.OLS(y_train, X_train_lm).fit()","fbee337e":"# Check the parameters obtained\n\nlr.params","37377cd7":"# Print a summary of the linear regression model obtained\nprint(lr.summary())","1d9f358c":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","ca9acd48":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","edc523ed":"# Dropping highly correlated variables and insignificant variables\n\nX_train2 = X_train.drop('Newspaper', 1,)\nX_train2.head()","7712fe9f":"# Build a second fitted model\nX_train_lm = sm.add_constant(X_train2)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","d27833a5":"# Print the summary of the model\nprint(lr_2.summary())","f706f6d1":"# Calculate the VIFs again for the new model\n\nvif = pd.DataFrame()\nvif['Features'] = X_train2.columns\nvif['VIF'] = [variance_inflation_factor(X_train2.values, i) for i in range(X_train2.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","a96cb3a8":"y_train_price = lr_2.predict(X_train_lm)","e6485434":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","45f04980":"df_test.head()","5013c1c3":"num_vars = ['TV', 'Radio', 'Sales'  ]\n\n#df_test[num_vars] = scaler.transform(df_test[num_vars])\n\n\ndf_test[num_vars] = scaler.fit_transform(df_test[num_vars])","028c646f":"df_test.describe()","4d22becc":"#### Dividing into X_test and y_test\ny_test = df_test.pop('Sales')\nX_test = df_test","e7441fbd":"# Adding constant variable to test dataframe\nX_test_m4 = sm.add_constant(X_test)","e1ef9e74":"X_test_m4.head()","67946818":"# Creating X_test_m4 dataframe by dropping variables from X_test_m4\n\nX_test_m4 = X_test_m4.drop([\"Newspaper\" ], axis = 1)","f18c70e8":"# Making predictions using the fourth model\n\ny_pred_m4 = lr_2.predict(X_test_m4)","110b912d":"X_test.head()","3efc2a32":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred_m4)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16)      \n \nplt.show()","75087998":"from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\ndf_train, df_test = train_test_split(advertising, train_size = 0.7, test_size = 0.3, random_state = 100)","b83b7837":"y_train = df_train.pop('Sales')\nX_train = df_train\nX_train  = X_train.drop('Newspaper', 1,)","6dc94ed0":"X_train.head()","169e892a":"# Add a constant\nX_train_lm = sm.add_constant(X_train )","db0ea164":"# Create a first fitted model\nlr_3 = sm.OLS(y_train, X_train_lm).fit()","0748e983":"# Check the parameters obtained\n\nlr_3.params","0d61ac49":"\n# Print a summary of the linear regression model obtained\nprint(lr_3.summary())","026fdaec":"y_train_price = lr_3.predict(X_train_lm)","5a10abdc":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","bd9d451b":"# Dividing into X_test and y_test\ny_test = df_test.pop('Sales')\nX_test = df_test","c0d0692a":"# Adding constant variable to test dataframe\nX_test_m3 = sm.add_constant(X_test)","6deda6e8":"# Creating X_test_m4 dataframe by dropping variables from X_test_m4\n\nX_test_m3 = X_test_m3.drop([\"Newspaper\" ], axis = 1)","42d725f1":"# Making predictions using the fourth model\n\ny_pred_m3 = lr_3.predict(X_test_m3)","94618d9c":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred_m4)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16)      ","3e07a488":"**Conclusion**\n\nAs seen in the pairplot and the heatmap, the variable  TV   seems to be most correlated with  Sales . So let's go ahead and perform simple linear regression using TV as our feature variable.","2098ff89":"## Step 4: Residual analysis \nTo validate assumptions of the model, and hence the reliability for inference\n\n\n#### Distribution of the error terms\nWe need to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","60e11f20":"### Rescaling the Features \n\nIn the above demonstration for Simple Linear Regression, scaling doesn't impact model. Here we can see that except for `TV`, all the columns have small integer values. So it is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients. This might become very annoying at the time of model evaluation. So it is advised to use standardization or normalization so that the units of the coefficients obtained are all on the same scale. There are two common ways of rescaling:\n\n1. Min-Max scaling \n2. Standardisation (mean-0, sigma-1) \n\nThis time, we will use MinMax scaling.","dcd6f8bf":"### Dropping the Variable and Updating the Model\n\nAs you can notice some of the NO variable have high VIF values as well as high p-values. No need to drop any variable now\n\n\nNow as you can see, the VIFs and p-values both are within an acceptable range. So we go ahead and make our predictions using this model only.","2eff54d3":"## Step 2: Visualising the Data\n\nVisualise the data using seaborn using a pairplot of all the variables present to visualise which variables are most correlated to `Sales`.","50fc324b":"**Conclusion**\n\nThe residuals are following the normally distributed with a mean 0. All good!","b0c75fa7":"## Step 9: Model Evaluation\n\nLet's now plot the graph for actual versus predicted values.","e4ffde6b":"**Observations**\n\nWe generally want a VIF that is less than 5. But we cant see any variable  that we need to drop. Therefore build second model by dropping Newspaper column as from Heatmap we found it is less correlated with `Sales` and also from p-Value is high for `Newspaper`","6d34e4f7":"## Step 8: Making Predictions Using the Final Model 3\n\nNow that we have fitted the model and checked the normality of error terms, it's time to go ahead and make predictions using the final, i.e. fourth model.","b01f4473":"### Linear Regression using `linear_model` in `sklearn`\n\nApart from `statsmodels`, there is another package namely `sklearn` that can be used to perform linear regression. Use the `linear_model` library from `sklearn` to build the model. Since, we hae already performed a train-test split, we don't need to do it again. \n\nWhen there's only a single feature, we need to add an additional column in order for the linear regression fit to be performed successfully.\n\n\n1. Create an object of LInearRegression\n2. Fit the model\n3. see params , make predictions\n4. evaluate","c0a39e56":"## Step 7: Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","0b9c4fc6":"## Step 8: Making Predictions Using the Final Model\n\nNow that we have fitted the model and checked the normality of error terms, it's time to go ahead and make predictions using the final, i.e. second model.","6057c63f":"## Building Model 2 without Scaling","130699f8":"###### Checking the R-squared on the test set","8bc36a9b":"**Final Conclusions:**\n\n1. R-sqaured and Adjusted R-squared - 0.910 and 0.909 - 90% of variance in data is explained by our model.\n\n2. F-stats  - 692.3  means Model fit is  Statistically significant and 90%\nvariance explained by model is just not by chance.\n\n\n3. p-values - p-values for all the coefficients is less than the significance level of 0.05. - means that all the predictors are statistically significant.","935622ae":"### Dividing into X and Y sets for the model building","7f493e65":"### Generic Steps in model building using `statsmodels`\n\nWe first assign the feature variable, `TV`, in this case, to the variable `X` and the response variable, `Sales`, to the variable `y`.","b5351bd8":"**Conclusion**\n\nWe are confident that the model fit isn't by chance, and has decent predictive power. The normality of residual terms allows some inference on the coefficients.\n\nAlthough, the variance of residuals increasing with X indicates that there is significant variation that this model is unable to explain.\n\n\nThe regression line is a pretty good fit to the data","37deccf6":"## Step 5: Predictions on the Test Set\n\nNow that we have fitted a regression line on your train dataset, it's time to make some predictions on the test data. For this, you first need to add a constant to the `X_test` data l did for `X_train` and then simply go on and predict the y values corresponding to `X_test` using the `predict` attribute of the fitted regression line.","13a90dbe":"As you might have noticed, `Sales` seems to the correlated to `TV` the most followed by `Radio` then `Newspaper`","9f98f31c":"#####  Looking at some key statistics from the summary\n\nThe values we are concerned with are - \n1. The coefficients and significance (p-values)\n2. R-squared\n3. F statistic and its significance\n\n##### 1. The coefficient for TV is 0.054, with a very low p value\nThe coefficient is statistically significant. So the association is not purely by chance.\n\n##### 2. R - squared is 0.816\nMeaning that 81.6% of the variance in `Sales` is explained by `TV`\n\nThis is a decent R-squared value.\n\n\n###### 3. F statistic has a very low p value (practically low)\nMeaning that the model fit is statistically significant, and the explained variance isn't purely by chance.\n\n\n---\nThe fit is significant. Let's visualize how well the model fit the data.\n\nFrom the parameters that we get, our linear regression equation becomes:\n\n$ Sales = 6.9487 + 0.0545 \\times TV $","b12eadd2":"### Simple Linear Regression \n\nFirst building a Simple linear regression model to predict Sales using an appropriate predictor variable.\n\nSteps: \n\n1. Importing data using the pandas library\n2. Understanding the structure of the data\n3. Training the Model\n4. Residual Analysis\n5. Predict & Evaluate the Model","5a66de58":"#### Applying the scaling on the test sets","aa49cf8e":"### Model1 - Lets make model with all features included","0cc3d0aa":"##  Step 3b : Building a Linear Model\n\nimport the `statsmodel.api` library to apply  linear regression on  the data.\n\n**About Statsmodel Library**\n\nBy default, the `statsmodels` library fits a line on the dataset which passes through the origin. But in order to have an intercept, we have to manually add intercept using the `add_constant` attribute of `statsmodels`. After adding the constant to `X_train` dataset, we can fit a regression line using the `OLS` (Ordinary Least Squares) attribute of `statsmodels` as shown below","27216a42":"\nWe can see that the equation of our best fitted line is:\n\n$ Sales = 4.4251 + 0.0540  \\times  TV + 0.1137  \\times  Radio   $\n","7cb41f36":"## Final Result\n\nThe equation we get is the same as what we got before!\n\n$ Sales = 6.948 + 0.0545* TV $\n\n\nSklearn linear model is useful as it is compatible with a lot of sklearn utilites (cross validation, grid search etc.)","01bc1f28":"**Observations**\n\nLooking at the p-values, it looks like some of the variables aren't really significant (in the presence of other variables).\n\nMaybe we could drop some?\n\nWe could simply drop the variable with the highest, non-significant p value. A better way would be to supplement this with the VIF information. ","c5ff114a":"#### Looking for patterns in the residuals","ed42c298":"### Checking VIF\n\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating `VIF` is:\n\n### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","5a71f358":"## Step 4: Splitting the Data into Training and Testing Sets\n\nAs you know, the first basic step for regression is performing a train-test split.","48c5a5a1":"**Observation**\n\nWe can clearly see no change in R-sq and Adjusted R-sq value\n\nBut F-Statistics is high for Model 2 - means Overall fit of model is more `Statistically significant` than the Model 1","5916e261":"## Importing Python Libraries","c2265b42":"# Multiple Linear Regression\n## Predicting Sales based on various advertisment factors \n\n \nSTEPS\n\nStep 1: Reading and Understanding the Data\n\nStep 2: Preparing the Data for modelling(Prepocessing)\n\nStep 3. Training the Model\n\nStep 4. Residual Analysis\n\nStep 5. Predict & Evaluate the Model","d6f5b3e7":"## Step 9b: Model Evaluation\n\nLet's now plot the graph for actual versus predicted values.","137a08bc":"#### Train-Test Split\n\nSplit  into training and testing sets by importing `train_test_split` from the `sklearn.model_selection` library. Generally, a good practice to keep 70% of the data as train dataset and the rest 30% as test dataset","6bad4463":"## Step 7: Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","3bfa68b8":"##### Visualizing the fit on the test set","47b25fae":"\nWe can see that the equation of our best fitted line is:\n\n$ Sales = 0.1127 + 0.6284  \\times  TV + 0.2221  \\times  Radio   $\n","05010cf6":"## Step 5: Building a linear model\n\nFit a regression line through the training data using `statsmodels`. Remember that in `statsmodels`, you need to explicitly fit a constant using `sm.add_constant(X)` because if we don't perform this step, `statsmodels` fits a regression line passing through the origin, by default.","146879f8":"## Model 2 ","bbad402b":"---\n## Step 3: Performing Simple Linear Regression\n\nEquation of linear regression<br>\n$y = c + m_1x_1 + m_2x_2 + ... + m_nx_n$\n\n-  $y$ is the response\n-  $c$ is the intercept\n-  $m_1$ is the coefficient for the first feature\n-  $m_n$ is the coefficient for the nth feature<br>\n\nIn our case:\n\n$y = c + m_1 \\times TV$\n\nThe $m$ values are called the model **coefficients** or **model parameters**.\n\n---","a7589c11":"##### Looking at the RMSE","2fd4580a":"###  Step 1: Reading and Understanding the Data","40c98999":"## Compare Simple LR Model & Multiple LR for Sales Prediction \n\n\n![image.png](attachment:image.png)"}}