{"cell_type":{"40a61f6d":"code","f1159a94":"code","b18ebd78":"code","cf74e872":"code","67767559":"code","ca5b83d7":"code","6ee56b13":"code","b1479043":"code","98382770":"code","1a4ea7d3":"code","ed1bd1c1":"code","b6da1105":"code","cc05741b":"code","59ef9b8e":"code","3211c35a":"code","ea8e5ec1":"code","2fe7c5da":"code","5937431c":"code","24b5aca7":"code","32f6bdb5":"code","3ea83eae":"code","5ed796db":"code","ad723430":"code","54bcfcb1":"code","496b9e1e":"code","38557cc2":"markdown","8ed9affb":"markdown","03004422":"markdown","327015e9":"markdown","300373f8":"markdown","7d9bb150":"markdown","02985890":"markdown","e82ac40a":"markdown","1f8ef669":"markdown","a129187a":"markdown","64e2b732":"markdown","8a306741":"markdown","2b2de19c":"markdown","ed5ec253":"markdown","356f8884":"markdown","8bb0f822":"markdown","c2295dc5":"markdown","f93529c3":"markdown","55b5880a":"markdown","85748cfe":"markdown","ce87c90a":"markdown","b04342a2":"markdown","f4647b8b":"markdown","36a90e77":"markdown","8ac9713e":"markdown"},"source":{"40a61f6d":"!pip install -U scikit-learn==0.23\n!pip install scikit-optimize==0.8.1\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import norm\nfrom skopt import gp_minimize,space\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold,cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom skopt.utils import use_named_args\nfrom hyperopt import hp,Trials,tpe,fmin\nfrom hyperopt.pyll.base import scope\nfrom hyperopt.plotting import main_plot_history\nimport optuna\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f1159a94":"dataset = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","b18ebd78":"dataset.shape","cf74e872":"dataset.head()","67767559":"dataset.isnull().sum()","ca5b83d7":"f , ax = plt.subplots()\nplt.pie(dataset[\"sex\"].value_counts(),explode=[0,.1],labels=[\"Male\",\"Female\"],startangle=90,shadow=True,autopct = '%1.1f%%')","6ee56b13":"f,ax = plt.subplots(figsize=(10,7))\nsns.countplot(\"sex\",hue=\"target\",data=dataset)\nbars = ax.patches\nhalf = int(len(bars)\/2)\nax.set_xticklabels([\"female\",\"male\"])\nax.legend([\"absence\",\"presence\"])\nfor first,second in (zip(bars[:half],bars[half:])):\n    height1= first.get_height()\n    height2= second.get_height()\n    total = height1 + height2\n    ax.text(first.get_x()+first.get_width()\/2,height1+2,'{0:.0%}'.format(height1\/total),ha=\"center\")\n    ax.text(second.get_x()+second.get_width()\/2,height2+2,'{0:.0%}'.format(height2\/total),ha=\"center\")","b1479043":"dataset.loc[:,\"age_band\"] = pd.cut(dataset.age,bins=[25,35,45,60,80])\nf,ax = plt.subplots(figsize=(10,8))\nsns.countplot(\"age_band\",hue=\"target\",data=dataset)\nbars = ax.patches\nhalf = int(len(ax.patches)\/2)\nax.legend([\"absence\",\"presence\"])\n\nfor first,second in zip(bars[:half],bars[half:]):\n    height1 =  first.get_height()\n    height2 = second.get_height()\n    total_height= height1+height2\n    ax.text(first.get_x()+first.get_width()\/2, height1+1,'{0:.0%}'.format(height1\/total_height), ha ='center')\n    ax.text(second.get_x()+second.get_width()\/2, height2+1,'{0:.0%}'.format(height2\/total_height), ha ='center')","98382770":"f,ax= plt.subplots()\nsns.countplot(\"age_band\",hue=\"sex\",data=dataset)\nax.legend([\"female\",\"male\"])\n","1a4ea7d3":"f,ax = plt.subplots(figsize=(10,7))\nsns.boxplot(\"target\",\"chol\",data=dataset)\nax.set_xticklabels([\"absence\",\"presence\"])","ed1bd1c1":"y= dataset[\"target\"]\ndataset.drop([\"target\",\"age_band\"],axis=1,inplace=True)\nX_train,X_test,y_train,y_test = train_test_split(dataset,y,test_size=0.3,random_state=42)","b6da1105":"param_space_skopt =[\n    space.Integer(3,10,name=\"max_depth\"),\n    space.Integer(50,1000,name=\"n_estimators\"),\n    space.Categorical([\"gini\",\"entropy\"],name=\"criterion\"),\n    space.Real(0.1,1,name=\"max_features\"),\n    space.Integer(2,10,name=\"min_samples_leaf\")\n]\n\nmodel = RandomForestClassifier()\n\n@use_named_args(param_space_skopt)\ndef objective_skopt(**params_skopt):\n    model.set_params(**params_skopt)\n    skf = StratifiedKFold(n_splits=5,random_state=42)\n    scores = -np.mean(cross_val_score(model,X_train,y_train,cv=skf,scoring=\"accuracy\"))\n    return scores","cc05741b":"result = gp_minimize(objective_skopt,dimensions= param_space_skopt, n_calls=25, n_random_starts=10,verbose=10,random_state=42)","59ef9b8e":"-result.fun","3211c35a":"from skopt.plots import plot_convergence\nplot_convergence(result)","ea8e5ec1":"model_skopt =RandomForestClassifier(n_estimators= result.x[1],criterion=result.x[2],max_depth=result.x[0],min_samples_leaf=result.x[4],max_features=result.x[3],random_state=42)\nmodel_skopt.fit(X_train,y_train)\ny_pred_skopt = model_skopt.predict(X_test)\nskopt_score = accuracy_score(y_test,y_pred_skopt)\nskopt_score","2fe7c5da":"param_space_hopt = {\n    \"max_depth\":scope.int(hp.quniform(\"max_depth\",3,10,1)),\n              \"n_estimators\":scope.int(hp.quniform(\"n_estimators\",50,1000,1)),\n               \"criterion\":hp.choice(\"criterion\",[\"gini\",\"entropy\"]),\n               \"max_features\":hp.uniform(\"max_features\",0.1,1),\n               \"min_samples_leaf\":scope.int(hp.quniform(\"min_samples_leaf\",2,10,1))\n              }\n\ndef objective_hopt(params_hopt):\n    model_hopt = RandomForestClassifier(**params_hopt)\n    skf = StratifiedKFold(n_splits=5,random_state=42)\n    scores = -np.mean(cross_val_score(model_hopt,X_train,y_train,cv=skf,scoring=\"accuracy\"))\n    return scores\n\ntrial_hopt = Trials()\nhyopt = fmin(fn=objective_hopt,space = param_space_hopt, algo=tpe.suggest,max_evals=25,trials=trial_hopt) ","5937431c":"hyopt","24b5aca7":"main_plot_history(trial_hopt)","32f6bdb5":"model_hopt =RandomForestClassifier(n_estimators= int(hyopt[\"n_estimators\"]),criterion=\"gini\",max_depth=int(hyopt[\"max_depth\"]),min_samples_leaf=int(hyopt[\"min_samples_leaf\"]),max_features=hyopt[\"max_features\"],random_state=42)\nmodel_hopt.fit(X_train,y_train)\ny_pred_hyopt = model_hopt.predict(X_test)\nhyopt_score = accuracy_score(y_test,y_pred_hyopt)\nhyopt_score","3ea83eae":"def optimization_optuna(trial_optuna):\n    \n    n_estimators = trial_optuna.suggest_int(\"n_estimators\",50,1000)\n    max_depth = trial_optuna.suggest_int(\"max_depth\",3,10)\n    criterion = trial_optuna.suggest_categorical(\"criterion\",[\"entropy\",\"gini\"])\n    min_samples_split = trial_optuna.suggest_int(\"min_samples_leaf\",2,10)\n    max_features = trial_optuna.suggest_uniform(\"max_features\",0.1,1)\n    \n\n    model_optuna = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,criterion=criterion,\n                                         min_samples_split=min_samples_split,max_features=max_features)\n    skf = StratifiedKFold(n_splits=5)\n    score = cross_val_score(model_optuna,X_train,y_train,cv=skf,scoring=\"accuracy\")\n    return np.mean(score)","5ed796db":"study = optuna.create_study(direction=\"maximize\")\nresult = study.optimize(optimization_optuna,n_trials=25)","ad723430":"study.best_params","54bcfcb1":"model_optuna =RandomForestClassifier(n_estimators= study.best_params[\"n_estimators\"],criterion=study.best_params[\"criterion\"],max_depth=study.best_params[\"max_depth\"],min_samples_leaf=study.best_params[\"min_samples_leaf\"],max_features=study.best_params[\"max_features\"],random_state=42)\nmodel_optuna.fit(X_train,y_train)\ny_pred_optuna = model_optuna.predict(X_test)\noptuna_score = accuracy_score(y_test,y_pred_optuna)\noptuna_score","496b9e1e":"optuna.visualization.plot_optimization_history(study)","38557cc2":"Check the best score received.","8ed9affb":"In optuna we can give the direction in which we evaluate the objective function. Earlier we used -ve since those objective functions evaluated for minimizing.\n\nHere we can define the direction and we choose maximize since it we use accuracy score. We haven't negated the score in the objective function.","03004422":"# Automated Hyperparameter Tuning and EDA\n\n This notebook would be focusing on automated hyperparameter techniques. We would be skipping Grid Search and Randomised Search as they are already commonly used in many of the notebooks\n \n### Automated Hyperparameter Tuning helps since we dont have to use time and resource intensive grid search techniques to get good results.\n\nThe three hyperparameter optimization techniques that we would use are as below:-\n\n1. Scikit-optimize\n\n2. Hyperopt\n\n3. Optuna\n\nEdit-\nDocumentation for the libraries are below:-\n\nhttps:\/\/scikit-optimize.github.io\/stable\/auto_examples\/hyperparameter-optimization.html\n\nhttp:\/\/hyperopt.github.io\/hyperopt\/\n\nhttps:\/\/optuna.readthedocs.io\/en\/stable\/\n\n\n### We would do some basic EDA before we start with the optimization. We would not be doing any feature engineering since our focus is hyperparameter tuning which gives us good results.","327015e9":"Except for the 60 to 80 age band, rest of the bands are highly skewed towards males.","300373f8":"Lets move on to modelling. First we would split our dataset in to train and test sets.","7d9bb150":"The 35 to 45 age band has the highest percentage of affected cases.","02985890":"We visualize the movement of scores according to the calls to the objective functions.","e82ac40a":"Importing the input data into a dataframe","1f8ef669":"According to our dataset females are at a higher risk of heart disease than males.","a129187a":"## Scikit-optimize Hyperparamter Tuning\nWe would use RandomForest for tuning. \n\nBelow we create the parameter space and the objective function to be minimized.","64e2b732":"Here we are testing the best parameters on our test set.","8a306741":"Let's check the best parameters","2b2de19c":"Let's check the percentage of males and females","ed5ec253":"We plot the scores against the calls to the objective function.","356f8884":"There is higher cholestrol count in cases where there in no disease, contrary to common knowledge.","8bb0f822":"We will check for any missing values. Upon checking it seem there are no missing values in this dataset.","c2295dc5":"Let's import all the necessary libraries","f93529c3":"Let's have a look at a few rows to get a sense of the data","55b5880a":"We call the gp_minimize function.","85748cfe":"## Hyperopt Hyperparameter Tuning\n\nBelow we defind the parameter space and the objective function.","ce87c90a":"We plot the results vs the calls to the objective function","b04342a2":"Let's check the best parameters.","f4647b8b":"## Optuna Hyperparamter Tuning\n\nWe define the objective function below.","36a90e77":"Checking the dimensions of the dataset imported","8ac9713e":"We evaluate the best parameters on the test data."}}