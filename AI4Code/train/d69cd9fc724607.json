{"cell_type":{"b7444388":"code","efbc2e8b":"code","a2f10932":"code","97336797":"code","65839d54":"code","f805c95a":"code","430afd03":"code","39a13161":"code","f26bb697":"code","83f10ee5":"code","dcdc59ad":"code","fcec3f6d":"code","b75486ba":"markdown","2fa5fac9":"markdown"},"source":{"b7444388":"import os\nimport shutil\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils import data\nfrom torchvision import transforms\nimport torch.nn.functional as F\nfrom sklearn.metrics import confusion_matrix","efbc2e8b":"# train folders\nos.mkdir('\/kaggle\/working\/train\/')\nfor img_class in os.listdir(\"\/kaggle\/input\/animals10\/raw-img\"):\n    os.mkdir('\/kaggle\/working\/train\/' + img_class + '\/')","a2f10932":"# test folders\nos.mkdir('\/kaggle\/working\/test\/')\nfor img_class in os.listdir(\"\/kaggle\/input\/animals10\/raw-img\"):\n    os.mkdir('\/kaggle\/working\/test\/' + img_class + '\/')","97336797":"# form train dataset\nfor img_class in tqdm(os.listdir('\/kaggle\/working\/train\/')):\n    img_ls = os.listdir('\/kaggle\/input\/animals10\/raw-img\/' + img_class)\n    for img in img_ls[:int(len(img_ls) * 0.8)]:\n\n        shutil.copy('\/kaggle\/input\/animals10\/raw-img\/' + img_class + '\/' + img, \n                    '\/kaggle\/working\/train\/' + img_class + '\/' + img)","65839d54":"# form test dataset\nfor img_class in tqdm(os.listdir('\/kaggle\/working\/test\/')):\n    img_ls = os.listdir('\/kaggle\/input\/animals10\/raw-img\/' + img_class)\n    for img in img_ls[int(len(img_ls) * 0.8):]:\n\n        shutil.copy('\/kaggle\/input\/animals10\/raw-img\/' + img_class + '\/' + img, \n                    '\/kaggle\/working\/test\/' + img_class + '\/' + img)","f805c95a":"train_data_path = \"\/kaggle\/working\/train\/\"\ntest_data_path = \"\/kaggle\/working\/test\/\"\n\ntransforms = transforms.Compose([\n    transforms.Resize((64,64)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225] )\n    ])\n\n# this function get folder with images\ntrain_data = torchvision.datasets.ImageFolder(root=train_data_path,\n                                              transform=transforms)\n\ntest_data = torchvision.datasets.ImageFolder(root=test_data_path,\n                                             transform=transforms)","430afd03":"batch_size=64\ntrain_data_loader = data.DataLoader(train_data, shuffle=True,\n                                    batch_size=batch_size)\n\ntest_data_loader  = data.DataLoader(test_data, shuffle=True, \n                                    batch_size=batch_size)","39a13161":"from torchvision import models\n# download pretrained ResNet\ntransfer_model = models.resnet50(pretrained=True)\n\n# freeze all layers in pretrained model\nfor name, param in transfer_model.named_parameters():\n    param.requires_grad = False\n    \n# replace last layer \ntransfer_model.fc = nn.Sequential(nn.Linear(transfer_model.fc.in_features,500),\nnn.ReLU(),\nnn.Dropout(), nn.Linear(500,10))\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")","f26bb697":"model = transfer_model \nmodel.to(device)\n# add optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.0003, amsgrad=True)\nepochs = 50\nloss_fn = torch.nn.CrossEntropyLoss()\nloss_lst, loss_val_lst = [], []\n\nfor epoch in range(epochs):\n    training_loss = 0.0\n    valid_loss = 0.0\n    model.train()\n    for batch in train_data_loader:\n        optimizer.zero_grad()\n        inputs, target = batch\n        inputs = inputs.to(device)\n        target = target.to(device)\n        output = model(inputs)\n        \n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n        training_loss += loss.data.item()\n    loss_lst.append(training_loss)\n\n    model.eval()\n    num_correct = 0\n    num_examples = 0\n    for batch in test_data_loader:\n        inputs, targets = batch\n        inputs = inputs.to(device)\n        output = model(inputs)\n        targets = targets.to(device)\n        loss = loss_fn(output,targets)\n        valid_loss += loss.data.item()\n        correct = torch.eq(torch.max(F.softmax(output), dim=1)[1],\n                        targets).view(-1)\n        num_correct += torch.sum(correct).item()\n        num_examples += correct.shape[0]\n    loss_val_lst.append(valid_loss)\n\n    print('Epoch: {}, Training Loss: {:.2f}, \\\n        Validation Loss: {:.2f}, \\\n        accuracy = {:.2f}'.format(epoch, training_loss, \\\n        valid_loss, num_correct \/ num_examples))","83f10ee5":"plt.plot(range(len(loss_lst)), loss_lst);\nplt.plot(range(len(loss_val_lst)), loss_val_lst);\nplt.xlabel('epochs');\nplt.ylabel('loss');","dcdc59ad":"all_output = np.array([])\nall_targets = np.array([])\n\nfor batch in tqdm(test_data_loader):\n    inputs, targets = batch\n    inputs = inputs.to(device)\n    output = model(inputs)\n    all_output = np.concatenate([all_output, output.max(dim=1).indices.cpu().numpy()])\n    all_targets = np.concatenate([all_targets, targets.numpy()])","fcec3f6d":"# plot confusion matrix\nconf_matr = confusion_matrix(all_output, \n                             all_targets)\nplt.figure(figsize = (10,7))\nsns.heatmap(conf_matr, annot=True);","b75486ba":"### Confusion matrix","2fa5fac9":"### Transfer Learning with Pytorch"}}