{"cell_type":{"d8bdba9d":"code","e309593d":"code","c1684260":"code","0eb92759":"code","0a7955a7":"code","7d070ac5":"code","a442b841":"code","9372111d":"code","74e489ec":"code","49b99d17":"code","494b3a2d":"code","abeb0fc3":"code","9ac903b7":"code","85473ce1":"code","6a319332":"code","207aac8f":"code","57d33451":"code","c437b033":"code","2193fa92":"code","7d340992":"code","c79a558a":"code","7d180144":"code","a41c6c4d":"code","fb227212":"markdown","49956522":"markdown"},"source":{"d8bdba9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as ply\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e309593d":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag","c1684260":"import pandas as pd\ndf = pd.read_json (r'\/kaggle\/input\/resume-entities-for-ner\/Entity Recognition in Resumes.json',lines=True)\ndf.to_csv (r'dataframe.csv', index = None)","0eb92759":"df","0a7955a7":"df['content'][20]","7d070ac5":"from nltk.corpus import stopwords\nimport re\nfrom nltk.stem import WordNetLemmatizer\nclean=[]","a442b841":"for i in range(0,220):\n    review = re.sub('(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)|^rt|http.+?\"', ' ', df['content'][i])\n    review = review.lower()\n    review = review.split()\n    lm= WordNetLemmatizer() \n    review = [lm.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    clean.append(review)","9372111d":"df['content'][0]","74e489ec":"clean[0]","49b99d17":"df['cleaned']=clean","494b3a2d":"df.head()","abeb0fc3":"pip install spacy","9ac903b7":"import spacy","85473ce1":"nlp = spacy.load(\"en\")","6a319332":"# def pos(sent):\n#     se=nlp(sent)\n#     for word in se:\n#         print(word,word.pos_)","207aac8f":"# pos(df['cleaned'][0])","57d33451":"from spacy.lang.en import English\n# Load English tokenizer, tagger, parser, NER and word vectors\nnlp = English()\n\ntext = df['cleaned'][100]\n\nmy_doc = nlp(text)","c437b033":"# Create list of word tokens\ntoken_list = []\nfor token in my_doc:\n    token_list.append(token.text)\nprint(token_list)","2193fa92":"# sent_list=[]\n# for token in my_doc.sents:\n#     sent_list.append(token.text)\n# print(sent_list)","7d340992":"import en_core_web_sm\n\n# load en_core_web_sm of English for vocabluary, syntax & entities\nnlp = en_core_web_sm.load()\n\n#  \"nlp\" Objectis used to create documents with linguistic annotations.\ndocs = nlp(df['cleaned'][100])","c79a558a":"for word in docs:\n    print(word.text,word.pos_)","7d180144":"from spacy import displacy\n\nnytimes= nlp(df['cleaned'][100])\n\nentities=[(i, i.label_, i.label) for i in nytimes.ents]\nentities","a41c6c4d":"displacy.render(nytimes, style = \"ent\",jupyter = True)","fb227212":"# Using NLTK Library of Part of Speech","49956522":"# Mostly words are recognised correctly."}}