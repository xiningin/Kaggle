{"cell_type":{"57851918":"code","2d4c93e8":"code","6e388957":"code","11dbdb29":"code","df18e53b":"code","bfc4867a":"code","6902c3e3":"code","aa727392":"code","a96d74de":"code","3bb37d37":"code","38a7b0c1":"code","cabc5731":"code","cf2009f8":"code","6b4a82dc":"code","d163f026":"code","df3acac2":"code","c7b07d39":"code","355a2848":"code","d4dab4e8":"code","dffd1f2e":"code","e56f0ec4":"code","ae2a3f35":"code","a15e5bdc":"code","36f2c697":"code","50dd78e6":"code","2854e0d9":"code","4ddf0f74":"code","89c3077b":"code","c0939b70":"code","44a57c3c":"code","e9a6849f":"code","6af91e9e":"code","ae62a336":"code","a02037c7":"code","648f9fde":"code","ad9cf23d":"code","1ceddf79":"code","8c37ec2e":"code","2a658139":"code","b364e644":"code","f1b6e7fd":"code","186dd6db":"code","672029e1":"code","706f5fe0":"code","cde15c7e":"code","62e0d37b":"code","a854f167":"code","4f4eb830":"code","86e35de5":"code","f5c96ce5":"markdown","505694d8":"markdown","720bdf6b":"markdown","d8bfc9f9":"markdown","ab29b2d4":"markdown","ac25f70c":"markdown","4ad05355":"markdown","a02cfb68":"markdown","98772f0f":"markdown","70628832":"markdown","536b87b6":"markdown","37a2ff79":"markdown","00c767f8":"markdown","69df2060":"markdown","fe5dce49":"markdown","178b454d":"markdown","374df9b6":"markdown","6c8e77a7":"markdown","6c015489":"markdown"},"source":{"57851918":"import pandas as pd\nimport seaborn as sb \nimport numpy as np\n\nfrom matplotlib import rcParams\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom collections import Counter\n","2d4c93e8":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","6e388957":"train.head(20)","11dbdb29":"test.head()","df18e53b":"train.shape, test.shape","bfc4867a":"train.info()","6902c3e3":"train.describe()","aa727392":"numaric_cols = train.select_dtypes(exclude=['object'])\ncategorical_cols = train.select_dtypes(include=['object'])","a96d74de":"#finding important features\ncorrelation_num = numaric_cols.corr()\ncorrelation_num.sort_values([\"SalePrice\"], ascending = False, inplace = True)\ncorrelation_num.SalePrice\n","3bb37d37":"#label encoding categorical columns\nfrom sklearn.preprocessing import LabelEncoder\ncat_le = categorical_cols.apply(LabelEncoder().fit_transform)\ncat_le['SalePrice'] = train['SalePrice']\n#finding important features from categorical values\ncorrelation_cat = cat_le.corr()\ncorrelation_cat.sort_values([\"SalePrice\"], ascending = False, inplace = True)\ncorrelation_cat.SalePrice","38a7b0c1":"cat_le.head()","cabc5731":"# dropping colmuns where number of null values is greater than 500\nnull_values = train.loc[:,train.isnull().sum()>500]\ntrain.drop(null_values,axis=1, inplace = True)","cf2009f8":"# list of less important features\nless_important = ['Id', 'MSSubClass', 'OverallCond', 'BsmtFinSF2', 'LowQualFinSF', 'BsmtHalfBath', 'KitchenAbvGr', 'EnclosedPorch',\n '3SsnPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n 'LotConfig', 'LandSlope', 'Condition2', 'BldgType', 'MasVnrType', 'ExterQual', 'BsmtQual', 'BsmtExposure','BsmtFinType1',\n 'Heating', 'HeatingQC', 'KitchenQual', 'GarageType', 'GarageFinish','SaleType']\n\n# dropping less important columns\ntrain.drop(less_important, axis = 1, inplace = True)","6b4a82dc":"#New train data\ntrain.head()","d163f026":"pd.set_option('display.max_rows', None)\npd.DataFrame(train.isna().sum())","df3acac2":"# filling null values\ntrain['LotFrontage'].fillna(train['LotFrontage'].mean(), inplace = True)\ntrain['MasVnrArea'].fillna(train['MasVnrArea'].mean(), inplace = True)\ntrain['BsmtCond'].fillna(train['BsmtCond'].mode(), inplace = True)\ntrain['BsmtFinType2'].fillna(train['BsmtFinType2'].mode(), inplace = True)\ntrain['Electrical'].fillna(train['Electrical'].mode(), inplace = True)\ntrain['GarageYrBlt'].fillna(train['GarageYrBlt'].mean(), inplace = True)\ntrain['GarageQual'].fillna(train['GarageQual'].mode(), inplace = True)\ntrain['GarageCond'].fillna(train['GarageCond'].mode(), inplace = True)","c7b07d39":"fig = sb.barplot(x = 'OverallQual',y = 'SalePrice', data = train)\nfig.set_xticklabels(labels=['Very Poor', 'Poor', 'Fair', 'Below Average', 'Average', 'Above Average',\n'Good', 'Very Good', 'Excellent', 'Very Excellent'], rotation=90);","355a2848":"fig = sb.barplot(x = 'Foundation',y = 'SalePrice', data = train)\nfig.set_xticklabels(labels=['Poured Contrete', 'Cinder Block', 'Brick & Tile', 'Wood', 'Slab', 'Stone'], rotation=90)\nplt.xlabel(\"Types of Foundation\");","d4dab4e8":"plt.scatter(train.GrLivArea, train.SalePrice)\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","dffd1f2e":"#There are some odd values that saying Ground living area is bigger than 4000. We will treat them as outliers.\n\n#removing outliers\n\ntrain = train[train.GrLivArea < 4000]","e56f0ec4":"plt.figure(figsize=(14,10))\nsb.heatmap(train.corr(),annot=True,cmap='seismic')","ae2a3f35":"sb.displot(x = \"SalePrice\", data = train, kde = True)\nskewness=str(train[\"SalePrice\"].skew())\n\nplt.legend([skewness],title=(\"skewness\"))\nplt.title(\"Before applying transform technique\")\nplt.show()","a15e5bdc":"#applying log transform\ntrain[\"SalePrice\"]=np.log(train[\"SalePrice\"])\nsb.displot(x = \"SalePrice\", data = train, kde = True)\nskewness=str(train[\"SalePrice\"].skew())\n\nplt.legend([skewness],title=(\"skewness\"))\nplt.title(\"After applying transform technique\")\nplt.show()","36f2c697":"sb.displot(x = 'OverallQual', data = train, kde = True)\nskewness=str(train[\"OverallQual\"].skew())\nkurtosis=str(train[\"OverallQual\"].kurt())\nplt.legend([skewness,kurtosis],title=(\"skewness and kurtosis\"))\nplt.title(\"Before applying transform technique\")\nplt.show()","50dd78e6":"#applying log transform\ntrain['OverallQual']=np.log(train['OverallQual'])\nsb.displot(x = 'OverallQual', data = train, kde = True)\nskewness=str(train['OverallQual'].skew())\nkurtosis=str(train['OverallQual'].kurt())\nplt.legend([skewness,kurtosis],title=(\"skewness and kurtosis\"))\nplt.title(\"After applying transform technique\")\nplt.show()","2854e0d9":"sb.displot(x = 'GrLivArea', data = train, kde = True)\nskewness=str(train[\"GrLivArea\"].skew())\nkurtosis=str(train[\"GrLivArea\"].kurt())\nplt.legend([skewness,kurtosis],title=(\"skewness and kurtosis\"))\nplt.title(\"Before applying transform technique\")\nplt.show()","4ddf0f74":"train['GrLivArea']=np.log(train['GrLivArea'])\nsb.displot(x = 'GrLivArea', data = train, kde = True)\nskewness=str(train[\"GrLivArea\"].skew())\nkurtosis=str(train[\"GrLivArea\"].kurt())\nplt.legend([skewness,kurtosis],title=(\"skewness and kurtosis\"))\nplt.title(\"After applying transform technique\")\nplt.show()","89c3077b":"x = train.drop(['SalePrice'], axis = 1)\ny = train['SalePrice']","c0939b70":"#labelencoding categorical variables from x\nfrom sklearn.preprocessing import LabelEncoder\nx = x.apply(LabelEncoder().fit_transform)","44a57c3c":"x.head()","e9a6849f":"y.head()","6af91e9e":"x.shape,y.shape","ae62a336":"#splitting the dataset into train and test set.\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state=31 )","a02037c7":"len(x_train), len(x_test), len(y_train), len(y_test)","648f9fde":"#Feature scaling\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nx_train = scale.fit_transform(x_train)\nx_test = scale.transform(x_test)","ad9cf23d":"#model evaluation function\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\n\ndef model_evaluate(y_true, y_pred):\n    r2 = r2_score(y_true, y_pred)\n    mae = mean_absolute_error(y_true, y_pred)\n    return r2, mae","1ceddf79":"from sklearn import linear_model\nlasso_reg = linear_model.Lasso(alpha=0.1, random_state = 32).fit(x_train, y_train)\ncv_r2 = cross_val_score(lasso_reg, x_train, y_train, cv = 10)\ny_preds = lasso_reg.predict(x_test)\ncv_r2 = np.mean(cv_r2)\nprint(\"Cross val score: \" + str(cv_r2))\nr2, mae = model_evaluate(y_test, y_preds)\nprint(\"R^2 score: \" + str(r2))\nprint(\"Mean Absolute Erro: \" + str(mae))","8c37ec2e":"from sklearn import linear_model\nridge_reg = linear_model.Ridge(alpha=.5).fit(x_train, y_train)\ncv_r2 = cross_val_score(ridge_reg, x_train, y_train, cv = 10)\ny_preds = ridge_reg.predict(x_test)\ncv_r2 = np.mean(cv_r2)\nprint(\"Cross val score: \" + str(cv_r2))\nr2, mae = model_evaluate(y_test, y_preds)\nprint(\"R^2 score: \" + str(r2))\nprint(\"Mean Absolute Erro: \" + str(mae))","2a658139":"from sklearn.ensemble import RandomForestRegressor\nrf_reg = RandomForestRegressor(n_estimators=1000).fit(x_train, y_train)\ncv_r2 = cross_val_score(rf_reg, x_train, y_train, cv = 10)\ny_preds = rf_reg.predict(x_test)\ncv_r2 = np.mean(cv_r2)\nprint(\"Cross val score: \" + str(cv_r2))\nr2, mae = model_evaluate(y_test, y_preds)\nprint(\"R^2 score: \" + str(r2))\nprint(\"Mean Absolute Erro: \" + str(mae))","b364e644":"from sklearn.ensemble import GradientBoostingRegressor\ngbr_reg = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01, max_depth=1, random_state=31).fit(x_train, y_train)\ncv_r2 = cross_val_score(gbr_reg, x_train, y_train, cv = 10)\ny_preds = gbr_reg.predict(x_test)\ncv_r2 = np.mean(cv_r2)\nprint(\"Cross val score: \" + str(cv_r2))\nr2, mae = model_evaluate(y_test, y_preds)\nprint(\"R^2 score: \" + str(r2))\nprint(\"Mean Absolute Erro: \" + str(mae))","f1b6e7fd":"import xgboost as XGB\nxgb = XGB.XGBRegressor(learning_rate=0.01, n_estimators=1000, objective='reg:squarederror', random_state = 31).fit(x_train, y_train)\ncv_r2 = cross_val_score(xgb, x_train, y_train, cv = 10)\ny_preds = xgb.predict(x_test)\ncv_r2 = np.mean(cv_r2)\nprint(\"Cross val score: \" + str(cv_r2))\nr2, mae = model_evaluate(y_test, y_preds)\nprint(\"R^2 score: \" + str(r2))\nprint(\"Mean Absolute Erro: \" + str(mae))","186dd6db":"test.head()","672029e1":"#drop less important featues and columns with null values > 500\n\nnull_values = test.loc[:, test.isnull().sum()>500]\ntest.drop(null_values, axis=1, inplace = True)\n\ntest.drop(less_important, axis = 1, inplace = True)","706f5fe0":"#checking for null values in test set\npd.DataFrame(test.isna().sum())","cde15c7e":"# taking care of null values\ntest['LotFrontage'].fillna(test['LotFrontage'].mean(), inplace = True)\ntest['MasVnrArea'].fillna(0 , inplace = True)\ntest['BsmtCond'].fillna('NA' , inplace = True)\ntest['BsmtFinType2'].fillna('NA' , inplace = True)\ntest['Electrical'].fillna('SBrkr' , inplace = True)\ntest['GarageYrBlt'].fillna(0 , inplace = True)\ntest['GarageQual'].fillna('NA' , inplace = True)\ntest['GarageCond'].fillna('NA' , inplace = True)\ntest['Exterior1st'].fillna('VinylSd' , inplace = True)\ntest['Exterior2nd'].fillna('VinylSd' , inplace = True)\ntest['BsmtFinSF1'].fillna(0 , inplace = True)\ntest['BsmtUnfSF'].fillna(0 , inplace = True)\ntest['TotalBsmtSF'].fillna(0 , inplace = True)\ntest['BsmtFullBath'].fillna(0 , inplace = True)\ntest['Functional'].fillna('Typ' , inplace = True)\ntest['GarageCars'].fillna(0 , inplace = True)\ntest['GarageArea'].fillna(0, inplace = True)","62e0d37b":"#labelencoding test data\ntest = test.apply(LabelEncoder().fit_transform)","a854f167":"#scaling test data\ntest = scale.transform(test)","4f4eb830":"#making prediction\npredictions = rf_reg.predict(test)","86e35de5":"predictions","f5c96ce5":"# Feature Engineering","505694d8":"# Preparing test set","720bdf6b":"# EDA\n\nOverallQual refers overall quality of the house. This is a important feature. SalePrice largly depends on it. Because if the house quality is Very Excellent than it is more likely to be sold with high price. Let's analyse this column.","d8bfc9f9":"# Feature Selection\n\nSeperating numaric columns and categorical columns","ab29b2d4":"# **MODELING**\n\nIt's time to create our independent and dependent matrix of feature.","ac25f70c":"# Observation \n* Total 1460 entries in our train data\n\n* Total 81 columns\n\n* With float data 3 column, Int data 35 column and Object data 43 column\n\n* There are many null value in our dataset","4ad05355":"#Imputing numerical data with mean value and categorical data values with mode value","a02cfb68":"Foundation is another important feature. It represent how strong a bulding can be. Buildings life depends on it. So, this column worth analysing.","98772f0f":"#  Import Necessary Tools\n\nThe following libraries are used here:\n\npandas: The Python Data Analysis Library is used for storing the data in dataframes and manipulation.\n\nnumpy: Python scientific computing library.\n\nmatplotlib: Python plotting library.\n\nseaborn: Statistical data visualization based on matplotlib.\n","70628832":"This shows some descriptive statistics on the\ndata set. Notice, it only shows the statistics on\nthe numerical columns. From here you can see\nthe following statistics:\n\n* Row count, which aligns to what the shape attribute showed us.\n* The mean, or average.\n* The standard deviation. or how spread out the data is.\n* The minimum and maximumn value of each colimns.\n* The number of items that fall within the first, second, and third percentiles.\n","536b87b6":"Reducing Skewness and kurtosis from data(x)","37a2ff79":"Let's check for null values. To view the full dataset first we need to set display.max_rows","00c767f8":"Let's remove features with less importance. Less important features was selected by correlation score","69df2060":"Plot visualizing GrLivArea and SalePrice","fe5dce49":"Used StandardScaler function in order to\nnormalize\/standardize the independent\nvaribles(i.e..'x')","178b454d":"Reducing Skewness.\nNormally distributed features are an assumption in Statistical algorithms. Deep learning & Regression-type algorithms also benefit from normally distributed data. Transformation is required to treat the skewed features and make them normally distributed. Right skewed features can be transformed to normality with Square Root\/ Cube Root\/ Logarithm transformation.","374df9b6":"# Data Preprocessing(Train set)","6c8e77a7":"Let's check for null values.\n","6c015489":"#  Lasso"}}