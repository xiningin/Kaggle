{"cell_type":{"b3de0b19":"code","786ac629":"code","d8d469bd":"code","64ad5516":"code","c28a4cd9":"code","b97090c3":"code","20830a3b":"code","bbd44b2d":"code","f2cede6d":"code","6fb15fff":"code","2e71940f":"code","db1a8d41":"code","0f13436b":"code","78f3da31":"code","1f93da02":"code","b40069f1":"code","ff0e5b3d":"markdown","fccd26af":"markdown","781d11d8":"markdown","dd5d6c33":"markdown","83f6ca6c":"markdown","57f6743e":"markdown","1ce74dc3":"markdown","940ab502":"markdown","6cd43675":"markdown"},"source":{"b3de0b19":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","786ac629":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes = True)\n%matplotlib inline\n\nimport nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords","d8d469bd":"data = pd.read_csv('\/kaggle\/input\/tweet-sentiment-and-emotion-analysis\/all_tweets.csv')\ndata.head()","64ad5516":"data.info()","c28a4cd9":"sns.countplot(x = 'label',data = data)","b97090c3":"sns.countplot(x = 'sentiment',data = data)","20830a3b":"#Data have some inpredictiable and useless character and also there are puncutations are present so will remove using Natural language preprocessing library and also we will remove stopwords.\nlem=WordNetLemmatizer()\n\ncorpus = []\nfor i in range(0, len(data)):\n    review = re.sub('[^a-zA-Z]', ' ', data['text'][i]) #Instead of Punctuations it just replacing with \" \".\n    review = review.lower() #Lowering each sentence\n    review = review.split() #After lowering and replacing puntucation will split the words\n    review = [lem.lemmatize(word) for word in review if not word in stopwords.words('english')] # Here we will drop words which are present in stopwords like ['can't,they,we,here,there].\n    review = ' '.join(review)\n    corpus.append(review) #All sentences are collect in corpus.\n","bbd44b2d":"corpus[7]","f2cede6d":"# Creating the TF-IDF model\nfrom sklearn.feature_extraction.text import TfidfVectorizer #Using TFIDF vectorizer we will convert document in vector form.\ncv = TfidfVectorizer(max_features = 5000,ngram_range = (1,3))\nx= cv.fit_transform(corpus).toarray()\nx","6fb15fff":"y = data.loc[:,['sentiment']]\ny","2e71940f":"# Train Test Split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 0)\nprint('Training data:{}'.format(X_train.shape))\nprint('Test data:{}'.format(X_test.shape))","db1a8d41":"cv.get_feature_names()[:20]","0f13436b":"cv.get_params()\n","78f3da31":"count_df = pd.DataFrame(X_train, columns=cv.get_feature_names())\n\ncount_df","1f93da02":"from sklearn.naive_bayes import MultinomialNB\nclf=MultinomialNB()\nclf.fit(X_train,y_train)\ny_predicted = clf.predict(X_test)\nscore = clf.score(X_test,y_test)\nprint(score)\n","b40069f1":"from sklearn.metrics import confusion_matrix\n\n#Confusion Matrix\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_predicted)\nnp.set_printoptions(precision=2)\ncnf_matrix","ff0e5b3d":"<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 style='background:#8bb79c; border:0; color:black'><center>INDEX<\/center><\/h1>","fccd26af":"- IMPORT LIBRARIES\n- IMPORT DATA\n- DATA VISUALIZATION\n- TEXT CLEANING\n- VALIDATION TECHNIQUE\n- BUILD MODEL","781d11d8":"<a id=\"6\"><\/a>\n<h1 style='background:#8bb79c; border:0; color:black'><center>BUILD MODEL<\/center><\/h1> ","dd5d6c33":"<a id=\"2\"><\/a>\n<h1 style='background:#8bb79c; border:0; color:black'><center>IMPORT DATA<\/center><\/h1> ","83f6ca6c":"<a id=\"3\"><\/a>\n<h1 style='background:#8bb79c; border:0; color:black'><center>DATA VISUALIZATION<\/center><\/h1> ","57f6743e":"<a id=\"1\"><\/a>\n<h1 style='background:#8bb79c; border:0; color:black'><center>IMPORT LIBRARIES<\/center><\/h1> ","1ce74dc3":"<a id=\"5\"><\/a>\n<h1 style='background:#8bb79c; border:0; color:black'><center>VALIDATION METHOD<\/center><\/h1> ","940ab502":"<a id=\"7\"><\/a>\n<h1 style='background:#8bb79c; border:0; color:black'><center>CONFUSION MATRIX<\/center><\/h1> ","6cd43675":"<a id=\"4\"><\/a>\n<h1 style='background:#8bb79c; border:0; color:black'><center>TEXT CLEANING<\/center><\/h1> "}}