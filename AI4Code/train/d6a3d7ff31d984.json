{"cell_type":{"d415e54f":"code","48c267f6":"code","1136cb3b":"code","a1a656bb":"code","aafc2f56":"code","8db7e816":"code","51b79d70":"code","a7dbf4f4":"code","ead520e4":"code","f553dc27":"code","7aca90cc":"code","d9e8f3cb":"code","d80ad9aa":"code","1a3ef4b6":"code","126e6912":"code","2e3df238":"code","dbe43576":"code","7223acf2":"code","c0a31207":"code","a67d89bf":"code","fb6b7f68":"code","a322b567":"code","374e1691":"code","f9acc9a9":"code","a44e3a46":"code","b5c0d81f":"markdown","bafe9469":"markdown","f9724fe3":"markdown","b8441e86":"markdown","a41fae0f":"markdown","367aa9a3":"markdown","77c0c017":"markdown","e19dc828":"markdown","03740566":"markdown","882a95b8":"markdown","b4ceaf28":"markdown","7a89b0ac":"markdown","d414416c":"markdown","5a35b696":"markdown","c742731d":"markdown","9763f3e0":"markdown","8c7c44dc":"markdown","f79315d6":"markdown","2be52601":"markdown","83506bd2":"markdown","b14d5c7e":"markdown","c192fab3":"markdown","aeee3b76":"markdown"},"source":{"d415e54f":"# kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.metrics import confusion_matrix,precision_score,recall_score,precision_recall_curve,auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport xgboost as xgb\nimport shap","48c267f6":"df = pd.read_csv('\/kaggle\/input\/lending-club\/accepted_2007_to_2018q4.csv\/accepted_2007_to_2018Q4.csv')\n#rej_df = pd.read_csv('\/kaggle\/input\/lending-club\/rejected_2007_to_2018q4.csv\/rejected_2007_to_2018Q4.csv')","1136cb3b":"with pd.option_context('display.max_columns',None):\n    display(df.head())\ndf.info()","a1a656bb":"df = df.drop(['id','policy_code','out_prncp','out_prncp_inv','url','pymnt_plan','hardship_flag','grade'], axis=1)","aafc2f56":"((df.isnull().sum())\/len(df)*100).plot.bar(title='Percentage of missing values per column')\n\nMAX_COL_PERC = 0.02\n\nperc = df.isnull().sum() \/ len(df)     # .sort_values(ascending=False)\nna_cols = perc.iloc[np.where(np.array(perc)>MAX_COL_PERC)].index\nprint(len(na_cols), \"columns dropped.\")\n# Uncomment to show which columns are dropped\n# with pd.option_context('display.max_rows',None):\n#     display(df.loc[:,na_cols].describe().transpose())\ndf = df.drop(na_cols, axis=1)","8db7e816":"sn.countplot(y='loan_status', data=df)\ndf = df[(df['loan_status'] == 'Fully Paid') | (df['loan_status'] == 'Charged Off')]\ndf['label'] = df.apply(lambda r: 1 if r['loan_status'] == 'Fully Paid' else 0, axis=1)\ndf = df.drop('loan_status', axis=1)","51b79d70":"with pd.option_context('display.max_rows',None):\n    display(df.describe(include=np.object).transpose())","a7dbf4f4":"date_fields = ['issue_d','earliest_cr_line','last_pymnt_d','last_credit_pull_d']\nfor col in date_fields:  \n    df[col] = pd.to_datetime(df[col]) \n    df[col + '_month'] = df[col].dt.month\n    df[col + '_year'] = df[col].dt.year\ndf = df.drop(date_fields, axis=1)","ead520e4":"MIN_DUMMY_PERC = 0.01\n\nvc = df['title'].value_counts()\ntitles = vc.iloc[np.where(np.array(vc)>MIN_DUMMY_PERC*len(df))].index\ndf['title'] = df.apply(lambda r: r['title'] if r['title'] in titles else 'Other title',axis=1)","f553dc27":"cat = df.select_dtypes(include=['object']).columns\ndf[cat] = df[cat].fillna(value='Missing')\ndf_cat = pd.get_dummies(data=df[cat])","7aca90cc":"print(\"Total # categorical columns: \", len(df_cat.columns))\ndf_cat = df_cat.drop([col for col, cnt in df_cat.sum().iteritems() if cnt < MIN_DUMMY_PERC*len(df_cat)], axis=1)\nprint(\"Reduced # categorical columns: \", len(df_cat.columns))","d9e8f3cb":"with pd.option_context('display.max_rows',None):\n    display(df.describe().transpose().apply(lambda s: s.apply('{0:.5f}'.format)))","d80ad9aa":"df = df.drop(cat,axis=1)\ndf = pd.concat([df,df_cat], axis=1)","1a3ef4b6":"with pd.option_context('display.max_columns',None):\n    display(df.head())\ndf.info()","126e6912":"corrs = df.corr()['label'].sort_values(ascending=True).drop('label')\npd.concat([corrs.iloc[:7],corrs.iloc[-7:]],axis=0).plot.bar()","2e3df238":"X = df.drop('label',axis=1).values\ny = df['label'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","dbe43576":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","7223acf2":"X_train = np.nan_to_num(X_train)\nX_test = np.nan_to_num(X_test)","c0a31207":"model = xgb.XGBClassifier()\nmodel.fit(X_train, y_train)","a67d89bf":"y_prob = model.predict_proba(X_test)[:,1]\ny_pred = model.predict(X_test)","fb6b7f68":"def evaluate_results(y_real,y_pred,y_prob=None):\n    real = 1-y_real\n    pred = 1-y_pred\n    print(\"Precision: \" + str(precision_score(real,pred)))\n    print(\"Recall: \" + str(recall_score(real,pred)))\n    if y_prob is not None:\n        prob = 1-y_prob\n        pr = precision_recall_curve(real, prob)\n        plt.plot(pr[1],pr[0])\n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        print(\"PR AUC: \" + str(auc(pr[1],pr[0])))\n    #conf = confusion_matrix(real,pred)\n    #confn = np.array([['TN','FP'],['FN','TP']])\n    #print(confn[0], conf[0])\n    #print(confn[1], conf[1])","a322b567":"print(\"Evaluation XGBoost - Test\")\nevaluate_results(y_test,y_pred,y_prob=y_prob)","374e1691":"import shap\nexplainer = shap.Explainer(model,feature_names=df.drop('label',axis=1).columns)\nshap_values = explainer(X_train)","f9acc9a9":"shap.plots.waterfall(shap_values[0])\nshap.plots.waterfall(shap_values[29])","a44e3a46":"shap.summary_plot(shap_values, X_train, max_display=10)","b5c0d81f":"The title field contains too many different values. Only keep the most frequent ones and set the rest to `'Other title'`. We only care about categories that occur in more than `MIN_DUMMY_PERC`% of the data.","bafe9469":"## 5. Interpretation\n---\nWe use the shap library to interpret the model and find the most important features for the decision.","f9724fe3":"The shawn values for all samples combined are visualized in the summary below. `total_rec_prncp`, `funded_amnt`and `recoveries` are clearly the most predictive indicators.","b8441e86":"Transform the date fields to numerical to capture ordering in time.","a41fae0f":"Scale the data in the range [0,1].","367aa9a3":"## 1. Import\n---\nImport the used libraries and the dataset of accepted loans.","77c0c017":"Remaining `null` values are set to 0. Ideally, extra columns would be added to indicate these values.","e19dc828":"The dataset now contains 1345310 rows and 128 columns and is ready for training a classifier.","03740566":"Calculate the output on the test set.","882a95b8":"Remove least frequent categories (similar to `title`).","b4ceaf28":"## 2. Data Cleaning\n---\nColumns with all identical values and unique identifiers (`id`, `url`) are dropped. `grade` is already captured in `subgrade`.","7a89b0ac":"The labels are constructed. We are only interested in loans that are overdue (`'Charged Off'`) and loans that are fully paid (`'Fully Paid'`). The labels are set to 0 and 1 respectively, and other rows are removed.","d414416c":"# Overdue loan prediction for credit risk\nFrom https:\/\/www.kaggle.com\/wordsforthewise\/lending-club\/tasks?taskId=2302","5a35b696":"Train an XGBoost classifier.","c742731d":"The numerical attributes are shown below.","9763f3e0":"Remove the labels and split into train and test data for evaluation.","8c7c44dc":"Transform the categorical variables into dummy-variables using one-hot. Missing values are given a separate value.","f79315d6":"## 4. Modelling\n---\nFirst, we look at all the correlations of the features with the output label in order to gain some intuition. The features with the highest correlation are plotted below.","2be52601":"Evaluate the results. Precision and recall are the most interesting metrics as we are dealing with class imbalance. The overdue loans are considered the positive examples.","83506bd2":"Below, the effect of the features for one charged off and one fully paid sample are visualized. Features that contribute to a higher probability of fully paying the loan are red, features that result in a lower probability are blue.","b14d5c7e":"## 3. Preprocessing\n---\nThe categorical attributes are shown below.","c192fab3":"Columns with too many `null` values are dropped. To simplify this classification task, columns need to be 98% full such that only about 50 columns are kept.","aeee3b76":"Combine numerical and categorical attributes again."}}