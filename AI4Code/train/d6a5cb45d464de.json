{"cell_type":{"c6b4f481":"code","36130a5a":"code","0218f550":"code","cb65e85d":"code","7fb74c4e":"code","403d8011":"code","92096a71":"code","ba2bc812":"code","bac06f1d":"code","6d1accc9":"code","2d7d7dbb":"code","acbef622":"code","4c192da9":"code","83103b08":"code","718dac94":"code","ebf0eaee":"code","9c36efb2":"code","0be62b9f":"code","e38071a5":"code","f746b2dc":"code","7638b0a7":"code","22e94b5f":"code","0276e90a":"code","f9334c8b":"code","46ff93d0":"markdown","68a5076b":"markdown","84e4ec83":"markdown","cfd4fd05":"markdown","001b4c3a":"markdown","52aa8a3d":"markdown","18be6b03":"markdown","76c90703":"markdown","bcb84407":"markdown","13764b1b":"markdown","aa63740a":"markdown","7b43ebca":"markdown","cca41293":"markdown","b7536e3a":"markdown","d2ed3cc5":"markdown","6c9d5caf":"markdown","fffa989a":"markdown","aba59820":"markdown","51124dd6":"markdown","cbc5a391":"markdown","0c168a06":"markdown","8ff626d9":"markdown"},"source":{"c6b4f481":"from __future__ import print_function\nfrom __future__ import division\n\nimport math\nimport os\nimport random\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport plotly.express as px\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport copy\nimport shutil\n\n!pip install torchsummary\nfrom timeit import default_timer as timer\nfrom torch import nn, optim, utils\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import random_split\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import ConcatDataset\nfrom torchsummary import summary\nfrom torch.utils.tensorboard import SummaryWriter\nfrom typing import Tuple, Callable, Dict\n\nfrom sklearn.model_selection import GridSearchCV","36130a5a":"!cp -R \"..\/input\/body-cavity-fluid-cytology-images\/body_fluid_cytology\" \"..\/body_fluid_cytology\"\npath_dataset = \"..\/body_fluid_cytology\"\npath_dataset_train = \"..\/train\"\npath_dataset_test = \"..\/test\"\npath_dataset_validation = \"..\/validation\"","0218f550":"if (os.path.isdir(path_dataset+\"\/benign\") and \n    os.path.isdir(path_dataset+\"\/malignant\")):\n  !mkdir \"..\/train\"\n  !mkdir \"..\/test\"\n  !mkdir \"..\/validation\"\n  !mkdir \"..\/train\/benign\"\n  !mkdir \"..\/train\/malignant\"\n  !mkdir \"..\/validation\/benign\"\n  !mkdir \"..\/validation\/malignant\"\n  !mkdir \"..\/test\/benign\"\n  !mkdir \"..\/test\/malignant\"\n  cont = 0\n  while cont < 45:  # 28% of 160\n      img = random.choice(os.listdir(path_dataset+\"\/benign\"))\n      shutil.move(path_dataset+\"\/benign\/\"+img, \"..\/train\/benign\/\"+img)\n      cont += 1\n  cont = 0\n  while cont < 150:  # 28% of 533\n      img = random.choice(os.listdir(path_dataset+\"\/malignant\"))\n      shutil.move(path_dataset+\"\/malignant\/\"+img, \"..\/train\/malignant\/\"+img)\n      cont += 1\n\n  cont = 0\n  while cont < 28:  # 17.3% of 160\n      img = random.choice(os.listdir(path_dataset+\"\/benign\"))\n      shutil.move(path_dataset+\"\/benign\/\"+img, \"..\/validation\/benign\/\"+img)\n      cont += 1\n  cont = 0\n  while cont < 92:  # 17.3% of 533\n      img = random.choice(os.listdir(path_dataset+\"\/malignant\"))\n      shutil.move(path_dataset+\"\/malignant\/\"+img, \"..\/validation\/malignant\/\"+img)\n      cont += 1\n\n  cont = 0\n  while cont < 87:\n      img = random.choice(os.listdir(path_dataset+\"\/benign\"))\n      shutil.move(path_dataset+\"\/benign\/\"+img, \"..\/test\/benign\/\"+img)\n      cont += 1\n  cont = 0\n  while cont < 291:\n      img = random.choice(os.listdir(path_dataset+\"\/malignant\"))\n      shutil.move(path_dataset+\"\/malignant\/\"+img, \"..\/test\/malignant\/\"+img)\n      cont += 1","cb65e85d":"def get_mean_and_std(dataset: torchvision.datasets.ImageFolder) -> Tuple[float, float]:\n    \"\"\"Compute mean and std for the dataset.\n\n    Args:\n        dataset: the dataset. \n\n    Returns:\n        The mean and the std on each channels computed over the dataset.\n    \"\"\"\n    dataset_loader = DataLoader(\n        dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=2\n    )\n    sums = torch.zeros(3)\n    sums_of_square = torch.zeros(3)\n    count = 0\n\n    for images, t in dataset_loader:\n        b, a, h, w = images.shape\n        num_pix_in_batch = b * h * w\n        sums += torch.sum(images, dim=[0, 2, 3])\n        sums_of_square += torch.sum(images ** 2, dim=[0, 2, 3])\n        count += num_pix_in_batch\n\n    mean = sums \/ count\n    var = (sums_of_square \/ count) - (mean ** 2)\n    std = torch.sqrt(var)\n    \n    return mean, std","7fb74c4e":"transform = transforms.Compose([transforms.ToTensor(), \n                                transforms.Resize((192, 256))])  # Resize required because at least an image has size different from 192x256\ndata_train = torchvision.datasets.ImageFolder(path_dataset_train, \n                                                   transform=transform)\ndata_val = torchvision.datasets.ImageFolder(path_dataset_train, \n                                                   transform=transform)\ndataset_for_std = ConcatDataset([data_train, data_val])\n\nmean_image_net, std_image_net = get_mean_and_std(dataset_for_std)\nprint(mean_image_net)\nprint(std_image_net)","403d8011":"def show_grid(dataset: torchvision.datasets.ImageFolder, \n              process: Callable = None) -> None:\n    \"\"\"Shows a grid with random images taken from the dataset.\n\n    Args:\n        dataset: the dataset containing the images.\n        process: a function to apply on the images before showing them.\n    \"\"\"\n    fig = plt.figure(figsize=(15, 5))\n    indices_random = np.random.randint(10, size=10, high=len(dataset))\n\n    for count, idx in enumerate(indices_random):\n        fig.add_subplot(2, 5, count + 1)\n        title = 1\n        plt.title(title)\n        image_processed = process(dataset[idx][0]) if process is not None else dataset[idx][0]\n        plt.imshow(transforms.ToPILImage()(image_processed))\n        plt.axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()","92096a71":"show_grid(dataset_for_std)","ba2bc812":"size_image = (192, 256)\nnum_workers = 2\nsize_batch_train = 8\nsize_batch_val = 64\n\nnormalize = transforms.Normalize(mean_image_net, std_image_net)\n\ntransform = transforms.Compose([transforms.ToTensor(), \n                                transforms.Resize((192, 256)), normalize, \n                                transforms.RandomHorizontalFlip(p=0.5), \n                                transforms.RandomVerticalFlip(p=0.5),\n                                transforms.GaussianBlur(3, sigma=(0.1, 1.0))])\ntransform_test = transforms.Compose([transforms.ToTensor(), \n                                transforms.Resize((192, 256)), normalize])\n    \ndata_train = torchvision.datasets.ImageFolder(path_dataset_train, \n                                              transform=transform)\ndata_val = torchvision.datasets.ImageFolder(path_dataset_validation, \n                                              transform=transform)\ndata_test = torchvision.datasets.ImageFolder(path_dataset_test, \n                                             transform=transform_test)\n\ntrain_loader = DataLoader(data_train, batch_size=size_batch_train, \n                          num_workers=num_workers, pin_memory=True, \n                          shuffle=True)\nvalid_loader = DataLoader(data_val, batch_size=size_batch_val, \n                          num_workers=num_workers, pin_memory=True, \n                          shuffle=True)\ntest_loader = DataLoader(data_test, batch_size=size_batch_val, \n                         num_workers=num_workers, pin_memory=True, \n                         shuffle=False)\n\nnum_classes = len(data_train.classes)\n\nprint(f'Samples -> Train = {len(data_train)} - Val = {len(data_val)} - Test = {len(data_test)}'\n      f' - Classes = {num_classes}')","bac06f1d":"def plot_histograms(dataset_train: torchvision.datasets.ImageFolder,\n                    dataset_valid: torchvision.datasets.ImageFolder,\n                    dataset_test: torchvision.datasets.ImageFolder,\n                    title: str) -> None:\n    \"\"\"Plot histograms with train and test or validation data distributions.\n\n    Args:\n        dataset_train: the train dataset.\n        dataset_valid: the validation dataset.\n        dataset_test: the test or validation dataset.\n        title: the title of the plot.\n    \"\"\"\n    classes = len(dataset_train.classes)\n    bins = np.linspace(0, classes, classes + 1, dtype=np.int)\n    plt.title(title)\n    plt.hist([dataset_train.targets, dataset_valid.targets, dataset_test.targets], bins=bins, \n             label=['Train', 'Valid', 'Test'])\n    plt.legend(loc='upper right')\n    plt.show()","6d1accc9":"plot_histograms(data_train, data_val, data_test, \"Data Distribution\")","2d7d7dbb":"class FirstCNN(nn.Module):\n    def __init__(self, num_classes: int):\n        super(FirstCNN, self).__init__()\n        \"\"\"First Convolution Neural Network.\n\n        Args:\n            num_classes: the number of classes.\n        \"\"\"\n        super().__init__()                        \n        self.conv1 = nn.Conv2d(in_channels=3, \n                               out_channels=6, \n                               kernel_size=5, \n                               stride=1)\n        self.conv2 = nn.Conv2d(in_channels=6, \n                               out_channels=16, \n                               kernel_size=3, \n                               stride=1)\n        self.conv3 = nn.Conv2d(in_channels=16, \n                               out_channels=120, \n                               kernel_size=3, \n                               stride=1)\n        self.conv4 = nn.Conv2d(in_channels=120, \n                               out_channels=256, \n                               kernel_size=3, \n                               stride=1)\n        \n        self.max_pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(35840, 1024)\n        self.fc2 = nn.Linear(1024, 84)\n        self.relu = nn.ReLU()\n        self.batchNorm_conv1 = nn.BatchNorm2d(6)\n        self.batchNorm_conv2 = nn.BatchNorm2d(16)\n        self.batchNorm_conv3 = nn.BatchNorm2d(120)\n        self.batchNorm_conv4 = nn.BatchNorm2d(256)\n        self.batchNorm_fc1 = nn.BatchNorm1d(1024)\n        self.batchNorm_fc2 = nn.BatchNorm1d(84)\n        self.output = nn.Linear(84, num_classes)\n        self.dropout = nn.Dropout(p=0.1)\n        self.dropout2d = nn.Dropout2d(p=0.1)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.batchNorm_conv1(x)\n        x = self.max_pool(x)\n        x = self.relu(x)\n        x = self.dropout2d(x)\n        \n        x = self.conv2(x) \n        x = self.batchNorm_conv2(x)    \n        x = self.max_pool(x)\n        x = self.relu(x)\n\n        x = self.conv3(x)\n        x = self.batchNorm_conv3(x)\n        x = self.max_pool(x)\n        x = self.relu(x)\n        x = self.dropout2d(x)\n\n        x = self.conv4(x)\n        x = self.batchNorm_conv4(x)\n        x = self.max_pool(x)\n        x = self.relu(x)\n\n        x =  torch.flatten(x, start_dim=1)\n\n        x = self.fc1(x)\n        x = self.batchNorm_fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.batchNorm_fc2(x)\n        x = self.relu(x)\n        \n        return self.output(x)  ","acbef622":"print(summary(FirstCNN(num_classes=len(data_train.classes)), input_size=(3, size_image[0], size_image[1]), batch_size=size_batch_train, device=\"cpu\"))","4c192da9":"def get_metrics(scores: torch.Tensor, labels: torch.Tensor) -> int:\n    \"\"\"Get the metrics: true negatives, true positives and number of correct samples.\n\n    Args:\n        scores: the probability distribution.\n        labels: the class labels.\n\n    Returns: :return: the number of correct samples, of true negatives and of true positives, predicted positives, predicted negatives\n\n    \"\"\"\n    classes_predicted = torch.argmax(scores, 1)\n    correct = (classes_predicted == labels).sum().item()\n    true_neg = (classes_predicted[classes_predicted==0] == labels[classes_predicted==0]).sum().item()\n    true_pos = (classes_predicted[classes_predicted==1] == labels[classes_predicted==1]).sum().item()\n    return correct, true_neg, true_pos, len(classes_predicted[classes_predicted==1]), len(classes_predicted[classes_predicted==0])","83103b08":"def train(writer: torch.utils.tensorboard.writer.SummaryWriter,\n          model: nn.Module,\n          train_loader: utils.data.DataLoader,\n          device: torch.device,          \n          optimizer: torch.optim,\n          lr_scheduler: torch.optim.lr_scheduler,\n          criterion: Callable[[torch.Tensor, torch.Tensor], float],\n          log_interval: int,\n          epoch: int) -> Tuple[float, float]:\n    \"\"\"Train loop to train a neural network for one epoch.\n\n    Args:\n        model: the model to train.\n        train_loader: the data loader containing the training data.\n        device: the device to use to train the model.        \n        optimizer: the optimizer to use to train the model.\n        criterion: the loss to optimize.\n        log_interval: the log interval.\n        epoch: the number of the current epoch\n\n    Returns:\n        the Cross Entropy Loss value on the training data, \n        the accuracy on the training data,\n        the recall (sensitivity),\n        the precision (positive predicted value),\n        the negative predicted value,\n        f1 score\n        specificity\n    \"\"\"    \n    correct = 0\n    true_neg = 0\n    true_pos = 0\n    pred_pos = 0\n    num_positives = 0\n    num_negatives = 0\n    pred_neg = 0\n    samples_train = 0\n    loss_train = 0\n    size_ds_train = len(train_loader.dataset)\n    num_batches = len(train_loader)\n\n    model.train()\n    for idx_batch, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        scores = model(images)\n\n        loss = criterion(scores, labels)\n        loss_train += loss.item() * len(images)\n        samples_train += len(images)\n        num_positives += len(labels[labels==1])\n        num_negatives += len(labels[labels==0])\n\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        correct_tmp, true_neg_tmp, true_pos_tmp, pred_pos_tmp, pred_neg_tmp = get_metrics(scores, labels)\n        correct += correct_tmp\n        true_neg += true_neg_tmp\n        true_pos += true_pos_tmp\n        pred_pos += pred_pos_tmp\n        pred_neg += pred_neg_tmp\n\n        if log_interval > 0:\n            if idx_batch % log_interval == 0:\n                running_loss = loss_train \/ samples_train\n                global_step = idx_batch + (epoch * num_batches)\n                writer.add_scalar('Metrics\/Loss_Train_IT', running_loss, global_step)\n                \n    loss_train \/= samples_train\n    accuracy_training = 100. * correct \/ samples_train\n    recall = 100. * true_pos \/ num_positives\n    precision = 100. * true_pos \/ (pred_pos+0.001)\n    npv = 100. * true_neg \/ (pred_neg+0.001)\n    f1 = 2*precision*recall\/(precision+recall+0.0001)\n    specificity = 100. * true_neg \/ num_negatives\n    return loss_train, accuracy_training, recall, precision, npv, f1, specificity","718dac94":"def validate(model: nn.Module,\n             data_loader: utils.data.DataLoader,\n             device: torch.device,\n             criterion: Callable[[torch.Tensor, torch.Tensor], float]) -> Tuple[float, float]:\n    \"\"\"Evaluate the model.\n\n    Args:\n        model: the model to evalaute.\n        data_loader: the data loader containing the validation or test data.\n        device: the device to use to evaluate the model.\n        criterion: the loss function.\n\n    Returns:\n        the loss value on the validation data \n        the accuracy on the validation data\n        the recall (sensitivity),\n        the precision (positive predicted value),\n        the negative predicted value,\n        f1 score\n    \"\"\"\n    correct = 0\n    true_neg = 0\n    true_pos = 0\n    num_positives = 0\n    num_negatives = 0\n    pred_pos = 0\n    pred_neg = 0\n    samples_val = 0\n    loss_val = 0.\n\n    model = model.eval()\n    with torch.no_grad():\n        for idx_batch, (images, labels) in enumerate(data_loader):\n            images, labels = images.to(device), labels.to(device)\n            scores = model(images)\n\n            loss = criterion(scores, labels)\n            loss_val += loss.item() * len(images)\n            samples_val += len(images)\n            num_positives += len(labels[labels==1])\n            num_negatives += len(labels[labels==0])\n            correct_tmp, true_neg_tmp, true_pos_tmp, pred_pos_tmp, pred_neg_tmp = get_metrics(scores, labels)\n            correct += correct_tmp\n            true_neg += true_neg_tmp\n            true_pos += true_pos_tmp\n            pred_pos += pred_pos_tmp\n            pred_neg += pred_neg_tmp\n\n    loss_val \/= samples_val\n    accuracy = 100. * correct \/ samples_val\n    recall = 100. * true_pos \/ num_positives\n    precision = 100. * true_pos \/ (pred_pos+0.001)\n    npv = 100. * true_neg \/ (pred_neg+0.001)\n    f1 = 2*precision*recall\/(precision+recall+0.0001)\n    specificity = 100. * true_neg \/ num_negatives\n    return loss_val, accuracy, recall, precision, npv, f1, specificity","ebf0eaee":"def training_loop(writer: torch.utils.tensorboard.writer.SummaryWriter,\n                  num_epochs: int, \n                  optimizer: torch.optim,\n                  lr_scheduler: torch.optim.lr_scheduler,\n                  log_interval: int, \n                  model: nn.Module, \n                  loader_train: utils.data.DataLoader, \n                  loader_val: utils.data.DataLoader, \n                  device: torch.device,\n                  verbose: bool=True)->Dict:\n    \"\"\"Executes the training loop.\n    \n        Args:\n            name_exp: the name for the experiment.\n            num_epochs: the number of epochs.\n            optimizer: the optimizer to use.\n            log_interval: intervall to print on tensorboard.\n            model: the mode to train.\n            loader_train: the data loader containing the training data.\n            loader_val: the data loader containing the validation data.\n            verbose:\n            device\n\n        Returns:  \n            A dictionary with the statistics computed during the train:\n            the values for the train loss for each epoch\n            the values for the train f1 score for each epoch\n            the values for the validation f1 score for each epoch\n            the time of execution in seconds for the entire loop\n        \"\"\"\n    # class_weights = torch.FloatTensor([3.315, 1.0]).to(device)\n    criterion = nn.CrossEntropyLoss()  # weight=class_weights)    \n    loop_start = timer()\n\n    losses_values = []\n    train_f1_values = []\n    val_f1_values = []\n    for epoch in range(1, num_epochs + 1):\n        time_start = timer()\n        loss_train, accuracy_train, recall_train, precision_train, npv_train, f1_train, spec_train = train(writer, model, loader_train, device, optimizer, \n                                lr_scheduler, criterion, log_interval, epoch)\n        loss_val, accuracy_val, recall_val, precision_val, npv_val, f1_val, spec_val = validate(model, loader_val, device, criterion)\n        time_end = timer()\n\n        losses_values.append(loss_train)\n        train_f1_values.append(f1_train)\n        val_f1_values.append(f1_val)\n        \n        lr =  optimizer.param_groups[0]['lr']\n        \n        if verbose:            \n            print(f'Epoch: {epoch} '\n                  f' Lr: {lr:.8f} '\n                  f' Loss: Train = [{loss_train:.4f}] - Val = [{loss_val:.4f}] '\n                  f' F1-score: Train = [{f1_train:.2f}%] - Val = [{f1_val:.2f}%] '\n                  f' Accuracy: Train = [{accuracy_train:.2f}%] - Val = [{accuracy_val:.2f}%] '\n                  f' Recall: Train = [{recall_train:.2f}%] - Val = [{recall_val:.2f}%] '\n                  f' Precision: Train = [{precision_train:.2f}%] - Val = [{precision_val:.2f}%] '\n                  f' NPV: Train = [{npv_train:.2f}%] - Val = [{npv_val:.2f}%] '\n                  f' Specificity: Train = [{spec_train:.2f}%] - Val = [{spec_val:.2f}%] '\n                  f' Time one epoch (s): {(time_end - time_start):.4f} ')\n          \n        writer.add_scalar('Hyperparameters\/Learning Rate', lr, epoch)\n        writer.add_scalars('Metrics\/Losses', {\"Train\": loss_train, \"Val\": loss_val}, epoch)\n        writer.add_scalars('Metrics\/Accuracy', {\"Train\": accuracy_train, \"Val\": accuracy_val}, epoch)\n        writer.add_scalars('Metrics\/F1-score', {\"Train\": f1_train, \"Val\": f1_val}, epoch)\n        writer.add_scalars('Metrics\/Recall', {\"Train\": recall_train, \"Val\": recall_val}, epoch)\n        writer.add_scalars('Metrics\/Precision', {\"Train\": precision_train, \"Val\": precision_val}, epoch)\n        writer.add_scalars('Metrics\/NPV', {\"Train\": npv_train, \"Val\": npv_val}, epoch)\n        writer.add_scalars('Metrics\/Specificity', {\"Train\": spec_train, \"Val\": spec_val}, epoch)\n        writer.flush()\n    \n    loop_end = timer()\n    time_loop = loop_end - loop_start\n    if verbose:\n        print(f'Time for {num_epochs} epochs (s): {(time_loop):.3f}') \n        \n    return {'loss_values': losses_values,\n            'train_f1_values': train_f1_values,\n            'val_f1_values': val_f1_values,\n            'time': time_loop}","9c36efb2":"def best_train_loop(writer: torch.utils.tensorboard.writer.SummaryWriter,\n                  num_epochs: int, \n                  optimizer: torch.optim,\n                  lr_scheduler: torch.optim.lr_scheduler,\n                  log_interval: int, \n                  model: nn.Module, \n                  loader_train: utils.data.DataLoader,\n                  device: torch.device,\n                  verbose: bool=True)->Dict:\n    \"\"\"Executes the training loop.\n    \n        Args:\n            name_exp: the name for the experiment.\n            num_epochs: the number of epochs.\n            optimizer: the optimizer to use.\n            log_interval: intervall to print on tensorboard.\n            model: the mode to train.\n            loader_train: the data loader containing the training data.\n            verbose:\n            device\n\n        Returns:  \n            A dictionary with the statistics computed during the train:\n            the values for the train loss for each epoch\n            the values for the train f1 score for each epoch\n            the time of execution in seconds for the entire loop\n        \"\"\"\n    # class_weights = torch.FloatTensor([3.315, 1.0]).to(device)\n    criterion = nn.CrossEntropyLoss()  # weight=class_weights)    \n    loop_start = timer()\n\n    losses_values = []\n    train_f1_values = []\n    for epoch in range(1, num_epochs + 1):\n        time_start = timer()\n        loss_train, accuracy_train, recall_train, precision_train, npv_train, f1_train, spec_train = train(writer, model, loader_train, device, optimizer, \n                                lr_scheduler, criterion, log_interval, epoch)\n        time_end = timer()\n\n        losses_values.append(loss_train)\n        train_f1_values.append(f1_train)\n        \n        lr =  optimizer.param_groups[0]['lr']\n        \n        if verbose:            \n            print(f'Epoch: {epoch} '\n                  f' Lr: {lr:.8f} '\n                  f' Loss: Train = [{loss_train:.4f}]'\n                  f' F1-score: Train = [{f1_train:.2f}%]'\n                  f' Accuracy: Train = [{accuracy_train:.2f}%]'\n                  f' Recall: Train = [{recall_train:.2f}%]'\n                  f' Precision: Train = [{precision_train:.2f}%]'\n                  f' NPV: Train = [{npv_train:.2f}%]'\n                  f' Specificity: Train = [{spec_train:.2f}%]'\n                  f' Time one epoch (s): {(time_end - time_start):.4f} ')\n               \n        writer.add_scalar('Hyperparameters\/Learning Rate', lr, epoch)\n        writer.add_scalars('Metrics\/Losses', {\"Train\": loss_train}, epoch)\n        writer.add_scalars('Metrics\/Accuracy', {\"Train\": accuracy_train}, epoch)\n        writer.add_scalars('Metrics\/F1-score', {\"Train\": f1_train}, epoch)\n        writer.add_scalars('Metrics\/Recall', {\"Train\": recall_train}, epoch)\n        writer.add_scalars('Metrics\/Precision', {\"Train\": precision_train}, epoch)\n        writer.add_scalars('Metrics\/NPV', {\"Train\": npv_train}, epoch)\n        writer.add_scalars('Metrics\/Specificity', {\"Train\": spec_train}, epoch)\n        writer.flush()\n    \n    loop_end = timer()\n    time_loop = loop_end - loop_start\n    if verbose:\n        print(f'Time for {num_epochs} epochs (s): {(time_loop):.3f}') \n        \n    return {'loss_values': losses_values,\n            'train_f1_values': train_f1_values,\n            'time': time_loop}","0be62b9f":"params = {\n    'lr': [0.0001, 0.001, 0.01],\n    'weight_decay': [0.0, 0.01, 0.1],\n    'amsgrad': [True, False],\n    'lr_decay_rate': [0.96, 0.99, 1]\n}","e38071a5":"#%load_ext tensorboard","f746b2dc":"#%tensorboard --logdir logs","7638b0a7":"num_epochs = 20\nlog_interval = 5\ndevice = \"cuda\"\n\nbest_f1_found = 0.0\nbest_lr = 0.0\nbest_amsgrad = False\nbest_weight_decay = 0\nbest_lr_decay_rate = 1.0\nbest_epoch_found = 0\ncont = 0\n\nfor lr in params['lr']:\n  for amsgrad in params['amsgrad']:\n    for weight_decay in params['weight_decay']:\n      for lr_decay_rate in params['lr_decay_rate']:\n        print(\"Starting lr = {}, amsgrad = {}, weights_decay = {}, lr_decay_rate = {}\".format(lr, amsgrad, weight_decay, lr_decay_rate))\n        print(\"\\t {}% done.\".format(100. * cont \/ (len(params['lr'])*\n                                                   len(params['amsgrad'])*\n                                                   len(params['weight_decay'])*\n                                                   len(params['lr_decay_rate'])\n                                                   )))\n        cont += 1\n        first_cnn = FirstCNN(num_classes=len(data_train.classes))\n        first_cnn = first_cnn.to(device)\n        optimizer = torch.optim.Adam(first_cnn.parameters(), lr=lr, \n                                    weight_decay=weight_decay, amsgrad=amsgrad)\n        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, \n                                                              gamma=lr_decay_rate)\n\n        writer = torch.utils.tensorboard.writer.SummaryWriter(\n            log_dir=os.path.join(\"logs\", \"first_cnn\"))\n        statistics = training_loop(writer, num_epochs, optimizer, lr_scheduler, \n                                   log_interval, first_cnn, train_loader, \n                                   valid_loader, device)\n        writer.close()\n\n        best_epoch = np.argmax(statistics['val_f1_values']) + 1\n        best_f1 = statistics['val_f1_values'][best_epoch - 1]\n\n        if best_f1_found < best_f1:\n          best_f1_found = best_f1\n          best_lr = lr\n          best_lr_decay_rate = lr_decay_rate\n          best_weight_decay = weight_decay\n          best_amsgrad = amsgrad\n          best_epoch_found = best_epoch\n\nprint(f'Best val f1: {best_f1_found:.2f} epoch: {best_epoch_found} lr: {best_lr} lr_decay_rate: {best_lr_decay_rate} weight_decay: {best_weight_decay} amsgrad: {best_amsgrad}')","22e94b5f":"lr = best_lr\nlr_decay_rate = best_lr_decay_rate\nweight_decay = best_weight_decay\namsgrad = best_amsgrad\nnum_epochs = best_epoch_found\n\nfinal_dataset = ConcatDataset([data_train, data_val])\ntrain_loader = DataLoader(final_dataset, batch_size=size_batch_train, \n                          num_workers=num_workers, pin_memory=True, \n                          shuffle=True)\n\nbest_model = FirstCNN(num_classes=len(data_train.classes))\nbest_model = best_model.to(device)\noptimizer = torch.optim.Adam(best_model.parameters(), lr=lr, \n                                   weight_decay=weight_decay, amsgrad=amsgrad)\nlr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, \n                                                      gamma=lr_decay_rate)\n\nwriter = torch.utils.tensorboard.writer.SummaryWriter(\n    log_dir=os.path.join(\"logs\", \"best_model\"))\nstatistics = best_train_loop(writer, num_epochs, optimizer, lr_scheduler, log_interval, \n                           best_model, train_loader, device)\nwriter.close()\n\nbest_epoch = np.argmax(statistics['train_f1_values']) + 1\nbest_f1 = statistics['train_f1_values'][best_epoch - 1]\n\nprint(f'Best val f1: {best_f1:.2f} epoch: {best_epoch}.')","0276e90a":"criterion = nn.CrossEntropyLoss()  \nloss_test, accuracy_test, recall_test, precision_test, npv_test, f1_test, spec_test = validate(best_model, test_loader, device, criterion)\nprint(f' Loss = [{loss_test:.4f}] '\n      f' F1-score = [{f1_test:.2f}%] '\n      f' Accuracy = [{accuracy_test:.2f}%] '\n      f' Recall = [{recall_test:.2f}%] '\n      f' Precision = [{precision_test:.2f}%] '\n      f' NPV = [{npv_test:.2f}%] '\n      f' Specificity = [{spec_test:.2f}%] ')","f9334c8b":"torch.save(best_model.state_dict(), \"best_model\")","46ff93d0":"### Train model with best hyper-parameters","68a5076b":"### Train methods","84e4ec83":"### Validate methods","cfd4fd05":"### Test best model","001b4c3a":"### Creating dataset and Standardizing input\n\nSince we are working with images with RGB color channels, we normalize each color channel with respect to the mean and standard deviation values calculated across all pixels in every images for the respective color channel.","52aa8a3d":"## Train","18be6b03":"### Distribution of samples per category","76c90703":"### Start training","bcb84407":"## Dataset creation","13764b1b":"Splitting with numbers given by dataset creator","aa63740a":"### Main training loop","7b43ebca":"### Save model","cca41293":"## Test","b7536e3a":"### Data visualization","d2ed3cc5":"## Results\nEvaluating the best model on the test dataset gave the following results\\:\n- Precision: 95.96%\n- Recall: 97.94%\n- F1-score: 96.94%\n- NPV: 92.59%\n- Accuracy: 95.24%\n- Specificity: 86.21%\n\nIt could be interesting to try with smaller networks and different batch sizes.","6c9d5caf":"# Title\nATTENTION: In this notebooks test and validation are used as follows: the test set is use only by the final model; the validation set is used to perform hyper-parameter tuning. \n","fffa989a":"### Best train main loop","aba59820":"### Select hyper-parameters for search","51124dd6":"## Network creation","cbc5a391":"### Splitting images in train, test and validation folder","0c168a06":"## Imports","8ff626d9":"### Separating train, test and validation"}}