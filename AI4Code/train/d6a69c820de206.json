{"cell_type":{"71813461":"code","876fb296":"code","4396ff83":"code","c190c27f":"code","0c679119":"code","88269488":"code","a62104c5":"code","9198dd40":"code","37be6da8":"code","d46b6972":"code","a441f290":"markdown","8c8fd592":"markdown","813df206":"markdown"},"source":{"71813461":"import numpy as np\nimport pandas as pd","876fb296":"base_path = '..\/input\/'","4396ff83":"jf_train_df = pd.read_csv(base_path + 'jigsaw-toxic-comment-classification-challenge\/train.csv')","c190c27f":"# \uac00\uc911\uce58 \ubd80\uc5ec \uac00\uacf5\ntoxic = 1.0\nsevere_toxic = 2.0\nobscene = 1.0\nthreat = 1.0\ninsult = 1.0\nidentity_hate = 2.0\n\ndef create_train_first(df):\n    df['y'] = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].max(axis=1)\n    df['y'] = df['y'] + df['severe_toxic']*severe_toxic\n    df['y'] = df['y'] + df['obscene']*obscene\n    df['y'] = df['y'] + df['threat']*threat\n    df['y'] = df['y'] + df['insult']*insult\n    df['y'] = df['y'] + df['identity_hate']*identity_hate\n    \n    # renameing\uc740 \uc6d0\ud558\ub294\ub300\ub85c \ubcc0\uacbd \uac00\ub2a5\n#     df= df[['comment_text', 'y', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].rename(columns={'comment_text': 'text'})\n    \n    #undersample\n    min_len = (df['y'] >= 1).sum()\n    df_y0_undersample = df[df['y'] == 0].sample(n=int(min_len*1.5), random_state=201)\n    df = pd.concat([df[df['y'] >= 1], df_y0_undersample])\n    \n    return df\n","0c679119":"jf_train_df = create_train_first(jf_train_df)\nprint(jf_train_df['y'].value_counts())","88269488":"js_train_df = pd.read_csv(base_path + 'jigsaw-unintended-bias-in-toxicity-classification\/train.csv')","a62104c5":"def create_train_second(df):\n    # non-toxic data remove by annotator_count\n    df = df.query('toxicity_annotator_count > 5')\n    \n    df['y'] = df[['severe_toxicity', 'obscene', 'sexual_explicit', 'identity_attack', 'insult', 'threat']].sum(axis=1)\n    df['y'] = df.apply(lambda row: row['target'] if row['target'] <= 0.5 else row['y'], axis=1)\n    \n    # renameing\uc740 \uc6d0\ud558\ub294\ub300\ub85c \ubcc0\uacbd \uac00\ub2a5\n#     js_train_df = js_train_df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\n\n    # undersampling\n    min_len = (df['y'] > 0.5).sum()\n    df_y0_undersample = df[df['y'] <= 0.5].sample(n=int(min_len*1.5), random_state=201)\n    df = pd.concat([df[df['y'] > 0.5], df_y0_undersample])\n    \n    return df","9198dd40":"js_train_df = create_train_second(js_train_df)\nprint(js_train_df['y'].value_counts())","37be6da8":"rud_df = pd.read_csv(base_path + 'ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv')","d46b6972":"rud_df['y'] = rud_df['offensiveness_score'].map(lambda x: 0.0 if x <=0 else x)\n# rud_df = rud_df[['txt', 'y']].rename(columns={'txt': 'text'})\nprint(rud_df['y'].value_counts())","a441f290":"# Model Toxic Comment Classification Challenge","8c8fd592":"# Model Ruddit: Norms of Offensiveness for English Reddit Comments","813df206":"# Model Unintended Bias in Toxicity Classification"}}