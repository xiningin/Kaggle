{"cell_type":{"77ec3360":"code","58db07f8":"code","09d09c33":"code","863d0ca5":"code","1b49f120":"code","cc752c65":"code","74c688c4":"code","e709532f":"code","b12d820d":"code","c08aa803":"code","51078422":"code","175bd93b":"code","213a7a05":"code","f1040038":"code","f8db05d5":"code","b5635030":"code","e2d2bc1c":"code","9abb01b8":"code","49ff84b8":"code","6af51a22":"code","70aad62e":"code","c1d45e8b":"code","2fea9767":"code","cdd46d92":"markdown","546a65bc":"markdown","781c824f":"markdown","524b1b68":"markdown","fba120a3":"markdown","dfdebaae":"markdown","e5139c91":"markdown","0d688b8e":"markdown","81202316":"markdown","4b8c322d":"markdown","f23bdabb":"markdown","089ab4af":"markdown","2d16e818":"markdown","80234520":"markdown","b6139ae1":"markdown","f843753e":"markdown","4d222b26":"markdown","efc786a4":"markdown","1054c113":"markdown"},"source":{"77ec3360":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","58db07f8":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, cross_validate, train_test_split\n\n","09d09c33":"data = pd.read_csv(\"..\/input\/hitters\/hitters.csv\")\ndf = data.copy()","863d0ca5":"df.shape","1b49f120":"df.dtypes","cc752c65":"df.head()","74c688c4":"df.isnull().sum()","e709532f":"df.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T","b12d820d":"df.describe().T","c08aa803":"cat_cols = [col for col in df.columns if (df[col].dtypes == \"O\") or (df[col].nunique() < 11 and\n                   df[col].dtypes != \"O\")]\ncat_cols","51078422":"num_cols = [col for col in df.columns if (df[col].dtypes != \"O\") and col not in cat_cols]\nnum_cols","175bd93b":"    corr = df.corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > 0.90)]\n    sns.set(rc={'figure.figsize': (15, 15)})\n    sns.heatmap(corr, cmap=\"RdBu\")\n    plt.show()","213a7a05":"for col in cat_cols:\n    print(pd.DataFrame({col: df[col].value_counts(),\n                        \"Ratio\": 100 * df[col].value_counts() \/ len(df)}))","f1040038":"# 1. Missing Values\ndf.isnull().sum()","f8db05d5":"# The missing values are dropped because they are included in the target variable.\ndf.dropna(inplace= True)\n\n# 2. Outliers\nnum_cols_without_salary = [col for col in num_cols if col != 'Salary']\n\nfor col in num_cols_without_salary:\n    quartile1 = df[col].quantile(0.25)\n    quartile3 = df[col].quantile(0.75)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    df.loc[(df[col] < low_limit), col] = low_limit\n    df.loc[(df[col] > up_limit), col] = up_limit\n    \n\n#  Outliers of Salary are repleced with thresholds:\nquartile1 = df[\"Salary\"].quantile(0.30)\nquartile3 = df[\"Salary\"].quantile(0.70)\ninterquantile_range = quartile3 - quartile1\nup_limit = quartile3 + 1.5 * interquantile_range\nlow_limit = quartile1 - 1.5 * interquantile_range\ndf.loc[(df[\"Salary\"] < low_limit), \"Salary\"] = low_limit\ndf.loc[(df[\"Salary\"] > up_limit), \"Salary\"] = up_limit\n\n# Multivariate Outlier Analysis: Local Outlier Factor\nnum_cols_without_salary = [col for col in num_cols if col != 'Salary']\nclf = LocalOutlierFactor(n_neighbors=20)\nclf.fit_predict(df[num_cols_without_salary])\ndf_scores = clf.negative_outlier_factor_\nscores = pd.DataFrame(np.sort(df_scores))\n\nscores.plot(stacked=True, xlim=[0, 20], style='.-')\nplt.show()\n\nth = np.sort(df_scores)[3]\ndf[df_scores < th]\n# drop these observations according to Multivariate Outlier Analysis\ndf.drop(axis=0, labels=df[df_scores < th].index, inplace= True)\ndf.reset_index(drop= True, inplace= True)","b5635030":"# 3. Encoding\n# Label Encoding\nbinary_cols = [col for col in df.columns if df[col].dtype not in [int, float]\n               and df[col].nunique() == 2]\n\nlabelencoder = LabelEncoder()\nfor col in binary_cols:\n    df[col] = labelencoder.fit_transform(df[col])\n\n# 4. Scaling\nfor col in num_cols_without_salary:\n    transformer = RobustScaler().fit(df[[col]])\n    df[col] = transformer.transform(df[[col]])\n    \n","e2d2bc1c":" df.drop(\"Division\", axis= 1, inplace= True)\n# When the feature importance is examined below, \n# it is observed that the effect of this variable is very very low. so we drop this variable.","9abb01b8":"df.loc[df[\"AtBat\"]!=0 , \"New_Hits_Over_AtBat\"] = df[\"Hits\"] \/ df[\"AtBat\"]\ndf.loc[df[\"AtBat\"]==0 , \"New_Hits_Over_AtBat\"] = df[\"Hits\"]\ndf.loc[df[\"CAtBat\"]!=0 , \"New_CHits_Over_CAtBat\"] = df[\"CHits\"] \/ df[\"CAtBat\"]\ndf.loc[df[\"CAtBat\"]==0 , \"New_CHits_Over_CAtBat\"] = df[\"CHits\"]\ndf.loc[df[\"Years\"]!=0 , \"New_CHits_Over_Years\"] = df[\"CHits\"] \/ df[\"Years\"]\ndf.loc[df[\"Years\"]==0 , \"New_CHits_Over_Years\"] = df[\"CHits\"]\ndf.loc[df[\"Years\"]!=0 , \"New_CRuns_Over_Years\"] = df[\"CRuns\"] \/ df[\"Years\"]\ndf.loc[df[\"Years\"]==0 , \"New_CRuns_Over_Years\"] = df[\"CRuns\"]\ndf[\"New_CHmRun2_CRuns\"] =df[\"CHmRun\"] * df[\"CHmRun\"]* df[\"CRuns\"]\ndf.loc[df[\"CHits\"]!=0 , \"New_Hits_Over_CHits\"] = df[\"Hits\"] \/ df[\"CHits\"]\ndf.loc[df[\"CHits\"]==0 , \"New_Hits_Over_CHits\"] = df[\"Hits\"]\ndf.loc[df[\"CRBI\"]!=0 , \"New_RBI_Over_CRBI\"] = df[\"RBI\"] \/ df[\"CRBI\"]\ndf.loc[df[\"CRBI\"]==0 , \"New_RBI_Over_CRBI\"] = df[\"RBI\"]\ndf.loc[df[\"CWalks\"]!=0 , \"New_Walks_Over_CWalks\"] = df[\"Walks\"] \/ df[\"CWalks\"]\ndf.loc[df[\"CWalks\"]==0 , \"New_Walks_Over_CWalks\"] = df[\"Walks\"]\ndf['New_PutOuts_Years'] = df['PutOuts'] * df['Years']\ndf[\"New_CRBI_CatBaT\"] = df['CRBI'] * df['CAtBat']\ndf[\"New_RBI_Walks\"] = df[\"RBI\"] * df[\"Walks\"]\ndf.loc[df[\"PutOuts\"]!=0 , \"New_AtBat_Over_Putouts\"] = df[\"AtBat\"] \/ df[\"PutOuts\"]\ndf.loc[df[\"PutOuts\"]==0 , \"New_AtBat_Over_Putouts\"] = df[\"AtBat\"]\ndf.loc[df[\"Errors\"]!=0 , \"New_Assists_Over_Errorss\"] = df[\"Assists\"] \/ df[\"Errors\"]\ndf.loc[df[\"Errors\"]==0 , \"New_Assists_Over_Errorss\"] = df[\"Assists\"]\ndf[\"New_General_Success\"] = df[\"New_RBI_Walks\"]*15 + df[\"CAtBat\"] * 13 \/ 100 + df[\"CHits\"] * 14 \/ 100 + df[\"CHmRun\"] * 12 \/ 100 + df[\"CRuns\"] * 13 \/ 100 + df[\"CRBI\"] * 14 \/ 100 + df[\"AtBat\"] * 11 \/ 100 + df[\"PutOuts\"] * 9 \/ 100\ndf[\"Overall_performance\"] = (df[\"AtBat\"] * 10 + df[\"Hits\"] * 20 + df[\"HmRun\"] * 30 + df[\"Runs\"] * 20 + df[\n        \"RBI\"] * 10 + df[\"Walks\"] * 10) \/ 100\ndf[\"Triple_interaction\"] = df[\"HmRun\"] * df[\"RBI\"] * df[\"Hits\"] \/ df[\"AtBat\"]\ndf.loc[df[\"Years\"] != 0, \"New_Triple\"] = (df[\"CHmRun\"] \/ df[\"Years\"]) * (df[\"CRBI\"] \/ df[\"Years\"]) * df[\"CHits\"] \/ df[\"CAtBat\"]\ndf.loc[df[\"Years\"] == 0, \"New_Triple\"] = df[\"CHmRun\"] * df[\"CRBI\"] * df[\"CHits\"] \/ df[\"CAtBat\"]\ndf.loc[df[\"Years\"] != 0, \"NEW_HitsCHits_Over_Years\"] = df[\"Hits\"] \/ (df[\"CHits\"] \/ df[\"Years\"])\ndf.loc[df[\"Years\"] == 0, \"NEW_HitsCHits_Over_Years\"] = df[\"Hits\"] \/ df[\"CHits\"]\ndf.loc[df[\"Years\"] != 0, \"NEW_Runs_Over_CRunsYears\"] = df[\"Runs\"] \/ (df[\"CRuns\"] \/ (df[\"Years\"]))\ndf.loc[df[\"Years\"] == 0, \"NEW_Runs_Over_CRunsYears\"] = df[\"Runs\"] \/ df[\"CRuns\"]\ndf.loc[(df[\"Years\"] != 0) & (df[\"CRBI\"] != 0), \"NEW_RBI_Over_CRBIYears\"] = df[\"RBI\"] \/ (df[\"CRBI\"] \/ (df[\"Years\"]))\ndf.loc[(df[\"Years\"] == 0) | (df[\"CRBI\"] == 0), \"NEW_RBI_Over_CRBIYears\"] = df[\"RBI\"]","49ff84b8":"y = df[\"Salary\"]\nX = df.drop([\"Salary\"], axis=1)\n\nmodels = [('LR', LinearRegression()),\n              (\"Ridge\", Ridge()),\n              (\"Lasso\", Lasso()),\n              (\"ElasticNet\", ElasticNet()),\n              ('KNN', KNeighborsRegressor()),\n              ('CART', DecisionTreeRegressor()),\n              ('RF', RandomForestRegressor()),\n              ('SVR', SVR()),\n              ('GBM', GradientBoostingRegressor()),\n              (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n              (\"LightGBM\", LGBMRegressor()),\n              (\"CatBoost\", CatBoostRegressor(verbose=False))]\n\n\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")","6af51a22":"y = df[\"Salary\"]\nX = df.drop([\"Salary\"], axis=1)\n\nrf_params = {\"max_depth\": np.random.randint(5, 40, 20),\n                        \"max_features\": [3,4, 5,6, 7, \"auto\", \"sqrt\"],\n                        \"min_samples_split\": [2,3,4,5,6,7,8,9,10,11,12,13],\n                        \"n_estimators\": [int(x) for x in np.linspace(start=200, stop=1500, num=10)]}\n\nlightgbm_params = {\"boosting_type\" : ['gbdt',\"dart\"],\n                       \"num_leaves\" :[15,20,25,30,31,32,37],\n                       \"max_depth\": [5,6,7,10,12,15,20],\n                       \"learning_rate\": [0.1, 0.01, 0.2, 0.02, 0.05, None],\n                       \"n_estimators\": [int(x) for x in np.linspace(start=200, stop=1500, num=10)],\n                       \"colsample_bytree\": [1.0,0.8, 0.7, 0.6, 0.5]}\n\ncatboost_params = { \"iterations\" : [300, 200, 400, 500, 100,600,750],\n                        \"learning_rate\" : [0.01, 0.02, 0.05],\n                        \"depth\" : [3,4,5, 6,7, 10]}\n\n\nregressors = [(\"RF\", RandomForestRegressor(), rf_params),\n                  (\"LightGBM\", LGBMRegressor(), lightgbm_params),\n                  (\"CatBoost\", CatBoostRegressor(verbose= False), catboost_params)]\n\nbest_models = {}\n\nfor name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n    rs_best = RandomizedSearchCV(regressor, params, cv=5,  n_iter= 50, n_jobs=-1, verbose= 0).fit(X, y)\n    final_model = regressor.set_params(**rs_best.best_params_)\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n    print(f\"{name} best params: {rs_best.best_params_}\", end=\"\\n\\n\")\n    best_models[name] = final_model","70aad62e":"y = df[\"Salary\"]\nX = df.drop([\"Salary\"], axis=1)\n\nrf_params = {\"max_depth\": [15,17, None],\n                 \"max_features\": [6, 8,\"auto\"],\n                 \"min_samples_split\": [2, 4, 5],\n                 \"n_estimators\": [250,500,1000]}\n\nlightgbm_params = {\"num_leaves\": [10, 30, 31, 32],\n                       \"max_depth\": [-1, 5, 15],\n                       \"learning_rate\": [0.01, 0.02, 0.05],\n                       \"n_estimators\": [100, 200, 300],\n                       \"colsample_bytree\": [0.6, 0.7, 1]}\n\ncatboost_params = {\"iterations\": [None, 500, 400],\n                       \"learning_rate\": [None, 0.01, 0.05],\n                       \"depth\": [3,6, None]}\n\nregressors = [(\"RF\", RandomForestRegressor(), rf_params),\n                  (\"LightGBM\", LGBMRegressor(), lightgbm_params),\n             (\"CatBoost\", CatBoostRegressor(verbose=False), catboost_params)]\n\nbest_models = {}\n\nfor name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=7, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n    gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n    final_model = regressor.set_params(**gs_best.best_params_)\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=7, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n    best_models[name] = final_model","c1d45e8b":"y = df[\"Salary\"]\nX = df.drop([\"Salary\"], axis=1)\nbest_models = {'RF': RandomForestRegressor(max_depth= None, max_features='auto', n_estimators=250),\n               'LightGBM' : LGBMRegressor(colsample_bytree = 0.7, learning_rate = 0.05, max_depth = 5, n_estimators = 100, num_leaves = 10),\n               'RF_D' : RandomForestRegressor()}\n\nvoting_reg = VotingRegressor(estimators=[('RF', best_models[\"RF\"]),\n                                         ('RF_D', best_models[\"RF_D\"]),\n                                         ('LightGBM', best_models['LightGBM'])])\nvoting_reg.fit(X, y)\n\nnp.mean(np.sqrt(-cross_val_score(voting_reg, X, y, cv=10, scoring=\"neg_mean_squared_error\")))","2fea9767":"feature_imp = pd.DataFrame({'Value': best_models['RF'].fit(X, y).feature_importances_,'Feature': X.columns})\nplt.figure(figsize=(10, 10))\nsns.set(font_scale=1)\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:len(X)])\nplt.title('Features')\nplt.tight_layout()\nplt.show()","cdd46d92":"A data frame with 322 observations of major league players on the following 20 variables.\n\nAtBat:  Number of times at bat in 1986\n\nHits:   Number of hits in 1986\n\nHmRun:  Number of home runs in 1986\n\nRuns:   Number of runs in 1986\n\nRBI:    Number of runs batted in in 1986\n\nWalks:  Number of walks in 1986\n\nYears:  Number of years in the major leagues\n\nCAtBat: Number of times at bat during his career\n\nCHits:  Number of hits during his career\n\nCHmRun: Number of home runs during his career\n\nCRuns:  Number of runs during his career\n\nCRBI:   Number of runs batted in during his career\n\nCWalks: Number of walks during his career\n\nLeague: A factor with levels A and N indicating player's league at the end of 1986\n\nDivision:A factor with levels E and W indicating player's division at the end of 1986\n\nPutOuts: Number of put outs in 1986\n\nAssists: Number of assists in 1986\n\nErrors:  Number of errors in 1986\n\nSalary:  1987 annual salary on opening day in thousands of dollars\n\nNewLeague:A factor with levels A and N indicating player's league at the beginning of 1987","546a65bc":"**Optimization**","781c824f":"**According to these results from RandomizedSearchCv, let's see also the GridSearchCV results:**","524b1b68":"**SALARY PREDICTION FOR HITTERS, MACHINE LEARNING**","fba120a3":"**Base Models**","dfdebaae":"**FEATURE ENGINEERING**","e5139c91":"All three look balanced.","0d688b8e":"Let's take a look at categorical columns:","81202316":"**RandomizedSearchCV for the regressors that bring best results**","4b8c322d":"\n**Report:**\n\nThe missing values present only in the target variable, these observations are dropped.\n\nOutliers are examined individually and together, suppressed at a certain level.\n\nLabel Encoding and RobustScaler implemented.\n\nNew variables are created.\n\nFor model selection and optimization, hyperparameter optimization is performed using RandomizedSearchCV and GridSearchCV methods, and the models with the best results are selected.\n\nThe final model is built with the best parameters, Random Forests and LightGBM models are included in the final model.\n\nAs a result of the cross validation for the final model,  RMSE = 186 is obtained.\n\nThe importance of the features in the model is examined.\n","f23bdabb":"**DATA PRE-PROCESSING**","089ab4af":"**EXPLORATORY DATA ANALYSIS**","2d16e818":"Categoric and numeric columns:","80234520":"Highly correlated columns:","b6139ae1":"**Stacking & Ensemble Learning**","f843753e":"**Tuning**","4d222b26":"When the graph is examined, there are columns with a high correlation. But when dropped, the prediction success is decreased. So no columns are dropped.","efc786a4":"**MODELING**","1054c113":"Developing a model that predicts salaries for baseball players, given salary information and some statistics from 1986."}}