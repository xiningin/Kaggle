{"cell_type":{"ce25b74e":"code","a3268926":"code","8b550640":"code","ae166396":"code","8c707346":"code","80d280b5":"code","4a11bc55":"code","df4f8045":"code","4cc9b5b8":"code","69873e07":"code","a7190523":"code","c21de469":"code","10183cd6":"code","3ab47a78":"code","f65fd8c8":"code","6bbd8cdb":"code","b7a2d489":"markdown","7869428d":"markdown","e9b06656":"markdown","03de5105":"markdown","f871d271":"markdown","261ff057":"markdown","7ca7115c":"markdown","0cd91651":"markdown","addadaae":"markdown","4e07d8f9":"markdown","a6ca3cda":"markdown","1a9d5740":"markdown","4d5bc743":"markdown"},"source":{"ce25b74e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n!pip install beautifulsoup4\nfrom bs4 import BeautifulSoup # for removing HTML tags\nimport re # removing punctuation and numbers\nimport nltk # removing stop words\nimport gensim # word vectors (word2vec)\nimport cython # speeding up training\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a3268926":"# Importing data (with column header, tab spacing, and ignoring double quotes)\ntrain_data = pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv.zip\", \\\n                         header=0, delimiter = \"\\t\", quoting=3)\ntest_data = pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/testData.tsv.zip\", \\\n                        header=0, delimiter = \"\\t\", quoting=3)\nunlabeled_train_data = pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/unlabeledTrainData.tsv.zip\", \\\n                                   header=0, delimiter = \"\\t\", quoting=3)\nprint(train_data.shape, test_data.shape)\nprint(train_data.columns.values)\ntrain_data.head()","8b550640":"from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n# inspired by tutorial\ndef review_to_wordlist(review, remove_stop_words=False):\n    '''Creates a clean review by removing\n    HTML tags, punctuation, numbers, and stop words'''\n    # remove HTML tags\n    review1 = BeautifulSoup(review).get_text()\n    # remove punctuation\n    review2 = re.sub(\"[^a-zA-Z0-9]\", \" \", review1)\n    # removing unneccesary newlines\/spaces + lower case\n    review3 = review2.lower().split()\n    if remove_stop_words:\n        # removing stop words\n        stops = set(stopwords.words(\"english\")) # faster search through set than list\n        review4 = [word for word in review3 if word not in stops]\n        # lemmatization and stemming\n        lemmatizer = WordNetLemmatizer()\n        porterStemmer = PorterStemmer()\n        review5 = [lemmatizer.lemmatize(porterStemmer.stem(word)) for \\\n                   word in review4]\n        # return final review (joined by spaces)\n        return \" \".join(review5)\n    else:\n        return review3","ae166396":"clean_train_reviews = []\nclean_test_reviews = []\nfor i in range(0, train_data[\"review\"].size):\n    clean_train_reviews.append(review_to_wordlist(train_data[\"review\"][i],True))\n    clean_test_reviews.append(review_to_wordlist(test_data[\"review\"][i],True))\n    if (i+1)%1000 == 0:\n        print(f\"{i+1} finished for train and test data\")","8c707346":"from sklearn.feature_extraction.text import CountVectorizer\n# creating the vectorizer for creating the bag of words\nvectorizer = CountVectorizer(analyzer = \"word\", \\\n                             tokenizer = None, \\\n                             preprocessor = None, \\\n                             stop_words = None, \\\n                             max_features = 5000)\n\ntrain_data_features = vectorizer.fit_transform(clean_train_reviews)\ntrain_data_features = train_data_features.toarray()\ntest_data_features = vectorizer.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()\ntrain_data_features.shape","80d280b5":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_bow = RandomForestClassifier(n_estimators=100)\nmodel_bow.fit(train_data_features, train_data[\"sentiment\"])","4a11bc55":"# Final predictions with the bag of words and XGBoost model\npredictions_bow = model_bow.predict(test_data_features)\noutput_bow = pd.DataFrame(data={\"id\":test_data[\"id\"], \"sentiment\": predictions_bow})\noutput_bow.to_csv(\"submissionBoW.csv\", index=False, quoting=3)\nprint(\"Submission file created!\")","df4f8045":"# Load punkt tokenizer\ntokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\n\n# Function to split review into sentences\ndef review_to_sentences(review, tokenizer, remove_stopwords=False):\n    '''Creates a clean review split into sentences'''\n    # Using tokenizer to split paragraph into sentences\n    sentences1 = tokenizer.tokenize(review.strip())\n    # go over each sentence\n    sentences = []\n    for sentence in sentences1:\n        if len(sentence) > 0:\n            # call review_to_wordlist\n            sentences.append(review_to_wordlist(sentence, remove_stopwords))\n    return sentences","4cc9b5b8":"sentences = []\n\n# adding train sentences together in a list for training the word2vec model (not removing stop words for better model training)\nfor i in range(0, train_data[\"review\"].size):\n    sentences += review_to_sentences(train_data[\"review\"][i], tokenizer, False)\n    if (i+1)%1000 == 0:\n        print(f\"{i+1} sentences finished for train data\")\n        \n# adding unlabeled train sentences in a list for training the word2vec model\nfor i in range(0,unlabeled_train_data[\"review\"].size):\n    sentences += review_to_sentences(unlabeled_train_data[\"review\"][i], tokenizer, False)\n    if (i+1)%1000 == 0:\n        print(f\"{i+1} sentences finished for unlabeled train data\")","69873e07":"# Taking a look at a sentence in sentences\nprint(sentences[0])","a7190523":"# Import the built-in logging module and configure it so that Word2Vec creates nice output messages (from tutorial)\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n                    level=logging.INFO)\n\n# Setting values for parameters\nnum_features = 400     # Word vector dimensionality                      \nmin_word_count = 40    # Minimum word count                        \nnum_workers = 5        # Number of threads to run in parallel\ncontext = 15           # Context window size                                                                                    \ndownsampling = 0.0005  # Downsample setting for frequent words\n\n# initializing and training the model\nfrom gensim.models import word2vec\nmodel_w2v = word2vec.Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count=min_word_count, \\\n                              window=context, sample=downsampling)","c21de469":"# some fun experimentation with the methods of the model\nprint(model_w2v.wv.doesnt_match(\"where when what why who is\".split())) # should be is\nprint(model_w2v.wv.doesnt_match(\"king queen servant\".split())) # should be servant\nprint(model_w2v.wv.most_similar(\"move\")) # mostly seem correct, either move as in walking or move as in pushing (though furiou doesn't really match)","10183cd6":"def review_to_vector(review, model, num_features):\n    '''Function to average all word vectors in a review'''\n    # initializing an empty np array\n    feature_vector = np.zeros((num_features,),dtype=\"float32\")\n    num_words = 0\n    # set of words in model's vocabulary\n    set_of_words = set(model.wv.index_to_key)\n    for word in review:\n        if word in set_of_words:\n            num_words += 1\n            feature_vector = np.add(feature_vector,model.wv[word])\n    feature_vector = np.divide(feature_vector, num_words)\n    return feature_vector\n\ndef get_feature_vectors(reviews, model, num_features):\n    '''Function to create feature vectors for all reviews'''\n    count = 0\n    review_vectors = np.zeros((len(reviews),num_features),dtype=\"float32\")\n    for review in reviews:\n        review_vectors[count] = review_to_vector(review, model, num_features)\n        count += 1\n        if count%1000 == 0:\n            print(f\"{count} reviews converted\")\n    return review_vectors","3ab47a78":"clean_train_reviews_vec = []\nclean_test_reviews_vec = []\nfor i in range(0, train_data[\"review\"].size):\n    clean_train_reviews_vec.append(review_to_wordlist(train_data[\"review\"][i],False))\n    clean_test_reviews_vec.append(review_to_wordlist(test_data[\"review\"][i],False))\n    if (i+1)%1000 == 0:\n        print(f\"{i+1} finished for train and test data\")\n\ntrain_data_vectors = get_feature_vectors(clean_train_reviews_vec, model_w2v, num_features)\ntest_data_vectors = get_feature_vectors(clean_test_reviews_vec, model_w2v, num_features)","f65fd8c8":"model_vec = RandomForestClassifier(n_estimators = 100)\nmodel_vec.fit(train_data_vectors, train_data[\"sentiment\"])","6bbd8cdb":"predictions_vec = model_vec.predict(test_data_vectors)\noutput_vec = pd.DataFrame(data={\"id\":test_data[\"id\"], \"sentiment\": predictions_vec})\noutput_vec.to_csv(\"submissionW2V.csv\", index=False, quoting=3)\nprint(\"Submission file created!\")","b7a2d489":"## Using RandomForestClassifier on Word Vectors\nFinally, for the Word Vector data, I will train a RandomForest model and use it to predict the outcomes on the test data.","7869428d":"## Data Cleaning\nHere, I'll do the data preprocessing (removing HTML tags, removing punctuation, removing unnecessary newlines\/spaces and lower casing, removing stop words, stemming, and lemmatization).","e9b06656":"## Data Imports\nI'll first import the necessary data.","03de5105":"I'll be using the following final parameters for the word2vec model:\n1. Architecture: skip-gram (default)\n2. Training Algorithm: Hierarchical Softmax (default)\n3. Downsampling of Frequent Words: 0.0005\n4. Word vector dimensionality: 400\n5. Context\/window size: 15\n6. Worker threads: 5\n7. Minimum word count: 40","f871d271":"This, from the submission, seems to get a final accuracy of 84.5%.","261ff057":"## Creating Word Vector Model\nWord vectors are a way of representing words as a list of numbers, which allow computers to find useful relationships between words, such as (quoted from the tutorial):\n> \"king - man + woman = queen\"\n\nHere, I will be creating word vectors as the second type of representation.\n\nFirst, I'll create a sentence splitter to get the data ready for Word2Vec.","7ca7115c":"This representation seems to get a lower accuracy of about 77.2%. Possible reasons might be:\n1. The Word2Vec model did not have much data (much less than a billion words), and thus it did not have sufficient training.\n2. There were not enough estimators in the RandomForestClassifier.\n3. There were too many features for the feature vectors (or other parameters were not the best that they could be).","0cd91651":"## Creating Bag of Words Representation\nHere, I will create the bag of words representation of the train and test data with CountVectorizer.","addadaae":"## Using Average Word Vectors\nIf we average the individual vectors in a text, the final vector can usually give good results when training (though the sentence order is lost). Therefore, we will be using average word vectors to convert our reviews.","4e07d8f9":"Next, I'll apply the sentence splitter to the reviews to get the sentences to train the word2vec model.","a6ca3cda":"## Using RandomForestClassifier on Bag of Words\nFinally, for the Bag of Words data, I will train a RandomForest model and use it to predict the outcomes on the test data.","1a9d5740":"### Thanks for reading!","4d5bc743":"# Using Random Forest + Bag of Words\/Word2Vec on Bag of Words Meets Bags of Popcorn Competition\nIn this competition, I will be using two types of models to predict the sentiments of movie reviews:\n1. The Bag of Words representation and a Random Forest model.\n2. Word2Vec and the Word Vector representation, and a Random Forest model.\n\nHere are the basic imports."}}