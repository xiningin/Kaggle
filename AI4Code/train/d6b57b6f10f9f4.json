{"cell_type":{"842cf6ff":"code","8b560095":"code","092fbf6f":"code","9cc90f18":"code","2a36e142":"code","974d665a":"code","c8ec956b":"code","4c93df6b":"code","45b13ade":"code","39d220a0":"code","4310026d":"code","74eaf7ea":"code","a738e9cb":"code","3fd07150":"code","a3cb2b79":"code","739b2f53":"code","ba013e54":"code","ba488d09":"code","3400fb3a":"code","77bbba38":"code","3019a9d5":"code","e724d538":"code","e38edee4":"code","b2b235a9":"code","8ca097b8":"code","0136599a":"markdown","b9cecb82":"markdown","5e68656a":"markdown","1ce3bb1a":"markdown","36acf70b":"markdown","d439c6f2":"markdown","b8e25915":"markdown","20340d16":"markdown","84a306d3":"markdown"},"source":{"842cf6ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8b560095":"# Install Auto Linear Regression (Part of KUtils Package)\n!pip install kesh-utils","092fbf6f":"# Load the custom packages from kesh-utils\nfrom KUtils.eda import chartil\nfrom KUtils.eda import data_preparation as dp","9cc90f18":"# The library is auto_linear_regression we give alias as autolr\nfrom KUtils.linear_regression import auto_linear_regression as autolr","2a36e142":"# Use 3 decimal places for decimal number (to avoid displaying as exponential format)\npd.options.display.float_format = '{:,.3f}'.format","974d665a":"import warnings  \nwarnings.filterwarnings('ignore')","c8ec956b":"# Load the dataset\ndiamond_df = pd.read_csv('..\/input\/diamonds.csv')","4c93df6b":"# Have a quick look on the top few records of the dataset \ndiamond_df.head()","45b13ade":"diamond_df.describe()","39d220a0":"# Drop first column which is just a sequence\ndiamond_df = diamond_df.drop(diamond_df.columns[0], axis=1)","4310026d":"diamond_df.shape","74eaf7ea":"# Null checks\ndiamond_df.isnull().sum()","a738e9cb":"# Plot the nulls as barchart (Null count in each features)\ndp.plotNullInColumns(diamond_df)","3fd07150":"# Number of unique values in each column (Check in both Train and Test for missing categorial label in any)\n{x: len(diamond_df[x].unique()) for x in diamond_df.columns}","a3cb2b79":"# Plot unique values in each feature\ndp.plotUnique(diamond_df, optional_settings={'sort_by_value':True})","739b2f53":"# Some EDA (Using kesh-utils chartil package)\nchartil.plot(diamond_df, diamond_df.columns, optional_settings={'include_categorical':True, 'sort_by_column':'price'})","ba013e54":"# Have a quick look on different feature and their relation\nchartil.plot(diamond_df, ['carat', 'depth','table', 'x','y','z','price', 'cut'], chart_type='pairplot', optional_settings={'group_by_last_column':True})","ba488d09":"chartil.plot(diamond_df, ['color', 'price'], chart_type='distplot')","3400fb3a":"chartil.plot(diamond_df, ['clarity', 'price'])","77bbba38":"model_info = autolr.fit(diamond_df, 'price', \n                     scale_numerical=True, acceptable_r2_change = 0.005,\n                     include_target_column_from_scaling=True, \n                     dummies_creation_drop_column_preference='dropFirst',\n                     random_state_to_use=100, include_data_in_return=True, verbose=True)","3019a9d5":"# Print all iteration info\nmodel_info['model_iteration_info'].head()","e724d538":"# Final set of features used in final model \nmodel_info['features_in_final_model']","e38edee4":"# vif-values of features used in final model  \nmodel_info['vif-values']","b2b235a9":"# p-values of features used in final model  \nmodel_info['p-values']","8ca097b8":"# Complete stat summary of the OLS model\nmodel_info['model_summary']","0136599a":"Above method returns 'model_info' dictionary which will have all the details used while performing auto fit. \nWill go thru one by one","b9cecb82":"# How Auto LR works?\nWe throw the cleaned dataset to autolr.fit(<<parameters>>)\nThe method will \n- Treat categorical variable if applicable(dummy creation\/One hot encoding)\n- First model - Run the RFE on dataset\n- For remaining features elimination - it follows backward elimination - one feature at a time\n    - combination of vif and p-values of coefficients (Eliminate with higher vif and p-value combination\n    - vif only (or eliminate one with higher vif)\n    - p-values only (or eliminate one with higher p-value)\n- Everytime when a feature is identified we build new model and repeat the process\n- on every iteration if adjusted R2 affected significantly, we re-add\/retain it and select next possible feature to eliminate.\n- Repeat until program can't proceed further with above logic.\n","5e68656a":"# Auto Linear Regression in Action\nThe method <b><u>autolr.fit()<\/u><\/b> has below parameters\n- df, (The full dataframe)\n- dependent_column, (Target column)\n- p_value_cutoff = 0.01, (Threashold p-values of features to use while filtering features during backward elimination step, Default 0.01)\n- vif_cutoff = 5, (Threashold co-relation of vif values of features to use while filtering features during backward elimination step, Default 5)\n- acceptable_r2_change = 0.02, (Restrict degradtion of model efficiency by controlling loss of change in R2, Default 0.02)\n- scale_numerical = False, (Flag to convert\/scale numerical fetures using StandardScaler)\n- include_target_column_from_scaling = True, (Flag to indiacte weather to include target column from scaling)\n- dummies_creation_drop_column_preference='dropFirst', (Available options dropFirst, dropMax, dropMin - While creating dummies which clum drop to convert to one hot)\n- train_split_size = 0.7, (Train\/Test split ration to be used)\n- max_features_to_select = 0, (Set the number of features to be qualified from RFE before entring auto backward elimination)\n- random_state_to_use=100, (Self explanatory)\n- include_data_in_return = False, (Include the data generated\/used in Auto LR which might have gobne thru scaling, dummy creation etc.)\n- verbose=False (Enable to print detailed debug messgaes)","1ce3bb1a":"# Action time\nLets try this library and see how it works\n\nTo demonstrate the library I used the one of the popular dataset [UCI Diamond dataset](https:\/\/www.kaggle.com\/shivam2503\/diamonds) from Kaggle\n\n","36acf70b":"Finally we have the model with respective coefficients. ","d439c6f2":"Price is target column and the first column seems to be just a sequence","b8e25915":"53,920 rows, 10 features including target column 'price'","20340d16":"# Auto Linear Regression\n#### We have seen Auto ML like H2O which is a blackbox approach to generate models. \nDuring our model building process, we try with brute force\/TrialnError\/several combinations to come up with best model. \nHowever trying these possibilities manually is a laborious process.\nIn order to overcome or atleast have a base model automatically I developed this auto linear regression using backward feature elimination technique.\n\nThe library\/package can be found [here](https:\/\/pypi.org\/project\/kesh-utils\/) and source code [here](https:\/\/github.com\/KeshavShetty\/ds\/tree\/master\/KUtils\/linear_regression)\n","84a306d3":"* ### Go back to autolr.fit() method and try different cut-offs or finetune other parameter\/options"}}