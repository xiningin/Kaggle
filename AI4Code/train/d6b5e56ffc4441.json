{"cell_type":{"bc32afc6":"code","70762e47":"code","64936d04":"code","58e2c5c6":"code","a6421079":"code","08e906da":"code","a6cbaf14":"code","9fea1cf0":"code","2886accf":"code","e6d65aac":"code","1a0aa736":"code","ed571501":"code","2dbef507":"code","f2f95e5d":"code","30d23322":"code","b93c1f0e":"code","f7769bf1":"code","f78b9145":"code","6aab6a14":"code","a0ce293a":"code","d4b2aa66":"code","cc7ceea8":"code","748a9421":"code","ae57ad3c":"code","7e8aa6dd":"code","bada9388":"code","09b3050e":"code","137ef13d":"code","ce08e24a":"code","8607091f":"code","b777dd27":"code","78f2645d":"code","6cdd534e":"code","a99f4a26":"code","b76a62c7":"code","2c6dddb6":"code","cbd81a1c":"code","e8ccbdf0":"code","26caf5bd":"code","23dd8417":"code","79c36ecd":"markdown","e9056ac1":"markdown","89c917a0":"markdown","16485523":"markdown","85e6556a":"markdown","8d81b439":"markdown","19aaed24":"markdown","4dbc354a":"markdown","7db0c6b2":"markdown","c12eda5f":"markdown","f3876f17":"markdown","bf32e41f":"markdown","e8cba3f4":"markdown","854210b4":"markdown","47324ae8":"markdown","d3011659":"markdown","1da8e3c2":"markdown"},"source":{"bc32afc6":"import pandas as pd\nimport numpy as np\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport holoviews as hv\nhv.extension('bokeh', 'matplotlib', logo=False)\n\n# Evitar que aparezcan advertencias\nimport warnings\nwarnings.filterwarnings('ignore')","70762e47":"df=pd.read_csv('..\/input\/hmeq.csv', low_memory=False) # Sin columnas duplicadas, sin columnas altamente correlacionadas\ndf.drop('DEBTINC', axis=1, inplace=True) # El significado de esta variable no est\u00e1 claro. Por lo que se excluye del planteamiento\ndf.dropna(axis=0, how='any', inplace=True)","64936d04":"df[df['BAD']==0].drop('BAD', axis=1).describe().style.format(\"{:.2f}\")","58e2c5c6":"df[df['BAD']==1].drop('BAD', axis=1).describe().style.format(\"{:.2f}\")","a6421079":"df.loc[df.BAD == 1, 'STATUS'] = 'DEFAULT'\ndf.loc[df.BAD == 0, 'STATUS'] = 'PAID'","08e906da":"g = df.groupby('REASON')\ng['STATUS'].value_counts(normalize=True).to_frame().style.format(\"{:.1%}\")","a6cbaf14":"g = df.groupby('JOB')\ng['STATUS'].value_counts(normalize=True).to_frame().style.format(\"{:.1%}\")","9fea1cf0":"%%opts Bars[width=700 height=400 tools=['hover'] xrotation=45]{+axiswise +framewise}\n\n# Categorical\n\ncols = ['REASON', 'JOB']\n\ndd={}\n\nfor col in cols:\n\n    counts=df.groupby(col)['STATUS'].value_counts(normalize=True).to_frame('val').reset_index()\n    dd[col] = hv.Bars(counts, [col, 'STATUS'], 'val') \n    \nvar = [*dd]\nkdims=hv.Dimension(('var', 'Variable'), values=var)    \nhv.HoloMap(dd, kdims=kdims)","2886accf":"%%opts Histogram[width=700 height=400 tools=['hover'] xrotation=0]{+axiswise +framewise}\n\ng = df.groupby('STATUS')\n\ncols = ['LOAN',\n        'MORTDUE', \n        'VALUE',\n        'YOJ',\n        'DEROG',\n        'DELINQ',\n        'CLAGE',\n        'NINQ',\n        'CLNO']\ndd={}\n\n# Histograms\nfor col in cols:\n    \n    freq, edges = np.histogram(df[col].values)\n    dd[col] = hv.Histogram((edges, freq), label='TODOS los pr\u00e9stamos').redim.label(x=' ')\n    \n    freq, edges = np.histogram(g.get_group('PAID')[col].values, bins=edges)\n    dd[col] *= hv.Histogram((edges, freq), label='Pr\u00e9stamos PAGADOS').redim.label(x=' ')\n    \n    freq, edges = np.histogram(g.get_group('DEFAULT')[col].values, bins=edges)\n    dd[col] *= hv.Histogram((edges, freq), label='Pr\u00e9stamos INCUMPLIDOS' ).redim.label(x=' ')   \n    \nvar = [*dd]\nkdims=hv.Dimension(('var', 'Variable'), values=var)    \nhv.HoloMap(dd, kdims=kdims)","e6d65aac":"%%opts Scatter[width=500 height=500 tools=['hover'] xrotation=0]{+axiswise +framewise}\n\ng = df.groupby('STATUS')\n\ncols = ['LOAN',\n        'MORTDUE',\n        'VALUE',\n        'YOJ',\n        'DEROG',\n        'DELINQ',\n        'CLAGE',\n        'NINQ',\n        'CLNO']\n\nimport itertools\nprod = list(itertools.combinations(cols,2))\n\ndd = {}\n\nfor p in prod:\n    dd['_'.join(p)] = hv.Scatter(g.get_group('PAID')[list(p)], label='Pr\u00e9stamos PAGADOS').options(size=5)\n    dd['_'.join(p)] *= hv.Scatter(g.get_group('DEFAULT')[list(p)], label='Pr\u00e9stamos INCUMPLIDOS').options(size=5, marker='x')\n    \nvar = [*dd]\nkdims=hv.Dimension(('var', 'Variable'), values=var)    \nhv.HoloMap(dd, kdims=kdims).collate()","1a0aa736":"g=sns.PairGrid(df.drop('BAD',axis=1), hue='STATUS', diag_sharey=False, palette={'PAID': 'C0', 'DEFAULT':'C1'})\ng.map_lower(sns.kdeplot)\ng.map_upper(sns.scatterplot)\ng.map_diag(sns.kdeplot, lw=3)\ng.add_legend()\nplt.show()","ed571501":"cols=['YOJ', 'CLAGE', 'NINQ']\n\nfor col in cols:\n    \n    plt.figure(figsize=(15,5))\n\n    sns.violinplot(x='JOB', y=col, hue='STATUS',\n                   split=True, inner=\"quart\",  palette={'PAID': 'C0', 'DEFAULT':'C1'},\n                   data=df)\n    \n    sns.despine(left=True)","2dbef507":"def compute_corr(df,size=10):\n    '''La funci\u00f3n traza una matriz de correlaci\u00f3n gr\u00e1fica para cada par de columnas en el marco de datos.\n\n    Entradas:\n        df: pandas DataFrame\n        size: vertical y horizontal tama\u00f1o del plot'''\n    import scipy\n    import scipy.cluster.hierarchy as sch\n    \n    corr = df.corr()\n    \n    # Clustering\n    d = sch.distance.pdist(corr)   # vector of ('55' choose 2) pairwise distances\n    L = sch.linkage(d, method='complete')\n    ind = sch.fcluster(L, 0.5*d.max(), 'distance')\n    columns = [df.select_dtypes(include=[np.number]).columns.tolist()[i] for i in list((np.argsort(ind)))]\n    \n    # Reordered df upon custering results\n    df = df.reindex(columns, axis=1)\n    \n    # Recompute correlation matrix w\/ clustering\n    corr = df.corr()\n    #corr.dropna(axis=0, how='all', inplace=True)\n    #corr.dropna(axis=1, how='all', inplace=True)\n    #corr.fillna(0, inplace=True)\n    \n    #fig, ax = plt.subplots(figsize=(size, size))\n    #img = ax.matshow(corr)\n    #plt.xticks(range(len(corr.columns)), corr.columns, rotation=45);\n    #plt.yticks(range(len(corr.columns)), corr.columns);\n    #fig.colorbar(img)\n    \n    return corr","f2f95e5d":"%%opts HeatMap [tools=['hover'] colorbar=True width=500  height=500 toolbar='above', xrotation=45, yrotation=45]\n\ncorr=compute_corr(df)\ncorr=corr.stack(level=0).to_frame('value').reset_index()\nhv.HeatMap(corr).options(cmap='Viridis')","30d23322":"import pandas as pd\nimport numpy as np\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import classification_report","b93c1f0e":"df=pd.read_csv('..\/input\/hmeq.csv', low_memory=False) # Sin columnas duplicadas, sin columnas altamente correlacionadas\ndf=pd.get_dummies(df, columns=['REASON','JOB'])\ndf.drop('DEBTINC', axis=1, inplace=True)\ndf.dropna(axis=0, how='any', inplace=True)\ny = df['BAD']\nX = df.drop(['BAD'], axis=1)","f7769bf1":"def cross_validate_model(model, X, y, \n                         scoring=['f1', 'precision', 'recall', 'roc_auc'], \n                         cv=12, n_jobs=-1, verbose=True):\n    \n    scores = cross_validate(pipe, \n                        X, y, \n                        scoring=scoring,\n                        cv=cv, n_jobs=n_jobs, \n                        verbose=verbose,\n                        return_train_score=False)\n\n    #sorted(scores.keys())\n    dd={}\n    \n    for key, val in scores.items():\n        if key in ['fit_time', 'score_time']:\n            continue\n        #print('{:>30}: {:>6.5f} +\/- {:.5f}'.format(key, np.mean(val), np.std(val)) )\n        name = \" \".join(key.split('_')[1:]).capitalize()\n        \n        dd[name] = {'value' : np.mean(val), 'error' : np.std(val)}\n        \n    return  pd.DataFrame(dd)    \n    #print()\n    #pprint(scores)\n    #print()","f78b9145":"def plot_roc(model, X_test ,y_test, n_classes=0):\n    \n    from sklearn.metrics import roc_curve, auc\n    \n    \"\"\"\n    Puntuaciones objetivo, pueden ser estimaciones de probabilidad\n    de la clase positiva, valores de confianza o\n    medida de decisiones sin umbral (como se devuelve\n    por \"decision_function\" en algunos clasificadores).\n    \"\"\"\n    try:\n        y_score = model.decision_function(X_test)\n    except Exception as e:\n        y_score = model.predict_proba(X_test)[:,1]\n    \n    \n    fpr, tpr, _ = roc_curve(y_test.ravel(), y_score.ravel())\n    roc_auc = auc(fpr, tpr)\n\n    # Compute micro-average ROC curve and ROC area\n    #fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n    #roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    \n    #plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('Tasa de falsos positivos')\n    plt.ylabel('Tasa de verdaderos positivos')\n    plt.title('Ejemplo de caracter\u00edstica de funcionamiento del receptor')\n    plt.legend(loc=\"lower right\")\n    #plt.show()\n    \n# mezclar y dividir conjuntos de entrenamiento y prueba\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n#                                                    random_state=0)","6aab6a14":"def plot_confusion_matrix(model, X_test ,y_test,\n                          classes=[0,1],\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    import itertools\n    from sklearn.metrics import confusion_matrix\n    \n    y_pred = model.predict(X_test)\n    \n    # Calcular matriz de confusi\u00f3n\n    cm = confusion_matrix(y_test, y_pred)\n    np.set_printoptions(precision=2)\n    \n    \"\"\"\n    Esta funci\u00f3n imprime y traza la matriz de confusi\u00f3n.\n    La normalizaci\u00f3n se puede aplicar configurando `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    #    print(\"Normalized confusion matrix\")\n    #else:\n    #    print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","a0ce293a":"def feature_importance(coef, names, verbose=False, plot=True):\n    \n    #importances = model.feature_importances_\n\n    \n    \n    #std = np.std([tree.feature_importances_ for tree in model.estimators_],\n    #             axis=0)\n    indices = np.argsort(coef)[::-1]\n    \n    if verbose:\n    \n        # Imprime la clasificaci\u00f3n de funciones\n        print(\"Feature ranking:\")\n    \n        for f in range(len(names)):\n            print(\"{:>2d}. {:>15}: {:.5f}\".format(f + 1, names[indices[f]], coef[indices[f]]))\n        \n    if plot:\n        \n        # Trazar la importancia de las caracter\u00edsticas del bosque\n        #plt.figure(figsize=(5,10))\n        plt.title(\"Feature importances\")\n        plt.barh(range(len(names)), coef[indices][::-1], align=\"center\")\n        #plt.barh(range(X.shape[1]), importances[indices][::-1],\n        #         xerr=std[indices][::-1], align=\"center\")\n        plt.yticks(range(len(names)), names[indices][::-1])\n        #plt.xlim([-0.001, 1.1])\n        #plt.show()","d4b2aa66":"def plot_proba(model, X, y, bins=40, show_class = 1):\n    \n    from sklearn.calibration import CalibratedClassifierCV\n    \n    model = CalibratedClassifierCV(model)#, cv='prefit')\n    \n    model.fit(X, y)\n    \n    proba=model.predict_proba(X)\n    \n    if show_class == 0:\n        sns.kdeplot(proba[y==0,0], shade=True, color=\"r\", label='True class')\n        sns.kdeplot(proba[y==0,1], shade=True, color=\"b\", label='Wrong class')\n        plt.title('Classification probability: Class 0')\n    elif show_class == 1:\n        sns.kdeplot(proba[y==1,1], shade=True, color=\"r\", label='True class')\n        sns.kdeplot(proba[y==1,0], shade=True, color=\"b\", label='Wrong class')\n        plt.title('Classification probability: Class 1')\n    plt.legend()","cc7ceea8":"from sklearn.linear_model import LogisticRegression\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', LogisticRegression(random_state=0))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","748a9421":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].coef_[0], X.columns)\n\nplt.tight_layout()","ae57ad3c":"logit_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nlogit_xval_res.T[['value','error']].style.format(\"{:.2f}\")","7e8aa6dd":"from sklearn.linear_model import SGDClassifier\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000, tol=1e-3, random_state=0))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","bada9388":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].coef_[0], X.columns)\n\nplt.tight_layout()","09b3050e":"sgd_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nsgd_xval_res.T[['value','error']].style.format(\"{:.2f}\")","137ef13d":"from sklearn.svm import SVC\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', SVC(random_state=0, kernel='linear', probability=True))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","ce08e24a":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].coef_[0], X.columns)\n\nplt.tight_layout()","8607091f":"svc_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nsvc_xval_res.T[['value','error']].style.format(\"{:.2f}\")","b777dd27":"from sklearn.ensemble import GradientBoostingClassifier\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', GradientBoostingClassifier(n_estimators=250, learning_rate=0.05, random_state=0))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","78f2645d":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].feature_importances_, X.columns)\n\nplt.tight_layout()","6cdd534e":"gbc_xval_res = cross_validate_model(pipe, X, y, verbose=False)\ngbc_xval_res.T[['value','error']].style.format(\"{:.2f}\")","a99f4a26":"from sklearn.ensemble import RandomForestClassifier\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', RandomForestClassifier(n_estimators=250, n_jobs=-1, random_state=0))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","b76a62c7":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].feature_importances_, X.columns)\n\nplt.tight_layout()","2c6dddb6":"rfc_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nrfc_xval_res.T[['value','error']].style.format(\"{:.2f}\")","cbd81a1c":"from sklearn.ensemble import ExtraTreesClassifier\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', ExtraTreesClassifier(n_estimators=250, n_jobs=-1, random_state=0, class_weight='balanced'))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","e8ccbdf0":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].feature_importances_, X.columns)\n\nplt.tight_layout()","26caf5bd":"ert_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nert_xval_res.T[['value','error']].style.format(\"{:.2f}\")","23dd8417":"from collections import OrderedDict\n\nres_comp = OrderedDict([\n    ('Regresi\u00f3n log\u00edstica'              , logit_xval_res[1:]),\n    ('Clasificador SGD'                   , sgd_xval_res[1:]  ),\n    ('Clasificador de vectores de apoyo'     , svc_xval_res[1:]  ),\n    ('Clasificador de bosque aleatorio'         , rfc_xval_res[1:]  ),\n    ('Clasificador de \u00e1rboles extremadamente aleatorio' , ert_xval_res[1:]  ),\n    ('Clasificador de aumento de gradiente'        , gbc_xval_res[1:]  ),\n])\n\nnew_columns = {'level_0' : 'Model'}\n\npd.concat(res_comp).reset_index().drop('level_1', axis=1).rename(columns=new_columns).set_index('Model').sort_values('F1', ascending=False).style.format(\"{:.2f}\")","79c36ecd":"<a id='conclusion'><\/a>\n## Comparaci\u00f3n de modelos y conclusiones\n\nLa siguiente tabla resume el desempe\u00f1o de los modelos de clasificaci\u00f3n que consider\u00e9 en este estudio. Las actuaciones se ordenan aumentando el valor de $ F_ {1} $. Los mejores rendimientos se obtienen mediante el ** \u00e1rbol extremadamente aleatorio **, seguido del ** bosque aleatorio ** y la ** regresi\u00f3n log\u00edstica **.\n\nEl \u00e1rbol extremadamente aleatorizado permite identificar hasta el 66% de los pr\u00e9stamos que causar\u00edan un INCUMPLIMIENTO mientras se retiene el 91% de los pr\u00e9stamos que ser\u00edan PAGADOS a tiempo. El valor de ROC AUC es tan alto como 96%, lo que indica que la probabilidad de que el clasificador se desempe\u00f1e mejor por elecci\u00f3n aleatoria es tan baja como 4%.","e9056ac1":"## An\u00e1lisis Exploratorio\n<a id='explo'><\/a>\n\nSe resume las principales caracter\u00edsticas del conjunto de datos con m\u00e9todos visuales y estad\u00edsticas resumidas. Se utiliza la variable objetivo (BAD) para dividir el conjunto de datos en submuestras y se busca espec\u00edficamente variables, caracter\u00edsticas y correlaci\u00f3n que contengan poder de clasificaci\u00f3n.\n\n### Estad\u00edsticas descriptivas de pr\u00e9stamos PAGADOS\n\n<a id='descp'><\/a>","89c917a0":"1. De las estad\u00edsticas descriptivas anteriores, podemos sacar la siguiente consideraci\u00f3n:\n\n* El monto del pr\u00e9stamo solicitado, el monto de la hipoteca adeudada y el valor de la garant\u00eda subyacente son estad\u00edsticamente consistentes tanto para los pr\u00e9stamos que fueron PAGADOS como para los que resultaron en INCUMPLIMIENTO. Esto sugiere que esas variables pueden no proporcionar un poder de discriminaci\u00f3n significativo para separar las dos clases.\n\n* El n\u00famero de a\u00f1os en el trabajo actual (YOJ) parece discriminar las dos clases, ya que los INCUMPLIMIENTOS parecen m\u00e1s frecuentes en los contratistas que tienen una antig\u00fcedad m\u00e1s corta. Esta tendencia est\u00e1 respaldada por los cuantiles de valor promedio correspondientes, que indican una distribuci\u00f3n sesgada hacia una antig\u00fcedad m\u00e1s corta.\n\n* Se aplican consideraciones similares a las variables relacionadas con el historial crediticio del contratista, tales como: el n\u00famero de informes despectivos importantes (DEROG), el n\u00famero de l\u00edneas de cr\u00e9dito morosas (DELINQ), la antig\u00fcedad de la l\u00ednea de cr\u00e9dito m\u00e1s antigua en meses (CLAGE) y el n\u00famero de consultas crediticias recientes (NINQ). En el caso de INCUMPLIMIENTO, la distribuci\u00f3n de estas variables est\u00e1 sesgada hacia valores que sugieren una historia crediticia peor que la distribuci\u00f3n correspondiente para los contratistas de pr\u00e9stamos PAGADOS.\n\n* Finalmente, el n\u00famero de l\u00edneas de cr\u00e9dito abiertas (CLNO) parece estad\u00edsticamente consistente en ambos casos, lo que sugiere que esta variable no tiene un poder de discriminaci\u00f3n significativo.","16485523":"# Caso de Estudio: Riesgo Crediticio\n\n*DESCARGO DE RESPONSABILIDAD: esta es un planteamiento para problemas de clasificaci\u00f3n. El texto que se documenta proviene de diferentes fuentes (descripci\u00f3n del conjunto de datos de kaggle, documentaci\u00f3n de sklearn, documentaci\u00f3n de matplotlib, wikipedia, etc.), para sustento del proyecto FEDU de la Universidad Nacional San Antonio Abad del Cusco.*\n\n## Table of content\n\n* [Descripci\u00f3n general del conjunto de datos](#ds)\n* [An\u00e1lisis exploratorio](#explo)\n    * [Estad\u00edsticas descriptivas de pr\u00e9stamos PAGADOS](#descp)\n    * [Estad\u00edsticas descriptivas para pr\u00e9stamos INCUMPLIDOS](#descd)\n    * [INCUMPLIMIENTO en funci\u00f3n del motivo de adquisici\u00f3n de los pr\u00e9stamos](#reason)\n    * [INCUMPLIMIENTO en funci\u00f3n de la ocupaci\u00f3n](#occupation)\n    * [Resumen gr\u00e1fico](#graph)\n    * [Trama de viol\u00edn](#violin)\n    * [Matriz de correlaci\u00f3n](#corr)\n* [Prueba de clasificadores predeterminados](#classification)\n* [Evaluaci\u00f3n del modelo](#eval)\n    * [Precisi\u00f3n y recuperaci\u00f3n](#per)\n    * [F1](#f1)\n    * [Caracter\u00edstica Operativa del Receptor](#roc)\n    * [Matriz de confusi\u00f3n](#confusion)\n    * [robabilidad de clasificaci\u00f3n](#prob)\n* [Regresi\u00f3n log\u00edstica](#logit)\n* [Clasificador SGD](#sgd)\n* [Clasificador de vectores de apoyo](#svc)\n* [Clasificador de aumento de gradiente](#gbrt)\n* [Bosque de \u00e1rbol al azar](#frt)\n    * [Clasificador de bosque aleatorio](#rfc)\n    * [\u00c1rbol extremadamente aleatorio](#ert)\n* [Comparaci\u00f3n y conclusi\u00f3n del modelo](#conclusion)\n\n## Visi\u00f3n General del Dataset\n<a id='ds'><\/a>\n\nEl conjunto de datos utilizado contiene informaci\u00f3n de referencia y de rendimiento de pr\u00e9stamos para 5,960 pr\u00e9stamos. El objetivo (BAD) es una variable binaria que indica si un solicitante finalmente incurri\u00f3 en incumplimiento o en grave mora en alguna entidad bancaria. Y por muestreo r\u00e1pido. Este resultado adverso se produjo en 1.189 casos (20%) del total de la muestra.\n\nPara cada aspirante a una tarjeta de cr\u00e9dito, se registraron 11 variables de entrada:\n\n> * BAD: 1 = candidato con pr\u00e9stamo incumplido o con mora; 0 = candidato que paga su deuda y no tiene registro negativo\n* LOAN: Monto de solicitud de pr\u00e9stamo\n* MORTDUE: Monto adeudado de la hipoteca existente\n* VALUE: Valor actual del bien o propiedad\n* REASON: DebtCon = consolidaci\u00f3n de la deuda; HomeImp = mejoras para el hogar\n* JOB: Categorias ocupacionales o profesionales\n* YOJ: A\u00f1os es esu trabajo actual\n* DEROG: N\u00famero de informes derogados o cancelados importantes\n* DELINQ: N\u00famero de lineas de cr\u00e9dito morosas\n* CLAGE: Antiguedad de la linea de cr\u00e9dito m\u00e1s antigua en meses\n* NINQ:N\u00famero de consultas crediticas recientes\n* CLNO: N\u00famero de l\u00edneas de cr\u00e9dito","85e6556a":"## Clasificador de vectores de apoyo\n\n<a id='svc'><\/a>\nUna m\u00e1quina de vectores de soporte construye un hiperplano o un conjunto de hiperplanos en un espacio dimensional alto o infinito, que se puede utilizar para clasificaci\u00f3n, regresi\u00f3n u otras tareas. Intuitivamente, se logra una buena separaci\u00f3n por el hiperplano que tiene la mayor distancia a los puntos de datos de entrenamiento m\u00e1s cercanos de cualquier clase (el llamado margen funcional), ya que en general cuanto mayor es el margen menor es el error de generalizaci\u00f3n del clasificador.\n\n\nLas ventajas de las m\u00e1quinas de vectores de soporte son:\n\n* Efectivo en espacios de alta dimensi\u00f3n.\n* Sigue siendo eficaz en casos en los que el n\u00famero de dimensiones es mayor que el n\u00famero de muestras.\n* Utiliza un subconjunto de los puntos de entrenamiento en la funci\u00f3n de decisi\u00f3n, por lo que es eficiente en la memoria.","8d81b439":"<a id='sgd'><\/a>\n## Clasificador SGD\nEste algoritmo implementa modelos lineales regularizados con aprendizaje de descenso de gradiente estoc\u00e1stico (SGD): el gradiente de la p\u00e9rdida se estima en cada muestra a la vez y el modelo se actualiza a lo largo del camino con un programa de fuerza decreciente (tambi\u00e9n conocido como tasa de aprendizaje). SGD permite el aprendizaje minibatch (en l\u00ednea \/ fuera del n\u00facleo).\n","19aaed24":"<a id='ert'><\/a>\n#### \u00c1rboles extremadamente aleatorizados\nEn \u00e1rboles extremadamente aleatorizados, la aleatoriedad va un paso m\u00e1s all\u00e1 en la forma en que se calculan las divisiones. Al igual que en los bosques aleatorios, se utiliza un subconjunto aleatorio de caracter\u00edsticas candidatas, pero en lugar de buscar los umbrales m\u00e1s discriminativos, los umbrales se dibujan al azar para cada caracter\u00edstica candidata y el mejor de estos umbrales generados aleatoriamente se elige como la regla de divisi\u00f3n. Esto generalmente permite reducir un poco m\u00e1s la varianza del modelo, a expensas de un aumento ligeramente mayor del sesgo.\n","4dbc354a":"* ###  INCUMPLIMIENTO en funci\u00f3n de la ocupaci\u00f3n\n\n<a id='occupation'><\/a>\nLa fracci\u00f3n de pr\u00e9stamos PAGADOS e INCUMPLIDOS muestra cierta dependencia de la ocupaci\u00f3n del contratista. Los trabajadores de oficina y los ejecutivos profesionales tienen la mayor probabilidad de pagar sus pr\u00e9stamos, mientras que los vendedores y los aut\u00f3nomos tienen la mayor probabilidad de impago. La ocupaci\u00f3n muestra un buen poder de discriminaci\u00f3n y probablemente ser\u00e1 una caracter\u00edstica importante de nuestro modelo de clasificaci\u00f3n.\n","7db0c6b2":"## Regresi\u00f3n log\u00edstica\n<a id='logit'><\/a>\n\nLa regresi\u00f3n log\u00edstica es el modelo lineal m\u00e1s simple para la clasificaci\u00f3n. La regresi\u00f3n log\u00edstica tambi\u00e9n se conoce en la literatura como regresi\u00f3n logit, clasificaci\u00f3n de m\u00e1xima entrop\u00eda (MaxEnt) o clasificador log-lineal. En este modelo, las probabilidades que describen los posibles resultados de un solo ensayo se modelan utilizando una funci\u00f3n log\u00edstica. El problema de optimizaci\u00f3n se resuelve minimizando una funci\u00f3n de costo utilizando un algoritmo de descenso de coordenadas altamente optimizado.\n","c12eda5f":"### Estad\u00edsticas descriptivas para pr\u00e9stamos INCUMPLIDOS\n<a id='descd'><\/a>","f3876f17":"### Trama de viol\u00edn\n<a id='violin'><\/a>\nLa gr\u00e1fica de viol\u00edn muestra las diferentes formas de la funci\u00f3n de densidad de probabilidad para algunas de las variables discutidas anteriormente que parecen las m\u00e1s prometedoras para la tarea de clasificaci\u00f3n. El gr\u00e1fico muestra, en diferentes colores, los pr\u00e9stamos PAGADOS e INCUMPLIDOS. Las l\u00edneas discontinuas horizontales indican la posici\u00f3n de la media y los cuantiles de las diferentes distribuciones. Dado que existe una dependencia de la probabilidad PREDETERMINADA de las categor\u00edas de ocupaci\u00f3n, se muestran los \"violines\" para cada una de ellas.","bf32e41f":"<a id='gbrt'><\/a>\n### Clasificador de aumento de gradiente\nGradient Tree Boosting o Gradient Boosted Regression Trees (GBRT) es una generalizaci\u00f3n del impulso a funciones de p\u00e9rdida diferenciables arbitrarias. GBRT produce un modelo de predicci\u00f3n en forma de un conjunto de modelos de predicci\u00f3n d\u00e9biles, t\u00edpicamente \u00e1rboles de decisi\u00f3n. Construye el modelo por etapas como lo hacen otros m\u00e9todos de impulso, y los generaliza al permitir la optimizaci\u00f3n de una funci\u00f3n de p\u00e9rdida diferenciable arbitraria. GBRT es un procedimiento est\u00e1ndar preciso y eficaz que se puede utilizar tanto para problemas de regresi\u00f3n como de clasificaci\u00f3n.\n","e8cba3f4":"### Resumen gr\u00e1fico\n\n<a id='graph'><\/a>\nA continuaci\u00f3n se muestra una descripci\u00f3n gr\u00e1fica coherente del conjunto de datos. Para cada variable, muestro un histograma para todo el conjunto de datos, para los pr\u00e9stamos PAGADOS y DEFUALT, respectivamente. Las correlaciones entre variables tambi\u00e9n se resumen en diagramas de dispersi\u00f3n bidimensionales.\n","854210b4":"<a id='frt'><\/a>\n### Bosques de \u00e1rboles aleatorizados\nLos \u00e1rboles de decisi\u00f3n (DT) son un m\u00e9todo de aprendizaje supervisado no param\u00e9trico que se utiliza para clasificaci\u00f3n y regresi\u00f3n. El objetivo es crear un modelo que prediga el valor de una variable objetivo mediante el aprendizaje de reglas de decisi\u00f3n simples inferidas de las caracter\u00edsticas de los datos.\n\nLa t\u00e9cnica del bosque de \u00e1rboles aleatorios incluye dos algoritmos de promediado basados \u200b\u200ben \u00e1rboles de decisi\u00f3n aleatorios: el algoritmo RandomForest y el m\u00e9todo Extra-Trees. Ambos algoritmos son t\u00e9cnicas de perturbaci\u00f3n y combinaci\u00f3n dise\u00f1adas espec\u00edficamente para \u00e1rboles. Esto significa que se crea un conjunto diverso de clasificadores mediante la introducci\u00f3n de aleatoriedad en la construcci\u00f3n del clasificador. La predicci\u00f3n del conjunto se da como la predicci\u00f3n promedio de los clasificadores individuales.\n\n<a id='rfc'><\/a>\n#### Clasificador de bosque aleatorio\n\nEn bosques aleatorios, cada \u00e1rbol del conjunto se construye a partir de una muestra extra\u00edda con reemplazo (es decir, una muestra de arranque) del conjunto de entrenamiento. Adem\u00e1s, al dividir un nodo durante la construcci\u00f3n del \u00e1rbol, la divisi\u00f3n que se elige ya no es la mejor divisi\u00f3n entre todas las caracter\u00edsticas. En cambio, la divisi\u00f3n que se elige es la mejor divisi\u00f3n entre un subconjunto aleatorio de caracter\u00edsticas. Como resultado de esta aleatoriedad, el sesgo del bosque generalmente aumenta ligeramente (con respecto al sesgo de un solo \u00e1rbol no aleatorio) pero, debido al promedio, su varianza tambi\u00e9n disminuye, generalmente m\u00e1s que compensando el aumento del sesgo. de ah\u00ed que produzca un modelo mejor en general.\n","47324ae8":"### Matriz de correlaci\u00f3n\n\n<a id='corr'><\/a>\nFinalmente se muestra la matriz de correlaci\u00f3n entre las variables discutidas hasta ahora. Las correlaciones son \u00fatiles porque pueden indicar una relaci\u00f3n predictiva que se puede aprovechar en la tarea de clasificaci\u00f3n.\n\nEl gr\u00e1fico est\u00e1 codificado por colores: los colores m\u00e1s fr\u00edos corresponden a una baja correlaci\u00f3n, mientras que los colores m\u00e1s c\u00e1lidos corresponden a una alta correlaci\u00f3n. Las variables tambi\u00e9n se agrupan seg\u00fan su correlaci\u00f3n, es decir, las variables con mayor correlaci\u00f3n est\u00e1n pr\u00f3ximas entre s\u00ed.\n\nLas variables relacionadas con el historial crediticio (DELINQ, DEROG, NINQ) son las m\u00e1s correlacionadas con el estado del pr\u00e9stamo (BAD), lo que sugiere que estas ser\u00e1n las variables m\u00e1s discriminatorias. Estas variables tambi\u00e9n est\u00e1n levemente correlacionadas entre ellas, lo que sugiere que parte de la informaci\u00f3n podr\u00eda ser redundante.\n\nComo ya se mencion\u00f3, el monto del pr\u00e9stamo o la garant\u00eda subyacente no parecen estar relacionados con el estado del pr\u00e9stamo. De todos modos, forman otro grupo de correlaci\u00f3n con otras variables como la antig\u00fcedad de la l\u00ednea de cr\u00e9dito m\u00e1s antigua (CLAGE) y el n\u00famero de l\u00edneas de cr\u00e9dito (CLNO). Esto es de esperar ya que esas variables est\u00e1n claramente relacionadas.","d3011659":"### INCUMPLIMIENTO en funci\u00f3n del motivo de adquisici\u00f3n de los pr\u00e9stamos\n<a id='reason'><\/a>\nLa fracci\u00f3n de pr\u00e9stamos PAGADOS e INCUMPLIDOS no parece depender en gran medida del motivo de la adquisici\u00f3n del pr\u00e9stamo. En promedio, se ha pagado el 80% de los pr\u00e9stamos, mientras que el 20% est\u00e1 en INCUMPLIMIENTO. La discrepancia del 2% observada no es estad\u00edsticamente significativa dada la cantidad de pr\u00e9stamos en el conjunto de datos.\n","1da8e3c2":"<a id='classification'><\/a>\n# Prueba de clasificadores predeterminados\nEl an\u00e1lisis exploratorio descrito anteriormente proporciona una buena perspectiva sobre el conjunto de datos y destaca las variables m\u00e1s prometedoras con un buen poder de discriminaci\u00f3n para identificar los pr\u00e9stamos que resultan en INCUMPLIMIENTO. En esta secci\u00f3n, desarrollo e investigo clasificadores de aprendizaje autom\u00e1tico superpuestos para predecir el resultado de los pr\u00e9stamos. Dada la gran cantidad de algoritmos disponibles en la literatura, empiezo con los m\u00e9todos simples, como la regresi\u00f3n log\u00edstica, y gradualmente aumento la complejidad del modelo hasta las t\u00e9cnicas de \u00e1rboles aleatorios. Finalmente comparo el desempe\u00f1o de cada modelo y analizo el m\u00e1s apropiado para esta tarea de clasificaci\u00f3n de pr\u00e9stamos. En esta secci\u00f3n se desarrollan los siguientes modelos:\n\n* [Regresi\u00f3n log\u00edstica](#logit)\n* [Clasificador SGD](#sgd)\n* [Clasificador de vectores de apoyo](#svc)\n* [Clasificador de aumento de gradiente](#gbrt)\n* [Bosque de \u00e1rbol al azar](#frt)\n    * [Clasificador de bosque aleatorio](#rfc)\n    * [\u00c1rbol extremadamente aleatorio](#ert)\n* [Comparaci\u00f3n y conclusi\u00f3n del modelo](#conclusion)\n\n<a id='eval'><\/a>\n## Evaluaci\u00f3n del modelo\nLa evaluaci\u00f3n del desempe\u00f1o de los clasificadores es relativamente compleja y depende de muchos factores, algunos de los cuales dependen del modelo. Con el fin de identificar el mejor modelo para nuestra tarea de clasificaci\u00f3n, adopto diferentes m\u00e9tricas de evaluaci\u00f3n que se resumen brevemente a continuaci\u00f3n.\n\nPara evitar el sobreentrenamiento, el rendimiento de nuestro modelo de clasificaci\u00f3n se eval\u00faa mediante validaci\u00f3n cruzada. El conjunto de entrenamiento se divide aleatoriamente en $ N $ subconjuntos distintos llamados pliegues, luego el modelo se entrena y eval\u00faa $ N $ veces mediante el uso de un pliegue diferente para la evaluaci\u00f3n de un modelo que se entrena en los otros $ N-1 $ pliegues. Los resultados del procedimiento consisten en puntajes de evaluaci\u00f3n de $ N $ para cada m\u00e9trica que luego se promedian. Estos promedios se utilizan finalmente para comparar las diferentes t\u00e9cnicas consideradas en este estudio.\n\n\n<a id='per'><\/a>\n### Precisi\u00f3n y recuperaci\u00f3n\nPrecision-Recall es una m\u00e9trica de rendimiento \u00fatil para evaluar un modelo en aquellos casos en que las clases est\u00e1n muy desequilibradas. En la recuperaci\u00f3n de informaci\u00f3n, la precisi\u00f3n es una medida de la relevancia de los resultados, mientras que la recuperaci\u00f3n es una medida de cu\u00e1ntos resultados verdaderamente relevantes se devuelven. Intuitivamente, la precisi\u00f3n es la capacidad del clasificador de no etiquetar como positiva una muestra negativa, y la recuperaci\u00f3n es la capacidad del clasificador de encontrar todas las muestras positivas.\n\nUn sistema con alta recuperaci\u00f3n pero baja precisi\u00f3n devuelve muchas etiquetas que tienden a predecirse incorrectamente en comparaci\u00f3n con las etiquetas de entrenamiento. Un sistema con alta precisi\u00f3n pero poca recuperaci\u00f3n es todo lo contrario, arrojando muy pocos resultados, pero la mayor\u00eda de las etiquetas predichas son correctas en comparaci\u00f3n con las etiquetas de entrenamiento. Un sistema ideal con alta precisi\u00f3n y alta recuperaci\u00f3n devolver\u00e1 muchos resultados, con muchos resultados etiquetados correctamente.\n\nLa precisi\u00f3n ($ P $) se define como el n\u00famero de verdaderos positivos ($ T_ {p} $) sobre el n\u00famero de verdaderos positivos m\u00e1s el n\u00famero de falsos positivos ($ T_ {p} + F_ {p} $):\n\n$P = \\frac{T_{p}}{T_{p}+F_{p}}$  \n\nLa recuperaci\u00f3n ($ R $) se define como el n\u00famero de verdaderos positivos ($ T_ {p} $) sobre el n\u00famero de verdaderos positivos m\u00e1s el n\u00famero de falsos negativos ($ T_ {p} + F_ {n} $):\n\n$R = \\frac{T_{p}}{T_{p}+F_{n}}$\n\n<a id='f1'><\/a>\n### Medida F1\nA menudo es conveniente combinar la precisi\u00f3n y la recuperaci\u00f3n en una \u00fanica m\u00e9trica llamada puntuaci\u00f3n $ F_ {1} $, definida como una media arm\u00f3nica ponderada de la precisi\u00f3n y la recuperaci\u00f3n:\n\n$F_{1} = 2\\times \\frac{P \\times R}{P+R}$\n\nMientras que la media regular trata todos los valores por igual, la media arm\u00f3nica da mucho m\u00e1s peso a los valores bajos. Como resultado, el clasificador solo obtendr\u00e1 una puntuaci\u00f3n F1 alta si tanto la memoria como la precisi\u00f3n son altas.\nLa puntuaci\u00f3n $ F_ {1} $ favorece a los clasificadores que tienen una precisi\u00f3n y una recuperaci\u00f3n similares. Esto no siempre es lo que desea: en algunos contextos, lo que m\u00e1s le importa es la precisi\u00f3n, y en otros contextos, realmente le importa el recuerdo.\n\n<a id='roc'><\/a>\n###  Caracter\u00edstica Operativa del Receptor\nUna caracter\u00edstica operativa del receptor (ROC), o simplemente una curva ROC, es un gr\u00e1fico que ilustra el rendimiento de un sistema clasificador binario a medida que var\u00eda su umbral de discriminaci\u00f3n. Se crea trazando la fracci\u00f3n de verdaderos positivos de los positivos (TPR = tasa de verdaderos positivos) frente a la fracci\u00f3n de falsos positivos de los negativos (FPR = tasa de falsos positivos), en varios valores de umbral. TPR tambi\u00e9n se conoce como sensibilidad, y FPR es uno menos la especificidad o tasa negativa verdadera.\nHay una compensaci\u00f3n: cuanto mayor es la recuperaci\u00f3n (TPR), m\u00e1s falsos positivos (FPR) produce el clasificador. La l\u00ednea de puntos representa la curva ROC de un clasificador puramente aleatorio; un buen clasificador permanece lo m\u00e1s lejos posible de esa l\u00ednea (hacia la esquina superior izquierda).\n\nEl \u00e1rea bajo la curva ROC, que tambi\u00e9n se denota por AUC, resume la informaci\u00f3n de la curva en un n\u00famero. El AUC debe interpretarse como la probabilidad de que un clasificador clasifique una istancia positiva elegida al azar m\u00e1s alta que una negativa elegida al azar. Un clasificador perfecto tendr\u00e1 un ROC AUC igual a 1, mientras que un clasificador puramente aleatorio tendr\u00e1 un ROC AUC igual a 0,5.\n\n<a id='confusion'><\/a>\n### Matriz de confusi\u00f3n\nLa matriz de confusi\u00f3n eval\u00faa la precisi\u00f3n de la clasificaci\u00f3n calculando la matriz de confusi\u00f3n con cada fila correspondiente a la clase verdadera. Por definici\u00f3n, la entrada $ i, j $ en una matriz de confusi\u00f3n es el n\u00famero de observaciones en realidad en el grupo $ i $, pero se predice que estar\u00e1n en el grupo $ j $. La matriz de confusi\u00f3n no se utiliza para la evaluaci\u00f3n del modelo, pero proporciona una buena comprensi\u00f3n del rendimiento general del modelo.\n\n<a id='prob'><\/a>\n### Probabilidad de clasificaci\u00f3n\nLa probabilidad de clasificaci\u00f3n proporciona una estimaci\u00f3n de la probabilidad de que una instancia determinada de los datos pertenezca a una clase determinada. En un problema de clasificaci\u00f3n binaria como el que se est\u00e1 considerando, el histograma de la probabilidad de clasificaci\u00f3n para las dos clases proporciona una buena comprensi\u00f3n visual del rendimiento del modelo. Cuanto m\u00e1s alejados est\u00e9n los picos de la probabilidad de clasificaci\u00f3n, mayor ser\u00e1 el poder de separaci\u00f3n del modelo."}}