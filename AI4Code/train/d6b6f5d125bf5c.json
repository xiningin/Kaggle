{"cell_type":{"73cd2d2b":"code","6981fc1c":"code","26595ec2":"code","16b2174e":"code","700d3537":"code","06eb2e2b":"code","2df60fbd":"code","833b4e61":"code","ef239afe":"code","870bb4a0":"code","7db1ad2d":"code","1ffa50a8":"code","a8497b7f":"code","a2c0e8c7":"code","d24fef1d":"code","f34f7811":"code","34333bbb":"code","6eaa6f04":"code","de7a953d":"code","f97eb858":"code","cfcac8de":"code","8ff8e886":"code","9790e6c7":"code","a190305d":"code","f9913eb5":"code","bd3644f6":"code","006e0fbb":"code","5723fef5":"code","cfaced42":"code","c26c2573":"code","5026c06f":"code","39224987":"code","a11542a6":"code","313ee902":"code","c10f96bd":"code","6a1c0132":"code","23aa2ecd":"code","259eecff":"code","5d21e75e":"code","8819b339":"code","1f67c1a8":"code","95eac4bd":"code","f2bd73eb":"markdown","45b71771":"markdown","65aae5b7":"markdown","2f21e20d":"markdown","fec34f58":"markdown","98720e16":"markdown","f843609b":"markdown","979abec9":"markdown","3f5f328c":"markdown","eef3561b":"markdown","5852ab34":"markdown","63e626a0":"markdown","e752eb23":"markdown","db12b0a8":"markdown","147637e2":"markdown","a00a187a":"markdown","20832a11":"markdown","6c630d4d":"markdown","6c90dc10":"markdown","6c84a81d":"markdown","55e32cd2":"markdown","ea4e759e":"markdown","3d605d69":"markdown","87a52f31":"markdown","8b221a26":"markdown","cc898523":"markdown"},"source":{"73cd2d2b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport os\n\n%matplotlib inline","6981fc1c":"import os\nprint(os.listdir(\"..\/input\/\"))","26595ec2":"df_train = pd.read_csv('..\/input\/train_labels.csv')\ndf_val   = pd.read_csv('..\/input\/val_labels.csv')\n\n# convert class to string\ndf_train['label'] = df_train['label'].astype(str)\ndf_val['label'] = df_val['label'].astype(str)\n\ndf_train.head()","16b2174e":"df_train.shape, df_val.shape","700d3537":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\n# network parameters\nsample      = 6000\nimage_size  = 256\ninput_shape = (image_size, image_size, 3)\nbatch_size  = 36\nepochs      = 50\n\nkernel_size = 3\nfilters     = 16\nlatent_dim  = 128\n\n# this is the augmentation configuration we will use for training\ntrain_datagen = ImageDataGenerator(\n                rescale=1.\/255,\n                shear_range=0.2,\n                zoom_range=0.2,\n                horizontal_flip=True)\n\n# this is the augmentation configuration we will use for testing:\n# only rescaling\ntest_datagen = ImageDataGenerator(rescale=1.\/255)","06eb2e2b":"df_train = df_train.sample(sample) \ndf_val  = df_val.sample(sample)\n\ndf_train.shape, df_val.shape","2df60fbd":"# this is a generator that will read pictures found in\n# subfolers of 'data\/train', and indefinitely generate\n# batches of augmented image data\ntrain_generator = train_datagen.flow_from_dataframe(\n                df_train,\n                x_col='img_name',\n                y_col='label',    \n                class_mode='input',\n                directory='..\/input\/train_set\/train_set\/',  # this is the target directory\n                shuffle = True,\n                target_size=(256, 256),  # all images will be resized to 150x150\n                batch_size=batch_size)  # since we use binary_crossentropy loss, we need binary labels\n\ntest_generator = test_datagen.flow_from_dataframe(\n                df_val,\n                x_col='img_name',\n                y_col='label',    \n                class_mode='input',\n                directory='..\/input\/val_set\/val_set\/',  # this is the target directory\n                shuffle = False,\n                target_size=(256, 256),  # all images will be resized to 150x150\n                batch_size=batch_size)  # since we use binary_crossentropy loss, we need binary labels\n\n\n# Predict train \ntrain_generator_inf = test_datagen.flow_from_dataframe(\n                        df_train,\n                        x_col='img_name',\n                        y_col='label',    \n                        class_mode='input',\n                        directory='..\/input\/train_set\/train_set\/',  # this is the target directory\n                        shuffle = False,\n                        target_size=(256, 256),  # all images will be resized to 150x150\n                        batch_size=batch_size)  # since we use binary_crossentropy loss, we need binary labels","833b4e61":"import numpy as np\nimport matplotlib.pyplot as plt\n\nw=10\nh=10\nfig=plt.figure(figsize=(8, 8))\ncolumns = 4\nrows = 5\nfor i in range(1, columns*rows +1):\n    img = train_generator.next()[0][0]\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img)\nplt.show()","ef239afe":"from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\n\ninput_img = Input(shape=(256, 256, 3))  # adapt this if using `channels_first` image data format\n\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\nx = MaxPooling2D((4, 4), padding='same')(x)\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = MaxPooling2D((2, 2), padding='same')(x)\n\n# at this point the representation is (4, 4, 8) i.e. 128-dimensional\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((4, 4))(x)\ndecoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = Model(input_img, decoded)\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n\nautoencoder.summary()","870bb4a0":"# Save best model\nfrom keras.callbacks import TensorBoard\n\nfilepath       = \"base-weights.hdf5\"\ncheckpoint     = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]","7db1ad2d":"hist = autoencoder.fit_generator(train_generator,\n                                steps_per_epoch=len(train_generator),\n                                validation_data=test_generator,\n                                validation_steps=len(test_generator),\n                                callbacks=callbacks_list,                                 \n                                epochs=epochs)","1ffa50a8":"def plt_hist(hist):\n    # summarize history for loss\n    plt.plot(hist.history['loss'])\n    plt.plot(hist.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    \nplt_hist(hist)","a8497b7f":"# Predict Embedding values\n# Only encode model\nencoder = Model(input_img, autoencoder.layers[8].output)\nencoder.summary()","a2c0e8c7":"def predict_generator(model, gen):\n    # Predict latent-space map\n    pred_maps = encoder.predict_generator(test_generator,steps = len(test_generator))\n    # Predict latent-space vector\n    pred_enb = pred_maps.reshape(df_val.shape[0], -1)\n    return pred_enb","d24fef1d":"# Predict latent-space vector\npred_enb = predict_generator(encoder, test_generator)\npred_enb.shape","f34f7811":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\ndef plt_reduce_space(x, color=[]):\n    '''\n    Plot Scatter with color\n    '''\n    plt.figure(figsize=(8, 8))\n    plt.scatter(x[:, 0], x[:, 1], c=color,\n            alpha=.4, s=3**2, cmap='viridis')\n    plt.show()","34333bbb":"# Reduce dimmension\nenb_reduce = TSNE(n_components=2).fit_transform(pred_enb)","6eaa6f04":"# Plot with merchant_category_id color\nplt_reduce_space(enb_reduce, df_val.label.values.astype(int))","de7a953d":"imgs      = train_generator.next()[0]\nimgs_pred = autoencoder.predict(imgs)","f97eb858":"def plot_images(imgs, imgs_pred):\n    fig=plt.figure(figsize=(10, 20))\n    columns = 2\n    rows    = 8\n\n    for i in range(1, 9, 2):\n        fig.add_subplot(rows, columns, i)\n        plt.imshow(imgs[i])\n        \n        fig.add_subplot(rows, columns, i+1)\n        plt.imshow(imgs_pred[i])\n    plt.show()\nplot_images(imgs, imgs_pred)","cfcac8de":"# train\ntrain_enb = predict_generator(encoder, train_generator_inf)\ndf_emb_train = pd.DataFrame(train_enb)\ndf_emb_train['img_name'] = df_train.img_name.values\n\n# Val\nval_enb = pred_enb#predict_generator(encoder, test_generator)\ndf_emb_val = pd.DataFrame(val_enb)\ndf_emb_val['img_name'] = df_val.img_name.values\n\ndf_emb_train.shape, df_emb_val.shape","8ff8e886":"df_emb_val.head()","9790e6c7":"df_emb_train.to_csv('df_emb_train.csv')\ndf_emb_val.to_csv('df_emb_val.csv')","a190305d":"from keras.layers import Lambda, Input, Dense\nfrom keras.models import Model\nfrom keras.datasets import mnist\nfrom keras.losses import mse, binary_crossentropy\nfrom keras.utils import plot_model\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Input, Dense, Lambda, Layer, Add, Multiply\nfrom keras.models import Model, Sequential\n\nimport argparse\nimport os","f9913eb5":"\n# reparameterization trick\n# instead of sampling from Q(z|X), sample eps = N(0,I)\n# then z = z_mean + sqrt(var)*eps\ndef sampling(args):\n    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n\n    # Arguments\n        args (tensor): mean and log of variance of Q(z|X)\n\n    # Returns\n        z (tensor): sampled latent vector\n    \"\"\"\n\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    # by default, random_normal has mean=0 and std=1.0\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\nclass KLDivergenceLayer(Layer):\n\n    \"\"\" Identity transform layer that adds KL divergence\n    to the final model loss.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.is_placeholder = True\n        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n\n    def call(self, inputs):\n\n        mu, log_var = inputs\n\n        kl_batch = - .5 * K.sum(1 + log_var -\n                                K.square(mu) -\n                                K.exp(log_var), axis=-1)\n\n        self.add_loss(K.mean(kl_batch), inputs=inputs)\n\n        return inputs","bd3644f6":"from keras.layers import Conv2D, Flatten, Lambda\nfrom keras.utils import plot_model\nfrom keras.layers import Reshape, Conv2DTranspose\n\n# VAE model = encoder + decoder\n# build encoder model\ninputs = Input(shape=input_shape, name='encoder_input')\nx = inputs\nfor i in range(2):\n    filters *= 2\n    x = Conv2D(filters=filters, kernel_size=kernel_size,\n               activation='relu', strides=2,\n               padding='same')(x)\n\n# shape info needed to build decoder model\nshape = K.int_shape(x)\n\n# generate latent vector Q(z|X)\nx = Flatten()(x)\nx = Dense(16, activation='relu')(x)\nz_mean    = Dense(latent_dim, name='z_mean')(x)\nz_log_var = Dense(latent_dim, name='z_log_var')(x)\nz_mean, z_log_var = KLDivergenceLayer()([z_mean, z_log_var])\n\n# use reparameterization trick to push the sampling out as input\n# note that \"output_shape\" isn't necessary with the TensorFlow backend\nz = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n# instantiate encoder model\nencoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\nprint(\"Encoder\")\nencoder.summary()\n#plot_model(encoder, to_file='vae_cnn_encoder.png', show_shapes=True)\n\n# build decoder model\nlatent_inputs = Input(shape=(latent_dim,), name='z_sampling')\nx = Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs)\nx = Reshape((shape[1], shape[2], shape[3]))(x)\n\nfor i in range(2):\n    x = Conv2DTranspose(filters=filters, kernel_size=kernel_size,\n                        activation='relu', strides=2,\n                        padding='same')(x)\n    filters \/\/= 2\n\noutputs = Conv2DTranspose(filters=3,\n                          kernel_size=kernel_size, activation='sigmoid',\n                          padding='same', name='decoder_output')(x)\n\n# instantiate decoder model\ndecoder = Model(latent_inputs, outputs, name='decoder')\nprint(\"Decoder\")\ndecoder.summary()\n\n# instantiate VAE model\noutputs = decoder(encoder(inputs)[2])\n\n# VAE\nvae = Model(inputs, outputs, name='vae')\nprint(\"VAE\")\nvae.summary()","006e0fbb":"# reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n# reconstruction_loss *= image_size * image_size\n# kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n# kl_loss = K.sum(kl_loss, axis=-1)\n# kl_loss *= -0.5\n# vae_loss2 = K.mean(reconstruction_loss + kl_loss)\n\n# def vae_loss(x, x_decoded_mean):\n#     xent_loss = binary_crossentropy(x, x_decoded_mean)\n#     kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n#     return xent_loss + kl_loss\n\ndef nll(y_true, y_pred):\n    \"\"\" Negative log likelihood (Bernoulli). \"\"\"\n\n    lh = K.tf.distributions.Bernoulli(probs=y_pred)\n\n    return - K.sum(lh.log_prob(y_true), axis=-1)","5723fef5":"vae.compile(optimizer='adam', loss=nll)","cfaced42":"filepath       = \"base-vae-weights.hdf5\"\ncheckpoint     = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]","c26c2573":"hist = vae.fit_generator(train_generator,\n                        steps_per_epoch=len(train_generator),\n                        validation_data=test_generator,\n                        validation_steps=len(test_generator),\n                        callbacks=callbacks_list,                                 \n                        epochs=epochs)","5026c06f":"plt_hist(hist)","39224987":"# encoder, from inputs to latent space\nencoder = Model(inputs, z)","a11542a6":"def predict_generator(model, gen):\n    # Predict latent-space map\n    pred_maps = encoder.predict_generator(test_generator,steps = len(test_generator))\n    # Predict latent-space vector\n    #mean, var, z = np.array(pred_maps)\n    return pred_maps","313ee902":"# Predict latent-space vector\npred_enb = predict_generator(encoder, test_generator)\npred_enb.shape","c10f96bd":"# Reduce dimmension\nenb_reduce = TSNE(n_components=2).fit_transform(pred_enb)","6a1c0132":"# Plot with merchant_category_id color\nplt_reduce_space(enb_reduce, df_val.label.values.astype(int))","23aa2ecd":"imgs      = train_generator.next()[0]\nimgs_pred = vae.predict(imgs)","259eecff":"def plot_images(imgs, imgs_pred):\n    fig=plt.figure(figsize=(10, 20))\n    columns = 2\n    rows    = 8\n\n    for i in range(1, 9, 2):\n        fig.add_subplot(rows, columns, i)\n        plt.imshow(imgs[i])\n        \n        fig.add_subplot(rows, columns, i+1)\n        plt.imshow(imgs_pred[i])\n    plt.show()\nplot_images(imgs, imgs_pred)","5d21e75e":"# train\ntrain_enb    = predict_generator(encoder, train_generator_inf)\ndf_emb_train = pd.DataFrame(train_enb)\ndf_emb_train['img_name'] = df_train.img_name.values\n\n# Val\nval_enb    = pred_enb#predict_generator(encoder, test_generator)\ndf_emb_val = pd.DataFrame(val_enb)\ndf_emb_val['img_name'] = df_val.img_name.values\n\ndf_emb_train.shape, df_emb_val.shape","8819b339":"df_emb_train.to_csv('df_vae_emb_train.csv')\ndf_emb_val.to_csv('df_vae_emb_val.csv')","1f67c1a8":"df_emb_val.head()","95eac4bd":"df_val.img_name.shape","f2bd73eb":"### What is Embedding ?","45b71771":"## Training Simple AutoEncoder (AE)\n![](https:\/\/blog.keras.io\/img\/ae\/autoencoder_schema.jpg)\nhttps:\/\/blog.keras.io\/building-autoencoders-in-keras.html","65aae5b7":"**How does a variational autoencoder work?**\n\nFirst, an encoder network turns the input samples x into two parameters in a latent space, which we will note *z_mean* and *z_log_sigma*. Then, we randomly sample similar points z from the latent normal distribution that is assumed to generate the data, via *z = z_mean + exp(z_log_sigma) * epsilon*, where epsilon is a random normal tensor. Finally, a decoder network maps these latent space points back to the original input data.\n\nThe parameters of the model are trained via two loss functions: a reconstruction loss forcing the decoded samples to match the initial inputs (just like in our previous autoencoders), and the KL divergence between the learned latent distribution and the prior distribution, acting as a regularization term. You could actually get rid of this latter term entirely, although it does help in learning well-formed latent spaces and reducing overfitting to the training data.","2f21e20d":"### Save Embedding\n\nJoin embedding with image food and save pandas","fec34f58":"## Visualization of latent space\n\nSince our latent space is not two-dimensional, we will use PCA to reduce dimensionality, so we can use some interesting visualizations that can be made at this point. One is to look at the neighborhoods of different classes in the latent 2D plane:","98720e16":"![](https:\/\/www.jeremyjordan.me\/content\/images\/2018\/03\/Screen-Shot-2018-03-18-at-12.24.19-AM.png)","f843609b":"![](http:\/\/vision.cornell.edu\/se3\/wp-content\/uploads\/2014\/09\/human-cvpr-badembedding.jpg)\nhttps:\/\/jaan.io\/food2vec-augmented-cooking-machine-intelligence\/\nhttps:\/\/jaan.io\/files\/food2vec_recipe_embeddings_tsne.html","979abec9":"So a natural language modelling technique like Word Embedding is used to map words or phrases from a vocabulary to a corresponding vector of real numbers. As well as being amenable to processing by learning algorithms, this vector representation has two important and advantageous properties:\n\n* **Dimensionality Reduction**\u200a\u2014\u200ait is a more efficient representation\n* **Contextual Similarity**\u200a\u2014\u200ait is a more expressive representation","3f5f328c":"## Training Variational autoencoder (VAE)\n\nThe VAE has a modular design. The encoder, decoder and VAE are 3 models that share weights. After training the VAE model, the encoder can be used to generate latent vectors\n\nhttps:\/\/keras.io\/examples\/variational_autoencoder_deconv\/\n\n[1] Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" https:\/\/arxiv.org\/abs\/1312.6114\n\n","eef3561b":"Train model autoenconder... ","5852ab34":"### What is Variational autoencoder (VAE)\n\n* https:\/\/www.jeremyjordan.me\/variational-autoencoders\/\n* https:\/\/blog.keras.io\/building-autoencoders-in-keras.html\n\nA variational autoencoder (VAE) provides a probabilistic manner for describing an observation in latent space. Thus, rather than building an encoder which outputs a single value to describe each latent state attribute, we'll formulate our encoder to describe a probability distribution for each latent attribute.","63e626a0":"# Variational AutoEncoder to create Embedding of Food\n\nIn this notebook, I will use a AutoEncoder (AE) and Variational AutoEncoder (VAE) to create a Food Embedding. This information can be used in ML algotithms with higher semantic quality and similarity betweeen Foods.\n\n* **Introduction**\n    * What is Embedding ?\n    * How to use Food Embedding ?\n    * What is Variational autoencoder (VAE)\n* **Data Preparation**\n    * Load Dataset\n    * Create Generate\n* **Training Simple AutoEncoder (AE)**\n    * Visualization of latent space\n* **Training VAE**\n    * Visualization of latent space\n","e752eb23":"An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an embedding captures some of the semantics of the input by placing semantically similar inputs close together in the embedding space. An embedding can be learned and reused across models.","db12b0a8":"## Predict Latend-Space","147637e2":"### Save Embedding\n\nJoin embedding with image food and save pandas","a00a187a":"The \"Sample from distributions\" it's owr Embedding Layer. I will encoder all Merchants and take a Embedding Layer.","20832a11":"We can use the Embedding as input of the model, containing a reduced dimensionality but with much semantic information. ","6c630d4d":"### Train","6c90dc10":"## Visualization of latent space\n\nSince our latent space is not two-dimensional, we will use PCA to reduce dimensionality, so we can use some interesting visualizations that can be made at this point. One is to look at the neighborhoods of different classes in the latent 2D plane:","6c84a81d":"### Vizualize Autoencoder Result\n\nPlot original imagem and predicted image from the autoencoder.","55e32cd2":"## Data Preparation\n\nThis session transform the variables from the original dataset, corrects missing values and normalizes the data for training. ","ea4e759e":"????","3d605d69":"##### continue....","87a52f31":"### Vizualize Autoencoder Result\n\nPlot original imagem and predicted image from the autoencoder.","8b221a26":"## Introduction","cc898523":"#### How to use Food Embedding ?"}}