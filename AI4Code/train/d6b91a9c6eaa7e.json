{"cell_type":{"9874f1ac":"code","40e28080":"code","7a36dc15":"code","24e8c916":"code","aa84cb47":"code","6b9af68f":"code","44ee4c1f":"code","3b2ee9e9":"code","a2dc84ab":"code","36aa6625":"code","4835577a":"code","3c9e6c11":"code","e1764813":"code","64c70527":"code","2c004644":"code","e86f933e":"code","40398ef9":"code","be88f646":"code","c6ebabbc":"code","174060b6":"code","e7e37eee":"code","71bf499e":"code","24ba3c49":"code","8549d457":"code","3357f6c3":"code","69a5ddeb":"code","0c11fc98":"code","42ccd3d0":"code","b77cda9a":"code","8e27cfed":"code","9dac63fc":"code","d3355958":"code","95c1254b":"markdown","f36def79":"markdown","1acf78bb":"markdown","690db657":"markdown","bc0105b7":"markdown","d9616829":"markdown","771440b9":"markdown","b0d095aa":"markdown","b0c7c376":"markdown","5509d3c6":"markdown","28ed7061":"markdown","cd83e27f":"markdown","ee1e94db":"markdown","c2c8ae38":"markdown","e1e6c21c":"markdown","213df250":"markdown","9c2d1c7f":"markdown","363a16c9":"markdown","8f8a0dd7":"markdown","360ce2a1":"markdown","b9153bda":"markdown","f0c9baca":"markdown","4ce42ab3":"markdown","724eafd0":"markdown","4f714000":"markdown","da04823c":"markdown","2266b286":"markdown","6345a949":"markdown","71dc895f":"markdown","dc019c50":"markdown"},"source":{"9874f1ac":"!pip install --upgrade pip","40e28080":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy\nimport scipy.sparse\n\nimport networkx as nx # creates graph data-structures\n\n\n# plot with matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#from plotnine import * # used to plot data\n\n# progress bar\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a36dc15":"import torch; print(torch.__version__)","24e8c916":"print(torch.version.cuda)","aa84cb47":"%%time\n\n# install pytorch libraries\n!pip install torch-scatter -f https:\/\/pytorch-geometric.com\/whl\/torch-1.7.0+cu110.html\n!pip install torch-sparse -f https:\/\/pytorch-geometric.com\/whl\/torch-1.7.0+cu110.html\n!pip install torch-cluster -f https:\/\/pytorch-geometric.com\/whl\/torch-1.7.0+cu110.html\n!pip install torch-spline-conv -f https:\/\/pytorch-geometric.com\/whl\/torch-1.7.0+cu110.html\n!pip install torch-geometric","6b9af68f":"from sklearn import preprocessing\n\nimport torch\nfrom torch_geometric.utils.convert import from_networkx\nfrom torch_geometric.data import InMemoryDataset, DataLoader\nfrom torch_geometric.data import Data\n","44ee4c1f":"marker_presence_matrix_file=\"\/kaggle\/input\/metagenomics-marker-presence-sparse-matrix\/marker_presence_matrix.npz\"\nmarkers2clades_DB_file=\"\/kaggle\/input\/human-metagenomics\/markers2clades_DB.csv\"\nabundance_file=\"\/kaggle\/input\/human-metagenomics\/abundance.csv\"\nmarker_presence_table_file=\"\/kaggle\/input\/human-metagenomics\/marker_presence.csv\"","3b2ee9e9":"%%time\n\n# import only the columns with the sample meta-data information\nsamples_df = pd.read_csv(abundance_file,\n                         sep=\",\", dtype=object,usecols=range(0,210))","a2dc84ab":"# print a table of the different dataset\n# and disease combinations\npd.DataFrame(samples_df.loc[:,['dataset_name','pubmedid','disease']].value_counts()).sort_values('dataset_name')","36aa6625":"t2dml_samples_df = samples_df.loc[samples_df['dataset_name']==\"t2dmeta_long\",:].copy()","4835577a":"t2dml_samples_df = t2dml_samples_df.replace(\"nd\", np.NaN)\nt2dml_samples_df = t2dml_samples_df.replace(\"na\", np.NaN)\nt2dml_samples_df = t2dml_samples_df.replace(\"-\", np.NaN)\nt2dml_samples_df = t2dml_samples_df.replace(' -', np.NaN)\nt2dml_samples_df = t2dml_samples_df.replace('unknown', np.NaN)","3c9e6c11":"# change the if statement to visualize\nif 1==0:\n    for col in t2dml_samples_df.loc[:, t2dml_samples_df.nunique() == 1].columns:\n        t2dml_samples_df[col].fillna(\"NaN\").value_counts().sort_values().plot(\n            kind = 'bar', title=col)\n        plt.show()\n        \nt2dml_samples_df = t2dml_samples_df.loc[:, t2dml_samples_df.nunique() > 1].copy()\nt2dml_samples_df.columns","e1764813":"bool_vals={'True':1,\n          'False':-1,\n          'Null':0}\nfor col in t2dml_samples_df.loc[:, t2dml_samples_df.nunique() < 4]:\n    if (\"yes\" in t2dml_samples_df[col].unique() and \"no\" in t2dml_samples_df[col].unique()):\n            t2dml_samples_df[col] = t2dml_samples_df[col].fillna(bool_vals['Null'])\n            t2dml_samples_df =t2dml_samples_df.replace({col: {'yes': bool_vals['True'], 'no': bool_vals['False']}})\n    elif (\"y\" in t2dml_samples_df[col].unique() and \"n\" in t2dml_samples_df[col].unique()):\n            t2dml_samples_df[col] = t2dml_samples_df[col].fillna(bool_vals['Null'])\n            t2dml_samples_df =t2dml_samples_df.replace({col: {'y': bool_vals['True'], 'n': bool_vals['False']}})\n    elif (\"positive\" in t2dml_samples_df[col].unique() and \"negative\" in t2dml_samples_df[col].unique()):\n            t2dml_samples_df[col] = t2dml_samples_df[col].fillna(bool_vals['Null'])\n            t2dml_samples_df =t2dml_samples_df.replace({col: {'positive': bool_vals['True'], 'negative': bool_vals['False']}})\n    elif (\"a\" in t2dml_samples_df[col].unique() and \"u\" in t2dml_samples_df[col].unique()):\n            t2dml_samples_df[col] = t2dml_samples_df[col].fillna(bool_vals['Null'])\n            t2dml_samples_df =t2dml_samples_df.replace({col: {'a': bool_vals['True'], 'u': bool_vals['False']}})","64c70527":"for col in t2dml_samples_df.loc[:, t2dml_samples_df.nunique() == 2].columns:\n    if (not(True in t2dml_samples_df[col].unique() and \n             False in t2dml_samples_df[col].unique())):\n        val_i = 0\n        first_val_null=True\n        first_val = np.NaN\n        while (first_val_null):\n            first_val = t2dml_samples_df[col].unique()[val_i]\n            if first_val == first_val:\n                first_val_null = False\n            else:\n                val_i += 1\n        val_i += 1\n        second_val_null=True\n        second_val= np.NaN\n        while (second_val_null):\n            second_val = t2dml_samples_df[col].unique()[val_i]\n            if second_val == second_val:\n                second_val_null = False\n            else:\n                val_i += 1\n        new_col_name=(\"%s:%s|%s\" % (col,first_val, second_val))\n        # change the column name\n        t2dml_samples_df = (t2dml_samples_df.rename(\n            columns={col:new_col_name}))\n        # change values in the column\n        t2dml_samples_df[new_col_name] = t2dml_samples_df[new_col_name].fillna(bool_vals['Null'])\n        t2dml_samples_df =t2dml_samples_df.replace({new_col_name: {first_val: bool_vals['False'],\n                                                       second_val: bool_vals['True']}})\ncategorical_cols=t2dml_samples_df.loc[:, t2dml_samples_df.nunique() < 20].columns","2c004644":"%%time\n\n# read in abundance values from abundance file\nabundance_df = pd.read_csv(abundance_file,sep=\",\", dtype=object).iloc[:,211:]\n# collect columns with abundance values\nabundance_cols = list(abundance_df.columns)\n# filter for only samples from the `t2dmeta_long` dataset\nt2dml_abundance_values_df = abundance_df.iloc[(\n    list(t2dml_samples_df.index))]\n# merge meta-data features with abundance features\nt2dml_abundance_df = t2dml_samples_df.merge(\n    abundance_df, how='inner',left_index=True, right_index=True)\nt2dml_abundance_df.shape","e86f933e":"graph_dict = {} # dictionary of graphs per sample from the t2dmeta_long dataset\n#n2v_dict={} # dictionary of vectors from graphs per sample\nfor i in list(t2dml_abundance_values_df.index):# single edge as tuple of two nodes range(len(abundance_df)):\n    G = nx.DiGraph()\n    for tree_str, abundance_val in t2dml_abundance_values_df.loc[i].to_dict().items():\n        if float(abundance_val) > 0:\n            bacteria_list = tree_str.split('|')\n            G.add_node(bacteria_list[-1],amount=abundance_val)\n            if len(bacteria_list) > 1:\n                G.add_edge(*bacteria_list[-2:]) # add single edge as tuple of two nodes\n    graph_dict[i]=G\n    #n2v_dict[i] = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)","40398ef9":"class t2dmlDataset(InMemoryDataset):\n    def __init__(self, root, transform=None, pre_transform=None):\n        super(t2dmlDataset, self).__init__(root, transform, pre_transform)\n        self.data, self.slices = torch.load(self.processed_paths[0])\n\n    @property\n    def raw_file_names(self):\n        return []\n    @property\n    def processed_file_names(self):\n        return ['data.pt']\n\n    def download(self):\n        pass\n    \n    def process(self):\n        samples_df = t2dml_samples_df\n        abundance_df = t2dml_abundance_values_df\n        data_list = []\n        \n        # generate output directory\n        outdir = os.path.dirname(self.processed_paths[0])\n        if not os.path.isdir(outdir):\n            os.mkdir(outdir)\n\n        # get embedding for target values (y)\n        le = preprocessing.LabelEncoder()\n        y = le.fit_transform(samples_df['disease:n|t2d'])\n\n        # get emnedding of all nodes\n        le_nodes = preprocessing.LabelEncoder()\n\n        # encode labels between 0 and n_classes-1 for each bacterial label\n        le_nodes.fit([col.split('|')[-1] for col in  abundance_df.columns])\n        \n        # process by each sample (row)\n        row_range = range(len(abundance_df))\n        for i in tqdm(row_range):\n            node_list = [] # list of [$cur_bacteria_name,$abundance_val]\n            edge_list = [] # list of [$parent_bacteria_name,$cur_bacteria_name]\n            for key, val in abundance_df.iloc[i].to_dict().items():\n                if float(val) > 0:\n                    bacteria_list = key.split('|')\n                    # the smallest abundance value is 3.0000e-05\n                    # but I need them all be ints. Therefore to \n                    # convert all the abundance values into\n                    # integers I scale them all up by 10e4 \n                    node = [le_nodes.transform([bacteria_list[-1]])[0],float(val)*10e4]\n                    node_list.append(node)\n                    if len(bacteria_list) >= 2:\n                        edge_list.append(le_nodes.transform(bacteria_list[-2:]))\n            # convert `y`, `node_list`, and `edge_list` into Tensor formats\n            edge_array = np.array(edge_list)\n            edge_index = torch.tensor([edge_array[:,0],edge_array[:,1]],dtype=torch.long)\n            node_features = torch.LongTensor(np.array(node_list))\n            label = torch.FloatTensor([y[i]])\n            # set these Tensors into a pytorch Data() object\n            # which is used to model graphs\n            data = Data(node_features,edge_index=edge_index,y=label)\n            data_list.append(data)\n        \n            \n        data, slices = self.collate(data_list)\n        torch.save((data, slices), self.processed_paths[0])","be88f646":"%%time\n\ndataset = t2dmlDataset('..\/working\/human-metagenomics')","c6ebabbc":"!ls ..\/working\/human-metagenomics\/processed","174060b6":"(dataset.data)","e7e37eee":"dataset.data.edge_index","71bf499e":"a = torch.FloatTensor([[1,2],[0.5,0.105]])\nprint(a.type())\nb = a.long()\nprint(b.type())\nb","24ba3c49":"dataset = dataset.shuffle()\ntrain_datalist = dataset[len(t2dml_abundance_values_df) \/\/ 10:]\ntest_datalist = dataset[:len(t2dml_abundance_values_df) \/\/ 10]","8549d457":"train_loader = DataLoader(train_datalist, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_datalist, batch_size=4)","3357f6c3":"#embed_dim = 32\n#CUDA_LAUNCH_BLOCKING=1\nfrom torch.nn import Sequential as Seq, Linear, ReLU\nfrom torch_geometric.utils import remove_self_loops, add_self_loops\nfrom torch_geometric.nn import TopKPooling,MessagePassing\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nimport torch.nn.functional as F\n\nclass SAGEConv(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n            super(SAGEConv, self).__init__(aggr='max') #  \"Max\" aggregation.\n            # each node is multiplied by a weight matrix, \n            # added a bias and passed through an \n            # activation function\n            self.lin = torch.nn.Linear(in_channels, out_channels)\n            self.act = torch.nn.ReLU()\n            self.update_lin = torch.nn.Linear(in_channels + out_channels, in_channels, bias=False)\n            self.update_act = torch.nn.ReLU()\n\n    def forward(self, x, edge_index):\n        # x has shape [N, in_channels]\n        # edge_index has shape [2, E]\n\n\n        edge_index, _ = remove_self_loops(edge_index)\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n\n\n        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n\n    def message(self, x_j):\n        # x_j has shape [E, in_channels]\n\n        x_j = self.lin(x_j)\n        x_j = self.act(x_j)\n        \n        return x_j\n\n    def update(self, aggr_out, x):\n        # aggr_out has shape [N, out_channels]\n\n\n        new_embedding = torch.cat([aggr_out, x], dim=1)\n        \n        new_embedding = self.update_lin(new_embedding)\n        new_embedding = self.update_act(new_embedding)\n        \n        return new_embedding\n\n\n","69a5ddeb":"from torch_geometric.nn import TopKPooling\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nimport torch.nn.functional as F\nclass Net(torch.nn.Module):\n    def __init__(self, embed_dim):\n        super(Net, self).__init__()\n\n        self.conv1 = SAGEConv(embed_dim, 128)\n        self.pool1 = TopKPooling(128, ratio=0.8)\n        self.conv2 = SAGEConv(128, 128)\n        self.pool2 = TopKPooling(128, ratio=0.8)\n        self.conv3 = SAGEConv(128, 128)\n        self.pool3 = TopKPooling(128, ratio=0.8)\n        # embedding object containing X Tensors of\n        # size Y where X equal the number of edges\n        # and Y equals $embed_dim\n        self.item_embedding = (\n            torch.nn.Embedding(\n                num_embeddings=(dataset.data.edge_index.shape[1]), \n                embedding_dim=embed_dim))\n        self.lin1 = torch.nn.Linear(256, 128)\n        self.lin2 = torch.nn.Linear(128, 64)\n        self.lin3 = torch.nn.Linear(64, 1)\n        self.bn1 = torch.nn.BatchNorm1d(128)\n        self.bn2 = torch.nn.BatchNorm1d(64)\n        self.act1 = torch.nn.ReLU()\n        self.act2 = torch.nn.ReLU()        \n  \n    def forward(self, data):\n        # get contents of data object. Recall:\n        # * The `edge_index` has shape 2 x E where `E` \n        # is the number of edges in a network\n        # * The `x` has shape nspecies x 2 where `nspecies`\n        # equals the number of bacteria species with abundance\n        # values\n        # * `y` length is equal to the number of samples\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = self.item_embedding(x)\n        x = x.squeeze(1)        \n\n        # run forward(self, x, edge_index) on self.conv1\n        # which has $embed_dim in_channels\n        # and 128 out_channels\n        x = F.relu(self.conv1(x, edge_index))\n\n        x, edge_index, _, batch, _ = self.pool1(x, edge_index, None, batch)\n        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n\n        x = F.relu(self.conv2(x, edge_index))\n     \n        x, edge_index, _, batch, _ = self.pool2(x, edge_index, None, batch)\n        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n\n        x = F.relu(self.conv3(x, edge_index))\n\n        x, edge_index, _, batch, _ = self.pool3(x, edge_index, None, batch)\n        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n\n        x = x1 + x2 + x3\n\n        x = self.lin1(x)\n        x = self.act1(x)\n        x = self.lin2(x)\n        x = self.act2(x)      \n        x = F.dropout(x, p=0.5, training=self.training)\n\n        x = torch.sigmoid(self.lin3(x)).squeeze(1)\n\n        return x","0c11fc98":"def train(model):\n    model.train()\n\n    loss_all = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        # run forward(self, data) on `model`\n        output = model(data)\n        label = data.y.to(device)\n        loss = F.nll_loss(output, data.y)\n        loss.backward()\n        loss_all += data.num_graphs * loss.item()\n        optimizer.step()\n    return loss_all \/ len(train_dataset)","42ccd3d0":"def evaluate(loader):\n    model.eval()\n\n    predictions = []\n    labels = []\n\n    with torch.no_grad():\n        for data in loader:\n\n            data = data.to(device)\n            pred = model(data).detach().cpu().numpy()\n\n            label = data.y.detach().cpu().numpy()\n            predictions.append(pred)\n            labels.append(label)\n\n","b77cda9a":"embed_dim = 128\ndevice = torch.device('cuda')\nmodel = Net(embed_dim).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n\n\nfor epoch in range(30):\n    loss = train(model)\n    train_acc = evaluate(train_loader)\n    val_acc = evaluate(val_loader)    \n    test_acc = evaluate(test_loader)\n    print('Epoch: {:03d}, Loss: {:.5f}, Train Auc: {:.5f}, Val Auc: {:.5f}, Test Auc: {:.5f}'.\n          format(epoch, loss, train_acc, val_acc, test_acc))\n\n","8e27cfed":"# return a list of pytorch Data() objects which each contain a graph\n# for each row in abundance_df\ndef graph_label(samples_df,abundance_df):\n    \n    # get embedding for target values (y)\n    le = preprocessing.LabelEncoder()\n    y = le.fit_transform(samples_df['disease:n|t2d'])\n    print(y)\n    \n    # get emnedding of all nodes\n    le_nodes = preprocessing.LabelEncoder()\n    \n    # encode labels between 0 and n_classes-1 for each bacterial label\n    le_nodes.fit([col.split('|')[-1] for col in  abundance_df.columns])\n    data_list = []\n    \n    # process by each sample (row)\n    for i in range(len(abundance_df)):\n        node_list = [] # list of [$cur_bacteria_name,$abundance_val]\n        edge_list = [] # list of [$parent_bacteria_name,$cur_bacteria_name]\n        for key, val in abundance_df.iloc[i].to_dict().items():\n            if float(val) > 0:\n                bacteria_list = key.split('|')\n                node = [le_nodes.transform([bacteria_list[-1]])[0],float(val)]\n                node_list.append(node)\n                if len(bacteria_list) >= 2:\n                    edge_list.append(le_nodes.transform(bacteria_list[-2:]))\n        # convert `y`, `node_list`, and `edge_list` into Tensor formats\n        edge_array = np.array(edge_list)\n        edge_index = torch.tensor([edge_array[:,0],edge_array[:,1]],dtype=torch.long)\n        #print(np.array(node_list))\n        node_features = torch.LongTensor(np.array(node_list))\n        label = torch.FloatTensor([y[i]])\n        # set these Tensors into a pytorch Data() object\n        # which is used to model graphs\n        data = Data(node_features,edge_index=edge_index,y=label)\n        data_list.append(data)\n    return data_list","9dac63fc":"data_list = graph_label(t2dml_samples_df,t2dml_abundance_values_df)\ntrain_datalist = data_list[len(data_list) \/\/ 10:]\ntest_datalist = data_list[:len(data_list) \/\/ 10]\n\ntrain_loader = DataLoader(train_datalist, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_datalist, batch_size=4)","d3355958":"%%time \n# Import this file without the first 211 columns (since we already dealt with those previously). \n# This file could be imported as a sparse numpy matrix. it is very large.\nif 1==0:\n    markers_reader = pd.read_csv(\n            marker_presence_table_file,\n            sep=\",\", \n            dtype=object,\n            usecols=range(211,288558),\n            nrows=10)\n    print(markers_reader.head())","95c1254b":"Print the datalist object","f36def79":"Now that we loaded the data, it is time to run some models! See this article for more info: https:\/\/towardsdatascience.com\/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8","1acf78bb":"To start, let's focus on the `t2dmeta_long` dataset.","690db657":"Next we need to embed these graphs into an architecture that can be imported into a neural network. ","bc0105b7":"### Method 2: Generate custom pytorch Dataset\nFirst we create out dataset using [PyTorch Geometric](https:\/\/pytorch-geometric.readthedocs.io\/en\/latest\/notes\/create_dataset.html) and the abstract class `torch_geometric.dad.InMemoryDataset`.\n\nNote that the real magic of the 't2dmlDataset' class is in the `process()` function. To create graphs for the abundance data we use [Data()](https:\/\/pytorch-geometric.readthedocs.io\/en\/latest\/_modules\/torch_geometric\/data\/data.html#Data) objects.\n\nData() objects require three parameters:\n1. **x**: a torch.LongTensor() containing an numpy array of 2D lists. The 2D list has the attributes\/features associated with each node. In our case, we set `x` to `node_features` whose attributes are the bacteria IDs given by the column name and the features are the abundance values. \n2. **edge_index**: a torch.tensor() of data type `torch.long` containing an array of 2 lists.The first list contains the parent IDs of each node and the second list has the child bacteria IDs with respect to the first list.\n3. **y**: a FloatTensor() which contains the list of target values for each sample","d9616829":"Ideas for the next step include converting each graph into a vector using the `Node2vec` constructor. See the documentation at: https:\/\/github.com\/eliorc\/node2vec. \n\nAnother idea includes using a PopPhy-CNN. A published architecture found at: https:\/\/doi.org\/10.1101\/257931. The github repo is at https:\/\/github.com\/derekreiman\/PopPhy-CNN.","771440b9":"Here we can build the dataset.","b0d095aa":"## Building models based on abundance file\nThe abundance data values are reported for different taxonomic ranks (kingdom, phylum, division, class, order, family, genus, and species) of the bacteria phylogenetic tree. The values, which are formatted into a table structure,would make more sense to a machine in a tree format. Therefore, we convert these values into a graph object using the `networkx` libary.","b0c7c376":"Now that we loaded the data, it is time to run some models! See this article for more info: https:\/\/towardsdatascience.com\/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8","5509d3c6":"Similarly, for columns that contain 2 values (not including null) I will convert the values to numbers. For example, I will change the column named \"gender\" to \"gender:Female|Male\". The values will be 1 for Female, 2 for Male, and 0 for null.","28ed7061":"I want to convert some columns into booleans. For example if the values are either:\n* \"yes\",\"no\", or null\n* \"y\",\"n\", or null\n* \"positve\", \"negative\", or null\n* \"a\"(affected), \"u\" (unaffected), or null\n\nI want to convert them into `2`, `1`, and `0` respectively.","cd83e27f":"After building the dataset, we call shuffle() to make sure it has been randomly shuffled and then split it into three sets for training, validation, and testing. We split the data so that 10% is in the training set and 90% is in the testing set. Then we use the DataLoader() function to import these datasets into pytorch.","ee1e94db":"### Method 3: List of pytorch Data() objects","c2c8ae38":"Different datasets were focused on specific disease and non-disease types","e1e6c21c":"# Introduction\n\nIn this notebook we explore metagenomics data. This dataset was created by the team of Edoardo Pasolli, Duy Tin Truong, Faizan Malik, Levi Waldron, and Nicola Segata; they published [a research article in July of 2016](https:\/\/journals.plos.org\/ploscompbiol\/article?id=10.1371\/journal.pcbi.1004977). The authors used 8 publicly available metagenomic datasets, and applied [MetaPhlAn2](https:\/\/github.com\/segatalab\/metaml#metaml---metagenomic-prediction-analysis-based-on-machine-learning) to generate species abundance features. **Here we are only going to focus on one of those datasets for now to predict type 2 diabetes.**\n\n## Logistics behind the Input Data\n\nThis notebook was created to further explore the meta-genomics data on kaggle. The link to the data-set is: https:\/\/www.kaggle.com\/antaresnyc\/metagenomics. The datasets include:\n* abundance.txt: a table containing the abundances of each organism type\n  * the first 210 features include meta-data about the samples\n  * the rest of the features include the abundance data in float-type\n* marker_presence.txt: a table containing the presence of strain-specific markers. \n  * the first 210 features include meta-data about the samples (same as abundance.txt)\n  * In a previous notebook I converted the marker presence feature data into a sparse matrix for easier downloading. This sparse matrix is found on [kaggle](https:\/\/www.kaggle.com\/sklasfeld\/metagenomics-marker-presence-sparse-matrix).\n* markers2clades_DB.txt: a lookup table to associate each marker identifier to the corresponding species.","213df250":"* The `edge_index` has shape 2 x E where `E` is the number of edges in a network\n* The `x` has shape nspecies x 2 where `nspecies` is the number of bacteria with abundance values\n* `y` length is equal to the number of samples","9c2d1c7f":"It may be faster to do below instead...","363a16c9":"We are ready to run the model. As you can see, however, I get an error. Here an essay about this type of error: https:\/\/towardsdatascience.com\/cuda-error-device-side-assert-triggered-c6ae1c8fa4c3\n\nI will fix it eventually. Comment below if you have any suggestions","8f8a0dd7":"## Cleaning Marker Presence file\nOnce we have created a model using the abundance data we can try to add the marker-presense information into the model. However, for now we will put this on hold. Ignore the code below. ","360ce2a1":"We split the data so that 10% is in the training set and 90% is in the testing set. Then we use the DataLoader() function to import these datasets into pytorch.","b9153bda":"See Table 1 in the paper for a summary of the datasets considered in the experiment.","f0c9baca":"It looks like `nd`, `na`, `unknown` and `-` all stands for no data. Therefore let's replace these values all with np.NaN","4ce42ab3":"Processed custom dataset files are output below","724eafd0":"We can remove all columns that have only 1 value (not including NaN). These do not seem to be too informative anyway.","4f714000":"## Libraries\nBelow I import some librarys that may be useful and then print the input files","da04823c":"# Cleaning the Data\nTo organize the data we think of it as three different data types:\n1. The meta information (found in the first 210 columns of both the abundance and marker_presense tables)\n2. The abundance data\n3. The marker_presense data\n","2266b286":"first we import abundance data into a pandas dataframe. To start we are only looking at data from the `t2dmeta_long` dataset. We create a table with the meta-data features and one with only the abundance values.","6345a949":"### Method 1: Model based on NetworkX\nWe create a dictionary called `graph_dict` to hold the directed graphs for each sample. The key is in the index of the sample in the dataframe, and the value is the `networkx` graph `G`.\n\nWe build 'G' by adding looking at each of the values in each row. If the value is greater than 0 then we can add a node to the graph. If the column name includes information beyond the kingdom then we can add a node to the previous node in the hierarcy. ","71dc895f":"## Cleaning meta features\nRecall that each of the given datasets are made up of multiple datasets which do not all include the same meta information.","dc019c50":"In summary we have 210 samples. We know the abundance of the organisms in the sample. If an organism is in a sample we have strain-specific marker information."}}