{"cell_type":{"b6e113e9":"code","62335576":"code","d7a4a785":"code","145ca8e7":"code","518db2f1":"code","0e43f13e":"code","3dc7dff5":"code","4e207715":"code","ed9ebb6c":"code","e3d5fd8b":"code","f5bae511":"code","7ba93ef9":"code","d6b24f82":"code","a49352dc":"code","65b01bb9":"code","04ef537e":"code","2600dfd9":"code","f5f2a671":"code","0af48fed":"code","6ded1b39":"code","b2eaba63":"code","6b389c61":"code","2f6dafe9":"code","eb68a3c8":"code","cc899808":"code","f98a8e91":"code","be398ffb":"code","8ce0c5d6":"code","3c96d339":"code","8fcb638f":"code","0700929e":"code","9fd803c6":"code","d876410d":"code","abb94583":"code","45113323":"code","0fd8fcbd":"code","dbf602d1":"code","48160102":"code","eb7a5992":"code","698630b5":"code","d72a6559":"code","66b3d804":"code","e62360dc":"code","de2b7fb9":"code","8c3112bc":"code","ae576680":"code","99653e40":"code","dc14a772":"code","0f20ea51":"code","0d599487":"code","f505b6b7":"code","f66e8e73":"code","c68a7e06":"code","00ae00d0":"code","ba3f230d":"code","ee2eeb69":"code","4a31e89f":"code","30703772":"code","b6baf736":"code","fab264cb":"code","0799c64d":"code","c26153f9":"code","7f7ed353":"code","c0f7bb3d":"code","2f55ebb3":"code","5ff790d5":"code","1f686f7f":"code","9be9b260":"code","6fc06859":"code","62d79d6e":"code","98faa8fb":"code","c9af6b4f":"code","244b397b":"code","f6e75bac":"code","96aafc33":"code","73bf2abf":"code","18d1b86c":"code","ab170c22":"code","61d35ad5":"code","dd54b0a7":"code","997987aa":"code","b5d336d0":"code","1166870a":"code","8b89c3eb":"code","c94ab9cc":"code","1e7e66dd":"code","46480fb3":"code","648c753e":"code","95e3f8aa":"code","e720ba95":"code","49d94ff2":"code","2fe07847":"code","ec79de79":"code","249cd06b":"code","985575c0":"code","5e258d33":"code","691a07e7":"code","59f73d20":"code","a8d6c518":"code","4ec62924":"code","d0deb377":"code","c9e850c0":"code","27cacb00":"code","01d0c8ec":"code","25bea0e1":"code","55c9b4ce":"code","7a16eedf":"code","6239554f":"code","0151aaf5":"code","4acc5c23":"code","b2aba658":"code","9a8d160d":"code","bcd4ff38":"code","bb8c6752":"code","df9aed27":"code","dad63f21":"code","0d29b63a":"code","11af99db":"code","99c80841":"code","13b1d329":"code","c108e273":"code","d384c850":"code","efd9e7fa":"code","09f9e9f1":"code","c8e0ffcb":"code","8d10606d":"code","f29abbf0":"code","34894b2b":"code","cca4a43e":"code","3d738ac1":"code","a93e1040":"code","0b380e0f":"code","a0a116d7":"markdown","9cd6c617":"markdown","6ee2a133":"markdown","70f70f65":"markdown","8b9df920":"markdown","a7b2542e":"markdown","9198b072":"markdown","d0f8750a":"markdown","e6255de3":"markdown","586aa8fe":"markdown","aa9c2abc":"markdown","8fae7483":"markdown","e25a984a":"markdown","4e1c4306":"markdown","7f376508":"markdown","cbadded4":"markdown","cf569eb0":"markdown","d4a2c49c":"markdown","19ffabc7":"markdown","f1405cf3":"markdown","5e20db54":"markdown","fc8a209e":"markdown","7686f79a":"markdown","f9889faa":"markdown","9b9db922":"markdown","b0874c3b":"markdown","798fba5c":"markdown","93c89c33":"markdown","435e890c":"markdown","a1fd3136":"markdown","cbc52e60":"markdown","ee6b5a07":"markdown","547bcae3":"markdown","c97fa828":"markdown","e890c48c":"markdown","8bea6154":"markdown","e5604aee":"markdown","e020ecc8":"markdown","19373b21":"markdown","ba96ead4":"markdown","44f1c903":"markdown","496a1337":"markdown","cae1f72a":"markdown","606f7c81":"markdown","884bcb7c":"markdown","face17ef":"markdown","1e473396":"markdown","cef59b4a":"markdown","9e0a0c7a":"markdown","820e592a":"markdown","f9fcb98b":"markdown","587657f2":"markdown","d3d1b531":"markdown","e4240741":"markdown","04482f1c":"markdown","1e4360bd":"markdown","2b931047":"markdown","acb8fa41":"markdown","8233f0ef":"markdown","b6c135ca":"markdown","da76fef5":"markdown","dfa0c58d":"markdown","d841cb24":"markdown","1de4573a":"markdown","a0fbc36d":"markdown","d3567b71":"markdown","074f787f":"markdown","4297eecb":"markdown"},"source":{"b6e113e9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (12, 8)\n\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 300)\npd.set_option('display.width', 400)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nimport sklearn.feature_selection as skfs\nimport sklearn.linear_model as sklm\nimport sklearn.decomposition as skd\nimport sklearn.ensemble as ske\nimport sklearn.tree as skt\nimport sklearn.feature_selection as skfs\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import svm\nimport xgboost as xgb\nimport random\nseed = 12\nnp.random.seed(seed)\n\nfrom datetime import date","62335576":"# important funtions\ndef datasetShape(df):\n    rows, cols = df.shape\n    print(\"The dataframe has\",rows,\"rows and\",cols,\"columns.\")\n    \n# select numerical and categorical features\ndef divideFeatures(df):\n    numerical_features = df.select_dtypes(include=[np.number])\n    categorical_features = df.select_dtypes(include=[np.object])\n    return numerical_features, categorical_features\n\ndef calc_missing(df):\n    missing = df.isna().sum().sort_values(ascending=False)\n    missing = missing[missing != 0]\n    missing_perc = missing\/df.shape[0]*100\n    return missing, missing_perc\n\ndef plotCorrelation(cols, df, figsize=(20,10)):\n    plt.figure(figsize=figsize)\n    sns.heatmap(df[cols].corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, center = 0)\n    plt.show()\n    \ndef plotAUC(y_true, y_pred_proba):\n    fpr, tpr, threshold = skm.roc_curve(y_true, y_pred_proba[:,1])\n    roc_auc = skm.auc(fpr, tpr)\n    plt.figure(figsize=(6,6))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.4f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n\ndef display_scores(y_true, y_pred, y_pred_proba, plot=False):\n    cfm = skm.confusion_matrix(y_true, y_pred)\n    tn, fp, fn, tp = cfm.ravel()\n    print(f\"Accuracy Score: {round(skm.accuracy_score(y_true, y_pred)*100,4)}%\")\n    print(f\"Sensitivity\/Recall\/TPR: {round(tp\/(tp+fn)*100,4)}%\")\n    print(f\"FPR: {round(fp\/(tn+fp)*100,4)}%\")\n    print(f\"Specificity: {round(tn\/(tn+fp)*100,4)}%\")\n    print(f\"Precision: {round(tp\/(tp+fp)*100,4)}%\")\n    print(f\"F1 Score: {round(skm.f1_score(y_true, y_pred)*100,4)}%\")\n    print(f\"Prediction AUC Score: {round(skm.roc_auc_score(y_true, y_pred)*100,4)}%\")\n    print(f\"Mean Square Error: {round(skm.mean_squared_error(y_true, y_pred),10)}\")\n    if plot:\n        plotAUC(y_true, y_pred_proba)\n\n# function for prediction metrics\ndef displayPredictionMetrics(model, X_train, y_train, showROC=False):\n    y_pred_proba = model.predict_proba(X_train)\n    y_pred = model.predict(X_train)\n    display_scores(y_train, y_pred, y_pred_proba, showROC)\n    \ndef plot_avgMonthlyCalls(data,calltype,colList):\n    # create a color palette\n    palette = plt.get_cmap('Set1')\n    \n    fig, ax = plt.subplots(figsize=(8,4))\n    ax.plot(data[colList].mean())\n    ax.set_xticklabels(['Jun','Jul','Aug'])\n\n    # Add titles\n    plt.title(\"Avg. \"+calltype+\" MOU  V\/S Month\", loc='left', fontsize=12, fontweight=0, color='orange')\n    plt.xlabel(\"Month\")\n    plt.ylabel(\"Avg. \"+calltype+\" MOU\")\n    plt.show()\n    \ndef plot_byChurnMou(data,colList, calltype):\n    fig, ax = plt.subplots(figsize=(7,4))\n    df=data.groupby(['churn'])[colList].mean().T\n    plt.plot(df)\n    ax.set_xticklabels(['Jun','Jul','Aug','Sep'])\n    ## Add legend\n    plt.legend(['Non-Churn', 'Churn'])\n    # Add titles\n    plt.title(\"Avg. \"+calltype+\" MOU  V\/S Month\", loc='left', fontsize=12, fontweight=0, color='orange')\n    plt.xlabel(\"Month\")\n    plt.ylabel(\"Avg. \"+calltype+\" MOU\")\n    \ndef plot_byChurn(data, col):\n    # per month churn vs Non-Churn\n    fig, ax = plt.subplots(figsize=(7,4))\n    colList = list(data.filter(regex=(col)).columns)\n    colList = colList[:3]\n    plt.plot(data.groupby('churn')[colList].mean().T)\n    ax.set_xticklabels(['Jun','Jul','Aug','Sep'])\n    ## Add legend\n    plt.legend(['Non-Churn', 'Churn'])\n    # Add titles\n    plt.title( str(col) +\" V\/S Month\", loc='left', fontsize=12, fontweight=0, color='orange')\n    plt.xlabel(\"Month\")\n    plt.ylabel(col)\n    plt.show()\n    # Numeric stats for per month churn vs Non-Churn\n    return data.groupby('churn')[colList].mean()","d7a4a785":"data_file = \"\/kaggle\/input\/telecom-churn-dataset\/telecom_churn_data.csv\"\ntelecom = pd.read_csv(data_file)\ntelecom.head()","145ca8e7":"# check dataset shape\ndatasetShape(telecom)","518db2f1":"# check for duplicates\nif(len(telecom) == len(telecom.mobile_number.unique())):\n    print(\"No duplicates found!!\")\nelse:\n    print(\"Duplicates occuring\")","0e43f13e":"# drop the mobile number\ntelecom.drop('mobile_number', axis=1, inplace=True)\n\n# drop the duplicate rows\ntelecom.drop_duplicates(inplace=True)\ndatasetShape(telecom)","3dc7dff5":"# remove all columns having no values\ntelecom.dropna(axis=1, how=\"all\", inplace=True)\ntelecom.dropna(axis=0, how=\"all\", inplace=True)\ndatasetShape(telecom)","4e207715":"telecom.head()","ed9ebb6c":"print(\"Verifying percentage of NaN values in remaining columns\")\nprint(telecom.isnull().mean().round(4) * 100)","e3d5fd8b":"# remove columns having null values more than 50%\ntelecom.dropna(thresh=telecom.shape[0]*0.5,how='all',axis=1, inplace=True)\ndatasetShape(telecom)","f5bae511":"# divide features\nnumerical_features, categorical_features = divideFeatures(telecom)\ntelecom.head()","7ba93ef9":"categorical_features.info()","d6b24f82":"featuresToDrop = categorical_features.columns.to_list()\ntelecom.drop(featuresToDrop, axis=1, inplace=True)\ndatasetShape(telecom)","a49352dc":"# plot missing values\n\nmissing, missing_perc = calc_missing(telecom)\nmissing.plot(kind='bar',figsize=(30,12))\nplt.title('Missing Values')\nplt.show()","65b01bb9":"# missing values with percentage\npd.concat([missing, missing_perc], axis=1, keys=['Total','Percent']).T","04ef537e":"telecom[missing.index].head()","2600dfd9":"telecom[missing.index].describe()","f5f2a671":"colsToImputeWithMedian = missing.index.to_list()\nfor col in colsToImputeWithMedian:\n    telecom.loc[telecom[col].isna(), col] = telecom[col].median()","0af48fed":"missing, missing_perc = calc_missing(telecom)\nprint(\"Any Missing Values?\",missing.values)","6ded1b39":"singleValuedFeatures = []\nfor x in telecom.columns.to_list():\n    if len(telecom[x].unique()) == 1:\n        singleValuedFeatures.append(x)","b2eaba63":"telecom.drop(singleValuedFeatures, axis=1, inplace=True)\ndatasetShape(telecom)","6b389c61":"telecom['avg_amt_m6m7'] = (telecom['total_rech_amt_6']+telecom['total_rech_amt_7'])\/2","2f6dafe9":"# mapping churn as 1 and not churn as 0\ntelecom['churn'] = ((telecom['total_ic_mou_9']==0) & (telecom['total_og_mou_9']==0) & (telecom['vol_2g_mb_9']==0) & (telecom['vol_3g_mb_9']==0)).map({True:1,False:0})\ntelecom['churn'].value_counts()\/telecom.shape[0]*100","eb68a3c8":"# filtering high value customers with value > 70% quantile in avg_amt_6_7 column\ntelecom = telecom[telecom[\"avg_amt_m6m7\"] >= telecom['avg_amt_m6m7'].quantile(0.7)]\ntelecom[\"avg_amt_m6m7\"].describe()","cc899808":"remainingCols = [x for x in telecom.columns.to_list() if '_9' not in x]\nremainingCols.remove('sep_vbc_3g')\ntelecom = telecom[remainingCols]\ndatasetShape(telecom)","f98a8e91":"numerical_features, categorical_features = divideFeatures(telecom)\ntelecom.head()","be398ffb":"# check for amount of defaulters in the data using countplot\nplt.figure(figsize=(16,3))\nsns.countplot(y=\"churn\", data=telecom)\nplt.show()\ntelecom[\"churn\"].value_counts()\/telecom.shape[0]*100","8ce0c5d6":"# plot aon with histplot\naon_bins = [0, 365, 730, 1095, 1460, 1825, 2190, 2555, 2920, 3285, 3650, 5015]\nbucket_l = [\"Year \"+str(x) for x in range(1,12)]\naon_bins_data = pd.cut(telecom.aon, aon_bins, labels=bucket_l)","3c96d339":"aon_bins_data.hist()\nplt.show()","8fcb638f":"# Plotting Avg. total monthly incoming MOU vs AON\nic_col = telecom.filter(regex ='total_ic_mou').columns\nplot_avgMonthlyCalls(telecom, calltype='incoming', colList=ic_col)","0700929e":"# Plotting Avg. total monthly outgoing MOU vs AON\nog_col = telecom.filter(regex ='total_og_mou').columns\nplot_avgMonthlyCalls(telecom, calltype='outgoing', colList=og_col)","9fd803c6":"# boxplots of numerical features for outlier detection\n\nfig = plt.figure(figsize=(16,100))\nfor i in range(len(numerical_features.columns)):\n    fig.add_subplot(34, 4, i+1)\n    sns.boxplot(y=numerical_features.iloc[:,i])\nplt.tight_layout()\nplt.show()","d876410d":"# distplots for categorical data\n\ncat_features = ['monthly_2g_6','monthly_2g_7','monthly_2g_8','monthly_3g_6','monthly_3g_7','monthly_3g_8', 'churn']\nfig = plt.figure(figsize=(16,10))\nfor i in range(len(cat_features)):\n    fig.add_subplot(3, 3, i+1)\n    telecom[cat_features].iloc[:,i].hist()\n    plt.xlabel(telecom.columns[i])\nplt.tight_layout()\nplt.show()","abb94583":"# graph for incoming and outgoing in month 6,7,8 by churn\nic_col = ['total_ic_mou_6','total_ic_mou_7', 'total_ic_mou_8']\nog_col = ['total_og_mou_6','total_og_mou_7', 'total_og_mou_8']\nplot_byChurnMou(telecom, ic_col, 'Incoming')\nplot_byChurnMou(telecom, og_col, 'Outgoing')","45113323":"# graph for total recharge amount in month 6,7,8 by churn\nplot_byChurn(telecom,'total_rech_amt')","0fd8fcbd":"# graph for arpu in month 6,7,8 by churn\nplot_byChurn(telecom,'arpu')","dbf602d1":"colsToPlot = ['arpu_6','arpu_7','arpu_8','onnet_mou_6','onnet_mou_7','onnet_mou_8','offnet_mou_6','offnet_mou_7','offnet_mou_8','loc_og_mou_6','loc_og_mou_7','loc_og_mou_8','total_og_mou_6','total_og_mou_7','total_og_mou_8','total_ic_mou_6','total_ic_mou_7','total_ic_mou_8','avg_amt_m6m7','aon']\n# plot scatter plots for all major numerical features\n\nsns.pairplot(telecom[colsToPlot], size=3)\nplt.show()","48160102":"# correlation within outgoing minutes of usage for month 6\nog_mou_6 = telecom.columns[telecom.columns.str.contains('.*_og_.*mou_6',regex=True)]\nplotCorrelation(og_mou_6, telecom)","eb7a5992":"# correlation within outgoing minutes of usage for month 7\nog_mou_7 = telecom.columns[telecom.columns.str.contains('.*_og_.*mou_7',regex=True)]\nplotCorrelation(og_mou_7, telecom)","698630b5":"# correlation within incoming minutes of usage for month 6\nic_mou_6 = telecom.columns[telecom.columns.str.contains('.*_ic_.*mou_6',regex=True)]\nplotCorrelation(ic_mou_6, telecom)","d72a6559":"# correlation within incoming minutes of usage for month 7\nic_mou_7 = telecom.columns[telecom.columns.str.contains('.*_ic_.*mou_7',regex=True)]\nplotCorrelation(ic_mou_7, telecom)","66b3d804":"# Checking outliers at 25%,50%,75%,90%,95% and 99%\ntelecom.describe(percentiles=[.25,.5,.75,.90,.95,.99])","e62360dc":"# plot sample skewed feature\nplt.figure(figsize=(16,4))\nsns.distplot(telecom.loc_og_mou_7)\nplt.show()","de2b7fb9":"# extract all skewed features\ntemp_numerical_features, temp_categorical_features = divideFeatures(telecom)\n# remove categorical features stored as int\ntemp_numerical_features.drop(cat_features, axis=1, inplace=True)\ntemp_numerical_features.drop(['arpu_6','arpu_7','arpu_8'], axis=1, inplace=True)\nskewed_features = temp_numerical_features.apply(lambda x: x.skew()).sort_values(ascending=False)","8c3112bc":"# transform skewed features\nfor feat in skewed_features.index:\n    # features which are more than 50% skewed are transformed\n    if skewed_features.loc[feat] > 0.5:\n        telecom[feat] = np.log1p(telecom[feat])","ae576680":"# plot sample treated feature\nplt.figure(figsize=(16,4))\nsns.distplot(telecom.loc_og_mou_7)\nplt.show()","99653e40":"# outlier treatment for categorical features\ncat_features.remove('churn')\ndef getCategoricalSkewed(categories, threshold):\n    tempSkewedFeatures = []\n    for feat in categories:\n        for featValuePerc in list(telecom[feat].value_counts()\/telecom.shape[0]):\n            if featValuePerc > threshold:\n                tempSkewedFeatures.append(feat)\n    return list(set(tempSkewedFeatures))\n\n# display all categorical skewed features which have value_counts > 90%\ncategoricalSkewed = getCategoricalSkewed(cat_features, .90)\nif len(categoricalSkewed) > 0:\n    for feat in categoricalSkewed:\n        print(\"Ratio of non-churn vs churn:\")\n        print(telecom[feat].value_counts()\/len(telecom)*100)\nelse:\n    print(\"No Categorical Skewed variables\")","dc14a772":"newFeaturesAdded = []\nnewFeatures = telecom.filter(regex='_6|_7').columns.str[:-2]\nfor idx, col in enumerate(newFeatures.unique()):\n    newF = \"avg_\"+col+\"_avg67\"\n    newFeaturesAdded.append(newF)\n    telecom[newF] = (telecom[col+\"_6\"]  + telecom[col+\"_7\"])\/ 2\ndatasetShape(telecom)\ntelecom[newFeaturesAdded].head()","0f20ea51":"# shuffle samples\ndf_shuffle = telecom.sample(frac=1, random_state=seed).reset_index(drop=True)","0d599487":"df_y = df_shuffle.pop('churn')\ndf_X = df_shuffle.copy()\n\n# split into train dev and test\nX_train, X_test, y_train, y_test = skms.train_test_split(df_X, df_y, train_size=0.75, random_state=seed)\nprint(f\"Train set has {X_train.shape[0]} records out of {len(df_shuffle)} which is {round(X_train.shape[0]\/len(df_shuffle)*100)}%\")\nprint(f\"Test set has {X_test.shape[0]} records out of {len(df_shuffle)} which is {round(X_test.shape[0]\/len(df_shuffle)*100)}%\")","f505b6b7":"scaler = skp.StandardScaler()\nnumerical_features, categorical_features = divideFeatures(X_train)\n\n# apply scaling to all numerical variables except dummy variables as they are already between 0 and 1\nX_train[numerical_features.columns] = scaler.fit_transform(X_train[numerical_features.columns])\n\n# scale test data with transform()\nX_test[numerical_features.columns] = scaler.transform(X_test[numerical_features.columns])\n\n# view sample data\nX_train.describe()","f66e8e73":"pca = skd.PCA(random_state=seed)\npca.fit(X_train)","c68a7e06":"pca.components_","00ae00d0":"pca.explained_variance_ratio_","ba3f230d":"var_cumu = np.cumsum(pca.explained_variance_ratio_)","ee2eeb69":"fig = plt.figure(figsize=[12,8])\nplt.vlines(x=65, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.95, xmax=150, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.plot(var_cumu)\nplt.ylabel(\"Cumulative variance explained\")\nplt.show()","4a31e89f":"pca_final = skd.IncrementalPCA(n_components=65)\ndf_train_pca = pca_final.fit_transform(X_train)","30703772":"corrmat = np.corrcoef(df_train_pca.transpose())\nplt.figure(figsize=[40,20])\nsns.heatmap(corrmat, annot=True, center = 0)\nplt.show()","b6baf736":"df_test_pca = pca_final.transform(X_test)\ndatasetShape(df_test_pca)","fab264cb":"# fit model\nrfc = ske.RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=seed)\nrfc.fit(df_train_pca,y_train)","0799c64d":"# find train prediction metrics\ndisplayPredictionMetrics(rfc, df_train_pca, y_train)","c26153f9":"# find test prediction metrics\ndisplayPredictionMetrics(rfc, df_test_pca, y_test, True)","7f7ed353":"def tune_rfc_hyperparameter(model, parameters,x_train,y_train,n_folds = 5):\n    \n    m = skms.GridSearchCV(model, parameters, cv=n_folds, n_jobs=-1, scoring=\"recall\", return_train_score=True, verbose=3)\n    m.fit(x_train, y_train)\n    scores = m.cv_results_\n\n    # find the value of the hyperparameter\n    for key in parameters.keys():\n        hyperparameter = key\n        break\n\n    print('We can get the sensitivity of',m.best_score_,'using',m.best_params_)\n    \n    # plotting accuracies for parameters\n    plt.figure(figsize=(16,5))\n    plt.plot(scores[\"param_\"+hyperparameter], scores[\"mean_train_score\"], label=\"training accuracy\")\n    plt.plot(scores[\"param_\"+hyperparameter], scores[\"mean_test_score\"], label=\"test accuracy\")\n    plt.xlabel(hyperparameter)\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()","c0f7bb3d":"rf_hyper_init = ske.RandomForestClassifier(class_weight='balanced', random_state=seed)","2f55ebb3":"# tuning max_depth\nparameters = {'max_depth': range(2, 30, 5)}\ntune_rfc_hyperparameter(rf_hyper_init, parameters,df_train_pca,y_train)","5ff790d5":"# tuning n_estimators\nparameters = {'n_estimators': [100, 200, 300]}\ntune_rfc_hyperparameter(rf_hyper_init, parameters,df_train_pca,y_train)","1f686f7f":"# tuning max_features\nparameters = {'max_features': [40, 50, 60]}\ntune_rfc_hyperparameter(rf_hyper_init, parameters,df_train_pca,y_train)","9be9b260":"# tuning min_samples_leaf\nparameters = {'min_samples_leaf': range(10, 80, 15)}\ntune_rfc_hyperparameter(rf_hyper_init, parameters,df_train_pca,y_train)","6fc06859":"# tuning min_samples_split\nparameters = {'min_samples_split': range(20, 110, 20)}\ntune_rfc_hyperparameter(rf_hyper_init, parameters,df_train_pca,y_train)","62d79d6e":"# tuning all final hyperparameters for rfc\n\nparameters = {\n    'max_depth': [2, 7, 12],\n    'n_estimators': [100],\n    'max_features': [55],\n    'min_samples_leaf': [50,70],\n    'min_samples_split': [100]}\n\nrfc_hyper = ske.RandomForestClassifier(class_weight='balanced')\n\n# cross validation\nmodel_cv_rfc_hyper = skms.GridSearchCV(estimator = rfc_hyper, n_jobs=-1, param_grid = parameters, \n                             scoring= 'recall', cv = 3, return_train_score=True, verbose = 3)            \nmodel_cv_rfc_hyper.fit(df_train_pca, y_train)","98faa8fb":"# display all final tuned hyper parameters for rfc\nprint('We can get the sensitivity of',model_cv_rfc_hyper.best_score_,'using',model_cv_rfc_hyper.best_params_)","c9af6b4f":"y_train_svc = y_train.map({1:1, 0:-1})\ny_test_svc = y_test.map({1:1, 0:-1})","244b397b":"# fit model\nsvc = svm.SVC(class_weight='balanced', probability=True, kernel='rbf')\nsvc.fit(df_train_pca,y_train_svc)","f6e75bac":"# find train prediction metrics\ndisplayPredictionMetrics(svc, df_train_pca, y_train_svc)","96aafc33":"# find test prediction metrics\ndisplayPredictionMetrics(svc, df_test_pca, y_test_svc, True)","73bf2abf":"def plot_svc_hyperparameters(scores, param):\n    gamma = scores[scores['param_gamma']==param]\n    plt.plot(gamma[\"param_C\"], gamma[\"mean_test_score\"])\n    plt.plot(gamma[\"param_C\"], gamma[\"mean_train_score\"])\n    plt.xlabel('C')\n    plt.ylabel('Accuracy')\n    plt.title(\"Gamma=\"+str(param))\n    plt.ylim([0, 1])\n    plt.legend(['test_score', 'training_score'])\n    plt.xscale('log')","18d1b86c":"# tuning all final hyperparameters for svc\ngamma = [1e-1,1e-2, 1e-3, 1e-4]\nC = [1, 10, 100, 1000]\nparameters = {'gamma': gamma, 'C': C}\nsvc_hyper = svm.SVC(class_weight='balanced', probability=True, kernel='rbf')\n\n# cross validation\nmodel_cv_svc_hyper = skms.GridSearchCV(estimator = svc_hyper, n_jobs=-1, param_grid = parameters, \n                             scoring= 'recall', cv = 3, return_train_score=True, verbose = 3)            \nmodel_cv_svc_hyper.fit(df_train_pca, y_train_svc)","ab170c22":"# display cv scores and plot gamma vs C\nsvc_scores = pd.DataFrame(model_cv_svc_hyper.cv_results_)\nsvc_scores['param_C'] = svc_scores['param_C']\n\nplt.figure(figsize=(16,5))\n\nfor x, g in enumerate(gamma):\n    plt.subplot(1, 4, x+1)\n    plot_svc_hyperparameters(svc_scores, g)\nplt.show()","61d35ad5":"# display all final tuned hyper parameters for svc\nprint('We can get the sensitivity of',model_cv_svc_hyper.best_score_,'using',model_cv_svc_hyper.best_params_)","dd54b0a7":"# fit model\nxgbc = xgb.XGBClassifier(scale_pos_weight=(y_train.value_counts()[0]\/y_train.value_counts()[1]), n_jobs=-1)\nxgbc.fit(df_train_pca,y_train)","997987aa":"# find train prediction metrics\ndisplayPredictionMetrics(xgbc, df_train_pca, y_train)","b5d336d0":"# find test prediction metrics\ndisplayPredictionMetrics(xgbc, df_test_pca, y_test, True)","1166870a":"def plot_xgb_hyperparameters(scores, param):\n    lr = scores[scores['param_subsample']==param]\n    plt.plot(lr[\"param_learning_rate\"], lr[\"mean_test_score\"])\n    plt.plot(lr[\"param_learning_rate\"], lr[\"mean_train_score\"])\n    plt.xlabel('learning_rate')\n    plt.ylabel('Accuracy')\n    plt.title(\"Subsample=\"+str(param))\n    plt.ylim([0.4, 1])\n    plt.legend(['test_score', 'training_score'])\n    plt.xscale('log')","8b89c3eb":"# tuning all final hyperparameters for xgb\nlearning_rate = [0.1,0.2,0.3]\nsubsample = [0.3,0.4,0.5]\nparameters = {'learning_rate': learning_rate, 'subsample': subsample}\nxgb_hyper = xgb.XGBClassifier(scale_pos_weight=(y_train.value_counts()[0]\/y_train.value_counts()[1]))\n\n# cross validation\nmodel_cv_xgb_hyper = skms.GridSearchCV(estimator = xgb_hyper, n_jobs=-1, param_grid = parameters, \n                             scoring= 'recall', cv = 3, return_train_score=True, verbose = 3)\nmodel_cv_xgb_hyper.fit(df_train_pca, y_train)","c94ab9cc":"# display cv scores and plot gamma vs C\nxgb_scores = pd.DataFrame(model_cv_xgb_hyper.cv_results_)\nxgb_scores['param_learning_rate'] = xgb_scores['param_learning_rate']\n\nplt.figure(figsize=(16,5))\n\nfor x, s in enumerate(subsample):\n    plt.subplot(1, 3, x+1)\n    plot_xgb_hyperparameters(xgb_scores, s)\nplt.show()","1e7e66dd":"# display all final tuned hyper parameters for xgb\nprint('We can get the sensitivity of',model_cv_xgb_hyper.best_score_,'using',model_cv_xgb_hyper.best_params_)","46480fb3":"print(\"Evaluating RFC Model with best parameters on Train Dataset\",model_cv_rfc_hyper.best_params_,\"\\n\")\ndisplayPredictionMetrics(model_cv_rfc_hyper, df_train_pca, y_train)","648c753e":"print(\"Evaluating RFC Model with best parameters on Test Dataset\",model_cv_rfc_hyper.best_params_,\"\\n\")\ndisplayPredictionMetrics(model_cv_rfc_hyper, df_test_pca, y_test, True)","95e3f8aa":"print(\"Evaluating SVC Model with best parameters on Train Dataset\",model_cv_svc_hyper.best_params_,\"\\n\")\ndisplayPredictionMetrics(model_cv_svc_hyper, df_train_pca, y_train_svc)","e720ba95":"print(\"Evaluating SVC Model with best parameters on Test Dataset\",model_cv_svc_hyper.best_params_,\"\\n\")\ndisplayPredictionMetrics(model_cv_svc_hyper, df_test_pca, y_test_svc, True)","49d94ff2":"print(\"Evaluating XGB Model with best parameters on Train Dataset\",model_cv_xgb_hyper.best_params_,\"\\n\")\ndisplayPredictionMetrics(model_cv_xgb_hyper, df_train_pca, y_train)","2fe07847":"print(\"Evaluating XGB Model with best parameters on Test Dataset\",model_cv_xgb_hyper.best_params_,\"\\n\")\ndisplayPredictionMetrics(model_cv_xgb_hyper, df_test_pca, y_test, True)","ec79de79":"# find stats for model using all features\nlr = sklm.LogisticRegression(random_state=seed, class_weight='balanced', n_jobs=-1)\nlr.fit(X_train,y_train)\ndisplayPredictionMetrics(lr, X_train, y_train)","249cd06b":"lr_rfe = sklm.LogisticRegression(random_state=seed, class_weight='balanced', n_jobs=-1, max_iter=500, verbose=1)\nrfe = skfs.RFE(lr_rfe, 30)\nrfe.fit(X_train, y_train)","985575c0":"rfeCols = X_train.columns[rfe.support_]\nX_train_rfe = X_train[rfeCols]\nX_test_rfe = X_test[rfeCols]\nprint(\"Selected features by RFE are\",list(rfeCols))","5e258d33":"# check metrics by building model with RFE selected features\nlr = sklm.LogisticRegression(random_state=seed, class_weight='balanced', n_jobs=-1)\nlr.fit(X_train_rfe,y_train)\ndisplayPredictionMetrics(lr, X_train_rfe, y_train)","691a07e7":"lr_glm = sm.GLM(y_train, X_train_rfe, family = sm.families.Binomial())\nlr_glm_model = lr_glm.fit()","59f73d20":"# function to drop the mentioned feature variable, build the model and print the summary and VIF\ndef displaySummary(dropFeature=None):\n    # update variable from global scope\n    global X_train_rfe, y_train\n    \n    # dropping variable with very high VIF\n    if dropFeature is not None:\n        X_train_rfe.drop(dropFeature, axis=1, inplace=True)\n        X_test_rfe.drop(dropFeature, axis=1, inplace=True)\n        print(f\"Removed feature: {dropFeature} \\n\")\n        \n    # print model metrics for current available features\n    lr = sklm.LogisticRegression(random_state=seed, class_weight='balanced', n_jobs=-1)\n    lr.fit(X_train_rfe, y_train)\n    displayPredictionMetrics(lr, X_train_rfe, y_train)\n\n    # Running the linear model\n    lr_glm = sm.GLM(y_train, X_train_rfe, family = sm.families.Binomial())\n    lr_glm_model = lr_glm.fit()\n    \n    # check the summary of our linear model\n    print(lr_glm_model.summary())\n    \n    # Calculate the VIFs for the new model after removing constant\n    if 'const' in X_train_rfe.columns:\n        X_train_rfe = X_train_rfe.drop(['const'], axis=1)\n    vif = pd.DataFrame()\n    X = X_train_rfe\n    vif['Features'] = X.columns\n    vif['VIF'] = [round(variance_inflation_factor(X.values, i), 2) for i in range(X.shape[1])]\n    vif = vif.sort_values(by = \"VIF\", ascending = False).set_index('Features')\n    print(vif)","a8d6c518":"displaySummary()","4ec62924":"displaySummary('arpu_7')","d0deb377":"displaySummary('monthly_3g_8')","c9e850c0":"displaySummary('avg_vol_3g_mb_avg67')","27cacb00":"displaySummary('arpu_8')","01d0c8ec":"displaySummary('loc_ic_mou_8')","25bea0e1":"displaySummary('loc_og_mou_8')","55c9b4ce":"displaySummary('avg_loc_ic_t2m_mou_avg67')","7a16eedf":"displaySummary('total_rech_amt_8')","6239554f":"displaySummary('std_og_t2m_mou_6')","0151aaf5":"displaySummary('onnet_mou_7')","4acc5c23":"displaySummary('total_og_mou_8')","b2aba658":"displaySummary('onnet_mou_8')","9a8d160d":"displaySummary('roam_og_mou_7')","bcd4ff38":"displaySummary('avg_max_rech_amt_avg67')","bb8c6752":"displaySummary('loc_og_mou_7')","df9aed27":"displaySummary('vol_3g_mb_8')","dad63f21":"# plot correlation among final selected featuers\nplt.figure(figsize = (20, 10))\nsns.heatmap(X_train_rfe.corr(), annot = True, linewidths=.2, cmap=\"YlGnBu\")\nplt.show()","0d29b63a":"# fit model\nrfc_nopca = ske.RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=seed)\nrfc_nopca.fit(X_train_rfe,y_train)","11af99db":"# find train prediction metrics\ndisplayPredictionMetrics(rfc_nopca, X_train_rfe, y_train)","99c80841":"# find test prediction metrics\ndisplayPredictionMetrics(rfc_nopca, X_test_rfe, y_test, True)","13b1d329":"rf_hyper_init_nopca = ske.RandomForestClassifier(class_weight='balanced', random_state=seed)","c108e273":"# tuning max_depth\nparameters = {'max_depth': range(2, 30, 5)}\ntune_rfc_hyperparameter(rf_hyper_init_nopca, parameters, X_train_rfe, y_train)","d384c850":"# tuning n_estimators\nparameters = {'n_estimators': range(100, 800, 200)}\ntune_rfc_hyperparameter(rf_hyper_init_nopca, parameters, X_train_rfe, y_train)","efd9e7fa":"# tuning max_features\nparameters = {'max_features': [8, 10, 12, 14]}\ntune_rfc_hyperparameter(rf_hyper_init_nopca, parameters, X_train_rfe, y_train)","09f9e9f1":"# tuning min_samples_leaf\nparameters = {'min_samples_leaf': range(1, 50, 10)}\ntune_rfc_hyperparameter(rf_hyper_init_nopca, parameters, X_train_rfe, y_train)","c8e0ffcb":"# tuning min_samples_split\nparameters = {'min_samples_split': range(10, 100, 10)}\ntune_rfc_hyperparameter(rf_hyper_init_nopca, parameters, X_train_rfe, y_train)","8d10606d":"# tuning all final hyperparameters for rfc\n\nparameters = {\n    'max_depth': [7],\n    'n_estimators': [100],\n    'max_features': [12],\n    'min_samples_leaf': [41],\n    'min_samples_split': [70,90]}\n\nrfc_hyper_nopca = ske.RandomForestClassifier(class_weight='balanced', random_state=seed)\n\n# cross validation\nmodel_cv_rfc_hyper_nopca = skms.GridSearchCV(estimator = rfc_hyper_nopca, n_jobs=-1, param_grid = parameters, \n                             scoring= 'recall', cv = 3, return_train_score=True, verbose = 3)            \nmodel_cv_rfc_hyper_nopca.fit(X_train_rfe, y_train)","f29abbf0":"# display all final tuned hyper parameters for rfc\nprint('We can get the sensitivity of',model_cv_rfc_hyper_nopca.best_score_,'using',model_cv_rfc_hyper_nopca.best_params_)","34894b2b":"rfc_interpretable = ske.RandomForestClassifier(class_weight='balanced', random_state=seed, max_depth=7, max_features=12, \n                          min_samples_leaf=41, min_samples_split=90, n_estimators=100)\nrfc_interpretable.fit(X_train_rfe, y_train)","cca4a43e":"print(\"Evaluating RFC Model with best parameters on Train Dataset\",model_cv_rfc_hyper_nopca.best_params_,\"\\n\")\ndisplayPredictionMetrics(rfc_interpretable, X_train_rfe, y_train)","3d738ac1":"print(\"Evaluating RFC Model with best parameters on Test Dataset\",model_cv_rfc_hyper_nopca.best_params_,\"\\n\")\ndisplayPredictionMetrics(rfc_interpretable, X_test_rfe, y_test, True)","a93e1040":"# rfc_interpretable model.feature_importances_\nfeature_importances = pd.DataFrame(rfc_interpretable.feature_importances_,\n                                   index = X_train_rfe.columns, columns=['importance']).sort_values('importance', ascending=False)","0b380e0f":"print(\"Top 10 Model parameters (excluding constant) are:\")\nfeature_importances[:10]","a0a116d7":"# Telecom Churn Analysis\n\nTo reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\nIn this analysis, we will analyse the customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.","9cd6c617":"There are outliers in many features. These outliers will be treated in Data Preparation step.","6ee2a133":"**We have removed multicollinearity from our dataset, and now our models will be much more stable**\n\nApplying the transformation on the test set","70f70f65":"### MultiVariate Analysis","8b9df920":"### Model 1 - Random Forest Classifier Model","a7b2542e":"## Model A - High Performance Model Using PCA","9198b072":"#### Manual Feature Selection\n\nNow we will proceed with **manual feature** selection by building the model using **statsmodel** library with **p-value & VIF**","d0f8750a":"**We have got 65 components to describe 95% of variance in the data**","e6255de3":"### Drop columns for churn phase i.e. 9","586aa8fe":"`We will evaluate this classifier in Model Evaluation step.`","aa9c2abc":"**As all the columns are numerical, have many outliers and missing values are less than 8% in these columns, we will impute all the missing value columns with median values.**","8fae7483":"## Model B - Feature Importance Model Without PCA","e25a984a":"**HyperParameter Tuning**","4e1c4306":"**Initial Model Building**","7f376508":"`We will evaluate this classifier in Model Evaluation step.`","cbadded4":"**HyperParameter Tuning**","cf569eb0":"**All object type features are only dates. We will drop all date columns that are object types.**","d4a2c49c":"### Model 2 - Support Vector Machine Model Evaluation","19ffabc7":"## Interpretable Model Recommendations","f1405cf3":"### Derive New Features\n\nWe will make new features by taking average for every feature for month 6 and 7.","5e20db54":"Looking at the explained variance ratio for each component","fc8a209e":"## Step 3: Data Visualization - EDA","7686f79a":"### RFE Feature Selection","f9889faa":"### Missing Value Imputation","9b9db922":"# The END","b0874c3b":"## Step 5: Data Modelling\n\n**Using class_weight='balanced' in Learning Algorithms to handle class imbalance problem.**\n\n**Using scoring='recall' for every GridSearch to tune the hyperparameters in order to improve the sensitivity.**","798fba5c":"## Step 6: Model Evaluation","93c89c33":"### Logistic Regression Model with all Features","435e890c":"### Model 3 - Gradient Boosting Model","a1fd3136":"`The sensitivity score of RandomForestClassifier model for Train is 74.8341% and Test is 77.918%`\n\n`The sensitivity score of SupportVectorClassifier model for Train is 84.9923% and Test is 84.2271%`\n\n`The sensitivity score of XGBClassifier model for Train is 96.0184% and Test is 71.1356%`\n\n| Classifier              | Train Sensitivity | Test Sensitivity |\n|-------------------------|-------------------|------------------|\n| RandomForestClassifier  | 74.8341%          | 77.918%          |\n| SupportVectorClassifier | 84.9923%          | **84.2271%**     |\n| XGBClassifier           | 96.0184%          | 71.1356%         |\n\n**The SupportVectorClassifier model is stable and performing good with around 84.2271% of test accuracy. It can be used in production environment.**","cbc52e60":"**Initial Model Building**","ee6b5a07":"## Step 4: Data Preparation","547bcae3":"**HyperParameter Tuning**","c97fa828":"### Univariate Analysis","e890c48c":"## High Performance Model Observations","8bea6154":"**All the features are having outliers in the data. We will not remove any outlier data but we will treat the skewed data.**","e5604aee":"### Derive New Features","e020ecc8":"Some patterns can be seen in above plotted categorical data. It will help in identifying useful features.","19373b21":"### Random Forest Classifier Model Building","ba96ead4":"### Filter High Value Customers","44f1c903":"**Initial Model Building**","496a1337":"## Step 6: Model Evaluation\n\n### Model 1 - Random Forest Classifier Model Evaluation","cae1f72a":"***All the features' p-values and VIF are optimal now. <br>These set of features are fine to proceed with the Model Selection.***\n\n### Correlation Heatmap","606f7c81":"## Step 2: Data Cleaning","884bcb7c":"**HyperParameter Tuning**","face17ef":"#### Components from the PCA","1e473396":"There are some correlated features present in dataset. We will use these features in model building.","cef59b4a":"### Feature Scaling","9e0a0c7a":"Plotting the heatmap of the corr matrix","820e592a":"**A less number of high value customers have churned as around 8-10%.**\n\n**Outgoing Calls on romaing for 8th month is strong indicators of churn behaviour. So it should be focused to identify if there is any issue with calling on roaming.**\n\n**Total Incoming Calls for 8th month is most strong indicator of churn behaviour.**\n\n**Customers that are joined in last 4 years are more likely to churn and should be focussed for retaining by providing special schemes.**\n\n**Last day recharge feature is also important for predicting churn behaviour.**\n\n**Using RFC with selected set of features, we can have 79.9% of sensitivity in predicting churn behaviour.**\n\n***The top 10 important features are:*** <br>\n<blockquote><b>total_ic_mou_8, roam_og_mou_8, last_day_rch_amt_8, loc_og_t2m_mou_8, loc_ic_t2m_mou_8, vol_2g_mb_8, aon, max_rech_amt_8, std_og_mou_8, avg_amt_m6m7<\/b><\/blockquote>","f9fcb98b":"### Drop Irrelevant Features","587657f2":"**Finding churn customers with total data and calls usage**","d3d1b531":"Making a scree plot for the explained variance","e4240741":"### Model 3 - XGBoost Model Evaluation","04482f1c":"### Outlier Treatment\n\nTreating with the SalePrice target feature and other numerical features, which are skewed. We will take log of the feature values using np.log1p()","1e4360bd":"**New column for average amount of total recharge amount for months 6th & 7th**","2b931047":"`We will evaluate this classifier in Model Evaluation step.`","acb8fa41":"### Model 2 - Support Vector Machine Model","8233f0ef":"Correlation will be used for feature selection.","b6c135ca":"### Bivariate Analysis","da76fef5":"### Find Important Features","dfa0c58d":"## Step 1: Reading and Understanding the Data","d841cb24":"**We can see most of the customers belong to last 4 years.**","1de4573a":"### Drop single valued features","a0fbc36d":"### Find Principal Components ","d3567b71":"All Missing values are treated.","074f787f":"### Split Train-Test Data","4297eecb":"## Step 5: Data Modelling"}}