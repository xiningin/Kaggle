{"cell_type":{"0aa652c3":"code","6766d980":"code","39962c83":"code","f2685296":"code","c3d793e8":"code","6cba57d1":"code","9f998968":"code","1821f4ff":"code","d3ebf60b":"code","8ecc1448":"code","cde788e0":"code","0a017b31":"code","2653ee6c":"code","8ca940bb":"code","bd3fc5ab":"code","d77c0b13":"code","9ea14320":"code","654d332d":"code","f7d985f7":"code","744c2bc0":"code","9acc79f4":"code","02d24496":"code","956e3ae5":"code","4c9740d0":"code","6671048e":"code","ae3492f9":"code","572aca50":"code","cde04954":"markdown"},"source":{"0aa652c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport random\nimport os\nimport cv2\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom keras.utils import plot_model\nfrom sklearn.metrics import classification_report\nfrom collections import Counter\nimport tensorflow as tf\n\nimport keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.vgg16 import VGG16\n\nfrom keras import Model, layers\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam, SGD\nfrom keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D, Dropout, Dense, Input, Conv2D, MaxPooling2D, Flatten,MaxPooling3D\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6766d980":"root_path = '\/kaggle\/input\/intel-image-classification\/'\ntrain_pred_test_folders = os.listdir(root_path)\n\nseg_train_folders = '..\/input\/intel-image-classification\/seg_train\/seg_train\/' #one more seg_train folder within\nseg_test_folders = '..\/input\/intel-image-classification\/seg_test\/seg_test\/'\nseg_pred_folders = '..\/input\/intel-image-classification\/seg_pred\/seg_pred\/'\nquantity_tr = {} \nquantity_te = {}\nfor folder in os.listdir(seg_train_folders):\n    quantity_tr[folder] = len(os.listdir(seg_train_folders+folder))\n\nfor folder in os.listdir(seg_test_folders):\n    quantity_te[folder] = len(os.listdir(seg_test_folders+folder))\n    \nquantity_train = pd.DataFrame(list(quantity_tr.items()), index=range(0,len(quantity_tr)), columns=['class','count'])\nquantity_test = pd.DataFrame(list(quantity_te.items()), index=range(0,len(quantity_te)), columns=['class','count'])\n\nfigure, ax = plt.subplots(1,2,figsize=(20,5))\nsns.barplot(x='class',y='count',data=quantity_train,ax=ax[0])\nsns.barplot(x='class',y='count',data=quantity_test,ax=ax[1])\n\nprint(\"Number of images in the train set : \", sum(quantity_tr.values()))\nprint(\"Number of images in the test set ; \",sum(quantity_te.values()))\nnumber_of_images_in_prediction_set = len(os.listdir(seg_pred_folders))\nprint(\"Number of images in prediction set : \",number_of_images_in_prediction_set)\n\nplt.show()","39962c83":"def save_history(history, model_name):\n    #convert the history.history dict to a pandas DataFrame:     \n    hist_df = pd.DataFrame(history.history) \n\n    # save to json:  \n    hist_json_file = model_name+'_history.json' \n    with open(hist_json_file, mode='w') as f:\n        hist_df.to_json(f)\n\n    # or save to csv: \n    hist_csv_file = model_name+'_history.csv'\n    with open(hist_csv_file, mode='w') as f:\n        hist_df.to_csv(f)\n        \ndef plot_accuracy_from_history(history, isinception=False):\n    color = sns.color_palette()\n    if(isinception == False):\n        acc = history.history['acc']\n        val_acc = history.history['val_acc']\n    else:\n        acc = history.history['accuracy']\n        val_acc = history.history['val_accuracy']\n    \n\n    epochs = range(len(acc))\n\n    sns.lineplot(epochs, acc, label='Training Accuracy')\n    sns.lineplot(epochs, val_acc,label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n    plt.figure()\n    plt.show()\n    \ndef plot_loss_from_history(history):\n    color = sns.color_palette()\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs = range(len(loss))\n    \n    sns.lineplot(epochs, loss,label='Training Loss')\n    sns.lineplot(epochs, val_loss, label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    plt.figure()\n    plt.show()\n    \ndef do_history_stuff(history, history_file_name, isinception=False):\n    save_history(history, history_file_name)\n    plot_accuracy_from_history(history, isinception)\n    plot_loss_from_history(history)","f2685296":"train_datagen = ImageDataGenerator( rescale = 1.0\/255.,shear_range=0.2,zoom_range=0.2)\n\n# we are rescaling by 1.0\/255 to normalize the rgb values if they are in range 0-255 the values are too high for good model performance. \ntrain_generator = train_datagen.flow_from_directory(seg_train_folders,\n                                                    batch_size=32,\n                                                    shuffle=True,\n                                                    class_mode='categorical',\n                                                    target_size=(150, 150))\n\nvalidation_datagen = ImageDataGenerator(rescale = 1.0\/255.) #we are only normalising to make the prediction, the other parameters were used for agumentation and train weights\nvalidation_generator = validation_datagen.flow_from_directory(seg_test_folders, shuffle=True, batch_size=1, class_mode='categorical', target_size=(150, 150))","c3d793e8":"inv_map_classes = {v: k for k, v in validation_generator.class_indices.items()}\nprint(validation_generator.class_indices)\nprint(inv_map_classes)","6cba57d1":"def show_few_images(number_of_examples=2, predict_using_model=None):\n    figure1, ax1 = plt.subplots(number_of_examples,len(os.listdir(seg_train_folders)), figsize=(20,4*number_of_examples))\n    ax1 = ax1.reshape(-1)\n    axoff_fun = np.vectorize(lambda ax:ax.axis('off'))\n    axoff_fun(ax1)\n    axs = 0\n    for i, folder in enumerate(os.listdir(seg_train_folders)):\n        image_ids = os.listdir(os.path.join(seg_train_folders,folder))\n        for j in [random.randrange(0, len(image_ids)) for i in range(0,number_of_examples)]:\n            \n            display = plt.imread(os.path.join(seg_train_folders,folder,image_ids[j]))\n            plt.axis('off')\n            ax1[axs].imshow(display)\n            title = 'True:'+folder\n            if(predict_using_model):\n                predicted_classname = inv_map_classes[np.argmax(inception_best_model.predict(np.array([display])))]\n                title = title+'\\nPredict :'+predicted_classname\n            ax1[axs].set_title(title)\n            axs=axs+1","9f998968":"show_few_images(2)","1821f4ff":"tf.keras.backend.clear_session()\n\n# epoch config\nbenchmark_epoch = 60\nvgg_epoch = 60\nresnet_epoch = 60\ninception_epoch = 60","d3ebf60b":"#random architecture\nbenchmark_model = Sequential()\n# Input here is 4D array (batchsize, height, width, channels) - we have already created the train_generator with batch size 32\n# 32 Images of size each 150x150 with 3 color channels will be input into this layer\nbenchmark_model.add(Conv2D(128, kernel_size=7, activation='relu', input_shape=(150,150,3)))\nbenchmark_model.add(MaxPooling2D(pool_size=(4,4), strides=(2,2)))\nbenchmark_model.add(Conv2D(64, kernel_size=5, activation='relu'))\nbenchmark_model.add(MaxPooling2D(pool_size=(4,4), strides=(2,2)))\nbenchmark_model.add(Flatten())\nbenchmark_model.add(Dense(128,activation='relu'))\nbenchmark_model.add(Dense(6,activation='softmax'))\n\nbenchmark_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n\nbenchmark_model.summary()","8ecc1448":"# plot_model(benchmark_model, to_file='model.png',show_shapes=True, show_layer_names=True)\n# from IPython.display import FileLink\n# FileLink(r'.\/resnet50_-saved-model-08-acc-0.75.hdf5')","cde788e0":"filepath = \"bench_mark_-model-{epoch:02d}-{val_acc:.2f}.hdf5\"\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=5, min_lr=0.000002)\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\nhistory = benchmark_model.fit(train_generator,epochs=benchmark_epoch, verbose=1, validation_data = validation_generator,callbacks=[reduce_lr,early_stopping,checkpoint])\n\nbenchmark_model.save(filepath)\ndo_history_stuff(history, 'benchmark_model')\n","0a017b31":"vgg16_model = VGG16(pooling='avg', weights='imagenet', include_top=False, input_shape=(150,150,3))\nfor layers in vgg16_model.layers:\n            layers.trainable=False\nlast_output = vgg16_model.layers[-1].output\nvgg_x = Flatten()(last_output)\nvgg_x = Dense(128, activation = 'relu')(vgg_x)\nvgg_x = Dense(6, activation = 'softmax')(vgg_x)\nvgg16_final_model = Model(vgg16_model.input, vgg_x)\nvgg16_final_model.compile(loss = 'categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n\n# VGG16\nnumber_of_epochs = vgg_epoch\nvgg16_filepath = 'vgg_16_'+'-saved-model-{epoch:02d}-acc-{val_acc:.2f}.hdf5'\nvgg_checkpoint = tf.keras.callbacks.ModelCheckpoint(vgg16_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nvgg_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\nvgg16_history = vgg16_final_model.fit(train_generator, epochs = number_of_epochs ,validation_data = validation_generator,callbacks=[vgg_checkpoint,vgg_early_stopping],verbose=1)\n\ndo_history_stuff(vgg16_history, 'vgg16_model')","2653ee6c":"ResNet50_model = ResNet50(weights='imagenet', include_top=False, input_shape=(150,150,3), classes=6)\n\nfor layers in ResNet50_model.layers:\n    layers.trainable=True\n\nopt = SGD(lr=0.01,momentum=0.7)\n# resnet50_x = Conv2D(64, (3, 3), activation='relu')(ResNet50_model.output)\n# resnet50_x = MaxPooling2D(pool_size=(3, 3))(resnet50_x)\nresnet50_x = Flatten()(ResNet50_model.output)\nresnet50_x = Dense(256,activation='relu')(resnet50_x)\nresnet50_x = Dense(6,activation='softmax')(resnet50_x)\nresnet50_x_final_model = Model(inputs=ResNet50_model.input, outputs=resnet50_x)\nresnet50_x_final_model.compile(loss = 'categorical_crossentropy', optimizer= opt, metrics=['acc'])\n\nnumber_of_epochs = resnet_epoch\nresnet_filepath = 'resnet50'+'-saved-model-{epoch:02d}-val_acc-{val_acc:.2f}.hdf5'\nresnet_checkpoint = tf.keras.callbacks.ModelCheckpoint(resnet_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nresnet_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=5, min_lr=0.000002)\ncallbacklist = [resnet_checkpoint,resnet_early_stopping,reduce_lr]\nresnet50_history = resnet50_x_final_model.fit(train_generator, epochs = number_of_epochs ,validation_data = validation_generator,callbacks=callbacklist,verbose=1)\n\ndo_history_stuff(resnet50_history, 'resnet50_model')","8ca940bb":"# this could also be the output a different Keras model or layer\n\nInceptionV3_model = InceptionV3(input_shape=(150,150,3),weights='imagenet', include_top=False)\nfor layer in InceptionV3_model.layers[:249]:\n   layer.trainable = False\nfor layer in InceptionV3_model.layers[249:]:\n   layer.trainable = True\nInceptionV3_last_output = InceptionV3_model.output\nInceptionV3_maxpooled_output = Flatten()(InceptionV3_last_output)\nInceptionV3_x = Dense(1024, activation='relu')(InceptionV3_maxpooled_output)\nInceptionV3_x = Dropout(0.5)(InceptionV3_x)\nInceptionV3_x = Dense(6, activation='softmax')(InceptionV3_x)\nInceptionV3_x_final_model = Model(inputs=InceptionV3_model.input,outputs=InceptionV3_x)\nInceptionV3_x_final_model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy',metrics=['accuracy'])\n\nnumber_of_epochs = inception_epoch\ninception_filepath = 'inceptionv3_'+'-saved-model-{epoch:02d}-loss-{loss:.2f}.hdf5'\ninception_checkpoint = tf.keras.callbacks.ModelCheckpoint(inception_filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\ninception_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\ninceptionv3_history = InceptionV3_x_final_model.fit(train_generator, epochs = number_of_epochs, validation_data = validation_generator,callbacks=[inception_checkpoint,inception_early_stopping],verbose=1)\n\ndo_history_stuff(inceptionv3_history, 'inceptionv3_model', True)  ","bd3fc5ab":"#vgg_best_model = keras.models.load_model('..\/input\/pretrained-models-on-intel-image-classification\/vgg_16_-saved-model-15-acc-0.88.hdf5')\n#resnet_best_model = keras.models.load_model('..\/input\/pretrained-models-on-intel-image-classification\/resnet50-saved-model-11-val_acc-0.92.hdf5')\n#inception_best_model = keras.models.load_model('..\/input\/pretrained-models-on-intel-image-classification\/inceptionv3_-saved-model-03-loss-0.22.hdf5')\n#benchmark_model = keras.models.load_model('..\/input\/pretrained-models-on-intel-image-classification\/bench_mark_-model-18-0.79.hdf5')\nvgg_best_model = vgg16_final_model \nresnet_best_model = resnet50_x_final_model\ninception_best_model = InceptionV3_x_final_model ","d77c0b13":"\ndef mode(my_list):\n    ct = Counter(my_list)\n    max_value = max(ct.values())\n    return ([key for key, value in ct.items() if value == max_value])\n\ntrue_value = []\ncombined_model_pred = []\nvgg_pred = []\nresnet_pred = []\ninception_pred = []\nbenchmark_model_pred = []\nfor folder in os.listdir(seg_test_folders):\n    \n    test_image_ids = os.listdir(os.path.join(seg_test_folders,folder))\n    \n    for image_id in test_image_ids[:int(len(test_image_ids))]:\n        \n        path = os.path.join(seg_test_folders,folder,image_id)\n        \n        true_value.append(validation_generator.class_indices[folder])\n        img = cv2.resize(cv2.imread(path),(150,150))\n        img_normalized = img\/255\n        #vgg\n        vgg16_image_prediction = np.argmax(vgg_best_model.predict(np.array([img_normalized])))\n        vgg_pred.append(vgg16_image_prediction)\n        \n        #resnet50\n        resnet_50_image_prediction = np.argmax(resnet_best_model.predict(np.array([img_normalized])))\n        resnet_pred.append(resnet_50_image_prediction)\n        \n        #Inception\n        inception_image_prediction = np.argmax(inception_best_model.predict(np.array([img_normalized])))\n        inception_pred.append(inception_image_prediction)\n        \n        #benchmark\n        benchmark_model_prediction = np.argmax(benchmark_model.predict(np.array([img_normalized])))\n        benchmark_model_pred.append(benchmark_model_prediction)\n        \n        #giving vgg16 high priority if they all predict something different\n        image_prediction = mode([vgg16_image_prediction, resnet_50_image_prediction, inception_image_prediction])                                  \n        combined_model_pred.append(image_prediction)","9ea14320":"from sklearn.metrics import confusion_matrix\nimport itertools\n#from mlxtend.plotting import plot_confusion_matrix\ndef clf_report(true_value, model_pred):\n    \n    classes = validation_generator.class_indices.keys()\n    TP_count = [true_value[i] == model_pred[i] for i in range(len(true_value))]\n    model_accuracy = np.sum(TP_count)\/len(TP_count)\n    print('Model Accuracy', model_accuracy)\n    \n    plt.figure(figsize=(7,7))\n    cm = confusion_matrix(true_value,model_pred)\n    plt.imshow(cm,interpolation='nearest',cmap=plt.cm.viridis)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    thresh = cm.max()*0.8\n    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n        plt.text(j,i,cm[i,j],\n                horizontalalignment=\"center\",\n                color=\"black\" if cm[i,j] > thresh else \"white\")\n        pass\n    \n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    pass\n    \n    print(classification_report(true_value, model_pred, target_names = list(classes)))\n    ","654d332d":"#benchmark model\nclf_report(true_value, benchmark_model_pred)","f7d985f7":"#combined vote\ncombined_model_pred = [ c[0] for c in combined_model_pred]\nclf_report(true_value, combined_model_pred)","744c2bc0":"# VGG model classification report\nclf_report(true_value, vgg_pred)","9acc79f4":"# Resnet50 model classification report\nclf_report(true_value, resnet_pred)","02d24496":"# Inception model classification report\nclf_report(true_value, inception_pred)","956e3ae5":"show_few_images(1,benchmark_model_pred)","4c9740d0":"show_few_images(1,vgg_pred)","6671048e":"show_few_images(1,resnet_pred)","ae3492f9":"show_few_images(1,inception_pred)","572aca50":"show_few_images(1,combined_model_pred)","cde04954":"### Combining the best models of VGG16, Resnet50 & InceptionV3"}}