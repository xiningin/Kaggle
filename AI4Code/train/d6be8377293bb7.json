{"cell_type":{"48588c5a":"code","03015b82":"code","9674be85":"code","765bfd9c":"code","fafd07bf":"code","ef2a5df4":"code","de004dc7":"code","f62c4be1":"code","e470e1e8":"code","5ac9cb03":"code","62bbb2e9":"code","761275cd":"code","27574641":"code","a44a1f0e":"markdown","0eb99198":"markdown","81b59adc":"markdown","931f17bb":"markdown","97b5ba14":"markdown","64d5811e":"markdown"},"source":{"48588c5a":"import os\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport torchaudio\nfrom collections import OrderedDict","03015b82":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","9674be85":"class FCN(nn.Module):\n    def __init__(self, num_classes=264, pretrained=False):\n        super().__init__()\n        self.head = models.vgg16_bn(pretrained=pretrained)\n        self.head.classifier[6] = nn.Sequential(\n            nn.Linear(4096, 1024, bias=True),\n            nn.BatchNorm1d(1024),\n            nn.Dropout(.25),\n            nn.ReLU(),\n            nn.Linear(1024, 512, bias=True),\n            nn.BatchNorm1d(512),\n            nn.Dropout(.5),\n            nn.ReLU(),\n            nn.Linear(512, 264, bias=True),\n        )\n    \n    def freeze(self, n_top=0, freeze_head=True):\n        \"\"\"\n        :param n_top: Number of layers to freeze off the top classification layer.\n        :param freeze_head: If the feature extractor of the network should be frozen.\n        \"\"\"\n        self.head.features.requires_grad = not freeze_head\n        for head in range(n_top):\n            self.head.classifier[head].requires_grad = False\n    \n    def forward(self, x):\n        x = x.repeat(1, 3, 1, 1)\n        x = self.head(x)\n        return torch.sigmoid(x)","765bfd9c":"model = FCN()","fafd07bf":"os.listdir('\/kaggle\/input\/birdsong-recognition\/')","ef2a5df4":"TEST = Path('\/kaggle\/input\/birdsong-recognition\/test_audio').exists()\nROOT = Path('\/kaggle\/input\/birdsong-recognition\/')\nif TEST:\n    TEST_DIR = Path('\/kaggle\/input\/birdsong-recognition\/test_audio')\n    META_DIR = Path('\/kaggle\/input\/birdsong-recognition\/')\nelse:\n    TEST_DIR = Path('\/kaggle\/input\/birdcall-check\/test_audio')\n    META_DIR = Path('\/kaggle\/input\/birdcall-check\/')\nmeta = pd.read_csv(META_DIR \/ 'test.csv')","de004dc7":"class TestDataset(data.Dataset):\n    def __init__(self, meta: pd.DataFrame, test_dir: Path, device, img_size=224):\n        self.root = test_dir\n        self.meta = meta\n        self.device = device\n        self.img_size = img_size\n        self.amptodb = torchaudio.transforms.AmplitudeToDB()\n    \n    def __len__(self):\n        return len(self.meta)\n    \n    def __getitem__(self, idx):\n        sample = self.meta.iloc[idx]\n        y, sr = torchaudio.load(self.root \/ f'{sample.audio_id}.mp3')\n        y = y[0].reshape(y.shape[1])\n        y = y.to(self.device)\n        if sample.site in ['site_1', 'site_2']:\n            start, end = sr * (sample.seconds - 5), sr * (sample.seconds)\n            y = y[int(start):int(end)]\n        else:\n            start, end = sr * (0), sr * (5)\n            y = y[int(start):int(end)]\n        spectrogram = torchaudio.transforms.MelSpectrogram(sr, n_fft=2048, hop_length=512, f_min=10, f_max=1600).to(self.device)\n        y = spectrogram(y).detach().cpu()\n        y = self.amptodb(y)\n        y = y.reshape([1, y.shape[1], y.shape[0]])\n        \n        return sample.row_id, y","f62c4be1":"train_meta = pd.read_csv(Path(ROOT \/ 'train.csv'))\nebird_codes = {index: code for index, code in enumerate(train_meta.ebird_code.unique())}","e470e1e8":"ebird_codes","5ac9cb03":"model = FCN()\ncheckpoints = torch.load('\/kaggle\/input\/pretrained-models-cornell-birdcall-recognition\/model1.pth', map_location=device)\n\nnew_state_dict = OrderedDict()\nfor k, v in checkpoints.items():\n    if 'module' not in k:\n        k = 'module.'+k\n    else:\n        k = k[7:]\n    new_state_dict[k]=v\n\nmodel.load_state_dict(new_state_dict)\nmodel = model.to(device)\nmodel.eval()","62bbb2e9":"TEST_DATASET = TestDataset(meta, TEST_DIR, device=device)\nTHRESHOLD = 0.80","761275cd":"submission = []\nfor sample in range(len(TEST_DATASET)):\n    row_id, image = TEST_DATASET[sample]\n    image = image.reshape((1, *image.shape))\n    image = image.to(device, dtype=torch.float)\n    \n    outputs = model(image)\n    outputs = outputs.cpu().detach().numpy()\n    \n    # Create output string\n    indicies = (outputs>=THRESHOLD).nonzero()[1]\n    if len(indicies) == 0:\n        s = 'nocall'\n    else:\n        s = ' '.join([ebird_codes[i] for i in indicies])\n    submission.append([row_id, s])\nsub = pd.DataFrame(submission, columns=['row_id', 'birds'])","27574641":"sub.to_csv('submission.csv', index=False)","a44a1f0e":"## Define Device","0eb99198":"## Architecture of Pre-Trained Model\n- Squishing Melspectrogram into 224x224x3 and expanding the intensity dimension.\n- Applying pre-trained computer vision models like VGG, InceptionNet.\n- Output using sigmoidal activation function: 264 classes.\n- If none of the predictions for the bird vector are over a certain threshold, then it is classified as nocall.","81b59adc":"## Detect Hidden Directory and Load Submission Files\n- There is an *invisible* test set, so we need to detect whether or not it exists during submission.","931f17bb":"# Birdcall Submission Module","97b5ba14":"## Test Dataset Class\n- Helper functions to create MelSpectrogram: mono-to-color spectrogram conversion.\n- Dataset object that acts as an interface for the test data.","64d5811e":"## Inference\n- Define the labels map, where each index matches to a specific bird.\n- Loading pre-trained model.\n- Creating test set object.\n- Looping through test set object and creating predictions."}}