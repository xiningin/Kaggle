{"cell_type":{"426d0de9":"code","be59a5a0":"code","a9b963af":"code","4c14ebbb":"code","93decb6b":"code","44bc72b8":"code","6c6af469":"code","3ee6d22a":"code","22c0d1f3":"code","854454fb":"code","dabebb9d":"code","4876356b":"code","5359bb5a":"code","781cdb04":"code","8af1a1cf":"code","997d0ca7":"code","ad2dc898":"code","07c87c01":"code","17c6f53a":"code","75c8744c":"code","b0997d37":"code","6d4ff389":"code","8829836a":"code","bab56368":"code","496f6dd5":"code","aedd0304":"code","bf41928e":"code","d6c8686e":"code","ce0282ac":"code","410ddfb3":"code","031c8632":"code","538944ab":"code","c52561ef":"code","ff4da09c":"code","78435dee":"code","f64884f2":"code","89e1e454":"code","ab8db88c":"code","152348b0":"code","fdb67a46":"code","acb80830":"code","d1540d0d":"code","35e2c6c2":"code","4c66557c":"code","d13fb388":"code","cf6be805":"code","adfcf1b0":"code","19246be3":"code","e7346bff":"code","5172dc97":"code","cc0f441e":"code","582388cd":"code","58cff707":"code","7f1ffb13":"code","8accbf17":"code","34c15239":"code","532c8bce":"code","36e9b5a8":"code","be5d9755":"code","e01dc1b8":"code","60635de7":"code","927c6c17":"code","9aa3f2be":"code","a08f547f":"code","b002967b":"code","8ea4b859":"code","5036a4c2":"code","6079cee7":"code","32fab8c6":"code","dc6d6a7d":"code","c947cc6c":"code","f66712c5":"code","553d2fa8":"code","2bc1befd":"code","32ade0f7":"code","4fc2f26b":"code","dc5a22c9":"code","3bac0fac":"code","a1f79842":"code","1a2c62de":"code","77d0f5c9":"code","94ea0184":"code","ab86f49a":"code","27328edf":"code","bc9fd6b3":"code","04b955e4":"code","1f076253":"code","b48569ed":"code","2f7e47b2":"code","fa6fbee2":"code","38659b0e":"code","d856b0a8":"code","c6a0a77a":"markdown","e53a6cfc":"markdown","f1c03ef9":"markdown","20824f32":"markdown","3d50923f":"markdown","e0e4672f":"markdown","38d49434":"markdown","c735a920":"markdown","813df494":"markdown","742ce853":"markdown","e36afb91":"markdown","3e747485":"markdown","7a0bd847":"markdown","97491a35":"markdown","12bc8e2e":"markdown","e996b869":"markdown","840d3a46":"markdown","7d5b53b6":"markdown","c1edcf65":"markdown","0710edb6":"markdown","265562ba":"markdown","170e13ba":"markdown","13068ae7":"markdown","e1ce2c16":"markdown","80a6cbf7":"markdown","46e655aa":"markdown","db911363":"markdown","b34db6b8":"markdown","46aa7884":"markdown","1797f3d7":"markdown","81d68c9d":"markdown","df0cc0ad":"markdown","906aaf5f":"markdown","e3ee8a99":"markdown","3001d15a":"markdown","a4db0114":"markdown","e0451acb":"markdown","60969153":"markdown","b2ed1a52":"markdown","439492ea":"markdown","f7623e70":"markdown","29ddfd51":"markdown","075fccf7":"markdown","6df32f77":"markdown","aa7f1e39":"markdown","926f9e49":"markdown","0a7781fe":"markdown","5068385a":"markdown","f24bed8f":"markdown","cd83b1a1":"markdown","2a175738":"markdown","46de0db0":"markdown","5692224f":"markdown","41b65a3e":"markdown","7c2fc122":"markdown","81770a98":"markdown","aed1289c":"markdown"},"source":{"426d0de9":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\n\n\nfrom sklearn.linear_model import LinearRegression, SGDRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom lightgbm import LGBMRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, Ridge\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer, KNNImputer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.preprocessing import FunctionTransformer\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, StratifiedKFold, train_test_split, cross_validate\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.ensemble import VotingRegressor, StackingRegressor\n\nfrom scipy.stats import skew, boxcox_normmax\nfrom scipy.special import boxcox1p, inv_boxcox1p\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nsns.set_style('darkgrid')\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","be59a5a0":"dataset_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndataset_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","a9b963af":"def screen_data(df):\n    rows = []\n    for col in df.columns:\n        rows.append([col, df[col].isnull().sum(), df[col].nunique(), df[col].dtypes])\n    return pd.DataFrame(rows, columns=['Col', 'Missing values', 'Unique values', 'Type'])","4c14ebbb":"def screen_missing_values(df, ratio=0.1, verbose=0):\n    total_rows = len(df)\n    total_columns = len(df.columns)\n    df = screen_data(df)\n    df_m = df.loc[df['Missing values'] != 0]#.sort_values(['Type', 'Missing values'])\n    total_missing_values = len(df_m)\n    df_m.loc[:, 'Percentage missing values'] = df_m['Missing values']\/total_rows\n    missing_values_less10 = len(df_m.loc[df_m['Percentage missing values'] <= ratio])\n    missing_values_more10 = len(df_m.loc[df_m['Percentage missing values'] > ratio])\n    print('Total missing values : ',total_missing_values, ' --> ', round(100*total_missing_values\/total_columns,0), '% of all features')\n    print('Columns with less than ', round(ratio*100,0),'% missing values : ',missing_values_less10, ' --> ', round(100*missing_values_less10\/total_missing_values, 0), '% of features with missing values')\n    print('Columns with more than ', round(ratio*100,0),'% missing values : ',missing_values_more10, ' --> ', round(100*missing_values_more10\/total_missing_values, 0), '% of features with missing values')\n    if(verbose >= 1):\n        print(\"\")\n        print(df_m.drop(columns=['Unique values', 'Type']).loc[df_m['Percentage missing values'] > ratio])","93decb6b":"dataset_train.shape, dataset_test.shape","44bc72b8":"dataset_train.head(10)","6c6af469":"df = screen_data(dataset_train)\ndf.T","3ee6d22a":"screen_missing_values(dataset_train, verbose=1)","22c0d1f3":"screen_missing_values(dataset_test, verbose=1)","854454fb":"print('Categorical features : ', len(dataset_train.drop(columns=['Id', 'SalePrice']).select_dtypes(include='object').columns))\nprint('Numerical features : ', len(dataset_train.drop(columns=['Id', 'SalePrice']).select_dtypes(exclude='object').columns))\nprint('Total features : ', len(dataset_train.drop(columns=['Id', 'SalePrice']).columns))","dabebb9d":"fig, ax = plt.subplots(1,2, figsize=(22,5))\nsns.boxplot(x=\"MSZoning\", y=\"SalePrice\", data=dataset_train, ax=ax[0])\nax[0].set_title('Price vs MSZoning');\nsns.countplot(x=\"MSZoning\", data=dataset_train, ax=ax[1])\nax[1].set_title('Count per MSZoning');","4876356b":"order = dataset_train.groupby('Neighborhood').agg(np.mean).sort_values('SalePrice').index.to_list()\nfig, ax = plt.subplots(2,1, figsize=(22,10))\nsns.boxplot(x=\"Neighborhood\", y=\"SalePrice\", data=dataset_train, order=order, ax=ax[0])\nax[0].set_title('Price vs Neighborhood');\nsns.countplot(x=\"Neighborhood\", data=dataset_train, order=order, ax=ax[1])\nax[1].set_title('Price vs Neighborhood');","5359bb5a":"order = dataset_train.groupby('Neighborhood').agg(np.mean).sort_values('SalePrice').index.to_list()\nfig, ax = plt.subplots(1,1, figsize=(22,7))\nsns.boxplot(x=\"Neighborhood\", y=\"SalePrice\", hue=\"MSZoning\", data=dataset_train, order=order, ax=ax)\nax.set_title('Price vs Neighborhood');","781cdb04":"dataset_train['TotalSF'] = dataset_train['1stFlrSF'] + dataset_train['2ndFlrSF'] + dataset_train['TotalBsmtSF']\nfig, ax = plt.subplots(2,3, figsize=(22,10))\nsns.scatterplot(x=\"LotArea\", y=\"SalePrice\", data=dataset_train, ax=ax[0][0])\nax[0][0].set_title('SalePrice vs LotArea');\nsns.scatterplot(x=\"TotalSF\", y=\"SalePrice\", data=dataset_train, ax=ax[0][1])\nax[0][1].set_title('SalePrice vs TotalSF');\nsns.scatterplot(x=\"GrLivArea\", y=\"SalePrice\", data=dataset_train, ax=ax[0][2])   # LowQualFinSF or BsmtUnfSF\nax[0][2].set_title('SalePrice vs GrLivArea');\nsns.scatterplot(x=\"1stFlrSF\", y=\"SalePrice\", data=dataset_train, ax=ax[1][0])\nax[1][0].set_title('SalePrice vs 1stFlrSF');\nsns.scatterplot(x=\"2ndFlrSF\", y=\"SalePrice\", data=dataset_train, ax=ax[1][1])\nax[1][1].set_title('SalePrice vs 2ndFlrSF');\nsns.scatterplot(x=\"TotalBsmtSF\", y=\"SalePrice\", data=dataset_train, ax=ax[1][2])\nax[1][2].set_title('SalePrice vs TotalBsmtSF');\n","8af1a1cf":"order_qual = dataset_train.groupby('OverallQual').agg(np.mean).sort_values('SalePrice').index.to_list()\norder_cond = dataset_train.groupby('OverallCond').agg(np.mean).sort_values('SalePrice').index.to_list()\nfig, ax = plt.subplots(2,1, figsize=(22,10))\nsns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=dataset_train, order=order_qual, ax=ax[0])\nax[0].set_title('Price vs OverallQual');\nsns.boxplot(x=\"OverallCond\", y=\"SalePrice\", data=dataset_train, order=order_qual, ax=ax[1])\nax[1].set_title('Price vs OverallCond');","997d0ca7":"#order = dataset_train.groupby('Neighborhood').agg(np.mean).sort_values('SalePrice').index.to_list()\nfig, ax = plt.subplots(1,1, figsize=(22,7))\nsns.boxplot(x=\"YearBuilt\", y=\"SalePrice\", data=dataset_train, ax=ax)\nax.set_title('Price vs YearBuilt');","ad2dc898":"dataset_train['DecadeBuilt'] = dataset_train['YearBuilt']\/10\ndataset_train['DecadeBuilt'] = dataset_train['DecadeBuilt'].astype(int)*10\n#dataset_train[['YearBuilt', 'DecadeBuilt']]\n\nfig, ax = plt.subplots(1,1, figsize=(22,7))\nsns.boxplot(x=\"DecadeBuilt\", y=\"SalePrice\", data=dataset_train, ax=ax)\nax.set_title('Price vs YearBuilt');","07c87c01":"fig, ax = plt.subplots(1,1, figsize=(22,7))\nsns.boxplot(x=\"YearRemodAdd\", y=\"SalePrice\", data=dataset_train, ax=ax)\nax.set_title('Price vs YearRemodAdd');","17c6f53a":"dataset_train['DecadeRemodAdd'] = dataset_train['YearRemodAdd']\/10\ndataset_train['DecadeRemodAdd'] = dataset_train['DecadeRemodAdd'].astype(int)*10\n#dataset_train[['YearBuilt', 'DecadeBuilt']]\n\nfig, ax = plt.subplots(1,1, figsize=(22,7))\nsns.boxplot(x=\"DecadeRemodAdd\", y=\"SalePrice\", data=dataset_train, ax=ax)\nax.set_title('Price vs DecadeRemodAdd');","75c8744c":"fig, ax = plt.subplots(1,2, figsize=(22,7))\nsns.boxplot(x=\"BedroomAbvGr\", y=\"SalePrice\", data=dataset_train, ax=ax[0])\nax[0].set_title('Price vs number of bedrooms');\nsns.boxplot(x=\"TotRmsAbvGrd\", y=\"SalePrice\", data=dataset_train, ax=ax[1])\nax[1].set_title('Price vs number of rooms');","b0997d37":"df_0bd = dataset_train.loc[dataset_train['BedroomAbvGr'] == 0]\ndf_0bd[['YearBuilt', 'BldgType', 'BsmtFinType1', 'BsmtFinType2', 'TotalBsmtSF', 'GrLivArea', 'TotRmsAbvGrd', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'KitchenAbvGr', 'SalePrice']].head(20)","6d4ff389":"df_0bd = dataset_test.loc[dataset_train['BedroomAbvGr'] == 0]\ndf_0bd[['YearBuilt', 'BldgType', 'BsmtFinType1', 'BsmtFinType2', 'TotalBsmtSF', 'GrLivArea', 'TotRmsAbvGrd', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'KitchenAbvGr']].head(20)\n","8829836a":"fig, ax = plt.subplots(2,3, figsize=(22,10))\nsns.boxplot(x=\"SaleType\", y=\"SalePrice\", data=dataset_train, ax=ax[0][0])\nax[0][0].set_title('Price vs SaleType');\nsns.boxplot(x=\"MoSold\", y=\"SalePrice\", data=dataset_train, ax=ax[0][1])\nax[0][1].set_title('Price vs month of sale');\nsns.boxplot(x=\"YrSold\", y=\"SalePrice\", data=dataset_train, ax=ax[0][2])\nax[0][2].set_title('Price vs year of sale');\nsns.countplot(x=\"SaleType\", data=dataset_train, ax=ax[1][0])\nax[1][0].set_title('Price vs SaleType');\nsns.countplot(x=\"MoSold\", data=dataset_train, ax=ax[1][1])\nax[1][1].set_title('Price vs month of sale');\nsns.countplot(x=\"YrSold\", data=dataset_train, ax=ax[1][2])\nax[1][2].set_title('Price vs year of sale');","bab56368":"dataset_train[['OverallQual', 'ExterQual', 'BsmtQual', 'HeatingQC', 'FireplaceQu', 'GarageQual', 'PoolQC']]","496f6dd5":"dataset_train[['OverallCond', 'ExterCond', 'BsmtCond', 'HeatingQC', 'GarageCond']]","aedd0304":"for col in ['ExterCond', 'BsmtCond', 'HeatingQC', 'GarageCond']:\n    dataset_train[col] = dataset_train[col].map({'Ex':9,'Gd':7,'TA':5,'Fa':3,'NA':np.nan}, na_action='ignore')\n \ndataset_train['OverallCond_calculated'] = dataset_train[['ExterCond', 'BsmtCond', 'HeatingQC', 'GarageCond']].mean(axis=1)\ndataset_train['OverallCond'] = dataset_train['OverallCond'].astype(float)\ndf = dataset_train.loc[abs(dataset_train['OverallCond'] - dataset_train['OverallCond_calculated']) > 1]\ndataset_train['OverallCond_calculated'] = dataset_train['OverallCond_calculated'].astype(int)\ndataset_train['OverallCond_calculated_2'] = dataset_train[['OverallCond_calculated', 'OverallCond']].mean(axis=1).astype(int)","bf41928e":"fig, ax = plt.subplots(3,1, figsize=(22,15))\nsns.boxplot(x=\"OverallCond\", y=\"SalePrice\", data=dataset_train, ax=ax[0])\nax[0].set_title('Price vs OverallQual');\nsns.boxplot(x=\"OverallCond_calculated\", y=\"SalePrice\", data=dataset_train, ax=ax[1])\nax[1].set_title('Price vs OverallCond calculated');\nsns.boxplot(x=\"OverallCond_calculated_2\", y=\"SalePrice\", data=dataset_train, ax=ax[2])\nax[2].set_title('Price vs OverallCond calculated 2');","d6c8686e":"#TotalSF = TotalBsmtSF + 1stFlrSF + 2ndFlrSF + BsmtUnfSF + LowQualFinSF\ndataset_train[['TotalSF', 'GrLivArea', 'TotalBsmtSF','1stFlrSF','2ndFlrSF', 'BsmtUnfSF','LowQualFinSF']]","ce0282ac":"len(dataset_train.loc[dataset_train['BedroomAbvGr']==0]), len(dataset_test.loc[dataset_test['BedroomAbvGr']==0])","410ddfb3":"dataset_train.loc[dataset_train['BedroomAbvGr']==0]","031c8632":"#df = dataset_train.loc[abs(dataset_train['TotRmsAbvGrd'] - dataset_train['KitchenAbvGr'] -  dataset_train['BedroomAbvGr']) >= 4]\ndf = dataset_train.copy()\ndf['bedrooms_ratio'] = dataset_train['BedroomAbvGr']\/dataset_train['TotRmsAbvGrd']\ndf[['TotRmsAbvGrd', 'KitchenAbvGr', 'BedroomAbvGr', 'bedrooms_ratio']].describe()","538944ab":"df.loc[(df['bedrooms_ratio'] <= 0.2) | (df['bedrooms_ratio'] >= 0.8), ['TotRmsAbvGrd', 'KitchenAbvGr', 'BedroomAbvGr', 'bedrooms_ratio']].head(20)","c52561ef":"fig, ax = plt.subplots(1,2, figsize=(22,7))\nsns.histplot(x=\"bedrooms_ratio\", data=df, stat=\"probability\", ax=ax[0])\nax[0].set_title('Count of bedrooms_ratio');\nsns.scatterplot(x=\"bedrooms_ratio\", y=\"TotRmsAbvGrd\", data=df, ax=ax[1])\nax[1].set_title('TotRmsAbvGrd vs bedrooms ratio');","ff4da09c":"mask = (((df['bedrooms_ratio'] < 0.5) & (df['TotRmsAbvGrd'] <= 4)) | ((df['bedrooms_ratio'] <= 0.33) & (df['TotRmsAbvGrd'] >= 5))) | (((df['bedrooms_ratio'] >= 0.7) & (df['TotRmsAbvGrd'] >= 6)) | ((df['bedrooms_ratio'] >= 0.8) & (df['TotRmsAbvGrd'] <= 5)))\ndf.loc[mask, ['TotRmsAbvGrd', 'KitchenAbvGr', 'BedroomAbvGr', 'bedrooms_ratio']].head(20)","78435dee":"#Let's first put a kitchen in every house\ndf.loc[df['KitchenAbvGr'] == 0, 'KitchenAbvGr'] = 1\nmask = (((df['bedrooms_ratio'] < 0.5) & (df['TotRmsAbvGrd'] <= 4)) | ((df['bedrooms_ratio'] <= 0.33) & (df['TotRmsAbvGrd'] >= 5))) | (((df['bedrooms_ratio'] >= 0.7) & (df['TotRmsAbvGrd'] >= 5)) | ((df['bedrooms_ratio'] >= 0.8) & (df['TotRmsAbvGrd'] <= 4)))\nwhile len(df.loc[mask]) != 0:\n    #small houses, this means that small house have more bedrooms than other room\n    df.loc[(df['bedrooms_ratio'] < 0.5) & (df['TotRmsAbvGrd'] <= 4), 'BedroomAbvGr'] += 1\n    df.loc[(df['bedrooms_ratio'] >= 0.8) & (df['TotRmsAbvGrd'] <= 4), 'BedroomAbvGr'] -= 1\n    #big houses\n    df.loc[(df['bedrooms_ratio'] < 0.33) & (df['TotRmsAbvGrd'] >= 5), 'BedroomAbvGr'] += 1\n    df.loc[(df['bedrooms_ratio'] >= 0.7) & (df['TotRmsAbvGrd'] >= 5), 'BedroomAbvGr'] -= 1\n    #update value of bedroom ratio\n    df.loc[:, 'bedrooms_ratio'] = df['BedroomAbvGr']\/df['TotRmsAbvGrd']\n    mask = (((df['bedrooms_ratio'] < 0.5) & (df['TotRmsAbvGrd'] <= 4)) | ((df['bedrooms_ratio'] <= 0.33) & (df['TotRmsAbvGrd'] >= 5))) | (((df['bedrooms_ratio'] >= 0.7) & (df['TotRmsAbvGrd'] >= 5)) | ((df['bedrooms_ratio'] >= 0.8) & (df['TotRmsAbvGrd'] <= 4)))","f64884f2":"df[['TotRmsAbvGrd', 'KitchenAbvGr', 'BedroomAbvGr', 'bedrooms_ratio']].head(30)","89e1e454":"df[['TotRmsAbvGrd', 'KitchenAbvGr', 'BedroomAbvGr', 'bedrooms_ratio']].describe()","ab8db88c":"fig, ax = plt.subplots(2,2, figsize=(22,15))\nsns.boxplot(x=\"BedroomAbvGr\", y=\"SalePrice\", data=df, ax=ax[0][0])\nax[0][0].set_title('Price vs number of bedrooms after normalization');\nsns.boxplot(x=\"BedroomAbvGr\", y=\"SalePrice\", data=dataset_train, ax=ax[0][1])\nax[0][1].set_title('Price vs number of bedrooms before normalization');\nsns.countplot(x=\"BedroomAbvGr\", data=df, ax=ax[1][0])\nax[1][0].set_title('Count of number of bedrooms after normalization');\nsns.countplot(x=\"BedroomAbvGr\",data=dataset_train, ax=ax[1][1])\nax[1][1].set_title('Count of number of bedrooms before normalization');","152348b0":"dataset_train['SF_per_BedroomAbvGr'] = dataset_train['TotalSF']\/dataset_train['BedroomAbvGr']\ndataset_train['SF_per_TotRmsAbvGrd_per_sf'] = dataset_train['TotalSF']\/dataset_train['TotRmsAbvGrd']\nfig, ax = plt.subplots(1,2, figsize=(22,7))\nsns.scatterplot(x=\"SF_per_BedroomAbvGr\", y=\"SalePrice\", data=dataset_train, ax=ax[0])\nax[0].set_title('Price vs SF_per_BedroomAbvGr');\nsns.scatterplot(x=\"SF_per_TotRmsAbvGrd_per_sf\", y=\"SalePrice\", data=dataset_train, ax=ax[1])\nax[1].set_title('Price vs SF_per_TotRmsAbvGrd_per_sf');","fdb67a46":"dataset_train['Total_bath'] = dataset_train['FullBath'] + 0.5*dataset_train['HalfBath'] + dataset_train['BsmtFullBath'] + 0.5*dataset_train['BsmtHalfBath']\nfig, ax = plt.subplots(1,2, figsize=(22,7))\nsns.boxplot(x=\"Total_bath\", y=\"SalePrice\", data=dataset_train, ax=ax[0])\nax[0].set_title('Price vs total number of bath');\nsns.countplot(x=\"Total_bath\", data=dataset_train, ax=ax[1])\nax[1].set_title('Price vs SF_per_TotRmsAbvGrd_per_sf');","acb80830":"dataset_train['Total_porch_SF'] = dataset_train[['WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch', 'ScreenPorch']].sum(axis=1)\nfig, ax = plt.subplots(1,2, figsize=(22,7))\nsns.scatterplot(x=\"Total_porch_SF\", y=\"SalePrice\", data=dataset_train, ax=ax[0])\nax[0].set_title('Price vs Total_porch_SF');","d1540d0d":"fig, ax = plt.subplots(1,3, figsize=(22,7))\nsns.boxplot(x=\"GarageCars\", y=\"SalePrice\", data=dataset_train, ax=ax[0])\nax[0].set_title('Price vs GarageCars');\nsns.scatterplot(x=\"GarageArea\", y=\"SalePrice\", data=dataset_train, ax=ax[1])\nax[1].set_title('Price vs GarageArea');\nsns.countplot(x=\"GarageCars\", data=dataset_train, ax=ax[2])\nax[2].set_title('Count of GarageCars');","35e2c6c2":"fig, ax = plt.subplots(2,3, figsize=(22,12))\nsns.boxplot(x=\"MiscFeature\", y=\"SalePrice\", data=dataset_train, ax=ax[0][0])\nax[0][0].set_title('Price vs MiscFeature');\nsns.scatterplot(x=\"PoolArea\", y=\"SalePrice\", data=dataset_train, ax=ax[0][1])\nax[0][1].set_title('Price vs PoolArea');\nsns.scatterplot(x=\"MiscVal\", y=\"SalePrice\", data=dataset_train, ax=ax[0][2])\nax[0][2].set_title('SalePrice of MiscVal');\nsns.boxplot(x=\"Fireplaces\", y=\"SalePrice\", data=dataset_train, ax=ax[1][0])\nax[1][0].set_title('Price vs Fireplaces');\nsns.boxplot(x=\"FireplaceQu\", y=\"SalePrice\", data=dataset_train, ax=ax[1][1])\nax[1][1].set_title('Price vs FireplaceQu');\nsns.countplot(x=\"Fireplaces\", data=dataset_train, ax=ax[1][2])\nax[1][2].set_title('Count of Fireplaces');","4c66557c":"dataset_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndataset_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain = dataset_train[['MSZoning', 'Neighborhood' ,'OverallQual', 'OverallCond', 'ExterCond', 'BsmtCond', 'HeatingQC', 'GarageCond', 'YearBuilt', 'YearRemodAdd', 'SaleType', 'BedroomAbvGr', 'TotRmsAbvGrd', 'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', 'KitchenAbvGr', 'GrLivArea', 'GarageCars']]\ntrain.isna().sum()","d13fb388":"train.describe()","cf6be805":"test = dataset_test[['MSZoning', 'Neighborhood' ,'OverallQual', 'OverallCond', 'ExterCond', 'BsmtCond', 'HeatingQC', 'GarageCond', 'YearBuilt', 'YearRemodAdd', 'SaleType', 'BedroomAbvGr', 'TotRmsAbvGrd', 'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', 'KitchenAbvGr', 'GrLivArea', 'GarageCars']]\ntest.isna().sum()","adfcf1b0":"def hist_train_test(train, test, col_type=\"numerical\"):\n    train['type'] = 'train'\n    test['type'] = 'test'\n    total = pd.concat([test, train])\n    if(col_type==\"numerical\"):\n        cols = total.select_dtypes(include=np.number).columns.to_list()\n    if(col_type==\"categorical\"):\n        cols = total.select_dtypes(include=['object', 'category']).columns.to_list()\n    nb_rows = int(round(0.9+(len(cols)\/3),0))\n    i = j = 0\n    fig, ax = plt.subplots(nb_rows,3, figsize=(22,nb_rows*6))\n    for col in cols:\n        if(col_type==\"numerical\"):\n            ax[i][j] = sns.histplot(x=col, data=total, hue=\"type\", multiple=\"dodge\", bins=20, ax=ax[i][j])\n        if(col_type==\"categorical\"):\n            ax[i][j] = sns.countplot(x=col, data=total, hue=\"type\", ax=ax[i][j])\n        if(j == 2):\n            i = i + 1\n        j = (j + 1)%3","19246be3":"hist_train_test(train, test, \"numerical\")","e7346bff":"hist_train_test(train, test, \"categorical\")","5172dc97":"#Preprocessing functions\ndef remove_outliers(df):\n    df.drop(df[df[\"GrLivArea\"] > 4000].index, inplace=True)\n    return df\n\ndef apply_label_encoder(df):\n    cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'OverallCond', \n        'YrSold', 'MoSold')\n    for c in cols:\n        lbl = LabelEncoder() \n        lbl.fit(list(df[c].values)) \n        df[c] = lbl.transform(list(df[c].values))\n    return df\n\n    \ndef get_house_particularities(df):\n    df[\"Has_shed\"] = (df[\"MiscFeature\"] == \"Shed\") * 1.  \n    df[\"Remodeled\"] = (df[\"YearRemodAdd\"] != df[\"YearBuilt\"]) * 1\n    df[\"Recent_remodel\"] = (df[\"YearRemodAdd\"] == df[\"YrSold\"]) * 1\n    df[\"Very_new_house\"] = (df[\"YearBuilt\"] == df[\"YrSold\"]) * 1\n    df[\"Has_2nd_floor\"] = (df[\"2ndFlrSF\"] != 0) * 1\n    df[\"Has_pool\"] = (df[\"PoolArea\"] != 0) * 1\n    df[\"Number_of_porch\"] = (df[\"OpenPorchSF\"] != 0) * 1 + (df[\"EnclosedPorch\"] != 0) * 1 + (df[\"3SsnPorch\"] != 0) * 1 + (df[\"ScreenPorch\"] != 0) * 1\n    df[\"Has_porch\"] = (df[\"Number_of_porch\"] != 0) * 1\n    df[\"Has_Wood_deck\"] = (df[\"WoodDeckSF\"] != 0) * 1    \n    return df\n\ndef normalize_SF_per_room(df):\n    df['SF_per_BedroomAbvGr'] = df['TotalSF']\/df['BedroomAbvGr']\n    df['SF_per_TotRmsAbvGrd'] = df['TotalSF']\/df['TotRmsAbvGrd']\n    return df\n\ndef get_estimation_OverallCond(df):\n    df_ = df.copy()\n    for col in ['ExterCond', 'BsmtCond', 'HeatingQC', 'GarageCond']:\n        df_[col+'_'] = df_[col].map({'Ex':9,'Gd':7,'TA':5,'Fa':3, 'Po':1, 'NA':np.nan}, na_action='ignore')\n    df['OverallCond_calculated_1'] = df_[['ExterCond_', 'BsmtCond_', 'HeatingQC_', 'GarageCond_']].mean(axis=1)\n    df['OverallCond'] = df['OverallCond'].astype(float)\n    df['OverallCond_calculated_2'] = df[['OverallCond_calculated_1', 'OverallCond']].mean(axis=1).astype(int)\n    #df = df.drop(columns=['ExterCond_', 'BsmtCond_', 'HeatingQC_', 'GarageCond_'],  axis=1)\n    return df\n\ndef get_estimation_bedrooms(df):\n    df.loc[:, 'bedrooms_ratio'] = df['BedroomAbvGr']\/df['TotRmsAbvGrd']\n    mask = (((df['bedrooms_ratio'] < 0.5) & (df['TotRmsAbvGrd'] <= 4)) | ((df['bedrooms_ratio'] <= 0.33) & (df['TotRmsAbvGrd'] >= 5))) | (((df['bedrooms_ratio'] >= 0.7) & (df['TotRmsAbvGrd'] >= 5)) | ((df['bedrooms_ratio'] >= 0.8) & (df['TotRmsAbvGrd'] <= 4)))\n    while len(df.loc[mask]) != 0:\n        #small houses, this means that small house have more bedrooms than other room\n        df.loc[(df['bedrooms_ratio'] < 0.5) & (df['TotRmsAbvGrd'] <= 4), 'BedroomAbvGr'] += 1\n        df.loc[(df['bedrooms_ratio'] >= 0.8) & (df['TotRmsAbvGrd'] <= 4), 'BedroomAbvGr'] -= 1\n        #big houses\n        df.loc[(df['bedrooms_ratio'] < 0.33) & (df['TotRmsAbvGrd'] >= 5), 'BedroomAbvGr'] += 1\n        df.loc[(df['bedrooms_ratio'] >= 0.7) & (df['TotRmsAbvGrd'] >= 5), 'BedroomAbvGr'] -= 1\n        #update value of bedroom ratio\n        df.loc[:, 'bedrooms_ratio'] = df['BedroomAbvGr']\/df['TotRmsAbvGrd']\n        mask = (((df['bedrooms_ratio'] < 0.5) & (df['TotRmsAbvGrd'] <= 4)) | ((df['bedrooms_ratio'] <= 0.33) & (df['TotRmsAbvGrd'] >= 5))) | (((df['bedrooms_ratio'] >= 0.7) & (df['TotRmsAbvGrd'] >= 5)) | ((df['bedrooms_ratio'] >= 0.8) & (df['TotRmsAbvGrd'] <= 4)))\n    return df\n\n\n\ndef bin_column(df, col, bins, labels):\n    df[col] = pd.cut(df[col], bins=bins, labels=labels)\n    df[col] = df[col].astype(float)\n    return df","cc0f441e":"class BoxcoxTransformer(BaseEstimator, TransformerMixin):\n    # This transformer returns dataframe instead of default ndarray\n    def __init__(self, skew_threshold=0.5):\n        self.lmbda = []\n        self.skewed_features = []\n        self.skew_threshold = skew_threshold\n\n    def fit(self, X, y=None):\n        skewed = X[X.dtypes[X.dtypes != \"object\"].index].apply(lambda x: skew(x.dropna().astype(float)))\n        self.skewed_features = skewed[skewed > self.skew_threshold].index.to_list()\n        for col in self.skewed_features:\n            self.lmbda.append(boxcox_normmax(X[col] + 1))\n        return self\n\n    def transform(self, X, *_):\n        X = pd.DataFrame(X)\n        if X[self.skewed_features].shape[1] != len(self.lmbda):\n            raise ValueError(\"X has %d features per sample, expected %d\"\n                             % (X[self.skewed_features].shape[1], len(self.lmbda)))\n        i = 0\n        for col in self.skewed_features:\n            X[col] = boxcox1p(X[col], self.lmbda[i])\n            i = i + 1\n        return X","582388cd":"class Classic_imputer():\n    def __init__(self, impute_0, impute_none, impute_med, impute_freq):\n        self.impute_0 = impute_0\n        self.impute_none = impute_none\n        self.impute_med = impute_med\n        self.impute_freq = impute_freq\n        \n        self.median_values = {}\n        self.most_frequent_values = {}\n\n    def fit(self, X, y=None):\n        #save the median and most frequent value to not have data leakage\n        for col in self.impute_med:\n            self.median_values[col] = X[col].mode()[0]\n        for col in self.impute_freq:\n            self.most_frequent_values[col] = X[col].mode()[0]\n        return self\n    \n    def transform(self, X):\n        #fill the NA value based on the rule\n        for col in self.impute_0:\n            X[col] = X[col].fillna(0)\n        for col in self.impute_none:\n            X[col] = X[col].fillna('none')\n        for col in self.impute_med:\n            X[col] = X[col].fillna(self.median_values[col])\n        for col in self.impute_freq:\n            X[col] = X[col].fillna(self.most_frequent_values[col])\n        return X","58cff707":"class Custom_imputer():\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n\n        return self\n    \n    def transform(self, X):\n\n        return self","7f1ffb13":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.exceptions import NotFittedError\nimport pandas as pd\nimport numpy as np\n\nclass DataFrameOneHotEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self,categories=\"auto\",drop=None,sparse=None, dtype=np.float64,handle_unknown=\"error\",col_overrule_params={}):\n        self.categories = categories\n        self.drop = drop\n        self.sparse = sparse\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n        self.col_overrule_params = col_overrule_params\n        \n    def fit(self, X, y=None):\n       \n        if type(X) != pd.DataFrame:\n            raise TypeError(f\"X should be of type dataframe, not {type(X)}\")\n\n        self.onehotencoders_ = []\n        self.column_names_ = []\n\n        for c in X.select_dtypes(include=[\"object\", \"category\"]).columns:\n            # Construct the OHE parameters using the arguments\n            ohe_params = {\n                \"categories\": self.categories,\n                \"drop\": self.drop,\n                \"sparse\": False,\n                \"dtype\": self.dtype,\n                \"handle_unknown\": self.handle_unknown,\n            }\n            # and update it with potential overrule parameters for the current column\n            ohe_params.update(self.col_overrule_params.get(c, {}))\n\n            # Regardless of how we got the parameters, make sure we always set the\n            # sparsity to False\n            ohe_params[\"sparse\"] = False\n\n            # Now create, fit, and store the onehotencoder for current column c\n            ohe = OneHotEncoder(**ohe_params)\n            self.onehotencoders_.append(ohe.fit(X.loc[:, [c]]))\n\n            # Get the feature names and replace each x0_ with empty and after that\n            # surround the categorical value with [] and prefix it with the original\n            # column name\n            feature_names = ohe.get_feature_names()\n            feature_names = [x.replace(\"x0_\", \"\") for x in feature_names]\n            feature_names = [f\"{c}_{x}\" for x in feature_names]\n\n            self.column_names_.append(feature_names)\n\n        return self\n\n    def transform(self, X):\n        if type(X) != pd.DataFrame:\n            raise TypeError(f\"X should be of type dataframe, not {type(X)}\")\n\n        if not hasattr(self, \"onehotencoders_\"):\n            raise NotFittedError(f\"{type(self).__name__} is not fitted\")\n\n        all_df = []\n\n        for i, c in enumerate(X.select_dtypes(include=[\"object\", \"category\"]).columns):\n            ohe = self.onehotencoders_[i]\n\n            transformed_col = ohe.transform(X.loc[:, [c]])\n\n            df_col = pd.DataFrame(transformed_col, columns=self.column_names_[i])\n            all_df.append(df_col)\n\n        return pd.concat([X.select_dtypes(exclude=[\"object\", \"category\"]), pd.concat(all_df, axis=1)], axis=1)","8accbf17":"class DataframeFunctionTransformer():\n    def __init__(self, func):\n        self.func = func\n\n    def transform(self, input_df, **transform_params):\n        return self.func(input_df)\n\n    def fit(self, X, y=None, **fit_params):\n        return self","34c15239":"def has_particularities(df):\n    df[\"Has_shed\"] = (df[\"MiscFeature\"] == \"Shed\") * 1  \n    df[\"Remodeled\"] = (df[\"YearRemodAdd\"] != df[\"YearBuilt\"]) * 1\n    df[\"Recent_remodel\"] = (df[\"YearRemodAdd\"] == df[\"YrSold\"]) * 1\n    df[\"Very_new_house\"] = (df[\"YearBuilt\"] == df[\"YrSold\"]) * 1\n    df[\"Bought_of_plan\"] = (df[\"YearBuilt\"] > df[\"YrSold\"]) * 1\n    df[\"Has_2nd_floor\"] = (df[\"2ndFlrSF\"] != 0) * 1\n    df[\"Has_pool\"] = (df[\"PoolArea\"] != 0) * 1\n    df[\"Number_of_porch\"] = (df[\"OpenPorchSF\"] != 0) * 1 + (df[\"EnclosedPorch\"] != 0) * 1 + (df[\"3SsnPorch\"] != 0) * 1 + (df[\"ScreenPorch\"] != 0) * 1\n    #df[\"Has_porch\"] = (df[\"Number_of_porch\"] != 0) * 1\n    df[\"Has_Wood_deck\"] = (df[\"WoodDeckSF\"] != 0) * 1\n    return df\n\ndef reformat_categorical_data(df):\n    df['MSSubClass'] = df['MSSubClass'].apply(str)\n    #df['YrSold'] = df['YrSold'].astype(str)\n    df['MoSold'] = df['MoSold'].astype(str)\n    return df\n\ndef new_features(df):\n    df.loc[(df['MSZoning'] != 'RL') & (df['MSZoning'] != 'RM'), 'MSZoning'] = 'other'\n    df['TotalSF'] = df['1stFlrSF'] + df['2ndFlrSF'] + df['TotalBsmtSF']\n    df['age'] = df['YrSold'] - df['YearBuilt']\n    df.loc[(df['SaleType'] != 'WD') & (df['SaleType'] != 'new'), 'SaleType'] = 'other'\n    df['Total_bath'] = df['FullBath'] + 0.5*df['HalfBath'] + df['BsmtFullBath'] + 0.5*df['BsmtHalfBath']\n    return df\n\ndef drop_features(df):\n    features = ['Utilities', 'Street', 'PoolQC', 'Id']\n    df_ = df.drop(columns = features)\n    return df_\n\ndef to_array(df):\n    return df.to_numpy()","532c8bce":"feature_engineering = Pipeline([\n    ('has_particularities' , DataframeFunctionTransformer(has_particularities)),\n    ('new_features' , DataframeFunctionTransformer(new_features))  \n])\n\nreformat_data_type = Pipeline([\n    ('reformat data type' , DataframeFunctionTransformer(reformat_categorical_data))\n])","36e9b5a8":"impute_0 = ['BsmtFullBath', 'BsmtHalfBath', 'TotalBsmtSF', 'BsmtFinSF2', 'GarageYrBlt', 'GarageArea', 'GarageCars', \n                          'MasVnrArea', 'BsmtUnfSF', 'BsmtFinSF1']\nimpute_none = ['MSSubClass', 'HeatingQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', \n                             'GarageCond', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtExposure', 'BsmtFinType2', 'MasVnrType', 'KitchenQual']\nimpute_med = ['LotFrontage']\nimpute_freq = ['SaleType', 'Electrical', 'Exterior1st', 'Exterior2nd', 'MSZoning', 'Functional']\n\n#divide the features in different categories for preprocessing\n#cat_features = train.drop(columns=already_selected_features).select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n#num_features = train.drop(columns=already_selected_features).select_dtypes(exclude=[\"object\", \"category\"]).columns.to_list()","be5d9755":"def get_column_names_from_ColumnTransformer(column_transformer):    \n    col_name = []\n    for transformer_in_columns in column_transformer.transformers_[:-1]:#the last transformer is ColumnTransformer's 'remainder'\n        raw_col_name = transformer_in_columns[2]\n        if isinstance(transformer_in_columns[1],Pipeline): \n            transformer = transformer_in_columns[1].steps[-1][1]\n        else:\n            transformer = transformer_in_columns[1]\n        try:\n            names = transformer.get_feature_names()\n            for i in range(len(names)):\n                split = names[i].split('_')\n                names[i] = raw_col_name[int(split[0][1:])] + '_' + split[1]\n        except AttributeError: # if no 'get_feature_names' function, use raw column name\n            names = raw_col_name\n        if isinstance(names,np.ndarray): # eg.\n            col_name += names.tolist()\n        elif isinstance(names,list):\n            col_name += names    \n        elif isinstance(names,str):\n            col_name.append(names)\n    return col_name","e01dc1b8":"def getDuplicateColumns(df):\n    duplicateColumnNames = set()\n    for x in range(df.shape[1]):\n        col = df.iloc[:, x]\n        for y in range(x + 1, df.shape[1]):\n            otherCol = df.iloc[:, y]\n            if col.equals(otherCol):\n                duplicateColumnNames.add(df.columns.values[y])\n\n    return list(duplicateColumnNames)","60635de7":"def get_feature_overfit(df, threshold=99.9, return_df = False):\n    col_overfit = []\n    value_overfit = []\n    for col in X_preprocessed.columns:\n        counts = X_preprocessed[col].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(X) * 100 > threshold:\n            col_overfit.append(col)\n            value_overfit.append(zeros \/ len(X_preprocessed) * 100)\n    if return_df:\n        return pd.DataFrame(value_overfit, index=col_overfit, columns=['value']).sort_values('value', ascending=False)\n    else:\n        return col_overfit","927c6c17":"#dataset_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ny = np.log1p(np.ravel(np.array(train['SalePrice']).T))\nX = train.drop(columns=['SalePrice'])\n\nX_pred = test.copy()\n\nfolds = KFold(n_splits=10, shuffle=True, random_state=0)","9aa3f2be":"#custom_imputer\n#boxcox_transformer\n#feature_engineering\n#drop\n#scaler\n\npipeline = Pipeline([\n    ('drop' , DataframeFunctionTransformer(drop_features)),\n    ('data type' , reformat_data_type),\n    ('imputer' , Classic_imputer(impute_0, impute_none, impute_med, impute_freq)),\n    ('boxcox transformer' , BoxcoxTransformer(skew_threshold = 0.5)),\n    ('feature_engineering', feature_engineering),\n    ('encoding' , DataFrameOneHotEncoder(handle_unknown=\"ignore\", sparse = False)),\n    #('scaler' ,        RobustScaler()),\n    #('scaler' ,        StandardScaler()),\n    #('pca'    ,        PCA(n_components=10))\n    #('to_array' , DataframeFunctionTransformer(to_array)),\n])\npipeline.fit(X)\nX_preprocessed = pipeline.transform(X)\nX_pred_preprocessed = pipeline.transform(X_pred)","a08f547f":"X","b002967b":"X_preprocessed","8ea4b859":"columns_full_of_0 = get_feature_overfit(X_preprocessed, threshold=99.9)\nX_preprocessed = X_preprocessed.drop(columns=columns_full_of_0)\nX_pred_preprocessed = X_pred_preprocessed.drop(columns=columns_full_of_0)","5036a4c2":"def get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return pd.DataFrame(pd.DataFrame(au_corr).reset_index().to_numpy(), columns=['col1', 'col2', 'corr'])","6079cee7":"corr = get_top_abs_correlations(X_preprocessed)\novercorrelated_features = corr.loc[corr['corr'] >= 0.98]['col1']","32fab8c6":"X_preprocessed = X_preprocessed.drop(columns=overcorrelated_features)\nX_pred_preprocessed = X_pred_preprocessed.drop(columns=overcorrelated_features)","dc6d6a7d":"(X_preprocessed.shape, X_pred_preprocessed.shape)","c947cc6c":"#xgb = XGBRegressor(n_estimators=200)\n#xgb.fit(X_preprocessed, y)\n#xgb.feature_importances_\n\n#sorted_idx = xgb.feature_importances_.argsort()[-25:]\n#xgb.feature_importances_[sorted_idx]\n#xgb.feature_importances_[sorted_idx]\n\n#fig, ax = plt.subplots(1,1, figsize=(22,12))\n#ax = plt.barh(X_preprocessed.columns[sorted_idx].map(str), xgb.feature_importances_[sorted_idx])\n#ax = plt.xlabel(\"Xgboost Feature Importance\")\n\n#print(X_preprocessed.columns[sorted_idx])\n","f66712c5":"def plot_result_regression(y_train, y_train_pred, algo_name):\n    # Plot residuals\n    plt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\n    plt.title(algo_name)\n    plt.xlabel(\"Predicted values\")\n    plt.ylabel(\"Residuals\")\n    plt.legend(loc = \"upper left\")\n    plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\n    plt.show()\n\n    # Plot predictions\n    plt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\n    plt.title(algo_name)\n    plt.xlabel(\"Predicted values\")\n    plt.ylabel(\"Real values\")\n    plt.legend(loc = \"upper left\")\n    plt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\n    plt.show()","553d2fa8":"models_list = {'Ridge': Ridge(),\n               #'DecisionTree Regressor': DecisionTreeRegressor(),\n               #'Random Forest': RandomForestRegressor(), \n               'SVR': SVR(), \n               'LGBMRegressor' :LGBMRegressor(verbosity = 0, force_row_wise=True), \n               #'XGBRegressor': XGBRegressor(learning_rate=0.008, n_estimators=6000, max_depth=4, min_child_weight=0, gamma=0.6, subsample=0.7, colsample_bytree=0.7,\n               #        objective='reg:squarederror', nthread=-1, scale_pos_weight=1, seed=27, reg_alpha=0.00006, use_label_encoder=False, verbosity = 0), \n               #'CatBoostRegressor': CatBoostRegressor(verbose = 0),\n               'Lasso': Lasso(alpha=0.0005),\n               #'KernelRidge': KernelRidge(),\n               'ElasticNet': ElasticNet(alpha=0.0006, l1_ratio=1),\n               'BayesianRidge': BayesianRidge(),\n               'GradientBoostingRegressor': GradientBoostingRegressor()\n              }\n\nscoring = {'max_error': 'max_error', 'neg_mean_squared_error': 'neg_mean_squared_error', 'r2':'r2'}\ncolumns = ['Model', 'Median fit time', 'Mean max error', 'Std max error', 'Mean error', 'Std mean error', 'Mean r2', 'Std r2']\n\nx = X_preprocessed.to_numpy()\n\nmodel_perf_matrix = []\npredictions = pd.DataFrame()\nfor model_name, model in models_list.items():\n    pipeline = Pipeline([\n        #('drop' , DataframeFunctionTransformer(drop_features)),\n        #('data type' , reformat_data_type),\n        #('imputer' , Classic_imputer(impute_0, impute_none, impute_med, impute_freq)),\n        #('boxcox transformer' , BoxcoxTransformer(skew_threshold = 0.5)),\n        #('feature_engineering', feature_engineering),\n        #('encoding' , DataFrameOneHotEncoder(handle_unknown=\"ignore\", sparse = False)),\n        #('scaler' ,        RobustScaler()),\n        #('scaler' ,        StandardScaler()),\n        #('pca'    ,        PCA(n_components=10)),\n        #('to_array' , DataframeFunctionTransformer(to_array)),\n        ('model' ,           model)\n    ])\n\n    cv_score = cross_validate(pipeline, x, y, cv=folds, scoring=scoring, verbose=0, error_score=\"raise\");\n    #cv_score = np.sqrt(-cross_val_score(pipeline, X, y, cv=folds, scoring=scoring));\n    model_perf_matrix.append([model_name, round(cv_score['fit_time'].mean(),3),\n                                          round(cv_score['test_max_error'].mean(),4), round(cv_score['test_max_error'].std(),4),\n                                          round(np.sqrt(-cv_score['test_neg_mean_squared_error']).mean(),4), round(np.sqrt(-cv_score['test_neg_mean_squared_error']).std(),4),\n                                          round(cv_score['test_r2'].mean(),4), round(cv_score['test_r2'].std(),4)])\n    \n    pipeline.fit(x,y)\n    predictions[model_name] = np.floor(np.expm1(pipeline.predict(X_pred_preprocessed.to_numpy()))).T\n    \ndf_model_perf = pd.DataFrame(model_perf_matrix, columns=columns)\ndf_model_perf","2bc1befd":"predictions.index = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\").Id\npredictions['avr'] = predictions[['Lasso', 'ElasticNet', 'GradientBoostingRegressor', 'LGBMRegressor']].mean(axis=1)","32ade0f7":"df = pd.read_csv(\"\/kaggle\/input\/house-price-best-prediction\/best_submission.csv\")\npredictions['best_pred'] = df['SalePrice'].to_list()\npredictions.head(20)","4fc2f26b":"predictions.sort_values('best_pred', ascending=True).head(20)","dc5a22c9":"predictions.sort_values('best_pred', ascending=False).head(20)","3bac0fac":"fig, ax = plt.subplots(2,3, figsize=(22,12))\nsns.scatterplot(x=\"Ridge\", y=\"best_pred\", data=predictions, ax=ax[0][0])\nax[0][0].set_title('Ridge vs best_pred');\nax[0][0].plot([0, 800000], [0, 800000], c = \"red\")\nsns.scatterplot(x=\"GradientBoostingRegressor\", y=\"best_pred\", data=predictions, ax=ax[0][1])\nax[0][1].set_title('GradientBoostingRegressor vs best_pred');\nax[0][1].plot([0, 800000], [0, 800000], c = \"red\")\nsns.scatterplot(x=\"LGBMRegressor\", y=\"best_pred\", data=predictions, ax=ax[0][2])\nax[0][2].set_title('LGBMRegressor vs best_pred');\nax[0][2].plot([0, 800000], [0, 800000], c = \"red\")\nsns.scatterplot(x=\"Lasso\", y=\"best_pred\", data=predictions, ax=ax[1][0])\nax[1][0].set_title('Lasso vs best_pred');\nax[1][0].plot([0, 800000], [0, 800000], c = \"red\")\nsns.scatterplot(x=\"ElasticNet\", y=\"best_pred\", data=predictions, ax=ax[1][1])\nax[1][1].set_title('ElasticNet vs best_pred');\nax[1][1].plot([0, 800000], [0, 800000], c = \"red\")\nsns.scatterplot(x=\"BayesianRidge\", y='best_pred', data=predictions, ax=ax[1][2])\nax[1][2].set_title('BayesianRidge vs best_pred');\nax[1][2].plot([0, 800000], [0, 800000], c = \"red\")","a1f79842":"models_list = { 'Lasso': { 'model' : Lasso(),\n                                  'param_grid' : { \n                                                        'alpha' : [5e-5, 1e-4, 0.0005, 0.0001]\n                                                }},\n                'ElasticNet': { 'model' : ElasticNet(),\n                                 'param_grid' : { \n                                                         'alpha' : [0.0004, 0.00045, 0.0005, 0.00055, 0.0006],\n                                                        'l1_ratio' : [0.9, 0.95, 0.99, 1, 1.01, 1.05]\n                                                         }},\n                'BayesianRidge': { 'model' : BayesianRidge(),\n                                         'param_grid' : { \n                                                         \n                                         }}}","1a2c62de":"%%time\n\nresults = {}\n\nfor model_name, model in models_list.items():\n    best_model = GridSearchCV(estimator=model['model'], param_grid=model['param_grid'], cv=folds, scoring='neg_mean_squared_error');\n    best_model.fit(X_preprocessed, y)\n    print(model_name)\n    print(best_model.best_params_)\n    best_model.best_score_ = np.sqrt(-best_model.best_score_)\n    print('Mean score : ', best_model.best_score_, ' Std : ', best_model.cv_results_['std_test_score'][best_model.best_index_])\n    model_results = {'estimator' : best_model.best_estimator_, 'best_params' : best_model.best_params_, \n                     'mean' : best_model.best_score_, 'std' : best_model.cv_results_['std_test_score'][best_model.best_index_]}\n    results[model_name] = model_results","77d0f5c9":"def blend_models_predict(models, weight, X):\n    i = 0\n    results = [0] * len(X)\n    for model in models:\n        results += models[model]['estimator'].predict(X) * weight[i]\n        i += 1\n    return results","94ea0184":"weight = [0.25, 0.25, 0.50]\nblended_score = np.sqrt(mean_squared_error(y, blend_models_predict(results, weight, X_preprocessed)))\nprint(blended_score)","ab86f49a":"#from sklearn.linear_model import RidgeCV\n#create a dictionary of our models\n#estimators = []\n#for model_name, model in results.items():\n#    estimators.append((model_name, model['estimator']))\n#create our voting classifier, inputting our models\n#ensemble = StackingRegressor(estimators=estimators,\n#                            final_estimator=RidgeCV(alphas=[7,10,10.5,11,11.5,12,15], cv=folds, scoring='neg_mean_squared_error'), cv=folds)\n","27328edf":"#ensemble.fit(X_preprocessed, y)\n#ensemble.score(X_preprocessed, y)","bc9fd6b3":"#y_pred = np.floor(inv_boxcox1p(blend_models_predict(results, weight, X_pred_preprocessed), lmbda))\ny_pred = np.floor(np.expm1(blend_models_predict(results, weight, X_pred_preprocessed)))","04b955e4":"output = pd.DataFrame({'Id': pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\").Id, 'SalePrice': y_pred})","1f076253":"output.head(100)","b48569ed":"q1 = output['SalePrice'].quantile(0.005)\nq2 = output['SalePrice'].quantile(0.995)\noutput['SalePrice'] = output['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\noutput['SalePrice'] = output['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)","2f7e47b2":"predictions['blended_model'] = output['SalePrice'].to_list()","fa6fbee2":"predictions.loc[predictions['blended_model'] > q2]","38659b0e":"predictions.loc[predictions['blended_model'] < q1]","d856b0a8":"output.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","c6a0a77a":"Actually there is only a few houses without bedrooms above ground, but some have a very high number of rooms. <br>\nThere is probably a way of estimating the number of bedrooms","e53a6cfc":"# 2. Dataset Exploratory analysis\nIn this section we will check the number and types of features for the dataset, if there is missing values, if there is corelation between some features, if we can remove some unnecessary features and if we can create new features from the ones we already have","f1c03ef9":"# 6. Pipeline results","20824f32":"## 4.1 Feature creation","3d50923f":"# 3. Feature exploratory analysis\n\nLet's start with two categorical data that describe the surroundings of the house MSZoning and Neighborhood.","e0e4672f":"## Description of the notebook","38d49434":"# 1. Data acquisition","c735a920":"#### Let's see if the total porch area is a good estimator of the price of the house.","813df494":"#### Let's now see if we can normalize the number of rooms and bedrooms by the size of the house.","742ce853":"#### Things we can learn from the previous analysis\n- The lot area is not much correlated to the price, but maybe we can transform this column to get a better correlation (log?)\n- The 1stFlrSF, 2ndFlrSF and TotalBsmtSF are correlated to the price but show an increase of the variance when the price increases.\n- Summing all these values together give us a better correlation, with some outliers that we might have to remove or transform","e36afb91":"It seems that by averaging the overall condition score and our calculated condition score and binning the result into two categories: <br>\ngood condition (>=5) and  average condition (<5) we could get a better feature.","3e747485":"Let's try to increase the have a bedroom ratio between 0.5 and 0.8 or 0.6 for big houses. <br>\nWe will increase or decrease the number of bedrooms until we reach a satisfactory ratio.","7a0bd847":"### Columns we will transform\n- OverallQual & OverallCond, we can probably aggregate these columns together\n- YearBuilt & YearRemodAdd\n- Check if all these columns ExterQual, ExterCond, BsmtCond, BsmtFinType1, BsmtFinType2, HeatingQC, KitchenQual, GarageQual and GarageCond give us a good aproximation of overall quality and overall condition or if there was some errors during the calculation\n- TotalSF = TotalBsmtSF + 1stFlrSF + 2ndFlrSF\n- TotalBath = FullBath + HalfBath + BsmtFullBath + BsmtHalfBath\n- TotalPorch = WoodDeckSF + openPorchSF + EnclosedPorch + 3SsnPorch + ScreenPorch","97491a35":"We can see that it doesn't include the basement SF which in my opinion is a mistake, for example on row number 1458, the house has half its surface in the basement. <br>\nThere must be some value in the surface.","12bc8e2e":"### Let's see what we can learn from the construction year and remodeling year","e996b869":"#### Things we can learn from the previous analysis\n- The overall quality seems a good indicator of the price.\n- The overall condition seems less good an indicator of the price, maybe with some binning we could get a better price indicator.","840d3a46":"#### Let's see if we can estimathe number of bedrooms","7d5b53b6":"The total size of the porch does not seem to be a good estimator of the SalePrice","c1edcf65":"# House price regression","0710edb6":"### Types of data","265562ba":"### Missing values","170e13ba":"## Let's check the document describing the data","13068ae7":"In the dataset test there is more columns with missing values, but most of them have less than 10% missing values. <br>\nThe column with more than 10% missing values are the same in train and test. <br>\nThe columns might have missing values to indicate an absence of pool or fireplace for example, we will have to analyse things a bit more to conclude.","e1ce2c16":"#### Let's look at the garage size","80a6cbf7":"### Let's see what we can learn from the number of room","46e655aa":"## 4.2 Checking for missing data\nFor now the features we are considering are : \n- MSZoning, Neighborhood\n- OverallQual & OverallCond, ExterCond, BsmtCond, HeatingQC, GarageCond\n- YearBuilt & YearRemodAdd\n- SaleType\n- Bedroom & TotRmsAbvGrd, FullBath + HalfBath + BsmtFullBath + BsmtHalfBath, Kitchen\n- TotalSF\n- GarageCars\n","db911363":"#### Things we can learn from the previous analysis\n- The price of recent houses is a bit higher than houses between 1900 and 1970.\n- House in the 1880's and the 1890's are also a bit more expensive.","b34db6b8":"There is differences of OverallCond between houses with the same score for ExterCond, BsmtCond, HeatingQC, GarageCond.\nWe could recalculate a new score for the overall condition.","46aa7884":"# 4. Data cleaning and feature selection\n\nLet's start by creating some new columns as we stated in the column analysis.","1797f3d7":"### Let's continue witht he size of the house\nWe will check the lot size and the size of the house.","81d68c9d":"The train and the test set are the same size, so it's going to be important not to overfit the model.","df0cc0ad":"### Let's check if the overall quality and condition of the house have an impact on the price","906aaf5f":"Based on this very small analysis we can see that we have 79 different features. <br>\nIn order to start analysing we will have to choose some feature we think might give us a good aproximation of the sale price. <br>\nIt is not possible to carry an extensive EDA on all the features. <br>\nLet's if we can drop some feature which give us the same info (abs(corr) > 0.85), or that represent the same idea.","e3ee8a99":"We could do like with Total_bath, 0, 1, 2 or 3+ categories for GarageCars","3001d15a":"## 4.3 Checking for similar distribution in the train and test datasets","a4db0114":"It does not seem interesting to take into account these Misc features. <br>\nMaybe we could integrate the number of fireplaces.","e0451acb":"### Let's dig a bit more in the houses with 0 bedroom above ground","60969153":"The distribution of the data between train set and test set is very close. <br>\nWe don't need to modify our data.","b2ed1a52":"We could categorize the total number of baths with 1, 2, 3 and 4+ number of bath","439492ea":"### Columns we will analyse\n- MSZoning because there is probably a difference between an agriculture zone, an industrial zone and a residential zone.\n- Lot area, which is the size of the lot\n- Neighborhood, probably the price is not the same depending on the neighborhood.\n- OverallQual & OverallCond\n- YearBuilt & YearRemodAdd\n- MoSold, YrSold: maybe the time of year has an impact on the price\n- SaleType\n- Bedroom & TotRmsAbvGrd\n- TotalSF = TotalBsmtSF + 1stFlrSF + 2ndFlrSF\n\n### Columns we will transform\n- OverallQual & OverallCond, we can probably aggregate thse columns together\n- YearBuilt & YearRemodAdd\n- Check if all these columns ExterQual, ExterCond, BsmtCond, BsmtFinType1, BsmtFinType2, HeatingQC, KitchenQual, GarageQual and GarageCond give us a good aproximation of overall quality and overall condition or if there was some errors during the calculation\n- TotalSF = TotalBsmtSF + 1stFlrSF + 2ndFlrSF\n- TotalBath = FullBath + HalfBath + BsmtFullBath + BsmtHalfBath\n- TotalPorch = WoodDeckSF + openPorchSF + EnclosedPorch + 3SsnPorch + ScreenPorch\n\n### Columns we will probably drop\n- LandSlope or LandContour, because they contain the same information\n- The columns with a lot of missing data, unless they provide very valuable data and we can consider that missing data is just an abscence of pool or fireplace.","f7623e70":"We almost didn't change the distribution of bedrooms in the houses but now we have a more logicalrelation between number of bedrooms and SalePrice.","29ddfd51":"#### Let's investigate a bit more the feature that don't appear often in the house like pool, fireplace and Misc and their value","075fccf7":"#### Things we can learn from the previous analysis\n- The price of houses that were remodeled very recently (2010) is also a bit higher than the rest.\n- There seem to be a small increase of the price the more recent the remodelation was done\n","6df32f77":"## 2.1 Dataframe information","aa7f1e39":"### Let's check if the conditions of the sales and the time of the sale have an impact on the price","926f9e49":"### The four different models all have very similar scores, let's use a combination of all these models ","0a7781fe":"#### Let's now check if the calculus of square feet is proprerly made.","5068385a":"# 7. Prediction","f24bed8f":"#### What can we learn from the previous analysis\n- The number of rooms is correlated to the price, which was to be expected\n- The number of bedroom presents something interesting.\n    - A house with 0 rooms above ground has a higher average price than houses with 1, 2 or 3 bedrooms above ground\n    - We need to dig a bit more into the houses with 0 bedroom.\n        - 1st idea, the number of rooms was badly indicated and we need to estimate it based on other features\n        - 2nd idea, the rooms are actually in the basement.","cd83b1a1":"#### Let's check the total number of bath","2a175738":"The year and month of sale does not seem to have an impact on the price. <br>\nThe sale type seems to have an impact but outside of new houses I don't think it is usefull to consider other types of sale because of the population size which is very small.","46de0db0":"#### Things we can learn from the previous analysis\n- The Neighborhood seems to have a big impact on the price\n- Inside a neighborhood, the price can change depending on the zone for example in Crawfor.","5692224f":"#### Let's start with a study of the quality and condition of the house","41b65a3e":"A lot of numerical and categorical data","7c2fc122":"## 4.4 Preprocessing pipeline\n\nSteps to include in the preprocessing pipeline:\n- Feature engineering\n    - Calculate TotalSF\n    - Create a square feet per room and square feet per bedroom feature\n    - Create a BuiltDecade and RemodallDecade\n    - Recalculate the number of bedrooms\n    - MSZoning, put the three smallest categories together in other\n    - SaleType, put the smallest categories together in other\n    - Total_bath = FullBath + HalfBath + BsmtFullBath + BsmtHalfBath an then we categorize them with 1, 2, 3, or 4+ number of bath\n- Impute missing values\n    - Either with most frequent value, the median or None\n- Remove outliers\n    - Remove two properties with very high SF and low price\n    - Apply log transform to columns\n- Encode feature\n    - Neighborhood\n    - MSZoning\n    - SaleType\n- Binning of features\n    - OverallCond into two categories: good condition (>=5) and  average condition (<5)\n    - GarageCars 0, 1, 2, 3+\n    - Fireplaces 0, 1, 2+\n- Scale features (standard scaler)\n- Remove feature (the remaining)","81770a98":"## 2.2 Data description","aed1289c":"This notebook is organized in 7 parts.\n\n1. Data acquisition: we will import the dataset and get a first glance at what it contains.\n\n2. Dataset exploratory analysis : we will analyze the data, get an understanding of the main features, of their types, if they have missing values...\n\n3. Features exploratory analysis : we will analyze the data,find the corelation between the features and the survival rate and decide which features to keep\n\n4. Data cleaning and feature selection : Select features that will be kept in the model and remove others, deal with missing values if there is any, create dummy variables for categories...\n\n5. Model preparation : prepare the train and test set and prepare the models we will use for the classification\n\n6. Pipeline evaluation and selection: We will run the models and get their scores, which will allow us the choose the best model.\n\n7. Predict : Final stage, we will run our final model to execute predictions."}}