{"cell_type":{"25827f74":"code","5d479e7d":"code","7105123c":"code","2a76d0a6":"code","b6942088":"code","b2018174":"code","2d86739f":"code","3162b884":"code","a556f154":"code","f8323a64":"code","a68d5c81":"code","2758f3f6":"code","ed2f7e61":"code","b8a444d1":"code","80921432":"code","45bd74e1":"code","755b07cc":"code","78c0ffb0":"code","e20b1188":"code","3c91d80d":"code","68180557":"code","845a2c66":"code","8beecb25":"code","04d26de8":"code","432cc93d":"code","8ae82d8f":"code","8e91325a":"code","753f7ab7":"code","2627e1cb":"code","4f3b724d":"code","4c77b64a":"code","5e1aacba":"code","7fb55fc8":"code","e9ced8c0":"code","3f445dbd":"code","e436b459":"code","2a69f7a6":"code","f7e18bf6":"code","91a9cfb0":"code","1b28dd9c":"code","be1371c5":"code","4c80559a":"code","04d33220":"code","28a4e41f":"markdown","a94fd7a4":"markdown","01017c8e":"markdown","7388e635":"markdown","404765b4":"markdown","5de45c67":"markdown","54d41af7":"markdown","3c55965f":"markdown","d8be2ce3":"markdown","28168109":"markdown","8acedf97":"markdown","c65d8a2b":"markdown","b3df3276":"markdown","2c109880":"markdown","ab31f287":"markdown","b349fb94":"markdown","c1e5bdcf":"markdown","a856002f":"markdown"},"source":{"25827f74":"%config Completer.use_jedi = False","5d479e7d":"from sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nimport os\nimport re\nimport torch\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\nfrom transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom transformers import AutoModelForSequenceClassification\nfrom datasets import Dataset, load_metric, DatasetDict\nimport nltk\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport pyarrow.feather as feather\nfrom transformers import default_data_collator","7105123c":"ISLOCAL = False #if True, then it is for local development else Kaggle\n\nPREDICTOR = \"discourse_type\"\n\nsquad_v2 = True\n\n####################################################################################\n# DATA Preprocessing Switches\n####################################################################################\nUSE_LOCAL_DATASET = True # Will try to load raw_datasets from local directory if True. Else will create the dataset from scratch.\n\nUSE_SMALL_DATASET = False #If you want to train on whole dataset\nSMALL_DATASET_SIZE = 50\n\nDF_TRAIN_P1 = \"df_train_01\"\n\nTOP_COLUMNS_TO_KEEP = 13 #Dont try to have a dataset with ALL The types of predictions (like Claim 9 etc..) Just keep top colunns\n\n####################################################################################\n\n\n\n\n\n####################################################################################\n# TOKENIZER SWITCHES\n####################################################################################\nTOKENIZER_NAME = \"distilbert-base-uncased\" #\"bert-base-uncased\" # \"allenai\/longformer-base-4096\" # \"distilbert-base-uncased\"\nTOKENIZED_PATH = \"{}_tokenized\".format(TOKENIZER_NAME)\nUSE_LOCAL_TOKENIZED_DATA = True\n\nMAX_LEN = 384\nDOC_STRIDE = 128\n\n\n####################################################################################\n\n\n####################################################################################\n# Tokenized DataSet Constants\n####################################################################################\nTOKENIZER_PATH = \"{}_er\".format(TOKENIZER_NAME)\n\n\n####################################################################################\n\n\n####################################################################################\n# Model Constants\n####################################################################################\nUSE_LOCAL_MODEL = True\nMODEL_NAME = \"distilbert-base-uncased\" #if training a new model then the type\nMODEL_PATH = \"{}mdl\".format(MODEL_NAME) #if training a new model then its save path\nCHECKPOINT = \"checkpoint-32000\"\n\nMODEL_CHECKPOINT = os.path.join(MODEL_PATH, CHECKPOINT)\nBATCH_SIZE = 8\nEPOCHS = 4\nINFER_ONLY = False\n####################################################################################\n\n\n\n#CHECKPOINT = \"checkpoint-125\"\n\n\nDATASET_PATH = \"raw_dataset\"\n\n\n\nTRAINOUT_PATH = \"{}-finetuned\".format(TOKENIZER_NAME)\n\n#SHOULD_I_TRAIN_MODEL = False #if only inference is needed\nSMALL_DATASET_SIZE = 50\n\nif ISLOCAL == True:\n    train_directory = \".\/train\"\n    test_directory = \".\/test\"\n    main_directory = \".\/\"\n    output_directory = \".\/QnA_Model_Output\/\"\nelse:\n    train_directory = \"..\/input\/feedback-prize-2021\/train\"\n    test_directory = \"..\/input\/feedback-prize-2021\/test\"\n    main_directory = \"..\/input\/feedback-prize-2021\/\"\n    output_directory = \"..\/input\/qnadataset-distilbert\/QnA_Model_Output\"\n    output_directory_model = \"..\/input\/qnadataset-distilbert\/\"","2a76d0a6":"def read_train_file(currid = \"423A1CA112E2\", curr_dir = train_directory):\n    with open(os.path.join(curr_dir, \"{}.txt\".format(currid)), \"r\") as f:\n        filetext = f.read()\n        \n    return filetext","b6942088":"if ISLOCAL:\n    train = pd.read_csv( os.path.join(main_directory, \"corrected_train.csv\") )\nelse:\n    train = pd.read_csv(  \"..\/input\/feedback-prize-corrected-train-csv\/corrected_train.csv\")","b2018174":"train.head()","2d86739f":"train[PREDICTOR].unique()","3162b884":"column_to_keep = train[PREDICTOR].value_counts()[:TOP_COLUMNS_TO_KEEP].index.values\nprint(\"We will only train the QnA data set for the columns : \", column_to_keep)","a556f154":"def return_training_dataset(dft, DIR = train_directory):\n    '''\n        This uses Prediction String to get start and end token numbers.\n    '''\n    DIR = train_directory\n    ret = []\n\n    for i in tqdm(train[\"id\"].unique()):\n        temp = train[ train[\"id\"] == i]\n        row = {\"id\" : i,\n               \"context\" : read_train_file(i, DIR)\n              }\n        for j in temp[PREDICTOR]:\n            p_str_beg = temp[temp[PREDICTOR] == j][\"predictionstring\"].values[0]\n            p_str_beg, p_str_end = p_str_beg.split()[0], p_str_beg.split()[-1]\n            \n            row.update( {\"start_{}\".format(j) : p_str_beg,\n                         \"end_{}\".format(j) : p_str_end\n                        }) #append the start and end tokens of the current discourse type as a column\n            \n        ret.append(row.copy()) #append a single row per file id here\n    df_train = pd.DataFrame(ret)\n    df_train = df_train.rename( columns = { i: i.replace(\" \", \"_\") for i in df_train.columns} )\n    df_train = df_train.fillna(-91) #-91 is arbitrary choice here.\n    \n    return df_train\n\ndef return_training_dataset_v2(dft, DIR = train_directory):\n    '''\n        This uses Character positions to get start and end token numbers.\n    '''\n    print(\"CAUTION : We are NOT USING TOKEN POSITIONS, But character positions in the dataset now.\")\n    DIR = train_directory\n    ret = []\n\n    for i in tqdm(train[\"id\"].unique()):\n        temp = train[ train[\"id\"] == i]\n        row = {\"id\" : i,\n               \"context\" : read_train_file(i, DIR)\n              }\n        for j in temp[temp[PREDICTOR].isin(column_to_keep)].itertuples():\n            p_str_beg = getattr(j , \"discourse_start\")\n            p_str_end = getattr(j , \"discourse_end\")\n            row.update( {\"start_{}\".format( getattr(j, PREDICTOR)) : p_str_beg,\n                         \"end_{}\".format( getattr(j, PREDICTOR) ) : p_str_end\n                        }) #append the start and end tokens of the current discourse type as a column\n\n            \n        ret.append(row.copy()) #append a single row per file id here\n    df_train = pd.DataFrame(ret)\n    df_train = df_train.rename( columns = { i: i.replace(\" \", \"_\") for i in df_train.columns} )\n    df_train = df_train.fillna(-91) #-91 is arbitrary choice here.\n    \n    return df_train\n\nif USE_LOCAL_DATASET == False:\n    df_train = return_training_dataset_v2(train,\n                                       train_directory)\nelse:\n    print(\"Will use local dataset and not generate right now\")","f8323a64":"#df_train.info()\nif USE_LOCAL_DATASET == False:\n    for i in df_train.columns:\n        try:\n            df_train[i] = pd.to_numeric(df_train[i], errors = 'raise')\n        except ValueError as ve:\n            #print(ve, i)\n            continue\nelse:\n    print(\"Will use local dataset and not generate right now\")","a68d5c81":"paths = os.path.join( output_directory, DF_TRAIN_P1)\n\nif USE_LOCAL_DATASET == False:\n    print(\"Trying to save the training dataset to {}\".format(paths))\n    feather.write_feather( df_train,\n                          paths)\nelse:\n    assert os.path.exists( paths ), \"Path does not exist for df_train part 01. {}\".format(paths)\n    df_train = feather.read_feather(paths)","2758f3f6":"print(\"loaded dataset shape is \", df_train.shape)","ed2f7e61":"def prepare_question_answer_dataset(df_t):\n    X_valid = df_t.copy()\n    \n    unique_d = column_to_keep ##train[\"discourse_type_num\"].unique()\n    unique_d = [x.replace(\" \", \"_\") for x in unique_d]\n    #X_valid\n\n    ret = []\n    for i in tqdm(X_valid.itertuples(), total = len(X_valid)):\n        for j in unique_d:\n            start_e = int(getattr( i, \"start_\" + j))\n            end_e = int(getattr(i, \"end_\" + j))\n            context = getattr(i, \"context\")\n            \n            \n\n            if (end_e == -91) or (start_e == -91):\n                answer_start = []\n                answer_text = []\n            else:\n                answer_start = [start_e]\n                answer_text = [context[start_e : end_e + 1]]\n                \n\n            ret.append( { \"id\" : getattr(i, \"id\"),\n                         \"context\" : context,\n                          \"question\" : j,\n                          \"answers\" : { \"text\" :  answer_text,\n                                        \"answer_start\" : answer_start},\n                          \"start_position\" : start_e,\n                          \"end_position\" : end_e,\n                          \n                        })\n    return pd.DataFrame(ret)\n\nif USE_LOCAL_DATASET == False:\n    df_train = prepare_question_answer_dataset(df_train)\nelse:\n    print(\"Will use local dataset and not generate right now\")","b8a444d1":"if USE_LOCAL_DATASET == False:\n    X_train, X_test = train_test_split( df_train,\n                                       train_size = 0.7,\n                                      random_state = 91)\n\n    X_test, X_valid = train_test_split( X_test,\n                                       train_size = 0.5,\n                                      random_state = 91)\n    del(df_train)\nelse:\n    print(\"Will use local dataset and not generate right now\")","80921432":"if USE_LOCAL_DATASET == False:\n    train_set = Dataset.from_pandas(X_train)\n    test_set = Dataset.from_pandas(X_test)\n    valid_set = Dataset.from_pandas(X_valid)\nelse:\n    print(\"Will use local dataset and not generate right now\")","45bd74e1":"if USE_LOCAL_DATASET == False:\n    raw_datasets = DatasetDict( {\n        'train' : train_set,\n        'test': test_set,\n        'validation': valid_set\n    })\nelse:\n    print(\"Will use local dataset and not generate right now\")","755b07cc":"paths = os.path.join(output_directory, DATASET_PATH)\n\nimport datasets\n\nif USE_LOCAL_DATASET == False:\n    print(\"Saving dataset to disk. \", paths)\n    \n    raw_datasets.save_to_disk( paths )\n    del(X_train)\n    del(X_test)\n    del(X_valid)\n    del(train_set)\n    del(test_set)\n    del(valid_set)\n    \nelse:\n    assert os.path.exists(paths), \"The dataset local path does not exist. Please retrain the notebook with appropriate switches.\"\n    print(\"Loading the dataset from local directory. \", paths)\n    raw_datasets = datasets.load_from_disk( paths )","78c0ffb0":"print(\"Shape of the raw datasets is : \", raw_datasets.shape)","e20b1188":"from transformers import AutoTokenizer","3c91d80d":"paths = os.path.join( output_directory, TOKENIZER_PATH)\nif USE_LOCAL_TOKENIZED_DATA == False:\n    print(\"Saving Tokenizer to disk\", paths)\n    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n    tokenizer.save_pretrained( paths )\nelse:\n    assert os.path.exists(paths), \"Tokenizer path does not exist {}\".format(paths)\n    \n    print(\"Loading the tokenizer now\")\n    tokenizer = AutoTokenizer.from_pretrained( paths )","68180557":"import transformers\nassert isinstance(tokenizer, transformers.PreTrainedTokenizerFast), \"Tokenizer is not an instance of Fast tokenizers written in Rust. Please check https:\/\/huggingface.co\/transformers\/index.html#bigtable\"","845a2c66":"pad_on_right = tokenizer.padding_side == \"right\"","8beecb25":"def prepare_train_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length = MAX_LEN,\n        stride = DOC_STRIDE,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start\/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the last word (edge case).\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples","04d26de8":"if USE_LOCAL_TOKENIZED_DATA == False:\n    tokenized_datasets = raw_datasets.map(prepare_train_features, \n                                          batched = True, \n                                          remove_columns = raw_datasets[\"train\"].column_names)\nelse:\n    print(\"Not tokenizing the dataset as load from local disk is set.\")","432cc93d":"paths = os.path.join( output_directory, TOKENIZED_PATH)\n\nif USE_LOCAL_TOKENIZED_DATA == False:\n    print(\"Saving the tokenized dataset to local disk for re-usage\")\n    tokenized_datasets.save_to_disk(paths)\nelse:\n    assert os.path.exists(paths), \"Switch set to local tokenized data loader but path does not exist.\"\n    tokenized_datasets = datasets.load_from_disk(paths)","8ae82d8f":"print(\"Tokenized dataset shape is :\", tokenized_datasets.shape)","8e91325a":"from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n\n","753f7ab7":"paths = os.path.join( output_directory_model , MODEL_CHECKPOINT)\n\n\nif USE_LOCAL_MODEL == False:\n    print(\"Trying to load model\", MODEL_NAME)\n    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\nelse:\n    assert os.path.exists(paths), \"Path to the checkpointed model does not exist {}\".format(paths)\n    model = AutoModelForQuestionAnswering.from_pretrained(paths)","2627e1cb":"\n\n\nif USE_SMALL_DATASET:\n    print(\"CAUTION: Using a small subset of data as per the switches. \")\n    small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed = 91).select(range(SMALL_DATASET_SIZE))\n    small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed = 91).select(range(SMALL_DATASET_SIZE))\n#    small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed = 91).select(range(SMALL_DATASET_SIZE))\nelse:\n    small_train_dataset = tokenized_datasets[\"train\"]\n    small_eval_dataset = tokenized_datasets[\"validation\"]\n#    small_test_dataset = tokenized_datasets[\"test\"]","4f3b724d":"args = TrainingArguments(\n    output_dir = MODEL_PATH,\n    logging_steps = min(5000, len(small_train_dataset)),\n    save_strategy = \"steps\",\n    evaluation_strategy = \"steps\",\n    learning_rate=2e-5,\n    per_device_train_batch_size = BATCH_SIZE,\n    per_device_eval_batch_size = BATCH_SIZE,\n    num_train_epochs = EPOCHS,\n    weight_decay = 0.01,\n    save_total_limit = 3,\n    fp16 = True,\n    fp16_full_eval = True,\n    \n    #load_best_model_at_end = True,\n    report_to = \"none\",\n)\n","4c77b64a":"data_collator = default_data_collator","5e1aacba":"trainer = Trainer(\n    model,\n    args,\n    train_dataset=small_train_dataset,   #tokenized_datasets[\"train\"],\n    eval_dataset=small_eval_dataset,     #tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","7fb55fc8":"if USE_LOCAL_MODEL == False:\n    trainer.train()\nelse:\n    print(\"No training today for you\")","e9ced8c0":"if USE_LOCAL_MODEL == False:\n    trainer.save_model( MODEL_PATH )\nelse:\n    print(\"Cannot train without my training shorts.\")","3f445dbd":"def prepare_validation_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=MAX_LEN,\n        stride=DOC_STRIDE,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n        # position is part of the context or not.\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","e436b459":"import collections\n\ndef calc_word_indices(full_text, discourse_start, discourse_end):\n    start_index = len(full_text[:discourse_start].split())\n    token_len = len(full_text[discourse_start:discourse_end].split())\n    output = list(range(start_index, start_index + token_len))\n    if output[-1] >= len(full_text.split()):\n        output = list(range(start_index, start_index + token_len-1))\n    return output\n\ndef get_raw_predictions( submission_dataset, \n                        model = trainer,\n                           column_to_be_removed = raw_datasets[\"validation\"].column_names ):\n    print(\"Processing a dataset of length \", len(submission_dataset) )\n\n    validation_features = submission_dataset.map(\n        prepare_validation_features,\n        batched = True,\n        remove_columns = column_to_be_removed\n    )\n    \n    validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))\n    \n    print(\"Now appending features per example.\")\n    \n    import collections\n    examples = submission_dataset\n    features = validation_features\n\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n    \n    \n    \n    return postprocess_qa_predictions(examples,\n                                      features,\n                                      model.predict(validation_features).predictions,\n                                      n_best_size = 20,\n                                      max_answer_length = 110\n                                     )\n\n\nfrom tqdm.auto import tqdm\n\ndef postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 110):\n    all_start_logits, all_end_logits = raw_predictions\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    # The dictionaries we have to fill.\n    #predictions = collections.OrderedDict()\n    predictions = []\n\n    # Logging.\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None # Only used if squad_v2 is True.\n        valid_answers = []\n        \n        context = example[\"context\"]\n        question = example[\"question\"]\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n\n            # Update minimum null prediction.\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char],\n                            \"discourse_start\" : start_char, \n                            \"discourse_end\" : end_char,\n                            \"question\" : question\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n            # failure.\n            best_answer = {\"score\": 0.0,\n                           \"text\": \"\", \n                           \"discourse_start\" : -91,\n                           \"discourse_end\" : -91,\n                           \"question\" : question}\n        \n        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n        if not squad_v2:\n            predictions.append( {\"id\" : example[\"id\"],\n                                 \"text\" : best_answer[\"text\"],\n                                 \"discourse_start\" : best_answer[\"discourse_start\"],\n                                 \"discourse_end\" : best_answer[\"discourse_end\"],\n                                 \"question\" : best_answer[\"question\"]\n                                }\n                              )\n            # predictions[example[\"id\"]] = best_answer[\"text\"]\n        else:\n            if min_null_score is None:# WARNING THIS IS ACTUALLY WRONG AND INDICATES A BUG SOMEWHERE - AAM 2021-12-29\n                min_null_score = 0 # WARNING THIS IS ACTUALLY WRONG AND INDICATES A BUG SOMEWHERE - AAM 2021-12-29\n            \n            #answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n            \n            any_answer = 0 #default answer is nothing..\n            prev_start, prev_end = 1000, -1000\n            \n            valid_answers = sorted(valid_answers, key = lambda x: x[\"discourse_start\"],reverse= False)\n            # we sort valid answers by position so that we can filter below.\n            for all_ans in valid_answers:\n                    if all_ans[\"score\"] >= min_null_score:\n                        curr_start, curr_end = all_ans[\"discourse_start\"], all_ans[\"discourse_end\"]\n                        if (curr_start > prev_end) or (curr_end < prev_start):\n                            #This sequence has no overlap with the previous. So we may keep it. (TODO make it foolproof please)\n                            any_answer += 1\n                            predictions.append( {\"id\" : example[\"id\"],\n                                                 \"text\" : all_ans[\"text\"],\n                                                 \"discourse_start\" : curr_start,\n                                                 \"discourse_end\" : curr_end,\n                                                 \"question\" : question\n                                                }\n                                              )\n                            prev_start = curr_start\n                            prev_end = curr_end\n                        \n            if any_answer == 0: #there was no answer better than the scoring threshold.\n                predictions.append( {\"id\" : example[\"id\"],\n                                     \"text\" : \"\",\n                                     \"discourse_start\" : -91,\n                                     \"discourse_end\" : -91,\n                                     \"question\" : question\n                                    }\n                                  )\n#             if answer == \"\":\n#                 predictions.append( {\"id\" : example[\"id\"],\n#                                      \"text\" : \"\",\n#                                      \"discourse_start\" : -91,\n#                                      \"discourse_end\" : -91,\n#                                      \"question\" : question\n#                                     }\n#                                   )\n#             else:\n                \n#                 predictions.append( {\"id\" : example[\"id\"],\n#                                      \"text\" : answer,\n#                                      \"discourse_start\" : best_answer[\"discourse_start\"],\n#                                      \"discourse_end\" : best_answer[\"discourse_end\"],\n#                                      \"question\" : question\n#                                     }\n#                                   )\n                \n            #predictions[example[\"id\"]] = answer\n\n    return predictions","2a69f7a6":"# aam = raw_datasets[\"test\"].shuffle(seed = 91).select(range(SMALL_DATASET_SIZE))\n# final_pred = get_raw_predictions( submission_dataset = aam, \n#                         model = trainer,\n#                            column_to_be_removed = raw_datasets[\"validation\"].column_names )\n# final_pred","f7e18bf6":"df_ss = pd.read_csv( os.path.join( main_directory, \"sample_submission.csv\" ) )","91a9cfb0":"# 'id', 'context', 'question', 'answers', 'start_position', 'end_position', '__index_level_0__'\ndef convert_subm_to_dataset(ss_df, experimental = False):\n    df_ss = ss_df.copy()\n    ret = []\n    counter = 0\n    for i in tqdm(df_ss[\"id\"].unique()):\n        for j in column_to_keep:\n            counter += 1\n            if experimental:\n                ret.append( {\"id\" : \"{}_{}\".format(i, counter),\n                         \"question\" : j.replace(\" \", \"_\"),\n                         \"context\" : read_train_file( i, test_directory ),\n                         \"answers\" : [],\n                         \"start_position\" : [],\n                         \"end_position\" : [],\n                         \"__index_level_0__\" : []\n                        } )\n            else:\n                ret.append( {\"id\" : i,\n                             \"question\" : j.replace(\" \", \"_\"),\n                             \"context\" : read_train_file( i, test_directory ),\n                             \"answers\" : [],\n                             \"start_position\" : [],\n                             \"end_position\" : [],\n                             \"__index_level_0__\" : []\n                            } )\n    \n    subm_pandas = pd.DataFrame( ret )\n    subm = Dataset.from_pandas( pd.DataFrame(ret) )\n    return subm\n\ndef preds_to_pandas(final_pred,\n                   experimental = False):\n    ret = []\n    for i in final_pred:\n        if i[\"text\"] != \"\":\n            if experimental == False:\n                fileid = i[\"id\"]\n            else:\n                fileid = i[\"id\"].split(\"_\")[0]\n                \n            txt = read_train_file( fileid , test_directory )\n            predictionstring = calc_word_indices(txt, i[\"discourse_start\"], i[\"discourse_end\"])\n            #print(predictionstring[:-1]) # THERE IS AN ERROR IN TRAINING MODEL. Last token is included by mistake.\n            ret.append({ \"id\" : fileid,\n                        \"class\" : i[\"question\"].replace(\"_\", \" \"),\n                        #\"class\" : i[\"question\"].split(\"_\")[0],\n                        \"predictionstring\" : \" \".join([str(x) for x in predictionstring[:-1]])\n                       }\n                      )\n    return pd.DataFrame(ret)\n\n# def get_prediction_output(ss_df):\n#     df_ss = ss_df.copy()\n#     ret = []\n#     for i in tqdm(df_ss[\"id\"].unique()):\n#         for j in column_to_keep:\n#             ret.append( {\"id\" : i,\n#                          \"question\" : j.replace(\" \", \"_\"),\n#                          \"context\" : read_train_file( i, test_directory )\n#                         } )\n    \n#     subm_pandas = pd.DataFrame( ret )\n#     subm = Dataset.from_pandas( pd.DataFrame(ret) )\n    \n#     cols_to_remove = list(set(raw_datasets[\"validation\"].column_names).intersection( set(subm_pandas.columns )))\n    \n#     final_pred = get_raw_predictions( submission_dataset = subm, \n#                         model = trainer,\n#                            column_to_be_removed = cols_to_remove )\n    \n#     ret = []\n#     for i in final_pred:\n#         if i[\"text\"] != \"\":\n#             txt = read_train_file( i[\"id\"] , test_directory )\n#             predictionstring = calc_word_indices(txt, i[\"discourse_start\"], i[\"discourse_end\"])\n#             #print(predictionstring[:-1]) # THERE IS AN ERROR IN TRAINING MODEL. Last token is included by mistake.\n#             ret.append({ \"id\" : i[\"id\"],\n#                         \"class\" : i[\"question\"].split(\"_\")[0],\n#                         \"predictionstring\" : \" \".join([str(x) for x in predictionstring[:-1]])\n#                        }\n#                       )\n#     return pd.DataFrame(ret)","1b28dd9c":"def get_line_by_line_predictions(df_ss,\n                                experimental = False):\n    aam2 = convert_subm_to_dataset(df_ss,\n                                  experimental = experimental)\n    if experimental == False:\n        preds = []\n        for i in tqdm(range(len(aam2))):\n            p = get_raw_predictions( aam2.select([i]),\n                                model = trainer,\n                                column_to_be_removed = raw_datasets[\"validation\"].column_names\n                               )\n            preds.extend(p)\n    else:\n        preds = get_raw_predictions( aam2,\n                                    model = trainer,\n                                    column_to_be_removed = raw_datasets[\"validation\"].column_names\n                                   )\n\n    final_merged = preds_to_pandas(preds,\n                                  experimental = experimental)\n    return final_merged\n\n#final_merged = get_line_by_line_predictions(df_ss)\n","be1371c5":"final_merged = get_line_by_line_predictions(df_ss, \n                                           experimental = True)","4c80559a":"final_merged = final_merged.sort_values( by = [\"id\", \"predictionstring\"] )\nfinal_merged.to_csv(\"submission.csv\", index = False)","04d33220":"final_merged.head(20)","28a4e41f":"# Step 1\n\n* Convert the dataset into a pandas table with text from the file and the positions of each discourse types (start and end word position)","a94fd7a4":"# Save Tokenized Dataset","01017c8e":"# Prepare the Model (Super-Models beware....)","7388e635":"# Step 3\n\n* Prepare a question and answer data set where the question is the discourse type num parameter","404765b4":"# Train Now","5de45c67":"# Constants Below","54d41af7":"# Apply tokenizer","3c55965f":"If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n\nTo be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n\nFirst you have to store your authentication token from the Hugging Face website (sign up [here](https:\/\/huggingface.co\/join) if you haven't already!) then execute the following cell and input your username and password:","d8be2ce3":"# Predict Test Set\n\nTry to apply the prediction function on the submission set now.","28168109":"# CAUTION\n\nThis is the first working version of **Question Answer** based **Distilbert** model with **Strides**. Trained on Squad v2 like structure (Where No Answer to a question is also possible)\n\n* Only trained for **1 EPOCH** so not optimal model. Should be trained more if someone has a decent GPU.\n* **Does NOT** require a **Longformer** or **Bigbird** as Strides are implemented in the code.\n\n---\n\nNo documentation yet. I will try to update the documentation and improve code. But I am running low on GPU and I am unable to train BERT on this code which would improve the overall performance. (Strides take care of long input sequences)\n\n---\n\n# Next Steps\n\n* Train BERT on the same code. \n* Rule based filter on the final output. (Leads & Concluding statements should have a specific structure. Rebuttals and Counter Claim have a specific relationship...)\n\n---\n\n# README FIRST\n\n* This code converts the problem to a Question\/ANswer model and uses strides in order to be able to use smaller models (models which have input sequence limitations of 512 or lower).\n\n* Due to computing power limitations, I was not able to train this model BERT\/Larger Models. There are atleast 2 things which should be done (if you have the right processing machine)\n    * Train for atleast 4 EPOCHS\n        * I only trained for 1 EPOCH and loss was decreasing till end. (Final loss was around 0.9 on distilbert)\n    * Train using a different model than distilbert\n\n* You may be able to improve the performance by changing the base model to BERT as I used distilbert due to memory footprint issue on my machine.\n\n---\n\n# Reference\nThe only resource your will need to understand the hocus pocus below\n\nhttps:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb\n\n---\n\n# Changelog\n\n## Version 3\n* Concluding Statement category was not reported in the model. Only fixed this category processing. Everything else is unchanged","8acedf97":"# Evaluation","c65d8a2b":"# Tokenizer Steps","b3df3276":"# Trim Dataset (If required)\n","2c109880":"# Training Argument Setup","ab31f287":"# Step 2\n\n* Remove the spaces in the resulting dataframe columns and fill na with -91","b349fb94":"# Initialize Trainer\n\n* Put those shorts on","c1e5bdcf":"# Load DataColator\n\n* After all no size is perfect","a856002f":"# Step 3\n\n* Train\/Test\/Valid split"}}