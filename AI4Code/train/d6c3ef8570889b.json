{"cell_type":{"37282cf9":"code","794fd87e":"code","fa6305c4":"code","81a25bef":"code","0f41e974":"code","8662455d":"code","c5fbd270":"code","4be84e62":"code","2876fa82":"code","1a234602":"code","93410523":"code","d90571b0":"code","2e1c5143":"code","213bad79":"code","0224ea1b":"code","7d561f22":"code","d28c20ec":"code","4c2e9eb8":"code","581e147c":"code","b3855ed6":"code","223743fb":"code","d8901e55":"code","dfde8d42":"code","f11edc7e":"markdown","469fdffd":"markdown"},"source":{"37282cf9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","794fd87e":"#get tools\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import discriminant_analysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import average_precision_score, precision_recall_curve, confusion_matrix, roc_curve, roc_auc_score, classification_report, auc \n","fa6305c4":"df=pd.read_csv('..\/input\/creditcard.csv')\nprint(df.head(5))","81a25bef":"#Check for missing data\nprint(df.isnull().values.any())","0f41e974":"#Great! Don't have any missing data.\n#Get a summary\ndf.describe()","8662455d":"#Check the distribution of Fraud vs Legitimate transactions\nFraudShare=round(df['Class'].value_counts()[1]\/len(df)*100,2)\nprint(FraudShare,'% of transactions are fraudulent')","c5fbd270":"#Highly uneven distribution between classes\n#Take a look at what fraudulent cases are like\ndf_temp=df.loc[df['Class']==1]\ndf_temp.describe()","4be84e62":"#Only 492 fraudulent transactions\n#Average fraudulent amount swiped $122\n#Half of fraudulent transactions are for amounts less than $10\n\n#Ideally, split data into train and test sets first\n#Then run PCA and standardization on train set to avoid information leak\n#But host data already has PCA run on V variables over entire data set\n#So standardize Amount and Time to \"jive\"\n\nsc=StandardScaler()\ndf['Amount_scaled']=sc.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['Time_scaled']=sc.fit_transform(df['Time'].values.reshape(-1,1))\ndf.drop(['Time','Amount'],axis=1,inplace=True)\nprint(df.head(3))","2876fa82":"#Given the highly uneven distribution between classes, need\n#(1) a training set with an even distribution of both classes; and\n#(2) an unseen test set with a majority to minority class distribution similar to original dataset\n\n#Random undersample the majority class for 1:1 distribution\n#First shuffle the data (argument 'frac' specifies fraction of rows to return in random sample)\ndf.sample(frac=1)\n\n#From description of Fraud set, we know we have 492 Fraud cases\n#Get 492 Non-fraud cases from randomly shuffled set\ndf_Norm=df.loc[df['Class']==0][:492]\ndf_Fraud=df.loc[df['Class']==1]\n\n#Join dfFraud and dfNorm to get a normally distributed df\ndf_norm_dist=pd.concat([df_Norm,df_Fraud])\n\n#Shuffle dataframe rows\ndf_New=df_norm_dist.sample(frac=1,random_state=22)\n\n#Get required non-fraud cases for test set\ndf_NormUp=df.loc[df['Class']==0][493:58000]","1a234602":"#We don't know what the 'V' variables are and how they affect 'Class'\n#How are variables correlated?\n#Can we remove non-supporting ones for more parsimonious model?\n\nsub_sample_corr=df_New.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(sub_sample_corr,cmap='coolwarm_r')","93410523":"#Drop uncorrelated, non-supporting variables from the training features\ndf_New.drop(['V8','V13','V23','V26','V27','V28','Amount_scaled'],axis=1,inplace=True)\nprint(df_New.head(3))","d90571b0":"#Identifying fraud vs. non-fraud cases is a classification problem\n#Need to check for extreme outliers in supporting variables\n#Plot box and whisker charts to see\n\n#Variables with positive correlation with class\n#V2,V4,V11,V19,Time_scaled\nf,axes=plt.subplots(ncols=5,figsize=(20,5))\n\nsns.boxplot(x='Class',y='V2',data=df_New,ax=axes[0])\naxes[0].set_title('V2 vs Class: Positive Correlation')\n\nsns.boxplot(x='Class',y='V4',data=df_New,ax=axes[1])\naxes[1].set_title('V4 vs Class: Positive Correlation')\n            \nsns.boxplot(x='Class',y='V11',data=df_New,ax=axes[2])\naxes[2].set_title('V11 vs Class: Positive Correlation')\n\nsns.boxplot(x='Class',y='V19',data=df_New,ax=axes[3])\naxes[3].set_title('V19 vs Class: Positive Correlation')\n\nsns.boxplot(x='Class',y='Time_scaled',data=df_New,ax=axes[4])\naxes[4].set_title('Time_scaled vs Class: Positive Correlation')\n\nplt.show()\n\n#Variables with negative correlation with class\n#V1,V3,V5,V6,V7,V9,V10,V12,V14,V15,V16,V17,V18\nf,axes=plt.subplots(ncols=5,figsize=(20,5))\n\nsns.boxplot(x='Class',y='V1',data=df_New,ax=axes[0])\naxes[0].set_title('V1 vs Class: Negative Correlation')\n\nsns.boxplot(x='Class',y='V3',data=df_New,ax=axes[1])\naxes[1].set_title('V3 vs Class: Negative Correlation')\n\nsns.boxplot(x='Class',y='V5',data=df_New,ax=axes[2])\naxes[2].set_title('V5 vs Class: Negative Correlation')\n\nsns.boxplot(x='Class',y='V6',data=df_New,ax=axes[3])\naxes[3].set_title('V6 vs Class: Negative Correlation')\n\nsns.boxplot(x='Class',y='V7',data=df_New,ax=axes[4])\naxes[4].set_title('V7 vs Class: Negative Correlation')\n\nplt.show()\n\nf,axes=plt.subplots(ncols=5,figsize=(20,5))\nsns.boxplot(x='Class',y='V9',data=df_New,ax=axes[0])\naxes[0].set_title('V9 vs Class: Negative Correlation')\n\nsns.boxplot(x='Class',y='V10',data=df_New,ax=axes[1])\naxes[1].set_title('V10 vs Class: Negative Correlation')\n\nsns.boxplot(x='Class',y='V12',data=df_New,ax=axes[2])\naxes[2].set_title('V12 vs Class: Negative Correlation')\n\nsns.boxplot(x='Class',y='V14',data=df_New,ax=axes[3])\naxes[3].set_title('V14 vs Class: Negative Correlation')\n\nsns.boxplot(x='Class',y='V15',data=df_New,ax=axes[4])\naxes[4].set_title('V15 vs Class: Negative Correlation')\n\nplt.show()\n\nf,axes=plt.subplots(ncols=5,figsize=(20,5))\n\nsns.boxplot(x='Class',y='V16',data=df_New,ax=axes[0])\naxes[0].set_title('V16 vs Class: Negative Correlation')\n\nsns.boxplot(x='Class',y='V17',data=df_New,ax=axes[1])\naxes[1].set_title('V17 vs Class: Negative Correlation')\n\nsns.boxplot(x='Class',y='V18',data=df_New,ax=axes[2])\naxes[2].set_title('V18 vs Class: Negative Correlation')\n\nplt.show()","2e1c5143":"#Get rid of variables with collinearity\n#Keep an edited set of predictors with fewer outliers\ndf_New.drop(['V2','V3','V5','V7','V9','V11','V15','V19','V20','V21','V22','V24','V25','Time_scaled'],axis=1,inplace=True)\ndf_New.head()","213bad79":"#Trim off extreme outliers in predictor variables\n#Start with a larger value since don't have a lot of data to work with\n#Using a smaller value, e.g. 2.5, might lose too much information\n\n#V1\nV1_fraud=df_New['V1'].loc[df_New['Class']==1].values\nq_25,q_75=np.percentile(V1_fraud,25),np.percentile(V1_fraud,75)\n#Get interquartile range\nV1_fraud_iqr=q_75-q_25\n#Set cut-off at 2 times interquartile range\nV1_fraud_cutoff=V1_fraud_iqr*4.25\nV1_fraud_low,V1_fraud_high=q_25-V1_fraud_cutoff,q_75+V1_fraud_cutoff\ndf_New=df_New.drop(df_New[(df_New['V1'] > V1_fraud_high) | (df_New['V1'] < V1_fraud_low)].index)\n\n#V6\nV6_fraud=df_New['V6'].loc[df_New['Class']==1].values\nq_25,q_75=np.percentile(V6_fraud,25),np.percentile(V6_fraud,75)\nV6_fraud_iqr=q_75-q_25\nV6_fraud_cutoff=V6_fraud_iqr*4.25\nV6_fraud_low,V6_fraud_high=q_25-V6_fraud_cutoff,q_75+V6_fraud_cutoff\ndf_New=df_New.drop(df_New[(df_New['V6'] > V6_fraud_high) | (df_New['V6'] < V6_fraud_low)].index)\n\n\n#V10\nV10_fraud=df_New['V10'].loc[df_New['Class']==1].values\nq_25,q_75=np.percentile(V10_fraud,25),np.percentile(V10_fraud,75)\nV10_fraud_iqr=q_75-q_25\nV10_fraud_cutoff=V10_fraud_iqr*4.25\nV10_fraud_low,V10_fraud_high=q_25-V10_fraud_cutoff,q_75+V10_fraud_cutoff\ndf_New=df_New.drop(df_New[(df_New['V10'] > V10_fraud_high) | (df_New['V10'] < V10_fraud_low)].index)\n\n#V12\nV12_fraud=df_New['V12'].loc[df_New['Class']==1].values\nq_25,q_75=np.percentile(V12_fraud,25),np.percentile(V12_fraud,75)\nV12_fraud_iqr=q_75-q_25\nV12_fraud_cutoff=V12_fraud_iqr*4.25\nV12_fraud_low,V12_fraud_high=q_25-V12_fraud_cutoff,q_75+V12_fraud_cutoff\ndf_New=df_New.drop(df_New[(df_New['V12'] > V12_fraud_high) | (df_New['V12'] < V12_fraud_low)].index)\n\n#V14\nV14_fraud=df_New['V14'].loc[df_New['Class']==1].values\nq_25,q_75=np.percentile(V14_fraud,25),np.percentile(V14_fraud,75)\nV14_fraud_iqr=q_75-q_25\nV14_fraud_cutoff=V14_fraud_iqr*4.25\nV14_fraud_low,V14_fraud_high=q_25-V14_fraud_cutoff,q_75+V14_fraud_cutoff\ndf_New=df_New.drop(df_New[(df_New['V14'] > V14_fraud_high) | (df_New['V14'] < V14_fraud_low)].index)\n","0224ea1b":"#Split into training and test sets first.\nX=df_New.drop(labels='Class',axis=1) #features\nY=df_New.loc[:,'Class'] #response\n\nX_train, X_test, Y_train, Y_test=train_test_split(X, Y, test_size=0.2, random_state=22)\n","7d561f22":"#Remember to prep a test set with uneven distribution of classes\n#Minority class represented only 0.17% of original\n#Therefore, we need to upsample the majority class in the test set\ndf_NormUp.drop(['V8','V13','V23','V26','V27','V28','Amount_scaled'],axis=1,inplace=True)\ndf_NormUp.drop(['V2','V3','V5','V7','V9','V11','V15','V19','V20','V21','V22','V24','V25','Time_scaled'],axis=1,inplace=True)\nx=df_NormUp.drop(labels='Class',axis=1)\ny=df_NormUp.loc[:,'Class']\n\ntestx=pd.concat([X_test,x])\ntesty=pd.concat([Y_test,y])\n","d28c20ec":"#Let's fit a dummy classifier to use as a no-skill benchmark\n\nfrom sklearn.dummy import DummyClassifier\nDUM_mod=DummyClassifier(random_state=22)\nDUM_mod.fit(X_train,Y_train)\nY_pred_DUM=DUM_mod.predict(testx)\n","4c2e9eb8":"#Fit K Neighbors Classifier, QDA, Logit, and Random Forest Classifier models to data\n#Time estimation process\n\nKNC_mod=KNeighborsClassifier()\nQDA_mod=discriminant_analysis.QuadraticDiscriminantAnalysis()\nLOG_mod=LogisticRegression()\nRFC_mod=RandomForestClassifier(max_depth=4,random_state=22)\n\nt0_est=time.time()\n\nKNC_mod.fit(X_train,Y_train)\nQDA_mod.fit(X_train,Y_train)\nLOG_mod.fit(X_train,Y_train)\nRFC_mod.fit(X_train,Y_train)\n\nt1_est=time.time()\nprint('Estimation of all four models took: {0:.4f} seconds'.format(t1_est-t0_est))","581e147c":"#Predict using test sample\n#Time process to see how long each takes\n\nt0_KNC=time.time()\nY_pred_KNC=KNC_mod.predict(testx)\nt1_KNC=time.time()\nprint(\"Predicting with K Neighbors Classifier model took: {0:.4f} seconds\".format(t1_KNC-t0_KNC))\n\nt0_QDA=time.time()\nY_pred_QDA=QDA_mod.predict(testx)\nt1_QDA=time.time()\nprint(\"Predicting with QDA model took: {0:.4f} seconds\".format(t1_QDA-t0_QDA))\n\nt0_LOG=time.time()\nY_pred_LOG=LOG_mod.predict(testx)\nt1_LOG=time.time()\nprint(\"Predicting with Logistic Classification model took: {0:.4f} seconds\".format(t1_LOG-t0_LOG))\n\nt0_RFC=time.time()\nY_pred_RFC=RFC_mod.predict(testx)\nt1_RFC=time.time()\nprint(\"Predicting with Random Forest Classifier model took: {0:.4f} seconds\".format(t1_RFC-t0_RFC))","b3855ed6":"#Plot precision recall curve\n\n#Calculate precision recall curves\nprecision_DUM,recall_DUM,threshold_DUM=precision_recall_curve(testy,Y_pred_DUM)\nprecision_KNC,recall_KNC,threshold_KNC=precision_recall_curve(testy,Y_pred_KNC)\nprecision_QDA,recall_QDA,threshold_QDA=precision_recall_curve(testy,Y_pred_QDA)\nprecision_LOG,recall_LOG,threshold_LOG=precision_recall_curve(testy,Y_pred_LOG)\nprecision_RFC,recall_RFC,threshold_RFC=precision_recall_curve(testy,Y_pred_RFC)\n\n#Calculate area under precision recall curves\nauprc_DUM=auc(recall_DUM,precision_DUM)\nauprc_KNC=auc(recall_KNC,precision_KNC)\nauprc_QDA=auc(recall_QDA,precision_QDA)\nauprc_LOG=auc(recall_LOG,precision_LOG)\nauprc_RFC=auc(recall_RFC,precision_RFC)\n\nprint (\"Area under precision recall curve, Dummy Classifier model: {0:.4f}\".format(auprc_DUM))\nprint (\"Area under precision recall curve, K Neighbors Classification model: {0:.4f}\".format(auprc_KNC))\nprint (\"Area under precision recall curve, QDA model: {0:.4f}\".format(auprc_QDA))\nprint (\"Area under precision recall curve, Logistic Classification model: {0:.4f}\".format(auprc_LOG))\nprint (\"Area under precision recall curve, Random Forest Classification model: {0:.4f}\".format(auprc_RFC))","223743fb":"#Hmm, all have AUPRC<0.5 but outperform dummy classifier\n\n#Check Precision Recall Score\nAP_DUM=average_precision_score(testy,Y_pred_DUM)\nAP_KNC=average_precision_score(testy,Y_pred_KNC)\nAP_QDA=average_precision_score(testy,Y_pred_QDA)\nAP_LOG=average_precision_score(testy,Y_pred_LOG)\nAP_RFC=average_precision_score(testy,Y_pred_RFC)\n\nprint('Average Precision Score, DUM model: {0:.4f}'.format(AP_DUM))\nprint('Average Precision Score, KNC model: {0:.4f}'.format(AP_KNC))\nprint('Average Precision Score, QDA model: {0:.4f}'.format(AP_QDA))\nprint('Average Precision Score, LOG model: {0:.4f}'.format(AP_LOG))\nprint('Average Precision Score, RFC model: {0:.4f}'.format(AP_RFC))","d8901e55":"#Not great. But show some skill relative to the dummy model\n\n#Make a classification report for each model\nlabels=['No Fraud','Fraud']\nprint('--'*30)\nprint('Dummy Classification Model')\nprint('--'*30)\nprint(classification_report(testy,Y_pred_DUM,target_names=labels))\nprint('--'*30)\nprint('K Neighbors Classification Model')\nprint('--'*30)\nprint(classification_report(testy,Y_pred_KNC,target_names=labels))\nprint('--'*30)\nprint('Quadratic Discriminant Analysis Model')\nprint('--'*30)\nprint(classification_report(testy,Y_pred_QDA,target_names=labels))\nprint('--'*30)\nprint('Logistic Regression Model')\nprint('--'*30)\nprint(classification_report(testy,Y_pred_LOG,target_names=labels))\nprint('--'*30)\nprint('Random Forest Classifier Model')\nprint('--'*30)\nprint(classification_report(testy,Y_pred_RFC,target_names=labels))\nprint('--'*30)","dfde8d42":"print(confusion_matrix(testy,Y_pred_DUM))\nprint(confusion_matrix(testy,Y_pred_KNC))\nprint(confusion_matrix(testy,Y_pred_QDA))\nprint(confusion_matrix(testy,Y_pred_LOG))\nprint(confusion_matrix(testy,Y_pred_RFC))","f11edc7e":"**Identify fraudulent credit card transactions**\n\nThis is a classication problem with a highly uneven class distribution, i.e. fraudulent activity comprised only 0.17% of the dataset.  To enable better prediction, I (i) undersampled the majority class (legitimate transactions) to create a training set with equal representation from both classes; and (ii) created a test set with a class imbalance ratio similar to the problem scenario.  Predictor variables that are uncorrelated with class, as well as those that exhibit collinearity, were eliminated.\n\nCompared to a no-skill benchmark created by dummy classifier, all four models showed considerable skill in identifying fraudulent credit card transactions.  In terms of area under precision recall curve and F1 score, K Neighbors Classifier had the best performance.  But it takes nearly two seconds to run, which can be problematic for customers looking to simply tap and go.  For a slightly reduced performance, the Random Forest Classifier takes less than one second and offers a more transparent, easier to explain decision path.  \n\nNext step: try to reduce to number of false positives that could drive customers away.\n\nSuggestions and advice are welcome!","469fdffd":"In terms of area under precision recall curve and f1 score, K Neighbors Classification model has the best performance among the models tried.  But, the area under precision recall curve for this model is still under 0.5. Also, it takes nearly 2 seconds to predict, which might annoy customers trying to pay and go ASAP.  Random Forest Classification model could be a better choice but there are still quite a large number of false positives that could annoy and drive away customers. "}}