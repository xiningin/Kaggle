{"cell_type":{"9309b9c9":"code","6dfa3ef6":"code","9dfbb171":"code","e47918d9":"code","699385b7":"code","f34a35b9":"code","b05eaf25":"markdown","68f7647a":"markdown","89a07103":"markdown","cc8e20f5":"markdown","01e13a0b":"markdown","d7dab312":"markdown","be3dc065":"markdown","d0471273":"markdown"},"source":{"9309b9c9":"from string import punctuation\nimport os\nimport pickle\n\ndef normalize(txt):\n    # removes punctuation and makes it small caps\n    return ' '.join(word.strip(punctuation) for word in txt.split() if word.strip(punctuation)).lower()\n\nroot = \".\/text\/\"\nall_files = []\nfor (dirname, subs, files) in os.walk(root):\n    for fname in files:\n        all_files.append(os.path.join(dirname, fname))\n\nword_to_id = {}\nwords_count = 0\nunique_words = set()\n\nprint(len(word_to_id))\n\ncount = 0\ndataset = open('dataset.txt', 'w')\n\n# We will use only the first 10 files (out of more than 7K)\nfor file_name in all_files[:10]:\n    sentences = []\n    file = open(file_name, 'rb')\n    if count % 100 == 0:\n        print('processing ', str(count), '\/', str(len(all_files)))\n    count += 1\n    for line in file.readlines():\n        text = line.decode('utf8')\n        if text.startswith('<doc') or text.startswith('<\/doc') or text=='\\n':\n            continue\n        tokens = normalize(text).split(' ')\n        sentences.append(tokens)\n        for t in tokens:\n            if t not in word_to_id.keys():\n                word_to_id[t] = words_count\n                words_count += 1 \n\n    # how far should we pay attention to the surrounding words\n    window_size = 2 \n    \n    for sentence in sentences:\n        for index, target in enumerate(sentence):\n            start_neighbor = max(index - window_size, 0)\n            end_neighbor = min(index + window_size, len(sentence)) + 1\n            \n            for neighbor in sentence[start_neighbor:end_neighbor]:\n                        if neighbor != target:\n                            target_id = word_to_id[target]\n                            neighbor_id = word_to_id[neighbor]\n                            dataset.write(str(target_id)+','+str(neighbor_id)+'\\n')\n\ndataset.close()\nprint('finished processing')\n\npickle.dump(word_to_id, open(\"word_to_id.pickle\", \"wb\"))\ndel word_to_id\n\n# randomising the data set before training\nimport random\nwith open('dataset.txt', 'r') as source:\n    data = [ (random.random(), line) for line in source ]\ndata.sort()\nwith open('dataset_random.txt','w') as target:\n    for _, line in data:\n        target.write( line )\ndel data","6dfa3ef6":"import random\nimport tensorflow as tf\nimport pickle\nimport numpy as np\nimport math\n\n\nword_to_id = pickle.load(open(\"..\/input\/word2vec\/word_to_id.pickle\", \"rb\"))\nvocab_size = len(word_to_id)\n\nlearning_rate = 5e-20\nembedding_size = 50\n\n# including batches was pointless in this case as the vocab_size is way too big (1M+)\nx = tf.placeholder(tf.int32, shape=(1))\ny = tf.placeholder(tf.int32, shape=(1))\n\none_hot = tf.one_hot(x, vocab_size)\n\n# W1 actually holds the embedding vector aka word2vec, which we would use later on\nW1 = tf.Variable(tf.random_normal([vocab_size, embedding_size], mean=0.0, stddev=1.0, dtype=tf.float32))\nb1 = tf.Variable(tf.random_normal([embedding_size], mean=0.0, stddev=1.0, dtype=tf.float32))\n\nhidden_layer = tf.nn.relu(tf.add(tf.matmul(one_hot, W1), b1))\ndropout = tf.nn.dropout(hidden_layer, rate=0.1)\n\nW2 = tf.Variable(tf.random_normal([embedding_size, vocab_size], mean=0.0, stddev=1.0, dtype=tf.float32))\nb2 = tf.Variable(tf.random_normal([vocab_size], mean=0.0, stddev=1.0, dtype=tf.float32))\n\nloss = tf.nn.softmax_cross_entropy_with_logits(\n    logits = tf.add(tf.matmul(dropout, W2), b2),\n    labels = (tf.one_hot(y, vocab_size)))\n\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ngradients = optimizer.compute_gradients(loss)\n\ntrainable_vars = tf.trainable_variables()\ngrads, _ = tf.clip_by_global_norm(tf.gradients(loss, trainable_vars), 1)\n\ntrain_op = optimizer.apply_gradients(zip(grads, trainable_vars))\n\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\ncounter = 0\niteration = 0\nwith open('..\/input\/word2vec\/dataset.txt', 'rb') as f:\n    while True:\n        entry = f.readline()\n        counter += 1\n        # make it at least somewhat random        \n        # taking every 300-th example from around 6 Million examples  will result in around 20,000 iterations        \n        if counter % 300 != 0:\n            continue\n        # There are about 130K entries, we will use roughly 10%.\n        if entry: \n            values = entry.decode('utf8').split(',')\n            inputs = np.asarray(int(values[0])).reshape((1))\n            outputs = np.asarray(int(values[1])).reshape((1))\n            \n            sess.run(train_op, feed_dict={x: inputs, y: outputs})\n        \n            if iteration % 500 == 0:\n                current_loss = sess.run(loss, feed_dict={x: inputs, y: outputs})\n                print('iteration ' \n                      + str(iteration) + ' loss of current step is : ', current_loss)\n                if math.isnan(current_loss):\n                    break\n            iteration += 1\n        else:\n            break\n\nprint('END')","9dfbb171":"word2vec = sess.run(W1)\npickle.dump(word2vec, open(\"word2vec.pickle\", \"wb\"))","e47918d9":"import tensorflow as tf\nimport numpy as np\nfrom string import punctuation\nimport pickle\n\n\ndef normalize(txt):\n    # removes punctuation and makes it small caps\n    return ' '.join(word.strip(punctuation) for word in txt.split() if word.strip(punctuation)).lower()\n\n\nword2vec = pickle.load(open(\"..\/input\/word2vec\/word2vec.pickle\", \"rb\"))\nword_to_id = pickle.load(open(\"..\/input\/word2vec\/word_to_id.pickle\", \"rb\"))\n        \nmax_len = 100\nvocab_size = len(word_to_id)\nembedding_size = word2vec[0].shape[0]\n\n\ndef words2vec(words):\n    result = []\n    for word in words:\n        if word in word_to_id.keys():\n            # simply skipping a word if it's not in the word2vec\n            result.append(word2vec[word_to_id[word]])\n    while len(result) < max_len:\n        result.append(np.zeros((embedding_size)))\n    return np.vstack(result[:max_len])\n","699385b7":"import tensorflow as tf\n\nlearning_rate = 0.00001\nhidden_layer_1_size = embedding_size\nhidden_layer_2_size = 16\n\nx = tf.placeholder(tf.float32, (max_len, embedding_size), name='x')\ny = tf.placeholder(tf.int32, name='y')\n\nlstm = tf.contrib.rnn.BasicLSTMCell(embedding_size)\ninitial_state = state = lstm.zero_state(1, dtype=tf.float32)\n\nfor i in range(max_len):\n    _, state = lstm(tf.reshape(x[i,:], (1, embedding_size)), state)\n    \nhidden_layer_1 = state\n\nW1 = tf.Variable(tf.random_normal([hidden_layer_1_size ,hidden_layer_2_size]))\nb1 = tf.Variable(tf.random_normal([hidden_layer_2_size]))\n\nhidden_layer_2 = tf.add(tf.matmul(hidden_layer_1, W1), b1)\n\nW2 = tf.Variable(tf.random_normal([hidden_layer_2_size, 2]))\nb2 = tf.Variable(tf.random_normal([2]))\n\n# in the last layer we have 2 classes - positive and negative review\n# we calculate the loss by subtracting the labels from the positive class likelihood  \nlogits = tf.add(tf.matmul(hidden_layer_2, W2), b2)\npredictions = tf.nn.softmax(logits, name='predictions')\n\nloss = tf.nn.softmax_cross_entropy_with_logits(\n    logits = logits,\n    labels = (tf.one_hot(y, 2)))\n\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ngradients = optimizer.compute_gradients(loss)\n\ntrainable_vars = tf.trainable_variables()\ngrads, _ = tf.clip_by_global_norm(tf.gradients(loss, trainable_vars), 1)\n\ntrain_op = optimizer.apply_gradients(zip(grads, trainable_vars))\n\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\nsaver = tf.train.Saver()\n\nimport bz2\n\ntrain_set = \"..\/input\/amazonreviews\/train.ft.txt.bz2\"\n\n# calculating_accuracy = False\niteration = 0\n# total_val_set = 0\n# accurate_val_set = 0\n\nfor line in bz2.BZ2File(train_set, \"r\"):\n    tokens = normalize(line.decode('utf8')).split(' ')\n    output = 1 if tokens[0] == 'label__2' else 0\n    input = words2vec(tokens[1:])\n#     if not calculating_accuracy:\n    sess.run(train_op, feed_dict={x: input, y: output})\n#     else:\n#         total_val_set += 1\n#         predictions = sess.run(predictions, feed_dict={x: input, y: output})\n#         if predictions[1] == 1 and output == 1:\n#             accurate_val_set += 1\n#         if predictions[0] == 0 and output == 0:\n#             accurate_val_set += 1\n    \n    if iteration % 100000 == 0:\n        print('iteration '\n              + str(iteration) + ' loss is : ',\n              sess.run(loss, feed_dict={x: input, y: output}))\n        saver.save(sess, 'sentiment-analysis')\n    iteration += 1\n#     if iteration > 20000:\n#         break\/\n    # i.e. we have passed 90% of the data     \n#         print('finished training')\n#         calculating_accuracy = True\n\n# print('accuracy over validation set:', accurate_val_set\/total_val_set)","f34a35b9":"import bz2\nimport tensorflow as tf\n\nwith tf.Session() as sess:\n    new_saver = tf.train.import_meta_graph('..\/input\/sentiment\/sentiment-analysis.meta')\n    new_saver.restore(sess, tf.train.latest_checkpoint('..\/input\/sentiment\/'))\n    \n    test_set = \"..\/input\/amazonreviews\/test.ft.txt.bz2\"\n\n    iteration = 0\n    total_test_set = 0\n    accurate_test_set = 0\n    graph = tf.get_default_graph()\n    \n    \n    for line in bz2.BZ2File(test_set, \"r\"):\n        tokens = normalize(line.decode('utf8')).split(' ')\n        output = 1 if tokens[0] == 'label__2' else 0\n        input = words2vec(tokens[1:])\n\n        total_test_set += 1\n        \n        x = graph.get_tensor_by_name(\"x:0\")\n        y = graph.get_tensor_by_name(\"y:0\")\n        predictions = graph.get_tensor_by_name(\"predictions:0\")\n        sess.run(predictions, feed_dict={x: input, y: output})\n        if predictions[1] == 1 and output == 1:\n            accurate_test_set += 1\n        if predictions[0] == 0 and output == 0:\n            accurate_test_set += 1\n\n        if iteration % 100 == 0:\n            print('accuracy over test set:', accurate_test_set\/total_test_set)\n            print('iteration ' + str(iteration))\n        iteration += 1\n","b05eaf25":"## Then we create predictions","68f7647a":"# 1. Word2Vec","89a07103":"## **Training **\nI still did not use the whole dataset. The files generated around 6M entries.\nTo make it somewhat random I am taking every 300-th skip-gram pair so that there is a variety of words that had been trained with\n\nI am aware I could have performed way better if training on the whole or at least bigger portion of the set.\nHowever I concentrated more on building the models rather than waiting for all the data to parse in the time given.","cc8e20f5":"# 2. LSTM Sentiment Analysis\n\nFor the sentiment model I have settled on a LSTMCell that recieves the review as a sequence of word2vec.\n\nThen it is followed by two fully connected layers that finally result in two logits, used to predict the class,\nAlthough it outputs only two classes - Good or Bad review, we could easily take the likelihood of a review being good and use it as our prediction.\n\ni.e. if a review is \n[0.0 - 0.4] good then we consider it Bad\n(0.4 - 0.6) good then we consider it Neutral\n[0.6 - 1.0] good then we consider it Good\n\nThis matches in percents to the 5-star system as well","01e13a0b":"## **Saving the Word2Vec** we actually care about\nThe file is generated in the output directory after the notebook has been commited\nI have downloaded it and uploaded it to the Word2Vec dataset after that","d7dab312":"## Training of the model\n\nThere are 3,6M train entries and 4M test entries in total","be3dc065":"## **Generating skip-grams**\n\nThis was run on my machine, I would have probably tried running the word2vec on the whole dataset but it was way too big.\nKaggle did not want to upload the wiki corpus as a dataset for some reason so I could not take advantage of its kernel.\n\nUsed the WikiExtractor (https:\/\/github.com\/attardi\/wikiextractor) to process the Wikipedia Corpus (https:\/\/dumps.wikimedia.org\/enwiki\/latest\/enwiki-latest-pages-articles.xml.bz2)\n\nThe wikicorpus' size (15GB) made it almost impossible for me to figure out how to process it myself in the time given.\n\nonly the first 10 files with text are taken into consideration\n(out of 7K+ generated from the wikiextractor, they might be actually even more... I did not wait till it was done)\nbecause the vocabulary became more than 1M words and my PC started to run into OOM problems (even with batch size 1)\n\nTherefore the accuracy of the whole task could improve because of missing words\nI read somewhere that there are about 3 Million words, so I got like a third of them maybe","d0471273":"> The task is to create a sentiment analysis model on the Amazon Reviews Dataset - https:\/\/www.kaggle.com\/bittlingmayer\/amazonreviews . The model should work on Word2Vec embedding trained on the English Wikipedia corpus. Each review should be classified in one of the three categories: Positive, Neutral or Negative. The accuracy of the model should be as high as possible. "}}