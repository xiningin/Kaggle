{"cell_type":{"fa1b0969":"code","66ef524e":"code","ce51a482":"code","d1d8536f":"code","dc121217":"code","ff274416":"code","7c8b22ce":"code","7b30b51c":"code","12d9a3cb":"code","920366bb":"code","798772a4":"code","03129188":"code","158fa2af":"code","ba4026f2":"code","904e333f":"code","498665c0":"code","4ddbf247":"code","c5f8f4b2":"code","ce91173b":"code","08fc7b7c":"code","27936975":"code","3a1a1349":"code","d5346731":"code","144f62fe":"code","6bc64918":"code","49fc6dd9":"code","95e5f0e3":"code","7d583577":"code","e8982bb5":"code","6c6e1fba":"code","dd5152a4":"markdown","fbf5d1ea":"markdown","fe60c620":"markdown"},"source":{"fa1b0969":"!pip install \/kaggle\/input\/icevision-052\/fastcore-1.3.2-py3-none-any.whl \/kaggle\/input\/icevision-052\/loguru-0.5.3-py3-none-any.whl \/kaggle\/input\/icevision-052\/pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl \/kaggle\/input\/icevision-052\/imagesize-1.2.0-py2.py3-none-any.whl \/kaggle\/input\/icevision-052\/timm-0.4.5-py3-none-any.whl \/kaggle\/input\/icevision-052\/omegaconf-2.0.6-py3-none-any.whl \/kaggle\/input\/icevision-052\/effdet-0.2.1-py3-none-any.whl \/kaggle\/input\/icevision-052\/icevision-0.5.2-py3-none-any.whl","66ef524e":"!pip uninstall fastai -y","ce51a482":"import numpy as np\nimport pandas as pd\nimport os\nfrom icevision.all import *\nfrom tqdm.contrib.concurrent import process_map\nfrom functools import partial\nfrom effdet import DetBenchPredict, unwrap_bench","d1d8536f":"FRAME_RANGE = 4\nVALID_PERCENT = 0.2\nSIZE = (512, 512)\nCLASSES_NUM = 2\nIMPACT_CLASS = 2\n\nIOU_THR = 0.35\nFILTER_OFFSET = 50","dc121217":"path = Path(\"\/kaggle\/input\/nfl-impact-detection\")\ntest_video_path = path\/'test'","ff274416":"def make_images(video_name, video_dir, video_labels, out_dir, only_with_impact=True, impact_cls=IMPACT_CLASS):\n    vidcap = cv2.VideoCapture(str(video_dir\/video_name))\n    frame = 0\n    while True:\n        read, img = vidcap.read()\n        if not read:\n            break\n        frame += 1\n        if only_with_impact:\n            query_str = 'video == @video_name and frame == @frame and impact == @impact_cls'\n            boxes = video_labels.query(query_str)\n            if len(boxes) == 0:\n                continue\n        image_path = f'{out_dir}\/{video_name}'.replace('.mp4', f'_{frame}.png')\n        _ = cv2.imwrite(image_path, img)","7c8b22ce":"test_images_path = Path('\/kaggle\/working\/test_images')\ntest_images_path.mkdir()","7b30b51c":"make_images_part = partial(make_images, video_dir=test_video_path, video_labels=None,\n                           out_dir=test_images_path, only_with_impact=False)\nprocess_map(make_images_part, os.listdir(test_video_path), max_workers=2);","12d9a3cb":"infer_tfms = tfms.A.Adapter([tfms.A.Resize(*SIZE), tfms.A.Normalize()])","920366bb":"class InferParser(parsers.Parser, parsers.FilepathMixin, parsers.SizeMixin):\n    def __init__(self, path):\n        self.images = get_image_files(path)\n\n    def __iter__(self):\n        yield from self.images\n\n    def __len__(self):\n        return len(self.images)\n\n    def imageid(self, o) -> Hashable:\n        return o.stem\n\n    def filepath(self, o) -> Union[str, Path]:\n        return o\n\n    def image_width_height(self, o) -> Tuple[int, int]:\n        return get_image_size(self.filepath(o))","798772a4":"parser = InferParser(\"\/kaggle\/working\/test_images\")\nsplitter = SingleSplitSplitter()","03129188":"infer_rs = parser.parse(data_splitter=splitter, autofix=False)[0]","158fa2af":"infer_ds = Dataset(infer_rs, infer_tfms)","ba4026f2":"!mkdir -p \/root\/.cache\/torch\/hub\/checkpoints\n!cp \/kaggle\/input\/nfl-models\/tf_efficientnet_b5_ra-9a3e5369.pth \/root\/.cache\/torch\/hub\/checkpoints\/","904e333f":"model = efficientdet.model(model_name=\"tf_efficientdet_d5\", num_classes=CLASSES_NUM, img_size=SIZE, pretrained=False)","498665c0":"state_dict = torch.load('\/kaggle\/input\/nfl-models\/efdb5_512_mixup_2cl_5-10ep.pth', map_location='cuda:0')","4ddbf247":"model.load_state_dict(state_dict)","c5f8f4b2":"model.eval();","ce91173b":"infer_dl = efficientdet.infer_dl(infer_ds, batch_size=64)","08fc7b7c":"box_list = []\nscore_list = []\ndetection_threshold = 0.4\n\nwith torch.no_grad():\n    device = torch.device('cuda:0')\n    bench = DetBenchPredict(unwrap_bench(model))\n    bench = bench.eval().to(device)\n\n    for batch, _ in tqdm(infer_dl):\n        imgs, img_info = batch\n        imgs = imgs.to(device)\n        img_info = {k: v.to(device) for k, v in img_info.items()}\n\n        raw_preds = bench(x=imgs, img_info=img_info)\n        dets = raw_preds.detach().cpu().numpy()\n\n        for det in dets:\n            boxes = det[:, :4]\n            scores = det[:, 4]\n            labels = det[:, 5]\n            indexes = np.where((scores > detection_threshold)\n                               & (labels == IMPACT_CLASS))[0]\n            box_list.append(boxes[indexes])\n            score_list.append(scores[indexes])","27936975":"result_image_ids = []\nresults_boxes = []\nresults_scores = []\nfor i in range(len(box_list)):\n    boxes = box_list[i]\n    scores = score_list[i]\n    image_id = infer_rs[i].filepath.name\n    boxes[:, 0] = (boxes[:, 0] * 1280 \/ SIZE[1])\n    boxes[:, 1] = (boxes[:, 1] * 720 \/ SIZE[0])\n    boxes[:, 2] = (boxes[:, 2] * 1280 \/ SIZE[1])\n    boxes[:, 3] = (boxes[:, 3] * 720 \/ SIZE[0])\n    boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n    boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n    boxes = boxes.astype(np.int32)\n    boxes[:, 0] = boxes[:, 0].clip(min=0, max=1280-1)\n    boxes[:, 2] = boxes[:, 2].clip(min=0, max=1280-1)\n    boxes[:, 1] = boxes[:, 1].clip(min=0, max=720-1)\n    boxes[:, 3] = boxes[:, 3].clip(min=0, max=720-1)\n    result_image_ids += [image_id] * len(boxes)\n    results_boxes.append(boxes)\n    results_scores.append(scores)\n    \nresults_boxes = np.concatenate(results_boxes)\nresults_scores = np.concatenate(results_scores)","3a1a1349":"len(results_boxes)","d5346731":"box_df = pd.DataFrame(results_boxes, columns=['left', 'top', 'width', 'height'])\ntest_df = pd.DataFrame({'scores':results_scores, 'image_name':result_image_ids})\ntest_df = pd.concat([test_df, box_df], axis=1)","144f62fe":"test_df['gameKey'] = test_df.image_name.str.split('_').str[0].astype(int)\ntest_df['playID'] = test_df.image_name.str.split('_').str[1].astype(int)\ntest_df['view'] = test_df.image_name.str.split('_').str[2]\ntest_df['frame'] = test_df.image_name.str.split('_').str[3].str.replace('.png','').astype(int)\ntest_df['video'] = test_df.image_name.str.rsplit('_',1).str[0] + '.mp4'\ntest_df = test_df[[\"gameKey\",\"playID\",\"view\",\"video\",\"frame\",\"left\",\"width\",\"top\",\"height\",\"scores\"]]\ntest_df.head()","6bc64918":"def iou(bbox1, bbox2):\n    bbox1 = [float(x) for x in bbox1]\n    bbox2 = [float(x) for x in bbox2]\n\n    (x0_1, y0_1, x1_1, y1_1) = bbox1\n    (x0_2, y0_2, x1_2, y1_2) = bbox2\n    x1_1 += x0_1\n    y1_1 += y0_1\n    x1_2 += x0_2\n    y1_2 += y0_2\n\n    # get the overlap rectangle\n    overlap_x0 = max(x0_1, x0_2)\n    overlap_y0 = max(y0_1, y0_2)\n    overlap_x1 = min(x1_1, x1_2)\n    overlap_y1 = min(y1_1, y1_2)\n\n    # check if there is an overlap\n    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:\n            return 0\n\n    # if yes, calculate the ratio of the overlap to each ROI size and the unified size\n    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)\n    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)\n    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)\n    size_union = size_1 + size_2 - size_intersection\n\n    return size_intersection \/ size_union","49fc6dd9":"keep_idx = []\nfor keys in test_df.groupby(['gameKey', 'playID']).size().to_dict().keys():\n    for view in [\"Endzone\", \"Sideline\"]:\n        tmp_df = test_df.query('gameKey == @keys[0] and playID == @keys[1] and view == @view').copy()\n        while (len(tmp_df) > 0):\n            boxes = tmp_df[[\"left\", \"top\", \"width\", \"height\"]].to_numpy()\n            max_box_idx = tmp_df[\"scores\"].idxmax()\n            max_box = tmp_df[\"scores\"].argmax()\n            ious = np.array(list(map(partial(iou, boxes[max_box]), boxes)))\n            frame = tmp_df.loc[max_box_idx, \"frame\"]\n            m = (ious > IOU_THR) & (tmp_df[\"frame\"].to_numpy() <= frame + FILTER_OFFSET) & (tmp_df[\"frame\"].to_numpy() >= frame - FILTER_OFFSET)\n            keep_idx.append(max_box_idx)\n            tmp_df.drop(tmp_df[m].index, inplace=True)\ntest_df = test_df[test_df.index.isin(keep_idx)].copy()","95e5f0e3":"len(test_df)","7d583577":"test_df.drop(columns=['scores'], inplace=True)\ntest_df.head(3)","e8982bb5":"import nflimpact\n\nenv = nflimpact.make_env()\nenv.predict(test_df)","6c6e1fba":"!rm -rf \/kaggle\/working\/test_images\/*","dd5152a4":"Convert bounding boxes from xyxy to xywh and to origin image size:","fbf5d1ea":"[Train notebook](https:\/\/www.kaggle.com\/nikitautin\/35th-place-efficientdet-train)","fe60c620":"NMS with frame range to filter false impact:"}}