{"cell_type":{"b3f104eb":"code","e0aeddc9":"code","b3cff89b":"code","520e4163":"code","7a847097":"code","d25834ca":"code","d42ef0f6":"code","77934c49":"code","fbe302cc":"code","3c95e93b":"code","856d068a":"code","3b7ae783":"code","26a7f4df":"code","b21162fe":"code","463fa975":"code","2fc6dc28":"code","4bdb7fdb":"code","7b31c1a2":"code","c3a630db":"code","74040dd1":"code","4209d5ac":"code","9e350c90":"code","712e9a76":"code","ac7f37a2":"code","c811355b":"code","7bc95c7f":"code","d7aa99cf":"code","3bfcc376":"code","f6dbd59a":"code","c6169a2c":"code","c02364dc":"code","455dd984":"code","65f82a8d":"code","d37611e8":"code","8727a1bb":"code","ca00b682":"code","8ec02483":"code","7186f549":"code","c59a156e":"code","8550b3f5":"code","935a4119":"code","556b986c":"markdown","d991f2c9":"markdown","3a465ca2":"markdown","fba85636":"markdown","89363977":"markdown","ecdf8c01":"markdown","45fca00e":"markdown","696c88d0":"markdown","c76f3cb0":"markdown","afe9e361":"markdown","5c4800ff":"markdown","4a5a52ec":"markdown","561f7b4c":"markdown","f0927d60":"markdown","ab4d8524":"markdown","7b111973":"markdown","c9293139":"markdown","269bf19a":"markdown","21217013":"markdown","8c415557":"markdown","34c246ee":"markdown","d3257ee6":"markdown","9078c758":"markdown","2c771680":"markdown","c0548930":"markdown","42bf3808":"markdown","635fd458":"markdown","3f25b7dd":"markdown","63d94f78":"markdown","95a14c56":"markdown","579b12fd":"markdown","8bb6d23f":"markdown","52119010":"markdown","5f584594":"markdown","f7898de2":"markdown"},"source":{"b3f104eb":"%%capture \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom pathlib import Path #flexible path files\nimport matplotlib.pyplot as plt #plotting\nfrom fastai import *  \nfrom fastai.tabular import *\nimport torch #Pytorch\nimport missingno as msno #library for missing values visualization\nimport warnings #ignoring warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","e0aeddc9":"# Input data files are available in the \"..\/input\/\" directory.\n# Any results you write to the current directory are saved as output.import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b3cff89b":"path = Path('\/kaggle\/input\/titanic')\ntrpath = path\/'train.csv'\ncvpath = path\/'test.csv'\n\ndf_train_raw = pd.read_csv(trpath)\ndf_test_raw = pd.read_csv(cvpath)\n\ndf_train = df_train_raw.copy(deep = True)\ndf_test  = df_test_raw.copy(deep = True)\n\ndata_cleaner = [df_train_raw, df_test_raw] #to clean both simultaneously","520e4163":"df_train.head(n=10)","7a847097":"df_train.info()","d25834ca":"varnames = list(df_train.columns)\nfor name in varnames:\n    print(name+\": \",type(df_train.loc[1,name]))","d42ef0f6":"print(\"Training Set\")\nprint(df_train.isnull().sum(axis=0))\nprint(\"Test Set\")\nprint(df_test.isnull().sum(axis=0))","77934c49":"msno.matrix(df_train)","fbe302cc":"msno.bar(df_test)","3c95e93b":"print('Overall survival quota:')\ndf_train['Survived'].value_counts(normalize = True)","856d068a":"plt.style.use('seaborn')","3b7ae783":"plt.rcParams['figure.figsize'] = [10, 10]\nplt.rc('xtick', labelsize=14) \nplt.rc('ytick', labelsize=14)\n\nplt.figure()\nfig = df_train.groupby('Survived')['Age'].plot.hist(histtype= 'bar', alpha = 0.7)\nplt.legend(('Died','Survived'), fontsize = 13)\nplt.xlabel('Age', fontsize = 18)\nplt.ylabel('Count', fontsize = 18)\nplt.suptitle('Histogram of the ages of survivors and decased ones',fontsize =22)\nplt.show()","26a7f4df":"df_train['Family onboard'] = df_train['Parch'] + df_train['SibSp']\nplt.rcParams['figure.figsize'] = [20, 7]\nplt.rc('xtick', labelsize=14) \nplt.rc('ytick', labelsize=14)\n\nfig, axes = plt.subplots(nrows=1, ncols=3)\ndf_train.groupby(['Parch'])['Survived'].value_counts(normalize=True).unstack().plot.bar(ax=axes[1],width = 0.85)\ndf_train.groupby(['SibSp'])['Survived'].value_counts(normalize=True).unstack().plot.bar(ax=axes[2],width = 0.85)\ndf_train.groupby(['Family onboard'])['Survived'].value_counts(normalize=True).unstack().plot.bar(ax=axes[0],width = 0.85)\n\naxes[0].set_xlabel('Family onboard',fontsize = 18)\naxes[1].set_xlabel('parents \/ children aboard',fontsize = 18)\naxes[2].set_xlabel(' siblings \/ spouses aboard',fontsize = 18)\n\nfor i in range(3):\n    axes[i].legend(('Died','Survived'),fontsize = 13, loc = 'upper left')\naxes[0].set_ylabel('Survival rate',fontsize = 18)\n\nfor ax in fig.axes:\n    plt.sca(ax)\n    plt.xticks(rotation=0)\n\nplt.suptitle('Survival rates over Number of relatives onboard',fontsize =22)\nplt.show()","b21162fe":"plt.rcParams['figure.figsize'] = [7, 5]\nplt.rc('xtick', labelsize=14) \nplt.rc('ytick', labelsize=14) \n\nplt.figure()\nfig = df_train.groupby(['Sex'])['Survived'].value_counts(normalize=True).unstack().plot.bar(width = 0.5)\nplt.legend(('Died','Survived'),fontsize = 13, loc = 'upper left')\nplt.xlabel('Gender',fontsize =18)\nplt.xticks(rotation=0)\nplt.ylabel('Survival rate',fontsize = 18)\n\n\nplt.suptitle('Survival rates over Gender',fontsize =22)\nplt.show()","463fa975":"plt.rcParams['figure.figsize'] = [8, 5]\nplt.rc('xtick', labelsize=14) \nplt.rc('ytick', labelsize=14) \n\nplt.figure()\nfig = df_train.groupby('Pclass')['Survived'].value_counts(normalize=True).unstack().plot.bar(width = 0.5)\nplt.legend(('Died','Survived'),fontsize = 13, loc = 'upper left')\nplt.xlabel('Ticket Class',fontsize =18)\nplt.ylabel('Survival rate',fontsize = 18)\nplt.suptitle('Survival rate over Ticket class', fontsize = 22)\nplt.xticks(rotation=0)\nplt.show()\n","2fc6dc28":"df_train['Title'] = df_train['Name'].str.split(',',expand = True)[1].str.split('.',expand = True)[0].str.strip()\nvarnames = list(df_train.columns)\n    \nprint(\"Training set: \" ,list(df_train['Title'].unique()))    \ndf_test['Title'] = df_test['Name'].str.split(',',expand = True)[1].str.split('.',expand = True)[0].str.strip()\nprint(\"Test set: \" ,list(df_test['Title'].unique()))    \n","4bdb7fdb":"def new_titles(df):\n    new_titles = dict()\n    assert 'Title' in df.columns\n    for key in df['Title'].unique():\n        females = ['Mrs','Miss','Ms','Mlle','Mme','Dona']\n        males = ['Mr','Don']\n        notable = ['Jonkheer','the Countess','Lady','Sir','Major','Col','Capt','Dr','Rev','Notable']\n        titles = [females,males,notable,'Master']\n        newtitles = ['Mrs','Mr','Notable','Master']\n        idx = [key in sublist for sublist in titles]\n        idx = np.where(idx)[0] \n        new_titles[key] = newtitles[idx[0]]\n    return new_titles\n\n\nnew_titles_dict = new_titles(df_train)\ndf_train['Title'] = df_train['Title'].replace(new_titles_dict)","7b31c1a2":"plt.rcParams['figure.figsize'] = [12, 5]\nplt.rc('xtick', labelsize=14) \nplt.rc('ytick', labelsize=14) \n\nplt.figure()\nfig = df_train.groupby(['Title'])['Survived'].value_counts(normalize=True).unstack().plot.bar(width = 0.7)\nplt.legend(('Died','Survived'),fontsize = 13, loc = 'upper left')\nplt.xlabel('Title',fontsize =16)\nplt.xticks(rotation=0)\n\n\nplt.suptitle('Survival rates over Title',fontsize =22)\nplt.show()","c3a630db":"df_train['Cabin'][df_train['Cabin'].isnull()]='Missing'\ndf_train['Cabin'] = df_train['Cabin'].str.split(r'(^[A-Z])',expand = True)[1]","74040dd1":"plt.rcParams['figure.figsize'] = [12, 5]\nplt.figure()\nfig = df_train.groupby(['Cabin'])['Survived'].value_counts(normalize=True).unstack().plot.bar(width = 0.9)\nplt.legend(('Died','Survived'),fontsize = 13, loc = 'upper left')\nplt.xlabel('Cabin Deck',fontsize =18)\nplt.suptitle('Survival rates over Cabin Deck',fontsize =22)\nplt.xticks(rotation=0)\nplt.show()","4209d5ac":"plt.rcParams['figure.figsize'] = [10, 5]\nplt.figure()\nfig = df_train.groupby(['Embarked'])['Survived'].value_counts(normalize=True).unstack().plot.bar(width = 0.7)\nplt.legend(('Died','Survived'),fontsize = 13, loc = 'upper left')\nplt.xlabel('Embarking Port',fontsize =18)\nplt.suptitle('Survival rates over embarking port',fontsize =22)\nplt.xticks(rotation=0)\nplt.show()","9e350c90":"df_train.groupby(['Embarked'])['Pclass'].value_counts(normalize=True).unstack()","712e9a76":"df_train.corr(method='pearson')['Age'].abs()","ac7f37a2":"def df_fill(datasets, mode):\n    assert mode =='median' or mode =='sampling'\n    datasets_cp =[]\n    np.random.seed(2)\n    varnames = ['Age','Fare']\n    for d in datasets:\n        df = d.copy(deep = True)\n        for var in varnames:\n            idx = df[var].isnull()\n            if idx.sum()>0:\n                if mode =='median':\n                    medians = df.groupby('Pclass')[var].median()\n                    for i,v in enumerate(idx):\n                        if v:\n                            df[var][i] = medians[df['Pclass'][i]]\n                else:\n                    g = df[idx==False].groupby('Pclass')[var]\n                    for i,v in enumerate(idx):\n                        if v:\n                            df[var][i] = np.random.choice((g.get_group(df['Pclass'][i])).values.flatten())\n    #Embarked                 \n        idx = df['Embarked'].isnull()\n        g = df[idx==False].groupby('Pclass')['Embarked']\n        for i,v in enumerate(idx):\n            if v:\n                df['Embarked'][i] = np.random.choice((g.get_group(df['Pclass'][i])).values.flatten())                   \n    #Cabin\n        df['Cabin'][df['Cabin'].isnull()]='Missing'\n        df['Cabin'] = df['Cabin'].str.split(r'(^[A-Z])',expand = True)[1]\n        datasets_cp.append(df)\n    return datasets_cp\n\ndata_clean = df_fill(data_cleaner,'median')","c811355b":"def prepare_data(datasets):\n        datasets_cp = []\n        for d in datasets:\n            df = d.copy(deep = True)\n            df['Family onboard'] = df['Parch'] + df['SibSp']\n            df['Title'] = df['Name'].str.split(',',expand = True)[1].str.split('.',expand = True)[0].str.strip()\n            new_titles_dict = new_titles(df)\n            df['Title'] = df['Title'].replace(new_titles_dict)\n            df.drop(columns = ['PassengerId','Name','Ticket'],axis = 1, inplace = True)\n            datasets_cp.append(df)\n        return datasets_cp\n        ","7bc95c7f":"train,test =prepare_data(df_fill(data_cleaner,mode = 'sampling'))  \nprint(\"Training data\")\nprint(train.isnull().sum())\nprint(\"Test data\")\nprint(test.isnull().sum())","d7aa99cf":"cont_names = ['Fare','Age','Pclass','SibSp','Parch','Family onboard']\ncat_names = ['Sex','Cabin','Embarked']\nprocs = [Categorify,Normalize]\ndep_var = 'Survived'\n\ndata_test = TabularList.from_df(test, cat_names=cat_names, cont_names=cont_names, procs=procs)\n\ndata = (TabularList.from_df(train, path='\/kaggle\/working', cat_names=cat_names, cont_names=cont_names, procs=procs)\n                           .split_by_rand_pct(0.2)\n                           .label_from_df(cols = dep_var)\n                           .add_test(data_test, label=0)\n                           .databunch()\n       )","3bfcc376":"learn = tabular_learner(data, \n                        layers=[500,200,100],\n                        metrics=accuracy,\n                        emb_drop=0.1,\n                       )\n\nlearn.model","f6dbd59a":"torch.device('cuda')\nlearn.fit_one_cycle(2, 2.5e-2)\nlearn.save('stage1')","c6169a2c":"learn.lr_find()\nlearn.recorder.plot()","c02364dc":"learn.unfreeze()\nlearn.fit_one_cycle(3, max_lr=slice(4e-1))\nlearn.save('stage2')","455dd984":"learn.lr_find()\nlearn.recorder.plot()","65f82a8d":"learn.unfreeze()\nlearn.fit_one_cycle(2, max_lr=slice(1e-2))\nlearn.save('stage3')","d37611e8":"learn.lr_find()\nlearn.recorder.plot()","8727a1bb":"learn.unfreeze()\nlearn.fit_one_cycle(2, max_lr=slice(5e-3))\nlearn.save('stage4')","ca00b682":"learn.lr_find()\nlearn.recorder.plot()","8ec02483":"learn.unfreeze()\nlearn.fit_one_cycle(5, max_lr=slice(9e-4))\nlearn.save('stage5')","7186f549":"learn.unfreeze()\nlearn.fit_one_cycle(5, max_lr=slice(5e-5))\nlearn.save('stage6')","c59a156e":"learn.recorder.plot_losses()","8550b3f5":"# learn.load('stage6')\npredictions, *_ = learn.get_preds(DatasetType.Test)\nlabels = np.argmax(predictions, 1)\nsubmission = pd.DataFrame({'PassengerId':df_test['PassengerId'],'Survived':labels})","935a4119":"submission.to_csv('submission-fastai.csv', index=False)","556b986c":"Training will be conducted using acyclical learning rates. To begin training we will fit 2 cycles, meaning 2 full epochs through the data with a learning rate of 0.025 . Then, we will call the learning rate finder that plots the training loss for differnt learning rates in order to pick the best value for further training.","d991f2c9":"# 2. Undestanding the Data","3a465ca2":"**Setting up plotting parameters:**","fba85636":"The variables included in the data are:\n\n* ***PassengerId***: Passenger index\n* Survived: Whether the passenger in the accident. Possible values:\n    * 0 = died , \n    * 1 = survived\n* ***Pclass***: Passenger class. Possible values:\n    * 1 = First class\n    * 2 = Second class\n    * 3 = Third class\n* ***Name***: Passenger name\n* ***Sex***: Passenger gender. Possible values:\n    * male\n    * female\n* ***Age***: Passenger Age\n* ***SibSp***: Number of siblings\/spouses on board\n* ***Parch***: Number of parents\/children on board\n* ***Ticket***: Ticket number\n* ***Fare***: Ticket cost\n* ***Cabin***: Cabin number\n* ***Embarked***: Port of Embarkation. Possible values:\n    * C = Cherbourg\n    * Q = Queenstown\n    * S = Southampton","89363977":"# 6. Veridct\n\nWhile during some runs of the model we were able to achieve ~87% accuracy on the validation set, running the model multiple times yields varying results. It appears that the obstacles of the small dataset and the missing values cannot be overcome. A different ML algorithm like Decision trees or SVM could achieve a much higher accuracy with the same feature engineering.","ecdf8c01":"Now we want to pick a learning rate where the loss decay is the steepest. Looking at the plot, we train 3 more cycles at a learning rate of 4e-1. And call the learning rate finder again.","45fca00e":"# 5. Train model","696c88d0":"This notebook contains an approach to predict the survivors of the Titanic ship sinking using Neural Networks. Every step in the process, from getting the data to predicting the survivors is thoroughly documented and explained. ","c76f3cb0":"We see that the strongest correlation of the variable age is with the variable Pclass (passenger class). Therefore, it is appropriate to use this information in order to sample the missing ages according to the pclass. We can either take the median of each Pclass group or sample a random value from that group. We are going to try both and see which one yields better results. Sampling from a distribution, however, seems like the more viable option, since we have a lot of missing values to replace and setting all of them to the same value would skew the distribution massively. For the other missing variables ( Fare, Embarked), the analysis we performed above leads us to believe that sampling according to the passengers' class is a viable method.","afe9e361":"# 1. Reading the data and setting up the environment","5c4800ff":"We can see that there are 2 major categories with missing data, as well as a couple of missing values in other two.\n\n**Major missing value variables: **\n* Age\n* Cabin\n\n**Minor missing value variables: **\n* Fare\n* Embarked","4a5a52ec":"We would like to see what performance a neural network can achieve. Neural networks are usually preferable when the data is high dimensional and the training set size is big, which is not the case here. However, we would like to determine whether a respectable result can be achieved with the feature engineering that was performed. To train the model, we will be using the fast.ai library which provides some very neat features to tune the hyperparameters of the model. To perform training, we need to define which variables are continuous and which are categorical and will be trained using embeddings. The target variable is 'Survived'. 20% of the training set has been chosen for validation. Before training, the data is also normalized to minimize the impact of outliers on the training and speeding-up the network.","561f7b4c":"Data file locations:","f0927d60":"As expected, first class passengers have a higher survival rate, meaning they were either given priority during evacuation or they were closer to the lifeboats. THis can be double-checked through the cabin feature that will be discussed later. \n\nWe would now to check if the title name of a person can be useful in determining whether that person survived or not. This assumption stems from the idea that people of higher status could have been given higher priority during the ship's evacuation.  Therefore, we create a new variable called 'Title'.","ab4d8524":"It is very important to understand whether and where there are missing values in the data (both train and test). This will help us determine a strategy for filling in the missing values.","7b111973":"Finally, we initially thought that the embarking port should be irrelevant to the task. However, passengers that embarked the ship in Cherbourg were more likely to survive. An explanation for that could be that more rich people embarked the ship and were travelling in a better class.","c9293139":"Before we start cleaning up the data, it is important to see which variables are of relevance, which can be ignored  and what is the most appropriate way to fill in the missing values. As we can see in the charts above, there are 3 variables with missing values in the training set(Age,Cabin and Embarked) and only 2 in the test set (Age,Cabin). In the test set, there is also 1 fare entry missing, which we will fill later on. We shall now try and decide what we are going to do with those values.","269bf19a":"Let's now explore the impact that the amount of relatives on board had on survival. For that, we create a new feature called 'Family onboard', which is the sum of parents\/children\/siblings\/spouses (variables Parch and SibSp).","21217013":"# 4. Data Cleaning","8c415557":"Some of these titles can be grouped up, since they mean the same thing. For example, \"Mrs\", \"Miss\", \"Ms\" will be grouped together under the label \"Mrs\". There are also some titles that appear to actually be a name instead of a title (Mlle, Mme, Dona) that will also be mapped to the same value. \"Don\" is probably an abbreviation to a male name and will be mapped to \"Mr\". The rest of the titles denote nobility, military or clergy service and doctors. To avoid sparse categories, they are all grouped under the title 'Notable'. Finally, 'Master' is kept as a standalone title that was given to men under 26 years of age.","34c246ee":"We see that the ages distribution between those who survived and those who did not is similar. We see, however, that more young-aged passengers were saved. This was expected, since it lines up with ship evacuation policies. Other than that, age is probably not a major factor that determined who survived the accident.","d3257ee6":"The layer architecture is a hyperparameter that the user can freely choose. After experimenting with it, a 3-layer network  with declining size has been chosen. Every layer has batch normalization and weight decay on,while the dropout probability is 10%. ","9078c758":"There are no more missing values in our data. We are now ready to create a model and start training!","2c771680":"We see that the cabin decks have different survival rates. As for the ones where the data was missing, the rates line up with the overall survival rate of the ship (~68%-32%).\n\n","c0548930":"# 3. Exploratory Data Analysis","42bf3808":"Again, we see that different titles have different survival probabilities. A small surprising result is that people under the title 'Notable' have a low survival rate. One would expect that 'notable' people would travel first class and would therefore have a higher survival chance (see above), but is appears that this is not the case. This result indicates that the higher survival rates of the first class passengers' have to do with their positioning on the ship. We shall now examine that. To do that, we only keep the cabin deck portion of the 'cabin' variable and, since there are a lot of missing cabin information, the missing values are denoted as 'M' for missing. ","635fd458":"We are now going to ensure that there are no missing values in the dataset and prepare it for training our model. The 4 categories that have missing values in the train and test sets are:\n* Age \n* Cabin \n* Embarked \n* Fare\n\nIn order to ease the documents' readability, any extra variables created above will be recreated here from scratch and will be encapsulated in a function. This is done to make it easier to the reader to find all feature engineering procedures in one place.","3f25b7dd":"Let's first take a look at the first couple of rows of the training data, as well as the types of variables that the dataframe posesses and their corresponding value types.","63d94f78":"We see a clear trend that passengers with a family size between 1 and 3 had the higher the chance of survival. They are the only columns where the survivors are more than the deceased ones. Family size also combines the other 2 variables nicely and gives a more clear picture of the survival chances. Therefore, we conclude that this is an interesting feature to include in our training data.","95a14c56":"We will now train a little bit more at the point of steepest loss decline and repeat the same procedure, until the training and validation loss stabilize.","579b12fd":"We also see that female passengers had a higher chance of survival than male ones. It was expected that females and children would be more likely to survive, as the evacuation protocol of the ship was instructing accordingly. Let us now compare the survival chances and the passengers' ticket class.","8bb6d23f":"First, explore how to fill in the missing ages. Several strategies pinpoint to replace the missing values with the mean or median of the whole distribution, which in our eyes doesn't seem a good choice. Instead, let's look into the correlation of age with the other variables.","52119010":"Now we can read the data into Pandas dataframes. A copy of the original data is kept should we require it later. Both training and test datasets are put together in a list so that we can iterate over both at the same time during data cleaning. ","5f584594":"We can now check the survival rates for each title to see if there is some useful information here.","f7898de2":"The first step to analyzing the data is to load all the libraries we are going to use. This is performed at the start so that we can know at any point which libraries are loaded in the notebook. "}}