{"cell_type":{"f0ded08d":"code","5d96f4b0":"code","4eedd083":"code","13cdce1e":"code","2d8503c1":"code","864d7eca":"code","c8531b41":"code","f608e4f6":"code","2fc524cc":"code","e0538397":"code","b0ce2943":"code","220b1b00":"code","49e4ecc0":"code","b7a8cc62":"code","f0c070e3":"code","33ceb4ab":"code","4d55ca65":"code","622ba37c":"code","763f9dc8":"code","66d1bbb5":"code","08ce03d1":"code","40b18262":"code","5ea5267f":"code","8f551b57":"code","c1045536":"code","0a499017":"code","fb6a769d":"code","4ecff635":"code","eed0dc3e":"code","302bee71":"code","7dcf1780":"code","a072c757":"code","565b0194":"code","c5b41a4a":"code","39b72b84":"code","e0979a13":"code","3a492240":"code","f99eda13":"code","e7464a97":"code","65592cce":"code","80667360":"code","d5f3b125":"code","0815192b":"code","6bdb1a23":"code","eb5f8afc":"code","370c20b0":"code","de6dec15":"code","df58d2dd":"code","c42d10d4":"code","c5ec7273":"code","0b55592e":"code","9a1d521c":"code","57731b97":"code","90a2b5e5":"code","46b30488":"code","2d5567d0":"code","375f17de":"code","0bccebeb":"code","9e5c2939":"code","2bc4eae2":"code","f0176904":"markdown","9fc0c10d":"markdown","5f429919":"markdown","ef0e5676":"markdown","cfa34a8b":"markdown","827ebca7":"markdown","d0de6d50":"markdown","f9b0827b":"markdown","a804ac56":"markdown","fea01235":"markdown","6a96bd08":"markdown","76efa7a1":"markdown","fc70a975":"markdown","d2a6f3a7":"markdown","907c9cd8":"markdown","e29a8363":"markdown","1b88f226":"markdown","0170362d":"markdown","1c7f3e2a":"markdown","521a3544":"markdown","7e80cfb9":"markdown","f82a58c9":"markdown","0d65df8d":"markdown","1c9b5f53":"markdown","8c8984b7":"markdown","e680e34c":"markdown","8e2d683b":"markdown","a345d6be":"markdown","c4d4c8f9":"markdown","81667a48":"markdown","95e653be":"markdown","7c64d003":"markdown","4f0cfbde":"markdown","a4b79075":"markdown","acb46b3b":"markdown","fc45c52f":"markdown","f7ba42f5":"markdown","200b7556":"markdown","30999af2":"markdown","4fdd5205":"markdown","b9d985f4":"markdown","7fdaa1a7":"markdown","9d3d81aa":"markdown","320d13ea":"markdown","b79a4d44":"markdown","f8195110":"markdown","ceb96d42":"markdown","2054382f":"markdown","e37e8c27":"markdown","d182804a":"markdown","4567ed56":"markdown","85aff234":"markdown","2df1c1be":"markdown","a72e96de":"markdown","14b25425":"markdown"},"source":{"f0ded08d":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns","5d96f4b0":"df = pd.read_csv('\/kaggle\/input\/walmart-dataset\/Walmart.csv')\ndf.head()","4eedd083":"df.shape","13cdce1e":"df.dtypes","2d8503c1":"df.describe().T","864d7eca":"print(df.columns.to_list())","c8531b41":"df.Date = pd.to_datetime(df.Date)\ndf.Date.dtype","f608e4f6":"df['year'], df['month'] = df['Date'].dt.year, df['Date'].dt.month\ndf.sample(5)","2fc524cc":"df[['Store', 'Date', 'Weekly_Sales', 'Holiday_Flag', 'Temperature', 'Fuel_Price']].nlargest(10,'Weekly_Sales')","e0538397":"# we convert the store column to categorical since each Value is unique store no\ndf.Store = pd.Categorical(df.Store)\ndf.Store.dtype","b0ce2943":"df['Holiday_Flag'] = pd.Categorical(df.Holiday_Flag)\ndf.Holiday_Flag.dtype","220b1b00":"# Print the maximum of the date column\nprint(df.Date.max())\n\n# Print the minimum of the date column\nprint(df.Date.min())","49e4ecc0":"# A custom IQR function\ndef iqr(column):\n    return column.quantile(0.75) - column.quantile(0.25)\n    \nprint(df[[\"Temperature\", \"Fuel_Price\", \"Unemployment\"]].agg([iqr,np.mean,np.median]))","b7a8cc62":"# Sort sales_1_1 by date\nsales_1_1 = df.sort_values('Date')\n\n# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\nsales_1_1['cum_weekly_sales'] = sales_1_1.Weekly_Sales.cumsum()\n\n# Get the max of weekly_sales, add as cum_max_sales col\nsales_1_1['max_sales'] = sales_1_1.Weekly_Sales.max()\n\n# See the columns we calculated\nsales_1_1[[\"Date\", \"Weekly_Sales\", \"cum_weekly_sales\", \"max_sales\"]]","f0c070e3":"# Subset the rows where is_holiday is True and drop duplicate dates\nholiday_dates = df[df['Holiday_Flag'] == 1].drop_duplicates(subset = 'Date')\n\n# Print date col of holiday_dates\nholiday_dates.Date","33ceb4ab":"Store = df.groupby(['Store']).agg({'Weekly_Sales':['mean','max','sum']})\nStore[:5]","4d55ca65":"plt.style.use('seaborn-darkgrid')","622ba37c":"plt.figure(figsize = (15,8))\nStore[('Weekly_Sales',  'sum')].plot()\nplt.show()","763f9dc8":"plt.figure(figsize = (15,8))\nStore[('Weekly_Sales',  'sum')].plot(kind = 'bar',color = 'blue')\nplt.xticks(rotation = 0)\nplt.title('Total Sum of Sales')\nplt.axhline(y=200000000,color = 'orange')\nplt.axhline(y=100000000,color = 'red')\nplt.axhline(y=300000000,color = 'green')\nplt.show()","66d1bbb5":"plt.figure(figsize = (15,8))\nStore[('Weekly_Sales',  'max')].plot(kind = 'bar',color = 'violet')\nplt.xticks(rotation = 0)\nplt.title('Max Weekly_Sales')\nplt.axhline(y=2000000,color = 'orange')\nplt.axhline(y=1000000,color = 'red')\nplt.axhline(y=3000000,color = 'green')\nplt.show()","08ce03d1":"# check for total values in Holiday_Flag column\ndf.Holiday_Flag.value_counts()","40b18262":"Store_new = df.groupby(['Store','Holiday_Flag']).agg({'Weekly_Sales':['mean','max','sum']})\nStore_new = Store_new.reset_index()\nStore_new","5ea5267f":"plt.figure(figsize=(15,8))\nsns.barplot(x = 'Store',y = ('Weekly_Sales',  'mean'),hue= 'Holiday_Flag',data=Store_new)\nplt.show()","8f551b57":"# Temparature vs Weekly Sales\nplt.figure(figsize=(15,8))\nsns.scatterplot(x = 'Temperature',y = 'Weekly_Sales',hue = 'Store',data = df,legend = False)\nplt.show()","c1045536":"# We can say that temperature doesn't have much impact","0a499017":"# CPI vs Weekly Sales\nplt.figure(figsize=(15,8))\nsns.scatterplot(x = 'CPI',y = 'Weekly_Sales',data = df,legend = False)\nplt.show()","fb6a769d":"# Again no significant pattern can be observed","4ecff635":"# Unemployment vs Week_sales\nplt.figure(figsize=(15,8))\nsns.scatterplot(x = 'Unemployment',y = 'Weekly_Sales',data = df,legend = False)\nplt.show()","eed0dc3e":"# Again no significant pattern can be observed","302bee71":"sns.pairplot(df)\nplt.show()","7dcf1780":"# Temperature\nfig, axs = plt.subplots(nrows=3, figsize=(15, 15))\nsns.boxplot(x = df['Temperature'], ax=axs[0])\nsns.violinplot(x = df['Temperature'], ax=axs[1])\nsns.boxenplot(x = df['Temperature'], ax=axs[2])\nplt.show()","a072c757":"# CPI\nfig, axs = plt.subplots(nrows=3, figsize=(15, 15))\nsns.boxplot(x = df['CPI'], ax=axs[0],color='lightblue')\nsns.violinplot(x = df['CPI'], ax=axs[1],color='lightblue')\nsns.boxenplot(x = df['CPI'], ax=axs[2],color='lightblue')\nplt.show()","565b0194":"# Unemployement\nplt.figure(figsize=(15,8))\nsns.histplot(x = 'Unemployment',data = df)\nplt.title(\"Histogram of Unemployement\")\nplt.show()","c5b41a4a":"df.sample(5)","39b72b84":"df.info()","e0979a13":"df_dummies = pd.get_dummies(df,columns=['Store','Holiday_Flag'])\nprint(df_dummies.columns.to_list())","3a492240":"X = df_dummies.drop(['Date','Weekly_Sales'],axis=1)\ny = df_dummies.Weekly_Sales","f99eda13":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n","e7464a97":"from sklearn.linear_model import LinearRegression","65592cce":"lr = LinearRegression()","80667360":"lr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)","d5f3b125":"df_new = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf_new.head()","0815192b":"print(f\"{y.mean()}\")\ny_pred.mean()","6bdb1a23":"from sklearn import metrics\nprint('Mean Absolute error: ', metrics.mean_absolute_error(y_test,y_pred))\nprint('Mean Squared Error: ', metrics.mean_squared_error(y_test,y_pred))\nprint('Root Mean Squared Error: ', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","eb5f8afc":"# variance score: 1 means perfect prediction\nprint(f\"The variance score is: {lr.score(X_test,y_test)}\")","370c20b0":"# rmse\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrmse = sqrt(mean_squared_error(y_test, y_pred))\n\nfrom sklearn.metrics import r2_score\nr2 = r2_score(y_test, y_pred)\n\nadj_r2 = 1 - float(len(y)-1)\/(len(y)-len(lr.coef_)-1)*(1 - r2)\n\nrmse, r2, adj_r2","de6dec15":"import statsmodels.formula.api as smf\nimport statsmodels.stats.api as sms\nfrom scipy import stats\nfrom statsmodels.compat import lzip\nimport statsmodels","df58d2dd":"print(df_dummies.columns.to_list())","c42d10d4":"model = smf.ols(\"Weekly_Sales~ Temperature + Fuel_Price + CPI + Unemployment + year + month + Store_1 + Store_2 + Store_3+ Store_4 + Store_5 + Store_6 + Store_7 + Store_8 + Store_9 + Store_10 + Store_11 + Store_12 + Store_13 + Store_14 + Store_15 + Store_16 + Store_17 + Store_18 + Store_19 + Store_20 + Store_21 + Store_22 + Store_23 + Store_24+ Store_25 + Store_26+ Store_27 + Store_28 + Store_29 + Store_30 + Store_31 + Store_32 + Store_33 + Store_34 + Store_35 + Store_36 + Store_37 + Store_38 + Store_39 + Store_40 + Store_41 + Store_42 + Store_43 + Store_44 + Store_45 + Holiday_Flag_0 + Holiday_Flag_1\",data = df_dummies).fit()\nmodel.summary()","c5ec7273":"# check normality using q-q plot\nstats.probplot(model.resid,dist = 'norm',plot = plt)\nplt.title(\"Model residuals Q-Q plot\")\nplt.show()","0b55592e":"# check for homoscedascity and heteroscedasticity\nname = ['Lagrange multiplier statistic','p-value','f-value','f p-value']\ntest = sms.het_breuschpagan(model.resid,model.model.exog)\nlzip(name,test)","9a1d521c":"sns.histplot(model.resid);","57731b97":"from sklearn.preprocessing import PolynomialFeatures\n\ndef prepare_data(df):\n    variable = X.copy()\n    variable_array = np.array(variable)\n    variable_array = PolynomialFeatures(1).fit_transform(variable_array)\n\n    return variable_array\n\ninput_train1 = prepare_data(X_train)\ninput_test1 = prepare_data(X_test)\noutput_train1 = np.array(y)\noutput_test1 = np.array(y)","90a2b5e5":"class my_linear_regression:\n    def __init__(self) : # initialize constructor for the object to assign the object its properties\n        self.X_train = []\n        self.y_train = []\n        self.weights = []\n        \n    def fit(self, X, y) :\n        self.X_train = X\n        self.y_train = y\n        self.weights = np.linalg.solve(X.T@X,X.T@y)\n    \n    def predict(self,X_test,y_test) : # method of the object that can be used\n        self.y_hat=np.sum(X_test*self.weights,axis=1)\n        \n        self.MSE= np.square(np.subtract(y_test,self.y_hat)).mean() \n        \n        return self.y_hat, self.MSE","46b30488":"model_1 = my_linear_regression()\nk = model_1.fit(input_train1,output_train1)\nmodel_1.y_hat,model_1.MSE = model_1.predict(input_train1,output_train1)","2d5567d0":"model_1.y_hat","375f17de":"model_1.MSE","0bccebeb":"model_1.weights[1:]","9e5c2939":"# rmse\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nfrom sklearn.metrics import r2_score\nr2 = r2_score(output_test1, model_1.y_hat)\n\nadj_r2 = 1 - float(len(y)-1)\/(len(y)-len(model_1.weights)-1)*(1 - r2)\n\nr2, adj_r2","2bc4eae2":"final_df = pd.DataFrame({'Model':['scikitlearn','statsmodel','Polynomial features'],\n                         'R-Square':[0.924,0.926,0.926],'Adjusted R-Square':[0.924,0.925,0.925]})\nfinal_df","f0176904":"### Line chart to view trend across stores for Total weekly_sales column","9fc0c10d":"#### Check for actual vs predicted mean","5f429919":"__From the charts we can say that stores 4,20 are the best performing stores while 5,33,44 are the least performing__","ef0e5676":"### Using Polynomial Features","cfa34a8b":"### Adding dummy variables for Categorical features","827ebca7":"### Convert the numerical columns to categorical","d0de6d50":"The function also receives an array y with the true y values to compute the mean square error (MSE) using the formula : $$MSE = \\dfrac{1}{N}\\sum_{i=1}^{N}(y_i -\\hat{y}_i)^2$$, where $y_i$ is y_test[i]. \n    \n**Implement the MSE.**","f9b0827b":"### Bar chart for better analysis","a804ac56":"**Step 3 - Training**\n\nHere we define a class called *my_linear_regression* which initializes with *__init__(self)* as empty the properties self.X_train, self.y_train and self.weights. Then we define the method *fit()* of the class which needs as parameters a matrix (X) and an array (y) with the outputs. With those parameters the class initializes self.x_train, self.y_train using the matrix and the array respectively and self.weigths using the formula of the linear regression: $$\\hat{W}=(\ud835\udc7f^\ud835\udc47\\ \ud835\udc7f)^{\u22121}\ud835\udc7f^\ud835\udc47\\ \ud835\udc9a$$\n\nWe are using the function *np.linalg.solve*($\ud835\udc7f^\ud835\udc47\\ \ud835\udc7f,\ud835\udc7f^\ud835\udc47\\ \ud835\udc9a$) to obtain $\\hat{W}$ because it is more efficient than matrix inversion and it avoids numerical inestabilities.\n\nThe last function in the object *my_linear_regression* is the function *predict()*. Given the weights obtained through fit, this method estimates new y values using the input matrix X_test: \n\n<code>self.y_hat=np.sum(X*self.weights,axis=1)<\\code>","fea01235":"#### Fitting the model","6a96bd08":"### Read the dataset","76efa7a1":"#### Check for error","fc70a975":"### Check the datatypes of each column","d2a6f3a7":"**Step 2 - Prepare the data:**\nWe split our data into two sets: one data set for training and another one that we will use at the end to test our model.\n\n1. Import the function ```train_test_split``` from ```sklearn.model_selection```\n2. Split our *df* in **X** made of all features except *Date*,*Weekly_Sales* and **y** made of the feature *Weekly_Sales* \n3. Use ```train_test_split``` with a *test_size*=0.20 (20 % of inputs became the test set) in following way to obtain a train set and a test set.\n\n    ```X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)```","907c9cd8":"#### rmse,rsquare and adjusted rsquare","e29a8363":"__What can be improved__:<br>\n* Consider the date feature\n* Do some more exploratory data analysis with other features like CPI,temperature\n* Run ridge regression and linear regression with Tensorflow or any other model to compare results\n* Using PCA to reduce the dimensions and check for the performance.\n","1b88f226":"### Set Style ","0170362d":"## __Machine Learning__","1c7f3e2a":"### Convert the date column from object to datetime","521a3544":"### Aggregating Weekly sales based on store and holiday flag","7e80cfb9":"### Check for Descriptive Statistics","f82a58c9":"__What is Homoskedastic?__ <br>\nHomoskedastic (also spelled \"homoscedastic\") refers to a condition in which the variance of the residual,or <br>error term, in a regression model is constant. That is, the error term does not vary much as the value of <br>the predictor variable changes. Another way of saying this is that the variance of the data points are <br>roughly the same for all data points. This suggests a level of consistency and makes it easier to model and <br>work with the data through regression. However, the lack of homoskedasticity may suggest that the regression <br>model may need to include additional predictor variables to explain the performance of the dependent variable.\n\nKEY TAKEAWAYS\n* Homoskedasticity occurs when the variance of the error term in a regression model is constant. \n* If the variance of the error term is homoskedastic, the model was well-defined. If there is too much variance, the model may not be defined well.<br> \n* Adding additional predictor variables can help explain the performance of the dependent variable.<br>\n* Oppositely, heteroskedasticity occurs when the variance of the error term is not constant.","0d65df8d":"### Checking for top 10 largest Sales","1c9b5f53":"#### Create a DataFrame of Actual vs Predicted values","8c8984b7":"__What Is Heteroskedasticity?__<br>\nIn statistics, heteroskedasticity (or heteroscedasticity) happens when the standard deviations of a predicted variable, <br>monitored over different values of an independent variable or as related to prior time periods, are non-constant. <br>With heteroskedasticity, the tell-tale sign upon visual inspection of the residual errors is that they will tend to fan out over time, as depicted in the image below.<br>\n\nHeteroskedasticity often arises in two forms: conditional and unconditional.<br> Conditional heteroskedasticity identifies nonconstant volatility related to prior period's (e.g., daily) volatility.<br> Unconditional heteroskedasticity refers to general structural changes in volatility that are not related to prior period volatility.<br> Unconditional heteroskedasticity is used when future periods of high and low volatility can be identified.<br>\nSource: _Investopedia_","e680e34c":"### About this file\nThis is the historical data that covers sales from 2010-02-05 to 2012-11-01, in the file WalmartStoresales.\nWithin this file you will find the following fields:<br>\n\n\n| **Field**   |     **Description** |  \n|-------------|---------------------|\n| Store       |  the store number | \n| Date        |  the week of sales |\n| Weekly_Sales| sales for the given store   |\n| Holiday_Flag| whether the week is a special holiday week 1 \u2013 Holiday week 0 \u2013 Non-holiday week | \n| Temperature | Temperature on the day of sale |\n| Fuel_Price  |  Cost of fuel in the region |\n| CPI         |  Prevailing consumer price index | \n| Unemployment|  Prevailing unemployment rate |\n| Holiday Events| Super Bowl;Labour Day;Thanksgiving;Christmas|","8e2d683b":"### Analysis by store","a345d6be":"### Plotting for all features","c4d4c8f9":"**Step 1:** The very first step is to have a deeper look into the data:\n1. Using pandas extract a dataframe called *df* from the file *walmart.csv*\n2. Print the result of the method  ```name_dataframe.d_types```, in this way you print out the data types associated to each of the fields in the table\n3. Run the method ```name_dataframe.head(N)``` to look at first N instances of the dataframe.\n4. Use the method ```name_dataframe.describe( )``` to generate descriptive statistics that summarize each field of the dataframe","81667a48":"Conclusion: All models have pretty much similar Rsquare and Adjusted Rsquare","95e653be":"#### Fit and predict the model","7c64d003":"## __Description:__ <br>\nOne of the leading retail stores in the US, Walmart, would like to predict the sales and demand accurately. There are certain events and holidays which impact sales on each day. There are sales data available for 45 stores of Walmart. The business is facing a challenge due to unforeseen demands and runs out of stock some times, due to the inappropriate machine learning algorithm. An ideal ML algorithm will predict demand accurately and ingest factors like economic conditions including CPI, Unemployment Index, etc. <br>\n\nWalmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of all, which are the Super Bowl, Labour Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Part of the challenge presented by this competition is modeling the effects of markdowns on these holiday weeks in the absence of complete\/ideal historical data. Historical sales data for 45 Walmart stores located in different regions are available.<br>\n\n__Acknowledgements__<br>\nThe dataset is taken from Kaggle. <br>\n\n__Objective:__<br>\nUnderstand the Dataset & cleanup (if required). <br>\nBuild Regression models to predict the sales w.r.t single & multiple features. <br>\nAlso evaluate the models & compare their respective scores like R2, RMSE, etc. <br>","4f0cfbde":"# __Walmart Dataset__ <br>\n_Walmart Store Sales Prediction - Regression Problem_","a4b79075":"## __Data Preprocessing__","acb46b3b":"### Check the shape of the dataset","fc45c52f":"### Check for columns","f7ba42f5":"###  Using the Holiday_flag column to check Weekly_sales","200b7556":"### Check for distribution of numerical features","30999af2":"**Store 14 has the most weekly_sales, while store 20,10,13,4 have are in top 10 twice**","4fdd5205":"### Import the libraries","b9d985f4":"## __Data Exploration__","7fdaa1a7":"#### Fit and predict the model","9d3d81aa":"**We can do the same for max Weekly sales for each store**","320d13ea":"__A bar plot is much more conclusive and we get the following observations__\n* Stores getting total sales below the redline are underperforming<br>\n* Stores between red and orange are average <br>\n* Stores between orange and green are performing above average<br>\n* Stores touching the green line are very well performing<br>","b79a4d44":"__Inference__: <br>\n* People generally tend to spend more during holiday weeks<br>\n* The average spending is more during holiday weeks than normal weeks\n    ","f8195110":"### Using sklearn","ceb96d42":"The dataset contains a lot of features that can be used to build the model. \n\n\\begin{equation}\n \\hat{y} = W_0 + W_1\\ x_{Store} + W_2\\ x_{Holiday Flag} + W_3\\ x_{Temperature} + W_4\\ x_{Fuel Price} +W_5\\ x_{CPI}+W_5\\ x_{Unemployment}\n\\end{equation}\n\nWe need to use *numpy* library and\n\n<code>from sklearn.preprocessing import PolynomialFeatures<\\code>\n\nwe define a function called *prepare_data(name_dataframe)* which does the following:\n1. extract the above features from the dataframe and assign to a variable\n2. transform our variable in numpy array <code>np.array(variable)<\/code>\n3. and then add a columns of ones (for the variable of $W_0$) to the dummy variable\n4. return our variable\n    \nWe then apply our function to *X_train* and *X_test* to obtain *input_train1* and *input_test1* respectively\nThen we create also *output_train* and *output_test* moving to numpy array *y_train1* and *y_test1* respectively. Test data will be used later in the code\n\n","2054382f":"## __Exploratory Data Analysis__","e37e8c27":"#### Checking for Normality","d182804a":"#### Checking for features to add in the model","4567ed56":"#### Check for rmse,rsquare,adjusted rsquare","85aff234":"12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\\ Labour Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\\ Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\\ Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13","2df1c1be":"### Features vs Weekly sales","a72e96de":"### Using Statsmodel","14b25425":"#### Importing the libraries"}}