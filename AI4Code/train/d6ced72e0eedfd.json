{"cell_type":{"ee83b4f8":"code","510e61ce":"code","b4b961a9":"code","53cb4384":"code","b098ec38":"code","a2a15873":"code","798623c0":"code","67f8162e":"code","9fccb714":"code","7772ab38":"code","0b190c72":"code","9a1e9200":"code","e6cbf4aa":"code","1513005e":"code","deafe562":"code","87889f46":"code","e8915095":"code","b0ab9706":"code","6530dc73":"code","9275e873":"code","6fa63199":"code","1c022d3e":"code","681b3c96":"code","cac13031":"code","fba6f9ee":"code","fb99671f":"code","836ea808":"code","cbd2e8b3":"code","311d4020":"code","edd4f43b":"code","86f88046":"code","1c10abdc":"code","fb40f45a":"code","65b82542":"code","9050a2d1":"code","185e3386":"code","942bd824":"code","7255ec14":"code","3fe5b7b9":"code","d638ddaf":"code","a1e48f2f":"code","1efcedd7":"code","927a279b":"code","b1b510a4":"code","4639910e":"code","d640143b":"code","a23ed2cb":"code","6c182aa2":"code","7ec17485":"code","8e41e409":"code","0a76d86f":"code","730e2925":"code","792403a5":"code","0b453292":"code","7fbb916e":"code","d90f8684":"code","3f447e41":"code","c2650c60":"code","463ad024":"code","e315d14f":"code","3adf9d65":"code","5c00294b":"code","b4d9595d":"code","44165c1f":"code","01d23acc":"code","e2e38f8f":"code","6c69c2e5":"code","5a9dbc64":"code","cd47a911":"code","793d6ae7":"code","37e24b24":"code","4f908797":"code","c3d615aa":"code","ddafe286":"code","5a0d17ce":"code","2405f438":"code","d15f6929":"code","7b55a07b":"code","da0e0e2e":"code","7576066c":"code","5a9adbd2":"code","8472fb9a":"code","d5d1f1cc":"code","939e9ced":"code","49c6ce08":"code","45fbeba0":"code","cd93b64d":"code","7d570965":"code","83ac4a5d":"code","12fc8c2a":"code","d7b31dd0":"code","d77b8ab6":"code","26da8cfd":"code","43773bbd":"code","3a1a3bf9":"code","da831a2f":"code","8b248f7c":"code","baef15f5":"code","ad3d126b":"code","af0cd28c":"code","3b28de55":"code","3ec927df":"code","156c2a57":"code","a658cb1c":"code","6fa0ced2":"code","89278fff":"code","409c54a7":"code","c9fed982":"code","22d30a53":"code","2160a53e":"code","13d267d1":"code","76bd35f8":"code","5f6d1fdc":"code","2336dc6a":"code","2f68b32c":"code","e7053bfb":"code","4dfe67e6":"code","bbe01509":"code","25f5295d":"code","54099c77":"code","3658d1fb":"markdown","d14ba9f5":"markdown","7199148a":"markdown","512fbf2d":"markdown","c7fddd2d":"markdown","c39f45b7":"markdown","fcb56668":"markdown","47450b7d":"markdown","577a0fbf":"markdown","c3a3a9be":"markdown","4708c501":"markdown","30f186d6":"markdown","4ecaa903":"markdown","47c25557":"markdown","1e958f27":"markdown","4713c400":"markdown","43dcfcf3":"markdown","3c632fa4":"markdown","46f0d8f1":"markdown","e0ac9aed":"markdown","9af8cb99":"markdown","9f05a817":"markdown","ef11a8f0":"markdown","562a05af":"markdown","a60f95e4":"markdown","079efbd5":"markdown","1feb0e2e":"markdown","7f37192c":"markdown","08ae8b6d":"markdown","cd27eefc":"markdown"},"source":{"ee83b4f8":"# Imported Libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")","510e61ce":"import os\nprint(os.listdir(\"..\/input\/creditcardfraud\"))","b4b961a9":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","53cb4384":"df.describe()","b098ec38":"# check for null values\ndf.isnull().sum().max()","a2a15873":"df.columns","798623c0":"#observe the different feature type present in the data","67f8162e":"df['Class'].count()","9fccb714":"fraud = df[df.Class == 1]\nnormal = df[df.Class == 0]","7772ab38":"classes=df['Class'].value_counts()\nnormal_share=classes[0]\/df['Class'].count()*100\nfraud_share=classes[1]\/df['Class'].count()*100","0b190c72":"# The classes are heavily skewed we need to solve this issue later.\nprint('No Frauds', round(normal_share,2), '% of the dataset')\nprint('Frauds', round(fraud_share,2), '% of the dataset')","9a1e9200":"# Create a count plot for the number and percentage of fraudulent vs non-fraudulent transcations\ncolors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('Class', data=df, palette=colors)\n\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)\n","e6cbf4aa":"# Create a scatter plot to observe the distribution of classes with time & amount\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Time of transaction vs Amount by class')\n\nax1.scatter(fraud.Time, fraud.Amount)\nax1.set_title('Fraud')\nax1.grid(color='k', linestyle='-', linewidth=0.1)\n\n\nax2.scatter(normal.Time, normal.Amount)\nax2.set_title('Normal')\nax2.grid(color='k', linestyle='-', linewidth=0.1)\n\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","1513005e":"# Distribution plots wrt Transaction Amount & Transaction Time to measure skewness\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\n\n\nplt.show()","deafe562":"# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\nfrom sklearn.preprocessing import RobustScaler\n\n# RobustScaler is less prone to outliers.\n\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)","87889f46":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\n\ndf.head()","e8915095":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n\n# Check the Distribution of the labels\n\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","b0ab9706":"# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n\n# Lets shuffle the data before creating the subsamples\n\ndf = df.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nsub_sampled_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nsub_sampled_df.head()","6530dc73":"print('Distribution of the Classes in the subsample dataset')\nprint(sub_sampled_df['Class'].value_counts()\/len(sub_sampled_df))\n\n\n\nsns.countplot('Class', data=sub_sampled_df, palette=colors)\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()","9275e873":"# Make sure we use the subsample in our correlation\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire DataFrame\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (not to be used for reference)\", fontsize=14)\n\n\nsub_sample_corr = sub_sampled_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\nax2.set_title('SubSample Correlation Matrix \\n (to be used for reference)', fontsize=14)\nplt.show()","6fa63199":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V17\", data=sub_sampled_df, palette=colors, ax=axes[0])\naxes[0].set_title('V17 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=sub_sampled_df, palette=colors, ax=axes[1])\naxes[1].set_title('V14 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=sub_sampled_df, palette=colors, ax=axes[2])\naxes[2].set_title('V12 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=sub_sampled_df, palette=colors, ax=axes[3])\naxes[3].set_title('V10 vs Class Negative Correlation')\n\nplt.show()","1c022d3e":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V11\", data=sub_sampled_df, palette=colors, ax=axes[0])\naxes[0].set_title('V11 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=sub_sampled_df, palette=colors, ax=axes[1])\naxes[1].set_title('V4 vs Class Positive Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=sub_sampled_df, palette=colors, ax=axes[2])\naxes[2].set_title('V2 vs Class Positive Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V19\", data=sub_sampled_df, palette=colors, ax=axes[3])\naxes[3].set_title('V19 vs Class Positive Correlation')\n\nplt.show()","681b3c96":"from scipy.stats import norm\n\nf, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n\nv14_fraud_dist = sub_sampled_df['V14'].loc[sub_sampled_df['Class'] == 1].values\nsns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nv12_fraud_dist = sub_sampled_df['V12'].loc[sub_sampled_df['Class'] == 1].values\nsns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n\n\nv10_fraud_dist = sub_sampled_df['V10'].loc[sub_sampled_df['Class'] == 1].values\nsns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nplt.show()","cac13031":"# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\nv14_fraud = sub_sampled_df['V14'].loc[sub_sampled_df['Class'] == 1].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\nv14_cut_off = v14_iqr * 1.5\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V10 outliers:{}'.format(outliers))\n\nsub_sampled_df = sub_sampled_df.drop(sub_sampled_df[(sub_sampled_df['V14'] > v14_upper) | (sub_sampled_df['V14'] < v14_lower)].index)\nprint('----' * 44)\n\n# -----> V12 removing outliers from fraud transactions\nv12_fraud = sub_sampled_df['V12'].loc[sub_sampled_df['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 Lower: {}'.format(v12_lower))\nprint('V12 Upper: {}'.format(v12_upper))\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\nprint('V12 outliers: {}'.format(outliers))\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\nsub_sampled_df = sub_sampled_df.drop(sub_sampled_df[(sub_sampled_df['V12'] > v12_upper) | (sub_sampled_df['V12'] < v12_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(sub_sampled_df)))\nprint('----' * 44)\n\n\n# Removing outliers V10 Feature\nv10_fraud = sub_sampled_df['V10'].loc[sub_sampled_df['Class'] == 1].values\nq25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\nv10_iqr = q75 - q25\n\nv10_cut_off = v10_iqr * 1.5\nv10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\nprint('V10 Lower: {}'.format(v10_lower))\nprint('V10 Upper: {}'.format(v10_upper))\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\nprint('V10 outliers: {}'.format(outliers))\nprint('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\nsub_sampled_df = sub_sampled_df.drop(sub_sampled_df[(sub_sampled_df['V10'] > v10_upper) | (sub_sampled_df['V10'] < v10_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(sub_sampled_df)))","fba6f9ee":"f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))\n\ncolors = ['#B3F9C5', '#f9c5b3']\n# Boxplots with outliers removed\n# Feature V14\nsns.boxplot(x=\"Class\", y=\"V14\", data=sub_sampled_df,ax=ax1, palette=colors)\nax1.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\nax1.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature V12\nsns.boxplot(x=\"Class\", y=\"V12\", data=sub_sampled_df, ax=ax2, palette=colors)\nax2.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\nax2.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.3), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature V10\nsns.boxplot(x=\"Class\", y=\"V10\", data=sub_sampled_df, ax=ax3, palette=colors)\nax3.set_title(\"V10 Feature \\n Reduction of outliers\", fontsize=14)\nax3.annotate('Fewer extreme \\n outliers', xy=(0.95, -16.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n\nplt.show()","fb99671f":"# sub_sampled_df is from the random undersample data (fewer instances)\nX = sub_sampled_df.drop('Class', axis=1)\ny = sub_sampled_df['Class']\n\n\n# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))\n\n# PCA Implementation\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"PCA took {:.2} s\".format(t1 - t0))\n\n# TruncatedSVD\nt0 = time.time()\nX_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"Truncated SVD took {:.2} s\".format(t1 - t0))","836ea808":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n# labels = ['No Fraud', 'Fraud']\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\nax1.grid(True)\n\nax1.legend(handles=[blue_patch, red_patch])\n\n\n# PCA scatter plot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax2.set_title('PCA', fontsize=14)\n\nax2.grid(True)\n\nax2.legend(handles=[blue_patch, red_patch])\n\n# TruncatedSVD scatter plot\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax3.set_title('Truncated SVD', fontsize=14)\n\nax3.grid(True)\n\nax3.legend(handles=[blue_patch, red_patch])\n\nplt.show()","cbd2e8b3":"X = sub_sampled_df.drop('Class', axis=1)\ny= sub_sampled_df['Class']","311d4020":"from sklearn.model_selection import train_test_split\n\n# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","edd4f43b":"# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","86f88046":"print(np.sum(y))\nprint(np.sum(y_train))\nprint(np.sum(y_test))","1c10abdc":"# Let's implement simple classifiers\nfrom xgboost import XGBClassifier\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"Random Forest Classifier\": RandomForestClassifier(),\n    \"XGBoost Classifier\": XGBClassifier()\n}","fb40f45a":"# try building the models and check the score for getting the best model\n\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","65b82542":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_\n\n# Random Forest Classifier\nrf_params = {\"n_estimators\": list(range(2,100,1)), \"criterion\": [\"entropy\"], \"max_depth\": list(range(2,100,10))}\ngrid_rf = GridSearchCV(RandomForestClassifier(), rf_params)\ngrid_rf.fit(X_train, y_train)\n\n# Random Forest best estimator\nrf_clf = grid_rf.best_estimator_\n\n# XGBoost Classifier\nxgb_params = {\"n_estimators\": list(range(2,100,1)), \"max_depth\": list(range(2,100,10)), \"learning_rate\": [0.1,0.3,0.5,0.7,0.9],\n             \"booster\": [\"gbtree\"], \"objective\":[\"binary:logistic\"]}\ngrid_xgb = GridSearchCV(XGBClassifier(), xgb_params)\ngrid_xgb.fit(X_train, y_train)\n\n# XGBoost best estimator\nxgb_clf = grid_xgb.best_estimator_","9050a2d1":"# Overfitting Case\n\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')\n\nrf_score = cross_val_score(rf_clf, X_train, y_train, cv=5)\nprint('Random Forest Classifier Cross Validation Score', round(rf_score.mean() * 100, 2).astype(str) + '%')\n\nxgb_score = cross_val_score(xgb_clf, X_train, y_train, cv=5)\nprint('XGBoost Classifier Cross Validation Score', round(xgb_score.mean() * 100, 2).astype(str) + '%')\n","185e3386":"# We will undersample during cross validating\nundersample_X = df.drop('Class', axis=1)\nundersample_y = df['Class']\n\nfor train_index, test_index in sss.split(undersample_X, undersample_y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values \n\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\n# Implementing NearMiss Technique \n# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\nX_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n# Cross Validating the right way\n\nfor train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) \n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n    \n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))","942bd824":"# Let's Plot LogisticRegression Learning Curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4,estimator5, estimator6, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4), (ax5,ax6)) = plt.subplots(3,2, figsize=(20,30), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    \n    # Fifth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator5, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax5.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax5.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax5.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax5.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax5.set_title(\"Random Forest Classifier \\n Learning Curve\", fontsize=14)\n    ax5.set_xlabel('Training size (m)')\n    ax5.set_ylabel('Score')\n    ax5.grid(True)\n    ax5.legend(loc=\"best\")\n    \n    # Sixth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator6, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax6.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax6.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax6.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax6.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax6.set_title(\"XGBoost Classifier \\n Learning Curve\", fontsize=14)\n    ax6.set_xlabel('Training size (m)')\n    ax6.set_ylabel('Score')\n    ax6.grid(True)\n    ax6.legend(loc=\"best\")\n    \n    return plt","7255ec14":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, rf_clf, xgb_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)","3fe5b7b9":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)\n\nrf_pred = cross_val_predict(rf_clf, X_train, y_train, cv=5)\n\nxgb_pred = cross_val_predict(xgb_clf, X_train, y_train, cv=5)","d638ddaf":"from sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))\nprint('Random Forest Classifier: ', roc_auc_score(y_train, rf_pred))\nprint('XGBoost Classifier: ', roc_auc_score(y_train, xgb_pred))","a1e48f2f":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\nrf_fpr, rf_tpr, rf_threshold = roc_curve(y_train, rf_pred)\nxgb_fpr, xgb_tpr, xgb_threshold = roc_curve(y_train, xgb_pred)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr,rf_fpr,\n                             rf_tpr,xgb_fpr, xgb_tpr):\n    plt.figure(figsize=(20,12))\n    plt.title('ROC Curve \\n Top 6 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot(rf_fpr, rf_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_train, rf_pred)))\n    plt.plot(xgb_fpr, xgb_tpr, label='XGBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_train, xgb_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr,rf_fpr,\n                             rf_tpr,xgb_fpr, xgb_tpr)\nplt.show()","1efcedd7":"def logistic_roc_curve(log_fpr, log_tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('Logistic Regression ROC Curve', fontsize=16)\n    plt.plot(log_fpr, log_tpr, 'b-', linewidth=2)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n    \n    \nlogistic_roc_curve(log_fpr, log_tpr)\nplt.show()","927a279b":"from sklearn.metrics import precision_recall_curve\n\nprecision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)","b1b510a4":"from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred = log_reg.predict(X_train)\n\n# Overfitting Case\nprint('---' * 45)\nprint('Overfitting: \\n')\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\n\nprint('---' * 45)\n\n# How it should look like\nprint('---' * 45)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\nprint('---' * 45)","4639910e":"undersample_y_score = log_reg.decision_function(original_Xtest)","d640143b":"from sklearn.metrics import average_precision_score\n\nundersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      undersample_average_precision))","a23ed2cb":"from sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(12,6))\n\nprecision, recall, _ = precision_recall_curve(original_ytest, undersample_y_score)\n\nplt.step(recall, precision, color='#004a93', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='#48a6ff')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          undersample_average_precision), fontsize=16)","6c182aa2":"# best classifier is log_reg\nclf = log_reg\nsplitChar = '('\nalgoName = str(clf).partition(splitChar)[0]","7ec17485":"coefficients  = pd.DataFrame(clf.coef_.ravel())\n\ncolumn_df     = pd.DataFrame(sub_sampled_df.columns)\ncoef_sumry    = (pd.merge(coefficients,column_df,left_index= True,\n                          right_index= True, how = \"left\"))\ncoef_sumry.columns = [\"coefficients\",\"features\"]\ncoef_sumry    = coef_sumry.sort_values(by = \"coefficients\",ascending = False)","8e41e409":"coef_sumry.head(10)","0a76d86f":"import plotly.graph_objs as go\nimport plotly.subplots as sub_plots\nimport plotly.figure_factory as ff\nimport plotly.offline as py \nimport matplotlib.pyplot as plt\npy.init_notebook_mode(connected=True)\n\n#plot coeffs\ntrace = go.Bar(x = coef_sumry[\"features\"],y = coef_sumry[\"coefficients\"],\n                name = \"coefficients\",\n                marker = dict(color = coef_sumry[\"coefficients\"],\n                              colorscale = \"Picnic\",\n                              line = dict(width = .6,color = \"black\")))\n\ndata = [trace]\nlayout = go.Layout(dict(title = \" Feature Importance \",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"Feature Importance\",\n                                         zerolinewidth=1,\n                                         ticklen=5,\n                                         gridwidth=2\n                                        ),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"percent\",\n                                         zerolinewidth=1,\n                                         ticklen=5,\n                                         gridwidth=2\n                                        ),\n                       )\n                  )\nfig  = go.Figure(data=data,layout=layout)\n\npy.iplot(fig)\n","730e2925":"from imblearn import over_sampling","792403a5":"# example of evaluating a decision tree with random oversampling\nfrom numpy import mean\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import RandomOverSampler\n# define oversampling strategy\noversample = RandomOverSampler(sampling_strategy='minority')\n\n# fit and apply the transform\nX_over, y_over = oversample.fit_resample(original_Xtrain, original_ytrain)\n\n","0b453292":"# Let's implement simple classifiers\nfrom xgboost import XGBClassifier\nclassifiers_over = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"Random Forest Classifier\": RandomForestClassifier(),\n    \"XGBoost Classifier\": XGBClassifier()\n}","7fbb916e":"# try building the models and check the score for getting the best model\n\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers_over.items():\n    classifier.fit(X_over, y_over)\n    training_score = cross_val_score(classifier, X_over, y_over, cv=5)\n    print(\"Classifier: \", classifier.__class__.__name__, \"has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","d90f8684":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params_over = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\n\ngrid_log_reg_over = GridSearchCV(LogisticRegression(), log_reg_params_over)\ngrid_log_reg_over.fit(X_over, y_over)\n# We automatically get the logistic regression with the best parameters.\nlog_reg_clf_over = grid_log_reg_over.best_estimator_\n\n\n","3f447e41":"log_reg_clf_over","c2650c60":"# DecisionTree Classifier\ntree_params_over = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree_over = GridSearchCV(DecisionTreeClassifier(), tree_params_over)\ngrid_tree_over.fit(X_over, y_over)\n\n# tree best estimator\ntree_clf_over = grid_tree_over.best_estimator_\n","463ad024":"tree_clf_over","e315d14f":"# Random Forest Classifier\nrf_params_over = {\"n_estimators\": list(range(2,100,30)), \"criterion\": [\"entropy\"], \"max_depth\": [50]}\ngrid_rf_over = GridSearchCV(RandomForestClassifier(), rf_params_over)\ngrid_rf_over.fit(X_over, y_over)\n\n# Random Forest best estimator\nrf_clf_over = grid_rf_over.best_estimator_\n","3adf9d65":"rf_clf_over","5c00294b":"# XGBoost Classifier\nxgb_params_over = {\"n_estimators\": [50], \"max_depth\": [50], \"learning_rate\": [0.9],\n             \"booster\": [\"gbtree\"], \"objective\":[\"binary:logistic\"]}\ngrid_xgb_over = GridSearchCV(XGBClassifier(), xgb_params_over)\ngrid_xgb_over.fit(X_over, y_over)\n\n# XGBoost best estimator\nxgb_clf_over = grid_xgb_over.best_estimator_","b4d9595d":"# Oversampled Dataset Case\n\nlog_reg_score_over = cross_val_score(log_reg_clf_over, X_over, y_over, cv=5)\nprint('Logistic Regression Cross Validation Score using Oversampling: ', round(log_reg_score_over.mean() * 100, 2).astype(str) + '%')\n\ntree_score_over = cross_val_score(tree_clf_over, X_over, y_over, cv=5)\nprint('DecisionTree Classifier Cross Validation Score using Oversampling', round(tree_score_over.mean() * 100, 2).astype(str) + '%')\n\nrf_score_over = cross_val_score(rf_clf_over, X_over, y_over, cv=5)\nprint('Random Forest Classifier Cross Validation Score using Oversampling', round(rf_score_over.mean() * 100, 2).astype(str) + '%')\n\nxgb_score_over = cross_val_score(xgb_clf_over, X_over, y_over, cv=5)\nprint('XGBoost Classifier Cross Validation Score using Oversampling', round(xgb_score_over.mean() * 100, 2).astype(str) + '%')\n","44165c1f":"# Let's Plot LogisticRegression Learning Curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve_over(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,30), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    \n    # Second Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Random Forest Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"XGBoost Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    \n    return plt","01d23acc":"cv_over = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\nplot_learning_curve_over(log_reg_clf_over,tree_clf_over, rf_clf_over, xgb_clf_over, X_over, y_over, (0.85, 1.05), cv=cv_over, n_jobs=4)","e2e38f8f":"# Predictions for different models\n\nlog_reg_pred_over = cross_val_predict(log_reg_clf_over, X_over, y_over, cv=5,method=\"decision_function\")\ntree_pred_over = cross_val_predict(tree_clf_over, X_over, y_over, cv=5)\nrf_pred_over = cross_val_predict(rf_clf_over, X_over, y_over, cv=5)\nxgb_pred_over = cross_val_predict(xgb_clf_over, X_over, y_over, cv=5)\n\n\nprint('Logistic Regression for Oversampling: ', roc_auc_score(y_over, log_reg_pred_over))\nprint('Decision Tree Classifier for Oversampling: ', roc_auc_score(y_over, tree_pred_over))\nprint('Random Forest Classifier for Oversampling: ', roc_auc_score(y_over, rf_pred_over))\nprint('XGBoost Classifier for Oversampling: ', roc_auc_score(y_over, xgb_pred_over))\n\n","6c69c2e5":"log_fpr_over, log_tpr_over, log_thresold_over = roc_curve(y_over, log_reg_pred_over)\ntree_fpr_over, tree_tpr_over, tree_threshold_over = roc_curve(y_over, tree_pred_over)\nrf_fpr_over, rf_tpr_over, rf_threshold_over = roc_curve(y_over, rf_pred_over)\nxgb_fpr_over, xgb_tpr_over, xgb_threshold_over = roc_curve(y_over, xgb_pred_over)\n","5a9dbc64":"def graph_roc_curve_multiple_over(log_fpr, log_tpr, tree_fpr, tree_tpr,rf_fpr,\n                             rf_tpr,xgb_fpr, xgb_tpr):\n    plt.figure(figsize=(20,12))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_over, log_reg_pred_over)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_over, tree_pred_over)))\n    plt.plot(rf_fpr, rf_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_over, rf_pred_over)))\n    plt.plot(xgb_fpr, xgb_tpr, label='XGBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_over, xgb_pred_over)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n ","cd47a911":"graph_roc_curve_multiple_over(log_fpr_over, log_tpr_over, tree_fpr_over, tree_tpr_over,rf_fpr_over,\n                             rf_tpr_over,xgb_fpr_over, xgb_tpr_over)\nplt.show()","793d6ae7":"def random_forest_roc_curve(rf_fpr, rf_tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('Random Forest ROC Curve', fontsize=16)\n    plt.plot(rf_fpr, rf_tpr, 'b-', linewidth=2)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n    \n    \nrandom_forest_roc_curve(rf_fpr_over, rf_tpr_over)\nplt.show()","37e24b24":"from sklearn.metrics import precision_recall_curve\n\nprecision_over, recall_over, threshold_over = precision_recall_curve(y_over, rf_pred_over)\n\nfrom sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred_over = rf_clf_over.predict(X_over)\n","4f908797":"# Oversampled Case\nprint('---' * 45)\nprint('Oversampled Case: \\n')\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_over, y_pred_over)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_over, y_pred_over)))\nprint('Recall Score: {:.2f}'.format(recall_score(y_over, y_pred_over)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_over, y_pred_over)))\n\nprint('---' * 45)\n\n# How it should look like\nprint('---' * 45)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\nprint('---' * 45)","c3d615aa":"oversample_y_score = rf_clf_over.predict_proba(original_Xtest)","ddafe286":"print(oversample_y_score)","5a0d17ce":"from sklearn.metrics import average_precision_score\n\noversample_average_precision = average_precision_score(original_ytest, oversample_y_score[:,1])\n\nprint('Average precision-recall score for Random Oversampled data: {0:0.2f}'.format(\n      oversample_average_precision))","2405f438":"fig = plt.figure(figsize=(12,6))\n\nprecision_over, recall_over, threshold_over = precision_recall_curve(original_ytest, undersample_y_score)\n\nplt.step(recall_over, precision_over, color='#004a93', alpha=0.2,\n         where='post')\nplt.fill_between(recall_over, precision_over, step='post', alpha=0.2,\n                 color='#48a6ff')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Oversampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          oversample_average_precision), fontsize=16)\n ","d15f6929":"# best classifier is Random Forest\nclf_over = rf_clf_over\nsplitChar = '('\nalgoName_over = str(clf_over).partition(splitChar)[0]\n\ncoefficients_over  = pd.DataFrame(clf_over.feature_importances_)\n\ncolumn_df_over     = pd.DataFrame(sub_sampled_df.columns)\ncoef_sumry_over    = (pd.merge(coefficients_over,column_df_over,left_index= True,\n                          right_index= True, how = \"left\"))\ncoef_sumry_over.columns = [\"coefficients\",\"features\"]\ncoef_sumry_over    = coef_sumry_over.sort_values(by = \"coefficients\",ascending = False)\n\ncoef_sumry_over.head(10)","7b55a07b":"#plot coeffs\ntrace_over = go.Bar(x = coef_sumry_over[\"features\"],y = coef_sumry_over[\"coefficients\"],\n                name = \"coefficients\",\n                marker = dict(color = coef_sumry_over[\"coefficients\"],\n                              colorscale = \"Picnic\",\n                              line = dict(width = .6,color = \"black\")))\n\ndata_over = [trace_over]\nlayout = go.Layout(dict(title = \" Feature Importance \",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"Feature Importance\",\n                                         zerolinewidth=1,\n                                         ticklen=5,\n                                         gridwidth=2\n                                        ),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"percent\",\n                                         zerolinewidth=1,\n                                         ticklen=5,\n                                         gridwidth=2\n                                        ),\n                       )\n                  )\nfig_over  = go.Figure(data=data_over,layout=layout)\n\npy.iplot(fig_over)\n","da0e0e2e":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nsm = over_sampling.SMOTE(random_state=0)\nX_train_smote, y_train_smote = sm.fit_resample(original_Xtrain, original_ytrain)\n# Artificial minority samples and corresponding minority labels from SMOTE are appended\n# below X_train and y_train respectively\n# So to exclusively get the artificial minority samples from SMOTE, we do\nX_train_smote_1 = X_train_smote[X_train.shape[0]:]\n\nX_train_1 = original_Xtrain[np.where(y_train==1.0)]\nX_train_0 = original_Xtrain[np.where(y_train==0.0)]\n\n\nplt.rcParams['figure.figsize'] = [20, 20]\nfig = plt.figure()\n\nplt.subplot(3, 1, 1)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 2)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_smote_1[:X_train_1.shape[0], 0], X_train_smote_1[:X_train_1.shape[0], 1],\n            label='Artificial SMOTE Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 3)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\nplt.legend()\n","7576066c":"classifiers_sm = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"Random Forest Classifier\": RandomForestClassifier(),\n    \"XGBoost Classifier\": XGBClassifier()\n}","5a9adbd2":"# try building the models and check the score for getting the best model\n\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers_sm.items():\n    classifier.fit(X_train_smote, y_train_smote)\n    training_score = cross_val_score(classifier, X_train_smote, y_train_smote, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","8472fb9a":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params_sm = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\ngrid_log_reg_sm = GridSearchCV(LogisticRegression(), log_reg_params_sm)\ngrid_log_reg_sm.fit(X_train_smote, y_train_smote)\n# We automatically get the logistic regression with the best parameters.\nlog_reg_sm = grid_log_reg_sm.best_estimator_\n\n","d5d1f1cc":"\n# DecisionTree Classifier\ntree_params_sm = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree_sm = GridSearchCV(DecisionTreeClassifier(), tree_params_sm)\ngrid_tree_sm.fit(X_train_smote, y_train_smote)\n\n# tree best estimator\ntree_clf_sm = grid_tree_sm.best_estimator_\n","939e9ced":"# Random Forest Classifier\nrf_params_sm = {\"n_estimators\": [10], \"criterion\": [\"entropy\"], \"max_depth\": [10]}\ngrid_rf_sm = GridSearchCV(RandomForestClassifier(), rf_params_sm)\ngrid_rf_sm.fit(X_train_smote, y_train_smote)\n\n# Random Forest best estimator\nrf_clf_sm = grid_rf_sm.best_estimator_\n","49c6ce08":"\n# XGBoost Classifier\nxgb_params_sm = {\"n_estimators\": [10], \"max_depth\": [10], \"learning_rate\": [0.5,0.7,0.9],\n             \"booster\": [\"gbtree\"], \"objective\":[\"binary:logistic\"]}\ngrid_xgb_sm = GridSearchCV(XGBClassifier(), xgb_params_sm)\ngrid_xgb_sm.fit(X_train_smote, y_train_smote)\n\n# XGBoost best estimator\nxgb_clf_sm = grid_xgb_sm.best_estimator_","45fbeba0":"# Oversampled Dataset Case\n\nlog_reg_score_sm = cross_val_score(log_reg_sm, X_train_smote, y_train_smote, cv=5)\nprint('Logistic Regression Cross Validation Score using SMOTE: ', round(log_reg_score_sm.mean() * 100, 2).astype(str) + '%')\n\ntree_score_sm = cross_val_score(tree_clf_sm, X_train_smote, y_train_smote, cv=5)\nprint('DecisionTree Classifier Cross Validation Score using SMOTE', round(tree_score_sm.mean() * 100, 2).astype(str) + '%')\n\nrf_score_sm = cross_val_score(rf_clf_sm, X_train_smote, y_train_smote, cv=5)\nprint('Random Forest Classifier Cross Validation Score using SMOTE', round(rf_score_sm.mean() * 100, 2).astype(str) + '%')\n\nxgb_score_sm = cross_val_score(xgb_clf_sm, X_train_smote, y_train_smote, cv=5)\nprint('XGBoost Classifier Cross Validation Score using SMOTE', round(xgb_score_sm.mean() * 100, 2).astype(str) + '%')\n","cd93b64d":"cv_sm = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve_over(log_reg_sm, tree_clf_sm, rf_clf_sm, xgb_clf_sm, X_train_smote, y_train_smote, (0.75, 1.1), cv=cv_sm, n_jobs=4)","7d570965":"# Predictions for different models\n\nlog_reg_pred_sm = cross_val_predict(log_reg_sm, X_train_smote, y_train_smote, cv=5,method=\"decision_function\")\ntree_pred_sm = cross_val_predict(tree_clf_sm, X_train_smote, y_train_smote, cv=5)\nrf_pred_sm = cross_val_predict(rf_clf_sm, X_train_smote, y_train_smote, cv=5)\nxgb_pred_sm = cross_val_predict(xgb_clf_sm, X_train_smote, y_train_smote, cv=5)\n\n\nprint('Logistic Regression for SMOTE: ', roc_auc_score(y_train_smote, log_reg_pred_sm))\nprint('Decision Tree Classifier for SMOTE: ', roc_auc_score(y_train_smote, tree_pred_sm))\nprint('Random Forest Classifier for SMOTE: ', roc_auc_score(y_train_smote, rf_pred_sm))\nprint('XGBoost Classifier for SMOTE: ', roc_auc_score(y_train_smote, xgb_pred_sm))\n\n\nlog_fpr_sm, log_tpr_sm, log_thresold_sm = roc_curve(y_train_smote, log_reg_pred_sm)\ntree_fpr_sm, tree_tpr_sm, tree_threshold_sm = roc_curve(y_train_smote, tree_pred_sm)\nrf_fpr_sm, rf_tpr_sm, rf_threshold_sm = roc_curve(y_train_smote, rf_pred_sm)\nxgb_fpr_sm, xgb_tpr_sm, xgb_threshold_sm = roc_curve(y_train_smote, xgb_pred_sm)\n","83ac4a5d":"def graph_roc_curve_multiple_sm(log_fpr, log_tpr, tree_fpr, tree_tpr,rf_fpr,\n                             rf_tpr,xgb_fpr, xgb_tpr):\n    plt.figure(figsize=(20,12))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train_smote, log_reg_pred_sm)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train_smote, tree_pred_sm)))\n    plt.plot(rf_fpr, rf_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_train_smote, rf_pred_sm)))\n    plt.plot(xgb_fpr, xgb_tpr, label='XGBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_train_smote, xgb_pred_sm)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n \n \ngraph_roc_curve_multiple_sm(log_fpr_sm, log_tpr_sm, tree_fpr_sm, tree_tpr_sm,rf_fpr_sm,\n                             rf_tpr_sm,xgb_fpr_sm, xgb_tpr_sm)\nplt.show()\n","12fc8c2a":"def xgb_roc_curve_sm(xgb_fpr, xgb_tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('XG Boost ROC Curve', fontsize=16)\n    plt.plot(rf_fpr, rf_tpr, 'b-', linewidth=2)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n    \n    \nxgb_roc_curve_sm(xgb_fpr_sm, xgb_tpr_sm)\nplt.show()","d7b31dd0":"precision_sm, recall_sm, threshold_sm = precision_recall_curve(y_train_smote, rf_pred_sm)\n\nfrom sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred_sm = xgb_clf_sm.predict(X_train_smote)\n\n\n# SMOTE Case\nprint('---' * 45)\nprint('SMOTE Case: \\n')\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train_smote, y_pred_sm)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train_smote, y_pred_sm)))\nprint('Recall Score: {:.2f}'.format(recall_score(y_train_smote, y_pred_sm)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train_smote, y_pred_sm)))\n\nprint('---' * 45)\n\n# How it should look like\nprint('---' * 45)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\nprint('---' * 45)\n","d77b8ab6":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\n\nprint('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n\n# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# best estimator is XG Boost\n\nrand_xgb_clf_sm = RandomizedSearchCV(XGBClassifier(), xgb_params_sm, n_iter=4)\n\n\n# Implementing SMOTE Technique \n# Cross Validating the right way\n# Parameters\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_xgb_clf_sm) # SMOTE happens during Cross Validation not before..\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_xgb_clf_sm.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('---' * 45)","26da8cfd":"labels = ['No Fraud', 'Fraud']\nsmote_prediction = best_est.predict(original_Xtest)\nprint(classification_report(original_ytest, smote_prediction, target_names=labels))","43773bbd":"y_score_sm = best_est.predict_proba(original_Xtest)","3a1a3bf9":"average_precision_sm = average_precision_score(original_ytest, y_score_sm[:,1])\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision_sm))","da831a2f":"fig = plt.figure(figsize=(12,6))\n\nprecision_sm, recall_sm, threshold_sm = precision_recall_curve(original_ytest, y_score_sm[:,1])\n\nplt.step(recall_sm, precision_sm, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall_sm, precision_sm, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('SMOTE Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision_sm), fontsize=16)","8b248f7c":"# SMOTE Technique (OverSampling) After splitting and Cross Validating\nsm = SMOTE(sampling_strategy=0.6, random_state=42)\n# Xsm_train, ysm_train = sm.fit_sample(X_train, y_train)\n\n\n# This will be the data were we are going to \nXsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)","baef15f5":"\n# XG Boost Classifier with SMOTE\nt0 = time.time()\n\nxgb_clf_sm.fit(Xsm_train, ysm_train)\nt1 = time.time()\nprint(\"Fitting SMOTE data took :{} sec\".format(t1 - t0))","ad3d126b":"# best classifier is XG Boost\nclf_sm = xgb_clf_sm\nsplitChar = '('\nalgoName_sm = str(clf_sm).partition(splitChar)[0]\n\ncoefficients_sm  = pd.DataFrame(clf_sm.feature_importances_)\n\ncolumn_df_sm     = pd.DataFrame(sub_sampled_df.columns)\ncoef_sumry_sm    = (pd.merge(coefficients_sm,column_df_sm,left_index= True,\n                          right_index= True, how = \"left\"))\ncoef_sumry_sm.columns = [\"coefficients\",\"features\"]\ncoef_sumry_sm    = coef_sumry_over.sort_values(by = \"coefficients\",ascending = False)\n\ncoef_sumry_sm.head(10)","af0cd28c":"#plot coeffs\ntrace_sm = go.Bar(x = coef_sumry_sm[\"features\"],y = coef_sumry_sm[\"coefficients\"],\n                name = \"coefficients\",\n                marker = dict(color = coef_sumry_sm[\"coefficients\"],\n                              colorscale = \"Picnic\",\n                              line = dict(width = .6,color = \"black\")))\n\ndata_sm = [trace_sm]\nlayout = go.Layout(dict(title = \" Feature Importance \",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"Feature Importance\",\n                                         zerolinewidth=1,\n                                         ticklen=5,\n                                         gridwidth=2\n                                        ),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"percent\",\n                                         zerolinewidth=1,\n                                         ticklen=5,\n                                         gridwidth=2\n                                        ),\n                       )\n                  )\nfig_sm  = go.Figure(data=data_sm,layout=layout)\n\npy.iplot(fig_sm)\n","3b28de55":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom imblearn import over_sampling\n\nada = over_sampling.ADASYN(random_state=0)\nX_train_adasyn, y_train_adasyn = ada.fit_resample(original_Xtrain, original_ytrain)\n# Artificial minority samples and corresponding minority labels from ADASYN are appended\n# below X_train and y_train respectively\n# So to exclusively get the artificial minority samples from ADASYN, we do\nX_train_adasyn_1 = X_train_adasyn[X_train.shape[0]:]\n\nX_train_1 = original_Xtrain[np.where(y_train==1.0)]\nX_train_0 = original_Xtrain[np.where(y_train==0.0)]\n\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [20, 20]\nfig = plt.figure()\n\nplt.subplot(3, 1, 1)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 2)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_adasyn_1[:X_train_1.shape[0], 0], X_train_adasyn_1[:X_train_1.shape[0], 1],\n            label='Artificial ADASYN Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 3)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\nplt.legend()\n","3ec927df":"classifiers_adasyn = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"Random Forest Classifier\": RandomForestClassifier(),\n    \"XGBoost Classifier\": XGBClassifier()\n}\n","156c2a57":"# try building the models and check the score for getting the best model\n\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers_adasyn.items():\n    classifier.fit(X_train_adasyn, y_train_adasyn)\n    training_score = cross_val_score(classifier, X_train_adasyn, y_train_adasyn, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n    ","a658cb1c":"# Logistic Regression \nlog_reg_params_adasyn = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\ngrid_log_reg_adasyn = GridSearchCV(LogisticRegression(), log_reg_params_adasyn)\ngrid_log_reg_adasyn.fit(X_train_adasyn, y_train_adasyn)\n# We automatically get the logistic regression with the best parameters.\nlog_reg_adasyn = grid_log_reg_adasyn.best_estimator_\n\n","6fa0ced2":"# DecisionTree Classifier\ntree_params_adasyn = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree_adasyn = GridSearchCV(DecisionTreeClassifier(), tree_params_adasyn)\ngrid_tree_adasyn.fit(X_train_adasyn, y_train_adasyn)\n\n# tree best estimator\ntree_clf_adasyn = grid_tree_adasyn.best_estimator_","89278fff":"# Random Forest Classifier\nrf_params_adasyn = {\"n_estimators\": [10], \"criterion\": [\"entropy\"], \"max_depth\": [10]}\ngrid_rf_adasyn = GridSearchCV(RandomForestClassifier(), rf_params_adasyn)\ngrid_rf_adasyn.fit(X_train_adasyn, y_train_adasyn)\n\n# Random Forest best estimator\nrf_clf_adasyn = grid_rf_adasyn.best_estimator_\n\n","409c54a7":"# XGBoost Classifier\nxgb_params_adasyn = {\"n_estimators\": [10], \"max_depth\": [10], \"learning_rate\": [0.5,0.7,0.9],\n             \"booster\": [\"gbtree\"], \"objective\":[\"binary:logistic\"]}\ngrid_xgb_adasyn = GridSearchCV(XGBClassifier(), xgb_params_adasyn)\ngrid_xgb_adasyn.fit(X_train_adasyn, y_train_adasyn)\n\n# XGBoost best estimator\nxgb_clf_adasyn = grid_xgb_adasyn.best_estimator_\n\n","c9fed982":"# Oversampled Dataset Case\n\nlog_reg_score_adasyn = cross_val_score(log_reg_adasyn, X_train_adasyn, y_train_adasyn, cv=5)\nprint('Logistic Regression Cross Validation Score using ADASYN: ', round(log_reg_score_adasyn.mean() * 100, 2).astype(str) + '%')\n\ntree_score_adasyn = cross_val_score(tree_clf_adasyn, X_train_adasyn, y_train_adasyn, cv=5)\nprint('DecisionTree Classifier Cross Validation Score using ADASYN', round(tree_score_adasyn.mean() * 100, 2).astype(str) + '%')\n\nrf_score_adasyn = cross_val_score(rf_clf_adasyn, X_train_adasyn, y_train_adasyn, cv=5)\nprint('Random Forest Classifier Cross Validation Score using ADASYN', round(rf_score_adasyn.mean() * 100, 2).astype(str) + '%')\n\nxgb_score_adasyn = cross_val_score(xgb_clf_adasyn, X_train_adasyn, y_train_adasyn, cv=5)\nprint('XGBoost Classifier Cross Validation Score using ADASYN', round(xgb_score_adasyn.mean() * 100, 2).astype(str) + '%')\n","22d30a53":"def plot_learning_curve_adasyn(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,15), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    \n    # Second Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Random Forest Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"XGBoost Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    \n    return plt","2160a53e":"cv_adasyn = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\nplot_learning_curve_adasyn(log_reg_adasyn,tree_clf_adasyn, rf_clf_adasyn, xgb_clf_adasyn, X_train_adasyn, y_train_adasyn, (0.5,1.5), cv=cv_adasyn, n_jobs=4)","13d267d1":"# Predictions for different models\n\nlog_reg_pred_adasyn = cross_val_predict(log_reg_adasyn, X_train_adasyn, y_train_adasyn, cv=5,method=\"decision_function\")\ntree_pred_adasyn = cross_val_predict(tree_clf_adasyn, X_train_adasyn, y_train_adasyn, cv=5)\nrf_pred_adasyn = cross_val_predict(rf_clf_adasyn, X_train_adasyn, y_train_adasyn, cv=5)\nxgb_pred_adasyn = cross_val_predict(xgb_clf_adasyn, X_train_adasyn, y_train_adasyn, cv=5)\n\n\nprint('Logistic Regression ROC-AUC score for ADASYN: ', roc_auc_score(y_train_adasyn, log_reg_pred_adasyn))\nprint('Decision Tree Classifier ROC-AUC score for ADASYN: ', roc_auc_score(y_train_adasyn, tree_pred_adasyn))\nprint('Random Forest Classifier ROC-AUC score for ADASYN: ', roc_auc_score(y_train_adasyn, rf_pred_adasyn))\nprint('XGBoost Classifier ROC-AUC score for ADASN: ', roc_auc_score(y_train_adasyn, xgb_pred_adasyn))\n\n\nlog_fpr_adasyn, log_tpr_adasyn, log_thresold_adasyn = roc_curve(y_train_adasyn, log_reg_pred_adasyn)\ntree_fpr_adasyn, tree_tpr_adasyn, tree_threshold_adasyn = roc_curve(y_train_adasyn, tree_pred_adasyn)\nrf_fpr_adasyn, rf_tpr_adasyn, rf_threshold_adasyn = roc_curve(y_train_adasyn, rf_pred_adasyn)\nxgb_fpr_adasyn, xgb_tpr_adasyn, xgb_threshold_adasyn = roc_curve(y_train_adasyn, xgb_pred_adasyn)\n","76bd35f8":"def graph_roc_curve_multiple_adasyn(log_fpr, log_tpr, tree_fpr, tree_tpr,rf_fpr,\n                             rf_tpr,xgb_fpr, xgb_tpr):\n    plt.figure(figsize=(20,12))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train_adasyn, log_reg_pred_adasyn)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train_adasyn, tree_pred_adasyn)))\n    plt.plot(rf_fpr, rf_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_train_adasyn, rf_pred_adasyn)))\n    plt.plot(xgb_fpr, xgb_tpr, label='XGBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_train_adasyn, xgb_pred_adasyn)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n ","5f6d1fdc":"graph_roc_curve_multiple_adasyn(log_fpr_adasyn, log_tpr_adasyn, tree_fpr_adasyn, tree_tpr_adasyn,rf_fpr_adasyn,\n                             rf_tpr_adasyn,xgb_fpr_adasyn, xgb_tpr_adasyn)\nplt.show()","2336dc6a":"def xgb_roc_curve(xgb_fpr, xgb_tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('XG Boost ROC Curve', fontsize=16)\n    plt.plot(rf_fpr, rf_tpr, 'b-', linewidth=2)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n    \n    \nxgb_roc_curve(xgb_fpr_adasyn, xgb_tpr_adasyn)\nplt.show()","2f68b32c":"precision_adasyn, recall_adasyn, threshold_adasyn = precision_recall_curve(y_train_adasyn, rf_pred_adasyn)\n\nfrom sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred_adasyn = xgb_clf_adasyn.predict(X_train_adasyn)\n\n\n# ADASYN Case\nprint('---' * 45)\nprint('ADASYN Case: \\n')\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train_adasyn, y_pred_adasyn)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train_adasyn, y_pred_adasyn)))\nprint('Recall Score: {:.2f}'.format(recall_score(y_train_adasyn, y_pred_adasyn)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train_adasyn, y_pred_adasyn)))\n\nprint('---' * 45)\n\n# How it should look like\nprint('---' * 45)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\nprint('---' * 45)\n","e7053bfb":"print('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n\n# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# best estimator is XG Boost\n\nrand_xgb_clf_adasyn = RandomizedSearchCV(XGBClassifier(), xgb_params_adasyn, n_iter=4)\n\n\n# Implementing ADASYN Technique \n# Cross Validating the right way\n# Parameters\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(over_sampling.ADASYN(random_state=0), rand_xgb_clf_adasyn) \n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_xgb_clf_adasyn.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('---' * 45)\n","4dfe67e6":"# Print the Precision-Recall graph\nlabels = ['No Fraud', 'Fraud']\nadasyn_prediction = best_est.predict(original_Xtest)\nprint(classification_report(original_ytest, adasyn_prediction, target_names=labels))\n\ny_score_adasyn = best_est.predict_proba(original_Xtest)\n\naverage_precision_adasyn = average_precision_score(original_ytest, y_score_adasyn[:,1])\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision_adasyn))\n  \nfig = plt.figure(figsize=(12,6))\n\nprecision_adasyn, recall_adasyn, threshold_adasyn = precision_recall_curve(original_ytest, y_score_adasyn[:,1])\n\nplt.step(recall_adasyn, precision_adasyn, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall_adasyn, precision_adasyn, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall curve using ADASYN: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision_adasyn), fontsize=16)","bbe01509":"# Print the coefficient summary\n\n# best classifier is XG Boost\nclf_adasyn = xgb_clf_adasyn\nsplitChar = '('\nalgoName_adasyn = str(clf_adasyn).partition(splitChar)[0]\n\ncoefficients_adasyn  = pd.DataFrame(clf_adasyn.feature_importances_)\n\ncolumn_df_adasyn     = pd.DataFrame(sub_sampled_df.columns)\ncoef_sumry_adasyn    = (pd.merge(coefficients_adasyn,column_df_adasyn,left_index= True,\n                          right_index= True, how = \"left\"))\ncoef_sumry_adasyn.columns = [\"coefficients\",\"features\"]\ncoef_sumry_adasyn    = coef_sumry_adasyn.sort_values(by = \"coefficients\",ascending = False)\n\ncoef_sumry_adasyn.head(10)","25f5295d":"#plot coeffs\ntrace_adasyn = go.Bar(x = coef_sumry_adasyn[\"features\"],y = coef_sumry_adasyn[\"coefficients\"],\n                name = \"coefficients\",\n                marker = dict(color = coef_sumry_adasyn[\"coefficients\"],\n                              colorscale = \"Picnic\",\n                              line = dict(width = .6,color = \"black\")))\n\ndata_adasyn = [trace_adasyn]\nlayout = go.Layout(dict(title = \" Feature Importance \",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"Feature Importance\",\n                                         zerolinewidth=1,\n                                         ticklen=5,\n                                         gridwidth=2\n                                        ),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"percent\",\n                                         zerolinewidth=1,\n                                         ticklen=5,\n                                         gridwidth=2\n                                        ),\n                       )\n                  )\nfig_adasyn  = go.Figure(data=data_adasyn,layout=layout)\n\npy.iplot(fig_adasyn)\n","54099c77":"# Final Score in the test set of logistic regression\nfrom sklearn.metrics import accuracy_score\n\n# Logistic Regression with Under-Sampling\ny_pred_latest = log_reg.predict(X_test)\nundersample_score_latest = accuracy_score(y_test, y_pred_latest)\n\n\n\n# XG Boost with ADASYN \ny_pred_adasyn_latest = xgb_clf_adasyn.predict(original_Xtest)\noversample_score_latest = accuracy_score(original_ytest, y_pred_adasyn_latest)\n\n\nd = {'Technique': ['Random Undersampling', 'Oversampling (ADASYN)'], 'Score': [undersample_score_latest, oversample_score_latest]}\nfinal_df = pd.DataFrame(data=d)\n\n# Move column\nscore = final_df['Score']\nfinal_df.drop('Score', axis=1, inplace=True)\nfinal_df.insert(1, 'Score', score)\n\n# Note how high is accuracy score it can be misleading! \nfinal_df","3658d1fb":"## Summary: \n<ul>\n<li> <b> Logistic Regression <\/b> classifier is more accurate than the other 5 classifiers in most cases. <\/li>\n<li><b> GridSearchCV <\/b> is used to determine the paremeters that gives the best predictive score for the classifiers. <\/li>\n<li> Logistic Regression has the best Receiving Operating Characteristic score  (ROC), meaning that LogisticRegression pretty accurately separates <b> fraud <\/b> and <b> non-fraud <\/b> transactions.<\/li>\n<\/ul>\n\n## Learning Curves:\n<ul>\n<li>The <b>wider the  gap<\/b>  between the training score and the cross validation score, the more likely your model is <b>underfitting<\/b>.<\/li>\n<li> If the score is low in both training and cross-validation sets<\/b> this is an indication that our model is <b>underfitting (high bias).<\/b><\/li>\n<li><b> Logistic Regression Classifier<\/b>  shows the best score in both training and cross-validating sets.<\/li>\n<\/ul>\n\n## Top 5 features using Logistic Regression are as follows:\n\n1. V25.\n2. V6.\n3. V27.\n4. V11.\n5. Scaled_time.","d14ba9f5":"## We are going to use the below methods for balanced datasets:\n### 1. Random Oversampling.\n### 2. SMOTE.\n### 3. ADASYN.\n\n### In oversampled balanced datasets, we are not going to deploy Knears & SVC as these methods are computationally very intensive and on average they took more than 8-9 hours to finish. Hence, we have removed them for the methods defined above.","7199148a":"### 1. Random Oversampling.\n\n<li>Random resampling provides a naive technique for rebalancing the class distribution for an imbalanced dataset.<\/li>\n<li>Random oversampling duplicates examples from the minority class in the training dataset and can result in overfitting for some models.<\/li>","512fbf2d":"### Equally Distributing and Correlating:","c7fddd2d":"### No null values.","c39f45b7":"## Exploratory data analysis","fcb56668":"Here we will observe the distribution of our classes","47450b7d":"### Save the original DataFrame.","577a0fbf":"# Credit Card Fraud Detection By Abhinav Pandey\n\nIn this project you will predict fraudulent credit card transactions with the help of Machine learning models. Please import the following libraries to get started.\n\n## Problem Statement\nThe problem statement chosen for this project is to predict fraudulent credit card transactions with the help of machine learning models.\n\n \n\nIn this project, you will analyse customer-level data which has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group. \n\n## Business Problem Overview\n\nFor many banks, retaining high profitable customers is the number one business goal. Banking fraud, however, poses a significant threat to this goal for different banks. In terms of substantial financial losses, trust and credibility, this is a concerning issue to both banks and customers alike.\n\n\nIt has been estimated by Nilson report that by 2020 the banking frauds would account to $30 billion worldwide. With the rise in digital payment channels, the number of fraudulent transactions is also increasing with new and different ways. \n\n \n\nIn the banking industry, credit card fraud detection using machine learning is not just a trend but a necessity for them to put proactive monitoring and fraud prevention mechanisms in place. Machine learning is helping these institutions to reduce time-consuming manual reviews, costly chargebacks and fees, and denials of legitimate transactions.","c3a3a9be":"### Dimensionality Reduction and Clustering: ","4708c501":"## Now, we will do Model building with balanced classes.","30f186d6":"##### Preserve X_test & y_test to evaluate on the test data once you build the model","4ecaa903":"### Splitting the data into train & test data","47c25557":"# This marks the end of this capstone project.","1e958f27":"### Random Under-Sampling:\n\nThis is a technique in which we reduce the dataset in order to create a more balanced dataset. This technique is however not perfect as we run a risk that our classification models will not perform as accurate as we would like to since there is a great deal of information loss (bringing 492 non-fraud transaction from 284,315 non-fraud transaction).","4713c400":"## Project Pipeline\nThe project pipeline can be briefly summarized in the following four steps:\n\n### Data Understanding: \nHere, you need to load the data and understand the features present in it. This would help you choose the features that you will need for your final model.\n\n### Exploratory data analytics (EDA): \nNormally, in this step, you need to perform univariate and bivariate analyses of the data, followed by feature transformations, if necessary. For the current data set, because Gaussian variables are used, you do not need to perform Z-scaling. However, you can check if there is any skewness in the data and try to mitigate it, as it might cause problems during the model-building phase.\n## Train\/Test Split: \nNow you are familiar with the train\/test split, which you can perform in order to check the performance of your models with unseen data. Here, for validation, you can use the k-fold cross-validation method. You need to choose an appropriate k value so that the minority class is correctly represented in the test folds.\n\n## Model-Building\/Hyperparameter Tuning: \nThis is the final step at which you can try different models and fine-tune their hyperparameters until you get the desired level of performance on the given dataset. You should try and see if you get a better model by the various sampling techniques.\n\n## Model Evaluation: \nEvaluate the models using appropriate evaluation metrics. Note that since the data is imbalanced it is is more important to identify which are fraudulent transactions accurately than the non-fraudulent. Choose an appropriate evaluation metric which reflects this business goal.\n\n## Let's begin.","43dcfcf3":"## Comparing the undersampled and best oversampled techniques:","3c632fa4":"### 3. ADASYN\n\nADASYN (Adaptive Synthetic) is an algorithm that generates synthetic data, and its greatest advantages are not copying the same minority data, and generating more data for \u201charder to learn\u201d examples.\n\nThe essential idea of ADASYN is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the ADASYN approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classification decision boundary toward the difficult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across five evaluation metrics.\n\n#### Weaknesses to ADASYN\nThere are two major weaknesses of ADASYN:\n1. For minority examples that are sparsely distributed, each neighbourhood may only contain 1 minority example.\n2. Precision of ADASYN may suffer due to adaptability nature.","46f0d8f1":"## Summary: \n<ul>\n<li> <b> XG Boost <\/b> classifier is more accurate than the other 3 classifiers in most cases. <\/li>\n<li><b> GridSearchCV <\/b> is used to determine the paremeters that gives the best predictive score for the classifiers. <\/li>\n<li> <b> XG Boost <\/b> classifier has the best Receiving Operating Characteristic score  (ROC), meaning that it pretty accurately separates <b> fraud <\/b> and <b> non-fraud <\/b> transactions.<\/li>\n<\/ul>\n\n## Learning Curves:\n<ul>\n<li>The <b>narrower the  gap<\/b>  between the training score and the cross validation score, the more likely your model is <b>overfitting (high variance)<\/b>.<\/li>\n<li> If the score is low in both training and cross-validation sets<\/b> this is an indication that our model is <b>underfitting (high bias).<\/b><\/li>\n<li><b> XG Boost Classifier<\/b>  shows the best score in both training and cross-validating sets.<\/li>\n<\/ul>\n\n## The top 5 features using XG Boost method are as follows:\n\n1. V14.\n2. V4.\n3. V12.\n4. Scaled_amount.\n5. V8.","e0ac9aed":"### Learning Curves for picking up the best model.","9af8cb99":"### Scaling.\nWe will first scale the columns <b>Time<\/b> and <b>Amount <\/b>. We will also  create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, which will help to better understand whether a transaction is a fraud or not.","9f05a817":"### Create a DataFrame with all the scores and the classifiers names.","ef11a8f0":"## Summary: \n<ul>\n<li> <b> XG Boost <\/b> classifier is more accurate than the other 3 classifiers in most cases. <\/li>\n<li><b> GridSearchCV <\/b> is used to determine the paremeters that gives the best predictive score for the classifiers. <\/li>\n<li> <b> XG Boost <\/b> classifier has the best Receiving Operating Characteristic score  (ROC), meaning that it pretty accurately separates <b> fraud <\/b> and <b> non-fraud <\/b> transactions.<\/li>\n<\/ul>\n\n## Learning Curves:\n<ul>\n<li>The <b>narrower the  gap<\/b>  between the training score and the cross validation score, the more likely your model is <b>overfitting (high variance)<\/b>.<\/li>\n<li> If the score is low in both training and cross-validation sets<\/b> this is an indication that our model is <b>underfitting (high bias).<\/b><\/li>\n<li><b> XG Boost Classifier<\/b>  shows the best score in both training and cross-validating sets.<\/li>\n<\/ul>\n\n## The top 5 features using XG Boost method are as follows:\n\n1. V14.\n2. V10.\n3. V12.\n4. V4.\n5. V11.","562a05af":"<h3> Correlation Matrices <\/h3>\nCorrelation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (subsample)  in order for us to see which features have a high positive or negative correlation with regards to fraud transactions.\n\n### Summary and Explanation: \n<ul>\n<li><b>Negative Correlations: <\/b>V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.  <\/li>\n<li> <b> Positive Correlations: <\/b> V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction. <\/li>\n<li> <b>BoxPlots: <\/b>  We will use boxplots to have a better understanding of the distribution of these features in fradulent and non fradulent transactions. <\/li>\n<\/ul>\n\n\n**Note: ** We have to make sure we use the subsample in our correlation matrix or else our correlation matrix will be affected by the high imbalance between our classes. This occurs due to the high class imbalance in the original dataframe.","a60f95e4":"### Classifiers (UnderSampling): \n\nHere, we will use six classifiers to train and test our model. Below are the classifiers:\n1. LogisticRegression.\n2. KNeighborsClassifier.\n3. SVC.\n4. DecisionTreeClassifier.\n5. RandomForestClassifier.\n6. XGBClassifier.","079efbd5":"## Closing Comments:\n\n### Best classifier with unbalanced dataset: LogisticRegression. Top 5 features: V25, V6, V27, V11 & Scaled_time.\n### Best classifier using Random Oversampling: Random Forest. Top 5 features: V14, V10, V4, V11 & V17.\n### Best classifier using SMOTE: XG Boost. Top 5 features: V14, V10, V12, V4 & V11.\n### Best classifier using ADASYN: XG Boost. Top 5 features: V14, V4, V12, Scaled_amount & V8.","1feb0e2e":"## Summary: \n<ul>\n<li> <b> Random Forest <\/b> classifier is more accurate than the other 3 classifiers in most cases. <\/li>\n<li><b> GridSearchCV <\/b> is used to determine the paremeters that gives the best predictive score for the classifiers. <\/li>\n<lRandom Forest> Random Forest has the best Receiving Operating Characteristic score  (ROC), meaning that Random Forest pretty accurately separates <b> fraud <\/b> and <b> non-fraud <\/b> transactions.<\/li>\n<\/ul>\n\n## Learning Curves:\n<ul>\n<li>The <b>narrower the  gap<\/b>  between the training score and the cross validation score, the more likely your model is <b>overfitting (high variance)<\/b>.<\/li>\n<li> If the score is low in both training and cross-validation sets<\/b> this is an indication that our model is <b>underfitting (high bias).<\/b><\/li>\n<li><b> Random Forest Classifier<\/b>  shows the best score in both training and cross-validating sets.<\/li>\n<\/ul>\n\n## The top 5 features using Random Forest method are as follows:\n\n1. V14.\n2. V10.\n3. V4.\n4. V11.\n5. V17.\n\n","7f37192c":"### Let's look at the Logistic Regression Classifier for different scores since it is giving the best result.","08ae8b6d":"### 2. SMOTE.\n\nSMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n\nSpecifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.","cd27eefc":"### Anomaly Detection:"}}