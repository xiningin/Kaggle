{"cell_type":{"f1c12c8a":"code","50a694a9":"code","3035404c":"code","954bf83c":"code","7849a8da":"code","1892fc45":"code","b44b416f":"code","2e981098":"code","86918c65":"code","9c4ba4e3":"code","913982cc":"code","7e734590":"code","a44be5a6":"markdown","2668e771":"markdown","5a6faebd":"markdown","6cf5a09e":"markdown","78072a42":"markdown","aa8f5ff0":"markdown","2a2c2b78":"markdown","fe330739":"markdown"},"source":{"f1c12c8a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50a694a9":"import numpy as np\nimport pandas as pd\n\ntrain=pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\",encoding='latin1')\ntest=pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\",encoding='latin1')","3035404c":"train.head(5)","954bf83c":"test.head(5)","7849a8da":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(train['OriginalTweet'])\nprint(X.shape)","1892fc45":"train['Sentiment'].head(20)","b44b416f":"\n\nsenties = train['Sentiment']\na = list()\nfor i in senties:\n    if (i == \"Extremely Negative\"):\n        a.append(\"Negative\")\n    elif(i == \"Extremely Positive\"):\n        a.append(\"Positive\")\n    else:\n        a.append(i)\n    \n","2e981098":"#print(a)","86918c65":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, a, test_size=0.01)\nlgs = LogisticRegression(max_iter=1000000, solver='lbfgs')\nlgs.fit(X_train, y_train)\nprint(lgs.score(X_test, y_test))","9c4ba4e3":"X_predict = test['OriginalTweet']\nX_predict = vectorizer.transform(X_predict)\n\ny_Predict = lgs.predict(X_predict)","913982cc":"b = list()\nfor i in test['Sentiment']:\n    if (i == \"Extremely Negative\"):\n        b.append(\"Negative\")\n    elif(i == \"Extremely Positive\"):\n        b.append(\"Positive\")\n    else:\n        b.append(i)","7e734590":"k=0\ncool=0\nfor i in y_Predict:\n    #print(i + \"   vs   \")  \n    #print(b[k])\n    #print(\"===========\")\n    if (str(i) == str(b[k])):\n        cool = cool + 1\n    k = k + 1\n    if (k==10000):\n        break\nprint(cool\/k)","a44be5a6":"\u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c 77%","2668e771":"\u0412\u0432\u043e\u0434 \u0434\u0430\u043d\u043d\u044b\u0445\n\u0414\u0430\u043d\u0430 \u0442\u0430\u0431\u043b\u0438\u0446\u0430 \u0441 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f\u043c\u0438 \u0438\u0437 \u0442\u0432\u0438\u0442\u0442\u0435\u0440\u0430. OriginalTweet - \u0442\u0435\u043a\u0441\u0442 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f, \u0430 Sentiment - \u0432\u0440\u0443\u0447\u043d\u0443\u044e \u043f\u0440\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u0430\u044f, \u043c\u0435\u0442\u043a\u0430","5a6faebd":"\u0422.\u043a. \u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u043b\u043e\u0441\u044c 5 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u043c\u0435\u0442\u043e\u043a, \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0434\u0430\u043d\u043d\u043e\u0433\u043e \"\u043a\u043e\u0441\u0442\u044b\u043b\u044f\" \u044f \u0441\u0443\u0437\u0438\u043b \u0447\u0438\u0441\u043b\u043e \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u0434\u043e 3\u0445","6cf5a09e":"\u0442\u043e\u0442 \u0436\u0435 \u0441\u0430\u043c\u044b\u0439 \"\u043a\u043e\u0441\u0442\u044b\u043b\u044c\" \u0447\u0442\u043e\u0431\u044b \u0431\u044b\u043b\u043e 3 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0445 \u043c\u0435\u0442\u043a\u0438","78072a42":"\u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445","aa8f5ff0":"\u041f\u0435\u0440\u0432\u044b\u0435 5 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432","2a2c2b78":"\u041f\u0435\u0440\u0432\u044b\u0435 5 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432","fe330739":"\u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c 80%-82% (\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f)"}}