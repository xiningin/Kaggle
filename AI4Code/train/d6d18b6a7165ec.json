{"cell_type":{"c5042b20":"code","2b05834c":"code","8bcf96cf":"code","5f1c0d43":"code","45488269":"code","a82f3c9e":"code","50b7ec15":"code","ecbcf558":"code","b6f762fe":"code","aba45b1e":"code","41b50100":"code","dc1a5357":"code","067fabc1":"code","5815751e":"code","fbd2673b":"code","41638d06":"code","538ff4aa":"code","e4ed71ed":"code","126097f7":"code","3f72b6e3":"code","7e1c768a":"code","46ac6469":"markdown","e6c829c3":"markdown","05f1d6c8":"markdown","3af056e5":"markdown","36289aa4":"markdown","6b46323a":"markdown"},"source":{"c5042b20":"pwd","2b05834c":"import pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing \nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import cross_validate\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')","8bcf96cf":"DATADIR = '..\/input\/titanic\/'\n\ntrain  = pd.read_csv('{0}train.csv'.format(DATADIR))\ntest   = pd.read_csv('{0}test.csv'.format(DATADIR))","5f1c0d43":"train['Sex'] = train['Sex'].apply(lambda x: 1 if x == 'male' else 0)\ntest['Sex'] = test['Sex'].apply(lambda x: 1 if x == 'male' else 0)","45488269":"def Name_Title_Code(x):\n    if x == 'Mr.':\n        return 1\n    if (x == 'Mrs.') or (x=='Ms.') or (x=='Lady.') or (x == 'Mlle.') or (x =='Mme'):\n        return 2\n    if x == 'Miss':\n        return 3\n    if x == 'Rev.':\n        return 4\n    return 5\n\ntrain['Name_Title'] = train['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\ntest['Name_Title'] = test['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0]) ","a82f3c9e":"def Age_feature(train, test):\n    for i in [train, test]:\n        i['Age_Null_Flag'] = i['Age'].apply(lambda x: 1 if pd.isnull(x) else 0)  \n        data = train.groupby(['Name_Title', 'Pclass'])['Age']\n        i['Age'] = data.transform(lambda x: x.fillna(x.mean()))\n#         i['Age'] = data.transform(lambda x: x.fillna(x.median()))\n    return train, test","50b7ec15":"def Family_feature(train, test):\n    for i in [train, test]:\n        i['Fam_Size'] = np.where((i['SibSp']+i['Parch']) == 0 , 'Solo',\n                           np.where((i['SibSp']+i['Parch']) <= 3,'Nuclear', 'Big'))\n        del i['SibSp']\n        del i['Parch']\n    return train, test ","ecbcf558":"def ticket_grouped(train, test):\n    for i in [train, test]:\n        i['Ticket_Lett'] = i['Ticket'].apply(lambda x: str(x)[0])\n        i['Ticket_Lett'] = i['Ticket_Lett'].apply(lambda x: str(x))\n        i['Ticket_Lett'] = np.where((i['Ticket_Lett']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']), i['Ticket_Lett'],\n                                    np.where((i['Ticket_Lett']).isin(['W', '4', '7', '6', 'L', '5', '8']),\n                                            'Low_ticket', 'Other_ticket'))\n        i['Ticket_Len'] = i['Ticket'].apply(lambda x: len(x))\n        del i['Ticket']\n    return train, test","b6f762fe":"def Cabin_feature(train, test):\n    for i in [train, test]:\n        i['Cabin_Letter'] = i['Cabin'].apply(lambda x: str(x)[0])\n        del i['Cabin']\n    return train, test","aba45b1e":"def cabin_num(train, test):\n    for i in [train, test]:\n        i['Cabin_num1'] = i['Cabin'].apply(lambda x: str(x).split(' ')[-1][1:])\n        i['Cabin_num1'].replace('an', np.NaN, inplace = True)\n        i['Cabin_num1'] = i['Cabin_num1'].apply(lambda x: int(x) if not pd.isnull(x) and x != '' else np.NaN)\n        i['Cabin_num'] = pd.qcut(train['Cabin_num1'],3)\n    train = pd.concat((train, pd.get_dummies(train['Cabin_num'], prefix = 'Cabin_num')), axis = 1)\n    test = pd.concat((test, pd.get_dummies(test['Cabin_num'], prefix = 'Cabin_num')), axis = 1)\n    del train['Cabin_num']\n    del test['Cabin_num']\n    del train['Cabin_num1']\n    del test['Cabin_num1']\n    return train, test","41b50100":"def embarked_impute(train, test):\n    for i in [train, test]:\n        i['Embarked'] = i['Embarked'].fillna('S')\n    return train, test","dc1a5357":"test['Fare'].fillna(train['Fare'].mean(), inplace = True)","067fabc1":"def dummies(train, test, columns = ['Pclass', 'Sex', 'Embarked', 'Ticket_Lett', 'Cabin_Letter', 'Name_Title', 'Fam_Size']):\n    for column in columns:\n        train[column] = train[column].apply(lambda x: str(x))\n        test[column] = test[column].apply(lambda x: str(x))\n        good_cols = [column+'_'+i for i in train[column].unique() if i in test[column].unique()]\n        train = pd.concat((train, pd.get_dummies(train[column], prefix = column)[good_cols]), axis = 1)\n        test = pd.concat((test, pd.get_dummies(test[column], prefix = column)[good_cols]), axis = 1)\n        del train[column]\n        del test[column]\n    return train, test","5815751e":"def drop(train, test, bye = ['PassengerId']):\n    for i in [train, test]:\n        for z in bye:\n            del i[z]\n    return train, test","fbd2673b":"train, test = Age_feature(train, test)\n \ntrain['Name_Title'] = train['Name_Title'].apply(Name_Title_Code)\ntest['Name_Title'] = test['Name_Title'].apply(Name_Title_Code)\ntrain = pd.get_dummies(columns = ['Name_Title'], data = train)\ntest = pd.get_dummies(columns = ['Name_Title'], data = test)\n\ntrain, test = cabin_num(train, test)\n\ntrain, test = Cabin_feature(train, test)\n\ntrain, test = embarked_impute(train, test)\n\ntrain, test = Family_feature(train, test)\n\ntest['Fare'].fillna(train['Fare'].mean(), inplace = True)\n\ntrain, test = ticket_grouped(train, test)\n\ntrain, test = dummies(train, test, columns = ['Pclass', 'Sex', 'Embarked', 'Ticket_Lett', 'Fam_Size','Cabin_Letter'])  \n\ntrain, test = drop(train, test)","41638d06":"train.drop('Name',axis=1,inplace=True)\ntest.drop('Name',axis=1,inplace=True)","538ff4aa":"\n# rf = RandomForestClassifier(max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n# param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10], \"min_samples_split\" : [2, 4, 10, 12, 16], \"n_estimators\": [50, 100, 400, 700, 1000]}\n# gs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n\n# gs = gs.fit(train.iloc[:, 1:], train.iloc[:, 0])\n\n# print(gs.best_score_)\n# print(gs.best_params_) \n","e4ed71ed":"from sklearn.ensemble import RandomForestClassifier\n \nrf = RandomForestClassifier(criterion='gini', \n                             n_estimators=700,\n                             min_samples_split=16,\n                             min_samples_leaf=1,\n                             max_features='auto',\n                             oob_score=True,\n                             random_state=1,\n                             n_jobs=-1) \n\nrf.fit(train.iloc[:, 1:], train.iloc[:, 0])\nprint(\"%.4f\" % rf.oob_score_)","126097f7":"pd.concat((pd.DataFrame(train.iloc[:, 1:].columns, columns = ['variable']), \n           pd.DataFrame(rf.feature_importances_, columns = ['importance'])), \n          axis = 1).sort_values(by='importance', ascending = False)[:20]","3f72b6e3":"submit = pd.read_csv('{0}gender_submission.csv'.format(DATADIR))\nsubmit.set_index('PassengerId',inplace=True)\n\nrf_res =  rf.predict(test)\nsubmit['Survived'] = rf_res\nsubmit['Survived'] = submit['Survived'].apply(int)\nsubmit.to_csv('submit.csv')","7e1c768a":"submit","46ac6469":"## 6. Learning together\n### If you have other techniques or expereriences to improve this PB score, I wish you could share with us. Let's learn together.","e6c829c3":"## 1. Introduction\n\n### In this notebook, we  will show our method which can get a PB score <font color = red > 0.84211 <\/font>. This notebook only uses single model random forest classifier, so there is still room to improve the performance like  using ensemble methods.  \n\n### Because our method based on other great minds and the intuition behind the feature engineering can be found in the following notebooks. Hence, I will omit the details and only provide the codes.\n\n#### 1. Titanic Random Forest: 82.78%\n#### 2. A Journey through Titanic\n#### 3. Titanic Data Science Solutions\n#### 4.Pytanic","05f1d6c8":"## 2. Feature Engineering","3af056e5":"## 5. Submit","36289aa4":"## 4. Model training","6b46323a":"## 2. Load Libraries and Raw Data"}}