{"cell_type":{"4673f8ba":"code","3105ead6":"code","dcbd4b85":"code","3996b692":"code","61704dea":"code","01810be3":"code","0a668bb4":"code","e54ff15c":"code","6970b0d4":"code","4487443b":"code","1a4ead22":"code","1bf67a6f":"code","a444ded7":"code","475fea26":"code","69e38d2f":"code","1c65c9e4":"code","02bce9a4":"code","da43d473":"code","71606096":"code","9628ad5a":"code","27169f22":"code","8d77bff2":"code","24931458":"code","0a1f3f06":"code","8d77a772":"code","ca714a7d":"code","fc1dd640":"code","93bcd75f":"code","949b975e":"code","024d2b32":"code","fc9fef54":"code","c77920c1":"code","7f72a0c2":"code","0685816f":"code","3dd2a54b":"code","c84d927e":"code","645fc1f9":"code","88ee2a8e":"code","79fd5571":"code","1907b688":"code","e44a5633":"code","d34effc8":"code","6e967434":"code","79823a27":"code","70b670f6":"code","bfb89197":"code","2631bb79":"code","1ae88840":"code","33b3a3b2":"code","816b5e07":"code","2cee94aa":"code","4fbcbfbf":"code","f2987460":"code","e5188790":"code","1787f476":"code","8ad70e2b":"code","ae11df11":"code","393e9fb6":"markdown","3ecde3cb":"markdown","cb79030a":"markdown","faf75d3c":"markdown","a995897a":"markdown","c4437f65":"markdown","da8cd762":"markdown","f822857d":"markdown","49b6b7d4":"markdown","ca366134":"markdown"},"source":{"4673f8ba":"# print date and time for given type of representation\ndef date_time(x):\n    if x==1:\n        return 'Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())\n    if x==2:    \n        return 'Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now())\n    if x==3:  \n        return 'Date now: %s' % datetime.datetime.now()\n    if x==4:  \n        return 'Date today: %s' % datetime.date.today() ","3105ead6":"import pandas as pd\nimport sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n","dcbd4b85":"tweet= pd.read_csv(\"..\/input\/twitter-airline-sentiment\/Tweets.csv\")\ntweet.head()","3996b692":"# discover missing data for each column\n(len(tweet)-tweet.count())\/len(tweet)","61704dea":"# remove columns that have high ratio of missing values\n\ntweet.drop(['airline_sentiment_gold', 'negativereason_gold', 'tweet_coord'], axis=1, inplace=True ) ","01810be3":"tweet.airline_sentiment.value_counts()","0a668bb4":"Sentiment_mood = tweet['airline_sentiment'].value_counts()","e54ff15c":"plt.bar([1,2,3],Sentiment_mood)\nplt.xticks([1,2,3], ['negative','neutral','positive'],rotation=45)\nplt.ylabel('Count')\nplt.xlabel('Mood')\nplt.title('Sentiment Moodel count')","6970b0d4":"tweet['airline'].value_counts()","4487443b":"def plot_Airline_sentiment(Airline):\n    df=tweet[tweet['airline']==Airline]\n    count=df['airline_sentiment'].value_counts()\n    Index = [1,2,3]\n    plt.bar(Index,count)\n    plt.xticks(Index,['negative','neutral','positive'])\n    plt.ylabel('Mood Count')\n    plt.xlabel('Mood')\n    plt.title('Count of Sentiment  Moods of '+Airline)","1a4ead22":"plt.figure(1,figsize=(12, 12))\nplt.subplot(231)\nplot_Airline_sentiment('US Airways')\nplt.subplot(232)\nplot_Airline_sentiment('United')\nplt.subplot(233)\nplot_Airline_sentiment('American')\nplt.subplot(234)\nplot_Airline_sentiment('Southwest')\nplt.subplot(235)\nplot_Airline_sentiment('Delta')\nplt.subplot(236)\nplot_Airline_sentiment('Virgin America')","1bf67a6f":"nr_Count=dict(tweet['negativereason'].value_counts(sort=False))","a444ded7":"def nr_Count():\n    df=tweet\n    count=dict(df['negativereason'].value_counts())\n    Unique_reason=list(tweet['negativereason'].unique())\n    Unique_reason=[x for x in Unique_reason if str(x) != 'nan']\n    Reason_frame=pd.DataFrame({'Reasons':Unique_reason})\n    Reason_frame['count']=Reason_frame['Reasons'].apply(lambda x: count[x])\n    return Reason_frame","475fea26":"def plot_reason():\n    df=nr_Count()\n    count=df['count']\n    Index = range(1,(len(df)+1))\n    plt.bar(Index,count)\n    plt.xticks(Index,df['Reasons'],rotation=90)\n    plt.ylabel('Count')\n    plt.xlabel('Reason')\n    plt.title('Count of Reasons ')","69e38d2f":"plot_reason()","1c65c9e4":"# we can see what people say about flights using Wordcloud\nfrom wordcloud import WordCloud,STOPWORDS","02bce9a4":"df=tweet[tweet['airline_sentiment']=='negative']\nwords = ' '.join(df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])","da43d473":"wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)","71606096":"plt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","9628ad5a":"import re\nimport nltk\nfrom nltk.corpus import stopwords","27169f22":"def Cleaning_tweets(tweets):\n    non_letters_removed = re.sub(\"[^a-zA-Z]\", \" \",tweets) \n    words = non_letters_removed.lower().split()                             \n    stop_words = set(stopwords.words(\"english\"))                  \n    stop_words_removed = [w for w in words if not w in stop_words] \n    return ( \" \".join( stop_words_removed ))\n                                                   ","8d77bff2":"tweet['sentiment']=tweet['airline_sentiment'].apply(lambda x: 0 if x=='negative' else 1)","24931458":"tweet['cleaned_tweet']=tweet['text'].apply(lambda x: Cleaning_tweets(x))\ntweet['Tweet_length']=tweet['cleaned_tweet'].apply(lambda x: len(x.split(' ')))\n","0a1f3f06":"tweet.head()","8d77a772":"from sklearn.model_selection import train_test_split","ca714a7d":"# split data to training and testing\ntrain,test = train_test_split(tweet,test_size=0.2,random_state=42)","fc1dd640":"train_data = train.cleaned_tweet.values\ntest_data = test.cleaned_tweet.values","93bcd75f":"vector =CountVectorizer(analyzer = \"word\")\ntrain_features = vector.fit_transform(train_data)\ntest_features= vector.transform(test_data)","949b975e":"##import machine learning classicla models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score","024d2b32":"Classifiers = [\n    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=200),\n    AdaBoostClassifier(),\n    GaussianNB()]","fc9fef54":"# convert data features to array\ntrain_to_array =train_features.toarray()\ntest_to_array = test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['sentiment'])\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(train_to_array,train['sentiment'])\n        pred = fit.predict(test_to_array)\n    accuracy = accuracy_score(pred,test['sentiment'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))    ","c77920c1":"Index = [1,2,3,4,5,6]\nplt.bar(Index,Accuracy)\nplt.xticks(Index, Model,rotation=45)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Accuracies of Models') ","7f72a0c2":"from keras.preprocessing.text import Tokenizer\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.preprocessing import sequence\n\nimport time\nimport datetime\n\n# Packages for modeling\nfrom keras.layers import Embedding, LSTM, Bidirectional\nfrom keras.layers import Input, Dense, Activation, BatchNormalization, Dropout, Flatten\nfrom keras import models\nfrom keras import layers\nfrom keras import regularizers\nimport collections\n\nMAX_LEN = 40\nNUM_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\nVAL_SIZE = 1000  # Size of the validation set\nEPOCHS = 20  # Number of epochs we usually start to train with\nBATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent","0685816f":"data_x = train.cleaned_tweet.values\ndata_y =train['sentiment'].values","3dd2a54b":"data_y.shape","c84d927e":"X_train, X_test, y_train, y_test = train_test_split(train['cleaned_tweet'], train['sentiment'], test_size=0.1, random_state=37)\nprint('# Train data samples:', X_train.shape[0])\nprint('# Test data samples:', X_test.shape[0])","645fc1f9":"X_train.shape","88ee2a8e":"tokenizer = Tokenizer(num_words=NUM_WORDS,\n               filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n',\n               lower=True,\n               split=\" \")\ntokenizer.fit_on_texts(X_train)\n\nprint('Fitted tokenizer on {} documents'.format(tokenizer.document_count))\nprint('{} words in dictionary'.format(tokenizer.num_words))\nprint('Top 5 most common words are:', collections.Counter(tokenizer.word_counts).most_common(5))","79fd5571":"X_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)","1907b688":"def one_hot_seq(seqs, nb_features = NUM_WORDS):\n    ohs = np.zeros((len(seqs), nb_features))\n    for i, s in enumerate(seqs):\n        ohs[i, s] = 1.\n    return ohs\n\nX_train_oh = one_hot_seq(X_train_seq)\nX_test_oh = one_hot_seq(X_test_seq)\n\nprint('\"{}\" is converted into {}'.format(X_train_seq[0], X_train_oh[0]))\nprint('For this example we have {} features with a value of 1.'.format(X_train_oh[0].sum()))","e44a5633":"X_train_oh.shape","d34effc8":"from keras.utils import np_utils\n\n#Convert the labels to One hot encoding vector for softmax for neural network\n\nnum_labels = len(np.unique(y_train))\nY_oh_train = np_utils.to_categorical(y_train, num_labels)\nY_oh_test = np_utils.to_categorical(y_test, num_labels)\nprint(Y_oh_train.shape)","6e967434":"X_train_2, X_valid, y_train_2, y_valid = train_test_split(X_train_oh, Y_oh_train, test_size=0.1, random_state=37)\n\n\nprint('Shape of validation set:',X_train_2.shape)","79823a27":"len(X_train_2[0])","70b670f6":"# CALLbacks\n# Deep Learning Callbacs - Keras\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=1,\n    verbose=1)\n\n\ncallbacks = [reduce_lr]","bfb89197":"def deep_model(model):\n    model.compile(optimizer='rmsprop',\n                  loss = 'categorical_crossentropy',\n                  metrics=['accuracy'])\n    \n    history = model.fit(X_train_2\n                       , y_train_2\n                       , epochs=EPOCHS\n                       , batch_size=BATCH_SIZE\n                       , callbacks=callbacks\n                       , validation_data=(X_valid, y_valid)\n                       , verbose=1)\n    \n    return history","2631bb79":"def Dense_model():\n    model = models.Sequential()\n    model.add(layers.Dense(128, activation='relu', input_shape=(NUM_WORDS,)))\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(2, activation='softmax'))\n    model.summary()\n    return model","1ae88840":"model = Dense_model()\nmodel_history = deep_model(model)","33b3a3b2":"def eval_metric(history, metric_name):\n    metric = history.history[metric_name]\n    val_metric = history.history['val_' + metric_name]\n\n    e = range(1, EPOCHS + 1)\n\n    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n    plt.legend()\n    plt.show()","816b5e07":"eval_metric(model_history, 'accuracy')","2cee94aa":"def plot_performance(history=None, figure_directory=None, ylim_pad=[0, 0]):\n    xlabel = 'Epoch'\n    legends = ['Training', 'Validation']\n\n    plt.figure(figsize=(20, 5))\n\n    y1 = history.history['accuracy']\n    y2 = history.history['val_accuracy']\n\n    min_y = min(min(y1), min(y2))-ylim_pad[0]\n    max_y = max(max(y1), max(y2))+ylim_pad[0]\n\n\n    plt.subplot(121)\n\n    plt.plot(y1)\n    plt.plot(y2)\n\n    plt.title('Model Accuracy\\n'+date_time(1), fontsize=17)\n    plt.xlabel(xlabel, fontsize=15)\n    plt.ylabel('Accuracy', fontsize=15)\n    plt.ylim(min_y, max_y)\n    plt.legend(legends, loc='upper left')\n    plt.grid()\n\n    y1 = history.history['loss']\n    y2 = history.history['val_loss']\n\n    min_y = min(min(y1), min(y2))-ylim_pad[1]\n    max_y = max(max(y1), max(y2))+ylim_pad[1]\n\n\n    plt.subplot(122)\n\n    plt.plot(y1)\n    plt.plot(y2)\n\n    plt.title('Model Loss\\n'+date_time(1), fontsize=17)\n    plt.xlabel(xlabel, fontsize=15)\n    plt.ylabel('Loss', fontsize=15)\n    plt.ylim(min_y, max_y)\n    plt.legend(legends, loc='upper left')\n    plt.grid()\n    if figure_directory:\n        plt.savefig(figure_directory+\"\/history\")\n\n    plt.show()","4fbcbfbf":"def ltsm_model():\n    \n    model = models.Sequential()\n    \n    model.add(Embedding(NUM_WORDS, 100, input_length=MAX_LEN))\n    \n    model.add(LSTM(64, recurrent_dropout=0.2))    \n\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    \n    model.add(Dense(256, activation='relu'))\n    \n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n    \n\n    model.add(Dense(2, activation='softmax'))\n  \n    model.summary()\n    return model","f2987460":"# Modify sequence model to max 25 in length\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\nX_train_deep = sequence.pad_sequences(X_train_seq, maxlen=MAX_LEN)\nX_test_deep = sequence.pad_sequences(X_test_seq, maxlen=MAX_LEN)","e5188790":"# split data for lstm model\nX_train_2, X_valid, y_train_2, y_valid = train_test_split(X_train_deep, Y_oh_train, test_size=0.1, random_state=37)\n\n\nprint('Shape of validation set:',X_train_2.shape)","1787f476":"model = ltsm_model()\nmodel_history = deep_model(model)","8ad70e2b":"plot_performance(history=model_history)","ae11df11":"as shown in figure above there is overfitting so we need more work like preprossessing data and change some model parameters","393e9fb6":"as shown in figure above there is overfitting so we need more work like preprossessing data and change some model parameters","3ecde3cb":"AS shown from graph, there is a bad mood from airlines users,\nbut we need to see which airlines have highest bad mood","cb79030a":"as shown, custmoer service have a bad service","faf75d3c":"as shown in graphs all airlines have bad moods, but in first three data skewed towords negative and \nin last three data more normally distributed ","a995897a":"### Convert data to vectors","c4437f65":"AS we can see RandomForestClassifieris  and AdaBoostClassifieris  do better than others","da8cd762":"# Preprocessing","f822857d":"###  2. Move To deep learning","49b6b7d4":"### Split data to train and validation","ca366134":"# Build LSTM MODEL "}}