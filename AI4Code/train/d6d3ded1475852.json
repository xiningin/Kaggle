{"cell_type":{"d05eec6d":"code","1585bbc1":"code","a867e788":"code","90c3a989":"code","54db3cd6":"code","175d3566":"code","0229ebdf":"code","79be1d1c":"code","1fd240ad":"code","a69b4957":"code","eecb60fd":"code","5a1cc87c":"code","d2ab13c4":"code","fefe7170":"code","4afa0960":"code","210d8b11":"code","963c5e2f":"code","761eaaef":"code","280eff38":"code","6db5be04":"code","db5f131b":"code","e335b152":"code","17784b1a":"markdown","4b5d3600":"markdown","6d77f446":"markdown","72c95eb1":"markdown","e6432b21":"markdown","7a6729f8":"markdown","0c3e64a8":"markdown","d05addc3":"markdown"},"source":{"d05eec6d":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import Image, display\n\nimport keras\nimport tensorflow as tf\n\nfrom keras import layers\nfrom keras.preprocessing import image\nfrom tensorflow.python.keras import backend as K\n\nimport imgaug as ia\nfrom imgaug import augmenters as iaa","1585bbc1":"test_cities = ['berlin', 'bielefeld', 'bonn', 'leverkusen', 'mainz', 'munich']\ntrain_cities = ['aachen', 'bochum', 'bremen', 'cologne', 'darmstadt', 'dusseldorf', 'erfurt', 'hamburg', 'hanover', 'jena', 'krefeld', 'monchengladbach', 'strasbourg', 'stuttgart', 'tubingen', 'ulm', 'weimar', 'zurich']\nval_cities = ['frankfurt', 'lindau', 'munster']","a867e788":"train_img_paths = []\ntrain_ann_paths = []\n\nfor cities in train_cities:\n\n    train_img_dir = \"..\/input\/city-scapes-images\/leftImg8bit\/train\/\" + cities\n    train_ann_dir = \"..\/input\/cityscapes-fine-annotations\/gtFine\/train\/\" + cities\n    \n    \n\n    train_img_paths = train_img_paths + sorted(\n        [\n            os.path.join(train_img_dir, fname)\n            for fname in os.listdir(train_img_dir)\n            if fname.endswith(\"_leftImg8bit.png\")\n        ]\n    )\n    train_ann_paths = train_ann_paths + sorted(\n        [\n            os.path.join(train_ann_dir, fname)\n            for fname in os.listdir(train_ann_dir)\n            if fname.endswith(\"_gtFine_labelIds.png\")\n        ]\n    )\n\nprint(\"Number of train images:\", len(train_img_paths))\n\nprint(\"Number of train annotations:\", len(train_ann_paths))\n\nval_img_paths = []\nval_ann_paths = []\n    \nfor cities in val_cities:\n    val_img_dir = \"..\/input\/city-scapes-images\/leftImg8bit\/val\/\" + cities\n    val_ann_dir = \"..\/input\/cityscapes-fine-annotations\/gtFine\/val\/\" + cities\n    \n\n\n    val_img_paths = val_img_paths + sorted(\n        [\n            os.path.join(val_img_dir, fname)\n            for fname in os.listdir(val_img_dir)\n            if fname.endswith(\"_leftImg8bit.png\")\n        ]\n    )\n    val_ann_paths = val_ann_paths + sorted(\n        [\n            os.path.join(val_ann_dir, fname)\n            for fname in os.listdir(val_ann_dir)\n            if fname.endswith(\"_gtFine_labelIds.png\")\n        ]\n    )\n\nprint(\"Number of val images:\", len(val_img_paths))\n\nprint(\"Number of val annotations:\", len(val_ann_paths))","90c3a989":"display(Image(filename=train_img_paths[1]))\ndisplay(Image(filename=train_ann_paths[1]))","54db3cd6":"img_size = (512, 512)\nnum_classes = 8\nbatch_size = 5\nimgaug_multiplier = 2\nepochs = 50\npatience = 5","175d3566":"# Reducing 32 categories to only 8 categories\n\ncats = {\n 'void': [0, 1, 2, 3, 4, 5, 6],\n 'flat': [7, 8, 9, 10],\n 'construction': [11, 12, 13, 14, 15, 16],\n 'object': [17, 18, 19, 20],\n 'nature': [21, 22],\n 'sky': [23],\n 'human': [24, 25],\n 'vehicle': [26, 27, 28, 29, 30, 31, 32, 33, -1]}\n\ndef convertCats(x):\n    if x in cats['void']:\n        return 0\n    elif x in cats['flat']:\n        return 1\n    elif x in cats['construction']:\n        return 2\n    elif x in cats['object']:\n        return 3\n    elif x in cats['nature']:\n        return 4\n    elif x in cats['sky']:\n        return 5\n    elif x in cats['human']:\n        return 6\n    elif x in cats['vehicle']:\n        return 7\n    \nconvertCats_v = np.vectorize(convertCats)\n\ndef preprocessImg(img):\n    image_matrix = np.expand_dims(img, 2)\n    \n    converted_image = convertCats_v(image_matrix)\n    return converted_image","0229ebdf":"def generateRandomParams(seed):\n    np.random.seed(seed)\n    angle = np.random.randint(26)\n    np.random.seed(seed*2)\n    positive = np.random.randint(2)\n    sigma = np.random.uniform(0, 1)\n    \n    if positive == 0:\n        angle = angle * -1\n        \n    crop = np.random.randint(3)\n    crop = crop \/ 10\n    \n    #print(angle, crop)\n        \n    return angle, crop, sigma","79be1d1c":"class Image(keras.utils.Sequence):\n    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n\n    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.input_img_paths = input_img_paths\n        self.target_img_paths = target_img_paths\n\n    def __len__(self):\n        return len(self.target_img_paths) \/\/ self.batch_size\n\n    def __getitem__(self, idx):\n        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n        i = idx * self.batch_size\n        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n        \n        x = np.zeros((self.batch_size * imgaug_multiplier,) + self.img_size + (3,), dtype=\"float32\")        \n        for j, path in enumerate(batch_input_img_paths):\n            img = image.load_img(path, target_size=self.img_size)\n            x[j] = img\n        y = np.zeros((self.batch_size * imgaug_multiplier,) + self.img_size + (1,), dtype=\"uint8\")\n        \n        for j, path in enumerate(batch_target_img_paths):\n            _img = image.load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n            y[j] = preprocessImg(_img)\n            \n            # Image AUGMENTATION         \n        for mul in range(1, imgaug_multiplier):  \n            for i in range(0, self.batch_size):\n                \n                \n                angle, crop, sigma = generateRandomParams((1 + i) * mul)\n                \n                photo_aug = iaa.Sequential([                        \n                    iaa.Affine(rotate=(angle)),\n                    iaa.Crop(percent=(crop)),\n                    iaa.GaussianBlur(sigma=(0.0, sigma))\n                ])\n                \n                label_aug = iaa.Sequential([                        \n                    iaa.Affine(rotate=(angle)),\n                    iaa.Crop(percent=(crop)),\n                ])\n                \n\n                image_aug = photo_aug(image=x[i])\n                x[batch_size * mul + i] = image_aug\n\n                image_aug = label_aug(image=y[i])\n                y[batch_size * mul + i] = image_aug # END Image AUGMENTATION  \n            \n            \n        return x, y","1fd240ad":"train_seq = Image(\n    batch_size, img_size, train_img_paths, train_ann_paths\n)\nval_seq = Image(\n    batch_size, img_size, val_img_paths, val_ann_paths\n)","a69b4957":"#Quick verif\nassert train_seq[0][0].shape == (batch_size * imgaug_multiplier, *img_size, 3)\nassert train_seq[0][1].shape == (batch_size * imgaug_multiplier, *img_size, 1)","eecb60fd":"from tensorflow.keras import layers\n\n\ndef get_model(img_size, num_classes):\n    inputs = keras.Input(shape=img_size + (3,))\n\n    ### [First half of the network: downsampling inputs] ###\n\n    # Entry block\n    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    previous_block_activation = x  # Set aside residual\n\n    # Blocks 1, 2, 3 are identical apart from the feature depth.\n    for filters in [64, 128, 256]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n        # Project residual\n        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n            previous_block_activation\n        )\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    ### [Second half of the network: upsampling inputs] ###\n\n    for filters in [256, 128, 64, 32]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.UpSampling2D(2)(x)\n\n        # Project residual\n        residual = layers.UpSampling2D(2)(previous_block_activation)\n        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model\n\n\n# Free up RAM in case the model definition cells were run multiple times\nkeras.backend.clear_session()\n\n# Build model\nmodel = get_model(img_size, num_classes)\nmodel.summary()","5a1cc87c":"model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])\n\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"checkpoint.h5\", save_best_only=True),\n    keras.callbacks.EarlyStopping(patience=patience, verbose=1)\n]\n\nepochs = epochs\nhistory = model.fit(train_seq, epochs=epochs, validation_data=val_seq, callbacks=callbacks)","d2ab13c4":"model.save('model.h5')","fefe7170":"plt.plot(history.history['sparse_categorical_accuracy'])\nplt.plot(history.history['val_sparse_categorical_accuracy'])\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\n\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n\nplt.plot( np.argmin(history.history[\"val_loss\"]), np.min(history.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"Lowest loss\")\nplt.plot( np.argmax(history.history[\"val_sparse_categorical_accuracy\"]), np.max(history.history[\"val_sparse_categorical_accuracy\"]), marker=\"x\", color=\"g\", label=\"Highest accuracy\")\n\nplt.show()","4afa0960":"if not os.path.exists('.\/model.h5'):\n    model = keras.models.load_model('..\/input\/model-gpu-v1\/model (6).h5')","210d8b11":"val_seq[0][0][0].shape","963c5e2f":"X = np.empty((1, *img_size + (3,)))\nX[0,] = val_seq[0][0][0]","761eaaef":"res = model.predict(X)","280eff38":"def combineMask(masks):\n    _output = np.empty((512,512) + (1,))\n    _x, _y = 0, 0\n    \n    for x in range(0, masks.shape[0]):\n        for y in range(0, masks.shape[1]):\n                   _target = masks[x][y]\n                   _output[x][y] = np.argmax(_target) \n    return _output","6db5be04":"testing = combineMask(res[0])","db5f131b":"testing.shape","e335b152":"keras.preprocessing.image.array_to_img(\n    testing, data_format=None, scale=True, dtype=None,\n    )","17784b1a":"# Prepare generator","4b5d3600":"# Unet results","6d77f446":"val_preds = model.predict(val_seq)","72c95eb1":"# Prepare data","e6432b21":"# Prepare U-net","7a6729f8":"def display_mask(i):\n    mask = np.argmax(val_preds[i], axis=-1)\n    mask = np.expand_dims(mask, axis=-1)\n    img = tf.keras.preprocessing.image.array_to_img(\n    mask, data_format=None, scale=True, dtype=None,\n    )\n    img = img.resize((500,250))\n    display(img)","0c3e64a8":"### alternative way to show result using batch size\n","d05addc3":"from IPython.display import Image, display\n\nfor i in range(0, 1):\n    display(Image(filename=val_img_paths[i], width = 500))\n    display_mask(i)"}}