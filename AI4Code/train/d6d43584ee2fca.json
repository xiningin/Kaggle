{"cell_type":{"4e2fb5c4":"code","c8530691":"code","5704190b":"code","35b0c702":"code","40df225c":"code","db6aea13":"code","a9c329c5":"code","0680e650":"code","e31648bd":"code","61b879e5":"code","f7f81f65":"code","449ea4ca":"code","98024781":"code","507f73a2":"code","1907b496":"code","906011f0":"code","3edf8328":"code","5e4f4af2":"code","4531e49b":"code","0149ef2d":"code","c382b7f2":"code","b254754f":"code","c2a00964":"code","f985b8bf":"code","58f5a252":"code","2145e2b3":"code","2fcdb703":"code","3cf5bf1c":"code","c719221f":"code","e5c23bc3":"code","908ef4d5":"code","7532bc7f":"code","538b7248":"code","96699a88":"code","d2aae069":"code","db160926":"code","c3228e5b":"code","adb59e03":"code","0062c244":"markdown","eeb560b6":"markdown","56e10486":"markdown","c2a74510":"markdown","f06c7dff":"markdown","86ddee80":"markdown","021db3df":"markdown","b7842792":"markdown","daf5ef46":"markdown","b411e342":"markdown","c79808ac":"markdown","7aad3abc":"markdown","6ed8cd49":"markdown","0a6dd0db":"markdown","96c5a3a1":"markdown","3e24895a":"markdown"},"source":{"4e2fb5c4":"import numpy as np\nimport pandas as pd\n\nfrom pandas import DataFrame,Series\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import r2_score","c8530691":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5704190b":"df = pd.read_csv('\/kaggle\/input\/bank-term-deposit-subscribers\/bank-additional-full.csv', sep = ';')","35b0c702":"df.head()","40df225c":"df.isnull().sum()","db6aea13":"df.shape","a9c329c5":"df.info()","0680e650":"df.describe()","e31648bd":"print('Number of unique values in each column:')\nfor col in df.columns[0:]:\n    print(col,':')\n    print('nunique =', df[col].nunique())\n    print('unique =', df[col].unique())\n    print()","61b879e5":"df['y'] = df['y'].replace({'yes':1,'no':0})","f7f81f65":"# Label Encoding\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","449ea4ca":"cols = df.select_dtypes(object).columns\n\nfor i in cols:\n    df[i] = le.fit_transform(df[i])","98024781":"y = df['y']\nx = df.drop(['y'], axis = 1)","507f73a2":"y.head()","1907b496":"x.head()","906011f0":"# split into train and test\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y,train_size=0.7, random_state=1)","3edf8328":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","5e4f4af2":"# Visualize the Y variable for oversampling check - bi class\n\nsns.countplot(df['y'])\nplt.show()","4531e49b":"term_dep_subs = len(df[df['y'] == 1])\nno_term_dep_subs = len(df[df['y'] == 0])\ntotal = term_dep_subs + no_term_dep_subs\n\nterm_dep_subs = (term_dep_subs \/ total) * 100\nno_term_dep_subs = (no_term_dep_subs \/ total) * 100\n\nprint('term_dep_subs:',term_dep_subs)\nprint('no_term_dep_subs:',no_term_dep_subs)","0149ef2d":"from sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, roc_curve,roc_auc_score,r2_score","c382b7f2":"def disp_confusion_matrix(model, x, y):\n    ypred = model.predict(x)\n    cm = confusion_matrix(y,ypred)\n    ax = sns.heatmap(cm,annot=True,fmt='d')\n\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True Labels')\n    ax.set_title('Confusion Matrix')\n    plt.show()\n    \n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n    accuracy = (tp+tn)\/(tp+fn+fp+tn)\n    precision = tp\/(tp+fp)\n    recall = tp\/(tp+fn)\n    f1 = (2*precision*recall)\/(precision+recall)\n    print('Accuracy =',accuracy)\n    print('Precision =',precision)\n    print('Recall =',recall)\n    print('F1 Score =',f1)","b254754f":"def disp_roc_curve(model, xtest, ytest):\n    yprob = model.predict_proba(xtest)\n    fpr,tpr,threshold = roc_curve(ytest,yprob[:,1])\n    roc_auc = roc_auc_score(ytest,yprob[:,1])\n\n    print('ROC AUC =', roc_auc)\n    plt.figure()\n    lw = 2\n    plt.plot(fpr,tpr,color='darkorange',lw=lw,label='ROC Curve (area = %0.2f)'%roc_auc)\n    plt.plot([0,1],[0,1],color='navy',lw=lw,linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.title('ROC Curve')\n    plt.legend(loc='lower right')\n    plt.show()","c2a00964":"#from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n#from imblearn.over_sampling import SMOTE\n# from imblearn.under_sampling import NearMiss\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n# from collections import Counter\n# from sklearn.model_selection import KFold, StratifiedKFold\nimport scipy.stats as st","f985b8bf":"y = df['y']\nx = df.drop(['y'], axis = 1)","58f5a252":"import statsmodels.api as sm","2145e2b3":"X_sm = x\nX_sm = sm.add_constant(X_sm)\nlm = sm.Logit(y,X_sm).fit()\nlm.summary()","2fcdb703":"# Base Model\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\n\ny_pred = logreg.predict(x_test)\n\nprint('Training set score = {:.3f}'.format(logreg.score(x_train,y_train)))\n\nprint('Test set score = {:.3f}'.format(logreg.score(x_test,y_test)))\n#print(\"R squared: {}\".format(r2_score(y_true=y_test,y_pred=y_pred)))","3cf5bf1c":"print(classification_report(y_test, y_pred))","c719221f":"disp_confusion_matrix(logreg, x_test, y_test)\ndisp_roc_curve(logreg, x_test, y_test)","e5c23bc3":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\ny_pred = dt.predict(x_test)\n\nprint('Training score =', dt.score(x_train, y_train))\nprint('Test score =', dt.score(x_test, y_test))\n#print(\"R squared: {}\".format(r2_score(y_true=y_test,y_pred=y_pred)))","908ef4d5":"print(classification_report(y_test, y_pred))","7532bc7f":"disp_confusion_matrix(dt, x_test, y_test)\ndisp_roc_curve(dt, x_test, y_test)","538b7248":"# knn 5 default\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\n\nprint('Training score =', knn.score(x_train, y_train))\nprint('Test score =', knn.score(x_test, y_test))\n#print(\"R squared: {}\".format(r2_score(y_true=y_test,y_pred=y_pred)))","96699a88":"print(classification_report(y_test, y_pred))","d2aae069":"disp_confusion_matrix(knn, x_test, y_test)\ndisp_roc_curve(knn, x_test, y_test)","db160926":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\ny_pred = nb.predict(x_test)\n\nprint('Training score =', nb.score(x_train, y_train))\nprint('Test score =', nb.score(x_test, y_test))\n#print(\"R squared: {}\".format(r2_score(y_true=y_test,y_pred=y_pred)))","c3228e5b":"print(classification_report(y_test, y_pred))","adb59e03":"disp_confusion_matrix(knn, x_test, y_test)\ndisp_roc_curve(knn, x_test, y_test)","0062c244":"## Logistic Regression","eeb560b6":"job               41188 non-null object\nmarital           41188 non-null object\neducation         41188 non-null object\ndefault           41188 non-null object\nhousing           41188 non-null object\nloan              41188 non-null object\ncontact           41188 non-null object\nmonth             41188 non-null object\nday_of_week       41188 non-null object","56e10486":"## Attribute Information:","c2a74510":"# Data Preprocessing","f06c7dff":"## Understanding the dataset","86ddee80":"# Base Model","021db3df":"## Decision Tree","b7842792":"## KNN","daf5ef46":"- No Missing \/ Null values in the dataset","b411e342":"# Reading the dataset","c79808ac":"# Base Model building ","7aad3abc":"## Naive Bayes","6ed8cd49":"# Importing required libararies","0a6dd0db":"# Classification Evaluation Metrics","96c5a3a1":"Attribute Information:\n------------------------    \n\nInput variables:\n# bank client data:\n1 - age (numeric)\n2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n5 - default: has credit in default? (categorical: 'no','yes','unknown')\n6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n# related with the last contact of the current campaign:\n8 - contact: contact communication type (categorical: 'cellular','telephone')\n9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n# other attributes:\n12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n14 - previous: number of contacts performed before this campaign and for this client (numeric)\n15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n# social and economic context attributes\n16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n20 - nr.employed: number of employees - quarterly indicator (numeric)\n\nOutput variable (desired target):\n21 - y - has the client subscribed a term deposit? (binary: 'yes','no')","3e24895a":"# Feature Engineering:"}}