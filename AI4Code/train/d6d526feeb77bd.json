{"cell_type":{"febb1f8f":"code","6c9e0548":"code","63dcc83a":"code","b7b995fa":"code","45fd4fbc":"code","48be23d2":"code","95657c8c":"code","fcd9b822":"code","65f5435b":"code","55f360d2":"code","8316fc7a":"code","fbad44ee":"code","48e0577b":"code","d7ca2b96":"code","1ae40383":"code","3a893c48":"code","fbdceece":"code","fce771ad":"code","ef5a94a0":"code","a5dce522":"code","aba52bd2":"code","68fd5298":"code","f7573ede":"code","35f8a80c":"code","ada9190a":"code","b066a373":"code","4be8c4e5":"code","76a8d852":"markdown","7498d843":"markdown","ef90a47c":"markdown","d02bba30":"markdown"},"source":{"febb1f8f":"!pip install autokeras","6c9e0548":"import pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nfrom autokeras import StructuredDataRegressor\n\ntrain_path = r'..\/input\/tabular-playground-series-aug-2021\/train.csv'\ntest_path = r'..\/input\/tabular-playground-series-aug-2021\/test.csv'\nsubmission_path = r'..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv'\n\nprint('TF',tf.__version__)\n\nRANDOM_SEED = 69420\nimport os\nimport gc\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything()","63dcc83a":"# From https:\/\/www.kaggle.com\/xhlulu\/ranzcr-efficientnet-tpu-training\ndef auto_select_accelerator():\n    TPU_DETECTED = False\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n        TPU_DETECTED =True\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return strategy, TPU_DETECTED\n\nstrategy, TPU_DETECTED = auto_select_accelerator()\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync","b7b995fa":"KFOLDS = 10\nBATCH_SIZES = 16*KFOLDS\nEPOCHS = 20*KFOLDS","45fd4fbc":"train = pd.read_csv(train_path, index_col=0)\ntrain.head()","48be23d2":"x = gc.collect()","95657c8c":"X = train.drop('loss', axis=1).values\ny = train.loss.values\n\nX.shape, y.shape","fcd9b822":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, shuffle=True)","65f5435b":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nsc = MinMaxScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# from joblib import dump\n# dump(sc, 'Temp\/full_pipe.joblib')","55f360d2":"import tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom sklearn.model_selection import KFold\nimport tensorflow.keras.backend as K\n\ndef make_layer(x, units, dropout_rate):\n    t = tfa.layers.WeightNormalization(tf.keras.layers.Dense(units))(x)\n    t = tf.keras.layers.Dense(units, tf.keras.activations.swish)(x)\n    t = tf.keras.layers.BatchNormalization()(t)\n    t = tf.keras.layers.Dropout(dropout_rate)(t)\n    return t\n\ndef make_model(data, units, dropout_rates):\n    \n    inputs = tf.keras.layers.Input(shape=(data.shape[1],))\n    x = tf.keras.layers.BatchNormalization()(inputs)\n\n    for i in range(len(units)):\n        u = units[i]\n        d = dropout_rates[i]\n        x = make_layer(x, u, d)\n       \n    y = tf.keras.layers.Dense(1, 'linear', name='dense_output')(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=y)\n    model.compile(loss='mse', optimizer='adam', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    return model\n\ndef fit_predict(n_splits, x_train, y_train, units, dropout_rates, epochs, x_test, y_test, verbose, random_state):\n\n    histories = []\n    scores = []\n    y_preds = []\n    \n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    for train_idx, valid_idx in cv.split(x_train, y_train):\n\n        x_train_train = x_train[train_idx]\n        y_train_train = y_train[train_idx]\n        x_train_valid = x_train[valid_idx]\n        y_train_valid = y_train[valid_idx]\n                \n        K.clear_session()\n        \n        with strategy.scope():\n            estimator = make_model(x_train, units, dropout_rates)\n\n        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=2e-5, patience=5,\n                                              verbose=verbose, mode='min', restore_best_weights=True)\n\n        rl = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=3, min_lr=1e-5,\n                                                  mode='min', verbose=verbose)\n\n        history = estimator.fit(\n            x_train_train, y_train_train,\n            batch_size=BATCH_SIZES,\n            epochs=EPOCHS,\n            steps_per_epoch=32,\n            callbacks=[es, rl],\n            validation_data=(x_train_valid, y_train_valid),\n            shuffle=True,\n            verbose=verbose\n        )\n        \n        if x_test is not None:\n            y_part = estimator.predict(x_test)\n            y_preds.append(y_part)\n\n        histories.append(history)\n        scores.append(history.history['val_root_mean_squared_error'][-1])\n    \n    if x_test is not None:\n        y_pred = np.mean(y_preds, axis=0)\n    else:\n        y_pred = None\n\n    score = np.mean(scores)\n    \n    return y_pred, histories, score\n","8316fc7a":"import optuna\ndef objective(trial):\n    \n    n_layers = trial.suggest_int('n_layers', 1, 4)\n    \n    units = []\n    dropout_rates = []\n    \n    for i in range(n_layers):\n        u = trial.suggest_categorical('units_{}'.format(i+1), [1024, 512, 256, 128])\n        units.append(u)\n        r = trial.suggest_loguniform('dropout_rate_{}'.format(i+1), 0.1, 0.5)\n        dropout_rates.append(r)\n    \n    print('Units:', units, \"Dropout rates:\", dropout_rates, \"Layers:\", n_layers)\n    \n    _, _, score = fit_predict(10, X_train, y_train, units, dropout_rates, 50, X_test, y_test, 0, 42)\n    return score","fbad44ee":"study = optuna.create_study(\n    direction='minimize',\n    sampler=optuna.samplers.TPESampler(),\n    study_name='Optuna_NN'\n)","48e0577b":"x = gc.collect()","d7ca2b96":"%%time\nstudy.optimize(\n    objective,\n    timeout=3600*7.5,\n    gc_after_trial=True\n)","1ae40383":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","3a893c48":"optuna.visualization.plot_optimization_history(study)","fbdceece":"params = study.best_trial.params\nparams","fce771ad":"n_layers = params['n_layers']\n    \nunits = []\ndropout_rates = []\n\nfor i in range(n_layers):\n    u = params['units'+f'_{i+1}']\n    units.append(u)\n    r = params['dropout_rate'+f'_{i+1}']\n    dropout_rates.append(r)","ef5a94a0":"model = make_model(X_train, units, dropout_rates)\nmodel.summary()","a5dce522":"%%time\ny_pred, histories, score = fit_predict(10, X_train, y_train, units, dropout_rates, 50, X_test, y_test, 1, 42)","aba52bd2":"print(f\"RMSE of Tuned Neural Network: {score}\")","68fd5298":"test = pd.read_csv(test_path, index_col=0)\ntest.head()","f7573ede":"submission = pd.read_csv(submission_path, index_col=0)\nsubmission.head()","35f8a80c":"test = sc.transform(test)","ada9190a":"submission['loss'] = np.abs(model.predict(test))","b066a373":"submission","4be8c4e5":"submission.to_csv('submission.csv')","76a8d852":"# Train Model","7498d843":"# Optuna Tuning","ef90a47c":"# Imports","d02bba30":"# Submit Model"}}