{"cell_type":{"82652d18":"code","5a179053":"code","50a5c61e":"code","adf388e6":"code","38acdc25":"code","f1c92702":"code","ed2984e7":"code","92424ef5":"code","ce633f2a":"code","7af59ff1":"code","a4cf3c29":"code","8cf95d1d":"code","db4751db":"code","b558d036":"code","4d3c1831":"code","dd8faf68":"code","b16ec4fd":"code","e558dc07":"code","914f2246":"code","c5e27d12":"code","548971b7":"code","7027cec9":"code","c1e96fee":"code","0312bf92":"code","8d2b3bde":"code","e2a3570e":"code","d0816e04":"code","222c750c":"code","55fb7a51":"code","37cac212":"code","e8898825":"code","70552d99":"code","d2514bea":"markdown","5852b482":"markdown","cf67256b":"markdown","76f6f69d":"markdown","be0d7d13":"markdown","226969ca":"markdown","160739eb":"markdown","29f410a0":"markdown","c4ded214":"markdown","a2aefae4":"markdown","a5ab4780":"markdown","b2c191e9":"markdown","c51fe8e3":"markdown","c6321909":"markdown","851a44a4":"markdown","06165018":"markdown","1a3be4a4":"markdown","192a3c3b":"markdown"},"source":{"82652d18":"!pip install miceforest\n{'name':'\u00c1lvaro Riob\u00f3o de Larriva',\n'start_date':'2021\/03\/09'}","5a179053":"#--< main modules >--#\nimport numpy as np \nimport pandas as pd \nimport scipy as sp\nimport statsmodels.api as sm\nimport sklearn\n\n#--< visualization >--#\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotnine as p9\n\n#--< others >--#\nfrom datetime import datetime\nimport time\nfrom IPython.display import display \nimport gc\n\nfor i in locals().copy():\n    try:\n        print(\"%s:\"%eval(i).__name__, eval(i).__version__)\n    except:\n        continue","50a5c61e":"###----------------< START of 'airquality_nb.ipynb'>---------------###","adf388e6":"# DATASET LOADING\nairq = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/shrikumarp\/airquality\/master\/AirQualityUCI.csv\",\n                  delimiter=\";\", decimal=\",\", \n                  usecols=range(15)) # dropna deletes empty lines, we need NAN the -200 values.\nairq = airq.dropna().replace(-200,np.nan) # 1 more fix after\n\nprint('Need to fix columns names a bit: %a\\n'%airq.columns)\nairq.columns = [c[0] for c in airq.columns.str.split(\"(\")]\n\n\n","38acdc25":"# DATA CLEANING & PARAMETERS\n##--<INDEX>--## # TIME_INDEX, drop rest of time-referers\ntry:\n    time_index = pd.Series([datetime.strptime(i, \"%d\/%m\/%Y%H.%M.%S\") for i in airq['Date'] + airq['Time']])\n    airq.insert(0, 'time_index', time_index)\n    airq = airq.drop(['Time','Date'], axis=1)\nexcept Exception as e:\n    print('Ya se ha realizado la operacion anteriormente\\nError:',e)\n\ndate_range = (airq['time_index'].min(), airq['time_index'].max())\nprint(date_range)\n\n##---<DTYPE COLS>---##\nnumeric_cols = airq.select_dtypes(np.float64).columns # all numericals were parsed as np.float64\ndate_cols = ['time_index']                           # time_index from dataset\nsensor_cols = airq.columns[airq.columns.str.startswith('PT08')]\nabundance_cols = ['CO','NMHC','NOx','NO2','C6H6']    # last 'C6H6' is not measured, they are put in order with sensor_cols\nairprops_cols = ['T','RH','AH']                      # air properties\nsensor_refs = [par for par in zip(sensor_cols,abundance_cols[:-1])]\n\n##---<PRINTS>---##\nfor i in airq.isna().sum():\n        print(\"NAN: %5i    p: %.2f \"%(i,i\/len(airq)))\n        \nprint('\\nSum NAN values:%i'%airq.isna().sum().sum())\nprint('\\nNew column names:%a\\n'%airq.columns)\ndisplay(airq.head())","f1c92702":"sensor_refs","ed2984e7":"# IMPUTE MISSING DATA WITH MICEFOREST -> MICE AND RANDOM FOREST GUESSING.\nimport miceforest as mf\n\nairq_amp = mf.ampute_data(airq.reset_index()[numeric_cols], random_state=1994)\n\n# Create kernel\nkds = mf.KernelDataSet(\n  airq_amp,\n  save_all_iterations=True,\n  random_state=1994\n)\n\n# Run the MICE algorithm for 3 iterations\nkds.mice(3)\n\n# Return the completed kernel data\nimputed_data = kds.complete_data()\n\ngc.collect()\n\nprint('Before imputation:%a'%airq.isna().sum())\nprint('Total NAN values after imputation:', imputed_data.isna().sum().sum())\n\nairq[numeric_cols] = imputed_data\n\nairq.set_index('time_index', inplace=True)\n","92424ef5":"# Display plain information, types and main stats.\ndescr = airq.describe()\nnunique_values = airq.apply( lambda x: x.nunique())\n\nairq.info()\nprint('number of unique values:\\n%a'%nunique_values)\ndisplay(descr)\n\nqlabels = ['min','25%', '50%', '75%', 'max']\nqvals = descr.loc[qlabels] # we keep quartile values from df description \nstatslabels = ['count','mean','std']\nstatsvals = descr.loc[statslabels] # also keep mean statistics\n ","ce633f2a":"# CORRPLOT\nmask = np.zeros_like(airq.corr(), dtype=np.bool)\nmask[np.tril_indices_from(mask)] = True\n\nplt.figure(figsize=(20,5))\nsns.heatmap(airq.corr(), mask=np.triu(np.ones_like(airq.corr())), cmap=plt.cm.viridis, annot=True, fmt=\".2f\")","7af59ff1":"# PAIRPLOT\nsns.pairplot(airq[sensor_cols])\nsns.pairplot(airq[abundance_cols])\nsns.pairplot(airq[airprops_cols])","a4cf3c29":"# NOx vs. S5(NOx)\nsns.lmplot(x='PT08.S3',y='NOx', scatter_kws={'color':'grey','alpha':.6}, data=airq,line_kws={'color':'green'})  # Vemos que efectivamente NOx y su sensor tienen correlaci\u00f3n negativa.\nplt.title(\"NOx & sensor S5(NOx)\")","8cf95d1d":"# AH vs. RH\nnormalize = lambda x: (x-x.mean())\/x.std()\n\nfig,axs = plt.subplots(1,3, figsize=(10,5))\nwith plt.style.context('seaborn'):\n    airq.plot(kind='scatter', x='RH', y='AH', ax=axs[0],\n             color=\"red\" )\n    axs[0].set_title('Scatter plot AH\/RH')\n    \n    airq.plot(kind=\"hist\", y='AH', ax=axs[1], \n              bins=50, fill='black', density=True, grid=True, rwidth=0.5)\n    axs[1].set_xlabel('AH')\n    axs[1].set_title('Histogram of AH')\n    \n    airq.plot(kind=\"hist\", y='RH', ax=axs[2], \n              bins=50, fill='black', density=True, grid=True, rwidth=0.5, color=\"green\")\n    axs[2].set_xlabel('RH')\n    axs[2].set_title('Histogram of RH')","db4751db":"# HISTOGRAMS\nfig, axs = plt.subplots(1,2, figsize=(10,5))\nairq.plot(kind=\"hist\", bins=30, density=True, alpha=0.7, ax=axs[0])\naxs[0].set_title('Histogram of numeric variables')\nairq.plot(kind=\"hist\", bins=30, density=True, alpha=0.7, stacked=True, ax=axs[1])\naxs[1].set_title('Histogram of numeric variables (stacked)')","b558d036":"# DENSITYPLOT of normalized dataset.\nfig ,ax = plt.subplots(figsize=(20,10))\nnormalize(airq).plot(kind=\"density\", ax=ax,\n                    style='-*', ms=0.6, cmap=plt.cm.Accent)\nplt.axvline(x=0,**{'marker':'^','color':'black'})","4d3c1831":"# DENSITYPLOT dists of gases:\nfig, axs = plt.subplots(1,2, figsize=(20,5), sharey=True)\nnormalize(airq[abundance_cols]).plot(kind=\"density\", ax=axs[0])\naxs[0].axvline(x=0, color='black', marker=\"|\" ,linestyle=\"--\")\naxs[0].set_title(r'Distributions: Concentration of gas')\n\nnormalize(airq[sensor_cols]).plot(kind=\"density\", ax=axs[1])\naxs[1].axvline(x=0, color='black', marker=\"|\" ,linestyle=\"--\")\naxs[1].set_title(r'Distributions: Sensor measurements of gas')\n\n","dd8faf68":"# AIR PROPERTIES\nrolling_freqs = ['24h', '7d','30d','120d','365d']\nairprops = airq[airprops_cols]\n\nwith plt.style.context('seaborn-notebook'): \n    \n    fig, axs = plt.subplots(1,len(rolling_freqs), figsize=(15,4))   \n    for i,freq in enumerate(rolling_freqs):\n        airprops.rolling(freq).mean().plot(ax=axs[i], grid=True)\n        axs[i].set_title(\"Air_props\\ win:%s\"%freq)\n        labels = axs[i].get_xticks()\n        plt.setp(axs[i].xaxis.get_majorticklabels(), rotation=90 )\n        \n    fig, axs = plt.subplots(1,len(rolling_freqs), figsize=(15,4))\n    for i,freq in enumerate(rolling_freqs): \n        normalize(airprops).rolling(freq).mean().plot(ax=axs[i], grid=True)\n        axs[i].set_title(\"N(0,1) Air_props\\ win:%s\"%freq)\n        plt.setp(axs[i].xaxis.get_majorticklabels(), rotation=90 )\n        \ngc.collect()","b16ec4fd":"hour_fraction_crit_values = []\nfor crit_value in [50,100,150,200]:\n    hour_fraction_crit_values.append(airq['NO2'].where(airq['NO2'] >=crit_value).count()\/len(airq))\nprint(hour_fraction_crit_values)","e558dc07":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error  \n","914f2246":"# DATA\/TARGET definition\ndata = airq.reset_index().drop(['time_index','NO2'], axis=1)\ntarget = airq.reset_index().drop('time_index', axis=1)['NO2']\n\n# TRAIN\/TEST subsets\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=1994)\n\n# SCALER\nrob = RobustScaler()\nX_train = rob.fit_transform(X_train)\nX_test = rob.fit_transform(X_test)","c5e27d12":"# We define a function to evaluate all models in a dictionary.\ndef train_all_models(models_dict, train_now=True):\n    if train_now:\n        for k,m in models_dict.items():\n            start_time = time.time()\n            print('MODEL:%s'%k)\n            m.fit(X_train, y_train)\n            print(\"(model_runtime= %.2f s)\\n\"%(time.time() - start_time))\n    else: \n        print(\"Not training for now\")\n    gc.collect()\n    return models_dict\n\ndef plot_predict_all_models(models_dict):\n    for k,m in models_dict.items():\n        y_pred = m.predict(X_test)\n        print('MODEL :%s\\n'%k, \n              'R^2= %.3f\\n'%r2_score(y_test, y_pred),\n              'MSE= %.3f\\n'%mean_squared_error(y_test, y_pred) , \n              'MAE= %.3f\\n'%mean_absolute_error(y_test, y_pred))\n        fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n        axs[0].set_title(\"Predicted\/Observed\")\n        axs[0].set_xlabel(\"Observed\") ; axs[0].set_ylabel(\"Predicted\")\n        axs[0].scatter(x=y_test, y=y_pred, color=\"g\", marker=\"o\", alpha=0.3)\n\n        axs[1].set_title(\"Error distribution\")\n        axs[1].set_xlabel(\"X\"); axs[1].set_ylabel(\"Y\")\n        axs[1].hist( y_pred-y_test, bins=30, rwidth=0.6, density=True)\n    gc.collect()","548971b7":"%timeit\n# MODELS WITH NO PARAMETERS (VANILLA)\n\nLr = LinearRegression()\nKNr = KNeighborsRegressor()\nDTr = DecisionTreeRegressor()\nRFr = RandomForestRegressor()\nSVr = SVR()\nXGBr = XGBRegressor()\n\nmodels_vanilla = {'Lr':Lr,\n                 'KNr':KNr,\n                 'DTr':DTr,\n                 'RFr':RFr,\n                 'SVr':SVr,\n                 'XGBr':XGBr}\n\n\n\nmodels_vanilla = train_all_models(models_vanilla)","7027cec9":"## EVALUATION VANILLA_MODELS\nplot_predict_all_models(models_vanilla)","c1e96fee":"%timeit\n# MODELS AND KEYWORD PARAMETERS DEFINED: RANDOMIZEDSEARCHCV  # NOTE: THIS IS THE MOST TIME-DEMANDING\nLr_kw = {'fit_intercept':[True,False],\n        'normalize':[True,False],\n        'positive':[True,False]}\nLr_cv = RandomizedSearchCV(Lr, Lr_kw, \n                           cv=4, random_state=1994)\n\nKNr_kw = {'n_neighbors':np.arange(1,13),\n          'weights':['uniform','distance'],\n          'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n          'leaf_size':np.arange(12,30)}\nKNr_cv = RandomizedSearchCV(KNr, KNr_kw, \n                            cv=5, random_state=1994)\n\nDTr_kw = {\"splitter\" : [\"best\",\"random\"], \n        \"min_samples_leaf\" : np.arange(1,9), \n        \"max_features\" : [\"auto\",\"sqrt\",\"log2\", None], \n        \"max_depth\" : [3,4,5,None], \n        \"criterion\" : ['mse',\"mae\",\"poisson\"]}\nDTr_cv = RandomizedSearchCV(DTr, DTr_kw,\n                           cv=5, random_state=1994)\n          \nRFr_kw = {\"n_estimators\" : [50,75,100], \n          \"min_samples_leaf\" : np.arange(1,7, 2), \n          \"max_features\" : [\"auto\",\"sqrt\",\"log2\"], \n          \"max_depth\" : [4,5,6], \n          \"criterion\" : ['mse','mae']}\nRFr_cv = RandomizedSearchCV(RFr, RFr_kw,\n                           cv=4, random_state=1994)\n          \nSVr_kw = {\"kernel\" : [\"linear\",\"poly\",\"rbf\",\"sigmoid\"],\n          \"degree\": [2, 3, 4],\n          \"gamma\" : [\"scale\",\"auto\"]\n         }\nSVr_cv = RandomizedSearchCV(SVr, SVr_kw,\n                           cv=5, random_state=1994)\n          \nXGBr_kw = {'n_estimators' : [100, 500, 1000],\n    'max_depth' : [2, 3, 5, 10],\n    'learning_rate' : [0.05, 0.1, 0.15],\n    'min_child_weight' : [1, 2, 3, 4],\n    'booster' : ['gbtree','gblinear'],\n    'base_score' : [0.25, 0.5, 0.75, 1]}\nXGBr_cv = RandomizedSearchCV(XGBr, XGBr_kw,\n                            cv=3, n_iter=5, \n                            scoring = 'neg_mean_absolute_error',\n                            n_jobs = -1,\n                            verbose = 5, \n                            return_train_score = True,\n                            random_state=1994)\n\n\nmodels_CV = {'Lr':Lr_cv,\n            'KNr':KNr_cv,\n            'DTr':DTr_cv,\n            'RFr':RFr_cv,\n            'SVr':SVr_cv,\n            'XGBr':XGBr_cv}\n\nmodels_CV = train_all_models(models_CV, train_now=True)\n        ","0312bf92":"# PRINT BEST_PARAMS FOUND BY RANDOM SEARCH\nfor k,v in models_CV.items():\n    print(\"Model:%s\"%k,\"\\nBest_parameters:%a\"%v.best_params_)\n    print('\\n')","8d2b3bde":"%timeit\n### MODELS AFTER HYPERPARAMETERS TUNING: IMPROVED\nLr_imp = LinearRegression(**models_CV['Lr'].best_params_) \nKNr_imp = KNeighborsRegressor(**models_CV['KNr'].best_params_)\nDTr_imp = DecisionTreeRegressor(**models_CV['DTr'].best_params_)\nRFr_imp = RandomForestRegressor(**models_CV['RFr'].best_params_)\nSVr_imp = SVR(**models_CV['SVr'].best_params_)\nXGBr_imp = XGBRegressor(**models_CV['XGBr'].best_params_)\n\nmodels_improved = {\"Lr\" : Lr_imp,\n                  \"KNr\" : KNr_imp,\n                  \"DTr\" : DTr_imp,\n                  \"RFr\" : RFr_imp,\n                  \"SVr\" : SVr_imp,\n                  \"XGBr\" : XGBr_imp}\n\nmodels_improved = train_all_models(models_improved)","e2a3570e":"## EVALUATION CV_MODELS\nplot_predict_all_models(models_improved)","d0816e04":"%timeit\n### BAGGING ENSEMBLE TO ALL METHODS\nLr_bag = BaggingRegressor(LinearRegression(**models_CV['Lr'].best_params_)) \nKNr_bag = BaggingRegressor(KNeighborsRegressor(**models_CV['KNr'].best_params_))\nDTr_bag = BaggingRegressor(DecisionTreeRegressor(**models_CV['DTr'].best_params_))\nRFr_bag = BaggingRegressor(RandomForestRegressor(**models_CV['RFr'].best_params_))\nSVr_bag = BaggingRegressor(SVR(**models_CV['SVr'].best_params_))\nXGBr_bag = BaggingRegressor(XGBRegressor(**models_CV['XGBr'].best_params_))\n\n\nmodels_bag = {\"Lr\" : Lr_bag,\n             \"KNr\" : KNr_bag,\n             \"DTr\" : DTr_bag,\n             \"RFr\" : RFr_bag,\n             \"SVr\" : SVr_bag,\n             \"XGBr\" : XGBr_bag}\n\n# TRAIN\nmodels_bag = train_all_models(models_bag, train_now=True)","222c750c":"## EVALUATION BAG_MODELS\nplot_predict_all_models(models_bag)\n\n","55fb7a51":"%timeit\n# MAIN RESULTS INTO DATAFRAMES\nalgorithms = ['Linear Regression',\n            'K-Nearest Neighbour Regressor',\n            'Decision Tree Regressor', \n            'Random Forest Regressor',\n            'Support Vector Machine Regressor',\n            'XGBoost']\n\nvanilla_results = pd.DataFrame( {\"Algorithm\": algorithms,\n                          \"Train_score\":[m.score(X_train,y_train) for m in models_vanilla.values()],\n                          \"Test_score\":[m.score(X_test,y_test) for m in models_vanilla.values()]\n                                }).sort_values('Test_score', ascending=False)\n\nimproved_results = pd.DataFrame({\"Algorithm\": algorithms,\n                          \"Train_score\":[m.score(X_train,y_train) for m in models_improved.values()],\n                          \"Test_score\":[m.score(X_test,y_test) for m in models_improved.values()]\n                                }).sort_values('Test_score', ascending=False)\n\nbag_results = pd.DataFrame({\"Algorithm\": algorithms,\n                          \"Train_score\":[m.score(X_train,y_train) for m in models_bag.values()],\n                          \"Test_score\":[m.score(X_test,y_test) for m in models_bag.values()]\n                               }).sort_values('Test_score', ascending=False)\n\ndisplay(\"VANILLA:\", vanilla_results)\ndisplay(\"IMPROVED:\", improved_results)\ndisplay(\"BAGGING:\", bag_results)","37cac212":"# EXPORT CLEANED AND IMPUTED DATASET TO CSV (for production)\nairq.to_csv(\"airq.csv\", header=True)\n\n# EXPORTS TO CSV\nvanilla_results.to_csv(\"airq_vanilla_results.csv\")\nimproved_results.to_csv(\"airq_improved_results.csv\")\nbag_results.to_csv(\"airq_bagging_results.csv\")\n\n# EXPORT TO PICKLE\nimport pickle\nmodel_filename = 'airq_xgboost_bag.sav'\npickle.dump(models_bag['XGBr'], open(model_filename, 'wb'))\n\n","e8898825":"# FEATURE IMPORTANCES\nfeature_importances = pd.Series(models_improved['XGBr'].feature_importances_).sort_values(ascending=False)\nfeature_importances.index = feature_importances.index.map({ i:col for i,col in enumerate(data.columns)})\ndisplay(feature_importances)","70552d99":"###----------------< END of 'airquality_nb.ipynb'>---------------###","d2514bea":"Vemos que nuestros modelos 'vanilla' mejor parados han sido:\n- Random Forest: $R^2=0.886$\n- XGBoost: $R^2=0.869$\n- K-Nearest Neighbours: $R^2=0.859$","5852b482":"# AIRQUALITY DATASET EXPLORATION","cf67256b":"Veremos ahora las propiedades del aire (Temperature, Relative Humidty, Absolute Humidity). Investigamos \u00e9sto un poco m\u00e1s en ventanas temporales.","76f6f69d":"## <ins>*CONCLUSIONES*:<\/ins>\n\n- 1. AN\u00c1LISIS EXPLORATORIO DE DATOS:\n\n    - La correlaci\u00f3n de Pearson entre variables indica lo siguiente: S1(CO) y S2(NMCH) son buenos predictores lineales de sus concentraciones, S3(NOx) est\u00e1 anticorrelacionado con su variable, sin embargo, la transformaci\u00f3n -np.log10(S3) llega a correlacionar 0.75 con NOx, S4 no correlaciona con NO2.\n    \n    - Los histogramas indican que las concentraciones siguen de forma m\u00e1s o menos diferenciada una distribuci\u00f3n con dos jorobas, pudi\u00e9ndose ajustar por dos lorentzianas diferentes, con un pico de aire limpio y otro de aire poluto. Los sensores y su respuesta promediada cada hora dan poca cuenta de este comportamiento, y se ajustan mejor en el caso de las concentraciones que no tienen picos muy altos y que pueden ajustarse a una distribuci\u00f3n normal.\n    \n    - Las propiedades del aire (Temperatura, Humedad relativa y Humedad absoluta) no son buenos predictores de las dem\u00e1s variables, ni siquiera se correlacionan demasiado entre s\u00ed. La temperatura y humedad absoluta se correlacionan positivamente, y \u00e9stas negativamente con la humedad relativa.\n    \n    - Las propiedades del aire son muy cambiantes y siguen ciclos establecidos. Probablemente los ciclos de estacionalidad y tendencia relacionados con la serie temporal sean los que expliquen la variabilidad en las propiedades del aire. Por ejemplo, si estamos en verano la humedad absoluta, relativa y la temperatura ser\u00e1n consistentemente diferentes a las del invierno. Hemos distinguido visualmente 3 frecuencias para los datos (1-2 dias, 1-2 semanas, 1a\u00f1o), lo cual podr\u00eda investigarse con modelos ARIMA.\n    \n- 2. BUSCANDO ALGORITMOS CANDIDATOS:\n\n\u00c9ste dataset no tiene un target pre-establecido. Sin embargo, el S4 no correlaciona con la concentraci\u00f3n **NO2** para el cual est\u00e1 dise\u00f1ado. Siendo el di\u00f3xido de nitr\u00f3geno un gas altamente peligroso para el ser humano y proveniente de veh\u00edculos, plantas el\u00e9ctricas, emisi\u00f3n industrial, construcci\u00f3n, y en definitiva de combustibles f\u00f3siles, debemos intentar medir y predecir los valores de \u00e9ste gas y que se sit\u00faen en un rango razonable. \n    \n    - (Bueno: 0-50, Moderado: 50-100, Peligroso para grupos sensibles: 100-150, Insano: 150-200). \n    - Nuestros datos indican lo siguiente:      Mean=111.4 , Max=340\n\nPor tanto, hemos definido nuestro target como 'NO2', separando el resto de datos (de los cuales hemos quitado tambi\u00e9n el tiempo).\n\nHemos estandarizado nuestros datos con un escalador de mediana y rango intercuartil para minimizar el impacto de valores extremos. Luego, hemos separado nuestros datos en 'train\/test' (sin validaci\u00f3n cruzada para una mayor simplicidad), con una fracci\u00f3n de test del 30%.\n\nHemos usado los siguientes y famosos algoritmos de sklearn para problemas de regresi\u00f3n:\n\n    - LinearRegression(): Modelo m\u00e1s simple, ajusta los coeficientes por medio de minimizar diferencias cuadr\u00e1ticas.\n    - KNeighborsRegressor(): Modelo sencillo que, una vez se establece k (el n\u00famero de predictores m\u00e1ximo), nos dar\u00e1 la importancia de predictores en torno a la variable de regresi\u00f3n.\n    - DecisionTreeRegressor(): Modelo sencillo que penaliza la distancia de nuestros datos en relaci\u00f3n a valores de prueba , construye un \u00e1rbol de decisi\u00f3n y permite realizar una regresi\u00f3n. Permite ver la importancia de predictores.\n    - RandomForestRegressor(): Modelo mejorado basado en \u00e1rboles de decisi\u00f3n que permite promediar una muestra significativa de ellos, sus resultados, y ver la importancia de predictores.\n    - SVR(): Modelo de m\u00e1quinas de soporte vectorial ('Support Vector Machines'), que busca hacer la mejor divisi\u00f3n de nuestros datos mediante hiperplanos que los separen.\n    - XGBRegressor(): Modelo complejo y \u00f3ptimo ('eXtreme Gradient Boosting') basado en \u00e1rboles de decisi\u00f3n, optimizaci\u00f3n del descenso del gradiente y una randomizazi\u00f3n de par\u00e1metros optimizada, entre otros.\n\nHemos usado 'RandomizedSearchCV' para buscar y quedarnos con los mejores hiperpar\u00e1metros de nuestros algoritmos. \n\n**NOTA: Hemos evaluado el $R^2$ (score) del modelo y otras m\u00e9tricas, tambi\u00e9n hemos dibujado el gr\u00e1fico de dispersi\u00f3n de la predicci\u00f3n respecto al objetivo, y el histograma del error asociado. \u00c9sto lo hemos hecho para todos los modelos.**\n\n- 3. EVALUACI\u00d3N Y MEJORAS PARA \u00c9STOS ALGORITMOS:\n\nUna vez buscado los mejores par\u00e1metros para nuestros modelos, hemos evaluado nuestros modelos con \u00e9stos, lo que hemos llamado modelos \"improved\".\n\nLuego, hemos usado un m\u00e9todo de ensamblado('BaggingRegressor') con los originales para mejorar nuestros modelos con par\u00e1metros optimizados, lo que hemos llamado modelos \"bagging\".\n\nLos algoritmos principales evaluados en test, tanto para los modelos \"improved\" como \"bagging\" nos dan como favoritos:\n    1. XGBoost\n    2. K-Nearest Neighbours\n    3. Random Forest\n\nSeg\u00fan nuestro mejor modelo (XGBoost mejorado en hiperpar\u00e1metros), que adem\u00e1s nos permite estimar la importancia de los predictores en la variable objetivo, ir\u00edan por orden:\n\nNOx        0.656528 \\\nAH         0.082007 \\\nCO         0.055414 \\\nRH         0.045607 \\\nPT08.S2    0.035066 \n\nLo cual quiere decir que pr\u00e1cticamente ninguno de nuestros sensores es \u00fatil para predecir la concentraci\u00f3n de NO2.","be0d7d13":"## 2. FINDING CANDIDATE ALGORITHMS","226969ca":"Podemos ver que el la referencia de 'NOx' resulta ser el mejor predictor, de lejos los sigue la humedad absoluta \"AH\", el gas \"CO\" y la humedad relativa (\"RH\"). El sensor de \"NHMC\" (PT08.S2) los sigue en importancia.\n\nComo adelant\u00e1bamos por el mapa de correlaci\u00f3n, el sensor asociado a \"NO2\" tiene pobre relevancia como predictor del mismo.","160739eb":"\u00c9ste dataset no tiene un target pre-establecido. Sin embargo, el S4 no correlaciona con la concentraci\u00f3n **NO2** para el cual est\u00e1 dise\u00f1ado. Siendo el di\u00f3xido de nitr\u00f3geno un gas peligroso para el ser humano y proveniente de veh\u00edculos, plantas el\u00e9ctricas, emisi\u00f3n industrial, construcci\u00f3n, y en definitiva de combustibles f\u00f3siles, debemos intentar medir y predecir la concentraci\u00f3n del gas y pretender que se sit\u00fae en un rango razonable. \n    - (Bueno: 0-50, Moderado: 50-100, Peligroso para grupos sensibles: 100-150, Insano: 150-200). \n    - Mean: 111.4 , Max: 340\nSeg\u00fan el output de la celda anterior, aprox. un ~21% del tiempo el nivel se sit\u00faa por encima de la calificaci\u00f3n de Peligroso, y un ~4% de \u00e9ste en el nivel Insano. \u00c9sto podr\u00eda afectar gravemente a la poblaci\u00f3n que viva o transite por \u00e9ste lugar a lo largo del d\u00eda. Por ello, **NO2 ser\u00e1 nuestra variable objetivo en este estudio**, aunque tambi\u00e9n podr\u00eda estudiarse la concentraci\u00f3n de **NOx**, otro gas t\u00f3xico y peligroso.","29f410a0":"### REFERENCE LINKS:\n- [UCI ML Airquality Dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Air+Quality) : Descripci\u00f3n de los datos\n- [AirQualityUCI.csv](https:\/\/github.com\/shrikumarp\/airquality\/blob\/master\/AirQualityUCI.csv) : Repositorio con los datos en CSV. Usaremos la versi\u00f3n raw para leer desde GitHub.\n\n\n*Columns:*\nIndex(['Date', 'Time', 'CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)',\n       'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)',\n       'PT08.S5(O3)', 'T', 'RH', 'AH'],\n      dtype='object')\n      \n**Data Set Information:**\n\nThe dataset contains 9358 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an Air Quality Chemical Multisensor Device. The device was located on the field in a significantly polluted area, at road level,within an Italian city. Data were recorded from March 2004 to February 2005 (one year)representing the longest freely available recordings of on field deployed air quality chemical sensor devices responses. Ground Truth hourly averaged concentrations for CO, Non Metanic Hydrocarbons, Benzene, Total Nitrogen Oxides (NOx) and Nitrogen Dioxide (NO2) and were provided by a co-located reference certified analyzer. Evidences of cross-sensitivities as well as both concept and sensor drifts are present as described in De Vito et al., Sens. And Act. B, Vol. 129,2,2008 (citation required) eventually affecting sensors concentration estimation capabilities. Missing values are tagged with -200 value.\nThis dataset can be used exclusively for research purposes. Commercial purposes are fully excluded.\n\n**Attribute Information:**\n\n0. Date (DD\/MM\/YYYY)\n1. Time (HH.MM.SS)\n\n2. True hourly averaged concentration CO in mg\/m^3 (reference analyzer)\n3. PT08.S1 (tin oxide) hourly averaged sensor response (nominally CO targeted)\n\n4. True hourly averaged overall Non Metanic HydroCarbons concentration in microg\/m^3 (reference analyzer)\n> 5. True hourly averaged Benzene concentration in microg\/m^3 (reference analyzer)\n6. PT08.S2 (titania) hourly averaged sensor response (nominally NMHC targeted)\n\n7. True hourly averaged NOx concentration in ppb (reference analyzer)\n8. PT08.S3 (tungsten oxide) hourly averaged sensor response (nominally NOx targeted)\n\n9. True hourly averaged NO2 concentration in microg\/m^3 (reference analyzer)\n10. PT08.S4 (tungsten oxide) hourly averaged sensor response (nominally NO2 targeted)\n\n> 11. PT08.S5 (indium oxide) hourly averaged sensor response (nominally O3 targeted)\n\n12. Temperature in \u00c2\u00b0C\n13. Relative Humidity (%)\n14. AH Absolute Humidity ","c4ded214":"## 0. LOAD DATASET AND PARAMETERS","a2aefae4":"Podemos ver informaci\u00f3n interesante en nuestra matriz de correlaci\u00f3n. Comentaremos la respuesta del sensor frente a la concentraci\u00f3n que pretende medir.\n\n- PT08.S1(C0)\/C0:     0.88 (BUENA)\n- PT08.S2(NMHC)\/NMHC: 0.90 (BUENA)\n- PT08.S3(NOx)\/NOx:  -0.66 (ANTICORR)\n- PT08.S4(NO2)\/NO2:   0.19  (POBRE)\n- PT08.S5(O3): Buena correlaci\u00f3n en torno a las dem\u00e1s concentraciones y sensores, no hay referencia de O3. No correlaciona con AH y poco con RH y T.\n- C6H6: Buena correlaci\u00f3n en torno a las dem\u00e1s concentraciones y sensores, no hay sensor de C6H6. No correlaciona con las propiedades del aire (T,RH,AH).\n- Las propiedades del aire no son buenos predictores de las dem\u00e1s variables, ni siquiera se correlacionan entre s\u00ed.","a5ab4780":"> Hemos escogido ventanas temporales para 1 dia,1 semana (7d), 1 mes (30d), 4 meses (120d), 1 a\u00f1o (365d). Se puede observar que nuestros datos son insuficientes para mostrar un ciclo estacional completo. Se estima que hay un ciclo de alrededor de 1 a\u00f1o, pero como tenemos una muestra temporal escasa y afectada gravemente por la tendencia positiva, no podemos concluirlo a simple vista. \n\nHabr\u00eda que realizar un modelo de autoregresi\u00f3n y media m\u00f3vil **(ARIMA)** para poder estimar la tendencia y estacionalidad de una manera m\u00e1s rigurosa. Se pueden valorar a simple vista 3 frecuencias naturales de nuestros datos:\n\n- ~ 1-2 semanas\n- ~ 1 mes\n- ~ 1 a\u00f1o\n    ","b2c191e9":"## 1. EXPLORATORY DATA ANALYSIS (EDA)","c51fe8e3":"Por un estudio similar [Fig 3., pag 1524](https:\/\/www.researchgate.net\/publication\/311459930_Summertime_ambient_ammonia_and_its_effects_on_ammonium_aerosol_in_urban_Beijing_China) realizado en China, vemos que han aproximado las curvas de concentraci\u00f3n de gases a dos Lorentzianas, una para el aire limpio y otra para el contaminado, con buenos resultados. De hecho, se pueden ver dos picos propios de las dos distribuciones en la mayor\u00eda de gr\u00e1ficas de este tipo. Un ajuste de estas curvas se expera que d\u00e9 como resultado los picos (~ medias de la distribuci\u00f3n) en un aire limpio y otro contaminado.\n\n- En los gr\u00e1ficos de sensores, se aprecia una correlaci\u00f3n positiva entre todos ellos, menos en el caso del S3, que anticorrelaciona con los dem\u00e1s.\n\n- En los gr\u00e1ficos de concentraciones de gases, se aprecia una correlaci\u00f3n positiva y clara entre todos ellos.\n\n- En los gr\u00e1ficos de propiedades del aire, se aprecia una correlaci\u00f3n positiva, aunque tambi\u00e9n una dependencia entre la humedad relativa y la absoluta.","c6321909":"Para la optimizaci\u00f3n de nuestros hiperpar\u00e1metros, usaremos \"RandomizedSearchCV\" de sklearn para quedarnos con la mejor combinaci\u00f3n posible de hiperpar\u00e1metros para cada algoritmo.\n\nExpondremos gr\u00e1ficos y los resultados principales de $R^2$ de nuestros modelos con los datos. \n\nGuardaremos los resultados de los modelos anteriores ('train_accuracy','test_accuracy') en un DataFrame y exportaremos a csv. Exportaremos tambi\u00e9n el mejor modelo con formato 'pickle'.","851a44a4":"En el gr\u00e1fico anterior normalizado, solemos ver mayormente poblaciones sesgadas a la izquierda de su media (centrada en 0). Vemos algunas distribuciones bimodales, pero sobre todo se pueden  observar exponenciales negativas, en teor\u00eda correspondientes a la referencia de las concentraciones de los gases que estamos midiendo. La ley que describe el comportamiento a grandes rasgos de la concentraci\u00f3n de un gas es: $\\frac{dN}{dt} = -\\lambda N \\rightarrow N(t)=N_0 e^{-t\/\\tau}$, que nos dice que la concentraci\u00f3n disminuye proporcionalmente a la concentraci\u00f3n en ese momento, por lo que esperamos que estas distribuciones sean exponenciales negativas en el tiempo ($\\tau$ es el tiempo de vida medio, por el cual la concentraci\u00f3n disminuye en $1\/e$ de su valor original). Podr\u00edamos m\u00e1s adelante ajustar a las generalizables funciones gamma.","06165018":"## 3. EVALUATION AND IMPROVEMENTS FOR THOSE ALGORITHMS","1a3be4a4":"Debemos estandarizar nuestro dataset, es decir, convertir nuestros datos a rangos y dispersi\u00f3n similares. Usaremos el RobustScaler(), puesto que nos ayudar\u00e1 a minimizar el impacto de los outliers sustrayendo la mediana y dividiendo por el rango intercuartil de nuestros datos.\n\nLa variable target ser\u00e1 **NO2** como se tratar\u00e1 de justificar un poco m\u00e1s adelante.\n\nPara la b\u00fasqueda de algoritmos candidatos, usaremos los siguientes para el problema de regresi\u00f3n:\n\n- LinearRegression()\n- KNeighborsRegressor()\n- DecisionTreeRegressor()\n- RandomForestRegressor()\n- SVR()\n- XGBRegressor()\n\n\n","192a3c3b":"Podemos ver en los gr\u00e1ficos anteriores que la respuesta de los sensores a las abundancias las tendencias bimodales son inapreciables y en general se parece m\u00e1s a una \u00fanica distribuci\u00f3n normal. La referencia posee picos mucho m\u00e1s puntiagudos, mientras que los sensores parecen dise\u00f1ados para tener una respuesta normal."}}