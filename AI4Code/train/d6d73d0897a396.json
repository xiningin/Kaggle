{"cell_type":{"c836c0de":"code","281910c1":"code","16aabcda":"code","d6146950":"code","8b23b0f7":"code","203f8a8e":"code","9a3b5017":"code","75bde9b5":"code","15e98b84":"code","17ec7417":"code","2235c29d":"code","589bb90b":"code","76b267a4":"code","869bef3b":"code","b58ce2f9":"code","af672097":"code","b6ff36dc":"code","1fc9c4bc":"code","849a102c":"code","91cb7b9d":"code","e91e42e4":"code","a222bdb3":"code","fc61657a":"code","77dbd7ca":"code","3edb466a":"code","9dfb7b24":"code","5e7c2cb5":"code","f130eb2b":"code","c0bf0005":"code","fb9416ff":"code","4d46e1f2":"code","8764e422":"code","f2ecea79":"code","c38dcb8b":"code","ea3c1fd0":"code","982ed9dc":"code","3c9a0189":"code","e5eeab47":"code","7c5f4fac":"code","5ad84c1d":"code","e2626a76":"code","03007185":"code","0b612150":"code","f8bd4333":"code","1101c226":"code","f71b59b9":"code","1dc3e47e":"code","6668aaab":"code","d74c1192":"code","0f083844":"code","735a6581":"code","71913950":"code","7302f923":"code","f1039a25":"code","2da3520b":"code","f3409a1d":"code","8517ad95":"code","9f06ad66":"code","2f99cc66":"code","0b82b0a4":"code","ad13e8d4":"markdown","4b9901b5":"markdown","abd01d42":"markdown","e2917ee1":"markdown","488f9e50":"markdown","5d57b73b":"markdown","2b520ac2":"markdown","bac613c0":"markdown","5f59ba8b":"markdown","6add03e7":"markdown","ee83fc9d":"markdown","3104d7fa":"markdown","ffe3ed13":"markdown","ada61e62":"markdown","5f5b911e":"markdown","1d7ed5f4":"markdown","3b6a310d":"markdown","61878915":"markdown","5925df17":"markdown","2334db28":"markdown","58dad8b0":"markdown","bc679151":"markdown","23136739":"markdown","0016a41b":"markdown","970c1a18":"markdown","b0029290":"markdown","da609fc3":"markdown","513d8b36":"markdown","f50adbb3":"markdown","85cd706a":"markdown","c10032db":"markdown","3d5c0670":"markdown","330278a1":"markdown","eba94e01":"markdown","5b0a39b4":"markdown","d9712808":"markdown","fff337c9":"markdown","b62f3a6c":"markdown","d093f47f":"markdown","d00877be":"markdown","4fa8ddd5":"markdown","618a47cf":"markdown","1eab5959":"markdown","56ba7929":"markdown"},"source":{"c836c0de":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = 10, 12\nplt.style.use(\"seaborn\")\nimport statsmodels.formula.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\n#supressing all the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport scipy.stats as stats\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_validate","281910c1":"loan_data_df = pd.read_csv(\"\/kaggle\/input\/loanapplicantdata\/LoanApplicantData.csv\")","16aabcda":"#Reading top 5 rows of the data set\nloan_data_df.head()","d6146950":"#number of observations and features\nloan_data_df.shape\n# will shows the result (#Rows, #Cols)","8b23b0f7":"#data types in the dataframe\nloan_data_df.info()\n# The below result shows for 13 columns having how many data, data types","203f8a8e":"#check for any column has missing values\nloan_data_df.isnull().any()\n#it will give the boolean value (True\/false) aginst all the columns, \n#if any column is having some missing values, then it will shows True otherwise it will show False.","9a3b5017":"## basic descriptive statistics\nloan_data_df.describe()","75bde9b5":"#map loan status \"Y\" to 1 and \"N\" to 0\nloan_data_df[\"Loan_Status\"] = loan_data_df[\"Loan_Status\"].map({\"Y\" : 1, \"N\" : 0})","15e98b84":"loan_data_df[\"Loan_Status\"].value_counts()","17ec7417":"def NumericalVariables_targetPlots(df,segment_by,target_var = \"Loan_Status\"):\n    \"\"\"A function for plotting the distribution of numerical variables and its effect on Loan_Status\"\"\"\n    \n    fig, ax = plt.subplots(ncols= 2, figsize = (14,6))    \n\n    #boxplot for comparison\n    sns.boxplot(x = target_var, y = segment_by, data=df, ax=ax[0])\n    ax[0].set_title(\"Comparision of \" + segment_by + \" vs \" + target_var)\n    \n    #distribution plot\n    ax[1].set_title(\"Distribution of \"+segment_by)\n    ax[1].set_ylabel(\"Frequency\")\n    sns.distplot(a = df[segment_by].dropna(), ax=ax[1], kde=False)\n    \n    plt.show()","2235c29d":"def CategoricalVariables_targetPlots(df, segment_by,invert_axis = False, target_var = \"Loan_Status\"):\n    \n    \"\"\"A function for Plotting the effect of variables(categorical data) on Loan_Status \"\"\"\n    \n    fig, ax = plt.subplots(ncols= 2, figsize = (14,6))\n    \n    #countplot for distribution along with target variable\n    #invert axis variable helps to inter change the axis so that names of categories doesn't overlap\n    if invert_axis == False:\n        sns.countplot(x = segment_by, data=df,hue=\"Loan_Status\",ax=ax[0])\n    else:\n        sns.countplot(y = segment_by, data=df,hue=\"Loan_Status\",ax=ax[0])\n        \n    ax[0].set_title(\"Comparision of \" + segment_by + \" vs \" + \"Loan_Status\")\n    \n    #plot the effect of variable on attrition\n    if invert_axis == False:\n        sns.barplot(x = segment_by, y = target_var ,data=df,ci=None)\n    else:\n        sns.barplot(y = segment_by, x = target_var ,data=df,ci=None)\n        \n    ax[1].set_title(\"Loan_Status rate by {}\".format(segment_by))\n    ax[1].set_ylabel(\"Average(Loan_Status)\")\n    plt.tight_layout()\n\n    plt.show()","589bb90b":"numeric_var_names = [key for key in dict(loan_data_df.dtypes) if dict(loan_data_df.dtypes)[key] in ['float64', 'int64', 'float32', 'int32']]\nprint(numeric_var_names)","76b267a4":"# we are checking the distribution of ApplicantIncome and its related to loan status or not\n\nNumericalVariables_targetPlots(loan_data_df, segment_by=\"ApplicantIncome\")","869bef3b":"print(\"Max of Applicant Income:\", loan_data_df[\"ApplicantIncome\"].max())\nprint(\"Max of Applicant Income:\", loan_data_df[\"ApplicantIncome\"].min())","b58ce2f9":"# we are checking the distribution of CoapplicantIncome and its related to loan status or not\n\nNumericalVariables_targetPlots(loan_data_df, segment_by=\"CoapplicantIncome\")","af672097":"# we are checking the distribution of LoanAmount and its related to loan status or not\n\nNumericalVariables_targetPlots(loan_data_df, segment_by=\"LoanAmount\")","b6ff36dc":"# we are checking the distribution of Credit_History and its related to loan status or not\n\nNumericalVariables_targetPlots(loan_data_df, segment_by=\"Credit_History\")","1fc9c4bc":"catgorical_var_names = [key for key in dict(loan_data_df.dtypes) if dict(loan_data_df.dtypes)[key] in ['object']]\nprint(catgorical_var_names)","849a102c":"CategoricalVariables_targetPlots(loan_data_df,\"Gender\")","91cb7b9d":"CategoricalVariables_targetPlots(loan_data_df,\"Married\")","e91e42e4":"CategoricalVariables_targetPlots(loan_data_df,\"Dependents\")","a222bdb3":"CategoricalVariables_targetPlots(loan_data_df,\"Education\")","fc61657a":"CategoricalVariables_targetPlots(loan_data_df,\"Self_Employed\")","77dbd7ca":"#check for number of missing values\nloan_data_df.isnull().sum()","3edb466a":"#impute gender, married, dependents, Self_Employed with mode since they are categorical features\n\nloan_data_df[\"Gender\"] = loan_data_df[\"Gender\"].fillna(loan_data_df[\"Gender\"].mode()[0])\nloan_data_df[\"Married\"] = loan_data_df[\"Married\"].fillna(loan_data_df[\"Married\"].mode()[0])\nloan_data_df[\"Dependents\"] = loan_data_df[\"Dependents\"].fillna(loan_data_df[\"Dependents\"].mode()[0])\nloan_data_df[\"Self_Employed\"] = loan_data_df[\"Self_Employed\"].fillna(loan_data_df[\"Self_Employed\"].mode()[0])","9dfb7b24":"#impute the LoanAmount, Loan_Amount_Term, Credit_History with median data because they are skewed\n\nloan_data_df[\"LoanAmount\"] = loan_data_df[\"LoanAmount\"].fillna(loan_data_df[\"LoanAmount\"].median())\nloan_data_df[\"Loan_Amount_Term\"] = loan_data_df[\"Loan_Amount_Term\"].fillna(loan_data_df[\"Loan_Amount_Term\"].median())\nloan_data_df[\"Credit_History\"] = loan_data_df[\"Credit_History\"].fillna(loan_data_df[\"Credit_History\"].median())","5e7c2cb5":"loan_data_df.head()","f130eb2b":"#take a copy of the data\nloan_data_encoded_df = loan_data_df.copy()","c0bf0005":"#convert 'Gender' -> Male to 1 and Female to 0\nloan_data_encoded_df[\"Gender\"] = loan_data_df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n\n#convert the Married variable Yes to 1 and No to 0\nloan_data_encoded_df[\"Married\"] = loan_data_df[\"Married\"].map({\"Yes\" : 1, \"No\" : 0})\n\n#convert the Self_Employed variable Yes to 1 and No to 0\nloan_data_encoded_df[\"Self_Employed\"] = loan_data_df[\"Self_Employed\"].map({\"Yes\" : 1, \"No\" : 0})\n\n#education: there is an order inolved. Graduate > Not graduate\nloan_data_encoded_df[\"Education\"] = loan_data_df[\"Education\"].map({\"Graduate\" : 2, \"Not Graduate\" : 1})\n\n#education: there is an order inolved. Graduate > Not graduate\nloan_data_encoded_df[\"Property_Area\"] = loan_data_df[\"Property_Area\"].map({\"Rural\" : 1, \"Semiurban\" : 2, \"Urban\" : 3})\n\n#replace dependents 3+ with 3.\nloan_data_encoded_df[\"Dependents\"] = loan_data_df[\"Dependents\"].map({\"3+\" : 3, \"1\": 1, \"2\": 2, \"0\" : 0})","fb9416ff":"loan_data_encoded_df.head()","4d46e1f2":"#removing the loan Id feature, since it doesn't give any predictive power\nloan_data_encoded_df.drop([\"Loan_ID\"], axis = 1, inplace=True)","8764e422":"#correlation matrix\ncorr_matrix = loan_data_encoded_df.corr()\n\nax = sns.heatmap(\n    corr_matrix, \n    vmin=-1, vmax=1, center=0,\n    cmap=\"coolwarm\",\n    square=True\n)\n\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","f2ecea79":"featurecolumns = loan_data_encoded_df.columns.difference(['Gender', 'Loan_Status'])\nfeaturecolumns","c38dcb8b":"X = loan_data_encoded_df[featurecolumns]\ny = loan_data_encoded_df[\"Loan_Status\"]","ea3c1fd0":"# Function for creating model pipelines\nfrom sklearn.pipeline import make_pipeline\n\n#function for crossvalidate score\nfrom sklearn.model_selection import cross_validate\n\n#to find the best \nfrom sklearn.model_selection import GridSearchCV","982ed9dc":"#80% training data and 20% test data\n\nX_train, X_test, y_train,y_test = train_test_split(X,y,test_size = 0.2,stratify = y,random_state = 100)","3c9a0189":"#check the proportion of data\ny_train.value_counts(normalize = True) * 100","e5eeab47":"pd.DataFrame(y_train.value_counts(normalize = True) * 100).plot(kind = \"bar\")\nplt.title(\"Distribution of Loan Status\")\nplt.show()","7c5f4fac":"#make a pipeline for decision tree model \n\npipelines = {\n    \"clf\": make_pipeline(DecisionTreeClassifier(max_depth=3,random_state=100))\n}","5ad84c1d":"decisiontree_hyperparameters = {\n    \"decisiontreeclassifier__max_depth\": np.arange(3,12),\n    \"decisiontreeclassifier__max_features\": np.arange(3,10),\n    \"decisiontreeclassifier__min_samples_split\": [2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n    \"decisiontreeclassifier__min_samples_leaf\" : np.arange(1,3)\n}","e2626a76":"#Create a cross validation object from decision tree classifier and it's hyperparameters\n\nclf_model = GridSearchCV(pipelines['clf'], decisiontree_hyperparameters, cv=5, n_jobs=-1)","03007185":"#fit the model with train data\nclf_model.fit(X_train, y_train)","0b612150":"#Display the best parameters for Decision Tree Model\nclf_model.best_params_","f8bd4333":"#Display the best score for the fitted model\nclf_model.best_score_","1101c226":"#Predicting the test cases\nbankloans_test_pred_dt = pd.DataFrame({'actual':y_test, 'predicted': clf_model.predict(X_test)})\nbankloans_test_pred_dt = bankloans_test_pred_dt.reset_index(drop = True)\n\n#predicted probability\nbankloans_test_pred_dt[\"predicted_prob\"] = pd.DataFrame([p[1] for p in clf_model.predict_proba(X_test)])\n\nbankloans_test_pred_dt.head()","f71b59b9":"#classification report\n\nprint(metrics.classification_report(bankloans_test_pred_dt.actual, bankloans_test_pred_dt.predicted))","1dc3e47e":"#confusion matrix\nmetrics.confusion_matrix(bankloans_test_pred_dt.actual,bankloans_test_pred_dt.predicted)","6668aaab":"#Area Under ROC Curve\n\nauc_score_test = metrics.roc_auc_score(bankloans_test_pred_dt.actual,bankloans_test_pred_dt.predicted)\nprint(\"AUROC Score:\",round(auc_score_test,4))","d74c1192":"##Plotting the ROC Curve\n\nfpr, tpr, thresholds = metrics.roc_curve(bankloans_test_pred_dt.actual,bankloans_test_pred_dt.predicted_prob,drop_intermediate=False)\n\n\nplt.figure(figsize=(8, 6))\nplt.plot( fpr, tpr, label='ROC curve (area = %0.4f)' % auc_score_test)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate or [1 - True Negative Rate]')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic cuve')\nplt.legend(loc=\"lower right\")\nplt.show()","0f083844":"#calculating the recall score\nprint(\"Recall Score:\",round(metrics.recall_score(bankloans_test_pred_dt.actual,bankloans_test_pred_dt.predicted) * 100,3))","735a6581":"#calculating the precision score\nprint(\"Precision Score:\",round(metrics.precision_score(bankloans_test_pred_dt.actual,bankloans_test_pred_dt.predicted) * 100,3))","71913950":"#compute f1 score\nmetrics.f1_score(bankloans_test_pred_dt.actual, bankloans_test_pred_dt.predicted)","7302f923":"pip install pydotplus","f1039a25":"from six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus as pdot\nimport graphviz as graphviz\n","2da3520b":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_validate","f3409a1d":"#write the dot data\ndot_data = StringIO()","8517ad95":"#saving into a variable to get graph\nclf_best_model = clf_model.best_estimator_.named_steps['decisiontreeclassifier']","9f06ad66":"#export the decision tree along with the feature names into a dot file format\n\nexport_graphviz(clf_best_model,out_file=dot_data,filled=True,\n                rounded=True,special_characters=True,feature_names = X_train.columns.values,class_names = [\"No\",\"Yes\"])","2f99cc66":"#make a graph from dot file \ngraph = pdot.graph_from_dot_data(dot_data.getvalue())","0b82b0a4":"Image(graph.create_png())","ad13e8d4":"**Loading dataset**","4b9901b5":"**Importing Librarires**","abd01d42":"**Dependents**","e2917ee1":"**LoanAmount**","488f9e50":"**Confusion Matrix**\n- The confusion matrix is a way of tabulating the number of misclassifications, i.e., the number of predicted classes which ended up in a wrong classification bin based on the true classes.","5d57b73b":"**Decision Tree**\n- Average accuracy of pipeline with Decision Tree Classifier is 83.48%\n- Cross-Validation and Hyper Parameters Tuning: Cross Validation is the process of finding the best combination of parameters for the model by traning and evaluating the model for each combination of the parameters\n- Declare a hyper-parameters to fine tune the Decision Tree Classifier\n- Decision Tree is a greedy algorithm it searches the entire space of possible decision trees. So, we need to find an optimum parameter(s) or criteria for stopping the decision tree at some point. We use the hyperparameters to prune the decision tree.","2b520ac2":"From the ROC Curve, we have a choice to make depending on the value we place on true positive and tolerance for false positive rate\n\n- If we wish to give loans to more customers, we could increase the true positive rate by adjusting the probability cutoff for classification. However by doing so would also increase the false positive rate. we need to find the optimum value of cutoff for classification","bac613c0":"**Observations**\n\n**Numerical Variables Analysis:**\n- The maximum applicant income is 81,000 and minimum applicant income is 150. There is a wide variation in the distribution of the applicant income.\n- Most of the applicants has income in the range of 0 - 10,000. Very few applicants have an income over 10,000.\n- From the box plot we can observe that the applicants with more income, will most likely get loan approved.\n- Most of the co-applicants income is zero. Al most half of the co-applicants income is zero.\n- From the box plot analysis, coapplicantincome is not very useful in distinguishing the loan status\n- Loan Amount follows almost normal distribution.\n\n**Categorical Variables Analysis:**\n- Gender plays an important role in deciding the loan status of an applicant. The loan eligibility for male customers is more than the female customers.\n- The average loan status rate is more for the male customers.\n- Married customers mostly likely get the loan approved compared to non married customers.\n- In the total dataset, most of the customers (50%) for the bank has no dependents. Very few customers have more than 3 dependents.\n- As expected, bank tend to give loan approvals to customers with no dependents. Since they don't have to spend money on dependent people.\n- This feature could be an important variable in model building\n- Education variable plays an important role in deciding the loan status of that customer. As expected banks tend to give loans to educated customers as they can get job and repay the loan.\n- Bank is conservative in giving loans to non-graduated applicants.\n- Education is an important feature in deciding the loan eligibility of the applicant.\n- Most of the banks customers are not self employed, meaning they are either employed in public or private organisations.\n- Self employment comes with a certain risk of uncertainity, bank doesn't want to give more loans to self employed applicants.","5f59ba8b":"**Separating the Target and the Predictors**","6add03e7":"**Education**","ee83fc9d":"**Observations**\n- Gender plays an important role in deciding the loan status of an applicant. The loan eligibility for male customers is more than the female customers.\n- The average loan status rate is more for the male customers.","3104d7fa":"**Model Performance Evaluation**\n- We will now predict the test set results and check the accuracy with each of our model. \n- The classification_report() function displays the Precision, Recall, f1-score and support for each class. Precision gives Percentage of positive instances out of the total predicted positive instances.  Recall gives Percentage of positive instances out of the total actual positive instances. F1 score summarizes both Precision and Recall and can be understood as the harmonic mean of the two measures.\n- To check the accuracy, we import Confusion Matrix method of Metrics Class. The Confusion matrix is a way of tabulating the number of misclassifications, i.e., the number of predicted classes which ended up in a wrong classification bin based on the true classes.\n- The ROC curve plots the true positive rate against the false positive rate, or the sensitivity against 1-specificity for each threshold. From the Curve, we have a choice to make depending on the value we place on true positive and tolerance for false positive rate. If we wish to give loans to more customers, we could increase the true positive rate by adjusting the probability cut-off for classification. However, by doing so we would also increase the false positive rate. We need to find the optimum value of cut-off for classification\n","ffe3ed13":"**Visualization of Decision Tree**","ada61e62":"**Analyizing the variables**\n- Categorical Variables","5f5b911e":"# Loan Application Data - A complete Solution of Machine Learning Model","1d7ed5f4":"**Married**","3b6a310d":"**Model Building and Model Diagnostics**\n - Logistic Regression\n - Decision Tree classifier\n - Supervised learning: Supervised learning is a type of system in which both Input and desired Output data are provided. They can be further grouped into Regression and Classification problems. Here, we have a  Classification Problem as the output variable \u201cLoan_Status\u201d which can take one out of two values \u201cY\u201d or \u201cN\u201d. We will start with Logistic Regression Algorithm.\n- We will use SciKit-Learn library in Python to import all the methods of Classification Algorithms.\n- We will make separate sets of Predictor and Target Variables.\n- Splitting the dataset: The data we use is usually split into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our model\u2019s prediction on this subset. We will do this using using the train_test_split method with Stratified Sampling which allows for best representation of the entire population being studied.","61878915":"**Maping of target variables**\n\u2022\tTarget variable\/Dependent variable is categorical in nature with two classes \u2013 \u2018Y\u2019 and \u2018N\u2019. We will be starting with Logistic Regression or Decision Tree #Classifier for the Binary Classification Problem at hand. So, we will have to convert the categorical variable in the dataset into numeric variables which we will achieve by Category Indexing. For this, we will assign a numeric value to each of the Class labels: \u2018Y\u2019 as 1 and \u2018N\u2019 as 0.","5925df17":"**Observations**\n- In the total dataset, most of the customers (50%) for the bank has no dependents. Very few customers have more than 3 dependents.\n- As expected, bank tend to give loan approvals to customers with no dependents. Since they don't have to spend money on dependent people.\n- This feature could be an important variable in model building","2334db28":"**Observations**\n- `Gender` and `Married` Features are correlated among themselves (auto correlation). We can drop `Married` feature because we know that the `Gender` is an important feature from our data analysis.\n- Remaining all other features are not auto-correlated.","58dad8b0":"**Feature Selection**\n- Correlation Check: The statistical relationship between two variables is referred to as their correlation. A correlation could be positive, meaning both variables move in the same direction, or negative, meaning that when one variable\u2019s value increases, the other variables\u2019 values decrease. Correlation can also be neutral or zero, meaning that the variables are unrelated.\n- Removing the \u201cLoan Id\u201d feature, since it doesn't give any predictive power\n- Gender and Married Features are correlated among themselves (auto correlation). We can drop Married feature because we know that the Gender is an important feature from our data analysis. \n- Remaining all other features are not auto-correlated.\n","bc679151":"**Observations**\n- Dataset consists of 614 observations and 13 variables - out of which \u201cLoan Status\u201d is Dependent Variable while others are Independent Variables.\n- 13 features is a mix of categorical and numerical data (\"Loan status\" is categorical variable, remaining all are numerical variable)\n- There are missing values in the dataset.\n- Data set is highly imbalanced data, An imbalanced dataset means instances of one of the two classes is higher than the other, in another way, the number of - observations is not the same for all the classes in a classification dataset. This can be reduced by implementing Over Sampling, Under Sampling,            - Cost Sensitive Learning Techniques and Ensemble Learning Techniques. We will be using Stratified Sampling .","23136739":"**Analyizing the variables**\n- Numerical Variables","0016a41b":"**Observations**\n- `Education` variable plays an important role in deciding the loan status of that customer. As expected banks tend to give loans to educated customers as they can get job and repay the loan. \n- Bank is conservative in giving loans to non-graduated applicants.\n- `Education` is an important feature in deciding the loan eligibility of the applicant.","970c1a18":"**Gender**","b0029290":"**Self_Employed**","da609fc3":"**Observations**\n- Most of the co-applicants income is zero. Al most half of the co-applicants income is zero.\n- From the box plot analysis, coapplicantincome is not very useful in distinguishing the loan status","513d8b36":"**Train-Test Split(Stratified Sampling of Y)**","f50adbb3":"**ApplicantIncome**","85cd706a":"***Descriptive Analysis***","c10032db":"**Decision Tree classifier with gini index**\n- Fit and tune models with cross-validation\n- Now that we have our pipelines and hyperparameters dictionaries declared, we're ready to tune our models with cross-validation.\n\n- We are doing 5 fold cross validation","3d5c0670":"**Problem Statement**\nIndia Housing Finance offers home loans for low-income housing. They have presence across all urban, semi urban and rural areas. When customer applies for home loan, the company validates the customer eligibility for loan. They want to automate the loan eligibility process based on customer details provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History etc.\n\n\n**Objectives:**\n1. Building a Predictive Model\n2. Evaluate the model.\n3. Refine the model, as appropriate\n\n**Working approach**\n1. Importing required Libraries\n2. Loading dataset\n3. Descriptive analysis (shape, describe, missing data etc)\n4. Exploratory Data Analysis\n5. Variable analysis\n6. Data Cleaning\n7. Handling categorical data\n8. Feature selection\n9. Model building and Model diagnostics (decision Tree)\n10. Model performance and evaluations\n11. Decision Tree visualization\n\n\n\n","330278a1":"**Exploratory Data Analysis**\n#Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns, to spot anomalies, to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.","eba94e01":"**Descriptive Statistics:** are the coefficients that summarize a given data set, which can be either a representation of the entire or a sample of a #population. Descriptive statistics are broken down into measures of central tendency and measures of variability (spread). Measures of central tendency #include the mean, median, and mode, while measures of variability include the standard deviation, variance, the minimum and maximum variables, and the #kurtosis and skewness.\n#(a)\tHere, we notice that mean value is less than median value for \u201cApplicantIncome\u201d, \u201cCoapplicantIncome\u201d, \u201cLoanAmount\u201d columns which is represented by 50%#(50th percentile) in index column while for \u201cLoan_Amount_Term\u201d and \u201cCredit_History\u201d mean lies above the median value. \n#(b)\tThere is notably a large difference between 75th %tile and max values of predictors for \u201cApplicantIncome\u201d, \u201cCoapplicantIncome\u201d, \u201cLoanAmount\u201d columns.\n#(c)\tThus observations (a) and (b) suggests that there are extreme values-Outliers in our data set.\n","5b0a39b4":"**Metrics**\n- Recall: Ratio of the total number of correctly classified positive examples divide to the total number of positive examples. High Recall indicates the class is correctly recognized\n- Precision: To get the value of precision we divide the total number of correctly classified positive examples by the total number of predicted positive examples. High Precision indicates an example labeled as positive is indeed positive","d9712808":"**Observations**\n- Loan Amount follows almost normal distribution.","fff337c9":"**Checking for missing data**","b62f3a6c":"**Cross-Validation and Hyper Parameters Tuning**\nCross Validation is the process of finding the best combination of parameters for the model by traning and evaluating the model for each combination of the parameters\n\n- Declare a hyper-parameters to fine tune the Decision Tree Classifier\n\n- Decision Tree is a greedy alogritum it searches the entire space of possible decision trees. so we need to find a optimum parameter(s) or criteria for stopping the decision tree at some point. We use the hyperparameters to prune the decision tree","d093f47f":"**Data Cleaning**\n- Imputing missing values : Impute Gender, Married, Dependents, Self_Employed with mode since they are categorical features. Mode is statistical strategy to impute missing values where missing data is replaced with the most frequent values\/mode within each column. Impute the LoanAmount, Loan_Amount_Term, Credit_History with median data because they are skewed. This works by calculating the median of the non-missing values in a column and then replacing the missing values within each column separately and independently from the others. It can only be used with numeric data.","d00877be":"**CoapplicantIncome**","4fa8ddd5":"**Handling Categorical Data**\n- Convert Categorical variables to encoded variables","618a47cf":"**Observations**\n- Married customers mostly likely get the loan approved compared to non married customers.","1eab5959":"**Observations**\n- Most of the banks customers are not self employed, meaning they are either employed in public or private organisations.\n- Self employment comes with a certain risk of uncertainity, bank doesn't want to give more loans to self employed applicants.","56ba7929":"**Credit_History**"}}