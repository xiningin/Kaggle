{"cell_type":{"5172a40a":"code","c19f0349":"code","d19e81d4":"code","28d3aa59":"code","4eab4b43":"code","dbad7f38":"code","9e749f2b":"code","6bcddfd8":"code","bad1797d":"code","83c52b4c":"code","d56641c5":"code","de8a7087":"code","aec140c0":"code","43cd2c9c":"code","4282e56a":"code","28ab2e33":"code","3ec3ae09":"code","2c03557c":"code","da100284":"code","4cf07e0e":"code","9fb0ae3b":"code","4117cdb2":"code","609e7d5f":"code","01b97a77":"code","6a71aeea":"code","a4871aa7":"code","85ac80b2":"code","d204c24a":"code","ddb61d84":"code","126d37a9":"code","e6b88f51":"code","4d1c28c7":"code","2a9c45cd":"code","74de7582":"code","5dc83861":"code","2fd7b29e":"code","73bd9125":"code","f9dff624":"code","f115ad56":"code","b77f6298":"code","59055681":"code","69d0d0ca":"code","90d393b0":"code","11c1acda":"code","eef8f27f":"code","14d1b3a9":"code","14984ae9":"code","9e5f01c2":"markdown","a78d362b":"markdown","ce869722":"markdown","ea10792b":"markdown","4aa58cc9":"markdown","f122494d":"markdown","f4271fd6":"markdown","8a24dc0c":"markdown","91179b52":"markdown","cf33df74":"markdown","f9c0dda1":"markdown","7359b742":"markdown","b73f7a53":"markdown","4dd03f8a":"markdown"},"source":{"5172a40a":"import numpy as np \nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt ","c19f0349":"data = pd.read_csv('..\/input\/ds5230usml-project\/Reviews.csv')","d19e81d4":"print('Shape:',data.shape)\n\ndata.head()","28d3aa59":"#Sorting the data with Product ID now\nsorted_data = data.sort_values('ProductId',axis = 0, inplace = False, kind = 'quicksort',ascending = True,na_position='last')\nsorted_data.head()\nprint(sorted_data.shape)","4eab4b43":"filtered_data = sorted_data.drop_duplicates(subset = {'UserId','ProfileName','Time','Text'} ,keep = 'first', inplace = False)\nprint('The values dropped from (568454, 11) -->',filtered_data.shape)\nprint('The percentage of data remaining is -->',(filtered_data.shape[0]\/sorted_data.shape[0])*100,'%')","dbad7f38":"# Creating the copy of the data here \nfinal = filtered_data.copy()\nfinal.shape\nfinal.head()","9e749f2b":"# To check if the helpfulness numerator is greater than the denominator anywhere\nfinal[final.HelpfulnessNumerator > final.HelpfulnessDenominator]","6bcddfd8":"# We cannot have these two as its not possible here\nfinal = final[final.HelpfulnessNumerator <= final.HelpfulnessDenominator]\nfinal.shape","bad1797d":"print(final.Text.values[0])\nprint(\"\")\nprint(final.Text.values[900])\nprint('')\nprint(final.Text.values[4900])\nprint('')\nprint(final.Text.values[25000])\nprint('-------------------')\nprint('We see that we have lots of other things like html tags and stuff like that so we have to remove it')\n","83c52b4c":"import re\n","d56641c5":"import nltk\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\nfrom nltk.corpus import wordnet\nimport string\nfrom nltk import pos_tag\nfrom nltk.tokenize import WhitespaceTokenizer\nfrom nltk.stem import WordNetLemmatizer","de8a7087":"final.head(3)","aec140c0":"data = final.copy()\n\n\n# Define the function to implement POS tagging:\ndef get_wordnet_pos(pos_tag):\n    if pos_tag.startswith('J'):\n        return wordnet.ADJ\n    elif pos_tag.startswith('V'):\n        return wordnet.VERB\n    elif pos_tag.startswith('N'):\n        return wordnet.NOUN\n    elif pos_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\n\n# Define the main function to clean text in various ways:\ndef clean_text(text):\n    \n    # Apply regex expressions first before converting string to list of tokens\/words:\n    # 1. remove @usernames\n    text = re.sub('@[^\\s]+', '', text)\n    \n    # 2. remove URLs\n    text = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))', '', text)\n    \n    # 3. remove hashtags entirely i.e. #hashtags\n    text = re.sub(r'#([^\\s]+)', '', text)\n    \n    # 4. remove emojis\n    # text = emoji_pattern.sub(r'', text)\n    \n    # 5. Convert text to lowercase\n    text = text.lower()\n    \n    # 6. tokenize text and remove punctuation\n    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n    \n    # 7. remove numbers\n    text = [word for word in text if not any(c.isdigit() for c in word)]\n    \n    # 8. remove stop words\n    # stop = stopwords.words('english')\n    text = [x for x in text if x not in stop]\n    \n    # 9. remove empty tokens\n    text = [t for t in text if len(t) > 0]\n    \n    # 10. pos tag text and lemmatize text\n    pos_tags = pos_tag(text)\n    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n    \n    # 11. remove words with only one letter\n    text = [t for t in text if len(t) > 1]\n    \n    # join all\n    text = \" \".join(text)\n    \n    return(text)","43cd2c9c":"# Apply function on the column 'text':\ndata['cleaned_text'] = data['Text'].apply(lambda x: clean_text(x))","4282e56a":"# Saving Progress to csv\n# data.to_csv('Clean_Text_1.csv', index=False)\n# Check out the shape again and reset_index\nprint(data.shape)\ndata.reset_index(inplace = True, drop = True)\n \n# Check out data.tail() to validate index has been reset\ndata.tail()","28ab2e33":"from nltk.sentiment.vader import SentimentIntensityAnalyzer","3ec3ae09":"# Create a sid object called SentimentIntensityAnalyzer()\nsid = SentimentIntensityAnalyzer()\n\n# Apply polarity_score method of SentimentIntensityAnalyzer()\ndata['sentiment'] = data['cleaned_text'].apply(lambda x: sid.polarity_scores(x))\n\ndata.head()","2c03557c":"# Keep only the compound scores under the column 'Sentiment'\ndata = pd.concat([data.drop(['sentiment'], axis = 1), data['sentiment'].apply(pd.Series)], axis = 1)","da100284":"data.head(5)","4cf07e0e":"# New column: number of characters in 'review'\ndata['numchars'] = data['cleaned_text'].apply(lambda x: len(x))\n\n# New column: number of words in 'review'\ndata['numwords'] = data['cleaned_text'].apply(lambda x: len(x.split(\" \")))\n\n# Check the new columns:\ndata.tail(5)","9fb0ae3b":"data.head()","4117cdb2":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color = 'white',\n        max_words = 100,\n        max_font_size = 40, \n        scale = 3,\n        random_state = 42\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize = (20, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize = 20)\n        fig.subplots_adjust(top = 2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n    \n# print wordcloud\nshow_wordcloud(data['cleaned_text'])","609e7d5f":"# Focusing only  on 'compound' scores here...\nsentimentclass_list = []\nfor i in range(0, len(data)):\n # current 'compound' score:\n    curr_compound = data.iloc[i,:]['compound']\n    if (curr_compound <= 1.0 and curr_compound >= 0.55):\n        sentimentclass_list.append(5)\n    elif (curr_compound < 0.55 and curr_compound >= 0.10):\n        sentimentclass_list.append(4)\n    elif (curr_compound < 0.10 and curr_compound > -0.10):\n        sentimentclass_list.append(3)\n    elif (curr_compound <= -0.10 and curr_compound > -0.55):\n        sentimentclass_list.append(2)\n    elif (curr_compound <= -0.55 and curr_compound >= -1.00):\n        sentimentclass_list.append(1)\n# Creating a new column here to add the sentiments classification\ndata['sentiment_class'] = sentimentclass_list","01b97a77":"# Backing up the data frame\ndata.iloc[0:5, :][['compound', 'sentiment_class']]\ndata.tail()\ndata.to_csv('Clean_Text_SentimentRating.csv', index=False)","6a71aeea":"import seaborn as sns\n# Distribution of sentiment_class\nplt.figure(figsize = (10,5))\nsns.set_palette('PuBuGn_d')\nsns.catplot(x=\"sentiment_class\", kind=\"count\", palette=\"ch:.25\", data=data)\nplt.title('Countplot of sentiment_class by Sentiment Analysis')\nplt.xlabel('sentiment class')\nplt.ylabel('No. of classes')\nplt.show()\n\nplt.figure(figsize = (10,5))\nsns.set_palette('PuBuGn_d')\nsns.catplot(x=\"Score\", kind=\"count\", palette=\"ch:.25\", data=data)\nplt.title('Countplot of Scores Given in the Dataset')\nplt.xlabel('Score Classes')\nplt.ylabel('No. of classes')\nplt.show()","a4871aa7":"# Display full text:\npd.set_option('display.max_colwidth', -1)","85ac80b2":"# Filter 10 negative reviews:\nprint(\"10 random negative original reviews and their sentiment classes and their Scores from data set:\")\ndata[(data['sentiment_class'] == 1) | (data['sentiment_class'] == 2)].sample(n=10)[['Text', 'sentiment_class','Score']]\n","d204c24a":"\n\n# Filter 10 neutral reviews:\nprint(\"10 random neutral original reviews and their sentiment classes and their score values:\")\ndata[(data['sentiment_class'] == 3)].sample(n=10)[['Text', 'sentiment_class','Score']]\n","ddb61d84":"# Filter 20 positive reviews:\nprint(\"10 random positive Revies and their sentiment classes mapped against their score values:\")\ndata[(data['sentiment_class'] == 4) | (data['sentiment_class'] == 5)].sample(n=10)[['Text', 'sentiment_class','Score']]","126d37a9":"# !pip3 install datetime ","e6b88f51":"import datetime\nfrom datetime import datetime\n\ndata['Time_converted']=data['Time'].apply(lambda col: datetime.utcfromtimestamp(int(col)).strftime('%Y-%m-%d'))\n\n#strftime('%Y-%m-%d %H:%M:%S'))\ndata['Time_converted_ym']=data['Time'].apply(lambda col: datetime.utcfromtimestamp(int(col)).strftime('%Y-%m')) \n\ndata.head(5)","4d1c28c7":"data.isnull().sum()","2a9c45cd":"data['Text_lenght']=data['Text'].apply(lambda col: len(col.split(' ')))\ndata.tail(2)","74de7582":"data['Text_lenght'].hist(bins=1000)\nplt.xlim(0,400)\nplt.title('Review length distribution',fontsize=14)","5dc83861":"data.Text_lenght.describe()","2fd7b29e":"def score_pos_neg(col):\n    \n    ''' To club the sentiments into positive neutral and negative sentiments\n    '''\n    \n    if col == 4 or col == 5:\n        \n        return 'positive'\n    elif col == 1 or col == 2:\n        \n        return 'negative'\n    \n    else:\n        return 'neutral'\n\ndata['score_pos_neg']=data['sentiment_class'].apply(score_pos_neg)","73bd9125":"# Checking correct assignments\ndata[data['score_pos_neg']=='neutral'].head()\n\n#plotting count of positive \"1\" and negative \"0\" reviews\nsns.countplot(x='score_pos_neg', data=data.sample(500));\nplt.xlabel('Score 0\/1',fontsize=14);\nplt.ylabel('Count',fontsize=14);\nplt.title('Review Score 0\/1 count 1999-2012',fontsize=14);","f9dff624":"# TOP Reviewers with count > 10\nuser_number_review=data.groupby(by=['UserId']).count().sort_values(by=['Text'],ascending=False)\n\nuser_number_review.head()\n\nuser_top_reviewer=user_number_review[user_number_review['Id']>10][['Id']]\nuser_top_reviewer.head()","f115ad56":"# distribution of review count per user\nuser_top_reviewer['Id'].hist(bins=350,label='Number of reviews for Top (>10) reviewer',alpha=0.7)\nplt.xlabel('Review count of top reviewers',fontsize=14)\nplt.ylabel('count',fontsize=14)\n\nplt.grid(linewidth=0.5,alpha=0.75)\n\nplt.xlim(10,100)\nplt.title('Review count for top reviewers',fontsize=14)","b77f6298":"# getting average data per user\nuser_average_info=data.groupby(by=['UserId']).mean()\n# top_reviewer_avg_data=pd.merge(user_average_info,user_top_reviewer,how='inner',on='UserId')\n\ntop_reviewer_avg_data=pd.merge(user_average_info,user_top_reviewer,how='inner',on='UserId')","59055681":"top_reviewer_avg_data['ratio_helpful']=top_reviewer_avg_data['HelpfulnessNumerator']\/top_reviewer_avg_data['HelpfulnessDenominator']","69d0d0ca":"top_reviewer_avg_data.head()\ntop_reviewer_avg_data['Time_converted']=top_reviewer_avg_data['Time'].apply(lambda col: datetime.utcfromtimestamp(int(col)).strftime('%Y-%m'))\ntop_reviewer_avg_data=top_reviewer_avg_data[(top_reviewer_avg_data['ratio_helpful']>0.0) & (top_reviewer_avg_data['ratio_helpful']<1.0)]\n\ncolors = np.random.rand(top_reviewer_avg_data.shape[0])\ntop_reviewer_avg_data.head()","90d393b0":"# review helpfulness distribution\ntop_reviewer_avg_data['ratio_helpful'].hist(bins=100,label='review helpfullness top reviewers',alpha=0.7);\n\nplt.xlabel('Review Helpfullness ratio',fontsize=14);\nplt.ylabel('count',fontsize=14);\n\n#plt.legend()\nplt.grid(linewidth=0.5,alpha=0.75)\n\n\nplt.title('Review helpfullness for top reviewers',fontsize=14);\nplt.savefig('helpfullness_top_reviewer_dist.png')","11c1acda":"# review length vs helpfulness\nplt.scatter(top_reviewer_avg_data['Text_lenght'],top_reviewer_avg_data['ratio_helpful'],alpha=0.7);\nplt.xlim(0,600);\nplt.xlabel('Review Length',fontsize=14);\nplt.ylabel('Review Helpfullness ratio',fontsize=14);\n#plt.legend()\nplt.grid(linewidth=0.5,alpha=0.75)\n\n\nplt.title('Review length for top reviewers',fontsize=14);\nplt.savefig('helpfullness_top_reviewer_length.png')","eef8f27f":"#revire count of top reviewers vs. review length\nplt.scatter(top_reviewer_avg_data['Id_y'],top_reviewer_avg_data['Text_lenght'],alpha=0.7); #,c=top_reviewer_avg_data['ratio_helpful'])\nplt.xlim(0,175);\n\nplt.xlabel('Top reviewer review count',fontsize=14);\nplt.ylabel('Review length for top reviewers',fontsize=14);\n\n#plt.legend()\nplt.grid(linewidth=0.5,alpha=0.75)\n\n\nplt.title('Review count vs. length top reviewers',fontsize=14);\nplt.savefig('helpfullness_top_reviewer_length_count.png')","14d1b3a9":"pos=data[data['score_pos_neg']=='positive']\nneg=data[data['score_pos_neg']=='negative']\nneu=data[data['score_pos_neg']=='neutral']\n\n\ngrp_date_pos=pos.groupby(by=['Time_converted_ym']).count();\ngrp_date_neg=neg.groupby(by=['Time_converted_ym']).count();\ngrp_date_neu=neu.groupby(by=['Time_converted_ym']).count();\n\n\ngrp_date_pos.reset_index(inplace=True);\ngrp_date_neg.reset_index(inplace=True);\ngrp_date_neu.reset_index(inplace=True);\n","14984ae9":"# review count by score for each month from 2000 to 2012\nplt.figure(figsize=(24,15))\n\nplt.plot_date(x=grp_date_pos['Time_converted_ym'],y=grp_date_pos['score_pos_neg'],label='Score=Positive');\nplt.plot_date(x=grp_date_neg['Time_converted_ym'],y=grp_date_neg['score_pos_neg'],label='Score=Negative');\nplt.plot_date(x=grp_date__neu['Time_converted_ym'],y=grp_date_neu['score_pos_neg'],label='Score=Neutral');\nplt.xticks(rotation=90);\nplt.legend()\nplt.grid(linewidth=0.7,alpha=0.75)\nplt.xlim('2000-01','2012-10');\nplt.xlabel('Date',fontsize=22)\nplt.ylabel('Number of review',fontsize=22)\nplt.title('Number of review trend from 2000 to 2012',fontsize=22);","9e5f01c2":"### *Review Helpfulness and Others*","a78d362b":"#### - The top reviewers that write more often do not write very long paragraphs above 200 words, while those that write longer have less than 20-30 review","ce869722":"## **VADER LEXICONS AND SENTIMENT ANALYSIS**","ea10792b":"**Creating Sentiment Column with the Compound score obtained from SentimentIntensityAnalyzer**","4aa58cc9":"### Review count by top reviewers","f122494d":"***Dropping the duplicate values of Prod ID Username,Time and Text***","f4271fd6":"- The trend start to change suddenly after 2006, There is a huge boost in scores specially in 5, this seems a bit unusual.\n    * possibly there are unverified accounts that are boosting the seller inappropriately with fake reviews.\n","8a24dc0c":"### **Displaying The reviews**","91179b52":"### **Processing Text**\n\nStep1: have a look at the data first\n","cf33df74":"### Getting the top reviewers","f9c0dda1":"#### Users generally find top reviewer with more than 200 words length review helpful, longer review better feedback","7359b742":"#### Majority find the top reviewers helpful","b73f7a53":"#### Reading the CSV","4dd03f8a":"**Importing the libraies**"}}