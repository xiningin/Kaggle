{"cell_type":{"17cac59d":"code","b6b3a396":"code","ba135e68":"code","3d37a442":"code","26decbac":"code","1688aed6":"code","e29ed47a":"code","2d512576":"code","04f4678d":"code","e201c0a7":"code","e028f268":"code","38c883a8":"code","d8945898":"code","64db431d":"code","3ec7d19b":"code","2953e889":"code","11b830af":"code","cd1e56f5":"code","57f13f16":"code","735ac23e":"code","11811d49":"code","5fee827a":"code","7a8f5334":"code","f93fe9a3":"code","8082b2a6":"code","32490f9e":"code","cf9a833a":"code","4858700b":"code","23fc30bf":"code","ef4ed4ea":"code","6461cad7":"code","49e73751":"code","629bc5bd":"code","9db7a5a2":"code","d2f98fcd":"code","9ed9fb9b":"code","71224323":"code","82fe7300":"code","622c56ce":"code","66646168":"code","b18a7e9f":"code","5b684326":"code","d685024e":"code","db3c877f":"code","75dc24fb":"code","716741cf":"code","c62266c5":"code","fc04d6f1":"code","e293d9ce":"code","4cebc362":"code","639efbe5":"code","aea745fb":"code","3ceafd37":"code","a525a22e":"code","04a4387d":"code","64c61913":"code","3b3c544f":"code","68f72e81":"code","2143269c":"code","53d8a145":"code","5c90d2f1":"code","5bedc2cc":"code","e27a7669":"code","57aeb2be":"code","b7165769":"code","c454508a":"code","b97fc6f3":"code","faca360c":"code","2f0e8c2d":"code","db9d93a2":"code","75c3468f":"code","0adec7e1":"code","4673bf9b":"code","83cb34da":"code","c8daa7b1":"markdown","2c255f60":"markdown","4d259637":"markdown","f3bee1fe":"markdown","b123f0d0":"markdown","67653777":"markdown","829134f4":"markdown","84902970":"markdown","5ebdfd40":"markdown","77a114a4":"markdown","47f4ac90":"markdown","9195a4a4":"markdown","660156c2":"markdown","ec083cd5":"markdown","03e09b90":"markdown","c4b7b99f":"markdown","3d3f0926":"markdown","bd10d0e1":"markdown","8e0d2b27":"markdown","b9afb7a3":"markdown","cfc8d9b4":"markdown","0ca1ff6e":"markdown","00196f9d":"markdown","24cccd6d":"markdown","d4a70d2a":"markdown","33807954":"markdown","d34083e6":"markdown","eb65d60b":"markdown","23792d79":"markdown","b6dafcd2":"markdown","996cf3fe":"markdown","565e0f94":"markdown","4d8aa247":"markdown","961aeb18":"markdown","a8ee64e1":"markdown"},"source":{"17cac59d":"import numpy as np\nimport pandas as pd","b6b3a396":"!pip install pynrrd","ba135e68":"from tqdm import tqdm\nimport os\nfrom random import randint\n\nimport numpy as np\nimport pandas as pd\n\nimport nibabel as nib\nimport pydicom as pdm\nimport nilearn as nl\nimport nilearn.plotting as nlplt\nimport nrrd\nimport h5py\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport matplotlib.animation as anim\n\nimport imageio\nfrom skimage.transform import resize\nfrom skimage.util import montage\n\nfrom IPython.display import Image as show_gif\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nfrom glob import glob\nfrom os.path import join as opj\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nsns.set_context('notebook')\n\nfrom tqdm.notebook import tqdm","3d37a442":"sample_filename = '..\/input\/trends-assessment-prediction\/fMRI_train\/10001.mat'\nmatlab_file = h5py.File(sample_filename)\nprint(matlab_file.keys())\nprint(matlab_file.values())\nprint(matlab_file['SM_feature'][()].shape)","26decbac":"# Loading reference image\nfmri_mask = nl.image.load_img('..\/input\/trends-assessment-prediction\/fMRI_mask.nii')\n\n# Reorienting the axis of 3D spatial map\nspatial_maps = np.moveaxis(matlab_file['SM_feature'][()], [0, 1, 2, 3], [3, 2, 1, 0]) \n\n# Loading 3D spatial maps\nspatial_maps_niimg = nl.image.new_img_like(ref_niimg=fmri_mask,\n                                           data=spatial_maps,\n                                           affine=fmri_mask.affine,\n                                           copy_header=True)","1688aed6":"img = img = list(nl.image.iter_img(spatial_maps_niimg))[0]\nview = nlplt.view_img_on_surf(img,\n                              title=f'10009.mat Spatial Map 0 view_img_on_surf',\n                              title_fontsize=20,\n                              threshold=1,\n                              black_bg=False)\nview.open_in_browser()\nview","e29ed47a":"path = '\/kaggle\/input\/trends-assessment-prediction\/'\n#targets = pd.read_csv(opj(path, 'train_scores.csv')).set_index('Id')","2d512576":"# Load the target features\ntargets = pd.read_csv('\/kaggle\/input\/trends-assessment-prediction\/train_scores.csv').set_index('Id')\ntargets.head()","04f4678d":"# How many null values does each target have\nn_nulls = targets.isnull().sum()\ndisplay(n_nulls)\nn_nulls.plot.barh();","e201c0a7":"targets.dropna(inplace=True)","e028f268":"sns.heatmap(targets.corr()*100, square=True, annot=True, fmt='.0f');","38c883a8":"targets.plot(lw=0, marker='.', markersize=1, subplots=True, figsize=(14, 8));","d8945898":"targets['age'].plot(lw=0, marker='.', markersize=1, figsize=(14, 4));","64db431d":"targets['age'].nunique()","3ec7d19b":"#distribution of age over target\nplt.plot(targets['age'].sort_values().values);","2953e889":"sns.pairplot(targets, plot_kws=dict(s=5, alpha=0.5));","11b830af":"plt.figure(figsize=(6, 6))\nd2 = targets.dropna().iloc[:, 3:].values\nplt.scatter(d2[:, 0], d2[:, 1], s=3);","cd1e56f5":"def rotate_origin(x, y, radians):\n    xx = x * np.cos(radians) + y * np.sin(radians)\n    yy = -x * np.sin(radians) + y * np.cos(radians)\n    return np.array([xx, yy]).T","57f13f16":"# Function to plot unique values in a given range\n\ndef plot_unique(start,end,d2,noOfVals):\n    n_uniques = []\n    for r in np.linspace(start, end, noOfVals):\n        d22_rot = rotate_origin(d2[:, 0], d2[:, 1], r)[:, 1]\n        n_uniques.append([r, len(np.unique(np.round(d22_rot, 6)))])\n    n_uniques = np.array(n_uniques)\n\n    plt.figure(figsize=(14, 2))\n    plt.scatter(n_uniques[:, 0], n_uniques[:, 1], s=3);","735ac23e":"# Trying for values from 0.85 to 0.95 radians\nplot_unique(0.85,0.95,d2,5000)","11811d49":"# As we can see sudden drop, trying for values from 0.905 to 0.910 radians\nplot_unique(0.905,0.910,d2,5000)","5fee827a":"# Trying for values from 0.90771 to 0.907715 radians\nplot_unique(0.90771,0.907715,d2,5000)","7a8f5334":"# rgets = pd.read_csv(opj(path, 'train_scores.csv')).set_index('Id')\n# Let's also create the rotated domain2 targets\nrot = 0.90771256655\nd2 = rotate_origin(targets.iloc[:, 3].values, targets.iloc[:, 4].values, rot)\ntargets['d21_rot'] = d2[:, 0]\ntargets['d22_rot'] = d2[:, 1]","f93fe9a3":"from scipy.stats import norm\nfor col in targets.columns:\n    plt.figure(figsize=(8, 2))\n    sns.distplot(targets[col], fit=norm, kde=True)\n    plt.show()","8082b2a6":"# Let's apply the power transformation to make the value distribution gaussian\npow_age = 1.0\npow_d1v1 = 1.5\npow_d1v2 = 1.5\npow_d2v1 = 1.5\npow_d2v2 = 1.5\npow_d21 = 1.5\npow_d22 = 1\npowers = [pow_age, pow_d1v1, pow_d1v2, pow_d2v1, pow_d2v2, pow_d21, pow_d22 ]\n\nfrom scipy.stats import norm\nfor i, col in enumerate(targets.columns):\n    plt.figure(figsize=(8, 2))\n    sns.distplot(np.power(targets[col], powers[i]), fit=norm, kde=True)\n    plt.show()","32490f9e":"for i, col in enumerate(targets.columns):\n    targets[col] = np.power(targets[col], powers[i])","cf9a833a":"targets = pd.read_csv(opj(path, 'train_scores.csv')).set_index('Id')","4858700b":"# Creating the rotated domain2 targets\nrot = 0.90771256655\nd2 = rotate_origin(targets.iloc[:, 3].values, targets.iloc[:, 4].values, rot)\ntargets['d21_rot'] = d2[:, 0]\ntargets['d22_rot'] = d2[:, 1]","23fc30bf":"pow_age = 1.0\npow_d1v1 = 1.5\npow_d1v2 = 1.5\npow_d2v1 = 1.5\npow_d2v2 = 1.5\npow_d21 = 1.5\npow_d22 = 1\npowers = [pow_age, pow_d1v1, pow_d1v2, pow_d2v1, pow_d2v2, pow_d21, pow_d22 ]\n\nfor i, col in enumerate(targets.columns):\n    targets[col] = np.power(targets[col], powers[i])","ef4ed4ea":"from sklearn.preprocessing import StandardScaler\n\n# And last but not least, let's scale the target features using ab\nscaler = StandardScaler()\ntargets.iloc[:, :] = scaler.fit_transform(targets)\ntargets.head()","6461cad7":"# Extract ID to separate train and test set\ntrain_id = targets.index.values\nsample_submission = pd.read_csv(opj(path, 'sample_submission.csv'))\ntest_id = np.unique(sample_submission.Id.str.split('_', expand=True)[0].astype('int'))\nprint(train_id.shape, test_id.shape)","49e73751":"# Load ICs from the loading file and separate them into train and test set\ndf_ic = pd.read_csv(opj(path, 'loading.csv'))\nic_train = df_ic[df_ic.Id.isin(train_id)].set_index('Id')\nic_test = df_ic[df_ic.Id.isin(test_id)].set_index('Id')\nprint(ic_train.shape, ic_test.shape)","629bc5bd":"# Load FNCs from file and separate them into train and test set\ndf_fnc = pd.read_csv(opj(path, 'fnc.csv'))\nfnc_train = df_fnc[df_fnc.Id.isin(train_id)].set_index('Id')\nfnc_test = df_fnc[df_fnc.Id.isin(test_id)].set_index('Id')\nprint(fnc_train.shape, fnc_test.shape)","9db7a5a2":"def plot_corr_matrix(df_train, df_test, c_restrict=200):\n\n    # Correlation matrix for ICA components\n    fig, ax = plt.subplots(ncols=3, figsize=(20, 10))\n    abs_max = 1.0\n    sns.heatmap(df_train.iloc[:, :c_restrict].corr(), square=True, vmin=-abs_max, vmax=abs_max, cbar=False, ax=ax[0]);\n    sns.heatmap(df_test.iloc[:, :c_restrict].corr(), square=True, vmin=-abs_max, vmax=abs_max, cbar=False, ax=ax[1]);\n    sns.heatmap(df_train.iloc[:, :c_restrict].corr()-df_test.iloc[:, :c_restrict].corr(),\n                square=True, vmin=-0.33, vmax=0.33, cbar=False, ax=ax[2]);\n    ax[0].set_title('Train')\n    ax[1].set_title('Test')\n    ax[2].set_title('Difference (Train - Test)');","d2f98fcd":"# Correlation matrix for IC features\nplot_corr_matrix(ic_train, ic_test, c_restrict=100)","9ed9fb9b":"# Correlation matrix for FNC features\nplot_corr_matrix(fnc_train, fnc_test, c_restrict=100)","71224323":"def plot_corr_matrix_target(targets, df_train, c_restrict=100):\n\n    # Merge target and feature matrix\n    df_temp = pd.merge(targets.reset_index(), df_train.reset_index())\n    df_temp = df_temp.set_index('Id').iloc[:, :c_restrict]\n    \n    # Correlation matrix for ICA components\n    plt.figure(figsize=(16, 3))\n    sns.heatmap(df_temp.corr().iloc[:7, 7:], square=True,\n                vmin=-0.5, vmax=0.5, cbar=False, cmap='Spectral');","82fe7300":"# Correlation between IC features and targets\nplot_corr_matrix_target(targets, ic_train, c_restrict=100)","622c56ce":"# Correlation between FNC features and targets\nplot_corr_matrix_target(targets, fnc_train, c_restrict=100)","66646168":"# Show highest correlation with target variables and IC dataset\ndf_corr = pd.concat([np.abs(ic_train.corrwith(targets.iloc[:, i])).sort_values(ascending=False).reset_index(drop=True) for i in range(7)], axis=1)\ndf_corr.columns = targets.columns\ndf_corr.head(5)","b18a7e9f":"# Show highest correlation with target variables and FNC dataset\ndf_corr = pd.concat([np.abs(fnc_train.corrwith(targets.iloc[:, i])).sort_values(ascending=False).reset_index(drop=True) for i in range(7)], axis=1)\ndf_corr.columns = targets.columns\ndf_corr.head(5)","5b684326":"#Explore between features and targets\n# Number of columns to investigate\nn_invest = 10\nsns.pairplot(ic_train.iloc[:, :n_invest], diag_kind=\"kde\", corner=True);","d685024e":"sns.pairplot(fnc_train.iloc[:, :n_invest], diag_kind=\"kde\", corner=True);","db3c877f":"def plot_markers(key, df_temp, ncolmarker=5, split_at=5, plot_max=15):\n\n    # Restrict dataframe to first X features\n    df_temp = df_temp.iloc[:, :plot_max]\n\n    # Compute dataset selecters\n    ncolumns = np.arange(df_temp.shape[1])\n    selecter = np.split(ncolumns, ncolumns[::split_at][1:])\n\n    for s in selecter:\n\n        print(key, s)\n        df_temp.iloc[:, s].plot(kind='line',subplots=True, sharex=True, marker='.', lw=0,\n                                ms=10, markeredgecolor='k', markeredgewidth=0.3,\n                     figsize=(5 * ncolmarker, 4 * df_temp.iloc[:, s].shape[1]\/\/ncolmarker), layout=(-1,ncolmarker));\n        plt.show()","75dc24fb":"plot_markers('Visualization of IC features:', ic_train)","716741cf":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n!pip install pycaret --quiet","c62266c5":"from sklearn.model_selection import KFold\n\nfrom pycaret.regression import *","fc04d6f1":"BASE_PATH = '..\/input\/trends-assessment-prediction'\n\nfnc_df = pd.read_csv(f\"{BASE_PATH}\/fnc.csv\")\nloading_df = pd.read_csv(f\"{BASE_PATH}\/loading.csv\")\nlabels_df = pd.read_csv(f\"{BASE_PATH}\/train_scores.csv\")","e293d9ce":"fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")\nlabels_df[\"is_train\"] = True\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()\nprint(f'Shape of train data: {df.shape}, Shape of test data: {test_df.shape}')","4cebc362":"target_cols = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\ndf.drop(['is_train'], axis=1, inplace=True)\ntest_df = test_df.drop(target_cols + ['is_train'], axis=1)\n\n\n# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\nFNC_SCALE = 1\/500\ndf[fnc_features] *= FNC_SCALE\ntest_df[fnc_features] *= FNC_SCALE","639efbe5":"def get_train_data(target):\n    other_targets = [tar for tar in target_cols if tar != target]\n    train_df = df.drop( other_targets, axis=1)\n    return train_df","aea745fb":"blacklist_models = ['ransac', 'tr', 'rf', 'et', 'ada', 'gbr', 'xgboost', 'catboost']","3ceafd37":"target = target_cols[0]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    exclude = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","a525a22e":"br_age = create_model(\n    estimator='br',\n    fold=7\n)","04a4387d":"tuned_br_age = tune_model(\n    br_age,\n    optimize = 'MAE'\n)","64c61913":"plot_model(tuned_br_age,plot = 'learning')","3b3c544f":"plot_model(tuned_br_age, plot = 'residuals')","68f72e81":"plot_model(tuned_br_age,plot = 'feature')","2143269c":"predictions =  predict_model(tuned_br_age, data=test_df)","53d8a145":"predictions[['Id','Label']].head()","5c90d2f1":"target = target_cols[1]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    exclude = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","5bedc2cc":"target = target_cols[2]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    exclude = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","e27a7669":"target = target_cols[3]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    exclude = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","57aeb2be":"target = target_cols[4]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    exclude = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","b7165769":"models = []\n\ntarget_models_dict = {\n    'age': 'br',\n    'domain1_var1':'lr',\n    'domain1_var2':'llar',\n    'domain2_var1':'lr',\n    'domain2_var2':'lr',\n}\n\ndef tune_and_ensemble(target):\n    train_df = get_train_data(target)    \n    exp_reg = setup(\n        data = train_df,\n        target = target,\n        train_size=0.8,\n        numeric_imputation = 'mean',\n        silent = True\n    )\n    model_name = target_models_dict[target]\n    mod = create_model(model_name,fold=7)\n    tuned_model = tune_model(mod, fold=7, optimize = 'MAE')\n    model = ensemble_model(tuned_model, fold=7, optimize = 'MAE', choose_better = True)\n    return model","c454508a":"for target in target_cols:\n    model = tune_and_ensemble(target)\n    models.append(model)","b97fc6f3":"models","faca360c":"def finalize_model_pipeline(model, target):\n    # this will train the model on houldout data\n    finalize_model(model)\n    save_model(model, f'{target}_{target_models_dict[target]}', verbose=True)\n    # making predictions on test data\n    predictions = predict_model(model, data=test_df)\n    test_df[target] = predictions['Label'].values","2f0e8c2d":"for index, target in enumerate(target_cols):\n    model = models[index]\n    finalize_model_pipeline(model,target)","db9d93a2":"sub_df = pd.melt(test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")\nsub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\nsub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\nassert sub_df.shape[0] == test_df.shape[0]*5\n\nsub_df.to_csv(\"submission1.csv\", index=False)\n\nsub_df.head(15)","75c3468f":"models[0].get_params()","0adec7e1":"age_train_df = get_train_data('age')\npreds = predict_model(models[0], data=age_train_df)\npreds\n","4673bf9b":"np.mean(np.sum(np.abs(preds['age'] - preds['Label']), axis=0)\/np.sum(preds['age'], axis=0))","83cb34da":"predictions = []\nop = 'Label'\noverall = 0.0\nfor i in range(5):\n    train_df = get_train_data(target_cols[i])\n    preds = predict_model(models[i], data=train_df)\n    predictions.append(preds)\n    score = np.mean(np.sum(np.abs(preds[target_cols[i]] - preds[op]), axis=0)\/np.sum(preds[target_cols[i]], axis=0))\n    overall+=score\n    print(f'{target_cols[i]}: \\t{score}')\nprint(f\"Overall score = {overall\/5}\")","c8daa7b1":"We can see again the stratification of age, but what is more interesting is the relationship within domain2","2c255f60":"Age is almost normally distributed but for the other targets power transformations can be applied.","4d259637":"Just looking at the targets, most seems to be normal. Except for age, something is unique.So let us have a closer look.","f3bee1fe":"So the best value to rotate is taken as 0.90771256655","b123f0d0":"### Predicting the other targets:","67653777":"Observation 1\nSo the first hunch is correct. Domain 1 and domain 2 seem to be connected, as they both contain the same amount of missing values.","829134f4":"If we try to predict this feature, it might be worth it to restrict the predictions to these 33 unique values.","84902970":"#### domain2_var2","5ebdfd40":"#### Creating a Bayesian Ridge model","77a114a4":"#### domain1_var1","47f4ac90":"#### To get the best rotation, we have to plot the length of unique values that can be notices in variable 2 with every rotation and take the angle at which the number of unique values are minimum.","9195a4a4":"#### Function to finalize and save model","660156c2":"#### Function to tune and ensemble (Bagging) best model for each target:","ec083cd5":"#### Plotting the feature importance","03e09b90":"**OBSERVATIONS:**\n1. age          - Bayesian Ridge\n2. domain1_var1 - Linear Regression\n3. domain1_var2 - Lasso Least Angle Regression\n4. domain2_var1 - Linear Regression\n5. domain2_var2 - Linear Regression\n","c4b7b99f":"#### Plotting the Residuals","3d3f0926":"#### Prediction on age for test dataset","bd10d0e1":"#### domain1_var2","8e0d2b27":"Conclusion 1\nThe datasets seem to contain a few outliers. We will take care of them at the very end.","b9afb7a3":"#### Plotting the Learning Curve","cfc8d9b4":"## Prediction","0ca1ff6e":"Observation 1\nFor both feature types, the correlation with age seems to be the highest. Let's explore this in a bit more detail. What is the highest correlation features can reach with the 5 targets?","00196f9d":"### Firstly taking the Age column for prediction","24cccd6d":"### Loading datasets and making train and test datasets","d4a70d2a":"#### Function to rotate a point around the origin (0, 0).","33807954":"### Feature Exploration","d34083e6":"Observation 2\nWhile there is a slight correlation within domain 2, there doesn't seem to be one within domain 1. So, the targets were measured in the same domain, but do not necessarily encode a connected property. Except for this domain2 connection","eb65d60b":"#### Plotting Distribution of Target variables","23792d79":"#### Tuning the Bayesian Ridge model to optimize on MAE (metric for the competition)","b6dafcd2":"Not using a few types of regression as it takes a lot of time","996cf3fe":"As pycaret supports only a single column prediction at a time, a utility function is defined to get values of a given col.","565e0f94":"### Creating submission csv file","4d8aa247":"### Data Scaling","961aeb18":"Loading the targets and doing necessary rotation and power transformations as done before without excluding the NULL values.","a8ee64e1":"#### domain2_var1"}}