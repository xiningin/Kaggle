{"cell_type":{"e44ad1e2":"code","47f99807":"code","c2b91707":"code","ad02830e":"code","f30840b0":"code","87b1cb0d":"code","834ecdb3":"code","3f4ab37d":"code","670db236":"code","48d39b3a":"code","7652424f":"code","beed2828":"code","aeb33bb5":"code","e4492b8e":"code","a86bcaf7":"code","cf3e6d39":"code","3f9c7f57":"code","f0a69b81":"code","6eedffdd":"code","e817c5a4":"code","ba3a90ef":"code","e1cec1d8":"code","c61bb295":"code","ad8fe1c6":"code","a789bc6e":"code","fb690e7e":"code","d9e83d19":"code","c9e9c49d":"code","61dd256e":"code","5a0f0eee":"code","efa50c96":"code","d0fab5b7":"code","a0f2d7c4":"code","6850042c":"code","7118c489":"code","f2fdf115":"code","378970be":"code","200e1a8f":"code","29a21785":"markdown","baf6cfea":"markdown","bfea9877":"markdown","dbd201f6":"markdown","38614053":"markdown","b6ad4244":"markdown","a35771f7":"markdown","886906ab":"markdown","3de23651":"markdown","48f2fa8d":"markdown","6d6209fe":"markdown","f3ed83aa":"markdown","c59cbc1c":"markdown","12182428":"markdown","80be3897":"markdown","fcd50197":"markdown","c72934ff":"markdown","e54dc936":"markdown"},"source":{"e44ad1e2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import dates\n%matplotlib inline\nimport statsmodels.api as sm\nimport seaborn as sns","47f99807":"df_stores=pd.read_csv('\/kaggle\/input\/retaildataset\/stores data-set.csv')\ndf_features=pd.read_csv('\/kaggle\/input\/retaildataset\/Features data set.csv')\ndf_sales=pd.read_csv('\/kaggle\/input\/retaildataset\/sales data-set.csv')","c2b91707":"df_sales.head()","ad02830e":"df_features['Date'] = pd.to_datetime(df_features['Date'],format=\"%d\/%m\/%Y\")\ndf_sales['Date'] = pd.to_datetime(df_sales['Date'],format=\"%d\/%m\/%Y\")","f30840b0":"df1=df_stores.merge(df_sales,on='Store',how='right')\ndf1.sort_values(\"Date\")","87b1cb0d":"df_features.head()","834ecdb3":"df=df1.merge(df_features,on=['Store','Date','IsHoliday'],how='left')","3f4ab37d":"df.info()","670db236":"df.info()","48d39b3a":"df.columns","7652424f":"df.drop(columns=['MarkDown1', 'MarkDown2', 'MarkDown3',\n       'MarkDown4', 'MarkDown5'],inplace=True)","beed2828":"df.info()","aeb33bb5":"df['Type'].value_counts()","e4492b8e":"df[['Type','Dept','Size','Date']]=df[['Type','Dept','Size','Date']].fillna(method='ffill') #forward fill for categorical data\n# if needed, here i don't need","a86bcaf7":"df.head()","cf3e6d39":"df.columns","3f9c7f57":"from sklearn.impute import KNNImputer\nX = df[['Store', 'Size', 'Dept',\n       'Temperature', 'Fuel_Price','CPI']].values\nimputer = KNNImputer(n_neighbors=2)\ndf['CPI']=imputer.fit_transform(X)[:,-1] #if you got null value in CPI while joining or otherwise","f0a69b81":"X = df[['Store', 'Size', 'Dept',\n       'Temperature', 'Fuel_Price','Unemployment']].values\nimputer = KNNImputer(n_neighbors=2)\ndf['Unemployment']=imputer.fit_transform(X)[:,-1]  #if you got null values in Unemployement while joining or otherwise","6eedffdd":"#df.drop(df[df['Weekly_Sales'].isna()].index,inplace=True) # drop na of weekly sales","e817c5a4":"df['IsHolidayNumeric']=df['IsHoliday'].apply(lambda x:1 if (str(x)==\"True\") else 0) # converted holiday true false to 0 and 1 respectively","ba3a90ef":"df.describe()","e1cec1d8":"df2=df.groupby(by=['Date'], as_index=False)['Weekly_Sales'].sum()\nf_1 = plt.figure(figsize=(12,6), dpi=100)\nax_1 = f_1.add_axes([0.0, 0.0, 0.9, 0.9])\nax_1.set_ylabel('Weekly_Sales')\nax_1.plot(df2['Date'], df2['Weekly_Sales'])\nax_1.grid(True, color='0.6', dashes=(5,2,1,2))\nax_1.set_facecolor((0.9, 0.9, 0.9))","c61bb295":"df2=df.groupby(by=['Date'], as_index=False)['Weekly_Sales'].sum()\nf_1 = plt.figure(figsize=(12,6), dpi=100)\nax_1 = f_1.add_axes([0.0, 0.0, 0.9, 0.9])\nax_1.set_ylabel('Weekly_Sales')\nax_1.plot(df2['Date'], df2['Weekly_Sales'])\n\nfor x in df[df['IsHolidayNumeric']==1]['Date']:\n    ax_1.axvline(x=x, color='g', linewidth=0.5)","ad8fe1c6":"df2=df.groupby(by=['Date'], as_index=False)[['Weekly_Sales','Unemployment']].sum()\ndf2['Unemployment']=df2['Unemployment']*1000\nf_1 = plt.figure(figsize=(12,6), dpi=100)\nax_1 = f_1.add_axes([0.0, 0.0, 0.9, 0.9])\nax_1.set_ylabel('Weekly_Sales')\nax_1.plot(df2['Date'], df2['Weekly_Sales'])\nax_1.plot(df2['Date'], df2['Unemployment'])\nax_1.legend(labels=['Weekly_Sales','Unemployment'])","a789bc6e":"df2=df.groupby(by=['Date'], as_index=False)[['Weekly_Sales','Size']].sum()\ndf2['Size']=df2['Size']*0.1\nf_1 = plt.figure(figsize=(12,6), dpi=100)\nax_1 = f_1.add_axes([0.0, 0.0, 0.9, 0.9])\nax_1.set_ylabel('Weekly_Sales')\nax_1.plot(df2['Date'], df2['Weekly_Sales'])\nax_1.plot(df2['Date'], df2['Size'])\nax_1.legend(labels=['Weekly_Sales','Size'])","fb690e7e":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ndf2=df.groupby(by=['Date'], as_index=False)['Weekly_Sales'].sum()\ndf2=df2.set_index(\"Date\")\n#df2 = df2.asfreq('W')\n#df2 = df2.fillna(method='bfill')\n#df2.index=pd.DatetimeIndex(df2.index,freq=None)\n\nres = seasonal_decompose(df2['Weekly_Sales'], model='mul')\nplt.figure(figsize=(22,3))\nres.seasonal.plot(title='Seasonal');\nplt.figure(figsize=(22,3))\nres.trend.plot(title='Trend');\nplt.figure(figsize=(22,3))\nres.resid.plot(title='Residual');","d9e83d19":"df2=df.groupby(by=['Date'], as_index=False)[['Weekly_Sales','Size']].sum()\ndf1=df2.set_index(\"Date\")\ndf1['Weekly_Sales'].plot(figsize=(20,6))\ndf1.rolling(window=6).mean()['Weekly_Sales'].plot()","c9e9c49d":"df1=df.groupby(by=['Date','IsHolidayNumeric'], as_index=False)['Weekly_Sales'].sum()\ndf1=df1.set_index(\"Date\")","61dd256e":"train_df = df1['Weekly_Sales'].iloc[:120]\ntest_df = df1['Weekly_Sales'].iloc[120:]","5a0f0eee":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n\nfit_model = ExponentialSmoothing(train_df,\n                                  seasonal='mul',\n                                  seasonal_periods=52).fit()\nprediction = fit_model.forecast(23)\nprediction\n\n\ntest_df.plot(figsize=(12,6))\nprediction.plot()","efa50c96":"def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\nprint(\"Mean Absolute Percentage Error = {a}%\".format(a=mean_absolute_percentage_error(test_df,prediction)))","d0fab5b7":"def mse(y1, y2, axis=0):\n    y1_np = y1.to_numpy()\n    y2_np = y2.to_numpy()\n    return ((y1_np - y2_np) ** 2).mean(axis=axis)\n\nprint(\"Root mean squared error = {a}\".format(a=np.sqrt(mse(test_df, prediction, None))))","a0f2d7c4":"#!pip install pmdarima","6850042c":"from statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pmdarima import auto_arima\nauto_arima(df1['Weekly_Sales'],exog=df1['IsHolidayNumeric'], seasonal=True, m=52, trace=True).summary() #auto arima gives the best fit","7118c489":"train_df = df1.iloc[:120]\ntest_df = df1.iloc[120:]","f2fdf115":"model = SARIMAX(train_df['Weekly_Sales'],exog=train_df['IsHolidayNumeric'], order=(2,0,2),\n               seasonal_order=(1, 0, 0, 52))\nres = model.fit()","378970be":"prediction=res.predict(start='2012-05-25',end=\"2012-10-26\",exog=test_df['IsHolidayNumeric'])","200e1a8f":"def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\nprint(\"Mean Absolute Percentage Error = {a}%\".format(a=mean_absolute_percentage_error(test_df['Weekly_Sales'],prediction)))","29a21785":"#### Unemployment does not have a much difference on sales , we can safely say that there is no trend here","baf6cfea":"### Further you can try analysis with Unemployement, Temperature, CPI","bfea9877":"# Time-Series Analysis","dbd201f6":"#### we can see that in holidays there are peaks so we can say that people are more likely to go to stores on holidays or events that creates that holiday","38614053":"#### Looks like from october to december the sales went up which we can tell that there could be event or sale on product at that point of time. As they mention holiday days lets check on that","b6ad4244":"# Data Cleaning","a35771f7":"#### We can clearly see that there is some trend going on plus the data is seasonal which was clear from the first visualization but here is the confirmation that there is seasonality ","886906ab":"## Holt-Winters Model","3de23651":"**** Error percentage is more here than without exog variable so here exog variable have very less effect ****","48f2fa8d":"****We got error of just 1.762% that mean our model is fitted with good accuracy****","6d6209fe":"****Common column between sales and store tables are store column lets connect on that****","f3ed83aa":"##### An **exogenous** variable is one whose value is determined outside the model and is imposed on the model. In other words, variables that affect a model without being affected by it.","c59cbc1c":"#### Company size also dosen't have a much of a difference in sales, we can safely say that there is no trend here","12182428":"## Forecasting","80be3897":"Using KNN imputer method to fill the na value","fcd50197":"We will not be working woth markdown so lets remove markdown","c72934ff":"****Common tables on df1 and feature table are store, date and is holiday lets join on that****","e54dc936":"## Sarima with Exogenous variable (SARIMAX)"}}