{"cell_type":{"18906d64":"code","04a7c8a5":"code","debcbf08":"code","567e6b24":"code","3f469551":"code","88e40765":"code","f8d0d8bb":"code","fd39e69f":"code","27d1ca7b":"code","b9905dd5":"code","897ab039":"code","1af750b1":"code","7339a03b":"code","a35cc54b":"code","7e72e390":"code","568669a2":"code","2d8f3493":"code","951ae9cf":"code","d5e3be87":"code","212c7517":"code","85b63eae":"code","b5a694ee":"code","12b80e06":"code","01ff3b0b":"code","c72e1cb2":"code","72547af2":"code","e9cee452":"code","787ee1f7":"code","64fb3a68":"code","22692bde":"code","091d594f":"code","d6bd5474":"code","43371aa0":"code","35223ef0":"code","53ef79fc":"code","6325e457":"code","d7d6cf3e":"code","ac364a31":"code","9ef96363":"code","18db492d":"code","e60838e1":"code","445051e8":"code","3fbc44dc":"code","407f306a":"code","57411886":"code","cb622832":"code","a4268c30":"code","eb9a81ae":"code","ae59af0d":"code","91811671":"code","1cc5fa90":"code","8dc3c6da":"code","192ed1b2":"code","97dd29f3":"markdown","d8ca107b":"markdown","df76e3ca":"markdown","8816aa9f":"markdown","c5395814":"markdown","9ebe5f55":"markdown","606ec4de":"markdown","98d1ee5a":"markdown","1cbd700d":"markdown","f839a515":"markdown","530066c8":"markdown","2cbb1ca5":"markdown","3d3a45b4":"markdown","2819e437":"markdown","656a18a8":"markdown","c36708f2":"markdown"},"source":{"18906d64":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy.stats as stats\nfrom scipy.stats import norm, skew\n\nsns.set_style(\"white\")","04a7c8a5":"train_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nprint(train_data.shape, test_data.shape)\ntrain_data.head()","debcbf08":"train_data.info()","567e6b24":"total = train_data.isnull().sum().sort_values(ascending=False)\npercent = (train_data.isnull().sum()\/train_data.isnull().count() * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","3f469551":"train_data['SalePrice'].describe()","88e40765":"fig = plt.figure(figsize = (10,6))\nsns.boxplot(x=train_data['SalePrice'])","f8d0d8bb":"train_z_outliers = train_data[['SalePrice', 'LotFrontage', 'MasVnrArea']].copy()\nprint(train_z_outliers.shape)\n\nz = np.abs(stats.zscore(train_z_outliers['SalePrice']))\nprint(z)\n\nthreshold = 3\nprint(np.where(z > 3))","fd39e69f":"# train_z_outliers[(np.abs(stats.zscore(train_z_outliers)) < 3).all(axis=1)]\ntrain_z_outliers = train_z_outliers[(np.abs(stats.zscore(train_z_outliers['SalePrice'])) < 3)]\nprint(train_z_outliers.shape)","27d1ca7b":"train_iqr_outliers = train_data[['SalePrice', 'LotFrontage', 'MasVnrArea']].copy()\nprint(train_iqr_outliers.shape)\n\nQ1 = train_iqr_outliers['SalePrice'].quantile(0.25)\nQ3 = train_iqr_outliers['SalePrice'].quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)\n\ntrain_iqr_outliers[(train_iqr_outliers['SalePrice'] < (Q1 - 1.5 * IQR))|(train_iqr_outliers['SalePrice'] > (Q3 + 1.5 * IQR))]\n\n# IQR = train_outliers['SalePrice'].quantile(0.75) - train_outliers['SalePrice'].quantile(0.25)\n# lower_boundary = df[variable].quantile(0.25) - (IQR * 1.5)\n# upper_boundary = df[variable].quantile(0.75) + (IQR * 1.5)","b9905dd5":"train_iqr_outliers = train_iqr_outliers[~((train_iqr_outliers['SalePrice'] < (Q1 - 1.5 * IQR))|(train_iqr_outliers['SalePrice'] > (Q3 + 1.5 * IQR)))]\nprint(train_iqr_outliers.shape)","897ab039":"from sklearn.ensemble import IsolationForest\n\ntrain_iso_outliers = train_data[['SalePrice', 'LotFrontage', 'MasVnrArea']].copy()\ntrain_iso_outliers['LotFrontage'] = train_iso_outliers['LotFrontage'].fillna(0)\ntrain_iso_outliers['MasVnrArea'] = train_iso_outliers['MasVnrArea'].fillna(0)\nprint(train_iso_outliers.shape)\n\niso = IsolationForest(contamination=0.1)\nyhat = iso.fit_predict(train_iso_outliers)\n\nmask = yhat != -1\ntrain_iso_outliers=train_iso_outliers[mask]\nprint(train_iso_outliers.shape)","1af750b1":"from sklearn.covariance import EllipticEnvelope\n\ntrain_ee_outliers = train_data[['SalePrice', 'LotFrontage', 'MasVnrArea']].copy()\ntrain_ee_outliers['LotFrontage'] = train_ee_outliers['LotFrontage'].fillna(0)\ntrain_ee_outliers['MasVnrArea'] = train_ee_outliers['MasVnrArea'].fillna(0)\nprint(train_ee_outliers.shape)\n\nee = EllipticEnvelope(contamination=0.01)\nyhat = ee.fit_predict(train_ee_outliers)\n\nmask = yhat != -1\ntrain_ee_outliers = train_ee_outliers[mask]\nprint(train_ee_outliers.shape)","7339a03b":"from sklearn.neighbors import LocalOutlierFactor\n\ntrain_lof_outliers = train_data[['SalePrice', 'LotFrontage', 'MasVnrArea']].copy()\ntrain_lof_outliers['LotFrontage'] = train_lof_outliers['LotFrontage'].fillna(0)\ntrain_lof_outliers['MasVnrArea'] = train_lof_outliers['MasVnrArea'].fillna(0)\nprint(train_lof_outliers.shape)\n\nlof = LocalOutlierFactor()\nyhat = lof.fit_predict(train_lof_outliers)\n\nmask = yhat != -1\ntrain_lof_outliers = train_lof_outliers[mask]\nprint(train_lof_outliers.shape)","a35cc54b":"def diagnostic_plots(df, variable):\n    fig, ax = plt.subplots(1,2,figsize=(20,6))\n    sns.histplot(ax=ax[0], data=df, x=variable, bins=50)\n    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n    \ndiagnostic_plots(train_data, 'SalePrice')\n\nprint(f\"Skewness: {train_data['SalePrice'].skew()}\")\nprint(f\"Kurtosis: {train_data['SalePrice'].kurt()}\")","7e72e390":"train_data[\"SalePrice_log\"] = np.log(train_data[\"SalePrice\"])\n\n(mu, sigma) = norm.fit(train_data['SalePrice_log'])\nprint(f'mu = {mu:.2f} and sigma = {sigma:.2f}')\n\ndiagnostic_plots(train_data, \"SalePrice_log\")","568669a2":"train_data[\"SalePrice_sqrt\"] = np.sqrt(train_data[\"SalePrice\"])\n\n(mu, sigma) = norm.fit(train_data['SalePrice_sqrt'])\nprint(f'mu = {mu:.2f} and sigma = {sigma:.2f}')\n\ndiagnostic_plots(train_data, \"SalePrice_sqrt\")","2d8f3493":"train_data[\"SalePrice_power\"] = np.power(train_data[\"SalePrice\"], 0.3)\n\n(mu, sigma) = norm.fit(train_data['SalePrice_power'])\nprint(f'mu = {mu:.2f} and sigma = {sigma:.2f}')\n\ndiagnostic_plots(train_data, \"SalePrice_power\")","951ae9cf":"train_data[\"SalePrice_boxcox\"], param = stats.boxcox(train_data[\"SalePrice\"])\nprint('Optimal \u03bb: ', param)\n\n# from scipy.special import boxcox\n# train_data[\"SalePrice_boxcox\"] = boxcox(train_data[\"SalePrice\"], -0.07)\n\n(mu, sigma) = norm.fit(train_data['SalePrice_boxcox'])\nprint(f'mu = {mu:.2f} and sigma = {sigma:.2f}')\n\ndiagnostic_plots(train_data, \"SalePrice_boxcox\")","d5e3be87":"train_data[\"SalePrice_yeojohnson\"], param = stats.yeojohnson(train_data[\"SalePrice\"])\nprint('Optimal \u03bb: ', param)\n\n(mu, sigma) = norm.fit(train_data['SalePrice_yeojohnson'])\nprint(f'mu = {mu:.2f} and sigma = {sigma:.2f}')\n\ndiagnostic_plots(train_data, \"SalePrice_yeojohnson\")","212c7517":"from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='yeo-johnson')\n\ntrain_data[\"SalePrice_yeojohnson2\"] = pt.fit_transform(train_data[['SalePrice']])\n\nprint(pt.lambdas_[0])\n\ntrain_data[\"SalePrice_inv_yeojohnson2\"] = pt.inverse_transform(train_data[['SalePrice_yeojohnson2']])\n\ndiagnostic_plots(train_data, \"SalePrice_yeojohnson2\")\ndiagnostic_plots(train_data, \"SalePrice_inv_yeojohnson2\")","85b63eae":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF']\nsns.pairplot(train_data[cols], height=2.5)","b5a694ee":"corr = train_data.corr(method = \"pearson\")\n# corr = train_data.corr(method = \"spearman\")\n# corr = train_data.corr(method = \"kendall\")\n\nf, ax = plt.subplots(figsize=(27, 27))\n\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax, annot=True);","12b80e06":"nunique=train_data.nunique().sort_values(ascending=False)\nnunique[:20]","01ff3b0b":"for col in train_data.columns:\n    if train_data[col].isnull().values.any():\n        unique=train_data[col].unique()\n#         print(f'{col}: {unique}')","c72e1cb2":"cols=[\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \n      'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageYrBlt', 'GarageArea', 'GarageCars', \n      'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtQual', 'BsmtCond', \n      'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'TotalBsmtSF',\n      'MasVnrType', 'Electrical', 'LotFrontage', 'MasVnrArea', 'FireplaceQu']\n    \nfor col in cols:\n    train_data[col] = train_data[col].fillna(0)\n#     print(f'{col}: {train_data[col].unique()}')","72547af2":"from sklearn.preprocessing import LabelEncoder\n\ncols=['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', \n      'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', \n      'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', \n      'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \n      'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', \n      'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', \n      'MiscFeature', 'SaleType', 'SaleCondition', 'MSSubClass', 'OverallCond', 'YrSold', 'MoSold']\n\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(train_data[c].values))\n    train_data[c] = lbl.transform(list(train_data[c].values))\n#     print(f'{c}: {train_data[c].unique()}')","e9cee452":"train_data['LotFrontage'] = train_data['LotFrontage'].astype(int)\ntrain_data['GarageYrBlt'] = train_data['GarageYrBlt'].astype(int)","787ee1f7":"train_data['TotalSF'] = train_data['TotalBsmtSF'] + train_data['1stFlrSF'] + train_data['2ndFlrSF']","64fb3a68":"numeric_feats = train_data.dtypes[train_data.dtypes != \"object\"].index\n\nskewed_feats = train_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(40)","22692bde":"skewness = skewness[abs(skewness) > 0.75]\n\nskewed_features = skewness.index\n\nfor feat in skewed_features:\n    train_data[feat],param = stats.yeojohnson(train_data[feat])\n#     print(f'{feat} - Optimal \u03bb: {param}')","091d594f":"train_data.columns","d6bd5474":"X = train_data[['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n                'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', \n                'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', \n                'ExterQual', 'ExterCond', 'Foundation', 'Functional', 'Fireplaces', 'FireplaceQu', \n                'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', \n                'Heating', 'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', \n                'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', \n                'GarageType','GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual','GarageCond', \n                'PavedDrive', 'WoodDeckSF', 'OpenPorchSF','EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n                'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType','SaleCondition','SalePrice_yeojohnson','TotalSF']]\n\ny = train_data['SalePrice_yeojohnson']","43371aa0":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scl = scaler.fit_transform(X)","35223ef0":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scl, y, test_size=0.2, random_state=0)","53ef79fc":"from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.linear_model import SGDRegressor, LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.cross_decomposition import PLSRegression\n\nfrom sklearn.model_selection import cross_validate, cross_val_score\nfrom sklearn.metrics import explained_variance_score, max_error, mean_absolute_error, r2_score, explained_variance_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, mean_squared_error, mean_squared_log_error\nfrom sklearn.metrics import median_absolute_error, mean_poisson_deviance, mean_gamma_deviance\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6325e457":"models=[(\"Linear Regression\", LinearRegression()),\n        (\"Ridge Regression\", Ridge()),\n        (\"Lasso Regression\", Lasso()),\n        (\"Elastic-Net Regression\", ElasticNet()),\n        (\"Stochastic Gradient Descent\", SGDRegressor()),\n        (\"Gaussian Process Regressor\", GaussianProcessRegressor()),\n        (\"PLS Regression\", PLSRegression()),\n        (\"Decision Tree\", DecisionTreeRegressor()),\n        (\"Random Forest\", RandomForestRegressor()),\n        (\"Extra Trees\", ExtraTreesRegressor()),\n        (\"Gradient Boostin\", GradientBoostingRegressor()),\n        (\"XGBoost\", XGBRegressor()),\n        (\"LightGBM\", LGBMRegressor()),\n        (\"Ada Boost\", AdaBoostRegressor()),\n        (\"KNeighbors\", KNeighborsRegressor()),\n        (\"SVM linear\", SVR(kernel='linear')),\n        (\"SVM rbf\", SVR(kernel='rbf'))]\n\nfor name, model in models:\n    results = cross_val_score(model, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n    print(f\"\\x1b[96m{name}\\x1b[0m: \\x1b[93m{results.mean():.6f}\\x1b[0m \u00b1 {results.std():.4f}\")","d7d6cf3e":"# features = np.arange(1, 50, 1)\n# results = []\n\n# for feature in features:\n#     rf = GradientBoostingRegressor(max_features=79, max_depth=3, n_estimators=72, random_state=feature)\n#     rf.fit(X_train, y_train)\n    \n#     results.append(mean_squared_error(y_test, rf.predict(X_test), squared=False))\n\n# fig, ax = plt.subplots(figsize=(25,8)) \n# plt.plot(features, results, 'b')\n\n# ax.set_axisbelow(True)\n# ax.minorticks_on()\n# ax.grid(which='major', linestyle='-', linewidth=0.5, color='black',)\n# ax.grid(which='minor', linestyle=':', linewidth=0.5, color='black', alpha=0.7)\n\n# plt.gca().xaxis.set_major_locator(plt.MultipleLocator(5))\n\n# print(results[results.index(min(results))])\n# print(features[results.index(min(results))])","ac364a31":"line = LinearRegression()\nline.fit(X_train, y_train)\n\nline_predict = line.predict(X_test)\nprint(f\"r2_score: {r2_score(y_test, line_predict):.7f}\")\nprint(f\"mean_absolute_error: {mean_absolute_error(y_test, line_predict):.7f}\")\nprint(f\"mean_squared_error: {mean_squared_error(y_test, line_predict):.7f}\")\nprint(f\"root_mean_squared_error: {mean_squared_error(y_test, line_predict, squared=False):.7f}\")\nprint(f\"max_error: {max_error(y_test, line_predict):.7f}\")","9ef96363":"r2 = line.score(X_test, y_test)\nn = X_test.shape[0]\np = X_test.shape[1]\n\nadjusted_r2 = 1-(1-r2)*(n-1)\/(n-p-1)\nprint(f\"Adjusted R2: {adjusted_r2}\")","18db492d":"from sklearn.feature_selection import f_regression\n\np_values = f_regression(X_test, y_test)[1]\np_values.round(3)","e60838e1":"line_summary = pd.DataFrame(data = X.columns.values, columns=['Features'])\nline_summary['Coefficients'] = line.coef_\nline_summary['p-values'] = p_values.round(3)\nline_summary","445051e8":"rdg = Ridge()\nrdg.fit(X_train, y_train)\n\nrdg_predict = rdg.predict(X_test)\nprint(f\"r2_score: {r2_score(y_test, rdg_predict):.7f}\")\nprint(f\"mean_absolute_error: {mean_absolute_error(y_test, rdg_predict):.7f}\")\nprint(f\"mean_squared_error: {mean_squared_error(y_test, rdg_predict):.7f}\")\nprint(f\"root_mean_squared_error: {mean_squared_error(y_test, rdg_predict, squared=False):.7f}\")\nprint(f\"max_error: {max_error(y_test, rdg_predict):.7f}\")","3fbc44dc":"rf = RandomForestRegressor(max_features=79, max_depth=3, n_estimators=72, random_state=49, n_jobs=-1)\nrf.fit(X_train, y_train)\n\nrf_predict = rf.predict(X_test)\nprint(f\"r2_score: {r2_score(y_test, rf_predict):.7f}\")\nprint(f\"mean_absolute_error: {mean_absolute_error(y_test, rf_predict):.7f}\")\nprint(f\"mean_squared_error: {mean_squared_error(y_test, rf_predict):.7f}\")\nprint(f\"root_mean_squared_error: {mean_squared_error(y_test, rf_predict, squared=False):.7f}\")\nprint(f\"max_error: {max_error(y_test, rf_predict):.7f}\")","407f306a":"gr_boosting = GradientBoostingRegressor(max_features=50, max_depth=3, n_estimators=72, random_state=49)\ngr_boosting.fit(X_train, y_train)\n\ngr_predict = gr_boosting.predict(X_test)\nprint(f\"r2_score: {r2_score(y_test, gr_predict):.7f}\")\nprint(f\"mean_absolute_error: {mean_absolute_error(y_test, gr_predict):.7f}\")\nprint(f\"mean_squared_error: {mean_squared_error(y_test, gr_predict):.7f}\")\nprint(f\"root_mean_squared_error: {mean_squared_error(y_test, gr_predict, squared=False):.7f}\")\nprint(f\"max_error: {max_error(y_test, gr_predict):.7f}\")","57411886":"feature_importance = gr_boosting.feature_importances_[20:]\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfig = plt.figure(figsize=(17, 8))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, np.array(X.columns)[sorted_idx])\nplt.title('Feature Importance')","cb622832":"gr_results = cross_validate(gr_boosting, X_scl, y, cv=10, \n                            scoring=('r2', 'neg_mean_squared_error', 'neg_mean_absolute_error', 'neg_root_mean_squared_error'), n_jobs=-1)\n\nprint(f\"r2_score: {gr_results['test_r2'].mean():.7f}\")\nprint(f\"mean_absolute_error: {gr_results['test_neg_mean_absolute_error'].mean():.7f}\")\nprint(f\"mean_squared_error: {gr_results['test_neg_mean_squared_error'].mean():.7f}\")\nprint(f\"root_mean_squared_error: {gr_results['test_neg_root_mean_squared_error'].mean():.7f}\")","a4268c30":"xgb_r = XGBRegressor()\nxgb_r.fit(X_train, y_train)\n\nxgb_predict = xgb_r.predict(X_test)\nprint(f\"r2_score: {r2_score(y_test, xgb_predict):.7f}\")\nprint(f\"mean_absolute_error: {mean_absolute_error(y_test, xgb_predict):.7f}\")\nprint(f\"mean_squared_error: {mean_squared_error(y_test, xgb_predict):.7f}\")\nprint(f\"root_mean_squared_error: {mean_squared_error(y_test, xgb_predict, squared=False):.7f}\")\nprint(f\"max_error: {max_error(y_test, xgb_predict):.7f}\")","eb9a81ae":"lgb = LGBMRegressor()\nlgb.fit(X_train, y_train)\n\nlgb_predict = lgb.predict(X_test)\nprint(f\"r2_score: {r2_score(y_test, lgb_predict):.7f}\")\nprint(f\"mean_absolute_error: {mean_absolute_error(y_test, lgb_predict):.7f}\")\nprint(f\"mean_squared_error: {mean_squared_error(y_test, lgb_predict):.7f}\")\nprint(f\"root_mean_squared_error: {mean_squared_error(y_test, lgb_predict, squared=False):.7f}\")\nprint(f\"max_error: {max_error(y_test, lgb_predict):.7f}\")","ae59af0d":"from sklearn.ensemble import VotingRegressor\n\nvote = VotingRegressor(estimators=[('rdg', rdg), ('rf', rf), ('grb', gr_boosting), ('xgb', xgb_r), ('lgb', lgb)], n_jobs=-1)\nvote = vote.fit(X_train, y_train)\n\nvote_predict = vote.predict(X_test)\nprint(f\"r2_score: {r2_score(y_test, vote_predict):.7f}\")\nprint(f\"mean_absolute_error: {mean_absolute_error(y_test, vote_predict):.7f}\")\nprint(f\"mean_squared_error: {mean_squared_error(y_test, vote_predict):.7f}\")\nprint(f\"root_mean_squared_error: {mean_squared_error(y_test, vote_predict, squared=False):.7f}\")\nprint(f\"max_error: {max_error(y_test, vote_predict):.7f}\")","91811671":"from sklearn.ensemble import StackingRegressor\n\nsr = StackingRegressor(estimators=[('rdg', rdg), ('rf', rf), ('grb', gr_boosting), ('xgb', xgb_r), ('lgb', lgb)], \n                       final_estimator=RandomForestRegressor(n_estimators=10, random_state=49), n_jobs=-1)\n\nsr = sr.fit(X_train, y_train)\n\nsr_predict = sr.predict(X_test)\nprint(f\"r2_score: {r2_score(y_test, sr_predict):.7f}\")\nprint(f\"mean_absolute_error: {mean_absolute_error(y_test, sr_predict):.7f}\")\nprint(f\"mean_squared_error: {mean_squared_error(y_test, sr_predict):.7f}\")\nprint(f\"root_mean_squared_error: {mean_squared_error(y_test, sr_predict, squared=False):.7f}\")\nprint(f\"max_error: {max_error(y_test, sr_predict):.7f}\")","1cc5fa90":"from sklearn.decomposition import PCA\n\nprint(X_scl.shape)\n\npca = PCA(n_components=50)\nX_new = pca.fit_transform(X_scl)\nprint(X_new.shape)\n# print(pca.explained_variance_)\n# print(pca.explained_variance_ratio_)\n# print(pca.singular_values_)","8dc3c6da":"X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=0)","192ed1b2":"gr_boosting = GradientBoostingRegressor(random_state=0)\ngr_boosting.fit(X_train, y_train)\n\ngr_predict = gr_boosting.predict(X_test)\nprint(f\"r2_score: {r2_score(y_test, gr_predict):.7f}\")\nprint(f\"mean_absolute_error: {mean_absolute_error(y_test, gr_predict):.7f}\")\nprint(f\"mean_squared_error: {mean_squared_error(y_test, gr_predict):.7f}\")\nprint(f\"root_mean_squared_error: {mean_squared_error(y_test, gr_predict, squared=False):.7f}\")\nprint(f\"max_error: {max_error(y_test, gr_predict):.7f}\")","97dd29f3":"## Transforming variables with square and cube root\nThe square and cube root transformations are two specific forms of power transformations\nwhere the exponents are 1\/2 and 1\/3, respectively.","d8ca107b":"## IQR score","df76e3ca":"## Principal Component Analysis","8816aa9f":"## Transforming variables with Yeo-Johnson function\nThe Yeo-Johnson transformation is an extension of the Box-Cox transformation and can be\nused on variables with zero and negative values, as well as positive values.","c5395814":"## Local Outlier Factor","9ebe5f55":"# Transformation of a variable","606ec4de":"## Transforming variables with power function","98d1ee5a":"# Dimensionality reduction","1cbd700d":"## Minimum Covariance Determinant","f839a515":"# Outliers","530066c8":"# Model","2cbb1ca5":"## Transforming variables with Box-Cox function","3d3a45b4":"## Transforming variable with the logarithm\nThe logarithm function is commonly used to transform variables. It has a strong effect on the shape of the variable distribution and can only be applied to positive variables.","2819e437":"## Isolation Forest","656a18a8":"# Imputing missing values","c36708f2":"## Z-score"}}