{"cell_type":{"14673ab9":"code","a2b56fd8":"code","edf8afc5":"code","e7a58708":"code","681c3274":"code","85592514":"code","d92f5de0":"code","5332babd":"code","5b417f3b":"code","596250ed":"code","fae11f97":"code","3e8984dc":"code","651f37bd":"code","5359132e":"code","fa5dfda1":"code","5d5c8274":"code","31a472b5":"code","5dc797b9":"code","700c0e78":"code","893cc73f":"code","4e15009e":"code","a56eb1f6":"code","06edb0ee":"code","978fe726":"code","8058afc3":"code","2b74cb35":"code","b1b79089":"code","8ef4ef51":"code","3e83e153":"code","bd74d52e":"code","d25e1c3e":"code","74983320":"code","fbb6849d":"code","363c4acd":"code","11c97872":"code","2ad73b98":"code","8fb0344d":"code","23532cb3":"code","4e81277b":"code","4a3a9ba8":"code","75d98c4e":"code","db671bd5":"code","179f5835":"code","161043cb":"code","34f0d530":"code","d3408052":"code","5301ee57":"code","29355f49":"code","3601f988":"code","ad0c4de4":"code","29ce40e4":"code","e17190dd":"code","31098252":"code","eafa3bea":"code","dc8718d6":"code","bbc2f150":"code","8b5d9efb":"code","31d4f7ce":"code","032ad768":"code","f1aab2fa":"code","0e746530":"code","86995f58":"code","7eebc329":"code","743c63cd":"code","a39c63b0":"code","a2c665b3":"code","79cd9049":"code","b9aa8281":"code","706233a1":"code","da8c1402":"code","32505ffd":"code","61a7ee0d":"code","5d53ffb3":"code","897a81d5":"code","1d5aed2e":"code","3414e393":"code","4763cbac":"code","4c341e32":"code","15ef8278":"code","a458b4c3":"code","dc4b8b89":"code","f6f1a696":"code","870927a7":"code","684f99c6":"code","96373604":"code","7f16cfd2":"code","8157e5a0":"code","486efb08":"code","2e7baf4f":"code","2d12c94a":"code","9b17bb07":"code","1fd91d0a":"code","ee68dff4":"code","0eb11176":"code","8c14014a":"code","8398938c":"code","0e8b02e9":"code","fd2f1439":"code","adeb0590":"code","ba59c8b8":"code","41531e5c":"code","903dd24d":"code","65b51401":"code","fea37f47":"code","34cbcb14":"code","cf3e33ca":"code","8a92b6b1":"code","ebfef9de":"code","986eca2c":"code","953af429":"code","74173424":"code","a84a8d1a":"code","8625435e":"code","688a90ea":"code","24394a00":"code","6f1014f1":"code","c195983d":"code","e00a7083":"code","899099fb":"code","291c2fba":"code","e7353629":"code","e9b3e196":"code","38126281":"code","23b3480a":"code","222788af":"code","5b2622ad":"code","f50b1483":"code","f2b9094c":"code","7154cf91":"code","929f111c":"code","dd4c1266":"code","5f1fca36":"code","4c6bc11c":"code","452cb1e9":"code","9b18b880":"code","bed11703":"code","bf415a21":"code","b023b355":"code","83e98706":"code","e96a4ada":"code","eff10ada":"code","c4f76497":"code","b7f6b935":"code","8d1b7c2b":"code","bd1eb02a":"code","7e7a828c":"code","0bf63c3e":"code","372259dd":"code","7baf9123":"code","ef55ddca":"code","723a0b18":"code","7412f3db":"code","6eaa45b2":"code","5b00b080":"code","2ee9dd90":"code","bddeda58":"code","a9915b0a":"code","ecd251e6":"code","40e08c36":"code","332e9ecb":"code","68c1e026":"code","b386a96b":"code","5adbdae6":"code","dc2367c6":"code","1862dfcf":"code","8a1b2f59":"code","9c8da0a2":"code","b78fb223":"code","7b74f798":"code","166d7372":"code","d86088f3":"code","a0ccb2b8":"code","1a9c1c72":"code","c49dea66":"code","15aa38ce":"code","fcef427f":"code","d4ee51ba":"code","909a0f37":"code","34dfe275":"code","0de2667c":"code","ce320f6e":"code","6c7aa86d":"code","e90957be":"code","25b57f1f":"code","77def0ea":"code","1b1fc0fd":"code","8511e83d":"code","1953e09c":"code","f1977422":"code","015e5180":"code","12deaa8f":"code","4bda12de":"code","4a40d922":"code","51f84807":"markdown","8122b738":"markdown","2261742a":"markdown","1e906b59":"markdown","3dd450e0":"markdown","5fe0bc48":"markdown","e0edc18e":"markdown","b7184d63":"markdown","dbb67c87":"markdown","08ac903f":"markdown","3a887d66":"markdown","e3a06c96":"markdown","3f916950":"markdown","ba6ae363":"markdown","ff319d00":"markdown","19cd0d1a":"markdown","e76ce43e":"markdown","fbb93fca":"markdown"},"source":{"14673ab9":"#ATTRIBUTES:\n#Email: Email of the customer\n#Address: Address of the customer\n#Avatar: Avatar chosen by the customer\n#Avg. Session Length: Average duration of the online session\n#Time on App: Time spent on App\n#Time on Website: Time spent on website\n#Length of Membership: Time period of membership\n#Yearly Amount Spent: Yearly amount spent by the customer\n","a2b56fd8":"import pandas as pd\nimport seaborn as sns\nimport statsmodels.formula.api as smf\nimport numpy as np\nimport statsmodels.api as sm\n%matplotlib inline\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt   \nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","edf8afc5":"df = pd.read_csv('..\/input\/Ecommerce.csv')","e7a58708":"df.head()","681c3274":"df.shape   ##Rows, Columns","85592514":"df.dtypes ##Datatypes","d92f5de0":"df.head()","5332babd":"df.describe().T ##Summary for Quantitative Variables","5b417f3b":"##Above we can see the summary that contains mean, count, min, max. etc. values from the quantitative variables","596250ed":"df.isna().sum()","fae11f97":"##We don't see any missing values in the data","3e8984dc":"df.describe(include=object).T ##Summary for Qualitative Variables","651f37bd":"for i in list(df.dtypes[df.dtypes==object].index):\n    print(i,'\\n\\n',(df[i].value_counts()\/df[i].shape[0])*100,'\\n\\n\\n\\n')","5359132e":"##We see around 0.2% count of values in Email and Address, which shows that they are unique\n##for Avatar, not all values are unique,and a few values are present as much as 1.4% in the data","fa5dfda1":"##We see that address and email is unique for all the people, even though it is a qualitative variable\n##Address and email won't really help us in predictions.\n##Address, as a whole, could help in deciding which area has how much spending\n\n##Avatar is a variable which has multiple counts for different people, so it looks like an important variable at the moment","5d5c8274":"for i in list(df.dtypes[df.dtypes==object].index):  ##Unique values in Categorical variables\n    print(i,'\\n\\n',df[i].nunique(),'\\n\\n')","31a472b5":"#Checking for null values\ndf.isna().sum()","5dc797b9":"##No missing values","700c0e78":"#Checking outliers\n\n#For quantitative variables\n\nfor i in list(df.dtypes[df.dtypes!=object].index):\n    print(i)\n    sns.boxplot(data=df,x=i,orient='v')\n    plt.show()","893cc73f":"##Our target variable, that is , Yearly amount spend, has quite a few outliers but we need to consider the outliers\n##for the independant variableS:\n#for the independant variables, Length of Membership has a lot of outliers, and Avg. Session Length and Time on App. have few\n#outliers. We might remove outliers from some of the independant variables, at a later stage in our analysis","4e15009e":"#Checking how the data is distributed:\n\n##Let us check:\nsns.distplot(df['Yearly Amount Spent'])\nplt.show()","a56eb1f6":"##Data ooks normally distributed. Let us check now:\n#We will use the Jarque Bera test to check the normality of our target\n\n#H0: Target normally distributed\n#H1: Target not normally distributed\n#Checking At 5% level of significance\n\nfrom scipy import stats\nprint(stats.jarque_bera(df['Yearly Amount Spent']))","06edb0ee":"##So, the Pvalue in our case is 0.1182510779254693, which is more than 0.05 (Level of significance),and hence we will\n#accept the null hypothesis, which states that Target is normally distributed","978fe726":"df.corr()","8058afc3":"sns.heatmap(df.corr(),annot=True)\nplt.show()","2b74cb35":"##We see that Length of Membership is highly correlated with the Target:Yearly Amount Spent(more than 0.5 value of correlation)\n##Time on App's correlation with Target(Yearly Amount Spent) is 0.5(which is not very high, but still it should be kept in \n#check while building the model)","b1b79089":"##As of now, we see that none of the independant variables are much correlated with each other. WE can check this using \n#as plot as well:\nsns.heatmap(df.drop(columns='Yearly Amount Spent').corr(),annot=True)\nplt.show()","8ef4ef51":"##We can clearly see here that the independant variables are not much correlated with each other.\n#Let's check pairplot as well\nsns.pairplot(data=df)\nplt.show()","3e83e153":"##Based on this analysis alone, I choose not to discard any variables at the moment. Yes, Length of memberships is\n#highly correlated with the Yearly amount spent, but for time being, I will allow it in my model. Later, if needed,\n#we can remove Length of Membership to check accuracy again. But for now, we'll continue with all variables.\n\n#Of course, we will drop\/modify certain categorical variables, which aren't seen here(seen the above shows only quantitative\n#variables).","bd74d52e":"for i in df.drop(columns='Yearly Amount Spent').columns:\n    sns.scatterplot(data=df,x=i,y='Yearly Amount Spent')\n    plt.show()","d25e1c3e":"## We can see that the relationships are not linear\n#We can see that all the variables except Length of Membership, are evenly scattered, with Yearly Amount Spent, as the Target\n##Even for Length of Membership, the relationship can't be called linear, but the correlation is very high","74983320":"##Based on the above, we can later transform or combine features to extract more information out of them","fbb6849d":"##For example, we can see that graphs for email and Address look a bit similar, and that is because of the randomness of it\n##We can transform Address in this case(to check for particular Areas)\n#and we can actually drop Email, because it Email IDs #won't really help in predictions\n##But, at the same time, we can also check the email provider and then maybe that can help us make predictions better\n##Avatar can be used as it is for the time being, and later maybe we can combine it with Area where the person resides,\n#or with some other variable, to extract the interaction of certain variables\n#Average Session length, Time on App, and Time on Website can be used as it is, because they will act as good precitors,\n#since they are not totally random, and at the same time they are not much correlated with the Target variable\n##for now, we will use Length of Membership in our initial model, but later we can think of dropping it, since it is \n#highly correlated with the Target","363c4acd":"df.head()","11c97872":"##Transforming Email:\ndf['Email'] = df['Email'].apply(lambda d: d.split('@')[1])","2ad73b98":"df['Email'].value_counts()","8fb0344d":"df['Area'] = df['Address'].apply(lambda d: d.split(',')[0])","23532cb3":"df['Area'] = df['Area'].apply(lambda d: d.split('\\n')[-1])","4e81277b":"df['State'] = df['Address'].apply(lambda d: d.split(',')[-1][:3])","4a3a9ba8":"df.head()","75d98c4e":"##Dropping Address: since we can already transformed features:\ndf = df.drop(columns='Address')","db671bd5":"df.head()","179f5835":"##Let us check quantitative summary again:\ndf.dtypes","161043cb":"df.describe(include=object)","34f0d530":"##Checking value counts and scatterplot again:\nfor i in list(df.dtypes[df.dtypes==object].index):  ##Unique values in Categorical variables\n    print(i,'\\n\\n',df[i].nunique(),'\\n\\n')","d3408052":"for i in list(df.dtypes[df.dtypes==object].index):\n    print(i,'\\n\\n',(df[i].value_counts()\/df[i].shape[0])*100,'\\n\\n\\n\\n')","5301ee57":"##count of values in categorical columns:\nfor i in list(df.dtypes[df.dtypes==object].index):\n    print(i,'\\n\\n',(df[i].value_counts()),'\\n\\n\\n\\n')","29355f49":"for i in df.drop(columns='Yearly Amount Spent').columns:\n    sns.scatterplot(data=df,x=i,y='Yearly Amount Spent')\n    plt.show()","3601f988":"##We see that the ditribution for email has changed a lot, but for the Area and the State it remains almost similar to\n#what it was for the Address","ad0c4de4":"df.head()","29ce40e4":"##For modelling, we will have to encode(get dummies) for Email, Avatar, Area, and State:\n\npd.get_dummies(data=df,columns=['Email','Avatar','Area','State'],drop_first=True)","e17190dd":"df['State'].unique()","31098252":"##Above we can see that Uni, USN, Bo and USS don't really seem like States, but since we have converted our data which will\n#be used for train and test, our predictions won't be biased","eafa3bea":"df['Email'].unique()","dc8718d6":"df['Area'].unique()","bbc2f150":"##We see that Area unique is also having certain Postal codes, which is not ideally desired, but of course, since we\n#have converted the data for both train and test, our predictions won't be biased","8b5d9efb":"##So, for now, we can actually Drop Email and Area, since they are extremely random and contain too many unique values\n## we will still be using State as an independant variable, and later if we find it is not useful, we can drop it as well","31d4f7ce":"df = df.drop(columns=['Area','Email'])","032ad768":"df2 = pd.get_dummies(data=df,columns=['Avatar','State'],drop_first=True)","f1aab2fa":"df2.head()","0e746530":"y = df2['Yearly Amount Spent']\nX = df2.drop(columns='Yearly Amount Spent')","86995f58":"X = sm.add_constant(X)","7eebc329":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)","743c63cd":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","a39c63b0":"##Checking if both train and test representative of the overall data","a2c665b3":"X.describe()","79cd9049":"X_train.describe()","b9aa8281":"X_test.describe()","706233a1":"##We can see that the mean data from X, X_test and X_train resemble each other(means are close by to each other, for\n#every independant variable,and hence we can claim that train and test are represetative of overall data)","da8c1402":"##We can do the same for the target by doing the count\nprint(y.value_counts(normalize=True))\ny.value_counts().plot.bar()\nplt.show()\nprint(y_train.value_counts(normalize=True))\ny_train.value_counts().plot.bar()\nplt.show()\nprint(y_test.value_counts(normalize=True))\ny_test.value_counts().plot.bar()\nplt.show()","32505ffd":"##We can see that the train and test data fairly represent the overall data, through the plots","61a7ee0d":"#We can also do a ttest for the target:","5d53ffb3":"from scipy.stats import ttest_1samp","897a81d5":"#ttest_1samp(sample,pop_mean)","1d5aed2e":"ttest_1samp(y_train,y.mean())","3414e393":"ttest_1samp(y_test,y.mean())","4763cbac":"print('Mean is %2.1f Sd is %2.1f' % (y.mean(),np.std(y,ddof = 1)))","4c341e32":"#PValue is greater than significance level-0.05 so accept null hypothesis, which means that train data\n#and test data does represent the population","15ef8278":"import warnings \nwarnings.filterwarnings('ignore')\nimport statsmodels.api as sm\n\nmodel1 = sm.OLS(y,X).fit()\nmodel1.summary()","a458b4c3":"##Checking the assumptions before Proceeding:","dc4b8b89":"#Checking normailty of residuals","f6f1a696":"residuals = model1.resid","870927a7":"sns.distplot(residuals)","684f99c6":"#Check linearity of residuals","96373604":"import scipy.stats as stats\nimport pylab\nfrom statsmodels.graphics.gofplots import ProbPlot\nstats.probplot(residuals,dist='norm',plot=pylab)\nplt.title('Probability plot')","7f16cfd2":"#Checking Heteroscedacity\n\ny_pred = model1.predict(X)\n\nsns.regplot(x=y_pred,y=residuals,lowess=True,line_kws={'color':'red'})\nplt.show()","8157e5a0":"#Shows homoscedacity","486efb08":"#Test-Goldfeld for Checking Homoscedacity\n\nimport statsmodels.stats.api as sms\ntest = sms.het_goldfeldquandt(y=model1.resid,x=X)\ntest","2e7baf4f":"#Null accepted as pvalue greater than 0.05. Which means that homoscedacity exists in the data","2d12c94a":"#a. What is the overall R2? Please comment on whether it is good or not.\n##overall R2 is 0.991 and Adjusted R2 is 0.984, which shows that there are certain variables which might be leading to\n#overfitting.\n##Also, the warnings show that a strong multi-collinearity does exist\n#From the above model, we can see the separate Pvalues of the variables, which shows that the variables coming from \n#States and Avatar are not significant as their Pvalues are much greater than 0.05","9b17bb07":"\nypred1 = model1.predict(X_test)\nypred1","1fd91d0a":"X_test","ee68dff4":"X_test['Predicted Values'] = ypred1","0eb11176":"X_test.head()","8c14014a":"#From the above model, we see that Average Session Length, Time on App, Length of Membership are the most significant\n#variables, since their Pvalues are much lesser than the level of significance i.e. 0.05.\n##Almost all the States and the Avatars are insignificant variables as per the above model","8398938c":"##Yes, multi-collinearity does exist, as we can see from the above model summary, and we can check it as well, using the\n#Variance Inflation factor:\n\n#Checking multicollearity using Heatmap and VIF:\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\npd.DataFrame({'vif': vif}, index=X.columns).sort_values(by='vif',ascending=False)","0e8b02e9":"##Generally we will remove variables with VIF>4   (Except the constant: constant is not a variable, so we don't need to see the\n#VIF for the constant)\n##We will remove highly collinear variables for our next model","fd2f1439":"#Root Mean Square Error\n##RMSE for test:\nypred1 = model1.predict(X_test.drop(columns='Predicted Values'))\nnp.sqrt(metrics.mean_squared_error(ypred1,y_test))","adeb0590":"##RMSE for train:\nypred2 = model1.predict(X_train)\nnp.sqrt(metrics.mean_squared_error(ypred2,y_train))","ba59c8b8":"##We see that the RMSE for test is just a bit above that of Train","41531e5c":"##Mean absolute error:\n#For test:\nmetrics.mean_absolute_error(y_test,ypred1)","903dd24d":"#For test:\nmetrics.mean_absolute_error(y_train,ypred2)","65b51401":"##For Mean Absolute Error, we see that it is again a bit higher for Test when compared to Train","fea37f47":"##MAPE: Mean Absolute Percentage Error:\n##For test:\nnp.mean(abs(((ypred1 - y_test)\/y_test)*100))","34cbcb14":"##For test:\nnp.mean(abs(((ypred2 - y_train)\/y_train)*100))","cf3e33ca":"##For Mean Absolute Percentage Error, we see that it is again a bit higher for Test when compared to Train","8a92b6b1":"import statsmodels.api as sm\ndef model(df):\n    y = df['Yearly Amount Spent']\n    x = df.drop(columns='Yearly Amount Spent')\n    X = sm.add_constant(x)\n    X_train1,X_test1,y_train1,y_test1 = train_test_split(X,y,test_size=0.3,random_state=0)\n    model1 = sm.OLS(y_train1,X_train1).fit()\n    ypredtrain = model1.predict(X_train1)\n    rmse_train = np.sqrt(metrics.mean_squared_error(ypredtrain,y_train1))\n    r2train = model1.rsquared\n    r2_adjtrain = model1.rsquared_adj\n    model2 = sm.OLS(y_test1,X_test1).fit()\n    ypredtest = model2.predict(X_test1)\n    rmse_test = np.sqrt(metrics.mean_squared_error(ypredtest,y_test1))\n    r2test = model2.rsquared\n    r2_adjtest = model2.rsquared_adj\n    df2 = pd.DataFrame({\"Train\":[X_train1.shape,rmse_train,r2train,r2_adjtrain],\"Test\":[X_test1.shape,rmse_test,r2test,r2_adjtest]},index=['Dataset_Shape','RMSE','Rsquared','RSquared-Adjusted'])\n    return df2\nmodel(df2)","ebfef9de":"##Above, we see the full model based on all the variables.\n##Now, we will use remove certain variables using VIF","986eca2c":"# removing collinear variables using vif\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calculate_vif(x):\n    thresh = 5.0\n    output = pd.DataFrame()\n    k = x.shape[1]\n    vif = [variance_inflation_factor(x.values, j) for j in range(x.shape[1])]\n    for i in range(1,k):\n        print(\"Iteration no.\")\n        print(i)\n        print(vif)\n        a = np.argmax(vif)\n        print(\"Max VIF is for variable no.:\")\n        print(a)\n        if vif[a] <= thresh :\n            break\n        if i == 1 :          \n            output = x.drop(x.columns[a], axis = 1)\n            vif = [variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n        elif i > 1 :\n            output = output.drop(output.columns[a],axis = 1)\n            vif = [variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n    return(output)","953af429":"out1 = calculate_vif(X)","74173424":"## The following includes only the relevant features for creating our new model with selected variables after removing\n#multi-collinearity:  The variables below don't seem important one, and hence there is a possibilty that this model won't be\n#useful to us\nout1.head()","a84a8d1a":"##Let's create a model using out1:\n\nimport statsmodels.api as sm\ndef model(df):\n    y = df['Yearly Amount Spent']\n    x = out1\n    X = sm.add_constant(x)\n    X_train1,X_test1,y_train1,y_test1 = train_test_split(X,y,test_size=0.3,random_state=0)\n    model1 = sm.OLS(y_train1,X_train1).fit()\n    ypredtrain = model1.predict(X_train1)\n    rmse_train = np.sqrt(metrics.mean_squared_error(ypredtrain,y_train1))\n    r2train = model1.rsquared\n    r2_adjtrain = model1.rsquared_adj\n    model2 = sm.OLS(y_test1,X_test1).fit()\n    ypredtest = model2.predict(X_test1)\n    rmse_test = np.sqrt(metrics.mean_squared_error(ypredtest,y_test1))\n    r2test = model2.rsquared\n    r2_adjtest = model2.rsquared_adj\n    df2 = pd.DataFrame({\"Train\":[X_train1.shape,rmse_train,r2train,r2_adjtrain],\"Test\":[X_test1.shape,rmse_test,r2test,r2_adjtest]},index=['Dataset_Shape','RMSE','Rsquared','RSquared-Adjusted'])\n    return df2\nmodel(df2)","8625435e":"##As guessed, we see that this model did not perform well. Let's try some other technique\n#We see that our training data and testing data have performed badly in this case, and that is majorly because the VIF selection\n#method could not work in this case. We can try removing the variables manually\n#or else, in our first model, where we saw that variables related to States and Avatars weren't really significant.\n#so, we can try dropping them or else creating polynomial features","688a90ea":"##So, mostly in our first model, we are seeing a case of Overfitting, which can be removed. Let's try:","24394a00":"df2.head()","6f1014f1":"pd.DataFrame({'vif': vif}, index=X.columns).sort_values(by='vif',ascending=False)","c195983d":"##We can try removing the highest VIF variable first:","e00a7083":"X = X.drop(columns='Avatar_SlateBlue')","899099fb":"X","291c2fba":"vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\npd.DataFrame({'vif': vif}, index=X.columns).sort_values(by='vif',ascending=False)","e7353629":"##We can try removing State_Bo now:\nX = X.drop(columns='State_ Bo')\nvif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\npd.DataFrame({'vif': vif}, index=X.columns).sort_values(by='vif',ascending=False)","e9b3e196":"##Now, the VIF for most variables is less than 4, so let's try building a model now:","38126281":"X.head()","23b3480a":"selected = X","222788af":"##Let's create a model:\n\nimport statsmodels.api as sm\ndef model(df):\n    y = df['Yearly Amount Spent']\n    x = selected\n    X = sm.add_constant(x)\n    X_train1,X_test1,y_train1,y_test1 = train_test_split(X,y,test_size=0.3,random_state=0)\n    model1 = sm.OLS(y_train1,X_train1).fit()\n    ypredtrain = model1.predict(X_train1)\n    rmse_train = np.sqrt(metrics.mean_squared_error(ypredtrain,y_train1))\n    r2train = model1.rsquared\n    r2_adjtrain = model1.rsquared_adj\n    model2 = sm.OLS(y_test1,X_test1).fit()\n    ypredtest = model2.predict(X_test1)\n    rmse_test = np.sqrt(metrics.mean_squared_error(ypredtest,y_test1))\n    r2test = model2.rsquared\n    r2_adjtest = model2.rsquared_adj\n    df2 = pd.DataFrame({\"Train\":[X_train1.shape,rmse_train,r2train,r2_adjtrain],\"Test\":[X_test1.shape,rmse_test,r2test,r2_adjtest]},index=['Dataset_Shape','RMSE','Rsquared','RSquared-Adjusted'])\n    return df2\nmodel(df2)","5b2622ad":"selected.head()","f50b1483":"import warnings \nwarnings.filterwarnings('ignore')\nimport statsmodels.api as sm\n\nmodel3 = sm.OLS(y,X).fit()\nmodel3.summary()","f2b9094c":"##We see a good Rsquare value, but then again we see that only very few variables from Avatar and State are actually\n#significant\n##This model also shows Time on Website as insignificant","7154cf91":"#Backward Elimination to select features\ncols = list(X.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values,index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)\nprint(len(selected_features_BE))","929f111c":"##So, let's try a model with these features:\n\nXselect = X[['const', 'Avg. Session Length', 'Time on App', 'Length of Membership', 'Avatar_DarkGreen', 'Avatar_ForestGreen', 'Avatar_IndianRed', 'Avatar_LightSkyBlue', 'Avatar_Peru', 'Avatar_Pink', 'Avatar_PowderBlue', 'Avatar_Silver', 'Avatar_Yellow', 'State_ DC', 'State_ MI', 'State_ MT']]","dd4c1266":"import warnings \nwarnings.filterwarnings('ignore')\nimport statsmodels.api as sm\n\nmodel4 = sm.OLS(y,Xselect).fit()\nmodel4.summary()","5f1fca36":"##On checking the Pvalues of the model built using Xselect, we see that all the independant variables have Pvalues lesser\n#than 0.05, which states that all of them are significant as per this model.\n##But we can still see that the multi-collinearity is present.\n#Let's check multi-colinearity using VIF and Heatmap","4c6bc11c":"plt.figure(figsize=(20,10))\nsns.heatmap(Xselect.drop(columns='const').corr(),annot=True)\nplt.show()","452cb1e9":"##From above itself, we can see that multi-collinearity does not exist among these variables: Still let's check\n#the VIF","9b18b880":"###Ideally, we desire a high Rsquare and low RMSE: But of course, the Rsquare should not be so high that it shows Overfitting\n#So, we need to check for a model which has high Rsquare and low RMSE","bed11703":"##Let's create a model: using Xselect\n\nimport statsmodels.api as sm\ndef model(df):\n    y = df['Yearly Amount Spent']\n    x = Xselect\n    X = sm.add_constant(x)\n    X_train1,X_test1,y_train1,y_test1 = train_test_split(X,y,test_size=0.3,random_state=0)\n    model1 = sm.OLS(y_train1,X_train1).fit()\n    ypredtrain = model1.predict(X_train1)\n    rmse_train = np.sqrt(metrics.mean_squared_error(ypredtrain,y_train1))\n    r2train = model1.rsquared\n    r2_adjtrain = model1.rsquared_adj\n    model2 = sm.OLS(y_test1,X_test1).fit()\n    ypredtest = model2.predict(X_test1)\n    rmse_test = np.sqrt(metrics.mean_squared_error(ypredtest,y_test1))\n    r2test = model2.rsquared\n    r2_adjtest = model2.rsquared_adj\n    df2 = pd.DataFrame({\"Train\":[X_train1.shape,rmse_train,r2train,r2_adjtrain],\"Test\":[X_test1.shape,rmse_test,r2test,r2_adjtest]},index=['Dataset_Shape','RMSE','Rsquared','RSquared-Adjusted'])\n    return df2\nmodel(df2)","bf415a21":"##Let's us consider Polynomial Features now:To model using Interaction of the variables as well","b023b355":"df2.head()","83e98706":"from sklearn.preprocessing import PolynomialFeatures\n\nX1 = X.drop('const',axis=1)\npf = PolynomialFeatures()\nXp1 = pf.fit_transform(X1)\ncols = pf.get_feature_names(X1.columns)\nXp = pd.DataFrame(Xp1,columns=cols)","e96a4ada":"##AS we can know that modelling using All the Polynomial features will not give us good results. WE don't want to burden the\n#model with so much data that it can't process. And ultimately this will cause overfitting(if we build model using all the\n#features created above). So, we will select certain feature(important ones)\n##Let us select the important features out these created Polynomial Features","eff10ada":"#Backward Elimination to select features out of the Polynomial Features:\nys = list(y)\ncols = list(Xp.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = Xp[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(ys,X_1).fit()\n    p = pd.Series(model.pvalues.values,index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)\nprint(len(selected_features_BE))","c4f76497":"##Again, we see that 20706 features were selected here, and we don't need so many features as we don't want to burden our data\n#so, we can try to find out polynomial functions from the Xselect which we got above, by selecting the features through\n#backward elimination","b7f6b935":"Xselect.head()","8d1b7c2b":"from sklearn.preprocessing import PolynomialFeatures\n\nX1 = Xselect.drop('const',axis=1)\npf = PolynomialFeatures()\nXp1 = pf.fit_transform(X1)\ncols = pf.get_feature_names(X1.columns)\nXp = pd.DataFrame(Xp1,columns=cols)","bd1eb02a":"#Backward Elimination to select features out of the Polynomial Features:\nys = list(y)\ncols = list(Xp.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = Xp[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(ys,X_1).fit()\n    p = pd.Series(model.pvalues.values,index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)\nprint(len(selected_features_BE))","7e7a828c":"##91 seems like an okay number for features: so let's proceed with these features","0bf63c3e":"Xs = Xp.loc[:,selected_features_BE]","372259dd":"Xs","7baf9123":"#Model using selected features\nols = sm.OLS(ys,Xs).fit()\nols.summary()","ef55ddca":"##again, we see so many nan values in the summary(even in the Pvalues) and this is not desired, since we want to understand\n#how and by how much strength are the independant variables important.\n##so we can now try building a machine learning model and we can tune it to find the perfect one","723a0b18":"X_train, X_test, y_train, y_test = train_test_split(Xp, y, test_size=0.3, random_state=1)","7412f3db":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)","6eaa45b2":"result = lr.fit(X_train,y_train)\ny_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)\n","5b00b080":"y_test_pred","2ee9dd90":"from sklearn.metrics import r2_score\nr2_score(y_test,y_test_pred)","bddeda58":"r2_score(y_train,y_train_pred)","a9915b0a":"from sklearn.metrics import mean_squared_error\n\nnp.sqrt(mean_squared_error(y_test,y_test_pred))","ecd251e6":"np.sqrt(mean_squared_error(y_train,y_train_pred))","40e08c36":"##We will use RFE to reduce over-fitting and the number of variables:\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import GridSearchCV,KFold","332e9ecb":"params = [{'n_features_to_select':list(range(1,60))}]\nlr = LinearRegression()\nrfe = RFE(lr)\n\nfolds = KFold(n_splits=3,random_state=1)\nmodel_cv = GridSearchCV(rfe,param_grid=params,cv=folds)\nmodel_cv.fit(Xp,y)\n","68c1e026":"model_cv.best_params_","b386a96b":"lr = LinearRegression()\nrfe = RFE(lr,n_features_to_select=1)\n\nrfe.fit(Xp,y)","5adbdae6":"cols = pd.DataFrame(list(zip(Xp.columns,rfe.support_,rfe.ranking_)),columns=['cols','select','rank'])\ncols.sort_values(by='rank').head(8)","dc2367c6":"a = list(cols.sort_values(by='rank').head(8)['cols'])","1862dfcf":"Xp[a]","8a1b2f59":"##Now building model with 8 top features(n features to select was 8)\nXp1 = Xp[a]","9c8da0a2":"X_train, X_test, y_train, y_test = train_test_split(Xp1, y, test_size=0.3, random_state=1)","b78fb223":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)","7b74f798":"result = lr.fit(X_train,y_train)\ny_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)\n","166d7372":"y_test_pred","d86088f3":"from sklearn.metrics import r2_score\nr2_score(y_test,y_test_pred)","a0ccb2b8":"r2_score(y_train,y_train_pred)","1a9c1c72":"from sklearn.metrics import mean_squared_error\n\nnp.sqrt(mean_squared_error(y_test,y_test_pred))","c49dea66":"np.sqrt(mean_squared_error(y_train,y_train_pred))","15aa38ce":"##WE see that on selecting the top 8 features, the RMSE for the test decreases and the train increases here\n##So, in this case let's consider if the Test RMSE was to be of utmost importance, then we would consider this model\n#since the RMSE has decreased and the Rsquare has also increased(which is good unless it is not overfitting)","fcef427f":"##Building statistical model using Selected variables:\nols = sm.OLS(ys,Xp1).fit()\nols.summary()","d4ee51ba":"import statsmodels.api as sm\ndef model(df):\n    y = df['Yearly Amount Spent']\n    x = Xp1\n    X = sm.add_constant(x)\n    X_train1,X_test1,y_train1,y_test1 = train_test_split(X,y,test_size=0.3,random_state=0)\n    model1 = sm.OLS(y_train1,X_train1).fit()\n    ypredtrain = model1.predict(X_train1)\n    rmse_train = np.sqrt(metrics.mean_squared_error(ypredtrain,y_train1))\n    r2train = model1.rsquared\n    r2_adjtrain = model1.rsquared_adj\n    model2 = sm.OLS(y_test1,X_test1).fit()\n    ypredtest = model2.predict(X_test1)\n    rmse_test = np.sqrt(metrics.mean_squared_error(ypredtest,y_test1))\n    r2test = model2.rsquared\n    r2_adjtest = model2.rsquared_adj\n    df2 = pd.DataFrame({\"Train\":[X_train1.shape,rmse_train,r2train,r2_adjtrain],\"Test\":[X_test1.shape,rmse_test,r2test,r2_adjtest]},index=['Dataset_Shape','RMSE','Rsquared','RSquared-Adjusted'])\n    return df2\nmodel(df2)","909a0f37":"##THE above model is the one which I have selected for final analysis. Please read more about it in the summary in last\n#question","34dfe275":"##We can try using Regularization technique on the above defined Xp such as LassoCV as well:\n##To visualize the importance of features we can try Lasso","0de2667c":"Xp.head()","ce320f6e":"y","6c7aa86d":"from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\nreg = LassoCV()\nreg.fit(Xp, y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(Xp,y))\ncoef = pd.Series(reg.coef_, index = Xp.columns)","e90957be":"imp_coef = coef.sort_values()\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (20, 20)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")","25b57f1f":"imp_coef","77def0ea":"imp_coef.reset_index().sort_values(by=0).tail(2)","1b1fc0fd":"imp_coef.reset_index().sort_values(by=0,ascending=False).head(10)","8511e83d":"##We see the variables from Lasso that show importance(either positive or negative)","1953e09c":"##For summarizing, I have selected my final model to be the one with 8 final features(the model below which I wrote that I was\n#going to explain more in the summary)\n#For this model, first the significant features were selected from the original(initial) model, and then polynomial \n#features(for creatig features with interaction) were #created using the selected features. \n#Out of these polynomial features, we then selected the final 8 features #using which we build our selected model\n##Out of these 8 select features, I have built both, the machine learning model and the statistical model\n#For ML model:\n#Test Rsquare:0.9883269083622762\n#Test RMSE: 9.047655455715873\n\n#for statistical model(Test)\n#RMSE: 9.56287\n#Rsquared:0.985422\n\n#so, this statistical model(since we build statistical as initial model) has been selected as a good model because it \n#contains very less number of features as compares to other\n#models that we build (and yet it gives a great Rsquare: which means that it explains the variance in the data extremely well)\n#Also, as we have seen, this model shows an RMSE which is lower than a few of the other models and higher than a few of the\n#other models, but it shows a good Rsquare ","f1977422":"##Below we see variables that positively and negatively affect the total amount spend\n#Please read business interpretation below:\nimp_coef = coef.sort_values()\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (20, 20)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")","015e5180":"imp_coef.reset_index().sort_values(by=0).tail(2) ##Variables negatively affecting amount spent and the interaction of \n#these variables negatively affecting amount spent(increase in these variables will decrease amount spend)","12deaa8f":"imp_coef.reset_index().sort_values(by=0,ascending=False).head(10) ##Variables positively affecting amount spent,and\n#the interaction of these variables positively affecting amount spent(increase in these variables will increase amount spent)","4bda12de":"##Most effect was done by Feature Selection and then Creating Polynomial Features\n\n#For this model, first the significant features were selected from the original(initial) model, and then polynomial \n#features(for creatig features with interaction) were #created using the selected features. \n#Out of these polynomial features, we then selected the final 8 features #using which we build our selected model","4a40d922":"##RMSE and Rsquare have to be kept in mind at all times. My model does have a very high Rsquare which could mean that it is \n#still over fitting and then again there was another model with a lower RMSE but that model was\n#not explaining the importance and interaction of the variables properly, which is why I chose this model as my final model\n\n##Another risk could be that, is some other model\/case, these variables would not have been of the same importance, and \n#the immportance of the features does not neccessaily depend on the model, but of course it does depend on how the model\n#was formed in the first place(using which variables, using which techniques): all that is very important to finally\n#interpret which variables would make a difference while writing the business interpretation","51f84807":"# DATA DESCRIPTION","8122b738":"# Base Model","2261742a":"# Using VIF to eliminate features","1e906b59":"# Journey Towards Improving the Model Accuracy\n","3dd450e0":"# How the base model was improved.","5fe0bc48":"# Evaluation Metrics: RMSE (Root Mean Square Error), MAE(Mean Absolute Error)","e0edc18e":"# Checking for Correlation among variables","b7184d63":"# Training the Model","dbb67c87":"# Feature Elimination: Backward Elimination","08ac903f":"# End Notes","3a887d66":"# Predictions","e3a06c96":"# SUMMARY\n","3f916950":"# Do you see multi-collinearity?","ba6ae363":"# Checking the relationship of Independant variables with the Target\n","ff319d00":"# Possible Insights for the Business Stakeholders","19cd0d1a":"This dataset is having data of customers who buys clothes online. This file has\ncustomer email, avg. session time with stylist, Time spent on the app and website, Length of Membership.\nOur main objective is to predict the Yearly amount spent by the customers.","e76ce43e":"# What are the significant variables?","fbb93fca":"# Loading Libraries"}}