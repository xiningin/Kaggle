{"cell_type":{"8a3666cb":"code","4113a492":"code","06d63164":"code","8019d441":"code","843b527e":"code","4ed12bb1":"code","c3a8e510":"code","a8b80c58":"code","6d58bf7a":"code","5d0874a4":"code","9f69ee9b":"code","b2673cc5":"code","2b358af5":"code","7a00928a":"code","8fa5d5f3":"code","d4c5db7f":"code","f9a9f1dc":"code","aa9df944":"code","b6063d44":"code","87fba6df":"code","0a61f854":"code","ddea1ec1":"code","2362f145":"code","03b61947":"code","3cd8e118":"code","7fe14d97":"code","2e06af2b":"code","d803eefa":"code","16d5ffe7":"code","0d9452a7":"code","aa25f7d0":"code","334cda33":"code","84ced41b":"code","2e819f75":"code","002c1794":"code","7c99f571":"code","bd94c246":"code","68830419":"code","f9c3530e":"code","930e63ee":"code","3191f502":"code","2a1b1f36":"code","f6e4d95f":"code","fd3611e9":"code","4443b3fc":"code","6c569faf":"code","89d1eb67":"code","eb6416bf":"code","c953fe78":"code","82495f94":"code","f559c994":"code","600f50e9":"code","7fa3c966":"code","45955ba7":"code","b8af704e":"code","f77df712":"code","fa1d0122":"markdown","5bfb826d":"markdown","19e32da9":"markdown","6c62df31":"markdown","ccab0268":"markdown","8f453e1e":"markdown","436bbd68":"markdown","7d0413a8":"markdown","fe1d75ce":"markdown","97dd84aa":"markdown","5b80ac26":"markdown","7494e32e":"markdown","6a22c479":"markdown","d8e1dfc7":"markdown","bb27cd8d":"markdown","54909d73":"markdown","5aead862":"markdown","e2389508":"markdown","96d5e26f":"markdown","69c9aac8":"markdown","45402761":"markdown","0b92a4e7":"markdown","eb5cc6e0":"markdown","bb32e415":"markdown","cfbf1948":"markdown","719443bc":"markdown","71652d5a":"markdown","e2ba3301":"markdown","cd854868":"markdown","99a87f6a":"markdown","a880c4d1":"markdown","2be8a7c0":"markdown","9447975c":"markdown"},"source":{"8a3666cb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport scipy as sp\nimport warnings\nimport datetime\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n","4113a492":"data= pd.read_csv('\/kaggle\/input\/forest-fires-in-brazil\/amazon.csv')","06d63164":"data.head()","8019d441":"data.describe()","843b527e":"data.info()","4ed12bb1":"data.value_counts","c3a8e510":"data.shape","a8b80c58":"data.columns","6d58bf7a":"data.dtypes","5d0874a4":"data.isnull().sum()","9f69ee9b":"data.isnull().any()","b2673cc5":"data.drop_duplicates(inplace = True)\n","2b358af5":"#lets find the categorialfeatures\nlist_1=list(data.columns)","7a00928a":"list_cate=[]\nfor i in list_1:\n    if data[i].dtype=='object':\n        list_cate.append(i)\n","8fa5d5f3":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\n","d4c5db7f":"for i in list_cate:\n    data[i]=le.fit_transform(data[i])\n","f9a9f1dc":"data","aa9df944":"data.hist(figsize=(20,14),color='g')\nplt.show()\n","b6063d44":"data.columns","87fba6df":"sns.boxplot(x='year',y='month',data=data)\n","0a61f854":"sns.stripplot(x='year',y='state',data=data)","ddea1ec1":"sns.scatterplot(x='year',y='number',data=data,color='g')","2362f145":"sns.lineplot(x='year',y='number',data=data,color='g')","03b61947":"data.corr()","3cd8e118":"\nsns.heatmap(data.corr(), annot = True, cmap = 'viridis')\n","7fe14d97":"sns.pairplot(data=data)\n","2e06af2b":"sns.jointplot(x='year',y='month',data=data,color='r')","d803eefa":"sns.regplot(x='year',y='number',data=data,color='b')","16d5ffe7":"sns.kdeplot(x='year',y='state',data=data,color='g')","0d9452a7":"plt.style.use(\"default\")\nsns.barplot(x=\"year\", y=\"number\",data=data)\nplt.title(\"YEAR vs NUMBER\",fontsize=15)\nplt.xlabel(\"YEAR\")\nplt.ylabel(\"NUMBER\")\nplt.show()\n","aa25f7d0":"plt.style.use(\"default\")\nsns.barplot(x=\"month\", y=\"state\",data=data)\nplt.title(\"MONTH vs STATE\",fontsize=15)\nplt.xlabel(\"MONTH\")\nplt.ylabel(\"STATE\")\nplt.show()\n","334cda33":"#lets find the categorialfeatures\nlist_1=list(data.columns)","84ced41b":"list_cate=[]\nfor i in list_1:\n    if data[i].dtype=='object':\n        list_cate.append(i)\n","2e819f75":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()","002c1794":"for i in list_cate:\n    data[i]=le.fit_transform(data[i])\n","7c99f571":"data","bd94c246":"y=data['year']\nx=data.drop('year',axis=1)\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0,test_size=0.2)","68830419":"print(len(x_test))\nprint(len(x_train))\nprint(len(y_test))\nprint(len(y_train))\n","f9c3530e":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train,y_train)\n","930e63ee":"y_pred=knn.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",knn.score(x_train,y_train)*100)","3191f502":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)\n","2a1b1f36":"y_pred=gnb.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",gnb.score(x_train,y_train)*100)\nprint(accuracy_score(y_test,y_pred)*100)\n","f6e4d95f":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123,criterion='entropy')\n\ndtree.fit(x_train,y_train)\n","fd3611e9":"from sklearn.metrics import classification_report,mean_squared_error\ny_pred=dtree.predict(x_test)\nconf =print(confusion_matrix(y_test, y_pred))\nclf =print(classification_report(y_test, y_pred))\nprint(\"Training Score:\\n\",dtree.score(x_train,y_train)*100)\n","4443b3fc":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nrfc=RandomForestClassifier()\nrfc.fit(x_train,y_train)\n","6c569faf":"y_pred=rfc.predict(x_test)\nconf =print(confusion_matrix(y_test, y_pred))\nclf =print(classification_report(y_test, y_pred))\nscore=accuracy_score(y_test,y_pred)\nscore\nprint(\"Training Score:\\n\",rfc.score(x_train,y_train)*100)\n","89d1eb67":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = None)\nadb.fit(x_train,y_train)","eb6416bf":"y_pred=adb.predict(x_test)\nconf =print(confusion_matrix(y_test, y_pred))\nclf =print(classification_report(y_test, y_pred))\nscore=accuracy_score(y_test,y_pred)\nscore\nprint(\"Training Score:\\n\",adb.score(x_train,y_train)*100)\n","c953fe78":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\ngbc.fit(x_train,y_train)","82495f94":"y_pred=gbc.predict(x_test)\nconf =print(confusion_matrix(y_test, y_pred))\nclf =print(classification_report(y_test, y_pred))\nscore=accuracy_score(y_test,y_pred)\nprint(score)\nprint(\"Training Score:\\n\",gbc.score(x_train,y_train)*100)\n","f559c994":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","600f50e9":"from xgboost import XGBClassifier\n\nxgb =XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxgb.fit(x_train, y_train)\n","7fa3c966":"y_pred=xgb.predict(x_test)\nconf =print(confusion_matrix(y_test, y_pred))\nclf =print(classification_report(y_test, y_pred))\nscore=accuracy_score(y_test,y_pred)\nscore\nprint(\"Training Score:\\n\",xgb.score(x_train,y_train)*100)\n","45955ba7":"from sklearn.ensemble import ExtraTreesClassifier\netc = ExtraTreesClassifier(n_estimators=100, random_state=0)\netc.fit(x_train,y_train)\n","b8af704e":"y_pred=etc.predict(x_test)\nconf =print(confusion_matrix(y_test, y_pred))\nclf =print(classification_report(y_test, y_pred))\nscore=accuracy_score(y_test,y_pred)\nprint(\"Training Score:\\n\",etc.score(x_train,y_train)*100)\n","f77df712":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata\n","fa1d0122":"![](https:\/\/www.fodors.com\/wp-content\/uploads\/2020\/11\/brazil-fire-1.jpg)","5bfb826d":"**KDE PLOT (DENSITY PLOT)**\n\n**KDE Plot described as Kernel Density Estimate is used for visualizing the Probability Density of a continuous variable. It depicts the probability density at different values in a continuous variable. We can also plot a single graph for multiple samples which helps in more efficient data visualization.**\n\n","19e32da9":"**LINEPLOT**\n\n**A Line plot can be defined as a graph that displays data as points or check marks above a number line, showing the frequency of each value.**\n\n","6c62df31":"**5. AdaBoostClassifier**\n\n**An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.**","ccab0268":"**BOXPLOT**\n\n**A boxplot is a standardized way of displaying the distribution of data based on a five number summary (\u201cminimum\u201d, first quartile (Q1), median, third quartile (Q3), and \u201cmaximum\u201d). ... It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.**\n\n","8f453e1e":"**HEATMAP**\n\n**A heatmap is a graphical representation of data that uses a system of color-coding to represent different values. Heatmaps are used in various forms of analytics but are most commonly used to show user behaviour on specific webpages or webpage templates.**","436bbd68":"# LOADING THE DATASET","7d0413a8":"# **Exploratory Data Analysis**","fe1d75ce":"**STRIPPLOT**\n\n**A strip plot is a graphical data anlysis technique for summarizing a univariate data set. ... The strip plot is an alternative to a histogram or a density plot. It is typically used for small data sets (histograms and density plots are typically preferred for larger data sets).**\n\n","97dd84aa":"**DATA PREPROCESSING**","5b80ac26":"**HISTOGRAM**\n\n**A histogram is basically used to represent data provided in a form of some groups.It is accurate method for the graphical representation of numerical data distribution.It is a type of bar plot where X-axis represents the bin ranges while Y-axis gives information about frequency.**\n\n","7494e32e":"**6. Gradient Boosting Classifier**\n\n**Gradient boosting classifiers are a group of machine learning algorithms that combine many weak learning models together to create a strong predictive model. Decision trees are usually used when doing gradient boosting.**","6a22c479":"# **BRAZIL FIRES PREDICTION**","d8e1dfc7":"**JOINTPLOT**\n\n**Seaborn's jointplot displays a relationship between 2 variables (bivariate) as well as 1D profiles (univariate) in the margins. This plot is a convenience class that wraps JointGrid.**\n\n","bb27cd8d":"**PAIRPLOT**\n\n**A pairplot plot a pairwise relationships in a dataset. The pairplot function creates a grid of Axes such that each variable in data will by shared in the y-axis across a single row and in the x-axis across a single column.**\n","54909d73":"# **Thank you** ","5aead862":"**Checking Null Values**\n\n","e2389508":"# MODELS","96d5e26f":"**4.Random Forest Classifier**\n\n**A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.**","69c9aac8":"**TRAINING AND TESTING DATA**","45402761":"**1. KNeighborsClassifier**\n\n**By default, the KNeighborsClassifier looks for the 5 nearest neighbors. We must explicitly tell the classifier to use Euclidean distance for determining the proximity between neighboring points.**\n\n","0b92a4e7":"**BARPLOT**\n\n**A bar plot represents an estimate of central tendency for a numeric variable with the height of each rectangle and provides some indication of the uncertainty around that estimate using error bars.**","eb5cc6e0":"# IMPORTING THE LIBRARIES","bb32e415":"![](https:\/\/hwnews.in\/wp-content\/uploads\/2019\/08\/Amazon-forest-fire-960x540.jpg)","cfbf1948":"![](https:\/\/i.guim.co.uk\/img\/media\/318697f53f1aba1c90b2d95e908e217314e37e7e\/0_164_2480_1489\/500.jpg?quality=85&auto=format&fit=max&s=86e302106077e9ac3c6da4485fcc629f)","719443bc":"**7. XGBClassifier**\n\n**XGBoost is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm, which attempts to accurately predict a target variable by combining the estimates of a set of simpler, weaker models.**\n\n","71652d5a":"**REGPLOT**\n\n**This method is used to plot data and a linear regression model fit. ... If strings, these should correspond with column names in \u201cdata\u201d. When pandas objects are used, axes will be labeled with the series name. data: This is dataframe where each column is a variable and each row is an observation.**\n\n","e2ba3301":"![](https:\/\/api.time.com\/wp-content\/uploads\/2019\/08\/sebastian-liste-amazon-fires-brazil-3.jpg)","cd854868":"**2.Naive Bayes** \n\n**Naive Bayes classifiers are a collection of classification algorithms based on Bayes' Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other.**","99a87f6a":"**SCATTER PLOT**\n\n**A scatter plot (aka scatter chart, scatter graph) uses dots to represent values for two different numeric variables. The position of each dot on the horizontal and vertical axis indicates values for an individual data point. Scatter plots are used to observe relationships between variables.**\n\n","a880c4d1":"# CONCLUSION:\n\n**So we get a good accuracy and training score of about 100 % using Random Forest Classifier, XG Boost , Gradient Boosting Classifier.**\n\n**The accuracy of other models can be increased by Hypertuning.**","2be8a7c0":"**3. DECISION TREE CLASSIFIER**\n\n**Decision trees use multiple algorithms to decide to split a node into two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. ... The decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.**\n\n","9447975c":"**8. ExtraTreesClassifier**\n\n**Extremely Randomized Trees Classifier(Extra Trees Classifier) is a type of ensemble learning technique which aggregates the results of multiple de-correlated decision trees collected in a \u201cforest\u201d to output it's classification result.**\n\n"}}