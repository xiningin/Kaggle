{"cell_type":{"234e2070":"code","c1383c8d":"code","84e5a374":"code","bbc8f7af":"code","87b639d3":"code","8d8d7703":"code","f36dbfb3":"code","96f25beb":"code","fa9aa41e":"code","e7138676":"code","cbc3b999":"code","590f1195":"code","4713c3e8":"code","24317802":"code","9d99aad3":"code","94450713":"code","4ac98f2a":"code","0c9c1705":"code","d4d9f33d":"code","28dc713f":"code","86d47397":"code","6511e6e2":"code","862f8c23":"code","ade4bcdf":"code","7dd7105e":"code","5a99d070":"code","3a0e7f2a":"code","9a646a96":"code","763f0d76":"code","98aeb695":"code","436d292e":"code","167d2b8d":"code","818a8cc4":"code","c8671640":"code","d9cb8980":"code","2670bf21":"code","e05034ee":"code","89251978":"code","b915e56d":"code","edd06e50":"code","dd1a7b5f":"code","8da97d9f":"code","3bcd9566":"code","214fefeb":"code","1b3ee482":"code","3cde450d":"code","eb0e760a":"code","9715fbd4":"code","91ffa932":"code","6189a9d0":"code","54f510b5":"code","50832e44":"code","ebc9a739":"code","5dcc92ea":"code","b29e8431":"code","bad38508":"code","a9ed9d0f":"code","7f41c422":"code","9dd7d6b4":"code","7a2f22d2":"code","b6e2589f":"code","cf172650":"code","a4f78b0c":"code","7772a88a":"code","a310b8f2":"code","238954f0":"code","74cc83d1":"code","d331f580":"code","95377a04":"code","6bae9fe6":"code","fb0b25b9":"code","96876df3":"code","72307624":"code","1e10c89c":"code","c9d01b00":"code","3e5852f3":"code","eca79aad":"code","a516ad0c":"code","2f441fef":"code","f742cc80":"code","f04f6a6c":"code","020a43e4":"code","2ef36783":"code","dc8f886d":"code","0a9781d8":"code","face6e7f":"code","0a38825a":"code","db8ec985":"code","e9b6db68":"code","c055ed24":"code","3179af4c":"code","8cadac98":"code","033b20de":"code","d0b5604a":"code","adcddbfb":"code","ec31b501":"code","f1da9a77":"code","e59d8be8":"code","3ca85866":"code","e3fb9b5b":"code","218ca65c":"code","3c09256b":"code","8749a871":"code","b6f27b90":"code","0640ed68":"code","f5ccf44d":"code","c293de24":"code","cbcda990":"code","6ec4b87d":"code","15d414fb":"code","4c28e971":"code","72602385":"code","6238b64b":"code","30038ec6":"code","63a64dc2":"code","6709cd53":"code","fb4d0af7":"code","4442e50e":"code","6565b9f9":"code","f0438537":"code","0abf643d":"code","edef4f2b":"code","340d8562":"code","1010d782":"code","765a708c":"code","a27028b0":"code","110d9ab9":"code","e44f5179":"code","8430c69f":"code","5e1a10d1":"code","e848a757":"code","33196af8":"code","24690efc":"code","6006e426":"code","417fd8e6":"code","98d14c27":"code","9125a3cf":"code","d40dfb75":"code","1f51b916":"code","1ff20d08":"code","d8664de5":"code","b5ac6def":"code","81f0fb57":"code","be0ea9aa":"code","f6bb0950":"code","53a2dac3":"code","1f514047":"code","5b4c967e":"code","cf09f812":"code","a2582047":"code","3dde8786":"code","5024bdd7":"code","8473fa1a":"code","33f1741f":"code","ee9ef8a5":"code","1864c596":"code","49fe8eb9":"code","d8885479":"code","f44c6e5b":"code","17046382":"code","d9c0604e":"code","3f970e95":"code","97203690":"code","7ee204f7":"code","23ca9678":"code","6d121bb4":"code","06e49462":"code","a2e9486f":"code","5aaf3ef7":"code","b085b499":"code","104285b2":"code","93a66168":"code","8b56682f":"code","05d3d525":"code","0fc06e55":"code","bbb29ad3":"code","3dfa801a":"code","746d0644":"code","0b6ef2d1":"code","b7809237":"code","1c95d7ca":"code","4333e098":"code","2a3168ab":"code","0bc42487":"code","8bac0320":"code","02e5883f":"code","7620a796":"code","c6487099":"code","9efc8d74":"code","a744013a":"code","7eb17934":"code","dfd9705d":"code","ad051301":"markdown","6767e771":"markdown","183a28b7":"markdown","f6b68f75":"markdown","4b927c52":"markdown","ca09d63e":"markdown","45bc9ad6":"markdown","12611c32":"markdown","e2ace933":"markdown","5a5b39ca":"markdown","87d07431":"markdown","ff93b40e":"markdown","0b72f730":"markdown","370dcef2":"markdown","6b2fb0e0":"markdown","ae63a456":"markdown","95659fa8":"markdown","6b0ffff0":"markdown","f361c318":"markdown","c7d562e9":"markdown","47bbf764":"markdown","8c49dd44":"markdown","8ad9b775":"markdown","3a6f50f0":"markdown","c5166bce":"markdown","d811e825":"markdown","3c05836b":"markdown","5f4e6df0":"markdown","578146ce":"markdown","7ad958d1":"markdown","93f9cdb1":"markdown","dca76dd0":"markdown","3e652b4f":"markdown","bc066825":"markdown","61415b26":"markdown","e84fc8cc":"markdown","217cc4d7":"markdown","74e4c446":"markdown","49ae54d5":"markdown","afe13850":"markdown","7ab453ab":"markdown","b2e4ff04":"markdown","28d49e43":"markdown","617332f7":"markdown","0280324c":"markdown","af7bea19":"markdown","d5bda902":"markdown","292951b8":"markdown","354caba0":"markdown","3cd9883b":"markdown","e88c7265":"markdown","213062fa":"markdown","ced82c06":"markdown","f4414011":"markdown","f780c352":"markdown","892aed8e":"markdown","65992561":"markdown","cffeafae":"markdown","4353f0a3":"markdown","ff7b6e16":"markdown","12049631":"markdown","cf19fd88":"markdown","5b93f331":"markdown","8a7ac265":"markdown","41d86331":"markdown","0c0dfb8f":"markdown","ad436438":"markdown","fb2bfc8b":"markdown","65555638":"markdown","968c8e72":"markdown","877f8f06":"markdown","e6c18652":"markdown","45c0f257":"markdown","9bfc5412":"markdown","880f8942":"markdown","fadd22a4":"markdown","effacf12":"markdown","430b1040":"markdown","7028869b":"markdown","e3dcf843":"markdown","43c0bf84":"markdown","1dd8ae5a":"markdown","38bf776a":"markdown","e28811ca":"markdown","f6b4afba":"markdown","e85ca88f":"markdown","bb3988d2":"markdown","47aaf965":"markdown","64bd0347":"markdown","12c21357":"markdown","3603765d":"markdown","6ddf9028":"markdown","7f262503":"markdown","157da93b":"markdown","77893fae":"markdown","f7de0edc":"markdown","19a6a8a0":"markdown","3ec08a71":"markdown","d5afab59":"markdown","ab98eba1":"markdown","ec21075c":"markdown","12c15b42":"markdown","691c4791":"markdown","13516cc7":"markdown","949b0aed":"markdown","c77590f6":"markdown","05655ec6":"markdown","bf96fa51":"markdown","def66460":"markdown","1ed035e1":"markdown","a17c19ba":"markdown","c52cf23f":"markdown","07a240aa":"markdown","0dafa2f6":"markdown","20eef620":"markdown","0c0b600e":"markdown","4cbcee9b":"markdown","fa3f4b91":"markdown","7b7a98d3":"markdown","95fbe7bc":"markdown","bb9147c9":"markdown","a940ab95":"markdown","1ab152bb":"markdown","77fc7722":"markdown","be20ac33":"markdown","db01bda1":"markdown","d40caf08":"markdown","9b6ac0f6":"markdown","bf81487d":"markdown","acfafb4a":"markdown","adcea1b9":"markdown","3568ed00":"markdown","a740205b":"markdown","f32c7125":"markdown","16ae3db5":"markdown","dc444068":"markdown","e3558221":"markdown","58a459be":"markdown","ee872c98":"markdown","02802e4f":"markdown","ef2d5e91":"markdown","aca76cd0":"markdown","f1907c2c":"markdown","5a4f67a0":"markdown","5d787b96":"markdown","06263319":"markdown","e35a2c7b":"markdown","2f929bb0":"markdown","193ebe05":"markdown","0260261d":"markdown","4237829d":"markdown","a2dd4987":"markdown"},"source":{"234e2070":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport seaborn as sns\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n%matplotlib inline\n\n# display all columns of the dataframe\npd.options.display.max_columns = None\n\n# display all rows of the dataframe\npd.options.display.max_rows = None\nfrom sklearn.preprocessing import MinMaxScaler\nimport statsmodels\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.graphics.gofplots import qqplot\nfrom statsmodels.stats.anova import anova_lm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.tools.eval_measures import rmse\n\n# import various functions from scipy\nfrom scipy import stats\nfrom scipy.stats import shapiro\n\n# 'metrics' from sklearn is used for evaluating the model performance\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\nfrom statsmodels.graphics.gofplots import qqplot\n\n# import 'stats'\nfrom scipy import stats\n\n# 'metrics' from sklearn is used for evaluating the model performance\nfrom sklearn.metrics import mean_squared_error\n\n# import functions to perform feature selection\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom sklearn.feature_selection import RFE\n\n# import function to perform linear regression\nfrom sklearn.linear_model import LinearRegression\n\n# import functions to perform cross validation\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n\n# import function to perform linear regression\nfrom sklearn.linear_model import LinearRegression\n\n# import StandardScaler to perform scaling\nfrom sklearn.preprocessing import StandardScaler \n\n# import SGDRegressor from sklearn to perform linear regression with stochastic gradient descent\nfrom sklearn.linear_model import SGDRegressor\n\n# import function for ridge regression\nfrom sklearn.linear_model import Ridge\n\n# import function for lasso regression\nfrom sklearn.linear_model import Lasso\n\n# import function for elastic net regression\nfrom sklearn.linear_model import ElasticNet\n\n# import function to perform GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import linear_model\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing","c1383c8d":"# display all columns of the dataframe\npd.options.display.max_columns = None\n# display all rows of the dataframe\npd.options.display.max_rows = None\n# return an output value upto 6 decimals\npd.options.display.float_format = '{:.6f}'.format","84e5a374":"# read csv file using pandas\ndata = pd.read_csv('..\/input\/co2-emissions-cannada\/CO2 Emissions_Canada.csv')\n\n# display the top 5 rows of the dataframe\ndata.head()","bbc8f7af":"data.info()","87b639d3":"# shape returns the dimension of the data\ndata.shape","8d8d7703":"data.dtypes","f36dbfb3":"missing_value = pd.DataFrame({\n    'Missing Value': data.isnull().sum(),\n    'Percentage': (data.isnull().sum() \/ len(data))*100\n})","96f25beb":"missing_value.sort_values(by='Percentage', ascending=False)","fa9aa41e":"# set the figure size\nplt.figure(figsize=(15, 8))\n\n# plot heatmap to check null values\n# isnull(): returns 'True' for a missing value\n# cbar: specifies whether to draw a colorbar; draws the colorbar for 'True' \nsns.heatmap(data.isnull(), cbar=False)\n\n# display the plot\nplt.show()","e7138676":"duplicate = data.duplicated().sum()\nprint('There are {} duplicated rows in the data'.format(duplicate))","cbc3b999":"data.drop_duplicates(inplace=True)","590f1195":"duplicate = data.duplicated().sum()\nprint('There are {} duplicated rows in the data'.format(duplicate))","4713c3e8":"data.shape","24317802":"data.tail()","9d99aad3":"data.reset_index(inplace=True)","94450713":"data.tail()","4ac98f2a":"data.drop(['index'],inplace=True,axis=1)","0c9c1705":"data.head()","d4d9f33d":"data.shape","28dc713f":"data.describe(include=np.number)","86d47397":"data.describe(include = object)","6511e6e2":"# select the numerical features in the dataset using 'select_dtypes()'\n# select_dtypes(include=np.number): considers the numeric variables\ndata_num_features = data.select_dtypes(include=np.number)\n\n# print the names of the numeric variables \nprint('The numerical columns in the dataset are: ',data_num_features.columns)","862f8c23":"# generate the correlation matrix\ncorr =  data_num_features.corr()\n\n# print the correlation matrix\ncorr","ade4bcdf":"plt.figure(figsize=(20,10))\ncorr =data_num_features.corr(method='pearson')\nsns.heatmap(corr, annot=True,cmap='tab20c')\nplt.show()","7dd7105e":"# create a list of all categorical variables\n# include=object: selects the categoric features\n# drop(['city'],axis=1): drops the city column from the dataframe\ndata_cat_features = data.select_dtypes(include='object')\n\n# plot the count distribution for each categorical variable \n# 'figsize' sets the figure size\n# plot a count plot for all the categorical variables\nfor variable in data_cat_features:\n    \n    cat_count  = data[variable].value_counts()\n    cat_count10 = cat_count[:10,]\n    plt.figure(figsize=(10,5))\n    sns.barplot(cat_count10.values,cat_count10.index, alpha=0.8)\n    if cat_count.size > 10:\n        plt.title('Top 10 {}'.format(variable))\n    else:\n        plt.title(variable)\n    plt.ylabel('{}'.format(variable), fontsize=12)\n    plt.xlabel('Number of Cars', fontsize=12)\n    plt.show()\n\n# avoid overlapping of the plots using tight_layout()    \nplt.tight_layout()   \n\n# display the plot\nplt.show()","5a99d070":"sns.distplot(data['CO2_Emissions'], bins=30, kde=True, axlabel='Carbon Dioxide Emission (30 bins)')","3a0e7f2a":"mean = data['CO2_Emissions'].mean()\n\n# calculate the mode\nmode = data['CO2_Emissions'].mode()\n\n# calculate the median\nmedian = data['CO2_Emissions'].median()\n\nprint('Mean for CO2 Emission is ',mean)\nprint('Median for CO2 Emission is ',median)\nprint('Mode for CO2 Emission is ',mode)","9a646a96":"# create two plots in single figure\n# I define two axes by passing the value 3 to the subplot function\n# sharey returns the y axis label\nfig, axes = plt.subplots(1,3, sharey=True, figsize=(15,8))\n\n# create a boxplot\n# orient=\"v\": create a vertical plot\n# ax = axes: axes object to draw plot\n# I use axes[0] to use the first axes for plotting\nsns.boxplot(y=data['CO2_Emissions'], orient=\"v\", ax = axes[0])\n\n# create a violinplot\n# orient=\"v\": create a vertical plot\n# ax = axes: axes object to draw plot\n# I use axes[1] to use the second axes for plotting\nsns.violinplot(y=data['CO2_Emissions'], orient=\"v\", ax = axes[1]);\n\n# add a value of mode in the empty subplot\n# fontsize: font size of the text\nplt.text(0.1, 200, \"Mode = 221\/246\", fontsize=12)\n\n# add a value of median in the empty subplot\n# fontsize: font size of the text\nplt.text(0.1, 300, \"Median = 246\", fontsize=12)\n\n# add a value of mean in the empty subplot\n# fontsize: font size of the text\nplt.text(0.1, 400, \"Mean = 251.16\", fontsize=12)\n\n# add the result in the empty subplot\n# fontsize: font size of the text\nplt.text(0.1, 100, \"Mode < Median < Mean\", fontsize=12)\n\n# remove the axis for the third subplot\nplt.axis(\"off\")\n\n# show the plot\nplt.show()","763f0d76":"make_co2 = data.groupby('Make')['CO2_Emissions'].mean().sort_values(ascending=False).head(10)\nmodel_co2 = data.groupby('Model')['CO2_Emissions'].mean().sort_values(ascending=False).head(10)\nvehicle_class_co2 = data.groupby('Vehicle_Class')['CO2_Emissions'].mean().sort_values(ascending=False).head(10)\ntransmission_co2 = data.groupby('Transmission')['CO2_Emissions'].mean().sort_values(ascending=False).head(10)\nfuel_type_co2 = data.groupby('Fuel_Type')['CO2_Emissions'].mean().sort_values(ascending=False).head()","98aeb695":"fig, axes = plt.subplots(5,1, figsize=(15,20))\nfig.suptitle('Average of Categorical Variables vs CO2 Emissions')\n\nsns.barplot(ax=axes[0],x = make_co2.values,y = make_co2.index)\naxes[0].set_title('CO2 Emissions v\/s Make')\n\nsns.barplot(ax=axes[1],x = model_co2.values,y = model_co2.index)\naxes[1].set_title('CO2 Emissions v\/s Model')\n\nsns.barplot(ax=axes[2],x = vehicle_class_co2.values,y = vehicle_class_co2.index)\naxes[2].set_title('CO2 Emissions v\/s Vehicle_Class')\n\nsns.barplot(ax=axes[3],x = transmission_co2.values,y = transmission_co2.index)\naxes[3].set_title('CO2 Emissions v\/s Transmission')\n\nsns.barplot(ax=axes[4], x=fuel_type_co2.values,y=fuel_type_co2.index)\naxes[4].set_title('CO2 Emissions v\/s Fuel Type')","436d292e":"# plot the scatter plot\n# use 'hue' to add 3rd variable in the scatter plot\nplt.rcParams[\"figure.figsize\"] = (15,10)\nsns.scatterplot('CO2_Emissions','Cylinders',data = data,hue='Fuel_Type')\n\n# set label for x-axis\nplt.xlabel(\"CO2 Emissions\", fontsize=20)\n\n# set label for y-axis\nplt.ylabel(\"Cylinders\", fontsize=20)\n\n# set title\nplt.title(\"Scatter Plot\", fontsize=20)\n\n# display the plot\nplt.show()","167d2b8d":"plt.figure(figsize=(10,5))\nsns.pairplot(data,kind=\"reg\")\nplt.show()","818a8cc4":"data['Make_Type'] = data['Make'].replace(['BUGATTI', 'PORSCHE', 'MASERATI', 'ASTON MARTIN', 'LAMBORGHINI',\n                                                       'JAGUAR','SRT'],\n                                                      'Sports')","c8671640":"data['Make_Type'] = data['Make_Type'].replace(['ALFA ROMEO', 'AUDI', 'BMW', 'BUICK',\n                                                         'CADILLAC', 'CHRYSLER', 'DODGE', 'GMC',\n                                                         'INFINITI', 'JEEP', 'LAND ROVER', 'LEXUS', 'MERCEDES-BENZ',\n                                                         'MINI', 'SMART', 'VOLVO'],\n                                                         'Premium')","d9cb8980":"data['Make_Type'] = data['Make_Type'].replace(['ACURA', 'BENTLEY', 'LINCOLN', 'ROLLS-ROYCE',\n                                                         'GENESIS'],\n                                                         'Luxury')","2670bf21":"data['Make_Type'] = data['Make_Type'].replace(['CHEVROLET', 'FIAT', 'FORD', 'KIA',\n                                                         'HONDA', 'HYUNDAI', 'MAZDA', 'MITSUBISHI',\n                                                         'NISSAN', 'RAM', 'SCION', 'SUBARU', 'TOYOTA',\n                                                         'VOLKSWAGEN'],\n                                                         'General')","e05034ee":"data['Make_Type'].unique()","89251978":"data['Make_Type'].value_counts()","b915e56d":"#Drop Make column\ndata = data.drop(['Make'], axis=1)","edd06e50":"data.head()","dd1a7b5f":"# set figure size\nplt.figure(figsize=(15,8))\n\n# boxplot of claim against region\n# x: specifies the data on x axis\n# y: specifies the data on y axis\n# data: specifies the dataframe to be used\nax = sns.boxplot(x=\"Make_Type\", y=\"CO2_Emissions\", data=data)\n\n# rotate labels using set_ticklabels\n# labels: specify the tick labels to be used\n# rotation: the angle by which tick labels should be rotated\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90)\n\n# show the plot\nplt.show()","8da97d9f":"data['Vehicle_Class_Type'] = data['Vehicle_Class'].replace(['COMPACT', 'MINICOMPACT', 'SUBCOMPACT'],\n                                                      'Hatchback')","3bcd9566":"data['Vehicle_Class_Type'] = data['Vehicle_Class_Type'].replace(['MID-SIZE', 'TWO-SEATER', 'FULL-SIZE', 'STATION WAGON - SMALL',\n                                                         'STATION WAGON - MID-SIZE'],\n                                                         'Sedan')","214fefeb":"data['Vehicle_Class_Type'] = data['Vehicle_Class_Type'].replace(['SUV - SMALL', 'SUV - STANDARD', 'MINIVAN'],\n                                                         'SUV')","1b3ee482":"data['Vehicle_Class_Type'] = data['Vehicle_Class_Type'].replace(['VAN - CARGO', 'VAN - PASSENGER', 'PICKUP TRUCK - STANDARD', 'SPECIAL PURPOSE VEHICLE',\n                                                         'PICKUP TRUCK - SMALL'],\n                                                         'Truck')","3cde450d":"# check the unique values of the Make_Type column\ndata['Vehicle_Class_Type'].unique()","eb0e760a":"data['Vehicle_Class_Type'].value_counts()","9715fbd4":"#Drop Vehicle_Class column\ndata = data.drop(['Vehicle_Class'], axis=1)","91ffa932":"data.head()","6189a9d0":"# set figure size\nplt.figure(figsize=(15,8))\n\n# boxplot of claim against region\n# x: specifies the data on x axis\n# y: specifies the data on y axis\n# data: specifies the dataframe to be used\nax = sns.boxplot(x=\"Vehicle_Class_Type\", y=\"CO2_Emissions\", data=data)\n\n# rotate labels using set_ticklabels\n# labels: specify the tick labels to be used\n# rotation: the angle by which tick labels should be rotated\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90)\n\n# show the plot\nplt.show()","54f510b5":"data.drop(['Model'],axis=1,inplace=True)","50832e44":"data.head()","ebc9a739":"df_num_features=data.select_dtypes(include=np.number)","5dcc92ea":"Q1 = df_num_features.quantile(0.25)\nQ3 = df_num_features.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","b29e8431":"outlier = pd.DataFrame((df_num_features < (Q1 - 1.5 * IQR)) | (df_num_features > (Q3 + 1.5 * IQR)))","bad38508":"for i in outlier.columns:\n    print('Total number of Outliers in column {} are {}'.format(i, (len(outlier[outlier[i] == True][i]))))","a9ed9d0f":"for column in enumerate(df_num_features):\n    plt.figure(figsize=(30,5))\n    sns.set_theme(style=\"darkgrid\")\n    sns.boxplot(x=column[1], data=  df_num_features)\n    plt.xlabel(column[1],fontsize=18)\n    plt.show()","7f41c422":"stat, p_value = shapiro(df_num_features)\n\n# print the test statistic and corresponding p-value \nprint('Test statistic:', stat)\nprint('P-Value:', p_value)","9dd7d6b4":"data = data[~((data < (Q1 - 1.5 * IQR)) |(data > (Q3 + 1.5 * IQR))).any(axis=1)]\ndata.shape","7a2f22d2":"data.reset_index(inplace=True)","b6e2589f":"data.drop(['index'],inplace=True,axis=1)","cf172650":"data.head()","a4f78b0c":"# select the numerical features in the dataset using 'select_dtypes()'\n# select_dtypes(include=np.number): considers the numeric variables\ndata_num_features = data.select_dtypes(include=np.number)\n\n# print the names of the numeric variables \nprint('The numerical columns in the dataset are: ',data_num_features.columns)","7772a88a":"# generate the correlation matrix\ncorr =  data_num_features.corr()\n\n# print the correlation matrix\ncorr","a310b8f2":"plt.figure(figsize=(20,10))\ncorr =data_num_features.corr(method='pearson')\nsns.heatmap(corr, annot=True,cmap='tab20b')\nplt.show()","238954f0":"df_dummies = pd.get_dummies(data = data[[\"Fuel_Type\",\"Transmission\",\"Make_Type\",\"Vehicle_Class_Type\"]], drop_first = True)\ndf_dummies.head()","74cc83d1":"df_num_features=data.select_dtypes(include=np.number)\ndf_num_features.head()","d331f580":"df_comb = pd.concat([df_num_features, df_dummies], axis = 1)\ndf_comb.head()","95377a04":"df_comb.drop(['CO2_Emissions'],inplace=True,axis=1)","6bae9fe6":"df_comb.head()","fb0b25b9":"df_comb.isna().sum()","96876df3":"X = df_comb.copy()","72307624":"X = sm.add_constant(X)\ny = data.CO2_Emissions","1e10c89c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n\nMLR_model1 = sm.OLS(y_train, X_train).fit()\nprint(MLR_model1.summary())","c9d01b00":"df_num_features.skew()","3e5852f3":"for col in df_num_features.columns:\n    print(\"Column \", col, \" :\", stats.shapiro(df_num_features[col]))","eca79aad":"df_num_features.drop('CO2_Emissions',axis=1,inplace=True)","a516ad0c":"mms = preprocessing.MinMaxScaler()\nmmsfit = mms.fit(df_num_features)\ndfxz = pd.DataFrame(mms.fit_transform(df_num_features), columns = ['Engine_Size','Cylinders','Fuel_Consumption_City','Fuel_Consumption_Hwy','Fuel_Consumption_Comb','Fuel_Consumption_Comb1'])","2f441fef":"dfxz.head()","f742cc80":"dfxz = pd.concat([dfxz, df_dummies], axis = 1)\ndfxz.head()","f04f6a6c":"X=dfxz\nX = sm.add_constant(X)\ny = data.CO2_Emissions","020a43e4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n\nMLR_model2 = sm.OLS(y_train, X_train).fit()\nprint(MLR_model2.summary())","2ef36783":"target = data['CO2_Emissions']\n\ntarget.dtype","dc8f886d":"# create an empty dataframe to store the VIF for each variable\nvif = pd.DataFrame()\n\n# calculate VIF using list comprehension \n# use for loop to access each variable \n# calculate VIF for each variable and create a column 'VIF_Factor' to store the values \nvif[\"VIF_Factor\"] = [variance_inflation_factor(df_num_features.values, i) for i in range(df_num_features.shape[1])]\n\n# create a column of variable names\nvif[\"Features\"] = df_num_features.columns\n\n# sort the dataframe based on the values of VIF_Factor in descending order\n# 'ascending = False' sorts the data in descending order\n# 'reset_index' resets the index of the dataframe\n# 'drop = True' drops the previous index\nvif.sort_values('VIF_Factor', ascending = False).reset_index(drop = True)","0a9781d8":"sklearn_pca = PCA()\npcafit = sklearn_pca.fit(dfxz)","face6e7f":"pcafit.explained_variance_","0a38825a":"pcafit.components_","db8ec985":"plt.plot(np.cumsum(pcafit.explained_variance_ratio_))\nplt.locator_params(axis=\"x\", nbins=len(pcafit.explained_variance_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","e9b6db68":"np.round(pcafit.explained_variance_ratio_.reshape(-1,1) * 100,1)","c055ed24":"dfx_pca = sklearn_pca.fit_transform(dfxz)\ndfx_pca.shape","3179af4c":"dfx_pca = pd.DataFrame(dfx_pca, columns=['pca0','pca1','pca2','pca3','pca4','pca5',\n                                         'pca6','pca7','pca8','pca9','pca10','pca11',\n                                         'pca12','pca13','pca14','pca15','pca16',\n                                         'pca17','pca18','pca19','pca20','pca21','pca22',\n                                         'pca23','pca24','pca25','pca26','pca27','pca28',\n                                         'pca29','pca30','pca31','pca32','pca33',\n                                         'pca34','pca35','pca36','pca37','pca38','pca39',\n                                         'pca40'])","8cadac98":"dfx_pca.head()","033b20de":"dfx_pca = sm.add_constant(dfx_pca)","d0b5604a":"X = dfx_pca[['const','pca0','pca1','pca2','pca3','pca4','pca5','pca6','pca7','pca8','pca9','pca10','pca11','pca12','pca13','pca14','pca15','pca16','pca17','pca18','pca19','pca20','pca21','pca22','pca23','pca24','pca25','pca26','pca27','pca28','pca29','pca30','pca31','pca32','pca33']]\nX.head()","adcddbfb":"y = data.CO2_Emissions","ec31b501":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n\nMLR_model_pca = sm.OLS(y_train, X_train).fit()\nprint(MLR_model_pca.summary())","f1da9a77":"# initiate linear regression model to use in feature selection\nlinreg = LinearRegression()\nlinreg_forward = sfs(estimator=linreg, k_features ='best', forward=True,\n                     verbose=2, scoring='r2')\n\n# fit the forward selection on training data using fit()\nsfs_forward = linreg_forward.fit(X_train, y_train)","e59d8be8":"# print the selected feature names when k_features = 12\nprint('Features selected using forward selection are: ')\nprint(sfs_forward.k_feature_names_)\n\n# print the R-squared value\nprint('\\nR-Squared: ', sfs_forward.k_score_)","3ca85866":"# initiate linear regression model to use in feature selection\nlinreg = LinearRegression()\nlinreg_backward = sfs(estimator = linreg, k_features ='best', forward = False,\n                     verbose = 2, scoring = 'r2')\n\n# fit the backward elimination on training data using fit()\nsfs_backward = linreg_backward.fit(X_train, y_train)","e3fb9b5b":"# print the selected feature names when k_features = 12\nprint('Features selected using backward elimination are: ')\nprint(sfs_backward.k_feature_names_)\n\n# print the R-squared value\nprint('\\nR-Squared: ', sfs_backward.k_score_)","218ca65c":"X = dfx_pca[['const','pca0', 'pca1', 'pca2', 'pca3', 'pca4', 'pca5', 'pca6', 'pca7', 'pca8', 'pca9', 'pca10', 'pca11', 'pca12', 'pca13', 'pca15', 'pca16', 'pca17', 'pca18', 'pca19', 'pca20', 'pca21', 'pca23', 'pca24', 'pca25', 'pca26', 'pca27', 'pca28', 'pca29', 'pca30', 'pca31', 'pca32', 'pca33']]\nX.head()","3c09256b":"y = data.CO2_Emissions","8749a871":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n\nMLR_full_model = sm.OLS(y_train, X_train).fit()\nprint(MLR_full_model.summary())","b6f27b90":"import seaborn as sns \nfig, ax = plt.subplots(nrows = 2, ncols= 2, figsize=(20, 15))\n\n# use for loop to create scatter plot for residuals and each independent variable (do not consider the intercept)\n# 'ax' assigns axes object to draw the plot onto \nfor variable, subplot in zip(X_train.columns[1:5], ax.flatten()):\n    sns.scatterplot(X_train[variable], MLR_full_model.resid , ax=subplot)\n\n# display the plot\nplt.show()","0640ed68":"# print the model summary\nprint(MLR_full_model.summary())","f5ccf44d":"# create vector of result parmeters\nname = ['f-value','p-value']\ntest = sms.het_breuschpagan(MLR_full_model.resid, MLR_full_model.model.exog)\nlzip(name, test[2:])","c293de24":"# set the plot size using 'rcParams'\n# once the plot size is set using 'rcParams', it sets the size of all the forthcoming plots in the file\n# pass width and height in inches to 'figure.figsize' \nplt.rcParams['figure.figsize'] = [15,8]\n\n# plot the Q-Q plot\n# 'r' represents the regression line\nqqplot(MLR_full_model.resid, line = 'r')\n\n# set plot and axes labels\n# set text size using 'fontsize'\nplt.title('Q-Q Plot', fontsize = 15)\nplt.xlabel('Theoretical Quantiles', fontsize = 15)\nplt.ylabel('Sample Quantiles', fontsize = 15)\n\n# display the plot\nplt.show()","cbcda990":"stat, p_value = shapiro(MLR_full_model.resid)\nprint('Test statistic:', stat)\nprint('P-Value:', p_value)","6ec4b87d":"y_train_pred = MLR_full_model.predict(X_train) \ny_train_pred.head()","15d414fb":"# calculate the SSR on train dataset\nssr = np.sum((y_train_pred - y_train.mean())**2)\nprint('Sum of Squared Regression:',ssr)","4c28e971":"# calculate the SSE on train dataset\nsse = np.sum((y_train - y_train_pred)**2)\nprint('Sum of Squared Error:',sse)","72602385":"# calculate the SST on train dataset\nsst = np.sum((y_train - y_train.mean())**2)\nprint('Sum of Sqaured Total:',sst)","6238b64b":"print('Sum of SSR and SSE is:',ssr+sse)","30038ec6":"r_sq =MLR_full_model.rsquared\n\n# print the R-squared value\nprint('R Squared is:',r_sq)","63a64dc2":"see = np.sqrt(sse\/(len(X_train) - 2))    \nprint(\"The standard error of estimate:\",see)","6709cd53":"MLR_full_model.summary()","fb4d0af7":"t_intercept =MLR_full_model.params[0] \/ MLR_full_model.bse[0]\nprint('t intercept:',t_intercept)","4442e50e":"t_coeff1 =MLR_full_model.params[1] \/ MLR_full_model.bse[1]\nprint('t coeff:',t_coeff1)","6565b9f9":"# calculate p-value for intercept\n# use 'sf' (Survival function) from t-distribution to calculate the corresponding p-value\n\n# pass degrees of freedom and t-statistic value for intercept\n# degrees of freedom = n - 1 = 4070 - 1 = 4069\npval = stats.t.sf(np.abs(t_intercept), 4069)*2 \nprint('p val for intercept:',pval)","f0438537":"# calculate p-value for slope\n# use 'sf' (Survival function) from t-distribution to calculate the corresponding p-value\n\n# pass degrees of freedom and t-statistic value for slope\n# degrees of freedom = n - 1 = 4070 - 1 = 4069\npval = stats.t.sf(np.abs(t_coeff1),4069)*2 \nprint('p val for slope:',pval)","0abf643d":"# CI for intercept\n# create a tuple using the above formula\n# here, t_table_value = 1.9622\nCI_inter_min, CI_inter_max = MLR_full_model.params[0] - (1.9622*MLR_full_model.bse[0]), MLR_full_model.params[0] + (1.9622*MLR_full_model.bse[0])\n\n# print the confidence interval for intercept \nprint('CI for intercept:', [CI_inter_min , CI_inter_max])","edef4f2b":"# CI for slope\n# create a tuple using the above formula\n# here, t_table_value = 1.9622\nCI_coeff1_min, CI_coeff1_max = MLR_full_model.params[1] - (1.9622*MLR_full_model.bse[1]), MLR_full_model.params[1] + (1.9622*MLR_full_model.bse[1])\n\n# print the confidence interval for slope\nprint('CI for coeff1:', [CI_coeff1_min, CI_coeff1_max])","340d8562":"print(MLR_full_model.summary())","1010d782":"r_sq_mlr = MLR_full_model.rsquared\n\n# print the value\nprint('r square in regression model:',r_sq_mlr)","765a708c":"# calculate adjusted R-Squared on train dataset\n# use 'rsquared_adj' from statsmodel\nadj_r_sq = MLR_full_model.rsquared_adj\n\n# print the value\nprint('Adjusted r square for regression model:',adj_r_sq)","a27028b0":"# compute f_value using the below formula \n# f_value = (r_sq \/ k-1)\/((1- r_sq)\/n-k)\n\n# k = number of beta coefficients\nk = len(X_train.columns)\n\n# n = number of observations\nn = len(X_train)\n\n# calculate value of F-statistic\n# 'r_sq_mlr' represents the R-Squared value\nf_value = (r_sq_mlr \/ (k - 1))\/((1-r_sq_mlr)\/(n - k))\n\n# print the value\nprint('f value for regression model:',f_value)","110d9ab9":"# degrees of freedom \n# dfn = k-1 = 32-1 = 31\n# dfd = n-k = 4396-32 = 4364\np_val = stats.f.sf(f_value, dfn = 31, dfd = 4364)\n\n# print the value\nprint('p value for regression model:',p_val)","e44f5179":"train_pred = MLR_full_model.predict(X_train)\ntest_pred = MLR_full_model.predict(X_test)","8430c69f":"train_pred.head()","5e1a10d1":"test_pred.head()","e848a757":"mse_train = round(mean_squared_error(y_train, train_pred),4)\n\n# print the MSE for the training set\nprint(\"Mean Squared Error (MSE) on training set: \", mse_train)\n\n# calculate the MSE for the test data\n# round the value upto 4 digits using 'round()'\nmse_test = round(mean_squared_error(y_test, test_pred),4)\n\n# print the MSE for the test set\nprint(\"Mean Squared Error (MSE) on test set: \", mse_test)","33196af8":"# calculate the MSE using the \"mean_squared_error\" function\n\n# MSE for the train data\nmse_train = mean_squared_error(y_train, train_pred)\nrmse_train = round(np.sqrt(mse_train), 4)\n\n# print the RMSE for the train set\nprint(\"Root Mean Squared Error (RMSE) on training set: \", rmse_train)\n\n# MSE for the test data\nmse_test = mean_squared_error(y_test, test_pred)\n\n# take the square root of the MSE to calculate the RMSE\n# round the value upto 4 digits using 'round()'\nrmse_test = round(np.sqrt(mse_test), 4)\n\n# print the RMSE for the test set\nprint(\"Root Mean Squared Error (RMSE) on test set: \", rmse_test)","24690efc":"# calculate the MAE using the \"mean_absolute_error\" function\n\n# calculate the MAE for the train data\n# round the value upto 4 digits using 'round()'\nmae_train = round(mean_absolute_error(y_train, train_pred),4)\n\n# print the MAE for the training set\nprint(\"Mean Absolute Error (MAE) on training set: \", mae_train)\n\n# calculate the MAE for the test data\n# round the value upto 4 digits using 'round()'\nmae_test = round(mean_absolute_error(y_test, test_pred),4)\n\n# print the MAE for the test set\nprint(\"Mean Absolute Error (MAE) on test set: \", mae_test)","6006e426":"def mape(actual, predicted):\n    return (np.mean(np.abs((actual - predicted) \/ actual)) * 100)","417fd8e6":"mape_train = round(mape(y_train, train_pred),4)\n\n# print the MAPE for the training set\nprint(\"Mean Absolute Percentage Error (MAPE) on training set: \", mape_train)\n\n# calculate the MAPE for the test data\n# round the value upto 4 digits using 'round()'\nmape_test = round(mape(y_test, test_pred),4)\n\n# print the MAPE for the test set\nprint(\"Mean Absolute Percentage Error (MAPE) on test set: \", mape_test)","98d14c27":"cols = ['Model_Name', 'R-squared', 'Adj. R-squared', 'MSE', 'RMSE', 'MAE', 'MAPE']\n\nresult_table = pd.DataFrame(columns = cols)","9125a3cf":"from statsmodels.tools.eval_measures import rmse\n\nMLR_full_model_metrics = pd.Series({'Model_Name': \"MLR Full Model\",\n                     'R-squared': MLR_full_model.rsquared,\n                     'Adj. R-squared': MLR_full_model.rsquared_adj,\n                     'MSE': mean_squared_error(y_test, test_pred),\n                     'RMSE': rmse(y_test, test_pred),\n                     'MAE': mean_absolute_error(y_test, test_pred),\n                     'MAPE': mape(y_test, test_pred)\n                   })\n\nresult_table = result_table.append(MLR_full_model_metrics, ignore_index = True)\n\nresult_table","d40dfb75":"sns.regplot(y = y_train,x = train_pred,color='red',line_kws={'color':'blue'},marker='x')","1f51b916":"a = np.random.randint(1,4070,1745)\ntrain_pred1 = list(train_pred)\nTrainPred2 = []","1ff20d08":"for i in a:\n    TrainPred2.append(train_pred1[i])","d8664de5":"sns.regplot(y = test_pred,x = TrainPred2)","b5ac6def":"from sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits = 10)","81f0fb57":"def Get_score(model, X_train_k, X_test_k, y_train_k, y_test_k):\n    model.fit(X_train_k, y_train_k)\n    return model.score(X_test_k, y_test_k)  ","be0ea9aa":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10, test_size = 0.3)","f6bb0950":"from sklearn.linear_model import LinearRegression\n\nscores = []\n \nfor train_index, test_index in kf.split(X_train):\n    X_train_k, X_test_k, y_train_k, y_test_k = X_train.iloc[train_index], X_train.iloc[test_index], \\\n                                               y_train.iloc[train_index], y_train.iloc[test_index]\n \n    scores.append(Get_score(LinearRegression(), X_train_k, X_test_k, y_train_k, y_test_k)) \n    \nprint('All scores: ', scores)\n\nprint(\"\\nMinimum score obtained: \", round(min(scores), 4))\n\nprint(\"Maximum score obtained: \", round(max(scores), 4))\n\nprint(\"Average score obtained: \", round(np.mean(scores), 4))","53a2dac3":"scores = cross_val_score(estimator = LinearRegression(), \n                         X = X_train, \n                         y = y_train, \n                         cv = 10, \n                         scoring = 'r2')","1f514047":"print('All scores: ', scores)\n\nprint(\"\\nMinimum score obtained: \", round(min(scores), 4))\n\nprint(\"Maximum score obtained: \", round(max(scores), 4))\n\nprint(\"Average score obtained: \", round(np.mean(scores), 4))","5b4c967e":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10, test_size = 0.2)","cf09f812":"def Get_score(model, X_train_k, X_test_k, y_train_k, y_test_k):\n    model.fit(X_train_k, y_train_k)                              \n    return model.score(X_test_k, y_test_k)","a2582047":"loocv_rmse = []\nloocv = LeaveOneOut()\n\nfor train_index, test_index in loocv.split(X_train):\n\n    X_train_l, X_test_l, y_train_l, y_test_l = X_train.iloc[train_index], X_train.iloc[test_index], \\\n                                               y_train.iloc[train_index], y_train.iloc[test_index]\n    \n    linreg = LinearRegression()\n    linreg.fit(X_train_l, y_train_l)\n \n    mse = mean_squared_error(y_test_l, linreg.predict(X_test_l))\n    \n    rmse = np.sqrt(mse)\n    \n    loocv_rmse.append(rmse)","3dde8786":"print(\"\\nMinimum rmse obtained: \", round(min(loocv_rmse), 4))\n\nprint(\"Maximum rmse obtained: \", round(max(loocv_rmse), 4))\n \nprint(\"Average rmse obtained: \", round(np.mean(loocv_rmse), 4))","5024bdd7":"def get_train_rmse(model):\n\n    train_pred = model.predict(X_train)\n\n    mse_train = mean_squared_error(y_train, train_pred)\n\n    rmse_train = round(np.sqrt(mse_train), 4)\n\n    return(rmse_train)","8473fa1a":"def get_test_rmse(model):\n\n    test_pred = model.predict(X_test)\n\n    mse_test = mean_squared_error(y_test, test_pred)\n\n    rmse_test = round(np.sqrt(mse_test), 4)\n\n    return(rmse_test)","33f1741f":"from sklearn.linear_model import SGDRegressor\n\nsgd = SGDRegressor(random_state = 10)\n\nlinreg_with_SGD = sgd.fit(X_train, y_train)\n\nprint('RMSE on train set:', get_train_rmse(linreg_with_SGD))\n\nprint('RMSE on test set:', get_test_rmse(linreg_with_SGD))","ee9ef8a5":"def plot_coefficients(model, algorithm_name):\n\n    df_coeff = pd.DataFrame({'Variable': X.columns, 'Coefficient': model.coef_})\n\n    sorted_coeff = df_coeff.sort_values('Coefficient', ascending = False)\n\n    sns.barplot(x = \"Coefficient\", y = \"Variable\", data = sorted_coeff)\n\n    plt.xlabel(\"Coefficients from {}\".format(algorithm_name), fontsize = 15)\n\n    plt.ylabel('Features', fontsize = 15)","1864c596":"MLR_model = linreg.fit(X_train, y_train)","49fe8eb9":"plt.subplot(1,2,1)\nplot_coefficients(MLR_model, 'Linear Regression (OLS)')\n\nplt.subplot(1,2,2)\nplot_coefficients(linreg_with_SGD, 'Linear Regression (SGD)')\n\nplt.tight_layout()","d8885479":"score_card = pd.DataFrame(columns=['Model_Name', 'Alpha (Wherever Required)', 'l1-ratio', 'R-Squared',\n                                       'Adj. R-Squared', 'Train_RMSE','Test_RMSE', 'Test_MAPE'])","f44c6e5b":"def get_test_mape(model):\n\n    test_pred = model.predict(X_test)\n\n    mape_test = mape(y_test, test_pred)\n\n    return(mape_test)","17046382":"def get_score(model):\n    \n    r_sq = model.score(X_train, y_train)\n\n    n = X_train.shape[0]\n\n    k = X_train.shape[1]\n\n    r_sq_adj = 1 - ((1-r_sq)*(n-1)\/(n-k-1))\n    \n    return ([r_sq, r_sq_adj])","d9c0604e":"def update_score_card(algorithm_name, model, alpha = '-', l1_ratio = '-'):\n    \n    global score_card\n    score_card = score_card.append({'Model_Name': algorithm_name,\n                       'Alpha (Wherever Required)': alpha, \n                       'l1-ratio': l1_ratio, \n                       'Test_MAPE': get_test_mape(model),\n                       'Train_RMSE': get_train_rmse(model),\n                       'Test_RMSE': get_test_rmse(model), \n                       'R-Squared': get_score(model)[0], \n                       'Adj. R-Squared': get_score(model)[1]}, ignore_index = True)","3f970e95":"update_score_card(algorithm_name = 'Linear Regression (using SGD)', model = linreg_with_SGD)\n\nscore_card","97203690":"from sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV","7ee204f7":"ridge = Ridge(alpha = 0.1, max_iter = 500)\n\nridge.fit(X_train, y_train)\n\nprint('RMSE on test set:', get_test_rmse(ridge))","23ca9678":"update_score_card(algorithm_name='Ridge Regression (with alpha = 0.1)', model = ridge, alpha = 0.1)\n\nscore_card","6d121bb4":"ridge = Ridge(alpha = 1, max_iter = 500)\n\nridge.fit(X_train, y_train)\n\nprint('RMSE on test set:', np.round(get_test_rmse(ridge),2))","06e49462":"update_score_card(algorithm_name='Ridge Regression (with alpha = 1)', model = ridge, alpha = 1)\n\nscore_card","a2e9486f":"ridge = Ridge(alpha = 2, max_iter = 500)\n\nridge.fit(X_train, y_train)\n\nprint('RMSE on test set:', get_test_rmse(ridge))","5aaf3ef7":"update_score_card(algorithm_name='Ridge Regression (with alpha = 2)', model = ridge, alpha = 2)\n\nscore_card","b085b499":"ridge = Ridge(alpha = 0.5, max_iter = 500)\n\nridge.fit(X_train, y_train)\n\nprint('RMSE on test set:', get_test_rmse(ridge))","104285b2":"plt.subplot(1,2,1)\nplot_coefficients(MLR_model, 'Linear Regression (OLS)')\n\nplt.subplot(1,2,2)\nplot_coefficients(ridge, 'Ridge Regression (alpha = 0.5)')\n\nplt.tight_layout()","93a66168":"lasso = Lasso(alpha = 0.01, max_iter = 500)\n\nlasso.fit(X_train, y_train)\n\nprint('RMSE on test set:', get_test_rmse(lasso))","8b56682f":"plt.subplot(1,2,1)\nplot_coefficients(MLR_model, 'Linear Regression (OLS)')\n\nplt.subplot(1,2,2)\nplot_coefficients(lasso, 'Lasso Regression (alpha = 0.01)')\n\nplt.tight_layout()","05d3d525":"lasso = Lasso(alpha = 0.05, max_iter = 500)\n\nlasso.fit(X_train, y_train)\n\nprint('RMSE on test set:', get_test_rmse(lasso))","0fc06e55":"plt.subplot(1,2,1)\nplot_coefficients(MLR_model, 'Linear Regression (OLS)')\n\nplt.subplot(1,2,2)\nplot_coefficients(lasso, 'Lasso Regression (alpha = 0.05)')\n\nplt.tight_layout()","bbb29ad3":"df_lasso_coeff = pd.DataFrame({'Variable': X.columns, 'Coefficient': lasso.coef_})\n\nprint('Insignificant variables obtained from Lasso Regression when alpha is 0.05')\ndf_lasso_coeff.Variable[df_lasso_coeff.Coefficient == 0].to_list()","3dfa801a":"update_score_card(algorithm_name = 'Lasso Regression', model = lasso, alpha = '0.05')\n\nscore_card","746d0644":"enet = ElasticNet(alpha = 0.1, l1_ratio = 0.55, max_iter = 500)\n\nenet.fit(X_train, y_train)\n\nprint('RMSE on test set:', get_test_rmse(enet))","0b6ef2d1":"update_score_card(algorithm_name = 'Elastic Net Regression', model = enet, alpha = '0.1', l1_ratio = '0.55')\n\nscore_card","b7809237":"enet = ElasticNet(alpha = 0.1, l1_ratio = 0.1, max_iter = 500)\n\nenet.fit(X_train, y_train)\n\nprint('RMSE on test set:', get_test_rmse(enet))","1c95d7ca":"update_score_card(algorithm_name = 'Elastic Net Regression', model = enet, alpha = '0.1', l1_ratio = '0.1')\n\nscore_card","4333e098":"enet = ElasticNet(alpha = 0.1, l1_ratio = 0.01, max_iter = 500)\n\nenet.fit(X_train, y_train)\n\nprint('RMSE on test set:', get_test_rmse(enet))","2a3168ab":"plt.subplot(1,2,1)\nplot_coefficients(MLR_model, 'Linear Regression (OLS)')\n\nplt.subplot(1,2,2)\nplot_coefficients(enet, 'Elastic Net Regression')\n\nplt.tight_layout()","0bc42487":"update_score_card(algorithm_name = 'Elastic Net Regression', model = enet, alpha = '0.1', l1_ratio = '0.01')\n\nscore_card","8bac0320":"tuned_paramaters = [{'alpha':[1e-15, 1e-10, 1e-8, 1e-4,1e-3, 1e-2, 0.1, 1, 5, 10, 20, 40, 60, 80, 100]}]\n \nridge = Ridge()\n\nridge_grid = GridSearchCV(estimator = ridge, \n                          param_grid = tuned_paramaters, \n                          cv = 10)\n\nridge_grid.fit(X_train, y_train)\n\nprint('Best parameters for Ridge Regression: ', ridge_grid.best_params_, '\\n')\n\nprint('RMSE on test set:', get_test_rmse(ridge_grid))","02e5883f":"update_score_card(algorithm_name = 'Ridge Regression (using GridSearchCV)', \n                  model = ridge_grid, \n                  alpha = ridge_grid.best_params_.get('alpha'))\n\nscore_card","7620a796":"tuned_paramaters = [{'alpha':[1e-15, 1e-10, 1e-8, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 20]}]\n \nlasso = Lasso()\n\nlasso_grid = GridSearchCV(estimator = lasso, \n                          param_grid = tuned_paramaters, \n                          cv = 10)\n\nlasso_grid.fit(X_train, y_train)\n\nprint('Best parameters for Lasso Regression: ', lasso_grid.best_params_, '\\n')\n\nprint('RMSE on test set:', get_test_rmse(lasso_grid))","c6487099":"update_score_card(algorithm_name = 'Lasso Regression (using GridSearchCV)', \n                  model = lasso_grid, \n                  alpha = lasso_grid.best_params_.get('alpha'))\n\nscore_card","9efc8d74":"tuned_paramaters = [{'alpha':[0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 20, 40, 60],\n                      'l1_ratio':[0.0001, 0.0002, 0.001, 0.01, 0.1, 0.2, 0.4, 0.55]}]\n\nenet = ElasticNet()\n\nenet_grid = GridSearchCV(estimator = enet, \n                          param_grid = tuned_paramaters, \n                          cv = 10)\n\nenet_grid.fit(X_train, y_train)\n\nprint('Best parameters for Elastic Net Regression: ', enet_grid.best_params_, '\\n')\n\nprint('RMSE on test set:', get_test_rmse(enet_grid))","a744013a":"update_score_card(algorithm_name = 'Elastic Net Regression (using GridSearchCV)', \n                  model = enet_grid, \n                  alpha = enet_grid.best_params_.get('alpha'), \n                  l1_ratio = enet_grid.best_params_.get('l1_ratio'))\n\nscore_card","7eb17934":"score_card = score_card.sort_values('Test_RMSE').reset_index(drop = True)\n\nscore_card.style.highlight_min(color = 'lightblue', subset = 'Test_RMSE')","dfd9705d":"# plot the accuracy measure for all models\n# secondary_y: specify the data on the secondary axis\nscore_card.plot(secondary_y=['R-Squared','Adj. R-Squared'])\n\n# display just the plot\nplt.show()","ad051301":"**Interpretation:** I observe that the p-value is less than 0.05; thus, I conclude that there is heteroskedasticity present in the data.","6767e771":"Since none of the numerical features are normally distributed (p-value<0.05) , I will perform Min-Max normalisation to scale the data","183a28b7":"#### 6.9.4.1 Q-Q Plot<a id=\"qq_plt\"><\/a>","f6b68f75":"# Problem Statement  \ud83d\ude9a\ud83c\udfed","4b927c52":"In this dataset I have 7384 records across 12 features","ca09d63e":"## 8.4 Mean Absolute Percentage Error (MAPE)<a id=\"mape\"><\/a>","45bc9ad6":"# 6. Building Multiple Linear Regression Models<a id='bui_mlr_mod'><\/a>","12611c32":"<br>Inferences:<\/br>\n<br>1. The average amount of CO2 emitted from cars is 251 g\/km<\/br>\n<br>2. Atleast 4 Litres of fuel is consumed be it the car is on city roads or highway<\/br>\n<br>3. About 75% of the cars have 6 or less cylinders<\/br>\n<br>4. The amount of fuel consumed by cars on city roads is comparitvely greater than that of highway<\/br>","e2ace933":"## 4.1 Preparing the Dataset <a id='Data_Preparing'><\/a>","5a5b39ca":"## Table of Contents\n\n1. **[Import Libraries](#import_lib)**\n2. **[Set Options](#set_options)**\n3. **[Read Data](#Read_Data)**\n4. **[Exploratory Data Analysis](#data_preparation)**\n    - 4.1 - [Preparing the Dataset](#Data_Preparing)\n        - 4.1.1 - [Data Dimension](#Data_Shape)\n        - 4.1.2 - [Data Types](#Data_Types)\n        - 4.1.3 - [Missing Values](#Missing_Values)\n        - 4.1.4 - [Duplicate Data](#duplicate)\n        - 4.1.5 - [Indexing](#indexing)\n        - 4.1.6 - [Final Dataset](#final_dataset)\n    - 4.2 - [Understanding the Dataset](#Data_Understanding)\n        - 4.2.1 - [Summary Statistics](#Summary_Statistics)\n        - 4.2.2 - [Correlation](#correlation)\n        - 4.2.3 - [Analyze Categorical Variables](#analyze_cat_var)\n        - 4.2.4 - [Anaylze Target Variable](#analyze_tar_var)\n        - 4.2.5 - [Analyze Relationship Between Target and Independent Variables](#analyze_tar_ind_var)\n        - 4.2.6 - [Feature Engineering](#feature_eng)\n5. **[Data Pre-Processing](#data_pre)**\n    - 5.1 - [Outliers](#out)\n        - 5.1.1 - [Discovery of Outliers](#dis_out)\n        - 5.1.2 - [Removal of Outliers](#rem_out)\n        - 5.1.3 - [Rechecking of Correlation](#rec_cor)\n    - 5.2 - [Categorical Encoding](#cat_enc)\n6. **[Building Multiple Linear Regression Models](#bui_mlr_mod)**\n    - 6.1 - [Multiple Linear Regression - Basic Model](#bas_mod)\n    - 6.2 - [Feature Transformation](#fea_tra)\n    - 6.3 - [Feature Scaling](#fea_sca)\n    - 6.4 - [Multiple Linear Regression - Full Model - After Feature Scaling](#mod_aft_sca)\n    - 6.5 - [Assumptions Before Multiple Linear Regression Model](#ass_bef)\n        - 6.5.1 - [Assumption #1: If Target Variable is Numeric](#tgt_num)\n        - 6.5.2 - [Assumption #2: Presence of Multi-Collinearity](#pre_mul_col)\n    - 6.6 - [Multiple Linear Regression - Full Model - After PCA](#mod_pca)\n    - 6.7 - [Feature Selection](#fea_sel)\n        - 6.7.1 - [Forward Selection](#for_sel)\n        - 6.7.2 - [Backward Elimination](#bac_eli)\n    - 6.8 - [Multiple Linear Regression - Full Model - After Feature Selection](#mod_fea_sel)\n    - 6.9 - [Assumptions After Multiple Linear Regression Model](#ass_aft)\n        - 6.9.1 - [Assumption #1: Linear Relationship Between Dependent and Independent Variable](#lr_dep_ind)\n        - 6.9.2 - [Assumption #2: Checking for Autocorrelation](#che_aut_cor)\n        - 6.9.3 - [Assumption #3: Checking for Heterskedacity](#che_het)\n        - 6.9.4 - [Assumption #4: Test for Normality](#tes_nor)\n            - 6.9.4.1 - [Q-Q Plot](#qq_plt)\n            - 6.9.4.2 - [Shapiro Wilk Test](#sha_wil_tes)\n7. **[Model Evaluation](#mod_eva)**\n    - 7.1 - [Measures of Variation](#mea_var)\n    - 7.2 - [Inferences about Intercept and Slope](#inf_int_slo)\n    - 7.3 - [Confidence Interval for Intercept and Slope](#con_int_slo)\n    - 7.4 - [Compare Regression Results](#com_reg_res)\n8. **[Model Performance](#mod_per)**\n    - 8.1 - [Mean Square Error(MSE)](#mse)\n    - 8.2 - [Root Mean Squared Error(RMSE)](#rmse)\n    - 8.3 - [Mean Absolute Error(MAE)](#mae)\n    - 8.4 - [Mean Absolute Percentage Error(MAPE)](#mape)\n    - 8.5 - [Resultant Table](#res_tab)\n9. **[Model Optimization](#mod_opt)**\n    - 9.1 - [Bias](#bias)\n    - 9.2 - [Variance](#var)\n    - 9.3 - [Model Validation](#mod_val)\n      - 9.3.1 - [Cross Validation](#cro_val)\n      - 9.3.2 - [Leave One Out Cross Validation(LOOCV)](#loocv)\n    - 9.4 - [Gradient Descent](#gra_des)\n    - 9.5 - [Regularization](#reg)\n      - 9.5.1 - [Ridge Regression Model](#ridge)\n      - 9.5.2 - [Lasso Regression Model](#lasso)\n      - 9.5.3 - [Elastic Net Regression Model](#ela_net)\n      - 9.5.4 - [Grid Search CV](#gri_sea)\n10. **[Displaying Score Summary](#dis_sco_sum)**\n11. **[Conclusion](#conclu)**\n12. **[Deployment](#deploy)**\n13. **[References](#Refer)**","87d07431":"# 7. Model Evaluation<a id=\"mod_eva\"><\/a>","ff93b40e":"Since all the features except Fuel_Consumption_Comb1 have a VIF value greater than 10 I cannot proceed with VIF method else I will lose all our features. Hence , I will proceed with PCA","0b72f730":"### 6.7.1 Forward Selection<a id=\"for_sel\"><\/a>","370dcef2":"https:\/\/scihub.se\/https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0959652620329875","6b2fb0e0":"## 7.2 Inferences about Intercept and Slope<a id=\"inf_int_slo\"><\/a>","ae63a456":"The dataset contains **5 object columns, 3 int column and 4 float columns**","95659fa8":"Recheck of correlation after treating outliers. There has been a slight change with respect to the correlation between numeric values","6b0ffff0":"**Create a new feature Vehicle_Class_Type by combining various Vehicle_Class on the basis of their size**","f361c318":"### 4.1.1 Data Dimensions <a id='Data_Shape'><\/a>","c7d562e9":"## 8.5 Resultant Table<a id=\"res_tab\"><\/a>","47bbf764":"### 6.9.2 Assumption #2: Checking for Autocorrelation<a id=\"che_aut_cor\"><\/a>","8c49dd44":"**Visualizing outliers using Boxplots**","8ad9b775":"**To predict the Carbon Dioxide emissions from a vehicle in Canada depending on the fuel consumption and other describing features of a vehicle.**","3a6f50f0":"## 6.6 Multiple Linear Regression - Full Model - After PCA<a id=\"mod_pca\"><\/a>","c5166bce":"## 9.3.2 Leave Out One Cross Validation(LOOCV)<a id=\"loocv\"><\/a>","d811e825":"**There are 16 unique Vehicle Classes. I will divide them into Hatchback, Sedan, SUV and Truck**","3c05836b":"The plot shows that the bigger the cars are the more CO2 they emit","5f4e6df0":"## 9.5.2 Lasso Regression Model<a id=\"lasso\"><\/a>","578146ce":"## 9.1 BIAS <a id=\"bias\"><\/a>","7ad958d1":"### 6.5.1 Assumption #1: If Target Variable is Numeric<a id=\"tgt_num\"><\/a>","93f9cdb1":"## 9.2 VARIANCE<a id=\"var\"><\/a>","dca76dd0":"Since Model has 2053 unique values and has no significance with respect to CO2 Emissions , I have dropped this column","3e652b4f":"### 4.1.4 Duplicate Data <a id='duplicate'><\/a>","bc066825":"## 9.5.1 Ridge Regression Model<a id=\"ridge\"><\/a>","61415b26":"All features except pca_14 and pca_22 have been retained for the betterment of the model","e84fc8cc":"# Data Dictionary","217cc4d7":"## 6.3 Feature Scaling<a id='fea_sca'><\/a>","74e4c446":"**The indexes have been reset but a new column 'index' is created which needs to be dropped**","49ae54d5":"## 7.4 Compare Regression Results<a id=\"com_reg_res\"><\/a>","afe13850":"Obtained similar results as that of Forward Selection where all features except pca_14 and pca_22 have been retained for the betterment of the model","7ab453ab":"**Interpretation:** The diagonal line (red line) is the regression line and the blue points are the cumulative distribution of the residuals. As some of the points are not close to the diagonal line, I conclude that the residuals do not follow a `normal distribution`.","b2e4ff04":"**Checking for duplicate data after removal of duplicates**","28d49e43":"### 6.9.3 Assumption #3: Checking for Heteroskedasticity<a id=\"che_het\"><\/a>","617332f7":"### 4.2.1 Summary Statistics <a id='Summary_Statistics'><\/a>","0280324c":"https:\/\/sci-hub.se\/https:\/\/ieeexplore.ieee.org\/abstract\/document\/7984819","af7bea19":"**Of all the optimization techniques used, I see that Lasso Regression using Grid search CV has been the most effective in reducing RMSE . the exact combination of features responsible for high CO2 emissions cannot be predicted  Since all the features are highly correlated . I can hereby conclude that I have successfully built a model that can predict amount of CO2 Emissions across different vehicle types at a high accuracy rate.**","d5bda902":"<br>Inferences from each Plot:<\/br>\n<br>    1. CO2 Emissions v\/s Make: While Ford cars are mainly found on the roads of Canada , its Bugatti that emit the most CO2 per car<\/br>\n<br>    2. CO2 Emissions v\/s Model: Bugatti Chiron is amongst the most CO2 emitting car model<\/br>\n<br>    3. CO2 Emissions v\/s Vehicle_Class: Most of the heavy vehicles like Vans , SUV and Pick-up truck are amongst the top few emitters of CO2<\/br>\n<br>    4. CO2 Emissions v\/s Transmission: Most of the cars with automatic transmission emit CO2<\/br>\n<br>    5. CO2 Emissions v\/s Fuel_Type: Cars using Fuel Type E are emitting the most CO2<\/br>","292951b8":"### 4.2.5 Analyse Relationship between Target and Independent Variables <a id='analyze_tar_ind_var'><\/a>","354caba0":"### 5.1.3 Re-checking Correlation<a id='rec_cor'><\/a>","3cd9883b":"**The R2 value is similar to the one obtained in the MLR model. There are no significant changes.**","e88c7265":"### 4.1.6 Final Dataset <a id='final_dataset'><\/a>","213062fa":"# 9.4 GRADIENT DESCENT<a id=\"gra_des\"><\/a>","ced82c06":"**Concatenate numerical and dummy encoded categorical variables**","f4414011":"**Create a new feature Make_Type by combining various car companies(Make) on the basis of their functionality**","f780c352":"Interpretations:\n    1. 99.3% of the variation in CO2 emissions is explained by the model .\n    2. The Durbin-Watson test statistic is 2.051 and indicates that there is no auto-correlation\n    3. The Condition Number is 23.4 which suggests that there is no mutli-collinearity","892aed8e":"**Visual proof that there are no missing values**","65992561":"## 8.1 Mean Squared Error (MSE)<a id=\"mse\"><\/a>","cffeafae":"<br>Inferences from each Plot:<\/br>\n<br>    1. Top 10 Make: Most of the cars on Canadian roads are made by Ford<\/br>\n<br>    2. Top 10 Model: The F-150 FFV is amongst the most famous models driven in Canada<\/br>\n<br>    3. Top 10 Vehicle_Class: SUV-Small is the preferred class of vehicle amongst the Canadians<\/br>\n<br>    4. Top 10 Transmission: More than 1000 cars have AS6 and AS8 transmission types<\/br>\n<br>    5. Fuel Type: Majority of the cars in Canada use Fuel type X and Z<\/br>","4353f0a3":"# 11. Conclusion<a id=\"conclu\"><\/a>","ff7b6e16":"**Interpretation:** I can see that the value of adjusted R-squared calculated using the formula and the one obtained from the model are nearly same. I can also obtain this value from the summary of the model.","12049631":"Breusch-Pagan is one of the tests for detecting heteroskedasticity in the residuals.<br>\nThe test hypothesis for the Breusch-Pagan test is given as:\n<p style='text-indent:25em'> <strong> H<sub>o<\/sub>:  There is homoscedasticity present in the data <\/strong> <\/p>\n<p style='text-indent:25em'> <strong> H<sub>1<\/sub>:  There is a heteroscedasticity present in the data <\/strong> <\/p>","cf19fd88":"## 8.3 Mean Absolute Error (MAE)<a id=\"mae\"><\/a>","5b93f331":"## 7.1 Measures of Variation<a id=\"mea_var\"><\/a>","8a7ac265":"### 4.1.3 Missing Values <a id='Missing_Values'><\/a>","41d86331":"CO2_Emissions is bi-modal in nature","0c0dfb8f":"The above output indicates how much variance each component holds and the last 6 components hold no variance","ad436438":"https:\/\/coemission.herokuapp.com\/","fb2bfc8b":"<table>\n    <tr>\n        <td>\n            <img src=\"https:\/\/i.gifer.com\/6FR.gif\">\n        <\/td>\n    <\/tr>\n<\/table>","65555638":"1. **Make**  \u2192 Company of the vehicle\n2. **Model**  \u2192 Car model\n3. **Vehicle_Class**  \u2192 Class of vehicle depending on their utility, capacity and weight\n4. **Engine_Size**  \u2192 Size of engine in terms of Litre\n5. **Cylinders**  \u2192 Number of cylinders\n6. **Transmission**  \u2192 Transmission type with number of gears\n7. **Fuel_Type**  \u2192 Type of Fuel used\n8. **Fuel_Consumption_City**  \u2192 Fuel consumption in city roads (L\/100 km) \n9. **Fuel_Consumption_Hwy**  \u2192 Fuel consumption in Hwy roads (L\/100 km)\n10. **Fuel_Consumption_Comb**  \u2192 The combined fuel consumption (55% city, 45% highway) is shown in L\/100 km\n11. **Fuel_Consumption_Comb1**   \u2192 The combined fuel consumption in both city and highway is shown in mile per gallon(mpg)\n12. **CO2_Emissions**   \u2192 The tailpipe emissions of carbon dioxide (in grams per kilometre) for combined city and highway driving (Target\/dependent variable)","968c8e72":"Interpretations:\n    1. 99.3% of the variation in CO2 emissions is explained by the model .\n    2. The Durbin-Watson test statistic is 2.053 and indicates that there is no auto-correlation\n    3. The Condition Number is 23.4 which suggests that there is no mutli-collinearity","877f8f06":"<table align=\"center\" width=100%>\n    <tr>\n        <td width=\"25%\">\n            <img src=\"https:\/\/monophy.com\/media\/3o7aD3LftJ423GBsVG\/monophy.gif\">\n        <\/td>\n        <td>\n            <div align=\"center\">\n                <font color=\"#0B2F02\" size=24px>\n                    <b>Carbon Dioxide Emissions\n                    <\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","e6c18652":"<b>Interpretation<\/b>: The second subplot (on the right) shows that the elastic-net regression has reduced the coefficients of some variables to zero.","45c0f257":"# 12.Deployment<a id=\"deploy\"><\/a>","9bfc5412":"## 7.3 Confidence Interval for Intercept and Slope<a id=\"con_int_slo\"><\/a>","880f8942":"The graph shows the performance metrics root mean squared error, R-squared and Adjusted R-squared of the models implemented: the X-axis has the model number as given in the table. \nThe plot gives a clear picture of the inverse relation of R squared values and the RMSE value, the better the R-squared value naturally the lesser is the RMSE value.\nFindings suggest that the Lasso Regression (using GridSearchCV) has the highest accuracy with lowest RMSE. Finally, it can be concluded that the Lasso Regression (using GridSearchCV) can be used to predict the amount of carbon dioxide emissions.","fadd22a4":"# 9.5  Regularization<a id=\"reg\"><\/a>","effacf12":"### 5.1.1 Discovery of Outliers<a id='dis_out'><\/a>","430b1040":"#### 6.9.4.2 Shapiro Wilk Test<a id=\"sha_wil_tes\"><\/a>","7028869b":"The final dataset has **6281 records and 12 features with no missing and duplicate values**","e3dcf843":"### 6.7.2 Backward Elimination<a id=\"bac_eli\"><\/a>","43c0bf84":"## 4.2 Understanding the Dataset <a id='Data_Understanding'><\/a>","1dd8ae5a":"<b>Interpretation<\/b>: The second subplot (on the right) shows that the lasso regression have reduced the coefficients of some variables to zero.","38bf776a":"### 4.2.3 Analyse Categorical Variables <a id='analyze_cat_var'><\/a>","e28811ca":"## 6.9 Assumptions After Multiple Linear Regression Model<a id=\"ass_aft\"><\/a>","f6b4afba":"# 9. Model Optimization<a id=\"mod_opt\"><\/a>","e85ca88f":"<br>Inferences:<\/br>\n<br>    1. Fuel_Consumption_Comb1 has a high negative correaltion(<-0.9) with CO2_Emissions, Fuel_Consumption_Comb and Fuel_Consumption_City<\/br>\n<br>    2. CO2_Emissions has high positive correlation(>0.9) with Fuel_Consumption_Comb and Fuel_Consumption_City<\/br>","bb3988d2":"The Shapiro Wilk test is used to check the normality of the residuals. The test hypothesis is given as:<br>\n\n<p style='text-indent:25em'> <strong> H<sub>o<\/sub>:  Residuals are normally distributed <\/strong> <\/p>\n<p style='text-indent:25em'> <strong> H<sub>1<\/sub>:  Residuals are not normally distributed <\/strong> <\/p>","47aaf965":"**Identifying outliers using IQR**","64bd0347":"### 4.2.6 Feature Engineering <a id='feature_eng'><\/a>","12c21357":"## 9.3.1 Cross Validation<a id=\"cro_val\"><\/a>","3603765d":"There are **6281 records** after dropping duplicates","6ddf9028":"## 6.2 Feature Transformation<a id='fea_tra'><\/a>","7f262503":"# 9.3. MODEL VALIDATION<a id=\"mod_val\"><\/a>","157da93b":"Since the numeric features are not normal I am removing the outliers using IQR method","77893fae":"## 6.8 Multiple Linear Regression - Full Model - After Feature Selection<a id=\"mod_fea_sel\"><\/a>","f7de0edc":"# 2. Set Options <a id='set_options'><\/a>","19a6a8a0":"### 6.9.1 Assumption #1: Linear Relationship Between Dependent and Independent Variable<a id=\"lr_dep_ind\"><\/a>","3ec08a71":"### 6.9.4 Assumption #4: Tests for Normality<a id=\"tes_nor\"><\/a>","d5afab59":"**Interpretation:** The value of R-squared is 0.993. Thus, I conclude that the 99.3% variation in the CO2_Emissions is explained by the model. I can also obtain this value from the summary of the model.","ab98eba1":"## 6.7 Feature Selection<a id=\"fea_sel\"><\/a>","ec21075c":"<br>From the above scatter plot i can see that:<\/br>\n<br>    1. As the number of cylinders increase, the CO2 emissions increase<\/br>\n<br>    2. Cars with 8 and less than 8 cylinders prefer using Fuel Type X which result in less emissions of CO2<\/br>\n<br>    3. Fuel Type Z results in more CO2 emissions than the other<\/br>","12c15b42":"## 5.2 Categorical Encoding<a id='cat_enc'><\/a>","691c4791":"Inferences:\n    1. Fuel_Consumption_Comb1 shows a negative relation with all the other numerical variables\n    2. Fuel_Consumption_City and Fuel_Consumption_Hwy are strongly postively related","13516cc7":"## 6.1 Multiple Linear Regression - Basic Model<a id='bas_mod'><\/a>","949b0aed":"**There are 42 unique Car Companies. I will divide these companies into Luxury, Sports, Premium and General cars**","c77590f6":"### 4.2.4 Analyse Target Variable <a id='analyze_tar_var'><\/a>","05655ec6":"## 5.1 Outliers <a id='out'><\/a>","bf96fa51":"<br>Inferences:<\/br>\n<br>    1. There are a total of 42 different car companies with 2053 different car models<\/br>\n<br>    2. Vehicles are divided into 16 different classes with SUV-Small vehicles frequenting the most<\/br>\n<br>    3. 4 different types of fuels used by cars have been identified and fuel X seems to be the most famous<\/br>\n<br>    4. Most of the cars have AS6 transmission<\/br>","def66460":"**Interpretation:** From the above output, I can verify that SST (Total variation) is the sum of SSR and SSE.","1ed035e1":"**Checking the normality of numeric features**","a17c19ba":"Interpretations:\n    1. 99.5% of the variation in CO2 emissions is explained by the model.\n    2. The Durbin-Watson test statistic is 2.006 and indicates that there is no auto-correlation\n    3. The Condition Number is 1.00e+16 which suggests that there is severe mutli-collinearity\n    4. The features taken into consideration are of different scales","c52cf23f":"### 4.2.2 Correlation <a id='correlation'><\/a>","07a240aa":"## 6.4 Multiple Linear Regression - Full Model - After Feature Scaling<a id='mod_aft_sca'><\/a>","0dafa2f6":"# 10. Displaying score summary<a id=\"dis_sco_sum\"><\/a>","20eef620":"**Numeric Variables**","0c0b600e":"**Interpretation:** The above plots show no specific pattern, implies that there is a linearity present in the data.","4cbcee9b":"Interpretations:\n    1. 99.5% of the variation in CO2 emissions is explained by the model .\n    2. The Durbin-Watson test statistic is 2.006 and indicates that there is no auto-correlation\n    3. The Condition Number is 1.24e+16 which suggests that there is severe mutli-collinearity","fa3f4b91":"Overall F-Test & p-value of the Model","7b7a98d3":"# 5. Data Preprocessing <a id='data_pre'><\/a>","95fbe7bc":"# 1. Import Libraries <a id='import_lib'><\/a>","bb9147c9":"Of all the three statistics, the mean is the largest, while the mode is the smallest thus CO2_Emissions is positively skewed which implies that most of the CO2 Emissions are less than the average CO2 Emissions.","a940ab95":"## 9.5.4 Grid Search CV<a id=\"gri_sea\"><\/a>","1ab152bb":"# 4. Exploratory Data Analysis <a id='data_preparation'><\/a>","77fc7722":"# 3. Read Data <a id='Read_Data'><\/a>","be20ac33":"**Categorical Variables**","db01bda1":"The plot shows that Sports cars and Luxury cars emit more CO2 compared to Premium and General use cars","d40caf08":"**Let's check the relationship between Cylinders and CO2 Emissions**","9b6ac0f6":"### 4.1.2 Data Types <a id='Data_Types'><\/a>","bf81487d":"<b> INTERPRETATION<\/b>: The bias is low and variance is high, hence I assume that the model is a complex one. I will have to employ optimization techniques to reduce the complexity and RMSE.","acfafb4a":"# 13. References<a id=\"Refer\"><\/a>","adcea1b9":"<table align=\"center\" width=100%>\n    <tr>\n        <td width=\"30%\">\n            <img src=\"https:\/\/i.pinimg.com\/originals\/60\/00\/50\/600050674a955d69dc5930c45321be30.gif\">\n        <\/td>\n        <td>\n            <div align=\"center\">\n                <font color=\"#208807 \" size=24px>\n                    <b>Thank You.\n                    <\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","3568ed00":"### 5.1.2 Removal of Outliers<a id='rem_out'><\/a>","a740205b":"Interpretation: I can see that Lasso Regression (using GridSearchCV) has the lowest test RMSE.","f32c7125":"## 8.2 Root Mean Squared Error (RMSE)<a id=\"rmse\"><\/a>","16ae3db5":"**Filter the numeric and categorical features**","dc444068":"**The last 5 index values range from 7379-7383 but I have only 6281 records thus the indexes need to be reset**","e3558221":"https:\/\/reader.elsevier.com\/reader\/sd\/pii\/S2352484719301088?token=807922D7C5CF2E7E78C846212A5D7F97FFCC0B513EDBEAAC2626D7FB0DBE7EFE67FEBE723E7610FC62CA1FA0F5B5110A&originRegion=eu-west-1&originCreation=20210510125616","58a459be":"There are **no missing values** present in this dataset","ee872c98":"## 9.5.3 Elastic-Net Regression Model<a id=\"ela_net\"><\/a>","02802e4f":"### 6.5.2 Assumption #2: Presence of Multi-Collinearity<a id=\"pre_mul_col\"><\/a>","ef2d5e91":"Since the skewness is relatively low, there is no need to perform any further transformations to reduce skewness","aca76cd0":"From the above histogram, I can see that CO2_Emissions is moderately positive skewed","f1907c2c":"# 8. Model Performance<a id=\"mod_per\"><\/a>","5a4f67a0":"**Interpretation:** From the above test I can see that the p-value is 2.153e-37 (less than 0.05), thus I can say that the residuals are not normally distributed.","5d787b96":"**Getting rid of duplicate data**","06263319":"<b>Interpretation:<\/b> The coefficients obtained from ridge regression have similar values as compared to the coefficients obtained from linear regression using OLS.","e35a2c7b":"### 4.1.5 Indexing <a id='indexing'><\/a>","2f929bb0":"## 6.5 Assumptions Before Multiple Linear Regression Model<a id=\"ass_bef\"><\/a>","193ebe05":"**Visualising missing values using Heatmap**","0260261d":"As you can see from the above graph, 28 components describe almost 98% of variance in features","4237829d":"**Interpretation:** From the above summary, I can observe that the value obtained from the `Durbin-Watson` test statistic is close to 2 (= 2.051). Thus, I conclude that there is no autocorrelation.","a2dd4987":"**Interpretation:** As, the p-value is less than 0.05, I accept the alternate hypothesis; i.e. the model is significant."}}