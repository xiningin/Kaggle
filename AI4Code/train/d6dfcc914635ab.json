{"cell_type":{"2524a2a5":"code","9e6b1efc":"code","c057a88b":"code","03ef501c":"code","48933790":"code","89a91a0d":"code","a6527651":"code","11f5f00e":"code","cf9cefb3":"code","769c7ae1":"code","a5673080":"code","c306566e":"code","6e88b84e":"markdown","8fe07b84":"markdown","6d82f6c0":"markdown","f9d26d1a":"markdown","209aa31c":"markdown","4a9f48d8":"markdown","d90649c9":"markdown","39ea5483":"markdown","853bb3cb":"markdown","6ea5fae4":"markdown","b2a825b5":"markdown","7560c297":"markdown"},"source":{"2524a2a5":"#import modules\nimport pandas as pd\nfrom keras import applications\nimport tensorflow as tf\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\nfrom keras.models import load_model\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Dense, Flatten, Dropout, Input\nfrom keras import Model","9e6b1efc":"df_train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","c057a88b":"y_train = df_train[\"label\"]\nonehot_encoder = OneHotEncoder(sparse=False, n_values=10)\ny_train = onehot_encoder.fit_transform(np.array(y_train).reshape(-1, 1))","03ef501c":"#remove the label column\nX_train = df_train.drop(labels = [\"label\"], axis = 1)\n#Test dataset does not have label column\nX_test = df_test\n#free some space\ndel df_train \n\nX_train = X_train.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)","48933790":"#convert the grayscale image (single channel) to RGB (3 channel) so it becomes 28x28x3\nX_train = tf.image.grayscale_to_rgb(X_train, name=None)\nX_test = tf.image.grayscale_to_rgb(X_test, name=None)\n\n#resize image to twice it size so we got an integer scale factor of 2, the image becomes 56x56x3\nX_train = tf.keras.backend.resize_images(X_train, height_factor=2, width_factor=2, data_format='channels_last')\nX_test = tf.keras.backend.resize_images(X_test, height_factor=2, width_factor=2,data_format='channels_last')","89a91a0d":"sess = tf.Session()\nwith sess.as_default():\n    X_train = X_train.eval()\n    X_test = X_test.eval()","a6527651":"X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.1, random_state=42)","11f5f00e":"\n#I already uploaded the vgg16 model(without the top layer because we are going to replace it with our own) in this notebook so we are just going to load it\nvgg_model = load_model('..\/input\/vgg16-model\/vgg16.h5')\n#create dictionary of model layers so its easy to access them later\nlayer_dict = dict([(layer.name, layer) for layer in vgg_model.layers])\nvgg_model.summary()","cf9cefb3":"#from the output above, we saw that block5_pool is the last\/top layer of the vgg16 we have so we will add our layers from that point\nx = layer_dict['block5_pool'].output\n#add flatten layer so we can add the fully connected layer later\nx = Flatten()(x)\n#Fully connected layer\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.3)(x)\n#this is the final layer so the size of output in this layer is equal to the number of class in our problem\nx = Dense(10, activation='softmax')(x)\n#create the new model\ncustom_model = Model(input=vgg_model.input, output=x)\n#freeze the vgg16 layers so they will not be retrained\nfor layer in custom_model.layers[:19]:\n    layer.trainable = False\n#compile the model\ncustom_model.compile(loss='categorical_crossentropy',\n                     optimizer='adam',\n                     metrics=['accuracy'])","769c7ae1":"is_train = False #set this to True to train the model otherwise it will load the model I have already trained\nif is_train is True:\n    custom_model.fit(x=X_train, y=y_train, epochs=5, validation_data=(X_cv, y_cv), batch_size=512)\nelse:\n    custom_model = load_model('..\/input\/digitrec-model\/digit_model.h5')","a5673080":"#predict the X_test using the model we created\npredict_all = False #set this to true to predict the entire test set otherwise it will only predict 5 as sample\nif predict_all is False:\n    X_test = X_test[:5, :, :, :]\nresults = custom_model.predict(X_test, verbose=1)\n#get the index of the maximum probability and use that as the representation of the class (digit)\nresults = np.argmax(results, axis = 1)","c306566e":"#create a series from results with label column\nresults = pd.Series(results,name=\"Label\")\n#combine the results with corresponding image id\nsubmission = pd.concat([pd.Series(range(1, X_test.shape[0] + 1),name = \"ImageId\"),results],axis = 1)\n#save dataframe to csv\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","6e88b84e":"Make predictions","8fe07b84":"Load dataset","6d82f6c0":"Create submission file","f9d26d1a":"Train the model","209aa31c":"Create new model by adding new layers to VGG-16 for the classification task using transfer learning\n\nThe idea is that we are going to use the whole VGG16 network to encode our image then retrain the layers we add for the classification task we have","4a9f48d8":"Load the vgg16 model with weights","d90649c9":"Format the X_train and X_test so that each sample m is an image of size 28x28x1 (this is the size of each image in the MNIST dataset)","39ea5483":"In the preprocessing stage the X_train and X_test were converted to tensors so here we are turning them back to numpy array.\n\nNote: The reason for this is that I got confused with batch generation in model.fit(I even tried fit_generator but I cannot iterate through tensors) when training the data as tensors so if anybody can suggest better options here please feel free to do so.","853bb3cb":"This kernel is especially made for beginners like me in order to introduce the concept of transfer learning.\n\nThe accuracy is only 95% and the training time is long but we can atleast see how transfer learning works.\n\nI used VGG-16 since it is the one I am familiar with.\n\nFeel free to comment any suggestions, it will help me a lot as well because I am also new to machine learning.\n","6ea5fae4":"Encode the ground truth values since it is a categorical variable","b2a825b5":"Create validation set","7560c297":"VGG-16 requires an image of size atleast 48x48 and 3 input channels so we are going to preprocess the data so that it can be made as input in the VGG-16 network."}}