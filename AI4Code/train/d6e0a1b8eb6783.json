{"cell_type":{"1a8087a1":"code","07dca23c":"code","727219ef":"code","1f89f37d":"code","f28d4f28":"code","042d17a9":"code","30af4e9b":"code","65e518f3":"code","e9250c13":"markdown","14d0c6a1":"markdown","7a8917ea":"markdown","d3b6ef38":"markdown","f0d24880":"markdown","0f54be2e":"markdown","cfddabad":"markdown","58070cc5":"markdown","9c356e01":"markdown"},"source":{"1a8087a1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","07dca23c":"import json\nimport string\n\nreviewtext = []\n\nwith open('\/kaggle\/input\/yelp-dataset\/yelp_academic_dataset_review.json') as reviews:\n        for index,review in enumerate(reviews):\n            reviewtext.append(json.loads(review)['text'])\n            if index > 500:\n                break\n\n# clean the text for tokenizing\ndef cleaner(text):\n    text = text.replace(\".\", \" fullstop \")\n    text = text.replace(\",\", \" comma \")\n    text = \"\".join(v for v in text if v not in string.punctuation).lower()\n    text = text.encode(\"utf8\").decode(\"ascii\",'ignore')\n    text = text.replace(\"\\n\", \" \")\n    return text\n\ncleaned_text = [cleaner(text) for text in reviewtext]","727219ef":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\n\ndef get_tokens(reviews):\n    \n    # fit the tokenizer on all of the strings in 'corpus' (or the cleaned review text)\n    tokenizer.fit_on_texts(reviews)\n    \n    # the total number of words is:\n    total_words = len(tokenizer.word_index) + 1\n    \n    input_sequences = []\n    \n    # for every review in the list of reviews, use the fitted tokenizer to convert the text to a sequence\n    for line in reviews:\n        \n        # get the list of words within that review\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        \n        # for each word, append the corresponding token to the input_sequences list (the sentence made up of tokenized words)\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n    return input_sequences, total_words\n\ninp_sequences, total_words = get_tokens(cleaned_text)","1f89f37d":"from keras.preprocessing.sequence import pad_sequences\nimport keras.utils as ku \n\ndef generate_padded_sequences(input_sequences):\n    \n    # find the longest sequence of words\n    max_sequence_len = max([len(x) for x in input_sequences])\n    \n    # create a numpy array of the sequences, padded to the maximum sequence length\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n    \n    # assign the predictors as the first column, and the labels as the last column?\n    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n    \n    # use kaggle utilities to convert the class vector (integers) to binary class matrix\n    label = ku.to_categorical(label, num_classes=total_words)\n    \n    return predictors, label, max_sequence_len\n\npredictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)","f28d4f28":"from keras.models import Sequential\nfrom keras import layers\n\ndef create_model(max_sequence_len, total_words):\n    \n    # number of inputs to the RNN is the longest combination of words\n    input_len = max_sequence_len-1\n    \n    model = Sequential([\n        # Input embedding layer\n        layers.Embedding(total_words, 10, input_length=input_len),\n        # LSTM hidden later\n        layers.LSTM(256, return_sequences=True),\n        layers.LSTM(256),\n        # Dropout 10% of the layer to reduce over fitting\n        layers.Dropout(0.1),\n        # Output later\n        layers.Dense(total_words, activation='softmax')  \n    ])\n    \n    model.compile(\n            loss='categorical_crossentropy',\n            optimizer='adam'\n    )\n    \n    return model\n    \nmodel = create_model(max_sequence_len, total_words)\nmodel.summary()","042d17a9":"model.fit(predictors, label, epochs=100, verbose=5)","30af4e9b":"def generate_review(seed_text, model, max_sequence_len, next_words=50):\n   \n    # for each new word\n    for _ in range(next_words):\n        \n        # turn the seed text into a value using the tokenizer\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        \n        # pad the seed text so that it's the same length as the longest sequence of words\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        \n        # predice the next word, based on the tokenised words used so far\n        predicted = model.predict_classes(token_list, verbose=0)\n        \n        output_word = \"\"\n        \n        # for each word in the list of words in the tokenizer, check if the predicted word matches\n        # if it does then the append the new word to the seed_text and make another prediction\n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \"+output_word\n    \n    # replace the punctuation words with actual punctuation\n    seed_text = seed_text.replace(\" fullstop\", \".\")\n    seed_text = seed_text.replace(\" comma\", \",\")\n    \n    # Regex to start all sentances with a capital\n    seed_text = '. '.join(sentence.capitalize() for sentence in seed_text.split(\". \")[:-1]) + \".\"\n    \n    return seed_text","65e518f3":"print(generate_review(\"I ate\", model, max_sequence_len, next_words=20))\nprint(generate_review(\"I really\", model, max_sequence_len, next_words=30))\nprint(generate_review(\"That was\", model, max_sequence_len, next_words=40))\nprint(generate_review(\"Wow\", model, max_sequence_len, next_words=50))\nprint(generate_review(\"I am\", model, max_sequence_len, next_words=60))\nprint(generate_review(\"Oh my\", model, max_sequence_len, next_words=70))\nprint(generate_review(\"This is\", model, max_sequence_len, next_words=80))\nprint(generate_review(\"What a\", model, max_sequence_len, next_words=90))\nprint(generate_review(\"My\", model, max_sequence_len, next_words=100))","e9250c13":"# Padding the sequences of tokens so that they have the same length as inputs to the RNN","14d0c6a1":"# Training the model","7a8917ea":"# Using Yelp restaurant reviews to generate fake reviews\nIn this notebook i'm exploring training a LSTM RNN with Yelp review data to generate fake reviews. It predicts the next word using a seed word(s) to build up a sentence.\n\nSome examples of the funny reviews it generated:\n\n* > That was so good. I have been to mesa grill three times. Twice for two times later. I was very excited.\n* > Wow, this babys got no love on yelp since 2010 whats up with that well we decided to try someplace new for the restaurants at the same amount of attentiveness. I thought i was going to see out of his side and walk away.\n* > I am actually horrified this place is still in business. This time, i ordered the following duck crepe soooo good, shrimp tamale very flavorful and fresh, salmon mediocre, at best... And i have had to wait a few minutes since everyone. I contacted carrie hood and asked for an update.\n* > Oh my favorite clevelandakron restaurant. Rib atmosphere. Free wifi as horrible and i thought even if i lived in vegas. This is a great place to have a good meal they have more in just have a man an apology was disintegrating into my food. The food was great and the food is very relaxed.\n* > This is a great little neighborhood pet food market that also has grooming available. More on that later. There are a lot of an donation that i have practiced yoga on the extra fries and see some of just another rice. It took me to my side and really a free birthday it.\n* > What a great restaurant in the south side we went there, and they even not offer to return, but there is minimal confusion every dish has a shuttle, the line can go out with a sampler sauce that got a time, there was a movie theater and bowling center. The buffet is very clean. They have live music jazz guitar or piano outside in the evenings most nights the well was much. The waffle was forgettable and expensive.\n* >My wife and i went there last night to celebrate my birthday. I called the word. They need to order a restaurant and per am hot of my food and i was pleasantly surprised you get a good time as they were a little too but they can like this time i had the southwestern omelette and i loved the salsa inside and above us decide like products.\n    \nThis was a learning exercise to delve into RNNs and text processing. I used [this notebook](https:\/\/www.kaggle.com\/shivamb\/beginners-guide-to-text-generation-using-lstms) for inspiration. The model seems to have learnt the correct order for basic sentences. It may be over trained and just spitting out portions of existing reviews.\n\nObservations: wider LSTMs are better at understanding the order of words, longer LSTMs are better at remembering context in longer sentences. It's difficult to train an LSTM within 9 hours! I would have liked to fed it more reviews and used a wider network but the kernel kept timing out.","d3b6ef38":"Please smash the upvote button if you found this interesting.","f0d24880":"# Loading reviews, and cleaning the text","0f54be2e":"# Examples","cfddabad":"# Generating reviews with seed words","58070cc5":"# Converting the text to sequences of tokens","9c356e01":"# Creating the RNN using LSTM (long short term memory)"}}