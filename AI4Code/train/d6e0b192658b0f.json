{"cell_type":{"7ffd64ed":"code","d4bc1ca9":"code","b8a40a92":"code","0c7017a6":"code","7776eda7":"code","7c757730":"code","92ac7108":"code","bbcedd92":"code","d1da7a1d":"code","450cfa60":"code","0f683505":"code","5437c984":"code","8d93deea":"code","b83c32cc":"code","2866dde2":"code","3fab2d52":"code","124c7548":"code","12443e3e":"code","8a9f36fd":"markdown","fe945f7d":"markdown","7f8a9263":"markdown","15e284a9":"markdown","b9b57dba":"markdown"},"source":{"7ffd64ed":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","d4bc1ca9":"train_df = pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\ntest_df = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])","b8a40a92":"hist_df = pd.read_csv(\"..\/input\/historical_transactions.csv\")","0c7017a6":"gdf = hist_df.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].size().reset_index()\ngdf.columns = [\"card_id\", \"num_hist_transactions\"]\ntrain_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")","7776eda7":"gdf = hist_df.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\ngdf.columns = [\"card_id\", \"sum_hist_trans\", \"mean_hist_trans\", \"std_hist_trans\", \"min_hist_trans\", \"max_hist_trans\"]\ntrain_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")","7c757730":"new_trans_df = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")","92ac7108":"gdf = new_trans_df.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].size().reset_index()\ngdf.columns = [\"card_id\", \"num_merch_transactions\"]\ntrain_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")","bbcedd92":"gdf = new_trans_df.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\ngdf.columns = [\"card_id\", \"sum_merch_trans\", \"mean_merch_trans\", \"std_merch_trans\", \"min_merch_trans\", \"max_merch_trans\"]\ntrain_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")","d1da7a1d":"train_df[\"year\"] = train_df[\"first_active_month\"].dt.year\ntest_df[\"year\"] = test_df[\"first_active_month\"].dt.year\ntrain_df[\"month\"] = train_df[\"first_active_month\"].dt.month\ntest_df[\"month\"] = test_df[\"first_active_month\"].dt.month\n\ncols_to_use = [\"feature_1\", \"feature_2\", \"feature_3\", \"year\", \"month\", \n               \"num_hist_transactions\", \"sum_hist_trans\", \"mean_hist_trans\", \"std_hist_trans\", \n               \"min_hist_trans\", \"max_hist_trans\",\n               \"num_merch_transactions\", \"sum_merch_trans\", \"mean_merch_trans\", \"std_merch_trans\",\n               \"min_merch_trans\", \"max_merch_trans\",\n              ]","450cfa60":"train_X = train_df[cols_to_use]\ntest_X = test_df[cols_to_use]\ntrain_y = train_df['target'].values","0f683505":"def bayes_parameter_opt_lgb(X, y, init_round=20, opt_round=30, n_folds=5, random_seed=6, n_estimators=10000,\n                            learning_rate=0.05, output_process=False):\n    # prepare data\n\n    train_data = lgb.Dataset(data=X, label=y)\n    # parameters\n\n    def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, \n                 lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n\n        params = {'objective':'regression','num_iterations':1000, 'learning_rate':0.05,\n                  'early_stopping_round':100, 'metric':'rmse'}\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['lambda_l1'] = max(lambda_l1, 0)\n        params['lambda_l2'] = max(lambda_l2, 0)\n        params['min_split_gain'] = min_split_gain\n        params['min_child_weight'] = min_child_weight\n        \n        cv_result = lgb.cv(params, train_data, nfold=3, seed=random_seed,\n                           stratified=False, verbose_eval =200, metrics=['rmse'])\n\n        return min(cv_result['rmse-mean'])\n\n    # setting range of the parameters\n    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (24, 45),\n                                            'feature_fraction': (0.1, 0.9),\n                                            'bagging_fraction': (0.5, 1),\n                                            'max_depth': (5, 8.99),\n                                            'lambda_l1': (0, 5),\n                                            'lambda_l2': (0, 3),\n                                            'min_split_gain': (0.001, 0.1),\n                                            'min_child_weight': (5, 60)}, random_state=0)\n    # optimize\n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n    # output optimization process\n    if output_process==True: lgbBO.points_to_csv(\"bayes_opt_result.csv\")\n    \n    # return\n    return lgbBO\n\nopt_params = bayes_parameter_opt_lgb(train_X, train_y, init_round=5, opt_round=10, n_folds=3,\n                                     random_seed=6, n_estimators=10000, learning_rate=0.05)","5437c984":"params = opt_params.max['params']","8d93deea":"params","b83c32cc":"params = {\n    \"objective\" : \"regression\",\n    \"metric\" : \"rmse\",\n    \"bagging_frequency\" : 5,\n    \"bagging_seed\" : 2018,\n    \"verbosity\" : -1,\n\n    # Selected rounded-off params\n    'bagging_fraction': 0.7,\n    'feature_fraction': 0.1,\n    'lambda_l1': 1,\n    'lambda_l2': 0,\n    'max_depth': 9,\n    'min_child_weight': 5,\n    'min_split_gain': 0,\n    'num_leaves': 24\n}","2866dde2":"params","3fab2d52":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, \n                      verbose_eval=100, evals_result=evals_result)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, evals_result\n\npred_test = 0\nkf = model_selection.KFold(n_splits=5, random_state=2018, shuffle=True)\nfor dev_index, val_index in kf.split(train_df):\n    dev_X, val_X = train_X.loc[dev_index,:], train_X.loc[val_index,:]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    \n    pred_test_tmp, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, test_X)\n    pred_test += pred_test_tmp\npred_test \/= 5.","124c7548":"fig, ax = plt.subplots(figsize=(12,10))\nlgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=15)\nplt.show()","12443e3e":"sub_df = pd.DataFrame({\"card_id\":test_df[\"card_id\"].values})\nsub_df[\"target\"] = pred_test\nsub_df.to_csv(\"baseline_lgb.csv\", index=False)","8a9f36fd":"## Quick Feature Engineering","fe945f7d":"## Params that were optimum","7f8a9263":"## Hyperparameter Tuning using Bayesian Hyperparameter Optimization ","15e284a9":"In this kernel we'll use the Bayesian Hyperparameter Tuning to find the optimum hyperparameters for LightGBM. This is the first kernel I'm writing so please let me know how I can improve. :)\n\nI used the following Kernels and blogs for reference\n\n* [Simple Exploration Notebook - Elo](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-elo)\n* [Simple Bayesian Optimization for LightGBM](https:\/\/www.kaggle.com\/sz8416\/simple-bayesian-optimization-for-lightgbm)\n* [A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning](https:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)\n* [Automated Machine Learning Hyperparameter Tuning in Python](https:\/\/towardsdatascience.com\/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a)","b9b57dba":"## Training LightGBM using our newly learned params."}}