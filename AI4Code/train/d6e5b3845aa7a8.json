{"cell_type":{"7117b524":"code","35636ea2":"code","e494cde5":"code","45bb6e65":"code","e5af8fdd":"code","76a292af":"code","0fe595c6":"code","978af995":"code","ed57d3ea":"code","6d104a0c":"code","8b10ec4b":"code","33d0c0c5":"code","c802a434":"code","30fe19bb":"code","45bb85bb":"code","7b33a6e9":"code","d76d4c54":"code","c4274266":"code","45b50ea6":"code","10bd67c8":"code","065544d7":"code","193814f7":"code","9ddea5bf":"code","533aee92":"code","84405c0a":"code","697254d0":"code","ab9fa9c2":"code","62ebd4c4":"code","c8e7caf4":"code","62bf547b":"code","75f0c0a9":"code","304d0590":"code","e3faa30d":"code","75f68c4f":"code","c57cae6d":"code","dd6cdf32":"code","f99aac5b":"code","9d46e04f":"code","b89e0489":"code","a79f0d13":"code","d50554fb":"code","b45ea188":"code","64d6bd21":"code","42b2dc15":"code","1f16959a":"code","5756f50c":"code","012944f7":"code","fba14f7b":"code","0b0732af":"markdown","5cf48eb1":"markdown","8468c008":"markdown","7ede6b1a":"markdown","db2eeca0":"markdown","be3fddc9":"markdown","957010eb":"markdown","dcde0408":"markdown","98afbb72":"markdown"},"source":{"7117b524":"#Import Libs\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nimport seaborn as sns\nfrom sklearn.utils import resample","35636ea2":"training_data= pd.read_csv(\"..\/input\/healthcare-dataset-stroke-data\/train_2v.csv\")\ntest_data=pd.read_csv(\"..\/input\/healthcare-dataset-stroke-data\/test_2v.csv\")","e494cde5":"training_data.head(n=5)","45bb6e65":"training_data.describe()","e5af8fdd":"test_data.describe()","76a292af":"training_data['stroke'].value_counts()","0fe595c6":"sns.catplot(x='ever_married',kind='count',col='stroke',data=training_data)","978af995":"sns.catplot(x='work_type',kind='count',col='stroke',data=training_data)","ed57d3ea":"sns.catplot(x='smoking_status',kind='count',col='stroke',data=training_data)","6d104a0c":"sns.catplot(x='Residence_type',kind='count',col='stroke',data=training_data)","8b10ec4b":"sns.catplot(x='hypertension',kind='count',col='stroke',data=training_data)","33d0c0c5":"sns.catplot(x='heart_disease',kind='count',col='stroke',data=training_data)","c802a434":"sns.distplot(training_data.loc[training_data['stroke'] == 0]['age'],kde=False, norm_hist=True, bins=20)\nsns.distplot(training_data.loc[training_data['stroke'] == 1]['age'],kde=False, norm_hist=True, bins=20)","30fe19bb":"sns.distplot(training_data.loc[training_data['stroke'] == 0]['bmi'],kde=False, norm_hist=True, bins=20)\nsns.distplot(training_data.loc[training_data['stroke'] == 1]['bmi'],kde=False, norm_hist=True, bins=20)","45bb85bb":"sns.distplot(training_data.loc[training_data['stroke'] == 0]['avg_glucose_level'],kde=False, norm_hist=True, bins=20)\nsns.distplot(training_data.loc[training_data['stroke'] == 1]['avg_glucose_level'],kde=False, norm_hist=True, bins=20)","7b33a6e9":"#check for missing trainign_data\nmissing_training_data = training_data.isnull()\nfor column in missing_training_data.columns.values.tolist():\n    if True in missing_training_data[column].value_counts().index.tolist():\n        print(column)\n        print (missing_training_data[column].value_counts())\n        print(\"\") ","d76d4c54":"#check for missing test_data\nmissing_test_data = test_data.isnull()\nfor column in missing_test_data.columns.values.tolist():\n    if True in missing_test_data[column].value_counts().index.tolist():\n        print(column)\n        print (missing_test_data[column].value_counts())\n        print(\"\") ","c4274266":"training_data['bmi'].replace(np.nan, training_data['bmi'].astype('float').mean(axis=0), inplace=True)\ntest_data['bmi'].replace(np.nan, test_data['bmi'].astype('float').mean(axis=0), inplace=True)","45b50ea6":"lbl=preprocessing.LabelEncoder()\ntraining_data['gender'] = lbl.fit_transform(training_data['gender'])\ntraining_data['ever_married'] = lbl.fit_transform(training_data['ever_married'])\ntraining_data['work_type'] = lbl.fit_transform(training_data['work_type'])\ntraining_data['Residence_type'] = lbl.fit_transform(training_data['Residence_type'])\ntest_data['gender'] = lbl.fit_transform(test_data['gender'])\ntest_data['ever_married'] = lbl.fit_transform(test_data['ever_married'])\ntest_data['work_type'] = lbl.fit_transform(test_data['work_type'])\ntest_data['Residence_type'] = lbl.fit_transform(test_data['Residence_type'])","10bd67c8":"#We can consider dropping smoking data all together\ntraining_data_nosmoke=training_data.drop(columns='smoking_status',axis=1,inplace=False)\ntest_data_nosmoke=test_data.drop(columns='smoking_status',axis=1,inplace=False)\n\n# Or replace by mode\ntraining_data['smoking_status'].fillna(training_data['smoking_status'].mode()[0], inplace=True)\ntraining_data['smoking_status'] = lbl.fit_transform(training_data['smoking_status'])\ntest_data['smoking_status'].fillna(test_data['smoking_status'].mode()[0], inplace=True)\ntest_data['smoking_status'] = lbl.fit_transform(test_data['smoking_status'])","065544d7":"# Correlation heatmap\ncorr = training_data.corr()\nsns.heatmap(corr,xticklabels=corr.columns, yticklabels=corr.columns)","193814f7":"features = list(training_data.columns[1:11])\nX = training_data[features]\ny = training_data['stroke']","9ddea5bf":"X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.2,random_state=1)\ntree = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n                       max_features=None, max_leaf_nodes=30,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=6,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=98, splitter='best')\ntree.fit(X_trainset,y_trainset)\n\n#prediction on the known test set\npred = tree.predict(X_testset)\nprint(\"Precision: \", metrics.precision_score(y_testset,pred))\nprint(\"Accuracy: \", metrics.accuracy_score(y_testset, pred))\nprint(\"Classification report: \")\nprint(metrics.classification_report(y_testset,pred))\nprint(\"\")\nprint(\"Confusion matrix: \")\nprint(metrics.confusion_matrix(y_testset,pred))","533aee92":"#Imbalanced data\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom collections import Counter\nros=RandomOverSampler(random_state=42)\nsm = SMOTE(random_state=27, ratio=1.0)\n\nprint(training_data.stroke.value_counts())\n\n\n#X_res,y_res =ros.fit_resample(X_trainset,y_trainset)\nX_res, y_res = sm.fit_sample(X_trainset, y_trainset)\n#con=pd.concat([X_trainset,y_trainset], axis=1)\n#stroke = con[con.stroke==1]\n#no_stroke = con[con.stroke==0]\n#stroke_upsampled = resample(stroke,\n#                          replace=True, # sample with replacement\n#                          n_samples=len(no_stroke), # match number in majority class\n#                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\n#upsampled = pd.concat([no_stroke, stroke_upsampled])\n\n# check new class counts\nprint('Resampled dataset shape %s' % Counter(y_res))\n\n\n#X_trainset_up = upsampled[features]\n#y_trainset_up = upsampled['stroke']\n\n\n#tree_up = DecisionTreeClassifier()\n#tree_up.fit(X_trainset_up,y_trainset_up)\n#pred_up = tree.predict(X_testset)\ntree_up = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=15,\n                       max_features=None, max_leaf_nodes=30,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=6,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=98, splitter='best')\ntree_up.fit(X_res,y_res)\npred_up = tree_up.predict(X_testset)\n#prediction on the known test set\nprint(\"Precision: \", metrics.precision_score(y_testset,pred_up))\nprint(\"Accuracy: \", metrics.accuracy_score(y_testset,pred_up))\nprint(\"Classification report: \")\nprint(metrics.classification_report(y_testset,pred_up))\nprint(\"\")\nprint(\"Confusion matrix: \")\nprint(metrics.confusion_matrix(y_testset,pred_up))","84405c0a":"#undersample\nfrom sklearn.utils import resample\n# downsample majority\nZ = pd.concat([X_trainset, y_trainset], axis=1)\nnot_stroke = Z[Z.stroke==0]\nstroke = Z[Z.stroke==1]\nnot_stroke_downsampled = resample(not_stroke,\n                                replace = False, # sample without replacement\n                                n_samples = len(stroke), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_stroke_downsampled, stroke])\n\n# checking counts\ndownsampled.stroke.value_counts()\ny_down= downsampled.stroke\nX_down = downsampled.drop('stroke', axis=1)\n\ntree_down = DecisionTreeClassifier()\ntree_down.fit(X_down,y_down)\npred_down = tree_up.predict(X_testset)\n#prediction on the known test set\nprint(\"Precision: \", metrics.precision_score(y_testset,pred_down))\nprint(\"Accuracy: \", metrics.accuracy_score(y_testset,pred_down))\nprint(\"Classification report: \")\nprint(metrics.classification_report(y_testset,pred_down))\nprint(\"\")\nprint(\"Confusion matrix: \")\nprint(metrics.confusion_matrix(y_testset,pred_down))","697254d0":"# plot feature importance\nplt.bar(range(len(tree.feature_importances_)), tree.feature_importances_)\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Importance\")\nplt.title(\"Feature Importance\")\nplt.xticks(range(len(tree.feature_importances_)), features,rotation='vertical')\nplt.show()\nprint(features)","ab9fa9c2":"# plot feature importance\nplt.bar(range(len(tree_up.feature_importances_)), tree_up.feature_importances_)\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Importance\")\nplt.title(\"Feature Importance\")\nplt.xticks(range(len(tree_up.feature_importances_)), features,rotation='vertical')\nplt.show()\nprint(features)","62ebd4c4":"#prediction on unknown test data\npred=tree.predict(test_data[features])\nprediction = pd.DataFrame(pred,columns=['pred_stroke'])\ntest_data['stroke']=prediction['pred_stroke']\ntest_data['stroke'].value_counts()","c8e7caf4":"#prediction on unknown test data\npred=tree_up.predict(test_data[features])\nprediction = pd.DataFrame(pred,columns=['pred_stroke'])\ntest_data['stroke']=prediction['pred_stroke']\ntest_data['stroke'].value_counts()","62bf547b":"sns.catplot(x='ever_married',kind='count',col='stroke',data=test_data)","75f0c0a9":"sns.catplot(x='work_type',kind='count',col='stroke',data=test_data)","304d0590":"sns.catplot(x='smoking_status',kind='count',col='stroke',data=test_data)","e3faa30d":"sns.catplot(x='Residence_type',kind='count',col='stroke',data=test_data)","75f68c4f":"sns.catplot(x='hypertension',kind='count',col='stroke',data=test_data)","c57cae6d":"sns.distplot(test_data.loc[test_data['stroke'] == 0]['age'],kde=False, norm_hist=True, bins=20)\nsns.distplot(test_data.loc[test_data['stroke'] == 1]['age'],kde=False, norm_hist=True, bins=20)","dd6cdf32":"sns.distplot(test_data.loc[test_data['stroke'] == 0]['avg_glucose_level'],kde=False, norm_hist=True, bins=20)\nsns.distplot(test_data.loc[test_data['stroke'] == 1]['avg_glucose_level'],kde=False, norm_hist=True, bins=20)","f99aac5b":"features = list(training_data_nosmoke.columns[1:10])\nprint(features)\nX = training_data_nosmoke[features]\ny = training_data_nosmoke['stroke']","9d46e04f":"X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.2, random_state=2)\ntree = DecisionTreeClassifier()\ntree.fit(X_trainset,y_trainset)","b89e0489":"#prediction on the known test set\npred = tree.predict(X_testset)\nprint(\"Precision: \", metrics.precision_score(y_testset,pred))\nprint(\"Accuracy: \", metrics.accuracy_score(y_testset, pred))\nprint(\"Classification report: \")\nprint(metrics.classification_report(y_testset,pred))\nprint(\"\")\nprint(\"Confusion matrix: \")\nprint(metrics.confusion_matrix(y_testset,pred))","a79f0d13":"ros=RandomOverSampler(random_state=42)\nsm = SMOTE(random_state=27, ratio=1.0)\n\nprint(training_data_nosmoke.stroke.value_counts())\n\n\n#X_res,y_res =ros.fit_resample(X_trainset,y_trainset)\nX_res, y_res = sm.fit_sample(X_trainset, y_trainset)\n#con=pd.concat([X_trainset,y_trainset], axis=1)\n#stroke = con[con.stroke==1]\n#no_stroke = con[con.stroke==0]\n#stroke_upsampled = resample(stroke,\n#                          replace=True, # sample with replacement\n#                          n_samples=len(no_stroke), # match number in majority class\n#                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\n#upsampled = pd.concat([no_stroke, stroke_upsampled])\n\n# check new class counts\nprint('Resampled dataset shape %s' % Counter(y_res))\n\n\n#X_trainset_up = upsampled[features]\n#y_trainset_up = upsampled['stroke']\n\n\n#tree_up = DecisionTreeClassifier()\n#tree_up.fit(X_trainset_up,y_trainset_up)\n#pred_up = tree.predict(X_testset)\ntree_up = DecisionTreeClassifier()\ntree_up.fit(X_res,y_res)\npred_up = tree_up.predict(X_testset)\n#prediction on the known test set\nprint(\"Precision: \", metrics.precision_score(y_testset,pred_up))\nprint(\"Accuracy: \", metrics.accuracy_score(y_testset,pred_up))\nprint(\"Classification report: \")\nprint(metrics.classification_report(y_testset,pred_up))\nprint(\"\")\nprint(\"Confusion matrix: \")\nprint(metrics.confusion_matrix(y_testset,pred_up))","d50554fb":"# plot feature importance\nplt.bar(range(len(tree.feature_importances_)), tree.feature_importances_)\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Importance\")\nplt.title(\"Feature Importance\")\nplt.xticks(range(len(tree.feature_importances_)), features,rotation='vertical')\nplt.show()\nprint(features)","b45ea188":"#prediction on unknown test data\npred=tree.predict(test_data_nosmoke[features])\nprediction = pd.DataFrame(pred,columns=['pred_stroke'])\ntest_data_nosmoke['stroke']=prediction['pred_stroke']\ntest_data_nosmoke['stroke'].value_counts()","64d6bd21":"sns.catplot(x='ever_married',kind='count',col='stroke',data=test_data_nosmoke)","42b2dc15":"sns.catplot(x='work_type',kind='count',col='stroke',data=test_data_nosmoke)","1f16959a":"sns.catplot(x='Residence_type',kind='count',col='stroke',data=test_data_nosmoke)","5756f50c":"sns.catplot(x='hypertension',kind='count',col='stroke',data=test_data_nosmoke)","012944f7":"sns.distplot(test_data_nosmoke.loc[test_data_nosmoke['stroke'] == 0]['age'],kde=False, norm_hist=True, bins=20)\nsns.distplot(test_data_nosmoke.loc[test_data_nosmoke['stroke'] == 1]['age'],kde=False, norm_hist=True, bins=20)","fba14f7b":"sns.distplot(test_data_nosmoke.loc[test_data_nosmoke['stroke'] == 0]['avg_glucose_level'],kde=False, norm_hist=True, bins=20)\nsns.distplot(test_data_nosmoke.loc[test_data_nosmoke['stroke'] == 1]['avg_glucose_level'],kde=False, norm_hist=True, bins=20)","0b0732af":"~30% of smoking data is missing, hence it seems reasonable to consider dropping this data or replacing it","5cf48eb1":"## Catagorised data\nwe are going to build decision tree so we need to turn \"ever_married, work_type, residence_type,smoking status\" to numerical values","8468c008":"#### With mode smoking data","7ede6b1a":"Predictive model: We use a predictive model to classify patients who have \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \nstroke. We evaluate the performance of the model and suggest which features may be \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 useful in stroke prediction.\n\nA report of the findings and analysis can be found [here](https:\/\/github.com\/TBourton\/Public\/blob\/master\/Stroke_with_Decision_Trees_project\/report.pdf)\n\nWritten by Thomas Bourton","db2eeca0":"#### With smoking removed","be3fddc9":"### Missing data","957010eb":"# Building the Decision Tree\n\nNow that we have cleaned up our data we will build our decision tree","dcde0408":"# Data Wrangling","98afbb72":"Clearly bmi and smoking_status contain a lot of missing values in both training and test data\nFor BMI we can replace the missing data with the average"}}