{"cell_type":{"b199accc":"code","325f091b":"code","ab754eba":"code","2d1faf9a":"code","68c75cab":"code","5a468395":"code","f758dae2":"markdown","a0b8dec6":"markdown","231939d3":"markdown","a88c57a3":"markdown","76ad33ae":"markdown","065e74c6":"markdown","8a3aa528":"markdown","9c4a6ad6":"markdown"},"source":{"b199accc":"#################################################################\n#### Libs and globals ####\n\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nseed = 11\n\n# libraries for cv\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\n# libraries for random search\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pandas.io.json import json_normalize\nfrom matplotlib import pyplot as plt\n\n# libs for timing\nimport time\nimport datetime\n\n# iterations\nn_iter=35\ncv=5\n\n#################################################################\n#### Reading data and splitting it to target and features ####\n\ntrain = pd.read_csv('..\/input\/learn-together\/train.csv', index_col=\"Id\")\ndel train['Soil_Type15']\ndel train['Soil_Type7']\n\nfeatures = train.drop(['Cover_Type'], axis=1)\ntarget = train.Cover_Type\n\ndef fit_and_draw(key, value):\n    \"\"\" performing random search and returning a result set for different parameter values (with cv)\"\"\"\n    param_dist = {key: value}\n    \n    start_time = time.time()\n\n    random_search = RandomizedSearchCV(estimator=model,\n                                 param_distributions=param_dist,\n                                 scoring ='accuracy',\n                                 n_iter=n_iter,\n                                 cv=cv,\n                                 n_jobs=-1, \n                                 verbose=0)\n    \n    random_search.fit(features, target)\n    \n    trial_time = time.time() - start_time \n    trial_time_per_search = trial_time\/(cv * n_iter)\n    trial_time = trial_time\/60\n    \n    print('')\n    print('')\n    print(f'>>>> Results for {key}:')    \n    print(f'> {(trial_time):.1f} minutes for search ({n_iter} iterations with {cv}-fold CV)')\n    print(f'> {(trial_time_per_search):.1f} seconds per 1 search without CV')\n\n    results = pd.DataFrame(random_search.cv_results_)[['params', 'mean_test_score']]\n\n    params = pd.io.json.json_normalize(results['params'])\n\n    result = (\n        pd.merge(results, params, how='inner', left_index=True, right_index=True)\n        .drop(columns=['params'])\n        .groupby(key).mean()\n    )\n\n    return result\n\ndef demo():\n    for key in param_dist:\n\n        random_search = RandomizedSearchCV(estimator=model,\n                                         param_distributions=param_dist,\n                                         scoring ='accuracy',\n                                         n_iter=n_iter,\n                                         cv=cv,\n                                         n_jobs=-1, \n                                         verbose=0)\n\n        random_search.fit(features, target)\n\n        # processing results\n        results = pd.DataFrame(random_search.cv_results_)[['params', 'mean_test_score']]\n        params = pd.io.json.json_normalize(results['params'])\n\n        result = (\n                pd.merge(results, params, how='inner', left_index=True, right_index=True)\n                .drop(columns=['params'])\n                .sort_values(by=key)\n        )\n\n        result.index = result[key]\n\n\n        # printing plot\n        ax = plt.subplot()\n        plt.plot(result.index, result.mean_test_score)\n        plt.title(key)\n        plt.show()","325f091b":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(random_state=seed)\nparam_dist = {'n_estimators':np.arange(1, 250)}\ndemo()","ab754eba":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_jobs=-1)\nparam_dist = {'n_neighbors': np.arange(1, 50)}\ndemo()","2d1faf9a":"param_dist = {'leaf_size' : np.arange(1, 150)}\ndemo()","68c75cab":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(random_state=seed, n_jobs=-1)\nparam_dist = {'intercept_scaling' : np.arange(0, 500)}\ndemo()","5a468395":"from sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression(random_state=seed, n_jobs=-1)\n\nLR_params = {\n    'tol': [1\/(6**c) for c in range(1, 10)],\n    'C' : np.arange(1, 250),\n    'intercept_scaling' : np.arange(0, 500),\n    'max_iter' : np.arange(0, 120) #(default=100)\n}\n\n\nfrom sklearn.linear_model import RidgeClassifier\n\nRidge = RidgeClassifier(random_state=seed)\n\nRidge_params = {\n    'alpha': np.arange(0, 150)\/100,\n    'max_iter' : np.arange(0, 50),\n    'tol': [1\/(2**c) for c in range(1, 15)],\n    'solver' : ['svd', 'cholesky','lsqr','sparse_cg']\n}\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nRF = RandomForestClassifier(random_state=seed, n_estimators=50, n_jobs=-1)\n\nRF_params = {\n    'n_estimators': np.arange(5, 250),\n    'max_depth' : np.arange(1, 50),\n    'min_samples_split' : np.arange(1, 99)\/100,\n    'min_samples_leaf' : np.arange(1, 50)\/100, \n    'min_weight_fraction_leaf' : np.arange(0, 50)\/100,  \n    'max_features' : np.arange(1, 100)\/100, \n    'max_leaf_nodes' : [None, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    'min_impurity_decrease' : np.arange(1, 99)\/100,\n}\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nKN = KNeighborsClassifier(n_jobs=-1)\n\nKN_params = {\n    'n_neighbors': np.arange(1, 50),\n    'leaf_size' : np.arange(1, 150)    \n}\n\n\nfrom lightgbm import LGBMClassifier\n\nLGBM = LGBMClassifier(random_state=seed, n_jobs=-1)\n\nLGBM_params = {\n    'num_leaves ': np.arange(1, 100),\n    'max_depth' : [2*c for c in range(0, 25)],\n    'learning_rate' : [1\/(1.8**c) for c in range(1, 15)], \n    'n_estimators' : np.arange(1, 150),  \n    'min_split_gain' : [1\/(1.4**c) for c in range(1, 15)],\n    'min_child_weight' : [1\/(2**c) for c in range(1, 15)],\n    'min_child_samples' : np.arange(1, 150),       \n    'subsample' : [1\/(2**c) for c in range(0, 5)],  \n    'subsample_freq' : [(2**c) for c in range(0, 5)],   \n    'colsample_bytree' : np.arange(1, 100)\/100,\n    'reg_alpha' : np.arange(0, 150)\/100,\n    'reg_lambda' : np.arange(0, 35)\n}\n\n\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier(random_state=seed, verbosity=0, n_jobs=-1)\n\nxgb_params = {\n    'max_depth' : np.arange(0, 50),\n    'learning_rate' : [1\/(1.8**c) for c in range(1, 15)],  \n    'n_estimators' : np.arange(1, 100),\n    'gamma' : np.arange(0, 100),\n    'min_child_weight' : np.arange(0, 50),\n    'max_delta_step': np.arange(0, 100),\n    'subsample': [1\/(1.1**c) for c in range(1, 15)],\n    'colsample_bytree' : [1\/(1.1**c) for c in range(1, 15)],\n    'colsample_bylevel' : [1\/(1.1**c) for c in range(1, 15)],\n    'colsample_bynode' : [1\/(1.1**c) for c in range(1, 15)],\n    'reg_alpha' : np.arange(0, 50),\n    'reg_lambda' : np.arange(0, 50),\n    'scale_pos_weight' : np.arange(0, 400)\/100\n}\n\n\n################################################################\n\n# drawing plots\n\nmodels_and_params = {\n    LR    :  LR_params,\n    Ridge :  Ridge_params,    \n    RF    :  RF_params,\n    KN    :  KN_params,\n#    LGBM : xgb_params,\n#    xgb   :  xgb_params\n}\n\nsession_start_time = time.time()\n\nfor key in models_and_params:\n\n    model = key\n    param_dist = models_and_params[key]\n\n    print('*'*80)\n    print('PRINTING PLOTS FOR MODEL:')\n    print(key)\n    \n    for key in param_dist:\n        result = fit_and_draw(key, param_dist[key])\n\n        ax = plt.subplot()\n        plt.plot(result.index, result.mean_test_score)\n        plt.title(key)\n        plt.show()\n\nsession_time = (time.time() - session_start_time) \/ 60\n\nprint('')\nprint('')\nprint('*'*80)\nprint(f'>>Drawing plots finished in {(session_time):.1f} minutes')","f758dae2":"I've chosen these models as they seemed to be one of the most popular linear, trees and boosting models. Similar models from the same algorithm group have similar parameters, so there was no need to do all of them because they have pretty same idea in tuning.\n\n**Important - gboost models are commented**. Kernels just do not want to fit them, I don't know why. Running script on the local machine works perfectly well. It you'd like to try gboost models - just download the script and run it at home.\n\nI will appreciate if someone will help me with learning gboost models on kaggle kernels. I can't figure out what am I doing wrong. \n\nI hope this notebook will help you with choosing the ranges of parameter tuning for your models.\n\nFeel free to upvote!","a0b8dec6":"Some others have no strong trend, like, for instance, `intercept_scaling` in LogisticRegression","231939d3":"# Types of hyperparameters score trends\n\nFor expample, RandomForest's `n_estimators` have a positive effect on the score, but just until it reaches its asymptotic level.","a88c57a3":"# Searching models\n\nI've chosen several models to show the graphs of the score variation in relation tith diferent hyperparameters. Models are:\n* LogisticRegression\n* RidgeClassifier\n* RandomForestClassifier\n* KNeighborsClassifier\n* LGBMClassifier\n* XGBClassifier","76ad33ae":"Other parameters like `n_neighbors` of KNeighborsClassifier peak the score on lower levels, and have a negative correlation with the score","065e74c6":"Some parameters like KNeighborsClassifier `leaf_size` can have no effect with defaults and can be valuable while using them with the appropriate set of other tuned params","8a3aa528":"That is why tuning parameters is going to be much easier if you know the ranges and usual trend of the score for the params.","9c4a6ad6":"Tuning hyperparameters can be complex. Some of the parameters should be in the 0-to-1 range, others have to be more than 0, others - more than 1 and so on...\n\nAlso, even if you know these acceptable ranges, you should also remember how do the trends of the score look like with tuning each of the parameters."}}