{"cell_type":{"149ddb0d":"code","2e519ca6":"code","0a01e0ed":"code","c070e75c":"code","b3f789b4":"code","e044328e":"code","8cdfc1d4":"code","e3c6fa62":"code","991cf666":"code","90133f61":"code","309f2f3e":"code","9a020ec5":"code","d8236074":"code","22c6e72f":"code","1b0ed3ec":"code","d7323d42":"code","634529a9":"code","fad4b27d":"code","0198bb3d":"code","c0b4ba89":"code","518cdce8":"code","99059526":"code","e279e52a":"code","20db3c0d":"code","1efe25cb":"code","f2f6a2f1":"code","e373e06e":"code","8e94adf4":"markdown","0964c4db":"markdown","74f359ce":"markdown","2b2140c9":"markdown","a1a24dc6":"markdown","d9c48eb1":"markdown","0904d5ef":"markdown","cb140074":"markdown","e5180b70":"markdown","03d7d184":"markdown","ab7c67af":"markdown","23f8c30e":"markdown","1330af73":"markdown","8d82f705":"markdown","b5d3ed66":"markdown","9d11512e":"markdown","14e41730":"markdown","c7b9d98e":"markdown","e680c215":"markdown","64b60fb5":"markdown","ef703ff3":"markdown","72d42cc9":"markdown","ec3d48b8":"markdown","f5f585e0":"markdown","572ae5ca":"markdown","8babd487":"markdown","b826f4a5":"markdown","dd907b93":"markdown","d9199a7d":"markdown","d75675df":"markdown","8fb6b231":"markdown","30b32266":"markdown","a8c4c5bf":"markdown","25af85dc":"markdown","7429917d":"markdown"},"source":{"149ddb0d":"#Easy data manipulation\nimport pandas as pd\nimport numpy as np\n\n#Plotting\nimport seaborn as sns\nsns.set(style='white', context='notebook', palette='deep')\nimport matplotlib.pyplot as plt\n\n#Who likes warnings anyway?\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Pre-processing, tuning of parameters and scoring tools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score, roc_auc_score, roc_curve\n\n#Basic text mining tools\nfrom sklearn.feature_extraction import stop_words\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction import text #Allow stop_words customization\n\n#Machine Learning models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\n\n#Used for distribution fitting, and representation\nfrom scipy import stats\n\n#Time measuring for model training\nfrom time import time","2e519ca6":"df = pd.read_csv('..\/input\/winemag-data-130k-v2.csv')","0a01e0ed":"print(\"Number of entries (rows):\", df.shape[0],\\\n      \"\\nNumber of features (columns):\", df.shape[1])\n\ndf.head(3)","c070e75c":"#Keep only the useful columns and rename them for ease of use\ndf = df[['description', 'points']]\ndf.rename(columns={'description':'Description',\n                   'points':'Score'},\n                   inplace=True)\n \n#Add a column with the length of the description in characters, we use it to check for empty descriptions\ndf[\"Description_Length\"] = [len(desc) for desc in df['Description']]\n\n#Check for missing values\nprint(\"Number of missing values for the Score feature: \", len(df[df['Score'].isnull()]))\nprint(\"Number of missing descriptions: \", len(df[df['Description_Length']==0]))","b3f789b4":"df.describe()","e044328e":"#Make a subplot grid\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (0.25, 0.75)})\n \n#Associate a plot to each of the subplot grid\nsns.boxplot(df[\"Score\"], ax=ax_box).set_title(\"Basic representation of the Score feature\\n\")\nsns.distplot(df[\"Score\"], ax=ax_hist, kde=False, fit=stats.gamma, bins=20) \n#We can fit a gamma distribution, just for the sake of representation.\n \n#Set axes legends\nax_box.set(xlabel='') #Remove x axis name for the boxplot\nax_hist.set(ylabel='Density')\n\nplt.show()\n","8cdfc1d4":"Q3 = np.quantile(df['Score'], 0.75) #Third quartile\nQ1 = np.quantile(df['Score'], 0.25) #First quartile\nIQR = Q3 - Q1 #Inter Quartile Range\n\noutlier_score_threshold =  Q3 + 1.5 * IQR\noutlier_number=len(df[ df['Score'] > outlier_score_threshold ])\n\nprint(\"Number of outliers:\", outlier_number,\n      \"\\nOutlier proportion:\", round(outlier_number\/len(df['Score'])*100, 3),\"%\",\n      \"\\nOutlier threshold score:\", outlier_score_threshold,\"\/ 100\")","e3c6fa62":"#Make a subplot grid\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (0.25, 0.75)})\n \n#Associate a plot to each of the subplot grid\nsns.boxplot(df[\"Description_Length\"], ax=ax_box).set_title(\"Basic description of the Description_Length\")\nsns.distplot(df[\"Description_Length\"], ax=ax_hist, kde=False, fit=stats.gamma, bins=50) \n#We can fit a gamma distribution, the fit is rather impressive here.\n\n#Parameters of the gamma distribution\nalpha, loc, beta = stats.gamma.fit(df['Description_Length'])\nplt.legend(['Gamma distribution \\nShape = {0:.2f} \\nLoc = {1:.2f}  \\nScale = {2:.2f}'.format(alpha, loc, beta)],loc='best')\n\n#Set axes legends\nax_box.set(xlabel='') #Remove x axis name for the boxplot\nax_hist.set(ylabel='Frequency')\n\nplt.show()\n\n#How good is the fit with the gamma distribution ?\nfig = plt.figure()\nres = stats.probplot(df['Description_Length'], dist=stats.gamma(a= alpha, loc=loc, scale=beta), plot=plt)\nplt.show()\n\n#Other basic statistics\nprint(\"Skewness: %f\" % df['Description_Length'].skew())\nprint(\"Kurtosis: %f\" % df['Description_Length'].kurt())","991cf666":"Q3 = np.quantile(df['Description_Length'], 0.75) #Third quartile\nQ1 = np.quantile(df['Description_Length'], 0.25) #First quartile\nIQR = Q3 - Q1 #Inter Quartile Range\n\noutlier_score_threshold_high = Q3 + 1.5 * IQR\noutlier_score_threshold_low = Q1 - 1.5 * IQR\n\noutlier_number_total=len(df[np.logical_or(df['Description_Length'] > outlier_score_threshold_high,\n                         df['Description_Length'] < outlier_score_threshold_low)])\n\noutlier_number_low = len(df[df['Description_Length'] < outlier_score_threshold_low])\noutlier_number_high = outlier_number_total - outlier_number_low\n\nprint(\"Number of outliers (high - low):\", outlier_number_total, \"(\",outlier_number_high,\"-\",outlier_number_low,\")\",\n      \"\\nOutlier proportion:\", round(outlier_number_total\/len(df['Description_Length'])*100, 3),\"%\",\n      \"\\nOutlier threshold lengths (high - low):\", outlier_score_threshold_high,\"-\",outlier_score_threshold_low)","90133f61":"#These are the entries with a very short description\nsub_df = df[df['Description_Length']<outlier_number_low]\n\n#Make a subplot grid\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (0.25, 0.75)})\n \n#Associate a plot to each of the subplot grid\nsns.boxplot(sub_df['Score'], ax=ax_box).set_title(\"Scores of wines given a description shorter than 70 words\\n\")\nsns.distplot(sub_df['Score'], ax=ax_hist, kde=False, fit=stats.gamma, bins=15) \n \n#Set axes legends\nax_box.set(xlabel='') #Remove x axis name for the boxplot\nax_hist.set(ylabel='Frequency')\n\nplt.show()\n\nmean_score = np.mean(sub_df['Score'])\nprint(\"Mean score given after a description shorter than 70 words:\", round(mean_score,2), \"\/ 100\")\n\nproportion_below_median = len(sub_df[sub_df['Score']<88]['Score'])\/len(sub_df['Score'])\nprint(\"Proportion of wines with short description with a score below the set's median:\", round(proportion_below_median*100,2),\"%\")\n","309f2f3e":"sns.jointplot(x=\"Score\", y=\"Description_Length\", data=df)\nplt.show()\n\ncorr= np.corrcoef(df[\"Score\"], df[\"Description_Length\"])[0,1]\nprint(\"Correlation between Score and Description_Length:\",round(corr,2))","9a020ec5":"corpus = df[\"Description\"].values\nY = df[\"Score\"].values","d8236074":"#Customize stop words after having a first look at the most frequent words\ncustomStopWords = text.ENGLISH_STOP_WORDS.union(['wine', '2009', '2010','2011', '2012', '2013', '2014', '2015','2016', '2017', '2018',\n                                                 '2019', '2020', '2021', '2022','2023', '2024', '2025', '2030', '100', '10', '12',\n                                                 '14', '15', '20', '25', '30','40', '50', '60', '70', '90'])\n#The words we add to the english stop words are mostly references to dates and prices, hence numbers.\n\n#Use the CountVectorizer: we consider 1000 features, either individual words or pairs\nCV = CountVectorizer(stop_words=customStopWords, max_features=1000, ngram_range=(1,2))\nX = CV.fit_transform(corpus) #Let's be careful here, X is a sparse Matrix\n\nprint(\"Number of entries (rows):\", X.shape[0],\\\n      \"\\nNumber of features (columns):\", X.shape[1])","22c6e72f":"X_array = X.toarray() #Convert X from a sparse matrix to a usual matrix\n\ninverted_dict = dict([[v,k] for k,v in CV.vocabulary_.items()]) # {X_array column index: \"word\"}\nfinal_dict = {} # {\"word\": total number of instances }\n\nfor x in range(len(X_array[0,:])): #Fill the final dict\n    final_dict[inverted_dict[x]]=np.sum(X_array[:,x]) \n\nprint(\"20 most frequent words:\",sorted(final_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse=True)[0:20]) #Display of the final dict","1b0ed3ec":"TFIDF = TfidfTransformer()\nX_TFIDF = TFIDF.fit_transform(X)","d7323d42":"Y_rep = [0] #Will stock the cumulative explained variance ratio of the SVD, for each n_components\n\n#Actual SVD\nSVD = TruncatedSVD(n_components = 999) \nX_SVD = SVD.fit_transform(X_TFIDF)\nvar = SVD.explained_variance_ratio_\n\n#This will help us decide on the final number of components we want to keep \nfor x in range(999): \n    Y_rep.append(Y_rep[-1]+var[x])\n\nplt.plot(Y_rep)\nplt.plot(sorted(var*78, reverse=True))\n\nplt.title(\"Explained variance ratio, and scaled marginal explained variance gain\\nw.r.t the number of components kept for SVD\\n\")\nplt.legend(['Explained variance ratio', 'Marginal gain of explained variance'], loc='best')\nplt.xlabel('Number of components')\nplt.ylabel('')\n\nplt.show() ","634529a9":"#Balanced discretization in 2 classes\nmedian = np.median(Y) #88.0\nY[Y < median] = 0\nY[Y >= median] = 1\n\n#Re-type Y as int\nY=Y.astype(int)\n\n#Concatenation of X_TFIDF and Description_Length to obtain our final X matrix\nX = np.append(X_TFIDF.toarray(), df[\"Description_Length\"].values[:, None], axis=1)\n\nprint(\"Number of entries:\", X.shape[0],\\\n      \"\\nNumber of features:\", X.shape[1])","fad4b27d":"#Test\/Train split\nX_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.3, shuffle=True)","0198bb3d":"#Normalization\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)    #We don't cheat here, we fit our scaler only on the train data\nX_test = scaler.transform(X_test)          #X_test is ONLY transformed, based on X_train fit","c0b4ba89":"def FitAndScore(classifier, X_train, X_test, y_train, y_test):\n    print(\"Fitting...\")\n    start = time()\n    classifier.fit(X_train, y_train)\n    end = time()\n    print(\"( {0:.2f} seconds )\".format(end-start))\n    \n    print(\"\\nPredicting...\")\n    start = time()\n    y_predicted = classifier.predict(X_test)\n    end = time()\n    print(\"( {0:.2f} seconds )\".format(end-start))\n    \n    print(\"\\nReporting...\\n\")\n    print(classification_report(y_test, y_predicted),\"\\n\")\n    print(\"Confusion matrix:\\n\")\n    print(confusion_matrix(y_test, y_predicted),\"\\n\")\n    print(\"Cohen's Kappa score : \",cohen_kappa_score(y_test, y_predicted),\"\\n\")\n    \n    #If we formulate the problem with binary classes, we can take a look at the ROC curve and associated score\n    if len(np.unique(y_test)) == 2:\n        print(\"AUC score : {0:.3f}\".format(roc_auc_score(y_test, y_predicted)))\n        fpr, tpr, thresholds = roc_curve(y_test, y_predicted)\n        plt.plot([0, 1], [0, 1], linestyle='--')\n        plt.plot(fpr, tpr, marker='.')\n        plt.title(\"ROC Curve\")\n        plt.show()","518cdce8":"dt = DecisionTreeClassifier(criterion='gini', max_depth=10)\nFitAndScore(dt, X_train, X_test, y_train, y_test)","99059526":"#We have negative values, so let's scale everyting between 0 and 1 to use naive bayes classifier\nmmscaler = MinMaxScaler()\nX_train_01 = mmscaler.fit_transform(X_train)    #Still no cheating with this scaling\nX_test_01 = mmscaler.transform(X_test)          \n\nnb = MultinomialNB()\nFitAndScore(nb, X_train_01, X_test_01, y_train, y_test)","e279e52a":"LSVC = LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=False,\n                 intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n                 multi_class='ovr', penalty='l2', tol=1e-05, verbose=2)\n\nFitAndScore(LSVC, X_train, X_test, y_train, y_test)","20db3c0d":"rf = RandomForestClassifier(n_estimators = 100, bootstrap=True, n_jobs=-1)\nFitAndScore(rf, X_train, X_test, y_train, y_test)","1efe25cb":"features = CV.vocabulary_\nfeatures['Description_Length']=1000\n\nimportances = rf.feature_importances_\n\nfeatures_inv = dict([[v,k] for k,v in features.items()]) #Structure {X index:'feature name'}\nfinal_features_dict = {} #Structure {'feature name':relative importance}\nfor i in range(len(importances)):\n    final_features_dict[features_inv[i]] = importances[i]\n\nsorted_feature_importance = sorted(final_features_dict.items(), key = lambda kv:[kv[1], kv[0]], reverse=True)\nsorted_feature_importance[0:20]","f2f6a2f1":"clf = RandomForestClassifier(n_estimators=100, n_jobs=-1);\nbclf = AdaBoostClassifier(base_estimator=clf, n_estimators=10)\nFitAndScore(bclf, X_train, X_test, y_train, y_test)","e373e06e":"mlp = MLPClassifier(activation='logistic', alpha=1e-03, batch_size='auto',\\\n                    beta_1=0.9, beta_2=0.999, early_stopping=False,\\\n                    epsilon=1e-08, hidden_layer_sizes=(600,400),\\\n                    learning_rate='constant', learning_rate_init=0.0001,\\\n                    max_iter=200, momentum=0.9, n_iter_no_change=10,\\\n                    nesterovs_momentum=True, power_t=0.5,\\\n                    shuffle=True, solver='adam', tol=0.00001,\\\n                    validation_fraction=0.1, verbose=True, warm_start=False)\n\nFitAndScore(mlp, X_train, X_test, y_train, y_test)","8e94adf4":"## 5. Machine Learning","0964c4db":"The results given by SVM with simple linear kernel are even better. Although, it took twice as long as our decision tree classifier, and 100 times longer than the Naive Bayes approach. So depending on the application, the model choice is not necessarily obvious here.\n\nLet's try to see if we can get even better result with a Random Forest Classifier with 100 estimators.","74f359ce":"In a more visual way, we can see most of these descriptive statistics on the following plots.","2b2140c9":"As usual, in order to make sure our models evaluation doesn't reward overfitting, we divide our explanatory features and class label in a train set (70% of entries) and a test set (30% of entries).\n\nWe will train models on train sets, and then evaluate them on test sets.","a1a24dc6":"Our neural network gives slightly better results than other models. But its training took A LOT of time. It might or might not be worth it depending on the application.\n\nOnce again, it's highly probable that with better hyperparameters, we could get even better results.","d9c48eb1":"Surprisingly enough, the Adaboosted Random Forest shows worse results than the simple random forest. Although, it would probably get better with more estimators, but the training would also be much longer, so here again, it all depends on the application.\n\nOur last machine learning model will be a simple neural network, a Multi-Layer Perceptron Classifier. Neural networks, when tuned correctly, have the potential to perform better than other models. The problem is that finding the right hyperparameters would require an entire dedicated kernel (or several), and the training time can take some time. So here, I'll just use the MLP classifier with mostly default parameters.","0904d5ef":"MultinomialNB was even faster than the simple decision tree classifier, and gives slightly better results. Here again, the model has more trouble predicting low scores (class 0) than high scores (class 1).\n\nLet's try if we can get even better results by using SVM with simple linear kernel. If our entries are linearly separable, this should find a relevant hyperplane.","cb140074":"To make the problem more convenient to work with, we will make it a classification problem. Let's define 2 balanced classes. The first one (0) will be associated to wines scoring below the median score, and the second (1) above the median score.","e5180b70":"Our setup is completed, we can now try some models. For more convenience, we begin by defining a simple function for training and scoring models in order to help us choose the best.","03d7d184":"## 0. Imports","ab7c67af":"At first, each additionnal component adds a lot to the explained variance ratio. With each additional component, the marginal gain of explained variance decreases steeply until around 50 components. After that, the marginal gain decreases a lot slower.\n\nWe will try to keep our 1000 components to learn from. If some models take too long, we might come back to dimension reduction and  Truncated SVD. But with no specific application in mind, it doesn't seem necessary to loose information to lower the training duration of some models.","23f8c30e":"We will now use TF-IDF. It it a classical method used to determine the importance of each word in the corpus.\n\nQuoting Wikipedia: \"The tf\u2013idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. Tf\u2013idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf\u2013idf.\"\n\n*Note : instead of using CountVectorizer, and then TF-IDF, it would have been equivalent to use directly TfidfVectorizer.*","1330af73":"For our study, we will only consider the following features:\n* \"description\": contains a short description of a specific wine, its flavor and its aspect redacted by a user. This is the only explanatory variable we will use.\n* \"points\": contains the score between 80\/100 and 100\/100 given to a wine by a user. This is the variable we want to predict based on the description.\n\nLet's see if there are any missing values for these two features in the data set, and simply drop the line if there are. ","8d82f705":"But before training, we have to normalize our data. Indeed, while TFIDF already kind of does that already for our 1000 first features, we need to normalize the Description_Length feature for better results with certain algorithms (like kNN for instance)","b5d3ed66":"This confirms that short descriptions tends to be about wines with lower scores. Indeed, more than 90% of the wines which were given a description with fewer than 70 words are scoring below the data set's average and median.\n\nThis also means that the Description_Length feature will definetly be useful in order to predict the scores.\n\nNow, let's make a scatter plot of the Desciption_Length with respect to the Score.","9d11512e":"We use sklearn's CountVectorizer with some custom stop_words to build a sparse matrix of 1000 features (words).","14e41730":"The descriptions are between 20 and 829 words long. The IQR is rather small: 50% of the descriptions are between 197 and 282 words long. Here we have a few more outliers than for the Score feature: 1969 entries are made of more than 409 words and 121 are made of fewer than 69. To put it in perspective, outliers represent 1.6% of the data set.\n\nThis could indicate that users are serious about reviewing wine, and want to justify their evaluation extensively. \n\nWe could also hyothesize that there is not a lot to be said about really bad wines, meaning that the 121 descriptions made of fewer than 69 words are only about bad wines. Let's verify that !","c7b9d98e":"We separate the descriptions and the scores. The Description_Length feature will be added later on.","e680c215":"Let's try a first simple model: a decision tree using Gini impurity as a criterion, and a max depth of 10.","64b60fb5":"## 6. Conclusion\n\nIn this kernel, we analyzed the Description and Score of wines from the dataset. We found out that the distribution of the number of words in a description is very close to a gamma distribution, except it has a bigger right tail. We also found that short descriptions are usually given to bad wines.\n\nThen, using elementary text mining techniques, we transformed the wines descriptions into a matrix containing 1001 features. Thanks to machine learning algorithms, we used this matrix to predict the adequate score corresponding to the description.\n\nThe best performing model \"out of the box\" was a simple random forest with 100 estimators. The most important feature it used was the description length. Its training took around 3 minutes. Of course it's possible to get even better results by tuning the hyperparameters used with GridSearchCV, RandomizedSearchCV, or even a fully automatized approach like tpot.\n\nThanks a lot for taking the time to read this kernel !\n","ef703ff3":"Looking at the QQ-plot, we see that Description_Length seems to follow a Gamma distribution. The only deviation from the gamma distribution is the Description_Length of above 400 words. In other words, the actual underlying distribution of Description_Length has an heavier right tail than the gamma distribution we fitted.\n\nNow, as we'll see in the following cell, these Description_Lengths of more than 400 words can mostly be considered as outliers.","72d42cc9":"Now, if we want to, we can take a look at the top 20 most frequent words \/ pair of words, we can. Although this is neither the most elegant, nor fastest way to do it, the following approach works just fine.","ec3d48b8":"50% of the scores are between 86\/100 and 91\/100. There are only a few outliers , 52 (representing 0.04% of entries), which scored over 98.5\/100.\n\nAn interpretation would be that excellent wines are rarer. Another could be that users reviewing wines are very hard to please.","f5f585e0":"### b) Dimension reduction\nAt that point, we have a (129971x1000) matrix describing the information contained in wines' descriptions. Having 1000 features might slow us down a lot when we'll use machine learning models on it.\n\nLet's see the impact of TruncatedSVD, a dimension reduction technique similar to PCA but for sparse matrices, on the loss of explained variance. Is it worth it to give up some information for easier computation ?","572ae5ca":"Perfect ! We have no missing scores or desciptions, so no entry to drop.\n\nWe will now keep the Description_Length feature to see if it is useful to help predict a wine's score. Let's take a look at the basic statistics of our 2 numerical features : Score and Description_Length. ","8babd487":"## 2. Checking for missing values","b826f4a5":"We read the data set.","dd907b93":"# Predict a wine's score based on its description\n*Lo\u00efc Gauthier*\n\nIn this kernel, I'm taking a look at a great data set about wines from Zack Thoutt. It contains about 130k user reviews (description, score, price...) of wines from various origins.\n\nMy goal for this kernel is to predict the score a user will give to a wine, solely based on the description he redacted. To that end, I'll be following the following structure.\n\n        O. Imports\n        1. First Look\n        2. Checking for missing values\n        3. Descriptive statistics, distribution fitting and outliers\n        4. Text Mining\n            a) Defining features\n            b) Dimension reduction\n        5. Machine Learning\n        6. Conclusion\n","d9199a7d":"This simple decision tree has been really fast to train, and gives already good results. It seems that our tree predicts above-median scoring wines with more ease.\n\nAnother rather simple classification algorithm would have been kNN. However, given the number of features, this approach would take too long. Instead, we can use a faster approach: a Naive Bayes classifier for multinomial models.\n\nIt's true that Naive Bayes classification is based on applying Bayes' theorem hypothesizing independence of the features. This condition is probably not fully fulfilled here, but let's try anyway.","d75675df":"## 4. Text mining\n### a) Defining features","8fb6b231":"There is a positive, albeit weak correlation between Score and Description_Length. It doesn't seem wise to try to predict a wine's score solely based on its description length with a linear model though.","30b32266":"The results keep getting better ! Sadly, the time needed for training is also mush higher. Here it is still reasonnable due to the relatively low number of trees generated.\n\nFor this model, let's take a look at the feature importances.","a8c4c5bf":"The most important feature is Description_Length by relatively far.\n\nA popular way to increase the results of a Random Forest Classifier is to use Adaboosting. Quoting sklearn's documentation: \"An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\"\n\nThe training of the model will be rather long as it will need to train multiple random forests. To keep that acceptable, we will only use 10 random forests of 100 trees. Better results might be achievable by increasing those numbers, but the training time would be just too long.","25af85dc":"## 1. First look","7429917d":"## 3. Descriptive statistics, distribution fitting and outliers"}}