{"cell_type":{"30cebc04":"code","f2a8407a":"code","377cbf0e":"code","bf1226b5":"code","5d60a6b4":"code","488cf15e":"code","ce317333":"code","d76547ce":"code","fb958d88":"code","4fcd0e48":"code","60fc7ab8":"code","518065b8":"code","6f02b604":"code","ff6739c1":"code","1c80c451":"code","aefa20f0":"code","b943c2c1":"markdown","85f668be":"markdown","e682b3cf":"markdown","c1806e1f":"markdown","11b555ed":"markdown","cb492953":"markdown","f6ea1cd8":"markdown","1c042d06":"markdown"},"source":{"30cebc04":"from sklearn.preprocessing import LabelEncoder\nimport datetime\nimport gc\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport time\nimport warnings\n\nfrom contextlib import contextmanager\nfrom pandas.core.common import SettingWithCopyWarning\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport pandas as pd\nimport time\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport gc\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n\nimport pandas as pd\nimport numpy as np\ndebug=False\nif debug:\n    df = pd.read_csv('..\/input\/train.csv')[:100]\n    test = pd.read_csv('..\/input\/test.csv')[:100]\n    sub = pd.read_csv('..\/input\/sample_submission.csv')[:100]\nelse:\n    \n    df = pd.read_csv('..\/input\/train.csv')\n    test = pd.read_csv('..\/input\/test.csv')\n    sub = pd.read_csv('..\/input\/sample_submission.csv')\n","f2a8407a":"import pandas as pd\npd.set_option('display.max_columns', 500)\n\ndf.checkin_date = pd.to_datetime(df.checkin_date, format=\"%d\/%m\/%y\")\n\ndf.checkout_date = pd.to_datetime(df.checkout_date, format=\"%d\/%m\/%y\")\n\ndf.booking_date= pd.to_datetime(df.booking_date, format=\"%d\/%m\/%y\")\n\ntest.checkin_date = pd.to_datetime(test.checkin_date, format=\"%d\/%m\/%y\")\n\ntest.checkout_date = pd.to_datetime(test.checkout_date, format=\"%d\/%m\/%y\")\n\ntest.booking_date= pd.to_datetime(test.booking_date, format=\"%d\/%m\/%y\")\n\n\n\n\ndef dt_feat(train,date_col):\n    print(train.shape)\n    train[date_col+'week_day'] = train[date_col].dt.dayofweek\n    train[date_col+'month'] = train[date_col].dt.month\n    train[date_col+'is_month_end'] = train[date_col].dt.is_month_end\n    train[date_col+'dayofyear'] = train[date_col].dt.dayofyear\n    train[date_col+'day'] = train[date_col].dt.day\n    train[date_col+'year'] = train[date_col].dt.year\n    train[date_col+'weekyear'] = train[date_col].dt.weekofyear\n    return train\n\n\ntrain1 =dt_feat(df,'checkin_date')\ntrain1=dt_feat(train1,'checkout_date')\ntrain1=dt_feat(train1,'booking_date')\ndf = train1.copy()\ntrain1 =dt_feat(test,'checkin_date')\ntrain1=dt_feat(train1,'checkout_date')\ntrain1=dt_feat(train1,'booking_date')\ntest = train1.copy()\ndf.columns\n\n\n\ncat_cols = [f for f in df.columns if (df[f].dtype == 'object' and f not in ['reservation_id'])]\ntrain = df.copy()\nfor col in cat_cols:\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))\n\ntrain['in_book']=train.checkin_date-train.booking_date\ntest['in_book']=test.checkin_date-test.booking_date\ntrain['in_out'] = train.checkout_date-train.checkin_date\ntest['in_out'] = test.checkout_date-test.checkin_date\ndef day_get(x):\n    return x.days\ntrain.in_out = train.in_out.apply(day_get)\ntest.in_out = test.in_out.apply(day_get)\ntrain.in_book = train.in_book.apply(day_get)\ntest.in_book = test.in_book.apply(day_get)\n\ntrain_bkp=train.copy()\ntest_bkp=test.copy()\n\nmerge = pd.concat([train_bkp,test_bkp])\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','memberid']).reservation_id.count()).reset_index(),suffixes=('','res_mem'),on=['resort_id','memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id']).reservation_id.count()).reset_index(),suffixes=('','_x_res'),on='resort_id',how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','checkout_dateyear','checkout_datemonth']).reservation_id.count()).reset_index(),suffixes=('','_res_month'),on=['resort_id','checkout_dateyear','checkout_datemonth'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid']).reservation_id.count()).reset_index(),suffixes=('','mem'),on=['memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid','checkout_dateyear']).reservation_id.count()).reset_index(),suffixes=('','mem_year'),on=['memberid','checkout_dateyear'],how='left')\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id']).roomnights.median()).reset_index(),suffixes=('','res_amount_median'),on=['resort_id'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id']).roomnights.max()).reset_index(),suffixes=('','res_amount_max'),on=['resort_id'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id']).roomnights.min()).reset_index(),suffixes=('','res_amount_min'),on=['resort_id'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id']).roomnights.std()).reset_index(),suffixes=('','res_amount_std'),on=['resort_id'],how='left')\n\n#0.9651884690956651\n\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','memberid']).roomnights.median()).reset_index(),suffixes=('','res_mem_amount_median'),on=['resort_id','memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','memberid']).roomnights.max()).reset_index(),suffixes=('','res_mem_amount_max'),on=['resort_id','memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','memberid']).roomnights.min()).reset_index(),suffixes=('','res_mem_amount_min'),on=['resort_id','memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','memberid']).roomnights.std()).reset_index(),suffixes=('','res_mem_amount_std'),on=['resort_id','memberid'],how='left')\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid']).roomnights.median()).reset_index(),suffixes=('','mem_amount_median'),on=['memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid']).roomnights.max()).reset_index(),suffixes=('','mem_amount_max'),on=['memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid']).roomnights.min()).reset_index(),suffixes=('','mem_amount_min'),on=['memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid']).roomnights.std()).reset_index(),suffixes=('','mem_amount_std'),on=['memberid'],how='left')\n\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','room_type_booked_code']).reservation_id.count()).reset_index(),suffixes=('','_res_type'),on=['resort_id','room_type_booked_code'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','checkout_datemonth','room_type_booked_code']).reservation_id.count()).reset_index(),suffixes=('','_res_code_month_x'),on=['resort_id','checkout_datemonth','room_type_booked_code'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','main_product_code']).reservation_id.count()).reset_index(),suffixes=('','_res_main_code'),on=['resort_id','main_product_code'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['channel_code','checkout_dateyear','checkout_datemonth']).reservation_id.count()).reset_index(),suffixes=('','channel_check_yearmonth'),on=['channel_code','checkout_dateyear','checkout_datemonth'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['checkout_dateyear','checkout_datemonth']).reservation_id.count()).reset_index(),suffixes=('','check_Date_month'),on=['checkout_dateyear','checkout_datemonth'],how='left')\n\n#0.9651884690956651\n\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','memberid']).in_out.median()).reset_index(),suffixes=('','res_meminoutt_median'),on=['resort_id','memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','memberid']).in_out.max()).reset_index(),suffixes=('','res_mem_inout_max'),on=['resort_id','memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','memberid']).in_out.min()).reset_index(),suffixes=('','res_mem_inout_min'),on=['resort_id','memberid'],how='left')\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','memberid']).in_out.std()).reset_index(),suffixes=('','res_meminoutt_std'),on=['resort_id','memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid']).in_out.median()).reset_index(),suffixes=('','mem_inout_median'),on=['memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid']).in_out.max()).reset_index(),suffixes=('','mem_inout_max'),on=['memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid']).in_out.min()).reset_index(),suffixes=('','mem_inout_min'),on=['memberid'],how='left')\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid']).in_out.std()).reset_index(),suffixes=('','mem_inout_std'),on=['memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid']).total_pax.median()).reset_index(),suffixes=('','mem_inout_median_pax'),on=['memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid']).total_pax.max()).reset_index(),suffixes=('','mem_inout_max_pax'),on=['memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid']).total_pax.min()).reset_index(),suffixes=('','mem_inout_min_pax'),on=['memberid'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid']).total_pax.std()).reset_index(),suffixes=('','mem_inout_std_pax'),on=['memberid'],how='left')\n\n\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id']).total_pax.median()).reset_index(),suffixes=('','res_inout_median_pax'),on=['resort_id'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id']).total_pax.max()).reset_index(),suffixes=('','res_inout_max_pax'),on=['resort_id'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id']).total_pax.min()).reset_index(),suffixes=('','res_inout_min_pax'),on=['resort_id'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id']).total_pax.std()).reset_index(),suffixes=('','res_inout_std_pax'),on=['resort_id'],how='left')\n\n\n\n# merge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','memberid']).total_pax.median()).reset_index(),suffixes=('','mem_res_inout_median_pax'),on=['resort_id','memberid'],how='left')\n# merge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','memberid']).total_pax.max()).reset_index(),suffixes=('','mem_res_inout_max_pax'),on=['resort_id','memberid'],how='left')\n# merge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','memberid']).total_pax.min()).reset_index(),suffixes=('','mem_res_inout_min_pax'),on=['resort_id','memberid'],how='left')\n# merge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','memberid']).total_pax.std()).reset_index(),suffixes=('','mem_res_inout_std_pax'),on=['resort_id','memberid'],how='left')\n\n\n\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','main_product_code']).reservation_id.count()).reset_index(),suffixes=('','res_memmain_product_code'),on=['resort_id','main_product_code'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['main_product_code']).reservation_id.count()).reset_index(),suffixes=('','_x_resmain_product_code'),on='main_product_code',how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid','resort_id','main_product_code']).reservation_id.count()).reset_index(),suffixes=('','memjknknk'),on=['memberid','resort_id','main_product_code'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid','main_product_code']).reservation_id.count()).reset_index(),suffixes=('','mem_yearmain_product_code'),on=['memberid','main_product_code'],how='left')\n\n\n\n#merge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','checkout_dateyear','checkin_dateweekyear']).total_pax.count()).reset_index(),suffixes=('','res_inout_std_pax','week_year'),on=['resort_id','checkout_dateyear','checkin_dateweekyear'],how='left')\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','checkout_dateyear','checkin_dateweekyear']).reservation_id.count()).reset_index(),suffixes=('','_res_month_week_year'),on=['resort_id','checkout_dateyear','checkin_dateweekyear'],how='left')\n\n\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['memberid','checkout_dateyear','checkin_dateweekyear']).reservation_id.count()).reset_index(),suffixes=('','_res_month_week_year_member'),on=['memberid','checkout_dateyear','checkin_dateweekyear'],how='left')\n\n\n\nimport datetime\n\nmerge['booking_dayes'] = (datetime.datetime.today()-merge['booking_date']).dt.days\nmerge['checkin_dayes'] = (datetime.datetime.today()-merge['checkin_date']).dt.days\nmerge['checkout_dayes'] = (datetime.datetime.today()-merge['checkout_date']).dt.days\n\n\n\nmerge['rooms_booked_per_night'] = merge.in_out\/merge.roomnights\nmerge['rooms_bookinf_per_night'] = merge.in_book\/merge.roomnights\n\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['state_code_residence']).total_pax.median()).reset_index(),suffixes=('','mem_inout_median_pax_state_code'),on=['state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['state_code_residence']).total_pax.max()).reset_index(),suffixes=('','mem_inout_max_pax_state_code'),on=['state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['state_code_residence']).total_pax.min()).reset_index(),suffixes=('','mem_inout_min_pax_state_code'),on=['state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['state_code_residence']).total_pax.std()).reset_index(),suffixes=('','mem_inout_std_pax_state_code'),on=['state_code_residence'],how='left')\n\n\n\n\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','state_code_residence']).roomnights.median()).reset_index(),suffixes=('','res_mem_amount_medianstate_code_resid'),on=['resort_id','state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','state_code_residence']).roomnights.max()).reset_index(),suffixes=('','res_mem_amount_maxstate_code_resid'),on=['resort_id','state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','state_code_residence']).roomnights.min()).reset_index(),suffixes=('','res_mem_amount_minstate_code_resid'),on=['resort_id','state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','state_code_residence']).roomnights.std()).reset_index(),suffixes=('','res_mem_amount_stdstate_code_resid'),on=['resort_id','state_code_residence'],how='left')\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['state_code_residence']).roomnights.median()).reset_index(),suffixes=('','mem_amount_medianstate_code_resid'),on=['state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['state_code_residence']).roomnights.max()).reset_index(),suffixes=('','mem_amount_maxstate_code_resid'),on=['state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['state_code_residence']).roomnights.min()).reset_index(),suffixes=('','mem_amount_minstate_code_resid'),on=['state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['state_code_residence']).roomnights.std()).reset_index(),suffixes=('','mem_amount_stdstate_code_resid'),on=['state_code_residence'],how='left')\n\n\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','state_code_residence']).in_out.median()).reset_index(),suffixes=('','res_meminoutt_medianstate_code_resid'),on=['resort_id','state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','state_code_residence']).in_out.max()).reset_index(),suffixes=('','res_mem_inout_maxstate_code_resid'),on=['resort_id','state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','state_code_residence']).in_out.min()).reset_index(),suffixes=('','res_mem_inout_minstate_code_resid'),on=['resort_id','state_code_residence'],how='left')\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','state_code_residence']).in_out.std()).reset_index(),suffixes=('','res_meminoutt_stdstate_code_resid'),on=['resort_id','state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['state_code_residence']).in_out.median()).reset_index(),suffixes=('','mem_inout_medianstate_code_resid'),on=['state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['state_code_residence']).in_out.max()).reset_index(),suffixes=('','mem_inout_maxstate_code_resid'),on=['state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['state_code_residence']).in_out.min()).reset_index(),suffixes=('','mem_inout_minstate_code_resid'),on=['state_code_residence'],how='left')\n\n\npredictors=[]\n\ndef do_prev_Click( df,agg_suffix='prevClick', agg_type='float32'):\n\n    print(f\">> \\nExtracting {agg_suffix} time calculation features...\\n\")\n    \n    GROUP_BY_NEXT_CLICKS = [\n    \n     {'groupby': ['resort_id', 'memberid']},\n     {'groupby': ['resort_id']},\n        \n    ]\n\n    # Calculate the time to next click for each group\n    for spec in GROUP_BY_NEXT_CLICKS:\n    \n       # Name of new feature\n        new_feature = '{}_{}'.format('_'.join(spec['groupby']),agg_suffix)    \n    \n        # Unique list of features to select\n        all_features = spec['groupby'] + ['booking_date']\n\n        # Run calculation\n        print(f\">> Grouping by {spec['groupby']}, and saving time to {agg_suffix} in: {new_feature}\")\n        df[new_feature] = (df.booking_date - df[all_features].groupby(spec[\n                'groupby']).booking_date.shift(+1) ).dt.days.astype(agg_type)\n        \n        predictors.append(new_feature)\n        gc.collect()\n    return (df)    \n\n\ndef do_next_Click( df,agg_suffix='nextClick', agg_type='float32'):\n    \n    print(f\">> \\nExtracting {agg_suffix} time calculation features...\\n\")\n    \n    GROUP_BY_NEXT_CLICKS = [\n    \n    # V3\n   \n     {'groupby': ['resort_id', 'memberid']},\n     {'groupby': ['resort_id']},\n    # {'groupby': ['memberid']}\n        ]\n\n    # Calculate the time to next click for each group\n    for spec in GROUP_BY_NEXT_CLICKS:\n    \n       # Name of new feature\n        new_feature = '{}_{}'.format('_'.join(spec['groupby']),agg_suffix)    \n    \n        # Unique list of features to select\n        all_features = spec['groupby'] + ['booking_date']\n\n        # Run calculation\n        print(f\">> Grouping by {spec['groupby']}, and saving time to {agg_suffix} in: {new_feature}\")\n        df[new_feature] = (df[all_features].groupby(spec[\n            'groupby']).booking_date.shift(-1) - df.booking_date).dt.days.astype(agg_type)\n        \n        #predictors.webpage_idend(new_feature)\n        gc.collect()\n    return (df)\n\n              \n\nmerge = do_prev_Click(merge)\nmerge = do_next_Click(merge)\n\ncategor_col =['member_age_buckets','memberid','resort_id',\n 'cluster_code',\n 'reservationstatusid_code',\n 'resort_id',\n 'channel_code',\n 'main_product_code',\n 'persontravellingid',\n 'resort_region_code',\n 'resort_type_code',\n 'room_type_booked_code',\n 'season_holidayed_code',\n 'state_code_residence',\n 'state_code_resort',\n 'member_age_buckets',\n 'booking_type_code']\n\nfor i in categor_col:\n    print(i,train[i].nunique())\n\nfor i in categor_col:\n    merge[i] = merge[i].astype('category')\n\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['state_code_residence']).reservation_id.count()).reset_index(),suffixes=('','_x_res_mem_state_score'),on=['state_code_residence'],how='left')\nmerge = merge.merge(pd.DataFrame(merge.groupby(by=['resort_id','state_code_residence']).reservation_id.count()).reset_index(),suffixes=('','_x_res_mem_state_scoreresort_id'),on=['resort_id','state_code_residence'],how='left')\n\n\ntrain = merge.loc[merge.amount_spent_per_room_night_scaled.notnull()]\ntest = merge.loc[merge.amount_spent_per_room_night_scaled.isnull()]\n\nX= train.drop(['reservation_id','booking_date','checkin_date','checkout_date','amount_spent_per_room_night_scaled','memberid'],axis=1)\n\ny=train.amount_spent_per_room_night_scaled","377cbf0e":"\n\ndef run_single_lgb(X,y):\n    print(\"Making predictions on Single LGB model\")\n    clf=LGBMRegressor(n_estimators=717,learning_rate=.05,subsample=0.9,reg_alpha=25,min_child_weight=49)\n    clf.fit(X,y)\n    pred_test_lgb_single_model = clf.predict(test[X.columns])\n    return pred_test_lgb_single_model\n\n","bf1226b5":"MAX_ROUNDS = 1231\nOPTIMIZE_ROUNDS = True\nLEARNING_RATE = 0.05\nmodel = CatBoostRegressor(verbose=100,\n    learning_rate=LEARNING_RATE, \n    l2_leaf_reg = 8, \n    iterations = MAX_ROUNDS,\n#    verbose = True,\n    loss_function='RMSE',\n)\ndef run_single_catboost(X,y):\n    fit_model = model.fit(X,y)\n    pred_catboost_model=model.predict(test[X.columns])\n    return pred_catboost_model","5d60a6b4":"def folded_catboost(X,y):\n    try:\n        X.drop('target',axis=1,inplace=True)\n    except:\n        print(\"Not there\")\n\n    K = 5\n    x_test = test[X.columns]\n    kf = KFold(n_splits = K, random_state = 1, shuffle = True)\n    y_valid_pred = 0*y\n    y_test_pred = 0\n    for i, (train_index, test_index) in enumerate(kf.split(X)):\n        # Create data for this fold\n        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n        X_train, X_valid = X.iloc[train_index,:], X.iloc[test_index,:]\n        print( \"\\nFold \", i)\n\n        # Run model for this fold\n        if OPTIMIZE_ROUNDS:\n            fit_model = model.fit(X_train, y_train, \n                                   eval_set=[(X_valid, y_valid)],\n                                   use_best_model=True\n                                 )\n            print( \"  N trees = \", model.tree_count_ )\n        else:\n            fit_model = model.fit( X_train, y_train )\n\n        # Generate validation predictions for this fold\n        pred = fit_model.predict(X_valid)\n        print( \"  Gini = \", np.sqrt(mean_squared_error(y_valid, pred) ))\n        y_valid_pred.iloc[test_index] = pred\n\n        # Accumulate test set predictions\n        y_test_pred += fit_model.predict(x_test)\n\n    y_test_pred \/= K  # Average test set predictions\n    return y_test_pred","488cf15e":"FEATS_EXCLUDED=['target']\ncategor_col =cat_cols+[\n       'channel_code',\n            'main_product_code', \n      'persontravellingid', 'resort_region_code',\n       'resort_type_code', 'room_type_booked_code', \n       'season_holidayed_code', 'state_code_residence', 'state_code_resort',\n       'member_age_buckets', 'booking_type_code', 'memberid',\n       ]\n\ncategor_col.remove('memberid')\ncategor_col.remove('memberid')\n\n\ndef kfold_lightgbm(train_df, test_df, num_folds, y,stratified = False, debug= False):\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n    train_df['target']=y\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=326)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=326)\n\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED]\n\n    # k-fold\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['target'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['target'].iloc[valid_idx]\n\n        # set data structure\n        lgb_train = lgb.Dataset(train_x,\n                                label=train_y,\n                                free_raw_data=False)\n        lgb_test = lgb.Dataset(valid_x,\n                               label=valid_y,\n                               free_raw_data=False)\n        params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", 'n_estimators':3000, 'early_stopping_rounds':200,\n              \"num_leaves\" : 31, \"learning_rate\" : 0.05, \"bagging_fraction\" : 0.9,\n               \"bagging_seed\" : 0, \"num_threads\" : 4,\"colsample_bytree\" : 0.8,\"lambda_l1\": 25,\"min_child_weight\" : 44.97\n          ,\"min_child_samples\": 5,\n             }\n\n\n\n        reg = lgb.train(\n                        params,\n                        lgb_train,\n                        valid_sets=[lgb_train, lgb_test],\n                        valid_names=['train', 'test'],\n                        num_boost_round=10000,\n                        early_stopping_rounds= 200,\n                        verbose_eval=100,\n            categorical_feature=categor_col\n                        )\n\n        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n        sub_preds += reg.predict(test_df[feats], num_iteration=reg.best_iteration) \/ folds.n_splits\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', iteration=reg.best_iteration))\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        print('Fold %2d RMSE : %.6f' % (n_fold + 1, rmse(valid_y, oof_preds[valid_idx])))\n        del reg, train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    # display importances\n    display_importances(feature_importance_df)\n\n    if not debug:\n        # save submission file\n        return sub_preds\n\n\n","ce317333":"def display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances.png')\n\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\nFEATS_EXCLUDED=['target']\n","d76547ce":"pred_test_lgb_single_model=run_single_lgb(X,y)\npred_catboost_model  = run_single_catboost(X,y)\ny_test_fold_cat = folded_catboost(X,y)\npred_lgb_5_fold = kfold_lightgbm(X,test,5,train.amount_spent_per_room_night_scaled)\npred_ensem= pred_test_lgb_single_model*0.3+pred_catboost_model*0.2+pred_lgb_5_fold*0.3+y_test_fold_cat*0.2","fb958d88":"pred_ensem","4fcd0e48":"try:\n    X.drop('target',axis=1,inplace=True)\nexcept:\n    print(\"Not there\")\n","60fc7ab8":"pred_ensem= pred_test_lgb_single_model*0.3+pred_catboost_model*0.2+pred_lgb_5_fold*0.3+y_test_fold_cat*0.2","518065b8":"sub.amount_spent_per_room_night_scaled=pred_ensem\nsub.to_csv('submission_18_rewrite_parallel.csv',index=None)","6f02b604":"# for i in categor_col:\n#     X[i]=X[i].astype(int)\n#     test[i]=test[i].astype(int)    \nfeatures = X.columns\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import skew\nimport xgboost as xgb\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom math import sqrt\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nimport gc\n\nNFOLDS = 3\nSEED = 0\nNROWS = None\ncategorical_feats =categor_col\n\n","ff6739c1":"x_train = X[features]\nx_test = test[features]\nntrain = x_train.shape[0]\nntest = x_test.shape[0]\ny_train=y\n\nkf = KFold(n_splits = NFOLDS, shuffle=True, random_state=SEED)\n\n\n\n\nclass SklearnWrapper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        print(x_train.isnull().sum().sum())\n        self.clf.fit(x_train.fillna(-999).astype(float), y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x.fillna(-999).astype(float))\n\nclass CatboostWrapper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_seed'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n        \nclass LightGBMWrapper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['feature_fraction_seed'] = seed\n        params['bagging_seed'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n\n\nclass XgbWrapper(object):\n    def __init__(self, seed=0, params=None):\n        self.param = params\n        self.param['seed'] = seed\n        self.nrounds = params.pop('nrounds', 250)\n\n    def train(self, x_train, y_train):\n        for i in categor_col:\n            \n            try:\n                x_train[i]=x_train[i].astype(int)\n            except:\n                print(i)\n        dtrain = xgb.DMatrix(x_train.drop(['season_holidayed_code','state_code_residence'],axis=1,inplace=True), label=y_train)\n        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n\n    def predict(self, x):\n        for i in categor_col:\n            try:\n                x[i]=x[i].astype(int)    \n            except:\n                print(i)\n                \n        return self.gbdt.predict(xgb.DMatrix(x.drop(['season_holidayed_code','state_code_residence'],axis=1,inplace=True)))\n\n\ndef get_oof(clf):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train.loc[train_index]\n        y_tr = y_train.loc[train_index]\n        x_te = x_train.loc[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\ndef get_oof_et(clf):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train.loc[train_index]\n        y_tr = y_train.loc[train_index]\n        x_te = x_train.loc[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test.fillna(-999))\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n\n\net_params = {\n    'n_jobs': 16,\n    'n_estimators': 200,\n    'max_features': 0.5,\n    'max_depth': 12,\n    'min_samples_leaf': 2,\n}\n\nrf_params = {\n    'n_jobs': 16,\n    'n_estimators': 200,\n    'max_features': 0.2,\n    'max_depth': 12,\n    'min_samples_leaf': 2,\n}\n\nxgb_params = {\n    'seed': 0,\n    'colsample_bytree': 0.7,\n    'silent': 1,\n    'subsample': 0.7,\n    'learning_rate': 0.075,\n    'objective': 'reg:linear',\n    'max_depth': 4,\n    'num_parallel_tree': 1,\n    'min_child_weight': 1,\n    'nrounds': 400\n}\n\ncatboost_params = {\n    'iterations': 1400,\n    'learning_rate': 0.05,\n    'depth': 8,\n    'l2_leaf_reg': 17,\n\n    'eval_metric': 'RMSE',\n    'od_type': 'Iter',\n    'allow_writing_files': False\n}\n\nlightgbm_params  = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n                    \n              \"num_leaves\" : 31, \"learning_rate\" : 0.05, \"bagging_fraction\" : 0.9,\n               \"bagging_seed\" : 0, \"num_threads\" : 4,\"colsample_bytree\" : 0.8,\"lambda_l1\": 25,\"min_child_weight\" : 44.97\n          ,\"min_child_samples\": 5,'iterations': 500,\n             }\n\n\n\nxg = XgbWrapper(seed=SEED, params=xgb_params)\ncb = CatboostWrapper(clf= CatBoostRegressor, seed = SEED, params=catboost_params)\nlg = LightGBMWrapper(clf = LGBMRegressor, seed = SEED, params = lightgbm_params)\n\n#xg_oof_train, xg_oof_test = get_oof(xg)\nprint(2)\ncb_oof_train, cb_oof_test = get_oof(cb)\nprint(3)\nlg_oof_train, lg_oof_test = get_oof(lg)\nprint(4)\n","1c80c451":"#print(\"XG-CV: {}\".format(sqrt(mean_squared_error(y_train, xg_oof_train))))\n#print(\"ET-CV: {}\".format(sqrt(mean_squared_error(y_train.head(100), et_oof_train))))\n#print(\"RF-CV: {}\".format(sqrt(mean_squared_error(y_train.head(100), rf_oof_train))))\nprint(\"CB-CV: {}\".format(sqrt(mean_squared_error(y_train, cb_oof_train))))\nprint(\"LG-CV: {}\".format(sqrt(mean_squared_error(y_train, lg_oof_train))))\nx_train = np.concatenate(( lg_oof_train, cb_oof_train), axis=1)\nx_test = np.concatenate((lg_oof_test, cb_oof_test), axis=1)\nprint(\"{},{}\".format(x_train.shape, x_test.shape))\nlogistic_regression = LinearRegression()\nlogistic_regression.fit(x_train,y_train)\npred = logistic_regression.predict(x_test)\n\n","aefa20f0":"sub.amount_spent_per_room_night_scaled=pred_ensem*0.8+pred*0.2\nsub.to_csv('xgb_cat_ensemb.csv',index=None)\n","b943c2c1":"# Single Catboost Model","85f668be":"## Folded Catboost model Fold=5","e682b3cf":"## Single LGB Model","c1806e1f":"Reading File\n# 1)Convert the date into datetimes format\n\n","11b555ed":"* ## Ensembling it with previous algos","cb492953":"# Making Ensemble of models","f6ea1cd8":"# Stacking COde","1c042d06":"## 5 fold LGB model"}}