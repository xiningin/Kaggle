{"cell_type":{"ffaaf00a":"code","3d123bc5":"code","156e60e7":"code","9b9a1255":"code","9ce6cffe":"code","237301ce":"code","ff2ac83e":"code","971fa520":"code","3856cc16":"code","a821a4ad":"code","ec80f0c2":"code","7d7e331f":"code","86ad9298":"code","41a10665":"code","31588f43":"code","8516b68e":"code","e0fd22f2":"markdown","75dac18d":"markdown","16dbde64":"markdown","9cca4198":"markdown","417c9563":"markdown","a7bad93f":"markdown","7b4e57b3":"markdown","503eda14":"markdown","c0851e1a":"markdown"},"source":{"ffaaf00a":"import pandas as pd\nimport numpy as np\nimport os\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split","3d123bc5":"tumor_dir_path='..\/input\/brian-tumor-dataset\/Brain Tumor Data Set\/Brain Tumor Data Set\/Brain Tumor'\nnon_tumor_path='..\/input\/brian-tumor-dataset\/Brain Tumor Data Set\/Brain Tumor Data Set\/Healthy'\nfilepaths = []\nlabels= []\ndict_list = [tumor_dir_path, non_tumor_path]\nfor i, j in enumerate(dict_list):\n    flist=os.listdir(j)\n    for f in flist:\n        fpath=os.path.join(j,f)\n        filepaths.append(fpath)\n        if i==0:\n          labels.append('brain_tumor')\n        else:\n          labels.append('good_health') \n    \nfpath_s = pd.Series(filepaths, name=\"filepaths\")\nlbl_s = pd.Series(labels, name=\"lbl\")\ntumor_data = pd.concat([fpath_s,lbl_s], axis=1)\ndf = pd.DataFrame(tumor_data)\nprint(df[\"lbl\"].value_counts())\n","156e60e7":"df.lbl.hist()","9b9a1255":"# Augment data\nbatch_size = 64\ntrain_input_shape = (224, 224, 3)\nn_classes = 2\nclasses = ['brain_tumor', 'good_health']\ntrain_datagen = ImageDataGenerator(validation_split=0.2,\n                                   rescale=1.\/255.,\n                                   rotation_range=15,\n                                   #width_shift_range=0.5,\n                                   #height_shift_range=0.5,\n                                   shear_range=0.1,\n                                   zoom_range=0.5,\n                                   #horizontal_flip=True,\n                                   #vertical_flip=True,\n                                  )\n\ntrain_generator = train_datagen.flow_from_dataframe(dataframe= df,x_col=\"filepaths\",y_col=\"lbl\",\n                                                    class_mode='categorical',\n                                                    target_size=train_input_shape[0:2],\n                                                    batch_size=batch_size,\n                                                    subset=\"training\",\n                                                    shuffle=True,\n                                                    classes=classes\n                                                   )\n\nvalid_generator = train_datagen.flow_from_dataframe(dataframe= df,x_col=\"filepaths\",y_col=\"lbl\",\n                                                    class_mode='categorical',\n                                                    target_size=train_input_shape[0:2],\n                                                    batch_size=batch_size,\n                                                    subset=\"validation\",\n                                                    shuffle=True,\n                                                    classes=classes\n                                                   )\n\nSTEP_SIZE_TRAIN = train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n\/\/valid_generator.batch_size\nprint(\"Total number of batches =\", STEP_SIZE_TRAIN, \"and\", STEP_SIZE_VALID)","9ce6cffe":"def brain_images(image_gen):\n    images, labels=next(image_gen) # get a sample batch from the generator \n    plt.figure(figsize=(20,20))\n    length = len(labels)\n    if length<25:\n        r=length\n    else:\n        r=25\n    for i in range(r):\n        plt.subplot(5,5,i+1)\n        image=(images[i]+1)\/2 #scale images between 0 and 1\n        plt.imshow(image)\n        index=np.argmax(labels[i])\n        class_name=classes[index]\n        plt.title(class_name, color=\"green\",fontsize=16)\n        plt.axis('off')\n    plt.show()","237301ce":"brain_images(train_generator)","ff2ac83e":"base_model = ResNet50(weights='imagenet', include_top=False, input_shape=train_input_shape)\n\nfor layer in base_model.layers:\n    layer.trainable = True\n    \nX = base_model.output\nX = Flatten()(X)\n\nX = Dense(512, kernel_initializer='he_uniform')(X)\nX = Dropout(0.4)(X)\nX = BatchNormalization()(X)\nX = Activation('relu')(X)\n\nX = Dense(128, kernel_initializer='he_uniform')(X)\nX = Dropout(0.4)(X)\nX = BatchNormalization()(X)\nX = Activation('relu')(X)\n\nX = Dense(16, kernel_initializer='he_uniform')(X)\nX = Dropout(0.4)(X)\nX = BatchNormalization()(X)\nX = Activation('relu')(X)\n\noutput = Dense(n_classes, activation='softmax')(X)\n\nmodel = Model(inputs=base_model.input, outputs=output)\n","971fa520":"optimizer = Adam(lr=0.00001)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer, \n              metrics=['accuracy'])","3856cc16":"n_epoch = 30\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, \n                           mode='auto', restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n                              verbose=1, mode='auto')","a821a4ad":"# Train the model - all layers\nhistory1 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n                              epochs=n_epoch,\n                              shuffle=True,\n                              verbose=1,\n                              callbacks=[reduce_lr],\n                              use_multiprocessing=True,\n                              workers=4\n                             )","ec80f0c2":"# Freeze core ResNet layers and train again \nfor layer in model.layers[-6:]:\n   layer.trainable = False\n\nfor layer in model.layers:\n    layer.trainable = True\n\noptimizer = Adam(lr=0.00001)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer, \n              metrics=['accuracy'])\n\nn_epoch = 30\nhistory2 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n                              epochs=n_epoch,\n                              shuffle=True,\n                              verbose=1,\n                              callbacks=[reduce_lr, early_stop],\n                              use_multiprocessing=True,\n                              workers=4                             \n                              )","7d7e331f":"# Merge history1 and history2\nhistory = {}\nhistory['loss'] = history1.history['loss'] + history2.history['loss']\nhistory['acc'] = history1.history['accuracy'] + history2.history['accuracy']\nhistory['val_loss'] = history1.history['val_loss'] + history2.history['val_loss']\nhistory['val_acc'] = history1.history['val_accuracy'] + history2.history['val_accuracy']\nhistory['lr'] = history1.history['lr'] + history2.history['lr']","86ad9298":"# Plot the training graph\nimport matplotlib.pyplot as plt\ndef plot_training(history):\n    acc = history['acc']\n    val_acc = history['val_acc']\n    loss = history['loss']\n    val_loss = history['val_loss']\n    epochs = range(len(acc))\n\n    fig, axes = plt.subplots(1, 2, figsize=(15,5))\n    \n    axes[0].plot(epochs, acc, 'r-', label='Training Accuracy')\n    axes[0].plot(epochs, val_acc, 'b--', label='Validation Accuracy')\n    axes[0].set_title('Training and Validation Accuracy')\n    axes[0].legend(loc='best')\n\n    axes[1].plot(epochs, loss, 'r-', label='Training Loss')\n    axes[1].plot(epochs, val_loss, 'b--', label='Validation Loss')\n    axes[1].set_title('Training and Validation Loss')\n    axes[1].legend(loc='best')\n    \n    plt.show()\n    \nplot_training(history)","41a10665":"# Prediction accuracy on train data\nscore = model.evaluate_generator(train_generator)\nprint(\"Prediction accuracy on train data =\", score[1])","31588f43":"# Prediction accuracy on CV data\nscore = model.evaluate_generator(valid_generator)\nprint(\"Prediction accuracy on CV data =\", score[1])","8516b68e":"from sklearn.metrics import *\nimport seaborn as sns\n\ntick_labels = classes\n\ndef showClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID):\n    # Loop on each generator batch and predict\n    y_pred, y_true = [], []\n    for i in range(STEP_SIZE_VALID):\n        (X,y) = next(valid_generator)\n        y_pred.append(model.predict(X))\n        y_true.append(y)\n    \n    # Create a flat list for y_true and y_pred\n    y_pred = [subresult for result in y_pred for subresult in result]\n    y_true = [subresult for result in y_true for subresult in result]\n    \n    # Update Truth vector based on argmax\n    y_true = np.argmax(y_true, axis=1)\n    y_true = np.asarray(y_true).ravel()\n    \n    # Update Prediction vector based on argmax\n    y_pred = np.argmax(y_pred, axis=1)\n    y_pred = np.asarray(y_pred).ravel()\n    \n    # Confusion Matrix\n    fig, ax = plt.subplots(figsize=(10,10))\n    conf_matrix = confusion_matrix(y_true, y_pred, labels=np.arange(n_classes))\n    conf_matrix = conf_matrix\/np.sum(conf_matrix, axis=1)\n    sns.heatmap(conf_matrix, annot=True, fmt=\".2f\", square=True, cbar=False, \n                cmap=plt.cm.jet, xticklabels=tick_labels, yticklabels=tick_labels,\n                ax=ax)\n    ax.set_ylabel('Actual')\n    ax.set_xlabel('Predicted')\n    ax.set_title('Confusion Matrix')\n    plt.show()\n    \n    print('Classification Report:')\n    print(classification_report(y_true, y_pred, labels=np.arange(n_classes), target_names=classes))\n\nshowClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID)\n","e0fd22f2":"# Transfer Learning using Resnet50","75dac18d":"# Data Distribution","16dbde64":"# Visualization","9cca4198":"# Brain Tumor Classification","417c9563":"# Data Augmentation","a7bad93f":"# Visualizing Accuracies and Losses","7b4e57b3":"# Report","503eda14":"# Training","c0851e1a":"# Data Preparation"}}