{"cell_type":{"2f25e2b6":"code","e6f301df":"code","59346efc":"code","e2a25db0":"code","10c27bf9":"code","d948524c":"code","adae5b70":"code","6a3f9cad":"code","512b0ded":"code","5ccfd6a2":"code","d328df5d":"code","eb7f6416":"code","40efc028":"code","39202967":"code","0e62cc72":"code","4ab18f94":"code","f30deba5":"code","8decc147":"code","bc3f5638":"code","344e4c76":"code","b1c00ed8":"code","d6e2d748":"code","3a055a81":"code","a9e099ae":"code","f3d41857":"code","e3122d80":"code","ffc31992":"code","056af28f":"code","7fc3d0b4":"markdown","39b0c094":"markdown","983d39ff":"markdown","8464ff38":"markdown","5617861d":"markdown","2da95ec4":"markdown","d278b3e5":"markdown","54b14254":"markdown","e0c90830":"markdown","06c17343":"markdown","ac7c0d97":"markdown","5c98ab2e":"markdown","45f51540":"markdown","b15eb1c3":"markdown","b9520c0b":"markdown","d5c7b0da":"markdown","947528fc":"markdown","b11eeec7":"markdown","523d663d":"markdown","5a155e36":"markdown","0a2119d6":"markdown","21e9bf88":"markdown","89309fc8":"markdown"},"source":{"2f25e2b6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e6f301df":"import re\nimport pandas as pd\nimport nltk\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding,Dense,LSTM,Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import classification_report\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('wordnet')\n","59346efc":"train_data = pd.read_csv(\"..\/input\/fake-news\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/fake-news\/test.csv\")","e2a25db0":"train_data.head(3)\n","10c27bf9":"test_data.head(3)\n","d948524c":"train_data.shape\n","adae5b70":"test_data.shape\n","6a3f9cad":"test_data.isnull().sum()\n","512b0ded":"train_data.isnull().sum()\n","5ccfd6a2":"train_data.fillna(\"missing\",inplace = True)\ntest_data.fillna(\"missing\",inplace = True)","d328df5d":"x = train_data.drop(columns=\"label\")\ny = train_data['label']","eb7f6416":"x_copy = x.copy()\nws = WordNetLemmatizer()\nnew_list = []\n# new_list1 = []\nfor i in range(len(x_copy)):\n    titles = re.sub('[^a-zA-z]',\" \",x_copy['title'][i])\n    titles = titles.lower()\n    titles = titles.split()\n    titles = [ws.lemmatize(word) for word in titles if word not in stopwords.words(\"english\")]\n    titles = \" \".join(titles)\n    new_list.append(titles)","40efc028":"vocab_size = 10000\none_hot_title = [one_hot(i,vocab_size) for i in new_list]","39202967":"new_list[0]","0e62cc72":"one_hot_title[0]","4ab18f94":"res = len(max(new_list, key = len))\nres","f30deba5":"sent_len = 356\nembed_title = pad_sequences(one_hot_title,maxlen = sent_len,padding = 'pre')","8decc147":"embed_title.shape","bc3f5638":"model = Sequential()\nmodel.add(Embedding(input_dim=vocab_size,output_dim = 40,input_length=356))\nmodel.add(LSTM(150))\nmodel.add(Dense(1,activation = 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics=['accuracy'])\n","344e4c76":"plot_model(model)","b1c00ed8":"model.fit(embed_title,y,epochs = 7,validation_split=0.2)","d6e2d748":"model.evaluate(embed_title,y)","3a055a81":"test_data[:3]","a9e099ae":"test_list = []\nfor i in range(len(test_data)):\n    title = re.sub(\"[^a-zA-Z]\",\" \",test_data['title'][i])\n    title = title.lower()\n    title = title.split()\n    title = [ws.lemmatize(word) for word in title if word not in stopwords.words('english')]\n    title = \" \".join(title)\n    test_list.append(title)\n    ","f3d41857":"one_hot_test = [one_hot(i,vocab_size) for i in test_list]\nembed_test = pad_sequences(one_hot_test,maxlen = 356,padding = 'pre')","e3122d80":"ypred = model.predict(embed_test)\npred = (ypred>0.5).astype('int')","ffc31992":"mysubmit = pd.DataFrame(data = pred,columns = ['label'])\nmysubmit['id'] = test_data['id']","056af28f":"mysubmit.head()\n","7fc3d0b4":"### Model Description","39b0c094":"### Final File for submission","983d39ff":"### Data Preprocessing","8464ff38":"### Making every sentence of the data of Same Length","5617861d":"### Size of train data","2da95ec4":"### Performing the one hot encoding on the Title Column of the data","d278b3e5":"### Size of test Data","54b14254":"### Making the Prediction","e0c90830":"### First Sentence of the Title","06c17343":"### Reading the Data","ac7c0d97":"### Missing Values in test and Train Data","5c98ab2e":"### Preprocessing of the test data","45f51540":"### Performing the one hot encoding on the Title Column of the test data and making the sentence of equal length","b15eb1c3":"### Importing the required Libraries","b9520c0b":"### Filling the missing Values","d5c7b0da":"### first 3 rows of Test Data","947528fc":"### Index of words located in the Dictionary","b11eeec7":"### Making The Model","523d663d":"### Splitting the train data into X and Y","5a155e36":"### First 3 Rows of Train Data","0a2119d6":"### Training the data","21e9bf88":"### Getting the max length sentence from the dataset","89309fc8":"### First 3 column of test data"}}