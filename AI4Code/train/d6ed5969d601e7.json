{"cell_type":{"d2d43b02":"code","652a32f4":"code","4c8f7be6":"code","5f757886":"code","b9db2bbf":"code","29f00824":"code","73ccd821":"code","dc29eaf4":"code","7ddb1f0f":"code","0b909155":"code","fd5ac90d":"code","162447af":"code","869a09d7":"code","df319c5a":"code","1d66d9ee":"code","b728eafd":"code","886307d9":"code","cf47c64f":"code","aaed5744":"code","9f32fb2b":"markdown","f3bc7216":"markdown","bed2e720":"markdown","26e33667":"markdown","30354286":"markdown","243c52b7":"markdown"},"source":{"d2d43b02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","652a32f4":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sys\nimport scipy\nimport sklearn\n\nimport warnings\nwarnings.filterwarnings('ignore') # To supress warnings\nsns.set(style=\"whitegrid\") # set the background for the graphs","4c8f7be6":"# Importing data\ndata = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndata.head(5)","5f757886":"print(data.columns)","b9db2bbf":"data.info()","29f00824":"data.isnull().sum()","73ccd821":"data.describe()","dc29eaf4":"data.shape","7ddb1f0f":"# random_state helps assure that you always get the same output when you split the data\n# this helps create reproducible results and it does not actually matter what the number is\n# frac is percentage of the data that will be returned\ndata = data.sample(frac = 0.2, random_state = 1)\nprint(data.shape)","0b909155":"# Visualize the count of survivors\nsns.countplot('Class', data=data)","fd5ac90d":"print(\"Fraud to NonFraud Ratio of {:.3f}%\".format(492\/284315*100))","162447af":"plt.style.use('bmh')\nsns.kdeplot(data.Amount[data.Class == 0], label = 'Fraud', shade=True)\nsns.kdeplot(data.Amount[data.Class == 1], label = 'NonFraud', shade=True)\nplt.xlabel('Amount')","869a09d7":"sns.kdeplot(data.Time[data.Class == 0], label = 'Fraud', shade=True)\nsns.kdeplot(data.Time[data.Class == 1], label = 'NonFraud', shade=True)\nplt.xlabel('Time')","df319c5a":"plt.figure(figsize=(15,15))\nsns.heatmap(data.corr()) # Displaying the Heatmap\n\nplt.title('Heatmap correlation')\nplt.show()","1d66d9ee":"# get the columns from the dataframe\ncolumns = data.columns.tolist()\n\n# filter the columns to remove the data we do not want\ncolumns = [c for c in columns if c not in ['Class']]\n\n# store the variable we will be predicting on which is class\ntarget = 'Class'\n\n# X includes everything except our class column\nX = data[columns]\n# Y includes all the class labels for each sample\n# this is also one-dimensional\nY = data[target]\n\n# print the shapes of X and Y\nprint(X.shape)\nprint(Y.shape)","b728eafd":"from sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor","886307d9":"# determine the number of fraud cases\nfraud = data[data['Class'] == 1]\nvalid = data[data['Class'] == 0]\n\noutlier_fraction = len(fraud) \/ float(len(valid))\nprint(outlier_fraction)\n\nprint('Fraud Cases: {}'.format(len(fraud)))\nprint('Valid Cases: {}'.format(len(valid)))","cf47c64f":"state = 1\n\n# define the outlier detection methods\nclassifiers = {\n    # contamination is the number of outliers we think there are\n    'Isolation Forest': IsolationForest(max_samples = len(X),\n                                       contamination = outlier_fraction,\n                                       random_state = state),\n    # number of neighbors to consider, the higher the percentage of outliers the higher you want to make this number\n    'Local Outlier Factor': LocalOutlierFactor(\n    n_neighbors = 20,\n    contamination = outlier_fraction)\n}","aaed5744":"n_outliers = len(fraud)\n\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    \n    # fit the data and tag outliers\n    if clf_name == 'Local Outlier Factor':\n        y_pred = clf.fit_predict(X)\n        scores_pred = clf.negative_outlier_factor_\n    else:\n        clf.fit(X)\n        scores_pred = clf.decision_function(X)\n        y_pred = clf.predict(X)\n        \n        \n        \n# reshape the prediction values to 0 for valid and 1 for fraud\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n\n    # calculate the number of errors\n    n_errors = (y_pred != Y).sum()\n     \n    # classification matrix\n    print('{}: {}'.format(clf_name, n_errors))\n    print(accuracy_score(Y, y_pred))\n    print(classification_report(Y, y_pred))","9f32fb2b":"Looking at precision for fraudulent cases (1) lets us know the percentage of cases that are getting correctly labeled. 'Precision' accounts for false-positives. 'Recall' accounts for false-negatives. Low numbers could mean that we are constantly calling clients asking them if they actually made the transaction which could be annoying.\n\nGoal: To get better percentages.\n\nOur Isolation Forest method (which is Random Forest based) was able to produce a better result. Looking at the f1-score 26% (or approx. 30%) of the time we are going to detect the fraudulent transactions.","f3bc7216":"As we can notice, most of the features are not correlated with each other.\n\nWhat can generally be done on a massive dataset is a dimension reduction. By picking the most important dimensions, there is a possiblity of explaining most of the problem, thus gaining a considerable amount of time while preventing the accuracy to drop too much.","bed2e720":"Looks like there a lot more instances of small fraud amounts than really large ones.","26e33667":"As we can see that the feature time doesn't seem to have an impact in the frequency of frauds.","30354286":"## **Exploratory Data Analysis** ##","243c52b7":"We do not know what actually V1,V2....V28 mean due to data confidentiality, but what we know is they're going to help us draw insights from the data."}}