{"cell_type":{"30aed4d9":"code","54829d0e":"code","6a705930":"code","840240b3":"code","80e91752":"code","215d0280":"code","e2b7d047":"code","9f393c8f":"code","1d6282ee":"code","843a8e54":"code","ea2cd9b5":"code","e73ffec1":"code","919f0c93":"code","3cb688f5":"code","e1eeead4":"code","5f14a2c5":"code","024c953c":"code","c6358c22":"code","33c84eca":"code","18db995a":"code","391b5fa5":"code","fa2ff6b6":"code","dd3c8edc":"markdown","4fe2ad91":"markdown","3a5bbd85":"markdown","7e43a84a":"markdown","211f298c":"markdown","95c6d71a":"markdown","4267945f":"markdown","c4d73e0c":"markdown","2fc05ce1":"markdown","7ac08ad4":"markdown","eecfc4b6":"markdown","1a949711":"markdown","d8947f4f":"markdown","c0bbe031":"markdown"},"source":{"30aed4d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","54829d0e":"df_data = pd.read_json(\"..\/input\/resume-entities-for-ner\/Entity Recognition in Resumes.json\", lines = True)\ndf_data.head(5)","6a705930":"df_data[\"content\"] = df_data[\"content\"].apply(lambda x: x.replace(\"\\n\", \" \"))\ndf_data[\"content\"].head(5)","840240b3":"# import logging\nimport json\nimport re\n\n# JSON formatting functions\ndef convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n    training_data = []\n    lines=[]\n    with open(dataturks_JSON_FilePath, 'r') as f:\n        lines = f.readlines()\n\n    for line in lines:\n        data = json.loads(line)\n        text = data['content'].replace(\"\\n\", \" \")\n        entities = []\n        data_annotations = data['annotation']\n        if data_annotations is not None:\n            for annotation in data_annotations:\n                #only a single point in text annotation.\n                point = annotation['points'][0]\n                labels = annotation['label']\n                # handle both list of labels or a single label.\n                if not isinstance(labels, list):\n                    labels = [labels]\n\n                for label in labels:\n                    point_start = point['start']\n                    point_end = point['end']\n                    point_text = point['text']\n\n                    lstrip_diff = len(point_text) - len(point_text.lstrip())\n                    rstrip_diff = len(point_text) - len(point_text.rstrip())\n                    if lstrip_diff != 0:\n                        point_start = point_start + lstrip_diff\n                    if rstrip_diff != 0:\n                        point_end = point_end - rstrip_diff\n                    entities.append((point_start, point_end + 1 , label))\n        training_data.append((text, {\"entities\" : entities}))\n    return training_data\n\ndef trim_entity_spans(data: list) -> list:\n    \"\"\"Removes leading and trailing white spaces from entity spans.\n\n    Args:\n        data (list): The data to be cleaned in spaCy JSON format.\n\n    Returns:\n        list: The cleaned data.\n    \"\"\"\n    invalid_span_tokens = re.compile(r'\\s')\n\n    cleaned_data = []\n    for text, annotations in data:\n        entities = annotations['entities']\n        valid_entities = []\n        for start, end, label in entities:\n            valid_start = start\n            valid_end = end\n            while valid_start < len(text) and invalid_span_tokens.match(\n                    text[valid_start]):\n                valid_start += 1\n            while valid_end > 1 and invalid_span_tokens.match(\n                    text[valid_end - 1]):\n                valid_end -= 1\n            valid_entities.append([valid_start, valid_end, label])\n        cleaned_data.append([text, {'entities': valid_entities}])\n    return cleaned_data","80e91752":"data = trim_entity_spans(convert_dataturks_to_spacy(\"..\/input\/resume-entities-for-ner\/Entity Recognition in Resumes.json\"))\ndata[0]","215d0280":"def clean_entities(training_data):\n    \n    clean_data = []\n    for text, annotation in training_data:\n        \n        entities = annotation.get('entities')\n        entities_copy = entities.copy()\n        \n        # append entity only if it is longer than its overlapping entity\n        i = 0\n        for entity in entities_copy:\n            j = 0\n            for overlapping_entity in entities_copy:\n                # Skip self\n                if i != j:\n                    e_start, e_end, oe_start, oe_end = entity[0], entity[1], overlapping_entity[0], overlapping_entity[1]\n                    # Delete any entity that overlaps, keep if longer\n                    if ((e_start >= oe_start and e_start <= oe_end) \\\n                    or (e_end <= oe_end and e_end >= oe_start)) \\\n                    and ((e_end - e_start) <= (oe_end - oe_start)):\n                        entities.remove(entity)\n                j += 1\n            i += 1\n        clean_data.append((text, {'entities': entities}))\n                \n    return clean_data\n\ndata = clean_entities(data)","e2b7d047":"# Changing data to appropriate format so as to feed it to the model\n\nfrom nltk.corpus import stopwords\nen_stops = set(stopwords.words('english'))\n\ndf_data = pd.DataFrame(columns = [\"clean_content\", \"entities_mapped\"])\n\nentities_mapped = []\nclean_content = []\n\nfor i in range(len(data)):\n    content = data[i][0].split()\n    entities = data[i][1][\"entities\"]\n    words = []\n    labels = []\n    \n    for word in content:\n        if (word.isalnum() or word.find(\".com\") != -1) and word not in en_stops:\n            words.append(word)\n            found = False\n            for entity in sorted(entities):\n                ent_start = entity[0]\n                ent_end = entity[1]\n                ent_label = entity[2]\n\n                if word in data[i][0][ent_start: ent_end].split(): \n                    labels.append(ent_label)\n                    found = True\n                    break\n            if not found:\n                labels.append(\"O\")\n    \n    entities_mapped.append(labels)\n    clean_content.append(words)\n    \ndf_data[\"entities_mapped\"] = entities_mapped\ndf_data[\"clean_content\"] = clean_content\ndf_data[\"clean_content\"] = df_data[\"clean_content\"].apply(lambda x: \" \".join(x))","9f393c8f":"df_data[\"clean_content\"]","1d6282ee":"df_data.info()","843a8e54":"df_data.head(5)","ea2cd9b5":"n_labels = len(set([e for entity in df_data[\"entities_mapped\"].values for e in entity]))\nprint(n_labels)","e73ffec1":"df_data.to_csv(\"resume_ner.csv\")","919f0c93":"df_data['clean_content'].apply(len).plot(kind = 'hist')","3cb688f5":"import transformers\n\nmax_len = 512\ntrain_batch_size = 16\nvalid_batch_size = 8\nepochs = 10\n\nbase_model_path = \"dbmdz\/bert-base-cased-finetuned-conll03-english\"\nmodel_path = \"model.bin\"\n\ntraining_file = \".\/resume_ner.csv\"\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\n    base_model_path,\n    do_lower_case = False\n)\n","e1eeead4":"import torch\n\nclass EntityDataset:\n    \n    def __init__(self, texts, tags):\n        # texts: [[\"Hello\", \"my\", \"name\", \"is\", \"Mohamed\"], [\"Hello\", \"...\", \"...\", ....], ...]\n        # pos\/tags: [[1, 2, 3, 4, 5], ..., ..., ....]]\n        self.texts = texts\n        self.tags = tags\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, item):\n        text = self.texts[item]\n        tags = self.tags[item]\n        \n        ids = []\n        target_tags = []\n        \n        for i, s in enumerate(text):\n            inputs = tokenizer.encode(\n                s,\n                add_special_tokens = False\n            )\n            # Mohamed: Mo ##ha ##med\n            input_len = len(inputs)\n            ids.extend(inputs)\n            target_tags.extend([tags[i]] * input_len)\n            \n        ids = ids[: max_len - 2]\n        target_tags = target_tags[: max_len - 2]\n\n        ids = [101] + ids + [102]\n        target_tags = [0] + target_tags + [0]\n\n        mask = [1] * len(ids)\n        token_type_ids = [0] * len(ids)\n\n        padding_len = max_len - len(ids)\n\n        ids = ids + ([0] * padding_len)\n        mask = mask + ([0] * padding_len)\n        token_type_ids = token_type_ids + ([0] * padding_len)\n        target_tags = target_tags + ([0] * padding_len)\n\n        return {\n            \"ids\": torch.tensor(ids, dtype = torch.long),\n            \"mask\": torch.tensor(mask, dtype = torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype = torch.long),\n            \"target_tags\": torch.tensor(target_tags, dtype = torch.long)\n        }\n            \n            ","5f14a2c5":"from tqdm import tqdm, tqdm_notebook\n\ndef train_fn(data_loader, model, optimizer, device, scheduler):\n    \n    model.train()\n    final_loss = 0\n    for data in tqdm_notebook(data_loader, total = len(data_loader)):\n        for k, v in data.items():\n            data[k] = v.to(device)\n        optimizer.zero_grad()\n        loss = model(**data)[0]\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        final_loss += loss.item()\n    return final_loss \/ len(data_loader)\n\ndef eval_fn(data_loader, model, device):\n    \n    model.eval()\n    final_loss = 0\n    for data in tqdm_notebook(data_loader, total = len(data_loader)):\n        for k, v in data.items():\n            data[k] = v.to(device)\n#         optimizer.zero_grad()\n        loss = model(**data)[0]\n#         loss.backward()\n#         optimizer.step()\n#         scheduler.step()\n        final_loss += loss.item()\n    return final_loss \/ len(data_loader)","024c953c":"import torch.nn as nn\n\ndef loss_fn(output, target, mask, num_labels):\n    lfn = nn.CrossEntropyLoss()\n    active_loss = mask.view(-1) == 1\n    active_logits = output.view(-1, num_labels)\n    active_labels = torch.where(\n        active_loss,\n        target.view(-1),\n        torch.tensor(lfn.ignore_index).type_as(target)\n    )\n    loss = lfn(active_logits, active_labels)\n    \n    return loss\n\nclass EntityModel(nn.Module):\n    \n    def __init__(self, enc_tag):\n        super(EntityModel, self).__init__()\n        \n        self.num_tag = len(enc_tag.classes_)\n        \n        self.config = transformers.AutoConfig.from_pretrained(base_model_path)\n        self.config._num_labels = self.num_tag\n        self.config.label2id = {k: v for k, v in zip(enc_tag.classes_, enc_tag.transform(enc_tag.classes_))}\n        self.config.id2label = {k: v for k, v in zip(enc_tag.transform(enc_tag.classes_), enc_tag.classes_)}\n        \n        self.bert = transformers.AutoModel.from_pretrained(base_model_path)\n        self.classifier = transformers.AutoModelForTokenClassification.from_config(self.config)\n        self.classifier.bert = self.bert\n#         self.bert_drop_1 = nn.Dropout(0.3)\n#         self.bert_drop_2 = nn.Dropout(0.3)\n#         self.out_tag = nn.Linear(768, self.num_tag)\n        \n    def forward(self, ids, mask, token_type_ids, target_tags):\n        \n        output_1 = self.classifier(ids, attention_mask = mask, token_type_ids = token_type_ids, labels = target_tags)\n#         bo_tag = self.bert_drop_1(ol)\n#         tag = self.out_tag(bo_tag)\n#         loss_tag = loss_fn(tag, target_tags, mask, self.num_tag)\n#         loss = loss_tag\n        \n        return output_1","c6358c22":"from sklearn import preprocessing\nfrom sklearn import model_selection\n\n\nimport joblib\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\ndef process_data(df):\n    \n#     df = pd.read_csv(data_path, encoding = \"latin-1\")\n    \n    enc_tag = preprocessing.LabelEncoder()\n    \n    all_ents = df['entities_mapped'].apply(pd.Series).stack().values\n    enc_tag.fit(all_ents)\n    \n    sentences = list(df[\"clean_content\"].str.split())\n    tag = list(df[\"entities_mapped\"].apply(enc_tag.transform))\n    \n    \n    return sentences, tag, enc_tag\n\nif __name__ == \"__main__\":\n    \n    sentences, tag, enc_tag = process_data(df_data)\n        \n    meta_data = {\n        \"enc_tag\": enc_tag\n    }\n    \n    joblib.dump(meta_data, \"meta.bin\")\n    \n    num_tag = len(list(enc_tag.classes_))\n\n    (\n        train_sentences,\n        test_sentences,\n        train_tag,\n        test_tag\n    ) = model_selection.train_test_split(sentences, tag, random_state = 42, test_size = 0.1)\n    \n    train_dataset = EntityDataset(\n        texts = train_sentences, tags = train_tag\n    )\n    \n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size = train_batch_size, num_workers = 4\n    )\n    \n    valid_dataset = EntityDataset(\n        texts = test_sentences, tags = test_tag\n    )\n    \n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size = train_batch_size, num_workers = 4\n    )\n    \n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset, batch_size = valid_batch_size, num_workers=1\n    )\n\n    \n    device = torch.device(\"cuda\")\n    model = EntityModel(enc_tag = enc_tag)\n    model.to(device)\n    \n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    \n    optimizer_parameters = [\n        {\n            \"params\": [\n                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.001\n        },\n        {\n            \"params\": [\n                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0\n        }\n    ]\n    \n    num_train_steps = int(len(train_sentences) \/ train_batch_size * epochs)\n    optimizer = AdamW(optimizer_parameters, lr = 3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps = 0, num_training_steps = num_train_steps \n    )\n    \n    best_loss = np.inf\n    \n    for epoch in range(epochs):\n        \n        train_loss = train_fn(train_data_loader, model, optimizer, device, scheduler)\n        test_loss = eval_fn(valid_data_loader, model, device)\n#         outputs = np.array(outputs) >= 0.5\n#         accuracy = metrics.accuracy_score(targets, outputs)\n        print(f\"Train Loss: {train_loss}, Test Loss: {test_loss}\")\n        \n        if test_loss < best_loss:\n            best_loss = test_loss\n            torch.save(model.state_dict(), model_path)","33c84eca":"!pip install seqeval","18db995a":"from seqeval.metrics import accuracy_score\n\ndef flat_accuracy(preds, labels):\n    flat_preds = np.argmax(preds, axis=2)\n    flat_labels = labels\n    return accuracy_score(flat_preds, flat_labels)","391b5fa5":"def valid(model, testing_loader, dev):\n    model.eval()\n    eval_loss = 0; eval_accuracy = 0\n    n_correct = 0; n_wrong = 0; total = 0\n    predictions , true_labels = [], []\n    nb_eval_steps, nb_eval_examples = 0, 0\n    with torch.no_grad():\n        for _, data in enumerate(testing_loader, 0):\n            \n            for k, v in data.items():\n                data[k] = v.to(device, dtype = torch.long)\n \n            ids = data['ids']\n            mask = data['mask']\n            targets = data['target_tags']\n\n            output = model(**data)\n            loss, logits = output[:2]\n            logits = logits.detach().cpu().numpy()\n            label_ids = targets.to('cpu').numpy()\n            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n            true_labels.append(label_ids)\n            accuracy = flat_accuracy(logits, label_ids)\n            eval_loss += loss.mean().item()\n            eval_accuracy += accuracy\n            nb_eval_examples += ids.size(0)\n            nb_eval_steps += 1\n        eval_loss = eval_loss\/nb_eval_steps\n#         print(\"Validation loss: {}\".format(eval_loss))\n#         print(\"Validation Accuracy: {}\".format(eval_accuracy\/nb_eval_steps))\n        \n    return flat_accuracy","fa2ff6b6":"# tags_vals = list(set(dataset[\"\"].values))\npredictions, true_labels = valid(model, valid_data_loader, device)","dd3c8edc":"### Evaluation and Inference","4fe2ad91":"## Data Exploration and Cleaning","3a5bbd85":"### Engine","7e43a84a":"### Model","211f298c":"### Entities Cleaning","95c6d71a":"### Model Configurations","4267945f":"### Text Cleaning","c4d73e0c":"### Entity Mapping","2fc05ce1":"## Dataset Loader","7ac08ad4":"## Named Entity Recognition","eecfc4b6":"### Training","1a949711":"# Dataset","d8947f4f":"### Overlapping Entities","c0bbe031":"### Text Length"}}