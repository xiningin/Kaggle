{"cell_type":{"d7d66f65":"code","6f9d3c59":"code","225b916e":"code","e49f330d":"code","fc9c3c2b":"code","d2375062":"code","7d9d20ba":"code","853d98dc":"code","09de7c66":"code","f4efed5e":"code","2d51b17e":"code","3a4dd408":"code","26cc24b6":"code","1c050c6c":"code","f33c0424":"code","1de34b8e":"code","67ec6c90":"code","e7d8c7ec":"code","e7cc75e1":"code","256f3086":"markdown","4bc2230a":"markdown","8ca20469":"markdown","e42d0f2b":"markdown","062588ed":"markdown"},"source":{"d7d66f65":"# Import all dependencies\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport warnings\nimport datetime\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport os\nimport gc\n\nwarnings.filterwarnings(\"ignore\")\nprint(os.listdir(\"..\/input\"))","6f9d3c59":"train = pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\ntest = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])\nhist_trans = pd.read_csv(\"..\/input\/historical_transactions.csv\")\nnew_trans = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")","225b916e":"train[\"month\"] = train[\"first_active_month\"].dt.month\ntest[\"month\"] = test[\"first_active_month\"].dt.month\ntrain[\"year\"] = train[\"first_active_month\"].dt.year\ntest[\"year\"] = test[\"first_active_month\"].dt.year\ntrain['elapsed_time'] = (datetime.date(2018, 2, 1) - train['first_active_month'].dt.date).dt.days\ntest['elapsed_time'] = (datetime.date(2018, 2, 1) - test['first_active_month'].dt.date).dt.days\ntrain = pd.get_dummies(train, columns=['feature_1', 'feature_2'])\ntest = pd.get_dummies(test, columns=['feature_1', 'feature_2'])\n#train.head()","e49f330d":"hist_trans = pd.get_dummies(hist_trans, columns=['category_2', 'category_3'])\nhist_trans['authorized_flag'] = hist_trans['authorized_flag'].map({'Y': 1, 'N': 0})\nhist_trans['category_1'] = hist_trans['category_1'].map({'Y': 1, 'N': 0})\n#hist_trans.head()","fc9c3c2b":"def aggregate_transactions(trans, prefix):  \n    trans.loc[:, 'purchase_date'] = pd.DatetimeIndex(trans['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['mean'],\n        'category_2_1.0': ['mean'],\n        'category_2_2.0': ['mean'],\n        'category_2_3.0': ['mean'],\n        'category_2_4.0': ['mean'],\n        'category_2_5.0': ['mean'],\n        'category_3_A': ['mean'],\n        'category_3_B': ['mean'],\n        'category_3_C': ['mean'],\n        'merchant_id': ['nunique'],\n        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n    }\n    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n    agg_trans.columns = [prefix + '_'.join(col).strip() \n                           for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n    \n    df = (trans.groupby('card_id')\n          .size()\n          .reset_index(name='{}transactions_count'.format(prefix)))\n    \n    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n    \n    return agg_trans","d2375062":"merch_hist = aggregate_transactions(hist_trans, prefix='hist_')\ndel hist_trans\ngc.collect()\ntrain = pd.merge(train, merch_hist, on='card_id',how='left')\ntest = pd.merge(test, merch_hist, on='card_id',how='left')\ndel merch_hist\ngc.collect()\n\nnew_trans = pd.get_dummies(new_trans, columns=['category_2', 'category_3'])\nnew_trans['authorized_flag'] = new_trans['authorized_flag'].map({'Y': 1, 'N': 0})\nnew_trans['category_1'] = new_trans['category_1'].map({'Y': 1, 'N': 0})\nmerch_new = aggregate_transactions(new_trans, prefix='new_')\ndel new_trans\ngc.collect()\n\ntrain = pd.merge(train, merch_new, on='card_id',how='left')\ntest = pd.merge(test, merch_new, on='card_id',how='left')\ndel merch_new\ngc.collect()","7d9d20ba":"IDS = test['card_id']\ntarget = train['target']\ndrops = ['card_id', 'first_active_month', 'target']\nuse_cols = [c for c in train.columns if c not in drops]\nfeatures = list(train[use_cols].columns)\ntrain[features].head()","853d98dc":"print(train[features].shape)\nprint(test[features].shape)","09de7c66":"train = train[features+['target']]\ntest = test[features]\nprint('Final train data shape is:',train.shape)\nprint('Final test data shape is:', test.shape)","f4efed5e":"import h2o\nfrom h2o.automl import H2OAutoML","2d51b17e":"# intializing H2o\nh2o.init()","3a4dd408":"# Convert pandas datarame into h2o dataframe\nhtrain = h2o.H2OFrame(train)\nhtest = h2o.H2OFrame(test)\ndel train, test\ngc.collect()\nprint(htrain.shape, htest.shape)\nprint(htrain.head)","26cc24b6":"# Assign x as Independent and y as Dependent\nx = htrain.columns\ny = \"target\"\nx.remove(y)","1c050c6c":"# Uncomment and run locally to understand the different Parameter\n#?? H2OAutoML\n\"\"\"\nH2OAutoML(nfolds=5, balance_classes=False, class_sampling_factors=None, \nmax_after_balance_size=5.0, max_runtime_secs=3600, max_models=None, stopping_metric='AUTO', \nstopping_tolerance=None, stopping_rounds=3, seed=None, project_name=None, \nexclude_algos=None, keep_cross_validation_predictions=False, \nkeep_cross_validation_models=False, keep_cross_validation_fold_assignment=False, \nsort_metric='AUTO')\n\"\"\"","f33c0424":"# Since our goal is AutoML, I am specifying the least parameters\nautoelo = H2OAutoML(max_runtime_secs=10000, seed=42)\nautoelo.train(x=x, y=y, training_frame=htrain)","1de34b8e":"# Stack up and display the results of the top models, the metrics displayed will be relevant to regression problems\nlb = autoelo.leaderboard\nlb.head(rows=lb.nrows)","67ec6c90":"# Display the properties of the leader\nautoelo.leader","e7d8c7ec":"# Let us get the predictions against the test data\npreds = autoelo.leader.predict(htest)\npreds = preds.as_data_frame()","e7cc75e1":"# Let us create a submission\nsub_df = pd.DataFrame({\"card_id\":IDS.values, \"target\": preds.predict.values})\nprint(sub_df.shape)\nsub_df.head()\n# Submit the prediction\nsub_df.to_csv('AML_sub.csv', index=False)","256f3086":"# Auto Elo  \n\nIn this kernel I try to the power of [AutoML](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html from H2O) I stongly believe that these can serve as good starting points.  \n","4bc2230a":"I did not spend time in creating features but I borrowed it from several fantastic starter notebooks below, the top 2  notables ones are:\n[Notebook 1](https:\/\/www.kaggle.com\/youhanlee\/hello-elo-ensemble-will-help-you)\n[Notebook 2](https:\/\/www.kaggle.com\/fabiendaniel\/elo-world)","8ca20469":"# Let us bring in H2O now","e42d0f2b":"# All File imports","062588ed":"# Feature Engineering"}}