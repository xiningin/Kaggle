{"cell_type":{"fdec432e":"code","bdddc4ea":"code","87e6804e":"code","12374968":"code","dd10dbdf":"code","5d05b840":"code","14d8751f":"code","dc846de8":"code","b17f3a4a":"code","c2075539":"code","18170ec9":"code","8b7f7577":"code","6ce47a2a":"code","57d1afd5":"code","ee8b5064":"code","1d83332a":"code","41634f00":"code","81724cb6":"code","21e6543f":"code","f770c926":"code","4365a41f":"code","11b45bf2":"code","50433362":"code","cbc43deb":"code","9a66ddbb":"code","1b254507":"code","286cf682":"code","49c8c2aa":"code","3f1c5e69":"code","b16b47af":"markdown","cc32c28a":"markdown","dcbb7d26":"markdown","b0136cf2":"markdown","982afa28":"markdown","ca9245af":"markdown","05138154":"markdown","8329fd20":"markdown","cfd6f41b":"markdown","6c92ced7":"markdown","73718859":"markdown","aee41b8b":"markdown","b97fb5d3":"markdown"},"source":{"fdec432e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, plot_confusion_matrix, roc_curve, auc\n\nimport warnings\nwarnings.filterwarnings('ignore')","bdddc4ea":"data = pd.read_csv('..\/input\/gender-classification-dataset\/gender_classification_v7.csv')\nprint(data.shape)\ndata.head()","87e6804e":"# Encode the 'gender' variable: 0 for 'Male & 1 for 'Female'\ncode = {'Male':0, 'Female':1}\ndata['gender'] = data['gender'].map(code)\ndata.head()","12374968":"data.info()","dd10dbdf":"# Turn the features into the right data type\ncategories = [i for i in data.columns if data[i].dtype == 'int64']\nfor i in categories:\n  data[i] = data[i].astype('category')\n\ndata.info()","5d05b840":"# Investigate the number of unique values in each column\nfor i in data.columns:\n  print(f'The column \"{i}\" has {len(data[i].value_counts())} unique values.')","14d8751f":"print(data['gender'].value_counts())\n\npie, ax = plt.subplots(figsize=[15,10])\nlabels = [1, 0]\ncolors = ['#7b77ff', '#7df691']\nplt.pie(x = data['gender'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors)\nplt.title('Gender distribution')\nplt.show()","dc846de8":"plt.figure(figsize=(8,12))\nmy_pall={1:'#7b77ff', 0:'#7df691'}\nsns.boxplot(x='gender', y=\"forehead_width_cm\", data=data, palette=my_pall)\nplt.title('Boxplot of forehead_width_cm by gender')\nplt.grid()\nplt.show()","b17f3a4a":"plt.figure(figsize=(8,12))\nmy_pall={1:'#7b77ff', 0:'#7df691'}\nsns.boxplot(x='gender', y=\"forehead_height_cm\", data=data, palette=my_pall)\nplt.title('Boxplot of forehead_height_cm by gender')\nplt.grid()\nplt.show()","c2075539":"genders_diff = data.groupby('gender')[['forehead_width_cm','forehead_height_cm' ]].mean()\n\nlabels = ['forehead width', 'forehead height']\nm_means =[genders_diff.iloc[0, 0], genders_diff.iloc[0,1]]\nf_means = [genders_diff.iloc[1, 0], genders_diff.iloc[1,1]]\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(12,8))\nrects1 = ax.bar(x - width\/2, m_means, width, label='0', color = '#7df691')\nrects2 = ax.bar(x + width\/2, f_means, width, label='1', color = '#7b77ff')\n\n\nax.set_ylabel('Mean')\nax.set_title(\"Forehead's MEAN width & height by gender\")\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\nplt.grid()\nplt.show()","18170ec9":"males = data[data['gender'] == 0]\nfemales = data[data['gender'] == 1]\n# HAIR comparison\npie, ax = plt.subplots(1,2, figsize=[15,10])\nlabels = ['Long', 'Not-Long']\ncolors_m = ['#35f154', '#adf9b9']\ncolors_f = ['#4a44ff', '#adaaff']\nax[0].pie(x = males['long_hair'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_m)\nax[0].set_title('Males hair type')\nax[1].pie(x = females['long_hair'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_f)\nax[1].set_title('Females hair type')\nplt.show()","8b7f7577":"# NOSE WIDE comparison\npie, ax = plt.subplots(1,2, figsize=[15,10])\nlabels = ['Wide', 'Non-wide']\ncolors_m = ['#35f154', '#adf9b9']\ncolors_f = ['#4a44ff', '#adaaff']\nax[0].pie(x = males['nose_wide'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_m)\nax[0].set_title('Males wide \/ non-wide nose')\nax[1].pie(x = females['nose_wide'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_f)\nax[1].set_title('Females wide \/ non-wide nose')\nplt.show()","6ce47a2a":"# NOSE LONG comparison \npie, ax = plt.subplots(1,2, figsize=[15,10])\nlabels = ['Long', 'Non-long']\ncolors_m = ['#35f154', '#adf9b9']\ncolors_f = ['#4a44ff', '#adaaff']\nax[0].pie(x = males['nose_long'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_m)\nax[0].set_title('Males long \/ non-long nose')\nax[1].pie(x = females['nose_long'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_f)\nax[1].set_title('Females long \/ non-long nose')\nplt.show()","57d1afd5":"# LIPS comparison\npie, ax = plt.subplots(1,2, figsize=[15,10])\nlabels = ['Thin', 'Non-thin']\ncolors_m = ['#35f154', '#adf9b9']\ncolors_f = ['#4a44ff', '#adaaff']\nax[0].pie(x = males['lips_thin'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_m)\nax[0].set_title('Males thin \/ non-thin lips')\nax[1].pie(x = females['lips_thin'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_f)\nax[1].set_title('Females thin \/ non-thin lips')\nplt.show()","ee8b5064":"# LIPS-NOSE distance comparison\npie, ax = plt.subplots(1,2, figsize=[15,10])\nlabels = ['Long', 'Short']\ncolors_m = ['#35f154', '#adf9b9']\ncolors_f = ['#4a44ff', '#adaaff']\nax[0].pie(x = males['distance_nose_to_lip_long'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_m)\nax[0].set_title('Males distance between nose and lips')\nax[1].pie(x = females['distance_nose_to_lip_long'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_f)\nax[1].set_title('Females distance between nose and lips')\nplt.show()","1d83332a":"# Separate features and target\nfeatures = data.iloc[:, :-1]\ntarget = data.iloc[:, -1]\n\n# Split them into training and testing set\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 123, shuffle = True, stratify = target)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","41634f00":"# Initiate classifiers\nLR = LogisticRegression()\nSGDC = SGDClassifier()\nSVC = SVC()\nKNN = KNeighborsClassifier()\nDT = DecisionTreeClassifier()\n\n# Initiate hyperparameters for classifiers\nparam_LR = {'C':[0.01, 0.1, 1, 10], 'penalty':['l1', 'l2']}\nparam_SGDC = {'alpha':[0.01, 0.1, 1, 10], 'loss':['hinge', 'log'], 'penalty':['l1', 'l2']}\nparam_SVC = {'C':[0.01, 0.1, 1, 10], 'gamma':[0.01, 0.1, 1, 10]}\nparam_KNN = {'n_neighbors':[2,3,4,5,6]}\nparam_DT = {'criterion':['gini', 'entropy'], 'max_depth': [3,4,5,6], 'min_samples_leaf':[0.1, 0.5, 1, 1.5, 2]}","81724cb6":"# Logistic egression\nsearch_LR = GridSearchCV(LR, param_LR)\nsearch_LR.fit(X_train, y_train)\nprint(f'Best CV params {search_LR.best_params_}')\nprint(f'Best CV accuracy {search_LR.best_score_}')\nprint(f'Test accuracy of best hypers {search_LR.score(X_test, y_test)}')","21e6543f":"# SGDClassifier\nsearch_SGDC = GridSearchCV(SGDC, param_SGDC)\nsearch_SGDC.fit(X_train, y_train)\nprint(f'Best CV params {search_SGDC.best_params_}')\nprint(f'Best CV accuracy {search_SGDC.best_score_}')\nprint(f'Test accuracy of best hypers {search_SGDC.score(X_test, y_test)}')","f770c926":"# SVC\nsearch_SVC = GridSearchCV(SVC, param_SVC)\nsearch_SVC.fit(X_train, y_train)\nprint(f'Best CV params {search_SVC.best_params_}')\nprint(f'Best CV accuracy {search_SVC.best_score_}')\nprint(f'Test accuracy of best hypers {search_SVC.score(X_test, y_test)}')","4365a41f":"# KNN\nsearch_KNN = GridSearchCV(KNN, param_KNN)\nsearch_KNN.fit(X_train, y_train)\nprint(f'Best CV params {search_KNN.best_params_}')\nprint(f'Best CV accuracy {search_KNN.best_score_}')\nprint(f'Test accuracy of best hypers {search_KNN.score(X_test, y_test)}')","11b45bf2":"# DecisionTree\nsearch_DT = GridSearchCV(DT, param_DT)\nsearch_DT.fit(X_train, y_train)\nprint(f'Best CV params {search_DT.best_params_}')\nprint(f'Best CV accuracy {search_DT.best_score_}')\nprint(f'Test accuracy of best hypers {search_DT.score(X_test, y_test)}')","50433362":"from sklearn.svm import SVC\n\n# Re-initiate the models with their best hyperparameters\nLR = LogisticRegression(C=0.1, penalty='l2')\nSGDC = SGDClassifier(alpha=0.01, loss='hinge', penalty='l2')\nSVC = SVC(C=10, gamma=0.1)\nKNN = KNeighborsClassifier(n_neighbors=5)\nDT = DecisionTreeClassifier(criterion='gini', max_depth=6, min_samples_leaf=2)\n\n# Define a list with tuples that contains classifier's name & classifier\nclassifiers = [('Logistic Regression', LR),\n               ('SGDClassifier', SGDC),\n               ('SVC', SVC),\n               ('KNN', KNN),\n               ('Decision Tree', DT)]","cbc43deb":"for c_name, c in classifiers:\n    c.fit(X_train, y_train)\n    preds = c.predict(X_test)\n    print(f'{c_name} accuracy: {accuracy_score(y_test, preds)}')","9a66ddbb":"# Initiate Voting Classifier\nVC = VotingClassifier(estimators=classifiers)\nVC.fit(X_train, y_train)\npreds = VC.predict(X_test)\nprint(f'Voting Classifier score: {accuracy_score(y_test, preds)}')","1b254507":"DT.fit(X_train, y_train)\npreds = DT.predict(X_test)\nconfusion_matrix(y_test, preds)","286cf682":"plot_confusion_matrix(DT, X_test, y_test)","49c8c2aa":"print(classification_report(y_test, preds))","3f1c5e69":"probs = DT.predict_proba(X_test)\npred = probs[:,1]\nfpr, tpr, threshold = roc_curve(y_test, pred)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(12,8))\nplt.title('ROC')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0,1], [0,1], 'r--')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","b16b47af":"## Hyperparameters tuning","cc32c28a":"# ***== Exploratory data analysis (EDA) ==***","dcbb7d26":"Using the ensemble learning, the accuracy was not improved.\n\nThe best performancce was achieved by the DecisionTreeClassifier.","b0136cf2":"As can be observed, most of the features are actually categorical, only 'forehead_width_cm' and 'forehead_height_cm' are numerical. ","982afa28":"## Ensemble Learning","ca9245af":"# ***== Data reading & cleaning ==***","05138154":"## Initialize classifiers and hyperparameters","8329fd20":"# ***== Data preparation ==***","cfd6f41b":"# ***== Modelling ==***","6c92ced7":"Out of 1001 samples, the DecisionTree missclassified 27.\n","73718859":"![](https:\/\/scx2.b-cdn.net\/gfx\/news\/hires\/2018\/gender.jpg)","aee41b8b":"# **Predicting gender with ensemble learning approach: VottingClassifier**","b97fb5d3":"# ***== Import libraries ==***"}}