{"cell_type":{"5d63582c":"code","923ea219":"code","4d264503":"code","4850ff40":"code","5ba0064b":"code","ce6fbe7a":"code","ac15426f":"code","0991fb3e":"code","305ef0b6":"code","f854f838":"code","bd1b9593":"code","a2ffca4f":"code","f48d89f2":"code","d6ce2061":"code","12e7b0cf":"code","a12d2b9f":"code","c306bb2a":"code","a5b92237":"code","e378041e":"code","4fdb4bde":"code","b2d0e1dc":"code","b12a3834":"code","bdc858bd":"code","c9bbddeb":"code","d55c7798":"code","802bbf5f":"code","a6bfcc59":"code","5d286eab":"code","20d489fc":"code","a8fd638c":"code","2b2841c1":"code","bb4b8073":"code","27a8c61a":"code","4666f8e8":"code","ec2953ef":"code","37ede34c":"code","2d9f4056":"code","fce9bcdb":"code","c0b65fb0":"code","bbe531a5":"code","d007f432":"code","9fad5f2a":"code","9649f8ae":"code","4ce97f6c":"code","5a24ca63":"code","0162e713":"code","bb607665":"code","d7fb5bfc":"code","afa043d1":"code","bd4c3638":"code","14bd807d":"code","afb453b7":"code","6c23895c":"code","8a6d0468":"code","266881b9":"code","97097256":"code","872712b9":"code","50e0174f":"code","461768bb":"markdown","64b95b62":"markdown","f25114ae":"markdown","3645e6dd":"markdown","f1d00e4b":"markdown","55dc30bb":"markdown","a5143561":"markdown","743e1b7f":"markdown","821ef170":"markdown","d80a54ad":"markdown","1d44209b":"markdown","466e5c67":"markdown","93ac3dd0":"markdown","7db15191":"markdown","a0359bba":"markdown","04df139e":"markdown","e9a4020c":"markdown","3efe7f19":"markdown","3be1eafa":"markdown","153692cd":"markdown","f6859a0c":"markdown","c0e15d23":"markdown","aa6584b2":"markdown","c53edc9b":"markdown","8a0c11d7":"markdown","61fa7599":"markdown","a4d40e68":"markdown","9dce6b24":"markdown","2c9f040b":"markdown","8c05fd67":"markdown"},"source":{"5d63582c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport regex as re\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity, linear_kernel\nfrom sklearn.decomposition import PCA\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","923ea219":"anime = pd.read_csv('\/kaggle\/input\/anime-recommendations-database\/anime.csv')\nrating = pd.read_csv('\/kaggle\/input\/anime-recommendations-database\/rating.csv')\nprint(anime.shape, rating.shape)\nanime.head()","4d264503":"anime.info()","4850ff40":"rating.head()\n# rating.info()","5ba0064b":"anime['name'].unique()[1:10]","ce6fbe7a":"def text_cleaning(text):\n    text = re.sub(r'&quot;', '', text) \n    text = re.sub(r'.hack\/\/', '', text)\n    text = re.sub(r'&#039;', '', text)\n    text = re.sub(r'A&#039;s', '', text)\n    text = re.sub(r'I&#039;', 'I\\'', text)\n    text = re.sub(r'&amp;', 'and', text)\n    text = re.sub(r'\u00c2\u00b0', '',text)\n    \n    return text\n\nanime['name'] = anime['name'].apply(text_cleaning)","ac15426f":"anime['name'].unique()[1:100]","0991fb3e":"df = pd.merge(anime,rating, on='anime_id',suffixes = ['', '_user'])\ndf = df.rename(columns={'name': 'anime_title', 'rating_user': 'user_rating'})\ndf.head()","305ef0b6":"df.info()\ndf.shape","f854f838":"df['anime_id']=df.anime_id.astype('object')\ndf['user_id']=df.user_id.astype('object')","bd1b9593":"df.info()","a2ffca4f":"# print('users are', len(rating['user_id'].value_counts()))\n# print('animes are', len(rating['anime_id'].value_counts()))\n# print(len(rating['user_id'].value_counts()), 'X', len(rating['anime_id'].value_counts()))","f48d89f2":"missing_values = df.isnull().sum()\nprint(missing_values[0:])","d6ce2061":"total_cells = np.product(df.shape)\ntotal_missing = missing_values.sum()\n(total_missing\/total_cells) * 100","12e7b0cf":"df.isnull().sum()","a12d2b9f":"df.dropna(inplace=True)","c306bb2a":"df.isnull().sum()","a5b92237":"duplicate = df.duplicated(subset=['anime_id','user_id']).sum()\nprint('There are {} duplicated rows in the data'.format(duplicate))","e378041e":"df.drop_duplicates(subset=['anime_id','user_id'],inplace=True)","4fdb4bde":"duplicate = df.duplicated().sum()\nprint('There are {} duplicated rows in the data'.format(duplicate))","b2d0e1dc":"df.shape","b12a3834":"df['user_id'].value_counts()","bdc858bd":"counts = df['user_id'].value_counts()\ndf = df[df['user_id'].isin(counts[counts >= 500].index)]\ndf['user_id'].value_counts()\ndf.shape","c9bbddeb":"df.tail()","d55c7798":"df.reset_index(drop=True,inplace=True)\ndf.tail()","802bbf5f":"animedf = df.copy()","a6bfcc59":"animedf.describe(include = 'object')","5d286eab":"# corr_matrix=animedf.corr()\n# corr_matrix\n# plt.figure(figsize=(5,5))\n# sns.heatmap(corr_matrix, cmap='Blues', annot=True, fmt=\".2f\")\n# sns.set(font_scale=1)","20d489fc":"anime_rating_count = df.groupby(by = ['anime_title'])['user_rating'].count().reset_index()[['anime_title', 'user_rating']]\nanime_rating_count.rename(columns = {'user_rating': 'totalRatingCount'},inplace=True)\nanime_rating_count.head()\n\ntop10_animerating = anime_rating_count[['anime_title', 'totalRatingCount']].sort_values(by = 'totalRatingCount',ascending = False).head(10)\nplt.figure(figsize=(15,5))\nax=sns.barplot(x=\"anime_title\", y=\"totalRatingCount\", data=top10_animerating, palette=\"gnuplot2\")\nax.set_xticklabels(ax.get_xticklabels(), fontsize=11, rotation=40, ha=\"right\")\nax.set_title('Top 10 Anime based on rating counts',fontsize = 22)\nax.set_xlabel('Anime',fontsize = 20) \nax.set_ylabel('Rating Count', fontsize = 20)","a8fd638c":"animedf=animedf.merge(anime_rating_count, left_on = 'anime_title', right_on = 'anime_title', how = 'left')\nanimedf.head()","2b2841c1":"plt.figure(figsize = (20, 5))\nplt.subplot(1,2,1)\ndf['rating'].hist(bins=50)\nplt.title(\"Anime Ratings\")\nplt.subplot(1,2,2)\ndf['user_rating'].hist(bins=50)\nplt.title(\"User Ratings\")","bb4b8073":"animedf[animedf.user_rating==-1].shape","27a8c61a":"animedf.user_rating.replace({-1:np.nan},inplace=True)\nanimedf.dropna(inplace=True)\nanimedf.isnull().sum()","4666f8e8":"plt.figure(figsize = (20, 5))\nplt.subplot(1,2,1)\nanimedf['rating'].hist(bins=50)\nplt.title(\"Anime Ratings\")\nplt.subplot(1,2,2)\nanimedf['user_rating'].hist(bins=50)\nplt.title(\"User Ratings\")","ec2953ef":"animedf['user_id'].value_counts()","37ede34c":"animedf","2d9f4056":"counts = animedf['totalRatingCount'].value_counts()\nanimedf = animedf[~(animedf['totalRatingCount']  <=200)]\ncounts2 = animedf['rating'].value_counts()\nanimedf = animedf[~(animedf['rating']  <=7)]\nanimedf.shape\nanimedf","fce9bcdb":"animedf2=animedf[['anime_id','user_id','user_rating','anime_title','type','episodes','genre','rating','totalRatingCount']]\nanimedf2.drop_duplicates(subset=['anime_id'],inplace=True)\nanimedf2.reset_index(drop=True,inplace=True)","c0b65fb0":"animedf2","bbe531a5":"plt.figure(figsize = (20, 5))\nplt.subplot(1,2,1)\nanimedf2['rating'].hist(bins=50)\nplt.title(\"Anime Ratings\")\nplt.subplot(1,2,2)\nanimedf2['user_rating'].hist(bins=25)\nplt.title(\"User Ratings\")","d007f432":"from scipy.sparse import csr_matrix\npivot=animedf.pivot_table(index='anime_title',columns='user_id',values='user_rating').fillna(0)\npivot.head()\n#Creating a sparse matrix\nanime_matrix = csr_matrix(pivot.values)\n\nfrom sklearn.neighbors import NearestNeighbors\n\n#Fitting the model\nmodel_knn = NearestNeighbors(metric = 'cosine')\nmodel_knn.fit(anime_matrix)","9fad5f2a":"names = []\nquery_index = np.random.choice(pivot.shape[0])\nprint(query_index)\ndistances, indices = model_knn.kneighbors(pivot.iloc[query_index,:].values.reshape(1, -1), n_neighbors = 101)\nfor i in range(0, len(distances.flatten())):\n    if i == 0:\n        print('Recommendations for {0}:\\n'.format(pivot.index[query_index]))\n    else:\n        print('{0}: {1}, with distance of {2}:'.format(i, pivot.index[indices.flatten()[i]], distances.flatten()[i]))\n        names.append(pivot.index[indices.flatten()[i]])","9649f8ae":"names;","4ce97f6c":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 3), \n                      stop_words = 'english')\n\n# Filling NaNs with empty string\nanimedf2['genre'] = animedf2['genre'].fillna('')\ngenres_str = animedf2['genre'].str.split(',').astype(str)\ntfv_matrix = tfv.fit_transform(genres_str)","5a24ca63":"from sklearn.metrics.pairwise import sigmoid_kernel\n\nsig = sigmoid_kernel(tfv_matrix, tfv_matrix)\n\n# indices of anime title\nindices = pd.Series(animedf2.index, index=animedf2['anime_title']).drop_duplicates()","0162e713":"\ndef recommend(title, sig=sig):\n    names2 = []\n    # Get the index corresponding to original_title\n    idx = indices[title]\n\n    # Get the pairwsie similarity scores \n    sig_scores = list(enumerate(sig[idx]))\n\n    # Sort the shows \n    sig_scores = sorted(sig_scores, key=lambda x: x[1], reverse=True)\n\n    # Scores of the 10 most similar shows\n    sig_scores = sig_scores[1:101]\n\n    # anime indices\n    anime_indices = [i[0] for i in sig_scores]\n    names2.append(animedf2['anime_title'].iloc[anime_indices].values)\n    # Top 10 most similar shows\n    return pd.DataFrame({'Anime name': animedf2['anime_title'].iloc[anime_indices].values,\n                         'Type': animedf2['type'].iloc[anime_indices].values,\n                             'Episodes': animedf2['episodes'].iloc[anime_indices].values,\n                                 'Rating': animedf2['rating'].iloc[anime_indices].values})","bb607665":"recommend('Plastic Memories')\n","d7fb5bfc":"# match = []\n# for i in range(0,100):\n#     recommend(format(pivot.index[query_index]))\n#     names2;\n#     lst = []\n#     for sublist in names2:\n#         for item in sublist:\n#             lst.append(item)\n#             sorted_list1 = sorted(names, key=str.lower)\n#             sorted_list2 = sorted(lst, key=str.lower)\n#             lst3 = set(sorted_list1) & set(sorted_list2)\n#             match.append(len(lst3))\n    ","afa043d1":"match","bd4c3638":"# names2\n# lst = []\n# for sublist in names2:\n#     for item in sublist:\n#         lst.append(item)","14bd807d":"lst;","afb453b7":"sorted_list1 = sorted(names, key=str.lower)\n# sorted_list1","6c23895c":"sorted_list2 = sorted(lst, key=str.lower)\n# sorted_list2","8a6d0468":"lst3 = set(sorted_list1) & set(sorted_list2)\nlst3","266881b9":"len(lst3)","97097256":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 3), \n                      stop_words = 'english')\n\n# Filling NaNs with empty string\nanimedf2['genre'] = animedf2['genre'].fillna('')\ngenres_str = animedf2['genre'].str.split(',').astype(str)\ntfv_matrix = tfv.fit_transform(genres_str)\n\nfrom sklearn.metrics.pairwise import sigmoid_kernel\n\nsig = sigmoid_kernel(tfv_matrix, tfv_matrix)\n\n# indices of anime title\nindices = pd.Series(animedf2.index, index=animedf2['anime_title']).drop_duplicates()","872712b9":"def recommend(title, sig=sig):\n#     names2 = []\n    # Get the index corresponding to original_title\n    idx = indices[title]\n\n    # Get the pairwsie similarity scores \n    sig_scores = list(enumerate(sig[idx]))\n\n    # Sort the shows \n    sig_scores = sorted(sig_scores, key=lambda x: x[1], reverse=True)\n\n    # Scores of the 10 most similar shows\n    sig_scores = sig_scores[1:100]\n\n    # anime indices\n    anime_indices = [i[0] for i in sig_scores]\n    names2.append(animedf2['anime_title'].iloc[anime_indices].values)\n    # Top 10 most similar shows\n    return pd.DataFrame({'Anime name': animedf2['anime_title'].iloc[anime_indices].values,\n                         'Type': animedf2['type'].iloc[anime_indices].values,\n                             'Episodes': animedf2['episodes'].iloc[anime_indices].values,\n                                 'Rating': animedf2['rating'].iloc[anime_indices].values})\n\n\nfor i in range(0,100):\n    names = []\n    names2 = []\n    lst = []\n    query_index = np.random.choice(pivot.shape[0])\n#     print(query_index);\n    distances, indices = model_knn.kneighbors(pivot.iloc[query_index,:].values.reshape(1, -1), n_neighbors = 101)\n    for i in range(0, len(distances.flatten())):\n        if i == 0:\n            format(pivot.index[query_index]);\n        else:\n            names.append(pivot.index[indices.flatten()[i]])\nfor i in range(0,100):\n    recommend(format(pivot.index[query_index]))\n    names2;\n    lst = []\n    for sublist in names2:\n        for item in sublist:\n            lst.append(item)\n","50e0174f":"for i in range(0,100):\n    recommend(format(pivot.index[query_index]))\n    names2;\n    lst = []\n    for sublist in names2:\n        for item in sublist:\n            lst.append(item)\n            lst3 = set(sorted_list1) & set(sorted_list2)","461768bb":"Dropping null values since it's a very little amount and won't affect our data","64b95b62":"Also using a pivot table to help in finding cosine similarities","f25114ae":"Sigmoid function was used to represent which anime to recommend. A recommended anime would get assigned a value of 1, and a value of 0 would be assigned for non-recommended anime.","3645e6dd":"From the graph, we can see that there is a lot of -1 ratings. I will get rid of those.","f1d00e4b":"Cleaning anime titles using regular expressions, getting rid of some symbols","55dc30bb":"Which shows match between the two list recommendations","a5143561":"Some users have only rated one time. We should filter it so that we get more meaningful data. Let's only include those who have rated at least 500 shows.","743e1b7f":"Merging our totalRatingCount into our main dataset so that we can reduce our dataset more by a minimum totalRatingCount.","821ef170":"names2 came out to be a list of lists, so I create one list for it.","d80a54ad":"We have a better visualization of our data. The two graphs correlate with each other which is a good sign.","1d44209b":"We can see that we have 1 million values, but our index still goes to 7 million. We should also reset our index since because we got rid of data.","466e5c67":"Let's see the top 10 anime based on rating counts, and also making a rating counts column for our dataset later.","93ac3dd0":"anime_id and user_id should not be int. These are id's, not values. Changed to object data type.","7db15191":"Our data became a little bit less normalized but it should still be ok, since we only care for shows with 7 or greater rating","a0359bba":"The KNN apporach was good but was random and limited out use, therefore it could be improved.","04df139e":"Checking for the counts of user ids","e9a4020c":"Using Content Based Filtering","3efe7f19":"Creating a function to grab recommendations from a title based on sigmoid\/similarity scores. Display top 10 recommended shows.","3be1eafa":"Now, let's get rid of low rating counts and set a threshold for minimum number of totalRatingCounts. This is done so that there aren't a lot of shows with a 10\/10 rating where there is only 1 rating.","153692cd":"This cut our dataset a good amount. This should still be alright to work with 1 million ratings, since our data should contain better data.","f6859a0c":"Merging the datasets","c0e15d23":"Importing\/reading files","aa6584b2":"Checking and dropping duplicate values","c53edc9b":"Getting a random anime and finding 10 recommendations for it. Returning 6 neighbors using KNN.","8a0c11d7":"Let's convert them to null values, and drop them since they're useless","61fa7599":"Taking another look at the anime names to make sure they were cleaned up","a4d40e68":"Using Collaborative filtering. Works by searching a large group of people and finding smaller sets with similar taste based on a user. I researched and found the cosine similarity metric would be best because even if the two similar components are far by distance, the oreintation may still be close together.","9dce6b24":"Between the two models, out of 100 recommendations of the same show, I only have 6 show recommendations that are the same between both lists. This turns out to not have very good confidence between the two models. Although coming from someone who has watched a lot of these shows, neither are too off. Meaning the recommendations given by both models are still very good. But there is work to be done to create a more accurate recommendation system.","2c9f040b":"Our final dataset to work with.","8c05fd67":"I used a TFIDF vectorizer. TF is term frequency, and IDF is Inverse Document Frequency. This was recommended to use when dealing with frequency of words, or in my case anime titles. The term frequency is the raw instance a word would appear in a document. The inverse document frequency measures how common or rare a word is in the entire dataset. It is measured from 0 being the closest to 1 being the farthest. The math behind this is taking the total number of documents and dividing it by the number of documents that contain that word. More can be learned here: https:\/\/monkeylearn.com\/blog\/what-is-tf-idf\/"}}