{"cell_type":{"3791e543":"code","158a0234":"code","ac69a4c1":"code","24e875e8":"code","907feb39":"code","91444edb":"code","43b078b8":"code","344b355d":"code","42b63e19":"code","272e75dc":"code","2a6d3b48":"code","17c4b439":"code","fa6cc1ad":"code","d407e638":"code","0992aea6":"code","d56a49cb":"code","9115d593":"code","86a28e4b":"code","73a59eb8":"code","0740a11a":"code","aec0d47f":"code","b199ce90":"code","420c7ec4":"code","a8b81872":"code","fb7d317b":"code","fbe3b0b5":"code","55993371":"code","67492240":"code","f53508bb":"code","753baf28":"code","842c8ae7":"code","088a8a42":"code","39a0b119":"code","0f0f12cb":"code","9177bdb6":"code","e1fcfb50":"code","48c54c2b":"code","73990ec8":"code","62ec2f85":"code","02de7f3d":"markdown","646e6d06":"markdown","9bf708d9":"markdown","ed8f7fc8":"markdown","85023359":"markdown","f9c6ae94":"markdown","cf7ca0a6":"markdown","917f5b19":"markdown","73b98779":"markdown","c1fcce13":"markdown","104cd0de":"markdown","0f1336ee":"markdown","811a1fd2":"markdown","d98d7436":"markdown","3e962c00":"markdown","9080b921":"markdown","90380bdb":"markdown","de982816":"markdown","3cbf4b31":"markdown","60c43d53":"markdown","5f33dec4":"markdown","dd62c390":"markdown","a12a83c9":"markdown","b1a378ee":"markdown","92ccd171":"markdown","fc791bb1":"markdown","3a8d8e7c":"markdown","fcb37f87":"markdown","7b1b15c6":"markdown"},"source":{"3791e543":"# book_train.parquet\u306e\u30c7\u30fc\u30bf\u3092\u78ba\u8a8d\nimport pandas as pd\nbook_example = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0')\nbook_example.head()","158a0234":"# trade_train.parquet\u306e\u30c7\u30fc\u30bf\u3092\u78ba\u8a8d\nimport pandas as pd\ntrade_example = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id=0')\ntrade_example.head()","ac69a4c1":"# \u597d\u304d\u306aHTML\u3092\u8aad\u307f\u8fbc\u3081\u308b\u3088\u3046\u306b\u306a\u308b\u3084\u3064\uff08\u753b\u50cf\u633f\u5165\u306a\u3069\u306b\u4f7f\u3046\u306e\u304b\uff1f\uff09\nfrom IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\n# \u306a\u305c\u304b2\u56deimport\u3057\u3066\u305f\u306e\u3067\u7121\u8996\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# \u5f15\u6570\u306b\u6307\u5b9a\u3055\u308c\u305f\u30d1\u30bf\u30fc\u30f3\u306b\u30de\u30c3\u30c1\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u540d\u3092\u53d6\u5f97\u3057\u3066\u304f\u308c\u308b\u30e2\u30b8\u30e5\u30fc\u30eb\n# \u4eca\u56de\u306f\u4f7f\u3063\u3066\u3044\u306a\u304b\u3063\u305f\nimport glob\n\n# os\u306b\u4f9d\u5b58\u3057\u3066\u3044\u308b\u6a5f\u80fd\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\n# \u4f8b\u3048\u3070os.walk()\u3067\u30d5\u30a1\u30a4\u30eb\u3084\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u4e00\u89a7\u3092\u53d6\u5f97\u3067\u304d\u308b\n# os.system(\"ls\")\u3067Unix\u30b3\u30de\u30f3\u30c9\u540c\u69d8\u306e\u7d50\u679c\u304c\u5f97\u3089\u308c\u308b\n# \u4eca\u56de\u306f\u4f7f\u3063\u3066\u3044\u306a\u304b\u3063\u305f\nimport os\n\n# \u30ac\u30d9\u30fc\u30b8\u30b3\u30ec\u30af\u30b7\u30e7\u30f3\uff08\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u5b9f\u884c\u4e2d\u306b\u5fc5\u8981\u306a\u30e1\u30e2\u30ea\u9818\u57df\u3092\u52d5\u7684\u306b\u78ba\u4fdd\u3059\u308b\u304c\u3001\u4e0d\u8981\u306b\u306a\u3063\u305f\u30e1\u30e2\u30ea\u9818\u57df\u3092\u81ea\u52d5\u7684\u306b\u89e3\u653e\u3059\u308b\u6a5f\u80fd\u3002\uff09\nimport gc\n\n# \u4e26\u5217\u5316\n# multiproccessing\u3068\u3044\u3046\u30e2\u30b8\u30e5\u30fc\u30eb\u3082\u4ed6\u306b\u3042\u308b\u3063\u307d\u3044\u304c\u4eca\u56de\u306fjoblib\n# CPU\u3092\u8907\u6570\u4f7f\u3063\u3066\u304f\u308c\u308b\u3088\u3046\u306b\u306a\u308b\u3089\u3057\u3044\u3002\u3053\u308c\u304c\u3042\u308b\u306e\u3068\u7121\u3044\u306e\u3068\u3067\u306f\u51e6\u7406\u306e\u6642\u9593\u304c\u5168\u7136\u9055\u3046\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler #\u6b63\u898f\u5316\uff08\u6700\u5c0f0,\u6700\u59271\u3068\u306a\u308b\u3088\u3046\u306b\u5909\u63db\uff09\nfrom sklearn.preprocessing import QuantileTransformer #\u5206\u4f4d\u306b\u3088\u308b\u5909\u63db\nfrom sklearn.metrics import r2_score #\u6c7a\u5b9a\u4fc2\u6570\u3092\u8a08\u7b97\u3059\u308b\u7528\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy.matlib\n\n# matplotlib\u306f\u3067\u304d\u308b\u3053\u3068\u304c\u591a\u304f\u3066\u3044\u3044\u3051\u3069\u3001\u6271\u3046\u306e\u304c\u8907\u96d1\u3067\u9762\u5012\n# seaborn\u306fmatplotlib\u3092\u30d9\u30fc\u30b9\u306b\u3057\u3066\u3044\u308b\u3051\u3069\u3001\u3082\u3063\u3068\u898b\u3084\u3059\u304f\u3066\u7dba\u9e97\u3067\u3001\u7c21\u5358\u306b\u304b\u3051\u308b\u304b\u3089\u6700\u9ad8\u3089\u3057\u3044\n\n# \u8a2d\u5b9a\u3057\u305f\u3082\u306e\u306e\u4eca\u56de\u306f\u4f7f\u3063\u3066\u3044\u306a\u304b\u3063\u305f\npath_submissions = '\/'\n\n# submission\u306e\u30ab\u30e9\u30e0\u540d\u306b\u5408\u308f\u305b\u3066\ntarget_name = 'target'\n# \u7a7a\u306e\u8f9e\u66f8\u578b\u3092\u7528\u610f\uff08\u304a\u305d\u3089\u304f\u63d0\u51fa\u7528\uff09\nscores_folds = {}","24e875e8":"# data directory\n# Kaggle\u3092\u3059\u308b\u969b\u306e\u74b0\u5883\u3092\u533a\u5225\u3057\u3066PATH\u3092\u8a2d\u5b9a\u3059\u308b\u30b3\u30fc\u30c9\n# \u4eca\u56de\u306fbook\u3068trade\u306e\u30c7\u30fc\u30bf\u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u5927\u91cf\u306b\u3042\u3063\u3066\u305d\u306e\u4e2d\u306b\u305d\u308c\u305e\u308c\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\u305f\u3081path\u3092\u6700\u521d\u306b\u6307\u5b9a\u3057\u3066\u3057\u307e\u3046\u306e\u304c\u697d\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\n# 1\u756a\u76ee\u306eWAP(\u52a0\u91cd\u5e73\u5747)\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\uff08\u6700\u9ad8\u58f2\u5024\u3068\u305d\u306e\u500b\u6570\u3001\u6700\u4f4e\u8cb7\u5024\u3068\u305d\u306e\u500b\u6570\u3092\u4f7f\u3063\u3066\u8a08\u7b97\uff09\n# \u5f15\u6570\u306bdf\u3092\u8a2d\u5b9a\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# 2\u756a\u76ee\u306eWAP(\u52a0\u91cd\u5e73\u5747)\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\uff082\u756a\u76ee\u306b\u9ad8\u3044\u58f2\u5024\u3068\u305d\u306e\u500b\u6570\u30012\u756a\u76ee\u306b\u4f4e\u3044\u8cb7\u5024\u3068\u305d\u306e\u500b\u6570\u3092\u4f7f\u3063\u3066\u8a08\u7b97\uff09\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# WAP\u3068\u3057\u3066\u3044\u308b\u304cbid\u540c\u58ebask\u540c\u58eb\u3092\u639b\u3051\u5408\u308f\u305b\u3066\u3044\u308b\u305f\u3081\u82e5\u5e72\u610f\u5473\u5408\u3044\u304c\u9055\u3046\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# wap3\u306e2\u756a\u76eever\ndef calc_wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap","907feb39":"# Log return\u306e\u95a2\u6570\u3092\u4f5c\u6210\n# np.log(series).diff() = np.log(series) - np.log(series.shift()) \uff08\u4e00\u500b\u524d\u306e\u5bfe\u6570\u306e\u5dee\u5206\uff09\ndef log_return(series):\n    return np.log(series).diff()","91444edb":"# \u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))","43b078b8":"# \u91cd\u8907\u306a\u304f\u6570\u3048\u305f\u6642\u306e\u500b\u6570\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\n# np.unique(series)\u3067series\u306e\u30e6\u30cb\u30fc\u30af\u306a\u5024\u304c\u4e00\u89a7\u3067\u53d6\u5f97\u3055\u308c\u3001\u305d\u308c\u306elen()\u306a\u306e\u3067\u500b\u6570\u304c\u53d6\u5f97\u3055\u308c\u308b\ndef count_unique(series):\n    return len(np.unique(series))","344b355d":"# \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u305f\u3081\u306e\u95a2\u6570\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # book\u30c7\u30fc\u30bf\u3068trade\u30c7\u30fc\u30bf\u3092\u7d50\u5408\u3059\u308b\u305f\u3081\u306eid\u3068\u306a\u308b\u30ab\u30e9\u30e0\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u305d\u308c\u305e\u308c\u306b\u8ffd\u52a0\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test","42b63e19":"# train\u3068test\u306e\u6982\u8981\u3092\u30c1\u30a7\u30c3\u30af\nread_train_test()","272e75dc":"# \u30aa\u30fc\u30c0\u30fc\u30d6\u30c3\u30af\u306e\u30c7\u30fc\u30bf\u3092\u52a0\u5de5\u3057\u3066\u3044\u304f\n# \u305d\u308c\u305e\u308c\u306e\u9298\u67c4\uff08stock_id\uff09\u3054\u3068\u306b\u524d\u51e6\u7406\u3092\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\u3092\u8a2d\u5b9a\ndef book_preprocessor(file_path):\n    # ApacheParquet\u3068\u306fcsv\u306a\u3069\u306e\u884c\u5fd7\u5411\u306e\u30c7\u30fc\u30bf\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3068\u9055\u3044\u3001\u5217\u5fd7\u5411\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3067\u3001\u5217\u5358\u4f4d\u3067\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u51fa\u3059\u5206\u6790\u7528\u9014\u306b\u5411\u3044\u3066\u308b\n    df = pd.read_parquet(file_path)\n    # WAP(\u52a0\u91cd\u5e73\u5747)\u3092\u8a08\u7b97\u3057\u305f\u4e0a\u3067\u7279\u5fb4\u91cf\u3068\u3057\u3066\u8ffd\u52a0\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    df['wap3'] = calc_wap3(df)\n    df['wap4'] = calc_wap4(df)\n    # Log retuen\u3092\u8a08\u7b97\u3057\u305f\u4e0a\u3067\u7279\u5fb4\u91cf\u3068\u3057\u3066\u8ffd\u52a0\n    # apply\u3067dataframe\u306b\u5bfe\u3057\u3066\u5168\u3066\u306e\u64cd\u4f5c\u3092\u884c\u3046\n    # \u4eca\u56de\u306ftime_id\u3054\u3068\u306e\u305d\u308c\u305e\u308c\u306ewap\u306e\u5217\u306b\u5bfe\u3057\u3066log_return\u3092\u884c\u3046\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n    # WAP1\u3068WAP2\u540c\u58eb\u306e\u5dee\u5206\u306e\u7d76\u5bfe\u5024\u3092\u53d6\u5f97\u3057\u7279\u5fb4\u91cf\u3068\u3057\u3066\u8ffd\u52a0\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # spread\uff08\u6700\u9ad8\u58f2\u5024\u3068\u6700\u4f4e\u8cb7\u5024\u306e\u6bd4\u7387\u3092\u3068\u3063\u3066\u8a08\u7b97\u3055\u308c\u308b\u5024\uff09\u3092\u8a08\u7b97\u3057\u3001\u7279\u5fb4\u91cf\u3068\u3057\u3066\u8ffd\u52a0\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # \u8f9e\u66f8\u578b\u3092\u7528\u610f\u3059\u308b\n    # \u30ab\u30e9\u30e0\u540d\u3068\u884c\u3046\u64cd\u4f5c\u3092\u30bb\u30c3\u30c8\n    create_feature_dict = {\n        'wap1': [np.sum, np.std],\n        'wap2': [np.sum, np.std],\n        'wap3': [np.sum, np.std],\n        'wap4': [np.sum, np.std],\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n        'wap_balance': [np.sum, np.max],\n        'price_spread':[np.sum, np.max],\n        'price_spread2':[np.sum, np.max],\n        'bid_spread':[np.sum, np.max],\n        'ask_spread':[np.sum, np.max],\n        'total_volume':[np.sum, np.max],\n        'volume_imbalance':[np.sum, np.max],\n        \"bid_ask_spread\":[np.sum,  np.max],\n    }\n    create_feature_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n    }\n    \n    # DataFrame\u306e\u7d44\u307f\u76f4\u3057\n    ## \u5f15\u6570\u306eadd_suffix\u306f\u4e0b\u8a18\uff08*\uff09\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # df\u306e\u4e2d\u3067\u3082df\u306e'seconds_in_bucket'\u30ab\u30e9\u30e0\u306b\u5165\u3063\u3066\u3044\u308b\u5024\u304cseconds_in_bucket\u3088\u308a\u5927\u304d\u3044\u6642\n        # \u8f9e\u66f8\u578b\u3068\u4f5c\u3063\u3066agg\u3068reset_index()\u3067\u65b0\u305f\u306bDataFrame\u3092\u4f5c\u308b\u30bb\u30c3\u30c8\u306e\u3088\u3046\u306a\u5f79\u5272\uff1f\n        # \u5f15\u6570\u306b\u4f55\u3082\u6307\u5b9a\u305b\u305areset_index()\u3092\u4f7f\u3046\u3068\u3001\u9023\u756a\u304c\u65b0\u305f\u306aindex\u3068\u306a\u308a\u3001\u5143\u306eindex\u304c\u65b0\u305f\u306a\u5217\u3068\u3057\u3066\u6b8b\u308b\n        # \u3053\u308c\u3092\u3084\u308b\u3053\u3068\u3067\u8f9e\u66f8\u578b\u3067\u8a2d\u5b9a\u3057\u305f\u6700\u5927\u5024\u3084\u5e73\u5747\u5024\u304c\u8a08\u7b97\u3055\u308c\u305f\u3082\u306e\u306eDataFrame\u3068\u306a\u308b\u305f\u3081\u6b20\u640d\u5024\u304c\u7121\u8996\u3055\u308c\u308b\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # \u63a5\u5c3e\u8f9e\u3092\u7d50\u5408\u3059\u308b\u5217\u306e\u540d\u524d\u3092\u5909\u66f4\u3059\u308b\n        # wap1 sum \u2192 wap1_sum\u306b\u3057\u305f\u3044\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # *\n        # add_suffix\uff1a\u30ab\u30e9\u30e0\u306e\u30b1\u30c4\u306b\u300c_\u79d2\u6570\u300d\u3092\u3064\u3051\u308b\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # seconds_in_bucket\u3092100\u79d2\u3054\u3068\u306bcreate_feature_dict\u3092\u8a08\u7b97\u3057\u3066DataFrame\u3068\u3059\u308b\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # DataFrame\u306e\u7d50\u5408\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # \u4e0d\u8981\u306b\u306a\u3063\u305f\u7d50\u5408\u7528\u306e\u30ab\u30e9\u30e0\u3092\u524a\u9664\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n    \n    \n    # \u306e\u3061\u306btrade\u30c7\u30fc\u30bf\u3068\u7d50\u5408\u3059\u308b\u7528\u306erow_id\u3068\u3044\u3046\u30ab\u30e9\u30e0\u3092\u4f5c\u6210\n    # stock_id=0\u3068\u306a\u3063\u3066\u3044\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e0\u306e\u90e8\u5206\u3060\u3051\u3092\u53d6\u308a\u51fa\u3057\u3001stock_id\u306b\u4ee3\u5165\u3059\u308b\n    stock_id = file_path.split('=')[1]\n    # time_id\u3068stock_id\u304c\u7e4b\u304c\u3063\u305f\u300c0-32\u300d\u306e\u3088\u3046\u306a\u5f62\u306erow_id\u3092\u4f5c\u6210\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature","2a6d3b48":"# \u30c8\u30ec\u30fc\u30c9\u30c7\u30fc\u30bf\n\n# \u30aa\u30fc\u30c0\u30fc\u30d6\u30c3\u30af\u3068\u540c\u3058\u3088\u3046\u306a\u64cd\u4f5c\u3092\u884c\u3046\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # df\u3092time_id\u3054\u3068\u306b\u30b0\u30eb\u30fc\u30d7\u5316\u3057\u3066\u305d\u306e\u3046\u3061\u306e\u300cprice\u300d\u30ab\u30e9\u30e0\u306blog_return\u3092\u9069\u7528\u3059\u308b\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    #  \u53d6\u5f15\u3055\u308c\u305f\u5408\u8a08\u91d1\u984d\u3092\u53d6\u5f97\u3057\u7279\u5fb4\u91cf\u3068\u3057\u3066\u8ffd\u52a0\n    df['amount']=df['price']*df['size']\n    # \u8f9e\u66f8\u578b\u3092\u7528\u610f\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, np.max, np.min],\n        'order_count':[np.sum,np.max],\n        'amount':[np.sum,np.max,np.min],\n    }\n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.sum],\n    }\n    \n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # book\u3068\u540c\u3058\u51e6\u7406\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # \u30b1\u30c4\u306b\u300c_df_feature\u306e\u30ab\u30e9\u30e0\u540d\u300d\u3092\u3064\u3051\u308b\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # \u30b1\u30c4\u306b\u300c_\u79d2\u6570\u300d\u3068\u3064\u3051\u308b\u305f\u3081\u306e\u81ea\u4f5c\u95a2\u6570 \n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    \n    def tendency(price, vol):\n        # price\u306e\u5dee\u5206\n        df_diff = np.diff(price)\n        # \u5897\u52a0\u7387\u7684\u306a\n        val = (df_diff\/price[1:])*100\n        # \u6307\u5b9a\u3057\u305f\u30dc\u30ea\u30e5\u30fc\u30e0\u306b\u5897\u52a0\u7387\u3092\u304b\u3051\u305f\u3082\u306e\u3092\u5408\u8a08\u3057\u305f\u5024\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    # n_tim_id\u306fdf\u5185\u306etime_id\u304c\u30e6\u30cb\u30fc\u30af\u306a\u3082\u306e\n    for n_time_id in df['time_id'].unique():\n        # \u30e6\u30cb\u30fc\u30af\u306atime_id\u3054\u3068\u306bdf\u306eid\u3092\u53d6\u5f97\uff08time_id\u304c0\u306e\u6642\u306edf\u3092df_id=0\u3068\u3059\u308b\uff09\n        df_id = df[df['time_id'] == n_time_id]\n        # \u305d\u308c\u305e\u308c\u306edf_id\u3054\u3068\uff08\u3064\u307e\u308atime_id\u3054\u3068\uff09\u306e\u5024\u6bb5\u3068\u682a\u5f0f\u6570\u3092\u304b\u3051\u305f\u5408\u8a08\u53d6\u5f15\u91d1\u984d\n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)\n        # \u5e73\u5747\u91d1\u984d\u3088\u308a\u3082\u5927\u304d\u3044\u91d1\u984d\u304b\u3069\u3046\u304b\n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # \u6a19\u6e96\u504f\u5dee\u306e\u7d76\u5bfe\u5024\u306e\u4e2d\u592e\u5024\uff1f\u7684\u306a\u30a4\u30e1\u30fc\u30b8\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))\n        # \u305d\u308c\u305e\u308c\u306eprice\u306e2\u4e57\u306e\u5e73\u5747\n        energy = np.mean(df_id['price'].values**2)\n        # \u56db\u5206\u4f4d\u70b9\u306e\u5f15\u304d\u7b97\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # \u682a\u5f0f\u6570\u3067\u3082\u540c\u3058\u64cd\u4f5c\u3092\u884c\u3046\n        \n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        # \u5148\u307b\u3069\u4f5c\u3063\u305f\u7a7a\u306e\u914d\u5217\u306b\u7528\u610f\u3057\u305fDataFrame\u3092\u8ffd\u52a0\n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    # DataFrame\u5316\n    df_lr = pd.DataFrame(lis)\n        \n   # df_feature\u306b\u7d50\u5408\n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # 100\u79d2\u3054\u3068\u306b\u5207\u3063\u305f\u305d\u308c\u305e\u308c\u306edf\u3082\u7d50\u5408\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # merge\u7528\u306e\u30ab\u30e9\u30e0\u3092\u524a\u9664\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n    \n    # add_suffix\u306e\u5148\u982d\u306b\u3064\u3051\u308bver\n    df_feature = df_feature.add_prefix('trade_')\n    # \u3053\u3061\u3089\u3082row_id\u3092\u53d6\u5f97\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature","17c4b439":"# stock_id\u3068time_id\u3054\u3068\u306e\u96c6\u8a08\u3092\u884c\u3046\u305f\u3081\u306e\u95a2\u6570\u3092\u8a2d\u5b9a\ndef get_time_stock(df):\n    # \u30ab\u30e9\u30e0\u540d\u3092\u4e00\u6b21\u5143\u914d\u5217\u3067\u6307\u5b9a\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n    # stock_id\u3054\u3068\u306b\u96c6\u8a08\uff08\u5e73\u5747\u3001\u6a19\u6e96\u504f\u5dee\u3001\u6700\u5927\u5024\u3001\u6700\u5c0f\u5024\u3092\u53d6\u5f97\uff09\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # stock\u306e\u65b9\u3060\u3068\u308f\u304b\u308b\u3088\u3046\u306b\u30b1\u30c4\u306b\u300c_stock\u300d\u3068\u8ffd\u52a0\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # time_id\u3054\u3068\u306b\u96c6\u8a08\uff08\u5e73\u5747\u3001\u6a19\u6e96\u504f\u5dee\u3001\u6700\u5927\u5024\u3001\u6700\u5c0f\u5024\u3092\u53d6\u5f97\uff09\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # time\u306e\u65b9\u3060\u3068\u308f\u304b\u308b\u3088\u3046\u306b\u30b1\u30c4\u306b\u300c_time\u300d\u3068\u8ffd\u52a0\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # df\u306bstock_id\u3054\u3068\u306b\u96c6\u8a08\u3057\u305f\u3082\u306e\u3068time_id\u3054\u3068\u306b\u96c6\u8a08\u3057\u305f\u3082\u306e\u3092\u7d50\u5408\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df","fa6cc1ad":"# \u4e26\u5217\u5316\n# \u4eca\u307e\u3067\u306b\u4f5c\u3063\u305f\u524d\u51e6\u7406\u7528\u306e\u95a2\u6570\u3092\u4e26\u5217\u5316\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parallel for loop\n    def for_joblib(stock_id):\n        if is_train:\n            # is_train=True\u3068\u6307\u5b9a\u3055\u308c\u305f\u5834\u5408train\u30c7\u30fc\u30bf\u306e\u65b9\u3078\u306epath\u3092\u4f5c\u6210\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        else:\n            # is_train=False\u3064\u307e\u308atest\u30c7\u30fc\u30bf\u306a\u3089\u3053\u3061\u3089\u3078\u306epath\u3092\u4f5c\u6210\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # book\u3068trade\u3092\u524d\u51e6\u7406\u3057\u305f\u4e0a\u3067\u7d50\u5408\u3059\u308b\n        # \u4e9b\u7d30\u306a\u5909\u5316\u3082\u5168\u3066\u30ec\u30b3\u30fc\u30c9\u3068\u3057\u3066\u8a18\u9332\u3055\u308c\u3066\u3044\u308bbook\u306e\u65b9\u306brow_id\u3092\u57fa\u6e96\u306btrade\u3092\u5916\u90e8\u7d50\u5408\u3059\u308b\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # \u7d50\u5408\u3057\u305fDataFrame\u3092\u8fd4\u3059\n        return df_tmp\n    \n    # \u4e26\u5217API\u3092\u4f7f\u7528\u3057\u3066for_joblib\u95a2\u6570\u3092\u547c\u3073\u51fa\u3059\n    # n_jobs:\u6700\u5927\u540c\u6642\u5b9f\u884c\u30b8\u30e7\u30d6\u6570\u3002-1\u3068\u3059\u308b\u3068\u5168\u3066\u306eCPU\u304c\u4f7f\u7528\u3055\u308c\u308b\n    # verbose:\u30ed\u30b0\u306e\u51fa\u529b\u30ec\u30d9\u30eb\uff08\u5197\u9577\u6027\uff09\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u4f55\u3082\u51fa\u529b\u3055\u308c\u306a\u3044\u3002\u5024\u3092\u5927\u304d\u304f\u3059\u308b\u3068\u51fa\u529b\u30ec\u30d9\u30eb\u304c\u4e0a\u304c\u308b\uff08\u5197\u9577\u6027\u304c\u5897\u3059\uff09\u300210\u3088\u308a\u5927\u304d\u3044\u3068\u3059\u3079\u3066\u306e\u30ed\u30b0\u304c\u51fa\u529b\u3055\u308c\u300150\u4ee5\u4e0a\u3060\u3068stdout\uff08\u6a19\u6e96\u51fa\u529b\uff09\u306b\u51fa\u529b\u3055\u308c\u308b\u3002\n    # delayed(<\u5b9f\u884c\u3059\u308b\u95a2\u6570>)(<\u95a2\u6570\u3078\u306e\u5f15\u6570>) for \u5909\u6570\u540d in \u30a4\u30c6\u30e9\u30d6\u30eb\n    # \u5b9f\u884c\u3059\u308b\u95a2\u6570\uff08book\u3068trade\u3092\u524d\u51e6\u7406\u3057\u3066\u7d50\u5408\uff09\u3092stock_id\u3054\u3068\u306b\u884c\u3046\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Parallel\u304b\u3089\u8fd4\u3055\u308c\u308b\u3059\u3079\u3066\u306eDataFrame\u3092\u7d50\u5408\n    # ignore_index=True\u3067index\u304cconcat\u524d\u306eindex\u3092\u7121\u8996\u3057\u3066\u9023\u756a\u3067\u632f\u3089\u308c\u308b\n    df = pd.concat(df, ignore_index = True)\n    return df","d407e638":"# RMSPE(\u5e73\u5747\u5e73\u65b9\u4e8c\u4e57\u8aa4\u5dee\u7387)\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))","0992aea6":"# RMSPE\u3067\u65e9\u671f\u505c\u6b62\u3059\u308b\u95a2\u6570\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","d56a49cb":"# train\u30c7\u30fc\u30bf\u3068test\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\ntrain, test = read_train_test()\n\n# \u30e6\u30cb\u30fc\u30af\u306astock\u306eid\u3092\u53d6\u5f97\u3059\u308b\ntrain_stock_ids = train['stock_id'].unique()\n# \u4e26\u5217\u51e6\u7406\u306b\u4f7f\u3046\u305f\u3081\u306b\u524d\u51e6\u7406\u3092\u884c\u3046\n# \u4eca\u56de\u306f\u4e26\u5217\u5316\u3057\u3066book\u3068trade\u3092stock_id\u3054\u3068\u306b\u51e6\u7406\u3092\u3057\u305f\u4e0a\u3067\u7d50\u5408\u3059\u308b\ntrain_ = preprocessor(train_stock_ids, is_train = True)\n# row_id\u3092\u57fa\u6e96\u306bleft join\u3059\u308b\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3082\u540c\u3058\u51e6\u7406\u3092\u884c\u3046\ntest_stock_ids = test['stock_id'].unique()\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# time_id\u3068stock_id\u304c\u4e00\u7dd2\u306b\u306a\u3063\u305f\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\n# get_time_stock\u306fstock_id\u3068time_id\u3054\u3068\u306e\u96c6\u8a08\u3092\u884c\u3046\u95a2\u6570\ntrain = get_time_stock(train)\ntest = get_time_stock(test)","9115d593":"# \u6642\u5b9a\u6570tau\n# \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u5fc5\u8981\uff1f\n# seconds_in_bucket_count\u304c\u3042\u3063\u3066\u63a5\u982d\u8a9e\u306b\u300ctrade_\u300d\u3068\u300c_unique\u300d\u3092\u3064\u3051\u305f\u3082\u306e\ntrain['size_tau'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique'] )\ntest['size_tau'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique'] )\n#train['size_tau_450'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_450'] )\n#test['size_tau_450'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_450'] )\ntrain['size_tau_400'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_400'] )\ntest['size_tau_400'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_400'] )\ntrain['size_tau_300'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_300'] )\ntest['size_tau_300'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_300'] )\n#train['size_tau_150'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_150'] )\n#test['size_tau_150'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_150'] )\ntrain['size_tau_200'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_200'] )\ntest['size_tau_200'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_200'] )","86a28e4b":"train['size_tau2'] = np.sqrt( 1\/ train['trade_order_count_sum'] )\ntest['size_tau2'] = np.sqrt( 1\/ test['trade_order_count_sum'] )\n#train['size_tau2_450'] = np.sqrt( 0.25\/ train['trade_order_count_sum'] )\n#test['size_tau2_450'] = np.sqrt( 0.25\/ test['trade_order_count_sum'] )\ntrain['size_tau2_400'] = np.sqrt( 0.33\/ train['trade_order_count_sum'] )\ntest['size_tau2_400'] = np.sqrt( 0.33\/ test['trade_order_count_sum'] )\ntrain['size_tau2_300'] = np.sqrt( 0.5\/ train['trade_order_count_sum'] )\ntest['size_tau2_300'] = np.sqrt( 0.5\/ test['trade_order_count_sum'] )\n#train['size_tau2_150'] = np.sqrt( 0.75\/ train['trade_order_count_sum'] )\n#test['size_tau2_150'] = np.sqrt( 0.75\/ test['trade_order_count_sum'] )\ntrain['size_tau2_200'] = np.sqrt( 0.66\/ train['trade_order_count_sum'] )\ntest['size_tau2_200'] = np.sqrt( 0.66\/ test['trade_order_count_sum'] )\n\n# tau\u306e\u3061\u3087\u3063\u3068\u3057\u305f\u5897\u5206\uff08\u30c7\u30eb\u30bf\uff09\ntrain['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\ntest['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']","73a59eb8":"# \u4ecatrain\u304c\u6301\u3063\u3066\u3044\u308b\u30c7\u30fc\u30bf\u306e\u30ab\u30e9\u30e0\u540d\u3092\u914d\u5217\u306b\u3057\u305f\u3082\u306e\n# not in\u306a\u306e\u3067{\"stock_id\", \"time_id\", \"target\", \"row_id\"}\u306f\u542b\u307e\u306a\u3044\ncolNames = [col for col in list(train.columns)\n            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n# \u4f55\u500b\u306e\u30ab\u30e9\u30e0\u304c\u3042\u308b\u306e\u304b\u78ba\u8a8d\nlen(colNames)","0740a11a":"# k\u8fd1\u508d\u6cd5\nfrom sklearn.cluster import KMeans\n# making agg features\n\ntrain_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n# index\u3068columns\u3068values\u3068values\u3092\u6307\u5b9a\u3057\u3066\u30c6\u30fc\u30d6\u30eb\u3092\u30d4\u30dc\u30c3\u30c8\uff08\u518d\u5f62\u6210\uff09\n# \u4eca\u56de\u306f\u7e26\u304ctime_id, \u6a2a\u304cstock_id, \u30af\u30ed\u30b9\u3059\u308b\u30bb\u30eb\u306b\u5165\u308b\u306e\u304ctime_id,stock_id\u304c\u7279\u5b9a\u306e\u5024\u306e\u6642\u306etarget\u306e\u5024\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n# \u78ba\u8a8d\ndisplay(train_p.head())\n\n# \u30d4\u30dc\u30c3\u30c8\u3057\u305ftrain_p\u306e\u5897\u3048\u304b\u305f\u3092\u9298\u67c4\u3054\u3068\u306b\u307e\u3068\u3081\u305f\u3082\u306e\n# \u3069\u306e\u9298\u67c4\u540c\u58eb\u3067\u76f8\u95a2\u304c\u9ad8\u3044\u304b\u3092\u78ba\u8a8d\u3057\u3001\u306e\u3061\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u5206\u6790\u306b\u95a2\u308f\u3063\u3066\u304f\u308b\ncorr = train_p.corr()\n\n# stock_id\u304c\u4e00\u6b21\u5143\u914d\u5217\u3067\u683c\u7d0d\u3055\u308c\u305f\u3082\u306e\n# ids = [0,1,2,3,4,...,111,112]\nids = corr.index\n\n\n# k-means\u6cd5, \u30af\u30e9\u30b9\u30bf\u30fc\u65707, random_state=0(\u540c\u3058\u5024)\n# \u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u306e\u5897\u3048\u65b9\u3067\u9298\u67c4\u306b\u7a2e\u985e\u304c\u3042\u308b\u306e\u3067\u306f\u306a\u3044\u304b\uff1f\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n# \u30af\u30e9\u30b9\u30bf\u30fc\u6570\u304c\u30e9\u30d9\u30eb\u306b\u306a\u3063\u30660~7\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\nprint(kmeans.labels_)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n# append\u3055\u308c\u3066\u3044\u308b\u306e\u304cx-1\u306e\u5024\u3067\u3001x\u304c0\u4ee5\u4e0a\u306e\u6642\u306fkmeans.labels_\u30680~6\u306e\u9593\u3092\u52d5\u304fn\u304c\u7b49\u3057\u3044\u6642\u3001ids+1\u3092\u8fd4\u3059\u3002\n# append\u3055\u308c\u3066\u3044\u308b\u306e\u304cx-1\u306a\u306e\u3067\uff08ids + 1\uff09-1\u306e\u5024\u304cl\uff08\u914d\u5217\uff09\u306b\u683c\u7d0d\u3055\u308cl\u306fstock_id\u306e\u4e00\u6b21\u5143\u914d\u5217\u306b\u306a\u308b\n# (kmeans.labels_ == n)\u304cTrue,False\u3067\u8fd4\u3063\u3066\u304f\u308b\u304c\u3001python\u3067\u306fTrue==1,Fales==0\u3068\u3057\u3066\u6271\u308f\u308c\u308b\uff08\u3060\u304b\u3089\u639b\u3051\u7b97\u304c\u51fa\u6765\u3066\u3044\u305f\uff09\ndisplay(l)\n    \nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean) #mean\u3068\u4e00\u7dd2\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n# \u3053\u3053\u307e\u3067for\u6587\u3002n\u304c0~6\u307e\u3067\uff08\u5272\u308a\u632f\u3089\u308c\u305f\u5168\u30af\u30e9\u30b9\u30bf\u30fc\uff09\u3067time_id\u3054\u3068\u306bstock_id\u3054\u3068\u306b\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u306a\u3069\u306a\u3069\u306e\u5e73\u5747\u3092\u3068\u3063\u305fdataframe\u3092\u4f5c\u6210\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()\ndisplay(mat1.head())","aec0d47f":"mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)\ndisplay(mat2.head())","b199ce90":"# \u30ab\u30e9\u30e0\u540d\u3092\u307e\u3068\u3081\u305f\u914d\u5217\u3092\u7528\u610f\nnnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_sum_0c1',\n     'total_volume_sum_1c1', \n     'total_volume_sum_3c1',\n     'total_volume_sum_4c1', \n     'total_volume_sum_6c1',\n     'trade_size_sum_0c1',\n     'trade_size_sum_1c1', \n     'trade_size_sum_3c1',\n     'trade_size_sum_4c1', \n     'trade_size_sum_6c1',\n     'trade_order_count_sum_0c1',\n     'trade_order_count_sum_1c1',\n     'trade_order_count_sum_3c1',\n     'trade_order_count_sum_4c1',\n     'trade_order_count_sum_6c1',      \n     'price_spread_sum_0c1',\n     'price_spread_sum_1c1',\n     'price_spread_sum_3c1',\n     'price_spread_sum_4c1',\n     'price_spread_sum_6c1',   \n     'bid_spread_sum_0c1',\n     'bid_spread_sum_1c1',\n     'bid_spread_sum_3c1',\n     'bid_spread_sum_4c1',\n     'bid_spread_sum_6c1',       \n     'ask_spread_sum_0c1',\n     'ask_spread_sum_1c1',\n     'ask_spread_sum_3c1',\n     'ask_spread_sum_4c1',\n     'ask_spread_sum_6c1',   \n     'volume_imbalance_sum_0c1',\n     'volume_imbalance_sum_1c1',\n     'volume_imbalance_sum_3c1',\n     'volume_imbalance_sum_4c1',\n     'volume_imbalance_sum_6c1',       \n     'bid_ask_spread_sum_0c1',\n     'bid_ask_spread_sum_1c1',\n     'bid_ask_spread_sum_3c1',\n     'bid_ask_spread_sum_4c1',\n     'bid_ask_spread_sum_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] \ntrain = pd.merge(train,mat1[nnn],how='left',on='time_id')\ntest = pd.merge(test,mat2[nnn],how='left',on='time_id')","420c7ec4":"import gc\ndel mat1,mat2\ngc.collect()","a8b81872":"# \u4ea4\u5dee\u691c\u8a3c\u3092\u3059\u308b\u6642\u306bKfold\u3092\u4f7f\u3046\n# \u30c7\u30fc\u30bf\u3092k\u500b\u306b\u5206\u3051\uff0cn\u500b\u3092\u8a13\u7df4\u7528\uff0ck-n\u500b\u3092\u30c6\u30b9\u30c8\u7528\u3068\u3057\u3066\u4f7f\u3046\uff0e\n# \u5206\u3051\u3089\u308c\u305fn\u500b\u306e\u30c7\u30fc\u30bf\u304c\u30c6\u30b9\u30c8\u7528\u3068\u3057\u3066\u5fc5\u305a1\u56de\u4f7f\u308f\u308c\u308b\u3088\u3046\u306bn\u56de\u691c\u5b9a\u3059\u308b\uff0e\nfrom sklearn.model_selection import KFold\n# lightgbm\nimport lightgbm as lgb\n\nseed0=2021\nparams0 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':100,\n    'min_data_in_leaf':500,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':seed0,\n    'feature_fraction_seed': seed0,\n    'bagging_seed': seed0,\n    'drop_seed': seed0,\n    'data_random_seed': seed0,\n    'n_jobs':-1,\n    'verbose': -1}\nseed1=42\nparams1 = {\n        'learning_rate': 0.1,        \n        'lambda_l1': 2,\n        'lambda_l2': 7,\n        'num_leaves': 800,\n        'min_sum_hessian_in_leaf': 20,\n        'feature_fraction': 0.8,\n        'feature_fraction_bynode': 0.8,\n        'bagging_fraction': 0.9,\n        'bagging_freq': 42,\n        'min_data_in_leaf': 700,\n        'max_depth': 4,\n        'categorical_column':[0],\n        'seed': seed1,\n        'feature_fraction_seed': seed1,\n        'bagging_seed': seed1,\n        'drop_seed': seed1,\n        'data_random_seed': seed1,\n        'objective': 'rmse',\n        'boosting': 'gbdt',\n        'verbosity': -1,\n        'n_jobs':-1,\n    }\n# rmspe\u3067\u65e9\u671f\u505c\u6b62\u3092\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\n# LightGBM\u3092\u5b9f\u88c5\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\ndef train_and_evaluate_lgb(train, test, params):\n    \n    # time_id, target, row_id\u4ee5\u5916\u3092features\u306b\u683c\u7d0d\n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n    y = train['target']\n    # train\u3068\u540c\u3058\u5f62\u30670\u304c\u5165\u3063\u305f\u914d\u5217\u3092\u4f5c\u6210\n    oof_predictions = np.zeros(train.shape[0])\n    # test\u3068\u540c\u3058\u5f62\u30670\u304c\u5165\u3063\u305f\u914d\u5217\u3092\u4f5c\u6210\n    test_predictions = np.zeros(test.shape[0])\n    # kfold\n    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n    # \u5404fold\u3067\u7e70\u308a\u8fd4\u3059\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # rmspe\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n        model = lgb.train(params = params,\n                          num_boost_round=1000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=50,\n                          feval = feval_rmspe)\n        # \u5404fold\u306b\u53d6\u5f97\u3057\u305f\u4e88\u6e2c\u5024\u3092\u8ffd\u52a0\u3059\u308b\n        oof_predictions[val_ind] = model.predict(x_val[features])\n        # \u4f5c\u6210\u3057\u305f\u30e2\u30c7\u30eb\u3067test\u30c7\u30fc\u30bf\u3092\u4e88\u6e2c\u3059\u308b\n        test_predictions += model.predict(test[features]) \/ 5\n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    lgb.plot_importance(model,max_num_features=20)\n    # test\u306e\u4e88\u6e2c\u5024\u3092\u8fd4\u308a\u5024\u306b\u8a2d\u5b9a\n    return test_predictions\n\n# \u5b9f\u969b\u306b\u5b9f\u884c\u3057\u3066\u307f\u308b\npredictions_lgb= train_and_evaluate_lgb(train, test,params0)\ntest['target'] = predictions_lgb\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","fb7d317b":"train.shape[1]","fbe3b0b5":"!pip install keras==2.3.1","55993371":"from numpy.random import seed\nseed(42)\n\n# \u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u5411\u3051\u30e9\u30a4\u30d6\u30e9\u30eatensorflow\nimport tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow import keras\nimport numpy as np\nfrom keras import backend as K\n\n# rmspe\u3092\u8a08\u7b97\ndef root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)\/ y_true )))\n\n# keras\u3067\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u3092\u4f5c\u6210\u3059\u308b\n# \u3053\u3061\u3089\u306fEarlyStopping\n# \u5b66\u7fd2\u30eb\u30fc\u30d7\u306b\u53ce\u675f\u5224\u5b9a\u3092\u4ed8\u4e0e\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u3001\u76e3\u8996\u3059\u308b\u5024\u3092\u8a2d\u5b9a\u3057\u3066\u3001\u305d\u308c\u304c\u53ce\u675f\u3057\u305f\u3089\u81ea\u52d5\u7684\u306b\u30eb\u30fc\u30d7\u3092\u629c\u3051\u308b\u51e6\u7406\u306b\u306a\u308b\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=20, verbose=0,\n    mode='min',restore_best_weights=True)\n# \u3053\u3061\u3089\u306fReduceLROnPlateau\n# \u8a55\u4fa1\u5024\u306e\u6539\u5584\u304c\u6b62\u307e\u3063\u305f\u6642\u306b\u5b66\u7fd2\u7387\u3092\u6e1b\u3089\u3059\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n    mode='min')","67492240":"# \u518d\u5ea6train\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u3068\u30c6\u30fc\u30d6\u30eb\u306e\u518d\u5f62\u6210\nout_train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\nout_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n\n# \u6b20\u640d\u5024\u3092\u5e73\u5747\u5024\u3067\u88dc\u5b8c\nout_train = out_train.fillna(out_train.mean())\nout_train.head()\n\n# \u6700\u521d\u306e\u5b9f\u884c\u5f8c\u306b\u8aad\u307f\u53d6\u3063\u305f\u30c7\u30fc\u30bf\u3060\u3051\u3092\u8ffd\u52a0\u3059\u308b\u30b3\u30fc\u30c9\n\n# knn\u306b\u57fa\u3065\u3044\u3066data\u3092\u5206\u3051\u308b\nnfolds = 5 # fold\u306e\u6570\nindex = []\ntotDist = []\nvalues = []\n# out_train\u306e\u5024\u3067\u884c\u5217\u3092\u751f\u6210\nmat = out_train.values\n\n# \u6b63\u898f\u5316\nscaler = MinMaxScaler(feature_range=(-1, 1))\nmat = scaler.fit_transform(mat)\n\nnind = int(mat.shape[0]\/nfolds) # fold\u3054\u3068\u306e\u30c7\u30fc\u30bf\u6570\n\n# \u6700\u5f8c\u306e\u5217\u306bindex\u3092\u8ffd\u52a0\nmat = np.c_[mat,np.arange(mat.shape[0])]\n\n# np.random.choice()\u3067\u30b5\u30a4\u30b3\u30ed\u3092\u4f5c\u6210\u3059\u308b\nlineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n\nlineNumber = np.sort(lineNumber)[::-1]\n\n# totDist\u306bmat\u306e\u884c\u6570 - 5\uff08fold\u306e\u6570\uff09\u5206\u306e0\u914d\u5217\u3092\u633f\u5165\nfor n in range(nfolds):\n    totDist.append(np.zeros(mat.shape[0]-nfolds))\n\n# values\u306b\u306findex\u3092\u4fdd\u5b58\nfor n in range(nfolds):\n    values.append([lineNumber[n]])\n\ndisplay(mat)\ndisplay(lineNumber)\ndisplay(totDist)\ndisplay(values)","f53508bb":"s=[]\nfor n in range(nfolds): #range(5)\n    # s\u306bmat\u306e(lineNumber\u306en\u884c\u76ee)\u306e\u914d\u5217\u306e\u8981\u7d20\u3092append\n    s.append(mat[lineNumber[n],:])\n    # s\u306b\u5165\u3063\u305f\u8981\u7d20\u306fmat\u304b\u3089\u306f\u524a\u9664\n    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\n# nind\u306ffold\u3054\u3068\u306e\u30c7\u30fc\u30bf\u6570\uff08int(mat.shape[0]\/nfolds)\uff09\nfor n in range(nind-1):    \n    # \u6700\u5c0f\u50240, \u6700\u5927\u50241\u306e\u4e71\u6570\u3092nfolds\u500b\u751f\u6210\uff08\u4eca\u306f5\u500b\uff09\n    luck = np.random.uniform(0,1,nfolds)\n    \n    for cycle in range(nfolds): #range(5)\n        # np.matlib.repmat\u3067s[cycle]\u3092mat\u306e\u884c\u6570\u5206\u306e\u884c*1\u5217\u306e\u884c\u5217\u306b\u5909\u63db\n        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n        # mat\u306e\u5168\u884c\u524d\u5217\u3068s\u306e\u5168\u884c\u524d\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u305f\u5024\u30922\u500d\u3057\u305f\u3082\u306e\u3092sumDist\u306b\u4ee3\u5165\n        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)\n        # toDist[cycle] = toDist[cycle] + sumDist\n        totDist[cycle] += sumDist\n        \n        f = totDist[cycle]\/np.sum(totDist[cycle]) # totdist\u3092\u6b63\u898f\u5316\n        j = 0\n        kn = 0\n        for val in f:\n            j += val\n            if (j > luck[cycle]):\n                break\n            kn +=1\n        lineNumber[cycle] = kn\n        \n        # \u4e0a\u3067\u5024\u304c\u8ffd\u52a0\u3055\u308c\u305f\u30ec\u30b3\u30fc\u30c9\u306ftoDist\u304b\u3089\u524a\u9664\n        for n_iter in range(nfolds):\n            \n            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n            j= 0\n        \n        s[cycle] = mat[lineNumber[cycle],:]\n        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\nfor n_mod in range(nfolds):\n    values[n_mod] = out_train.index[values[n_mod]]","753baf28":"# \u8a08\u7b97\u4e0a\u53d6\u5f97\u3055\u308c\u3066\u3057\u307e\u3063\u305ftrain\u3068test\u306e\u6b63\u8ca0\u7121\u9650\u3092nan\u306b\u7f6e\u63db\ntrain.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\nqt_train = []\ntrain_nn=train[colNames].copy()\ntest_nn=test[colNames].copy()\nfor col in colNames:\n    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n    train_nn[col] = qt.fit_transform(train_nn[[col]])\n    test_nn[col] = qt.transform(test_nn[[col]])    \n    qt_train.append(qt)","842c8ae7":"train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\ntest_nn[['stock_id','time_id']]=test[['stock_id','time_id']]","088a8a42":"# \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\nfrom sklearn.cluster import KMeans\ntrain_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])","39a0b119":"nnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_sum_0c1',\n     'total_volume_sum_1c1', \n     'total_volume_sum_3c1',\n     'total_volume_sum_4c1', \n     'total_volume_sum_6c1',\n     'trade_size_sum_0c1',\n     'trade_size_sum_1c1', \n     'trade_size_sum_3c1',\n     'trade_size_sum_4c1', \n     'trade_size_sum_6c1',\n     'trade_order_count_sum_0c1',\n     'trade_order_count_sum_1c1',\n     'trade_order_count_sum_3c1',\n     'trade_order_count_sum_4c1',\n     'trade_order_count_sum_6c1',      \n     'price_spread_sum_0c1',\n     'price_spread_sum_1c1',\n     'price_spread_sum_3c1',\n     'price_spread_sum_4c1',\n     'price_spread_sum_6c1',   \n     'bid_spread_sum_0c1',\n     'bid_spread_sum_1c1',\n     'bid_spread_sum_3c1',\n     'bid_spread_sum_4c1',\n     'bid_spread_sum_6c1',       \n     'ask_spread_sum_0c1',\n     'ask_spread_sum_1c1',\n     'ask_spread_sum_3c1',\n     'ask_spread_sum_4c1',\n     'ask_spread_sum_6c1',   \n     'volume_imbalance_sum_0c1',\n     'volume_imbalance_sum_1c1',\n     'volume_imbalance_sum_3c1',\n     'volume_imbalance_sum_4c1',\n     'volume_imbalance_sum_6c1',       \n     'bid_ask_spread_sum_0c1',\n     'bid_ask_spread_sum_1c1',\n     'bid_ask_spread_sum_3c1',\n     'bid_ask_spread_sum_4c1',\n     'bid_ask_spread_sum_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1']","0f0f12cb":"# \u5148\u307b\u3069mat2\u306ftime_id\u304c5\u306e\u6642\u306b\u9650\u5b9a\u3057\u3066\u3044\u305f\u304c\u4eca\u56de\u306f\u5168\u30c7\u30fc\u30bf\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)\ndisplay(mat2)","9177bdb6":"import gc\ntrain_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\ntest_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')\ndel mat1,mat2\ndel train,test\ngc.collect()","e1fcfb50":"# https:\/\/bignerdranch.com\/blog\/implementing-swish-activation-function-in-keras\/\n# sigmoid\u95a2\u6570\nfrom keras.backend import sigmoid\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nget_custom_objects().update({'swish': Activation(swish)})\n\nhidden_units = (128,64,32)\nstock_embedding_size = 24\n\ncat_data = train_nn['stock_id']\n\ndef base_model():\n    # \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5165\u529b\u5c64\uff08shape\u306f\u5165\u529b\u306e\u6b21\u5143\uff09\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(244,), name='num_data')\n\n    #embedding\uff08\u57cb\u3081\u8fbc\u307f\uff09, flatenning\uff08\u5e73\u5766\u5316, concatenating\uff08\u9023\u7d50\uff09\n    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    out = keras.layers.Concatenate()([stock_flattened, num_input])\n    \n    # \u96a0\u308c\u5c64\n    for n_hidden in hidden_units:\n\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n        \n\n    # out = keras.layers.Concatenate()([out, num_input])\n    # \u51fa\u529b\u5c64\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n    \n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out,\n    )\n    \n    return model","48c54c2b":"# \u3082\u3046\u4e00\u5ea6\u95a2\u6570\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u308b\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","73990ec8":"target_name='target'\nscores_folds = {}\nmodel_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 5\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = list(train_nn)\n\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\ntry:\n    features_to_consider.remove('pred_NN')\nexcept:\n    pass\n\n\ntrain_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\ntest_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n\ntrain_nn[pred_name] = 0\ntest_nn[target_name] = 0\ntest_predictions_nn = np.zeros(test_nn.shape[0])\n\nfor n_count in range(n_folds):\n    print('CV {}\/{}'.format(counter, n_folds))\n    \n    indexes = np.arange(nfolds).astype(int)    \n    indexes = np.delete(indexes,obj=n_count, axis=0) \n    \n    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n    \n    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n    \n    #############################################################################################\n    # NN\n    #############################################################################################\n    \n    model = base_model()\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.006),\n        loss=root_mean_squared_per_error\n    )\n    \n    try:\n        features_to_consider.remove('stock_id')\n    except:\n        pass\n    \n    num_data = X_train[features_to_consider]\n    \n    scaler = MinMaxScaler(feature_range=(-1, 1))         \n    num_data = scaler.fit_transform(num_data.values)    \n    \n    cat_data = X_train['stock_id']    \n    target =  y_train\n    \n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n    cat_data_test = X_test['stock_id']\n\n    model.fit([cat_data, num_data], \n              target,               \n              batch_size=2048,\n              epochs=1000,\n              validation_data=([cat_data_test, num_data_test], y_test),\n              callbacks=[es, plateau],\n              validation_batch_size=len(y_test),\n              shuffle=True,\n             verbose = 1)\n\n    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    \n    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    \n    tt =scaler.transform(test_nn[features_to_consider].values)\n    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\/n_folds\n    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n       \n    counter += 1\n    features_to_consider.append('stock_id')","62ec2f85":"test_nn[\"row_id\"] = test_nn[\"stock_id\"].astype(str) + \"-\" + test_nn[\"time_id\"].astype(str) \ntest_nn[target_name] = (test_predictions_nn+predictions_lgb)\/2\n\nscore = round(rmspe(y_true = train_nn[target_name].values, y_pred = train_nn[pred_name].values),5)\nprint('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n\ndisplay(test_nn[['row_id', target_name]].head(3))\ntest_nn[['row_id', target_name]].to_csv('submission.csv',index = False)\n#test[['row_id', target_name]].to_csv('submission.csv',index = False)\n#kmeans N=5 [0.2101, 0.21399, 0.20923, 0.21398, 0.21175]","02de7f3d":"## \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f","646e6d06":"## RMSPE\u3067\u7121\u99c4\u306a\u51e6\u7406\u304c\u8d70\u308b\u306e\u3092\u6b62\u3081\u308b","9bf708d9":"### \u30aa\u30fc\u30c0\u30fc\u30d6\u30c3\u30af\u306e\u30a4\u30e1\u30fc\u30b8\uff08\u516c\u5f0f\u3088\u308a\uff09\n![](https:\/\/i.imgur.com\/16Qt255l.png)\n- bid_price1:\u6700\u9ad8\u58f2\u5024\u3002\u753b\u50cf\u3067\u3044\u3046\u3068147\u306b\u3042\u305f\u308b\n- ask_price1:\u6700\u4f4e\u8cb7\u5024\u3002\u753b\u50cf\u3067\u3044\u3046\u3068148\u306b\u3042\u305f\u308b\n- bid_price2:2\u756a\u76ee\u306b\u9ad8\u3044\u58f2\u5024\u3002\u753b\u50cf\u3067\u3044\u3046\u3068146\u306b\u3042\u305f\u308b\n- ask_price2:2\u756a\u76ee\u306b\u4f4e\u3044\u8cb7\u5024\u3002\u753b\u50cf\u3067\u3044\u3046\u3068149\u306b\u3042\u305f\u308b\n- bid_size1:bid_price1\u306e\u6642\u306e\u6570\u3002\u753b\u50cf\u3067\u3044\u3046\u3068251\n- ask_size1:ask_price1\u306e\u6642\u306e\u6570\u3002\u753b\u50cf\u3067\u3044\u3046\u3068221\n- bid_size2:bid_price2\u306e\u6642\u306e\u6570\u3002\u753b\u50cf\u3067\u3044\u3046\u3068321\n- ask_size2:ask_price2\u306e\u6642\u306e\u6570\u3002\u753b\u50cf\u3067\u3044\u3046\u3068148\n\nseconds_in_bucket\u306f\u5fc5\u305a0\u304b\u3089\u59cb\u307e\u308a\u3001\u30aa\u30fc\u30c0\u30fc\u30d6\u30c3\u30af\u306b\u4f55\u304b\u3057\u3089\u306e\u52d5\u304d\u304c\u3042\u308b\u3068\u968f\u6642\u30ec\u30b3\u30fc\u30c9\u304c\u8ffd\u52a0\u3055\u308c\u308b\u3002\nseconds_in_bucket\u304c1\u306e\u6642\u306fask_size1\u304c226\u304b\u3089100\u306b\u5909\u5316\u3057\u305f\u305f\u3081\u3001\u30ec\u30b3\u30fc\u30c9\u304c\u8ffd\u52a0\u3055\u308c\u305f\u3002","ed8f7fc8":"\u30aa\u30fc\u30c0\u30fc\u30d6\u30c3\u30af\u3092\u3082\u3068\u306b\u5b9f\u969b\u306b\u58f2\u8cb7\u304c\u884c\u308f\u308c\u308b\u3068trade\u30c7\u30fc\u30bf\u306b\u30ec\u30b3\u30fc\u30c9\u304c\u8ffd\u52a0\u3055\u308c\u308b\u3002\n\u6700\u521d\u306e\u30ec\u30b3\u30fc\u30c9\u3067\u306f\u3001\ntime_id=5\u306e21\u79d2\u306e\u6642\u306bprice=1.002301\u3067\u682a\u5f0f\u6570326\u3001\u53d6\u5f15\u6ce8\u658712\u306e\u3084\u308a\u3068\u308a\u304c\u884c\u308f\u308c\u305f","85023359":"WAP\u3068\u306f\u5225\u3067bid\u540c\u58ebask\u540c\u58eb\u3092\u639b\u3051\u5408\u308f\u305b\u305f\u3082\u306e\u3092\u306a\u305c\u4f5c\u6210\u3057\u305f\u306e\u304b\u306f\u4e0d\u660e","f9c6ae94":"## \u8a13\u7df4\u30c7\u30fc\u30bf\u3068test\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\u3057\u305f\u95a2\u6570\u306b\u3088\u3063\u3066\u8aad\u307f\u8fbc\u3080","cf7ca0a6":"### Log retuen\n\u682a\u4fa1\u306e\u5272\u5408\u306e\u6bd4\u7387\u3092\u5bfe\u6570\u3067\u53d6\u5f97\u3057\u305f\u5024\u3002log\u306e\u5272\u308a\u7b97\u306f\u5f15\u304d\u7b97\u306b\u5909\u63db\u3067\u304d\u308b\u305f\u3081\u3001\u3042\u308b\u6642\u9593t2\u3067\u306eWAP\u304b\u3089\u3042\u308b\u6642\u9593t1\u3067\u306eWAP\u306e\u305d\u308c\u305e\u308c\u306e\u5bfe\u6570\u3092\u5f15\u3044\u305f\u5024\n$$\n\ud835\udc5f\ud835\udc611,\ud835\udc612 = \\log(\\frac{WAP t2}{WAPt1})\n$$\n$$\n= \\log(WAP t2) - \\log(WAP t1)\n$$","917f5b19":"\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3068\u306f\u4f55\u304b\u3092\u7406\u89e3\u3059\u308b\u524d\u306bWAP\u306b\u3064\u3044\u3066\u610f\u5473\u5408\u3044\u3092\u7406\u89e3\u3059\u308b\n### WAP(Weighted averaged price)\n\u65e5\u672c\u8a9e\u8a33\u3067\u306f\u52a0\u91cd\u5e73\u5747\u3002\n\u30a6\u30a7\u30a4\u30c8\u3092\u8003\u616e\u3057\u305f\u5e73\u5747\u5024\u306e\u3088\u3046\u306a\u30a4\u30e1\u30fc\u30b8\u3067\u4e0b\u8a18\u306e\u5f0f\u3067\u8a08\u7b97\u3055\u308c\u308b\n$$\n    \ud835\udc4a\ud835\udc34\ud835\udc43 = \\frac{\ud835\udc35\ud835\udc56\ud835\udc51\ud835\udc43\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc521\u2217\ud835\udc34\ud835\udc60\ud835\udc58\ud835\udc46\ud835\udc56\ud835\udc67\ud835\udc521+\ud835\udc34\ud835\udc60\ud835\udc58\ud835\udc43\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc521\u2217\ud835\udc35\ud835\udc56\ud835\udc51\ud835\udc46\ud835\udc56\ud835\udc67\ud835\udc521}{\ud835\udc35\ud835\udc56\ud835\udc51\ud835\udc46\ud835\udc56\ud835\udc67\ud835\udc521+\ud835\udc34\ud835\udc60\ud835\udc58\ud835\udc46\ud835\udc56\ud835\udc67\ud835\udc521}\n$$\n\ud835\udc34\ud835\udc60\ud835\udc58\ud835\udc46\ud835\udc56\ud835\udc67\ud835\udc521\u3068\ud835\udc35\ud835\udc56\ud835\udc51\ud835\udc46\ud835\udc56\ud835\udc67\ud835\udc521\u306b\u6ce8\u76ee\u3057\u305f\u969b\u306b\u3001\ud835\udc34\ud835\udc60\ud835\udc58\ud835\udc46\ud835\udc56\ud835\udc67\ud835\udc52\u304c\u5927\u304d\u304f\u306a\u308c\u3070\ud835\udc35\ud835\udc56\ud835\udc51\ud835\udc43\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc52\u3092\u639b\u3051\u308b\u5272\u5408\u3082\u5168\u4f53\u306b\u5bfe\u3057\u3066\u5927\u304d\u304f\u306a\u308a\u3001\u8a08\u7b97\u3055\u308c\u305f\u5024\u306f\ud835\udc35\ud835\udc56\ud835\udc51\ud835\udc43\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc52\u3068\ud835\udc34\ud835\udc60\ud835\udc58\ud835\udc43\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc52\u3067\u306f\ud835\udc35\ud835\udc56\ud835\udc51\ud835\udc43\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc52\u306b\u8fd1\u3065\u304f\n\n\u3053\u306e\u3088\u3046\u306b\u305d\u308c\u305e\u308c\u3092\u5e73\u7b49\u306b\u8003\u3048\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u8981\u7d20\u306e\u5927\u304d\u3055\u3092\u8003\u616e\u3057\u305f\u4e0a\u3067\u5e73\u5747\u5024\u3092\u53d6\u5f97\u3059\u308b\u8003\u3048\u65b9\u304cWAP","73b98779":"## stock_id\u3068time_id\u3054\u3068\u306e\u96c6\u8a08\u3092\u884c\u3046\u305f\u3081\u306e\u95a2\u6570\u3092\u8a2d\u5b9a","c1fcce13":"## \u4eca\u56de\u306e\u76ee\u7684\n\u4e0e\u3048\u3089\u308c\u305f10\u5206\u9593\u306e\u53d6\u5f15\u306b\u304a\u3051\u308b\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u304b\u3089\u6b21\u306e10\u5206\u306e\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u4e88\u6e2c\u3059\u308b\n\n\u2192stock_id\uff08\u9298\u67c4\uff09\u3054\u3068\u306etime_id\uff08\u6642\u9593\uff09\u3054\u3068\u306e\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u4e88\u6e2c\u3059\u308b\n\n\u2192\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u306b\u3064\u3044\u3066\u306e\u8a73\u7d30\u306f\u306e\u3061\u306b\u89e3\u8aac\n\n================================================================================\n\n\u26a0\ufe0fkaggle\u521d\u5fc3\u8005\u304c\u5f37\u5f15\u306b\u548c\u8a33\u3057\u306a\u304c\u3089\u6d41\u308c\u3092\u78ba\u8a8d\u3057\u305f\u306e\u3067\u89e3\u91c8\u304c\u9593\u9055\u3063\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3054\u6ce8\u610f\u304f\u3060\u3055\u3044\u3002\n\n================================================================================\n\n## \u30c7\u30fc\u30bf\u306e\u6982\u8981\n### train.csv\nstock_id, time_id, target\uff08\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\uff09\u304c\u542b\u307e\u308c\u308b\n### test.csv\nstock_id\u304c0\u306e\u3082\u306e\u306e\u3001time_id\u304c4\/32\/34\u306e3\u884c\u306e\u307f\n\u2192row_id\u3068\u3044\u3046\u5217\u306b0-4\u306a\u3069stock_id\u3068time_id\u3092\u30cf\u30a4\u30d5\u30f3\u3067\u63a5\u7d9a\u3057\u305f\u3082\u306e\u304c\u542b\u307e\u308c\u3066\u3044\u308b\n### sample_submission.csv\nrow_id, target\u306e3\u884c2\u5217\u306e\u307f\uff08row_id = 0-4, 0-32, 0-34\uff09\n### book_train.parquet\/book_test.parquet\nstock_id\u3054\u3068\u306e\u30aa\u30fc\u30c0\u30fc\u30d6\u30c3\u30af\u306e\u30c7\u30fc\u30bf\u304cparquet\u3068\u3044\u3046\u5f62\u3067\u5165\u3063\u3066\u3044\u308b\uff08test\u306fstock_id=0\u306e\u307f\uff09\n### trade_train.parquet\/trade_test.parquet\nstock_id\u3054\u3068\u306e\u5b9f\u969b\u306e\u30c8\u30ec\u30fc\u30c9\u30c7\u30fc\u30bf\u304cparquet\u3068\u3044\u3046\u5f62\u3067\u5165\u3063\u3066\u3044\u308b\uff08test\u306fstock_id=0\u306e\u307f\uff09","104cd0de":"## \u30aa\u30fc\u30c0\u30fc\u30d6\u30c3\u30af\u3068\u30c8\u30ec\u30fc\u30c9\u30c7\u30fc\u30bf\u3092\u96c6\u8a08\u3057\u305f\u4e0a\u3067\u7d50\u5408\u3059\u308b\u305f\u3081\u306e\u95a2\u6570","0f1336ee":"## WAP\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\u3092\u4f5c\u6210","811a1fd2":"\u305d\u308c\u305e\u308c\u306brow_id\u304c\u8ffd\u52a0\u3055\u308c\u305f\u305f\u3081merge\u304c\u53ef\u80fd","d98d7436":"## \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af","3e962c00":"## Log return\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\u3092\u4f5c\u6210","9080b921":"\u4e0b\u8a18\u3092\u5b9f\u884c\u3059\u308b\u306b\u3042\u305f\u3063\u3066\u6700\u521d\n```\nfrom keras import backend as K\n```\n\u3092\u884c\u306a\u3063\u305f\u969b\u306bNo module\u3068\u306a\u3063\u3066\u3057\u307e\u3063\u305f\u306e\u3067pip list\u3067\u8abf\u3079\u305f\u3068\u3053\u308dversion\u304c2.6.0\u3060\u3063\u305f\u306e\u3067version\u3092\u5909\u66f4\u3059\u308b\u3068\u56de\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u306e\u3067\u6ce8\u610f\n```\n!pip uninstall -y keras\n!pip install keras==2.3.1\n```","90380bdb":"kaggle\u306enotebook\u3060\u306830~40\u5206\u304f\u3089\u3044\u304b\u304b\u308b\u3002\u30c7\u30fc\u30bf\u6570\u304c1\u5104\u8d85\u3048\u3066\u308b\u304b\u3089\u3057\u3087\u3046\u304c\u306a\u3044\u3001\u3001","de982816":"## \u7e26\u5217time_id, \u6a2a\u5217stock_id\u3067\u30af\u30ed\u30b9\u3059\u308b\u30bb\u30eb\u306b\u305d\u308c\u305e\u308c\u306etime_id,stock_id\u306e\u6642\u306etarget\u304c\u5165\u3063\u3066\u3044\u308b\u30c6\u30fc\u30d6\u30eb\u304c\u6b32\u3057\u3044","3cbf4b31":"\u5727\u5012\u7684stock_id","60c43d53":"stock_id=0\u3060\u3051\u3067\u3082\u30c7\u30fc\u30bf\u6570\u304c120\u4e07\u304f\u3089\u3044\u3042\u308b\u305f\u3081\u7dcf\u6570\u30671\u5104\u306f\u8d85\u3048\u305d\u3046\u3002\u96c6\u8a08\u3057\u3066\u304a\u304b\u306a\u3044\u3068pc\u304c\u843d\u3061\u308b","5f33dec4":"\u30aa\u30fc\u30c0\u30fc\u30d6\u30c3\u30af\u3068\u30c8\u30ec\u30fc\u30c9\u30c7\u30fc\u30bf\u3092\u9298\u67c4\u3054\u3068\u306b\u64cd\u4f5c\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\u306f\u8a2d\u5b9a\u5b8c\u4e86\n\n\u9053\u4e2d\u3067\u884c\u306a\u3063\u3066\u3044\u305f\u64cd\u4f5c\u306f\u5b66\u7fd2\u306b\u5fc5\u8981\u306a\u7279\u5fb4\u91cf\u3092\u8ffd\u52a0\u3059\u308b\u305f\u3081\u306e\u3082\u306e\u3067\u3001\u3053\u3053\u3092\u5f8c\u3005\u81ea\u5206\u3067\u3082\u3069\u3093\u306a\u3082\u306e\u304c\u826f\u3044\u304b\u8003\u3048\u308b\u5fc5\u8981\u304c\u3042\u308b","dd62c390":"## Realized volatility\n\u4e0a\u3067\u53d6\u5f97\u3057\u305fLog return\u306e\u6a19\u6e96\u504f\u5dee\u3092\uff11\u5e74\u9593\u5206\u8a08\u7b97\u3057\u305f\u5024\u3067\u3042\u308b\u5e74\u9593\u6a19\u6e96\u504f\u5dee\u3092\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3068\u3088\u3076\u3002\n$$\n\ud835\udf0e = \\sqrt{\\sum_{t}^{}\ud835\udc5f^2\ud835\udc61-1,\ud835\udc61\\quad}\n$$\n\u4eca\u56de\u306f10\u5206\uff08second_in_bucket\u304c0\u304b\u3089600\uff09\u5358\u4f4d\u3067Log return\u3092\u6c42\u3081\u3053\u3068\u3092\u4e00\u5e74\u5206\u884c\u3044\u3001\u6a19\u6e96\u504f\u5dee\u3092\u6c42\u3081\u308b\u3002\n\n\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u304c\u9ad8\u3044\u307b\u3069\u58f2\u8cb7\u306e\u52d5\u304d\u304c\u76db\u3093\u3067\u3042\u308b\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308a\u3001\u624b\u6570\u6599\u3067\u4f1a\u793e\u304c\u5132\u304b\u308b\u304b\u3089\u3044\u3044\u5e02\u5834\u3068\u3044\u3046\u3053\u3068\uff08\uff1f\uff09","a12a83c9":"## \u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3068\u306f\uff1f\n","b1a378ee":"## \u4e26\u5217\u51e6\u7406","92ccd171":"\u524d\u7f6e\u304d\u3067\u89e3\u8aac\u3057\u305fWAP, Log return, \u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\u3092\u8a2d\u5b9a\n\n\u7d9a\u3044\u3066\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f","fc791bb1":"## \u76ee\u7684\u5909\u6570\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570","3a8d8e7c":"## lgbm baseline\u3092\u65e5\u672c\u8a9e\u3067\u89e3\u91c8\nmost votes\u4e0a\u4f4d\u3067\u3042\u3063\u305f\u300clgbm baseline\u300d\u306e\u5206\u6790\u3092\u8e0f\u8972\u3057\u3064\u3064\u3001\u30b3\u30f3\u30da\u306e\u7406\u89e3\u3092\u6df1\u3081\u3066\u3044\u304f\u3002\n\u4e0b\u8a18\u304b\u3089\u5b9f\u969b\u306e\u30b3\u30fc\u30c9\uff08\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\u5206\u306f\u9069\u5b9c\u8ffd\u52a0\uff09","fcb37f87":"## \u6642\u5b9a\u6570tau\u3092\u5b9a\u7fa9","7b1b15c6":"\u4e00\u901a\u308a\u6d41\u308c\u3092\u8ffd\u3063\u3066\u5b9f\u884c\u3057\u3066\u307f\u305f\u304c\u30011\u9031\u76ee\u3067\u306f\u30a4\u30de\u30a4\u30c1\u308f\u304b\u3089\u306a\u3044\u7b87\u6240\u304c\u591a\u304b\u3063\u305f\u3002\n\n\u4ed6\u306enotebook\u3082\u62dd\u898b\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u306a\u304c\u3089\u5468\u56de\u3057\u3066\u3069\u306e\u3088\u3046\u306a\u7279\u5fb4\u91cf\u3092\u751f\u6210\u3057\u3066\u3044\u308b\u306e\u304b\u3092\u4e2d\u5fc3\u306b\u5fa9\u7fd2\u3059\u308b\u3002"}}