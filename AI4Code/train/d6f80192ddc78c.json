{"cell_type":{"f97076ca":"code","0bdc411c":"code","2b7553a8":"code","a76c3ba0":"code","87ce0b38":"code","fcbd20ee":"code","5f03aaae":"code","5b2df1c1":"code","0b31b04d":"code","f9250e7f":"code","986f5cfb":"code","7695c457":"code","2d3f5729":"code","34cc3e82":"code","6173a305":"code","1c8c25b9":"code","51e10d4b":"code","de7b4353":"code","6e6438ae":"code","9699eceb":"code","a01db72a":"code","2c9bcf7b":"code","4cfef884":"code","2741b592":"code","f3065439":"code","b0d8cbc8":"code","21cbdd51":"code","6bd8e691":"code","b5f378d0":"code","de24a892":"code","675d7a69":"code","9d02ae0a":"code","c9f1bd93":"code","b62de766":"code","e1cf5215":"code","05dff447":"code","a942492e":"code","f7f42952":"code","b53a9617":"code","81194377":"code","2f3a8d30":"code","e78d67c4":"code","42dd77b7":"code","789e63d3":"code","04a6514d":"code","e02c06a0":"code","c93a8260":"code","8823972a":"code","e77b9cc3":"code","eda0ecb1":"code","4d78c802":"code","b117e7bc":"code","58bc5e58":"code","8ed523a2":"code","0a47e1cb":"code","90bbb22e":"code","71792b6c":"code","076a8bc7":"code","7e9e6506":"code","71b41d87":"code","e050f006":"code","bcdc7311":"code","049330b1":"code","49c516bc":"code","d7bcb9b0":"code","ad37a5ff":"code","83463823":"code","52b0afc7":"code","78f36360":"code","afb85f6a":"code","37774280":"code","a2d00594":"code","539e5140":"code","4c5435bd":"code","bdc933d1":"code","e7302739":"code","dcefd6bd":"code","fd0d6c02":"code","13be3951":"code","5c575f70":"code","76dbb3ee":"code","021d2d04":"code","ab47b7ac":"code","260be49b":"code","0f110024":"code","8d5abdbf":"code","210d8365":"code","842242c2":"code","30cba6f3":"code","29ffdaf1":"code","08e080ca":"code","6465b9d5":"code","c72a594e":"code","6d3db501":"code","68ccde78":"code","54993d14":"code","9747395f":"code","f86709c6":"code","9020c8ad":"code","a4119069":"code","6d2dddd3":"code","d77074b8":"code","00ad6444":"code","c98ad05b":"code","743fa395":"code","4a311dfa":"code","ea6d9e19":"code","8b3732aa":"code","ed1a4444":"code","c131cc4f":"code","b3aa5dc9":"code","a3f5afb9":"code","525516e4":"code","7c253b7e":"code","0469e44a":"code","9f911996":"code","7f1e6e07":"code","2b76788e":"code","be733044":"code","d3918256":"code","991fef16":"code","d6bb3830":"code","bcb681bb":"code","8cde6b49":"code","0bdc6de2":"code","759b8cd2":"code","df850b41":"code","c1782b4b":"code","73d1cad4":"code","b0136736":"markdown","4871d5d9":"markdown","301d6759":"markdown","55ec8400":"markdown","1840dfcc":"markdown","21f9732e":"markdown","9035ce0b":"markdown","cab7e4d4":"markdown","90e95700":"markdown","061c9611":"markdown","db39def7":"markdown","fee8f8aa":"markdown","805dab68":"markdown","38e47d34":"markdown","36b0ce9d":"markdown","05630d41":"markdown","9285f08f":"markdown","7ffd8d98":"markdown","8f13d574":"markdown","44253d09":"markdown","cfcedcee":"markdown","43eaef23":"markdown","c9d009ae":"markdown","cc231610":"markdown","0d967ee2":"markdown","cd49e81d":"markdown","1d82e01b":"markdown","4bf5f2ee":"markdown","d01937ee":"markdown","68e3bebb":"markdown","1c65f50a":"markdown","aa022bfe":"markdown","18c5388a":"markdown","0b497acc":"markdown","d5c9fe4f":"markdown","9bad7ac8":"markdown","6a304fa7":"markdown","2d13626c":"markdown","c26be9bd":"markdown","2ff4b8e4":"markdown","03ab824c":"markdown","2a57015e":"markdown","ef698b59":"markdown","e810eca8":"markdown","7237e25f":"markdown","51977858":"markdown","028592d6":"markdown","dc4ca4a7":"markdown","ec25440f":"markdown","3874ed3c":"markdown","461a0c56":"markdown","a4c53294":"markdown","1090cffb":"markdown","9a13f228":"markdown","9a99437e":"markdown","6ba0b0f1":"markdown","885e67b3":"markdown","3a102ac6":"markdown","a5b1dfe4":"markdown","2acd2f85":"markdown","6fca0088":"markdown","0dad2151":"markdown","204a7b6b":"markdown","1f3f58f6":"markdown","17cdf38f":"markdown","508d3fe0":"markdown","f0bf8489":"markdown","dfa80d1f":"markdown","db858f0d":"markdown"},"source":{"f97076ca":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom sklearn import linear_model\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom pprint import pprint\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix, roc_auc_score\nimport os\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')","0bdc411c":"# Importing Housing.csv\ntelecom = pd.read_csv(\"..\/input\/telecom-customer\/Telecom_customer churn.csv\")","2b7553a8":"# summary of the dataset: 99999 rows, 226 columns\ntelecom.info()","a76c3ba0":"telecom.shape","87ce0b38":"telecom.head()","fcbd20ee":"# Checking columns which have missing values\n\ntelecom.isnull().mean().sort_values(ascending=False)","5f03aaae":"# columns with more than 50% missing values\n\ncolumn_missing_data = telecom.loc[:,telecom.isnull().mean() >= 0.5 ]\nprint(\"Number of columns with missing data {}\".format(len(column_missing_data.columns)))\ncolumn_missing_data.columns","5b2df1c1":"# Droping columns with more than 50% missing values\ntelecom = telecom.loc[:, telecom.isnull().mean() <= .5]","0b31b04d":"telecom.shape","f9250e7f":"# Droping date columns since there will be no time series analysis\n\ndate_cols = telecom.columns[telecom.columns.str.contains(pat = 'date')]\ntelecom = telecom.drop(date_cols, axis = 1) ","986f5cfb":"# Dropping Mobile Number\n\ntelecom = telecom.drop('mobile_number', axis = 1) ","7695c457":"telecom.shape","2d3f5729":"# Checking percentage of missing values in dataset\n\ntelecom.isnull().mean().sort_values(ascending=False)","34cc3e82":"# All the column names with missing values\ntelecom.loc[:,telecom.isnull().mean() > 0].columns","6173a305":"# Plotting missing values\nplt.figure(figsize=(20, 5))\nsns.heatmap(telecom.isnull())","1c8c25b9":"# Remove Columns which have only 1 unique Value\n\ncol_list = telecom.loc[:,telecom.apply(pd.Series.nunique) == 1]\ntelecom = telecom.drop(col_list, axis = 1)\ntelecom.shape","51e10d4b":"telecom.describe()","de7b4353":"# Storing column names before imputing\n\ncol_name = telecom.columns\ncol_name","6e6438ae":"# Imputing median values using SimpleImputer\nfrom sklearn.impute import SimpleImputer\n\nimp_mean = SimpleImputer( strategy='median') \nimp_mean.fit(telecom)\ntelecom = imp_mean.transform(telecom)","9699eceb":"telecom= pd.DataFrame(telecom)\ntelecom.columns = col_name\ntelecom.head()","a01db72a":"plt.figure(figsize=(20, 5))\nsns.heatmap(telecom.isnull())","2c9bcf7b":"# Renaming columns\n\ntelecom.rename(columns={'jun_vbc_3g': 'vbc_3g_6', \n                        'jul_vbc_3g': 'vbc_3g_7', \n                        'aug_vbc_3g': 'vbc_3g_8', \n                        'sep_vbc_3g': 'vbc_3g_9'}, inplace=True)","4cfef884":"total_rech_amt_6_7 = telecom[['total_rech_amt_6','total_rech_amt_7']].sum(axis=1)","2741b592":"# Selecting top 30 percent subscribers for churn prediction\n\np70 = np.percentile(total_rech_amt_6_7, 70.0)\n\ntele_top30 = telecom[total_rech_amt_6_7 > p70]\ntele_top30.shape","f3065439":"tele_top30['total_usage_9'] = tele_top30[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']].sum(axis=1)","b0d8cbc8":"tele_top30['churn'] = tele_top30['total_usage_9'].apply(lambda x: 1 if x==0 else 0 )","21cbdd51":"tele_top30.head()","6bd8e691":"mon_9_cols = tele_top30.columns[tele_top30.columns.str.contains(pat = '_9')]\nmon_9_cols","b5f378d0":"tele_top30.drop(mon_9_cols, axis=1, inplace = True)\ntele_top30.shape","de24a892":"# Converting age on network to years from days\n\ntele_top30['aon_yr'] = round(tele_top30['aon']\/365,2)\ntele_top30.drop('aon',axis=1,inplace=True)","675d7a69":"col_list = tele_top30.columns[tele_top30.columns.str.contains('_6|_7')]\nlen(col_list)","9d02ae0a":"unique_col_list = col_list.str[:-2].unique()\nlen(unique_col_list)","c9f1bd93":"unique_col_list","b62de766":"for col in unique_col_list:\n    col_new_name = col+\"_6_7\"\n    col_6_name = col+\"_6\"\n    col_7_name = col+\"_7\"\n    tele_top30[col_new_name] = tele_top30[[col_6_name,col_7_name]].sum(axis=1)","e1cf5215":"tele_top30.shape","05dff447":"tele_top30.drop(col_list, axis=1, inplace=True)\ntele_top30.shape","a942492e":"tele_top30.head()","f7f42952":"tele_top30.describe()","b53a9617":"tele_top30['churn'].describe()","81194377":"# Storing churn data in new dataframe\nChurn = pd.DataFrame(tele_top30['churn'])\n\n# Dropping churn column from tele_top30 before capping operation\ntele_top30 = tele_top30.drop(['churn'], axis=1)","2f3a8d30":"# Derving 25th and 75th percentile\n\nQ1=tele_top30.quantile(0.25)\nQ3=tele_top30.quantile(0.75)\n\n# Deriving Inter Quartile Range\nIQR=Q3-Q1\n\n# Derving the Upper limit and Lower limit\nLL = Q1 - 3*IQR\nUL = Q3 + 3*IQR ","e78d67c4":"# Capping the data using Upper Limit and Lower Limit\n\nq = [LL,UL]\ntele_top30 = tele_top30.clip(LL,UL,axis=1)\nprint(tele_top30.shape)","42dd77b7":"tele_top30.describe()","789e63d3":"# Removing columns which have only one value after capping operation\n\ncol_list = tele_top30.loc[:,tele_top30.apply(pd.Series.nunique) == 1]\ntele_top30 = tele_top30.drop(col_list, axis = 1)\ntele_top30.shape","04a6514d":"# Adding churn column to tele_top30\n\ntele_top30 = pd.concat([tele_top30,Churn], axis=1)\ntele_top30.shape","e02c06a0":"# Plotting the correlation matrix using seaborn heatmap\n\ncorr_mat = tele_top30.corr()\nplt.figure(figsize=(20, 10))\nsns.heatmap(corr_mat)","c93a8260":"# Finding the pairs of most correlated features\n\nabs(corr_mat).unstack().sort_values(ascending = False).drop_duplicates().head(10)","8823972a":"# Plotting the jointplot to check correlation\n\nsns.jointplot(x = 'total_rech_amt_6_7', y = 'arpu_6_7', data=tele_top30, kind='reg')","e77b9cc3":"# Plotting the jointplot to check correlation\n\nsns.jointplot(x = 'total_rech_amt_8', y = 'arpu_8', data=tele_top30, kind='reg', color = [255\/255,152\/255,150\/255])","eda0ecb1":"#Finding highest correlated features with churn\n\ncorr_tgt = abs(corr_mat[\"churn\"]).sort_values(ascending = False)\ntop_features = corr_tgt.loc[((corr_tgt > 0.2) & (corr_tgt != 1))]\ntop_features","4d78c802":"# Plotting absolute correlation value of churn with all other varibales\n\nplt.figure(figsize=(20,5))\ncorr_tgt.sort_values(ascending = False).plot(kind='bar')","b117e7bc":"# Checking the imbalance in churn feature\n\ntele_top30['churn'].value_counts()*100.0 \/len(tele_top30)","58bc5e58":"plt.figure(figsize=(3, 4))\nsns.countplot('churn', data=tele_top30)\nplt.title('Churn distribution')\nplt.show()","8ed523a2":"Interpretable_Model_df = tele_top30","0a47e1cb":"y = Interpretable_Model_df.pop('churn')\nX = Interpretable_Model_df\nX.shape","90bbb22e":"X_cols = X.columns","71792b6c":"# Scaling the data using standard scaler\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)","076a8bc7":"# Creating the test train split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)","7e9e6506":"# Balancing the dataSet using SMOTE method\n\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(sampling_strategy='auto', random_state=100)\nX_train_bal, y_train_bal = sm.fit_sample(X_train, y_train)","71b41d87":"print(X_train_bal.shape)\nprint(y_train_bal.shape)","e050f006":"plt.figure(figsize=(3, 4))\nsns.countplot(y_train_bal)\nplt.title('Churn distribution')\nplt.show()","bcdc7311":"from sklearn.feature_selection import SelectFromModel\n\nC = [100, 10, 1, 0.5, 0.1, 0.01, 0.001]\n\nfor c in C:\n    lassoclf = LogisticRegression(penalty='l1', solver='liblinear', C=c).fit(X_train_bal, y_train_bal)\n    model = SelectFromModel(lassoclf, prefit=True)\n    X_lasso = model.transform(X_train_bal)\n    print('C Value - ',c, ' selects',X_lasso.shape[1],' no. of Features')\n    ","049330b1":"lassoclf = LogisticRegression(penalty='l1', solver='liblinear', C=.001).fit(X_train_bal, y_train_bal)\nmodel = SelectFromModel(lassoclf, prefit=True)\nX_train_lasso = model.transform(X_train_bal)\npos = model.get_support(indices=True)\nselected_features = list(Interpretable_Model_df.columns[pos])\nprint(selected_features)","49c516bc":"X_train_lasso = pd.DataFrame(X_train_lasso)\nX_train_lasso.columns = selected_features\nX_train_lasso","d7bcb9b0":"# Defining common code\n\ndef print_all_scores(y_test, test_prediction, y_train, train_prediction):\n    print('Precision on test set:\\t'+str(round(precision_score(y_test,test_prediction) *100,2))+\"%\")\n    print('Recall on test set:\\t'+str(round(recall_score(y_test,test_prediction) *100,2))+\"%\")\n    print(\"Training Accuracy: \"+str(round(accuracy_score(y_train,train_prediction) *100,2))+\"%\")\n    print(\"Test Accuracy: \"+str(round(accuracy_score(y_test,test_prediction) *100,2))+\"%\")","ad37a5ff":"# Creating a base logistic regression model\nlr = LogisticRegression(random_state=100)\n\n# Lookin at the parameters used by our base model\nprint('Parameters currently in use:\\n')\npprint(lr.get_params())","83463823":"# fit the model\nlr.fit(X_train_lasso, y_train_bal)\n\n# Predicting values\nX_test_lasso = pd.DataFrame(data=X_test).iloc[:, pos]\nX_test_lasso.columns = selected_features\npredictions = lr.predict(X_test_lasso)\ntrain_pred = lr.predict(X_train_lasso)","52b0afc7":"# Accuracy, precision, recall\/sensitivity of the model\nprint_all_scores(y_test,predictions, y_train_bal, train_pred)","78f36360":"# Initialising logistic Regression\nlog_reg = LogisticRegression(random_state = 100)\n\n# Creating hyper parameter grid\nparameter_grid = {'solver': ['newton-cg', 'lbfgs','liblinear','sag'],\n                  'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n                  'C': [100, 10, 1.0, 0.1, 0.01]}\n\ngs = GridSearchCV(estimator=log_reg, param_grid=parameter_grid, n_jobs=-1, cv=3, scoring='accuracy', error_score=0)","afb85f6a":"# Fitting the model\ngrid_result = gs.fit(X_train_lasso, y_train_bal)\n\n# Finding the best model\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","37774280":"# Initialising hyper tuned logistic Regression\nlog_reg_ht = LogisticRegression(C= 1.0, penalty= 'l2', solver= 'liblinear', random_state = 100)\n\n# Fitting the model\nlog_reg_ht.fit(X_train_lasso, y_train_bal)","a2d00594":"# Predicting the labels\ntrain_pred = log_reg_ht.predict(X_train_lasso)\ntest_pred = log_reg_ht.predict(X_test_lasso)","539e5140":"# Accuracy, precision, recall\/sensitivity of the model\nprint_all_scores(y_test,test_pred,y_train_bal,train_pred)","4c5435bd":"# To get the weights of all the variables\nweights = pd.Series(log_reg_ht.coef_[0],\n                 index=selected_features)\nweights.sort_values(ascending = False).plot(kind = 'bar')","bdc933d1":"# Running the random forest with default parameters.\nrfc = RandomForestClassifier(random_state = 100)\n\n# Lookin at the parameters used by our base model\nprint('Parameters currently in use:\\n')\npprint(rfc.get_params())","e7302739":"# fit the model\nrfc.fit(X_train_lasso,y_train_bal)\n\n# Making predictions\npredictions = rfc. predict(X_test_lasso)\ntrain_pred = rfc. predict(X_train_lasso)","dcefd6bd":"# Accuracy, precision, recall\/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","fd0d6c02":"max_depth = [int(x) for x in np.linspace(10, 50, num = 5)]\nmax_depth.append(None)\n\n# Create the random parameter grid\nparameter_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)],\n                  'max_features': ['auto', 'sqrt'],\n                  'max_depth': max_depth,\n                  'min_samples_split': [100, 500, 1000],\n                  'min_samples_leaf': [50, 250, 500],\n                  'bootstrap': [True, False]}\n\npprint(parameter_grid)\n\n# Searching across different combinations for best model parameters\nrf_random = RandomizedSearchCV(estimator = rfc, param_distributions = parameter_grid, n_iter = 100, \n                               cv = 3, verbose=2, random_state=100, n_jobs = -1)","13be3951":"# Fit the random search model\nrf_random.fit(X_train_lasso, y_train_bal)\n\n# Finding the best parameters\nrf_random.best_params_","5c575f70":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [15, 20, 25],\n    'min_samples_leaf': range(40, 50, 60),\n    'min_samples_split': range(80, 100, 120),\n    'n_estimators': [800, 1000, 1200], \n    'max_features': ['sqrt'],\n    'bootstrap': [False]\n}\n\n# Create a based model\nrf = RandomForestClassifier(random_state = 100)\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1,verbose = 1)","76dbb3ee":"# Fitting the model\ngrid_result = grid_search.fit(X_train_lasso, y_train_bal)\n\n# Finding the best model\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","021d2d04":"# Model with the best hyperparameters\n\nrfc = RandomForestClassifier(bootstrap=False,\n                             max_depth=20,\n                             min_samples_leaf=40, \n                             min_samples_split=80,\n                             max_features='sqrt',\n                             n_estimators=800)\n\n# Fit\nrfc.fit(X_train_lasso, y_train_bal)","ab47b7ac":"# Predict\ntrain_pred = rfc.predict(X_train_lasso)\ntest_pred = rfc.predict(X_test_lasso)","260be49b":"# Accuracy, precision, recall\/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","0f110024":"# To get the weights of all the variables\nweights = pd.Series(rfc.feature_importances_,\n                 index=selected_features)\nweights.sort_values(ascending = False).plot(kind = 'bar')","8d5abdbf":"# fit model on training data with default hyperparameters\nxgb = XGBClassifier()\n\n# Lookin at the parameters used by our base model\nprint('Parameters currently in use:\\n')\npprint(xgb.get_params())","210d8365":"# Fitting the model\nxgb.fit(X_train_lasso,y_train_bal)\n\n# Making predictions\npredictions = xgb.predict(X_test_lasso)\ntrain_pred = xgb.predict(X_train_lasso)","842242c2":"# Accuracy, precision, recall\/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","30cba6f3":"# AUC Score\nprint(\"AUC Score on test set:\\t\" +str(round(roc_auc_score(y_test,predictions) *100,2)))","29ffdaf1":"# hyperparameter tuning with XGBoost\n\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.2, 0.6], \n             'subsample': [0.3, 0.6, 0.9]}          \n\n\n# specify model\nxgb_ht = XGBClassifier(max_depth=2, n_estimators=200)\n\n# set up GridSearchCV()\ngs = GridSearchCV(estimator = xgb_ht, param_grid = param_grid, scoring= 'roc_auc', \n                        cv = 3, verbose = 1, return_train_score=True, n_jobs = -1)     ","08e080ca":"# Fitting the model\ngrid_result = gs.fit(X_train_lasso,y_train_bal) \n\n# Finding the best model\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","6465b9d5":"# Model with the best hyperparameters\nxgb_ht = XGBClassifier(max_depth=2, n_estimators=200, learning_rate = 0.6, subsample = 0.9)\n\n# Fit\nxgb_ht.fit(X_train_lasso, y_train_bal)","c72a594e":"# Predict\ntrain_pred = xgb_ht.predict(X_train_lasso)\ntest_pred = xgb_ht.predict(X_test_lasso)","6d3db501":"# Accuracy, precision, recall\/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","68ccde78":"# To get the weights of all the variables\nweights = pd.Series(xgb_ht.feature_importances_,\n                 index=selected_features)\nweights.sort_values(ascending = False).plot(kind = 'bar')","54993d14":"def draw_roc( y_test_churn, y_pred_churn ):\n    fpr, tpr, thresholds = metrics.roc_curve(  y_test_churn, y_pred_churn,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score(  y_test_churn, y_pred_churn )\n    print(\"ROC score: {}\".format(auc_score))\n    plt.figure(figsize=(6, 6))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr, tpr, thresholds","9747395f":"# Running pca with default parameters.\npca = PCA(random_state=100)\n\n# Fitting the model\npca.fit(X_train_bal)","f86709c6":"# cumulative variance\nvar_cumu = np.cumsum(pca.explained_variance_ratio_)\n\n# code for Scree plot\nfig = plt.figure(figsize=[12,8])\nplt.vlines(x=30, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.95, xmax=30, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.plot(var_cumu)\nplt.ylabel(\"Cumulative variance explained\")\nplt.show()","9020c8ad":"# Initializing the PCA model\npca_inc = IncrementalPCA(n_components=30)\n\n# Fitting the model\ndf_train_pca_inc = pca_inc.fit_transform(X_train_bal)\n\n# Looking at the shape\ndf_train_pca_inc.shape","a4119069":"df_train_pca_inc","6d2dddd3":"# Plottong correlation\n\ncorrmat = np.corrcoef(df_train_pca_inc.transpose())\nplt.figure(figsize=[15,5])\nsns.heatmap(corrmat)","d77074b8":"# Applying the transformation on test\n\ndf_test_pca_inc = pca_inc.transform(X_test)\ndf_test_pca_inc.shape","00ad6444":"# Creating a base logistic regression model\nlr = LogisticRegression(random_state=100)\n\n# Lookin at the parameters used by our base model\nprint('Parameters currently in use:\\n')\npprint(lr.get_params())","c98ad05b":"# fit the model\nlr.fit(df_train_pca_inc, y_train_bal)\n\n# Predicting values\npredictions = lr.predict(df_test_pca_inc)\ntest_pred = lr.predict(df_train_pca_inc)","743fa395":"# Accuracy, precision, recall\/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","4a311dfa":"# Initialising logistic Regression\nlog_reg = LogisticRegression(random_state = 100)\n\n# Creating hyper parameter grid\nparameter_grid = {'solver': ['newton-cg', 'lbfgs','liblinear','sag'],\n                  'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n                  'C': [100, 10, 1.0, 0.1, 0.01]}\n\ngs = GridSearchCV(estimator=log_reg, param_grid=parameter_grid, n_jobs=-1, cv=3, scoring='accuracy', error_score=0)","ea6d9e19":"# Fitting the model\ngrid_result = gs.fit(df_train_pca_inc, y_train_bal)\n\n# Finding the best model\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","8b3732aa":"# Initialising hyper tuned logistic Regression\nlog_reg_ht = LogisticRegression(C= 0.1, penalty= 'l2', solver= 'newton-cg', random_state = 100)\n\n# Fitting the model\nlog_reg_ht.fit(df_train_pca_inc, y_train_bal)","ed1a4444":"# Predicting the labels\ntrain_pred = log_reg_ht.predict(df_train_pca_inc)\ntest_pred = log_reg_ht.predict(df_test_pca_inc)","c131cc4f":"# Accuracy, precision, recall\/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","b3aa5dc9":"# Running the random forest with default parameters.\nrfc = RandomForestClassifier(random_state = 100)\n\n# Lookin at the parameters used by our base model\nprint('Parameters currently in use:\\n')\npprint(rfc.get_params())","a3f5afb9":"# fit the model\nrfc.fit(df_train_pca_inc,y_train_bal)\n\n# Making predictions\npredictions = rfc.predict(df_test_pca_inc)\ntrain_pred = rfc.predict(df_train_pca_inc)","525516e4":"# Accuracy, precision, recall\/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","7c253b7e":"max_depth = [int(x) for x in np.linspace(10, 30, num = 3)]\n\n# Create the random parameter grid\nparameter_grid = {'n_estimators': [int(x) for x in np.linspace(start = 600, stop = 1000, num = 5)],\n                  'max_features': ['auto', 'sqrt'],\n                  'max_depth': max_depth,\n                  'min_samples_split': [500, 1000],\n                  'min_samples_leaf': [250, 500],\n                  'bootstrap': [True, False]}\n\npprint(parameter_grid)\n\n# Searching across different combinations for best model parameters\nrf_random = RandomizedSearchCV(estimator = rfc, param_distributions = parameter_grid, n_iter = 50, \n                               cv = 3, verbose=2, random_state=100, n_jobs = -1)","0469e44a":"# Fit the random search model\nrf_random.fit(df_train_pca_inc, y_train_bal)\n\n# Finding the best parameters\nrf_random.best_params_","9f911996":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [15, 20, 25],\n    'min_samples_leaf': range(200, 300, 50),\n    'min_samples_split': range(400, 600, 100),\n    'n_estimators': [800, 1000, 1200], \n    'max_features': ['auto'],\n    'bootstrap': [False]\n}\n\n# Create a based model\nrf = RandomForestClassifier(random_state = 100)\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1,verbose = 1)","7f1e6e07":"# Fitting the model\ngrid_result = grid_search.fit(df_train_pca_inc, y_train_bal)\n\n# Finding the best model\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","2b76788e":"# Model with the best hyperparameters\n\nrfc = RandomForestClassifier(bootstrap=False,\n                             max_depth=20,\n                             min_samples_leaf=200, \n                             min_samples_split=400,\n                             max_features='auto',\n                             n_estimators=800)\n\n# Fit\nrfc.fit(df_train_pca_inc, y_train_bal)","be733044":"# Predict\ntrain_pred = rfc.predict(df_train_pca_inc)\ntest_pred = rfc.predict(df_test_pca_inc)","d3918256":"# Accuracy, precision, recall\/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","991fef16":"# fit model on training data with default hyperparameters\nxgb = XGBClassifier()\n\n# Lookin at the parameters used by our base model\nprint('Parameters currently in use:\\n')\npprint(xgb.get_params())","d6bb3830":"# Fitting the model\nxgb.fit(df_train_pca_inc,y_train_bal)\n\n# Making predictions\npredictions = xgb.predict(df_test_pca_inc)\ntrain_pred = xgb.predict(X_train_lasso)","bcb681bb":"# Accuracy, precision, recall\/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","8cde6b49":"# AUC Score\nprint(\"AUC Score on test set:\\t\" +str(round(roc_auc_score(y_test,predictions) *100,2)))","0bdc6de2":"# hyperparameter tuning with XGBoost\n\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.2, 0.6], \n             'subsample': [0.3, 0.6, 0.9]}          \n\n\n# specify model\nxgb_ht = XGBClassifier(max_depth=2, n_estimators=200)\n\n# set up GridSearchCV()\ngs = GridSearchCV(estimator = xgb_ht, param_grid = param_grid, scoring= 'roc_auc', \n                        cv = 3, verbose = 1, return_train_score=True, n_jobs = -1)     ","759b8cd2":"# Fitting the model\ngrid_result = gs.fit(df_train_pca_inc,y_train_bal) \n\n# Finding the best model\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","df850b41":"# Model with the best hyperparameters\nxgb_ht = XGBClassifier(max_depth=2, n_estimators=200, learning_rate = 0.6, subsample = 0.9)\n\n# Fit\nxgb_ht.fit(df_train_pca_inc, y_train_bal)","c1782b4b":"# Predict\ntrain_pred = xgb_ht.predict(df_train_pca_inc)\ntest_pred = xgb_ht.predict(df_test_pca_inc)","73d1cad4":"# Accuracy, precision, recall\/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","b0136736":"### Interpreatation of scores\n\nEven with the hyperparametres tunning there is some improvement with our model\n\n1. Precision on test set:\t36.69%\n2. Recall on test set:\t72.39%\n3. Training Accuracy: 83.51%\n4. Test Accuracy: 86.18%\n\n\n- Precision score is still low and recall score is good, which means the model has a tendency for classifying most of the customers to churn, even though the customers are not likely to churn ( actual churn \/ total predicted churn ). In this case, As most of the customers are classified that they would churn hence, the model would capture all the customers who would likely churn.\n- Observing test and training score, we can be sure that model is not overfitted\n\n<b> Verdict: Many customers who would not likely to churn would recive offers, which would lead to loss of revenue for the organization <\/b>","4871d5d9":"From the grapgh we can infer that 30 compenents would be ideal","301d6759":"### Interpreatation of scores for Logistic regression ( default and hyperparametre tuned )\n\n1. Precision on test set:\t32.18%\n2. Recall on test set:\t79.63%\n3. Training Accuracy: 83.59%\n4. Test Accuracy: 82.94% \n\n\n- Precision score is low and recall score is good, which means the model has a tendency for classifying most of the customers to churn, even though the customers are not likely to churn ( actual churn \/ total predicted churn ). In this case, As most of the customers are classified that they would churn hence, the model would capture all the customers who would likely churn.\n- Observing test and training score, we can be sure that model is not overfitted\n\n<b> Verdict: Many customers who would not likely to churn would recive offers, which would lead to loss of revenue for the organization <\/b>","55ec8400":"**Fitting the final model with the best parameters obtained from grid search.**","1840dfcc":"## 2.3 High Accuracy Model 3 - Using XgBoost","21f9732e":"#### Fitting the final model with the best parameters obtained from grid search.","9035ce0b":"## Note: Random Forest with hyper tuning would take more than 1 hour for execution","cab7e4d4":"### Random Forrest Base Model - Default Parameters","90e95700":"### XgBoost - Hyperparameter Tuning","061c9611":"## Significant features that would help in identifying the churn","db39def7":"#### Selecting c = 0.001 to have 18 important features","fee8f8aa":"#### Creating a hyperparameter grid","805dab68":"### Balancing","38e47d34":"### XgBoost - Hyperparameter Tuning","36b0ce9d":"### Feature Selection using Lasso Logistic Regression","05630d41":"Data is now balanced","9285f08f":"## 1.3 Interpretable Model 3 - Using XgBoost","7ffd8d98":"### Random Forrest - Hyperparameter Tuning","8f13d574":"#### Since we have outliers in most of the columns we will do imputation of missing values using median","44253d09":"### Random Forrest - Hyperparameter Tuning","cfcedcee":"### Logistic Regression Base Model - Default parameters","43eaef23":"### XgBoost Base Model - Default Parameters","c9d009ae":"### Derving new total columns by combing month 6 and 7","cc231610":"After capping many columns have only 1 unique value. So dropping these columns","0d967ee2":"## Verifying there is no correlation exist after PCA","cd49e81d":"### Logistic Regression - Hyperparameter Tuning","1d82e01b":"### Interpreatation of scores\n\nThis model has no significant improvement apart from the the training accuracy\n\n1. Precision on test set:\t62.38%\n2. Recall on test set:\t54.11%\n3. Training Accuracy: 95.92%\n4. Test Accuracy: 92.88%\n\n\n- Precision score is better and recall score is not good, which means the model has a tendency for classifying most of the customers will not churn, even though the customers are likely to churn. This model is not good for the objective we have. As we dont need our model to predict customers who are about to churn as this would impact the business more.\n- Observing test and training score, looks like model is not overfitted and performs better with test data\n\n<b> Verdict: This model is not suitable as we would miss 50% of customers who would likely to churn <\/b>","4bf5f2ee":"## Conclusion:\n\nBased on the model's performance, Model built with <b><i> Random forest using PCA  <\/i> <\/b> helps us in identyfing upto 70% of the customers who are about to churn, though we have identified some customers would churn even though they are not would not cause much harm as providing the offers to them also helps in keeping the revenue up rather than losing the customers\n\nPCA helps us in reducing the dimensions which would further reduce the model building","d01937ee":"### Deriving total recharge amount for month 6 and 7","68e3bebb":"#### Creating a hyperparameter grid","1c65f50a":"# 1. Interpretable Models - Without PCA","aa022bfe":"#### Creating a hypertuned logistic regression model","18c5388a":"We see presence of outliers, hence performing capping operation","0b497acc":"**Fitting the final model with the best parameters obtained from grid search.**","d5c9fe4f":"We can see high correlation between month 6,7 and 8 features","9bad7ac8":"### Interpreatation of scores\n\n1. Precision on test set:\t31.29%\n2. Recall on test set:\t78.9%\n3. Training Accuracy: 83.51%\n4. Test Accuracy: 82.39%\n\n\n- Precision score is low and recall score is good, which means the model has a tendency for classifying most of the customers to churn, even though the customers are not likely to churn ( actual churn \/ total predicted churn ). In this case, As most of the customers are classified that they would churn hence, the model would capture all the customers who would likely churn.\n- Observing test and training score, we can be sure that model is not overfitted\n\n<b> Verdict: Many customers who would not likely to churn would recive offers, which would lead to loss of revenue for the organization <\/b>","6a304fa7":"### Logistic Regression Base Model - Default parameters","2d13626c":"The dataset is higly imbalnced ","c26be9bd":"### XgBoost Base Model - Default Parameters","2ff4b8e4":"## 2.2 High Accuracy Model 2 - Random Forrest","03ab824c":"#### Building the model around the random parameter obtained","2a57015e":"### Interpreatation of scores\n\nEven with the hyperparametres tunning there is significant improvement with our random forest model\n\n1. Precision on test set:\t43.19%\n2. Recall on test set:\t72.02%\n3. Training Accuracy: 87.78%\n4. Test Accuracy: 88.88%\n\n\n- Precision score is better than logistic regression and recall score is good too, This model would identify most of the customers who are about churn and also the model would relativly less likely to miss classify non churn customers leading to significant loss in revenue\n- Observing test and training score, we can be sure that model is not overfitted which would likely to expect in the tree based models\n\n<b> Verdict: some customers who would not likely to churn would recive offers, which would lead to loss of revenue for the organization to some extent, If we can improve our precision it would be more benificial and this model would take more computational resorce and time consuming hence it can't make prediction quickly<\/b>","ef698b59":"### Manually finding what would be ideal number of components*","e810eca8":"### As per the model the following features are important\n\n1. arpu_8\n2. loc_og_t2t_mou_8\n3. loc_og_t2m_mou_8\n4. loc_og_t2f_mou_8\n5. spl_og_mou_8\n6. total_og_mou_8\n7. loc_ic_t2f_mou_8\n8. std_ic_t2f_mou_8\n9. total_ic_mou_8\n10. ic_others_8\n11. total_rech_num_8\n12. max_rech_amt_8\n13. last_day_rch_amt_8\n14. vol_2g_mb_8\n15. aon_yr\n16. arpu_6_7\n17. roam_og_mou_6_7\n18. std_og_mou_6_7","7237e25f":"## 2.1 High Accuracy Model 1 - Using Logistic Regression","51977858":"### Interpreatation of scores\n\nThis model has no significant improvement apart from the the training accuracy\n\n1. Precision on test set:\t62.38%\n2. Recall on test set:\t54.11%\n3. Training Accuracy: 95.92%\n4. Test Accuracy: 92.88%\n\n\n- Precision score is better and recall score is not good, which means the model has a tendency for classifying most of the customers will not churn, even though the customers are likely to churn. This model is not good for the objective we have. As we dont need our model to predict customers who are about to churn as this would impact the business more.\n- Observing test and training score, looks like model is not overfitted and performs better with test data\n\n<b> Verdict: This model is not suitable as we would miss 50% of customers who would likely to churn <\/b>","028592d6":"### Dropping month 9 columns ","dc4ca4a7":"# Telecom Churn Case Study","ec25440f":"## 1.2 Interpretable Model 2 - Random Forrest","3874ed3c":"### Deriving churn flag using month 9 data","461a0c56":"#### Building the model around the random parameter obtained","a4c53294":"### According to EDA\n\n1. arpu_8          \n2. total_rech_amt_8\n3. total_ic_mou_8  \n4. total_og_mou_8  \n\nThe above mentioned features are the top features which are corelated with Churn","1090cffb":"All missing values imputed","9a13f228":"**Fitting the final model with the best parameters obtained from grid search.**","9a99437e":"### 1. Data Understanding and Cleaning\n\nLet's first have a look at the dataset and understand the size, attribute names etc.","6ba0b0f1":"#### Using Screeplot for identifying the component size","885e67b3":"## Deriving New columns","3a102ac6":"# Model Building","a5b1dfe4":"### Interpreatation of scores\n\n1. Precision on test set:\t62.38%\n2. Recall on test set:\t54.11%\n3. Training Accuracy: 83.51%\n4. Test Accuracy: 92.39%\n\n\n- Precision score is better and recall score is not good, which means the model has a tendency for classifying most of the customers will not churn, even though the customers are likely to churn. This model is not good for the objective we have. As we dont need our model to predict customers who are about to churn as this would impact the business more.\n- Observing test and training score, looks like model is not overfitted and performs better with test data\n\n<b> Verdict: This model is not suitable as we would miss 50% of customers who would likely to churn <\/b>","2acd2f85":"### Interpreatation of scores\n\nThere is no significant improvement in the model\n\n1. Precision on test set:\t43.19%\n2. Recall on test set:\t72.02%\n3. Training Accuracy: 92.83%\n4. Test Accuracy: 88.88%\n\n\n- Precision score is low and recall score is good, which means the model has a tendency for classifying most of the customers to churn, even though the customers are not likely to churn. In this case, As most of the customers are classified that they would churn hence, the model would capture all the customers who would likely churn.\n- Observing test and training score, we can be sure that model is not overfitted\n\n<b> Verdict: Many customers who would not likely to churn would recive offers, which would lead to loss of revenue for the organization upto some extent <\/b>","6fca0088":"### Interpreatation of scores for Random Forest ( deafault and hypertuned model )\n\n\n1. Precision on test set:\t43.19%\n2. Recall on test set:\t72.02%\n3. Training Accuracy: 92.83%\n4. Test Accuracy: 88.88%\n\n\n- Precision score is low and recall score is good, which means the model has a tendency for classifying most of the customers to churn, even though the customers are not likely to churn. In this case, As most of the customers are classified that they would churn hence, the model would capture all the customers who would likely churn.\n- Observing test and training score, we can be sure that model is not overfitted\n\n<b> Verdict: Many customers who would not likely to churn would recive offers, though it would be better than losing the customer <\/b>","0dad2151":"#### Creating a hypertuned logistic regression model","204a7b6b":"### Random Forrest Base Model - Default Parameters","1f3f58f6":"Churn column is also imbalanced, so we will not cap it, we will balance the dataset later","17cdf38f":"### Scaling","508d3fe0":"### Logistic Regression - Hyperparameter Tuning","f0bf8489":"# 2. High Accuracy Models - Using PCA","dfa80d1f":"#### Using incremental PCA","db858f0d":"## 1.1 Interpretable Model 1 - Logistic Regression"}}