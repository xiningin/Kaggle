{"cell_type":{"677dd581":"code","248076e8":"code","b4fec030":"code","656bdb4d":"code","c1ae6d3f":"code","b7982fd2":"code","f1f2895c":"code","fbe9efeb":"code","8ca163f6":"code","16477f4b":"code","4d782c41":"code","d96bac30":"code","8361808d":"code","12a6347c":"code","358e3b24":"code","6e5c311c":"code","8f523a4a":"code","3353bd58":"code","8e2836b7":"code","5b4e40a2":"code","d726733a":"code","ae964757":"code","f91f8c3c":"code","2e744a96":"code","38d95d28":"code","d2a8183f":"code","e4c6e21e":"code","90be856f":"code","e340fe90":"markdown","9da8e8e8":"markdown","2c84436a":"markdown","ba9c7237":"markdown","274d64cf":"markdown","865aafbf":"markdown","5c5d2030":"markdown","b9f1d487":"markdown","8d6d693b":"markdown","ad4e35e4":"markdown","8d921927":"markdown","7e3852e0":"markdown","fa6d5cae":"markdown","13a3899d":"markdown","25c0bbc7":"markdown","90c5f9c5":"markdown","31dd40fb":"markdown","65a74b06":"markdown","bf89560c":"markdown","1ff4c03c":"markdown","6e76cc5a":"markdown","c386a2ab":"markdown","75e40c31":"markdown","631eeec6":"markdown","9fca3d7c":"markdown","a507d727":"markdown","b33c9276":"markdown","409b8e07":"markdown","e20e56e5":"markdown","fe110150":"markdown"},"source":{"677dd581":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Import statements required for Plotly \nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (accuracy_score, log_loss, classification_report)\nfrom imblearn.over_sampling import SMOTE\nimport xgboost\n\n# Import and suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')","248076e8":"attrition = pd.read_csv('..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nattrition.head()","b4fec030":"# Looking for NaN\ndisplay(attrition.isnull().any())","656bdb4d":"# Plotting the KDEplots\nf, axes = plt.subplots(3, 3, figsize=(10, 8), \n                       sharex=False, sharey=False)\n\n# Defining our colormap scheme\ns = np.linspace(0, 3, 10)\ncmap = sns.cubehelix_palette(start=0.0, light=1, as_cmap=True)\n\n# Generate and plot\nx = attrition['Age'].values\ny = attrition['TotalWorkingYears'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True, cut=5, ax=axes[0,0])\naxes[0,0].set( title = 'Age against Total working years')\n\ncmap = sns.cubehelix_palette(start=0.333333333333, light=1, as_cmap=True)\n# Generate and plot\nx = attrition['Age'].values\ny = attrition['DailyRate'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[0,1])\naxes[0,1].set( title = 'Age against Daily Rate')\n\ncmap = sns.cubehelix_palette(start=0.666666666667, light=1, as_cmap=True)\n# Generate and plot\nx = attrition['YearsInCurrentRole'].values\ny = attrition['Age'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[0,2])\naxes[0,2].set( title = 'Years in role against Age')\n\ncmap = sns.cubehelix_palette(start=1.0, light=1, as_cmap=True)\n# Generate and plot\nx = attrition['DailyRate'].values\ny = attrition['DistanceFromHome'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,0])\naxes[1,0].set( title = 'Daily Rate against DistancefromHome')\n\ncmap = sns.cubehelix_palette(start=1.333333333333, light=1, as_cmap=True)\n# Generate and plot\nx = attrition['DailyRate'].values\ny = attrition['JobSatisfaction'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,1])\naxes[1,1].set( title = 'Daily Rate against Job satisfaction')\n\ncmap = sns.cubehelix_palette(start=1.666666666667, light=1, as_cmap=True)\n# Generate and plot\nx = attrition['YearsAtCompany'].values\ny = attrition['JobSatisfaction'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,2])\naxes[1,2].set( title = 'Daily Rate against distance')\n\ncmap = sns.cubehelix_palette(start=2.0, light=1, as_cmap=True)\n# Generate and plot\nx = attrition['YearsAtCompany'].values\ny = attrition['DailyRate'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,0])\naxes[2,0].set( title = 'Years at company against Daily Rate')\n\ncmap = sns.cubehelix_palette(start=2.333333333333, light=1, as_cmap=True)\n# Generate and plot\nx = attrition['RelationshipSatisfaction'].values\ny = attrition['YearsWithCurrManager'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,1])\naxes[2,1].set( title = 'Relationship Satisfaction vs years with manager')\n\ncmap = sns.cubehelix_palette(start=2.666666666667, light=1, as_cmap=True)\n# Generate and plot\nx = attrition['WorkLifeBalance'].values\ny = attrition['JobSatisfaction'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,2])\naxes[2,2].set( title = 'WorklifeBalance against Satisfaction')\n\nf.tight_layout()","c1ae6d3f":"# Define a dictionary for the target mapping\ntarget_map = {'Yes':1, 'No':0}\n# Use the pandas apply method to numerically encode our attrition target variable\nattrition[\"Attrition_numerical\"] = attrition[\"Attrition\"].apply(lambda x: target_map[x])","b7982fd2":"# creating a list of only numerical values\nnumerical = [u'Age', u'DailyRate', u'DistanceFromHome', \n             u'Education', u'EmployeeNumber', u'EnvironmentSatisfaction',\n             u'HourlyRate', u'JobInvolvement', u'JobLevel', u'JobSatisfaction',\n             u'MonthlyIncome', u'MonthlyRate', u'NumCompaniesWorked',\n             u'PercentSalaryHike', u'PerformanceRating', u'RelationshipSatisfaction',\n             u'StockOptionLevel', u'TotalWorkingYears',\n             u'TrainingTimesLastYear', u'WorkLifeBalance', u'YearsAtCompany',\n             u'YearsInCurrentRole', u'YearsSinceLastPromotion',u'YearsWithCurrManager']\ndata = [\n    go.Heatmap(\n        z= attrition[numerical].astype(float).corr().values, # Generating the Pearson correlation\n        x=attrition[numerical].columns.values,\n        y=attrition[numerical].columns.values,\n        colorscale='Viridis',\n        reversescale = False,\n#         text = True ,\n        opacity = 1.0\n        \n    )\n]\n\n\nlayout = go.Layout(\n    title='Pearson Correlation of numerical features',\n    xaxis = dict(ticks='', nticks=36),\n    yaxis = dict(ticks='' ),\n    width = 900, height = 700,\n    \n)\n\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='labelled-heatmap')","f1f2895c":"# Refining our list of numerical variables\nnumerical = [u'Age', u'DailyRate',  u'JobSatisfaction',\n       u'MonthlyIncome', u'PerformanceRating',\n        u'WorkLifeBalance', u'YearsAtCompany', u'Attrition_numerical']\n\ng = sns.pairplot(attrition[numerical], hue='Attrition_numerical', palette='seismic', diag_kind = 'kde',diag_kws=dict(shade=True))\ng.set(xticklabels=[])","fbe9efeb":"# Drop the Attrition_numerical column from attrition dataset first - Don't want to include that\nattrition = attrition.drop(['Attrition_numerical'], axis=1)\n\n# Empty list to store columns with categorical data\ncategorical = []\nfor col, value in attrition.iteritems():\n    if value.dtype == 'object':\n        categorical.append(col)\n\n# Store the numerical columns in a list numerical\nnumerical = attrition.columns.difference(categorical)","8ca163f6":"numerical","16477f4b":"# Store the categorical data in a dataframe called attrition_cat\nattrition_cat = attrition[categorical]\nattrition_cat = attrition_cat.drop(['Attrition'], axis=1) # Dropping the target column","4d782c41":"attrition_cat = pd.get_dummies(attrition_cat)\nattrition_cat.head(3)","d96bac30":"# Store the numerical features to a dataframe attrition_num\nattrition_num = attrition[numerical]","8361808d":"# Concat the two dataframes together columnwise\nattrition_final = pd.concat([attrition_num, attrition_cat], axis=1)","12a6347c":"# Define a dictionary for the target mapping\ntarget_map = {'Yes':1, 'No':0}\n# Use the pandas apply method to numerically encode our attrition target variable\ntarget = attrition[\"Attrition\"].apply(lambda x: target_map[x])\ntarget.head(3)","358e3b24":"data = [go.Bar(\n            x=attrition[\"Attrition\"].value_counts().index.values,\n            y= attrition[\"Attrition\"].value_counts().values\n    )]\n\npy.iplot(data, filename='basic-bar')","6e5c311c":"# Import the train_test_split method\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.cross_validation import StratifiedShuffleSplit\n\n# Split data into train and test sets as well as for validation and testing\ntrain, test, target_train, target_val = train_test_split(attrition_final, \n                                                         target, \n                                                         train_size= 0.80,\n                                                         random_state=0);\n#train, test, target_train, target_val = StratifiedShuffleSplit(attrition_final, target, random_state=0);","8f523a4a":"oversampler=SMOTE(random_state=0)\nsmote_train, smote_target = oversampler.fit_sample(train,target_train)","3353bd58":"seed = 0   # We set our random seed to zero for reproducibility\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 1000,\n#     'warm_start': True, \n    'max_features': 0.3,\n    'max_depth': 4,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'random_state' : seed,\n    'verbose': 0\n}","8e2836b7":"rf = RandomForestClassifier(**rf_params)","5b4e40a2":"rf.fit(smote_train, smote_target)\nprint(\"Fitting of Random Forest finished\")","d726733a":"rf_predictions = rf.predict(test)\nprint(\"Predictions finished\")","ae964757":"print(\"Accuracy score: {}\".format(accuracy_score(target_val, rf_predictions)))\nprint(\"=\"*80)\nprint(classification_report(target_val, rf_predictions))","f91f8c3c":"# Scatter plot \ntrace = go.Scatter(\n    y = rf.feature_importances_,\n    x = attrition_final.columns.values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 13,\n        #size= rf.feature_importances_,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = rf.feature_importances_,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = attrition_final.columns.values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n     xaxis= dict(\n         ticklen= 5,\n         showgrid=False,\n        zeroline=False,\n        showline=False\n     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        showgrid=False,\n        zeroline=False,\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","2e744a96":"from sklearn import tree\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\nimport re\n\ndecision_tree = tree.DecisionTreeClassifier(max_depth = 4)\ndecision_tree.fit(train, target_train)\n\n# Predicting results for test dataset\ny_pred = decision_tree.predict(test)\n\n# Export our trained model as a .dot file\nwith open(\"tree1.dot\", 'w') as f:\n     f = tree.export_graphviz(decision_tree,\n                              out_file=f,\n                              max_depth = 4,\n                              impurity = False,\n                              feature_names = attrition_final.columns.values,\n                              class_names = ['No', 'Yes'],\n                              rounded = True,\n                              filled= True )\n        \n#Convert .dot to .png to allow display in web notebook\ncheck_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\n\n# Annotating chart with PIL\nimg = Image.open(\"tree1.png\")\ndraw = ImageDraw.Draw(img)\nimg.save('sample-out.png')\nPImage(\"sample-out.png\", height=2000, width=1900)","38d95d28":"# Gradient Boosting Parameters\ngb_params ={\n    'n_estimators': 1500,\n    'max_features': 0.9,\n    'learning_rate' : 0.25,\n    'max_depth': 4,\n    'min_samples_leaf': 2,\n    'subsample': 1,\n    'max_features' : 'sqrt',\n    'random_state' : seed,\n    'verbose': 0\n}","d2a8183f":"gb = GradientBoostingClassifier(**gb_params)\n# Fit the model to our SMOTEd train and target\ngb.fit(smote_train, smote_target)\n# Get our predictions\ngb_predictions = gb.predict(test)\nprint(\"Predictions have finished\")","e4c6e21e":"print(accuracy_score(target_val, gb_predictions))\nprint(classification_report(target_val, gb_predictions))","90be856f":"# Scatter plot \ntrace = go.Scatter(\n    y = gb.feature_importances_,\n    x = attrition_final.columns.values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 13,\n        #size= rf.feature_importances_,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = gb.feature_importances_,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = attrition_final.columns.values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Gradient Boosting Model Feature Importance',\n    hovermode= 'closest',\n     xaxis= dict(\n         ticklen= 5,\n         showgrid=False,\n        zeroline=False,\n        showline=False\n     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        showgrid=False,\n        zeroline=False,\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter')","e340fe90":"The next step after prepping our Random Forest model would be to start building a forest of trees using our training set and fitting it to our attrition target variable. We do so by simply using the **fit** call as follows","9da8e8e8":"Having defined our parameters, we can now apply the usual fit and predict methods on our train and test sets respectively ","2c84436a":"As evinced from the dataset, our target column with which we can point our model to train on would be the \"Attrition\" column. \n\nFurthermore, we see that we have a mix of numerical and categorical data types. For the categorical columns, we shall handle their numerical encoding in the latter chapter. This section will be devoted to data exploration and as a first step, let us quickly carry our some simple data completeness checks to see if there are nulls or infinite values in the data\n\n**Data quality checks**\n\nTo look for any null values, we can just invoke the **isnull** call as follows","ba9c7237":"# 2. Feature Engineering & Categorical Encoding\n\nHaving carried out a brief exploration into the dataset, let us now proceed onto the task of Feature engineering and numerically encoding the categorical values in our dataset. Feature engineering in a nutshell involves creating new features and relationships from the current features that we have. Feature engineering has been quite \n\nTo start off, we shall segregate numerical columns from categorical columns via the use of the dtype method as follows:","274d64cf":"Having defined our parameters, we can initialise a Random Forest object by using scikit-learn's **RandomForestClassifier** and unpacking the parameters by adding the double asterisks symbols as follows","865aafbf":"### Feature Ranking via the Gradient Boosting Model\n\nMuch like the Random Forest, we can invoke the feature_importances_ attribute of the gradient boosting model and dump it in an interactive Plotly chart","5c5d2030":"Having identified which of our features contain categorical data, we can set about numerically encoding the data. To do this, I shall use the **get_dummies** method from Pandas which creates encoded dummy variables from the categorical variables.","b9f1d487":"Applying the **get_dummies** method, we see that we have encoded our categorical values conveniently by just applying one line of Python code. ","8d6d693b":"# 3. Implementing Machine Learning Models\n\nHaving performed some exploratory data analysis and simple feature engineering as well as having ensured that all categorical values are encoded, we are now ready to proceed onto building our models.\n\nAs alluded to in the introduction of this notebook, we will aim to evaluate and contrast the performances of a handful of different learning models. \n\n**Splitting Data into Train and Test sets**\n\nBut before we even start training a model, we will have to partition our dataset into a training set and a test set (unlike Kaggle competitions where the train and test data are already segregated for you). To split our data we will utilise sklearn's ","ad4e35e4":"### Pairplot Visualisations\n\nNow let us create some Seaborn pairplots and set it against the target variable which is our Attrition column to get a feel for how the various features are distributed vis-a-vis employee attrition","8d921927":"**Takeaway from the plots**\n\nFrom the correlation plots, we can see that quite a lot of our columns seem to be poorly correlated with one another. Generally when making a predictive model, it would be preferable to train a model with features that are not too correlated with one another so that we do not need to deal with redundant features. In the case that we have quite a lot of correlated features one could perhaps apply a technique such as Principal Component Analysis (PCA) to reduce the feature space.","7e3852e0":"**Target variable**\n\nOne final step that that we have to remember is to generate our target variable. The target in this case is given by the column **Attrition** which contains categorical variables therefore requires numerical encoding. We numerically encode it by creating a dictionary with the mapping given as 1 : Yes and 0 : No","fa6d5cae":"Having encoded our categorical columns as well as engineering and created some new features from the numerical data, we can now proceed to merging both dataframes into a final set with which we will train and test our models on. ","13a3899d":"**SMOTE to oversample due to the skewness in target**\n\nSince we have already noted the severe imbalance in the values within the target variable, let us implement the SMOTE method in the dealing with this skewed value via the imblearn Python package.","25c0bbc7":"### Correlation of Features\n\nThe next tool in a data explorer's arsenal is that of a correlation matrix. By plotting a correlation matrix, we have a very nice overview of how the features are related to one another. For a Pandas dataframe, we can conveniently use the call **.corr** which by default provides the Pearson Correlation values of the columns pairwise in that dataframe.\n\nIn this correlation plot, I will use the the Plotly library to produce a interactive Pearson correlation matrix via the Heatmap function as follows:","90c5f9c5":"### Visualising Tree Diagram with Graphviz\n\nLet us now visualise how a single decision tree traverses the features in our data as the DecisionTreeClassifier object of sklearn comes with a very convenient **export_graphviz** method that exports the tree diagram into a .png format which you can view from the output of this kernel.","31dd40fb":"**Initialising Random Forest parameters**\n\nWe will utilise the Scikit-learn library to construct a Random Forest model. To do so, we have to first define our set of parameters that we will feed into our Random Forest classifier as follows","65a74b06":"However just by a quick inspection of the counts of the number of 'Yes' and 'No' in the target variable tells us that there is quite a large skew in target as shown","bf89560c":"**Creating new features from Numerical data**","1ff4c03c":"### Distribution of the dataset\n\nGenerally one of the first few steps in exploring the data would be to have a rough idea of how the features are distributed with one another. To do so, I shall invoke the familiar **kdeplot** function from the Seaborn plotting library and this generates bivariate plots as follows:","6e76cc5a":"Scoring the model","c386a2ab":"### CONCLUSION\n\nWe have constructed a very simple pipeline of predicting employee attrition, from some basic Exploratory Data Analysis to feature engineering as well as implementing two learning models in the form of a Random Forest and a Gradient Boosting classifier. This whole notebook takes less than a minute to run and it even returns a 89% accuracy in its predictions.\n\nThat being said, there is quite a lot of room for improvement. For one, more features could be engineered from the data.  Furthermore one could squeeze performance out of this pipeline by perhaps using some form of blending or stacking of models. I myself am quite keen to implement a classifier voting where a handful of classifiers votes on the outcome of the predictions and we take the majority vote. ","75e40c31":"### Feature Ranking via the Random Forest \n\nThe Random Forest classifier in Sklearn also contains a very convenient  attribute **feature_importances_** which tells us which features within our dataset has been given most importance through the Random Forest algorithm. Shown below is an Interactive Plotly diagram of the various feature importances.","631eeec6":"----\n## A. Random Forest Classifier \n\nThe Random Forest method, first introduced by Breiman in 2001 can be grouped under the category of ensemble models. Why ensemble? The building block of a Random Forest is the ubiquitous Decision Tree. The decision tree as a standalone model is often considered a \"weak learner\" as its predictive performance is relatively poor. However a Random Forest gathers a group (or ensemble) of decision trees and uses their combined predictive capabilities to obtain relatively strong predictive performance - \"strong learner\". \n\nThis principle of using a collection of \"weak learners\" to come together to create a \"strong learner\" underpins the basis of ensemble methods which one regularly comes across in Machine learning. For a really good read that drives home the basics of the Random Forest, refer to this [CitizenNet blog][1]\n\n\n  [1]: http:\/\/blog.citizennet.com\/blog\/2012\/11\/10\/random-forests-ensembles-and-performance-metrics","9fca3d7c":"# Introduction \n\nThe issue of keeping one's employees happy and satisfied is a perennial and age-old challenge. If an employee you have invested so much time and money leaves for \"greener pastures\",  then this would mean that you would have to spend even more time and money to hire somebody else. In the spirit of Kaggle, let us therefore turn to our predictive modelling capabilities and see if we can predict employee attrition on this synthetically generated IBM dataset. \n\nThis notebook is structured as follows:\n\n 1. **Exploratory Data Analysis** : In this section, we explore the dataset by taking a look at the feature distributions, how correlated one feature is to the other and create some Seaborn and Plotly visualisations\n 2. **Feature Engineering and Categorical Encoding** : Conduct some feature engineering as well as encode all our categorical features into dummy variables\n 3. **Implementing Machine Learning models** : We implement a Random Forest and a Gradient Boosted Model after which we look at feature importances from these respective models","a507d727":"----\n\n## B. Gradient Boosted Classifier\n\nGradient Boosting is also an ensemble technique much like the Random Forest where a combination of weak Tree learners are brought together to form a relatively stronger learner. The technique involves defining some sort of function (loss function) that you want minimised and an method\/algorithm to minimise this. Therefore as the name suggests, the algorithm used to minimise the loss function is that of a gradient descent method which adds decision trees which \"point\" in the direction that reduces our loss function (downward gradient).\n\nTo set up a Gradient Boosting classifier is easy enough in Sklearn and it involves only a handful of lines of code. Again we first set up our classifier's parameters\n\n**Initialising Gradient Boosting Parameters** \n\nIn general there are a handful of key parameter when setting up tree-based or gradient boosted models. These are always going to be the number of estimators, the maximum depth with which you want your model to be trained to, and the minimum samples per leaf","b33c9276":"Therefore we have to keep in mind that there is quite a big imbalance in our target variable. Many statistical techniques have been put forth to treat imbalances in data (oversampling or undersampling). In this notebook, I will use an oversampling technique known as SMOTE to treat this imbalance.","409b8e07":"Having fitted our forest of trees with our parameters to the training set against our target variable, we now have a learning model **rf** which we can make predictions out of. To use our Random Forest in predicting against our test data, we can use sklearn's **.predict** method as follows","e20e56e5":"**Accuracy of the model**\n\nAs observed, our Random Forest returns an accuracy of approx 88% for its predictions and on first glance this might seem to be a pretty good performing model. However when we think about how skewed our target variable where the distribution of yes and no's are 84% and 26%, therefore our model is only predicting slightly better than random guessing. \n\nIt would be more informative to balance out the precision and recall scores as show in the classification report outputs. Where it falls down to the business considerations over whether one should prioritise for a metric over the other - i.e. your Precision vs Recall.","fe110150":"# 1. Exploratory Data Analysis\n\nLet us load in the dataset via the trusty Pandas package into a dataframe object and have a quick look at the first few rows"}}