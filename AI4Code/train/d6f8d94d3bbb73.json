{"cell_type":{"78421a3c":"code","eac4da74":"code","38996330":"code","9a260c2c":"code","46470306":"code","af1da697":"code","df8fb877":"code","05513983":"code","510764b2":"code","7cde6672":"code","ad027627":"code","36ef2eeb":"code","b0d2d8c0":"code","9a1a8d54":"code","3d7b408f":"code","ad963df5":"code","2c6a0e06":"code","5f85e9ce":"code","efc4708e":"code","683818eb":"code","e0696bd1":"code","eac41449":"code","b6ff2362":"code","b47b9f14":"code","3d3b4a22":"code","50301e46":"code","6da2d562":"code","62ad34fc":"code","e1061d03":"code","d18b6ef4":"code","8bf97a9a":"code","f67fbafd":"code","65343e3e":"code","bb5f2246":"markdown","21f455cd":"markdown","1155fba0":"markdown","347d90cc":"markdown","901ed17b":"markdown","ffcc7758":"markdown","18ec1272":"markdown","1518ce23":"markdown","7fdc552e":"markdown","c96dde28":"markdown","d5aab49e":"markdown","13b68dc8":"markdown","35b82b12":"markdown","c310090c":"markdown","fa2fccf9":"markdown","a7d5847a":"markdown","a5130203":"markdown","2ec17eb7":"markdown","56b6964d":"markdown","c912e9b0":"markdown","2adad8ff":"markdown","e0c95588":"markdown","96e008c8":"markdown","11ae6626":"markdown","7a5694d9":"markdown","d76480bf":"markdown","39da5bab":"markdown","de58b98b":"markdown"},"source":{"78421a3c":"import numpy as np \nimport pandas as pd\nimport cv2\nimport torch\nimport torchvision\nfrom torchvision import utils as vutils\nfrom torchvision import transforms\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils import data\nfrom torch.optim import SGD, Adam\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nfrom statistics import mean","eac4da74":"'''\nExtract number of positive vs negative samples (where negative samples have an all-zero mask)\nCheck for files without corresponding mask and vice versa\n'''\nroot = '..\/input\/lgg-mri-segmentation\/kaggle_3m'\n\nno_mask = 0\nno_mask_files = []\nno_file = 0\nno_files = []\nnum_empty_masks = 0\nnum_nonempty_masks = 0\n\nS = 3 # number of pos\/neg samples to take\nempty_mask_samples = []\nnonempty_mask_samples = []\n\nimg_dimensions = []\nmsk_dimensions = []\n\nn_files = 0\nfor directory in [os.path.join(root,x) for x in os.listdir(root) if os.path.isdir(os.path.join(root, x))]:\n    for file in os.listdir(directory):\n        n_files += 1\n        img_dimensions.append(np.array(cv2.imread(os.path.join(directory, file))).shape)\n        #count files with no mask\n        if 'mask' not in file:\n            #check if mask exists\n            mask_path = os.path.join(directory, file[:file.find('.tif')]+'_mask.tif')\n            if not os.path.exists(mask_path):\n                no_mask += 1\n                no_mask_files.append(os.path.join(directory, file))\n        else:\n            msk_dimensions.append(np.array(cv2.imread(os.path.join(directory, file), cv2.IMREAD_UNCHANGED)).shape)\n            #count masks with no file\n            f_path = os.path.join(directory, file[:file.find('mask')-1]+'.tif')\n            #check if file exists\n            if not os.path.exists(f_path):\n                no_file += 1\n                no_files.append(os.path.join(directory, file))\n                \n            #check if mask is empty\n            j = np.max(cv2.imread(os.path.join(directory, file), cv2.IMREAD_UNCHANGED))\n            if j > 0:\n                num_nonempty_masks += 1\n                if len(nonempty_mask_samples) < S:\n                    nonempty_mask_samples.append(os.path.join(directory, file))\n            else:\n                num_empty_masks += 1\n                if len(empty_mask_samples) < S:\n                    empty_mask_samples.append(os.path.join(directory, file))","38996330":"#check variance in dimensions\nprint('Image Dimensions')\nprint(len(set(img_dimensions)))\nprint(set(img_dimensions))\nprint('Mask Dimensions')\nprint(len(set(msk_dimensions)))\nprint(set(msk_dimensions))","9a260c2c":"print(f'#No Mask: {no_mask}')\nprint(f'#Mask w\/ No File: {no_file}')\nprint(f'#Total Files {n_files}')\n\nplt.figure()\nplt.title('Num Samples with Pos\/Neg Diagnosis')\nplt.bar([0,1], [num_empty_masks, num_nonempty_masks], tick_label=['Negative', 'Positive'])","46470306":"#display images of negative and positive samples - negative samples have an empty mask\nfig, axs = plt.subplots(6,2, figsize=(15,15))\nfor ax, col in zip(axs[0], ['MRI', 'Mask']):\n    ax.set_title(col)\n    \ndisplay_imgs = empty_mask_samples+nonempty_mask_samples #concat lists for easy plotting\n#print(display_imgs)\nfor i in range(len(display_imgs)):\n    mask_path = display_imgs[i]\n    img_path = mask_path[:mask_path.find('mask')-1]+'.tif'\n    axs[i,0].imshow(np.array(cv2.imread(img_path)))\n    axs[i,1].imshow(np.array(cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)))","af1da697":"#store list of train files for dataset creation of the form (train_img, mask_img, pos_neg_result(1\/0))\n#these tuples are fed into the datasets we create below\nfile_list = []\nfor directory in [os.path.join(root,x) for x in os.listdir(root) if os.path.isdir(os.path.join(root, x))]:\n    for file in os.listdir(directory):\n        #add files to list\n        if 'mask' not in file:\n            result = 0\n            img_path = os.path.join(directory, file)\n            mask_path = os.path.join(directory, file[:file.find('.tif')]+'_mask.tif')\n                \n            #check if mask is nonempty\n            if np.max(cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)) > 0:\n                result = 1\n            \n            file_list.append([img_path, mask_path, result])\n            ","df8fb877":"class Brain_MRI_Classification_Dataset(data.Dataset):\n    def __init__(self, inputs, transform=None):\n        self.inputs = inputs\n        self.transform = transform\n        self.input_dtype = torch.float32\n        self.target_dtype = torch.float32\n        \n    def __len__(self):\n        return len(self.inputs)\n    \n    def __getitem__(self, index):\n        #for classification return only the image and the binary label\n        img_path = self.inputs[index][0]\n        image = cv2.imread(img_path)\n        x = torch.from_numpy(np.transpose(np.array(image), (2,0,1))).type(self.input_dtype)\n        y = torch.from_numpy(np.array(self.inputs[index][2])).type(self.target_dtype)\n        \n        if self.transform is not None:\n            x = self.transform(x)\n        \n        return x,y\n    \n    ","05513983":"class Brain_MRI_Segmentation_Dataset(data.Dataset):\n    def __init__(self, inputs, transform=None):\n        self.inputs = inputs\n        self.transform = transform\n        self.input_dtype = torch.float32\n        self.target_dtype = torch.float32\n        \n    def __len__(self):\n        return len(self.inputs)\n    \n    def __getitem__(self, index):\n        #for classification return only the image and the binary label\n        img_path = self.inputs[index][0]\n        mask_path = self.inputs[index][1]\n        #mask_img = cv2.normalize(cv2.imread(mask_path), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n        mask_img = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n        x = torch.from_numpy(np.transpose(np.array(cv2.imread(img_path)), (2,0,1))).type(self.input_dtype)\n        y = torch.from_numpy(np.resize(np.array(mask_img)\/255., (1,256,256))).type(self.target_dtype)\n        \n        if self.transform is not None:\n            x = self.transform(x)\n            y = self.transform(y)\n        \n        return x,y","510764b2":"def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor, threshold=0.5):\n    # You can comment out this line if you are passing tensors of equal shape\n    # But if you are passing output from UNet or something it will most probably\n    # be with the BATCH x 1 x H x W shape\n    SMOOTH = 1e-6\n    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n    labels = labels.squeeze(1)\n    \n    bin_out = torch.where(outputs > threshold, 1, 0).type(torch.int16)\n    labels = labels.type(torch.int16)\n    \n    intersection = (bin_out & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n    union = (bin_out | labels).float().sum((1, 2))         # Will be zero if both are 0\n    \n    iou = (intersection + SMOOTH) \/ (union + SMOOTH)  # We smooth our division to avoid 0\/0\n    \n    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() \/ 10  # This is equal to comparing with thresolds\n    \n    return thresholded.mean()  # Or thresholded.mean() if you are interested in average across the batch","7cde6672":"#Dice Loss Function - taken from: https:\/\/github.com\/kevinzakka\/pytorch-goodies\/blob\/master\/losses.py\ndef dice_loss(logits, true, eps=1e-7):\n    \"\"\"Computes the S\u00f8rensen\u2013Dice loss.\n    Note that PyTorch optimizers minimize a loss. In this\n    case, we would like to maximize the dice loss so we\n    return the negated dice loss.\n    Args:\n        true: a tensor of shape [B, 1, H, W].\n        logits: a tensor of shape [B, C, H, W]. Corresponds to\n            the raw output or logits of the model.\n        eps: added to the denominator for numerical stability.\n    Returns:\n        dice_loss: the S\u00f8rensen\u2013Dice loss.\n    \"\"\"\n    num_classes = logits.shape[1]\n    if num_classes == 1:\n        true_1_hot = torch.eye(num_classes + 1)[true.long().squeeze(1)]\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n        pos_prob = torch.sigmoid(logits)\n        neg_prob = 1 - pos_prob\n        probas = torch.cat([pos_prob, neg_prob], dim=1)\n    else:\n        true_1_hot = torch.eye(num_classes)[true.long().squeeze(1)]\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        probas = F.softmax(logits, dim=1)\n    true_1_hot = true_1_hot.type(logits.type())\n    dims = (0,) + tuple(range(2, true.ndimension()))\n    intersection = torch.sum(probas * true_1_hot, dims)\n    cardinality = torch.sum(probas + true_1_hot, dims)\n    dice_loss = (2. * intersection \/ (cardinality + eps)).mean()\n    return (1 - dice_loss)","ad027627":"#combined cross entropy and dice loss\ndef bce_dice_loss(output, target):\n    bce = nn.BCEWithLogitsLoss()\n    return bce(output, target) + dice_loss(output, target)","36ef2eeb":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ConvBlock, self).__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.block(x)\n\nclass DownConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DownConv, self).__init__()\n        self.sequence = nn.Sequential(\n            ConvBlock(in_channels, out_channels),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n    def forward(self, x):\n        return self.sequence(x)\n        \nclass UpConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UpConv, self).__init__()\n        self.sequence = nn.Sequential(\n            nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2),\n            ConvBlock(in_channels, out_channels)\n        )\n        \n    def forward(self, x):\n        return self.sequence(x)\n        \nclass UNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1):\n        super(UNet, self).__init__()\n        #input_dim = 256\n        self.encoder = nn.ModuleList([\n            DownConv(in_channels, 64), #128\n            DownConv(64, 128), #64\n            DownConv(128, 256), #32\n            DownConv(256, 512) #16\n        ])\n        \n        self.bottleneck = ConvBlock(512, 1024)\n        \n        #extra channels allow for concatenation of skip connections in upsampling block\n        self.decoder = nn.ModuleList([\n            UpConv(512+1024,512), #32\n            UpConv(256+512,256), #64\n            UpConv(128+256,128), #128\n            UpConv(64+128,64) #256\n        ])\n        \n        self.output_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        skips = []\n        o = x\n        for layer in self.encoder:\n            o = layer(o)\n            skips.append(o)\n        \n        o = self.bottleneck(o)\n        \n        for i, layer in enumerate(self.decoder):\n            #print(o.size())\n            o = torch.cat((skips[len(skips)-i-1],o), dim=1)\n            #print(o.size())\n            o = layer(o)\n        \n        return self.output_conv(o)","b0d2d8c0":"#depth 5 is the default UNet++ architecture, layer depth-1 is the bottleneck for the traditional UNet architecture\nclass UNetPlusPlus(nn.Module):\n    def __init__(self, input_channels=3, output_channels=1, depth=5):\n        super(UNetPlusPlus, self).__init__()\n        self.depth = depth\n        self.output_channels = output_channels\n        self.conv_map = nn.ModuleDict({})\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        \n        #construct all convolution blocks\n        #sums in input channels account for concatenated skip connections from previous j nodes\n        for i in range(self.depth):\n            for j in range(self.depth-i):\n                if i == 0 and j == 0:\n                    self.conv_map[str((i,j))] = ConvBlock(in_channels=input_channels, out_channels=32)\n                #elif i == 0:\n                 #   self.conv_map[(i,j)] = ConvBlock(in_channels=(32*j)+64, out_channels=32)\n                elif j == 0:\n                    self.conv_map[str((i,j))] = ConvBlock(in_channels=32*(2**(i-1)), out_channels=32*(2**i))\n                else:\n                    self.conv_map[str((i,j))] = ConvBlock(in_channels=((32*(2**i))*j)+32*(2**(i+1)), out_channels=32*(2**i))\n                    \n        #implement 1x1 convolutions which enable us to compute completed segmentation maps at the supervised nodes:\n        #X_0,j; j ={1,2,3,...,depth-1}\n            \n        self.output_convs = nn.ModuleList()\n        for j in range(depth-1):\n            self.output_convs.append(nn.Conv2d(32, output_channels, kernel_size=1))\n\n    def forward(self, x):\n        outputs = {}\n        for j in range(self.depth):\n            for i in range(self.depth-j):\n                #X_0,0 takes the input image\n                if i == 0 and j == 0:\n                    outputs[(i,j)] = self.conv_map[str((i,j))](x)\n                #X_i,0 convolves over the downsampled output of the previous convolution in the backbone\n                elif j == 0:\n                    outputs[(i,j)] = self.conv_map[str((i,j))]( self.pool(outputs[(i-1,j)]) )\n                else:\n                    #concatenate all skip connections from same row\n                    concats = outputs[(i,0)]\n                    for j_ in range(1,j):\n                        concats = torch.cat((outputs[(i,j_)], concats), dim=1)\n                        \n                    #concatenate upsampled output from lower depth to skip connections\n                    concats = torch.cat( (concats, self.upsample(outputs[(i+1,j-1)])), dim=1 )\n                    \n                    outputs[(i,j)] = self.conv_map[str((i,j))](concats)\n        \n        #compute averaged output\n        dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n        averaged_output = torch.zeros(x.size()[0], self.output_channels, x.size()[2], x.size()[3], requires_grad=True, device=dev)\n        for j in range(1,self.depth):\n            averaged_output =  averaged_output + self.output_convs[j-1](outputs[(0,j)])\n        \n        averaged_output = averaged_output \/ (self.depth-1)\n        \n        return averaged_output\n            \n                ","9a1a8d54":"#train model\n#batch -\n#N,3,256,256\n#N,1,256,256\n\ndef run_experiment(model_name, model, optimizer, criterion, train_loader, val_loader, device='cuda', num_epochs=50, clear_mem=True):\n    \n    #######################\n    #Train model          #\n    #######################\n    print('Model sent to ' + str(device))\n    model.to(device)\n    losses = []\n    train_scores = [] # hold IoU scores\n    iters = 0\n    for epoch in range(num_epochs):\n        if epoch % 10 == 0:\n            print(f'Epoch {epoch+1}\/{num_epochs}')\n        \n        for i,batch in enumerate(train_loader):\n            img = batch[0].to(device)\n            msk = batch[1].to(device)\n\n            optimizer.zero_grad()\n            output = model(img)\n            loss = criterion(output, msk)\n            loss.backward()\n            optimizer.step()\n\n            losses.append(loss.item())\n            train_scores.append(iou_pytorch(output.detach(), msk))\n\n            iters += 1\n\n            #if iters % 500 == 0:\n                #print(f'Loss: [{loss}]')\n                \n    #for i in range(len(train_scores)):\n     #   train_scores[i] = train_scores[i].mean()\n        \n    #######################\n    #Validate model       #\n    #######################\n    \n    model.eval()\n    val_losses = []\n    val_scores = []\n\n    for i,batch in enumerate(val_loader):\n            img = batch[0].to(device)\n            msk = batch[1].to(device)\n\n            output = model(img)\n            loss = criterion(output, msk)\n            val_scores.append(iou_pytorch(output.detach(), msk))\n            val_losses.append(loss.item())\n            \n    results = {\n        'model_name': model_name,\n        'train_losses': losses,\n        'train_scores': train_scores,\n        'val_losses': val_losses,\n        'val_scores': val_scores\n    }\n    \n    if clear_mem:\n        del model\n        del optimizer\n        del criterion\n        torch.cuda.empty_cache()\n        \n    return results","3d7b408f":"def plot_training_scores(losses, train_scores):\n    #plot loss and IoU\n    fig, axs = plt.subplots(1,2, figsize=(5,5))\n    axs[0].set_title('Train BCE Loss')\n    axs[0].plot(range(len(losses)), losses)\n\n    axs[1].set_title('IoU Score vs Training Step')\n    axs[1].plot(range(len(train_scores)), train_scores)\n    \n    print(f'MEAN TRAIN IOU: {torch.mean(torch.tensor(train_scores))}')\n\ndef plot_validation_scores(val_losses, val_scores):\n    #plot loss and IoU scores - we use a histogram because we care about the distribution of losses, not its progression\n    fig, axs = plt.subplots(1,2, figsize=(7,7))\n    axs[0].set_title('BCE Loss on Validation Set')\n    axs[0].hist(val_losses)\n\n    temp = [t.cpu().item() for t in val_scores]\n    axs[1].set_title('IoU Scores on Validation Set')\n    axs[1].hist(temp)\n    axs[1].axvline(np.median(np.array(temp)), color='k', linestyle='dashed', linewidth=1)\n    print(f'MEAN VAL IOU: {mean(temp)}')\n\ndef visualize_segmentation(model, data_loader, num_samples=5, device='cuda'):\n    #visualize segmentation on unseen samples\n    fig, axs = plt.subplots(num_samples, 3, figsize=(60,60))\n\n    for ax, col in zip(axs[0], ['MRI', 'Ground Truth', 'Predicted Mask']):\n        ax.set_title(col)\n\n    index = 0\n    for i,batch in enumerate(data_loader):\n            img = batch[0].to(device)\n            msk = batch[1].to(device)\n\n            output = model(img)\n\n            for j in range(batch[0].size()[0]): #iterate over batchsize\n                axs[index,0].imshow(np.transpose(img[j].detach().cpu().numpy(), (1,2,0)).astype(np.uint8), cmap='bone', interpolation='none')\n\n                axs[index,1].imshow(np.transpose(img[j].detach().cpu().numpy(), (1,2,0)).astype(np.uint8), cmap='bone', interpolation='none')\n                axs[index,1].imshow(torch.squeeze(msk[j]).detach().cpu().numpy(), cmap='Blues', interpolation='none', alpha=0.5)\n\n                axs[index,2].imshow(np.transpose(img[j].detach().cpu().numpy(), (1,2,0)).astype(np.uint8), cmap='bone', interpolation='none')\n                axs[index,2].imshow(torch.squeeze(output[j]).detach().cpu().numpy(), cmap='Greens', interpolation='none', alpha=0.5)\n\n                index += 1\n\n            if index >= num_samples:\n                break\n\n    plt.tight_layout()","ad963df5":"#load data\npositive_diagnoses = [x for x in file_list if x[2] == 1]\n#print(positive_diagnoses[:5])\nmri_dataset = Brain_MRI_Segmentation_Dataset(positive_diagnoses)\n\nvalidation_size = int(0.3 * len(mri_dataset))\ntrain_set, val_set = data.random_split(mri_dataset, [len(mri_dataset)-validation_size, validation_size])\n\ntrain_loader = data.DataLoader(dataset=train_set, batch_size=2, shuffle=True)\nval_loader = data.DataLoader(dataset=val_set, batch_size=2, shuffle=False)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nnum_epochs = 50\n\n\nmodel = UNet()\noptimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\ncriterion = nn.BCEWithLogitsLoss()\n\nresults = run_experiment('Unet_SGD', model, optimizer, criterion, train_loader, val_loader, device=device, num_epochs=num_epochs, clear_mem=False)","2c6a0e06":"plot_training_scores(results['train_losses'], results['train_scores'])\nplot_validation_scores(results['val_losses'], results['val_scores'])","5f85e9ce":"#visualization over unseen validation samples\nvisualize_segmentation(model, val_loader, num_samples=10, device='cuda')\ndel model\ndel optimizer\ndel criterion\ntorch.cuda.empty_cache()","efc4708e":"num_epochs = 50\nmodel = UNet()\noptimizer = Adam(model.parameters())\ncriterion = nn.BCEWithLogitsLoss()\n\nresults = run_experiment('Unet_Adam', model, optimizer, criterion, train_loader, val_loader, device=device, num_epochs=num_epochs, clear_mem=False)","683818eb":"plot_training_scores(results['train_losses'], results['train_scores'])\nplot_validation_scores(results['val_losses'], results['val_scores'])","e0696bd1":"#visualization over unseen validation samples\nvisualize_segmentation(model, val_loader, num_samples=10, device='cuda')\ndel model\ndel optimizer\ndel criterion\ntorch.cuda.empty_cache()","eac41449":"transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(90),\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 0.8))\n])\n\naugmented_mri_dataset = Brain_MRI_Segmentation_Dataset(positive_diagnoses, transform=transform)\n\nvalidation_size = int(0.3 * len(mri_dataset))\natrain_set, aval_set = data.random_split(mri_dataset, [len(mri_dataset)-validation_size, validation_size])\n\naug_train_loader = data.DataLoader(dataset=atrain_set, batch_size=2, shuffle=True)\naug_val_loader = data.DataLoader(dataset=aval_set, batch_size=2, shuffle=False)\n\nnum_epochs = 50\nmodel = UNet()\noptimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\ncriterion = nn.BCEWithLogitsLoss()\n\nresults = run_experiment('Unet_SGD_AugmentedData', model, optimizer, criterion, aug_train_loader, aug_val_loader, device=device, num_epochs=num_epochs, clear_mem=False)","b6ff2362":"plot_training_scores(results['train_losses'], results['train_scores'])\nplot_validation_scores(results['val_losses'], results['val_scores'])","b47b9f14":"#visualization over unseen validation samples\nvisualize_segmentation(model, aug_val_loader, num_samples=10, device='cuda')\ndel model\ndel optimizer\ndel criterion\ntorch.cuda.empty_cache()","3d3b4a22":"num_epochs = 50\nmodel = UNet()\noptimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\ncriterion = dice_loss\n\nresults = run_experiment('Unet_SGD_AugmentedData_DiceLoss', model, optimizer, criterion, aug_train_loader, aug_val_loader, device=device, num_epochs=num_epochs, clear_mem=False)","50301e46":"plot_training_scores(results['train_losses'], results['train_scores'])\nplot_validation_scores(results['val_losses'], results['val_scores'])","6da2d562":"#visualization over unseen validation samples\nvisualize_segmentation(model, aug_val_loader, num_samples=10, device='cuda')\ndel model\ndel optimizer\ndel criterion\ntorch.cuda.empty_cache()","62ad34fc":"num_epochs = 50\nmodel = UNet()\noptimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\ncriterion = bce_dice_loss\n\nresults = run_experiment('Unet_SGD_AugmentedData_CE-DiceLoss', model, optimizer, criterion, aug_train_loader, aug_val_loader, device=device, num_epochs=num_epochs, clear_mem=False)","e1061d03":"plot_training_scores(results['train_losses'], results['train_scores'])\nplot_validation_scores(results['val_losses'], results['val_scores'])","d18b6ef4":"#visualization over unseen validation samples\nvisualize_segmentation(model, aug_val_loader, num_samples=10, device='cuda')\ndel model\ndel optimizer\ndel criterion\ntorch.cuda.empty_cache()","8bf97a9a":"num_epochs = 50\nmodel = UNetPlusPlus()\noptimizer = Adam(model.parameters(), lr=3e-4)\ncriterion = bce_dice_loss\n\nresults = run_experiment('Unet++_AugmentedData_CE-DiceLoss', model, optimizer, criterion, aug_train_loader, aug_val_loader, device=device, num_epochs=num_epochs, clear_mem=False)","f67fbafd":"plot_training_scores(results['train_losses'], results['train_scores'])\nplot_validation_scores(results['val_losses'], results['val_scores'])","65343e3e":"#visualization over unseen validation samples\nvisualize_segmentation(model, aug_val_loader, num_samples=10, device='cuda')","bb5f2246":"# EDA","21f455cd":"**UNet with Adam**","1155fba0":"# Intro","347d90cc":"**UNet with Combined CrossEntropy-Dice Loss**","901ed17b":"# Conclusion","ffcc7758":"**UNet with Augmented Dataset and SGD**","18ec1272":"Image segmentation has various applications in industry, but particularly in the medical industry the possiblities for automated diagnoses and cheaper medical imaging programs motivate researchers to invest in producing robust image segmentation models for human tissue. In this notebook we explore various model architectures, optimization algorithms, and preprocessing techniques with experiments in order to evaluate their perforamnce on the Brain MRI Segmentation Dataset with the goal of detecting and segmenting gliomas present in the MRIs.","1518ce23":"# Train\/Validation Loop","7fdc552e":"**Classification**","c96dde28":"# Models","d5aab49e":"UNet++ seems to learn the masks a bit more accurately, this is likely because of both the combined crossentropy-dice loss as well as due to the dense structure of UNet++'s residual connections, there is a clear improvement over UNet as it extrapolates more accurately.","13b68dc8":"**UNet with SGD**","35b82b12":"**UNet++**","c310090c":"**Dice Loss**","fa2fccf9":"# Create Torch Dataset","a7d5847a":"Luckily no files are missing their counterpart either, as some files have a completely negative diagnoses it may be a problem to train our segmentation model on an unbalanced dataset like this. A few approaches can be taken to deal with this:\n\n1. We balance our sampling during training time to avoid biasing the segmentation algorithm, this could still be faulty\n2. We employ a classifier to make a diagnoses (positive\/negative) and only segment positive diagnoses\n\nIn this notebook we will assume the use of the second approach - sometimes practitioners will make this classifier another output layer of the segmenter so that they can share important features, other times we make use of transfer learning and train something like ResNet50 in order to create a robust classifier to support the segmenter. Here we out goal is mainly to increase the quality of the segmentation, so we will just train the segmenter on only positive samples.","a5130203":"Let's first get an idea of the data and make sure there are no edge cases that we will have to deal with. It will also be useful to understand the degree of class imbalance in the data as we can tailor our training techniques to this.","2ec17eb7":"Between Crossentropy and Dice Loss it is clear that while mathematically Crossentropy produces nicer gradients, that it is worthwhile to combine the two metrics for an effective loss metric. This enables us to produce usable gradients while also optimizing for the performance metric that we actually care about - including the Dice Loss makes the masks less noisy and less prone to sprawling over the whole image when the model's confidence is low, this behavior is preferable as it makes analysis easier for human contributors and also reduces false positives.\n\nIt is also worth noting that the Adam optimizer performs slightly worse than the SGD optimizer, so using SGD with a small batch size and high momentum as suggested in the original UNet paper seems to be a good choice. Similarly augmenting the dataset gives better results as we would expect. \n\nDeeper models usually do perform better, but it is clear that the dense architecture of UNet++ provides a significant advantage over the otherwise mundane UNet architecture. There is clear merit to the idea that dense connections will bridge the semantic gap between the encoder and decoder portions of the network.\n\nEvery model struggled to model over masks which depicted very small tumors, often producing noise throughout the rest of the mask. This problem seems to become more evident as the tumor size grows smaller. Perhaps this is because the model's feature maps are not optimized to detect smaller tumor sizes and\/or the dataset does not contain a significant proportion of these samples. Concievably, we could alter model architecture so that feature maps would also be learnt for smaller portions of the image.\n\nFurther work on this could consist of hyperparameter optimization, experimentation with dataset augmentation, comparison with transfer learning models, and using some pretrained backbone network as the encoder portion of the segmentation model. This notebook is simply meant to explore preliminary experimentation on this dataset.","56b6964d":"Adam and SGD Optimizers did not provide too much of a difference, but we can observe that the validation distribution of IoU scores is more strongly skewed towards 1 when we use the augmented dataset.","c912e9b0":"It's notable that using the Dice Loss seems to minimize the crossentropy for the validation set very well and that using the dice loss in conjuction with cross entropy produces a distribution of validation dice scores whose median is closest to one when compared with using only crossentropy and only dice loss. \n\nThis makes sense as by including the dice loss we can incentivize the model to optimize for what we care about: segmentation accuracy, while including a cross entropy component allows us to take advantage of its clean gradients - this is especially important because the dice loss can sometimes return very large or very small gradients.\n\nAll the distributions of Dice Scores on the validation set are bimodal, indicating that the model variance may be high. This can be handled in a few ways: \n1. Our model capacity should be reduced so that it does not overfit the training data\n2. Our model needs to train for less time in order to learn a less complex representation of the input\n3. We need a more representative dataset or need to identify the class of samples that are more challenging and preferentially sample them","2adad8ff":"# Experiments","e0c95588":"Another area to experiment is the loss function, while for segmentation tasks we actually want to minimize the dice loss, it sometimes provides bad gradients which leads to crossentropy being used in practice. Let's compare the performance of our previous cross entropy optimization with optimization of the dice loss and a combined crossentropy-dice loss. From now on we'll use the augmented dataset as the model performs best on this.","96e008c8":"**UNet**","11ae6626":"**UNet++ with Combined CrossEntropy-Dice Loss**\nWe will use an Adam optimizer here as suggested in the original UNet++ paper","7a5694d9":"**UNet with Dice Loss**","d76480bf":"**Segmentation**","39da5bab":"Luckily as seen below the image dimensions for this dataset are uniform so we will not have to wrangle the data with respect to this attribute.","de58b98b":"**IoU Metric**\nObtained from: https:\/\/www.kaggle.com\/iezepov\/fast-iou-scoring-metric-in-pytorch-and-numpy"}}