{"cell_type":{"1f6889e8":"code","cffe537a":"code","fc1278c5":"code","4001d9d8":"code","19caeb79":"code","59016a07":"code","9537d182":"code","06a3c690":"code","d1cfddd9":"code","5b7de2f9":"code","729c80c6":"code","21df32cf":"code","5b1ec444":"code","5d5fbf03":"code","5cffc585":"code","520031db":"code","5f89efd0":"code","1558e47d":"code","e75bc6cb":"code","78d95dcf":"code","bec1454d":"code","d382837f":"code","03472e1f":"code","539bcb98":"code","2f7e462d":"code","6fe44f7e":"code","ef0bfa80":"code","b8366a21":"code","50d70eef":"code","225072b8":"code","80e0d676":"code","f70f0a07":"code","8f4ab2be":"code","74580fcf":"markdown","7a99d2e3":"markdown","81fb2483":"markdown","142adc4a":"markdown","3b054cac":"markdown","cfb3c5e2":"markdown","8bfdf596":"markdown"},"source":{"1f6889e8":"# import data manipulation library\nimport numpy as np\nimport pandas as pd\n\n# import data visualization library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import scientific computing library\nimport statsmodels.api as sm\n\n# import xgboost model class\nimport xgboost as xgb\n\n# import sklearn model selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\n# import sklearn model evaluation regression metrics\nfrom sklearn.metrics import mean_squared_error","cffe537a":"# acquiring training and testing data\ndf_train = pd.read_csv('..\/input\/train.csv', parse_dates=['date'], index_col='date')\ndf_test = pd.read_csv('..\/input\/test.csv', parse_dates=['date'], index_col='date')","fc1278c5":"# visualize head of the training data\ndf_train.head(n=5)","4001d9d8":"# visualize tail of the testing data\ndf_test.tail(n=5)","19caeb79":"# combine training and testing dataframe\ndf_train['datatype'], df_test['datatype'] = 'training', 'testing'\ndf_train.insert(0, 'id', 0)\ndf_test.insert(df_test.shape[1] - 1, 'sales', np.nan)\ndf_data = pd.concat([df_train, df_test], ignore_index=False)","59016a07":"def scatterplot(numerical_x: list or str, numerical_y: list or str, data: pd.DataFrame, figsize: tuple = (4, 3), ncols: int = 5, nrows: int = None) -> plt.figure:\n    \"\"\" Return a scatter plot applied for numerical variable in x-axis vs numerical variable in y-axis.\n    \n    Args:\n        numerical_x (list or str): The numerical variable in x-axis.\n        numerical_y (list or str): The numerical variable in y-axis.\n        data (pd.DataFrame): The data to plot.\n        figsize (tuple): The matplotlib figure size width and height in inches. Default to (4, 3).\n        ncols (int): The number of columns for axis in the figure. Default to 5.\n        nrows (int): The number of rows for axis in the figure. Default to None.\n    \n    Returns:\n        plt.figure: The plot figure.\n    \"\"\"\n    \n    numerical_x, numerical_y = [numerical_x] if type(numerical_x) == str else numerical_x, [numerical_y] if type(numerical_y) == str else numerical_y\n    if nrows is None: nrows = (len(numerical_x)*len(numerical_y) - 1) \/\/ ncols + 1\n    \n    fig, axes = plt.subplots(figsize=(figsize[0]*ncols , figsize[1]*nrows), ncols=ncols, nrows=nrows)\n    axes = axes.flatten()\n    _ = [sns.scatterplot(x=vj, y=vi, data=data, ax=axes[i*len(numerical_x) + j], rasterized=True) for i, vi in enumerate(numerical_y) for j, vj in enumerate(numerical_x)]\n    return fig","9537d182":"# describe training and testing data\ndf_data.describe(include='all')","06a3c690":"# feature exploration: histogram of all numeric features\n_ = df_data.hist(bins=20, figsize=(10, 6))","d1cfddd9":"# feature exploration: season for store 1 to 10 and item 1\nfor i in range(1, 11):\n    fig, axes = plt.subplots(figsize=(20, 3))\n    _ = df_data.loc[(df_data['store'] == i) & (df_data['item'] == 1) & (df_data['datatype'] == 'training'), 'sales'].plot()\n    axes.set_title('store %d, item %d' %(i, 1))","5b7de2f9":"# feature exploration: seasonal decompose for store 5 and item 1\nseasonal = sm.tsa.seasonal_decompose(df_data.loc[(df_data['store'] == 5) & (df_data['item'] == 1) & (df_data['datatype'] == 'training'), 'sales']).plot()\nseasonal.set_figwidth(20)\nseasonal.set_figheight(15)\nplt.tight_layout(); plt.show()","729c80c6":"# feature extraction: combination of keyword date\ndf_data['date'] = df_data.index\ndf_data['year'] = df_data['date'].dt.year - 2000\ndf_data['quarter'] = df_data['date'].dt.quarter\ndf_data['month'] = df_data['date'].dt.month\ndf_data['weekofyear'] = df_data['date'].dt.weekofyear\ndf_data['dayofweek'] = df_data['date'].dt.dayofweek","21df32cf":"# feature extraction: statistic features for store, item and quarter\ndf_data['item_quarter_mean'] = df_data.groupby(['quarter', 'item'])['sales'].transform('mean')\ndf_data['store_quarter_mean'] = df_data.groupby(['quarter', 'store'])['sales'].transform('mean')\ndf_data['store_item_quarter_mean'] = df_data.groupby(['quarter', 'store', 'item'])['sales'].transform('mean')","5b1ec444":"# feature extraction: statistic features for store, item and month\ndf_data['item_month_mean'] = df_data.groupby(['month', 'item'])['sales'].transform('mean')\ndf_data['store_month_mean'] = df_data.groupby(['month', 'store'])['sales'].transform('mean')\ndf_data['store_item_month_mean'] = df_data.groupby(['month', 'store', 'item'])['sales'].transform('mean')","5d5fbf03":"# feature extraction: statistic features for store, item and weekofyear\ndf_data['item_weekofyear_mean'] = df_data.groupby(['weekofyear', 'item'])['sales'].transform('mean')\ndf_data['store_weekofyear_mean'] = df_data.groupby(['weekofyear', 'store'])['sales'].transform('mean')\ndf_data['store_item_weekofyear_mean'] = df_data.groupby(['weekofyear', 'store', 'item'])['sales'].transform('mean')","5cffc585":"# feature extraction: statistic features for store, item and dayofweek\ndf_data['item_dayofweek_mean'] = df_data.groupby(['dayofweek', 'item'])['sales'].transform('mean')\ndf_data['store_dayofweek_mean'] = df_data.groupby(['dayofweek', 'store'])['sales'].transform('mean')\ndf_data['store_item_dayofweek_mean'] = df_data.groupby(['dayofweek', 'store', 'item'])['sales'].transform('mean')","520031db":"# feature extraction: shifted features for store, item and weekofyear shift 90 days\ndf_data['store_item_shift90'] = df_data.groupby(['store', 'item'])['sales'].transform(lambda x: x.shift(90))\ndf_data['item_weekofyear_shift90_mean'] = df_data.groupby(['weekofyear', 'item'])['sales'].transform(lambda x: x.shift(13).mean())\ndf_data['store_weekofyear_shift90_mean'] = df_data.groupby(['weekofyear', 'store'])['sales'].transform(lambda x: x.shift(13).mean())","5f89efd0":"# feature extraction: shifted features for store, item and weekofyear shift 180 days\ndf_data['store_item_shift180'] = df_data.groupby(['store', 'item'])['sales'].transform(lambda x: x.shift(180))\ndf_data['item_weekofyear_shift180_mean'] = df_data.groupby(['weekofyear', 'item'])['sales'].transform(lambda x: x.shift(26).mean())\ndf_data['store_weekofyear_shift180_mean'] = df_data.groupby(['weekofyear', 'store'])['sales'].transform(lambda x: x.shift(26).mean())","1558e47d":"# feature extraction: shifted features for store, item and weekofyear shift 270 days\ndf_data['store_item_shift270'] = df_data.groupby(['store', 'item'])['sales'].transform(lambda x: x.shift(270))\ndf_data['item_weekofyear_shift270_mean'] = df_data.groupby(['weekofyear', 'item'])['sales'].transform(lambda x: x.shift(39).mean())\ndf_data['store_weekofyear_shift270_mean'] = df_data.groupby(['weekofyear', 'store'])['sales'].transform(lambda x: x.shift(39).mean())","e75bc6cb":"# feature extraction: shifted features for store, item and weekofyear shift 365 days\ndf_data['store_item_shift365'] = df_data.groupby(['store', 'item'])['sales'].transform(lambda x: x.shift(365))\ndf_data['item_weekofyear_shift365_mean'] = df_data.groupby(['weekofyear', 'item'])['sales'].transform(lambda x: x.shift(52).mean())\ndf_data['store_weekofyear_shift365_mean'] = df_data.groupby(['weekofyear', 'store'])['sales'].transform(lambda x: x.shift(52).mean())","78d95dcf":"# feature extraction: fillna with 0\ncol_fillnas = ['store_item_shift90', 'store_item_shift180', 'store_item_shift270', 'store_item_shift365']\ndf_data[col_fillnas] = df_data[col_fillnas].fillna(0)","bec1454d":"# feature exploration: sales\ncol_number = df_data.select_dtypes(include=['number']).columns.drop(['id']).tolist()\n_ = scatterplot(col_number, 'sales', df_data[df_data['datatype'] == 'training'])","d382837f":"# feature extraction: fillna with 0\ndf_data['sales'] = df_data['sales'].fillna(0)","03472e1f":"# convert category codes for data dataframe\ndf_data = pd.get_dummies(df_data, columns=None, drop_first=True)","539bcb98":"# describe data dataframe\ndf_data.describe(include='all')","2f7e462d":"# verify dtypes object\ndf_data.info()","6fe44f7e":"# compute pairwise correlation of columns, excluding NA\/null values and present through heat map\ncorr = df_data[df_data['datatype_training'] == 1].corr()\nfig, axes = plt.subplots(figsize=(200, 150))\nheatmap = sns.heatmap(corr, annot=True, cmap=plt.cm.RdBu, fmt='.1f', square=True, vmin=-0.8, vmax=0.8)","ef0bfa80":"def symmetric_mean_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\" Return the symmetric mean absolute percentage error (mape).\n    \n    Args:\n        y_true (np.ndarray): The ground truth (correct) labels.\n        y_pred (np.ndarray): The predicted labels.\n    \n    Returns:\n        float: The symmetric mean absolute percentage error.\n    \"\"\"\n    \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    condition = (y_true > 0) & (y_pred > 0)\n    return np.mean(2 * np.abs((y_pred[condition] - y_true[condition])) \/ (np.abs(y_pred[condition]) + np.abs(y_true[condition]))) * 100","b8366a21":"def symmetric_mean_absolute_percentage_error_scoring(model: object, x: np.ndarray, y: np.ndarray) -> float:\n    \"\"\" Return the symmetric mean absolute percentage error (mape) scoring.\n    \n    Args:\n        y_true (np.ndarray): The ground truth (correct) labels.\n        y_pred (np.ndarray): The predicted labels.\n    \n    Returns:\n        float: The symmetric mean absolute percentage error scoringg.\n    \"\"\"\n    \n    y_pred = model.predict(x)\n    return symmetric_mean_absolute_percentage_error(y, y_pred)","50d70eef":"# select all features\nx = df_data[df_data['datatype_training'] == 1].drop(['id', 'sales', 'date', 'datatype_training'], axis=1)\ny = df_data.loc[df_data['datatype_training'] == 1]['sales']","225072b8":"# perform train-test (validate) split\nx_train, x_validate, y_train, y_validate = train_test_split(x, y, random_state=58, test_size=0.25)","80e0d676":"# xgboost regression model setup\nmodel_xgbreg = xgb.XGBRegressor(max_depth=5, learning_rate=0.1, n_estimators=1000, objective='reg:linear', booster='gbtree', gamma=0, subsample=0.9, colsample_bytree=0.9, reg_alpha=0.1, reg_lambda=0.9, random_state=58)\n\n# xgboost regression model fit\nmodel_xgbreg.fit(x_train, y_train, eval_set=[(x_train, y_train), (x_validate, y_validate)], early_stopping_rounds=50, verbose=False, callbacks=[xgb.callback.print_evaluation(period=50)])\n\n# xgboost regression model prediction\nmodel_xgbreg_ypredict = model_xgbreg.predict(x_validate)\n\n# xgboost regression model metrics\nmodel_xgbreg_mape = symmetric_mean_absolute_percentage_error(y_validate, model_xgbreg_ypredict)\nprint('xgboost regression\\n  symmetric mean absolute percentaged error: %0.4f' %model_xgbreg_mape)","f70f0a07":"# model selection\nfinal_model = model_xgbreg\n\n# prepare testing data and compute the observed value\nx_test = df_data[df_data['datatype_training'] == 0].drop(['id', 'sales', 'date', 'datatype_training'], axis=1)\ny_test = pd.DataFrame(final_model.predict(x_test), columns=['sales'], index=df_data.loc[df_data['datatype_training'] == 0, 'id'])","8f4ab2be":"# submit the results\nout = pd.DataFrame({'id': y_test.index, 'sales': y_test['sales']})\nout.to_csv('submission.csv', index=False)","74580fcf":"> **Supply or submit the results**\n\nOur submission to the competition site Kaggle is ready. Any suggestions to improve our score are welcome.","7a99d2e3":"> **Problem overview**\n\nThis competition is provided as a way to explore different time series techniques on a relatively simple and clean dataset. You are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores.\n\nWhat's the best way to deal with seasonality? Should stores be modeled separately, or can you pool them together? Does deep learning work better than ARIMA? Can either beat xgboost?","81fb2483":"> **Acquiring training and testing data**\n\nWe start by acquiring the training and testing datasets into Pandas DataFrames.","142adc4a":"> **Analyze and identify patterns by visualizations**\n\nLet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilize the Seaborn plotting package which allows us to plot very conveniently as follows.\n\nThe Pearson Correlation plot can tell us the correlation between features with one another. If there is no strongly correlated between features, this means that there isn't much redundant or superfluous data in our training data. This plot is also useful to determine which features are correlated to the observed value.\n\nThe pairplots is also useful to observe the distribution of the training data from one feature to the other.\n\nThe pivot table is also another useful method to observe the impact between features.","3b054cac":"> **Model, predict and solve the problem**\n\nNow, it is time to feed the features to Machine Learning models.","cfb3c5e2":"After extracting all features, it is required to convert category features to numerics features, a format suitable to feed into our Machine Learning models.","8bfdf596":"> **Feature exploration, engineering and cleansing**\n\nHere we generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution together with exploring some data."}}