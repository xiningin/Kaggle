{"cell_type":{"c81d0c02":"code","c1a4c0bd":"code","5e14d4e6":"code","9a3fa505":"code","7c505a20":"code","034487c0":"code","192ecc5d":"markdown","5aa4142e":"markdown","b3e5e68d":"markdown","3d1e92ec":"markdown","51524df6":"markdown"},"source":{"c81d0c02":"# coding: utf-8\n__author__ = 'ZFTurbo: https:\/\/kaggle.com\/zfturbo'\n\nimport numpy as np\nimport gzip\nimport pickle\nimport os\nimport glob\nimport time\nimport operator\nimport pandas as pd\nfrom collections import Counter, defaultdict\nfrom multiprocessing import Process, Manager\nfrom sklearn.metrics import accuracy_score, log_loss, roc_auc_score\nimport random\nimport datetime\nfrom PIL import Image\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.preprocessing import normalize\nfrom tqdm import tqdm\nimport gc\nimport psutil\nimport warnings\nimport shutil\nfrom keras.callbacks import Callback\n\n\nINPUT_PATH = '..\/input\/'\nMODELS_PATH = '.\/'\nOUTPUT_PATH = '.\/'\nMODELS_PATH_KERAS = '.\/'\nFEATURES_PATH = '.\/'\nCACHE_PATH = '.\/'\nSUBM_PATH = '.\/'\n\ndef read_train():\n    train = pd.read_csv(INPUT_PATH + 'train.csv', low_memory=True)\n    return train\n\n\ndef read_test():\n    test = pd.read_csv(INPUT_PATH + 'test.csv', low_memory=True)\n    return test","c1a4c0bd":"def get_real_test_data():\n    test = read_test()\n    ids = test['ID_code'].values.copy()\n    test.drop(['ID_code'], axis=1, inplace=True)\n    features = test.columns.values\n    df_test = test.values\n\n    unique_samples = []\n    unique_count = np.zeros_like(df_test)\n    for feature in tqdm(range(df_test.shape[1])):\n        _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n        unique_count[index_[count_ == 1], feature] += 1\n\n    # Samples which have unique values are real the others are fake\n    real_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n    synthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\n    print(len(real_samples_indexes))\n    print(len(synthetic_samples_indexes))\n\n    test1 = read_test()\n    ids = ids[real_samples_indexes]\n    return ids\n","5e14d4e6":"def get_magic_features():\n    d = dict()\n    for i in range(200):\n        d['var_{}'.format(i)] = np.str\n\n    train = pd.read_csv(INPUT_PATH + 'train.csv', dtype=d)\n    test = pd.read_csv(INPUT_PATH + 'test.csv', dtype=d)\n\n    real_ids = get_real_test_data()\n    test = test[test['ID_code'].isin(real_ids)]\n    print(len(test))\n    features = sorted(list(d.keys()))\n    print(train.shape, test.shape)\n    table = pd.concat((train[['ID_code'] + features], test[['ID_code'] + features]), axis=0)\n    print(table.shape)\n    feat_to_store = []\n    for f in features:\n        print('Go {}'.format(f))\n        new_feat = []\n        v = dict(pd.value_counts(table[f]))\n        for el in table[f].values:\n            new_feat.append(v[el])\n        table[f + '_counts_sum'] = new_feat\n        feat_to_store.append(f + '_counts_sum')\n\n    train = table[['ID_code'] + feat_to_store][:200000]\n    test = table[['ID_code'] + feat_to_store][200000:]\n    train.to_csv(FEATURES_PATH + 'counts_of_values_train.csv', index=False)\n    test.to_csv(FEATURES_PATH + 'counts_of_values_test.csv', index=False)\n    return train, test","9a3fa505":"train = read_train()\ntest = read_test()\n\nt1, t2 = get_magic_features()\ntrain = train.merge(t1, on='ID_code', how='left')\ntest = test.merge(t2, on='ID_code', how='left')\nf_add = list(t1.columns.values)\nf_add.remove('ID_code')\ntest.fillna(-1, inplace=True)\n\nfor i in range(200):\n    train['var_{}_mul'.format(i)] = train['var_{}'.format(i)] * train['var_{}_counts_sum'.format(i)]\n    test['var_{}_mul'.format(i)] = test['var_{}'.format(i)] * test['var_{}_counts_sum'.format(i)]\n    train['var_{}_div'.format(i)] = train['var_{}'.format(i)] \/ train['var_{}_counts_sum'.format(i)]\n    test['var_{}_div'.format(i)] = test['var_{}'.format(i)] \/ test['var_{}_counts_sum'.format(i)]\n\nfeatures = []\nfor i in range(200):\n    features.append('var_{}'.format(i))\n    features.append('var_{}_counts_sum'.format(i))\n    features.append('var_{}_mul'.format(i))\n    features.append('var_{}_div'.format(i))\nprint('Features: [{}] {}'.format(len(features), features))","7c505a20":"def get_keras_model(input_features):\n    from keras.models import Model\n    from keras.layers import Input, Dense, BatchNormalization, Conv1D, Reshape, Flatten, MaxPooling1D, Concatenate\n    from keras.layers.core import Activation, Dropout, Lambda\n    from keras.layers.merge import concatenate\n\n    inp = Input(shape=(input_features, 1))\n    x = BatchNormalization(axis=-2)(inp)\n    x = Dense(128, activation='relu')(x)\n    x = Flatten()(x)\n    preds = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs=inp, outputs=preds)\n    return model\n\n\ndef get_kfold_split(folds_number, len_train, target, random_state):\n    train_index = list(range(len_train))\n    folds = StratifiedKFold(n_splits=folds_number, shuffle=True, random_state=random_state)\n    ret = []\n    for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_index, target)):\n        ret.append([trn_idx, val_idx])\n    return ret\n\n\ndef get_model_memory_usage(batch_size, model):\n    import numpy as np\n    from keras import backend as K\n\n    shapes_mem_count = 0\n    for l in model.layers:\n        single_layer_mem = 1\n        for s in l.output_shape:\n            if s is None:\n                continue\n            single_layer_mem *= s\n        shapes_mem_count += single_layer_mem\n\n    trainable_count = np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n    non_trainable_count = np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n\n    number_size = 4.0\n    if K.floatx() == 'float16':\n         number_size = 2.0\n    if K.floatx() == 'float64':\n         number_size = 8.0\n\n    total_memory = number_size*(batch_size*shapes_mem_count + trainable_count + non_trainable_count)\n    gbytes = np.round(total_memory \/ (1024.0 ** 3), 3)\n    return gbytes\n\n\nclass ModelCheckpoint_AUC(Callback):\n    \"\"\"Save the model after every epoch. \"\"\"\n\n    def __init__(self, filepath, filepath_static, monitor='val_loss', verbose=0,\n                 save_best_only=False, save_weights_only=False,\n                 mode='max', period=1, patience=None, validation_data=()):\n        super(ModelCheckpoint_AUC, self).__init__()\n        self.interval = period\n        self.X_val, self.y_val, self.batch_size = validation_data\n        self.monitor = monitor\n        self.verbose = verbose\n        self.filepath = filepath\n        self.filepath_static = filepath_static\n        self.save_best_only = save_best_only\n        self.save_weights_only = save_weights_only\n        self.period = period\n        self.epochs_since_last_save = 0\n        self.monitor_op = np.greater\n        self.best = -np.Inf\n\n        # part for early stopping\n        self.epochs_from_best_model = 0\n        self.patience = patience\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        self.epochs_since_last_save += 1\n        if self.epochs_since_last_save >= self.period:\n            self.epochs_since_last_save = 0\n\n            y_pred = self.model.predict(self.X_val, verbose=0, batch_size=self.batch_size)\n            score = roc_auc_score(self.y_val, y_pred)\n            filepath = self.filepath.format(epoch=epoch + 1, score=score, **logs)\n            print(\"AUC score: {:.6f}\".format(score))\n            if score > self.best:\n                self.epochs_from_best_model = 0\n            else:\n                self.epochs_from_best_model += 1\n\n            if self.save_best_only:\n                current = score\n                if current is None:\n                    warnings.warn('Can save best model only with %s available, '\n                                  'skipping.' % (self.monitor), RuntimeWarning)\n                else:\n                    if self.monitor_op(current, self.best):\n                        if self.verbose > 0:\n                            print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n                                  ' saving model to %s'\n                                  % (epoch + 1, self.monitor, self.best,\n                                     current, filepath))\n                        self.best = current\n                        if self.save_weights_only:\n                            self.model.save_weights(filepath, overwrite=True)\n                        else:\n                            self.model.save(filepath, overwrite=True)\n                        shutil.copy(filepath, self.filepath_static)\n                    else:\n                        if self.verbose > 0:\n                            print('\\nEpoch %05d: %s did not improve' %\n                                  (epoch + 1, self.monitor))\n            else:\n                if self.verbose > 0:\n                    print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n                if self.save_weights_only:\n                    self.model.save_weights(filepath, overwrite=True)\n                else:\n                    self.model.save(filepath, overwrite=True)\n                shutil.copy(filepath, self.filepath_static)\n\n            if self.patience is not None:\n                if self.epochs_from_best_model > self.patience:\n                    print('Early stopping: {}'.format(self.epochs_from_best_model))\n                    self.model.stop_training = True\n\n\ndef batch_generator_train_random_sample(X, y, batch_size):\n    rng = list(range(X.shape[0]))\n    feature_group_number = 4\n\n    while True:\n        index1 = random.sample(rng, batch_size)\n        input1 = X[index1, :].copy()\n        output1 = y[index1].copy()\n\n        input1_0 = input1[output1 == 0, :]\n        input1_1 = input1[output1 == 1, :]\n        output1_0 = output1[output1 == 0]\n        output1_1 = output1[output1 == 1]\n\n        for i in range(0, input1.shape[1], feature_group_number):\n            index = np.arange(0, input1_0.shape[0])\n            np.random.shuffle(index)\n            for j in range(feature_group_number):\n                input1_0[:, i + j] = input1_0[index, i + j]\n\n            index = np.arange(0, input1_1.shape[0])\n            np.random.shuffle(index)\n            for j in range(feature_group_number):\n                input1_1[:, i + j] = input1_1[index, i + j]\n\n        input1 = np.concatenate((input1_0, input1_1), axis=0)\n        output1 = np.concatenate((output1_0, output1_1), axis=0)\n        input1 = np.expand_dims(input1, axis=2)\n        yield input1, output1\n                    \n\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.optimizers import Adam\n\ntarget = train['target'].values\noverall_train_predictions = np.zeros(target.shape[0], dtype=np.float64)\nmodel_list = []\nall_splits = []\nnum_fold = 0\nret = get_kfold_split(5, train.shape[0], target, 890)\n\nfor train_index, valid_index in ret:\n    num_fold += 1\n    print('Start fold {}'.format(num_fold))\n    X_train = train.loc[train_index].copy()\n    X_valid = train.loc[valid_index].copy()\n    y_train = target[train_index]\n    y_valid = target[valid_index]\n\n    print('Train data:', X_train.shape, y_train.shape)\n    print('Valid data:', X_valid.shape, y_valid.shape)\n\n    X_train_matrix = X_train[features].values\n    X_valid_matrix = X_valid[features].values\n\n    print('Train data:', X_train_matrix.shape, y_train.shape)\n    print('Valid data:', X_valid_matrix.shape, y_valid.shape)\n    print('Valid sum: {}'.format(y_valid.sum()))\n\n    optim_name = 'Adam'\n    batch_size_train = 1000\n    batch_size_valid = 1000\n    learning_rate = 0.002\n    epochs = 50\n    patience = 5\n    print('Batch size: {}'.format(batch_size_train))\n    print('Learning rate: {}'.format(learning_rate))\n    steps_per_epoch = (X_train.shape[0] \/\/ batch_size_train)\n    validation_steps = 1\n    print('Steps train: {}, Steps valid: {}'.format(steps_per_epoch, validation_steps))\n\n    final_model_path = MODELS_PATH_KERAS + '{}_fold_{}.h5'.format('keras', num_fold)\n    cache_model_path = MODELS_PATH_KERAS + '{}_temp_fold_{}.h5'.format('keras', num_fold)\n    cache_model_path_auc = MODELS_PATH_KERAS + '{}_temp_fold_{}'.format('keras', num_fold) + '_{score:.4f}.h5'\n\n    model = get_keras_model(X_train_matrix.shape[1])\n    print(model.summary())\n    print('Model memory usage: {} GB'.format(get_model_memory_usage(batch_size_train, model)))\n\n    optim = Adam(lr=learning_rate)\n    model.compile(optimizer=optim, loss='binary_crossentropy', metrics=['accuracy'])\n\n    callbacks = [\n        # EarlyStopping(monitor='val_loss', patience=patience, verbose=0),\n        ModelCheckpoint_AUC(cache_model_path_auc, cache_model_path,\n                            validation_data=(np.expand_dims(X_valid_matrix, axis=2), y_valid, batch_size_valid),\n                            save_best_only=True,\n                            verbose=0,\n                            patience=patience),\n        # ModelCheckpoint(cache_model_path, save_best_only=False)\n        ReduceLROnPlateau(monitor='loss', factor=0.95, patience=5, min_lr=1e-9, min_delta=0.00001,\n                          verbose=1, mode='min'),\n    ]\n\n    history = model.fit_generator(generator=batch_generator_train_random_sample(X_train_matrix, y_train, batch_size_train),\n                                  epochs=epochs,\n                                  steps_per_epoch=steps_per_epoch,\n                                  validation_data=batch_generator_train_random_sample(X_valid_matrix, y_valid, batch_size_valid),\n                                  validation_steps=validation_steps,\n                                  verbose=2,\n                                  max_queue_size=10,\n                                  # class_weight=class_weight,\n                                  callbacks=callbacks)\n\n    min_loss = min(history.history['val_loss'])\n    print('Minimum loss for given fold: ', min_loss)\n    model.load_weights(cache_model_path)\n    model.save(final_model_path)\n\n    pred = model.predict(np.expand_dims(X_valid[features].values, axis=2))\n    overall_train_predictions[valid_index] += pred[:, 0]\n    score = roc_auc_score(y_valid, pred[:, 0])\n    print('Fold {} score: {:.6f}'.format(num_fold, score))\n    model_list.append(model)\n    all_splits.append(ret)\n\nscore = roc_auc_score(target, overall_train_predictions)\nprint('Total AUC score: {:.6f}'.format(score))\n\ntrain['target'] = overall_train_predictions\ntrain[['ID_code', 'target']].to_csv(SUBM_PATH + 'train_auc_{}.csv'.format(score), index=False, float_format='%.8f')\n","034487c0":"def predict_with_keras_model(test, features, model_list):\n    full_preds = []\n    for m in model_list:\n        preds = m.predict(np.expand_dims(test[features].values, axis=2), batch_size=1000)\n        full_preds.append(preds)\n    preds = np.array(full_preds).mean(axis=0)\n    return preds\n\noverall_test_predictions = predict_with_keras_model(test, features, model_list)\ntest['target'] = overall_test_predictions\ntest[['ID_code', 'target']].to_csv(SUBM_PATH + 'test_auc_{}.csv'.format(score), index=False, float_format='%.8f')\n","192ecc5d":"**1) Clean test from fake data**","5aa4142e":"3) Read data with magic features and create additional useful features var_N **mul** magic_N and var_N **div** magic_N","b3e5e68d":"**6) Predict on test**","3d1e92ec":"4) Create model\n\n5) Use random shuffle of columns during training","51524df6":"**2) Create Magic features**"}}