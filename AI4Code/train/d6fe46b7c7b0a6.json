{"cell_type":{"7c8c897c":"code","4103b31d":"code","91d10a95":"code","fa270b4a":"code","100bb0e0":"code","2283bbf1":"code","a7a27a90":"code","561646b0":"code","515076fa":"code","6ad93630":"code","66fcba79":"code","47128bf9":"code","800cb4bd":"code","e26c18ab":"code","2b3690af":"code","548df418":"code","262ff42e":"code","10cf42b4":"code","6b19bc29":"code","1a58a474":"code","51f74c98":"code","13258795":"code","111c01eb":"code","2b1e4159":"code","6270863b":"code","c0464e33":"code","d6663a8d":"code","183cd315":"code","8121d995":"code","742e7448":"code","ce45ed62":"code","1dd7634c":"code","a9784928":"code","dc141760":"code","14d91151":"markdown","53d3e430":"markdown","31c46bec":"markdown","8db4a08a":"markdown","f344158f":"markdown","3f8c142b":"markdown","4e1d85bd":"markdown","1efa8f15":"markdown","ff1c88a4":"markdown","ae78d07c":"markdown","069adde4":"markdown","df4b0cf8":"markdown","535411e0":"markdown"},"source":{"7c8c897c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4103b31d":"import warnings\nwarnings.filterwarnings('ignore')","91d10a95":"df=pd.read_csv('\/kaggle\/input\/the-boston-houseprice-data\/boston.csv')","fa270b4a":"df","100bb0e0":"df.describe()","2283bbf1":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew\n%matplotlib inline","a7a27a90":"sns.heatmap(df.corr())","561646b0":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression,Ridge,LassoCV\nfrom sklearn.metrics import mean_squared_error","515076fa":"x=df.drop('MEDV',axis=1)\ny=df['MEDV']","6ad93630":"x_train, x_test, y_train, y_test = train_test_split(x, y,train_size=0.8,random_state=0)","66fcba79":"model_lr=LinearRegression()\nmodel_lr.fit(x_train,y_train)\npred_train=model_lr.predict(x_train)\npred_test=model_lr.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_train, pred_train)))\nprint(np.sqrt(mean_squared_error(y_test, pred_test)))","47128bf9":"MEDVS=pd.DataFrame({'medvs':df['MEDV'],'log(medvs+1)':np.log1p(df['MEDV'])})\nprint(MEDVS, '\u00a5n')\n\nprint('medvs skew        :',skew(MEDVS['medvs']))\nprint('log(medvs+1) skew:', skew(MEDVS['log(medvs+1)']))\n\nMEDVS.hist()","800cb4bd":"df['MEDV']=np.log1p(df['MEDV'])","e26c18ab":"df1=df.drop('MEDV',axis=1)\ndf1_skew=df1.apply(lambda x:skew(x))\nprint(df1_skew)","2b3690af":"df1_skew = df1_skew[df1_skew > 0.75]\nprint('-----Skewness greater than 0.75-----')\nprint(df1_skew)\ndf1_skew = df1_skew.index\n\ndf1[df1_skew] = np.log1p(df1[df1_skew])\ndf1[df1_skew]","548df418":"df1","262ff42e":"df1.describe()","10cf42b4":"X=df1\nY=df['MEDV']","6b19bc29":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y,train_size=0.8,random_state=0)","1a58a474":"model_lr=LinearRegression()","51f74c98":"model_lr.fit(X_train,Y_train)\npred_train=model_lr.predict(X_train)\npred_test=model_lr.predict(X_test)\nprint(np.sqrt(mean_squared_error(Y_train, pred_train)))\nprint(np.sqrt(mean_squared_error(Y_test, pred_test)))","13258795":"def rmse_cv(model):\n    rmse = np.sqrt(\n        -cross_val_score(\n            model, X_train, Y_train,\n            scoring=\"neg_mean_squared_error\", \n            cv = 5))\n    return(rmse)","111c01eb":"model_rg = Ridge()\n\nalphas = [0,0.05, 0.1, 0.5]\ncv_rg = [rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]\ncv_rg = pd.Series(cv_rg, index = alphas)\n\nprint('Ridge RMSE loss:')\nprint(cv_rg, '\\n')\n\nprint('Ridge RMSE loss Mean:')\nprint(cv_rg.mean())\n\n\nplt.\ufb01gure(\ufb01gsize=(10, 5))\nplt.plot(cv_rg)\nplt.grid()\nplt.title('Validation - by regularization strength')\nplt.xlabel('Alpha')\nplt.ylabel('RMSE')\nplt.show()","2b1e4159":"model_rg.fit(X_train,Y_train)\npred1=model_rg.predict(X_test)\nnp.sqrt(mean_squared_error(Y_test, pred1))","6270863b":"model_ls = LassoCV(\n    alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, Y_train)\n\nprint('Lasso regression RMSE loss:')\nprint(rmse_cv(model_ls))\n\nprint('Average loss:', rmse_cv(model_ls).mean())\nprint('Minimum loss:', rmse_cv(model_ls).min())\nprint('Best alpha  :', model_ls.alpha_) ","c0464e33":"model_ls.fit(X_train,Y_train)\npred2=model_ls.predict(X_test)\nnp.sqrt(mean_squared_error(Y_test, pred2))","d6663a8d":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, label = Y_train)\n\nparams = {\"max_depth\":3, \"eta\":0.1}\n\ncross_val = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=1000,\n    early_stopping_rounds=50)\ncross_val","183cd315":"plt.\ufb01gure(\ufb01gsize=(8, 6))\nplt.plot(cross_val.loc[10:,[\"test-rmse-mean\", \"train-rmse-mean\"]])\nplt.grid()\nplt.xlabel('num_boost_round')\nplt.ylabel('RMSE')\nplt.show()","8121d995":"model_xgb = xgb.XGBRegressor(\n    n_estimators=225,\n    max_depth=3,\n    learning_rate=0.1)\nmodel_xgb.fit(X_train, Y_train)\npred3=model_xgb.predict(X_test)\n\nprint('xgboost RMSE loss:')\nprint(rmse_cv(model_xgb).mean())\nprint(np.sqrt(mean_squared_error(Y_test, pred3)))","742e7448":"xgb.plot_importance(model_xgb)","ce45ed62":"X_train1=X_train.drop(['ZN','CHAS'],axis=1)\nX_test1=X_test.drop(['ZN','CHAS'],axis=1)","1dd7634c":"dtrain = xgb.DMatrix(X_train1, label = Y_train)\n\nparams = {\"max_depth\":3, \"eta\":0.1}\n\ncross_val = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=1000,\n    early_stopping_rounds=50)\ncross_val","a9784928":"plt.\ufb01gure(\ufb01gsize=(8, 6))\nplt.plot(cross_val.loc[10:,[\"test-rmse-mean\", \"train-rmse-mean\"]])\nplt.grid()\nplt.xlabel('num_boost_round')\nplt.ylabel('RMSE')\nplt.show()","dc141760":"model_xgb = xgb.XGBRegressor(\n    n_estimators=170,\n    max_depth=3,\n    learning_rate=0.1)\nmodel_xgb.fit(X_train1, Y_train)\npred4=model_xgb.predict(X_test1)\n\nprint('xgboost RMSE loss:')\nprint(rmse_cv(model_xgb).mean())\nprint(np.sqrt(mean_squared_error(Y_test, pred4)))","14d91151":"# Among Linear Regression, Ridge and LassoCV, \"Linear Regression\" seems to be better than others.","53d3e430":"# 2.Logarithmic","31c46bec":"# 1)LinearRegression","8db4a08a":"# Find the features which have high (>0.75) skewness.","f344158f":"# 2) Ridge","3f8c142b":"# 4) XGBoost","4e1d85bd":"# 1. Baseline by LinearRegression","1efa8f15":"# XGboost seems to be better than Linear Regression.","ff1c88a4":"# 3) LassoCV","ae78d07c":"# MEDV skewness is 1.1 and logarithmic transformation\u3000skewness is -0.24. It is getting better!","069adde4":"Input features in order:\n1) CRIM: per capita crime rate by town\n2) ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n3) INDUS: proportion of non-retail business acres per town\n4) CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n5) NOX: nitric oxides concentration (parts per 10 million) [parts\/10M]\n6) RM: average number of rooms per dwelling\n7) AGE: proportion of owner-occupied units built prior to 1940\n8) DIS: weighted distances to five Boston employment centres\n9) RAD: index of accessibility to radial highways\n10) TAX: full-value property-tax rate per $10,000 [$\/10k]\n11) PTRATIO: pupil-teacher ratio by town\n12) B: The result of the equation B=1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n13) LSTAT: % lower status of the population\n\nOutput variable:\n1) MEDV: Median value of owner-occupied homes in $1000's [k$]\n\n","df4b0cf8":"# 3. Prediction Model","535411e0":"# Drop the columns 'ZN' and 'CHAS' which include over 50% '0'."}}