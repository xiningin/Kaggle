{"cell_type":{"71da7b5f":"code","9cdfd3c9":"code","4a56b821":"code","600cf9d9":"code","9c4ab3b5":"code","8792aa6d":"code","fdba3dbf":"code","56b5485e":"code","12be2f5b":"code","8957414f":"code","30a9a68d":"code","25ab46a6":"code","2d029793":"code","8db6c218":"code","e40df331":"code","d19e25ec":"code","7a49ed7b":"code","80cc4f0e":"code","64036b10":"code","33879d61":"code","09fdf947":"code","9ef13a38":"code","ac3786de":"code","fcb4c8ac":"code","c41b1693":"code","f53d73a3":"markdown","86f59b92":"markdown","d50e7b8a":"markdown","a10c35ec":"markdown","091315c8":"markdown","e7fd5009":"markdown","8fb38f58":"markdown","424ce568":"markdown","60a59de3":"markdown","becc9a6e":"markdown","96d16ab1":"markdown","fdadc6fe":"markdown","dec3f77b":"markdown","e8acdd47":"markdown","dccf6fd8":"markdown","0b50c3ae":"markdown","df834e0f":"markdown","a7fd581a":"markdown","3d6ab6b1":"markdown","7cead807":"markdown","2041019c":"markdown"},"source":{"71da7b5f":"# visualization tools\nimport matplotlib.pyplot as plt\n4\n# torch- Our deep learning framework\nimport torch\nfrom torchvision import datasets, transforms, models","9cdfd3c9":"data_dir = '..\/input\/plants-smaller-data\/small_plant_dataset'","4a56b821":"train_data = datasets.ImageFolder(data_dir + '\/train')\nval_data = datasets.ImageFolder(data_dir + '\/val')\n","600cf9d9":"dataiter = iter(train_data)\nimages, clases = dataiter\nprint(type(images))\nprint(images.shape)\nprint(labels.shape)","9c4ab3b5":"train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=64)","8792aa6d":"dataiter = iter(train_loader)\nimages, clases = dataiter\nprint(type(images))\nprint(images.shape)\nprint(labels.shape)","fdba3dbf":"train_transforms = transforms.Compose([transforms.ToTensor()])\n\nval_transforms = transforms.Compose([transforms.ToTensor(),\n])","56b5485e":"train_data = datasets.ImageFolder(data_dir + '\/train', transform=train_transforms)\nval_data = datasets.ImageFolder(data_dir + '\/val', transform=val_transforms)\n\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=64)","12be2f5b":"dataiter = iter(train_loader)\nimages, clases = dataiter.next()\nprint(type(images))\nprint(images.shape)\nprint(labels.shape)","8957414f":"train_transforms = transforms.Compose([\n                                       transforms.RandomResizedCrop(224),\n                                       transforms.ToTensor()])\n\nval_transforms = transforms.Compose([\n                                      transforms.RandomResizedCrop(224),\n                                      transforms.ToTensor()],\n                                      )\n","30a9a68d":"train_data = datasets.ImageFolder(data_dir + '\/train', transform=train_transforms)\nval_data = datasets.ImageFolder(data_dir + '\/val', transform=val_transforms)\n\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=64)","25ab46a6":"dataiter = iter(train_loader)\nimages, labels = dataiter.next()\nprint(type(images))\nprint(images.shape)\nprint(labels.shape)","2d029793":"def imshow(image):\n\n    fig, ax = plt.subplots()\n    \n    image = image.numpy().transpose(1, 2, 0)\n    print(image.shape)\n\n    \n    ax.imshow(image)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n    \n    return ax","8db6c218":"\nimshow(images[4]);\nimshow(images[23]);\nimshow(images[30]);","e40df331":"import torch.nn as nn\nimport numpy as np\n\nclass ConvNet(nn.Module):\n    # initialize the class and the parameters\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        \n        # convolutional layer 1 & max pool layer 1\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3),\n            nn.MaxPool2d(kernel_size=2))\n        \n        # convolutional layer 2 & max pool layer 2\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=4),\n            nn.MaxPool2d(kernel_size=2))\n        \n        #Fully connected layer\n        self.fc = nn.Linear(32*54*54, 6)\n     \n\n    \n    \n    # Feed forward the network\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        return out\n\nmodel = ConvNet()\nprint(model)","d19e25ec":"import torch.optim as optim\n\n# specify loss function\ncriterion = nn.CrossEntropyLoss()\n\n# specify optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.01)","7a49ed7b":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","80cc4f0e":"model.to(device)","64036b10":"epochs = 1\n\nfor epoch in range(epochs):\n    running_loss = 0\n    for images, classes in train_loader:\n        # Use GPU \n        images, classes = images.to(device), classes.to(device)\n        \n        # clears old gradients from the last step\n        optimizer.zero_grad()\n        \n        # train the images\n        outputs = model(images)\n        \n        #calculate the loss given the ouputs and the classes\n        loss = criterion(outputs, classes)\n        \n        # compute the loss of every parameter\n        loss.backward()\n        \n        # apply the optimizer and it's parameters\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n    else:\n        validation_loss = 0\n        accuracy = 0\n        \n        # to make the model run faster we are using the gradients on the train\n        with torch.no_grad():\n            # specify that this is validation and not training\n            model.eval()\n            for images, classes in val_loader:\n                \n                # Use GPU\n                images, classes = images.to(device), classes.to(device)\n                \n                # validate the images\n                outputs = model(images)\n                \n                # compute validation loss\n                loss = criterion(outputs, classes)\n                \n                validation_loss += loss.item()\n                \n                # \n                ps = torch.exp(outputs)\n                \n                #Returns the k largest elements of the given input tensor along a given dimension.\n                top_p, top_class = ps.topk(1, dim=1)\n                \n                # reshape the tensor\n                equals = top_class == classes.view(*top_class.shape)\n                \n                # calculate the accuracy.\n                accuracy += torch.mean(equals.type(torch.FloatTensor))\n        \n        # change the mode to train for other epochs\n        model.train()\n\n        print(\"Epoch: {}\/{}.. \".format(epoch+1, epochs),\n              \"Training Loss: {:.3f}.. \".format(running_loss\/len(train_loader)),\n              \"Valid Loss: {:.3f}.. \".format(validation_loss\/len(val_loader)),\n              \"Valid Accuracy: {:.3f}\".format(accuracy\/len(val_loader)))","33879d61":"model.class_to_idx = train_data.class_to_idx","09fdf947":"model.class_to_idx.items()","9ef13a38":"def process_image(image_path):\n    \n    test_transform = transforms.Compose([\n                                       transforms.RandomResizedCrop(224),\n                                       transforms.ToTensor()])\n    \n    from PIL import Image\n    im = Image.open(image_path)\n    im = test_transform(im)\n\n    return im","ac3786de":"def predict(image, model):\n    # we have to process the image as we did while training the others\n    image = process_image(image)\n    \n    image_input = image.unsqueeze(0)\n    \n    image_input.to(device)\n    \n#     model.eval()\n    outputs = model(image_input)\n   \n    ps = torch.exp(outputs)\n    \n    top_p, top_cls = ps.topk(5, dim=1)\n    top_cls = top_cls.detach().numpy().tolist()[0]\n    \n    # covert indices to classes\n    idx_to_class = {v: k for k, v in model.class_to_idx.items()}\n    \n    top_cls = [idx_to_class[top_class] for top_class in top_cls]\n    \n    return top_p, top_cls","fcb4c8ac":"import seaborn as sns\ndef plot_solution(image_path, ps, classes):\n    image = process_image(image_path)\n    imshow(image)\n\n    plt.subplot(2,1,2)\n    sns.barplot(x=ps, y=classes, color=sns.color_palette()[2]);\n    plt.show()","c41b1693":"\n\nimage = \"..\/input\/plants-smaller-data\/small_plant_dataset\/test\/2b433e05-810e-447c-92b6-023779814127___FREC_Pwd.M 4966.JPG\"\nps, classes = predict(image, model)\nps = ps.detach().numpy().tolist()[0]\nprint(ps)\nprint(classes)\n\nplot_solution(image, ps, classes)","f53d73a3":"The error above shows that we can not iterate(meaning loop through) over the dataset. Pytorch has a module for that **[DataLoader](https:\/\/pytorch.org\/docs\/stable\/data.html)**. It represents a Python iterable over a dataset.\nTherefore we iterate over the dataset:\n","86f59b92":"### 3.2 calculating output dimensions.\n\nIn the unit Convolution Neural Network(CNN), the trainer instructed you on how to calculate output dimensions.\n\n\nO =$(\\frac{W-K + 2P}{S} + 1)$\n\nWhere;\n\n    O - The ouptut height\/length\n    W - The input height\/length\n    K - The kernel size\n    P - Padding\n    S - the stride\n\n\nIn our simple example we will not work with padding. Therefore our calculation will be as follows; Our input image is 224 x224 x 3\n\n\n\n**On the First convolutional layer**\n- W = 224\n- K = 3\n\nO =$(\\frac{224-3 + 2*0}{1} + 1)$\n\n(224 - 3) + 1 = 222 \n\nour ouput will be 222 x 222 x 16 : **Note** 16 in the channel\/color dimensions which we randomly selected\n\n\n\n**Calculation through a maxpooling layer is determined by:**\n\n $(\\frac{W}{K})$\n        \n In our example we are passing a 224 x 224 x 16 image to a maxpool layer of kernel size 2. Our ouput will be:\n 222 \/ 2 = 111\n \n Our output image will be 111 x 111 x 16\n \n \n \n **On the second Convolutional Layer** \n \n - W = 111 \n - k = 4\n \n O =$(\\frac{111-4 + 2*0}{1} + 1)$\n \n (111 - 4) + 1 = 108 \n \n  Our output image will be 108 x 108 x 32\n  \n  \n \n ***Passing through a maxpooling layer***\n \n 108\/2 = 54\n \n Therfore our output image will be \n \n 54 x 54 x 32\n \n **fully connected layer**\n \n This is the image we'll flatten for our fully connected layers. In the fully connected layer you pass a flattened image and the number of output clases required in this case is 6.","d50e7b8a":"#### 2.2 Data Transformation\n\nFrom the error above it's clear that we need to use tensors. In out introduction we indicated that Pytorch behaves alot like Numpy arrays but instead of arrays it uses tensors. We will then pass the transformation to the dataset and load it again.\n\n**Tensors** ","a10c35ec":"When we iterate this time, this should work since the tensors are of equal sizes","091315c8":"First up, we need to get our dataset. Our dataset is the **data-plant** directory. We will provide the path as shown below","e7fd5009":"### 3. 1 Convolutional Neural Network:\n\nA CNN is made up of multiple layers of neurons. The layers mainly include convolutional layers and pooling layers. The convolutional layers have weights that need to be trained, while the pooling layers transform the activation using a fixed function.\n\n - convolutional layer - contains a set of filters whose height and weight are smaller than those of the input image.\n - max pooling layer - A pooling layer is usually incorporated between two successive convolutional layers. The pooling layer reduces the number of parameters and computation by down-sampling the representation. The pooling function can be max or average.\n \n - Fully connected layers\n \n \n \n \nIn our example we will implement 2 convolutional layers, 2 pooling layers and 1 fully connected later.\n ","8fb38f58":"## Convolutional Neural Network\n\nWe will build a simple CNN network that can take an image and determine the type of crop disease affecting a plant leaf. Here we'll use the planr dataset which consists of 38 different classes of plant diseases. Each class has a cetrain number of RGB images.\n\nCreating a CNN model involves the following:\n\n    Step 1: loading the dataset both train and test\n    Step 2: Define a Convolutional Neural Network\n    Step 3: Define a loss function\n    Step 4: Train the network on the training data\n    Step 5: Test the network on the test data","424ce568":"We have created a transform to resixe the images to 224 X 224. As we load each image using the **datasets ImageFolder** module we also transform it.\nWe then make iterable using **dataloader**","60a59de3":"#### 2.1 Data loader","becc9a6e":"Pytorch uses **[torchvision](https:\/\/pytorch.org\/docs\/stable\/torchvision\/index.html)** module to load datasets.The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.\nWe will use the **[ImageFolder](https:\/\/pytorch.org\/docs\/stable\/torchvision\/datasets.html#imagefolder)** class to load our dataset.\n\n**Note** Your folder **MUST** be arranged in this order:\n\n            train\/class_1\/1.png\n            train\/class_1\/2.png\n            train\/class_1\/n.png\n\n            train\/class_2\/x.png\n            train\/class_2\/y.png\n            train\/class_2\/z_n.png\n            \nIf you have the folder arranged in this format the ImageFolder will not work:\n\n            train\/1.png\n            train\/2.png\n            train\/n.png\n\nAll the images **must** be in their respective classes directory","96d16ab1":"### 3.4 Model prediction ","fdadc6fe":"### 3 Model\n\nPyTorch provides a module [nn](https:\/\/pytorch.org\/docs\/stable\/nn.html) that makes building networks much simpler. ","dec3f77b":"Iterate with transformation","e8acdd47":"Let's iterate over the dataset.","dccf6fd8":"### 1 Import the necessary packages","0b50c3ae":"***The tensors are not on equal sizes*** We have all taken photos will different gadgets, some of good quality that others and this means the resolution and dimensions of the images are different as well. We are dealing with such dataset. Therefore we will need to resize the images so that they are of the same shape. For image transformation in Pytorch refer [here](https:\/\/pytorch.org\/docs\/stable\/torchvision\/transforms.html).","df834e0f":"### 2 Load dataset","a7fd581a":"**Loss:**  We want to determine how far our model deviates from predicting true values\n\n - Cross entropy loss is good for calculating multi-class classifications\n\n\n**Optimizer**: methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses\n","3d6ab6b1":"### 3.3 Model Training:\n\n**Epoch** Number of iterations\n\n- For this excercise we use 1 epoch. You can try to run more epochs but this might require a higher processing power.","7cead807":"In image processing every image is inform of image pixels which translate to arrays. Pytorch uses the [PIL](http:\/\/https:\/\/pillow.readthedocs.io\/en\/stable\/), which is a library for image processing. i.e reading, opening and transforamtion of images.\n","2041019c":"#### 2.3 Data Visualisation \n\nLets view some of the images present. First we'll need to transform the image from tensor to a numpy array- Matplotlib works with arrays and not tensors\nThe shape of the numpy array will be 3 x 224 x224. since we are ploting using 2 dimensional array we will need to use [numpy.transpose](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.transpose.html). "}}