{"cell_type":{"bb8f56b7":"code","ed53d7e8":"code","1ea45031":"code","e9d2b2a6":"code","56c39b4a":"code","2c5e9bfe":"code","208addec":"code","b4f2e900":"code","447ef379":"code","34055849":"code","14551a5a":"code","d357b90b":"code","da3c039d":"code","c8b08ef0":"code","af11f3bc":"code","e56844b6":"code","fca23ec3":"code","1d1960a7":"code","5e5b1279":"code","12186747":"code","509bb2d3":"code","c665ab44":"code","a73c695c":"code","66a3a346":"code","a2a38519":"code","76aa58cd":"code","72eb4960":"code","c8835ae9":"code","71a42dd4":"code","259bb8db":"code","caef0b17":"code","d7a4eff3":"code","4344e495":"code","0a239eba":"code","809a9e29":"code","f5d9e334":"code","3abb6f6b":"markdown","52b63adb":"markdown","d0491dcd":"markdown","7ed3515b":"markdown","f3f6cb84":"markdown"},"source":{"bb8f56b7":"#Install libraries\n\n#!pip install tweepy\n#!pip install pycountry\n#!pip install wordcloud\n","ed53d7e8":"#Import libraries\n\nfrom textblob import TextBlob\nimport tweepy #get text\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport pycountry\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom PIL import Image\nfrom langdetect import detect\nfrom sklearn.feature_extraction.text import CountVectorizer\n","1ea45031":"consumerKey = \"\"\nconsumerSecret = \"\"\naccessToken = \"\"\naccessTokenSecret = \"\"\n\nauth = tweepy.OAuthHandler(consumerKey, consumerSecret)\nauth.set_access_token(accessToken, accessTokenSecret)\napi = tweepy.API(auth)","e9d2b2a6":"# tags = [\"koronavirus\",\"korona\",\"Covid-19\",\"pandemi\"]\n\ntweet_list = []\n\nkeyword = input(\"Please enter keyword or tag to search: \")\n\nnoOfTweet = int(input (\"Please enter how many tweets to analyze: \"))\n\ntweets = tweepy.Cursor(api.search, q=keyword).items(noOfTweet)\n\nfor tweet in tweets:\n    if tweet.lang == \"tr\":\n        tweet_list.append(tweet.text)\n","56c39b4a":"df = pd.DataFrame(tweet_list)\ndf.to_excel(r\"\/Users\/ulasarikaya\/Desktop\/korona.xlsx\", index = False)","2c5e9bfe":"df = pd.DataFrame(tweet_list)\ndf.head()","208addec":"data = pd.read_excel(\"\/Users\/ulasarikaya\/Desktop\/asd.xlsx\")  \ndata","b4f2e900":"data.drop_duplicates(inplace = True)\n\ndata.reset_index(drop=True)\n\n#unique tweets","447ef379":"import re\n\ntw_list = pd.DataFrame(data)\n\ntw_list[\"text\"] = tw_list[0]\n\n\nremove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n\nrt = lambda x: re.sub('[,\\.!?();:$%&#\"]', '', x)\n\ntw_list[\"text\"] = tw_list.text.map(remove_rt).map(rt)\ntw_list[\"text\"] = tw_list.text.str.lower()\n","34055849":"import nltk \nfrom nltk.corpus import stopwords\n\nWPT = nltk.WordPunctTokenizer()\nstop_word_list = nltk.corpus.stopwords.words('turkish')\n\ndef token(values):\n    filtered_words = [word for word in values.split() if word not in stop_word_list]\n    not_stopword_doc = \" \".join(filtered_words)\n    return not_stopword_doc\n\ndocs = tw_list['text']\n\ndocs = docs.map(lambda x: token(x))\ntw_list['text'] = docs\n\n\ntw_list.head(10)","14551a5a":"tw_list.drop(0, inplace=True,axis =1)","d357b90b":"from google_trans_new import google_translator  \n\ntranslator = google_translator() \n\ntw_list['eng'] = tw_list['text'].apply(lambda x: translator.translate(x, lang_tgt='en',lang_src='tr'))\n\ntw_list","da3c039d":"df2 = pd.DataFrame(tw_list)\ndf2.to_excel(r\"\/Users\/ulasarikaya\/Desktop\/out2.xlsx\", index = False)","c8b08ef0":"#Calculating Negative, Positive, Neutral values using textblob lib\n\nimport nltk\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\ntw_list[['polarity', 'subjectivity']] = tw_list['eng'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n\n\nfor index, row in tw_list['eng'].iteritems():\n    \n    score = SentimentIntensityAnalyzer().polarity_scores(row)\n    neg = score['neg']\n    neu = score['neu']\n    pos = score['pos']\n    comp = score['compound']\n    \n    if neg > pos:\n        tw_list.loc[index, 'sentiment'] = \"negative\"\n    elif pos > neg:\n        tw_list.loc[index, 'sentiment'] = \"positive\"\n    else:\n        tw_list.loc[index, 'sentiment'] = \"neutral\"\n        \n    tw_list.loc[index, 'neg'] = neg\n    tw_list.loc[index, 'neu'] = neu\n    tw_list.loc[index, 'pos'] = pos\n    tw_list.loc[index, 'compound'] = comp\n","af11f3bc":"#tw_list.drop(\"eng\", inplace=True , axis = 1)\n#tw_list.reset_index(inplace= True)\npd.set_option('display.max_colwidth', None)\ntw_list","e56844b6":"tw_list_negative = tw_list[tw_list[\"sentiment\"]==\"negative\"]\ntw_list_positive = tw_list[tw_list[\"sentiment\"]==\"positive\"]\ntw_list_neutral = tw_list[tw_list[\"sentiment\"]==\"neutral\"]","fca23ec3":"def count_values_in_column(data,feature):\n    total=data.loc[:,feature].value_counts(dropna=False)\n    \n    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n    \n    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])","1d1960a7":"count_values_in_column(tw_list,\"sentiment\")","5e5b1279":"tw_list_negative","12186747":"tw_list_positive","509bb2d3":"pichart = count_values_in_column(tw_list,\"sentiment\")\nnames= pichart.index\nsize=pichart[\"Percentage\"]\n \nmy_circle = plt.Circle( (0,0), 0.6, color='white')\n\nplt.pie(size, labels=names, colors=['green','blue','red'])\n\np=plt.gcf()\n\np.gca().add_artist(my_circle)\n\nplt.show()","c665ab44":"def create_wordcloud(text):\n    \n    mask = np.array(Image.open(\"\/Users\/ulasarikaya\/Desktop\/bulut.png\"))\n    stopwords = set(STOPWORDS)\n    wc = WordCloud(background_color=\"white\",\n                  mask = mask,\n                  max_words=3000,\n                  stopwords=stopwords,\n                  repeat=True)\n    \n    wc.generate(str(text))\n    wc.to_file(\"\/Users\/ulasarikaya\/Desktop\/wc.png\")\n    \n    print(\"Word Cloud Saved Successfully\")\n    path=\"\/Users\/ulasarikaya\/Desktop\/wc.png\"\n    display(Image.open(path))\n    ","a73c695c":"create_wordcloud(tw_list[\"text\"].values) #for all tweets","66a3a346":"create_wordcloud(tw_list_positive[\"text\"].values) #for positive sentiment","a2a38519":"create_wordcloud(tw_list_negative[\"text\"].values) #for neg sentiment","76aa58cd":"tw_list['text_len'] = tw_list['text'].astype(str).apply(len)\ntw_list['text_word_count'] = tw_list['text'].apply(lambda x: len(str(x).split()))","72eb4960":"round(pd.DataFrame(tw_list.groupby('sentiment').text_len.mean()),2)","c8835ae9":"import string \n\nstopword = nltk.corpus.stopwords.words('turkish')\nps = nltk.PorterStemmer()\n\n\ndef clean_text(text):\n    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n    text_rc = re.sub('[0-9]+', '', text_lc)\n    tokens = re.split('\\W+', text_rc)    # tokenization\n    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n    return text","71a42dd4":"countVectorizer = CountVectorizer(analyzer=clean_text) \ncountVector = countVectorizer.fit_transform(tw_list['text'])\n\nprint('{} Number of reviews has {} words'.format(countVector.shape[0], countVector.shape[1]))","259bb8db":"count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\ncount_vect_df.head()","caef0b17":"#most used words\n\n#count = pd.DataFrame(count_vect_df.sum())\n#countdf = count.sort_values(0,ascending=False).head(20)\n\ncountdf[0:18]","d7a4eff3":"from collections import Counter\n\n\nlabels = Counter(tw_list['sentiment']).keys()\nsum_ = Counter(tw_list['sentiment']).values()\ndf = pd.DataFrame(zip(labels,sum_), columns = ['sentiment', 'Toplam'])\n\ndf.plot(x = 'sentiment' , y = 'Toplam',kind = 'bar', legend = False, grid = True, figsize = (15,5))\nplt.title('Kategori Say\u0131lar\u0131n\u0131n G\u00f6rselle\u015ftirilmesi', fontsize = 20)\nplt.xlabel('Kategoriler', fontsize = 15)\nplt.ylabel('Toplam', fontsize = 15);","4344e495":"fig, ax = plt.subplots(figsize=(15, 10))\nax.pie(df.Toplam, labels =df.sentiment, autopct = '%1.2f%%',  startangle = 90 )\nax.axis('equal')\nplt.show()","0a239eba":"words = clean_text(''.join(str(tw_list['text'].tolist())))\n\nwords[:10]","809a9e29":"(pd.Series(nltk.ngrams(words, 2)).value_counts())[3:18]","f5d9e334":"bigrams_series = (pd.Series(nltk.ngrams(words, 2)).value_counts())[3:18]\n\nbigrams_series.sort_values().plot.barh(color='navy', width=.9, figsize=(12, 8))\nplt.title('En S\u0131k Kar\u015f\u0131la\u015f\u0131lan 15 Bigram')\nplt.ylabel('Bigram')\nplt.xlabel('Kelime Frekans\u0131')","3abb6f6b":"# N-grams","52b63adb":"# Duygu Analizi Yapabilmek \u0130\u00e7in Tweetleri Temizliyoruz","d0491dcd":"# Duygu Analizi","7ed3515b":"# Anahtar Kelime veya Hashtag ile Tweet \u00c7ekme","f3f6cb84":"# Twitter API i\u00e7in kimlik do\u011frulama"}}