{"cell_type":{"4b2d6981":"code","6e662625":"code","44ff5726":"code","1c3659bb":"code","44d33686":"code","c860d2f4":"code","bc6b795e":"code","052a0377":"code","2ff15002":"code","ffa212b0":"code","780c0364":"code","b2025396":"code","e98ac9d9":"code","0817dc39":"code","cf629d39":"code","2ab229c2":"code","eeede215":"code","0d3bacec":"code","6a4b4e70":"code","57b2b4cd":"code","57e429d9":"code","dfb0bc0d":"code","5996eb58":"code","64eae52f":"code","130a66f8":"code","6f309647":"code","83e189a8":"code","73f11667":"code","ab35bc86":"code","a314cea8":"code","beadf616":"code","510d57eb":"code","9f8aeabf":"code","3a2305e9":"code","5da1b7bf":"code","ea49dde9":"code","2f9d90e9":"code","881adcc5":"code","cafce7df":"code","b9df64aa":"code","3bf520ca":"code","eb78af3a":"code","915dca4c":"code","43b4a53b":"code","92bac82a":"code","b716ab93":"code","53f45692":"code","5e59d561":"code","6328fb46":"code","297938a5":"code","37bba935":"code","1d67b7b8":"code","f3d32bf3":"code","0259f4c8":"code","e2dd3c84":"code","bd498e65":"code","040c43db":"code","6202c65f":"code","ae0defef":"code","f9a7bdbf":"code","650e3d80":"code","2d687a2c":"code","7a1484f0":"markdown","4bb07dd3":"markdown","89726c2b":"markdown","06ba546d":"markdown","c4bf208d":"markdown","03c72719":"markdown","9f7226d3":"markdown","4c9be46d":"markdown","4316aa4f":"markdown","ba4f2b0f":"markdown","d9c359fc":"markdown","34447d67":"markdown","bb660f0c":"markdown","a77698f0":"markdown","dcdcd6f9":"markdown","e77a42aa":"markdown","9f6a2f13":"markdown","1ad0a993":"markdown","c296a026":"markdown","b3f43f88":"markdown","7304de69":"markdown","2205aecd":"markdown","822724cc":"markdown","061814be":"markdown","f9d51791":"markdown","455bfe12":"markdown","f79cc70a":"markdown","1b53d7a5":"markdown","2dd95087":"markdown","9550679e":"markdown","40343213":"markdown","6286c4e4":"markdown","6d48a942":"markdown","41c92211":"markdown","3d5816f6":"markdown","f0815d34":"markdown","a4ebcdaf":"markdown","d9e04be7":"markdown","885ab966":"markdown","8dde0088":"markdown","455bbefa":"markdown","fa0f7524":"markdown","31c72df1":"markdown","1866f582":"markdown","8f482631":"markdown","d07f7035":"markdown","2a2a4330":"markdown","aa416481":"markdown","46a5c52a":"markdown","9efa3f2d":"markdown","33c56bae":"markdown","76c128df":"markdown","b8ad9f8d":"markdown","b9fd540b":"markdown","83cecfaa":"markdown","eb7d836f":"markdown","1cfba5f4":"markdown","08c4722a":"markdown","35d23e3e":"markdown","bffb822f":"markdown","35960f61":"markdown","ef6dc11e":"markdown","c7228965":"markdown","29f32361":"markdown","56b8f56f":"markdown","a66743c6":"markdown","2e71d5fe":"markdown","5ac9f2a8":"markdown"},"source":{"4b2d6981":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set()\n\nplt.style.use('seaborn')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category = DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category = FutureWarning)\n\nfrom sklearn.utils.testing import ignore_warnings","6e662625":"train_raw = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_raw = pd.read_csv('..\/input\/titanic\/test.csv')\n\npassengerid = test_raw['PassengerId']","44ff5726":"train_raw.sample(4)","1c3659bb":"test_raw.sample(4)","44d33686":"train_copy = train_raw.copy(deep = True)\ntest_copy = test_raw.copy(deep = True)","c860d2f4":"plt.figure(figsize = (4,4))\nsns.countplot(x = 'Survived',data = train_copy)\nplt.title(\"1 Feature\")\nplt.show()","bc6b795e":"male_df = train_copy.loc[train_copy['Sex']=='male']\nfemale_df = train_copy.loc[train_copy['Sex']=='female']\n\nfig, axes = plt.subplots(1,2, figsize = (9,4))\nsns.countplot(x = 'Survived', data = male_df, ax = axes[0])\nsns.countplot(x = 'Survived',data = female_df, ax = axes[1])\nplt.show()","052a0377":"plt.figure(figsize = (6,4))\nsns.countplot(x = 'Survived',data = train_copy, hue = 'Sex')\nplt.title(\"Survival by Sex\")\nplt.show()","2ff15002":"sns.catplot(x = 'Survived', kind = 'count',col = 'Sex', data = train_copy, height = 3.3, aspect = 1.2)\nplt.title(\"Survival by Sex\")\nplt.show()","ffa212b0":"sns.catplot(x = 'Survived', kind = 'count',row = 'Sex',col = 'Embarked', data = train_copy,margin_titles = True, \n            height = 2.6, aspect = 1.2)\nplt.show()","780c0364":"n_rows = 2\nn_cols = 2\n\nfig, axes = plt.subplots(n_rows, n_cols,figsize=(12,8))\n\nsns.countplot(x = 'Embarked',hue = 'Survived',data = train_copy,ax = axes[0][0])\nsns.countplot(x = 'Pclass',hue = 'Survived',data = train_copy,ax = axes[0][1])\nsns.countplot(x = 'SibSp',hue = 'Survived',data = train_copy,ax = axes[1][0])\nsns.countplot(x = 'Parch',hue = 'Survived',data = train_copy,ax = axes[1][1])\n\naxes[1][0].legend(loc = 'upper right')\naxes[1][1].legend(loc = 'upper right')\n\nplt.show()","b2025396":"fig, axes = plt.subplots(figsize=(9,5))\nsns.heatmap(train_copy.isnull(), cbar = False, cmap = 'magma',ax = axes)\nplt.title(\"Missing values in train data\")\nplt.show()","e98ac9d9":"fig, axes = plt.subplots(figsize=(9,5))\nsns.heatmap(test_copy.isnull(), cbar = False, cmap = 'magma',ax = axes)\nplt.title(\"Missing values in test data\")\nplt.show()","0817dc39":"women_survived = train_copy.loc[train_copy['Sex']=='female']['Survived'].sum()\ntotal_women = train_copy.loc[train_copy['Sex']=='female'].shape[0]\n\nmen_survived = train_copy.loc[train_copy['Sex']=='male']['Survived'].sum()\ntotal_men = train_copy.loc[train_copy['Sex']=='male'].shape[0]\n\nfig, (ax1,ax2) = plt.subplots(nrows = 1, ncols = 2,figsize = (14,6))\n\nax1.pie([women_survived,total_women-women_survived],\n        labels = ['Survived','Dead'],\n        autopct = '%1.1f%%',\n        startangle = 20,\n        explode = [0,0.2]\n       )\nax1.set_title(\"Female\")\n\nax2.pie([men_survived,total_men-men_survived],\n        labels = ['Survived','Dead'],\n        autopct = '%1.1f%%',\n        startangle = 180,\n        explode = [0,0.2]\n       )\nax2.set_title(\"Male\")\n\nplt.show()","cf629d39":"sns.catplot(x = 'Pclass',y='Survived', kind = 'point',data = train_copy, hue = 'Sex',\n           height = 4,aspect = 2)\nplt.title(\"Pointplot : Survival vs Pclass\")\nplt.show()","2ab229c2":"sns.catplot(x = 'Pclass', col = 'Survived', kind = 'count',data = train_copy)\nplt.show()","eeede215":"sns.catplot(x = 'Embarked', y= 'Survived', kind = 'bar',data = train_copy)\nplt.title(\"Survival vs Embarked\")\nplt.show()","0d3bacec":"sns.catplot(x = 'Embarked', col = 'Survived', kind = 'count', data = train_copy,\n           height = 5, aspect = 1)\nplt.show()","6a4b4e70":"sns.catplot(x = 'Embarked',y = 'Survived',kind = 'point', hue = 'Sex',data = train_copy,height = 4, aspect = 2)\nplt.title(\"Survived vs Embarked by Sex\")\nplt.show()","57b2b4cd":"sns.catplot(x='Embarked', kind = 'count', col = 'Pclass', data=train_copy)\nplt.show()","57e429d9":"sns.catplot(x = 'Embarked', kind = 'point',y = 'Survived',col = 'Pclass', data = train_copy,margin_titles = True)\nplt.show()","dfb0bc0d":"sns.catplot(x = 'Embarked',y = 'Survived',kind = 'point',col = 'Pclass', hue = 'Sex', data = train_copy)\nplt.show()","5996eb58":"train_copy.loc[(train_copy['Sex']=='female') & (train_copy['Embarked']=='S') & (train_copy['Pclass']==1)&(train_copy['Survived']==0)]","64eae52f":"plt.figure(figsize = (16,9))\nsns.pointplot(x = 'Fare',y = 'Survived',data = train_copy)\nplt.title(\"Bear Grylls isn't happy with this chart for sure\")\nplt.xticks(rotation = 'vertical')\nplt.show()","130a66f8":"fig, axes = plt.subplots(figsize = (13,7))\nsns.swarmplot(y = 'Age', x = 'Pclass', hue = 'Survived',dodge = True,data = train_copy, ax = axes)\nplt.title(\"Swarmplot : Age vs Survival by Pclass\")\nplt.show()","6f309647":"sns.catplot(kind = 'swarm',x = 'Pclass', y = 'Age', col = 'Sex', hue = 'Survived',dodge = True,data = train_copy)\nplt.show()","83e189a8":"fig, axes = plt.subplots(figsize = (13,7))\nsns.swarmplot(x = 'Pclass', y = 'Fare', hue = 'Survived',dodge = True,data = train_copy, ax = axes)\nplt.show()","73f11667":"sns.catplot(kind = 'swarm',x = 'Pclass', y = 'Fare', col = 'Sex', hue = 'Survived',dodge = True,data = train_copy)\nplt.show()","ab35bc86":"common_titles = ['Mr.','Miss.','Mrs.','Master.'] #From prev analysis\n\nfor df in [train_copy, test_copy]:\n    \n    df['FamilySize'] = df['Parch']+df['SibSp']+1\n    \n    df['IsAlone']=0\n    df.loc[(df.FamilySize==1),'IsAlone'] = 1\n    \n    df['NameLen'] = df['Name'].apply(lambda x : len(x))\n    \n    df['Title'] = df['Name'].apply(lambda x : x.split(',')[1].strip().split()[0])\n    df['Title'] = df['Title'].apply( lambda x : x if x in common_titles else 'Misc.')","a314cea8":"plt.figure(figsize = (22,10))\nsns.lineplot(x = 'NameLen', y = 'Survived',data = train_copy, marker = 'o')\nplt.xticks(train_copy['NameLen'])\nplt.title(\"Survival by NameLen\")\nplt.show()","beadf616":"train_copy.loc[train_copy['NameLen']==54]","510d57eb":"sns.catplot(kind = 'bar', x = 'NameLen',y = 'Survived', data = train_copy, height = 7, aspect = 2)\nplt.show()","9f8aeabf":"train_copy['NameLenBin'],train_namelen_bins = pd.cut(train_copy['NameLen'].astype(int),20,retbins = True)","3a2305e9":"sns.countplot(data = train_copy, x = 'NameLenBin')\nplt.xticks(rotation = 'vertical')\nplt.title(\"Count of passengers by NameLen\")\nplt.show()","5da1b7bf":"sns.barplot(data = train_copy, x = 'NameLenBin', y = 'Survived')\nplt.xticks(rotation = 'vertical')\nplt.title(\"Survival by NameLenBin\")\nplt.show()","ea49dde9":"fig, axes = plt.subplots(1,2, figsize=(11,5))\n\nsns.countplot(x = 'Title', data = train_copy, ax = axes [0])\nsns.barplot(x = 'Title', y = 'Survived', data = train_copy, ax = axes[1])\nplt.show()","2f9d90e9":"fig, axes = plt.subplots(1,2, figsize = (15,6))\n\nsns.countplot(x = 'FamilySize', data = train_copy, ax = axes [0])\nsns.barplot(x = 'FamilySize', y = 'Survived', data = train_copy, ax = axes[1])\nplt.show()","881adcc5":"fig, saxis = plt.subplots(1, 2,figsize=(12,5))\n\nsns.pointplot(x = 'IsAlone', y = 'Survived',  kind = 'point', data = train_copy, ax = saxis[0])\nsns.pointplot(x = 'IsAlone', y = 'Survived', hue = 'Sex', kind = 'point', data = train_copy, ax = saxis[1])\nplt.show()","cafce7df":"#Filling missing ages by title\n\ntitlewise_grouping = train_copy.groupby('Title')\ntitlewise_agemean = titlewise_grouping.mean()\n\ntitlewise_agemean['Age'].to_frame()","b9df64aa":"train_copy['Age'] = titlewise_grouping['Age'].apply(lambda x: x.fillna(x.mean()))","3bf520ca":"#Same for test now\n\ntitlewise_grouping = test_copy.groupby('Title')\ntitlewise_agemean = titlewise_grouping.mean()\n\ntest_copy['Age'] = titlewise_grouping['Age'].apply(lambda x: x.fillna(x.mean()))","eb78af3a":"#Filling Embarked and Fare and compiling them into bins\n\nfor dataset in [train_copy, test_copy]:\n    dataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].mean())\n    dataset['Embarked'] = dataset['Embarked'].fillna(dataset['Embarked'].mode()[0])","915dca4c":"#NameLenBins already visualized above\ntrain_copy['AgeBin'],train_age_bins = pd.cut(train_copy['Age'],8,retbins = True)\ntrain_copy['FareBin'],train_fare_bins = pd.qcut(train_copy['Fare'],8,retbins = True)","43b4a53b":"test_age_bins = np.concatenate(([-np.inf], train_age_bins[1:-1], [np.inf]))\ntest_namelen_bins = np.concatenate(([-np.inf], train_namelen_bins[1:-1], [np.inf]))\n\ntest_copy['AgeBin'] = pd.cut(test_copy['Age'], test_age_bins)\ntest_copy['NameLenBin'] = pd.cut(test_copy['NameLen'], test_namelen_bins)\n\ntest_fare_bins = np.concatenate(([-np.inf], train_fare_bins[1:-1], [np.inf]))\ntest_copy['FareBin'] = pd.cut(test_copy['Fare'], test_fare_bins)","92bac82a":"print(\"Done. Final Check\")\nprint(train_copy.isnull().sum())\nprint(\"-\"*25)\nprint(test_copy.isnull().sum())","b716ab93":"train_copy.sample(5)","53f45692":"test_copy.sample(5)","5e59d561":"for dataset in [train_copy,test_copy]:\n    dataset = dataset.drop(['Cabin','PassengerId','Ticket','Name','FareBin','SibSp','Parch','NameLenBin','Age'],axis = 'columns',inplace = True)","6328fb46":"train_copy = pd.get_dummies(train_copy,columns = ['Embarked'],drop_first = False)\ntest_copy = pd.get_dummies(test_copy, columns = ['Embarked'],drop_first = False)","297938a5":"for dataframe in [train_copy,test_copy]:\n    \n    dataframe['Sex'] = dataframe['Sex'].map( {'male':0, 'female': 1 } )\n    dataframe['Title'] = dataframe['Title'].map( {'Mr.':1, 'Misc.': 0, 'Master.':2, 'Miss.': 3, 'Mrs.': 4 } )\n    dataframe['Pclass'] = dataframe['Pclass'].map( {1:3,2:2,3:1} )","37bba935":"from sklearn.preprocessing import LabelEncoder\n\nlabeler = LabelEncoder()\n\ntrain_copy['AgeBin'] = labeler.fit_transform(train_copy['AgeBin'])\ntest_copy['AgeBin'] = labeler.fit_transform(test_copy['AgeBin'])","1d67b7b8":"train_copy","f3d32bf3":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n#TRAIN\nscaler.fit(train_copy.drop(['Survived'],axis=1)) \n#As inplace = True has not been written the scaler will get train_copy without 'Survived', but train_copy is unaffected\n\nscaled_features = scaler.transform(train_copy.drop(['Survived'],axis=1))\ntrain_copy_scaled = pd.DataFrame(scaled_features)\n\n#TEST\ntest_copy.fillna(test_copy.mean(), inplace=True)\n\nscaled_features = scaler.transform(test_copy)\ntest_copy_scaled = pd.DataFrame(scaled_features)","0259f4c8":"train_copy_scaled.sample(5)","e2dd3c84":"X_train_scaled = train_copy_scaled\ny_train_scaled = train_copy['Survived']\nX_test_scaled = test_copy_scaled","bd498e65":"X_train = train_copy.drop('Survived',axis = 1)\ny_train = train_copy['Survived']\nX_test = test_copy.copy(deep = True)","040c43db":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import GridSearchCV","6202c65f":"svc = SVC(C=100, gamma=0.01)\nscores_rfc = cross_val_score(svc, X_train, y_train, cv=10, scoring='accuracy')\nprint(\"Mean score for 10 folds is \",scores_rfc.mean()*100)","ae0defef":"svc = SVC(C=100, gamma=0.01)\nscores_rfc = cross_val_score(svc, X_train_scaled, y_train_scaled, cv=10, scoring='accuracy')\nprint(\"Mean score for 10 folds is \",scores_rfc.mean()*100)","f9a7bdbf":"def get_best_score(model):\n    \n    print(model.best_score_)    \n    print(model.best_params_)\n    print(model.best_estimator_)\n    \n    return model.best_score_","650e3d80":"param_grid = {'C': [0.1,10, 100, 1000,5000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}\n\nbest_svc = GridSearchCV(SVC(), param_grid, cv=10, refit=True, verbose=0)\nbest_svc.fit(X_train_scaled,y_train_scaled)","2d687a2c":"pred_all_svc = best_svc.predict(X_test_scaled)\n\nsub_svc = pd.DataFrame()\n\nsub_svc['PassengerId'] = passengerid\nsub_svc['Survived'] = pred_all_svc\n\nsub_svc.to_csv('bear_grylls.csv',index=False)","7a1484f0":"We'll make a deep copy of the `train_raw` and `test_raw` and perform changes on them. This will be helpful if we ever need to refer to the original uneditted dataframe.\n\n- We perform a deep copy using `copy(deep = True)`\n\nA deep copy can be understood with the similar concept of \"pass by reference\" vs \"pass by value\" concept in functions. The following gif will suffice.\n\n![Shallow Copy (Left) vs Deep Copy (Right)](https:\/\/s7.gifyu.com\/images\/ezgif-4-d1192ecfb56b.gif)","4bb07dd3":"We can already see that the passengers who paid above 500 (3 of them) survived safely.\n\nFor the rest, the distribution seems kind of uniform and there seems to be no distinction in how to distinguish between them. We got people who paid high dying, and we got people paying low who survived too, across all Pclass.\n\nLet us have another layer of segregation using Sex to see if we discover something.","89726c2b":"# Survival by Fare and Pclass\nFare is also continuous variable, so we'll use swarmplots again to study Fare.","06ba546d":"# Data Wrangling\n\nWe have seen all the data vizualizations above without filling any NaN values. This is so that we could kind of get an unadulterated trend in the training data before filling and treating them.\n\nFrom this part on, I will be slightly changing some methods and techniques so we can land at a solid 79.9% accuracy.\nFeel free to change the technique, filling methods, binning frequency, distribution etc to change the accuracy on test score.\n\nThe visuals and insights above will suffice to achieve a score over 80% on the test data.\n\n## A moment of your time.\n\nKeeping this whole humorous-themed kernel aside, there is an honest observation I'd like to share\n\n- In my various submissions over time, I have so far got only 3 submissions that scored over 80%. So, honestly in my opinion, a 79.9% accuracy is a pretty great score; and I think me getting 80% in some submissions was totally do-able, but a bit stochatic in nature too. Contrary to that I've got 79.9% accuracy in a lot of submissions.\n- I believe that unless you can get some consistent accuracy on test data that is above 82% (top 3% on the leaderloard), getting 80% score on test data boils down to model, features and a good portion of luck.\n- There has been a sudden boom in the number of people registring at Kaggle, and more than ever, there are people who just submit answer csv files without much work. So, don't judge yourself too hard by the leaderboard rankings.\n- A score of 79.9% should anyway get you somewhere under top 15%, because a lot of people do submit high scoring kernel's output files wihtout much thought. So, don't let the leaderboard decide your worth.\n- As long as you can learn and get a score above 79%, you can assure you did good. The prize for this competition is 'Knowledge' and that's all you should aim for. Nothing more, nothing less.","c4bf208d":"**We can clearly see the tremendous fluctuations for Q in Pclass 1 and Pclass 2**\nLet's see if we can add another segregation wrt Sex and notice some pattern.","03c72719":"Let's have a quick overview of the train_copy and test_copy","9f7226d3":"If needed, we can use `hue` too and get comparisions between 4 features. If it were a more complicated graph (countplots show count of only *one* feature, while lineplots, swarmplots etc show realtionship between 2 features in themselves), then we could get much more comparisions. However, it becomes absurd and impractical after a point to visualize so many features together.\n\nSome kind of plots are not supported under `kind` of catplots; in such case we can use subplots to get our desired visuals. A sample example of using subplots is given below to show the count of total passengers on basis of\n\n- Embarked\n- Pclass\n- SibSp : Number of siblings\/spouse onboard the titanic\n- Parch : No of parents\/children onboard the titanic","4c9be46d":"Let's explore a `IsAlone` column, created from `FamilySize`","4316aa4f":"Family size had been calculated by `df['FamilySize'] = df['Parch']+df['SibSp']+1`, so those who are travelling alone in the titanic, inclusive of themselves form a family size of 1. \n\n- It is pretty clear that a majority of people were travelling alone (more than 500 in train data) and they had a very low survival rate (close to 30%) with little deviations.\n- Family size of 5,6 and 7 also range around the 30% survival rate mark, but since the count of people having that family range is fairly low, and the black bar shows a lot of deviations\n- Tt's safe to assume that being alone on the titanic is not a great idea for survival.","ba4f2b0f":"## Survival by Age and Pclass\n\nSince age is a continuous variable, it'll clutter up the barcharts, pointplots and countplots. We will use something more suitable like swarmplots for such variables.\n\nAnyway, if you're unsure why we should not use pointplots and likewise for continuous variables, feel free to unhide the output of the next code.","d9c359fc":"We could go with a similar approach for Fare and Embarked too, but since this dataset has been used so much and uit's not really hidden at this point, we'll ease off the process by just using mean() and mode() to fill in the missing values to save time.","34447d67":"## IsAlone and Sex","bb660f0c":"If we draw an imaginary line at the 10 Age mark, we can see a lot of green dots (Survived) belong majorly to Pclass1 and Pclass2. The survival of young kids don't seem to be that good in Pclass3.\n\nAlso, since the number of females dying in Pclass 1 is only 3, and two of them overlap in the region of those who survived, it's quite safe to assume that almost all females belonging to Pclass 1 will infact survive irrespective of thier age. The same is true for females in Pclass 2.\n\nThe most deaths of female occur in Pclass3 where they tend to overlap fairly with those who survived.","a77698f0":"Now we can bring the normal conventional `X_train`,`y_train`,`X_test` and `y_test` nomenclature. <br>\n\nWe'll have a shallow copy of the train_copy and test_copy so that if you make any changes in `train_copy` and `test_copy` above, final `X_train`,`y_train`,`X_test` and `y_test` reflect those changes.","dcdcd6f9":" A lot of people died in 3rd Pclass, but of those who survived, thier distribution is somewhat similar to other classes.\n\nHowever, in total cummulative picture, a survival decreases with higher Pclass. \nThus, we can conclude that survival decreases if you don't belong to top class, irrespective of the gender.\n\nIf you're still wondering **why Pclass\/Socio Economic factor is important**, just tell me honestly which Pclass Bear Grylls do you think will survive?\n\n![](https:\/\/i.imgur.com\/Cenb5jX.png)","e77a42aa":"There seems to be some correlation between how the NameLength of a passenger is related to the survival. Especially in the 15 to 35-ish range, we can see a pretty linear trend going on. After 30 however, the trend stars to go erratic.\n\nThere's a sharp decline near the 54 mark, and since there is no background colored area to show deviation, I am speculating just one person who had Name Length equal to 54 died.","9f6a2f13":"It's interesting to note that passengers who actually paid more in Pclass1 and Pclass 2 died for males.\n\nFor females, we already knew from the swarmplot above that a majority of them would survive in Pclass1 and Pclass2. This holds true here also.\nOf those females in Pclass3 who died, those who paid more in Pclass 3 died (about 9 of them)\n\nSince Age and Fare are continuous variables, it'll make more sense to put the in bins and try to detect a pattern, else our assumptions may lead to overfitting.\nWe'll do that in Feature Engg. section since it's realted to making new columns.","1ad0a993":"# Starter Code\n\nLet us get the initials done rightaway.","c296a026":"**VERDICT WITH BEAR GRYLLS**\n\n- Survival as well as Death rate goes in the trend S>C>Q.\n- However, in cummulative view, S class people had majority of passengers dying. <br>\n- C class and Q class had much lower people dying in it, which is also contributed by the fact that majority of passengers were travelling to S.\n- As count of total passengers in S class is very high, it's survival count is also very high as a result despite having the highest death rate too.\n\nIt's like compairing a small population country (Country A) with a large population country (Country B) and stating that there is a much larger workforce available in Country B, even when most of them could be enemployed.","b3f43f88":"# Introduction\n\nAfter a previous colaboration with John Wick, we have a new guest in the house to guide the many beginners for Titanic Machine Learning - the Survival expert, Bear Grylls\n\n![](https:\/\/i.imgur.com\/IfUnawa.png)\n\nBear has read many kernels, so much so that he concludes that a **[score above 80% accuracy is considered very good](https:\/\/www.kaggle.com\/carlmcbrideellis\/titanic-leaderboard-a-score-0-8-is-great)**. Bear wants to help all the other people by giving them the insights and visuals; however, he also believes that trying things on own is the only way to learn.\n\nSo, he was kind enough to extract all the insights needed to score above 80%, but shared with me only a baseline kernel to get a 79.9% accuracy, leaving the readers to explore the uncharted waters for themselves.","7304de69":"## Visual of empty values","2205aecd":"- We can clearly make out by the first glance that a lot of passengers going to S belong to 3rd Pclass. We already know passengers in Pclass3 have a lower chance at survival. Also, the total number of passengers is not evenly distributed for each destination.\n- People going to S ranks highest in all the Pclass.\n- Since people embarking to Q are very less in Pclass 1 and Pclass 2 (Pclasses with higher survival rates), Survival function wrt Embarked for Q will tend to have a lot of deviations, as the number of passengers are very less, so changes in state of 1 passenger shakes up the scale very much","822724cc":"Let us Bin the Age and Fare variables now. We'll make binnings in the train_data and return those bins.\nWe'll the modify the first and last range value of this returned bin and use it to arrange the test data.\nJust like we filled NameLenBin before, we do not need to worry about any unseen number in test.","061814be":"### Survival Percentage by Gender\n\nWe all know that females in Titanic had a much better chance at survival","f9d51791":"### 2 features analysis\nThere are many ways to visualize a feature with respect to another feature, ie, vizualizing two features at once.\n\n- Using `hue` property\n- Using subplots\n- Using `row` \/ `col` with catplot","455bfe12":"**Using subplots** <br>\nThe conventional subplot methods fits everywhere.","f79cc70a":"We can see a pretty smooth linear realation except at the end. But for a majority of the passengers, NameLength can be a useful feature.\n\nBear Grylls would agree on this.\n<br>\n\n![](https:\/\/i.imgur.com\/hL2irAq.png)","1b53d7a5":"Now would be a good idea to actually see how many females died in Pclass 3 to begin with. We will segregate the above swarm plot by Sex into two columns.\nWe should see a fairly low number of deaths for female in Pclass1 and Pclass2 too.","2dd95087":"## CV Score\n\n### Unscaled features","9550679e":"## Scaling down features\nThis is to ensure that large values don't overestimate or have a bigger influence on the target variables.\nWe'll use the StandardScaler class from sklearn.","40343213":"## Dropping features\n\nWe'll be dropping 'Cabin','PassengerId','Ticket','Name','FareBin','SibSp','Parch','NameLenBin' and 'Age'.\n\n![](https:\/\/s7.gifyu.com\/images\/ezgif-4-37c465b943ed.gif)","6286c4e4":"**\u0909\u0926\u094d\u092f\u092e\u0947\u0928 \u0939\u093f \u0938\u093f\u0926\u094d\u0927\u094d\u092f\u0928\u094d\u0924\u093f \u0915\u093e\u0930\u094d\u092f\u093e\u0923\u093f \u0928 \u092e\u0928\u094b\u0930\u0925\u0948\u0903\u0964**<br\/>\n**\u0928\u0939\u093f \u0938\u0941\u092a\u094d\u0924\u0938\u094d\u092f \u0938\u093f\u0902\u0939\u0938\u094d\u092f \u092e\u0941\u0916\u0947 \u092a\u094d\u0930\u0935\u093f\u0936\u0928\u094d\u0924\u093f \u092e\u0943\u0917\u093e\u0903\u0964\u0964**\n    \n*A task can be accomplished only through proactive enthusiasm,<br\/>\nnever by imaginary thoughts alone.*","6d48a942":"# Feature Engg\n- Title\n- NameLen\n- IsAlone\n- Family Size\n- AgeBin\n- FareBin\n\n![](https:\/\/s7.gifyu.com\/images\/tenor7b4dde9c45596648.gif)","41c92211":"# End Credits\n\nPrevious Guest : [John Wick](https:\/\/www.kaggle.com\/siraznaorem\/50-python-tricks-of-coder-john-wick-s-beginner)<br>\nCarl Mc BrdieEllis : [Titanic score of 0.8](https:\/\/www.kaggle.com\/carlmcbrideellis\/titanic-leaderboard-a-score-0-8-is-great)","3d5816f6":"There are only two such women.\n- For females, Pclass1 and Pclass2 are the best status\n- For females in Pclass3, the survival dips down significantly, and is very much affected by the Embarked feature.","f0815d34":"A very strange trend being detetcted is that males belonging to all Pclass have a survival chance drop if they are embarking to Q.\nWe can conclude that the fluctuations in the precious graph for Pclass 1 and Pclass 2 were cuased because while most males died, the females survived.\n\n**VERDICT WITH BEAR GRYLLS**\n\n- Males in Pclass1 and Pclass2 have significantly lesser survival rates compared to even males of Pclass 3\n- We can almost guarantee that a female belonging to Pclass1 or Pclass2 and who is embarking to Q will have a good survival rate. \n\nSurvival of female is the least when embarking to 'S' provided she belongs to Pclass3. \nHowever, if you're a female and embarking to 'S', you're very safe if you belong to either Pclass1 or Pclass2. Let us just see the number of females embarking to S belong to Pclass1 who actually died.\nThe graphs suggest that the number won't be large.","a4ebcdaf":"![](https:\/\/i.imgur.com\/VZ3CxvH.png)\n\n## Final Note\n\nBefore ending off, there's a small snippet I think will be helpful if anyone tries to use XGBoost. XGBoost doesn't allow `[`,`]` and `<` characters to be column names during fitting. If you happen to One Hot Encode any of the \"Bins\", you can remove these characters from the column names using REGEX.\n\nTHIS IS ONLY FOR XGBOOST\n\n`\nimport re\nregex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\nX_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\nX_test.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_test.columns.values]\n`","d9e04be7":"## NameLength as Survival function\nThis was a rather interesting feature I found in a lot of kernels...\n\nAhemm... I meant which Bear found in a lot of kernels.","885ab966":"### Scaled Features","8dde0088":"### Unscaled Features","455bbefa":"## Custom Mappings\n\nThis section is only for actually mapping Titles to numerical values.\n\nThe mapping for Pclass and Sex has been done just for personal preferences, as \n- survival in Pclass1>Pclass2>Pclass3\n- survival of female>male\n\nYou need not do it; ML models will accordingly detect the relationship as either positively or negatively correalted for Label Encoded features. It is just for my personal preference and shouldn't affect the accuracy of the final model in the kernel.","fa0f7524":"# Prep for 79.9% accuracy","31c72df1":"## Survival by Pclass\n\nThis is a very important feature as the trend shown by it is almost perfect with the target variable.\nPclass is a measure of the Socio Economic status of the passenger.\n\nPclass1 is the first class<br>\nPclass2 is the second class<br>\nPclass3 is the third class<br>","1866f582":"# Model Selection : SVC\n\nI'll be using SVC for the final model predicition. Feel free to use better models or ensemble techniques.","8f482631":"### Scaled features","d07f7035":"Let us apply title_wise age filling in the transformations back in test data too. Any other title which does not fall under `['Mr.','Mrs.','Master.','Miss.']` will be automaticall treated as `'Misc.'`, so you do not need to worry about any unseen label in test.","2a2a4330":"Let's bin up using pd.cut()\n\n- pd.cut() arranges continuous variables into bins which are a fixed distance apart.\n- pd.qcut() arranges continuous variables into bins which are made with dynamic ranges, such that each bin has approx uniform distribution.\n\nA good plus point of using pd.cut() is that we can obtain the bins formed on training data using `retbins = True`. We can then use this returned bin to bin the test data by modifying the first and last limit to some extreme value, eg, for binning NameLength in it can be test_copy['NameLen'].min()\n\nThus, we use the bins which were trained on training data, and whose range is unaffected by test_data, yet holds the capability to bin them.\nSince this is a knowledge competition and I already have the test data seen, I'll use `np.inf` to ease the code for now, though this should be avoided in other cases where this approach will fail for outliers\/extreme values.","aa416481":"# EDA\n\nBefore jumping to EDA, for those who are purely new, we'll be using seaborn library on top of matplotlib.\nIf you've gone over the Vizualization basics in Kaggle microcourse, this won't be very new. But for those who are new, plus, who want to just refresh a bit of what was taught, I'll sum things up.","46a5c52a":"Since Embarked was such a complicated feature to get a trend of, we'll simply use OHE on it and let the model decide the trend itself on basis of other features. A good idea while making One Hot Encoded features is to drop the first Column to avoid Dummy Variable Trap.\n\nHowever, since we are looking at a compeition, it's a practical practice to sometimes give in conventional practices to get higher accuracy. It does make the model unfit for general purposes, but we'll have an accuracy-oriented mindset here and set `drop_first = False`","9efa3f2d":"In cumulative view, we can say that people who are not alone have a tendency to survive better.\n\nA Sex-wise segregation reveals that this trend is opposite in case of females. \n\n- Females that are infact Alone, do better at survival (close to 80%) as compared to females who are not alone (close to 70%). \n- However, their overall survival chances are much more than men, irrespective of wether they are alone or not.","33c56bae":"Let us draw a barplot to visualize this same chart without the stretch in x-axis due to varible Name Length.\nThen, we can try to put these Lengths in some approprate bins to generalize the trend.","76c128df":"We can see that most people have Name Length of around 15-33. For the rest of people with longer names, we can see thier count decreases.\nNeedless to say, if those people who are less in number, but have a longer name, have better chances of survival than those with smaller names, we can have a pretty good feature for survival in the making.","b8ad9f8d":"## Survival by Embarked and Pclass : Interesting finds","b9fd540b":"## Label Encoding AgeBin Class\nFeel free to experiement here to alter the accuracy. I will be only label encoding the AgeBin variable here.","83cecfaa":"**Using catplots**\n\nThis is a very powerful combined plot which allows you to define a lot of relationships between different features, thanks to it's available arguments.\n- row = will plt the all features w.r.t the feature passed in row, and draw them on different rows\n- col = will plt the all features w.r.t the feature passed in row, and draw them on different cols\n- hue = will allow vizualization by separating other features wrt the feature defined in hue and color coding them\n\nAn important point to remember is that catplots don't allow axes, so you can't use them concurrently with subplots. You can resize catplots using `height` and `aspect`","eb7d836f":"- Note that for passenger's going to 'Q', women have lesser survival chances to other women, \n- But for men, being a passenger embarking to 'S' has the least survival chances.\n\nIn cumulative picture however, women have better survival rates than men irrespective of Embarking destination.","1cfba5f4":"### 1 feature Analysis\n\nEssentially, what I mean by 1-D is that the visualization will focus on only one feature. This can be for example, a count of some categorical feature.","08c4722a":"**VERDICT WITH BEAR GRYLLS**\n\n- We can see a decline in the survival rate as the Pclass increases.\n- People with higher class have a better chance of survival than others. (for both males as well as female)\n\nBut we do have to ask, how many people were even belonging to 1st class (Pclass) to begin with before making assumptions.<br>\nThis is because in the rare case that only there are very less people in Pclass2 and Pclass3 combined, the survival of Pclass being 1 would be a very biased metric.","35d23e3e":"## GridSearchCV for Hyperparameters","bffb822f":"Now that we're done with the recap let's continue with some EDA.","35960f61":"### 3 features analysis\nAs mentioned above, we can pass many arguments in catplot. If we use row and col together with another feature being plotted, we will get three features worth of distinction.\n\nTo elaborate this point, let us assume we want to see a count of those passengers along with \n- wether they survived or not\n- thier emabrking destination\n- thier gender all together.","ef6dc11e":"**Using `hue`** <br>\nHue will segregate the data w.r.t the feature passed in as argument.","c7228965":"## Survival by Embarked\n\nEmbarked is what I found to be the most interesting feature. It gives pretty interesting insights when looked at with other features in consideration, especially with Pclass.\n\nEmbarked has two points worth nothing about\n- They don't necesarily follow a straight-forward trend like Pclass\n- The count distribution of passengers going to different destinations is NOT evenly distributed at all, with destionation 'S' dominating as the passengers destination.","29f32361":"![](https:\/\/i.imgur.com\/MJ7GgzC.png)","56b8f56f":"Let's have a look at the count distribution for Embarked.","a66743c6":"## Family size as Survival Function","2e71d5fe":"## Title as Survival Function","5ac9f2a8":"A lot of people are male (Mr.) and they inturn have the least survival rate (less than 20%). \nSo, if we use Title feature correctly, we can have a huge advantage as Names has 0 null values. \n\nThe extracted title can help us fill Ages too, and make a much better model."}}