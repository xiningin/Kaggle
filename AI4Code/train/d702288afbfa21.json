{"cell_type":{"98444ca6":"code","011bb0e7":"code","3b67e1b7":"code","b0e36251":"code","517c87df":"code","82a92948":"code","8b8f14e8":"code","55e570f2":"code","3cd4fa6a":"code","ff91661e":"code","9a60fe90":"code","6db7e69a":"code","08e11773":"code","393aa6cc":"code","0d969331":"code","d326fc73":"code","b7996ad2":"markdown","d377c5d9":"markdown","c9b2ed4d":"markdown"},"source":{"98444ca6":"%%capture\nimport os\nimport sys\nimport random\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom keras.preprocessing.image import img_to_array\ntry:\n    from imutils import paths\nexcept ImportError:\n    !pip install imutils\n    from imutils import paths\n    \nimport matplotlib.pyplot as plt\n%matplotlib inline","011bb0e7":"# lets start by examining the number of images available in training set\ndir_train = \"..\/input\/intel-image-classification\/seg_train\/seg_train\/\"\nfor segment in os.listdir(dir_train):\n    print(\"'{}' images in '{}' category\".format(\n        len(list(paths.list_images(dir_train + segment))),\n        segment\n    ))\nprint(\"Total '{}' images in training set\".format(len(list(paths.list_images(dir_train)))))","3b67e1b7":"class_list = os.listdir(dir_train)\n\n# number of images to display for each class\ncolumns = 3\n\n# input shape 150x150\nshape = (150, 150)\n\n# let's create a dictionary with all the classes and some random images to display\nclasses = { \n    cls : [\n        # read each mage and change the default colorspace\n        cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB) for img in random.sample(\n            # randomly sample images (length of columns) per class\n            list(paths.list_images(dir_train + cls)), columns\n        )\n    ] for cls in class_list\n}\n\n# method to displays images for 'len(classes)' classes 'len(columns)' images per row\ndef display(classes, columns, cmap=None, figsize=(10, 10)):\n    for _class in classes:\n        # print(random_images)\n        fig, axes = plt.subplots(\n            nrows=1, ncols=columns, \n            figsize=figsize, squeeze=False\n        )\n        fig.tight_layout()\n        for l in range(1):\n            for m, img in enumerate(classes[_class]):\n                # set map\n                if len(img.shape) == 2:\n                    cmap = \"gray\"\n                axes[l][m].imshow(img, cmap=cmap)\n                axes[l][m].axis(\"off\")\n                axes[l][m].set_title(_class)\n    # done displaying\n    \n# display images\ndisplay(classes, columns)","b0e36251":"# let us read a random image from a class and examine it for preprocessing\nkey = random.choice(list(classes.items()))[0]\nimage = random.sample(list(classes[key]), 1)","517c87df":"# let's display the image dimension and the image itself\ndisplay({key:image}, 1, figsize=(4, 4))\nprint(\"Dimensions: \" + str(image[0].shape))","82a92948":"def preprocess(image, reshape=True):\n    # copy for later usage?\n    img = image.astype(np.uint8)\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    # split\n    h,s,v = cv2.split(img)\n    # histogram equalization on value \n    v_ = cv2.equalizeHist(v)\n    # merge back\n    img = cv2.merge([h,s,v_])\n    # to rgb\n    img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n    # apply gaussian blur\n    img = cv2.GaussianBlur(img, (3, 3), 1)\n    # to grayscale\n    # img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    #rescale\n    img = img\/255.\n    # done\n    return img \n\nimg = image[0]\n# before after image display\ndisplay({\"before\": [img], \"after\": [preprocess(img, reshape=False)]}, 1, figsize=(4,4))","8b8f14e8":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\n\n# seed constant for reproducable results\nSEED = 255\nnp.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\ntf.random.set_seed(SEED)\n# bunch of other stuff\nfrom keras import backend as K\ntf_config = tf.compat.v1.ConfigProto(\n    intra_op_parallelism_threads=1, \n    inter_op_parallelism_threads=1, \n    device_count = { \"CPU\": 1,\"GPU\": 1 } \n)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=tf_config)\ntf.compat.v1.keras.backend.set_session(sess)\n\nIM_WIDTH = 128\nIM_HEIGHT = 128\nBATCH_SIZE = 32\n\ndef preprocess_grayscale(image, reshape=True):\n    # copy for later usage?\n    img = image.astype(np.uint8)\n    # split\n    # h,s,v = cv2.split(img)\n    # histogram equalization on value \n    # v_ = cv2.equalizeHist(v)\n    # merge back\n    # img = cv2.merge([h,s,v_])\n    # to rgb\n    # img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n    # histogram equalization on value \n    # img = cv2.equalizeHist(img)\n    # apply gaussian blur\n    img = cv2.GaussianBlur(img, (5, 5), 1)\n    if reshape:\n        img = img.reshape(IM_WIDTH, IM_HEIGHT, 3)\n    #rescale\n    img = img\/255.\n    # done\n    return img \n\n# generator for training\n_generator = ImageDataGenerator(\n    preprocessing_function=preprocess_grayscale,\n    validation_split=0.2\n)\n\n_train = _generator.flow_from_directory(\n    directory=\"..\/input\/intel-image-classification\/seg_train\/seg_train\/\",\n    target_size=(IM_WIDTH, IM_HEIGHT),\n    shuffle=True,\n    seed=SEED,\n    color_mode=\"rgb\",\n    class_mode=\"categorical\",\n    batch_size=BATCH_SIZE,\n    subset=\"training\"\n    \n)\n\n_valid = _generator.flow_from_directory(\n    directory=\"..\/input\/intel-image-classification\/seg_train\/seg_train\/\",\n    target_size=(IM_WIDTH, IM_HEIGHT),\n    shuffle=True,\n    seed=SEED,\n    color_mode=\"rgb\",\n    class_mode=\"categorical\",\n    batch_size=BATCH_SIZE,\n    subset=\"validation\"\n)\n\n# generator for validation and test sets\n_test_generator = ImageDataGenerator(\n    preprocessing_function=preprocess_grayscale\n)\n\n# test set\n_test = _generator.flow_from_directory(\n    directory=\"..\/input\/intel-image-classification\/seg_test\/seg_test\/\",\n    target_size=(IM_WIDTH, IM_HEIGHT),\n    shuffle=False,\n    seed=SEED,\n    color_mode=\"rgb\",\n    class_mode=\"categorical\",\n    batch_size=1\n)\n\n_train.class_indices","55e570f2":"from keras import callbacks\nfrom keras.regularizers import l1, l2\nfrom keras.layers import LeakyReLU, ReLU\nfrom keras.models import Sequential, Model\nfrom keras.metrics import categorical_accuracy, top_k_categorical_accuracy\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.layers.core import Activation, Flatten, Dropout, Dense\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\nfrom keras.applications import ResNet50 as resnet\n\n# from tensorflow.python.client import device_lib\n# print(device_lib.list_local_devices())","3cd4fa6a":"class Network(object):\n\n    def __init__(self, height, width, channels, classes, parameter_scaling):\n        self.height = height\n        self.width = width\n        self.channels = channels\n        self.output_classes = classes\n        self.scale = parameter_scaling\n\n    def model(self, weights=\"imagenet\"):\n        # initiate model\n        _model = Sequential()\n        input_shape = (self.height, self.width, self.channels)\n        ### --\n        axis = -1\n        alpha = 0.1\n        # reg = l2(1e-4)\n        reg = None\n        # convinience for stacking conv blocks\n        def conv_block(model, scale, axis, input_shape=None):\n            if input_shape:\n                model.add(\n                    Conv2D(scale, (3,3), padding=\"same\", input_shape=input_shape, kernel_regularizer=reg)\n                )\n            else:\n                model.add(Conv2D(scale, (3,3), padding=\"same\", kernel_regularizer=reg))\n            model.add(Activation(\"relu\"))\n            model.add(BatchNormalization(axis=axis))\n            # conv_2\n            model.add(Conv2D(scale, (3, 3), padding=\"same\", kernel_regularizer=reg))\n            model.add(Activation(\"relu\"))\n            model.add(BatchNormalization(axis=axis))\n            # pool_1\n            model.add(MaxPooling2D(pool_size=(2, 2)))\n            model.add(Dropout(0.1))\n            return model\n        # 128x128\n        _model = conv_block(_model, self.scale, axis, input_shape=input_shape)\n        # ~64x64\n        _model = conv_block(_model, self.scale*(2**1), axis)\n        # ~32x32\n        _model = conv_block(_model, self.scale*(2**2), axis)\n        # ~16x16\n        _model = conv_block(_model, self.scale*(2**3), axis)\n        # ~8x8\n        # _model = conv_block(_model, self.scale*16, axis)\n        # ~4x4\n        # _model = conv_block(_model, self.scale*32, axis)\n        # ~2x2\n        # _model = conv_block(_model, self.scale*64, axis)\n        \n        # Fully connected layers\n        _model.add(Flatten())\n        \n        # FC1\n        _model.add(Dense(256, kernel_regularizer=reg))\n        _model.add(Activation(\"relu\"))\n        _model.add(Dropout(0.25))\n        # FC2\n        _model.add(Dense(256, kernel_regularizer=reg))\n        _model.add(Activation(\"relu\"))\n        _model.add(Dropout(0.25))\n        \n        # classifier\n        _model.add(Dense(self.output_classes))\n        _model.add(Activation(\"softmax\"))\n\n        # return model\n        return _model","ff91661e":"%%time\nLR = 1e-3\nSCALE = 16\nBATCH_SIZE = 64\nEPOCHS = 35\n\ndef lr_scheduler(epoch, lr):\n    if epoch > 15:\n        return (1e-4)\n    elif epoch > 25:\n        return (1e-5)\n    else:\n        return lr\n    \n# define callbacks\n_callbacks = [LearningRateScheduler(lr_scheduler)]\n\n# initiate model\nmodel = Network(\n    height=IM_HEIGHT, width=IM_WIDTH, \n    channels=3, classes=6, \n    parameter_scaling=SCALE\n).model()\n\n# compile\nmodel.compile(\n    loss=\"categorical_crossentropy\",optimizer=Adam(lr=LR), metrics=[categorical_accuracy]\n)\n# print(_train.class_indices)\n\n# start training\nwith tf.device(\"\/device:GPU:0\"):\n    history = model.fit_generator(\n        generator=_train,\n        steps_per_epoch=_train.samples\/\/BATCH_SIZE,\n        validation_data=_valid,\n        validation_steps=_valid.samples\/\/64,\n        callbacks=_callbacks,\n        epochs=EPOCHS,\n        verbose=1\n    )","9a60fe90":"# Let us plot the loss and accuracy functions for both training and validation sets to see how our model behaved over training epochs\n# PLOTTING\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# plot the loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()\n\n# plot the accuracy\nplt.plot(history.history['categorical_accuracy'])\nplt.plot(history.history['val_categorical_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='lower right')\nplt.show()","6db7e69a":"test_loss, test_acc = model.evaluate_generator(_test, steps=_test.samples, verbose=1)\n\nprint('val_loss:', test_loss)\nprint('val_cat_acc:', test_acc)","08e11773":"# PREDICT\npredictions = model.predict_generator(_test, steps=_test.samples, verbose=1)\n# print(predictions.shape)\ny_pred = np.argmax(predictions, axis=1)\ny_test = _test.classes","393aa6cc":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\nreport = classification_report(y_test, y_pred, target_names=['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street'])\nprint(report)","0d969331":"# heatmap\nsns.heatmap(\n    confusion_matrix(y_test, y_pred), \n    annot=True, \n    fmt=\"d\", \n    cbar = False, \n    cmap = plt.cm.Blues\n)","d326fc73":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\n# accuracy score\nprint(\"Accuracy score of the model: {:.2f}\".format(accuracy_score(y_test, y_pred)))\n\n# f1 score\nprint(\"F1 score of the model: {:.2f}\".format(f1_score(y_test, y_pred, average=\"weighted\")))\n\n# precision\nprint(\"Precision score of the model: {:.2f}\".format(precision_score(y_test, y_pred, average=\"weighted\")))\n\n# recall\nprint(\"Recall score of the model: {:.2f}\".format(recall_score(y_test, y_pred, average=\"weighted\")))\n","b7996ad2":"### Model Construction","d377c5d9":"So the training set looks failry balanced with approximately similar number of images in each class. Let's pick some random images from each class and try to visualize them","c9b2ed4d":"#### Let's start preparing for the training \n\nIn this section let us create our generators an image augumentors required for our training"}}