{"cell_type":{"50ba5b88":"code","f289cb1b":"code","3637b464":"code","d640987e":"code","d7ab68f3":"code","9a0e9d1c":"code","95e07425":"code","1ce2d23a":"code","35f4dc19":"code","8e7d1eb8":"code","ba647990":"code","bede0a77":"code","b0bde926":"code","5839c810":"code","c87b1f38":"code","d662868e":"code","096629aa":"code","93d265cb":"code","4b3e3118":"code","7496e519":"code","99ac4673":"code","0381d18b":"code","892f0dfa":"code","97cf472d":"code","3ae9328a":"code","0be0fa1b":"code","34bfb043":"code","3c9a125f":"code","12f86c84":"code","4db9b2d1":"markdown","9a932c0f":"markdown","33029d8f":"markdown","1576172e":"markdown","16585176":"markdown","590043c1":"markdown","829eb799":"markdown","ae5ec20d":"markdown"},"source":{"50ba5b88":"%matplotlib inline\nimport xgboost as xgb\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport gc\nimport matplotlib.pyplot as plt\nimport shap\n\n# load JS visualization code to notebook\nshap.initjs()\nxgb.__version__","f289cb1b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3637b464":"train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')","d640987e":"train.head()","d7ab68f3":"train.shape","9a0e9d1c":"columns = test.columns[1:]\ncolumns\n","95e07425":"target = train['target'].values\n","1ce2d23a":"cat_features = columns[:19]\ncat_features","35f4dc19":"def label_encode(train_df, test_df, column):\n    le = LabelEncoder()\n    new_feature = \"{}_le\".format(column)\n    le.fit(train_df[column].unique().tolist() + test_df[column].unique().tolist())\n    train_df[new_feature] = le.transform(train_df[column])\n    test_df[new_feature] = le.transform(test_df[column])\n    return new_feature","8e7d1eb8":"cat_cols = [col for col in columns if 'cat' in col]\ncont_cols = [col for col in columns if 'cont' in col]","ba647990":"le_cols = []\nfor feature in cat_cols:\n    le_cols.append(label_encode(train, test, feature))","bede0a77":"columns = le_cols + cont_cols","b0bde926":"xgb_params=  {'learning_rate': 0.005,\n              'objective': 'binary:logistic',\n              'eval_metric': 'auc',\n                'tree_method': 'gpu_hist',\n                'predictor': 'gpu_predictor',\n                'gpu_id': 0,\n                'max_bin': 623,\n                'max_depth': 10,\n                'alpha': 0.5108154566815425,\n                'gamma': 1.9276236172849432,\n                'reg_lambda': 11.40999855634382,\n                'colsample_bytree': 0.705851334291963,\n                'subsample': 0.8386116751473301,\n                'min_child_weight': 2.5517043283716605}","5839c810":"test = xgb.DMatrix(test[columns])","c87b1f38":"%%time\ntrain_oof = np.zeros((300000,))\ntest_preds = 0\ntrain_oof.shape\nNUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=137)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n        #print(f'Fold {f}')\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        train_target, val_target = target[train_ind], target[val_ind]\n        \n        train_df = xgb.DMatrix(train_df, label=train_target)\n        val_df = xgb.DMatrix(val_df, label=val_target)\n        \n        model =  xgb.train(xgb_params, train_df, 7000)\n        temp_oof = model.predict(val_df)\n        temp_test = model.predict(test)\n\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test\/NUM_FOLDS\n        \n        print(roc_auc_score(val_target, temp_oof))\n        \nprint(roc_auc_score(target, train_oof))","d662868e":"0.895597940122485","096629aa":"0.8945190953989157","93d265cb":"np.save('xgb_train_oof_0', train_oof)\nnp.save('xgb_test_preds_0', test_preds)","4b3e3118":"%%time\nshap_preds = model.predict(test, pred_contribs=True)","7496e519":"train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')\nle_cols = []\nfor feature in cat_cols:\n    le_cols.append(label_encode(train, test, feature))\ncolumns = le_cols + cont_cols","99ac4673":"# summarize the effects of all the features\nshap.summary_plot(shap_preds[:,:-1], test[columns])","0381d18b":"shap.summary_plot(shap_preds[:,:-1], test[columns], plot_type=\"bar\")","892f0dfa":"#%%time\nshap_interactions = model.predict(xgb.DMatrix(test[columns]), pred_interactions=True)","97cf472d":"def plot_top_k_interactions(feature_names, shap_interactions, k):\n    # Get the mean absolute contribution for each feature interaction\n    aggregate_interactions = np.mean(np.abs(shap_interactions[:, :-1, :-1]), axis=0)\n    interactions = []\n    for i in range(aggregate_interactions.shape[0]):\n        for j in range(aggregate_interactions.shape[1]):\n            if j < i:\n                interactions.append(\n                    (feature_names[i] + \"-\" + feature_names[j], aggregate_interactions[i][j] * 2))\n    # sort by magnitude\n    interactions.sort(key=lambda x: x[1], reverse=True)\n    interaction_features, interaction_values = map(tuple, zip(*interactions))\n    plt.bar(interaction_features[:k], interaction_values[:k])\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n    \nplot_top_k_interactions(columns, shap_interactions, 10)","3ae9328a":"%%time\nxgb_params=  {\n    'objective': 'binary:logistic',\n    'booster': 'gbtree',\n    'eval_metric': 'auc',\n    'random_state': 199,\n    'tree_method': 'gpu_hist',\n    'max_depth': 8,\n    'learning_rate': 0.01,\n    'min_child_weight': 20,\n    'gamma': 0.1,\n    'alpha': 0.2,\n    'lambda': 9,\n    'colsample_bytree': 0.2,\n    'subsample': 0.8\n}\n\ntest = xgb.DMatrix(test[columns])\n\ntrain_oof_1 = np.zeros((300000,))\ntest_preds_1 = 0\nprint(train_oof_1.shape)\nNUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=137)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n        #print(f'Fold {f}')\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        train_target, val_target = target[train_ind], target[val_ind]\n        \n        train_df = xgb.DMatrix(train_df, label=train_target)\n        val_df = xgb.DMatrix(val_df, label=val_target)\n        \n        model =  xgb.train(xgb_params, train_df, 4200)\n        temp_oof = model.predict(val_df)\n        temp_test = model.predict(test)\n\n        train_oof_1[val_ind] = temp_oof\n        test_preds_1 += temp_test\/NUM_FOLDS\n        \n        print(roc_auc_score(val_target, temp_oof))\n        \nprint(roc_auc_score(target, train_oof_1))\n\nnp.save('xgb_train_oof_1', train_oof_1)\nnp.save('xgb_test_preds_1', test_preds_1)","0be0fa1b":"sub['target'] = test_preds\nsub.to_csv('submission_0.csv', index=False)","34bfb043":"sub['target'] = test_preds_1\nsub.to_csv('submission_1.csv', index=False)","3c9a125f":"sub['target'] = 0.9*test_preds_1+0.1*test_preds\nsub.to_csv('blend_0.csv', index=False)","12f86c84":"sub.head()","4db9b2d1":"Next, we calculate the SHAP values for the test set.","9a932c0f":"Now let's do some plots of these values.\n\n","33029d8f":"It took 45 minutes to calculate these values. On CPU this would take up to a day to compute.\n\nNow let's take a look at what are the top interactions in this dataset.","1576172e":"Finally we make the submission.","16585176":"In this notebook we'll explore feature importance using SHAP values. SHAP values are the most mathematically consistent way for getting feature importances, and they work particulalry nicely with the tree-based models. Unfortunately, calculating SHAP values is an extremely resource intensive process. However, starting with XGBoost 1.3 it is possible to calcualte these values on GPUs, which speeds up the process by a factor of 20X - 50X compared to calculating the same on a CPU. Furthermore, it is also possible to calculate SHAP values for feature interactions. The GPU speedup for those is even more dramatic - it takes a few minutes, as opposed to days or even longer on a CPU.","590043c1":"Let's applay label encoder to the categorical features.\n","829eb799":"We'll now try a different set of XGBoost Hyperparameters","ae5ec20d":"Next, we'll calculate SHAP values for featue interactions. There will be 30x30x200,000 + 200,000 numbers that need to be computed."}}