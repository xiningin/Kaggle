{"cell_type":{"f7d1b800":"code","fbf88de9":"code","4d929e65":"code","32cd8a26":"code","aca8118c":"code","bd451452":"code","034538e3":"code","8e522aa0":"code","6686d808":"code","c4525b86":"code","0e99e2a7":"code","d5ca4402":"code","2571593f":"code","2ded3ae3":"code","77872f5f":"code","57f77c95":"code","ee4e49eb":"code","de0dcfaa":"code","6fdcd7fd":"code","a106080f":"code","4997c876":"code","b6866beb":"code","bde2b1ed":"code","435e55a5":"code","76e29993":"code","77f06373":"code","15f14b62":"code","abc4ddb2":"code","7f5adfad":"code","beb0c0e3":"code","7f1751d8":"code","492e2eb3":"code","a6e15acc":"code","c38c8620":"code","aea91458":"code","c2abe36c":"code","dc853780":"code","22dc5363":"code","c56b3e9f":"code","94fd44ef":"code","e2f70dc4":"code","d92bd681":"code","23e2eb1c":"code","66bd3652":"code","f9fce8b6":"code","8a7c13c0":"code","b764bac8":"code","186b7736":"code","b605875c":"code","c8f7c67b":"code","1d37e938":"code","3584c0dc":"code","b3fa703e":"code","198cdfde":"code","b8feb75c":"code","af47e49f":"code","63761dd7":"code","7401c712":"code","752a9cf6":"code","79f7212e":"code","3fad69e9":"code","824649f1":"code","1fe4722b":"markdown","f34e1fb6":"markdown","c4b1c77f":"markdown","178ae97f":"markdown","478cac64":"markdown","5067c595":"markdown","e4bf2e56":"markdown","041a72e8":"markdown","b214dfb5":"markdown","8c7a118c":"markdown","2cbfb34f":"markdown","6bdac824":"markdown","ec857485":"markdown","0c4e7a2f":"markdown","9ccd5dda":"markdown","b87998ea":"markdown","37a71a5c":"markdown","261c3150":"markdown","6826a0cc":"markdown","83d71027":"markdown","0c7aa0b8":"markdown","eb357b2f":"markdown","f11bd47a":"markdown","235037d2":"markdown","7e92c9ee":"markdown","c6eedb33":"markdown","73631875":"markdown","07d936fd":"markdown","76684a33":"markdown","fc585ee7":"markdown","a680e946":"markdown","127cac36":"markdown","bf6e05b2":"markdown","131b362d":"markdown","b3ef7544":"markdown","b789f0f9":"markdown","2112ba3b":"markdown","0752c26a":"markdown","662b8c9c":"markdown","15e7354d":"markdown"},"source":{"f7d1b800":"#importing base libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#magic function to view plot inside jupyter notebook\n%matplotlib inline \n\n#loading datasets\ntrain=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"..\/input\/titanic\/test.csv\")","fbf88de9":"import missingno as msno #for visualizing missing data\n\n#train dataset missing data and preview\nprint(\"Train data rows x columns: \",train.shape,\"\\n\")\nprint(train.isnull().sum())\ntrain.head()","4d929e65":"#test dataset missing data and preview\nprint(\"Test data rows x columns: \",train.shape,\"\\n\")\nprint(test.isnull().sum())\ntest.head()","32cd8a26":"#visualize missing data in train as matrix\nmsno.matrix(train)","aca8118c":"#visualize missing data in train as matrix\nmsno.matrix(test)","bd451452":"#visualize missing data in train as bar chart\nmsno.bar(train)","034538e3":"#visualize missing data in test as bar chart\nmsno.bar(test)","8e522aa0":"#check if more than 40% of information is missing in columns for train data\nprint(train.isnull().sum()>int(0.40*train.shape[0]))","6686d808":"#check if more than 40% of information is missing in columns for test data\nprint(train.isnull().sum()>int(0.40*train.shape[0]))","c4525b86":"#Data is mssing in Age columns. Histogram plot for train data.\nsns.distplot(train['Age'].dropna(),hist=True, kde=True,rug=True, bins=40)","0e99e2a7":"#Histogram plot for test data\nsns.distplot(test['Age'].dropna(),hist=True, kde=True, bins=40, rug=True)","d5ca4402":"sns.countplot(x=\"Survived\",data=train,palette=\"deep\")","2571593f":"sns.countplot(x=\"Survived\",hue=\"Pclass\",data=train,palette=\"deep\")","2ded3ae3":"corr =train.corr()\nsns.heatmap(corr,annot=True)","77872f5f":"#Check the data type of each feature\ntrain.info()","57f77c95":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer","ee4e49eb":"#intantiate label encoder\nle=LabelEncoder()","de0dcfaa":"#Combining both datasets\ndataset=[train,test]","6fdcd7fd":"#Dropping the PassengerId column only in train as we equire passengerid in our test dataset for submission\ntrain.drop(\"PassengerId\",axis=1,inplace=True)","a106080f":"train[\"Name\"]","4997c876":"for data in dataset:\n    data[\"Title\"]=data[\"Name\"].str.extract('([A-Za-z]+)\\.',expand=False)","b6866beb":"train[\"Title\"].value_counts()","bde2b1ed":"test[\"Title\"].value_counts()","435e55a5":"#create mapping\ntitle_mapping={\"Mr\":0,\"Miss\":1,\"Mrs\":2,\"Master\":3,\"Col\":3,\"Rev\":3,\"Ms\":3,\"Dr\":3,\"Dona\":3,\"Major\":3,\n         \"Mlle\":3,\"Countess\":3,\"Sir\":3,\"Jonkheer\":3,\"Don\":3,\"Mme\":3,\"Capt\":3,\"Lady\":3}","76e29993":"for data in dataset:\n    data[\"Title\"]=data[\"Title\"].map(title_mapping)","77f06373":"#check the count of survived according ot titles\nsns.countplot(x=\"Survived\",hue=\"Title\",data=train,palette=\"deep\")","15f14b62":"#Dropping the column Names as it is no longer required\nfor data in dataset:\n    data.drop(\"Name\",axis=1,inplace=True)","abc4ddb2":"for data in dataset:\n    data[\"Sex\"]=le.fit_transform(data[\"Sex\"])","7f5adfad":"Skewness = 3*(train[\"Age\"].mean()-train[\"Age\"].median())\/train[\"Age\"].std()\nSkewness","beb0c0e3":"for data in dataset:\n    data[\"Age\"].fillna(data.groupby(\"Title\")[\"Age\"].transform(\"median\"),inplace=True)","7f1751d8":"for data in dataset:\n    data[\"Family Size\"]=data[\"SibSp\"]+data[\"Parch\"]+1","492e2eb3":"#dropping the Parch and SibSp columns\nfor data in dataset:\n    data.drop([\"SibSp\",\"Parch\"],axis=1,inplace=True)","a6e15acc":"#dropping Ticket column\nfor data in dataset:\n    data.drop([\"Ticket\"],axis=1,inplace=True)","c38c8620":"for data in dataset:\n    data[\"Fare\"].fillna(data[\"Fare\"].mean(),inplace=True)","aea91458":"test[\"Cabin\"].value_counts()","c2abe36c":"for data in dataset:\n    data[\"Cabin\"] = data[\"Cabin\"].str[:1]","dc853780":"cabin_mapping={\"A\":0,\"B\":0.4,\"C\":0.8,\"D\":1.2,\"E\":1.6,\"F\":2,\"G\":2.4,\"T\":2.8} #This is called feature scaling, please explore more on this advanced topic","22dc5363":"for data in dataset:\n    data[\"Cabin\"] = data[\"Cabin\"].map(cabin_mapping)","c56b3e9f":"for data in dataset:\n    data[\"Cabin\"].fillna(data.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"),inplace=True)","94fd44ef":"for data in dataset:\n    data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0],inplace=True)","e2f70dc4":"#Label encoding Embarked\nfor data in dataset:\n    data[\"Embarked\"]=le.fit_transform(data[\"Embarked\"])","d92bd681":"#for train dataset\nsns.heatmap(train.isnull(),cmap = 'magma' )","23e2eb1c":"#For test dataset\nsns.heatmap(test.isnull(),cmap = 'magma' )","66bd3652":"#separating labels\ny_train = train[\"Survived\"]\n#separating features\ntrain.drop(\"Survived\", axis=1,inplace=True)\nX_train=train","f9fce8b6":"#checking train dataset\nX_train.head()","8a7c13c0":"#checking test dataset after removing passengerid as a copy of test data, as we reuire passengerid in final submission on Kaggle\nX_test = test.drop(\"PassengerId\",axis=1).copy()\nX_test.head()","b764bac8":"#Finally check the shapes\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)","186b7736":"#import classifiers\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","b605875c":"from sklearn.model_selection import KFold, cross_val_score\n#set values for K-folds\nfolds= KFold(n_splits=10,shuffle=True,random_state=0)\nmetric=\"accuracy\"","c8f7c67b":"knn=KNeighborsClassifier(n_neighbors=10)\nscore= cross_val_score(knn,X_train,y_train,cv=folds,n_jobs=1,scoring=metric)\nprint(score)","1d37e938":"#mean score rounded to 2 decimal points\nround(np.mean(score)*100,2)","3584c0dc":"dtc=DecisionTreeClassifier()\nscore= cross_val_score(dtc,X_train,y_train,cv=folds,n_jobs=1,scoring=metric)\nprint(score)","b3fa703e":"#mean score rounded to 2 decimal points\nround(np.mean(score)*100,2)","198cdfde":"rfc=RandomForestClassifier()\nscore= cross_val_score(rfc,X_train,y_train,cv=folds,n_jobs=1,scoring=metric)\nprint(score)","b8feb75c":"#mean score rounded to 2 decimal points\nround(np.mean(score)*100,2)","af47e49f":"gnb=GaussianNB()\nscore= cross_val_score(gnb,X_train,y_train,cv=folds,n_jobs=1,scoring=metric)\nprint(score)","63761dd7":"#mean score rounded to 2 decimal points\nround(np.mean(score)*100,2)","7401c712":"svmcl=SVC()\nscore= cross_val_score(svmcl,X_train,y_train,cv=folds,n_jobs=1,scoring=metric)\nprint(score)","752a9cf6":"#mean score rounded to 2 decimal points\nround(np.mean(score)*100,2)","79f7212e":"#using the classifier that gave highest average accuracy on train dataset\nclf=RandomForestClassifier()\nclf.fit(X_train,y_train)\ny_test = clf.predict(X_test)","3fad69e9":"#wrapping up into a submission dataframe\nsubmission=pd.DataFrame({\"PassengerId\": test[\"PassengerId\"],\"Survived\":y_test})\n\n#converting to submission csv\nsubmission.to_csv(\"submission.csv\",index=False)","824649f1":"submission=pd.read_csv(\"submission.csv\")\nsubmission.head()","1fe4722b":"### c. Random Forest Classifier","f34e1fb6":"### I. Try Different Models","c4b1c77f":"Import libraries and functions for cross validation and metrics for accuracy","178ae97f":"## 1. Project Skeleton\n\nBefore starting out any project, we must first plan our steps and have clarity on what type of problem we are tackling and what tools can be used and what cannot be used and why not?. This \"why not\" question will help you gain more insights on your ML journey. The following are key points I took into consideration.","478cac64":"### e. Support Vector Machine Classifier","5067c595":"#### Staircase\n1. What kind of ML problem statement is it? Try to define it\n2. Understand the type of data?\n3. Plot the values counts\n4. Check what data is missing (and also how can it be filled?)\n5. Relationships between various features\n6. Try your intuition about the field:\n    i. Are people with family more likely to survive?\n    ii. Are richer people more likely to survive\n    iii. Location of cabin and how the ship actually sank, did the front part sink first?\n7. Feature Engineering\n8. Pre-processing and scaling features\n9. Apply some base models and move to more advanced ones\n10. Cross validate on different subsets of data\n11. Chose the best one and predict on Test data\n","e4bf2e56":"### II. Survival Prediction on Test Data","041a72e8":"### III. Count Plot for Features","b214dfb5":"Check the Titles that were extracted","8c7a118c":"### j. Embarked\nSince Embarked has we missing values and is Categorical, we will just simply fill it by its mode from values S,C,Q.","2cbfb34f":"### g. Ticket\nThis feature does not contain much data so we can drop it directly","6bdac824":"Fill the missing value by grouping by Pclass, since cabins are related to class of booking","ec857485":"Checking for null values if any:","0c4e7a2f":"## 3. Exploratory Data Analysis\nMajor portion of time while solving a ML problem will go into EDA and feature engineering. Applying a model is really 2-4 lines of code and easy part. So be patient here and use your own intuition. if you have strong foundation in stats, this can be really easy for you. Here we will first explore misisng data, then visualize the features and generate correlation to have an idea about how features are inter-related to each other.","9ccd5dda":"## 4. Data Pre-processing and Feature Engineering","b87998ea":"# Titanic: Machine Learning from Disaster\n\nThis notebook is detailed walkthrough of all the steps I took to make my first Kaggle submission. The Challenges I faced in the process are not unique to you or someone who is just starting with Kaggle. Keeping in mind I have provided a detailed description of steps along with references and links. Taking reference from various internet blogs and videos, I finally put my code to work, I hope you find it useful and a kickstart for your ML journey. As you explore through more and more projects and get your hands dirty, you will gain more knowlege. The tough part is to get started, and believe me to just take a leap of faith. and its okay if you dont understand the code completely, with repetions the things form a clear picture of concepts.So lets dive in!","37a71a5c":"### II. Data Interpretation and Visualization\nReference for seaborn: https:\/\/towardsdatascience.com\/data-visualization-using-seaborn-fc24db95a850","261c3150":"### b. Name\nName looks like it does not contain much information but, we can still create new feature \"Title\" and extract titles from each name in test and train data set which can be informative going further. To create this new feature we can use regular expressions","6826a0cc":"### a. K-nearest neighbor Classifier","83d71027":"### d. Age\nSince has lots of missing values lets check how we can fill it. Lets first check the skewness","0c7aa0b8":"## 2. Loading Datasets\nHere I have basically downloaded data into my repository and loaded them into pandas dataframe. Nothing too fancy","eb357b2f":"Skewness is positive so we can use medium to fill the missing values. But in our dataset we can do something better. We have feature \"Names\" from which we have extract titles like Mr,Mrs, master, miss, and now we can use the median of each group to fill the missing values for better accuracy of our model. ","f11bd47a":"### e. SibSp\n\nit contains information about the family, So we will combine this feature along with Parch to have a new feature called \"Family Size\"","235037d2":"### III. Re-Check Datasets \nit is good practice to re-check datasets before actually applying models. Check if both train and test datasets have no null values. Here we must also separate labels and features from train dataset","7e92c9ee":"### d. Gaussian NB Classifier","c6eedb33":"### i. Cabin\nSince more than 40% of value in cabin data is missing, we can drop it directly if we want. But here I will extract the first letter of each cabin name and apply a numerical mapping on it and hence fill the missing values later. ","73631875":"### I. Exploring Missing Values\n\nReference for msno: https:\/\/towardsdatascience.com\/visualize-missing-values-with-missingno-ad4d938b00a1","07d936fd":"### b. Decision Tree Classifier","76684a33":"## 6. Conclusion\n\nThe output csv is generated in the right pane in the kernel under folder \"output\" You can submit it on Kaggle using the following link:\n\nhttps:\/\/www.kaggle.com\/c\/titanic\n\nI am new to ML path, so this may not be the finest implementation but I tried to get my hands on with the knowledge I had. Feel free to comment on techniques I can use to improve the score of the models and dont forget to Upvote!\n\n\n\n","fc585ee7":"### f. Parch\nThis also contains information about no of parents and children so let us combine it with SibSip to have new feature as \"Family Size\"","a680e946":"### I. Check Feature Data Types","127cac36":"## 5. Modelling\nWe will start with some basic models and then go through more advanced ensemble models. Lets drive straight into it!","bf6e05b2":"### c. Sex \nSince sex is a categorical variable we need to encode it using some sort of encoding, maybe manually using a dictionary or by the use of label encoder. Here I have used Label Encoder.","131b362d":"### IV. Feature Relationships\nTo get an idea about how features are related to each other, we can generate a correlation matrix and check the pearson correlation coefficient\n\nReference: https:\/\/www.statisticshowto.com\/probability-and-statistics\/correlation-coefficient-formula\/","b3ef7544":"### h. Fare\nSince Fare has very few missing values, we can replace fare with its mean value","b789f0f9":"Since most of the values are in Mr, Miss, Mrs, we can include the others in separate category and hence have four categories. We could have used label encoder but lot of small categories can be clubbed together into single category","2112ba3b":"## Contents:\n\n1. Project Skeleton\n2. Loading dataset (Downloaded from Kaggle)\n3. Exploratory Data Analysis\n    -   Exploring Missing Values\n    -  Data Interpretation and Visualization\n    - Count Plot for Features\n    -  Feature Relationships\n4. Data Pre-processing and Feature Engineering\n    -   Check Feature Data Types\n    -  Walk Through Each Feature One by One\n    - Re-Check Datasets\n5. Modelling\n    -  Try Different Models\n    - Survival Prediction on Test Data\n6. Conclusion","0752c26a":"#### Best Model: Random Forest Classifier\n#### Best Score on train data: 80.92\n#### Score on submission: 74.16","662b8c9c":"### II. Walk Through Each Feature One by One\nHere we explore each feature one by one and check if we can directly drop it, use it or extract some information from it.","15e7354d":"### a. PassengerId\nThis is just the serial number identification for passengers and doe not contain much infromation. we can drop it directly"}}