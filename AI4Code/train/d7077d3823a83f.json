{"cell_type":{"310571cd":"code","6bf7e184":"code","3ebef46c":"code","aa2ab213":"code","09840d0f":"code","06a1ce2c":"code","62aba3ea":"code","2303590f":"code","643bebb4":"code","f5784761":"code","70d3163e":"code","2e7e0b9b":"markdown","359ec7ee":"markdown","fdb77911":"markdown","dae87d07":"markdown"},"source":{"310571cd":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision.transforms import transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom IPython.display import HTML\nimport pandas as pd","6bf7e184":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")","3ebef46c":"transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,))\n    ])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef get_digits(df):\n    labels = []\n    start_inx = 0\n    if 'label' in df.columns:\n        labels = [l for l in df.label.values]\n        start_inx = 1\n        \n    \n    digits = []\n    for i in range(df.pixel0.size):\n        digit = df.iloc[i].astype(float).values[start_inx:]\n        digit = np.reshape(digit, (28,28))\n        digit = transform(digit).type('torch.FloatTensor')\n        if len(labels) > 0:\n            digits.append([digit, labels[i]])\n        else:\n            digits.append(digit)\n\n    return digits\n\ntrainX = get_digits(train)\n\n  \nbatchSize  = 250 \nvalidSize  = 0.2  \n\n\nnumTrain = len(trainX)\nindices   = list(range(numTrain))\nnp.random.shuffle(indices)\nsplit     = int(np.floor(validSize * numTrain))\ntrainIndex, validIndex = indices[split:], indices[:split]\n\n\nfrom torch.utils.data.sampler import SubsetRandomSampler\ntrainSampler = SubsetRandomSampler(trainIndex)\nvalidSampler = SubsetRandomSampler(validIndex)\n\ntrainLoad = torch.utils.data.DataLoader(trainX, batch_size=batchSize,\n                    sampler=trainSampler)\nvalidLoad = torch.utils.data.DataLoader(trainX, batch_size=batchSize, \n                    sampler=validSampler)\n\ndataiter = iter(trainLoad)\nimages, labels = dataiter.next()\nprint(type(images))\nprint(images.shape)\nprint(labels.shape)","aa2ab213":"class myModel(nn.Module):\n    \n    def __init__(self):\n        super(myModel, self).__init__()\n        \n        # The two Convolutional-Pooling Sets of Layers, \n        # connected through RELU functions.\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 30, kernel_size = 5, \n                      padding = 3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 3, \n                         stride = 3)\n            )\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(30, 60, kernel_size = 5, \n                      padding = 3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 3, \n                         stride = 3)\n            )\n        \n        # Dropout \n        self.dropoutLayer = nn.Dropout()\n        \n        # Linear Layers\n        self.linear1 = nn.Linear(4 * 4 * 60, 1100)\n        self.linear2 = nn.Linear(1100, 500)\n        self.linear3 = nn.Linear(500, 10)\n    \n    def forward(self, x):\n        \n        # Feed-forward network function\n        net = self.layer1(x)\n        net = self.layer2(net)\n        net = net.reshape(net.size(0), -1) \n        net = self.dropoutLayer(net)\n        net = self.linear1(net)\n        net = self.linear2(net)\n        \n        return net","09840d0f":"CNN = myModel()\nlossFunc = nn.CrossEntropyLoss()\noptimizer = optim.Adam(CNN.parameters(), lr = 0.0001)\ntotalLosses = []\n\nfor epoch in range(1, 21):\n    # Training for twenty epochs\n    \n    lossAtEpoch = 0.0\n    \n    for index, data in enumerate(trainLoad, start = 0):\n        inputs, names = data\n        optimizer.zero_grad() # Zero the parameter gradients\n        \n        outputs = CNN(inputs)\n        loss = lossFunc(outputs, names)\n        \n        loss.backward() # Make loss into Tensor\n        optimizer.step()\n        \n        lossAtEpoch += loss.item()\n        \n        batchSize2 = 134 # Miniature Batch Size\n        \n        if index % batchSize2 == (batchSize2 - 1):\n            print(\"Loss (Epoch \" + str(epoch) + \") : \" + str(lossAtEpoch \/ batchSize2))\n            totalLosses.append(lossAtEpoch \/ batchSize2)\n            \n            lossAtEpoch = 0.0","06a1ce2c":"arrayOfLosses = np.array(totalLosses)\narrayOfLosses.shape # =(38,0)\narrayOfLosses = arrayOfLosses.reshape(2,10)","62aba3ea":"plt.plot(totalLosses)\nplt.title(\"Loss Function of CNN with MNIST Dataset\")\nplt.ylabel(\"Loss Function (CrossEntropyLoss)\")\nplt.xlabel(\"x-th Mini Batch (N = 250)\")\nplt.xticks(np.arange(len(totalLosses)), np.arange(1, len(totalLosses)+1))\nplt.show()","2303590f":"df = pd.DataFrame({\"CNN Loss (Optim = Adam)\": totalLosses})\nHTML(df.to_html(index = False, classes = \"dataframe\"))","643bebb4":"numCorrect = [0. for i in range(10)]\nnumTotal = [0. for i in range(10)]\n\nwith torch.no_grad():\n    for data in trainLoad:\n        \n        images, labels = data\n        outputs = CNN(images)\n        \n        _, predicted = torch.max(outputs, 1)\n        \n        c = (predicted == labels).squeeze()\n        \n        for i in range(4):\n            label = labels[i]\n            numCorrect[label] += c[i].item()\n            numTotal[label] += 1\n\n# Average accuracy\navg = 0\n\nfor i in range(10):\n    avg += 100 * numCorrect[i] \/ numTotal[i]\n    print('Accuracy of %5s : %2d %%' % (\n        i + 1, 100 * numCorrect[i] \/ numTotal[i]))\n\nprint(\"Average Accuracy: \" + str(avg \/ 10))","f5784761":"# Define the test data loader\ntest        = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntestX      = get_digits(test)\ntestLoad = torch.utils.data.DataLoader(testX, batch_size=batchSize)","70d3163e":"ImageId = []\nLabel = []\n\n# Loop through the data and get the predictions\nfor data in testLoad:\n    # Move tensors to GPU if CUDA is available\n    data = data.to(device)\n    # Make the predictions\n    output = CNN(data)\n    # Get the most likely predicted digit\n    _, pred = torch.max(output, 1)\n    \n    for i in range(len(pred)):        \n        ImageId.append(len(ImageId)+1)\n        Label.append(pred[i].cpu().numpy())\n\nsub = pd.DataFrame(data={'ImageId':ImageId, 'Label':Label})\nsub.describe\nsub.to_csv(\"submission.csv\", index = False)","2e7e0b9b":"The below cell provides a rough accuracy of the model, though the true accuracy can be evaluated from the actual Kaggle score.","359ec7ee":"The following cell contains the Convolutional Neural Network that I have constructed for the purposes of analyzing the MNIST Dataset.  I have done a lot of researching and (hopefully) learned the basics of a neural network and\/or CNN. ","fdb77911":"I would like to **thank** the following submission, https:\/\/www.kaggle.com\/jcardenzana\/mnist-pytorch-convolutional-neural-nets, for helping me transition the method through which I loaded the data to the Kaggle Digit Recognizer Dataset instead of PyTorch's respective dataset. I would also like to credit this notebook with helping me with the last cell in my own notebook (the construction of a pandas-made csv for submission purposes). ","dae87d07":"This notebook is my baseline attempt for constructing a working CNN using the MNIST Dataset through the PyTorch framework. After research and consultation with more proficient Kaggle users' notebooks (principally regarding the conversion of the foundational data medium from torchvision.datasets to pandas, and the subsequent submission to Kaggle that is common with all projects), I have opted to construct this using a basic CNN with Pooling, Dropout, etc. and the Adam optimizer."}}