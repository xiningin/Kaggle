{"cell_type":{"db61d7de":"code","9bd06101":"code","e7dbc06d":"code","4ad5a54b":"code","c964f277":"code","f7863a6c":"code","bc9b5179":"code","8bca58f2":"code","e9ac6e24":"code","f9a150f2":"code","1600f0ca":"code","658f044a":"code","4fa05dd1":"code","7bb2e25f":"code","bebbbacc":"code","17929c41":"code","e0abe3e3":"code","fed4607d":"code","e25c397e":"code","cc87dbd1":"code","d6f5ab65":"code","cb387b9e":"code","86b36c51":"code","be38b9cd":"code","92b14958":"code","b763ef43":"markdown","1302ae71":"markdown","a9931571":"markdown","269ff00a":"markdown","d98faaa3":"markdown","e781527a":"markdown","6860db95":"markdown","fa1c4092":"markdown","dddb379d":"markdown","1e4a6be7":"markdown","9331127a":"markdown","2c5d12e0":"markdown","d9b9f11e":"markdown","e875cd7f":"markdown"},"source":{"db61d7de":"## importing modules\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib.image as mpimg\nimport PIL.Image\nfrom tqdm import tqdm\nmpl.rcParams['figure.figsize'] = (12,12)\nmpl.rcParams['axes.grid'] = False","9bd06101":"def load_img(path_to_img):\n    max_dim = 512\n    # read file\n    img = tf.io.read_file(path_to_img)\n    # decode with three channels RGB\n    img = tf.image.decode_image(img, channels=3)\n    # convert the datatype of tensor to float32\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    # get the size of the image as float32\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    # get the longer dimension\n    long_dim = max(shape)\n    # define a scaling factor\n    scale = max_dim \/ long_dim\n    # scale the image by that factor and save the new shape as int32\n    # this keeps the aspect ratio\n    new_shape = tf.cast(shape * scale, tf.int32)\n    # resize the image to the new shape\n    img = tf.image.resize(img, new_shape)\n    # make it a batch\n    img = img[tf.newaxis, :]\n    return img","e7dbc06d":"# let's load the images\ncontent_image = load_img(\"..\/input\/photos\/photo_2020-07-29_21-52-26.jpg\")\nstyle_image = load_img(\"..\/input\/dataset4\/style.png\")\n\nplt.subplot(1, 2, 1)\nplt.imshow(content_image[0])\n\nplt.subplot(1, 2, 2)\nplt.imshow(style_image[0])","4ad5a54b":"vgg = tf.keras.applications.VGG19(include_top=False, weights=\"imagenet\")\n# The top is the last layer in the network, this is used for classification so we don't need it","c964f277":"# the feature maps are extracted from the convolutional layers of the model\n# let's find out how they're named\nvgg.summary()","f7863a6c":"# you can change these if you want, playing around is never bad ;)\ncontent_layers = [\"block5_conv4\"]\nstyle_layers = [\n    \"block1_conv1\",\n    \"block2_conv2\",\n    \"block3_conv2\",\n    \"block4_conv3\",\n    \"block5_conv3\",\n]\nno_content = len(style_layers)\nno_style = len(content_layers)","bc9b5179":"outputs = [vgg.get_layer(name).output for name in style_layers + content_layers]\nextractor = tf.keras.Model(inputs=vgg.inputs, outputs=outputs)\nextractor.trainable = False","8bca58f2":"# let's take it for a spin\noutputs = extractor(content_image)\nlen(outputs)","e9ac6e24":"def gram_matrix(input_tensor):\n    # the b dimension is just the batch, don't worry about it\n    result = tf.linalg.einsum(\"bijc,bijd->bcd\", input_tensor, input_tensor)\n    input_shape = tf.shape(input_tensor)\n    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n    return result \/ num_locations\n    ","f9a150f2":"# the first layer in the outputs is block1_conv1\n# this one has output shape of (None, None, None, 64)\n# its gram matrix should have shape (None, 64, 64) with none being the batches (just 1)\ng = gram_matrix(outputs[0])\ng.shape","1600f0ca":"class StyleContentExtractor(tf.keras.models.Model):\n    def __init__(self, style_layers, content_layers):\n        super(StyleContentExtractor, self).__init__()\n        outputs = [vgg.get_layer(name).output for name in style_layers + content_layers]\n        self.model = tf.keras.Model(inputs=vgg.inputs, outputs=outputs)\n        self.model.trainable = False\n        \n        self.style_layers = style_layers\n        self.content_layers = content_layers\n        self.no_style_layers = len(style_layers)\n        \n    def call(self, inputs):\n        inputs = inputs*255.\n        inputs = tf.keras.applications.vgg19.preprocess_input(inputs)\n        outputs = self.model(inputs)\n        \n        content_outputs = outputs[self.no_style_layers:]\n        style_outputs = [gram_matrix(out) for out in outputs[:self.no_style_layers]]\n        \n        content_dict = {name: val for name, val in zip(self.content_layers, content_outputs)}\n        style_dict = {name: val for name, val in zip(self.style_layers, style_outputs)}\n        \n        return {\"style\": style_dict, \"content\": content_dict}\n        \n        \nextractor = StyleContentExtractor(style_layers, content_layers)","658f044a":"content_target = extractor(content_image)[\"content\"]\nstyle_target = extractor(style_image)[\"style\"]\nalpha = 1e4\nbeta = 1e-2","4fa05dd1":"def content_style_loss(outputs):\n    style_outputs = outputs['style']\n    content_outputs = outputs['content']\n    \n    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_target[name])**2) \n                           for name in style_outputs.keys()])\n    \n    style_loss *= beta \/ no_style\n    \n    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_target[name])**2) \n                           for name in content_outputs.keys()])\n    \n    content_loss *= alpha \/ no_content\n    \n    return content_loss + style_loss\n    ","7bb2e25f":"# let'd define the variable for which we want to optimize the loss ( O )\n# this is the generated image\n# initially, it has the exact contents of the content image\nimage = tf.Variable(content_image)","bebbbacc":"opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n\n# we need to make sure the tensor values remain between 0 and 1\n# let's clip its values\ndef clip_0_1(image):\n    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n\n\n@tf.function()\ndef step(img):\n    # this context manager is used to calculate the gradient\n    with tf.GradientTape() as tape:\n        outputs = extractor(img)\n        loss = content_style_loss(outputs)\n\n    grad = tape.gradient(loss, img)\n    opt.apply_gradients([(grad, img)])\n    img.assign(clip_0_1(img))","17929c41":"# What we'll get is a tensor having our information.\n# let's write a function that turns it into an image\ndef tensor_to_image(tensor):\n    # tensor images are normalized so denormalize them\n    tensor = tensor * 255\n    tensor = np.array(tensor, dtype=np.uint8)\n    \n    if np.ndim(tensor)>3:\n        assert tensor.shape[0] == 1\n        tensor = tensor[0]\n    return PIL.Image.fromarray(tensor)","e0abe3e3":"# let's run a few steps and see what we've got\nfor _ in tqdm(range(40)):\n    step(image)","fed4607d":"out = tensor_to_image(image)\nplt.figure(figsize=(6, 6))\nplt.imshow(out)","e25c397e":"diff = 1\ndef high_pass_x_y(image):\n    x_var = image[:,:,diff:,:] - image[:,:,:-diff,:] # fast variations on the x axis\n    y_var = image[:,diff:,:,:] - image[:,:-diff,:,:] # fast variations on the y axis\n\n    return x_var, y_var\n","cc87dbd1":"x_deltas, y_deltas = high_pass_x_y(content_image)\n\nplt.figure(figsize=(14,10))\nplt.subplot(2,2,1)\nplt.title(\"Horizontal Deltas: Original\")\nplt.imshow(clip_0_1(2*y_deltas[0]+0.5))\n\nplt.subplot(2,2,2)\nplt.title(\"Vertical Deltas: Original\")\nplt.imshow(clip_0_1(2*x_deltas[0]+0.5))\n\nx_deltas, y_deltas = high_pass_x_y(image)\n\nplt.subplot(2,2,3)\nplt.title(\"Horizontal Deltas: Styled\")\nplt.imshow(clip_0_1(2*y_deltas[0]+0.5))\n\nplt.subplot(2,2,4)\nplt.title(\"Vertical Deltas: Styled\")\nplt.imshow(clip_0_1(2*x_deltas[0]+0.5))","d6f5ab65":"noise_weight = 1e3\ndef content_style_noise_loss(outputs):\n    style_outputs = outputs['style']\n    content_outputs = outputs['content']\n    \n    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_target[name])**2) \n                           for name in style_outputs.keys()])\n    \n    style_loss *= beta \/ no_style\n    \n    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_target[name])**2) \n                           for name in content_outputs.keys()])\n    \n    content_loss *= alpha \/ no_content\n    \n    x_deltas, y_deltas = high_pass_x_y(image)\n    noise_loss = tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))\n    \n    noise_loss *= noise_weight\n    \n    return content_loss + style_loss + noise_loss","cb387b9e":"# let's do it again\n@tf.function()\ndef step(img):\n    # this context manager is used to calculate the gradient\n    with tf.GradientTape() as tape:\n        outputs = extractor(img)\n        loss = content_style_noise_loss(outputs) # added noise\n\n    grad = tape.gradient(loss, img)\n    opt.apply_gradients([(grad, img)])\n    img.assign(clip_0_1(img))","86b36c51":"image = tf.Variable(content_image)\nfor _ in tqdm(range(40)):\n    step(image)","be38b9cd":"# ready? even if not, here we go\nout_filtered = tensor_to_image(image)\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"With noise\")\nplt.imshow(out)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Without noise\")\nplt.imshow(out_filtered)","92b14958":"plt.figure(figsize=(16, 4))\n\nplt.subplot(1, 4, 1)\nplt.imshow(content_image[0])\n\nplt.subplot(1, 4, 2)\nplt.imshow(style_image[0])\n\nplt.subplot(1, 4, 3)\nplt.imshow(out)\n\nplt.subplot(1, 4, 4)\nplt.imshow(out_filtered)\n","b763ef43":"# Neural Style Transfer\nThe idea behind this project is to make a program repaint a content image in the style of a style image.  \nlet's say I want to have this picture of this addorable gentelman to be repainted in the style of that other image.  \n![image.png](attachment:image.png)","1302ae71":"## Getting a pretrained CNN\nThe `tensorflow.keras.applications` module has a lot of powerful CNNs that we can use.  \nI'll be using the VGG19 network in this notebook but feel free to use whatever you want.","a9931571":"## defining a loss function\nNow we need something to optimize, don't we?  \nLet's define the loss functions $ J_{content} $ and $ J_{style} $\n$$ J_{content} = \\frac{(C-O)^2}{C*H*W} \\quad \\quad J_{style} = \\frac{(Gram(S)-Gram(O))^2}{C*H*W} $$","269ff00a":"\n# Plan of attack\nThis is not a very straight forward problem so let me break it down for you.  \nWe need to extract features out of these images, we don't want to have the exact pixel values in them, we just need the features.  \nTo extract these features we could ( and will ) use a pretrained CNN to encode these images into feature maps.  \nThen we need to figure out a way to mathematically model the style of the style image.  \nThen we need to define a function that models the content loss (a metric that measures how different the image in hand is from the content image)\nThen we need to define a function that models the style loss (a metric that measures how different the style of the image in hand is from the style of the style image)\n\n**To sum up**\n* Use a pretrained CNN to convert images to feature maps\n* Define a function to model the style of an image given its feature map\n* Define a function to calculate the loss $ min (J), \\quad J = \\alpha J_{content} + \\beta J_{style} $\n* **OPTIONAL** get rid of noise\n\n**NOTE**\n> The optimization variable this time is not going to be the weights of the CNN, those will remain unchanged\n> it will rather be the pixel data of the target image that you want to create.","d98faaa3":"### Nice\nWe now have six output for six layers, five for style and one for content.  \nGetting feature maps ... check.\n","e781527a":"## The Gram matrix\nWe need a way to model the style, the gram matrix does that job well.  \nFirst you need to understand how the style is defined, it is defined as the corelation between features.  \nThe gram matrix calculates just that and implementing it is very easy.  \nFor any layer $ l $ with output $ F(x) $ with dimensions $ i, j, c $ also modeled as $ i, j, d $, The gram matrix G is defined as: \n  \n  \n$$ G_{cd}^l = \\frac{\\sum_{ij} F_{ij}^l(x) * F_{ij}^l(x)}{IJ} $$  \n\nYou can calculate that in tensorflow using the beautiful [`tensorflow.linalg.einsum`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/einsum) function","6860db95":"Let's now extract the target content and target styles so we can use them in our loss function.  \nlet's also set a weight for each loss $ \\alpha $ and $ \\beta $ where $$ J = \\alpha J_{content} + \\beta J_{style} $$","fa1c4092":"Now let's take some of those names and store them in two lists\n* One will be hold content layers from which we extract feature maps to calculate the content loss\n* The other will hold the style layers form which we extract feature maps to calculate the style loss","dddb379d":"Obviously the styled image has much more high frequency changes.  \nLet's get rid of those by adding them to the loss function","1e4a6be7":"## Awesome\nThat's it for that project\nI encourage you to play around with the following and see if you get different results\n* The model\n* The content and style layers\n* The content, style and noise loss weights\n\nI would appreciate if you share interesting results with me.  \nHave an awesome day!","9331127a":"### Almost done\nNow we need to define an optimizer.  \nWe also need to define a function that we can call in the tensorflow graph to take a gradient descent step.  \nIn order to be able to call the function in the tensorflow graph you need to decorate it with the [`tf.function`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/function) decorator","2c5d12e0":"### Looking good\nNow we can get the style of any image.  \nLet's dive into the next step.  \n> **BUT** before we continue let's define a class that outputs the style and content encodings of any image just to make our lives easier when we get to the loss functions","d9b9f11e":"### hmmmmm!\nIt's picking up the style, but it's a bit noisy.  \nI wonder if we can define a loss function for noise and add that to our loss function so we can minimize it.  \nWell, my friend, you can.  \nlet's do that now.  \n  \nI'll define noise as fast changes or edges both horizontal and vertical.  \nLet's see what that looks like","e875cd7f":"Now let's define a model which has the inputs of the vgg model and outputs of those layers"}}