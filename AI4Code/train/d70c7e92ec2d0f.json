{"cell_type":{"6526721f":"code","fecfca4d":"code","85c6d2e8":"code","e1db7466":"code","1410d520":"code","611dad71":"code","e0875f3f":"code","ed42e41c":"code","6ac37088":"code","758bc7cb":"code","c10b3aa6":"code","80f62220":"code","40854ec7":"code","7cf9cfc7":"code","a2090faa":"code","4c6f9241":"code","9f344a6e":"code","a2896797":"code","ae05afc8":"code","054e914d":"code","dacbd653":"code","420f560f":"code","d60517ce":"code","3a8d9f7b":"code","c7647572":"code","6e92718b":"code","4807f77d":"code","21ca6be3":"code","dc934bbe":"code","61705cb3":"code","6c252bde":"code","a98b8cb8":"code","b64d7039":"markdown","862f7330":"markdown","bf8adae9":"markdown","6bdc5273":"markdown","22e3e635":"markdown","1733d15b":"markdown","35d521af":"markdown","18ca8bbd":"markdown","3060f0cf":"markdown","d771d82f":"markdown","b4a47249":"markdown","60cf58ad":"markdown","68f6a705":"markdown","9ab2cada":"markdown","1fea3d89":"markdown","c37fa236":"markdown","23cbc459":"markdown","59b5b1f4":"markdown","c9009f69":"markdown","62a05286":"markdown","f3391ab9":"markdown","f1c04e4d":"markdown","b15eb4de":"markdown","d6490ad7":"markdown"},"source":{"6526721f":"#Libraries for basic operation\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Libraries for feature engineering\nimport missingno as msno\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n#Import libraries for model and chart creation\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc,roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nimport math\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.sparse import hstack,vstack\nfrom sklearn.model_selection import GridSearchCV","fecfca4d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","85c6d2e8":"#Understand train data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nprint(train_data.shape)\ntrain_data.head()\n","e1db7466":"train_data.describe()","1410d520":"#Finding unique values for each column\nprint(\"*********Available columns in train dataset********\\n\",train_data.columns,\"\\n\")\nprint(\"*********Unique count of each column********\\n\",train_data.nunique(),\"\\n\")\nprint(\"*********Unique value of Pclass column********\\n\",train_data[\"Pclass\"].value_counts(),\"\\n\")\nprint(\"*********Unique value of Sex column********\\n\",train_data[\"Sex\"].value_counts(),\"\\n\")\nprint(\"*********Unique value of SibSp column********\\n\",train_data[\"SibSp\"].value_counts(),\"\\n\")\nprint(\"*********Unique value of Parch column********\\n\",train_data[\"Parch\"].value_counts(),\"\\n\")\nprint(\"*********Unique value of Embarked column********\\n\",train_data[\"Embarked\"].value_counts(),\"\\n\")","611dad71":"#Bucketize passengers based on age\n#bucket = [0,15,30,45,60,75,90,105]\nbucket = [0,10,20,30,40,50,60,70,80,90,100]\nlabels = [1,2,3,4,5,6,7,8,9,10]\ntrain_data[\"ageGroup\"] = pd.cut(train_data[\"Age\"],bucket)\ntrain_data[\"ageGrouplabels\"] = pd.cut(train_data[\"Age\"],bucket,labels=labels)\n\nprint(\"*********Passenger count by age group********\\n\",train_data[\"ageGroup\"].value_counts().sort_values(axis=0),\"\\n\")\nprint(\"*********Male passenger count by age group********\\n\",(train_data.where(train_data[\"Sex\"]==\"male\"))[\"ageGroup\"].value_counts().sort_values(axis=0),\"\\n\")\nprint(\"*********Female passenger count by age group********\\n\",(train_data.where(train_data[\"Sex\"]==\"female\"))[\"ageGroup\"].value_counts().sort_values(axis=0),\"\\n\")\n#Class wise gender distribution\nprint(\"*********Class wise male distribution********\\n\",(train_data.where(train_data[\"Sex\"]==\"male\"))[\"Pclass\"].value_counts().sort_values(axis=0),\"\\n\")\nprint(\"*********Class wise female distribution********\\n\",(train_data.where(train_data[\"Sex\"]==\"female\"))[\"Pclass\"].value_counts().sort_values(axis=0),\"\\n\")\n#Class wise ageGroup distribution\nprint(\"*********1st Class wise ageGroup distribution********\\n\",(train_data.where(train_data[\"Pclass\"]==1))[\"ageGroup\"].value_counts().sort_values(axis=0),\"\\n\")\nprint(\"*********2nd Class wise ageGroup distribution********\\n\",(train_data.where(train_data[\"Pclass\"]==2))[\"ageGroup\"].value_counts().sort_values(axis=0),\"\\n\")\nprint(\"*********3rd Class wise ageGroup distribution********\\n\",(train_data.where(train_data[\"Pclass\"]==3))[\"ageGroup\"].value_counts().sort_values(axis=0),\"\\n\")","e0875f3f":"#Scatter plot between feature 1) Age 2) Fare\n\nsb.set_style(\"whitegrid\");\nsb.FacetGrid(train_data,hue=\"Survived\",height=10)\\\n.map(plt.scatter,\"Age\",\"Fare\")\\\n.add_legend();\nplt.show();\n\n\n","ed42e41c":"#scatter plot between Parch and SibSp\nsb.set_style(\"whitegrid\");\nsb.FacetGrid(train_data,hue=\"Survived\",height=10)\\\n.map(plt.scatter,\"SibSp\",\"Parch\")\\\n.add_legend();\nplt.show();","6ac37088":"#Pair Plot to show more relation\n\nplt.close\nsb.set_style(\"whitegrid\")\nsb.pairplot(train_data[['Survived','Pclass','Age', 'SibSp','Parch','Fare']],hue=\"Survived\",height=3);\nplt.show();\n","758bc7cb":"#Univariate analysis of features based on histogram\/PDF\n#Pclass\nsb.FacetGrid(train_data,hue=\"Survived\",height = 5)\\\n.map(sb.histplot,\"Pclass\").add_legend()\nplt.show()\n#Age\nsb.FacetGrid(train_data,hue=\"Survived\",height = 5)\\\n.map(sb.histplot,\"Age\").add_legend()\nplt.show()\n#SibSp\nsb.FacetGrid(train_data,hue=\"Survived\",height = 5)\\\n.map(sb.histplot,\"Sex\").add_legend()\nplt.show()\n#SibSp\nsb.FacetGrid(train_data,hue=\"Survived\",height = 5)\\\n.map(sb.histplot,\"Fare\").add_legend()\nplt.show()","c10b3aa6":"# Violin plot to know the percentile\nsb.violinplot(x=\"Survived\",y=\"Age\",data=train_data)\nplt.show()\n\n\nsb.violinplot(x=\"Survived\",y=\"Pclass\",data=train_data)\nplt.show()\n\n\nsb.violinplot(x=\"Survived\",y=\"Fare\",data=train_data)\nplt.show()\n\nsb.violinplot(x=\"Survived\",y=\"Sex\",data=train_data)\nplt.show()\n","80f62220":"print(train_data.corr())\n#sb.heatmap(train_data.corr(),cmap=\"YlGnBu\",annot=True)\nsb.heatmap(train_data.corr(),cmap=\"Blues\",annot=True)\nplt.show()","40854ec7":"#Review Data\nprint(train_data.columns)\ntrain_data.describe()","7cf9cfc7":"msno.matrix(train_data)","a2090faa":"#Check which all columns have missing values\ntrain_data.loc[:, train_data.isnull().any()].columns","4c6f9241":"#Fill the missing values with Mode\ntrain_data[\"Age\"]=train_data[\"Age\"].fillna(train_data[\"Age\"].mode()[0])\ntrain_data[\"Embarked\"]=train_data[\"Embarked\"].fillna(train_data[\"Embarked\"].mode()[0])\ntrain_data.loc[:, train_data.isnull().any()].columns","9f344a6e":"#Now since we have imputed data for Age missing values, let's drop ageGroup column as it is not having much input\ntrain_data = train_data.drop(columns =[\"ageGroup\",\"ageGrouplabels\"])\n\n#Let's drop other unwanted columns such as:\n    #1. PassengerId - drop this as this is unique for each passenger and just an Id will not have any input for model\n    #2. Name - Name column will not have any impact on survival of passenger\n    #3. Ticket - Ticket number column will not have any impact on survival of passenger\n    #4. Cabin - Since cabin column has lot of missing values hence let's drop it\n\ntrain_data = train_data.drop(columns=[\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\"])\nprint(train_data.columns)","a2896797":"msno.matrix(train_data)","ae05afc8":"#Standardize Fare column\n\nprint(min(train_data[\"Fare\"]),max(train_data[\"Fare\"]),train_data[\"Fare\"].median()) #As Fare column data is highly skewed, lets standardize this\n\nFare_Scale = StandardScaler(with_mean=False)\nFare_Scale.fit(train_data[\"Fare\"].values.reshape(-1,1))\ntrain_data[\"train_fareStd\"] = Fare_Scale.transform(train_data[\"Fare\"].values.reshape(-1,1))\nprint(train_data[\"train_fareStd\"].shape)\n","054e914d":"#Add feature called IsAccompanied - passenger who are travelling with one or more SibSP and\/or Parch will have value as true\ntrain_data[\"IsAccompanied\"] = 0\ntrain_data.loc[(train_data[\"SibSp\"]>0)| (train_data[\"Parch\"]>0), 'IsAccompanied'] = 1\n\n\n#Add feature called \"IsMinor\" if the age is less then 5 years\ntrain_data[\"IsMinor\"] = 0\ntrain_data.loc[(train_data[\"Age\"]<5), 'IsMinor'] = 1\ntrain_data.shape","dacbd653":"#Label encode Pclass\n\nle = LabelEncoder()\nle.fit(train_data[\"Pclass\"].values)\nle.fit(train_data[\"Pclass\"].values)\ntrain_data[\"train_Pclass_le\"] = le.transform(train_data[\"Pclass\"])\nprint(train_data[\"train_Pclass_le\"].shape)\n\n\n# Label encode Sex column\nfrom sklearn.preprocessing import LabelEncoder\nle_sex = LabelEncoder()\nle_sex.fit(train_data[\"Sex\"].values)\nle_sex.fit(train_data[\"Sex\"].values)\ntrain_data[\"train_Sex_le\"] = le_sex.transform(train_data[\"Sex\"])\nprint(train_data[\"train_Sex_le\"].shape)\n\n# Label encode Embarked column\nfrom sklearn.preprocessing import LabelEncoder\nle_Embarked = LabelEncoder()\nle_Embarked.fit(train_data[\"Embarked\"].values)\nle_Embarked.fit(train_data[\"Embarked\"].values)\ntrain_data[\"train_Embarked_le\"] = le_Embarked.transform(train_data[\"Embarked\"])\nprint(train_data[\"train_Embarked_le\"].shape)\n","420f560f":"#Load test data\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nprint(test_data.shape)\n#Add Additional features\n#Add feature called IsAccompanied - passenger who are travelling with one or more SibSP and\/or Parch will have value as true\ntest_data[\"IsAccompanied\"] = 0\ntest_data.loc[(test_data[\"SibSp\"]>0)| (test_data[\"Parch\"]>0), 'IsAccompanied'] = 1\n\n\n#Add feature called \"IsMinor\" if the age is less then 5 years\ntest_data[\"IsMinor\"] = 0\ntest_data.loc[(test_data[\"Age\"]<5), 'IsMinor'] = 1\n\n\n#Fill the missing values with Mode\ntest_data[\"Age\"]=test_data[\"Age\"].fillna(test_data[\"Age\"].mode()[0])\ntest_data[\"Embarked\"]=test_data[\"Embarked\"].fillna(test_data[\"Embarked\"].mode()[0])\ntest_data[\"Fare\"]=test_data[\"Fare\"].fillna(test_data[\"Fare\"].mode()[0])\n\nprint(\"*********************************************************\")\n\n#Standardize fare column in test data\ntest_data[\"test_fareStd\"] = Fare_Scale.transform(test_data[\"Fare\"].values.reshape(-1,1))\n#Label encode categorical column\ntest_data[\"test_Pclass_le\"] = le.transform(test_data[\"Pclass\"])\ntest_data[\"test_Sex_le\"] = le_sex.transform(test_data[\"Sex\"])\ntest_data[\"test_Embarked_le\"] = le_Embarked.transform(test_data[\"Embarked\"])\n\n#Drop non required columns from test data\n#Let's drop other unwanted columns such as:\n    #1. Name - Name column will not have any impact on survival of passenger\n    #2. Ticket - Ticket number column will not have any impact on survival of passenger\n    #3. Cabin - Since cabin column has lot of missing values hence let's drop it\ntest_data = test_data.drop(columns=[\"Name\",\"Ticket\",\"Cabin\"])\n\n#Check for missing values in test data\nmsno.matrix(test_data)\n#Print shape of each feature\nprint(test_data[\"test_fareStd\"].shape)\nprint(test_data[\"test_Pclass_le\"].shape)\nprint(test_data[\"test_Sex_le\"].shape)\nprint(test_data[\"test_Embarked_le\"].shape)\nprint(test_data.columns)\n","d60517ce":"print(train_data.columns)\nprint(test_data.columns)","3a8d9f7b":"\nx_train = train_data[['IsMinor','train_fareStd','train_Pclass_le','train_Sex_le','IsAccompanied']]\ny_train = train_data[['Survived']]\n\n#Get test x and y data\nx_test = test_data[['IsMinor','test_fareStd','test_Pclass_le','test_Sex_le','IsAccompanied']]\n\nprint(\"Shape of x_train and y_train: \",x_train.shape, y_train.shape)\nprint(\"Shape of x_test: \",x_test.shape)","c7647572":"#Split data into train and cross validation\n\nxtrain, xcv, ytrain, ycv = train_test_split(x_train, y_train, test_size=0.33, stratify=y_train)\nprint(\"Shape of train data points X and Y respectively: \",xtrain.shape,ytrain.shape)\nprint(\"Shape of CV data points X and Y respectively: \",xcv.shape,ycv.shape)\n","6e92718b":"#Define some helper methods\n\ndef plotConfusionMatrix(y_true,probability,threshold,tpr,fpr,title):\n    #Print confusion matrix\n    tr_cf=confusion_matrix(y_true,getpredbasedonThreshold(probability,threshold,fpr,tpr))\n    #Reference: with the help of code-related discussion on slack\n    labels = np.array(\n        [\n            ['TN={}'.format(tr_cf[0][0]),'FP={}'.format(tr_cf[0][1])],\n            ['FN={}'.format(tr_cf[1][0]), 'TP={}'.format(tr_cf[1][1])]\n        ]\n    )\n    plt.figure()\n    sb.heatmap(tr_cf,annot=labels,linewidth=0.5,fmt='',cbar=False)\n    plt.xlabel('predicted label')\n    plt.ylabel('True label');\n    plt.title(title)\n    \ndef plot_roc_curve(train_fpr,train_tpr,test_fpr,test_tpr,label):\n    #Plot AUC Curve with tpr and fpr for train and cross validation\n    train_roc_auc = auc(train_fpr,train_tpr)\n    test_roc_auc = auc(test_fpr,test_tpr)\n\n    plt.plot([0, 1], [0, 1], color='black',linestyle='--') #Reference: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html\n\n    plt.plot(train_fpr,train_tpr,label='Train AUC='+str(train_roc_auc))\n    plt.plot(test_fpr,test_tpr,label=label+str(test_roc_auc))\n    plt.legend(loc=\"upper left\")\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC Curve')\n    plt.show()\n    #Reference: with the help of code-related discussion on slack and https:\/\/stackoverflow.com\/questions\/28719067\/roc-curve-and-cut-off-point-python\ndef plotauc(Hyperparamrange,trainAUC,cv_AUC):\n    plt.plot(np.log(Hyperparamrange),trainAUC,color='b',label='train AUC')\n    plt.plot(np.log(Hyperparamrange),cv_AUC, color='g',label='CV AUC')\n    plt.scatter(np.log(Hyperparamrange), trainAUC, label='Train AUC points')\n    plt.scatter(np.log(Hyperparamrange), cv_AUC, label='CV AUC points')\n    plt.legend(loc=\"upper right\")\n    plt.xlabel('Hyper parameter C')\n    plt.ylabel('Area under ROC Curve')\n    plt.title('Model performance for different C value')\n    plt.show()\n    \ndef applyOptimalLR(xtrain,ytrain,xcv,ycv,clf):\n    LRclf = clf\n    #Fit the train data with LR classifier and train the data\n    LRclf.fit(xtrain,ytrain)\n    \n    #Reference: https:\/\/scikit-learn.org\/0.18\/auto_examples\/model_selection\/plot_roc_crossval.html\n    #Generate AUC curve for train\n    tr_proba = LRclf.predict_proba(xtrain)\n   \n    tr_fpr, tr_tpr, tr_thresholds= roc_curve(ytrain,tr_proba[:,1])\n    #Generate AUC curve for cross validation\n    proba = LRclf.predict_proba(xcv)\n    #pred=LRclf.predict(X_test)\n    fpr, tpr,thresholds = roc_curve(ycv,proba[:,1])\n    plot_roc_curve(tr_fpr,tr_tpr,fpr,tpr,'Test AUC')#Plot roc curve\n    plotConfusionMatrix(ytrain,tr_proba[:,1],tr_thresholds,tr_tpr,tr_fpr,\"Train Confusion Matrix\")\n    # Plot confusion matrix for train and test\n    plotConfusionMatrix(ycv,proba[:,1],thresholds,tpr,fpr,\"Test Confusion Matrix\")# Plot confusion matrix for train and test\n    return LRclf\n\ndef getpredbasedonThreshold(probability,threshold,fpr,tpr):\n    tshd = threshold[np.argmax(tpr-fpr)]\n    predictedvalue = []\n    for p in probability:\n        if p>=tshd:\n            predictedvalue.append(1)\n        else:\n            predictedvalue.append(0)\n    return predictedvalue","4807f77d":"#Define classifier method using logistic regression\n\ndef LRclassifier(xtrain,ytrain,xcv,ycv):\n    CVal=[]\n    a_cv_auc=[]\n    a_tr_auc=[]\n    C=[0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n    for i in C:\n        LRclf = LogisticRegression(penalty='l2',C=i, class_weight=None,max_iter=100,verbose=0,n_jobs=None)\n        #Fit the train data with LR classifier and train the data\n        LRclf.fit(xtrain,ytrain)\n        CVal.append(i)\n\n        #Reference: https:\/\/scikit-learn.org\/0.18\/auto_examples\/model_selection\/plot_roc_crossval.html\n\n        #Generate AUC curve for train\n        tr_proba = LRclf.predict_proba(xtrain)\n        tr_auc=roc_auc_score(ytrain,tr_proba[:,1])\n        a_tr_auc.append(tr_auc)\n        #Generate AUC curve for cross validation\n        proba = LRclf.predict_proba(xcv)\n        cv_auc = roc_auc_score(ycv,proba[:,1])\n        a_cv_auc.append(cv_auc)\n        print('\\nFor C =%f, Train AUC= %f and CV AUC=%f'%(i,tr_auc,cv_auc))\n#---------------------For loop ends here ------------------------------------------------\n    #Plot train and cross validation AUC value for different value of C\n    plotauc(CVal,a_tr_auc,a_cv_auc)\n    #Identify best C value, considering the gap between train and CV is less and AUC of CV is high\n    bestCdf = pd.DataFrame({'tr_auc':a_tr_auc,'cv_auc':a_cv_auc,'CVal':CVal, 'logval':np.log(CVal)})\n    bestCdf['auc_diff'] =bestCdf['tr_auc']-bestCdf['cv_auc']\n    FinalC=bestCdf.sort_values(by=['cv_auc'], ascending=[False])\n    FinalC =FinalC['CVal'].iloc[0]\n    print('\\n Optimal C value = %f'% FinalC)\n    return FinalC","21ca6be3":"#call the model\nopt_C = LRclassifier(xtrain,ytrain,xcv,ycv)\n","dc934bbe":"#Applying optimal value of C to the train and cv data\nbest_model = LogisticRegression(penalty='l2',C=opt_C, class_weight=None,max_iter=100,verbose=0,n_jobs=None)\nfinal_model = applyOptimalLR(xtrain,ytrain,xcv,ycv,best_model)\nprint(final_model)\n","61705cb3":"final_model.fit(x_train,y_train)\nsurvival_prediction = final_model.predict(x_test)\n","6c252bde":"#Final output of test data\nresult_df = pd.DataFrame(columns = ['PassengerId','Survived'])\nfor i in range(test_data[\"PassengerId\"].count()):\n    new_row = pd.DataFrame({'PassengerId':test_data.loc[i,\"PassengerId\"], 'Survived':survival_prediction[i]},index =[0])\n    result_df=pd.concat([new_row, result_df]).reset_index(drop = True)\n\nresult_df.head()\n","a98b8cb8":"result_df.to_csv('my_submission.csv', index=False)\n#print(\"Your submission was successfully saved!\")","b64d7039":"**Findings**\n* Pclass and SibSp seems highly correlated","862f7330":"**Findings**\n* Based on the pair plot nothing much can be figured out as only Age and Fare seem to have some distinguition of data else others are overlapping","bf8adae9":"**Findings**\n* Passengers who bought high fare tickets and having middle age group between 20 to 50 are mostly survived","6bdc5273":"### Predict the values for submission","22e3e635":"### Encode features","1733d15b":"# Import Libraries","35d521af":"# Feature Engineering\nEncode feature, standardize, normalize etc.","18ca8bbd":"## EDA Conclusion\n* Based on EDA most important features are\n    * Age\n    * Fare\n    * Pclass\n    * Sex\n* Other features which are having low importance are\n    * SibSp\n    * Parch\n* Also we need to normalize fare values","3060f0cf":"**Findings**\n* More passengers with upper class is survived\n* Middle class passengers have ~50% survival rate\n* Lower class passengers have very low percentage of survival\n* More child passengers have been saved\n* Percentage of female passengers is high\n","d771d82f":"# Load Data","b4a47249":"### Get final train, cross validation and test data for model with required columns only","60cf58ad":"# Data Cleansing & Preparation","68f6a705":"**Findings**\n* Most of the passengers are between age group 10 to 40\n* Gender wise age group distribution looks similar\n* Class wise gender distribution also looks similar\n* Class wise age group is distributed similar, the only thing is lower class has more passengers","9ab2cada":"# Exploratroy data analysis","1fea3d89":"### Check Missing Values","c37fa236":"### Let's plot the data to visually understand","23cbc459":"### Describe train data","59b5b1f4":"**Findings**\n* Few passenger's age is not available\n* Passengers are with mixed age group, mostly below 50 years\n* There are few passengers with age group below zero, need to validate that data\n* There are more passengers with the less fare tickets\n","c9009f69":"# Create Model","62a05286":"# Data correlation","f3391ab9":"**Findings**\n* There are more passengers of lower class as compared to upper and middle\n* Approximately male passengers are double of female passengers\n* More passengers are travelling alone (approximately half of the passengers are travelling alone)\n* Approximately 25% of passengers are travelling with siblings, spouse, mother,father, children\n* Half of the passengers have boarded ship from Southampton (S)","f1c04e4d":"### Get test data too for preprocessing","b15eb4de":"**Findings**\n* Passengers having lesser count of SibSP and Parch are survived","d6490ad7":"### Write result to csv"}}