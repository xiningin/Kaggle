{"cell_type":{"46d42b6b":"code","5f51a570":"code","ab42bef6":"code","3dd04374":"code","60876c31":"code","7622c82d":"code","8744f6b4":"code","e84ee67a":"code","a9efed04":"code","a76d30ef":"code","6cf0b13f":"code","76380f3c":"code","86b1dfd5":"code","b87170c4":"code","860a228f":"code","945c85de":"code","7b23c45a":"code","ffc0f1f1":"code","b5c85fa9":"code","5ff93fab":"code","380ff8ee":"code","9b22570c":"code","65acab62":"code","069900e2":"code","f390df93":"code","7229ae6e":"code","b87411bc":"code","88167ace":"code","e6474f66":"code","ec392ea6":"code","3457e9c9":"code","331c08c0":"code","971b5545":"code","fb7cf728":"markdown","e4a68023":"markdown","3f12557a":"markdown","3e586029":"markdown","7be92a6d":"markdown","2aa74efe":"markdown"},"source":{"46d42b6b":"# Importing the requisite libraries\nimport os\nimport pandas as pd\nimport json\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport sklearn.metrics\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\n\n\n\n#Converting the json files to dataframes\n#with open('train.json', 'r') as f:\n    #data = json.load(f)\n#df_train = pd.DataFrame(data)\n\ndf_train = pd.read_json('..\/input\/train.json')\n\ndf_train['ingredients'] = df_train['ingredients'].astype('str') \n\ndf_test = pd.read_json('..\/input\/test.json')\n\ndf_test['ingredients'] = df_test['ingredients'].astype('str')","5f51a570":"#checking the dataframes\ndf_train.head()\n","ab42bef6":"df_train.dtypes","3dd04374":"df_test.head()","60876c31":"# Exploring the training dataset\nprint(\"The data-set has %d rows and %d columns\"%(df_train.shape[0],df_train.shape[1]))","7622c82d":"# Exploring the testing dataset\nprint(\"The data-set has %d rows and %d columns\"%(df_test.shape[0],df_test.shape[1]))","8744f6b4":"# Checking the number of categories for cuisine in the tranining data\ncategory_counter={x:0 for x in set(df_train['cuisine'])}\nfor each_cat in df_train['cuisine']:\n    category_counter[each_cat]+=1\n\nprint(category_counter)\n","e84ee67a":"#corpus means collection of text. For this particular data-set, in our case it is Review_text\n\ncorpus=df_train.ingredients\ncorpus","a9efed04":"#Initializing TFIDF vectorizer to conver the raw corpus to a matrix of TFIDF features and also enabling the removal of stopwords.\nno_features = 10000\nvectorizer = TfidfVectorizer(max_df=0.70, min_df=0.0001, max_features=no_features, stop_words='english',ngram_range=(1,2))","a76d30ef":"#creating TFIDF features sparse matrix by fitting it on the specified corpus\ntfidf_matrix=vectorizer.fit_transform(corpus).todense()\n#grabbing the name of the features.\ntfidf_names=vectorizer.get_feature_names()\n\nprint(\"Number of TFIDF Features: %d\"%len(tfidf_names)) #same info can be gathered by using tfidf_matrix.shape","6cf0b13f":"# Training data split into training and test data set using 60-40% ratio \n\n#considering the TFIDF features as independent variables to be input to the classifier\n\nvariables = tfidf_matrix\n\n#considering the category values as the class labels for the classifier.\n\nlabels = df_train.cuisine\n\n#splitting the data into random training and test sets for both independent variables and labels.\n\nvariables_train, variables_test, labels_train, labels_test  =   train_test_split(variables, labels, test_size=.2)","76380f3c":"#analyzing the shape of the training and test data-set:\nprint('Shape of Training Data: '+str(variables_train.shape))\nprint('Shape of Test Data: '+str(variables_test.shape))","86b1dfd5":"#Applying Naive Bayes:\n\n#initializing the object\n\nbnb_classifier=BernoulliNB()\n\n#fitting the classifier or training the classifier on the training data\nbnb_classifier=bnb_classifier.fit(variables_train,labels_train)\n\n#after the model has been trained, we proceed to test its performance on the test data\nbnb_predictions=bnb_classifier.predict(variables_test)\n\n#the trained classifier has been used to make predictions on the test data-set. To evaluate the performance of the model,\n#there are a number of metrics that can be used as follows:\nnb_ascore=sklearn.metrics.accuracy_score(labels_test, bnb_predictions)\n\nprint(\"Bernoulli Naive Bayes Accuracy Score:  %f\" %(nb_ascore))","b87170c4":"#Applying Multinomial Naive Bayes:\n\n#initializing the object\nmn_bayes=MultinomialNB()\n\n#fitting the classifier or training the classifier on the training data\nmn_bayes_fit=mn_bayes.fit(variables_train,labels_train)\n\n#after the model has been trained, we proceed to test its performance on the test data\nprediction_mn=mn_bayes_fit.predict(variables_test)\n\n#the trained classifier has been used to make predictions on the test data-set. To evaluate the performance of the model,\n#there are a number of metrics that can be used as follows:\nmn_ascore=sklearn.metrics.accuracy_score(labels_test, prediction_mn) \n\nprint(\"Accuracy Score of Multi-Nomial Naive Bayes: %f\" %(mn_ascore))","860a228f":"#Applying Random Forest Classifier:\n\n#initializing the object\nrf_classifier=RandomForestClassifier(n_estimators=200)\n\n#fitting the classifier or training the classifier on the training data\nrf_classifier=rf_classifier.fit(variables_train,labels_train)\n\n#after the model has been trained, we proceed to test its performance on the test data\nrf_predictions=rf_classifier.predict(variables_test)\n\n#the trained classifier has been used to make predictions on the test data-set. To evaluate the performance of the model,\n#there are a number of metrics that can be used as follows:\nrf_ascore=sklearn.metrics.accuracy_score(labels_test, rf_predictions)\n\nprint (\"Accuracy Score of Random Forests Classifier: %f\" %(rf_ascore))\n","945c85de":"#Applying Linear Classifier (SVM) using Stochastic Gradient Descent\n\n#initializing the object\nsvm_classifier=linear_model.SGDClassifier(loss='modified_huber',alpha=0.0001)\n\n#fitting the classifier or training the classifier on the training data\nsvm_classifier=svm_classifier.fit(variables_train, labels_train)\n\n#after the model has been trained, we proceed to test its performance on the test data\nsvm_predictions=svm_classifier.predict(variables_test)\n\n#the trained classifier has been used to make predictions on the test data-set. To evaluate the performance of the model,\n#there are a number of metrics that can be used as follows:\n\nsvm_ascore=sklearn.metrics.accuracy_score(labels_test, svm_predictions)\n\nprint (\"Accuracy Score of Linear SVM Classifier: %f\" %(svm_ascore))\n\n","7b23c45a":"#Applying Logistic Regression\n\n#initializing the object\nLogreg_classifier= LogisticRegression(random_state=0)\n\n#fitting the classifier or training the classifier on the training data\nLogreg_classifier=Logreg_classifier.fit(variables_train, labels_train)\n\n#after the model has been trained, we proceed to test its performance on the test data\nLogreg_predictions=Logreg_classifier.predict(variables_test)\n\n#the trained classifier has been used to make predictions on the test data-set. To evaluate the performance of the model,\n#there are a number of metrics that can be used as follows:\n\nLogreg_ascore=sklearn.metrics.accuracy_score(labels_test, Logreg_predictions)\n\nprint (\"Accuracy Score of Logistic Regression Classifier: %f\" %(Logreg_ascore))","ffc0f1f1":"#now we have seen that with all the algorithms that we have tried and with all the parameter tuning \n# we have tried the maximum accurcy hovers around 76%. \n#The options are - Try ensembling the models already trained to see if the accuracy improves\n#                - Try cleaning up data and try data pre-processing to see if the accuracy improves\n\n","b5c85fa9":"# Picking up the models that have shown highest accuracy for ensembling\n\nensemble_classifier = VotingClassifier(estimators=[('LR',Logreg_classifier),('RF',rf_classifier), ('SVML',svm_classifier)],\n                        voting='soft',\n                        weights=[2,1,4])\n\n#fitting the classifier or training the classifier on the training data\nensemble_classifier = ensemble_classifier.fit(variables_train, labels_train)\n\n#after the model has been trained, we proceed to test its performance on the test data\nensemble_predictions = ensemble_classifier.predict(variables_test)\n\n#the trained classifier has been used to make predictions on the test data-set. To evaluate the performance of the model,\n#there are a number of metrics that can be used as follows:\n\nensemble_ascore=sklearn.metrics.accuracy_score(labels_test, ensemble_predictions)\n\nprint (\"Accuracy Score of Ensemble Classifier: %f\" %(ensemble_ascore))","5ff93fab":"# Now that we have tried most of the ML algorithms, we could try deep learning models now to see if we can increase the accuracy\n# Below codes are sourced from https:\/\/shrikar.com\/deep-learning-with-keras-and-python-for-multiclass-classification\/\n# Importing all the necessary libraries\n\nimport keras \nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dense, Dropout, Embedding, LSTM, Flatten\nfrom keras.models import Model\nfrom keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","380ff8ee":"df_train.head()","9b22570c":"# Check class distributions\ndf_train.cuisine.value_counts()","65acab62":"# Convert Cuisines to integers as most of the machine learning models deal with integer or float\ndf_train['cuisine'] = df_train.cuisine.astype('category').cat.codes","069900e2":"# For a deep learning model we need to know what the input sequence length for our model should be\n# Calculate the number of words in the ingredients column and find the maximum value to be used for input sequence\n\ndf_train['num_words'] = df_train.ingredients.apply(lambda x : len(x.split()))\n\ndf_train['num_words'].max()","f390df93":"# Set number of classes and target variable\nnum_class = len(np.unique(df_train.cuisine.values))\ny = df_train['cuisine'].values","7229ae6e":"#Tokenize the input\nMAX_LENGTH = 137\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train.ingredients.values)\npost_seq = tokenizer.texts_to_sequences(df_train.ingredients.values)\npost_seq_padded = pad_sequences(post_seq, maxlen=MAX_LENGTH)","b87411bc":"vocab_size = len(tokenizer.word_index) + 1","88167ace":"# Creating a train test split with 30% in testing data set\nX_train, X_test, y_train, y_test = train_test_split(post_seq_padded, y, test_size=0.05)","e6474f66":"# Deep Learning Model : Simple\n# Let start with a simple model where the build an embedded layer, Dense followed by our prediction\n\ninputs = Input(shape=(MAX_LENGTH, ))\nembedding_layer = Embedding(vocab_size,\n                            200,\n                            input_length=MAX_LENGTH)(inputs)\nx = Flatten()(embedding_layer)\nx = Dense(64, activation='relu')(x)\n\npredictions = Dense(num_class, activation='softmax')(x)\nmodel = Model(inputs=[inputs], outputs=predictions)\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['acc'])\n\nmodel.summary()\nfilepath=\"weights-simple.hdf5\"\ncheckpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nhistory = model.fit([X_train], batch_size=64, y=to_categorical(y_train), verbose=1, validation_split=0.20, \n          shuffle=True, epochs=5, callbacks=[checkpointer])","ec392ea6":"# Deep Learning Model : Recurrent Neural Networks with LSTM cells\n\ninputs = Input(shape=(MAX_LENGTH, ))\nembedding_layer = Embedding(vocab_size,\n                            200,\n                            input_length=MAX_LENGTH)(inputs)\n\nx = LSTM(72)(embedding_layer)\nx = Dense(64, activation='relu')(x)\npredictions = Dense(num_class, activation='softmax')(x)\nmodel = Model(inputs=[inputs], outputs=predictions)\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['acc'])\n\nmodel.summary()\n\nfilepath=\"weights.hdf5\"\ncheckpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nhistory = model.fit([X_train], batch_size=64, y=to_categorical(y_train), verbose=1, validation_split=0.20, \n          shuffle=True, epochs=7, callbacks=[checkpointer])\n","3457e9c9":"#Lets look at prediction accuracy\npredicted = model.predict(X_test)\npredicted = np.argmax(predicted, axis=1)\naccuracy_score(y_test, predicted)","331c08c0":"#Predicting on the test data \n\n#Preparing the TF-IDF out of the test data\n\ncorpus1=df_test['ingredients']\n\ntfidf_matrix1=vectorizer.transform(corpus1).todense()\n\nvariables1 = tfidf_matrix1\n\nEns_Test_predictions = ensemble_classifier.predict(variables1)\n\ntest_id = df_test['id']\n\nsub_file = pd.DataFrame({'id': test_id, 'cuisine': Ens_Test_predictions}, columns=['id', 'cuisine'])\n\nsub_file.to_csv('seventh_sub.csv', index=False)\n","971b5545":"sub_file.head()","fb7cf728":"![image.png](attachment:image.png)","e4a68023":"#### Here is the code for implementing SVM non linear model, it has been converted to mark down becuase, it takes hours to execute and the accuracy is also lower\n#### But people can try playing with values of \"C\", \"gamma\" or try changing the \"kernel\" to improve performance","3f12557a":"#Applying Xtreme Gradient Boosting Classifier: (converted to markdown coz takes lot of time to train)\n\n#initializing the object\nXGB_classifier = XGBClassifier()\n\n#fitting the classifier or training the classifier on the training data\nXGB_classifier=XGB_classifier.fit(variables_train, labels_train)\n\n#after the model has been trained, we proceed to test its performance on the test data\nXGB_predictions=XGB_classifier.predict(variables_test)\n\n#the trained classifier has been used to make predictions on the test data-set. To evaluate the performance of the model,\n#there are a number of metrics that can be used as follows:\n\nXGB_ascore=sklearn.metrics.accuracy_score(labels_test, XGB_predictions)\n\nprint (\"Accuracy Score of Linear SVM Classifier: %f\" %(XGB_ascore))\n","3e586029":"\n\nApplying Non Linear Classifier (SVM) using Stochastic Gradient Descent\n\ninitializing the object\nnl_svm_classifier=SVC(C=1.0, gamma='auto', kernel='rbf')\n\nfitting the classifier or training the classifier on the training data\nnl_svm_classifier=nl_svm_classifier.fit(variables_train,labels_train)\n\nafter the model has been trained, we proceed to test its performance on the test data\nnl_svm_predictions=nl_svm_classifier.predict(variables_test)\n\nthe trained classifier has been used to make predictions on the test data-set. To evaluate the performance of the model,\nthere are a number of metrics that can be used as follows:\n\nnl_svm_ascore=sklearn.metrics.accuracy_score(labels_test, nl_svm_predictions)\n\nprint (\"Accuracy Score of Non Linear SVM Classifier: %f\" %(nl_svm_ascore))","7be92a6d":"![image.png](attachment:image.png)","2aa74efe":"## The problem at hand is a supervised multiclass classification learning problem. As the data is in text form, we could try few algorithms which are known to work well in such settings\n\n#### The algorithms to be tried are\n1. Naive Bayes - Bernoulli\n2. Naive Bayes - Multinomial\n3. Random Forest\n4. SVM - Linear\n5. SVM - Non Linear (not applied as it takes long time to run and accuracy is not impresseive - codes are available)\n6. Logistic Regression"}}