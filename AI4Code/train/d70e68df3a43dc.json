{"cell_type":{"18b19695":"code","638f949d":"code","4dd75518":"code","70f43875":"code","3acbb203":"code","b989377e":"code","416891d3":"code","dbbc1fba":"code","e86369f9":"code","a2c036c3":"code","4af05725":"code","c62e0667":"code","7d503659":"code","72db19c6":"code","bec94130":"code","c1c4355d":"code","66967af9":"code","9857f67d":"code","1e03c82f":"code","0e2ee5b5":"code","966562fe":"code","6e58e143":"code","1e684bef":"markdown","02a03a50":"markdown","2f026180":"markdown","1509d6b2":"markdown","1f3d52a2":"markdown","59a45dfb":"markdown","b3ee5434":"markdown","048daf09":"markdown","14b562dd":"markdown","37741f1f":"markdown","00aea392":"markdown","0060fda4":"markdown","e7b51286":"markdown","369d41ee":"markdown","c3357598":"markdown","55d0ae02":"markdown","3eb0754b":"markdown","08a551de":"markdown","87c74d9b":"markdown","93bd177d":"markdown","f6c48c51":"markdown","5d4357be":"markdown","b2fd07b6":"markdown","a264de93":"markdown","5fa37272":"markdown","b45535e4":"markdown"},"source":{"18b19695":"import numpy as np # Linear algebra\nimport pandas as pd # Data processing.\n\nimport seaborn as sns # Visualizing (Heat Map)\nimport matplotlib.pyplot as plt # Visualizing\n\nfrom sklearn.metrics import confusion_matrix # Comparing\n\nimport warnings # For ignore warnings\nwarnings.filterwarnings(\"ignore\")","638f949d":"data = pd.read_csv(\"..\/input\/data.csv\")","4dd75518":"data.drop([\"id\",\"Unnamed: 32\"],axis = 1, inplace = True)\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\n\ny = data.diagnosis\nx_data = data.drop([\"diagnosis\"],axis = 1)\n\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)) # Normalize data\n\ncompare = []","70f43875":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.15,random_state = 42)","3acbb203":"y_true = y_test","b989377e":"from sklearn.neighbors import KNeighborsClassifier\nscores = []\nfor i in range(1,len(x_test)):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    scores.append(knn.score(x_test,y_test))\nk_value = scores.index(max(scores))+1\nprint(\"Optimal n_neighbors values is :\", k_value)\n# We write max(scores)+1 because normally counting starts from 0 in software but scores list is starting with 1","416891d3":"knn2 = KNeighborsClassifier(n_neighbors=k_value)\nknn2.fit(x_train,y_train) # Fit data\ny_predict = knn2.predict(x_test)\n\ncm = confusion_matrix(y_true,y_predict)","dbbc1fba":"f,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 1, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Real\")\nplt.show()\n\nknn_correct = cm[1][1] + cm[0][0]\nknn_accuracy = knn.score(x_test,y_test)\nprint(\"Number of Correct :\",knn_correct)\nprint(\"KNN accuracy\", knn_accuracy)\ncompare.append([\"KNN\",knn_correct,knn_accuracy])","e86369f9":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\n\ny_predict = lr.predict(x_test)\n\ncm = confusion_matrix(y_true,y_predict)","a2c036c3":"f,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 1, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Real\")\nplt.show()\n\nlr_correct = cm[1][1] + cm[0][0]\nlr_accuracy = lr.score(x_test,y_test)\nprint(\"Number of Correct :\",lr_correct)\nprint(\"LR accuracy\", lr_accuracy)\ncompare.append([\"LR\",lr_correct,lr_accuracy])","4af05725":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\ny_predict = nb.predict(x_test)\n\ncm = confusion_matrix(y_true,y_predict)","c62e0667":"f,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 1, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Real\")\nplt.show()\n\nnb_correct = cm[1][1] + cm[0][0]\nnb_accuracy = nb.score(x_test,y_test)\nprint(\"Number of Correct :\",nb_correct)\nprint(\"NB accuracy\", nb_accuracy)\ncompare.append([\"NB\",nb_correct,nb_accuracy])","7d503659":"from sklearn.ensemble import RandomForestClassifier\nscores = []\nfor i in range(1,10):\n    rf = RandomForestClassifier(n_estimators = i,random_state = 42)\n    rf.fit(x_train,y_train)\n    scores.append(rf.score(x_test,y_test))\noptimal_n = scores.index(max(scores))+1\nprint(\"Optimal n_estimator :\", optimal_n)","72db19c6":"from sklearn.ensemble import RandomForestClassifier\nrf2 = RandomForestClassifier(n_estimators = 8,random_state = 42)\nrf2.fit(x_train,y_train)\ny_predict = rf2.predict(x_test)\n\ncm = confusion_matrix(y_true,y_predict)","bec94130":"f,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 1, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Real\")\nplt.show()\n\nrf_correct = cm[1][1] + cm[0][0]\nrf_accuracy = nb.score(x_test,y_test)\nprint(\"Number of Correct :\",rf_correct)\nprint(\"RF accuracy\", rf_accuracy)\ncompare.append([\"RF\",rf_correct,rf_accuracy])","c1c4355d":"from sklearn.svm import SVC\nsvm = SVC(random_state = 42)\nsvm.fit(x_train,y_train)\ny_predict = svm.predict(x_test)\n\ncm = confusion_matrix(y_true,y_predict)","66967af9":"f,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 1, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Real\")\nplt.show()\n\nsvm_correct = cm[1][1] + cm[0][0]\nsvm_accuracy = svm.score(x_test,y_test)\nprint(\"Number of Correct :\",svm_correct)\nprint(\"SVM accuracy\", svm_accuracy)\ncompare.append([\"SVM\",svm_correct,svm_accuracy])","9857f67d":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ny_predict = dt.predict(x_test)\n\ncm = confusion_matrix(y_true,y_predict)","1e03c82f":"f,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 1, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Real\")\nplt.show()\n\ndt_correct = cm[1][1] + cm[0][0]\ndt_accuracy = dt.score(x_test,y_test)\nprint(\"Number of Correct :\",dt_correct)\nprint(\"DT accuracy\", dt_accuracy)\ncompare.append([\"DT\",dt_correct,dt_accuracy])","0e2ee5b5":"accuracy = []\ncorrect = []\nindex = []\nfor i in compare:\n    accuracy.append(i[2])\n    correct.append(i[1])\n    index.append(i[0])\ndata = {\"Correct\":correct,\"Accuracy\":accuracy}\n\npd.options.display.float_format = '{:,.3f}'.format # We write this code cause of to show 3 digits after the comma\ndf = pd.DataFrame(data,index = index)\ndf","966562fe":"sns.lineplot(index,correct,color = \"red\")\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Number of Correct\")\nplt.show()","6e58e143":"sns.lineplot(index,accuracy,color = \"blue\")\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Accuracy\")\nplt.show()","1e684bef":"- K-Nearest Neighbour (KNN) Classification -> KNN\n- Logistic Regression Classification -> LR\n- Naive Bayes Classification -> NB\n- Random Forest Classification -> RF\n- Suppor Vector Machine (SVM) Classification -> SVM\n- Decision Tree Classification -> DT","02a03a50":"Visualizaiton","2f026180":"**Firstly, Let's find optimal k value**","1509d6b2":"Preparing Data","1f3d52a2":"Visualization","59a45dfb":"Prepare Mode","b3ee5434":"Visualization","048daf09":"Visualizing","14b562dd":"Visualizing","37741f1f":"Prepare Model","00aea392":"- **When we look at this comparison, we see that,the most suitable model is Linear Regression. but this does not indicate that it is the best classification algorithm. Since all classification algorithms have different mathematical background, we get different results in different datasets.**","0060fda4":"# Algorithms\n- K-Nearest Neighbour (KNN) Classification\n- Logistic Regression Classification\n- Naive Bayes Classification\n- Random Forest Classification\n- Suppor Vector Machine (SVM) Classification\n- Decision Tree Classification","e7b51286":"Prepare Model","369d41ee":"Visualize","c3357598":"## Logistic Regression Classification","55d0ae02":"## Compare","3eb0754b":"Prepare Model","08a551de":"## K-Nearest Neighbour (KNN) Classification","87c74d9b":"**Firstly, Let's find optimal n_estimators value**","93bd177d":"## Random Forest Classification","f6c48c51":"## Result","5d4357be":"## Decision Tree Classification","b2fd07b6":"Prepare Model","a264de93":"Train - Test Split","5fa37272":"## Suppor Vector Machine (SVM) Classification","b45535e4":"## Naive Bayes Classification"}}