{"cell_type":{"6b9e9e04":"code","00dcd079":"code","731c4e39":"code","0345c977":"code","62727e78":"code","5f435f04":"code","23c99ced":"code","2cd81d33":"code","a9ffafa3":"code","6078d3ef":"code","19af3c95":"code","b2a734c7":"code","2f774f58":"code","b900c5ba":"code","61d4ec6c":"code","b5ae6e65":"code","f644152e":"code","e0732d0a":"code","9cf46194":"code","da2c39f4":"code","c9aac5ef":"code","b4adf616":"code","afd06b4c":"code","89b1e59e":"code","74d89307":"code","ff3b18a6":"code","2a08adbd":"code","06963c8d":"code","68a5e3d2":"code","e9f96552":"code","66ef92f5":"code","bb6d072c":"code","f565bf98":"code","afdd3152":"code","01157e26":"code","13e4fbfd":"code","93e369d9":"code","6aa85718":"code","8770ffcc":"code","2fe47b44":"code","02510a3c":"code","3cd1a76c":"code","e102bccd":"code","ce63089c":"code","b96f5738":"code","5fd08bed":"code","2fe63581":"code","a16b647e":"code","75b4086f":"code","b7c8cb86":"code","30f24979":"code","ed8055e0":"code","65dbeae7":"code","ddfa0e9c":"code","a244c9a2":"code","3db76fee":"code","e838f9fa":"code","aef342c2":"code","f58c521d":"code","7fc12755":"code","c9aef53c":"markdown","3fe4d519":"markdown","fc87e2ff":"markdown","426b6820":"markdown","0afb4d08":"markdown","0842387f":"markdown","76e08590":"markdown","576f591e":"markdown","3116ac73":"markdown","344db917":"markdown","53b4c0ce":"markdown","dbe01509":"markdown","4d4f31e6":"markdown","ef5b7521":"markdown","3c0fe512":"markdown","7a43b426":"markdown","10ce44a0":"markdown","3133728d":"markdown","085222e1":"markdown","4d19c6e8":"markdown","f37a2158":"markdown","e80101e0":"markdown","adcbe823":"markdown","bdae0521":"markdown","66e01adb":"markdown","fc0f056e":"markdown","574b8ee8":"markdown","d86d2eb8":"markdown","59052482":"markdown","1469d674":"markdown","065e1d8e":"markdown","07361bf1":"markdown","661b2ed9":"markdown","2c461c7d":"markdown","e8bcd9ff":"markdown","20754619":"markdown","cecc01e6":"markdown","6c5a6630":"markdown","c2120f8a":"markdown","85036c6d":"markdown","d40545ad":"markdown","77d90e4c":"markdown","a31e5956":"markdown","e2663d44":"markdown","fb91980c":"markdown","d07bb63a":"markdown","0aebef56":"markdown","98d1d6c2":"markdown","d5bd0327":"markdown","336c679e":"markdown","53c84f8b":"markdown","171a290d":"markdown","851a9b09":"markdown","401128a8":"markdown","a5c5bb22":"markdown","c63908de":"markdown","86952359":"markdown","b3ae5a6c":"markdown","394ae350":"markdown"},"source":{"6b9e9e04":"import nltk\nfrom bs4 import BeautifulSoup\nimport urllib.request \n\nimport pandas as pd","00dcd079":"response = urllib.request.urlopen('http:\/\/php.net\/')\nhtml = response.read()\n\nsoup = BeautifulSoup(html, \"html5lib\")\ntext = soup.get_text(strip=True)","731c4e39":"tokens = [t for t in text.split()]","0345c977":"freq = nltk.FreqDist(tokens)","62727e78":"freq.plot(20, cumulative=False)","5f435f04":"from nltk.corpus import stopwords","23c99ced":"clean_tokens = tokens[:]\nsr = stopwords.words('english')\n\nfor token in tokens:\n    if token in sr:\n        clean_tokens.remove(token)","2cd81d33":"freq_clean = nltk.FreqDist(clean_tokens)\nfreq_clean.plot(20, cumulative=False)","a9ffafa3":"from nltk.tokenize import sent_tokenize","6078d3ef":"mytext = \"Hello Mr. Adam, how are you? I hope everything is going well.  Today is a good day, see\"\nmytest_sent = sent_tokenize(mytext)\n\nprint(mytest_sent)","19af3c95":"from nltk.tokenize import word_tokenize","b2a734c7":"mytext_word = word_tokenize(mytext)\nprint(mytext_word)","2f774f58":"mytext = \"Bonjour M. Adam, comment allez-vous? J'esp\u00e8re que tout va bie\"\nmy_text_french = sent_tokenize(mytext, \"french\")\n\nprint(my_text_french)","b900c5ba":"from nltk.corpus import wordnet","61d4ec6c":"syn = wordnet.synsets(\"pain\")\n\nprint(syn[0].definition())\nprint(syn[0].examples())","b5ae6e65":"syn = wordnet.synsets('NLP')\nprint(syn[0].definition())\n\nsyn = wordnet.synsets('Python')\nprint(syn[0].definition())","f644152e":"synonyms = []\n\nfor syn in wordnet.synsets('computer'):\n    for lemma in syn.lemmas():\n        synonyms.append(lemma.name())\n    ","e0732d0a":"print(synonyms)","9cf46194":"from nltk.stem import PorterStemmer","da2c39f4":"stemmer = PorterStemmer()\nstemmed_word = stemmer.stem('playing')","c9aac5ef":"print(stemmed_word)","b4adf616":"from nltk.stem import WordNetLemmatizer","afd06b4c":"lemmmatizer = WordNetLemmatizer()\nword_lemma = lemmmatizer.lemmatize('playing')","89b1e59e":"print(word_lemma)","74d89307":"word_lemma_verb = lemmmatizer.lemmatize('playing', pos=\"v\")","ff3b18a6":"print(word_lemma_verb)","2a08adbd":"word_lemma_verb = lemmmatizer.lemmatize('playing', pos=\"v\")\nword_lemma_noun = lemmmatizer.lemmatize('playing', pos=\"n\")\nword_lemma_a = lemmmatizer.lemmatize('playing', pos=\"a\")\nword_lemma_r = lemmmatizer.lemmatize('playing', pos=\"r\")","06963c8d":"print(word_lemma_verb)\nprint(word_lemma_noun)\nprint(word_lemma_a)\n#print(word_lemma_r)","68a5e3d2":"print(stemmer.stem('calculates'))\nprint(lemmmatizer.lemmatize('calculates'))\n\nprint(stemmer.stem('purple'))\nprint(lemmmatizer.lemmatize('purple'))","e9f96552":"from nltk import pos_tag\nfrom nltk import RegexpParser","66ef92f5":"text = \"learn NLP from kaggle kernel and make implementation easy\".split()\nprint(\"after split: \",text)","bb6d072c":"tokens_tag = pos_tag(text)\nprint(\"after token: \",tokens_tag)","f565bf98":"patterns = \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\nchunker = RegexpParser(patterns)","afdd3152":"output = chunker.parse(tokens_tag)\n\nprint(\"After Chunking: \",output)","01157e26":"text = \"Temperature of NewYork\"\ntokens = nltk.word_tokenize(text)\ntag = nltk.pos_tag(tokens)\n\ngrammar = \"NP: {<DT>?<JJ>*<NN>}\"\ncp = nltk.RegexpParser(grammar)\nresult = cp.parse(tag)","13e4fbfd":"print(result)","93e369d9":"!pip install svgling\nimport svgling","6aa85718":"svgling.draw_tree(result)","8770ffcc":"from nltk import load_parser\ncp = load_parser('grammars\/book_grammars\/sql0.fcfg')","2fe47b44":"query = 'What cities are located in China'\n\n# parsing the above statement to meaningful format(parse a query into SQL)\ntrees = list(cp.parse(query.split()))\n\nanswer = trees[0].label()['SEM']\nanswer = [s for s in answer  if s]\nq = ' '.join(answer)\nprint(q)","02510a3c":"from nltk.sem import chat80","3cd1a76c":"rows = chat80.sql_query('corpora\/city_database\/city.db',q)\nfor r in rows:\n    print(r[0], end=\" \")","e102bccd":"read_dexpr = nltk.sem.DrtExpression.fromstring","ce63089c":"drs1 = read_dexpr('([x,y], [angus(x), dog(y), own(x,y)])')\ndrs2 = read_dexpr('([u,z] ,[PRO(u), irene(z), bite(u,z)])')","b96f5738":"drs_final = drs1 + drs2\nprint(drs_final.simplify().resolve_anaphora())","5fd08bed":"from sklearn.feature_extraction.text import CountVectorizer","2fe63581":"vectorizer = CountVectorizer()\n\ndata = ['Kaggle is best place to learn data science. Kaggle also provide oppourtunity to compete.']\n\nvocabulary = vectorizer.fit(data)\nx = vectorizer.transform(data)\n\nprint(x.toarray())\nprint(vocabulary.get_feature_names())","a16b647e":"import gensim\nfrom nltk.corpus import abc","75b4086f":"model = gensim.models.Word2Vec(abc.sents())\n\nX = list(model.wv.vocab)\ndata = model.most_similar('computer')\n\nprint(data)","b7c8cb86":"# importing all necessary modules\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nimport warnings \n\nwarnings.filterwarnings(action = 'ignore')","30f24979":"sample = open('..\/input\/test-text\/test_text\/alice.txt')\ns = sample.read()\n\n#replace escape character with space\nf = s.replace(\"\\n\", \" \")","ed8055e0":"data = []\n\n#iterate through each sentence in the file\nfor i in sent_tokenize(f):\n    temp = []\n    \n    #tokenize the sentence into words\n    for j in word_tokenize(i):\n        temp.append(j.lower())\n    data.append(temp)","65dbeae7":"#Create CBOW model\nmodel1 = gensim.models.Word2Vec(data, min_count=1, size=100, window=5)\nmodel1['alice']","ddfa0e9c":"#create the Skip-Gram model\nmodel2 = gensim.models.Word2Vec(data, min_count=1, size=100, window=5, sg=1)\nmodel2['alice']","a244c9a2":"#similarity between words using CBOW\nprint(\"CBOW Cosine similarity between alica and wonderland {}\".format(model1.similarity('alice','wonderland')))\nprint(\"CBOW Cosine similarity between pool and tears {}\".format(model1.similarity('pool','tears')))\n\nprint('\\n')\n\n#similarity between words using skip-gram\nprint(\"SKIPGRAM Cosine similarity between alica and wonderland {}\".format(model2.similarity('alice','wonderland')))\nprint(\"SKIPGRAM Cosine similarity between pool and tears {}\".format(model2.similarity('pool','tears')))","3db76fee":"model1.similar_by_word('caterpillar')","e838f9fa":"from sklearn.manifold import TSNE\n\ndef tsne_plot(model):\n    labels = []\n    tokens = []\n    \n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n        \n    \n    tsne_model = TSNE(perplexity=40, n_components=3, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n    \n    x = []\n    y = []\n    z = []\n    \n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        z.append(value[2])\n    \n    return x,y,z,labels  ","aef342c2":"x,y,z,labels = tsne_plot(model2)","f58c521d":"#make dataframe of x,y,z,words\nimport pandas as pd\n\ndf = pd.DataFrame()\n\ndf['pos1'] = x\ndf['pos2'] = y\ndf['pos3'] = z\ndf['pos_avg'] = (df['pos1']+df['pos2']+df['pos3'])\/3\ndf['word'] = labels","7fc12755":"import plotly.express as px\n\n\n#requires huge memory, so may not load if more words visualize in 3d scatter plot\n# 2d scatter plot is able to visualize whole words\nfig = px.scatter_3d(df[:500], x=\"pos1\",y=\"pos2\", z= \"pos3\", text=\"word\",color='pos_avg')\n\nfig.update_traces(\n    marker_coloraxis = None,\n)\n\nfig.update_layout(\n    title_text = 'Visualization of Document',\n    height=1000\n)\n\nfig.show()","c9aef53c":"# Semantic and Pragmatic Analysis","3fe4d519":"**Semantic analysis** is a larger term, meaning to analyse the meaning contained within text. It looks for relationships among the words, how they are combined and how often certain words appear together.\n\n**To gain a deeper insight into your text, you could read about topics such as:**\n\n* Semantic Analysis in general might refer to your starting point, where you parse a sentence to understand and label the various parts of speech (POS). A tool for this in Python is spaCy, which words very nicely and also provides visualisations to show to your boss.\n\n* Named Entity Recognition (NER) - finding parts of speech (POS) that refer to an entity and linking them to pronouns appearing later in the text. An example is to distinguish between Apple the company, and apple the fruit.\n\n* Embeddings - finding latent representation of individual words e.g. using Word2Vec. Text is processed to produce a single embedding for individual words in the form of an n-dimensional vector. You can then compute similarity measures (e.g. cosine similarity) between the vectors for certain words to analyse how they are related.\n\n* Lemmatisation - this method reduces many forms of words to their base forms, which means they appear more regularly and we don't consider e.g. verb conjugations as separate words. As an example, tracking, tracked, tracker, might all be reduced to the base form: track.\n\n**Python NLP libraries found to be the most useful.**\n**Do I need to learn every library below?**\n\nNo, it all depends on your use case. Here\u2019s a summary:\n\n* We recommend NLTK only as an education and research tool. Its modularized structure makes it excellent for learning and exploring NLP concepts, but it\u2019s not meant for production.\n\n* TextBlob is built on top of NLTK, and it\u2019s more easily-accessible. This is our favorite library for fast-prototyping or building applications that don\u2019t require highly optimized performance. Beginners should start here.\n    \n* Stanford\u2019s CoreNLP is a Java library with Python wrappers. It\u2019s in many existing production systems due to its speed.\n    \n* SpaCy is a new NLP library that\u2019s designed to be fast, streamlined, and production-ready. It\u2019s not as widely adopted, but if you\u2019re building a new application, you should give it a try.\n    \n* Gensim is most commonly used for topic modeling and similarity detection. It\u2019s not a general-purpose NLP library, but for the tasks it does handle, it does them well.\n","fc87e2ff":"# Get Synonyms From WordNet\n\n* If you remember we installed NLTK packages using nltk.download(). One of the packages was WordNet. WordNet is a database built for natural language processing. It includes groups of synonyms and a brief definition.\n\n* You can get these definitions and examples for a given word like this:","426b6820":"**From the graph, you can be sure that this article is talking about PHP. Great! There are some words like \"the,\" \"of,\" \"a,\" \"an,\" and so on. These words are stop words. Generally, stop words should be removed to prevent them from affecting our results.**","0afb4d08":"# Count Word Frequency\n\n**There is a function in NLTK called FreqDist() that does the job**","0842387f":"Cosine similarity is used to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.\n\n<img src=\"https:\/\/miro.medium.com\/max\/1394\/1*_Bf9goaALQrS_0XkBozEiQ.png\">","76e08590":"**There are many more: **\n\n![image.png](attachment:image.png)","576f591e":"**Shortcoming of Bag of Words method**\n\n* It ignores the order of the word,\n* It ignores the context of words.\n\n**How Word2vec works?**\n\n* Word2vec learns word by predicting its surrounding context.\n \nFor example, let us take the word \"**He loves Football.**\"\n\nWe want to calculate the word2vec for the word: loves.\nSuppose \n\n    > loves =  Vin. P(Vout \/ Vin) is calculated\t\n    > where,\t\n    > Vin is the input word. \t\n    > P is the probability of likelihood.\t\n    > Vout is the output word. \n    \n* Word loves moves over each word in the corpus. Syntactic as well as the Semantic relationship between words is encoded. This helps in finding similar and analogies words.\n\n* All random features of the word loves is calculated. These features are changed or update concerning neighbor or context words with the help of a back propagation method.\n\n* Another way of learning is that if the context of two words are similar or two words have similar features, then such words are related. ","3116ac73":"# Splitting text into tokens","344db917":"A **bag-of-words** is a representation of text that describes the occurrence of words within a document. It involves two things: A vocabulary of known words. A measure of the presence of known words.","53b4c0ce":"<img src=\"https:\/\/www.guru99.com\/images\/1\/122118_0534_NLPNaturalL3.png\" >","dbe01509":"**Word2vec Architecture**\n\nThere are two architectures used by word2vec\n\n* Continuous Bag of words (CBOW)\n  \n> In CBOW, the current word is predicted using the window of surrounding context windows. For example, if wi-1,wi-2,wi+1,wi+2are given words or context, this model will provide wi  \n\n* skip gram \n\n> Skip-Gram performs opposite of CBOW which implies that it predicts the given sequence or context from the word. You can reverse the example to understand it. If wi is given, this will predict the context or wi-1,wi-2,wi+1,wi+2. ","4d4f31e6":"**Tokenize Text Using NLTK**\n\n* We just saw how to split the text into tokens using the split function. Now, we will see how to tokenize the text using NLTK. Tokenizing text is important since text can't be processed without tokenization. Tokenization process means splitting bigger parts to small parts.\n \n* You can tokenize paragraphs to sentences and tokenize sentences to words according to your needs. NLTK is shipped with a sentence tokenizer and a word tokenizer.\n\n* Let's assume that we have a sample text like the following:\n\n* To tokenize this text to sentences, we will use sentence tokenizer:\n","ef5b7521":"# Stemming and Lemmatization Difference\n\nOK, let's try stemming and lemmatization for some words:","3c0fe512":"The result might end up with a synonym or a different word with the same meaning. Sometimes, if you try to lemmatize a word like the word playing, it will end up with the same word. This is because the default part of speech is nouns. To get verbs, you should specify it like this:","7a43b426":"**Steps Involved:**\n\n* Tokenize text (word_tokenize)\n* apply pos_tag to above step that is nltk.pos_tag(tokenize_text) ","10ce44a0":"# Semantic Analysis\n\n* Semantic Analysis is a structure created by the syntactic analyzer which assigns meanings. This component transfers linear sequences of words into structures. It shows how the words are associated with each other.\n \n* Semantics focuses only on the literal meaning of words, phrases, and sentences. This only abstracts the dictionary meaning or the real meaning from the given context. The structures assigned by the syntactic analyzer always have assigned meaning\n \n* E.g.. \"colorless green idea.\" This would be rejected by the Semantic analysis as colorless Here; green doesn't make any sense. ","3133728d":"Actually, this is a very good level of text compression. You end up with about 50% to 60% compression. The result could be a verb, noun, adjective, or adverb:","085222e1":"There are some other stemming algorithms, like the Lancaster stemming algorithm. The output of this algorithm shows a bit different results for few words.","4d19c6e8":"# Visualizing frequency distribution","f37a2158":"# Pragmatic Analysis\n\n* Pragmatic Analysis deals with the overall communicative and social content and its effect on interpretation. It means abstracting or deriving the meaningful use of language in situations. In this analysis, the main focus always on what was said in reinterpreted on what is meant.\n\n* Pragmatic analysis helps users to discover this intended effect by applying a set of rules that characterize cooperative dialogues.\n\n* E.g., \"close the window?\" should be interpreted as a request instead of an order. ","e80101e0":"# Discourse Semantics\n\n**Discourse Representation Theory**\n\n* A discourse is a sequence of sentences. Very often, the interpretation of a sentence in a discourse depends what preceded it. A clear example of this comes from anaphoric pronouns, such as he, she and it. Given discourse such as Angus used to have a dog. But he recently disappeared., you will probably interpret he as referring to Angus's dog. However, in Angus used to have a dog. He took him for walks in New Town., you are more likely to interpret he as referring to Angus himself.\n\na. Angus owns a dog. It bit Irene.\n\nb. \u2203x.(dog(x) \u2227 own(Angus, x) \u2227 bite(x, Irene))\n\n* That is, the NP a dog acts like a quantifier which binds the it in the second sentence. Discourse Representation Theory (DRT) was developed with the specific goal of providing a means for handling this and other semantic phenomena which seem to be characteristic of discourse. \n\n* A discourse representation structure (DRS) presents the meaning of discourse in terms of a list of discourse referents and a list of conditions. The discourse referents are the things under discussion in the discourse, and they correspond to the individual variables of first-order logic. \n\n* The DRS conditions apply to those discourse referents, and correspond to atomic open formulas of first-order logic.\n\n<img src=\"https:\/\/www.nltk.org\/images\/drs1.png\" >","adcbe823":"**Why Word2vec?**\n\n* Before Word Embedding\n  * Approach for Latent Semantic Analysis\n    \n>     This is the approach which was used before word embedding. It used the concept of Bag of words where words are represented in the form of encoded vectors. It is a sparse vector representation where the dimension is equal to the size of vocabulary. If the word occurs in the dictionary, it is counted, else not.","bdae0521":"<img src=\"https:\/\/pediaa.com\/wp-content\/uploads\/2018\/08\/Difference-Between-Semantics-and-Pragmatics_Figure-1.png\" >","66e01adb":"* From the graph, we can conclude that \"Temperature\" and \"NewYork\" are two different tokens but are categorized as Noun Phrase whereas token \"of\" does not belong to Noun Phrase.\n\n* Chunking is used to categorize different tokens into the same chunk. The result will depend on grammar which has been selected. Further chunking is used to tag patterns and to explore text corpora. ","fc0f056e":"POS tagger is used to assign grammatical information of each word of the sentence.","574b8ee8":"# Lemmatizing Words Using WordNet\n\nWord lemmatizing is similar to stemming, but the difference is the result of lemmatizing is a real word. Unlike stemming, when you try to stem some words, it will result in something like this:","d86d2eb8":"# NLTK Word Stemming\n\n* Word stemming means removing affixes from words and returning the root word. (The stem of the word working is work.) Search engines use this technique when indexing pages, so many people write different versions for the same word and all of them are stemmed to the root word.\n\n* There are many algorithms for stemming, but the most used algorithm is the Porter stemming algorithm. NLTK has a class called PorterStemmer that uses this algorithm.","59052482":"# Compute Similarities between words","1469d674":"# Morphological and Lexical Analysis","065e1d8e":"**The conclusion from the above example: \"make\" is a verb which is not included in the rule, so it is not tagged as mychunk **","07361bf1":"**The word Mr. is one word, as expected. NLTK uses PunktSentenceTokenizer, which is a part of the nltk.tokenize.punkt module. This tokenizer is trained well to work with many languages**","661b2ed9":"* The **skipgram** model learns to predict a target word thanks to a nearby word. On the other hand, the **CBOW** model predicts the target word according to its context. The context is represented as a bag of the words contained in a fixed size window around the target word.","2c461c7d":"**This is just the basic overview. Checkout the references for more information**","e8bcd9ff":"# Chunking\n\nChunking is used to add more structure to the sentence by following parts of speech (POS) tagging. It is also known as shallow parsing. The resulted group of words is called \"chunks.\" In shallow parsing, there is maximum one level between roots and leaves while deep parsing comprises of more than one level. Shallow Parsing is also called light parsing or chunking.\n\nThe primary usage of chunking is to make a group of \"noun phrases.\" The parts of speech are combined with regular expressions.\n\n**Rules for Chunking:**\n\nThere are no pre-defined rules, but you can combine them according to need and requirement. ","20754619":"# POS (Part-Of-Speech) Tagging & Chunking \n\n* Parts of speech Tagging is responsible for reading the text in a language and assigning some specific token (Parts of Speech) to each word.\n\n* e.g.Input: Everything to permit us.\n\nOutput: [('Everything', NN),('to', TO), ('permit', VB), ('us', PRP)] ","cecc01e6":"# Notes:\n \n* **Corpora** is the plural for corpus.\n \n* **Corpus** basically means a body, and in the context of Natural Language Processing (NLP), it means a body of text. \n \n* **Lexicon** is a vocabulary, a list of words, a dictionary.\n \n* In NLTK, any lexicon is considered a corpus since a list of words is also a body of text. E.g. a list of stopwords can be found in NLTK corpus API.\n\n\n* **Difference Between Semantics and Pragmatics**\n * Semantics is simply the branch of linguistics that concerns studying the meanings of words as well as their meanings within a sentence.\n \n * Pragmatics is another branch of linguistics. Similar to semantics, pragmatics also studies the meanings of words, but it pays emphasis on their context.\n \n * The **main difference** between **semantics** and **pragmatics** is that the semantics studies the meaning of words and their meaning within sentences whereas the pragmatics studies the same words and meanings but with emphasis on their context as well. Both semantics and pragmatics are two main branches of study in linguistics.\n \n * Semantics is the study of words and their meanings in a language while pragmatics is the study of words and their meaning in a language with concern to their context\n \n \n* **Similarities Between Semantics and Pragmatics**\n\n    * Both semantics and pragmatics are main branches of linguistics.\n    * Semantics and pragmatics both basically focus on studying the meanings of words in a language.\n    \n    \n* **As the Linguist Jenny Thomas points out, pragmatics considers three basic principles:**\n  \n  * The negotiation of meaning between speaker and listener.\n  * The context of the utterance.\n  * The meaning potential of an utterance.","6c5a6630":"# Visualizing Word Vectors using t-SNE","c2120f8a":"# Analyzing the Meaning of Sentences\n\n","85036c6d":"# References:\n\n* https:\/\/dzone.com\/articles\/nlp-tutorial-using-python-nltk-simple-examples?fbclid=IwAR1wBESVlkEiRfVMyR5isXtZ7G0QNqO49Q5r0uXkWsMuZNF7tJQ7NRGIWSk\n\n* https:\/\/www.guru99.com\/nlp-tutorial.html\n\n* https:\/\/www.guru99.com\/pos-tagging-chunking-nltk.html\n\n* https:\/\/stackoverflow.com\/questions\/31526644\/what-is-the-difference-between-corpus-and-lexicon-in-nltk-python\n\n* https:\/\/www.brighthubeducation.com\/english-homework-help\/105856-understanding-pragmatic-vs-semantic-meaning\/\n\n* https:\/\/pediaa.com\/difference-between-semantics-and-pragmatics\/\n\n* https:\/\/datascience.stackexchange.com\/questions\/37025\/nlp-how-to-perform-semantic-analysis\n\n* https:\/\/elitedatascience.com\/python-nlp-libraries\n\n* https:\/\/www.nltk.org\/book\/ch10.html\n\n* https:\/\/www.geeksforgeeks.org\/python-word-embedding-using-word2vec\/\n* https:\/\/towardsdatascience.com\/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92","d40545ad":"# Remove Stop Words Using NLTK","77d90e4c":"**Execute the query over the database city.db and retrieve some results**","a31e5956":"# Syntax analysis\n\n* The words are commonly accepted as being the smallest units of syntax. The syntax refers to the principles and rules that govern the sentence structure of any individual languages.\n\n* Syntax focus about the proper ordering of words which can affect its meaning. This involves analysis of the words in a sentence by following the grammatical structure of the sentence. The words are transformed into the structure to show hows the word are related to each other. ","e2663d44":"**You can use WordNet to get synonymous words like this:**","fb91980c":"# Chunking\n\n* Chunking is used to add more structure to the sentence by following parts of speech (POS) tagging. It is also known as shallow parsing. The resulted group of words is called \"chunks.\" In shallow parsing, there is maximum one level between roots and leaves while deep parsing comprises of more than one level. Shallow Parsing is also called light parsing or chunking.\n \n* The primary usage of chunking is to make a group of \"noun phrases.\" The parts of speech are combined with regular expressions.\n\n**Rules for Chunking: **\n* There are no pre-defined rules, but you can combine them according to need and requirement. ","d07bb63a":"**Natural Language Understanding**\n\n* Querying a Database","0aebef56":"# Tokenize Text Using NLTK","98d1d6c2":"![image.png](attachment:image.png)","d5bd0327":"# Morphological and Lexical Analysis\n\n* Lexical analysis is a vocabulary that includes its words and expressions. It depicts analyzing, identifying and description of the structure of words. It includes dividing a text into paragraphs, words and the sentences\n\n* Individual words are analyzed into their components, and nonword tokens such as punctuations are separated from the words. ","336c679e":"# Syntax analysis","53c84f8b":"# CBOW (Continuous Bag of Words)\n\n* **CBOW: ** model predicts the current word given context words within specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent current word present at the output layer.\n\n<img src=\"https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/cbow-1.png\" >\n\n* **Skip Gram** : Skip gram predicts the surrounding context words within specific window given current word. The input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer.\n\n<img src=\"https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/skip_gram.png\" >\n\n* The basic idea of word embedding is words that occur in similar context tend to be closer to each other in vector space. ","171a290d":"* Stemming works on words without knowing their context, which is why it has lower accuracy and is faster than lemmatization.\n\n* In my opinion, lemmatizing is better than stemming. Word lemmatizing returns a real word even if it's not the same word; it could be a synonym, but at least it's a real word. Sometimes, you don't care about this level of accuracy, and all you need is speed. In this case, stemming is better.","851a9b09":"# Use Case of Chunking\n\n* Chunking is used for entity detection. An entity is that part of the sentence by which machine get the value for any intention\n \n* Example: \n>     Temperature of New York. \n>     Here Temperature is the intention and New York is an entity. \n \n* In other words, chunking is used as selecting the subsets of tokens. ","401128a8":"# Tokenize Non-English Languages Text\n\nTo tokenize other languages, you can specify the language like this","a5c5bb22":"**WordNet includes a lot of definitions:**","c63908de":"# Five main Component of Natural Language processing are:\n\n* Morphological and Lexical Analysis\n* Syntactic Analysis\n* Semantic Analysis\n* Discourse Integration\n* Pragmatic Analysis","86952359":"# Discourse Integration\n\n* It means a sense of the context. The meaning of any single sentence which depends upon that sentences. It also considers the meaning of the following sentence.\n\n* For example, the word \"that\" in the sentence \"He wanted that\" depends upon the prior discourse context. ","b3ae5a6c":"**Let's try the word tokenizer to see how it will work**","394ae350":"# Word Embedding: word2vec\n\n**What is Word Embedding?**\n\n* Word Embedding is a type of word representation that allows words with similar meaning to be understood by machine learning algorithms. Technically speaking, it is a mapping of words into vectors of real numbers using the neural network, probabilistic model, or dimension reduction on word co-occurrence matrix. It is language modeling and feature learning technique. Word embedding is a way to perform mapping using a neural network. There are various word embedding models available such as word2vec (Google), Glove (Stanford) and fastest (Facebook). \n\n* Word Embedding is also called as distributed semantic model or distributed represented or semantic vector space or vector space model. \n\n* As you read these names, you come across the word semantic which means categorizing similar words together. \n\n* For example fruits like apple, mango, banana should be placed close whereas books will be far away from these words. \n\n* In a broader sense, word embedding will create the vector of fruits which will be placed far away from vector representation of books. \n\n**What is word2vec? **\n\n* Word2vec is the technique\/model to produce word embedding for better word representation. It captures a large number of precise syntactic and semantic word relationship. It is a shallow two-layered neural network. Before going further, please see the difference between shallow and deep neural network:\n\n* The shallow neural network consists of the only a hidden layer between input and output whereas deep neural network contains multiple hidden layers between input and output. Input is subjected to nodes whereas the hidden layer, as well as the output layer, contains neurons. \n\n* word2vec is a two-layer network where there is input one hidden layer and output.\n\n**What word2vec does?**\n\n* Word2vec represents words in vector space representation. Words are represented in the form of vectors and placement is done in such a way that similar meaning words appear together and dissimilar words are located far away. This is also termed as a semantic relationship. Neural networks do not understand text instead they understand only numbers. Word Embedding provides a way to convert text to a numeric vector.\n\n* Word2vec reconstructs the linguistic context of words."}}