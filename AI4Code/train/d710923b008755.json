{"cell_type":{"4b58b60b":"code","63a68cb2":"code","1c836c20":"code","9370f1e2":"code","d1f36f31":"code","59611314":"code","8ed9253b":"code","c68e1cb1":"code","a16a9586":"code","be021853":"code","c78fc64d":"code","29ebbbb9":"code","dd7bae41":"code","29a40860":"code","82c92998":"code","41bd611e":"code","9d8aa110":"code","dab4c357":"code","4f0a5238":"code","4f65f4d5":"code","2ccf696b":"markdown","3d801acd":"markdown","577558ac":"markdown","6dbed908":"markdown","2056259d":"markdown","d199a09e":"markdown","bb4e038b":"markdown","a12b0664":"markdown","f069fc7b":"markdown","0b580e11":"markdown","73f6e580":"markdown","5d800df7":"markdown","5766f6a7":"markdown","338ced60":"markdown","50adadca":"markdown","c6542cad":"markdown","133842b1":"markdown","ab8d632c":"markdown","d250e846":"markdown","00b45c4c":"markdown","f3f5596f":"markdown","f9456da7":"markdown"},"source":{"4b58b60b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n%matplotlib inline","63a68cb2":"df = pd.read_csv(\n     filepath_or_buffer='https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/iris\/iris.data', \n     header=None, \n     sep=',')\ndf.columns=['sepal_length(cm)', 'sepal_width(cm)', 'petal_length(cm)', 'petal_width(cm)', 'class']","1c836c20":"print(df.isnull().values.any())","9370f1e2":"df.tail()","d1f36f31":"df.boxplot(by=\"class\", layout=(2, 2), figsize=(14, 14))","59611314":"# Separate the Target column that is the class column values in y array and rest of the values of the \n# independent features in X array variables as below.\n\nX = df.iloc[:,0:4].values\ny = df.iloc[:,4].values","8ed9253b":"X.shape, y.shape","c68e1cb1":"X_std = StandardScaler().fit_transform(X)","a16a9586":"#mean_vec = np.mean(X_std, axis=0)\n#cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) \/ (X_std.shape[0]-1)\n\n\"\"\"\nAnother way to implement Cov-matrix: \n\nApproach 1 -\n\ncov_mat = (X.T @ X) \/ (X.shape[0] - 1)\n\n--------\nApproach 2 -\n\ncov_mat= np.cov(X_std, rowvar=False)\n\n\"\"\"","be021853":"print('Covariance matrix \\n')\n\ncov_mat = np.cov(X_std.T)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\nprint('Eigenvectors \\n%s' %cov_mat)\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)","c78fc64d":"sq_eig=[]\nfor i in eig_vecs:\n    sq_eig.append(i**2)\nprint(sq_eig)\nsum(sq_eig)\nprint(\"\\nsum of squares of each values in an  eigen vector is \\n\", 0.27287211+ 0.13862096+0.51986524+ 0.06864169)\nfor ev in eig_vecs:\n    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))","29ebbbb9":"#Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\nprint(type(eig_pairs))\n#Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort()\neig_pairs.reverse()\nprint(\"\\n\",eig_pairs)\n#Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('\\n\\n\\nEigenvalues in descending order:')\nsorted_eigen_values = []\nsorted_eigen_vectors = []\n\nfor i,k in zip(eig_pairs,range(len(eig_pairs))):\n    print(i[0])\n    sorted_eigen_values.append(i[0])\n    sorted_eigen_vectors.append(list(eig_pairs[k][1]))\n    \nsorted_eigen_vectors","dd7bae41":"tot = sum(eig_vals)\nprint(\"\\n\",tot)\nvar_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)]\nprint(\"\\n\\n1. Variance Explained\\n\",var_exp)\ncum_var_exp = np.cumsum(var_exp)\nprint(\"\\n\\n2. Cumulative Variance Explained\\n\",cum_var_exp)\nprint(\"\\n\\n3. Percentage of variance the first two principal components each contain\\n \",var_exp[0:2])\nprint(\"\\n\\n4. Percentage of variance the first two principal components together contain\\n\",sum(var_exp[0:2]))\n","29a40860":"xint = range(1, len(cum_var_exp) + 1)\nplt.plot(xint, cum_var_exp)\n\nplt.xlabel(\"Number of components\")\nplt.ylabel(\"Cumulative explained variance\")\nplt.xticks(xint)\nplt.xlim(1, 4, 1)","82c92998":"print(eig_pairs[0][1])\nprint(eig_pairs[1][1])\nmatrix_w = np.hstack((eig_pairs[0][1].reshape(4,1), \n                      eig_pairs[1][1].reshape(4,1)))\n#hstack: Stacks arrays in sequence horizontally (column wise).\nprint('Matrix W:\\n', matrix_w)","41bd611e":"Y = X_std.dot(matrix_w)\nprincipalDf = pd.DataFrame(data = Y\n          , columns = ['principal component 1', 'principal component 2'])\nprincipalDf.head()","9d8aa110":"finalDf = pd.concat([principalDf,pd.DataFrame(y,columns = ['species'])], axis = 1)\nfinalDf.head()","dab4c357":"fig = plt.figure(figsize = (8,5))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 Component PCA', fontsize = 20)\ntargets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\ncolors = ['r', 'b', 'y']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['species'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","4f0a5238":"pca = PCA(n_components=2) # Here we can also give the percentage as a paramter to the PCA function as pca = PCA(.95). .95 means that we want to include 95% of the variance. Hence PCA will return the no of components which describe 95% of the variance. However we know from above computation that 2 components are enough so we have passed the 2 components.\nprincipalComponents = pca.fit_transform(X_std) \nprincipalDf = pd.DataFrame(data = principalComponents\n              , columns = ['principal component 1', 'principal component 2']) \nprincipalDf.head(5) # prints the top 5 rows","4f65f4d5":"finalDf = pd.concat([principalDf, finalDf[['species']]], axis = 1)\nfinalDf.head(5)","2ccf696b":"<a id=\"section-3\"><\/a>\n## 3.a. Compute Eigen Values and Eigen Vectors:\n\n#### Here I am using numpy array to calculate Eigenvectors and Eigenvalues of the standardized feature space values.","3d801acd":"## Importing the Libraries.","577558ac":"## 3.b. Eigen Vectors verification:\n\n#### As we know that sum of square of each value in an Eigenvector is 1. So let\u2019s see if it holds true which mean we have computed Eigenvectors correctly.","6dbed908":"# PCA : Principal Component Analysis -\n\n>\n***Principal component analysis (PCA)* is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance. Finding such new variables, the principal components, reduces to solving an eigenvalue\/eigenvector problem.**","2056259d":"#### Compiling into a fresh DataFrame.","d199a09e":"#### From the plot, we can see that over 95% of the variance is captured within the two largest principal components. Therefore, it is acceptable to choose the first two largest principal components to make up the projection matrix W.","bb4e038b":"![PCA](https:\/\/ashutoshtripathicom.files.wordpress.com\/2019\/07\/pca_title-1.jpg)","a12b0664":"## Table Of Contents:\n\n>\n>### Steps Involved in PCA:\n>>\n>>**1. [Standardize the data. (with mean =0 and variance = 1)](#section-1)**\n\n>>**2. [Compute covariance matrix of dimensions.](#section-2)**\n\n>>**3. [Obtain the Eigenvectors and Eigenvalues from the covariance matrix](#section-3) (we can also use correlation matrix or even Single value decomposition, covered in seperate kernels).**\n\n>>**4. [Sort eigenvalues in descending order and choose the top k Eigenvectors](#section-4) that correspond to the k largest eigenvalues (k will become the number of dimensions of the new feature subspace k\u2264d, d is the number of original dimensions)**.\n\n>>**5. [Construct the projection matrix W](#section-5) from the selected k Eigenvectors.**\n\n>>**6. Transform the original data set X via W to [obtain the new k-dimensional feature subspace Y](#section-6).**\n\n>>**7. [Visualise](#section-7) the newly generated high variance features ( We are doing it here for 2 PCs.)**\n>>\n>### **Bonus :** [Doing it all with the Python Library.](#section-8)\n>\n>### [Conclusion.](#conclusion)\n","f069fc7b":"## Visualizing the Target Classes on the axes of the 2 new PCs - PC1 and PC2.","0b580e11":"<a id=\"conclusion\"><\/a>\n**Note:** As we can see, we have achieved the similar results with the Python library as well.\n\n**Conclusion:** Together, the first two principal components contain 95.80% of the information. The first principal component contains 72.77% of the variance and the second principal component contains 23.03% of the variance. The third and fourth principal component contained the rest of the variance of the data set.","73f6e580":"<a id=\"section-6\"><\/a>\n## 6. Projection Onto the New Feature Space:\n\n#### In this last step we will use the 4\u00d72-dimensional projection matrix W to transform our samples onto the new subspace via the equation Y=X\u00d7W, where the output matrix Y will be a 150\u00d72 matrix of our transformed samples.","5d800df7":"# THANK YOU!","5766f6a7":"<a id=\"section-2\"><\/a>\n## 2. Covariance Matrix :\n\n#### The classic approach to PCA is to perform the Eigen decomposition on the covariance matrix \u03a3, which is a d\u00d7d matrix where each element represents the covariance between two features. Note d is the number of original dimensions of the data set.\n","338ced60":"#### The boxplots show us a number of details such as virginica having the largest median petal length.There are barely a few outliers in the data.","50adadca":"<a id=\"section-7\"><\/a>\n## **Bonus:** Doing this with the Python Library.","c6542cad":"#### data set is now stored in the form of a 150\u00d74 matrix where the columns are the different features, and every row represents a separate flower sample.\n\n\n#### Each sample row x can be pictured as a 4-dimensional vector as we can see in the above screenshot of x output values.","133842b1":"Ref to read a comprehensive article on the subject : https:\/\/towardsdatascience.com\/a-step-by-step-introduction-to-pca-c0d78e26a0dd","ab8d632c":"<a id=\"section-1\"><\/a>\n## 1. Standardisation (Normalise the data) :\n\n#### When there are different scales used for the measurement of the values of the features, then it is advisable to do the standardization to bring all the feature spaces with mean = 0 and variance = 1.\n\n#### The reason why standardization is very much needed before performing PCA is that PCA is very sensitive to variances. Meaning, if there are large differences between the scales (ranges) of the features, then those with larger scales will dominate over those with the small scales.\n\n\n![formula](https:\/\/ashutoshtripathicom.files.wordpress.com\/2019\/07\/standardization-1.png)","d250e846":"<a id=\"section-4\"><\/a>\n## 4. Selecting the K Principal Components:\n\n#### how to select the new set of Principal components?\n\n#### -The rule behind is that we sort the Eigenvalues in descending order and then choose the top k features with respect to top k Eigenvalues.\n\n#### The idea here is that by choosing top k we have decided that the variance which corresponds to those k feature space is enough to describe the data set. And by losing the remaining variance of those not selected features, won\u2019t cost the accuracy much or we are OK to loose that much accuracy that costs because of neglected variance.\n\n#### So this is the decision which we have to make based on the problem set given and also based on business case. There is no perfect rule to decide it.\n\n","00b45c4c":"<a id=\"section-5\"><\/a>\n## 5. Construct the projection matrix W from the selected k eigenvectors:\n\n#### -Projection matrix will be used to transform the Iris data onto the new feature subspace or we say new transformed data set with reduced dimensions.\n\n#### -It is matrix of our concatenated top k Eigenvectors.\n#### -Here, we are reducing the 4-dimensional feature space to a 2-dimensional feature subspace, by choosing the \u201ctop 2\u201d Eigenvectors with the highest Eigenvalues to construct our d\u00d7k-dimensional Eigenvector matrix W.\n\n","f3f5596f":"## There are two major approaches to accomplish PCA :\n\n### 1. using Eigen Vector Decomposition (EVD) - Eigen Values and Eigen Vectors [covered in this kernel]\n### 2. using Singular Value Decomposition (SVD) [covered in this [Kernel](https:\/\/www.kaggle.com\/atulanandjha\/dimensionality-reduction-ii-pca-using-svd)]","f9456da7":"### -> Explained Variance :\n\n#### -A useful measure is the so-called \u201cexplained variance,\u201d which can be calculated from the eigenvalues.\n#### -The explained variance tells us how much information (variance) can be attributed to each of the principal components."}}