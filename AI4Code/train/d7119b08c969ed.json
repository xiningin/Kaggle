{"cell_type":{"903db036":"code","32d4a87f":"code","eeebf59f":"code","1c6252a0":"code","b9ca7fa9":"code","08637ec3":"code","76bad526":"code","f80de0b9":"code","c4bc2a39":"code","f57a12e9":"code","dda19cfd":"code","176fbf47":"code","02f064d6":"code","861febce":"code","564aee34":"code","334afeea":"code","76abe5b8":"code","dcb00109":"code","be30d5ee":"code","3363027b":"code","5b0b1d4a":"code","eb55d63d":"code","b99939a6":"code","19ce59f5":"code","d6f002df":"code","2b672999":"code","f185be57":"code","19bb2e3c":"code","c858feca":"code","cbc2aab0":"code","36c6d5d8":"code","7ed47eb4":"code","0a92ab74":"code","bc167e5b":"code","04e2b98d":"code","9e8cdf9e":"code","980163be":"code","5d43f2f8":"code","c04c2a66":"code","d33da6cd":"code","0bbd6988":"code","a9e508ec":"code","598a528f":"code","5e27ead6":"code","0bb122b1":"code","b4836a50":"code","9082e573":"code","e6bf8f17":"code","21417867":"code","1a4ad835":"code","bb951146":"code","c0ea35bf":"code","3704d352":"code","63e3afaf":"code","2253e36a":"code","726163e0":"code","39647346":"code","ae991629":"code","2a345ef8":"code","f7e23712":"code","782abebe":"markdown","116f5b3f":"markdown","1a954a61":"markdown","a4a82dd5":"markdown","4b446bae":"markdown","08ee39bf":"markdown","ee2576d5":"markdown","a23ab028":"markdown","db6facb2":"markdown","8bcaf8cc":"markdown","b70e0295":"markdown","f872b14d":"markdown","c795f90b":"markdown","cdcbec0b":"markdown","34ceed83":"markdown","7df6f66a":"markdown","11099ae2":"markdown"},"source":{"903db036":"# Loading packages\nimport pandas as pd #Analysis \nimport matplotlib.pyplot as plt #Visulization\nimport seaborn as sns #Visulization\nimport numpy as np #Analysis \nfrom scipy.stats import norm #Analysis \nfrom sklearn.preprocessing import StandardScaler #Analysis \nfrom scipy import stats #Analysis \nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport gc","32d4a87f":"import os\nprint(os.listdir(\"..\/input\"))","eeebf59f":"train = pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\nprint(\"shape of train : \",train.shape)","1c6252a0":"train.head()","b9ca7fa9":"test = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])\nprint(\"shape of test : \",test.shape)","08637ec3":"# Code in https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-elo\n# SRK - Simple Exploration Notebook \n\ncnt_srs = train['first_active_month'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('First active month', fontsize=12)\nplt.ylabel('Number of cards', fontsize=12)\nplt.title(\"First active month count in train set\")\nplt.show()\n\ncnt_srs = test['first_active_month'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('First active month', fontsize=12)\nplt.ylabel('Number of cards', fontsize=12)\nplt.title(\"First active month count in test set\")\nplt.show()","76bad526":"train.corr()","f80de0b9":"#histogram\nf, ax = plt.subplots(figsize=(14, 6))\nsns.distplot(train['target'])","c4bc2a39":"train['target'].describe()","f57a12e9":"fig, ((ax1, ax2)) = plt.subplots(nrows=1, ncols=2, figsize=(14,8))\n\ndata = pd.concat([train['target'], train['feature_1']], axis=1)\nfig = sns.boxplot(x='feature_1', y=\"target\", data=data,ax=ax1)\n\n# feature 1\nsns.violinplot(x=\"feature_1\", y=\"target\", data=data,ax=ax2)\nplt.xticks(rotation='vertical')\nplt.xlabel('Feature 1', fontsize=12)\nplt.ylabel('target', fontsize=12)\nplt.title(\"Feature 1 distribution\")\nplt.show()","dda19cfd":"fig, ((ax1, ax2)) = plt.subplots(nrows=1, ncols=2, figsize=(14,8))\n\ndata = pd.concat([train['target'], train['feature_2']], axis=1)\nfig = sns.boxplot(x='feature_2', y=\"target\", data=data,ax=ax1)\n\n# feature 1\nsns.violinplot(x=\"feature_2\", y=\"target\", data=data,ax=ax2)\nplt.xticks(rotation='vertical')\nplt.xlabel('Feature 2', fontsize=12)\nplt.ylabel('target', fontsize=12)\nplt.title(\"Feature 2 distribution\")\nplt.show()","176fbf47":"fig, ((ax1, ax2)) = plt.subplots(nrows=1, ncols=2, figsize=(14,8))\n\ndata = pd.concat([train['target'], train['feature_3']], axis=1)\nfig = sns.boxplot(x='feature_3', y=\"target\", data=data,ax=ax1)\n\n# feature 1\nsns.violinplot(x=\"feature_3\", y=\"target\", data=data,ax=ax2)\nplt.xticks(rotation='vertical')\nplt.xlabel('feature_3', fontsize=12)\nplt.ylabel('target', fontsize=12)\nplt.title(\"feature_3 distribution\")\nplt.show()","02f064d6":"fig, ((ax1, ax2)) = plt.subplots(nrows=1, ncols=2, figsize=(14,8))\n\nsns.boxplot(x=\"feature_1\", y=\"target\", hue=\"feature_3\",\n               data=train, palette=\"Set3\",ax=ax1)\n\nsns.boxplot(x=\"feature_3\", y=\"target\", hue=\"feature_1\",\n               data=train, palette=\"Set3\",ax=ax2)","861febce":"fig, ((ax1, ax2)) = plt.subplots(nrows=1, ncols=2, figsize=(14,8))\n\nsns.boxplot(x=\"feature_1\", y=\"target\", hue=\"feature_2\",\n               data=train, palette=\"Set3\",ax=ax1)\n\nsns.boxplot(x=\"feature_2\", y=\"target\", hue=\"feature_1\",\n               data=train, palette=\"Set3\",ax=ax2)","564aee34":"# Code in https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-elo\n# SRK - Simple Exploration Notebook \ntrain_low_m30 = train[train['target']<-30]\ncnt_srs = train_low_m30['first_active_month'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('First active month', fontsize=12)\nplt.ylabel('Number of cards', fontsize=12)\nplt.title(\"First active month count in target low than -30\")\nplt.show()","334afeea":"train_low_m30.head()","76abe5b8":"# checking missing data\ntotal = train.isnull().sum().sort_values(ascending = False)\npercent = (train.isnull().sum()\/train.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","dcb00109":"# checking missing data\ntotal = test.isnull().sum().sort_values(ascending = False)\npercent = (test.isnull().sum()\/test.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","be30d5ee":"import datetime\n\nfor df in [train,test]:\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['year'] = df['first_active_month'].dt.year\n    df['month'] = df['first_active_month'].dt.month\n    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n\ntarget = train['target']\ndel train['target']","3363027b":"train.head()","5b0b1d4a":"ht = pd.read_csv(\"..\/input\/historical_transactions.csv\")\nprint(\"shape of historical_transactions : \",ht.shape)","eb55d63d":"ht.head()","b99939a6":"temp = ht[\"authorized_flag\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\nplt.figure(figsize = (6,6))\nplt.title('authorized_flag - Y or N')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'labels', y=\"values\", data=df)\nlocs, labels = plt.xticks()\nplt.show()","19ce59f5":"temp = ht[\"category_1\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\nplt.figure(figsize = (6,6))\nplt.title('category_1 - Y or N')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'labels', y=\"values\", data=df)\nlocs, labels = plt.xticks()\nplt.show()","d6f002df":"temp = ht[\"category_3\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\nplt.figure(figsize = (6,6))\nplt.title('category_3 - A B C')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'labels', y=\"values\", data=df)\nlocs, labels = plt.xticks()\nplt.show()","2b672999":"temp = ht[\"category_2\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\nplt.figure(figsize = (6,6))\nplt.title('category_2 - 1,2,3,4,5')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'labels', y=\"values\", data=df)\nlocs, labels = plt.xticks()\nplt.show()","f185be57":"#histogram\nf, ax = plt.subplots(figsize=(14, 6))\nsns.distplot(ht['month_lag'])","19bb2e3c":"#histogram\nf, ax = plt.subplots(figsize=(14, 6))\nsns.distplot(ht['purchase_amount'])","c858feca":"#histogram\nf, ax = plt.subplots(figsize=(14, 6))\nsns.distplot(ht['installments'])","cbc2aab0":"temp = ht[\"city_id\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\nplt.figure(figsize = (18,8))\nplt.title('city_id')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'labels', y=\"values\", data=df)\nlocs, labels = plt.xticks()\nplt.show()","36c6d5d8":"ht['authorized_flag'] = ht['authorized_flag'].map({'Y':1, 'N':0})","7ed47eb4":"def aggregate_historical_transactions(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n        }\n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['hist_' + '_'.join(col).strip() \n                           for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='hist_transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n\nhistory = aggregate_historical_transactions(ht)\ndel ht\ngc.collect()","0a92ab74":"train = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')","bc167e5b":"merchant = pd.read_csv(\"..\/input\/merchants.csv\")\nprint(\"shape of merchant : \",merchant.shape)","04e2b98d":"merchant.head()","9e8cdf9e":"# checking missing data\ntotal = merchant.isnull().sum().sort_values(ascending = False)\npercent = (merchant.isnull().sum()\/merchant.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","980163be":"#histogram\nf, ax = plt.subplots(figsize=(14, 6))\nsns.distplot(merchant['numerical_1'])","5d43f2f8":"#histogram\nf, ax = plt.subplots(figsize=(14, 6))\nsns.distplot(merchant['numerical_2'])","c04c2a66":"temp = merchant[\"category_1\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\nplt.figure(figsize = (6,6))\nplt.title('category_1 in merchant')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'labels', y=\"values\", data=df)\nlocs, labels = plt.xticks()\nplt.show()","d33da6cd":"#most_recent_sales_range, most_recent_purchases_range, category_4\ntemp = merchant[\"category_2\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\nplt.figure(figsize = (6,6))\nplt.title('category_2 in merchant')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'labels', y=\"values\", data=df)\nlocs, labels = plt.xticks()\nplt.show()","0bbd6988":"#most_recent_sales_range, most_recent_purchases_range, category_4\ntemp = merchant[\"most_recent_sales_range\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\nplt.figure(figsize = (6,6))\nplt.title('most_recent_sales_range in merchant')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'labels', y=\"values\", data=df)\nlocs, labels = plt.xticks()\nplt.show()","a9e508ec":"#most_recent_sales_range, most_recent_purchases_range, category_4\ntemp = merchant[\"most_recent_purchases_range\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\nplt.figure(figsize = (6,6))\nplt.title('most_recent_purchases_range in merchant')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'labels', y=\"values\", data=df)\nlocs, labels = plt.xticks()\nplt.show()","598a528f":"#most_recent_sales_range, most_recent_purchases_range, category_4\ntemp = merchant[\"category_4\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\nplt.figure(figsize = (6,6))\nplt.title('category_4 in merchant')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'labels', y=\"values\", data=df)\nlocs, labels = plt.xticks()\nplt.show()","5e27ead6":"new_merchant = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")\nprint(\"shape of new_merchant_transactions : \",new_merchant.shape)","0bb122b1":"new_merchant.head()","b4836a50":"# checking missing data\ntotal = new_merchant.isnull().sum().sort_values(ascending = False)\npercent = (new_merchant.isnull().sum()\/new_merchant.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","9082e573":"new_merchant['authorized_flag'] = new_merchant['authorized_flag'].map({'Y':1, 'N':0})","e6bf8f17":"def aggregate_new_transactions(new_trans):    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'month_lag': ['min', 'max']\n        }\n    agg_new_trans = new_trans.groupby(['card_id']).agg(agg_func)\n    agg_new_trans.columns = ['new_' + '_'.join(col).strip() \n                           for col in agg_new_trans.columns.values]\n    agg_new_trans.reset_index(inplace=True)\n    \n    df = (new_trans.groupby('card_id')\n          .size()\n          .reset_index(name='new_transactions_count'))\n    \n    agg_new_trans = pd.merge(df, agg_new_trans, on='card_id', how='left')\n    \n    return agg_new_trans\n\nnew_trans = aggregate_new_transactions(new_merchant)","21417867":"train = pd.merge(train, new_trans, on='card_id', how='left')\ntest = pd.merge(test, new_trans, on='card_id', how='left')","1a4ad835":"use_cols = [col for col in train.columns if col not in ['card_id', 'first_active_month']]\n\ntrain = train[use_cols]\ntest = test[use_cols]\n\nfeatures = list(train[use_cols].columns)\ncategorical_feats = [col for col in features if 'feature_' in col]","bb951146":"for col in categorical_feats:\n    print(col, 'have', train[col].value_counts().shape[0], 'categories.')","c0ea35bf":"from sklearn.preprocessing import LabelEncoder\nfor col in categorical_feats:\n    print(col)\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))","3704d352":"df_all = pd.concat([train, test])\ndf_all = pd.get_dummies(df_all, columns=categorical_feats)\n\nlen_train = train.shape[0]\n\ntrain = df_all[:len_train]\ntest = df_all[len_train:]","63e3afaf":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\nlgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 11, \"min_child_samples\": 20, \n               \"reg_alpha\": 1, \"reg_lambda\": 1,\n               \"num_leaves\" : 128, \"learning_rate\" : 0.005, \n               \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \n               \"verbosity\": -1}\n\nFOLDs = KFold(n_splits=9, shuffle=True, random_state=1989)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 10000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 50)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) \/ FOLDs.n_splits\n    \n\nprint(np.sqrt(mean_squared_error(oof_lgb, target)))","2253e36a":"cols = (feature_importance_df_lgb[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df_lgb.loc[feature_importance_df_lgb.feature.isin(cols)]\n\nplt.figure(figsize=(14,14))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","726163e0":"sub_df = pd.read_csv('..\/input\/sample_submission.csv')\nsub_df[\"target\"] = predictions_lgb \nsub_df.to_csv(\"submission_lgb.csv\", index=False)","39647346":"import xgboost as xgb\n\nxgb_params = {'eta': 0.005, 'max_depth': 11, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True}\n\n\n\nFOLDs = KFold(n_splits=9, shuffle=True, random_state=1989)\n\noof_xgb = np.zeros(len(train))\npredictions_xgb = np.zeros(len(test))\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = xgb.DMatrix(data=train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=train.iloc[val_idx], label=target.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"xgb \" + str(fold_) + \"-\" * 50)\n    num_round = 10000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=50, verbose_eval=1000)\n    oof_xgb[val_idx] = xgb_model.predict(xgb.DMatrix(train.iloc[val_idx]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb += xgb_model.predict(xgb.DMatrix(test), ntree_limit=xgb_model.best_ntree_limit+50) \/ FOLDs.n_splits\n\nnp.sqrt(mean_squared_error(oof_xgb, target))","ae991629":"print('lgb', np.sqrt(mean_squared_error(oof_lgb, target)))\nprint('xgb', np.sqrt(mean_squared_error(oof_xgb, target)))","2a345ef8":"total_sum = 0.5 * oof_lgb + 0.5 * oof_xgb\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(total_sum, target)**0.5))","f7e23712":"sub_df = pd.read_csv('..\/input\/sample_submission.csv')\nsub_df[\"target\"] = 0.5 * predictions_lgb + 0.5 * predictions_xgb\nsub_df.to_csv(\"submission_ensemble.csv\", index=False)","782abebe":"- card_id\t: Card identifier\n- month_lag\t: month lag to reference date\n- purchase_date\t: Purchase date\n- authorized_flag\t: Y' if approved, 'N' if denied\n- category_3\t: anonymized category\n- installments\t: number of installments of purchase\n- category_1 : \tanonymized category\n- merchant_category_id\t: Merchant category identifier (anonymized )\n- subsector_id\t: Merchant category group identifier (anonymized )\n- merchant_id\t: Merchant identifier (anonymized)\n- purchase_amount\t: Normalized purchase amount\n- city_id\t: City identifier (anonymized )\n- state_id\t: State identifier (anonymized )\n- category_2 : anonymized category","116f5b3f":"### Make a Baseline model\nkernel : https:\/\/www.kaggle.com\/youhanlee\/hello-elo-ensemble-will-help-you","1a954a61":"### Simple Exploration : new_merchants.csv","a4a82dd5":"### Simple Exploration : merchants.csv","4b446bae":"- merchant_id : Unique merchant identifier\n- merchant_group_id\t: Merchant group (anonymized )\n- merchant_category_id\t: Unique identifier for merchant category (anonymized )\n- subsector_id\t: Merchant category group (anonymized )\n- numerical_1\t: anonymized measure\n- numerical_2\t: anonymized measure\n- category_1\t: anonymized category\n- most_recent_sales_range\t: Range of revenue (monetary units) in last active month --> A > B > C > D > E\n- most_recent_purchases_range\t: Range of quantity of transactions in last active month --> A > B > C > D > E\n- avg_sales_lag3\t: Monthly average of revenue in last 3 months divided by revenue in last active month\n- avg_purchases_lag3\t: Monthly average of transactions in last 3 months divided by transactions in last active month\n- active_months_lag3\t: Quantity of active months within last 3 months\n- avg_sales_lag6\t: Monthly average of revenue in last 6 months divided by revenue in last active month\n- avg_purchases_lag6\t: Monthly average of transactions in last 6 months divided by transactions in last active month\n- active_months_lag6\t: Quantity of active months within last 6 months\n- avg_sales_lag12\t: Monthly average of revenue in last 12 months divided by revenue in last active month\n- avg_purchases_lag12\t: Monthly average of transactions in last 12 months divided by transactions in last active month\n- active_months_lag12\t: Quantity of active months within last 12 months\n- category_4\t: anonymized category\n- city_id\t: City identifier (anonymized )\n- state_id\t: State identifier (anonymized )\n- category_2\t: anonymized category","08ee39bf":"- train.csv - the training set\n- test.csv - the test set\n- sample_submission.csv - a sample submission file in the correct format - contains all card_ids you are expected to predict for.\n- historical_transactions.csv - up to 3 months' worth of historical transactions for each card_id\n- merchants.csv - additional information about all merchants \/ merchant_ids in the dataset.\n- new_merchant_transactions.csv - two months' worth of data for each card_id containing ALL purchases that card_id made at merchant_ids that were not visited in the historical data.","ee2576d5":"### Missing value","a23ab028":"### Simple Exploration","db6facb2":"- first_active_month : ''YYYY-MM', month of first purchase\n- feature_1,2,3 : Anonymized card categorical feature\n- target : Loyalty numerical score calculated 2 months after historical and evaluation period","8bcaf8cc":"### Simple Exploration : historical_transactions","b70e0295":"feature_2 has not 3 when feature_1 == 5\nbut what is target low than -30???","f872b14d":"- card_id\t: Card identifier\n- month_lag\t: month lag to reference date\n- purchase_date\t: Purchase date\n- authorized_flag\t: Y' if approved, 'N' if denied\n- category_3\t: anonymized category\n- installments\t: number of installments of purchase\n- category_1\t: anonymized category\n- merchant_category_id\t: Merchant category identifier (anonymized )\n- subsector_id\t: Merchant category group identifier (anonymized )\n- merchant_id\t: Merchant identifier (anonymized)\n- purchase_amount\t: Normalized purchase amount\n- city_id\t: City identifier (anonymized )\n- state_id\t: State identifier (anonymized )\n- category_2\t: anonymized category\n","c795f90b":"`feature_1 - feature_3` has 0.58 but `target - feature` low correaltion value","cdcbec0b":"First_active_month of Train and Test looks similiar","34ceed83":"Data Description : Imagine being hungry in an unfamiliar part of town and getting restaurant recommendations served up, based on your personal preferences, at just the right moment. The recommendation comes with an attached discount from your credit card provider for a local place around the corner!\n\nRight now, Elo, one of the largest payment brands in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders. But do these promotions work for either the consumer or the merchant? Do customers enjoy their experience? Do merchants see repeat business? Personalization is key.\n\nElo has built machine learning models to understand the most important aspects and preferences in their customers\u2019 lifecycle, from food to shopping. But so far none of them is specifically tailored for an individual or profile. This is where you come in.\n\nIn this competition, Kagglers will develop algorithms to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty. Your input will improve customers\u2019 lives and help Elo reduce unwanted campaigns, to create the right experience for customers :)","7df6f66a":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/10445\/logos\/thumb76_76.png?t=2018-10-24-17-14-05)\n\nReference \n- https:\/\/www.kaggle.com\/youhanlee\/hello-elo-ensemble-will-help-you\n- https:\/\/www.kaggle.com\/ashishpatel26\/beginner-guide-of-elo-eda-kfold-lightgbm\n- https:\/\/www.kaggle.com\/kailex\/tidy-elo-starter-3-813\n- https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-elo","11099ae2":"feature_3 has 1 when feautre_1 high than 3"}}