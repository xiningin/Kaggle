{"cell_type":{"7eeb209a":"code","77458108":"code","b22b540c":"code","e11fbf2d":"markdown"},"source":{"7eeb209a":"!!pip install tensorflow_addons==0.9.1","77458108":"import tensorflow_addons as tfa","b22b540c":"import tensorflow as tf\nfrom tensorflow.keras.layers import *\nimport pandas as pd\nimport numpy as np\nimport random\nfrom tensorflow.keras.callbacks import Callback, LearningRateScheduler\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import losses, models, optimizers\nimport tensorflow_addons as tfa\nimport gc\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import f1_score\n\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 500)\n\nimport os\n# Any results you write to the current directory are saved as output.\nFOLD = 4 # fold 1 3 tends to be broken...\nAUG_CNT = 15\n\n# configurations and main hyperparammeters\nEPOCHS = 180\nNNBATCHSIZE = 16\nGROUP_BATCH_SIZE = 4000\nSEED = 321\nLR = 0.001\nSPLITS = 5\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \n# read data\ndef read_data():\n    train = pd.read_csv('..\/input\/data-without-drift\/train_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n    test  = pd.read_csv('..\/input\/data-without-drift\/test_clean.csv', dtype={'time': np.float32, 'signal': np.float32})\n    sub  = pd.read_csv('..\/input\/liverpool-ion-switching\/sample_submission.csv', dtype={'time': np.float32})\n\n    '''\n    oof_files = ['rf_shift_20_per4000_group_split.pkl',\n                 'wavenet_stacked_bigru_oof.pkl']\n    \n    from tqdm import tqdm\n    for o_i, oof in enumerate(tqdm(oof_files)):\n        Y_train_proba = pd.read_pickle('..\/oof\/{}'.format(oof))['train'] #np.load(\"\/kaggle\/input\/ion-shifted-rfc-proba\/Y_train_proba.npy\")\n        Y_test_proba = pd.read_pickle('..\/oof\/{}'.format(oof))['test']  #np.load(\"\/kaggle\/input\/ion-shifted-rfc-proba\/Y_test_proba.npy\")\n        \n        for i in range(11):\n            train[f\"proba_{o_i}_{i}\"] = Y_train_proba[:, i]\n            test[f\"proba_{o_i}_{i}\"] = Y_test_proba[:, i]\n    '''\n    return train, test, sub\n\n# create batches of 4000 observations\ndef batching(df, batch_size):\n    df['group'] = df.groupby(df.index\/\/batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df['group'] = df['group'].astype(np.uint16)\n    return df\n\n# normalize the data (standard scaler). We can also try other scalers for a better score!\ndef normalize(train, test):\n    train_input_mean = train.signal.mean()\n    train_input_sigma = train.signal.std()\n    train['signal'] = (train.signal - train_input_mean) \/ train_input_sigma\n    test['signal'] = (test.signal - train_input_mean) \/ train_input_sigma\n    return train, test\n\n# get lead and lags features\ndef lag_with_pct_change(df, windows):\n    for window in windows:    \n        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n    return df\n\n# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\ndef run_feat_engineering(df, batch_size):\n    # create batches\n    df = batching(df, batch_size = batch_size)\n    # create leads and lags (1, 2, 3 making them 6 features)\n\n    #df = lag_with_pct_change(df, [i for i in range(1, 3)])\n    # create signal ** 2 (this is the new feature)\n    df['signal_2'] = df['signal']** 2\n    \n    return df\n\ndef train_update_groups(df):\n    df['group_ind'] = -1\n    step_size = 500_000\n    groups = [0, 0, 1, 2, 4, 3, 1, 2, 3, 4]\n    \n    for i in range(df.shape[0]\/\/step_size):\n        a = i * step_size\n        b = (i+1) * step_size\n        df.loc[df.index[a:b], 'group_ind'] = groups[i]\n    \n    df['group_ind'] = df['group_ind'].astype(int)\n    return df\n\ndef test_update_groups(df):\n    df['group_ind'] = -1\n    step_size = 100_000\n    groups = [0, 2, 3, 0, 1, 4, 3, 4, 0, 2,\n              0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    \n    for i in range(df.shape[0]\/\/step_size):\n        a = i * step_size\n        b = (i+1) * step_size\n        df.loc[df.index[a:b], 'group_ind'] = groups[i]\n    \n    df['group_ind'] = df['group_ind'].astype(int)\n    return df\n\ndef convert_signal_into_probs(df, is_train=True, fit_dict=None):\n\n    from scipy.stats import norm\n    if is_train:\n        df['group_ind'] = train_update_groups(df.copy())['group']\n\n        filt = ((df.time>=364.229) & (df.time<382.343)) | ((df.time>=47.857) & (df.time<47.863))\n        keep_filt= ~filt\n        \n        fit_dict = {}\n        for gp in df['group_ind'].unique():\n            \n            x = df.loc[(df['group_ind'] == gp) & keep_filt, 'signal'].values\n            y = df.loc[(df['group_ind'] == gp) & keep_filt, 'open_channels'].values\n            \n            fit_dict[gp] = {}\n            fit_dict[gp]['open_channels'] = y.max()+1\n            fit_dict[gp]['per_oc_dist'] = {}\n            \n            for s in np.arange(fit_dict[gp]['open_channels']):\n                filt = y==s\n                mean_, std_ = np.mean(x[filt]), np.std(x[filt])\n                fit_dict[gp]['per_oc_dist'][s] = (mean_, std_)\n                print(\"GP = {:2d}, OC = {:2d}, Mean={:.4f}, Std={:.4f}\".format(gp, s, mean_, std_))\n                    \n    else:\n        assert fit_dict is not None, \"fit_dict should be provided when is_train is False\"\n        df['group_ind'] = test_update_groups(df.copy())['group']\n\n    \n    p_signal = np.zeros((df.shape[0], 11))\n    for gp in df['group_ind'].unique():\n        \n        filt = df['group_ind'] == gp\n        x = df.loc[filt, 'signal'].values\n            \n        for s in range(fit_dict[gp]['open_channels']):\n            dist = fit_dict[gp]['per_oc_dist'][s]\n            p_signal[filt, s] = norm.cdf(x, *dist)\n            p_signal[filt, s] = np.where(p_signal[filt, s]>0.5, 1-p_signal[filt, s], p_signal[filt, s])\n        \n        p_signal_sum = p_signal.sum(axis=1)\n        zero_sum_filt = p_signal_sum==0\n        filt_1 = (filt)&(~zero_sum_filt); p_signal[filt_1, :] = p_signal[filt_1, :] \/ p_signal_sum[filt_1].reshape((-1,1))\n        filt_2 = (filt)&(zero_sum_filt); p_signal[filt_2, :] = 1.\/(fit_dict[gp]['open_channels']-1) # for outlier signal, the prob for each channel is zero\n        \n        #print(gp, p_signal[filt])\n        \n    # remove signal column\n    df.drop(['signal', 'group_ind'], axis=1, inplace=True)\n    \n    for s in range(11):\n        df[f'sig_prob_{s}'] = p_signal[:, s]\n        \n    return df, fit_dict\n\n# fillna with the mean and select features for training\ndef feature_selection(train, test):\n    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time', 'group_ind']]\n    print(features)\n    train = train.replace([np.inf, -np.inf], np.nan)\n    test = test.replace([np.inf, -np.inf], np.nan)\n    for feature in features:\n        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n        train[feature] = train[feature].fillna(feature_mean).astype(np.float32)\n        test[feature] = test[feature].fillna(feature_mean).astype(np.float32)\n    return train, test, features\n\n# model function (very important, you can try different arquitectures to get a better score. I believe that top public leaderboard is a 1D Conv + RNN style)\n\n#####\n# wavenet + gru: cv: .940, lb: .942\n#####\ndef Classifier_Wavenet_Gru(shape_):\n    \n    def cbr(x, out_layer, kernel, stride, dilation):\n        x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        return x\n    \n    def wave_block(x, filters, kernel_size, n):\n        dilation_rates = [2**i for i in range(n)]\n        \n        x = Conv1D(filters, kernel_size=1, dilation_rate=1, strides=1, padding=\"same\")(x)\n        \n        res_x = x\n        for dilation_rate in dilation_rates:\n            x = Conv1D(filters = filters,\n                       kernel_size = kernel_size,\n                       padding = 'causal',\n                       dilation_rate = dilation_rate)(x)\n            \n            tanh_out = Activation('tanh')(x)\n            sigm_out = Activation('sigmoid')(x)\n            \n            x = Multiply()([tanh_out, sigm_out])\n            \n            x = SpatialDropout1D(0.1)(x)\n            \n            x = Conv1D(filters = filters,\n                       kernel_size = 1,\n                       padding = 'same')(x)\n            res_x = Add()([res_x, x])\n            \n        x = Activation(\"relu\")(x)\n        return res_x\n    \n    def bidirectional_wave_block(x, filters, kernel_size, n):\n        x_f = wave_block(x, filters, kernel_size, n)\n        x_rev = Lambda(lambda t: t[:,::-1,:])(x)\n        x_b = wave_block(x_rev, filters, kernel_size, n)\n        x_b = Lambda(lambda t: t[:,::-1,:])(x_b)\n        x = concatenate([x_f, x_b])\n        return x\n    \n    inp = Input(shape = (shape_))\n    x = inp\n\n    # reverse the seq\n    x_rev = Lambda(lambda t: t[:,::-1,:])(x)\n    \n    # forward features\n    x = wave_block(x, 64, 2, 13)\n    x = wave_block(x, 64, 2, 13)\n    x = wave_block(x, 64, 2, 13)\n    \n    # backward features\n    x_rev = wave_block(x_rev, 64, 2, 13)\n    x_rev = wave_block(x_rev, 64, 2, 13)\n    x_rev = wave_block(x_rev, 64, 2, 13)\n    x_rev = Lambda(lambda t: t[:,::-1,:])(x_rev)\n    \n    # stack features\n    x = concatenate([x, x_rev])\n    \n    \n    x = Bidirectional(tf.compat.v1.keras.layers.CuDNNGRU(64, return_sequences=True))(x)\n    x = Bidirectional(tf.compat.v1.keras.layers.CuDNNGRU(64, return_sequences=True))(x)\n    out = Dense(11, activation = 'softmax', name = 'out')(x)\n    \n    model = models.Model(inputs = inp, outputs = out)\n    \n    opt = Adam(lr = LR)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(loss = losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n    return model\n\ndef Classifier_Wavenet(shape_):\n    \n    def cbr(x, out_layer, kernel, stride, dilation):\n        x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        return x\n    \n    def wave_block(x, filters, kernel_size, n):\n        dilation_rates = [2**i for i in range(n)]\n        \n        x = Conv1D(filters, kernel_size=1, dilation_rate=1, strides=1, padding=\"same\")(x)\n        \n        res_x = x\n        for dilation_rate in dilation_rates:\n            x = Conv1D(filters = filters,\n                       kernel_size = kernel_size,\n                       padding = 'same',\n                       dilation_rate = dilation_rate)(x)\n            \n            tanh_out = Activation('tanh')(x)\n            sigm_out = Activation('sigmoid')(x)\n            \n            x = Multiply()([tanh_out, sigm_out])\n            \n            x = SpatialDropout1D(0.1)(x)\n            \n            x = Conv1D(filters = filters,\n                       kernel_size = 1,\n                       padding = 'same')(x)\n            res_x = Add()([res_x, x])\n            \n        x = Activation(\"relu\")(x)\n        return res_x\n    \n    inp = Input(shape = (shape_))\n    x = inp\n    \n    x = wave_block(x, 64, 3, 12)\n    x = wave_block(x, 64, 3, 8)\n    x = wave_block(x, 64, 3, 4)\n    x = wave_block(x, 64, 3, 1)\n    \n    #x = Bidirectional(tf.compat.v1.keras.layers.CuDNNGRU(64, return_sequences=True))(x)\n    #x = Bidirectional(tf.compat.v1.keras.layers.CuDNNGRU(64, return_sequences=True))(x)\n    \n    out = Dense(11, activation = 'softmax', name = 'out')(x)\n    \n    model = models.Model(inputs = inp, outputs = out)\n    \n    opt = Adam(lr = LR)\n    opt = tfa.optimizers.SWA(opt, clipvalue=1.0)\n    model.compile(loss = losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n    return model\n# function that decrease the learning as epochs increase (i also change this part of the code)\ndef lr_schedule(epoch):\n    if epoch < 30:\n        lr = LR\n    elif epoch < 40:\n        lr = LR \/ 3\n    elif epoch < 50:\n        lr = LR \/ 5\n    elif epoch < 60:\n        lr = LR \/ 7\n    elif epoch < 70:\n        lr = LR \/ 9\n    elif epoch < 80:\n        lr = LR \/ 11\n    elif epoch < 90:\n        lr = LR \/ 13\n    else:\n        lr = LR \/ 100\n    return lr\n\n# class to get macro f1 score. This is not entirely necessary but it's fun to check f1 score of each epoch (be carefull, if you use this function early stopping callback will not work)\nclass MacroF1(Callback):\n    def __init__(self, model, inputs, targets):\n        self.model = model\n        self.inputs = inputs\n        self.targets = np.argmax(targets, axis = 2).reshape(-1)\n        \n    def on_epoch_end(self, epoch, logs):\n        pred = np.argmax(self.model.predict(self.inputs), axis = 2).reshape(-1)\n        score = f1_score(self.targets, pred, average = 'macro')\n        print(f'F1 Macro Score: {score:.5f}')\n\ndef do_augment(train, feats, trn_idx, aug_time=5):\n    \n    import matplotlib.pyplot as plt\n    #np.random.seed(423)\n    \n    # for more group 3 and 4 data\n    aug_gp_mapping = {\n        3: 1,\n        4: 3\n    }\n    \n    all_dfs = []\n    \n    for gp in [0,1,2,3,4]:\n        \n        orig_trn = train.loc[trn_idx, ['signal', 'group_ind', 'open_channels', 'group']]\n        orig_trn = orig_trn.loc[orig_trn.group_ind == gp]\n        orig_trn_x = orig_trn['signal'].values\n        orig_trn_y = orig_trn['open_channels'].values\n        \n        print(orig_trn_x.shape, orig_trn_y.shape)\n        print(orig_trn['open_channels'].value_counts().sort_index())\n        means_ = []\n        vars_ = []\n        for oc in range(orig_trn_y.max()+1):\n            filt = orig_trn_y == oc\n            signals = orig_trn_x[filt]\n            means_ += [signals.mean()]\n            vars_ += [signals.var()]\n            print(oc, signals.mean(), signals.var())\n            \n        if gp not in aug_gp_mapping.keys() or aug_time == 0:\n            res = pd.DataFrame()\n            res['signal'] = orig_trn_x\n            res['open_channels'] = orig_trn_y\n            all_dfs += [res]\n            continue\n            \n        trn = train.loc[trn_idx, ['signal', 'group_ind', 'open_channels', 'group']]\n        trn = trn.loc[trn.group_ind == aug_gp_mapping[gp],]\n        trn_x = trn['signal'].values\n        trn_y = trn['open_channels'].values\n        signal_segments = trn.groupby('group').apply(lambda x: x['signal'].values)\n        open_chnannel_segments = trn.groupby('group').apply(lambda x: x['open_channels'].values)\n        \n        # double the training size, per 4000 split, collect trn_X_.shape[0]\/\/4000 segments\n        signal_segments = [s for s in signal_segments if len(s) == GROUP_BATCH_SIZE]\n        open_chnannel_segments = [s for s in open_chnannel_segments if len(s) == GROUP_BATCH_SIZE]\n    \n        # collect statistics before augmentation\n        plt.title('before augment signal'); plt.plot(orig_trn_x[::1000]); plt.show()\n        plt.title('before augment label'); plt.plot(orig_trn_y[::1000]); plt.show()\n        \n        # linear regression to find mean\\var per open channel\n        seq_len = orig_trn_y.max()+1\n        selected_len = int(seq_len * 2 \/ 3)\n        from sklearn.linear_model import LinearRegression\n        lr = LinearRegression()\n        lr.fit(np.arange(seq_len-selected_len, seq_len, 1).reshape((-1,1)), means_[-selected_len:])\n        new_means = lr.predict(np.arange(seq_len).reshape((-1,1))) \n        \n        new_vars = [v if (orig_trn_y == i).sum()>30 else max(vars_) for i, v in enumerate(vars_)]\n        \n        # adjust mean for original data\n        for oc in range(orig_trn_y.max()+1):\n            filt = orig_trn_y == oc\n            signals = orig_trn_x[filt]\n            src_m, src_v = signals.mean(), signals.var()\n            if (orig_trn_y == oc).sum()<=30:\n                src_v = max(vars_)\n                \n            tar_m, tar_v = new_means[oc], new_vars[oc] \n        \n            signals = (signals-src_m)\/(src_v**.5)*(tar_v**.5) + tar_m\n            orig_trn_x[orig_trn_y == oc] = signals\n            \n        # augment data\n        aug_size = aug_time*orig_trn_x.shape[0]\/\/GROUP_BATCH_SIZE\n        selections = [np.random.choice(np.arange(len(signal_segments)), orig_trn_y.max()\/\/trn['open_channels'].max(), replace=False) for _ in range(aug_size)]\n        \n        combined_signal_segments = []\n        combined_oc_segments = []\n        for inds in selections:\n            combined_signal_segments += [np.vstack([signal_segments[ix] for ix in inds]).sum(axis=0)]\n            combined_oc_segments += [np.vstack([open_chnannel_segments[ix] for ix in inds]).sum(axis=0)]\n        \n        aug_trn_x = np.concatenate(combined_signal_segments)\n        aug_trn_y = np.concatenate(combined_oc_segments)\n        print(pd.Series(aug_trn_y).value_counts().sort_index())\n        \n        for oc in range(orig_trn_y.max()+1):\n            aug_signals = aug_trn_x[aug_trn_y == oc]\n            src_m, src_v = aug_signals.mean(), aug_signals.var()\n            if (aug_trn_y == oc).sum()<=30:\n                src_v = max(vars_)\n                \n            tar_m, tar_v = new_means[oc], new_vars[oc]\n            \n            aug_signals = (aug_signals-src_m)\/(src_v**.5)*(tar_v**.5) + tar_m\n            aug_trn_x[aug_trn_y == oc] = aug_signals\n          \n        print('Augment Data Shape:', aug_trn_x.shape)\n        \n        trn_x = np.concatenate([orig_trn_x, aug_trn_x], axis=0)\n        trn_y = np.concatenate([orig_trn_y, aug_trn_y], axis=0)\n        #assert False\n        \n        plt.title('after augment'); plt.plot(trn_x[::1000]); plt.show()\n        plt.title('after augment label'); plt.plot(trn_y[::1000]); plt.show()\n        for oc in range(trn_y.max()+1):\n            signals = trn_x[trn_y == oc]\n            print(oc, signals.mean(), signals.var())\n    \n        res = pd.DataFrame()\n        res['signal'] = trn_x\n        res['open_channels'] = trn_y\n        all_dfs += [res]\n        \n    df = pd.concat(all_dfs, axis=0).reset_index(drop=True)\n    df = run_feat_engineering(df, batch_size=GROUP_BATCH_SIZE)\n    \n    tr = pd.concat([pd.get_dummies(df.open_channels), df[['group']]], axis=1)\n    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n    target_cols = ['target_'+str(i) for i in range(11)]\n    targets = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n    signals = np.array(list(df.groupby('group').apply(lambda x: x[feats].values)))\n    \n    return signals, targets\n\n# main function to perfrom groupkfold cross validation (we have 1000 vectores of 4000 rows and 8 features (columns)). Going to make 5 groups with this subgroups.\ndef run_cv_model_by_batch(train, test, splits, batch_col, feats, sample_submission, nn_epochs, nn_batch_size):\n    \n    seed_everything(SEED)\n    K.clear_session()\n    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n    tf.compat.v1.keras.backend.set_session(sess)\n    oof_ = np.zeros((len(train), 11)) # build out of folds matrix with 11 columns, they represent our target variables classes (from 0 to 10)\n    preds_ = np.zeros((len(test), 11))\n    target = ['open_channels']\n    group = train['group']\n    kf = GroupKFold(n_splits=5)\n    splits = [x for x in kf.split(train, train[target], group)]\n\n    new_splits = []\n    for sp in splits:\n        new_split = []\n        new_split.append(np.unique(group[sp[0]]))\n        new_split.append(np.unique(group[sp[1]]))\n        new_split.append(sp[0])   \n        new_split.append(sp[1])    \n        new_splits.append(new_split)\n    # pivot target columns to transform the net to a multiclass classification estructure (you can also leave it in 1 vector with sparsecategoricalcrossentropy loss function)\n    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n\n    orig_train = train.copy()\n    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n    target_cols = ['target_'+str(i) for i in range(11)]\n    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n    f1_scores = []\n    \n    for n_fold, (tr_idx, val_idx, trn_orig_ix, val_orig_idx) in enumerate(new_splits[0:], start=0):\n        print(f'Training fold {n_fold + 1} started')\n        if n_fold not in [FOLD]:\n            continue\n        \n        # augtime = 5: fold 1 is very bad\n        # aug = 1: [0.9375797429851022, 0.8188899525577876, 0.936195957056339, 0.7684088439541757, 0.9371989134178409]\n        # aug = 1 (gp4 aug only): fold 0 worse, fold 1 still pretty bad\n        # aug = 1 (gp3 aug only): fold 0 worse, fold 1 still pretty bad\n        # train starting for fold 1: fold 1 is good=>.9385\n        # remove signal **2, train from fold 0: .9365\n        # use signal absolute instead, train from fold 0: .9358\n        # use grad clip value = 1, sig**2 feat, train from fold 0: 0.93785, fold 1 breaks...\n        # use fixed random seed in the beginning of augmentation\n        # separate train for each fold...: fold 1: .9392, fold 3: .9369\n        \n        #train_x, train_y = train[tr_idx], train_tr[tr_idx]\n        train_x, train_y = do_augment(orig_train, feats, trn_orig_ix, aug_time=AUG_CNT)\n        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n        print(f'Our training dataset shape is {train_x.shape}')\n        print(f'Our validation dataset shape is {valid_x.shape}')\n\n        gc.collect()\n        shape_ = (None, train_x.shape[2]) # input is going to be the number of feature we are using (dimension 2 of 0, 1, 2)\n        model = Classifier_Wavenet(shape_)\n        # using our lr_schedule function\n        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n        model.fit(train_x,train_y,\n                  epochs = nn_epochs,\n                  callbacks = [cb_lr_schedule, MacroF1(model, valid_x, valid_y)], # adding custom evaluation metric for each epoch\n                  batch_size = nn_batch_size,verbose = 2,\n                  validation_data = (valid_x,valid_y))\n        preds_f = model.predict(valid_x)\n        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro') # need to get the class with the biggest probability\n        print(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n        oof_[val_orig_idx,:] += preds_f\n        te_preds = model.predict(test)\n        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n        preds_ += te_preds \/ SPLITS\n        f1_scores += [f1_score_]\n    # calculate the oof macro f1_score\n    \n    f1_score_ = f1_score(np.argmax(train_tr, axis = 2).reshape(-1),  np.argmax(oof_, axis = 1), average = 'macro') # axis 2 for the 3 Dimension array and axis 1 for the 2 Domension Array (extracting the best class)\n    print(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n    print('all fold f1 scores = ', f1_scores)\n    sample_submission['open_channels'] = np.argmax(preds_, axis = 1).astype(int)\n    sample_submission.to_csv('submission_wavenet.csv', index=False, float_format='%.4f')\n    \n    pd.to_pickle({\n        'train': oof_,\n        'test': preds_,\n    }, 'wavenet_aug_oof_{}_{}.pkl'.format(FOLD, AUG_CNT))\n    \n    import matplotlib.pyplot as plt\n    plt.plot(sample_submission['open_channels'].values[::1000]); plt.show()\n    \n# this function run our entire program\ndef run_everything():\n    \n    print('Reading Data Started...')\n    train, test, sample_submission = read_data()\n    #train, test = normalize(train, test)\n    print('Reading and Normalizing Data Completed')\n        \n    print('Creating Features')\n    print('Feature Engineering Started...')\n    \n    train = train_update_groups(train)\n    test = test_update_groups(test)\n    \n    train = run_feat_engineering(train, batch_size = GROUP_BATCH_SIZE)\n    test = run_feat_engineering(test, batch_size = GROUP_BATCH_SIZE)\n    \n    #train, fit_dict = convert_signal_into_probs(train, is_train=True, fit_dict=None)\n    #test, _ = convert_signal_into_probs(test, is_train=False, fit_dict=fit_dict)\n    \n    train, test, features = feature_selection(train, test)\n    \n    print('Feature Engineering Completed...')\n        \n   \n    print(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started...')\n    run_cv_model_by_batch(train, test, SPLITS, 'group', features, sample_submission, EPOCHS, NNBATCHSIZE)\n    print('Training completed...')\n        \nrun_everything()\n\n# 7: 0.94086\n# 8: 0.9397","e11fbf2d":"# Thanks to https:\/\/www.kaggle.com\/siavrez\/wavenet-keras and Sergey Bryansky.\n# You can take a look at Sergey's kernel [here](https:\/\/www.kaggle.com\/sggpls\/shifted-rfc-pipeline) or [here](https:\/\/www.kaggle.com\/sggpls\/wavenet-with-shifted-rfc-proba). Also, Sergey's [data is here.](https:\/\/www.kaggle.com\/sggpls\/ion-shifted-rfc-proba)"}}