{"cell_type":{"4902d48a":"code","90de5522":"code","54980320":"code","a32f18dc":"code","df743ffb":"code","1c2904be":"code","b2ea49e5":"code","d9afefa0":"code","8d2043f9":"code","7ff175ad":"code","caf63bf1":"code","0d2810de":"code","18866c51":"code","7bced10a":"code","6e4edced":"code","0bc943eb":"code","25d81967":"code","d0b089c4":"code","95b269d3":"code","722d57e7":"code","58a7507e":"markdown","c5bbb7c7":"markdown","6797588d":"markdown","9359756f":"markdown","c3ec14dc":"markdown","7f39422c":"markdown","b9e74650":"markdown","4e320481":"markdown","8048b4ed":"markdown","6d74b892":"markdown","65988479":"markdown","2332b005":"markdown"},"source":{"4902d48a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","90de5522":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib.pylab as pylab\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\n\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.linear_model import Perceptron\nfrom sklearn import svm \nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.model_selection import train_test_split \nfrom sklearn import metrics \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.model_selection import cross_val_predict \nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n","54980320":"# \u8aad\u307f\u8fbc\u3080\u30c7\u30fc\u30bf\u304c\u683c\u7d0d\u3055\u308c\u305f\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30d1\u30b9\uff0c\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u5909\u66f4\u306e\u5fc5\u8981\u3042\u308a\n\npath = \"\/kaggle\/input\/titanic\/\"\n\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\ntrain[\"train\"] = 1\ntest[\"train\"] = 0\ncombine = pd.concat([train, test])","a32f18dc":"combine[\"Sex\"] = combine[\"Sex\"].replace(\"male\", \"0\").replace(\"female\", \"1\")\ncombine[\"Sex\"] = combine[\"Sex\"].astype(int)\n\ncombine[\"Age\"].fillna(combine.Age.mean(), inplace=True) \n\ncombine['honorific'] = combine['Name'].map(lambda x: x.split(', ')[1].split('. ')[0])\ncombine['honorific'] = combine['honorific'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ncombine['honorific'] = combine['honorific'].replace('Mlle', 'Miss')\ncombine['honorific'] = combine['honorific'].replace('Ms', 'Miss')\ncombine['honorific'] = combine['honorific'].replace('Mme', 'Mrs')\nSalutation_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5} \ncombine['honorific'] = combine['honorific'].map(Salutation_mapping) \ncombine['honorific'] = combine['honorific'].fillna(0) \n\ncombine[\"FamilySize\"] = combine[\"SibSp\"] + combine[\"Parch\"] + 1\n\ncombine['IsAlone'] = 0\ncombine.loc[combine['FamilySize'] == 1, 'IsAlone'] = 1\n\ncombine['Ticket_Alphabet'] = combine['Ticket'].apply(lambda x: str(x)[0])\ncombine['Ticket_Alphabet'] = combine['Ticket_Alphabet'].apply(lambda x: str(x)) \ncombine['Ticket_Alphabet'] = np.where((combine['Ticket_Alphabet']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']), combine['Ticket_Alphabet'], np.where((combine['Ticket_Alphabet']).isin(['W', '4', '7', '6', 'L', '5', '8']), '0','0')) \ncombine['Ticket_Alphabet']=combine['Ticket_Alphabet'].replace(\"1\",1).replace(\"2\",2).replace(\"3\",3).replace(\"0\",0).replace(\"S\",3).replace(\"P\",0).replace(\"C\",3).replace(\"A\",3) \ncombine['Ticket_Len'] = combine['Ticket'].apply(lambda x: len(x)) \n    \ncombine['Cabin_Alphabet'] = combine['Cabin'].apply(lambda x: str(x)[0]) \ncombine['Cabin_Alphabet'] = combine['Cabin_Alphabet'].apply(lambda x: str(x)) \ncombine['Cabin_Alphabet'] = np.where((combine['Cabin_Alphabet']).isin([ 'F', 'E', 'D', 'C', 'B', 'A']),combine['Cabin_Alphabet'], np.where((combine['Cabin_Alphabet']).isin(['W', '4', '7', '6', 'L', '5', '8']), '0','0'))\ncombine['Cabin_Alphabet']=combine['Cabin_Alphabet'].replace(\"A\",1).replace(\"B\",2).replace(\"C\",1).replace(\"0\",0).replace(\"D\",2).replace(\"E\",2).replace(\"F\",1) \n\ncombine['Fare'].fillna(combine['Fare'].median(), inplace = True)\n\ncombine['Embarked'] = combine['Embarked'].fillna(combine[\"Embarked\"].mode()[0])\ncombine['Embarked'] = combine['Embarked'].replace('S', \"0\").replace( 'C', \"1\").replace('Q', \"2\")\ncombine['Embarked'] = combine['Embarked'].astype(int)\ncombine[\"Embarked\"].fillna(combine.Embarked.mean(), inplace=True) \n","df743ffb":"train = combine[combine[\"train\"]==1]\ntest = combine[combine[\"train\"]==0]","1c2904be":"train.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"train\"], axis=1, inplace=True)\ntest.drop(['PassengerId', \"Name\", \"Ticket\", \"Cabin\", \"train\"], axis=1, inplace=True)","b2ea49e5":"y_train  = train[\"Survived\"]  \nX_train = train.drop([\"Survived\"], axis=1)\nX_test = test.drop([\"Survived\"], axis=1)","d9afefa0":"random_forest=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=25, max_features='auto', max_leaf_nodes=None,\n            min_samples_leaf=1, min_samples_split=15,\n            min_weight_fraction_leaf=0.0, n_estimators=51, n_jobs=4,\n            oob_score=False, random_state=0, verbose=0, warm_start=False)\n\nrandom_forest.fit(X_train, y_train)\nY_pred_rf = random_forest.predict(X_test)","8d2043f9":"path = \"\/kaggle\/input\/titanic\/\"\n\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\n\ntrain[\"train\"] = 1\ntest[\"train\"] = 0\ndata = pd.concat([train, test])\ndata[\"Name\"] = data[\"Name\"].apply(lambda x: str(x)[x.find(\",\")+2 : x.find(\".\")])\ndata[\"Female_boy\"] = 0\ndata.loc[(data[\"Sex\"]==\"female\")|(data[\"Name\"]==\"Master\"), \"Female_boy\"] = 1\n\ndata = data[[\"PassengerId\", 'Survived', 'Ticket', 'Female_boy', \"train\"]]\ndata = data[data['Female_boy']==1]\nindex = data.index\nnum_Ticket = data.Ticket.value_counts()\nnum_Ticket = pd.DataFrame(num_Ticket)\nnum_Ticket.reset_index(inplace=True)\nnum_Ticket.rename({'index': 'Ticket', 'Ticket':'num_Ticket'}, inplace=True, axis=1)\ndata = pd.merge(data, num_Ticket, on=['Ticket'], how='left')\ndata['Ticket_sv'] = data.groupby('Ticket')['Survived'].transform('mean')\ndata.index = index\ntest_data = data[data[\"train\"]==0]\ntest_data = test_data[[\"PassengerId\", 'Ticket_sv']]\n\nsubmission_rf = pd.read_csv(path + \"gender_submission.csv\")\nsubmission_rf[\"Survived\"] = Y_pred_rf\nsubmission_rf = submission_rf.merge(test_data, on=[\"PassengerId\"], how=\"left\")\nsubmission_rf.loc[submission_rf['Ticket_sv']==1.0, \"Survived\"] = 1\nsubmission_rf.loc[submission_rf['Ticket_sv']==0.0, \"Survived\"] = 0\ndel submission_rf['Ticket_sv']\nsubmission_rf","7ff175ad":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain[\"train\"] = 1\ntest[\"train\"] = 0\ncombine = pd.concat([train, test])","caf63bf1":"combine[\"Sex\"] = combine[\"Sex\"].replace(\"male\", \"0\").replace(\"female\", \"1\")\ncombine[\"Sex\"] = combine[\"Sex\"].astype(int)\n\ncombine['honorific'] = combine['Name']\nfor name_string in combine['Name']:\n    combine['honorific'] = combine['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss','Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ncombine.replace({'honorific': mapping}, inplace=True)\ntitles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\nfor title in titles:\n    age_to_impute = combine.groupby('honorific')['Age'].median()[titles.index(title)]\n    combine.loc[(combine['Age'].isnull()) & (combine['honorific'] == title), 'Age'] = age_to_impute\ncombine.drop('honorific', axis = 1, inplace = True)\n\ncombine['Family_Size'] = combine['Parch'] + combine['SibSp']\ncombine['Family_Name'] = combine['Name'].apply(lambda x: str.split(x, \",\")[0])\n\npre = 0.5\ncombine['Family_Survival'] = pre\nfor grp, grp_df in combine[['Survived','Name', 'Family_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Family_Name', 'Fare']):\n    if (len(grp_df) != 1):\n        # len(grp_df) != 1\u306a\u3089\u5bb6\u65cf\u304c\u3044\u308b\u3063\u3066\u4e8b\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                combine.loc[combine['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                combine.loc[combine['PassengerId'] == passID, 'Family_Survival'] = 0\n                \nfor _, grp_df in combine.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    combine.loc[combine['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    combine.loc[combine['PassengerId'] == passID, 'Family_Survival'] = 0\n\n\nlabel = LabelEncoder()\ncombine['Fare'].fillna(combine['Fare'].median(), inplace = True)\ncombine['Farelabal'] = pd.qcut(combine['Fare'], 5)\ncombine['Farelabal'] = label.fit_transform(combine['Farelabal'])\n\ncombine['Agelabel'] = pd.qcut(combine['Age'], 4)\ncombine['Agelabel'] = label.fit_transform(combine['Agelabel'])\n\n","0d2810de":"train = combine[combine[\"train\"]==1]\ntest = combine[combine[\"train\"]==0]","18866c51":"\ntrain.drop(['PassengerId', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'train', 'Family_Name'], axis = 1, inplace = True)\ntest.drop(['PassengerId', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin','Embarked', 'train', 'Family_Name'], axis = 1, inplace = True)\n","7bced10a":"X = train.drop('Survived',  axis = 1)\ny = train['Survived']\nX_test = test.drop('Survived',  axis = 1)","6e4edced":"std_scaler = StandardScaler()\nX = std_scaler.fit_transform(X)\nX_test = std_scaler.transform(X_test)","0bc943eb":"knn = KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=6, p=2, \n                           weights='uniform')\nknn.fit(X, y)\ny_pred_knn = knn.predict(X_test)","25d81967":"path = \"\/kaggle\/input\/titanic\/\"\n\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\n\ntrain[\"train\"] = 1\ntest[\"train\"] = 0\ndata = pd.concat([train, test])\ndata[\"Name\"] = data[\"Name\"].apply(lambda x: str(x)[x.find(\",\")+2 : x.find(\".\")])\ndata[\"Female_boy\"] = 0\ndata.loc[(data[\"Sex\"]==\"female\")|(data[\"Name\"]==\"Master\"), \"Female_boy\"] = 1\n\ndata = data[[\"PassengerId\", 'Survived', 'Ticket', 'Female_boy', \"train\"]]\ndata = data[data['Female_boy']==1]\nindex = data.index\nnum_Ticket = data.Ticket.value_counts()\nnum_Ticket = pd.DataFrame(num_Ticket)\nnum_Ticket.reset_index(inplace=True)\nnum_Ticket.rename({'index': 'Ticket', 'Ticket':'num_Ticket'}, inplace=True, axis=1)\ndata = pd.merge(data, num_Ticket, on=['Ticket'], how='left')\ndata['Ticket_sv'] = data.groupby('Ticket')['Survived'].transform('mean')\ndata.index = index\ntest_data = data[data[\"train\"]==0]\ntest_data = test_data[[\"PassengerId\", 'Ticket_sv']]\n\nsubmission_knn = pd.read_csv(path + \"gender_submission.csv\")\nsubmission_knn[\"Survived\"] = y_pred_knn\nsubmission_knn = submission_knn.merge(test_data, on=[\"PassengerId\"], how=\"left\")\nsubmission_knn.loc[submission_knn['Ticket_sv']==1.0, \"Survived\"] = 1\nsubmission_knn.loc[submission_knn['Ticket_sv']==0.0, \"Survived\"] = 0\ndel submission_knn['Ticket_sv']\nsubmission_knn","d0b089c4":"sum_pred = submission_rf.merge(submission_knn, on=[\"PassengerId\"], how=\"left\")\nsum_pred[\"Survived_x\"] = sum_pred[\"Survived_x\"].astype(int)\nsum_pred[\"Survived_sum\"] = (sum_pred[\"Survived_x\"] + sum_pred[\"Survived_y\"])\/2\nsum_pred[\"Survived_sum\"].value_counts()","95b269d3":"for i in range(418):\n    if sum_pred[\"Survived_sum\"][i]<=0.5:\n        sum_pred[\"Survived_sum\"][i]=0\n    else:\n        sum_pred[\"Survived_sum\"][i]=1\nsum_pred[\"Survived_sum\"] = sum_pred[\"Survived_sum\"].astype(int)        \nsum_pred[\"Survived_sum\"].value_counts()","722d57e7":"submission = pd.DataFrame({\n        \"PassengerId\": pre_test[\"PassengerId\"],\n        \"Survived\":pre_test[\"Survived_sum\"]\n    })\n\nsubmission.to_csv('submission.Titanic_Best1.csv', index=False)","58a7507e":"# **\u30e2\u30c7\u30eb\uff11**","c5bbb7c7":"# \u5f8c\u51e6\u7406","6797588d":"# \u5171\u901a\u30e9\u30a4\u30d6\u30e9\u30ea\u30fc","9359756f":"# \u7279\u5fb4\u91cf\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0","c3ec14dc":"# \u30e2\u30c7\u30eb\u878d\u5408\uff08\u30e2\u30c7\u30eb\uff11\u3068\u30e2\u30c7\u30eb\uff12\u306e\u5e73\u5747\u3092\u53d6\u308b\uff09\n#### \"Survived\"=0.5\u306e\u4eba\u306e\u5224\u5225\u306f\u305d\u308c\u305e\u308c\u500b\u4eba\u306e\u60c5\u5831\u3092\u62bd\u51fa\u3057\u3001\u500b\u5225\u306b\u5224\u5b9a\u3057\u3088\u3046\u3068\u3057\u305f\u304c\u3001\u5be9\u67fb\u3067Hand_Labeling\u306e\u5bfe\u8c61\u3068\u306a\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u305f\u3081\u30010.5\u21921\u3068\u3057\u305f\u5834\u5408\u30680.5\u21920\u3068\u3057\u305f\u5834\u5408\u306e\u4e21\u65b9\u306e\u30b5\u30d6\u30df\u30c3\u30b7\u30e7\u30f3\u3092\u63d0\u51fa\u3057\u3001\u30b9\u30b3\u30a2\u306e\u826f\u3044\u65b9\uff080.5\u21920\uff09\u3092\u6700\u7d42\u30e2\u30c7\u30eb\u3068\u3057\u305f\u3002","7f39422c":"# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f","b9e74650":"# \u30e2\u30c7\u30eb\u63d0\u51fa","4e320481":"# \u30d9\u30b9\u30c8\u30e2\u30c7\u30ea\u30f3\u30b0\uff12\uff08KNeighborsClassifier\uff09","8048b4ed":"# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\n","6d74b892":"# \u5f8c\u51e6\u7406","65988479":"# \u30d9\u30b9\u30c8\u30e2\u30c7\u30eb\u30ea\u30f3\u30b0\uff11\uff08RandomForestClassifier\uff09","2332b005":"# \u30e2\u30c7\u30eb\uff12"}}