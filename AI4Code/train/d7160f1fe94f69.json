{"cell_type":{"dc7800e9":"code","ef7ecb10":"code","cfc67326":"code","5444e7ec":"code","d0a61c11":"code","fd9a5ce7":"code","3637bd26":"code","3fce2a6b":"code","2685f78c":"code","dd7f1097":"code","e2c28f5f":"code","0086dfab":"markdown","92c85947":"markdown","54479ace":"markdown"},"source":{"dc7800e9":"import numpy as np\nimport pandas as pd\nimport gensim\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error","ef7ecb10":"data_dir = '..\/input\/commonlitreadabilityprize\/'\ntrain = pd.read_csv(data_dir + 'train.csv')\ntest = pd.read_csv(data_dir + 'test.csv')\nsample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n\ntarget = train['target'].to_numpy()","cfc67326":"word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin', binary=True)\nprint(word2vec_model.vectors.shape)","5444e7ec":"def avg_feature_vector(sentence, model, num_features):\n    words = sentence.replace('\\n',\" \").replace(',',' ').replace('.',\" \").split()\n    feature_vec = np.zeros((num_features,),dtype=\"float32\")#\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\n    i=0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, model[word])\n        except KeyError as error:\n            feature_vec \n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words)- i)\n    return feature_vec","d0a61c11":"word2vec_train = np.zeros((len(train.index),300),dtype=\"float32\")#\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\nword2vec_test = np.zeros((len(test.index),300),dtype=\"float32\")\n\nfor i in range(len(train.index)):\n    word2vec_train[i] = avg_feature_vector(train[\"excerpt\"][i],word2vec_model, 300)\n    \nfor i in range(len(test.index)):\n    word2vec_test[i] = avg_feature_vector(test[\"excerpt\"][i],word2vec_model, 300) ","fd9a5ce7":"print(word2vec_train.shape)\nprint(target.shape)\nprint(word2vec_test.shape)","3637bd26":"#parameter settings\nparams = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'seed': 42,\n    'learning_rate': 0.01,\n    \"n_jobs\": -1,\n    \"verbose\": -1\n}\n\npred = np.zeros(test.shape[0])","3fce2a6b":"#KFold \u3000n_splits=5\nfrom sklearn.model_selection import KFold\nfold = KFold(n_splits=5, shuffle=True, random_state=42)\ncv=list(fold.split(word2vec_train, target))","2685f78c":"rmses = []\nfor tr_idx, val_idx in cv: \n    x_tr, x_va = word2vec_train[tr_idx], word2vec_train[val_idx]\n    y_tr, y_va = target[tr_idx], target[val_idx]\n        \n    train_set = lgb.Dataset(x_tr, y_tr)\n    val_set = lgb.Dataset(x_va, y_va, reference=train_set)\n        \n    # Training\n    model = lgb.train(params, train_set, num_boost_round=10000, early_stopping_rounds=100,\n                      valid_sets=[train_set, val_set], verbose_eval=-1)\n        \n    y_pred = model.predict(x_va)\n    rmse =  np.sqrt(mean_squared_error(y_va, y_pred))\n    rmses.append(rmse)\n        \n    #Inference\n    test_pred = model.predict(word2vec_test)\n    pred += test_pred \/ 5  \n        \nprint(\"\\n\", \"Mean Fold RMSE:\", np.mean(rmses))    ","dd7f1097":"sample_submission.target = pred\nsample_submission.to_csv('submission.csv',index=False)","e2c28f5f":"sample_submission","0086dfab":"# Training & Inference\nlightgbm (KFold=5)","92c85947":"# Embedding by Word2vec","54479ace":"\n\n\n\nThis notebook is a LightGBM learning & inference model using Word2vec.  It's a very light model so it can be run on a CPU.\n\nWord2vec represents words in 300 dimensions. By averaging the 300-dimensional vectors of the words in the sentence, the sentence was represented in 300 dimensions.\n\nSince the parameters are hardly changed, there is a possibility of improving the score."}}