{"cell_type":{"96dfbafd":"code","c4eeba06":"code","3c8f2b57":"code","dce0c8e1":"code","c1340ac3":"code","5a12fbe8":"code","34e9bc49":"code","d0348788":"code","eca21c88":"code","8e62fe00":"code","b46aafb6":"code","956d712f":"code","6b36304c":"code","59479d9d":"code","21f96277":"code","923b9fca":"code","fb1e67e6":"markdown","593c816c":"markdown","218bc7c3":"markdown","97adb921":"markdown","8e435a8e":"markdown","876c27b1":"markdown","afe686b1":"markdown","86fedf5f":"markdown","e387112e":"markdown","b25c143f":"markdown","92cac8bc":"markdown"},"source":{"96dfbafd":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn import tree\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy.stats import gaussian_kde\n\nplt.style.use('ggplot')\n\n\ndata = pd.read_csv('..\/input\/StudentsPerformance.csv')","c4eeba06":"data.isnull().sum()","3c8f2b57":"data.duplicated().sum()","dce0c8e1":"data.head()","c1340ac3":"data.shape","5a12fbe8":"info_dict = {}\nfor i,item in enumerate(data.columns):\n    if i < 5:\n        info_list = list(set(list(data[item])))\n        info_list.sort()\n        info_dict[item]=info_list\ninfo_dict","34e9bc49":"data_summary = data.describe()\ndata_summary.loc['skewness'] = data.skew()\ndata_summary.loc['kurtosis'] = data.kurtosis()\ndata_summary","d0348788":"xs = np.linspace(0,100,100)\n\nfig = plt.figure(figsize=[8, 13.2])\nfig.suptitle('Score Histograms and Kernel Density Estimations',fontsize=14,fontweight='bold')\n\nax1 = fig.add_subplot(311)\nfig.subplots_adjust(top=0.945)\nax1.set_title('Math Score',fontsize=12)\nax1.set_ylabel('Number of Students')\n\ndensity = gaussian_kde(data['math score'])\ndensity.covariance_factor = lambda : .25\ndensity._compute_covariance()\nax1.plot(xs,density(xs)*1000,color = '#4B4B4B')\nax1.hist(data['math score'],xs,color = '#FF3366')\n\nax2 = fig.add_subplot(312)\nax2.set_title('Reading Score',fontsize=12)\nax2.set_ylabel('Number of Students')\n\ndensity = gaussian_kde(data['reading score'])\ndensity.covariance_factor = lambda : .25\ndensity._compute_covariance()\nax2.plot(xs,density(xs)*1000,color = '#4B4B4B')\nax2.hist(data['reading score'],xs,color = '#6666FF')\n\nax3 = fig.add_subplot(313)\nax3.set_title('Writing Score',fontsize=12)\nax3.set_ylabel('Number of Students')\n\ndensity = gaussian_kde(data['writing score'])\ndensity.covariance_factor = lambda : .25\ndensity._compute_covariance()\nax3.plot(xs,density(xs)*1000,color = '#4B4B4B')\nax3.hist(data['writing score'],xs,color = '#FFFF33')\n\nplt.show()\n","eca21c88":"df_math = pd.DataFrame()\nfor item in info_dict.keys():\n    for features in info_dict[item]:\n        df_math[features]=data.loc[data[item]==features].describe()['math score']\ndf_math.loc['skewness'] = df_math.skew()\ndf_math.loc['kurtosis'] = df_math.kurtosis()\ndf_math['Total'] = data_summary['math score']\ndf_math","8e62fe00":"fig = plt.figure(figsize=[8, 20])\nfig.suptitle('Kernel Density Estimations of Math Score for each Feature',fontsize=14,fontweight='bold')\nfig.subplots_adjust(top=0.95)\n\ncolor_list = ['#0000FF','#FF0000','#00FFFF','#FF00FF','#FFFF00','#00FF00']\n\nfor i,item in enumerate(info_dict.keys()):\n    ax = fig.add_subplot(6,1,i+1)\n    ax.set_title(item,fontsize=12)\n    ax.set_ylabel('Probability Density')\n    for ii,features in enumerate(info_dict[item]): \n        density = gaussian_kde(data.loc[data[item]==features]['math score'])\n        density.covariance_factor = lambda : .3\n        density._compute_covariance()\n        ax.plot(xs,density(xs),color = color_list[ii])\n    ax.legend(labels = info_dict[item], loc = 'best')\n","b46aafb6":"df_reading = pd.DataFrame()\nfor item in info_dict.keys():\n    for features in info_dict[item]:\n        df_reading[features]=data.loc[data[item]==features].describe()['reading score']\ndf_reading.loc['skewness'] = df_reading.skew()\ndf_reading.loc['kurtosis'] = df_reading.kurtosis()\ndf_reading['Total'] = data_summary['reading score']\ndf_reading","956d712f":"fig = plt.figure(figsize=[8, 20])\nfig.suptitle('Kernel Density Estimations of Reading Score for each Feature',fontsize=14,fontweight='bold')\nfig.subplots_adjust(top=0.95)\n\ncolor_list = ['#0000FF','#00FF00','#FF0000','#00FFFF','#FF00FF','#FFFF00']\n\nfor i,item in enumerate(info_dict.keys()):\n    ax = fig.add_subplot(6,1,i+1)\n    ax.set_title(item,fontsize=12)\n    ax.set_ylabel('Probability Density')\n    for ii,features in enumerate(info_dict[item]): \n        density = gaussian_kde(data.loc[data[item]==features]['reading score'])\n        density.covariance_factor = lambda : .3\n        density._compute_covariance()\n        ax.plot(xs,density(xs),color = color_list[ii])\n    ax.legend(labels = info_dict[item], loc = 'best')","6b36304c":"df_writing = pd.DataFrame()\nfor item in info_dict.keys():\n    for features in info_dict[item]:\n        df_writing[features]=data.loc[data[item]==features].describe()['writing score']\ndf_writing.loc['skewness'] = df_writing.skew()\ndf_writing.loc['kurtosis'] = df_writing.kurtosis()\ndf_writing['Total'] = data_summary['writing score']\ndf_writing","59479d9d":"fig = plt.figure(figsize=[8, 20])\nfig.suptitle('Kernel Density Estimations of Writing Score for each Feature',fontsize=14,fontweight='bold')\nfig.subplots_adjust(top=0.95)\n\ncolor_list = ['#0000FF','#00FF00','#FF0000','#00FFFF','#FF00FF','#FFFF00']\n\nfor i,item in enumerate(info_dict.keys()):\n    ax = fig.add_subplot(6,1,i+1)\n    ax.set_title(item,fontsize=12)\n    ax.set_ylabel('Probability Density')\n    for ii,features in enumerate(info_dict[item]): \n        density = gaussian_kde(data.loc[data[item]==features]['writing score'])\n        density.covariance_factor = lambda : .3\n        density._compute_covariance()\n        ax.plot(xs,density(xs),color = color_list[ii])\n    ax.legend(labels = info_dict[item], loc = 'best')","21f96277":"X = data[['math score','reading score','writing score']].values.tolist()\nY = data[['gender']].values.tolist()\n\nX_embedded = TSNE(n_components=2).fit_transform(X)\nx_min, x_max = np.min(X, 0), np.max(X, 0)\nX_normalized = (X - x_min) \/ (x_max - x_min) \n\nplt.figure(figsize=[8,7])\n\nblue_dot_x = []\nblue_dot_y = []\nred_dot_x = []\nred_dot_y = []\n\nfor i in range(X_normalized.shape[0]):\n    if Y[i] == ['male']:\n        blue_dot_x.append(X_normalized[i,0])\n        blue_dot_y.append(X_normalized[i,1])\n    else:\n        red_dot_x.append(X_normalized[i,0])\n        red_dot_y.append(X_normalized[i,1])\nplt.scatter(blue_dot_x, blue_dot_y, color = '#0000FF', label = 'male')\nplt.scatter(red_dot_x, red_dot_y, color = '#FF0000',  label = 'female')\nplt.xlabel(\"Dimension 1\")\nplt.ylabel(\"Dimension 2\")\nplt.title('t_SNE plot with perplexity 30')\nplt.legend()\nplt.show()","923b9fca":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n                                        test_size=0.1, random_state=42)\n\nclf = tree.DecisionTreeClassifier(max_depth = 6)\nclf = clf.fit(X_train,Y_train)\n\nprint('The accuracy on training set is %6.4f' % clf.score(X_train,Y_train))\nprint('The accuracy on testing set is %6.4f' %clf.score(X_test, Y_test))","fb1e67e6":"It's quite amazing to have such a high accuracy rate, what is better is that decision tree is easy to visualize. I'm gonna skip this visulization part since it requires two more libraries, pydot and GraphViz (software also required). ","593c816c":"Then, we need to do data cleaning. It is a neccessary process, if we have missing values, inapproporate data type, duplicated entry, wrong calculation, abnormal or extreme vaule, any of those can affect our result. In our case, we only need to concern about missing values or duplicated entry based on the natrue of our data.","218bc7c3":"We could conclude that these three distributions of scores are negative skewed, which means having right-leaning curve. Moreover, math score distribution has observable positive kurtosis, implying a fatter-tail compared to a normal distribution with that same mean and std.\nTo visualize, histrograms and their respective kernel density estimation is plotted below.","97adb921":"## Exploratory Data Analysis\n\nWith clean dataset on hand, we are able to step further to explore more.\n\n### General proporties of scores distribution and visualization\n\nWe run a fast check of distribution proporties to be set as a base line in later evaluation.","8e435a8e":"# Student Performance Report\n\nThis is an evaluation based on marks secured by the students in high school Students from the United States, whose aim is to analysis how 5 factors affect the students' performance.","876c27b1":"Compare to the three figures used before, the t_SNE plot is more friendly to read. It is obvious to see that we can tell that it is possible to distinguish from genders by barely look at grades even though there is an overlapping area. \n\nGiven such a differnce, we can use a simple decision tree model to try making prediction whether it is a male or female student given his\/her grades.\n\nWe first separate or dataset into training set and testing set, then use ski-kit learn to bulid and optimize our model, and eventually apply our model onto the test set to check accuracy. ","afe686b1":"We have 1000 entries in total representing 1000 students and there are eight elements for each individual, three of which are test scores and five of which are certain demographical features may or may not relate to their test scores.\nLet's look into details of these features.","86fedf5f":"## Data Preprocessing\n\nFirst, we need to import dependencies and introduce data itself. We import numpy and pandas to manipulate data; sci-kit learn for simple machine learning and matplotlib for visualization.","e387112e":"As seen above, there are no missing or duplicated values in our dataframe, which is perfect and ideal to move onto next stage. We would like to take a deeper glance into the dataset to gain more insights.","b25c143f":"Though we can tell that there is certain difference among each groups in a certain feature, those figures still looks massy because of there are too many lines. The reason why we need so many of them is that human lives in a three dimensional world, it's easy for us to understand 2D image, still reasonable to recognize 3D object, but it become nonsense with 4D or higher. To deal with such high-dimensional infomation, rather than putting then into tons of graphs, another technique is introduced: t-Distributed Stochastic Neighbor Embedding (t-SNE).\n\nSpecifically, t-SNE models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\n\nIn our case, we use gender for the first example. ","92cac8bc":"### Relevance bewteen features and scores\n\nWith five features and three scores on hand, it's natural to ask if there are connections among them. Such connection could be observed by a shift in the probability distribution. In this subsection, we will take a closer look at each feature."}}