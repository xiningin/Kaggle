{"cell_type":{"05ac6e7f":"code","2d75cbe4":"code","e719d55e":"code","7bb3f429":"code","0545658a":"code","cc3dd4a0":"code","9381ff76":"code","cb3dab11":"code","8f508523":"code","0bb89ab0":"code","b7af1d45":"code","a93f91af":"code","e26011d5":"code","16c43e1c":"code","b9d77418":"code","d4cbf317":"code","528405fe":"code","6e8c7f71":"code","56d97b59":"code","1f713b2b":"code","3649f8d5":"code","de8bfcb2":"code","12316d4b":"code","40736bfd":"code","e088bb8a":"code","0dbb4d98":"code","03e0ef9f":"code","3b45fc9f":"code","e2081925":"code","6cac381e":"code","0dbc54df":"code","88f5ee3e":"code","2ff60b32":"code","e04d3de7":"code","33dda756":"code","94df4235":"code","f8a61f12":"code","98a10944":"code","7a40f4d4":"code","edcd308a":"code","78c7c7d9":"code","aa5bef2f":"code","bc3b61a2":"code","13d97816":"code","8282a899":"code","6ee78190":"code","0c394d99":"code","8243e842":"code","93beac7e":"code","97ff4550":"code","456e5e08":"code","1c25822a":"code","afc0e28c":"code","497612b1":"code","cb524120":"code","aad3752e":"code","6d05e135":"code","a11e3dd9":"code","6bed15fa":"code","cd812294":"code","0c592c65":"code","b28b8676":"code","e60f1186":"code","fb342ac5":"code","9681e393":"code","1b586e4f":"code","d0203a8f":"code","8f52c089":"code","eb4d934c":"code","ba235274":"code","6ebdd7e0":"code","6e927ce3":"code","0765ae16":"code","761b6ebb":"code","d2abaf5e":"code","085a3bad":"code","a566928f":"code","f670a73b":"code","126bd0f5":"code","d72c3fa6":"code","451dde73":"code","c84146f2":"code","62371344":"code","cef96be9":"code","feb43609":"code","ae16df4c":"code","78eea0ae":"code","4839b57a":"code","48b6a4ef":"code","dc1b56d4":"code","1b0781a7":"code","c803ba7e":"code","4f1e0acb":"code","c1ee079f":"code","178d8c59":"code","e4989520":"code","428ffa61":"code","f1263a19":"code","a4816dc5":"code","949d5cb6":"code","e9a31180":"code","fa38afdd":"code","a1a6c756":"code","65f2dcb3":"markdown","2e2d377d":"markdown","bb7457ce":"markdown","8713b2f0":"markdown","165551d9":"markdown","ca16637f":"markdown","4d1b10a6":"markdown","6b9b1ceb":"markdown","95269ce6":"markdown","f09fb39f":"markdown","c420e85b":"markdown"},"source":{"05ac6e7f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2d75cbe4":"import numpy as np\nimport pandas as pd\nfrom pandas import Series, DataFrame\nimport pandas_profiling\nimport scipy.stats as stats\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.rc(\"font\", size=14)\nplt.rcParams['axes.grid'] = True\nplt.figure(figsize=(6,3))\nplt.gray()\n\nfrom matplotlib.backends.backend_pdf import PdfPages\n\nimport statsmodels.formula.api as sm\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom patsy import dmatrices","e719d55e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\ntitanic_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntitanic_train.head()","7bb3f429":"titanic_train = titanic_train.drop(['Name', 'Ticket'],axis = 1)\ntitanic_train","0545658a":"titanic_train['Survived'] = titanic_train.Survived.astype(str)","cc3dd4a0":"titanic_train","9381ff76":"titanic_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntitanic_test","cb3dab11":"titanic_train.describe()","8f508523":"titanic_train.info()","0bb89ab0":"titanic_test.describe()","b7af1d45":"titanic_test.info()","a93f91af":"titanic_train.columns","e26011d5":"len(titanic_train.columns)","16c43e1c":"titanic_train.dtypes","b9d77418":"titanic_train.isnull()","d4cbf317":"titanic_train.shape","528405fe":"# Plotting correlation between all important features\ncorr = titanic_train.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr, annot=True)\nplt.plot()","6e8c7f71":"titanic_train['Sex'] = pd.factorize(titanic_train.Sex)[0]\ntitanic_train","56d97b59":"titanic_train['Embarked'] = pd.factorize(titanic_train.Embarked)[0]\ntitanic_train","1f713b2b":"numeric_var_names=[key for key in dict(titanic_train.dtypes) if dict(titanic_train.dtypes)[key] in ['float64', 'int64', 'float32', 'int32','uint8']]\ncat_var_names=[key for key in dict(titanic_train.dtypes) if dict(titanic_train.dtypes)[key] in ['object']]\nprint(numeric_var_names)\nprint(cat_var_names)","3649f8d5":"numeric_var_names","de8bfcb2":"cat_var_names","12316d4b":"titanic_train_num = titanic_train[numeric_var_names]\ntitanic_train_num","40736bfd":"titanic_train_cat = titanic_train[cat_var_names]\ntitanic_train_cat","e088bb8a":"# Use a general function that returns multiple values\ndef var_summary(x):\n    return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),  x.std(), x.var(), x.min(), x.dropna().quantile(0.01), x.dropna().quantile(0.05),x.dropna().quantile(0.10),x.dropna().quantile(0.25),x.dropna().quantile(0.50),x.dropna().quantile(0.75), x.dropna().quantile(0.90),x.dropna().quantile(0.95), x.dropna().quantile(0.99),x.max()], \n                  index=['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1' , 'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P99' ,'MAX'])\n\nnum_summary = titanic_train_num.apply(lambda x: var_summary(x)).T","0dbb4d98":"num_summary","03e0ef9f":"#def outlier_capping(x):\n#    x = x.clip_upper(x.quantile(0.99))\n#    x = x.clip_lower(x.quantile(0.01))\n#    return x","3b45fc9f":"#titanic_train_num=titanic_train_num.apply(lambda x: outlier_capping(x))\n#titanic_train_num","e2081925":"def Missing_imputation(x):\n    x = x.fillna(x.median())\n    return x","6cac381e":"titanic_train_num = titanic_train_num.apply(lambda x: Missing_imputation(x))\ntitanic_train_num","0dbc54df":"def Cat_Missing_imputation(x):\n    x = x.fillna(x.mode())\n    return x","88f5ee3e":"titanic_train_cat = titanic_train_cat.apply(lambda x:  Cat_Missing_imputation(x))\ntitanic_train_cat","2ff60b32":"titanic_train_new = pd.concat([titanic_train_num, titanic_train_cat], axis=1)\ntitanic_train_new","e04d3de7":"pandas_profiling.ProfileReport(titanic_train_new)","33dda756":"titanic_train_new.head()","94df4235":"titanic_train_new.describe().T.head(10)","f8a61f12":"titanic_train_num.Fare.hist()","98a10944":"titanic_train_num.Sex.hist()","7a40f4d4":"titanic_train_num.Embarked.hist()","edcd308a":"titanic_train_num.Age.hist()","78c7c7d9":"titanic_train_num.Pclass.hist()","aa5bef2f":"# An utility function to create dummy variable\ndef create_dummies( titanic_train_new, colname ):\n    col_dummies = pd.get_dummies(titanic_train_new[colname], prefix=colname, drop_first=True)\n    #col_dummies.drop(col_dummies.columns[0], axis=1, inplace=True)\n    titanic_train_new = pd.concat([titanic_train_new, col_dummies], axis=1)\n    titanic_train_new.drop( colname, axis = 1, inplace = True )\n    return titanic_train_new","bc3b61a2":"cat_var_names","13d97816":"#for c_feature in categorical_features\ntitanic_train_cat_new = titanic_train_cat\nfor c_feature in cat_var_names:\n    titanic_train_cat_new[c_feature] = titanic_train_cat_new[c_feature].astype('category')\n    titanic_train_cat_new = create_dummies(titanic_train_cat_new , c_feature )","8282a899":"titanic_train_cat_new","6ee78190":"titanic_train_new = pd.concat([titanic_train_num, titanic_train_cat_new], axis=1)\ntitanic_train_new","0c394d99":"titanic_train_new.columns","8243e842":"bp = PdfPages('WOE Plots.pdf')\n\nfor num_variable in titanic_train_new.columns.difference(['Survived_1']):\n    binned = pd.cut(titanic_train_new[num_variable], bins=10, labels=list(range(1,11)))\n    #binned = binned.dropna()\n    odds = titanic_train_new.groupby(binned)['Survived_1'].sum() \/ (titanic_train_new.groupby(binned)['Survived_1'].count()-titanic_train_new.groupby(binned)['Survived_1'].sum())\n    log_odds = np.log(odds)\n    fig,axes = plt.subplots(figsize=(10,4))\n    sns.barplot(x=log_odds.index,y=log_odds)\n    plt.ylabel('Log Odds Ratio')\n    plt.title(str('Logit Plot for identifying if the bucketing is required or not for variable ') + str(num_variable))\n    bp.savefig(fig)\n\nbp.close()","93beac7e":"titanic_train_new.columns","97ff4550":"pandas_profiling.ProfileReport(titanic_train_new)","456e5e08":"import statsmodels.formula.api as sm","1c25822a":"logreg_model = sm.logit('Survived_1~Pclass',data = titanic_train_new).fit()","afc0e28c":"p = logreg_model.predict(titanic_train_new)\np","497612b1":"metrics.roc_auc_score(titanic_train_new['Survived_1'],p)","cb524120":"2*metrics.roc_auc_score(titanic_train_new['Survived_1'],p)-1","aad3752e":"titanic_train_new1 = titanic_train_new.loc[:,[\"Age\",\n\"Cabin_A14\",\n\"Cabin_A16\",\n\"Cabin_A19\",\n\"Cabin_A20\",\n\"Cabin_A23\",\n\"Cabin_A24\",\n\"Cabin_A26\",\n\"Cabin_A31\",\n\"Cabin_A32\",\n\"Cabin_A34\",\n\"Cabin_A36\",\n\"Cabin_A5\",\n\"Cabin_A6\",\n\"Cabin_A7\",\n\"Cabin_B101\",\n\"Cabin_B102\",\n\"Cabin_B18\",\n\"Cabin_B19\",\n\"Cabin_B20\",\n\"Cabin_B22\",\n\"Cabin_B28\",\n\"Cabin_B3\",\n\"Cabin_B30\",\n\"Cabin_B35\",\n\"Cabin_B37\",\n\"Cabin_B38\",\n\"Cabin_B39\",\n\"Cabin_B4\",\n\"Cabin_B41\",\n\"Cabin_B42\",\n\"Cabin_B49\",\n\"Cabin_B5\",\n\"Cabin_B50\",\n\"Cabin_B51_B53_B55\",\n\"Cabin_B57_B59_B63_B66\",\n\"Cabin_B58_B60\",\n\"Cabin_B69\",\n\"Cabin_B71\",\n\"Cabin_B73\",\n\"Cabin_B77\",\n\"Cabin_B78\",\n\"Cabin_B79\",\n\"Cabin_B80\",\n\"Cabin_B82_B84\",\n\"Cabin_B86\",\n\"Cabin_B94\",\n\"Cabin_B96_B98\",\n\"Cabin_C101\",\n\"Cabin_C103\",\n\"Cabin_C104\",\n\"Cabin_C106\",\n\"Cabin_C110\",\n\"Cabin_C111\",\n\"Cabin_C118\",\n\"Cabin_C123\",\n\"Cabin_C124\",\n\"Cabin_C125\",\n\"Cabin_C126\",\n\"Cabin_C128\",\n\"Cabin_C148\",\n\"Cabin_C2\",\n\"Cabin_C22_C26\",\n\"Cabin_C23_C25_C27\",\n\"Cabin_C30\",\n\"Cabin_C32\",\n\"Cabin_C45\",\n\"Cabin_C46\",\n\"Cabin_C47\",\n\"Cabin_C49\",\n\"Cabin_C50\",\n\"Cabin_C52\",\n\"Cabin_C54\",\n\"Cabin_C62_C64\",\n\"Cabin_C65\",\n\"Cabin_C68\",\n\"Cabin_C7\",\n\"Cabin_C70\",\n\"Cabin_C78\",\n\"Cabin_C82\",\n\"Cabin_C83\",\n\"Cabin_C85\",\n\"Cabin_C86\",\n\"Cabin_C87\",\n\"Cabin_C90\",\n\"Cabin_C91\",\n\"Cabin_C92\",\n\"Cabin_C93\",\n\"Cabin_C95\",\n\"Cabin_C99\",\n\"Cabin_D\",\n\"Cabin_D10_D12\",\n\"Cabin_D11\",\n\"Cabin_D15\",\n\"Cabin_D17\",\n\"Cabin_D19\",\n\"Cabin_D20\",\n\"Cabin_D21\",\n\"Cabin_D26\",\n\"Cabin_D28\",\n\"Cabin_D30\",\n\"Cabin_D33\",\n\"Cabin_D35\",\n\"Cabin_D36\",\n\"Cabin_D37\",\n\"Cabin_D45\",\n\"Cabin_D46\",\n\"Cabin_D47\",\n\"Cabin_D48\",\n\"Cabin_D49\",\n\"Cabin_D50\",\n\"Cabin_D56\",\n\"Cabin_D6\",\n\"Cabin_D7\",\n\"Cabin_D9\",\n\"Cabin_E10\",\n\"Cabin_E101\",\n\"Cabin_E12\",\n\"Cabin_E121\",\n\"Cabin_E17\",\n\"Cabin_E24\",\n\"Cabin_E25\",\n\"Cabin_E31\",\n\"Cabin_E33\",\n\"Cabin_E34\",\n\"Cabin_E36\",\n\"Cabin_E38\",\n\"Cabin_E40\",\n\"Cabin_E44\",\n\"Cabin_E46\",\n\"Cabin_E49\",\n\"Cabin_E50\",\n\"Cabin_E58\",\n\"Cabin_E63\",\n\"Cabin_E67\",\n\"Cabin_E68\",\n\"Cabin_E77\",\n\"Cabin_E8\",\n\"Cabin_F2\",\n\"Cabin_F33\",\n\"Cabin_F38\",\n\"Cabin_F4\",\n\"Cabin_F_E69\",\n\"Cabin_F_G63\",\n\"Cabin_F_G73\",\n\"Cabin_G6\",\n\"Cabin_T\",\n\"Embarked\",\n\"Fare\",\n\"Parch\",\n\"PassengerId\",\n\"Pclass\",\n\"Sex\",\n\"SibSp\",\n\"Survived_1\"]]\ntitanic_train_new1","6d05e135":"somersd_df = pd.DataFrame()\nfor num_variable in titanic_train_new1.columns.difference(['Survived_1']):\n    logreg_model = sm.logit(formula = str('Survived_1 ~ ')+str(num_variable), data=titanic_train_new1)\n    result = logreg_model.fit()\n    #result = logit.fit(method='bfgs')\n    y1_score = pd.DataFrame(result.predict())\n    y1_score.columns = ['Score']\n    somers_d = 2*metrics.roc_auc_score(titanic_train_new1['Survived_1'],y1_score) - 1\n    temp = pd.DataFrame([num_variable,somers_d]).T\n    temp.columns = ['Variable Name', 'SomersD']\n    somersd_df = pd.concat([somersd_df, temp], axis=0)\nsomersd_df","a11e3dd9":"somersd_df.sort_values('SomersD', ascending=False, inplace=True)","6bed15fa":"somersd_df","cd812294":"titanic_train_new2 = titanic_train_new.loc[:,[\"Age\",\n\"Cabin_A14\",\n\"Cabin_A16\",\n\"Cabin_A19\",\n\"Cabin_A20\",\n\"Cabin_A23\",\n\"Cabin_A24\",\n\"Cabin_A26\",\n\"Cabin_A31\",\n\"Cabin_A32\",\n\"Cabin_A34\",\n\"Cabin_A36\",\n\"Cabin_A5\",\n\"Cabin_A6\",\n\"Cabin_A7\",\n\"Cabin_B101\",\n\"Cabin_B102\",\n\"Cabin_B18\",\n\"Cabin_B19\",\n\"Cabin_B20\",\n\"Cabin_B22\",\n\"Cabin_B28\",\n\"Cabin_B3\",\n\"Cabin_B30\",\n\"Cabin_B35\",\n\"Cabin_B37\",\n\"Cabin_B38\",\n\"Cabin_B39\",\n\"Cabin_B4\",\n\"Cabin_B41\",\n\"Cabin_B42\",\n\"Cabin_B49\",\n\"Cabin_B5\",\n\"Cabin_B50\",\n\"Cabin_B51_B53_B55\",\n\"Cabin_B57_B59_B63_B66\",\n\"Cabin_B58_B60\",\n\"Cabin_B69\",\n\"Cabin_B71\",\n\"Cabin_B73\",\n\"Cabin_B77\",\n\"Cabin_B78\",\n\"Cabin_B79\",\n\"Cabin_B80\",\n\"Cabin_B82_B84\",\n\"Cabin_B86\",\n\"Cabin_B94\",\n\"Cabin_B96_B98\",\n\"Cabin_C101\",\n\"Cabin_C103\",\n\"Cabin_C104\",\n\"Cabin_C106\",\n\"Cabin_C110\",\n\"Cabin_C111\",\n\"Cabin_C118\",\n\"Cabin_C123\",\n\"Cabin_C124\",\n\"Cabin_C125\",\n\"Cabin_C126\",\n\"Cabin_C128\",\n\"Cabin_C148\",\n\"Cabin_C2\",\n\"Cabin_C22_C26\",\n\"Cabin_C23_C25_C27\",\n\"Cabin_C30\",\n\"Cabin_C32\",\n\"Cabin_C45\",\n\"Cabin_C46\",\n\"Cabin_C47\",\n\"Cabin_C49\",\n\"Cabin_C50\",\n\"Cabin_C52\",\n\"Cabin_C54\",\n\"Cabin_C62_C64\",\n\"Cabin_C65\",\n\"Cabin_C68\",\n\"Cabin_C7\",\n\"Cabin_C70\",\n\"Cabin_C78\",\n\"Cabin_C82\",\n\"Cabin_C83\",\n\"Cabin_C85\",\n\"Cabin_C86\",\n\"Cabin_C87\",\n\"Cabin_C90\",\n\"Cabin_C91\",\n\"Cabin_C92\",\n\"Cabin_C93\",\n\"Cabin_C95\",\n\"Cabin_C99\",\n\"Cabin_D\",\n\"Cabin_D10_D12\",\n\"Cabin_D11\",\n\"Cabin_D15\",\n\"Cabin_D17\",\n\"Cabin_D19\",\n\"Cabin_D20\",\n\"Cabin_D21\",\n\"Cabin_D26\",\n\"Cabin_D28\",\n\"Cabin_D30\",\n\"Cabin_D33\",\n\"Cabin_D35\",\n\"Cabin_D36\",\n\"Cabin_D37\",\n\"Cabin_D45\",\n\"Cabin_D46\",\n\"Cabin_D47\",\n\"Cabin_D48\",\n\"Cabin_D49\",\n\"Cabin_D50\",\n\"Cabin_D56\",\n\"Cabin_D6\",\n\"Cabin_D7\",\n\"Cabin_D9\",\n\"Cabin_E10\",\n\"Cabin_E101\",\n\"Cabin_E12\",\n\"Cabin_E121\",\n\"Cabin_E17\",\n\"Cabin_E24\",\n\"Cabin_E25\",\n\"Cabin_E31\",\n\"Cabin_E33\",\n\"Cabin_E34\",\n\"Cabin_E36\",\n\"Cabin_E38\",\n\"Cabin_E40\",\n\"Cabin_E44\",\n\"Cabin_E46\",\n\"Cabin_E49\",\n\"Cabin_E50\",\n\"Cabin_E58\",\n\"Cabin_E63\",\n\"Cabin_E67\",\n\"Cabin_E68\",\n\"Cabin_E77\",\n\"Cabin_E8\",\n\"Cabin_F2\",\n\"Cabin_F33\",\n\"Cabin_F38\",\n\"Cabin_F4\",\n\"Cabin_F_E69\",\n\"Cabin_F_G63\",\n\"Cabin_F_G73\",\n\"Cabin_G6\",\n\"Cabin_T\",\n\"Embarked\",\n\"Fare\",\n\"Parch\",\n\"PassengerId\",\n\"Pclass\",\n\"Sex\",\n\"SibSp\",\n\"Survived_1\"]]\ntitanic_train_new2","0c592c65":"from sklearn import datasets\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nX = titanic_train_new2[titanic_train_new2.columns.difference(['Survived_1'])]\nlogreg = LogisticRegression()\nrfe = RFE(logreg, 15)\nrfe = rfe.fit(X, titanic_train_new2[['Survived_1']] )\n\nprint(rfe.support_)\nprint(rfe.ranking_)","b28b8676":"X.columns","e60f1186":"# summarize the selection of the attributes\nimport itertools\nfeature_map = [(i, v) for i, v in itertools.zip_longest(X.columns, rfe.get_support())]\n\nfeature_map\n\n#Alternative of capturing the important variables\nRFE_features=X.columns[rfe.get_support()]\n\nselected_features_from_rfe = X[RFE_features]","fb342ac5":"RFE_features","9681e393":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif, mutual_info_classif","1b586e4f":"X = titanic_train_new2[titanic_train_new2.columns.difference(['Survived_1'])]\nX_new = SelectKBest(f_classif, k=15).fit(X, titanic_train_new2[['Survived_1']] )","d0203a8f":"X_new.get_support()","8f52c089":"X_new.scores_","eb4d934c":"# summarize the selection of the attributes\nimport itertools\nfeature_map = [(i, v) for i, v in itertools.zip_longest(X.columns, X_new.get_support())]\n\nfeature_map\n\n#Alternative of capturing the important variables\nKBest_features=X.columns[X_new.get_support()]\n\nselected_features_from_KBest = X[KBest_features]","ba235274":"KBest_features","6ebdd7e0":"X = pd.concat([titanic_train_new2[titanic_train_new2.columns.difference(['Survived_1'])],titanic_train_new2['Survived_1']], axis=1)\nfeatures = \"+\".join(titanic_train_new2.columns.difference(['Survived_1']))\nX.head()","6e927ce3":"features","0765ae16":"a,b = dmatrices(formula_like='Survived_1 ~ '+ 'Age+Cabin_A14+Cabin_A16+Cabin_A19+Cabin_A20+Cabin_A23+Cabin_A24+Cabin_A26+Cabin_A31+Cabin_A32+Cabin_A34+Cabin_A36+Cabin_A5+Cabin_A6+Cabin_A7+Cabin_B101+Cabin_B102+Cabin_B18+Cabin_B19+Cabin_B20+Cabin_B22+Cabin_B28+Cabin_B3+Cabin_B30+Cabin_B35+Cabin_B37+Cabin_B38+Cabin_B39+Cabin_B4+Cabin_B41+Cabin_B42+Cabin_B49+Cabin_B5+Cabin_B50+Cabin_B51_B53_B55+Cabin_B57_B59_B63_B66+Cabin_B58_B60+Cabin_B69+Cabin_B71+Cabin_B73+Cabin_B77+Cabin_B78+Cabin_B79+Cabin_B80+Cabin_B82_B84+Cabin_B86+Cabin_B94+Cabin_B96_B98+Cabin_C101+Cabin_C103+Cabin_C104+Cabin_C106+Cabin_C110+Cabin_C111+Cabin_C118+Cabin_C123+Cabin_C124+Cabin_C125+Cabin_C126+Cabin_C128+Cabin_C148+Cabin_C2+Cabin_C22_C26+Cabin_C23_C25_C27+Cabin_C30+Cabin_C32+Cabin_C45+Cabin_C46+Cabin_C47+Cabin_C49+Cabin_C50+Cabin_C52+Cabin_C54+Cabin_C62_C64+Cabin_C65+Cabin_C68+Cabin_C7+Cabin_C70+Cabin_C78+Cabin_C82+Cabin_C83+Cabin_C85+Cabin_C86+Cabin_C87+Cabin_C90+Cabin_C91+Cabin_C92+Cabin_C93+Cabin_C95+Cabin_C99+Cabin_D+Cabin_D10_D12+Cabin_D11+Cabin_D15+Cabin_D17+Cabin_D19+Cabin_D20+Cabin_D21+Cabin_D26+Cabin_D28+Cabin_D30+Cabin_D33+Cabin_D35+Cabin_D36+Cabin_D37+Cabin_D45+Cabin_D46+Cabin_D47+Cabin_D48+Cabin_D49+Cabin_D50+Cabin_D56+Cabin_D6+Cabin_D7+Cabin_D9+Cabin_E10+Cabin_E101+Cabin_E12+Cabin_E121+Cabin_E17+Cabin_E24+Cabin_E25+Cabin_E31+Cabin_E33+Cabin_E34+Cabin_E36+Cabin_E38+Cabin_E40+Cabin_E44+Cabin_E46+Cabin_E49+Cabin_E50+Cabin_E58+Cabin_E63+Cabin_E67+Cabin_E68+Cabin_E77+Cabin_E8+Cabin_F2+Cabin_F33+Cabin_F38+Cabin_F4+Cabin_F_E69+Cabin_F_G63+Cabin_F_G73+Cabin_G6+Cabin_T+Embarked+Parch+PassengerId+Sex+SibSp', data = titanic_train_new2, return_type='dataframe')\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(b.values, i) for i in range(b.shape[1])]\nvif[\"features\"] = b.columns\n\nprint(vif)","761b6ebb":"vif.to_csv(\"vif.csv\")","d2abaf5e":"#for logistic regression using statsmodels\ntrain1, test1 = train_test_split(titanic_train_new2, test_size=0.3, random_state=0)","085a3bad":"train1","a566928f":"import statsmodels.formula.api as sm\nimport sklearn.metrics as metrics","f670a73b":"logreg = sm.logit(formula='Survived_1 ~ Age+Sex+Cabin_A5+Cabin_A6+Cabin_A7+Cabin_B3+Cabin_B4+Cabin_B5+Embarked+PassengerId+SibSp', data = train1)\nresult = logreg.fit()","126bd0f5":"print(result.summary())","d72c3fa6":"train_gini = 2*metrics.roc_auc_score(train1['Survived_1'], result.predict(train1)) - 1\nprint(\"The Gini Index for the model built on the Train Data is : \", train_gini)\n\ntest_gini = 2*metrics.roc_auc_score(test1['Survived_1'], result.predict(test1)) - 1\nprint(\"The Gini Index for the model built on the Test Data is : \", test_gini)\n\ntrain_auc = metrics.roc_auc_score(train1['Survived_1'], result.predict(train1))\ntest_auc = metrics.roc_auc_score(test1['Survived_1'], result.predict(test1))\n\nprint(\"The AUC for the model built on the Train Data is : \", train_auc)\nprint(\"The AUC for the model built on the Test Data is : \", test_auc)\n                                 ","451dde73":"## Intuition behind ROC curve - predicted probability as a tool for separating the '1's and '0's\ntrain_predicted_prob = pd.DataFrame(result.predict(train1))\ntrain_predicted_prob.columns = ['prob']\ntrain_actual = train1['Survived_1']\n# making a DataFrame with actual and prob columns\ntrain_predict = pd.concat([train_actual, train_predicted_prob], axis=1)\ntrain_predict.columns = ['actual','prob']\ntrain_predict.head()","c84146f2":"## Intuition behind ROC curve - predicted probability as a tool for separating the '1's and '0's\ntest_predicted_prob = pd.DataFrame(result.predict(test1))\ntest_predicted_prob.columns = ['prob']\ntest_actual = test1['Survived_1']\n# making a DataFrame with actual and prob columns\ntest_predict = pd.concat([test_actual, test_predicted_prob], axis=1)\ntest_predict.columns = ['actual','prob']\ntest_predict.head()","62371344":"## Intuition behind ROC curve - confusion matrix for each different cut-off shows trade off in sensitivity and specificity\nroc_like_df = pd.DataFrame()\ntrain_temp = train_predict.copy()\n\nfor cut_off in np.linspace(0,1,50):\n    train_temp['cut_off'] = cut_off\n    train_temp['predicted'] = train_temp['prob'].apply(lambda x: 0.0 if x < cut_off else 1.0)\n    train_temp['tp'] = train_temp.apply(lambda x: 1.0 if x['actual']==1.0 and x['predicted']==1 else 0.0, axis=1)\n    train_temp['fp'] = train_temp.apply(lambda x: 1.0 if x['actual']==0.0 and x['predicted']==1 else 0.0, axis=1)\n    train_temp['tn'] = train_temp.apply(lambda x: 1.0 if x['actual']==0.0 and x['predicted']==0 else 0.0, axis=1)\n    train_temp['fn'] = train_temp.apply(lambda x: 1.0 if x['actual']==1.0 and x['predicted']==0 else 0.0, axis=1)\n    sensitivity = train_temp['tp'].sum() \/ (train_temp['tp'].sum() + train_temp['fn'].sum())\n    specificity = train_temp['tn'].sum() \/ (train_temp['tn'].sum() + train_temp['fp'].sum())\n    accuracy = (train_temp['tp'].sum()  + train_temp['tn'].sum() ) \/ (train_temp['tp'].sum() + train_temp['fn'].sum() + train_temp['tn'].sum() + train_temp['fp'].sum())\n    roc_like_table = pd.DataFrame([cut_off, sensitivity, specificity, accuracy]).T\n    roc_like_table.columns = ['cutoff', 'sensitivity', 'specificity', 'accuracy']\n    roc_like_df = pd.concat([roc_like_df, roc_like_table], axis=0)","cef96be9":"roc_like_df","feb43609":"## Finding ideal cut-off for checking if this remains same in OOS validation\nroc_like_df['total'] = roc_like_df['sensitivity'] + roc_like_df['specificity']","ae16df4c":"roc_like_df.head()","78eea0ae":"#Cut-off based on highest sum(sensitivity+specicity)   - common way of identifying cut-off\nroc1=roc_like_df[roc_like_df['total']==roc_like_df['total'].max()]\nroc1","4839b57a":"#Cut-off based on highest accuracy   - some teams use this as methodology to decide the cut-off\nroc2=roc_like_df[roc_like_df['accuracy']==roc_like_df['accuracy'].max()]\nroc2","48b6a4ef":"#Cut-off based on highest sensitivity\nroc3=roc_like_df[roc_like_df['sensitivity']==roc_like_df['sensitivity'].max()]\nroc3","dc1b56d4":"#Choosen Best Cut-off is 0.367347 based on highest (sensitivity+specicity)\n\ntest_predict['predicted'] = test_predict['prob'].apply(lambda x: 1 if x > 0.367347 else 0)\ntrain_predict['predicted'] = train_predict['prob'].apply(lambda x: 1 if x > 0.367347 else 0)","1b0781a7":"test_predict.head()","c803ba7e":"train_predict.head()","4f1e0acb":"sns.heatmap(pd.crosstab(train_predict['actual'], train_predict['predicted']), annot=True, fmt='.0f')\nplt.title('Train Data Confusion Matrix')\nplt.show()\nsns.heatmap(pd.crosstab(test_predict['actual'], test_predict['predicted']), annot=True, fmt='.0f')\nplt.title('Test Data Confusion Matrix')\nplt.show()","c1ee079f":"print(\"The overall accuracy score for the Train Data is : \", metrics.accuracy_score(train_predict.actual, train_predict.predicted))\nprint(\"The overall accuracy score for the Test Data  is : \", metrics.accuracy_score(test_predict.actual, test_predict.predicted))","178d8c59":"print(metrics.classification_report(train_predict.actual, train_predict.predicted))","e4989520":"print(metrics.classification_report(test_predict.actual, test_predict.predicted))","428ffa61":"train_predict['Deciles']=pd.qcut(train_predict['prob'],10, labels=False)\n\ntrain_predict.head()","f1263a19":"test_predict['Deciles']=pd.qcut(test_predict['prob'],10, labels=False)\n\ntest_predict.head()","a4816dc5":"# Decile Analysis for train data\n\nno_1s = train_predict[['Deciles','actual']].groupby(train_predict.Deciles).sum().sort_index(ascending=False)['actual']\nno_total = train_predict[['Deciles','actual']].groupby(train_predict.Deciles).count().sort_index(ascending=False)['actual']\nmax_prob = train_predict[['Deciles','prob']].groupby(train_predict.Deciles).max().sort_index(ascending=False)['prob']\nmin_prob = train_predict[['Deciles','prob']].groupby(train_predict.Deciles).min().sort_index(ascending=False)['prob']","949d5cb6":"Decile_analysis_train1 = pd.concat([max_prob, min_prob, no_1s, no_total-no_1s, no_total], axis=1)\n\nDecile_analysis_train1.columns = ['max_prob','min_prob','#1','#0','total']","e9a31180":"Decile_analysis_train1","fa38afdd":"# Decile Analysis for train data\n\nno_1s = test_predict[['Deciles','actual']].groupby(test_predict.Deciles).sum().sort_index(ascending=False)['actual']\nno_total = test_predict[['Deciles','actual']].groupby(test_predict.Deciles).count().sort_index(ascending=False)['actual']\nmax_prob = test_predict[['Deciles','prob']].groupby(test_predict.Deciles).max().sort_index(ascending=False)['prob']\nmin_prob = test_predict[['Deciles','prob']].groupby(test_predict.Deciles).min().sort_index(ascending=False)['prob']\n\nDecile_analysis_test1 = pd.concat([max_prob, min_prob, no_1s, no_total-no_1s, no_total], axis=1)\n\nDecile_analysis_test1.columns = ['max_prob','min_prob','#1','#0','total']","a1a6c756":"Decile_analysis_test1","65f2dcb3":"Handling missings - Method2","2e2d377d":"Variable Reduction using Recursive Feature Elimination\n\n\nRecursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features.","bb7457ce":"creating data auditing report","8713b2f0":"Handling Outliers - Method2","165551d9":"Accuracy Metrics","ca16637f":"Handling missings - Method2","4d1b10a6":"Variable Reduction using univariate Regression (short list based on Somer's D values)","6b9b1ceb":"Model Building","95269ce6":"Variable reduction using Select K-Best technique\u00b6","f09fb39f":"Variance Inflation Factor assessment\u00b6\u00b6","c420e85b":"Variable reduction using WOE or log(odds)\n\n\nIdentify important variables using WOE or log(odds) comparing with Y Variable Transformation: (i) Bucketing if the variables are not having linear relationship with log(odds)"}}