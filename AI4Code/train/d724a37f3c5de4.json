{"cell_type":{"43be25f1":"code","c60cf757":"code","22560a17":"code","932a6f01":"code","4f8344f7":"code","74cf03b5":"code","849fa199":"code","24717b87":"code","559b7a68":"code","81642511":"code","371f55da":"code","4892c447":"code","10d16096":"code","c834a5fc":"code","dfcb6409":"code","10344964":"code","fdd797c5":"code","6e173fc3":"code","bfd74266":"code","4c76ef6d":"code","1c41a0c0":"code","36bdea1d":"code","4c7f80d7":"code","c1e90963":"code","4c43a0a8":"code","872b5b17":"code","11fb359e":"markdown","6adeb09d":"markdown","d3535c32":"markdown","7b997e4d":"markdown","6d5f2595":"markdown","77c61052":"markdown","78883d29":"markdown","aaeac57f":"markdown","5f6c3b6b":"markdown","d89e3a79":"markdown","328d2eae":"markdown","4b2c0d95":"markdown"},"source":{"43be25f1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","c60cf757":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","22560a17":"df.info()\n","932a6f01":"df.describe()\n","4f8344f7":"non_fraud = len(df[df.Class == 0])\nfraud = len(df[df.Class == 1])\nfraud_percent = (fraud \/ (fraud + non_fraud)) * 100\nprint(\"Number of Genuine transactions: \", non_fraud)\nprint(\"Number of Fraud transactions: \", fraud)\nprint(\"Percentage of Fraud transactions: {:.4f}\".format(fraud_percent))\n","74cf03b5":"labels = [\"Genuine\", \"Fraud\"]\ncount_classes = df.value_counts(df['Class'], sort= True)\ncount_classes.plot(kind = \"bar\", rot = 0)\nplt.title(\"Visualization of Labels\")\nplt.ylabel(\"Count\")\nplt.xticks(range(2), labels)\nplt.show()","849fa199":"from sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\ndf['scaled_amount'] = rs.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rs.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time', 'Amount'], axis=1, inplace=True)\nscaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(0, 'scaled_time', scaled_time)\ndf.head()","24717b87":"x = df.drop([\"Class\"], axis= 1)\ny = df[\"Class\"]\nfrom sklearn.model_selection import train_test_split\n(x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size= 0.2, random_state= 42)\nprint(\"Shape of train_X: \", x_train.shape)\nprint(\"Shape of test_X: \", x_test.shape)\n","559b7a68":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve, accuracy_score\ndef metrics(actuals, predictions):\n    print(\"Accuracy: {:.5f}\".format(accuracy_score(actuals, predictions)))\n    print(\"Precision: {:.5f}\".format(precision_score(actuals, predictions)))\n    print(\"Recall: {:.5f}\".format(recall_score(actuals, predictions)))\n    print(\"F1-score: {:.5f}\".format(f1_score(actuals, predictions)))\n\n","81642511":"def result (model, x_train, y_train, x_test, y_test):\n    x = model()\n    x.fit(x_train, y_train)\n    y_pred = x.predict(x_test)\n    cnf_matrix = confusion_matrix(y_test, y_pred)\n    print()\n    metrics(y_test, y_pred.round())\n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n    plt.ylabel('Actual Label')\n    plt.xlabel('Predicted Label')\n","371f55da":"print('Evaluation of BaggingClassifier  without SMOTE')\nresult(BaggingClassifier, x_train, y_train, x_test, y_test)","4892c447":"print('Evaluation of DecisionTreeClassifier  without SMOTE')\nresult(DecisionTreeClassifier, x_train, y_train, x_test, y_test)","10d16096":"print('Evaluation of RandomForestClassifier without SMOTE')\n\nresult(RandomForestClassifier, x_train, y_train, x_test, y_test)","c834a5fc":"print('Evaluation of AdaBoostClassifier  Befor SMOTE')\n\nresult(AdaBoostClassifier, x_train, y_train, x_test, y_test)","dfcb6409":"from imblearn.over_sampling import SMOTE\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nx_train_s, y_train_s = sm.fit_resample(x_train, y_train)\n\nprint('After OverSampling, the shape of train_x: {}'.format(x_train_s.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_s.shape))\n\nprint(\"After OverSampling, counts of label '1', %: {}\".format(sum(y_train_s==1)\/len(y_train_s)*100.0,2))\nprint(\"After OverSampling, counts of label '0', %: {}\".format(sum(y_train_s==0)\/len(y_train_s)*100.0,2))\n\nsns.countplot(x=y_train_s, data=df, palette='CMRmap')\n","10344964":"def result (model, x_train, y_train, x_test, y_test):\n    x = model()\n    x.fit(x_train, y_train)\n    y_pred = x.predict(x_test)\n    cnf_matrix = confusion_matrix(y_test, y_pred)\n    print()\n    metrics(y_test, y_pred.round())\n\n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n    plt.ylabel('Actual Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n    \n    y_pred_prob = x.predict_proba(x_test)[:,1]\n    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n    plt.plot(precision, recall)\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision Recall Curve')\n    plt.show()\n","fdd797c5":"print('Evaluation of BaggingClassifier After SMOTE')\nresult(BaggingClassifier, x_train_s, y_train_s, x_test, y_test)","6e173fc3":"print('Evaluation of DecisionTreeClassifier After SMOTE')\nresult(DecisionTreeClassifier, x_train_s, y_train_s, x_test, y_test)","bfd74266":"print('Evaluation of RandomForestClassifier After SMOTE')\nresult(RandomForestClassifier, x_train_s, y_train_s, x_test, y_test)","4c76ef6d":"print('Feature importance ranking\\n\\n')\nrf= RandomForestClassifier()\nrf.fit(x_train_s, y_train_s)\ny_pred = rf.predict(x_test)\nimportances = rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_],axis=0)\nindices = np.argsort(importances)[::-1]\nvariables = df.columns\nimportance_list = []\nfor f in range(x.shape[1]):\n    variable = variables[indices[f]]\n    importance_list.append(variable)\n    print(\"%d.%s(%f)\" % (f + 1, variable, importances[indices[f]]))\nplt.figure(figsize=(20, 8))\nplt.title(\"Feature importances\")\nplt.bar(importance_list, importances[indices],\n       color=\"purple\", yerr=std[indices], align='center')\n","1c41a0c0":"print('Evaluation of AdaBoostClassifier After SMOTE')\nresult(AdaBoostClassifier, x_train_s, y_train_s, x_test, y_test)","36bdea1d":"n_inputs = x_train_s.shape[1]","4c7f80d7":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nmodel = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')])\nmodel.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(x_train_s, y_train_s, validation_split=0.2, batch_size=300, epochs=25, shuffle=True, verbose=2)\n","c1e90963":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.plot()\n","4c43a0a8":"score = model.evaluate(x_test, y_test)\nprint(score)\n","872b5b17":"y_pred= np.argmax(model.predict(x_test), axis=-1) \nprint('Evaluation of Neural Network Model After SMOT')\nprint()\nmetrics(y_test, y_pred.round())\nnn_f1_score = f1_score(y_test, y_pred)\nnn_cnf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(nn_cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\nplt.ylabel('Actual Label')\nplt.xlabel('Predicted Label')\n","11fb359e":"- Let's know what are the features are importance in RandomForestClassifier model.","6adeb09d":"- Now we will do the same thing with all model so I decided to make a function instead of repeat all these lines 5 times","d3535c32":"## 4. Applying Models With SMOTE:\n### We will use the same function but add a simple part to print \"Precision Recall Curve\"","7b997e4d":"- The genuine transactions are over 99%!.\n\n- Let\u2019s apply scaling techniques on the features \u201cAmount - Time\u201d to transform the range of values. We drop the original \u201cAmount - Time\u201d columns and add a new columns with the scaled values.","6d5f2595":"## Introduction: \n- This Notebook for beginners who wants to know how we handle this data and make a prediction\n- first import some libraries \n","77c61052":"### 4. Neural Network","78883d29":"## 2. Scaling features","aaeac57f":"## 1. Load The data","5f6c3b6b":"## Conclusion:\n- Implementing SMOTE Technique on our imbalanced dataset helped us to balanced the data.\n- Try more than algorithm and compare between them.\n- After comparison between models, we found that RandomForestClassifier (Befor and After) SMOTE gave us the best result.","d89e3a79":"- Now the data is ready, Let's Try some models without SMOTE","328d2eae":"We import the models and make a function to print the classification report for each model.","4b2c0d95":"## 3. Try The model without SMOTE"}}