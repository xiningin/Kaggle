{"cell_type":{"b6f687e3":"code","cb214099":"code","7a34bddb":"code","6a0a0607":"code","3bb75e05":"code","f3d908cf":"code","cc056aa3":"code","2b61050a":"code","88f09df6":"code","8f2127f1":"code","de5ce8fe":"code","32f608ba":"code","035c9848":"code","f711a504":"code","b44408dd":"code","50c9021f":"markdown","850aae28":"markdown","f9a962e2":"markdown"},"source":{"b6f687e3":"# ETL is credit to @ inversion kernel\nimport numpy as np  \nimport pandas as pd  \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n \nimport os\nprint(os.listdir('..\/input\/'))","cb214099":"from sklearn import preprocessing\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold","7a34bddb":"train_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')","6a0a0607":"train_transaction.head()","3bb75e05":"train_identity.head()","f3d908cf":"train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(\"shape of train is .....\"+str(train.shape))\nprint(\"shape of test is .....\"+str(test.shape))\n\ny_train = train['isFraud'].copy()\n\n# Drop target, fill in NaNs\nX_train = train.drop('isFraud', axis=1)\nX_test = test.copy()\nX_train = X_train.fillna(-999)\nX_test = X_test.fillna(-999)","cc056aa3":"from matplotlib import pyplot as plt \ny_train = train['isFraud'].copy()\nplt.hist(y_train);\nplt.title('isFraud distribution');","2b61050a":"del train, test, train_transaction, train_identity, test_transaction, test_identity","88f09df6":"# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))   ","8f2127f1":"param = {'num_leaves': 120,\n         'metric': 'auc',\n         'objective': 'binary',\n         'is_unbalance': True,  \n         'max_depth': -1,\n         'n_estimators': 50,\n         'learning_rate': 0.05,\n         \"min_child_samples\": 30,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_fraction\": 0.9 ,\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}","de5ce8fe":"def train_lgb(df_train,target ,df_test,features, params) :\n# define folds for cross validation\n    folds= KFold(n_splits=10,random_state=820)\n    oof = np.zeros(len(df_train))\n    predictions=  np.zeros(len(df_test))\n    features_importance= pd.DataFrame({'Feature':[], 'Importance':[]})\n    for fold, (trn_idx, val_idx) in enumerate(folds.split(df_train, target)): \n        print(\" fold nb:{}\".format(fold))\n        train_df= lgb.Dataset(df_train.iloc[trn_idx][features], label=target[trn_idx])\n        validation_df= lgb.Dataset(data=df_train.iloc[val_idx][features], label=target[val_idx])\n\n        num_round = 500 # you might change this number \n        clf = lgb.train(params,train_df, num_round,valid_sets=[train_df,validation_df],verbose_eval=100, early_stopping_rounds = 100)\n        oof[val_idx]= clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n        fold_importance_df= pd.DataFrame({'Feature':[], 'Importance':[]})\n        fold_importance_df['Feature']= features\n        fold_importance_df['Importance']= clf.feature_importance()\n        fold_importance_df[\"fold\"] = fold + 1\n        features_importance = pd.concat([features_importance, fold_importance_df], axis=0)\n\n        predictions += clf.predict(df_test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n    \n    return clf, predictions, features_importance, oof\n","32f608ba":"features = X_train.columns\nclf, predictions, features_importance, oof= train_lgb(X_train, y_train.values ,X_test,features, param)","035c9848":"import seaborn as sns\nfrom matplotlib import pyplot as plt\ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"Feature\", \"Importance\"]].groupby(\"Feature\").mean().sort_values(by=\"Importance\", ascending=False)[:10].index\n    best_features = feature_importance_df_[[\"Feature\", \"Importance\"]].groupby(\"Feature\").mean().sort_values(by=\"Importance\", ascending=False)[:50]\n    best_features.reset_index(inplace=True)\n    print(best_features.dtypes)\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"Importance\", y=\"Feature\", data=best_features)\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    \n","f711a504":"display_importances(features_importance)","b44408dd":"sample_submission['isFraud'] = predictions\nsample_submission.to_csv('submission.csv')","50c9021f":"as expected we are going to deal with highly unbalanced data","850aae28":"##### Target","f9a962e2":"### Merge all training data in one  tables "}}