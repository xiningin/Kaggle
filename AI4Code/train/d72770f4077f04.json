{"cell_type":{"62ef4f32":"code","4c7657b5":"code","fcfa6065":"code","56d34cdf":"code","0809856e":"code","bb5f5358":"code","90715721":"code","da93bf01":"code","4c7285c8":"code","06b99b9d":"code","6fa81bb3":"code","8971b998":"code","d9d4e804":"code","cbf91d63":"code","9add1495":"code","d72b85bf":"markdown","430eb108":"markdown","e215fe1d":"markdown","069c1222":"markdown","921903dd":"markdown","12f1dec0":"markdown","41631fc3":"markdown","5d538ca2":"markdown","a2bc5ec0":"markdown"},"source":{"62ef4f32":"import re\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow_hub as hub\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.notebook import tqdm\nfrom collections import OrderedDict\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\n\nnp.random.seed(0)\ntf.random.set_seed(0)\nnp.set_printoptions(suppress=True)\nprint(tf.__version__)","4c7657b5":"!pip install ..\/input\/sacremoses > \/dev\/null\n\nsys.path.insert(0, \"..\/input\/transformers\/\")\nfrom transformers import *\nlogging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)","fcfa6065":"PATH = '..\/input\/google-quest-challenge\/'\nMAX_SEQUENCE_LENGTH = 256\nNUM_GENERATE = 20000\nNUM_PRODUCE_PER_SAMPLE = 20\n\ncore = 'bert'\nBERT_PATH = '..\/input\/bert-base-uncased-huggingface-transformer\/'\n# config = BertConfig()\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n# Model = TFBertModel\n\nmodule_url = \"..\/input\/universalsentenceencoderlarge4\/\"\nembed = hub.load(module_url)\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","56d34cdf":"def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    \n    def return_id(str1, str2, truncation_strategy, length):\n\n        inputs = tokenizer.encode_plus(str1, str2,\n            add_special_tokens=True,\n            max_length=length,\n            truncation_strategy=truncation_strategy)\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids_q, input_masks_q, input_segments_q = return_id(\n        title + ' ' + question, None, 'longest_first', max_sequence_length)\n    \n    input_ids_a, input_masks_a, input_segments_a = return_id(\n        answer, None, 'longest_first', max_sequence_length)\n    \n    input_ids_qa, input_masks_qa, input_segments_qa = return_id(\n        title + ' ' + question, answer, 'longest_first', max_sequence_length)\n    \n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a,\n            input_ids_qa, input_masks_qa, input_segments_qa]\n\n\ndef compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    input_ids_qa, input_masks_qa, input_segments_qa = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        inputs = _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n        ids_q, masks_q, segments_q = inputs[:3]\n        ids_a, masks_a, segments_a = inputs[3:6]\n        ids_qa, masks_qa, segments_qa = inputs[6:]\n        \n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n        \n        input_ids_qa.append(ids_qa)\n        input_masks_qa.append(masks_qa)\n        input_segments_qa.append(segments_qa)\n        \n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(input_masks_q, dtype=np.int32), \n            np.asarray(input_segments_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(input_masks_a, dtype=np.int32), \n            np.asarray(input_segments_a, dtype=np.int32),\n            np.asarray(input_ids_qa, dtype=np.int32), \n            np.asarray(input_masks_qa, dtype=np.int32), \n            np.asarray(input_segments_qa, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","0809856e":"def _tokenize_raw(title, question, answer, tokenizer):\n\n    def return_id(str1, str2):\n        inputs = tokenizer.encode_plus(str1, str2, add_special_tokens=True)\n        input_ids = inputs['input_ids']\n        input_masks = [1]*len(input_ids)\n        input_segments = inputs['token_type_ids']\n        return [input_ids, input_masks, input_segments]\n\n    input_ids_q, input_masks_q, input_segments_q = return_id(title + ' ' + question, None)\n    input_ids_a, input_masks_a, input_segments_a = return_id(answer, None)\n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a]\n\n\ndef _crop_or_pad(ids, masks, segments, max_length, padding_id=0):\n    seq_length = len(ids)\n\n    if seq_length <= max_length:\n        padding_length = max_length - seq_length\n        ids += [padding_id] * padding_length\n        masks += [0] * padding_length\n        segments += [0] * padding_length\n    else:\n        i_start = np.random.randint(0, seq_length-max_length)\n        i_end = i_start + max_length\n        ids = ids[i_start: i_end]\n        masks = masks[i_start: i_end]\n        segments = segments[i_start: i_end]\n\n    return [ids, masks, segments]\n\n\ndef augment_arrays(df, columns, tokenizer, max_sequence_length, \n                   num_generate=10000):\n    num_samples = len(df)\n    tmp_q, tmp_a = [], []\n    # Full tokenization\n    for _, instance in tqdm(df[columns].iterrows(), desc='processing raw sequences ...'):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n        inputs = _tokenize_raw(t, q, a, tokenizer) # get full tokens\n        tmp_q.append(inputs[:3]) # [ids_q, masks_q, segments_q]\n        tmp_a.append(inputs[3:]) # [ids_a, masks_a, segments_a]\n\n    # Population by reproducing sample\n    pad_id = tokenizer.pad_token_id\n    sample_indexes = np.random.choice(np.arange(num_samples), size=num_generate)\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    for i in tqdm(sample_indexes, desc='populating training data ...'):\n        ids_q, masks_q, segments_q = _crop_or_pad(*tmp_q[i], max_sequence_length, pad_id)\n        ids_a, masks_a, segments_a = _crop_or_pad(*tmp_a[i], max_sequence_length, pad_id)\n\n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n\n    # Concatenation\n    inputs = [np.asarray(input_ids_q, dtype=np.int32), \n              np.asarray(input_masks_q, dtype=np.int32), \n              np.asarray(input_segments_q, dtype=np.int32),\n              np.asarray(input_ids_a, dtype=np.int32), \n              np.asarray(input_masks_a, dtype=np.int32), \n              np.asarray(input_segments_a, dtype=np.int32)]\n\n    return inputs, sample_indexes\n\n\ndef compute_test_arrays(df_test, columns, tokenizer, \n                        max_sequence_length, n_produce_per_sample=10):\n    tmp_q, tmp_a = [], []\n    # Full tokenization\n    for _, instance in tqdm(df_test[columns].iterrows(), desc='processing raw sequences ...'):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n        inputs = _tokenize_raw(t, q, a, tokenizer) # get full tokens\n        tmp_q.append(inputs[:3]) # [ids_q, masks_q, segments_q]\n        tmp_a.append(inputs[3:]) # [ids_a, masks_a, segments_a]\n\n    pad_id = 0 # tokenizer.pad_token_id\n    test_indexes = []\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    for i, (raw_q, raw_a) in enumerate(tqdm(zip(tmp_q, tmp_a), desc='populating test data ...')):\n        for _ in range(n_produce_per_sample):\n            ids_q, masks_q, segments_q = _crop_or_pad(*raw_q, max_sequence_length, pad_id)\n            ids_a, masks_a, segments_a = _crop_or_pad(*raw_a, max_sequence_length, pad_id)\n\n            input_ids_q.append(ids_q)\n            input_masks_q.append(masks_q)\n            input_segments_q.append(segments_q)\n\n            input_ids_a.append(ids_a)\n            input_masks_a.append(masks_a)\n            input_segments_a.append(segments_a)\n\n            test_indexes.append(i)\n\n    # Concatenation\n    inputs = [np.asarray(input_ids_q, dtype=np.int32), \n              np.asarray(input_masks_q, dtype=np.int32), \n              np.asarray(input_segments_q, dtype=np.int32),\n              np.asarray(input_ids_a, dtype=np.int32), \n              np.asarray(input_masks_a, dtype=np.int32), \n              np.asarray(input_segments_a, dtype=np.int32)]\n    test_indexes = np.asarray(test_indexes, dtype=np.int32)\n\n    return inputs, test_indexes","bb5f5358":"# Count feature functions\ndef split_sentence(sentence):\n    return re.split('[\\.\\?]+\\s*', sentence)\n\ndef count_wps(sentence):\n    sentence_list = split_sentence(sentence)\n    w_per_s = np.mean([len(sentence.split()) for sentence in sentence_list\n                       if len(sentence) > 0])\n    return w_per_s\n\ndef build_count_features(df):\n    count_features = OrderedDict()\n\n    count_features['n_title_words'] = df.question_title.apply(lambda x: len(x.split()))\n    count_features['n_body_words'] = df.question_body.apply(lambda x: len(x.split()))\n    count_features['n_answer_words'] = df.answer.apply(lambda x: len(x.split()))\n\n    count_features['n_body_sentences'] = df.question_body.apply(lambda x: len(split_sentence(x)))\n    count_features['n_answer_sentences'] = df.answer.apply(lambda x: len(split_sentence(x)))\n\n    count_features['n_title_wps'] = df.question_title.apply(count_wps)\n    count_features['n_body_wps'] = df.question_body.apply(count_wps)\n    count_features['n_answer_wps'] = df.answer.apply(count_wps)\n\n    df_count_features = pd.DataFrame(count_features)\n    return df_count_features\n\n\n# Host feature function\ndef build_host_features(df, df_host_count, popular_host_thr):\n    def _convert(x):\n        if x not in df_host_count.index:\n            return 'other'\n        if df_host_count[x] > popular_host_thr:\n            return x\n        else:\n            return 'other'\n    return df.host.apply(_convert)\n\ndef build_features(df, df_host_count, popular_host_thr):\n    return pd.concat(\n        [build_count_features(df),\n         build_host_features(df, df_host_count, 100)],\n        axis=1)\n\n# universal sentence encoder feature\ndef build_use_feature(df, columns):\n    qa_embeds = []\n    for _, instance in tqdm(df[columns].iterrows(), desc='Calculating USE feature ...'):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n        q_embed, a_embed = embed([t+' '+q, a])['outputs'].numpy()\n        qa_embed = np.concatenate([q_embed, a_embed], axis=-1)\n        qa_embeds.append(qa_embed)\n\n    return np.asarray(qa_embeds, dtype=np.float32)","90715721":"def compute_spearmanr(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, batch_size, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        \n        self.batch_size = batch_size\n        self.fold = fold\n        self.best_rho = -1\n        self.best_weights = None\n        self.best_epoch = 0\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        rho_val = compute_spearmanr(\n            self.valid_outputs,\n            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n\n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n\n        if rho_val > self.best_rho:\n            self.best_rho = rho_val\n            self.best_weights = self.model.get_weights()\n            self.best_epoch = epoch+1\n        else:\n            self.model.stop_training = True\n            self.model.set_weights(self.best_weights)\n            self.model.save_weights(f'bert-base-{self.fold+1}fold-{self.best_epoch}epoch.h5')\n            print(f'Training stopped with rho: {self.best_rho}')\n            \n            \ndef aggregate_values(values, indexes):\n    df = pd.DataFrame(values, index=indexes)\n    aggregated = df.groupby(level=0).mean().values\n    return aggregated\n\n\nclass FrequentCallback(tf.keras.callbacks.Callback):\n    def __init__(self, valid_data, valid_indexes,\n                 batch_size, validation_step, \n                 patients=2, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.valid_indexes = valid_indexes\n        \n        self.batch_size = batch_size\n        self.validation_step = validation_step\n        self.patients = patients\n        self.wait = 0\n        self.stop_training = False\n        self.fold = fold\n\n        self.current_epoch = 1\n        self.best_rho = -1\n        self.best_weights = None\n        self.best_batch = 0\n        self.best_epoch = 0\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n\n    def on_train_batch_end(self, batch, logs={}):\n        if (batch+1)%self.validation_step == 0:\n            preds = self.model.predict(self.valid_inputs, batch_size=self.batch_size)\n\n            preds_agg = aggregate_values(preds, self.valid_indexes)\n            outs_agg = aggregate_values(self.valid_outputs, self.valid_indexes)\n            rho_val = compute_spearmanr(outs_agg, preds_agg)\n            \n            print(f'\\n Validation rho at {batch+1}batch: {rho_val}')\n            \n            if rho_val > self.best_rho:\n                self.best_rho = rho_val\n                self.best_weights = self.model.get_weights()\n                self.best_batch = batch+1\n                self.best_epoch = self.current_epoch\n                self.wait = 0\n            else:\n                self.wait += 1\n                if self.wait >= self.patients:\n                    self.model.stop_training = True\n                    print(f'\\tLoss saturation detected')\n\n    def on_epoch_end(self, epoch, logs={}):\n        self.current_epoch += 1\n\n    def on_train_end(self, logs={}):\n        print(f'\\n***** Best validation rho: {self.best_rho} at {self.best_epoch}epoch, {self.best_batch}batch. *****')\n        self.model.set_weights(self.best_weights)\n        filename = f'bert-base-{self.fold+1}fold-{self.best_batch}batch-{self.best_epoch}epoch.h5'\n        self.model.save_weights(filename)","da93bf01":"# (title+body, answer, title+body+answer)x3 + count-feature\ndef create_model_10inputs():\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    qa_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    qa_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    qa_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n                                   \n    feat_count = tf.keras.layers.Input((8, ), dtype=tf.float32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    # bert_model = TFBertForSequenceClassification.from_pretrained(\n    #     BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n    bert_model = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5',\n                                             config=config)\n    \n    # will only use the transformer (\"bert\") from TFBertForSequencabseClassification\n    # if config.output_hidden_states = True, obtain hidden states via .bert(...)[-1]\n    # q_embedding = bert_model.bert(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    # a_embedding = bert_model.bert(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    qa_embedding = bert_model(qa_id, attention_mask=qa_mask, token_type_ids=qa_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    qa = tf.keras.layers.GlobalAveragePooling1D()(qa_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a, qa])\n    x = tf.keras.layers.Concatenate()([x, feat_count]) # pre-dropout\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn,\n                                          a_id, a_mask, a_atn,\n                                          qa_id, qa_mask, qa_atn,\n                                          feat_count], \n                                  outputs=x)\n    return model, bert_model","4c7285c8":"# Original (title+question, answer)x3 model\ndef create_model_6inputs():\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    bert_model = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5',\n                                             config=config)\n    \n    # will only use the transformer (\"bert\") from TFBertForSequencabseClassification\n    # if config.output_hidden_states = True, obtain hidden states via .bert(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a])\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn,\n                                          a_id, a_mask, a_atn,], \n                                  outputs=x)\n    return model, bert_model","06b99b9d":"# (title+question, answer)x3 + count-feature + USE-feature\ndef create_model_8inputs():\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\n    feat_count = tf.keras.layers.Input((8, ), dtype=tf.float32)\n\n    qa_use = tf.keras.layers.Input((512*2, ), dtype=tf.float32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    # bert_model = TFBertForSequenceClassification.from_pretrained(\n    #     BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n    bert_model = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5', \n                                             config=config)\n    \n    # will only use the transformer (\"bert\") from TFBertForSequencabseClassification\n    # if config.output_hidden_states = True, obtain hidden states via .bert(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a, feat_count, qa_use]) # (None, 768x2+8+512x2)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn,\n                                          a_id, a_mask, a_atn,\n                                          feat_count, qa_use], \n                                  outputs=x)\n    \n    return model, bert_model","6fa81bb3":"# Build count-based features\ninputs_count = build_count_features(df_train).apply(np.log1p)\ntest_inputs_count = build_count_features(df_test).apply(np.log1p)\n\n# Standardize\nss = StandardScaler()\ninputs_count = ss.fit_transform(inputs_count)\ntest_inputs_count = ss.transform(test_inputs_count)\n\n# inputs_count = np.asarray(inputs_count, dtype=np.float32)\ninputs_count = np.asarray(inputs_count, dtype=np.float32)\ntest_inputs_count = np.asarray(test_inputs_count, dtype=np.float32)\nprint(f'\\nCount-based feature shape (train): {inputs_count.shape}')\nprint(f'\\nCount-based feature shape (test): {test_inputs_count.shape}')\n\n# Build USE feature\ninputs_use = build_use_feature(df_train, input_categories)\ntest_inputs_use = build_use_feature(df_test, input_categories)\nprint(f'\\nUSE feature shape (train): {inputs_use.shape}')\nprint(f'\\nUSE feature shape (test): {test_inputs_use.shape}')","8971b998":"# outputs = compute_output_arrays(df_train, output_categories)\n# inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n# test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)","d9d4e804":"# '''\ninputs, sample_indexes = augment_arrays(df_train, input_categories, tokenizer,\n                                        MAX_SEQUENCE_LENGTH, NUM_GENERATE)\ninputs.append(inputs_count[sample_indexes])\ninputs.append(inputs_use[sample_indexes])\n\noutputs = compute_output_arrays(df_train, output_categories)\noutputs = outputs[sample_indexes]\n\ntest_inputs, test_indexes = compute_test_arrays(df_test, input_categories, tokenizer,\n                                                MAX_SEQUENCE_LENGTH, NUM_PRODUCE_PER_SAMPLE)\ntest_inputs.append(test_inputs_count[test_indexes])\ntest_inputs.append(test_inputs_use[test_indexes])\n\n# check input shape \nfor i, (inp, test_inp) in enumerate(zip(inputs, test_inputs)):\n    print(f'input-{i+1} shape: train: {inp.shape}, test:{test_inp.shape}')\n\nprint(f'# of unique train samples: {len(set(sample_indexes))}')\nprint(f'# of unique test samples: {len(set(test_indexes))}')\n# '''","cbf91d63":"n_splits = 10\nepochs = 3\nbatch_size = 8\npatients = 3\nlearning_rate = 3e-5\nn_validate_per_epoch = 5\n\n# gkf = GroupKFold(n_splits=n_splits).split(X=df_train.question_body, groups=df_train.question_body)\ngkf = GroupKFold(n_splits=n_splits).split(X=sample_indexes, groups=sample_indexes)\n\nprint(f'Consume {len(inputs)} inputs')\n\nmodel, _ = create_model_8inputs()\noptimizer = tf.keras.optimizers.Adam(learning_rate)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer)\ninit_weights = model.get_weights()\n\ntest_preds = []\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    model.set_weights(init_weights)\n    # will actually only do 2 folds (out of 5) to manage < 2h\n    # if fold in [0]:\n    if True:\n        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n        train_outputs = outputs[train_idx]\n        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n        valid_outputs = outputs[valid_idx]\n        unique_train_idx = set(sample_indexes[train_idx])\n        unique_valid_idx = set(sample_indexes[valid_idx])\n        overlap_idx = unique_train_idx&unique_valid_idx\n        print(f'# of unique train samples: {len(unique_train_idx)}',\n              f'# of unique valid samples: {len(unique_valid_idx)}',\n              f'# of overlap samples: {len(overlap_idx)}', sep='\\n')\n\n        callback = FrequentCallback(\n            valid_data=(valid_inputs, valid_outputs),\n            valid_indexes=valid_idx,\n            batch_size=batch_size,\n            validation_step=int(len(train_idx)\/batch_size\/n_validate_per_epoch),\n            patients=patients,\n            fold=fold\n        )\n        model.fit(train_inputs, train_outputs, \n                  epochs=epochs,\n                  batch_size=batch_size,\n                  callbacks=[callback])\n        \n        t_preds = model.predict(test_inputs, batch_size=batch_size)\n        t_preds_agg = aggregate_values(t_preds, test_indexes)\n        test_preds.append(t_preds_agg)\n        print(f'Test prediction for {fold+1}fold conpleted.\\n')","9add1495":"df_sub.iloc[:, 1:] = np.average(test_preds, axis=0) # for weighted average set weights=[...]\n\ndf_sub.to_csv('submission.csv', index=False)","d72b85bf":"#### 6. Process and submit test predictions\n\nAverage fold predictions, then save as `submission.csv`","430eb108":"#### 4. Obtain inputs and targets, as well as the indices of the train\/validation splits","e215fe1d":"### Bert-base TensorFlow 2.0\n\nThis kernel does not explore the data. For that you could check out some of the great EDA kernels: [introduction](https:\/\/www.kaggle.com\/corochann\/google-quest-first-data-introduction), [getting started](https:\/\/www.kaggle.com\/phoenix9032\/get-started-with-your-questions-eda-model-nn) & [another getting started](https:\/\/www.kaggle.com\/hamditarek\/get-started-with-nlp-lda-lsa). This kernel is an example of a TensorFlow 2.0 Bert-base implementation, using ~~TensorFow Hub~~ Huggingface transformer. <br><br>\n\n---\n**Update 1 (Commit 7):**\n* removing penultimate dense layer; now there's only one dense layer (output layer) for fine-tuning\n* using BERT's sequence_output instead of pooled_output as input for the dense layer\n---\n\n**Update 2 (Commit 8):**\n* adjusting `_trim_input()` --- now have a q_max_len and a_max_len, instead of 'keeping the ratio the same' while trimming.\n* **importantly:** now also includes question_title for the input sequence\n---\n\n**Update 3 (Commit 9)**\n<br><br>*A lot of experiments can be made with the title + body + answer sequence. Feel free to look into e.g. (1) inventing new tokens (add it to '..\/input\/path-to-bert-folder\/assets\/vocab.txt'), (2) keeping \\[SEP\\] between title and body but modify `_get_segments()`, (3) using the \\[PAD\\] token, or (4) merging title and body without any kind of separation. In this commit I'm doing (2). I also tried (3) offline, and they both perform better than in commit 8, in terms of validation rho.*<br>\n\n* ignoring first \\[SEP\\] token in `_get_segments()`.\n\n---\n\n**Update 4 (Commit 11)**\n* **Now using Huggingface transformer instead of TFHub** (note major changes in the code). This creates the possibility to easily try out different architectures like XLNet, Roberta etc. As well as easily outputting the hidden states of the transformer.\n* two separate inputs (title+body and answer) for BERT\n* removed snapshot average (now only using last (third) epoch). This will likely decrease performance, but it's not feasible to use ~ 5 x 4 models for a single bert prediction in practice. \n* only training for 2 epochs instead of 3 (to manage 2h limit)\n---\n\nFork\n\n---\n\n**Update 1**\n* three separate inputs (title+body, answer, title+body+answer) for BERT\n* concat count-based features with BERT-features and feed into NNs\n---","069c1222":"#### 2.1 Randomize inputs","921903dd":"#### 2.2 Additional inputs","12f1dec0":"#### 1. Read data and tokenizer\n\nRead tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)","41631fc3":"#### 5. Training, validation and testing\n\nLoops over the folds in gkf and trains each fold for 3 epochs --- with a learning rate of 3e-5 and batch_size of 6. A simple binary crossentropy is used as the objective-\/loss-function. ","5d538ca2":"#### 3. Create model\n\n`compute_spearmanr()` is used to compute the competition metric for the validation set\n<br><br>\n`create_model()` contains the actual architecture that will be used to finetune BERT to our dataset.\n","a2bc5ec0":"#### 2. Preprocessing functions\n\nThese are some functions that will be used to preprocess the raw text data into useable Bert inputs.<br>\n\n*update 4:* credits to [Minh](https:\/\/www.kaggle.com\/dathudeptrai) for this implementation. If I'm not mistaken, it could be used directly with other Huggingface transformers too! Note that due to the 2 x 512 input, it will require significantly more memory when finetuning BERT."}}