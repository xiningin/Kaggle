{"cell_type":{"9545f9ae":"code","3efa26f0":"code","c759a761":"code","ad35be15":"code","886f10e0":"code","145e1a5c":"code","4866aefa":"markdown","a3676e99":"markdown","b0f69cc5":"markdown","3a386824":"markdown"},"source":{"9545f9ae":"from keras.layers import *\nfrom keras.models import Model, load_model\nfrom keras.datasets import mnist\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras import regularizers\nfrom keras import optimizers\nfrom keras import backend as K\nimport scipy\nimport matplotlib.pyplot as plt\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox, TextArea\nimport pandas as pd\nimport numpy as np","3efa26f0":"x_train = pd.read_csv('..\/input\/fashion-mnist_train.csv')\nx_test = pd.read_csv('..\/input\/fashion-mnist_test.csv')\n\ny_train = x_train.pop('label')\ny_test = x_test.pop('label')\n\nx_train = np.array(x_train).reshape(-1, 28, 28) \/ 255.0\nx_test = np.array(x_test).reshape(-1, 28, 28) \/ 255.0","c759a761":"labeldict = {\n    0: 'T-shirt\/top',\n    1: 'Trouser',\n    2: 'Pullover',\n    3: 'Dress',\n    4: 'Coat',\n    5: 'Sandal',\n    6: 'Shirt',\n    7: 'Sneaker',\n    8: 'Bag',\n    9: 'Ankle boot'\n}\n\nfig, ax = plt.subplots(1, 10, figsize=(20, 2))\nfor i in range(10):\n    ax[i].imshow(x_train[i], cmap='gray')\n    ax[i].set_title(labeldict[y_train[i]])","ad35be15":"x_train = x_train.reshape(-1, 28, 28, 1)\nx_test = x_test.reshape(-1, 28, 28, 1)\n\ndef make_and_fit():\n    inputs = Input(shape=(28, 28, 1))\n\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(32, (2, 2), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(16, (2, 2), activation='relu', padding='same')(x)\n    x = Conv2D(4, (2, 2), activation='relu', padding='same')(x)\n    x = Conv2D(1, (2, 2), activation='relu', padding='same')(x)\n    x = Flatten()(x)\n    encoded = Dense(2, activation='relu')(x)\n\n    encoder = Model(inputs=inputs, outputs=encoded)\n    \n    encoded_inputs = Input(shape=(2,))\n\n    x = Dense(4, activation='relu')(encoded_inputs)\n    x = Reshape((2, 2, 1))(x)\n    x = Conv2D(4, (2, 2), activation='relu', padding='same')(x)\n    x = Conv2D(16, (2, 2), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = UpSampling2D((7, 7))(x)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\n    decoder = Model(inputs=encoded_inputs, outputs=decoded)\n    \n    x = encoder(inputs)\n    x = decoder(x)\n    model = Model(inputs=inputs, outputs=x)\n    model.compile(optimizer=optimizers.Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy', 'mse'])\n\n    print(model.summary())\n    \n    clr = ReduceLROnPlateau(\n        monitor='loss',\n        factor=0.5,\n        patience=3,\n        min_delta=0.01,\n        cooldown=0,\n        min_lr=1e-7,\n        verbose=1)\n\n    model.fit(\n        x_train,\n        x_train,\n        batch_size=256,\n        epochs=50,\n        shuffle=True,\n        validation_data=(x_test, x_test),\n        callbacks=[clr])\n\n    return model, encoder, decoder\n\nmodel, encoder, decoder = make_and_fit()","886f10e0":"def get_triple(inputs):\n    latent_repr = encoder.predict(inputs)\n    outputs = decoder.predict(latent_repr)\n    latent_repr = latent_repr.reshape((latent_repr.shape[0], latent_repr.shape[1], 1))\n\n    return inputs, latent_repr, outputs\n\ndef show_encodings(inputs, latent_repr, outputs):\n    n = len(inputs)\n    fig, axes = plt.subplots(2, n, figsize=(2*n, 5))\n    for i in range(n):\n        axes[1, i].set_title('({0:.2f}, {1:.2f})'.format(float(latent_repr[i, 0]), float(latent_repr[i, 1])))\n        axes[0, i].imshow(inputs[i].reshape(28, 28), cmap='gray')\n        axes[1, i].imshow(outputs[i].reshape(28, 28), cmap='gray')\n    for ax in axes.flatten():\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n    \nshow_encodings(*get_triple(x_test[:10]))\ninputs = np.random.random(size=(10, 4, 4, 1))\ninputs = scipy.ndimage.zoom(inputs, (1, 7, 7, 1))\nshow_encodings(*get_triple(inputs))","145e1a5c":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\ndef plot_latent(mode, count, technique):\n    idx = np.random.choice(len(x_test), count)\n    inputs = x_test[idx]\n    fig, ax = plt.subplots(figsize=(10, 7))\n    ax.set_title(technique)\n    if technique == 'autoencoder':\n        coords = encoder.predict(inputs)\n    elif technique == 'pca':\n        coords = PCA(n_components=2).fit_transform(inputs.reshape(count, -1))\n    elif technique == 'tsne':\n        coords = TSNE(n_components=2).fit_transform(inputs.reshape(count, -1))\n\n    if mode == 'imgs':\n        for image, (x, y) in zip(inputs, coords):\n            im = OffsetImage(image.reshape(28, 28), zoom=1, cmap='gray')\n            ab = AnnotationBbox(im, (x, y), xycoords='data', frameon=False)\n            ax.add_artist(ab)\n        ax.update_datalim(coords)\n        ax.autoscale()\n    elif mode == 'dots':\n        classes = y_test[idx]\n        plt.scatter(coords[:, 0], coords[:, 1], c=classes)\n        plt.colorbar()\n        for i in range(10):\n            class_center = np.mean(coords[classes == i], axis=0)\n            text = TextArea('{} ({})'.format(labeldict[i], i))\n            ab = AnnotationBbox(text, class_center, xycoords='data', frameon=True)\n            ax.add_artist(ab)\n    plt.show()\n\nplot_latent('dots', 10000, 'autoencoder')\nplot_latent('dots', 10000, 'pca')\nplot_latent('dots', 2000, 'tsne')\n\nplot_latent('imgs', 300, 'autoencoder')\nplot_latent('imgs', 300, 'pca')\nplot_latent('imgs', 300, 'tsne')","4866aefa":"Let's build an autoencoder to perform dimensionality reduction on the Fashion MNIST dataset!\n\nFirst, we'll load the training and test set and display a few of the images to make sure everything is loaded correctly.","a3676e99":"Now that the network is trained, let's visualize the results. Here are two sets of images that the model encodes then decodes. The first is taken out of the test set, the second is random noise.\n\nBetween the two images is displayed the encoded representation of the input images (two variables).","b0f69cc5":"Now, let's compare this approach to other common dimensionality reduction techniques: PCA and T-SNE. We will do this by reducing the 784-dimensional space of the images to two using a technique we want to test. Then we will plot the distribution of the ten classes in the new two-dimensional space and see how separated they are.","3a386824":"Now we can get to building the autoencoder. I decided to give it only two latent variables, so that we can compare this approach to other dimensionality reduction techniques by visualizing the images on a plane."}}