{"cell_type":{"8eebdb06":"code","eb18d589":"code","a4fe5935":"code","bcc99a45":"code","9437571b":"code","b8f03111":"code","a5ebe88a":"code","af2b9083":"code","b2db024c":"code","5723f832":"code","a00fda26":"code","563387e3":"code","22a93aa0":"code","337e3ae4":"code","bda36668":"code","092b9ce0":"code","a3416e8b":"code","12c6614b":"code","97a43006":"code","4d3e57be":"code","c313ebe9":"code","4fd41112":"code","236ea60f":"code","913cc9c9":"code","50ab3f34":"code","f35af9aa":"markdown","e7afb317":"markdown","e76e98a1":"markdown","c1bafa99":"markdown","3a24e959":"markdown","574ab7c9":"markdown","13687939":"markdown","7eb4e86b":"markdown","695dfefb":"markdown","1a10baf1":"markdown","5c94ac5a":"markdown","651fe3e0":"markdown","f510c4cf":"markdown","7932055a":"markdown","4b24e835":"markdown","ffc7b405":"markdown","ed4908fa":"markdown","8d754324":"markdown","cb071abf":"markdown","43c40c14":"markdown","46652454":"markdown","2c5e0d90":"markdown","7a7c2886":"markdown"},"source":{"8eebdb06":"!pip install fairlearn","eb18d589":"import fairlearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgb\n\n# from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\nfrom sklearn.metrics import balanced_accuracy_score, roc_auc_score\n\nfrom imblearn.ensemble import BalancedBaggingClassifier\n\nfrom fairlearn.widget import FairlearnDashboard\nfrom fairlearn.reductions import ExponentiatedGradient, DemographicParity\nfrom fairlearn.metrics import (\n    group_summary, selection_rate, selection_rate_group_summary,\n    demographic_parity_difference, demographic_parity_ratio,\n    balanced_accuracy_score_group_summary, roc_auc_score_group_summary,\n    equalized_odds_difference, difference_from_summary)\n\n# Helper functions\ndef get_metrics_df(models_dict, y_true, group):\n    metrics_dict = {\n        \"Overall selection rate\": (\n            lambda x: selection_rate(y_true, x), True),\n        \"Demographic parity difference\": (\n            lambda x: demographic_parity_difference(y_true, x, sensitive_features=group), True),\n        \"Demographic parity ratio\": (\n            lambda x: demographic_parity_ratio(y_true, x, sensitive_features=group), True),\n        \"-----\": (lambda x: \"\", True),\n        \"Overall balanced error rate\": (\n            lambda x: 1-balanced_accuracy_score(y_true, x), True),\n        \"Balanced error rate difference\": (\n            lambda x: difference_from_summary(\n                balanced_accuracy_score_group_summary(y_true, x, sensitive_features=group)), True),\n        \"Equalized odds difference\": (\n            lambda x: equalized_odds_difference(y_true, x, sensitive_features=group), True),\n        \"------\": (lambda x: \"\", True),\n        \"Overall AUC\": (\n            lambda x: roc_auc_score(y_true, x), False),\n        \"AUC difference\": (\n            lambda x: difference_from_summary(\n                roc_auc_score_group_summary(y_true, x, sensitive_features=group)), False),\n    }\n    df_dict = {}\n    for metric_name, (metric_func, use_preds) in metrics_dict.items():\n        df_dict[metric_name] = [metric_func(preds) if use_preds else metric_func(scores) \n                                for model_name, (preds, scores) in models_dict.items()]\n    return pd.DataFrame.from_dict(df_dict, orient=\"index\", columns=models_dict.keys())","a4fe5935":"df_orig = pd.read_csv('\/kaggle\/input\/promotion.csv')\ndf_orig.head()","bcc99a45":"df = df_orig.copy()\ndf.dtypes","9437571b":"obj_columns = df.select_dtypes(['object']).columns\nfor c in obj_columns:\n    df[c] = df[c].astype('category')\n\ncat_columns = df.select_dtypes(['category']).columns\ndf[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n\ndf.dtypes","b8f03111":"df.isnull().sum()","a5ebe88a":"df['education'] = df['education'].fillna(df['education'].mode()[0])\ndf['previous_year_rating'] = df['previous_year_rating'].fillna(df['previous_year_rating'].mode()[0])\n\nprint(\"Number of missing values:\", df.isnull().sum().sum())","af2b9083":"plt.rcParams['figure.figsize'] = (15, 5)\nplt.style.use('seaborn-white')\n\ndf['is_promoted'].value_counts().plot(kind = 'pie',\n                                      autopct = '%.2f%%',\n                                      startangle = 90,\n                                      labels = ['Not promoted','Promoted'],\n                                      pctdistance = 0.5)\nplt.axis('off')\n\nplt.suptitle('Target Class Balance', fontsize = 16)\nplt.show()","b2db024c":"# Creating a label vector and a feature vector\nX = df.drop([\"employee_id\",\"is_promoted\"], axis=1)\ny = df[\"is_promoted\"]\n\nA = df_orig['department']\n\nX_train, X_test, y_train, y_test, A_train, A_test = train_test_split(X, \n                                                                     y, \n                                                                     A,\n                                                                     test_size = 0.2,\n                                                                     random_state=0,\n                                                                     stratify=y)","5723f832":"plt.rcParams['figure.figsize'] = (15, 5)\nplt.style.use('seaborn-white')\n\nplt.subplot(1, 2, 1)\n\ny_train.value_counts().plot(kind = 'pie',\n                            autopct = '%.2f%%',\n                            startangle = 90,\n                            labels = ['Not promoted','Promoted'],\n                            pctdistance = 0.5)\n\nplt.xlabel('Training dataset', fontsize = 14)\n\nplt.subplot(1, 2, 2)\n\ny_test.value_counts().plot(kind = 'pie',\n                           autopct = '%.2f%%',\n                           startangle = 90,\n                           labels = ['Not promoted','Promoted'],\n                           pctdistance = 0.5)\n\nplt.xlabel('Testing dataset', fontsize = 14)\n\nplt.suptitle('Target Class Balance', fontsize = 16)\nplt.show()","a00fda26":"clf = BalancedBaggingClassifier()","563387e3":"cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(clf, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint('Mean ROC AUC: %.3f' % np.mean(scores))","22a93aa0":"clf.fit(X_train, y_train)\n\ntest_scores = clf.predict_proba(X_test)[:, 1]\n\n# Predictions (0 or 1) on test set\ny_pred = (test_scores >= np.mean(y_test)) * 1\n\nprint('Mean ROC AUC: %.3f' % roc_auc_score(y_test, test_scores))","337e3ae4":"importances = np.mean([est.steps[1][1].feature_importances_ for est in clf.estimators_], axis=0)\n\nindices = np.argsort(importances)[::-1]\n\n# Plot the impurity-based feature importances of the forest\nplt.figure()\nplt.style.use('seaborn-white')\nplt.title(\"Feature importances\", fontsize = 16)\nplt.bar(range(X.shape[1]), importances[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), X.columns, rotation='vertical')\nplt.xlim([-1, X.shape[1]])\nplt.show()","bda36668":"models_dict = {\"Unmitigated\": (y_pred, test_scores)}\nget_metrics_df(models_dict, y_test, A_test)","092b9ce0":"gs = group_summary(roc_auc_score, y_test, y_pred, sensitive_features=A_test)\ngs","a3416e8b":"plt.figure()\nplt.style.use('seaborn-white')\nplt.title(\"AUC per group before mitigating model biases\", fontsize = 16)\nplt.bar(range(len(gs[\"by_group\"])), list(gs[\"by_group\"].values()), align='center')\nplt.xticks(range(len(gs[\"by_group\"])), list(gs[\"by_group\"].keys()))\nplt.ylim(0, 1)\nplt.show()","12c6614b":"srg = selection_rate_group_summary(y_test, y_pred, sensitive_features=A_test)\n\nplt.figure()\nplt.style.use('seaborn-white')\nplt.title(\"Selection rate per group before mitigating model biases\", fontsize = 16)\nplt.bar(range(len(srg[\"by_group\"])), list(srg[\"by_group\"].values()), align='center')\nplt.xticks(range(len(srg[\"by_group\"])), list(srg[\"by_group\"].keys()))\nplt.ylim(0, 1)\nplt.show()","97a43006":"FairlearnDashboard(sensitive_features=A_test, sensitive_feature_names=['department'],\n                   y_true=y_test,\n                   y_pred={\"Unmitigated\": y_pred})","4d3e57be":"lgb_params = {\n    'objective' : 'binary',\n    'metric' : 'auc',\n    'learning_rate': 0.03,\n    'num_leaves' : 10,\n    'max_depth' : 3,\n    'random_state': 9\n}\n\nnp.random.seed(0)  # set seed for consistent results with ExponentiatedGradient\n\nconstraint = DemographicParity()\nclf = lgb.LGBMClassifier(**lgb_params)\nmitigator = ExponentiatedGradient(clf, constraint)\nmitigator.fit(X_train, y_train, sensitive_features=A_train)\n\ny_pred_mitigated = mitigator.predict(X_test)","c313ebe9":"models_dict = {\"ExponentiatedGradient\": (y_pred_mitigated, y_pred_mitigated)}\nget_metrics_df(models_dict, y_test, A_test)","4fd41112":"gs = group_summary(roc_auc_score, y_test, y_pred_mitigated, sensitive_features=A_test)\ngs","236ea60f":"plt.figure()\nplt.style.use('seaborn-white')\nplt.title(\"Group summary after mitigating model biases\", fontsize = 16)\nplt.bar(range(len(gs[\"by_group\"])), list(gs[\"by_group\"].values()), align='center')\nplt.xticks(range(len(gs[\"by_group\"])), list(gs[\"by_group\"].keys()))\nplt.ylim(0, 1)\nplt.show()","913cc9c9":"srg = selection_rate_group_summary(y_test, y_pred_mitigated, sensitive_features=A_test)\n\nplt.figure()\nplt.style.use('seaborn-white')\nplt.title(\"Selection rate per group after mitigating model biases\", fontsize = 16)\nplt.bar(range(len(srg[\"by_group\"])), list(srg[\"by_group\"].values()), align='center')\nplt.xticks(range(len(srg[\"by_group\"])), list(srg[\"by_group\"].keys()))\nplt.ylim(0, 1)\nplt.show()","50ab3f34":"# The widget does not show up in Kaggle Kernels (see examples here: https:\/\/fairlearn.github.io\/quickstart.html)\nFairlearnDashboard(sensitive_features=A_test, sensitive_feature_names=['department'],\n                   y_true=y_test,\n                   y_pred={\"Unmitigated\": y_pred,\n                           \"ExponentiatedGradient\": y_pred_mitigated})","f35af9aa":"According to the global importance plot, the 5 main features are: department, region, education, gender and recruitment channel. It means that these features will have highest impact in predicting employees that are likely to be promoted. To avoid any bias in promotions, it's important to validate that, for example, all departments and regions are treated equally. This is validated in 3.4.","e7afb317":"We will evaluate this model using a repeated stratified k-fold cross-validation with 3 repeats and 10 folds. To evaluate the performance of the model, the mean ROC AUC score across all folds and repeats will be used.","e76e98a1":"The Exponentiated Gradient algorithm significantly reduces the disparity according to multiple metrics. However, the performance metrics (balanced error rate as well as AUC) get worse. Before deploying such a model in practice, it would be important to analyze why we observe such a performance descrease. One of the reasons might be the lack of informativeness of available features for one of the groups of a protected feature `department`.\n\nNote that unlike the unmitigated model, Exponentiated Gradient produces 0\/1 predictions, so its balanced error rate difference is equal to the AUC difference, and its overall balanced error rate is equal to 1 - overall AUC.","c1bafa99":"### 3.2 Split into training and testing datasets <a name=\"split\"><\/a>\n\nTo split the promotion dataset into training and testing sets, we will use `train_test_split` function with `stratify=y`. The `stratify` parameter makes a split so that the proportion of values in the training and testing sets is the same as the proportion of values in `y` (i.e. `is_promoted`).\n\nAs mentioned above, `y` (`is_promoted`) is a binary categorical variable and there are 91.48% of zeros and 8.52% of ones. Thus, `stratify=y` will make sure that a random split has 91.48% of 0's and 8.52% of 1's.","3a24e959":"Let's check a global importance of features used by the model. To do so, we will calculate the mean importance across the estimators of the `BalancedBaggingClassifier`.","574ab7c9":"First, we will encode categorical features: department, region, education, gender, recruitment_channel.","13687939":"Finally, we can compare this model with the unmitigated `BalancedBaggingClassifier` model using the `fairlearn` dashboard.","7eb4e86b":"Finally, we can see that the promotion dataset is imbalanced: 91.48% and 8.52% employees were not promoted and promoted, accordingly. This aspect will be considered when splitting the data into training and testing datasets, as well as we will take it into account during the model development.","695dfefb":"## 1. Introduction <a name=\"introduction\"><\/a>\n\n\nNegative attitudes (e.g. stereotypes, prejudice) and unintentional biases (e.g. organisational practices or internalised stereotypes) can provoke a discriminative behaviour in humans. AI systems trained on datasets that include biased human decisions will learn making biased decisions rather than fair decisions. When dealing with sensitive features (e.g. gender, age, nationality, etc.), it is crucial to to ensure fairness of a model before deploying it in a real-world environment.\n\nMicrosoft recently announced a new open-source Python package called [fairlearn](https:\/\/github.com\/fairlearn\/fairlearn) that can be used for analyzing the model's fairness and mitigating any observed unfairness issues. In this Kernel I\u2019ll show some basic functionalities of this package.","1a10baf1":"The mean ROC AUC is acceptable, and we can fit the whole training data to the estimator.","5c94ac5a":"### 2.1 Model bias detection <a name=\"theory-model-bias\"><\/a>\n\nSupported fairness metrics:\n\n* **DemographicParity:** this metric states that the proportion of each segment of a protected feature (e.g. gender) \nshould receive the positive outcome at equal rates.\n\n* **EqualisedOdds** this metric states that the model should correctly identify the positive outcome at equal rates \nacross groups, but also miss-classify the positive outcome at equal rates \nacross groups (creating the same proportion of False Positives across groups).\n\n\n### 2.2 Model bias mitigation <a name=\"theory-model-bias-mitigation\"><\/a>\n\nSupported methods:\n\n* **Model estimation approaches:**\n  - *ExponentiatedGradientReduction:* the reduction approach for achieving fairness in a binary classification setting. \nUnderlying classification method is treated as a black box.\n\n  - *GridSearch:* the reduction approach for achieving fairness in a binary classification and regression settings.\nIf the protected attribute is non-binary, then grid search is not feasible.\n\n<br>\n\n* **Post-processing approaches:**\n\n  - *ThresholdOptimizer:* the postprocessing approach in which the classifier is obtained by applying \ngroup-specific thresholds to the provided estimator. The thresholds are chosen to optimize the provided \nperformance objective (e.g. accuracy score) subject to the provided fairness constraints (e.g. demographic parity).","651fe3e0":"### 3.1 Data loading and pre-processing <a name=\"data\"><\/a>","f510c4cf":"Let's impute the missing values in the data set.","7932055a":"It can be noticed that some of the departments have significantly higher AUC values and higher selection rate in comparison to others. \n\n**We can conclude that the model is likely to overpredict (predict 1 when the true label is 0) and underpredict (predict 0 when the true label is 1). Therefore, the application of such model in a real-world environment may cause discrimination of employees based on departments.**\n\nMore details can be found with an interactive widget `FairlearnDashboard` (see examples here: https:\/\/fairlearn.github.io\/quickstart.html). If the dashboard does not show up in Kaggle Kernels, please open the notebook in Jupyter Notebook.","4b24e835":"## Table of contents\n1. [Introduction](#introduction)<br>\n2. [Which techniques are supported by Fairlearn?](#fairlearn)<br>\n   2.1 [Model bias detection](#theory-model-bias)<br>\n   2.2 [Model bias mitigation](#theory-model-bias-mitigation)<br>\n3. [Example: Let's decide who will be promoted](#example)<br>\n   3.1 [Data loading and pre-processing](#data)<br>\n   3.2 [Split into training and testing datasets](#split)<br>\n   3.3 [Prediction model](#model)<br>\n   3.4 [Is the model fair?](#model-bias)<br>\n   3.5 [Achieving non-discrimination with Fairlearn](#model-bias-mitigation)<br>\n\n**I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated.**","ffc7b405":"The table above shows the overall AUC of 0.89 and the overall balanced error rate of 0.23. Both of these are satisfactory in our application context. \n\n**However, there is a considerable disparity in accuracy rates (as indicated by the balanced error rate difference) and even larger when we consider the equalized-odds difference.**\n\nIn addition, we can also analyze AUC and selection rate per each `department` as shown below.","ed4908fa":"### 3.3 Prediction model <a name=\"model\"><\/a>\n\nLet's start with building a fairness unaware ML model. The imbalanced-learn library provides a classifier called `BalancedBaggingClassifier` that implements a random undersampling strategy on the majority class within a bootstrap sample in order to balance the two classes. ","8d754324":"## 2. Which techniques are supported by Fairlearn? <a name=\"fairlearn\"><\/a>","cb071abf":"### 3.4 Is the model fair? <a name=\"model-bias\"><\/a>\n\nThe `fairlearn` package provides fairness-related metrics that can be compared between groups and for the overall population (see 2.1). The goal is to assure that neither of the departments has substantially larger false-positive rates or false-negative rates than the other groups. Therefore, as a protected (sensitive) feature we will set `department` that has a highest impact on predictions of the trained model.\n\nUsing existing metric definitions from scikit-learn we can evaluate metrics to get a group summary. As the overall performance metric we will apply the area under ROC curve (AUC), which is suited to classification problems with a large imbalance between positive and negative examples.\n\nAs the fairness metric we will use equalized odds and demographic parity explained in 2.1. ","43c40c14":"# Fairlearn - Open-source Python package of Microsoft\n## Are you sure that your model doesn't have any discriminative behaviour?\n\n***Liana Napalkova***\n\n***10 July 2020***","46652454":"### 3.5 Achieving non-discrimination with Fairlearn <a name=\"model-bias-mitigation\"><\/a>\n\nThe `fairlearn` package supports different techniques for mitigating model bias (see 2.2). Examples of these techniques can be found [in this notebook](https:\/\/github.com\/fairlearn\/fairlearn\/blob\/master\/notebooks\/Binary%20Classification%20with%20the%20UCI%20Credit-card%20Default%20Dataset.ipynb).\n\nIn our case, we will create a new model while specifying the fairness constraint called Demographic Parity, because there are disparities between groups of `department`. The Exponentiated Gradient mitigation technique developed by Microsoft fits the provided classifier using Demographic Parity as the objective, leading to a vastly reduced difference in selection rate.\n\nSince `BalancedBaggingClassifier` of imbalanced-learn is not yet supported in `fairlearn`, we will use `LGBMClassifier` as a base model.","2c5e0d90":"The data set contains missing values that should be imputed before training ML models.","7a7c2886":"## 3. Example: Let's decide who will be promoted <a name=\"example\"><\/a>\n\nI will use the HR's promotion dataset to predict employees that are likely to be promoted based on their personal and performance parameters. This is the binary classification task with the target `is_promoted`. The target can be equal to 0 or 1. When `is_promoted` is equal to 0, an employee is not promoted. Otherwise, an employee is considered for the promotion.\n\nThe dataset contains potentially sensitive features, such as gender, age, region and department. Let's see which results we will get using the fairness unaware and the fairness aware ML models."}}