{"cell_type":{"20d007cf":"code","6a166c73":"code","9296b140":"code","11e51151":"code","fbbb1a36":"code","636319ec":"code","b37ebc00":"code","00cc4431":"code","0afc6326":"code","ebf27e9e":"code","eba42bde":"code","9c2a15fb":"code","5da82a1f":"code","d90f0617":"code","40bed5f4":"code","bac1188d":"code","fb982d9d":"code","211b1162":"code","d5a81d53":"code","05ff20c4":"code","8202bc4f":"code","6b04c9a7":"code","689a79a6":"code","1c47d9a8":"code","2d72563b":"code","f0371468":"code","90bcac93":"code","db6bdf9f":"code","f016719b":"code","d224438a":"code","c534c654":"code","2d549468":"code","73d602c1":"code","9756c125":"code","8282ce15":"code","845962e2":"code","297b7921":"code","c2ecfe33":"code","9636393d":"code","0a87d44c":"code","b44f9cea":"code","fce2d1bf":"code","92a7937d":"code","bce09610":"code","3ee881c4":"code","a0ae6057":"code","d0d9abb2":"code","c24afdae":"code","c9d07a47":"code","8a6f7d2c":"code","f3d886f9":"code","4e503486":"code","a28e1fd6":"code","1b838dec":"code","ba156461":"code","e4cb3899":"code","ee83a1f1":"markdown","447c4853":"markdown","ce5b1937":"markdown","646ccbff":"markdown","ebcaf139":"markdown","0a642fa8":"markdown","fd839d7e":"markdown","396124bb":"markdown","38f45047":"markdown","0a4699d6":"markdown","e38abc7c":"markdown","fa5c6de8":"markdown","0205964f":"markdown","9d8bdd4a":"markdown","d14afe3a":"markdown","e257b4c6":"markdown","a790e98a":"markdown","bc5fdbac":"markdown","94696c53":"markdown","b748eeb0":"markdown","051f30d0":"markdown","c50dcf50":"markdown","52b86d7a":"markdown","72f96908":"markdown","96b5e69b":"markdown","85908648":"markdown","95be55c4":"markdown","c175294e":"markdown","397c52c0":"markdown","d72f6017":"markdown","c9166b5a":"markdown","191a4e4e":"markdown","f15f5d71":"markdown","2b5ea329":"markdown","62b1fdb6":"markdown","4cf5c36a":"markdown","732ce39a":"markdown","7c4d48ac":"markdown","314b0523":"markdown","44f8d6bf":"markdown","38683a6f":"markdown","a959ee3f":"markdown","ee9353a3":"markdown","842ea968":"markdown","7c014d5b":"markdown","ac5efae5":"markdown","8b647f76":"markdown","8e7aebc0":"markdown","5f10a44a":"markdown","d6cea584":"markdown","9df577ac":"markdown"},"source":{"20d007cf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\npd.set_option('display.max_columns', 100)","6a166c73":"# Change name of .csv file if necessary\ndf = pd.read_csv('\/kaggle\/input\/eval-lab-1-f464-v2\/train.csv')","9296b140":"df.head()","11e51151":"df.info()","fbbb1a36":"# replace \"?\" with NaN\n# df.replace('?',np.nan, inplace = True)","636319ec":"df.isnull().head(5)","b37ebc00":"# Count number of NaNs in each column\nmissing_count = df.isnull().sum(axis = 0) #axis = 1 to count in each row\nmissing_count[missing_count > 0]","00cc4431":"df_dtype_nunique = pd.concat([df.dtypes, df.nunique()],axis=1)\ndf_dtype_nunique.columns = [\"dtype\",\"unique\"]\ndf_dtype_nunique","0afc6326":"numerical_features = []\n# Convert these numerical features to float\ndf[numerical_features] = df[numerical_features].astype(np.float)","ebf27e9e":"df.dtypes","eba42bde":"df.fillna(df.mean(), inplace = True)","9c2a15fb":"# df['---'].value_counts()","5da82a1f":"# df['---'].value_counts().idxmax()","d90f0617":"#replace the missing 'num-of-doors' values by the most frequent \n# df['num-of-doors'].fillna(df['num-of-doors'].value_counts().idxmax(), inplace = True)","40bed5f4":"df.head()","bac1188d":"df.isnull().any().any()","fb982d9d":"df.describe()","211b1162":"df.describe(include='object')","d5a81d53":"df['type'].unique()","05ff20c4":"df_group = df[['type','feature1','rating']]","8202bc4f":"grouped_test1 = df_group.groupby(['type'],as_index=False).mean()\ngrouped_test1","6b04c9a7":"for i, col in enumerate(df.columns):\n  if col not in ['type','rating']:\n    plt.figure(i)\n    sns.regplot(x=col, y = 'rating', data=df)\n    # print(col)","689a79a6":"sns.boxplot(x = 'rating',y='type', data = df)","1c47d9a8":"df.corr()","2d72563b":"# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=0.5, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\nplt.show()","f0371468":"numerical_features = ['feature1','feature2','feature3','feature4','feature5','feature6','feature7','feature8','feature9','feature10','feature11']\ncategorical_features = ['type']","90bcac93":"X = df[numerical_features + categorical_features].copy()\ny = df['rating'].copy()","db6bdf9f":"X.head()","f016719b":"y.head()","d224438a":"for i, col in enumerate(numerical_features):\n    plt.figure(i)\n    plt.hist(X[col], bins=50)\n    plt.xlabel(col)\n    plt.show()","c534c654":"from statsmodels.graphics.gofplots import qqplot\nfor i, col in enumerate(numerical_features):\n    plt.figure(i)\n    qqplot(X[col], line = 's')\n    plt.xlabel(col)\n    plt.show()","2d549468":"temp_code = {'old':0,'new':1}\nX['type'] = X['type'].map(temp_code)\nX.head()","73d602c1":"from sklearn.model_selection import train_test_split\nimport random\n\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.20,random_state=random.randint(1,500))  #Checkout what does random_state do\n","9756c125":"#TODO\nfrom sklearn.preprocessing import RobustScaler\n\ntransformer1 = RobustScaler().fit(X_train[numerical_features])\ntransformer1.transform(X_train[numerical_features])\n\ntransformer2 = RobustScaler().fit(X_val[numerical_features])\ntransformer2.transform(X_val[numerical_features])\n\nX_train[numerical_features].head()","8282ce15":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis()\nX_train_lda = lda.fit_transform(X_train, y_train)\nprint(X_train_lda)\n\nX_val_lda = lda.transform(X_val)\nprint(X_val_lda)\n# X_train_lda = X_train\n# X_val_lda = X_val","845962e2":"# min_max_scaler_features = ['feature1','feature6','feature8']\n# standard_scaler_features = ['feature4','feature7']\n# robust_scaler_features = ['feature2','feature3','feature5','feature9','feature10','feature11']","297b7921":"X.head()","c2ecfe33":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initialize and train\nclf1 = DecisionTreeClassifier().fit(X_train_lda,y_train)\nclf2 = RandomForestClassifier().fit(X_train_lda,y_train)","9636393d":"from sklearn.metrics import accuracy_score  #Find out what is accuracy_score\nfrom sklearn.metrics import r2_score, mean_squared_error, make_scorer\n\ny_pred_1 = clf1.predict(X_val_lda)\ny_pred_2 = clf2.predict(X_val_lda)\n\nacc1 = accuracy_score(y_pred_1,y_val)*100\nacc2 = accuracy_score(y_pred_2,y_val)*100\n\nprint(\"Accuracy score of clf1: {}\".format(acc1))\nprint(\"Accuracy score of clf2: {}\".format(acc2))\n\nmse1 = mean_squared_error(y_val, y_pred_1)\nmse2 = mean_squared_error(y_val, y_pred_2)\n\nprint(\"Mse score of clf1: {}\".format(mse1))\nprint(\"Mse score of clf2: {}\".format(mse2))","0a87d44c":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\n#TODO\nclf = RandomForestClassifier()        #Initialize the classifier object\n\nparameters = {'n_estimators':[10,30,50,80,100],\n              'criterion': ['gini','entropy']\n              }    #Dictionary of parameters\n\nscorer = make_scorer(mean_squared_error)         #Initialize the scorer using make_scorer\n\ngrid_obj = GridSearchCV(clf,parameters,scoring=scorer)         #Initialize a GridSearchCV object with above parameters,scorer and classifier\n\ngrid_fit = grid_obj.fit(X_train_lda,y_train)        #Fit the gridsearch object with X_train,y_train\n\nbest_clf = grid_fit.best_estimator_         #Get the best estimator. For this, check documentation of GridSearchCV object\n\nunoptimized_predictions = (clf.fit(X_train_lda, y_train)).predict(X_val_lda)      #Using the unoptimized classifiers, generate predictions\noptimized_predictions = best_clf.predict(X_val_lda)        #Same, but use the best estimator\n\nacc_unop = accuracy_score(y_val, unoptimized_predictions)*100       #Calculate accuracy for unoptimized model\nacc_op = accuracy_score(y_val, optimized_predictions)*100         #Calculate accuracy for optimized model\n\nprint(\"Accuracy score on unoptimized model:{}\".format(acc_unop))\nprint(\"Accuracy score on optimized model:{}\".format(acc_op))","b44f9cea":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\n\nreg = LinearRegression().fit(X_train_lda, y_train)\ny_pred1 = reg.predict(X_val_lda)\ny_pred1 = np.rint(y_pred1)\n\nregr = RandomForestRegressor(n_estimators=100).fit(X_train_lda, y_train)\ny_pred2 = regr.predict(X_val_lda)\ny_pred2 = np.rint(y_pred2)\n\nmse1 = mean_squared_error(y_val, y_pred1)\nmse2 = mean_squared_error(y_val, y_pred2)\n\nprint(\"Mse score of clf1: {}\".format(mse1))\nprint(\"Mse score of clf2: {}\".format(mse2))","fce2d1bf":"clf = RandomForestRegressor()        #Initialize the classifier object\n\nparameters = {'n_estimators':[10,30,50,80,100,200,250,300],\n              'criterion': ['mse']\n              }    #Dictionary of parameters\n\nscorer = make_scorer(mean_squared_error)         #Initialize the scorer using make_scorer\n\ngrid_obj = GridSearchCV(clf,parameters,scoring=scorer, verbose = 2)         #Initialize a GridSearchCV object with above parameters,scorer and classifier\n\ngrid_fit = grid_obj.fit(X_train_lda,y_train)        #Fit the gridsearch object with X_train,y_train\n\nbest_clf = grid_fit.best_estimator_         #Get the best estimator. For this, check documentation of GridSearchCV object\n\nprint(best_clf)\n\nunoptimized_predictions = (clf.fit(X_train_lda, y_train)).predict(X_val_lda)      #Using the unoptimized classifiers, generate predictions\noptimized_predictions = best_clf.predict(X_val_lda)        #Same, but use the best estimator\n\nunoptimized_predictions = np.rint(unoptimized_predictions)\noptimized_predictions = np.rint(optimized_predictions)\n\nacc_unop = accuracy_score(y_val, unoptimized_predictions)*100       #Calculate accuracy for unoptimized model\nacc_op = accuracy_score(y_val, optimized_predictions)*100         #Calculate accuracy for optimized model\n\nprint(\"Accuracy score on unoptimized model:{}\".format(acc_unop))\nprint(\"Accuracy score on optimized model:{}\".format(acc_op))\n\nmse1 = mean_squared_error(y_val,unoptimized_predictions)\nmse2 = mean_squared_error(y_val,optimized_predictions)\n\nprint(\"mse unop \",mse1)\nprint(\"mse op\",mse2)","92a7937d":"test = pd.read_csv('\/kaggle\/input\/eval-lab-1-f464-v2\/test.csv')","bce09610":"test.head()","3ee881c4":"test_X = test[numerical_features + categorical_features].copy()","a0ae6057":"test_X.isnull().any().any()","d0d9abb2":"missing_count = test_X.isnull().sum(axis = 0) #axis = 1 to count in each row\nmissing_count[missing_count > 0]","c24afdae":"test_X_dtype_nunique = pd.concat([test_X.dtypes, test_X.nunique()],axis=1)\ntest_X_dtype_nunique.columns = [\"dtype\",\"unique\"]\ntest_X_dtype_nunique","c9d07a47":"test_X.fillna(test_X.mean(), inplace = True)","8a6f7d2c":"test.head()","f3d886f9":"temp_code = {'old':0,'new':1}\ntest_X['type'] = test_X['type'].map(temp_code)\ntest_X.head()","4e503486":"# Make sure that the scaler is same as the one used before\ntransformer = RobustScaler().fit(test_X[numerical_features])\ntransformer.transform(test_X[numerical_features])","a28e1fd6":"test_X = lda.transform(test_X)","1b838dec":"# Enter the classifier\npredicted_val = best_clf.predict(test_X)\npredicted_val = np.rint(predicted_val)","ba156461":"my_submission = pd.DataFrame({'Id': test.id, 'rating': predicted_val})","e4cb3899":"my_submission.to_csv('submission.csv', index=False)","ee83a1f1":"Because the data is normally distributed, I dont think correlation is of any importance any more and We should keep all of the data.","447c4853":"## Data Wrangling","ce5b1937":"<ul>\n<li>\nThe Standard Scaler\n<ol>\n<li>\n(xi\u2013mean(x))\/stdev(x)\n<\/li>\n<li>\nIt assumes that your data follows a Gaussian distribution (Gaussian distribution is the same thing as Normal distribution)\n<\/li>\n<li>\nIf the data is not normally distributed, it\u2019s not recommended to use the Standard Scaler.\n<\/li>\n<\/ol>\n<\/li>\n\n<li>\nThe Min-Max Scaler\n<ol>\n<li>\n(xi\u2013min(x))\/ (max(x)\u2013min(x))\n<\/li>\n<li>\nIt transforms the data so that it\u2019s now in the range you specified. You specifiy the range by passing in a tuple to the feature_range parameter.\n<\/li>\n<li>\nNote that, by default, it transforms the data into a range between 0 and 1 (-1 and 1, if there are negative values).\n<\/li>\n<li>\nIt can be used as an alternative to The Standard Scaler or when the data is not normally distributed.\n<\/li>\n<li>\nIt uses the min and max values, so it\u2019s very sensitive to outliers.\n<\/li>\n<li>\nBe careful not to use it when your data has noticeable outliers. The Robust Scaler below is a lot more suitable for working with outliers.\n<\/li>\n<\/ol>\n<\/li>\n\n<li>\nThe Robust Scaler\n<ol>\n<li>\n(xi\u2013Q1(x))\/( Q3(x)\u2013Q1(x))\n<\/li>\n<li>\nThis usage of interquartiles means that they focus on the parts where the bulk of the data is. This makes them very suitable for working with outliers.\n<\/li>\n<\/ol>\n<\/li>\n\n<\/ul>\n\n<b>Dont just use Robust or MinMax depending on the outliers first check with the histogram. If there are two peaks then it might not be sensible to use Robust Scaler.<\/b>\n\n<i>Reference - https:\/\/medium.com\/@ian.dzindo01\/feature-scaling-in-python-a59cc72147c1\n<\/i>","646ccbff":"**Lets list the data types and number of unique values for each column**","ebcaf139":"## Import Necessary Libraries","0a642fa8":"#### Correct data format\n\nIn Pandas, we use \n<p><b>.dtype()<\/b> to check the data type<\/p>\n<p><b>.astype()<\/b> to change the data type<\/p>","fd839d7e":"Predict categorical data it seems.\n\nId column may be irrelevant to the prediction for other features need to find out which feature should be used.\n\nExcept type all other features seem numerical initially.","396124bb":"The default setting of \"describe\" skips variables of type object. We can apply the method \"describe\" on the variables of type 'object' as follows:","38f45047":"Lots of features predict almost always 3 irrespective of the value. To drop these features or not? Some other features have a greater variance and produce more outputs than 3. What to do with these too? Distribution looks normally distribute.","0a4699d6":"Some of the coulmns have null in them need to handle.\n\nAll of the rating coulmns are present so probably wont need to drop any rows.\n\nData seems well defined as there does not seem to be any object types except the categorical data. So would not need to transform the types.","e38abc7c":"**If there are features with incorrect column type then convert them to the correct data type**","fa5c6de8":"The \"groupby\" method groups data by different categories. The data is grouped based on one or several variables and analysis is performed on the individual groups.","0205964f":"\"True\" stands for missing value, while \"False\" stands for not missing value.","9d8bdd4a":"Data Wrangling is the process of converting data from the initial format to a format that may be better for analysis.","d14afe3a":"### Data Visualization\n<p>When visualizing individual variables, it is important to first understand what type of variable you are dealing with. This will help us find the right visualization method for that variable.<\/p>\n","e257b4c6":"#### Thoughts?","a790e98a":"#### Replacing \"NaN\" with mode (most frequent) for categorical features","bc5fdbac":"#### Replacing \"NaN\" by mean value for all numeric features in one go","94696c53":"### Encode Categorical Variables","b748eeb0":"### Feature Selection","051f30d0":"Figure it out yourself","c50dcf50":"### Grouping","52b86d7a":"### Feature Scaling","72f96908":"<b> Options available are OrdinalEncoder, OneHotEncoder, LabelEncoder <\/b>","96b5e69b":"If there are missing values then we need to remove them because those values may hinder our further analysis. \nSo, how do we identify all those missing values and deal with them?\n\n\n**Steps for working with missing data:**\n<ol>\n    <li>Identify missing data<\/li>\n    <li>Deal with missing data<\/li>\n    <li>Correct data format<\/li>\n<\/ol>","85908648":"#### Thoughts?","95be55c4":"#### Thoughts?\n","c175294e":"# Test Prediction","397c52c0":"#### Dropping rows with \"NaN\"","d72f6017":"#### Let us list the columns after the conversion","c9166b5a":"**Replace all the '?' or wrong entries in the numeric coulmns by null**","191a4e4e":"Both look the same!","f15f5d71":"New and old do not seem to seperate rating but they do seem to have a correlationg with feature1. Can it be used?","2b5ea329":"#### Categorical variables\n\n<p>These are variables that describe a 'characteristic' of a data unit, and are selected from a small group of categories. The categorical variables can have the type \"object\" or \"int64\". A good way to visualize categorical variables is by using boxplots.<\/p>","62b1fdb6":"### Train-Test Split","4cf5c36a":"### Reduce the dimensions seperately","732ce39a":"### Dealing with missing data\n**How to deal with missing data?**\n\n<ol>\n    <li>drop data<br>\n        a. drop the whole row<br>\n        b. drop the whole column\n    <\/li>\n    <li>replace data<br>\n        a. replace it by mean<br>\n        b. replace it by frequency<br>\n        c. replace it based on other functions\n    <\/li>\n<\/ol>","7c4d48ac":"### Scale the two datasets seperately\n","314b0523":"## Data Analysis","44f8d6bf":"#### Thoughts?","38683a6f":"#### Thoughts?","a959ee3f":"## Data Preprocessing","ee9353a3":"To see which values are present in a particular column, we can use the \".value_counts()\" method:","842ea968":"### Descriptive Statistical Analysis","7c014d5b":"#### Thoughts?\n","ac5efae5":"### Correlation\n","8b647f76":"#### Count missing values in each column\nThe missing values are converted to Python's default. We use Python's built-in functions to identify these missing values. There are two methods to detect missing data:\n<ol>\n    <li><b>.isnull()<\/b><\/li>\n    <li><b>.notnull()<\/b><\/li>\n<\/ol>\nThe output is a boolean value indicating whether the value that is passed into the argument is in fact missing data.","8e7aebc0":"Continuous numerical variables:\nContinuous numerical variables are variables that may contain any value within some range. Continuous numerical variables can have the type \"int64\" or \"float64\". A great way to visualize these variables is by using scatterplots with fitted lines.\n\nIn order to start understanding the (linear) relationship between an individual variable and the price. We can do this by using \"regplot\", which plots the scatterplot plus the fitted regression line for the data.","5f10a44a":"#### Plot histograms","d6cea584":"MinMaxScaler -> (1,6,8)\nStandardScaler -> (4,7)\nRobustScaler -> (2,3,5,9,10,11)","9df577ac":"#### Thoughts (check for null, whether data type transformation is needed, and all the different types of data)?"}}