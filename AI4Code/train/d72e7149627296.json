{"cell_type":{"5092cfc3":"code","e0057bd2":"code","e7b9bb30":"code","963158a8":"code","78f59296":"code","5ea71fa3":"code","7b337008":"code","4c32f46e":"code","829f928c":"code","b35abf07":"code","7ad5d7ed":"code","8ab7fca0":"code","ba8befa8":"code","bbc38c3d":"code","ebd44bac":"code","926723dc":"code","0e74d562":"code","e0df7459":"markdown"},"source":{"5092cfc3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.model_selection import GroupKFold\nfrom keras.preprocessing.text import Tokenizer","e0057bd2":"import torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport torchvision.models as models","e7b9bb30":"cudnn.benchmark = True\nNUM_FOLD = 5\nSEED = 42\nBATCH_SIZE = 64\nNUM_CLASS = 1\nLR_RANGE = [1e-3, 1e-6]\nNUM_EPOCH = 64\nNUM_CYCLE = 2\nSIZE_REDUCE = -1\nVERBOSE = 100\nNUM_TOKEN = 256\nLEN_TOKEN = 1024\nEPOCH_DEVIDE = 16\nFOLD_LIST = [1]\nnp.random.seed(SEED)","963158a8":"df_train = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")\ndf_test['target'] = 0\nprint(df_train.shape, df_test.shape)\ndf_train.head()","78f59296":"# Tokenizer\nstarttime = time.time()\ndf_traintest = pd.concat([df_train, df_test]).reset_index(drop=True)\ntokenizer = Tokenizer(num_words=None, filters='', char_level=True, lower=False)\ntokenizer.fit_on_texts(df_traintest['comment_text'].values.tolist())\nprint(\"done. {:.1f} sec\".format(time.time()-starttime))","5ea71fa3":"tokenizer.oov_token = tokenizer.index_word[NUM_TOKEN-1]\nprint(tokenizer.oov_token)\ntmp = '12345\u3042'\ntmp = np.array(tokenizer.texts_to_sequences([tmp])[0])\nprint(tmp)","7b337008":"# GroupKFold\ndf_train['group'] = df_train['id']\ndf_train['group'][pd.isna(df_train['parent_id'])==False] = df_train['parent_id'][pd.isna(df_train['group'])==False]\nshuffle_idx = np.arange(len(df_train))\nshuffle_idx = np.random.permutation(shuffle_idx)\ndf_train_shuffle = df_train.loc[shuffle_idx].reset_index(drop=True)\nfolds = list(GroupKFold(n_splits=NUM_FOLD).split(\n    np.arange(len(df_train_shuffle)), \n    df_train_shuffle['target'], \n    df_train_shuffle['group']))\n\ndf_train_shuffle['fold'] = 0\nfor i in range(NUM_FOLD):\n    df_train_shuffle['fold'][folds[i][1]] = i\n\ndf_train = df_train_shuffle.sort_values('id').reset_index(drop=True)\ndf_train[['id', 'group','fold']].head(10)","4c32f46e":"df_train['valid'] = 0\ndf_train['valid'][(df_train['male']>0) \n                    | (df_train['female']>0)\n                    | (df_train['homosexual_gay_or_lesbian']>0)\n                    | (df_train['christian']>0)\n                    | (df_train['jewish']>0)\n                    | (df_train['muslim']>0)\n                    | (df_train['black']>0)\n                    | (df_train['white']>0)\n                    | (df_train['psychiatric_or_mental_illness']>0)\n                   ] = 1\ndf_train.head()","829f928c":"from math import cos, pi\n\n\ndef cycle(iterable):\n    \"\"\"\n    dataloader\u3092iterator\u306b\u5909\u63db\n    :param iterable:\n    :return:\n    \"\"\"\n    while True:\n        for x in iterable:\n            yield x\n\nclass CosineLR(_LRScheduler):\n    \"\"\"SGD with cosine annealing.\n    \"\"\"\n\n    def __init__(self, optimizer, step_size_min=1e-5, t0=100, tmult=2, curr_epoch=-1, last_epoch=-1):\n        self.step_size_min = step_size_min\n        self.t0 = t0\n        self.tmult = tmult\n        self.epochs_since_restart = curr_epoch\n        super(CosineLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        self.epochs_since_restart += 1\n\n        if self.epochs_since_restart > self.t0:\n            self.t0 *= self.tmult\n            self.epochs_since_restart = 0\n\n        lrs = [self.step_size_min + (\n                    0.5 * (base_lr - self.step_size_min) * (1 + cos(self.epochs_since_restart * pi \/ self.t0)))\n               for base_lr in self.base_lrs]\n\n        # print(lrs)\n\n        return lrs\n\n    \nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","b35abf07":"def make_layer(input, output, filter_size):\n    x = nn.Sequential(\n        nn.Conv1d(input, output, kernel_size=filter_size, padding=(filter_size-1)\/\/2),\n        nn.BatchNorm1d(output),\n        nn.ReLU(),\n    )\n    return x\n\nclass CharacterLevel1DCNN(nn.Module):\n    \"\"\"\n    Chalararacter level 1D CNN\n    \"\"\"\n\n    def __init__(self, num_classes=53, num_token=205):\n        super(CharacterLevel1DCNN, self).__init__()\n\n        self.num_classes = num_classes\n        self.mode = 'train'\n        self.conv1 = make_layer(num_token, 16, 7)\n        self.conv2 = make_layer(16, 32, 7)\n        self.conv3 = make_layer(32, 64, 7)\n        self.conv4 = make_layer(64, 128, 7)\n        self.conv5 = make_layer(128, 256, 7)\n        self.mp = nn.MaxPool1d(2)\n        self.gmp = nn.AdaptiveMaxPool1d(1)\n        self.dense_layers = nn.Sequential(\n            nn.Linear(256, 1024),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(1024, 1024),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(1024, NUM_CLASS),\n        )\n\n    def feature(self, input):\n        x = self.conv1(input)\n        x = self.mp(x)\n        x = self.conv2(x)\n        x = self.mp(x)\n        x = self.conv3(x)\n        x = self.mp(x)\n        x = self.conv4(x)\n        x = self.mp(x)\n        x = self.conv5(x)\n        x = self.mp(x)\n        x = self.gmp(x).view(input.size(0), -1)\n\n        return concat\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.mp(x)\n        x = self.conv2(x)\n        x = self.mp(x)\n        x = self.conv3(x)\n        x = self.mp(x)\n        x = self.conv4(x)\n        x = self.mp(x)\n        x = self.conv5(x)\n        x = self.mp(x)\n        x = self.gmp(x).view(input.size(0), -1)\n\n        x = self.dense_layers(x)\n\n        return x","7ad5d7ed":"class TokenDataset(Dataset):\n    def __init__(self, df, idx, len_token=LEN_TOKEN, num_token=NUM_TOKEN,\n                 aug=True):\n        self.X_train = df['comment_text']\n        self.y_train = df['target']>0.5\n        self.idx = idx\n        self.aug = aug\n        self.len_token = len_token\n        self.num_token = num_token\n\n    def do_slice(self, input):\n        if self.len_token==-1:\n            input_new = input\n        else:\n            input_new = np.zeros(self.len_token, np.int64)\n            if input.shape[0]<self.len_token:\n                shift = np.random.randint(0, self.len_token - input.shape[0])\n                if self.aug==True: shift = 0\n                input_new[shift:shift + input.shape[0]] = input\n            elif input.shape[0]==self.len_token:\n                input_new = input\n            else:\n                shift = np.random.randint(0, input.shape[0]-self.len_token)\n                if self.aug==True: shift = 0\n                input_new = input[shift:shift+self.len_token]\n        return input_new\n\n    def __getitem__(self, index):\n        idx = self.idx[index]\n        input = self.X_train[idx]\n        input = np.array(tokenizer.texts_to_sequences([input])[0])\n        input[input>=self.num_token-1] = self.num_token-1\n        input = self.do_slice(input)\n        input = np.eye(self.num_token)[input].transpose([1,0]).astype(np.float32)\n        target = self.y_train[idx].astype(np.float32).reshape([NUM_CLASS])\n        \n        return input, target\n\n    def __len__(self):\n        return len(self.idx)","8ab7fca0":"from sklearn import metrics\n\ndef train(train_loader, model, optimizer, scheduler, epoch, verbose, steps):\n    mse_avr = AverageMeter()\n    criterion_mse = nn.MSELoss().cuda()\n    criterion_bce = nn.BCELoss().cuda()\n    sigmoid = torch.nn.Sigmoid().cuda()\n\n    # switch to train mode\n    model.train()\n\n    starttime = time.time()\n    preds = np.zeros([0, NUM_CLASS], np.float32)\n    y_true = np.zeros([0, NUM_CLASS], np.float32)\n    for i in range(steps):\n        # prepare batches\n        input, target = next(train_loader)\n        input = torch.autograd.Variable(input.cuda(async=True))\n        target = torch.autograd.Variable(target.cuda(async=True))\n        # get model outputs\n        output = sigmoid(model(input))\n        mse = criterion_bce(output, target)\n        \n        # calc losses\n        loss = mse\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        # log\n        pred = output.data.cpu().numpy()\n        mse_avr.update(mse.data, input.size(0))\n        preds = np.concatenate([preds, pred])\n        y_true = np.concatenate([y_true, target.data.cpu().numpy()])\n        if verbose!=-1 and (i+1)%verbose==0:\n            print(\"step: {}\/{} \".format(i + 1, steps)\n                  + \"MSE: {:.3f} \".format(mse_avr.avg.item())\n                  + \"Sec: {:.1f} \".format(time.time()-starttime)\n                  )\n    auc = metrics.roc_auc_score(y_true[:,0]>0.5, preds[:,0])\n    return mse_avr.avg.item(), auc\n\n\ndef validate(val_loader, model):\n    mse_avr = AverageMeter()\n    criterion_mse = nn.MSELoss().cuda()\n    criterion_bce = nn.BCELoss().cuda()\n    sigmoid = torch.nn.Sigmoid().cuda()\n\n    # switch to eval mode\n    model.eval()\n\n    starttime = time.time()\n    preds = np.zeros([0, NUM_CLASS], np.float32)\n    y_true = np.zeros([0, NUM_CLASS], np.float32)\n    for i, (input, target) in enumerate(val_loader):\n        # prepare batches\n        input = torch.autograd.Variable(input.cuda(async=True))\n        target = torch.autograd.Variable(target.cuda(async=True))\n        \n        # get model outputs\n        output = sigmoid(model(input))\n        mse = criterion_bce(output, target)\n\n        # log\n        pred = output.data.cpu().numpy()\n        mse_avr.update(mse.data, input.size(0))\n        preds = np.concatenate([preds, pred])\n        y_true = np.concatenate([y_true, target.data.cpu().numpy()])\n        \n    auc = metrics.roc_auc_score(y_true[:,0]>0.5, preds[:,0])\n    return mse_avr.avg.item(), auc","ba8befa8":"# # Training\n# log_columns = ['epoch', 'mse', 'auc', 'val_mse', 'val_auc', 'time']\n\n# for fold in range(NUM_FOLD):\n#     if fold+1 not in FOLD_LIST: continue\n#     starttime = time.time()\n#     print(\"fold: {}\".format(fold + 1))\n\n#     # build model\n#     model = CharacterLevel1DCNN(num_classes=NUM_CLASS, num_token=NUM_TOKEN).cuda()  # \u751f\u5f92\u30e2\u30c7\u30eb\n\n#     ###\u3000prepare batch generator\n#     idx_train = df_train[df_train['fold']!=fold].index\n#     if SIZE_REDUCE!=-1:\n#         idx_train = idx_train[:SIZE_REDUCE]\n#     idx_valid = df_train[(df_train['fold']==fold) & (df_train['valid']==1)].index[:10000]\n# #     idx_valid = df_train[(df_train['fold']==fold)].index[:10000]\n\n#     # train dataset\n#     dataset_train = TokenDataset(df_train, idx_train, aug=True, len_token=1024)\n#     # dataloader\n#     train_loader = DataLoader(dataset_train,\n#                               batch_size=BATCH_SIZE,\n#                               shuffle=True,\n#                               num_workers=1,\n#                               pin_memory=True,\n#                               )\n#     steps = int(len(train_loader)\/EPOCH_DEVIDE)\n#     train_itr = cycle(train_loader)  # dataloader to generator\n\n#     # valid dataset\n#     dataset_valid = TokenDataset(df_train, idx_valid, aug=False, len_token=1024)\n#     valid_loader = DataLoader(dataset_valid,\n#                               batch_size=BATCH_SIZE,\n#                               shuffle=False,\n#                               num_workers=1,\n#                               pin_memory=True\n#                               )\n\n#     optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_RANGE[0])  # Adam\n#     scheduler = CosineLR(optimizer, step_size_min=LR_RANGE[1], t0=len(train_loader) * NUM_CYCLE * EPOCH_DEVIDE,\n#                          tmult=1)  # Cyclic lr\n\n#     train_log = pd.DataFrame(columns=log_columns)\n#     for epoch in range(NUM_EPOCH):  # epoch\u6570 = NUM_EPOCH0\n#         # train 1 epoch\n#         mse, auc = train(train_itr, model, optimizer, scheduler, epoch, VERBOSE, steps)\n\n#         #  validation\n#         val_mse, val_auc = validate(valid_loader, model)\n\n#         # save log\n#         endtime = time.time() - starttime\n#         train_log_epoch = pd.DataFrame([[epoch+1, mse, auc, val_mse, val_auc, endtime]],\n#                                        columns=log_columns)\n#         train_log = pd.concat([train_log, train_log_epoch])\n#         train_log.to_csv(\"train_log_fold{}.csv\".format(fold + 1), index=False)\n\n#         # display log\n#         print(\"Epoch: {}\/{} \".format(epoch + 1, NUM_EPOCH)\n#               + \"MSE: {:.3f} \".format(mse)\n#               + \"AUC: {:.3f} \".format(auc)\n#               + \"Valid MSE: {:.3f} \".format(val_mse)\n#               + \"Valid AUC: {:.3f} \".format(val_auc)\n#               + \"Sec: {:.1f} \".format(time.time()-starttime)\n#               )\n#         if (epoch+1)%(NUM_CYCLE*EPOCH_DEVIDE)==0:\n#             torch.save(model.state_dict(), \"weight_fold_{}_epoch_{}.pth\".format(fold+1, epoch+1))\n            \n#     torch.save(model.state_dict(), \"weight_fold_{}_epoch_{}.pth\".format(fold+1, epoch+1))\n#     torch.save(optimizer.state_dict(), 'optimizer_fold_{}_epoch_{}.pth'.format(fold+1, epoch+1))","bbc38c3d":"def predict(val_loader, model):\n    sigmoid = torch.nn.Sigmoid().cuda()\n\n    # switch to eval mode\n    model.eval()\n\n    starttime = time.time()\n    preds = np.zeros([0, NUM_CLASS], np.float32)\n    for i, (input, _) in enumerate(val_loader):\n        # prepare batches\n        input = torch.autograd.Variable(input.cuda(async=True))\n        \n        # get model outputs\n        output = sigmoid(model(input))\n\n        # compute gradient and do SGD step\n        pred = output.data.cpu().numpy()\n        preds = np.concatenate([preds, pred])\n        if i%100==0:\n            print(\"step: {}\/{} \".format(i + 1, len(val_loader))\n                  + \"Sec: {:.1f} \".format(time.time()-starttime)\n                  )\n        \n    return preds","ebd44bac":"model = CharacterLevel1DCNN(num_classes=NUM_CLASS, num_token=NUM_TOKEN).cuda()\npreds_test = np.zeros([NUM_FOLD, len(df_test)])\nfor fold in range(NUM_FOLD):\n    if fold+1 not in FOLD_LIST: break\n    starttime = time.time()\n    print(\"fold: {}\".format(fold + 1))\n\n    # build model\n    model.load_state_dict(\n        torch.load(\"..\/input\/characerlevel-cnn-weights\/models\/weight_fold_{}_epoch_{}.pth\".format(fold+1, NUM_EPOCH)))\n\n    dataset_test = TokenDataset(df_test, np.arange(len(df_test)), aug=False, len_token=1024)\n    test_loader = DataLoader(dataset_test,\n                              batch_size=BATCH_SIZE*4,\n                              shuffle=False,\n                              num_workers=1,\n                              pin_memory=True\n                              )\n\n    preds_test[fold] = predict(test_loader, model)[:,0]\nnp.save(\"preds_test.npy\", preds_test)","926723dc":"df_sub = df_test[['id']]\ndf_sub['prediction'] = preds_test.mean(axis=0)\ndf_sub.to_csv(\"submission.csv\", index=None)\ndf_sub.head()","0e74d562":"preds_test[[FOLD_LIST]].mean(axis=0).shape","e0df7459":"I ended up my 5 day challenge of this competition.  \nHere I share my solution, Character-level CNN.  \n0.87 in LB without external data is not so bad, doesn't it?\nIn this notebook I use trained weight.  \nTraining procedure is included in this notebook."}}