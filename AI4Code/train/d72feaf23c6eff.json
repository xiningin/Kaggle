{"cell_type":{"ee26b796":"code","c164d5c3":"code","d4b2db1c":"code","824e701e":"code","01e6cee3":"code","5f8a9dca":"code","c0baafa0":"code","3fdd0daa":"code","9e4a3aa6":"code","bf94fa3c":"code","4b621fed":"code","1a804559":"code","3c2a5b2a":"code","eb48491e":"code","3a0ba94f":"code","ad00e717":"code","9fa39d04":"code","b58fe2f8":"code","62c1cbe1":"code","9bf8c652":"code","60710bb0":"code","75df8c0e":"code","ef1dc7ec":"code","f004c3a4":"code","c1239f8e":"code","31d8ddf1":"code","16330599":"code","f95cf0f5":"code","817ddbda":"code","70e53c14":"code","6f2ae15e":"code","511cd322":"markdown","80989325":"markdown","5d91f0c7":"markdown","c03f14dd":"markdown","542778c3":"markdown","e799ae47":"markdown","cd345bd7":"markdown","f4899631":"markdown","64be5dcd":"markdown","6d290eb3":"markdown","809d172a":"markdown","87915117":"markdown","3a8217a9":"markdown","42700434":"markdown","c5ed9025":"markdown","a6e72f31":"markdown","683475ee":"markdown","f25ed3ed":"markdown","1392d61a":"markdown","87f67759":"markdown","125e5e1f":"markdown","78b92633":"markdown","3a11fb62":"markdown","8db3e062":"markdown","0f2a33c0":"markdown"},"source":{"ee26b796":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.preprocessing import StandardScaler\nimport category_encoders as ce\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n# Regressors\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge, Lasso\n\nsns.set(rc={'figure.figsize':(12,8.27)})\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c164d5c3":"#Wrapper to train a list of models at once\n# X : Feature Dataframe\n# y : Target columns Series\ndef train_models(X,y,kfolds=12):\n    kfold = KFold(kfolds)\n    for model in models:\n     # model['regressor'].fit(X,y)\n      cf_result =  cross_val_score(model['regressor'], X, y, cv=kfold,scoring='neg_mean_squared_log_error')\n      model['cv_results'] = np.sqrt(cf_result*-1)\n      msg = f\"Regressor: {model['name']},   rmsle:{cf_result.mean().round(11)} \" \n      print(msg)\n    \n# A Class to combine 2 or more models, so we can merge the predictions in one\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","d4b2db1c":"models = []\nmodels.append({'name':'XBR','regressor':XGBRegressor(random_state=0) })\nmodels.append({'name' :'Random Forest','regressor': RandomForestRegressor(criterion='mse',max_depth=35,max_features='sqrt',n_estimators=150,random_state=0)})\nmodels.append({'name': 'Ridge','regressor' :Ridge(alpha=2.0,copy_X=True,fit_intercept=False,max_iter=1000,normalize=True,random_state=0)})\nmodels.append({'name': 'AveragingModels','regressor' :AveragingModels(models = (XGBRegressor(),Lasso(), Ridge(alpha=2.0), RandomForestRegressor(n_estimators=200,random_state=0)))})","824e701e":"original_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\noriginal_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ny_column = 'SalePrice'\noriginal_train.head()","01e6cee3":"# We don't need the Id columns, let's drop it!\ntrain = original_train.copy().drop(columns=['Id'])\ntest = original_test.copy()","5f8a9dca":"# Get the numeical features to get insights later.\nnumeric_features = train.select_dtypes([int,float])\n   \n# visualising some  outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\n\n\nfor (i,feature) in enumerate(numeric_features.columns,1):\n    plt.subplot(len(numeric_features.columns), 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', data=train)\n    plt.xlabel(f'{feature}', size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","c0baafa0":"sns.boxplot(train.GrLivArea)","3fdd0daa":"fig, ax = plt.subplots(figsize=(25,15))         # Sample figsize in inches\nsns.heatmap(train.corr().abs(),annot=True)","9e4a3aa6":"# A Barplot with the most correlated with the target\ncorrelations =train.corr().abs()['SalePrice'].sort_values(ascending=False)[1:]\nax = sns.barplot(x=correlations.values,y=correlations.index).set_title('Most Correlated with SalePrice')","bf94fa3c":"train.isnull().sum().sort_values(ascending=False)","4b621fed":"no_missing_train = train.copy().drop(columns=['PoolQC','MiscFeature','Alley','Fence'])\n\n# Fill with NA\nno_missing_train.FireplaceQu = no_missing_train.FireplaceQu.fillna('NA')\nno_missing_train.BsmtQual = no_missing_train.BsmtQual.fillna('NA')\nno_missing_train.BsmtFinType1 = no_missing_train.BsmtFinType1.fillna('NA')\nno_missing_train.BsmtCond   = no_missing_train.BsmtCond.fillna('NA')\n\n\n#Fill with mean\nno_missing_train.LotFrontage = no_missing_train.LotFrontage.mean()\n\n#Fill with mode\nfor column in no_missing_train.isnull().sum().sort_values(ascending=False).index:\n    no_missing_train[column].fillna(no_missing_train[column].mode()[0],inplace=True)\n\n#TEST\n\nno_missing_test = test.copy().drop(columns=['PoolQC','MiscFeature','Alley','Fence'])\n# Fill with NA\nno_missing_test.FireplaceQu = no_missing_test.FireplaceQu.fillna('NA')\nno_missing_test.BsmtQual = no_missing_test.BsmtQual.fillna('NA')\nno_missing_test.BsmtFinType1 = no_missing_test.BsmtFinType1.fillna('NA')\nno_missing_test.BsmtCond   = no_missing_test.BsmtCond.fillna('NA')\n#Fill with mean\nno_missing_test.LotFrontage = no_missing_test.LotFrontage.mean()\n\n#Fill with mode\nfor column in no_missing_test.isnull().sum().sort_values(ascending=False).index:\n    no_missing_test[column].fillna(no_missing_test[column].mode()[0],inplace=True)","1a804559":"no_missing_test.isnull().sum().sort_values(ascending=False)","3c2a5b2a":"train_eng = no_missing_train.copy()\ntest_eng = no_missing_test.copy()\n# New Features & drop Features\ntrain_eng['TotalBath'] = train_eng.FullBath + (train_eng.HalfBath * 0.5)\ntrain_eng.drop(columns=['FullBath','HalfBath'],inplace=True)\n\n\ntest_eng['TotalBath'] = test_eng.FullBath + (test_eng.HalfBath * 0.5)\ntest_eng.drop(columns=['FullBath','HalfBath'],inplace=True)\n\n","eb48491e":"X_train_eng = train_eng.drop(columns=[y_column]).select_dtypes([int,float,bool])\ny_train_eng = train_eng[y_column]\nX_test_eng = test_eng.drop(columns=['Id']).select_dtypes([int,float,bool])\n","3a0ba94f":"train_featured = train_eng.copy()\ntest_featured = test_eng.copy()\n\n# log(1+x) transform\ntrain_featured[\"SalePrice\"] = np.log1p(train_featured[\"SalePrice\"])\n\n#Categories to Values\nqualities = {'NA':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5}\ntrain_featured.BsmtQual.replace(qualities,inplace=True)\ntrain_featured.BsmtCond.replace(qualities,inplace=True)\ntrain_featured.HeatingQC.replace(qualities,inplace=True)\ntrain_featured.FireplaceQu.replace(qualities,inplace=True)\ntrain_featured.GarageQual.replace(qualities,inplace=True)\ntrain_featured.GarageCond.replace(qualities,inplace=True)\ntrain_featured.KitchenQual.replace(qualities,inplace=True)\ntrain_featured.ExterQual.replace(qualities,inplace=True)\ntrain_featured.ExterCond.replace(qualities,inplace=True)\n\ntest_featured.BsmtQual.replace(qualities,inplace=True)\ntest_featured.BsmtCond.replace(qualities,inplace=True)\ntest_featured.HeatingQC.replace(qualities,inplace=True)\ntest_featured.FireplaceQu.replace(qualities,inplace=True)\ntest_featured.GarageQual.replace(qualities,inplace=True)\ntest_featured.GarageCond.replace(qualities,inplace=True)\ntest_featured.KitchenQual.replace(qualities,inplace=True)\ntest_featured.ExterQual.replace(qualities,inplace=True)\ntest_featured.ExterCond.replace(qualities,inplace=True)\n\n\n#Binary\ny_n = {'N':0,'Y':1}\ntrain_featured.CentralAir.replace(y_n,inplace=True)\n\ntest_featured.CentralAir.replace(y_n,inplace=True)","ad00e717":"X_train_feat = train_featured.drop(columns=[y_column]).select_dtypes([int,float,bool])\ny_train_feat = train_featured[y_column]\nX_test_feat = test_featured.drop(columns=['Id']).select_dtypes([int,float,bool])\n\nprint(\"Training model for feature transformation technique\")\ntrain_models(X_train_feat,y_train_feat)","9fa39d04":"train_rm = train_featured.copy()\ntest_rm = test_featured.copy()\n#train_rm.drop(columns=['LotFrontage','LowQualFinSF'],inplace=True)\n#test_rm.drop(columns=['LotFrontage','LowQualFinSF'],inplace=True)","b58fe2f8":"X_train_rm = train_rm.drop(columns=[y_column]).select_dtypes([int,float,bool])\ny_train_rm = train_rm[y_column]\nX_test_rm = test_rm.drop(columns=['Id']).select_dtypes([int,float,bool])\n\nprint(\"Training model for feature removal technique\")\ntrain_models(X_train_rm,y_train_rm)","62c1cbe1":"train_out = train_rm.copy()\ntest_out = test_rm.copy()\n\ntrain_out = train_out.drop(train_out[train_out.LotArea > 50000].index)\ntrain_out = train_out.drop(train_out[train_out.GrLivArea > 4000].index)","9bf8c652":"X_train_out = train_out.drop(columns=[y_column]).select_dtypes([int,float,bool])\ny_train_out = train_out[y_column]\nX_test_out = test_out.drop(columns=['Id']).select_dtypes([int,float,bool])\n\nprint(\"Training model for outliers removal technique\")\ntrain_models(X_train_out,y_train_out)","60710bb0":"sns.set_style(\"white\")\n#sns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train_out['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","75df8c0e":"# plotting the probability plot.\n# The goal is fit the plot against the line, this means a good target distribuition.\n\nfrom scipy import stats\nstats.probplot(train_out.SalePrice,plot=plt)","ef1dc7ec":"train_encoded = train_out.copy()\ntest_encoded = test_out.copy()\n\ntrain_encoded[train_encoded.select_dtypes('object').columns] = train_encoded[train_encoded.select_dtypes('object').columns].astype('category')\ntest_encoded[test_encoded.select_dtypes('object').columns] = test_encoded[test_encoded.select_dtypes('object').columns].astype('category')\n\ncategory_columns = train_encoded.select_dtypes('category').columns\n\n\nencoder = ce.CatBoostEncoder(cols=category_columns)\nencoder.fit(train_encoded[category_columns],train_encoded['SalePrice'])\n                              \n                \ntrain_encoded[category_columns] = encoder.transform(train_encoded[category_columns])\ntest_encoded[category_columns] = encoder.transform(test_encoded[category_columns])\n","f004c3a4":"X_train_encoded = train_encoded.drop(columns=[y_column]).select_dtypes([int,float,bool])\ny_train_encoded = train_encoded[y_column]\nX_test_encoded = test_encoded.drop(columns=['Id']).select_dtypes([int,float,bool])\n\nprint(\"Training model for encoded features technique\")\ntrain_models(X_train_encoded,y_train_encoded)\n\n#X_train, X_test = X_train.align(X_test, join='outer', axis=1, fill_value=0)","c1239f8e":"from sklearn.feature_selection import RFE\nestimator = XGBRegressor(random_state=0)\nselector = RFE(estimator, n_features_to_select=25, step=1)\nselector = selector.fit(X_train_encoded, y_train_encoded)\nbest_columns = X_train_encoded.columns[selector.support_]\n\ntrain_models(X_train_encoded[best_columns],y_train_encoded)","31d8ddf1":"# Line plot comparing our models\nmodel_names = list(map(lambda x: x['name'],models))\nmodel_results_mean = list(map(lambda x: x['cv_results'].mean(),models))\nsns.lineplot( x=model_names, y=model_results_mean).set_title('Performance by model')","16330599":"from sklearn.model_selection import GridSearchCV\nparameters ={\"alpha\"    : [0.05, 0.10, 0.50, 0.75, 1, 2,4,5 ]  }\nrdg = Ridge()\nclf = GridSearchCV(rdg, parameters)\nclf.fit(X_train_encoded,y_train_encoded)","f95cf0f5":"# the best parameter\nclf.best_params_","817ddbda":"best_model = AveragingModels(models = ( Ridge(alpha=2.0,copy_X=True,fit_intercept=False,max_iter=1000,normalize=True,random_state=0), RandomForestRegressor(criterion='mse',max_depth=35,max_features='sqrt',n_estimators=150,random_state=0)))\nbest_model.fit(X_train_encoded,y_train_encoded)","70e53c14":"predicted = test[['Id']].copy()\npredicted['SalePrice'] = np.expm1(best_model.predict(X_test_encoded))\n#model.predict(X_test)","6f2ae15e":"predicted.to_csv('HousePricingEduardoRenz.csv',index=False)","511cd322":"## Select best hyper parameters with GridSearch\n\nWe nee to tune our models, finding the best combination of parameters. GridSearchCV will help us with this task.","80989325":"## Remove Features","5d91f0c7":"## Missing Values\n\n* Drop High Missing values Columns\n* Fill other values with mode","c03f14dd":"### Tip 2 : Outliers = Bad\n\nIn this example, we can se a outlier on GrLivArea.\nMakes no sense a cheap house with a big area, comparing with the other examples.\nLater we will drop this row","542778c3":"Regressor: XBR,   rmsle:-0.00011229325 \n\nRegressor: Random Forest,   rmsle:-0.00011517446 ","e799ae47":"Regressor: XBR,   rmsle:-0.00010520197 \n\nRegressor: Random Forest,   rmsle:-9.923111e-05 \n\nRegressor: Ridge,   rmsle:-8.554925e-05 \n","cd345bd7":"## Define the model's list\n\nHe create a list with all models that we want to train.\nSo we train all the models at once to choose the best one.","f4899631":"## Objective\n\n* Predict Houses Sale Price using Regression","64be5dcd":"## Feature Engeenering\n\nML models somtimes can't understand some features correlations and his importances, like a sum of all bathrooms in the house.\nSo let's create a new column called **TotalBath**","6d290eb3":"## Feature selection using RFE\n\nA good way to get the most important features in our model, is using the RFE Technique.\nWe need to reduce our dimensionality whenever possible.","809d172a":"# Remove Outliers\n\nHere we performed that task that I had previously mentioned about removing outliers.\n","87915117":"## Category Encoding\n\nSome of categorycal data needs another techniques of transformations.\nLuclky we have a library called **category_encoders**\n\nSee [this link](https:\/\/contrib.scikit-learn.org\/category_encoders\/) for more details.","3a8217a9":"### Feature Transformation\n\nML = Mathematics and Statistics in their background.\nSo all the features must be numbers. \nCategorical features (normaly string types) need to be converted to numeric.\n\nAll the features that have quality score can be converted to a scale.","42700434":"## SalesPrice Variance\n\nThere as lot of SalesPrice variance, this is not good for the model.\n","c5ed9025":"### Tip 3 : Find the most correlated features\n\nAnother good way to find the best features for your model is plotting a heatmap of all correlations.\n\nSome Observations in general cases:\n\n* **High Correlation with the target** = Good for model, try to keep this features.\n* **High Correlation between features** = Usualy its better to drop one of them, high dimensional DataFrame it's not good for the model.\n","a6e72f31":"Vizualize some features","683475ee":"## Util Functions\n\nWe define some functions to use in our exploration.\nThis is a good way to reuse some code.","f25ed3ed":"Regressor: XBR,   rmsle:-0.00010794983 \n\nRegressor: Random Forest,   rmsle:-0.00011003451 ","1392d61a":"### Tip 1 : Start comparing all the features with the target.\n\nThis DataFrame have a huge amount of features, and we don't have any clue what is their relation with our goal.\nA good way to start is compare each one with plot chars.\n\nThe more linear chart is, better for the model.****","87f67759":"# Import the libs\n\nHere we import all the needes libs used in this notebook","125e5e1f":"For example, whe can see a good relation with **OveralQual**, or even the **TotalBsmtSF**.\nKeep track of features like this.","78b92633":"## Predict Test for Competition","3a11fb62":"Check if we don't have missing values anymore.","8db3e062":"Let's do some feature corections, sush fill missing values","0f2a33c0":"## Get data and initial analysis\n\nObs: Along the notebook, i prefer to copy and transform the Dataframe in diffrent variables along the exploration. This is a good pratice in functional programing, in order to avoid messing the data."}}