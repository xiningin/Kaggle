{"cell_type":{"c8b30164":"code","fa352fa7":"code","3d35cdca":"code","2821983c":"code","ea764776":"code","479ece63":"code","179bdd97":"code","4807660d":"code","c2946e4f":"code","288c4393":"code","10bdde4a":"code","7ad6d01d":"code","e5cb9ef3":"code","92d3d887":"code","c35cd704":"markdown"},"source":{"c8b30164":"import numpy as np\nimport pandas as pd\npd.set_option(\"display.max_columns\", 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport pickle\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom hyperopt import fmin, hp, tpe, space_eval\n\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nimport lightgbm as lgb\nfrom time import time\nfrom tqdm import tqdm_notebook\nimport qgrid\n\nfrom xgboost import XGBClassifier\nimport os\n\nfrom sklearn.model_selection import KFold\nfrom scipy import stats\nfrom sklearn.metrics import roc_curve\n\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\n","fa352fa7":"train = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntest  = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\nsub   = pd.read_csv('..\/input\/petfinder-pawpularity-score\/sample_submission.csv')","3d35cdca":"train.head()","2821983c":"test.head()","ea764776":"print(train.shape, test.shape)","479ece63":"Id = 'Id'\ntarget = 'Pawpularity'","179bdd97":"not_used = [Id, target]\nused_features = [x for x in test.columns if x not in not_used]\nprint(train[used_features].shape, test[used_features].shape, train[target].shape)","4807660d":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state = 889)\n\nquick = False\nif quick:\n    lr = 0.1\n    Early_Stopping_Rounds = 150\nelse:\n    lr = 0.01\n    Early_Stopping_Rounds = 300\n\nN_round = 6000\nVerbose = 100\nparams = {\n            'objective': 'regression',\n            'metric': 'rmse',\n            'boosting': 'gbdt',\n            'learning_rate': lr, #small learn rate, large number of iterations\n            'num_leaves': 2 ** 3,\n            'bagging_fraction': 0.95,\n            'bagging_freq': 1,\n            'bagging_seed': 66,\n            'feature_fraction': 0.7,\n            'feature_fraction_seed': 66,\n            'max_bin': 100,\n            'max_depth': 5\n        }\n","c2946e4f":"import pickle","288c4393":"lgb_sub = sub\nlgb_sub[target] = 0\n\nRMSLEs = []\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = train[used_features].columns\n\nN_MODEL = 1.0\nfor model_i in tqdm_notebook(range(int(N_MODEL))):\n\n    if N_MODEL != 1.0:\n        params['seed'] = model_i + 1123\n\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(train[used_features])):\n\n        start_time = time()\n        print('Training on model {} - fold {}'.format(model_i + 1, fold_n + 1))\n        \n        trn_data = lgb.Dataset(train[used_features].iloc[train_index], label=train[target].iloc[train_index], categorical_feature=\"\")\n        val_data = lgb.Dataset(train[used_features].iloc[valid_index], label=train[target].iloc[valid_index], categorical_feature=\"\")\n        clf = lgb.train(params, trn_data, num_boost_round=N_round, valid_sets=[trn_data, val_data], verbose_eval=Verbose,\n                        early_stopping_rounds=Early_Stopping_Rounds)  # , feval=evalerror\n        \n        # dump model with pickle\n        with open(f'model_{model_i}_fold_{fold_n}.pkl', 'wb') as fout:\n            pickle.dump(clf, fout)\n\n        feature_importances['model_{}-fold_{}'.format(model_i + 1, fold_n + 1)] = clf.feature_importance()\n        \n        val = clf.predict(train[used_features].iloc[valid_index])\n        pred = clf.predict(test[used_features])\n        lgb_sub[target] = lgb_sub[target] + pred \/ n_fold \/ N_MODEL\n\n        rmsle_ = np.sqrt(mean_squared_error(np.log(train.iloc[valid_index][target]), np.log(val)))\n        \n        print('RMSLE: {}'.format(rmsle_))\n        RMSLEs.append(rmsle_)\n        print('Model {} - Fold {} finished in {}'.format(model_i + 1, fold_n + 1,\n                                                         str(datetime.timedelta(seconds=time() - start_time))))\nprint(\"done!\")\n","10bdde4a":"print(RMSLEs)\nprint('Mean RMSLE:', np.mean(RMSLEs))","7ad6d01d":"feature_importances['average'] = feature_importances[[x for x in feature_importances.columns if x != \"feature\"]].mean(axis=1)\nfeature_importances = feature_importances.sort_values(by = \"average\", ascending = False)\nfeature_importances.to_csv('feature_importances.csv')\nprint(feature_importances.head())","e5cb9ef3":"lgb_sub.head()","92d3d887":"lgb_sub.to_csv(\"submission.csv\", index=False, header = True)","c35cd704":"Inference can be found: [petfinder-lightgbm-sub](https:\/\/www.kaggle.com\/poteman\/petfinder-lightgbm-sub)"}}