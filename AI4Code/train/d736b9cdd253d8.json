{"cell_type":{"d08cee7c":"code","909cdb48":"code","753594aa":"code","a731c9b7":"code","89a40d46":"code","77059dc6":"code","42e3af61":"code","674eb951":"code","b396f299":"code","4c69b354":"code","9f84978f":"code","a1df00e8":"code","b9fce1cf":"code","5247af02":"code","6f99d8ee":"code","d1b54ca6":"code","3d8f19c0":"code","82607c64":"code","6a54aa95":"code","efec7cc8":"code","f1046990":"code","91b7bb8e":"code","60410d0a":"code","2e608200":"code","2622e3cf":"code","4fad0525":"markdown"},"source":{"d08cee7c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","909cdb48":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport re\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (6,6)\n\nimport transformers #huggingface transformers library\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport sklearn\n\n\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\n\nfrom keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\nfrom keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\nfrom keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\nfrom keras.layers import Reshape, merge, Concatenate, Lambda, Average\nfrom keras.models import Sequential, Model, load_model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.initializers import Constant\nfrom keras.layers.merge import add\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.utils import np_utils\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport unicodedata\nimport nltk\nimport spacy\nfrom nltk.tokenize.toktok import ToktokTokenizer\nstopword_list = nltk.corpus.stopwords.words('english')\ntokenizer = ToktokTokenizer()\nnlp = spacy.load('en_core_web_sm', parse = False, tag=False, entity=False)\n#from contractions import CONTRACTION_MAP\nimport nltk\nnltk.download('words')\nwords = set(nltk.corpus.words.words())\nstopword_list = nltk.corpus.stopwords.words('english')","753594aa":"data = pd.read_json('\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json',lines=True)","a731c9b7":"# checking TPU\ntry:\n    \n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","89a40d46":"# Data cleaning\n\n# defining all functions\n# Remove any emails \ndef remove_emails(text):\n    text = re.sub(r'\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b', ' ', text)\n    return text\n\ndef remove_hyperlink(text):\n    text=re.sub(r'(http|https):\/\/[^\\s]*',' ',text)\n    return text\n\n# Removing Digits\ndef remove_digits(text):\n    #text= re.sub(r\"\\b\\d+\\b\", \"\", text)\n    text= re.sub(r\"(\\s\\d+)\", \" \", text)\n    return text\n    \n\n# Removing Special Characters\ndef remove_special_characters(text):\n    text = re.sub('[^a-zA-Z\\s]', ' ', text)\n    return text\n\n\n# removing accented charactors\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n\n # Removing Stopwords\ndef remove_stopwords(text,is_lower_case):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n\n    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)   \n    return filtered_text\n\n# Lemmetization\ndef lemmatize_text(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text\n\n\n# Combine all the functions and creating a preprocessing pipeline\n# # Text preprocessing\ndef text_preprocessing(corpus,isRemoveEmail,isRemoveDigits,isRemoveHyperLink, \n                     isRemoveSpecialCharac,isRemoveAccentChar,\n                       text_lower_case,text_lemmatization, stopword_removal):\n    \n    normalized_corpus = []\n    \n    for doc in corpus:\n        \n        if text_lower_case:\n            doc = doc.lower()\n        \n        if isRemoveEmail:\n            doc = remove_emails(doc)\n        \n        if isRemoveHyperLink:\n            doc=remove_hyperlink(doc)\n             \n        if isRemoveAccentChar:\n            doc = remove_accented_chars(doc)\n            \n        if isRemoveDigits:\n            doc = remove_digits(doc)\n        \n        # remove extra newlines\n        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n        # insert spaces between special characters to isolate them    \n        special_char_pattern = re.compile(r'([{.(-)!}])')\n        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n        \n        if text_lemmatization:\n            doc = lemmatize_text(doc)\n        \n        if isRemoveSpecialCharac:\n            doc = remove_special_characters(doc)\n        # remove extra whitespace\n        doc = re.sub(' +', ' ', doc)\n        \n        if stopword_removal:\n            doc = remove_stopwords(doc,is_lower_case=text_lower_case)\n                \n        normalized_corpus.append(doc)\n        \n    return normalized_corpus\n","77059dc6":"EMAIL_FLAG=True\nDIGIT_FLAG=True\nHYPER_LINK_FLAG=True\nALL_SPEC_CHAR_FLAG=True\nACCENT_CHAR_FLAG=True\nLOWER_CASE_FLAG=True\nLEMMETIZE_FLAG=False\nSTOPWORD_FLAG=True\n\nclean_headline= text_preprocessing(data['headline'],EMAIL_FLAG,DIGIT_FLAG,HYPER_LINK_FLAG,\n                   ALL_SPEC_CHAR_FLAG,ACCENT_CHAR_FLAG,\n                  LOWER_CASE_FLAG,LEMMETIZE_FLAG,STOPWORD_FLAG)\nclean_short_Desc = text_preprocessing(data['short_description'],EMAIL_FLAG,DIGIT_FLAG,HYPER_LINK_FLAG,\n                   ALL_SPEC_CHAR_FLAG,ACCENT_CHAR_FLAG,\n                  LOWER_CASE_FLAG,LEMMETIZE_FLAG,STOPWORD_FLAG)","42e3af61":"data['clean_headline']=clean_headline\ndata['clean_short_Desc'] = clean_short_Desc\n\n# Merging both the columns\ndata['MergedColumn'] = data[data.columns[6:8]].apply(\n    lambda x: ' '.join(x.astype(str)),\n    axis=1\n)\n\ndf = data.copy()\ndel data\ndf.drop(columns=['headline', 'authors', 'link', 'short_description', 'date',\n                   'clean_headline', 'clean_short_Desc'],axis=1,inplace=True)","674eb951":"print(f\"The dataset contains { df.category.nunique() } unique categories\")","b396f299":"#label encoding the categories. After this each category would be mapped to an integer.\nencoder = LabelEncoder()\ndf['categoryEncoded'] = encoder.fit_transform(df['category'])","4c69b354":"# Using hugging face tokenizer\n\ndef regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","9f84978f":"#bert large uncased pretrained tokenizer\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased')","a1df00e8":"X_train,X_test ,y_train,y_test = train_test_split(df['MergedColumn'], df['categoryEncoded'], random_state = 2020, test_size = 0.3)","b9fce1cf":"#tokenizing the news descriptions and converting the categories into one hot vectors using tf.keras.utils.to_categorical\nXtrain_encoded = regular_encode(X_train.astype('str'), tokenizer, maxlen=80)\nytrain_encoded = tf.keras.utils.to_categorical(y_train, num_classes=41,dtype = 'int32')\nXtest_encoded = regular_encode(X_test.astype('str'), tokenizer, maxlen=80)\nytest_encoded = tf.keras.utils.to_categorical(y_test, num_classes=41,dtype = 'int32')","5247af02":"def build_model(transformer, loss='categorical_crossentropy', max_len=512):\n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    #adding dropout layer\n    x = tf.keras.layers.Dropout(0.3)(cls_token)\n    #using a dense layer of 41 neurons as the number of unique categories is 41. \n    out = tf.keras.layers.Dense(41, activation='softmax')(x)\n    model = tf.keras.Model(inputs=input_word_ids, outputs=out)\n    #using categorical crossentropy as the loss as it is a multi-class classification problem\n    model.compile(tf.keras.optimizers.Adam(lr=3e-5), loss=loss, metrics=['accuracy'])\n    return model","6f99d8ee":"#building the model on tpu\nwith strategy.scope():\n    transformer_layer = transformers.TFAutoModel.from_pretrained('bert-large-uncased')\n    model = build_model(transformer_layer, max_len=80)\nmodel.summary()","d1b54ca6":"#creating the training and testing dataset.\nBATCH_SIZE = 32*strategy.num_replicas_in_sync\nAUTO = tf.data.experimental.AUTOTUNE \ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((Xtrain_encoded, ytrain_encoded))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(Xtest_encoded)\n    .batch(BATCH_SIZE)\n)","3d8f19c0":"#training for 10 epochs\nn_steps = Xtrain_encoded.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    epochs=10\n)","82607c64":"#making predictions\npreds = model.predict(test_dataset,verbose = 1)\n#converting the one hot vector output to a linear numpy array.\npred_classes = np.argmax(preds, axis = 1)","6a54aa95":"#extracting the classes from the label encoder\nencoded_classes = encoder.classes_\n#mapping the encoded output to actual categories\npredicted_category = [encoded_classes[x] for x in pred_classes]\ntrue_category = [encoded_classes[x] for x in y_test]","efec7cc8":"result_df = pd.DataFrame({'description':X_test,'true_category':true_category, 'predicted_category':predicted_category})\nresult_df.head()","f1046990":"print(f\"Accuracy is {sklearn.metrics.accuracy_score(result_df['true_category'], result_df['predicted_category'])}\")","91b7bb8e":"result_df.to_csv('Predictions.csv', index = False)","60410d0a":"result_df[result_df['true_category']!=result_df['predicted_category']]","2e608200":"confusion_mat = confusion_matrix(y_true = true_category, y_pred = predicted_category, labels=list(encoded_classes))","2622e3cf":"df_cm = pd.DataFrame(confusion_mat, index = list(encoded_classes),columns = list(encoded_classes))\nplt.rcParams['figure.figsize'] = (20,20)\nsns.heatmap(df_cm)","4fad0525":"Evaluation"}}