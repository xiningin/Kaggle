{"cell_type":{"02692a59":"code","9e231816":"code","4a0ad0dc":"code","f4b57984":"code","53bee9ac":"code","c68d0221":"code","a8256262":"code","57cce1b6":"code","70cf9a18":"code","ffbc3424":"code","427302c0":"code","e47c339f":"code","46de4b52":"code","fde6b016":"code","0658ff6e":"code","9cfd5ab6":"code","c874bb6c":"code","87f27945":"code","af5fa821":"code","4722dc1d":"code","d1d39916":"code","f9754c8b":"code","ffb54fbb":"code","9af01f97":"code","df61baa2":"code","7599bae7":"code","606c4500":"code","30299869":"code","5db00390":"code","9316f7ce":"code","1f11f5c6":"code","bc8d15a1":"code","6c6a5123":"code","13b1d211":"code","fdbac922":"code","c40b5871":"code","55439646":"code","931d28e3":"code","90b198c7":"code","1ff54e25":"code","052e3978":"code","7f6fbbdc":"code","7efde578":"code","50be1eb6":"code","db91add9":"code","c207f343":"code","a4196502":"code","dcab53b5":"code","4a693638":"code","16b9bff0":"code","aac554ee":"code","5a84216c":"code","b23cce78":"code","22c5453a":"code","2b1e1eb0":"code","c6f3b98a":"code","7a09f1af":"code","8013e820":"code","bf52bfab":"code","5983c5e9":"code","4dd77a8e":"code","febf09cd":"code","e1da3727":"code","93c5baaa":"code","d7e93ce3":"code","2918591d":"code","7e455732":"code","f4ca9450":"code","9698f240":"markdown","c3e158d4":"markdown","00f8a0c1":"markdown","bcdc392a":"markdown","98f459ec":"markdown","9ffda35d":"markdown","a0e67fe3":"markdown","3f446ea0":"markdown","b7b88a39":"markdown","e3723e95":"markdown","4254241f":"markdown","55232a3d":"markdown","d13b488e":"markdown","345363d4":"markdown","0b4d4c01":"markdown","9fc153c1":"markdown","421812ec":"markdown","6c4ca9dd":"markdown","c2ce4107":"markdown","613ed2e1":"markdown","50f14de7":"markdown","fee238d8":"markdown","24346965":"markdown","dc99ff31":"markdown","b2b5dc53":"markdown","b4a96306":"markdown","43f85ba3":"markdown"},"source":{"02692a59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport ast\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport operator\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport eli5\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom sklearn.metrics import make_scorer\nimport lightgbm as lgb\nfrom PIL import Image\nfrom urllib.request import urlopen\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nplt.style.use('ggplot')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9e231816":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\n\ntrain_df.head()","4a0ad0dc":"train_df.shape","f4b57984":"\ndef fix_date(x):\n    \"\"\"\n    Fixes dates which are in 20xx\n    \"\"\"\n    year = x.split('\/')[2]\n    if int(year) <= 19:\n        return x[:-2] + '20' + year\n    else:\n        return x[:-2] + '19' + year\ntrain_df['release_date'] = pd.to_datetime(train_df['release_date'].apply(lambda x: fix_date(x)))\ntest_df.loc[test_df['release_date'].isnull() == True, 'release_date'] = '01\/01\/98'\ntest_df['release_date'] = pd.to_datetime(test_df['release_date'].apply(lambda x: fix_date(x)))\ntrain_df['release_date'].head()","53bee9ac":"# creating features based on dates\ndef process_date(df):\n    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n    for part in date_parts:\n        part_col = 'release_date' + \"_\" + part\n        df[part_col] = getattr(df['release_date'].dt, part).astype(int)\n    \n    return df\n\ntrain_df = process_date(train_df)\ntest_df = process_date(test_df)","c68d0221":"dict_columns = ['belongs_to_collection', 'genres', 'production_companies', 'production_countries', \n                'spoken_languages', 'Keywords', 'cast', 'crew']\ndef convert_dict(df):\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: ast.literal_eval(x) if not pd.isnull(x) else np.NaN)\n    return df","a8256262":"train_df = convert_dict(train_df)\ntest_df = convert_dict(test_df)","57cce1b6":"collections = train_df['belongs_to_collection']\ncollections[:5]","70cf9a18":"train_df['belongs_to_collection'].isnull().sum()","ffbc3424":"train_df['has_collection'] = train_df['belongs_to_collection'].apply(lambda x: 1 if not pd.isnull(x) else 0)\ntest_df['has_collection'] = test_df['belongs_to_collection'].apply(lambda x: 1 if not pd.isnull(x) else 0)\ntrain_df['has_collection'][:5]","427302c0":"train_df['collection_name'] = train_df['belongs_to_collection'].apply(lambda x: x[0]['name'] if not pd.isnull(x) else 0)\ntest_df['collection_name'] = test_df['belongs_to_collection'].apply(lambda x: x[0]['name'] if not pd.isnull(x) else 0)\ntrain_df['collection_name'][:5]","e47c339f":"train_df = train_df.drop(\"belongs_to_collection\", axis =1)\ntest_df = test_df.drop(\"belongs_to_collection\", axis =1)","46de4b52":"def dictToInd(var, num):\n    train_df[var] = train_df[var].fillna(0)\n    test_df[var] = test_df[var].fillna(0)\n    \n    train_df['num_' + var] = train_df[var].apply(lambda x: len(x) if x!= 0 else 0) \n    test_df['num_' + var] = test_df[var].apply(lambda x: len(x) if x!= 0 else 0) \n    \n    train_df['all_'+var] = train_df[var].apply(lambda x: [i['name'] for i in x] if x !=0 else 0)\n    test_df['all_'+var] = test_df[var].apply(lambda x: [i['name'] for i in x] if x !=0 else 0)\n    \n    d = dict()\n    \n    for i, e in enumerate(train_df['all_' + var]):\n        if e != 0:\n            for k in e:\n                if k in d:\n                    d[k] += 1\n                else:\n                    d[k] = 1\n    top_d = dict(sorted(d.items(), key = operator.itemgetter(1), reverse = True)[:num])\n    \n    for i in top_d:\n        train_df[var +'_' + i] = train_df['all_' + var].apply(lambda x: 1 if x!=0 and i in x else 0)\n        test_df[var +'_' + i] = test_df['all_' + var].apply(lambda x: 1 if x!=0 and i in x else 0)","fde6b016":"dictToInd('genres', 30)\ntrain_df.head()","0658ff6e":"train_df['num_genres'].value_counts()","9cfd5ab6":"genres = []\n\nfor i, e in enumerate(train_df['all_genres']):\n    if e != 0:\n        for genre in e:\n            if genre not in genres:\n                genres.append(genre)\n            \nprint(genres)","c874bb6c":"train_df['genres'].head()\nlist_of_genres = train_df['genres'].apply(lambda x: x if x != {} else [])\nlist_of_genres","87f27945":" train_df['production_companies'].fillna(0).apply(lambda x: len(x) if x != 0 else 0).value_counts()","af5fa821":"dictToInd('production_companies', 6)\ntrain_df.head()","4722dc1d":" train_df['production_countries'].fillna(0).apply(lambda x: len(x) if x != 0 else 0).value_counts()","d1d39916":"dictToInd('production_countries',5)\ntrain_df.head()","f9754c8b":"train_df['spoken_languages'].fillna(0).apply(lambda x: len(x) if x != 0 else 0).value_counts()","ffb54fbb":"dictToInd('spoken_languages', 5)\ntrain_df.head()","9af01f97":"train_df['Keywords'].fillna(0).apply(lambda x: len(x) if x !=0 else 0).value_counts()","df61baa2":"train_df['Keywords']","7599bae7":"list_of_keywords = list(train_df['Keywords'].fillna(0).apply(lambda x: [i['name'] for i in x] if x != 0 else []).values)\nlist_of_keywords[:19]","606c4500":"plt.figure(figsize=(12,18))\ntext = ' '.join([i for j in list_of_keywords for i in j])\nwordcloud = WordCloud(max_font_size = None, background_color='white', collocations=False,\n                      width = 1200, height=1000).generate(text)\nplt.imshow(wordcloud)\n","30299869":"dictToInd('Keywords', 10)\ntrain_df.head()","5db00390":"train_df['cast'].fillna(0).apply(lambda x: len(x) if x != 0 else 0).value_counts()","9316f7ce":"dictToInd('cast', 10)\ntrain_df.head()","1f11f5c6":"dictToInd('crew', 10)\ntrain_df.head()","bc8d15a1":"for i in train_df.columns:\n    num_nas = train_df[i].isnull().sum()\n    if num_nas > 0:\n        print(i, \" :\", num_nas)","6c6a5123":"for i in test_df.columns:\n    num_nas = test_df[i].isnull().sum()\n    if num_nas > 0:\n        print(i, \" :\", num_nas)","13b1d211":"test_df['status'].value_counts()","fdbac922":"train_df['runtime'] = train_df['runtime'].fillna(train_df['runtime'].median())\ntest_df['runtime'] = test_df['runtime'].fillna(test_df['runtime'].median())\ntrain_df['tagline'] = train_df['tagline'].fillna('')\ntest_df['tagline'] = test_df['tagline'].fillna('')\n\ntrain_df['homepage'] = train_df['homepage'].fillna(0)\ntest_df['homepage'] = test_df['homepage'].fillna(0)\n\ntrain_df['has_tagline'] = train_df['tagline'].apply(lambda x: 1 if x != '' else 0)\ntest_df['has_tagline'] = test_df['tagline'].apply(lambda x: 1 if x != '' else 0)\n\ntrain_df['has_homepage'] = train_df['homepage'].apply(lambda x: 1 if x != 0 else 0)\ntest_df['has_homepage'] = test_df['homepage'].apply(lambda x: 1 if x != 0 else 0)\n\ntrain_df['budget'] = train_df['budget'].apply(lambda x: x if x > 0 else np.NaN)\ntest_df['budget'] = test_df['budget'].apply(lambda x: x if x > 0 else np.NaN)\n\ntrain_df['budget'] = train_df['budget'].fillna(train_df['budget'].median())\ntest_df['budget'] = test_df['budget'].fillna(test_df['budget'].median())\n\ntest_df['status'] = test_df['status'].fillna(test_df['status'].mode()[0])\n","c40b5871":"le = LabelEncoder()\ntrain_df['status'] = le.fit_transform(train_df['status'])\ntest_df['status'] = le.fit_transform(test_df['status'])\ntrain_df['status'].value_counts()","55439646":"fig, axes = plt.subplots(1,2, figsize=(16,6))\nsns.set(color_codes=True, style='darkgrid')\nsns.distplot(train_df['revenue'], color ='y', ax=axes[0]).set_title('Revenue Distribution')\nsns.distplot(np.log1p(train_df['revenue']), color='b', ax=axes[1]).set_title('Log Revenue Distribution')\n","931d28e3":"fig, axes = plt.subplots(2,2, figsize=(16,8))\nsns.set(color_codes=True, style='darkgrid')\nsns.distplot(train_df['budget'], color ='y', ax=axes[0,0]).set_title('Budget Distribution')\nsns.distplot(np.log1p(train_df['budget']), ax=axes[0,1]).set_title('Log Budget Distribution')\n\nsns.scatterplot(train_df['budget'], train_df['revenue'],color ='y', ax=axes[1,0]).set_title('Revenue vs. Budget')\nsns.scatterplot(np.log1p(train_df['budget']), np.log1p(train_df['revenue']), ax=axes[1,1]).set_title('Log Revenue vs. Log Budget')","90b198c7":"fig, axes = plt.subplots(1,2, figsize=(16,6))\nplt.subplots_adjust(top=0.9)\nfig.suptitle('Revenue for film with and without tagline\/homepage')\nsns.catplot(x='has_homepage', y='revenue', ax=axes[0], data=train_df);\n\nsns.catplot(x='has_tagline', y='revenue', ax=axes[1], data=train_df)\nplt.close(2)\nplt.close(3)","1ff54e25":"train_df['log_revenue'] = np.log1p(train_df['revenue'])\ntrain_df['log_budget'] = np.log1p(train_df['budget'])\ntest_df['log_budget'] = np.log1p(test_df['budget'])","052e3978":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nsns.boxplot(x='original_language', y='revenue', data=train_df.loc[train_df['original_language'].isin(train_df['original_language'].value_counts().head(10).index)]);\nplt.title('Mean revenue per language');\nplt.subplot(1, 2, 2)\nsns.boxplot(x='original_language', y='log_revenue', data=train_df.loc[train_df['original_language'].isin(train_df['original_language'].value_counts().head(10).index)]);\nplt.title('Mean log revenue per language');","7f6fbbdc":"vectorizer = TfidfVectorizer(\n            sublinear_tf=True,\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            ngram_range=(1, 2),\n            min_df=5)\n\noverview_text = vectorizer.fit_transform(train_df['overview'].fillna(''))\nlinreg = LinearRegression()\nlinreg.fit(overview_text, train_df['log_revenue'])\neli5.show_weights(linreg, vec=vectorizer, top=20, feature_filter=lambda x: x != '<BIAS>')","7efde578":"print('Target value:', train_df['log_revenue'][10])\neli5.show_prediction(linreg, doc=train_df['overview'].values[10], vec=vectorizer)","50be1eb6":"train_texts = train_df[['title', 'tagline', 'overview', 'original_title']]\ntest_texts = test_df[['title', 'tagline', 'overview', 'original_title']]\n\ntrain_df[['title', 'tagline', 'overview', 'original_title']].head()","db91add9":"for col in ['title','overview', 'tagline','original_title']:\n    train_df['len_' + col] = train_df[col].fillna('').apply(lambda x: len(str(x)))\n    train_df['words_' + col] = train_df[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    train_df = train_df.drop(col, axis=1)\n    test_df['len_' + col] = test_df[col].fillna('').apply(lambda x: len(str(x)))\n    test_df['words_' + col] = test_df[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    test_df = test_df.drop(col, axis=1)","c207f343":"f, axes = plt.subplots(4, 5, figsize=(24, 16))\nplt.suptitle('Violinplot of revenue vs genres')\nfor i, e in enumerate([col for col in train_df.columns if 'genres_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train_df, ax=axes[i \/\/ 5][i % 5]);","a4196502":"plt.style.use('ggplot')\nd1 = train_df['release_date_year'].value_counts().sort_index()\nd2 = test_df['release_date_year'].value_counts().sort_index()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='train'), go.Scatter(x=d2.index, y=d2.values, name='test')]\nlayout = go.Layout(dict(title = \"Number of films per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","dcab53b5":"f = ['budget', 'popularity', 'runtime', 'revenue', 'log_revenue', 'log_budget']\nsns.pairplot(train_df[f].dropna())","4a693638":"corr = train_df.drop('release_date', axis=1).corr()\ncorr.style.background_gradient(cmap='Blues')","16b9bff0":"train_df = pd.concat([train_df, pd.get_dummies(train_df['release_date_weekday'], prefix='release_weekday')], axis=1)\ntrain_df = pd.concat([train_df, pd.get_dummies(train_df['release_date_month'], prefix='release_month')], axis=1)\ntrain_df = pd.concat([train_df, pd.get_dummies(train_df['release_date_quarter'], prefix='release_quarter')], axis=1)\n\ntest_df = pd.concat([test_df, pd.get_dummies(test_df['release_date_weekday'], prefix='release_weekday')], axis=1)\ntest_df = pd.concat([test_df, pd.get_dummies(test_df['release_date_month'], prefix='release_month')], axis=1)\ntest_df = pd.concat([test_df, pd.get_dummies(test_df['release_date_quarter'], prefix='release_quarter')], axis=1)","aac554ee":"def new_features(df):\n    df['budget_to_popularity'] = df['budget'] \/ df['popularity']\n    df['budget_to_runtime'] = df['budget'] \/ df['runtime']\n    \n    # some features from https:\/\/www.kaggle.com\/somang1418\/happy-valentines-day-and-keep-kaggling-3\n    df['_budget_year_ratio'] = df['budget'] \/ (df['release_date_year'] * df['release_date_year'])\n    df['_releaseYear_popularity_ratio'] = df['release_date_year'] \/ df['popularity']\n    df['_releaseYear_popularity_ratio2'] = df['popularity'] \/ df['release_date_year']\n    \n    df['runtime_to_mean_year'] = df['runtime'] \/ df.groupby(\"release_date_year\")[\"runtime\"].transform('mean')\n    df['popularity_to_mean_year'] = df['popularity'] \/ df.groupby(\"release_date_year\")[\"popularity\"].transform('mean')\n    df['budget_to_mean_year'] = df['budget'] \/ df.groupby(\"release_date_year\")[\"budget\"].transform('mean')\n        \n    return df","5a84216c":"train_df =new_features(train_df)\ntest_df =new_features(test_df)\ntrain_df.head()","b23cce78":"y = train_df['log_revenue']\nX = train_df.drop(['id', 'genres', 'homepage', 'imdb_id','original_language', 'poster_path', 'production_companies', 'production_countries', 'spoken_languages',\n                  'Keywords', 'cast', 'crew', 'revenue', 'collection_name','all_genres', 'all_production_companies', 'all_production_countries', 'all_spoken_languages', \n                        'all_Keywords', 'all_cast', 'all_crew', 'release_date', 'revenue', 'log_revenue', 'budget', 'release_date_quarter',\n                  'release_date_day', 'release_date_weekofyear', 'release_date_month', 'release_date_weekday', \n                   'budget_to_runtime' \n                  ], axis = 1)\nX_test= test_df.drop(['id', 'genres', 'homepage', 'imdb_id','original_language',  'poster_path', 'production_companies', 'production_countries', 'spoken_languages',\n                   'Keywords', 'cast', 'crew', 'collection_name','all_genres', 'all_production_companies', 'all_production_countries', 'all_spoken_languages', \n                        'all_Keywords', 'all_cast', 'all_crew', 'release_date', 'budget', 'release_date_quarter', 'release_date_day',\n                     'release_date_weekofyear', 'release_date_month', 'release_date_weekday', \n                       'budget_to_runtime'\n                     ], axis = 1)\nX.head()","22c5453a":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1)","2b1e1eb0":"simple_forest = RandomForestRegressor(criterion = 'mse',\n                              n_estimators =100,\n                              random_state = 1)","c6f3b98a":"simple_forest.fit(X_train, y_train)\nsimple_forest_pred = simple_forest.predict(X_valid)","7a09f1af":"mse = mean_squared_error(simple_forest_pred, y_valid)\nmse","8013e820":"simple_forest_pred = simple_forest.predict(X_test)\nsimple_forest_rev = np.expm1(simple_forest_pred)\nos.chdir('..\/working')\nsubmit = pd.DataFrame({'Id': test_df['id'], 'revenue': simple_forest_rev})\nsubmit.to_csv('submission_simple_forest.csv', index=False)\nsubmit.head()","bf52bfab":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\nlight_gbm = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1, random_state=1)\nlight_gbm.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=200)","5983c5e9":"np.square(1.82195)","4dd77a8e":"eli5.show_weights(light_gbm, feature_filter=lambda x: x != '<BIAS>')","febf09cd":"light_gbm_pred = light_gbm.predict(X_test)\nlight_gbm_rev = np.expm1(light_gbm_pred)\nos.chdir('..\/working')\nsubmit = pd.DataFrame({'Id': test_df['id'], 'revenue': light_gbm_rev})\nsubmit.to_csv('submission_light_gbm.csv', index=False)\nsubmit.head()","e1da3727":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 8, 10, 15]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 3, 5, 7, 10]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","93c5baaa":"rf_best = RandomForestRegressor(n_estimators=1400,\n                               max_features= 'auto',\n                               max_depth= 80,\n                               min_samples_split= 2,\n                               min_samples_leaf= 3,\n                               bootstrap= True,\n                               random_state=1)","d7e93ce3":"rf_best.fit(X_train, y_train)\nrf_best_pred = rf_best.predict(X_valid)\nmse_best= mean_squared_error(rf_best_pred, y_valid)\nmse_best","2918591d":"rf_best_pred = rf_best.predict(X_test)\nrf_best_pred[:5]","7e455732":"feature_importances = pd.DataFrame(rf_best.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\nplt.barh(feature_importances.index[:15],feature_importances['importance'][:15])","f4ca9450":"rf_best_rev = np.expm1(rf_best_pred)\nos.chdir('..\/working')\nsubmit = pd.DataFrame({'Id': test_df['id'], 'revenue': rf_best_rev})\nsubmit.to_csv('submission_rf_best.csv', index=False)\nsubmit.head()","9698f240":"The movies have between 0 and 7 genres, with most having between 2 and 5 genres. below we can see there are 20 possible genres","c3e158d4":"Now we need to eliminate the nulls by filling them in or removing columns with too many NAs. ","00f8a0c1":"**Budget**","bcdc392a":"Since almost 80% of the records do not belong to a collection, most of this information will not be useful for modeling. For now, we will create two additional variables:\n\n* has_collection: This will indicate wheather the movie has a collection or not\n* collection_name: We will just keep the name of the collection for the movies with a collection","98f459ec":"The above plots show that budget is a good predictor of revenue and popularity has a weak correlation with revenue. Movies that are very short or very long tend to make less revenue than movies of an averga (1.5-2hr) length.","9ffda35d":"The distribution of revenue varies by the genre of the movie. ","a0e67fe3":"Turn month and day of week into dummy variables for modeling and create some additional variables.","3f446ea0":"There's lots of possibilities for cast and number of cast, we'll create indicators for the 10 most popular actors, we'll do the same for crew. Perhaps popular actors or crew memebers impact the amount of revenue the movie will make.","b7b88a39":"Most the movies only have one production country, but we'll make indicators for the top 5 most popular ones","e3723e95":"We can see above that some words within the movies overview have a correlation with the amount of revenue the movie made and some words reduce the amount of revenue the movie made.","4254241f":"#Use the random grid to search for best hyperparameters**\n#First create the base model to tune\nrf = RandomForestRegressor()\n#Random search of parameters, using 3 fold cross validation, \n#search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n#Fit the random search model\nrf_random.fit(X_train, y_train)\n\nrf_random.best_params_","55232a3d":"**Load and review the training data**","d13b488e":"The release data is stored as an object, convert the object to a datetime and create additional variables for year, month, day of week, week of year, quarter and day.","345363d4":"**Overview**\n\nThis kernel will use data from [The Movie Database](https:\/\/www.themoviedb.org\/) (TMDB) to predict worldwide movie revenue. The movie industry is on the rise and box office sales have skyrocketed, can we use data about past movie revenue to predict future revenue? Could this information be used to create more profitable movies","0b4d4c01":"**Columns with NAN values**\n* homepage: This column is mostly null so we will drop the column\n* overview: We can serch the overview for popular key words to see if we can utilize this column\n* poster_path: This is a link to the moview poster, we will remove this column from this analysis\n* runtime: We can fill in the 2 NAN values with the median\n* tagline: This is the tagline for the movie, we will create an indicator for if the movie has a tagline and remove this column","9fc153c1":"We need to address the remaining characters values we want to utilize as predictors, we'll use sklearn LabelEncoder to convert them to numeric. Most of the movies are released so status will probbably not be a useful predictor.","421812ec":"**TARGET**\n* The distribution of Revenue is highly skewed, It will be better to use Log Revenue for modeling purposes.","6c4ca9dd":"It does seem as though some languages produce greater revenue than others, it does make sense that langagues spoken in more places of the world (ie. English) would have the oppurtunity for greater revenue.","c2ce4107":"Create variables to count the number of genres, production companies, production countries, and spoken languages. In addition I will create dummy variables for the different genres, companies, countries and languages. ","613ed2e1":"The correlation matrix above shows the variables with the strongest correlation to revenue are:\n* Budget: 0.752965\n* Popularity: 0.46146\n* Num Crew: 0.37211\n* Has Collection: 0.339425\n* Num Cast: 0.335737\n* Genre Adevenuture: 0.328439\n* Has Homepage: 0.263179\n* Runtime: 0.216417","50f14de7":"The movies have between 1 and 17 production companies with most having 6 or less. We'll add the top 6 most popular production companies","fee238d8":"There's lots of possible keywords, but some that appear most often are relationship, woman, love, director, based, murder and film. \nI'll start by pulling in the top 10 most popular key words to see if the have any predictive power.","24346965":"More movies are being produced each year so the total movie revenue by year is going up.","dc99ff31":"**DATA CLEANING AND VARIABLE EXTRACTION**\n\n* Some of the fields contain lists with dictionaries, but these columns are stored as text, we need to convert them back to dictionaries to view all the data\n* Extract relevant fields from the dictionaries, for genre, language, production countires and production companies\n* Most do not belong to a collection so I will just create a flag for those that do belong to a collection and keep the collection name.","b2b5dc53":"It does appear that movies with a homepage and\/or tagline produce greater revenue, maybe this is because people are able to learn more about the movie.","b4a96306":"The Gradient Boosting Model had the best results and I would recommend performing a full GridSearchCV on that model to optimize the model and obtain the best possible parameters.","43f85ba3":"**MODELING**\n\nLets create our X and y, the y variable is revenue and we'll drop the predictors that we wont be using for our X variable.\n\nModel Evaluation:\n* Simple Random Forest Model: mse=3.79\n* Gradient Boosting Model: mse: mse=3.32\n* Random Forest Model Optimized with RandomizedSearchCV: mse=3.55"}}