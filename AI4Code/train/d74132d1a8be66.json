{"cell_type":{"f360f0f1":"code","39476e0a":"code","777a7ebd":"code","45c88048":"code","a17bb5d2":"code","bc6da08e":"code","3089485f":"code","b029ef81":"code","c09c1492":"code","74cb719a":"code","5a8e9743":"code","6a48cbf9":"code","23f67230":"code","0165dbd9":"code","831bcd24":"code","0753cd3d":"markdown"},"source":{"f360f0f1":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import cohen_kappa_score,confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom functools import partial \nimport scipy as sp","39476e0a":"df = pd.read_csv('..\/input\/train\/train.csv')","777a7ebd":"dfDropped = df.drop(['Name','RescuerID','Description','PetID','AdoptionSpeed'],axis=1)\nX = dfDropped.values\ny = df[\"AdoptionSpeed\"]","45c88048":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = cohen_kappa_score(y, X_p,weights='quadratic')\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']","a17bb5d2":"#from sklearn.model_selection import StratifiedKFold\n#def skfold_gen(X,y,n_splits=5, random_state=42, shuffle=True):\n#    for train, test in skf.split(X, y):\n#        yield train, test","bc6da08e":"#clf = GridSearchCV(RandomForestRegressor()\n#                   , dict(max_depth=[5,10,15,20], n_estimators=[ 200, 250,300,350,400,450]), cv=skfold_gen(X,y),\n#                 scoring='neg_mean_squared_error',verbose=2,n_jobs=3)\n\n#clf.fit(X,y)","3089485f":"# (-1.1598095155985777, {'max_depth': 10, 'n_estimators': 300})\n#\u00a0clf.best_score_,clf.best_params_","b029ef81":"#\u00a0optr.fit(clf.predict(X),y)\n#\u00a0optr.coefficients()\n# array([0.44385399, 2.07305103, 2.47256317, 2.93407633])","c09c1492":"rfr = RandomForestRegressor(max_depth=10, n_estimators=300)\nrfr.fit(X,y)","74cb719a":"optr = OptimizedRounder()\noptr.fit(rfr.predict(X),y)","5a8e9743":"optr.coefficients()","6a48cbf9":"df_t = pd.read_csv('..\/input\/test\/test.csv')\nX_t = df_t.drop(['Name','RescuerID','Description','PetID'],axis=1).values","23f67230":"X_t.shape","0165dbd9":"import pandas as pd\n\nsubmission = pd.DataFrame(dict(PetID=df_t['PetID'], AdoptionSpeed=optr.predict(rfr.predict(X_t), optr.coefficients()).astype(int)))\nsubmission.to_csv('submission.csv', index=False)","831bcd24":"!head submission.csv","0753cd3d":"In this notebook I try to gather up 3 basic ideas in various kernels with a simple sklearn RF model. Thanks to work in [PetFinder Simple LGBM Baseline](https:\/\/www.kaggle.com\/skooch\/petfinder-simple-lgbm-baseline)\n1. Use a regressor instead of a classifier. \n  * That's simply because in many standard classifiers misclassification of 0 as 1 and 0 as 4 have the same 0\/1 penalty.\n  * However `weights=quadratic` makes $0 \\rightarrow 1$ less costly than $0 \\rightarrow 4$, which can better be represented by a RMSE like error function\n2. Since you will implement a regressor, we need to define `cv`manually to have proper strafied sampling.\n3. Use `OptimizedRounder` idea that everybody uses to optimize class boundries.\n  * Natural idea is to classify a regressor outcome `0.44` as class 0 but it turns out to be classifying it as 1 yields a higher $\\kappa$-score\n  \n  Following those 3 ideas, I increase my $\\kappa$-score from `0.310` (RF Classifier) to `0.360`(RF Regressor + OptimizedRounder + manual cv)"}}