{"cell_type":{"307938f1":"code","eb1db864":"code","24d34e1e":"code","9cb7eba8":"code","84d88c44":"code","95c5f198":"code","77f1a50f":"code","1c2ed44d":"code","2a6a276c":"code","b4bfcb31":"code","911885ed":"code","9d73dd4d":"code","db26ee1c":"code","b5199f96":"code","46168277":"code","d741306a":"code","995915ed":"code","94985d1f":"code","cb72d068":"code","19c8c180":"code","e6a7f678":"code","6e2c0952":"code","183729de":"code","e3d2f34a":"code","be0ff9b7":"code","adb6d4c4":"code","adab1df3":"code","7aeeb28f":"code","b2c7e51c":"markdown","f11a1231":"markdown","31fed8ae":"markdown","abcd2bcd":"markdown","595ed999":"markdown","722a2782":"markdown","ad66efd5":"markdown","3e715b5d":"markdown","c5440199":"markdown","56588d2e":"markdown","2214a5d0":"markdown","4a4af0f2":"markdown","2d27cc20":"markdown","fc130250":"markdown"},"source":{"307938f1":"###########################################\n# Suppress matplotlib user warnings\n# Necessary for newer version of matplotlib\nimport warnings\nwarnings.filterwarnings(\"ignore\", category = UserWarning, module = \"matplotlib\")\n#\n# Display inline matplotlib plots with IPython\nfrom IPython import get_ipython\nget_ipython().run_line_magic('matplotlib', 'inline')\n###########################################\n\nimport matplotlib.pyplot as pl\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import ShuffleSplit, train_test_split, learning_curve, validation_curve, KFold, cross_val_score, GridSearchCV\nimport seaborn as sns\nimport pandas as pd\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Lasso, ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error,make_scorer,r2_score","eb1db864":"# Pretty display for notebooks\n%matplotlib inline\n\n# Load the Boston housing dataset\nfilename = (\"..\/input\/housing.csv\")\nnames = ['CRIME', 'ResZoNe', 'N-Retail Bussiness', 'CHARS River', 'NitrOxd', 'Rooms\/dwell', 'AGE', 'Dis to comp', 'Dist to highway',\n         'TAX', 'PTRATIO', 'B','LSTAT', 'MedV']\ndataset = pd.read_csv(filename, delim_whitespace=True, names=names)\nprices = dataset['MedV']\nfeatures = dataset.drop('MedV', axis = 1)\n# Success\nprint(\"Boston housing dataset has {} data points with {} variables each.\".format(*dataset.shape))","24d34e1e":"features.head(10)","9cb7eba8":"# histograms\ndataset.hist(figsize=(9,7),grid=False);","84d88c44":"features=features.drop(['CRIME', 'ResZoNe', 'CHARS River', 'NitrOxd', 'AGE', 'Dis to comp',  'TAX', 'B',], axis=1)","95c5f198":"features.head()","77f1a50f":"sns.distplot(prices)","1c2ed44d":"corr=dataset.corr()\npl.figure(figsize=(10, 10))\nsns.heatmap(corr, linewidths=0.01,\n            square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\npl.title('Correlation between features');","2a6a276c":"features=features.drop(['N-Retail Bussiness','Dist to highway'], axis=1)","b4bfcb31":"features.head()","911885ed":"\n# Feature observation\npl.figure(figsize=(20, 5))\nfor i, col in enumerate(features.columns):\n    pl.subplot(1, 13, i+1)\n    pl.plot(dataset[col], prices, 'o')\n    pl.title(col)\n    pl.xlabel(col)\n    pl.ylabel('prices')","9d73dd4d":"X_train, X_test, y_train, y_test = train_test_split(features, prices,test_size=0.2, random_state=31)\n\nprint(\"Training and testing split was successful.\")","db26ee1c":"# Test options and evaluation metric using Root Mean Square error method\nnum_folds = 10\nseed = 7\nRMS = 'neg_mean_squared_error'","b5199f96":"# Spot Check Algorithms\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))","46168277":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=RMS)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","d741306a":"def boxplot():\n    fig = pl.figure()\n    fig.suptitle('Algorithm Comparison')\n    ax = fig.add_subplot(111)\n    pl.boxplot(results)\n    ax.set_xticklabels(names)\n    pl.show()\nboxplot()","995915ed":"pipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\npipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\npipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\npipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=RMS)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","94985d1f":"boxplot()","cb72d068":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nk_values = np.array([1,3,5,7,9,11,13,15,17,19,21])\nparam_grid = dict(n_neighbors=k_values)\nmodel = KNeighborsRegressor()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=RMS, cv=kfold)\ngrid_result = grid.fit(rescaledX, y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","19c8c180":"ensembles = []\nensembles.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor(n_neighbors=7))])))\nensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor())])))\nensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\nensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor(n_estimators=100))])))\nensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor())])))\nresults = []\nnames = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=RMS)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","e6a7f678":"boxplot()","6e2c0952":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nparam_grid = dict(n_estimators=range(21,41,1))\nmodel_train = GradientBoostingRegressor(random_state=seed)\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model_train, param_grid=param_grid, scoring=RMS, cv=kfold)\ngrid_result = grid.fit(rescaledX, y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","183729de":"def PlotLearningCurve(X, y):\n    \"\"\" Calculates the performance of several models with varying sizes of training data.\n        The learning and testing scores for each model are then plotted. \"\"\"\n\n    # Create 10 cross-validation sets for training and testing\n    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n\n    # Generate the training set sizes increasing by 50\n    train_sizes = np.rint(np.linspace(1, X.shape[0]*0.8 - 1, 9)).astype(int)\n\n    # Create the figure window\n    fig = pl.figure(figsize=(10,7))\n\n    # Create three different models based on n_estimators\n    for k, n_est in enumerate([20,25,30,35]):\n        \n        regressor=GradientBoostingRegressor(n_estimators=n_est)\n        # Calculate the training and testing scores\n        sizes, train_scores, test_scores = learning_curve(regressor, X, y, \\\n            cv = cv, train_sizes = train_sizes, scoring = 'r2')\n\n        # Find the mean and standard deviation for smoothing\n        train_std = np.std(train_scores, axis = 1)\n        train_mean = np.mean(train_scores, axis = 1)\n        test_std = np.std(test_scores, axis = 1)\n        test_mean = np.mean(test_scores, axis = 1)\n\n        # Subplot the learning curve\n        ax = fig.add_subplot(2, 2, k+1)\n        ax.plot(sizes, train_mean, 'o-', color = 'r', label = 'Training Score')\n        ax.plot(sizes, test_mean, 'o-', color = 'g', label = 'Testing Score')\n        ax.fill_between(sizes, train_mean - train_std, \\\n            train_mean + train_std, alpha = 0.15, color = 'r')\n        ax.fill_between(sizes, test_mean - test_std, \\\n            test_mean + test_std, alpha = 0.15, color = 'g')\n\n        # Labels\n        ax.set_title('n_estimators = %s'%(n_est))\n        ax.set_xlabel('Number of Training Points')\n        ax.set_ylabel('Score')\n        ax.set_xlim([0, X.shape[0]*0.8])\n        ax.set_ylim([-0.05, 1.05])\n\n    # Visual aesthetics\n    ax.legend(bbox_to_anchor=(1.05, 2.05), loc='lower left', borderaxespad = 0.)\n    fig.suptitle('Learning Algorithm LC Performances', fontsize = 16, y = 1.03)\n    fig.tight_layout()\n    fig.show()","e3d2f34a":"def PLotComplexityCurve(X, y):\n    \"\"\" Calculates the performance of the model as model complexity increases.\n        The learning and testing errors rates are then plotted. \"\"\"\n\n    # Create 10 cross-validation sets for training and testing\n    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n\n    # Vary the n_estimators parameter from 1 to 100\n    n_estimators = np.arange(1,100)\n\n    # Calculate the training and testing scores\n    train_scores, test_scores = validation_curve(GradientBoostingRegressor(), X, y, \\\n        param_name = \"n_estimators\", param_range = n_estimators, cv = cv, scoring = 'r2')\n\n    # Find the mean and standard deviation for smoothing\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    # Plot the validation curve\n    pl.figure(figsize=(7, 5))\n    pl.title('Learning algorithm Complexity Performance')\n    pl.plot(n_estimators, train_mean, 'o-', color = 'r', label = 'Training Score')\n    pl.plot(n_estimators, test_mean, 'o-', color = 'g', label = 'Validation Score')\n    pl.fill_between(n_estimators, train_mean - train_std, \\\n        train_mean + train_std, alpha = 0.15, color = 'r')\n    pl.fill_between(n_estimators, test_mean - test_std, \\\n        test_mean + test_std, alpha = 0.15, color = 'g')\n\n    # Visual aesthetics\n    pl.legend(loc = 'lower right')\n    pl.xlabel('N Estimators')\n    pl.ylabel('Score')\n    pl.ylim([-0.05,1.05])\n    pl.show() ","be0ff9b7":"PlotLearningCurve(X_train, y_train)","adb6d4c4":"PLotComplexityCurve(X_train, y_train)","adab1df3":"# prepare the model\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = GradientBoostingRegressor(random_state=seed, n_estimators=28)\nmodel.fit(rescaledX, y_train)\n# transform the validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = model.predict(rescaledValidationX)\nprint(mean_squared_error(y_test, predictions))","7aeeb28f":"predictions=predictions.astype(int)\nsubmission = pd.DataFrame({\n        \"Org House Price\": y_test,\n        \"Pred House Price\": predictions\n    })\n\nprint(submission.head(10))","b2c7e51c":"Variables having strong Corelation with target varible MEDV are RM (.7), LSTAT(-.74), PTRATIO(-.51). dropping others","f11a1231":"# Produce learning curves for varying training set sizes and number of regressors","31fed8ae":"# Various Ensemblor regressors","abcd2bcd":"# Data visualizations","595ed999":"Prices are deviated from normal distribution have appreciable positive skewness and showed peakedness","722a2782":"## Standardize the dataset","ad66efd5":"# Evaluate Each Model","3e715b5d":"# Comparing Regressors","c5440199":"We can see that some variables have an exponential distribution like B,Crime, Chars river, res zone. And Dist to highway, tax had bimodal dist\nI do feel medv, rooms\/dwell, PTRatio & lstat are important removing other noise","56588d2e":"# Comparing Scaled Regressors","2214a5d0":"## Dataset Import","4a4af0f2":"# Tuning scaled Gradient Boost Model","2d27cc20":"# Produce complexity curves for varying training set sizes and number of regressors","fc130250":"# Correlation HeatMap"}}