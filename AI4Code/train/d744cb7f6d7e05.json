{"cell_type":{"24d2396a":"code","d7979af4":"code","1a3849f0":"code","c12d159e":"code","2a1105e0":"code","c60194f2":"code","365db4e2":"code","b2b18ca6":"code","91968d3d":"code","974ec788":"code","ff013d67":"code","d4775fcb":"code","ecf18d3d":"code","d3c06df8":"code","a2acea21":"code","99008600":"code","8e4f2157":"code","cc752115":"code","796c9ec9":"code","c15ce53d":"code","d4ba57cb":"code","84432644":"code","d5aacbdf":"code","f1befe86":"code","ef97865c":"code","3c635e11":"code","194b2750":"code","a2f09cde":"code","389c8233":"code","0c204e9e":"code","a323ddd4":"code","e738b813":"code","a956e439":"code","9cfa51c4":"code","e85e2d42":"code","09f64782":"code","6d9f949d":"code","d42aacaa":"code","d31344a1":"code","1df8d1e6":"code","5b16c2a0":"code","509199ad":"code","9a7f605c":"code","112f7519":"code","1149d70f":"code","47134f92":"code","36f06b89":"code","3e1f240b":"code","c6028a1d":"code","3354e3df":"code","e8983c4e":"code","d5304a6f":"code","4599970f":"code","a8afda41":"code","629e6022":"code","df76d2d0":"code","c28648b9":"code","93a3529a":"code","eed921ad":"code","7df4bb2d":"code","23db241f":"code","6f178001":"code","24ae76a9":"code","8d0536c6":"code","c021b253":"code","75ec950c":"code","c9f9c9e2":"code","b4f5609b":"code","b1ac3032":"code","296c699c":"code","9009c111":"code","198aae8f":"code","4ab48b23":"code","4d1c6737":"code","b03f0cdc":"code","18d4a58d":"code","901bc1c6":"code","38bdb467":"code","ce6dcbf0":"code","34a1cab6":"code","039c1e37":"code","db44573a":"code","2e8f8801":"code","98dd083f":"code","9424e73e":"code","2874a817":"code","339c0354":"code","063df14b":"code","0a21266e":"code","5df9c7ef":"code","a187f67c":"code","a0ad0505":"code","b1a301d3":"code","fc4aec33":"code","36fbd1bd":"code","549a2d8d":"code","499057a4":"code","33a303e0":"code","7083ee55":"code","e60a8018":"code","0f927bb7":"code","c443f44f":"code","c7c82687":"code","ad94659c":"code","dd6d3fdb":"code","d0e26600":"code","3752db0b":"code","fdba164f":"code","2217d3a7":"code","2468d20f":"code","80fd2fc2":"code","c21a8cd5":"code","34e487ec":"code","7954bbf8":"code","e63d3d34":"code","9ec8d319":"code","d951277d":"code","2d4c4deb":"code","fe0d12b4":"code","bd3902b9":"code","3483f489":"code","e35b1d8c":"code","60106ecb":"code","773e51c9":"markdown","fa8b063b":"markdown","510d0da9":"markdown","b6e8ca91":"markdown","b22ef16d":"markdown","a98683b1":"markdown","b455439c":"markdown","6367e1d5":"markdown","1582ff7b":"markdown","cebceeef":"markdown","c9fbf556":"markdown","f3f1dab9":"markdown","a9c51f41":"markdown","b9c3fba3":"markdown","c7ef352f":"markdown","9c750a06":"markdown","b19657d1":"markdown","54becdea":"markdown","ab207e4f":"markdown","87de4270":"markdown","20422fb2":"markdown","6a04fee4":"markdown","25bc50c4":"markdown","5c6673ca":"markdown","e66fc6cc":"markdown","058b3fa1":"markdown","a0872b65":"markdown","a5c55d59":"markdown","615ba189":"markdown","5150bc63":"markdown","42b22b2a":"markdown","47d7b749":"markdown","f1bac1b5":"markdown","635f9ab8":"markdown","ae6b6b88":"markdown","1b36480f":"markdown"},"source":{"24d2396a":"from __future__ import print_function  # Compatability with Python 3\n\nprint( 'Print function ready to serve.')","d7979af4":"# NumPy for numerical computing\nimport numpy as np\n\n# Pandas for DataFrames\nimport pandas as pd\npd.set_option('display.max_columns', 100)\n\n# Matplotlib for visualization\nfrom matplotlib import pyplot as plt\n# display plots in the notebook\n%matplotlib inline \n\n# Seaborn for easier visualization\nimport seaborn as sns\n\nfrom scipy import stats\nsns.set()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n#warnings.filterwarnings(\"ignore\")\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"\/\"]).decode(\"utf8\"))\n\nimport os\nprint(os.listdir(\"..\/input\"))","1a3849f0":"# Load real estate data from CSV\ndf_train = pd.read_csv(\"..\/input\/train.csv\", index_col=0)\ndf_test = pd.read_csv(\"..\/input\/test.csv\", index_col=0)","c12d159e":"df_test_id_test = pd.read_csv(\"..\/input\/test.csv\")","2a1105e0":"# setting the number of cross validations used in the Model part \nnr_cv = 5\n\n# switch for using log values for SalePrice and features     \nuse_logvals = 1    \n# target used for correlation \ntarget = 'SalePrice_Log'\n    \n# only columns with correlation above this threshold value  \n# are used for the ML Regressors in Part 3\nmin_val_corr = 0.4    \n    \n# switch for dropping columns that are similar to others already used and show a high correlation to these     \ndrop_similar = 1","c60194f2":"def get_best_score(grid):\n    \n    best_score = np.sqrt(-grid.best_score_)\n    print(best_score)    \n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    \n    return best_score","365db4e2":"def print_cols_large_corr(df, nr_c, targ) :\n    corr = df.corr()\n    corr_abs = corr.abs()\n    print (corr_abs.nlargest(nr_c, targ)[targ])","b2b18ca6":"def plot_corr_matrix(df, nr_c, targ) :\n    \n    corr = df.corr()\n    corr_abs = corr.abs()\n    cols = corr_abs.nlargest(nr_c, targ)[targ].index\n    cm = np.corrcoef(df[cols].values.T)\n\n    plt.figure(figsize=(nr_c\/1.5, nr_c\/1.5))\n    sns.set(font_scale=1.25)\n    sns.heatmap(cm, linewidths=1.5, annot=True, square=True, \n                fmt='.2f', annot_kws={'size': 10}, \n                yticklabels=cols.values, xticklabels=cols.values\n               )\n    plt.show()","91968d3d":"# Dataframe dimensions\nprint(df_train.shape)\nprint(\"*\"*50)\nprint(df_test.shape)","974ec788":"# Column datatypes\nprint(df_train.dtypes)\nprint(\"*\"*50)\nprint(df_test.dtypes)","ff013d67":"# Type of df.types\ntype(df_train.dtypes)","d4775fcb":"# Display first 5 rows of df_train\ndf_train.head()","ecf18d3d":"# Display first 5 rows of df_test\ndf_test.head()","d3c06df8":"# Filter and display only df.dtypes that are 'object'\ndf_train.dtypes[df_train.dtypes == 'object']","a2acea21":"# Loop through categorical feature names and print each one\nfor feature in df_train.dtypes[df_train.dtypes == 'object'].index:\n    print(feature)","99008600":"# Display the first 10 rows of data\ndf_train.head(10)","8e4f2157":"# Display last 5 rows of data\ndf_train.tail()","cc752115":"# Plot histogram grid\ndf_train.hist(figsize=(20,20), xrot=-45)\n\n# Clear the text \"residue\"\nplt.show()","796c9ec9":"# Summarize numerical features\ndf_train.describe()","c15ce53d":"df_test.describe()","d4ba57cb":"sns.distplot(df_train['SalePrice']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","84432644":"df_train['SalePrice_Log'] = np.log1p(df_train['SalePrice'])\n\nsns.distplot(df_train['SalePrice_Log']);\n# skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice_Log'].kurt())","d5aacbdf":"# Summarize categorical features\ndf_train.describe(include=['object'])","f1befe86":"numerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\nprint(\"Number of Numerical features: \", len(numerical_feats))\n\ncategorical_feats = df_train.dtypes[df_train.dtypes == \"object\"].index\nprint(\"Number of Categorical features: \", len(categorical_feats))","ef97865c":"print(df_train[numerical_feats].columns)\nprint(\"*\"*100)\nprint(df_train[categorical_feats].columns)","3c635e11":"# Plot bar plot for each categorical feature\n\nfor feature in df_train.dtypes[df_train.dtypes == 'object'].index:\n    sns.countplot(y=feature, data=df_train)\n    plt.show()","194b2750":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","a2f09cde":"# columns where NaN values have meaning e.g. no pool etc.\ncols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n               'GarageQual','GarageCond','GarageFinish','GarageType', 'Electrical',\n               'KitchenQual', 'SaleType', 'Functional', 'Exterior2nd', 'Exterior1st',\n               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2',\n               'MSZoning', 'Utilities']\n\n# replace 'NaN' with 'None' in these columns\nfor col in cols_fillna:\n    df_train[col].fillna('None',inplace=True)\n    df_test[col].fillna('None',inplace=True)","389c8233":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(5)","0c204e9e":"# fillna with mean or mode for the remaining values\ndf_train.fillna(df_train.mean(), inplace=True)\ndf_test.fillna(df_test.mean(), inplace=True)\ndf_train.fillna(df_train.mode(), inplace=True)\ndf_test.fillna(df_test.mode(), inplace=True)","a323ddd4":"df_train.isnull().sum().sum()","e738b813":"df_test.isnull().sum().sum()","a956e439":"for col in numerical_feats:\n    print(col)\n    print(\"Skewness: %f\" % df_train[col].skew())\n    print(\"Kurtosis: %f\" % df_train[col].kurt())\n    print(\"*\"*50)","9cfa51c4":"sns.distplot(df_train['GrLivArea']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['GrLivArea'].skew())\nprint(\"Kurtosis: %f\" % df_train['GrLivArea'].kurt())","e85e2d42":"sns.distplot(df_train['LotArea']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['LotArea'].skew())\nprint(\"Kurtosis: %f\" % df_train['LotArea'].kurt())","09f64782":"for df in [df_train, df_test]:\n    df['GrLivArea_Log'] = np.log(df['GrLivArea'])\n    df.drop('GrLivArea', inplace= True, axis = 1)\n    df['LotArea_Log'] = np.log(df['LotArea'])\n    df.drop('LotArea', inplace= True, axis = 1)\n    \n    \n    \nnumerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index","6d9f949d":"sns.distplot(df_train['GrLivArea_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['GrLivArea_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['GrLivArea_Log'].kurt())","d42aacaa":"sns.distplot(df_train['LotArea_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['LotArea_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['LotArea_Log'].kurt())","d31344a1":"nr_rows = 12\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nli_num_feats = list(numerical_feats)\nli_not_plot = ['Id', 'SalePrice', 'SalePrice_Log']\nli_plot_num_feats = [c for c in list(numerical_feats) if c not in li_not_plot]\n\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_plot_num_feats):\n            sns.regplot(df_train[li_plot_num_feats[i]], df_train[target], ax = axs[r][c])\n            stp = stats.pearsonr(df_train[li_plot_num_feats[i]], df_train[target])\n            #axs[r][c].text(0.4,0.9,\"title\",fontsize=7)\n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()","1df8d1e6":"corr = df_train.corr()\ncorr_abs = corr.abs()\n\nnr_num_cols = len(numerical_feats)\nser_corr = corr_abs.nlargest(nr_num_cols, target)[target]\n\ncols_abv_corr_limit = list(ser_corr[ser_corr.values > min_val_corr].index)\ncols_bel_corr_limit = list(ser_corr[ser_corr.values <= min_val_corr].index)","5b16c2a0":"print(ser_corr)\nprint(\"*\"*30)\nprint(\"List of numerical features with r above min_val_corr :\")\nprint(cols_abv_corr_limit)\nprint(\"*\"*30)\nprint(\"List of numerical features with r below min_val_corr :\")\nprint(cols_bel_corr_limit)","509199ad":"for catg in list(categorical_feats) :\n    print(df_train[catg].value_counts())\n    print('#'*50)","9a7f605c":"li_cat_feats = list(categorical_feats)\nnr_rows = 15\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_cat_feats):\n            sns.boxplot(x=li_cat_feats[i], y=target, data=df_train, ax = axs[r][c])\n    \nplt.tight_layout()    \nplt.show()   ","112f7519":"catg_strong_corr = [ 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', \n                     'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']\n\ncatg_weak_corr = ['Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n                  'LandSlope', 'Condition1',  'BldgType', 'HouseStyle', 'RoofStyle', \n                  'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', \n                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', \n                  'HeatingQC', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', \n                  'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', \n                  'SaleCondition' ]\n      ","1149d70f":"nr_feats = len(cols_abv_corr_limit)","47134f92":"plot_corr_matrix(df_train, nr_feats, target)","36f06b89":"id_test = df_test_id_test['Id']","3e1f240b":"\n\nto_drop_num  = cols_bel_corr_limit\nto_drop_catg = catg_weak_corr\n\n# cols_to_drop = ['Id'] + to_drop_num + to_drop_catg \n\ncols_to_drop = to_drop_num + to_drop_catg \n\nfor df in [df_train, df_test]:\n    df.drop(cols_to_drop, inplace= True, axis = 1)","c6028a1d":"catg_list = catg_strong_corr.copy()\ncatg_list.remove('Neighborhood')\n\nfor catg in catg_list :\n    #sns.catplot(x=catg, y=target, data=df_train, kind='boxen')\n    sns.violinplot(x=catg, y=target, data=df_train)\n    plt.show()\n    #sns.boxenplot(x=catg, y=target, data=df_train)\n    #bp = df_train.boxplot(column=[target], by=catg)","3354e3df":"fig, ax = plt.subplots()\nfig.set_size_inches(16, 5)\nsns.violinplot(x='Neighborhood', y=target, data=df_train, ax=ax)\nplt.xticks(rotation=45)\nplt.show()","e8983c4e":"for catg in catg_list :\n    g = df_train.groupby(catg)[target].mean()\n    print(g)","d5304a6f":"# 'MSZoning'\nmsz_catg2 = ['RM', 'RH']\nmsz_catg3 = ['RL', 'FV'] \n\n\n# Neighborhood\nnbhd_catg2 = ['Blmngtn', 'ClearCr', 'CollgCr', 'Crawfor', 'Gilbert', 'NWAmes', 'Somerst', 'Timber', 'Veenker']\nnbhd_catg3 = ['NoRidge', 'NridgHt', 'StoneBr']\n\n# Condition2\ncond2_catg2 = ['Norm', 'RRAe']\ncond2_catg3 = ['PosA', 'PosN'] \n\n# SaleType\nSlTy_catg1 = ['Oth']\nSlTy_catg3 = ['CWD']\nSlTy_catg4 = ['New', 'Con']\n\n#[]","4599970f":"for df in [df_train, df_test]:\n    \n    df['MSZ_num'] = 1  \n    df.loc[(df['MSZoning'].isin(msz_catg2) ), 'MSZ_num'] = 2    \n    df.loc[(df['MSZoning'].isin(msz_catg3) ), 'MSZ_num'] = 3        \n    \n    df['NbHd_num'] = 1       \n    df.loc[(df['Neighborhood'].isin(nbhd_catg2) ), 'NbHd_num'] = 2    \n    df.loc[(df['Neighborhood'].isin(nbhd_catg3) ), 'NbHd_num'] = 3    \n\n    df['Cond2_num'] = 1       \n    df.loc[(df['Condition2'].isin(cond2_catg2) ), 'Cond2_num'] = 2    \n    df.loc[(df['Condition2'].isin(cond2_catg3) ), 'Cond2_num'] = 3    \n    \n    df['Mas_num'] = 1       \n    df.loc[(df['MasVnrType'] == 'Stone' ), 'Mas_num'] = 2 \n    \n    df['ExtQ_num'] = 1       \n    df.loc[(df['ExterQual'] == 'TA' ), 'ExtQ_num'] = 2     \n    df.loc[(df['ExterQual'] == 'Gd' ), 'ExtQ_num'] = 3     \n    df.loc[(df['ExterQual'] == 'Ex' ), 'ExtQ_num'] = 4     \n   \n    df['BsQ_num'] = 1          \n    df.loc[(df['BsmtQual'] == 'Gd' ), 'BsQ_num'] = 2     \n    df.loc[(df['BsmtQual'] == 'Ex' ), 'BsQ_num'] = 3     \n \n    df['CA_num'] = 0          \n    df.loc[(df['CentralAir'] == 'Y' ), 'CA_num'] = 1    \n\n    df['Elc_num'] = 1       \n    df.loc[(df['Electrical'] == 'SBrkr' ), 'Elc_num'] = 2 \n\n\n    df['KiQ_num'] = 1       \n    df.loc[(df['KitchenQual'] == 'TA' ), 'KiQ_num'] = 2     \n    df.loc[(df['KitchenQual'] == 'Gd' ), 'KiQ_num'] = 3     \n    df.loc[(df['KitchenQual'] == 'Ex' ), 'KiQ_num'] = 4      \n    \n    df['SlTy_num'] = 2       \n    df.loc[(df['SaleType'].isin(SlTy_catg1) ), 'SlTy_num'] = 1  \n    df.loc[(df['SaleType'].isin(SlTy_catg3) ), 'SlTy_num'] = 3  \n    df.loc[(df['SaleType'].isin(SlTy_catg4) ), 'SlTy_num'] = 4  ","a8afda41":"new_col_num = ['MSZ_num', 'NbHd_num', 'Cond2_num', 'Mas_num', 'ExtQ_num', 'BsQ_num', 'CA_num', 'Elc_num', 'KiQ_num', 'SlTy_num']\n\nnr_rows = 4\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(new_col_num):\n            sns.regplot(df_train[new_col_num[i]], df_train[target], ax = axs[r][c])\n            stp = stats.pearsonr(df_train[new_col_num[i]], df_train[target])\n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()   ","629e6022":"catg_cols_to_drop = ['Neighborhood' , 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']\n\ncorr1 = df_train.corr()\ncorr_abs_1 = corr1.abs()\n\nnr_all_cols = len(df_train)\nser_corr_1 = corr_abs_1.nlargest(nr_all_cols, target)[target]\n\nprint(ser_corr_1)\ncols_bel_corr_limit_1 = list(ser_corr_1[ser_corr_1.values <= min_val_corr].index)\n\n\nfor df in [df_train, df_test] :\n    df.drop(catg_cols_to_drop, inplace= True, axis = 1)\n    df.drop(cols_bel_corr_limit_1, inplace= True, axis = 1)    ","df76d2d0":"corr2 = df_train.corr()\ncorr_abs_2 = corr2.abs()\n\nnr_all_cols = len(df_train)\nser_corr_2 = corr_abs_2.nlargest(nr_all_cols, target)[target]\n\nprint(ser_corr_2)","c28648b9":"df_train.head()","93a3529a":"df_test.head()","eed921ad":"corr = df_train.corr()\ncorr_abs = corr.abs()\n\nnr_all_cols = len(df_train)\nprint (corr_abs.nlargest(nr_all_cols, target)[target])","7df4bb2d":"nr_feats=len(df_train.columns)\nplot_corr_matrix(df_train, nr_feats, target)","23db241f":"cols = corr_abs.nlargest(nr_all_cols, target)[target].index\ncols = list(cols)\n\nif drop_similar == 1 :\n    for col in ['GarageArea','1stFlrSF','TotRmsAbvGrd','GarageYrBlt'] :\n        if col in cols: \n            cols.remove(col)","6f178001":"cols = list(cols)\nprint(cols)","24ae76a9":"feats = cols.copy()\nfeats.remove('SalePrice_Log')\nfeats.remove('SalePrice')\n\nprint(feats)","8d0536c6":"df_test.head()","c021b253":"df_train_ml = df_train[feats].copy()\ndf_test_ml  = df_test[feats].copy()\n\ny = df_train[target]\n\nprint(target)","75ec950c":"# Dataframe dimensions\nprint(df_train.shape)\nprint(\"*\"*50)\nprint(df_test.shape)\nprint(\"*\"*50)\nprint(df_train_ml.shape)\nprint(\"*\"*50)\nprint(df_test_ml.shape)","c9f9c9e2":"numerical_feats = df_train_ml.dtypes[df_train_ml.dtypes != \"object\"].index\nprint(\"Number of Numerical features: \", len(numerical_feats))\n\ncategorical_feats = df_train_ml.dtypes[df_train_ml.dtypes == \"object\"].index\nprint(\"Number of Categorical features: \", len(categorical_feats))","b4f5609b":"df_all = pd.concat([df_train_ml, df_test_ml])\n\nlen_train = df_train_ml.shape[0]\n\nxtrain = df_all[:len_train]\nxtest = df_all[len_train:]","b1ac3032":"# Dataframe dimensions\nprint(xtrain.shape)\nprint(\"*\"*50)\nprint(xtest.shape)","296c699c":"xtrain.head()\n","9009c111":"xtest.head()","198aae8f":"# StandardScaler from Scikit-Learn\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize instance of StandardScaler\nscaler = StandardScaler()\n\n# Fit and transform item_data\ndata_scaled = scaler.fit_transform(df_all)\n\n# Display first 5 rows of item_data_scaled\ndata_scaled[:5]","4ab48b23":"# Plot scatterplot of scaled x1 against scaled x2\nplt.scatter(data_scaled[:,0], data_scaled[:,1])\n\n# Put plot axes on the same scale\nplt.axis('equal')\n\n# Label axes\nplt.xlabel('x1 (scaled)')\nplt.ylabel('x2 (scaled)')\n\n# Clear text residue\nplt.show()","4d1c6737":"total = df_all.isnull().sum().sort_values(ascending=False)\npercent = (df_all.isnull().sum()\/df_all.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","b03f0cdc":"# Initialize instance of PCA transformation\nfrom sklearn.decomposition import PCA\npca = PCA()\n\n# Fit the instance\npca.fit(data_scaled)","18d4a58d":"# Display principal components\npca.components_","901bc1c6":"# Plot scaled dataset and make it partially transparent\nplt.scatter(data_scaled[:,0], data_scaled[:,1], alpha=0.3)\n\n# Plot first principal component in black\nplt.plot([0, 2*pca.components_[0,0]], [0, 2*pca.components_[0,1]], 'k')\n\n# Plot second principal component in red\nplt.plot([0, pca.components_[1,0]], [0, pca.components_[1,1]], 'r')\n\n# Set axes\nplt.axis('equal')\nplt.xlabel('x1 (scaled)')\nplt.ylabel('x2 (scaled)')\n\n# Clear text residue\nplt.show()","38bdb467":"# Generate new features\nPC = pca.transform(data_scaled)\n\n# Display first 5 rows\nPC[:5]","ce6dcbf0":"# Cumulative explained variance\ncumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n\n# Plot cumulative explained variance\nplt.plot(range(len(cumulative_explained_variance)), cumulative_explained_variance)","34a1cab6":"# How much variance we'd capture with the first 8 components\ncumulative_explained_variance[8]","039c1e37":"# Initialize PCA transformation, only keeping 8 components\npca = PCA(n_components=8)\n\n# Fit and transform item_data_scaled\nPC_items = pca.fit_transform(data_scaled)\n\n# Display shape of PC_items\nPC_items.shape","db44573a":"# Put PC_items into a dataframe\nitems_pca = pd.DataFrame(PC_items)\n\n# Name the columns\nitems_pca.columns = ['PC{}'.format(i + 1) for i in range(PC_items.shape[1])]\n\n# Update its index\nitems_pca.index = df_all.index\n\n# Display first 5 rows\nitems_pca.head()\n\n","2e8f8801":"len_train = df_train_ml.shape[0]\n\npca_xtrain = items_pca[:len_train]\npca_xtest = items_pca[len_train:]","98dd083f":"# Dataframe dimensions\nprint(pca_xtrain.shape)\nprint(\"*\"*50)\nprint(pca_xtest.shape)","9424e73e":"# Plot transformed dataset\nplt.scatter(PC[:,0], PC[:,1], alpha=0.3, color='g')\n\n# Plot first principal component in black\nplt.plot([0, 2], [0, 0], 'k')\n\n# Plot second principal component in red\nplt.plot([0, 0], [0, 1], 'r')\n\n# Set axes\nplt.axis('equal')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\n\n# Clear text residue\nplt.show()","2874a817":"# Display explained variance ratio\npca.explained_variance_ratio_","339c0354":"# NumPy for numerical computing\nimport numpy as np\n\n# Pandas for DataFrames\nimport pandas as pd\npd.set_option('display.max_columns', 100)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n# Matplotlib for visualization\nfrom matplotlib import pyplot as plt\n# display plots in the notebook\n%matplotlib inline \n\n# Seaborn for easier visualization\nimport seaborn as sns\n\n# Scikit-Learn for Modeling\nimport sklearn\n\n# Import Elastic Net, Ridge Regression, and Lasso Regression\nfrom sklearn.linear_model import ElasticNet, Ridge, Lasso\n\n# Import Random Forest and Gradient Boosted Trees\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor","063df14b":"X = pca_xtrain.copy()\ny = df_train[target]\nX_test = pca_xtest.copy()\n#y_test = df_train[target]\n\nX.info()\nX_test.info()","0a21266e":"# Split X and y into train and test sets\nX_train = pca_xtrain.copy()\nX_test = pca_xtest.copy()\ny_train = df_train[target]\n#y_test = df_test[target]","5df9c7ef":"X.head()","a187f67c":"X_test.head()","a0ad0505":"print( len(X_train), len(X_test), len(y_train) )","b1a301d3":"# Summary statistics of X_train\nX_train.describe()","fc4aec33":"# Standardize X_train\nX_train_new = (X_train - X_train.mean()) \/ X_train.std()","36fbd1bd":"# Summary statistics of X_train_new\nX_train_new.describe()","549a2d8d":"# Function for creating model pipelines\nfrom sklearn.pipeline import make_pipeline","499057a4":"# For standardization\nfrom sklearn.preprocessing import StandardScaler","33a303e0":"make_pipeline(StandardScaler(), Lasso(random_state=123))","7083ee55":"# Create pipelines dictionary\npipelines = {\n    'lasso' : make_pipeline(StandardScaler(), Lasso(random_state=123)),\n    'ridge' : make_pipeline(StandardScaler(), Ridge(random_state=123)),\n    'enet'  : make_pipeline(StandardScaler(), ElasticNet(random_state=123))\n}","e60a8018":"# Add a pipeline for 'rf'\npipelines['rf'] = make_pipeline(StandardScaler(), RandomForestRegressor(random_state=123))\n\n# Add a pipeline for 'gb'\npipelines['gb'] = make_pipeline(StandardScaler(), GradientBoostingRegressor(random_state=123))","0f927bb7":"# Check that we have all 5 algorithms, and that they are all pipelines\nfor key, value in pipelines.items():\n    print( key, type(value) )","c443f44f":"# List tuneable hyperparameters of our Lasso pipeline\npipelines['lasso'].get_params()","c7c82687":"# Lasso hyperparameters\nlasso_hyperparameters = { \n    'lasso__alpha' : [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10] \n}\n\n# Ridge hyperparameters\nridge_hyperparameters = { \n    'ridge__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]  \n}","ad94659c":"# Elastic Net hyperparameters\nenet_hyperparameters = { \n    'elasticnet__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],                        \n    'elasticnet__l1_ratio' : [0.1, 0.3, 0.5, 0.7, 0.9]  \n}","dd6d3fdb":"# Random forest hyperparameters\nrf_hyperparameters = { \n    'randomforestregressor__n_estimators' : [100, 200],\n    'randomforestregressor__max_features': ['auto', 'sqrt', 0.33],\n}","d0e26600":"# Boosted tree hyperparameters\ngb_hyperparameters = { \n    'gradientboostingregressor__n_estimators': [100, 200],\n    'gradientboostingregressor__learning_rate' : [0.05, 0.1, 0.2],\n    'gradientboostingregressor__max_depth': [1, 3, 5]\n}","3752db0b":"# Create hyperparameters dictionary\nhyperparameters = {\n    'rf' : rf_hyperparameters,\n    'gb' : gb_hyperparameters,\n    'lasso' : lasso_hyperparameters,\n    'ridge' : ridge_hyperparameters,\n    'enet' : enet_hyperparameters\n}","fdba164f":"for key in ['enet', 'gb', 'ridge', 'rf', 'lasso']:\n    if key in hyperparameters:\n        if type(hyperparameters[key]) is dict:\n            print( key, 'was found in hyperparameters, and it is a grid.' )\n        else:\n            print( key, 'was found in hyperparameters, but it is not a grid.' )\n    else:\n        print( key, 'was not found in hyperparameters')","2217d3a7":"# Helper for cross-validation\nfrom sklearn.model_selection import GridSearchCV","2468d20f":"# Create cross-validation object from Lasso pipeline and Lasso hyperparameters\nmodel = GridSearchCV(pipelines['lasso'], hyperparameters['lasso'], cv=10, n_jobs=-1)","80fd2fc2":"type(model)","c21a8cd5":"# Fit and tune model\nmodel.fit(X_train, y_train)","34e487ec":"# Create empty dictionary called fitted_models\nfitted_models = {}\n\n# Loop through model pipelines, tuning each one and saving it to fitted_models\nfor name, pipeline in pipelines.items():\n    # Create cross-validation object from pipeline and hyperparameters\n    model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1)\n    \n    # Fit model on X_train, y_train\n    model.fit(X_train, y_train)\n    \n    # Store model in fitted_models[name] \n    fitted_models[name] = model\n    \n    # Print '{name} has been fitted'\n    print(name, 'has been fitted.')","7954bbf8":"# Check that we have 5 cross-validation objects\nfor key, value in fitted_models.items():\n    print( key, type(value) )","e63d3d34":"from sklearn.exceptions import NotFittedError\n\nfor name, model in fitted_models.items():\n    try:\n        pred = model.predict(X_test)\n        print(name, 'has been fitted.')\n    except NotFittedError as e:\n        print(repr(e))","9ec8d319":"# Display best_score_ for each fitted model\nfor name, model in fitted_models.items():\n    print( name, model.best_score_ )","d951277d":"# Import r2_score and mean_absolute_error functions\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error","2d4c4deb":"# Display fitted random forest object\nfitted_models['rf']","fe0d12b4":"# Predict test set using fitted random forest\npred = fitted_models['rf'].predict(X_test)","bd3902b9":"# Calculate and print R^2 and MAE, for training data set\npred_train = fitted_models['rf'].predict(X_train)\nprint( 'R^2:', r2_score(y_train, pred_train ))\nprint( 'MAE:', mean_absolute_error(y_train, pred_train))","3483f489":"print(pred)","e35b1d8c":"df_test.head()","60106ecb":"# Create final table\n\npred_pd = pd.DataFrame()\npred_pd['Id'] = id_test\npred_pd['SalePrice'] = np.exp(pred)\n\npred_pd.head\npred_pd.to_csv('submission_mrig_PCA.csv',index=False)","773e51c9":"Checking correlation to SalePrice for the new numerical columns","fa8b063b":"Columns of Numerical and Categorical features","510d0da9":"There are few columns with quite large correlation to SalePrice (NbHd_num, ExtQ_num, BsQ_num, KiQ_num).\n<br> These will probably be useful for optimal performance of the Regressors in part 3.\n\n<br> Dropping the converted categorical columns and the new numerical columns with weak correlation\n\n<br> columns and correlation before dropping\n\n","b6e8ca91":"Correlation matrix 1\n<br> Features with largest correlation to SalePrice_Log\n<br> all numerical features with correlation coefficient above threshold","b22ef16d":"Relation to SalePrice for all categorical features","a98683b1":"#### Correlation Matrix 2 : All features with strong correlation to SalePrice","b455439c":"Filling missing values\nFor a few columns there is lots of NaN entries.\nHowever, reading the data description we find this is not missing data:\nFor PoolQC, NaN is not missing data but means no pool, likewise for Fence, FireplaceQu etc.","6367e1d5":"Outliers\n\nFind columns with strong correlation to target\nOnly those with r > min_val_corr are used in the ML Regressors in Part 3\nThe value for min_val_corr can be chosen in global settings","1582ff7b":"## Distributions of numeric features","cebceeef":"### shape, info, head and describe","c9fbf556":"List of numerical features and their correlation coefficient to target","f3f1dab9":"Creating Datasets for ML algorithms","a9c51f41":"##### import the libraries we'll need","b9c3fba3":"Display summary statistics for categorical features.","c7ef352f":"Check for Multicollinearity\n\n<br> Strong correlation of these features to other, similar features:\n\n<br> 'GrLivArea_Log' and 'TotRmsAbvGrd'\n\n<br> 'GarageCars' and 'GarageArea'\n\n<br> 'TotalBsmtSF' and '1stFlrSF'\n\n<br> 'YearBuilt' and 'GarageYrBlt'\n\n<br> Of those features we drop the one that has smaller correlation coeffiecient to Target.","9c750a06":"Conclusion from EDA on numerical columns:\n\nWe see that for some features like 'OverallQual' there is a strong linear correlation (0.79) to the target.\nFor other features like 'MSSubClass' the correlation is very weak.\nFor this kernel I decided to use only those features for prediction that have a correlation larger than a threshold value to SalePrice.\nThis threshold value can be choosen in the global settings : min_val_corr\n\nWith the default threshold for min_val_corr = 0.4, these features are dropped in Part 2, Data Wrangling:\n'Id', 'MSSubClass', 'LotArea', 'OverallCond', 'BsmtFinSF2', 'BsmtUnfSF', 'LowQualFinSF', 'BsmtFullBath', 'BsmtHalfBath', 'HalfBath',\n'BedroomAbvGr', 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'\n\nWe also see that the entries for some of the numerical columns are in fact categorical values.\nFor example, the numbers for 'OverallQual' and 'MSSubClass' represent a certain group for that feature ( see data description txt)","b19657d1":"the target variable SalePrice is not normally distributed.\nThis can reduce the performance of the ML models because they assume normal distribution, see sklearn info on preprocessing\n\nTherfore we make a log transformation, the resulting distribution looks much better.","54becdea":"log transform\nLike the target variable, also some of the feature values are not normally distributed and it is therefore better to use log values in df_train and df_test. Checking for skewness and kurtosis:","ab207e4f":"### import the libraries we'll need","87de4270":"Of those features with the largest correlation to SalePrice, some also are correlated strongly to each other.\n\n<br> To avoid failures of the ML regression models due to multicollinearity, these are dropped in part 2.\n\n<br> This is optional and controlled by the switch drop_similar (global settings)","20422fb2":"### Plots of relation to target for all numerical features","6a04fee4":"Convert categorical columns to numerical\n<br> For those categorcial features where the EDA with boxplots seem to show a strong dependence of the SalePrice on the \n<br> category, we transform the columns to numerical. To investigate the relation of the categories to SalePrice in more detail, we \n<br> make violinplots for these features Also, we look at the mean of SalePrice as function of category.","25bc50c4":"# Exploratory Analysis","5c6673ca":"## Part 2: Data wrangling\n<br> Drop all columns with only small correlation to SalePrice\n<br> Transform Categorical to numerical \n<br> Handling columns with missing data\n<br> Log values\n<br> Drop all columns with strong correlation to similar features\n\n<br> Numerical columns : drop similar and low correlation\n\n<br> Categorical columns : Transform to numerical\n\n<br> Dropping all columns with weak correlation to SalePrice","e66fc6cc":"List of features with missing values","058b3fa1":"List of categorical features and their unique values","a0872b65":"### List of all features with strong correlation to SalePrice_Log\n<br> after dropping all coumns with weak correlation","a5c55d59":"# Relation of features to target (SalePrice_log)","615ba189":"columns and correlation after dropping","5150bc63":"Conclusion from EDA on categorical columns:\n\nFor many of the categorical there is no strong relation to the target.\nHowever, for some fetaures it is easy to find a strong relation.\nFrom the figures above these are : 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType' Also for the categorical features, I use only those that show a strong relation to SalePrice. So the other columns are dropped when creating the ML dataframes in Part 2 :\n'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleCondition'","42b22b2a":"Some useful functions","47d7b749":"### The target variable : Distribution of SalePrice","f1bac1b5":"new dataframes","635f9ab8":"Number of Numerical and Categorical features","ae6b6b88":"#### List of features used for the Regressors in Part 3","1b36480f":"## Basic information"}}