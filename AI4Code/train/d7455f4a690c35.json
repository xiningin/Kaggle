{"cell_type":{"272a2ba2":"code","2b8c3aec":"code","09d519ba":"code","ef656c8d":"code","c695c5f9":"code","2fcbcedd":"code","a46016fa":"code","b07ffa7c":"code","a38aca86":"code","92d16b8e":"code","1e91c9a1":"code","921ee9be":"code","3eb16e4f":"code","1265a00b":"code","98194f89":"code","7666142d":"code","cff43625":"code","872f4dce":"code","450f275e":"code","52032e33":"code","c62f9261":"code","50fde50f":"code","2e404b9e":"code","c9913cee":"code","95ffbb7d":"code","b77c0982":"code","d231b23f":"code","fd9e186e":"code","3b0a9d5a":"code","7021aba5":"code","b91d218a":"code","84124a91":"code","02a67d8b":"code","0613c840":"code","d1e429e4":"code","2f3804f9":"code","a81e9674":"code","de32bcb2":"code","05c3a13c":"code","fd4dfe6b":"code","dae616bb":"code","a10bd16d":"code","5dd67198":"code","1502bbd3":"code","5803d603":"code","4daf36af":"code","f68fc428":"code","8c7a88b6":"markdown","77bb530d":"markdown","46c6312e":"markdown","fe2dc4fe":"markdown","d1885856":"markdown","8c38c864":"markdown","3cc1c9a0":"markdown","8c0b3bea":"markdown","21a2e81a":"markdown","da041fa3":"markdown","c25bb324":"markdown","03b6cdbb":"markdown","4427d290":"markdown","c66ee063":"markdown","265adef7":"markdown","d537e590":"markdown","07df8ab2":"markdown","d52081e8":"markdown","072467ec":"markdown","fc62bcaf":"markdown","dd5efd71":"markdown","e190abf0":"markdown","56f69192":"markdown","a1432891":"markdown","93c972bc":"markdown","b1416b3d":"markdown","ff6f7540":"markdown","0a476f49":"markdown","9d42d505":"markdown","2c1ba1ae":"markdown","6be150c7":"markdown"},"source":{"272a2ba2":"import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns ","2b8c3aec":"### BEGIN SOLUTION\nfilepath = '..\/input\/wine-quality\/Wine_Quality_Data.csv'\ndata = pd.read_csv(filepath, sep=',')","09d519ba":"data.shape","ef656c8d":"data.head()","c695c5f9":"data.describe()","2fcbcedd":"data.dtypes","a46016fa":"data.color.value_counts()","b07ffa7c":"data.color.value_counts(normalize=True)","a38aca86":"data.isnull().sum().sum()","92d16b8e":"data['color'] = data['color'].replace('white',0).replace('red',1).astype(np.int)","1e91c9a1":"data.iloc[:,-1]","921ee9be":"data.iloc[:,-1].value_counts()","3eb16e4f":"feature_cols = [x for x in data.columns if x not in 'color']\nfeature_cols","1265a00b":"from sklearn.model_selection import StratifiedShuffleSplit\n\n# Split the data into two parts with 1000 points in the test data\n# This creates a generator\nstrat_shuff_split = StratifiedShuffleSplit(n_splits=1, test_size=1000, random_state=42)\n\n# Get the index values from the generator\ntrain_idx, test_idx = next(strat_shuff_split.split(data[feature_cols], data['color']))\n\n# Create the data sets\nX_train = data.loc[train_idx, feature_cols]\ny_train = data.loc[train_idx, 'color']\n\nX_test = data.loc[test_idx, feature_cols]\ny_test = data.loc[test_idx, 'color']","98194f89":"y_test.value_counts(normalize=True).sort_index()","7666142d":"y_train.value_counts(normalize=True).sort_index()","cff43625":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state=42)\ndt = dt.fit(X_train, y_train)","872f4dce":"dt.tree_.node_count, dt.tree_.max_depth","450f275e":"dt.classes_","52032e33":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef measure_error(y_true, y_pred, label):\n    return pd.Series({'accuracy':accuracy_score(y_true, y_pred),\n                      'precision': precision_score(y_true, y_pred),\n                      'recall': recall_score(y_true, y_pred),\n                      'f1': f1_score(y_true, y_pred)},\n                      name=label)","c62f9261":"y_train_pred = dt.predict(X_train)\ny_test_pred = dt.predict(X_test)","50fde50f":"a=measure_error(y_train, y_train_pred, 'train')\nb=measure_error(y_test, y_test_pred, 'test')","2e404b9e":"c=pd.concat([a,b],axis=1)\nc","c9913cee":"# The error on the training and test data sets\ny_train_pred = dt.predict(X_train)\ny_test_pred = dt.predict(X_test)\n\ntrain_test_full_error = pd.concat([measure_error(y_train, y_train_pred, 'train'),\n                              measure_error(y_test, y_test_pred, 'test')],\n                              axis=1)\n\ntrain_test_full_error\n### END SOLUTION","95ffbb7d":"### BEGIN SOLUTION\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'max_depth':range(1, dt.tree_.max_depth+1, 2),\n              'max_features': range(1, dt.n_features_+1)}\n\nGR = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  n_jobs=-1)\n\nGR = GR.fit(X_train, y_train)","b77c0982":"GR.best_score_","d231b23f":"GR.best_estimator_","fd9e186e":"GR.best_estimator_.tree_.node_count, GR.best_estimator_.tree_.max_depth","3b0a9d5a":"GR.classes_","7021aba5":"y_train_pred_gr = GR.predict(X_train)\ny_test_pred_gr = GR.predict(X_test)\n\ntrain_test_gr_error = pd.concat([measure_error(y_train, y_train_pred_gr, 'train'),\n                                 measure_error(y_test, y_test_pred_gr, 'test')],\n                                axis=1)","b91d218a":"train_test_gr_error","84124a91":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","02a67d8b":"cm=confusion_matrix(y_train,y_train_pred_gr, labels=GR.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=GR.classes_)\ndisp.plot(cmap='Blues')\nplt.title('Confusion matrix for training dataset')","0613c840":"cm=confusion_matrix(y_test,y_test_pred_gr, labels=GR.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=GR.classes_)\ndisp.plot(cmap='Blues')\nplt.title('Confusion matrix for testing dataset')","d1e429e4":"### BEGIN SOLUTION\nfeature_cols = [x for x in data.columns if x != 'residual_sugar']\n\n# Create the data sets\nX_train = data.loc[train_idx, feature_cols]\ny_train = data.loc[train_idx, 'residual_sugar']\n\nX_test = data.loc[test_idx, feature_cols]\ny_test = data.loc[test_idx, 'residual_sugar']","2f3804f9":"data[feature_cols].dtypes","a81e9674":"from sklearn.tree import DecisionTreeRegressor\n\ndr = DecisionTreeRegressor().fit(X_train, y_train)\n\nparam_grid = {'max_depth':range(1, dr.tree_.max_depth+1, 2),\n              'max_features': range(1, dr.n_features_+1)}\n\nGR_sugar = GridSearchCV(DecisionTreeRegressor(random_state=42),\n                     param_grid=param_grid,\n                     scoring='neg_mean_squared_error',\n                      n_jobs=-1)\n\nGR_sugar = GR_sugar.fit(X_train, y_train)","de32bcb2":"GR_sugar.best_estimator_","05c3a13c":"GR_sugar.best_estimator_.tree_.node_count, GR_sugar.best_estimator_.tree_.max_depth","fd4dfe6b":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\ny_train_pred_gr_sugar = GR_sugar.predict(X_train)\ny_test_pred_gr_sugar  = GR_sugar.predict(X_test)\n\ntrain_test_gr_sugar_error = pd.Series({'train': mean_squared_error(y_train, y_train_pred_gr_sugar),\n                                         'test':  mean_squared_error(y_test, y_test_pred_gr_sugar)},\n                                          name='MSE').to_frame().T\n\ntrain_test_gr_sugar_r2 = pd.Series({'train': r2_score(y_train, y_train_pred_gr_sugar),\n                                         'test':  r2_score(y_test, y_test_pred_gr_sugar)},\n                                          name='R2 score').to_frame().T\n\npd.concat([train_test_gr_sugar_error, train_test_gr_sugar_r2])","dae616bb":"ph_test_predict = pd.DataFrame({'test':y_test.values,\n                                'predict': y_test_pred_gr_sugar}).set_index('test').sort_index()\nph_test_predict","a10bd16d":"sns.set_context('notebook')\nsns.set_style('white')\nfig = plt.figure(figsize=(6,6))\nax = plt.axes()\n\nph_test_predict = pd.DataFrame({'test':y_test.values,\n                                'predict': y_test_pred_gr_sugar}).set_index('test').sort_index()\n\nph_test_predict.plot(marker='o', ls='', ax=ax)\nax.set(xlabel='Test', ylabel='Predict', xlim=(0,35), ylim=(0,35));","5dd67198":"fig = plt.figure(figsize=(6,6))\nax = plt.axes()\nax.scatter(y_test,y_test_pred_gr_sugar)\n\nax.set(xlabel='Test', ylabel='Predict', xlim=(0,35), ylim=(0,35));","1502bbd3":"!conda install -c conda-forge pydotplus -y","5803d603":"from io import StringIO\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz\nimport pydotplus","4daf36af":"### BEGIN SOLUTION\n# Create an output destination for the file\ndot_data = StringIO()\n\nexport_graphviz(dt, out_file=dot_data, filled=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n\n# View the tree image\nfilename = 'wine_tree.png'\ngraph.write_png(filename)\nImage(filename=filename) ","f68fc428":"# Create an output destination for the file\ndot_data = StringIO()\n\nexport_graphviz(GR.best_estimator_, out_file=dot_data, filled=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n\n# View the tree image\nfilename = 'wine_tree_prune.png'\ngraph.write_png(filename)\nImage(filename=filename) \n### END SOLUTION","8c7a88b6":"### Let's define a function which can compute error metrics of our model:","77bb530d":"### Let's measure the errors on the train and test sets as before and compare them to those from the previous tree:","46c6312e":"**The decision tree predicts a little better on the training data than the test data, which is consistent with (mild) overfitting. Also notice the perfect recall score for the training data. In many instances, this prediction difference is even greater than that seen here.**","fe2dc4fe":"# Decision Tree Classifier:\nLet's define our model without setting limits on maximum depth, features, or leaves.","d1885856":"## Splitting our dataset:","8c38c864":"### Let's convert our color label to an integer, this is a quick way to do it using Pandas.","3cc1c9a0":"Determine how many nodes are present and the depth of this tree:","8c0b3bea":"# Decision Tree Regressor:","21a2e81a":"#### Hi, welcome to my project! Today we will be classifying wine color based on its features using Decision Tree algorithms and then perform a regression in order to predict a continuous value. \n#### We will be using the wine quality data set for these exercises. This data set contains various chemical properties of wine, such as acidity, sugar, pH, and alcohol. It also contains a quality metric (3-9, with highest being better) and a color (red or white). The name of the file is Wine_Quality_Data.csv","da041fa3":"### Let's import all libraries we will need at the beginning of the analysis","c25bb324":"**White=0, Red=1** ","03b6cdbb":"### In order to display the decision trees we built, we require an additional command line program (GraphViz) and Python library (PyDotPlus). GraphViz can be installed with a package manager on Linux and Mac. For PyDotPlus, either pip or conda (conda install -c conda-forge pydotplus) can be used to install the library.\n\n# Displaying decision trees: \n\n### First decision tree, where wine color was predicted and the number of features and\/or splits are not limited.\n### Last decision tree, where wine color was predicted but a grid search was used to find the optimal depth and number of features.","4427d290":"### More meaningful:","c66ee063":"Below we can see that all of the feature columns are numerical type which is suit to use for regression algorithms.","265adef7":"Import the dataset and examine the features, then look for null values and delete rows which contain them:","d537e590":"The number of nodes and the maximum depth of the tree:","07df8ab2":"## Plotting of actual vs predicted residual sugar:","d52081e8":"### Now let's use our function with its corresponding arguments:","072467ec":"### Now check the percent composition of each quality level in the train and test data sets. The data set is mostly white wine, as can be seen below.","fc62bcaf":"Let's see the best parameters found by the GridSeachCV:","dd5efd71":"### Let's compute error metrics on train and test data sets. Take into account that this case of study is continuous, so we will use mean squared error and coefficient of determination (r2 score):","e190abf0":"In this part of the project we will develop a DTR model which can help us predict a continuous label, in this case we will deal with residual sugar being our label.","56f69192":"Or we could just use the scatter plot from matplotlib:","a1432891":"We could create a new dataframe with actual test label and predicted as columnn, then set the first one as index and use the plot tool:","93c972bc":"We can see our dataset does not contain any null values in its fields, so we can continue with replacing or encoding our label column.","b1416b3d":"## Confusion matrix for both training and testing datasets: ","ff6f7540":"These test metrics are a little better than the previous ones. So it would seem like the previous example overfit the data, but only slightly.","0a476f49":"### Now we have to split our data intro train and test sets, in this project we will use StratifiedShuffleSplit. If possible, preserve the indices of the split for later.","9d42d505":"The number of nodes and the maximum depth of the tree. This tree has lots of nodes, which is not so surprising given the continuous data.","2c1ba1ae":"Predicting y with our model for x_train and x_test:","6be150c7":"### Let's use grid search with cross validation to find the best parameters of our decision tree. "}}