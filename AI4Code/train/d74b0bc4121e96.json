{"cell_type":{"461e41be":"code","d1dd0e38":"code","e3a2de3b":"code","2a5e102a":"code","7a646858":"code","c959c337":"code","9d3a331b":"code","61f0d646":"code","5ec5bd76":"code","28e43076":"code","2505693f":"code","1ceee9d3":"code","375c05eb":"code","552443f6":"code","727534be":"code","bf14f220":"code","c6b92d75":"code","d7c1ebbf":"markdown"},"source":{"461e41be":"!pip install pyspark","d1dd0e38":"from pyspark.sql import SparkSession\n\n# Create the Session\nspark = SparkSession.builder\\\n    .master(\"local\")\\\n    .appName(\"PySpark Tutorial\")\\\n    .getOrCreate()","e3a2de3b":"sc = spark.sparkContext\nrdd = sc.textFile('..\/input\/stockmarketdatafrom1996to2020\/Data\/Data\/FB\/FB.csv')\nrdd.take(2)","2a5e102a":"from pathlib import Path\ncontents = list(Path('..\/input\/stockmarketdatafrom1996to2020\/Data\/Data').iterdir())\n#print(contents)","7a646858":"stock_1 = spark.read.csv('..\/input\/stockmarketdatafrom1996to2020\/Data\/Data\/AAPL\/AAPL.csv', inferSchema=True, header=True)\nstock_1.show(5)","c959c337":"stock_2 = spark.read.csv('..\/input\/stockmarketdatafrom1996to2020\/Data\/Data\/MSFT\/MSFT.csv', inferSchema=True, header=True)\nstock_2.show(5)","9d3a331b":"stock_1.printSchema()","61f0d646":"stock_1.select(\"Close\").show(10)","5ec5bd76":"from pyspark.sql import functions as F\nstock_1.filter(F.col(\"Close\")>148.00).select(\"Date\",\"Close\").show(10)","28e43076":"rdd = rdd.map(lambda line: line.split(\",\"))\nrdd.top(5)","2505693f":"num = sc.parallelize([23, 1, 4, 5, 6, 7])\nnum_sum = num.reduce(lambda a,b:a+b)\nprint(num_sum)","1ceee9d3":"from pyspark.sql import Row\n\nheader = rdd.first()\nstock_3 = rdd.filter(lambda line: line != header)\\\n             .map(lambda line: Row(date=line[0],\n                                   open=line[1],\n                                   high=line[2],\n                                   low=line[3],\n                                   close=line[4],\n                                   adj_close=line[5],\n                                   volume=line[6])).toDF()\nstock_3.show(5)","375c05eb":"from pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.feature import VectorAssembler\n\ninput_1 = stock_1.select(\"Adj Close\")\ninput_1.show(5)\ninput_2 = stock_2.select(\"Adj Close\")\n#######################\n\ninput_1 = input_1.withColumnRenamed(\"Adj Close\",\"label\")\ninput_2 = input_2.withColumnRenamed(\"Adj Close\",\"feature\")\n\ninput_data = input_1.join(input_2)\ninput_data.show(5)","552443f6":"assembler = VectorAssembler(\n    inputCols=[\"feature\"],\n    outputCol=\"features\")\n\ninput_data = assembler.transform(input_data)\n\nstandardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\nscaler = standardScaler.fit(input_data.select(\"features\"))\n\ndf = scaler.transform(input_data)\ndf.show(5)","727534be":"from pyspark.ml.regression import LinearRegression\n\ntrain_data, test_data = df.randomSplit([.8,.2], seed=42)\n\nreg = LinearRegression(labelCol=\"label\", featuresCol=\"features_scaled\", maxIter=5)\nmodel = reg.fit(train_data)","bf14f220":"# Predict test_data\npredicted = model.transform(test_data)\n\n# Take predictions and the true label - zip them\npredictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\nlabels = predicted.select(\"label\").rdd.map(lambda x: x[0])\npred_lab = predictions.zip(labels).collect()\n\n# Print out first 5  predictions\npred_lab[:5]","c6b92d75":"# Model coefficients\nprint(model.coefficients)\n\n# Intercept\nprint(model.intercept)\n\n# RMSE\nprint(model.summary.rootMeanSquaredError)","d7c1ebbf":"# Apache Spark Guide\n\nThis is an Apache Spark Guide that will be featured on the [Algotrading101 blog](https:\/\/algotrading101.com\/learn\/).\nIf you want the explanation of what the code does I'd suggest you to visit the blog article that will be linked [here](https:\/\/algotrading101.com\/learn\/pyspark-guide\/) upon release."}}