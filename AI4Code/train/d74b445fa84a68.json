{"cell_type":{"3c8404be":"code","f6cf33d6":"code","ae6a1454":"code","28a088fc":"code","00e99389":"code","55447eaa":"code","00866c00":"code","af17d579":"markdown"},"source":{"3c8404be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f6cf33d6":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","ae6a1454":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","28a088fc":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","00e99389":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","55447eaa":"from sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBRegressor\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = XGBRegressor(n_estimators=140, learning_rate=0.005, random_state=0)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n#mae =mean_absolute_error()\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","00866c00":"\nimport pandas as pd\nimport keras\nfrom keras.models import load_model\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import cross_val_score, KFold\nimport random\nimport math\nimport numpy as np\nimport pandas as pd\nfrom collections import deque\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Reshape\nfrom keras.layers import Flatten\n# from tensorflow.keras.constraints import max_norm\nfrom keras.engine.input_layer import Input\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\ndef model_builder():\n\t\"\"\"\u041c\u043e\u0434\u0435\u043b\u044c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 \u0434\u043b\u044f \u0430\u0433\u0435\u043d\u0442\u0430\"\"\"\n\tinputA = Input(shape=(10))\n# \tline = Flatten()(inputA)\n# \tline = tf.keras.layers.GaussianNoise(0.0001)(inputA)\n\tline = Dense(128, activation='relu')(inputA)\n# \t\tline = LSTM(4, return_sequence = False, return_state=False)(line)\n# \t\tline = Dense(16, activation='relu')(inputA)\n# \tline = tf.keras.layers.Dropout(0.4)(line)\n\tline = Dense(64, activation='relu', )(line)\n# \tline = tf.keras.layers.Dropout(0.4)(line)\n# \tline = Dense(16, activation='relu', )(line)\n# \tline = Dense(16, activation='relu', )(line)\n# \tline = Dense(16, activation='relu', )(line)\n# \tline = Dense(16, activation='relu', )(line)\n\tline = Dense(32, activation='relu')(line)\n# \tline = tf.keras.layers.Dropout(0.4)(line)\n\tline = Dense(16, activation='relu')(line)\n# \t\tline = Reshape(self.action_space)(line)\n\toutputA = Dense(units=1, activation='sigmoid')(line)\n\t\n\tmodel = Model(inputs=inputA, outputs=outputA)\n\t#model = load_model('.\/input\/titanic-network\/best_model_titanic.h5')\n\tmodel.compile(loss = 'binary_crossentropy',\n\t\t\t   optimizer = Adam(lr=0.0001),\n\t\t\t    metrics=['accuracy'],\n\t\t\t\t)\n\treturn model\n# =============================================================================\n# Load data\n# =============================================================================\n# Read the data\nX_full = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId') \nX_test_full = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')\n\n# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['Survived'], inplace=True)\ny = X_full.Survived\nX_full.drop(['Survived'], axis=1, inplace=True)\nX_full[\"Feature1\"] = X.GrLivArea + X.TotalBsmtSF\nX_full[\"Feature2\"] = X.YearRemodAdd * X.TotalBsmtSF\n# =============================================================================\n# Splitting dataset for training and validating\n# =============================================================================\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(\n\tX_full, y, train_size=0.8, test_size=0.2, random_state=0)\n\n\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality\n# (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns \\\n\t\t\t\t\tif X_train_full[cname].nunique() < 10\n\t\t\t\t\tand X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns \\\n\t\t\t\t  if X_train_full[cname].dtype in ['int64', 'float64']]\n\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\nprint(X_train.head())\n# =============================================================================\n# Imputation\n# =============================================================================\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='median')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(\n\tsteps=[('imputer', SimpleImputer(strategy='most_frequent')),\n\t\t('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n\ttransformers=[\\\n\t\t\t   ('num', numerical_transformer, numerical_cols),\n\t\t\t   ('cat', categorical_transformer, categorical_cols)])\n\n# =============================================================================\n# Training\n# =============================================================================\n# model = RandomForestRegressor(n_estimators=200, random_state=0)\nscaler = MinMaxScaler(feature_range=(0, 1))\n\ncheckpoint_filepath = 'models\/best_model_titanic.h5'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n\tfilepath=checkpoint_filepath,\n\tsave_weights_only=False,\n\tmonitor='accuracy',\n\tmode='max',\n\tverbose=1,\n# \tsave_freq = 20,\n\tsave_best_only=True)\n\nepochs = 5\n\n# class CustomCallback(keras.callbacks.Callback):\n# \tdef on_epoch_end(self, epoch, logs=None):\n# \t\tprint(\"Epoch {}: {}\".format(epoch, loss(self.model.predict(x_valid), y_valid))\n\nmodel = KerasRegressor(\n\tbuild_fn=model_builder,verbose=1,epochs=epochs,\n\tcallbacks=[model_checkpoint_callback],\n\tbatch_size=3,)\n# \t\t\t\t\t   validation_data=(X_valid,y_valid),\n# \t\t\t\t\t   )\n\n# model.fit(trainX_data, trainY_data, epochs=10, verbose=0)\n# =============================================================================\n# Evaluating\n# =============================================================================\nmy_pipeline = Pipeline(\n\tsteps=[('preprocessor', preprocessor),\n\t\t('scale', scaler),\n# \t\t ('model', RandomForestRegressor(n_estimators, random_state=0))\n\t\t ('model', model)\n\t\t ])\nmy_pipeline.fit(X_train, y_train, model__validation_split=0.2)\n# from sklearn.model_selection import cross_val_score\n\n# kfold = KFold(n_splits=10, random_state=0)\n# # results = cross_val_score(estimator, X_train, Y_train, cv=kfold)\n# results = cross_val_score(my_pipeline, X = X_train, y = y_train, cv=kfold)\n# print(\"Network: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n# print(\"RMSE\", math.sqrt(results.std()))\n\npreds = my_pipeline.predict(X_valid)\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)\n\n\n# Output\n# =============================================================================\n\n# Save predictions in format used for competition scoring\npreds_test = my_pipeline.predict(X_test)\nfor i, item in enumerate(preds_test):\n\tpreds_test[i] = 1 if item >= 0.5 else 0\n\tpreds_test[i] = int(preds_test[i])\noutput = pd.DataFrame({'PassengerId': X_test.index,'Survived': preds_test})\noutput.to_csv('submission_titanic.csv', index=False)","af17d579":"cross-validation"}}