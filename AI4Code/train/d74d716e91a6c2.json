{"cell_type":{"a52024dc":"code","93596797":"code","61bfbddf":"code","206e8e7e":"code","db683b36":"code","f78116aa":"code","694b7cb9":"code","651f7557":"code","d796840c":"code","2bc81227":"code","376df84e":"code","441d4df8":"markdown","87c96559":"markdown","241659ec":"markdown","65142108":"markdown","0ed18542":"markdown","a1816eeb":"markdown","d151bba4":"markdown","818b65d7":"markdown"},"source":{"a52024dc":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np \nimport pandas as pd \nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom category_encoders.target_encoder import TargetEncoder\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\nfrom category_encoders.m_estimate import MEstimateEncoder\n\npd.set_option('max.rows',400)\npd.set_option('max.columns',80)\n\n'''Load the data'''\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\n'''Columns with more than 70% of missing values'''\ndrop_cols = ['Alley', 'PoolQC', 'Fence', 'MiscFeature']\ntrain.drop(drop_cols, axis = 1, inplace =True)\n\n'''Segregate the categoric columns'''\ncategoric_cols = train.select_dtypes('object').columns\ntrain_categoric = train[categoric_cols]\n\n'''Segregate ordinal and nominal columns'''\nnominal_cols = ['MSZoning', 'Street','LandContour','Neighborhood','Condition1','Condition2',\n                'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Foundation',\n                'Heating','GarageType','SaleType','SaleCondition']\nordinal_cols = ['ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n                'BsmtFinType2','HeatingQC','CentralAir','Electrical','KitchenQual','Functional',\n                'FireplaceQu','GarageFinish','GarageQual','GarageCond','PavedDrive','LotShape',\n                'Utilities','LandSlope','BldgType','HouseStyle','LotConfig']","93596797":"\"\"\"Impute the features before encoding it.\"\"\"\ndef mode_imputation(train_categoric):\n    \"\"\"\n    Mode imputation of categoric columns\n    \"\"\"\n    for col in train_categoric.columns:\n        mode = train_categoric[col].mode().iloc[0]\n        train_categoric[col] = train_categoric[col].fillna(mode)\n    return train_categoric\n\ntrain_categoric = mode_imputation(train_categoric)","61bfbddf":"'''Segregate the ordinal columns'''\ntrain_ordinal = train_categoric[ordinal_cols]\ndef ordinal_encode(train_ordinal):\n    \"\"\"\n    Ordinal encoding of the categorical features\n    \"\"\"\n    ord_enc = OrdinalEncoder(dtype=int)\n    enc_freq = ord_enc.fit_transform(train_ordinal)\n    train_ordinal_encoded = pd.DataFrame(enc_freq, columns= train_ordinal.columns, index =train_ordinal.index)\n    return train_ordinal_encoded\n\ntrain_ordinal_encoded = ordinal_encode(train_ordinal)\ntrain_ordinal_encoded.head()","206e8e7e":"def label_encode(train_ordinal):\n    \"\"\"\n    Label encoding of the categorical features\n    \"\"\"\n    '''Create a copy of train_ordinal'''\n    train_label_encoded = train_ordinal.copy()\n    lab_enc_dict = {}\n    for col in train_label_encoded:\n        lab_enc_dict[col] = LabelEncoder()\n        train_label_encoded[col] = lab_enc_dict[col].fit_transform(train_ordinal[col])\n    return train_label_encoded\n\ntrain_label_encoded = label_encode(train_ordinal)\ntrain_label_encoded.head()","db683b36":"'''Segregate the nominal columns'''\ntrain_nominal = pd.concat([train_categoric[nominal_cols], train['SalePrice']], axis = 1)\ndef frequency_encode(train_nominal):\n    train_freq_encoded = pd.DataFrame()\n    for col in train_nominal:\n        if(col != 'SalePrice'):\n            freq = train_nominal.groupby(col).size()\/ len(train_nominal)\n            train_freq_encoded[col] = train_nominal[col].map(freq)\n    return train_freq_encoded\n\ntrain_freq_encoded = frequency_encode(train_nominal)\ntrain_freq_encoded.head()","f78116aa":"def target_encode_udf(train_nominal):\n    \"\"\"\n    Target encoding (user defined function).\n    \"\"\"\n    train_target_encoded = train_nominal.copy()\n    for col in train_nominal:\n        if(col != 'SalePrice'):\n            mean = train_nominal.groupby(col)['SalePrice'].agg('mean')\n            train_target_encoded[col] = train_nominal[col].map(mean)\n    return train_target_encoded\n\ntrain_target_encoded = target_encode_udf(train_nominal)\ntrain_target_encoded.head()","694b7cb9":"def target_encode_builtin(train_nominal):\n    \"\"\"\n    Target encoding (using built-in class)\n    \"\"\"\n    target_enc = TargetEncoder()\n    x_train, y_train = train_nominal[train_nominal.columns[:-1]], train_nominal['SalePrice']\n    train_target_encoded = target_enc.fit_transform(x_train, y_train)\n    return train_target_encoded\n\ntrain_target_encoded = target_encode_builtin(train_nominal)\ntrain_target_encoded.head()","651f7557":"def onehot_encode(train_nominal):\n    train_onehot_encoded = pd.get_dummies(train_nominal[train_nominal.columns[:-1]])\n    return train_onehot_encoded\ntrain_onehot_encoded = onehot_encode(train_nominal)\ntrain_onehot_encoded.head()","d796840c":"def leave_oneout_encode(train_nominal):\n    lo_enc = LeaveOneOutEncoder()\n    x_train, y_train = train_nominal[train_nominal.columns[:-1]], train_nominal['SalePrice']\n    train_lo_encoded = lo_enc.fit_transform(x_train, y_train)\n    return train_lo_encoded\n\ntrain_lo_encoded = leave_oneout_encode(train_nominal)\ntrain_lo_encoded.head()","2bc81227":"def m_estimate_udf(train_nominal,col, M=100):    \n    \"\"\"\n    M-Estimate encoding (User defined function)\n    \"\"\"\n    agg_result = train_nominal.groupby(col).agg(['count','mean'])['SalePrice']\n    category_counts = agg_result['count']\n    category_means = agg_result['mean']\n    target_mean = train_nominal['SalePrice'].mean()\n    m_est = ((category_counts * category_means) + (M * target_mean))\/(category_counts + M)\n    return train_nominal[col].map(m_est)\n\ntrain_mest_encoded = train_nominal.copy()\nfor col in train_nominal:\n    if(col != 'SalePrice'):\n        train_mest_encoded[col] = m_estimate_udf(train_nominal,col, M=100)\n\ntrain_mest_encoded.head()","376df84e":"def m_estimate_encode_builtin(train_nominal, M=100):\n    \"\"\"M-Estimate encoding (using built-in function)\"\"\"\n    me = MEstimateEncoder(m=M)\n    x_train, y_train = train_nominal[train_nominal.columns[:-1]], train_nominal['SalePrice']\n    train_mest_encoded = me.fit_transform(x_train,y_train)\n    return train_mest_encoded\n\ntrain_mest_encoded = m_estimate_encode_builtin(train_nominal, 100)\ntrain_mest_encoded.head()","441d4df8":"# Categorical Feature Encoding\nCategorical feature encoding is an important step in data preprocessing. Categorical feature encoding is the process of converting the categorical features into numeric features. Categorical variables are also called as **Qualitative** variables. The results produced by the model varies when different encoding techniques are used.\n\nTwo types of categorical features exist,\n* **Ordinal Features**\n* **Nominal Features**\n\n## Ordinal Features:\n* Ordinal features are the features that have inherent ordering. \n    * Eg: Ratings such as Good, Bad. \n\n## Nominal Features:\n* Nominal features are the features that doesn't have inherent ordering as opposed to Ordinal features.\n    * Eg: Names, gender, yes or no.\n\n## Need for categorical feature encoding\n* Categorical features must be encoded before feeding it to the model because many Machine Learning algorithms don't support categorical features as their input.\n* Machine Learning algorithms and Deep Learning algorithms would support only **numerical variables\/ quantitative variables**.\n\n### Ordinal Encoding Techniques \n* Label Encoding or Ordinal Encoding\n\n### Nominal Encoding Techniques \n* Frequency Encoding\n* Target Encoding\n* One-hot Encoding\n* Leave One Out Encoding\n* M-Estimate Encoding\n","87c96559":"# Nominal Encoding\n## 1. Frequency Encoding\n$Frequency Encoding  = \\frac{frequency(category)}{size(data)}$\n\nCategory refers to each of the unique values in a feature.\n* ***Frequency(category)*** = Number of values in that category\n* ***Size(data)*** = Size of the entire dataset.\n\n**Disadvantage:** If two categories have the same frequency then it is hard to distinguish between them.","241659ec":"## 2. Ordinal Encoding using LabelEncoder in sklearn\n**LabelEncoder** would also produce the same result produced by the **OrdinalEncoder**.","65142108":"# Ordinal Encoding\n## 1. Ordinal Encoding using OrdinalEncoder in sklearn\n**OrdinalEncoder** is used to assign numerical values to the categories in the ordinal features.","0ed18542":"## 5. M-Estimate Encoding\nM-Estimate Encoding is also called as additive smoothing overcomes the disadvantages of the Target Encoding (overfitting) by considering a smoothing factor **M** to encode. \n\n$M-Estimate Encoding = \\frac{[count(category) * mean(category)] + [(M * mean(target)]}{count(category) + M}$\n\n","a1816eeb":"## 2. Target Encoding\nEach of the categories is replaced with the mean of the target variable.\n* *Target Encoding = mean(target of a category)*\n\n**Disadvantage:** Tends to overfit the data if some of the categories have low number of occurrences.","d151bba4":"## 3. One-Hot Encoding\nOne Hot Encoding replaces the categories with binary values.\n\n**Disadvantage:** Tree algorithms cannot be applied to one-hot encoded data since it creates sparse matrix.","818b65d7":"## 4. Leave One Out Encoding\nLeave One Out Encoding(LOOE) is very similar to Target Encoding but the difference is LOOE doesn't consider the current row while calculating the mean of the target.\n\n**Disadvantage:** Tends to overfit to the data."}}