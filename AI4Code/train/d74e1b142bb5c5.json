{"cell_type":{"b7272578":"code","4af1c822":"code","1d854280":"code","44ae7bcd":"code","0b3670e8":"code","fefb5a10":"code","28b9fcf8":"code","6f1dc75c":"code","715c8764":"code","ad692f59":"code","048ea040":"code","4c82b94a":"code","33c41731":"code","cffe59d2":"code","73d5d453":"code","eb9fd314":"code","5ed3ce8a":"code","d3eb00bd":"code","a9e680b2":"code","b74b6b4d":"code","4f9a0429":"code","c4295120":"code","dcbb6ddf":"code","f3429955":"code","c16080d3":"code","ac5499bf":"code","57cd33e8":"code","0318282f":"code","0f718023":"code","48a3980c":"code","f6ddd585":"code","fc1d1e66":"code","e33873fe":"code","e113262f":"code","051afe0c":"code","e4b83d9b":"markdown","df2a155d":"markdown","3ffd7c95":"markdown","4669b10c":"markdown","b55e6d0e":"markdown","bb8d4351":"markdown","95a3e0dd":"markdown","0b340dbd":"markdown","c3675132":"markdown","9c23f02b":"markdown","b11ab779":"markdown","ed4bf686":"markdown","490501cf":"markdown","7f49e305":"markdown","5ed305a0":"markdown","963ccbec":"markdown","0932a10f":"markdown","f907623d":"markdown","bdef4712":"markdown"},"source":{"b7272578":"%%capture\n!pip install -qqq -U scikit-learn scikit-plot imbalanced-learn seaborn","4af1c822":"import os\nimport csv\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scikitplot as skplt\n\nfrom lightgbm import LGBMClassifier\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.metrics import classification_report_imbalanced\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GroupKFold, StratifiedKFold\nfrom sklearn.metrics import (matthews_corrcoef, roc_auc_score,\n                             average_precision_score, f1_score)\n\nfrom sklearn.experimental import enable_halving_search_cv # noqa\nfrom sklearn.model_selection import HalvingGridSearchCV","1d854280":"# Set seed for reproducability\nSEED = 1\nrandom.seed(SEED)\nnp.random.seed(SEED)\nos.environ['PYTHONHASHSEED'] = str(SEED)\n\n# Either to used previous grid search results or perform new search\nUSE_TUNED_PARAMS = True","44ae7bcd":"def read_csv(file_path: str, nrows=None, dtype=np.float32) -> pd.DataFrame:\n    with open(file_path, 'r') as f:\n        column_names = next(csv.reader(f))\n    dtypes = {x: dtype for x in column_names if\n            x != 'Class'}\n    dtypes['Class'] = np.int8\n    return pd.read_csv(file_path, nrows=nrows, dtype=dtypes)","0b3670e8":"df = read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.info()","fefb5a10":"df.head()","28b9fcf8":"df.describe()","6f1dc75c":"df.isna().sum()","715c8764":"df_copy = df.copy()","ad692f59":"ax = sns.displot(df_copy, x='Time', hue='Class', kind='kde', fill=True, common_norm=False)\nax.set(title='Time Density Plot per Class')","048ea040":"ax = sns.displot(df_copy, x='Amount', hue='Class', kind='kde', fill=True, common_norm=False)\nax.set(title='Amount Density Plot per Class')","4c82b94a":"df_copy['Hour'] = df_copy['Time'].apply(lambda x: np.ceil(x \/ 3600) % 24)\n\nax = sns.displot(df_copy, x='Hour', hue='Class', kind='kde', fill=True, common_norm=False)\nax.set(title='Amount Density Plot per Class')","33c41731":"# This cell code was obtained from this source:\n# https:\/\/www.kaggle.com\/gpreda\/credit-card-fraud-detection-predictive-models\n\ntmp = df_copy.groupby(['Hour', 'Class'])['Amount'].aggregate(['min', 'max', 'count', 'sum', 'mean', 'median', 'var']).reset_index()\nstats = pd.DataFrame(tmp)\nstats.columns = ['Hour', 'Class', 'Min', 'Max', 'Transactions', 'Sum', 'Mean', 'Median', 'Var']\nstats.head()","cffe59d2":"fig, ax = plt.subplots(1,2, figsize=(12,5))\nsns.lineplot(x='Hour', y='Transactions', data=stats.query('Class == 0'), ax=ax[0])\nsns.lineplot(x='Hour', y='Transactions', data=stats.query('Class == 1'), color='red', ax=ax[1])\nfig.suptitle('Total Number of Transactions per Hour of the Day')\nax[0].set(title='Legit Transactions')\nax[1].set(title='Fraud Transactions')\nfig.show()","73d5d453":"df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)","eb9fd314":"y = df['Class']\nX = df.drop('Class', axis=1)\n\nfeature_names = X.columns.tolist()","5ed3ce8a":"y.value_counts(normalize=True)","d3eb00bd":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True, random_state=SEED)","a9e680b2":"def get_group_stats(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Create features by calculating statistical moments.\n    \n    :param df: Pandas DataFrame containing all features\n    \"\"\"\n    cols = list(filter(lambda x: x.startswith('V'), df.columns))\n    \n    # avoid warnings about returning-a-view-versus-a-copy\n    ds = df.copy()\n    \n    ds['V_mean'] = df[cols].mean(axis=1)\n    ds['V_std'] = df[cols].std(axis=1)\n    ds['V_skew'] = df[cols].skew(axis=1)\n    \n    return ds\n\nX_train = get_group_stats(X_train)\nX_test = get_group_stats(X_test)","b74b6b4d":"X_train.head()","4f9a0429":"# Tested stratified kfold as well\nsfold = StratifiedKFold(n_splits=3)\n\nhour_train = X_train['Time'].apply(lambda x: np.ceil(float(x) \/ 3600) % 24)\n\ngfold = GroupKFold(n_splits=3)\ngroups = list(gfold.split(X_train, y_train, groups=hour_train))","c4295120":"def plot_correlation(corr: str) -> plt.Axes:\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n    f, ax = plt.subplots(figsize=(11, 9))\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","dcbb6ddf":"def select_features_correlation(df: pd.DataFrame) -> list:\n    # Calculate correlations with target\n    full_corr = df.corr(method='spearman')\n    corr_with_target = full_corr['Class'].T.apply(abs).sort_values(ascending=False)\n    \n    min_threshold = corr_with_target.quantile(.25)\n    \n    # Select features with highest correlation to the target variable\n    features_correlation = corr_with_target[corr_with_target >= min_threshold]\n    features_correlation.drop('Class', inplace=True)\n    return features_correlation.index.tolist()","f3429955":"plot_correlation(X_train.corr(method='spearman'))","c16080d3":"sampler = RandomUnderSampler(random_state=SEED)\nX_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\nX_resampled['Class'] = y_resampled\n\nplot_correlation(X_resampled.corr(method='spearman'))","ac5499bf":"cols = select_features_correlation(X_resampled)\nX_train = X_train[cols]\nX_test = X_test[cols]","57cd33e8":"pipeline = Pipeline([\n    ('standardscaler', StandardScaler()),\n    ('clf', LGBMClassifier(n_jobs=-1, random_state=SEED)),\n])","0318282f":"mcc_results = []\nap_results = []\nroc_auc_results = []\nf1_results = []\n\nfor train_index, test_index in groups:\n    pipeline.fit(X_train.values[train_index], y_train.values[train_index])\n    test_data = pipeline['standardscaler'].transform(X_train.values[test_index])\n    \n    y_pred = pipeline.predict(test_data)\n    \n    mcc_results.append(matthews_corrcoef(y_train.values[test_index], y_pred))\n    ap_results.append(average_precision_score(y_train.values[test_index], y_pred))\n    roc_auc_results.append(roc_auc_score(y_train.values[test_index], y_pred))\n    f1_results.append(f1_score(y_train.values[test_index], y_pred))\n\n\nprint(f'Baseline MCC score: {np.mean(mcc_results)}')\nprint(f'Baseline AP score: {np.mean(ap_results)}')\nprint(f'Baseline ROC AUC score: {np.mean(roc_auc_results)}')\nprint(f'Baseline F1 score: {np.mean(f1_results)}')","0f718023":"gbm_grid = {\n    'clf__n_estimators': [500, 1000],\n    'clf__learning_rate': [0.1, 0.01],\n    'clf__max_depth': [4, 6, 8],\n    'clf__colsample_bytree': np.linspace(0.6, 1.0, num=5),\n    # 'clf__reg_alpha': np.linspace(0., 1.0, num=5),\n    # 'clf__subsample': np.linspace(0.7, 1.0, num=4),\n}","48a3980c":"def grid_search_tuning(pipeline, X, y, grid, fold):\n    grid = HalvingGridSearchCV(pipeline, param_grid=grid, cv=fold, scoring='f1',\n                               random_state=SEED, n_jobs=-1, verbose=1)\n    grid.fit(X, y)\n    \n    print(f\"\\nMean test score: {np.mean(grid.cv_results_['mean_test_score'])}\")\n    print(f'Best parameters: {grid.best_params_}')\n    \n    return grid.best_estimator_","f6ddd585":"if USE_TUNED_PARAMS:\n    print('Using previously tuned model parameters.')\n    \n    best_params = {\n        'n_estimators': 500,\n        'learning_rate': 0.01,\n        'max_depth': 8,\n        'colsample_bytree': 0.6,\n    }\n    \n    pipeline = Pipeline([\n        ('standardscaler', StandardScaler()),\n        ('clf', LGBMClassifier(**best_params, n_jobs=-1, random_state=SEED)),\n    ])\n    \n    grid = pipeline.fit(X_train, y_train)\nelse:\n    pipeline = Pipeline([\n        ('standardscaler', StandardScaler()),\n        ('clf', LGBMClassifier(n_jobs=-1, random_state=SEED)),\n    ])\n\n    # It can take a while to run\n    grid = grid_search_tuning(pipeline, X_train, y_train, gbm_grid, groups)","fc1d1e66":"def score_model(clf, X_test, y_test, feature_names):\n    y_pred = clf.predict(X_test)\n    y_probas = clf.predict_proba(X_test)\n\n    print(classification_report_imbalanced(y_test, y_pred, target_names=['Legit', 'Fraud']))\n    print(f'MCC: {matthews_corrcoef(y_test, y_pred)}\\nAP: ' +\n          f'{average_precision_score(y_test, y_pred)}\\nROC AUC: {roc_auc_score(y_test, y_pred)}')\n\n    fig, ax = plt.subplots(1,2, figsize=(12,5))\n    skplt.metrics.plot_precision_recall(y_test, y_probas, ax=ax[0])\n    skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True, ax=ax[1])\n    fig.show()\n    \n    skplt.estimators.plot_feature_importances(clf, feature_names=feature_names, figsize=(16,5))\n\n    return y_probas","e33873fe":"scaled_X_test = grid['standardscaler'].transform(X_test)\n\ny_probas = score_model(grid['clf'], scaled_X_test, y_test, feature_names)","e113262f":"skplt.metrics.plot_calibration_curve(y_test, [y_probas], ['LightGBM'], figsize=(6,5))","051afe0c":"skplt.estimators.plot_learning_curve(grid['clf'], X_train, y_train, scoring='f1', n_jobs=-1, random_state=SEED)","e4b83d9b":"Model's performance evaluation on the test set and feature importance.","df2a155d":"## Importing the Libraries","3ffd7c95":"We will check if there are any missing values.","4669b10c":"## Fine-Tuning","b55e6d0e":"For the train\/test split, we will divide them into 80% for the train data and 20% for the test data in a stratified fashion.","bb8d4351":"As we can observe, the data is highly unbalanced, with less than 1% of positive fraud examples.","95a3e0dd":"# Modelling\n\nWe will use grid search to tune the hyperparameters of the model. And, we will apply cross-validation using the group fold strategy as mentioned above. To evaluate the performance:\n- We will use the *F1-score*, a more relevant metric in this situation since false positives and false negatives are both critical.\n- Using Accuracy in this scenario can output misleading results.\n- We will use a maximum depth value to prevent overfitting the training data.\n- We will test using different percentages of features used by each tree.\n- Finally, we will try out two possibilities for the learning rate.\n- I kept other parameters to fine-tune the alpha regularization value and the subsample of the training set for future changes.\n\n## Baseline\n\nFirst, we will train a simple baseline model with default parameters to have some initials results that we can compare with and identify what things we aim to improve.","0b340dbd":"# Data Exploration","c3675132":"# Credit Card Fraud Detection\n\n**Last updated:** 2021\/05\/29","9c23f02b":"# Conclusions\n\nWith this dataset, resampling the data didn't produce good results. Using under-sampling, over-sampling or a combination of both didn't improve compared with unchanged class proportions.\nPerhaps, since most variables were from a PCA transformation, that affected the impact of sampling. Another potential reason is the very low number of fraudulent examples caused the under-sampling to produce a tiny dataset, lacking enough data to train a decent model.\n\nIn a fraud detection system, two cases are essential for a successful solution: achieving the primary goal of detecting fraudulent transactions (true positive examples) and avoiding targeting genuine transactions as fraudulent (false positives). These two are the most costly to a business when the system does not perform well. This model delivers good results at identifying true positives with an F1 score of 0.87 and does not label any genuine transactions as fraudulent.\n\nI hope that this notebook has helpful and if you have any remarks or questions, feel free to leave a comment.\n\nYou can also follow me on Twitter [@hmatalonga](https:\/\/twitter.com\/hmatalonga). I talk about Data Science, Machine Learning, among other topics I am interested in.","b11ab779":"# Feature Engineering\n\nWe will compute some additional features containing statistical information about the anonymized variables.","ed4bf686":"# Loading the Data\n\nWe will define a customized `read_csv` method to optimize data types and use less memory.","490501cf":"When we plot the volume of transactions per class grouped by hour of the day, we can observe some interesting patterns as fraud transactions seem to have spikes at specific hours and occur during typical sleeping hours.","7f49e305":"# Data Preparation\n\nTo build the model, I incrementally increased the fraction of data used by 20% to adjust the model parameters progressively to avoid overfitting. For this final build, we will use the entirety of the dataset.","5ed305a0":"We will train our model using cross-validation. To generate the splits, we will be applying a `GroupKFold` strategy. The group will be based on the **hour of the day** of the given transaction. From a previous plot, we could observe that specific hours have a higher volume of fraudulent transactions.","963ccbec":"# Introduction\n\nIn this notebook, we will explore the \"Credit Card Fraud Detection\" dataset and build a model to predict which transactions are legit and fraudulent. To describe the contents of the dataset, I will quote the original description:\n\n> The dataset contains transactions made by credit cards in September 2013 by European cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\n> It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\nWe are informed that the dataset is highly unbalanced and that most variables are anonymized and were obtained via PCA transformation. Therefore, we will try a few strategies to handle imbalanced classification and incrementally build a model combining these techniques to optimize its performance. Here are some common approaches to deal with Imbalanced Data:\n\n1. Resampling the Data, either undersampling or oversampling or a combination of both.\n2. Generate Synthetic Data. Use GANs or other methods to create more data points.\n3. Choosing a more appropriate performance metric. Selecting the wrong metric can generate misleading results.\n4. Using different Class weights or Cost-sensitive models.\n5. Collecting more data, in particular for the minority class if possible.\n6. Frame the problem differently. Consider things like Anomaly Detection. \n\nI have written a blog post about this notebook, where I go into more detail in some sections. Take a look:\n\n[\ud83d\udcb3 Handling Imbalanced Datasets: Fraud Detection Study Case](https:\/\/hmatalonga.com\/blog\/handling-imbalanced-datasets-fraud-detection)","0932a10f":"Because the data is highly unbalanced, it is difficult to see the correlation between some features. However, after experimenting with undersampling the data to reduce the number of examples of the majority class (legit transactions) and have equal class proportions, we can now see that the correlation values are more noticeable. We will use the correlation amount to select higher correlated features with our target class.","f907623d":"We will select features that have a correlation with our target class higher than the lower quartile value. Dropping features with low correlation can help to reduce the complexity of the model.","bdef4712":"We will use a copy of the DataFrame to make modifications for visualisation purposes only."}}