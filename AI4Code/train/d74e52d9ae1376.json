{"cell_type":{"5c9e5b31":"code","7d51603c":"code","95f16935":"code","cd8c64ab":"code","0057e340":"code","7aed8982":"code","cd46b7d3":"code","e46d9f4f":"code","bd48a872":"code","e9b3c9a9":"code","3260f290":"code","ad321a0c":"code","ea89ff63":"code","a622713c":"code","8bd47dc0":"code","9c1f6b68":"code","9406af22":"code","b02c548f":"code","138a28ab":"code","05e76d5f":"code","6d2326da":"code","0eb41a7e":"code","225ec45d":"code","c4315540":"code","517c039d":"code","ad668ccc":"code","03a797cd":"code","eee98dcc":"code","ab11daa1":"code","17b7b310":"code","c29fb019":"code","55f631fb":"code","61362b1b":"code","a5b3ca31":"code","f62345ec":"code","ec251131":"code","0e871df9":"code","98fe4fd4":"code","d44e8835":"code","f42f3627":"code","ca28f239":"code","a2f41bd9":"markdown","c9afb118":"markdown","cdb3c8f8":"markdown","15792749":"markdown","70b4d2d6":"markdown","0d4ba9bd":"markdown","8d670fb8":"markdown","2f2ad6f8":"markdown","041c9838":"markdown","9ac31a70":"markdown","56f1e4c8":"markdown","558fbf28":"markdown","a3ee129f":"markdown","816617b7":"markdown","0d0aabfa":"markdown","dea92c11":"markdown","281aacc9":"markdown","51874346":"markdown","2aa9c7fb":"markdown"},"source":{"5c9e5b31":"import numpy as np\nimport pandas as pd\nimport xgboost\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import RandomForestClassifier as rfc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import confusion_matrix\n\nfrom mlxtend.plotting import plot_confusion_matrix","7d51603c":"df = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf","95f16935":"print('Total number of rows are:',df.shape[0])\nprint('Total number of columns are:',df.shape[1])","cd8c64ab":"df.columns","0057e340":"df.describe()","7aed8982":"df.isna().sum(axis=0)","cd46b7d3":"df.corr()","e46d9f4f":"correlation_mat = df.corr()\ncorr_features = correlation_mat.index\nplt.figure(figsize=(20,20))\ng = sns.heatmap(df[corr_features].corr(),annot=True,cmap='RdYlGn')\nplt.show()","bd48a872":"print('There are total',df['Outcome'].nunique(),'unique values in the outcome column')\nprint('Unique values in outcome column are',df['Outcome'].unique())","e9b3c9a9":"print('Total number of 0(False count) are',(df['Outcome']==0).sum())\nprint('Total number of 1(True count) are',(df['Outcome']==1).sum())","3260f290":"plt.figure(figsize=(5,5))\ncolors = ['#04FFCD','#FF04E6']\nsns.countplot(x='Outcome',data=df,palette=colors)\nplt.show()","ad321a0c":"df.head(15)","ea89ff63":"print('Number of rows missing Glucose: {0}'.format(len(df.loc[df['Glucose'] == 0])))\nprint('Number of rows missing Blood Pressure: {0}'.format(len(df.loc[df['BloodPressure'] == 0])))\nprint('Number of rows missing Insulin: {0}'.format(len(df.loc[df['Insulin'] == 0])))\nprint('Number of rows missing BMI: {0}'.format(len(df.loc[df['BMI'] == 0])))\nprint('Number of rows missing Skin Thickness: {0}'.format(len(df.loc[df['SkinThickness'] == 0])))\nprint('Number of rows missing Age: {0}'.format(len(df.loc[df['Age'] == 0])))\nprint('Number of rows missing Diabetes Pedigree Function: {0}'.format(len(df.loc[df['DiabetesPedigreeFunction'] == 0])))","a622713c":"x = df['Glucose'].mean()\ndf['Glucose'].replace(0,x,inplace=True)\nx = df['BloodPressure'].mean()\ndf['BloodPressure'].replace(0,x,inplace=True)\nx = df['Insulin'].mean()\ndf['Insulin'].replace(0,x,inplace=True)\nx = df['BMI'].mean()\ndf['BMI'].replace(0,x,inplace=True)\nx = df['SkinThickness'].mean()\ndf['SkinThickness'].replace(0,x,inplace=True)","8bd47dc0":"df.head(10)","9c1f6b68":"!pip install autoviz","9406af22":"!pip install xlrd","b02c548f":"from autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()","138a28ab":"dft = AV.AutoViz('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv', \n                 dfte=df,\n                 header=0, \n                 verbose=2, \n                 lowess=False,\n                 chart_format=\"svg\", \n                 max_rows_analyzed=1000, \n                 max_cols_analyzed=10)","05e76d5f":"y = df['Outcome']\nX = df.drop(columns=['Outcome'])\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)","6d2326da":"X_train","0eb41a7e":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_train_scaled","225ec45d":"X_test_scaled = scaler.transform(X_test)\nX_test_scaled","c4315540":"rfc_model = rfc(random_state=10)\nrfc_model.fit(X_train_scaled, y_train.ravel())","517c039d":"y_predicted = rfc_model.predict(X_test_scaled)\nprint(\"Accuracy of Random Forest Model is = {0: .3f}\".format(metrics.accuracy_score(y_test,y_predicted)))","ad668ccc":"y_actual = y_test\ny_actual = y_actual.to_numpy() #  COnverting to numpy array\ny_actual","03a797cd":"print(\"The mean squared error is:\",mse(y_actual,y_predicted))","eee98dcc":"target_names = ['class 0', 'class 1']\nprint(classification_report(y_actual, y_predicted, target_names=target_names))","ab11daa1":"conf_matrix = confusion_matrix(y_true=y_actual, y_pred=y_predicted)\nfig, ax = plot_confusion_matrix(conf_mat=conf_matrix, figsize=(8,8), cmap=plt.cm.Greens)\nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)\nplt.show()","17b7b310":"params = {\n    \"learning_rate\" : [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4],\n    \"max_depth\" : [3,4,5,6,8,10,12,13,15],\n    \"min_child_weight\" : [1,3,5,7],\n    \"gamma\" : [0,0.1,0.2,0.3,0.4,0.42,0.45],\n    \"colsample_bytree\" : [0.3,0.4,0.5,0.7],\n}","c29fb019":"xgb_model = xgboost.XGBClassifier(eval_metric='logloss')\nrandom_search=RandomizedSearchCV(xgb_model,\n                                 param_distributions=params,\n                                 n_iter=5,\n                                 scoring='roc_auc',\n                                 n_jobs=1,\n                                 cv=5,\n                                 verbose=3\n                                )","55f631fb":"random_search.fit(X_train_scaled,y_train.ravel())","61362b1b":"estimator = random_search.best_estimator_\nestimator.missing=1\nprint(estimator)","a5b3ca31":"xgb_model = estimator","f62345ec":"xgb_model.fit(X_train_scaled,y_train)","ec251131":"y_predicted = xgb_model.predict(X_test_scaled)\ny_predicted","0e871df9":"score = cross_val_score(xgb_model,X_train_scaled,y_train.ravel(),cv=10)\nscore","98fe4fd4":"print(\"Score obtained after hyper parameter tuning in XgBoost is:\",score.mean())","d44e8835":"target_names = ['class 0', 'class 1']\nprint(classification_report(y_actual, y_predicted, target_names=target_names))","f42f3627":"print(\"The mean squared error is:\",mse(y_actual,y_predicted))","ca28f239":"conf_matrix = confusion_matrix(y_true=y_actual, y_pred=y_predicted)\nfig, ax = plot_confusion_matrix(conf_mat=conf_matrix, figsize=(8,8), cmap=plt.cm.Greens)\nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)\nplt.show()","a2f41bd9":"## 6. Using Autoviz library for data visualization ","c9afb118":"> Below obtained is a classification report and a confusion matrix is plotted.","cdb3c8f8":"> Now, we will calculate mean square error.","15792749":"## 4. Checking whether the dataset is balanced or not ","70b4d2d6":"## 2. Checking if there are any NA values present or not ","0d4ba9bd":"> Now, I will split the dataset into training and testing in the below block of code and then I will apply various machine learning algorithms for prediction.\n\n","8d670fb8":"## 9. Hyper-parameter Optimization using RandomizedSearchCV in XgBoost Classifier","2f2ad6f8":"## 7. Splitting the dataset for training & testing and standardizing the data","041c9838":"> Now, we will calculate mean square error.","9ac31a70":"## 5. Checking some consistency in the dataset ","56f1e4c8":"> From the above output dataframe, it is clearly visible that some of the features have 0 as a value. Hence, in this scenario one can say that the dataset is not consistent as these values just cannot be 0. Below I will just check that how many missing 0's are present in the feature columns. \n\n> In order to compute the 0 values, I have computed the mean of a feature which has 0 values and then replaced that 0 values with the computed mean below.","558fbf28":"## Name: Jay Shah\n## Date: 11-08-2021\n### Pima Indians Diabetes Analysis ","a3ee129f":"> Below is the timer function which will calculate how much time is taken by RandomizedSearchCV","816617b7":"> Below obtained is the classification report","0d0aabfa":"## 1. Reading the dataframe","dea92c11":"> Now, we will standardize the whole data.Data standardization is the process of rescaling the attributes so that they have mean as 0 and variance as 1.\n\n> The ultimate goal to perform standardization is to bring down all the features to a common scale without distorting the differences in the range of the values.\n\n> In sklearn.preprocessing.StandardScaler(), centering and scaling happens independently on each feature.\n\n> The formula which performs standardization is $(x-mean)\/(sd)$\n\n> fit_transform() is used on the training data so that we can scale the training data and also learn the scaling parameters of that data.","281aacc9":"> Plotting the confusion matrix below:","51874346":"## 8. Applying Random Forest Classifier Algorithm for prediction ","2aa9c7fb":"## 3. Building a correlation matrix"}}