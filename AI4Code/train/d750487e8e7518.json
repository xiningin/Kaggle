{"cell_type":{"a525e0dd":"code","e00f0a3d":"code","60cb7960":"code","7b8803ba":"code","ba3d8c2d":"code","1a069be2":"code","e7a1ef1d":"code","8c282c6b":"code","59e38d19":"code","016432d3":"code","70241236":"code","b3ec410b":"code","c8a8d93c":"code","e52e2178":"code","43ff34ce":"code","370cb54f":"code","22682652":"code","bb56925d":"code","64e67e78":"code","178137cb":"code","ce65f510":"code","934757ff":"code","ddbcc883":"code","ecc13a92":"code","174980f3":"code","a52445de":"code","f63cf133":"code","2e5ce6cf":"code","f989ebb3":"code","ec547471":"code","3d80de2d":"code","2a498910":"code","f3d9f788":"code","19998474":"code","ae75a41e":"code","2ca87d80":"code","6523284d":"code","f972e936":"code","d73085ce":"code","5c8d0501":"code","b5122c87":"code","a24c2470":"markdown","46e91793":"markdown","bf742bd0":"markdown","e4f609df":"markdown","67cdf4f9":"markdown","98dec322":"markdown","c6126f61":"markdown","bcf7f7d8":"markdown","fa2264ba":"markdown","7178e246":"markdown","e915d302":"markdown","37026c0b":"markdown","1aea9bc6":"markdown"},"source":{"a525e0dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e00f0a3d":"df=pd.read_csv('\/kaggle\/input\/productivity-prediction-of-garment-employees\/garments_worker_productivity.csv')","60cb7960":"import warnings\nwarnings.filterwarnings('ignore')","7b8803ba":"df","ba3d8c2d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime","1a069be2":"df['date']=pd.to_datetime(df['date'])","e7a1ef1d":"df.describe()","8c282c6b":"df.isnull().sum()","59e38d19":"df.groupby('targeted_productivity')['targeted_productivity'].count().plot.bar()","016432d3":"df.groupby('actual_productivity')['actual_productivity'].count().plot()","70241236":"df.groupby('smv')['smv'].count().plot.bar()","b3ec410b":"df.groupby('wip')['wip'].count().plot()","c8a8d93c":"sns.heatmap(df.corr())","e52e2178":"df['month']=df['date'].dt.month","43ff34ce":"df","370cb54f":"x=df.drop(['date','actual_productivity'],axis=1)\ny=df['actual_productivity']","22682652":"x=pd.get_dummies(x)","bb56925d":"x=x.fillna(x.mean())","64e67e78":"x","178137cb":"x['team']=x['team'].astype(str)","ce65f510":"x=pd.get_dummies(x)","934757ff":"x","ddbcc883":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression,Ridge,LassoCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","ecc13a92":"x_train, x_test, y_train, y_test = train_test_split(x, y,train_size=0.8,random_state=0)","174980f3":"model_lr=LinearRegression()\nmodel_lr.fit(x_train,y_train)\npred_train=model_lr.predict(x_train)\npred_test=model_lr.predict(x_test)\nprint(\"train_RMSE:\",np.sqrt(mean_squared_error(y_train, pred_train)))\nprint(\"test_RMSE:\",np.sqrt(mean_squared_error(y_test, pred_test)))\nprint(\"train_MAE:\",mean_absolute_error(y_train, pred_train))\nprint(\"test_MAE:\",mean_absolute_error(y_test, pred_test))\nprint(\"R^2:{}\".format(model_lr.score(x_test, y_test)))","a52445de":"from scipy.stats import skew","f63cf133":"AP=pd.DataFrame({'ap':y,'log(ap+1)':np.log1p(y)})\nprint(AP, '\u00a5n')\n\nprint('ap skew        :',skew(AP['ap']))\nprint('log(ap+1) skew:', skew(AP['log(ap+1)']))\n\nAP.hist()","2e5ce6cf":"x_skew=x.apply(lambda x:skew(x))\nprint(x_skew)","f989ebb3":"x_skew = x_skew[x_skew > 0.75]\nprint('-----Skewness greater than 0.75-----')\nprint(x_skew)\nx_skew = x_skew.index\n\nx[x_skew] = np.log1p(x[x_skew])\nx[x_skew]","ec547471":"x","3d80de2d":"x_train, x_test, y_train, y_test = train_test_split(x, y,train_size=0.8,random_state=0)","2a498910":"model_lr=LinearRegression()","f3d9f788":"model_lr.fit(x_train,y_train)\npred_train=model_lr.predict(x_train)\npred_test=model_lr.predict(x_test)\nprint(\"train_RMSE:\",np.sqrt(mean_squared_error(y_train, pred_train)))\nprint(\"test_RMSE:\",np.sqrt(mean_squared_error(y_test, pred_test)))\nprint(\"train_MAE:\",mean_absolute_error(y_train, pred_train))\nprint(\"test_MAE:\",mean_absolute_error(y_test, pred_test))\nprint(\"R^2:{}\".format(model_lr.score(x_test, y_test)))","19998474":"def rmse_cv(model):\n    rmse = np.sqrt(\n        -cross_val_score(\n            model, x_train, y_train,\n            scoring=\"neg_mean_squared_error\", \n            cv = 5))\n    return(rmse)","ae75a41e":"model_rg = Ridge()\n\nalphas = [0.3,0.4, 0.5, 0.6,0.7]\ncv_rg = [rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]\ncv_rg = pd.Series(cv_rg, index = alphas)\n\nprint('Ridge RMSE loss:')\nprint(cv_rg, '\\n')\n\nprint('Ridge RMSE loss Mean:')\nprint(cv_rg.mean())\n\n\nplt.\ufb01gure(\ufb01gsize=(10, 5))\nplt.plot(cv_rg)\nplt.grid()\nplt.title('Validation - by regularization strength')\nplt.xlabel('Alpha')\nplt.ylabel('RMSE')\nplt.show()","2ca87d80":"model_rg.fit(x_train,y_train)\npred1=model_rg.predict(x_test)\nprint(\"test_RMSE:\",np.sqrt(mean_squared_error(y_test, pred1)))\nprint(\"test_MAE:\",mean_absolute_error(y_test, pred1))\nprint(\"R^2:{}\".format(model_rg.score(x_test, y_test)))","6523284d":"model_ls = LassoCV(\n    alphas = [1, 0.1, 0.001, 0.0005]).fit(x_train, y_train)\n\nprint('Lasso regression RMSE loss:')\nprint(rmse_cv(model_ls))\n\nprint('Average loss:', rmse_cv(model_ls).mean())\nprint('Minimum loss:', rmse_cv(model_ls).min())\nprint('Best alpha  :', model_ls.alpha_) ","f972e936":"model_ls.fit(x_train,y_train)\npred2=model_ls.predict(x_test)\nprint(\"test_RMSE:\",np.sqrt(mean_squared_error(y_test, pred2)))\nprint(\"test_MAE:\",mean_absolute_error(y_test, pred2))\nprint(\"R^2:{}\".format(model_ls.score(x_test, y_test)))","d73085ce":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(x_train, label = y_train)\n\nparams = {\"max_depth\":3, \"eta\":0.1}\n\ncross_val = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=1000,\n    early_stopping_rounds=50)\ncross_val","5c8d0501":"plt.\ufb01gure(\ufb01gsize=(8, 6))\nplt.plot(cross_val.loc[0:,[\"test-rmse-mean\", \"train-rmse-mean\"]])\nplt.grid()\nplt.xlabel('num_boost_round')\nplt.ylabel('RMSE')\nplt.show()","b5122c87":"model_xgb = xgb.XGBRegressor(\n    n_estimators=110,\n    max_depth=3,\n    learning_rate=0.1)\nmodel_xgb.fit(x_train, y_train)\npred3=model_xgb.predict(x_test)\n\nprint('xgboost RMSE loss:')\nprint(rmse_cv(model_xgb).mean())\nprint(\"test_RMSE:\",np.sqrt(mean_squared_error(y_test, pred3)))\nprint(\"test_MAE:\",mean_absolute_error(y_test, pred3))\nprint(\"R^2:{}\".format(model_xgb.score(x_test, y_test)))","a24c2470":"I tired to logarithmic features whose skews are greater than 0.75.","46e91793":"# In four models, XGBoost with Logarithmic seems to be better. MAE0.08< The baseline performance error0.15.","bf742bd0":"# 3)LassoCV","e4f609df":"I should not use logarithmic transformation in \"y\".","67cdf4f9":"# I tried MAE which is less than the baseline(0.15) with XGBoost.","98dec322":"# It seems that there are a lot of features which are not normal distribution.","c6126f61":"# 1. Visualization","bcf7f7d8":"# 3.Logarithmic","fa2264ba":"# 4.Prediction Model","7178e246":"# 2) Ridge","e915d302":"# 4)XGBoost","37026c0b":"# 1)LinearRegression","1aea9bc6":"# 2. Firat Step Analysis: LinearRegression"}}