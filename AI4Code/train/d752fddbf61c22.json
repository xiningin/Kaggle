{"cell_type":{"bb421502":"code","716127a1":"code","1812dc68":"code","af6ae215":"code","a1e5d576":"code","44580b14":"code","1bf27468":"code","0bda31cd":"code","879c99cc":"code","4ecaafdb":"code","3a5fa22d":"code","d3432ec9":"code","d1cbae5a":"code","88812423":"code","56c4ead7":"code","c1469d40":"code","c1ed73a1":"code","e5f8f518":"code","0d6f1beb":"code","ef524d19":"code","b8c541e2":"code","ebdca423":"code","1ec5910c":"code","d886feef":"code","29e0f2ba":"code","c2bf2e54":"code","6d044232":"code","3389b4d8":"code","166dce9f":"code","22058bb7":"code","66442372":"code","606cca85":"code","ec056b98":"code","3cdea135":"code","79e5c422":"code","26366fd2":"code","8c8937ff":"code","5c742b22":"code","62129952":"code","59329343":"markdown","e1dc40b9":"markdown","60317d1f":"markdown"},"source":{"bb421502":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","716127a1":"import glob\nimport numpy as np\nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array","1812dc68":"import os\nimport cv2                 \nfrom random import shuffle\nfrom tqdm import tqdm  \nimport tensorflow as tf \nfrom tensorflow.keras import Model\nfrom tensorflow.keras.utils import plot_model\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , ModelCheckpoint\nfrom collections import Counter\nimport cv2\nfrom tqdm import tqdm\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Input, merge, UpSampling2D, Cropping2D, ZeroPadding2D, Reshape, core, Convolution2D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.optimizers import SGD\nfrom keras.layers.merge import concatenate\nfrom sklearn.metrics import fbeta_score","af6ae215":"glob.glob('\/kaggle\/input\/diseasecotton\/data\/train\/*')","a1e5d576":"TrainImage=\"\/kaggle\/input\/diseasecotton\/data\/train\"\nTestImage=\"\/kaggle\/input\/diseasecotton\/data\/val\"\ndiseased_cotton_leaf_images = os.listdir(TrainImage + \"\/diseased cotton leaf\")\ndiseased_cotton_plant_images = os.listdir(TrainImage + \"\/diseased cotton plant\")\nfresh_cotton_plant_images = os.listdir(TrainImage + \"\/fresh cotton plant\")\nfresh_cotton_leaf_images = os.listdir(TrainImage + \"\/fresh cotton leaf\")","44580b14":"print(len(diseased_cotton_leaf_images), len(diseased_cotton_plant_images), len(fresh_cotton_plant_images), len(fresh_cotton_leaf_images))\nNUM_TRAINING_IMAGES = len(diseased_cotton_leaf_images)+len(diseased_cotton_plant_images)+len(fresh_cotton_plant_images)+len(fresh_cotton_leaf_images)\nprint(NUM_TRAINING_IMAGES)","1bf27468":"image_size = 224 \nBATCH_SIZE = 16 \nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\n\ndata_path = '\/kaggle\/input\/diseasecotton'\n\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   zoom_range = 0.2,\n                                   rotation_range=15,\n                                   horizontal_flip = True)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntraining_set = train_datagen.flow_from_directory(data_path + '\/data\/train',\n                                                 target_size = (image_size, image_size),\n                                                 batch_size = BATCH_SIZE,\n                                                 class_mode = 'categorical',\n                                                 shuffle=True)\n\ntesting_set = test_datagen.flow_from_directory(data_path + '\/data\/val',\n                                            target_size = (image_size, image_size),\n                                            batch_size = BATCH_SIZE,\n                                            class_mode = 'categorical',\n                                            shuffle = True)","0bda31cd":"print(\"train batch \", training_set.__getitem__(0)[0].shape)\nprint(\"test batch \", testing_set.__getitem__(0)[0].shape)","879c99cc":"training_set.class_indices","4ecaafdb":"labels = [\"diseased cotton leaf\",\"diseased cotton plant\",\"fresh cotton leaf\",\"fresh cotton plant\"]","3a5fa22d":"sample_data = testing_set.__getitem__(1)[0] \nsample_label = testing_set.__getitem__(1)[1] ","d3432ec9":"plt.figure(figsize=(15,12))\nfor i in range(15):\n    plt.subplot(3, 6, i + 1)\n    plt.axis('off')\n    plt.imshow(sample_data[i])\n    plt.title(labels[np.argmax(sample_label[i])])","d1cbae5a":"def display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(15,15), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","88812423":"# https:\/\/keras.io\/examples\/vision\/grad_cam\/\nfrom tensorflow import keras\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, classifier_layer_names):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer\n    last_conv_layer = model.get_layer(last_conv_layer_name)\n    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n\n    # Second, we create a model that maps the activations of the last conv\n    # layer to the final class predictions\n    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n    x = classifier_input\n    for layer_name in classifier_layer_names:\n        x = model.get_layer(layer_name)(x)\n    classifier_model = keras.Model(classifier_input, x)\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        # Compute activations of the last conv layer and make the tape watch it\n        last_conv_layer_output = last_conv_layer_model(img_array)\n        tape.watch(last_conv_layer_output)\n        # Compute class predictions\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n\n    # This is the gradient of the top predicted class with regard to\n    # the output feature map of the last conv layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = np.maximum(heatmap, 0) \/ np.max(heatmap)\n    return heatmap, top_pred_index.numpy()","56c4ead7":"# https:\/\/keras.io\/examples\/vision\/grad_cam\/\ndef superimposed_img(image, heatmap):\n    # We rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # We use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # We use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # We create an image with RGB colorized heatmap\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((image_size, image_size))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * 0.4 + image\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n    return superimposed_img","c1469d40":"#label smoothing https:\/\/www.linkedin.com\/pulse\/label-smoothing-solving-overfitting-overconfidence-code-sobh-phd\/\ndef categorical_smooth_loss(y_true, y_pred, label_smoothing=0.1):\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=label_smoothing)\n    return loss","c1ed73a1":"!pip install efficientnet\nimport efficientnet.tfkeras as efn","e5f8f518":"# training call backs \nlr_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, epsilon=0.0001, patience=15, verbose=1)\nes_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)","0d6f1beb":"# https:\/\/stackoverflow.com\/questions\/42586475\/is-it-possible-to-automatically-infer-the-class-weight-from-flow-from-directory\ncounter = Counter(training_set.classes)                          \nmax_val = float(max(counter.values()))       \nclass_weights = {class_id : max_val\/num_images for class_id, num_images in counter.items()}\nclass_weights","ef524d19":"print(efn.EfficientNetB7(weights='imagenet').input_shape) ","b8c541e2":"# EXTRA In case you want to save the h5 file \n# please dont forget to change the model names.\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor='val_acc', \n    verbose=1, \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto'\n)","ebdca423":"from keras import Input\nfrom keras.layers import Conv2D, MaxPooling2D, Embedding, Reshape, Concatenate, SeparableConv2D\nimport tensorflow as tf\nfrom keras.layers.normalization import BatchNormalization\n\ninputs = Input(shape=(224, 224, 3))\n\n# First conv block\nx = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\nx = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Second conv block\nx = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Third conv block\nx = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Fourth conv block\nx = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)\n\n# Fifth conv block\nx = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)\n\n# FC layer\nx = Flatten()(x)\nx = Dense(units=512, activation='relu')(x)\nx = Dropout(rate=0.7)(x)\nx = Dense(units=128, activation='relu')(x)\nx = Dropout(rate=0.3)(x)\nx = Dense(units=64, activation='relu')(x)\nx = Dropout(rate=0.5)(x)\n\n# Output layer\noutput = Dense(units=4, activation='softmax')(x)\n\n# Creating model and compiling\nmodel = tf.keras.Model(inputs=inputs, outputs=output)\nmodel.compile(optimizer='RMSprop', loss=categorical_smooth_loss, metrics=['accuracy'])\n\n# Callbacks\ncheckpoint = ModelCheckpoint(filepath='best_weights.hdf5', save_best_only=True, save_weights_only=True)\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')","1ec5910c":"history = model.fit_generator(training_set,\n                              validation_data=testing_set,\n                              callbacks=[lr_reduce, es_callback],\n                              epochs=20)","d886feef":"display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\ndisplay_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 212)","29e0f2ba":"pretrained_efnet = efn.EfficientNetB7(input_shape=(image_size, image_size, 3), weights='noisy-student', include_top=False)\n\nfor layer in pretrained_efnet.layers:\n  layer.trainable = False\n\nx2 = pretrained_efnet.output\nx2 = tf.keras.layers.AveragePooling2D(name=\"averagepooling2d_head\")(x2)\nx2 = tf.keras.layers.Flatten(name=\"flatten_head\")(x2)\nx2 = tf.keras.layers.Dense(64, activation=\"relu\", name=\"dense_head\")(x2)\nx2 = tf.keras.layers.Dropout(0.5, name=\"dropout_head\")(x2)\nmodel_out = tf.keras.layers.Dense(4, activation='softmax', name=\"predictions_head\")(x2)\n\nmodel_efnet = Model(inputs=pretrained_efnet.input, outputs=model_out)\nmodel_efnet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=categorical_smooth_loss,metrics=['accuracy'])\nmodel_efnet.summary()","c2bf2e54":"history_efnet = model_efnet.fit_generator(training_set,\n                                          validation_data=testing_set,\n                                          callbacks=[lr_reduce, es_callback],\n                                          epochs=15)","6d044232":"display_training_curves(history_efnet.history['loss'], history_efnet.history['val_loss'], 'loss', 211)\ndisplay_training_curves(history_efnet.history['accuracy'], history_efnet.history['val_accuracy'], 'accuracy', 212)","3389b4d8":"#try\npretrained_densenet = tf.keras.applications.DenseNet121(input_shape=(image_size, image_size, 3), weights='imagenet', include_top=False)\n\nfor layer in pretrained_densenet.layers[:-3]:\n  layer.trainable = False\n\nx1 = pretrained_densenet.output\nx1 = tf.keras.layers.AveragePooling2D(name=\"averagepooling2d_head\")(x1)\nx1 = tf.keras.layers.Flatten(name=\"flatten_head\")(x1)\nx1 = tf.keras.layers.Dense(64, activation=\"relu\", name=\"dense_head\")(x1)\nx1 = tf.keras.layers.Dropout(0.5, name=\"dropout_head\")(x1)\nmodel_out = tf.keras.layers.Dense(4, activation='softmax', name=\"predictions_head\")(x1)\n\nmodel_densenet = Model(inputs=pretrained_densenet.input, outputs=model_out)\nmodel_densenet.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),loss=categorical_smooth_loss,metrics=['accuracy'])\n\nmodel_densenet.summary()","166dce9f":"history_densenet = model_densenet.fit_generator(training_set,\n                                                validation_data=testing_set,\n                                                callbacks=[lr_reduce, es_callback],\n                                                epochs=10)","22058bb7":"#Best model","66442372":"pretrained_densenet = tf.keras.applications.DenseNet201(input_shape=(image_size, image_size, 3), weights='imagenet', include_top=False)\n\nfor layer in pretrained_densenet.layers:\n  layer.trainable = False\n\nx1 = pretrained_densenet.output\nx1 = tf.keras.layers.AveragePooling2D(name=\"averagepooling2d_head\")(x1)\nx1 = tf.keras.layers.Flatten(name=\"flatten_head\")(x1)\nx1 = tf.keras.layers.Dense(64, activation=\"relu\", name=\"dense_head\")(x1)\nx1 = tf.keras.layers.Dropout(0.5, name=\"dropout_head\")(x1)\nmodel_out = tf.keras.layers.Dense(4, activation='softmax', name=\"predictions_head\")(x1)\n\nmodel_densenet = Model(inputs=pretrained_densenet.input, outputs=model_out)\nmodel_densenet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=categorical_smooth_loss,metrics=['accuracy'])\n\nmodel_densenet.summary()","606cca85":"plot_model(model_densenet, show_shapes=True, to_file='model_densenet.png')","ec056b98":"history_densenet = model_densenet.fit_generator(training_set,\n                                                validation_data=testing_set,\n                                                callbacks=[lr_reduce, es_callback],\n                                                epochs=30)","3cdea135":"display_training_curves(history_densenet.history['loss'], history_densenet.history['val_loss'], 'loss', 211)\ndisplay_training_curves(history_densenet.history['accuracy'], history_densenet.history['val_accuracy'], 'accuracy', 212)","79e5c422":"last_conv_layer_name = \"conv5_block32_concat\"\nclassifier_layer_names = [\n    \"bn\",\n    \"relu\",\n    \"averagepooling2d_head\",\n    \"flatten_head\",\n    \"dense_head\",\n    \"dropout_head\",\n    \"predictions_head\"\n]","26366fd2":"# test image\nfile_path =  '\/kaggle\/input\/diseasecotton\/data\/test\/fresh cotton plant\/dsd (405).jpg'\ntest_image = cv2.imread(file_path)\ntest_image = cv2.resize(test_image, (224,224),interpolation=cv2.INTER_NEAREST)\nplt.imshow(test_image)\ntest_image = np.expand_dims(test_image,axis=0)","8c8937ff":"heatmap, top_index = make_gradcam_heatmap(test_image, model_densenet, last_conv_layer_name, classifier_layer_names)\nprint(\"predicted as\", labels[top_index])","5c742b22":"plt.matshow(heatmap)\nplt.show()","62129952":"plt.figure(figsize=(15,10))\nfor i in range(12):\n    plt.subplot(3, 4, i + 1)\n    plt.axis('off')\n    heatmap, top_index = make_gradcam_heatmap(np.expand_dims(sample_data[i], axis=0), model_densenet, last_conv_layer_name, classifier_layer_names)\n    img = np.uint8(255 * sample_data[i])\n    s_img = superimposed_img(img, heatmap)\n    plt.imshow(s_img)\n    plt.title(labels[np.argmax(sample_label[i])] + \" pred as: \" + labels[top_index], fontsize=8)","59329343":"Best Model= InceptionV3; Accuracy=97%+.......\nVisit this link:https:\/\/www.kaggle.com\/sohelranaccselab\/cotton-disease-classification-part-1?scriptVersionId=43108908","e1dc40b9":"The EfficientNetB7 did not work well for us","60317d1f":"\/kaggle\/input\/diseasecotton\/data\/test\/fresh cotton plant\/dsd (405).jpg\n\/kaggle\/input\/diseasecotton\/data\/test\/fresh cotton leaf\/d (396).jpg\n\/kaggle\/input\/diseasecotton\/data\/test\/fresh cotton leaf\/d (366).jpg"}}