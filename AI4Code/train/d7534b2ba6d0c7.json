{"cell_type":{"404ccfbf":"code","a43f16f5":"code","af34597d":"code","850291cb":"code","a5603761":"code","48f08653":"code","0ee06d11":"code","799f7c46":"code","911ca43a":"code","fc729b86":"code","efd8e008":"code","85c512e6":"code","6ad91f27":"code","3054739a":"code","f3ba70f6":"code","be734207":"code","f0c00360":"code","5860143f":"code","cab68fc0":"code","29b1debd":"code","c50c6abc":"code","19f3bced":"code","67c6cefb":"code","03f14e93":"code","1268220b":"code","4e46d4d0":"code","3efd76d2":"code","fc2b5e09":"code","219589e9":"code","1d24942e":"code","f4a80b99":"markdown","3fe029b0":"markdown","6cb5e139":"markdown","5aa52bab":"markdown","efe0a7e7":"markdown","2f47cb83":"markdown","7813dc95":"markdown","e6bc7362":"markdown","e35e6ecc":"markdown","8fe25f7b":"markdown","d333758f":"markdown","0e9260ee":"markdown","b0a3a9a0":"markdown","27cdb3ad":"markdown","23e299f0":"markdown","f964ed7b":"markdown","343b9297":"markdown","ec1059fa":"markdown","5662765f":"markdown","ddca245f":"markdown","223e050d":"markdown","156623cc":"markdown","f7bc2204":"markdown","6fd683db":"markdown"},"source":{"404ccfbf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a43f16f5":"from sklearn import tree","af34597d":"D = pd.read_table('\/kaggle\/input\/chapter4\/Table4-2.csv', sep='\\s+,\\s+', index_col=0)\nD","850291cb":"features = D.columns[:-1]\ntarget = D.columns[-1]\n\nX = D.loc[:, features]\nY = D.loc[:, target]\n\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, Y)\n\ntree.plot_tree(clf, feature_names=features, class_names=['spam', 'ham'], impurity=False, filled=True);","a5603761":"xs = np.linspace(0.001, 1, 100)\nys = -np.log2(xs)\nplt.plot(xs, ys);","48f08653":"p_aces = 9\/12\np_jacks = 3\/12\n\nH_x = -(p_aces * np.log2(p_aces) + p_jacks * np.log2(p_jacks))\n\nprint(\"H(x) = {:.2} bits\".format(H_x))","0ee06d11":"# We could also write our code to be a bit more general, so that we can easily process a set of arbitrary size. \ndef entropy(xs):\n    # This calculation is a truer representation of the general equation for calculating entropy; the sum operator becomes a for loop\n    return np.sum([-x * np.log2(x) for x in xs])\n\n\nH_x = entropy([9\/12, 3\/12])\n\nprint(\"H(x) = {:.2} bits\".format(H_x))","799f7c46":"# Let's also make a version that prints out each term\ndef verbose_entropy(xs):\n    terms = [-x * np.log2(x) for x in xs]\n    ans = entropy(xs)\n    \n    print(\"x = \" + \", \".join([\"{:.2}\".format(x) for x in xs]))\n    print()\n    print(\"H(x) = \" + \" + \".join([\"-{:.2} * {:.2}\".format(x, y) for x, y in zip(xs, np.log2(xs))]))\n    print(\"H(x) = \" + \" + \".join([\"{:.2}\".format(x) for x in terms]))\n    print(\"H(x) = {:.2} bits\".format(ans))\n    \n    return ans\n\nH_x = verbose_entropy([9\/12, 3\/12])","911ca43a":"print(\"H(x) = {:.2} bits\".format(entropy([1\/52 for _ in range(52)])))","fc729b86":"print(\"H(x) = {:.2} bits\".format(entropy([13\/52 for _ in range(4)]))) # because there are 4 suits and so 52\/4=13 cards of each suit","efd8e008":"verbose_entropy([13\/52 for _ in range(4)])","85c512e6":"def H(dataset, target):\n        N = len(dataset)\n        levels = np.unique(dataset[target])\n        probs = [len(dataset.loc[dataset[target] == c]) \/ N for c in levels]\n        \n        return entropy(probs)\n    \ndef rem(dataset, feature, target):\n        N = len(dataset)\n        levels = np.unique(dataset[feature])\n        \n        ans = 0\n        for c in levels:\n            dataset_c = dataset.loc[dataset[feature] == c]\n            ans += len(dataset_c) \/ N * H(dataset_c, target)\n            \n        return ans\n            \ndef information_gain(dataset, feature, target):\n    return H(dataset, target) - rem(dataset, feature, target)\n    ","6ad91f27":"for x in features:\n    print(f\"Feature: {x}\")\n    print(f\"  H({target}, D): {H(D, target)}\")\n    print(f\"  rem({x}, D): {rem(D, x, target)}\")\n    print(f\"  IG({x}, D): {information_gain(D, x, target)}\")\n    print()","3054739a":"D","f3ba70f6":"ig_values = {x: information_gain(D, x, target) for x in ['suspicious words', 'unknown sender', 'contains images']}\nig_values","be734207":"max(ig_values, key=ig_values.get)","f0c00360":"D_suspicious_words_true = D.loc[D['suspicious words'] == True, :]\nD_suspicious_words_true","5860143f":"ig_values = {x: information_gain(D_suspicious_words_true, x, target) for x in ['unknown sender', 'contains images']}\nig_values","cab68fc0":"D_suspicious_words_false = D.loc[D['suspicious words'] == False, :]\nD_suspicious_words_false","29b1debd":"ig_values = {x: information_gain(D_suspicious_words_false, x, target) for x in ['unknown sender', 'contains images']}\nig_values","c50c6abc":"D = pd.read_table('\/kaggle\/input\/chapter4\/Table4-3.csv', sep='\\s+,\\s+', index_col=0)\nD","19f3bced":"D.describe()","67c6cefb":"# due to limitations in scikit-learn, we need to transform our data into a binary encoding... let's see what this will look like\npd.get_dummies(D[['Stream', 'Slope', 'Elevation']])","03f14e93":"features = ['Stream', 'Slope', 'Elevation']\ntarget = 'Vegetation'                         \n\nX = pd.get_dummies(D[features])     # note we're not using .loc; when we're reading data we can just use the [] operator\nY = D[target]                       # however, when modifying a data frame, we want to use the .loc method\n\nclf = tree.DecisionTreeClassifier(criterion='entropy')\nclf = clf.fit(X, Y)","1268220b":"plt.figure(figsize=(10,15))\ntree.plot_tree(clf, feature_names = X.columns, class_names=['chapparal', 'conifer', 'riparian'], filled=True);","4e46d4d0":"D","3efd76d2":"features = ['Stream', 'Slope', 'Elevation']\ntarget = 'Vegetation'                         \n\nX = pd.get_dummies(D[features])     # note we're not using .loc; when we're reading data we can just use the [] operator\nY = D[target]                       # however, when modifying a data frame, we want to use the .loc method\n\nclf = tree.DecisionTreeClassifier(criterion='gini')\nclf = clf.fit(X, Y)","fc2b5e09":"plt.figure(figsize=(10,15))\ntree.plot_tree(clf, feature_names = X.columns, class_names=['chapparal', 'conifer', 'riparian'], filled=True);","219589e9":"new_data = pd.DataFrame(data={'Stream': [True, False], 'Slope': ['flat', 'moderate'], 'Elevation': ['highest', 'low']})\nx = pd.get_dummies(D.iloc[:, :-1].append(new_data, ignore_index=True))\nx","1d24942e":"clf.predict(x)","f4a80b99":"## Information Gain\n\nWhat would really make our life easy is if we could identify the feature that provides the most \"information\", in the sense that if we were to split the dataset using that feature, it would produce the two branches with the lowest possible entropy of all possible branchings at that point.\n\n- The **information gain** of a feature can be understood as a measure of the reduction in the overall entropy of a prediction task by testing on that feature.","3fe029b0":"This dataset represents a set of topological features for a map. We would like to use this data to figure out what sort of vegetation we might find based on a set of readings for Stream, Slope, and Elevation.\n\nLooking at this new dataset, there does not appear to be a clean separation that we can identify at a glance... but that's okay, we can use a `DecisionTreeClassifier`","6cb5e139":"## Entropy\n\nClaude Shannon's entropy model defines a computational measure of the impurity of the elements of a set.\n- One way to understand the entropy of a set is to think in terms of the uncertainty associated with guessing the result if you were to make a random selection from the list\n- Another way is to think of entropy as the amount of heterogeneity in a set\n\nEntropy is related to the probability of an outcome\n- A high probability of occurrance is associated with a low entropy\n- A low probability of occurance is associated with a high probability\n\nWe can model the relationship between entropy and probability using the negative $\\log$ function.\n- We will actually use $\\log_2$ because the idea comes from information theory where bits are the unit of measurement.","5aa52bab":"As an example, consider our spam\/ham dataset","efe0a7e7":"Consider the following dataset:","2f47cb83":"To calculate the information gain of a feature, $d$, given a dataset, $\\mathcal{D}$, we need to solve the following equation:\n\n$$IG(d, \\mathcal{D}) = H(t, \\mathcal{D}) - rem(d, \\mathcal{D})$$\n\nwhere\n\n$$rem(d, \\mathcal{D}) = \\sum_{l \\in levels(d)} \\frac{|\\mathcal{D}_{d=l}|}{|\\mathcal{D}|} H(t, \\mathcal{D_{d=l}})$$\n\nand where\n\n$$H(t, \\mathcal{D}) = - \\sum_{l \\in levels(t)} p(t=l) \\log_2 \\left( p(t=l) \\right)$$","7813dc95":"At this point, we could implement the ID3 algorithm ourself...\n\n... or we can use scikit learn and explore the result of another dataset.","e6bc7362":"The ID3 algorithm attempts to automatically create the shallowest decision tree possible by recursively calculating the information gain of each feature. \n- Begin by choosing the best descriptive feature to test using information gain\n- Add a root node labeled with the selected test feature\n- Partition the dataset using the test\n- Add a branch from the parent to each partition\n- Repeat the process for each child node, but exclude the test feature from the dataset","e35e6ecc":"Having created a root node, we then need to create one branch for each for each level. In this case, there are two: true and false.","8fe25f7b":"To create the first node, we want to find out which feature has the highest information gain.","d333758f":"When we branch on true, we see two things:\n1. all the remaining instances have the same level: spam\n2. we end up with an information gain of 0 for the remaining features \n\nThis indicates that we're at a leaf node, and the label of that leaf should be `spam`","0e9260ee":"As with entropy, we can write a function to calculate this for us.","b0a3a9a0":"From the data, we would like to determine how to classify whether or not an email is spam. One approach is to ask questions about each feature - in this case, is feature X true or false? \n- We might notice that `suspicious words` perfectly splits the data, and so we decide to ask about that feature first.","27cdb3ad":"What about the entropy of a deck of cards where we only care about the suit and not the face?","23e299f0":"When we branch on false, we see a similar result, except this in case we create a leaf node with the label `ham`\n\nThus, we end up with the same tree that was displayed at the beginning of our discussion!","f964ed7b":"We can also choose to build our tree using the gini coefficient instead of the information gain. This will produce a slightly different tree.","343b9297":"Because each value in a set has a different probability of occurring, we want to account for those differences when calculating the total entropy of a set. This leads us to the following definition for entropy:\n\n$$H(x) = - \\sum_{i=1}^{n} \\left( p(x=i) \\log_2 p(x=i) \\right)$$\n\nThe entropy of a set, denoted with $H(x)$ is measured in bits.","ec1059fa":"A closer look at the data reveals that we're not dealing with straight binary data. This will require some special treatment to get things working with scikit learn (and most ML libraries)\n- Fun fact, the scikit-learn Decision Tree algorithm does not support categorical features.","5662765f":"We will use the notation $H(d, \\mathcal{D})$ when we are talking about the entropy of a feature, d, within the dataset, D.","ddca245f":"## The ID3 Algorithm","223e050d":"# Decision Trees\n\nScikit Learn Reference: https:\/\/scikit-learn.org\/stable\/modules\/tree.html\n\n\nA decision tree consists of\n- a root node\n- interior nodes\n- leaf nodes\n\nEach of the non-leaf nodes in the tree specifies a test to be carried out on one of the query's descriptive features.\n\nEach of the leaf nodes specifies a predicted classification for the query.","156623cc":"What would the entropy be if we had 12 cards, with 9 Ace of Spades and 3 Jack of Hearts?","f7bc2204":"What if we wanted to calculate the entropy of a deck of 52 standard playing cards?","6fd683db":"In larger datasets, we cannot do this by eye, and so we need a systematic way to build up this tree.\n- Entropy is the key"}}