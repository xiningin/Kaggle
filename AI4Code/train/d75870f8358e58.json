{"cell_type":{"3e84496f":"code","df4cd177":"code","6dd2486b":"code","81f52e45":"code","b1f2f353":"code","434b1342":"code","b28939e1":"code","a3b40986":"code","07952fc7":"code","f917e176":"code","bbeb1572":"code","34768947":"code","6f7d62f1":"code","c28f3944":"code","c68980b9":"code","b8667dc3":"code","c4721927":"code","36502516":"code","8fbe10ae":"code","e5da2b32":"code","44c58237":"code","080c4cf1":"code","efc6d02f":"code","052ef20d":"code","46df772a":"code","d1a26c5c":"code","e529cc66":"code","7f928358":"code","081915c7":"code","9fc25d32":"code","69c3e5fe":"code","06b2b4fb":"code","edd29d0a":"code","83aec79e":"code","dd136f50":"code","a614d467":"code","8a67a754":"code","5857b3b5":"code","5dcac660":"code","38a9f2b1":"code","141e255b":"code","9aa59081":"code","35c19958":"code","e1591ece":"code","fb683bb2":"code","46694f34":"code","7d146abf":"code","878d513c":"code","086cfcab":"code","5e761369":"code","fe090fe7":"code","6797ed45":"code","3413f6ba":"code","c94c2520":"code","b3b917ae":"code","daa76126":"code","283a8bf6":"code","0cfe4308":"code","2b56ff0a":"code","9cd5494e":"code","7360b408":"code","2be9f8e1":"code","09911265":"code","b21c32c8":"code","9e678191":"code","2d9c1e78":"code","b80e21ef":"code","4ba358c1":"code","e3eaead6":"code","d505cf39":"code","6fb79700":"code","2dd072b9":"code","94a39e02":"code","0d94b513":"code","e0300637":"code","c5441b9b":"code","1eb164b6":"code","25dd9080":"code","dba68615":"code","c1cf3839":"code","1ece739a":"code","5573e7b0":"code","712c0183":"code","e3be4c76":"code","38a57c8e":"code","96d2fbc4":"code","ef13dffd":"code","a65af65f":"code","fd05d595":"code","57b9e5b2":"code","ecba2371":"code","2e1b4694":"code","fb1a855a":"code","74e138ab":"code","40e0b86d":"code","2b6d8cd1":"code","516a5dfb":"code","af0edb7f":"code","28647286":"code","5d371a32":"code","70ac8106":"code","0d50f995":"code","4baaaa01":"code","44853ace":"code","87243370":"code","10a4d8f7":"code","8c1947ea":"code","ed3a43c7":"code","ed32d4f3":"code","7a1f7393":"code","28a7f7be":"code","027c64a3":"code","c1ab799a":"code","365d6f1e":"code","8fb53acc":"code","a087e1e5":"code","15ee33d7":"code","b30cc76e":"code","7c339a22":"code","d6f50920":"code","605c8881":"code","31462b34":"code","22be1cb0":"code","7d62a7df":"code","d184c850":"code","4069fd27":"code","ecfb5bd5":"code","48dd9740":"code","54f7afa6":"code","493f8352":"code","ca67b060":"code","83de04ef":"code","5e9f109e":"code","f41c60d7":"code","1037cbb8":"code","7788c46f":"code","636320ee":"code","9b318fa3":"code","b3508f91":"code","ccc9be80":"code","b4fc4d25":"code","50c49d9c":"code","6db185d4":"code","92216785":"code","3ad652f2":"code","2c0e0c65":"code","5a20abcb":"code","11963933":"code","3b2903a7":"code","ad32c542":"code","8ef17336":"code","a19af667":"code","d9187361":"code","2e1823ed":"code","9990cc3d":"code","05e6fea4":"code","746fffdd":"code","f6519fa5":"code","72b1f345":"code","3012aac4":"code","bd5f69fd":"code","d230fe89":"code","01b69987":"code","10686284":"code","fbeeede2":"code","2595650c":"code","4b334389":"code","eea0a17f":"code","07d31eb1":"code","97c5bc4f":"code","c2115451":"code","4ff45297":"code","5b45f578":"code","a0c0e7da":"code","bbdb668b":"code","2f18ad3f":"code","3e763495":"code","ba6461d9":"code","1bbbf30d":"code","a11c8207":"code","c50e5e64":"code","be5fe31e":"code","301b1fcb":"markdown","03aaef80":"markdown","c8c95a36":"markdown","be813d0f":"markdown","589e9407":"markdown","f46b26a4":"markdown","0bdeb0b6":"markdown","2fac913b":"markdown","6405896f":"markdown","58090abb":"markdown","53504f73":"markdown","0fb8d6b9":"markdown","c24d042d":"markdown","5a7d57ca":"markdown","ae9f0c97":"markdown","47ef0490":"markdown","43cbede9":"markdown","3ce89167":"markdown","8038e012":"markdown","08d834f4":"markdown","c411fa65":"markdown","00857a84":"markdown","083f3781":"markdown","d4d4a80c":"markdown","75c6fa01":"markdown","0727d0b4":"markdown","b0c4d8f0":"markdown","332aec94":"markdown","56e37267":"markdown","69ce6fb9":"markdown","cc02dc9a":"markdown","10f19060":"markdown","239877af":"markdown","bd737d44":"markdown","7e3f7aa7":"markdown","ca79d08c":"markdown","b9867ad6":"markdown","e747b6b6":"markdown","4ffb1a70":"markdown","12c17d51":"markdown","62675fa4":"markdown","bd331f89":"markdown","39251bf3":"markdown","dedf71dc":"markdown","1958d452":"markdown","c51cb86b":"markdown","90b8b05c":"markdown","4858d05d":"markdown","626abdf2":"markdown","64a38b32":"markdown","c48b5ab3":"markdown","f2831fd7":"markdown","91b480e0":"markdown","d631233b":"markdown","66f001f0":"markdown","24366140":"markdown","5a817a5e":"markdown","59715551":"markdown","43c9f251":"markdown","cbe18289":"markdown","bb8d017a":"markdown","4842464f":"markdown","ed26c3e2":"markdown","8f5805f0":"markdown","8a34bce8":"markdown","b189d24f":"markdown","7ed8acd3":"markdown","0b3033f0":"markdown","cd64429c":"markdown","b6d295b6":"markdown","06a21791":"markdown","5df78d0d":"markdown","251d0787":"markdown","ad34e479":"markdown","d2e52c67":"markdown","4c000437":"markdown","efd1bad0":"markdown","f862ccd0":"markdown","6d0dcc3f":"markdown","e93c5525":"markdown","8b7ff6c2":"markdown","7a82dc60":"markdown"},"source":{"3e84496f":"import pandas as pd\nimport numpy as np\n\nimport statsmodels.api as sm\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import metrics\nimport seaborn as sns\npd.set_option(\"display.max_columns\",None)\npd.set_option(\"display.max_rows\",None)\nfrom scipy.stats import shapiro,mannwhitneyu,chi2_contingency\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz\nimport imblearn\nimport lightgbm\nimport hyperopt\n\nimport warnings \nwarnings.filterwarnings('ignore')","df4cd177":"df = pd.read_csv('https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00468\/online_shoppers_intention.csv')","6dd2486b":"df.head()","81f52e45":"df.shape","b1f2f353":"df.info()","434b1342":"df.isnull().sum()","b28939e1":"df.describe()","a3b40986":"df.describe(include=['object','bool'])","07952fc7":"df.skew()","f917e176":"df.kurtosis()","bbeb1572":"num_col=['Administrative_Duration',\n       'Informational_Duration', 'ProductRelated_Duration',\n       'BounceRates', 'ExitRates', 'PageValues']","34768947":"plt.figure(figsize=(30,10))\ndf.boxplot()","6f7d62f1":"df.hist(column=num_col,figsize=(20,20))","c28f3944":"plt.figure(figsize=(10,10))\ndf['Revenue'].value_counts().plot(kind='pie',autopct='%1.1f', textprops={'fontsize': 15},startangle=90,explode =(0.1,0),colors=['slategray','cornflowerblue'])\nplt.title('Revenue', fontsize = 18)\nplt.ylabel('')","c68980b9":"plt.title('Number of Customers adding Revenue')\nsns.countplot(df['Revenue'])","b8667dc3":"column1l=['Administrative','Informational','ProductRelated','SpecialDay','OperatingSystems','Browser','Region','TrafficType','Month','VisitorType','Weekend']    \nplt.figure(figsize=(30,30))\nplot_number = 0\nfor i in column1l:\n    plot_number = plot_number + 1\n    ax = plt.subplot(6, 2, plot_number,adjustable='datalim')\n    sns.countplot(df[i],hue=df['Revenue'])\n    ax.set_title('Customers adding Revenue based on '+ i,fontdict=None)\n    plt.tight_layout()","c4721927":"pip install bubbly","36502516":"from plotly.offline import init_notebook_mode, iplot\nfrom bubbly.bubbly import bubbleplot \n\ndf['Revenue']=df['Revenue'].astype('object')\nfigure = bubbleplot(dataset=df, x_column='BounceRates', y_column='ExitRates', z_column='ProductRelated', size_column='ProductRelated',color_column='Revenue',  \n    bubble_column='Revenue',x_title=\"BounceRates\", y_title=\"ExitRates\",z_title=\"ProductRelated\", title='Bubble Plot', scale_bubble=3, height=650)\n\niplot(figure, config={'scrollzoom': True})","8fbe10ae":"figure = bubbleplot(dataset=df, x_column='Administrative_Duration', y_column='Informational_Duration', z_column='ProductRelated_Duration',\n    bubble_column='ProductRelated', size_column='ProductRelated',color_column='Revenue',  \n    x_title=\"Administrative_Duration\", y_title=\"Informational_Duration\",z_title=\"ProductRelated_Duration\", title='Bubble Plot', scale_bubble=3, height=650)\n\niplot(figure, config={'scrollzoom': True})","e5da2b32":"df2=df.copy()","44c58237":"cat_cols=['Administrative','Informational','ProductRelated','Month','OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType',\n       'Weekend', 'SpecialDay']","080c4cf1":"#check weather Revenue is influenced by administrative column \n#H0=proportion of revenue accross all the administrative category is same\n#H1=proportion of revenue at least in two administrative category is different\nscol=[]\nspval=[]\nss=[]\nfor n in cat_cols:\n    scol.append(n)\n    cp=chi2_contingency(pd.crosstab(df2[n],df2['Revenue']))[1]\n    spval.append(round(cp,4))\n    if (cp<0.05):\n        #rejects Null\n        ss.append('*')#signficant\n    else:#Accepts Null\n        ss.append('**')#not significant","efc6d02f":"pd.DataFrame({'Feature':scol,'P-Value':spval,'Significance':ss})","052ef20d":"numerical_columns=['Administrative_Duration','Informational_Duration','ProductRelated_Duration','BounceRates', 'ExitRates', 'PageValues']","46df772a":"from scipy.stats import levene\n\n#print('Two-Sample-T-Test of ','\\n')\ntcol=[]\ntpval=[]\nts=[]\nfor n in numerical_columns:\n    tcol.append(n)\n    #splitting into 2 groups(Revenue=True, Revenue=False)\n    g1=df2[n][df['Revenue']==False]\n    g2=df2[n][df['Revenue']==True]\n    #Test for normality(Shapiro Test)\n    #H0:Data is normal\n    #H0:Data is not normal\n    # if p<0.05---reject Null\n    for b in [g1]:\n        s,p=shapiro(b)\n    for c in [g2]:\n        s1,p1=shapiro(c)\n    if (p>0.05 or p1>0.05):\n        w,lp=levene(g1,g2)\n    #If data dosen't pass normality or variance test, we do non-parametric Test(Mann Whitney U Test)\n    if (p<=0.05 or p1<=0.05 or lp<=0.05):\n        ms,mp=mannwhitneyu(g1,g2)\n        #print('Mannwhitneyu P-Value: ',mp,'\\n')\n        tpval.append(round(mp,4))\n        if (mp<0.05):\n            ts.append('*')       #significat\n        else:\n            ts.append('**')      #not significant","d1a26c5c":"pd.DataFrame({'Feature':tcol,'P-Value':tpval,'Significant':ts})","e529cc66":"plt.figure(figsize=(62,20))\ndf2.boxplot()","7f928358":"df2.info()","081915c7":"num_col=['Administrative_Duration',\n       'Informational_Duration', 'ProductRelated_Duration',\n       'BounceRates', 'ExitRates', 'PageValues']","9fc25d32":"# identify outliers with standard deviation\nfrom numpy.random import seed\nfrom numpy.random import randn\nfrom numpy import mean\nfrom numpy import std\nout_per=[]\nfor i in num_col:\n    data_mean, data_std = mean(df2[i]), std(df2[i])\n# identify outliers\n    cut_off = data_std * 3\n    lower, upper = data_mean - cut_off, data_mean + cut_off\n    print(i,': \\n')\n# identify outliers\n    outliers = [x for x in df2[i] if x < lower or x > upper]\n    \n    num_out=len(outliers)\n    print('Identified outliers: %d' % num_out)\n    outliers_removed = [x for x in df2[i] if x >= lower and x <= upper]\n    num_nout=len(outliers_removed)\n    print('Non-outlier observations: %d' % num_nout)\n    outlier_percent=(num_out\/(num_out+num_nout))*100\n    print('Percent of outlers:',outlier_percent ,'\\n')\n    out_per.append(outlier_percent)","69c3e5fe":"Outliers=pd.DataFrame({'Feature':num_col,'% Of Outliers':out_per})\noutlier_sorted=Outliers.sort_values('% Of Outliers',ascending=False)\noutlier_sorted","06b2b4fb":"plt.figure(figsize=(8,5))\nsns.barplot(y=outlier_sorted['Feature'],x=outlier_sorted['% Of Outliers'],palette='GnBu_d')\nplt.title('Percent of Outliers by columns')\nplt.ylabel('Column Name')\nfor i, v in enumerate(list(outlier_sorted['% Of Outliers'])):\n    plt.text(v,i-(-0.1),round(list(outlier_sorted['% Of Outliers'])[i],2),fontsize=8)\n    \n    #plt.savefig('Outliers.jpeg',bbox_inches='tight',dpi=150)","edd29d0a":"df_copy=df2.copy()\ndf_copy.head()","83aec79e":"\ndf_copy.hist(column=num_col,figsize=(20,20))","dd136f50":"df_copy['Administrative_Duration']=1\/(df['Administrative_Duration']+1)\ndf_copy['Informational_Duration'],i = st.boxcox(df['Informational_Duration']+1)\ndf_copy['ProductRelated_Duration'],pd = st.boxcox(df['ProductRelated_Duration']+1)\ndf_copy['BounceRates']= df['BounceRates']**0.2 \ndf_copy['ExitRates']=df['ExitRates']**0.2 \ndf_copy['PageValues'],p = st.boxcox(df['PageValues']+1)","a614d467":"df_copy.hist(column=num_col,figsize=(20,20))","8a67a754":"plt.figure(figsize=(30,10))\ndf_copy.boxplot()","5857b3b5":"df_copy.head()","5dcac660":"df2.head()","38a9f2b1":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ndf_copy['Weekend'] = le.fit_transform(df_copy['Weekend'])\ndf_copy['Revenue'] = le.fit_transform(df_copy['Revenue'])\ndf_copy.head()","141e255b":"df_copy['TrafficType'].value_counts()","9aa59081":"top_10_traffic = [x for x in df_copy['TrafficType'].value_counts().sort_values(ascending=False).head(10).index]\ntop_10_traffic","35c19958":"def one_hot_top_x(dataframe,variable,top_x_labels):\n    for label in top_x_labels:\n        df_copy[variable+'_'+str(label)] = np.where(df_copy[variable]==label,1,0)","e1591ece":"one_hot_top_x(df_copy,'TrafficType',top_10_traffic)\ndf_copy.head()","fb683bb2":"top_8_browser = [x for x in df_copy['Browser'].value_counts().sort_values(ascending=False).head(8).index]\ntop_8_browser","46694f34":"one_hot_top_x(df_copy,'Browser',top_8_browser)\ndf_copy.head()","7d146abf":"df_copy.Month.hist()","878d513c":"top_8_month = [x for x in df_copy['Month'].value_counts().sort_values(ascending=False).head(8).index]\ntop_8_month","086cfcab":"one_hot_top_x(df_copy,'Month',top_8_month)\ndf_copy.head()","5e761369":"top_5_os = [x for x in df_copy['OperatingSystems'].value_counts().sort_values(ascending=False).head(5).index]\ntop_5_os","fe090fe7":"one_hot_top_x(df_copy,'OperatingSystems',top_5_os)\ndf_copy.head()","6797ed45":"labels = [x for x in df_copy['VisitorType'].value_counts().sort_values(ascending=False).head().index]\nlabels","3413f6ba":"def one_hot_encode(dataframe,variable,labels):\n    for label in labels:\n        df_copy[variable+'_'+str(label)] = np.where(df_copy[variable]==label,1,0)\none_hot_encode(df_copy,'VisitorType',labels)","c94c2520":"df_copy.head()","b3b917ae":"df_vif=df_copy[['Administrative_Duration','Informational_Duration','ProductRelated_Duration','BounceRates','ExitRates','PageValues']]","daa76126":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)","283a8bf6":"X = df_vif","0cfe4308":"X.info()","2b56ff0a":"calc_vif(X)","9cd5494e":"df_copy.shape","7360b408":"df_copy.head()","2be9f8e1":"df_final = df_copy.drop(['Month','OperatingSystems','Browser','TrafficType'],axis=1)\ndf_final.head()","09911265":"df_final.drop('VisitorType',axis=1,inplace=True)","b21c32c8":"df_final.drop('Region',axis=1,inplace=True)","9e678191":"df_final.shape","2d9c1e78":"df_final.head()","b80e21ef":"X=df_final.drop(columns=['Revenue'],axis=1)\ny=df_final['Revenue']","4ba358c1":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nXs_fa=ss.fit_transform(X)","e3eaead6":"pip install factor_analyzer","d505cf39":"import pandas as pd\nfrom sklearn.datasets import load_iris\nfrom factor_analyzer import FactorAnalyzer\nimport matplotlib.pyplot as plt","6fb79700":"from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\nchi_square_value,p_value=calculate_bartlett_sphericity(Xs_fa)\nchi_square_value, p_value","2dd072b9":"from factor_analyzer.factor_analyzer import calculate_kmo\nkmo_all,kmo_model=calculate_kmo(Xs_fa)","94a39e02":"kmo_model","0d94b513":"X=df_final.drop(columns=['Revenue'],axis=1)\ny=df_final['Revenue']","e0300637":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nXs_pca=ss.fit_transform(X)","c5441b9b":"from sklearn.decomposition import PCA\npca = PCA()\nX_pca = pca.fit_transform(Xs_pca)","1eb164b6":"# Principal Components Weights (Eigenvectors)\ndf_pca_loadings = pd.DataFrame(pca.components_)\ndf_pca_loadings.head()","25dd9080":"pca.explained_variance_ratio_","dba68615":"Xc=Xs_pca.astype(float)\ncov_matrix = np.cov(X_pca.T)\nprint('Covariance Matrix \\n%s', cov_matrix)","c1cf3839":"eig_vals, eig_vecs = np.linalg.eig(cov_matrix)\nprint('Eigen Vectors \\n%s', eig_vecs)\nprint('\\n Eigen Values \\n%s', eig_vals)","1ece739a":"eigen_pairs = [(np.abs(eig_vals[i]), eig_vecs[ :, i]) for i in range(len(eig_vals))]","5573e7b0":"tot = sum(eig_vals)\nvar_exp = [( i \/tot ) * 100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\nprint(\"Cumulative Variance Explained\", cum_var_exp)","712c0183":"sns.set_style(style='whitegrid', rc=None)","e3be4c76":"plt.figure(figsize=(18 , 10))\n\nplt.plot(pca.explained_variance_ratio_,'*')\nplt.plot(pca.explained_variance_ratio_)\nplt.xticks(range(0,45,2))\nplt.xlabel('Principle Components')\nplt.ylabel('Eigen Values')\nplt.show()","38a57c8e":"plt.figure(figsize=(18 , 10))\nplt.bar(range(45), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\nplt.step(range(45), cum_var_exp, where='mid', label = 'Cumulative explained variance')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Number of Principal Components')\nplt.legend(loc = 'best')\nplt.tight_layout()\nplt.show()","96d2fbc4":"plt.figure(figsize=(20,20))\nax = sns.heatmap(pca.components_,\n                 cmap='YlGnBu',\n                 yticklabels=[ \"PCA\"+str(x) for x in range(1,pca.n_components_+1)],\n                 xticklabels=list(X.columns))\nax.set_aspect(\"equal\")\nplt.savefig('PCA Heatmap.jpeg',bbox_inches='tight',dpi=150)","ef13dffd":"pca2 = PCA(n_components=2)\nX_pca2 = pca2.fit_transform(Xs_pca)","a65af65f":"principalDf = pd.DataFrame(data = X_pca2\n             , columns = ['principal component 1', 'principal component 2'])","fd05d595":"finalDf = pd.concat([principalDf, df_final[['Revenue']]], axis = 1)","57b9e5b2":"X=df_final.drop(columns=['Revenue'],axis=1)","ecba2371":"from scipy.spatial import ConvexHull\n\ndef encircle(x,y, ax=None, **kw):\n    if not ax: ax=plt.gca()\n    p = np.c_[x,y]\n    hull = ConvexHull(p)\n    poly = plt.Polygon(p[hull.vertices,:], **kw)\n    ax.add_patch(poly)\n","2e1b4694":"fig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [0, 1]\ncolors = ['r', 'b']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['Revenue'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 120)\n    encircle(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , ec = color, fc=\"none\", linewidth=2.5)\n    #shading\n    encircle(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , ec = 'k', fc=color, alpha=0.05)\nax.legend(targets)\nax.grid()\nplt.savefig('PCA cluster.jpeg',bbox_inches='tight',dpi=150)","fb1a855a":"import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\n\npca = PCA(n_components=3)\npca.fit(Xs_pca)\nc=pd.DataFrame({})\nc['Revenue']=pd.Categorical(df_final['Revenue'])\nmy_color=c['Revenue'].astype(np.float)\n\nfig = plt.figure(figsize=(20, 10))\n \n# Store results of PCA in a data frame\nresult=pd.DataFrame(pca.transform(Xs_pca), columns=['PCA%i' % i for i in range(3)], index=df.index)\n \n# Plot initialisation\nfig = plt.figure(figsize=(20, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(result['PCA0'], result['PCA1'], result['PCA2'], c=my_color, cmap=\"Set2_r\", s=60)\n \n# make simple, bare axis lines through space:\nxAxisLine = ((min(result['PCA0']), max(result['PCA0'])), (0, 0), (0,0))\nax.plot(xAxisLine[0], xAxisLine[1], xAxisLine[2], 'r')\nyAxisLine = ((0, 0), (min(result['PCA1']), max(result['PCA1'])), (0,0))\nax.plot(yAxisLine[0], yAxisLine[1], yAxisLine[2], 'r')\nzAxisLine = ((0, 0), (0,0), (min(result['PCA2']), max(result['PCA2'])))\nax.plot(zAxisLine[0], zAxisLine[1], zAxisLine[2], 'r')\n \n# label the axes\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_zlabel(\"PC3\")\nax.set_title(\"PCA on the data set\")\n#plt.show()\nplt.savefig('PCA scatter 1.jpeg',bbox_inches='tight',dpi=150)","74e138ab":"def myplot(score,coeff,labels=None):\n    xs = score[:,0]\n    ys = score[:,1]\n    n = coeff.shape[0]\n    scalex = 1.0\/(xs.max() - xs.min())\n    scaley = 1.0\/(ys.max() - ys.min())\n    plt.scatter(xs * scalex,ys * scaley,s=5,c='y')\n    for i in range(n):\n        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'b',alpha = 0.5)\n        if labels is None:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'black', ha = 'center', va = 'center')\n        else:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'black', ha = 'center', va = 'center')\n \n    plt.xlabel(\"PC{}\".format(1))\n    plt.ylabel(\"PC{}\".format(2))\n    plt.grid()\nplt.figure(figsize=(20,15))\nmyplot(X_pca[:,0:2],np.transpose(pca.components_[0:2, :]),list(X.columns))\nplt.show()\n#plt.savefig('PCA Biplot.jpeg',bbox_inches='tight',dpi=150)","40e0b86d":"from numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as  plt\n \n\n \n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    for i in range(1,45):\n        steps = [('pca', PCA(n_components=i)), ('m', LogisticRegression())]\n        models[str(i)] = Pipeline(steps=steps)\n    return models\n \n# evaluate a given model using cross-validation\ndef evaluate_model(model):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores\n \n# define dataset\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nprint('>PC   Accuracy   Variance Error')\nfor name, model in models.items():\n    scores = evaluate_model(model)\n    results.append(scores)\n    names.append(name)\n    print('>%s     %.3f        (%.3f)' % (name, mean(scores), np.var(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(25,10))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.xticks(rotation=45)\nplt.xlabel('PCA Components')\nplt.ylabel('Accuracy Score')\nplt.show()\n#plt.savefig('PCA logistic.jpeg',bbox_inches='tight',dpi=100)","2b6d8cd1":"X=df_final.drop(columns=['Revenue'],axis=1)\ny=df_final['Revenue']","516a5dfb":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nXs_pca=ss.fit_transform(X)","af0edb7f":"pca = PCA(n_components=0.95)\nX_pca = pca.fit_transform(Xs_pca)","28647286":"X_pca.shape","5d371a32":"from sklearn.tree import DecisionTreeClassifier\ndt_pca=DecisionTreeClassifier(criterion='entropy',random_state=0)\n\nfrom sklearn.linear_model import LogisticRegression\nLR_pca=LogisticRegression()\n\nfrom sklearn.naive_bayes import GaussianNB\nnb_pca=GaussianNB()\n\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve\nfrom sklearn.ensemble import RandomForestClassifier\nrfc_pca=RandomForestClassifier(n_estimators=100,random_state=0)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_pca=KNeighborsClassifier()\n\nfrom sklearn.ensemble import AdaBoostClassifier\nada_pca=AdaBoostClassifier(random_state=0)\n\nimport lightgbm as lgb\nlgbm_pca=lgb.LGBMClassifier(random_state=0)\n\nfrom xgboost import XGBClassifier\nxgb_pca=XGBClassifier(random_state=0)\n\nfrom sklearn import svm\nsvc_pca=svm.SVC(random_state=0)","70ac8106":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\n\npca_rfc_tunned=RandomForestClassifier(n_estimators=100,random_state=0)\nparams={'n_estimators':sp_randint(1,100),\n        'max_features':sp_randint(1,33),\n        'max_depth': sp_randint(2,10),\n        'min_samples_split':sp_randint(2,20),\n        'min_samples_leaf':sp_randint(1,20),\n        'criterion':['gini','entropy']}\n\nrsearch_rfc_pca=RandomizedSearchCV(pca_rfc_tunned,params,cv=3,scoring='roc_auc',n_jobs=-1,random_state=0)\n\nrsearch_rfc_pca.fit(X_pca,y)","0d50f995":"rsearch_rfc_pca.best_params_","4baaaa01":"pca_rfc_tunned=RandomForestClassifier(**rsearch_rfc_pca.best_params_,random_state=0)","44853ace":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom scipy.stats import randint as sp_randint\n\npca_knn_tunned=KNeighborsClassifier()\n\nparams={'n_neighbors':sp_randint(1,50),'p':sp_randint(1,5)}\n\nrsearch_knn_pca=RandomizedSearchCV(pca_knn_tunned,params,cv=3,scoring='roc_auc',n_jobs=-1,random_state=0)\nrsearch_knn_pca.fit(X_pca,y)","87243370":"rsearch_knn_pca.best_params_","10a4d8f7":"pca_knn_tunned=KNeighborsClassifier(**rsearch_knn_pca.best_params_)","8c1947ea":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform \n\npca_lgbm_tunned=lgb.LGBMClassifier(random_state=0)\nparams={'n_estimators':sp_randint(5,200),\n       'max_depth': sp_randint(2,25),\n        'learning_rate':sp_uniform(0.001,0.05),\n        'num_leaves':sp_randint(2,60)\n       }\n\nrsearch_lgbm_pca=RandomizedSearchCV(pca_lgbm_tunned,param_distributions=params,cv=3,n_iter=200,n_jobs=-1,random_state=0)\n\nrsearch_lgbm_pca.fit(X_pca,y)","ed3a43c7":"rsearch_lgbm_pca.best_params_","ed32d4f3":"pca_lgbm_tunned=lgb.LGBMClassifier(**rsearch_lgbm_pca.best_params_,random_state=0)","7a1f7393":"XGB_tunned_pca=XGBClassifier(random_state=0)\nparams = {\n        'min_child_weight': sp_randint(1,20),\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': sp_randint(2,10)\n        }\n\nrsearch_xgb_pca=RandomizedSearchCV(XGB_tunned_pca,param_distributions=params,cv=3,n_iter=200,n_jobs=-1,random_state=0)\n\nrsearch_xgb_pca.fit(X_pca,y)","28a7f7be":"rsearch_xgb_pca.best_params_","027c64a3":"XGB_tunned_pca=XGBClassifier(**rsearch_xgb_pca.best_params_,random_state=0)","c1ab799a":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\nfrom sklearn import svm\nSVM_tunned_pca=svm.SVC(random_state=0)\nparams = {'C': [0.1, 1, 10, 100, 1000], \n          'degree': sp_randint(0,20),\n          'gamma': sp_uniform(0.1,0.5), \n          'kernel': ['rbf']} \n\nrsearch_svm_pca=RandomizedSearchCV(SVM_tunned_pca,param_distributions=params,cv=3,n_iter=200,n_jobs=-1,random_state=0)\n\nrsearch_svm_pca.fit(X_pca,y)","365d6f1e":"rsearch_svm_pca.best_params_","8fb53acc":"SVM_tunned_pca=svm.SVC(**rsearch_svm_pca.best_params_,random_state=0)","a087e1e5":"from sklearn.ensemble import VotingClassifier\n#hard_voting_pca=VotingClassifier(estimators=[('Logistic',LR),('Random Forest Tunned',rfc_tunned),('LGBM Tunned',lgbm_tunned)],voting='hard',flatten_transform=False)\n\nsoft_voting_pca=VotingClassifier(estimators=[('Logistic',LR_pca),('Random Forest Tunned',pca_rfc_tunned),('LGBM Tunned',pca_lgbm_tunned)],voting='soft')\n\nweighted_soft_pca=VotingClassifier(estimators=[('Logistic',LR_pca),('Random Forest Tunned',pca_rfc_tunned),('LGBM Tunned',pca_lgbm_tunned)],weights=[3,1,2],voting='soft')\n#weighted_hard_pca=VotingClassifier(estimators=[('Logistic',LR),('Random Forest Tunned',rfc_tunned),('LGBM Tunned',lgbm_tunned)],weights=[3,1,2],voting='hard',flatten_transform=False)","15ee33d7":"models_pca=[]\nmodels_pca.append(('Logistic',LR_pca))\nmodels_pca.append(('Decision Tree',dt_pca))\nmodels_pca.append(('Naive Bayes',nb_pca))\nmodels_pca.append(('Random Forest',rfc_pca))\nmodels_pca.append(('Random Forest Tuned',pca_rfc_tunned))\nmodels_pca.append(('KNN',knn_pca))\nmodels_pca.append(('KNN Tuned',pca_knn_tunned))\nmodels_pca.append(('Ada-Boost',ada_pca))\nmodels_pca.append(('LGBM',lgbm_pca))\nmodels_pca.append(('LGBM Tuned',pca_lgbm_tunned))\n#models_pca.append(('Hard Voting',hard_voting_pca))\n#models_pca.append(('Weighted Hard Voting',weighted_hard_pca))\nmodels_pca.append(('Soft Voting',soft_voting_pca))\nmodels_pca.append(('Weighted Soft Voting',weighted_soft_pca))\nmodels_pca.append(('XG Boost',xgb_pca))\nmodels_pca.append(('XG Boost Tuned',XGB_tunned_pca))\nmodels_pca.append(('SVM',svc_pca))\nmodels_pca.append(('SVM Tuned',SVM_tunned_pca))","b30cc76e":"from sklearn.model_selection import cross_val_score\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\nresults_pca=[]\nacc_score_pca=[]\nauc_score_pca=[]\nbias_pca=[]\nf1_score_pca=[]\nprecision_score_pca=[]\nrecall_score_pca=[]\nnames_pca=[]\nfor name,model in models_pca:\n    kfold=model_selection.KFold(shuffle=True,n_splits=10,random_state=0)\n    cv_results=model_selection.cross_val_score(model,X_pca,y,cv=kfold,scoring='roc_auc')\n    results_pca.append(cv_results)\n    bias_pca.append(np.var(cv_results,ddof=1))\n    auc_score_pca.append(np.mean(cv_results))\n    f1=model_selection.cross_val_score(model,X_pca,y,cv=kfold,scoring='f1_weighted')\n    f1_score_pca.append(np.mean(f1))\n    \n    acc=model_selection.cross_val_score(model,X_pca,y,cv=kfold,scoring='accuracy')\n    acc_score_pca.append(np.mean(acc))\n    \n    p=model_selection.cross_val_score(model,X_pca,y,cv=kfold,scoring='precision_weighted')\n    precision_score_pca.append(np.mean(p))\n    \n    r=model_selection.cross_val_score(model,X_pca,y,cv=kfold,scoring='recall_weighted')\n    recall_score_pca.append(np.mean(r))\n    \n    names_pca.append(name)\n\nresult_pca_df=pd.DataFrame({'Model':names_pca,\n                           'Accuracy Score':acc_score_pca,\n                            'ROC-AUC Score':auc_score_pca,\n                            'Variance Error':bias_pca,\n                            'F1 Score':f1_score_pca,\n                            'Precision Score':precision_score_pca,\n                            'Recall Score':recall_score_pca})","7c339a22":"result_pca_df","d6f50920":"fig=plt.figure(figsize=(20,8))\nfig.suptitle('Algorithemic Comparison')\nax=fig.add_subplot(111)\n\nplt.boxplot(results_pca)\nax.set_xticklabels(names_pca)\nplt.show()","605c8881":"import statsmodels.api as sm","31462b34":"from statsmodels.tools import add_constant\ndf_constant = add_constant(df_final)\ndf_constant.head()","22be1cb0":"X=df_final.drop(columns=['Revenue','ExitRates'],axis=1)\ny=df_final['Revenue']\nX=X.astype(float)\n\nXc=sm.add_constant(X)","7d62a7df":"model = sm.Logit(y,Xc)\nresult = model.fit()\nresult.summary()","d184c850":"cols = list(X.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE) ","4069fd27":"len(selected_features_BE)","ecfb5bd5":"result.summary()","48dd9740":"X=df_final[['Administrative', 'Informational', 'ProductRelated', 'BounceRates', 'PageValues', 'TrafficType_1', \n            'TrafficType_3', 'TrafficType_13', 'TrafficType_6', 'TrafficType_8', 'Browser_6', 'Month_May', 'Month_Nov', \n            'Month_Oct', 'Month_Sep', 'Month_Aug', 'Month_Jul', 'VisitorType_New_Visitor']]\nX.head()","54f7afa6":"import statsmodels.api as sm\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score,log_loss,f1_score","493f8352":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nXs=ss.fit_transform(X)","ca67b060":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier(criterion='entropy',random_state=0)\n\nfrom sklearn.linear_model import LogisticRegression\nLR=LogisticRegression()\n\nfrom sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\n\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve\nfrom sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(n_estimators=100,random_state=0)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier()\n\nfrom sklearn.ensemble import AdaBoostClassifier,BaggingClassifier,GradientBoostingClassifier\nada=AdaBoostClassifier(random_state=0)\ngb=GradientBoostingClassifier(random_state=0)\nbc=BaggingClassifier(random_state=0)\n\nimport lightgbm as lgb\nlgbm=lgb.LGBMClassifier(random_state=0)\n\nfrom xgboost import XGBClassifier\nxgb=XGBClassifier(random_state=0)\n\nfrom sklearn import svm\nsvc=svm.SVC(random_state=0)","83de04ef":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\n\nrfc_tunned=RandomForestClassifier(n_estimators=100,random_state=0)\nparams={'n_estimators':sp_randint(1,200),\n        'max_features':sp_randint(1,18),\n        'max_depth': sp_randint(2,20),\n        'min_samples_split':sp_randint(2,40),\n        'min_samples_leaf':sp_randint(1,30),\n        'criterion':['gini','entropy']}\n\nrsearch_rfc=RandomizedSearchCV(rfc_tunned,params,cv=3,scoring='roc_auc',n_jobs=-1,random_state=0)\n\nrsearch_rfc.fit(Xs,y)","5e9f109e":"rsearch_rfc.best_params_","f41c60d7":"rfc_tunned=RandomForestClassifier(**rsearch_rfc.best_params_,random_state=0)","1037cbb8":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom scipy.stats import randint as sp_randint\n\nknn_tunned=KNeighborsClassifier()\n\nparams={'n_neighbors':sp_randint(1,40),'p':sp_randint(1,10)}\n\nrsearch_knn=RandomizedSearchCV(knn_tunned,params,cv=3,scoring='roc_auc',n_jobs=-1,random_state=0)\nrsearch_knn.fit(Xs,y)","7788c46f":"rsearch_knn.best_params_","636320ee":"knn_tunned=KNeighborsClassifier(**rsearch_knn.best_params_)","9b318fa3":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform \n\nlgbm_tunned=lgb.LGBMClassifier(random_state=0)\nparams={'n_estimators':sp_randint(2,200),\n       'max_depth': sp_randint(2,30),\n        'learning_rate':sp_uniform(0.001,0.05),\n        'num_leaves':sp_randint(2,50)\n       }\n\nrsearch_lgbm=RandomizedSearchCV(lgbm_tunned,param_distributions=params,cv=3,n_iter=200,n_jobs=-1,random_state=0)\n\nrsearch_lgbm.fit(Xs,y)","b3508f91":"rsearch_lgbm.best_params_","ccc9be80":"lgbm_tunned=lgb.LGBMClassifier(**rsearch_lgbm.best_params_,random_state=0)","b4fc4d25":"XGB_tunned=XGBClassifier(random_state=0)\nparams = {\n        'min_child_weight': sp_randint(2,10),\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': sp_randint(2,10)\n        }\n\nrsearch_xgb=RandomizedSearchCV(XGB_tunned,param_distributions=params,cv=3,n_iter=200,n_jobs=-1,random_state=0)\n\nrsearch_xgb.fit(Xs,y)","50c49d9c":"rsearch_xgb.best_params_","6db185d4":"XGB_tunned=XGBClassifier(**rsearch_xgb.best_params_,random_state=0)","92216785":"from sklearn import svm\nSVM_tunned=svm.SVC(random_state=0)\nparams = {'C': [0.1, 1, 10, 100, 1000], \n          'degree': sp_randint(1,15),\n          'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n          'kernel': ['rbf']} \n\nrsearch_svm=RandomizedSearchCV(SVM_tunned,param_distributions=params,cv=3,n_iter=200,n_jobs=-1,random_state=0)\n\nrsearch_svm.fit(Xs,y)","3ad652f2":"rsearch_svm.best_params_","2c0e0c65":"SVM_tunned=svm.SVC(**rsearch_svm.best_params_,random_state=0)","5a20abcb":"from sklearn.ensemble import VotingClassifier\n\nsoft_voting=VotingClassifier(estimators=[('Logistic',LR),('Random Forest Tunned',rfc_tunned),('LGBM Tunned',lgbm_tunned)],voting='soft')\n\nweighted_soft=VotingClassifier(estimators=[('Logistic',LR),('Random Forest Tunned',rfc_tunned),('LGBM Tunned',lgbm_tunned)],weights=[3,1,2],voting='soft')","11963933":"models_a=[]\nmodels_a.append(('Logistic',LR))\nmodels_a.append(('Decision Tree',dt))\nmodels_a.append(('Naive Bayes',nb))\nmodels_a.append(('Random Forest',rfc))\nmodels_a.append(('Random Forest Tuned',rfc_tunned))\nmodels_a.append(('KNN',knn))\nmodels_a.append(('KNN Tuned',knn_tunned))\nmodels_a.append(('Bagging',bc))\nmodels_a.append(('Ada-Boost',ada))\nmodels_a.append(('Gradient Boost',gb))\nmodels_a.append(('LGBM',lgbm))\nmodels_a.append(('LGBM Tunned',lgbm_tunned))\nmodels_a.append(('Soft Voting',soft_voting))\nmodels_a.append(('Weighted Soft Voting',weighted_soft))\nmodels_a.append(('XG Boost',xgb))\nmodels_a.append(('XG Boost Tuned',XGB_tunned))\nmodels_a.append(('SVM',svc))\nmodels_a.append(('SVM Tuned',SVM_tunned))","3b2903a7":"from sklearn.model_selection import cross_val_score\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\nresults_a=[]\nacc_score_a=[]\nauc_score_a=[]\nbias_a=[]\nf1_score_a=[]\nprecision_score_a=[]\nrecall_score_a=[]\nnames_a=[]\nfor name,model in models_a:\n    kfold=model_selection.KFold(shuffle=True,n_splits=10,random_state=0)\n    cv_results=model_selection.cross_val_score(model,Xs,y,cv=kfold,scoring='roc_auc')\n    results_a.append(cv_results)\n    bias_a.append(np.var(cv_results,ddof=1))\n    auc_score_a.append(np.mean(cv_results))\n    \n    f1=model_selection.cross_val_score(model,Xs,y,cv=kfold,scoring='f1_weighted')\n    f1_score_a.append(np.mean(f1))\n    \n    acc=model_selection.cross_val_score(model,Xs,y,cv=kfold,scoring='accuracy')\n    acc_score_a.append(np.mean(acc))\n    \n    p=model_selection.cross_val_score(model,Xs,y,cv=kfold,scoring='precision_weighted')\n    precision_score_a.append(np.mean(p))\n    \n    r=model_selection.cross_val_score(model,Xs,y,cv=kfold,scoring='recall_weighted')\n    recall_score_a.append(np.mean(r))\n    \n    names_a.append(name)\n    \nresult_df=pd.DataFrame({'Model':names_a,\n                        'Accuracy Score':acc_score_a,\n                        'ROC-AUC Score':auc_score_a,\n                        'Variance Error':bias_a,\n                        'F1 Score':f1_score_a,\n                        'Precision Score':precision_score_a,\n                        'Recall Score':recall_score_a})","ad32c542":"result_df","8ef17336":"fig=plt.figure(figsize=(20,8))\nfig.suptitle('Algorithemic Comparison')\nax=fig.add_subplot(111)\nplt.boxplot(results_a)\nax.set_xticklabels(names_a)\nplt.show()","a19af667":"X=df_final[['Administrative', 'Informational', 'ProductRelated', 'BounceRates', 'PageValues', 'TrafficType_1', \n            'TrafficType_3', 'TrafficType_13', 'TrafficType_6', 'TrafficType_8', 'Browser_6', 'Month_May', 'Month_Nov', \n            'Month_Oct', 'Month_Sep', 'Month_Aug', 'Month_Jul', 'VisitorType_New_Visitor']]\nX.head()","d9187361":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)","2e1823ed":"print(X_train.shape,X_test.shape)","9990cc3d":"from imblearn.over_sampling import SMOTE\nsmote=SMOTE(sampling_strategy='minority',random_state=3)\n\nX_train_sm,y_train_sm=smote.fit_sample(X_train,y_train)","05e6fea4":"pd.Series(y_train).value_counts() #Target before smote","746fffdd":"pd.Series(y_train_sm).value_counts() #target After Smote","f6519fa5":"from sklearn.tree import DecisionTreeClassifier\ndt_sm=DecisionTreeClassifier(criterion='entropy',random_state=0)\n\nfrom sklearn.linear_model import LogisticRegression\nLR_sm=LogisticRegression()\n\nfrom sklearn.naive_bayes import GaussianNB\nnb_sm=GaussianNB()\n\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve\nfrom sklearn.ensemble import RandomForestClassifier\nrfc_sm=RandomForestClassifier(n_estimators=100,random_state=0)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_sm=KNeighborsClassifier(n_neighbors=6)\n\nfrom sklearn.ensemble import AdaBoostClassifier,BaggingClassifier,GradientBoostingClassifier\nada_sm=AdaBoostClassifier(random_state=0)\ngb_sm=GradientBoostingClassifier(random_state=0)\nbc_sm=BaggingClassifier(random_state=0)\n\nimport lightgbm as lgb\nlgbm_sm=lgb.LGBMClassifier(random_state=0)\n\nfrom xgboost import XGBClassifier\nxgb_sm=XGBClassifier(random_state=0)\n\nfrom sklearn import svm\nsvc_sm=svm.SVC(random_state=0,probability=True)","72b1f345":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\n\nsm_rfc_tunned=RandomForestClassifier(n_estimators=100,random_state=0)\nparams={'n_estimators':sp_randint(1,1200),\n        'max_features':sp_randint(1,18),\n        'max_depth': sp_randint(1,60),\n        'min_samples_split':sp_randint(2,40),\n        'min_samples_leaf':sp_randint(2,20),\n        'criterion':['gini','entropy']}\n\nrsearch_rfc_sm=RandomizedSearchCV(sm_rfc_tunned,params,cv=3,scoring='roc_auc',n_jobs=-1,random_state=0)\n\nrsearch_rfc_sm.fit(X_train_sm,y_train_sm)","3012aac4":"rsearch_rfc_sm.best_params_","bd5f69fd":"sm_rfc_tunned=RandomForestClassifier(**rsearch_rfc_sm.best_params_,random_state=0)","d230fe89":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom scipy.stats import randint as sp_randint\n\nsm_knn_tunned=KNeighborsClassifier()\n\nparams={'n_neighbors':sp_randint(1,150),'p':sp_randint(1,70)}\n\nrsearch_knn_sm=RandomizedSearchCV(sm_knn_tunned,params,cv=3,scoring='roc_auc',n_jobs=-1,random_state=0)\nrsearch_knn_sm.fit(X_train_sm,y_train_sm)","01b69987":"rsearch_knn_sm.best_params_","10686284":"sm_knn_tunned=KNeighborsClassifier(**rsearch_knn_sm.best_params_)","fbeeede2":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform \n\nsm_lgbm_tunned=lgb.LGBMClassifier(random_state=0)\nparams={'n_estimators':sp_randint(1,500),\n       'max_depth': sp_randint(2,20),\n        'learning_rate':sp_uniform(0.001,0.05),\n        'num_leaves':sp_randint(10,70)\n       }\n\nrsearch_lgbm_sm=RandomizedSearchCV(sm_lgbm_tunned,param_distributions=params,cv=3,n_iter=200,n_jobs=-1,random_state=0)\n\nrsearch_lgbm_sm.fit(X_train_sm,y_train_sm)","2595650c":"rsearch_lgbm_sm.best_params_","4b334389":"sm_lgbm_tunned=lgb.LGBMClassifier(**rsearch_lgbm_sm.best_params_,random_state=0)","eea0a17f":"XGB_tunned_sm=XGBClassifier(random_state=0)\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n\nrsearch_xgb_sm=RandomizedSearchCV(XGB_tunned_sm,param_distributions=params,cv=3,n_iter=200,n_jobs=-1,random_state=0)\n\nrsearch_xgb_sm.fit(X_train_sm,y_train_sm)","07d31eb1":"rsearch_xgb_sm.best_params_","97c5bc4f":"XGB_tunned_sm=XGBClassifier(**rsearch_xgb_sm.best_params_,random_state=0)","c2115451":"from sklearn import svm\nSVM_tunned_sm=svm.SVC(random_state=0,probability=True)\nparams = {'C': [0.1, 1, 10, 100, 1000], \n          'degree': [0, 1, 2, 3, 4, 5, 6],\n          'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n          'kernel': ['rbf']} \n\nrsearch_svm_sm=RandomizedSearchCV(SVM_tunned_sm,param_distributions=params,cv=3,n_iter=200,n_jobs=-1,random_state=0)\n\nrsearch_svm_sm.fit(X_train_sm,y_train_sm)","4ff45297":"rsearch_svm_sm.best_params_","5b45f578":"SVM_tunned_sm=svm.SVC(**rsearch_svm_sm.best_params_,random_state=0,probability=True)","a0c0e7da":"from sklearn.ensemble import VotingClassifier\n\nsoft_voting_sm=VotingClassifier(estimators=[('Bagging',bc_sm),('LGBM Tunned ',lgbm_sm),('Random Forest Tunned',rfc_sm)],voting='soft')\n\nweighted_soft_sm=VotingClassifier(estimators=[('Bagging',bc_sm),('LGBM Tunned ',lgbm_sm),('Random Forest Tunned',rfc_sm)],weights=[3,1,2],voting='soft')\n","bbdb668b":"models_sm=[]\nmodels_sm.append(('Logistic',LR_sm))\nmodels_sm.append(('Decision Tree',dt_sm))\nmodels_sm.append(('Naive Bayes',nb_sm))\nmodels_sm.append(('Random Forest',rfc_sm))\nmodels_sm.append(('Random Forest Tuned',sm_rfc_tunned))\nmodels_sm.append(('KNN',knn_sm))\nmodels_sm.append(('KNN Tuned',sm_knn_tunned))\nmodels_sm.append(('Bagging',bc_sm))\nmodels_sm.append(('Ada-Boost',ada_sm))\nmodels_sm.append(('Gradient Boost',gb_sm))\nmodels_sm.append(('LGBM',lgbm_sm))\nmodels_sm.append(('LGBM Tuned',sm_lgbm_tunned))\nmodels_sm.append(('Soft Voting',soft_voting_sm))\nmodels_sm.append(('Weighted Soft Voting',weighted_soft_sm))\nmodels_sm.append(('XGB',xgb_sm))\nmodels_sm.append(('XGB Tuned',XGB_tunned_sm))\nmodels_sm.append(('SVM',svc_sm))\nmodels_sm.append(('SVM Tuned',SVM_tunned_sm))","2f18ad3f":"X_train_sm=np.array(X_train_sm)\ny_train_sm=np.array(y_train_sm) \nX_test=np.array(X_test)\ny_test=np.array(y_test)","3e763495":"from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve\nacc_train_sm=[]\nauc_train_sm=[]\n\nacc_test_sm=[]\nauc_test_sm=[]\nnames_sm=[]\ndef model_eval(name,algo, Xtrain,ytrain,Xtest,ytest):\n    names_sm.append(name)    \n    \n    algo.fit(Xtrain,ytrain)\n    ytrain_pred =algo.predict(Xtrain)\n    ytrain_prob = algo.predict_proba(Xtrain)[:,1]\n\n    acc_train_sm.append(accuracy_score(ytrain, ytrain_pred))\n    auc_train_sm.append(roc_auc_score(ytrain, ytrain_prob))\n\n    ytest_pred = algo.predict(Xtest)\n    ytest_prob = algo.predict_proba(Xtest)[:,1]\n\n    acc_test_sm.append(accuracy_score(ytest, ytest_pred))\n    auc_test_sm.append(roc_auc_score(ytest, ytest_prob))","ba6461d9":"for name,algo in models_sm:\n    model_eval(name,algo,X_train_sm,y_train_sm,X_test,y_test)","1bbbf30d":"result_sm_df=pd.DataFrame({'Model':names_sm,\n                           \n                           'Accuracy Score Train':acc_train_sm,\n                           'ROC-AUC Score Train':auc_train_sm,\n                           \n                           'Accuracy Score Test':acc_test_sm,\n                           'ROC-AUC Score Test':auc_test_sm\n                            })","a11c8207":"result_sm_df","c50e5e64":"def model_graph(name,algo, Xtrain,ytrain,Xtest,ytest):   \n    \n    algo.fit(Xtrain,ytrain)\n    ytrain_pred =algo.predict(Xtrain)\n    ytrain_prob = algo.predict_proba(Xtrain)[:,1]\n\n    acc_train_sm.append(accuracy_score(ytrain, ytrain_pred))\n    auc_train_sm.append(roc_auc_score(ytrain, ytrain_prob))\n\n    ytest_pred = algo.predict(Xtest)\n    ytest_prob = algo.predict_proba(Xtest)[:,1]\n\n    acc_test_sm.append(accuracy_score(ytest, ytest_pred))\n    auc_test_sm.append(roc_auc_score(ytest, ytest_prob))\n\n\n    fpr,tpr,th=roc_curve(ytest, ytest_prob)\n    \n    plt.plot(fpr,tpr,label=name)\n    plt.plot(fpr,fpr)\n    plt.legend(loc=\"lower right\")\n\nplt.show()","be5fe31e":"plt.figure(figsize=(10,7))\nmodel_sm_final=[('Random Forest Tuned',sm_rfc_tunned),('XGB Tuned',XGB_tunned_sm),\n                ('LGBM Tuned',sm_lgbm_tunned),('Gradient Boost',gb_sm)]\nfor name,algo in model_sm_final:\n    model_graph(name,algo,X_train_sm,y_train_sm,X_test,y_test)","301b1fcb":"### Model After SMOTE","03aaef80":"# Outliers","c8c95a36":"### Hyperparameter Tuning of KNN","be813d0f":"### Hyperparameter Tuning of XGB","589e9407":"### SMOTE Model Results","f46b26a4":"### Model Comparison","0bdeb0b6":"#### OPERATING SYSTEM","2fac913b":"### Kurtosis:","6405896f":"### Class Imbalance :","58090abb":"### Model Results","53504f73":"### Model Results of PCA components","0fb8d6b9":"# Modeling without PCA","c24d042d":"# PCA","5a7d57ca":"### Observation:\n\nWe can see that the first thirty PC\u2019s explain over 90% variance of the data","ae9f0c97":"### Observation:\n\nThe PCA biplot represents the variables with calibrated axes and observations as points allowing you to project the observations onto the axes to make an approximation of the original values of the variables. The angles between the vectors tell us how characteristics correlate with one another\n\n\n\u2022 When two vectors are close, forming a small angle, the two variables they represent are positively correlated. [Browser2 and OperatingSyatem_2]\n\n\n\u2022 If they meet each other at 90\u00b0, they are not likely to be correlated. [Administrative_Duration and Browser1]\n\n\n\u2022 When they diverge and form a large angle (close to 180\u00b0), they are negative correlated. [Browser2 and Browser1]","47ef0490":"## Univariate Analysis","43cbede9":"### Hyperparameter Tuning of Random Forest","3ce89167":"# Please Upvote if you like and want to support my work!!!","8038e012":"### Hyperparameter Tuning of LGBM","08d834f4":"### Hyperparameter Tuning of LGBM","c411fa65":"#### BACKWARD ELIMINATION","00857a84":"It can be observed that there is an imbalance within the target variable. \n\nWe will have to implement measure to sample the data correctly in order to build a good ML model.","083f3781":"### Compare PCA number of components with logistic regression algorithm for classification","d4d4a80c":"### FEATURE SELECTION","75c6fa01":"### Hyperparameter Tuning of KNN","0727d0b4":"# Importing Required Libraries","b0c4d8f0":"### Stacking","332aec94":"### Observation:\n\nWe an see influence on each of the Principle components by the features.","56e37267":"### Hyperparameter Tuning of Random Forest","69ce6fb9":"### Observation:\n\nThough there is a certain degree of overlap, the points belonging to the same category are distinctly clustered and region bound. \n\nThis proves that the data captured in the first three PCs is informative enough to discriminate the categories from each other. \n\n\nIn other words, we now have evidence that the data is not completely random, but rather can be used to discriminate or explain the target variable.","cc02dc9a":"### Building Model Using PCA components that explain 95% variance of the data","10f19060":"<h3>Visualization of Outliers<\/h3>","239877af":"### Skewnss:","bd737d44":"#### BROWSER","7e3f7aa7":"### PCA Bi-Plot","ca79d08c":"# Shape of Data","b9867ad6":"### Kaiser-Meyer-Olkin (KMO) Test","e747b6b6":"### 3D Visualization of PCA components","4ffb1a70":"### Distribution of all numerical columns:","12c17d51":"# Model Using selected Features","62675fa4":"### Comparison of Top 4 best performing Models","bd331f89":"### Model Comparison","39251bf3":"### Hyperparameter Tuning of KNN","dedf71dc":"### Hyperparameter Tuning of SVC","1958d452":"### Hyperparameter Tuning of XGB","c51cb86b":"## Statistical Test for Numerical column vs Target Column","90b8b05c":"### Hyperparameter Tuning of LGBM","4858d05d":"#### VISITOR TYPE","626abdf2":"# Descriptive Statistics:","64a38b32":"# Number of null Values","c48b5ab3":"# Online Shoppers Purchasing Intention\n\n<p>The data used in this analysis is an Online Shoppers Purchasing Intention data set provided on the UC Irvine\u2019s Machine Learning Repository.  \nThe data set was formed so that each session would belong to a different user in a 1-year period to avoid any tendency to a specific campaign, special day, user profile, or period.The primary purpose of the data set is to predict the purchasing intentions of a visitor to this particular store\u2019s website.<\/p>\n\n<p><img style=\"float: left;margin:5px 20px 5px 1px\" src=\"https:\/\/miro.medium.com\/max\/10944\/1*_fi-xsfqDgTQCivURGd3Ow.jpeg\" width=\"1000\" height=\"100\"><\/p>\n\n\n\n<h2><strong>Attribute Information :<\/strong><\/h2>\n<p>The dataset consists of 10 numerical and 8 categorical attributes.\nThe 'Revenue' attribute can be used as the class label. Of the 12,330 sessions in the dataset, 84.5% (10,422) were negative class samples that did not end with shopping, and the rest (1908) were positive class samples ending with shopping.<\/p>\n\n\n<p>\"Administrative\", \"Administrative Duration\", \"Informational\", \"Informational Duration\", \"Product Related\" and \"Product Related Duration\" represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. \n\nThe values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another. \n\nThe \"Bounce Rate\", \"Exit Rate\" and \"Page Value\" features represent the metrics measured by \"Google Analytics\" for each page in the e-commerce site. The value of \"Bounce Rate\" feature for a web page refers to the percentage of visitors who enter the site from that page and then leave (\"bounce\") without triggering any other requests to the analytics server during that session. \n\nThe value of \"Exit Rate\" feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session. The \"Page Value\" feature represents the average value for a web page that a user visited before completing an e-commerce transaction. \n\nThe \"Special Day\" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother\u2019s Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction. \n\nThe value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina\u2019s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8. \n\nThe dataset also includes operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.<\/p>","f2831fd7":"##### Since KMO Test does not pass for the data we do not continue with Factor Analysis, hence we can do PCA.","91b480e0":"# Reading the data set","d631233b":"#### MONTH","66f001f0":"### Proportion of variance plot","24366140":"# Converting Numeric to Categorical","5a817a5e":"### Scree Plot","59715551":"# Multicollinearity","43c9f251":"### Boxplot of all columns","cbe18289":"## Droping columns that have high Multicollinearity","bb8d017a":"#### TRAFFIC TYPE","4842464f":"### Weights or Loadings of Principal Components","ed26c3e2":"### Stacking","8f5805f0":"### Effect of variables on each component using Heat map.","8a34bce8":"# Factor Analysis","b189d24f":"# SMOTE","7ed8acd3":"#### WEEKEND AND REVENUE","0b3033f0":"### Hyperparameter Tuning of SVC","cd64429c":"### Hyperparameter Tuning of XGB","b6d295b6":"### Observation:\n\nWe see a general trend of increased performance as the number of dimensions is increased. On this dataset, the results suggest a trade-off in the number of dimensions vs. the classification accuracy of the model.\n\nWe don\u2019t see any improvement beyond 15 components. This matches our definition of the problem where only the first 15 components contain information about the class and the remaining are redundant.","06a21791":"# EDA","5df78d0d":"### Hyperparameter Tuning of SVC","251d0787":"<h3>Distribution After Transformation<\/h3>","ad34e479":"### Bartlett\u2019s test","d2e52c67":"# Statistical Tests","4c000437":"## Statistical Test for Categorical column vs Target Column","efd1bad0":"### Principal Components Weights (Eigenvectors)","f862ccd0":"<h3>Transforming the columns with the best transfrmation technique given by above code<\/h3>","6d0dcc3f":"### 2D Visualization of PCA components","e93c5525":"<h3>Distribution Before Transformation<\/h3>","8b7ff6c2":"### Hyperparameter Tuning of Random Forest","7a82dc60":"## Multi-Variate Analysis"}}