{"cell_type":{"7e352d2b":"code","a1564f42":"code","3ae7a4c5":"code","66fcffe9":"code","d36f6f92":"code","f3376d67":"code","bb6086ab":"code","d9425f3f":"code","f7851e1b":"code","8310a7c4":"code","1b077ed9":"code","ee5974e5":"code","72e9d6cc":"code","16dd8e7b":"code","5116735d":"code","f57fd915":"code","ba28dc31":"code","12c56987":"code","18d5188b":"code","52959e3d":"code","ffa8807e":"code","214df504":"code","77c9d3f6":"code","23fbbfab":"code","f2f41efc":"code","02ed4389":"code","bb01e49d":"code","c2470aee":"code","a41125a2":"code","a20ba926":"code","93d7c2c4":"code","a2cccd02":"code","b69c3e38":"code","cf5c8393":"code","c2ac2235":"code","3d95d98c":"code","fa34bc23":"code","ed5f702c":"code","573fbbfa":"code","badc6d30":"code","9a5fd86c":"code","dc1b64ef":"code","8e25045c":"code","ca4fe8dd":"code","df178d3d":"code","e5f2015b":"code","3df00643":"code","2514d3a8":"code","290fc9e9":"code","a3a3a196":"code","96ec3380":"markdown","710a0250":"markdown","8c6f02d8":"markdown","ff780249":"markdown"},"source":{"7e352d2b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras import layers\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D,MaxPool2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom keras.models import Model,Sequential\n\n        \nimport keras.backend as K\nK.set_image_data_format('channels_last')\n\nfrom matplotlib.pyplot import imshow\nfrom keras.preprocessing import image\nfrom keras import applications\nfrom keras.models import Sequential\nimport os,sys\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport cv2\nfrom keras.preprocessing.image import ImageDataGenerator","a1564f42":"os.listdir('..\/input\/s')","3ae7a4c5":"img = cv2.imread('..\/input\/stanford-dogs-dataset-traintest\/cropped\/test\/n02086240-Shih-Tzu\/n02086240_11551.jpg')\nprint(img.shape)\nimg1 = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.imshow(img1)","66fcffe9":"train_dir = '..\/input\/stanford-dogs-dataset-traintest\/cropped\/train\/'\ntest_dir = '..\/input\/stanford-dogs-dataset-traintest\/cropped\/test\/'","d36f6f92":"# Training generator\ndatagen = ImageDataGenerator( \n    rescale=1.\/255,\n)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255,    validation_split=0.33)\n\n\ntrain_generator = datagen.flow_from_directory(\n    directory=train_dir,\n    target_size=(224, 224),\n    color_mode=\"rgb\",\n    batch_size=64,\n    class_mode=\"categorical\",\n    shuffle=True,\n    seed=42, \n)\n\n# Valid generator\n\nvalid_generator = test_datagen.flow_from_directory(\n    directory=train_dir,\n    target_size=(224, 224),\n    color_mode=\"rgb\",\n    batch_size=64,\n    class_mode=\"categorical\",\n    shuffle=True,\n    seed=42, \n    subset=\"validation\"\n)\n\n# Test generator\n\ntest_generator = test_datagen.flow_from_directory(\n    directory=test_dir,\n    target_size=(224, 224),\n    color_mode=\"rgb\",\n    batch_size=32,\n    class_mode=\"categorical\",\n    shuffle=False,\n    seed=42\n)","f3376d67":"from keras import optimizers\nfrom keras.callbacks import ReduceLROnPlateau\n\noptimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n\nSTEP_SIZE_TRAIN=train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID=valid_generator.n\/\/valid_generator.batch_size\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","bb6086ab":"trdata = ImageDataGenerator()\ntraindata = trdata.flow_from_directory(directory=train_dir,target_size=(224,224))\ntsdata = ImageDataGenerator()\ntestdata = tsdata.flow_from_directory(directory=test_dir, target_size=(224,224))","d9425f3f":"model = Sequential()\n\nmodel.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(units=4096,activation=\"relu\"))\nmodel.add(Dense(units=4096,activation=\"relu\"))\n\nmodel.add(Dense(units=120, activation=\"softmax\"))\n\n\n","f7851e1b":"from keras.optimizers import Adam\nmodel.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])","8310a7c4":"\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\ncheckpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\nearly = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\nhist = model.fit_generator(steps_per_epoch=100,generator=traindata, validation_data= testdata, validation_steps=10,epochs=100,callbacks=[checkpoint,early])","1b077ed9":"import matplotlib.pyplot as plt\nplt.plot(hist.history[\"acc\"])\nplt.plot(hist.history['val_acc'])\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title(\"model accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\nplt.show()","ee5974e5":"from keras.applications import VGG16\nimg_width, img_height=(224,224)\n\nbatch_size = 48*3\nnb_train_samples = 12000\nnb_validation_samples = ( 8000 \/\/ batch_size ) * batch_size\nepochs = 6\n\ndatagen = ImageDataGenerator(\n    horizontal_flip=True,\n    shear_range=0.2,\n    rescale=1. \/ 255)\n\nvdatagen = ImageDataGenerator(rescale=1.\/255)\n\ntraingen = datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    follow_links=True,\n    shuffle=True)\n\nvalgen = vdatagen.flow_from_directory(\n    test_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    follow_links=True,\n    shuffle=True)\n\nvgg_model = VGG16(input_shape=(224,224,3), weights=\"imagenet\", include_top=False)\nfor layer in vgg_model.layers:\n    layer.trainable = False\nmodel = Sequential()\nmodel.add(vgg_model)\nmodel.add(Flatten())\nmodel.add(Dense(units=4096,activation=\"relu\"))\nmodel.add(Dense(units=4096,activation=\"relu\"))\nmodel.add(Dense(120, activation='softmax'))\nmodel.compile(optimizer=Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit_generator(traingen,\n          epochs=epochs,\n          steps_per_epoch=nb_train_samples \/\/ batch_size,\n          validation_data=valgen,\n          validation_steps=nb_validation_samples \/\/ batch_size)","72e9d6cc":"import matplotlib.pyplot as plt\nplt.plot(history.history[\"acc\"])\nplt.plot(history.history['val_acc'])\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title(\"model accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\nplt.show()","16dd8e7b":"pred=vgg_model.predict(np.expand_dims(img,0))\nlen(pred[0][0][0])","5116735d":"plt.imshow(l)","f57fd915":"xception = Xception(input_shape=(224,224,3), weights=\"imagenet\", include_top=False)\nfor layer in xception.layers:\n    layer.trainable = False\nmodel = Sequential()\nmodel.add(xception)\nmodel.add(Flatten())\nmodel.add(Dense(units=4096,activation=\"relu\"))\nmodel.add(Dense(units=4096,activation=\"relu\"))\nmodel.add(Dense(120, activation='softmax'))\nmodel.compile(optimizer=Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n\n\n\n\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\ncheckpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\nearly = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n\n\n\nfit_stats = model.fit_generator(generator=train_generator,\n                    steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_data=valid_generator,\n                    validation_steps=STEP_SIZE_VALID,\n                    epochs=25,\n                    verbose=1,\n                    callbacks=[learning_rate_reduction]\n)","ba28dc31":"from keras.applications.xception import Xception\nbase_model = Xception(weights='imagenet', include_top=False)","12c56987":"x = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\npredictions = Dense(120, activation='softmax')(x)\n\n\nfor layer in base_model.layers:\n    layer.trainable = False\n    \nxception = Model(inputs=base_model.input, outputs=predictions)","18d5188b":"optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n\nxception.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])","52959e3d":"from keras.callbacks import ModelCheckpoint, EarlyStopping\ncheckpoint = ModelCheckpoint(\"dog_class.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\nearly = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n\nfit_stats = xception.fit_generator(generator=train_generator,\n                    steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_data=valid_generator,\n                    validation_steps=STEP_SIZE_VALID,\n                    epochs=25,\n                    verbose=1,\n                    callbacks=[learning_rate_reduction,checkpoint,early]\n)","ffa8807e":"STEP_SIZE_TEST=test_generator.n\/\/test_generator.batch_size","214df504":"loss, acc = xception.evaluate_generator(generator=test_generator, steps=STEP_SIZE_TEST, verbose=0)","77c9d3f6":"print(loss, acc)","23fbbfab":"plt.plot(fit_stats.history['acc'])","f2f41efc":"pred=xception.predict_generator(test_generator,\nsteps=STEP_SIZE_TEST,\nverbose=1)","02ed4389":"predicted_class_indices=np.argmax(pred,axis=1)\n\nlabels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]","bb01e49d":"predictions","c2470aee":"def get_pred(pred):\n    predicted_class_indices=np.argmax(pred,axis=1)\n    labels = (traingen.class_indices)\n    labels = dict((v,k) for k,v in labels.items())\n    print(predicted_class_indices)\n    return labels[predicted_class_indices[0]]","a41125a2":"img = img\/255","a20ba926":"plt.imshow(img)","93d7c2c4":"pred=xception.predict(np.expand_dims(img,0))\npred[0][4]*100","a2cccd02":"np.argmax(pred)","b69c3e38":"a = np.argsort(-pred)","cf5c8393":"a","c2ac2235":"inv_map = {v: k for k, v in train_generator.class_indices.items()}","3d95d98c":"inv_map[a[0][0]]","fa34bc23":"b = train_generator.class_indices","ed5f702c":"from PIL import Image\nimport requests\nfrom io import BytesIO\ndef load_img(url):\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    return img","573fbbfa":"pred=xception.predict(np.expand_dims(img,0))\n","badc6d30":"pred = pred*100","9a5fd86c":"h=np.argmax(pred,1)","dc1b64ef":"pred","8e25045c":"train_generator.class_indices","ca4fe8dd":"url = 'https:\/\/cdn3-www.dogtime.com\/assets\/uploads\/2020\/03\/akita-pit-mixed-dog-breed-pictures-1-1442x958.jpg'\n\nresponse = requests.get(url)\nimg = Image.open(BytesIO(response.content))\n","df178d3d":"def predict(url):\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    img=np.array(img)\n    plt.imshow(img)\n\n    img= cv2.resize(img,(224,224))\n    pred = xception.predict(np.expand_dims(img,0))\n    return get_pred(pred)\n    ","e5f2015b":"predict(url)","3df00643":"predict('https:\/\/i.pinimg.com\/236x\/44\/9a\/cd\/449acd48951b5a2370b7e2d125030a33.jpg')","2514d3a8":"model = Model()","290fc9e9":"model.load_weights('dog_class.h5')","a3a3a196":"import pickle\n\n# obj0, obj1, obj2 are created here...\n\n# Saving the objects:\nwith open('classes.pkl', 'wb') as f:  # Pyth\n    pickle.dump(b, f)\n","96ec3380":"# Dog breed classification with Stanford Dogs Dataset.\n","710a0250":"# VGG Model","8c6f02d8":"# Xception model","ff780249":"# My Model"}}