{"cell_type":{"bd09daa2":"code","b1478485":"code","6b0df50f":"code","fa96f9f0":"code","3efbd0ad":"code","42cbec91":"code","2cd3757c":"code","f5caf2f6":"code","1e98f651":"code","81271dd7":"code","e2d87b4f":"code","15fa008a":"code","8c848f97":"code","983af376":"code","582354ba":"code","a39fdbed":"code","dba8f32a":"code","9da96a7e":"markdown","4cbbfdc6":"markdown","af1c1526":"markdown","fd808d4e":"markdown","49d97559":"markdown","5836cd91":"markdown","c47f0178":"markdown","4999f196":"markdown","bcdbe6ba":"markdown","85e08822":"markdown","0ebdc064":"markdown","323dea86":"markdown","e11a592a":"markdown"},"source":{"bd09daa2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom keras.layers import Conv2D, Dense, BatchNormalization, Activation, Dropout, MaxPooling2D, Flatten,AveragePooling2D\nfrom keras.optimizers import Adam, SGD\nfrom keras import Sequential\nimport matplotlib.pyplot as plt\nfrom keras.utils import plot_model\nimport cv2\nimport os\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\ntrain_dir = '..\/input\/fer2013\/train\/'\ntest_dir = '..\/input\/fer2013\/test\/'\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b1478485":"#count \nrow, col = 48, 48\nclasses = 7\n\ndef count_exp(path, set_):\n    dict_ = {}\n    for expression in os.listdir(path):\n        dir_ = path + expression\n        dict_[expression] = len(os.listdir(dir_))\n    df = pd.DataFrame(dict_, index=[set_])\n    return df\ntrain_count = count_exp(train_dir, 'train')\ntest_count = count_exp(test_dir, 'test')\nprint(train_count)\nprint(test_count)","6b0df50f":"print('training pictures\\n')\nplt.figure(figsize=(14,22))\ni = 1\nfor expression in os.listdir(train_dir):\n    img = load_img((train_dir + expression +'\/'+ os.listdir(train_dir + expression)[5]))\n    img = img.resize(224,224)\n    plt.subplot(1,7,i)\n    plt.imshow(img)\n    plt.title(expression)\n    plt.axis('off')\n    i += 1\nplt.show()\n\nprint('testing pictures\\n')\nplt.figure(figsize=(14,22))\ni = 1\nfor expression in os.listdir(test_dir):\n    img = load_img((test_dir + expression +'\/'+ os.listdir(test_dir + expression)[5]))\n    img = img.resize(224,224)\n    plt.subplot(1,7,i)\n    plt.imshow(img)\n    plt.title(expression)\n    plt.axis('off')\n    i += 1\nplt.show()","fa96f9f0":"train_count.transpose().plot(kind= 'bar')\ntest_count.transpose().plot(kind= 'bar')","3efbd0ad":"train_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   zoom_range=0.5,\n                                   horizontal_flip=True)\n\ntraining_set = train_datagen.flow_from_directory(train_dir,\n                                                batch_size=32,\n                                                target_size=(224,224),\n                                                shuffle=True,\n                                                color_mode='grayscale',\n                                                class_mode='categorical')\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_set = test_datagen.flow_from_directory(test_dir,\n                                                batch_size=32,\n                                                target_size=(224,224),\n                                                shuffle=True,\n                                                color_mode='grayscale',\n                                                class_mode='categorical')","42cbec91":"def plot_history(history):\n    plt.figure(figsize=(14,5))\n    plt.subplot(1,2,2)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend(['train', 'test'], loc='upper left')\n\n    plt.subplot(1,2,1)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","2cd3757c":"from tensorflow.keras.applications.vgg16 import VGG16\nmodel = VGG16(weights='imagenet', include_top=False )","f5caf2f6":"# Block1_conv1 weights are of the format [3, 3, 3, 64] -> this is for RGB images\n# For grayscale, format should be [3, 3, 1, 64]. Weighted average of the features has to be calculated across channels.\n# RGB weights: Red 0.2989, Green 0.5870, Blue 0.1140\n\n# getting weights of block1 conv1.\nblock1_conv1 = model.get_layer('block1_conv1').get_weights()\nweights, biases = block1_conv1\n\n# :weights shape = [3, 3, 3, 64] - (0, 1, 2, 3)\n# convert :weights shape to = [64, 3, 3, 3] - (3, 2, 0, 1)\nweights = np.transpose(weights, (3, 2, 0, 1))\n\n\nkernel_out_channels, kernel_in_channels, kernel_rows, kernel_columns = weights.shape\n\n# Dimensions : [kernel_out_channels, 1 (since grayscale), kernel_rows, kernel_columns]\ngrayscale_weights = np.zeros((kernel_out_channels, 1, kernel_rows, kernel_columns))\n","1e98f651":"# iterate out_channels number of times\nfor i in range(kernel_out_channels):\n\n\t# get kernel for every out_channel\n\tget_kernel = weights[i, :, :, :]\n\n\ttemp_kernel = np.zeros((3, 3))\n\n\t# :get_kernel shape = [3, 3, 3]\n\t# axis, dims = (0, in_channel), (1, row), (2, col)\n\n\t# calculate weighted average across channel axis\n\tin_channels, in_rows, in_columns = get_kernel.shape\n\n\tfor in_row in range(in_rows):\n\t\tfor in_col in range(in_columns):\n\t\t\tfeature_red = get_kernel[0, in_row, in_col]\n\t\t\tfeature_green = get_kernel[1, in_row, in_col]\n\t\t\tfeature_blue = get_kernel[2, in_row, in_col]\n\n\t\t\t# weighted average for RGB filter\n\t\t\ttotal = (feature_red * 0.2989) + (feature_green * 0.5870) + (feature_blue * 0.1140)\n\n\t\t\ttemp_kernel[in_row, in_col] = total\n\n\n\t# :temp_kernel is a 3x3 matrix [rows x columns]\n\t# add an axis at the end to specify in_channel as 1\n\n\t# 2 ways of doing this,\n\n\t# First: Add axis directly at the end of :temp_kernel to make its shape: [3, 3, 1], but this might be \n\t# an issue when concatenating all feature maps\n\n\t# Second: Add axis at the start of :temp_kernel to make its shape: [1, 3, 3] which is [in_channel, rows, columns]\n\ttemp_kernel = np.expand_dims(temp_kernel, axis=0)\n\n\t# Now, :temp_kernel shape is [1, 3, 3]\n\n\t# Concat :temp_kernel to :grayscale_weights along axis=0\n\tgrayscale_weights[i, :, :, :] = temp_kernel\n\n# Dimension of :grayscale_weights is [64, 1, 3, 3]\n# In order to bring it to tensorflow or keras weight format, transpose :grayscale_weights\n\n# dimension, axis of :grayscale_weights = (out_channels: 0), (in_channels: 1), (rows: 2), (columns: 3)\n# tf format of weights = (rows: 0), (columns: 1), (in_channels: 2), (out_channels: 3)\n\n# Go from (0, 1, 2, 3) to (2, 3, 1, 0)\ngrayscale_weights = np.transpose(grayscale_weights, (2, 3, 1, 0)) # (3, 3, 1, 64)\n\n# combine :grayscale_weights and :biases\nnew_block1_conv1 = [grayscale_weights, biases]\n\n\n# Reconstruct the layers of VGG16 but replace block1_conv1 weights with :grayscale_weights\n\n# get weights of all the layers starting from 'block1_conv2'\nvgg16_weights = {}\nfor layer in model.layers[2:]:\n\tif \"conv\" in layer.name:\n\t\tvgg16_weights[\"1024_\" + layer.name] = model.get_layer(layer.name).get_weights()\n\ndel model","81271dd7":"from keras.layers import Input\nfrom keras.models import load_model, Model\n\n# Custom build VGG16\ninput = Input(shape=(224, 224, 1), name='1024_input')\n# Block 1\nx = Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(1024, 1024, 1), data_format=\"channels_last\", name='1024_block1_conv1')(input)\nx = Conv2D(64, (3, 3), activation='relu', padding='same', name='1024_block1_conv2')(x)\nx = MaxPooling2D((2, 2), strides=(2, 2), name='1024_block1_pool')(x)\n\n# Block 2\nx = Conv2D(128, (3, 3), activation='relu', padding='same', name='1024_block2_conv1')(x)\nx = Conv2D(128, (3, 3), activation='relu', padding='same', name='1024_block2_conv2')(x)\nx = MaxPooling2D((2, 2), strides=(2, 2), name='1024_block2_pool')(x)\n\n# Block 3\nx = Conv2D(256, (3, 3), activation='relu', padding='same', name='1024_block3_conv1')(x)\nx = Conv2D(256, (3, 3), activation='relu', padding='same', name='1024_block3_conv2')(x)\nx = Conv2D(256, (3, 3), activation='relu', padding='same', name='1024_block3_conv3')(x)\nx = MaxPooling2D((2, 2), strides=(2, 2), name='1024_block3_pool')(x)\n\n# Block 4\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='1024_block4_conv1')(x)\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='1024_block4_conv2')(x)\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='1024_block4_conv3')(x)\nx = MaxPooling2D((2, 2), strides=(2, 2), name='1024_block4_pool')(x)\n\n# Block 5\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='1024_block5_conv1')(x)\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='1024_block5_conv2')(x)\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='1024_block5_conv3')(x)\nx = MaxPooling2D((8, 8), strides=(8, 8), name='1024_block5_pool')(x)\n\nbase_model = Model(inputs=input, outputs=x)\n\nbase_model.get_layer('1024_block1_conv1').set_weights(new_block1_conv1)\nfor layer in base_model.layers[2:]:\n\tif 'conv' in layer.name:\n\t\tbase_model.get_layer(layer.name).set_weights(vgg16_weights[layer.name])\n\nbase_model.summary()\n\n#print base_model.get_layer('block3_conv2').get_weights()\nbase_model.save('vgg_grayscale_1024.hdf5')","e2d87b4f":"for layer in base_model.layers:\n    layer.trainable = False","15fa008a":"vgg16Model = Sequential()\nvgg16Model.add(base_model)\nvgg16Model.add(Flatten())\nvgg16Model.add(Dense( 7, activation = \"softmax\"))\n\nvgg16Model.summary()\n","8c848f97":"steps_per_epoch = training_set.n \/\/ training_set.batch_size\nvalidation_steps = test_set.n \/\/ test_set.batch_size\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\ncheckpoint1 = ModelCheckpoint(\"vgg16.h5\",monitor = \"val_accuracy\",save_best_only = True,verbose=1)\nearlystop1 = EarlyStopping(monitor=\"val_accuracy\",patience=8,verbose=1)\nvgg16Model.compile(optimizer=\"adam\",loss = \"categorical_crossentropy\",metrics = [\"accuracy\"])\n\n\nhist = vgg16Model.fit(x=training_set,\n                 validation_data=test_set,\n                 epochs=35,\n                 callbacks=[checkpoint1,earlystop1],\n                 steps_per_epoch=steps_per_epoch,\n                 validation_steps=validation_steps)\n","983af376":"plot_history(hist)","582354ba":"cnnModel = Sequential()\n#1st convolution layer\ncnnModel.add(Conv2D(64, (5, 5), activation='relu', input_shape=(224,224,1)))\ncnnModel.add(MaxPooling2D(pool_size=(5,5), strides=(2, 2)))\n \n#2nd convolution layer\ncnnModel.add(Conv2D(64, (3, 3), activation='relu'))\ncnnModel.add(Conv2D(64, (3, 3), activation='relu'))\ncnnModel.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n \n#3rd convolution layer\ncnnModel.add(Conv2D(128, (3, 3), activation='relu'))\ncnnModel.add(Conv2D(128, (3, 3), activation='relu'))\ncnnModel.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n \ncnnModel.add(Flatten())\n \n#fully connected neural networks\ncnnModel.add(Dense(1024, activation='relu'))\ncnnModel.add(Dropout(0.2))\ncnnModel.add(Dense(1024, activation='relu'))\ncnnModel.add(Dropout(0.2))\n \ncnnModel.add(Flatten())\ncnnModel.add(Dense(4096 , activation= 'relu'))\ncnnModel.add(Dense(4096 , activation= 'relu'))\ncnnModel.add(Dense(7, activation='softmax'))\ncnnModel.summary()\n","a39fdbed":"steps_per_epoch = training_set.n \/\/ training_set.batch_size\nvalidation_steps = test_set.n \/\/ test_set.batch_size\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\ncheckpoint1 = ModelCheckpoint(\"cnn.h5\",monitor = \"val_accuracy\",save_best_only = True,verbose=1)\nearlystop1 = EarlyStopping(monitor=\"val_accuracy\",patience=8,verbose=1)\ncnnModel.compile(optimizer=\"adam\",loss = \"categorical_crossentropy\",metrics = [\"accuracy\"])\n\n\nhist = cnnModel.fit(x=training_set,\n                 validation_data=test_set,\n                 epochs=35,\n                 callbacks=[checkpoint1,earlystop1],\n                 steps_per_epoch=steps_per_epoch,\n                 validation_steps=validation_steps)","dba8f32a":"plot_history(hist)","9da96a7e":"show sample of pictures","4cbbfdc6":"first to make the net learn better it needs the images to be\naugumented \n\nit is faster to work with dataloders but it is possible to load images with open cv ","af1c1526":"# My simple CNN net","fd808d4e":"this code is from https:\/\/github.com\/RohitSaha\/VGG_Imagenet_Weights_GrayScale_Images\/blob\/master\/convert_vgg_grayscale.py\n\nAn experement of working with grayscale images on a colored trained net (VGG16)","49d97559":"> # **preproccess**\n","5836cd91":"results are not very good","c47f0178":"smaller net is not better","4999f196":"show class count ","bcdbe6ba":"function for ploting history","85e08822":"# VGG16 with grayscale pictures","0ebdc064":"# Modeling","323dea86":"count how many pictures there are from each mood (train , test)","e11a592a":"less is more ... (maybe)"}}