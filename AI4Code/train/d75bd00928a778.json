{"cell_type":{"a2f31a83":"code","9ab117ee":"code","04d0b89c":"code","be315a50":"code","d85afa39":"code","9acb233f":"code","0d1327a1":"code","eb0232b6":"code","27b8a64a":"code","b9bc7481":"code","d0683e70":"code","675d947a":"code","e0404c64":"code","8184371d":"code","3ae0ca2c":"code","f60372e7":"code","a6da48b2":"code","d8360487":"code","4c622327":"code","6e24d710":"code","bb6be51d":"code","1b9683f9":"code","97288cd5":"code","5c75ec2d":"code","48203a94":"code","721f944f":"code","4dd84630":"code","cb5bc042":"code","6affe981":"code","c9c1d3e8":"code","3d44e289":"code","0d801209":"code","fed51e94":"code","13d786bd":"code","02a58214":"code","d68426a5":"code","1374d110":"code","6bf78a14":"code","baf4db24":"code","e5b16e56":"markdown","bf9367fd":"markdown","0d21cf54":"markdown","298c7c0b":"markdown","8336dda6":"markdown","a822ed14":"markdown","8e2da06a":"markdown","bc423ef0":"markdown","df6ab977":"markdown","015ef0f5":"markdown","c0092226":"markdown","2990da3a":"markdown","2a06441c":"markdown","97f41992":"markdown","4a64e0a3":"markdown"},"source":{"a2f31a83":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score","9ab117ee":"train_file = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\") \n# Load the testing dataset\ntest_file = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","04d0b89c":"pd.set_option('display.max_columns', None)\nprint(train_file.shape)\ntrain_file.head(3)","be315a50":"train_file[[i for i in train_file.columns if train_file[i].isnull().sum()>0]].isnull().sum()","d85afa39":"#Table Information\ntrain_file.info()","9acb233f":"train_file.describe()","0d1327a1":"#if the columns having more than 50% of missing values, then i'm removing it\ntrain_file = train_file.drop(['Id', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis = 1)","eb0232b6":"\n# As per our data, if Bmst value is Not available -> No basement, Garage value is Not available -> No Garage, Alley Value is NA -> No alley Access\ntrain_file['BsmtQual'] = np.where(train_file['BsmtQual'].isnull() == True, \"No Basement\", train_file['BsmtQual'])\ntrain_file['BsmtCond'] = np.where(train_file['BsmtCond'].isnull() == True, \"No Basement\", train_file['BsmtCond'])\ntrain_file['BsmtExposure'] = np.where(train_file['BsmtExposure'].isnull() == True, \"No Basement\", train_file['BsmtExposure'])\ntrain_file['BsmtFinType1'] = np.where(train_file['BsmtFinType1'].isnull() == True, \"No Basement\", train_file['BsmtFinType1'])\ntrain_file['BsmtFinType2'] = np.where(train_file['BsmtFinType2'].isnull() == True, \"No Basement\", train_file['BsmtFinType2'])\n#--------------------\ntrain_file['Alley'] = np.where(train_file['Alley'].isnull() == True, \"No Alley Access\", train_file['Alley'])\n#--------------------\ntrain_file['GarageType'] = np.where(train_file['GarageType'].isnull() == True, \"No Garage\", train_file['GarageType'])\ntrain_file['GarageYrBlt'] = np.where(train_file['GarageYrBlt'].isnull() == True, \"No Garage\", train_file['GarageYrBlt'])\ntrain_file['GarageFinish'] = np.where(train_file['GarageFinish'].isnull() == True, \"No Garage\", train_file['GarageFinish'])\ntrain_file['GarageQual'] = np.where(train_file['GarageQual'].isnull() == True, \"No Garage\", train_file['GarageQual'])\ntrain_file['GarageCond'] = np.where(train_file['GarageCond'].isnull() == True, \"No Garage\", train_file['GarageCond'])","27b8a64a":"#Replace the grouped mode value of MasVnrType, Elecctrical, MasVnrArea\ntrain_file['MasVnrType'] = train_file.groupby(['YearBuilt'], sort=False)['MasVnrType'].apply(lambda x: x.fillna(x.mode().iloc[0]))\ntrain_file['Electrical'] = train_file.groupby(['YearBuilt'], sort=False)['Electrical'].apply(lambda x: x.fillna(x.mode().iloc[0]))\ntrain_file['MasVnrArea'] = train_file['MasVnrArea'].fillna(train_file.groupby(['YearBuilt'])['MasVnrArea'].transform('mean'))","b9bc7481":"train_LotFrontage_main = train_file[train_file.LotFrontage.isnull() != True]\ntest_LotFrontage_main = train_file[train_file.LotFrontage.isnull() == True]\n#--------------------\ntest_LotFrontage = test_LotFrontage_main.drop('LotFrontage', axis = 1)\ntrain_LotFrontage_xtrain = train_LotFrontage_main[['LotArea', 'Street', 'LotShape', 'LandSlope', 'LotConfig', 'HouseStyle', 'GarageArea']]\ntrain_LotFrontage_ytrain = train_LotFrontage_main['LotFrontage']\n#--------------------\ntest_LotFrontage = test_LotFrontage[['LotArea', 'Street', 'LotShape', 'LandSlope', 'LotConfig', 'HouseStyle', 'GarageArea']]","d0683e70":"#Lotconfig and Street are falls under Nominal category. So, I used nominal encoding technique to convert it to integer\ntrain_LotFrontage_xtrain_Nomina_Encoding = pd.get_dummies(train_LotFrontage_xtrain[['LotConfig', 'Street']])\ntrain_LotFrontage_xtrain = pd.concat([train_LotFrontage_xtrain, train_LotFrontage_xtrain_Nomina_Encoding], 1)\ntrain_LotFrontage_xtrain = train_LotFrontage_xtrain.drop(['LotConfig', 'Street'], axis = 1)\n#--------------------\n#LotShape, LandSlope and HouseStyle are falls under Ordinal category. So, I used Ordinal encoding technique to convert it to integer\ntrain_LotFrontage_xtrain['LotShape'] = train_LotFrontage_xtrain.LotShape.map({'IR3':0, 'IR2':1, 'IR1': 2, 'Reg': 3})\ntrain_LotFrontage_xtrain['LandSlope'] = train_LotFrontage_xtrain.LandSlope.map({'Sev':0, 'Mod':1, 'Gtl': 2})\ntrain_LotFrontage_xtrain['HouseStyle'] = train_LotFrontage_xtrain.HouseStyle.map({'1Story':0, '1.5Fin':1, '1.5Unf': 2, '2Story':3, '2.5Fin':4, '2.5Unf': 5, 'SFoyer':6, 'SLvl':7})\ntrain_LotFrontage_xtrain = train_LotFrontage_xtrain.drop('LotConfig_FR3', axis = 1)","675d947a":"#Lotconfig and Street are falls under Nominal category. So, I used nominal encoding technique to convert it to integer\ntest_LotFrontage_Nomina_Encoding = pd.get_dummies(test_LotFrontage[['LotConfig', 'Street']])\ntest_LotFrontage = pd.concat([test_LotFrontage, test_LotFrontage_Nomina_Encoding], 1)\ntest_LotFrontage = test_LotFrontage.drop(['LotConfig', 'Street'], axis = 1)\n#--------------------\n#LotShape, LandSlope and HouseStyle are falls under Ordinal category. So, I used Ordinal encoding technique to convert it to integer\ntest_LotFrontage['LotShape'] = test_LotFrontage.LotShape.map({'IR3':0, 'IR2':1, 'IR1': 2, 'Reg': 3})\ntest_LotFrontage['LandSlope'] = test_LotFrontage.LandSlope.map({'Sev':0, 'Mod':1, 'Gtl': 2})\ntest_LotFrontage['HouseStyle'] = test_LotFrontage.HouseStyle.map({'1Story':0, '1.5Fin':1, '1.5Unf': 2, '2Story':3, '2.5Fin':4, '2.5Unf': 5, 'SFoyer':6, 'SLvl':7})","e0404c64":"#By using Random Forest Algorithm I replaced the missing values of LotFrontage with predicted values\nreg_rf = RandomForestRegressor(n_estimators=1000,min_samples_split=2,min_samples_leaf=1,max_features='sqrt',max_depth=25)\nreg_rf.fit(train_LotFrontage_xtrain, train_LotFrontage_ytrain)\ny_pred= reg_rf.predict(test_LotFrontage)\ntest_LotFrontage_main['LotFrontage'] = y_pred\nprint(\"Accuracy on Traing set: \",reg_rf.score(train_LotFrontage_xtrain,train_LotFrontage_ytrain))","8184371d":"\ntrain_file = test_LotFrontage_main.append(train_LotFrontage_main).sort_index()\nprint(train_file[[i for i in train_file.columns if train_file[i].isnull().sum()>0]].isnull().sum())\ntrain_file.head(3)","3ae0ca2c":"train_file = train_file[['LotFrontage','LotArea','Alley','LotShape','Utilities','LandSlope','HouseStyle','OverallQual','OverallCond','YearBuilt','YearRemodAdd','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinSF1','BsmtFinType2','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','HeatingQC','CentralAir','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','KitchenQual','TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','GarageQual','GarageCond','PavedDrive','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','YrSold', 'SalePrice']]","f60372e7":"train_file['Alley'] = np.where(train_file['Alley'] == 'No Alley Access', 0, 1)\ntrain_file['LotShape'] = train_file.LotShape.map({'IR3':0, 'IR2':1, 'IR1': 2, 'Reg': 3})\ntrain_file['Utilities'] = train_file.Utilities.map({'ELO':0, 'NoSeWa':1, 'NoSewr': 2, 'AllPub': 3})\ntrain_file['LandSlope'] = train_file.LandSlope.map({'Sev':0, 'Mod':1, 'Gtl': 2})\ntrain_file['HouseStyle'] = train_file.HouseStyle.map({'1Story':0, '1.5Fin':1, '1.5Unf': 2, '2Story':3, '2.5Fin':4, '2.5Unf': 5, 'SFoyer':6, 'SLvl':7})\ntrain_file['ExterQual'] = train_file.ExterQual.map({'Po':0, 'Fa':1, 'TA': 2, 'Gd': 3,'Ex': 4})\ntrain_file['ExterCond'] = train_file.ExterCond.map({'Po':0, 'Fa':1, 'TA': 2, 'Gd': 3,'Ex': 4})\ntrain_file['BsmtQual'] = train_file.BsmtQual.map({'Po':1, 'Fa':2, 'TA': 3, 'Gd': 4,'Ex': 5, 'No Basement': 0})\ntrain_file['BsmtCond'] = train_file.BsmtCond.map({'Po':1, 'Fa':2, 'TA': 3, 'Gd': 4,'Ex': 5, 'No Basement': 0})\ntrain_file['BsmtExposure'] = train_file.BsmtExposure.map({'No Basement':0, 'No':1, 'Mn': 2, 'Av': 3,'Gd': 4})\ntrain_file['BsmtFinType1'] = train_file.BsmtFinType1.map({'Unf':1, 'LwQ':2, 'Rec': 3, 'BLQ': 4,'ALQ': 5,'GLQ': 6, 'No Basement': 0})\ntrain_file['BsmtFinType2'] = train_file.BsmtFinType2.map({'Unf':1, 'LwQ':2, 'Rec': 3, 'BLQ': 4,'ALQ': 5,'GLQ': 6, 'No Basement': 0})\ntrain_file['HeatingQC'] = train_file.HeatingQC.map({'Po':0, 'Fa':1, 'TA': 2, 'Gd': 3,'Ex': 4})\ntrain_file['CentralAir'] = np.where(train_file['CentralAir'] == 'Y', 1, 0)\ntrain_file['KitchenQual'] = train_file.KitchenQual.map({'Po':0, 'Fa':1, 'TA': 2, 'Gd': 3,'Ex': 4})\ntrain_file['GarageQual'] = train_file.GarageQual.map({'Po':1, 'Fa':2, 'TA': 3, 'Gd': 4,'Ex': 5, 'No Garage': 0})\ntrain_file['GarageCond'] = train_file.GarageCond.map({'Po':1, 'Fa':2, 'TA': 3, 'Gd': 4,'Ex': 5, 'No Garage': 0})\ntrain_file['PavedDrive'] = train_file.PavedDrive.map({'N':0, 'P':1, 'Y': 2})\n\ntrain_file['YearBuilt'] = train_file['YrSold'] - train_file['YearBuilt']\ntrain_file['YearRemodAdd'] = train_file['YrSold'] - train_file['YearRemodAdd']\n\ntrain_file = train_file.rename(columns={\"YearBuilt\": \"BuiltYearsBack\", \"YearRemodAdd\": \"RemodYearsBack\"})\nprint(train_file.shape)\ntrain_file.head(3)","a6da48b2":"# Spliting data for training the model. Splitting the data will be done at the begining of feature seletion phase\nX = train_file.drop('SalePrice', axis = 1)\ny = train_file['SalePrice']\nX_train, X_test, Y_train, Y_test = tts(X, y, test_size=0.20,random_state=42)","d8360487":"Standardscaler = StandardScaler()\nX_train_col = X_train.columns\nX_train_ADS = pd.DataFrame(Standardscaler.fit_transform(X_train),columns = X_train_col )\nX_train_ADS.head(2)","4c622327":"# determine the mutual information\nmutual_info = mutual_info_regression(X_train_ADS.fillna(0), Y_train)\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train_ADS.columns\nmutual_info.sort_values(ascending=False)\n#--------------------\n#Considering the columns for training the model which are atleast 10% of information shared with dependent variable\/feature\nReq_Cols = list(mutual_info[mutual_info>0.1].index)\nReq_Cols","6e24d710":"#Creating the Training ADS with selected columns\nTrain_ADS = X_train_ADS[Req_Cols]\nTrain_ADS.head(3)","bb6be51d":"#Applying Scaling technique to Test Dataset\nStandardscaler = StandardScaler()\nX_test_col = X_test.columns\nX_test_ADS = pd.DataFrame(Standardscaler.fit_transform(X_test),columns = X_test_col )\n#--------------------\n#Creating the Testing ADS with selected columns\nTest_ADS = X_test_ADS[Req_Cols]\nprint(Test_ADS.shape)\nTest_ADS.head(2)\n","1b9683f9":"linear_reg = LinearRegression()\nlinear_reg.fit(Train_ADS, Y_train)\ny_pred= linear_reg.predict(Test_ADS)\nscore_1=r2_score(Y_test,y_pred)\nprint(\"Accuracy on Traing set: \",linear_reg.score(Train_ADS,Y_train))\nprint(\"Accuracy on Testing set: \",linear_reg.score(Test_ADS,Y_test))\nprint(\"R2 score\", score_1)","97288cd5":"from sklearn.neighbors import KNeighborsRegressor\nneigh = KNeighborsRegressor(n_neighbors=2)\nneigh.fit(Train_ADS, Y_train)\ny_pred= neigh.predict(Test_ADS)\nscore_1=r2_score(Y_test,y_pred)\nprint(\"Accuracy on Traing set: \",neigh.score(Train_ADS,Y_train))\nprint(\"Accuracy on Testing set: \",neigh.score(Test_ADS,Y_test))\nprint(\"R2 score\", score_1)","5c75ec2d":"Score = []\nfor i in range(1,40):\n    knn = KNeighborsRegressor(n_neighbors=i)\n    knn.fit(Train_ADS, Y_train)\n    pred_i = knn.predict(Test_ADS)\n    score_1=r2_score(Y_test,pred_i)\n    Score.append(score_1)\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),Score,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=1)\nplt.title('Accuracy vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Accuracy')\nprint(\"Max Accuracy error:-\",max(Score),\"at K =\",Score.index(max(Score))+1)","48203a94":"reg_rf = RandomForestRegressor()\nreg_rf.fit(Train_ADS, Y_train)\ny_pred= reg_rf.predict(Test_ADS)\nscore_1=r2_score(Y_test,y_pred)\nprint(\"Accuracy on Traing set: \",reg_rf.score(Train_ADS,Y_train))\nprint(\"Accuracy on Testing set: \",reg_rf.score(Test_ADS,Y_test))\nprint(\"R2 score\", score_1)","721f944f":"from sklearn.model_selection import RandomizedSearchCV\n#Randomized Search CV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 40)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 40, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 20,25,30,35,40,100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nrf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n\nrf_random.fit(Train_ADS, Y_train)","4dd84630":"rf_random.best_params_","cb5bc042":"reg_rf = RandomForestRegressor(n_estimators=1220,min_samples_split=2,min_samples_leaf=1,max_features='sqrt',max_depth=33)\nreg_rf.fit(Train_ADS, Y_train)\ny_pred= reg_rf.predict(Test_ADS)\nscore_1=r2_score(Y_test,y_pred)\nprint(\"Accuracy on Traing set: \",reg_rf.score(Train_ADS,Y_train))\nprint(\"Accuracy on Testing set: \",reg_rf.score(Test_ADS,Y_test))\nprint(\"R2 score\", score_1)","6affe981":"#!pip install xgboost\nimport xgboost as XGB\n\nxgb_model = XGB.XGBRegressor()\nxgb_model.fit(Train_ADS, Y_train)\ny_pred= xgb_model.predict(Test_ADS)\nscore_1=r2_score(Y_test,y_pred)\nprint(\"Accuracy on Traing set: \",xgb_model.score(Train_ADS,Y_train))\nprint(\"Accuracy on Testing set: \",xgb_model.score(Test_ADS,Y_test))\nprint(\"R2 score\", score_1)","c9c1d3e8":"learning_rate = [0.01, 0.1]\nmax_depth = [int(x) for x in np.linspace(5, 40, num = 6)]\nmin_child_weight = [int(x) for x in np.linspace(1, 20, num = 6)]\nsubsample =  [0.5, 0.7]\ncolsample_bytree = [0.5, 0.7]\nobjective = ['reg:squarederror']\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 40)]\n\n\nrandom_grid = {'learning_rate': learning_rate,\n               'max_depth': max_depth,\n               'min_child_weight': min_child_weight,\n               'subsample': subsample,\n               'colsample_bytree': colsample_bytree,\n               'objective': objective,\n               'n_estimators': n_estimators}\n\n\nrf_random = RandomizedSearchCV(estimator = xgb_model, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n\nrf_random.fit(Train_ADS, Y_train)","3d44e289":"rf_random.best_params_","0d801209":"xgb_model = XGB.XGBRegressor(subsample=0.5, objective='reg:squarederror', n_estimators=1561, min_child_weight=16, max_depth=12, learning_rate=0.01, colsample_bytree=0.5)\nxgb_model.fit(Train_ADS, Y_train)\ny_pred= xgb_model.predict(Test_ADS)\nscore_1=r2_score(Y_test,y_pred)\nprint(\"Accuracy on Traing set: \",xgb_model.score(Train_ADS,Y_train))\nprint(\"Accuracy on Testing set: \",xgb_model.score(Test_ADS,Y_test))\nprint(\"R2 score\", score_1)","fed51e94":"test_file[[i for i in test_file.columns if test_file[i].isnull().sum()>0]].isnull().sum()","13d786bd":"test_file['BuiltYearsBack'] = test_file['YrSold'] - test_file['YearBuilt']\ntest_file['RemodYearsBack'] = test_file['YrSold'] - test_file['YearRemodAdd']\n\ntrain_LotFrontage_main = test_file[test_file.LotFrontage.isnull() != True]\ntest_LotFrontage_main = test_file[test_file.LotFrontage.isnull() == True]\n\n\ntest_LotFrontage = test_LotFrontage_main.drop('LotFrontage', axis = 1)\ntrain_LotFrontage_xtrain = train_LotFrontage_main[['LotArea', 'Street', 'LotShape', 'LandSlope', 'LotConfig', 'HouseStyle', 'GarageArea']]\ntrain_LotFrontage_ytrain = train_LotFrontage_main['LotFrontage']\n\ntest_LotFrontage = test_LotFrontage[['LotArea', 'Street', 'LotShape', 'LandSlope', 'LotConfig', 'HouseStyle', 'GarageArea']]\n\n#Lotconfig and Street are falls under Nominal category. So, I used nominal encoding technique to convert it to integer\ntrain_LotFrontage_xtrain_Nomina_Encoding = pd.get_dummies(train_LotFrontage_xtrain[['LotConfig', 'Street']])\ntrain_LotFrontage_xtrain = pd.concat([train_LotFrontage_xtrain, train_LotFrontage_xtrain_Nomina_Encoding], 1)\ntrain_LotFrontage_xtrain = train_LotFrontage_xtrain.drop(['LotConfig', 'Street'], axis = 1)\n\n#LotShape, LandSlope and HouseStyle are falls under Ordinal category. So, I used Ordinal encoding technique to convert it to integer\ntrain_LotFrontage_xtrain['LotShape'] = train_LotFrontage_xtrain.LotShape.map({'IR3':0, 'IR2':1, 'IR1': 2, 'Reg': 3})\ntrain_LotFrontage_xtrain['LandSlope'] = train_LotFrontage_xtrain.LandSlope.map({'Sev':0, 'Mod':1, 'Gtl': 2})\ntrain_LotFrontage_xtrain['HouseStyle'] = train_LotFrontage_xtrain.HouseStyle.map({'1Story':0, '1.5Fin':1, '1.5Unf': 2, '2Story':3, '2.5Fin':4, '2.5Unf': 5, 'SFoyer':6, 'SLvl':7})\ntrain_LotFrontage_xtrain = train_LotFrontage_xtrain.drop('LotConfig_FR3', axis = 1)\n\n#Lotconfig and Street are falls under Nominal category. So, I used nominal encoding technique to convert it to integer\ntest_LotFrontage_Nomina_Encoding = pd.get_dummies(test_LotFrontage[['LotConfig', 'Street']])\ntest_LotFrontage = pd.concat([test_LotFrontage, test_LotFrontage_Nomina_Encoding], 1)\ntest_LotFrontage = test_LotFrontage.drop(['LotConfig', 'Street','LotConfig_FR3'], axis = 1)\n\n#LotShape, LandSlope and HouseStyle are falls under Ordinal category. So, I used Ordinal encoding technique to convert it to integer\ntest_LotFrontage['LotShape'] = test_LotFrontage.LotShape.map({'IR3':0, 'IR2':1, 'IR1': 2, 'Reg': 3})\ntest_LotFrontage['LandSlope'] = test_LotFrontage.LandSlope.map({'Sev':0, 'Mod':1, 'Gtl': 2})\ntest_LotFrontage['HouseStyle'] = test_LotFrontage.HouseStyle.map({'1Story':0, '1.5Fin':1, '1.5Unf': 2, '2Story':3, '2.5Fin':4, '2.5Unf': 5, 'SFoyer':6, 'SLvl':7})\n\n\ntrain_LotFrontage_xtrain[[i for i in train_LotFrontage_xtrain.columns if train_LotFrontage_xtrain[i].isnull().sum()>0]].isnull().sum()\ntest_file['GarageArea'] = test_file['GarageArea'].fillna(test_file.groupby('HouseStyle')['GarageArea'].transform('mean'))\n\n","02a58214":"#By using Random Forest Algorithm I replaced the missing values of LotFrontage with predicted values\nreg_rf = RandomForestRegressor(n_estimators=1000,min_samples_split=2,min_samples_leaf=1,max_features='sqrt',max_depth=25)\nreg_rf.fit(train_LotFrontage_xtrain.fillna(0), train_LotFrontage_ytrain)\ny_pred= reg_rf.predict(test_LotFrontage)\ntest_LotFrontage_main['LotFrontage'] = y_pred\nprint(\"Accuracy on Traing set: \",reg_rf.score(train_LotFrontage_xtrain.fillna(0),train_LotFrontage_ytrain))\n#--------------------\nTest_ADS = test_LotFrontage_main.append(train_LotFrontage_main).sort_index()\nTest_ADS = Test_ADS[Req_Cols]\nTest_ADS['BsmtQual'] = np.where(Test_ADS['BsmtQual'].isnull() == True, \"No Basement\", Test_ADS['BsmtQual'])\nTest_ADS['BsmtFinType1'] = np.where(Test_ADS['BsmtFinType1'].isnull() == True, \"No Basement\", Test_ADS['BsmtFinType1'])\nTest_ADS['HouseStyle'] = Test_ADS.HouseStyle.map({'1Story':0, '1.5Fin':1, '1.5Unf': 2, '2Story':3, '2.5Fin':4, '2.5Unf': 5, 'SFoyer':6, 'SLvl':7})\nTest_ADS['ExterQual'] = Test_ADS.ExterQual.map({'Po':0, 'Fa':1, 'TA': 2, 'Gd': 3,'Ex': 4})\nTest_ADS['BsmtQual'] = Test_ADS.BsmtQual.map({'Po':1, 'Fa':2, 'TA': 3, 'Gd': 4,'Ex': 5, 'No Basement': 0})\nTest_ADS['BsmtFinType1'] = Test_ADS.BsmtFinType1.map({'Unf':1, 'LwQ':2, 'Rec': 3, 'BLQ': 4,'ALQ': 5,'GLQ': 6, 'No Basement': 0})\nTest_ADS['HeatingQC'] = Test_ADS.HeatingQC.map({'Po':0, 'Fa':1, 'TA': 2, 'Gd': 3,'Ex': 4})\nTest_ADS['KitchenQual'] = Test_ADS.KitchenQual.map({'Po':0, 'Fa':1, 'TA': 2, 'Gd': 3,'Ex': 4})\nTest_ADS.head(4)","d68426a5":"Test_col = Test_ADS.columns\nTest_ADS = pd.DataFrame(Standardscaler.fit_transform(Test_ADS),columns = Test_col )\nprint(Train_ADS.shape)\nTest_ADS.head(2)","1374d110":"#Applying Scaling technique to Test Dataset\nStandardscaler = StandardScaler()\nX_test_col = X.columns\nTrain_ADS = pd.DataFrame(Standardscaler.fit_transform(X),columns = X_test_col )\n\n#Creating the Testing ADS with selected columns\nTrain_ADS = Train_ADS[Req_Cols]\nprint(Train_ADS.shape)\nTrain_ADS.head(2)\nY_train = y","6bf78a14":"xgb_model = XGB.XGBRegressor(subsample=0.5,  n_estimators=1561, min_child_weight=16, max_depth=12, learning_rate=0.01, colsample_bytree=0.5)\nxgb_model.fit(Train_ADS, Y_train)\ny_pred= xgb_model.predict(Test_ADS)\n\nprint(\"Accuracy on Traing set: \",xgb_model.score(Train_ADS,Y_train))\n","baf4db24":"test_file['Pred Price'] = y_pred\n\nSub_file = test_file[['Id','Pred Price']]\nSub_file.head(10)\n#Sub_file.to_csv('\/submission.csv',index=False)","e5b16e56":"In linear reegression we didn't obtain good acuracy. So, Lets try with another alogorithm: RandomForest Regressor\n\n## KNeighbors Regressor ","bf9367fd":"## Feature Engineering - Data Cleaning and Creating an ADS for Analysis","0d21cf54":"Before training the model, we make sure that all the features are in the form of int or float. But we have categorical features in our dataset. For \"Logconig, Street\" I'm used one hot enoding because those two columns are nominal categories for for \"LotShape, LandSlope, HouseStyle\" I'm used ordinal encoding technique.","298c7c0b":"I've tried with three algorithms (Linear Regression, KNN, Random Forest) and hyperparameter tuning for XGB regressor. If you like the notebook the Please Upvote add up your comments to this nootebook Happy coding","8336dda6":"After hyper parameter tuning we obtain good acuracy. But, Lets try with another alogorithm: XGB Regressor\n\n## XGBoost Regressor ","a822ed14":"## Linear regression","8e2da06a":"After hyper parameter tuning we obtain good acuracy. But, Lets try with another alogorithm: RandomForest Regressor\n\n## RandomForest Regressor ","bc423ef0":"## Feature Engineering & Feature Selection For Test Dataset","df6ab977":"## Data Information\n- Null value finding\n- Table Information\n- 5pt Summary","015ef0f5":"In RandomForest Regressor, accuracy is comparitively better than than above two models. To improve the accuracy, hyperparameter tuing is performed","c0092226":"Scaled the data for each metrics by using feature scaling techniques to reduce the bias, to normalize the data within a range and speeding up the calculation while training the model. After applying the Standard Scaler, data range is in between -3 to 3\n","2990da3a":"#### Treating the Null Values for LotFrontage \n- In our dataset, We have 259 null values for lotfrontage, if we replace with mean\/median or some random values, It might effects on accuracy or model performance. To make model more efficiency, I'm predicting the value by consdering the relavent parameters of lotfrontage.\n- Here I'm making LotFrontage as a dependent variable and rest all are independent variables. All the null values of LotFrontage, I'm considering as test dataset. So that we can predict the missing values.","2a06441c":"Compare with Linear regressor, accuracy for KNN with K=2 is better. So I performed hyper parameter tunning to find the optimal K value","97f41992":"## Feature Selection-Information gain - mutual information In Regression Problem Statements\n\n- Feature Selection-Information gain - mutual information In Regression Problem Statements Mutual Information\n\n- Estimate mutual information for a continuous target variable.\n\n- Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n\n- The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances\n\n- Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable.\n","4a64e0a3":"## Sales price prediction for test dataset"}}