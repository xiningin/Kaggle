{"cell_type":{"048f489f":"code","7626cb0c":"code","62841b55":"code","4d9e5ef7":"code","cf01ba7f":"code","7e70a9cf":"code","d653e5d3":"code","d521d687":"code","bb3e2836":"code","817a2f89":"code","9d734878":"code","4daac5da":"code","6c4c4ca0":"code","37966023":"code","3526bb7d":"code","afaaa535":"code","fcf68c2c":"code","5bf071a1":"code","0fee18b0":"code","56b8eff1":"code","14e5243e":"code","f3ab51c3":"code","ddf8e375":"code","c3c3589b":"code","5731260a":"code","d04033b1":"code","6a2d2596":"code","b584c8bd":"code","6321cc81":"code","edc62b71":"code","a458ebcc":"code","6fdc3a6c":"code","8ecd6a80":"code","f9b65134":"code","350c9e28":"code","b32b904f":"code","bca81c89":"code","4dec6616":"code","c79f977c":"code","a7fe3772":"code","aca0d3f5":"code","b3cc9007":"code","c080f1d0":"code","99bd8641":"code","c846e517":"markdown","c1751719":"markdown","46c1e310":"markdown","e826c24c":"markdown","f53062f5":"markdown","af3b2ab5":"markdown","dba60c36":"markdown","14367b4f":"markdown","6a2d3178":"markdown","84525f38":"markdown","7675a2a6":"markdown","b437d83c":"markdown","5591d5bf":"markdown","69b1c24a":"markdown","f3478f8c":"markdown","75568d0c":"markdown","841d038f":"markdown","0c15a5d2":"markdown","1d38e3b6":"markdown","c979d434":"markdown","68232200":"markdown","23e5cd3a":"markdown","5a22141c":"markdown","bbaf1126":"markdown","72e53490":"markdown","1efa461b":"markdown","3009c53f":"markdown"},"source":{"048f489f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport IPython\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7626cb0c":"pd.set_option('max_columns', None) #to display all the columns\nX = pd.read_csv(\"..\/input\/summeranalytics2020\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/summeranalytics2020\/test.csv\")\nX.head()","62841b55":"y = X['Attrition']\nX.drop(['Attrition'], axis =1, inplace=True)","4d9e5ef7":"X.shape","cf01ba7f":"from sklearn.model_selection import train_test_split\nX.drop(columns='Id',inplace=True)\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.3,random_state=1)\n","7e70a9cf":"#check the unique values in object columns to see whether to do hotencoding or labelencoding on them\nobj_dict_train={obj_col: list(X_train[obj_col].unique()) for obj_col in X_train.select_dtypes(include='object')}\nobj_dict_train","d653e5d3":"obj_dict_valid={obj_col: list(X_valid[obj_col].unique()) for obj_col in X_valid.select_dtypes(include='object')}\nobj_dict_valid","d521d687":"label_col = ['BusinessTravel', 'OverTime']     # these two have ordinal categorical data\nhotencode_col = list(set(X.select_dtypes(include='object').columns)-set(label_col))   ","bb3e2836":"X_train = pd.concat([X_train, pd.get_dummies(X_train[hotencode_col])], axis=1)\nX_valid = pd.concat([X_valid, pd.get_dummies(X_valid[hotencode_col])], axis=1)\nX_train.drop(hotencode_col, axis=1, inplace=True)\nX_valid.drop(hotencode_col,axis=1,inplace=True)\n\nX_test = pd.concat([X_test, pd.get_dummies(X_test[hotencode_col])], axis=1)\nX_test.drop(hotencode_col, axis=1, inplace=True)","817a2f89":"#for ordinally encoding BussinessTravel column in train set\ncat1 = pd.Categorical(X_train.BusinessTravel, categories=['Non-Travel', 'Travel_Rarely', 'Travel_Frequently'], ordered=True)\nlabels1, unique1 = pd.factorize(cat1, sort=True)\nX_train['BusinessTravel'] = labels1\n\n#for ordinally encoding OverTime column in train set\ncat2 = pd.Categorical(X_train.OverTime, categories=['No','Yes'], ordered=True)\nlabels2, unique2 = pd.factorize(cat2, sort=True)\nX_train['OverTime'] = labels2\n\n\n#for ordinally encoding BussinessTravel column in validation set\ncat_1 = pd.Categorical(X_valid.BusinessTravel, categories=['Non-Travel', 'Travel_Rarely', 'Travel_Frequently'], ordered=True)\nlabels_1, unique_1 = pd.factorize(cat_1, sort=True)\nX_valid['BusinessTravel'] = labels_1\n\n#for ordinally encoding OverTime column in validation set\ncat_2 = pd.Categorical(X_valid.OverTime, categories=['No','Yes'], ordered=True)\nlabels_2, unique_2 = pd.factorize(cat_2, sort=True)\nX_valid['OverTime'] = labels_2\n\n\n#for ordinally encoding BussinessTravel column in test set\ncat_test1 = pd.Categorical(X_test.BusinessTravel, categories=['Non-Travel', 'Travel_Rarely', 'Travel_Frequently'], ordered=True)\nlabels_test1, unique_test1 = pd.factorize(cat_test1, sort=True)\nX_test['BusinessTravel'] = labels_test1\n\n#for ordinally encoding OverTime column in test set\ncat_test2 = pd.Categorical(X_test.OverTime, categories=['No','Yes'], ordered=True)\nlabels_test2, unique_test2 = pd.factorize(cat_test2, sort=True)\nX_test['OverTime'] = labels_test2","9d734878":"# dropping useless columns\nX_train.drop(columns =['Behaviour','Gender_Female'], inplace=True)\nX_valid.drop(columns =['Behaviour','Gender_Female'], inplace=True)\nX_test.drop(columns =['Behaviour','Gender_Female'], inplace=True)\n","4daac5da":"# As all the columns now have numerical dataype we see the number of unique values to find \n#the columns with numerical data\nX_train.nunique()","6c4c4ca0":"num_cols = [col for col in X_train.columns if X_train[col].nunique()>5]","37966023":"from sklearn.preprocessing import StandardScaler\nnum_scaler = StandardScaler(copy=False)\nscaled_train = pd.DataFrame(num_scaler.fit_transform(X_train[num_cols]))\nscaled_valid = pd.DataFrame(num_scaler.transform(X_valid[num_cols]))\nscaled_test = pd.DataFrame(num_scaler.transform(X_test[num_cols]))\nscaled_test.columns  = num_cols\nscaled_train.columns = num_cols\nscaled_valid.columns = num_cols\nscaled_train.index=X_train.index\nscaled_valid.index=X_valid.index\nscaled_test.index=X_test.index","3526bb7d":"X_train.drop(columns = num_cols, inplace=True)\nX_valid.drop(columns = num_cols, inplace=True)\nX_test.drop(columns = num_cols, inplace =True)\nX_train = pd.concat([X_train, scaled_train], axis =1)\nX_valid = pd.concat([X_valid, scaled_valid], axis =1)\nX_test = pd.concat([X_test, scaled_test], axis =1)","afaaa535":"%matplotlib inline\nmatplotlib.style.use('ggplot')\nsns.set_style('white')","fcf68c2c":"#preparing dataset for plotting\ndata_count = pd.concat([X_train.drop(columns = num_cols), y], axis=1)","5bf071a1":"fig, axes = plt.subplots(round((len(data_count.columns)-1) \/ 3), 3, figsize=(12, 25))\n\nfor i, ax in enumerate(fig.axes):\n    if i <= len(data_count.columns):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        sns.countplot(x=data_count.columns[i], hue=data_count.Attrition, alpha=0.7, data=data_count, ax=ax)\n\nfig.tight_layout()","0fee18b0":"X_train[num_cols].hist(figsize=(14,10),xlabelsize=8, ylabelsize=8);","56b8eff1":"for col in num_cols:\n    plt.figure(figsize=(15,3))\n    sns.scatterplot(x=X[col],y=y)\n    plt.show()","14e5243e":"data_pair = pd.concat([X_train[num_cols],y_train], axis=1)","f3ab51c3":"corr_matrix = data_pair.corr()\ncmap = sns.diverging_palette(300, 0, as_cmap=True)\nplt.figure(figsize=(11,11))\nsns.heatmap(corr_matrix, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","ddf8e375":"rows = round(len(num_cols)*(len(num_cols)-1)\/6)\nfig, axes = plt.subplots(rows, 3, figsize=(16, 60))\ni=0\nj=i+1\nfor __, ax in enumerate(fig.axes):\n    if i==len(num_cols): break\n    ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n    sns.scatterplot(data=data_pair, x=data_pair.columns[i], y=data_pair.columns[j], hue='Attrition', alpha=0.7, ax=ax)\n    j+=1\n    if j==len(num_cols): \n        i+=1\n        j=i+1\nfig.tight_layout()","c3c3589b":"X_train['stock0'] = (X_train.StockOptionLevel==0).astype(int)\nX_valid['stock0'] = (X_valid.StockOptionLevel==0).astype(int)\nX_test['stock0'] = (X_test.StockOptionLevel==0).astype(int)\n\nX_train['jobsatisfied'] = (X_train.JobSatisfaction==4).astype(int)\nX_valid['jobsatisfied'] = (X_valid.JobSatisfaction==4).astype(int)\nX_test['jobsatisfied'] = (X_test.JobSatisfaction==4).astype(int)\n\nX_train['well_communicate'] = (X_train.CommunicationSkill==5).astype(int)\nX_valid['well_communicate'] = (X_valid.CommunicationSkill==5).astype(int)\nX_test['well_communicate'] = (X_test.CommunicationSkill==5).astype(int)\n\nX_train['less_mi'] = (X_train.MonthlyIncome<0.2).astype(int)\nX_valid['less_mi'] = (X_valid.MonthlyIncome<0.2).astype(int)\nX_test['less_mi'] = (X_test.MonthlyIncome<0.2).astype(int)","5731260a":"X_train['mi_age']=X_train.MonthlyIncome*X_train.Age\nX_valid['mi_age']=X_valid.MonthlyIncome*X_valid.Age\nX_test['mi_age']=X_test.MonthlyIncome*X_test.Age\n\nX_train['twy_age']=X_train.TotalWorkingYears*X_train.Age\nX_valid['twy_age']=X_valid.TotalWorkingYears*X_valid.Age\nX_test['twy_age']=X_test.TotalWorkingYears*X_test.Age\n\nX_train['yac_age']=np.square(X_train.YearsAtCompany)\/0.7+np.square(X_train.Age)\nX_valid['yac_age']=np.square(X_valid.YearsAtCompany)\/0.7+np.square(X_valid.Age)\nX_test['yac_age']=np.square(X_test.YearsAtCompany)\/0.7+np.square(X_test.Age)\n\nX_train['mi_dfh']=np.square(X_train.MonthlyIncome)\/0.2-np.square(X_train.DistanceFromHome)\nX_valid['mi_dfh']=np.square(X_valid.MonthlyIncome)\/0.2-np.square(X_valid.DistanceFromHome)\nX_test['mi_dfh']=np.square(X_test.MonthlyIncome)\/0.2-np.square(X_test.DistanceFromHome)\n\nX_train['yac_dfh']=np.square(X_train.YearsAtCompany)\/0.5-np.square(X_train.DistanceFromHome)\nX_valid['yac_dfh']=np.square(X_valid.YearsAtCompany)\/0.5-np.square(X_valid.DistanceFromHome)\nX_test['yac_dfh']=np.square(X_test.YearsAtCompany)\/0.5-np.square(X_test.DistanceFromHome)\n\nX_train['mi_twc']=X_train.MonthlyIncome*X_train.TotalWorkingYears\nX_valid['mi_twc']=X_valid.MonthlyIncome*X_valid.TotalWorkingYears\nX_test['mi_twc']=X_test.MonthlyIncome*X_test.TotalWorkingYears\n\nX_train['mi_yac']=X_train.MonthlyIncome*X_train.YearsAtCompany\nX_valid['mi_yac']=X_valid.MonthlyIncome*X_valid.YearsAtCompany\nX_test['mi_yac']=X_test.MonthlyIncome*X_test.YearsAtCompany\n\nX_train['twy_ywcm']=np.square(X_train.TotalWorkingYears)\/0.6-np.square(X_train.YearsWithCurrManager)\/8\nX_valid['twy_ywcm']=np.square(X_valid.TotalWorkingYears)\/0.6-np.square(X_valid.YearsWithCurrManager)\/8\nX_test['twy_ywcm']=np.square(X_test.TotalWorkingYears)\/0.6-np.square(X_test.YearsWithCurrManager)\/8","d04033b1":"# scaling the newly formed features\nnew_cols =['mi_age','twy_age','yac_age','mi_dfh','yac_dfh','mi_twc','mi_yac','twy_ywcm']\nscaled_train1 = pd.DataFrame(num_scaler.fit_transform(X_train[new_cols]))\nscaled_valid1 = pd.DataFrame(num_scaler.transform(X_valid[new_cols]))\nscaled_test1 = pd.DataFrame(num_scaler.transform(X_test[new_cols]))\nscaled_test1.columns = new_cols\nscaled_train1.columns = new_cols\nscaled_valid1.columns = new_cols\nscaled_train1.index=X_train.index\nscaled_valid1.index=X_valid.index\nscaled_test1.index=X_test.index\n\nX_train.drop(columns = new_cols, inplace=True)\nX_valid.drop(columns = new_cols, inplace=True)\nX_test.drop(columns = new_cols, inplace =True)\nX_train = pd.concat([X_train, scaled_train1], axis =1)\nX_valid = pd.concat([X_valid, scaled_valid1], axis =1)\nX_test = pd.concat([X_test, scaled_test1], axis =1)","6a2d2596":"num_cols2 = [col for col in X_train.columns if X_train[col].nunique()>10]","b584c8bd":"# using boxplot to check for outliers and skewedness\nplt.figure(figsize=(15,5))\nax = sns.boxplot(data=X_train[num_cols2])\nplt.xticks(rotation=90)\nplt.show()","6321cc81":"X_train['twy_ywcm'] = np.log(2+X_train.twy_ywcm\/2)\nX_train['mi_yac'] = np.log(3+X_train.mi_yac)\nX_train['mi_twc'] = np.log(3+X_train.mi_twc)\nX_train['yac_dfh'] = np.log(2+X_train.yac_dfh)\nX_train['mi_dfh'] = np.log(2+X_train.mi_dfh)\nX_train['yac_age'] = np.log(2+X_train.yac_age)\nX_train['twy_age'] = np.sqrt(3+X_train.twy_age)\nX_train['mi_age'] = np.sqrt(3+X_train.mi_age)\nX_train['YearsSinceLastPromotion'] = np.log(1.5+X_train.YearsSinceLastPromotion\/2)\nX_train['MonthlyIncome'] = np.log(2+X_train.MonthlyIncome)\nX_train['TotalWorkingYears'] = np.log(2+X_train.TotalWorkingYears)\nX_train['YearsAtCompany'] = np.sqrt(3+X_train.YearsAtCompany\/5)","edc62b71":"# again plot the boxplot to see the difference\nplt.figure(figsize=(15,5))\nax = sns.boxplot(data=X_train[num_cols2])\nplt.xticks(rotation=90)\nplt.show()","a458ebcc":"# creating same features for validation set\nX_valid['twy_ywcm'] = np.log(2+X_valid.twy_ywcm\/2)\nX_valid['mi_yac'] = np.log(3+X_valid.mi_yac)\nX_valid['mi_twc'] = np.log(3+X_valid.mi_twc)\nX_valid['yac_dfh'] = np.log(2+X_valid.yac_dfh)\nX_valid['mi_dfh'] = np.log(2+X_valid.mi_dfh)\nX_valid['yac_age'] = np.log(2+X_valid.yac_age)\nX_valid['twy_age'] = np.sqrt(3+X_valid.twy_age)\nX_valid['mi_age'] = np.sqrt(3+X_valid.mi_age)\nX_valid['YearsSinceLastPromotion'] = np.log(1.5+X_valid.YearsSinceLastPromotion\/2)\nX_valid['MonthlyIncome'] = np.log(2+X_valid.MonthlyIncome)\nX_valid['TotalWorkingYears'] = np.log(2+X_valid.TotalWorkingYears)\nX_valid['YearsAtCompany'] = np.sqrt(3+X_valid.YearsAtCompany\/5)","6fdc3a6c":"# creating same features for test set\nX_test['twy_ywcm'] = np.log(2+X_test.twy_ywcm\/2)\nX_test['mi_yac'] = np.log(3+X_test.mi_yac)\nX_test['mi_twc'] = np.log(3+X_test.mi_twc)\nX_test['yac_dfh'] = np.log(2+X_test.yac_dfh)\nX_test['mi_dfh'] = np.log(2+X_test.mi_dfh)\nX_test['yac_age'] = np.log(2+X_test.yac_age)\nX_test['twy_age'] = np.sqrt(3+X_test.twy_age)\nX_test['mi_age'] = np.sqrt(3+X_test.mi_age)\nX_test['YearsSinceLastPromotion'] = np.log(1.5+X_test.YearsSinceLastPromotion\/2)\nX_test['MonthlyIncome'] = np.log(2+X_test.MonthlyIncome)\nX_test['TotalWorkingYears'] = np.log(2+X_test.TotalWorkingYears)\nX_test['YearsAtCompany'] = np.sqrt(3+X_test.YearsAtCompany\/5)","8ecd6a80":"X_train_final = pd.concat([X_train,X_valid],axis=0)\ny_train_final = pd.concat([y_train,y_valid],axis=0)\nX_train_final.reset_index(drop=True)\ny_train_final.reset_index(drop=True)","f9b65134":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score,accuracy_score","350c9e28":"num_iters=np.arange(200,501,50)\nC = np.arange(0.1,3,0.1, dtype=np.float32)\narg=[]\nfor i in C:\n    for j in num_iters: arg.append((j,i))","b32b904f":"# model for checking the validation score\ndef model(arg):\n    num_iters = arg[0]\n    C=arg[1]\n    lr=LogisticRegression(solver='liblinear', max_iter=num_iters,C=C, verbose=0, random_state=7)\n    lr.fit(X_train,y_train)\n    predictions = lr.predict(X_valid)\n    return roc_auc_score(y_valid,predictions)","bca81c89":"for params in arg:\n    score = model(params)\n    print(\"C={}, iterations={} : {}\".format(params[1],params[0],score))\n","4dec6616":"\nlr_final = LogisticRegression( solver='liblinear',C=1.8, max_iter=300,random_state=7)\nlr_final.fit(X_train,y_train)\npred_lr = lr_final.predict_proba(X_valid)[:,1]\nroc_auc_score(y_valid,pred_lr)","c79f977c":"feature_weights = np.array(lr_final.coef_).reshape(-1)","a7fe3772":"feature_val = pd.DataFrame({'features':X_train.columns, 'weights':feature_weights})\nfeature_val.sort_values(by='weights', ascending=False, inplace=True)\nplt.figure(figsize=(14,16))\nsns.barplot(y='features',x='weights',data=feature_val,color='blue')\nplt.show()","aca0d3f5":"#Columns which have absolute value of weights less than 0.1\nless_weight_cols= feature_val[abs(feature_val.weights)<0.1].features.to_list()\nless_weight_cols","b3cc9007":"#---------------\nX_train_final.drop(columns=less_weight_cols,inplace=True)\n\nX_test.drop(columns=less_weight_cols,inplace=True)","c080f1d0":"lr_final.fit(X_train_final,y_train_final)\nlr_final","99bd8641":"final_pred = lr_final.predict_proba(X_test.drop(columns='Id'))[:,1]\noutput = pd.concat([X_test.Id,pd.Series(final_pred)], axis=1)\noutput.columns=['Id','Attrition']\noutput.set_index('Id')\noutput.to_csv(\"submission.csv\",index=False)","c846e517":"## Model \n","c1751719":"See the transformations have decreased the skewness of the data. Although it is better than what we originally had, still some improvements can be done. I shall leave it to you to experiment with more features and let me know if you get better results.","46c1e310":"I know I have not given much explaination here as to how I engineered these features. I tried many combinations and after plotting them, I have chosen these set of features. If you  have any confusion or any suggestion, feel free to ping me.","e826c24c":"## Feature Selection\n\nSo what is the need of Feature Selection?\n* It enables the machine learning algorithm to train faster.\n* It reduces the complexity of a model and makes it easier to interpret.\n* It improves the accuracy of a model if the right subset is chosen.","f53062f5":"### Ordinal Encoding","af3b2ab5":"## Encoding","dba60c36":"We shall use these plots later on for feature engineering. Now lets visualize numerical data.","14367b4f":"### Logistic Regression","6a2d3178":"# Employee Attrition Prediction\n\nFirst of all I would like to thank Consultancy and Analytics Club of IIT Guwahati, for conducting such a wonderful hackathon. It was really fun in applying what we learned in the course. Also this has been an ice breaker for me as from now onwards I can start kaggling. Here I share my first kernel, do upvote it if you find it helpful.\n\nAs the COVID-19 keeps unleashing its havoc, the world continues to get pushed into the crisis of the great economic recession, more and more companies start to cut down their underperforming employees. Companies firing hundreds and thousands of Employees is a typical headline today. Cutting down employees or reducing an employee salary is a tough decision to take. Here in this project we look at various parameters responsible for attrition of employee and at the end will build a model to predict Employee Attrition.","84525f38":"## Data Visualization\n\n> Data visualization is useful for data cleaning, exploring data structure, detecting outliers and unusual groups, identifying trends and clusters, spotting local patterns, evaluating modeling output, and presenting results.\n\nThis is what the google says (XD).","7675a2a6":"These are **very important** plots for doing feature engineering. We shall use these visualizations to combine two columns to form new features. Lets begin the feature engineering part.","b437d83c":"As we can see many columns are skewed and have outliers. To treat them, we shall perform some transformations (`log` or `sqrt` mainly) on the selected columns to minimize their skewness. Again I have already tried many combinations before choosing the below transformations.","5591d5bf":"Machine learning algorithm just sees number \u2014 if there is a vast difference in the range say few ranging in thousands and few ranging in the tens, and it makes the underlying assumption that higher ranging numbers have superiority of some sort. So these more significant number starts playing a more decisive role while training the model.\nSo these more significant number starts playing a more decisive role while training the model. Thus feature scaling is needed to bring every feature in the same footing without any upfront importance.\n\nFor more information check out [why we should do feature scaling?](http:\/\/https:\/\/towardsdatascience.com\/all-about-feature-scaling-bcc0ad75cb35)","69b1c24a":"### Hot Encoding","f3478f8c":"### Numerical Features","75568d0c":"Here we can see some columns like\n`MonthlyIncome`, `TotalWorkingYears` , `YearsAtCompany`, `YearsInCurrentRole` and `YearsWithCurrentManager` have a rough threshold which divides the the two categories.","841d038f":"We shall look at the numerical columns to treat any skewed columns if present.","0c15a5d2":"## Feature Scaling","1d38e3b6":"## Feature Engineering","c979d434":"### Handling Skewed columns\n","68232200":"We can see that the columns are skewed. We shall come back to it later. ","23e5cd3a":"### Numerical Data","5a22141c":"## Importing libraries and Data","bbaf1126":"We checked for the unique values in validation set as well in oreder to check that both training set and validation set have the same unique values, otherwise label encoding will fail.","72e53490":"## Output","1efa461b":"### Categorical Features","3009c53f":"### Categorical Data"}}