{"cell_type":{"3931ff90":"code","ea415b26":"code","fff05f31":"code","48b4a826":"code","9813d26e":"code","f0a65b07":"code","89271148":"code","36830b54":"code","d8c4de7e":"code","04c6a6cf":"code","09c2e339":"code","17ae3c28":"code","dcdda71a":"code","a1e5c496":"code","7ec1e219":"code","da42902d":"code","ea95b9d0":"code","ad803ad0":"code","d0384d6c":"code","d88b5be3":"code","c40d9225":"code","2f952997":"code","340ac8f7":"code","61b20162":"code","460e30e8":"code","7a7404a3":"code","018314a4":"code","19e2ca57":"code","9aa224c2":"code","5ba413ea":"code","c89af112":"code","719946b0":"code","b1fd82e5":"code","c8e87dee":"code","86e57ae0":"markdown","1cb07d06":"markdown","d72ee277":"markdown","2d491264":"markdown","4af35c5e":"markdown","aa689914":"markdown","b056a203":"markdown","5ca9d3eb":"markdown","b5b0bcc9":"markdown","647e5c7c":"markdown","71fcc5c4":"markdown","346a5f79":"markdown","44cbd074":"markdown","0d6014b0":"markdown","d73d3b7d":"markdown","173fcaae":"markdown","7c50ad5b":"markdown","70301905":"markdown","93dc5d67":"markdown","6c74d42d":"markdown"},"source":{"3931ff90":"import spacy\nnlp = spacy.load('en_core_web_sm')\n\n# Create a Doc object\ndoc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')\n\n# Print each token separately\nfor token in doc:\n    print(token.text, token.pos_, token.dep_)","ea415b26":"nlp.pipeline","fff05f31":"mystring = '\"We\\'re going on holiday with my wife\"'\nprint(mystring)","48b4a826":"doc = nlp(mystring)\n\nfor token in doc:\n    print(token.text, end=' | ')","9813d26e":"doc1 = nlp(u'A 5km NYC cab ride costs $10.30')\n\nfor t in doc1:\n    print(t)","f0a65b07":"doc1[2:5]","89271148":"for token in doc1:\n    print(token.text, end=' | ')\n\nprint('\\n----')\n\nfor ent in doc1.ents:\n    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))","36830b54":"from spacy import displacy\ndisplacy.render(doc1, style='dep', jupyter=True, options={'distance': 110})","d8c4de7e":"import nltk\n\nfrom nltk.stem.porter import *\n\np_stemmer = PorterStemmer()\n\nwords = ['cared','university','fairly','easily','singing', \n       'sings','sung','singer','sportingly'] \n\nfor word in words:\n    print(word+' --> '+p_stemmer.stem(word))","04c6a6cf":"\nfrom nltk.stem.snowball import SnowballStemmer \n#the stemmer requires a language parameter \nsnow_stemmer = SnowballStemmer(language='english') \n  \n#list of tokenized words \nwords = ['cared','university','fairly','easily','singing', \n       'sings','sung','singer','sportingly'] \n  \n#stem's of each word \nstem_words = [] \nfor w in words: \n    x = snow_stemmer.stem(w) \n    stem_words.append(x) \n      \n#print stemming results \nfor e1,e2 in zip(words,stem_words): \n    print(e1+' ----> '+e2)","09c2e339":"doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran today\")\n\nfor token in doc1:\n    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)","17ae3c28":"from spacy.matcher import Matcher\nm_tool = Matcher(nlp.vocab)","dcdda71a":"p1 = [{'LOWER': 'quickbrownfox'}]\np2 = [{'LOWER': 'quick'}, {'IS_PUNCT': True}, {'LOWER': 'brown'}, {'IS_PUNCT': True}, {'LOWER': 'fox'}]\np3 = [{'LOWER': 'quick'}, {'LOWER': 'brown'}, {'LOWER': 'fox'}]\np4 =  [{'LOWER': 'quick'}, {'LOWER': 'brownfox'}]","a1e5c496":"m_tool.add('QBF', None, p1, p2, p3, p4)\n\n#apply the matcher on a text document and see if we can get any match\nsentence = nlp(u'The quick-brown-fox jumps over the lazy dog. The quick brown fox eats well. \\\n               the quickbrownfox is dead. the dog misses the quick brownfox')\n\nphrase_matches = m_tool(sentence)\nprint(phrase_matches )","7ec1e219":"import numpy as np\nimport pandas as pd\n\ndata = pd.read_csv('..\/input\/smsspam\/smsspamcollection.tsv', sep='\\t')\ndata.head()\n","da42902d":"len(data)","ea95b9d0":"data.isnull().sum()","ad803ad0":"data['label'].unique()","d0384d6c":"data['label'].value_counts()","d88b5be3":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.xscale('log')\nbins = 1.15**(np.arange(0,50))\nplt.hist(data[data['label']=='ham']['length'],bins=bins,alpha=0.9)\nplt.hist(data[data['label']=='spam']['length'],bins=bins,alpha=0.8)\nplt.legend(('ham','spam'))\nplt.show()","c40d9225":"plt.xscale('log')\nbins = 1.5**(np.arange(0,15))\nplt.hist(data[data['label']=='ham']['punct'],bins=bins,alpha=0.8)\nplt.hist(data[data['label']=='spam']['punct'],bins=bins,alpha=0.8)\nplt.legend(('ham','spam'))\nplt.show()","2f952997":"X = data[['length','punct']]  #  double set of brackets\ny = data['label']","340ac8f7":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nprint('Training Data Shape:', X_train.shape)\nprint('Testing Data Shape: ', X_test.shape)","61b20162":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(solver='lbfgs')\n\nlr_model.fit(X_train, y_train)","460e30e8":"from sklearn import metrics\n\n# Create a prediction set:\npredictions = lr_model.predict(X_test)\n\n# Print a confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))","7a7404a3":"df = pd.DataFrame(metrics.confusion_matrix(y_test,predictions), index=['ham','spam'], columns=['ham','spam'])\ndf","018314a4":"print(metrics.classification_report(y_test,predictions))","19e2ca57":"print(metrics.accuracy_score(y_test,predictions))","9aa224c2":"from sklearn.naive_bayes import MultinomialNB\n\nnb_model = MultinomialNB()\n\nnb_model.fit(X_train, y_train)","5ba413ea":"predictions = nb_model.predict(X_test)\nprint(metrics.confusion_matrix(y_test,predictions))","c89af112":"print(metrics.classification_report(y_test,predictions))","719946b0":"print(metrics.accuracy_score(y_test,predictions))","b1fd82e5":"from sklearn.svm import SVC\nsvc_model = SVC(gamma='auto')\nsvc_model.fit(X_train,y_train)\n\npredictions = svc_model.predict(X_test)\nprint(metrics.confusion_matrix(y_test,predictions))","c8e87dee":"print(metrics.accuracy_score(y_test,predictions))","86e57ae0":"# **Vocabulary and Matching**\n\nThe spaCy library comes with Matcher tool that can be used to specify custom rules for phrase matching. The process to use the Matcher tool is pretty straight forward. ","1cb07d06":"# **Train a SVM(Support Vector Machine)**\n\nA Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side.\n\n![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_iris_svc_001.png)\n","d72ee277":"One of the simplest multi-class classification tools is logistic regression. Scikit-learn offers a variety of algorithmic solvers","2d491264":"# **Natural Language Processing(NLP)**\n\n**Natural Language Processing**, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language.\nThe ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable.\nMost NLP techniques rely on machine learning to derive meaning from human languages.\n\n\n**Syntactic analysis** and **semantic analysis** are the main techniques used to complete Natural Language Processing tasks.\n\n\n**1. Syntax**\nSyntax refers to the arrangement of words in a sentence such that they make grammatical sense\n* **Lemmatization**: It entails reducing the various inflected forms of a word into a single form for easy analysis.\n* **Morphological segmentation**: It involves dividing words into individual units called morphemes.\n* **Word segmentation**: It involves dividing a large piece of continuous text into distinct units.\n*** Part-of-speech tagging**: It involves identifying the part of speech for every word.\n* **Parsing**: It involves undertaking grammatical analysis for the provided sentence.\n* **Sentence breaking**: It involves placing sentence boundaries on a large piece of text.\n* **Stemming**: It involves cutting the inflected words to their root form.\n\n**2. Semantics**\nSemantics refers to the meaning that is conveyed by a text. Semantic analysis is one of the difficult aspects of Natural Language Processing that has not been fully resolved yet\n\n* **Named entity recognition (NER)**: It involves determining the parts of a text that can be identified and categorized into preset groups. Examples of such groups include names of people and names of places.\n* **Word sense disambiguation**: It involves giving meaning to a word based on the context.\n* **Natural language generation**: It involves using databases to derive semantic intentions and convert them into human language.\n\n1. Tokenization\n1. Stemming\n1. Lemmatization\n1. Scikit-learn\n","4af35c5e":"# **Stemming**\n\nThe process of reducing inflection towards their root forms are called Stemming, this occurs in such a way that depicting a group of relatable words under the same stem, even if the root has no appropriate meaning. \n\n Stemming is a rule-based approach because it slices the inflected words from prefix or suffix as per the need using a set of commonly underused prefix and suffix, like \u201c-ing\u201d, \u201c-ed\u201d, \u201c-es\u201d, \u201c-pre\u201d, etc. It results in a word that is actually not a word.","aa689914":"# **Prefixes, Suffixes and Infixes**\n\nA morpheme is the atomic units carrying meaning in a language\n*  There are two main categories of morphemes\n*  Stem \u2013 the main morpheme in a word (it defines the word meaning)\n*  Affixes \u2013 they add additional meanings of different types\n1. * prefixes \u2013 the recede the stem (unset : un- set; in Italian in-, ri- , dis-,\u2026)\n1. * suffixes \u2013 they follow the stem (boys: boy \u2013s; in Italian \u2013mente, -tore, -zione, ..)\n1. * infixes \u2013 they are inserted into the stem (in some languages \u2013 Tagalog\n Philippines)\n1. * circumfixes \u2013 they precede and follow the stem (in German sagen [to say] the\n past participle is ge- sag -t)\n*  Prefixes and suffixes are often referred to concatenative morphology since a word is obtained by the concatenation of morphemes\n*  Some languages have more complex compositional rules and follow a\n non-concatenative morphology (f.i. involving infixes)\n\n\n![](http:\/\/mrpregnant.com\/wp-content\/uploads\/2016\/05\/image-4-e1464131388625.png)","b056a203":"better accuracy(According to 0.84)","5ca9d3eb":"# **Train a Naive Bayes**\nNaive Bayes methods are a set of supervised learning algorithms based on applying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable\n\n![](https:\/\/www.analyticsvidhya.com\/wp-content\/uploads\/2015\/09\/Bayes_rule-300x172.png)\n\n\nAbove,\n* P(c|x) is the posterior probability of class (c, target) given predictor (x, attributes).\n* P(c) is the prior probability of class.\n* P(x|c) is the likelihood which is the probability of predictor given class.\n* P(x) is the prior probability of predictor.","b5b0bcc9":"In the above script,\n\n* p1 looks for the phrase \"quickbrownfox\"\n* p2 looks for the phrase \"quick-brown-fox\"\n* p3 tries to search for \"qucik brown fox\"\n* p4 looks for the phrase \"quick brownfox\"\nThe token attribute LOWER defines that the phrase should be converted into lower case before matching.","647e5c7c":"better accuracy(According to 0.86)","71fcc5c4":"The optional 'distance' argument sets the distance between tokens. If the distance is made too small, text that appears beneath short arrows may become too compressed to read.","346a5f79":"# **Porter Stemmer**\n\nThe Porter stemming algorithm (or \u2018Porter stemmer\u2019) is a process for removing the commoner morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems.","44cbd074":"# **Snowball Stemmer**\n\n It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer.","0d6014b0":"# **Tokenization**\n\nGiven a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.\n![](http:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/05\/rnn.gif)","d73d3b7d":" confusions is 131+78=**209** and best value compared to others","173fcaae":"The next step is to define the patterns that will be used to filter similar phrases. Suppose we want to find the phrases \"quick-brown-fox\", \"quick brown fox\", \"quickbrownfox\" or \"quick brownfox\"","7c50ad5b":"# **Train-Test Split**\n\nThe train-test split is a technique for evaluating the performance of a machine learning algorithm.\n\nIt can be used for classification or regression problems and can be used for any supervised learning algorithm.\n\nThe procedure involves taking a dataset and dividing it into two subsets. The first subset is used to fit the model and is referred to as the training dataset. The second subset is not used to train the model; instead, the input element of the dataset is provided to the model, then predictions are made and compared to the expected values. This second dataset is referred to as the test dataset.\n\nTrain Dataset: Used to fit the machine learning model.\nTest Dataset: Used to evaluate the fit machine learning model.\nThe objective is to estimate the performance of the machine learning model on new data: data not used to train the model.","70301905":"# **Lemmatization**\n\nLemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source\n\n![](http:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/img100.png)","93dc5d67":"# **Scikit-learn**\n\nScikit-learn is probably the most useful library for machine learning in Python. The sklearn library contains a lot of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction.\n\n![](https:\/\/www.analyticsvidhya.com\/wp-content\/uploads\/2015\/01\/ml_map.jpg)","6c74d42d":"From the output, you can see that four phrases have been matched. The first long number in each output is the id of the phrase matched, the second and third numbers are the starting and ending positions of the phrase"}}