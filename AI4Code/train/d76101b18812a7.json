{"cell_type":{"04bd86e7":"code","1962df57":"code","c0be54ba":"code","6e4ca853":"code","67a075aa":"code","a4f0d64a":"code","b109f524":"code","fcf883ae":"code","09405128":"code","07c7b367":"code","6da5542b":"code","b14e4e12":"code","36b51df0":"code","2a7230bd":"code","9f9a3a24":"code","1cb47aa6":"code","0bbb1e12":"code","d5f822b5":"code","608c696f":"code","3a7a9843":"code","9e4c7c7b":"code","6a5d2265":"code","fac8f59e":"code","672c9f7e":"code","ee887dee":"markdown","54a284df":"markdown","f9521597":"markdown","143cb55a":"markdown","aeee79e6":"markdown","f4796f27":"markdown","8a9a8f41":"markdown","85739e42":"markdown","1232f21b":"markdown","0bfcd99e":"markdown","2c25dbc7":"markdown","fe9e0a33":"markdown","16d39587":"markdown"},"source":{"04bd86e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1962df57":"# import libraries  \nimport nltk\nimport re, random, os\nimport string, pprint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# spacy for basic preprocessing, optional, can use nltk as well (lemmatisation etc.)\nimport spacy\n\n# gensim for LDA \nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\n#from pyLDAvis import gensim_models as pg\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","c0be54ba":"df = pd.read_csv('..\/input\/amazon-product-reviews-dataset\/7817_1.csv')\nprint(df.shape)\ndf.head()","6e4ca853":"# filter for product id = amazon echo\ndf = df[df['asins']==\"B01BH83OOM\"]\nprint(df.shape)\ndf.head()","67a075aa":"# tokenize using gensim simple_preprocess\ndef sent_to_words(sentences, deacc=True): # deacc=True removes punctuations\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence)))  \n\n\n# convert to list\ndata = df['reviews.text'].values.tolist()\ndata_words = list(sent_to_words(data))\n\n# sample\nprint(data_words[3])","a4f0d64a":"# create list of stop words\n# string.punctuation (from the 'string' module) contains a list of punctuations\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english') + list(string.punctuation)\n","b109f524":"# functions for removing stopwords and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","fcf883ae":"# call functions\n\n# remove stop words\ndata_words_nostops = remove_stopwords(data_words)\n\n# initialize spacy 'en' model, use only tagger since we don't need parsing or NER \n# python3 -m spacy download en\nnlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n\n# lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[3])","09405128":"# compare the nostop, lemmatised version with the original one\n# note that speakers is lemmatised to speaker; \nprint(' '.join(data_words[3]), '\\n')\nprint(' '.join(data_lemmatized[3]))","07c7b367":"# create dictionary and corpus\n# create dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create corpus\ncorpus = [id2word.doc2bow(text) for text in data_lemmatized]\n\n# sample\nprint(corpus[2])","6da5542b":"# human-readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","b14e4e12":"# help(gensim.models.ldamodel.LdaModel)\n# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=10, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","36b51df0":"# print the 10 topics\npprint.pprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","2a7230bd":"# coherence score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","9f9a3a24":"# visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","1cb47aa6":"# compute coherence value at various values of alpha and num_topics\ndef compute_coherence_values(dictionary, corpus, texts, num_topics_range, alpha_range):\n    \n    coherence_values = []\n    model_list = []\n    for alpha in alpha_range:\n        for num_topics in num_topics_range:\n            lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                               id2word=dictionary,\n                                               num_topics=num_topics, \n                                               alpha=alpha,\n                                               per_word_topics=True)\n            model_list.append(lda_model)\n            coherencemodel = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n            coherence_values.append((alpha, num_topics, coherencemodel.get_coherence()))\n        \n\n    return model_list, coherence_values","0bbb1e12":"# build models across a range of num_topics and alpha\nnum_topics_range = [2, 6, 10, 15, 20]\nalpha_range = [0.01, 0.1, 1]\nmodel_list, coherence_values = compute_coherence_values(dictionary=id2word, \n                                                        corpus=corpus, \n                                                        texts=data_lemmatized, \n                                                        num_topics_range=num_topics_range, \n                                                        alpha_range=alpha_range)","d5f822b5":"coherence_df = pd.DataFrame(coherence_values, columns=['alpha', 'num_topics', 'coherence_value'])\ncoherence_df","608c696f":"coherence_df.sort_values('coherence_value')","3a7a9843":"# plot\ndef plot_coherence(coherence_df, alpha_range, num_topics_range):\n    plt.figure(figsize=(16,6))\n\n    for i, val in enumerate(alpha_range):\n\n        # subplot 1\/3\/i\n        plt.subplot(1, 3, i+1)\n        alpha_subset = coherence_df[coherence_df['alpha']==val]\n\n        plt.plot(alpha_subset[\"num_topics\"], alpha_subset[\"coherence_value\"])\n        plt.xlabel('num_topics')\n        plt.ylabel('Coherence Value')\n        plt.title(\"alpha={0}\".format(val))\n        plt.ylim([0.30, 1])\n        plt.legend('coherence value', loc='upper left')\n        plt.xticks(num_topics_range)\n\nplot_coherence(coherence_df, alpha_range, num_topics_range)","9e4c7c7b":"#Finally building the LDA Model by selecting \n\nFinal_LDA_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                                  id2word=id2word,\n                                                  num_topics=6,\n                                                  random_state=100,\n                                                  update_every=1,\n                                                  chunksize=100,\n                                                  passes=10,\n                                                  alpha=1,\n                                                  per_word_topics=True)","6a5d2265":"# print the 10 topics\npprint.pprint(Final_LDA_model.print_topics())\ndoc_lda = Final_LDA_model[corpus]","fac8f59e":"# coherence score\ncoherence_model_lda = CoherenceModel(model=Final_LDA_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","672c9f7e":"# visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(Final_LDA_model, corpus, id2word)\nvis","ee887dee":"i just re-code the Amazon Product Reviews - Topic Modelling notebook vote the real one i am just wrote it for learning purpose","54a284df":"Let's now evaluate the model using coherence score.","f9521597":"# Hyperparameter Tuning - Number of Topics and Alpha\nLet's now tune the two main hyperparameters - number of topics and alpha. The strategy typically used is to tune these parameters such that the coherence score is maximised.\n\n","143cb55a":"Important Note: All models are not automatically downloaded with spacy, so you will need to do a python -m spacy downlo","aeee79e6":"The (3, 7) above represents the fact that the word with id=3 appears 7 times in the second document (review), word id 12 appears twice and so on. The nested list below shows the frequencies of words in the first document.\n","f4796f27":"## Description:\nThe dataset is samples of Amazon Ratings for select produts. The reviews are picked randomly and the corpus has nearly 1.6k reviews of different customers.\\ Amazon aims to understand what are the main topics of these reviews to classify them for easier search.\\ Can you build a strong model that differentiates the topics based on the reviews corpus?\n\n## Acknowledgements\nThe dataset is referred from Kaggle.\n\n## Objective:\n* Understand the Dataset & perform the necessary cleanup.\n* Build a strong Topic Modelling Algorithm to classify the topics.","8a9a8f41":"Now lets visualise the topics. The pyLDAvis library comes with excellent interactive visualisation capabilities.","85739e42":"Let's now print the topics found in the dataset.# print the 10 topics\npprint.pprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","1232f21b":"##Preprocessing\nLet's first do some preprocessing. For tokenisation, though one can use NLTK as well, let's try using gensim's simple_preprocess this time. The preprocessing pipeline is mentioned below.\n\n1. Tokenize each review (using gensim)\n2. Remove stop words (including punctuations)\n3. Lemmatize (using spacy)\nThough you can build topic models without lemmatisation, it is actually quite important (and highly recommended) because otherwise you may end up getting topics having similar words for e.g. speaker, speakers etc. (which are basically referring to the same thing - speaker).\n\nNote that lemmatization uses POS tags of words, so we need to specify a list of POS tags - here we've used['NOUN', 'ADJ'] ","0bfcd99e":"Let's now filter the dataframe to only one product - Amazon Echo. If you are not aware of Echo, here's the amazon page.","2c25dbc7":"Gensim's LDA requires the data in a certain format. Firstly, it needs the corpus as a dicionary of id-word mapping, where each word has a unique numeric ID. This is for computationally efficiency purposes. Secondly, it needs the corpus as a term-document frequency matrix which contains the frequency of each word in each document.\n\n","fe9e0a33":"## AI & ML Project - Amazon Product Reviews Topic\n***Domain: E-Commerce***","16d39587":"## Building the Topic Model \nLet's now build the topic model. We'll define 10 topics to start with. The hyperparameter alpha affects sparsity of the document-topic (theta) distributions, whose default value is 1. Similarly, the hyperparameter eta can also be specified, which affects the topic-word distribution's sparsity."}}