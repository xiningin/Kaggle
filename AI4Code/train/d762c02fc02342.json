{"cell_type":{"d4a1b0cd":"code","6a4230fe":"code","f68a2112":"code","53807906":"code","0783dfed":"code","a79473eb":"code","d1035cd7":"code","fafe6962":"code","d8652c32":"code","bcd690c7":"code","b136b4ed":"code","41112efe":"code","8049b419":"code","ef39bc40":"code","9e226ac1":"code","16ee4150":"code","eb76ea60":"code","55d637f0":"code","e38a8409":"code","850096a4":"code","1c73ff7b":"code","5be5f45d":"code","9bd3fb5c":"code","158c4f61":"code","4ac5610e":"code","a86470c3":"code","c6b2a7bb":"code","45e884ff":"code","0b44ff07":"code","0c33b6c8":"code","3073e4c9":"code","b849eb02":"code","f60ff820":"code","c40ab649":"code","51b74039":"code","9a290b9f":"code","0fccecbf":"code","92200078":"code","daaf6e3d":"code","b5400428":"code","52cc8c79":"code","594477aa":"code","0a60daa5":"code","491a3c0a":"code","b23a5acc":"code","a01ff366":"code","50267526":"code","c387e4c0":"code","e981d7a1":"code","ad5c7db1":"code","4e85ac32":"code","dd95d2fc":"markdown","964d3c99":"markdown","7ae4af7d":"markdown","505004d6":"markdown","f16ad743":"markdown","9fda0b1e":"markdown","e5c3a7f1":"markdown","f4cefdbd":"markdown","42f43ac4":"markdown","c73c62e7":"markdown","8642bcb0":"markdown","5078f2e2":"markdown","ecaccd58":"markdown","1f576086":"markdown","f7079571":"markdown","eec0ea51":"markdown","93789846":"markdown","c46cb72a":"markdown","4357ce03":"markdown","30ad1b55":"markdown","bda6b7b3":"markdown","05691430":"markdown","54d4051e":"markdown","7293004c":"markdown","14fe15cb":"markdown","97ea4a97":"markdown","fb91f363":"markdown","01207bda":"markdown","d850d49e":"markdown","87c592ab":"markdown","a0ac77b1":"markdown","e9d9dd14":"markdown","21ebb1ae":"markdown","d01309a2":"markdown","2ade7904":"markdown"},"source":{"d4a1b0cd":"import pandas as pd\n#import numpy as np\n#import random as rnd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import svm\nfrom sklearn import linear_model\nprint(\"Libraries imported\")\n#Importing libraries","6a4230fe":"#importing training and testing  data\ntrain_df=pd.read_csv('..\/input\/train.csv')\ntest_df=pd.read_csv('..\/input\/test.csv')\nprint(\"Training and testing data imported\")\ntrain_df.shape, test_df.shape","f68a2112":"train_df.columns","53807906":"TotalRecsTrain=train_df['PassengerId'].count() #total number of records in training data\nTotalRecsTest=test_df['PassengerId'].count()# total number of records in testing data","0783dfed":"#Checking for nulls in training data\nfor column in train_df:\n    if(train_df[column].isnull().any()):\n        print(\"Column Name:\",column,\"|Column Data type:\",train_df[column].dtype,\",|Not null count:\",train_df[column].count(),'Total count:',TotalRecsTrain,\"|Null Values\",TotalRecsTrain-train_df[column].count())","a79473eb":"#Checking for nulls in testing data\nfor column in test_df:\n    if(test_df[column].isnull().any()):\n        print(\"Column Name:\",column,\"|Column Data type:\",test_df[column].dtype,\",|Not null count:\",test_df[column].count(),'Total count:',TotalRecsTest,\"|Null Values\",TotalRecsTest-test_df[column].count())","d1035cd7":"freq_age_train=train_df.Age.dropna().mode()[0]\nfreq_embarked=train_df.Embarked.dropna().mode()[0]\nfreq_age_test=test_df.Age.dropna().mode()[0]\nfreq_fare_test=test_df.Fare.dropna().mode()[0]\ntrain_df['Age']=train_df['Age'].fillna(freq_age_train)\ntrain_df['Embarked']=train_df['Embarked'].fillna(freq_embarked)\ntest_df['Age']=test_df['Age'].fillna(freq_age_test)\ntest_df['Fare']=test_df['Fare'].fillna(freq_fare_test)\n#After filling null values, lets drop Cabin column from test and train data.\ntrain_df=train_df.drop(['Cabin'],axis=1)\ntest_df=test_df.drop(['Cabin'],axis=1)\n","fafe6962":"# To be on safe side, whenever a column is dropped then check shape of data set\ntrain_df.shape,test_df.shape","d8652c32":"train_df.columns","bcd690c7":"train_df=train_df.drop(['Ticket'],axis=1)\ntest_df=test_df.drop(['Ticket'],axis=1)\ntrain_df.shape,test_df.shape","b136b4ed":"train_df_grp_Pclass=train_df.groupby(['Survived','Pclass']).size()\ntrain_df_grp_Pclass","41112efe":"#now lets check the gender now\ntrain_df_grp_gender=train_df.groupby(['Survived','Sex']).size()\ntrain_df_grp_gender","8049b419":"train_df_grp_embarked=train_df.groupby(['Survived','Embarked']).size()\ntrain_df_grp_embarked","ef39bc40":"train_df_grp_emb_gen=train_df.groupby(['Survived','Embarked','Sex']).size()\ntrain_df_grp_emb_gen","9e226ac1":"# Total number of people boared from each port\nPortCppl=train_df['Embarked'].loc[train_df['Embarked']=='C'].count()\nPortSppl=train_df['Embarked'].loc[train_df['Embarked']=='S'].count()\nPortQppl=train_df['Embarked'].loc[train_df['Embarked']=='Q'].count()\n\n#Females survived on each port\nFemalesSuvPortC=train_df['Survived'].loc[train_df['Embarked']=='C'].loc[train_df['Sex']=='female'].loc[train_df['Survived']==1].count()\nFemalesSuvPortS=train_df['Survived'].loc[train_df['Embarked']=='S'].loc[train_df['Sex']=='female'].loc[train_df['Survived']==1].count()\nFemalesSuvPortQ=train_df['Survived'].loc[train_df['Embarked']=='Q'].loc[train_df['Sex']=='female'].loc[train_df['Survived']==1].count()\n#Females died on each port\nFemalesDiedPortC=train_df['Survived'].loc[train_df['Embarked']=='C'].loc[train_df['Sex']=='female'].loc[train_df['Survived']==0].count()\nFemalesDiedPortS=train_df['Survived'].loc[train_df['Embarked']=='S'].loc[train_df['Sex']=='female'].loc[train_df['Survived']==0].count()\nFemalesDiedPortQ=train_df['Survived'].loc[train_df['Embarked']=='Q'].loc[train_df['Sex']=='female'].loc[train_df['Survived']==0].count()\n#Males survived on each port\nMalesSuvPortC=train_df['Survived'].loc[train_df['Embarked']=='C'].loc[train_df['Sex']=='male'].loc[train_df['Survived']==1].count()\nMalesSuvPortS=train_df['Survived'].loc[train_df['Embarked']=='S'].loc[train_df['Sex']=='male'].loc[train_df['Survived']==1].count()\nMalesSuvPortQ=train_df['Survived'].loc[train_df['Embarked']=='Q'].loc[train_df['Sex']=='male'].loc[train_df['Survived']==1].count()\n#Males died on each port\nMalesDiedPortC=train_df['Survived'].loc[train_df['Embarked']=='C'].loc[train_df['Sex']=='male'].loc[train_df['Survived']==0].count()\nMalesDiedPortS=train_df['Survived'].loc[train_df['Embarked']=='S'].loc[train_df['Sex']=='male'].loc[train_df['Survived']==0].count()\nMalesDiedPortQ=train_df['Survived'].loc[train_df['Embarked']=='Q'].loc[train_df['Sex']=='male'].loc[train_df['Survived']==0].count()\n#Lets create a dataframe to visualize all this now\nembarked_df=pd.DataFrame({\"1_Total People Embarked\":[PortCppl,PortSppl,PortQppl],\"2_Males Survived\" : [MalesSuvPortC,MalesSuvPortS,MalesSuvPortQ],\"2_Males Died\" : [MalesDiedPortC,MalesDiedPortS,MalesDiedPortQ],\"3_Females Survived\":[FemalesSuvPortC,FemalesSuvPortS,FemalesSuvPortQ],\"3_Females Died\":[FemalesDiedPortC,FemalesDiedPortS,FemalesDiedPortQ],\n                          \"4_Overall Survival Rate\":[round(((MalesSuvPortC+FemalesSuvPortC)\/PortCppl)*100,2),round(((MalesSuvPortS+FemalesSuvPortS)\/PortSppl)*100,2),round(((MalesSuvPortQ+FemalesSuvPortQ)\/PortQppl)*100,2)]\n                          \n                         ,'5_Female Survival Rate':[round((FemalesSuvPortC\/(FemalesDiedPortC+FemalesSuvPortC))*100,2),round((FemalesSuvPortS\/(FemalesDiedPortS+FemalesSuvPortS))*100,2),round((FemalesSuvPortQ\/(FemalesDiedPortQ+FemalesSuvPortQ))*100,2)]\n                         \n                         ,'6_Male Survival Rate':[round((MalesSuvPortC\/(MalesDiedPortC+MalesSuvPortC))*100,2),round((MalesSuvPortS\/(MalesDiedPortS+MalesSuvPortS))*100,2),round((MalesSuvPortQ\/(MalesDiedPortQ+MalesSuvPortQ))*100,2)]\n                         \n                         },index=[\"C\",\"S\",\"Q\"])\nembarked_df.index.name=\"Port\"\nembarked_df\n\n\n","16ee4150":"#Name doest seem to have much of an impact but it has titles so will extract titles\ntrain_df['Name'].str.split(',', expand = True)[1].str.split('.',expand=True)[0]\ntrain_df['Title']=train_df['Name'].str.split(',', expand = True)[1].str.split('.',expand=True)[0].str.strip()\ntest_df['Title']=test_df['Name'].str.split(',', expand = True)[1].str.split('.',expand=True)[0].str.strip()\n\n#Dropping names column\ntrain_df=train_df.drop(['Name'],axis=1)\ntest_df=test_df.drop(['Name'],axis=1)\ntrain_df.shape,test_df.shape","eb76ea60":"train_df_grp_title=train_df.groupby(['Survived','Sex','Title']).size()\ntrain_df_grp_title","55d637f0":"train_df.groupby(['Title']).groups.keys()","e38a8409":"#We are going to map these titles to more generic ones in training and testing data\ntrain_df['Title']=train_df['Title'].map({'Miss':'Miss','Lady':'Miss','Mlle':'Miss','Ms':'Miss','the Countess':'Miss','Mme':'Miss','Mrs':'Mrs','Mr':'Mr','Master':'Master','Capt':'Others','Col':'Others','Don':'Others','Dr':'Others','Jonkheer':'Others','Major':'Others','Rev':'Others','Sir':'Others'})\ntest_df['Title']=test_df['Title'].map({'Miss':'Miss','Lady':'Miss','Mlle':'Miss','Ms':'Miss','the Countess':'Miss','Mme':'Miss','Mrs':'Mrs','Mr':'Mr','Master':'Master','Capt':'Others','Col':'Others','Don':'Others','Dr':'Others','Jonkheer':'Others','Major':'Others','Rev':'Others','Sir':'Others'})","850096a4":"train_df_grp_title=train_df.groupby(['Survived','Sex','Title']).size()\ntrain_df_grp_title","1c73ff7b":"train_df.groupby(['SibSp']).groups.keys()","5be5f45d":"alonePassengers=len(train_df.groupby(['SibSp']).groups[0])\nfamMem1=len(train_df.groupby(['SibSp']).groups[1])\nfamMem2=len(train_df.groupby(['SibSp']).groups[2])\nfamMem3=len(train_df.groupby(['SibSp']).groups[3])\nfamMem4=len(train_df.groupby(['SibSp']).groups[4])\nfamMem5=len(train_df.groupby(['SibSp']).groups[5])\nfamMem8=len(train_df.groupby(['SibSp']).groups[8])","9bd3fb5c":"AloneSurvived=train_df['Survived'].loc[train_df['SibSp']==0].loc[train_df['Survived']==1].count()\nprint('Passengers travelling alone:',alonePassengers,'| Passengers survived:',AloneSurvived,'|Survival %:',round((AloneSurvived\/alonePassengers)*100,2))\nWith1Person=train_df['Survived'].loc[train_df['SibSp']==1].loc[train_df['Survived']==1].count()\nprint('Passengers travelling with one spouse or family member:',famMem1,'| Passengers survived:',With1Person,'|Survival %:',round((With1Person\/famMem1)*100,2))\nWith2Person=train_df['Survived'].loc[train_df['SibSp']==2].loc[train_df['Survived']==1].count()\nprint('Passengers travelling with spouse or family member(2):',famMem2,'|Passengers survived:',With2Person,'|Survival %:',round((With2Person\/famMem2)*100,2))\nWith3Person=train_df['Survived'].loc[train_df['SibSp']==3].loc[train_df['Survived']==1].count()\nprint('Passengers travelling with spouse or family member(3):',famMem3,'|Passengers survived:',With3Person,'|Survival %:',round((With3Person\/famMem3)*100,2))\nWith4Person=train_df['Survived'].loc[train_df['SibSp']==4].loc[train_df['Survived']==1].count()\nprint('Passengers travelling with spouse or family member(4):',famMem4,'|Passengers survived:',With4Person,'|Survival %:',round((With4Person\/famMem4)*100,2))\nWith5Person=train_df['Survived'].loc[train_df['SibSp']==5].loc[train_df['Survived']==1].count()\nprint('Passengers travelling with spouse or family member(5):',famMem5,'|Passengers survived:',With5Person,'|Survival %:',round((With5Person\/famMem5)*100,2))\nWith8Person=train_df['Survived'].loc[train_df['SibSp']==8].loc[train_df['Survived']==1].count()\nprint('Passengers travelling with spouse or family member(8):',famMem8,'|Passenfers survived:',With8Person,'|Survival %:',round((With8Person\/famMem8)*100,2))","158c4f61":"train_df['SibSp']=train_df['SibSp'].map({0:1,1:1,2:1,3:2,4:2,5:2,8:2})\ntest_df['SibSp']=test_df['SibSp'].map({0:1,1:1,2:1,3:2,4:2,5:2,8:2})","4ac5610e":"train_df.groupby(['Parch']).groups.keys()","a86470c3":"AloneP=len(train_df.groupby(['Parch']).groups[0])\nPMem1=len(train_df.groupby(['Parch']).groups[1])\nPMem2=len(train_df.groupby(['Parch']).groups[2])\nPMem3=len(train_df.groupby(['Parch']).groups[3])\nPMem4=len(train_df.groupby(['Parch']).groups[4])\nPMem5=len(train_df.groupby(['Parch']).groups[5])\nPMem6=len(train_df.groupby(['Parch']).groups[6])\n\nAlonePS=train_df['Survived'].loc[train_df['Parch']==0].loc[train_df['Survived']==1].count()\nprint('Passengers travelling alone:',AloneP,'| Passengers survived:',AlonePS,'|Survival %:',round((AlonePS\/AloneP)*100,2))\nWith1Parent=train_df['Survived'].loc[train_df['Parch']==1].loc[train_df['Survived']==1].count()\nprint('Passengers travelling parent or child:',PMem1,'| Passengers survived:',With1Parent,'|Survival %:',round((With1Parent\/PMem1)*100,2))\nWith2Parent=train_df['Survived'].loc[train_df['Parch']==2].loc[train_df['Survived']==1].count()\nprint('Passengers travelling with parent or child(2):',PMem2,'|Passengers survived:',With2Parent,'|Survival %:',round((With2Parent\/PMem2)*100,2))\nWith3Parent=train_df['Survived'].loc[train_df['Parch']==3].loc[train_df['Survived']==1].count()\nprint('Passengers travelling with parent or child(3):',PMem3,'|Passengers survived:',With3Parent,'|Survival %:',round((With3Parent\/PMem4)*100,2))\nWith4Parent=train_df['Survived'].loc[train_df['Parch']==4].loc[train_df['Survived']==1].count()\nprint('Passengers travelling with parent or child(4):',PMem4,'|Passengers survived:',With4Parent,'|Survival %:',round((With4Parent\/PMem4)*100,2))\nWith5Parent=train_df['Survived'].loc[train_df['Parch']==5].loc[train_df['Survived']==1].count()\nprint('Passengers travelling with parent or child(5):',PMem5,'|Passengers survived:',With5Parent,'|Survival %:',round((With5Parent\/PMem5)*100,2))\nWith6Parent=train_df['Survived'].loc[train_df['Parch']==6].loc[train_df['Survived']==1].count()\nprint('Passengers travelling with spouse or family member(8):',PMem6,'|Passenfers survived:',With6Parent,'|Survival %:',round((With6Parent\/PMem6)*100,2))","c6b2a7bb":"train_df['Parch']=train_df['Parch'].map({0:1,1:1,2:1,3:1,4:2,5:2,6:2})\ntest_df['Parch']=test_df['Parch'].map({0:1,1:1,2:1,3:1,4:2,5:2,6:2,9:2})","45e884ff":"#Total number of passengers travelling in each class\nclass1=train_df['Pclass'].loc[train_df['Pclass']==1].count()\nclass2=train_df['Pclass'].loc[train_df['Pclass']==2].count()\nclass3=train_df['Pclass'].loc[train_df['Pclass']==3].count()\n\n#Total number of passengers survived in each class\nclass1Sur=train_df['Pclass'].loc[train_df['Pclass']==1].loc[train_df['Survived']==1].count()\nclass2Sur=train_df['Pclass'].loc[train_df['Pclass']==2].loc[train_df['Survived']==1].count()\nclass3Sur=train_df['Pclass'].loc[train_df['Pclass']==3].loc[train_df['Survived']==1].count()\n\n# Creating dataset for a better understanding\nSurDF=pd.DataFrame({'0_Class':[1,2,3],'1_Total passengers':[class1,class2,class3],'2_Survived':[class1Sur,class2Sur,class3Sur],'3_Not Survived':[class1-class1Sur,class2-class2Sur,class3-class3Sur],'4_Survival Percent':[round((class1Sur\/class1)*100,2),round((class2Sur\/class2)*100,2),round((class3Sur\/class3)*100,2)]})\nSurDF.sort_values(by='4_Survival Percent',ascending=False)\n","0b44ff07":"#Now comes the fare part\nprint('Average fare in 1st class: ',train_df['Fare'].loc[train_df['Pclass']==1].mean())\nprint('Average fare in 2nd class: ',train_df['Fare'].loc[train_df['Pclass']==2].mean())\nprint('Average fare in 3rd class: ',train_df['Fare'].loc[train_df['Pclass']==3].mean())","0c33b6c8":"train_df=train_df.drop(['Fare'],axis=1)\ntest_df=test_df.drop(['Fare'],axis=1)\ntrain_df.shape,test_df.shape","3073e4c9":"#Now lets analyze age\nminAge=train_df['Age'].min()\nmaxAge=train_df['Age'].max()\nprint('Age Range:',minAge,' to ',maxAge)","b849eb02":"index1=train_df['Age'].loc[(train_df['Age']>=0) & (train_df['Age']<10)].index\nindex2=train_df['Age'].loc[(train_df['Age']>=10) & (train_df['Age']<20)].index\nindex3=train_df['Age'].loc[(train_df['Age']>=20) & (train_df['Age']<30)].index\nindex4=train_df['Age'].loc[(train_df['Age']>=30) & (train_df['Age']<40)].index\nindex5=train_df['Age'].loc[(train_df['Age']>=40) & (train_df['Age']<50)].index\nindex6=train_df['Age'].loc[(train_df['Age']>=50) & (train_df['Age']<60)].index\nindex7=train_df['Age'].loc[(train_df['Age']>=60)].index\nfor idx in index1:\n    train_df.loc[idx,'Age']=1\nfor idx in index2:\n    train_df.loc[idx,'Age']=2\nfor idx in index3:\n    train_df.loc[idx,'Age']=3\nfor idx in index4:\n    train_df.loc[idx,'Age']=4\nfor idx in index5:\n    train_df.loc[idx,'Age']=5\nfor idx in index6:\n    train_df.loc[idx,'Age']=6\nfor idx in index7:\n    train_df.loc[idx,'Age']=7","f60ff820":"#Converting to integer type\ntrain_df['Age']=train_df['Age'].astype(int)","c40ab649":"Between_1_10=train_df[['Age','Survived']].loc[(train_df['Age']==1) &(train_df['Survived']==1)].size\nBetween_10_20=train_df[['Age','Survived']].loc[(train_df['Age']==2) &(train_df['Survived']==1)].size\nBetween_20_30=train_df[['Age','Survived']].loc[(train_df['Age']==3) &(train_df['Survived']==1)].size\nBetween_30_40=train_df[['Age','Survived']].loc[(train_df['Age']==4) &(train_df['Survived']==1)].size\nBetween_40_50=train_df[['Age','Survived']].loc[(train_df['Age']==5) &(train_df['Survived']==1)].size\nBetween_50_60=train_df[['Age','Survived']].loc[(train_df['Age']==6) &(train_df['Survived']==1)].size\nAbove_60=train_df[['Age','Survived']].loc[(train_df['Age']==7) &(train_df['Survived']==1)].size\n","51b74039":"Total_Between_1_10=train_df[['Age','Survived']].loc[(train_df['Age']==1)].size\nTotal_Between_10_20=train_df[['Age','Survived']].loc[(train_df['Age']==2) ].size\nTotal_Between_20_30=train_df[['Age','Survived']].loc[(train_df['Age']==3) ].size\nTotal_Between_30_40=train_df[['Age','Survived']].loc[(train_df['Age']==4) ].size\nTotal_Between_40_50=train_df[['Age','Survived']].loc[(train_df['Age']==5) ].size\nTotal_Between_50_60=train_df[['Age','Survived']].loc[(train_df['Age']==6) ].size\nTotal_Above_60=train_df[['Age','Survived']].loc[(train_df['Age']==7) ].size\n\n# Making data set to present survival data in tabular form\nAge_Sur_df=pd.DataFrame({'Total':[Total_Between_1_10,Total_Between_10_20,Total_Between_20_30,Total_Between_30_40,Total_Between_40_50,Total_Between_50_60,Total_Above_60],'Survived':[Between_1_10,Between_10_20,Between_20_30,Between_30_40,Between_40_50,Between_50_60,Above_60],'Age':['Child<10 years','Between 10 and 20','Between 20 and 30','Between 30 and 40','Between 40 and 50','Between 50 and 60','Above 60'],\n                         'Survival %':[round((Between_1_10\/Total_Between_1_10)*100,2),round((Between_10_20\/Total_Between_10_20)*100,2),round((Between_20_30\/Total_Between_20_30)*100,2),round((Between_30_40\/Total_Between_30_40)*100,2),round((Between_40_50\/Total_Between_40_50)*100,2),round((Between_50_60\/Total_Between_50_60)*100,2),round((Above_60\/Total_Above_60)*100,2)]\n                        \n                        \n                        },index={1,2,3,4,5,6,7})\n\nAge_Sur_df","9a290b9f":"index1_test=test_df['Age'].loc[(test_df['Age']>=0) & (test_df['Age']<10)].index\nindex2_test=test_df['Age'].loc[(test_df['Age']>=10) & (test_df['Age']<20)].index\nindex3_test=test_df['Age'].loc[(test_df['Age']>=20) & (test_df['Age']<30)].index\nindex4_test=test_df['Age'].loc[(test_df['Age']>=30) & (test_df['Age']<40)].index\nindex5_test=test_df['Age'].loc[(test_df['Age']>=40) & (test_df['Age']<50)].index\nindex6_test=test_df['Age'].loc[(test_df['Age']>=50) & (test_df['Age']<60)].index\nindex7_test=test_df['Age'].loc[(test_df['Age']>=60)].index\nfor idx in index1_test:\n    test_df.loc[idx,'Age']=1\nfor idx in index2_test:\n    test_df.loc[idx,'Age']=2\nfor idx in index3_test:\n    test_df.loc[idx,'Age']=3\nfor idx in index4_test:\n    test_df.loc[idx,'Age']=4\nfor idx in index5_test:\n    test_df.loc[idx,'Age']=5\nfor idx in index6_test:\n    test_df.loc[idx,'Age']=6\nfor idx in index7_test:\n    test_df.loc[idx,'Age']=7","0fccecbf":"#converting test data age column to int\ntest_df['Age']=test_df['Age'].astype(int)","92200078":"def EncodeColumn(cats,ColName,df):\n    dummiesTrain=pd.get_dummies(df[ColName],prefix=ColName,prefix_sep='_')\n    dummiesTrain=dummiesTrain.T.reindex(cats).T.fillna(0).astype(int)\n    df=pd.concat([df,dummiesTrain],axis=1)\n    df.drop([ColName],axis=1,inplace=True)\n    return df","daaf6e3d":"train_df=EncodeColumn(['Sex_male','Sex_female'],'Sex',train_df)\ntest_df=EncodeColumn(['Sex_male','Sex_female'],'Sex',test_df)","b5400428":"#Coverting embarked column\ncats_embarked=['Embarked_S','Embarked_C','Embarked_Q']\ntrain_df=EncodeColumn(cats_embarked,'Embarked',train_df)\ntest_df=EncodeColumn(cats_embarked,'Embarked',test_df)","52cc8c79":"test_df.isnull().any()","594477aa":"train_df.isnull().any()","0a60daa5":"test_df[pd.isnull(test_df).any(axis=1)]","491a3c0a":"test_df.loc[414,'Title']='Miss'\ntest_df['Title'].isnull().any()","b23a5acc":"#Now lets encode title column\n\ntitleCats=['Title_Master','Title_Miss','Title_Mr','Title_Mrs','Title_Others']\ntrain_df=EncodeColumn(titleCats,'Title',train_df)\ntest_df=EncodeColumn(titleCats,'Title',test_df)\n","a01ff366":"y=train_df['Survived']\nX=train_df.drop(['Survived','PassengerId'],axis=1)\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.7)","50267526":"#Random Forest\nclf=RandomForestClassifier(n_estimators=300)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\naccuracy_randF=round(metrics.accuracy_score(y_test,y_pred),2)","c387e4c0":"#SVC with linear kernel\nclf_SVMLin = svm.SVC(kernel='linear', C = 1)\nclf_SVMLin.fit(X_train,y_train)\ny_pred_lin=clf_SVMLin.predict(X_test)\naccuracy_SVCLin=round(metrics.accuracy_score(y_test,y_pred_lin),2)\n","e981d7a1":"#SVC with default DBF kernel\nclf_SVM_RBF = svm.SVC(kernel='rbf', C = 1.0)\nclf_SVM_RBF.fit(X_train,y_train)\ny_pred_rbf=clf_SVM_RBF.predict(X_test)\naccuracy_svm_rbf=round(metrics.accuracy_score(y_test,y_pred_rbf),2)\n","ad5c7db1":"Acc_dataSet=pd.DataFrame({'Classifier Name':['Random Forest','Linear SVC','RBF SVC'],\n                          'Accuracy':[accuracy_randF,accuracy_SVCLin,accuracy_svm_rbf]\n    \n})\nAcc_dataSet.sort_values(by='Accuracy',ascending=False)","4e85ac32":"X_test_final=test_df.drop(['PassengerId'],axis=1)\ny_pred_final=clf_SVMLin.predict(X_test_final)\nsubmission=pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':y_pred_final})\n#submission.to_csv('TitanicPred.csv',index=False)","dd95d2fc":"As we can see that childern had highest survival rate among other age groups. Applying same transformations on test data as well.","964d3c99":"Now we will see \"Name\" column, an important feature that can be extracted from name is title. We will see if people with higher social status titles had more chance of survival? So we will engineer a new feature \"Title\" and drop \"Name\" column.","7ae4af7d":"As we can see that Linear SVC is giving the best accuracy so we will use this classifier to make our prediction.","505004d6":"Now we will see if Pclass had any impact on survival. Here 1 means high class, 2 means second class and 3 means third or economy class.","f16ad743":"Lets count people in each age band.","9fda0b1e":"As we can see that passengers travelling in 1st class had better chance of survivial. Now lets analyze gender now.","e5c3a7f1":"Now we will train our data. We will split our training data into train and test set further because thats how we will evaluate which classifier is best. ","f4cefdbd":"As expected, passengers travelling in higher class had more chances of survival. People travelling in economy class werent so lucky after all.\nNow lets check \"Fare\" column. We will see what was the average fare in each class.","42f43ac4":"As we can see it follows same pattern as SipSb, so we will assign categories to this variable in same way as we did for SibSp.","c73c62e7":"As we can see as the family size increase so it had a negative impact on survival. So i am going to classify SibSp into three categories, person travelling alone or upto two people will be in category 1, greater than two in category 2.","8642bcb0":"Lets divied age range into bands of 10 years and see how it turns out, age above 60 is one band:","5078f2e2":"We will see if any column has any null value in test or training dataset.","ecaccd58":"Now we will see the the survival of each sub group in SibSp column.","1f576086":"There are two types of variables present in dataset and we will process them accordingly.\n\n* Categorical variables: These are variables which can have a finite set of values. There is no order involved, like you cannot tell which one is superior. Example of such variable is color, color can be red, blue, green, white or black and so on but there are limited number of colors available and there is no order involve. Like you cannot decide if red is superior than black. It is better not to map these variables numerically, you can assign red as 1 and blue as 2 and so on based on your choice, but algorithim might think that red has lower order than blue. In order to avoid that we will use encoding vector technique. In such cases we will use get_dummies function, what it does is take an input and convert it into a vector. For example if possible values for color column are red, blue and green and a row has color=red then it will be converted into three columns color_red,color_blue and color_green with values 1,0 and 0 respectively. Such a column in this data set is Sex, female and male.\n* Ordinal variables: These are variables which are categorical i.e. they can have a finite set of values but there is an order involved. For example as there is column about \"Pclass\", we know that Pclass=1 means that class 1 is superior to class 2 and class 3 so we will not encode them.\n\nLets define encode function:\n","f7079571":"As ticket number cannot tell much about who survived and who didn't so we will drop this column.","eec0ea51":"Titanic sank on 15th April 1912, after colliding with an iceberg. 67% passengers died in this tragedy. However survival might seem sheer luck, but we will explore data to see if some people had a better chance at survival? and how much it had to do with luck. So lets import the libraries first.","93789846":"Cabin column has too much null values in training and testing data set. So there is no point in keeping it. So we will drop this column and in training data set, for age and embarked column. We will find the most frequent value and fill it. Same will happen for age and Fare column in test data set.","c46cb72a":"Lets find percentage of males and females survived on each port.","4357ce03":"Now we will check count of nulls and not nulls and see if number of nulls is too high then we will drop that column, else we will fill null values. We will do this for both training and testing data.","30ad1b55":"Lets group this on gender as well as just this doesnt clarify picture","bda6b7b3":"Overall survival rate is high on port C while same for port Q and port S. Here again, more females survived than males.","05691430":"Now we will analyze column \"Parch\" which means if passengers were travelling with parent or child. We will apply same strategy as we applied on SibSp column i.e. find total count and compare with count of people who survived.","54d4051e":"So females survival count is way higher than male survival so we will keep this column.\nEmbarked column means the port from where passengers embarked on this voyage. So lets see if that had anything to do with survial.","7293004c":"In supervised machine learning problem target variable i.e. variable to be predicted is part of the training dataset. Here in this case column \"Survived\" is our target variable. Rest are features i.e. target variable is dependant on these independent variables. \nAs in this case, a person either survived (1) or didn't (0), i.e. we can classify output into two discret classes hence this is a classification problem. However if output was a continious variable like weight or height i.e. if it could take any value on real number line then we would have called it a regression problem. \nYou can have a look at my attempt on regression problem here \n[House Sale Regression Problem](https:\/\/www.kaggle.com\/brekhnaa\/house-sale-analysis)\n\n\nNow lets have a look at columns of dataset:","14fe15cb":"Here we will now merge some titles because Ms and Miss are same but with different spellings. So lets list all the unique titles and then map them.","97ea4a97":"**1. Data Prepration**\n\n\nIn order to proceed with analysis, we need to have a look at data and see what type of variables we have got. For analysis, all data should be converted into number data types and there should not be any null values. We need to deal with NULLs first. So lets do that:","fb91f363":"As we can see that 1st class had higher fare so whoever paid more had better survival chances. As we already have this pattern from column Pclass so we will drop column Fare.","01207bda":"As we can see that passenger was a female, so we will extract the most frequent female title from dataset and set that value.","d850d49e":"As title is based on gender so we will group survival with title and gender as well.","87c592ab":"As we can see that in test data one record has null value for title. We will see if passenger was a male or female.","a0ac77b1":"Now lets see if class had anything to do with survival. Surivied=0 means person died and 1 means survived.","e9d9dd14":"Lets count now how much people survived in each band:","21ebb1ae":"Now we will train our data on four classifiers, after training the data,  we will evaluate the accuracy each classifier and see which one is best. \nAccuracy is the ratio of correct observations predicted to the total number of observations i.e. if model has predicted 350 instances correctly and total count was 400 then accuracy will be 350\/400.\n\nI am going to use [Random Forest](https:\/\/www.datacamp.com\/community\/tutorials\/random-forests-classifier-python) and [SVC](https:\/\/www.datacamp.com\/community\/tutorials\/svm-classification-scikit-learn-python) classifiers.","d01309a2":"As we can see that among males, people with title Master survived more. RMS Titanic was a British ship, and in Britian Master is title for a young boy of nobility so we can safely assume that male childeren of high class had better luck with survival.\n\nNow comes the SibSp column. This column has count of siblings or spouse travelling together. So we will see if this has any impact on survival. We will first check how much distinct values this column has and get the count for each value.","2ade7904":"Lets see now if age had anything to do with survival? Did young and brave saved themselves or was courtesy was shown to elder people during the chaos?"}}