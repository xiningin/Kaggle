{"cell_type":{"75111771":"code","ce727ea0":"code","71cf2809":"code","e6fee949":"code","5e6d2773":"code","b5d9b1a4":"code","f4cbb81c":"code","32300327":"code","f5e6778d":"code","18854874":"code","ff645fed":"code","6cbdf559":"code","f6b4539b":"code","0b46173a":"code","4036ffa9":"code","79d4634c":"code","0724f28a":"code","2c6d798f":"code","053c1399":"code","64855d51":"code","c3f1e614":"code","58b50226":"code","d108703e":"code","ae394d25":"code","63c16189":"markdown","daa36c02":"markdown","a5645ef5":"markdown","fa3e6fe7":"markdown","e23e8e10":"markdown","b340cc69":"markdown","68d85fe4":"markdown","aacc425e":"markdown","11fd2a7f":"markdown","7bc519ca":"markdown","a257cf97":"markdown","4084e54d":"markdown","09452ed4":"markdown","37ae1fa4":"markdown","91f3aa89":"markdown","56aebfc4":"markdown","5e89ee1f":"markdown","c858a5f4":"markdown","6e4d65d2":"markdown","88cdb5c9":"markdown","c5200268":"markdown","ba09513a":"markdown","9767c5d7":"markdown","a3231f2c":"markdown","5afd7b01":"markdown","3feec926":"markdown"},"source":{"75111771":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce727ea0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nsns.set_style('darkgrid')\n\nfrom wordcloud import WordCloud,STOPWORDS\nstopwords = list(STOPWORDS)\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score \nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer as CVTZ\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n\ndef RMSE(Y,YHAT):\n    return np.sqrt(mean_squared_error(Y,YHAT))\n\nplt.rc('figure',figsize=(20,11))","71cf2809":"data = pd.read_csv('..\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv')\ndata.head()","e6fee949":"# checking for null values\n\ndata.isnull().sum()","5e6d2773":"import re\n\ndef  clean_text(text):\n    \"\"\"\n    Fuction to clean the text data\n    * symbols\n    * change to lower_case\n    \"\"\"\n    text = text.str.lower()\n    text = text.apply(lambda T: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)|^rt|http.+?\", \"\", T))  \n        \n    return text","b5d9b1a4":"data['Review']= clean_text(data['Review'])","f4cbb81c":"data['Review'] = data['Review'].str.replace('#','')","32300327":"data.head()","f5e6778d":"data.Rating.value_counts()","18854874":"sns.countplot(data=data,x='Rating', palette=\"Set3\")","ff645fed":"## Getting the number of words by splitting them by a space\nwords_per_review = [len(x.split(\" \")) for x in data['Review']]\nsns.distplot(words_per_review,fit=norm, kde=False)","6cbdf559":"def wordCloud_generator(data):\n    wordcloud = WordCloud(width = 800, height = 800,\n                          background_color ='black',\n                          min_font_size = 10,\n                          colormap='Pastel1'\n                         ).generate(\" \".join(data.values))\n    # plot the WordCloud image                        \n    plt.figure(figsize = (10, 10), facecolor = None) \n    plt.imshow(wordcloud, interpolation='bilinear') \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.title(\"Most common words in Reviews\",fontsize=30)\n    plt.show() \n    \nwordCloud_generator(data['Review'])","f6b4539b":"punc=string.punctuation\n\nstop_words = set(stopwords.words('english'))\n\nstemmer = PorterStemmer()\n\nlemmatizer = WordNetLemmatizer()\n\ndef data_preprocessing(txt):\n    \n    #converting to lowercase\n    txt=txt.lower()\n    \n    #Removing Punctuation\n    txt=\"\".join([x for x in txt if x not in punc])\n    \n    #Removing stopwords\n    txt=\" \".join([word for word in str(txt).split() if word not in stop_words])\n    \n    #Stemming\n    txt = \" \".join([stemmer.stem(word) for word in txt.split()])\n    \n    #Lemmatization\n    txt = \" \".join([lemmatizer.lemmatize(word) for word in txt.split()])\n\n    return txt\n\ndata['text'] = data['Review'].apply(data_preprocessing)\n","0b46173a":"data","4036ffa9":"### Creating a python object of the class TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidfconverter = TfidfVectorizer(max_features=400, min_df=0.05, max_df=0.9)\ntfidf = tfidfconverter.fit_transform(data['text']).toarray()","79d4634c":"X_train,X_test,y_train,y_test = train_test_split(tfidf,data['Rating'],test_size=0.2,random_state=42)","0724f28a":"#-----Upsampling----\nfrom sklearn.utils import resample\nfrom collections import Counter\n\nprint(\"Before Upsampling:-\")\nprint(Counter(y_train))\n\n\n# Let's use SMOTE to oversample\nfrom imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX_train, y_train = oversample.fit_resample(X_train,y_train)\n\nprint(\"After Upsampling:-\")\nprint(Counter(y_train))","2c6d798f":"# training a Naive Bayes classifier \n#very fast\nfrom sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB().fit(X_train, y_train) \n\ny_pred_NB=mnb.predict(X_test)\n\nprint(\"Accuracy of Multinominal Naive Balyes:\",accuracy_score(y_test, y_pred_NB))\nprint(classification_report(y_pred_NB,y_test))","053c1399":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression().fit(X_train, y_train)\n\ny_pred_lr=logreg.predict(X_test)\n\nprint(\"Accuracy of Logistic Regression:\",accuracy_score(y_test, y_pred_lr))\nprint(classification_report(y_pred_lr,y_test))","64855d51":"des_tree = DecisionTreeClassifier().fit(X_train, y_train)\n\ny_pred_dt=des_tree.predict(X_test)\n\nprint(\"Accuracy of Decision Tree Classifier:\",accuracy_score(y_test, y_pred_dt))\nprint(classification_report(y_pred_dt,y_test))","c3f1e614":"rf = RandomForestClassifier().fit(X_train, y_train)\n\ny_pred_rf=rf.predict(X_test)\n\nprint(\"Accuracy of Random Forest Classifier:\",accuracy_score(y_test, y_pred_rf))\nprint(classification_report(y_pred_rf,y_test))","58b50226":"#takes huge amount of time to execute\nimport xgboost as xgb\n\nxgboost_clf = xgb.XGBClassifier().fit(X_train, y_train)\n\ny_pred_xgb=xgboost_clf.predict(X_test)\n\nprint(\"Accuracy of XGBoost Classifier:\",accuracy_score(y_test, y_pred_xgb))\nprint(classification_report(y_pred_xgb,y_test))","d108703e":"from sklearn.neighbors import KNeighborsClassifier \n\nknn = KNeighborsClassifier(n_neighbors = 5).fit(X_train, y_train)\n\ny_pred_knn=knn.predict(X_test)\n\nprint(\"Accuracy of k-nearest neighbours Classifier:\",accuracy_score(y_test, y_pred_knn))\nprint(classification_report(y_pred_knn,y_test))","ae394d25":"from sklearn.svm import SVC\n\nsvc = SVC()\n\ny_pred_svm=knn.predict(X_test)\n\nprint(\"Accuracy of SVM Classifier:\",accuracy_score(y_test, y_pred_svm))\nprint(classification_report(y_pred_svm,y_test))","63c16189":"### 6. k-Nearest Neighbours(KNN)","daa36c02":"## Divide into training and test sets","a5645ef5":"We can see that Naive Bayes, Logistic Regression and Random forest are giving us maximum accuracy, In the next version I'll be fine tuning these algorithms using some hyperparameter optimization techniques and will use word embeddings like Word2Vec and Glove or even BERT to encode my input data.","fa3e6fe7":"### 1. Naive Bayes","e23e8e10":"## Upsampling using SMOTE\n","b340cc69":"Now let us preprocess Reviews using some NLP tchniques like:\n1. converting to lowercase\n2. Removing Punctuation\n3. Removing stopwords\n4. Stemming\n5. Lemmatization","68d85fe4":"Now we will vectorise the ratings using TF-IDF scores and we will use ```toarray()``` to convert resultant sparse matrix to dense matrix. ","aacc425e":"## Check for null values","11fd2a7f":"### 5. XGBoost","7bc519ca":"### 4. Random Forest","a257cf97":"## Load the dataset","4084e54d":"## Vectorizing the input text","09452ed4":"# Trip Advisor hotel review sentiments prediction and review rating prediction","37ae1fa4":"## Data cleaning","91f3aa89":"## Count of each rating","56aebfc4":"Its clearly visible that the target variable(Rating) is not balanced, as we have huge difference in rating 1 and 5. So we will be using some sampling technique to balance these classes.","5e89ee1f":"## Wordcloud of most common words","c858a5f4":"## Getting the number of words in each review","6e4d65d2":"## Load all the required libraries","88cdb5c9":"Since our target variable is not balanced, we will use SMOTE algorithm to upsample the minority classes.","c5200268":"### 7. Support Vector Machines(SVM)","ba09513a":"### To be continued...","9767c5d7":"## Text preprocessing","a3231f2c":"### 2. Logistic Regression","5afd7b01":"## Model building","3feec926":"### 3. Decision Tree"}}