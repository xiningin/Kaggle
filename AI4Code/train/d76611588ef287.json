{"cell_type":{"5ef242d2":"code","51af4fba":"code","6630965b":"code","c7066e38":"code","bc7bcea8":"code","ee94c218":"code","59d12f03":"code","586c4f48":"code","ab54165e":"code","022dca6b":"code","38bb80fd":"code","f3b14126":"code","c991cb78":"code","ba68cc5e":"code","245b52bc":"code","49c85c49":"code","df3bc4b5":"code","35ef6edf":"code","b08dc134":"code","4c8c2e40":"code","2db88bb8":"code","79c38802":"code","34a27f89":"code","522910e5":"code","af617171":"markdown"},"source":{"5ef242d2":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\n\n","51af4fba":"train_df = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntest_df = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrain_target_df = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\nsample_sub = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\ntarget_cols = train_target_df.columns[1:]\nN_TARGETS = len(target_cols)","6630965b":"print(train_df[\"cp_dose\"].unique())\nprint(train_df[\"cp_type\"].unique())","c7066e38":"def preprocess_df(df, target=False):\n    \n    \n    scaler = MinMaxScaler()\n    df[\"cp_time\"]=scaler.fit_transform(df[\"cp_time\"].values.reshape(-1, 1))\n    \n    df[\"cp_dose\"]=(df[\"cp_dose\"]==\"D1\").astype(int)\n    df[\"cp_type\"]=(df[\"cp_type\"]==\"trt_cp\").astype(int)\n    \n    return df","bc7bcea8":"train_df","ee94c218":"train_target_df","59d12f03":"test_df","586c4f48":"x_train = preprocess_df(train_df.drop([\"sig_id\"], axis=1))\ny_train = train_target_df.drop([\"sig_id\"], axis=1)\n\nx_test = preprocess_df(test_df.drop([\"sig_id\"], axis=1))","ab54165e":"sample_sub","022dca6b":"x_train.shape, y_train.shape, x_test.shape","38bb80fd":"def get_keras_model(input_dim=875, output_dim=206):\n    \n    model = Sequential()\n    model.add(L.Dense(512, input_dim=875, activation=\"elu\"))\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.5))\n    model.add(L.Dense(256, activation=\"elu\"))\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.3))\n    model.add(L.Dense(128, activation=\"elu\"))\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.2))\n    model.add(L.Dense(206, activation=\"sigmoid\"))\n    \n    return model\n    \n    ","f3b14126":"model = get_keras_model()\nmodel.summary()","c991cb78":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n             loss=\"binary_crossentropy\",\n             metrics=[\"accuracy\", \"AUC\"])","ba68cc5e":"def multi_log_loss(y_true, y_pred):\n    losses = []\n    for col in y_true.columns:\n        losses.append(log_loss(y_true.loc[:, col], y_pred.loc[:, col]))\n    return np.mean(losses)","245b52bc":"#https:\/\/github.com\/bckenstler\/CLR\n\nclass CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https:\/\/arxiv.org\/abs\/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w\/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi\/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1\/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary\/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations\/(2*self.step_size))\n        x = np.abs(self.clr_iterations\/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())","49c85c49":"clr = CyclicLR(base_lr=0.003, max_lr=0.004,\n                    step_size=745, mode='exp_range',\n                    gamma=0.99994)","df3bc4b5":"hist = model.fit(x_train, y_train, epochs=10, callbacks=[clr])","35ef6edf":"#TODO: Cross validation","b08dc134":"y_train.values.shape","4c8c2e40":"ps = model.predict(x_train); ps.shape","2db88bb8":"ps_df = y_train.copy()\nps_df.iloc[:, : ] = ps\n\ntr_score = multi_log_loss(y_train, ps_df)\n\nprint(f\"Train score: {tr_score}\")","79c38802":"test_df","34a27f89":"test_preds = sample_sub.copy()\ntest_preds[target_cols] = 0\n\ntest_preds.loc[:,target_cols] = model.predict(x_test)\n\ntest_preds.loc[x_test['cp_type'] == 0, target_cols] = 0\ntest_preds.to_csv('submission.csv', index=False)","522910e5":"test_preds","af617171":"Binary targets"}}