{"cell_type":{"eaf06af9":"code","20e462bf":"code","4e1bdb70":"code","a257acfb":"code","646310fe":"code","f7af5bfd":"code","a413c4f5":"code","38dde659":"code","50505577":"code","2c4e56cb":"code","1a93c4d3":"code","435888f9":"code","2fa57c8b":"code","29a14786":"code","898bff7a":"code","52906628":"code","543e8cf3":"code","acacd647":"code","1b02f3b8":"code","7c1706e0":"code","02095d4b":"code","d7e4c140":"code","568abc3c":"code","ada28152":"code","d80998ff":"code","0963c9af":"code","33255c13":"code","f8d0cf51":"code","b03662ba":"code","47d42aba":"code","487d495f":"code","6e5fd4c8":"code","b8af848c":"code","02788048":"code","cbcdd90f":"code","3d888356":"code","9187af1d":"code","886cddef":"code","63b8112f":"code","6004f974":"markdown","64082af4":"markdown","b021078e":"markdown","815478b1":"markdown","3907fa3e":"markdown","e0ad1104":"markdown","d6c75536":"markdown","f63802d1":"markdown","99a9ffc5":"markdown","6534f9e6":"markdown","bdcd938c":"markdown","0bbd79a9":"markdown","5d8f42b7":"markdown","dcb9cb74":"markdown","d1d74a5a":"markdown","beba1f06":"markdown","15c4136e":"markdown","5c07cf01":"markdown","e211074d":"markdown","170156ff":"markdown","c87282eb":"markdown","a071edc8":"markdown","ce6595b7":"markdown","75d39c10":"markdown","f6692bd8":"markdown","4ecda882":"markdown","cd952fbc":"markdown","1af3ff48":"markdown","19b63808":"markdown","34dc34c5":"markdown","19fe45da":"markdown","dd5b9871":"markdown","d3ed9a27":"markdown","5bc60c68":"markdown","753c6af0":"markdown","2c92d2b8":"markdown","cb83abb6":"markdown","0d493902":"markdown","6baef761":"markdown","f3b6cc5b":"markdown","c530bfc4":"markdown","998a988b":"markdown","bc8a79d1":"markdown","5b98c6bb":"markdown","4eab8d0c":"markdown","82ed5427":"markdown","335b09a1":"markdown"},"source":{"eaf06af9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","20e462bf":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score\nfrom collections import Counter\nimport seaborn as sns\n\n# Modeling\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\n%matplotlib inline\n","4e1bdb70":"# Load dataset using python pandas and display top 5 data\n\ndf = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf.head()","a257acfb":"def detect_outliers(df,n,features):\n    outlier_indices = []\n    \"\"\"\n    Detect outliers from given list of features. It returns a list of the indices\n    according to the observations containing more than n outliers according\n    to the Tukey method\n    \"\"\"\n    # iterate over features(columns)\n    for col in features:\n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col],75)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from numeric features\noutliers_to_drop = detect_outliers(df, 2 ,[\"age\",\"anaemia\",\"creatinine_phosphokinase\",\"platelets\", \"serum_creatinine\", \"serum_sodium\", \"time\"])","646310fe":"df.loc[outliers_to_drop] # Show the outliers rows","f7af5bfd":"# Find columns with null values\ndf.isnull().sum()","a413c4f5":"# Infos\ndf.info()\ndf.describe()","38dde659":"df.dtypes","50505577":"plt.figure(figsize=(13,10))\nsns.heatmap(df.corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","2c4e56cb":"# Explore Age vs DEATH_EVENT\nplt.figure(figsize=(13,6))\ng = sns.kdeplot(df[\"age\"][df[\"DEATH_EVENT\"] == 1], color=\"Red\", shade = True)\ng = sns.kdeplot(df[\"age\"][df[\"DEATH_EVENT\"] == 0], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng.legend([\"Not Survived\",\"Survived\"])\n","1a93c4d3":"# Explore Fare distribution \ng = sns.distplot(df[\"creatinine_phosphokinase\"], color=\"m\", label=\"Skewness : %.2f\"%(df[\"creatinine_phosphokinase\"].skew()))\ng = g.legend(loc=\"best\")","435888f9":"# Apply log to Fare to reduce skewness distribution\ndf[\"creatinine_phosphokinase\"] = df[\"creatinine_phosphokinase\"].map(lambda i: np.log(i) if i > 0 else 0)\n","2fa57c8b":"g = sns.histplot(df[\"creatinine_phosphokinase\"], color=\"m\", label=\"Skewness : %.2f\"%(df[\"creatinine_phosphokinase\"].skew()))\ng = g.legend(loc=\"best\")","29a14786":"# Explore Fare distribution \ng = sns.histplot(df[\"serum_creatinine\"], color=\"m\", label=\"Skewness : %.2f\"%(df[\"serum_creatinine\"].skew()))\ng = g.legend(loc=\"best\")","898bff7a":"# Apply log to Fare to reduce skewness distribution\ndf[\"serum_creatinine\"] = df[\"serum_creatinine\"].map(lambda i: np.log(i) if i > 0 else 0)\ng = sns.histplot(df[\"serum_creatinine\"], color=\"m\", label=\"Skewness : %.2f\"%(df[\"serum_creatinine\"].skew()))\ng = g.legend(loc=\"best\")","52906628":"g = sns.barplot(x=\"anaemia\",y=\"DEATH_EVENT\",data=df)\ng.set_ylabel(\"Survival Probability\")","543e8cf3":"g = sns.barplot(x=\"sex\",y=\"DEATH_EVENT\",data=df)\ng.set_ylabel(\"Survival Probability\")","acacd647":"df[[\"sex\",\"DEATH_EVENT\"]].groupby('sex').mean()\n","1b02f3b8":"# Explore diabetes vs Survived by Sex\ng = sns.catplot(x=\"diabetes\", y=\"DEATH_EVENT\", hue=\"sex\", data=df,\n                   kind=\"bar\", palette=\"muted\")\ng.set_ylabels(\"survival probability\")","7c1706e0":"g = sns.barplot(x=\"high_blood_pressure\",y=\"DEATH_EVENT\",data=df)\ng.set_ylabel(\"Survival Probability\")","02095d4b":"# Explore diabetes vs Survived by Sex\ng = sns.catplot(x=\"high_blood_pressure\", y=\"DEATH_EVENT\", hue=\"sex\", data=df,\n                    kind=\"bar\", palette=\"muted\")\ng.set_ylabels(\"survival probability\")\ng.set_xlabels(\"High Blood Preasure\")","d7e4c140":"# Explore Serum Sodium vs Death Event vs Smoking\n\nplt.figure(figsize=(10,6))\nsns.boxplot(x=df[\"DEATH_EVENT\"], y=df['serum_sodium'], hue=df['smoking'], palette=[\"#6daa9f\",\"#774571\"])\nplt.show()","568abc3c":"plt.figure(figsize=(10,6))\nsns.violinplot(data=df, x=\"DEATH_EVENT\", y=\"platelets\", hue=\"anaemia\",\n               split=True, inner=\"quart\", linewidth=1,\n               )","ada28152":"# Explore Age vs Sex, Parch , Pclass and SibSP\nsns.catplot(y=\"age\",x=\"sex\",data=df,kind=\"box\")\nsns.catplot(y=\"age\",x=\"sex\",hue=\"smoking\", data=df,kind=\"box\")\nsns.catplot(y=\"age\",x=\"sex\",hue=\"diabetes\", data=df,kind=\"box\")","d80998ff":"df = pd.get_dummies(df, columns = [\"anaemia\"], prefix=\"ena\")\ndf = pd.get_dummies(df, columns = [\"diabetes\"], prefix=\"dia\")\ndf = pd.get_dummies(df, columns = [\"high_blood_pressure\"], prefix=\"hbp\")\ndf = pd.get_dummies(df, columns = [\"sex\"], prefix=\"sex\")\ndf = pd.get_dummies(df, columns = [\"smoking\"], prefix=\"smk\")","0963c9af":"df.time.value_counts()","33255c13":"## Separate train dataset and test dataset\nfeatures = df.drop([\"DEATH_EVENT\"], axis=1)\nlabels = df[\"DEATH_EVENT\"]\nx_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.30, random_state=7)","f8d0cf51":"def evaluate_model(models):\n    \"\"\"\n    Takes a list of models and returns chart of cross validation scores using mean accuracy\n    \"\"\"\n    \n    # Cross validate model with Kfold stratified cross val\n    kfold = StratifiedKFold(n_splits = 10)\n    \n    result = []\n    for model in models :\n        result.append(cross_val_score(model, x_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\n    cv_means = []\n    cv_std = []\n    for cv_result in result:\n        cv_means.append(cv_result.mean())\n        cv_std.append(cv_result.std())\n\n    result_df = pd.DataFrame({\n        \"CrossValMeans\":cv_means,\n        \"CrossValerrors\": cv_std,\n        \"Models\":[\n            \"LogisticRegression\",\n            \"DecisionTreeClassifier\",\n            \"AdaBoostClassifier\",\n            \"SVC\",\n            \"RandomForestClassifier\",\n            \"GradientBoostingClassifier\",\n            \"KNeighborsClassifier\"\n        ]\n    })\n\n    # Generate chart\n    bar = sns.barplot(x = \"CrossValMeans\", y = \"Models\", data = result_df, orient = \"h\")\n    bar.set_xlabel(\"Mean Accuracy\")\n    bar.set_title(\"Cross validation scores\")\n    return result_df","b03662ba":"# Modeling step Test differents algorithms \nrandom_state = 30\nmodels = [\n    LogisticRegression(random_state = random_state, solver='liblinear'),\n    DecisionTreeClassifier(random_state = random_state),\n    AdaBoostClassifier(DecisionTreeClassifier(random_state = random_state), random_state = random_state, learning_rate = 0.2),\n    SVC(random_state = random_state),\n    RandomForestClassifier(random_state = random_state),\n    GradientBoostingClassifier(random_state = random_state),\n    KNeighborsClassifier(),\n]\nevaluate_model(models)","47d42aba":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits = 10)","487d495f":"# Hyperparameter tuning for LogisticRegression\n\nlogi_reg = LogisticRegression()\nlogi_params = {\n    'penalty' : ['l1', 'l2'],\n    'C' : np.logspace(-4, 4, 20),\n    'solver' : ['liblinear']\n}\n\ngs_logi = GridSearchCV(logi_reg, param_grid = logi_params, cv = kfold, scoring=\"accuracy\", verbose = 1, n_jobs= 3)\ngs_logi.fit(x_test, y_test)\n\n# Display best score\ngs_logi.best_score_","6e5fd4c8":"# Random Forest\nrand_forest = RandomForestClassifier()\n\n# Parameters\nrand_params = {'bootstrap': [False], 'max_depth': [None], 'max_features': [1, 5, 10], \n               'min_samples_leaf': [1, 4, 7], 'min_samples_split': [4, 8, 10], \n               'n_estimators': [100, 250], \"criterion\": [\"gini\"]}\n\ngs_rand = GridSearchCV(rand_forest, param_grid = rand_params, cv = kfold, n_jobs= 5, scoring = \"accuracy\", verbose = 1)\ngs_rand.fit(x_test, y_test)\n\n# Display best score\ngs_rand.best_score_\n","b8af848c":"# Gradient boosting\n\ngred_boosting = GradientBoostingClassifier()\ngbs_params = {'n_estimators' : [100,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'min_samples_leaf': [4, 8, 10],\n              'loss' : [\"deviance\"],\n              'max_depth': [2, 8],\n              'max_features': [0.3, 0.1] \n              }\n\ngs_gbc = GridSearchCV(gred_boosting, param_grid = gbs_params, cv=kfold, scoring=\"accuracy\", n_jobs = 3, verbose = 1)\ngs_gbc.fit(x_test, y_test)\n\n# Best score\ngs_gbc.best_score_","02788048":"# DecisionTree\ndtc = DecisionTreeClassifier()\ndtc_params = {\n    \"max_depth\" : [2, 6, 12],\n    \"min_samples_leaf\" : [3, 5, 10],\n    \n}\ngs_dtc = GridSearchCV(dtc, param_grid = dtc_params, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngs_dtc.fit(x_test, y_test)\n\n# Show best score\ngs_dtc.best_score_","cbcdd90f":"def show_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n","3d888356":"# Show learning curves\n\nshow_learning_curve(gs_logi.best_estimator_, \"LogisticRegression Learning Curve\", x_train, y_train, cv=kfold)\nshow_learning_curve(gs_rand.best_estimator_,\"RandomForest Learning Curves\", x_train, y_train, cv=kfold)\nshow_learning_curve(gs_gbc.best_estimator_,\"Gradient boosting Learning Curves\", x_train, y_train, cv=kfold)\nshow_learning_curve(gs_dtc.best_estimator_,\"DecisionTree Learning Curves\", x_train, y_train, cv=kfold)\n","9187af1d":"# Combining Models\n\nvoting = VotingClassifier(estimators=[\n    ('gs_logi', gs_logi.best_estimator_),\n    ('gs_rand', gs_rand.best_estimator_),\n    ('gs_gbc',gs_gbc.best_estimator_),\n    ('gs_dtc',gs_dtc.best_estimator_)], voting='soft', n_jobs = 5)\n\nvoting = voting.fit(x_train, y_train)","886cddef":"# Prediction\n\ny_pred  = voting.predict(x_test)\naccuracy = np.round(accuracy_score(y_test, y_pred) * 100, 3)\nprecision = np.round(precision_score(y_test, y_pred) * 100, 3)\n\nprint('Test dataset accuracy score: ', accuracy)\nprint('Test dataset precision score: ', precision)","63b8112f":"# Plotting the confusion matrix in heatmap\n\nmatrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()","6004f974":"# <font size=\"4\">4.2 Categorical Values<\/font>","64082af4":"Please upvote and share my notebook if you find it useful - It keeps me motivated :)","b021078e":"# <font size=\"4\">3.3 Check for missing values<\/font>","815478b1":"**Explore Platelets vs Anemia vs Death Event**","3907fa3e":"# <font size=\"4\">4.1 Numeric Values<\/font>","e0ad1104":"Awesome, There is not features containing more than 2 outliers","d6c75536":"Female with high blood preasure has more thances of heart fails and if female does not have blood preasure then she has very less chances comparing to male","f63802d1":"Dataset does not have any missing values","99a9ffc5":"# <font size=\"4\">6.1 Data Splitting<\/font>\n","6534f9e6":"On the base of validation scores, I decided to go with LogisticRegression, RandomForest, GradientBoosting , and DecisionTree classifiers for the ensemble modeling\n\n","bdcd938c":"Heart failure is a chronic, progressive condition in which the heart muscle is unable to pump enough blood to meet the body's needs for blood and oxygen. Basically, the heart can't keep up with its workload. There are certain factors which increase chances of heart fail. It can be either mental or physical factors.\n\nIn this notebook, i will classification with feature engineering and ensemble modeling. First of all i will display some feature analysis and in that last i will do model evalution and preditions.","0bbd79a9":"I compared 7 most popular classification models and evaluate the mean accuracy by kfold cross validation procedure\n\n* Logistic regression\n* Decision Tree\n* AdaBoost\n* SVC\n* Random Forest\n* Gradient Boosting\n* KNN\n\n","5d8f42b7":"**Sex**","dcb9cb74":"People with high blood preasure has more chances of heart fail. Lets ","d1d74a5a":"Gender of patient Male = 1, Female = 0\n\nAccrding to observation male and female both have similar chances of survival. So Sex, might not play an important role in the prediction of the death.\n\n","beba1f06":"# <font size=\"4\">3.2 Outlier detection<\/font>\n","15c4136e":"**diabetes**","5c07cf01":"Its clearly obvious that female with diabetes has more chances of heart fail","e211074d":"# <font size=\"4\">3.1 Load data<\/font>","170156ff":"# 2. Install Libraries","c87282eb":"As we can see, creatine phosphokinase distribution is very skewed. This can lead to overweigth very high values in the model, even if it is scaled.\n\nIn this case, it is better to transform it with the log function to reduce this skew.","a071edc8":"age, serum_creatinine feature seems to have a significative correlation with the death event probability. ejection_fraction and serum_sodium also negatively corelated with death event\n\nThis does not mean that other features are not useful. We need to explore in detail these features to detarmine it corelation\n\n","ce6595b7":"I kept n_jobs to 2, you can increase it according to your cpu core","75d39c10":"Randomforest and Gredientboosting are overfitting in training set. DecisionTree and LogisticRegressoin is seems better generalize the prediction. In next section i will use Ensemble modeling for model voting","f6692bd8":"I used Tukey method. Tukey method is a single-step multiple comparison procedure and statistical test. It can be used to find means that are significantly different from each other. I detected outliers from the numerical values features (Age, anaemia, creatinine_phosphokinase etc). Then, i considered outliers as rows that have at least two outlied numerical values.\n","4ecda882":"Serum creatinine distribution is very skewed. In this case, it is better to transform it with the log function to reduce this skew.","cd952fbc":"# <font size=\"4\">6.4 Learning Curves<\/font>\n","1af3ff48":"# 5. Feature engineering","19b63808":"# <font size=\"4\">6.2 Cross Validate Models<\/font>\n","34dc34c5":"# <font size=\"4\">6.5 Ensemble modeling<\/font>\n","19fe45da":"I kept n_jobs to 5. You can change it according to your processor cores","dd5b9871":"**High blood pressure**","d3ed9a27":"hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. \n\n**wiki**: https:\/\/en.wikipedia.org\/wiki\/Hyperparameter_optimization\n\n\n\n","5bc60c68":"**Serum Sodium \/ Smoking**","753c6af0":"Skewness is clearly reduced after the log transformation\n","2c92d2b8":"# 4. Feature analysis","cb83abb6":"We have person informations like Age, anaemia, creatinine, diabetes etc which are features and DEATH_EVENT is label","0d493902":"# 1. Introduction","6baef761":"There is know correlation between smoking and Serum Sodium. Compared to non-smoker users, smokers had significantly increased levels of serum sodium. \n\nhttps:\/\/www.researchgate.net\/publication\/259625450_Effect_of_cigarette_smoking_on_blood_sodium_and_potassium_levels_in_sudanese_subjects\n","f3b6cc5b":"When we superimpose the two densities , we cleary see a peak correponsing (between 45 and 75) to babies and very young childrens.\n","c530bfc4":"# 6. Modeling","998a988b":"**Anaemia**","bc8a79d1":"Learning curves helps me to find movel overfitting. Overfitting is one of the common issues i have ever faced. It effects model accuricy","5b98c6bb":"# <font size=\"4\">5.1 Create Categorical Values<\/font>","4eab8d0c":"# Heart Fail Analysis with ensemble modeling\n\n**Gopal Joshi, Diploma**\n\n1. Introduction\n2. Install libraries\n3. Load and check data<br>\n    3.1 Load data<br>\n    3.2 Outlier detection<br>\n    3.3 Check for missing values<br>\n4. Feature analysis<br>\n\t4.1 Numeric Values<br>\n\t4.2 Categorical Values<br>\n5. Feature engineering<br>\n\t5.1 Create Categorical Values<br>\n6. Simple Modeling<br>\n\t6.1 Data Splitting<br>\n\t6.2 Cross Validate Models<br>\n\t6.3 Hyperparamater tunning for selected models<br>\n\t6.4 Plot learning curves<br>\n\t6.4 Feature importance of the tree based classifiers<br>\n7. Ensemble modeling<br>\n\t7.1 Combining models<br>\n8. Prediction\n\n","82ed5427":"# <font size=\"4\">6.3 Hyperparameter tuning for selected models<\/font>\n","335b09a1":"# 3. Load and check data"}}