{"cell_type":{"047cc26e":"code","a2ded381":"code","e38405f7":"code","47c14567":"code","0aa1a773":"code","56799076":"code","8853d5c5":"code","b78a4b44":"code","2e69e90c":"code","3209b6b7":"code","5df021c8":"code","69b4535f":"code","f2741260":"code","b53bd539":"code","188de8eb":"code","cd6793a5":"code","d7e3b195":"code","6a1c756e":"code","dac3fef3":"code","294cc32f":"code","075eb9db":"code","fbcb5ccf":"code","d4a93933":"code","b69396bf":"markdown","9de28e22":"markdown","df26d900":"markdown","53f56492":"markdown","d37e04d3":"markdown","5c44a10b":"markdown","d87ffdbc":"markdown","bfcd62a9":"markdown","19e6c8b8":"markdown","b68b2f99":"markdown","ddfdc5e3":"markdown","6913f003":"markdown","484a80d9":"markdown","47162c4b":"markdown","e34a9966":"markdown","e93b1701":"markdown","18aaffba":"markdown","cdeee7fb":"markdown"},"source":{"047cc26e":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split, ShuffleSplit, cross_validate\nfrom sklearn import feature_selection\nfrom sklearn import metrics\nfrom sklearn import linear_model, ensemble, gaussian_process\nfrom xgboost import XGBClassifier","a2ded381":"df_train_orig = pd.read_csv('..\/input\/train.csv')\ndf_test_orig = pd.read_csv('..\/input\/test.csv')\n\ndf_train = df_train_orig.copy(deep=True)\ndf_train.name = 'Training set'\ndf_test = df_test_orig.copy(deep=True)\ndf_test.name = 'Test set'","e38405f7":"print(df_train_orig.info())\ndf_train_orig.sample(10)","47c14567":"def show_nulls(df):\n    print('{} columns with null values '.format(df.name))\n    print(df.isnull().sum())\n    print(\"\\n\")\n    \nfor df in [df_train, df_test]:\n    show_nulls(df)","0aa1a773":"for df in [df_train, df_test]:    \n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n    \ndf_train.drop(['PassengerId','Cabin', 'Ticket'], axis=1, inplace=True)","56799076":"for df in [df_train, df_test]:\n    show_nulls(df)","8853d5c5":"df_survive = df_train_orig['Survived'].value_counts()\nprint(df_survive)\nax = df_survive.plot.bar()\nax.set_xticklabels(('Not Survived', 'Survived'))","b78a4b44":"for df in [df_train, df_test]:    \n    df['Family_Members'] = df['SibSp'] + df['Parch'] + 1\n    \n    df['Is_Alone'] = 1\n    df['Is_Alone'].loc[df['Family_Members'] > 1] = 0\n    \n    df['Title'] = df['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0] \n\ndf_train.sample(10)","2e69e90c":"df_train['Title'].value_counts()","3209b6b7":"train_title_names = (df_train['Title'].value_counts() < 10)\ndf_train['Title'] = df_train['Title'].apply(lambda x: 'Other' if train_title_names.loc[x] == True else x)\n\ndf_train['Title'].value_counts()","5df021c8":"test_title_names = (df_test['Title'].value_counts() < 10)\ndf_test['Title'] = df_test['Title'].apply(lambda x: 'Other' if test_title_names.loc[x] == True else x)\n\ndf_test['Title'].value_counts()","69b4535f":"le = LabelEncoder()\nfor df in [df_train, df_test]:    \n    df['Sex_Label'] = le.fit_transform(df['Sex'])\n    df['Embarked_Label'] = le.fit_transform(df['Embarked'])\n    df['Title_Label'] = le.fit_transform(df['Title'])\n    \ndf_train.head()","f2741260":"X_cols = ['Sex', 'Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'Family_Members', 'Is_Alone']\n\ndf_train_dummy = pd.get_dummies(df_train[X_cols])\ndf_test_dummy = pd.get_dummies(df_test[X_cols])\ndf_train_dummy['Survived'] = df_train['Survived']\n\ndf_train_dummy.head()","b53bd539":"X_train = df_train_dummy.drop(['Survived'], axis=1)\nY_train = df_train_dummy['Survived']","188de8eb":"seed = 0\n\nmodels = [ensemble.RandomForestClassifier(n_estimators=65, min_impurity_decrease=0.1, random_state=seed),\n          ensemble.AdaBoostClassifier(n_estimators=11, algorithm='SAMME.R'),\n          ensemble.GradientBoostingClassifier(loss='exponential', learning_rate=0.01, n_estimators=100, criterion='friedman_mse', max_depth=4),\n         linear_model.LogisticRegressionCV(cv=3, penalty='l2', solver='newton-cg'),\n         XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100)]\n\nfits = [model.fit(X_train, Y_train) for model in models]\n\nfits","cd6793a5":"Y_hats = {model.__class__.__name__: model.predict(X_train) for model in models}","d7e3b195":"cv_split = ShuffleSplit(n_splits=10, test_size=.3, train_size=.6, random_state=seed)\ncv_split","6a1c756e":"for model in models:\n    cv_results = cross_validate(model, X_train, Y_train, cv=cv_split)\n    print(model.__class__.__name__)\n    print(cv_results)","dac3fef3":"for model, Y_hat in Y_hats.items():\n    print(model)\n    print(metrics.classification_report(Y_train, Y_hat, target_names=['Not Survived', 'Survived']))\n    print('\\n')","294cc32f":"submission_model = models[4]\nsubmission_model","075eb9db":"submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = df_test_orig['PassengerId']\nsubmission_df['Survived'] = submission_model.predict(df_test_dummy)","fbcb5ccf":"submission_df.head(10)","d4a93933":"submission_df.to_csv('submissions.csv', header=True, index=False)","b69396bf":">> The labeled columns are converted to one-hot encoding with `get_dummies()` function. The given columns are converted to one-hot encoding if they are labels of the categorical data.","9de28e22":">> ## 2.2 Predictions\nEach of the predictions predicted by the models are stored in Y_hats. It's dictionary of model name:predictions pairs.","df26d900":">> Since the Title column is categorical, we can group up some values to a big one. Titles like Master and Dr might have a higher priority at the evacuation, so this feature might be worth exploring. We are going group Titles that are coming after Dr to Other because their titles are not as significant as others I think.","53f56492":">> Titles that are less than 7, are grouped into Other, so we keep the number of doctors.","d37e04d3":">> ## 0.2. Loading the data set\nAfter loading the train and test sets to the memory, copying them recursively with the `copy()` function because we don't want changes to be reflected to the original data frame. After that, assigning a name attribute for data frames for later use\n","5c44a10b":"> # 2. Machine Learning\n>> ## 2.1 Models\nWe are going to try several types of machine learning algorithms with different parameters in this part. First storing the models in a list and fitting them.","d87ffdbc":"> # 4. Submission\nWe can conclude that the highest performance is achieved by the XGBClassifier model, so we are using it in the final submission..","bfcd62a9":">> ## 1.6. Separating X and Y\n* Pclass, SibSp, Parch, Age, Fare, Family_Members, Is_Alone, Sex_female, Sex_male, Embarked_C, Embarked_Q, Embarked_S, Title_Master, Title_Miss, Title_Mr, Title_Mrs, Title_Other is  X_train (Training Input)\n* Survived is Y_train (Training Output)","19e6c8b8":">> Checking the null values again and we can see that there are no null values left in the training set since we dropped the Cabin column","b68b2f99":">> ## 1.2. Fixing null values\nAs seen from the random sample, the Cabin and Age columns have null values. They have to be managed but let's see which columns also have null values and how many. This function below outputs the sum of null values in all columns in both training and test set.","ddfdc5e3":">> ## 1.3. Checking the distribution of data\nY_train is not equally distributed, but the gap is not that big, so the bias is not significant. We don't need to balance the distribution in this case.","6913f003":">> ## 3.2 F1-score\nEvaluating the models with F1-score.","484a80d9":">> ## 1.4. Feature Engineering\n* Family_Members is created by adding SibSp, Parch and 1. Since we know that SibSp is siblings and spouse, and Parch is parents and children, we can add those columns to find the count of family members of the person. Finally, adding 1 is the person himself or herself.\n* Is_Alone column is based on the number of Family_Members. If Family_Members' value is more than 1, Is_Alone is set to 0, otherwise it is set to 1\n* Title column is created by extracting prefix before the Name column","47162c4b":"> # 1. Data Analysis\n>> ## 1.1. Overview\n* We use `info()` to get an overview of the types of the features\n* Using `sample(10)` to get random 10 rows from the training set","e34a9966":"> # 0. Setup\n>> ## 0.1. Libraries\n* NumPy and pandas are used for exploratory data analysis in order to summarize the main characteristics of the data\n* NumPy and pandas are also used for feature engineering which will come in handy later in machine learning\n* matplotlib is used for visualization in order to assist data analysis\n* Scikit-learn is used for the machine learning algorithms, train\/test split, model evaluation, etc.","e93b1701":"> # 3. Evaluation\n>> ## 3.1 Cross-validation\nWe are scoring the model with cross-validation. `ShuffleSplit()` is a random permutation cross-validator which yields indices to split data into training and test sets. Evaluating the models with the cross-validation by their train_score and test_score. ","18aaffba":">> ## 1.5. Categorical to dummy\nCategorical data are transformed to numerical data with the `LabelEncoder()` from scikit-learn. It basically labels the categories from 0 to n.","cdeee7fb":">> * Training set have null values in Age, Cabin and Embarked columns\n* Test set have null values in Age, Fare and Embarked columns.\n\n>> The percentage of null values in Age, Embarked and Fare columns are relatively smaller compared to the length of the sets, but more than 80% of the Cabin column are null values in both training and test tests. In this case, we fill the null values of Age column with median, Embarked column with mode since it is categorical and Fare column with median.\n\n>> Since the large portion of the Cabin column is missing and if we fill the null values, it will dramatically affect the training accuracy. The model will probably fail on test set. That's why we are dropping Cabin column along with PassengerId and Ticket in the training set. PassengerId and Ticket columns are dropped because they are unique values and they don't have any effect on the output."}}