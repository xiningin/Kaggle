{"cell_type":{"7ffe29e8":"code","8280c4b3":"code","585ccf40":"code","2e9e6eea":"code","cd8c62b4":"code","d83e1774":"code","0fb2df83":"code","e9188cc4":"code","1585271a":"code","c732c8ea":"code","593aba4f":"code","172c1218":"code","3e68a1d3":"code","95c7436c":"code","e705cedd":"code","d6d2695a":"code","47e91140":"code","acfdeccf":"code","4d980281":"markdown","4a86208c":"markdown","338fc00d":"markdown","45433c70":"markdown"},"source":{"7ffe29e8":"import os\nimport gc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.model_selection import KFold\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, LongformerTokenizer","8280c4b3":"device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )","585ccf40":"class config:\n    batch_size = 4\n    acc_steps = 8\n    max_len = 2048\n    lr = 2e-5\n    weight_decay=1e-3","2e9e6eea":"segment2label = {\n    \"B\": 0,\n    \"I\": 1,\n    \"O\": 2\n}\n\nlabel2segment = {\n    0: \"B\",\n    1: \"I\",\n    2: \"O\"\n}\n\ndiscourse2label={\n    'Lead': 0,\n    'Position' : 1,\n    'Evidence' : 2,\n    'Claim' : 3,\n    'Concluding Statement' : 4,\n    'Counterclaim' : 5,\n    'Rebuttal': 6,\n    'O': 7\n}\nlabel2discourse={\n    0: 'Lead',\n    1: 'Position',\n    2: 'Evidence',\n    3: 'Claim',\n    4: 'Concluding Statement',\n    5: 'Counterclaim',\n    6: 'Rebuttal',\n    7: 'O'\n}","cd8c62b4":"class FeedbackModel(nn.Module):\n    def __init__(self):\n        super(FeedbackModel, self).__init__()\n        modelconfig = AutoConfig.from_pretrained(config.model_name)\n\n        self.backbone = AutoModel.from_pretrained(config.model_name)\n        self.fc_segment = nn.Linear(modelconfig.hidden_size, 3)\n        self.fc_discourse = nn.Linear(modelconfig.hidden_size, 8)\n    \n    def forward(self, input_ids, attn_mask):\n        attn_outputs = self.backbone(input_ids, attn_mask)\n        ysegment   = self.fc_segment(attn_outputs.last_hidden_state)\n        ydiscourse = self.fc_discourse(attn_outputs.last_hidden_state)\n        return ysegment, ydiscourse","d83e1774":"class FeedbackDataset( torch.utils.data.Dataset ):\n    def __init__(self, df, tokenizer):\n        self.tokenizer=tokenizer\n        df=df.copy()\n        self.content = df.content.values\n    \n    def get_tokenized_inputs(self, essay):\n        tokenized_inputs = self.tokenizer(essay, is_split_into_words=True)\n        word_ids = tokenized_inputs.word_ids()\n        return (tokenized_inputs, word_ids)\n    \n    \n    def __getitem__(self, idx):\n        essay  = self.content[idx]\n        (tokenized_inputs, word_ids) = self.get_tokenized_inputs(essay)\n        word_ids[0] = -100\n        word_ids[-1] = -100\n        \n        input_ids = tokenized_inputs['input_ids'][:config.max_len]\n        attn_mask = tokenized_inputs['attention_mask'][:config.max_len]\n        word_ids = word_ids[:config.max_len]\n        seq_len = len(input_ids)\n        \n        if seq_len < config.max_len:\n            len_diff = config.max_len - seq_len\n            attn_mask += [0] * len_diff\n            input_ids += [self.tokenizer.pad_token_id] * len_diff\n            word_ids += [-100] * len_diff\n        \n        rpercentile = ((1 + np.arange(0, config.max_len))\/seq_len) - 0.5        \n        input_ids=torch.tensor(input_ids, dtype=torch.long)\n        attn_mask = torch.tensor(attn_mask, dtype=torch.long)\n        seq_len = torch.tensor(seq_len, dtype=torch.long)\n        word_ids= torch.tensor(word_ids, dtype=torch.long)\n        rpercentile = torch.tensor(rpercentile, dtype=torch.float32)\n        \n        return {\n            'input_ids': input_ids,\n            'attn_mask': attn_mask,\n            'word_ids': word_ids,\n            'seq_len': seq_len,\n            'rpercentile': rpercentile\n        }\n    def __len__(self):\n        return len(self.content)","0fb2df83":"tokenizer=AutoTokenizer.from_pretrained(\n    '..\/input\/longformer-base-tokenizer\/longformer_base_tokenizer'\n)\nmodels = [\n    #torch.load('..\/input\/longformer-multitask-baselinemodel\/model1.pt', map_location = device),\n    #torch.load('..\/input\/longformer-multitask-baselinemodel\/model2.pt', map_location = device),\n    torch.load('..\/input\/feedback-longformer-fold0\/model.pt', map_location=device),\n    \n    #torch.load('..\/input\/longformer-baseline-multitask2-models\/model3.pt', map_location=device),\n    #torch.load('..\/input\/longformer-baseline-multitask2-models\/model4.pt', map_location=device)\n]","e9188cc4":"def read_essay(filename):\n    essay_folder='..\/input\/feedback-prize-2021\/test'\n    filepath = os.path.join(essay_folder, filename)\n    essay = ''\n    with open(filepath) as file:\n        essay = file.read()\n    essay=essay.split()\n    return essay","1585271a":"test_files = os.listdir('..\/input\/feedback-prize-2021\/test')\ntest_df = []\nfor filename in test_files:\n    test_df.append({\n        'id': filename.replace(\".txt\", ''),\n        'content': read_essay(filename)\n    })\ntest_df = pd.DataFrame.from_dict(test_df)","c732c8ea":"val_dataset   = FeedbackDataset(test_df, tokenizer)\nval_dataloader = torch.utils.data.DataLoader(val_dataset,\n                                             batch_size=2,\n                                             shuffle=False,\n                                             drop_last=False\n                                            )","593aba4f":"def postprocess( y, word_ids):\n    seq_len = len(y)\n    prv_word_id=None\n    predSegment=[]\n    predTokens=[]\n    \n    preds=[]\n    for i in range(seq_len):\n        word_id = word_ids[i]\n        if  (word_id== -100) or (prv_word_id == word_id):\n            continue\n        prv_word_id = word_id\n        if y[i] not in label2discourse:\n            continue\n        \n        segment = label2discourse[ y[i] ]\n        if segment == 'O':\n            continue\n        predSegment.append(segment)\n        predTokens.append( word_id )\n    \n    if len(predSegment) == 0:\n        return []\n    \n    if len(predSegment) == 1:\n        preds.append({\n            'segment': predSegment[0],\n            'word_ids': [predTokens[0]]\n        })\n        return preds\n    else:\n        num_tokens=len(predTokens)\n        prv_id=0\n        cur_id=0\n        prv_segment=predSegment[0]\n        \n        for i in range(1, num_tokens+1):\n            cur_id=i\n            if (i!=num_tokens) and (predTokens[i] == 1+predTokens[i-1]) and (predSegment[i] == predSegment[i-1]):\n                continue\n            \n            pred_token_list=[]\n            for j in range(prv_id, cur_id):\n                pred_token_list.append(predTokens[j])\n            \n            preds.append({\n                'segment': prv_segment,\n                'word_ids': pred_token_list\n            })\n            if i!=num_tokens:\n                prv_segment = predSegment[i]\n                prv_id=cur_id\n    return preds","172c1218":"def prediction(val_dataloader):\n    predvalues = []\n    for inputs in val_dataloader:\n        input_ids = inputs['input_ids']\n        attn_mask = inputs['attn_mask']\n        word_ids = inputs['word_ids']\n        seq_len = inputs['seq_len']\n        rpercentile = inputs['rpercentile']\n        batch_max_seqlen = torch.max(seq_len).item()\n        \n        input_ids = input_ids[:, :batch_max_seqlen].to(device)\n        attn_mask = attn_mask[:, :batch_max_seqlen].to(device)\n        rpercentile = rpercentile[:, :batch_max_seqlen].to(device)\n        \n        bsize = attn_mask.shape[0]\n        yhat_discourse = torch.zeros((bsize, batch_max_seqlen, 8))\n        \n        for model in models:\n            model.eval()\n            with torch.no_grad():\n                (_, ycur_discourse)  = model(input_ids, attn_mask)\n                #yhat_segment = yhat_segment.softmax(dim=-1).argmax(dim=-1).cpu()\n                ycur_discourse = ycur_discourse.softmax(dim=-1).cpu()\n                yhat_discourse += ycur_discourse\n        yhat_discourse = yhat_discourse\/len(models)\n        yhat_discourse = yhat_discourse.argmax(dim=-1)\n        \n        for i in range(bsize):\n            yhat_discourse_i = yhat_discourse[i].numpy()\n            word_ids_i = word_ids[i].numpy()\n            \n            pred_tokens = postprocess(yhat_discourse_i, word_ids_i)\n            for token in pred_tokens:\n                token['word_ids'] = [str(x) for x in token['word_ids']]\n                token['word_ids'] = ' '.join(token['word_ids'])\n            predvalues.append(pred_tokens)\n    return predvalues","3e68a1d3":"test_df['predvalues'] = prediction(val_dataloader)\ntest_df.head()","95c7436c":"min_number_threshold={\n    'Lead': 8,\n    'Position': 5,\n    'Evidence': 11,\n    'Claim': 3,\n    'Concluding Statement': 9,\n    'Counterclaim': 5,\n    'Rebuttal': 4\n}","e705cedd":"submission_data=[]\nfor index, row in test_df.iterrows():\n    predvalues = row.predvalues\n    \n    for pred in predvalues:\n        submission_data.append({\n            'id': row.id,\n            'class': pred['segment'],\n            'predictionstring': pred['word_ids']\n        })\nsubmission_df = pd.DataFrame.from_dict(submission_data)\nsubmission_df.head()\n","d6d2695a":"def filter_submission(row):\n    segment = row['class']\n    threshold = min_number_threshold[segment]\n    num_words = len(row.predictionstring.split())\n    if num_words <= threshold:\n        return False\n    return True","47e91140":"submission_df = submission_df [ submission_df.apply(filter_submission, axis=1) ].copy()\nsubmission_df.head()\n","acfdeccf":"submission_df.to_csv(\"submission.csv\", index=False)","4d980281":"this threshold is picked from the training data with <1% number of words","4a86208c":"# model","338fc00d":"# dataset","45433c70":"# load models"}}