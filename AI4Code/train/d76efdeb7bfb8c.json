{"cell_type":{"2c597b7b":"code","e0fff6ea":"code","3fccc652":"code","c2ca6598":"code","4b4f513d":"code","7c7337e2":"code","02e92e89":"code","f7061d87":"code","5b81a418":"code","906a5cdb":"code","ac99cbe7":"code","4e9deea4":"code","a5805676":"code","e9823813":"code","f3d27afa":"code","6e2318cb":"code","a9c86495":"markdown","baad7264":"markdown","9a1abeb8":"markdown","64a88a69":"markdown","519e01af":"markdown","3274b44c":"markdown","3e12c039":"markdown","1193daef":"markdown","9bfe6f18":"markdown","26bf0ffb":"markdown","5f1b841a":"markdown","c47c786f":"markdown","03a7adde":"markdown","fdeda7c7":"markdown","5175dda2":"markdown","6e5b0183":"markdown"},"source":{"2c597b7b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if filename.endswith('.jpg'):\n            break\n        print(os.path.join(dirname, filename))","e0fff6ea":"sample_sub = pd.read_csv('..\/input\/herbarium-2020-fgvc7\/sample_submission.csv')\ndisplay(sample_sub)","3fccc652":"import json, codecs\nwith codecs.open(\"..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/metadata.json\", 'r',\n                 encoding='utf-8', errors='ignore') as f:\n    train_meta = json.load(f)\n    \nwith codecs.open(\"..\/input\/herbarium-2020-fgvc7\/nybg2020\/test\/metadata.json\", 'r',\n                 encoding='utf-8', errors='ignore') as f:\n    test_meta = json.load(f)","c2ca6598":"display(train_meta.keys())","4b4f513d":"train_df = pd.DataFrame(train_meta['annotations'])\ndisplay(train_df)","7c7337e2":"train_cat = pd.DataFrame(train_meta['categories'])\ntrain_cat.columns = ['family', 'genus', 'category_id', 'categort_name']\ndisplay(train_cat)","02e92e89":"train_img = pd.DataFrame(train_meta['images'])\ntrain_img.columns = ['file_name', 'height', 'image_id', 'license', 'width']\ndisplay(train_img)","f7061d87":"train_reg = pd.DataFrame(train_meta['regions'])\ntrain_reg.columns = ['region_id', 'region_name']\ndisplay(train_reg)","5b81a418":"train_df = train_df.merge(train_cat, on='category_id', how='outer')\ntrain_df = train_df.merge(train_img, on='image_id', how='outer')\ntrain_df = train_df.merge(train_reg, on='region_id', how='outer')","906a5cdb":"print(train_df.info())\n\ndisplay(train_df)\n","ac99cbe7":"na = train_df.file_name.isna()\nkeep = [x for x in range(train_df.shape[0]) if not na[x]]\ntrain_df = train_df.iloc[keep]","4e9deea4":"dtypes = ['int32', 'int32', 'int32', 'int32', 'object', 'object', 'object', 'object', 'int32', 'int32', 'int32', 'object']\nfor n, col in enumerate(train_df.columns):\n    train_df[col] = train_df[col].astype(dtypes[n])\nprint(train_df.info())\ndisplay(train_df)","a5805676":"test_df = pd.DataFrame(test_meta['images'])\ntest_df.columns = ['file_name', 'height', 'image_id', 'license', 'width']\nprint(test_df.info())\ndisplay(test_df)","e9823813":"#train_df.to_csv('full_train_data.csv', index=False)\n#test_df.to_csv('full_test_data.csv', index=False)","f3d27afa":"print(len(train_df.category_id.unique()))","6e2318cb":"sub = pd.DataFrame()\nsub['Id'] = test_df.image_id\nsub['Predicted'] = list(map(int, np.random.randint(1, 32093, (test_df.shape[0]))))\ndisplay(sub)\nsub.to_csv('submission.csv', index=False)","a9c86495":"Now, we will be unifying the metadata from the `*.json` files. We will first work with the `train` data.","baad7264":"Next is for `plant categories`:","9a1abeb8":"# Last Steps\n\nBefore we end this kernel, let's check the total number of targets for this dataset:","64a88a69":"For the `*.json` files, we cannot load them to a DataFrame as there's two items that prevents this: `license` and `info`. So, I manually read the `*.json` files as follows:","519e01af":"Perfect!\n\nNow, we can go ahead and save this dataframe as a `*.csv` file for future use!","3274b44c":"After selecting the `non-NaN` items, we now reiterate on their file types. We need to save on memory, as we reached `102+ MB` for this DataFrame Only.","3e12c039":"# Peek\n\nThis notebook is here to just unify the dataset into one. I will perform further analysis and the Deep Learning algorithm in a future kernel. If you like this kernel, or forked this version, please upvote.\n\nFirst step, we peek at the data paths:","1193daef":"First, we access the `annotations` list and convert it to a df.","9bfe6f18":"# Submission\n\nLet's create a submission file to check the format! I'll be using a random number generator for the targets!","26bf0ffb":"Looking closer, there's a line with `NaN` values there. We need to remove rows with `NaN`s so we proceed to the next line:","5f1b841a":"Followed by the `image properties`:","c47c786f":"Finally, for our `test` dataset. Since it only contains one key, `images`:","03a7adde":"Since there's a lot of images included there, we only checked non-image files and got the three above. Next, we will load the sample submission and check.","fdeda7c7":"It's a shocking `32,093` unique targets! I can't think of how to approach this to simplify the data so for now, let's end it here!","5175dda2":"Then, we will merge all the DataFrames and see what we got:","6e5b0183":"And lastly, the `region`:"}}