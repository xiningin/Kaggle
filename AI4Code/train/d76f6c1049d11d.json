{"cell_type":{"c1a34057":"code","32ab540a":"code","7489c8fa":"code","645e06f7":"code","a9b517bb":"code","77e2f8ca":"code","618bae33":"code","06a57e06":"code","7cdf72e6":"code","a1f9465d":"code","b734706f":"code","6cfc52da":"code","5c60e09f":"code","30a97253":"code","f39d0af3":"code","778f6e59":"code","3d8ec22d":"code","737fc87f":"code","79b04500":"code","4cbc4cdc":"code","bfcc0a97":"code","86c0e6ea":"code","1ed09f5f":"code","7c19b728":"code","25580b4f":"code","df63e13d":"code","e18a02b3":"code","07bf49fd":"code","fd816401":"code","e6c844e0":"code","c23dea55":"code","802afbf7":"code","f1e426ae":"code","e11d3766":"code","d5cb72e2":"code","8be0e856":"code","a49b76ae":"code","55599055":"code","7540b591":"code","df0babad":"code","ab140214":"code","235fef01":"code","b142f79a":"code","9b762198":"code","f7e0b3b5":"code","f091ac4b":"code","c21474ec":"code","7b87d2c7":"markdown","39e1f7df":"markdown","424ba7a8":"markdown","dfc2489f":"markdown","c882ffb0":"markdown","f43d51a9":"markdown","40c02418":"markdown","94474154":"markdown","44704e96":"markdown","a11496a7":"markdown","6101ad90":"markdown","15e6202d":"markdown","1cfcd857":"markdown","193aeb51":"markdown","488bf6fe":"markdown","0b7bd1c8":"markdown","6b7bd7b3":"markdown","0d954144":"markdown"},"source":{"c1a34057":"# Importing the neccesary libraries we are going to need\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nsns.set()","32ab540a":"path = '\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv'\n\ndf = pd.read_csv(path)","7489c8fa":"df.head(5)","645e06f7":"df.info()\n#From the result we see that the dataset is clean i.e no misssing values","a9b517bb":"grade = [] #Declaring a new list\nfor i in df['quality']: \n    if i > 6.5:\n        i = 1\n        grade.append(i)\n    else:\n        i = 0\n        grade.append(i)\ndf['grade'] = grade # A new column to hold our already categoried quality ","77e2f8ca":"df.head(10)","618bae33":"df.drop('quality', axis = 1, inplace = True) #Dropping the quality column since we won't be needing it anymore","06a57e06":"df.describe() #shows describption for only numerical variables","7cdf72e6":"sns.distplot(df['fixed acidity']) #we can see those few outliers shown by the longer right tail of the distribution","a1f9465d":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['fixed acidity'].quantile(0.99)\ndf = df[df['fixed acidity'] < q]\n\nsns.distplot(df['fixed acidity'])","b734706f":"sns.distplot(df['volatile acidity']) #we can see those few outliers shown by the longer right tail of the distribution","6cfc52da":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['volatile acidity'].quantile(0.99)\ndf = df[df['volatile acidity'] < q]\n\nsns.distplot(df['volatile acidity'])","5c60e09f":"sns.distplot(df['citric acid']) #we can see those few outliers shown by the longer right tail of the distribution","30a97253":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['citric acid'].quantile(0.99)\ndf = df[df['citric acid'] < q]\n\nsns.distplot(df['citric acid'])","f39d0af3":"sns.distplot(df['residual sugar']) #we can see those few outliers shown by the longer right tail of the distribution","778f6e59":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['residual sugar'].quantile(0.99)\ndf = df[df['residual sugar'] < q]\n\nsns.distplot(df['residual sugar'])","3d8ec22d":"sns.distplot(df['chlorides']) #we can see those few outliers shown by the longer right tail of the distribution","737fc87f":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['chlorides'].quantile(0.99)\ndf = df[df['chlorides'] < q]\n\nsns.distplot(df['chlorides'])","79b04500":"sns.distplot(df['free sulfur dioxide']) #we can see those few outliers shown by the longer right tail of the distribution","4cbc4cdc":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['free sulfur dioxide'].quantile(0.99)\ndf = df[df['free sulfur dioxide'] < q]\n\nsns.distplot(df['free sulfur dioxide'])","bfcc0a97":"sns.distplot(df['total sulfur dioxide']) #we can see those few outliers shown by the longer right tail of the distribution","86c0e6ea":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['total sulfur dioxide'].quantile(0.99)\ndf = df[df['total sulfur dioxide'] < q]\n\nsns.distplot(df['total sulfur dioxide'])","1ed09f5f":"sns.distplot(df['density']) #we can see those few outliers shown by the longer left tail of the distribution","7c19b728":"#Removing the bottom 1% of the observation will help us to deal with the outliers\nq = df['density'].quantile(0.01)\ndf = df[df['density'] > q]\n\nsns.distplot(df['density'])","25580b4f":"sns.distplot(df['pH']) #we can see those few outliers shown by the longer right tail of the distribution","df63e13d":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['pH'].quantile(0.99)\ndf = df[df['pH'] < q]\n\nsns.distplot(df['pH'])","e18a02b3":"sns.distplot(df['sulphates']) #we can see those few outliers shown by the longer right tail of the distribution","07bf49fd":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['sulphates'].quantile(0.99)\ndf = df[df['sulphates'] < q]\n\nsns.distplot(df['sulphates'])","fd816401":"sns.distplot(df['alcohol']) #we can see those few outliers shown by the longer right tail of the distribution","e6c844e0":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['alcohol'].quantile(0.99)\ndf = df[df['alcohol'] < q]\n\nsns.distplot(df['alcohol'])","c23dea55":"df.columns.values","802afbf7":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\n# the target column (in this case 'grade') should not be included in variables\n#Categorical variables may or maynot be added if any\nvariables = df[['fixed acidity', 'volatile acidity', 'citric acid',\n       'residual sugar', 'chlorides', 'free sulfur dioxide',\n       'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol',]]\nx = add_constant(variables)\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(x.values,i) for i in range (x.shape[1])]\nvif['features'] = x.columns\nvif\n\n#Using 10 as the minimum vif values i.e any independent variable 10 and above will have to be dropped\n#From the results all independent variable are below 10","f1e426ae":"#Declaring independent variable i.e x\n#Declaring Target variable i.e y\nx = df.drop('grade', axis =1 )\ny = df['grade']","e11d3766":"scaler = StandardScaler()\nscaler.fit(x)\nscaled_x = scaler.transform(x)","d5cb72e2":"#Splitting our data into train and test dataset\nx_train, x_test, y_train, y_test = train_test_split(scaled_x, y , test_size = 0.2, random_state  = 365)","8be0e856":"reg = LogisticRegression() #select the algorithm\nreg.fit(x_train,y_train) # we fit the algorithm with the training data and the training output","a49b76ae":"y_hat = reg.predict(x_test) # y_hat holding the prediction made with the algorithm using x_test","55599055":"acc = metrics.accuracy_score(y_hat,y_test)# To know the accuracy\nacc","7540b591":"reg.intercept_ # Intercept of the regression","df0babad":"reg.coef_ # coefficients of the variables \/ features ","ab140214":"result = pd.DataFrame(data = x.columns, columns = ['Features'])\nresult['weight'] = np.transpose(reg.coef_)\nresult['odds'] = np.exp(np.transpose(reg.coef_))\nresult","235fef01":"cm = confusion_matrix(y_hat,y_test)\ncm","b142f79a":"# Format for easier understanding\ncm_df = pd.DataFrame(cm)\ncm_df.columns = ['Predicted 0','Predicted 1']\ncm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\ncm_df","9b762198":"from sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours","f7e0b3b5":"dd = DecisionTreeClassifier()\ndd.fit(x_train,y_train)\ny_1 = dd.predict(x_test)\nacc_1 = metrics.accuracy_score(y_1,y_test)\nacc_1","f091ac4b":"sv = svm.SVC() #select the algorithm\nsv.fit(x_train,y_train) # we train the algorithm with the training data and the training output\ny_2 = sv.predict(x_test) #now we pass the testing data to the trained algorithm\nacc_2 = metrics.accuracy_score(y_2,y_test)\nacc_2","c21474ec":"knc = KNeighborsClassifier()\nknc.fit(x_train,y_train)\ny_3 = knc.predict(x_test)\nacc_3 = metrics.accuracy_score(y_3,y_test)\nacc_3","7b87d2c7":"### LOGISTIC REGRESSION","39e1f7df":"### DEALING WITH MISSING VALUES ","424ba7a8":"####  plotting the distribution of our numerical variables will help us to detect outliers and any other abnormalities","dfc2489f":"#### Let's check that our dataset are not violating any of this assumptions which includes:\n#### 1. No Endogeneity\n#### 2. Normality and Homoscedasticity\n#### 3.No Autocorrelation\n#### 4.NO multicollinearity: making sure our independents variables are not strongly related(correlated) with each other\n\n####  We are not violating  assumptions 1 through 3 but for NO multicollinearity we need to check","c882ffb0":"### CHECKING OLS ASSUMPTIONS","f43d51a9":"#### The first thing we will want to to do is categories our target variable that is 'quality' into 'good' or 'bad'. In this case if the quality is greater than 6.5  the quality is good value less or equal to 6.5 is represented as bad\n#### NOTE that 'good' is represented by 1 while 'bad' by 0","40c02418":"### OUTLIERS","94474154":"#### Our model predicted '0' correctly 250 times while predicting '0' incorrectly 18 times\n#### Also it predicted  '1'  correctly 10 times while predicting '1' incorrectly 4 times","44704e96":"### INTRODUCTION\n\n#### Predicting the quality of wine mainly through logistic regression\n#### Wine quality was classified into two categories  good(0) and bad(1)\n#### Steps taken in preprocessing includes Data cleaning, Outliers Removal, Standardization etc\n\n### SIDE NOTE\n#### You can leave your question about any unclear part in the comment section\n#### Any correction will be highly welcomed","a11496a7":"#### If you find this notebook useful don't forget to upvote. #Happycoding","6101ad90":"### Standardization","15e6202d":"### CONFUSION MATRIX","1cfcd857":"###  USING OTHER MODELS","193aeb51":"#### Standardizing helps to give our independent varibles a more standard and relatable numeric scale, it also helps in improving model accuracy","488bf6fe":"#### Remember we standardized all independents variables so the odds values have no direct interpretation\n#### Nevertheless using acohol as an example we can say for one standard deviation increase in acohol it is twice more likely to cause a change in our target variables","0b7bd1c8":"### DATA CLEANING","6b7bd7b3":"#### After comparison to some other models, LogisticRegression still gives us the highest (~92%)","0d954144":"### LOADING THE DATASET"}}