{"cell_type":{"314b5048":"code","aa5855c0":"code","b236f130":"code","76a4d5d6":"code","94aa1072":"code","e6f18c2b":"code","8a7b7bd7":"code","67e0ac81":"code","30e8c4ce":"code","d75f457a":"code","90dbf26d":"code","c996a415":"code","56c6f163":"code","465d0953":"code","f0d3b063":"code","4b0e5222":"code","7d1620c2":"code","05cda036":"code","8f04a87f":"code","28220e40":"code","30db5008":"code","ac39c408":"code","57e034ad":"code","b5d156ad":"code","3975c978":"code","81d50200":"code","5082a108":"code","60f83320":"code","5da86cb7":"code","51d1431c":"code","92535a8b":"code","6efbb087":"code","09e8ad44":"code","ab6ab382":"code","44794fe1":"code","9a47d4b2":"code","0fb2af3b":"code","90f6a57a":"code","2bfe8bdd":"code","7031824b":"code","ab8ae1b0":"code","7eac4e09":"code","7708e534":"code","18bc2374":"code","94fa42ee":"code","e8a654df":"code","f45ac29f":"code","81856d5f":"code","a7d9822a":"code","a2622faf":"code","b083ae8b":"code","966134ba":"code","bc04c930":"code","b5d89a6c":"code","303d7c75":"code","3294a781":"code","2118bd41":"code","069af349":"code","df929649":"code","c99597ad":"code","986d4ad9":"code","66c357e3":"code","58cedf9f":"code","c4f42781":"code","7461e12d":"code","99cf0344":"code","ca3e4e9e":"code","01eb4c86":"code","abf24fad":"code","1067978d":"code","b7509b07":"code","d7ed33b8":"code","e880232d":"code","79598e28":"code","c9f50817":"code","4e22d6bb":"code","bcfe6b8a":"code","a471b66c":"code","0d92ccf3":"code","055ab04d":"code","a4ff7f06":"code","a01c8e05":"code","f5693584":"code","c2ecaede":"code","7f03ba99":"code","6afdb8d7":"code","f77ef95c":"code","c979262d":"code","677b82e4":"code","eee422c8":"code","805e534d":"code","3492de9a":"code","66b5d734":"code","aea74bc9":"code","65551f7d":"code","1133efa9":"code","5e445e12":"code","eb323722":"code","85f24d2d":"code","188cf480":"code","877fc33e":"code","a7d0a364":"markdown","883416c3":"markdown","647f3f7a":"markdown","b02a214e":"markdown","878248d7":"markdown","1b8090e7":"markdown","8f7b1bf0":"markdown","0d7036c9":"markdown","3f459b54":"markdown","b3cd50a9":"markdown","e7c3c7e7":"markdown","83defdf5":"markdown","4d6eea44":"markdown","386c1b61":"markdown","b15ca1de":"markdown","e2d33d54":"markdown","1eaadd6c":"markdown","d3e69d80":"markdown","b54f86af":"markdown","d24d551d":"markdown","aef1290b":"markdown","ab868667":"markdown","4324ed84":"markdown","d5a61162":"markdown","7379a3d5":"markdown","dc1196ce":"markdown","41e1bba0":"markdown","60cbef05":"markdown","c30bc674":"markdown","8566647a":"markdown","4b566fa6":"markdown","80506f4d":"markdown","ce75f87e":"markdown","fe4cf494":"markdown","c74399b7":"markdown","3c1bdd3a":"markdown","4190f73e":"markdown","654402af":"markdown","dd3973cf":"markdown","2eb12757":"markdown","9a6018ed":"markdown","ab22439a":"markdown","ddfbae1f":"markdown","7dc96c87":"markdown","a5791d2c":"markdown","1de7024b":"markdown","dc4f8d00":"markdown","f16bceaf":"markdown","11bb29ad":"markdown","a29ca7cd":"markdown","93b74d98":"markdown","3d5e160a":"markdown","411f4df8":"markdown","99f0df11":"markdown","72ede532":"markdown","8801d0bc":"markdown","461e6f4c":"markdown","d990bd10":"markdown","e52b723f":"markdown","98d28e9a":"markdown","c085fff1":"markdown","2f5ba484":"markdown","8e65902a":"markdown","95d33820":"markdown","dc2c44d0":"markdown","1fbba4be":"markdown","4aba3937":"markdown","a080d6e2":"markdown","c742144d":"markdown","7f0c53dc":"markdown","4c25ba4f":"markdown","95d01ccc":"markdown","feff185a":"markdown","ff61e615":"markdown"},"source":{"314b5048":"# General tools\nimport pandas as pd\nimport numpy as np\nimport os, math\nfrom collections import Counter\n\n# Plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nmorancolor=sns.color_palette(['#6a2202', '#bc7201', '#e5ab09', '#22180d', '#0f1a26','#241c24', '#745656', '#c7b44f', '#977f48', '#392c23'])\nplt.style.use(\"fivethirtyeight\")\nsns.set_palette(morancolor)\n\nplt.rcParams['font.family']='serif'\nplt.rcParams['figure.dpi'] =100 # high resolution\n\n# Manage warnings\nimport warnings\nwarnings.filterwarnings('ignore')","aa5855c0":"def despine():\n    sns.despine(top=1,bottom=1,right=1,left=1)\n    \ndef title(title,fontsize=13):\n    plt.title(title,fontweight='bold',fontsize=fontsize)","b236f130":"def countall(df,lst,h=4,w=10,cut=3,hspace=.5,wspace=.25,annotsize=10):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        sns.countplot(df[col])\n        for p in ax.patches:\n            ax.annotate(f\"{p.get_height()\/df[col].shape[0]*100:.2f}%\",xy=[p.get_x(),p.get_height()],fontsize=annotsize)\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold')\n\nfrom scipy.stats import skew\ndef kdeall(df,lst,h=4,w=10,cut=3,hspace=.5,wspace=.25,meanskew=True,kdecut=0,legendsize=10,xlabelsize=13,loc='best'):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        if meanskew==True:\n            sns.kdeplot(df.dropna(subset=[col])[col],cut=kdecut,label=f'Skewness: {skew(df.dropna(subset=[col])[col]):.2f}',lw=3)\n            plt.axvline(df.dropna(subset=[col])[col].mean(),label='mean',color='#22180d',lw=1.5)\n            plt.axvline(df.dropna(subset=[col])[col].median(),label='median',ls='--',color='#22180d',lw=1.5)\n            plt.legend(fontsize=legendsize,loc=loc)\n        else: sns.kdeplot(df.dropna(subset=[col])[col],cut=kdecut,lw=3)\n        sns.rugplot(df.dropna(subset=[col])[col])\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold',fontsize=xlabelsize)\n        \ndef boxall(df,lst,h=4,w=10,cut=3,hspace=.5,wspace=.25,target=None,choose=1,xlabelsize=14,xticksize=12,yticksize=12):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        if target==None: sns.boxplot(df[col])\n        else: \n            if choose==1: sns.boxplot(df[target],df[col])\n            else: sns.boxplot(df[col],df[target])\n        despine()\n        plt.ylabel('')\n        plt.yticks(fontsize=yticksize)\n        plt.xticks(fontsize=xticksize)\n        plt.xlabel(col,fontweight='bold',fontsize=xlabelsize)\n        \ndef pointall(df,lst,target,h=4,w=10,cut=3,hspace=.5,wspace=.25,annotsize=10,choose=1,titlesize=14):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        if choose==1: sns.pointplot(x=df[target],y=df[col],lw=3)\n        else: sns.pointplot(x=df[col],y=df[target],lw=3)\n        despine()\n        plt.ylabel('')\n        plt.xlabel('')\n        plt.title(col,fontweight='bold',fontsize=titlesize)\n\ndef kde2(df,lst,target,h=4,w=10,cut=3,hspace=.5,wspace=.25,legendsize=10,legendlabelsize=12):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        g=sns.kdeplot(df[col],hue=df[target],cut=0,lw=3)\n        plt.setp(g.get_legend().get_texts(), fontsize=legendsize) # for legend text\n        plt.setp(g.get_legend().get_title(), fontsize=legendlabelsize, fontweight='bold') # for legend title\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold')        ","76a4d5d6":"def count_rate(df,x,y,hue,w=10,h=4):\n    f,ax=plt.subplots(1,2,figsize=(w,h))\n    sns.pointplot(data=df,x=x,y=y,hue=hue,ax=ax[0])\n    sns.countplot(data=df,x=x,hue=hue,ax=ax[1])\n    despine()","94aa1072":"train=pd.read_csv(\"..\/input\/loan-prediction-analytics-vidhya\/train_ctrUa4K.csv\")\ntest=pd.read_csv(\"..\/input\/loan-prediction-analytics-vidhya\/test_lAUu6dG.csv\")\ntrain.shape, test.shape","e6f18c2b":"train.info()","8a7b7bd7":"test.info()","67e0ac81":"train.describe(include='O')","30e8c4ce":"train.describe()","d75f457a":"for i in [train,test]:\n    print(f\"\\n{i.isnull().sum().sort_values(ascending=False)}\")","90dbf26d":"countall(train,[\"Loan_Status\"],cut=1,w=8)\ntitle('Distribution of Loan Status',fontsize=17)","c996a415":"train.apply(lambda x: pd.factorize(x)[0]).corr().Loan_Status.abs().sort_values(ascending=False)[1:][::-1].plot(kind=\"barh\",figsize=(7,5))\nplt.yticks(fontsize=11)\nplt.show()","56c6f163":"{i:train[i].nunique() for i in train.select_dtypes('O').columns.tolist()}","465d0953":"catcol=train.select_dtypes('O').columns.tolist()[1:-1]\ncatcol","f0d3b063":"countall(train,catcol,h=7,annotsize=8)\nplt.suptitle('Distribution of categorical features',fontweight='bold',fontsize=17)\nplt.show()","4b0e5222":"numcol=train.select_dtypes('number').columns\nnumcol","7d1620c2":"kdeall(train,numcol,h=7,w=9,wspace=.3)","05cda036":"kde2(train,numcol,'Loan_Status',h=7,w=9,wspace=.4)","8f04a87f":"boxall(train,numcol)","28220e40":"train.Loan_Status=train.Loan_Status.map({'Y':1,'N':0}).astype(int)","30db5008":"pointall(train,catcol,'Loan_Status',choose=0,h=7)","ac39c408":"pointall(train,numcol,'Loan_Status',choose=1,h=7)","57e034ad":"sub=train.drop(columns='Loan_ID').apply(lambda x: pd.factorize(x)[0]).corr()\nplt.figure(figsize=(8,6))\nmask = np.triu(np.ones_like(sub))\nsns.heatmap(sub,vmax=1,vmin=-1,cmap='coolwarm',annot=True,fmt='.2f',mask=mask,annot_kws={\"size\":8},cbar=False)\nplt.yticks(fontsize=9)\nplt.xticks(fontsize=9)\nplt.show()","b5d156ad":"count_rate(train,\"Gender\",\"Loan_Status\",\"Married\")\nplt.suptitle('Loan status by Gender and Married',fontweight='bold')\nplt.show()","3975c978":"count_rate(train,\"Dependents\",\"Loan_Status\",\"Married\")\nplt.suptitle('Loan status by Dependents and Married',fontweight='bold')\nplt.show()","81d50200":"f,ax=plt.subplots(1,2,figsize=(10,4))\nplt.subplots_adjust(wspace=.25)\nsns.violinplot(train.Married,train.CoapplicantIncome,hue=train.Loan_Status,split=True,scale='count',cut=0,ax=ax[0])\nsns.pointplot(train.Married,train.CoapplicantIncome,hue=train.Loan_Status,ax=ax[1])\nplt.suptitle('Loan status by Coapplicant income and Married',fontweight='bold')\ndespine()","5082a108":"f,ax=plt.subplots(1,2,figsize=(10,4))\nfor i,col in enumerate([\"ApplicantIncome\",\"CoapplicantIncome\"]):\n    sns.boxplot(data=train,y='Dependents',x=col,hue='Loan_Status',ax=ax[i])\n    ax[i].set_xscale('log')\n    ax[i].set_title(f'{col} and Dependents',fontsize=13,fontweight='bold')\n    despine()","60f83320":"f,ax=plt.subplots(1,2,figsize=(10,4))\nplt.subplots_adjust(wspace=.25)\nfor i,col in enumerate([\"ApplicantIncome\",\"CoapplicantIncome\"]):\n    sns.pointplot(train.Education,train[col],hue=train.Loan_Status,ax=ax[i])\n    ax[i].set_title(f\"{col} and Education\",fontweight='bold',fontsize=14)\n    despine()\n    plt.yscale('log')","5da86cb7":"f,ax=plt.subplots(1,2,figsize=(10,4))\nplt.subplots_adjust(wspace=.25)\nfor i,col in enumerate([\"ApplicantIncome\",\"CoapplicantIncome\"]):\n    sns.pointplot(hue=train.Credit_History,y=train[col],x=train.Loan_Status,ax=ax[i])\n    ax[i].set_title(f\"{col} and Credit history\",fontweight='bold',fontsize=13)\n    despine()\n    plt.yscale('log')","51d1431c":"train.drop(columns='Loan_ID').apply(lambda x: pd.factorize(x)[0]).corr().Loan_Status.abs().sort_values(ascending=False)[1:][::-1].plot(kind='barh',figsize=(7,5))\nplt.yticks(fontsize=12)\ndespine()","92535a8b":"df=pd.concat([train,test]).drop('Loan_ID',axis=1)\ndf.info()","6efbb087":"df.isnull().sum().sort_values(ascending=False)","09e8ad44":"for i in [\"Married\",\"Gender\",\"Loan_Amount_Term\",\"Dependents\",\"Self_Employed\",\"Credit_History\"]:\n    df[i][df[i].isnull()]=df[i].mode()[0]","ab6ab382":"df.isnull().sum()","44794fe1":"df.LoanAmount[(df.LoanAmount.isnull())&(df.Education=='Graduate')]=df.LoanAmount[df.Education=='Graduate'].median()\ndf.LoanAmount[(df.LoanAmount.isnull())&(df.Education!='Graduate')]=df.LoanAmount[df.Education!='Graduate'].median()","9a47d4b2":"# let's check for missing values again\ndf.isnull().sum()","0fb2af3b":"df['TotalIncome']=df.ApplicantIncome+df.CoapplicantIncome\ndf['EMI']=df.LoanAmount\/df.Loan_Amount_Term\ndf['BalanceIncome']=df.TotalIncome-df.EMI*1000 # multiple 1000 to make  units equal","90f6a57a":"df.info()","2bfe8bdd":"train.drop(columns='Loan_ID').apply(lambda x: pd.factorize(x)[0]).corr().Loan_Status.abs().sort_values(ascending=False)[1:][::-1].plot(kind='barh',figsize=(7,5))\nplt.yticks(fontsize=12)\ndespine()","7031824b":"numcol=df.drop('Loan_Status',axis=1).select_dtypes('number').columns.tolist()\nnumcol","ab8ae1b0":"boxall(df,numcol,h=7)","7eac4e09":"logfeature=[\"ApplicantIncome\",\"CoapplicantIncome\",\"LoanAmount\",\"TotalIncome\"]\nkdeall(df.dropna(),logfeature,cut=2,h=7)","7708e534":"df[logfeature]=np.log10(df[logfeature]+1)","18bc2374":"df.isnull().sum().sort_values(ascending=False)","94fa42ee":"kdeall(df.dropna(),logfeature,cut=2,h=7)","e8a654df":"df.boxplot(vert=0,figsize=(7,5))","f45ac29f":"feature=['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'TotalIncome', \"BalanceIncome\", \"EMI\"]","81856d5f":"kdeall(df.dropna(),feature,h=7)\nplt.suptitle('Before scaling',fontweight='bold')\nplt.show()","a7d9822a":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nsub=[\"LoanAmount\",\"ApplicantIncome\",\"TotalIncome\"]\ndf[sub]=sc.fit_transform(df[sub].values)","a2622faf":"from sklearn.preprocessing import Normalizer\nsub=list(set(feature)-set(sub))\nsc=Normalizer()\ndf[sub]=sc.fit_transform(df[sub].values)","b083ae8b":"# let's view our changes\nkdeall(df.dropna(),feature,h=7)","966134ba":"catcol","bc04c930":"from sklearn.preprocessing import OneHotEncoder\noe=OneHotEncoder(drop='if_binary')\ndf[oe.get_feature_names().tolist()]=oe.fit_transform(df[catcol]).toarray()","b5d89a6c":"df.drop(columns=catcol,inplace=True)","303d7c75":"df.info()","3294a781":"test=df[df.Loan_Status.isnull()].drop('Loan_Status',axis=1)\ntrain=df[~df.Loan_Status.isnull()]\ntrain.shape,test.shape","2118bd41":"X=train.drop(\"Loan_Status\",axis=1)\ny=train.Loan_Status\nX.shape,y.shape","069af349":"from sklearn.decomposition import PCA\ndef reduct_pca(X,w=8,h=3):\n    pca=PCA(n_components=X.shape[1],random_state=0)\n    Xpca=pca.fit_transform(X)\n    f=plt.figure(figsize=(w,h))\n    plt.bar(range(1,X.shape[1]+1),pca.explained_variance_ratio_,label='individual explained variance')\n    plt.step(range(1,X.shape[1]+1), np.cumsum(pca.explained_variance_ratio_),where='mid',label='cumulative explained variance')\n    plt.xticks(range(1,X.shape[1]+1))\n    plt.xlabel('# of principal components')\n    plt.ylabel('explained variance ratio',fontsize=13)\n    plt.yticks(fontsize=12)\n    plt.legend(fontsize=12,loc='best')\n    despine()\n    return Xpca","df929649":"Xpca=pd.DataFrame(reduct_pca(X,w=10)[:,:2])\nXpca.shape","c99597ad":"import statsmodels.api as sm\ndef select_by_pvalue(target,df,fence=.05,w=6,h=4.5,textw=.5,texth=.2,ysize=12):\n    mod=sm.OLS(df[target],df.drop(target,axis=1))\n    fii=mod.fit()\n    sub=fii.summary2().tables[1][\"P>|t|\"].sort_values()\n    sub.plot(kind='barh',figsize=(w,h))\n    pvalue_set=sub[sub<=fence].index.tolist()\n    plt.axvline(x=fence,color='black')\n    plt.figtext(textw, texth, 'dropped',fontsize=15, fontweight='bold')\n    plt.yticks(fontsize=ysize)\n    return pvalue_set","986d4ad9":"pvalset=select_by_pvalue('Loan_Status',train,h=7)\npvalset","66c357e3":"from statsmodels.tools.tools import add_constant\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\ndef vif(x):\n    X=add_constant(x)\n    return pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])],index=X.columns).sort_values(ascending=False)","58cedf9f":"vif(X[pvalset])","c4f42781":"vif(X[pvalset].drop(columns=[\"x2_2\",\"x5_Rural\",\"BalanceIncome\"]))","7461e12d":"pvalset=X[pvalset].drop(columns=[\"x2_2\",\"x5_Rural\",\"BalanceIncome\"]).columns.tolist()\npvalset","99cf0344":"orgset=[i for i in df.columns.tolist() if i not in [\"TotalIncome\",\"EMI\",\"BalanceIncome\",\"Loan_Status\"]]\norgset","ca3e4e9e":"pointall(df,[\"TotalIncome\",\"BalanceIncome\",\"EMI\"],'Loan_Status',h=3)","01eb4c86":"EDAset=[i for i in df.columns.tolist() if i not in [\"ApplicantIncome\",\"CoapplicantIncome\",\"LoanAmount\",\"Loan_Amount_Term\",\"Loan_Status\"]]\nvif(X[EDAset].drop(columns=[\"x2_1\",\"x5_Urban\"]))","abf24fad":"EDAset=X[EDAset].drop(columns=[\"x2_1\",\"x5_Urban\"]).columns.tolist()\nEDAset","1067978d":"from sklearn.model_selection import StratifiedKFold\ndef check(clf,X,y=y,n_splits=10):\n    kf=StratifiedKFold(n_splits=n_splits,random_state=0,shuffle=True)\n    k_tracc,k_teacc=[],[]\n    for (tr,te) in kf.split(X,y):\n        clf.fit(X.iloc[tr],y.iloc[tr])\n        k_tracc.append(clf.score(X.iloc[tr],y.iloc[tr]))\n        k_teacc.append(clf.score(X.iloc[te],y.iloc[te]))\n    print(f\"Train score: {np.mean(k_tracc)}\")\n    print(f\"Test score: {np.mean(k_teacc)}\")\n    return clf\n\n\ndef selectmoran(modellst,X=X,y=y,n_splits=10,random_state=0):\n    kf=StratifiedKFold(n_splits=n_splits,random_state=random_state,shuffle=True)\n    namelst,imp,tr_acc,te_acc=[],[],[],[]\n    for clf in modellst:\n        namelst.append(type(clf).__name__)\n        k_tracc, k_teacc, k_f1, k_imp=[],[],[],[]\n        for (tr,te) in kf.split(X,y): # train, test index\n            clf.fit(X.iloc[tr],y.iloc[tr])\n            k_tracc.append(clf.score(X.iloc[tr],y.iloc[tr]))\n            k_teacc.append(clf.score(X.iloc[te],y.iloc[te]))\n            if hasattr(clf,\"feature_importances_\"): k_imp.append(clf.feature_importances_)\n        tr_acc.append(np.mean(k_tracc))\n        te_acc.append(np.mean(k_teacc))\n        if len(k_imp)==0: imp.append(False)\n        else: imp.append(np.mean(k_imp,axis=0))\n    score=pd.DataFrame({'Model':namelst,'Train_accuracy':tr_acc,'Test_accuracy':te_acc}).sort_values('Test_accuracy',ascending=False)\n    return score,imp\n\ndef plotscoring(score,title,w=7,h=5,alpha=.97,axvline=.9,yticksize=12):\n    f,ax=plt.subplots(figsize=(w,h))\n    print(f\"Mean accuracy for all models: {np.mean(score.Test_accuracy)}\\n\")\n    print(score)\n    sns.barplot(x=score.Test_accuracy,y=score.Model,alpha=alpha,color='#bc7201')\n    sns.barplot(x=-score.Train_accuracy,y=score.Model,alpha=alpha,color='#6a2202')\n    ax.set_xlim(-1,1)\n    plt.axvline(x=0,color='black')\n    plt.xlabel('Train\/ Test accuracy')\n    plt.ylabel('')\n    plt.title(f\"Accuracy score for {title}\",fontweight='bold')\n    plt.axvline(x=axvline,ls=':')\n    plt.yticks(fontsize=yticksize)\n    despine()\n    \nfrom sklearn.model_selection import GridSearchCV\ndef grid(clf,params,X,y=y,cv=5):\n    grid=GridSearchCV(clf,params,cv=cv)\n    grid.fit(X,y)\n    print(f\"Best score: {grid.best_score_}\")\n    print(f\"Best params: {grid.best_params_}\")\n    return grid.best_estimator_\n\ndef plotting_importances(score,imp,X,w=8,h=3,rotation=90,xsize=10):\n    for (a,b) in zip(score.Model,imp):\n        if b is not False:\n            ind=np.argsort(b)[::-1]\n            cols=X.columns\n            plt.figure(figsize=(w,h))\n            plt.title(f\"Feature importances via {a}\",fontweight='bold',fontsize=13)\n            plt.bar(range(X.shape[1]),b[ind])\n            plt.xticks(range(X.shape[1]),cols[ind],rotation=rotation,fontsize=xsize)\n            plt.xlim([-1,X.shape[1]])\n            plt.tight_layout()","b7509b07":"# for our modeling stage\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier,GradientBoostingClassifier,AdaBoostClassifier,ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression,Perceptron,SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nmodellst=[RandomForestClassifier(),\n          BaggingClassifier(),\n          GradientBoostingClassifier(),\n          AdaBoostClassifier(n_estimators=100,random_state=0),\n          ExtraTreesClassifier(),\n          LogisticRegression(solver='liblinear'),\n          Perceptron(),\n          SGDClassifier(),         \n          LGBMClassifier(),\n          CatBoostClassifier(),\n          XGBClassifier(),\n          XGBRFClassifier(),\n          SVC(),\n          LinearSVC(),\n          LinearDiscriminantAnalysis()]","d7ed33b8":"score_org,imp_org=selectmoran(modellst,X[orgset])","e880232d":"plotscoring(score_org,'Initial features')","79598e28":"plotting_importances(score_org,imp_org,X[orgset],rotation=70,h=4)","c9f50817":"score_pval,imp_pval=selectmoran(modellst,X[pvalset])","4e22d6bb":"plotscoring(score_pval,'P-value subset')","bcfe6b8a":"plotting_importances(score_pval,imp_pval,X[pvalset],rotation=70,h=4)","a471b66c":"score_eda,imp_eda=selectmoran(modellst,X[EDAset])","0d92ccf3":"plotscoring(score_eda,'EDA subset')","055ab04d":"plotting_importances(score_eda,imp_eda,X[EDAset],rotation=70,h=4)","a4ff7f06":"score_pca,imp_pca=selectmoran(modellst,Xpca)","a01c8e05":"plotscoring(score_pca,'PCA subset')","f5693584":"def plot_meanscore(meanscore_set,title,axvline=.9):\n    print(meanscore_set)\n    f=plt.figure(figsize=(8,3))\n    meanscore_set.Test_accuracy[::-1].plot(kind='barh',color='#0f1a26',alpha=.95)\n    (-1*meanscore_set.Train_accuracy[::-1]).plot(kind='barh',alpha=.95)\n    plt.xlim([-1,1])\n    plt.axvline(x=0,c='black')\n    plt.axvline(x=axvline,ls=':')\n    despine()\n    plt.title(f\"{title} accuracy\",fontsize=17,fontweight='bold')","c2ecaede":"meanscore=pd.DataFrame({'Test_accuracy':[np.mean(i.Test_accuracy) for i in [score_eda,score_org,score_pca,score_pval]],\n             'Train_accuracy':[np.mean(i.Train_accuracy) for i in [score_eda,score_org,score_pca,score_pval]]},\n             index=['EDA subset','Initial features','PCA subset','P-value subset']).sort_values('Test_accuracy',ascending=False)\n\nmxscore=pd.DataFrame({'Test_accuracy':[np.max(i.Test_accuracy) for i in [score_eda,score_org,score_pca,score_pval]],\n             'Train_accuracy':[np.max(i.Train_accuracy) for i in [score_eda,score_org,score_pca,score_pval]]},\n             index=['EDA subset','Initial features','PCA subset','P-value subset']).sort_values('Test_accuracy',ascending=False)","7f03ba99":"plot_meanscore(meanscore,'Mean')","6afdb8d7":"plot_meanscore(mxscore,'Max')","f77ef95c":"plotscoring(score_pca,'PCA subset')","c979262d":"tuning_pca=[GradientBoostingClassifier(learning_rate=0.005, max_depth=1, subsample=0.8),\n           LinearSVC(C=2, loss='hinge'),\n           BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=50, n_jobs=4, oob_score=True),\n           LGBMClassifier(boosting_type='dart', learning_rate=0.01, max_depth=2, random_state=42),\n           CatBoostClassifier(**{'leaf_estimation_method': 'Gradient', 'learning_rate': 0.0003},random_state=1),\n           ExtraTreesClassifier(bootstrap=True, n_estimators=200, n_jobs=2, random_state=1),\n           RandomForestClassifier(n_estimators=80, n_jobs=4, random_state=1),\n           SGDClassifier(random_state=1)]\nscore_tupca,_=selectmoran(tuning_pca,Xpca)","677b82e4":"plotscoring(score_tupca,'Hypertuning - PCA subset')","eee422c8":"plotscoring(score_pval,'P-value subset')","805e534d":"tuning_pval=[GradientBoostingClassifier(n_estimators=10,learning_rate=.04,warm_start=True,criterion='mae',min_samples_split=3,random_state=0),\n            BaggingClassifier(base_estimator=LinearDiscriminantAnalysis(), n_estimators=20, random_state=1),\n            LGBMClassifier(boosting_type='dart',learning_rate=.01,subsample=.9,n_jobs=4,random_state=0),\n            RandomForestClassifier(max_depth=3, n_estimators=80, random_state=0),\n            AdaBoostClassifier(learning_rate=0.01, n_estimators=10),\n            XGBClassifier(max_depth=2,n_estimators=60,learning_rate=.07,random_state=0,booster='dart',n_jobs=4),\n            XGBRFClassifier(max_depth=2,random_state=42,learning_rate=.03,booster='dart',n_jobs=4,n_estimators=10),\n            CatBoostClassifier(depth=4,n_estimators=70,leaf_estimation_method='Newton',random_state=1)]\nscore_tupv,_=selectmoran(tuning_pval,X[pvalset])","3492de9a":"plotscoring(score_tupv,'Hypertuning P-value subset')","66b5d734":"plotscoring(score_eda,'EDA subset')","aea74bc9":"tuning_eda=[LogisticRegression(C=1, penalty='l1', solver='liblinear', warm_start=True, n_jobs=4, random_state=1),\n           LinearSVC(C=1, loss='hinge'),\n           BaggingClassifier(base_estimator=LinearDiscriminantAnalysis(), n_estimators=40, random_state=0),\n           LGBMClassifier(boosting_type='dart', learning_rate=0.01, n_jobs=4, random_state=0, subsample=0.9),\n           XGBClassifier(max_depth=3,n_estimators=130,learning_rate=.01,booster='dart',n_jobs=4),\n           GradientBoostingClassifier(criterion='mae', max_depth=4, n_estimators=70, random_state=1, warm_start=True),\n           SGDClassifier(loss='huber', n_jobs=4, penalty='elasticnet', random_state=42, warm_start=True),\n           XGBRFClassifier(max_depth=2,random_state=0,learning_rate=1,booster='dart',n_jobs=4,n_estimators=20),\n           CatBoostClassifier(depth=2,n_estimators=10,leaf_estimation_method='Gradient',random_state=42),\n           RandomForestClassifier(criterion='entropy', max_depth=2, max_features='sqrt', n_estimators=70, random_state=42)]\nscore_tueda,_=selectmoran(tuning_eda,X[EDAset])","65551f7d":"plotscoring(score_tueda,'Hypertuning EDA subset')","1133efa9":"plotscoring(score_org,'Initial features')","5e445e12":"tuning_org=[GradientBoostingClassifier(criterion='mse', learning_rate=0.3, loss='exponential', max_depth=2, n_estimators=10, random_state=1, subsample=0.9),\n           LGBMClassifier(boosting_type='dart', learning_rate=0.02, n_estimators=30, n_jobs=4, random_state=0, subsample=0.8),\n           RandomForestClassifier(criterion='entropy', max_depth=4, max_features='sqrt', n_estimators=70, random_state=0),\n           BaggingClassifier(base_estimator=LinearDiscriminantAnalysis(), n_estimators=40, random_state=0),\n           XGBClassifier(max_depth=3,n_estimators=90,learning_rate=.03,booster='dart',n_jobs=4),\n           XGBRFClassifier(max_depth=3,random_state=1,learning_rate=.07,booster='dart',n_jobs=4,n_estimators=20),\n           CatBoostClassifier(depth=3,n_estimators=20,random_state=0,learning_rate=.2)]\nscore_tuorg,_=selectmoran(tuning_org,X[orgset])","eb323722":"plotscoring(score_tuorg,'Hypertuning - Initial features')","85f24d2d":"meanscoretu=pd.DataFrame({'Test_accuracy':[np.mean(i.Test_accuracy) for i in [score_tueda,score_tuorg,score_tupca,score_tupv]],\n             'Train_accuracy':[np.mean(i.Train_accuracy) for i in [score_tueda,score_tuorg,score_tupca,score_tupv]]},\n             index=['EDA subset','Initial features','PCA subset','P-value subset']).sort_values('Test_accuracy',ascending=False)\n\nmxscoretu=pd.DataFrame({'Test_accuracy':[np.max(i.Test_accuracy) for i in [score_tueda,score_tuorg,score_tupca,score_tupv]],\n             'Train_accuracy':[np.max(i.Train_accuracy) for i in [score_tueda,score_tuorg,score_tupca,score_tupv]]},\n             index=['EDA subset','Initial features','PCA subset','P-value subset']).sort_values('Test_accuracy',ascending=False)","188cf480":"plot_meanscore(mxscore,'Max')","877fc33e":"clf=check(GradientBoostingClassifier(n_estimators=10,learning_rate=.04,warm_start=True,criterion='mae',min_samples_split=3,random_state=0),X[pvalset])\nclf","a7d0a364":"# 2. Exploratory data analysis","883416c3":"## 3.3 Outliers treatment","647f3f7a":"## 5.4 PCA susbet","b02a214e":"- The skewnesses of our features are smaller after using log function","878248d7":"- Our train and test set contain 614 and 367 observations, respectively\n- We have similar features for both files, except the Loan_Status, which is our target variable","1b8090e7":"- In general, applicants with more dependents tend to have higher incomes (this can be shown in the weak correlation between applicant income and dependents, the correlation coefficient is 0.07)\n- We do not see such trend for Coapplicant income","8f7b1bf0":"- Extremely large outliers appear in Applicant income and coapplicant income features","0d7036c9":"We will create new features:\n- Total income: combine Applicant income and Coapplicant income. Here we form a hypothesis, applicants with higher total might get higher chance of loan approval\n- EMI: the monthly amount to be paid by the applicants to repay the loan\n- Balance income: the income left after the EMI has been paid","3f459b54":"- We will not consider the distribution of Loan_ID, because it works as an index","b3cd50a9":"### Coapplicant income and married","e7c3c7e7":"## 6.2 P-value subset","83defdf5":"- We are going to use log function with Applicant income, Coapplicant income, and Loan amount","4d6eea44":"### 2.1.1 Our target variable: Loan status","386c1b61":"### 2.1.2 Distribution of our independent features","b15ca1de":"- From the pointplot, we can see that married applicants tend to have higher coapplicant income and also have a higher chance of being approved for a loan\n- Extremely large outliers appear in the group of unmarried applicants","e2d33d54":"## 5.2 P-value subset","1eaadd6c":"## 3.1 Dealing with missing values","d3e69d80":"- Seems to have a roughly linear trend between dependents and loan status, regardless of married status","b54f86af":"## 5.5 Conclusion","d24d551d":"- Some features show significant correlation, such as married and gender, married and dependents, married and coapplicant income, or coapplicant income and applicant income\n- Other features show weak correlation together\n- Credit history seems to be a good predictor for our target variable, loan status","aef1290b":"- Our features sorted by importances are probably: credit history, property area, education, coapplicant income, married, loan amount intern\n- Surprisingly, applicant income seems not to be a good predictor","ab868667":"## 6.3 EDA subset","4324ed84":"Applicants would have more chances of loan approval:\n- Married (approximately 72%)\n- Graduated (approximately 72%)\n- Coming from semi-urban\n- Male (slightly higher chance compared to female)\n\nSelf-employed doesn't seem like a good predictor","d5a61162":"### Numerical features","7379a3d5":"First, we should convert our target variable into integer","dc1196ce":"- Our target variable's distribution is not really well-balanced","41e1bba0":"It will be more convenient to combine our train and test files (we also drop Loan_ID, which works as an index)","60cbef05":"Features containing missing values: credit history, selft-employed, loan amount, dependents, loan amount term, gender, and married (Loan status contains missing data, but these data belongs to test set, so no need processing)\n\nWe will consider the following methods to fill the missing data:\n- For categorical features: imputation using mode\n- For numerical features: imputation using median or mean (based on the correlation between the feature and other features)","c30bc674":"## 5.1 Initial features","8566647a":"## 5.3 EDA subset","4b566fa6":"- Gender: Most applicants in our dataset are male (approximately 80%)\n- Married: Around 65% of the applicants in the dataset are married\n- Dependents: Applicants are more likely to have no dependents\n- Education: Most applicants are graduated (about 78%)\n- Self-employed: Only 13% applicants are self-employed\n- Property area: Most applicants are from semi-urban","80506f4d":"### Dependents and Married","ce75f87e":"## 1.4 Data completeness","fe4cf494":"## 2.1 Univariate analysis","c74399b7":"- Some features have a quite large skewness, such as Applicant income and Coapplicant income\n- All of our continuous features are skewed","3c1bdd3a":"## 6.4 Initial features","4190f73e":"- We also save our initial features","654402af":"- Missing data appears in several features, such as Credit history, self-employed, dependents, loan amount, etc","dd3973cf":"## 4.1 Dimensionality reduction using PCA","2eb12757":"# 1 Overview","9a6018ed":"## 1.2 Defining functions","ab22439a":"## 3.4 Features encoding","ddfbae1f":"- And our EDA subset","7dc96c87":"# 3. Data preprocessing","a5791d2c":"The applicant will have a better chance of getting a loan approved::\n- Having coapplicant with lower income (or this could be related to the fact that applicants without dependents having the coapplicant income is equal to 0)\n- Lower loan amount\n- Higher credit history (we also notice that the error bars for credit history are shorter than for other independent features)\n- Lower loan amount term\n\n**Surprisingly, the applicant's income does not seem to be a good predictor, higher income does not mean a higher chance of loan approval**","1de7024b":"### 2.1.3 Our target variable and other independent features","dc4f8d00":"- Outliers tend to appear in several features, such as loan amount, applicant income, and coapplicant income due to the large difference between values in these features","f16bceaf":"## 6.1 PCA subset","11bb29ad":"    Loan_ID: Unique Loan ID\n    Gender: Male\/ Female\n    Married: Applicant married (Y\/N)\n    Dependents: Number of dependents\n    Education: Applicant Education (Graduate\/ Under Graduate)\n    Self_Employed: Self employed (Y\/N)\n    ApplicantIncome: Applicant income\n    CoapplicantIncome: Coapplicant income\n    LoanAmount: Loan amount in thousands\n    Loan_Amount_Term: Term of loan in months\n    Credit_History: credit history meets guidelines\n    Property_Area: Urban\/ Semi Urban\/ Rural\n    Loan_Status: (Target) Loan approved (Y\/N)","a29ca7cd":"- Some features contain extremely large values to the right, such as Applicant income and Coapplicant income, we should find out the reason for this (did it come from the qualifications different education?)\n- The distribution of Loan amount looks pretty bell-shaped, except for some of the higher values on the right\n- Most applicant and coapplicant incomes range from 0 to 15000 and from 0 to 5000, respectively","93b74d98":"## 2.2 Multivariate analysis","3d5e160a":"#### Distribution of our categorical features","411f4df8":"### Coapplicant income, Applicant income and Dependents","99f0df11":"## 4.2 P-value subset","72ede532":"# Content\n1. Overview\n    - 1.1 Importing python libraries\n    - 1.2 Defining functions\n    - 1.3 Importing our dataset\n    - 1.4 Data completeness\n2. Exploratory data analysis\n    - 2.1 Univariate analysis\n    - 2.2 Multivariate analysis\n    - 2.3 Summary\n3. Data preprocessing\n    - 3.1 Dealing with missing values\n    - 3.2 Feature engineering\n    - 3.3 Outliers treatment\n    - 3.4 Features scaling\n    - 3.5 Categorical features ecoding\n4. Feature selection\n    - 4.1 Dimensionality reduction using PCA\n    - 4.2 P-value subset\n5. Modeling\n    - 5.1 Initial features\n    - 5.2 P-value subset\n    - 5.3 EDA subset\n    - 5.4 PCA subset\n    - 5.5 Conclusion\n6. Hypertuning parameters\n    - 6.1 PCA subset\n    - 6.2 P-value subset\n    - 6.3 EDA subset\n    - 6.4 Initial features\n    - 6.5 Conclusion","8801d0bc":"- There is no clear linear trend, but among our applicants, males are more likely to get married than females","461e6f4c":"- We will standardize Applicant income and Loan amount because their distribution is quite bell-shaped\n- We will also normalize Coapplicant income","d990bd10":"# 6. Hypertuning parameters","e52b723f":"## 1.3 Importing our dataset","98d28e9a":"## 2.3 Summary","c085fff1":"- Binary categorical features: gender, married, education, self-employed, and loan status\n- Categorical features: dependents, property area, and loan_id (it works as an index)","2f5ba484":"## 1.1 Importing python libraries","8e65902a":"### Categorical features","95d33820":"- The most important features are probably: credit history, property area, eduacation, and coapplicant income","dc2c44d0":"# 5. Modeling","1fbba4be":"### Credit history, Applicant income and Coapplicant income","4aba3937":"- No clear linear trend","a080d6e2":"### Married and Gender","c742144d":"## 3.4 Features scaling","7f0c53dc":"## 3.2 Features engineering","4c25ba4f":"- The pointplots show that graduated applicants are more likely to have higher income and their coapplicants also tend to have higher income","95d01ccc":"#### Distribution of our numerical features","feff185a":"# 4. Feature selection","ff61e615":"### Coapplicant income, Applicant income and Education"}}