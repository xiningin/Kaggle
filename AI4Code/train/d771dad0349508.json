{"cell_type":{"a37b6389":"code","5032198d":"code","d7421934":"code","a07b98e9":"code","1027ab93":"code","4f0e0aae":"code","c62ae7ef":"code","2205cc90":"code","2f1054a9":"code","746f0b1a":"code","ec20ed6b":"code","283ffafd":"code","15312da4":"code","d3d43b5f":"markdown","3b7b539f":"markdown","88946348":"markdown","610de4f7":"markdown","63ae3a12":"markdown","cb2cf11c":"markdown","c96334aa":"markdown","a6031802":"markdown","f96216a7":"markdown","65e42844":"markdown","5d625b39":"markdown","8b19f564":"markdown","d23f5edc":"markdown","d20cffb7":"markdown"},"source":{"a37b6389":"# Input data files are available in the \"..\/input\/\" directory.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5032198d":"# Pandas for data reading and writing\nimport pandas as pd\n# Numpy for Numerical operations\nimport numpy as np\n# Import ColumnTransformer\nfrom sklearn.compose import ColumnTransformer\n# Import Pipeline\nfrom sklearn.pipeline import Pipeline\n# Import SimpleImputer\nfrom sklearn.impute import SimpleImputer\n# Import StandardScaler, OneHotEncodr and OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n# Import Random Forest for Classification\nfrom sklearn.ensemble import RandomForestClassifier\n# Import GridSearch\nfrom sklearn.model_selection import GridSearchCV","d7421934":"# Read the train data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n# See some info\ntrain_data.info()","a07b98e9":"# Load test data\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.info()","1027ab93":"# Split the data into predictors and target\nX_train = train_data.drop(['Survived', 'Name'], axis = 1)\nX_test = test_data.drop(['Name'], axis = 1)\ny_train = train_data['Survived']","4f0e0aae":"# Now, we will create a pipline for the numeric features\n# Difine a list with the numeric features\nnumeric_features = ['Age', 'Fare']\n# Define a pipeline for numer\"ic features\nnumeric_features_pipeline = Pipeline(steps= [\n    ('imputer', SimpleImputer(strategy = 'median')), # Impute with median value for missing\n    ('scaler', StandardScaler())                     # Conduct a scaling step\n])","c62ae7ef":"# Now, we will create a pipline for the categorical features\n# Difine a list with the categorical features\ncategorical_features = ['Embarked', 'Sex']\n# Define a pipeline for categorical features\ncategorical_features_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value = 'missing')), # Impute with the word 'missing' for missing values\n    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))     # Convert all categorical variables to one hot encoding\n])","2205cc90":"# Now, we will create a pipline for the ordinal features\n# Define a list with the ordinal features\nordinal_features = ['Pclass']\n# Define a pipline for ordinal features \nordinal_features_pipeline = Pipeline(steps=[\n    ('ordinal', OrdinalEncoder(categories= [[1, 2, 3]]))\n])","2f1054a9":"# Now, we will create a transformer to handle all columns\npreprocessor = ColumnTransformer(transformers= [\n    # transformer with name 'num' that will apply\n    # 'numeric_features_pipeline' to numeric_features\n    ('num', numeric_features_pipeline, numeric_features),\n    # transformer with name 'cat' that will apply \n    # 'categorical_features_pipeline' to categorical_features\n    ('cat', categorical_features_pipeline, categorical_features),\n    # transformer with name 'ord' that will apply \n    # 'ordinal_features_pipeline' to ordinal_features\n    ('ord', ordinal_features_pipeline, ordinal_features) \n    ])","746f0b1a":"# Now, we will create a full prediction pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                     ('classifier', RandomForestClassifier(n_estimators = 120, max_leaf_nodes = 100))])","ec20ed6b":"# Let's fit our classifier\nclf.fit(X_train, y_train)","283ffafd":"param_grid = {\n    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n    'classifier__n_estimators': [100, 120, 150, 170, 200],\n    'classifier__max_leaf_nodes' : [100, 120, 150, 170, 200]\n}\n\ngrid_search = GridSearchCV(clf, param_grid, cv=10)\ngrid_search.fit(X_train, y_train)\nprint((\"best random forest from grid search: %.3f\"\n       % grid_search.score(X_train, y_train)))\nprint('The best parameters of Simple Imputer and C are:')\nprint(grid_search.best_params_)","15312da4":"# Generate predictions\npredictions = grid_search.predict(X_test)\n# Generate results dataframe\nresults_df = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# Save to csv file\nresults_df.to_csv('submission.csv', index = False)\nprint('Submission CSV has been saved!')","d3d43b5f":"# **Prediction Pipeline**\n*Now, we will create a full prediction pipeline that uses our preprocessor and then transfer it to our classifier of choice 'Random Forest'*.","3b7b539f":"# **Reading Data**\nIn the following cells, we will read the train and test data and check for NaNs.","88946348":"It's obvious that we had to deal with NaNs","610de4f7":"# **Categorical features handling**\n*It's clear that we have some categorical features that have some missing values to be imputed and they have to be encoded using one hot encoding.*\n\n*In the following cell, we will handle the categorical features separtely i.e \"Embarked\" and \"Sex\"*\n\n*Note: I choose simple imputer for the missing cells to impute with 'missing' word. My aim was to gather all missing cells in one category for further encoding.*","63ae3a12":"# **Construct a comprehended preprocessor**\n*Now, we will create a preprocessor that can handle all columns in our dataset using ColumnTransformer*","cb2cf11c":"# **Splitting Data**\n","c96334aa":"# **Generate Predictions**\n*Let's generate predictions now using our grid search model and submit the results*","a6031802":"# **Pipeline Training**\n*Let's train our pipeline now*\n","f96216a7":"# **Continuous and Numerical features handling**\n*It's clear that we have some numerical features that have some missing values to be imputed and they have to be of the same scale also.*\n\n*In the following cell, we will handle the numerical features separtely i.e \"Age\" and \"Fare\"*\n","65e42844":"# **Pipeline Tuning**\n*The question now, can we push it a little bit further? i.e. can we tune every single part or our Pipeline?*\n\n*Here, I will use GridSearch to decide three things:*\n>- Simple Imputer strategy : mean or median\n>- n_estimators of Random Forest\n>- max leaf nodes of Random Forest\n\n*Note, you can access any parameter from the outer level to the next adjacent inner one*\n\n*For Example: to access the strategy of the Simple Imputer you can do the following*\npreprocessor__num__imputer__strategy\n\n*Let's see this into action*\n","5d625b39":"# **Mounting Filesystem**\n","8b19f564":"# **Ordinal features handling**\n*Passenger class or 'Pclass' for short is an ordinal feature that must be handled keeping in mind that class 3 is much higher than 2 and so on.*","d23f5edc":"# **Introduction**\n*Neither Titanic dataset nor sklearn a new thing for any data scientist but there are some important features in scikit-learn that will make any model preprocessing and tuning easier, to be specific this notebook will cover the following concepts:*\n\n>- ColumnTransformer\n>- Pipeline\n>- SimpleImputer\n>- StandardScalar\n>- OneHotEncoder\n>- OrdinalEncoder\n>- GridSearch","d20cffb7":"# **Import Packages**\n"}}