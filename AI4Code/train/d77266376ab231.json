{"cell_type":{"c4fcae92":"code","50ec0efe":"code","2e72ff17":"code","99970307":"code","1976418c":"code","1d29c527":"code","e9e8e5c9":"code","8cdce37f":"code","deefeb10":"code","eefdd2d4":"code","d4a66ccb":"code","37bcc47a":"code","75e6ef18":"code","0e5547e6":"code","4e5ba4b4":"code","5e532d33":"code","71a00829":"code","d811389c":"code","6005dea2":"code","da0073c3":"code","e1edd4eb":"code","fe99fb93":"code","1538dc7b":"code","33c5a9c8":"markdown","3dabf4f8":"markdown","f165e519":"markdown","de63c726":"markdown","23caa866":"markdown","2e7849f1":"markdown","e3fa63c7":"markdown","0d3f0c51":"markdown","4be82f2a":"markdown","79ecf265":"markdown","f5e6b2ea":"markdown","19b0faed":"markdown","4d9cdbd0":"markdown","9264825e":"markdown","53ed8ea1":"markdown","5c7766eb":"markdown","6b835257":"markdown","a6b97004":"markdown","2d8a6677":"markdown","4eb5361f":"markdown","6ba14310":"markdown","32691c3b":"markdown","4b2ea53e":"markdown","8371d97b":"markdown","2eba6425":"markdown","e6dd5bac":"markdown"},"source":{"c4fcae92":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n%matplotlib inline ","50ec0efe":"df = pd.read_csv('..\/input\/CompleteDataset.csv')\ndf.head()","2e72ff17":"df.columns","99970307":"# GK attributes are not our interest\ncolumns_needed = ['Acceleration', 'Aggression', 'Agility', 'Balance', 'Ball control',\n       'Composure', 'Crossing', 'Curve', 'Dribbling', 'Finishing',\n       'Free kick accuracy', 'Heading accuracy', 'Interceptions',\n       'Jumping', 'Long passing', 'Long shots', 'Marking', 'Penalties',\n       'Positioning', 'Reactions', 'Short passing', 'Shot power',\n       'Sliding tackle', 'Sprint speed', 'Stamina', 'Standing tackle',\n       'Strength', 'Vision', 'Volleys', 'Preferred Positions']\n\n# attack attribute first, then defence, then mixed\ncolumns_needed_rearranged = ['Aggression','Crossing', 'Curve', 'Dribbling', 'Finishing',\n       'Free kick accuracy', 'Heading accuracy', 'Long shots','Penalties', 'Shot power', 'Volleys', \n       'Short passing', 'Long passing',\n       'Interceptions', 'Marking', 'Sliding tackle', 'Standing tackle',\n       'Strength', 'Vision', 'Acceleration', 'Agility', \n       'Reactions', 'Stamina', 'Balance', 'Ball control','Composure','Jumping', \n       'Sprint speed', 'Positioning','Preferred Positions']\n\ndf = df[columns_needed_rearranged]\ndf.head()","1976418c":"df['Preferred Positions'] = df['Preferred Positions'].str.strip()\ndf = df[df['Preferred Positions'] != 'GK']\ndf.head()\n\n","1d29c527":"df.isnull().values.any()","e9e8e5c9":"p = df['Preferred Positions'].str.split().apply(lambda x: x[0]).unique()\np","8cdce37f":"# copy a structure\ndf_new = df.copy()\ndf_new.drop(df_new.index, inplace=True)\n\nfor i in p:\n    df_temp = df[df['Preferred Positions'].str.contains(i)]\n    df_temp['Preferred Positions'] = i\n    df_new = df_new.append(df_temp, ignore_index=True)\n    \ndf_new.iloc[::2000, :]\n            \n\n","deefeb10":"cols = [col for col in df_new.columns if col not in ['Preferred Positions']]\n\nfor i in cols:\n    df_new[i] = df_new[i].apply(lambda x: eval(x) if isinstance(x,str) else x)\n\ndf_new.iloc[::1000, :]","eefdd2d4":"df_new_normalized = df_new.iloc[:,:-1].div(df_new.iloc[:,:-1].sum(axis=1), axis=0)\nmapping = {'ST': 1, 'RW': 1, 'LW': 1, 'RM': 1, 'CM': 1, 'LM': 1, 'CAM': 1, 'CF': 1, 'CDM': 0, 'CB': 0, 'LB': 0, 'RB': 0, 'RWB': 0, 'LWB': 0}\ndf_new_normalized['Preferred Positions'] = df_new['Preferred Positions']\ndf_new_normalized = df_new_normalized.replace({'Preferred Positions': mapping})\n\ndf_new_normalized.iloc[::2000,]\n","d4a66ccb":"X_train, X_test, y_train, y_test = train_test_split(df_new_normalized.iloc[:,:-1], df_new_normalized.iloc[:,-1], random_state=0)\n\nprint('X train shape: {}'.format(X_train.shape))\nprint('X test shape: {}'.format(X_test.shape))\nprint('y train shape: {}'.format(y_train.shape))\nprint('y test shape: {}'.format(y_test.shape))\n","37bcc47a":"clf_d = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\nacc_d = clf_d.score(X_test, y_test)\nprint ('Dummy Classifier (most frequent class): {}'.format(acc_d))\n\nclf = LogisticRegression().fit(X_train, y_train)\nacc = clf.score(X_test, y_test)\nprint ('Logistic Regression Accuracy: {}'.format(acc))\n","75e6ef18":"Coef_list = list(sorted(zip(X_train.columns, abs(clf.coef_[0])),key=lambda x: -x[1]))\nCoef_table = pd.DataFrame(np.array(Coef_list).reshape(-1,2), columns = ['Attributes', 'Coef'])\n\nprint (Coef_table)","0e5547e6":"target_cols = Coef_table[:10]['Attributes'].tolist()\n\nclf_2 = LogisticRegression().fit(X_train[target_cols], y_train)\nacc_2 = clf_2.score(X_test[target_cols], y_test)\nprint ('Logistic Regression Accuracy (10 features): {}'.format(acc_2))","4e5ba4b4":"cov_mat = np.cov(df_new.iloc[:,:-1].T)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\n# Calculation of Explained Variance from the eigenvalues\ntot = sum(eig_vals)\nvar_exp = [(i\/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\ncum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n\nprint(list(zip(range(29),cum_var_exp)))\n\n# PLOT OUT THE EXPLAINED VARIANCES SUPERIMPOSED \nplt.figure(figsize=(10, 10))\nplt.bar(range(len(var_exp)), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\nplt.step(range(len(var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.show()\n\n","5e532d33":"pca = PCA(n_components=17)\n\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(df_new.iloc[:,:-1], df_new.iloc[:,-1], random_state=0)\n\nX_train_2_pca = pca.fit_transform(X_train_2)\nX_train_2_pca = pd.DataFrame(X_train_2_pca)\n\nx_test_2_pca = pca.transform(X_test_2)\n\nclf_17d = LogisticRegression().fit(X_train_2_pca, y_train_2)\nacc_17d = clf_17d.score(x_test_2_pca, y_test_2)\nprint ('Logistic Regression Accuracy with PCA (17 components): {}'.format(acc_17d))\n","71a00829":"lda = LDA(n_components=None)\n\nX_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(df_new.iloc[:,:-1], df_new.iloc[:,-1], random_state=0)\n\nX_lda = lda.fit(X_train_3, y_train_3)\n\nlda_var_ratios = lda.explained_variance_ratio_\n\n# get number of components needed to explain 95% variance\ndef select_n_components(var_ratio, goal_var: float) -> int:\n    \n    total_variance = 0.0\n    n_components = 0\n    \n    for explained_variance in var_ratio:\n        total_variance += explained_variance\n        n_components += 1\n        if total_variance >= goal_var:\n            break\n\n    return n_components\n\nprint('Number of components needed to explain 95% variance: {}'.format(select_n_components(lda_var_ratios, 0.95)))","d811389c":"lda_n = LDA(n_components=3)\nX_train_3_lda = lda_n.fit_transform(X_train_3, y_train_3)\nX_train_3_lda = pd.DataFrame(X_train_3_lda)\n\nX_test_3_lda = lda_n.transform(X_test_3)\n\nclf_3d = LogisticRegression().fit(X_train_3_lda, y_train_3)\nacc_3d = clf_3d.score(X_test_3_lda, y_test_3)\nprint ('Logistic Regression Accuracy with LDA (3 components): {}'.format(acc_3d))","6005dea2":"df_new_normalized_all = df_new.copy()\nmapping_all = {'ST': 0, 'RW': 1, 'LW': 2, 'RM': 3, 'CM': 4, 'LM': 5, 'CAM': 6, 'CF': 7, 'CDM': 8, 'CB': 9, 'LB': 10, 'RB': 11, 'RWB': 12, 'LWB': 13}\n\ndf_new_normalized_all = df_new_normalized_all.replace({'Preferred Positions': mapping_all})\ndf_new_normalized_all.iloc[::1000,]\n","da0073c3":"X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(df_new_normalized_all.iloc[:,:-1], df_new_normalized_all.iloc[:,-1], random_state=0)\n\nprint('X train shape: {}'.format(X_train_all.shape))\nprint('X test shape: {}'.format(X_test_all.shape))\nprint('y train shape: {}'.format(y_train_all.shape))\nprint('y test shape: {}'.format(y_test_all.shape))","e1edd4eb":"clf_d_all = DummyClassifier(strategy = 'most_frequent').fit(X_train_all, y_train_all)\nacc_d_all = clf_d_all.score(X_test_all, y_test_all)\nprint ('Dummy Classifier (most frequent class): {}'.format(acc_d_all))\n\nclf_all = LogisticRegression().fit(X_train_all, y_train_all)\nacc_all = clf_all.score(X_test_all, y_test_all)\nprint ('Logistic Regression Accuracy: {}'.format(acc_all))","fe99fb93":"clf_all_for = RandomForestClassifier(random_state=0).fit(X_train_all, y_train_all)\nacc_all_for = clf_all_for.score(X_test_all, y_test_all)\nprint ('Random Forest Accuracy (Default parameters): {}'.format(acc_all_for))\n","1538dc7b":"parameters_f = [{'max_depth': range(2,10), 'n_estimators': range(2,8,2), 'max_features': range(10,20)}]\nclf_all_for_g = GridSearchCV(RandomForestClassifier(random_state=0), parameters_f)\nclf_all_for_g.fit(X_train_all, y_train_all)\n\nprint('Best score for train data:', clf_all_for_g.best_score_)\nprint('Best depth:',clf_all_for_g.best_estimator_.max_depth)\nprint('Best n trees:',clf_all_for_g.best_estimator_.n_estimators)\nprint('Best n features:',clf_all_for_g.best_estimator_.max_features)\nprint('Score for test data:',clf_all_for_g.score(X_test_all, y_test_all))\n\n","33c5a9c8":"## Feature selections","3dabf4f8":"Check any missing data:","f165e519":"Split train test dataset:","de63c726":"Handle players with multiple preferred positions: duplicate a set of data for each","23caa866":"## Study LDA impact on model accuracy","2e7849f1":"Gather only columns that we need for this analysis purpose:","e3fa63c7":"First let's import some packages:","0d3f0c51":"Accuracy isn't satisfying, let's try use gridsearchcv to find better parameters to the model:","4be82f2a":"Let's try limit the features to top 10 only:","79ecf265":"\n\n# FIFA - Predict Player Position \n\n\n\n","f5e6b2ea":"The pattern after normalization looks much more obvious. Lets do below:\n\n* Normalize the whole dataset\n\n\n* Reclassify the target value (preferred positions) to binary groups as below:\n- 1 =  attack positions = ST, RW, LW, RM, CM, LM, CAM, CF \n- 0 = defened positions = CDM, CB, LB, RB, RWB, LWB\n\n","19b0faed":"Let's find the reasonable number of component of PCA *(I re-used most codes from above article and only modify to suit my needs)*:","4d9cdbd0":"17 components are needed for 95%+ of variance level:","9264825e":"Some of the attributes have '+\/-' sign, let's perform the calculation rather than keeping them as string:","53ed8ea1":"Can we further improve the model accuracy by looking at feature importance?","5c7766eb":"## Predict binary targets (attack vs defend positions) with logistic regression","6b835257":"Apply logistic regression to training set:","a6b97004":"All possible outcome for preferred position:","2d8a6677":"Load the data:","4eb5361f":"## Predict all positions (14 targets) with logistic regression","6ba14310":"Check out columns:","32691c3b":"Fit the model:","4b2ea53e":"## Predict all positions (14 targets) with random forest","8371d97b":"We don't want to classify GK because it will be too obvious:","2eba6425":"## Study PCA impact on model accuracy","e6dd5bac":"## Data manipulation"}}