{"cell_type":{"c1858ed0":"code","b2c2f7c3":"code","8915fd67":"code","1bba91c5":"markdown","d90212d3":"markdown","6d46760f":"markdown"},"source":{"c1858ed0":"import pandas  as pd\n\n#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\n\n#===========================================================================\n# features to rank\n#===========================================================================\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\", \"Fare\"]\n\n#===========================================================================\n# for the features that are categorical we use pd.get_dummies:\n# \"Convert categorical variable into dummy\/indicator variables.\"\n#===========================================================================\nX_train       = pd.get_dummies(train_data[features])\ny_train       = train_data[\"Survived\"]\n\n#===========================================================================\n# perform the classification\n#===========================================================================\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(max_features='auto', \n                                    min_samples_leaf=10, \n                                    random_state=42)\n\nclassifier.fit(X_train, y_train)","b2c2f7c3":"import warnings\nwarnings.filterwarnings(\"ignore\")","8915fd67":"#===========================================================================\n# perform the PermutationImportance\n#===========================================================================\nimport eli5\nfrom   eli5.sklearn import PermutationImportance\n\nperm_import = PermutationImportance(classifier, random_state=1).fit(X_train, y_train)\n\n# now visualize the results\neli5.show_weights(perm_import, top=None, feature_names = X_train.columns.tolist())","1bba91c5":"We see that the most important fearues are `Sex`, and the passenger class (`Pclass`).\n# Related reading\n* [Permutation feature importance](https:\/\/scikit-learn.org\/stable\/modules\/permutation_importance.html) on Scikit-kearn\n* [Feature selection using BorutaShap](https:\/\/www.kaggle.com\/carlmcbrideellis\/feature-selection-using-borutashap)","d90212d3":"# Titanic: Feature selection using permutation importance\nIn this short notebook we shall look at ranking a selection of the Titanic features using a technique known as **permutation importance**. For more background on permutation importance may I highly recommend reading the [excellent introductory notebook](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance) on the subject written by [Dan Becker](https:\/\/www.kaggle.com\/dansbecker).\n\nFor today we shall not use the passenger names (although some ingenious people have used titles, such as *Master*, to infer ages very effectively in other notebooks), tickets, cabin (way too much missing data) nor ages, although this can be a useful feature if it is treated correctly.\n\nNote that:\n> Features that are deemed of low importance for a bad model (low cross-validation score) could be very important for a good model. Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is for a particular model.\n\nHere we shall use the [random forest](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html) as our model:","6d46760f":"Now make use of the [PermutationImportance](https:\/\/eli5.readthedocs.io\/en\/latest\/blackbox\/permutation_importance.html) wrapper from the [ELI5 python library](https:\/\/eli5.readthedocs.io\/en\/latest\/):"}}