{"cell_type":{"2f7efc75":"code","9dbeeda8":"code","5fbe771b":"code","f6d4c9dd":"code","fce300fb":"code","7e11a501":"code","161ec34e":"code","bca826a7":"code","40a2804c":"code","29a7429d":"code","bc4ada46":"code","ceb62ae9":"code","d3e75e6b":"code","94f343b5":"code","280eec43":"code","e842e692":"code","c8035ca7":"code","61d2efdd":"code","4d28c593":"code","354aa89c":"code","494adbd7":"code","4e5b0939":"code","b08b9531":"code","7a9d61a4":"code","d1a408b5":"code","47fd910d":"code","813d7299":"code","15c851cc":"code","c98ef0ea":"code","e620ae7a":"code","3df94980":"code","6eb0e91b":"code","bcc00a97":"code","65308aa1":"code","3ad6f761":"code","09224fb7":"code","9c865d76":"code","5b450a4f":"markdown","948f171e":"markdown","2775c9ff":"markdown","e487245a":"markdown","9f0523fa":"markdown","bc0ba4d6":"markdown","ece88fef":"markdown","74b196c8":"markdown","d1c3a64c":"markdown","c77e5237":"markdown","262af303":"markdown","3a693113":"markdown","d1552bac":"markdown","7a7c563f":"markdown","a825ab09":"markdown","238d2777":"markdown","e83bfb64":"markdown","2d48bed9":"markdown","ccb5a0cd":"markdown","89476b27":"markdown","13166a97":"markdown","c25c8978":"markdown","ada8605d":"markdown","d6ed08e6":"markdown","1997a95e":"markdown","b390c85b":"markdown","82aee8ab":"markdown","43284a2b":"markdown","89281b64":"markdown","9d0f01e1":"markdown","ea931aeb":"markdown","930e0c8f":"markdown","04c5ee9e":"markdown","c55bbf1b":"markdown","88d2e1bf":"markdown","3ade6253":"markdown","d6d125ae":"markdown","d054f0af":"markdown","6cf74a28":"markdown","dc7e68eb":"markdown","1243acdb":"markdown","44d327a2":"markdown","e4cbf167":"markdown","e0b4f09b":"markdown","5ab5d969":"markdown","d09a7f5b":"markdown","71a0faaa":"markdown","51f11d5c":"markdown","8b78ee00":"markdown","3f62b47d":"markdown","4b23577e":"markdown","717b03e3":"markdown","65c19531":"markdown","602ea1a6":"markdown","249c3069":"markdown","42ae3254":"markdown"},"source":{"2f7efc75":"## Data Analysis Phase\n## Main aim is to understand more about the data\nimport pandas as pd\nimport numpy as np\n\nimport sklearn \nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\n#Metrics Libraries\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pylab as pylab\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","9dbeeda8":"customer=pd.read_csv(\"..\/input\/online-shoppers-intention\/online_shoppers_intention.csv\")\ncustomer_copy=customer.copy()\n\n## print shape of dataset with rows and columns and information \nprint (\"The shape of the  data is (row, column):\"+ str(customer_copy.shape))\nprint (customer_copy.info())","5fbe771b":"customer_copy.head()","f6d4c9dd":"customer_copy.describe()","fce300fb":"import missingno as msno \nmsno.matrix(customer_copy)","7e11a501":"print('Data columns with null values:',customer_copy.isnull().sum(), sep = '\\n')","161ec34e":"for cols in ['Administrative','Informational','ProductRelated']:\n    customer_copy[cols].replace(0, np.nan, inplace= True)\nfor cols in ['Administrative','Informational','ProductRelated']:\n    print('{} null values:'.format(cols),customer_copy[cols].isnull().sum(), sep = '\\n')","bca826a7":"for cols in ['Administrative','Informational','ProductRelated']:\n    median_value=customer_copy[cols].median()\n    customer_copy[cols]=customer_copy[cols].fillna(median_value)\nfor cols in ['Administrative','Informational','ProductRelated']:\n    print('{} null values:'.format(cols),customer_copy[cols].isnull().sum(), sep = '\\n')","40a2804c":"for cols in ['Administrative_Duration','Informational_Duration','ProductRelated_Duration']:\n    customer_copy[cols].replace(0, np.nan, inplace= True)\nfor cols in ['Administrative_Duration','Informational_Duration','ProductRelated_Duration']:\n    customer_copy[cols].replace(-1, np.nan, inplace= True)\nfor cols in ['Administrative_Duration','Informational_Duration','ProductRelated_Duration']:\n    print('{} null values:'.format(cols),customer_copy[cols].isnull().sum(), sep = '\\n')","29a7429d":"for cols in ['Administrative_Duration','Informational_Duration','ProductRelated_Duration','BounceRates','ExitRates']:\n    mean_value=customer_copy[cols].mean()\n    customer_copy[cols]=customer_copy[cols].fillna(mean_value)\nfor cols in ['Administrative_Duration','Informational_Duration','ProductRelated_Duration','BounceRates','ExitRates']:\n    print('{} null values:'.format(cols),customer_copy[cols].isnull().sum(), sep = '\\n')","bc4ada46":"plt.figure(figsize = (15, 5))\n#plt.style.use('seaborn-white')\nplt.subplot(131)\nsns.scatterplot(x=\"Administrative\", y=\"Administrative_Duration\",hue=\"Revenue\", data=customer_copy)\nplt.subplot(132)\nsns.scatterplot(x=\"Informational\", y=\"Informational_Duration\",hue=\"Revenue\", data=customer_copy)\nplt.subplot(133)\nsns.scatterplot(x=\"ProductRelated\", y=\"ProductRelated_Duration\",hue=\"Revenue\", data=customer_copy)","ceb62ae9":"fig, ax1 = plt.subplots(figsize=(10,6))\ncolor = 'tab:green'\nax1.set_title('Average Page Value by Month', fontsize=16)\nax1.set_xlabel('Month', fontsize=16)\nax1.set_ylabel('Avg Temp', fontsize=16, color=color)\nax2 = sns.barplot(x='Month', y='PageValues', data = customer_copy, palette='summer',hue='Revenue')\nax1.tick_params(axis='y')\nax2 = ax1.twinx()\ncolor = 'tab:red'\nax2.set_ylabel('Avg Percipitation %', fontsize=16, color=color)\nax2 = sns.lineplot(x='Month', y='PageValues', data = customer_copy, sort=False, color=color)\nax2.tick_params(axis='y', color=color)\nplt.show()","d3e75e6b":"sns.relplot(x=\"BounceRates\", y=\"ExitRates\",col=\"Revenue\",hue=\"Revenue\",style=\"Weekend\", data=customer_copy)","94f343b5":"sns.catplot(x=\"VisitorType\", y=\"ExitRates\",\n                hue=\"Weekend\", col=\"Revenue\",\n                data=customer_copy, kind=\"box\");","280eec43":"sns.heatmap(customer_copy.corr(),annot=True,fmt='.1g',cmap='Greys')","e842e692":"feature_customer=customer_copy.copy()","c8035ca7":"plt.figure(figsize = (15, 10))\nplt.style.use('seaborn-white')\nax=plt.subplot(231)\nplt.boxplot(feature_customer['BounceRates'])\nax.set_title('BounceRates')\nax=plt.subplot(232)\nplt.boxplot(feature_customer['ExitRates'])\nax.set_title('ExitRates')\nax=plt.subplot(233)\nplt.boxplot(feature_customer['Administrative_Duration'])\nax.set_title('Administrative_Duration')\nax=plt.subplot(234)\nplt.boxplot(feature_customer['Informational_Duration'])\nax.set_title('Informational_Duration')\nax=plt.subplot(235)\nplt.boxplot(feature_customer['ProductRelated_Duration'])\nax.set_title('ProductRelated_Duration')\nax=plt.subplot(236)\nplt.boxplot(feature_customer['PageValues'])\nax.set_title('PageValues')","61d2efdd":"numerical_features=['BounceRates','ExitRates','Administrative_Duration','ProductRelated_Duration']\nfor cols in numerical_features:\n    Q1 = feature_customer[cols].quantile(0.25)\n    Q3 = feature_customer[cols].quantile(0.75)\n    IQR = Q3 - Q1     \n\n    filter = (feature_customer[cols] >= Q1 - 1.5 * IQR) & (feature_customer[cols] <= Q3 + 1.5 *IQR)\n    feature_customer=feature_customer.loc[filter]","4d28c593":"plt.figure(figsize = (15, 10))\nplt.style.use('seaborn-white')\nax=plt.subplot(221)\nplt.boxplot(feature_customer['BounceRates'])\nax.set_title('BounceRates')\nax=plt.subplot(222)\nplt.boxplot(feature_customer['ExitRates'])\nax.set_title('ExitRates')\nax=plt.subplot(223)\nplt.boxplot(feature_customer['Administrative_Duration'])\nax.set_title('Administrative_Duration')\nax=plt.subplot(224)\nplt.boxplot(feature_customer['ProductRelated_Duration'])\nax.set_title('ProductRelated_Duration')","354aa89c":"feature_customer.loc[feature_customer['SpecialDay'] > 0.4, 'SpecialDay'] = 1\nfeature_customer.loc[feature_customer['SpecialDay'] <= 0.4, 'SpecialDay'] = 0\nfeature_customer['SpecialDay'].value_counts()","494adbd7":"feature_customer['SpecialDay']=feature_customer['SpecialDay'].astype('bool')\nfeature_customer['SpecialDay'].value_counts()","4e5b0939":"for cols in ['Administrative','Informational','ProductRelated','OperatingSystems','Browser',\n             'Region','TrafficType','VisitorType']:\n    feature_customer[cols] = feature_customer[cols].astype('category')","b08b9531":"feature_customer.dtypes","7a9d61a4":"Categorical_variables=['Weekend','Revenue','Administrative','Informational','ProductRelated','SpecialDay',\n 'OperatingSystems','Browser','Region','Month','TrafficType','VisitorType']\n\n\nfeature_scale=[feature for feature in feature_customer.columns if feature not in Categorical_variables]\n\n\nscaler=StandardScaler()\nscaler.fit(feature_customer[feature_scale])","d1a408b5":"scaled_data = pd.concat([feature_customer[['Weekend','Revenue','Administrative','Informational',\n                                    'ProductRelated','SpecialDay','OperatingSystems',\n                                    'Browser','Region','Month','TrafficType','VisitorType']].reset_index(drop=True),\n                    pd.DataFrame(scaler.transform(feature_customer[feature_scale]), columns=feature_scale)],\n                    axis=1)\nscaled_data.head()","47fd910d":"encoded_features=['Month','VisitorType']\n\nlabel_data = scaled_data.copy()\nlabel_encoder = LabelEncoder()\nfor col in encoded_features:\n    label_data[col] = label_encoder.fit_transform(scaled_data[col])\n    \nlabel_data.head()","813d7299":"X=label_data.drop(['Revenue'],axis=1)\ny=label_data.Revenue\n\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)","15c851cc":"print(model.feature_importances_)","c98ef0ea":"feat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(17).plot(kind='barh')\nplt.show()","e620ae7a":"X=label_data.drop(['SpecialDay','VisitorType','Weekend','Revenue'],axis=1)\ny=label_data.Revenue\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8,random_state=1)\nprint(\"Input Training:\",X_train.shape)\nprint(\"Input Test:\",X_test.shape)\nprint(\"Output Training:\",y_train.shape)\nprint(\"Output Test:\",y_test.shape)","3df94980":"#creating the objects\nlogreg_cv = LogisticRegression(random_state=0)\ndt_cv=DecisionTreeClassifier()\nrt_cv=RandomForestClassifier()\nknn_cv=KNeighborsClassifier()\nsvc_cv=SVC(kernel='linear')\nnb_cv=BernoulliNB()\ncv_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'RandomForest',3:'KNN',4:'SVC',5:'Naive Bayes'}\ncv_models=[logreg_cv,dt_cv,rt_cv,knn_cv,svc_cv,nb_cv]\n\n\nfor i,model in enumerate(cv_models):\n    print(\"{} Test Accuracy: {}\".format(cv_dict[i],cross_val_score(model, X, y, cv=10, scoring ='accuracy').mean()))","6eb0e91b":"#Creating the pipeline\npipeline_lr=Pipeline([('lr_classifier',LogisticRegression(random_state=0))])\npipeline_dt=Pipeline([('dt_classifier',DecisionTreeClassifier())])\npipeline_randomforest=Pipeline([('rf_classifier',RandomForestClassifier())])\npipeline_knn=Pipeline([('knn_classifier',KNeighborsClassifier())])\npipeline_svc=Pipeline([('svc_classifier',SVC(kernel='linear'))])\npipeline_nb=Pipeline([('nb_classifier',BernoulliNB())])\n\n#Assigning the pipeline and relevant outcome variable\npipelines = [pipeline_lr, pipeline_dt, pipeline_randomforest,pipeline_knn,pipeline_svc,pipeline_nb]\nbest_accuracy=0.0\nbest_classifier=0\nbest_pipeline=\"\"\n\n# Dictionary of pipelines and classifier types for ease of reference\npipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'RandomForest',3:'KNN',4:'SVC',5:'Naive Bayes'}\n\n# Fit the pipelines\nfor pipe in pipelines:\n\tpipe.fit(X_train, y_train)\n\n#Evaluating each model\nfor i,model in enumerate(pipelines):\n    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(X_test,y_test)))","bcc00a97":"#Choosing the best model for our problem\nfor i,model in enumerate(pipelines):\n    if model.score(X_test,y_test)>best_accuracy:\n        best_accuracy=model.score(X_test,y_test)\n        best_pipeline=model\n        best_classifier=i\nprint('Classifier with best accuracy:{}'.format(pipe_dict[best_classifier]))","65308aa1":"# Create a pipeline\npipe = make_pipeline((RandomForestClassifier()))\n# Create dictionary with candidate learning algorithms and their hyperparameters\ngrid_param = [{\"randomforestclassifier\": [RandomForestClassifier()],\n                 \"randomforestclassifier__n_estimators\": [10, 100, 1000],\n                 \"randomforestclassifier__max_depth\":[5,8,15,25,30,None],\n                 \"randomforestclassifier__min_samples_leaf\":[1,2,5,10,15,100],\n                 \"randomforestclassifier__max_leaf_nodes\": [2, 5,10]}]\n#Gridsearch of the pipeline, the fit the best model\ngridsearch = GridSearchCV(pipe, grid_param, cv=5, verbose=0,n_jobs=-1) # Fit grid search\nbest_model = gridsearch.fit(X_train,y_train)","3ad6f761":"print(best_model.best_estimator_)\nprint(\"The mean accuracy of the model is:\",best_model.score(X_test,y_test))","09224fb7":"rt=RandomForestClassifier(max_depth=30, max_leaf_nodes=10,min_samples_leaf=15)\nrt.fit(X_train,y_train)\ny_pred=rt.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","9c865d76":"svc_classifier = SVC(kernel='linear',random_state = 0)\nsvc_classifier.fit(X_train,y_train)\ny_pred=svc_classifier.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\\n\",classification_report(y_test,y_pred))","5b450a4f":"## Hit upvote if you like my work and also check out my [other notebooks](https:\/\/www.kaggle.com\/benroshan\/notebooks)","948f171e":"<a id=\"subsection-eighteen\"><\/a>\n## 7.6 Feature Selection\nLet's check the feature importances and prune our features to make our model perform well. ","2775c9ff":"We have now figured out the real number null values hiding in the dataset. Let's impute them with the mean value","e487245a":"<a id=\"section-one\"><\/a>\n# *5: Prepare Data for Consumption*\n<a id=\"subsection-five\"><\/a>\n## 5.1 Import Libraries\nLet's import all necessary libraries for the analysis and along with it let's bring down our dataset","9f0523fa":"**Insights:**\n* The Exit rates are **very low** in spread when there is a revenue and there isn't much change in exit rates considering the weekend and type of visitors\n* A **large variation** is seen in exit rates other category when it is in weekend and haven't made any revenue. May be they are of casual window shopping kind.\n* **New visitors** have low exit rates which and are pretty much same in both the revenue cases. It's pretty good to hold the new visitors.","bc0ba4d6":"<a id=\"subsection-fifteen\"><\/a>\n## 7.3 Converting Dtypes \nSince we have catgorical variables which are identified as numerical. I guess it is better to convert them to categories","ece88fef":"We have encoded the required features, Let's prune our features through Feature selection","74b196c8":"If you have noticed in the plot, there was no null values shown. But while checking each records we have **14 null records in 8 features**. Let's decide on how to deal these missing records","d1c3a64c":"<a id=\"subsection-fourteen\"><\/a>\n## 7.2 Special Day- Feature Clubbing !\nHere I'm planning to club the special day feature which has 0.2,0.4,0.6,0.8,1 . So let's club the values and replace the values based on a condition. These are probability values, so\n* If it is greater than 0.4 it is '1' which indicates it is a 'Special day'. \n* If it is less than or equal to 0.4 it is 0 which indicates 'Not a Special day'","c77e5237":"<a id=\"subsection-twelve\"><\/a>\n## 6.5 Correlation map ","262af303":"<a id=\"subsection-twentytwo\"><\/a>\n## 8.3 Random Forest with Hyperparameter Tuning\nLet's build a random forest classifier model with hyperparameter tuning ","3a693113":"**Choice of models:**\n1. Random Forest(Ensemble)\n2. SVC\n\nFirst let's build our Random Forest model along with Hyperparameter tuning","d1552bac":"\nWe have scaled our numerical features using standard scaler","7a7c563f":"### b)Handling Missing Values\n\n**Part1**\n\nThe Administration, Informational and product related features are types of pages, technically they are nominal data. So I guess we impute that data with median, Also before doing that we have the min value as 0(page type as 0 means it should be null value) should also be considered as NaN value, so we convert it into NaN value and impute it. ","a825ab09":"<a id=\"subsection-seven\"><\/a>\n## 5.3 Data Cleaning\n### a) Checking for missing values\nFirst lets check it visually","238d2777":"<a id=\"section-five\"><\/a>\n# *7.Feature Engineering and Selection*\nNow let's work on the data little bit and jump into the feature engineering. First, Let's remove the outliers","e83bfb64":"**Part 2**\n\nWe are left with page duration bounce and exit rates where we have -1 as the minimum value in duration(time cannot be negative) which should be considered as a null value and 0 duration(time can't be zero) occurs when the page type was 0 which we imputed earlier, so we can convert this into NaN and impute this as well.<br>\nFor rates, we can impute the the NaN values directly(Here we don't need to worry about rates being bounce rates having 0 values, as there are many such cases where bounce rates can be 0 because the user must have liked the webiiste and moved onto other web pages towards transactions)","2d48bed9":"<a id=\"subsection-thirteen\"><\/a>\n## 7.1 Handling Outliers\nLet's check out our numerical feature outliers through boxplot","ccb5a0cd":"We have now figured out the real number null values hiding in the dataset. Let's impute them with the median","89476b27":"<a id=\"subsection-sixteen\"><\/a>\n## 7.4 Feature Scaling\nLet's standardize our numerical features as we have more outliers","13166a97":"<a id=\"subsection-twentythree\"><\/a>\n## 8.4 SVC with Hyperparameter Tuning\nLet's build our second classifier model SVC by giving some hyperparameters\n\n**Note: Hyperparameter tuning of SVM took more than hours to run in my device. So I'm bypassing hyperparameter tuning for this section. I will update once I have a better device in my hands**","c25c8978":"**Insights:**\n* High duration users are found for the initial types of pages **(0-15: Administrative;0-10:Informational;0-300:Product Related)**\n* Revenue hasn't been generated for the **latter types**\n* There is a linear relationship between product_related and Product duration **(multicollinearity)**\n","ada8605d":"**Insights:**\n* The page value has considerably increased over the months with some fall **in mid of the year**\n* Page value is **high** when there is a revenue\n* There is a **high variance** in page value data considering each month","d6ed08e6":"<a id=\"section-six\"><\/a>\n# *8. Modelling our Data*\nLet's enter into the crucial phase of building THE machine learning model. Before checking \"what could be the best algorithm for prediction\" we have to decide on the \"why\". It is highly important. \n### Why?\nOur main aim is to predict whether there is a revenue transaction made owing to those values from the features. The output is either going to be 0 or 1. So we can decide that we can use classification models for our problem\n### What ?\nTo decide on what can be the best possible classification models let's not waste time running models. Instead we do quality code by creating pipeline and check all the model accuracy at once. After that we will select one model based on it's accuracy. ","1997a95e":"Now, let's change the values into boolean as it makes more sense\n* 1-True-Special Day\n* 0-False-Not a Special Day","b390c85b":"<a id=\"subsection-twenty\"><\/a>\n## 8.1 Selecting the right model- Cross Validation\nLet's select our model using cross validation","82aee8ab":"<a id=\"subsection-ten\"><\/a>\n## 6.3 Bounce & Exit Rates Vs Revenue","43284a2b":"From the test results, we can choose **Random forest** as our model. It gave us more accurate results since it is an ensemble model. So Let's also consider working in **SVC** as well. \n\nNext we will also test our choice via pipelines as well.","89281b64":"<a id=\"subsection-twentyone\"><\/a>\n## 8.2 Selecting the right model-Pipeline\nLet's select our model using pipeline","9d0f01e1":"<a id=\"subsection-seventeen\"><\/a>\n## 7.5 Label Encoding\nLet's encode our month feature using Label encoder","ea931aeb":"We have converted them to categories, Now let's scale numerical features","930e0c8f":"We have got 90.6% accuracy. Great ","04c5ee9e":"<a id=\"subsection-six\"><\/a>\n## 5.2 Meet and Greet Data\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent\/feature variables(s)), what's its goals in life (dependent\/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview of variable datatypes (i.e. qualitative vs quantitative). Click here for the [Source Data Dictionary](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Shoppers+Purchasing+Intention+Dataset).","c55bbf1b":"***Dataset :***\n\nThe dataset consists of feature vectors belonging to 12,330 sessions. The dataset was formed so that each session would belong to a different user in a 1-year period to avoid any tendency to a specific campaign, special day, user profile, or period.\n\n***Attribute :***\n* **Revenue** => class whether it can make a revenue or not\n* **Administrative, Administrative Duration, Informational, Informational Duration, Product Related and Product Related Duration** => represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories.\n* **Bounce Rate** => percentage of visitors who enter the site from that page and then leave (\"bounce\") without triggering any other requests to the analytics server during that session\n* **Exit Rate** => the percentage that were the last in the session\n* **Page Value** => feature represents the average value for a web page that a user visited before completing an e-commerce transaction\n* **Special Day** => indicates the closeness of the site visiting time to a specific special day (e.g. Mother\u2019s Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction. For example, for Valentina\u2019s day, this value takes a nonzero value between February 2 and February 12, zero,before and after this date unless it is close to another special day, and its maximum value of 1 on February 8\n* **Operating system,browser, region, traffic type** => Different types of operating systems, browser, region and traffic type used to visit the website\n* **Visitor type** => Whether the customer is a returning or new visitor\n* **Weekend** => A Boolean value indicating whether the date of the visit is weekend\n* **Month** => Month of the year\n\nLet's look at our dataset in a dataframe","88d2e1bf":"oof, we have a lot of outliers, if you notice informational_duration and page values, they don't have any distribution and if you remove the outliers there will be one value left in it. So except that two features, we are removing the outliers via IQR method","3ade6253":"<a id=\"section-seven\"><\/a>\n# Conclusion \n![giphy-7.gif](attachment:giphy-7.gif)\nThe significant importance of  PageValue suggests that customers look at considerably different products and its recommendations. Hence a significant improvement on recommendation engines and bundle packages would bring in more conversions. Including more products exploiting the long tail effect in e-commerce will also bring in more revenue drivers. <br>\n\nHere are the revised pointers than can help improve the converion rate\n1. Following a minimalist approach in UI\n2. Being transparent to the visitors about the prices and information of product\n3. Improving the stay duration by providing them targetted ads like discounts and offers\n4. Reducing the bounce rates through faster refreshing rate and attractive landing page which has highly targetted products exclusive for the visitors\n5. Personalized emails for each visitors and engaging the loyal visitors(returning visitor) through coupons and exclusive access of products","d6d125ae":"<a id=\"subsection-nineteen\"><\/a>\n## 7.7 Train and Test Split (80:20)\nLet's drop the required features and split the data into train and test","d054f0af":"<a id=\"section-one\"><\/a>\n<a id=\"subsection-one\"><\/a>\n# *1.Problem Definition*\n![5e28e1bd0a3e8437ab4646ff_marketing%20analytics.gif](attachment:5e28e1bd0a3e8437ab4646ff_marketing%20analytics.gif)\nI'm a Data Analyst of XYZ Ltd. and this data consists of various information related to customer behavior in online shopping websites. It looks like our ecommerce client firm ABC needs to improve on its online sales conversion rate as they are looking forward for a huge investment into their website portal.This dataset helps us to perform Marketing Analytics and understand the KPIs and Metrics related to it. We should also provide marketing strategies to improve their conversion rate by making the customer go through the marketing funnel.\n\n<a id=\"subsection-two\"><\/a>\n# *2. About Project:*\nIn this project, I will analyze the Customer's Intentions based on the transactions, duration and rates made online in a year <br>\n#### Project Summary: \nThe increasing popularity of online shopping has led to the emergence of new economic activities. To succeed in the highly competitive e-commerce environment, it is vital to understand consumer intention. Understanding what motivates consumer intention is critical because such intention is key to survival in this fast-paced and hypercompetitive environment. Where prior research has attempted at most a limited adaptation of the information system success model, we propose a comprehensive, empirical model that separates the \u2018use\u2019 construct into \u2018intention to use\u2019 and \u2018actual use\u2019. This makes it possible to test the importance of user intentions in determining their online shopping behaviour. Our results suggest that the consumer's intention to use is quite important, and accurately predicts the usage behaviour of consumers. In contrast, consumer satisfaction has a significant impact on intention to use but no direct causal relation with actual use.\n\n<a id=\"subsection-three\"><\/a>\n# *3. Kernel Goals*\n1. Meet and Greet Data\n2. Prepare the Data for consumption (Feature Engineering and Selection)\n3. Perform Exploratory Analysis (Visualizations)\n4. Model the Data using Machine Learning\n5. Validate and implement data model \n6. Optimize and Strategize","6cf74a28":"**Insights:**\n* There is a high correlation with **page values and revenue**\n* Bounces rates got **negative** influence on revenue\n* Pages which has **high bounce rates tends to have high exit rates** as well\n* The website earns a lot of revenue from **product related pages**","dc7e68eb":"We have handled all the null values. Let's try to visualize the data ","1243acdb":"<a id=\"subsection-nine\"><\/a>\n## 6.2 Month Vs Page value Vs Revenue","44d327a2":"<a id=\"section-three\"><\/a>\n# *6. Data Visualization*\nSince we have much numerical data, let's keep our plots much targetted towards our machine learning models. Also let's figure out which feature importances and prune away least important ones","e4cbf167":"### Fitting all the parameters to the model\nLet's fit all the parameters we derived by hyperparameter tuning into the actual model","e0b4f09b":"<a id=\"subsection-eight\"><\/a>\n## 6.1 Duration of pages vs Revenue","5ab5d969":"<a id=\"subsection-four\"><\/a>\n# *4. Gathering the Data*\nThe dataset is also given to us on a golden plate csv file at Kaggle: [Online Shopper's Intention](https:\/\/www.kaggle.com\/roshansharma\/online-shoppers-intention)","d09a7f5b":"Looks like we don't have any null values except one. But plots sometimes deceive us, numbers don't. Let's check with the numbers","71a0faaa":"We have fitted the model by creating a list for all parameters and we will be evaluating based on cross validation","51f11d5c":"We have got the best parameters for the model and the mean accuracy is 90.2%","8b78ee00":"<a id=\"section-four\"><\/a>\n# *Strategies for the client to increase sales conversion rate*\n\n### Strategy 1:\nThe landing page has to be very attractive and UI friendly. We can provide minimum information and lot more icon features rather than bombarding the visitor with too much information(words) which can increase the exit rates. Also the website can be transparent about the shipping charges along with the price of product in the landing page. If the shipping charges suddenly pops up on checkout page that might increase exit rates\n### Strategy 2:\nCategorize each user and send personalized emails. Personalized emails can reduce retention and improve loyalty to the brand\n### Strategy 3:\nIntroducing ad banners when the visitor tries to quit the page. The ad banner can be about discounts\/offers which can make  the visitor stay in the page\n### Strategy 4:\nEnagaging loyal customer by giving them loyalty badge and providing unique access to the products like they can book before the official release and offers for them\n### Strategy 5:\nIncrease the duration of website on monthly basis by introducing monthly themed offers and products.This can increase the conversion rate\n### Strategy 6:\nOptimize the SEO from Google other sources based on the place, community and language. They can get targetted with ads in regional language or regional hot topic to make the visitor hooked to the website\n### Strategy 7:\nSmooth UI and interactions with the website across all the browsers and they can read OTP inside the checkout page rather than redirecting it into another page. Even if it is directed to other page, it should automatically redirect to the page ensuring the purchase success.","3f62b47d":"**Insights:**\n* High Bounce and Exit Rates lead to **no Revenue**\n* Revenue data is **heavily imbalanced**\n* More Bounce and Exit rates when it is **not a weekend**","4b23577e":"From the bar plot we can see the importances of features based on it's impact towards output. Let's  take up the top 14 features ","717b03e3":"Great. We have got pretty good accurate results from our model. We can see that **Random Forest** have the highest accuracy being an ensemble model. It usually does have higher accuracy. Let's select the best model","65c19531":"# Table of Contents:\n1. [Introduction](#section-one)\n    - [Problem Definition](#subsection-one)\n    - [About Project](#subsection-two)\n    - [Kernel Goals](#subsection-three)\n    - [Gathering Data](#subsection-four)\n2. [Prepare Data for Consumption](#section-two)\n    - [Import Libraries](#subsection-five)\n    - [Meet and Greet Data](#subsection-six)\n    - [Data Cleaning](#subsection-seven)\n3. [Data Visualization](#section-three)\n    - [Duration of pages vs Revenue](#subsection-eight)\n    - [Month Vs Page value Vs Revenue](#subsection-nine)\n    - [Bounce & Exit Rates Vs Revenue](#subsection-ten)\n    - [Visitor type & Exit Rates Vs Revenue](#subsection-eleven)\n    - [Correlation map ](#subsection-twelve)\n4. [Strategies to improve conversion rate](#section-four)\n5. [Feature Engineering and Selection](#section-five)\n    - [Handling Outliers](#subsection-thirteen)\n    - [Special Day- Feature Clubbing !](#subsection-fourteen)\n    - [Converting Dtypes](#subsection-fifteen)\n    - [Feature Scaling](#subsection-sixteen)\n    - [Label Encoding](#subsection-seventeen)\n    - [Feature Selection](#subsection-eighteen)\n    - [Train and Test Split (80:20)](#subsection-nineteen)\n6. [Modelling our Data](#section-six)\n    - [Selecting the right model- Cross Validation](#subsection-twenty)\n    - [Selecting the right model-Pipeline](#subsection-twentyone)\n    - [Random Forest with Hyperparameter Tuning](#subsection-twentytwo)\n    - [SVC with Hyperparameter Tuning](#subsection-twentythree)\n6. [Conclusion](#section-seven)\n    ","602ea1a6":"<a id=\"subsection-eleven\"><\/a>\n## 6.4 Visitor type & Exit Rates Vs Revenue","249c3069":"We have removed a good number of outliers !","42ae3254":"We have 90% accuracy and Also we have  91% precision"}}