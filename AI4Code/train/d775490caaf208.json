{"cell_type":{"9b686f50":"code","cb7c5060":"code","892d3a63":"code","73eda5f6":"code","bb0e7f09":"code","0e4151bb":"code","4bdf59da":"code","00b5e1ca":"code","bd0886e7":"code","17adb998":"code","a1036a61":"code","4dee9e2b":"code","f0f0516a":"code","b8fd3a27":"code","7428ab9e":"code","93ee7e79":"code","6e4d8a8f":"code","fd42a930":"code","f87ccb76":"code","70af8045":"code","ae9785ed":"code","afe2bfd0":"code","cb5ea18f":"code","7d545e21":"code","02ed5256":"code","7cd41d83":"code","7d7c5ef3":"code","daddb7b9":"code","51bb1b43":"code","5e8622f9":"code","e2a1934b":"code","99bafc82":"code","864e8f01":"code","0760fff3":"code","23f8f263":"code","c93f77d8":"code","8e1e89ae":"code","6284b0f2":"code","2b543d33":"code","98e3a915":"code","b0c34edc":"code","14bf79a3":"code","05f9d974":"code","dc662580":"code","b0b4e9aa":"code","2fa79d63":"markdown","6809589a":"markdown","e71b85de":"markdown","c0938025":"markdown","d7237f79":"markdown","c0240d0f":"markdown","6d380249":"markdown","109229de":"markdown","29e8c006":"markdown","80ac099c":"markdown","fc7d83c7":"markdown","6464d485":"markdown","e0010e57":"markdown","955162ad":"markdown","3dab1eab":"markdown","1777f3e3":"markdown","5e6db681":"markdown","15af3e2e":"markdown","c8ab4f31":"markdown","b9a84949":"markdown","00321cd2":"markdown","5eb60069":"markdown","96ba6909":"markdown","a2939840":"markdown","ea20c168":"markdown","f24b4b75":"markdown","002625ac":"markdown","1104fad9":"markdown","46508480":"markdown","5bd2620a":"markdown","08cc43ea":"markdown","0e6c8463":"markdown","f3e6f239":"markdown","dca1fe8c":"markdown","80793a8f":"markdown","5be0825b":"markdown","0d953e9d":"markdown","0f3c180f":"markdown","1e58d056":"markdown"},"source":{"9b686f50":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n","cb7c5060":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","892d3a63":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","73eda5f6":"train.head()","bb0e7f09":"y_train = train.iloc[:, 1].values","0e4151bb":"test.head()","4bdf59da":"# Countplot \nsns.catplot(x =\"Sex\", hue =\"Survived\", kind =\"count\", data = train) ","00b5e1ca":"group = train.groupby(['Pclass', 'Survived']) \npclass_survived = group.size().unstack() \n  \nsns.heatmap(pclass_survived, annot = True, fmt =\"d\") ","bd0886e7":"#Code : Factor plot for Family_Size (Count Feature) and Family Size.\n\n# Adding a column Family_Size \ntrain['Family_Size'] = 0\ntrain['Family_Size'] = train['Parch']+train['SibSp'] \n  \n# Adding a column Alone \ntrain['Alone'] = 0\ntrain.loc[train.Family_Size == 0, 'Alone'] = 1\n  \n# Factorplot for Family_Size \nsns.factorplot(x ='Family_Size', y ='Survived', data = train) \n  \n# Factorplot for Alone \nsns.factorplot(x ='Alone', y ='Survived', data = train) ","17adb998":"#Code : Bar Plot for Fare\n\n\n# Divide Fare into 4 bins \ntrain['Fare_Range'] = pd.qcut(train['Fare'], 4) \n  \n# Barplot - Shows approximate values based  \n# on the height of bars. \nsns.barplot(x ='Fare_Range', y ='Survived', data = train)","a1036a61":"sns.distplot(train['Age'].dropna(), bins=15, kde=False)","4dee9e2b":"#Code : Categorical Count Plots for Embarked Feature\n\n\n# Countplot \nsns.catplot(x ='Embarked', hue ='Survived', kind ='count', col ='Pclass', data = train)","f0f0516a":"train.head()","b8fd3a27":"extra_eda_cols = ['SibSp', 'Parch', 'Family_Size', 'Fare_Range', 'Alone']\ntrain = train.drop(extra_eda_cols, axis = 1, inplace = False)\ntrain.head()","7428ab9e":"#droping the  unnecessary columns\n\nextra_cols = ['PassengerId', 'Name', 'Ticket', 'Fare', 'Cabin']\ntrain = train.drop(extra_cols, axis = 1, inplace = False)\ntrain.head()","93ee7e79":"x_train = train.drop('Survived', axis = 1, inplace = False)\nprint(x_train)","6e4d8a8f":"sns.heatmap(x_train.isnull())","fd42a930":"train.isnull().sum()","f87ccb76":"#For 'Age' column\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(x_train[['Age']])\nx_train[['Age']]= imputer.transform(x_train[['Age']])\n\n\n#For 'Embarked' column\n\nimputers = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputers.fit(x_train[['Embarked']])\nx_train[['Embarked']]= imputers.transform(x_train[['Embarked']])","70af8045":"x_train.isnull().sum().any()","ae9785ed":"x_train.head()","afe2bfd0":"from sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() \n\n\n#Sex Column  \nx_train['Sex']= label_encoder.fit_transform(x_train['Sex']) \n\n#Embarked Column\nx_train['Embarked']= label_encoder.fit_transform(x_train['Embarked'])","cb5ea18f":"x_train.head()","7d545e21":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)","02ed5256":"test.head()","7cd41d83":"test.isnull().sum().any()","7d7c5ef3":"sns.heatmap(test.isnull())","daddb7b9":"#For 'Age' column\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(test[['Age']])\ntest[['Age']]= imputer.transform(test[['Age']])\n\n\n#For 'Embarked' column\n\nimputers = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputers.fit(test[['Embarked']])\ntest[['Embarked']]= imputers.transform(test[['Embarked']])","51bb1b43":"extra_cols_test = ['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin']\ntest = test.drop(extra_cols_test, axis = 1, inplace = False)\ntest.head()","5e8622f9":"from sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() \n\n\n#Sex Column  \ntest['Sex']= label_encoder.fit_transform(test['Sex']) \n\n#Embarked Column\ntest['Embarked']= label_encoder.fit_transform(test['Embarked'])","e2a1934b":"test.head()","99bafc82":"from sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\ntest = sc_x.fit_transform(test)","864e8f01":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(x_train, y_train)","0760fff3":"y_pred = classifier.predict(test)\nfrom sklearn.model_selection import cross_val_score\nacc_Tree = cross_val_score(classifier, x_train, y_train, cv=10, scoring='accuracy').mean()\nacc_Tree","23f8f263":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(test)","c93f77d8":"y_pred = classifier.predict(test)\nfrom sklearn.model_selection import cross_val_score\nacc_Tree = cross_val_score(classifier, x_train, y_train, cv=10, scoring='accuracy').mean()\nacc_Tree","8e1e89ae":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(test)","6284b0f2":"y_pred = classifier.predict(test)\nfrom sklearn.model_selection import cross_val_score\nacc_Tree = cross_val_score(classifier, x_train, y_train, cv=10, scoring='accuracy').mean()\nacc_Tree","2b543d33":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(x_train, y_train)","98e3a915":"y_pred = classifier.predict(test)\nfrom sklearn.model_selection import cross_val_score\nacc_Tree = cross_val_score(classifier, x_train, y_train, cv=10, scoring='accuracy').mean()\nacc_Tree","b0c34edc":"from sklearn.svm import SVC\nclassifier = SVC()\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(test)","14bf79a3":"y_pred = classifier.predict(test)\nfrom sklearn.model_selection import cross_val_score\nacc_Tree = cross_val_score(classifier, x_train, y_train, cv=10, scoring='accuracy').mean()\nacc_Tree","05f9d974":"accuracy = {'Model' : ['Logistic Regression', 'K- Nearest Neighbor', 'SVC', 'Decision Tree', 'Random Forest'],\n                  'Accuracy' : [0.7890, 0.8047, 0.8226, 0.7935, 0.8037]\n                 }\nall_cross_val_scores = pd.DataFrame(accuracy, columns = ['Model', 'Accuracy'])\nall_cross_val_scores.head()","dc662580":"test_df = pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': y_pred\n})\nsubmission.to_csv('titanic_prediction.csv', index=False)\nprint('File Saved')","b0b4e9aa":"submission","2fa79d63":"After observing the above graph, we can say that women were more likely to survived than men as they have high rate of survival than man. Hence, in determining whether a passenger will survive or not, gender(male or female) plays an important role.","6809589a":"Checking how many missing values are there.","e71b85de":"Analysing data with graphs.","c0938025":"# Encoding Categorical Data","d7237f79":"We will build a number of Classification models and at the end we will take the model having highest accuracy.\nSo let's get started........","c0240d0f":"'Cabin' has maximum number of missing values. 'Age' column also have many NaN values and 'Fare' cloumn has 1 missing value.","6d380249":"# Building Various Classification Models","109229de":"## Checking for missing values","29e8c006":"We will explore the given data with various given features in datasets before jumping to modeling the data. Here, our main ojective is to gain as much knowledge as we can. We will have maximum insights of data with library Seaborn.","80ac099c":"It helps in determining if higher-class passengers had more survival rate than the lower class ones or vice versa. Class 1 passengers have a higher survival chance compared to classes 2 and 3. It implies that Pclass contributes a lot to a passenger\u2019s survival rate.","fc7d83c7":"Alright as we can see, '**SVC**' has highest score. So, here we have best model.","6464d485":"## Logistic Regression","e0010e57":"## KNN","955162ad":"## Filling missing values column by column","3dab1eab":"Now we are done with all the models. Now let's make a dataframe showing models with their cross_val_score for visualizing in a good way.","1777f3e3":"### Filling the missing values column by column using scikit-learn.","5e6db681":"# Applying Feature Scaling on Test Set","15af3e2e":"## Random Forest Classifier","c8ab4f31":"\n1. Women survived more than men.\n2. Class 1 passengers were more lucky than Class 2 and 3.\n3. Unfortunately, Class 3 was most affected.\n4. Alone passengers had less survival rate.\n5. Survival rate is more for passengers who paid higher fare.\n6. Most of the passangers were of age between 20-40.\n7. Majority of the passengers were boarded from 'S'.","b9a84949":"**Some notable observations are:**\n\n* Majority of the passengers boarded from S. \n\n* Majority of class 3 passengers boarded from Q.","00321cd2":"First we will drop unnecessary columns because they do not contribute to final output.","5eb60069":"# Dropping unnecessary columns","96ba6909":"# Exploratory Data Analysis ","a2939840":"'Age' has 177 and 'Embarked' has 2 missing values.","ea20c168":"# IMPORTING THE LIBRARIES","f24b4b75":"Fare denotes the fare paid by a passenger. As the values in this column are continuous, they need to be put in separate bins(as done for Age feature) to get a clear idea. It can be concluded that if a passenger paid a higher fare, the survival rate is more.","002625ac":"Family_Size denotes the number of people in a passenger\u2019s family. It is calculated by summing the SibSp and Parch columns of a respective passenger. Also, another column Alone is added to check the chances of survival of alone passenger against the one with a family.\n\nImportant observations \u2013\n\nIf a passenger is alone, the survival rate is less.\nIf the family size is greater than 5, chances of survival decreases considerably.","1104fad9":"Many passensgers are of age 15-40 yrs.","46508480":"We are done with EDA. Now, we will perform Data Preprocessing on both train and test dataset followed by Feature Scaling and then finally we will train our datasets on various models.\n\n**Excited......????\n      \n  So let's get one step closer to solve this problem...","5bd2620a":"## Decision Tree Classifier","08cc43ea":"# Overall Conclusions from EDA:","0e6c8463":"# Encoding Categorical Data ","f3e6f239":"Let's get started with Titanic Problem","dca1fe8c":"# Preprocessing on TEST Dataset","80793a8f":"## Checking the missing values","5be0825b":"# IMPORTING THE 'TRAIN' AND 'TEST' DATASETS","0d953e9d":"As we see, now there is not any missing value in any column.","0f3c180f":"# Applying Feature Scaling on training data","1e58d056":"## SVC"}}