{"cell_type":{"fdcd0807":"code","8b01a94c":"code","b13d2cf6":"code","9fe81142":"code","fd0be1ba":"code","ce08fcb8":"code","5e7451e8":"code","bd54ffa8":"code","8da62018":"code","8ed5e62b":"code","6345bd40":"code","ffe8bb99":"code","9a295935":"code","52e535ae":"code","50590785":"code","9fee3f97":"code","9dbee6c9":"code","75b69826":"code","66bfcf1e":"code","5b8a4446":"code","b945c0df":"code","25fcef95":"code","7d61002a":"code","e449dc55":"code","a8d2d031":"code","b2412c2b":"code","7d7ace1c":"code","2ce6619a":"code","2221eae5":"code","af935b07":"code","71b8ea0e":"code","b5e31751":"markdown","f231bb67":"markdown","0ca18cab":"markdown","5d364e28":"markdown","5e0a554b":"markdown","05285252":"markdown","119e8279":"markdown","cd85e099":"markdown","b4bbec11":"markdown","0a2c204f":"markdown","37ec6433":"markdown","f7dad763":"markdown","97ee2197":"markdown"},"source":{"fdcd0807":"import pandas as pd  #for loading the dataset as DataFrame\nimport numpy as np   #for handling multi-D arrays and mathematical computations\nimport seaborn as sb #highly interactive visualization of dataset\nfrom matplotlib import pyplot as plt #visualization the data\nfrom sklearn.model_selection import train_test_split  # split the data into trian and test sets\n# different algorithms for comparisons\nfrom sklearn.ensemble import RandomForestClassifier # also used for feature selection\nfrom sklearn.ensemble import GradientBoostingClassifier #boosting algorithm\nfrom sklearn.tree import DecisionTreeClassifier     \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC   # support vector classifier (SVC)","8b01a94c":"data = pd.read_csv('..\/input\/loan-prediction.csv') \n#our dataset is about Loan Status of different applicants with several features","b13d2cf6":"data.head() #overview of our dataset","9fe81142":"data.isna().sum() #check how many number of Nan are present in each column","fd0be1ba":"data.shape   #dimension of our dataset","ce08fcb8":"data.fillna(method='bfill',inplace=True) # here we are use backward filling to remove our Nan from Dataset","5e7451e8":"#countplot of different gender on the basis of there loan status\nsb.countplot(x='Gender',\n             data=data,\n             hue='Loan_Status',\n             palette=\"GnBu_d\") ","bd54ffa8":"#here we are clearly obeserve the what is applicantIncome and whether he\/she is self_employed or not,\n#with there Loan Status\nsb.catplot(x='Gender',\n           y='ApplicantIncome',\n           data=data,\n           kind ='bar',\n           hue='Loan_Status',\n           col='Self_Employed')","8da62018":"#here we are clearly obeserve the what is co-applicantIncome and whether he\/she is self_employed or not,\n#with there Loan Status\nsb.catplot(x='Gender',\n           y='CoapplicantIncome',\n           data=data,\n           kind ='bar',\n           hue='Loan_Status',\n           col='Self_Employed')","8ed5e62b":"data['ApplicantIncome'].plot(kind='hist',bins=50) #histogram of Applicant-Income\n# we see that most of them are in between 0-10000","6345bd40":"data['CoapplicantIncome'].plot(kind='hist',bins=50)\n#histogram of coappicantIncome which almost similar to the ApplicantIncome's histogram","ffe8bb99":"#it is more useful to use ApplicantIncome and CoapplicantIncome as one featue i.e, Total_Income\n#for reducing the feature set\n#So, I am creating new column Total_Income as the sum of ApplicantIncome and CopplicantIncome\ndata['Total_Income']=(data['ApplicantIncome']+data['CoapplicantIncome'])\ndata['Total_Income'].plot(kind='hist',bins=50) #histogram of Total_Income which is almost similar with above two\ndata.drop(columns=['ApplicantIncome','CoapplicantIncome'],inplace=True) ","9a295935":"sb.countplot(data.Dependents,data=data,hue='Loan_Status')\n#count of different dependents with respect to there Loan_status","52e535ae":"sb.countplot(data.Education,data=data,hue='Loan_Status',palette='Blues')\n#count of graduated or non-graduated with respect to there Loan_status","50590785":"sb.countplot(data.Married,data=data,hue='Loan_Status')\n#count of Married or non-Married applicant with respect to there Loan_status","9fee3f97":"sb.barplot(x='Credit_History',y='Property_Area',data=data,hue='Loan_Status',orient='h')\n# relation of credit history in different Property Area with respect to there Loan_Status","9dbee6c9":"sb.barplot(x='Loan_Amount_Term',y='LoanAmount',data=data,hue='Loan_Status',palette='Blues')\n#visualizing LoanAmount on the basis of LoanAmountTerm with respect to Loan_Status","75b69826":"#As above we observe that our there are so many columns with categorical values.\n#which are useful feature for predicting our Loan Status at the end\n#for the sake of simplicity I am coverting these categorical values in to numeric values.\nx = pd.Categorical(data['Gender'])               # Male=1,Female=0\ndata['Gender']=x.codes\n\nx = pd.Categorical(data['Married'])              # Yes=1,No=0\ndata['Married']=x.codes\n\nx = pd.Categorical(data['Education'])            #Graduate=0,Non-graduated=1\ndata['Education']=x.codes\n\nx = pd.Categorical(data['Self_Employed'])        #Yes=1,No=0\ndata['Self_Employed']=x.codes\n\nx = pd.Categorical(data['Property_Area'])        # Rural=0,SemiUrban=1,Urban=2\ndata['Property_Area']=x.codes\n\nx = pd.Categorical(data['Loan_Status'])          #Y=1,N=0\ndata['Loan_Status'] = x.codes\n\n#in dependent column we clearly see that there is + sign for dependents more than 3\n#which makes it column of object data type \n#So, I am going to remove this sign and convert it into numeric value\ndata['Dependents'] = data['Dependents'].str.replace('+','')     \ndata['Dependents'] = pd.to_numeric(data['Dependents'])","66bfcf1e":"plt.figure(figsize=(10,7))\nsb.heatmap(data.corr(),cmap='Greens',annot=True)\n#Visualizing the correlation matrix using heatmap ","5b8a4446":"#We are going to predict the Loan_Status\nY=data['Loan_Status']\nX=data.drop(columns=['Loan_Status','Loan_ID']) #X is all columns except Loan_Status and Loan_ID\n# split the train and test dataset where test set is 30% of original dataset\nxtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=0.3) ","b945c0df":"clf = RandomForestClassifier(n_estimators=400,max_depth=5) #defining RandomForest Classifier","25fcef95":"clf = clf.fit(xtrain,ytrain)  #fitting our train dataset","7d61002a":"clf.score(xtest,ytest)       #score on our test dataset","e449dc55":"pd.Series(clf.feature_importances_,xtrain.columns).sort_values(ascending=False)\n#feature importance in descending order\n#So, I am using only top 4 features as my input","a8d2d031":"#Respliting the trianing and testing dataset\nY=data['Loan_Status']\nX=data[['Credit_History','Total_Income','LoanAmount']] #X is top 3 feature having more feature importance values\n# split the train and test dataset where test set is 30% of original dataset\nxtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=0.3) ","b2412c2b":"#Re-applying the RandomForest Classifiers\nclf = RandomForestClassifier(n_estimators=400,max_depth=5) \nclf = clf.fit(xtrain,ytrain) \nclf.score(xtest,ytest)\n#we can clearly observe that it increases the accuracy percentage","7d7ace1c":"clf = LogisticRegression()  #defining Logistic Regression\nclf = clf.fit(xtrain,ytrain) \nclf.score(xtest,ytest)","2ce6619a":"clf = SVC()  #defining Support Vector Classifier\nclf = clf.fit(xtrain,ytrain) \nclf.score(xtest,ytest)","2221eae5":"clf = KNeighborsClassifier(n_neighbors=3)  #defining K-nearest Neighbors(KNN) Classifier\nclf = clf.fit(xtrain,ytrain) \nclf.score(xtest,ytest)","af935b07":"clf = DecisionTreeClassifier(max_depth=3)  #defining DecisionTree Classifier\nclf = clf.fit(xtrain,ytrain) \nclf.score(xtest,ytest)","71b8ea0e":"clf = GradientBoostingClassifier(n_estimators=100,learning_rate=0.1,max_depth=2)  #defining Logistic Regression\nclf = clf.fit(xtrain,ytrain) \nclf.score(xtest,ytest)","b5e31751":"<div style='font-size:18px;color:brown;font-weight:bold'>DecisionTree Classifier :<\/div>\n<p style='font-size:15px'>A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.<\/p>","f231bb67":"<div style='font-size:20px'>Visualization of data set :<\/div>","0ca18cab":"<div style='font-size:20px'>Handling the Nan Values :<\/div>","5d364e28":"<div style='font-size:18px;color:brown;font-weight:bold'>Logistic Regression :<\/div>\n<p style='font-size:15px'>The logistic model is a widely used statistical model that, in its basic form, uses a logistic function to model a binary dependent variable; many more complex extensions exist.<\/p>","5e0a554b":"<div style='font-size:18px;color:brown;font-weight:bold'>GradientBoosting Classifier :<\/div>\n<p style='font-size:15px'>Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.<\/p>","05285252":"<div style='font-size:20px'>Handling Categorical Columns :<\/div>","119e8279":"<div style='font-size:20px'>Feature Selection Process :<\/div>\n<div style='font-size:15px'>There many methods for selecting some of the specific features but, now I am using RandomForest Classifier for feature selection.\n<\/div>\n\n<div style='font-size:20px;padding-top:20px'>\nSteps are :\n<ul style='font-size:15px'>\n<li>Fit training data set to RandomForest Classifier<\/li>\n<li>Use feature_importance to see the importance of each feature for predicting<\/li>\n<li>Then pick few features having high feature_importance value.<\/li>\n<\/ul>\n<\/div>","cd85e099":"<div style='font-size:18px;color:brown;font-weight:bold'>RandomForest Classifier :<\/div>\n<p style='font-size:15px'>Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean prediction of the individual trees. <\/p>","b4bbec11":"<div style='font-size:20px'>Start Analysing :<\/div>","0a2c204f":"<div align='center' style='font-size:25px;color:green;font-weight:bold'>Classification Algorithms<\/div>\n<div style='font-size:16px;padding-top:20px '>\nWe are starting with some simple algorithms to advanced algorithms for classification.And also boosting our skills on the classification problems on real world datasets.\n<\/div>\n\n<div style='padding-top:25px'><p style='font-weight:bold;font-size:17px' >Algorithms I am using are :<\/p>\n<ul style='font-size:15px'>\n<li>Support Vector Classifier<\/li>\n<li>K-nearest Neighbors Classifier<\/li>\n<li>Logistic Regression<\/li>\n<li>DecisionTree Classifier<\/li>\n<li>RandomForest Classifier<\/li>\n<li>GradientBoosting Classifier<\/li>\n<\/ul>\n<\/div>","37ec6433":"<div style='font-size:18px;color:brown;font-weight:bold'>K-nearest Neighbors Classifier :<\/div>\n<p style='font-size:15px'>An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.<\/p>","f7dad763":"<div style='font-size:18px;color:brown;font-weight:bold'>Support Vector Classifier :<\/div>\n<p style='font-size:15px'>Support-Vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.A support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection.<\/p>","97ee2197":"<div style='font-size:20px'>Reading the Dataset :<\/div>"}}