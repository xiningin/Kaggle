{"cell_type":{"cfd150d0":"code","868e9eab":"code","2862723f":"code","f17fc3e9":"code","b779e114":"code","2eaa055d":"code","bad45118":"code","16d0b58b":"code","fb4d6093":"code","c9663ce8":"code","68a8ca21":"code","52f19fd0":"code","59b43f3d":"code","d4616e9c":"code","3802ee8f":"code","a94a93b8":"code","d12877d5":"code","9c28c31e":"code","57438103":"code","7e72b3ee":"code","50118763":"code","3efc8245":"code","01591f56":"code","5de27593":"code","328b9d16":"code","e49b6da5":"code","567905a8":"code","f08622d4":"code","f447df26":"code","f07ec660":"code","0dad3a33":"code","adebd98e":"code","ef4988e5":"code","58190803":"code","3eab32fd":"code","a07100fb":"code","b19fa091":"code","c15bb889":"code","a54286c1":"code","fa5d0eaf":"code","aa5805fd":"code","ad6566fd":"code","51fb1c5e":"code","699191d7":"code","8b69ceae":"code","83ff2688":"code","22eb9cc2":"code","666a8e84":"code","48a55155":"markdown","a32d62ff":"markdown","7efc0f2a":"markdown","11f1dd12":"markdown","137bc0b5":"markdown","0612d7f8":"markdown","eb38cb63":"markdown","72da4fa3":"markdown","1c43477f":"markdown","376ca89a":"markdown","1ca12b80":"markdown","7074c930":"markdown","928b03b0":"markdown","60474762":"markdown","8a97a869":"markdown","faef059c":"markdown","cea0d308":"markdown","45297888":"markdown","c490e78c":"markdown","d891206f":"markdown","3e5ae0ba":"markdown","2649edde":"markdown","02096e75":"markdown","efc20b02":"markdown","fb2ee9d7":"markdown","94d5e523":"markdown","8a75a1b6":"markdown","ce71f0da":"markdown","30f4bdd5":"markdown","33c1701d":"markdown","8f4f3037":"markdown","e262fb4d":"markdown","88c61017":"markdown","f2152644":"markdown","35fd8f22":"markdown","156ff97a":"markdown","5e20b9e0":"markdown","01d2cb29":"markdown","bc87bce7":"markdown","177414b3":"markdown","51418575":"markdown","bc667ac2":"markdown","d684f477":"markdown","b69647be":"markdown","9049f468":"markdown","5fc3ed0b":"markdown","e49cbd6c":"markdown","5503aefc":"markdown","81c81a83":"markdown"},"source":{"cfd150d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","868e9eab":"data=pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","2862723f":"data.head()","f17fc3e9":"data.describe()","b779e114":"data1=data.drop('Outcome',axis=1)\ndata1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))\nplt.show()\n","2eaa055d":"def bar_plot(variable):\n    var =data[variable]\n    varValue = var.value_counts()\n    plt.figure(figsize=(15,7))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    \n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))","bad45118":"data.columns","16d0b58b":"category1 = ['Pregnancies','Age']\n    \nfor c in category1:\n    bar_plot(c)","fb4d6093":"from matplotlib import pyplot\na4_dims = (18, 8)\nfig, ax = pyplot.subplots(figsize=a4_dims)\nsns.countplot(x='Age',hue='Outcome',data=data, linewidth=1,ax=ax)","c9663ce8":"a4_dims = (18, 8)\nfig, ax = pyplot.subplots(figsize=a4_dims)\nsns.countplot(x='Pregnancies',hue='Outcome',data=data, linewidth=1,ax=ax)","68a8ca21":"colors = {0:'#cd1076', 1:'#008080'}\nfig, ax = plt.subplots()\ngrouped = data.groupby('Outcome')\nfor key, group in grouped:\n    group.plot(ax=ax, kind='scatter'\n               ,x='Glucose', y='Age', label=key\n               ,color=colors[key])\nplt.show()","52f19fd0":"colors = {0:'#cd1076', 1:'#008080'}\nfig, ax = plt.subplots()\ngrouped = data.groupby('Outcome')\nfor key, group in grouped:\n    group.plot(ax=ax, kind='scatter'\n               ,x='BMI', y='Age', label=key\n               ,color=colors[key])\nplt.show()","59b43f3d":"data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(9,9))\nplt.show\nvarValue = data.Outcome.value_counts()\nprint(varValue)","d4616e9c":"from sklearn.utils import resample\ndf_majority = data.loc[data.Outcome == 0].copy()\ndf_minority = data.loc[data.Outcome == 1].copy()\ndf_minority_upsampled = resample(df_minority,\n                             replace=True,  # sample with replacement\n                            n_samples=500,  # to match majority class\n                            random_state=123) \ndata = pd.concat([df_majority, df_minority_upsampled])","3802ee8f":"data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(9,9))\nplt.show\nvarValue = data.Outcome.value_counts()\nprint(varValue)","a94a93b8":"data.isnull().sum()\n","d12877d5":"from sklearn.ensemble import IsolationForest\nfrom collections import Counter\nrs=np.random.RandomState(0)\nclf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) \nclf.fit(data)\ny_pred_train = clf.predict(data)\nsay\u0131 = Counter(y_pred_train)\nprint(say\u0131)","9c28c31e":"from collections import Counter\ndef detect_outliers(data,features):\n    outlier_indices = []\n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(data[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(data[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","57438103":"data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'])]","7e72b3ee":"data = data.drop([298,349,78,261,193,706,125,177,579,220,715,655,584,618,661,659,45,243], axis=0)\n","50118763":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndata[\"Outcome\"] = data.Outcome\nX = data.drop(\"Outcome\",1)\ny = data[\"Outcome\"]\ndata.head()\nplt.figure(figsize=(15,7))\ncor = data.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()\n\ncor_target = abs(cor[\"Outcome\"]) #absolute value\n#High Correlations\nrelevant_features = cor_target[cor_target>=0.2]\nrelevant_features","3efc8245":"newdata=data.drop(['BloodPressure', 'SkinThickness', 'Insulin','DiabetesPedigreeFunction'],axis=1)","01591f56":"newdata.head()","5de27593":"data=pd.DataFrame(newdata)","328b9d16":"from sklearn.preprocessing import StandardScaler\nX = data.iloc[:, 0:4]\nY = data.iloc[:, 4]\nnd = StandardScaler()\nnd.fit(X)\nX =nd.transform(X)\nprint(Y)","e49b6da5":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import f1_score\nfrom sklearn import model_selection\n                  \nX = data.iloc[:, 0:4]\nY = data.iloc[:, 4]\nX_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.2, random_state = 100)\n\n#to plot a graph\naccuracies ={} \nmeans={}\nrandoms={}\n","567905a8":"from catboost import CatBoostClassifier\ncparams = {'depth':range(1,15),\n          'iterations':[100],\n          'learning_rate':[0.03,0.001,0.1], \n          'l2_leaf_reg':[3,1,5,10,100],\n          'border_count':[32,5,10,50,100,200],\n          \n          }\nmodel = CatBoostClassifier()\nc_randomcv_model=RandomizedSearchCV(estimator=model,param_distributions=cparams, cv=5, scoring='accuracy', n_jobs=-1, verbose=2).fit(X_train,y_train)","f08622d4":"print('Catboost_randomcv_model accuracy = {}'.format(c_randomcv_model.best_score_))\nrandom=c_randomcv_model.best_score_*100\nrandoms['Catboost']=random","f447df26":"kfold=model_selection.KFold(n_splits=5)\nmodelL=CatBoostClassifier(learning_rate=0.03,l2_leaf_reg=3,iterations=100,depth=12,border_count=32)\nresults=model_selection.cross_val_score(modelL,X,Y,cv=kfold)\n","f07ec660":"print(results)\nprint(results.mean()*100)\nmean=results.mean()*100\nmeans['Catboost']=mean","0dad3a33":"from xgboost import XGBClassifier\naccuracy = []\nfor n in range(1,11):\n    xgb =XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.78,\n                           colsample_bytree=1, max_depth=n)\n    xgb.fit(X_train,y_train)\n    prediction = xgb.predict(X_test)\n    accuracy.append(accuracy_score(y_test, prediction))\nprint(accuracy)    \nplt.plot(range(1,11), accuracy,color='#cd5555')\nplt.xlabel('Max_depth')\nplt.ylabel('Accuracy')\nplt.show()    ","adebd98e":"from sklearn.model_selection import RandomizedSearchCV\nxgb_params = {\n    'learning_rate' : [0.08, 0.06, 0.04, 0.09],      \n    'max_depth': range(1,40),\n    'n_estimators': [100, 200, 300,500,1000]}\nxgb =XGBClassifier()\nxgb_randomcv_model=RandomizedSearchCV(estimator=xgb, param_distributions=xgb_params, n_iter=2, cv=5, scoring='accuracy', n_jobs=-1, verbose=2).fit(X_train,y_train)\nprint(xgb_randomcv_model.best_params_)\nprint('xgb_randomcv_model accuracy = {}'.format(xgb_randomcv_model.best_score_))\nrandom=xgb_randomcv_model.best_score_*100\nrandoms['XGBoost']=random","ef4988e5":"from sklearn.model_selection import GridSearchCV\nxgb_params = { 'learning_rate' : [0.08, 0.06, 0.04, 0.09],      \n    'max_depth': range(1,40),\n    'n_estimators': [100, 200, 300,500,1000]}\nxgb =XGBClassifier()\nxgb_gridcv_model = GridSearchCV(estimator=xgb, param_grid=xgb_params, cv=5, scoring='accuracy', n_jobs=-1, verbose=2).fit(X_train,y_train)\nprint(xgb_gridcv_model.best_params_)\nprint('rf gridcv model accuracy score = {}'.format(xgb_gridcv_model.best_score_))\nacc=xgb_gridcv_model.best_score_ *100\naccuracies[' XGBoost Gridsearch']=acc\n","58190803":"kfold=model_selection.KFold(n_splits=5)\nmodelL=XGBClassifier(n_estimators=100, max_depth=11,learning_rate=0.09)\nresults=model_selection.cross_val_score(modelL,X,Y,cv=kfold)\nprint(results)\nprint(results.mean()*100)\nmean=results.mean()*100\nmeans['XGBoost']=mean","3eab32fd":"from sklearn.ensemble import RandomForestClassifier\nrf_params = {\n   'max_depth': range(1,40),\n    'max_features': range(1,40),\n    'min_samples_leaf': range(1,20),\n    'min_samples_split': range(1,20),\n    'n_estimators': [100, 200, 300,500,1000]}\nrf=RandomForestClassifier()\nrf_randomcv_model=RandomizedSearchCV(estimator=rf, param_distributions=rf_params, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, verbose=2).fit(X_train,y_train)\nprint(rf_randomcv_model.best_params_)\nprint('rf_randomcv_model accuracy score = {}'.format(rf_randomcv_model.best_score_))\nrandom=rf_randomcv_model.best_score_*100\nrandoms['Random Forest']=random","a07100fb":"kfold=model_selection.KFold(n_splits=5)\nmodelL=RandomForestClassifier(n_estimators=100,min_samples_split=14, min_samples_leaf=4,max_depth=7)\nresults=model_selection.cross_val_score(modelL,X,Y,cv=kfold)\nprint(results)\nprint(results.mean()*100)\nmean=results.mean()*100\nmeans['Random Forest']=mean","b19fa091":"from sklearn.tree import DecisionTreeClassifier\ndt_params = {'min_weight_fraction_leaf' : [0.0 , 0.2 , 0.4 , 0.6 ,0.8],\n   'max_depth': range(1,40),\n    'max_features': range(1,40),\n    'min_samples_leaf': range(1,40),\n    'max_leaf_nodes' : range(1,40)\n    \n    }\ndt=DecisionTreeClassifier()\ndt_randomcv_model=RandomizedSearchCV(estimator=dt, param_distributions=dt_params, n_iter=100, cv=5, scoring='accuracy', n_jobs=-1, verbose=2).fit(X_train,y_train)\nprint(dt_randomcv_model.best_params_)\nprint('rf_randomcv_model accuracy score = {}'.format(dt_randomcv_model.best_score_))\nrandom=dt_randomcv_model.best_score_*100\nrandoms['Decision Tree']=random","c15bb889":"kfold=model_selection.KFold(n_splits=5)\nmodelL=DecisionTreeClassifier(min_weight_fraction_leaf=0.0,max_features=3, min_samples_leaf=15,max_depth=7,max_leaf_nodes=28)\nresults=model_selection.cross_val_score(modelL,X,Y,cv=kfold)\nprint(results)\nprint(results.mean()*100)\nmean=results.mean()*100\nmeans['Decision Tree']=mean","a54286c1":"from sklearn.neighbors import KNeighborsClassifier\nknn_params = {'n_neighbors' : range(1,10)\n   }\nknn=KNeighborsClassifier()\nknn_randomcv_model=RandomizedSearchCV(estimator=knn, param_distributions=knn_params, n_iter=100, cv=5, scoring='accuracy', n_jobs=-1, verbose=2).fit(X_train,y_train)\nprint(knn_randomcv_model.best_params_)\nprint('rf_randomcv_model accuracy score = {}'.format(knn_randomcv_model.best_score_))\nrandom=knn_randomcv_model.best_score_*100\nrandoms['KNN']=random","fa5d0eaf":"from sklearn.neighbors import KNeighborsClassifier\nknn_params = {'n_neighbors' : range(1,10),\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto','ball_tree','kd_tree','brute'],\n              'p' : [1,2]\n   }\nknn=KNeighborsClassifier()\nknn_gridcv_model=GridSearchCV(estimator=knn, param_grid=knn_params, cv=5, scoring='accuracy', n_jobs=-1, verbose=2).fit(X_train,y_train)\nprint(knn_gridcv_model.best_params_)\nprint('rf_randomcv_model accuracy score = {}'.format(knn_gridcv_model.best_score_)) \nacc=knn_gridcv_model.best_score_ *100\naccuracies['KNN Gridsearch']=acc\n","aa5805fd":"kfold=model_selection.KFold(n_splits=5)\nmodelL=KNeighborsClassifier(n_neighbors= 1)\nresults=model_selection.cross_val_score(modelL,X,Y,cv=kfold)\nprint(results)\nprint(results.mean()*100)\nmean=results.mean()*100\nmeans['KNN']=mean","ad6566fd":"from sklearn.linear_model import LogisticRegression\nlr_params = {'penalty' : ['l1','l2', 'elasticnet','none'],\n              'C' : range(1,7),\n            'solver' :['newton-cg','lbfgs','liblinear','sag','saga'],\n             'max_iter' : [100,200],\n             'multi_class' : ['ovr','multinomial']\n   }\nlr=LogisticRegression()\nlr_gridcv_model=GridSearchCV(estimator=lr, param_grid=lr_params, cv=5, scoring='accuracy', n_jobs=-1, verbose=2).fit(X_train,y_train)\nprint(lr_gridcv_model.best_params_)\nprint('rf_gridcv_model accuracy score = {}'.format(lr_gridcv_model.best_score_)) \nrandom=lr_gridcv_model.best_score_*100\nrandoms['Logistic Regression']=random","51fb1c5e":"kfold=model_selection.KFold(n_splits=5)\nmodel=LogisticRegression(C=2,max_iter=100,multi_class='ovr',penalty='l2',solver='liblinear')\nresults=model_selection.cross_val_score(model,X,Y,cv=kfold)\nprint(results)\nprint(results.mean()*100)\nmean=results.mean()*100\nmeans['Logistic Regression']=mean","699191d7":"from sklearn.svm import SVC\nsvc_params= {'C' : [0.1,0.2,0.3,0.001,0.003],\n             'kernel': ['linear','poly','rbf','sigmoid']}\nsvc=SVC()\nsvc_gridcv_model=GridSearchCV(estimator=svc, param_grid=svc_params, cv=5, scoring='accuracy', n_jobs=-1, verbose=2).fit(X_train,y_train)\nprint(svc_gridcv_model.best_params_)\nprint('rf_gridcv_model accuracy score = {}'.format(svc_gridcv_model.best_score_)) \nacc=svc_gridcv_model.best_score_ *100\naccuracies['SVC Gridsearch']=acc","8b69ceae":"kfold=model_selection.KFold(n_splits=5)\nmodel=SVC(C=0.1,kernel='linear')\nresults=model_selection.cross_val_score(model,X,Y,cv=kfold)\nprint(results)\nprint(results.mean()*100)\nmean=results.mean()*100\nmeans['SVC']=mean","83ff2688":"colors = [\"#C06C84\", \"#5E1742\", \"#005D8E\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,101,5))\nplt.ylabel(\"GridSearch Scores%\")\nplt.xlabel(\"\\n\\n Algorithms\")\nsns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","22eb9cc2":"colors = [\"#00008b\", \"#00e5ee\", \"#cd1076\", \"#008080\",\"#cd5555\",'black']\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,101,5))\nplt.ylabel(\"Random Search Scores %\")\nplt.xlabel(\"\\n\\n Algorithms\")\nsns.barplot(x=list(randoms.keys()), y=list(randoms.values()), palette=colors)\nplt.show()","666a8e84":"colors = [\"#C06C84\", \"#5E1742\", \"#005D8E\", \"#00ADB5\",\"#3E606F\",\"#EFAB1F\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,101,5))\nplt.ylabel(\"Cross Validation Scores %\")\nplt.xlabel(\"\\n\\n Algorithms\")\nsns.barplot(x=list(means.keys()), y=list(means.values()), palette=colors)\nplt.show()\n","48a55155":"### GridSearchCV","a32d62ff":"<a id = \"15\"><\/a><br>\n# Logistic Regression\n1. Grid Search CV\n1. Cross Validation","7efc0f2a":"# Current Version:","11f1dd12":"<a id = \"5\"><\/a><br>\n# Outlier Detection\n* Isolation Forest\n* Interquartile Range (IQR)","137bc0b5":"### Cross Validation","0612d7f8":"<a id = \"11\"><\/a><br>\n# XGBOOST Classifier\n1. Manual Tuning\n1. Randomized Search CV\n1. Grid Search CV\n1. Cross Validation","eb38cb63":"<a id = \"3\"><\/a><br>\n# And the distribution of our target column","72da4fa3":"### GridSearchCV","1c43477f":"### Cross Validation","376ca89a":"## When we compare the minimum and maximum values with the average, it is understood that we have some outliers. Let's see them in boxplot.","1ca12b80":"![original.jpeg](attachment:original.jpeg)","7074c930":"<font color = '#F0C243'>\nContent:\n    \n1. [Load and check data](#1)\n1. [Analysis of variables](#2)\n1. [Sampling](#3)    \n1. [Missing Value](#4)     \n1. [Outlier Detection](#5)\n    *           [Isolation Forest](#6)\n    *           [Interquartile Range(IQR)](#7)\n1. [Feature Selection](#8) \n1. [Normalization](#9)\n1. [Models](#30)    \n    *           [CatBoost Classifier](#10)\n    *           [XGBoost Classifier](#11)\n    *           [Random Forest](#12) \n    *           [Decision Tree](#13)\n    *           [KNN](#14)\n    *           [Logistic Regression](#15)\n    *           [Support Vector Classifier](#16)\n1. [Comparisons](#17)\n    ","928b03b0":"## GridSearch CV","60474762":"### Cross Validation","8a97a869":"<a id = \"2\"><\/a><br>\n# Analysis of variables","faef059c":"# Pima Indians Diabetes ","cea0d308":"### RandomizedSearchCV\n1. Randomized Search CV\n1. Cross Validation","45297888":"<a id = \"30\"><\/a><br>\n# Models","c490e78c":"### RandomizedSearchCV","d891206f":"<a id = \"10\"><\/a><br>\n# CatBoost Classifier\n### CatBoost is a machine learning algorithm that uses gradient boosting on decision trees.\n1. RandomCV\n1. Cross Validation","3e5ae0ba":"<a id = \"9\"><\/a><br>\n# Normalization","2649edde":"### RandomizedSearchCV","02096e75":"<a id = \"13\"><\/a><br>\n# Decision Tree\n1. Randomized Search CV\n1. Cross Validation","efc20b02":"### RandomizedSearchCV","fb2ee9d7":"### Cross Validation","94d5e523":"<a id = \"4\"><\/a><br>\n# Missing Value","8a75a1b6":"<a id = \"8\"><\/a><br>\n# Feature Selection","ce71f0da":"### Cross Validation","30f4bdd5":"## No missing value.","33c1701d":"<a id = \"14\"><\/a><br>\n# KNN\n1. RandomizedSearch CV\n1. GridSearch CV\n1. Cross Validation","8f4f3037":"### Gridsearch CV","e262fb4d":"### RandomizedSearchCV","88c61017":"### Cross Validation","f2152644":"# We drop the rows with outliers.","35fd8f22":"![causes-of-diabetes.jfif](attachment:causes-of-diabetes.jfif)","156ff97a":"<a id = \"17\"><\/a><br>\n# Comparisons","5e20b9e0":"## RandomSearch CV","01d2cb29":"<a id = \"12\"><\/a><br>\n# Random Forest ","bc87bce7":"### Manual Tuning","177414b3":"### RandomizedSearchCV","51418575":"<a id = \"1\"><\/a><br>\n# Load and Check Data","bc667ac2":"<a id = \"7\"><\/a><br>\n# Interquartile Range (IQR)","d684f477":"<a id = \"6\"><\/a><br>\n# Isolation Forest\n## -1= Number of Outliers","b69647be":"### Cross Validation","9049f468":"## Our dataset does not seem too balance. Let's balance it with Upsampling method","5fc3ed0b":"### GridSearchCV","e49cbd6c":"## Cross Validation","5503aefc":"## Let's look at the distribution of variables according to the target.","81c81a83":"<a id = \"16\"><\/a><br>\n# Support Vector Classifier\n1. Grid Search CV\n1. Cross Validation"}}