{"cell_type":{"875e4cd2":"code","450b502a":"code","07b3ae3e":"code","0dafedaa":"code","a9e66964":"code","c23b7e83":"code","bbb85e91":"code","37851ab7":"code","d41a0403":"code","f6594bd4":"code","7978109a":"code","0bf00a86":"code","ceaf536f":"code","07202f32":"code","d474d090":"code","5ac98ea6":"code","1eaa69d2":"code","7301dd21":"code","436c526d":"code","67f728d1":"code","c0a3f310":"code","f798f531":"code","019a1c90":"code","1821cff5":"code","db83a85e":"code","a07de7a3":"code","c8cdff09":"code","420577f1":"code","8b0aabd4":"code","753a31f9":"code","184807a5":"code","a9f3db66":"code","cc2474c2":"code","e526aea3":"code","a8d3d034":"markdown","4330fca9":"markdown","57fbbb17":"markdown","97d1ed59":"markdown","aac1c4fc":"markdown","7d60a63e":"markdown","f8c529ad":"markdown","84201e4f":"markdown","8bdb1da1":"markdown","97fa5012":"markdown","6e36c8e2":"markdown","c5d1bbc9":"markdown","c1d4b973":"markdown","f1ca330d":"markdown","73667692":"markdown","acd7d1cb":"markdown","4bcbc4f4":"markdown","4ac2b345":"markdown","5442a74f":"markdown","a51fbac9":"markdown","435b7413":"markdown","eca3645e":"markdown","4b43323e":"markdown"},"source":{"875e4cd2":"# Ignore Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","450b502a":"# Import Main Packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Import Main Packages For Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n","07b3ae3e":"# Show Our Fils Data\nimport os\nprint(os.listdir(\"..\/input\"))","0dafedaa":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nsub = pd.read_csv(\"..\/input\/sample_submission.csv\")\n\nprint(\"Data are Ready!!\")","a9e66964":"train_df.head()","c23b7e83":"if (train_df.isnull().values.any() == False):\n    print(\"No Missing Data\")\nelse:\n    train_df.isnull().sum()","bbb85e91":"print(\"Train Data Size {}\\nTest Data Size {}\".format(train_df.shape, test_df.shape))","37851ab7":"# Show Labels Values\ntrain_df['target'].value_counts()","d41a0403":"# Describe 0 Value\ntrain_df[train_df.target == 0].describe()","f6594bd4":"# Describe 1 Value\ntrain_df[train_df.target == 1].describe()","7978109a":"features = train_df.drop(['ID_code', 'target'], axis=1)\nlabel = train_df['target']","0bf00a86":"# EDA\nfig, ax = plt.subplots(1, 2, figsize=(20, 8))\n\nsns.countplot(label, ax=ax[0])\nsns.violinplot(x=label.values, y=label.index.values, ax=ax[1])","ceaf536f":"trn_corr = features.corr()\ntrn_corr = trn_corr.values.flatten()\ntrn_corr = trn_corr[trn_corr != 1]\n\nplt.figure(figsize=(20, 8))\nsns.distplot(trn_corr, color=\"Green\", label=\"train\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend();","07202f32":"train_correlations = train_df.drop([\"target\"], axis=1).corr()\ntrain_correlations = train_correlations.values.flatten()\ntrain_correlations = train_correlations[train_correlations != 1]\n\ntest_correlations = test_df.corr()\ntest_correlations = test_correlations.values.flatten()\ntest_correlations = test_correlations[test_correlations != 1]\n\nplt.figure(figsize=(20,8))\nsns.distplot(train_correlations, color=\"Red\", label=\"train\")\nsns.distplot(test_correlations, color=\"Green\", label=\"test\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend();","d474d090":"# Import Models\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, ComplementNB\n\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.model_selection import train_test_split # You Can Comment It\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","5ac98ea6":"X = features.values.astype('float64')\ny = label.values.astype('float64')","1eaa69d2":"X_train, X_test, y_train,  y_test = train_test_split(X, y, test_size=0.5, random_state=42)","7301dd21":"model = GaussianNB() # Set Model\nmodel.fit(X, y) # Fit Features and labels\n\ny_pred = model.predict(X_test)","436c526d":"accuracy_score(y_test, y_pred)","67f728d1":"plt.figure(figsize=(12, 8))\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True)\nplt.xlabel('true label')\nplt.ylabel('predicted label');\nplt.title(\"Confusion Matrix\");","c0a3f310":"print(classification_report(y_test, y_pred))","f798f531":"from sklearn.metrics import roc_curve, auc\n\nfpr, tpr, thr = roc_curve(y, model.predict_proba(X)[:,1])\nplt.figure(figsize=(12, 8))\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Plot')\nauc(fpr, tpr) * 100","019a1c90":"from sklearn.pipeline import make_pipeline # Import pipeline\nfrom sklearn.preprocessing import QuantileTransformer # For Processing Data.\n\n# Set Model\npipeline = make_pipeline(QuantileTransformer(output_distribution='normal'), GaussianNB())\npipeline.fit(X, y)","1821cff5":"p_pred = pipeline.predict(X_test)","db83a85e":"accuracy_score(y_test, p_pred)","a07de7a3":"print(classification_report(y_test, p_pred))","c8cdff09":"from sklearn.metrics import roc_curve, auc\n\nfpr, tpr, thr = roc_curve(y, pipeline.predict_proba(X)[:,1])\nplt.figure(figsize=(12, 8))\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Plot')\nauc(fpr, tpr) * 100","420577f1":"# Show Test File\ntest_df.head()","8b0aabd4":"# Drop Unused columns\nx_test = test_df.drop(['ID_code'], 1).values","753a31f9":"gnb_pred = model.predict_proba(x_test)[:, 1] # GaussianNB => gnb\npip_pred = pipeline.predict_proba(x_test)[:, 1] # Pipeline => pip","184807a5":"mean_pred = (gnb_pred + pip_pred) \/ 2.0","a9f3db66":"sub.head()","cc2474c2":"sub['target'] = gnb_pred\nsub.to_csv('gnb_submission.csv', index=False)\nsub.head()","e526aea3":"sub['target'] = pip_pred\nsub.to_csv('pip_submission.csv', index=False)\nsub.head()","a8d3d034":"<h1>Submission Our Files<\/h1>","4330fca9":"Use Pipeline Algo. For Getting better accuracy.","57fbbb17":"<h1><center><font size=\"6\">Santander Customer Transaction Prediction<\/font><\/center><\/h1>\n<h1><center><font size=\"5\">Can you identify who will make a transaction?<\/font><\/center><\/h1>\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/4\/4a\/Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg\/640px-Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg\" width=\"500\"><\/img>\n\n<br>\n\n<b>\n    \nOur data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\n\nIn this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.\n\nThe data is anonimyzed, each row containing 200 numerical values identified just with a number.<\/b>","97d1ed59":"**Then We Have 200k Rows And 202 Features.**","aac1c4fc":"**Find Receiver Operating Characteristic**","7d60a63e":"**Ok, After All Of That We Get 1% more accuracy than GaussianNB**\n<br>\n<h1>Good<\/h1>\n<hr>","f8c529ad":"<h2>Check For Any Missing Data.<h2>","84201e4f":"<hr>\n<h1><center>Models<\/center><\/h1> ","8bdb1da1":"Answer On Question On The Second Line.","97fa5012":"Linear correlations\n<br>\nI have already seen some correlation heatmaps in public kernels and it seems as if there is almost no correlation between features. Let's check this out by computing all correlation values and plotting the overall distribution:","6e36c8e2":"<h1 style=\"color:blue;\">Loading packages<\/h1>","c5d1bbc9":"**First**, We Will Use Simple Gaussian Naive Bayes Model.\n<br>\n**Then** , We Will Use Simple Pipeline With Gaussian Naive Bayes Model.","c1d4b973":"<h2 style='color:red'>Split Training Data To Features And Label.<\/h2>","f1ca330d":"**GaussianNB Submission File**","73667692":"<h2>Set Our Gaussian Naive Bayes Model<\/h2>","acd7d1cb":"<h1><\/h1>","4bcbc4f4":"**Pipeline Submission File**","4ac2b345":"**Agian Set Our Features And Labels**","5442a74f":"**After Show Labels Values Describe It**","a51fbac9":"<center><h1 style='color:blue'>Thanks For Watching, Hope You Benefit From This Kernel.<\/h1>\n<h2 style='color:red'>I Would Be So Glad To Answer Your Questions In Comments.<\/h2><\/center>","435b7413":"**Find Correlations Between Train And Test Features.**","eca3645e":"<h1><center>**Let's Identify Who Will Make A Transaction.**<\/center><\/h1>","4b43323e":"**Now, Predict x_test File By Two Ways**"}}