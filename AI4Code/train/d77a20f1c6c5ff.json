{"cell_type":{"59f3b498":"code","26384afc":"code","a9c4445f":"code","45125f7f":"code","e53c23a3":"code","a5155e4d":"code","dc6a10c6":"code","fe5c7cd4":"code","a26303bb":"code","ffb61722":"code","8620f704":"code","f6ec64b1":"code","55aa034d":"code","a3804a2c":"code","46fc9df9":"code","c12eb78a":"code","d187b50d":"code","536ca4fc":"code","85319ff0":"code","b3c644e8":"code","939470ec":"code","40a6e58f":"code","b41707bf":"code","29dbd295":"code","d9f44d94":"markdown","83b8b8d1":"markdown","42f9593f":"markdown","871014db":"markdown","421f65ef":"markdown","1f07c6d9":"markdown","db058255":"markdown","59992d41":"markdown","056308d7":"markdown","1a25fcfa":"markdown","51bbe658":"markdown","8df62dd5":"markdown","b6693f55":"markdown","41883b05":"markdown","1d1d3dbb":"markdown","da7bc1cf":"markdown","09da4c7a":"markdown","3a3d0fce":"markdown","2aba57e6":"markdown","719a4c18":"markdown","688747e6":"markdown","fbe2d7f8":"markdown","7e0e5af8":"markdown","347ac1a5":"markdown","df2c4346":"markdown","6921c061":"markdown","2ea9b84b":"markdown","653d67bb":"markdown","634bed5c":"markdown"},"source":{"59f3b498":"import numpy as np\nimport pandas as pd\nimport time\n\nimport tensorflow as tf\nfrom sklearn.metrics import mean_absolute_error\n\nimport matplotlib.pyplot as plt\n\n# Map plotting library\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n\n# Input data files are available in the \"..\/input\/\" directory.\n# Any results you write to the current directory are saved as output.","26384afc":"param = 't'  # parameter to study, here the temperature \nzone = 'NW'  # zone to study \npath = '\/kaggle\/input\/meteonet\/NW_Ground_Stations\/NW_Ground_Stations\/NW_Ground_Stations_'  # path to the data \ncols = ['number_sta','lat','lon','date',param]    # columns we need in the array\n\ndf = pd.concat([pd.read_csv(path + '2016.csv',usecols = cols, parse_dates=['date'],infer_datetime_format=True),\n                pd.read_csv(path + '2017.csv',usecols = cols, parse_dates=['date'],infer_datetime_format=True),\n                pd.read_csv(path + '2018.csv',usecols = cols,parse_dates=['date'],infer_datetime_format=True)], axis=0)","a9c4445f":"display(df.head())\ndisplay(df.tail())","45125f7f":"for id_sta in np.unique(df['number_sta'])[0:3]:\n    print('Station number:',id_sta)    \n    uni_data = df[(df['number_sta'] == id_sta)][{param,'date'}]\n    uni_data = uni_data[uni_data[param].notnull()]\n    if (uni_data.empty == False):\n        t = [t.year for t in uni_data['date']]\n        print('Parameter ',param,' available for the following years : ',np.unique(t))\n    else:\n        print('Param ',param,' missing') ","e53c23a3":"date = '2016-01-01T06:00:00'\nd_sub = df[df['date'] == date]\nplt.figure()\nplt.scatter(d_sub['lon'], d_sub['lat'], c=d_sub[param], cmap='jet')\nplt.xlabel('Longitude (\u00b0E)')\nplt.ylabel('Latitude (\u00b0N)')\nplt.title(date)\nplt.colorbar().set_label('Temperature (K)')\nplt.show()","a5155e4d":"# Coordinates of studied area boundaries (in \u00b0N and \u00b0E)\nlllat = 46.25  #lower left latitude\nurlat = 51.896  #upper right latitude\nlllon = -5.842  #lower left longitude\nurlon = 2  #upper right longitude\nextent = [lllon, urlon, lllat, urlat]\n\nfig = plt.figure(figsize=(9,5))\n\n# Select projection\nax = plt.axes(projection=ccrs.PlateCarree())\n\n# Plot the data\nplt.scatter(d_sub['lon'], d_sub['lat'], c=d_sub[param], cmap='jet')\nplt.colorbar().set_label('Temperature (K)')\nplt.title(date)\n\nax.coastlines(resolution='50m', linewidth=1)\nax.add_feature(cfeature.BORDERS.with_scale('50m'))\n\n# Adjust the plot to the area we defined \n#\/!\\# this line causes a bug of the kaggle notebook and clears all the memory. That is why this line is commented and so\n# the plot is not completely adjusted to the data\n#ax.set_extent(extent)\n\nplt.show()","dc6a10c6":"number_sta = 29277001\nuni_data = df[(df['number_sta'] == number_sta)][param]\nuni_data.index = df[(df['number_sta'] == number_sta)]['date']\n\nplt.figure(figsize=(10,5))\nplt.ylabel('Temperature (K)')\nplt.title('Temperature evolution of the station '+str(number_sta))\nuni_data.plot(subplots=True)\nplt.show()","fe5c7cd4":"#replace nan values by the mean \nuni_data_mean = np.nanmean(uni_data)\nuni_data = np.nan_to_num(uni_data,nan=uni_data_mean)","a26303bb":"coeff_train = 0.7  #proportion of the dataset in the training set\nTRAIN_SPLIT = round(uni_data.shape[0]*coeff_train)","ffb61722":"def univariate_data(x_data, start_index, end_index,input_size ,target_size):\n    data = []\n    labels = []\n    \n    start_index = start_index + input_size\n    if end_index is None:\n        end_index = len(x_data) - target_size\n\n    for i in range(start_index, end_index):\n        # Reshape data from (input_size,) to (input_size, 1)\n        data.append(np.expand_dims(x_data[(i-input_size):i], axis=1))\n        labels.append(np.max(x_data[i:i+target_size]))\n    return np.array(data), np.array(labels)","8620f704":"univariate_past_history = 240\nunivariate_future_target = 240\n\nx_train,y_train = univariate_data(uni_data, 0,TRAIN_SPLIT,univariate_past_history,univariate_future_target)\nx_val,y_val = univariate_data(uni_data, TRAIN_SPLIT, None,univariate_past_history,univariate_future_target)","f6ec64b1":"x_train_mean = np.mean(x_train)\nx_train_std = np.std(x_train)\n\ny_train_mean = np.mean(y_train)\ny_train_std = np.std(y_train)\n\n#Let's standardize the data:\nx_data_train = (x_train-x_train_mean)\/x_train_std\nx_data_val = (x_val-x_train_mean)\/x_train_std\ny_data_train = (y_train-y_train_mean)\/y_train_std\ny_data_val = (y_val-y_train_mean)\/y_train_std","55aa034d":"print ('Single window of past history, first elements')\nprint (x_data_train[0][0:10])\nprint ('\\n Target ',param,' to predict')\nprint (y_data_train[0])","a3804a2c":"def create_time_steps(length):\n  return list(range(-length, 0))","46fc9df9":"def show_plot(ind, plot_data, delta):\n    labels = ['History : x', 'True Future : y']\n    marker = ['.-', 'rx', 'go']\n    time_steps = create_time_steps(plot_data[0].shape[0])\n    if delta:\n        future = delta\n    else:\n        future = 0\n\n    plt.figure()  \n    plt.title('Sample example '+str(ind))\n    for i, x in enumerate(plot_data):\n        if i:\n            plt.plot(future, plot_data[i], marker[i], markersize=10,\n               label=labels[i])\n        else:\n            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n    plt.legend()\n    plt.xlim([time_steps[0], (future+5)*2])\n    plt.xlabel('Time-Step')\n    plt.ylabel('Standardized temperature')\n    plt.show()\n\nind = 0\nshow_plot(ind,[x_data_train[ind], y_data_train[ind]], 0)","c12eb78a":"def baseline(history):\n    return np.max(history)","d187b50d":"def show_baseline(ind, plot_data, delta):\n    labels = ['History : x', 'True Future : y','Baseline : Max(History)']\n    marker = ['.-', 'rx', 'go']\n    time_steps = create_time_steps(plot_data[0].shape[0])\n    if delta:\n        future = delta\n    else:\n        future = 0\n\n    plt.figure()  \n    plt.title('Sample example '+str(ind))\n    for i, x in enumerate(plot_data):\n        if i:\n            plt.plot(future, plot_data[i], marker[i], markersize=10,\n               label=labels[i])\n        else:\n            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n    plt.legend()\n    plt.xlim([time_steps[0], (future+5)*2])\n    plt.xlabel('Time-Step')\n    plt.ylabel('Standardized temperature')\n    plt.show()\n\nind = 240\nshow_baseline(ind,[x_data_train[ind], y_data_train[ind], baseline(x_data_train[ind])], 0)","536ca4fc":"tf.random.set_seed(13)","85319ff0":"BATCH_SIZE = 256\nBUFFER_SIZE = 10000\n\ntrain_univariate = tf.data.Dataset.from_tensor_slices((x_data_train, y_data_train))\ntrain_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n\nval_univariate = tf.data.Dataset.from_tensor_slices((x_data_val, y_data_val))\nval_univariate = val_univariate.batch(BATCH_SIZE).repeat()","b3c644e8":"simple_lstm_model = tf.keras.models.Sequential([\n    tf.keras.layers.LSTM(8, input_shape=x_data_train.shape[-2:]),\n    tf.keras.layers.Dense(1)\n])\n\nsimple_lstm_model.compile(optimizer='adam', loss='mae')","939470ec":"for x,y in val_univariate.take(1):\n    print(simple_lstm_model.predict(x).shape)","40a6e58f":"EPOCHS = 5\nSTEPS_PER_EPOCH = (x_data_train.shape[0]\/BATCH_SIZE\/\/1)+1\nVAL_STEPS = (x_data_val.shape[0]\/BATCH_SIZE\/\/1)+1\n\nsimple_lstm_model.fit(train_univariate, epochs=EPOCHS,\n                      steps_per_epoch=STEPS_PER_EPOCH,\n                      validation_data=val_univariate, validation_steps = VAL_STEPS)","b41707bf":"def show_result(index, plot_data, delta):\n    labels = ['History : x', 'True Future : y', 'Prediction','Baseline : Max(History)']\n    marker = ['.-', 'rx', 'go','go']\n    colors = ['blue','red','green','darkblue']\n    time_steps = create_time_steps(plot_data[0].shape[0])\n    if delta:\n        future = delta\n    else:\n        future = 0\n\n    plt.figure()  \n    plt.title('Sample example '+str(ind))\n    for i, x in enumerate(plot_data):\n        if i:\n            plt.plot(future, plot_data[i], marker[i], markersize=10,\n               label=labels[i], color = colors[i])\n        else:\n            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i], color = colors[i])\n    plt.legend()\n    plt.xlim([time_steps[0], (future+5)*2])\n    plt.xlabel('Time-Step')\n    plt.ylabel('Standardized temperature')\n    plt.show()","29dbd295":"ind = 0\nfor x, y in val_univariate.take(3):\n    y_true=[]\n    y_true.append(y[ind].numpy())\n    basel=[]\n    basel.append(baseline(x[ind].numpy()))\n    print('mae baseline',mean_absolute_error(y_true,basel))\n    print('mae LSTM',mean_absolute_error(y_true,simple_lstm_model.predict(x)[ind]))\n    plot = show_result(ind,[x[ind].numpy(), y[ind].numpy(),simple_lstm_model.predict(x)[ind],baseline(x[ind].numpy())], 0)","d9f44d94":"Let's train the model now. ","83b8b8d1":"A Recurrent Neural Network (RNN) is a type of neural network well-suited to time series data. RNNs process a time series step-by-step, maintaining an internal state summarizing the information they've seen so far. In this notebook, you will use a specialized RNN layer called Long Short Term Memory (LSTM).","42f9593f":"## Quick data exploration\nThis first part will help you explore quickly the observation data we will use in this example. To get more details about the entire observation data, check the *open_ground_stations* notebook, or navigate to https:\/\/meteofrance.github.io\/meteonet\/data\/ground-observations\/. Here, we will use 3 years of observation data for the temperature parameter and measured in the North West quarter of France. ","871014db":"You will see the LSTM requires the input shape of the data it is being given.","421f65ef":"## Baseline\n\nBefore proceeding to train a model, let's first set a simple baseline. Given an input point, the baseline predicts the next point to be the maximum value of the last 24 hours. ","1f07c6d9":"#### Single example of the preprocessed dataset\n\nNow that the data has been created, let's take a look at a single example. The information given to the network is given in blue, and it must predict the value at the red cross.","db058255":"The following visualisation should help you understand how the data is represented after batching:","59992d41":"If you wonder why some data points are in the middle of the sea, they were measured on small islands ;)\n\n### Plot the evolution of the temperature at one station","056308d7":"Let's see if you can beat this baseline using a recurrent neural network.","1a25fcfa":"Let's get a look at our data !","51bbe658":"## Recurrent neural network","8df62dd5":"### Check the data availability \nWe have 3 years of data, but we might need to check that the data is available during this entire period for a given station.  ","b6693f55":"### Training and validation datasets\n\nLet's start by splitting the dataset into a training and a validation dataset. \nIt is also important to rescale the data before training our neural network. Standardization is a common way of doing this rescaling by subtracting the mean and dividing by the standard deviation of the feature.","41883b05":"### Predict using the simple LSTM model\n\nNow that you have trained your simple LSTM, let's try and make a few predictions.","1d1d3dbb":"### Plot the map of temperature for a given date","da7bc1cf":"# Note\n\n<font size=\"4.5\">To use <span style=\"color:blue\">**Cartopy**<\/span>, a library to plot data with basemaps (see cells below), it is necessary to <span style=\"color:red\">activate the internet connection<\/span> of that notebook (in edit mode, you can find on the right column, in the *Settings* section, a row entitled *Internet*, put the slider bar on **on**).  <\/font>","09da4c7a":"Let's add the map to our plot with Cartopy !","3a3d0fce":"Let's now use `tf.data` to shuffle, batch, and cache the dataset.","2aba57e6":"Let's make a sample prediction, to check the output of the model:","719a4c18":"Let's compute the training and validation set:","688747e6":"We can see that the temperature is rising and falling with the different seasons. ","fbe2d7f8":"![](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/images\/time_series.png)","7e0e5af8":"# Univariate time series prediction \n\nThis notebook is an example of how you can use observation data. We will try to forecast a univariate time series, the temperature at one ground station, using Recurrent Neural Networks (RNNs).","347ac1a5":"This looks better than the baseline. \n\nNow that you have seen some basics, let's go ahead, play with the data, propose your own methods ! ","df2c4346":"First we set the seed to ensure the reproducibility of the experiment:","6921c061":"### Loading the data[](http:\/\/)","2ea9b84b":"Let's standardize the data:","653d67bb":"## Time series prediction\n\nLet's come back to our example of univariate time series prediction ! We will start by preprocessing the data. ","634bed5c":"Let's now create the data for the univariate model. Our model will take as input an array of consecutive values of size `input_size` and will use this 'past' information to infer the  maximum 'future' value of the parameter on a given period, which is a new array of size `target_size`.\n\nWe choose that the model will be given the last 24h recorded temperature observations (`input_size`=24x10, the data time step is 6min, so we have 10 observations per hour) and learn to predict the maximum temperature at the next 24 hours (`target_size`=240)."}}