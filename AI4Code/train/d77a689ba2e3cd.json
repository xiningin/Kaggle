{"cell_type":{"2485ba62":"code","2c590b15":"code","42a54e59":"code","099987cb":"code","810768c0":"code","884dd909":"code","0e398ea5":"code","57e32a40":"code","2ab9344b":"code","73d6a09e":"code","1171cca1":"code","80159f30":"code","4fd54983":"code","411223ff":"code","e739f0f0":"code","09ca1dbe":"code","beabbd72":"code","b559fcea":"code","654589e2":"code","bb35cc70":"code","fc5739f2":"code","c2d45bad":"code","95b95c02":"code","5a4f614d":"code","6c278cee":"code","c3a861dd":"code","75e93c89":"code","b256461a":"code","4dd30b5a":"code","ee6ff4d3":"code","8b0fddb5":"code","b11ed8a1":"code","e79ea0e6":"code","a7a9bced":"code","f39f4caa":"code","1dafb8a0":"code","092af0c1":"code","9602e2b9":"code","bfb9c772":"code","68c02c6b":"code","1475bae7":"code","eb4734a9":"code","0642ec65":"code","f564f15d":"code","ad64a754":"code","e93983c8":"code","56e6d3cc":"code","189a5961":"code","613396df":"code","88c891a6":"code","721d67c1":"code","2b4c0ea8":"code","c067132d":"code","ab3d154b":"code","269085cd":"code","13ac0da0":"code","6a7c4608":"code","936a818b":"code","aef150d0":"code","ed0b5b9d":"code","aafecb63":"code","57445b2c":"code","d42bc2ba":"code","139a566c":"code","b6f28c89":"code","00d27f56":"code","a9e5761f":"code","26c194cd":"code","e36c4047":"code","5ffe5fe3":"code","0153e348":"code","0e0dcc65":"code","b2cff9f3":"code","09d7a8fc":"code","562e7181":"code","38e37d3a":"code","cd2c965c":"code","313b9b2f":"code","1d05c092":"code","804b1a82":"code","aa763c32":"code","e53b74ed":"code","967ce9e3":"code","9e4b12a0":"code","58440afb":"code","b9efb606":"markdown","fec4bfbf":"markdown","39fb342d":"markdown","cc75c702":"markdown","085c4c09":"markdown","0f34b393":"markdown","9989ae7e":"markdown","a1f3a8c6":"markdown","8474d0c7":"markdown","aef17757":"markdown","9329b1d3":"markdown","dada3546":"markdown","4a5c33f9":"markdown","c58851b4":"markdown","a1264b7e":"markdown","aabb5c26":"markdown","13bf286d":"markdown","95be6e0b":"markdown","d6cae0d1":"markdown","db0fbe66":"markdown","2a4aec37":"markdown","b1b9ad4e":"markdown"},"source":{"2485ba62":"!pip install tensorflow-addons","2c590b15":"import tensorflow as tf\nprint(tf.__version__)\nprint(tf.config.list_physical_devices('GPU'))\ntf.random.set_seed(142)","42a54e59":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nimport tensorflow_addons as tfa\nfrom tensorflow_addons.metrics import FBetaScore \n\nimport tqdm.notebook as tq\nimport os\nimport logging\nimport warnings\nwarnings.filterwarnings('ignore')\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)","099987cb":"from google.colab import drive\ndrive.mount('\/content\/gdrive\/')","810768c0":"# # Dataset folders are manually downloaded from Kaggle as .tar.7z file\n# # Install 7zip if not in the environment yet \n# ! sudo apt-get install p7zip-full\n\n# # Unzip dataset\n# ! 7z x -so \/content\/gdrive\/MyDrive\/PROJECT\/AMAZON\/data\/train-jpg.tar.7z | tar xf - -C \/content\/gdrive\/MyDrive\/PROJECT\/AMAZON\/data","884dd909":"PROJECT_FOLDER = '\/content\/gdrive\/MyDrive\/PROJECT\/AMAZON'\nDATA_PATH = os.path.join(PROJECT_FOLDER, \"data\")\nTRAIN_JPG_DIR = os.path.join(DATA_PATH, \"train-jpg\")\nTRAIN_CSV_PATH = os.path.join(DATA_PATH, \"train_v2.csv\")","0e398ea5":"df = pd.read_csv(TRAIN_CSV_PATH)\ndf.head(5)","57e32a40":"dummies = df['tags'].str.get_dummies(' ')\ndf = pd.concat([df, dummies], axis=1)\n\nlabels = dummies.columns.values\nN_LABELS = len(labels)\ndummies","2ab9344b":"print(f\"There are {N_LABELS} unique labels including {labels}\")","73d6a09e":"# Countplot of label distribution\nlabel_count = dummies.sum(axis=0).sort_values()\nprint(label_count)","1171cca1":"label_count.plot(kind='barh', figsize=(15, 10))\nfor i in range(label_count.shape[0]):\n    plt.text(label_count.iloc[i] + 4, i, label_count.iloc[i], va='center')","80159f30":"images_title = [df[df['tags'].str.contains(label)].iloc[i]['image_name'] + '.jpg' for i, label in enumerate(labels)]","4fd54983":"_, axs = plt.subplots(5, 4, sharex='col', sharey='row', figsize=(15, 20))\naxs = axs.ravel()\n\nfor i, (image_name, label) in enumerate(zip(images_title, labels)):\n    img_path = os.path.join(TRAIN_JPG_DIR, image_name)\n    img = plt.imread(img_path)\n    axs[i].imshow(img)\n    axs[i].set_title(f'{image_name} - {label}')","411223ff":"!pip install iterative_stratification -q\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","e739f0f0":"y = df[labels].values\nX = df['image_name'].values\n\ndf['fold'] = np.nan\n\nmskf = MultilabelStratifiedKFold(n_splits=5, random_state=104)\nfor i, (_, test_index) in enumerate(mskf.split(X, y)):\n    df.iloc[test_index, -1] = i\n   \ndf['fold'] = df['fold'].astype('int')\ndf['is_valid'] = False\ndf['is_valid'][df['fold'] == 0] = True","09ca1dbe":"# Number of observations of each tags in each fold. \ndf.groupby('fold')[labels].sum()","beabbd72":"TRAIN_SIZE = df.shape[0] - df['is_valid'].sum()\nVAL_SIZE = df['is_valid'].sum()","b559fcea":"# Converting the values into features\n\ndef _image_feature(value):\n    \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value).numpy()]))\n\ndef _int64_feature(value):\n    if type(value) != list:\n        value = [value]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\ndef _bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))): # if value ist tensor\n        value = value.numpy()\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef serialize_array(array):\n    array = tf.io.serialize_tensor(array)\n    return array\n\n\ndef image_feature(path, label):\n    image = plt.imread(path)\n\n    # image = tf.io.decode_jpeg(image, channels=3)\n    feature = {'height': _int64_feature(image.shape[0]),\n               'width': _int64_feature(image.shape[1]),\n               'channel': _int64_feature(image.shape[2]),\n               'image': _bytes_feature(serialize_array(image)),\n               'label': _int64_feature(label),}\n    return tf.train.Example(features=tf.train.Features(feature=feature))\n\n\ndef create_record(df, folder_path, record_name):\n    all_image_paths = df['image_name'].apply(lambda x: os.path.join(TRAIN_JPG_DIR, x+'.jpg')).values\n    all_labels = df[labels].values\n\n    record_path = os.path.join(folder_path, f\"{record_name}.tfrecords\")\n    writer = tf.io.TFRecordWriter(record_path) \n\n    for i in tq.tqdm(range(df.shape[0])):\n        path = all_image_paths[i]\n        label = all_labels[i].tolist()\n        example = image_feature(path, label)\n        writer.write(example.SerializeToString())\n    \n    writer.close()","654589e2":"# for i in range(5): \n#     create_record(df[df['fold'] == i], DATA_PATH, f'fold_{i}')","bb35cc70":"RECORDS = tf.io.gfile.glob(str(DATA_PATH + '\/*.tfrecords'))\nRECORDS","fc5739f2":"IMG_WIDTH, IMG_HEIGHT = 192, 192\nCHANNELS = 3\n\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"height\": tf.io.FixedLenFeature([], tf.int64),\n        \"width\": tf.io.FixedLenFeature([], tf.int64),\n        \"channel\": tf.io.FixedLenFeature([], tf.int64),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"label\": tf.io.FixedLenFeature([17], tf.int64, default_value=np.zeros((17,)).astype('int').tolist())\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n\n    # Extract information\n    height = example['height']\n    width = example['width']\n    channel = example['channel']\n    image = example['image']\n    label = example['label']\n\n    # Convert raw image back to array\n    image = tf.io.parse_tensor(image, out_type=tf.uint8)\n    image = tf.reshape(image, shape=[height, width, channel])\n    if channel == 4:\n        image = image[:,:,:3]\n\n    image = tf.image.resize(image, [IMG_WIDTH, IMG_HEIGHT])\n\n    return (image, label)","c2d45bad":"BATCH_SIZE = 64\nSHUFFLE_BUFFER_SIZE = 1024\nAUTOTUNE = tf.data.experimental.AUTOTUNE","95b95c02":"def augmentation(image, label):\n    image = tf.image.random_brightness(image, .1)\n    image = tf.image.random_contrast(image, lower=0.0, upper=1.0)\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    return image, label\n\ndef load_dataset(filenames, shuffle=False, augment=False):\n    \"\"\"Load a list of pahts of TFRecords \n       and split them into train and validation set.\"\"\"\n\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n\n    # dataset = dataset.cache()\n\n    if shuffle == True:\n        dataset = dataset.shuffle(buffer_size = SHUFFLE_BUFFER_SIZE).repeat()\n\n    if augment == True:\n        dataset.map(augmentation, num_parallel_calls=AUTOTUNE)\n    \n    dataset = dataset.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n    \n    return dataset","5a4f614d":"RECORDS","6c278cee":"train_ds = load_dataset(RECORDS[:4], shuffle=True, augment=True)\nval_ds = load_dataset(RECORDS[4], shuffle=False, augment=False)","c3a861dd":"train_ds","75e93c89":"val_ds","b256461a":"for i in train_ds.take(1):\n    plt.imshow(i[0][1].numpy() \/ 255.)\n    plt.show()","4dd30b5a":"# sample = df[df['fold'] == 0]\n# sample.head()","ee6ff4d3":"# sample[labels].sum() \/ df[labels].sum()","8b0fddb5":"# X_sample = sample['image_name'].apply(lambda x: os.path.join(DATA_PATH, 'train-jpg', x+\".jpg\")).values\n# y_sample = sample[labels].values ","b11ed8a1":"# sample.drop(columns=['fold', 'is_valid'], inplace=True)","e79ea0e6":"# sample['fold'] = np.nan\n\n# mskf = MultilabelStratifiedKFold(n_splits=5, random_state=104)\n# for i, (_, test_index) in enumerate(mskf.split(X_sample, y_sample)):\n#     sample.iloc[test_index, -1] = i\n   \n# sample['fold'] = sample['fold'].astype('int')\n# sample['is_valid'] = False\n# sample['is_valid'][sample['fold'] == 0] = True","a7a9bced":"# sample.groupby('is_valid')[labels].sum()","f39f4caa":"# create_record(sample[sample['is_valid'] == False], DATA_PATH, '1_train_sample')","1dafb8a0":"# create_record(sample[sample['is_valid'] == True], DATA_PATH, '1_val_sample')","092af0c1":"# sample['is_valid'].sum()","9602e2b9":"# TRAIN_SIZE = sample.shape[0] - sample['is_valid'].sum()\n# VAL_SIZE = sample['is_valid'].sum()","bfb9c772":"# train_ds = load_dataset(RECORDS[-2], shuffle=True, augment=True)\n# val_ds = load_dataset(RECORDS[-1])","68c02c6b":"# for i in train_ds.take(1):\n#     plt.imshow(i[0][1].numpy() \/ 255.)\n#     plt.show()","1475bae7":"def build_model(trainable = False, fine_tune_at = 0):\n    \"\"\"Build a Sequential model with the MobileNetv2 as base model and additional top layers.\n       Certain number of layers of the MobileNetv2 can be trained.\n       args:\n           trainable: boolean, whether transfer learning model can be trained or not.\n           fine_tune_at: int, number of trainable layers.\n    \n    \"\"\"\n    mobile_net = tf.keras.applications.MobileNetV2(input_shape=(IMG_WIDTH, IMG_HEIGHT, CHANNELS), include_top=False)\n    if trainable == True:\n        mobile_net.trainable=True\n\n        for layer in mobile_net.layers[:fine_tune_at]:\n            layer.trainable = False\n    else: \n        mobile_net.trainable = False\n    \n\n    input = tf.keras.Input(shape=(IMG_WIDTH, IMG_HEIGHT, CHANNELS), name='input')\n    x = tf.keras.applications.mobilenet.preprocess_input(input)\n    x = mobile_net(x)\n    x = tf.keras.layers.Dense(1024, activation = 'relu')(x)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    output = tf.keras.layers.Dense(N_LABELS, activation = 'sigmoid')(x)\n\n    model = tf.keras.Model(input, output)\n    return model","eb4734a9":"model = build_model()\nmodel.summary()","0642ec65":"for batch in train_ds: \n    print(model.predict(batch[0]))\n    break","f564f15d":"from datetime import datetime\ntoday = str(datetime.now().date())\ntry:\n    os.mkdir(os.path.join(PROJECT_FOLDER, 'log', today))\nexcept:\n    print('Folder exists.')","ad64a754":"# Setting up CheckPoint \ncheckpoint_path = os.path.join(PROJECT_FOLDER, 'log', today, f\"full_1_decay.h5\")\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create a callback that saves the model's weights by default it saves the weights every epoch\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_best_only = True,\n                                                 save_weights_only=True,\n                                                 mornitor = \"val_loss\",\n                                                 verbose=1)\n\n# Learning rate decay\nlr_decay = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                                factor=0.2, \n                                                patience=3, \n                                                verbose=1, \n                                                mode='auto', \n                                                epsilon=0.0001, \n                                                cooldown=0, \n                                                min_lr=0.0000001)\n\n# Logger\nlog_path = os.path.join(PROJECT_FOLDER, 'log', today, f\"full_1_decay.csv\")\nlogger = tf.keras.callbacks.CSVLogger(log_path, separator=',', append=True)","e93983c8":"LR = 1e-5\nEPOCHS = 60\nnum_steps_train = tf.math.ceil(float(TRAIN_SIZE)\/BATCH_SIZE)              \nnum_steps_val = tf.math.ceil(float(VAL_SIZE)\/BATCH_SIZE)\n\nfbeta = FBetaScore(num_classes=N_LABELS,\n                   average='weighted',\n                   beta=2.0,\n                   threshold=0.2,\n                   name='fbeta')","56e6d3cc":"# Compile model with optimizer\nmodel.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = LR),\n               loss = 'binary_crossentropy',\n               metrics = [fbeta, tf.keras.metrics.AUC()])","189a5961":"# model.load_weights(checkpoint_path)","613396df":"# Train model\nhistory = model.fit(train_ds,\n                  steps_per_epoch = num_steps_train,\n                  epochs = EPOCHS,\n                  validation_data = val_ds,\n                  validation_steps = num_steps_val,\n                  callbacks=[cp_callback, lr_decay, logger])","88c891a6":" def plot_stats(training_stats, val_stats, x_label='Training Steps', stats='loss'):\n    stats, x_label = stats.title(), x_label.title()\n    legend_loc = 'upper right' if stats=='loss' else 'lower right'\n    training_steps = len(training_stats)\n    test_steps = len(val_stats)\n\n    plt.figure()\n    plt.ylabel(stats)\n    plt.xlabel(x_label)\n    plt.plot(training_stats, label='Training ' + stats)\n    plt.plot(np.linspace(0, training_steps, test_steps), val_stats, label='Validation ' + stats)\n    plt.ylim([0,max(plt.ylim())])\n    plt.legend(loc=legend_loc)\n    plt.show()","721d67c1":"plt.figure(figsize = (15, 10))\n\nplot_stats(history.history['loss'], history.history['val_loss'], x_label='Epochs', stats='loss')\nplot_stats(history.history['fbeta'], history.history['val_fbeta'], x_label='Epochs', stats='fbeta');","2b4c0ea8":"SAVE_PATH = os.path.join(PROJECT_FOLDER, 'log', today, '1_full_decay.h5')\nmodel.save(SAVE_PATH)","c067132d":"SAVE_PATH = '\/content\/gdrive\/MyDrive\/PROJECT\/AMAZON\/data\/log\/2021-08-23\/1_full_decay.h5'\nmodel = tf.keras.models.load_model(SAVE_PATH)","ab3d154b":"def read_tfrecord_label_only(example):\n    tfrecord_format = {\n        \"height\": tf.io.FixedLenFeature([], tf.int64),\n        \"width\": tf.io.FixedLenFeature([], tf.int64),\n        \"channel\": tf.io.FixedLenFeature([], tf.int64),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"label\": tf.io.FixedLenFeature([17], tf.int64, default_value=np.zeros((17,)).astype('int').tolist())\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n\n    # Extract information\n    height = example['height']\n    width = example['width']\n    channel = example['channel']\n    image = example['image']\n    label = example['label']\n\n    return label","269085cd":"train_label_ds = tf.data.TFRecordDataset(RECORDS[:4])\ntrain_label_ds = train_label_ds.map(read_tfrecord_label_only, num_parallel_calls=AUTOTUNE)","13ac0da0":"true = list(train_label_ds.as_numpy_iterator())\ntrue = np.array(true)\ntrue.shape","6a7c4608":"train_image_ds = load_dataset(RECORDS[:4], shuffle=False, augment=False)","936a818b":"predictions = model.predict(train_image_ds)\nfinal_predictions = (predictions > 0.2).astype('int')\nfinal_predictions.shape","aef150d0":"from sklearn.metrics import fbeta_score\nfbeta_score(true, final_predictions, average='weighted', beta=2)","ed0b5b9d":"from sklearn.metrics import classification_report\nprint(classification_report(true, final_predictions))","aafecb63":"for data in train_image_ds.take(1):\n    sample_images = data[0].numpy().astype('int')\n    sample_labels = data[1].numpy().astype('bool')\n\nsample_images = sample_images[:40]\nsample_labels = sample_labels[:40]\nsample_predictions = final_predictions[:40]","57445b2c":"fig, axes = plt.subplots(10, 4, figsize=(20, 30))\naxes = axes.ravel()\n\nfor i, (image, label) in enumerate(zip(sample_images, sample_labels)):\n    axes[i].imshow(image)\n    predict_label = labels[sample_predictions[i] == 1]\n    predict_label = ', '.join(predict_label)\n    correct = ', '.join(labels[label])\n    axes[i].set_title(f\"PREDICT: {predict_label} \\nCORRECT: {correct}\")\n\nplt.subplots_adjust(wspace=1, hspace=1)\nplt.show()","d42bc2ba":"def perf_grid(y_hat_val, y_val, label_names, n_thresh=100):\n    \n    # Find label frequencies in the validation set\n    label_freq = y_val.sum(axis=0)\n\n    # Define thresholds\n    thresholds = np.linspace(0, 1, n_thresh+1).astype(np.float32)\n    \n    # Compute all metrics for all labels\n    ids, labels, freqs, tps, fps, fns, precisions, recalls, f1s, f2s = [], [], [], [], [], [], [], [], [], []\n    \n    for i in tq.tqdm(range(len(label_names))):\n        for thresh in thresholds:   \n            ids.append(i)\n            labels.append(label_names[i])\n            freqs.append(round(label_freq[i]\/len(y_val),2))\n\n            y = y_val[:, i]\n            y_pred = y_hat_val[:, i] > thresh\n\n            tp = np.count_nonzero(y_pred  * y)\n            fp = np.count_nonzero(y_pred * (1-y))\n            fn = np.count_nonzero((1-y_pred) * y)\n            precision = tp \/ (tp + fp + 1e-16)\n            recall = tp \/ (tp + fn + 1e-16)\n            f1 = 2*tp \/ (2*tp + fn + fp + 1e-16)\n            f2 = fbeta_score(y, y_pred, average='weighted', beta=2)\n            \n            tps.append(tp)\n            fps.append(fp)\n            fns.append(fn)\n            precisions.append(precision)\n            recalls.append(recall)\n            f1s.append(f1)\n            f2s.append(f2)\n            \n    # Create the performance dataframe\n    grid = pd.DataFrame({'id':ids,\n                         'label':labels,\n                         'freq':freqs,\n                         'threshold':list(thresholds)*len(label_names),\n                         'tp':tps,\n                         'fp':fps,\n                         'fn':fns,\n                         'precision':precisions,\n                         'recall':recalls,\n                         'f1':f1s,\n                         'f2': f2s})\n    \n    return grid","139a566c":"predictions.shape","b6f28c89":"true.shape","00d27f56":"# Performance table\ngrid = perf_grid(predictions, true, labels)","a9e5761f":"grid[grid['label'].str.contains('primary')].head(20)","26c194cd":"# Choose the best threshold of \ngrid_max = grid.loc[grid.groupby(['id', 'label'])[['f2']].idxmax()['f2'].values]\ngrid_max","e36c4047":"# # # Dataset folders are manually downloaded from Kaggle as .tar.7z file\n# # # Install 7zip if not in the environment yet \n# ! sudo apt-get install p7zip-full\n\n# # # Unzip dataset\n# ! 7z x -so \/content\/gdrive\/MyDrive\/PROJECT\/AMAZON\/data\/test-jpg-additional.tar.7z | tar xf - -C \/content\/gdrive\/MyDrive\/PROJECT\/AMAZON\/data","5ffe5fe3":"test_paths = tf.io.gfile.glob(str(DATA_PATH + '\/test-jpg\/*.jpg')) \nadditional_paths = tf.io.gfile.glob(str(DATA_PATH + '\/test-jpg-additional\/*.jpg')) ","0153e348":"def create_test_record(data_folder, record_name):\n    paths = tf.io.gfile.glob(str(data_folder + f'\/{record_name}\/*.jpg')) \n\n    record_path = os.path.join(data_folder, f\"{record_name}.tfrecords\")\n    writer = tf.io.TFRecordWriter(record_path) \n\n    for i in tq.tqdm(range(len(paths))):\n        path = paths[i]\n        image = plt.imread(path)\n        feature = {'height': _int64_feature(image.shape[0]),\n                   'width': _int64_feature(image.shape[1]),\n                   'channel': _int64_feature(image.shape[2]),\n                   'image': _bytes_feature(serialize_array(image))}\n        example = tf.train.Example(features=tf.train.Features(feature=feature))\n        writer.write(example.SerializeToString())\n    writer.close()","0e0dcc65":"create_test_record(DATA_PATH, 'test-jpg')\ncreate_test_record(DATA_PATH, 'test-jpg-additional')","b2cff9f3":"def read_test_record(example): \n    tfrecord_format = {\n            \"height\": tf.io.FixedLenFeature([], tf.int64),\n            \"width\": tf.io.FixedLenFeature([], tf.int64),\n            \"channel\": tf.io.FixedLenFeature([], tf.int64),\n            \"image\": tf.io.FixedLenFeature([], tf.string)\n            }\n\n    example = tf.io.parse_single_example(example, tfrecord_format)\n\n    # Extract information\n    height = example['height']\n    width = example['width']\n    channel = example['channel']\n    image = example['image']\n\n    # Convert raw image back to array\n    image = tf.io.parse_tensor(image, out_type=tf.uint8)\n    image = tf.reshape(image, shape=[height, width, channel])\n    if channel == 4:\n        image = image[:,:,:3]\n\n    image = tf.image.resize(image, [IMG_WIDTH, IMG_HEIGHT])\n    return image","09d7a8fc":"def load_test_dataset(filenames, shuffle=False, augment=False):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_test_record, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n    \n    return dataset","562e7181":"RECORDS = tf.io.gfile.glob(str(DATA_PATH + '\/*.tfrecords'))\nRECORDS","38e37d3a":"test_ds = load_test_dataset(RECORDS[-2])\nadditional_ds = load_test_dataset(RECORDS[-1])","cd2c965c":"test_predictions = model.predict(test_ds)\nadditional_predictions = model.predict(additional_ds)","313b9b2f":"threshold = { 'agriculture':0.164,\n          'artisinal_mine':0.114,\n          'bare_ground':0.138,\n          'blooming':0.168,\n          'blow_down':0.2,\n          'clear':0.13,\n          'cloudy':0.076,   \n          'conventional_mine':0.1,\n          'cultivation':0.204,\n          'habitation':0.17,\n          'haze':0.204,\n          'partly_cloudy':0.112,\n          'primary':0.204,\n          'road':0.156,\n          'selective_logging':0.154,\n          'slash_burn':0.38,\n          'water':0.182\n            }\n            \nthresholds = np.fromiter(threshold.values(), dtype=float)","1d05c092":"def get_tag(prediction):\n    return ' '.join(labels[(prediction >= threshold_values)])\n\nfinal_test_predictions = list(map(get_tag, test_predictions))\nfinal_additional_predictions = list(map(get_tag, additional_predictions))","804b1a82":"len(final_test_predictions)","aa763c32":"test_filenames = list(map(lambda x: x.split('\/')[-1][:-4], test_paths))\nadditional_filenames = list(map(lambda x: x.split('\/')[-1][:-4], additional_paths))","e53b74ed":"submission = pd.DataFrame({'image_name': test_filenames, 'tags': final_test_predictions})\nsubmission['count'] = submission['image_name'].str.strip('test_').astype('int')\nsubmission = submission.sort_values('count', ascending=True).reset_index(drop=True)\nsubmission.drop(columns=['count'], inplace=True)\nsubmission","967ce9e3":"submission_2 = pd.DataFrame({'image_name': additional_filenames, 'tags': final_additional_predictions})\nsubmission_2['count'] = submission_2['image_name'].str.strip('file_').astype('int')\nsubmission_2 = submission_2.sort_values('count', ascending=True).reset_index(drop=True)\nsubmission_2.drop(columns=['count'], inplace=True)\nsubmission_2","9e4b12a0":"final_submission = pd.concat([submission, submission_2], axis=0)\nfinal_submission","58440afb":"final_submission.to_csv(PROJECT_FOLDER + \"\/final_submission_2_time.csv\", index=False)","b9efb606":"### 2.3 Export model","fec4bfbf":"### 1.4. Split dataset\n\n- The datatset is divided into 80% train and 20% validation. We also split the train into 4 folds, which later will be store in 4 TFRecords shards. \n- `MultilabelStratifiedKFold` is used to maintain the ratio of label across each shard. ","39fb342d":"# Planet: Understanding the Amazon from Space \ud83c\udf33\ud83e\udd8c\n***By Nhan Phan, November 2019, as an entry to the competition [Planet: Understanding the Amazon from Space](https:\/\/www.kaggle.com\/c\/planet-understanding-the-amazon-from-space\/data) by Kaggle.***\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/6322\/logos\/header.png)\n\nEvery minute, the world loses an area of forest the size of 48 football fields. And deforestation in the Amazon Basin accounts for the largest share, contributing to reduced biodiversity, habitat loss, climate change, and other devastating effects. But better data about the location of deforestation and human encroachment on forests can help governments and local stakeholders respond more quickly and effectively.\n\nThis analysis uses Deep Learning to classify the spatial images of the Amazon forest taken by the satilite. And from that, it hopes to shed a light on understanding how the forest has change naturally and manually. Thus, help preventing deforestation.\n\nThe dataset is acquired from the Kaggle competition in 2016: https:\/\/www.kaggle.com\/c\/planet-understanding-the-amazon-from-space\/data\n\nThe dataset contains more than 40.000 images, taken by Planet using sattelites.\n\n\n\n> Planet, designer and builder of the world\u2019s largest constellation of Earth-imaging satellites, will soon be collecting daily imagery of the entire land surface of the earth at 3-5 meter resolution. While considerable research has been devoted to tracking changes in forests, it typically depends on coarse-resolution imagery from Landsat (30 meter pixels) or MODIS (250 meter pixels). This limits its effectiveness in areas where small-scale deforestation or forest degradation dominate.\n\n<center><img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/6322\/media\/planet.png\" width=300'><\/center>\n\n\n\n### \u2606 **RESULT**\nThe project successfully got the score of 0.90 on the official test set.\n\n|  | THIS PROJECT | WINNER |\n|:--:|:--:|:--:|\n| **Score (F-Beta)** | 0.90 | 0.93 |\n\nTraining information:\n\n|  | Loss | F-Beta Score |\n|:--:|:--:|:--:|\n| **Train** | 0.09 | 0.90 |\n| **Validation** | 0.11 | 0.89 |\n\n\n\n\n### **\u2606 CHALLENGE**\nSeveral key learnings undercovered through the analysis:\n\n1. **Multi-label:** Each image is labeled with multiple tags (at least 2, at max 9). The tags fall into 17 categories, which are the forest landscape types. Since the tags in each label are mutually exclusive, they are treated as multiple binary classification problems. Thus, `binary cross-entropy` are chosen to be the loss function. \n\n2. **Imbalance:** The dataset is severely imbalance with tags like Primary or Agriculture appear in 90% of the dataset. While other tags like Blooming or Conventional Mine can only be seen in less than 500 observations (even less than 100 for Burn Down).\n\n  To tackle the problem of imbalance dataset, evaluation metrics has to be chosen carefully. In the first base-line experiment, the model was totally bias toward the major tags. It predicts the major tags to appear in every data and almost never made a prediction with the minor tags. \n\n  `F2` is chosen to be the main metrics to evaluate the training. It watches over the harmonic mean between the Precision and Recall while favors Recall specifically. In other word, it is the attempt to reduce the number of False Negative, where the model fails to identify the absence of a tag. \n\n3. **Optimization:** 400.000 images, a CNN model, and Google Colab's limited resource do not seem to mix well together. The training was slow at first and interupted often. Several improvements, mostly on the Tensorflow pipeline, were conducted to speed up the training:\n\n  - Using **TFRecord** to convert the raw images into byte-like data to reduce the amount of time spending on reading data from their paths. \n  - Using [**tf.data.Dataset**](https:\/\/www.tensorflow.org\/guide\/data_performance) with `shuffle`, `map`, `batch`, `prefetch` to optimize the reading data process by redistributing the tasks for agents to work concurrently, thus, avoid bottleneck. An attempt to use `cache` was also made but failed due to the limited RAM. \n\n4. **Processing image with Tensorflow:** The dataset contains images in JPG - RGBA. The built-in decode function `tf.io.decode_jpeg` only works on 1 or 3-channel image. Attempt on encoding a JPG RGBA image returns black black and black. We need a tensorflow encoding function to work in this part because the pipeline is built entirely on Tensor for the optimization purpose. \n\n  To tackle the problem, the raw images were first read by Matplotlib then converted into byte-like and wrote into TFRecords. When reading the data from TF Record, instead of using the built-in decode image function, we use `tf.io.parse_tensor` following with reshaping.\n\n\n","cc75c702":"### 1.4. Export raw data to TFRecords","085c4c09":"As we can see, the dataset's labels are not evenly distributed. The `primary` and `clear` tags appear in more than 80% of the dataset while some others, for examples, `blooming`, `slash burn` or `blow_down` are rarely observed.\n\nLet's take a closer look as what these labels visually depict.","0f34b393":"Now, our data is ready to flow into the model.\n\nEach batch will be a pair of arrays (one that holds the features and another that hold labels). \n- The features array will be of shape (BATCH_SIZE, IMG_WIDTH, IMG_HEIGHT, CHANNELS).\n- The labels array will be of shape (BATCH_SIZE, N_LABELS).","9989ae7e":"One-hot encode the labels.","a1f3a8c6":"### \u274a Read sample\n\nFor acquiring a small sample of dataset (20%) and split to train-validation (80\/20) ","8474d0c7":"## 3. TUNING THRESHOLD\n\nSince the multi-label problem is viewed as multiple binary classification tasks, it is necessary to define an appropriate threshold. Any predicted probability above the threshold yield a positive prediction for that tag. The value of the threshold affects the F score greatly, especially in the context of an imbalance dataset, where each label has different frequency.\n\nIn the experiment of tuning threshold, we increase the threshold from 0 to 1 by a step of 0.01 for each label and observe the change of Precision, Recall, F1 and FBeta Score.","aef17757":"## 4. PREDICT ON TEST ","9329b1d3":"### 2.4 Predict","dada3546":"### 2.1. Build model\n\nInstead of building and training a new model from scratch, we will use a pre-trained model in a process called transfer learning. In this case, we use the MobileNetv2 model.\n\n![alt text](https:\/\/miro.medium.com\/max\/1400\/1*yT0lWepQ39hrwn5KBaMz_A.png)","4a5c33f9":"### 1.1 Import Libraries","c58851b4":"## 5. SUMMARY","a1264b7e":"### 2.2. Train\n\nUsing **Checkpoint**, **LearningRateDecay**, and **CSVLogger** to assist the training. ","aabb5c26":"### 1.5 Load data from TFRecord","13bf286d":"## 1. PREPROCESS DATA\n\n","95be6e0b":"Read more about the definition of each label [HERE](https:\/\/www.kaggle.com\/c\/planet-understanding-the-amazon-from-space\/data) ","d6cae0d1":"### 1.2 Explore Dataset","db0fbe66":"In this project, we tackle a 17-label classification problem. We succeed to reach the FBeta Score of 0.90 on the official test dataset. Yet, there are still many puzzles that have not been solved. \n\n- One biggest challenge is the performance of the model on minor class. It is, still, very poor.\n<img src='https:\/\/i.imgur.com\/G8ZXOsb.png' width=500>\n\n- Several techniques can be considered for future tuning:\n  - Attempt on using XGBoost or other ensemble learning techniques.\n  - Using dehaze in preprocessing images to highlight the obscure features. \n  - Exploring on the original `.tiff` dataset instead of the converted JPG.\n\nOtherwise, this is the end of the project \ud83c\udf7a","2a4aec37":"## 2. MODELLING","b1b9ad4e":"Take a look at prediction. The model will return a list of 17 values according to 17 labels. Each value represents the probability that the observation includes that label. "}}