{"cell_type":{"b8dfe85a":"code","3afbc349":"code","8ea41cc0":"code","c441e504":"code","5e85ea33":"code","253b7ffe":"code","2c8c3eda":"code","0e3f0b83":"code","cf85d039":"code","d2033e73":"code","df02b0a4":"code","00721f0d":"code","b38ad12d":"code","ad364459":"code","0d0501b0":"code","cbedb30c":"code","a77b4343":"code","5f54697b":"code","e4606ff7":"code","ccb53c5e":"code","25830f63":"code","5c4a1177":"code","e751f627":"code","b32ed765":"code","2ce0fedf":"code","18abb571":"code","c3baa91f":"code","416c28da":"code","b1c45216":"code","45c573fc":"code","7dece3c3":"code","65057b63":"code","21814717":"code","fb5e307e":"code","daa112b2":"code","f4a68274":"code","51a53679":"code","7233f7ab":"code","f8504757":"code","d60c76b4":"code","bf296ec4":"code","31df93ee":"code","4947e5b9":"code","864e66c6":"code","4d003bc2":"code","8a005059":"code","523a2eac":"code","94b2049e":"code","e6b1f485":"code","821bc069":"code","f1bca41e":"code","6ef25562":"code","b64350d2":"code","121cef60":"code","d4915e04":"code","740cfad7":"code","1a76584f":"code","7ef017d6":"code","f1d26451":"code","f9379e6d":"code","decda988":"code","e4d6a94b":"code","1d217b8e":"code","3df48764":"code","ce740532":"code","00b24f0f":"code","ef7ff271":"code","c2ab0612":"code","dfe3d73e":"code","da06c599":"code","79efc367":"code","8df97d1a":"code","5721e422":"code","b2d022fa":"code","23cbc840":"code","f079cad1":"code","450b0e1a":"code","12f6f777":"code","7d1467fa":"code","71d87929":"code","b7eb7136":"code","7cfef367":"code","975aea05":"code","5ac2414f":"code","08dff5e5":"code","2aae39a2":"code","b583d4e4":"code","693bebc0":"code","791cbd3c":"code","37f01cf2":"code","3f1a16ad":"code","e37dc94e":"code","5c1b45e9":"code","c5815376":"code","ea95f61e":"code","6df6ab82":"code","5346412b":"code","1cf5c8b3":"code","d6f5164c":"code","36196609":"code","f5d7ccaa":"code","e84b75cf":"code","f28e487a":"code","c0ba734b":"code","5dc4122b":"code","d9a59195":"code","f6a9ff9e":"code","205f9c9b":"code","45fa8e24":"code","79b1e2c1":"code","b78b3a3f":"code","e4117601":"code","17108233":"code","78d61af7":"markdown","3ab2096a":"markdown","05556242":"markdown","d377ec3e":"markdown","369703fb":"markdown","af74df42":"markdown","89071483":"markdown","62b42164":"markdown","4a3cf970":"markdown","ba17c2bf":"markdown","036c4a85":"markdown","879183ef":"markdown","20681622":"markdown","a9844b64":"markdown","6e4fb209":"markdown","4cb0d813":"markdown","5c40d0db":"markdown","b9bf9434":"markdown","e75ef777":"markdown","533074cb":"markdown","9d0c08c6":"markdown","6fcabb34":"markdown","107dc67e":"markdown","52b298f6":"markdown","13ecf02e":"markdown","6a0b2a09":"markdown","e0595328":"markdown","210929d3":"markdown","9f8f285c":"markdown","f70faa56":"markdown","e8250cee":"markdown","e8b682b5":"markdown","f43689bf":"markdown","4185338a":"markdown","ec523a0d":"markdown","454748e4":"markdown","cbbe1a81":"markdown","536b180d":"markdown","1911b878":"markdown","19ffb3e4":"markdown","5ee6eca4":"markdown","338f7143":"markdown","47a7d59f":"markdown","eef84bea":"markdown","6f2b42b1":"markdown","fcf9b9c9":"markdown","c4611feb":"markdown","89eb118f":"markdown","c6fce064":"markdown","ed835aef":"markdown","d3717557":"markdown","3c91c093":"markdown","1ba7887f":"markdown","5e94a495":"markdown","55b725cb":"markdown","d58014a4":"markdown","b829bbbc":"markdown","a11605f3":"markdown","d03a3fb0":"markdown","65813213":"markdown","4ae8de9c":"markdown","e2888584":"markdown","2d4005a4":"markdown","dce571db":"markdown","d3c71474":"markdown","8168f7dc":"markdown","1073bced":"markdown","9dbd15ef":"markdown","f3dddcbf":"markdown","5ee1669d":"markdown","6508d4e4":"markdown","20163666":"markdown","2ecb6394":"markdown","f327d1ed":"markdown","b7692792":"markdown","88273a5f":"markdown","fbcc47bd":"markdown","4fcfbe1e":"markdown"},"source":{"b8dfe85a":"import pandas as pd\nimport numpy as np\nimport sklearn as skl\n#import kaggle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42\n#Pandas - Displaying more rorws and columns\npd.set_option(\"display.max_rows\", 500)\npd.set_option(\"display.max_columns\", 500)","3afbc349":"def my_df_summary(data):\n    '''Own summary function'''\n    try:\n        dat = data.copy()\n        Q1 = dat.quantile(0.25)\n        Q3 = dat.quantile(0.75)\n        IQR = Q3 - Q1        \n    \n        df = pd.DataFrame([dat.sum(), dat.min(), dat.max(), ((dat < (Q1 - 1.5 * IQR)) | (dat > (Q3 + 1.5 * IQR))).sum(), dat.mean(), dat.std(), dat.median(), dat.count(), dat.isna().sum(), dat.nunique(), dat.dtypes],\n                     index=['Summe','Minimum', 'Maximum', 'Ausrei\u00dfer', 'Mittelwert', 'Stand. Abw.', 'Median', 'Anzahl', '#NA', '#Uniques', 'dtypes'])        \n        print('In total there are {} rows with {} variables in the dataset.'.format(len(data),(len(data.columns))))\n\n        return df\n    except:\n        print('A summary of the dataset was not possible.')\n        return data","8ea41cc0":"def plot_num(df):\n    try:\n\n        '''Function for Plotting all numeric columns (histograms and boxplots) for df'''\n        fig = plt.figure(figsize=(20,50))\n\n        df_plot = df.select_dtypes('number')\n\n        for i in range(len(df_plot.columns)):\n            ax = int(i*2)+1\n            ax1=fig.add_subplot(9,2,ax)\n            df_plot.iloc[:,i].plot(kind='hist', bins=50,ax=ax1)\n            plt.title(df_plot.columns[i])\n            ax = ax+1\n            ax2=fig.add_subplot(9,2,ax)\n            df_plot.iloc[:,i].plot(kind='box', ax=ax2)\n            plt.title(df_plot.columns[i])\n        return plt.show()  \n    except ValueError:\n        pass  # do nothing!    ","c441e504":"def df_norm(df,target_col,force_num=None,norm_target=True):\n    '''Normalize dataframe df. Target column will be put at the end. force_num can be an additional column which is forced to be numeric. '''\n    if force_num != None:\n        df[force_num] = pd.to_numeric(df[force_num], errors='ignore')\n\n    cats = df.select_dtypes('object')\n    nums = df.select_dtypes('number')\n\n    from sklearn.preprocessing import MinMaxScaler\n    minmax = MinMaxScaler()\n    num_tf = pd.DataFrame(minmax.fit_transform(nums))\n    num_tf.columns=nums.columns\n    if norm_target == False:\n        num_tf.drop(target_col,axis=1,inplace=True)\n        cats = cats.apply(LabelEncoder().fit_transform)\n        cats = cats.reset_index(drop=True)\n        df = pd.concat([cats, num_tf, df[target_col]],axis=1)\n    else:\n        cats = cats.apply(LabelEncoder().fit_transform)\n        cats = cats.reset_index(drop=True)\n        df = pd.concat([cats,num_tf],axis=1)\n    return df","5e85ea33":"def fill_na_all(df):\n    '''Fill all na for dataframe df. For numerics take mean, for categories take most frequent.'''\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    \n    cats = df.select_dtypes(exclude=numerics)\n    nums = df.select_dtypes(include=numerics)\n    \n    for i in nums.columns:\n        nums[i].fillna(nums[i].mean(),inplace=True)\n    for n in cats.columns:\n        cats[n].fillna(cats[n].mode()[0])\n    cats = cats.reset_index(drop=True)\n    nums = nums.reset_index(drop=True)\n    df = pd.concat([cats,nums],axis=1)\n    return df","253b7ffe":"# ignoring chained assignments error\npd.options.mode.chained_assignment = None  # default='warn'","2c8c3eda":"#!kaggle competitions download -c titanic","0e3f0b83":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ndf = pd.concat([train_data, test_data],axis=0,sort=False)","cf85d039":"df","d2033e73":"my_df_summary(df)","df02b0a4":"plot_num(train_data)","00721f0d":"plot_num(test_data)","b38ad12d":"name = df[\"Name\"].copy()\nname = name.str.split(\"[,.]\", expand=True)\nname.columns = [\"LastName\",\"Title\",\"FirstName\",\"Additional\"]\nname","ad364459":"name[name.iloc[:,3].isna()==False]","0d0501b0":"name = name.iloc[:,0:3]","cbedb30c":"name.iloc[:,1].unique()","a77b4343":"plt.figure(figsize=(16,8))\nsns.barplot(x=name.iloc[:,1].value_counts().index,y=name.iloc[:,1].value_counts()).set_title('Title Feature Value Counts Before Grouping', size=20, y=1.05)\nplt.show()","5f54697b":"name.iloc[:,1].value_counts()","e4606ff7":"name[\"Title\"] = name[\"Title\"].str.strip()","ccb53c5e":"name[\"Title\"] = name[\"Title\"].replace(['Lady', 'the Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\nname[\"Title\"].replace(['Mlle','Ms'], 'Miss',inplace=True)\nname[\"Title\"].replace('Mme', 'Mrs',inplace=True)\nname[\"Title\"].value_counts()","25830f63":"plt.figure(figsize=(10,6))\nsns.barplot(x=name.iloc[:,1].value_counts().index,y=name.iloc[:,1].value_counts()).set_title('Title Feature Value Counts After Grouping', size=20, y=1.05)\nplt.show()","5c4a1177":"df = pd.concat([df,name[\"Title\"]],axis=1, ignore_index=False)","e751f627":"df['Title'].value_counts()","b32ed765":"plt.figure(figsize=(10,6))\nsns.countplot(x=df['Title'],\n             hue=df[\"Survived\"],\n             order = df['Title'].value_counts().index).set_title(\n                    'Title Feature Value Counts After Grouping', \n                    size=20, \n                    y=1.05)\nplt.legend(loc='upper right', labels=['Not Survived', 'Survived'])\nplt.show()","2ce0fedf":"df[\"FamilySize\"] = df[\"SibSp\"]+df[\"Parch\"]+1","18abb571":"df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c3baa91f":"fig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=df['FamilySize'].value_counts().index, y=df['FamilySize'].value_counts().values, ax=axs[0][0])\nsns.countplot(x='FamilySize', hue='Survived', data=df, ax=axs[0][1])\n\naxs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\naxs[0][1].set_title('Survival Counts in Family Size ', size=20, y=1.05)\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndf['FamilySizeGrouped'] = df['FamilySize'].map(family_map)\n\nsns.barplot(x=df['FamilySizeGrouped'].value_counts().index, y=df['FamilySizeGrouped'].value_counts().values, ax=axs[1][0])\nsns.countplot(x='FamilySizeGrouped', hue='Survived', data=df, ax=axs[1][1])\n\naxs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)\naxs[1][1].set_title('Survival Counts in Family Size After Grouping', size=20, y=1.05)\n\nfor i in range(2):\n    axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n        axs[i][j].set_xlabel('')\n        axs[i][j].set_ylabel('')\n\nplt.show()","416c28da":"df.groupby('Ticket')['Ticket'].transform('count')","b1c45216":"df['TicketFrequency'] = df.groupby('Ticket')['Ticket'].transform('count')","45c573fc":"plt.figure(figsize=(10,6))\nsns.countplot(x='TicketFrequency', hue='Survived', data=df)\n\nplt.xlabel('Ticket Frequency', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Ticket Frequency'), size=15, y=1.05)\n\nplt.show()","7dece3c3":"df[\"HasCabin\"]= \"\"\ndf[\"HasCabin\"][df[\"Cabin\"].isnull()==True]=0\ndf[\"HasCabin\"][df[\"Cabin\"].isnull()==False]=1","65057b63":"plt.figure(figsize=(10,6))\nsns.countplot(x='HasCabin', hue='Survived', data=df)\n\nplt.xlabel('Has Cabin', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Has Cabin'), size=15, y=1.05)\n\nplt.show()","21814717":"df['IsWomanOrBoy'] = ((df.Title == 'Master') | (df.Sex == 'female'))","fb5e307e":"plt.figure(figsize=(10,6))\nsns.countplot(x='IsWomanOrBoy', hue='Survived', data=df)\n\nplt.xlabel('Is either a Woman or a Boy', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('IsWomanOrBoy'), size=15, y=1.05)\n\nplt.show()","daa112b2":"plt.figure(figsize=(10,6))\ny = np.round(df.isna().sum().sort_values()\/len(df)*100,1)\ndf.isna().sum().sort_values().plot(kind=\"barh\")\n\nfor index, value in enumerate(y):\n    plt.text(value*14, index - 0.2, '{:.1f}%'.format(float(value)))\n\nplt.title('Percentage of missing values for all variables')\nplt.xticks([])\nplt.xlim(0,1200)\nplt.show()","f4a68274":"plt.figure(figsize=(14,6))\nsns.heatmap(df.isnull(), cbar = False).set_title(\"Missing values heatmap\")","51a53679":"df[df[\"Embarked\"].isnull()==True]","7233f7ab":"df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0], inplace = True)","f8504757":"df.Embarked.isnull().sum()","d60c76b4":"df.groupby([\"Sex\"])[\"Age\"].mean()","bf296ec4":"df.groupby([\"Pclass\"])[\"Age\"].mean()","31df93ee":"df.groupby([\"Sex\",\"Pclass\"])[[\"Age\"]].median()","4947e5b9":"df.groupby([\"Sex\",\"Pclass\"])[[\"Age\"]].mean().unstack(0).plot(kind=\"barh\",figsize=(10,6))\nplt.legend(('Female', 'Male'))","864e66c6":"grid = sns.FacetGrid(df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()\nplt.show()","4d003bc2":"ages = df.groupby([\"Sex\",\"Pclass\"])[[\"Age\"]].mean()\nages","8a005059":"df[\"Age\"] = df.groupby([\"Sex\",\"Pclass\"])[[\"Age\"]].transform(\n    lambda grp: grp.fillna(np.mean(grp))\n)","523a2eac":"def age_to_group(age):\n    if 0 < age < 12:\n        # children\n        return 0\n    elif 12 <= age < 18:\n        # teenie\n        return 1\n    elif 18 <= age < 50:\n        # adult\n        return 2\n    elif age >= 50:\n        # elderly people\n        return 3","94b2049e":"df['AgeGroup'] = df['Age'].apply(age_to_group)","e6b1f485":"df[\"Cabin\"].fillna(\"NA\", inplace=True)","821bc069":"df[\"Fare\"].mean()","f1bca41e":"df[\"Fare\"].fillna(np.mean(df['Fare']), inplace=True)","6ef25562":"df['FareBand'] = pd.qcut(df['Fare'], 4)\n","b64350d2":"df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","121cef60":"df['Fare'][df['Fare'] <= 7.896] = 0\ndf['Fare'][(df['Fare'] > 7.896) & (df['Fare'] <= 14.454)] = 1\ndf['Fare'][(df['Fare'] > 14.454) & (df['Fare'] <=  31.275)]  = 2\ndf['Fare'][df['Fare'] >  31.275] = 3\ndf['Fare'] = df['Fare'].astype(int)\n\ndf = df.drop(['FareBand'], axis=1)","d4915e04":"df.head()","740cfad7":"df['HasCabin'] = df['HasCabin'].astype('int')","1a76584f":"# Mapping the sex\ndf['Sex'] = df['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n# Mapping the title\ntitle_mapping = {\"Mr\": 1, \"Mrs\": 2, \"Miss\": 3, \"Master\": 4, \"Rare\": 5}\ndf['Title'] = df['Title'].map(title_mapping)\n\ndf['IsWomanOrBoy'] = df['IsWomanOrBoy'].map( {True: 1, False: 0} )","7ef017d6":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nle = LabelEncoder()\ndf['Embarked'] = le.fit_transform(df.Embarked)\n#df['FamilySizeGrouped'] =le.fit_transform(df.FamilySizeGrouped)","f1d26451":"cat_features = ['Pclass', 'Sex', 'Embarked', 'Title', 'FamilySizeGrouped', 'HasCabin', 'IsWomanOrBoy']\nencoded_features = []","f9379e6d":"for feature in cat_features:\n        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        n = df[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)","decda988":"df_final = df.copy()","e4d6a94b":"df_final = pd.concat([df_final, *encoded_features], axis=1)","1d217b8e":"df_final.drop(cat_features,axis=1, inplace=True)","3df48764":"df_final.set_index(\"PassengerId\", inplace=True)","ce740532":"df_final.drop(['Name','Ticket','Cabin','SibSp','Parch', 'Age', 'FamilySize'],axis=1, inplace=True)","00b24f0f":"df_final.rename(columns={'IsWomanOrBoy_2':'IsWomanOrBoy_True',\n                         'IsWomanOrBoy_1':'IsWomanOrBoy_False',\n                         'HasCabin_1': 'HasCabin_False',\n                         'HasCabin_2':'HasCabin_True',\n                         'FamilySizeGrouped_1':'FamilySizeGrouped_Alone',\n                         'FamilySizeGrouped_4':'FamilySizeGrouped_Small',\n                         'FamilySizeGrouped_3':'FamilySizeGrouped_Medium',\n                         'FamilySizeGrouped_2':'FamilySizeGrouped_Large'\n                        }, inplace=True)","ef7ff271":"df_final.head()","c2ab0612":"my_df_summary(df_final)","dfe3d73e":"plt.figure(figsize=(18,10))\ncorr = df.iloc[:,1:].corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr,mask=mask, vmin=-1,cmap='PiYG', annot=False);\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)\n#Nur notwendig, weil Grafik bei einer anderen Version von matplotlib (>3.3) sonst verzerrt. Bekannter Bug.\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\n\nplt.show()","da06c599":"pd.DataFrame(df_final.iloc[:,1:].corr().iloc[0].sort_values(ascending=False))","79efc367":"df[[\"Pclass\", 'Survived']].groupby([\"Pclass\"], as_index=False).agg(['mean', 'count', 'sum'])","8df97d1a":"df[[\"FamilySizeGrouped\", 'Survived']].groupby([\"FamilySizeGrouped\"], as_index=False).agg(['mean', 'count', 'sum'])","5721e422":"from sklearn.model_selection import train_test_split","b2d022fa":"data_test = df_final[df_final[\"Survived\"].isna()==True]\ndata_train = df_final[df_final[\"Survived\"].isna()==False]","23cbc840":"data_train.head()","f079cad1":"i_split = 0.20\nX_train, X_test, y_train, y_test = train_test_split(data_train.iloc[:,1:], data_train.iloc[:,0], test_size=i_split,\n                                                    random_state=1234)","450b0e1a":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score, classification_report\n\nmodel = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1100,\n                                           max_depth=5,\n                                           min_samples_split=4,\n                                           min_samples_leaf=5,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=1) \nmodel.fit(X_train,y_train)","12f6f777":"predictions = model.predict(X_test)","7d1467fa":"df_conf_matrix = confusion_matrix(y_test,predictions)","71d87929":"print(classification_report(y_test,predictions))","b7eb7136":"from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport time","7cfef367":"models = [\n    BernoulliNB(),\n    GaussianNB(),\n    KNeighborsClassifier(),\n    LogisticRegression(solver='lbfgs',max_iter=10000),\n    SGDClassifier(),\n    LinearSVC(max_iter=10000),\n    SVC(gamma='auto',max_iter=-1),\n    DecisionTreeClassifier(criterion='gini',max_depth=None),\n    RandomForestClassifier(criterion='gini',n_estimators=1100,max_depth=5,min_samples_split=4,min_samples_leaf=5,max_features='auto',oob_score=True,random_state=SEED,n_jobs=-1,verbose=1),\n    AdaBoostClassifier(),\n    MLPClassifier(max_iter=2000)\n]","975aea05":"comparison = []\n\nfor model in models:\n    before_model = time.time()\n    classifier_name = str(type(model).__name__)\n    list_of_labels = sorted(list(set(y_train)))\n    fit = model.fit(X_train, y_train)\n    before_pred = time.time()\n    predictions = fit.predict(X_test)\n\n    precision = precision_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n    recall = recall_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n    accuracy = accuracy_score(y_test, predictions)\n    cms = confusion_matrix(y_test,predictions)\n    \n    comparison.append([classifier_name, str(round(accuracy*100,2))+'%',str(round(before_pred-before_model,4))+'s',str(round(time.time()-before_pred,4))+'s'])\n    \ncomparison = pd.DataFrame(comparison)\ncomparison.columns = ['Model','Accuracy','Modelling Time', 'Prediction Time']\ncomparison.set_index('Model')\ncomparison.sort_values(\"Accuracy\", ascending=False)","5ac2414f":"#data_test.set_index('PassengerId',inplace=True)","08dff5e5":"model = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1100,\n                                           max_depth=5,\n                                           min_samples_split=4,\n                                           min_samples_leaf=5,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=1) \nfit = model.fit(data_train.iloc[:,1:],data_train.iloc[:,0])\npredictions = fit.predict(data_test.iloc[:,1:])","2aae39a2":"data_test.reset_index(inplace=True)","b583d4e4":"submission = pd.DataFrame({\n        \"PassengerId\": data_test[\"PassengerId\"],\n        \"Survived\": predictions\n    })","693bebc0":"submission = submission.astype('int64')\nsubmission.to_csv('submissionRF.csv',index=False)","791cbd3c":"parameters = pd.Series(fit.feature_importances_, index=data_test.iloc[:,2:].columns).sort_values(ascending=False)\nplt.figure(figsize=(15, 20))\nsns.barplot(x=parameters, y=parameters.index)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.title('Random Forest Classifier Mean Feature Importance', size=15)\n\nplt.show()","37f01cf2":"from sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\n\nmy_pipeline = make_pipeline(SimpleImputer(), RandomForestClassifier(n_estimators=100))","3f1a16ad":"from sklearn.metrics import accuracy_score\nscores = cross_val_score(my_pipeline, data_train.iloc[:,1:], data_train.iloc[:,0], \n                         scoring='accuracy', \n                         cv=10)\nprint(scores)","e37dc94e":"print('Accuracy %.02f' %(100*scores.mean()) +'%')","5c1b45e9":"comparison = []\n\nfor model in models:\n    before_model = time.time()\n    classifier_name = str(type(model).__name__)\n    list_of_labels = sorted(list(set(y_train)))\n    before_pred = time.time()\n    \n    my_pipeline = make_pipeline(SimpleImputer(), model)\n    scores = cross_val_score(my_pipeline, data_train.iloc[:,1:], data_train.iloc[:,0], scoring='accuracy', cv=10)\n\n    \n    comparison.append([classifier_name, str(round(scores.mean()*100,2))+'%',str(round(before_pred-before_model,4))+'s',str(round(time.time()-before_pred,4))+'s'])\n    \ncomparison = pd.DataFrame(comparison)\ncomparison.columns = ['Model','Accuracy','Modelling Time', 'Prediction Time']\ncomparison.set_index('Model')\ncomparison.sort_values(\"Accuracy\", ascending=False)","c5815376":"data_test.set_index('PassengerId',inplace=True)","ea95f61e":"model = LogisticRegression(solver='lbfgs',max_iter=10000)\nfit = model.fit(data_train.iloc[:,1:],data_train.iloc[:,0])\npredictions = fit.predict(data_test.iloc[:,1:])","6df6ab82":"data_test.reset_index(inplace=True)","5346412b":"submission = pd.DataFrame({\n        \"PassengerId\": data_test[\"PassengerId\"],\n        \"Survived\": predictions\n    })","1cf5c8b3":"submission = submission.astype('int64')\nsubmission.to_csv('submissionLR.csv',index=False)","d6f5164c":"coeff_df = pd.DataFrame(data_train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(model.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","36196609":"model = SVC(gamma='auto')\nfit = model.fit(data_train.iloc[:,1:],data_train.iloc[:,0])","f5d7ccaa":"predictions = fit.predict(data_test.iloc[:,2:])","e84b75cf":"data_test.head()","f28e487a":"submission = pd.DataFrame({\n        \"PassengerId\": data_test[\"PassengerId\"],\n        \"Survived\": predictions\n    })","c0ba734b":"submission = submission.astype('int64')","5dc4122b":"submission.to_csv('submissionSVC.csv',index=False)","d9a59195":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\nk_fold = StratifiedKFold(n_splits=5)\nRF_classifier = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nRF_paramgrid = {\"max_depth\": [None],\n                  \"max_features\": [1, 2 ,3, 4, 5, 10, 'auto'],\n                  \"min_samples_split\": [2 , 5,  10],\n                  \"min_samples_leaf\": [2, 5, 10],\n                  \"bootstrap\": [True],\n                  \"n_estimators\" :[100,200,300,400,500,1000,1100,1200,1300,1400,1500],\n                  \"criterion\": [\"gini\"],\n                  \"oob_score\": [True, False]\n                   }\n\n\nRF_classifiergrid = GridSearchCV(RF_classifier, param_grid = RF_paramgrid, cv=k_fold, scoring=\"accuracy\", n_jobs= -1, verbose=1)\n\nRF_classifiergrid.fit(X_train,y_train)\n\nRFC_optimum = RF_classifiergrid.best_estimator_\n\n# Best Accuracy Score\nRF_classifiergrid.best_score_","f6a9ff9e":"RFC_optimum","205f9c9b":"model_rffinal=RandomForestClassifier(\n                                    n_estimators=500,\n                                    criterion='gini',\n                                    max_depth=None,\n                                    min_samples_split=2,\n                                    min_samples_leaf=5,\n                                    min_weight_fraction_leaf=0.0,\n                                    max_features=4,\n                                    max_leaf_nodes=None,\n                                    min_impurity_decrease=0.0,\n                                    min_impurity_split=None,\n                                    bootstrap=True,\n                                    oob_score=True,\n                                    n_jobs=None,\n                                    random_state=SEED,\n                                    verbose=0,\n                                    warm_start=False,\n                                    class_weight=None\n                                )","45fa8e24":"model_rffinal = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n                       max_depth=None, max_features=10, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=4,\n                       min_weight_fraction_leaf=0.0, n_estimators=200,\n                       n_jobs=-1, oob_score=False, random_state=None, verbose=1,\n                       warm_start=False)","79b1e2c1":"fit = model_rffinal.fit(data_train.iloc[:,1:],data_train.iloc[:,0])","b78b3a3f":"predictions = fit.predict(data_test.iloc[:,2:])","e4117601":"submission = pd.DataFrame({\n        \"PassengerId\": data_test[\"PassengerId\"],\n        \"Survived\": predictions\n    })\nsubmission = submission.astype('int64')","17108233":"submission.to_csv('submissionrfgrid.csv',index=False)","78d61af7":"There are 891 people in the dataset with 342 of them have survived the incident.\n\nIn total the 12 different variables are:\n\n|Variable|Definition|Key|\n|:---|:---|:---|\n|Passengerid| Unique ID for each passenger | |\n|survival| \tSurvival of Person| \t0 = No, 1 = Yes|\n|pclass| \tTicket class | 1 = 1st, 2 = 2nd, 3 = 3rd|\n|Name| Passenger Name including title| |\n|sex| \tSex| |\n|Age| \tAge in years | |\n|sibsp| # of siblings \/ spouses aboard the Titanic | |\n|parch|  of parents \/ children aboard the Titanic |\t|\n|ticket| \tTicket number | \t|\n|fare| \tPassenger fare \t|\n|cabin| \tCabin number |\t\n|embarked| \tPort of Embarkation| \tC = Cherbourg, Q = Queenstown, S = Southampton|","3ab2096a":"One way to look at relations in the data is to take a look at correlations.","05556242":"1. Survived & Fare: The higher the Fare, the higher the chance of survival\n2. Survived & Pclass: The lower the class (1st class is lower than 2nd) the betther the chance of survival\n3. Survived & HasCabin: People having a Cabin seem to have a higher probability of surviving\n4. Survived & FamilySizeGrouped_Small: A small family group has a higher chance of survival than when traveling alone","d377ec3e":"## Data Pre-Processing: Feature Engineering <a class=\"anchor\" id=\"third-bullet\"><\/a>","369703fb":"### Logistic Regression - submission\nAs Logistic Regression was performing second-best in our CV model comparison, let's hand in a submission with LogisticRegression","af74df42":"So far, so good. Now that we have done some feature engineering and filled NAs let's take another look at the data.","89071483":"## Data Pre Processing: Dropping unnecessary features <a class=\"anchor\" id=\"fourthand1-bullet\"><\/a>","62b42164":"# Titanic: Machine Learning from Disaster\n\nThis is my first Kaggle competition applying the knowledge I have acquired due to the last months \/ year.\nThis version is on purpose not shortened to display the learning process made during the write-up.\n\n## Table of Content\n* [Loading own functions](#first-bullet)\n* [Data Overview](#second-bullet)\n* [Data Pre-Processing: Feature Engineering](#third-bullet)\n* [Data Pre-Processing: Handling Missing Values](#thirdand1-bullet)\n* [Data Pre Processing: Transforming the variables into numericals](#fourth-bullet)\n* [Data Pre Processing: Dropping uncessesary features](#fourthand1-bullet)\n* [Exploratory Data Analysis](#fifth-bullet)\n* [Machine Learning Models ](#sixth-bullet)\n* [Cross Validation](#seventh-bullet)\n* [Grid Search](#eight-bullet)\n","4a3cf970":"Finally, the SVC model reaches an accuracy of 79.425%.","ba17c2bf":"For a full list of scoring metrics see this [link](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html)","036c4a85":"We can furthermore, create age bands and determine their correlations with Survived","879183ef":"### TicketFrequency (i.e. GroupSize)","20681622":"### Title","a9844b64":"Besides, we have some positive correlations:\n1. HasCabin & Pclass_1: People with a Cabin are usually travelling in Class 1\n2. FamilySizeGrouped and Fare: The higher the Grouped Family Size (i.e. the larger the group) the higher the Fare\n","6e4fb209":"For those categegories, which are ordinal variables **label encoding** works quite well as you can rank \/ order the outcomes of a feature (e.g. group of Age or Fare). For tree-based models (like decision trees and random forests), you can expect label encoding to work well with ordinal variables.\n\nFor most of our categorical variables (i.e. nominal variables), we cannot set a logical order (e.g. woman not better than men) therefore we use **One-Hot-Encoding**. One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., generally do not use it for variables taking more than 15 different values).","4cb0d813":"However, we have to ignore correlations between two object variables, e.g.:\n* Title and Sex: There is no meaning of the correlation and obviously a woman with a title \"Mrs\" is in the womanorboy group","5c40d0db":"We could use the latter to fill ages per sex and group.","b9bf9434":"### Fare\n\nNext up is Fare. Let's take a look at the mean here.","e75ef777":"As mentioned before, due to the high number of missing values, we could simply delete the cabin variable. \nHowever, we can also fill them with \"NA\", for now.","533074cb":"Furthermore, Group Sizes are further grouped:\n\n* Group Size with 1 are labeled as Alone\n* Group Size with 2, 3 and 4 are labeled as Small\n* Group Size with 5 and 6 are labeled as Medium\n* Group Size with 7, 8 and 11 are labeled as Large\n","9d0c08c6":"Now with 10-times cross validation, we see more realistic accuracy values for all models.\nStill, the SVC and Random Forest are among the best models together with Logistic Regression and MLP achieving an accuracy of ~83%.","6fcabb34":"Before we start with the modeling, we will set PassengerId as index and drop all unnecessary object variables.  \nLet us also drop Parch and SibSp features in favor of TravelsAlone and GroupSize, as well as the OneHotEncoded features","107dc67e":"Let's use these group means for filling the NAs in the Age group.","52b298f6":"## Machine Learning Models <a class=\"anchor\" id=\"sixth-bullet\"><\/a>","13ecf02e":"### Cabin","6a0b2a09":"Next, we need to change the type of the data into numericals for modelling.","e0595328":"Since this is only ony case, we can ignore the additional column","210929d3":"Otherwise, we can also use the label encoder. \n\nLabelEncoder basically labels the classes from 0 to n. This process is necessary for models to learn from those features.","9f8f285c":"There are several way how to handle this problem. Either we could do some research, to see if there is any information about how to fill these two rows. Or we could use backward or forward fill. Finally, we could also use the most frequent place, where people embarked (mode).","f70faa56":"### FamilySize","e8250cee":"Let's do the same thing again for all of our other models.","e8b682b5":"### Random Forest - submission","f43689bf":"Logistic regression, however, only achieves 77.99% of accuracy.","4185338a":"## Data Pre Processing: Transforming the variables into numericals <a class=\"anchor\" id=\"fourth-bullet\"><\/a>","ec523a0d":"## Exploratory Data Analysis <a class=\"anchor\" id=\"fifth-bullet\"><\/a>","454748e4":"Grouping tickets by their frequencies, reveals that many passengers travelled along with groups consisting of friends, nannies, maids, etc. They were not count as family, but used the same ticket.","cbbe1a81":"According to the graph , groups with 2,3 and 4 members had a higher survival rate. Passengers who travel alone has the lowest survival rate. After 4 group members, survival rate decreases drastically. \n\nThis pattern is very similar to FamilySize feature but there are minor differences. TicketFrequency values are not grouped like FamilySize because that would basically create the same feature with perfect correlation. This kind of feature wouldn't provide any additional information gain.","536b180d":"### Further models & Model comparison","1911b878":"We can use a Logistic Regression to validate our assumptions and decisions for feature creation. This can be done by calculating the coefficient of the features in the decision function.\n\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n\n* FamilySizeGrouped_Alone is the highest positive coefficient, meaning that when belonging to this group (i.e. traveling alone) the probability of survival increases the most.\n* IsWomanOrBoy is the second highest positive coefficient, meaning that when IsWomanOrBoy increases from 0 to 1 (i.e. is True), the probability of Survived = 1 increases the second most.\n* Likely, HasCabin or Fare both have a positive influence once they increase (i.e. the person traveling has a cabin or has paid a higher Fare)\n* On the other extreme, the higher the Class (i.e. the lower the Fare paid - see correlation above) the lower the chance of survival\n* The Same counts for AgeGroup or when the person is not a woman or boy","19ffb3e4":"### Embarked","5ee6eca4":"Taking the Fare price, we can create a Fare band, dividing fare into four different groups:","338f7143":"By looking at different group means, we can further analyze our data:","47a7d59f":"At a first glance, it seems like the Name variable is unnecessary but let's see if we can extract additional information from it e.g. through extracting the title. ","eef84bea":"From the two columns SibSp (Number of Siblings or Spouses on board) and Parch (number of Parents or Children on board) we can create a new variable representing the group size.","6f2b42b1":"Let's see how we perform when we submit the data, using our random forest  model:","fcf9b9c9":"Let's start with the embarked column.","c4611feb":"### Age","89eb118f":"## Loading own functions <a class=\"anchor\" id=\"first-bullet\"><\/a>","c6fce064":"Starting with modeling, we need to divide our data into a training and a test dataset.  \nWe will then use the training data for our model to learn and the test dataset to see which model performs best.\n  \nWe will use a split of 80% (training) to 20% (test dataset)","ed835aef":"In comparison to our previous accuracy of 85%, we now get a lower, more realistic accuracy of 83%.","d3717557":"Between the two datasets there does not seem to be any obvious differences in distribution of the features.\n\nWe see that we have some outliers in Fare, SibSp and Parch. We need to handle them later.","3c91c093":"Thanks for Feature Engineering:\n* https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n* https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n* https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#2.-Feature-Engineering\n\nThanks for Grid Search:\n* https:\/\/www.kaggle.com\/umarsajjad\/top-3-titanic-survival-determination#6---Feature-Engineering","1ba7887f":"From here we can see some variables which are highly negative correlated:\n1. HasCabin & Fare: People without a cabin pay a lower Fare than with a cabin","5e94a495":"The submission reveals an accuracy of 79.425%. A little lower than expected. However, it might have two reasons:\n1. we need to do cross validation to ensure we are picking a random model\n2. we might be overfitting, using too many variables\n\nLet's start with the first idea in short, but first, let's take a look at the importance of our features.","55b725cb":"We will now start with a random forest model:\n\nFurthermore we also import some metrics to see e.g. the accuracy of our model.","d58014a4":"Let's see what and how many different titles we currently have.","b829bbbc":"We have missing values in four features:  \n1. age: 177 - adding up to ~20% so we could try filling the NAs  \n2. class: 687 - since almost 78% are missing data, we might want to drop this column. Also, we have already used it in the Feature Engineering Part where we created the variable HasCabin.  \n3. embarked: 2 - adding up to less than 1% so we should fill the NAs\n4. Fare: 1 - adding up to less than 1% so we should fill the NAs","a11605f3":"From the Cabin number we can find out whether the person travelling had a cabin or not. ","d03a3fb0":"Finally, let's also hand in our support vector machine model.","65813213":"More interestingly, when looking at the correlation between survived and other features:","4ae8de9c":"## Data Pre-Processing: Handling Missing Values <a class=\"anchor\" id=\"thirdand1-bullet\"><\/a>","e2888584":"## Cross Validation <a class=\"anchor\" id=\"seventh-bullet\"><\/a>","2d4005a4":"Therefore, we should convert the categorical features Pclass, Sex, Embarked, \tTitle, GroupSizeGrouped, HasCabin, TravelsAlone and IsWomanOrBoy","dce571db":"Unfortunately, after the grid search, the handed in model  reaches 80.382% of accuracy. This is so far our best model and a slight improvement in comparison to our 79.425% we achieved earlier. Currently this is ranked among the top 8% in kaggle titanic competition.","d3c71474":"How do other models perform in comparison to Random Forest?","8168f7dc":"SVC and Random Forest seem to be our best two models, performing better than 85%","1073bced":"## Data Overview  <a class=\"anchor\" id=\"second-bullet\"><\/a>","9dbd15ef":"We seem to have one name with more than one , or . in it (i.e. column 3 is not NA)","f3dddcbf":"### Random Forest","5ee1669d":"### SVC - submission","6508d4e4":"Besides the most common titles Mr, Miss and Mrs, there seem to be more rarely used titles (e.g. Dr, Don or Lady).","20163666":"Another way of displaying missing values is the following:","2ecb6394":"Let us now replace Age with ordinals based on these bands.","f327d1ed":"## Grid Search <a class=\"anchor\" id=\"eight-bullet\"><\/a>\n\nWe can finally improve our best model (RF) through grid search.","b7692792":"Looking at the classification report, we predict with an accuracy of 85%, which is quite good.\nWe seem to have a higher precision with predicting the survivals (i.e. survived = 1) than the non-survivals (i.e. survived = 0). ","88273a5f":"We saw, that we have 177 missing values in the age column. As a starting point, we could take a look at the mean age looking at different splits like Sex or Pclass (or even both).","fbcc47bd":"### HasCabin","4fcfbe1e":"### IsWomanOrBoy\n\nWe have already taken a look at the different titles. However, we can take a closer look at the title \"Master\".   \n\nTaking a look at the origin of the title Master [(source)](https:\/\/en.wikipedia.org\/wiki\/Master_(form_of_address)):_Master was retained as a form of address only for boys who had not yet entered society. By the late 19th century, etiquette dictated that men be addressed as Mister, and boys as Master._ \n\nAs commonly known, in a ship tragedy, woman or children are rescued first. So, we can use this to create a new variable calld IsWomanOrBoy"}}