{"cell_type":{"0ee0dfa5":"code","ce1d5813":"code","2e2ceb17":"code","d9a15c62":"code","5cbed640":"code","b697969b":"code","26ca2d86":"code","3a8eb36e":"code","3f313f39":"code","4e17d6f5":"code","d7cdef4e":"code","3bb27d0b":"code","40696c90":"code","528f689c":"code","41d8a078":"code","7eaa5055":"code","7055ebad":"code","061354fe":"code","70fa068d":"code","6c02edb3":"code","6ac1f0ae":"code","68a94ee0":"code","c39ca535":"code","67303b09":"code","bc5ed3e1":"code","67619e17":"markdown","9ad2b203":"markdown","b1efadc0":"markdown","c6564b14":"markdown","1ccdc1ee":"markdown","ecf2be43":"markdown","13c53f52":"markdown","5e13042f":"markdown","7f37c9a0":"markdown","89d9b30f":"markdown","932d8fc3":"markdown","4c0b8a65":"markdown","bbeccb07":"markdown","58b0b73b":"markdown","c2dd27c2":"markdown","e3ed79a3":"markdown","c658638c":"markdown","b7bc293f":"markdown","00067a27":"markdown","41dee4cb":"markdown","076ab155":"markdown"},"source":{"0ee0dfa5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","ce1d5813":"df_train=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","2e2ceb17":"df_train.columns","d9a15c62":"# Descriptive statistics summary\ndf_train['SalePrice'].describe()","5cbed640":"#Histogram\nsns.distplot(df_train['SalePrice'])\nplt.title(\"Relationship of SalePrice vs Density\")\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['top'].set_visible(False)\n#plt.gca().spines['left'].set_visible(False)\n#plt.gca().spines['bottom'].set_visible(False)\nplt.show()","b697969b":"# skewness and kurtosis\nprint(\"The skewness is %f \" %df_train['SalePrice'].skew())\nprint(\"The Kurtosis is %f \"%df_train['SalePrice'].kurt())","26ca2d86":"#scatter plot\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'],df_train[var]],axis=1)\ndata.plot.scatter(x=var,y='SalePrice',ylim = (0,800000))\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.title(\"Relationship of SalePrice vs GrLivArea\")\nplt.show()","3a8eb36e":"#scatter plot\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'],df_train[var]],axis=1)\ndata.plot.scatter(x=var,y='SalePrice',ylim = (0,800000))\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['top'].set_visible(False)\nplt.title(\"Relationship of SalePrice vs TotalBsmtSF\")\nplt.show()","3f313f39":"var = 'OverallQual'\ndata=pd.concat([df_train['SalePrice'],df_train[var]],axis=1)\nf,ax = plt.subplots(figsize=(8,6))\nsns.boxplot(x=var,y='SalePrice',data=data)\nplt.axis(ymin=0,ymax= 800000)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.show()","4e17d6f5":"var = 'YearBuilt'\ndata=pd.concat([df_train['SalePrice'],df_train[var]],axis=1)\nf,ax = plt.subplots(figsize=(16,8))\nsns.boxplot(x=var,y='SalePrice',data=data)\nplt.axis(ymin=0,ymax= 800000)\nplt.xticks(rotation=90)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['top'].set_visible(False)\nplt.show()","d7cdef4e":"correlation = df_train.corr()\nf,ax = plt.subplots(figsize=(15,8))\nsns.heatmap(correlation,vmax=0.8)\nplt.show()","3bb27d0b":"k = 10\ncols = correlation.nlargest(k,'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nf,ax=plt.subplots(figsize=(15,8))\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm,annot=True,xticklabels=cols,yticklabels=cols)\nplt.show()","40696c90":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols],size=3)\nplt.show()","528f689c":"total = pd.Series(df_train.isnull().sum()).sort_values(ascending=False)\npercentage = (pd.Series(df_train.isnull().sum())\/(df_train.isnull().count()).sort_values(ascending=False))\nmissing_data = pd.concat([total,percentage],axis=1,keys=['total','value'])\nmissing_data.head(20)","41d8a078":"#removing the null values\ndf_train=df_train.drop(missing_data[missing_data['total']>1].index,axis=1)\ndf_train=df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\ndf_train.isnull().sum().max()\n#Checking for the null values","7eaa5055":"saleprice_scaled = StandardScaler().fit_transform(df_train[\"SalePrice\"][:,np.newaxis])\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint(\"Outer range (low) of the distribution: \")\nprint(low_range)\nprint(\"Out range (high) of the distribution: \")\nprint(high_range)","7055ebad":"plt.figure(figsize=(13,5))\nplt.subplot(1, 2, 1) # n is the position of your subplot (1 to 4)\nsns.scatterplot(df_train['GrLivArea'],df_train['SalePrice'])\nplt.subplot(1, 2, 2) # n is the position of your subplot (1 to 4)\nsns.scatterplot(df_train['TotalBsmtSF'],df_train['SalePrice'])\nw = plt.gca().get_yaxis() \nw.set_visible(False) \nplt.tight_layout()","061354fe":"#histogram and normal probability plot\nfrom scipy.stats import norm\nsns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","70fa068d":"#histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","6c02edb3":"#data transformation\ndf_train['GrLivArea'] = np.log(df_train['GrLivArea'])\n#transformed histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","6ac1f0ae":"#histogram and normal probability plot\nsns.distplot(df_train['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['TotalBsmtSF'], plot=plt)","68a94ee0":"#create column for new variable (one is enough because it's a binary categorical feature)\n#if area>0 it gets 1, for area==0 it gets 0\ndf_train['HasBsmt'] = pd.Series(len(df_train['TotalBsmtSF']), index=df_train.index)\ndf_train['HasBsmt'] = 0 \ndf_train.loc[df_train['TotalBsmtSF']>0,'HasBsmt'] = 1\n#transform data\ndf_train.loc[df_train['HasBsmt']==1,'TotalBsmtSF'] = np.log(df_train['TotalBsmtSF'])\n#histogram and normal probability plot\nsns.distplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)","c39ca535":"# Let us again plot the scatter plot between TotalBsmtSf and SalePrice\n#scatter plot\nplt.scatter(df_train['GrLivArea'], df_train['SalePrice']);","67303b09":"#scatter plot\nplt.scatter(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], df_train[df_train['TotalBsmtSF']>0]['SalePrice']);","bc5ed3e1":"#convert categorical variable into dummy\ndf_train = pd.get_dummies(df_train)","67619e17":"Oops, truly good friend, as one increases the other increases, what about 'TotalBstSF'?","9ad2b203":"Okay, so done about the GrLivArea, next ....","b1efadc0":"## Thanks for now, and if you think something could be improved please let me know, I will be more than happy :)","c6564b14":"You are truly beautiful, now what I could infer from your pictures are the following facts :\n* You have a right skewness\n* The maximum frequency lies in the range near the 2 lakh, or in other words, the most houses are priced near to 2 lakh\n* The spread is also quite good, std was near to 0.8 lakh\n\nCan you show me your body measures ?","1ccdc1ee":"## Inferences\n* So, in the first plot, we can see two points on the bottom right side, maybe the agricultural land, who knows, because, intuiton :)..., and the two on the top right, maybe let us see what we can  do\n* The one point on the right bottom, and yeah, maybe the two points on the top center\n\n# 5. Getting more theoretical\n\n### In search of Normality\nBasically, we now will perform statistical analysis onour target variable \"SalePrice\", and to quantify our analysis, we will bring in some of the concepts of Normal Distribution such as skewness, kurtosis, etc","ecf2be43":"In her new outift, what we can observe are :\n* Most of the minimum arguments are near to -1 (some are nearer to -2 also)\n* And the maximum value seems to have a sudden jump, i.e from 5.58, they directly went ot 7. something, we will take precaution in such stuffs\n\n### Now, Bivariate Analysis\n\nWhat we saw was, in the scatter plot, had, linear relationship with some, and also, one had a kind of exponential relationship, but this time, we will see it from a new perspective,(atleast I hope that), let us see...\n\nOur focus will be on:\n1. SalePrice vs GrLivArea\n2. SalePrice vs TotalBsmtSF\n\nI was about to write TotalQual, but it is a categorical variable, so dropped it","13c53f52":"## Inferences :\n  1. SalePrice with year built seems somewhat like an exponential relationship\n  2. TotalBsmtSF and GrLivArea, seems to have linear relationship, let me elaborate, basically, practically we can see that, as TotalBsmtSF increases, GrLivArea would also rise\n  \n  Ok, so I have also heard that the missing data handling is an important process, hence we will see it next :)\n# 4. Missing Data\n\nWhy do we need missing Data ?\nIf I would had a company, and if I would be interested in the data, and some of the values are found to be null, I would be concerned about the following factors :\n1. Do this missing values, add some insights or removes insights from the total value of the data.\n2. Do this missing data follows some pattern ?\n3. If you could think of more, please let me know in the comment section, I would be happy to learn more !!\n\nWith this said, let us get started","5e13042f":"Somehat good, but not as good as GrLivArea, I also can see your outlier :), but thats fine, I got to know our common friends, but what a minute, I could sense an exponential relationship with you (too emotional relationship ), moreover, sometimes, TotalBsmtSF gives zero credit to SalePrice\n\nRelationship with the cateogrical variables","7f37c9a0":"Most of the credit goes to the Notebook titled <a href='https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\/notebook'> COMPREHENSIVE DATA EXPLORATION WITH PYTHON <\/a>\n\n# 1. What will we be expecting ?\nThe following variables would be playing an important role in the problem statement\n* OverallQual\n* YearBuilt\n* TotalBsmtSF\n* GrLivArea\n\n# 2. First things first : analysing 'SalePrice'\n\nSo, let us build a story, I am currently a teenage,and now I went to a party, definitely for the reason of girls(not really), maybe for other peple, it could be drinks, friends etc\n\nNow, as I was enjoying the song, I saw a girl who had her shoes, probably there for dancing, now since I am an analytical person, involved in the predictive modelling and analysis, I am quite weak to directly approach\n\nSo, I went to her, and said Hi, I am DS, and you .... 'SalePrice', wanted to analyse the factors, which are responsible for making our relationship long lasting and beautiful. So let us get started\n","89d9b30f":"## Inferences :\n1. How should I read this cmap ?\n    ### For SalePrice    \n    First of all, look back to our objective, what were we going to do with this cmap ? Finding the correlation of SalePrice with other variables, and **multicollinearity**\n    \n    ### For Multicollinearity\n    See the red area, not necessary all of them, (definitely you could everyone, if you have more time :) ), but many of the authors have done it for us :), so let us continue our journey\n    \nLet us see, the top correlated variables with SalePrice","932d8fc3":"### So, the handling of missing data has been done, now the outliers !!!!\n#### Are Outliers important ?\n**They are important due to the following factors :**\n1. They definitely affect the behaviour of the model, since the model will be based on the total data, which would contain the outliers, it would be important to know the outliers\n2. Also, outliers help us in providing knowledge about a particular attribute, maybe this could be beneficial for my company, wait a minute, not the company, the $SalesPrice$, how can I forget here ....\n\n### Let us get onto work of getting the insight of the outliers....\n\n### The flow will be :\n1. Univariate Analysis (of the SalesPrice)\n2. Multivariate Analysis (for the target variable and some intuited variables)\n\nIn the univariate Analysis, there could be many perspective :\n1. Perform only the analysis of the target variable (we would do this)\n2. Take many variables into account\n\nThe aim of univariate analysis is :\n**To mark a point with surety, that whatever after this or before this would be considered as an outlier, ok so let us get started**","4c0b8a65":"So, now with the years, you were really costly in the 1890s and then directly in 2010, really, you are having an awesome nature (although in 1970s you became costly)\n\nBut **note** that, inflation also should be taken into account, because the 5 lakh of 2010 is not equal to 1890s 5 lakh, because of the inflation factor\n\nHence, if the price is not constant, we can see that the nearby years are quite comparable with each other\n\n## Summarizing our intuiton:\n\n### 1. GrLivArea :By plotting the scatterplot, we came to know the fact that both had a linear relationship\n### 2. TotalBsmSF : So, the scatterplot seems to describe the fact that both the varaibles had more a kind of exponential relationship rather than linear relationship\n### 3. OverAllQuality : By plotting the boxplot, we came to know the fact that the uncertanity or the standard deviation was high in the region were the OverallQuality was high and definitely the median price was also high\n### 4. YearBuilt : The houses that were built quite long ago, and that were built recently had the high price value, but definitely we described the fact that, the factor of inflation also should be taken into consideration, hence the house prices were quite comparable, i.e the SalePrice was not constant\n\n# 3. Now, the relationship between 'SalePrice' and her friends should be taken\nHowever, it is to be noted that, what we had considered, i.e. TotalBsmtSF, YearBuilt, OverAllQuality and GrLivArea, are from our perspective, i.e if I wanted to buy a house, I would take these factors into account, but shouldn't we take into consideration the fact that, there could be more variables, which would be highly correlated with the SalePrice, and could highly affect our model in predicting the SalePrice ?\n\nSo, next, we would do the following things :\n1. Correlation Map\n2. Pair Plot between the variables that proves to be highly correlated with the SalePrice (from the correlation map)\n\nSo, let us get started..........","bbeccb07":"## Inference :\nSo, I got somewhat the feeling of **parabola**, yea, there are 2 outliers, but still looks like a quadratic or an exponential relationship\n\nLet us get to TotalBsmtSF","58b0b73b":"That's truly amazing, my dear friend, so in conclusion in the first meet I have got the following things from you :\n* Your statistical descriptions\n* Your measures (i.e the skewness and kurtosis)\n\nThanks a lot for sparing time with me, hope to see you soon again the next week, for now, have a great day ahead\n\n# SalePrices, her buddies and her other factors responsible for making her happy\n\nOk, so having met her and getting her appearances and her measurement, now I want to met here near and dear ones, so that I could gain more information about her, and feel more comfortable to met her, and get some topic to talk with her. No stalkings !\n\nFor that, I need to go to social media and meet her friends and talk to them about her,so for us we will see the common friends and then would focus on common interests. So, we have **GrLivArea** and **TotalBsmSF** as our common friends, so let us meet them and then would move to our common interests.\n\n### Realtionships with numerical variables","c2dd27c2":"So, you are really having many colors, and that makes you truly beautiful, so,in my mind : what about Cafe Coffee Day?","e3ed79a3":"So, SalePrice is highly correlated with  :\n        1. OverallQual\n        2. GrdLivArea\n        3. GarageCars (Ignore GarageArea, because GarageCars includes the factor of GarageArea)\n        4. TotalBsmtSF (Similarily, isn't the 1stFlrSf depended upon the TotalBsmtSF)\n        5. FullBath (Ok, sounds interesting)\n        6. YearBuilt (Let us see this person)\n        \n        \n        \nOk, so now pairplot >>>>>>>","c658638c":"## Inferences :\nSo, what I could see is the fact, that the variables whose about $15$ **percent** of the data or more is missing, it is better to simply remove them, and believe that they did never existed\n\nSo, from the above table what we could see are :\n1. $PoolQC$ has about 99.5% of the missing data, but let us question, if I had money and I would be interested in buying a house, I would never see the **Pool Quality**, and also we took a note that, only some of the features were quite important while others did not play much important role\n2. How could I prove that the dropped data, will not prove any dange for me, while designing the model, well , I cannot say, but some intuitons :)\n3. Similar is the case of $MiscFeature$, although it was not important for me (for buying the house), similar with the other variables below $MiscFeature$, \n4. Okay, also the Garage..... variables, will be covered by $GarageCar$, hence it should work fine, if we dropped all the Garage.. variables, except the GarageCars\n5. Moreover, as described earlier the variables such as $Alley,Fence,FireplaceQu,LotFrontage, BsmtQual,BsmtCond,etc..$ and also, the variables such as $MasVnrType, MasVnrType$,etc can also be dropped\n6. And, the electrical one, I am thinking of dropping that value :), maybe that would work fine\n\n### In summary : We would remove all the variables, that are having null values, except the electrical one, for that we would drop that particular index\n\n### This LatEx code is quite impressive, back to topic :)","b7bc293f":"## Inferences :\n1. I could see the skewness of the data\n2. And also, could see many zeros in the Probability Plot, which means that they are the minimum values, because nobody is less than them, house without basement .... fascinating !!!\n3. So, how will we apply the log transformation...\n\nStep 1 : Initialize a series called \"HasBsmt\"\n\nStep 2 : Find if a house has surface area or not, and then on the basis of that, make HasMt 1 or 0\n\nStep 3 : Aplly log transformation where \"HasBsmt\" == 1","00067a27":"## Note about the Probability Plot or P-P\n\n**A probability plot graphs each value versus the percentage of values in the sample that are less than or equal to it**\n\n\n* The probability plot is a graphical technique for knowing whether or not a data set follows a given distribution such as normal or Weibuli\n\n* The data are plotted against a theoretical distribution in such a way that the points should form approximately a straight line. Departures from this straight line indicate departures from the specified distribution\n\n* The correlation coeeficent associated with the linear fit to the data in the probability plot is a measure of the goodness of the fit. Estimates of the location and scale parameters of the distribution are given by the intercept and slope\n\n* Probability plots can be generated for the several competing distributions to see which provides the best fit, and the probability plt generating the highest correlation coefficient is the best choice since it generates the straightest probability plot \n\n<a href = \"https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/probplot.htm\"> Source <\/a>","41dee4cb":"Now, what I could see is, your minimum value is greater than zero (ofcourse it is obvious), but I could see that the mean is around 1.8 lacs, and the median is 1.63 lakh, oops, you have some skewness (I guess that, I will see it visually), and the deviation is around 0.8lakh, and max is 7.5 lakh, do you have some picture ?","076ab155":"Okay, so there are outliers, and we need to see them, let us see them in the next notebook, for now, we have done a good exercise, and in the next part of this notebook, I will be doing the Machine Learning method to know, whether she likes Linear Regression, or some ensembling techniques.\n"}}