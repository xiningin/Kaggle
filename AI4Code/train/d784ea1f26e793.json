{"cell_type":{"a67ef724":"code","b0816dfd":"code","a602d2d2":"code","5e62638f":"code","ec0c5629":"code","bd011c96":"code","e3fc5115":"code","d58c51b9":"code","a95d5d02":"code","9f7f0fb6":"code","15fce2a7":"code","942433af":"code","c97cc011":"code","2c12f846":"code","9664f0e3":"code","98fe84d7":"code","8795581f":"code","2fb4cae9":"code","69242cc7":"code","0a75589f":"code","6ba1cd48":"code","da08e87d":"markdown","c563814b":"markdown","7998b6d4":"markdown","1481245f":"markdown","72e0e2ca":"markdown","ce15b797":"markdown","a0b46ba3":"markdown","fee02905":"markdown","351fa6c8":"markdown"},"source":{"a67ef724":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0816dfd":"!pip install vaderSentiment","a602d2d2":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport nltk\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n\nstop_words = nltk.corpus.stopwords.words('english')\nstop_words.extend(['co', 'wa', 'ha'])","5e62638f":"data = pd.read_csv('..\/input\/all-trumps-twitter-insults-20152021\/trump_insult_tweets_2014_to_2021.csv')\ndata['date'] = pd.to_datetime(data.date, errors = 'ignore')\ndata.head()","ec0c5629":"data.info()","bd011c96":"data.describe()","e3fc5115":"data.dtypes","d58c51b9":"# Initialize the Lemmatizer and Whitespace Tokenizer\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\n# lemmatize text column by using a lemmatize function\ndef lemmatize_text(text):\n    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text.lower())]\n\ndef cleantext(txt):\n        \n    # A bit of cleaning\n    txt = txt.lower()\n    # remove special characters from text column\n    txt = re.sub(r'\\W', ' ', txt)\n    #Remove twitter handlers\n    txt = re.sub('@[^\\s]+','', txt)\n    #Remove digits\n    txt = re.sub(r'\\d+',' ', txt)\n    # remove urls spaces with single space\n    txt = re.sub(r\"\\\"?http\\S+\", ' ', txt)\n    # remove urls spaces with single space\n    txt = re.sub(r\"\\\"?www\\S+\", ' ', txt)\n    #remove all single characters\n    txt = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', txt)\n    # remove multiple spaces with single space\n    txt = re.sub(r'\\s+', ' ', txt)\n    # Lemmatizes\n    txt = lemmatize_text(txt)\n    tokens = []\n    for w in txt:\n        if w not in stop_words:\n            tokens.append(w)\n        \n    return tokens","a95d5d02":"# Let's see the length of the tweets\nseq_length = [len(i) for i in data['tweet']]\n\npd.Series(seq_length).hist(bins=25, color='red')","9f7f0fb6":"data['cleaned_tweets'] = data['tweet'].apply(lambda x: cleantext(x))\n#data['cleaned_tweets']\ndata.head(5)\n\n## use explode to expand the lists into separate rows\ndtump_tweets = data.cleaned_tweets.explode().to_frame().reset_index(drop=True)\ndtump_tweets\n# plot dfe\nsns.countplot(x='cleaned_tweets', data=dtump_tweets, order=dtump_tweets.cleaned_tweets.value_counts().iloc[:10].index)\nplt.xlabel('10 Most common used words in Trump\\'s tweets: ')\nplt.ylabel('Frequency [%]')\nplt.xticks(rotation=70)","15fce2a7":"data['cleaned_insults'] = data['insult'].apply(lambda x: cleantext(x))\n#data['cleaned_tweets']\ndata.head(5)\n\n## use explode to expand the lists into separate rows\ndtump_insults = data.cleaned_insults.explode().to_frame().reset_index(drop=True)\ndtump_insults\n# plot dfe\nsns.countplot(x='cleaned_insults', data=dtump_insults, order=dtump_insults.cleaned_insults.value_counts().iloc[:10].index)\nplt.xlabel('10 Most common used insults in Trump\\'s tweets: ')\nplt.ylabel('Frequency [%]')\nplt.xticks(rotation=70)","942433af":"sns.countplot(x='target', data=data, order=data.target.value_counts().iloc[:10].index)\nplt.xlabel('Trump\\'s targets: ')\nplt.ylabel('Frequency [%]')\nplt.xticks(rotation=70)","c97cc011":"# Get a string of tweets \ntweet_text = \",\".join(tw.lower() for tw in data.tweet)\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=100, \n                      stopwords=stop_words,\n                      scale=5,\n                      background_color=\"white\").generate(tweet_text)\n\nplt.figure(figsize=(12,8))\nplt.title('Most repeated words in tweets',fontsize=15)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","2c12f846":"# Get a string of insults \ninsult_text = \",\".join(tw.lower() for tw in data.insult)\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=100, \n                      stopwords=stop_words,\n                      scale=5,\n                      background_color=\"white\").generate(insult_text)\n\nplt.figure(figsize=(12,8))\nplt.title('Most repeated words in tweets',fontsize=15)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","9664f0e3":"analyzer = SentimentIntensityAnalyzer() ","98fe84d7":"data['neg'] = data['tweet'].apply(lambda x:analyzer.polarity_scores(x)['neg'])\ndata['neu'] = data['tweet'].apply(lambda x:analyzer.polarity_scores(x)['neu'])\ndata['pos'] = data['tweet'].apply(lambda x:analyzer.polarity_scores(x)['pos'])\ndata['compound'] = data['tweet'].apply(lambda x:analyzer.polarity_scores(x)['compound'])\ndata","8795581f":"yearperiod = data.date.dt.to_period(\"Y\")\nygroup = data.groupby(yearperiod)\nygroup.mean()","2fb4cae9":"#data = data.sort_values('date', ascending=True)\nplt.plot(np.unique(yearperiod.values.astype(str)), ygroup.mean()['compound'])\nplt.title('Vader\\'s compound score per year',fontsize=15)\nplt.xlabel('Vader\\'s compound score')\nplt.ylabel('Year')\nplt.xticks(rotation='vertical')","69242cc7":"#data = data.sort_values('date', ascending=True)\nplt.plot(np.unique(yearperiod.values.astype(str)), ygroup.mean()['pos'])\nplt.title('Vader\\'s positive score per year',fontsize=15)\nplt.xlabel('Vader\\'s possitive score')\nplt.ylabel('Year')\nplt.xticks(rotation='vertical')","0a75589f":"#data = data.sort_values('date', ascending=True)\nplt.plot(np.unique(yearperiod.values.astype(str)), ygroup.mean()['neg'])\nplt.title('Vader\\'s negative score per year',fontsize=15)\nplt.xlabel('Vader\\'s negative score')\nplt.ylabel('Year')\nplt.xticks(rotation='vertical')","6ba1cd48":"data.date.dt.year.value_counts()","da08e87d":"# Pre-processing tweets\n\nNLP is an art to extract some information from the text. To do so, it's very important to get the data cleaned by lowercasing ALL your text data (although commonly overlooked, is one of the simplest and most effective form of text preprocessing), lemmatize our tweets, remove stopwords and useless punctuation and a bit of cleaning of the text will help us get more understandable data.","c563814b":"As we can see, more positive tweets have been tweeted since 2017. There's certaintly a peak of negativitiness from 2014 to 2015 which has been mantained untill 2017 but then the negativity decreases a lot and very abruptly. But as we can see in the next cell, the number of tweets per year have been increasing since 2014. Of course, there are no much 2021 tweets as we have just started the year, but in ay case there's a tendency of lowering the negativity.","7998b6d4":"As we can see, Trum has mostly targeted his tweets to the media","1481245f":"Citation for VADER:\n\nHutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.","72e0e2ca":"# General EDA\n\nTo understand better our tweets data we must visualize what is happening. First, I am going to check what's the lenght of the tweet's written by Trump. Then I am going to perform a deeper analysis by checking what's his 10 favourite words (most used words), his 10 most used insults and his main targets. This will give us a better idea of our data.","ce15b797":"So what does those results mean?\n<ul>\n    <li><b>pos<\/b>: The probability of the sentiment to be positive. <\/li> \n    <li><b>neu<\/b>: The probability of the sentiment to be neutral. <\/li>\n    <li><b>neg<\/b>: The probability of the sentiment to be negative. <\/li>\n    <li><b>compound<\/b>: The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). <\/li>\n<\/ul>\n\n\nNotice that the pos, neu and neg probabilities add up to 1. Also, the compound score is a very useful metric in case we want a single measure of sentiment. Typical threshold values are the following:\n<ul>\n    <li><b>positive<\/b>: compound score>=0.05<\/li>\n    <li><b>neutral<\/b>: compound score between -0.05 and 0.05<\/li>\n    <li><b>negative<\/b>: compound score<=-0.05 <\/li>\n<\/ul>\n\nNow with this results we'll see what's Trump's evolution by plotting the compound sentiment score by year.","a0b46ba3":"It is interesting to check that 3 (bad, never, fake) out of his 10 most used words are considered as \"negative\" and 2 (great and like) have a positive polarity. Let's keep digging on the insult part of the data.","fee02905":"# Wordcloud for Trump tweets\n\nWordClouds are a popular way of displaying how important words are in a collection of texts. Basically, the more frequent the word is, the greater space it occupies in the image. One of the uses of WordClouds is to help us get an intuition about what the collection of texts is about","351fa6c8":"# Sentiment analysis\n\n[VADER](https:\/\/pypi.org\/project\/vaderSentiment\/) (Valence Aware Dictionary and Sentiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. \n\nIt is used for sentiment analysis of text which has both the polarities i.e. positive\/negative. VADER is used to quantify how much of positive or negative emotion the text has and also the intensity of emotion.\n\n## Advantages\n\nHere are the advantages of using VADER which makes a lot of things easier:\n\n<ul>\n    <li>It does not require any training data. <\/li>\n    <li>It can very well understand the sentiment of a text containing emoticons, slangs, conjunctions, capital words, punctuations and much more. <\/li>\n    <li>It works excellent on social media text.<\/li>\n    <li> VADER can work with multiple domains. <\/li>\n<\/ul>"}}