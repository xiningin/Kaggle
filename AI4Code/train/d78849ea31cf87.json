{"cell_type":{"c8847f8e":"code","90ca6b5a":"code","0903989c":"code","be3cbea0":"code","23f9278e":"code","0fa974c7":"code","52a292bb":"code","6753f03c":"code","a7198b4d":"code","aa75141a":"code","5b04a486":"code","c66a7d7d":"code","257acae5":"code","c5288024":"code","ce1735fb":"code","2b242883":"code","fac5d3d0":"code","5bcb9de2":"code","458b5338":"code","ffb7f717":"code","4165048d":"code","66a27113":"code","e122cb9a":"code","93a353a0":"markdown","ae2bc17f":"markdown","0af56f77":"markdown","81c648c4":"markdown","2753e3bf":"markdown","24f2e64e":"markdown","b603161e":"markdown","300001e9":"markdown"},"source":{"c8847f8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","90ca6b5a":"from IPython.display import display\nfrom tqdm import tqdm\nfrom collections import Counter\nimport ast\n\n#Libraries for NLP\nimport nltk\n\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom textblob import TextBlob\nimport scipy.stats as stats\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\n\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\noutput_notebook()\n\n%matplotlib inline","0903989c":"#reading in the files\ndf_test = pd.read_csv('\/kaggle\/input\/topic-modeling-for-research-articles\/test.csv')\ndf_train = pd.read_csv('\/kaggle\/input\/topic-modeling-for-research-articles\/train.csv')\n\ndf_test.head()","be3cbea0":"df_train.head(15)","23f9278e":"#Getting abstracts from the test and train data\ntrain_text = df_train[\"ABSTRACT\"]\ntest_text = df_test[\"ABSTRACT\"]","0fa974c7":"#Function to get top words and counts\ndef get_top_n_words(n_top_words, count_vectorizer, text_data):\n    '''\n    returns a tuple of the top n words in a sample and their \n    accompanying counts, given a CountVectorizer object and text sample\n    '''\n    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n    vectorized_total = np.sum(vectorized_headlines, axis=0)\n    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n    \n    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n    for i in range(n_top_words):\n        word_vectors[i,word_indices[0,i]] = 1\n\n    words = [word[0].encode('ascii').decode('utf-8') for \n             word in count_vectorizer.inverse_transform(word_vectors)]\n\n    return (words, word_values[0,:n_top_words].tolist()[0])","52a292bb":"#Removing stop words and developing a list of top words in abstracts\ncv = CountVectorizer(stop_words='english')\nwords, word_val = get_top_n_words(n_top_words=15, count_vectorizer=cv, text_data=test_text)\nfig, ax = plt.subplots(figsize=(12, 8))\nax.bar(words, word_val)\nax.set_xticklabels(words, rotation='vertical');\nax.set_title('Top words (excluding stop words)');\nax.set_xlabel('Word');\nax.set_ylabel('Number of occurences');\nplt.show()\nprint(range(len(words)))","6753f03c":"#Making a class for both tokenization and lemmatization\nlemm = nltk.WordNetLemmatizer()\nclass LemmaCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))","a7198b4d":"# Storing all training text in a list\ntext = list(train_text)\n\n# Calling the overwritten count vectorizer\ntf_vectorizer = LemmaCountVectorizer(max_df=0.95, min_df=2, stop_words='english', \n                                     decode_error='ignore')\ntf = tf_vectorizer.fit_transform(text)","aa75141a":"#number of topics\nn_topics=10","5b04a486":"#Creating an LDA instance\nlda = LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method='online', \n                                learning_offset=50., random_state=0, verbose =0)\nlda.fit(tf)\n\n#Making an LDA topic matrix of the corpus\nlda_topic_matrix = lda.fit_transform(tf)","c66a7d7d":"# Define helper functions\ndef get_keys(topic_matrix):\n    '''\n    returns an integer list of predicted topic \n    categories for a given topic matrix\n    '''\n    keys = topic_matrix.argmax(axis=1).tolist()\n    return keys\n\ndef keys_to_counts(keys):\n    '''\n    returns a tuple of topic categories and their \n    accompanying magnitudes for a given list of keys\n    '''\n    count_pairs = Counter(keys).items()\n    categories = [pair[0] for pair in count_pairs]\n    counts = [pair[1] for pair in count_pairs]\n    return (categories, counts)","257acae5":"# Getting the categories and counts from the lda model\nlda_keys = get_keys(lda_topic_matrix)\nlda_categories, lda_counts = keys_to_counts(lda_keys)","c5288024":"# Function to get top words from each category\ndef get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n    '''\n    returns a list of n_topic strings, where each string contains the n most common \n    words in a predicted category, in order\n    '''\n    top_word_indices = []\n    for topic in range(n_topics):\n        temp_vector_sum = 0\n        for i in range(len(keys)):\n            if keys[i] == topic:\n                temp_vector_sum += document_term_matrix[i]\n        temp_vector_sum = temp_vector_sum.toarray()\n        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n        top_word_indices.append(top_n_word_indices)   \n    top_words = []\n    for topic in top_word_indices:\n        topic_words = []\n        for index in topic:\n            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n            temp_word_vector[:,index] = 1\n            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n        top_words.append(\" \".join(topic_words))         \n    return top_words","ce1735fb":"# Printing words from each topic\ntop_n_words = get_top_n_words(20, lda_keys, tf, tf_vectorizer)\n\nfor i in range(len(top_n_words)):\n    print(\"Topic {}:\\n \".format(i), top_n_words[i] + \"\\n\")","2b242883":"#Transforming the LDA topic matrix to 2 dimensions for plotting\ntsne_lda_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsne_lda_vectors = tsne_lda_model.fit_transform(lda_topic_matrix)","fac5d3d0":"# Define helper functions\ndef get_mean_topic_vectors(keys, two_dim_vectors):\n    '''\n    returns a list of centroid vectors from each predicted topic category\n    '''\n    mean_topic_vectors = []\n    for t in range(n_topics):\n        articles_in_that_topic = []\n        for i in range(len(keys)):\n            if keys[i] == t:\n                articles_in_that_topic.append(two_dim_vectors[i])    \n        \n        articles_in_that_topic = np.vstack(articles_in_that_topic)\n        mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)\n        mean_topic_vectors.append(mean_article_in_that_topic)\n    return mean_topic_vectors","5bcb9de2":"# Colourmap for the visualization\ncolormap = np.array([\n    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\" ])\ncolormap = colormap[:n_topics]","458b5338":"#Top 3 word from each topic to use in plot\ntop_3_words_lda = get_top_n_words(3, lda_keys, tf, tf_vectorizer)\n\n#Getting the mean of the topic vector for the visualization\nlda_mean_topic_vectors = get_mean_topic_vectors(lda_keys, tsne_lda_vectors)\n\n#Creating the t-SNE plot\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), plot_width=600, plot_height=600)\nplot.scatter(x=tsne_lda_vectors[:,0], y=tsne_lda_vectors[:,1], color=colormap[lda_keys])\n\nfor t in range(n_topics):\n    label = Label(x=lda_mean_topic_vectors[t][0], y=lda_mean_topic_vectors[t][1], \n                  text=top_3_words_lda[t], text_color=colormap[t])\n    plot.add_layout(label)\n\nshow(plot)","ffb7f717":"doc_topic = lda.transform(tf)\n\nfor n in range(20):\n    topic_most_pr = doc_topic[n].argmax()\n    print(\"Document #{} - topic: {}\\n\".format(n,topic_most_pr))","4165048d":"# making a dataframe from the document-topic matrix\ndoc_topic_df = pd.DataFrame(data=doc_topic)\ndoc_topic_df","66a27113":"# printing the top 'n' articles for each topic\nfor (columnName, columnData) in doc_topic_df.iteritems():\n    n = 3\n    print('Topic #', columnName)\n    sorted_topic = pd.DataFrame(data=columnData.values).sort_values(by=0, ascending=False)\n    sorted_topic.columns = [columnName]\n    print(sorted_topic[:n])\n    \n    # store IDs and titles of top articles in a dataframe\n    ids = sorted_topic[:n].index","e122cb9a":"#plotting the distribution of documents over each topic\nsns.set(rc={'figure.figsize':(10,5)})\ndoc_topic_df.idxmax(axis=1).value_counts().plot.bar(color='lightblue')\n\n#store the distributions in a dataframe\ndistribution = doc_topic_df.idxmax(axis=1).value_counts()\ndistribution","93a353a0":"My main goal is to\n1. Find the most relevant articles for each topic so they can be used as the links for further research\n2. maybe find another way to list the most relevant articles such as using rating or views (later)","ae2bc17f":"## LDA (Sklearn)","0af56f77":"Next steps:\n1. calculate article score\n2. extract articles from .json files","81c648c4":"# Exploratory Analysis","2753e3bf":"# Topic Modeling","24f2e64e":"## Advanced Analysis of LDA","b603161e":"# Setup","300001e9":"## Preprocessing for LDA"}}