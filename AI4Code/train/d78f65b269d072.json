{"cell_type":{"ffeef594":"code","997485b8":"code","f3817a80":"code","d5d2a77e":"code","58eacf1d":"code","0a506aec":"code","f5e9bb55":"code","d4013e74":"code","6665ac34":"code","0a213b61":"code","2986efac":"code","0d55d9d2":"code","ef1cb1e3":"code","1eb1a9dd":"code","7b02cf7f":"code","840ed437":"markdown","0e641ba3":"markdown","864063bc":"markdown","0eeaa9f5":"markdown","56d92315":"markdown","b0ff4337":"markdown","19ad6998":"markdown","20b5c6d8":"markdown","09356ee5":"markdown","9dbb397f":"markdown","ec79b7a2":"markdown","b308dfe2":"markdown","1c2e6c09":"markdown","8bdfc314":"markdown","f93ae947":"markdown","6be359c7":"markdown","c22e6b03":"markdown","bbf487ab":"markdown","2de6b283":"markdown","ca4b99b9":"markdown","30e00d01":"markdown"},"source":{"ffeef594":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnp.random.seed(123)","997485b8":"# gen some rvs\nX1 = np.arange(0, 1, 0.01)\nX2 = 3*X1 + 4                   # linearly dependant on X1 (increases w\/ X1) \nX3 = -0.3*X1 + 4                # linearly dependant on X1 (decreases w\/ X1)\nX4 = np.random.uniform(0,1,100) # independant of X1","f3817a80":"def covar(X1, X2):\n    \"\"\"\ud835\udc4b1  and  \ud835\udc4b2  are 1-D continous distributions of rvs\"\"\"\n    if len(X1) != len(X2): raise(\"Number of samples must be same!\")\n    \n    mean_X1 = np.mean(X1)\n    mean_X2 = np.mean(X2)\n    \n    return np.sum( (X1-mean_X1) * (X2-mean_X2) ) \/ len(X1) ","d5d2a77e":"# find respective covars\ncovarX1X2 = covar(X1,X2)\ncovarX1X3 = covar(X1,X3)\ncovarX1X4 = covar(X1,X4)\n\nprint(f\"covar(X1,X2): {covarX1X2:.2f} \\ncovar(X1,X3): {covarX1X3:.2f} \\ncovar(X1,X4): {covarX1X4:.2f}\")","58eacf1d":"fig, axarr = plt.subplots(1, 3)\nfig.set_size_inches(15,4)\n\naxarr[0].scatter(X1, X2)\naxarr[0].title.set_text(f\"X1-X2 -> positive covariance: {covarX1X2:.2f}\")\naxarr[0].set_xlabel(\"X1\")\naxarr[0].set_ylabel(\"X2\")\naxarr[0].legend([\"x-axis: X1 y-axis: X2\"])\n\naxarr[1].scatter(X1, X3)\naxarr[1].title.set_text(f\"X1-X3 -> negative covariance: {covarX1X3:.2f}\")\naxarr[1].set_xlabel(\"X1\")\naxarr[1].set_ylabel(\"X3\")\naxarr[1].legend([\"x-axis: X1 y-axis: X3\"])\n\naxarr[2].scatter(X1, X4)\naxarr[2].title.set_text(f\"X1-X4 -> zero covariance: {covarX1X4:.2f}\")\naxarr[2].set_xlabel(\"X1\")\naxarr[2].set_ylabel(\"X4\")\naxarr[2].legend([\"x-axis: X1 y-axis: X4\"])\n\nplt.legend()\nplt.show()","0a506aec":"def pcc(X1, X2):\n    \"\"\"\ud835\udc4b1  and  \ud835\udc4b2  are 1-D continous distributions of rvs\"\"\"\n    if len(X1) != len(X2): raise(\"Number of samples must be same!\")    \n    factor = np.std(X1) * np.std(X2)\n    return covar(X1, X2) \/ factor","f5e9bb55":"# find respective coavrs\npccX1X2 = pcc(X1,X2)\npccX1X3 = pcc(X1,X3)\npccX1X4 = pcc(X1,X4)\n\nprint(f\"pcc(X1,X2): {pccX1X2:.2f} \\npcc(X1,X3): {pccX1X3:.2f} \\npcc(X1,X4): {pccX1X4:.2f}\")","d4013e74":"# X5 -> less linearly dependant (increases slowly with X1)\nERRORS = np.random.uniform(-0.3,0.3, len(X1))\nX5 =  3*X1 + 4 + ERRORS\n\npccX1X5 = pcc(X1,X5)\nprint(f\"pcc(X1,X5): {pccX1X5}\")","6665ac34":"fig, axarr = plt.subplots(1,2)\nfig.set_size_inches(15,4)\n\naxarr[0].scatter(X1, X2)\naxarr[0].title.set_text(f\"stronger +ve covariance \\nPCC(X1-X2) = {pccX1X2:.2f}\")\naxarr[0].set_xlabel(\"X1\")\naxarr[0].set_ylabel(\"X2\")\naxarr[0].legend([\"x-axis: X1 y-axis: X2\"])\n\naxarr[1].scatter(X1, X5)\naxarr[1].title.set_text(f\"weaker +ve covariance \\nPCC(X1-X5) = {pccX1X5:.2f}\")\naxarr[1].set_xlabel(\"X1\")\naxarr[1].set_ylabel(\"X5\")\naxarr[1].legend([\"x-axis: X1 y-axis: X5\"])\n\nplt.show()","0a213b61":"def to_ranks(rvs):\n    \"\"\" returns ranks instead of raw rvs \"\"\"\n    ordered_idxs = rvs.argsort()\n    ranks = np.empty_like(ordered_idxs)\n    ranks[ordered_idxs] = np.arange(len(rvs))\n    return ranks\n\ndef srcc(X1, X2):\n    \"\"\"\ud835\udc4b1  and  \ud835\udc4b2  are 1-D continous distributions of rvs\"\"\"\n    if len(X1) != len(X2): raise(\"Number of samples must be same!\")  \n        \n    X1_to_ranks = to_ranks(X1)\n    X2_to_ranks = to_ranks(X2)    \n    \n    return pcc(X1_to_ranks, X2_to_ranks)","2986efac":"# generate monotonic function\nmonotoneX2 = np.array([4,5,5.1,5.2,7.01,8.95])\nmonotoneX1 = np.arange(0, len(monotoneX2), 1)\n\nplt.scatter(monotoneX1, monotoneX2)\nplt.plot(monotoneX1, monotoneX2)\nplt.title(\"Monotonic funtion\")\nplt.xlabel(\"monotoneX1\")\nplt.ylabel(\"monotoneX2\")\nplt.show()","0d55d9d2":"print(\"=\"*80, f\"\\nPCC(monotoneX1, monotoneX2): \\t{pcc(monotoneX1, monotoneX2)}\"   , \"\\n\"+\"=\"*80)\nprint(f\"SRCC(monotoneX1, monotoneX2): \\t{srcc(monotoneX1, monotoneX2)}\" , \"\\n\"+\"=\"*80)","ef1cb1e3":"# generate outlier data\nfeat1 = np.append(X1, np.random.uniform(0.8,1.0,10))\nfeat2 = np.append(X5, np.random.uniform(4.0,4.5,10))\n\n# plot\nplt.scatter(feat1, feat2)\n\nplt.title(\"Features w\/ outliers\")\nplt.xlabel(\"feat1\")\nplt.ylabel(\"feat2\")\nplt.show()","1eb1a9dd":"print(\"=\"*80, f\"\\nPCC(feat1, feat2): \\t{pcc(feat1, feat2):.3f}\"   , \"\\n\"+\"=\"*80)\nprint(f\"SRCC(feat1, feat2): \\t{srcc(feat1, feat2):.3f}\" , \"\\n\"+\"=\"*80)","7b02cf7f":"print(f\"PCC(X1,X2): {pcc(X1, X2):.3f} \\tSRCC(X1,X2): {srcc(X1, X2):.3f}\")\nprint(f\"PCC(X1,X5): {pcc(X1, X5):.3f} \\tSRCC(X1,X5): {srcc(X1, X5):.3f}\")","840ed437":"> *Cannot show relationship for all polynomial curves but works well for **monotonic relationships** (Better than PCC in this way)*\n\n\n**Range:** $[-1,1]$","0e641ba3":"**Implement SRCC**","864063bc":"Standardized w\/ std-dev: $\\text{standard deviation} = \\sigma(X) = \\sqrt{var(X)}$\n\n$$\\rho = \\frac{\\text{covariance}(X1, X2)}{\\sigma_{X1} \\sigma_{X2}} \\Rightarrow \\frac{1}{NumOfSamples} * \\frac{\\sum{{(\\mu_1 - x_1})(\\mu_2 - x_2)}}{\\sigma_{X1} \\sigma_{X2}}$$\n\n**Range:** $[-1,1]$\n\n**How to Interpret**\n\n|Observation|Interpretation|\n|---|---|\n|more towards +1| As one increases other increases too |\n| zero | No relationship|\n| more towards -1 | As one increases other decreases |\n\n**NOTE**\n\n- Works only for **Linear relationships**. Will not work for **curves**. \n- Robust to outliers\n- Does not vary with slope of linear relationship (see second row of figure from wikipedia)\n    - Can add $tan(\\theta)$ as a factor if want to vary it w\/ slope (only the range of PCC will change)","0eeaa9f5":"- SRCC 1.0 mean there is a strong relationship (It checks both linear as well as monotonic reltionship) <br>\n\n- PCC 0.92 shows it's inability to see\/capture relationship b\/w the two features compared to SRCC. (It checks only linear relationship - which is present here but monotonic relationship is also present which is ignored)","56d92315":"# Finding Relationship Between Two Features\n\n1. Co-variance\n2. Pearson Correlation Coefficient\n3. Spearman Rank Correlation Coefficient","b0ff4337":"## 01. Co-Variance\n\n**Variance** is given by formula, $$var = \\sigma^{2} = \\frac{1}{NumOfSamples} \\sum{{(\\mu - x_i)}^2}$$\n> It indicates absolute **spread of the distribution** in squared units -- How much rvs are deviating from mean\n\n\n**Co-variance($X_1 , X_2$):**\n\n- $X1$ and $X2$ are continous distributions of rvs  \n- How much $X1$ and $X2$ together deviate from their two means  \n\n$$\\frac{1}{NumOfSamples} \\sum{{(\\mu_1 - x_1})(\\mu_2 - x_2)}$$\n\n> *Note: property of being \"squared\" disappears as X1 and X2 are different*\n\n**Range:** $(-\\infty, +\\infty)$\n\n**How to Interpret**\n\n|Observation|Interpretation|\n|---|---|\n|more positive| As one increases other increases too |\n| zero | No relationship|\n| more negative | As one increases other decreases |\n\n**Con:** It can **not** be used for comparison as *More* in more positive \/ more negative is not defined.\n> Needs to be *standardized*","19ad6998":"[source: wikipedia](https:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient)\n\n> *It cannot find polynomial relationships*","20b5c6d8":"**NOTE**\n\n- It is a comparison coefficient just like PCC\n    - When the data are roughly elliptically distributed and there are no prominent outliers, the SRCC and PCC give similar values.\n- SRCC **Not** robust to outliers! (Compared to PCC)\n    - because, ranks of outliers affect SRCC significantly\n- Instead of using raw rvs, **use their ranks instead in PCC**\n    - It doesn't care about magnitude\/direction of rvs -- only cares about their order(ranks). Hence, captures monotonic information\n ","09356ee5":"**B. Ability to ignore outliers**\n\n> **NOT** better than PCC","9dbb397f":"> Standardized covariance --> suitable for comparison!","ec79b7a2":"**Implement covariance based on above formula**","b308dfe2":"**Can we use SRCC instead of PCC for non-monotonic(but linearly related) fuctions as well?**\n\n- Check with X1-X2 and X1-X5 where PCC worked just fine","1c2e6c09":"## 02. Pearson Correlation Coeff. (PCC or $\\rho$) \n\n","8bdfc314":"**Implement PCC**","f93ae947":"#### Changes in PCC","6be359c7":"## 03. Spearman [Rank] Correlation Coefficient (Spearman's $\\rho$)","c22e6b03":"As PCCs and SRCCs are almost same, we use SRCC instead of PCC for non-monotonic(but linearly related) fuctions as well. We have some significant advantages\/disadvantages of using SRCC instead of PCC (discussed above)","bbf487ab":"**A. Ability to detect monotonic functions**\n\n> Better than PCC","2de6b283":"![image.png](attachment:image.png)","ca4b99b9":"> Note how PCC is more robust to outliers compared to SRCC","30e00d01":"> See how PCC can be used for comparison of relationship b\/w X1-X2 and X1-X5 features"}}