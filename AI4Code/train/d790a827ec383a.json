{"cell_type":{"9a939c7f":"code","3e858e26":"code","fb4a3d92":"code","1b196c47":"code","53ad752e":"code","50a526a3":"code","4b8a6952":"code","bb4c6d4f":"code","64ea1b38":"code","216029aa":"code","913eef8f":"code","a6cfd7bc":"code","d0e52a5d":"code","08152b9f":"code","9b66a0c5":"code","a9d974cd":"code","a562749c":"code","c10648a8":"code","3b5a2975":"code","2a2eae72":"code","7e13f3f1":"code","56a68acf":"code","4a8af2f1":"code","a6075411":"code","324f96ad":"code","2589a6ab":"code","eb98d6da":"code","9430fc6b":"code","a38ad3f3":"code","02219bbe":"code","1697457a":"code","22a92942":"code","231c08b6":"code","190b57b9":"code","d5a56b97":"code","74b62e0e":"code","53d9c741":"code","54a273ab":"code","94f70a67":"code","0fb8242d":"code","c24c30b5":"code","b045f4ad":"code","e0757b3d":"code","1fdc59fd":"code","86be46b2":"code","8537ef84":"code","c12206a8":"code","24d51211":"code","3ffdb72c":"code","9f7267a1":"code","56a2229d":"code","248b2766":"code","acd3981a":"code","e10c1320":"code","0e41b01a":"code","3d1422df":"code","fad95ca2":"code","7b4f1d1e":"code","d68064ee":"code","44d87f0a":"code","ed905b33":"code","1866ee06":"code","01b2e236":"code","9cc8ff65":"code","c737ba1e":"code","28e84d08":"code","ad612b54":"code","996d4ac7":"code","b7d27552":"code","f7b1d1f2":"code","63183de0":"code","57f324ac":"code","c299a9c6":"code","78d118b4":"code","08e71ee3":"code","2da71d1f":"code","9856a751":"code","c7ecfebd":"code","5c20fcc0":"code","f91914d6":"code","663ec84d":"code","3e7063f1":"code","6d6138d8":"code","7e3ec8a5":"code","86fd2b30":"code","beee2412":"code","02eac197":"code","f33f0363":"code","44549525":"code","93d34b38":"code","59a08d2f":"code","e28f6a4c":"code","e357a173":"markdown","53962cfe":"markdown","6a791067":"markdown","df66bf65":"markdown","41c8e1d1":"markdown","0ef0abd8":"markdown","ca82715e":"markdown","e1281e94":"markdown","285ada5f":"markdown","db573279":"markdown","c97cedd4":"markdown","522ba15b":"markdown","fc241e13":"markdown","cf24a209":"markdown","495ac354":"markdown","46afa591":"markdown","b88bf29d":"markdown","f6acc2a7":"markdown","893f3144":"markdown","96601243":"markdown","6fdf25b7":"markdown","507b4758":"markdown","4abc257a":"markdown","3ef93a26":"markdown","b5efd22f":"markdown","abd8f36d":"markdown","d832cf9d":"markdown"},"source":{"9a939c7f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e858e26":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\",index_col='Id')\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\",index_col='Id')\n\nsample_submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\",index_col='Id')\nwith open('..\/input\/house-prices-advanced-regression-techniques\/data_description.txt') as f:\n    data_description = f.readlines()","fb4a3d92":"train.to_csv('train.csv', index=True, index_label='Id')\ntest.to_csv('test.csv', index=True, index_label='Id')\n\nsample_submission.to_csv('sample_submission.csv', index=True, index_label='Id')\n\ntextfile = open(\"data_description.txt\", \"w\")\nfor element in data_description:\n    textfile.write(element + \"\\n\")\ntextfile.close()","1b196c47":"train.head()","53ad752e":"train.info()","50a526a3":"test.info()","4b8a6952":"train.describe()","bb4c6d4f":"test.describe()","64ea1b38":"# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"end_to_end_project\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)","216029aa":"%matplotlib inline\nimport matplotlib.pyplot as plt\ntrain.hist(bins=50, figsize=(20,15))\nsave_fig(\"attribute_histogram_plots\")\nplt.show()","913eef8f":"train.plot(kind=\"scatter\", figsize=(30,20), x=\"Neighborhood\", y=\"SalePrice\", alpha=0.4)\nplt.legend()\nsave_fig(\"housing_prices_scatterplot\")","a6cfd7bc":"corr_matrix = train.corr()","d0e52a5d":"corr_matrix[\"SalePrice\"].sort_values(ascending=False)","08152b9f":"# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"SalePrice\", \"OverallQual\", \"GrLivArea\",\n              \"GarageCars\", \"GarageArea\", \"TotalBsmtSF\",\n              \"1stFlrSF\", \"FullBath\",\"TotRmsAbvGrd\", \"YearBuilt\"]\nscatter_matrix(train[attributes], figsize=(20, 10))\nsave_fig(\"scatter_matrix_plot\")","9b66a0c5":"housing = train.copy()","a9d974cd":"housing_tr = housing.copy()\n","a562749c":"housing_tr.head()","c10648a8":"housing_labels = housing_tr[\"SalePrice\"].copy()\nhousing_tr = housing_tr.drop(\"SalePrice\", axis=1) # drop labels for training set\n","3b5a2975":"sample_incomplete_rows = housing_tr[housing_tr.isnull().any(axis=1)]\nsample_incomplete_rows.info()","2a2eae72":"housing_num = housing_tr.select_dtypes(include=[np.number])","7e13f3f1":"housing_cat = housing_tr.select_dtypes(include=['object'])","56a68acf":"housing_num.head()","4a8af2f1":"housing_cat.head()","a6075411":"housing_cat.columns.values","324f96ad":"list(housing_cat)","2589a6ab":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")","eb98d6da":"imputer.fit(housing_num)","9430fc6b":"imputer.statistics_","a38ad3f3":"housing_num.median().values","02219bbe":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","1697457a":"from sklearn.preprocessing import OneHotEncoder\ncat_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"constant\")),\n        ('encoder', OneHotEncoder(handle_unknown='ignore')),\n#         ('std_scaler', StandardScaler()),\n    ])\nhousing_cat_tr = cat_pipeline.fit_transform(housing_cat)","22a92942":"housing_cat_tr","231c08b6":"# housing_tr.info()","190b57b9":"\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = list(housing_cat)\n\nfull_pipeline = ColumnTransformer(transformers=[\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", cat_pipeline, cat_attribs),\n#         (\"cat2\", OneHotEncoder(handle_unknown='ignore'), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing_tr)","d5a56b97":"pd.DataFrame(housing_prepared).info()","74b62e0e":"housing_prepared","53d9c741":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","54a273ab":"# let's try the full preprocessing pipeline on a few training instances\nsome_data = housing_tr.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\n\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))","94f70a67":"print(\"Labels:\", list(some_labels))","0fb8242d":"from sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","c24c30b5":"from sklearn.metrics import mean_absolute_error\n\nlin_mae = mean_absolute_error(housing_labels, housing_predictions)\nlin_mae","b045f4ad":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(housing_prepared, housing_labels)","e0757b3d":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","1fdc59fd":"len(housing_predictions)","86be46b2":"from sklearn.ensemble import GradientBoostingRegressor\n\nreg = GradientBoostingRegressor(n_estimators=200, learning_rate=0.01, max_depth=5, random_state=42,\n        ).fit(housing_prepared, housing_labels)","8537ef84":"housing_predictions = reg.predict(housing_prepared)\ngradient_mse = mean_squared_error(housing_labels, housing_predictions)\ngradient_rmse = np.sqrt(gradient_mse)\ngradient_rmse","c12206a8":"from xgboost import XGBRegressor\nxgb = XGBRegressor(objective = \"reg:squarederror\", max_depth=8, n_estimators=360, seed=0, booster = \"dart\", rate_drop = 0.1,\n         skip_drop = 0.5).fit(housing_prepared, housing_labels)","24d51211":"housing_predictions = xgb.predict(housing_prepared)\nxgb_mse = mean_squared_error(housing_labels, housing_predictions)\nxgb_rmse = np.sqrt(xgb_mse)\nxgb_rmse\n","3ffdb72c":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\n","9f7267a1":"param_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [30, 100, 150, 200], 'max_features': [4, 5, 6, 8, 9]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","56a2229d":"grid_search.best_params_","248b2766":"grid_search.best_estimator_","acd3981a":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","e10c1320":"pd.DataFrame(grid_search.cv_results_)","0e41b01a":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\nfrom scipy.stats import uniform\n\n","3d1422df":"# # reg = GradientBoostingRegressor(n_estimators=200, learning_rate=0.01, max_depth=5, random_state=42,\n# param_distribs = {\n#         'n_estimators': randint(low=50, high=200),\n#         'max_depth': randint(low=1, high=12),\n#     }\n\n# gradient_reg = GradientBoostingRegressor(random_state=42)\n# # train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n# grid_gradient_search = RandomizedSearchCV(gradient_reg, param_distributions=param_distribs,\n#                                 n_iter=200, cv=5, scoring='neg_mean_squared_error', random_state=42)\n# grid_gradient_search.fit(housing_prepared, housing_labels)","fad95ca2":"# cvres = grid_gradient_search.cv_results_\n# for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n#     print(np.sqrt(-mean_score), params)","7b4f1d1e":"param_distribs = {\n        'n_estimators': randint(low=90, high=150),\n        'max_features': randint(low=4, high=12),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=100, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)","d68064ee":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","44d87f0a":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","ed905b33":"# extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\n# cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n# cat_one_hot_attribs = list(cat_encoder.categories_[0])\n# attributes = num_attribs + cat_one_hot_attribs\n# sorted(zip(feature_importances, attributes), reverse=True)","1866ee06":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(housing_prepared, housing_labels)","01b2e236":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","9cc8ff65":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","c737ba1e":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)","28e84d08":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","ad612b54":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)","996d4ac7":"scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\npd.Series(np.sqrt(-scores)).describe()","b7d27552":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel=\"linear\")\nsvm_reg.fit(housing_prepared, housing_labels)\nhousing_predictions = svm_reg.predict(housing_prepared)\nsvm_mse = mean_squared_error(housing_labels, housing_predictions)\nsvm_rmse = np.sqrt(svm_mse)\nsvm_rmse","f7b1d1f2":"housing_test = test.copy()\n# housing_test = test.dropna(thresh=1458, axis=1)\n# housing_test = housing_test.drop(\"Electrical\", axis=1)\nhousing_test.head()","63183de0":"housing_test.info()","57f324ac":"len(housing_test)","c299a9c6":"final_model = xgb #rnd_search.best_estimator_\n\nX_test = housing_test.copy()\n# y_test = test[\"SalePrice\"].copy()\n\n# X_test_prepared = full_pipeline.transform(X_test) #old\n","78d118b4":"# housing_labels","08e71ee3":"full_pipeline_with_predictor = Pipeline([\n        (\"preparation\", full_pipeline),\n        (\"final_model\", final_model)\n    ])\n\n# Training\nfull_pipeline_with_predictor.fit(housing_tr, housing_labels)\n# Testing\nfinal_predictions = full_pipeline_with_predictor.predict(X_test)","2da71d1f":"# final_predictions = final_model.predict(X_test_prepared) #old\n\nfinal_predictions","9856a751":"# final_mse = mean_squared_error(y_test, final_predictions)\n# final_rmse = np.sqrt(final_mse)","c7ecfebd":"# predictions = tree_reg.predict(t)","5c20fcc0":"# final_predictions=np.mean(np.column_stack(predictions), axis=1)","f91914d6":"my_model = full_pipeline_with_predictor","663ec84d":"import joblib\njoblib.dump(my_model, \"my_model.pkl\") # DIFF\n#...\nmy_model_loaded = joblib.load(\"my_model.pkl\") # DIFF","3e7063f1":"from scipy.stats import geom, expon\ngeom_distrib=geom(0.5).rvs(10000, random_state=42)\nexpon_distrib=expon(scale=1).rvs(10000, random_state=42)\nplt.hist(geom_distrib, bins=50)\nplt.show()\nplt.hist(expon_distrib, bins=50)\nplt.show()","6d6138d8":"from sklearn.base import BaseEstimator, TransformerMixin\n\ndef indices_of_top_k(arr, k):\n    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n\nclass TopFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_importances, k):\n        self.feature_importances = feature_importances\n        self.k = k\n    def fit(self, X, y=None):\n        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n        return self\n    def transform(self, X):\n        return X[:, self.feature_indices_]","7e3ec8a5":"k = 5","86fd2b30":"# feature_importances","beee2412":"top_k_feature_indices = indices_of_top_k(feature_importances, k)\ntop_k_feature_indices","02eac197":"# np.array(attributes)[top_k_feature_indices]","f33f0363":"attributes","44549525":"sorted(zip(feature_importances, attributes), reverse=True)[:k]","93d34b38":"t=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nDf = pd.DataFrame({'Id':t['Id'].apply(int), 'SalePrice':(final_predictions)})","59a08d2f":"Df.to_csv('submission.csv', index=False)","e28f6a4c":"Df","e357a173":"Let's double check that these are indeed the top k features:","53962cfe":"## Evaluate Your System on the Test Set","6a791067":"# Select and Train a Model","df66bf65":"## NA check","41c8e1d1":"Let's define the number of top features we want to keep:","0ef0abd8":"# Data exploration","ca82715e":"## Grid search","e1281e94":"## Adding a transformer in the preparation pipeline to select only the most important attributes.","285ada5f":"Now let's look for the indices of the top k features:","db573279":"# Fine-tune","c97cedd4":"## Analyze the Best Models and Their Errors","522ba15b":"# Submission","fc241e13":"# Transformation Pipelines","cf24a209":"Linear regression baseline","495ac354":"## Randomized Search","46afa591":"## Model persistence using joblib","b88bf29d":"# Cross-Validation","f6acc2a7":"## A full pipeline with both preparation and prediction","893f3144":"Note: this feature selector assumes that you have already computed the feature importances somehow (for example using a `RandomForestRegressor`). You may be tempted to compute them directly in the `TopFeatureSelector`'s `fit()` method, however this would likely slow down grid\/randomized search since the feature importances would have to be computed for every hyperparameter combination (unless you implement some sort of cache).","96601243":"# Fine-Tune Your Model","6fdf25b7":"Let's look at the score of each hyperparameter combination tested during the grid search:","507b4758":"## Stratified sampling?","4abc257a":"SVR","3ef93a26":"## GradientBoostingClassifier","b5efd22f":"# Dataset split","abd8f36d":"# Final model","d832cf9d":"## DecisionTreeRegressor"}}