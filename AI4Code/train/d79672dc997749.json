{"cell_type":{"eb3edbcf":"code","91a863d9":"code","f71ae2e5":"code","a6f2ce74":"code","233163df":"code","43252aaa":"code","b4c86884":"code","3dbb43c9":"code","f13e67bc":"code","1d072746":"code","594e27b3":"code","0392754c":"code","1da2b508":"code","399f1c79":"code","34645fd9":"code","be8f5250":"code","240355d7":"code","40e952ad":"code","cc7cb08b":"code","ff907fd5":"code","0a35b97a":"code","0c07c6f8":"code","de91b39a":"code","3a271e91":"code","6adefefb":"code","21ae9475":"code","ebf2a1c1":"code","25afb722":"code","aa981579":"code","3b4e71bc":"code","7d15588f":"code","71556260":"code","3c41d733":"code","24950624":"code","7612e923":"code","e27909f4":"code","a0782155":"code","7262ce6e":"code","9c729887":"code","4e4ec671":"code","025eb37c":"code","8157a545":"code","4860a0df":"code","5c32b231":"code","75657228":"code","1f84271f":"code","a2ab2d01":"code","341a25ac":"code","7a2c50da":"code","37225c0b":"code","37319571":"code","bf4692c0":"code","4d30caef":"code","872a0ca7":"code","1785dbd6":"code","6b534e91":"code","44f40080":"code","898279ef":"code","b3f01eda":"code","41ec4be0":"code","78c4f737":"code","b2dbb8d8":"code","9a1712d4":"code","a6d613ce":"code","8b71c54b":"code","a968c389":"code","c4e71703":"code","a11103af":"code","8e8e245a":"code","a567b33e":"code","84ff866f":"code","d9a3b885":"code","6b0d71d8":"code","947dc96f":"code","49fc7067":"code","6f9450c0":"code","626f03de":"code","df40e2eb":"code","b208e172":"code","864f5f9f":"code","c7d86b51":"code","08b23ed0":"code","3ec592eb":"code","f3163756":"code","c06f94e9":"code","f04dcc28":"code","968bc610":"code","41c2fd62":"code","c372a889":"code","27e555e7":"code","ab433969":"code","ecd1a811":"code","a0cc82c5":"code","9128c70e":"code","d76a5fb3":"code","7c24b718":"code","74ac20e0":"code","a85f2743":"code","2c8e5652":"code","2b2b48dc":"code","cd8d4fc6":"code","3294c8e2":"code","169191c8":"code","370a24c4":"code","3ea516d9":"code","9702556a":"code","314f3cac":"code","cbe6f37e":"code","813b0e94":"code","dfcebac6":"code","1b8a626d":"code","89f007e5":"code","36de9e8e":"code","a965ff91":"code","43aaafb0":"code","c55abad8":"code","da1e2353":"code","daf5df55":"code","36ddc2ef":"code","552188ef":"code","b3720697":"code","61e40b8e":"code","dbd00ef7":"code","e5ded6e7":"code","e973344b":"code","c7234661":"code","51f223b1":"code","381262ca":"code","ee92554f":"code","da8caf1d":"code","7c39e628":"code","74bb1555":"code","e824cd21":"code","cb1b8098":"code","aeabed44":"code","4f9ca7bf":"code","44bf8168":"code","0c9a10b0":"code","4caac1bc":"code","6c9921dc":"code","c4f49b13":"code","e7c9d026":"code","e4547bc0":"code","bddffc06":"code","af1b263a":"code","a5eb3a9a":"code","c3ab81b6":"code","5ebe36fe":"code","354ea772":"code","2dad5585":"markdown","74572ce1":"markdown","bb331251":"markdown","dc563210":"markdown","4df379f0":"markdown","c7716beb":"markdown","50d36dd0":"markdown","d512b804":"markdown","c9910988":"markdown","65c6764a":"markdown","00365868":"markdown","bfab7abf":"markdown","55bc9816":"markdown","e1accdd8":"markdown","b96b0d88":"markdown","36f9ecf5":"markdown","7debae10":"markdown","2281c56b":"markdown","0215e10c":"markdown","3f1118b2":"markdown","013a7334":"markdown","75f94824":"markdown","d50f67f1":"markdown","c704c60b":"markdown","5fcab83d":"markdown","fe85e095":"markdown","fb7f90e3":"markdown","8cd38053":"markdown","e4d87eb7":"markdown","67c54143":"markdown","69ef79e1":"markdown","d44a24b6":"markdown","834bd7e5":"markdown","1172157e":"markdown","896154b1":"markdown","beaa4f91":"markdown","2b3086e1":"markdown","f137120e":"markdown","1ebdd3f6":"markdown","34965a38":"markdown","a589c089":"markdown","7cc1a790":"markdown","4e07dc82":"markdown","8dfc6da1":"markdown","fcee7979":"markdown","21ea5e8e":"markdown","5335e0d6":"markdown","5358e383":"markdown","8acdace2":"markdown","f6283d95":"markdown","482e2926":"markdown","81f77247":"markdown","67aaad6e":"markdown","bdc52041":"markdown","4a9ac446":"markdown","e9a35922":"markdown","a1050792":"markdown","5203dfc8":"markdown","3820a397":"markdown","7c8b8293":"markdown","04275937":"markdown"},"source":{"eb3edbcf":"import matplotlib.pyplot as plt \nimport time\nimport numpy as np\nimport seaborn as sns \nimport time\n","91a863d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.model_selection import cross_val_predict , KFold, cross_val_score\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom math import log1p\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom datetime import datetime\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom xgboost import plot_importance\n\n# Any results you write to the current directory are saved as output.","f71ae2e5":"#import sys\n#sys.path.append('..\/input\/feature-selector\/')\n#from feature_selector import FeatureSelector","a6f2ce74":"train= pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\ntest = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])\nprint(\"{} observations and {} features in train set.\".format(train.shape[0],train.shape[1]))\nprint(\"{} observations and {} features in test set.\".format(test.shape[0],test.shape[1]))","233163df":"print(len(list(test.keys())))\nprint(len(list(train.keys())))\n","43252aaa":"history = pd.read_csv(\"..\/input\/historical_transactions.csv\", low_memory=True)","b4c86884":"#history.head(n=5)","3dbb43c9":"#fig, ax = plt.subplots(figsize=(12, 3))\n#sns.boxplot(x='target', data=train)","f13e67bc":"#fig, ax = plt.subplots(figsize=(16, 5))\n#sns.distplot(train.target, ax=ax)","1d072746":"train['feature_1'].unique()","594e27b3":"def missing_data_function(frame):\n\n    total = frame.isnull().sum().sort_values(ascending=False)\n    percent = (frame.isnull().sum()*100 \/ frame.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data","0392754c":"def reduce_mem_usage_func(df):\n    \"\"\" Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n        iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n","1da2b508":"history=reduce_mem_usage_func(history)","399f1c79":"history.reset_index(inplace=True)","34645fd9":"new_transactions = pd.read_csv('..\/input\/new_merchant_transactions.csv',  low_memory=True)","be8f5250":"new_transactions.head(n=5)","240355d7":"new_transactions=reduce_mem_usage_func(new_transactions)","40e952ad":"#time.sleep(30)","cc7cb08b":"new_transactions.reset_index(inplace=True)","ff907fd5":"#history.head()","0a35b97a":"def merge_train_test(train, test , df ):\n    \n    train=pd.merge(left=train , right=df, how = 'left', on ='card_id')\n    test=pd.merge(left=test , right=df, how = 'left', on ='card_id')\n    return train, test\n    \n","0c07c6f8":"history['purchase_date'] = pd.to_datetime(history['purchase_date'])","de91b39a":"new_transactions['purchase_date'] = pd.to_datetime(new_transactions['purchase_date'])","3a271e91":"def features(df):\n    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n    df['feature_mean'] = df['feature_sum']\/3\n    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n    return df \n\n\n\n    \n    \n    \n    ","6adefefb":"train=features(train)","21ae9475":"test=features(test)","ebf2a1c1":"def deal_missing(df):\n    df['category_2'].fillna(1.0,inplace=True)\n    df['category_3'].fillna('A',inplace=True)\n    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n    df['installments'].replace(-1, np.nan,inplace=True)\n    df['installments'].replace(999, np.nan,inplace=True)\n    df['purchase_amount'] = df['purchase_amount'].apply(lambda x: min(x, 0.8))\n    return df","25afb722":"history=deal_missing(history)","aa981579":"new_transactions=deal_missing(new_transactions)","3b4e71bc":"def mapping(df):\n    df['authorized_flag'] = df['authorized_flag'].map({'Y': 1, 'N': 0}).astype('int8')\n    df['category_1'] = df['category_1'].map({'Y': 1, 'N': 0}).astype('int8')\n    df['category_3'] = df['category_3'].map({'A':0, 'B':1, 'C':2}).astype('int8')\n    return df","7d15588f":"history=mapping(history)","71556260":"new_transactions=mapping(new_transactions)","3c41d733":"def new_features(df):\n    df['month'] = df['purchase_date'].dt.month.astype('int8')\n    df['day'] = df['purchase_date'].dt.day.astype('int8')\n    df['hour'] = df['purchase_date'].dt.hour.astype('int8')\n    df['weekofyear'] = df['purchase_date'].dt.weekofyear.astype('int8')\n    df['weekday'] = df['purchase_date'].dt.weekday.astype('int8')\n    df['weekend'] = (df['purchase_date'].dt.weekday >=5).astype('int8')\n    time.sleep(60)\n    df['price'] = df['purchase_amount'] \/ (1+df['installments'])\n    df['month_diff'] = (((datetime.today() - df['purchase_date']).dt.days)\/\/30).astype('int8')\n    df['month_diff'] += history['month_lag']\n    time.sleep(60)\n    df['Christmas_Day_2017']=(pd.to_datetime('2017-12-25')-df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0).astype('int8')\n    df['Mothers_Day_2017']=(pd.to_datetime('2017-06-04')-df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0).astype('int8')\n    df['Valentine_Day_2017']=(pd.to_datetime('2017-06-12')-df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0).astype('int8')\n    df['Black_Friday_2017']=(pd.to_datetime('2017-11-24') - df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0).astype('int8')\n    df['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0).astype('int8')\n    time.sleep(60)\n    df['activity_100_days']=(df['purchase_date'].max()-df['purchase_date']).dt.days.apply(lambda x: 1 if x > 0 and x < 100 else 0).astype('int8')\n    df['duration'] = df['purchase_amount']*df['month_diff']\n    df['amount_month_ratio'] = df['purchase_amount']\/df['month_diff']\n    df['price'] = df['purchase_amount'] \/ df['installments']\n\n    return df \n#","24950624":"history=new_features(history)","7612e923":"#history=reduce_mem_usage_func(history)","e27909f4":"history.head(n=5)","a0782155":"new_transactions=new_features(new_transactions)","7262ce6e":"for col in ['category_2','category_3']:\n    history[col+'_mean'] = history.groupby([col])['purchase_amount'].transform('mean')\n    new_transactions[col+'_mean'] = new_transactions.groupby([col])['purchase_amount'].transform('mean')","9c729887":"def aggregation(frame,name):\n    agg_func = {\n        'subsector_id':['nunique']\n        ,'merchant_id':['nunique']\n        ,'merchant_category_id':['nunique']\n        ,'month':['nunique', 'mean', 'min', 'max']\n        ,'hour':['nunique', 'mean', 'min', 'max']\n        ,'weekofyear':['nunique', 'mean', 'min', 'max']\n        ,'weekday':['nunique', 'mean', 'min', 'max']\n        ,'day':['nunique', 'mean', 'min', 'max']\n        ,'purchase_amount': ['sum','max','min','mean','var','skew']\n        ,'installments' :['sum','max','mean','var','skew']\n        ,'purchase_date' : ['max','min']\n        ,'month_lag' :['max','min','mean','var','skew']\n        ,'month_diff':['mean','var','skew']\n        ,'weekend':['mean']\n        ,'month':['mean', 'min', 'max']\n        ,'weekday':['mean', 'min', 'max']\n        ,'category_1' :['mean','std']\n        ,'category_2': ['mean','std']\n        ,'category_3': ['mean','std']\n        ,'card_id':['size','count']\n        ,'price': ['mean','max','min','var']\n        ,'Christmas_Day_2017': ['mean']\n        ,'Black_Friday_2017': ['mean']\n        ,'Mothers_Day_2018':['mean']\n        ,'activity_100_days':['mean']\n        ,'category_2_mean':['mean']\n        ,'category_3_mean':['mean']\n        ,'duration' : ['mean','min','max','var','skew']\n        ,'amount_month_ratio': ['mean','min','max','var','skew']\n        }\n    agg_new_trans = frame.groupby(['card_id']).agg(agg_func)\n    agg_new_trans.columns = [str(name) + '_'.join(col).strip() \n                           for col in agg_new_trans.columns.values]\n    agg_new_trans.reset_index(inplace=True)\n    return agg_new_trans\n    \n    \n\n\n","4e4ec671":"time.sleep(60)","025eb37c":"#df = aggregation(history,'history_')","8157a545":"train , test = merge_train_test(train, test , aggregation(history,'history_') )","4860a0df":"time.sleep(30)","5c32b231":"time.sleep(20)","75657228":"train , test = merge_train_test(train, test ,aggregation(new_transactions,'new_') )","1f84271f":"new_transactions.head(n=10)","a2ab2d01":"new_transactions.keys()","341a25ac":"#train=train.drop(columns=['amount_mean_x', 'amount_std_x', 'amount_max_x','amount_min_x', 'amount_sum_x','amount_mean_y', 'amount_std_y', 'amount_max_y',\n                         # 'amount_min_y', 'amount_sum_y'])","7a2c50da":"train.keys()","37225c0b":"#istory['installments']=history['installments'].replace({-1:0,999:0}, inplace=True)","37319571":"def shopping_days(df, name):\n    days_of_shopping=df.groupby('card_id')['purchase_date'].max()-df.groupby('card_id')['purchase_date'].min()\n    days_of_shopping=days_of_shopping.reset_index()\n    days_of_shopping.columns = ['card_id',str(name)+'_'+'shopping_days']\n    days_of_shopping[str(name)+'_'+'shopping_days']=days_of_shopping[str(name)+'_'+'shopping_days'].dt.days\n    days_of_shopping.reset_index(inplace=True)\n    return days_of_shopping\n","bf4692c0":"history_shopping=shopping_days(history,'history')","4d30caef":"train , test = merge_train_test(train, test , history_shopping )","872a0ca7":"new_shopping=shopping_days(new_transactions, 'new')","1785dbd6":"new_shopping.head(n=10)","6b534e91":"train , test =merge_train_test(train, test , new_shopping )","44f40080":"#history['day_of_week']=history['purchase_date'].dt.weekday\n#history['day_of_month']=history['purchase_date'].dt.day\n#history['month_of_year']=history['purchase_date'].dt.month\n","898279ef":"#most_frequent_day_of_week=history.groupby('card_id')['day_of_week'].agg(mode).reset_index(name='most_frequent_day_of_week')","b3f01eda":"#train , test = merge_train_test(train, test , most_frequent_day_of_week )","41ec4be0":"#most_frequent_day_of_month=history.groupby('card_id')['day_of_month'].agg(mode).reset_index(name='most_frequent_day_of_month')","78c4f737":"#train , test = merge_train_test(train, test , most_frequent_day_of_month )","b2dbb8d8":"#most_frequent_month=history.groupby('card_id')['month_of_year'].agg(mode).reset_index(name='most_frequent_month')","9a1712d4":"#train , test = merge_train_test(train, test , most_frequent_month )","a6d613ce":"last_buy = history.groupby('card_id')['purchase_date'].max()\nlast_buy = last_buy.reset_index(name='last_one')\nlast_buy['last_one']=(datetime.today() - last_buy['last_one']).dt.days.reset_index()","8b71c54b":"train , test = merge_train_test(train, test , last_buy )","a968c389":"First_buy = new_transactions.groupby('card_id')['purchase_date'].min().reset_index(name='first_one')\nFirst_buy['first_one']=(datetime.today() - First_buy['first_one']).dt.days.reset_index()","c4e71703":"train , test = merge_train_test(train, test , First_buy )","a11103af":"train['between']=train['first_one'] - train['last_one']\ntest['between']=test['first_one'] - test['last_one']","8e8e245a":"train['from']=(datetime.today() - train['first_active_month']).dt.days\ntest['from']=(datetime.today() - test['first_active_month']).dt.days","a567b33e":"train['activation_month'] = train[\"first_active_month\"].dt.month\ntest['activation_month'] = test [\"first_active_month\"].dt.month","84ff866f":"train['activation_year'] = train[\"first_active_month\"].dt.year\ntest['activation_year'] = test [\"first_active_month\"].dt.year","d9a3b885":"def aggregate_per_month(df,history):\n    \n\n    agg_func = {\n            'purchase_amount': ['count', 'mean', 'min', 'max', 'std'],\n            'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            }\n\n    intermediate_group = history[history['authorized_flag']==1].groupby(['card_id', 'month_lag']).agg(agg_func)\n    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n    intermediate_group.reset_index(inplace=True)\n\n    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n    final_group.reset_index(inplace=True)\n    final_group = pd.merge(df, final_group, on='card_id', how='left')\n    return final_group","6b0d71d8":"import time\ntime.sleep(60)","947dc96f":"train = aggregate_per_month(train,history)","49fc7067":"import time\ntime.sleep(60)","6f9450c0":"test=aggregate_per_month(test,history)","626f03de":"import time\ntime.sleep(60)","df40e2eb":"def successive_aggregates(tr,df, field1, field2, name ):\n    t = df.groupby(['card_id', field1])[field2].mean()\n    u = pd.DataFrame(t).reset_index().groupby('card_id')[field2].agg(['mean', 'min', 'max', 'std'])\n    u.columns = [str(name)+field1 + '_' + field2 + '_' + col for col in u.columns.values]\n    u.reset_index(inplace=True)\n    tr=pd.merge(tr, u, on='card_id', how='left')\n    return tr","b208e172":"train = successive_aggregates(train,new_transactions, 'category_1', 'purchase_amount','new')\ntest  = successive_aggregates(test,new_transactions, 'category_1', 'purchase_amount','new')\n\ntrain = successive_aggregates(train,new_transactions, 'category_2', 'purchase_amount','new')\ntest  = successive_aggregates(test,new_transactions, 'category_2', 'purchase_amount','new')\n\ntrain = successive_aggregates(train,new_transactions, 'category_3', 'purchase_amount','new')\ntest  = successive_aggregates(test,new_transactions, 'category_3', 'purchase_amount','new')\n\n\ntrain = successive_aggregates(train,new_transactions,  'installments', 'purchase_amount','new')\ntest  = successive_aggregates(test,new_transactions,  'installments', 'purchase_amount','new')\n\ntrain = successive_aggregates(train,new_transactions, 'city_id', 'purchase_amount','new')\ntest  = successive_aggregates(test,new_transactions, 'city_id', 'purchase_amount','new')\n\ntrain = successive_aggregates(train,new_transactions, 'category_1', 'installments','new')\ntest  = successive_aggregates(test,new_transactions, 'category_1', 'installments','new')\n\ntrain = successive_aggregates(train,new_transactions, 'category_2', 'installments','new')\ntest  = successive_aggregates(test,new_transactions, 'category_2', 'installments','new')\n\ntrain = successive_aggregates(train,new_transactions, 'category_3', 'installments','new')\ntest  = successive_aggregates(test,new_transactions, 'category_3', 'installments','new')\n","864f5f9f":"time.sleep(15)","c7d86b51":"train = successive_aggregates(train,history, 'category_1', 'purchase_amount','history')\ntest  = successive_aggregates(test,history, 'category_1', 'purchase_amount','history')\n\ntrain = successive_aggregates(train,history, 'category_2', 'purchase_amount','history')\ntest  = successive_aggregates(test,history, 'category_2', 'purchase_amount','history')\n\ntrain = successive_aggregates(train,history, 'category_3', 'purchase_amount','history')\ntest  = successive_aggregates(test,history, 'category_3', 'purchase_amount','history')","08b23ed0":"import time\ntime.sleep(15)","3ec592eb":"train = successive_aggregates(train,history, 'category_1', 'installments','history')\ntest  = successive_aggregates(test,history, 'category_1', 'installments','history')\n\ntrain = successive_aggregates(train,history, 'category_2', 'installments','history')\ntest  = successive_aggregates(test,history, 'category_2', 'installments','history')\n\ntrain = successive_aggregates(train,history, 'category_3', 'installments','history')\ntest  = successive_aggregates(test,history, 'category_3', 'installments','history')","f3163756":"time.sleep(15)","c06f94e9":"train = successive_aggregates(train,history,  'installments', 'purchase_amount','history')\ntest  = successive_aggregates(test,history,  'installments', 'purchase_amount','history')\n\ntrain = successive_aggregates(train,history, 'city_id', 'purchase_amount','history')\ntest  = successive_aggregates(test,history, 'city_id', 'purchase_amount','history')","f04dcc28":"train['outliers'] = 0\ntrain.loc[train['target'] < -30, 'outliers'] = 1","968bc610":"#for f in ['feature_1','feature_2','feature_3']:\n#    order_label = train.groupby([f])['outliers'].mean()\n#    train[f+'map'] = train[f].map(order_label)\n#    test[f+'map'] =  test[f].map(order_label)","41c2fd62":"train.head(n=15)","c372a889":"import time\ntime.sleep(60)","27e555e7":"def aggregate_new_transactions(new_trans): \n    new_trans['authorized_flag'] = \\\n    new_trans['authorized_flag'].map({'Y':1, 'N':0})\n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'month_lag': ['min', 'max','mean','std']\n        }\n    agg_new_trans = new_trans.groupby(['card_id']).agg(agg_func)\n    agg_new_trans.columns = ['new_' + '_'.join(col).strip() \n                           for col in agg_new_trans.columns.values]\n    agg_new_trans.reset_index(inplace=True)\n    \n    df = (new_trans.groupby('card_id')\n          .size()\n          .reset_index(name='new_transactions_count'))\n    \n    #agg_new_trans = pd.merge(df, agg_new_trans, on='card_id', how='left')\n    \n    return agg_new_trans","ab433969":"#df = aggregate_new_transactions(new_transactions)","ecd1a811":"import time\ntime.sleep(20)","a0cc82c5":"#train , test = merge_train_test(train, test , df )","9128c70e":"import time\ntime.sleep(10)","d76a5fb3":"#train = other_features (train )\n#test= other_features (test )","7c24b718":"import time\ntime.sleep(120)","74ac20e0":"#history = pd.get_dummies(history, columns=['category_2', 'category_3'])","a85f2743":"import time\ntime.sleep(60)","2c8e5652":"def convert(df):\n    for i in df.columns:\n        if df[i].dtype =='uint8' : df[i]=df[i].astype(float)\n    return df ","2b2b48dc":"import time\ntime.sleep(120)","cd8d4fc6":"new_transactions.head(n=5)","3294c8e2":"def aggregate_transactions(df,frame,name):\n    \n    #frame.loc[:, 'purchase_date'] = pd.DatetimeIndex(frame['purchase_date']).\\\n                                     # astype(np.int64) * 1e-9\n    \n    agg_func = {\n    #'category_1_N': [ 'mean'],\n    #'category_1_Y': ['mean'],   \n    'category_2_1.0': ['mean'],\n    'category_2_2.0': ['mean'],\n    'category_2_3.0': ['mean'],\n    'category_2_4.0': ['mean'],\n    'category_2_5.0': ['mean'],\n    'category_3_A': ['mean'],\n    'category_3_B': ['mean'],\n    'category_3_C': ['mean'],\n    \n    }\n    \n    agg_history = frame.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = [str(name)+'_'.join(col).strip() for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    agg_new_trans = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_new_trans","169191c8":"history.keys()","370a24c4":"#train.head(n=5)","3ea516d9":"print(test.shape)\nprint(train.shape)","9702556a":"#train = aggregate_per_month(train,history)","314f3cac":"#test=aggregate_per_month(test,history)","cbe6f37e":"history.keys()","813b0e94":"history.head(n=10)","dfcebac6":"for f in ['feature_1','feature_2','feature_3']:\n    order_label = train.groupby([f])['outliers'].mean()\n    train[f] = train[f].map(order_label)\n    test[f] = test[f].map(order_label)","1b8a626d":"X = train.drop(columns=['first_active_month','target','card_id'])\ntest_X = test.drop(columns=['first_active_month','card_id'])\n\nY=train['target']","89f007e5":"#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\nX.shape","36de9e8e":"corr_matrix = X.corr().abs()","a965ff91":"plt.matshow(corr_matrix)","43aaafb0":"upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))","c55abad8":"to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]","da1e2353":"to_drop","daf5df55":"#X = X.drop(columns=to_drop)\n#test_X = test_X.drop(columns=to_drop)","36ddc2ef":"X.shape","552188ef":"test_X.shape","b3720697":"from sklearn.model_selection import StratifiedKFold","61e40b8e":"folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)","dbd00ef7":"for fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):\n    print(trn_idx, val_idx)","e5ded6e7":"xgb_params = {'eta': 0.01, 'max_depth': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, 'lambda' : 0.1 , 'alpha' : 0.4,'min_child_weight':1,\n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True, 'nthread': 4}","e973344b":"#xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n               # max_depth = 5, alpha = 10, lamda=2 , n_estimators = 10, eval_metric='rmse')","c7234661":"import lightgbm as lgb","51f223b1":"param = {'num_leaves': 32,\n         'min_data_in_leaf': 149, \n         'objective':'regression',\n         'max_depth': 4,\n         'learning_rate': 0.005,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.7522,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.7083 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.45,\n         \"lambda_l2\":0.15,\n         \"random_state\": 133,\n         \"verbosity\": -1}","381262ca":"features = [c for c in X.columns if c not in ['card_id', 'first_active_month', 'target','month_lag_mean_y','month_lag_std_y','outliers',\n                                             'history_purchase_date_max', 'history_purchase_date_min', 'new_purchase_date_max', 'new_purchase_date_min']]\ncategorical_feats = ['feature_1','feature_2','feature_3']","ee92554f":"len(list(X.keys()))","da8caf1d":"Y.head()","7c39e628":"#missing_data_function(X)","74bb1555":"#X=X.fillna(-1)","e824cd21":"#missing_data_function(test_X)","cb1b8098":"#test_X=test_X.fillna(-1)","aeabed44":"X.shape","4f9ca7bf":"X=X.drop(columns=['history_purchase_date_max', 'history_purchase_date_min', 'new_purchase_date_max', 'new_purchase_date_min'])","44bf8168":"rest=features","0c9a10b0":"folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\noof_lgbm = np.zeros(len(train))\npredictions_lgbd = np.zeros(len(test))\nstart = time.time()\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(X.iloc[trn_idx][features],\n                           label=Y.iloc[trn_idx],\n                           categorical_feature=categorical_feats\n                          )\n    val_data = lgb.Dataset(X.iloc[val_idx][features],\n                           label=Y.iloc[val_idx],\n                           categorical_feature=categorical_feats\n                          )\n\n    num_round = 10000\n    clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds = 100)\n    \n    oof_lgbm[val_idx] = clf.predict(X.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    \n    \n    predictions_lgbd += clf.predict(test_X[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\n","4caac1bc":"test_X.keys()","6c9921dc":"folds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(train))\noof_xgb_3 = np.zeros(len(train))\npredictions_xg = np.zeros(len(test))\n\nstart = time.time()\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):\n        \n        X_train, X_valid = X.iloc[trn_idx][features], X.iloc[val_idx][features]\n        y_train, y_valid = Y.iloc[trn_idx], Y.iloc[val_idx]\n        \n        \n        \n        train_data = xgb.DMatrix(data=X_train, label=y_train)\n        valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n        watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n        clf_xg = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=100, verbose_eval=500, params=xgb_params)\n        y_pred_valid = clf_xg.predict(xgb.DMatrix(X_valid), ntree_limit=clf_xg.best_ntree_limit)\n        oof_xgb_3[val_idx] = clf_xg.predict(xgb.DMatrix(X_valid), ntree_limit=clf_xg.best_ntree_limit)\n        y_pred = clf_xg.predict(xgb.DMatrix(test_X[features]), ntree_limit=clf_xg.best_ntree_limit)\n        predictions_xg += y_pred \/ folds.n_splits\n        \n        \n       \n","c4f49b13":"#predictions_xg","e7c9d026":"param_outliers = {'num_leaves': 8,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 3,\n         'learning_rate': 0.01,\n         \"boosting\": \"rf\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'binary_logloss',\n         \"lambda_l1\": 0.15,\n         \"lambda_l2\": 0.0,        \n         \"verbosity\": -1,\n         \"random_state\": 2333}","e4547bc0":"#predictions_xg","bddffc06":"folds = KFold(n_splits=5, shuffle=True, random_state=15)\noof_outliers = np.zeros(len(train))\npredictions_outliers = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\ntarget=train['outliers']\nstart = time.time()\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n\n    num_round = 10000\n    clf = lgb.train(param_outliers, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n    oof_outliers[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions_outliers += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\n#print(\"CV score: {:<8.5f}\".format(log_loss(target, oof))","af1b263a":"from sklearn.linear_model import Ridge","a5eb3a9a":"train_stack = np.vstack([oof_lgbm, oof_xgb_3,oof_outliers]).transpose()\ntest_stack = np.vstack([predictions_lgbd, predictions_xg,predictions_outliers]).transpose()\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(train_stack.shape[0])\npredictions = np.zeros(test_stack.shape[0])\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    X_train, X_valid = train_stack[trn_idx], train_stack[val_idx]\n    y_train, y_valid = Y.iloc[trn_idx], Y.iloc[val_idx]\n\n    train_data = xgb.DMatrix(data=X_train, label=y_train)\n    valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n    watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n    clf_xg = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=xgb_params)\n    y_pred_valid = clf_xg.predict(xgb.DMatrix(X_valid), ntree_limit=clf_xg.best_ntree_limit)\n    #oof_xgb_3[val_idx] = clf_xg.predict(xgb.DMatrix(train.iloc[val_idx][rest]), ntree_limit=clf_xg.best_ntree_limit)\n    y_pred = clf_xg.predict(xgb.DMatrix(test_stack), ntree_limit=clf_xg.best_ntree_limit)\n    predictions += y_pred \/ folds.n_splits\n    \n    #oof[val_idx] = clf.predict(X_valid)\n    #predictions += clf.predict(test_stack) \/ folds.n_splits\"\"\"\n\n","c3ab81b6":"predictions_xg","5ebe36fe":"sub_df = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\nsub_df[\"target\"] =predictions_xg\nsub_df.to_csv(\"submit.csv\", index=False)","354ea772":"sub_df.head(n=100)","2dad5585":"new_transactions=new_transactions.replace({'installments': {-1:0,999:0}})","74572ce1":"First_buy.head(n=10)","bb331251":"train , test = merge_train_test(train, test , installments )","dc563210":"def other_features (df):\n    df['authorized_percent']=df['authorized_flag_count']\/df['purchase_cnt']\n    df['number_of_merchant_percent']=df['number_of_merchant']\/df['purchase_cnt']\n    df['installments_count_percent']=df['installments_count']\/df['purchase_cnt']\n    df['authorized_percent']=test['authorized_flag_count'] \/df['purchase_cnt']\n    df['number_of_merchant_percent']=df['number_of_merchant']\/df['purchase_cnt']\n    df['installments_count_percent']=df['installments_count']\/df['purchase_cnt']\n    \n    return df ","4df379f0":"installments_number_new=new_transactions.groupby('card_id')['installments'].apply(lambda x: (x>0).sum()).reset_index(name='installments_count_new')","c7716beb":"missing_installments = history.groupby('card_id').agg({'missing_installments':['mean','sum']})\nmissing_installments.columns = ['missing_installments_mean','missing_installments_sum']\nmissing_installments=missing_installments.reset_index()","50d36dd0":"number of transactions : to make the difference between new and old history ","d512b804":"train=pd.merge(left=train , right=most_frequent_city, how = 'left', on ='card_id')","c9910988":"train , test = merge_train_test(train, test , missing_installments )","65c6764a":"month_diff_mean.head(n=10)","00365868":"to eliminate the next ","bfab7abf":"train['diff_month_lag_mean']=train['month_diff_mean']-train['month_diff_mean_new']\ntest['diff_month_lag_mean']=test['month_diff_mean']-test['month_diff_mean_new']","55bc9816":"history.head()","e1accdd8":"new_transactions = pd.get_dummies(new_transactions, columns=['category_2', 'category_3'])","b96b0d88":"train=aggregate_transactions(train,new_transactions,'new')","36f9ecf5":"train['diff_purchase']=train['purchase_cnt']-train['purchase_cnt_new']\ntest['diff_purchase']=test['purchase_cnt']-test['purchase_cnt_new']","7debae10":"train , test = merge_train_test(train, test , days_cnt)","2281c56b":"df = pd.DataFrame()\ndf['card_id']=history['card_id']\ndf['purchase_date_unique']=history['purchase_date'].apply(lambda x: x.date())\ndays_cnt = df.groupby('card_id')['purchase_date_unique'].nunique()\ndays_cnt.reset_index(inplace=True)","0215e10c":"cat=convert(cat)","3f1118b2":"A optimiser ","013a7334":"train , test = merge_train_test(train, test ,month_diff_mean_new )","75f94824":"merchant_cnt = history.groupby('card_id')['merchant_id'].nunique()\nmerchant_cnt=merchant_cnt.reset_index()\nmerchant_cnt.columns = ['card_id','number_of_merchant']","d50f67f1":"train , test = merge_train_test(train, test ,week_end )","c704c60b":"history['month_diff'] = ((datetime.today() - history['purchase_date']).dt.days)\/\/30\nhistory['month_diff'] += history['month_lag']","5fcab83d":"test=aggregate_transactions(test,new_transactions,'new')","fe85e095":"train=pd.merge(left=train , right=card_id_cnt_new, how = 'left', on ='card_id')\ntest=pd.merge(left=test , right=card_id_cnt_new, how = 'left', on ='card_id')","fb7f90e3":"history_shopping.head(n=10)","8cd38053":"train , test = merge_train_test(train, test , month_lags )","e4d87eb7":"train , test = merge_train_test(train, test , most_frequent_city_new )","67c54143":"week_end=history.groupby('card_id')['weekend'].agg(['sum', 'mean']).reset_index()\nweek_end.columns = ['card_id','weekend_sum', 'weekend_mean']","69ef79e1":"test=pd.merge(left=test , right=most_frequent_city, how = 'left', on ='card_id')","d44a24b6":"mode = lambda x: x.mode().values[0] if len(x) > 1 else x.values[0]","834bd7e5":"new_transactions['month_diff'] = ((datetime.today() - new_transactions['purchase_date']).dt.days)\/\/30\nnew_transactions['month_diff'] += new_transactions['month_lag']\n","1172157e":"train=aggregate_transactions(train,cat,'history')","896154b1":"month_diff_mean_new = history.groupby('card_id')['month_diff'].mean().reset_index(name='month_diff_mean_new')","beaa4f91":"train , test = merge_train_test(train, test , month_diff_mean )","2b3086e1":"city_cnt = history.groupby('card_id')['city_id'].nunique().reset_index(name='city_cnt')\n","f137120e":"type(merchant_cnt)","1ebdd3f6":"installments = history.groupby('card_id').agg({'installments':['mean','std','max','min']})\ninstallments.columns = ['installments_mean','installments_std','installments_max','installments_min']\ninstallments=installments.reset_index()","34965a38":"train=pd.merge(left=train , right=card_id_cnt, how = 'left', on ='card_id')\ntest=pd.merge(left=test , right=card_id_cnt, how = 'left', on ='card_id')","a589c089":"subsector_id_cnt = history.groupby('card_id')['subsector_id'].nunique().reset_index(name='subsector_id_cnt')\nsubsector_id_cnt.columns = ['card_id','subsector_id_cnt']","7cc1a790":"train , test = merge_train_test(train, test , city_cnt )","4e07dc82":"to add to features engineering first function ","8dfc6da1":"train , test = merge_train_test(train, test , merchant_cnt )","fcee7979":"train , test = merge_train_test(train, test , installments_number_new)","21ea5e8e":"history['weekend'] = (history.purchase_date.dt.weekday >=5).astype(int)","5335e0d6":"cat.reset_index(inplace=True)","5358e383":"history=history.replace({'installments': {-1:0,999:0}})","8acdace2":"amount = history.groupby('card_id').agg({'purchase_amount':['mean','std','max','min','sum']})\namount.columns = ['amount_mean','amount_std','amount_max','amount_min', 'amount_sum']\namount = amount.reset_index()","f6283d95":"\ncat=history[['card_id','category_2', 'category_3']]\ncat=pd.get_dummies(cat, columns=['category_2', 'category_3'])","482e2926":"week_end.head()","81f77247":"amount.head(n=10)","67aaad6e":"modeling ","bdc52041":"card_id_cnt_new = new_transactions.groupby('card_id')['city_id'].count().reset_index(name='purchase_cnt_new')\ncard_id_cnt_new.columns = ['card_id','purchase_cnt_new']","4a9ac446":"train=pd.merge(left=train , right=amount, how = 'left', on ='card_id')\ntest=pd.merge(left=test , right=amount, how = 'left', on ='card_id')","e9a35922":"train.head(n=5)","a1050792":"train=pd.merge(left=train , right=subsector_id_cnt, how = 'left', on ='card_id')\ntest=pd.merge(left=test , right=subsector_id_cnt, how = 'left', on ='card_id')","5203dfc8":"month_lags = history.groupby('card_id').agg({'month_lag':['mean','std']})\nmonth_lags.columns = ['month_lag_mean','month_lag_std']\nmonth_lags = month_lags.reset_index()\n\n","3820a397":"test=aggregate_transactions(test,cat,'history')","7c8b8293":"month_diff_mean = history.groupby('card_id')['month_diff'].mean().reset_index(name='month_diff_mean')\n","04275937":"new_transactions=convert(new_transactions)"}}