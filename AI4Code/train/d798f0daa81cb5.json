{"cell_type":{"680e5c37":"code","d68c29de":"code","316f1fca":"code","7cfa5304":"code","6413160f":"code","7c272769":"code","6b31ab46":"code","31777ae1":"code","5fd992a6":"code","370bc316":"code","d9231d26":"code","2dd78e30":"code","5db65da3":"code","4d4062c7":"code","352d726f":"code","2f151b53":"code","94c631d0":"markdown","17cfcfec":"markdown","8596552a":"markdown","614b792a":"markdown","dcaefb1f":"markdown","31839d96":"markdown","eaa04d8d":"markdown","9070a4e5":"markdown","38d8bbad":"markdown","b53d340e":"markdown"},"source":{"680e5c37":"import pandas as pd\nimport numpy as np\nfrom typing import Dict\nfrom collections import Counter\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\n","d68c29de":"def getPdArr(dir):\n    print(\"read data!!!\")\n    train = pd.read_csv(dir+'train.csv')\n    train_labels = pd.read_csv(dir+'train_labels.csv')\n    specs = pd.read_csv(dir+'specs.csv')\n    test = pd.read_csv(dir+'test.csv')\n    submission = pd.read_csv(dir+'sample_submission.csv')\n    print(\"read over!!!\")\n    return train, train_labels, specs, test, submission\n\n\ndef encode_title(train, test, train_labels):\n    # encode title\n    print(\"encode and getList...\")\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(\n        set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100 * np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    # assess_titles\n    print(\"over!!!\")\n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code\n\n\ndef get_data(\n        win_code, list_of_user_activities,\n        list_of_event_code, activities_labels, assess_titles,\n        list_of_event_id, all_title_event_code,\n        user_sample, test_set=False):\n    assess_title: Dict[str, int] = {t_eve: 0 for t_eve in assess_titles}\n\n    # Constants and parameters declaration\n    last_activity = 0\n\n    user_activities_count = {'Clip': 0, 'Activity': 0, 'Assessment': 0, 'Game': 0}\n\n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0: 0, 1: 0, 2: 0, 3: 0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0\n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()}\n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n\n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]\n\n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session) > 1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens:\n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            \n            features = user_activities_count.copy()\n            features.update(assess_title.copy())\n#             features[session_title_text] = 1\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n\n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            #add session title\n        \n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts\n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy \/ counter if counter > 0 else 0\n            accuracy = true_attempts \/ (true_attempts + false_attempts) if (true_attempts + false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group \/ counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n\n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts + false_attempts > 0:\n                all_assessments.append(features)\n\n            counter += 1\n\n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n            num_of_session_count = Counter(session[col])\n            for k in num_of_session_count.keys():\n                x = k\n                if col == 'title':\n                    x = activities_labels[k]\n                counter[x] += num_of_session_count[k]\n            return counter\n\n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n\n            # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments\n\n\ndef get_train_and_test(win_code, list_of_user_activities,\n        list_of_event_code, activities_labels, assess_titles,\n        list_of_event_id, all_title_event_code, train, test):\n    compiled_train = []\n    compiled_test = []\n    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=17000):\n        compiled_train += get_data(\n            win_code, list_of_user_activities,\n            list_of_event_code, activities_labels, assess_titles,\n            list_of_event_id, all_title_event_code,\n            user_sample)\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=1000):\n        test_data = get_data(\n            win_code, list_of_user_activities,\n            list_of_event_code, activities_labels, assess_titles,\n            list_of_event_id, all_title_event_code,\n            user_sample, test_set=True)\n        compiled_test.append(test_data)\n    reduce_train = pd.DataFrame(compiled_train)\n    print(reduce_train.shape)\n\n    reduce_test = pd.DataFrame(compiled_test)\n    print(reduce_test.shape)\n    categoricals = ['session_title']\n    return reduce_train, reduce_test, categoricals\n\n\ndef preprocess(assess_titles, reduce_train, reduce_test):\n    print(\"preprocess...\")\n    for df in [reduce_train, reduce_test]:\n        df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n        df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')\n        # df['installation_duration_std'] = df.groupby(['installation_id'])['duration_mean'].transform('std')\n        df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n\n        df['sum_event_code_count'] = df[\n            [2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020,\n             4021,\n             4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080,\n             2035,\n             2040, 4090, 4220, 4095]].sum(axis=1)\n\n        df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform(\n            'mean')\n\n    features = reduce_train.loc[\n        (reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns  # delete useless columns\n    features = [x for x in features if x not in ['accuracy_group', 'installation_id']]\n    print(\"preprocess over!!!\")\n    return reduce_train, reduce_test, features","316f1fca":"from sklearn.base import BaseEstimator, TransformerMixin\nclass MainTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, convert_cyclical: bool = False, create_interactions: bool = False, n_interactions: int = 20):\n        \"\"\"\n        Main transformer for the data. Can be used for processing on the whole data.\n\n        :param convert_cyclical: convert cyclical features into continuous\n        :param create_interactions: create interactions between features\n        \"\"\"\n\n        self.convert_cyclical = convert_cyclical\n        self.create_interactions = create_interactions\n        self.feats_for_interaction = None\n        self.n_interactions = n_interactions\n\n    def fit(self, X, y=None):\n\n        if self.create_interactions:\n            self.feats_for_interaction = [col for col in X.columns if 'sum' in col\n                                          or 'mean' in col or 'max' in col or 'std' in col\n                                          or 'attempt' in col]\n            self.feats_for_interaction1 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n            self.feats_for_interaction2 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        if self.create_interactions:\n            for col1 in self.feats_for_interaction1:\n                for col2 in self.feats_for_interaction2:\n                    data[f'{col1}_int_{col2}'] = data[col1] * data[col2]\n\n        if self.convert_cyclical:\n            data['timestampHour'] = np.sin(2 * np.pi * data['timestampHour'] \/ 23.0)\n            data['timestampMonth'] = np.sin(2 * np.pi * data['timestampMonth'] \/ 23.0)\n            data['timestampWeek'] = np.sin(2 * np.pi * data['timestampWeek'] \/ 23.0)\n            data['timestampMinute'] = np.sin(2 * np.pi * data['timestampMinute'] \/ 23.0)\n\n#         data['installation_session_count'] = data.groupby(['installation_id'])['Clip'].transform('count')\n#         data['installation_duration_mean'] = data.groupby(['installation_id'])['duration_mean'].transform('mean')\n#         data['installation_title_nunique'] = data.groupby(['installation_id'])['session_title'].transform('nunique')\n\n#         data['sum_event_code_count'] = data[['2000', '3010', '3110', '4070', '4090', '4030', '4035', '4021', '4020', '4010', '2080', '2083', '2040', '2020', '2030', '3021', '3121', '2050', '3020', '3120', '2060', '2070', '4031', '4025', '5000', '5010', '2081', '2025', '4022', '2035', '4040', '4100', '2010', '4110', '4045', '4095', '4220', '2075', '4230', '4235', '4080', '4050']].sum(axis=1)\n\n        # data['installation_event_code_count_mean'] = data.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\n\n\nclass FeatureTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, main_cat_features: list = None, num_cols: list = None):\n        \"\"\"\n\n        :param main_cat_features:\n        :param num_cols:\n        \"\"\"\n        self.main_cat_features = main_cat_features\n        self.num_cols = num_cols\n\n    def fit(self, X, y=None):\n\n#         self.num_cols = [col for col in X.columns if 'sum' in col or 'mean' in col or 'max' in col or 'std' in col\n#                          or 'attempt' in col]\n        \n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n#         for col in self.num_cols:\n#             data[f'{col}_to_mean'] = data[col] \/ data.groupby('installation_id')[col].transform('mean')\n#             data[f'{col}_to_std'] = data[col] \/ data.groupby('installation_id')[col].transform('std')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)","7cfa5304":"def qwk(a1, a2):\n    \"\"\"\n    reference: https:\/\/www.kaggle.com\/c\/data-science-bowl-2019\/discussion\/114133#latest-660168\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e \/ a1.shape[0]\n\n    return 1 - o \/ e\n\n\ndef eval_qwk_lgb(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n\n    y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n    return 'cappa', qwk(y_true, y_pred), True\n\n\ndef eval_qwk_lgb_regr(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    [1.05642867 1.66245117 2.19676548]\n    \"\"\"\n    y_pred[y_pred <= 1.05642867] = 0\n    y_pred[np.where(np.logical_and(y_pred > 1.05642867, y_pred <= 1.66245117))] = 1\n    y_pred[np.where(np.logical_and(y_pred > 1.66245117, y_pred <=  2.19676548))] = 2\n    y_pred[y_pred >  2.19676548] = 3\n\n    # y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n\n    return 'cappa', qwk(y_true, y_pred), True\n\ndef confusion_matrix(preds, labels, conf_matrix):\n    for p, t in zip(preds, labels):\n#         print(p,t)\n        conf_matrix[p,t] += 1\n    return conf_matrix\n\nimport itertools\n# plot confusion_matrix\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    Input\n    - cm : \u8ba1\u7b97\u51fa\u7684\u6df7\u6dc6\u77e9\u9635\u7684\u503c\n    - classes : \u6df7\u6dc6\u77e9\u9635\u4e2d\u6bcf\u4e00\u884c\u6bcf\u4e00\u5217\u5bf9\u5e94\u7684\u5217\n    - normalize : True:\u663e\u793a\u767e\u5206\u6bd4, False:\u663e\u793a\u4e2a\u6570\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","6413160f":"## validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\n\n\n\nclass LGBWrapper_regr(object):\n    \"\"\"\n    A wrapper for lightgbm model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = lgb.LGBMRegressor()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n        X_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n        X_valid.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_valid.columns]\n        if params['objective'] == 'regression':\n            eval_metric = eval_qwk_lgb_regr\n        else:\n            eval_metric = 'auc'\n\n        eval_set = [(X_train, y_train)]\n        eval_names = ['train']\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n            eval_names.append('valid')\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n            eval_names.append('holdout')\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = \"auto\"\n        else:\n            categorical_columns = \"auto\"\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_metric,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n                       categorical_feature=categorical_columns)\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict(self, X_test):\n        return self.model.predict(X_test, num_iteration=self.model.best_iteration_)\n    def save_model(self, path):\n        self.model.booster_.save_model(path)\n    \n\n\n\n\n    \n    \n    \n    ","7c272769":"class RegressorModel(object):\n    \"\"\"\n    A wrapper class for classification models.\n    It can be used for training and prediction.\n    Can plot feature importance and training progress (if relevant for model).\n\n    \"\"\"\n\n    def __init__(self, columns: list = None, model_wrapper=None):\n        \"\"\"\n\n        :param original_columns:\n        :param model_wrapper:\n        \"\"\"\n        self.columns = columns\n        self.model_wrapper = model_wrapper\n        self.result_dict = {}\n        self.train_one_fold = False\n        self.preprocesser = None\n\n    def fit(self, X: pd.DataFrame, y,\n            X_holdout: pd.DataFrame = None, y_holdout=None,\n            folds=None,\n            params: dict = None,\n            eval_metric='rmse',\n            cols_to_drop: list = None,\n            preprocesser=None,\n            transformers: dict = None,\n            adversarial: bool = False,\n            plot: bool = True):\n        \"\"\"\n        Training the model.\n\n        :param X: training data\n        :param y: training target\n        :param X_holdout: holdout data\n        :param y_holdout: holdout target\n        :param folds: folds to split the data. If not defined, then model will be trained on the whole X\n        :param params: training parameters\n        :param eval_metric: metric for validataion\n        :param cols_to_drop: list of columns to drop (for example ID)\n        :param preprocesser: preprocesser class\n        :param transformers: transformer to use on folds\n        :param adversarial\n        :return:\n        \"\"\"\n\n        if folds is None:\n            folds = KFold(n_splits=3, random_state=42)\n            self.train_one_fold = True\n\n        self.columns = X.columns if self.columns is None else self.columns\n        self.feature_importances = pd.DataFrame(columns=['feature', 'importance'])\n        self.trained_transformers = {k: [] for k in transformers}\n        self.transformers = transformers\n        self.models = []\n        self.folds_dict = {}\n        self.eval_metric = eval_metric\n        n_target = 1\n        self.oof = np.zeros((len(X), n_target))\n        self.n_target = n_target\n\n        X = X[self.columns]\n        if X_holdout is not None:\n            X_holdout = X_holdout[self.columns]\n\n        if preprocesser is not None:\n            self.preprocesser = preprocesser\n            self.preprocesser.fit(X, y)\n            X = self.preprocesser.transform(X, y)\n            self.columns = X.columns.tolist()\n            if X_holdout is not None:\n                X_holdout = self.preprocesser.transform(X_holdout)\n\n        for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y, X['installation_id'])):\n\n            if X_holdout is not None:\n                X_hold = X_holdout.copy()\n            else:\n                X_hold = None\n            self.folds_dict[fold_n] = {}\n            if params['verbose']:\n                print(f'Fold {fold_n + 1} started at {time.ctime()}')\n            self.folds_dict[fold_n] = {}\n\n            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            if self.train_one_fold:\n                X_train = X[self.original_columns]\n                y_train = y\n                X_valid = None\n                y_valid = None\n\n            datasets = {'X_train': X_train, 'X_valid': X_valid, 'X_holdout': X_hold, 'y_train': y_train}\n            X_train, X_valid, X_hold = self.transform_(datasets, cols_to_drop)\n\n            self.folds_dict[fold_n]['columns'] = X_train.columns.tolist()\n\n            model = copy.deepcopy(self.model_wrapper)\n\n            if adversarial:\n                X_new1 = X_train.copy()\n                if X_valid is not None:\n                    X_new2 = X_valid.copy()\n                elif X_holdout is not None:\n                    X_new2 = X_holdout.copy()\n                X_new = pd.concat([X_new1, X_new2], axis=0)\n                y_new = np.hstack((np.zeros((X_new1.shape[0])), np.ones((X_new2.shape[0]))))\n                X_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new)\n\n            model.fit(X_train, y_train, X_valid, y_valid, X_hold, y_holdout, params=params)\n\n            self.folds_dict[fold_n]['scores'] = model.best_score_\n            if self.oof.shape[0] != len(X):\n                self.oof = np.zeros((X.shape[0], self.oof.shape[1]))\n            if not adversarial:\n                self.oof[valid_index] = model.predict(X_valid).reshape(-1, n_target)\n\n            fold_importance = pd.DataFrame(list(zip(X_train.columns, model.feature_importances_)),\n                                           columns=['feature', 'importance'])\n            self.feature_importances = self.feature_importances.append(fold_importance)\n            self.models.append(model)\n\n        self.feature_importances['importance'] = self.feature_importances['importance'].astype(int)\n\n        # if params['verbose']:\n        self.calc_scores_()\n\n        if plot:\n            # print(classification_report(y, self.oof.argmax(1)))\n            fig, ax = plt.subplots(figsize=(16, 12))\n            plt.subplot(2, 2, 1)\n            self.plot_feature_importance(top_n=20)\n            plt.subplot(2, 2, 2)\n            self.plot_metric()\n            plt.subplot(2, 2, 3)\n            plt.hist(y.values.reshape(-1, 1) - self.oof)\n            plt.title('Distribution of errors')\n            plt.subplot(2, 2, 4)\n            plt.hist(self.oof)\n            plt.title('Distribution of oof predictions');\n            # plot_confusion_matrix\n#             conf_matrix = np.zeros((4,4))\n#             conf_matrix = confusion_matrix(self.oof, y.values.reshape(-1, 1), conf_matrix)\n#             plot_classes = [0, 1, 2, 3]\n#             plot_confusion_matrix(conf_matrix, classes=plot_classes, normalize=True, title='Normalized confusion matrix')\n\n    def transform_(self, datasets, cols_to_drop):\n        for name, transformer in self.transformers.items():\n            transformer.fit(datasets['X_train'], datasets['y_train'])\n            datasets['X_train'] = transformer.transform(datasets['X_train'])\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = transformer.transform(datasets['X_valid'])\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = transformer.transform(datasets['X_holdout'])\n            self.trained_transformers[name].append(transformer)\n        if cols_to_drop is not None:\n            cols_to_drop = [col for col in cols_to_drop if col in datasets['X_train'].columns]\n\n            datasets['X_train'] = datasets['X_train'].drop(cols_to_drop, axis=1)\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = datasets['X_valid'].drop(cols_to_drop, axis=1)\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = datasets['X_holdout'].drop(cols_to_drop, axis=1)\n        self.cols_to_drop = cols_to_drop\n\n        return datasets['X_train'], datasets['X_valid'], datasets['X_holdout']\n\n    def calc_scores_(self):\n        print()\n        datasets = [k for k, v in [v['scores'] for k, v in self.folds_dict.items()][0].items() if len(v) > 0]\n        self.scores = {}\n        for d in datasets:\n            scores = [v['scores'][d][self.eval_metric] for k, v in self.folds_dict.items()]\n            print(f\"CV mean score on {d}: {np.mean(scores):.4f} +\/- {np.std(scores):.4f} std.\")\n            self.scores[d] = np.mean(scores)\n\n    def predict(self, X_test, averaging: str = 'usual'):\n        \"\"\"\n        Make prediction\n\n        :param X_test:\n        :param averaging: method of averaging\n        :return:\n        \"\"\"\n        full_prediction = np.zeros((X_test.shape[0], self.oof.shape[1]))\n        if self.preprocesser is not None:\n            X_test = self.preprocesser.transform(X_test)\n        for i in range(len(self.models)):\n            X_t = X_test.copy()\n            for name, transformers in self.trained_transformers.items():\n                X_t = transformers[i].transform(X_t)\n\n            if self.cols_to_drop is not None:\n                cols_to_drop = [col for col in self.cols_to_drop if col in X_t.columns]\n                X_t = X_t.drop(cols_to_drop, axis=1)\n            y_pred = self.models[i].predict(X_t[self.folds_dict[i]['columns']]).reshape(-1, full_prediction.shape[1])\n\n            # if case transformation changes the number of the rows\n            if full_prediction.shape[0] != len(y_pred):\n                full_prediction = np.zeros((y_pred.shape[0], self.oof.shape[1]))\n\n            if averaging == 'usual':\n                full_prediction += y_pred\n            elif averaging == 'rank':\n                full_prediction += pd.Series(y_pred).rank().values\n\n        return full_prediction \/ len(self.models)\n\n    def plot_feature_importance(self, drop_null_importance: bool = True, top_n: int = 10):\n        \"\"\"\n        Plot default feature importance.\n\n        :param drop_null_importance: drop columns with null feature importance\n        :param top_n: show top n columns\n        :return:\n        \"\"\"\n\n        top_feats = self.get_top_features(drop_null_importance, top_n)\n        feature_importances = self.feature_importances.loc[self.feature_importances['feature'].isin(top_feats)]\n        feature_importances['feature'] = feature_importances['feature'].astype(str)\n        top_feats = [str(i) for i in top_feats]\n        sns.barplot(data=feature_importances, x='importance', y='feature', orient='h', order=top_feats)\n        plt.title('Feature importances')\n\n    def get_top_features(self, drop_null_importance: bool = True, top_n: int = 10):\n        \"\"\"\n        Get top features by importance.\n\n        :param drop_null_importance:\n        :param top_n:\n        :return:\n        \"\"\"\n        grouped_feats = self.feature_importances.groupby(['feature'])['importance'].mean()\n        if drop_null_importance:\n            grouped_feats = grouped_feats[grouped_feats != 0]\n        return list(grouped_feats.sort_values(ascending=False).index)[:top_n]\n\n    def plot_metric(self):\n        \"\"\"\n        Plot training progress.\n        Inspired by `plot_metric` from https:\/\/lightgbm.readthedocs.io\/en\/latest\/_modules\/lightgbm\/plotting.html\n\n        :return:\n        \"\"\"\n        full_evals_results = pd.DataFrame()\n        for model in self.models:\n            evals_result = pd.DataFrame()\n            for k in model.model.evals_result_.keys():\n                evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n            evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n            full_evals_results = full_evals_results.append(evals_result)\n\n        full_evals_results = full_evals_results.melt(id_vars=['iteration']).rename(columns={'value': self.eval_metric,\n                                                                                            'variable': 'dataset'})\n        sns.lineplot(data=full_evals_results, x='iteration', y=self.eval_metric, hue='dataset')\n        plt.title('Training progress')","6b31ab46":"from functools import partial\nimport scipy as sp\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https:\/\/www.kaggle.com\/naveenasaithambi\/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n        return -qwk(y, X_p)\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","31777ae1":"dir = '\/kaggle\/input\/data-science-bowl-2019\/'\ntrain, train_labels, specs, test, submission = getPdArr(dir)\ntrain, test, train_labels, win_code, list_of_user_activities, \\\nlist_of_event_code, activities_labels, assess_titles, \\\nlist_of_event_id, all_title_event_code = encode_title(\n    train, test, train_labels)\nreduce_train, reduce_test, categoricals = get_train_and_test(\n    win_code, list_of_user_activities,\n    list_of_event_code, activities_labels, assess_titles,\n    list_of_event_id, all_title_event_code,\n    train, test)\nreduce_train, reduce_test, features = \\\n    preprocess(assess_titles, reduce_train, reduce_test)\n\n\n# Any results you write to the current directory are saved as output.","5fd992a6":"reduce_train.to_csv(\"reduce_train.csv\")\nreduce_test.to_csv(\"reduce_test.csv\")","370bc316":"# reduce_train = pd.read_csv(\"reduce_train.csv\")\n# reduce_test = pd.read_csv(\"reduce_test.csv\")\ny = reduce_train['accuracy_group']","d9231d26":"reduce_train.head(5)","2dd78e30":"params = {'n_estimators':2000,\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 1,\n            'learning_rate': 0.04,\n            'feature_fraction': 0.9,\n         'max_depth': 15,\n            'lambda_l1': 1,  \n            'lambda_l2': 1,\n            'verbose': 100,\n            'early_stopping_rounds': 100, 'eval_metric': 'cappa'\n            }\n\ncols_to_drop = ['game_session', 'installation_id', 'timestamp', 'accuracy_group', 'timestampDate']\nmt = MainTransformer()\nft = FeatureTransformer()\ntransformers = {'ft': ft}","5db65da3":"reduce_train['accuracy_group'].value_counts(normalize=True)","4d4062c7":"import copy\nimport time\nimport datetime\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nimport random\nmodels = []\ncoefficients_arr = []\nloops = 10\nfor i in range(loops):\n    print(\"loops:\", i)\n    n_fold = 5\n    folds = GroupKFold(n_splits=n_fold)\n    model = RegressorModel(model_wrapper=LGBWrapper_regr())\n    rd = random.randint(20, 500)\n    print(\"rd:\", rd)\n    shuffle_train, shuffle_y = shuffle(reduce_train, y, random_state=rd)\n    model.fit(X=shuffle_train, y=shuffle_y, folds=folds, params=params, preprocesser=mt, transformers=transformers,\n                        eval_metric='cappa', cols_to_drop=cols_to_drop)\n    fold_result = [item[0] for item in model.oof]\n    optR = OptimizedRounder()\n    optR.fit(fold_result, shuffle_y)\n    coefficients = optR.coefficients()\n    print(coefficients)\n    opt_preds = optR.predict(fold_result, coefficients)\n    print(qwk(shuffle_y, opt_preds)) \n    models.append(model)\n    coefficients_arr.append(coefficients)","352d726f":"coefficients_arr = np.array(coefficients_arr)\nprint(coefficients_arr)\ncoefficients = [np.mean(coefficients_arr, axis=0)] \nprint(\"coefficients: \", coefficients)\npr1 = np.zeros(len(y.values))\nfor model in models:\n    result = model.predict(reduce_train)\n    result = [item[0] for item in result]\n    result = np.array(result)\n    pr1 += result \/ len(models)\n# print(coeffients_arr)\n# fold_result = [item[0] for item in model.oof]\n# plot distribution\n# plt.hist(pr1)\n# optR = OptimizedRounder()\n# optR.fit(fold_result, y)\n# coefficients = optR.coefficients()\n# print(coefficients)\n# opt_preds = optR.predict(fold_result, coefficients)\n# print(qwk(y, pr1))\n# conf_matrix = np.zeros((4,4))\n# conf_matrix = confusion_matrix(pr1, y.values.reshape(-1, 1), conf_matrix)\n# plot_classes = [0, 1, 2, 3]\n# plot_confusion_matrix(conf_matrix, classes=plot_classes, normalize=True, title='Normalized confusion matrix')\n# plot_confusion\nconf_matrix = np.zeros((4,4))\ncoefficients = [1.05642867, 1.66245117, 2.19676548]\npr1 = optR.predict(pr1, coefficients)\nprint(qwk(y, pr1))\n# print(pr1)\nconf_matrix = confusion_matrix(pr1, y.values.reshape(-1, 1), conf_matrix)\nplot_classes = [0, 1, 2, 3]\nplot_confusion_matrix(conf_matrix, classes=plot_classes, normalize=True, title='Normalized confusion matrix')\n","2f151b53":"# print(coefficients_arr)\n# lgb.plot_importance(model,max_num_features=20, importance_type = \"gain\")\nresult = np.zeros(len(reduce_test))\nfor model in models:\n    result1 = model.predict(reduce_test)\n    result1 = [item[0] for item in result1]\n    result1 = np.array(result1)\n    result += result1 \/ len(models)\nplt.hist(result)\nplt.show()\nresult = optR.predict(result.reshape(-1, ), coefficients)\nprint(\"save\")\nsubmission['accuracy_group'] = result.astype(int)\nsubmission['accuracy_group'].head(10)\nsubmission.to_csv('submission.csv', index=False)\nprint(\"predict over!!!\")\nsubmission['accuracy_group'].value_counts(normalize=True)","94c631d0":"**import lib**","17cfcfec":"**data preprogress**","8596552a":"**evaluation**","614b792a":"**train()**","dcaefb1f":"**params**","31839d96":"**Main()**","eaa04d8d":"**find optimal thresholds**","9070a4e5":"**Transformer**","38d8bbad":"*eval*","b53d340e":"**Model**"}}