{"cell_type":{"a05fa323":"code","6f14041c":"code","38c527ec":"code","577376b6":"markdown","5c896390":"markdown","a9c30afc":"markdown"},"source":{"a05fa323":"import pandas as pd\nimport numpy as np\nnp.set_printoptions(precision=4)\nimport catboost\nfrom catboost import datasets\nfrom catboost import *\nprint(\"catboost version:\", catboost.__version__)\n\n# Read data from Amazon.com_Employee Access Challenge\ntrain_df = pd.read_csv('..\/input\/amazoncom-employee-access-challenge\/train.csv')\ntest_df = pd.read_csv('..\/input\/amazoncom-employee-access-challenge\/test.csv')\n\ny = train_df.ACTION\nX = train_df.drop('ACTION', axis=1)\ncat_features = list(range(0, X.shape[1]))\n\n# train\/valid split\ntrain_count = int(X.shape[0] * 0.8)\nX_train = X.iloc[:train_count,:]\ny_train = y[:train_count]\nX_validation = X.iloc[train_count:, :]\ny_validation = y[train_count:]\n\n# Use widely default params to show the difference\nparams = {'learning_rate': 0.05, 'iterations': 500, 'random_seed': 3,\n          'custom_loss': ['Accuracy']}\n\nmod1 = CatBoostClassifier(**params, task_type='GPU')\nmod2 = CatBoostClassifier(**params, task_type='CPU')\n\nargs = (X_train, y_train)\nkwargs = {'eval_set': (X_validation, y_validation), 'cat_features': cat_features, 'verbose': 100}\n\nprint(\"train on GPU (mod1)...\")\nmod1.fit(*args, **kwargs)\n\nprint(\"train on CPU (mod2)...\")\nmod2.fit(*args, **kwargs)","6f14041c":"# Compare all params that are different\nparams_gpu = mod1.get_all_params()\nparams_cpu = mod2.get_all_params()\n\nfor k in set(params_cpu.keys())|set(params_gpu.keys()):\n    val_gpu = params_gpu[k] if k in params_gpu.keys() else 'None'\n    val_cpu = params_cpu[k] if k in params_cpu.keys() else 'None'\n    if val_cpu == val_gpu: continue\n    print(f'{k:<30}  {str(val_cpu):<40}  {str(val_gpu):<40}')","38c527ec":"params = {'learning_rate': 0.05, 'iterations': 500, 'random_seed': 3,\n          'custom_loss': ['Accuracy']}\n\n# Try to use the same params for both CPU\/GPU\nparams['bootstrap_type'] = 'MVS'\nparams['boosting_type'] = 'Plain'         # GPU: much worse (because dataset is small)\nparams['boosting_type'] = 'Ordered'       # CPU: no difference\nparams['model_shrink_mode'] = 'Constant'  # default, ignored by GPU\nparams['model_shrink_rate'] = 0           # default for mode=Constant (should not shrink at all), ignored by GPU\nparams['sampling_frequency'] = 'PerTree'  # doc error: default is 'PerTree' not 'PerTreeLevel'\nparams['posterior_sampling'] = False      # same as None? ignored by GPU\nparams['bagging_temperature'] = 1         # same as None\nparams['border_count'] = 254              # no impact, CPU default: 254, GPU default: 128\nparams['penalties_coefficient'] = 1       # same as None\nparams['fold_permutation_block'] = 64     # 0 ignored by GPU\nparams['subsample'] = 0.8                 # no impact\n\n# ctr: FeatureFreq is not implemented on CPU, Counter not on GPU, use only Border for both.\nparams['simple_ctr'] = ['Borders:CtrBorderCount=15:CtrBorderType=Uniform:TargetBorderCount=1:TargetBorderType=MinEntropy:Prior=0\/1:Prior=0.5\/1:Prior=1\/1']\nparams['combinations_ctr'] = ['Borders:CtrBorderCount=15:CtrBorderType=Uniform:TargetBorderCount=1:TargetBorderType=MinEntropy:Prior=0\/1:Prior=0.5\/1:Prior=1\/1']\n\nparams['ctr_history_unit'] = 'Sample'     # undocumented, ignored by CPU\n#params['fold_size_loss_normalization'] = False  # unexpected keyword argument by GPU\n#params['min_fold_size'] = 100             # undocumented, unexpected keyword argument by GPU\n#params['observations_to_bootstrap'] = 'TestOnly'  # undocumented, unexpected keyword argument for GPU\n\n# Same default for both (but mentioned in Issue report)\nparams['leaf_estimation_method'] = 'Newton'                # same default for both\nparams['leaf_estimation_iterations'] = 10                  # same default for both\nparams['leaf_estimation_backtracking'] = 'AnyImprovement'  # same default for both\n\nmod1 = CatBoostClassifier(task_type='GPU', **params)\nmod2 = CatBoostClassifier(task_type='CPU', **params)\n\nargs = (X_train, y_train)\nkwargs = {'eval_set': (X_validation, y_validation), 'cat_features': cat_features, 'verbose': 100}\n\nprint(\"train on GPU (mod1)...\")\nmod1.fit(*args, **kwargs)  # 1 min setup time\n\nprint(\"train on CPU (mod2)...\")\nmod2.fit(*args, **kwargs)","577376b6":"### Typical results for some random seeds:\n    model   seed=0  seed=1  seed=2  seed=3  seed=64\n    gpu     0.141   0.143   0.143   0.141   0.144\n    cpu     0.138   0.138   0.137   0.136   0.137","5c896390":"Still the difference is about the same.","a9c30afc":"As noticed by others [here](https:\/\/github.com\/catboost\/catboost\/issues\/1408), results from catboost training on GPU can be consistently inferior to results obtained on CPU. Here I try to reproduce the CPU results on the GPU using, where implemented, the same parameters.\n\nThis is just a demo example, the dataset is too small to really benefit from the GPU."}}