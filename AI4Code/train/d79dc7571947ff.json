{"cell_type":{"99503647":"code","5bfb2358":"code","2c49babc":"code","c07ce95f":"code","cb68847c":"code","d173e735":"code","e28cebd9":"code","22a1eca8":"code","acd6946b":"code","b719ce4f":"code","adf65a02":"code","441c269e":"code","89645ed5":"code","3f1981c0":"code","47e51fe0":"code","015db727":"code","8796c470":"code","30908a9e":"code","c7fdf201":"code","a3116baf":"code","b6ab95c1":"code","9224eae0":"code","ff7943c4":"code","bc79ee84":"code","4b29fbe4":"code","b4041179":"code","83d5ab93":"code","8d8810a3":"code","c2dde1a7":"code","1077cdd0":"code","cfe0ec71":"code","01394c1c":"code","ad1b22f6":"code","f7dc30e7":"code","0da8db69":"code","865f5915":"code","53fe835d":"code","271a9acd":"code","a144069b":"code","e0d80faf":"code","eb658e63":"code","7e3345f8":"code","7e636089":"code","8a672346":"code","f2fb59c5":"code","d733351d":"code","b947246d":"code","7dc38158":"code","da8a38ff":"code","d498d766":"code","84751a6b":"code","980a0c1a":"code","5582278f":"code","92e2bf82":"code","34281ea5":"code","c83059cc":"code","2f91372e":"code","221e6c91":"code","b03fa526":"code","4b52ff17":"code","0596dfad":"code","ecc6540c":"code","638d4ef8":"code","aa8250dc":"code","324b6625":"code","e6a26366":"code","f24c211b":"code","0a374700":"code","c80090cc":"code","1c9b5c32":"markdown","21ad416a":"markdown","ea0ab68c":"markdown","fcf9a82a":"markdown","77196132":"markdown","537e52ac":"markdown","792c46a9":"markdown","27422301":"markdown","7cce236b":"markdown","44c6b0bc":"markdown","d6d0a269":"markdown","b59ac7ec":"markdown","247051ca":"markdown","e264d149":"markdown","efc6f892":"markdown","837235ca":"markdown","953ddf31":"markdown","08cc65c7":"markdown","8f57da76":"markdown","2ded588e":"markdown","d30d9ba2":"markdown","98bedd3d":"markdown","9052dded":"markdown","2ae18443":"markdown","1fd56f9d":"markdown","619abf34":"markdown","9d95ba1f":"markdown","fdf34cb7":"markdown","3ac9828a":"markdown","45aed99d":"markdown","57f6678d":"markdown","8fac64d8":"markdown","e01a0e0d":"markdown","11a725be":"markdown","b875ea41":"markdown","ac1fbe05":"markdown","7c68aa19":"markdown","30d10780":"markdown","7b529aa0":"markdown","9beafdce":"markdown","dd042a5c":"markdown","fc882e9e":"markdown","7f26d371":"markdown","b15ac3df":"markdown","2bff4904":"markdown","4f8692a5":"markdown","d8e01eb5":"markdown","58c7c274":"markdown","28c1799c":"markdown"},"source":{"99503647":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5bfb2358":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","2c49babc":"data=pd.read_csv('..\/input\/land_train.csv')","c07ce95f":"test_data=pd.read_csv('..\/input\/land_test.csv')","cb68847c":"print(\"The Columns in the Training Dataset: {}\".format(data.columns.values))","d173e735":"print(\"The Columns in the Testing Dataset: {}\".format(test_data.columns.values))","e28cebd9":"print(data.head(3))","22a1eca8":"print(\"The Types of the Features in the dataframe are {} \".format(data.info()))","acd6946b":"if(data.isnull().values.any()==True):\n    print(\"Missing Values Found.\\n\")\nelse:\n    print(\"No Missing Values found\")","b719ce4f":"#function for plotting the distribution plot , Violin Plot and Box Plot of the Feature Columns\ndef univariate(df,col,vartype,hue =None):\n    sns.set(style=\"whitegrid\")\n\n    fig, ax=plt.subplots(nrows =1,ncols=3,figsize=(10,4))\n    ax[0].set_title(\"Distribution Plot\")\n    sns.distplot(df[col],ax=ax[0])\n    ax[1].set_title(\"Violin Plot\")\n    sns.violinplot(data =df, x=col,ax=ax[1], inner=\"quartile\")\n    ax[2].set_title(\"Box Plot\")\n    sns.boxplot(data =df, x=col,ax=ax[2],orient='v')\nplt.show()","adf65a02":"#X1\nunivariate(df=data,col='X1',vartype=0)","441c269e":"#X2\nunivariate(df=data,col='X2',vartype=0)","89645ed5":"#X3\nunivariate(df=data,col='X3',vartype=0)","3f1981c0":"#X4\nunivariate(df=data,col='X4',vartype=0)","47e51fe0":"univariate(df=data,col='X5',vartype=0)","015db727":"univariate(df=data,col='X6',vartype=0)","8796c470":"univariate(df=data,col='I1',vartype=0)","30908a9e":"univariate(df=data,col='I3',vartype=0)","c7fdf201":"univariate(df=data,col='I4',vartype=0)","a3116baf":"#I5\nunivariate(df=data,col='I5',vartype=0)","b6ab95c1":"#I6\nunivariate(df=data,col='I6',vartype=0)","9224eae0":"#JOINT PLOT OF X1 and X2\nsns.jointplot(x=\"X1\", y=\"X2\", data=data, size=7)","ff7943c4":"#using seaborn's FacetGrid to color the scatterplot by Target class\n#TARGET CLASS WISE JOINT PLOTS OF X1 and X2\nsns.FacetGrid(data, hue=\"target\", size=7) \\\n   .map(plt.scatter, \"X1\",\"X2\") \\\n   .add_legend()","bc79ee84":"#Similarly we can use seborn's pairplot to show bivariate relations b\/w features\nsns.pairplot(data, hue=\"target\", size=3)","4b29fbe4":"import seaborn as sns\n# data['target']=labels\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = data.corr()\nsns.heatmap(corr[(corr >= 0.5) | (corr <= -0.4)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 8}, square=True);","b4041179":"from sklearn.feature_selection import RFECV, RFE, mutual_info_classif\nfrom sklearn.svm import SVR","83d5ab93":"# X1_train=np.array(sm_X).delete([1],axis)\n# print(data.shape)\np=data.drop(['target'],axis=1)\nprint(p.shape)\np=np.array(p)\nX1=p\nY1=data['target']\nmutual_info_classif(X1, Y1)","8d8810a3":"from sklearn.decomposition import PCA\npca = PCA()\nX1[: ,:6] = pca.fit_transform(X1[: ,:6])\npca.explained_variance_ratio_","c2dde1a7":"pca = PCA(0.95).fit(X1[: ,:6])\nprint(' Out of X1 to X6 ,only %d components explain 95%% of the variation in data' % pca.n_components_)","1077cdd0":"pca2=PCA()\nX1[: ,6:] = pca2.fit_transform(X1[: ,6:])\npca2.explained_variance_ratio_","cfe0ec71":"pca2=PCA(0.95).fit(X1[: ,6:])\nprint(' Out of I1 to I6 ,only %d components explain 95%% of the variation in data' % pca2.n_components_)","01394c1c":"#selector=RFECV(SVR(kernel=\"linear\"), step=1, cv=5)\n#selector.fit(X1_train,Y1_train)","ad1b22f6":"labels=data['target']\ndata =data.drop(['I6','target'],axis=1)\nprint(data.head())\nprint(labels.head())","f7dc30e7":"test_data=test_data.drop(['I6'],axis=1)","0da8db69":"from collections import Counter\ndef detect_outliers(df,n,features):\n    outlier_indices = []\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        # outlier step\n        outlier_step = 1.5 * IQR\n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        # select observations containing more than 2 outliers\n        outlier_indices = Counter(outlier_indices)        \n#         print((outlier_indices.items))\n        multiple_outliers = list( k for k, v in outlier_indices.items() if v >= n )\n#         print(multiple_outliers.values())\n        return multiple_outliers ","865f5915":"\nOutliers_to_drop = detect_outliers(data,1,[\"X1\",'X2','X3','X4','X5','X6'])\n# print(Outliers_to_drop)\nprint(\"No. of Rows with Atleast One Outlier: {}\".format(len(data.loc[Outliers_to_drop]))) # Show the outliers rows\n# Drop outliers\ndata= data.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\nlabels=labels.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","53fe835d":"print(data.shape)","271a9acd":"print(labels.shape)","a144069b":"#generates a class map of the samples\ndef get_class_map(labels):\n    class_map={}\n    for i in labels:\n        if str(i) not in class_map:\n            class_map[str(i)]=1\n        else:\n            class_map[str(i)]+=1\n#     print(class_map)\n    return class_map\n\np=get_class_map(labels.values)\nprint(p)\nq=[i for i in p.keys()]\n# p=get_class_map(labels.values)\nsizes = [i for i in p.values()]\ncolors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\nplt.pie(sizes, labels=q, colors=colors,\n        autopct='%1.1f%%', shadow=True)\nplt.show()","e0d80faf":"from imblearn.over_sampling import SMOTE,ADASYN\n#function for generating additional samples to balance the classes\ndef class_balancer(dataset,labels):\n    sm_X,sm_Y=SMOTE().fit_resample(dataset,labels)\n    ad_X,ad_Y=ADASYN().fit_resample(dataset,labels)\n    return sm_X,sm_Y,ad_X,ad_Y","eb658e63":"sm_X,sm_Y,ad_X,ad_Y=class_balancer(data,labels)\nprint(\"SMOTE's class Balanced Dataset:{}\".format(get_class_map(sm_Y)))\nprint(\"ADASYN's class Balanced Dataset:{}\".format(get_class_map(ad_Y)))\n# print(type(sm_Y))\np=get_class_map(get_class_map(sm_Y))\nsizes = [i for i in p.values()]\ncolors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\nplt.title('SMOTEs plot of class map')\nplt.pie(sizes, labels=q, colors=colors,\n        autopct='%1.1f%%', shadow=True)\nplt.show()","7e3345f8":"X1_train=sm_X\nY1_train=sm_Y\nprint(X1_train.shape)\nprint(Y1_train.shape)","7e636089":"# from sklearn.utils import shuffle\n# X1,Y1=shuffle(X1_train,Y1_train,random_state=0)\n# print(X1)","8a672346":"for i in data.columns:\n    print(i+str(\"\\t[\")+str(data[i].min())+\"\\t\"+str(data[i].max())+\"]\")","f2fb59c5":"# from sklearn.preprocessing import StandardScaler\n# scaler=StandardScaler()\n# print(X1_train.shape)\n# X1_train=scaler.fit_transform(X1_train)\n# print(X1_train.shape)","d733351d":"from sklearn.preprocessing import RobustScaler\nscaler=RobustScaler()\nprint(X1_train.shape)\nX1_train=scaler.fit_transform(X1_train)\nprint(X1_train.shape)","b947246d":"X1_test=np.array(test_data)\nX1_test=scaler.fit_transform(X1_test)","7dc38158":"X1_train","da8a38ff":"def one_hot_encode(x, n_classes):\n    return np.eye(n_classes)[x-1]\n\n\nY1_train=Y1_train.reshape((-1,1))\nencoded_Y1_train=one_hot_encode(Y1_train,4)\nencoded_Y1_train=(encoded_Y1_train.reshape((-1,4)))\nprint(encoded_Y1_train)","d498d766":"# various hyperparameter variables default values\ndefault_validation_split=0.25\nepochs=15\nnumber_of_classes=4\nbatch_size=100\nhidden_neurons_1=20\nhidden_neurons_2=20\ninput_size=11\nprint(X1_train.shape)\nprint(encoded_Y1_train.shape)","84751a6b":"# Shuffling and Splitting","980a0c1a":"from sklearn.model_selection import train_test_split\n\nX_train,X_validate,encoded_Y_train,encoded_Y_validate = train_test_split(X1_train,encoded_Y1_train,test_size=default_validation_split,shuffle=True)\nprint(\"Training X_train size : {} \".format(X_train.shape))\nprint(\"Training Y_train size : {} \".format(encoded_Y_train.shape))\nprint(\"Validating X_test size : {} \".format(X_validate.shape))\nprint(\"Validating Y_test size : {} \".format(encoded_Y_validate.shape))\nprint(encoded_Y_validate)","5582278f":"from keras.models import Sequential\nfrom keras.layers import Dense,Flatten,Dropout\nfrom keras.callbacks import EarlyStopping\n\n#Three layered architecture i.e. having two hidden layers .\nlandClassifier = Sequential()\nlandClassifier.add(Dense( hidden_neurons_1,  kernel_initializer = 'uniform', activation='sigmoid' , input_dim=11))\n# landClassifier.add(Dropout(0.2))\nlandClassifier.add(Dense(hidden_neurons_2, kernel_initializer = 'uniform', activation='sigmoid'))\nlandClassifier.add(Dense(4, kernel_initializer = 'uniform',activation='softmax'))\n\nlandClassifier.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'],)\n    \nlandClassifier.summary()\n","92e2bf82":"# from sklearn.utils import shuffle\n# X1_train, encoded_Y1_train = shuffle(X1_train , encoded_Y1_train,random_state=0)\n# print(X1_train)","34281ea5":"# early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n# X1_train=np.delete(X1_train, [X1_train.shape[1]-1], axis=1)\noutput=landClassifier.fit(X_train,encoded_Y_train, \n                   batch_size=batch_size,\n                   epochs=epochs,\n                   validation_data=(X_validate,encoded_Y_validate),\n                   shuffle=False\n#                    callbacks=[early_stopping]\n                  )","c83059cc":"accs=output.history['acc']\nval_accs=output.history['val_acc']\nx_axis=[i+1 for i in range(epochs)]\nplt.plot(x_axis,accs)\nplt.plot(x_axis,val_accs)\nplt.show()","2f91372e":"#Creating Confusion Matrix for the Whole Dataset Based on the Trained Classififer\nfrom sklearn.metrics import classification_report\ny_trained_by_model=np.argmax(landClassifier.predict(X1_train),axis=1)\ny_trained_by_model=y_trained_by_model+1\nprint(classification_report(Y1_train, y_trained_by_model))","221e6c91":"historys=[]\nbatch_size=[]\nvalidation=[]\nbatch_size_grid=[50,100,150]\nvalidation_grid=[0.1,0.15,0.20]\nfor i in batch_size_grid:\n    for j in validation_grid:\n        h=landClassifier.fit(X_train,encoded_Y_train, \n                   batch_size=int(i),\n                   epochs=epochs,\n                   validation_split=float(j),\n                   shuffle=True,\n#                    callbacks=[early_stopping]\n                  )\n        historys.append(h.history['val_acc'])\n        batch_size.append(i)\n        validation.append(j)","b03fa526":"history_array=[np.array(i) for i in historys]\nvalid_acc_history=[np.mean(i,axis=0) for i in history_array]\nprint(len(valid_acc_history))\nprint(\"Validation Accuracies obtained in various grid points: \\n{} \".format(valid_acc_history))","4b52ff17":"print(X1_test.shape)","0596dfad":"print(X1_test)","ecc6540c":"y_predicted=np.argmax(landClassifier.predict(X1_test),axis=1)","638d4ef8":"print(y_predicted)","aa8250dc":"y_predicted=y_predicted+1","324b6625":"print(y_predicted)","e6a26366":"get_class_map((y_predicted))","f24c211b":"test_data['target']=pd.DataFrame(y_predicted)","0a374700":"print(test_data)","c80090cc":"temp=pd.read_csv('..\/input\/land_test.csv')\ndf=pd.DataFrame(data=test_data)\ndf['I6']=temp['I6']\nl=df['target']\ndf=df.drop(['target'],axis=1)\ndf['target']=l\nprint(df)\ndf.to_csv('labelled_land_test.csv',index=False)","1c9b5c32":"*X5 and X6 have relatively less exlpaination of target variable as<br> compared to X1 which explains atound 79 percent of variation alone.*","21ad416a":"# *Preprocessing the Input:*","ea0ab68c":"Three layered architecture i.e. having two hidden layers .","fcf9a82a":"### *This shows that irrespective what class we have , these two X1 and X2 are strong positively correlated with eah other*","77196132":"## *Plot of Loss Accuracy and Accuracy versus Epoch*","537e52ac":"## *Since From the boxplot in the Univariate EDA of features ,  we saw that many points in X1,X2,X3,X4,X5,X6 were outside the whiskers* ,","792c46a9":"*Note : Earlier I used StandardScaler , but Now I have used the Concept of Robust Scaling , Robust Scaling is very effective for highly tail ended distributions and the distributions which might have good number of Outliers*","27422301":"# *Univariate EDA*","7cce236b":"*In case of eda of continuous variables, we need to understand the central tendency and spread of the variable.These are measured using methods such as Boxplot,Histogram\/Distribution Plot, Violin Plot etc.*","44c6b0bc":"# *Multivariate EDA of Features*","d6d0a269":"# *Best Validation Accuracy 96% comes out to be at Combination  7 which is with Hyperparameters<br> Validation Split = 0.1 <br> Batch Size =150* ","b59ac7ec":"*Part 1 -> Lets enode the classes into one hot enocoding* ","247051ca":"*We can clearly see from above that the I6 can easily be removed* <br> *from the dataset as it has explains very less variance of the target.*","e264d149":"Part 2-> Setting the Default hyperparameters in our architecture:-\n<br>\ndefault_test_split,\nepochs,\nnumber_of_classes,\nbatch_size,\nnumber of hidden layer neurons in first hidden layer\nnumber of hidden layer neurons in seond hidden layer","efc6f892":"## *Plots of X1 against X2*:","837235ca":"## *1. Mutual Classif Info Test:*\n<br> *This estimates mutual information for a  target variable.*","953ddf31":"*Using mutual information to select which features to include for classification ,  We can clearly see that I1 ,I3,l4 <br>   provide similar mutual information.*","08cc65c7":"### *This shows that a strong positive correlation exists b\/w X1 and X2*","8f57da76":"* ***Missing Values are a big problem if not treated earlier , lets check if there are any in this dataset***","2ded588e":"*Construct a **Coorelation matrix** of the int64 and float64 feature types*","d30d9ba2":"1.Batch_Size_Grid: Contains the Batch_Size Options<br>\n2.Validation_Split_Grid: Contains the Validation Split Options","98bedd3d":"## *Confusion Matrix*","9052dded":"# *An Example of Class Imbalance Problem*","2ae18443":"# *Predicting using the trained Model*","1fd56f9d":"# (Update Version- \n# Containing the Outlier Treatment Approach Using Tukey Method , and the Use of the Concpet of Robust Scaling)","619abf34":"#  *Exploratory Data Analysis*","9d95ba1f":"## *So Dropping Rows which have atleast one outlier point*","fdf34cb7":"*There are two approaches -><br>\neither you over sample it (i.e. you increase the number of samples in minority class)<br>\n. or you under sample it ( decrease the samples in the majority class to make it equal)\n*","3ac9828a":"# *Bivariate EDA of Features*","45aed99d":"## *2-> PCA -Principal Component Analysis*","57f6678d":"# *Using Tukey Method :*","8fac64d8":"* ***Printing the first 3 rows from the datafram***","e01a0e0d":"# *With Default Hyperparameters , We get around 93 percent Validation Accuracy which is not that bad .*","11a725be":"*As the ranges of different features are so different , we need to standardize them,otherwise it would be very difficult for the neural network to learn the information*","b875ea41":"# *Analyzing the Features*\n<br>**Consists of** \n<br>         *Univariate EDA *\n<br>          *BIvariate EDA* \n<br>          *Multivariate EDA* ","ac1fbe05":"# *Lets see if we can improve it by Tuning  Our hyperparameter*s\n# *Approach->  Using ( Grid Search )*","7c68aa19":"# *Building the Architecture for Training Dataset*","30d10780":"#  *Fitting the Architecture\/Model to our Dataset*","7b529aa0":"## *Dropping I6 from Dataset*","9beafdce":"*I don't prefer under sample as it some times removes the information in the data*","dd042a5c":"# *Hence We increased our Validation Accuracy from  93%  to   96%     which is pretty well*","fc882e9e":"## *Finally My Dataset looks like :*","7f26d371":"1. ***Loading the Training File***","b15ac3df":"# *Lets generate a class map : class vs number of samples*","2bff4904":"*I will be using SMOTE-Synthetic Minority OverSampling Method<br>\nand ADASYN -Adaptive Synthetic Oversanpling<br>\nfor balancing dataset and see if which one is better here*","4f8692a5":"# Land Classification - Analysis and Approach to Classifying the Land piece\n","d8e01eb5":"## *Analyzing the Training Results:*","58c7c274":"# *What to do with the Outliers: Lets Treat them !*","28c1799c":"# *Understanding the Relevance of the Feature  in Dataset*"}}