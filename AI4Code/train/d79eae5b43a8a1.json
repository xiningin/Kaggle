{"cell_type":{"d91ecd55":"code","15f1b3aa":"code","5855eaf4":"code","5c64194a":"code","f75a287a":"code","b0e7f13b":"code","f1e9d108":"code","94f93871":"code","74e04d2e":"code","25b26ca2":"code","0f6b2572":"code","1caaf5c4":"code","f25f8a49":"markdown","b5e5fa5a":"markdown","06a3aa4e":"markdown","b066eb14":"markdown","51c07713":"markdown","e59a4d46":"markdown","3ea745f2":"markdown"},"source":{"d91ecd55":"import matplotlib.pyplot as plt\nplt.figure(figsize=(18,10))\nplt.imshow(plt.imread('\/kaggle\/input\/model-predict-text-complexity\/model.png'))\nplt.axis('off')\nplt.show()\n#","15f1b3aa":"from nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\nclass TextComplexity():\n    '''\n    Return the complexity of a text\n    the more complex it is, the higher the number\n    \n    1. Fit the model on a big text or a list of strings\n       The model will create a ranking of the most used words\n       (tokens) in the text.\n    2. Predict: Calculate the median usage ranking of the words\n                in a string\n    '''\n    \n    def __init__(self):\n        # The pattern might have to be updated depending\n        # on the language\n        \n        self.tokenizer = RegexpTokenizer('[a-z\u00f6\u00e4\u00fc\u00df]+')\n        \n    def tokenize(self,text):\n        # Tokenize the text\n        \n        text = text.lower()\n        tokens_raw = self.tokenizer.tokenize(text)\n        \n        # Keep only tokens with at least two letters\n        tokens = []\n        for t in tokens_raw:\n            if len(t)>1:\n                tokens.append(t)\n            \n        return tokens\n    \n    \n    \n    def fit(self,X):\n        '''Fit the model with a text or a list of strings\n        '''\n        try:\n            object_iterator = iter(X)\n        except TypeError as te:\n            print('Error: Input X is not iterable.')\n            \n        if not isinstance(X,str):\n            # If X is a list of strings: Join the strings together\n            X = \" \".join(X)\n        \n        # Tokenize the text\n        tokens = self.tokenize(X)\n                \n        complete_dic = Counter(tokens)\n        \n        # Rank the tokens depending on their count\n        lst = list(complete_dic.items())\n        lst = sorted(lst, key = lambda x: x[1], reverse = True)\n        word = [x[0] for x in lst]\n        word_count = [x[1] for x in lst]\n        self.df = pd.DataFrame({'word':word, 'word_count': word_count})\n        rank_dict = self.df.to_dict()\n        self.rank_word = {v:k for k,v in rank_dict['word'].items()}\n        \n        self.corpus = list(self.rank_word.keys()) # List of the corpus\n        self.corpus_len = len(self.corpus) # The number of distinct tokens in the corpus\n        self.number_words = len(tokens) # The total number of tokens\n        \n        return None\n      \n        \n    def predict(self, X, topn = 100):\n        '''\n        Predict: Calculate the median usage ranking of the words\n                 in a string\n                 \n        Input: \n            X (string or list of strings)\n            topn (int): The % of words in the top-n most frequent words\n        '''\n        \n        try:\n            object_iterator = iter(X)\n        except TypeError as te:\n            print('Error: Input X is not iterable.')\n        \n        if not isinstance(X,str):\n            # If X is a list of strings: Join the strings together\n            X = \" \".join(X)\n            \n        # Tokenize the text\n        tokens = self.tokenize(X)\n\n        # Create a list with the rank of each token\n        lst_rank = []\n        for t in tokens:\n            if t in self.rank_word:\n                lst_rank.append(self.rank_word[t])\n                \n                # If the following part is ignored\n                # the tokens not in the corpus will be ignored\n                \n#             else: \n                # If a token isn't in the corpus\n                # add to the list the highest rank (corpus length)\n#                 lst_rank.append(self.corpus_len)\n\n        median = int(np.median(lst_rank))\n        percent_topn = sum([t < topn for t in lst_rank]) \/ len(lst_rank)\n\n        return median, percent_topn, lst_rank","5855eaf4":"import seaborn as sns\n\ndef plot_lst_rank(lst_rank, title):\n    hist_np = np.histogram(lst_rank,bins = 20, range=(0,10000))\n    x_labels = hist_np[1][:-1].astype(int)\n    y = hist_np[0]\n    y = 100 * y \/ sum(y)\n    plt.figure(figsize = (12,5))\n    sns.barplot(x = x_labels, y = y, color='gray')\n    plt.title(title)\n    plt.xlabel('')\n    plt.ylim(0,70)\n    plt.ylabel('% of words')\n    plt.show()\n    \nimport requests\n!pip install bs4 # bs4 isn't installed by default in this environment\nfrom bs4 import BeautifulSoup\n\ndef text_from_url(url):\n    ''' Return the text of a webpage\n    \n    Input:\n        url (string): The URL of the webpage\n        \n    Return:\n        output (string): The text of the webpage\n    \n    '''\n    result = requests.get(url)\n    html_page = result.content\n    sp = BeautifulSoup(html_page, 'html.parser')\n    txt = sp.find_all(text=True)\n    output = ''\n    blacklist = ['[document]',\n        'noscript',\n        'meta',\n        'header',\n        'html',\n        'head', \n        'input',\n        'script',\n        'style'\n    ]\n\n    for t in txt:\n        if t.parent.name not in blacklist:\n            output += '{} '.format(t)\n\n    return output\n\ndef predict_plot(text,topn = 100):\n    ''' Predict and plot the result for the text    \n    '''\n\n    complexity, topn_, lst_rank = model.predict(text, topn = topn)\n    title_start = 'Frequency of the words by their ranking\\n\\n'\n    complexity_info = f'Complexity of the text: {complexity}'\n    topn_info = f'\\nWords in the top{topn}: {topn_*100:.2f}%'\n    title = title_start + complexity_info + topn_info\n    plot_lst_rank(lst_rank,title)","5c64194a":"import logging\nimport sqlite3 as sql\ndb = '''..\/input\/english-wikipedia-articles-20170820-sqlite\/enwiki-20170820.db'''\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n\ndef query(select, db=db):\n    '''\n    - Connects to database\n    - Executes select statement\n    - Return results and column names\n    \n    Input: 'select * from articles limit 2'\n    Output: ([(1, 2, 3)], ['col1', 'col2', 'col3'])\n    '''\n    with sql.connect(db) as conn:\n        c = conn.cursor()\n        c.execute(select)\n        col_names = [str(name[0]).lower() for name in c.description]\n    return c.fetchall(), col_names","f75a287a":"%%time\n# Get articles from the database\nselect = 'select section_text from articles limit 100000'\noutput = query(select, db=db)\n\n# Convert the output to a list of texts\nlst_output = [art[0] for art in output[0]]\n\n# Create and fit the model\nmodel = TextComplexity()\nmodel.fit(lst_output)","b0e7f13b":"# Size of the list\nlen(lst_output)","f1e9d108":"# Have a look at one text\ntext = lst_output[0].replace('\\n','')\nprint(text[:10000])","94f93871":"# Predict the complexity of the text with the model\nn = 1000\ncomplexity, topn, lst_rank = model.predict(text, topn = n)\n\nprint(f'Complexity of the text: {complexity}')\nprint(f'\\nWords in the top{n}: {topn*100:.2f}%')","74e04d2e":"# Display more information about the complexity of the text\npredict_plot(text, 1000)","25b26ca2":"url = 'https:\/\/en.wikipedia.org\/wiki\/Ford_Mustang'\ntext = text_from_url(url)\n\n# Have a look at the start of the text of the webpage\nprint(text[:1000])","0f6b2572":"predict_plot(text,1000)","1caaf5c4":"# Simple text\ntext = 'This is a car'\npredict_plot(text,1000)","f25f8a49":"# 2. Fit the model<a class=\"anchor\" id=\"2\"><\/a>","b5e5fa5a":"### Predict the complexity of the simple text","06a3aa4e":"# Predict text complexity\n\n![text](https:\/\/i.imgur.com\/t9FCc3i.png)\n\n\n# Table of contents\n\n[<h3>1. Class & Functions<\/h3>](#1)\n\n[<h3>2. Fit the model<\/h3>](#2)\n\n[<h3>3. Predict the complexity of texts<\/h3>](#3)","b066eb14":"# Model overview","51c07713":"# 3. Predict the complexity of texts<a class=\"anchor\" id=\"3\"><\/a>","e59a4d46":"### Predict the complexity of the text on a webpage","3ea745f2":"# 1. Class & Functions<a class=\"anchor\" id=\"1\"><\/a>"}}