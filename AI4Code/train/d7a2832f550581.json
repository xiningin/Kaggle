{"cell_type":{"f6468bbe":"code","7bd53a9f":"code","b7ffcc2e":"code","7fe5ffbb":"code","2133d8ec":"code","17be18a9":"code","765b1192":"code","e81598bb":"code","072af504":"code","3425c07e":"code","cb6846b9":"code","28914376":"code","34857a66":"code","a638fe7d":"code","0f092804":"code","53679835":"code","524159dc":"code","11892e2d":"code","985320c4":"code","c311b552":"code","26ba915c":"code","2c7af14a":"code","e7c37a3d":"code","b93666f0":"code","54b97018":"code","ab84892d":"code","66fef2c6":"code","a4319432":"code","510c8733":"code","4f327882":"code","a36f95bb":"markdown","7bae62e7":"markdown","6eaa86e2":"markdown","faa3ab74":"markdown","fb2b755b":"markdown","61fb934f":"markdown","ed9591ef":"markdown","5a3aa9fa":"markdown","4aa9b06c":"markdown","2488a318":"markdown","0819b53c":"markdown","45046ad2":"markdown","90dc0e1d":"markdown","d66a9ef7":"markdown","ac958c09":"markdown","536e9bbf":"markdown"},"source":{"f6468bbe":"#Import EDA (exploratory data analysis) and plotting library\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n#Import models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n#Model evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score,f1_score\nfrom sklearn.metrics import plot_roc_curve","7bd53a9f":"data=pd.read_csv(\"..\/input\/dataset-of-songs-in-spotify\/genres_v2.csv\")\ndata.head()","b7ffcc2e":"#Check the number of rows and columns\ndata.shape","7fe5ffbb":"#Check and view data in transposed form to view all columns\ndata.head().T","2133d8ec":"#Since we are going to predict the genre , let's check the \"genre\" column for data types\ndata[\"genre\"].value_counts()","17be18a9":"#Let's plot the above date from \"genre\" columns in bar plot for better vizualization\ndata[\"genre\"].value_counts().plot(kind=\"barh\",color=[\"lightblue\"],title=\"Genres\");","765b1192":"#Let's check and see if we have missing data\ndata.isna().sum()","e81598bb":"#Finally we can check the data types in our data frame\ndata.dtypes\n#Based on below we will base the prediction analysis only on numerical data","072af504":"#Correlation analysis using Seaborn heatmap for data analysis\ncorr_matrix=data.corr()\nfig,ax=plt.subplots(figsize=(15,10))\nax=sns.heatmap(corr_matrix,\n              annot=True,\n              linewidths=0.5,\n              fmt=\".2f\",\n              cmap=\"YlGnBu\");","3425c07e":"#Split the data into X and y\nnum_data=data.drop([\"title\",\"Unnamed: 0\",\"song_name\",\"analysis_url\",\"track_href\",\"uri\",\"id\",\"type\"],axis=1) #drop all non-numeric columns\nX=num_data.drop(\"genre\",axis=1)\ny=num_data[\"genre\"]","cb6846b9":"#Split into training and test sets\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)","28914376":"#For that we will create a fuction in order to evaluate and compare models easily\nmodels={\"LogReg\":LogisticRegression(),\n       \"KNN\":KNeighborsClassifier(),\n       \"Random Forest\":RandomForestClassifier()}\ndef fit_and_score (models,X_train,X_test,y_train,y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models\n    \"\"\"\n    np.random.seed(1)\n    model_scores={}\n    for name , model in models.items():\n        model.fit(X_train,y_train)\n        model_scores[name]=model.score(X_test,y_test)\n    return model_scores","34857a66":"model_scores=fit_and_score(models=models,X_train=X_train,X_test=X_test,y_train=y_train,y_test=y_test)\nmodel_scores","a638fe7d":"model_compare=pd.DataFrame(model_scores,index=[\"Accuracy\"])\nmodel_compare.T.plot.barh(color=[\"lightblue\"]);","0f092804":"#Tuning for KNN model\ntrain_scores=[]\ntest_scores=[]\n#Create list of different n-neighbors\nneighbors = range(1,15)\n#Setup KNN instance\nknn=KNeighborsClassifier()\n#Loop through different neigbors\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    knn.fit(X_train,y_train) #Fit the model\n    train_scores.append(knn.score(X_train,y_train)) #Update the train score list\n    test_scores.append(knn.score(X_test,y_test)) #Update test scores list","53679835":"plt.plot(neighbors,train_scores,label=\"Train scores\")\nplt.plot(neighbors,test_scores,label=\"Test scores\")\nplt.xticks(np.arange(1,15,1))\nplt.yticks(np.arange(0.1,1,0.1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model scores\")\nplt.legend;","524159dc":"#Hyperparameter grid for Logistic Regression\nlog_reg_grid={\"C\": np.logspace(-4,4,20),\n             \"solver\":[\"liblinear\"]}\n#Hyperparameter grid for Random Forest\nrf_grid={\"n_estimators\": np.arange(10,1000,50),\n         \"max_depth\":[None,3,5,10],\n         \"min_samples_split\":np.arange(2,20,2),\n         \"min_samples_leaf\":np.arange(1,20,2)}","11892e2d":"#Tune LogReg model\nnp.random.seed(1)\nrs_log_reg=RandomizedSearchCV(LogisticRegression(),\n                             param_distributions=log_reg_grid,\n                             cv=5,\n                             n_iter=20,\n                             verbose=True)\n#Fit random hyperparameter search model to LogReg\nrs_log_reg.fit(X_train,y_train)","985320c4":"#Let's check the best parameters\nrs_log_reg.best_params_","c311b552":"rs_log_reg.score(X_test,y_test)","26ba915c":"#Tune Random Forest model\nnp.random.seed(42)\nrs_rf=RandomizedSearchCV(RandomForestClassifier(),\n                        param_distributions=rf_grid,\n                        n_iter=20,\n                        verbose=True)\nrs_rf.fit(X_train,y_train)","2c7af14a":"rs_rf.best_params_","e7c37a3d":"rs_rf.score(X_test,y_test)","b93666f0":"#Make predictions with tuned model\ny_preds=rs_rf.predict(X_test)\npreds_df=pd.DataFrame(y_preds)\npreds_df.head()","54b97018":"#Check actual vs predictions\ncomparison=pd.DataFrame(data={\"actual\":y_preds,\"prediction\":y_test})\ncomparison.head()","ab84892d":"#Calculate number of true and false predictions\ncomparison[\"result\"]=comparison[\"actual\"]==comparison[\"prediction\"]\ncomparison[\"result\"].value_counts().plot(kind=\"barh\",color=[\"lightblue\"],title=\"Comparison\");","66fef2c6":"#Classification report for precision, recall, f1score and accuracy\nprint(classification_report(y_test,y_preds))","a4319432":"feat=RandomForestClassifier(n_estimators= 460,       #Using best params obtained earlier\n                            min_samples_split= 6,\n                            min_samples_leaf= 9,\n                            max_depth= None)\nfeat.fit(X_train,y_train)","510c8733":"feat.feature_importances_","4f327882":"#Match coefficients of features to columns\nfeature_dict=dict(zip(num_data.columns,list(feat.feature_importances_)))\n#Vizualize the feature importance\nfeature_data=pd.DataFrame(feature_dict,index=[0])\nfeature_data.T.plot.barh(title=\"Feature importance\",legend=False,color=\"lightblue\",grid=False);","a36f95bb":"The above score is very very small increase from initial scoring, now let's try same with Random Forest and see how it can improve the scoring","7bae62e7":"## Hyperparameter tuning\n1. HP tuning by hand\n2. HP tuning with RandomizedSearchCV","6eaa86e2":"#### Feature importance\nChecking with features contributed most to the outcome","faa3ab74":"### Evaluating tuned machine learning classifier , beyond accuracy\n\n* Comparison of real and predicted results\n* Classification report\n* Precision\n* Recall\n* F1 score\n\nSince Random Forest tuned model give the best results, going further we will evaluate only that model","fb2b755b":"## Modelling","61fb934f":"## Model comparison","ed9591ef":"As we can see above, missing values are only in last 3 columns and this will not affect our analysis of predicting genre","5a3aa9fa":"#### Hyperparameter tuning for KNN manually","4aa9b06c":"#### Classification report for Precision, F1 Score and Recall","2488a318":"# Predicting song's genre using ML\n* Problem definition - check several classification models and try to achieve best accuracy in prediction genre of a song from data provided\n* Data - Data has been obtained from Kaggle: https:\/\/www.kaggle.com\/mrmorj\/dataset-of-songs-in-spotify\n* Evaluation - try several models and reach the best possible accuracy\n* Features - data consists of several features such as danceability, energy, key, loudness, mode in numerical format and other categorical features.\n* Modelling - we will try K-Nearest Neigbors, Random Forest and Logistic Regression model for evaluation\n* Experimentation - this section will involve fine tuning some of the hyperparameters to see if we can improve the accuracy","0819b53c":"#### We will use 3 different models for this problem:\n    1. Logistic regression\n    2. K-Nearest\n    3. Random Forest","45046ad2":"#### Hyperparameter tuning with RandomizedSearchCV for LogReg and Random Forest models\nLet's tune LogReg and Random Forest models using RandomizedSearchCV","90dc0e1d":"The above chart shows that the best K-value giving higher prediction is *1* where we can see 36% accuracy level which is still low.\nLet's look for other models and see whether we can improve accuracy","d66a9ef7":"Preparing the tools\nPandas,Matplotlib, and Numpy for data analysis and manipulation","ac958c09":"#### Now as we have initial Accuracy scoring of 3 models, let's look at following:\n* Hyperparameter tuning\n* Feature importance\n* Confusion matrix\n* Cross validation\n* Precision, Recall, F1 score\n* Classification report","536e9bbf":"## Load data"}}