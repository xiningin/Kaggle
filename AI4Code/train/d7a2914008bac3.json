{"cell_type":{"2158ae88":"code","80a28d33":"code","ae31844c":"code","1a174f53":"code","eda738b3":"code","90c8e845":"code","26237724":"code","17ce9278":"code","c22fe2b5":"code","6d1f3d42":"code","9869cc7a":"code","f8bdee41":"code","4a0b1974":"code","a3ebdfb5":"markdown","56d6dda2":"markdown","ccdb4370":"markdown","70303f34":"markdown","be4a3ead":"markdown","0f90bcf4":"markdown","31496f00":"markdown","624453e6":"markdown","601a2508":"markdown","e311e3e5":"markdown","74bd34d0":"markdown"},"source":{"2158ae88":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv(\"..\/input\/5minute-crafts-video-views-dataset\/5-Minute Crafts.csv\")\ndf.head()","80a28d33":"from collections import defaultdict\n\nwordFreqDict = defaultdict(int)\n\nallTitles = [text for idx, text in df.title.items()]\nfor title in allTitles:\n    tokenized = title.split()\n    for token in tokenized:\n        wordFreqDict[token] += 1\n\nsortedByFreq = sorted(list(wordFreqDict.items()), key=lambda pair:pair[1], reverse=True) #Most common first\nkeyWords = [keyWord for keyWord, _ in sortedByFreq]\nfrequencies = [freq for _, freq in sortedByFreq]\n\ndfWordFreq = pd.DataFrame(sortedByFreq, columns=[\"Key Word\", \"Frequency\"])\ndfWordFreq.head(20)","ae31844c":"import matplotlib.pyplot as plt\n\nfreqList = dfWordFreq.Frequency.astype(\"int\").tolist() \nplt.hist(freqList, bins=1000)\nplt.xlim(0, 50)\nplt.xlabel(\"No. Of Words\")\nplt.ylabel(\"Word Frequency\")\nprint(\"Total no. of unique words:{0}\".format(len(freqList)))","1a174f53":"dfNumbers = dfWordFreq.loc[dfWordFreq[\"Key Word\"].apply(lambda text: text.isnumeric())]\nsrNumbers = dfNumbers.copy()\nsrNumbers[\"Key Word\"] = srNumbers[\"Key Word\"].astype(\"int\")\nsrNumbers.sort_values(by=\"Key Word\", ascending=False).head(10)","eda738b3":"#Markov Probability matrix generation\n#(tokens:string[], dict<current:string, dict<string:next, int: count>>) => void\ndef countNextTokens(tokens, nextWordCountDict):\n    nTokens = len(tokens)\n    if nTokens == 1:\n        return #can't add to the markov chain\n    for ind in range(1, nTokens):\n        current = tokens[ind]\n        prev = tokens[ind - 1]\n        nextWordCountDict[prev][current] += 1\n        \n#(dict<current:string, dict<string:next, int: count>>) => dict<current:string, dict<string:next, float: count>>\ndef counts2Probability(nextWordCountDict):\n    nextWordProbDict = {} #sum of all next word probalities will equal 1 for each \"current\" word\n    for word, counts in nextWordCountDict.items():\n        cumulativeCount = sum([count for _, count in counts.items()])\n        scale = 1 \/ cumulativeCount;\n        nextWordProbDict[word] = {nextWord: (freq * scale) for nextWord, freq in counts.items()}\n    return nextWordProbDict\n\n#(string[]) => dict<current:string, dict<string:next, float: count>>\ndef genMarkovChainSparseMatrix(sentences):\n    #after counting, the probability of the next word can be computed\n    #dict<current:string, dict<string:next, int: count>>\n    nextWordCount = defaultdict(lambda:defaultdict(int))\n    for sentence in sentences:\n        countNextTokens(sentence.split(), nextWordCount)\n    return counts2Probability(nextWordCount)\n\n#Get the likelihood of the subsequent words\nmarkovMatrix = genMarkovChainSparseMatrix(allTitles)","90c8e845":"import random \nrandom.seed(0)\n\n#Generating statistically expected sentences using the above markov chain\n#Generation ends when a word is encountered with no subsequent state in the markov chain\ndef GenerateSentence(seedWord, markovMatrix):\n    if seedWord not in markovMatrix: #unknown token, no way to predict using the Markov Chain (could try to find closest matching word though)\n        return []\n    tokens = [seedWord]\n    currentToken = seedWord\n    while currentToken in markovMatrix: #i.e. while future state transitions are available\n        r = random.random()\n        cumSum = 0\n        for wordCandidate, prob in markovMatrix[currentToken].items():\n            cumSum += prob\n            if cumSum >= r:\n                tokens.append(wordCandidate)\n                currentToken = wordCandidate\n                break\n    #Merge the chosen word tokens into a single string\n    return \" \".join(tokens)\n\nfor i in range(5): \n    genSentence = GenerateSentence(\"Pregnancy\", markovMatrix)\n    print(\"{0}\\n{1}\".format((\"-\"*5) + str(i) + (\"-\"*5),genSentence))","26237724":"#n-gram extension\n\n#(tokens:string[], dict<current:tuple<string...>,dict<string:next, int: count>>) => void\ndef countNextNGramTokens(tokens, nextWordCountDict, nGramCount):\n    nTokens = len(tokens)\n    if nTokens <= nGramCount:\n        return \n    currentNGram = (*tokens[:nGramCount],) #Python dicts conveniently support tuple keys\n    for nextToken in tokens[3:]:\n        nextWordCountDict[currentNGram][nextToken] += 1\n        currentNGram = currentNGram[1:] + (nextToken,)\n\n#(string[]) => dict<current:tuple<string...>, dict<string:next, float: count>>\ndef genNGramMarkovChainSparseMatrix(sentences, nGramCount):\n    #dict<current:tuple<string...>, dict<string:next, int: count>>\n    nextWordCount = defaultdict(lambda:defaultdict(int))\n    for sentence in sentences:\n        countNextNGramTokens(sentence.split(), nextWordCount, nGramCount)\n    return counts2Probability(nextWordCount) #can be reused in this case\n\ndef genSentenceUsingNGram(seedWord, markovMatrix):\n    startingCandidates = [key for key in markovMatrix.keys() if key[0] == seedWord]\n    nCandidates = len(startingCandidates)\n    if nCandidates == 0:\n        return []\n    \n    currentNGram = startingCandidates[random.randint(0, nCandidates - 1)]\n    tokens = list(currentNGram)\n    while currentNGram in markovMatrix:\n        r = random.random()\n        cumSum = 0\n        for wordCandidate, prob in markovMatrix[currentNGram].items():\n            cumSum += prob\n            if cumSum >= r:\n                tokens.append(wordCandidate)\n                currentNGram = currentNGram[1:] + (wordCandidate,)\n                break\n    #Merge the chosen word tokens into a single string\n    return \" \".join(tokens)\n    \n    \n#Calculates the arthmetric mean no. of possible state transitions per n-gram. Values closer to 0 indicate lower prob. of variation in sentences\n#(dict<TKey1, dict<TKey2, TValue>>) => float\ndef avgBranchFactor(nGramMatrix):\n    nKeys = len(nGramMatrix)\n    nTotalChildBranches = sum([len(childDict) for _, childDict in nGramMatrix.items()])\n    return nTotalChildBranches \/ nKeys\n\nnGramMatrices = [(nGramCount, genNGramMarkovChainSparseMatrix(allTitles, nGramCount)) for nGramCount in range(1,6)]\nbranchFactors = [(count,avgBranchFactor(mat)) for count,mat in nGramMatrices]\nfor nGramCount, branchFactor in branchFactors:\n    print(\"n-gram count:{0},\\tBranch Factor:{1}\".format(nGramCount, branchFactor))","17ce9278":"#plot histograms of the no. of branches per n-gram\nplt.figure(figsize=(15, 5))\nfor nGramCount, mat in nGramMatrices:\n    branches = [len(childDict) for _,childDict in mat.items()]\n    plt.hist(branches, bins=200, alpha=0.25, label=\"n-gram:{0}\".format(nGramCount))\n\nplt.xlabel(\"Branches\", size=14)\nplt.ylabel(\"Counts\", size=14)\nplt.ylim(0,5000)\nplt.xlim(1,6)\nplt.legend()\nplt.show()","c22fe2b5":"def getUniqueSentences(seedWord, nGramModel, nTries):\n    random.seed(0)\n    results = []\n    for i in range(nTries):\n        results.append(genSentenceUsingNGram(seedWord, nGramModel))\n    return list(set(results))\n\nfor nLevel in range(5):\n    print(\"-\"*5 + \" {0}-Gram Examples: \".format(nLevel + 1) + \"-\"*5)\n    for sent in getUniqueSentences(\"5-MINUTE\", nGramMatrices[nLevel][1], 5):\n        print(sent)","6d1f3d42":"#perplexity -> lower scores indicate better fit to the markov model\n#(testSentence:string, nGramModel:dict<tuple(string...), dict<string, float>>) => float (-1 for OoV)\ndef perplexity(testSentence, nGramModel):\n    firstKey = next(iter(nGramModel))\n    nGramCount = len(firstKey)\n    testTokens = testSentence.split()\n    if len(testTokens) < nGramCount:\n        return -1\n    currentTokens = (*testTokens[:nGramCount],)\n    probabilities = []\n    for nextToken in testTokens[nGramCount:]:\n        if (currentTokens in nGramModel) and (nextToken in nGramModel[currentTokens]):\n            #record down conditional probability of the next toke\n            probabilities.append(nGramModel[currentTokens][nextToken])\n        else:\n            #special value to indicate unknown (out of vocabulary) words\n            probabilities.append(-1)\n        currentTokens = currentTokens[1:] + (nextToken,)\n    #Compute perplexity (simply return -1 if out-of-vocab words are found)\n    if any([p < 0 for p in probabilities]):\n        return -1\n    else:\n        #return inverse of Geometric Mean\n        exponent = len(probabilities)\n        #Calculate inv individually to avoid precision errors from multiplying many small numbers together\n        invertedProbabilities = [pow(p, (-1.0\/exponent)) for p in probabilities]\n        return np.prod(invertedProbabilities) \n    \nprint(perplexity(\"5-MINUTE LIFE HACKS FOR YOUR BEAUTY\", nGramMatrices[1][1]))","9869cc7a":"#(string[]) => dict<current:tuple<string...>, dict<string:next, float: count>>\n#unknown words are assigned the token \" unk\"\ndef genMarkovChainSparseMatrixWithAddK(sentences, kFactor, unknownThreshold):\n    #First get the global count of all words to gauge which low-frequency words should be thrown into the \"unknown\" category\n    wordFreq = defaultdict(int)\n    for sentence in sentences:\n        for tokens in sentence.split():\n            wordFreq[tokens] += 1\n    \n    #Filter out words that are below the threshold frequency\n    totalUnderThreshold = 0\n    unignoredWords = set()\n    for key, count in wordFreq.items():\n        if count > unknownThreshold:\n            unignoredWords.add(key)\n    \n    #Count the transition states after mapping low-freq words to \" unknown\"\n    #dict<current:string, dict<string:next, int: count>>\n    nextWordCount = defaultdict(lambda:defaultdict(int))\n    nextWordCount[\" unknown\"] #ensure it gets added\n    for sentence in sentences:\n        mappedTokens = [ token if (token in unignoredWords) else \" unknown\" for token in sentence.split()]\n        countNextTokens(mappedTokens, nextWordCount)\n\n    allPossibleTokens = list(unignoredWords) + [\" unknown\"]\n    #Now perform the add-k step for transition state\n    for prevToken, nextStates in nextWordCount.items():\n        for token in allPossibleTokens:\n            nextStates[token] += kFactor\n    return counts2Probability(nextWordCount) \n\ndef smoothed1GramPerplexity(testSentence, smoothedOneGramModel):\n    tokens = [t if t in smoothedOneGramModel else \" unknown\" for t in testSentence.split()]\n    prevToken = tokens[0]\n    probabilities = []\n    for nextToken in tokens[1:]:\n        probabilities.append(smoothedOneGramModel[prevToken][nextToken])\n        prevToken = nextToken \n    exponent = -1\/len(probabilities)\n    invertedProbabilities = [pow(p, exponent) for p in probabilities]\n    return np.prod(invertedProbabilities)\n\nsmoothed1Gram = genMarkovChainSparseMatrixWithAddK(allTitles, 0.05, 0)\ntestSentences = [\n    \"5-MINUTE LIFE HACKS FOR YOUR BEAUTY\",\n    \"LATE SUMMER HACKS TO SAVE YOUR\",\n    \"5-HOUR KAGGLE GPU KERNELS FOR INSANITY\",\n    \"THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\",\n    \"ACCORDING TO ALL KNOWN LAWS OF AVIATION\",\n    \"THERE IS NO WAY A BEE SHOULD BE ABLE TO FLY\",\n    \"ITS WINGS ARE TOO SMALL TO GET ITS FAT LITTLE BODY OFF THE GROUND\"\n]\nfor sentence in testSentences:\n    perplexity = smoothed1GramPerplexity(sentence, smoothed1Gram)\n    print(\"Perplexity:{0}\\tSentence:{1}\".format(perplexity, sentence))","f8bdee41":"#(testSentence:string, markovModels:dict<int, dict<tuple<string...>, dict<string, float>>, lambdaFactor:float) => float\ndef smoothedPerplexityWithStupidBackoff(testSentence, markovModels, lambdaFactor):\n    def getPerplexity(priorTokens, currentToken, nGramLevel):\n        currentModel = markovModels[nGramLevel]\n        if currentToken in currentModel:\n            if(nGramLevel > 1):\n                return currentModel[priorTokens][currentToken]\n            else:\n                return currentModel[priorTokens[0]][currentToken] #1-gram model expects a string, not a tuple for the prior\n        else:\n            #Search a lower n-gram model to find a possible fit, but progressively reduce the conditional probability while traversing down the models\n            #In this case, n-gram=1 will be a markov model with k-factor smoothing (i.e. accounts for all possibilities using the \" unknown\" token)\n            return lambdaFactor * getPerplexity(priorTokens[1:], currentToken, nGramLevel - 1)\n    highestModel = max(list(markovModels.keys()))\n    #Use the 1-Gram to get the known vocabulary\n    oneGramModel = markovModels[1]\n    tokens = [t if t in oneGramModel else \" unknown\" for t in testSentence.split()]\n    prevTokens = (*tokens[:highestModel],)\n    probabilities = []\n    for nextToken in tokens[highestModel:]:\n        probabilities.append(getPerplexity(prevTokens, nextToken, highestModel))\n        prevTokens = prevTokens[1:] + (nextToken,)\n    exponent = -1\/len(probabilities)\n    invertedProbabilities = [pow(p, exponent) for p in probabilities]\n    return np.prod(invertedProbabilities)\n\n# The 1-Gram will be a smoothed model\n#(sentences:string[], maxNGramLevel:int) => dict<int, dict<tuple<string...>, dict<string, float>>\ndef createStupidBackoffModelDictionary(sentences, maxNGramLevel):\n    modelDict = {}\n    for nGramLevel in range(2, maxNGramLevel + 1):\n        modelDict[nGramLevel] = genNGramMarkovChainSparseMatrix(sentences, nGramLevel)\n    #insert a k-smoothed level for the 1-Gram\n    modelDict[1] = genMarkovChainSparseMatrixWithAddK(sentences, 0.05, 0)\n    return modelDict\n\nstupidBackoffModel = createStupidBackoffModelDictionary(allTitles, 3)","4a0b1974":"for sentence in testSentences:\n    perplexity = smoothedPerplexityWithStupidBackoff(sentence, stupidBackoffModel, 0.4)\n    print(\"Perplexity:{0}\\tSentence:{1}\".format(perplexity, sentence))","a3ebdfb5":"With higher values of N, the chance of encountering unseen word orderings are higher, since the number of combinations of preceding states is also greater. While Add-K smoothing can also be applied to N-Gram models, another approach would be to back off to a lower N-Gram model, and attempt to search for a possible transition there. While doing so, a penalty factor to the conditional probabilty is given (since it is technically outside of the trained Corpus for the highest N-Gram). The back off process is repeated until a match is found. This process is the basis of *Stupid-Backoff*.\n\n$$\nP^{N - Gram}(\\text{Word}_{next}|\\{\\text{Word}_{i-1},\\text{Word}_{i-2}\\}) =\\begin{cases} \nP^{N - Gram}(\\text{Word}_{next}|\\{\\text{Word}_{i-1},\\text{Word}_{i-2}\\}) & \\text{if available} \\\\\n\\lambda P^{(N-1)- Gram}(\\text{Word}_{next}|\\{\\text{Word}_{i-1}\\}) & \\text{otherwise}\n\\end{cases}\n$$\n\nIn the following example, the 1-Gram model has K-Smoothing applied to it, to ensure that a conditional probability will always be available when it reaches the terminal N=1 state.","56d6dda2":"From the generated sentences, it is evident that generating sentences by choosing new words solely based on the previous word has its shortcomings, with the notion of grammatical coherency being completely forsaken at times.\nWithout resorting to complex Long-Term Short-Term Memory Neural Networks, a simple extension to the probability-based Markov Model can be used to help it retain more information about previous words while choosing new words during generation.","ccdb4370":"Now that the word-to-word probabilities have been computed for all the titles, text generation can begin. Generating text simply involves 1) keeping track of the latest word that was generated (*currentToken*), 2) looking up which state it corresponds to in the Markov Model 3) and randomly selecting the next word based on the previously computed transitional probabilities. In this example, generation stops when a state with no possible transitions is encountered (i.e. a word within the corpus that is never followed by anything else).\n\nNote that while a seed-word from the corpus is chosen to start the generated sentence, it is entirely possible to create the Markov Model with special tokens denoting the start of a sentence (in this case, any string containing a space, such as \" start\" would do since words are split based on whitespace). This would add an additional state (*Token<sub>sentence start<\/sub>*) that can be used to start the generation chain instead of a user-supplied word. ","70303f34":"# Handling Out Of Vocabulary words during evaluation (Smoothing)\n\nIn the previous example, the perplexity of a sentence was computed. However, using the raw Markov Model, if an unseen word (Out of Vocabulary) or, more commonly, a word follows a prior word in an order which was not observed in the original corpus, the evaluated conditional probability would be 0 (essentially resulting in infinite perplexity). This would confine useful values of perplexity to only sentences that both 1)Fit within the known vocabulary 2)Fit word orders that are only seen in the training Corpus. Clearly, this would be too restrictive to be useful. The following are some ways to work around this issue:\n\n**1) Add-K smoothing**\n\nTo avoid encountering transition states with 0-probability, after the counting phase, a count of *k* (any real number, even fractions) observations can be added to each and every transition state. This, coupled with the inclusion of an *unknown* word token (to which every Out-Of-Vocabulary word is mapped to), allows previously unencountered words and word orderings to produce more reasonable conditional probabilities.","be4a3ead":"# N-Grams: Markov Models with <i><sub>slightly<\/sub><\/i> more memory\n\nBasic Markov Models only use a single word to represent each state, with each transition probability represented as:\n$$P^{Markov-Chain}(\\text{word}_{next}|\\text{word}_{prev})$$\n\nN-Grams take this a step further. Instead of having a just a single word, a sequence of words are used to represent each state, thereby giving the stochastic model a little more \"memory\". State transitions to a word(*word<sub>next<\/sub>*) are now based on the sequence of words(*word<sub>i - 1<\/sub>, word<sub>i - 2<\/sub>, ...*) that preceded it.\n$$P^{N-Gram}(\\text{word}_{next}|\\{\\text{word}_{i-1},\\text{word}_{i-2},\\text{word}_{i-3}...\\})$$\n\nThere is potential to improve the model by selecting higher values of *N*(the number of words within each state), with 3-5 being commonplace for larger text corpuses. In this case, a lower value of N is required due to the small Corpus size (Vocabulary size of \u2248 3000, with many words used only once). Additionally, higher values of N result in more exponentially more possible states, which results in a lower probability of observing alternative transition states after a given n-gram, potentially resulting in a highly determistic model instead (i.e. if the model has a large number of N-Grams which only have one possible transition state).","0f90bcf4":"As expected, sentences clearly outside of the Corpus score much higher perplexity values.","31496f00":"# **Markov Chain Text Generation**\n\nMarkov Chains can be thought of as probability-based state machines, where a transition the act of choosing a state to transition to is simply based on the probability assigned to that transition. In the context of text generation, each state can be viewed as a single word, whereby new words are added to a sentence by determining which state (word) to transition next to.\n\nFor the word-to-word transition probabilities, probability weights can be computed by counting the total number of times a particular word (*word<sub>next<\/sub>*) is seen after a prior word (*word<sub>prior<\/sub>*). This count can be divided by the total numbers of words that are observed directly after *word<sub>prior<\/sub>* within a corpus to get its transitional probability within a Markov Model.","624453e6":"Some sample sentences generated using the Bigram (N = 2) show an improvement over the rambling nature of the Unigram (N=1) model, akin to someone who has learned to stay on topic instead of spouting gems such as <i>\"CRAFTS AND MAGIC TRICKS THAT WORK MAGIC TRICKS TO TRICK PEOPLE CAN TRY RIGHT NOW ABOUT TRUE STORUES ON A KITCHEN PRO\"<\/i>.\n\nThe limitations of the small corpus is also evident within the 5-Gram example, where the lack of examples within the corpus is likely causing states to repeatedly loop in circuits, generating repetitive fragments such as <i>\"CRAYON HACKS CRAYON HACKS\"<\/i> and <i>\"CRAFTS AND HACKS AND HACKS\"<\/i>","601a2508":"# Perplexity: A measure of similarity to the underlying Corpus\n\nAside from generation, it is also possible to use Markov Models to evaluate the similarity of an input text fragment to the Corpus used to the the model. Given that the Markov Model represents the probabilty of observing a word after another, the statistical probability of encountering a fragment can be calculated by multiplying all those probabilities together. This product is then used as a measure of \"similarity\" to the corpus.\n\nAdmittedly, multiplying conditional probabilities together (ranging from 0-1) repeatedly results in very small numbers. Additionally, the final probability is bound to grow smaller as more words are added (more multiplications need to occur). To resolve these 2 issues, a different metric is used for evaluating Markov\/N-Gram models - *Perplexity*.\n\nInstead of multiplying conditional probailities, the inverse of each conditional probability is multiplied together (yielding values greater than 1). The final product is then raised to the power of $1\/N_{tokens}$. This is essentially the Geometric Mean of the inverse of each probability, with the exponent helping to mitigate the change in scale when more words are added.\n$$Perplexity = \\sqrt[n]{\\prod_{} \\frac{1}{w_{i}} }$$\nSmaller perplexity values indicate a closer fit to the model, whilst larger values indicate the opposite.","e311e3e5":"In this case, the average number of transition states for each N-gram in the model is calculated (named as *Branch Factor*). Switching from a Unigram (N = 1) to a Digram (N = 2) results in a drastic decrease in the average number of transition states from each N-gram. The histogram below further reveals how few states have more than 1 transition for the Diagram case (only $\\approx$1000 out of a  possible $\\text{Vocabulary}^{2} \\approx 3000^{2} = 9000000$ states)","74bd34d0":"**2) Stupid Backoff**"}}