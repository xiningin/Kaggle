{"cell_type":{"5662c85a":"code","86baa273":"code","b8401a87":"code","5b6c0726":"code","4f7ae168":"code","9f43a94f":"code","b28a8be3":"code","1ca521c5":"code","c8ad57b1":"code","0571de35":"code","1676020c":"code","b9f3ac68":"code","794d3adc":"code","74d39dad":"code","e879d326":"code","a1c8cd19":"code","92133e80":"code","301c98bc":"markdown","6506207e":"markdown","dcffc805":"markdown","c6b537c0":"markdown","03445240":"markdown","1f788c69":"markdown","223b3ac1":"markdown","b9e8b403":"markdown","634c38e0":"markdown","6fe8a43d":"markdown","9ab76d91":"markdown","7d074349":"markdown","3e478a27":"markdown","a3c1a4f8":"markdown"},"source":{"5662c85a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# The files are on paths \/kaggle\/input\/digit-recognizer\/*.csv\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# torch libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler","86baa273":"def get_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","b8401a87":"train_dataset=torch.from_numpy(np.genfromtxt('\/kaggle\/input\/digit-recognizer\/train.csv',delimiter=',',skip_header=1))\ntest_dataset=torch.from_numpy(np.genfromtxt('\/kaggle\/input\/digit-recognizer\/test.csv',delimiter=',',skip_header=1))\nprint(train_dataset.shape, test_dataset.shape)","5b6c0726":"class ConvolutionalNNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self. verbose = False\n        self.lr = 0.01 # Learning rate\n        \n        self.pool = nn.MaxPool1d(2)                                       # bs * 16 * (arr_length\/2 392)\n        \n        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1) # bs * 16 * arr_length 784\n        self.conv2 = nn.Conv1d(16, 16, kernel_size=3, stride=1, padding=1) # bs * 16 * (arr_length\/2)\n        self.conv3 = nn.Conv1d(16, 16, kernel_size=3, stride=1, padding=1) # bs * 16 * (arr_length\/4)\n        self.conv4 = nn.Conv1d(16, 16, kernel_size=3, stride=1, padding=1) # bs * 16 * (arr_length\/8)\n\n        self.fc1 = nn.Linear(784, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = self.pool(F.relu(self.conv4(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        \n        return x\n    \n    @staticmethod\n    def accuracy(y_pred, y_act):\n        _, preds = torch.max(y_pred, dim=1)\n        # return f1_score(y_act, preds, average='weighted')\n        # return accuracy_score(preds, y_act)\n        return torch.sum(preds==y_act).item()\/ len(preds)","4f7ae168":"def func1(self, optimizer_class=None):\n    optimizer_class = torch.optim.SGD if optimizer_class is None else optimizer_class\n    self.optmzr = optimizer_class(self.parameters(), lr=self.lr)\n\n# Add the function to class\nConvolutionalNNet.set_optimizer = func1","9f43a94f":"def func2(self, loss_fn, xb, yb, opt=None, metric=None):\n    preds = self(xb)\n    # calculate loss\n    loss = loss_fn(preds, yb)\n\n    if opt is not None: # no optimization in validation set\n        loss.backward() # compute gradients\n        opt.step() # perform optimization\n        opt.zero_grad() # reset gradients\n\n    metric_result = None\n    if metric is not None:\n        metric_result = metric(preds, yb)\n\n    return loss.item(), len(xb), metric_result\n\n# Add the function to class\nConvolutionalNNet.loss_batch = func2","b28a8be3":"def func1(self, loss_fn, metric, test_data_dl):\n    \"\"\"Function to evaluate\"\"\"\n    with torch.no_grad(): # This flag tells not to calculate the gradients\n        # Pass all the test data through the model\n        for yb,xb in test_data_dl:\n            avg_loss, num_ds, avg_metric = self.loss_batch(\n                loss_fn, xb, yb, metric=metric\n            )\n    return avg_loss, avg_metric\n\ndef func2(self, train_cuda_dl, epochs=1, evaluate_dl=None):\n    \"\"\"Function to fit model with the training data\"\"\"\n    time_to_95_set = False\n    train_loss,train_acc,test_loss,test_acc = [],[],[],[]\n    loss_fn = F.cross_entropy\n\n    iter_cnt = 0\n    for epoch in range(epochs):\n        self.train()\n        for yb,xb in train_cuda_dl:\n            tr_loss, n_count, tr_metric = self.loss_batch(loss_fn, xb, yb, self.optmzr, ConvolutionalNNet.accuracy)\n\n            if iter_cnt % 10 == 0:\n                # append to training outputs\n                train_loss.append(tr_loss)\n                train_acc.append(tr_metric)\n\n                # appending the test outputs\n                if evaluate_dl is not None:\n                    self.eval()\n                    tst_loss, tst_metric = self.evaluate(loss_fn, ConvolutionalNNet.accuracy, evaluate_dl)\n                    \n                    if (not time_to_95_set) and (tst_metric >= 0.95):\n                        self.time_to_95 = iter_cnt\n                        time_to_95_set = True\n                    \n                    test_loss.append(tst_loss)\n                    test_acc.append(tst_metric)\n                    \n                    if self.verbose:\n                        print(f\"{iter_cnt} Training loss: {tr_loss} , accuracy: {tr_metric} Testing loss: {tst_loss} , accuracy: {tst_metric}\")\n\n            # increasing counter\n            iter_cnt += 1\n    return train_loss,train_acc,test_loss,test_acc\n\n# Add the functions to class\nConvolutionalNNet.evaluate = func1\nConvolutionalNNet.fit = func2","1ca521c5":"def func(self, test_dl):\n    preds_final = None\n    for xb in test_dl:\n        y_pred = self(xb)\n        _, preds = torch.max(y_pred, dim=1)\n        if preds_final is None:\n            preds_final = preds\n        else:\n            preds_final = torch.cat((preds_final, preds))\n#         print(preds)\n#         break\n    return preds_final\n\n# Predict function\nConvolutionalNNet.predict = func","c8ad57b1":"# Creating a network instance\nnet = ConvolutionalNNet()\nnet.set_optimizer()\n# If the device is CUDA, then move the model into the CUDA\nif get_device()==torch.device('cuda'):\n    net.cuda()","0571de35":"class MNISTHandWrittenDigitsDataset(Dataset):\n    \"\"\"Class as a custom dataset\"\"\"\n    def __init__(self, ds, x_lb=1, y_lb=0):\n        \"\"\"Constructor for the class\n        :params ds: data to be loaded in the dataset\n        :params x_lb: starting index for x or features\n        :params y_lb: starting index for y or response. -1 for testing data\n        \"\"\"\n        self.is_train = False if y_lb == -1 else True # Check if it training or test data\n        \n        self.X=ds[:,x_lb:]\n        d_len, arr_len = self.X.shape\n        self.X=self.X.reshape(d_len,1,arr_len)\n        \n        # transform to required datatypes\n        self.X=self.X.to(dtype=torch.float32)\n        \n        # Load and transform for testing data\n        if self.is_train:\n            self.y=ds[:,y_lb]\n            self.y=self.y.to(dtype=torch.int64) # labels should be integers\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        \"\"\"return y and x if training data, else return x only\n        :params idx: index to return\n        \"\"\"\n        if self.is_train:\n            return self.y[idx], self.X[idx]\n        else:\n            return self.X[idx]","1676020c":"class DeviceDataLoader:\n    \"\"\"Class as a data loader\"\"\"\n    def __init__(self, dl, device):\n        self.dl=dl\n        self.device=device\n    \n    def __iter__(self):\n        for b in self.dl:\n            yield self.to_device(b, self.device)\n    \n    def __len__(self):\n        return len(self.dl)\n    \n    def to_device(self, data, device):\n        if isinstance(data, (tuple,list)):\n            return [self.to_device(x, device) for x in data]\n        return data.to(device, non_blocking=True)","b9f3ac68":"def split_indices(n, val_pct=0.1, seed=99):\n    # Determin the size of validation set\n    n_val = int(val_pct*n)\n    # Set random seed\n    np.random.seed(seed)\n    # Create random permutation of 0 to n-1 \n    idxs = np.random.permutation(n)\n    # Pick first n_val indices for validation set\n    return idxs[n_val:], idxs[:n_val]","794d3adc":"train_idx, valid_idx = split_indices(len(train_dataset), 0.2)\nprint(len(train_idx), len(valid_idx))\n\n# Training sampler and data loader\ntrain_sampler = SubsetRandomSampler(train_idx)\ntrain_dl = DataLoader(\n    MNISTHandWrittenDigitsDataset(train_dataset),\n    batch_size = 100, # Change the batch size here\n    sampler=train_sampler\n)\ntrain_cuda_dl = DeviceDataLoader(\n    train_dl,\n    get_device()\n)\n\n# Validation sampler and data loader\n# We want to load the entire validation data at once, not in chunk\nvalid_sampler = SubsetRandomSampler(valid_idx)\nvalid_dl = DataLoader(\n    MNISTHandWrittenDigitsDataset(train_dataset),\n    batch_size = len(valid_idx), # load all the data at once\n    sampler=valid_sampler\n)\nvalid_cuda_dl = DeviceDataLoader(\n    valid_dl,\n    get_device()\n)\n\n# Testing data\ntest_dl = DataLoader(\n    MNISTHandWrittenDigitsDataset(test_dataset, x_lb=0, y_lb=-1),\n    batch_size = 500  # load all the data at once\n)\ntest_cuda_dl = DeviceDataLoader(\n    test_dl,\n    get_device()\n)","74d39dad":"a,b,c,d=net.fit(train_cuda_dl, 2, valid_cuda_dl)","e879d326":"df = pd.DataFrame({'train_loss':a,'train_acc':b,'test_loss':c,'test_acc':d})\ndf[['train_loss','test_loss']].plot.line(figsize=(12,8), xlabel='Calculated loss', ylabel='iterations', title='Training ~ Validation Loss')\ndf[['train_acc','test_acc']].plot.line(figsize=(12,8), xlabel='Accuracy', ylabel='iterations', title='Training ~ Validation Accuracy')","a1c8cd19":"predictions=net.predict(test_cuda_dl)\npredictions","92133e80":"df = pd.DataFrame({\n    'batch_size': [100, 200, 500, 750, 1000,100, 200, 500, 750, 1000],\n    'type':['cpu','cpu','cpu','cpu','cpu','gpu','gpu','gpu','gpu','gpu'],\n    'exec_time': [133.0568618774414, 131.0257797241211, 131.81717801094055, 130.84673261642456, 131.0255696773529,17.37652015686035, 17.138057947158813, 17.143101453781128, 17.339112520217896, 17.434961795806885],\n    'accuracy': [0.9544047619047619, 0.9563095238095238, 0.9519047619047619, 0.9458333333333333, 0.958452380952381,0.9546428571428571, 0.9530952380952381, 0.9521428571428572, 0.9539285714285715, 0.9534523809523809],\n    'iter_count': [540, 490, 570, 450, 470, 580, 560, 500, 640, 510]\n    })\ndf.set_index(['type','batch_size']).T","301c98bc":"### Reading dataset","6506207e":"#### Evaluate and fit function","dcffc805":"#### Setting optimizer\nA optimizer is the function that corrects the weight of the neural network. For the neural network, we can set the optimizer by using the function below. By default, it is stochastic gradient descent.","c6b537c0":"### Loading Data to GPU\nBefore computing, the data needs to be moved to GPU. However, it is not good practice to load entire data into GPU. We would create a custom data loader that would move only the required batch to GPU and perform the calculation. The size of the batch is determinable.","03445240":"### Metrics montoring\nWe are going to monitor various following metrics for different hyper parameters.\n\n#### Hyperparameters\n* Batch Size\n* GPU vs CPU\n\n#### Metrics\n* Iteration Count to Reach 95% accuracy\n* Accuracy\n* Executing Time","1f788c69":"#### Training and Validation data split\nThe function would randomly generate indices from the dataset. The training and validation split is 9:1, which can be changed by passing in as val_pct.","223b3ac1":"### Device to use\nThe function get_device would provide CPU if GPUs are not available. It would make the code run generically on all GPU and CPU environment.","b9e8b403":"#### Splitting training dataset\nThe training data set would be splitted into training and validation. The classification model would not see the validation set. Thus, It would be used for evaluation purpose.","634c38e0":"### Transformation\nWe can use the data directly, however, we are going to use [Data loader module](https:\/\/pytorch.org\/tutorials\/beginner\/basics\/data_tutorial.html#iterate-through-the-dataloader) provided by torch, which has useful tools such as randomly sampling as well as batching data. To use the data loader, the data needs to be in specific format, so we are going to wrap a class, [Dataset](https:\/\/pytorch.org\/tutorials\/beginner\/basics\/data_tutorial.html#creating-a-custom-dataset-for-your-files), on top of the data.\n\nThe MNISTHandWrittenDigitsDataset is the wrapper class which works for both training as well as testing data. To flag it as testing data `y_lb` should be -1.","6fe8a43d":"# PyTorch on GPUs: Handwritten Digit Recognition\nIn this blog, I would like to demonstrate on how to train a neural network on GPUs. If you have laptop or PC with NVidia graphics card, it is possible to train the neural network presented in the course locally. On the contrary,  [kaggle](https:\/\/www.kaggle.com\/) provides 30 hour worth of GPU Accelerator which could be used for training purpose. GPUs are processing unit made for graphics, honed for huge matrix multiplication which are required for computer graphics in games. Fortunately, neural networks are huge and repeatitive matrix calculation which could be efficiently carried out in GPUs.\n\n## Guidance\n1. Appropriate comments are provided along with the code.\n2. Read the description before the code to understand the working of the code.\n\nReferences: [PyTorch for Deep Learning - Full Course \/ Tutorial](https:\/\/www.youtube.com\/watch?v=GIsg-ZUy0MY&t=21573s)\n\n## Dataset\nFor the demonstration purpose, we will be using popular MNIST [handwritten digit dataset](https:\/\/www.kaggle.com\/c\/digit-recognizer\/data).\n\n## Methodology\nLet's start by importing required libraries. The libraries used are:\n* numpy\n* pandas\n* pytorch","9ab76d91":"#### Predict function","7d074349":"#### Calculating batch loss\nWe need to calculate of the model for the forward pass. The loss is measured using CrossEntropy function. If the optimizer is passed, the backward pass is performed or else only metric is calculated. Finally, total loss and metric value are returned. Here, accuracy is being used as metric, however, F1-score will be much preferred.","3e478a27":"### Neural Network\nWe are going to use Convolutional Neural Network here, check the references for further information. It only has constructor, forward pass, and accuracy method. Accuracy method is defined as static function.","a3c1a4f8":"Initialization of the Neural network model. Re-run the block below to reinitialize the weights and bais of the network."}}