{"cell_type":{"b946099e":"code","94b670ee":"code","7f30201b":"code","ec0d188a":"code","d969357a":"code","098f5e03":"code","a4a3bf67":"code","b10048f6":"code","b0de1d2c":"code","80689333":"code","b0b9424a":"code","4d4c8520":"code","4a1c73e7":"code","7ddc8d87":"code","5ea4e14f":"code","24dfe70c":"code","e86adc1e":"code","e89db805":"code","ca4bd5e2":"code","33628493":"code","d086609c":"code","405f7c17":"code","104dba06":"code","a07f945f":"code","8298c3f4":"code","7347e639":"code","216e3525":"code","f3200973":"code","6d945b3f":"code","817527b9":"code","9d1bfaaa":"code","ae5c0695":"code","86af20cc":"code","f75f84d5":"code","2a69e121":"code","1996e6fa":"code","11cad2da":"code","3d60bdab":"code","27d1310c":"code","be56ba19":"code","bbdfacbe":"code","d65d109f":"code","51e25b47":"code","166b6eb0":"code","f5bcbb40":"code","a3bd9363":"code","c268c955":"code","38fb264a":"code","fe8dae58":"code","3b683332":"code","c972e8a2":"code","494f8c6a":"code","7a3b41c2":"code","053e72e9":"code","b1902663":"code","fedfa8d7":"code","0fe7b7e8":"code","84b3f694":"code","a0e6150b":"code","695e6028":"code","e22f0bcf":"code","4ea73103":"code","c06065e0":"code","8bab8d33":"code","7642706b":"code","d374a797":"code","a8fe3614":"code","21744e68":"code","53349209":"code","1772aef5":"code","529b0752":"code","23b48326":"code","815304c6":"code","4009d4bf":"code","4bc4d0a7":"code","14040555":"code","52f5500e":"code","c76d89ff":"code","1833d559":"code","ccfe7dfa":"code","b61d12a5":"code","542fed64":"code","05b8c6ef":"code","40584eeb":"code","f01a5046":"code","5ee92cb8":"code","77f7b0e4":"code","3360e4db":"code","12cf144a":"code","241e7c5f":"code","21f5d6fe":"code","b8980338":"code","758106b4":"code","7f1f06fe":"code","69f65078":"code","254dfc66":"code","5af0fd19":"code","45c0ad58":"code","2c6c40a7":"code","37c29b96":"code","ac65e585":"code","bf87799b":"code","b92f512c":"code","eec33b26":"code","557f4884":"code","ccddc236":"code","b1c7e9ef":"code","d4f6bf9d":"code","817e21c6":"code","7a5dfdb7":"code","b8150f6f":"code","7b093dfc":"code","1cd57647":"code","a7e39c3d":"code","9d1b67a0":"code","00c3b273":"code","28c5ade1":"code","029f8ec8":"code","73932355":"code","34343704":"code","84d207ef":"code","6102a6f6":"code","a7b50be0":"code","3dd261b6":"code","635ec98a":"code","c78e1db9":"code","b7f974c4":"code","812a4325":"code","b26da3bc":"code","98e5a3b8":"code","9c58326b":"code","a847a81e":"code","3a8f764d":"code","13dd69a8":"code","545934ef":"code","bc053298":"code","0af8b45c":"code","c83b1da1":"code","936c93f1":"code","8e9a2914":"code","c2054263":"code","cb748b30":"code","f448e28c":"code","f619d6fb":"code","5b611ba8":"code","ca72d48d":"code","a332a192":"code","06392af8":"code","1e550299":"code","e947305a":"code","c8f4fb78":"code","587beb24":"code","005230c7":"code","e7a4bc4c":"code","90b5d163":"code","6b048347":"code","ada6d2f8":"code","9fc22e7c":"code","f9a6320c":"code","3f7b029a":"code","f0db97fc":"code","1baf7961":"code","70152049":"code","81384997":"code","d3d7526f":"code","25e0c646":"code","24fc2312":"code","f6738eab":"code","5c7f1170":"code","9fb21cb3":"code","d411c558":"code","8cb6f3d9":"code","82805e93":"code","34e45942":"code","c02d2d13":"code","d9668cff":"code","79ca3d6c":"code","2c9f696e":"code","8dc4223a":"code","80abd973":"code","b097868b":"code","8d80ee2c":"code","d0676439":"code","a231c65d":"code","5d478c55":"code","4b0c15a5":"code","4fd93643":"code","1bce1641":"code","407f87dc":"code","7eef7788":"code","b5145afb":"code","324ccd22":"code","319a14fe":"code","c06864e3":"code","b783f2d4":"code","844ba0fb":"code","a2b0ec09":"code","fd43a7c5":"code","dacd137b":"code","3b465517":"code","ca23ddec":"code","493d916c":"code","a97d2a1e":"code","8cf40b1f":"code","0e693266":"code","81fe82e1":"code","150e0a2b":"code","61274c4b":"code","d700ba97":"code","7473aa87":"code","3eacf4f0":"code","0253fb1a":"code","7db89c64":"code","d3735f51":"code","6a8c6186":"code","95dddfd4":"code","6619a5b7":"code","21c3db06":"code","caf85d96":"code","bc88d1c5":"code","ad826e15":"code","b8d380e3":"code","384d4282":"code","f44622dd":"code","a9098193":"code","42022683":"code","32744ee4":"code","22af39ff":"code","9484b40f":"code","f2775e2f":"code","66b24c23":"code","f9cea4e9":"code","3f78237b":"code","c9492f7a":"code","2555fffa":"code","25f04640":"code","dcb3c470":"code","d26c3167":"code","8b398ca4":"code","ae5a453a":"code","08f11abc":"code","8ed3dd2a":"code","8010fa4b":"code","c820113f":"code","64a92a70":"code","6795a910":"code","dd366e8e":"code","0cfa88ce":"code","e0baad4e":"code","04fce065":"code","d8394259":"code","fe17f677":"code","773680eb":"code","5454af9a":"code","df927d4a":"code","9883f602":"code","d4cf7342":"code","722493ef":"code","e40b2b8a":"code","2d447e95":"code","165591ce":"code","fdc58a65":"code","7f22aa24":"code","c3a9367e":"code","767782b0":"code","08b1f611":"code","db8ce73f":"markdown","21870cb5":"markdown","e6512671":"markdown","baaa413b":"markdown","907a34d5":"markdown","3708c2fc":"markdown","831193be":"markdown","da0944d9":"markdown","26ee5627":"markdown","b2d0f818":"markdown","bbcaf420":"markdown","7b8e1e68":"markdown","07a4375b":"markdown","813aa706":"markdown","230b7109":"markdown","2bf397c9":"markdown","70d82649":"markdown","eb285b19":"markdown","79802f3f":"markdown","2a3215de":"markdown","f9a31ce0":"markdown","ac690c3f":"markdown","dba06fd0":"markdown","042083d1":"markdown","9827d60c":"markdown","c9fd5a0e":"markdown","aaee6f90":"markdown","0eba6ddb":"markdown","65a403cf":"markdown","d32daf11":"markdown","fb429187":"markdown","d915c434":"markdown","6b9acf25":"markdown","065c9557":"markdown","2f9f9bec":"markdown","f945b808":"markdown","852f6c2d":"markdown","3bcbca83":"markdown","c7c55211":"markdown","67d2e2fb":"markdown","fb45005f":"markdown","c354758c":"markdown","194c0c0c":"markdown","81a803aa":"markdown","6d2c2112":"markdown","4b183419":"markdown","a0f7c40c":"markdown","d8b13814":"markdown","c18265f8":"markdown","61398ed8":"markdown","9a7e54af":"markdown","f8937467":"markdown","c8d953cc":"markdown","629178eb":"markdown","34da44aa":"markdown","2f037511":"markdown"},"source":{"b946099e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom collections import Counter\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nfrom sklearn.feature_extraction import FeatureHasher\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.linear_model import Lasso, LinearRegression, Ridge, LogisticRegression\nfrom sklearn.metrics import (\n    make_scorer,\n    mean_absolute_error,\n    mean_squared_error,\n    r2_score,\n    confusion_matrix,\n    classification_report,\n    roc_auc_score,\n)\nfrom sklearn.svm import SVR\nfrom sklearn.utils import shuffle\nfrom sklearn import datasets\nfrom tqdm import tqdm as progress_bar","94b670ee":"#Make sure to be able to see all columns and rows in the data set.\npd.set_option(\"display.max_columns\", 50)\npd.set_option(\"display.max_rows\", None)","7f30201b":"data_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndata_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","ec0d188a":"data_raw = pd.concat([data_train, data_test])\ndata_raw_duplicate = pd.concat([data_train, data_test])","d969357a":"data_raw.head(10)","098f5e03":"data_raw.shape","a4a3bf67":"data_raw.info()","b10048f6":"continuous = ['Age','Fare']\n\ndiscrete = ['PassengerId','Survived','SibSp','Parch']\n\nordinal = ['Pclass']\n\nnominal = ['Name','Sex','Ticket','Cabin','Embarked']","b0de1d2c":"data_raw[continuous].head()","80689333":"data_raw[continuous].isna().sum().sort_values(ascending=False)","b0b9424a":"data_raw[data_raw['Age'].isna()]","4d4c8520":"data_raw['AgeHelperColumn'] = data_raw.apply(lambda row: 1 if row['Age'] > -1 else 0, axis=1)","4a1c73e7":"data_raw[data_raw['Fare'].isna()]","7ddc8d87":"plt.figure(figsize=(15,8))\nsns.histplot(data=data_raw, x=\"Fare\", hue=\"Pclass\")","5ea4e14f":"median = data_raw.groupby([\"Pclass\"])[\"Fare\"].median()\nmean = data_raw.groupby([\"Pclass\"])[\"Fare\"].mean()\nfare = pd.concat([median, mean], axis=1, keys = [\"Median Fare\", \"Mean Fare\"])\nfare","24dfe70c":"data_raw['Fare'] = data_raw['Fare'].fillna(8.0500)","e86adc1e":"data_raw[discrete].head()","e89db805":"data_raw[discrete].isna().sum().sort_values(ascending=False)","ca4bd5e2":"data_raw[ordinal].head()","33628493":"data_raw[ordinal].isna().sum().sort_values(ascending=False)","d086609c":"data_raw[nominal].head()","405f7c17":"data_raw[nominal].isna().sum().sort_values(ascending=False)","104dba06":"data_raw['Cabin'] = data_raw['Cabin'].fillna('U')","a07f945f":"data_raw[data_raw['Embarked'].isna()]","8298c3f4":"data_raw.loc[(data_raw['Cabin'] == 'B28')]","7347e639":"data_raw.groupby(['Pclass','Embarked'])['Embarked'].count()","216e3525":"data_raw['Embarked'] = data_raw['Embarked'].fillna('NoEmbarked')","f3200973":"data_raw_Sex = pd.get_dummies(data_raw[['Sex']],prefix='Sex')\ndata_raw = pd.concat([data_raw,data_raw_Sex],axis=1)","6d945b3f":"nominal.append('Sex_female')\nnominal.append('Sex_male')","817527b9":"data_raw_Embarked = pd.get_dummies(data_raw[['Embarked']],prefix='Emb')\ndata_raw = pd.concat([data_raw,data_raw_Embarked],axis=1)","9d1bfaaa":"nominal.append('Emb_C')\nnominal.append('Emb_NoEmbarked')\nnominal.append('Emb_Q')\nnominal.append('Emb_S')","ae5c0695":"data_raw['Title']=data_raw.apply(lambda row: \"Mr.\" if \"Mr.\" in row.Name else \n                             (\"Mrs.\" if \"Mrs.\" in row.Name else\n                              (\"Miss.\" if \"Miss.\" in row.Name else\n                               (\"Miss.\" if \"Ms.\" in row.Name else\n                                (\"Master.\" if \"Master.\" in row.Name else\n                                 (\"Rev.\" if \"Rev.\" in row.Name else\n                                  (\"Dr.\" if \"Dr.\" in row.Name else \"NoTitle\")))))), axis=1 )","86af20cc":"data_raw.head()","f75f84d5":"data_raw.groupby(['Title'])['PassengerId'].count()","2a69e121":"data_raw.loc[(data_raw['Title'] == 'Rev.')]","1996e6fa":"g = sns.FacetGrid(data_raw, row=\"Pclass\", height=4, aspect=2)\ng.map(sns.barplot, \"Title\", \"Survived\", order=[\"Dr.\",\"Master.\",\"Miss.\",\"Mr.\",\"Mrs.\",\"NoTitle\",\"Rev.\"])","11cad2da":"data_raw_Title = pd.get_dummies(data_raw[['Title']],prefix='T')\ndata_raw = pd.concat([data_raw,data_raw_Title],axis=1)","3d60bdab":"nominal.append('T_Dr.')\nnominal.append('T_Master.')\nnominal.append('T_Miss.')\nnominal.append('T_Mr.')\nnominal.append('T_Mrs.')\nnominal.append('T_NoTitle')\nnominal.append('T_Rev.')","27d1310c":"data_raw['Deck']= data_raw.apply(lambda row: \"A\" if \"A\" in row.Cabin else \n                                 (\"B\" if \"B\" in row.Cabin else \n                                  (\"C\" if \"C\" in row.Cabin else \n                                   (\"D\" if \"D\" in row.Cabin else \n                                    (\"E\" if \"E\" in row.Cabin else \n                                     (\"F\" if \"F\" in row.Cabin else \n                                      (\"G\" if \"G\" in row.Cabin else \n                                       (\"T\" if \"T\" in row.Cabin else \"U\"))))))), axis=1)","be56ba19":"data_raw.groupby(['Deck'])['PassengerId'].count()","bbdfacbe":"data_raw.loc[(data_raw['Deck'] == 'A')]","d65d109f":"data_raw.groupby(['Deck','Pclass'])['PassengerId'].count()","51e25b47":"h = sns.FacetGrid(data_raw, row=\"Pclass\", height=4, aspect=2)\nh.map(sns.barplot, \"Deck\", \"Survived\", order=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"T\",\"U\"])","166b6eb0":"data_raw_Deck = pd.get_dummies(data_raw[['Deck']],prefix='D')\ndata_raw = pd.concat([data_raw,data_raw_Deck],axis=1)","f5bcbb40":"nominal.append('Deck')\nnominal.append('D_A')\nnominal.append('D_B')\nnominal.append('D_C')\nnominal.append('D_D')\nnominal.append('D_E')\nnominal.append('D_F')\nnominal.append('D_G')\nnominal.append('D_T')\nnominal.append('D_U')","a3bd9363":"data_raw['Fare'].min()","c268c955":"data_raw['FarePlus'] = data_raw.apply(lambda row: row.Fare + 0.001, axis = 1)","38fb264a":"# I use a function to normalize the continuous columns with box-cox \nToTransform = ['FarePlus']\n\nnew_transformed_columns = []\n\nfor column in ToTransform:\n    X = data_raw[[column]]\n    pt = PowerTransformer(method=\"box-cox\", standardize=False) \n    #fit the column\n    box_cox = pt.fit(X)\n    #Check if the lambda is between -3 and 3\n    lambda_ = box_cox.lambdas_\n    if -3 <= lambda_ <= 3:\n        new_column = f\"{column} (box-cox)\"\n        data_raw[new_column] = (\n            np.log(X) if lambda_ <= 0.001 else (((X ** lambda_) - 1) \/ lambda_)\n        )\n        new_transformed_columns.append(new_column)\n        continuous.append(new_column)\n    else:\n        print(X, lambda_, \"lambda not in range\")","fe8dae58":"data_raw[new_transformed_columns].head()","3b683332":"data_raw.head()","c972e8a2":"del data_raw['FarePlus']","494f8c6a":"predictAge = ['Survived', 'Pclass', 'SibSp', 'Parch', 'Sex_female', 'Emb_C', 'Emb_NoEmbarked', 'Emb_Q', 'Emb_S', \n              'T_Dr.', 'T_Master.', 'T_Miss.', 'T_Mr.', 'T_Mrs.', 'T_NoTitle', 'T_Rev.', 'D_A', 'D_B', 'D_C', \n              'D_D', 'D_E', 'D_F', 'D_U', 'FarePLus(box-cox)']","7a3b41c2":"data_raw = data_raw.reset_index()","053e72e9":"dataRaw_train = data_raw.loc[(data_raw['Age'].notna())]\ndataRaw_toPredict = data_raw.loc[(data_raw['Age'].isna())]","b1902663":"dataRaw_train.head()","fedfa8d7":"dataRaw_toPredict.head()","0fe7b7e8":"del dataRaw_train['PassengerId']\ndel dataRaw_train['Name']\ndel dataRaw_train['Sex']\ndel dataRaw_train['Ticket']\ndel dataRaw_train['Fare']\ndel dataRaw_train['Cabin']\ndel dataRaw_train['Embarked']\ndel dataRaw_train['AgeHelperColumn']\ndel dataRaw_train['Sex_male']\ndel dataRaw_train['Title']\ndel dataRaw_train['Deck']\ndel dataRaw_train['Survived']","84b3f694":"del dataRaw_toPredict['PassengerId']\ndel dataRaw_toPredict['Name']\ndel dataRaw_toPredict['Sex']\ndel dataRaw_toPredict['Ticket']\ndel dataRaw_toPredict['Fare']\ndel dataRaw_toPredict['Cabin']\ndel dataRaw_toPredict['Embarked']\ndel dataRaw_toPredict['AgeHelperColumn']\ndel dataRaw_toPredict['Sex_male']\ndel dataRaw_toPredict['Title']\ndel dataRaw_toPredict['Deck']\ndel dataRaw_toPredict['Survived']","a0e6150b":"del dataRaw_toPredict['Age']","695e6028":"x1 = dataRaw_train.drop(columns=['Age']).values\ny1 = dataRaw_train['Age'].values","e22f0bcf":"data_raw['Age'].mean()","4ea73103":"data_raw['Age'].median()","c06065e0":"def validation(x, y, *, model, k=10, log=False, desc=None):\n    r2, rmse = [], []\n    for train, test in progress_bar(KFold(n_splits=k).split(x), desc=desc, total=k):\n        model.fit(x[train], y[train])\n        y_predict = model.predict(x[test])\n        y_true, y_predict = y[test], y_predict\n        r2.append(r2_score(y_true, y_predict))\n        rmse.append(mean_squared_error(y_true, y_predict))\n    return {\n        \"r2\": np.round(np.mean(r2), 3),\n        \"rmse\": np.round(np.sqrt(np.mean(rmse))),\n    }","8bab8d33":"results = {}","7642706b":"lr = LinearRegression()","d374a797":"results[('lr','o')] = validation(x1,y1, model=lr)","a8fe3614":"results[('lr','o')]","21744e68":"grid_search = GridSearchCV(\n    estimator=Lasso(tol=0.1, selection=\"random\", random_state=None),\n    param_grid={\"alpha\": [2 ** x for x in range(-8, 4)] + list(range(12, 65, 4))},\n    cv=KFold(n_splits=4),\n    n_jobs=-1,\n)","53349209":"grid_search.fit(x1, y1)\nalpha = grid_search.best_params_[\"alpha\"]\nalpha","1772aef5":"results[(\"lasso\", \"o\")] = validation(x1, y1, model=Lasso(alpha=alpha, tol=0.1))\nresults[(\"lasso\", \"o\")]","529b0752":"grid_search = GridSearchCV(\n    estimator=Ridge(),\n    param_grid={\"alpha\": [2 ** x for x in range(-8, 4)] + list(range(12, 65, 4))},\n    cv=KFold(n_splits=4),\n    n_jobs=-1,\n)","23b48326":"grid_search.fit(x1, y1)\nalpha = grid_search.best_params_[\"alpha\"]\nalpha","815304c6":"results[(\"ridge\", \"o\")] = validation(x1, y1, model=Ridge(alpha=alpha))\nresults[(\"ridge\", \"o\")]","4009d4bf":"prediction = lr.predict(dataRaw_toPredict)","4bc4d0a7":"dataRaw_toPredict['Age'] = prediction.tolist()","14040555":"dataRaw_toPredict.head()","52f5500e":"dataRaw = pd.concat([dataRaw_train, dataRaw_toPredict], sort=False).sort_index()","c76d89ff":"AgeImpute = dataRaw['Age'].tolist()","1833d559":"data_raw['AgeImpute'] = AgeImpute","ccfe7dfa":"data_raw['Age'] = data_raw['Age'].fillna(28.0)","b61d12a5":"data_raw['Age'].min()","542fed64":"data_raw['AgeImpute'].min()","05b8c6ef":"data_raw['AgeImpute'] = data_raw.apply(lambda row: row.AgeImpute if row.AgeImpute > 0 else 0.001, axis = 1)","40584eeb":"data_raw['AgeImpute'].min()","f01a5046":"# I use a function to normalize the continuous columns with box-cox (same code as with Fare)\nToTransform = ['Age', 'AgeImpute']\n\nnew_transformed_columns = []\n\nfor column in ToTransform:\n    X = data_raw[[column]]\n    pt = PowerTransformer(method=\"box-cox\", standardize=False) \n    #fit the column\n    box_cox = pt.fit(X)\n    #Check if the lambda is between -3 and 3\n    lambda_ = box_cox.lambdas_\n    if -3 <= lambda_ <= 3:\n        new_column = f\"{column} (box-cox)\"\n        data_raw[new_column] = (\n            np.log(X) if lambda_ <= 0.001 else (((X ** lambda_) - 1) \/ lambda_)\n        )\n        new_transformed_columns.append(new_column)\n        continuous.append(new_column)\n    else:\n        print(X, lambda_, \"lambda not in range\")","5ee92cb8":"data_raw[new_transformed_columns].head()","77f7b0e4":"data_raw.head()","3360e4db":"data_raw['FamilyCount'] = data_raw.apply(lambda row: row.Parch + row.SibSp + 1, axis=1)","12cf144a":"sns.catplot(x=data_raw['FamilyCount'], kind=\"count\",\n            palette=\"rocket\", edgecolor=\".6\", aspect=2,\n            data=data_raw)","241e7c5f":"sns.countplot(x=data_raw['FamilyCount'],\n            hue=\"Survived\",\n            data=data_raw)","21f5d6fe":"discrete.append('FamilyCount')","b8980338":"data_raw['LastName'] = data_raw.apply(lambda row: row.Name.partition(\",\")[0], axis=1)","758106b4":"data_raw.head()","7f1f06fe":"data_raw['MotherChild'] = data_raw.apply(lambda row: row.LastName if row.Sex_female == 1 and row.Parch != 0 and row.Age > 18 else \n                                         (row.LastName if row.Age < 9 and row.Parch != 0 else ''), axis=1)","69f65078":"ListMotherChild = data_raw['MotherChild'].tolist()","254dfc66":"ListMotherChild","5af0fd19":"counterMotherChild = Counter(ListMotherChild)","45c0ad58":"counterMotherChild","2c6c40a7":"data = data_raw","37c29b96":"data = data.reset_index()","ac65e585":"data['CountMomCh'] = 0","bf87799b":"data['CountMomCh'] = data.apply(lambda row: counterMotherChild[row.MotherChild], axis=1)","b92f512c":"data.groupby(['CountMomCh'])['Sex'].count()","eec33b26":"data['CountMomChSurvived'] = 0","557f4884":"data['countSurvived'] = 0\ndata['countNanSurvived'] = 0\n\ndef mommie(ListMotherChild):\n    for i in range(len(ListMotherChild)-1):\n        print(i)\n        count = 0\n        countNan = 0\n        listIndex = []\n        if data['CountMomCh'].loc[i] != 1124:\n            \n            if data.loc[i, 'Survived'] > -1:\n                count += data.loc[i, 'Survived']\n                listIndex.append(i)\n                \n                for j in range(len(ListMotherChild)-1):\n                    \n                    if (i != j)  and (ListMotherChild[i] == ListMotherChild[j]):\n                        if data.loc[j, 'Survived'] > -1:\n                            count += data.loc[j, 'Survived']\n                            listIndex.append(j)\n                        else:\n                            countNan += 1\n                            listIndex.append(j)\n                        \n            else:\n                countNan += 1\n                listIndex.append(i)\n                \n                for j in range(len(ListMotherChild)-1):\n\n                    if (i != j)  and (ListMotherChild[i] == ListMotherChild[j]):\n                        if data.loc[j, 'Survived'] > -1:\n                            count += data.loc[j, 'Survived']\n                            listIndex.append(j)\n                        else:\n                            countNan += 1\n                            listIndex.append(j)\n        \n            for k in listIndex:\n                print(i, k, count, countNan)\n                data.loc[k, 'countSurvived'] = count\n                data.loc[k, 'countNanSurvived'] = countNan\n            ","ccddc236":"mommie(ListMotherChild)","b1c7e9ef":"data.head(100)","d4f6bf9d":"data['dummySurvived'] = data.apply(lambda row: 1 if row.MotherChild != '' and ((row.countSurvived + row.countNanSurvived)\/row.CountMomCh) > 0.5 else\n                                   (-1 if row.MotherChild != '' and ((row.countSurvived + row.countNanSurvived)\/row.CountMomCh) <= 0.5 else 0), axis=1)","817e21c6":"del data['CountMomCh']\ndel data['CountMomChSurvived']\ndel data['countSurvived']\ndel data['countNanSurvived']","7a5dfdb7":"dataCopy = data.copy()\n\nfg = FeatureHasher(n_features=3, input_type='string')\nhashed_features = fg.fit_transform(data['MotherChild'])\nhashed_features = hashed_features.toarray()\n\ndata = pd.concat([data, pd.DataFrame(hashed_features)], axis=1)","b8150f6f":"data.rename(columns = {0:'MC_0', 1:'MC_1',2:'MC_2'}, inplace = True)","7b093dfc":"del data['level_0']\ndel data['index']","1cd57647":"data.head()","a7e39c3d":"TicketList = data['Ticket'].tolist()","9d1b67a0":"counterTicketList = Counter(TicketList)\ncounterTicketList","00c3b273":"data['TicketAmount'] = data.apply(lambda row: counterTicketList.get(row.Ticket, None), axis = 1)","28c5ade1":"data[['Ticket','TicketAmount']].head()","029f8ec8":"CabinList = data['Cabin'].tolist()","73932355":"counterCabinList = Counter(CabinList)\ncounterCabinList","34343704":"data['CabinAmount'] = data.apply(lambda row: counterCabinList.get(row.Cabin, None), axis = 1)","84d207ef":"data['CabinAmount'] = data.apply(lambda row: 1 if row.CabinAmount == 1014 else row.CabinAmount, axis = 1)","6102a6f6":"data[['Cabin','CabinAmount']].head()","a7b50be0":"data[['Pclass','FamilyCount', 'Cabin','Ticket','CabinAmount','TicketAmount']].head(500)","3dd261b6":"data['groupSolo'] = data.apply(lambda row: \"solo\" if row.TicketAmount == 1 else \n                              (\"duo\" if row.TicketAmount == 2 else\n                              (\"trio\" if row.TicketAmount == 3 else\n                              (\"quatro\" if row.TicketAmount == 4  else \"large\"))), axis = 1)","635ec98a":"data['groupId'] = data.apply(lambda row: row.Ticket if row.TicketAmount > 1 else '', axis = 1)","c78e1db9":"dataCopy = data.copy()\n\nfg = FeatureHasher(n_features=4, input_type='string')\nhashed_features = fg.fit_transform(data['groupId'])\nhashed_features = hashed_features.toarray()\n\ndata = pd.concat([data, pd.DataFrame(hashed_features)], axis=1)","b7f974c4":"data.rename(columns = {0:'GrId_0', 1:'GrId_1',2:'GrId_2', 3: 'GrId_3'}, inplace = True)","812a4325":"data.head()","b26da3bc":"dataCopy = data.copy()","98e5a3b8":"data.columns.tolist()","9c58326b":"data_model = data[['PassengerId',\n 'Survived',\n 'Pclass',\n 'Age',\n 'SibSp',\n 'Parch',\n 'Fare',\n 'Sex_female',\n 'Sex_male',\n 'Emb_C',\n 'Emb_NoEmbarked',\n 'Emb_Q',\n 'Emb_S',\n 'T_Dr.',\n 'T_Master.',\n 'T_Miss.',\n 'T_Mr.',\n 'T_Mrs.',\n 'T_NoTitle',\n 'T_Rev.',\n 'D_A',\n 'D_B',\n 'D_C',\n 'D_D',\n 'D_E',\n 'D_F',\n 'D_G',\n 'D_T',\n 'D_U',\n 'FarePlus (box-cox)',\n 'AgeImpute',\n 'Age (box-cox)',\n 'AgeImpute (box-cox)',\n 'FamilyCount',\n 'dummySurvived',\n 'MC_0',\n 'MC_1',\n 'MC_2',\n 'TicketAmount',\n 'CabinAmount',\n 'GrId_0',\n 'GrId_1',\n 'GrId_2',\n 'GrId_3']]","a847a81e":"data_model.head()","3a8f764d":"continuous","13dd69a8":"continuous.append('AgeImpute')","545934ef":"discrete","bc053298":"discrete.remove('Survived')","0af8b45c":"discrete.append('FamilyCount')\ndiscrete.append('dummySurvived')\ndiscrete.append('MC_0')\ndiscrete.append('MC_1')\ndiscrete.append('MC_2')\ndiscrete.append('TicketAmount')\ndiscrete.append('CabinAmount')\ndiscrete.append('GrId_0')\ndiscrete.append('GrId_1')\ndiscrete.append('GrId_2')\ndiscrete.append('GrId_3')","c83b1da1":"ordinal","936c93f1":"nominal","8e9a2914":"nominal.remove('Ticket')\nnominal.remove('Cabin')\nnominal.remove('Embarked')\nnominal.remove('Deck')\nnominal.remove('Name')\nnominal.remove('Sex')","c2054263":"dependent = ['Survived']","cb748b30":"continuous_correlation = sorted(continuous) + dependent\ncorrelation = data_model[continuous_correlation].corr(method=\"pearson\")\nmask = np.triu(np.ones_like(correlation, dtype=bool))\nfig = plt.figure(figsize=(5,5), dpi=100, facecolor = 'w', edgecolor=\"k\")\nfigure = sns.heatmap(correlation, mask=mask, cmap= \"Spectral\", center=0, linewidths=1, fmt=\".2f\")","f448e28c":"correlationDO = sorted(discrete) + sorted(ordinal) + dependent\ncorrelation = data_model[correlationDO].corr(method=\"spearman\")\nmask = np.triu(np.ones_like(correlation, dtype=bool))\nfig = plt.figure(figsize=(10,10), dpi=100, facecolor = 'w', edgecolor=\"k\")\nfigure = sns.heatmap(correlation, mask=mask, cmap= \"Spectral\", center=0, linewidths=1, fmt=\".2f\")","f619d6fb":"correlationDO = sorted(nominal) + dependent\ncorrelation = data_model[correlationDO].corr(method=\"spearman\")\nmask = np.triu(np.ones_like(correlation, dtype=bool))\nfig = plt.figure(figsize=(10,10), dpi=100, facecolor = 'w', edgecolor=\"k\")\nfigure = sns.heatmap(correlation, mask=mask, cmap= \"Spectral\", center=0, linewidths=1, fmt=\".2f\")","5b611ba8":"data = data_model.set_index('PassengerId')","ca72d48d":"data_train = data.loc[(data['Survived'].notna())]\ndata_toPredict = data.loc[(data['Survived'].isna())]","a332a192":"del data_toPredict['Survived']","06392af8":"data_toPredict.head()","1e550299":"data_train.head()","e947305a":"x1 = data_train.drop(columns=[\"Survived\"]).values\ny1 = data_train[\"Survived\"].values","c8f4fb78":"lr = LogisticRegression()","587beb24":"x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, train_size=0.6, test_size=0.4, random_state=100)","005230c7":"scaler = StandardScaler()\nx1_train = scaler.fit_transform(x1_train)\nx1_test = scaler.transform(x1_test)","e7a4bc4c":"lr.fit(x1_train, y1_train)","90b5d163":"print(lr.score(x1_train, y1_train))","6b048347":"print(lr.score(x1_test, y1_test))","ada6d2f8":"x1 = scaler.fit_transform(x1)","9fc22e7c":"lr.fit(x1, y1)","f9a6320c":"print(lr.score(x1, y1))","3f7b029a":"data_toPredict = scaler.fit_transform(data_toPredict)","f0db97fc":"Survived = lr.predict(data_toPredict)","1baf7961":"SurvivedList = []","70152049":"for x in Survived:\n    SurvivedList.append(int(x))","81384997":"submission = data.loc[(data['Survived'].isna())]\nsubmission.head()","d3d7526f":"del submission['Survived']","25e0c646":"submission['Survived'] = np.array(SurvivedList)","24fc2312":"submission = submission[['Survived']]","f6738eab":"submission.head()","5c7f1170":"submission.to_csv(\"version1.csv\")","9fb21cb3":"data_train2 = data.loc[(data['Survived'].notna())]\ndata_toPredict2 = data.loc[(data['Survived'].isna())]","d411c558":"data_train2.head()","8cb6f3d9":"data_train2.columns.tolist()","82805e93":"data_train2 = data_train2[['Survived',\n 'Pclass',\n 'SibSp',\n 'Parch',\n 'Sex_female',\n 'Emb_NoEmbarked',\n 'Emb_Q',\n 'Emb_S',\n 'T_Dr.',\n 'T_Master.',\n 'T_Miss.',\n 'T_Mr.',\n 'T_Mrs.',\n 'T_NoTitle',\n 'T_Rev.',\n 'D_B',\n 'D_C',\n 'D_D',\n 'D_E',\n 'D_F',\n 'D_G',\n 'D_T',\n 'D_U',\n 'FarePlus (box-cox)',\n 'AgeImpute (box-cox)',\n 'dummySurvived',\n 'MC_0',\n 'MC_1',\n 'MC_2',\n 'TicketAmount',\n 'GrId_0',\n 'GrId_1',\n 'GrId_2',\n 'GrId_3']]","34e45942":"data_train2.head()","c02d2d13":"data_toPredict2 = data_toPredict2[['Survived',\n 'Pclass',\n 'SibSp',\n 'Parch',\n 'Sex_female',\n 'Emb_NoEmbarked',\n 'Emb_Q',\n 'Emb_S',\n 'T_Dr.',\n 'T_Master.',\n 'T_Miss.',\n 'T_Mr.',\n 'T_Mrs.',\n 'T_NoTitle',\n 'T_Rev.',\n 'D_B',\n 'D_C',\n 'D_D',\n 'D_E',\n 'D_F',\n 'D_G',\n 'D_T',\n 'D_U',\n 'FarePlus (box-cox)',\n 'AgeImpute (box-cox)',\n 'dummySurvived',\n 'MC_0',\n 'MC_1',\n 'MC_2',\n 'TicketAmount',\n 'GrId_0',\n 'GrId_1',\n 'GrId_2',\n 'GrId_3']]","d9668cff":"del data_toPredict2['Survived']","79ca3d6c":"x1 = data_train2.drop(columns=[\"Survived\"]).values\ny1 = data_train2[\"Survived\"].values","2c9f696e":"x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, train_size=0.8, test_size=0.2, random_state=100)","8dc4223a":"x1_train = scaler.fit_transform(x1_train)\nx1_test = scaler.transform(x1_test)","80abd973":"lr.fit(x1_train, y1_train)","b097868b":"print(lr.score(x1_train, y1_train))","8d80ee2c":"print(lr.score(x1_test, y1_test))","d0676439":"x1 = scaler.fit_transform(x1)","a231c65d":"lr.fit(x1, y1)\nprint(lr.score(x1, y1))","5d478c55":"data_toPredict2 = scaler.fit_transform(data_toPredict2)","4b0c15a5":"Survived2 = lr.predict(data_toPredict2)","4fd93643":"SurvivedList2 = []","1bce1641":"for x in Survived2:\n    SurvivedList2.append(int(x))","407f87dc":"submission = data.loc[(data['Survived'].isna())]\nsubmission.head()","7eef7788":"del submission['Survived']","b5145afb":"submission['Survived'] = np.array(SurvivedList2)","324ccd22":"submission = submission[['Survived']]","319a14fe":"submission.head()","c06864e3":"submission.to_csv(\"version2.csv\")","b783f2d4":"data_train3 = data.loc[(data['Survived'].notna())]\ndata_toPredict3 = data.loc[(data['Survived'].isna())]","844ba0fb":"del data_toPredict3['Survived']","a2b0ec09":"x1 = data_train3.drop(columns=[\"Survived\"]).values\ny1 = data_train3[\"Survived\"].values","fd43a7c5":"type(x1)","dacd137b":"x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, train_size=0.8, test_size=0.2, random_state=1)","3b465517":"random_state = 4\nmodelRF = RandomForestClassifier(random_state=4)\nparameters = {\"n_estimators\": [100,125,150,175],\n             \"max_depth\": [3,4,5,6,7]}\n\nkfold = KFold(n_splits=3, random_state=random_state, shuffle = True)\ncv = GridSearchCV(modelRF, param_grid = parameters, cv=kfold, scoring = \"accuracy\")\ncv.fit(x1_train, y1_train)\n\ny1_predict = cv.predict(x1_test)\ny1_pred_proba = cv.predict_proba(x1_test)[:,1]\n\nprint(\"Accuracy train: {}\".format(cv.score(x1_train, y1_train)))\nprint(\"Accuracy test: {}\".format(cv.score(x1_test, y1_test)))\nprint(\"AUC score: {}\".format(roc_auc_score(y1_test, y1_pred_proba)))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\nprint(confusion_matrix(y1_test, y1_predict))\nprint(classification_report(y1_test, y1_predict))","ca23ddec":"forest = RandomForestClassifier(n_estimators = 150, max_depth=4, random_state=4)\nforest.fit(x1, y1)\n\nprint(forest.score(x1, y1))","493d916c":"RandomForestPrediction = forest.predict(data_toPredict3)\nRandomForestPrediction = [int(x) for x in RandomForestPrediction]\nprint(RandomForestPrediction)\nprint(type(RandomForestPrediction[0]))","a97d2a1e":"data_toPredict3['Survived'] = RandomForestPrediction","8cf40b1f":"data_toPredict3.head()","0e693266":"submission = data_toPredict3[['Survived']]","81fe82e1":"submission.head()","150e0a2b":"submission.to_csv(\"version3.csv\")","61274c4b":"data_train5 = data.loc[(data['Survived'].notna())]\ndata_toPredict5 = data.loc[(data['Survived'].isna())]","d700ba97":"# First without DummySurvived\ndata_train5 = data_train5[['Survived',\n 'Pclass',\n 'SibSp',\n 'Parch',\n 'Sex_female',\n 'Emb_NoEmbarked',\n 'Emb_Q',\n 'Emb_S',\n 'T_Dr.',\n 'T_Master.',\n 'T_Miss.',\n 'T_Mr.',\n 'T_Mrs.',\n 'T_NoTitle',\n 'T_Rev.',\n 'D_B',\n 'D_C',\n 'D_D',\n 'D_E',\n 'D_F',\n 'D_G',\n 'D_T',\n 'D_U',\n 'FarePlus (box-cox)',\n 'AgeImpute (box-cox)',\n 'MC_0',\n 'MC_1',\n 'MC_2',\n 'TicketAmount',\n 'GrId_0',\n 'GrId_1',\n 'GrId_2',\n 'GrId_3']]","7473aa87":"data_toPredict5 = data_toPredict5[['Pclass',\n 'SibSp',\n 'Parch',\n 'Sex_female',\n 'Emb_NoEmbarked',\n 'Emb_Q',\n 'Emb_S',\n 'T_Dr.',\n 'T_Master.',\n 'T_Miss.',\n 'T_Mr.',\n 'T_Mrs.',\n 'T_NoTitle',\n 'T_Rev.',\n 'D_B',\n 'D_C',\n 'D_D',\n 'D_E',\n 'D_F',\n 'D_G',\n 'D_T',\n 'D_U',\n 'FarePlus (box-cox)',\n 'AgeImpute (box-cox)',\n 'MC_0',\n 'MC_1',\n 'MC_2',\n 'TicketAmount',\n 'GrId_0',\n 'GrId_1',\n 'GrId_2',\n 'GrId_3']]","3eacf4f0":"x1 = data_train5.drop(columns=[\"Survived\"]).values\ny1 = data_train5[\"Survived\"].values","0253fb1a":"x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, train_size=0.8, test_size=0.2, random_state=1)","7db89c64":"random_state = 4\nmodelRF = RandomForestClassifier(random_state=4)\nparameters = {\"n_estimators\": [150,200,250,300,350],\n             \"max_depth\": [4,5,6],\n             \"max_features\": [\"auto\", \"log2\", None]}\n\nkfold = KFold(n_splits=3, random_state=random_state, shuffle = True)\ncv = GridSearchCV(modelRF, param_grid = parameters, cv=kfold, scoring = \"accuracy\")\ncv.fit(x1_train, y1_train)\n\ny1_predict = cv.predict(x1_test)\ny1_pred_proba = cv.predict_proba(x1_test)[:,1]\n\nprint(\"Accuracy train: {}\".format(cv.score(x1_train, y1_train)))\nprint(\"Accuracy test: {}\".format(cv.score(x1_test, y1_test)))\nprint(\"AUC score: {}\".format(roc_auc_score(y1_test, y1_pred_proba)))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\nprint(confusion_matrix(y1_test, y1_predict))\nprint(classification_report(y1_test, y1_predict))","d3735f51":"names = data_toPredict5.columns.tolist()","6a8c6186":"forest = RandomForestClassifier(n_estimators = 200, max_depth=5, random_state=4)\nforest.fit(x1, y1)\n\nprint(forest.score(x1, y1))\n\nfeature_imp = pd.Series(forest.feature_importances_,index=names).sort_values(ascending=False)\nprint(feature_imp)\n\n%matplotlib inline\n# Creating a bar plot\nplt.figure(figsize=(10,10))\nplt.title(\"Feature importances\")\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Feature Importance\")\nplt.legend()\nplt.show()","95dddfd4":"RandomForestPrediction = forest.predict(data_toPredict5)\nRandomForestPrediction = [int(x) for x in RandomForestPrediction]\nprint(RandomForestPrediction)\nprint(type(RandomForestPrediction[0]))","6619a5b7":"data_toPredict5['Survived'] = RandomForestPrediction","21c3db06":"submission = data_toPredict5[['Survived']]","caf85d96":"submission.to_csv(\"version5.csv\")","bc88d1c5":"data_train5B = data.loc[(data['Survived'].notna())]\ndata_toPredict5B = data.loc[(data['Survived'].isna())]","ad826e15":"# Now without MC\ndata_train5B = data_train5B[['Survived',\n 'Pclass',\n 'SibSp',\n 'Parch',\n 'Sex_female',\n 'Emb_NoEmbarked',\n 'Emb_Q',\n 'Emb_S',\n 'T_Dr.',\n 'T_Master.',\n 'T_Miss.',\n 'T_Mr.',\n 'T_Mrs.',\n 'T_NoTitle',\n 'T_Rev.',\n 'D_B',\n 'D_C',\n 'D_D',\n 'D_E',\n 'D_F',\n 'D_G',\n 'D_T',\n 'D_U',\n 'FarePlus (box-cox)',\n 'AgeImpute (box-cox)',\n 'dummySurvived',\n 'TicketAmount',\n 'GrId_0',\n 'GrId_1',\n 'GrId_2',\n 'GrId_3']]","b8d380e3":"data_toPredict5B = data_toPredict5B[['Pclass',\n 'SibSp',\n 'Parch',\n 'Sex_female',\n 'Emb_NoEmbarked',\n 'Emb_Q',\n 'Emb_S',\n 'T_Dr.',\n 'T_Master.',\n 'T_Miss.',\n 'T_Mr.',\n 'T_Mrs.',\n 'T_NoTitle',\n 'T_Rev.',\n 'D_B',\n 'D_C',\n 'D_D',\n 'D_E',\n 'D_F',\n 'D_G',\n 'D_T',\n 'D_U',\n 'FarePlus (box-cox)',\n 'AgeImpute (box-cox)',\n 'dummySurvived',\n 'TicketAmount',\n 'GrId_0',\n 'GrId_1',\n 'GrId_2',\n 'GrId_3']]","384d4282":"x1 = data_train5B.drop(columns=[\"Survived\"]).values\ny1 = data_train5B[\"Survived\"].values","f44622dd":"x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, train_size=0.8, test_size=0.2, random_state=1)","a9098193":"random_state = 4\nmodelRF = RandomForestClassifier(random_state=4)\nparameters = {\"n_estimators\": [500,750,1000],\n             \"max_depth\": [4,6],\n             \"max_features\": [\"auto\", \"log2\", None]}\n\nkfold = KFold(n_splits=3, random_state=random_state, shuffle = True)\ncv = GridSearchCV(modelRF, param_grid = parameters, cv=kfold, scoring = \"accuracy\")\ncv.fit(x1_train, y1_train)\n\ny1_predict = cv.predict(x1_test)\ny1_pred_proba = cv.predict_proba(x1_test)[:,1]\n\nprint(\"Accuracy train: {}\".format(cv.score(x1_train, y1_train)))\nprint(\"Accuracy test: {}\".format(cv.score(x1_test, y1_test)))\nprint(\"AUC score: {}\".format(roc_auc_score(y1_test, y1_pred_proba)))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\nprint(confusion_matrix(y1_test, y1_predict))\nprint(classification_report(y1_test, y1_predict))","42022683":"names = data_toPredict5B.columns.tolist()","32744ee4":"forest = RandomForestClassifier(n_estimators = 500, max_depth=4, random_state=4)\nforest.fit(x1, y1)\n\nprint(forest.score(x1, y1))\n\nfeature_imp = pd.Series(forest.feature_importances_,index=names).sort_values(ascending=False)\nprint(feature_imp)\n\n%matplotlib inline\n# Creating a bar plot\nplt.figure(figsize=(10,10))\nplt.title(\"Feature importances\")\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Feature Importance\")\nplt.legend()\nplt.show()","22af39ff":"RandomForestPrediction = forest.predict(data_toPredict5B)\nRandomForestPrediction = [int(x) for x in RandomForestPrediction]\nprint(RandomForestPrediction)\nprint(type(RandomForestPrediction[0]))","9484b40f":"data_toPredict5B['Survived'] = RandomForestPrediction","f2775e2f":"submission = data_toPredict5B[['Survived']]","66b24c23":"submission.to_csv(\"Version6.csv\")","f9cea4e9":"data.head()","3f78237b":"sns.displot(data, x='AgeImpute')","c9492f7a":"data['AgeGroups'] = data.apply(lambda row: 1 if row.AgeImpute < 11 else\n                              (2 if row.AgeImpute < 31 else\n                              (3 if row.AgeImpute < 51 else 4)), axis = 1)","2555fffa":"sns.displot(data, x='AgeGroups', hue='Survived')","25f04640":"sns.displot(data, x='Fare')","dcb3c470":"data['FareGroups'] = data.apply(lambda row: 1 if row.Fare < 20 else\n                               (2 if row.Fare < 40 else\n                               (3 if row.Fare < 80 else\n                               (4 if row.Fare < 100 else\n                               (5 if row.Fare < 200 else 6)))), axis=1)","d26c3167":"data['FareGroups2'] = data.apply(lambda row: 1 if row.Fare < 10 else\n                               (2 if row.Fare < 20 else\n                               (3 if row.Fare < 40 else\n                               (4 if row.Fare < 80 else\n                               (5 if row.Fare < 100 else 6)))), axis=1)","8b398ca4":"sns.displot(data, x='FareGroups', hue='Survived')","ae5a453a":"sns.displot(data, x='FareGroups2', hue='Survived')","08f11abc":"data_train6 = data.loc[(data['Survived'].notna())]\ndata_toPredict6 = data.loc[(data['Survived'].isna())]","8ed3dd2a":"data_train6.head()","8010fa4b":"data_train6 = data_train6[['Survived',\n 'Pclass',\n 'SibSp',\n 'Parch',\n 'Sex_female',\n 'Emb_NoEmbarked',\n 'Emb_Q',\n 'Emb_S',\n 'T_Dr.',\n 'T_Master.',\n 'T_Miss.',\n 'T_Mr.',\n 'T_Mrs.',\n 'T_NoTitle',\n 'T_Rev.',\n 'D_B',\n 'D_C',\n 'D_D',\n 'D_E',\n 'D_F',\n 'D_G',\n 'D_T',\n 'D_U',\n 'FareGroups2',\n 'AgeGroups',\n 'dummySurvived',\n 'TicketAmount',\n 'GrId_0',\n 'GrId_1',\n 'GrId_2',\n 'GrId_3']]","c820113f":"data_toPredict6 = data_toPredict6[['Pclass',\n 'SibSp',\n 'Parch',\n 'Sex_female',\n 'Emb_NoEmbarked',\n 'Emb_Q',\n 'Emb_S',\n 'T_Dr.',\n 'T_Master.',\n 'T_Miss.',\n 'T_Mr.',\n 'T_Mrs.',\n 'T_NoTitle',\n 'T_Rev.',\n 'D_B',\n 'D_C',\n 'D_D',\n 'D_E',\n 'D_F',\n 'D_G',\n 'D_T',\n 'D_U',\n 'FareGroups2',\n 'AgeGroups',\n 'dummySurvived',\n 'TicketAmount',\n 'GrId_0',\n 'GrId_1',\n 'GrId_2',\n 'GrId_3']]","64a92a70":"x1 = data_train6.drop(columns=[\"Survived\"]).values\ny1 = data_train6[\"Survived\"].values","6795a910":"x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, train_size=0.8, test_size=0.2, random_state=1)","dd366e8e":"random_state = 4\nmodelRF = RandomForestClassifier(random_state=4)\nparameters = {\"n_estimators\": [600,750,800],\n             \"max_depth\": [4,5,6],\n             \"max_features\": [\"auto\", \"log2\", None]}\n\nkfold = KFold(n_splits=3, random_state=random_state, shuffle = True)\ncv = GridSearchCV(modelRF, param_grid = parameters, cv=kfold, scoring = \"accuracy\")\ncv.fit(x1_train, y1_train)\n\ny1_predict = cv.predict(x1_test)\ny1_pred_proba = cv.predict_proba(x1_test)[:,1]\n\nprint(\"Accuracy train: {}\".format(cv.score(x1_train, y1_train)))\nprint(\"Accuracy test: {}\".format(cv.score(x1_test, y1_test)))\nprint(\"AUC score: {}\".format(roc_auc_score(y1_test, y1_pred_proba)))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\nprint(confusion_matrix(y1_test, y1_predict))\nprint(classification_report(y1_test, y1_predict))","0cfa88ce":"names = data_toPredict6.columns.tolist()","e0baad4e":"forest = RandomForestClassifier(n_estimators = 750, max_depth=5, random_state=4, max_features=\"auto\")\nforest.fit(x1, y1)\n\nprint(forest.score(x1, y1))\n\nfeature_imp = pd.Series(forest.feature_importances_,index=names).sort_values(ascending=False)\nprint(feature_imp)\n\n%matplotlib inline\n# Creating a bar plot\nplt.figure(figsize=(10,10))\nplt.title(\"Feature importances\")\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Feature Importance\")\nplt.legend()\nplt.show()","04fce065":"RandomForestPrediction = forest.predict(data_toPredict6)\nRandomForestPrediction = [int(x) for x in RandomForestPrediction]\nprint(RandomForestPrediction)\nprint(type(RandomForestPrediction[0]))","d8394259":"data_toPredict6['Survived'] = RandomForestPrediction","fe17f677":"submission = data_toPredict6[['Survived']]","773680eb":"submission.to_csv(\"Version7.csv\")","5454af9a":"data_train7 = data.loc[(data['Survived'].notna())]\ndata_toPredict7 = data.loc[(data['Survived'].isna())]","df927d4a":"data_train7 = data_train7[['Survived',\n 'Pclass',\n 'SibSp',\n 'Parch',\n 'Sex_female',\n 'Emb_Q',\n 'Emb_S',\n 'T_Dr.',\n 'T_Master.',\n 'T_Miss.',\n 'T_Mr.',\n 'T_Mrs.',\n 'D_B',\n 'D_D',\n 'D_E',\n 'D_U',\n 'FareGroups2',\n 'AgeGroups',\n 'dummySurvived',\n 'TicketAmount',\n 'GrId_0',\n 'GrId_1',\n 'GrId_2',\n 'GrId_3']]","9883f602":"data_toPredict7 = data_toPredict7[['Pclass',\n 'SibSp',\n 'Parch',\n 'Sex_female',\n 'Emb_Q',\n 'Emb_S',\n 'T_Dr.',\n 'T_Master.',\n 'T_Miss.',\n 'T_Mr.',\n 'T_Mrs.',\n 'D_B',\n 'D_D',\n 'D_E',\n 'D_U',\n 'FareGroups2',\n 'AgeGroups',\n 'dummySurvived',\n 'TicketAmount',\n 'GrId_0',\n 'GrId_1',\n 'GrId_2',\n 'GrId_3']]","d4cf7342":"x1 = data_train7.drop(columns=[\"Survived\"]).values\ny1 = data_train7[\"Survived\"].values","722493ef":"x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, train_size=0.8, test_size=0.2, random_state=1)","e40b2b8a":"random_state = 4\nmodelRF = RandomForestClassifier(random_state=4)\nparameters = {\"n_estimators\": [500,750,1000],\n             \"max_depth\": [4,6,8],\n             \"max_features\": [\"auto\", \"log2\", None]}\n\nkfold = KFold(n_splits=3, random_state=random_state, shuffle = True)\ncv = GridSearchCV(modelRF, param_grid = parameters, cv=kfold, scoring = \"accuracy\")\ncv.fit(x1_train, y1_train)\n\ny1_predict = cv.predict(x1_test)\ny1_pred_proba = cv.predict_proba(x1_test)[:,1]\n\nprint(\"Accuracy train: {}\".format(cv.score(x1_train, y1_train)))\nprint(\"Accuracy test: {}\".format(cv.score(x1_test, y1_test)))\nprint(\"AUC score: {}\".format(roc_auc_score(y1_test, y1_pred_proba)))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\nprint(confusion_matrix(y1_test, y1_predict))\nprint(classification_report(y1_test, y1_predict))","2d447e95":"random_state = 4\nmodelRF = RandomForestClassifier(random_state=4)\nparameters = {\"n_estimators\": [750,1000,1250],\n             \"max_depth\": [5,6,7],\n             \"max_features\": [\"auto\", \"log2\", None]}\n\nkfold = KFold(n_splits=3, random_state=random_state, shuffle = True)\ncv = GridSearchCV(modelRF, param_grid = parameters, cv=kfold, scoring = \"accuracy\")\ncv.fit(x1_train, y1_train)\n\ny1_predict = cv.predict(x1_test)\ny1_pred_proba = cv.predict_proba(x1_test)[:,1]\n\nprint(\"Accuracy train: {}\".format(cv.score(x1_train, y1_train)))\nprint(\"Accuracy test: {}\".format(cv.score(x1_test, y1_test)))\nprint(\"AUC score: {}\".format(roc_auc_score(y1_test, y1_pred_proba)))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\nprint(confusion_matrix(y1_test, y1_predict))\nprint(classification_report(y1_test, y1_predict))","165591ce":"names = data_toPredict7.columns.tolist()","fdc58a65":"forest = RandomForestClassifier(n_estimators = 750, max_depth=5, random_state=4, max_features=\"auto\")\nforest.fit(x1, y1)\n\nprint(forest.score(x1, y1))\n\nfeature_imp = pd.Series(forest.feature_importances_,index=names).sort_values(ascending=False)\nprint(feature_imp)\n\n%matplotlib inline\n# Creating a bar plot\nplt.figure(figsize=(10,10))\nplt.title(\"Feature importances\")\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Feature Importance\")\nplt.legend()\nplt.show()","7f22aa24":"RandomForestPrediction = forest.predict(data_toPredict7)\nRandomForestPrediction = [int(x) for x in RandomForestPrediction]\nprint(RandomForestPrediction)\nprint(type(RandomForestPrediction[0]))","c3a9367e":"data_toPredict6['Survived'] = RandomForestPrediction","767782b0":"submission = data_toPredict6[['Survived']]","08b1f611":"submission.to_csv(\"Version8.csv\")","db8ce73f":"##### Model Linear","21870cb5":"# Titanic Project - PART 3 - Model","e6512671":"#### DECK","baaa413b":"Before I can normalize the age column with a box-cox transformation, I need to impute the missing values. Since I skipped this in the first part of the project due to a lack of a good imputation strategy. \n\nI will try to create a small machine learning model to predict the missing age values. \ncolumns to use:\n","907a34d5":"##### LogisticRegression 2","3708c2fc":"### CONTINUOUS","831193be":"##### RandomForest 4 - with grouping for the continuous variables","da0944d9":"## Let's look at missing values first","26ee5627":"##### Model Lasso","b2d0f818":"#### SEX","bbcaf420":"##### Model Ridge","7b8e1e68":"It looks like the chances are that either the entire group survives, or all of them die. I see one case in which a mother and three of her children survive but one son, the oldest dies. This means that my age threshold for kids might be a bit too high. I choose 11. Maybe a bit younger would be a better predictor.\n\nI therefore went back and changed it to 9.","07a4375b":"The model is not performing exceptionally well... which was to be expected. ","813aa706":"Let's see what kind of situations we encounter in the data:\n- CabinAmount = TicketAmount >>> Probably a good indication of the group\n- CabinAmount < TicketAmount, with Cabin U = unknown >>> TicketAmount is probs the better indication of the group\n- CabinAmount > TicketAmount, with Cabin != U & Pclass = 3 >>> Probably the cabin is shared with other people. \n- CabinAmount < TicketAmount, with Cabin B\/D, class 1 >>> Probably travelling together based on ticket but two different cabins.\n\nAll together, the ticketAmount seems to be the better predicter of a group travelling together. \n","230b7109":"Since I cannot find a good imputation value for the location the two ladies embarked, I will fill the missing values with 'NoEmbarked'.","2bf397c9":"#### TITLE","70d82649":"#### OTHER POSSIBLE GROUPS THAT TRAVELLED TOGETHER","eb285b19":"- Created a new copy of the data, called \"data\"  and reset the index.\n- Created a column CountMomCh, in here, the size of the group (mothers with children) will be saved, 0 when a person does not belong to a group at all\n- two more additional columns. One CountSurvived. In here, I count the survivors within the group. One CountNanSurvived, in here I count the people in the group of whom we do not know ","79802f3f":"What could be other indicators that two (or more) people travelled together?\n- Last name\n- Ticket number, is it the same, than it was probably purchased at the same time (check also Fare in that case)\n- Cabin, although you can ofcourse stay in different Cabins and still travel together\n- The chances that people traveled together but not departed at the same harbor seem slim. So this can be used to check if people truly travelled together.\n\n\nFor the Ticket and the Cabin columns, I will use a counter to see how often the values exist within the dataset. I will add this number to two new columns: TicketAmount, CabinAmount","2a3215de":"## ORDINAL","f9a31ce0":"## Load the data","ac690c3f":"##### RandomForest 5 - less features","dba06fd0":"I will create a column that holds the last name of the passenger when the passenger is either a mother or a young (younger than 11) child. Based on this column, I will check how the group as a whole did on surviving the disaster. ","042083d1":"#### EMBARKED","9827d60c":"##### RandomForest 3 A and B","c9fd5a0e":"Let's fit the model on the entire train set. In order to be able to get the best model with this data. After training the model, I will make a prediction and give the first results a try in Kaggle.","aaee6f90":"Feateare engineering:\n- sex >  One hot encoding\n- embarked > One hot encoding\n- Age groups > ?\n- Age, box-cox\n- Fare groups > ?\n- Fare, box-cox\n- Deck, from Cabin > One hot encoding\n- Title, from Name > One hot encoding\n- Mother-child... both survive, both die\n- Family count... does family size influence the survival chances?\n- Travelling together yes\/no > is that enough? can we do something more with this?","0eba6ddb":"So now we have imputed any missing values, it is time to look at normalization using box-cox. For this, no negative values may exist. So let's check that first.","65a403cf":"## DISCRETE","d32daf11":"I will not specify the NoTitle further. Since they are with so few, I do not think this will help the model predict the chance to survive the Titanic. ","fb429187":"Overall, there are two continuous variables: fare and age. I created a few more but I will focus on the two original variables to create groups. \n\nFirst for Age, after for Fare.","d915c434":"All titles that we did not extract belong to people in the first class.","6b9acf25":"## Splitting data by Type","065c9557":"##### LogisticRegression 1","2f9f9bec":"From two first class female passengers, we do not know where they embarked. Funnily enough, it looks like they have no family relationship (SibSp and Parch columns do not show proof of such a relation), but they are seem to be travelling together (see the columns Ticket and Cabin). Let's check if there are more people residing in the same Cabin. ","f945b808":"# Titanic Project - PART 2 - EDA","852f6c2d":"It looks like travelling with a small family increased the chances of survival.","3bcbca83":"#### FARE","c7c55211":"I will now test what happens to the model when I delete either the Dummyurvived feature, or the MC features.","67d2e2fb":"It looks like we miss a lot of Cabin numbers\/information. I do not think the Cabin info itself will have an influence on the survival chances of the passengers. It might however provide additional usefull information regarding the passengers later on. I will impute the missing Cabin values with NoCabin.","fb45005f":"The missing value for the Fare belongs to a third class male passenger. I think the Fare of all third class passengers was rather low. And I am guessing, the marginal differences here will not influence the chances for survival.","c354758c":"So, the new column 'AgeImpute' contains the ages of the passenger + the imputed age values by the model. I will keep the regular Age column and imput the NaN values with the median. When I will later build the model to predict the survival of the passengers, I can see which column performs better.","194c0c0c":"#### FAMILY COUNT","81a803aa":"It looks like we have no missing values, except for the Survived... which is the column that we need to predict. ","6d2c2112":"I will concatenate both datasets (with survival and without survival). I also create a copy 'data_raw_duplicate'  to be on the safe side. ","4b183419":"#### AGE","a0f7c40c":"##### RandomForest 1","d8b13814":"### NOMINAL","c18265f8":"In this section I will build some models. I will start with Logistic Regression and will than continue with RandomForest. \nFor the logistic regression I will first use all features and will after use a subset to precent overfitting and multicollinearity. ","61398ed8":"#### MOTHER-CHILD","9a7e54af":"There are quiet a few ages missing from the dataset. \n- Can we find a better estimate of the age by looking at whom these passengers travelled with? \n\nIn order to deal with these missing values, I will first create a column that indicates I imputed the ages for these people. If I can find a better way to impute, I can still see which values belong to the original dataset and which were imputed.\n* Column 'AgeHelperColumn' == 1 if the Age value is not imputed.\n* Column 'AgeHelperColumn' == 0 if the Age valye is imputed.","f8937467":"In order to use this in a ml model, I will take two different routes. \n1. I will hash encode the MotherChild column\n2. I will create an additional column, dummySurvived. This is for testing purposes. To see how the model will react. I will add a 1 to the column if the chances of the group surviving are high, a -1 if the chances of the group dying are high, and a 0 if the person does not belong to a mother-child group.\n3. I will drop all additional columns that were created to get to this point: CountMomCh, CountMomChSurvived, CountSurvived, CountNanSurvived. I will keep LastName for a bit longer... it might still be useful to name other groups of people travelling together. ","c8d953cc":"## Let's take a look at correlation","629178eb":"# Titanic Project - PART 1 - Cleaning Data","34da44aa":"## Let's create a dataframe with useful columns","2f037511":"In order to normalize the continous variables, I will use a box-cox transformation. The function is added below. \nSince box-cox only operates on strictly positive numbers (0 is not allowed), I create a temporary new column FarePlus where I add + 0.001 to all fares to avoid 0 values. I will drop this column once the box-cox transformation is realized."}}