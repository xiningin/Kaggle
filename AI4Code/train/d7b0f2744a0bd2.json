{"cell_type":{"f975adfb":"code","53981951":"code","6cf9a72a":"code","10344234":"code","075c3bd0":"code","a94c921f":"code","04ed09a5":"code","315ca7c7":"code","0dd140b7":"code","d3dfd7ba":"code","e89ae04f":"code","775d0343":"code","abba0ccb":"code","0120f8c0":"code","d7ded6e9":"code","e5c43b6b":"code","727e0730":"code","b687143a":"code","f16c89a3":"code","131c91ba":"code","637a1232":"code","7417716a":"code","d99e8658":"code","22bd44d3":"code","4578a67e":"code","488257a0":"code","e5051226":"code","1722d697":"code","e2d2e993":"code","4cab504a":"code","6c6c7781":"code","363bb583":"code","22ac2468":"code","8f69e674":"code","9a68b64d":"code","a73c6bae":"code","8bf5b0e5":"code","c44a7936":"code","7c8cd641":"code","14e1a198":"code","96996a98":"code","a73b1378":"code","de3581dc":"code","0eb68a35":"code","bc722eab":"code","62dd57a8":"code","463fc7b7":"markdown","b472feb4":"markdown","398f81b5":"markdown","5b84ddfd":"markdown","0d4d90c9":"markdown","ac558542":"markdown","0fb93f7c":"markdown","ad40b21e":"markdown","65842603":"markdown","f830ff75":"markdown","911b31a2":"markdown","6ba7097b":"markdown","15256719":"markdown","9d9856d9":"markdown","f1e7518b":"markdown","66e9f9b5":"markdown","0f9613d3":"markdown","2106dec1":"markdown","99d893d1":"markdown","bca3da88":"markdown","49cdc153":"markdown","d579d30e":"markdown","548cd302":"markdown","f7594a8e":"markdown","456e5f13":"markdown","13b97af3":"markdown","e13bc941":"markdown","309a5002":"markdown","26281869":"markdown","e6c2ee28":"markdown","faa5b17b":"markdown","ed4c24c4":"markdown","3e01d1d8":"markdown","6b8c3a56":"markdown","95090ab1":"markdown","e6d043d2":"markdown","0e2d885b":"markdown","951a4708":"markdown","c0a70ade":"markdown","b37ff59d":"markdown","fd4478b2":"markdown"},"source":{"f975adfb":"import pandas as pd       \nimport matplotlib.pyplot as plt    \nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n#from google.colab import files\nimport warnings\nwarnings.filterwarnings('ignore')","53981951":"df_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\ndf_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\ndf_train.head()","6cf9a72a":"df_test.head()","10344234":"print(\"Shape of training data: \", df_train.shape)\nprint(\"Shape of testing data: \", df_test.shape)","075c3bd0":"print(\"Training Data Info: \\n\", df_train.info())","a94c921f":"print(\"Testing Data Info: \\n\", df_test.info())","04ed09a5":"print(\"Null values in Training Data: \\n\", sum(df_train.isnull().sum()))\nprint(\"Null values in Testing Data: \\n\", sum(df_test.isnull().sum()))","315ca7c7":"df_train.describe().T","0dd140b7":"num_col = df_train.select_dtypes(include='number').columns.to_list()\nfor i in num_col:\n    fig, ax = plt.subplots()\n    ax.boxplot(df_train[i])\n    plt.ylabel(i)","d3dfd7ba":"col = ['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9']\nfor i in col: \n    df_train[i].value_counts().plot(kind='pie', autopct = \"%1.0f%%\")\n    print(\"For attribute\" ,i, \"\\n\\n\", df_train[i].value_counts())\n    plt.show()","e89ae04f":"df_train.hist(figsize=(15,15));","775d0343":"plt.figure(figsize = (15, 15))\nsns.heatmap(df_train.corr(),annot=True);","abba0ccb":"df_train.drop('id', axis=1, inplace=True)\nX_TEST = df_test.drop('id', axis=1)\n\nprint(\"Shape of training data: \", df_train.shape)\nprint(\"Shape of test data: \", X_TEST.shape)","0120f8c0":"X = df_train.drop('target', axis = 1)\ny = df_train['target']","d7ded6e9":"print(\"Fianl Shape of training data\" , X.shape)\nprint(\"Final shape of Target column: \", y.shape)","e5c43b6b":"X.head()","727e0730":"from sklearn.preprocessing import OrdinalEncoder\noe = OrdinalEncoder()\nX[col] = oe.fit_transform(X[col])\nX_TEST[col] = oe.fit_transform(X_TEST[col])","b687143a":"X.head()","f16c89a3":"X_TEST.head()","131c91ba":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)","637a1232":"print(\"Dataset shape:\", df_train.shape)\nprint(\"Input Features shape: \", x_train.shape, y_train.shape)\nprint(\"Output Features shape: \", x_test.shape, y_test.shape)","7417716a":"dt = DecisionTreeRegressor(random_state=0,max_depth=15)\n\n# Training Model\ndt.fit(x_train, y_train)","d99e8658":"dt_pred_train = dt.predict(x_train)\ndt_pred_test = dt.predict(x_test)","22bd44d3":"print(\"Training Accuracy: \", r2_score(y_train, dt_pred_train ))\nprint(\"Test Accuracy: \", r2_score(y_test, dt_pred_test))","4578a67e":"print(\"RMSE On Trainng Data: \",np.sqrt(mean_squared_error(y_train, dt_pred_train)))\nprint(\"RMSE On Test Data: \", np.sqrt(mean_squared_error(y_test, dt_pred_test)))","488257a0":"test_preds = dt.predict(X_TEST)\ndf=pd.DataFrame({\"id\":df_test.loc[:,\"id\"], \"target\":test_preds})\ndf.to_csv(\"DT.csv\",index=False)","e5051226":"#files.download('DT.csv')","1722d697":"df.head()","e2d2e993":"rf1 = RandomForestRegressor(n_estimators = 100, min_samples_split = 3, min_samples_leaf= 4, max_features = 'sqrt', max_depth= 120, bootstrap=False)  \n\n# Training Model\nrf1.fit(x_train, y_train)\n\n# Prediction \nrf1_pred_train = rf1.predict(x_train)\nrf1_pred = rf1.predict(x_test)","4cab504a":"print(\"Training Accuracy: \", r2_score(y_train, rf1_pred_train))\nprint(\"Test Accuracy: \", r2_score(y_test, rf1_pred))","6c6c7781":"print(\"RMSE on Training Data: \", np.sqrt(mean_squared_error(y_train, rf1_pred_train)))\nprint(\"RMSE On Test Data: \", np.sqrt(mean_squared_error(y_test, rf1_pred)))","363bb583":"test_pred = rf1.predict(X_TEST)\ndf = pd.DataFrame({\"id\":df_test.loc[:,\"id\"], \"target\":test_pred})\ndf.to_csv(\"RF.csv\",index=False)\n#files.download('RF.csv')","22ac2468":"df.head()","8f69e674":"xg = XGBRegressor(random_state=42, \n                         n_jobs=4,\n                         n_estimators= 5000,\n                         learning_rate= 0.01,\n                         subsample= 0.9,\n                         max_depth= 5,\n                         colsample_bytree= 0.5,\n                         reg_alpha = 30, eval_metric='rmse')\nxg.fit(x_train, y_train)\nxg_pred_train = xg.predict(x_train)\nxg_pred = xg.predict(x_test)","9a68b64d":"print(\"RMSE on Training Data: \", np.sqrt(mean_squared_error(y_train, xg_pred_train)))\nprint(\"RMSE On Test Data: \", np.sqrt(mean_squared_error(y_test, xg_pred)))","a73c6bae":"test_pred_xg = xg.predict(X_TEST)\ndf = pd.DataFrame({\"id\":df_test.loc[:,\"id\"], \"target\":test_pred_xg})\ndf.to_csv(\"XG.csv\",index=False)\n#files.download('XG.csv')","8bf5b0e5":"df.head()","c44a7936":"xg1 = XGBRegressor(\n    n_estimators=  5000,\n    learning_rate= 0.12,\n    subsample= 0.96,\n    colsample_bytree= 0.12,\n    max_depth= 2,\n    booster= 'gbtree', \n    reg_lambda= 65.1,\n    reg_alpha= 15.9,\n    random_state=40\n)\nxg1.fit(x_train, y_train)\nxg_pred_train = xg1.predict(x_train)\nxg_pred = xg1.predict(x_test)","7c8cd641":"print(\"RMSE on Training Data: \", np.sqrt(mean_squared_error(y_train, xg_pred_train)))\nprint(\"RMSE On Test Data: \", np.sqrt(mean_squared_error(y_test, xg_pred)))","14e1a198":"test_pred_xg = xg1.predict(X_TEST)\ndf = pd.DataFrame({\"id\":df_test.loc[:,\"id\"], \"target\":test_pred_xg})\ndf.to_csv(\"XG1.csv\",index=False)\n#files.download('XG1.csv')","96996a98":"df.head()","a73b1378":"gb = GradientBoostingRegressor()\ngb.fit(x_train, y_train)","de3581dc":"gb_pred_train = gb.predict(x_train)\ngb_pred = gb.predict(x_test)","0eb68a35":"print(\"RMSE on Training Data: \", np.sqrt(mean_squared_error(y_train, gb_pred_train)))\nprint(\"RMSE On Test Data: \", np.sqrt(mean_squared_error(y_test, gb_pred)))","bc722eab":"test_pred_gb = gb.predict(X_TEST)\ndf = pd.DataFrame({\"id\":df_test.loc[:,\"id\"], \"target\":test_pred_gb})\ndf.to_csv(\"GB.csv\",index=False)\n#files.download('GB.csv')","62dd57a8":"df.head()","463fc7b7":"### Importing essential libraries","b472feb4":"### Prediction on Test Data","398f81b5":"- There are no null values in Test data","5b84ddfd":"<h2 align='center'> Gradient Boost <\/h2>","0d4d90c9":"- Dropped Id column from both test and Training Data","ac558542":"<h2 align='center'> XG Boost <\/h2>","0fb93f7c":"### Prediction on Test Data","ad40b21e":"### Prediction on Training data","65842603":"### Prediction on Test Data","f830ff75":"- There are 300000 rows and 26 columns in Training Data ie out of 26 columns 1 column is target attribute and rest 25 are input attributes\n- There are 200000 rows and 25 columns in Test Data","911b31a2":"- Id column is irreevent as it has no co-relation to target column","6ba7097b":"### Prediction on Training Data","15256719":"### Numerical attribute analysis","9d9856d9":"### Inspecting Data","f1e7518b":"### Prediction on Training Data","66e9f9b5":"### Kaggle score for Gradient Boost : 0.73270","0f9613d3":"### Prediction on Training dataset","2106dec1":"### Categorical attribute analysis","99d893d1":"### Prediction on Test Data","bca3da88":"- We can see some of the features are normally distributed \n- Target attributes is left skewed and most of the values are 8(mode)","49cdc153":"<h2 align='center'> Random Forest <\/h2>","d579d30e":"### Statistical summary for numerical columns","548cd302":"#### Checking outliers in data by using boxplot","f7594a8e":"- There are no null values in Training data","456e5f13":"- There are some outliers in data","13b97af3":"### Prediction on Training Data","e13bc941":"### Kaggle score for XG Boost (Hyper-parameter tunning): 0.71889","309a5002":"### Data-Preprocessing","26281869":"### Spliting Data into input variables and target feature","e6c2ee28":"<h2 align='center'> 30 Days of ML <\/h2>\n\nOverView: \n- EDA \n- Data preprocessing\n- Modeliing with Decision Tree,  Random Forest, XG Boost, Gradient Boost","faa5b17b":"### Correlation matrix for training data","ed4c24c4":"### If you like the notebook, please do upvote :) Thank You ","3e01d1d8":"### Kaggle score for XG Boost:  0.72033 ","6b8c3a56":"### Loading dataset","95090ab1":"### Kaggle score for Decision Tree : 0.81426 ","e6d043d2":"### Train Test Split","0e2d885b":"### Kaggle score for Random Forest : 0.73539 ","951a4708":"<hr> \n\n<h2 align='center'> Conclusion <\/h2> \n\n- XG boost with hyper-parameter tunning Turns out to be good model with a kaggle score of 0.71889 ","c0a70ade":"<h2 align='center'> Decision Tree <\/h2>","b37ff59d":"### Hyper-Parameter Tunning","fd4478b2":"### Encoding categorical attributes"}}