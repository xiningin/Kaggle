{"cell_type":{"08ffcf40":"code","610a4f14":"code","04e6192a":"code","836837d8":"code","3d1c678a":"code","df972d06":"code","15cfd5f0":"code","0b464cae":"code","09c49fdb":"code","3becca81":"code","a1f5209e":"code","a359bf8a":"code","7dd347fb":"code","4d2e662e":"code","aab8feba":"code","5d37c99e":"code","6e24635f":"code","bc3108f6":"code","95ed0a1b":"code","588c8618":"code","b7840528":"code","0e2bb715":"code","b8e21790":"code","096aa42e":"code","210feaa7":"code","d613cb94":"code","298b9ae3":"code","763cf19f":"code","90c0d265":"code","e83c338c":"code","ac8b3519":"code","9bf780cb":"code","94ec1c75":"code","c26d9504":"code","8cd167e8":"code","2ad67248":"code","a6048642":"code","0c533fd0":"code","3cee41b3":"code","daf4a1a5":"markdown","d5289487":"markdown","fe43fd65":"markdown","1565c2d0":"markdown","ded202ff":"markdown","dbd632ca":"markdown","1e047719":"markdown"},"source":{"08ffcf40":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as tfs\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import make_grid\n\nimport cv2\nimport random as rnd\n%matplotlib inline","610a4f14":"data_dir_path = '..\/input\/fer2013modified\/Emotion Updated 1'","04e6192a":"\ntrain_dir_path = data_dir_path + '\/train\/'\ntest_dir_path = data_dir_path + '\/test\/'","836837d8":"classes = os.listdir(train_dir_path)","3d1c678a":"classes","df972d06":"def show_img(path):\n    plt.figure(figsize=(5,5))\n    imgs = cv2.imread(path)\n    print(\"Shape of the Image: \",imgs.shape)\n    plt.imshow(imgs)","15cfd5f0":"for i in range(5):\n    cat = rnd.choice(os.listdir(train_dir_path))\n    img = rnd.choice(os.listdir(train_dir_path+cat))\n    path = train_dir_path+cat+\"\/\"+img\n    print(\"Label : \",cat)\n    show_img(path)\n","0b464cae":"for i in classes:\n    path = train_dir_path+i\n    print(f\"Number of images in {i} train data class: {len(os.listdir(path))} \")","09c49fdb":"for i in classes:\n    path = test_dir_path+i\n    print(f\"Number of images in {i} test data class: {len(os.listdir(path))} \")","3becca81":"input_shape = (3, 56, 56)","a1f5209e":"stats = ((0.5),(0.5))\ntrain_transforms = tfs.Compose([\n    tfs.Resize(input_shape[1:]),\n    tfs.Grayscale(),\n#     tfs.RandomCrop(56,padding=4,padding_mode='reflect'),\n    tfs.RandomHorizontalFlip(),\n    tfs.RandomRotation(10),\n    tfs.Resize(input_shape[1:]),\n    tfs.ToTensor(),\n    tfs.Normalize(*stats,inplace=True),\n])\n\ntest_transforms = tfs.Compose([\n    tfs.Resize(input_shape[1:]),\n    tfs.Grayscale(),\n    tfs.ToTensor(),\n    tfs.Normalize(*stats,inplace=True),\n])","a359bf8a":"train_dataset = ImageFolder(root=train_dir_path,transform=train_transforms)\nvalid_dataset = ImageFolder(root=test_dir_path,transform=test_transforms)","7dd347fb":"train_dataset","4d2e662e":"valid_dataset","aab8feba":"batch_size = 128","5d37c99e":"train_loader = DataLoader(train_dataset,batch_size,shuffle=True,num_workers=2,pin_memory=True)\nvalid_loader = DataLoader(valid_dataset,batch_size,num_workers=2,pin_memory=True)","6e24635f":"for img, label in train_loader:\n    print(img.shape)\n    break","bc3108f6":"def denormalize(images, means, std):\n  means = torch.tensor(means).reshape(1, 1, 1, 1)\n  stds = torch.tensor(std).reshape(1, 1, 1, 1)\n  return images * stds + means\n\ndef show_batch(dl):\n  for images, label in dl:\n    fig, ax = plt.subplots(figsize=(12,12))\n    ax.set_xticks([]); ax.set_yticks([])\n    denorm = denormalize(images, *stats)\n    ax.imshow(make_grid(denorm[:64], nrow=8).permute(1, 2, 0).clamp(0,1))\n    break","95ed0a1b":"show_batch(train_loader)","588c8618":"def get_default_device():\n  \"\"\"Pick GPU if availabel, else CPU\"\"\"\n  if torch.cuda.is_available():\n    return torch.device('cuda')\n  else:\n    return torch.device('cpu')\n\ndef to_device(data, device):\n  if isinstance(data, (list,tuple)):\n    return [to_device(x, device) for x in data]\n  return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n  \"\"\" Wrap a dataloader to move data into device\"\"\"\n  def __init__(self, dl, device):\n    self.dl = dl\n    self.device = device\n\n  def __iter__(self):\n    \"\"\" Yeild a batch of data after moving it to device\"\"\"\n    for b in self.dl:\n      yield to_device(b, self.device)\n\n  def __len__(self):\n    \"\"\" Number of batches\"\"\"\n    return len(self.dl)","b7840528":"device = get_default_device()\ndevice","0e2bb715":"train_loader = DeviceDataLoader(train_loader, device)\nvalid_loader = DeviceDataLoader(valid_loader, device)","b8e21790":"class ImageClassificationBaseline(nn.Module):\n  \n  def training_step(self,batch):\n    images, labels = batch\n    out = self(images)\n    loss = F.cross_entropy(out, labels)\n    return loss\n\n  def validation_step(self,batch):\n    images, labels = batch\n    out = self(images)\n    loss = F.cross_entropy(out, labels)\n    acc = self.accuracy(out, labels)\n    return {'val_loss':loss, 'val_acc':acc}\n\n  def accuracy(self, outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ preds.numel())\n\n  def validation_step_end(self, val_outputs, train_outputs):\n    val_batch_losses = [x['val_loss'] for x in val_outputs]\n    val_epoch_loss = torch.stack(val_batch_losses).mean()\n    val_batch_acc = [x['val_acc'] for x in val_outputs]\n    val_epoch_acc = torch.stack(val_batch_acc).mean()\n\n    train_batch_losses = [x['val_loss'] for x in train_outputs]\n    train_epoch_loss = torch.stack(train_batch_losses).mean()\n    train_batch_acc = [x['val_acc'] for x in train_outputs]\n    train_epoch_acc = torch.stack(train_batch_acc).mean()\n\n    return {'train_loss':train_epoch_loss.item(), 'train_acc':train_epoch_acc.item(), 'val_loss':val_epoch_loss.item(), 'val_acc':val_epoch_acc.item()}\n\n  def epoch_end(self, epoch, num_epochs, results):\n    print(\" > train_loss: {:.4f}, train_acc: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, num_epochs, \n                                    results['train_loss'], results['train_acc'], results['val_loss'], results['val_acc']))","096aa42e":"def accuracy(outputs, labels):\n  _, preds = torch.max(outputs, dim=1)\n  return torch.tensor( torch.sum(preds == labels).item() \/ preds.numel())\n","210feaa7":"def conv_block(in_channels, out_channels, pool=False):\n  layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)]\n  if pool: layers.append(nn.MaxPool2d(2))\n  return nn.Sequential(*layers)\n\nclass ResNet9(ImageClassificationBaseline):\n  def __init__(self, in_channels, num_classes):\n    super().__init__()\n    # 1 x 56 x 56\n    self.conv1 = conv_block(in_channels, 64) # 64 x 56 x 56\n    self.conv2 = conv_block(64, 128) # 128 x 56 x 56\n    self.res1 = nn.Sequential(conv_block(128, 128),\n                              conv_block(128, 128)) # 128 x 56 x 56\n\n    self.conv3 = conv_block(128, 256, pool=True) # 256 x 28 x 28\n    self.conv4 = conv_block(256, 256) # 512 x 28 x 28\n    self.res2 = nn.Sequential(conv_block(256, 256),\n                              conv_block(256, 256)) # 256 x 28 x 28\n    \n    self.conv5 = conv_block(256, 512, pool=True) # 512 x 14 x 14\n    self.conv6 = conv_block(512, 512)\n    self.res3 = nn.Sequential(conv_block(512, 512),\n                              conv_block(512,512)) \n    \n    self.conv7 = conv_block(512, 1024, pool=True) # 1024 x 7 x 7\n    self.conv8 = conv_block(1024, 2048)\n    self.res4 = nn.Sequential(conv_block(2048, 2048), \n                              conv_block(2048, 2048))\n    \n    self.classifier = nn.Sequential(nn.MaxPool2d(7),  # 1024 x 1 x 1\n                                    nn.Flatten(),     # 1024\n                                    nn.Dropout(0.3), \n                                    nn.Linear(2048, num_classes)) \n    \n  def forward(self, xb):\n\n    out = self.conv1(xb)\n    out = self.conv2(out)\n    out = self.res1(out) + out\n\n    out = self.conv3(out)\n    out = self.conv4(out)\n    out = self.res2(out) + out\n    \n    out = self.conv5(out)\n    out = self.conv6(out)\n    out = self.res3(out) + out\n    \n    out = self.conv7(out)\n    out = self.conv8(out)\n    out = self.res4(out) + out\n    \n    out = self.classifier(out)\n\n    return out\n  ","d613cb94":"model = to_device(ResNet9(1, 5), device)\nmodel","298b9ae3":"@torch.no_grad() # No need to calculate any gradients\ndef evaluate(model, val_loader, train_loader):\n  model.eval() # Tells the model that it is evaluation step, so no backpropagation, dropout, and batch normalization during evaluation\n  val_outputs = [model.validation_step(batch) for batch in val_loader]\n  train_outputs = [model.validation_step(batch) for batch in train_loader]\n  return model.validation_step_end(val_outputs, train_outputs)\n\n\ndef get_lr(optimizer):\n  for param_group in optimizer.param_groups:\n    return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, weight_decay=0, grad_clip=None, opt_func = torch.optim.SGD):\n\n  torch.cuda.empty_cache()\n  history = []\n\n  # setup custom optimizer with weight decay\n  optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n  # setup one-cycle learning rate scheduler\n  sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader))\n\n  for epoch in range(1,epochs+1):\n    # Training phase\n    model.train() # Model is trained \n    lrs = []\n\n    for batch in train_loader:\n      loss = model.training_step(batch)\n      loss.backward()\n\n      # Grad-Clipping\n      if grad_clip:\n        nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n\n      optimizer.step()\n      optimizer.zero_grad()\n\n      # Record and update learning rate\n      lrs.append(get_lr(optimizer))\n      sched.step()\n\n    # Validation phase\n    results = evaluate(model, valid_dl, train_dl)\n    results['lrs'] = lrs\n    model.epoch_end(epoch, epochs, results)\n    history.append(results)\n\n  return history\n","763cf19f":"history = [evaluate(model, valid_loader, train_loader)]\nhistory","90c0d265":"print('We can see that before training the train accuracy is {} % and valid accuracy is {}%'.format(100 * history[0]['train_acc'],100*history[0]['val_acc']))","e83c338c":"epochs = 10\nmax_lr = 0.01\nweight_decay = 1e-4\ngrad_clip = 0.2\nopt_func = torch.optim.Adam","ac8b3519":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, train_loader, valid_loader, \n                         weight_decay=weight_decay, grad_clip=grad_clip, opt_func=opt_func)","9bf780cb":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","94ec1c75":"def plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","c26d9504":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","8cd167e8":"plot_accuracies(history)","2ad67248":"plot_losses(history)","a6048642":"plot_lrs(history)","0c533fd0":"from sklearn.metrics import confusion_matrix\n\nnb_classes = 5\n\n# Initialize the prediction and label lists(tensors)\npredlist=torch.zeros(0,dtype=torch.long, device='cpu')\nlbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n\nwith torch.no_grad():\n    for i, (inputs, classes) in enumerate(valid_loader):\n        inputs = inputs.to(device)\n        classes = classes.to(device)\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n\n        # Append batch prediction results\n        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n\n# Confusion matrix\nconf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\nprint(\"Confusion Matrix is :\\n\",conf_mat)\n\n# Per-class accuracy\nclass_accuracy=100*conf_mat.diagonal()\/conf_mat.sum(1)\nprint(\"\\nPer Class Accuracy is :\\n\",class_accuracy)","3cee41b3":"torch.save(model.state_dict(), 'emotion-classification-resnet9.pth')","daf4a1a5":"# Using a GPU","d5289487":" Above is the sample of images in a batch\n","fe43fd65":"## Train the Model","1565c2d0":"Therefore the average size of the list is [3, 76, 76]","ded202ff":"From the above images we can see that all the images are grayscale and sizes of the images are different","dbd632ca":"# Read and Analyze data","1e047719":"## Model "}}