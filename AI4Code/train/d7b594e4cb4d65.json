{"cell_type":{"73760273":"code","e8b8cf27":"code","9bae7166":"code","176ea34a":"code","8a3b7145":"code","c32dacda":"code","4f63f861":"code","1b227697":"code","a3830743":"code","e137d9ae":"code","0caeca64":"code","1c87cb72":"code","17f8f713":"code","4ea4b3cb":"code","46d0ed6f":"code","2e91fc0e":"markdown","be99b7d0":"markdown","0d5708e7":"markdown","170a57e4":"markdown","3e387a2f":"markdown"},"source":{"73760273":"# update torch and torch vision\n!pip install -q torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html","e8b8cf27":"# install kornia, we will give it a try to accelarate our preprocessing\n!pip install -q --upgrade kornia\n!pip install -q allennlp==1.1.0.rc4","9bae7166":"# and install fastai2\n!pip install -q --upgrade fastai","176ea34a":"import torch\nprint(torch.__version__)\nprint(torch.cuda.is_available())\n\nimport fastai\nprint(fastai.__version__)\n\n# other imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image\n\nfrom fastai.vision.all import *","8a3b7145":"import torch\nimport fastai\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image\n\nfrom fastai.vision.all import *\n\ndef open_tif(fn, chnls=None, cls=torch.Tensor):\n    im = (np.array(Image.open(fn))).astype('float32')\n    return cls(im)\n\n# The map_filename function makes it easier to map from one folder to another by replacing strings\ndef map_filename(base_fn, str1, str2):\n    return Path(str(base_fn).replace(str1, str2))\n\ndef get_filenames(red_filename):\n    return [red_filename,\n            map_filename(red_filename, str1='red', str2='green'),\n            map_filename(red_filename, str1='red', str2='blue'),\n            map_filename(red_filename, str1='red', str2='nir'),\n           ]\n\n\n# the open multi-spectral tif function will be in charge of opening the separate tifs and collate them\ndef open_ms_tif(files):\n    ms_img = None\n    \n    for path in files:\n        img = open_tif(path)\n        \n        if ms_img is None:\n            ms_img = img[None]\n        else:\n            ms_img = np.concatenate([ms_img, img[None]], axis=0)\n            \n    return TensorImage(ms_img)\n    \n","c32dacda":"# get items from both datasets\nitems_95 = get_files('\/kaggle\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_red_additional_to38cloud', extensions='.TIF')\nitems_38 = get_files('\/kaggle\/input\/38cloud-cloud-segmentation-in-satellite-images\/38-Cloud_training\/train_red\/', extensions='.TIF')\nall_items = items_95 + items_38","4f63f861":"# now select just the non empty ones\nn_empty = pd.read_csv('\/kaggle\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/training_patches_95-cloud_nonempty.csv')\n\ndef non_empty(item):\n    \n    if n_empty.name.isin([item.stem[4:]]).any():\n        return True\n    else:\n        return False\n    \nitems_mask = all_items.map(non_empty)\nitems = all_items[items_mask]\nitems","1b227697":"idx=5\nimg_pipe = Pipeline([get_filenames, open_ms_tif])\nimg = img_pipe(items[idx])\n\nmask_pipe = Pipeline([partial(map_filename, str1='red', str2='gt'), \n                      partial(open_tif, cls=TensorMask)])\n\nmask = mask_pipe(items[idx])\nprint(img.shape, mask.shape)\n\n_, ax = plt.subplots(1, 2, figsize=(12,5))\nax[0].imshow(img.permute(1, 2, 0)[..., :3]\/20000)\nmask.show(ctx=ax[1])","a3830743":"def show_img(tensor_img, ctx=None):\n    ctx = plt.subplot() if ctx is None else ctx\n    \n    #normalize to fit between 0 and 1\n    if tensor_img.max() > 0:\n        tensor_img = tensor_img \/ tensor_img.max()\n    \n    ctx.imshow(tensor_img.permute(1, 2, 0)[..., :3])\n    \n# To create this DataBlock we don't need to specify the get_items function \n# because we will pass the list of files as the source\ndb = DataBlock(blocks=(TransformBlock([get_filenames, open_ms_tif, lambda x: x\/10000]), \n                       TransformBlock([partial(map_filename, str1='red', str2='gt'), \n                                       partial(open_tif, cls=TensorMask)])),\n               splitter=RandomSplitter(valid_pct=0.2, seed=0)\n              )\n\n# Now We could call db.summary() to see if everything goes well\n# %time db.summary(source=items)\n# Instead, we will create the dataloader and display a batch sample\n    \nds = db.datasets(source=items)\ndl = db.dataloaders(source=items, bs=4)\nbatch = dl.one_batch()\nprint(batch[0].shape, batch[1].shape)\n\n# # display the batch\n# _, ax = plt.subplots(2, batch[0].shape[0], figsize=(batch[0].shape[0]*4, 7))\n# for i in range(batch[0].shape[0]):\n    \n#     show_img(batch[0][i], ctx=ax[0, i])\n#     TensorMask(batch[1][i]).show(ax[1, i])","e137d9ae":"import albumentations as A","0caeca64":"import pdb\nclass SegmentationAlbumentationsTransform(ItemTransform):\n#     split_idx=0\n    def __init__(self, aug, **kwargs): \n        super().__init__(**kwargs)\n        self.aug = aug\n        \n    def encodes(self, x):\n        img,mask = x\n        \n        img = img\/img.max()\n        \n#         print('applying augmentation')\n        # for albumentations to work correctly, the channels must be at the last dimension\n        aug = self.aug(image=np.array(img.permute(1,2,0)), mask=np.array(mask))\n        return TensorImage(aug['image'].transpose(2,0,1)), TensorMask(aug['mask'])\n\n# Now we will create a pipe of transformations\naug_pipe = A.Compose([A.ShiftScaleRotate(p=.9),\n                      A.HorizontalFlip(),\n                      A.RandomBrightnessContrast(contrast_limit=0.0, p=1., brightness_by_max=False)])\n\n# Create our class with this aug_pipe\naug = SegmentationAlbumentationsTransform(aug_pipe)\n\n# And check the results\nidx = 5\naug_number = 4\n\n# Display original and some augmented samples\n_, ax = plt.subplots(aug_number+1, 2, figsize=(8,aug_number*4))\n\nshow_img(ds[idx][0], ctx=ax[0,0])\nds[idx][1].show(ctx=ax[0,1])\n\n# print(ds[idx][0])\n\nfor i in range(1, aug_number+1):\n    img, mask = aug.encodes(ds[idx])\n    show_img(img, ctx=ax[i,0])\n    mask.show(ctx=ax[i,1])\n    \n#     print(img)","1c87cb72":"db = DataBlock(blocks=(TransformBlock([get_filenames, open_ms_tif, lambda x: x\/10000]), \n                       TransformBlock([partial(map_filename, str1='red', str2='gt'), \n                                       partial(open_tif, cls=TensorMask)])),\n               splitter=RandomSplitter(valid_pct=0.2),\n               item_tfms=aug,\n              )\n\ndl = db.dataloaders(items, bs=12)\n\n# check if it is being applied correctly\n_, ax = plt.subplots(2, 4, figsize=(16, 8))\n\nfor i in range(0, 4):\n    img, mask = dl.do_item(idx)\n\n    show_img(img, ctx=ax[0,i])\n    mask.show(ctx=ax[1,i])","17f8f713":"def acc_metric(input, target):\n    target = target.squeeze(1)\n    return (input.argmax(dim=1)==target).float().mean()\n\ndef loss_fn(pred, targ):\n    targ[targ==255] = 1\n    return torch.nn.functional.cross_entropy(pred, targ.squeeze(1).type(torch.long))\n\nlearn = unet_learner(dl, resnet18, n_in=4, n_out=2, pretrained=False, loss_func=loss_fn, metrics=acc_metric)","4ea4b3cb":"# At this notebook version we are starting from a previously saved checkpoint\n# We will then, load the previous weights and train for 5 more epochs\n\n# The final objective is to compare the final accuracy, with the accuracy introducing augmentations\ntry:\n    learn.load('\/kaggle\/input\/remotesensing-fastai2-multiband-augmentations\/models\/95_cloud-resnet18-50-35epochs_aug.learner')\n    print('Loaded sucessfully')\n    learn.fit_one_cycle(15, lr_max=1e-4, wd=1e-1)\n    learn.save('.\/95_cloud-resnet18-50-35epochs_aug.learner')\nexcept:\n    \n    print('failed loading checkpoint')\n    \n\n# learn.lr_find()","46d0ed6f":"# learn.lr_find()","2e91fc0e":"## Creating Dataset and Dataloader","be99b7d0":"In the medium story I created a new class MSTensorImage to keep basic functionalities of Fastai2. However, to keep things really simple here, I will just create a function tha opens the .TIF file (TensorImages doesn't support it) and another function to collate the 4 bands.","0d5708e7":"## Augmentations\nFor the augmentations, we will use the albumentations library that is already available in Kaggle.","170a57e4":"### Multi-spectral augmentations in Fastai2\n## It starts from 25 epochs pre-trained and goes to 50 epochs with augmentations\n\nThe objective of this notebook is to to implement augmentations in multi-spectral images and use them in Fastai v2.\nThis notebook is a continuation of https:\/\/www.kaggle.com\/cordmaur\/remotesensing-fastai2-simpletraining\/ which explains how to create a dataset and train a simple U-Net model using Fastai.\n\nThe cloud dataset is a good playground and the accuracies with and without augmentations will be assessed.\n\nThe in-depth explanation is provided at this medium story:\nhttps:\/\/medium.com\/@cordmaur\/how-to-implement-augmentations-for-multispectral-satellite-images-segmentation-using-fastai-v2-and-ea3965736d1\n\nIn this notebook we will see how to:\n- Implement data augmentation in multispectral images\n- Augment the image and the corresponding mask\n- Use the augmentation within the Fastai 2 api\n\nWe will not cover the data itself, because there is already a good EDA for this dataset:\nhttps:\/\/www.kaggle.com\/polavr\/cloud95-exploracion-de-datos\n\nSo, Let's go:\n\nAs fastai2 is not yet default in Kaggle, our first task is to install the library and its dependencies.\n","3e387a2f":"## Opening the images"}}