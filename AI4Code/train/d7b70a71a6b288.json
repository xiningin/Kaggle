{"cell_type":{"beac0c00":"code","ca312ebe":"code","68dcf290":"code","76fe6191":"code","ba8d8b05":"code","27833f7d":"code","7d8ba818":"code","4c52e4db":"code","8aa0dad2":"code","1341b07f":"code","a8d19dd0":"code","0bf3e41a":"code","1686e465":"code","2d953cde":"code","71008325":"code","be56ae28":"code","aa41fb7a":"code","facbfdfa":"code","08cbffdd":"code","bd32f59e":"code","662d809c":"code","87dca4ab":"code","81d03198":"code","22473c15":"code","384b79aa":"code","4b55cfad":"code","538bd66a":"code","61ff8e38":"code","1f83d142":"code","f77fd91e":"code","8fed9ff8":"code","c902a6d7":"code","407020d7":"code","4091bc93":"code","9ca5dabb":"code","5eedd73d":"code","f1b37e8d":"code","9df39615":"code","d2a08859":"code","fdb7751f":"code","6e6640d5":"code","53810f80":"markdown","1cef73fe":"markdown","68ea8d59":"markdown","40827060":"markdown","ff5e051f":"markdown","4d6a77e3":"markdown","ec3ac9ae":"markdown","b9cce3a5":"markdown","73d4b3f1":"markdown","5715c727":"markdown","8f521590":"markdown","94432d34":"markdown","cb77624a":"markdown","246fb9a7":"markdown","a380f52c":"markdown","74cf4440":"markdown","c328ad17":"markdown","883c0558":"markdown","73701582":"markdown"},"source":{"beac0c00":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca312ebe":"from statistics import mean\nimport matplotlib.pyplot as plt","68dcf290":"class GetDfForPreprocessing:\n    \"\"\"\n    this function will prepare data form dataframe for preprocessing\n    \n    Return\n    ------\n    dataframe\n    \"\"\"\n    def __init__(self, df:pd.DataFrame):\n        \n        self.df = df.copy()\n        \n    def print_df_info(self) -> None:\n        \"\"\"\n        this function will print the info of the datafame\n\n        Return\n        ------\n        None\n        \"\"\"\n        print('Retrieving info from data...')\n        #save the number of columns and names\n        col_info = 'The number of colum(s): {}.\\nThe column(s) is\/are : {} and {}\\n'.format(len(self.df.columns),', '.join(self.df.columns[:-2]), self.df.columns[-1])  \n        \n        #save the number of rows\n        num_rows = \"\\nThe total number of rows: {}\".format(len(self.df))\n        \n        na_cols = list(self.df.columns[self.df.isnull().any()])\n        \n        #save the number of missing values\n        num_na_cols = \"\\nThe number of columns having missing value(s): {}\".format(len(na_cols))\n        \n        #save the columns with missing value and the num of values missing\n        na_cols_num_na = ''\n        \n        na_col_val_dict = {}\n        for col in na_cols:\n            missing_vals = self.df[col].isnull().sum()\n            na_col_val_dict[col] = missing_vals\n            na_cols_num_na += \"\\nThe number of rows with missing value(s) in [{}]: {}\".format(col, missing_vals)\n        \n        # save the total number of missing values\n        tot_na = \"\\nThe total number of missing value(s): {}\".format(self.df.isnull().sum().sum())\n        \n        self.na_cols = na_cols\n        self.na_col_val_dict = na_col_val_dict\n        \n        print(col_info, num_rows, num_na_cols, na_cols_num_na)\n        \n        \n    def drop_cols_abv_na_trshld(self, threshold:float, exclude=[], output=False) -> pd.DataFrame:\n        \"\"\"\n        this function will drop columns with missing values above a specified threshold\n\n        Return\n        ------\n        dataframe\n        \"\"\"\n        print('\\nComparing threshold with fraction of missing values ...')\n        df = self.df.copy()\n        try:\n            if self.na_col_val_dict:\n                na_col_val_dict = self.na_col_val_dict\n                na_cols = self.na_cols\n        except:\n            na_cols = df.columns[df.isnull().any()]\n            na_col_val_dict = {}\n            for col in na_cols:\n                missing_vals = df[col].isnull().sum()\n                na_col_val_dict[col] = missing_vals\n            \n        tot_entries = len(df)\n        above_treshold = []\n        \n        print('\\nRetrieving columns to be dropped ...')\n        for col in na_cols:\n            if na_col_val_dict[col] > threshold * tot_entries:\n                above_treshold.append(col)\n                \n        print('\\nColumns to be dropped :', above_treshold)\n        print('\\nThe column(s) to be excluded is\/are {}'.format([exclude]))\n        if len(exclude)>0:\n            for col in exclude:\n                above_treshold.remove(col)\n                \n        print('\\nDropping columns with missing values above the threshold ...') \n        df.drop(above_treshold, axis=1, inplace=True)\n        print('\\nDropping columns completed')\n\n        print('\\nRemoving dropped columns from memory...')\n        for col in above_treshold:\n            na_cols.remove(col)\n            del na_col_val_dict[col]\n        print('\\nRemoval of dropped columns from memory completed')\n\n        self.na_col_val_dict = na_col_val_dict\n        self.na_cols = na_cols\n        self.df = df.copy()\n        if output:\n            return df\n\n    def fill_missing(self, exclude=['CompetitionOpenSinceMonth'], method={'num':'mean'}):\n        df = self.df.copy()\n        na_col_val_dict = self.na_col_val_dict\n        na_cols = self.na_cols\n        print('\\nThe colums with missing values to be filled are {}'.format(na_cols))\n        print('\\nThe column(s) to be excluded is\/are {}'.format(exclude))\n        \n        if len(exclude)>0:\n            for col in exclude:\n                na_cols.remove(col)\n\n        for col in na_cols:\n            print('\\nFilling missing values in {}'.format(col))\n            if(df[col].dtype == np.float64 or df[col].dtype == np.int64):\n                if method['num'] == 'mean':\n                    df[col].fillna(df[col].mean(), inplace=True)\n                else:\n                    df[col].fillna(df[col].median(), inplace=True)\n            else:\n                df[col].fillna(df[col].mode()[0], inplace=True)\n\n        print('\\nFilling missing values comppleted')\n        return df\n","76fe6191":"dist_df = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv')\nprod_df = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv')","ba8d8b05":"dist_df.head()","27833f7d":"prod_df.head()","7d8ba818":"prep_dist = GetDfForPreprocessing(dist_df)\nprep_dist.print_df_info()","4c52e4db":"a = list(dist_df['state'].unique())\nlen(a)","8aa0dad2":"dist_df['state'].mode()","1341b07f":"dist_st_notna = dist_df[dist_df['state'].notna()].copy()","a8d19dd0":"prep_ = GetDfForPreprocessing(dist_st_notna)\nprep_.print_df_info()","0bf3e41a":"prep_.drop_cols_abv_na_trshld(threshold=0.3)","1686e465":"dist_clean_df = prep_.fill_missing(exclude=[])","2d953cde":"dist_clean_df.head()","71008325":"dist_clean_df['locale'].unique()","be56ae28":"# define function to get average of range inputs\ndef avg(entry):\n    return mean([float(i) for i in entry.strip('[').split(',')])","aa41fb7a":"rnge_cols = dist_clean_df.columns[3:]  # retrieve columns with range values\n\nfor col in rnge_cols:\n    dist_clean_df[col] = dist_clean_df[col].apply(avg) # find the average of the reanges\ndist_clean_df.reset_index(drop=True, inplace=True) # reset the index","facbfdfa":"dist_clean_df.head()","08cbffdd":"from collections import defaultdict\n\np_esn_funct = defaultdict(set)\nfor e in prod_df['Primary Essential Function'].unique():\n    try:\n        temp = e.split(' - ')\n        p_esn_funct[temp[0]].add(temp[1])\n    except:\n        pass\n\nfor key in p_esn_funct.keys():\n    prod_df[key] = 0\n    for i in range(len(prod_df)):\n        try:\n            if key in prod_df['Primary Essential Function'][i]:\n                prod_df[key][i] = 1\n                \n        except:\n            pass\n\nsectors = set()\nfor e in prod_df['Sector(s)'].unique():\n    try:\n        if ';' in e:\n            temp = e.split('; ')\n            for i in temp:\n                sectors.add(i)\n        else:\n            sectors.add(i)\n    except:\n        pass\n\nfor key in list(sectors):\n    prod_df[key] = 0\n    for i in range(len(prod_df)):\n        try:\n            if key in prod_df['Sector(s)'][i]:\n                prod_df[key][i] = 1\n                \n        except:\n            pass\n","bd32f59e":"prod_df.dropna(inplace=True)","662d809c":"# get the filter to be used to extract data based on states\nfilters_dict = {}\nfor st in dist_clean_df['state']:\n    flt = dist_clean_df['state'] == st\n    filters_dict[st] = flt","87dca4ab":"# get the dictionary with data based on states\nState_df = {k:dist_clean_df[filters_dict[k]].copy() for k in filters_dict.keys()}","81d03198":"# Integrate data from engagement with district data\nState_df_full ={}   # to clear memory usage                            \nfor key in State_df.keys():\n    df = State_df[key]\n    for d_id, idx in zip(df['district_id'].values, df.index):\n        temp_df = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/'+ str(d_id) +'.csv', parse_dates=['time'])\n        temp_df['district_id'] = d_id\n        temp_df['month'] = temp_df['time'].apply(lambda x:x.month)\n        temp_df['weekday'] = temp_df['time'].apply(lambda x:x.weekday() + 1)\n        temp_df['locale'] = df['locale'][idx]\n        temp_df['state'] = key\n        temp_df['pct_black\/hispanic'] = df['pct_black\/hispanic'][idx]\n        temp_df['pct_free\/reduced'] = df['pct_free\/reduced'][idx]\n        temp_df['county_connections_ratio'] = df['county_connections_ratio'][idx]\n        temp_df.dropna(inplace=True)\n        if key in State_df_full.keys():\n            State_df_full[key] = pd.concat([temp_df,State_df_full[key]],ignore_index=True)\n        else:\n            State_df_full[key] = temp_df","22473c15":"# Integrate data from products with district and engagement data\nState_df={}    # to clear memory usage\nState_df_full_prod = {}\nfor key in State_df_full.keys():\n    State_df_full_prod[key] = State_df_full[key].merge(prod_df,left_on='lp_id',right_on='LP ID')","384b79aa":"# get the state and the locales('suburb','ciites',...) in each state {state:[locales]}\nstate_locale = {key:State_df_full_prod[key]['locale'].unique() for key in State_df_full_prod}","4b55cfad":"slice1=['LC','CM','SDO','LC\/CM\/SDO']","538bd66a":"def plot_viz(state, slice_col, about=''):\n    dfs = State_df_full_prod[state]\n    fig, ax = plt.subplots(figsize=(15, 10))\n    plt.rcParams.update({'axes.titlesize': 'Large'})\n\n    ax.tick_params(axis='x', labelsize=16)\n    ax.tick_params(axis='y', labelsize=16)\n    ax.set_title('Trends In '+ about + ' In ' + state, fontsize=22)\n    ax.grid(color='black', linestyle='--', linewidth=0.1)\n\n    dfs.groupby('time').sum()[slice_col].plot(ax=ax, grid=True)\n\n    locales = state_locale[state]\n\n    for locale in locales:\n        fig1, ax1 = plt.subplots(figsize=(15, 10))\n        plt.rcParams.update({'axes.titlesize': 'Large'})\n\n        ax1.tick_params(axis='x', labelsize=16)\n        ax1.tick_params(axis='y', labelsize=16)\n        \n        if locale == 'City':\n            ax1.set_title('Trends In ' + about + ' In ' + 'Cities In ' + state, fontsize=22)\n        else:\n            ax1.set_title('Trends In ' + about + ' In ' + locale + 's In ' + state, fontsize=22)\n        \n        ax1.grid(color='black', linestyle='--', linewidth=0.1)\n\n        mask = dfs['locale'] == locale\n        dfs[mask].groupby('time').sum()[slice_col].plot(ax=ax1, grid=True)","61ff8e38":"def plot_top():\n    pass","1f83d142":"def plot_top(state, slice_col,top=10, about=''):\n    dfs = State_df_full_prod[state]\n    fig, ax = plt.subplots(figsize=(15, 10))\n    plt.rcParams.update({'axes.titlesize': 'Large'})\n\n    ax.tick_params(axis='x', labelsize=16)\n    ax.tick_params(axis='y', labelsize=16)\n    ax.set_title('Trends '+ about + ' In ' + state, fontsize=22)\n    ax.grid(color='black', linestyle='--', linewidth=0.1)\n\n    dfs[slice_col].value_counts().head(top).plot(grid=True, kind='bar', ax=ax)\n\n    locales = state_locale[state]\n\n    for locale in locales:\n        fig1, ax1 = plt.subplots(figsize=(15, 10))\n        plt.rcParams.update({'axes.titlesize': 'Large'})\n\n        ax1.tick_params(axis='x', labelsize=16)\n        ax1.tick_params(axis='y', labelsize=16)\n        \n        if locale == 'City':\n            ax1.set_title('Trends In ' + about + ' In ' + 'Cities In ' + state, fontsize=22)\n        else:\n            ax1.set_title('Trends In ' + about + ' In ' + locale + 's In ' + state, fontsize=22)\n        \n        ax1.grid(color='black', linestyle='--', linewidth=0.1)\n\n        mask = dfs['locale'] == locale\n        dfs[mask][slice_col].value_counts().head(top).plot(ax=ax1, grid=True, kind='bar')","f77fd91e":"states = list(state_locale.keys())","8fed9ff8":"len(states)","c902a6d7":"plot_viz(state=states[0], slice_col=slice1, about='Product Type Usage')","407020d7":"plot_viz(state=states[0], slice_col=['engagement_index'], about='Engagement')","4091bc93":"plot_top(state=states[0], slice_col=['Product Name'],top=10, about='Specific Product Usage')","9ca5dabb":"plot_top(state=states[0], slice_col=['Provider\/Company Name'],top=10, about='Specific Product Providers')","5eedd73d":"plot_viz(state=states[1], slice_col=slice1, about='Product Type Usage') \nplot_viz(state=states[1], slice_col=['engagement_index'], about='Engagement')\nplot_top(state=states[1], slice_col=['Product Name'],top=10, about='Specific Product Usage')\nplot_top(state=states[1], slice_col=['Provider\/Company Name'],top=10, about='Specific Product Providers')","f1b37e8d":"plot_viz(state=states[2], slice_col=slice1, about='Product Type Usage') \nplot_viz(state=states[2], slice_col=['engagement_index'], about='Engagement')\nplot_top(state=states[2], slice_col=['Product Name'],top=10, about='Specific Product Usage')\nplot_top(state=states[2], slice_col=['Provider\/Company Name'],top=10, about='Specific Product Providers')","9df39615":"plot_viz(state=states[3], slice_col=slice1, about='Product Type Usage') \nplot_viz(state=states[3], slice_col=['engagement_index'], about='Engagement')\nplot_top(state=states[3], slice_col=['Product Name'],top=10, about='Specific Product Usage')\nplot_top(state=states[3], slice_col=['Provider\/Company Name'],top=10, about='Specific Product Providers')","d2a08859":"plot_viz(state=states[4], slice_col=slice1, about='Product Type Usage') \nplot_viz(state=states[4], slice_col=['engagement_index'], about='Engagement')\nplot_top(state=states[4], slice_col=['Product Name'],top=10, about='Specific Product Usage')\nplot_top(state=states[4], slice_col=['Provider\/Company Name'],top=10, about='Specific Product Providers')","fdb7751f":"plot_viz(state=states[5], slice_col=slice1, about='Product Type Usage') \nplot_viz(state=states[5], slice_col=['engagement_index'], about='Engagement')\nplot_top(state=states[5], slice_col=['Product Name'],top=10, about='Specific Product Usage')\nplot_top(state=states[5], slice_col=['Provider\/Company Name'],top=10, about='Specific Product Providers')","6e6640d5":"plot_viz(state=states[5], slice_col=slice1, about='Product Type Usage') \nplot_viz(state=states[5], slice_col=['engagement_index'], about='Engagement')\nplot_top(state=states[5], slice_col=['Product Name'],top=10, about='Specific Product Usage')\nplot_top(state=states[5], slice_col=['Provider\/Company Name'],top=10, about='Specific Product Providers')","53810f80":"### Products are labeled as one of these three categories: LC = Learning & Curriculum, CM = Classroom Management, and SDO = School & District Operations","1cef73fe":"### Create A Dictionary with states as keys and DataFrames as values","68ea8d59":"### Build class for data preprocessing","40827060":"There are 23 states in the district data frame. The most occuring state is Connecticut","ff5e051f":"## State of Utah\nLC products were the most used in the state. There was a steady rise in usage from January to April. Usage ranged between 800 and 3600 from January to April. March, Aprol and May also recorded a steady rise with localised decline in April. The usage in this period ranged between 1800 and 3900.\nThere was a steady decline from May to June and a very steep decline from May to June. The lowest usage (700)   was recorded from July to August. The usage remained stable at low values from July to August with values ranging between 700 and 1100. \nA steep rise was recorded from August to September and a gentle rise from September to October. There was a drop in usage in the middle of September followed by a rise which led to the highest usage all year in October. In general, the usage from September to December was the highest. \nSDO was the second most used product type from January to April. April to May saw CM catching up with SDO usage and doing slightly better from May to June. Between June to August where the usage of both products declined and remained generally the same. From August to September, there was a rise in usage of SDO and CM. In the remaining months in the year, CM performed better than SDO.\n### Locales\nThere was data for districts in Towns, Cities, Rurals and Suburbs.\nThe product usage in **cities** followed the trends seen in the state. LC peaked at 500, CM and SDO both peaked at 80. The lowest usage of LC was about 20 which is less than the highest usage of CM and SDO. This trend was not seen in the state.\nThe product usage in **towns** followed the trends seen in the state. LC peaked at 1100, CM and SDO both peaked at 160. The lowest usage of LC was about 80 which is less than the highest usage of CM and SDO. This trend was not seen in the state.\nThe product usage in **rurals** followed the trends seen in the state. LC peaked at 275, CM and SDO both peaked at 45. The lowest usage of LC was about 0 which coincided with the lowest usage of CM and SDO. This trend was not seen in the state.\nThe product usage in **suburbs** followed the trends seen in the state. LC peaked at 2400, CM and SDO both peaked at 350.\n## The covid effect\nFrom the trends before the closure and during the closure of schools indicates increment in the use of products during the closure period. As expected, Covid increased the usage of products in the state of Utah and further implies greater reach through products.\n\n","4d6a77e3":"### engagement_index : sum of Total page-load events per one thousand students of a given product and on a given day\n### Products are labeled as one of these three categories: LC = Learning & Curriculum, CM = Classroom Management, and SDO = School & District Operations","ec3ac9ae":"### Initial questions :\n**1. What are the trends in the type of product used, engagements, top products, top companies?**\n\n**2. What are the trends in the various locales of each state?**\n\n**3. What are the trends in the states taking all locales?**","b9cce3a5":"### Read Data","73d4b3f1":"### Fill missing values with the mode of each column","5715c727":"### Process District data","8f521590":"### Clean District data","94432d34":"### Drop clumn with missing values more than 30% of the total rows","cb77624a":"### Data has been processed and merged in a way to enable analysis on monthly, daily and weekly basis. It is also in a format where analysis can be made on state by state with breakdown for locales in states. This will enable analyst to link performance of states in chosen parameters and recommend those that worked and those that need little adjustments. The data is saed in dictionaries with keys being names of sates and values being DataFrames of data specific to the name of the state being the key.","246fb9a7":"### Drop null values in ['state'] column","a380f52c":"## State of Illinois\nThe LC product type was the most used in the state. Between the periods of January and the end of February the usage for LC products alternated between 1500 and 2100. This period precedes the closure of schools during the spring (March, April, May).\nDuring the Spring Period there was an increment in usage relative to the period from January to the end of February. The usage ranged between 1500 and 3000 with April recording usage between 1800 and 2200.\nThere was a declined in usage from June to August. This period recorded the lowest usage all year with values ranging between 900 and 1200. August to December saw a rise in usage. There was a steep rise from August to September. The highest usage values were recorded between August and December.\nThe usage of CM and SDO product types were generally at par. The trends mirrored that of the LC types but with a lower usage. The usage for the two did not exceed 500.\nSDO type was having slightly more usage than CM from January to march. CM usage caught up with SDO usage from April to September beyond which it gained slightly more usage than SDO.\n### Locales\nThe plot of the **Suburbs** mirrors the plot for the state. This indicates that the suburb districts are the major driving force of the usage in the state. The usage of LC peaks at 2700. That of CM and SDO both peaks at an approximate value of 400\nThe **rural** districts also mirror the trends of the state in general but have LC usage peaking at 600 and both CM and SDO at approximately 100\nThe trends in the **town** districts are a little bit different. Data for the usage starts from March 2020 to January 2021. The usage difference between LC and the other types is much closer than what is observed in the other locales. The profile (horizontal trends) follows that of the state. The usage of LC peaks at approximately 75. That of CM and SDO both peaks at approximately 10.\n\n## The covid effect\nFrom the trends before the closure and during the closure of schools indicates increment in the use of products during the closure period. As expected, Covid increased the usage of products in the state of Illinois and further implies greater reach through products.","74cf4440":"### Get insight about states","c328ad17":"### Drop null values in product data","883c0558":"### Extract info from ['Primary Essential Function'] column","73701582":"### Visualize trends in product type usuage by states"}}