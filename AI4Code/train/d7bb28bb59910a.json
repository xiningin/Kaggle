{"cell_type":{"a8d499ec":"code","5053b849":"code","072ad84f":"code","501213f2":"code","6aebd3db":"code","9eff1be9":"code","45811bab":"code","d1a44d32":"code","3481b9af":"code","5fddcb05":"code","0fe5ca37":"code","495bf818":"code","8c9c27b9":"code","c9d3c34c":"code","c01052f6":"code","62f6465a":"code","8f821c9d":"code","6cad5110":"code","1b5ae55f":"code","19012699":"code","21ca5ab6":"code","42dd50a2":"code","4e0dbd28":"code","574214b1":"code","47112b6e":"code","82da2233":"code","b11f1be0":"code","03972a95":"code","c5b096f7":"code","f294f155":"code","27f593c8":"code","f8a63afb":"code","596063e6":"code","4dff969b":"code","ad277a2a":"code","1f6a45bd":"code","529b697b":"code","b7d52e51":"code","29777228":"code","85a50b6e":"code","b425f25d":"code","0abfb785":"code","2918d415":"code","ae2243e8":"code","1f45b369":"code","e70bd412":"code","5fb89044":"code","a27cbc4d":"code","4cb13ca5":"code","dbfeff97":"code","63fa9a69":"code","2a9f327e":"code","c0c29bf5":"code","37ccb9e3":"code","f906692b":"code","3235e6f1":"code","82588422":"code","d7174c94":"code","0e68d5e9":"code","9d9dbf8b":"code","5280e96b":"code","78cd1c4d":"code","48e5eafa":"code","f19a89a0":"code","5d4b8c20":"code","befc9485":"code","91e1138f":"code","baa6f59f":"markdown","cbe68c78":"markdown","9feadd61":"markdown","862c88ba":"markdown","501fbcba":"markdown","a92e39cb":"markdown","e5e9e294":"markdown","4aee1921":"markdown","2e8a6cf3":"markdown","39577ab2":"markdown","36c00071":"markdown","38ba7adf":"markdown","596c6c98":"markdown","1c7c8495":"markdown","13455c35":"markdown","78b7403f":"markdown","66e79aca":"markdown","6b54a812":"markdown","37911a63":"markdown","9f886f0d":"markdown","931e8ef7":"markdown","a801dfd2":"markdown","7796998d":"markdown","dec8b201":"markdown","a0fc64a4":"markdown","0587a57c":"markdown","e8de7328":"markdown","2801382c":"markdown","1cfeea27":"markdown","7415ca0e":"markdown","6ae2c8d9":"markdown","8b721e66":"markdown","49df4359":"markdown"},"source":{"a8d499ec":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')","5053b849":"train = pd.read_csv('\/kaggle\/input\/telstra-recruiting-network\/train.csv.zip')\ntest = pd.read_csv('\/kaggle\/input\/telstra-recruiting-network\/test.csv.zip')\nfeature = pd.read_csv('\/kaggle\/input\/telstra-recruiting-network\/log_feature.csv.zip')\nevent = pd.read_csv('\/kaggle\/input\/telstra-recruiting-network\/event_type.csv.zip')\nresource = pd.read_csv('\/kaggle\/input\/telstra-recruiting-network\/resource_type.csv.zip')\nseverity = pd.read_csv('\/kaggle\/input\/telstra-recruiting-network\/severity_type.csv.zip')","072ad84f":"df_list = [train,test,feature,event,resource,severity]\nfor df in df_list:\n    print(df.columns[-1],':',len(df))","501213f2":"# for the training data\nmerge_1 = pd.merge(train,feature) \nmerge_2 = pd.merge(merge_1,event) \nmerge_3 = pd.merge(merge_2,resource) \nmerge_4 = pd.merge(merge_3,severity) \nprint(merge_4.shape)","6aebd3db":"merge_4.isna().sum(axis=0)","9eff1be9":"merge_4.head(5)","45811bab":"merge_4.drop_duplicates(subset = 'id', inplace = True) \nmerge_4.shape","d1a44d32":"train = merge_4.set_index(merge_4.id).drop('id',axis = 1)\ntrain.head(5)","3481b9af":"# for the testing data \nmerge_5 = pd.merge(test,feature) \nmerge_6 = pd.merge(merge_5,event) \nmerge_7 = pd.merge(merge_6,resource) \nmerge_8 = pd.merge(merge_7,severity) \nprint(merge_8.shape)","5fddcb05":"merge_8.isna().sum(axis=0)","0fe5ca37":"merge_8.head(5)","495bf818":"merge_8.drop_duplicates(subset = 'id', inplace = True) \nprint(merge_8.shape)","8c9c27b9":"merge_8.head(5)","c9d3c34c":"test = merge_8.set_index(merge_8.id).drop('id',axis = 1)\ntest.head(5)","c01052f6":"train.head(5)","62f6465a":"# remove duplicate prefixes for the trainning set \n# the need for category features of the lgb algorithm\nremoval = train.iloc[:,[0,2,4,5,6]].apply(lambda i:i.apply(lambda x:x.replace(x,x.split(' ')[-1])))\nremoval.head()","8f821c9d":"removal.dtypes","6cad5110":"train = pd.concat([removal,train.iloc[:,[3,1]]],axis = 1)\ntrain.head()","1b5ae55f":"# remove duplicate prefixes for the testing set\nremoval = test.iloc[:,[0,1,3,4,5]].apply(lambda i:i.apply(lambda x:x.replace(x,x.split(' ')[-1])))\nremoval.head()","19012699":"removal.dtypes","21ca5ab6":"test = pd.concat([removal,test.iloc[:,2]],axis = 1)\ntest.head(5)","42dd50a2":"train.info()","4e0dbd28":"train.dtypes","574214b1":"# plot the pie chart for the target variables\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(3,3))\nplt.pie(x = train.fault_severity.value_counts().values,\n        labels = train.fault_severity.value_counts().index,\n        colors = ('aliceblue','lightsteelblue','pink'),autopct = \"%.2f%%\")\nplt.title('fault_severity')\nplt.legend() \nplt.show()","47112b6e":"# plot the bar_chart for the less-level categorical variables\nimport seaborn as sns\nless_level = train.iloc[:,[3,4,6]]\nplt.figure(figsize=(15,5))\ncount = 1\nfor col in less_level.columns[:-1]:\n    plt.subplot(1,2,count)\n    temp = pd.crosstab(less_level.fault_severity,less_level[col])\n    temp1 = temp.T.stack().reset_index()\n    sns.barplot(temp1[col], temp1[0], hue = temp1.fault_severity, palette='PuBu')\n    count += 1","82da2233":"# plot the histograms for the numerical variable\nplt.figure(figsize = (5,5))\nsns.distplot(train[train.fault_severity == 0]['volume'], kde=False, label='NoFault', bins=3)\nsns.distplot(train[train.fault_severity == 1]['volume'], kde=False, label='Several Faults', bins=3)\nsns.distplot(train[train.fault_severity == 2]['volume'], kde=False, label='Serious Faults', bins=3)\nplt.legend()","b11f1be0":"train.isna().sum(axis=0)","03972a95":"test.isna().sum(axis=0)","c5b096f7":"sns.boxplot( x = train.volume,orient = 'v',palette = 'PuBu')","f294f155":"from scipy import stats\nx = np.abs(stats.zscore(train.volume)) < 3\ntrain.volume = np.where(x,train.volume,np.nan)\ntrain.isna().sum(axis=0)","27f593c8":"train.volume = train.volume.replace(np.nan, train.volume.sum()\/(len(train.volume) - (train.volume.isna()).sum()))\ntrain.isna().sum(axis=0)","f8a63afb":"# although the large range didn't affect the performance of the tree model, we conduct the detection.\ntrain.describe()","596063e6":"train.skew()\n# the distribution of the input variables didn't affect the performance of the tree model.\n# no numerical variable is highly skewed(skewness>10).","4dff969b":"plt.figure(figsize = (3, 3))\nsns.heatmap(train.corr(), annot = True, vmax=1, vmin=-1, cmap='YlGnBu_r')\nplt.show()\n# the muliticollineity doesn't affect the performance of the boosting tree.","ad277a2a":"train.dtypes","1f6a45bd":"train.iloc[:,0:5] = train.iloc[:,0:5].astype('category')\ntrain.dtypes","529b697b":"from sklearn.model_selection import train_test_split\nx = train.iloc[:,0:-1]\ny = train.fault_severity\nx_train, x_val, y_train, y_val= train_test_split(x, y, test_size = 0.2, random_state = 1)","b7d52e51":"from collections import Counter\nCounter(y_train)\ncount_y = Counter(y_train)\nplt.figure(figsize=(3,3))\nplt.pie(x = count_y.values(), labels = count_y.keys(), \n        colors = ('lightsteelblue','aliceblue','pink'), autopct = \"%.2f%%\")\nplt.title('Class for training data')\nplt.legend() \nplt.show()","29777228":"%%time\nfrom lightgbm import LGBMClassifier as lgbc\nfrom catboost import CatBoostClassifier as cbc\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.preprocessing import OneHotEncoder\n\ngbm_b = lgbc(objective = 'multiclass', max_depth = 6, reg_lambda = 3.0, random_state = 1)\ngbm_b = gbm_b.fit(X = x_train, y = y_train, eval_set = (x_val,y_val), early_stopping_rounds = 50, verbose = 10)\nprint('\\t')\nprint('bestTest = ',gbm_b.best_score_['valid_0']['multi_logloss'])\nprint('bestIteration = ',gbm_b.best_iteration_)\nprint('\\t')\n\ncat_b = cbc(objective = 'MultiClass', learning_rate = 0.1, n_estimators = 100, random_state = 1)\ncat_b = cat_b.fit(X = x_train, y = y_train, eval_set = (x_val,y_val), \n                 cat_features = np.where(x_train.dtypes != np.float)[0], \n                 early_stopping_rounds = 50, verbose = 10)\n\nensembles = [gbm_b,cat_b]\nTRAIN_ACC = []\nVAL_ACC = []\nMulti_Logloss = []\nfor model in ensembles:\n    train_acc = model.score(x_train,y_train)\n    TRAIN_ACC.append(train_acc)\n    \n    y_pred = model.predict(x_val)\n    \n    val_acc = accuracy_score(y_val,y_pred)\n    VAL_ACC.append(val_acc)\n    \n    y_pred = y_pred.reshape(-1,1)\n    y_true = np.array(y_val).reshape(-1,1)\n    one_hot = OneHotEncoder(sparse = False)\n    y_true = one_hot.fit_transform(y_true)\n    y_pred = one_hot.fit_transform(y_pred)\n    multi_logloss = log_loss(y_true, y_pred)\n    Multi_Logloss.append(multi_logloss)\n    \nind=['train_acc','val_acc','multi_logloss']\ncol=['gbm_b','cat_b']\nsummary=pd.DataFrame(np.vstack((TRAIN_ACC,VAL_ACC,Multi_Logloss)),columns = col,index = ind)\nprint(summary)","85a50b6e":"import lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\ntrain_set = lgb.Dataset(data = x_train, label = y_train)\ndef lgb_eval(learning_rate, n_estimators, max_depth, num_leaves, min_data_in_leaf, bagging_fraction, bagging_freq, feature_fraction,\n             lambda_l1, lambda_l2):\n    params = {'objective': 'multiclass', 'num_class': 3, 'learning_rate': 0.1, \n              'seed': 1, 'force_col_wise': True, 'feature_pre_filter': False, 'verbose' : -1 }\n    params['learning_rate'] = learning_rate\n    params['n_estimators'] = round(n_estimators)\n    params['max_depth'] = round(max_depth)\n    params['num_leaves'] = round(num_leaves)\n    params['min_data_in_leaf'] = round(min_data_in_leaf)\n    params['bagging_freq'] = round(bagging_freq)\n    params['bagging_fraction'] = min(bagging_fraction,1.0)\n    params['feature_fraction'] = min(feature_fraction,1.0)\n    params['lambda_l1'] = lambda_l1\n    params['lambda_l2'] = lambda_l2\n    cv_result = lgb.cv(params, train_set, nfold = 5, early_stopping_rounds = 50, \n                       verbose_eval = 50, eval_train_metric = True)\n    return -(min(cv_result['valid multi_logloss-mean']))\n\nlgb_BO_1 = BayesianOptimization(lgb_eval,     \n    {'learning_rate': (0.05,0.2),\n     'n_estimators': (10,500),\n     'max_depth': (3,8),\n     'max_depth': (3,8),\n     'num_leaves': (7, 255),\n     'min_data_in_leaf': (18,22),\n     'bagging_fraction':(0.8,1),\n     'bagging_freq':(1,5),\n     'feature_fraction': (0.8,1),\n     'lambda_l1': (0.1,3), \n     'lambda_l2': (0.1,3)\n}, random_state = 1)\nlgb_BO_1.maximize()\nlgb_BO_1.max","b425f25d":"print('the multi-logloss improvement:', gbm_b.best_score_['valid_0']['multi_logloss'] - abs(lgb_BO_1.max['target']))","0abfb785":"# lr = 0.1, n = 5000\ndef lgb_eval(max_depth, num_leaves, min_data_in_leaf, bagging_fraction, bagging_freq, feature_fraction,\n             lambda_l1, lambda_l2):\n    params = {'objective': 'multiclass', 'num_class': 3, 'seed': 1,\n              'learning_rate': 0.1,  'force_col_wise': True, 'feature_pre_filter': False, 'verbose' : -1 }\n    params['max_depth'] = round(max_depth)\n    params['num_leaves'] = round(num_leaves)\n    params['min_data_in_leaf'] = round(min_data_in_leaf)\n    params['bagging_freq'] = round(bagging_freq)\n    params['bagging_fraction'] = min(bagging_fraction,1.0)\n    params['feature_fraction'] = min(feature_fraction,1.0)\n    params['lambda_l1'] = lambda_l1\n    params['lambda_l2'] = lambda_l2\n    cv_result = lgb.cv(params, train_set, nfold = 5, num_boost_round = 5000, early_stopping_rounds = 50, \n                       verbose_eval = 50, eval_train_metric = True)\n    return -(min(cv_result['valid multi_logloss-mean']))\n\nlgb_BO_2 = BayesianOptimization(lgb_eval,     \n    {'max_depth': (3,8),\n     'num_leaves': (7, 255),\n     'min_data_in_leaf': (18,22),\n     'bagging_fraction':(0.8,1),\n     'bagging_freq':(1,5),\n     'feature_fraction': (0.8,1),\n     'lambda_l1': (0.1,3), \n     'lambda_l2': (0.1,3)\n}, random_state = 1)\n\nlgb_BO_2.maximize()\nlgb_BO_2.max","2918d415":"%%time\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfolds = KFold(n_splits= 5, shuffle= True, random_state= 1)\nparams = {'learning_rate':[0.005,0.01,0.05,0.08,0.1,0.2]}\ngbm_lr = lgbc(objective = 'multiclass', random_state = 1, \n              num_leaves = 220, max_depth = 8, min_data_in_leaf = 21, bagging_freq = 5, \n              bagging_fraction = 1.0, feature_fraction = 0.8,\n              lambda_l1 = 0.3547216179898903, lambda_l2 = 3.0)\ngs_lr = GridSearchCV(gbm_lr, params, scoring = 'neg_log_loss', cv = folds, n_jobs = -1, verbose = 2, return_train_score = True )\ngs_lr.fit(x_train,y_train) \nprint(gs_lr.best_params_,gs_lr.best_score_)\ngs_lr_results = pd.DataFrame(gs_lr.cv_results_)\ngs_lr_scores = gs_lr_results[['param_learning_rate','mean_test_score','mean_train_score']]\ngs_lr_scores ","ae2243e8":"%%time\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 1)\nparams = {'n_estimators':range(10,201,10)}  \ngbm_n = lgbc(objective = 'multiclass', learning_rate = gs_lr.best_params_['learning_rate'],random_state = 1, \n             num_leaves = 220, max_depth = 8, min_data_in_leaf = 21, bagging_freq = 5, \n             bagging_fraction = 1.0, feature_fraction = 0.8,\n             lambda_l1 = 0.3547216179898903, lambda_l2 = 3.0)\ngs_n = GridSearchCV(gbm_n, params, scoring = 'neg_log_loss', cv = folds, n_jobs = -1, verbose = 2, return_train_score = True )\ngs_n.fit(x_train,y_train) \nprint(gs_n.best_params_,gs_n.best_score_)\ngs_n_results = pd.DataFrame(gs_n.cv_results_)\ngs_n_scores = gs_n_results[['param_n_estimators','mean_test_score','mean_train_score']]\ngs_n_scores ","1f45b369":"%%time\nfolds = KFold(n_splits= 5, shuffle= True, random_state= 1)\nparams = {'n_estimators':range(70,90,1)}\ngbm_n = lgbc(objective = 'multiclass', learning_rate = gs_lr.best_params_['learning_rate'],random_state = 1, \n             num_leaves = 220, max_depth = 8, min_data_in_leaf = 21, bagging_freq = 5, \n             bagging_fraction = 1.0, feature_fraction = 0.8,\n             lambda_l1 = 0.3547216179898903, lambda_l2 = 3.0)\ngs_n = GridSearchCV(gbm_n, params, scoring = 'neg_log_loss', cv = folds, n_jobs = -1, verbose = 2, return_train_score = True )\ngs_n.fit(x_train,y_train) \nprint(gs_n.best_params_,gs_n.best_score_)\ngs_n_results = pd.DataFrame(gs_n.cv_results_)\ngs_n_scores = gs_n_results[['param_n_estimators','mean_test_score','mean_train_score']]\ngs_n_scores ","e70bd412":"params = {'objective': 'multiclass', 'num_class': 3, 'seed': 1,\n          'force_col_wise': True, 'feature_pre_filter': False, 'verbose' : -1,\n          'learning_rate': gs_lr.best_params_['learning_rate'], 'num_leaves': 220, 'max_depth': 8, \n          'min_data_in_leaf': 21, 'bagging_freq': 5, \n          'bagging_fraction': 1.0, 'feature_fraction': 0.8,  \n          'lambda_l1': 0.3547216179898903, 'lambda_l2': 3.0}\ncv_results = lgb.cv(params, train_set, nfold = 5, num_boost_round = 5000, early_stopping_rounds = 50, \n                    verbose_eval = 50, eval_train_metric = True)\ncv_summary = pd.DataFrame(cv_results)\nprint('best n_estimators:', cv_summary.shape[0])\nprint('best val_logloss score:', cv_summary.iloc[-1,2])","5fb89044":"plt.figure(figsize=(20,5))\nplt.plot(range(1,cv_summary.shape[0]+1),cv_summary.iloc[:,0],color='lightsteelblue',label='train-logloss')\nplt.plot(range(1,cv_summary.shape[0]+1),cv_summary.iloc[:,2],color='pink',label='val-logloss')\nplt.legend()\nplt.show()","a27cbc4d":"# summary the performance of the lgb\nind = ['lgb_baseline','lgb_BO','lgb_BO_GSCV','lgb_BO_GSCV_CV']\ncol = ['multi-logloss']\nmulti_logloss_lgb = [gbm_b.best_score_['valid_0']['multi_logloss'], abs(lgb_BO_1.max['target']), \n                     abs(gs_n.best_score_), cv_summary.iloc[-1,2]]\nsummary_lgb = pd.DataFrame(multi_logloss_lgb,columns = col,index = ind)\nprint(summary_lgb)","4cb13ca5":"# boosting_type = Ordered\u3001auto_class_weights = Balanced\nimport catboost as cb\ntrain_pool = cb.Pool(data = x_train, label = y_train, cat_features = np.where(x_train.dtypes != np.float)[0])\ndef cb_eval(learning_rate, n_estimators, max_depth, reg_lambda):\n    params = {'objective': 'MultiClass', \n              'boosting_type': 'Ordered', 'auto_class_weights': 'Balanced', \n              'random_state': 1 }\n    params['learning_rate'] = learning_rate\n    params['n_estimators'] = round(n_estimators)\n    params['max_depth'] = round(max_depth)\n    params['reg_lambda'] = reg_lambda\n    cv_result = cb.cv(pool = train_pool, params = params, nfold = 5, \n                      early_stopping_rounds = 50, verbose = 50)\n    return -(min(cv_result['test-MultiClass-mean']))\n        \ncb_BO_1 = BayesianOptimization(cb_eval,     \n                             {'learning_rate': (0.05,0.2),\n                              'n_estimators': (10,500),\n                              'max_depth': (4,10),\n                              'reg_lambda': (0.1,3)}, random_state = 1)\ncb_BO_1.maximize(init_points = 5, n_iter = 5)\ncb_BO_1.max\n# 0.706790767154976","dbfeff97":"# boosting_type = 'Plain'\u3001auto_class_weights = Balanced\ndef cb_eval(learning_rate, n_estimators, max_depth, reg_lambda):\n    params = {'objective': 'MultiClass', 'auto_class_weights': 'Balanced', 'random_state': 1 }\n    params['learning_rate'] = learning_rate\n    params['n_estimators'] = round(n_estimators)\n    params['max_depth'] = round(max_depth)\n    params['reg_lambda'] = reg_lambda\n    cv_result = cb.cv(pool = train_pool, params = params, nfold = 5, \n                      early_stopping_rounds = 50, verbose = 50)\n    return -(min(cv_result['test-MultiClass-mean']))\n        \ncb_BO_2 = BayesianOptimization(cb_eval,     \n                             {'learning_rate': (0.05,0.2),\n                              'n_estimators': (10,500),\n                              'max_depth': (4,10),\n                              'reg_lambda': (0.1,3)}, random_state = 1)\ncb_BO_2.maximize(init_points = 5, n_iter = 5)\ncb_BO_2.max\n# 0.7092479497327548","63fa9a69":"# boosting_type = Ordered, auto_class_weights = None = 1\ndef cb_eval(learning_rate, n_estimators, max_depth, reg_lambda):\n    params = {'objective': 'MultiClass', 'boosting_type': 'Ordered', 'random_state': 1 }\n    params['learning_rate'] = learning_rate\n    params['n_estimators'] = round(n_estimators)\n    params['max_depth'] = round(max_depth)\n    params['reg_lambda'] = reg_lambda\n    cv_result = cb.cv(pool = train_pool, params = params, nfold = 5, \n                      early_stopping_rounds = 50, verbose = 50)\n    return -(min(cv_result['test-MultiClass-mean']))\n        \ncb_BO_3 = BayesianOptimization(cb_eval,     \n                             {'learning_rate': (0.05,0.2),\n                              'n_estimators': (10,500),\n                              'max_depth': (4,10),\n                              'reg_lambda': (0.1,3)}, random_state = 1)\ncb_BO_3.maximize(init_points = 5, n_iter = 5)\ncb_BO_3.max\n# 0.6009923381161436","2a9f327e":"# Keeps the number of optimizations and searches consistent with using LGBM\ndef cb_eval(learning_rate, n_estimators, max_depth, reg_lambda):\n    params = {'objective': 'MultiClass', 'boosting_type': 'Ordered', 'random_state': 1 }\n    params['learning_rate'] = learning_rate\n    params['n_estimators'] = round(n_estimators)\n    params['max_depth'] = round(max_depth)\n    params['reg_lambda'] = reg_lambda\n    cv_result = cb.cv(pool = train_pool, params = params, nfold = 5, \n                      early_stopping_rounds = 50, verbose = 50)\n    return -(min(cv_result['test-MultiClass-mean']))\n        \ncb_BO_4 = BayesianOptimization(cb_eval,     \n                             {'learning_rate': (0.05,0.2),\n                              'n_estimators': (10,500),\n                              'max_depth': (4,10),\n                              'reg_lambda': (0.1,3)}, random_state = 1)\ncb_BO_4.maximize(init_points = 15,n_iter = 15)\ncb_BO_4.max","c0c29bf5":"ind = ['lgb_baseline','cb_baseline','lgb_BO','lgb_BO_GSCV','lgb_BO_GSCV_CV','cb_BO_1', 'cb_BO_2', 'cb_BO_3', 'cb_BO_4']\ncol = ['tuned-multi-logloss']\ntuned_multi_logloss = [gbm_b.best_score_['valid_0']['multi_logloss'],cat_b.best_score_['validation']['MultiClass'],\n                       abs(lgb_BO_1.max['target']), abs(gs_n.best_score_), cv_summary.iloc[-1,2] ,\n                       abs(cb_BO_1.max['target']), abs(cb_BO_2.max['target']), abs(cb_BO_3.max['target']), \n                       abs(cb_BO_4.max['target'])]\ntuning_records = pd.DataFrame(tuned_multi_logloss,columns = col,index = ind)\nprint(tuning_records)","37ccb9e3":"cat_f = cbc(objective = 'MultiClass', learning_rate = 0.05716226472904498, n_estimators = 292,\n            max_depth = 9, reg_lambda = 1.4979851182199662, random_state = 1)\ncat_f = cat_f.fit(X = train_pool, verbose = 10)\ntrain_acc_f = cat_f.score(x_train,y_train)\ny_pred = cat_f.predict(x_val)\nval_acc_f = accuracy_score(y_val,y_pred)\ny_pred = y_pred.reshape(-1,1)\ny_true = np.array(y_val).reshape(-1,1)\none_hot = OneHotEncoder(sparse = False)\ny_true = one_hot.fit_transform(y_true)\ny_pred = one_hot.fit_transform(y_pred)\nmulti_logloss_f = log_loss(y_true, y_pred)\nprint('train_acc_final: ',train_acc_f)\nprint('val_train_acc_final: ',val_acc_f)\nprint('multi_logloss_final: ',multi_logloss_f)","f906692b":"summary['cat_f'] = [train_acc_f, val_acc_f,multi_logloss_f]\nsummary","3235e6f1":"print('The loss of the whole model decreases only by', summary.iloc[2,1]-summary.iloc[2,2])","82588422":"# re-tune the n_estimators by cross-validation and set the n_estimators as 300\nval_pool = cb.Pool(data = x_val, label = y_val, cat_features = np.where(x_val.dtypes != np.float)[0])\ncat_f2 = cbc(objective = 'MultiClass', learning_rate = 0.05716226472904498, n_estimators = 300,\n             max_depth = 9, reg_lambda = 1.4979851182199662, random_state = 1)\ncat_f2 = cat_f2.fit(X = train_pool, eval_set = val_pool, use_best_model = True,\n                    early_stopping_rounds = 50, verbose = 10, plot = True)","d7174c94":"train_acc_f2 = cat_f2.score(x_train,y_train)\ny_pred = cat_f2.predict(x_val)\nval_acc_f2 = accuracy_score(y_val,y_pred)\ny_pred = y_pred.reshape(-1,1)\ny_true = np.array(y_val).reshape(-1,1)\none_hot = OneHotEncoder(sparse = False)\ny_true = one_hot.fit_transform(y_true)\ny_pred = one_hot.fit_transform(y_pred)\nmulti_logloss_f2 = log_loss(y_true, y_pred)\nprint('train_acc_final2: ',train_acc_f2)\nprint('val_train_acc_final2: ',val_acc_f2)\nprint('multi_logloss_final2: ',multi_logloss_f2)","0e68d5e9":"summary['cat_f2'] = [train_acc_f2, val_acc_f2,multi_logloss_f2]\nsummary","9d9dbf8b":"print('The loss of the whole model decreases by', summary.iloc[2,1]-summary.iloc[2,3])","5280e96b":"cat_f2.tree_count_","78cd1c4d":"cat_f2.best_iteration_","48e5eafa":"cat_f2.best_score_","f19a89a0":"# plot the feature importance\ncat_f2_im = cat_f2.feature_importances_\nim_ind = np.argsort(cat_f2_im)[::-1]\nfor f in range(x_train.shape[1]):\n    print(f + 1, x.columns[im_ind[f]], cat_f2_im[im_ind[f]])","5d4b8c20":"x_columns_ = [x.columns[i] for i in im_ind] \nfor i in range(x.columns.shape[0]): \n    plt.bar(i, cat_f2_im[im_ind[i]], color='lightsteelblue', align='center') \n    plt.xticks(np.arange(x.columns.shape[0]), x_columns_, rotation=90, fontsize=11) ","befc9485":"y_pred = cat_f2.predict_proba(test)\ny_pred","91e1138f":"submission = pd.DataFrame(y_pred,columns=['predict_0', 'predict_1', 'predict_2'])\nre_test = test.reset_index()\nsubmission = pd.concat([re_test['id'], submission], axis=1)\nsubmission.to_csv('submission.csv',index = 0)\nsubmission.head(5)","baa6f59f":"# Model evaluation","cbe68c78":"### Trial 1 - BO","9feadd61":"use gridCV to search the learning rate","862c88ba":"Search all the paramaters through Bayesian Optimization","501fbcba":"gridCV - for n_estimators with learning_rate = 0.05","a92e39cb":"## Correlation analysis","e5e9e294":"# Data Preprocessing","4aee1921":"### Trial 2","2e8a6cf3":"# Model prediction","39577ab2":"## Outliers detection and removing","36c00071":"### Trial 3","38ba7adf":"## Skewed variables detection ","596c6c98":"#### n_estimators - CV","1c7c8495":"It seems that searching all the parameters through BO is better.","13455c35":"### Trial 1","78b7403f":"## Missing value detection","66e79aca":"## lgb","6b54a812":"Use Bayesian optimization to search for all parameters except learning_rate and n_estimators and\ninitialize learning rate as 0.1 and n_estimators as 5000","37911a63":"## Exploratory data analysis","9f886f0d":"Narrow the scope to find more accurate paramater.","931e8ef7":"From the above table, we can see that the last tuning loss is minimal.\nTherefore, we substitute the parameters into the final model, and then train.","a801dfd2":"#### learning rate - GSCV","7796998d":"## Transform the data types for lgb","dec8b201":"Although the location\u3001log_feature\u3001event_type are categorical variables,\nthey own much levels, which is hard to visualize.","a0fc64a4":"### Trial 2 - BO+","0587a57c":"## cb","e8de7328":"# Model building","2801382c":"#### n_estimators - GSCV","1cfeea27":"# Data partition","7415ca0e":"to control the consistency of the default value of the parameters for the two algorithms","6ae2c8d9":"the performance of the two algorithms is similar, hence we conduct tuning for the two models.","8b721e66":"use lgb.cv to search the n_estimators with learning_rate = 0.05","49df4359":"## Data loading and merging"}}