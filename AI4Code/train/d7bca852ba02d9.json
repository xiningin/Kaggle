{"cell_type":{"464da565":"code","e78b1ac6":"code","5659d799":"code","0b7c98ea":"code","9fc5d26f":"code","3d048b2a":"code","4fd2b1ef":"code","72ae1a6f":"code","32c4e49a":"code","8a9e5f0a":"code","a9bf0b38":"code","839fbccb":"code","a64004a4":"code","393698d9":"code","dd2218cc":"code","901b5552":"code","2ca3a745":"code","dcfd7430":"code","ad649b81":"code","db671521":"code","7f3f6b2e":"code","4a93d8d6":"code","33bd4a12":"code","da9e318c":"code","e7e828e4":"code","d766a467":"code","0b542b9c":"code","e17ee491":"code","14a086c2":"code","284525fa":"code","d370fee3":"code","991c1779":"code","6d0e3e71":"code","2617e38e":"code","5041ffd4":"code","59add374":"code","e38b296d":"code","4749370d":"code","263cdf0e":"markdown"},"source":{"464da565":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom scipy import integrate","e78b1ac6":"data = '..\/input\/australian-weather-prediction\/weatherAUS_train.csv'\n\ndata_train = pd.read_csv(data)\ndata_train.head(10)","5659d799":"df = data_train","0b7c98ea":"df.columns","9fc5d26f":"len(df['Location'].unique()), df['Location'].unique()","3d048b2a":"len(df['WindGustDir'].unique()), df['WindGustDir'].unique()","4fd2b1ef":"len(df['WindGustDir'].unique()), df['WindGustDir'].unique()\n#so must be changed 'WindDir9am' 'WindDir3pm'","72ae1a6f":"len(df['RainToday'].unique()), df['RainToday'].unique()","32c4e49a":"[df[i][:5] for i in df.columns]","8a9e5f0a":"np.shape(data_train)","a9bf0b38":"for i in range(len(df.columns)):\n    #if there 17 and less informative columns\n    print(23 - i, ') ', '~%.2f%% of data is corrupted ' % ((df.notna().sum(axis=1) < len(df.columns) - i).sum(axis=0) \/ df.shape[0] * 100))","839fbccb":"#where and how much data is corrupted\nfor i in df.keys():\n    print(i, (df[i].notna() == False).sum(axis=0))","a64004a4":"dfq = df.drop(df[(df.notna().sum(axis=1) < 17)].index).copy()\nnp.shape(dfq)\n","393698d9":"dfq.head(10)","dd2218cc":"dfq[\"RainToday\"] = pd.factorize(dfq[\"RainToday\"])[0]\ndfq.head(5)","901b5552":"dir_wind = {0 : 0, 'N' : 1, 'NNE' : 2, 'NE' : 3, 'ENE' : 4, 'E' : 5, 'ESE' : 6, 'SE' : 7, \n            'SSE' : 8, 'S' : 9, 'SSW' : 10, 'SW' : 11, 'WSW' : 12, 'W' : 13, 'WNW' : 14, 'NW' : 15, 'NNW' :16}\ndir_wind.get('N')","2ca3a745":"dir_wind.get(0)","dcfd7430":"dfq['WindGustDir'] = dfq['WindGustDir'].fillna(value = 0)\ndfq['WindDir9am'] = dfq['WindDir9am'].fillna(value = 0)\ndfq['WindDir3pm'] = dfq['WindDir3pm'].fillna(value = 0)\n\ndfq['WindGustDirNum'] = dfq['WindGustDir'].apply(lambda x: dir_wind.get(x))\ndfq['WindDir9amNum'] = dfq['WindDir9am'].apply(lambda x: dir_wind.get(x))\ndfq['WindDir3pmNum'] = dfq['WindDir3pm'].apply(lambda x: dir_wind.get(x))\n\ndfq.tail(10)","ad649b81":"#6 types of cities depending on their geography\ncity_dict = {'Uluru' : 1, 'AliceSprings' : 1, 'Katherine' : 2, 'Darwin' : 2, 'Cairns' : 2, \n             'Townsville' : 2, 'PearceRAAF' : 3, 'PerthAirport' : 3, 'Perth' : 3, 'Witchcliffe' : 3, \n             'Walpole' : 3, 'Albany' : 3, 'SalmonGums' : 3, 'Launceston' : 4, 'Hobart' : 4, \n             'Woomera' : 5, 'Mildura' : 5, 'Cobar' : 5, 'Moree' : 5, 'Sydney' : 6, 'Sale' : 6, \n             'Richmond' : 6, 'Wollongong' : 6, 'Canberra' : 6, 'Nuriootpa' : 6, 'Dartmoor' : 6, \n             'Williamtown' : 6, 'WaggaWagga' : 6, 'GoldCoast' : 6, 'Watsonia' : 6, 'SydneyAirport' : 6, \n             'Albury' : 6, 'MountGinini' : 6, 'Nhil' : 6, 'BadgerysCreek' : 6,'MelbourneAirport' : 6, \n             'Brisbane' : 6,'MountGambier' : 6, 'Bendigo' : 6, 'Newcastle' : 6, 'Tuggeranong' : 6, \n             'NorahHead' : 6,'CoffsHarbour' : 6, 'Penrith' : 6, 'Portland' : 6, 'NorfolkIsland' : 6,\n             'Melbourne' : 6, 'Ballarat' : 6, 'Adelaide' : 6}\n#Seasons -- winter and summer\n#season_dict = {1 : 0, 2 : 0, 3 : 0, 4 : 0, 5 : 1, 6 : 1, 7 : 1, 8 : 1, 9 : 1, 10 : 1, 11 : 0, 12 : 0}\nseason_dict = {1 : 2, 2 : 2, 3 : 3, 4 : 3, 5 : 3, 6 : 4, 7 : 4, 8 : 4, 9 : 1, 10 : 1, 11 : 1, 12 : 2}\n# 1 - spring (in Australia)\n# 2 - summer\n# 3 - autumn\n# 4 - winter\nlen(city_dict)","db671521":"dfq['LocationNum'] = dfq['Location'].apply(lambda x: city_dict.get(x))\ndfq['Month'] = dfq['Date'].apply(lambda x: int(x[5:7]))\ndfq['Season'] = dfq['Date'].apply(lambda x: season_dict.get(int(x[5:7])))\ndfq.head(10)","7f3f6b2e":"dfq['Evaporation'].sum() \/ len(dfq['Evaporation'].dropna()), dfq['Evaporation'].mean()","4a93d8d6":"dfq_prep = dfq.drop(labels = ['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'Month'], axis = 1)\nsample = dfq_prep.groupby(['LocationNum', 'Season']).median()\n#sample.loc([[3,1, 'MinTemp']])\n\nsample","33bd4a12":"(sample.notna().sum(axis=1) < len(sample.columns)).sum(axis=0)","da9e318c":"for i in range(1, 7):  # LocationNum\n    for j in range(1, 5): # Season\n        for k in dfq_prep.columns[:-7]: #float numbers\n            #print(dfq_prep.loc[(i, j), k].head(1))\n            #dfq_prep.loc[(i, j), k] = dfq_prep.loc[(i, j), k].fillna(sample.loc[(i, j), k])\n            dfq_prep.loc[(dfq_prep['LocationNum'] == i) & (dfq_prep['Season'] == j), k] = dfq_prep[(dfq_prep['LocationNum'] == i) & (dfq_prep['Season'] == j)][k].fillna(sample.loc[(i, j), k])\n        for k in dfq_prep.columns[-7:-2]: #integer numbers\n            #dfq_prep.loc[(i, j), k] = dfq_prep.loc[(i, j), k].fillna(int(round(sample.loc[(i, j), k])))\n            dfq_prep.loc[(dfq_prep['LocationNum'] == i) & (dfq_prep['Season'] == j), k] = dfq_prep[(dfq_prep['LocationNum'] == i) & (dfq_prep['Season'] == j)][k].fillna(int(round(sample.loc[(i, j), k])))\ndfq_prep.head()","e7e828e4":"#the data is ready\nprint('~%.2f%% of data is corrupted ' % ((dfq_prep.notna().sum(axis=1) < len(dfq_prep.columns)).sum(axis=0) \n                                         \/ dfq_prep.shape[0] * 100))\n(dfq_prep.notna().sum(axis=1) < len(dfq_prep.columns)).sum(axis=0)","d766a467":"a = np.asarray(dfq_prep.corr())\ncorr_matrix = a - (np.indices((23, 23))[0] == np.indices((23, 23))[1]).astype(np.float32)\nprint(np.round(np.max(corr_matrix, axis=0), decimals=2))\nprint(np.round(np.min(corr_matrix, axis=0), decimals=2))\ndfq_prep.corr().round(decimals=2)\n","0b542b9c":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)","e17ee491":"X = dfq_prep.drop(['Evaporation','Sunshine', 'Pressure9am', 'Temp3pm', 'Temp9am'],axis=1)\ncalc_vif(X)","14a086c2":"def standartization(features):\n    return (features - features.mean(axis = 0)) \/ features.std(axis = 0)","284525fa":"dfq_std = standartization(dfq_prep.drop(['RainTomorrow', 'RainToday'],axis=1)[::])\ndfq_std['RainTomorrow'] = dfq_prep['RainTomorrow']\ndfq_std['RainToday'] = dfq_prep['RainToday']\ndfq_std.head()","d370fee3":"# Divide data into test and train\ndf_in = dfq_std.drop(['RainTomorrow'],axis=1)\nX = np.vstack([df_in[df_in.columns[i]] for i in range(len(df_in.columns))]).T\nX_train, X_test, y_train, y_test = train_test_split(X, np.array(dfq_prep['RainTomorrow']), test_size=0.3, random_state=42)","991c1779":"X_train, y_train, np.shape(X_train), np.shape(y_train)","6d0e3e71":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(multi_class='multinomial', solver='lbfgs')\nmodel.fit(X_train, y_train)","2617e38e":"y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)","5041ffd4":"from sklearn.metrics import classification_report\nprint('Train metrics')\nprint(classification_report(y_train, y_train_pred))\n\nprint('Test metrics')\nprint(classification_report(y_test, y_test_pred))","59add374":"data_test = '..\/input\/australian-weather-prediction\/weatherAUS_test.csv'\n\ndata_test = pd.read_csv(data_test)\ndata_test.head(10)","e38b296d":"data_test[\"RainToday\"] = pd.factorize(data_test[\"RainToday\"])[0]\n\ndata_test['WindGustDir'] = data_test['WindGustDir'].fillna(value = 0)\ndata_test['WindDir9am'] = data_test['WindDir9am'].fillna(value = 0)\ndata_test['WindDir3pm'] = data_test['WindDir3pm'].fillna(value = 0)\n\ndata_test['WindGustDirNum'] = data_test['WindGustDir'].apply(lambda x: dir_wind.get(x))\ndata_test['WindDir9amNum'] = data_test['WindDir9am'].apply(lambda x: dir_wind.get(x))\ndata_test['WindDir3pmNum'] = data_test['WindDir3pm'].apply(lambda x: dir_wind.get(x))\n\ndata_test['LocationNum'] = data_test['Location'].apply(lambda x: city_dict.get(x))\ndata_test['Season'] = data_test['Date'].apply(lambda x: season_dict.get(int(x[5:7])))\n\ndata_prep = data_test.drop(labels = ['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm'], axis = 1)\nsample_test = data_prep.groupby(['LocationNum', 'Season']).mean()\n\nfor i in range(1, 7):  # LocationNum\n    for j in range(1, 5): # Season\n        for k in data_prep.columns[:-7]: #float numbers\n            data_prep.loc[(data_prep['LocationNum'] == i) & (data_prep['Season'] == j), k] = data_prep[(data_prep['LocationNum'] == i) & (data_prep['Season'] == j)][k].fillna(sample_test.loc[(i, j), k])\n        for k in data_prep.columns[-7:-2]: #integer numbers\n            data_prep.loc[(data_prep['LocationNum'] == i) & (data_prep['Season'] == j), k] = data_prep[(data_prep['LocationNum'] == i) & (data_prep['Season'] == j)][k].fillna(int(round(sample_test.loc[(i, j), k])))\n\ndata_std = standartization(data_prep.drop(['RainToday'],axis=1)[::])\ndata_std['RainToday'] = data_prep['RainToday']\n            \ndata_in = data_std.drop(['Id'],axis=1)\nX_real = np.vstack([data_in[data_in.columns[i]] for i in range(len(data_in.columns))]).T\n\ny_pred = model.predict(X_real)\n\ny_pred","4749370d":"data_real = '..\/input\/australian-weather-prediction\/weatherAUS_test_sample_solution.csv'\ndata_real = pd.read_csv(data_real)\ndata_real['RainTomorrow'] = y_pred\ndata_real.head()\ndata_real['RainTomorrow'].to_csv('weatherAUS_test_solution.csv')","263cdf0e":"## Homework\n\n* Get the trainning part of weather dataset at https:\/\/www.kaggle.com\/c\/australian-weather-prediction, a goal is to predict RainTomorrow label using binary logistic regression.\n* Explore the variables. Think what to do with categorical variables and missing values.\n* Explore data collinearity. \n* Think about data normalization.\n* Think to compose new features.\n* Using a standard sklearn.linear_model.LogisticRegression model build the classification model.\n* Try to improve the model varying model hyperparameters.\n* Hint: look at the tutorial https:\/\/www.kaggle.com\/prashant111\/logistic-regression-classifier-tutorial for some ideas. Think broader!\n* Make a prediction on a test set with your best model and submit to kaggle competition together with notebook file."}}