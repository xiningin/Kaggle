{"cell_type":{"8b69f85a":"code","67e51299":"code","2ae9e443":"code","b16c4020":"code","0e9083d1":"code","bf257380":"code","d53c3d89":"code","2f752756":"code","03d450a1":"code","24866ba2":"code","5d2eb3d9":"code","15355725":"code","e61d7f02":"code","a7898cc5":"code","d230c78d":"code","bcaf5e71":"code","61585874":"code","1695aca2":"code","91fa20fb":"code","6d94dacb":"code","4423577b":"code","03e0c19a":"code","0079f2cc":"code","77ebe6c6":"code","19f3c0e4":"code","a2d63a5b":"code","67e3810f":"markdown","bdbe3969":"markdown","6f6e50f4":"markdown","c480354f":"markdown","f057dead":"markdown","be4fb651":"markdown"},"source":{"8b69f85a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67e51299":"sPathTranslated = '\/kaggle\/input\/predict-future-sales-translated-dataset\/'\nsPathTrain = '\/kaggle\/input\/competitive-data-science-predict-future-sales\/'\nsPathSup = '\/kaggle\/input\/predict-future-sales-supplementary\/'\n","2ae9e443":"dfShops = pd.read_csv(sPathTranslated + 'shops_en.csv', index_col='shop_id')\ndfItems = pd.read_csv(sPathTranslated + 'items_en.csv', index_col='item_id')\ndfCateg = pd.read_csv(sPathTranslated + 'item_categories_en.csv', index_col='item_category_id')\ndfSalesTrain = pd.read_csv(sPathTrain + 'sales_train.csv')\ndfSalesTest = pd.read_csv(sPathTrain + 'test.csv', index_col='ID')\ndfSubm  = pd.read_csv(sPathTrain + 'sample_submission.csv', index_col='ID')\ndfCalendar = pd.read_csv(sPathSup + 'calendar.csv')","b16c4020":"from itertools import product\nfrom collections import Counter\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nimport category_encoders as ce\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","0e9083d1":"dfCateg","bf257380":"# Extraindo categorias e grupos de categorias\ndfCateg['grupo'] = dfCateg['item_category_name'].str.extract(r'(^[\\w\\s]*)')\ndfCateg['grupo'] = dfCateg['grupo'].str.strip()\n\ndfCateg['group_id'] = le.fit_transform(dfCateg.grupo.values)\ndfCateg.sample(5)","d53c3d89":"dfItems","2f752756":"# Join categoria, grupo e group_id no dfItems\ndfItems = dfItems.join(dfCateg, on='item_category_id')\ndfItems.sample(10)","03d450a1":"dfShops.head()","24866ba2":"dfShops['shop_name'] = dfShops['shop_name'].str.replace('!','').str.lstrip().str.rstrip()\ndfShops['city'] = dfShops['shop_name'].str.split(' ').map(lambda x: x[0])\ndfShops['city_code'] = LabelEncoder().fit_transform(dfShops['city'])\ndfShops = dfShops[['shop_name','city_code', 'city']]","5d2eb3d9":"dfShops = dfShops.drop([10])\ndfShops","15355725":"dfSalesTrain.loc[dfSalesTrain.shop_id == 10, 'shop_id'] = 11\ndfSalesTrain.loc[dfSalesTrain.shop_id == 11]","e61d7f02":"dfSalesTrain.isnull().sum()","a7898cc5":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","d230c78d":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=dfSalesTrain.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(dfSalesTrain.item_price.min(), dfSalesTrain.item_price.max()*1.1)\nsns.boxplot(x=dfSalesTrain.item_price)\n\ndfSalesTrain = dfSalesTrain[dfSalesTrain.item_price<100000]\ndfSalesTrain = dfSalesTrain[dfSalesTrain.item_cnt_day<1001]","bcaf5e71":"median = dfSalesTrain[\n    (dfSalesTrain.shop_id==32)&\n    (dfSalesTrain.item_id==2973)&\n    (dfSalesTrain.date_block_num==4)&\n    (dfSalesTrain.item_price>0)\n].item_price.median()\n\ndfSalesTrain.loc[dfSalesTrain.item_price<0, 'item_price'] = median","61585874":"dfSalesTrain","1695aca2":"#Para cada par shop_id\/item_id devemos criar uma linha para cada m\u00eas (0 - 33)\n\ngrid = [] \n\nindex_cols = ['date_block_num','shop_id', 'item_id']\nmeses = dfSalesTrain['date_block_num'].unique()\n\n#We construct a grid of all possible shop_id\/item_id pairs for a given month\nfor mes in meses:\n    shop_ids = dfSalesTrain[dfSalesTrain['date_block_num'] == mes].shop_id.unique()\n    item_ids = dfSalesTrain[dfSalesTrain['date_block_num'] == mes].item_id.unique()\n    grid.append(np.array(list(product(*[[mes], shop_ids, item_ids])), dtype='int16'))\n\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)  \n\n#We join the grid with the aggregated sales data per month\ngb = dfSalesTrain.groupby(index_cols, as_index = False).agg({ 'item_cnt_day':'sum'})\ngb.rename(columns = {'item_cnt_day':'item_cnt_month'},inplace = True)\ndf_sales = pd.merge(grid,gb,how='left',on=index_cols).fillna(0)\n\n#We add item price\ngb = dfSalesTrain.groupby('item_id',as_index = False).agg({ 'item_price':'mean'})\ngb.rename(columns ={'item_price':'avg_item_price'}, inplace = True)\ndf_sales = pd.merge(df_sales, gb,how='left', on='item_id').fillna(0)\n\n#Clip target values\ndf_sales['item_cnt_month'] = np.clip(df_sales['item_cnt_month'], 0, 20)\ndf_sales.sort_values(index_cols, inplace=True)","91fa20fb":"grid","6d94dacb":"dfSalesTest","4423577b":"#We assign next Date Block Num to the test set\ndfSalesTest['date_block_num'] = 34\n\n#Concatenate train and test dataframes\ndfSales = pd.concat([dfSalesTrain,dfSalesTest], ignore_index=True)\n","03e0c19a":"dfSales","0079f2cc":"dfSales.fillna(0, inplace = True)","77ebe6c6":"#Splitting training into training and validation\n#Using gridsearch on XGBoost will take very long time, so I decided to go with a fixed validation set for month 33\n#and use evaluation built in functionnality of XGBoost to determine the best iteration\nX_train = df_sales[df_sales['date_block_num'] < 33].drop('item_cnt_month',axis = 1)\ny_train = df_sales[df_sales['date_block_num'] < 33].item_cnt_month\nX_val = df_sales[df_sales['date_block_num'] == 33].drop('item_cnt_month',axis = 1)\ny_val = df_sales[df_sales['date_block_num'] == 33].item_cnt_month\n#Test Set\nX_test  = df_sales[df_sales['date_block_num'] == 34].drop('item_cnt_month',axis = 1)","19f3c0e4":"import time\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.model_selection as skt\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import plot_importance\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import ElasticNet","a2d63a5b":"#Model training and fiting\n#Model is already serialized, uncomment to train the model again\nts = time.time()\n\nxgb = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    subsample=0.8,\n    colsample_bytree=0.8,\n    eta = 0.3,\n    seed=42)\n\nxgb.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_val, y_val)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\ntime.time() - ts","67e3810f":"# Tratando Outliers","bdbe3969":"# **Tratando Features**","6f6e50f4":"# Carregando o Dataset","c480354f":"**Shops\/Cats\/Items preprocessamento\n**\n\n\nObserva\u00e7\u00f5es:\n\nCada \"shop_name\" come\u00e7a com o nome da cidade.\n\nCada \"category\" ou categoria do produto cont\u00e9m um tipo e subtipo em seu nome.\n","f057dead":"Foi detectado um item com valor abaixo de zero. Ser\u00e1 preenchido com o valor m\u00e9dio.","be4fb651":"in progress... "}}