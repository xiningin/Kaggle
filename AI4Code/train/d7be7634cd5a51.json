{"cell_type":{"6c6d9c71":"code","130433ee":"code","bd716c57":"code","9d5d9ca6":"code","4528eb8f":"code","e2efcb9a":"code","6c3d7428":"code","49c90483":"code","7828ea52":"code","09a12c19":"code","f82effad":"code","c1a8a3f2":"code","2dc84ec8":"code","d425cda4":"code","cb81638a":"code","2f341014":"code","b4c977e5":"code","b9b8284b":"code","8e9b167b":"code","fbc28b85":"code","579f6976":"code","be1b9c3b":"code","c056bd42":"code","e271cdde":"code","f4f72450":"code","d45b76fe":"code","5fc54301":"code","fc63ec86":"code","a4228b0a":"code","9cf2fa86":"code","9bf5d11e":"code","8e78e568":"code","7696bd6d":"code","9b1418f8":"code","13a2caa5":"code","d251a025":"code","7174c2ef":"code","7dd040ec":"code","5a5e749f":"code","3502fc84":"code","c032b662":"code","97705f2f":"code","8f6b809e":"code","a7727aed":"code","4448de4c":"code","0651e460":"code","21ae35db":"code","5d42c1d8":"code","ee6ced13":"code","c91db805":"code","3a727792":"code","cf4b602b":"code","28b071ee":"code","cb32cf26":"code","a0add086":"code","c87f3c72":"code","8037d5e0":"code","e5d657e2":"code","44810b71":"code","b66e7ef3":"code","5b5a21a6":"code","9edcee62":"code","8a6433bc":"code","d96ed01f":"code","16645fd0":"code","6a528562":"code","0ba3ee5f":"code","13b93b0e":"code","f80beaed":"code","fc8307de":"code","01f4b383":"code","42e29120":"code","0996a287":"code","4a205559":"code","0ab9bcd3":"code","918020e1":"code","27b97463":"code","51ebd9a0":"code","d496cf37":"code","6221e33b":"code","00b30222":"code","ac8c9751":"code","9ebbc8f5":"code","7182624e":"code","da6e807a":"code","070b17cd":"code","73f535a1":"code","81057aa4":"code","8058acab":"code","6842b9fc":"code","f53df6c6":"code","98c7b570":"code","a1120983":"code","9771ec39":"code","4e2695b6":"code","ec541c47":"code","4da7ec7e":"code","c7ce65a0":"code","b1720848":"code","bf231b68":"code","fee69758":"code","90f2d58b":"code","e3c6f197":"code","b4a254fc":"code","d283c61c":"code","4b8d3d8a":"code","b047864a":"code","61f91e52":"code","bd272170":"code","a6bef3b5":"code","965d31ae":"code","ef2b0ad0":"code","d4e2c69e":"code","eb927dec":"code","2db31abc":"code","e6c64b2b":"code","9bd812ba":"code","164187a4":"code","b231faef":"code","f1b817b8":"code","ebb94a50":"code","4e1f051c":"code","6573f9f0":"code","361b9690":"code","ab4b5aeb":"code","19ed62ab":"code","696bbac4":"code","7c713054":"code","1688314a":"code","fc69ef99":"code","7e6953a4":"code","71b769ee":"markdown","261e35f2":"markdown","c88cba9b":"markdown","1c0b28e5":"markdown","eceb91ed":"markdown","36303e87":"markdown","e160e92b":"markdown","ce9feb25":"markdown","7b74254a":"markdown","2f8780fc":"markdown","a35dc444":"markdown","9628d465":"markdown","0c391be0":"markdown","ddf2e824":"markdown","23b2e9c0":"markdown","a95127e3":"markdown","3f74d58e":"markdown","65d5c048":"markdown","26d1563c":"markdown","99f5aabf":"markdown","3761c70f":"markdown","fa2c38ce":"markdown","d36fbac8":"markdown","192b4d56":"markdown","979a8af1":"markdown","769fc85e":"markdown","3306c412":"markdown","4c4e68f9":"markdown","a6cf8421":"markdown","f33bf178":"markdown","10f6a879":"markdown","e997ab8c":"markdown","0a36855a":"markdown","3e9c52bf":"markdown","a879d2d8":"markdown","1206a891":"markdown","6c7268d3":"markdown","55089fb2":"markdown","97ad4c78":"markdown","070bbc2d":"markdown","2cccb8d4":"markdown","9acfe66e":"markdown","c9a2c1e8":"markdown","d499a7ec":"markdown","9365c2c9":"markdown"},"source":{"6c6d9c71":"#Required Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os # library for file and folder traversal\nfrom keras.datasets import imdb # keras.datasets library has a list of popular data\nfrom keras import models # this library calls model\nfrom keras import layers # required for building dense layers\nfrom keras import optimizers # define Dense layer optimizers\nimport matplotlib.pyplot as plt # required for ploting loss vs epochs\n\nfrom keras.utils import to_categorical # library to encode labels","130433ee":"print(os.listdir(\"..\/input\/imdb-data\/\"))","bd716c57":"(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)","9d5d9ca6":"#train_data\n#test_data\n#train_labels\ntest_labels","4528eb8f":"word_index = imdb.get_word_index() # word_index is a dictionary mapping words to an integer index\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()]) # reverse it, mapping integer indices to words\ndecoded_review = ' '.join([reverse_word_index.get(i -3, '?') for i in train_data[2]]) # decode the review, first 3 indices are not relevent","e2efcb9a":"decoded_review","6c3d7428":"def vectorize_sequences(sequences, dimension = 10000): \n    # sequence arguement denotes number of review i.e. number of rows in dataset\n    # dimension argument denotes number of columns\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1\n    return results # Returns, a tensor of shape (len(sequence), dimension)","49c90483":"x_train = vectorize_sequences(train_data) # \nx_test = vectorize_sequences(test_data)","7828ea52":"x_train","09a12c19":"x_test","f82effad":"y_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(test_labels).astype('float32')","c1a8a3f2":"y_train","2dc84ec8":"y_test","d425cda4":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","cb81638a":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","2f341014":"model.summary()","b4c977e5":"model.compile(optimizer= optimizers.RMSprop(lr=0.001),\n             loss='binary_crossentropy',\n             metrics=['accuracy'])","b9b8284b":"history = model.fit(x_train,\n                   y_train,\n                   epochs=4,\n                   batch_size=512,\n                   validation_data=(x_test, y_test))","8e9b167b":"history_dict = history.history\nhistory_dict.keys()","fbc28b85":"history_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\n\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss') \nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","579f6976":"plt.clf()\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","be1b9c3b":"results = model.evaluate(x_test, y_test)","c056bd42":"results","e271cdde":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","f4f72450":"model.summary()","d45b76fe":"model.compile(optimizer= optimizers.RMSprop(lr=0.001),\n             loss='binary_crossentropy',\n             metrics=['accuracy'])","5fc54301":"history = model.fit(x_train,\n                   y_train,\n                   epochs=4,\n                   batch_size=512,\n                   validation_data=(x_test, y_test))","fc63ec86":"history_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\n\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss') \nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","a4228b0a":"plt.clf()\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","9cf2fa86":"results = model.evaluate(x_test, y_test)\nresults","9bf5d11e":"model = models.Sequential()\nmodel.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","8e78e568":"model.summary()","7696bd6d":"model.compile(optimizer= optimizers.RMSprop(lr=0.001),\n             loss='binary_crossentropy',\n             metrics=['accuracy'])\n\nhistory = model.fit(x_train,\n                   y_train,\n                   epochs=4,\n                   batch_size=512,\n                   validation_data=(x_test, y_test))","9b1418f8":"history_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\n\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss') \nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","13a2caa5":"plt.clf()\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","d251a025":"results = model.evaluate(x_test, y_test)\nresults","7174c2ef":"model = models.Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()","7dd040ec":"model.compile(optimizer= optimizers.RMSprop(lr=0.001),\n             loss='binary_crossentropy',\n             metrics=['accuracy'])\n\nhistory = model.fit(x_train,\n                   y_train,\n                   epochs=4,\n                   batch_size=512,\n                   validation_data=(x_test, y_test))","5a5e749f":"history_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\n\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss') \nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","3502fc84":"plt.clf()\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","c032b662":"results = model.evaluate(x_test, y_test)\nresults","97705f2f":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()","8f6b809e":"model.compile(optimizer= optimizers.RMSprop(lr=0.001),\n             loss='MSE',\n             metrics=['accuracy'])\n\nhistory = model.fit(x_train,\n                   y_train,\n                   epochs=4,\n                   batch_size=512,\n                   validation_data=(x_test, y_test))","a7727aed":"history_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\n\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss') \nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","4448de4c":"plt.clf()\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","0651e460":"results = model.evaluate(x_test, y_test)\nresults","21ae35db":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='tanh', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='tanh'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()","5d42c1d8":"model.compile(optimizer= optimizers.RMSprop(lr=0.001),\n             loss='binary_crossentropy',\n             metrics=['accuracy'])\n\nhistory = model.fit(x_train,\n                   y_train,\n                   epochs=4,\n                   batch_size=512,\n                   validation_data=(x_test, y_test))","ee6ced13":"history_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\n\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss') \nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","c91db805":"plt.clf()\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","3a727792":"results = model.evaluate(x_test, y_test)\nresults","cf4b602b":"from keras.layers import LeakyReLU","28b071ee":"model = models.Sequential()\nmodel.add(layers.Dense(16, input_shape=(10000,)))\nmodel.add(LeakyReLU(alpha=0.05))\nmodel.add(layers.Dense(16))\nmodel.add(LeakyReLU(alpha=0.05))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()","cb32cf26":"model.compile(optimizer= optimizers.RMSprop(lr=0.001),\n             loss='binary_crossentropy',\n             metrics=['accuracy'])\n\nhistory = model.fit(x_train,\n                   y_train,\n                   epochs=4,\n                   batch_size=512,\n                   validation_data=(x_test, y_test))","a0add086":"history_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\n\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss') \nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","c87f3c72":"plt.clf()\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","8037d5e0":"results = model.evaluate(x_test, y_test)\nresults","e5d657e2":"from keras.datasets import reuters\n\n(train_data, train_labels),(test_data, test_labels)= reuters.load_data(num_words=10000)","44810b71":"print(len(train_data))\nprint(len(test_data))","b66e7ef3":"print(train_data[2])","5b5a21a6":"print(test_data[2])","9edcee62":"word_index = reuters.get_word_index()\n\nreverse_word_index = dict([(value,key) for (key, value) in word_index.items()])\ndecoded_newswire = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[0]])","8a6433bc":"print(decoded_newswire)","d96ed01f":"decoded_newswire = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[1]])","16645fd0":"print(decoded_newswire)","6a528562":"def vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence]=1\n    return results","0ba3ee5f":"x_train = vectorize_sequences(train_data)","13b93b0e":"x_test = vectorize_sequences(test_data)","f80beaed":"x_test","fc8307de":"x_train","01f4b383":"def to_one_hot(labels, dimension=46):\n    results = np.zeros((len(labels), dimension))\n    for i, label in enumerate(labels):\n        results[i, label] =1\n    return results","42e29120":"one_hot_train_labels = to_one_hot(train_labels)","0996a287":"one_hot_test_labels = to_one_hot(test_labels)","4a205559":"print(one_hot_test_labels)","0ab9bcd3":"print(one_hot_train_labels)","918020e1":"# using keras.utils to import to_categorical\n\none_hot_train_labels= to_categorical(train_labels)\none_hot_test_labels = to_categorical(test_labels)","27b97463":"print(one_hot_train_labels)","51ebd9a0":"print(one_hot_test_labels)","d496cf37":"model = models.Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(46, activation='softmax'))","6221e33b":"model.summary()","00b30222":"model.compile(optimizer='rmsprop',\n             loss = 'categorical_crossentropy',\n             metrics=['accuracy'])","ac8c9751":"history = model.fit(x_train,\n                   one_hot_train_labels,\n                    epochs = 20,\n                    batch_size=16,\n                    validation_data=(x_test, one_hot_test_labels)\n                   )","9ebbc8f5":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","7182624e":"plt.clf()                     \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","da6e807a":"model = models.Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(46, activation='softmax'))","070b17cd":"model.compile(optimizer='rmsprop',\n             loss = 'categorical_crossentropy',\n             metrics=['accuracy'])","73f535a1":"history = model.fit(x_train,\n                   one_hot_train_labels,\n                    epochs = 9,\n                    batch_size=16,\n                    validation_data=(x_test, one_hot_test_labels)\n                   )","81057aa4":"results= model.evaluate(x_test, one_hot_test_labels)","8058acab":"print(results)","6842b9fc":"predictions = model.predict(x_test)","f53df6c6":"predictions[0]","98c7b570":"np.argmax(predictions[50])","a1120983":"model = models.Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(4, activation='relu'))\nmodel.add(layers.Dense(46, activation='softmax'))","9771ec39":"model.summary()","4e2695b6":"model.compile(optimizer='rmsprop',\n             loss='categorical_crossentropy',\n             metrics=['accuracy'])","ec541c47":"model.fit(x_train,\n         one_hot_train_labels,\n         epochs=9,\n          batch_size=16,\n         validation_data=(x_test, one_hot_test_labels))","4da7ec7e":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","c7ce65a0":"plt.clf()                     \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","b1720848":"evaluations= model.evaluate(x_test, one_hot_test_labels)","bf231b68":"print(evaluations)","fee69758":"model = models.Sequential()\nmodel.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dense(46, activation='softmax'))","90f2d58b":"model.compile(optimizer='rmsprop',\n             loss='categorical_crossentropy',\n             metrics=['accuracy'])","e3c6f197":"model.fit(x_train,\n         one_hot_train_labels,\n         epochs=20,\n          batch_size=16,\n         validation_data=(x_test, one_hot_test_labels))","b4a254fc":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","d283c61c":"plt.clf()                     \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","4b8d3d8a":"evaluations= model.evaluate(x_test, one_hot_test_labels)\nprint(evaluations)","b047864a":"model = models.Sequential()\nmodel.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(46, activation='softmax'))","61f91e52":"model.compile(optimizer='rmsprop',\n             loss='categorical_crossentropy',\n             metrics=['accuracy'])\n\nmodel.fit(x_train,\n         one_hot_train_labels,\n         epochs=20,\n          batch_size=16,\n         validation_data=(x_test, one_hot_test_labels))","bd272170":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","a6bef3b5":"plt.clf()                     \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","965d31ae":"evaluations= model.evaluate(x_test, one_hot_test_labels)\nprint(evaluations)","ef2b0ad0":"model = models.Sequential()\nmodel.add(layers.Dense(256, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dense(46, activation='softmax'))","d4e2c69e":"model.compile(optimizer='rmsprop',\n             loss='categorical_crossentropy',\n             metrics=['accuracy'])\n\nmodel.fit(x_train,\n         one_hot_train_labels,\n         epochs=20,\n          batch_size=16,\n         validation_data=(x_test, one_hot_test_labels))","eb927dec":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","2db31abc":"plt.clf()                     \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","e6c64b2b":"evaluations= model.evaluate(x_test, one_hot_test_labels)\nprint(evaluations)","9bd812ba":"model = models.Sequential()\nmodel.add(layers.Dense(256, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(46, activation='softmax'))","164187a4":"model.compile(optimizer='rmsprop',\n             loss='categorical_crossentropy',\n             metrics=['accuracy'])\n\nmodel.fit(x_train,\n         one_hot_train_labels,\n         epochs=20,\n          batch_size=16,\n         validation_data=(x_test, one_hot_test_labels))","b231faef":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","f1b817b8":"plt.clf()                     \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","ebb94a50":"evaluations= model.evaluate(x_test, one_hot_test_labels)\nprint(evaluations)","4e1f051c":"model = models.Sequential()\nmodel.add(layers.Dense(256, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(46, activation='softmax'))","6573f9f0":"model.compile(optimizer='rmsprop',\n             loss='categorical_crossentropy',\n             metrics=['accuracy'])\n\nmodel.fit(x_train,\n         one_hot_train_labels,\n         epochs=20,\n          batch_size=16,\n         validation_data=(x_test, one_hot_test_labels))","361b9690":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","ab4b5aeb":"plt.clf()                     \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","19ed62ab":"evaluations= model.evaluate(x_test, one_hot_test_labels)\nprint(evaluations)","696bbac4":"model = models.Sequential()\nmodel.add(layers.Dense(256, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(46, activation='softmax'))","7c713054":"model.compile(optimizer='rmsprop',\n             loss='categorical_crossentropy',\n             metrics=['accuracy'])\n\nmodel.fit(x_train,\n         one_hot_train_labels,\n         epochs=20,\n          batch_size=16,\n         validation_data=(x_test, one_hot_test_labels))","1688314a":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","fc69ef99":"plt.clf()                     \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","7e6953a4":"evaluations= model.evaluate(x_test, one_hot_test_labels)\nprint(evaluations)","71b769ee":"#### Experiment-4\nUse 2 hidden layers.","261e35f2":"#### Importance of having sufficiently large intermediate layers\n\nYou should not use fewer nodes in the intermediate layer than the number of ouptu classes. For example, _Reuters_ data has 46 classes, what would happen if we use say 4 nodes in the intermediate layer. Results would detoriate .i.e. the neural network would loose infromation due to insufficient number of nodes in the intermediate layer. Let us build a neurl networks model with 4 nodes in intermediate layer and check its accuracy.","c88cba9b":"One would note that each _Reuter_ entry is a list of number showing word indices e.g. here we show entry of 2nd row in train and test data.","1c0b28e5":"* Following line of code, decodes it back to words for 1st entry.\nNote that the indices are offset by 3 becasue 0,1, and 2 are reserved indices for padding start and unknown.","eceb91ed":"Below, line of code gets maximum accuracy from 46 labels suggested by prediction model.","36303e87":"**Discussion**: The results show that the network starts to overfit after 9 epoches i.e. model's train accuracy keeps on increasing but tst accuracy or validation accuracy decreases. Therefore, we wil kee 9 epochs and re-train the model.","e160e92b":"Argument __num_words__ = 10000, allows us to keep first 10k most common words.\n\nOuput of train_data, and test_data are lists of reviews from user. Additionally, the list of output from train_labels and test_labels are list of 0s and 1s, where 0 stands for negative and 1 stands for positive feeback\/review.","ce9feb25":"We could also use _keras_, built-in method ```to_categorical``` to one-hot-encode too. Shown below,","7b74254a":"# Classifying moview review: a binary classification example\n\n\nTwo-class classification, or binary classification. In this example, we learn to classify movie reviews as positive or negative based on the text content of the reviews.\n\n","2f8780fc":"**Conclusion**: Results show that 64 neuron architecture is better than 32 neurons but not better than 16 neurons. Architecture with 64 neurons gave accuracy of 87.30% which is lower than the architecture with 16 neurons. ","a35dc444":"#### Experiment-6\n\n* Use ```LeakyReLU``` instead of ```relu```.","9628d465":"#### Experiment-6\nTrain the model with 4 hidden intermediate layers.","0c391be0":"### Build the neural network model\n\nNumber of output for _topic_classification_ here is 46. Therefore, the output layer has much larger dimension as compared to binary classification problem, where we used 1 output node.\n\nIn previous problem, we used 16 node in each layer. Using same number of node for each _Dense layer_ is not recommended for this problem because we have 46 output classes. Hence, using only 16 dimension neural node would curtain information related to classification. Therefore, here we will use 64 nodes as hows below:","ddf2e824":"#### Discussion\n\nTake aways:\n* One needs to do a bit of pre-processing on raw data to feed it as tensors into a neural network. Sequences of words can be encoded as binary vectors, but there are other encoding options too.\n* Stacks of _Dense layers_ with _relu_ activations can solve a wide range of problems, and it is most frequently used activation function.\n* In a binary classification problem (two output classes), the network should end with a _Dense_ layer with one unit and a _sigmoid_ activation i.e. the output of the network should be scalar between 0 and 1, after encoding a probability.\n* With a scalar sigmoid output on a binary classification problem, the loss function you should use is ```binary_crossentropy```.\n* The _rmsprop_ optimizer is generally a good enough choice, whatever your problem. That is one less thing for you to worry about. That is one less thing for you to worry about.\n* As they get better their training data, neural networks eventually start overfitting and end up obtaining increasingly worse results on data they have never seen before. Be sure to always monitor performance on data that is outside of the training set.","23b2e9c0":"Here, we are vectorizing ```test_data```.","a95127e3":"Similarly, we should vectorize train_labels and test_labels.","3f74d58e":"```categorical_crossentropy``` is best _loss function_ for multi-label classification problems.","65d5c048":"This is fairly naive approach and achieves an accuraccy of 88%; with state-of-art approches, we should be able to get close to 95%.","26d1563c":"**Conclusion** NN architecture with two layer gave accuracy of 88.48%, and architecture with three layer gave accuracy of 87.36%. Therefore, it can be concluded that for this classification problem, 2 layer of Dense layer architecture is better than 3 layer of neural networks.","99f5aabf":"#### Experiment-3\n\nIncrease hidden units in dense layer to 64 units with 2 dense layers.","3761c70f":"#### Experiment-4: \n\nUse ```mas_loss``` instead of ```binary_crossentropy```.","fa2c38ce":"#### Experiment-2\n\nIncrease hidden units in dense layer to 32 units, but keep only 2 layers of dense layer as it performed better than 3 layers of NN.\n","d36fbac8":"Note that the call to ```model.fit()``` returns a ```History``` object. This object has a member _history_, which is a dictionary containing data about everything that happened during training. ","192b4d56":"## Model\n\nA three layer network\n![image.png](attachment:image.png)\n\n#### What are the activation functions, and why are they necessary?\n\nWithout an activation function like _relu_(also called a _non-linearity_), the _Dense layer_ would consist of two linear operations-a dot product and an addition:\n``` output = dot(W, input) + b ```\n\nSO the layer could only learn _linear transformations_(affine transformations) of the input data: the _hypothesis space_ of the layer would be the set of all possible linear transformations of the input data into a 16-dimensional space. Such a hypothesis space is too restricted and would not benefit from multiple layers of representations, because a deep stack of linear layers would still implement a linear operation: adding more layers would not extend the hypothesis space.\n\nIn order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function. _relu_ is the most popular activation function in deep learning, but there are many other candidates, which all come with similarly strange names e.g. _prelu, elu,_ and so on.","979a8af1":"## Decode reviews\nDecoding some of the reviews from numeric to text.","769fc85e":"#### Data Prepration\nHere, we will vectorize the train and test data so that it can be consumed by our neural network layers defined later.","3306c412":"Decoded word from 2nd row.","4c4e68f9":"#### Experiment-5\nTrain model with 3 hidden layers","a6cf8421":"## Experiment with architecture changes\n\nLet us change the architecture choices:\n\n* Experiment-1. Take 3 hidden layer and evaluate the results.\n* Experiment-2. Increase hidden units in dense layer to 32 units,\n* Experiment-3. Increase hidden units in dense layer to 64 units,\n* Experiment-4. Use ```mas_loss``` instead of ```binary_crossentropy```,\n* Experiment-5. Try ```tanh``` activation instead of ```relu```.\n\n#### Experiment-1\n\nTake 3 hidden layer and evaluate the results.","f33bf178":"Here, train_labels are one-hot-encoded:","10f6a879":"**Conclusion**: The architecture with loss=```MSE``` gave an accuracy of 87.36%, which is not an improvement. Therefore, discard MSE as loss function.","e997ab8c":"# Multi-class classification\n\nIn the previous problem, we saw how to classify vector inputs into two mutually exclusive classes using a densely connected neural network. But what happens when you have more than two classes?\n\nHere, we will build a neural network to classify _Reuter Newswires_ into 46 mutually exclusive topics. Becasue we have many classes, this problem is an instance of _mutliclass classification;_ and because each data point should be classified into only one category, the problem is more specifically an instance of _single-lable, multiclass classification._ If each data point could belong to multiple categories, you would be facing a _multilabel, multiclass classification_ problem.\n\n### Reuters Dataset\n\nWe will work with _Reuters_ dataset in this section, a set of short newswires and their topics, published by Reuters in 1986. There are 46 different topics.\n\nThe _Reuters_ dataset comes packaged with _keras_.","0a36855a":"**Discussion**: \n\n\n#### Experiment-2\nTrain model with 128 units(nodes) in the intermediate layer.","3e9c52bf":"**Conclusion**: Results show the architecture with 32 nurons per layer degraded model's accuracy to 87.16%, which is not an improvement over the architecture with 16 neurons per layer. Therefore, keep 16 neurons in the model.","a879d2d8":"Finally, one needs to choose a _loss function_ and an _optimizer_. Because you are facing a binary classification problem and the output of you network is a probability ( you end your network with a single-unit layer with a sigmoid activation), it is best to use the ``` binary_crossentropy``` loss for binary classification problem. It is not the only viable choise that you could use, for instance, ```mean_squared_error```. But crossentropy is usually the best choice when you are dealing with models that output probabilities. _Crossentropy_ is quantity from the field of _Information Theory_ that measures the distance between probability distributions or, in this case, between the ground-truth distribution and your predictions.\n\nHere is the step where you configure the model with the ```RMSprop``` _optimizer_ and the ```binary_crossentropy``` loss function. Note that you will also monitor accuracy during training. \n\nWe can pass _optimizer, loss function, and metrics_ as strings in the ```model.compile()``` function because ```RMSprop, binary_crossentropy, and accuracy``` are packaged as part of _Keras_. \n\n**Notes:** _Sometimes one needs to configure the parameters of optimizer or pass a custom loss function or metric function. This can be done by passing function objects as the loss and\/or metrics arguments._","1206a891":"We should note following two important points in the present neural network architecure:\n* We have ended the neural network with 46 units. Therefore, the neural network will output 46-dimensional vector ( one-hot coded).\n* The last layer uses a _'softmax'_ activation function. ","6c7268d3":"Vectorizing ```train_data``` here:","55089fb2":"So, this apporch gives an accuracy of 78.63%.\n\nHere are the steps to generate _labels_ for new data.","97ad4c78":"#### Experiment-5:\n\n* Choose ```tanh``` as activation function over ```relu```.","070bbc2d":"#### Experiment-3\nTrain model with 256 units(nodes) in the intermediate layer.","2cccb8d4":"** Discussion**: Results show that the model with 4 nodes in intermediate layer gave accuacy of 69.67% in comparison to 79.74% with neural network having 64 nodes in the intermediate layer. Therefore, it can be concluded that keeping fewer nodes in the intermediate layer than number of output class detoriates models accuracy. Let us carryout somemore experiments.\n\n### Experiment\nNow let us carryout following experiments:\n* Experiment-1. Train model with 32 units (nodes) in the intermediate layer.\n* Experiment-2: Train model with 128 units(nodes) in the intermediate layer.\n* Experiment-3: Train model with 256 units(nodes) in the intermediate layer.\n* Experiment-4: Use 2 hidden layers.\n* Experiment-5: Use 3 hidden layers.\n* Experiment-6: Use 4 hidden layers.\n\n#### Experiment-1\nTrain the model with 32 units in the intermediate layers.","9acfe66e":"## Data preparation\n\n* Function _vectorize_sequences_ creates an all zero matrix of shape (len(data), num_of_columns).\n* Later this function is called to vectorize training data, and test data.","c9a2c1e8":"Similarly, for label data i.e train_label and test_label, the numeric value for 46 classes range between 0 and 45.","d499a7ec":"Here, test_labels are one-hot-encoded:","9365c2c9":"Similar to ```train_data``` and ```test_data```, we need to encode ```train_label``` and ```test_label```. This can be achieved using following two techniques:\n* One-hot encoding, and \n* Cast each label as integer tensor.\n\nIn this example, we are using _categorical encoding_ i.e. one-hot encoding of the 46 labels."}}