{"cell_type":{"9ce433a4":"code","9837e9a3":"code","f8937ce5":"code","12d0e7f4":"code","34c8e61c":"code","88f91334":"code","2556fecf":"code","8ac66427":"code","87203200":"code","eb81ff12":"code","8070133b":"code","ba958d98":"code","4876169e":"code","39f0966a":"code","c48772c4":"code","cfee717a":"code","259b2324":"code","b3830213":"code","c63aebf0":"code","2579a20c":"code","10ffb03a":"code","1291a88f":"code","1129c004":"code","bf133a6e":"code","eb637272":"code","6cb26e9e":"code","f4520f21":"code","158c81ab":"code","603cf641":"code","c54fa700":"code","a0c67a1f":"code","6fcbe440":"code","9e493f0a":"code","95c1a848":"code","348ffc38":"code","85c3fc5e":"code","024d0daa":"code","9577fab9":"markdown","393cacc3":"markdown","75c56b99":"markdown","fba23429":"markdown","44f8b6a8":"markdown","46d98230":"markdown","70c79638":"markdown","067076da":"markdown","f22b4dbc":"markdown","192b4a2f":"markdown","633e2702":"markdown","0f032446":"markdown","ee054a9e":"markdown","bb0b111f":"markdown","4bc5509a":"markdown","51739ccd":"markdown","e04a724b":"markdown","92712aa6":"markdown","bfc46f3b":"markdown","3f322cea":"markdown","12a833da":"markdown","a1af73bd":"markdown","ade6bfca":"markdown","dace9bc8":"markdown","8baf2bee":"markdown","5ec88d53":"markdown","beea1a90":"markdown","ed2eb4f9":"markdown","1106d782":"markdown","933d53bb":"markdown","7364f09f":"markdown","c91a516b":"markdown","9c08e1b4":"markdown"},"source":{"9ce433a4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport gc\nimport os\nimport sys\n\nsns.set_style('darkgrid')\nsns.set_palette('bone')\n\n#pd.options.display.float_format = '{:.5g}'.format\npd.options.display.float_format = '{:,.3f}'.format\n\nprint(os.listdir(\"..\/input\"))","9837e9a3":"def toTapleList(list1,list2):\n    return list(itertools.product(list1,list2))","f8937ce5":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                #el\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else:\n            #df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","12d0e7f4":"%%time\ntrain = pd.read_csv('..\/input\/train_V2.csv')\ntrain = reduce_mem_usage(train)\ntest = pd.read_csv('..\/input\/test_V2.csv')\ntest = reduce_mem_usage(test)\nprint(train.shape, test.shape)","34c8e61c":"train.info()","88f91334":"null_cnt = train.isnull().sum().sort_values()\nprint('null count:', null_cnt[null_cnt > 0])\n# dropna\ntrain.dropna(inplace=True)","2556fecf":"train.describe(include=np.number).drop('count').T","8ac66427":"for c in ['Id','groupId','matchId']:\n    print(f'unique [{c}] count:', train[c].nunique())","87203200":"cols = ['kills','teamKills','DBNOs','revives','assists','boosts','heals','damageDealt',\n    'walkDistance','rideDistance','swimDistance','weaponsAcquired']\ncols.extend(['killPlace','winPlacePerc'])\ngroup = train.groupby(['matchId','groupId'])[cols]\n\nfig, ax = plt.subplots(3, 1, figsize=(12, 18), sharey=True)\nfor df, ax in zip([group.mean(), group.min(), group.max()], ax.ravel()):\n    sns.heatmap(df.corr(), annot=True, linewidths=.6, fmt='.2f', vmax=1, vmin=-1, center=0, cmap='Blues', ax=ax)\n\ndel df","eb81ff12":"all_data = train.append(test, sort=False).reset_index(drop=True)\ndel train, test\ngc.collect()","8070133b":"match = all_data.groupby('matchId')\nall_data['killPlacePerc'] = match['kills'].rank(pct=True).values\nall_data['walkDistancePerc'] = match['walkDistance'].rank(pct=True).values\n#all_data['damageDealtPerc'] = match['damageDealt'].rank(pct=True).values","ba958d98":"all_data['_totalDistance'] = all_data['rideDistance'] + all_data['walkDistance'] + all_data['swimDistance']\nall_data['playerAliveTime'] =  (all_data['assists']*2) + (all_data['boosts']*4) + (all_data['damageDealt']\/100)  + (all_data['DBNOs']*2) + (all_data['headshotKills']*1) + (all_data['heals']*6) + (all_data['kills']*2) + (all_data['revives']*10) + (all_data['rideDistance']\/36.111) + (all_data['swimDistance']\/6.3) + (all_data['vehicleDestroys']*6) + (all_data['walkDistance']\/6.3) + (all_data['weaponsAcquired']*1)\n#all_data['_rideBin'] = (all_data['rideDistance'] > 0).astype(int)\n#all_data['_swimBin'] = (all_data['swimDistance'] > 0).astype(int)","4876169e":"def fillInf(df, val):\n    numcols = df.select_dtypes(include='number').columns\n    cols = numcols[numcols != 'winPlacePerc']\n    df[df == np.Inf] = np.NaN\n    df[df == np.NINF] = np.NaN\n    for c in cols: df[c].fillna(val, inplace=True)","39f0966a":"all_data['_healthItems'] = all_data['heals'] + all_data['boosts']\nall_data['_headshotKillRate'] = all_data['headshotKills'] \/ all_data['kills']\nall_data['_killPlaceOverMaxPlace'] = all_data['killPlace'] \/ all_data['maxPlace']\nall_data['_killsOverWalkDistance'] = all_data['kills'] \/ all_data['walkDistance']\n#all_data['_killsOverDistance'] = all_data['kills'] \/ all_data['_totalDistance']\n#all_data['_walkDistancePerSec'] = all_data['walkDistance'] \/ all_data['matchDuration']\n\nfillInf(all_data, 0)","c48772c4":"all_data.drop(['boosts','heals','killStreaks','DBNOs'], axis=1, inplace=True)\nall_data.drop(['headshotKills','roadKills','vehicleDestroys'], axis=1, inplace=True)\nall_data.drop(['rideDistance','swimDistance','matchDuration'], axis=1, inplace=True)\nall_data.drop(['rankPoints','killPoints','winPoints'], axis=1, inplace=True)\n","cfee717a":"match = all_data.groupby(['matchId'])\ngroup = all_data.groupby(['matchId','groupId','matchType'])\n\n# target feature (max, min)\nagg_col = list(all_data.columns)\nexclude_agg_col = ['Id','matchId','groupId','matchType','maxPlace','numGroups','winPlacePerc']\nfor c in exclude_agg_col:\n    agg_col.remove(c)\nprint(agg_col)\n\n# target feature (sum)\nsum_col = ['kills','killPlace','damageDealt','walkDistance','_healthItems']","259b2324":"''' match sum, match max, match mean, group sum\n'''\nmatch_data = pd.concat([\n    match.size().to_frame('m.players'), \n    match[sum_col].sum().rename(columns=lambda s: 'm.sum.' + s), \n    match[sum_col].max().rename(columns=lambda s: 'm.max.' + s),\n    match[sum_col].mean().rename(columns=lambda s: 'm.mean.' + s)\n    ], axis=1).reset_index()\nmatch_data = pd.merge(match_data, \n    group[sum_col].sum().rename(columns=lambda s: 'sum.' + s).reset_index())\nmatch_data = reduce_mem_usage(match_data)\n\nprint(match_data.shape)","b3830213":"''' ranking of kills and killPlace in each match\n'''\nminKills = all_data.sort_values(['matchId','groupId','kills','killPlace']).groupby(\n    ['matchId','groupId','kills']).first().reset_index().copy()\nfor n in np.arange(4):\n    c = 'kills_' + str(n) + '_Place'\n    nKills = (minKills['kills'] == n)\n    minKills.loc[nKills, c] = minKills[nKills].groupby(['matchId'])['killPlace'].rank().values\n    match_data = pd.merge(match_data, minKills[nKills][['matchId','groupId',c]], how='left')\n    #match_data[c].fillna(0, inplace=True)\nmatch_data = reduce_mem_usage(match_data)\ndel minKills, nKills\n\nprint(match_data.shape)","c63aebf0":"match_data.head()\n","2579a20c":"''' group mean, max, min\n'''\nall_data = pd.concat([\n    group.size().to_frame('players'),\n    group.mean(),\n    group[agg_col].max().rename(columns=lambda s: 'max.' + s),\n    group[agg_col].min().rename(columns=lambda s: 'min.' + s),\n    ], axis=1).reset_index()\nall_data = reduce_mem_usage(all_data)\n\nprint(all_data.shape)","10ffb03a":"numcols = all_data.select_dtypes(include='number').columns.values\nnumcols = numcols[numcols != 'winPlacePerc']","1291a88f":"''' match summary, max\n'''\nall_data = pd.merge(all_data, match_data)\ndel match_data\ngc.collect()\n\nall_data['enemy.players'] = all_data['m.players'] - all_data['players']\nfor c in sum_col:\n    #all_data['enemy.' + c] = (all_data['m.sum.' + c] - all_data['sum.' + c]) \/ all_data['enemy.players']\n    #all_data['p.sum_msum.' + c] = all_data['sum.' + c] \/ all_data['m.sum.' + c]\n    #all_data['p.max_mmean.' + c] = all_data['max.' + c] \/ all_data['m.mean.' + c]\n    all_data['p.max_msum.' + c] = all_data['max.' + c] \/ all_data['m.sum.' + c]\n    all_data['p.max_mmax.' + c] = all_data['max.' + c] \/ all_data['m.max.' + c]\n    all_data.drop(['m.sum.' + c, 'm.max.' + c], axis=1, inplace=True)\n    \nfillInf(all_data, 0)\nprint(all_data.shape)","1129c004":"''' match rank\n'''\nmatch = all_data.groupby('matchId')\nmatchRank = match[numcols].rank(pct=True).rename(columns=lambda s: 'rank.' + s)\nall_data = reduce_mem_usage(pd.concat([all_data, matchRank], axis=1))\nrank_col = matchRank.columns\ndel matchRank\ngc.collect()\n\n# instead of rank(pct=True, method='dense')\nmatch = all_data.groupby('matchId')\nmatchRank = match[rank_col].max().rename(columns=lambda s: 'max.' + s).reset_index()\nall_data = pd.merge(all_data, matchRank)\nfor c in numcols:\n    all_data['rank.' + c] = all_data['rank.' + c] \/ all_data['max.rank.' + c]\n    all_data.drop(['max.rank.' + c], axis=1, inplace=True)\ndel matchRank\ngc.collect()\n\nprint(all_data.shape)","bf133a6e":"killMinorRank = all_data[['matchId','min.kills','max.killPlace']].copy()\ngroup = killMinorRank.groupby(['matchId','min.kills'])\nkillMinorRank['rank.minor.maxKillPlace'] = group.rank(pct=True).values\nall_data = pd.merge(all_data, killMinorRank)\n\nkillMinorRank = all_data[['matchId','max.kills','min.killPlace']].copy()\ngroup = killMinorRank.groupby(['matchId','max.kills'])\nkillMinorRank['rank.minor.minKillPlace'] = group.rank(pct=True).values\nall_data = pd.merge(all_data, killMinorRank)\n\ndel killMinorRank\ngc.collect()","eb637272":"# drop constant column\nconstant_column = [col for col in all_data.columns if all_data[col].nunique() == 1]\nprint('drop columns:', constant_column)\nall_data.drop(constant_column, axis=1, inplace=True)","6cb26e9e":"'''\nsolo  <-- solo,solo-fpp,normal-solo,normal-solo-fpp\nduo   <-- duo,duo-fpp,normal-duo,normal-duo-fpp,crashfpp,crashtpp\nsquad <-- squad,squad-fpp,normal-squad,normal-squad-fpp,flarefpp,flaretpp\n'''\nall_data['matchType'] = all_data['matchType'].apply(mapper)\n\nall_data = pd.concat([all_data, pd.get_dummies(all_data['matchType'])], axis=1)\nall_data.drop(['matchType'], axis=1, inplace=True)\n\nall_data['matchId'] = all_data['matchId'].apply(lambda x: int(x,16))\nall_data['groupId'] = all_data['groupId'].apply(lambda x: int(x,16))","f4520f21":"null_cnt = all_data.isnull().sum().sort_values()\nprint(null_cnt[null_cnt > 0])","158c81ab":"#all_data.drop([],axis=1,inplace=True)\n\ncols = [col for col in all_data.columns if col not in ['Id','matchId','groupId']]\nfor i, t in all_data.loc[:, cols].dtypes.iteritems():\n    if t == object:\n        all_data[i] = pd.factorize(all_data[i])[0]\n\nall_data = reduce_mem_usage(all_data)\nall_data.head()","603cf641":"X_train = all_data[all_data['winPlacePerc'].notnull()].reset_index(drop=True)\nX_test = all_data[all_data['winPlacePerc'].isnull()].drop(['winPlacePerc'], axis=1).reset_index(drop=True)\ndel all_data\ngc.collect()\n\nY_train = X_train.pop('winPlacePerc')\nX_test_grp = X_test[['matchId','groupId']].copy()\ntrain_matchId = X_train['matchId']\n\n# drop matchId,groupId\nX_train.drop(['matchId','groupId'], axis=1, inplace=True)\nX_test.drop(['matchId','groupId'], axis=1, inplace=True)\n\nprint(X_train.shape, X_test.shape)","c54fa700":"print(pd.DataFrame([[val for val in dir()], [sys.getsizeof(eval(val)) for val in dir()]],\n                   index=['name','size']).T.sort_values('size', ascending=False).reset_index(drop=True)[:10])","a0c67a1f":"from sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import minmax_scale\nimport lightgbm as lgb\n\nparams={'learning_rate': 0.1,\n        'objective':'mae',\n        'metric':'mae',\n        'num_leaves': 31,\n        'verbose': 1,\n        'random_state':42,\n        'bagging_fraction': 0.7,\n        'feature_fraction': 0.7\n       }\n\nreg = lgb.LGBMRegressor(**params, n_estimators=10000)\nreg.fit(X_train, Y_train)\npred = reg.predict(X_test, num_iteration=reg.best_iteration_)","6fcbe440":"# Plot feature importance\nfeature_importance = reg.feature_importances_\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\nsorted_idx = sorted_idx[len(feature_importance) - 30:]\npos = np.arange(sorted_idx.shape[0]) + .5\n\nplt.figure(figsize=(12,8))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_train.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","9e493f0a":"X_train.columns[np.argsort(-feature_importance)].values\n","95c1a848":"X_test_grp['_nofit.winPlacePerc'] = pred\n\ngroup = X_test_grp.groupby(['matchId'])\nX_test_grp['winPlacePerc'] = pred\nX_test_grp['_rank.winPlacePerc'] = group['winPlacePerc'].rank(method='min')\nX_test = pd.concat([X_test, X_test_grp], axis=1)","348ffc38":"fullgroup = (X_test['numGroups'] == X_test['maxPlace'])\n\n# full group (201366) --> calculate from rank\nsubset = X_test.loc[fullgroup]\nX_test.loc[fullgroup, 'winPlacePerc'] = (subset['_rank.winPlacePerc'].values - 1) \/ (subset['maxPlace'].values - 1)\n\n# not full group (684872) --> align with maxPlace\nsubset = X_test.loc[~fullgroup]\ngap = 1.0 \/ (subset['maxPlace'].values - 1)\nnew_perc = np.around(subset['winPlacePerc'].values \/ gap) * gap  # half&up\nX_test.loc[~fullgroup, 'winPlacePerc'] = new_perc\n\nX_test['winPlacePerc'] = X_test['winPlacePerc'].clip(lower=0,upper=1)","85c3fc5e":"# edge cases\nX_test.loc[X_test['maxPlace'] == 0, 'winPlacePerc'] = 0\nX_test.loc[X_test['maxPlace'] == 1, 'winPlacePerc'] = 1  # nothing\nX_test.loc[(X_test['maxPlace'] > 1) & (X_test['numGroups'] == 1), 'winPlacePerc'] = 0\nX_test['winPlacePerc'].describe()","024d0daa":"test = pd.read_csv('..\/input\/test_V2.csv')\ntest['matchId'] = test['matchId'].apply(lambda x: int(x,16))\ntest['groupId'] = test['groupId'].apply(lambda x: int(x,16))\n\nsubmission = pd.merge(test, X_test[['matchId','groupId','winPlacePerc']])\nsubmission = submission[['Id','winPlacePerc']]\nsubmission.to_csv(\"submission103.csv\", index=False)","9577fab9":"# Feature Engineering","393cacc3":"## Solutions this feature provide\n\nFirst of all by now you might have imagined immense correlation between 'PlayerAliveTime' with 'winPlacePerc'. But let me point to things this can explain.\n\n*  We will be able to understand those few players' rankings who did very poor in the game but still are rated very high. Usually your better teammate completes the game at the better stage than you died\/left, so yes having good teammate really matters.\n*  We will be able to find positive correlation between highly active player with the score.\n\n### Disadvantage\n\nThere are few numbers of players who play with extreme caution, hiding in buildings, grass and stay hidden unless forced otherwise. Our secret feature will off course give us less time as alive time.\n\nBut according to me this activity automatically balances user to the other side, since he\/she usually recive less score in other areas (such as killing, damage etc).","75c56b99":"Here you can also see the top features according to Pubg. And **'Survive'** out of them.","fba23429":"## Trying to find for how much time player was alive","44f8b6a8":"## grouping\n\n* need to predict the order of places for groups within each match.\n* train on group-level instead of the user-level","46d98230":"![](http:\/\/)![Untitled.jpg](attachment:Untitled.jpg)","70c79638":"There is no feature in the dataset which suggests time entity, except for '*matchDuration*'. So how can we possibly find, for how much time, player was alive?\n\nWe know simple formula:","067076da":"## aggregate feature","f22b4dbc":"To talk about this, I want to start the reverse approach of this game. We have done seen really nice analysis so far which tells us features positively and negatively correlated to '**winning**' which are essentially the actions player performed.","192b4a2f":"## rank as percent","633e2702":"So far most of us understand the way this game is played, many kernels have done extraordinary analysis and indeed provided lot of insights even to the real life PubG Gamer. Features provided in the dataset are quite sufficient to extract most of the juice out, but according to me, one essential key is missing.","0f032446":"## drop feature","ee054a9e":"## encode","bb0b111f":"### Finding Running Time","4bc5509a":"We know from the dataset for how much distance player has covered while running, driving\/riding, swimming etc. If we know, by what speed player has done this activity, we have for how long (in time) player did that activity.","51739ccd":"## killPlace rank of group and kills","e04a724b":"There are huge varients in running speed as follows:\n\n![Untitled.png](attachment:Untitled.png)\n![Untitled2.png](attachment:Untitled2.png)\n\nsource: https:\/\/pubg.gamepedia.com\/Movement_Speed\n\nStanding Sprint Speed = 6.3 m\/s (Baseline)\n\nEven though, it matters on many variables that if the plyer is running with\/without weapon, or if he\/she is crowching\/crawling, we assume the best posssible speed for simplicity.\n\n                        Running Time(in seconds) = Distance covered by player(in meters) \/ Speed(m\/s)\n                        Running time = distance\/(6.3 m\/s)","92712aa6":"## delete feature","bfc46f3b":"### Finding Driving Time\n\nAs you might have guessed, many types and speeds:\n\n![Untitle2d.jpg](attachment:Untitle2d.jpg)\n\nSource: https:\/\/pubg.gamepedia.com\/Vehicles\n\nWe assumed the on an average people drive UAZ (without boost).\nNeed to convert km\/s into m\/s - \n130Km\/Hr = 36.111 m\/s\n\n                                                Speed of car = 36.111 m\/s","3f322cea":"# Data Analysis","12a833da":"![Untitled1.jpg](attachment:Untitled1.jpg)","a1af73bd":"## new feature","ade6bfca":"## Id, groupId, matchId","dace9bc8":"**Hopefully this feature will enhance your model.**\nI would love to know if this feature has enhanced your model.\nHappy Kaggeling!","8baf2bee":"# Predict","5ec88d53":"Here is the final screen after you die in the game or after you servived to the last man standing(or possibly last team standing).\n\nOur features are provided here as a measure of our 'performance'. It explains alot, and so does our data. The most important feature here that is being missed here is **Survival Rating**. \n\nBy the term 'Survival', it suggests for how much time, you managed to stay alive.","beea1a90":"# **Secret feature that everyone seems to be missing**","ed2eb4f9":"# Submit","1106d782":"# Load Data","933d53bb":"## distance","7364f09f":"### Finding time taken to perform other activities\n\n* Swimming Speed = Assumed that walking speed and swimming speed is equal.\n\n#### Below Features need simple multiplication with how many times activity was done\n\n* Boosts Speed = 4 sec assumed (we specifically know what type of boost will take how many seconds, but there are many types of boosts fall in this category having different values, and hence the assumptions)\n* Heals Speed = 6 sec assumed\n* DBNO Speed = 2 sec assumed\n* Headshot Speed = 2 sec assumed\n* Revive Speed = 10 sec observed\n* Weapon aquiring Speed = 1 sec\n\nSo the total time in secconds where player was alive is greater than or equal to sum of time spent by player performing all above activities.\n(It is perfectly safe to assume that most of the activities mentioned above can be done one at a time.)\n\n    df['playerAliveTime'] =  (df['assists']*2) + \n                            (df['boosts']*4) + \n                            (df['damageDealt']\/100)  + \n                            (df['DBNOs']*2) + \n                            (df['headshotKills']*1) + \n                            (df['heals']*6) + \n                            (df['kills']*2) + \n                            (df['revives']*10) + \n                            (df['rideDistance']\/36.111) + \n                            (df['swimDistance']\/6.3) + \n                            (df['vehicleDestroys']*6) + \n                            (df['walkDistance']\/6.3) + \n                            (df['weaponsAcquired']*1)","c91a516b":"## Conclusion\n\nWhen applied this feature, I have observed that there was huge correlation between  'PlayerAliveTime' with 'winPlacePerc'.\n\n(Will share correlation matrix shortly)\n\nIn model building feature imprtance index, I have found this reature rated in top 3.\n\n","9c08e1b4":"![Screenshot_20180707-002758.jpg](attachment:Screenshot_20180707-002758.jpg)"}}