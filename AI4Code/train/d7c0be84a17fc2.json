{"cell_type":{"77956077":"code","8ebacdf2":"code","8e2b250a":"code","aac50522":"code","6a675729":"code","c1c3c282":"code","3a0ebee4":"code","0e1d21bb":"code","21458f23":"code","defccb18":"code","c874bb39":"code","495d0407":"code","dcf64ef9":"code","d8d1f645":"code","a0cab843":"code","9394b6d9":"code","d7b0d260":"code","aba88b33":"code","1fa4b616":"code","52ed512f":"code","62998ede":"code","60285b50":"code","a96a3c68":"code","98490fba":"code","8c9c9533":"code","6d78baf9":"code","08717dd0":"code","0292fb0d":"code","21989be6":"code","1cfdd38d":"code","0a1c7824":"code","f17637eb":"code","949719f8":"code","71f55ec3":"code","132bdb8a":"code","4bbaf7ea":"code","df8349f3":"code","5727889a":"code","11a64268":"code","43dc1685":"code","38f7deb3":"code","ea0b52f4":"code","bfcbf5ce":"code","370d8be3":"code","7c03c3fb":"code","d61b6d0e":"code","0aea32e2":"code","a6481a86":"code","10d9b3c8":"code","ba53c0bd":"code","89ad5618":"code","2e228661":"code","2eec494d":"code","22a743c4":"code","468993bc":"code","382392fe":"code","5d69d7bb":"code","f0a9879d":"code","0b7bf648":"code","980afc7e":"code","4ad15fa8":"code","b0bbae39":"code","2b4e3906":"code","71ab625f":"markdown","fa40e8d9":"markdown","c8179db8":"markdown","aa1abe02":"markdown","318015dc":"markdown","ef166a04":"markdown","6ed4bf94":"markdown","d7f183d9":"markdown","b7585827":"markdown","505bab9a":"markdown"},"source":{"77956077":"import numpy as np\nimport pandas as pd\nimport pydicom\nimport os\nimport seaborn as sns\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder,normalize\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport category_encoders as ce\nfrom keras.optimizers import Adam\n\nimport cv2\n\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use('seaborn-darkgrid')","8ebacdf2":"import tensorflow as tf\n\n# GPU device Check.\ndevice_name = tf.test.gpu_device_name()\nif device_name == '\/device:GPU:0':\n    print('Found GPU at: {}'.format(device_name))\nelse:\n    raise SystemError('GPU device not found')","8e2b250a":"import torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # PyTorch use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","aac50522":"train = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/train.csv')\ntest = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')","6a675729":"sub = pd.read_csv(\"..\/input\/osic-pulmonary-fibrosis-progression\/sample_submission.csv\")\nsub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nsub =  sub[['Patient','Weeks','Confidence','Patient_Week']]\nsub = sub.merge(test.drop('Weeks', axis=1), on=\"Patient\")","c1c3c282":"sub","3a0ebee4":"train.head(100)","0e1d21bb":"test.head()","21458f23":"print(train.shape, test.shape)","defccb18":"print(train.isnull().sum(),'\\n')\nprint(test.isnull().sum())","c874bb39":"train.info()","495d0407":"plt.figure(figsize=(16,10))","dcf64ef9":"sns.barplot(train['Sex'].value_counts().index, train['Sex'].value_counts())","d8d1f645":"sns.barplot(train['SmokingStatus'].value_counts().index, train['SmokingStatus'].value_counts())","a0cab843":"sa = pd.crosstab(train['SmokingStatus'],train['Sex'])\nsa.plot(kind=\"bar\",title='No of passengers survived')\nplt.show()","9394b6d9":"plt.rcParams['figure.figsize'] = 10,5\nax = train['Age'].hist(bins = 15,alpha = 0.9, color = 'green')\nax.set(xlabel = 'Age',ylabel = 'Count',title = 'Visualization of Ages')\nplt.show()","d7b0d260":"plt.rcParams['figure.figsize'] = 10,5\nax = train['Weeks'].hist(bins = 15,alpha = 0.9, color = 'green')\nax.set(xlabel = 'Weeks',ylabel = 'Count',title = 'Visualization of Ages')\nplt.show()","aba88b33":"plt.scatter(train['Weeks'],train['FVC'])","1fa4b616":"plt.scatter(train['Age'],train['FVC'])","52ed512f":"plt.rcParams['figure.figsize'] = 10,10\nsb.heatmap(train.corr(),annot = True,square = True,linewidths = 2,linecolor = 'black')","62998ede":"train[train['FVC'] == train['FVC'].max()]","60285b50":"train[train['FVC'] == train['FVC'].min()]","a96a3c68":"imdir_max = \"\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/train\/ID00219637202258203123958\"\nimdir_min = \"\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/train\/ID00225637202259339837603\"\n\nfig=plt.figure(figsize=(12, 12))\n\ncolumns = 4\nrows = 5\n\nimglist = os.listdir(imdir_max)\n\n\nfor i in range(1, columns*rows +1):\n    filename = imdir_max + \"\/\" + str(i) + \".dcm\"\n    ds = pydicom.dcmread(filename)\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(ds.pixel_array, cmap='jet')\nplt.show()","98490fba":"fig=plt.figure(figsize=(12, 12))\n\ncolumns = 4\nrows = 5\n\nimglist = os.listdir(imdir_min)\n\n\nfor i in range(1, columns*rows +1):\n    filename = imdir_min + \"\/\" + str(i) + \".dcm\"\n    ds = pydicom.dcmread(filename)\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(ds.pixel_array, cmap='jet')\nplt.show()","8c9c9533":"train['Patient_Week'] = train['Patient'].astype(str) + '_' + train['Weeks'].astype(str)\ntrain.head()","6d78baf9":"test['Patient_Week'] = test['Patient'].astype(str) + '_' + test['Weeks'].astype(str)\ntest.head()","08717dd0":"print(train.shape)\nprint(test.shape)\ntrain","0292fb0d":"# construct train input\n\noutput = pd.DataFrame()\ngb = train.groupby('Patient')\ntk0 = tqdm(gb, total=len(gb))\nfor _, usr_df in tk0:\n    usr_output = pd.DataFrame()\n    for week, tmp in usr_df.groupby('Weeks'):\n        rename_cols = {'Weeks': 'base_Week', 'FVC': 'base_FVC', 'Percent': 'base_Percent', 'Age': 'base_Age'}\n        tmp = tmp.drop(columns='Patient_Week').rename(columns=rename_cols)\n        drop_cols = ['Age', 'Sex', 'SmokingStatus', 'Percent']\n        _usr_output = usr_df.drop(columns=drop_cols).rename(columns={'Weeks': 'predict_Week'}).merge(tmp, on='Patient')\n        _usr_output['Week_passed'] = _usr_output['predict_Week'] - _usr_output['base_Week']\n        usr_output = pd.concat([usr_output, _usr_output])\n    output = pd.concat([output, usr_output])\n\ntrain = output[output['Week_passed']!=0].reset_index(drop=True)\n\ntrain.head()","21989be6":"# construct test input\n\ntest = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')\\\n        .rename(columns={'Weeks': 'base_Week', 'FVC': 'base_FVC', 'Percent': 'base_Percent', 'Age': 'base_Age'})\nsubmission = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/sample_submission.csv')\nsubmission['Patient'] = submission['Patient_Week'].apply(lambda x: x.split('_')[0])\nsubmission['predict_Week'] = submission['Patient_Week'].apply(lambda x: x.split('_')[1]).astype(int)\ntest = submission.drop(columns=['FVC', 'Confidence']).merge(test, on='Patient')\ntest['Week_passed'] = test['predict_Week'] - test['base_Week']\n\ntest.head()","1cfdd38d":"print(submission.shape)\nprint(test.shape)\nprint(train.shape)","0a1c7824":"print(train.isnull().sum(),'\\n')\nprint(test.isnull().sum())","f17637eb":"train.set_index(['Patient_Week'],inplace = True)\n\ntest.set_index(['Patient_Week'],inplace = True)","949719f8":"train","71f55ec3":"y = train['FVC']\nX = train.drop(['FVC'], axis=1)\nX = X.drop(['Patient'], axis=1)\n\n#y_test = test['FVC']\ntest_X = test.drop(['Patient'], axis=1)\n#test_X = test.drop(['FVC'], axis=1)","132bdb8a":"X","4bbaf7ea":"# getting dummy variables column\n\nenc = LabelEncoder()\n\nX['Sex'] = enc.fit_transform(X['Sex'])\n\nX['SmokingStatus'] = enc.fit_transform(X['SmokingStatus'])","df8349f3":"X","5727889a":"test_X['Sex'] = enc.fit_transform(test_X['Sex'])\ntest_X['SmokingStatus'] = enc.fit_transform(test_X['SmokingStatus'])","11a64268":"test_X","43dc1685":"y","38f7deb3":"#Normalizing\n\nfrom sklearn.preprocessing import normalize\n\nX = normalize(X)\ntest_X = normalize(test_X)","ea0b52f4":"sub = test_X \npe = np.zeros((test_X.shape[0], 3)) #for predict test\n# pred = np.zeros((X.shape[0], 3)) #for predict val\nze = normalize(sub)","bfcbf5ce":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\n\n# C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n# #=============================#\n# def score(y_true, y_pred):\n#     tf.dtypes.cast(y_true, tf.float32)\n#     tf.dtypes.cast(y_pred, tf.float32)\n#     sigma = y_pred[:, 2] - y_pred[:, 0]\n#     fvc_pred = y_pred[:, 1]\n    \n#     #sigma_clip = sigma + C1\n#     sigma_clip = tf.maximum(sigma, C1)\n#     delta = tf.abs(y_true[:, 0] - fvc_pred)\n#     delta = tf.minimum(delta, C2)\n#     sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n#     metric = (delta \/ sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n#     return K.mean(metric)\n# #============================#\n# def qloss(y_true, y_pred):\n#     # Pinball loss for multiple quantiles\n#     qs = [0.2, 0.50, 0.8]\n#     q = tf.constant(np.array([qs]), dtype=tf.float32)\n#     e = y_true - y_pred\n#     v = tf.maximum(q*e, (q-1)*e)\n#     return K.mean(v)\n# #=============================#\n# def mloss(_lambda):\n#     def loss(y_true, y_pred):\n#         return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n#     return loss\n# #=================\ndef make_model():\n    z = L.Input((8,), name=\"Patient\")\n    x = L.Dense(1000, activation=\"relu\", name=\"d1\")(z)\n    x = L.Dense(1000, activation=\"relu\", name=\"d2\")(x)\n    x = L.Dense(500, activation=\"relu\", name=\"d3\")(x)\n    x = L.Dense(500, activation=\"relu\", name=\"d4\")(x)\n    p1 = L.Dense(3, activation=\"relu\", name=\"p1\")(x)\n    p2 = L.Dense(3, activation=\"relu\", name=\"p2\")(x)\n    preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n                     name=\"preds\")([p1, p2])\n    \n    model = M.Model(z, preds, name=\"CNN\")\n    #model.compile(loss=qloss, optimizer=\"adam\", metrics=[score])\n    opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01, amsgrad=False)\n    model.compile(loss = 'binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    return model \n\n \nnet = make_model()\nnet.summary()","370d8be3":"from sklearn.svm import SVR\nfrom sklearn.model_selection import KFold\n\nscores = []\n\nNFOLD = 5\nkf = KFold(n_splits=NFOLD)\nBATCH_SIZE = 8","7c03c3fb":"%%time\ncnt = 0\nfor tr_idx, val_idx in kf.split(X):  \n    cnt += 1\n    print(f\"FOLD {cnt}\")\n    net = make_model()  \n    net.fit(X[tr_idx], y[tr_idx], batch_size=BATCH_SIZE, epochs=400, \n            validation_data=(X[val_idx], y[val_idx]), verbose=0) #\n#     print(\"train\", net.evaluate(X[tr_idx], y[tr_idx], verbose=0, batch_size=BATCH_SIZE))\n#     print(\"val\", net.evaluate(X[val_idx], y[val_idx], verbose=0, batch_size=BATCH_SIZE))\n#     print(\"predict val...\")\n#     pred[val_idx] = net.predict(X[val_idx], batch_size=BATCH_SIZE, verbose=0)    \n    print(\"predict test...\")   \n    pe += net.predict(ze, batch_size=BATCH_SIZE, verbose=0) \/ NFOLD ","d61b6d0e":"# #K Fold Cross Validation\n  \n# from sklearn.model_selection import KFold \n# import xgboost as xgb\n\n\n# kf = KFold(n_splits=20, random_state=42, shuffle=True)\n\n# for train_index, val_index in kf.split(X):\n#     print(\"TRAIN:\", train_index, \"TEST:\", val_index)\n#     X_train, X_val = X[train_index], X[val_index]\n#     y_train, y_val = y[train_index], y[val_index]","0aea32e2":"# dtrain = xgb.DMatrix(X_train, label=y_train)\n# dvalid = xgb.DMatrix(X_val, label=y_val)\n# watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# xgb_pars = {'min_child_weight': 10, 'eta': 0.04, 'colsample_bytree': 0.8, 'max_depth': 15,\n#             'subsample': 0.75, 'lambda': 2, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1, 'gamma' : 0,\n#             'eval_metric': 'rmse', 'objective': 'reg:linear'}    \n\n# model = xgb.train(xgb_pars, dtrain, 500, watchlist, early_stopping_rounds=250,\n#                   maximize=False, verbose_eval=15) ","a6481a86":"# ze = normalize(test_X)\n# ze","10d9b3c8":"# dtest = xgb.DMatrix(ze)\n\n# pred = model.predict(dtest)","ba53c0bd":"# #K Fold Cross Validation\n\n# from sklearn.model_selection import KFold\n\n\n# kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n\n# for train_index, val_index in kf.split(X):\n#     print(\"TRAIN:\", train_index, \"TEST:\", val_index)\n#     X_train, X_val = X[train_index], X[val_index]\n#     y_train, y_val = y[train_index], y[val_index]","89ad5618":"# print(X_train.shape)\n# print(y_train.shape)\n# print(X_val.shape)\n# print(y_val.shape)\n\n# #reshape for rnn\n\n# X_train = X_train.reshape(-1, 1, 8)\n# X_val  = X_val.reshape(-1, 1, 8)\n# y_train = y_train.values #convert pd to array\n# y_train = y_train.reshape(-1, 1,)\n# y_val = y_val.values #convert pd to array\n# y_val = y_val.reshape(-1, 1,)\n\n# print(X_train.shape)\n# print(y_train.shape)\n# print(X_val.shape)\n# print(y_val.shape)","2e228661":"# from tensorflow.keras.layers import Conv2D,LSTM,LeakyReLU, MaxPooling2D,Concatenate,Input, Dropout, Flatten, Dense, GlobalAveragePooling2D,Activation, BatchNormalization\n# from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n# from tensorflow.keras.models import Model\n\n\n#   # create model\n    \n\n# #input \n# input_layer = Input(shape=(1,8))\n# main_rnn_layer = LSTM(64, return_sequences=True, recurrent_dropout=0.2)(input_layer)\n\n    \n# #output\n# rnn = LSTM(32)(main_rnn_layer)\n# dense = Dense(128)(rnn)\n# dropout_c = Dropout(0.3)(dense)\n# dense = Dense(64)(dropout_c)\n# dropout_c = Dropout(0.3)(dense)\n# dense = Dense(32)(dropout_c)\n# dropout_c = Dropout(0.3)(dense)\n\n# classes = Dense(1, activation= LeakyReLU(alpha=0.1),name=\"class\")(dropout_c)\n\n# model = Model(input_layer, classes)\n\n# # Compile model\n# callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=1, factor=0.6),\n#              EarlyStopping(monitor='val_loss', patience=20),\n#              ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n# model.compile(loss=[tf.keras.losses.MeanSquaredLogarithmicError(),tf.keras.losses.MeanSquaredLogarithmicError()], optimizer=\"adam\")\n\n\n# model.summary()\n# # Fit the model\n# history = model.fit(X_train, y_train, \n#           epochs = 250, \n#           batch_size = 8, \n#           validation_data=(X_val,  y_val), \n#           callbacks=callbacks)","2eec494d":"# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('Loss over epochs')\n# plt.ylabel('Loss')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Validation'], loc='best')\n# plt.show()","22a743c4":"# model.load_weights(\"best_model.h5\")\n\n# test_X = test_X.reshape(-1, 1,8)\n\n\n# predictions = model.predict(test_X)","468993bc":"print(pe.shape)","382392fe":" pe[:,0].shape","5d69d7bb":"test","f0a9879d":"submis = test.copy() #for Patient_Week\nsubmis","0b7bf648":"import math\n\n#calcule Confidence\n \nsubmis['FVC_pred'] = pe[:,0]\n\n# baseline score\nsubmis['Confidence'] = 100\nsubmis['sigma_clipped'] = submis['Confidence'].apply(lambda x: max(x, 70))\nsubmis['diff'] = abs(submis['base_FVC'] - submis['FVC_pred'])\nsubmis['delta'] = submis['diff'].apply(lambda x: min(x, 1000))\nsubmis['score'] = -math.sqrt(2)*submis['delta']\/submis['sigma_clipped'] - np.log(math.sqrt(2)*submis['sigma_clipped'])\nscore = submis['score'].mean()\nprint(score)","980afc7e":"import scipy as sp\nfrom functools import partial\n\ndef loss_func(weight, row):\n    confidence = weight\n    sigma_clipped = max(confidence, 70)\n    diff = abs(row['base_FVC'] - row['FVC_pred'])\n    delta = min(diff, 1000)\n    score = -math.sqrt(2)*delta\/sigma_clipped - np.log(math.sqrt(2)*sigma_clipped)\n    return -score\n\nresults = []\ntk0 = tqdm(submis.iterrows(), total=len(submis))\nfor _, row in tk0:\n    loss_partial = partial(loss_func, row=row)\n    weight = [100]\n    #bounds = [(70, 100)]\n    #result = sp.optimize.minimize(loss_partial, weight, method='SLSQP', bounds=bounds)\n    result = sp.optimize.minimize(loss_partial, weight, method='SLSQP')\n    x = result['x']\n    results.append(x[0])","4ad15fa8":"# optimized score\nsubmis['Confidence'] = results\nsubmis['sigma_clipped'] = submis['Confidence'].apply(lambda x: max(x, 70))\nsubmis['diff'] = abs(submis['base_FVC'] - submis['FVC_pred'])\nsubmis['delta'] = submis['diff'].apply(lambda x: min(x, 1000))\nsubmis['score'] = -math.sqrt(2)*submis['delta']\/submis['sigma_clipped'] - np.log(math.sqrt(2)*submis['sigma_clipped'])\nscore = submis['score'].mean()\nprint(score)","b0bbae39":"submis=submis.reset_index()\nsubmis","2b4e3906":"submis_final =  submis[['Patient_Week', 'FVC_pred', 'Confidence']].copy()\nsubmis_final = submis_final.rename(columns={\"FVC_pred\": \"FVC\"})\nsubmis_final['FVC'] = submis_final['FVC']\nsubmis_final.to_csv('submission.csv', index=False)\nsubmis_final","71ab625f":"The patient of Max FVC","fa40e8d9":"Submission File:\n*     Patient_Week,FVC,Confidence\n*     ID00002637202176704235138_1,2000,100\n*     ID00002637202176704235138_2,2000,100\n*     ID00002637202176704235138_3,2000,100\n\nconfidence = standard deviation \u03c3","c8179db8":"2-xgboost","aa1abe02":"Data Preparation","318015dc":"FVC : forced vital capacity, i.e. the volume of air exhaled","ef166a04":"1-DNN","6ed4bf94":"![](https:\/\/i.imgur.com\/EV7xPrl.png)","d7f183d9":"3-RNN","b7585827":"Modeling","505bab9a":"The patient of Min FVC"}}