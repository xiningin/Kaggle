{"cell_type":{"e6830061":"code","f36017e3":"code","b969becc":"code","86180393":"code","2b25f055":"code","5cd4e5d9":"code","bde75062":"code","93cd7785":"code","538c597a":"markdown","2f371d50":"markdown","723d9706":"markdown","e072d36d":"markdown","cceeddb7":"markdown","c72f22dd":"markdown","a0d43337":"markdown"},"source":{"e6830061":"import pandas as pd\nimport numpy as np\nfrom numpy import array\nfrom numpy import argmax\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM\n\nfilepath = 'd:\/AS_Data\/temp\/name_test.csv'\nmax_rows = 500000 # Reduction due to memory limitations\n\ndf = (pd.read_csv(filepath, usecols=['first_name', 'gender'])\n        .dropna(subset=['first_name', 'gender'])\n        .assign(first_name = lambda x: x.first_name.str.strip())\n        .head(max_rows))\n\n# In the case of a middle name, we will simply use the first name only\ndf['first_firstname'] = df['first_name'].apply(lambda x: str(x).split(' ', 1)[0])\n\n# Sometimes people only but the first letter of their name into the field, so we drop all name where len <3\ndf.drop(df[df['first_firstname'].str.len() < 3].index, inplace=True)","f36017e3":"# Parameters\npredictor_col = 'first_firstname'\nresult_col = 'gender'\n\naccepted_chars = 'abcdefghijklmnopqrstuvwxyz\u00f6\u00e4\u00fc-'\n\nword_vec_length = min(df[predictor_col].apply(len).max(), 25) # Length of the input vector\nchar_vec_length = len(accepted_chars) # Length of the character vector\noutput_labels = 2 # Number of output labels\n\nprint(f\"The input vector will have the shape {word_vec_length}x{char_vec_length}.\")","b969becc":"# Define a mapping of chars to integers\nchar_to_int = dict((c, i) for i, c in enumerate(accepted_chars))\nint_to_char = dict((i, c) for i, c in enumerate(accepted_chars))\n\n# Removes all non accepted characters\ndef normalize(line):\n    return [c.lower() for c in line if c.lower() in accepted_chars]\n\n# Returns a list of n lists with n = word_vec_length\ndef name_encoding(name):\n\n    # Encode input data to int, e.g. a->1, z->26\n    integer_encoded = [char_to_int[char] for i, char in enumerate(name) if i < word_vec_length]\n    \n    # Start one-hot-encoding\n    onehot_encoded = list()\n    \n    for value in integer_encoded:\n        # create a list of n zeros, where n is equal to the number of accepted characters\n        letter = [0 for _ in range(char_vec_length)]\n        letter[value] = 1\n        onehot_encoded.append(letter)\n        \n    # Fill up list to the max length. Lists need do have equal length to be able to convert it into an array\n    for _ in range(word_vec_length - len(name)):\n        onehot_encoded.append([0 for _ in range(char_vec_length)])\n        \n    return onehot_encoded\n\n# Encode the output labels\ndef lable_encoding(gender_series):\n    labels = np.empty((0, 2))\n    for i in gender_series:\n        if i == 'm':\n            labels = np.append(labels, [[1,0]], axis=0)\n        else:\n            labels = np.append(labels, [[0,1]], axis=0)\n    return labels","86180393":"# Split dataset in 60% train, 20% test and 20% validation\ntrain, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n\n# Convert both the input names as well as the output lables into the discussed machine readable vector format\ntrain_x =  np.asarray([np.asarray(name_encoding(normalize(name))) for name in train[predictor_col]])\ntrain_y = lable_encoding(train.gender)\n\nvalidate_x = np.asarray([name_encoding(normalize(name)) for name in validate[predictor_col]])\nvalidate_y = lable_encoding(validate.gender)\n\ntest_x = np.asarray([name_encoding(normalize(name)) for name in test[predictor_col]])\ntest_y = lable_encoding(test.gender)","2b25f055":"hidden_nodes = int(2\/3 * (word_vec_length * char_vec_length))\nprint(f\"The number of hidden nodes is {hidden_nodes}.\")","5cd4e5d9":"# Build the model\nprint('Build model...')\nmodel = Sequential()\nmodel.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=output_labels))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])","bde75062":"batch_size=1000\nmodel.fit(train_x, train_y, batch_size=batch_size, epochs=10, validation_data=(validate_x, validate_y))","93cd7785":"validate['predicted_gender'] = ['m' if prediction[0] > prediction[1] else 'f' for prediction in model.predict(validate_x)]\nvalidate[validate['gender'] != validate['predicted_gender']].head()","538c597a":"# Predicting Gender from the First Name with an LSTM using Keras\nIn this short article, I want to go over how we can use a very basic LSTM to predict the gender from a given first name. Since there are many great courses on the math and general concepts behind Recurring Neural Networks (RNN), e.g. [Andrew Ng's deep learning specialization](https:\/\/www.coursera.org\/specializations\/deep-learning) or on [Medium](https:\/\/medium.com\/search?q=lstm), I will not dig deeper into them and perceive this knowledge as given. Instead, we will only focus on the high-level implementation using Keras. The goal is to get a more practical understanding of decisions one has to make building a neural network like this, especially on how to chose some of the hyper parameters.\n\nOn Keras: Latest since its TensorFlow Support in 2017, [Keras](https:\/\/keras.io\/) has made a huge splash a relatively easy to use and intuitive interface into more complex machine learning libraries. As you will see, building the actual neural network, as well as training the model is going to be the shortest part in our script.\n\nThe first step is to determine the type of network we want to use since that decision can impact our data preparation process. In a name, the order of characters matters, meaning that, if we want to analyze a name using a neural network, RNN are the logical choice. Long Short-Term Memory (LSTM) as a special form of RNNs are especially powerful when it comes to finding the right features when the chain of input-chunks becomes longer. Input in our case is always a string (the name) and the output a 1x2 vector indicating if the name belongs to a male or a female person.\n\nAfter making this decision, We will start with loading all the packages that we will need as well as the dataset - a file containing over 1.5 Mio German users with their name and gender (encoded as <i>f<\/i> for female and <i>m<\/i> for male.","2f371d50":"## Preprocessing the data\nThe next step in any natural language processing is to convert the input into a machine-readable vector format. In theory, neural networks in Keras are able to handle inputs with a variable shape. In praxis, however, having a fixed input length in Keras can improve performance noticeably, especially during the training. The reason for this behavior is that fixed lengths allow for the creation of tensors of fixed shaped and therefore more stable weights.\n\nFirst, we will convert every (first) name into a vector. The method we'll be using is the so-called One-Hot Encoding. \nHere, every word is represented by a vector of n binary sub-vectors, where n is the number of different chars in the alphabet (26 using the English alphabet). The reason why we can not simply convert every character to its position in the alphabet, e.g. a - 1, b - 2 etc.) is that this would lead the network to assume that the characters are on an ordinal scale, instead of a categorical - z not is \"worth more\" than a.\n\nExample:<br>\ns becomes:<br>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n_hello_ becomes:<br>\n[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br>\n  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br>\n [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br>\n [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br>\n [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n\nNow that we determined how the input has to look like, we have two decisions to make: How long shall the char vector be (how many different chars do we allow for) and how long shall the name vector be (how many chars we want to look at). We will only allow for the most common characters in the German alphabet (standard latin + \u00f6\u00e4\u00fc) and the hyphen, which is part of many older names.\nFor simplicity purposes, we will set the length of the name vector to be the length of the longest name in our dataset, but with 25 as an upper bound to make sure our input vector doesn't grow too large just because one person made a mistake during the name entering the process.","723d9706":"Scikit-learn already incorporates a One Hot Encoding algorithm in it's preprocessing library. However, in this case, because of our special situation that we are not converting labels into vectors but split every string apart into its characters, the creation of a custom algorithm seemed to be quicker than the preprocessing otherwise needed.\n\n[It has been shown](https:\/\/www.draketo.de\/english\/python-memory-numpy-list-array) that Numpy arrays need around 4 times less memory compared to Python lists. For that reason, we use list comprehension as a more pythonic way of creating the input array but already convert every word vector into an array inside of the list. When working with Numpy arrays, we have to make sure that all lists and\/or arrays that are getting combined have the same shape. ","e072d36d":"As mentioned, the same uncertainty about the amount also exists for the number of hidden layers to use. Again, the ideal number for any given use case will be different and is best to be decided by running different models against each other. Generally, 2 layers have shown to be enough to detect more complex features. More layers can be better but also harder to train. As a general rule of thumb - 1 hidden layer work with simple problems, like this, and two are enough to find reasonably complex features. <br>\nIn our case, adding a second layer only improves the accuracy by ~0.2% (0.9807 vs. 0.9819) after 10 epochs.\n\n### Choosing additional Hyper-Parameters\n\nEvery LSTM layer should be accompanied by a Dropout layer. This layer will help to prevent overfitting by ignoring randomly selected neurons during training, and hence reduces the sensitivity to the specific weights of individual neurons. 20% is often used as a good compromise between retaining model accuracy and preventing overfitting.\n\nAfter our LSTM layer(s) did all the work to transform the input to make predictions towards the desired output possible, we have to reduce (or, in rare cases extend) the shape to match our desired output. In our case, we have two output labels and therefore we need two-output units. \n\nThe final layer to add is the activation layer. Technically, this can be included into the density layer, but there is a reason to split this apart. While not relevant here, splitting the density layer and the activation layer makes it possible to retrieve the reduced output of the density layer of the model. Which activation function to use is, again, depending on the application. For our problem at hand, we have multiple classes (male and female) but only one of the classes can be present at a time. For these types of problems, generally, the [softmax activation function](https:\/\/en.wikipedia.org\/wiki\/Softmax_function) works best, because it allows us (and your model) to interpret the outputs as probabilities.\n\nLoss function and activation function are often chosen together. Using the softmax activation function points us to cross-entropy as our preferred loss function or more precise the binary cross-entropy, since we are faced with a binary classification problem. Those two functions work well with each other because the cross-entropy function cancels out the plateaus at each end of the soft-max function and therefore speeds up the learning process.\n\nFor choosing the optimizer, adaptive moment estimation, short _Adam_, has been shown to work well in most practical applications and works well with only little changes in the hyperparameters. Last but not least we have to decide, after which metric we want to judge our model. Keras offered [multiple accuracy functions](https:\/\/keras.io\/metrics\/). In many cases, judging the models' performance from an overall _accuracy_ point of view will be the option easiest to interpret as well as sufficient in resulting model performance.\n\n## Building, training and evaluating the model\nAfter getting some intuition about how to chose the most important parameters, let's put them all together and train our model:","cceeddb7":"Looking at the results, at least some of the false predictions seem to occur for people that typed in their family name into the first name field. Seeing that, a good next step seems to clean the original dataset from those cases. For now, the result looks pretty promising. With the accuracy we can archive, this model could already be used in many real-world situations. Also, training the model for more epochs might increase its performance, important here is to looks out for the performance on the validation set to prevent a possible overfitting\n\n## Conclusion\nIn this article, we have successfully build a small model to predict the gender from a given (German) first name with an over 98% accuracy rate. While Keras frees us from writing complex deep learning algorithms, we still have to make choices regarding some of the hyperparameters along the way. In some cases, e.g. choosing the right activation function, we can rely on rules of thumbs or can determine the right parameter based on our problem. However, in some other cases, the best result will come from testing various configurations and then evaluating the outcome.","c72f22dd":"Now that we have our input ready, we can start building our neural network. We already decided on the model (LSTM). In Keras we can simply stack multiple layers on top of each other, for this we need to initialize the model as _Sequential()_.\n\n## Choosing the right amount of nodes and layers\n\nThere is no final, definite, rule of thumb on how many nodes (or hidden neurons) or how many layers you should choose, and very often a trial and error approach will give you the best results for your individual problem. The most common framework for this is most likely the [k-fold cross validation](http:\/\/en.wikipedia.org\/wiki\/Cross-validation_%28statistics%29#K-fold_cross-validation). However, even for a testing procedure, we need to choose some (k) numbers of nodes.\nThe following  [formula](https:\/\/stats.stackexchange.com\/a\/136542) can give you a starting point:\n$$N_h = \\frac{N_s} {(\\alpha * (N_i + N_o))}$$\n$N_i$ is the number of input neurons, $N_o$ the number of output neurons, $N_s$ the number of samples in the trainings data, and $\\alpha$ represents a scaling factor that is usually between 2 and 10. We can calculate 8 different numbers to feed into our valdiation procedure and find the optimal model, based on the resulting validation loss. \n\nIf the problem is simple and time an issue, there are various other rules of thumbs to determine the number of nodes, which are mostly simply based on the input and output neurons. We have to keep in mind that, while easy to use, they will rarely yield the optimal result. Here is just one example, which we will use for this basic model:\n$$N_h = \\frac{2} {3} *(N_i + N_o)$$","a0d43337":"An accuracy of 98.2% is pretty impressive and will most likely result from the fact that most names in the validation set were already present in our test set. Using our validation set we can take a quick look on where our model went wrong:"}}