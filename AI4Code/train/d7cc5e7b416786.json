{"cell_type":{"7935e366":"code","4ff1727f":"code","8d31ee89":"code","943204c1":"code","204841c9":"code","4370e8bc":"code","8c69510e":"code","a5367383":"code","54ea322e":"code","2108143b":"code","2f70366b":"code","88993669":"code","8fdd704d":"code","f414b801":"code","77efe2aa":"code","29611bdd":"code","27eba911":"code","ee798ec8":"code","2c323d7a":"code","24578830":"code","1c0c9d54":"code","06c38a4e":"code","dd0379ac":"code","5a143c04":"code","afe7fa76":"code","5f2286ba":"code","45b91c22":"markdown","0293fcd5":"markdown","d665e171":"markdown","ea3a2a84":"markdown","54f9faa9":"markdown","00759f9b":"markdown","445f5cb2":"markdown","9ab7ad75":"markdown","b915540c":"markdown","d77278cd":"markdown"},"source":{"7935e366":"import numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd\nimport seaborn as sns \nfrom sklearn.datasets import load_boston\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')","4ff1727f":"train = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')\nprint('train shape:',train.shape)\nprint('test shape:',test.shape)","8d31ee89":"# found that one of the test data is in the train set and removed it\n# Dropping the last row which is 2011-01-01 00:00:00\ntrain=train.loc[~(train['date_time']=='2011-01-01 00:00:00')].reset_index(drop=True)","943204c1":"print('train shape:',train.shape)","204841c9":"all_data = pd.concat([train, test])\n# convert to datatime format\nall_data['date_time'] = pd.to_datetime(all_data['date_time'])\nall_data.head()","4370e8bc":"fig,ax=plt.subplots(4,2,figsize=(20,15))\nfor i,col in enumerate(train.columns[1:9]):\n    ax[i%4][i\/\/4].hist(train[col],bins=40,color='darkblue',label=f'{col}')\n    ax[i%4][i\/\/4].set_title(f'Distribution of {col}',fontsize=15)\n    ax[i%4][i\/\/4].set_xlabel(f'{col}')\n    ax[i%4][i\/\/4].set_ylabel('Dist')\n    plt.subplots_adjust(hspace=0.45)","8c69510e":"fig,ax=plt.subplots(3,1,figsize=(8,10))\nfor i,col in enumerate(train.columns[9:12]):\n    ax[i%3].hist(train[col],bins=40,color='darkblue',label=f'{col}')\n    ax[i%3].set_title(f'Distribution of {col}',fontsize=15)\n    ax[i%3].set_xlabel(f'{col}')\n    ax[i%3].set_ylabel('Dist')\n    plt.subplots_adjust(hspace=0.45)","a5367383":"# \u5b57\u4e32\u8f49\u63db\u65e5\u671f\u683c\u5f0f\ntrain['date_time']=pd.to_datetime(train['date_time'],format='%Y-%m-%d %H:%M:%S')\ntest['date_time']=pd.to_datetime(test['date_time'],format='%Y-%m-%d %H:%M:%S')\n# Following code is inspired from - https:\/\/www.kaggle.com\/nroman\/eda-for-ashrae\nfig,ax=plt.subplots(1,1,figsize=(12,6))\ntrain[['date_time','deg_C']].set_index('date_time').resample('D').mean()['deg_C'].plot(ax=ax,label='by hour(train)',alpha=1,color='blue').set_ylabel('deg C',fontsize=10)\nax.set_title('Trend of Mean deg_C by Day',fontsize=12)\nax.set_xlabel('')","54ea322e":"fig,ax=plt.subplots(1,1,figsize=(12,6))\ntest[['date_time','deg_C']].set_index('date_time').resample('D').mean()['deg_C'].plot(ax=ax,label='by hour(train)',alpha=1,color='blue').set_ylabel('deg C',fontsize=10)\nax.set_title('Trend of Mean deg_C by Day',fontsize=12)\nax.set_xlabel('')","2108143b":"all_data.isnull().sum()","2f70366b":"# all_data['hr'] = all_data.date_time.dt.hour*60+all_data.date_time.dt.minute\n# all_data['day'] =all_data.date_time.dt.weekday\/\/5\n# all_data['satday'] = all_data.date_time.dt.weekday==5\n# all_data['hr1'] = all_data.date_time.dt.hour*60+all_data.date_time.dt.minute","88993669":"# all_data['year'] = all_data['date_time'].dt.year\n# all_data['month'] = all_data['date_time'].dt.month\n# all_data['week'] = all_data['date_time'].dt.week\n# all_data['day'] = all_data['date_time'].dt.day\n# all_data['dayofweek'] = all_data['date_time'].dt.dayofweek\n# all_data['hour'] = all_data['date_time'].dt.hour\n# # convert datetime to timestamp(s)\n# all_data['time'] = all_data['date_time'].astype(np.int64)\/\/10**9\n# all_data.drop(columns = 'date_time', inplace = True)\n# print('all_data shape:', all_data.shape)\n# all_data.head()","8fdd704d":"# The months will be used for folds split\nmonths = all_data[\"date_time\"].dt.month[:len(train)]\n## New idea\nall_data[\"hour\"] = all_data[\"date_time\"].dt.hour\nall_data[\"working_hours\"] =  all_data[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\nall_data[\"is_weekend\"] = (all_data[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\nall_data['hr'] = all_data.date_time.dt.hour*60+all_data.date_time.dt.minute\nall_data['satday'] = (all_data.date_time.dt.weekday==5).astype(\"int\")\n# add sensor shift\nall_data[\"s1-6\"] = all_data[\"sensor_1\"] - all_data[\"sensor_1\"].shift(periods=6, fill_value=0)\nall_data[\"s2-6\"] = all_data[\"sensor_2\"] - all_data[\"sensor_2\"].shift(periods=6, fill_value=0)\nall_data[\"s3-6\"] = all_data[\"sensor_3\"] - all_data[\"sensor_3\"].shift(periods=6, fill_value=0)\nall_data[\"s4-6\"] = all_data[\"sensor_4\"] - all_data[\"sensor_4\"].shift(periods=6, fill_value=0)\nall_data[\"s5-6\"] = all_data[\"sensor_5\"] - all_data[\"sensor_5\"].shift(periods=6, fill_value=0)\nall_data.drop(columns = 'hour', inplace = True)\n# convert datetime to timestamp(s)\nall_data['time'] = all_data['date_time'].astype(np.int64)\/\/10**9\nall_data.drop(columns = 'date_time', inplace = True)","f414b801":"X=all_data[:len(train)].drop(columns = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']).values\ny=all_data[:len(train)][['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']]\ny_log=np.log1p(y)\nX_test=all_data[len(train):].drop(columns = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']).values\nprint('X_train shape:', X.shape)\nprint('y_train shape:', y.shape)\nprint('X_test shape:', X_test.shape)","77efe2aa":"# load submission\npreds = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')","29611bdd":"# Sets of hyperparameters optimized by Optuna for each target\n# cb_params = [\n#                 {'learning_rate': 0.010169009412219588,\n#                  'l2_leaf_reg': 8.908337085912136,\n#                  'bagging_temperature': 8.384477224270551,\n#                  'random_strength': 1.950237493637981,\n#                  'depth': 6,\n#                  'grow_policy': 'Lossguide',\n#                  'leaf_estimation_method': 'Newton'},\n#                 {'learning_rate': 0.166394867169309,\n#                  'l2_leaf_reg': 8.704675157564441,\n#                  'bagging_temperature': 3.340826164726799,\n#                  'random_strength': 1.538518016574368,\n#                  'depth': 3,\n#                  'grow_policy': 'Depthwise',\n#                  'leaf_estimation_method': 'Newton'},\n#                 {'learning_rate': 0.028141156076957437,\n#                  'l2_leaf_reg': 3.116523267336638,\n#                  'bagging_temperature': 4.420661209459851,\n#                  'random_strength': 1.8011752694610028,\n#                  'depth': 6,\n#                  'grow_policy': 'Depthwise',\n#                  'leaf_estimation_method': 'Newton'},\n#             ]\ncb_params = [\n                {'learning_rate': 0.11152721528043753, 'l2_leaf_reg': 9.495998186799408, 'bagging_temperature': 1.6183369518324908, 'random_strength': 1.7628253181122102, 'depth': 7, 'grow_policy': 'Lossguide', 'leaf_estimation_method': 'Gradient'},\n                {'learning_rate': 0.005613720196384217, 'l2_leaf_reg': 0.8262159912383316, 'bagging_temperature': 8.414314200388226, 'random_strength': 1.0777361548370274, 'depth': 4, 'grow_policy': 'Depthwise', 'leaf_estimation_method': 'Newton'},\n                {'learning_rate': 0.3402632799551716, 'l2_leaf_reg': 9.163175050035028, 'bagging_temperature': 5.471927179930505, 'random_strength': 1.008934010879257, 'depth': 4, 'grow_policy': 'SymmetricTree', 'leaf_estimation_method': 'Gradient'}\n]","27eba911":"# %%time\n# from sklearn.metrics import mean_squared_log_error\n# from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n# from catboost import CatBoostRegressor\n# from sklearn.multioutput import MultiOutputRegressor\n\n# all_fi = []\n# splits = 10\n# target_names = y_log.columns\n\n# for i, target in enumerate(target_names):\n#     print(f\"\\nTraining for {target}...\")\n#     skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n#     oof_preds = np.zeros((X_scaled.shape[0],))\n#     model_preds = 0\n#     model_fi = 0\n#     for num, (train_idx, valid_idx) in enumerate(skf.split(X_scaled, months)):\n#         X_train, X_valid = X_scaled[[train_idx]], X_scaled[[valid_idx]]\n#         y_train, y_valid = y_log.loc[train_idx, target], y_log.loc[valid_idx, target]\n#         model = CatBoostRegressor(random_state=42,\n#                                  thread_count=4,\n#                                  verbose=False,\n#                                  loss_function='RMSE',\n#                                  eval_metric='RMSE',\n#                                  od_type=\"Iter\",\n#                                  early_stopping_rounds=500,\n#                                  use_best_model=True,\n#                                  iterations=10000,\n#                                  **cb_params[i])\n#         model.fit(X_train, y_train,\n#                   eval_set=(X_valid, y_valid),\n#                   verbose=False)\n#         model_preds += np.expm1(model.predict(X_test_scaled)) \/ splits\n#         model_fi += model.feature_importances_\n#         oof_preds[valid_idx] = np.expm1(model.predict(X_valid))\n#         print(f\"Fold {num} RMSLE: {np.sqrt(mean_squared_log_error(np.expm1(y_valid), oof_preds[valid_idx]))}\")\n#     print(f\"\\nOverall RMSLE: {np.sqrt(mean_squared_log_error(np.expm1(y_log[target]), oof_preds))}\")    \n#     preds[target] = model_preds\n#     all_fi.append(dict(zip(X.columns, model_fi)))","ee798ec8":"%%time\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom catboost import CatBoostRegressor\n\nall_fi = []\nsplits = 10\ntarget_names=y_log.columns\n\nfor i, target in enumerate(target_names):\n    print(f\"\\nTraining for {target}...\")\n    logo = LeaveOneGroupOut()\n    oof_preds = np.zeros((X.shape[0],))\n    model_preds = 0\n    model_fi = 0\n    for num, (train_idx, valid_idx) in enumerate(logo.split(X, y_log, months)):\n        X_train, X_valid = X[[train_idx]], X[[valid_idx]]\n        y_train, y_valid = y_log.loc[train_idx, target], y_log.loc[valid_idx, target]\n        model = CatBoostRegressor(random_state=42,\n                                 thread_count=4,\n                                 verbose=False,\n                                 loss_function='RMSE',\n                                 eval_metric='RMSE',\n                                 od_type=\"Iter\",\n                                 early_stopping_rounds=500,\n                                 use_best_model=True,\n                                 iterations=10000,\n                                 task_type=\"CPU\",\n                                 **cb_params[i])\n        \n        model.fit(X_train, y_train,\n                  eval_set=(X_valid, y_valid),\n                  verbose=False)\n        model_preds += np.expm1(model.predict(X_test)) \/ splits\n        model_fi += model.feature_importances_\n        oof_preds[valid_idx] = np.expm1(model.predict(X_valid))\n        print(f\"Fold {num} RMSLE: {np.sqrt(mean_squared_log_error(np.expm1(y_valid), oof_preds[valid_idx]))}\")\n    print(f\"\\nOverall RMSLE: {np.sqrt(mean_squared_log_error(np.expm1(y_log[target]), oof_preds))}\")    \n    preds[target] = model_preds\n    all_fi.append(dict(zip(all_data.columns, model_fi)))","2c323d7a":"# Creating feature list from feature importance dictionaries\nfeature_list = set()\nfor i in np.arange(len(all_fi)):\n    feature_list = set.union(feature_list, set(all_fi[i].keys()))\nprint(f\"There are {len(feature_list)} unique features used for training: {feature_list}\")","24578830":"# Combining feature importances of different models into one dataframe\ndf = pd.DataFrame(columns=[\"Feature\"])\ndf[\"Feature\"] = list(feature_list)\nfor i in np.arange(len(all_fi)):\n    for key in all_fi[i].keys():\n        df.loc[df[\"Feature\"] == key, \"Importance_\" + str(i+1)] = all_fi[i][key] \/ 1000\ndf.fillna(0, inplace=True)\ndf.sort_values(\"Importance_1\", axis=0, ascending=False, inplace=True)","1c0c9d54":"x = np.arange(0, len(df[\"Feature\"]))\nheight = 0.3\n\nfig, ax = plt.subplots(figsize=(12, 9))\nbars1 = ax.barh(x-height, df[\"Importance_1\"], height=height,\n                color=\"cornflowerblue\",\n                edgecolor=\"black\",\n                label=target_names[0])\nbars2 = ax.barh(x, df[\"Importance_2\"], height=height,\n                color=\"palevioletred\",\n                edgecolor=\"black\",\n                label=target_names[1])\nbars3 = ax.barh(x+height, df[\"Importance_3\"], height=height,\n                color=\"mediumseagreen\",\n                edgecolor=\"black\",\n                label=target_names[2])\nax.set_title(\"Feature importances\", fontsize=20, pad=5)\nax.set_ylabel(\"Feature names\", fontsize=15, labelpad=5)\nax.set_xlabel(\"Feature importance\", fontsize=15, labelpad=5)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=12)\nax.tick_params(axis=\"x\", labelsize=12)\nax.grid(axis=\"x\")\nax.legend(fontsize=13, loc=\"lower right\")\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()","06c38a4e":"# preds.head()","dd0379ac":"# all_pred=np.vstack(([preds['target_carbon_monoxide'].values, preds['target_benzene'].values, preds['target_nitrogen_oxides'].values])).T\n# scaler.inverse_transform(all_pred)","5a143c04":"# preds['target_carbon_monoxide']=scaler.inverse_transform(all_pred)[:,0]\n# preds['target_benzene']=scaler.inverse_transform(all_pred)[:,1]\n# preds['target_nitrogen_oxides']=scaler.inverse_transform(all_pred)[:,2]","afe7fa76":"preds.head()","5f2286ba":"preds.to_csv('submission.csv', index=False)","45b91c22":"### \u5206\u6790\u6bcf\u5929\u7684\u8cc7\u8a0a\nLooking at the day wise trend,we see that there has been sudden peak and dips for certain days over the month.While the temperatures have been above 20 deg after late may, there is a dip in temperature less than 15 dec after Nov but there is a sudden increase in mid december.","0293fcd5":"## Feature importances\nThanks to: @Maxim Kazantsev https:\/\/www.kaggle.com\/maximkazantsev\/tps-07-21-eda-catboost","d665e171":"## Train Model","ea3a2a84":"For the test set, if we try to compare between March month of train, we could see the temperatures have started from approx 3 deg and increased above 15 deg.","54f9faa9":"### CatBoost_18Feature_Cross_Validation","00759f9b":"## LeaveOneGroupOut\ncat_features=[\"working_hours\",\"is_weekend\",\"satday\"] 8\u30019\u300111\n\nPublic Score: \n- 0.19336 with 13 feature \n - ['deg_C', 'relative_humidity', 'absolute_humidity', 'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'working_hours', 'is_weekend', 'time']\n\ninside test error:\n- 0.18441005 with 13 features (0.14049546445212416+0.09160805296249219+0.3211266334764782)","445f5cb2":"## EDA\n- The distribution of deg_C shows peaks between 20 to 30 deg.\n- There is a dip in relative humidity at 40% and there are two peaks at 30% and 45% approx.\n- The absolute humidity value shows peaks at 0.25g\/m3(i have assumed it to be g\/m3.Data info did not explicitly mention any units).\n- The distribution of sensor_1,2,3 & 5 appears to be left skewed whereas sensor-4 is normal with outliers at 500.","9ab7ad75":"## Data preprocessing\n\u9032\u4e00\u6b65\u8655\u7406\u4e4b\u524d\u5148\u78ba\u8a8d\u662f\u5426\u6709\u7f3a\u5931\u503c\uff1a","b915540c":"## Save Predict File","d77278cd":"## StratifiedKFold\nPublic Score: \n- 0.21217 with 11 features \n - ['deg_C', 'relative_humidity', 'absolute_humidity', 'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'working_hours', 'is_weekend', 'hr', 'satday', 'time']\n- 0.19965 with 13 feature \n - ['deg_C', 'relative_humidity', 'absolute_humidity', 'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'working_hours', 'is_weekend', 'time']\n\ninside test error:\n- 0.126380933 with 11 features \n- 0.119173847 with 13 features (0.09045966192305815+0.08042533289018933+0.18663654561947446)"}}