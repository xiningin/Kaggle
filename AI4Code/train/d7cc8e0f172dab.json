{"cell_type":{"fd497425":"code","ba2218f4":"code","f044d310":"code","c59ba0b4":"code","9daf839b":"code","1ff6a7b9":"code","46709e94":"code","b94ec9f0":"code","524a3fb5":"code","7d7cc922":"code","487c0d1d":"code","e03aa72b":"code","888ddb44":"code","28a1d44f":"code","79d065f2":"code","0d269684":"code","053fc174":"code","8431f314":"code","9904ddb4":"code","eb75c381":"code","e37c9e1f":"code","8969317b":"code","b5838ec9":"code","469a0e90":"code","38134d3b":"code","947b2cd6":"code","f80533a7":"code","97da432e":"code","699e8bd6":"code","ed8385f5":"code","1bc3d21a":"code","b9eec185":"code","ac4fc1c8":"code","c51c19be":"code","32159dd6":"code","7a4f1136":"markdown","0a3fc437":"markdown","ce76aef5":"markdown","99673ae4":"markdown","3a1e05db":"markdown","3437f906":"markdown","ea41a4fe":"markdown","3254f1bb":"markdown","4367fea0":"markdown","8a4497f8":"markdown","e9a0d506":"markdown","3dc08dbd":"markdown","3e765441":"markdown","ab007d59":"markdown","de8d8cd3":"markdown","6e249db1":"markdown","0084fec6":"markdown","922b796e":"markdown","76ca94ac":"markdown","86672aad":"markdown","65eaa3ba":"markdown","92ea3160":"markdown","c210821a":"markdown","16d9b017":"markdown","db646dc9":"markdown","83ebfb59":"markdown","dbc32fbb":"markdown","56e4a0ae":"markdown","936ed3f9":"markdown","85ca7154":"markdown","7bfc1908":"markdown","c812e609":"markdown","4b1791ee":"markdown"},"source":{"fd497425":"!pip install --upgrade pip","ba2218f4":"#!pip3 install --upgrade pandas","f044d310":"import pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport re\nimport os\nimport json\nimport nltk \nfrom math import log, sqrt\nfrom collections import defaultdict\nfrom copy import deepcopy\nimport glob\nimport sys\nimport gzip, pickle, pickletools\nfrom tabulate import tabulate\nfrom IPython.display import display, HTML\nimport warnings\nwarnings.filterwarnings('ignore')","c59ba0b4":"file_path = \"..\/input\/CORD-19-research-challenge\/\"\n\nmy_df =  pd.read_csv(file_path+'metadata.csv',low_memory=False)\n\nmy_df.head()","9daf839b":"def display_barchart(my_df):\n    column = my_df.columns.tolist()\n    # https:\/\/datatofish.com\/convert-pandas-dataframe-to-list\/\n    valid_cnt = list(my_df.count())\n    plt.bar(column,valid_cnt,align = \"center\",width = 0.5,alpha = 1)\n    plt.xticks(rotation=90)\n\n    \ndisplay_barchart(my_df)","1ff6a7b9":"\nmy_df.drop_duplicates(subset=['title'],keep='first')\nmy_df.drop_duplicates(subset=['abstract'],keep='first')\nmy_df.drop_duplicates(subset=['doi'],keep='first')\nmy_df.count()\n\n","46709e94":"\nvalid_cnt = list(my_df.count())\nwith_abstract = valid_cnt[8]\ntotal = valid_cnt[0]\nwithout_abstract  = total - with_abstract\n\ntot = plt.bar(1,total,color='green',width = 1)\nplt.annotate(str(total), xy=(1,total), ha='center', va='bottom')\n\nabstract = plt.bar(2,with_abstract,color='yellow',width = 1)\nplt.annotate(str(with_abstract), xy=(2,with_abstract), ha='center', va='bottom')\n\nlost = plt.bar(3,without_abstract,color='red',width = 1)\nplt.annotate(str(without_abstract), xy=(3,without_abstract), ha='center', va='bottom')\n\nplt.legend((tot, abstract,lost), ('Total literature', 'literatures with abstract','literatures without abstract'))","b94ec9f0":"my_df.dropna(how='all')\nmy_df.dropna(subset=['abstract'],inplace=True)\nmy_df.count()\n","524a3fb5":"my_df = my_df[pd.DatetimeIndex(my_df.publish_time).year>2018]\n# https:\/\/www.interviewqs.com\/ddi_code_snippets\/extract_month_year_pandas\n# https:\/\/stackoverflow.com\/questions\/13851535\/delete-rows-from-a-pandas-dataframe-based-on-a-conditional-expression-involving\nmy_df.info()\n\ndf = my_df","7d7cc922":"\nmy_df = df\ncovid_terms =['covid', 'coronavirus disease 19', 'sars cov 2', '2019 ncov', '2019ncov', '2019 n cov', '2019n cov',\n              'ncov 2019', 'n cov 2019', 'coronavirus 2019', 'wuhan pneumonia', 'wuhan virus', 'wuhan coronavirus',\n              'coronavirus 2', 'covid-19', 'SARS-CoV-2', '2019-nCov']\ncovid_terms = [elem.lower() for elem in covid_terms]\ncovid_terms = re.compile('|'.join(covid_terms))\n\ndef checkYear(date):\n    return int(date[0:4])\n\ndef checkCovid(row, covid_terms):\n    return bool(covid_terms.search(row['abstract'].lower())) and checkYear(row['publish_time']) > 2019\n\n","487c0d1d":"my_df['is_covid'] = my_df.apply(checkCovid, axis=1, covid_terms=covid_terms)\nmy_df.head()","e03aa72b":"df_covid_only = my_df[my_df['is_covid']==True]\ndf_covid_only = df_covid_only.reset_index(drop=True)\ndf_covid_only.info()","888ddb44":"key_words = ['transmission','transmitted','long','symptomatic','asymptomatic','infected','infection','range', 'incubation', 'periods', 'surfaces', 'prevent','protective','SARS-CoV-2','infectious','reported','respiratory', 'secretions', 'saliva', 'droplets','short', 'time', 'fomites','sanitation']\npattern = '|'.join(key_words)\ndf_covid_only = df_covid_only.loc[df_covid_only['abstract'].str.contains(pattern, case=False)]\ndf_covid_only.info()","28a1d44f":"df_covid_only.head().abstract\ndisplay_barchart(df_covid_only)","79d065f2":"\ndf_covid_only.dropna(subset=['pdf_json_files'],inplace=True)\ndf_covid_only.info()\ndisplay_barchart(df_covid_only)","0d269684":"base_path = '..\/input\/CORD-19-research-challenge\/'\nall_selected_json = df_covid_only['pdf_json_files']\nprint(base_path+all_selected_json[0])","053fc174":"# This piece of code was adopted from the original source at:\n# https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\/notebook \n\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    for section, text in texts:\n        texts_di[section] += text\n    body = \"\"\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n    return \"; \".join(formatted)\n\ndef load_files(file_count,filenames_selected = all_selected_json):\n\n    raw_files = []\n    i = 0\n    for filename in filenames_selected:\n        if i == file_count:\n            break\n        \n        try:\n            file = json.load(open(filename, 'rb'))\n            raw_files.append(file)   \n            i = i+1\n        except:\n            try:\n                filename = base_path + filename\n                file = json.load(open(filename, 'rb'))\n                raw_files.append(file)\n                i = i+1\n            except:\n                x = 1\n    return raw_files\n    \n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n\n    for file in all_files:\n\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n        cleaned_files.append(features)\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df = clean_df.drop(columns=['authors','affiliations','bibliography',\n                                      'raw_authors','raw_bibliography'])\n    return clean_df","8431f314":"!pip install cdqa ","9904ddb4":"def get_corpus():\n    num_of_papers = {}\n    corpus = pd.DataFrame(columns=['paper_id','title','abstract','text'])\n    \n    file_count = 40000\n    \n    print('Reading ', file_count, 'json files')\n    print('Loading......')\n    files = load_files(file_count)\n    print('Generating clean dataframe')\n    df = generate_clean_df(files)\n    print('Generated......')\n    print('Forming Corpus.......')\n    print(df.shape[0])\n    corpus = pd.concat([corpus, df], ignore_index=True, sort=False)\n    print('4')\n    \n    print('Corpus includes {0} scientific articles.'.format(len(corpus)))\n    return corpus, num_of_papers\n\ncorpus, num_of_papers = get_corpus()","eb75c381":"# This processing algorithm can originaly be found at:\n# https:\/\/github.com\/nilayjain\/text-search-engine\n\ninverted_index = defaultdict(list)\nnum_of_documents = len(corpus)\nvects_for_docs = [] \ndocument_freq_vect = {}\n \ndef iterate_over_all_docs():\n    print('Processing corpus...')\n    for i in range(num_of_documents):\n        if np.mod(i, 1000) == 0:\n            print('{0} of {1}'.format(str(i).zfill(len(str(num_of_documents))),num_of_documents))\n        doc_text = corpus['title'][i] + ' ' + corpus['abstract'][i] + ' ' + corpus['text'][i]\n        token_list = get_tokenized_and_normalized_list(doc_text)\n        vect = create_vector(token_list)\n        vects_for_docs.append(vect)\n    print('{0} of {1}'.format(num_of_documents, num_of_documents))\n\ndef create_vector_from_query(l1):\n    vect = {}\n    for token in l1:\n        if token in vect:\n            vect[token] += 1.0\n        else:\n            vect[token] = 1.0\n    return vect\n\ndef generate_inverted_index():\n    count1 = 0\n    for vector in vects_for_docs:\n        for word1 in vector:\n            inverted_index[word1].append(count1)\n        count1 += 1\n\ndef create_tf_idf_vector():\n    vect_length = 0.0\n    for vect in vects_for_docs:\n        for word1 in vect:\n            word_freq = vect[word1]\n            temp = calc_tf_idf(word1, word_freq)\n            vect[word1] = temp\n            vect_length += temp ** 2\n        vect_length = sqrt(vect_length)\n        for word1 in vect:\n            vect[word1] \/= vect_length\n\ndef get_tf_idf_from_query_vect(query_vector1):\n    vect_length = 0.0\n    for word1 in query_vector1:\n        word_freq = query_vector1[word1]\n        if word1 in document_freq_vect:\n            query_vector1[word1] = calc_tf_idf(word1, word_freq)\n        else:\n            query_vector1[word1] = log(1 + word_freq) * log(\n                num_of_documents)\n        vect_length += query_vector1[word1] ** 2\n    vect_length = sqrt(vect_length)\n    if vect_length != 0:\n        for word1 in query_vector1:\n            query_vector1[word1] \/= vect_length\n\ndef calc_tf_idf(word1, word_freq):\n    return log(1 + word_freq) * log(num_of_documents \/ document_freq_vect[word1])\n\ndef get_dot_product(vector1, vector2):\n    if len(vector1) > len(vector2):\n        temp = vector1\n        vector1 = vector2\n        vector2 = temp\n    keys1 = vector1.keys()\n    keys2 = vector2.keys()\n    sum = 0\n    for i in keys1:\n        if i in keys2:\n            sum += vector1[i] * vector2[i]\n    return sum\n\ndef get_tokenized_and_normalized_list(doc_text):\n    tokens = nltk.word_tokenize(doc_text)\n    ps = nltk.stem.PorterStemmer()\n    stemmed = []\n    for words in tokens:\n        stemmed.append(ps.stem(words))\n    return stemmed\n\ndef create_vector(l1):\n    vect = {}  \n    global document_freq_vect\n    for token in l1:\n        if token in vect:\n            vect[token] += 1\n        else:\n            vect[token] = 1\n            if token in document_freq_vect:\n                document_freq_vect[token] += 1\n            else:\n                document_freq_vect[token] = 1\n    return vect\n\ndef get_result_from_query_vect(query_vector1):\n    parsed_list = []\n    for i in range(num_of_documents - 0):\n        dot_prod = get_dot_product(query_vector1, vects_for_docs[i])\n        parsed_list.append((i, dot_prod))\n        parsed_list = sorted(parsed_list, key=lambda x: x[1])\n    return parsed_list\n\n# ..\/input\/CORD-19-research-challenge\ndef pickle_to_file(file_name,object_to_pickle):\n    filepath = \".\/data\/\"+file_name\n    with gzip.open(filepath, \"wb\") as f:\n        pickled = pickle.dumps(object_to_pickle)\n        optimized_pickle = pickletools.optimize(pickled)\n        f.write(optimized_pickle)\n        \ndef pickle_all():\n    pickle_to_file(\"inverted_index.pkl\",inverted_index)\n    pickle_to_file(\"vects_for_docs.pkl\",vects_for_docs)\n    pickle_to_file(\"document_freq_vect.pkl\",document_freq_vect)\n    \niterate_over_all_docs()\ngenerate_inverted_index()\ncreate_tf_idf_vector()\n# pickle_all()","e37c9e1f":"'''Tried pickling for direct access to corpus and document vector''' \n           \n# def load_all_from_pickle():\n#     print(\"inverted_index loading...\")\n#     filepath = \"..\/input\/pickle-files\/inverted_index.pkl\"\n#     with gzip.open(filepath, 'rb') as f:\n#         p = pickle.Unpickler(f)\n#         global inverted_index\n#         inverted_index = p.load()\n#         print(sys.getsizeof(inverted_index))\n#     print(\"inverted_index loaded.\")\n    \n#     print(\"vects_for_docs loading...\")   \n#     filepath = \"..\/input\/pickle-files\/vects_for_docs.pkl\"\n#     with gzip.open(filepath, 'rb') as f:\n#         global vects_for_docs\n#         p = pickle.Unpickler(f)\n#         vects_for_docs = p.load()\n#         print(len(vects_for_docs))\n#     print(\"vects_for_docs loaded.\")\n        \n#     print(\"document_freq_vect loading...\")    \n#     filepath = \"..\/input\/pickle-files\/document_freq_vect.pkl\"\n#     with gzip.open(filepath, 'rb') as f:\n#         global document_freq_vect\n#         p = pickle.Unpickler(f)\n#         document_freq_vect = p.load()\n#         print(sys.getsizeof(document_freq_vect))\n#     print(\"document_freq_vect loaded.\")\n        \n\n# load_all_from_pickle()","8969317b":"# The End-To-End Closed Domain Question Answering System is used here.\n# It is available at: https:\/\/pypi.org\/project\/cdqa\/\n\nfrom cdqa.utils.filters import filter_paragraphs\nfrom cdqa.utils.download import download_model, download_bnpp_data\nfrom cdqa.pipeline.cdqa_sklearn import QAPipeline\n\ndownload_bnpp_data(dir='.\/data\/bnpp_newsroom_v1.1\/')\ndownload_model(model='bert-squad_1.1', dir='.\/models')","b5838ec9":"def add_Each_Details(papers_info,top_n_papers,prediction,query,x,result):\n    for i in range(top_n_papers):\n        if papers_info['title'][i] == prediction[x][1]:\n            pid = papers_info['id'][i]\n\n#     print(x+1,'. PAPER ID : {0}\\n'.format(pid))\n#     print('POSSIBLE ANSWER FROM PAPER: {0}\\n'.format(prediction[x][0]))\n#     print('PAPER TITLE: {0}\\n'.format(prediction[x][1]))\n#     print('HIGHLIGHT FROM PAPER: {0}\\n'.format(prediction[x][2]))\n    temp = pd.DataFrame({\"Paper ID\":[pid], \"Paper Title\":[prediction[x][1]], \"Highlight from Paper\":[prediction[x][2]],\"Short answer\":[prediction[x][0]]})\n    result = result.append(temp,ignore_index= True)\n    \n    return result\n    \n    \ndef find_relevant_articles(query=None, top_n_papers=20, min_n_papers=3):\n    if query == None:\n        query = input('Please enter your query...')\n#     print('\\n\\n'+'*'*34+' PROCESSING NEW QUERY '+'*'*34+'\\n')  \n    \n    query_list = get_tokenized_and_normalized_list(query)\n    query_vector = create_vector_from_query(query_list)\n    get_tf_idf_from_query_vect(query_vector)\n    result_set = get_result_from_query_vect(query_vector)\n    \n    papers_info = {'query':query, 'query list':query_list, 'query vector':query_vector,\n                   'id':[], 'title':[], 'abstract':[], 'text':[], 'weight':[], 'index':[]}\n    \n    for i in range(1, top_n_papers+1):\n        tup = result_set[-i]\n        papers_info['id'].append(corpus['paper_id'][tup[0]])\n        papers_info['title'].append(corpus['title'][tup[0]])\n        papers_info['abstract'].append(corpus['abstract'][tup[0]])\n        papers_info['text'].append(corpus['text'][tup[0]])\n        papers_info['weight'].append(tup[1])\n        papers_info['index'].append(tup[0])\n        \n    colms = ['date','title', 'category', 'link', 'abstract', 'paragraphs']\n    df = pd.DataFrame(columns=colms)\n\n    for i in range(len(papers_info['text'])):\n        papers_info['text'][i] = papers_info['text'][i].replace('\\n\\n', ' ')\n        CurrentText = papers_info['text'][i]\n        CurrentText = CurrentText.split('. ')\n        CurrentList = [\"None\", papers_info['title'][i], \"None\", \"None\", papers_info['abstract'][i], CurrentText]\n        CurrentList = np.array(CurrentList)\n        CurrentList = CurrentList.reshape(1, CurrentList.shape[0])\n        CurrentList = pd.DataFrame(data = CurrentList, columns=colms)\n        df = pd.concat([df, CurrentList], ignore_index=True)\n    df = filter_paragraphs(df)\n\n    # Loading QAPipeline with CPU version of BERT Reader pretrained on SQuAD 1.1\n    cdqa_pipeline = QAPipeline(reader='models\/bert_qa.joblib')\n\n    #Drop possible duplicates\n    df.drop_duplicates(subset=['title'],keep='first')\n    df.drop_duplicates(subset=['link'],keep='first')\n    df.dropna(subset=['title'],inplace=True)\n    # Fitting the retriever to the list of documents in the dataframe\n    cdqa_pipeline.fit_retriever(df=df)\n\n    query = papers_info['query']\n\n    # Sending a question to the pipeline and getting prediction\n    prediction = cdqa_pipeline.predict(query=query,n_predictions = min_n_papers)\n    \n    #Creating result dataframe\n    column_names = [\"Paper ID\", \"Paper Title\", \"Highlight from Paper\",\"Short answer\"]\n    result = pd.DataFrame(columns = column_names)\n    \n    \n    print('QUERY: {0}\\n'.format(query))\n#     display(HTML('QUERY: '+query))\n    for i in range(min_n_papers):\n        result = add_Each_Details(papers_info,top_n_papers,prediction,query,i,result)\n       \n    display(HTML(result.to_html()))","469a0e90":"query = 'What is the incubation period for covid19 ?'\nfind_relevant_articles(query=query, top_n_papers=500, min_n_papers=5)","38134d3b":"query = 'What is range of incubation period for coronavirus SARS-CoV-2 COVID-19 in humans'\nfind_relevant_articles(query=query, top_n_papers=500, min_n_papers=5)","947b2cd6":"query = 'What is optimal quarantine period for coronavirus COVID-19'\nfind_relevant_articles(query=query, top_n_papers=500, min_n_papers=5)","f80533a7":"query = 'What is effective quarantine period for coronavirus COVID-19'\nfind_relevant_articles(query=query, top_n_papers=500, min_n_papers=5)","97da432e":"query = 'What is percentage of death cases for coronavirus SARS-CoV-2 COVID-19'\nfind_relevant_articles(query=query, top_n_papers=500, min_n_papers=5)","699e8bd6":"query = 'What is death rate for coronavirus COVID-19 and air pollution'\nfind_relevant_articles(query=query, top_n_papers=500, min_n_papers=5)","ed8385f5":"query = 'At which temperature coronavirus COVID-19 can survive'\nfind_relevant_articles(query=query, top_n_papers=500, min_n_papers=5)","1bc3d21a":"query = 'How long coronavirus SARS-CoV-2 can survive on plastic surface'\nfind_relevant_articles(query=query, top_n_papers=500, min_n_papers=5)","b9eec185":"query = 'What are risk factors for coronavirus COVID-19'\nfind_relevant_articles(query=query, top_n_papers=500, min_n_papers=5)","ac4fc1c8":"query = 'What is origin of coronavirus COVID-19'\nfind_relevant_articles(query=query, top_n_papers=500, min_n_papers=5)","c51c19be":"\nqueries =  ['At which temperature coronavirus cannot survive',\n           'What is the range of incubation periods for coronavirus SARS-CoV-2 COVID-19 in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery',\n           'What are the prevalence of asymptomatic shedding and transmission of coronavirus COVID-19(e.g., particularly children)',\n           'Mention the seasonality COVID-19 transmission',\n           'Explain the physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding)',\n           'What is the period of persistence and stability of coronavirus on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood)',\n           'How long does coronavirus persists on surfaces of different materials (e,g., copper, stainless steel, plastic)',\n           'Explain the natural history of the coronavirus COVID-19 and shedding of it from an infected person']","32159dd6":"for query in queries:\n    find_relevant_articles(query,500,5)","7a4f1136":"# Step 6. Search of the most relevant articles and competent answer on the query","0a3fc437":"# Step 3. Corpus formation","ce76aef5":"Load csv to dataframe.","99673ae4":"2. Call the function find_relevant_articles().\n\nFor example,","3a1e05db":"List of queries","3437f906":"Upgrade the version of Pandas","ea41a4fe":"Removal of the duplicate entries.","3254f1bb":"Filtering only papers related to Covid-19. ","4367fea0":"Below one can see a list of 10 queries and answers, which have been found by the system due to text mining.","8a4497f8":"First, for each query the system arranges all the scientific papers within the corpus in the relevant order.","e9a0d506":"Covid-19 is found in 2019.\nSo there is no need to keep literatures that was published before 2019.","3dc08dbd":"Filtering the literatures based on the terms present in the \"key_word\" list.","3e765441":"Counting the number of valid entries for each column.","ab007d59":"Install the [End-To-End Closed Domain Question Answering System](http:\/\/pypi.org\/project\/cdqa\/)","de8d8cd3":"# Step 2. Extraction of data from json files to dataframe format","6e249db1":"# Step 7. Getting practical answers and the most relevant papers (query based approach)","0084fec6":"# Step 5. Using pretrained BERT model","922b796e":"Abstract plays an important role in finding related journals because it contains the objective or overview of the literature.","76ca94ac":"# Step 4. Processing of Corpus","86672aad":"# Scientific Literature Retriever Using BERT and CDQA","65eaa3ba":"The papers i.e. literatures which do not contain any terms from \"covid_terms\" must be removed.","92ea3160":"Second, the system analize texts of top N the mosr relevant papers to answer to the query in the best way.","c210821a":"# Step 1. Data Pre-processing","16d9b017":"Removal of the empty rows or in other words the rows without an abstract.","db646dc9":"Path to json files","83ebfb59":"Removal of the literatures that were published before the year 2019.","dbc32fbb":"Ploting a bar graph related to above result.","56e4a0ae":"# How to use the system","936ed3f9":"# **COVID-19 Open Research Dataset (CORD-19) Analysis**","85ca7154":"Import all necessary modules","7bfc1908":"Considering the topic we choose, we store the terms related it in \"key_words\" list. And we filter papers based on the listed terms. ","c812e609":"When Steps 0-7 have been completed with a corpus of scientific papers, the system is ready to process your queries. To get an answer to a query, follow two steps: \n\n1. Input any query in the form of string type variable.\n\nFor example,","4b1791ee":"\nSo here, We visualize it with bar chart to get insights."}}