{"cell_type":{"85ae919a":"code","a1b3d872":"code","4efe59f1":"code","fb649731":"code","9d8779b5":"code","6b583b62":"code","d5344b6b":"code","be846301":"code","fed32f06":"code","b81dfb87":"code","4579bc2a":"code","cd0d386c":"code","1e08cb47":"code","be69cb26":"code","c3df2a0f":"code","f945e62b":"code","97de9ab3":"code","9c83cc28":"code","8bd3c19e":"code","2c9e8d98":"code","49424943":"code","79069dc5":"code","891d41cd":"code","a9c26b8c":"code","a87b9551":"code","199a6e6a":"code","5e164d9e":"code","94da8db0":"code","31f1db6a":"code","fad55f0d":"code","c01ff1a3":"markdown","375aafe7":"markdown","0391f0dc":"markdown","e94a27f8":"markdown","c72d43fd":"markdown","87b9c01e":"markdown","b308bca6":"markdown","158b45d6":"markdown","f9b17f86":"markdown","f1f53f4d":"markdown","76c0d823":"markdown","a6d84df1":"markdown","0eb9ede4":"markdown","d248bfbc":"markdown","b0963681":"markdown","3a98f3a6":"markdown","c93e6ab8":"markdown","610b6e70":"markdown","8e170bf0":"markdown","b2fe6fc8":"markdown","3781afb4":"markdown","c12cf9c8":"markdown","7a0dfb0e":"markdown","14794bdc":"markdown","8da12315":"markdown","7e5a750c":"markdown","d11c7703":"markdown","cd7198a8":"markdown","cc3ffbfa":"markdown","4bdd702e":"markdown"},"source":{"85ae919a":"# !pip install --upgrade pip","a1b3d872":"!pip install -U scikit-learn==0.20\n\n!pip install scikit-optimize==0.8.dev0\n\n# !pip install scikit-optimize==0.7.2\n# # !pip install https:\/\/github.com\/scikit-optimize\/scikit-optimize.git\n# # !conda install -c conda-forge scikit-optimize==0.7.3","4efe59f1":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport gc\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split\nimport sklearn\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.callbacks import *\n\nimport skopt\nfrom skopt import gp_minimize, forest_minimize\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import *\nfrom skopt.utils import use_named_args\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","fb649731":"skopt.__version__, sklearn.__version__, tf.__version__","9d8779b5":"# see: https:\/\/github.com\/mardani72\/Hyper-Parameter_optimization\/blob\/master\/Hyper_Param_Facies_tf_final.ipynb\n\ndef compare_facies_plot(logs, compadre, facies_colors):\n    #make sure logs are sorted by depth\n    logs = logs.sort_values(by='Depth')\n    cmap_facies = colors.ListedColormap(\n            facies_colors[0:len(facies_colors)], 'indexed')\n    \n    ztop=logs.Depth.min(); zbot=logs.Depth.max()\n    \n    cluster1 = np.repeat(np.expand_dims(logs['Facies'].values,1), 100, 1)\n    cluster2 = np.repeat(np.expand_dims(logs[compadre].values,1), 100, 1)\n    \n    f, ax = plt.subplots(nrows=1, ncols=7, figsize=(16, 10))\n    ax[0].plot(logs.GR, logs.Depth, '-g', alpha=0.8, lw = 0.9)\n    ax[1].plot(logs.ILD_log10, logs.Depth, '-b', alpha=0.8, lw = 0.9)\n    ax[2].plot(logs.DeltaPHI, logs.Depth, '-k', alpha=0.8, lw = 0.9)\n    ax[3].plot(logs.PHIND, logs.Depth, '-r', alpha=0.8, lw = 0.9)\n    ax[4].plot(logs.PE, logs.Depth, '-c',  alpha=0.8, lw = 0.9)\n    im1 = ax[5].imshow(cluster1, interpolation='none', aspect='auto',\n                    cmap=cmap_facies,vmin=1,vmax=9)\n    im2 = ax[6].imshow(cluster2, interpolation='none', aspect='auto',\n                    cmap=cmap_facies,vmin=1,vmax=9)\n    \n    divider = make_axes_locatable(ax[6])\n    cax = divider.append_axes(\"right\", size=\"20%\", pad=0.05)\n    cbar=plt.colorbar(im2, cax=cax)\n    cbar.set_label((5*' ').join([' SS ', 'CSiS', 'FSiS', \n                                'SiSh', ' MS ', ' WS ', ' D  ', \n                                ' PS ', ' BS ']))\n    cbar.set_ticks(range(0,1)); cbar.set_ticklabels('')\n    \n    for i in range(len(ax)-2):\n        ax[i].set_ylim(ztop,zbot)\n        ax[i].invert_yaxis()\n        ax[i].grid()\n        ax[i].locator_params(axis='x', nbins=3)\n    \n    ax[0].set_xlabel(\"GR\")\n    ax[0].set_xlim(logs.GR.min(),logs.GR.max())\n    ax[1].set_xlabel(\"ILD_log10\")\n    ax[1].set_xlim(logs.ILD_log10.min(),logs.ILD_log10.max())\n    ax[2].set_xlabel(\"DeltaPHI\")\n    ax[2].set_xlim(logs.DeltaPHI.min(),logs.DeltaPHI.max())\n    ax[3].set_xlabel(\"PHIND\")\n    ax[3].set_xlim(logs.PHIND.min(),logs.PHIND.max())\n    ax[4].set_xlabel(\"PE\")\n    ax[4].set_xlim(logs.PE.min(),logs.PE.max())\n    ax[5].set_xlabel('Facies')\n    ax[6].set_xlabel(compadre)\n    \n    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])\n    ax[4].set_yticklabels([]); ax[5].set_yticklabels([]); ax[6].set_yticklabels([])\n    ax[5].set_xticklabels([])\n    ax[6].set_xticklabels([])\n    f.suptitle('Well: %s'%logs.iloc[0]['Well Name'], fontsize=14,y=0.94)\n    return f","6b583b62":"import matplotlib.colors as colors\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\ndef make_facies_log_plot(logs, facies_colors):\n    #make sure logs are sorted by depth\n    logs = logs.sort_values(by='Depth')\n    cmap_facies = colors.ListedColormap(\n            facies_colors[0:len(facies_colors)], 'indexed')\n    \n    ztop=logs.Depth.min(); zbot=logs.Depth.max()\n    \n    cluster=np.repeat(np.expand_dims(logs['Facies'].values,1), 100, 1)\n    \n    f, ax = plt.subplots(nrows=1, ncols=6, figsize=(15, 10))\n    ax[0].plot(logs.GR, logs.Depth, '-g')\n    ax[1].plot(logs.ILD_log10, logs.Depth, '-')\n    ax[2].plot(logs.DeltaPHI, logs.Depth, '-', color='0.40')\n    ax[3].plot(logs.PHIND, logs.Depth, '-', color='r')\n    ax[4].plot(logs.PE, logs.Depth, '-', color='black')\n    im=ax[5].imshow(cluster, interpolation='none', aspect='auto',\n                    cmap=cmap_facies,vmin=1,vmax=9)\n    \n    divider = make_axes_locatable(ax[5])\n    cax = divider.append_axes(\"right\", size=\"20%\", pad=0.05)\n    cbar=plt.colorbar(im, cax=cax)\n    cbar.set_label((5*' ').join([' SS ', 'CSiS', 'FSiS', \n                                'SiSh', ' MS ', ' WS ', ' D  ', \n                                ' PS ', ' BS ']))\n    cbar.set_ticks(range(0,1)); cbar.set_ticklabels('')\n    \n    for i in range(len(ax)-1):\n        ax[i].set_ylim(ztop,zbot)\n        ax[i].invert_yaxis()\n        ax[i].grid()\n        ax[i].locator_params(axis='x', nbins=3)\n    \n    ax[0].set_xlabel(\"GR\")\n    ax[0].set_xlim(logs.GR.min(),logs.GR.max())\n    ax[1].set_xlabel(\"ILD_log10\")\n    ax[1].set_xlim(logs.ILD_log10.min(),logs.ILD_log10.max())\n    ax[2].set_xlabel(\"DeltaPHI\")\n    ax[2].set_xlim(logs.DeltaPHI.min(),logs.DeltaPHI.max())\n    ax[3].set_xlabel(\"PHIND\")\n    ax[3].set_xlim(logs.PHIND.min(),logs.PHIND.max())\n    ax[4].set_xlabel(\"PE\")\n    ax[4].set_xlim(logs.PE.min(),logs.PE.max())\n    ax[5].set_xlabel('Facies')\n    \n    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])\n    ax[4].set_yticklabels([]); ax[5].set_yticklabels([])\n    ax[5].set_xticklabels([])\n    f.suptitle('Well: %s'%logs.iloc[0]['Well Name'], fontsize=14,y=0.94)\n    plt.show()\n#     return f","d5344b6b":"df = pd.read_csv('..\/input\/well-log-facies-dataset\/facies_data.csv')\n\n# hold out a blind set for evalkation\nhold_out_df = df[df['Well Name'] == 'NEWBY']   # 'SHANKLE'\n\n# training set\ntrain_df = df[df['Well Name'] != 'NEWBY']      ","be846301":"train_df.sample(10)","fed32f06":"train_df.info()","b81dfb87":"# prepare data\ntarget = 'Facies'\n\ndummies = pd.get_dummies(train_df[target]) \nfacies_classes = dummies.columns \ny = dummies.values                  \n\n# select predictors \ndrop_cols = ['Facies', 'Formation', 'Well Name', ] #'Depth' +[target]\n\nX = train_df.drop(drop_cols, axis=1)\n\n# hold out set\ny_hold_out = pd.get_dummies(pd.Categorical(hold_out_df[target], categories=facies_classes)).values    \nX_hold_out = hold_out_df.drop(drop_cols, axis=1).values\n\n\nprint('training shape:', X.shape, y.shape)\nprint('hold out shape:', X_hold_out.shape, y_hold_out.shape)","4579bc2a":"facies_labels = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D','PS', 'BS']\n\nfacies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00',\n       '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n\n#facies_color_map is a dictionary that maps facies labels to their respective colors\n\nfacies_color_map = {}\nfor ind, label in enumerate(facies_labels):\n    facies_color_map[label] = facies_colors[ind]\n\ndef label_facies(row, labels):\n    return labels[ row['Facies'] -1]\n    \ntrain_df.loc[:,'FaciesLabels'] = train_df.apply(lambda row: label_facies(row, facies_labels), axis=1)\n\n# target classes \nfacies_counts = train_df[target].value_counts().sort_index()\nfacies_counts.index = facies_labels\n\nprint(facies_counts)\n%matplotlib inline\nfacies_counts.plot(kind='bar',color=facies_colors, title='Target Distribution');","cd0d386c":"%matplotlib inline\nsns.pairplot(train_df.drop(['Well Name','Facies','Formation','Depth','NM_M','RELPOS'],axis=1),\n             hue='FaciesLabels', palette=facies_color_map,\n             hue_order=list(reversed(facies_labels)))","1e08cb47":"# %matplotlib inline\nmake_facies_log_plot(train_df[train_df['Well Name'] == 'SHRIMPLIN'], facies_colors)","be69cb26":"# %matplotlib inline\nmake_facies_log_plot(train_df[train_df['Well Name'] == train_df['Well Name'].unique()[2]], facies_colors)","c3df2a0f":"# standardize data\nsc = StandardScaler()\n\nX = sc.fit_transform(X)\nX_hold_out = sc.transform(X_hold_out)\n\n# Data split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=train_df[target])\n\nprint('training samples:', x_train.shape[0])\nprint('testing samples:', x_test.shape[0])\nprint('hold out samples:', X_hold_out.shape[0])","f945e62b":"dim_learning_rate = Real(low=1e-4, high=1e-1, prior='uniform', name='learning_rate')\ndim_num_dense_layers = Integer(low=3, high=10, name='num_dense_layers')\ndim_num_dense_nodes = Integer(low=128, high=512, name='num_dense_nodes')\ndim_activation = Categorical(categories=['relu', 'elu'], name='activation')\ndim_dropout = Integer(low=1, high=5, name='dp')\n\n\ndimensions = [\n    dim_learning_rate,\n    dim_num_dense_layers,\n    dim_num_dense_nodes,\n    dim_activation,\n    dim_dropout\n]\n\n\n# set default params - make sure are within the search space\ndefault_params = [1e-2, 4, 128, 'relu', 1]","97de9ab3":"def create_model(learning_rate, num_dense_layers,\n                 num_dense_nodes, activation, dp):   \n    \n    \n    model = Sequential()\n\n    model.add(InputLayer(input_shape=(X.shape[1])))\n    \n    for i in range(num_dense_layers):\n        name = 'layer_dense_{0}'.format(i+1)\n\n        # add dense layer\n        model.add(Dense(num_dense_nodes, activation=activation, name=name))\n        model.add(Dropout(0.1*dp))   \n\n    # use softmax-activation for classification.\n    model.add(Dense(y.shape[1], activation='softmax'))\n    \n    # Use the Adam method for training the network.\n    opt = Adam(lr=learning_rate)\n    \n    #compile the model so it can be trained.\n    model.compile(optimizer=opt,\n                  loss='categorical_crossentropy',\n                  metrics=['AUC'])\n    \n    return model","9c83cc28":"path_best_model = 'model.h5'\nbest_accuracy = 0.0","8bd3c19e":"@use_named_args(dimensions=dimensions)\ndef fitness(learning_rate, num_dense_layers,\n            num_dense_nodes, activation, dp):   \n    \n    \"\"\"\n    Hyper-parameters:\n    learning_rate:     Learning-rate for the optimizer.\n    num_dense_layers:  Number of dense layers.\n    num_dense_nodes:   Number of nodes in each dense layer.\n    activation:        Activation function for all layers.\n    dp:                Dropout rate\n    \"\"\"\n\n    \n    # Print the hyper-parameters.\n    print('learning rate: {0:.1e}'.format(learning_rate))\n    print('num_dense_layers:', num_dense_layers)\n    print('num_dense_nodes:', num_dense_nodes)\n    print('activation:', activation)\n    print('dropout:', dp*0.1)\n    print()\n    \n    # Create the neural network with these hyper-parameters.\n    model = create_model(learning_rate=learning_rate,\n                         num_dense_layers=num_dense_layers,\n                         num_dense_nodes=num_dense_nodes,\n                         activation=activation,\n                         dp=dp)  \n\n    \n    # Create callback-functions\n    es = EarlyStopping(monitor='val_accuracy', patience=15)\n    rlr = ReduceLROnPlateau(monitor='val_loss', patience=10)\n   \n    # train the model\n    history = model.fit(x = x_train,\n                        y = y_train,\n                        epochs=2000,\n                        batch_size=128,\n                        validation_data=(x_test, y_test),\n                        callbacks=[es, rlr]\n                       )\n\n    # Get the classification accuracy on the validation-set\n    score = max(history.history['val_auc'])   # [-1]\n\n    # Print the classification accuracy.\n    print('-'*20)\n    print(f\"> AUC: {score:.2%}\")\n    print('-'*20)\n\n    # Save the model if it improves on the best-found performance.\n    global best_accuracy\n\n    # If the classification accuracy of the saved model is improved ...\n    if score > best_accuracy:\n        # Save the new model to harddisk.\n        model.save(path_best_model)\n        \n        # Update the classification accuracy.\n        best_accuracy = score\n\n    # Delete the Keras model with these hyper-parameters from memory.\n    del model\n    gc.collect()\n    \n    # Clear the Keras session, to empty the TensorFlow graph \n    K.clear_session()\n    \n    return -1.0 * score","2c9e8d98":"# check objective function\n\nfitness(default_params)","49424943":"search_result = skopt.gp_minimize(func=fitness,   \n                            dimensions=dimensions,\n                            acq_func='gp_hedge',   #   'EI'\n                            n_calls=15,\n                            random_state=1982,\n                            x0=default_params)","79069dc5":"%matplotlib inline\nplot_convergence(search_result); \n\nplt.show()","891d41cd":"print(f'optimal hyper-parameters \\\n      lr: {search_result.x[0]} - layers: {search_result.x[1]} - dense_units: {search_result.x[2]} - activ: {search_result.x[3]}  - dropout: {search_result.x[4]}') #","a9c26b8c":"pd.DataFrame(sorted(zip(search_result.func_vals, search_result.x_iters)), index=np.arange(15), columns=['score', 'params'])","a87b9551":"# create a list for plotting\ndim_names = ['learning_rate', 'num_dense_layers', 'num_dense_nodes', 'activation', 'dropout' ]\n\n# %matplotlib inline\nplot_objective(result=search_result, dimensions=dim_names);","199a6e6a":"opt_par = search_result.x\n\n# use hyper-parameters from optimization \nlearning_rate = opt_par[0]\nnum_layers = opt_par[1] \nnum_nodes = opt_par[2] \nactivation = opt_par[3]\ndp=opt_par[4]\n\nmodel = create_model(learning_rate=learning_rate,\n                     num_dense_layers=num_layers,\n                     num_dense_nodes=num_nodes,\n                     activation=activation,\n                     dp=dp)\n\nes = EarlyStopping(monitor='val_accuracy', min_delta=1e-3, patience=100, verbose=1, mode='auto', restore_best_weights=True)\nrlr = ReduceLROnPlateau(monitor='val_loss', patience=20)\n\nhist = model.fit(x_train,y_train, \n                 validation_data=(x_test,y_test),\n                 epochs=2000,\n                 batch_size=128,\n                 verbose=1, \n                 callbacks=[es, rlr],\n                 )","5e164d9e":"result = model.evaluate(X_hold_out, y_hold_out)\nprint(\"{0}: {1:.2%}\".format(model.metrics_names[1], result[1]))","94da8db0":"# get model predictions \n\ny_pred = model.predict(X_hold_out)                            # probabilities\n\nhold_out_df['Predictions']= np.argmax(y_pred, axis=1) + 1     # +1 becuase facies starts from 1 (not zero-like index)","31f1db6a":"fig = compare_facies_plot(hold_out_df, 'Predictions', facies_colors)","fad55f0d":"# plot confusion matrix\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(sklearn.metrics.confusion_matrix(y_hold_out.argmax(1), y_pred.argmax(1), labels=facies_classes), annot=True, cmap='Blues')\nplt.xlabel('True classes');\nplt.ylabel('Predicted classes');\nplt.show()","c01ff1a3":"### Distribution of Classes ","375aafe7":"# Run Bayesian Optimization ","0391f0dc":"# Load data","e94a27f8":"# Reproduce Model with Optimal Parameters","c72d43fd":"The idea with Bayesian optimization is to construct another model of the search-space for hyper-parameters. One kind of model is known as a Gaussian Process. This gives us an estimate of how the performance varies with changes to the hyper-parameters. Whenever we evaluate the actual performance for a set of hyper-parameters, we know for a fact what the performance is - except perhaps for some noise. We can then ask the Bayesian optimizer to give us a new suggestion for hyper-parameters in a region of the search-space that we haven't explored yet, or hyper-parameters that the Bayesian optimizer thinks will bring us most improvement. We then repeat this process a number of times until the Bayesian optimizer has built a good model of how the performance varies with different hyper-parameters, so we can choose the best parameters.\n\nThe flowchart of the algorithm is roughly:\n\n\n![nn_img.png](attachment:nn_img.png)","87b9c01e":"###  Log Facies Classification\n\nThis notebook demonstrates how to train a neural net classifier and find an optimal set of hyper-parameters (for a given metric) in order\nto predict facies from well log data. ","b308bca6":"# Plot facies logs ","158b45d6":"## Create NN Model ","f9b17f86":"## Evaluate on the hold out set","f1f53f4d":"### Please, if you find any part of this kernel usefull - upvote it so to save it in your favourites -:)","76c0d823":"Let's visualize the progress of the whole optimization session, where the fitness values are shown on y-axis.","a6d84df1":"The dataset comes from a class excercise from The University of Kansas and contains logs from the largest gas fields in North America, the Hugoton and Panoma Fields.\n\nFor more info on the origin of the data, see [Dubois et al. (2007)](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0098300406001956?via%3Dihub)\nand [here](https:\/\/github.com\/mardani72\/Facies-Classification-Machine-Learning\/blob\/master\/Facies_Classification_Various_ML_Final.ipynb).\n\nFacies (types of rocks) are studied from core samples in every half foot and matched with logging data in well location. Feature variables include five from wireline log measurements and two geologic constraining variables that are derived from geologic knowledge. \n\nI made also the dataset [public](https:\/\/www.kaggle.com\/imeintanis\/well-log-facies-dataset) so you can experiment with this as well. \n\nThe seven variables are:\n- **GR**: this wireline logging tools measure gamma emission\n- **ILD_log10**: this is resistivity measurement\n- **PE**: photoelectric effect log\n- **DeltaPHI**: Phi is a porosity index in petrophysics.\n- **PNHIND**: Average of neutron and density log.\n- **NM_M**:nonmarine-marine indicator\n- **RELPOS**: relative position\n\nThe nine discrete facies (classes of rocks) are:\n- **SS**: Nonmarine sandstone\n- **CSiS**: Nonmarine coarse siltstone\n- **FSiS**: Nonmarine fine siltstone\n- **SiSH**: Marine siltstone and shale\n- **MS**: Mudstone (limestone)\n- **WS**: Wackestone (limestone)\n- **D**: Dolomite\n- **PS**: Packstone-grainstone (limestone)\n- **BS**: Phylloid-algal bafflestone (limestone)\n\n\nThese facies aren't discrete, and gradually blend into one another. Some have neighboring facies that are rather close. Mislabeling within these neighboring facies can be expected to occur. The following table lists the facies, their abbreviated labels and their approximate neighbors.\n\n| Facies | Label | Adjacent Facies |\n|--------|-------|-----------------|\n| 1      | SS    | 2               |\n| 2      | CSiS  | 1,3             |\n| 3      | FSiS  | 2               |\n| 4      | SiSh  | 5               |\n| 5      | MS    | 4,6             |\n| 6      | WS    | 5,7             |\n| 7      | D     | 6,8             |\n| 8      | PS    | 6,7,9           |\n| 9      | BS    | 7,8             |\n","0eb9ede4":"This is the function that creates and trains a neural network with the given hyper-parameters, and then evaluates its performance on the validation-set. The function then returns the so-called fitness value (aka. objective value), which is the negative AUC on the validation-set. It is negative because skopt performs minimization instead of maximization.\n","d248bfbc":"A summary of all runs is shown below ","b0963681":"Next, we prepare our dataset for training\n- separate features (X) \n- seperate target variable (y)\n- encode y to categorical (one-hot-encode)","3a98f3a6":"# Visualize model predictions vs hold out set","c93e6ab8":"The data set contains logs from 8 wells - let's keep aside a hold oud set for final evaluation ","610b6e70":"## Distribution of Features","8e170bf0":"# Fittness function to Optimize [skopt]","b2fe6fc8":"- Class #7 (D) has the lowest no. of samples\n- Classes #1,4,5,9  have an avg. no. of samples \n- Classes #2,3,6,8  have the max. no. of samples ","3781afb4":"## Scale data and Split to Train\/Test sets","c12cf9c8":"# NN hyper-parameters search with Bayesian Optimization","7a0dfb0e":"Next we build the objective function that is the function we wish to optimize\/minimize. \n\nThe main steps that we need to perform \n- build and train a network with given hyper-parameters \n- evaluate the model performance with the validation dataset\n\nIt returns fitness value, in our case the (negative) AUC in validation set. \n\nNote the negative sign since skopt performs minimization rather than maximization.\n\nYou can control the runtime of the optimization process by  `n_calls`: selecting the no. of evaluation runs","14794bdc":"Let's try to find out the optimized parameters for:\n\n- Learning rate\n- Number of dense layers\n- Number of nodes for each layer\n- Activation function\n- Dropout rate","8da12315":"In machine learning, model parameters can be divided into two main categories:\n\n1- Trainable parameters: such as weights in neural networks learned by training algorithms and the user does not interfere in the process,\n\n2- Hyper-parameters: users can set them before training operation such as learning rate or the number of dense layers in the model.","7e5a750c":"# Helpers","d11c7703":"# Use Bayesian Optimization for Hyper-parameter search","cd7198a8":"## Credits - Resources\n\n-  [towardsdatascience article](https:\/\/towardsdatascience.com\/bayesian-hyper-parameter-optimization-neural-networks-tensorflow-facies-prediction-example-f9c48d21f795)\n- [skopt documentation and examples](https:\/\/scikit-optimize.github.io\/stable\/user_guide.html)","cc3ffbfa":"# Results","4bdd702e":"# Problem Description - Dataset"}}