{"cell_type":{"bc058aa6":"code","59ccac66":"code","a2825b50":"code","81ace773":"code","b143366a":"markdown","5cfb4447":"markdown"},"source":{"bc058aa6":"import os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/flowershd5dataset\/flowers-hd5\/data\/flowers\/\"))\nprint(os.listdir(\"..\/input\/flowershd5words\/\"))\nprint(os.listdir(\"..\/input\/checkpoints\/\"))\nhdf5_fpath = \"..\/input\/flowershd5dataset\/flowers-hd5\/data\/flowers\/flowers.hdf5\"","59ccac66":"#All imports are here\n#Commented library imports are not readily available in Kaggle (or) we are not using them in this code\n\n###### trainer.py imports ######\n\nimport argparse\nimport torch\nimport time\nimport yaml\nimport pickle\nfrom math import ceil\nimport numpy as np\n#import db_helper\nfrom PIL import Image\nfrom torch import nn\nfrom datetime import timedelta\nimport matplotlib.pyplot as plt\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\n#from models.gan_factory import gan_factory\n#from training.train_discriminator import TrainDiscriminator\n#from training.train_target_discriminator import TrainTargetDiscriminator\n#from training.train_generator import TrainGenerator\n#from training.train_mix_discriminator import TrainMixDiscriminator\n#from txt2image_dataset import Text2ImageDataset\n#from hash_txt2image_dataset import HashText2ImageDataset\n#from color_txt2image_dataset import ColorText2ImageDataset\n#from utils import Utils, Logger\n#from testing.tsne import tsne\n#from testing.nn import NearestNeighbor\n#from testing.cca import CCA_Test\n\n###################################\n\n###### gan.py imports ######\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\n\n###################################\n\n###### txt2img_dataset.py imports ######\n\nimport io\nfrom torch.utils.data import Dataset, DataLoader\nimport h5py\nimport numpy as np\nimport pdb\nfrom PIL import Image\nimport torch\nfrom torch.autograd import Variable\nimport pdb\nimport torch.nn.functional as F\n\n###################################\n\n###### nn.py (NearestNeighbour) imports ######\n\nimport pickle\nfrom torch.autograd import Variable\nimport numpy as np\nfrom sklearn.neighbors import NearestNeighbors\n\n###################################\n\nprint(\"All libraries imported!\")","a2825b50":"#view contents in HDF5 files\nf = h5py.File(hdf5_fpath)\n\n#1. to know the categories in hdf5 file\nprint(list(f))\nprint(\"\\nNo. of items in test = \",len(list(f['test'])))\nprint(\"\\nNo. of items in train = \",len(list(f['train'])))\nprint(\"\\nNo. of items in valid = \",len(list(f['valid'])))\n\n#2. see what data is stored in each category\n#print(\"\\n\\ntest = \\n\",list(f['test']))\n\n#print(\"\\n\\ntrain = \\n\",list(f['train']))\n\n#print(\"\\n\\nvalid = \\n\",list(f['valid']))","81ace773":"################ cca.py ###################\n\nIGNORE_WORDS = ['flower', 'and', 'this', 'has', 'this', 'has', 'are', 'with', 'a', 'the', 'that', 'in', 'is', 'of',\n                'on', 'as', ]\nCOLORS = [\"yellow\", \"white\", \"pink\", \"purple\", \"red\", \"green\", \"orange\", \"blue\", \"brown\", \"black\", \"maroon\", \"gold\",\n          \"gray\", \"lime\", \"grey\", \"lavender\", \"magenta\", \"violet\", \"rose\", \"lilac\", \"peach\", \"charcoal\"]\n\n\nclass CCA_Test:\n    def __init__(self, dataset, max_words=1000, ignore_words=False):\n        self.max_words = max_words\n        if dataset == 'flowers_only':\n            source_words_f = open('..\/input\/flowershd5words\/flowers.hdf5.words', 'r')\n            target_words_f = open('..\/input\/flowershd5words\/flowers.hdf5.words', 'r')\n        self.target_words = []\n        self.source_words = []\n        for line in source_words_f:\n            word = line.split(',')[0]\n            if not ignore_words or word not in IGNORE_WORDS:\n                self.source_words.append(word)\n        for line in target_words_f:\n            word = line.split(',')[0]\n            if not ignore_words or word not in IGNORE_WORDS:\n                self.target_words.append(word)\n        # Take only the most common words\n        self.source_words = self.source_words[:max_words]\n        self.target_words = self.target_words[:max_words]\n        self.s_to_i = {k: v for v, k in enumerate(self.source_words)}\n        self.t_to_i = {k: v for v, k in enumerate(self.target_words)}\n\n    def sentences_to_one_hot_vector(self, sentences, source=True):\n        if source:\n            words = self.source_words\n            word_to_index = self.s_to_i\n        else:\n            words = self.target_words\n            word_to_index = self.t_to_i\n        mat = np.zeros((len(sentences), len(words)))\n        for i, sentence in enumerate(sentences):\n            sentence = ''.join(c for c in sentence if c not in '\"();:!@#$,.\\n')\n            sentence = sentence.split(' ')\n            for word in sentence:\n                if word in word_to_index:\n                    mat[i][word_to_index[word]] = 1\n        return mat\n\n    def run(self, source_sentences, target_sentences, embedding=False):\n        print('---begin CCA RUN---')\n        print('source len {}, target len {}'.format(len(source_sentences), len(target_sentences)))\n        min_size = min(len(source_sentences), len(target_sentences))\n        if embedding:\n            X = np.asmatrix(source_sentences[:min_size])\n            Y = np.asmatrix(target_sentences[:min_size])\n        else:\n            X = self.sentences_to_one_hot_vector(source_sentences[:min_size], True)\n            Y = self.sentences_to_one_hot_vector(target_sentences[:min_size], False)\n        cca = rcca.CCA(kernelcca=False, reg=0.01, numCC=500)\n        cca.train([X, Y])\n        print('more than {} {}'.format(0.6, np.sum(cca.cancorrs > 0.6)))\n        print('more than {} {}'.format(0.5, np.sum(cca.cancorrs > 0.5)))\n        print('more than {} {}'.format(0.4, np.sum(cca.cancorrs > 0.4)))\n        return float(np.average(cca.cancorrs)), float(np.sum(cca.cancorrs > 0.6))\n\n################ cca.py ends here ###################\n\n################ nn.py (NearestNeighbour) ###################\n\nclass NearestNeighbor:\n    def __init__(self, dataset, source, cuda, ngf):\n        self.dataset = dataset\n        data = None\n        representation = None\n        labels = []\n        embeddings = []\n        path = '..\/input\/checkpoints\/'\n        data_path = path + 'source_{}_nn_data.pl'.format(source)\n        labels_path = path + 'source_{}_nn_labels.pl'.format(source)\n        nbrs_path = path + 'source_{}_nn.pl'.format(source)\n        embeddings_path = path + 'source_{}_nn_embeddings.pl'.format(source)\n        self.model = gan_factory.generator_factory('vae', ngf, False)\n        if cuda:\n            self.model = self.model.cuda()\n        #self.model.load_state_dict(torch.load('.\/checkpoints\/flowers_autoencoder\/gen.pth'))\n\n        if os.path.exists(data_path):\n            print('start loading data for NN test {}'.format(data_path))\n            data = pickle.load(open(data_path, 'rb'))\n            labels = pickle.load(open(labels_path, 'rb'))\n            nbrs = pickle.load(open(nbrs_path, 'rb'))\n            embeddings = pickle.load(open(embeddings_path, 'rb'))\n        else:\n            print('start creating data for NN test {}'.format(data_path))\n            for i, sample in enumerate(dataset):\n                #print(\"**** iter i = \",i)\n                if data is None:\n                    data = sample['right_images'].numpy()\n                    data_var = Variable(sample['right_images'].float(), volatile=True)\n                    if cuda:\n                        data_var = data_var.cuda()\n                    representation = self.model.encoder_only(data_var).data.cpu().numpy()\n                    labels = sample['txt']\n                    embeddings = sample['right_embed']\n                else:\n                    data = np.append(data, sample['right_images'].numpy(), axis=0)\n                    data_var = Variable(sample['right_images'].float(), volatile=True)\n                    if cuda:\n                        data_var = data_var.cuda()\n                    representation = np.append(representation, self.model.encoder_only(data_var).data.cpu().numpy(),\n                                               axis=0)\n                    labels += sample['txt']\n                    embeddings = np.append(embeddings, sample['right_embed'].numpy(), axis=0)\n            nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(representation.reshape(-1, 228))\n            pickle.dump(data, open(data_path, 'wb'))\n            pickle.dump(labels, open(labels_path, 'wb'))\n            pickle.dump(nbrs, open(nbrs_path, 'wb'))\n            pickle.dump(embeddings, open(embeddings_path, 'wb'))\n        print('finish loading data for NN test')\n        self.data = data\n        self.labels = labels\n        self.nbrs = nbrs\n        self.embeddings = embeddings\n\n    def get_text(self, samples, limit=-1):\n        text_results, _, _ = self.get_text_and_images_and_embedding(samples, limit)\n        return text_results\n\n    def get_text_and_images(self, samples, limit):\n        text_results, image_results, _ = self.get_text_and_images_and_embedding(samples, limit)\n        return text_results, image_results\n\n    def get_text_and_images_and_embedding(self, samples, limit=-1):\n        samples_embedding = self.model.encoder_only(samples).data.cpu().numpy().reshape(-1, 228)\n        if limit != -1:\n            samples_embedding = samples_embedding[:limit]\n        distances, indices = self.nbrs.kneighbors(samples_embedding)\n        text_results = [self.labels[index] for index in indices[:, 0]]\n        image_results = [self.data[index] for index in indices[:, 0]]\n        embedding_results = [self.embeddings[index] for index in indices[:, 0]]\n        return text_results, image_results, embedding_results\n    \n################ nn.py (NearestNeighbour) ends here ###################\n\n################ txt2image_dataset.py ###################\n\nclass Text2ImageDataset(Dataset):\n\n    def __init__(self, datasetFile, transform=None, split=0):\n        self.datasetFile = datasetFile\n        self.transform = transform\n        self.dataset = None\n        self.dataset_keys = None\n        self.split = 'train' if split == 0 else 'valid' if split == 1 else 'test'\n        self.h5py2int = lambda x: int(np.array(x))\n\n    def __len__(self):\n        f = h5py.File(self.datasetFile, 'r')\n        self.dataset_keys = [str(k) for k in f[self.split].keys()]\n        length = len(f[self.split])\n        f.close()\n\n        return length\n\n    def __getitem__(self, idx):\n        if self.dataset is None:\n            self.dataset = h5py.File(self.datasetFile, mode='r')\n            self.dataset_keys = [str(k) for k in self.dataset[self.split].keys()]\n\n        example_name = self.dataset_keys[idx]\n        example = self.dataset[self.split][example_name]\n\n        # pdb.set_trace()\n\n        right_image = bytes(np.array(example['img']))\n        right_embed = np.array(example['embeddings'], dtype=float)\n        wrong_image = bytes(np.array(self.find_wrong_image(example['class'])))\n        inter_embed = np.array(self.find_inter_embed())\n\n        right_image = Image.open(io.BytesIO(right_image)).resize((64, 64))\n        wrong_image = Image.open(io.BytesIO(wrong_image)).resize((64, 64))\n\n        right_image = self.validate_image(right_image)\n        wrong_image = self.validate_image(wrong_image)\n\n        txt = np.array(example['txt']).astype(str)\n        class_ = np.array(example['class']).astype(str)\n\n        sample = {\n                'right_images': torch.FloatTensor(right_image),\n                'right_embed': torch.FloatTensor(right_embed),\n                'wrong_images': torch.FloatTensor(wrong_image),\n                'inter_embed': torch.FloatTensor(inter_embed),\n                'txt': str(txt),\n                'class': str(class_)\n                 }\n\n        sample['right_images'] = sample['right_images'].sub_(127.5).div_(127.5)\n        sample['wrong_images'] =sample['wrong_images'].sub_(127.5).div_(127.5)\n\n        return sample\n\n    def find_wrong_image(self, category):\n        idx = np.random.randint(len(self.dataset_keys))\n        example_name = self.dataset_keys[idx]\n        example = self.dataset[self.split][example_name]\n        _category = example['class']\n\n        if _category != category:\n            return example['img']\n\n        return self.find_wrong_image(category)\n\n    def find_inter_embed(self):\n        idx = np.random.randint(len(self.dataset_keys))\n        example_name = self.dataset_keys[idx]\n        example = self.dataset[self.split][example_name]\n        return example['embeddings']\n\n\n    def validate_image(self, img):\n        img = np.array(img, dtype=float)\n        if len(img.shape) < 3:\n            rgb = np.empty((64, 64, 3), dtype=np.float32)\n            rgb[:, :, 0] = img\n            rgb[:, :, 1] = img\n            rgb[:, :, 2] = img\n            img = rgb\n\n        return img.transpose(2, 0, 1)\n\n################ txt2image_dataset.py ends here ###################\n\n################ utils.py ###################\n\nclass Concat_embed(nn.Module):\n    def __init__(self, embed_dim, projected_embed_dim):\n        super(Concat_embed, self).__init__()\n        self.projection = nn.Sequential(nn.Linear(in_features=embed_dim, out_features=projected_embed_dim),\n                                        nn.BatchNorm1d(num_features=projected_embed_dim),\n                                        nn.LeakyReLU(negative_slope=0.2, inplace=True))\n\n    def forward(self, inp, embed):\n        projected_embed = self.projection(embed)\n        replicated_embed = projected_embed.repeat(4, 4, 1, 1).permute(2, 3, 0, 1)\n        hidden_concat = torch.cat([inp, replicated_embed], 1)\n\n        return hidden_concat\n\nclass Utils(object):\n    def __init__(self, cuda):\n        self.is_cuda = cuda\n\n    def cuda(self, variable):\n        return variable.cuda() if self.is_cuda else variable\n\n    @staticmethod\n    def smooth_label(tensor, offset):\n        return tensor + offset\n\n    @staticmethod\n    def save_checkpoint(netD, netG, dir_path, epoch):\n        path = dir_path #os.path.join(dir_path, subdir_path)\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        torch.save(netD.state_dict(), '{0}\/disc_{1}.pth'.format(path, epoch))\n        torch.save(netG.state_dict(), '{0}\/gen_{1}.pth'.format(path, epoch))\n        \n    @staticmethod\n    def weights_init(m):\n        classname = m.__class__.__name__\n        if classname.find('Conv') != -1:\n            m.weight.data.normal_(0.0, 0.02)\n        elif classname.find('BatchNorm') != -1:\n            m.weight.data.normal_(1.0, 0.02)\n            m.bias.data.fill_(0)\n\n################ utils.py ends here ###################\n\n################ gan.py ###################\n\nclass vanilla_generator(nn.Module):\n    def __init__(self):\n        super(vanilla_generator, self).__init__()\n        self.image_size = 64\n        self.num_channels = 3\n        self.noise_dim = 100\n        self.ngf = 64\n\n        # based on: https:\/\/github.com\/pytorch\/examples\/blob\/master\/dcgan\/main.py\n        self.netG = nn.Sequential(\n            nn.ConvTranspose2d(self.noise_dim, self.ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(self.ngf * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.ngf * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.ngf * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(self.ngf * 2,self.ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.ngf),\n            nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False),\n            nn.Tanh()\n             # state size. (num_channels) x 64 x 64\n            )\n\n    def forward(self, z):\n        return self.netG(z)\n\n\nclass vanilla_discriminator(nn.Module):\n    def __init__(self):\n        super(vanilla_discriminator, self).__init__()\n        self.image_size = 64\n        self.num_channels = 3\n        self.ndf = 64\n\n        self.B_dim = 128\n        self.C_dim = 16\n\n        self.netD_1 = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(self.num_channels, self.ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n\n        self.netD_2 = nn.Sequential(\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(self.ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n            )\n\n    def forward(self, inp):\n        x_intermediate = self.netD_1(inp)\n        output =  self.netD_2(x_intermediate)\n        return output.view(-1, 1).squeeze(1), x_intermediate\n\n################ gan.py ends here ###################\n\n################ vae.py ###################\n\nclass vae_encoder_generator(nn.Module):\n    def __init__(self, ngf):\n        super(vae_encoder_generator, self).__init__()\n        self.vae_encoder = vae_encoder(ngf)\n        self.vae_generator = vae_generator(ngf)\n\n    def forward(self, inp):\n        x = self.vae_encoder(inp)\n        x = self.vae_generator(x)\n        return x\n\n    def generator_only(self, latent):\n        return self.vae_generator(latent)\n\n    def encoder_only(self, inp):\n        return self.vae_encoder(inp.cuda())\n\n\nclass vae_generator(nn.Module):\n    def __init__(self, ngf):\n        super(vae_generator, self).__init__()\n        self.image_size = 64\n        self.num_channels = 3\n        self.noise_dim = 100\n        self.embed_dim = 1024\n        self.projected_embed_dim = 128\n        self.latent_dim = self.noise_dim + self.projected_embed_dim\n        self.ngf = ngf\n\n        # self.projection = nn.Sequential(nn.Linear(in_features=self.embed_dim, out_features=self.projected_embed_dim),\n        #     nn.BatchNorm1d(num_features=self.projected_embed_dim), nn.LeakyReLU(negative_slope=0.2, inplace=True))\n\n        # based on: https:\/\/github.com\/pytorch\/examples\/blob\/master\/dcgan\/main.py\n        self.netG = nn.Sequential(\n            nn.ConvTranspose2d(self.latent_dim, self.ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(self.ngf * 8),\n            nn.ReLU(True), # state size. (ngf*8) x 4 x 4\n\n            nn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.ngf * 4),\n            nn.ReLU(True), # state size. (ngf*4) x 8 x 8\n\n            nn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.ngf * 2),\n            nn.ReLU(True), # state size. (ngf*2) x 16 x 16\n\n            nn.ConvTranspose2d(self.ngf * 2, self.ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.ngf),\n            nn.ReLU(True),\n\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (num_channels) x 64 x 64\n        )\n\n    def forward(self, latent_vector):\n        # projected_embed = self.projection(embed_vector).unsqueeze(2).unsqueeze(3)\n        # latent_vector = torch.cat([projected_embed, z], 1)\n        latent_vector = latent_vector.view(-1, self.latent_dim, 1, 1)\n        #print(\"**** latent_vector is cuda = \",latent_vector.cpu().is_cuda)\n        output = self.netG(latent_vector.cpu())\n\n        return output\n\n\nclass vae_encoder(nn.Module):\n    def __init__(self, ngf):\n        super(vae_encoder, self).__init__()\n        self.image_size = 64\n        self.num_channels = 3\n        self.embed_dim = 1024\n        self.noise_dim = 100\n        self.projected_embed_dim = 128\n        self.latent_dim = self.noise_dim + self.projected_embed_dim\n        self.ngf = ngf\n\n        self.projection = nn.Sequential(nn.Linear(in_features=self.embed_dim, out_features=self.projected_embed_dim),\n            nn.BatchNorm1d(num_features=self.projected_embed_dim), nn.LeakyReLU(negative_slope=0.2, inplace=True))\n\n        # based on: https:\/\/github.com\/pytorch\/examples\/blob\/master\/dcgan\/main.py\n        self.netE = nn.Sequential(\n            # state size. (num_channels) x 64 x 64\n            nn.Conv2d(self.num_channels, self.ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.ngf),\n            nn.ReLU(True),\n\n            # state size. (ngf) x 32 x 32\n            nn.Conv2d(self.ngf, self.ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.ngf * 2),\n            nn.ReLU(True),\n\n            # state size. (ngf*2) x 16 x 16\n            nn.Conv2d(self.ngf * 2, self.ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.ngf * 4),\n            nn.ReLU(True),\n\n            # state size. (ngf*4) x 8 x 8\n            nn.Conv2d(self.ngf * 4, self.ngf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.ngf * 8),\n            nn.ReLU(True),\n\n            # state size. (ngf*8) x 4 x 4\n            nn.Conv2d(self.ngf * 8, self.latent_dim, 4, 1, 0, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, images):\n        output = self.netE(images)\n        #print(output.is_cuda)\n        return output\n\n\nclass vae_discriminator(nn.Module):\n    def __init__(self, remove_noise):\n        super(discriminator, self).__init__()\n        self.image_size = 64\n        self.num_channels = 128\n        self.embed_dim = 1024\n        if remove_noise:\n            self.projected_embed_dim = 228\n            self.noise_dim = 0\n        else:\n            self.projected_embed_dim = 128\n            self.noise_dim = 100\n        self.B_dim = 128\n        self.C_dim = 16\n        self.minibatch_discriminator = minibatch_discriminator(self.num_channels, self.B_dim, self.C_dim)\n        #\n        self.netD_1 = nn.Sequential(\n            nn.Linear(self.projected_embed_dim + self.noise_dim, 228),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(228, 128),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        self.netD_2 = nn.Sequential(\n            nn.Linear(128 + self.B_dim, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, inp):\n        x = inp.view(-1, self.projected_embed_dim + self.noise_dim)\n        x = self.netD_1(x)\n        x = self.minibatch_discriminator(x)\n        x = self.netD_2(x)\n\n        return x.view(-1)\n\n################ vae.py ends here ###################\n\n################ gan_cls.py ###################\n\nclass generator(nn.Module):\n    def __init__(self, remove_noise, variational):\n        super(generator, self).__init__()\n        self.image_size = 64\n        self.num_channels = 3\n        self.embed_dim = 1024\n        self.remove_noise = remove_noise\n        if remove_noise:\n            self.noise_dim = 0\n            self.projected_embed_dim = 228\n        else:\n            self.noise_dim = 100\n            self.projected_embed_dim = 128\n        self.latent_dim = self.noise_dim + self.projected_embed_dim\n        self.ngf = 64\n        self.variational = variational\n        self.mu = None\n        self.sd = None\n\n        self.projection = nn.Sequential(nn.Linear(in_features=self.embed_dim, out_features=self.projected_embed_dim),\n            nn.BatchNorm1d(num_features=self.projected_embed_dim), nn.LeakyReLU(negative_slope=0.2, inplace=True))\n\n        if variational:\n            self.en_mu = nn.Conv2d(self.projected_embed_dim, self.projected_embed_dim, 1, 1, 0)\n            self.en_sigma = nn.Conv2d(self.projected_embed_dim, self.projected_embed_dim, 1, 1, 0)\n            self.softplus = nn.Softplus()\n            self.en_mu.weight.data.normal_(0, 0.002)\n            self.en_mu.bias.data.normal_(0, 0.002)\n            self.en_sigma.weight.data.normal_(0, 0.002)\n            self.en_sigma.bias.data.normal_(0, 0.002)\n\n        # based on: https:\/\/github.com\/pytorch\/examples\/blob\/master\/dcgan\/main.py\n        self.netG = nn.Sequential(nn.ConvTranspose2d(self.latent_dim, self.ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(self.ngf * 8), nn.ReLU(True), # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ngf * 4),\n            nn.ReLU(True), # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ngf * 2),\n            nn.ReLU(True), # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(self.ngf * 2, self.ngf, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ngf), nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False), nn.Tanh()\n            # state size. (num_channels) x 64 x 64\n        )\n\n    def forward(self, embed_vector, z, noise):\n        return self.netG(self.encoder_only(embed_vector, z, noise))\n\n    def encoder_only(self, embed_vector, z, noise):\n        projected_embed = self.projection(embed_vector).unsqueeze(2).unsqueeze(3)\n        if self.variational:\n            self.mu = self.en_mu(projected_embed)\n            self.sd = self.softplus(self.en_sigma(projected_embed))\n            projected_embed = self.mu + self.sd.mul(noise)\n        if self.remove_noise:\n            latent_vector = projected_embed\n        else:\n            latent_vector = torch.cat([projected_embed, z], 1)\n        return latent_vector\n\n    def generator_only(self, latent_vector):\n        return self.netG(latent_vector)\n\n\nclass discriminator(nn.Module):\n    def __init__(self, remove_noise):\n        super(discriminator, self).__init__()\n        self.image_size = 64\n        self.num_channels = 3\n        self.embed_dim = 1024\n        if remove_noise:\n            self.projected_embed_dim = 228\n        else:\n            self.projected_embed_dim = 128\n        self.ndf = 64\n        self.B_dim = 128\n        self.C_dim = 16\n\n        self.netD_1 = nn.Sequential(# input is (nc) x 64 x 64\n            nn.Conv2d(self.num_channels, self.ndf, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False), nn.BatchNorm2d(self.ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True), )\n\n        self.projector = Concat_embed(self.embed_dim, self.projected_embed_dim)\n\n        self.netD_2 = nn.Sequential(# state size. (ndf*8) x 4 x 4\n            nn.Conv2d(self.ndf * 8 + self.projected_embed_dim, 1, 4, 1, 0, bias=False), nn.Sigmoid())\n\n    def forward(self, inp, embed):\n        x_intermediate = self.netD_1(inp)\n        x = self.projector(x_intermediate, embed)\n        x = self.netD_2(x)\n\n        return x.view(-1, 1).squeeze(1), x_intermediate\n\n################ gan_cls.py ends here ###################\n\n################ gan_factory.py ###################\n\nclass gan_factory(object):\n    @staticmethod\n    def generator_factory(type, ngf, remove_noise, variational=False):\n        if type == 'gan':\n            return generator(remove_noise, variational)\n        elif type == 'vanilla_gan':\n            return vanilla_generator()\n        elif type == 'vae':\n            return vae_encoder_generator(ngf)\n\n    @staticmethod\n    def discriminator_factory(type, remove_noise):\n        if type == 'gan':\n            return discriminator(remove_noise)\n        elif type == 'vanilla_gan':\n            return vanilla_discriminator()\n        elif type == 'vae':\n            return vae_discriminator(remove_noise)\n\n################ gan_factory.py ends here ###################\n\n################ trainer.py ###################\n\nclass Trainer(object):\n    def __init__(self, type, dataset, split, lr, diter, vis_screen, save_path, l1_coef, l2_coef, pre_trained_gen,\n                 pre_trained_disc, batch_size, num_workers, epochs, args, params_search=False):\n        self.config = args\n        self.cuda = torch.cuda.is_available()\n\n        self.generator = gan_factory.generator_factory(type, args.ngf, args.remove_noise_2, args.variational)\n        self.discriminator = gan_factory.discriminator_factory(type, args.remove_noise_2)\n\n        self.target_generator = gan_factory.generator_factory(args.target_type, args.ngf, args.remove_noise_2)\n        \n        if self.cuda:\n            self.generator = self.generator.cuda()\n            self.discriminator = self.discriminator.cuda()\n\n        if pre_trained_disc:\n            print('loading {} from {}'.format('discriminator', pre_trained_disc))\n            self.discriminator.load_state_dict(torch.load(pre_trained_disc))\n        else:\n            if not params_search:\n                print('creating fresh params for {}'.format('discriminator'))\n            self.discriminator.apply(Utils.weights_init)\n\n        if pre_trained_gen:\n            print('loading {} from {}'.format('generator', pre_trained_gen))\n            self.generator.load_state_dict(torch.load(pre_trained_gen))\n        else:\n            if not params_search:\n                print('creating fresh params for {}'.format('generator'))\n            self.generator.apply(Utils.weights_init)\n\n        if dataset == 'flowers_only':\n            self.dataset = Text2ImageDataset(self.config.flowers_dataset_path, split=0)\n            self.target_dataset = Text2ImageDataset(self.config.flowers_dataset_path, split=2)\n        else:\n            print('Dataset not supported, please select either birds, flowers or flowers_only.')\n            exit()\n\n        self.noise_dim = 100\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.lr = lr\n        self.beta1 = 0.5\n        self.num_epochs = epochs\n        self.DITER = diter\n\n        self.l1_coef = l1_coef\n        self.l2_coef = l2_coef\n\n        self.data_loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True,\n                                      num_workers=self.num_workers)\n        self.target_data_loader = DataLoader(self.target_dataset, batch_size=self.batch_size, shuffle=True,\n                                             num_workers=self.num_workers)\n\n        self.optimD = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n        self.optimG = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n\n        self.save_path = save_path\n        self.type = type\n        # self.h_el = args.h_el\n        self.args = args\n        if not params_search:\n            self.checkpoints_path = 'checkpoints\/{}'.format(vis_screen)\n            if not os.path.exists(self.checkpoints_path):\n                os.makedirs(self.checkpoints_path)\n            print(\"***Calling Nearest Neighbour***\")\n            self.nn = NearestNeighbor(self.target_data_loader, dataset, self.cuda, args.ngf)\n            print(\"***Calling CCA_Test***\")\n            self.cca = CCA_Test(dataset, max_words=1000, ignore_words=False)\n        self.params_search = params_search\n        \n    def train(self, cls=False):\n        print(\"*** Inside train() func ***\")\n        #if self.type == 'wgan':\n            #self._train_wgan(cls)\n        if self.type == 'gan':\n            self._train_gan(cls)\n        #elif self.type == 'vanilla_wgan':\n            #self._train_vanilla_wgan()\n        #elif self.type == 'vanilla_gan':\n            #self._train_vanilla_gan()\n        \n    def _train_gan(self, cls):\n        print(\"*** Inside _train_gan() func ***\")\n        criterion = nn.BCELoss()\n        l2_loss = nn.MSELoss()\n        l1_loss = nn.L1Loss()\n        iteration = 0\n\n        for epoch in range(self.num_epochs):\n            for sample in self.data_loader:\n                iteration += 1\n                right_images = sample['right_images']\n                right_embed = sample['right_embed']\n                wrong_images = sample['wrong_images']\n\n                right_images = Variable(right_images.float()).cuda()\n                right_embed = Variable(right_embed.float()).cuda()\n                wrong_images = Variable(wrong_images.float()).cuda()\n\n                real_labels = torch.ones(right_images.size(0))\n                fake_labels = torch.zeros(right_images.size(0))\n\n                # ======== One sided label smoothing ==========\n                # Helps preventing the discriminator from overpowering the\n                # generator adding penalty when the discriminator is too confident\n                # =============================================\n                smoothed_real_labels = torch.FloatTensor(Utils.smooth_label(real_labels.numpy(), -0.1))\n\n                real_labels = Variable(real_labels).cuda()\n                smoothed_real_labels = Variable(smoothed_real_labels).cuda()\n                fake_labels = Variable(fake_labels).cuda()\n\n                # Train the discriminator\n                self.discriminator.zero_grad()\n                outputs, activation_real = self.discriminator(right_images, right_embed)\n                real_loss = criterion(outputs, smoothed_real_labels)\n                real_score = outputs\n\n                if cls:\n                    outputs, _ = self.discriminator(wrong_images, right_embed)\n                    wrong_loss = criterion(outputs, fake_labels)\n                    wrong_score = outputs\n\n                if self.args.remove_noise:\n                    noise = Variable(torch.zeros(right_images.size(0), self.noise_dim)).cuda()\n                else:\n                    noise = Variable(torch.randn(right_images.size(0), self.noise_dim)).cuda()\n                noise = noise.view(noise.size(0), 100, 1, 1)\n                fake_images = self.generator(right_embed, noise, noise)\n                outputs, _ = self.discriminator(fake_images, right_embed)\n                fake_loss = criterion(outputs, fake_labels)\n                fake_score = outputs\n\n                d_loss = real_loss + fake_loss\n\n                if cls:\n                    d_loss = d_loss + wrong_loss\n\n                d_loss.backward()\n                self.optimD.step()\n\n                # Train the generator\n                self.generator.zero_grad()\n                if self.args.remove_noise:\n                    noise = Variable(torch.zeros(right_images.size(0), self.noise_dim)).cuda()\n                else:\n                    noise = Variable(torch.randn(right_images.size(0), self.noise_dim)).cuda()\n                noise = noise.view(noise.size(0), 100, 1, 1)\n                fake_images = self.generator(right_embed, noise, noise)\n                outputs, activation_fake = self.discriminator(fake_images, right_embed)\n                _, activation_real = self.discriminator(right_images, right_embed)\n\n                activation_fake = torch.mean(activation_fake, 0)\n                activation_real = torch.mean(activation_real, 0)\n\n                # ======= Generator Loss function============\n                # This is a customized loss function, the first term is the regular cross entropy loss\n                # The second term is feature matching loss, this measure the distance between the real and generated\n                # images statistics by comparing intermediate layers activations\n                # The third term is L1 distance between the generated and real images, this is helpful for the conditional case\n                # because it links the embedding feature vector directly to certain pixel values.\n                # ===========================================\n                g_loss = criterion(outputs, real_labels) +\\\n                         self.l2_coef * l2_loss(activation_fake, activation_real.detach()) +\\\n                         self.l1_coef * l1_loss(fake_images, right_images)\n\n                g_loss.backward()\n                self.optimG.step()\n\n                if iteration % 25 == 0:\n                    print(\"Epoch: %d, d_loss= %f, g_loss= %f, D(X)= %f, D(G(X))= %f\" % (epoch, d_loss.data.cpu().mean(), g_loss.data.cpu().mean(), real_score.data.cpu().mean(), fake_score.data.cpu().mean()))\n\n            if (epoch) % 10 == 0:\n                Utils.save_checkpoint(self.discriminator, self.generator, self.save_path, epoch)\n                \n    def test(self):\n        self.generator.eval()\n        self.target_generator.eval()\n        number_of_images = 2\n        sample = next(iter(self.data_loader))\n        all_nn_texts = []\n        all_nn_images = []\n        all_fake_sources = []\n        all_transfers = []\n        text = sample['txt']\n        right_images = sample['right_images']\n        right_embed = sample['right_embed']\n        for i in range(number_of_images):\n            right_images_v = Variable(right_images.float(), volatile=True)\n            right_embed_v = Variable(right_embed.float(), volatile=True)\n            if self.args.remove_noise:\n                noise = Variable(torch.zeros(right_images_v.size(0), self.noise_dim), volatile=True)\n            else:\n                noise = Variable(torch.randn(right_images_v.size(0), self.noise_dim), volatile=True)\n            if self.cuda:\n                right_embed_v = right_embed_v.cuda()\n                noise = noise.cuda()\n\n            noise = noise.view(noise.size(0), self.noise_dim, 1, 1)\n            #print(\"right_embed_v type = \",type(right_embed_v),\"noise = \",type(noise))\n            fake_target = self.target_generator.generator_only(self.generator.encoder_only(right_embed_v, noise, noise))\n            all_transfers.append(fake_target)\n            fake_source = self.generator(right_embed_v, noise,noise)\n            all_fake_sources.append(fake_source)\n            \n            fake_source = fake_source.cuda()\n            #print(right_images[0])\n            #print(\"*** text = \",text[0])\n            #print(\"*** fake_source shape = \",fake_source.detach().shape)\n            plt.figure(1 , figsize = (20 , 20))\n            for i in range(9):\n                plt.subplot(9 , 1 , i+1)\n                plt.subplots_adjust(hspace = 0.7)\n                plt.imshow(fake_source[i].cpu().detach().permute(1, 2, 0))\n\n                plt.title('Text: {}'.format(text[i]))\n                plt.xticks([])\n                plt.yticks([])\n            #plt.imshow(fake_source[0].cpu().detach().permute(1, 2, 0))\n            plt.show()\n\n            nn_text, nn_images = self.nn.get_text_and_images(fake_target, -1)\n            all_nn_texts.append(nn_text)\n            all_nn_images.append(nn_images)\n            \n        for i, sentence in enumerate(text):\n            nn_sentences = [sents[i] for sents in all_nn_texts]\n            print(\"\\ncombined text: \",i,\"original sentence: \",sentence,\"nn_sentences: \",nn_sentences)\n\n        for i, image in enumerate(right_images):\n            nn_images = [imgs[i] for imgs in all_nn_images]\n            fake_source_images = [imgs[i].data.cpu().numpy() for imgs in all_fake_sources]\n            transfers_images = [imgs[i].data.cpu().numpy() for imgs in all_transfers]\n            image_tile = np.tile(image, (len(nn_images), 1, 1, 1))\n            #self.logger.draw_test(image_tile, fake_source_images, transfers_images, nn_images, 'image {}'.format(i))\n        print(\"*** end of testing ***\")\n   \n################ trainer.py ends here ###################\n\n################ runtime.py ###################\nclass Struct:\n    def __init__(self, **entries):\n        self.__dict__.update(entries)\n        \nparser = argparse.ArgumentParser()\nparams = dict()\n\nparams['type']='gan' #change this if you want to train any other gan\nparams['target_type']='vae'\n\nparams['lr']=0.0002\nparams['l1_coef']=50\nparams['l2_coef']=100\nparams['d_g_coef']=5\nparams['t_d_g_coef']=1\nparams['t_d_coef']=3\nparams['t_l1_coef']=50\nparams['m_d_coef']=1\nparams['g_m_d_coef']=1\nparams['var_coef']=1\nparams['diter']=5\n\nparams['cls']='wgan'\nparams['save_path']='tmp\/'\nparams['inference']=False\nparams['target_train']=False\nparams['h_el']=1\n\nparams['dataset']='flowers_only'\nparams['split']=0\nparams['batch_size']=100\nparams['num_workers']=1 \n#changed num_workers to 1\n#Reason: encountered an error similar to one reported here - https:\/\/github.com\/pytorch\/pytorch\/issues\/11887\n\nparams['ngf']=64\nparams['epochs']=3\nparams['remove_noise']=False\nparams['remove_noise_2']=False\n\nparams['variational']=False\nparams['vis_screen']=False\nparams['pre_trained_disc']=\"..\/input\/checkpoints\/disc_75.pth\"\nparams['pre_trained_gen']=\"..\/input\/checkpoints\/gen_75.pth\"\nparams['pre_trained_target_gen']=False\nparams['pre_trained_target_disc']=False\n\nparams['flowers_dataset_path']=\"..\/input\/flowershd5dataset\/flowers-hd5\/data\/flowers\/flowers.hdf5\"\n\nargs = Struct(**params) #Convert nested Python dict to object\n\ntrainer = Trainer(type=args.type, dataset=args.dataset, split=args.split, lr=args.lr, diter=args.diter,\n                  vis_screen=args.vis_screen, save_path=args.save_path, l1_coef=args.l1_coef, \n                  l2_coef=args.l2_coef,pre_trained_disc=args.pre_trained_disc, \n                  pre_trained_gen=args.pre_trained_gen, batch_size=args.batch_size, \n                  num_workers=args.num_workers, epochs=args.epochs, args=args)\n\nstart_time = time.time()\n\n'''if not args.inference:\n    if args.target_train:\n        trainer.target_train(args.cls)\n    else:\n        trainer.train(args.cls)'''\n#else:\nprint(\"*** Calling test() ***\")\ntrainer.test()\n\nelapsed = str(timedelta(seconds=int(time.time() - start_time)))\nprint('Running {} took {}'.format(\"GAN-CLS\", elapsed))\n\n################ runtime.py ends here ###################","b143366a":"**BOTTOM-UP approach ahead!!!**\n\nThe code in the next cell follows bottom up execution flow, to understand which method is getting called when start from the last part i.e., **runtime.py** code snippet.","5cfb4447":"**Please note! This kernel is for practice purposes only. The references used are mentioned below.**\n\n**Reference links:**\n- Paper - https:\/\/arxiv.org\/pdf\/1605.05396.pdf\n- Git repo - https:\/\/github.com\/ashual\/Text-to-Image-Synthesis \n- Datasets used in this kernel are same as the datasets used in the above git repository."}}