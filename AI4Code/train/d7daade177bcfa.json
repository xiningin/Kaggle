{"cell_type":{"4c33bdf8":"code","264c1962":"code","23c8c4c6":"code","c3b865da":"code","ebb2da5f":"code","428cb554":"code","64a15574":"code","a626ba12":"code","810f906e":"code","91ea0b0f":"code","96952345":"code","1dfb256b":"code","d52ecf3a":"code","2701ac4d":"code","2b569e3d":"markdown","da13c7e3":"markdown","3aad3a2f":"markdown","78a5cbe2":"markdown","f3608f25":"markdown","68e927af":"markdown","8d2338b3":"markdown","987c658a":"markdown","e8b8c53a":"markdown","3d09dfa8":"markdown","7d9f012f":"markdown","09151bfd":"markdown","98a7d6ad":"markdown"},"source":{"4c33bdf8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk(\"..\/input\/art-portraits\/Portraits\/\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nnp.random.seed(42)","264c1962":"#Importing Libraries\nimport random\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport PIL\nfrom PIL import Image\nimport tensorflow  as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Reshape, UpSampling2D, Conv2D, BatchNormalization\nfrom tensorflow.keras.layers import LeakyReLU, Dropout, ZeroPadding2D, Flatten, Activation\nfrom tensorflow.keras.optimizers import Adam\nimport tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#Settings\nsns.set(rc={\"axes.facecolor\":\"#EDE9DE\",\"figure.facecolor\":\"#D8CA7E\"})","23c8c4c6":"#Importing data\ndata_path = \"..\/input\/art-portraits\/Portraits\/\"\nbatch_s = 64\n#Import as tf.Dataset\ndata = tf.keras.preprocessing.image_dataset_from_directory(data_path, label_mode = None, image_size = (64,64), batch_size = batch_s)","c3b865da":"#Defing a function to see images\ndef Show_Img(data):\n    plt.figure(figsize=(15,15))\n    for images in data.take(1):\n        for i in range(18):\n            ax = plt.subplot(6, 6, i + 1)\n            ax.imshow(images[i].numpy().astype(\"uint8\"))\n            ax.axis(\"off\")\n#Plotting the images in dataset            \nShow_Img(data)","ebb2da5f":"#Preprocessing the dataset for model\ndata = data.map(lambda x: x \/ 255.0)\ndata","428cb554":"latent_dim = 100\ng_resolution=2\n\n#Building a Generator\ngenerator = Sequential()\ngenerator.add(Dense(4*4*256,activation=\"relu\",input_dim=latent_dim))\ngenerator.add(Reshape((4,4,256)))\ngenerator.add(UpSampling2D())\ngenerator.add(Conv2D(256,kernel_size=3,padding=\"same\"))\ngenerator.add(BatchNormalization(momentum=0.8))\ngenerator.add(Activation(\"relu\"))\ngenerator.add(UpSampling2D())\ngenerator.add(Conv2D(256,kernel_size=3,padding=\"same\"))\ngenerator.add(BatchNormalization(momentum=0.8))\ngenerator.add(Activation(\"relu\"))\ngenerator.add(UpSampling2D())\ngenerator.add(Conv2D(256,kernel_size=3,padding=\"same\"))#\ngenerator.add(BatchNormalization(momentum=0.8))\ngenerator.add(Activation(\"relu\"))\ngenerator.add(UpSampling2D())\ngenerator.add(Conv2D(128,kernel_size=3,padding=\"same\"))\ngenerator.add(BatchNormalization(momentum=0.8))\ngenerator.add(Activation(\"relu\"))\ngenerator.add(Conv2D(3,kernel_size=3,padding=\"same\"))\ngenerator.add(Activation(\"tanh\"))\n\ngenerator.summary()","64a15574":"#Creating a random seed and output from generator\nseed = tf.random.normal([1, latent_dim])\nGenerated_Portrait = generator(seed, training=False)\n#Plotting the image output of generator without training \nplt.imshow(Generated_Portrait[0, :, :, 0])\nplt.axis(\"off\")","a626ba12":"#Building a Discriminator\ndiscriminator = Sequential()\ndiscriminator.add(Conv2D(32, kernel_size=3, strides=2, input_shape=(64,64,3), padding=\"same\"))\ndiscriminator.add(LeakyReLU(alpha=0.2))\ndiscriminator.add(Dropout(0.25))\ndiscriminator.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\ndiscriminator.add(ZeroPadding2D(padding=((0,1),(0,1))))\ndiscriminator.add(BatchNormalization(momentum=0.8))\ndiscriminator.add(LeakyReLU(alpha=0.2))\ndiscriminator.add(Dropout(0.25))\ndiscriminator.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\ndiscriminator.add(BatchNormalization(momentum=0.8))\ndiscriminator.add(LeakyReLU(alpha=0.2))\ndiscriminator.add(Dropout(0.25))\ndiscriminator.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\ndiscriminator.add(BatchNormalization(momentum=0.8))\ndiscriminator.add(LeakyReLU(alpha=0.2))\ndiscriminator.add(Dropout(0.25))\ndiscriminator.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\ndiscriminator.add(BatchNormalization(momentum=0.8))\ndiscriminator.add(LeakyReLU(alpha=0.2))\ndiscriminator.add(Dropout(0.25))\ndiscriminator.add(Flatten())\ndiscriminator.add(Dense(1, activation=\"sigmoid\"))\n\ndiscriminator.summary()","810f906e":"#for the random image generated\nDiscriminator_Verdict = discriminator(Generated_Portrait)\nprint (Discriminator_Verdict)","91ea0b0f":"#Code Sourced from keras sample. find the link in the Resorces below\n\nclass GAN(tf.keras.Model):\n    def __init__(self, discriminator, generator, latent_dim):\n        super(GAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n\n    def compile(self, d_optimizer, g_optimizer, loss_fn):\n        super(GAN, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.loss_fn = loss_fn\n        self.d_loss_metric = tf.keras.metrics.Mean(name=\"d_loss\")\n        self.g_loss_metric = tf.keras.metrics.Mean(name=\"g_loss\")\n\n    @property\n    def metrics(self):\n        return [self.d_loss_metric, self.g_loss_metric]\n\n    def train_step(self, real_images):\n        # Sample random points in the latent space\n        batch_size = tf.shape(real_images)[0]\n        seed = tf.random.normal(shape=(batch_size, self.latent_dim))\n        # Decode them to fake images\n        generated_images = self.generator(seed)\n        # Combine them with real images\n        combined_images = tf.concat([generated_images, real_images], axis=0)\n        # Assemble labels discriminating real from fake images\n        labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n        # Add random noise to the labels - important trick!\n        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n        # Train the discriminator\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(combined_images)\n            d_loss = self.loss_fn(labels, predictions)\n        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n\n        # Sample random points in the latent space\n        seed = tf.random.normal(shape=(batch_size, self.latent_dim))\n\n        # Assemble labels that say \"all real images\"\n        misleading_labels = tf.zeros((batch_size, 1))\n\n        # Train the generator (note that we should *not* update the weights of the discriminator)!\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(self.generator(seed))\n            g_loss = self.loss_fn(misleading_labels, predictions)\n        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n\n        # Update metrics\n        self.d_loss_metric.update_state(d_loss)\n        self.g_loss_metric.update_state(g_loss)\n        return {\"d_loss\": self.d_loss_metric.result(), \"g_loss\": self.g_loss_metric.result()}","96952345":"#Defining the number of epochs\nepochs = 200\n#The optimizers for Generator and Discriminator\ndiscriminator_opt = tf.keras.optimizers.Adamax(1.5e-4,0.5)\ngenerator_opt = tf.keras.optimizers.Adamax(1.5e-4,0.5)\n#To compute cross entropy loss\nloss_fn = tf.keras.losses.BinaryCrossentropy()\n\n#Defining GAN Model\nmodel = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n\n#Compiling GAN Model\nmodel.compile(d_optimizer=discriminator_opt, g_optimizer=generator_opt, loss_fn=loss_fn)\n\n#Fitting the GAN\nhistory = model.fit(data, epochs=epochs)","1dfb256b":"pal=[\"#994F5F\",\"#E2AB30\"]\n#Plotting the learning curve\nhistory_df = pd.DataFrame(history.history)\nfig = plt.figure(figsize=(15,4))\nax=sns.lineplot(data=history_df, palette= pal)\nax.set(xlabel =\"Epochs\")\nax.set(ylabel =\"Loss\")\nax.set_title(\"Learning Curve\")","d52ecf3a":"#Number of images to be generate\nnum_img=18\n\n#A function to generate and save images\ndef Potrait_Generator():\n    Generated_Paintings = []\n    seed = tf.random.normal([num_img, latent_dim])\n    generated_image = generator(seed)\n    generated_image *= 255 \n    generated_image = generated_image.numpy()\n    for i in range(num_img):\n            img = tf.keras.preprocessing.image.array_to_img(generated_image[i])\n            Generated_Paintings.append(img)\n            img.save(\"Potraits{:02d}.png\".format(i)) \n    return \n\n#Generating images\nImages = Potrait_Generator()","2701ac4d":"#Loading generated images\nGenerated_path = \".\/\"\nPotraits_generated = tf.keras.preprocessing.image_dataset_from_directory(Generated_path, label_mode = None)\n#Plotting generated images\nShow_Img(Potraits_generated)","2b569e3d":"<a id=\"5\"><\/a>\n# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">TRAINING THE MODEL<\/p>\n\n<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">Train the model<\/p> Calling the above created GAN function trains the generator and discriminator simultaneously. \n\nTo implement the GAN we must define:\n* Number of epochs\n* The optimizers for Generator and Discriminator\n* The cross-entropy loss\n\nAfter defing optimizers and numbers of epochs, We will define, compile and fit the model. ","da13c7e3":"Now with this discriminator(untrained), let us see what verdict it has for the preiously generated image with random noise. ","3aad3a2f":"<a id=\"6\"><\/a>\n# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">EVALUATING THE MODEL<\/p>\n\nNow that I have my model trained, let us see how it performs.\nHaving a look at the performance of the model via Learning Curves\n\n<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">Ploting the Learning Curves<\/p>","78a5cbe2":"<a id=\"7\"><\/a>\n# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">CONCLUSION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#95856a\">In the evaluation of the model: We can see that the GAN picked up the patterns in the portraits. It worked quite well. For further improvement,  as GANs are notorious for being data-hungry, I would consider increasing the dataset. There are many inconsistencies in the data which is rather complicated for the GAN to learn. Cleaning the data with some consistencies in the portrait styles would certainly help. Training it longer i.e. for more epochs would also help. Lastly, one can always strive to make a  more robust architecture for the Neural Networks. <\/p> \n\n**Read More On My Blog:** [here](https:\/\/karnikakapoor.blogspot.com\/2021\/12\/ai-art.html)\n     \n\n**Resources:**\n\n[TensorFlow DCGAN](https:\/\/www.tensorflow.org\/tutorials\/generative\/dcgan)\n\n[Keras DCGAN](https:\/\/keras.io\/examples\/generative\/dcgan_overriding_train_step\/) \n\n[Training a GAN](https:\/\/machinelearningmastery.com\/how-to-code-the-generative-adversarial-network-training-algorithm-and-loss-functions\/)\n\n\n<p style=\"background-color:#EDE9DE;font-family:newtimeroman;color:#95856a;font-size:130%;text-align:center;border-radius:20px 20px;\"> If you liked this Notebook, please do upvote.<br> If you have any questions, feel free to comment!<br> \u2728Best Wishes\u2728<\/p>\n\n<a id=\"8\"><\/a>\n# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">END<\/p>","f3608f25":"Clearly, the output is a random seed containing noise as the Generator is not trained yet. \n\n<a id=\"3.2\"><\/a>\n# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">The Discriminator<\/p>\n\nIn GANs the Generator works along with the Discriminator. \n\nThe Discriminator network decided whether the data is fake aka created by the Generator or real i.e. from the original input data. To do so it applies a binary classification method using a sigmoid function to get an output in the range of 0 to 1.\n\n<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">Building a Discriminator<\/p>","68e927af":"Most of the images are portraits. A portrait is a painting representation of a person, The face is predominantly depicted portraits along with expressions and postures. To represent the personality of the subject. Since our model is relative a smaller GAN we have reduced the size of the image. \n\n<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">Preprocessing the data<\/p>\n\n**Normalization:** For the data normalization, I will convert the data in the range between 0 to 1. This helps in fast convergence and makes it easy for the computer to do calculations faster. \nEach of the three RGB channels in the image can take pixel values ranging from 0 to 256. Dividing it by 255 converts it to a range between 0 to 1. By doing this we ","8d2338b3":"The output of the discriminator i.e. The Verdict, Says that there is almost a fifty-fifty chance of the image being real. This is so because the Discriminator is not yet trained. So basically, An untrained Generarator generated some pixel-noise and the untrained Discriminator classified it as \"can't tell\". So far we are on a right track. \n\nLet us proceed and build the GAN architecture to train.\n\n<a id=\"4\"><\/a>\n# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">GAN COMPILATION<\/p>\n\nGAN training has two sections:\n\n**Section 1**: The Discriminator is trained while the Generator is idle. \nThe discriminator is trained real images and random noise (from an untrained generator). This trains it to tell between fake and real. This accommodates the discriminator to predict as fakes.\n\n**Section 2**: The Generator is trained while the Discriminator is idle.  In this section, the generator is trained.  After training the Discriminator, this step uses the predictions from the discriminator. Grants the generator to adjust the weights to try to deceive the discriminator. \n\nThe above method is repeated for a few epochs.  \n\n<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">The next section defines the GAN training<\/p>","987c658a":"##### <a id=\"2\"><\/a>\n# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">DATA LOADING & PREPREPROCESSING<\/p>\n\nFor this project, I am using .jpg files of images of portraits. The dataset includes various artists. I am loading data as TensorFlow.Dataset,, with a batch size of 64. I have reduced the image size to (64,64), presuming, it will be computationally less taxing on the GPU.\n\n<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">Loading the data<\/p>","e8b8c53a":"Now the data is up as a tensorflow Dataset object and is Prepocessed. Next up is building the GAN. \n\n<a id=\"3\"><\/a>\n# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">BUILDING GAN<\/p>\n\nGANs employs deep learning methods. It is a dexterous way of posing the problem as a supervised learning problem. It is composed of two models namely Generator and a Discriminator.\n\nTwo models are trained simultaneously by an adversarial process. A generator (\"the artist\") learns to create images that look like the dataset while a discriminator (\"the art critic\") learns to tell real images apart from fakes.\n\nDuring training, the generator progressively becomes better at creating images that look real, while the discriminator becomes better at telling them apart. The process reaches equilibrium when the discriminator can no longer distinguish real images from fakes.\n\n<img src=\"https:\/\/github.com\/KarnikaKapoor\/Files\/blob\/main\/Gan2.png?raw=true\">    \n\n**In this section I will be:**\n* Building a Generator\n* Building a Discriminator\n\n<a id=\"3.1\"><\/a>\n# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">The Generator<\/p>\n\nThe Generator is a neural network that generates the images. It takes in a random noise as seed and outputs sample data. As the GAN's training progresses the Generator output becomes more and more like the training set, as the Generator tries to improve the output so that the discrimination passes the output as a real image. \n\n**Following steps are involved in the models building**\n\n* Initialising the Model\n* Defining by adding layers\n\n<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">Building a Generator<\/p>","3d09dfa8":"# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">ART BY GAN<\/p>\n\n<img src=\"https:\/\/github.com\/KarnikaKapoor\/Files\/blob\/main\/Gan1.gif?raw=true\">\n\n<p style=\"font-family:newtimeroman;font-size:120%;color:#95856a;\">In this Notebook, I will build a Generative Adversarial Network  (GAN) to illustrate the workings of a Generative Adversarial Network and to generate images. Generative modelling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data. As GANs work by identifying the patterns in the data, I will be using oil painted portraits. However, glancing over the dataset gives me an idea that it is going to be a long shot. The orientation and poses in the dataset vary vastly. Keeping that in mind I am still willing to give it a try. Only because portraits are my jam. I basically love oil painted portraits.<\/p> \n\n<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">TABLE OF CONTENTS<\/p>   \n    \n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. DATA LOADING & PREPREPROCESSING](#2)\n    \n* [3. BUILDING GAN](#3)\n    * [3.1 The Generator](#3.1)\n    * [3.2 The Discriminator](#3.2)\n    \n    \n* [4. GAN COMPILATION](#4)  \n    \n* [5. TRAINING THE MODEL](#5) \n      \n* [6. EVALUATING THE MODEL](#6)\n    \n* [7. CONCLUSION](#7)\n    \n* [8. END](#8)\n\n\n<a id=\"1\"><\/a>\n# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">IMPORTING LIBRARIES<\/p>\n    \n<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">The following Libraries will be used in the project<\/p>\n    ","7d9f012f":"Now that I have the dataset loaded, let us have a look at a few images.","09151bfd":"Now that the Generator is framed, let us see what random output our untrained Generator produces to get an idea of the process. ","98a7d6ad":"This looks alright-ish! \n\nLet us get some portraits done by the GAN and appreciate the art created by this AI. \nTo get the art output I will create a function that saves the output portraits generated. We will be plotting the generated Portraits\n\n<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">AI makes Artwork<\/p>"}}