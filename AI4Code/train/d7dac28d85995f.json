{"cell_type":{"6a20e6a3":"code","50841aa2":"code","d82b2ffa":"code","dde2f8c7":"code","b766023e":"code","c76e4d68":"code","7905b8b9":"code","255512ac":"code","b863b068":"code","bd031ebb":"code","d16b392d":"code","ecccd1b9":"code","656bea32":"code","e0c18955":"code","905c9bc7":"code","2c323be0":"code","13889aac":"code","78e57fe2":"code","490e1da0":"code","4b8554fb":"code","be112248":"code","68b77cc6":"code","42549caa":"code","50e5c871":"code","d5eb9b3a":"code","052c4c13":"code","c7697328":"code","5299a156":"code","a757d894":"code","3a4fdda6":"code","6985c20f":"code","a92d94fc":"code","cf94921b":"code","b2a6ec05":"code","15c5a140":"code","75490c79":"code","aaf2e1e0":"code","6160700a":"code","bee12708":"code","8aada121":"code","bc9936f0":"code","941d3193":"code","db74a568":"code","8c76d8d8":"code","e819db36":"code","ac9b0c9a":"markdown","adfdd4f4":"markdown","80ca7d16":"markdown","f4e9bc08":"markdown","fac56493":"markdown","8189deb6":"markdown","0efef64f":"markdown"},"source":{"6a20e6a3":"%cd \/kaggle\/\n%ls","50841aa2":"!mkdir training\n%cd training","d82b2ffa":"# Download YOLOv5\n!git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n%cd yolov5\n# Install dependencies\n%pip install -qr requirements.txt  # install dependencies\n\n%cd ..\/\n\n%pip install --upgrade torch\n%pip install --upgrade torchvision\nimport torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","dde2f8c7":"# Install W&B \n%pip install -q --upgrade wandb\n# Login with token, follow with the guide\nimport wandb\nwandb.login()","b766023e":"# Import lib\nimport os\nimport gc\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport json\nimport yaml\nfrom shutil import copyfile\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold","c76e4d68":"#load json file\njson_file_path = '\/kaggle\/input\/cowboyoutfits\/train.json'\ndata = json.load(open(json_file_path, 'r'))","7905b8b9":"# Display the labels vs. numbers\ncount = 0\nla = [87, 1034, 131, 318, 588]\n\nfor x in la:\n    for i in range(len(data['annotations'])):\n        if data['annotations'][i]['category_id'] == x:\n            count+=1\n    print(x, count)","255512ac":"# convert the bounding box from COCO to YOLO format.\n\ndef cc2yolo_bbox(img_width, img_height, bbox):\n    dw = 1. \/ img_width\n    dh = 1. \/ img_height\n    x = bbox[0] + bbox[2] \/ 2.0\n    y = bbox[1] + bbox[3] \/ 2.0\n    w = bbox[2]\n    h = bbox[3]\n \n    x = x * dw\n    w = w * dw\n    y = y * dh\n    h = h * dh\n    return (x, y, w, h)","b863b068":"# \u9700\u8981\u6ce8\u610f\u4e0b\u56e0\u4e3a\u6211\u4eec\u7684annotation lable\u662f\u4e0d\u8fde\u7eed\u7684,\u4f1a\u5bfc\u81f4\u540e\u9762\u62a5\u9519,\u6240\u4ee5\u8fd9\u91cc\u751f\u6210\u4e00\u4e2amap\u6620\u5c04\ncate_id_map = {}\nnum = 0\nfor cate in data['categories']:\n    cate_id_map[cate['id']] = num\n    num+=1\n    \nprint(cate_id_map)","bd031ebb":"# transfer the format of annotation and generated a custom train dataframe for further analysis\n\ntrain_pd = pd.DataFrame(columns=['id','file_name','annotation','box_count','object_count', 'object_id', 'image_width','image_height'])\n\nfor i in tqdm(range(len(data['images']))):\n    filename = data['images'][i]['file_name']\n    image_width = data['images'][i]['width']\n    image_height = data['images'][i]['height']\n    image_id = data['images'][i]['id']\n    \n    annotation = []\n    box_count = 0\n    object_count = 0\n    object_id = []\n    \n    for anno in data['annotations']:\n        if anno['image_id'] == image_id:\n            box_count+=1\n            yolo_bbox = cc2yolo_bbox(image_width, image_height, anno['bbox']) # \"bbox\": [x,y,width,height]\n            temp_annotation = '{} {} {} {} {}'.format(cate_id_map[anno['category_id']], yolo_bbox[0], yolo_bbox[1], yolo_bbox[2], yolo_bbox[3])\n            \n            annotation.append(temp_annotation)\n            \n            if cate_id_map[anno['category_id']] not in object_id:\n                object_count+=1\n                object_id.append(cate_id_map[anno['category_id']])\n            \n    train_pd.loc[i] = image_id, filename, annotation, box_count, object_count, object_id, image_width, image_height","d16b392d":"train_pd.sample(10)\n\n#\u4e3b\u8981\u8bf4\u660e2\u70b9\uff0c\u65b0\u589e\u4e86\n# box_count,\u76ee\u6807\u6846\u6570\u91cf\n# object_count, \u76ee\u6807\u79cd\u7c7b\u6570\u91cf\n# object_id, \u76ee\u6807\u79cd\u7c7b","ecccd1b9":"# prepare the k-fold data\nNUM_FOLD = 5\n\nFold = StratifiedKFold(n_splits=NUM_FOLD, shuffle=True, random_state=233)\n\ndf_folds = train_pd.copy()\n\ndf_folds.loc[:, 'stratify_group'] = np.char.add(\n    df_folds['object_count'].values.astype(str),\n    df_folds['box_count'].apply(lambda x: f'_{x \/\/ 5}').values.astype(str)\n)\n\nfor fold_number,(train_index, val_index) in enumerate(Fold.split(X=df_folds.index, y=df_folds['stratify_group'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number","656bea32":"train_pd['box_count'].value_counts()","e0c18955":"df_folds.sample(5)","905c9bc7":"# \u770b\u770b\u521a\u521a\u7684\u65b0\u7c7b\u5206\u5e03\ndf_folds['stratify_group'].value_counts()","2c323be0":"#\u67e5\u770b fold \u5206\u5e03\u6570\u91cf\ndf_folds['fold'].value_counts()","13889aac":"#\u8fd9\u91cc\u518d\u5199\u4e00\u4e2a \u68c0\u67e5\u51fd\u6570\uff0c\u6765double check\u6211\u4eec\u5206\u7ec4\u662f\u5426\u5408\u7406\u3002\n\ndef list_class_number(fold_number):\n    pd_tmp = df_folds.loc[df_folds['fold'] == fold_number]\n\n    belt = 0\n    sunglasses = 0\n    boot = 0\n    cowboy_hat = 0\n    jacket = 0\n\n    for i in range(len(pd_tmp)):\n        for x in pd_tmp.iloc[i]['object_id']:\n            if x == 0:\n                belt+=1\n            if x == 1:\n                sunglasses+=1\n            if x == 2:\n                boot+=1\n            if x == 3:\n                cowboy_hat+=1\n            if x == 4:\n                jacket+=1\n\n    print(f'fold_{fold_number}, belt:{belt}, sunglasses:{sunglasses}, boot:{boot}, cowboy_bat:{cowboy_hat}, jacket:{jacket}\\n')\n    print(pd_tmp['box_count'].value_counts())\n    print(pd_tmp['object_count'].value_counts())","78e57fe2":"# \u7b2c\u4e00\u4e2a\u5206\u7ec4\nlist_class_number(0)\n\n# 0 2 300 39 57 236","490e1da0":"#\u7b2c\u4e8c\u4e2a\u5206\u7ec4\nlist_class_number(1)\n\n# 1 2 293 46 76 219","4b8554fb":"NUM_FOLD = 5\ndf = df_folds.copy()","be112248":"for fold in range(NUM_FOLD):\n        print(fold)\n        # Prepare train and valid df\n        train_df = df.loc[df.fold != fold].reset_index(drop=True)\n        valid_df = df.loc[df.fold == fold].reset_index(drop=True)\n\n        # Make dirs\n        os.makedirs(f'\/kaggle\/training\/dataset_folds_{fold}\/images\/train', exist_ok=True)\n        os.makedirs(f'\/kaggle\/training\/dataset_folds_{fold}\/images\/valid', exist_ok=True)\n        os.makedirs(f'\/kaggle\/training\/dataset_folds_{fold}\/labels\/train', exist_ok=True)\n        os.makedirs(f'\/kaggle\/training\/dataset_folds_{fold}\/labels\/valid', exist_ok=True)\n\n        # Move image and annotations to relevant split folder.\n        \n        for i in tqdm(range(len(train_df))):\n            train_row = train_df.loc[i]\n            train_name = train_row.file_name.split('.')[0]\n            copyfile(f'\/kaggle\/input\/cowboyoutfits\/images\/{train_name}.jpg', f'\/kaggle\/training\/dataset_folds_{fold}\/images\/train\/{train_name}.jpg')\n            yolo_txt_file = open(f'\/kaggle\/training\/dataset_folds_{fold}\/labels\/train\/{train_name}.txt', 'w')\n            for ann in train_row['annotation']:\n                yolo_txt_file.write(f'{ann}\\n')\n            \n        yolo_txt_file.close()\n        \n        for i in tqdm(range(len(valid_df))):\n            valid_row = valid_df.loc[i]\n            valid_name = valid_row.file_name.split('.')[0]\n            copyfile(f'\/kaggle\/input\/cowboyoutfits\/images\/{valid_name}.jpg', f'\/kaggle\/training\/dataset_folds_{fold}\/images\/valid\/{valid_name}.jpg')\n            yolo_txt_file = open(f'\/kaggle\/training\/dataset_folds_{fold}\/labels\/valid\/{valid_name}.txt', 'w')\n            for ann in valid_row['annotation']:\n                yolo_txt_file.write(f'{ann}\\n')\n            \n        yolo_txt_file.close()","68b77cc6":"# CHECK OUR DATA FOLDER STRUCTURE\n%ls \/kaggle\/training\/","42549caa":"%cat \/kaggle\/training\/dataset_folds_0\/labels\/train\/21f1961c263887ad.txt","50e5c871":"# Create  yaml file\n\nfor fold in range(NUM_FOLD):\n    data_yaml = dict(\n        train = f'..\/dataset_folds_{fold}\/images\/train\/',\n        val = f'..\/dataset_folds_{fold}\/images\/valid',\n        nc = 5,\n        names = ['belt', 'sunglasses', 'boot', 'cowboy_hat', 'jacket']\n    )\n\n    # we will make the file under the yolov5\/data\/ directory.\n    with open(f'\/kaggle\/training\/yolov5\/data\/data_folds_{fold}.yaml', 'w') as outfile:\n        yaml.dump(data_yaml, outfile, default_flow_style=True)\n    \n%cat \/kaggle\/training\/yolov5\/data\/data_folds_0.yaml # show your YAML file","d5eb9b3a":"# CHECK OUR DATA FOLDER STRUCTURE\n%ls \/kaggle\/training\/yolov5\/data\/","052c4c13":"%cd yolov5\/","c7697328":"IMG_SIZE = 640\nBATCH_SIZE = 64 # wisely choose, please check my W&B page for differencet performance with size [16,32,64]\nEPOCHS = 20\nMODEL = 'yolov5s.pt'  #\nname = f'{MODEL}_BS_{BATCH_SIZE}_EP_{EPOCHS}_fold_'","5299a156":"for fold in range(NUM_FOLD):    \n    print('FOLD NUMBER: ', fold)\n    print('<---------------------------------------------------------------->\\n')\n    \n    \n    if fold > 1:\n        break;\n    \n    !python train.py --img {IMG_SIZE} \\\n                     --batch {BATCH_SIZE} \\\n                     --epochs {EPOCHS} \\\n                     --data data_folds_{fold}.yaml \\\n                     --weights {MODEL} \\\n                     --save_period 2\\\n                     --project \/kaggle\/working\/kaggle-cwoboy \\\n                     --name {name}-{fold} \\\n                     --cache\n    print('<---------------------------------------------------------------->\\n')","a757d894":"%ls \/kaggle\/working\/kaggle-cwoboy\/","3a4fdda6":"# zip our result for loacl analysis\n\n!zip -r \/kaggle\/working\/training_result.zip \/kaggle\/working\/kaggle-cwoboy\/","6985c20f":"test_df = pd.read_csv('\/kaggle\/input\/cowboyoutfits\/test.csv')","a92d94fc":"os.makedirs('\/kaggle\/training\/inference\/test', exist_ok=True)","cf94921b":"for i in tqdm(range(len(test_df))):\n    test_row = test_df.loc[i]\n    test_name = test_row.file_name.split('.')[0]\n    copyfile(f'\/kaggle\/input\/cowboyoutfits\/images\/{test_name}.jpg', f'\/kaggle\/training\/inference\/test\/{test_name}.jpg')","b2a6ec05":"TEST_PATH = '..\/inference\/test\/'\n\nMODEL_PATH = [\n    '\/kaggle\/working\/kaggle-cwoboy\/yolov5s.pt_BS_64_EP_20_fold_-0\/weights\/best.pt',\n    '\/kaggle\/working\/kaggle-cwoboy\/yolov5s.pt_BS_64_EP_20_fold_-1\/weights\/best.pt',\n    #'\/kaggle\/working\/kaggle-cwoboy\/yolov5s.pt_BS_64_EP_20_fold_-2\/weights\/best.pt',\n    #'\/kaggle\/working\/kaggle-cwoboy\/yolov5s.pt_BS_64_EP_20_fold_-3\/weights\/best.pt',\n    #'\/kaggle\/working\/kaggle-cwoboy\/yolov5s.pt_BS_64_EP_20_fold_-4\/weights\/best.pt',\n    \n]\n#IMAGE_PATH = '..\/cowboyoutfits\/images\/'\n\n#fold 0   CONF 0.680","15c5a140":"# GO DETECT\n%cd \/kaggle\/training\/yolov5\/","75490c79":"!python detect.py --weights {MODEL_PATH[0]} {MODEL_PATH[1]} \\\n                  --source {TEST_PATH} \\\n                  --conf 0.25 \\\n                  --iou-thres 0.5 \\\n                  --save-txt \\\n                  --save-conf \\\n                  --max-det 50 \\\n                  --augment \\\n                  --half","aaf2e1e0":"#GET THE RIGHT PRED PATH\nPRED_PATH = '\/kaggle\/training\/yolov5\/runs\/detect\/exp3\/labels\/'\n\nprediction_files = os.listdir(PRED_PATH)\nprint('Number of test images predicted as opaque: ', len(prediction_files))","6160700a":"from PIL import Image","bee12708":"def yolo2cc_bbox(img_width, img_height, bbox):\n    x = (bbox[0] - bbox[2] * 0.5) * img_width\n    y = (bbox[1] - bbox[3] * 0.5) * img_height\n    w = bbox[2] * img_width\n    h = bbox[3] * img_height\n    \n    return (x, y, w, h)","8aada121":"cate_id_map = {87: 0, 1034: 1, 131: 2, 318: 3, 588: 4}\n\nre_cate_id_map = dict(zip(cate_id_map.values(), cate_id_map.keys()))\n\nre_cate_id_map","bc9936f0":"def make_submission(df, PRED_PATH, IMAGE_PATH):\n    output = []\n    for i in tqdm(range(len(df))):\n        row = df.loc[i] #\u6839\u636e\u76ee\u6807df\uff0c\u9009\u62e9\u884c\n        image_id = row['id'] #\u627e\u5230\u56fe\u7247\u7684id\n        file_name = row['file_name'].split('.')[0] #\u53bb\u6389\u6587\u4ef6\u540d\u5b57\u7684\u540e\u7f00'.jpg'\uff0c \u65b9\u4fbf\u540e\u9762\u8bfb\u53d6annotation\u6587\u4ef6\uff0c\u56e0\u4e3a\u662f\u4e00\u6837\u7684.\n        prediction_files = os.listdir(PRED_PATH)\n        \n        if f'{file_name}.txt' in prediction_files: # \u6839\u636e\u6587\u4ef6\u540d\u5b57\uff0c\u68c0\u7d22annotation\u6587\u4ef6\u5939\u4e0b\u7684\u6587\u4ef6, \u5982\u679c\u627e\u5230\u7684\u8bdd\uff1a\n            img = Image.open(f'{IMAGE_PATH}\/{file_name}.jpg') #\u8fd9\u91cc\u662f\u4e3a\u4e86\u540e\u9762\u8f6c\u6362yolo to coco \u683c\u5f0f\u65f6\u5019\uff0c\u8981\u5f97\u5230\u56fe\u7247\u7684size\n            width, height = img.size\n            with open(f'{PRED_PATH}\/{file_name}.txt', 'r') as file: #\u6839\u636e\u524d\u9762\u5224\u65ad\u627e\u5230\u6709annotation\u7684\uff0c\u51c6\u5907\u8bfb\u53d6\u6587\u4ef6\u3002\n                for line in file: #\u56e0\u4e3aannotaion\u662f\u6309\u7167\u884c\u5b58\u7684\uff0c\u4e00\u4e2a\u6587\u4ef6\u4e0b\u9762\u6709\u591a\u4e2aannotation \u5c31\u662f\u591a\u4e2a\u732b\u6846\u3002\n                    preds = line.strip('\\n').split(' ')  #\u628a\u6bcf\u884c\u8f6c\u6362\u6210list\u4fdd\u5b58\n                    preds = list(map(float, preds)) #conver string to float\n                    cc_bbox = yolo2cc_bbox(width, height, preds[1:-1]) # \u8f6c\u6362 \u951a\u6846\u683c\u5f0f.\n                    result = {\n                        'image_id': image_id,\n                        'category_id': re_cate_id_map[preds[0]],\n                        'bbox': cc_bbox,\n                        'score': preds[-1]\n                    }\n\n                    output.append(result)\n    return output","941d3193":"out_sub = make_submission(test_df, PRED_PATH, TEST_PATH)","db74a568":"op_pd = pd.DataFrame(out_sub)\n\nop_pd","8c76d8d8":"import zipfile \n\nname = f'{MODEL}_BS_{BATCH_SIZE}_EP_{EPOCHS}_fold_CONF_0.25_IOU_0.5'\n\nop_pd.to_json('\/kaggle\/working\/answer.json',orient='records')\nzf = zipfile.ZipFile(f'\/kaggle\/working\/{name}.zip', 'w')\nzf.write('\/kaggle\/working\/answer.json', f'answer.json')\nzf.close()","e819db36":"#delete the result directory\uff0csave space :D\n\n!rm -rf \/kaggle\/working\/kaggle-cwoboy\/","ac9b0c9a":"# DATA PROCESSING","adfdd4f4":"# INFERENCE","80ca7d16":"# Environment","f4e9bc08":"\ud83d\udccc\ud83d\udccc\n\u8fd9\u91cc\u7b80\u5355\u8bf4\u660e\u4e00\u4e0b\uff0c\u5206\u7ec4\u601d\u8def\u662f\u57fa\u4e8e`StractifiedKfold`, \u4f46\u662f\u56e0\u4e3a\u5bf9\u4e8e\u76ee\u6807\u68c0\u6d4b\uff0c\u9664\u4e86label balance \u8fd8\u6709\u5f88\u591a\u56e0\u7d20\u9700\u8981\u8003\u8651\uff0c\u6bd4\u5982\u8bf4\u76ee\u6807\u68c0\u6d4b\u6846\u7684\u6570\u91cf\u5206\u5e03\uff0c\u79cd\u7c7b\u5206\u5e03\uff0c\u751a\u81f3\u9762\u79ef\u5927\u5c0f\u5206\u5e03\uff0c\u6240\u4ee5\u8fd9\u8fb9\u53c2\u8003\u4e86\u4e00\u4e0bstackoverflow \u4e0a\u4e00\u4e2a[\u8ba8\u8bba](https:\/\/stackoverflow.com\/questions\/64164932\/stratified-k-fold-for-multi-class-object-detection)\u3002 \u4f7f\u7528\u4e86floor division `\/\/5` \u662f\u56e0\u4e3a\uff0c\u6839\u636e\u89c2\u5bdf\u6846\u7684\u6570\u91cf\u5206\u5e03\uff0c\u5927\u90e8\u5206\u90fd\u57285\u4ee5\u5185\u4e86\uff0c\u6240\u4ee5\u4f7f\u7528\u4e865 \u5bf9\u6570\u636e\u8fdb\u884c\u5206\u7ea7\uff0c\u540c\u65f6\u548cobjec_number\u4e00\u8d77\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u5b50\u7c7b, \u5305\u542b\u6846\u548c\u76ee\u6807\u7c7b\u7684\u6570\u636e\u3002 \u7136\u540e\u518d\u8fdb\u884ck-fold\u5206\u7ec4\u3002 \u540c\u65f6\u548c\u8fd9\u8fb9\u5e94\u8be5\u8fd8\u4e0d\u662f\u6700\u4f18\u89e3,\u4e5f\u8bb8\u8fd8\u6709\u5f88\u591a\u95ee\u9898\uff0c\u53ef\u4ee5\u8ba8\u8bba\u4e00\u4e0b\u3002","fac56493":"# TRAINING","8189deb6":"# PREPARE DATA FOLDERS","0efef64f":"# MAKE SUBMISSION"}}