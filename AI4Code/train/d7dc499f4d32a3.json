{"cell_type":{"92abdce0":"code","5822c0d4":"code","a2b327bd":"code","68f1cf96":"code","e02387cf":"code","f83c8f36":"code","d45eab64":"code","a0201021":"code","fa3513c4":"code","4e27d8fb":"code","58f23fb7":"code","564f48b5":"code","b7bdee89":"code","f449cef9":"code","4a78a08d":"code","74b3c229":"code","fda9446d":"code","55fcca85":"code","497865d6":"code","6cc34044":"code","434d3dd6":"code","2df6e379":"markdown","96df37bf":"markdown","40869af2":"markdown","4937fe0a":"markdown","dd8680d5":"markdown","c0391ed2":"markdown","74ad53a9":"markdown","4a4505df":"markdown","5abfd1d2":"markdown","a9fa0874":"markdown","42a707ea":"markdown","5bd16f7d":"markdown","f12921b6":"markdown","504dc05a":"markdown","aba829a9":"markdown","50fc910f":"markdown","30573606":"markdown","0b9b07bb":"markdown","9a3e4d79":"markdown","12802dcc":"markdown"},"source":{"92abdce0":"# Matplotlib Inline\n%matplotlib inline\n\n# Install Transformers\n!pip install transformers==4.12.5\n\n# Import Modules\nimport gc\nimport random\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom typing import Tuple\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom transformers import *","5822c0d4":"# Configure Strategy. Assume TPU...if not set default for GPU\/CPU\ntpu = None\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    \n# Seeds\ndef set_seeds(seed: int)->None:\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed) \n    \n# Generic Constants\nMAX_LEN = 512\nTEST_SIZE = 0.2\nLR = 0.00002\nVERBOSE = 1\nSEED = 1000\nset_seeds(SEED)\n\n# Set Autotune\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# Set Batch Size\nBASE_BATCH_SIZE = 4         # Modify to match your GPU card.\nif tpu is not None:         \n    BASE_BATCH_SIZE = 8     # TPU v2 or up...\nBATCH_SIZE = BASE_BATCH_SIZE * strategy.num_replicas_in_sync","a2b327bd":"# Summary\nprint(f'Seed: {SEED}')\nprint(f'Replica Count: {strategy.num_replicas_in_sync}')\nprint(f'Batch Size: {BATCH_SIZE}')\nprint(f'Learning Rate: {LR}')","68f1cf96":"def get_dpgnews_df(seed):\n    # Set 1: Articles\n    articles_df = pd.read_json('..\/input\/dpgmedia2019\/dpgMedia2019-articles-bypublisher.jsonl', lines = True)\n    articles_df = articles_df.set_index('id')\n    \n    # Set 2: Labels\n    labels_df = pd.read_json('..\/input\/dpgmedia2019\/dpgMedia2019-labels-bypublisher.jsonl', lines = True)\n    labels_df = labels_df.set_index('id')\n    \n    # Finalize Full Data\n    dpgnews_df = articles_df.join(labels_df, on = ['id'], how = 'inner')\n    \n    # Randomize all rows...\n    dpgnews_df = dpgnews_df.sample(frac = 1.0, random_state = seed)\n    dpgnews_df.reset_index(inplace = True)\n    print(f'DPGNews2019 Dataframe Shape: {dpgnews_df.shape}') \n\n    return dpgnews_df\n\ndef create_dataset(df, max_len, tokenizer, batch_size, shuffle = False):\n    total_samples = df.shape[0]\n\n    # Placeholders input\n    input_ids, input_masks = [], []\n    \n    # Placeholder output\n    labels = []\n\n    # Tokenize\n    for index, row in tqdm(zip(range(0, total_samples), df.iterrows()), total = total_samples):\n        \n        # Get title and description as strings\n        text = row[1]['text']\n        partisan = row[1]['partisan']\n\n        # Encode\n        input_encoded = tokenizer.encode_plus(text, add_special_tokens = True, max_length = max_len, truncation = True, padding = 'max_length')\n        input_ids.append(input_encoded['input_ids'])\n        input_masks.append(input_encoded['attention_mask'])\n        labels.append(1 if partisan == 'true' else 0)\n\n    # Prepare and Create TF Dataset.\n    all_input_ids = tf.constant(input_ids)\n    all_input_masks = tf.constant(input_masks)\n    all_labels = tf.constant(labels)\n    dataset =  tf.data.Dataset.from_tensor_slices(({'input_ids': all_input_ids, 'attention_mask': all_input_masks}, all_labels))\n    if shuffle:\n        dataset = dataset.shuffle(1024, reshuffle_each_iteration = True)\n    dataset = dataset.batch(batch_size, drop_remainder = True)\n    dataset = dataset.prefetch(AUTOTUNE)\n\n    return dataset\n\ndef create_t5_dataset(df, max_len, max_label_len, tokenizer, batch_size, shuffle = False):\n    total_samples = df.shape[0]\n\n    # Placeholders input\n    input_ids, input_masks = [], []\n    \n    # Placeholders output\n    output_ids, output_masks, labels = [], [], []\n\n    # Tokenize\n    for index, row in tqdm(zip(range(0, total_samples), df.iterrows()), total = total_samples):\n        \n        # Get title and description as strings\n        text = row[1]['text']\n        partisan = row[1]['partisan']\n        \n        # Process Input\n        input_encoded = tokenizer.encode_plus('classificeer: ' + text, add_special_tokens = True, max_length = max_len, truncation = True, padding = 'max_length')\n        input_ids.append(input_encoded['input_ids'])\n        input_masks.append(input_encoded['attention_mask'])\n\n        # Process Output\n        labels.append(1 if partisan == 'true' else 0)\n        partisan_label = 'politiek' if partisan == 'true' else 'neutraal'\n        output_encoded = tokenizer.encode_plus(partisan_label, add_special_tokens = True, max_length = max_label_len, truncation = True, padding = 'max_length')\n        output_ids.append(output_encoded['input_ids'])\n        output_masks.append(output_encoded['attention_mask'])\n\n    # Prepare and Create TF Dataset.\n    all_input_ids = tf.constant(input_ids)\n    all_output_ids = tf.constant(output_ids)\n    all_input_masks = tf.constant(input_masks)\n    all_output_masks = tf.constant(output_masks)\n    dataset =  tf.data.Dataset.from_tensor_slices(({'input_ids': all_input_ids, \n                                                    'labels': all_output_ids, \n                                                    'attention_mask': all_input_masks, \n                                                    'decoder_attention_mask': all_output_masks}))\n    if shuffle:\n        dataset = dataset.shuffle(1024, reshuffle_each_iteration = True)\n    dataset = dataset.batch(batch_size, drop_remainder = True)\n    dataset = dataset.prefetch(AUTOTUNE)\n\n    return dataset","e02387cf":"def ModelCheckpoint(model_name):\n    return tf.keras.callbacks.ModelCheckpoint(model_name, \n                                              monitor = 'val_accuracy', \n                                              verbose = 1, \n                                              save_best_only = True, \n                                              save_weights_only = True, \n                                              mode = 'max', \n                                              period = 1)\n\ndef create_mbert_model(model_type, strategy, config, lr):\n    # Create 'Standard' Classification Model\n    with strategy.scope():   \n        model = TFBertForSequenceClassification.from_pretrained(model_type, config = config)\n        \n        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\n        model.compile(optimizer = optimizer, loss = loss, metrics = [metric])        \n        \n        return model\n    \ndef create_xlm_roberta_model(model_type, strategy, config, lr):            \n    # Create 'Standard' Classification Model\n    with strategy.scope():   \n        model = TFXLMRobertaForSequenceClassification.from_pretrained(model_type, config = config)\n        \n        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\n        model.compile(optimizer = optimizer, loss = loss, metrics = [metric])        \n        \n        return model\n\nclass T5_Accuracy(tf.keras.metrics.Metric):\n    def __init__(self, label_length, name = 'accuracy', **kwargs):\n        super(T5_Accuracy, self).__init__(name = name, **kwargs)\n        self.t5_accuracy = self.add_weight(name = 'accuracy', initializer = 'zeros')\n        self.steps_counter = self.add_weight(name = 'steps_counter', initializer = 'zeros')\n        self.label_length = label_length\n\n    @tf.function\n    def update_state(self, y_true, y_pred, sample_weight = None):\n        # Reshape\n        y_pred = tf.reshape(y_pred, [-1, y_pred.shape[-1]])\n        y_true = tf.reshape(y_true, [-1])\n\n        # Get Max Indexes\n        y_pred = tf.math.argmax(y_pred, 1, output_type = 'int32')\n\n        # Cast to Int32\n        y_true = tf.cast(y_true, 'int32')\n        \n        # Reshape according to max label length...we want to compare the exact predictions made.\n        y_pred = tf.reshape(y_pred, [-1, self.label_length])\n        y_true = tf.reshape(y_true, [-1, self.label_length])\n        \n        # Compare Predicted and Labelled\n        y_comparison = tf.math.equal(y_pred, y_true)\n        \n        accuracy = tf.keras.backend.mean(tf.cast(tf.math.reduce_all(y_comparison, 1), tf.keras.backend.floatx()))\n        self.t5_accuracy.assign_add(accuracy)\n        self.steps_counter.assign_add(tf.ones(shape = ()))\n        \n    @tf.function    \n    def result(self):\n        return self.t5_accuracy \/ self.steps_counter\n\n    @tf.function    \n    def reset_state(self):\n        for var in self.variables:\n            var.assign(tf.zeros(shape = var.shape))\n\nclass KerasTFMT5ForConditionalGeneration(TFMT5ForConditionalGeneration):\n    def __init__(self, *args, log_dir = None, cache_dir = None, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.loss_tracker= tf.keras.metrics.Mean(name = 'loss') \n    \n    @tf.function\n    def train_step(self, data):\n        x = data\n        y = x['labels']\n        y = tf.reshape(y, [-1, 1])\n        with tf.GradientTape() as tape:\n            outputs = self(x, training = True)\n            loss = outputs[0]\n            logits = outputs[1]\n            loss = tf.reduce_mean(loss)\n            grads = tape.gradient(loss, self.trainable_variables)\n            \n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n        self.loss_tracker.update_state(loss)        \n        self.compiled_metrics.update_state(y, logits)\n        metrics = {m.name: m.result() for m in self.metrics}\n        \n        return metrics\n\n    def test_step(self, data):\n        x = data\n        y = x['labels']\n        y = tf.reshape(y, [-1, 1])\n        output = self(x, training = False)\n        loss = output[0]\n        loss = tf.reduce_mean(loss)\n        logits = output[1]\n        \n        self.loss_tracker.update_state(loss)\n        self.compiled_metrics.update_state(y, logits)\n        \n        return {m.name: m.result() for m in self.metrics}\n\nclass KerasTFByT5ForConditionalGeneration(TFT5ForConditionalGeneration):\n    def __init__(self, *args, log_dir = None, cache_dir = None, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.loss_tracker= tf.keras.metrics.Mean(name = 'loss') \n    \n    @tf.function\n    def train_step(self, data):\n        x = data\n        y = x['labels']\n        y = tf.reshape(y, [-1, 1])\n        with tf.GradientTape() as tape:\n            outputs = self(x, training = True)\n            loss = outputs[0]\n            logits = outputs[1]\n            loss = tf.reduce_mean(loss)\n            grads = tape.gradient(loss, self.trainable_variables)\n            \n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n        self.loss_tracker.update_state(loss)        \n        self.compiled_metrics.update_state(y, logits)\n        metrics = {m.name: m.result() for m in self.metrics}\n        \n        return metrics\n\n    def test_step(self, data):\n        x = data\n        y = x['labels']\n        y = tf.reshape(y, [-1, 1])\n        output = self(x, training = False)\n        loss = output[0]\n        loss = tf.reduce_mean(loss)\n        logits = output[1]\n        \n        self.loss_tracker.update_state(loss)\n        self.compiled_metrics.update_state(y, logits)\n        \n        return {m.name: m.result() for m in self.metrics}\n\ndef create_mt5_model(model_type, strategy, config, lr, max_label_len):\n    # Create Model\n    with strategy.scope():\n        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n        model = KerasTFMT5ForConditionalGeneration.from_pretrained(model_type, config = config)\n        model.compile(optimizer = optimizer, metrics = [T5_Accuracy(label_length = max_label_len)])\n        \n        return model\n\ndef create_byt5_model(model_type, strategy, config, lr, max_label_len):\n    # Create Model\n    with strategy.scope():\n        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n        model = KerasTFByT5ForConditionalGeneration.from_pretrained(model_type, config = config)\n        model.compile(optimizer = optimizer, metrics = [T5_Accuracy(label_length = max_label_len)])\n        \n        return model","f83c8f36":"# Get DpgNews Dataframe\ndpgnews_df = get_dpgnews_df(SEED)\ndpgnews_df.head()","d45eab64":"# Set Seaborn Style\nsns.set_theme(style = \"whitegrid\")\n\n# Get Counts for Text\ndpgnews_df['text_wordcount'] = dpgnews_df.text.str.split(' ').str.len()\n\n# Plot Words Count\ng = sns.displot(dpgnews_df, kind = 'kde', rug = True, x = 'text_wordcount', hue = 'partisan')\ng.set_axis_labels('Words Per Article Count', 'Density', labelpad = 10)\ng.fig.set_size_inches(20, 12)\nplt.show()","a0201021":"# XLM-RoBERTa Tokenizer\nmodel_type = 'jplu\/tf-xlm-roberta-base'\ntokenizer = AutoTokenizer.from_pretrained(model_type, add_prefix_space = False, do_lower_case = False)\n\n# Get Token Count \/ PLot Token Count\ndpgnews_df['text_token_count'] = 0\nfor index, row in tqdm(dpgnews_df.iterrows(), total = dpgnews_df.shape[0]):\n    # Get title and description as strings\n    text = row['text']\n    \n    # Get the full tokenized Text... No Max Length, No Truncation etc...\n    input_encoded = tokenizer.encode_plus(text, add_special_tokens = True)\n    dpgnews_df.loc[index, 'text_token_count'] = len(input_encoded['input_ids']) \n\n# Plot Token Count per Article\ng = sns.displot(dpgnews_df, kind = 'kde', rug = True, x = 'text_token_count', hue = 'partisan')\ng.set_axis_labels(f'Tokens Per Article Count - Model Type: {model_type}', 'Density', labelpad = 10)\ng.fig.set_size_inches(20, 12)\nplt.show()","fa3513c4":"# Text Token Counts Smaller\/Greater than 512 .. which is max input size for Transformers model\nprint(f'\\nArticles with 512 or less tokens: {dpgnews_df[dpgnews_df[\"text_token_count\"] <= 512].shape[0]}')\nprint(f'Articles with more than 512 tokens: {dpgnews_df[dpgnews_df[\"text_token_count\"] > 512].shape[0]}')","4e27d8fb":"# Multi-Lingual BERT Constants\nEPOCHS = 4\nmodel_type = 'bert-base-multilingual-cased'\n\n# Set Config\nconfig = AutoConfig.from_pretrained(model_type, num_labels = 2) # 2 labels because we do binary classification\n\n# Set Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_type, add_prefix_space = False, do_lower_case = False)\n\n# Cleanup\ntf.keras.backend.clear_session()    \nif tpu is not None:\n    tf.tpu.experimental.initialize_tpu_system(tpu)\ngc.collect()\n\n# Create Train Test Split\ntrain_df, val_df = train_test_split(dpgnews_df, \n                                    stratify = dpgnews_df.partisan.values, \n                                    test_size = TEST_SIZE, \n                                    random_state = SEED)\n\n# Create Train and Validation Datasets\ntrain_dataset = create_dataset(train_df, MAX_LEN, tokenizer, BATCH_SIZE, shuffle = True)\nvalidation_dataset = create_dataset(val_df, MAX_LEN, tokenizer, BATCH_SIZE, shuffle = False)\n\n# Steps\ntrain_steps = train_df.shape[0] \/\/ BATCH_SIZE\nval_steps = val_df.shape[0] \/\/ BATCH_SIZE\nprint(f'Train Steps: {train_steps}')\nprint(f'Val Steps: {val_steps}')\n\n# Create Model\nmodel = create_mbert_model(model_type, strategy, config, LR)\n\n# Model Summary\nprint(model.summary())\n\n# Fit Model\nhistory = model.fit(train_dataset,\n                    steps_per_epoch = train_steps,\n                    validation_data = validation_dataset,\n                    validation_steps = val_steps,\n                    epochs = EPOCHS, \n                    verbose = VERBOSE,\n                    callbacks = [ModelCheckpoint('mbert_model.h5')])","58f23fb7":"# Evaluate Dataset\nmodel.load_weights('mbert_model.h5') # Reload the Best Model\neval = model.evaluate(validation_dataset, steps = val_steps, verbose = VERBOSE)\nprint(f'\\n===== Multi-Lingual BERT Classification Accuracy: {eval[1] * 100}%')","564f48b5":"# XLM-RoBERTa Constants\nEPOCHS = 4\nmodel_type = 'jplu\/tf-xlm-roberta-base'\n\n# Set Config\nconfig = AutoConfig.from_pretrained(model_type, num_labels = 2) # 2 labels because we do binary classification\n\n# Set Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_type, add_prefix_space = False, do_lower_case = False)\n\n# Cleanup\ntf.keras.backend.clear_session()    \nif tpu is not None:\n    tf.tpu.experimental.initialize_tpu_system(tpu)\ngc.collect()\n\n# Create Train Test Split\ntrain_df, val_df = train_test_split(dpgnews_df, \n                                    stratify = dpgnews_df.partisan.values, \n                                    test_size = TEST_SIZE, \n                                    random_state = SEED)\n\n# Create Train and Validation Datasets\ntrain_dataset = create_dataset(train_df, MAX_LEN, tokenizer, BATCH_SIZE, shuffle = True)\nvalidation_dataset = create_dataset(val_df, MAX_LEN, tokenizer, BATCH_SIZE, shuffle = False)\n\n# Steps\ntrain_steps = train_df.shape[0] \/\/ BATCH_SIZE\nval_steps = val_df.shape[0] \/\/ BATCH_SIZE\nprint(f'Train Steps: {train_steps}')\nprint(f'Val Steps: {val_steps}')\n\n# Create Model\nmodel = create_xlm_roberta_model(model_type, strategy, config, LR)\n\n# Model Summary\nprint(model.summary())\n\n# Fit Model\nhistory = model.fit(train_dataset,\n                    steps_per_epoch = train_steps,\n                    validation_data = validation_dataset,\n                    validation_steps = val_steps,\n                    epochs = EPOCHS, \n                    verbose = VERBOSE,\n                    callbacks = [ModelCheckpoint('xlmroberta_model.h5')])","b7bdee89":"# Evaluate Dataset\nmodel.load_weights('xlmroberta_model.h5')\neval = model.evaluate(validation_dataset, steps = val_steps, verbose = VERBOSE)\nprint(f'\\n===== XLM-RoBERTa Classification Accuracy: {eval[1] * 100}%')","f449cef9":"# mT5 Constants\nEPOCHS = 12\nMAX_LABEL_LEN = 3   # For MT5 ==> 3 ... 'politiek' \/ 'neutraal' is tokenized to 3 tokens.\nmodel_type = 'google\/mt5-base'\n\n# Set Config\nconfig = AutoConfig.from_pretrained(model_type)\n\n# Set Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_type)","4a78a08d":"# Label Example - True\nprint(f'\\nLabelling Example: True ==> politiek')\npartisan_label = 'politiek'\noutput_encoded = tokenizer.encode_plus(partisan_label, \n                                       add_special_tokens = True, \n                                       max_length = MAX_LABEL_LEN, \n                                       truncation = True, \n                                       padding = 'max_length')\nprint(output_encoded['input_ids'])\nprint(output_encoded['attention_mask'])\nprint(tokenizer.decode(output_encoded['input_ids']))\n\n# Label Example - False\nprint(f'\\nLabelling Example: False ==> neutraal')\npartisan_label = 'neutraal'\noutput_encoded = tokenizer.encode_plus(partisan_label, \n                                       add_special_tokens = True, \n                                       max_length = MAX_LABEL_LEN, \n                                       truncation = True, \n                                       padding = 'max_length')\nprint(output_encoded['input_ids'])\nprint(output_encoded['attention_mask'])\nprint(tokenizer.decode(output_encoded['input_ids']))","74b3c229":"# Cleanup\ntf.keras.backend.clear_session()    \nif tpu is not None:\n    tf.tpu.experimental.initialize_tpu_system(tpu)\ngc.collect()\n\n# Create Train Test Split\ntrain_df, val_df = train_test_split(dpgnews_df, \n                                    stratify = dpgnews_df.partisan.values, \n                                    test_size = TEST_SIZE, \n                                    random_state = SEED)\n\n# Create Train and Validation Datasets\ntrain_dataset = create_t5_dataset(train_df, MAX_LEN, MAX_LABEL_LEN, tokenizer, BATCH_SIZE, shuffle = True)\nvalidation_dataset = create_t5_dataset(val_df, MAX_LEN, MAX_LABEL_LEN, tokenizer, BATCH_SIZE, shuffle = False)\n\n# Steps\ntrain_steps = train_df.shape[0] \/\/ BATCH_SIZE\nval_steps = val_df.shape[0] \/\/ BATCH_SIZE\nprint(f'Train Steps: {train_steps}')\nprint(f'Val Steps: {val_steps}')\n\n# Create Model\nmodel = create_mt5_model(model_type, strategy, config, LR, MAX_LABEL_LEN)\n\n# Model Summary\nprint(model.summary())\n\n# Fit Model\nhistory = model.fit(train_dataset,\n                    steps_per_epoch = train_steps,\n                    validation_data = validation_dataset,\n                    validation_steps = val_steps,\n                    epochs = EPOCHS, \n                    verbose = VERBOSE,\n                    callbacks = [ModelCheckpoint('mt5_model.h5')])        ","fda9446d":"# Evaluate Dataset\nmodel.load_weights('mt5_model.h5')\neval = model.evaluate(validation_dataset, steps = val_steps, verbose = VERBOSE)\nprint(f'\\n===== mT5 Classification Accuracy: {eval[0] * 100}%')","55fcca85":"# ByT5 Constants\nEPOCHS = 12\nMAX_LABEL_LEN = 9   # For ByT5 ==> 9 ... 'politiek' \/ 'neutraal' is tokenized to 9 tokens\/characters.\nmodel_type = 'google\/byt5-base'\n\n# Set Config\nconfig = AutoConfig.from_pretrained(model_type)\n\n# Set Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_type)","497865d6":"# Label Example - True\nprint(f'\\nLabelling Example: True ==> politiek')\npartisan_label = 'politiek'\noutput_encoded = tokenizer.encode_plus(partisan_label, \n                                       add_special_tokens = True, \n                                       max_length = MAX_LABEL_LEN, \n                                       truncation = True, \n                                       padding = 'max_length')\nprint(output_encoded['input_ids'])\nprint(output_encoded['attention_mask'])\nprint(tokenizer.decode(output_encoded['input_ids']))\n\n# Label Example - False\nprint(f'\\nLabelling Example: False ==> neutraal')\npartisan_label = 'neutraal'\noutput_encoded = tokenizer.encode_plus(partisan_label, \n                                       add_special_tokens = True, \n                                       max_length = MAX_LABEL_LEN, \n                                       truncation = True, \n                                       padding = 'max_length')\nprint(output_encoded['input_ids'])\nprint(output_encoded['attention_mask'])\nprint(tokenizer.decode(output_encoded['input_ids']))","6cc34044":"# Cleanup\ntf.keras.backend.clear_session()    \nif tpu is not None:\n    tf.tpu.experimental.initialize_tpu_system(tpu)\ngc.collect()\n\n# Create Train Test Split\ntrain_df, val_df = train_test_split(dpgnews_df, \n                                    stratify = dpgnews_df.partisan.values, \n                                    test_size = TEST_SIZE, \n                                    random_state = SEED)\n\n# Create Train and Validation Datasets\ntrain_dataset = create_t5_dataset(train_df, MAX_LEN, MAX_LABEL_LEN, tokenizer, BATCH_SIZE, shuffle = True)\nvalidation_dataset = create_t5_dataset(val_df, MAX_LEN, MAX_LABEL_LEN, tokenizer, BATCH_SIZE, shuffle = False)\n\n# Steps\ntrain_steps = train_df.shape[0] \/\/ BATCH_SIZE\nval_steps = val_df.shape[0] \/\/ BATCH_SIZE\nprint(f'Train Steps: {train_steps}')\nprint(f'Val Steps: {val_steps}')\n\n# Create Model\nmodel = create_byt5_model(model_type, strategy, config, LR, MAX_LABEL_LEN)\n\n# Model Summary\nprint(model.summary())\n\n# Fit Model\nhistory = model.fit(train_dataset,\n                    steps_per_epoch = train_steps,\n                    validation_data = validation_dataset,\n                    validation_steps = val_steps,\n                    epochs = EPOCHS, \n                    verbose = VERBOSE,\n                    callbacks = [ModelCheckpoint('byt5_model.h5')])        ","434d3dd6":"# Evaluate Dataset\nmodel.load_weights('byt5_model.h5')\neval = model.evaluate(validation_dataset, steps = val_steps, verbose = VERBOSE)\nprint(f'\\n===== ByT5 Classification Accuracy: {eval[0] * 100}%')","2df6e379":"## Introduction\n\nRecently when working with some NLP models I was getting curious about the performance of multi-lingual models based on languages other than the (usually..) default language English.\nWhile recent models such as GPT-3 are catching the majority of media attention when it comes to state of the art NLP models it is good to realize that there are many more NLP models available than can be very usefull for both small and large companies.\nHaving a wide range of Multi-Lingual NLP models available guarantees that these models can be used by as much people as possible. It is always possible to train one of those models yourself from scratch based on a new language...but that poses a lot more challenges and requires a lot more resources. Using an already pretrained model is the preferred choice.\n\nWith my own native language Dutch being one of those smaller languages when it comes to the amount of text that is used in the larger datasets to pretrain those models that would be a very interresting comparison to make. I've picked a rather small dutch dataset to finetune 4 Multi-Lingual NLP models on a classification task. The classification accuracy achieved by each of those models provides an indication of what is possible with these 4 models.\n\nIf your language belongs to the roughly 100 various languages that are supported by the 4 models you should be able to train on that pretty easy. In the provided links to the various papers describing the models you should be able to find any information about the supported languages. The code as used in this notebook should provide a good starting point to do your own experiments.\n\n## Dataset\n\nAs dataset for the comparison of the models I picked a dutch dataset from a few years back. It is the 'DpgMedia2019: A Dutch News Dataset for Partisanship Detection' dataset which is available at the following [link](https:\/\/github.com\/dpgmedia\/partisan-news2019). It is a Dutch dataset with news articles aimed at detecting partisan news. For each news article a label is provided to indicate whether the article is partisan or not. It contains roughly 100K of news articles from various dutch news papers. With a roughly 50\/50 split for the label (true\/false) it is also very nicely balanced. Since I couldn't find any further NLP model implementations for this dataset I thought it would be nice to use it ;-)\n\n## Models\n\nI've chosen the following 4 Multi-Lingual NLP models to compare. Note that for each of the 4 models larger model sizes are available than the selected Base size. Picking a larger model size could lead to a higher achieved classification accuracy. The main reason to pick the current size was to make sure that all 4 models would fit within 1 session run in Kaggle taking into account a maximum session time of 9 hours.\n\nThe following are the 4 models:\n* Multi-Lingual BERT Base\n* XLM-RoBERTa Base\n* mT5 Base\n* ByT5 Base\n\nFor ByT5 and mT5 the largest size available is XXL with 12.9B parameters. Recently for XLM-RoBERTa there were also larger sized models released as described in the following [paper](https:\/\/arxiv.org\/abs\/2105.00572). For XLM-RoBERTa there is now also an XXL sized model with 10.9B parameters available. So if you have the budget or hardware available...\n\n## Setup and Limitations\n\nTo make it possible todo a decent level of comparison for the 4 models I choose to use the same basic setup and hyperparameters for all 4 models. The number of epochs used by each model does however differ. XLM-RoBERTa and Multi-Lingual BERT already had already some level of fine-tuning with the default model weights as provided by Huggingface. These models need a few epochs before they already converge and possibly start to overfit. For the mT5 and ByT5 models as provided by Huggingface they only have been pre-trained. As stated in the HuggingFace documentation these models still need to be fine-tuned on their specific downstream task(s) and require more epochs to reach convergence. \n\nFor training the models a simple train-test split will be used. The dataset will be split in 80% used for training and 20% used for validation\/evaluation.For a more rigorous training and validation approach a StratifiedKFold Cross-Validation strategy would be the preferred choice. This requires however a lot more available TPU hours or other high-end GPU hardware...which I don't have ;-)\n\nFor the Multi-Lingual BERT, XLM-RoBERTa and mT5 models the text will be tokenized and only the first 512 tokens will be used. Any tokenized text beyond the 512 tokens will be discarded. \n\nFor the ByT5 model the input text will be tokenized based on the individual characters and not based on words or part of words. This means that only the first 512 characters of the input text will be tokenized.\n\nFine-tuning the hyperparameters specifically for each model could further improve classification performance. Also improving on using the data beyond the first 512 tokens or characters could improve the performance of the models.","96df37bf":"To maximize the reproducibility for each model run we will use the same Seed, Batch Size and Learning Rate.","40869af2":"While we can see there are a few extreme outliers a lot of the articles will miss few or no information because of the truncated input sequences.\n\nLets count how much articles are within the 512 token limit for XLM-RoBERTa and how much have more tokens. Do note that this will vary slightly for mT5 and Multi-Lingual BERT as they have different vocabularies. For ByT5 - which is tokenized based on the raw input characters - this will vary a lot more.","4937fe0a":"After training the model we can perform the evaluation on the validation set. Let's see what classification accuracy has been achieved.","dd8680d5":"## Models Code","c0391ed2":"The above image shows a distribution plot of the count of individual words per news article separated by partisan lable. While there are some extreme outliers the majority of articles has less than 1000 - 1500 words per article.\n\nThe used Multi-lingual Transformer models have a maximum input size length of 512 tokens. Each news article will be tokenized and any sequence of tokens longer then 512 tokens will be truncated. While this process will lead to some loss of information I will still use it this way to have a generic approach accross all models.\n\nIf we compare this to the pre-transformers way of NLP we would also remove stop words, remove punctuation, perform stemming or lemmatization. So while in a different way we would also lose information.\n\nThe following distribution plot shows the token count (without truncation...) per news article separated by partisan lable. This will give an impression of the amount of information lost when truncation will be performed. The tokenization count is based on the tokenizer as is used for the 'XLM-RoBERTa' model.","74ad53a9":"One 'issue' that we have with mT5 (and for ByT5...) is that it is a generative model. It generates text and doesn't have a Dense output layer as Multi-Lingual BERT or XLM-RoBERTa where we output a probability between 0 and 1 to predict the partisan label.\n\nWhat we can do however is generate 'text-labels' that present the classification label.\n\nSo we will train the mT5 and ByT5 models to predict the following 'text-labels':\n* Partisan label: True ==> mT5\/ByT5 label to generate\/classify as 'politiek'\n* Partisan label: False ==> mT5\/ByT5 label to generate\/classify as 'neutraal'\n\nBelow you can see how the labels are encoded and what their token values are.","4a4505df":"## ByT5\n\nThe fourth and final model we will put to the test is ByT5. It is based on the earlier released 'Text to Text Transfer Transformer' mT5. Just as mT5 it is trained on a new 101 languages version of the CommonCrawl dataset. To review the paper for ByT5 use the following [link](https:\/\/arxiv.org\/abs\/2105.13626). The main difference is that ByT5 doesn't use tokens but operates directly on raw text or bytes.\n\nSince ByT5 was only pre-trained in an unsupervised manner it still needs to be fine-tuned on any downstream task. Since we only train ByT5 on one task it is not necessary to use the task-prefix ('classificeer: ' ==> Dutch for 'Classify' ... but basically you can choose any name for the task-prefix). However when you modify the code to perform multi-task training it is necessary to use a task-prefix. So consider it added for your own convenience.\n\nNote that we will train the model for 12 epochs. Since it needs to be completely fine-tuned for the specific task of classifying the partisan news articles it requires a lot more epochs to reach convergence.","5abfd1d2":"The following section contains the code for setting up the different models, saving the model files and a custom accuracy metric implementation for the mT5 and ByT5 models.","a9fa0874":"## Multi-Lingual BERT\n\nThe first model we will put to the test is Multi-Lingual BERT. When released in 2018 BERT caused a small revolution by improving drastically the scores achieved on multiple NLP tasks. To review the paper use the following [link](https:\/\/arxiv.org\/abs\/1810.04805).\n\nMulti-Lingual BERT is the same model...however pre-trained on a large multi-lingual Wikipedia dataset containing the top 104 languages. The model was pre-trained on 2 objectives: Masked Language Modelling and Next Sentence Prediction.\n\nNote that we will train the model for 4 epochs only. With the size of the used dataset this is more than sufficient to make sure the model converges.","42a707ea":"## Summary and Results\n\nAfter training all the models and running the evaluation on the validation set we can see the achieved accuracy for each of the models.\n\nBelow an overview of the achieved accuracy scores (these scores are based on the previous version...so the scores can vary slightly because of the randomness involved...):\n1. XLM-RoBERTa: 96.0%\n2. Multi-Lingual BERT: 95.1%\n3. mT5: 93.2%\n4. ByT5: 90.6%\n\nWe can clearly see that XLM-RoBERTa achieves the highest score while being closely followed by Multi-Lingual BERT.\n\nThe mT5 and ByT5 models achieve lower accuracy scores but considering that this is achieved by generating the correct label I think this shows clearly the strengths of those models.\n\nAs mentioned earlier specific fine-tuning of the hyperparameters of each model very likely will further increase the achieved scores.\n\n## Citation Dataset\n\n\n    @misc{1908.02322,\n      Author = {Chia-Lun Yeh and Babak Loni and Mari\u00eblle Hendriks and Henrike Reinhardt and Anne Schuth},\n      Title = {DpgMedia2019: A Dutch News Dataset for Partisanship Detection},\n      Year = {2019},\n      Eprint = {arXiv:1908.02322},\n    }","5bd16f7d":"After training the model we can perform the evaluation on the validation set. Let's see what classification accuracy has been achieved.","f12921b6":"After training the model we can perform the evaluation on the validation set. Let's see what classification accuracy has been achieved.","504dc05a":"## EDA\n\nLet's do some simple Exploratory Data Analysis. First lets take a look at the amount of words per news article.","aba829a9":"## Get DPGNews Dataset\n\nNext it is time to create the news dataset that will be used for training and validation of the 4 models.\n\nIn the dataframe sample output below you can see the 'text' column that will be used as input text for each model. Also visible is the column 'partisan' that will be used as the label for which the models will learn to classify the input text.","50fc910f":"## mT5\n\nThe third model we will put to the test is mT5. It is based on the earlier released 'Text to Text Transfer Transformer' T5. But in this case it is the Multi-Lingual version trained on a new 101 languages version of the CommonCrawl dataset. To review the paper for mT5 use the following [link](https:\/\/arxiv.org\/abs\/2010.11934).\n\nSince mT5 was only pre-trained in an unsupervised manner it still needs to be fine-tuned on any downstream task. Since we only train mT5 on one task it is not necessary to use the task-prefix ('classificeer: ' ==> Dutch for 'Classify' ... but basically you can choose any name for the task-prefix). However when you modify the code to perform multi-task training it is necessary to use a task-prefix. So consider it added for your own convenience.\n\nNote that we will train the model for 12 epochs. Since it needs to be completely fine-tuned for the specific task of classifying the partisan news articles it requires a lot more epochs to reach convergence.","30573606":"As explained already for the mT5 model we use 'text-labels' that the model can generate.\n\nBelow you can see the same 'text-labels' as were used for mT5 but now in the way that ByT5 encodes them. You can see that it performs character based encoding.","0b9b07bb":"After training the model we can perform the evaluation on the validation set. Let's see what classification accuracy has been achieved.","9a3e4d79":"The next section contains some plumbing code to get and combine the different json files of the dataset into a Pandas DataFrame.\n\nAlso the necessary code to create the Tensorflow Datasets is provided.","12802dcc":"## XLM-RoBERTa\n\nThe second model we will put to the test is XLM-RoBERTa. It is based on the earlier released RoBERTa model. In the paper it is mentioned that XLM-RoBERTa outperforms Multi-Lingual BERT on various tasks. To review the paper use the following [link](https:\/\/arxiv.org\/abs\/1911.02116).\n\nXLM-RoBERTa was pre-trained on 2.5TB of filtered text from the Common Crawl dataset. The dataset contains text for the top 100 languages. The model was pre-trained on 1 objective: Masked Language Modelling.\n\nNote that we will train the model for 4 epochs only. With the size of the used dataset this is more than sufficient to make sure the model converges."}}