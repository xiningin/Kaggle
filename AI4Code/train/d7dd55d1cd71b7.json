{"cell_type":{"9340fc7b":"code","e5fc7937":"code","67ea2fa2":"code","e30bc046":"code","ac81ca66":"code","0abec943":"code","b0f0e24a":"code","0d73017d":"code","37a26ab6":"code","b045ba5f":"code","2fd8b0fe":"code","1ce671bf":"code","53151295":"code","97d9d51e":"code","cd01648d":"code","e457e5c8":"code","8698f89d":"code","1cdcaa2e":"code","09dd02bc":"code","c6a1b62f":"code","d2f4bb5c":"code","c9306cdc":"code","9a04f266":"code","acbdbc7c":"code","baecc93a":"code","06ffcccb":"code","be211a12":"code","3aaf5542":"code","561f8a22":"markdown","e1c14da6":"markdown","9627538f":"markdown","2715725a":"markdown","7c90cefd":"markdown","89d2ee5c":"markdown","ec42969e":"markdown","c27d44c2":"markdown","ed168a7d":"markdown","79aaf0f0":"markdown","5237a789":"markdown","78e78b65":"markdown","6594a076":"markdown","614f7b06":"markdown","2a2ccdd8":"markdown","bd7324fb":"markdown","2f128ec8":"markdown","d5ba2f59":"markdown","a7023827":"markdown","5ea9d860":"markdown","d42b06f2":"markdown"},"source":{"9340fc7b":"from IPython.display import Image\nfrom IPython.core.display import HTML\nImage(url= \"https:\/\/lmb.informatik.uni-freiburg.de\/people\/ronneber\/u-net\/u-net-architecture.png\")","e5fc7937":"import numpy as np \nimport pandas as pd \n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport matplotlib.pyplot as plt\n\nimport PIL\nimport requests\nfrom io import BytesIO ","67ea2fa2":"#This dataset is already included in TensorFlow Datasets and we can simply download it.\n\n!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=oxford_iiit_pet:3.1.0\n# download the dataset and get info\ndataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)","e30bc046":"# this contains the test and train splits.\nprint(dataset.keys())","ac81ca66":"# see information about the dataset\nprint(info)","0abec943":"# Preprocessing Utilities\n\ndef random_flip(input_image, input_mask):\n    '''does a random flip of the image and mask'''\n    if tf.random.uniform(()) > 0.5:\n        # we wand to randomly flip images if a random genarated value \n        # is bigger than 0.5 we will flip the image, else leave it \n        input_image = tf.image.flip_left_right(input_image)\n        input_mask = tf.image.flip_left_right(input_mask)\n\n    return input_image, input_mask\n\n\ndef normalize(input_image, input_mask):\n    '''\n    normalizes the input image pixel values to be from [0,1].\n    subtracts 1 from the mask labels to have a range from [0,2]\n    '''\n    # Normalize the input image by casting to float32 and the divide \n    # each pixel value in the input imageby 255 \n    input_image = tf.cast(input_image, tf.float32) \/ 255.0\n    \n    # adjust the segmentation mask's pixel values.\n    # in the original dataset the pixels in the segmentation mask are labeled as such:\n    # 1:foreground, 2:background, 3:Not Classified\n    # subtract 1 from these values and we will interpret these as {'pet', 'background', 'outline'}\n    input_mask -= 1\n    return input_image, input_mask\n\n# The data is already splitted to training and testing datasets\n\n@tf.function\ndef load_image_train(datapoint):\n    '''resizes, normalizes, and flips the training data'''\n    \n    # resize the input image to 128 x 128\n    input_image = tf.image.resize(datapoint['image'], (128, 128), method='nearest')\n    \n    # resize the mask to 128 x 128\n    input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128), method='nearest')\n    \n    # Augmentation and normalization using the previosly defined fumctions\n    input_image, input_mask = random_flip(input_image, input_mask)\n    input_image, input_mask = normalize(input_image, input_mask)\n\n    return input_image, input_mask\n\n\ndef load_image_test(datapoint):\n    '''resizes and normalizes the test data'''\n    \n    # preparing the test images without Augmentation \n    input_image = tf.image.resize(datapoint['image'], (128, 128), method='nearest')\n    input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128), method='nearest')\n    input_image, input_mask = normalize(input_image, input_mask)\n\n    return input_image, input_mask","b0f0e24a":"# preprocess the train and test sets\ntrain = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ntest = dataset['test'].map(load_image_test)","0d73017d":"BATCH_SIZE = 16\nBUFFER_SIZE = 1000\n\n# shuffle and group the train set into batches\ntrain_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n\n# do a prefetch to optimize processing\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n# group the test set into batches\ntest_dataset = test.batch(BATCH_SIZE)","37a26ab6":"# class list of the mask pixels\nclass_names = ['pet', 'background', 'outline']\n\n\ndef display_with_metrics(display_list, iou_list, dice_score_list):\n    '''displays a list of images\/masks and overlays a list of IOU and Dice Scores'''\n  \n    metrics_by_id = [(idx, iou, dice_score) for idx, (iou, dice_score) in enumerate(zip(iou_list, dice_score_list)) if iou > 0.0]\n    metrics_by_id.sort(key=lambda tup: tup[1], reverse=True)  # sorts in place\n  \n    display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(class_names[idx], iou, dice_score) for idx, iou, dice_score in metrics_by_id]\n    display_string = \"\\n\\n\".join(display_string_list)\n\n    display(display_list, [\"Image\", \"Predicted Mask\", \"True Mask\"], display_string=display_string) \n\n\ndef display(display_list,titles=[], display_string=None):\n    '''displays a list of images\/masks'''\n\n    plt.figure(figsize=(15, 15))\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(titles[i])\n        plt.xticks([])\n        plt.yticks([])\n        if display_string and i == 1:\n            plt.xlabel(display_string, fontsize=12)\n        img_arr = tf.keras.preprocessing.image.array_to_img(display_list[i])\n        plt.imshow(img_arr)\n  \n    plt.show()\n\n\ndef show_image_from_dataset(dataset):\n    '''displays the first image and its mask from a dataset'''\n\n    for image, mask in dataset.take(1):\n        sample_image, sample_mask = image, mask\n    display([sample_image, sample_mask], titles=[\"Image\", \"True Mask\"])\n\n\ndef plot_metrics(metric_name, title, ylim=5):\n    '''plots a given metric from the model history'''\n    plt.title(title)\n    plt.ylim(0,ylim)\n    plt.plot(model_history.history[metric_name],color='blue',label=metric_name)\n    plt.plot(model_history.history['val_' + metric_name],color='green',label='val_' + metric_name)","b045ba5f":"# display an image from the train set\nshow_image_from_dataset(train)\n\n# display an image from the test set\nshow_image_from_dataset(test)","2fd8b0fe":"Image(url= \"https:\/\/lmb.informatik.uni-freiburg.de\/people\/ronneber\/u-net\/u-net-architecture.png\")","1ce671bf":"# Encoder Utilities\n\ndef conv2d_block(input_tensor, n_filters, kernel_size = 3):\n    '''\n    Adds 2 convolutional layers with the parameters passed to it\n\n    Args:\n      input_tensor (tensor) -- the input tensor\n      n_filters (int) -- number of filters\n      kernel_size (int) -- kernel size for the convolution\n\n    Returns:\n      tensor of output features\n    '''\n    # first layer\n    x = input_tensor\n    for i in range(2):\n        x = tf.keras.layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\n                                   kernel_initializer = 'he_normal', padding = 'same')(x)\n        x = tf.keras.layers.Activation('relu')(x)\n  \n    return x\n\n\ndef encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3):\n    '''\n    Adds two convolutional blocks and then perform down sampling on output of convolutions.\n\n    Args:\n      input_tensor (tensor) -- the input tensor\n      n_filters (int) -- number of filters\n      kernel_size (int) -- kernel size for the convolution\n\n    Returns:\n      f - the output features of the convolution block \n      p - the maxpooled features with dropout\n    '''\n\n    f = conv2d_block(inputs, n_filters=n_filters)\n    p = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(f)\n    p = tf.keras.layers.Dropout(0.3)(p)\n\n    return f, p\n\n\ndef encoder(inputs):\n    '''\n    This function defines the encoder or downsampling path.\n\n    Args:\n      inputs (tensor) -- batch of input images\n\n    Returns:\n      p4 - the output maxpooled features of the last encoder block\n      (f1, f2, f3, f4) - the output features of all the encoder blocks\n    '''\n    f1, p1 = encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3)\n    f2, p2 = encoder_block(p1, n_filters=128, pool_size=(2,2), dropout=0.3)\n    f3, p3 = encoder_block(p2, n_filters=256, pool_size=(2,2), dropout=0.3)\n    f4, p4 = encoder_block(p3, n_filters=512, pool_size=(2,2), dropout=0.3)\n\n    return p4, (f1, f2, f3, f4)","53151295":"def bottleneck(inputs):\n    '''\n    This function defines the bottleneck convolutions to extract more features before the upsampling layers.\n    '''\n  \n    bottle_neck = conv2d_block(inputs, n_filters=1024)\n\n    return bottle_neck","97d9d51e":"# Decoder Utilities\n\ndef decoder_block(inputs, conv_output, n_filters=64, kernel_size=3, strides=3, dropout=0.3):\n    '''\n    defines the one decoder block of the UNet\n\n    Args:\n      inputs (tensor) -- batch of input features\n      conv_output (tensor) -- features from an encoder block\n      n_filters (int) -- number of filters\n      kernel_size (int) -- kernel size\n      strides (int) -- strides for the deconvolution\/upsampling\n      padding (string) -- \"same\" or \"valid\", tells if shape will be preserved by zero padding\n\n    Returns:\n      c (tensor) -- output features of the decoder block\n   '''\n    u = tf.keras.layers.Conv2DTranspose(n_filters, kernel_size, strides = strides, padding = 'same')(inputs)\n    c = tf.keras.layers.concatenate([u, conv_output])\n    c = tf.keras.layers.Dropout(dropout)(c)\n    c = conv2d_block(c, n_filters, kernel_size=3)\n\n    return c\n\n\ndef decoder(inputs, convs, output_channels):\n    '''\n    Defines the decoder of the UNet chaining together 4 decoder blocks. \n  \n    Args:\n      inputs (tensor) -- batch of input features\n      convs (tuple) -- features from the encoder blocks\n      output_channels (int) -- number of classes in the label map\n\n    Returns:\n      outputs (tensor) -- the pixel wise label map of the image\n    '''\n  \n    f1, f2, f3, f4 = convs\n\n    c6 = decoder_block(inputs, f4, n_filters=512, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n    c7 = decoder_block(c6, f3, n_filters=256, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n    c8 = decoder_block(c7, f2, n_filters=128, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n    c9 = decoder_block(c8, f1, n_filters=64, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n\n    outputs = tf.keras.layers.Conv2D(output_channels, (1, 1), activation='softmax')(c9)\n\n    return outputs","cd01648d":"OUTPUT_CHANNELS = 3\n\ndef unet():\n    '''\n    Defines the UNet by connecting the encoder, bottleneck and decoder.\n    '''\n\n    # specify the input shape\n    inputs = tf.keras.layers.Input(shape=(128, 128,3,))\n\n    # feed the inputs to the encoder\n    encoder_output, convs = encoder(inputs)\n\n    # feed the encoder output to the bottleneck\n    bottle_neck = bottleneck(encoder_output)\n\n    # feed the bottleneck and encoder block outputs to the decoder\n    # specify the number of classes via the `output_channels` argument\n    outputs = decoder(bottle_neck, convs, output_channels=OUTPUT_CHANNELS)\n  \n    # create the model\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    return model\n\n# instantiate the model\nmodel = unet()\n\n# see the resulting model architecture\nmodel.summary()","e457e5c8":"# configure the optimizer, loss and metrics for training\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","8698f89d":"# configure the training parameters and train the model\n\nTRAIN_LENGTH = info.splits['train'].num_examples\nEPOCHS = 20\nVAL_SUBSPLITS = 15\nSTEPS_PER_EPOCH = TRAIN_LENGTH \/\/ BATCH_SIZE\nVALIDATION_STEPS = info.splits['test'].num_examples\/\/BATCH_SIZE\/\/VAL_SUBSPLITS\n\n# this will take around 20 minutes to run\nmodel_history = model.fit(train_dataset, epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          validation_steps=VALIDATION_STEPS,\n                          validation_data=test_dataset)","1cdcaa2e":"# Plot the training and validation loss\nplot_metrics(\"loss\", title=\"Training vs Validation Loss\", ylim=1)","09dd02bc":"# Prediction Utilities\n\ndef get_test_image_and_annotation_arrays():\n    '''\n      Unpacks the test dataset and returns the input images and segmentation masks\n  '''\n\n    ds = test_dataset.unbatch()\n    ds = ds.batch(info.splits['test'].num_examples)\n  \n    images = []\n    y_true_segments = []\n\n    for image, annotation in ds.take(1):\n        y_true_segments = annotation.numpy()\n        images = image.numpy()\n  \n    y_true_segments = y_true_segments[:(info.splits['test'].num_examples - (info.splits['test'].num_examples % BATCH_SIZE))]\n  \n    return images[:(info.splits['test'].num_examples - (info.splits['test'].num_examples % BATCH_SIZE))], y_true_segments\n\n\ndef create_mask(pred_mask):\n    '''\n    Creates the segmentation mask by getting the channel with the highest probability. Remember that we\n    have 3 channels in the output of the UNet. For each pixel, the predicition will be the channel with the\n    highest probability.\n    '''\n    pred_mask = tf.argmax(pred_mask, axis=-1)\n    pred_mask = pred_mask[..., tf.newaxis]\n    return pred_mask[0].numpy()\n\n\ndef make_predictions(image, num=1):\n    '''\n    Feeds an image to a model and returns the predicted mask.\n    '''\n\n    image = np.reshape(image,(1, image.shape[0], image.shape[1], image.shape[2]))\n    pred_mask = model.predict(image)\n    pred_mask = create_mask(pred_mask)\n\n    return pred_mask","c6a1b62f":"def class_wise_metrics(y_true, y_pred):\n    class_wise_iou = []\n    class_wise_dice_score = []\n\n    smoothening_factor = 0.00001\n    for i in range(3):\n    \n        intersection = np.sum((y_pred == i) * (y_true == i))\n        y_true_area = np.sum((y_true == i))\n        y_pred_area = np.sum((y_pred == i))\n        combined_area = y_true_area + y_pred_area\n\n        iou = (intersection + smoothening_factor) \/ (combined_area - intersection + smoothening_factor)\n        class_wise_iou.append(iou)\n\n        dice_score =  2 * ((intersection + smoothening_factor) \/ (combined_area + smoothening_factor))\n        class_wise_dice_score.append(dice_score)\n\n    return class_wise_iou, class_wise_dice_score","d2f4bb5c":"# Setup the ground truth and predictions.\n\n# get the ground truth from the test set\ny_true_images, y_true_segments = get_test_image_and_annotation_arrays()\n\n# feed the test set to th emodel to get the predicted masks\nresults = model.predict(test_dataset, steps=info.splits['test'].num_examples\/\/BATCH_SIZE)\nresults = np.argmax(results, axis=3)\nresults = results[..., tf.newaxis]","c9306cdc":"# compute the class wise metrics\ncls_wise_iou, cls_wise_dice_score = class_wise_metrics(y_true_segments, results)","9a04f266":"# show the IOU for each class\nfor idx, iou in enumerate(cls_wise_iou):\n    spaces = ' ' * (10-len(class_names[idx]) + 2)\n    print(\"{}{}{} \".format(class_names[idx], spaces, iou))","acbdbc7c":"# show the Dice Score for each class\nfor idx, dice_score in enumerate(cls_wise_dice_score):\n    spaces = ' ' * (10-len(class_names[idx]) + 2)\n    print(\"{}{}{} \".format(class_names[idx], spaces, dice_score))","baecc93a":"def Make_a_prediction_on_an_image_from_the_web(url):\n    \n    response = requests.get(url)\n    img = PIL.Image.open(BytesIO(response.content))\n    img = img.resize((128, 128))\n    img_array = np.array(img)\n    pred = make_predictions(img_array,  num=1)\n    display([img, pred],titles=['Image', 'Prediction'], display_string=None)","06ffcccb":"img_2 = \"https:\/\/thumbs.dreamstime.com\/z\/cat-kitten-white-background-26220873.jpg\"\nMake_a_prediction_on_an_image_from_the_web(img_2)","be211a12":"img_3 = \"https:\/\/www.dbrg.uk\/uploads\/5\/5\/5\/6\/55561953\/published\/guide-buying-puppy.jpg?1513968022\"\nMake_a_prediction_on_an_image_from_the_web(img_3)","3aaf5542":"img_4 = \"https:\/\/www.oliverpetcare.com\/wp-content\/uploads\/2019\/11\/puppy-dog-animal-pet-mammal-pug-1046756-pxhere.com_-e1572947696385.jpg\"\nMake_a_prediction_on_an_image_from_the_web(img_4)","561f8a22":"## Encoder","e1c14da6":"## Decoder","9627538f":"## Bottleneck","2715725a":"## Putting it all together","7c90cefd":"# Define the model","89d2ee5c":"For each decoder block there are 2 inputs 1) the output of the previos decoder block and the output of the encoder block at the same level. \n\nThere are 4 decoder blocks starting from below to ubove, let us take the first one for example, **the first input** is the output of the bottleneck, the bottleneck output will be 8 x 8 so we upsample it using Conv2DTranspose to be 16 x 16 and **the second input** is the first output (before applying the max pooling) of the encoder at the same level of the decoder block we are now. Now we concatenate them using tf.keras.layers.concatenate, pass it to a dropout layer and finally a conv block. \n\nThe same thing happen in each decoder block. ","ec42969e":"# Make a prediction on an image from the web ","c27d44c2":"# Imports ","ed168a7d":"# Compile and Train the model","79aaf0f0":"A UNet consists of an encoder (downsampler) and decoder (upsampler) with a bottleneck in between.","5237a789":"# Download the dataset","78e78b65":"- Simple augmentation by flipping the image\n\n- Normalizing the pixel values\n\n- Resizing the images","6594a076":"Here the output of encoder function is **p4** the second output (after applying the max pooling) of the last encoder block that will be passed to the **Bottleneck** and a tuple of first output (before applying the max pooling) for each encoder block to be passed to the **Decoder blocks**","614f7b06":"# Make predictions","2a2ccdd8":"Note: The code here is taken from a course that I take on coursera, I modified it and explain it. Enjoy learning :) ","bd7324fb":"**For example lets take the last encoder block.** \n\n\nThis block take the second output (after applying the max pooling) of the third block, the input dimention is (16 x 16), this input will be passed through 2 conv layers each of them has 512 filters and the output will be stored in **f4** varialble showen bellow, then it will be passed through a max pooling layer then dropout layer and the output will be stored in p4 varialble showen bellow. ","2f128ec8":"**Configure dataset for performance** <br>\n\nTo train a model with this dataset you will want the data:<br>\n\n- To be well shuffled.<br>\n- To be batched.<br>\n- Batches to be available as soon as possible.<br>\n\nThese features can be added using the tf.data API. For more details:\n\nhttps:\/\/www.tensorflow.org\/tutorials\/load_data\/images#using_tfdata_for_finer_control\n\n\n`.cache()` keeps the images in memory after they're loaded off disk during the first epoch.\nThis will ensure the dataset does not become a bottleneck while training your model.\nIf your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n\n`.prefetch()` overlaps data preprocessing and model execution while training.\n\nsource: https:\/\/www.tensorflow.org\/tutorials\/load_data\/images#configure_the_dataset_for_perfor","d5ba2f59":"### Some utilities to help us visualize our data and metrics.","a7023827":"Bottleneck used to extract more features. This does not have a pooling layer so the dimensionality remains the same.\nThe Bottleneck takes the second output (after applying the max pooling) of the last encoder block","5ea9d860":"# Prepare the Dataset","d42b06f2":"As we can see from the figure above, the **Encoder** cinsists of 4 encoder blocks each one of them consists of 2 convolutional layers activated by ReLU then max pooling layer and finnaly dropout layer. \n\nwe will take 2 outputs from each block: 1) **before applying the max pooling, 2) after applying the max pooling.** \n\nThe second output (after applying the max pooling) will go to the next encoder block (the burgundy colored arrows). The first output (before applying the max pooling) will go to an decoder block in the same level of it as we see in the ubove figure (the gray arrows).\n\nIn the next section we will take the second output (after applying the max pooling) of the last encoder block and feed it to the **Bottleneck**. "}}