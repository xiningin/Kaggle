{"cell_type":{"ae71da16":"code","ed356c92":"code","d1a06d56":"code","632a4c09":"code","102d3ca7":"code","c8927ae2":"code","329e205a":"code","08636056":"code","8150984b":"code","ebe86d0f":"code","5d07dfbe":"code","a67330fb":"code","baef55ee":"code","bc8ae968":"code","258b729f":"code","c9b84ac5":"code","20d06290":"code","95ed62af":"code","68b850e1":"code","4c36ab82":"code","732d3c40":"code","e7d05583":"code","59b11b8f":"code","dc8b812f":"code","d041b786":"code","7af3f7b9":"code","e47ec5f4":"code","8acd2872":"code","7ea30b86":"code","480307ee":"code","42812eed":"code","2592cf64":"code","0b564147":"code","4910c371":"code","e7986b8a":"code","c1ce021f":"code","daacbe3e":"code","0a269241":"code","540fdfcb":"code","ca597e2d":"code","4bbf9c26":"code","177c40e6":"code","23743998":"code","e211c5a9":"markdown","73e3f4b3":"markdown","b97bd2ed":"markdown","0fca1244":"markdown","50eab083":"markdown","19829c6f":"markdown","724ab9a0":"markdown"},"source":{"ae71da16":"from google.cloud import bigquery\nfrom google.cloud.bigquery import magics\nfrom kaggle.gcp import KaggleKernelCredentials\n\nPROJECT_ID = 'kaggle-bqml-256206'\n\nclient = bigquery.Client(project=PROJECT_ID, location=\"US\")\ndataset = client.create_dataset('bqml_example', exists_ok=True)\n# using magics.context object to set credentials\nmagics.context.credentials = KaggleKernelCredentials()\n# using magics.context object to set project\nmagics.context.project = PROJECT_ID","ed356c92":"# create a reference to our table\ntrain_table = \"kaggle-competition-datasets.geotab_intersection_congestion.train\"\ntest_table = 'kaggle-competition-datasets.geotab_intersection_congestion.test'","d1a06d56":"import os\nimport json\nimport numpy as np\nDATA_FOLDER = '..\/input\/bigquery-geotab-intersection-congestion'\nwith open(os.path.join(DATA_FOLDER, 'submission_metric_map.json'), \n          'rt') as fp:\n    submission_map = json.load(fp)\nLABELS = {val:key for key, val in submission_map.items()}\nprint(LABELS)","632a4c09":"feature_cols = ['RowId', 'IntersectionId', 'Latitude', 'Longitude', 'EntryStreetName', \n                'ExitStreetName', 'EntryHeading', 'ExitHeading', 'Hour', 'Weekend', 'Month', \n                'Path', 'City']\ntable = client.get_table(train_table)\nfor field in table.schema:\n    if field.name in feature_cols:\n        print(field.name, field.field_type)","102d3ca7":"import pandas as pd\nfrom itertools import repeat, chain\nfrom google.cloud.bigquery.magics import _run_query \n\ndef stack_results(results):\n    keys = list(chain.from_iterable(\n        [repeat(k, len(v)) for k, v in results]))\n    values = [v for k, v in results] \n    frame = pd.concat(values)\n    if len(keys) == len(frame):\n        frame['label'] = keys\n        frame = frame.set_index('label')\n    return frame\n\ndef make_query(query_text, job_config=None, **kwargs):\n    query = _run_query(\n        client, query_text.format(**kwargs),\n        job_config=job_config)\n    return query.to_dataframe()\n\ndef make_queries(query_text, configs=None, **kwargs):\n    results = []\n    for label in submission_map.values():\n        percent = label[-2:]\n        if label.startswith('T'):\n            model = \"bqml_example.model_TTS\"\n        elif label.startswith('D'):\n            model = \"bqml_example.model_DF\"\n        model += percent\n        if configs is None:\n            jobConfig=None\n        else:\n            query_params = configs[model]\n            jobConfig = bigquery.QueryJobConfig()\n            jobConfig.query_parameters = query_params\n        df = make_query(query_text,\n                        job_config=jobConfig,\n                        model_name=model,\n                        label_name=label,\n                        **kwargs)\n        results.append((label, df))\n    return results\n    \ndef iterate_query(query_text, configs=None, **kwargs):\n    results = make_queries(query_text, configs=configs,\n                           **kwargs)\n    frame = stack_results(results)\n    return frame\n\ndef change_columns(df, model_num):\n    df['RowId'] = df['RowId'].apply(str) + '_%s'%(model_num)\n    df.rename(columns={'RowId': 'TargetId', \n                       submission_map[model_num]: 'Target'}, \n              inplace=True)","c8927ae2":"%load_ext google.cloud.bigquery","329e205a":"%%bigquery rowids\nSELECT \n    DISTINCT RowId,\n    TotalTimeStopped_p20,\n    TotalTimeStopped_p50,\n    TotalTimeStopped_p80, \n    DistanceToFirstStop_p20,\n    DistanceToFirstStop_p50,\n    DistanceToFirstStop_p80\nFROM\n`kaggle-competition-datasets.geotab_intersection_congestion.train`\nORDER BY RowId ASC","08636056":"# find a good split given the evalution fraction\neval_frac = 0.1\nsplit_index = int(len(rowids)*0.9)\nsplit_rowid = rowids.iloc[split_index][0]\ninternal_split = rowids.iloc[int(((rowids.RowId <= split_rowid).sum())*0.9)][0]\nprint(\"using the RowId = {} as splitting point\\n\"\n      \"training set fraction   = {:.3}\\n\"\n      \"evaluation set fraction = {:.3}\".format(\n          split_rowid, \n          ((rowids.RowId <= split_rowid).sum() \/ len(rowids)),\n          ((rowids.RowId > split_rowid).sum() \/ len(rowids))\n     ))","8150984b":"import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(3, 1, figsize=(8, 6))\nfor i, label in enumerate(list(LABELS.keys())[3:]):\n  plt.sca(axes[i])\n  plt.title(\"RowId vs %s\" % label)\n  plt.plot(rowids['RowId'], rowids[label])\n  axes[i].axvline(split_rowid, ls='--', color='r')\n  top = max(rowids[label]) + 200\n  axes[i].text(split_rowid + 10, top,\"test split\", fontsize=12,\n               ha=\"left\", rotation=30)\n  axes[i].axvline(internal_split, ls='--', color='g')\n  axes[i].text(internal_split  + 10, top,\"valid split\", fontsize=12,\n               ha=\"left\", rotation=30)\n  \nplt.tight_layout()","ebe86d0f":"# create process feature table \nproced_feature_stmt = \"\"\"\nCREATE TABLE IF NOT EXISTS `{dest_table}` AS \nWITH ProcedFeatures AS ( # create a table with processing string feautres\n    SELECT * REPLACE(\n     REGEXP_REPLACE(\n         REGEXP_REPLACE(\n             IFNULL(EntryStreetName, \"Unkonwn\"), \n                     r\"St.?( |$)\",\"Street\"),\n         r\"Ave.?( |$)\", \"Avenue\"\n     ) AS EntryStreetName,\n     REGEXP_REPLACE(\n        REGEXP_REPLACE(\n             IFNULL(ExitStreetName, \"Unkonwn\"),\n                 r\"St.?( |$)\",\"Street\"),\n         r\"Ave.?( |$)\", \"Avenue\"\n     ) AS ExitStreetName)\n    FROM\n      `{source_table}`\n      ORDER BY RowId),\n    DirectionEnc AS ( # creating a 8x8=64 direction to angle mapping\n      SELECT \n        entry.directions AS EntryDirection,\n        entry.angles AS EntryAngle,\n        exit.directions AS ExitDirection,\n        exit.angles AS ExitAngle\n      FROM \n        (SELECT ARRAY<STRUCT<directions STRING, angles INT64>>[\n            (\"N\", 0), (\"NE\", 45), (\"E\", 90), (\"SE\", 135), \n            (\"S\", 180), (\"SW\", 225), (\"W\", 270), (\"NW\", 325)] as code)\n            AS book,\n        UNNEST(book.code) as entry,\n        UNNEST(book.code) as exit),\n    AngleJoin AS( # create a table grouped by entryheading and exitheading \n    SELECT\n      EntryHeading,\n      EntryAngle,\n      ExitHeading,\n      Exitangle,\n      heading\n      FROM \n       (SELECT \n            EntryHeading, \n            ExitHeading,\n            ARRAY_AGG(\n              STRUCT(RowId, EntryStreetName, ExitStreetName) ORDER BY Rowid) as heading\n         FROM ProcedFeatures\n         GROUP BY EntryHeading, ExitHeading)\n         INNER JOIN\n         DirectionEnc As d\n         on d.EntryDirection=EntryHeading \n         AND d.ExitDirection=ExitHeading\n       ),\n    PackedGeo AS ( # create a table grouped longitutde and latitude by City\n       SELECT \n          City,\n          ST_CENTROID_AGG(ST_GeogPoint(Longitude,Latitude)) AS Center,\n          ARRAY_AGG(STRUCT(Longitude, Latitude, RowId) ORDER BY RowId) AS Points\n       FROM ProcedFeatures\n    GROUP BY City\n  )\nSELECT * FROM\n(\n  SELECT\n  u.RowId,\n  distance,\n  EntryAngle,\n  ExitAngle,\n  (ExitAngle - EntryAngle) AS rotation,\n  (u.EntryStreetName = u.ExitStreetName) AS OntoSameStreet\nFROM \n  AngleJoin,\n  UNNEST(AngleJoin.heading) AS u\n  INNER JOIN\n  (SELECT \n    ST_DISTANCE(Center, ST_GeogPoint(\n     citypoint.Longitude, citypoint.Latitude)) AS distance,\n     City,\n     RowId\n   FROM \n     PackedGeo,\n     UNNEST(Points) as citypoint) As d\n   ON u.RowId = d.RowId\n   ORDER BY u.RowId\n   )\n   INNER JOIN ProcedFeatures\n   USING (RowId)\n   {mask_stmt}\n\"\"\"\nmask_stmt=\"\"\"\n   INNER JOIN `kaggle-bqml-256206.bqml_example.mask_train`\n   USING (RowId)\n\"\"\"\ndef create_processed_table(source_table):\n    suffix = source_table.split('.')[-1]\n    table_name = \"proced_%s\" % suffix\n    dest_table = \"{project_id}.{dataset_id}.{table_name}\".format(\n            project_id=dataset.project,\n            dataset_id=dataset.dataset_id,\n            table_name=table_name)\n    if suffix == 'train':\n        query_text = proced_feature_stmt.format(\n            source_table=source_table,\n            dest_table=dest_table,\n            mask_stmt=mask_stmt)\n    else:\n        query_text = proced_feature_stmt.format(\n            source_table=source_table,\n            dest_table=dest_table,\n            mask_stmt=''\n    )\n    job_config = bigquery.QueryJobConfig()\n    job_config.dry_run = False\n    job_config.use_query_cache = True\n    _ = _run_query(client, query_text, \n                   job_config=job_config)\n    return dest_table","5d07dfbe":"proc_train = create_processed_table(train_table)\nproc_test  = create_processed_table(test_table)","a67330fb":"no_proc_stmt=\"\"\"\nFROM\n  `{table}`\n\"\"\"\nproc_stmt=\"\"\"\n   ,distance,\n    OntoSameStreet,\n    EntryAngle,\n    ExitAngle,\n    rotation\nFROM \n    `{table}`\n\"\"\"\nfeature_stmt=\"\"\"\n        Weekend,\n        #Hour,\n        CAST(Hour AS STRING) as hour,\n        #Month,\n        CAST(Month AS STRING) as month,\n        City,\n        #Path,\n        #IntersectionId,\n        #CAST(IntersectionId AS STRING) as intersection,\n        EntryHeading,\n        ExitHeading,\n        #EntryStreetName, \n        #IFNULL(EntryStreetName, \"Unkonwn\") AS EntryStreetNameNL,\n        #ExitStreetName\n        #IFNULL(ExitStreetName, \"Unkonwn\") AS ExitStreetNameNL\n        FORMAT(\"%d %d\", month, hour) as day,\n        FORMAT(\"%s %d\", City, IntersectionId) as cityinter\n        {proc_stmt}\n\"\"\"\nno_proc_features = feature_stmt.format(\n    proc_stmt=no_proc_stmt.format(table=train_table))\nwith_proc_features =  feature_stmt.format(\n    proc_stmt=proc_stmt.format(table=proc_train))","baef55ee":"eval_train_template = \"\"\"SELECT\n  *\nFROM\n  ML.TRAINING_INFO(MODEL `{model_name}`) \nORDER BY iteration \n\"\"\"\n\neval_model_template=\"\"\"\nSELECT\n  mean_squared_error,\n  r2_score,\n  explained_variance\nFROM ML.EVALUATE(MODEL `{model_name}`, (\n  SELECT\n    #RowId,\n    {label_name}_imask,\n    {label_name} as label,\n    {feature_stmt}\n  WHERE\n    #RowId > {cutoff_stmt}\n    {label_name}_mask = True\n    ))\n\"\"\"\n\nfeature_exam_template = \"\"\"\nSELECT\n  *\nFROM\n  ML.FEATURE_INFO(MODEL `{model_name}`)\n\"\"\"\npredict_with_correct_stmt=\"\"\"\nSELECT\n  RowId,\n  {label_name},\n  predicted_label\nFROM\n  ML.PREDICT(MODEL `{model_name}`,\n    (\n    SELECT\n        RowId,\n        {label_name},\n        {feature_stmt}\n    )\n  )\n  ORDER BY RowId ASC\n\"\"\"\npredict_template=\"\"\"\nSELECT\n  RowId,\n  predicted_label AS {label_name}\nFROM\n  ML.PREDICT(MODEL `{model_name}`,\n    (\n    SELECT\n        RowId,\n        {feature_stmt}\n    )\n  )\n  ORDER BY RowId ASC\n\"\"\"\n","bc8ae968":"experimental = True\nif experimental:\n    create_stmt = \"CREATE OR REPLACE MODEL\"\nelse: \n    create_stmt = \"CREATE MODEL IF NOT EXISTS\"\ncreate_model_template = \"\"\"\n{is_experimental} `{model_name}`\n    OPTIONS(MODEL_TYPE = 'LINEAR_REG',\n            OPTIMIZE_STRATEGY = 'BATCH_GRADIENT_DESCENT',\n            LEARN_RATE_STRATEGY = 'LINE_SEARCH',\n            L1_REG = @reg_value,\n            LS_INIT_LEARN_RATE = @init_lr,\n            MAX_ITERATIONS = 10, \n            EARLY_STOP = FALSE,\n            #MIN_REL_PROGRESS = 0.001,\n            DATA_SPLIT_METHOD = 'CUSTOM',\n            DATA_SPLIT_COL = @split_col\n            #DATA_SPLIT_METHOD = 'SEQ',\n            #DATA_SPLIT_COL = 'RowId',\n            #DATA_SPLIT_EVAL_FRACTION = 0.1 # 0.2 by default\n) AS\nSELECT\n    #RowId,\n    {label_name}_imask,\n    {label_name} as label,\n    {feature_stmt}\nWHERE\n    #RowId <= {cutoff_stmt}\n    {label_name}_mask = False\nORDER BY RowId ASC\n\"\"\"\nconfigs= {\n    \"bqml_example.model_TTS20\":[\n        bigquery.ScalarQueryParameter(\"init_lr\", \"FLOAT64\", 0.2),\n        bigquery.ScalarQueryParameter(\"reg_value\", \"FLOAT64\", 100),\n        bigquery.ScalarQueryParameter(\"split_col\", \"STRING\", \"TotalTimeStopped_p20_imask\")\n    ],\n    \"bqml_example.model_TTS50\":[\n        bigquery.ScalarQueryParameter(\"init_lr\", \"FLOAT64\", 0.2),\n        bigquery.ScalarQueryParameter(\"reg_value\", \"FLOAT64\", 1000),\n        bigquery.ScalarQueryParameter(\"split_col\", \"STRING\", \"TotalTimeStopped_p50_imask\")\n    ],\n    \"bqml_example.model_TTS80\":[\n        bigquery.ScalarQueryParameter(\"init_lr\", \"FLOAT64\", 0.2),\n        bigquery.ScalarQueryParameter(\"reg_value\", \"FLOAT64\", 1000),\n        bigquery.ScalarQueryParameter(\"split_col\", \"STRING\", \"TotalTimeStopped_p80_imask\")\n    ],\n    \"bqml_example.model_DF20\": [\n        bigquery.ScalarQueryParameter(\"init_lr\", \"FLOAT64\", 0.2),\n        bigquery.ScalarQueryParameter(\"reg_value\", \"FLOAT64\", 10000),\n        bigquery.ScalarQueryParameter(\"split_col\", \"STRING\", \"DistanceToFirstStop_p20_imask\")\n    ],\n    \"bqml_example.model_DF50\": [\n        bigquery.ScalarQueryParameter(\"init_lr\", \"FLOAT64\", 0.2),\n        bigquery.ScalarQueryParameter(\"reg_value\", \"FLOAT64\", 10000),\n        bigquery.ScalarQueryParameter(\"split_col\", \"STRING\", \"DistanceToFirstStop_p50_imask\")\n    ],\n    \"bqml_example.model_DF80\": [\n        bigquery.ScalarQueryParameter(\"init_lr\", \"FLOAT64\", 0.2),\n        bigquery.ScalarQueryParameter(\"reg_value\", \"FLOAT64\", 1000),\n        bigquery.ScalarQueryParameter(\"split_col\", \"STRING\", \"DistanceToFirstStop_p80_imask\")\n    ]\n}","258b729f":"import re\nfeature_set = ''\nmodel='bqml_example.model_DF80'\nidx2feature = []\nfor i, each_line in enumerate(\n        #no_proc_features.split('\\n')):\n        with_proc_features.split('\\n')):\n    each_line = each_line.strip()\n    if each_line.startswith('FROM'):\n        break\n    if len(each_line) > 0 and each_line.find('#') < 0:\n        job_config = bigquery.QueryJobConfig()\n        job_config.query_parameters = configs[model]\n        feature_set += each_line\n        assert(feature_set.rstrip(', ')[-1]!=',')\n        idx2feature.append(\n            feature_set.rstrip(', ') + \\\n            no_proc_stmt.format(table=proc_train))\n        model_name = 'bqml_example.model_DF80_%d' % len(idx2feature)\n        # creating model\n        _ = make_query(create_model_template, \n            job_config=job_config,\n            model_name=model_name,\n            label_name='DistanceToFirstStop_p80',\n            is_experimental=create_stmt,\n            feature_stmt=idx2feature[-1],\n            cutoff_stmt=split_rowid)\n        print(model_name, \"is complete\")","c9b84ac5":"print(idx2feature[-1])","20d06290":"del_models = False\nfor model in client.list_models('kaggle-bqml-256206.bqml_example'):\n    print(model.path)\n    if del_models :\n        client.delete_model(model)","95ed62af":"feature2loss = {}\nfor i, this_feature in enumerate(idx2feature): \n    # evaluating model\n    train_info = make_query(\n            eval_train_template,\n            model_name='bqml_example.model_DF80_%d' % (i + 1),\n            label_name='DistanceToFirstStop_p80')\n    eval_info = make_query(\n            eval_model_template,\n            model_name='bqml_example.model_DF80_%d' % (i + 1),\n            label_name='DistanceToFirstStop_p80',\n            feature_stmt=this_feature,\n            cutoff_stmt=split_rowid)\n    feature2loss[i] = {'eval': eval_info, \n                       'train':train_info.loc[train_info['iteration'].idxmax(),\n                                              ['loss', 'eval_loss']]}\n    print(i + 1, \"train_loss (train, eval)= \",\n              *train_info.loc[\n            train_info['iteration'].idxmax(),\n            ['loss', 'eval_loss']])","68b850e1":"last_feat_info = make_query(\n            feature_exam_template,\n            model_name='bqml_example.model_DF80_%d' % len(idx2feature))","4c36ab82":"last_feat_info","732d3c40":"%matplotlib inline\nimport matplotlib.pyplot as plt\neval_features = pd.concat([feature2loss[idx]['eval'] for idx in feature2loss], ignore_index=True)\nloss = pd.concat([feature2loss[idx]['train'] for idx in feature2loss], axis=1)\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharex=True)\naxes[0].plot(eval_features.index.values + 1, eval_features['r2_score'], label='r2_score', marker='x')\naxes[0].plot(eval_features.index.values + 1, eval_features['explained_variance'], label='explained_variance',\n            marker='o')\naxes[0].set_xticklabels(eval_features.index.values + 1)\naxes[0].set_xticks(eval_features.index.values + 1)\naxes[0].set_xlabel('feature set number')\naxes[0].set_ylabel('r2_score \/ explained_variance')\naxes[0].set_title('Different Feature Set vs Model capability')\naxes[0].legend();\ntrain_loss = loss.loc['loss'].reset_index()\ntrain_loss.columns = ['max_iteration', 'loss']\naxes[1].plot(train_loss.index.values + 1, train_loss['loss'].apply(np.sqrt), \n             label='train_RMSD', marker='x');\neval_loss = loss.loc['eval_loss'].reset_index()\neval_loss.columns = ['max_iteration', 'loss']\naxes[1].plot(eval_loss.index.values + 1, eval_loss['loss'].apply(np.sqrt), \n             label='eval', marker='o');\naxes[1].plot(eval_features.index.values + 1, \n             eval_features['mean_squared_error'].apply(np.sqrt), \n             label='test', marker='*');\naxes[1].set_xticklabels(eval_loss.index.values + 1)\naxes[1].set_xticks(eval_loss.index.values + 1)\naxes[1].set_xlabel('feature set number')\naxes[1].set_ylabel('RMSD')\naxes[1].set_title('Different Feature Set vs Model loss (RMSE)')\naxes[1].legend();","e7d05583":"_ = iterate_query(create_model_template, \n                  configs=configs,\n                  is_experimental=create_stmt,\n                  #feature_stmt=with_geo_features,\n                  #feature_stmt=no_geo_features,\n                  feature_stmt=idx2feature[-1],\n                  cutoff_stmt=split_rowid)","59b11b8f":"for label in submission_map.values():\n    percent = label[-2:]\n    if label.startswith('T'):\n        model_name = \"bqml_example.model_TTS\"\n    elif label.startswith('D'):\n        model_name = \"bqml_example.model_DF\"\n    model_name += percent\n    # examine models for learning rate \n    model = client.get_model(\"kaggle-bqml-256206.%s\" % model_name)\n    model_settings = model.training_runs[0].training_options\n    results = model.training_runs[0].results\n    print(model_name, model_settings.initial_learn_rate, \n          results[0].learn_rate)","dc8b812f":"feat_info = iterate_query(feature_exam_template)","d041b786":"#feat_info\nfeat_info.loc['DistanceToFirstStop_p80']","7af3f7b9":"for label in LABELS:\n    # path is the concatenation of 'EntryStreetName', 'EntryHeading'\n    # 'ExitStreetName', 'ExitHeading' \n    percent = label[-2:]\n    if label.startswith('T'):\n        model = \"bqml_example.model_TTS\"\n    elif label.startswith('D'):\n        model = \"bqml_example.model_DF\"\n    model += percent\n    print(\"{model_name} has the number of features = {}\".format(\n        int(feat_info.loc[label]['category_count'].fillna(1.0).sum()), \n        model_name=model)\n         )","e47ec5f4":"# reading weight\nweight_info_template=\"\"\"\nSELECT\n  *\nFROM\n  ML.WEIGHTS(MODEL `{model_name}`,\n    STRUCT(false AS standardize))\n\"\"\"\nresults = make_queries(weight_info_template)\nresults[-1]","8acd2872":"def handle_weight_result(weights_info, model_name):\n    # numerical one\n    numerical_weights = weights_info[~weights_info['weight'].isnull()]\n    numerical_weights = numerical_weights[\n        numerical_weights.columns[:-1]]\n    numerical_weights.loc[:, 'category'] = \\\n        numerical_weights.loc[:, 'processed_input']\n    numerical_weights.loc[:, 'processed_input'] = 'Numeric'\n    caterical_list = []\n    for _, frame in weights_info[\n            weights_info['weight'].isnull()].iterrows():\n        cate_frame = pd.DataFrame(frame['category_weights'])\n        cate_frame['processed_input'] = frame['processed_input']\n        caterical_list.append(cate_frame)\n    caterical_weights = pd.concat(caterical_list)\n    # total weight\n    ttl_weights = pd.concat([numerical_weights, caterical_weights], \n                       sort=False, ignore_index=True)\n    ttl_weights['model'] = model_name\n    return  ttl_weights\n\nmodel_weights = pd.concat([handle_weight_result(res, k) \n                           for k, res in results],\n                         sort=False, ignore_index=True)","7ea30b86":"# box-plot for each model\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nax = sns.boxenplot(x='model', y='weight', data=model_weights[['weight', 'model']])\nfig = plt.gcf()\nfig.autofmt_xdate();\nfig.set_size_inches(10, 6)\nax.set_title('Weight Distribution Across 6 Models');","480307ee":"fig, axes = plt.subplots(2, 3, figsize=(12, 10))\nfor i, (model_name, weights_per_model) in enumerate(model_weights.groupby('model')):\n    row = i \/\/ 3\n    col = i % 3\n    axes[row, col].set_title('histogram of weights \\n for predicting %s' % model_name)\n    plt.sca(axes[row, col])\n    for grp_name, members in  weights_per_model['weight'].groupby(\n        weights_per_model['processed_input']):\n        sns.distplot(members, bins=25, hist_kws={'alpha': 0.6}, label=grp_name,\n                     hist=True, kde=False, norm_hist=False) \n        axes[row, col].legend();\nplt.tight_layout()","42812eed":"train_info = iterate_query(eval_train_template)","2592cf64":"train_info\n#train_info.loc['DistanceToFirstStop_p80', ['loss', 'eval_loss']]","0b564147":"# plot the learning rate\nmarkers = ['x', 'o', '*', 'v', '+', 's']\nfor i, (model_name, sub_info) in enumerate(\n    train_info.groupby(train_info.index)):\n    plt.plot(sub_info['iteration'], sub_info['learning_rate'], \n             label=model_name, marker=markers[i], linestyle='dashed')\nplt.title('Learning rate change along iteration')\nplt.legend(bbox_to_anchor=(1.5, 1.0));","4910c371":"# plot the training curve\nfig, axes = plt.subplots(2, 3, figsize=(16, 12))\nfor i, (model_name, sub_info) in enumerate(\n        train_info.groupby(train_info.index)):\n    row = i \/\/ 3\n    col = i % 3\n    ax = axes[row, col]\n    ax.plot(sub_info['iteration'], np.sqrt(sub_info['loss']), label='train')\n    ax.plot(sub_info['iteration'], np.sqrt(sub_info['eval_loss']), label='evaluation')\n    ax.set_ylabel('RMSD')\n    ax.set_title('training curve for predicting\\n %s' % model_name)\n    ax.legend();","e7986b8a":"eval_info = iterate_query(eval_model_template,\n                          feature_stmt=idx2feature[-1],\n                          cutoff_stmt=split_rowid)","c1ce021f":"eval_info","daacbe3e":"loss_and_weights = pd.merge(eval_info['mean_squared_error'],\n         model_weights['weight'].abs().groupby(\n             model_weights['model']).sum(), \n         left_index=True, right_index=True)\nloss_and_weights['loss_to_WeightSum_ratio'] = \\\n    loss_and_weights['mean_squared_error'] \/ loss_and_weights['weight']\nloss_and_weights['percent_of_zero_weights'] = (model_weights['weight'] == 0).astype(\n    np.float).groupby(model_weights['model']).mean()\nloss_and_weights","0a269241":"# random split will give lower RMSD, a better but optimistic result\nrandom_eval = np.sqrt(train_info['eval_loss'].mean())\n# sequential split will give higher RMSD, a better but pessimistic result\nseq_eval = np.sqrt(eval_info['mean_squared_error'].mean())\nprint('validation result = {:.7}'.format(random_eval))\nprint('test set result = {:.7}'.format(seq_eval))\nprint('average result = {:.7}'.format((random_eval + seq_eval) \/ 2))\n","540fdfcb":"# making predictions for training set\nresults = make_queries(predict_with_correct_stmt,\n                       feature_stmt=idx2feature[-1]\n                      )","ca597e2d":"fig, axes = plt.subplots(2, 3, figsize=(12, 12))\nfor i, (label_name, pred_res) in enumerate(results):\n    row = i \/\/ 3\n    col = i % 3\n    axes[row, col].plot(pred_res[label_name], pred_res[label_name], \n                       ls=':', lw=2, color='r')\n    axes[row, col].set_title(label_name)\n    axes[row, col].scatter(pred_res[label_name], pred_res['predicted_label'])\n    axes[row, col].set_ylim(0, pred_res['predicted_label'].max() + 1)","4bbf9c26":"with_proc_test =  feature_stmt.format(\n    proc_stmt=proc_stmt.format(table=proc_test))\nresults = make_queries(predict_template,\n                       feature_stmt=with_proc_test\n                      )\n# execute this if you want a single query\n#results = make_query(predict_template,\n#    model_name='bqml_example.model_TTS80',\n#    label_name='TotalTimeStopped_p80')","177c40e6":"predictions = [vframe.copy(deep=True) for _, vframe in results]\nkeys = [k for k, _ in results]\nfor k, frame in zip(keys, predictions):\n    change_columns(frame, LABELS[k])\ndf = pd.concat(predictions)","23743998":"df.to_csv('bq_submission.csv', index=False)","e211c5a9":"# Feature Engineering ","73e3f4b3":"This kernel is based on [the starter](https:\/\/www.kaggle.com\/sirtorry\/bigquery-ml-template-intersection-congestion) and it's still under developed. Welcome for any comments and ideas. ","b97bd2ed":"A way to diagnose a model is to visualize weights. BigQuery ML provides a **ML.WEIGHTS** function for this purpose. One can follow [this link](https:\/\/cloud.google.com\/bigquery-ml\/docs\/reference\/standard-sql\/bigqueryml-syntax-weights#mlweights_function) to read the official reference. It shows examples of how to call **ML.WEIGHTS** and what will be returned. \n\nThe following SQL statement is a lazy person's query. It will return all the columns from a **ML.WEIGHTS** query. They are **'processed_input'**, **'weight'** and **'category_weights'**. If your feature is numeric, then **'category_weights'** will be an empty list. Based on the reference, a numeric feature will return NULL for **'category_weights'** column. If your feature is categorical and encoded as **one-hot-encoding**. **'weight'** column is NaN value but **'category_weights'** column will be a list of dict objects. \n\nEach entry of this list is a mapping of category name and weight. Returning a list is because often more than one category exists.  If you only want to examine one category, you can use the example given in the reference. That example will UNNEST an ARRAY structure based on the **processed_input** given in the WHERE clause. The ARRAY structure is what selecting category_weights will return. \n\nBut I'm more a pandas faithful user than a SQL expert, I decided to \"unnest\" myself through pandas. \n\nThe last thing is about `STRUCT(false AS standardize)`. It is a statement to ensure the weights returned to be standardized beforehand when setting true. ","0fca1244":"# Train-Validation Split\nUsing `RowId` to split the train \/ validatio will result in a biased set. As you can see in the following figure in which I plot the response value against `RowId`. You can see a simple `RowId` split will give oversampling result. In this result, the low values (close to zero) will be over-representative. For any learning algorithm such as \"early stopping\" depending on the performance of a validation set, picking a right validation set is very important.   \nA better way is to create validation set from the same distribution. You can use whatever sampling scheme you want. Once you have the validation set at hand, you can create a boolean column to hold the split result. When creating the BigQueryML model, using \"CUSTOM\" split and supply the ","50eab083":"# Feature Engineering and Selection","19829c6f":"Kaggle competition dataset (`kaggle-competition-datasets`) is a private dataset which doesn't allow public viewing. In order to acquire \"read\" access, one should follow the instruction file, `BigQuery-Dataset-Access.md`, listed in Data folder to join [the specific google group](https:\/\/groups.google.com\/d\/forum\/bigquery-geotab ). \n\nA minor notice is that you need to join the `bigquery-geotab` google group with the google acount which is also the BigQuery project owner. \nMy case is signing in Kaggle with one google account and using another for BigQuery cloud project. Without explicitly telling which account for use, it might automatically re-direct to the Kaggle account by default. Either opening a clean tab\/window, or explicitly logging into the BigQuery user, and then join google group for dataset access permission. ","724ab9a0":"Based on the BigQuery API references about [\"create model\" statement](https:\/\/cloud.google.com\/bigquery-ml\/docs\/reference\/standard-sql\/bigqueryml-syntax-create), the BigQuery ML model will handle minimal feature engineering as long as the inputs are recogniable types. \n\nThe supported input can be referenced from the table given within the same page under **Supported inputs** title. Once you get all input types supported, you can get the input transformation for free. You can check the corresponding transformations under **Input variable transformations** title. \n\nUnfortunately, sometimes the default option might not be the best one. So we are going to check on model features to ensure it's a good choice for our best understanding. "}}