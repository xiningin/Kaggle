{"cell_type":{"30f557bc":"code","240aa2a3":"code","c4fa2d86":"code","46f62019":"code","0b8573c7":"code","311f6ebb":"code","bd9a2327":"code","193371a3":"code","f2047f5f":"code","0c8347e1":"code","ff7dd2a4":"code","dc939681":"code","8e8daff5":"code","d39359e5":"code","bbd807cf":"code","c9666319":"code","5448b549":"code","2cc59912":"code","301c63d2":"code","375e1a33":"code","4ab4aa42":"markdown","5fa96e69":"markdown","f156dafd":"markdown"},"source":{"30f557bc":"# ! pip install torchsummary","240aa2a3":"import os\nimport time\nimport math\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\nimport random\nimport feather\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold,StratifiedKFold, GroupKFold\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils import data\nimport torch.nn.functional as F\n# from torchsummary import summary","c4fa2d86":"class Config:\n    competition = \"TPS_202111\"\n    seed = 42\n    n_folds = 10\n    batch_size = 1024\n    epochs = 125\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    es_patience = 20\n    lr_patience = 7\n    lr = 0.01","46f62019":"# this func follows pytorch lightning's seed_everythin --> https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/_modules\/pytorch_lightning\/utilities\/seed.html#seed_everything\ndef seed_everything(seed=Config.seed):\n    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)","0b8573c7":"data_dir = Path('..\/input\/tabular-playground-series-nov-2021')","311f6ebb":"%%time\n# train_df = pd.read_csv(data_dir \/ \"train.csv\", \n# #                        nrows=10000\n#                       )\n# test_df = pd.read_csv(data_dir \/ \"test.csv\",\n# #                      nrows=1000\n#                      )\n\n# Loading files in feather format\ntrain_df = feather.read_dataframe('..\/input\/tpsnov21\/train.feather')\ntest_df = feather.read_dataframe('..\/input\/tpsnov21\/test.feather')\nsample_submission = pd.read_csv(data_dir \/ \"sample_submission.csv\")\n\nprint(f\"train data: Rows={train_df.shape[0]}, Columns={train_df.shape[1]}\")\nprint(f\"test data : Rows={test_df.shape[0]}, Columns={test_df.shape[1]}\")","bd9a2327":"# features = [col for col in train_df.columns if col not in ('id', 'target')]\nfeatures = ['f1', 'f10', 'f11', 'f14', 'f15', 'f16', 'f17', 'f2', 'f20', 'f21', 'f22', 'f24', 'f25', 'f26', 'f27', 'f28', 'f3', 'f30', 'f31', 'f32', 'f33', 'f34', 'f36', 'f37', 'f4', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f5', 'f50', 'f51', 'f53', 'f54', 'f55', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f64', 'f66', 'f67', 'f70', 'f71', 'f76', 'f77', 'f8', 'f80', 'f81', 'f82', 'f83', 'f87', 'f89', 'f9', 'f90', 'f91', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98']","193371a3":"class TPSDataset(data.Dataset):\n    def __init__(self, X, y=None):\n        super(TPSDataset).__init__()\n        self.X = X\n        self.y = y\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        if self.y is not None:\n            return {\n                'X' : torch.tensor(self.X.values[idx], dtype=torch.float),\n                'y' : torch.tensor(self.y.values[idx], dtype=torch.float)\n            }\n        else:\n            return {\n                'X' : torch.tensor(self.X.values[idx], dtype=torch.float),\n            }\n","f2047f5f":"scaler = StandardScaler()\n\ntrain_df[features] = scaler.fit_transform(train_df[features])\ntest_df[features] = scaler.transform(test_df[features])","0c8347e1":"y_train = train_df.target\n\nX_test = test_df.drop(columns=[\"id\"], axis=1)\nX_train = train_df.drop(columns=[\"id\", \"target\"], axis=1)","ff7dd2a4":"# remove the unimportant features\nX_train = X_train[features]\nX_test = X_test[features]","dc939681":"train_dataset = TPSDataset(X_train, y_train)\ntest_dataset = TPSDataset(X_test)","8e8daff5":"test_loader = data.DataLoader(test_dataset, batch_size = 1024)","d39359e5":"def initialize_weights(model):\n    if isinstance(model, nn.Linear):\n#         nn.init.normal_(model.weight.data)\n#         nn.init.xavier_uniform_(model.weight.data)\n        nn.init.kaiming_uniform_(model.weight.data, nonlinearity=\"relu\")\n        nn.init.constant_(model.bias.data, 0)\n    elif isinstance(model, nn.Conv2d):\n        nn.init.kaiming_uniform_(model.weight.data, nonlinearity=\"relu\")\n        if model.bias is not None:\n            nn.init.constant_(model.bias.data, 0)","bbd807cf":"class Model(nn.Module):\n    def __init__(self, in_features, activation=F.relu):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(in_features, 128)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.fc2 = nn.Linear(128, 64)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.fc3 = nn.Linear(64, 32)\n        self.bn3 = nn.BatchNorm1d(32)\n        self.fc4 = nn.Linear(32, 1)\n        self.flatten = nn.Flatten()\n        self.activation = activation\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.bn1(self.activation(self.fc1(x)))\n        x = self.bn2(self.activation(self.fc2(x)))\n        x = self.bn3(self.activation(self.fc3(x)))\n        x = torch.sigmoid(self.fc4(x))\n        \n        return torch.squeeze(x, dim=1)        ","c9666319":"class EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n\n    def __init__(\n        self, patience=7, verbose=False, delta=0, path=\"checkpoint.pt\", trace_func=print\n    ):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            trace_func (function): trace print function.\n                            Default: print\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(\n                f\"EarlyStopping counter: {self.counter}\/{self.patience}\"\n            )\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.counter = 0\n            self.save_checkpoint(val_loss, model)\n\n    def save_checkpoint(self, val_loss, model):\n        \"\"\"Saves model when validation loss decrease.\"\"\"\n        checkpoint = {\"config\": Config, \"model_state_dict\": model.state_dict()}\n\n        if self.verbose:\n            self.trace_func(\n                f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\"\n            )\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","5448b549":"model = Model(in_features=len(features), activation=F.hardswish).to(Config.device)\nmodel.apply(initialize_weights)\n\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Number of parameters: {num_params}\")\n\n# summary(model, (len(features)), batch_size=-1, device=\"cpu\")","2cc59912":"%%time\nseed_everything()\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nhistories = []\n\nkf = StratifiedKFold(n_splits=Config.n_folds, random_state=Config.seed, shuffle=True)\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = X_train, y = y_train)):\n    print(10*\"=\", f\"Fold={fold+1}\/{Config.n_folds}\", 10*\"=\")\n    start_time = time.time()\n\n    train_subset = data.Subset(train_dataset, train_idx)\n    valid_subset = data.Subset(train_dataset, valid_idx)\n    train_loader = data.DataLoader(train_subset, batch_size = Config.batch_size, shuffle=True)\n    valid_loader = data.DataLoader(valid_subset, batch_size = Config.batch_size)\n        \n    model = Model(in_features=len(features), activation=F.hardswish).to(Config.device)\n    model.apply(initialize_weights)\n    criterion = nn.BCELoss()\n#     optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n    optimizer = optim.Adam(model.parameters(), Config.lr)\n    \n    # initialize the early_stopping object\n    early_stopping = EarlyStopping(\n        patience=Config.es_patience, verbose=True, path=f\".\/model_checkpoint_{fold}.pt\"\n    )   \n    \n    # initialize the learning rate scheduler\n#     lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n#         optimizer, patience=Config.lr_patience, verbose=True\n#     )\n    Q = math.floor(len(train_idx)\/Config.batch_size)\n    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=Q, verbose=True\n    )\n    \n    val_loss = []\n    for epoch in range(Config.epochs):\n        epoch_loss = 0.0\n        for idx, batch in enumerate(train_loader):\n            X, y = batch[\"X\"].to(Config.device), batch[\"y\"].to(Config.device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            pred = model(X)            \n            loss = criterion(pred, y)\n#             score = roc_auc_score(y, pred.detach()\n\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n\n        # print statistics\n        print('Epoch %d\/%d -> loss: %.4f' % (epoch + 1, Config.epochs, epoch_loss))\n                \n        model.eval()\n        running_val_loss = 0.0\n        for idx, batch in enumerate(valid_loader):\n            with torch.no_grad():\n                X, y = batch[\"X\"].to(Config.device), batch[\"y\"].to(Config.device)\n                val_pred = model(X)\n                loss = criterion(val_pred, y)\n                running_val_loss += loss.item()\n                \n                \n        # early_stopping needs the validation loss to check if it has decreased,\n        # and if it has, it will make a checkpoint of the current model\n        early_stopping(running_val_loss, model)\n\n        if early_stopping.early_stop:\n            print(\"Early stopping...\")\n            break\n            \n#         print(f\"running val loss: {running_val_loss}\")\n#         lr_scheduler.step(running_val_loss) # use with ReduceLROnPlateau scheduler\n        lr_scheduler.step() # use with CosineAnnealingLR scheduler\n\n    # load the last checkpoint with the best model\n    model.load_state_dict(torch.load(f\".\/model_checkpoint_{fold}.pt\"))\n    \n    # Predictions for OOF\n    print(\"--- Predicting OOF ---\")\n    valid_preds = []\n    scores = []\n    model.eval()\n    for idx, batch in enumerate(valid_loader):\n        with torch.no_grad():\n            X, y = batch[\"X\"].to(Config.device), batch[\"y\"].to(Config.device)\n            pred = model(X)\n            auc = roc_auc_score(y.cpu().numpy(), pred.detach().cpu().numpy())\n            valid_preds.extend(pred.detach().cpu().numpy()) \n            scores.append(auc)\n\n\n    final_valid_predictions.update(dict(zip(valid_idx, valid_preds)))\n    \n#     auc = roc_auc_score(y_valid,  valid_preds)\n#     scores.append(auc)\n    \n    # Predictions for Test Data\n    print(\"--- Predicting Test Data ---\")\n    test_preds = []\n    model.eval()\n    for idx, batch in enumerate(test_loader):\n        with torch.no_grad():\n            X = batch[\"X\"].to(Config.device)\n            pred = model(X)\n            test_preds.extend(pred.detach().cpu().numpy()) \n\n    final_test_predictions.append(test_preds)\n    \n    run_time = time.time() - start_time\n    print(f\"Fold={fold+1}, auc: {auc:.8f}, Run Time: {run_time:.2f}\")","301c63d2":"print(f\"Scores -> corrected: {np.mean(scores)-np.std(scores):.8f}, mean: {np.mean(scores):.8f}, std: {np.std(scores):.8f}\")","375e1a33":"sample_submission['target'] = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.to_csv(\"test_pred_2.csv\",index=None)\nsample_submission.to_csv(\"submission.csv\",index=None)\nsample_submission","4ab4aa42":"# Version\n\n- V7:\n    - load the last checkpoint with the best model\n    - Learning Rate Scheduler (CosineAnnealingLR)\n- V6:\n    - es_patience=20\n    - lr_patience=7\n    - epochs=125\n- V5:\n    - kaiming_uniform_ weights initializer\n    - One extra Dense layer\n- V4: \n    - Add Early Stopping: patience=5, \n    - Learning Rate Scheduler (ReduceLROnPlateau): patience=3\n    - Adam optimizer: learning_rate=0.01\n- V3: Swish activation\n- V2: \n    - Epochs=30\n    - n_folds=10\n- V1: \n    - SGD optimizer\n    - ReLU activation\n    - Epochs=5\n    - n_folds=5\n    - Model: 3 Linear layers (128-64-1)\n\n# References\n\n- https:\/\/www.kaggle.com\/mmellinger66\/tps-nov-21-keras-tuner\n- https:\/\/www.kaggle.com\/lucamassaron\/feature-selection-by-boruta-shap\n- https:\/\/www.kaggle.com\/hiro5299834\/tps-nov-2021-pytorch-lightning\n- https:\/\/www.kaggle.com\/yasufuminakama\/ventilator-pressure-lstm-starter","5fa96e69":"## Training with Cross Validation","f156dafd":"## PyTorch Model"}}