{"cell_type":{"db9c8c16":"code","b3177c99":"code","024f0d8e":"code","2f424f03":"code","2750e32c":"code","ae97cbea":"code","601f865b":"code","c5a394ed":"code","4ceebd6a":"code","826b5561":"code","04fa5690":"code","ada2c529":"code","abd49470":"code","0649785a":"code","d60cffae":"code","4fb3f524":"code","80abda16":"code","6f8cb31e":"code","9fc857da":"code","3774e4f9":"code","bdf173ad":"code","6ecf03fd":"code","71401244":"code","717cc77d":"code","103bfcd6":"code","dc60dafa":"code","0f0f3ef1":"code","73a95912":"markdown","28f72fd9":"markdown","b004580d":"markdown","b1ec6c0f":"markdown","6fd4e174":"markdown","80d6238c":"markdown","6a3cdbbf":"markdown","f7a0c293":"markdown","3c035810":"markdown","0b580e00":"markdown","4e4a2e0a":"markdown"},"source":{"db9c8c16":"#imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn import *\nimport seaborn as sns\nimport scipy\nfrom sklearn.metrics import f1_score\nimport statistics\n\n#This is because the data is split into 500000 rows\nBATCH_LENGTH = 500000","b3177c99":"train = pd.read_csv(\"..\/input\/liverpool-ion-switching\/train.csv\")\ntest = pd.read_csv(\"..\/input\/liverpool-ion-switching\/test.csv\")\n\ntrain.head()","024f0d8e":"#Define the function to graph the signals for a set of data :)\ndef graphSignalChannelData(data, includeSignal=True):\n    for i in range(int(len(data) \/ BATCH_LENGTH)):\n        start = BATCH_LENGTH * i\n        end = BATCH_LENGTH + (i * BATCH_LENGTH) - 1\n        print(f\"{i} : {start}, {end}\")\n        y_signal = data[\"signal\"][start:end]\n        x = range(start,end)\n        plt.figure(figsize=(30,5))\n        plt.plot(x, y_signal, color='g', label=\"Signal\")\n        if includeSignal:\n            y_channels = data[\"open_channels\"][start:end]\n            plt.plot(x, y_channels, color='b', label=\"Channels\")\n        plt.legend()\n        plt.show()","2f424f03":"#Now graph our training data :)\ngraphSignalChannelData(train)","2750e32c":"#Lets look at some distributions\ndef distGraphData(data):\n    for i in range(10):\n        start = BATCH_LENGTH * i\n        end = BATCH_LENGTH + (i * BATCH_LENGTH)\n        print(f\"{i} : {start}, {end}\")\n        signals = data[\"signal\"][start:end]\n        plt.figure(figsize=(30,5))\n        sns.distplot(signals)\n        plt.show()","ae97cbea":"distGraphData(train)","601f865b":"#This is going to be our un-drifted training data\ntrain2 = train.copy()","c5a394ed":"#We have linear drift from 500000 --> 600000 in batch 1 (500000:1000000)\nplt.figure(figsize=(30,5))\nplt.plot(range(500000,1000000), train['signal'][500000:1000000])\nplt.title(\"1: No Change\")\nplt.show()\n\na=500000; b=600000 # CLEAN TRAIN BATCH 2\ntrain2.loc[train.index[a:b],'signal'] = train2.signal[a:b].values - 3*(train2.time.values[a:b] - 50)\/10.\nplt.figure(figsize=(30,5))\nplt.plot(range(500000,1000000), train2['signal'][500000:1000000])\nplt.title(\"1: Undrifted\")\nplt.show()","4ceebd6a":"def f(x,low,high,mid): return -((-low+high)\/625)*(x-mid)**2+high -low\n\n# CLEAN TRAIN BATCH 6\nbatch = 7; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,-1.817,3.186,325)\n# CLEAN TRAIN BATCH 7\nbatch = 8; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,-0.094,4.936,375)\n# CLEAN TRAIN BATCH 8\nbatch = 9; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,1.715,6.689,425)\n# CLEAN TRAIN BATCH 9\nbatch = 10; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,3.361,8.45,475)\n\nplt.figure(figsize=(30,5))\nplt.plot(range(0, 5000), train['signal'][::1000], color='r')\nplt.plot(range(0, 5000), train2['signal'][::1000], color='g')\nplt.show()","826b5561":"patterns = {\n    \"v_low\":[0,1],\n    \"low\":[2,6],\n    \"med\":[3,7],\n    \"high\":[5,8],\n    \"v_high\":[4,9]\n}\n\nmodels = {}","04fa5690":"RANGE = 200\nOFFSET = 1500000\nWINDOW = 3\nWINDOW_OFFSET = 1\n\nsignals = []\nfor index, row in train2.iloc[OFFSET:].iterrows():\n    if row['open_channels'] != 0:\n#         print(row['signal'])\n        indices = range(index - RANGE, index + RANGE)\n        signal = []\n        signal_2 = []\n        channels = []\n        for i in indices:\n            row_index = train2.iloc[i]\n            signal.append(row_index.signal)\n            signal_2.append(row_index.signal ** 10)\n            channels.append(row_index.open_channels)\n        \n        data_df = pd.DataFrame(data=signal,columns=[\"signal\"])\n        \n#         f = scipy.signal.hilbert(signal)\n        \n        plt.figure(figsize=(30,10))\n        \n        plt.plot(indices, signal, color='r')\n        plt.plot(indices, channels, color='g')\n        plt.plot(indices, signal_2, color='y')\n#         plt.plot(indices, f, color='b')\n        \n        plt.show()\n        break","ada2c529":"train2.head()","abd49470":"batches = []\nfor i in range(10):\n    start = BATCH_LENGTH * i\n    end = BATCH_LENGTH + (i * BATCH_LENGTH)\n    for j in range(start,end):\n        batches.append(i)\ntrain2['batch'] = batches","0649785a":"train2.groupby('batch')[['signal','open_channels']].agg(['min', 'max', 'median'])","d60cffae":"#Some constants\nWINDOW = 3\nMIN_MAX_WINDOW = 2\nWINDOW_OFFSET = 1\nSHIFT_WINDOW = 2","4fb3f524":"def repeatedFeatures(data, data_df, exponent):\n    \n    change = pd.Series(data).diff().to_numpy()\n    change[0] = 0\n    data_df[f\"change_exp_{exponent}\"] = change\n    \n    pct_change = pd.Series(data).pct_change().to_numpy()\n    pct_change[0] = 0\n    for i in range(len(pct_change)):\n        if np.isinf(pct_change[i]):\n            pct_change[i] = change[i]\n    data_df[f\"pct_change_exp_{exponent}\"] = pct_change\n    \n    rolling_min = data_df.rolling(MIN_MAX_WINDOW).min()[f\"signal_exp_{exponent}\"].to_numpy()\n    rolling_min[:(WINDOW-1)] = [min(data_df[f\"signal_exp_{exponent}\"])] * (MIN_MAX_WINDOW - 1)\n    data_df[f\"rolling_min_exp_{exponent}\"] = rolling_min\n    \n    rolling_max = data_df.rolling(MIN_MAX_WINDOW).max()[f\"signal_exp_{exponent}\"].to_numpy()\n    rolling_max[:(WINDOW-1)] = [max(data_df[f\"signal_exp_{exponent}\"])] * (MIN_MAX_WINDOW - 1)\n    data_df[f\"rolling_max_exp_{exponent}\"] = rolling_max\n    \n    #Shift the signal by 1 -> WINDOW and add the features\n    for i in range(1, SHIFT_WINDOW + 1):\n        shift = data_df.signal.shift(i).fillna(0).to_numpy()\n        data_df[f\"shift_{i}_exp_{exponent}\"] = shift\n\n        neg_shift = data_df.signal.shift(-i).fillna(0).to_numpy()\n        data_df[f\"neg_shift_{i}_exp_{exponent}\"] = neg_shift\n        \n    return data_df","80abda16":"def generateFeatures(data):\n    data_df = pd.DataFrame(data=data,columns=[\"signal\"])\n    \n    rolling_mean = data_df.rolling(WINDOW).mean().signal.to_numpy()\n    rolling_mean = np.concatenate(([statistics.mean(signal[:(WINDOW-WINDOW_OFFSET)])] * (WINDOW-WINDOW_OFFSET), rolling_mean[(WINDOW):], [statistics.mean(signal[(WINDOW_OFFSET):])] * (WINDOW_OFFSET)))\n    data_df[\"rolling_mean\"] = rolling_mean\n    \n    rolling_median = data_df.rolling(WINDOW).median().signal.to_numpy()\n    rolling_median = np.concatenate(([statistics.median(signal[:(WINDOW-WINDOW_OFFSET)])] * (WINDOW-WINDOW_OFFSET), rolling_median[(WINDOW):], [statistics.median(signal[(WINDOW_OFFSET):])] * (WINDOW_OFFSET)))\n    data_df[\"rolling_median\"] = rolling_median\n    \n    rolling_min = data_df.rolling(MIN_MAX_WINDOW).min().signal.to_numpy()\n    rolling_min[:(WINDOW-1)] = [min(data)] * (MIN_MAX_WINDOW - 1)\n    data_df[\"rolling_min\"] = rolling_min\n    \n    rolling_max = data_df.rolling(MIN_MAX_WINDOW).max().signal.to_numpy()\n    rolling_max[:(WINDOW-1)] = [max(data)] * (MIN_MAX_WINDOW - 1)\n    data_df[\"rolling_max\"] = rolling_max\n    \n    #This seems to lower the score :(\n#     std = data_df.signal.std()\n#     mean = data_df.signal.mean()\n#     normalized_signal = ((data_df.signal - mean) \/ std).to_numpy()\n#     data_df[\"normalized_signal\"] = normalized_signal\n    \n    exponents = [2]\n    for exp in exponents:\n        signal_exp = []\n        for datum in data:\n            signal_exp.append(datum ** exp)\n        data_df[f\"signal_exp_{exp}\"] = signal_exp\n        data_df = repeatedFeatures(signal_exp, data_df, exp)\n    \n    return data_df","6f8cb31e":"for i in patterns.keys():\n    print(f\"STARTING {i} CLASSIFIER\")\n\n    batches = patterns[i]\n    batch0 = train2.loc[train2[\"batch\"] == batches[0]]\n    batch1 = train2.loc[train2[\"batch\"] == batches[1]]\n    batch_df = pd.concat([batch0, batch1])\n    \n    num_channels = len(batch_df.open_channels.unique())\n    \n    channels = batch_df[\"open_channels\"]\n\n#     clf = tree.DecisionTreeClassifier()\n#     clf = ensemble.RandomForestClassifier()\n    clf = ensemble.GradientBoostingClassifier(verbose=1, n_estimators=150, learning_rate=0.2)\n#     clf = svm.SVC()\n#     clf = neural_network.MLPClassifier()\n    \n    print(f\"GENERATING FEATURES...\")\n    data_points = generateFeatures(batch_df[\"signal\"])\n    print(f\"FITTING MODEL...\")\n    clf.fit(data_points, channels)\n    print(f\"DONE FITTING!\")\n    models[i] = clf\n    print(f\"Score: {clf.score(data_points, channels)}\")\n    print(\"\\n\")\n    print(\"Feature ranking:\")\n    feature_importances = clf.feature_importances_\n    scores = []\n    for score, col in zip(feature_importances, data_points.columns):\n        if score != 0.0:\n            scores.append([col, score])\n    scores.sort(reverse=True,key=lambda x: x[1])\n    for score in scores:\n        print(f\"{score[0]} : {score[1]}\")\n    \n    print(\"\\n\\n\")","9fc857da":"for i in patterns.keys():\n    print(f\"Checking classifier results for {i}\")\n    \n    batches = patterns[i]\n    batch0 = train2.loc[train2[\"batch\"] == batches[0]]\n    batch1 = train2.loc[train2[\"batch\"] == batches[1]]\n    batch_df = pd.concat([batch0, batch1])\n    \n    signal = batch_df[\"signal\"].to_numpy()\n    open_channels = batch_df[\"open_channels\"].to_numpy()\n    \n    clf = models[i]\n    data_points = generateFeatures(signal)\n    predictions = clf.predict(data_points)\n    \n    SIGNAL_RANGE = 20\n    for prediction, actual, i in zip(predictions, open_channels, range(len(predictions))):\n        if prediction != actual:\n            signal_i = signal[i-SIGNAL_RANGE:i+SIGNAL_RANGE]\n            prediction_i = predictions[i-SIGNAL_RANGE:i+SIGNAL_RANGE]\n            actual_i = open_channels[i-SIGNAL_RANGE:i+SIGNAL_RANGE]\n            \n            plt.figure(figsize=(20,10))\n\n            indices = range(i-SIGNAL_RANGE, i+SIGNAL_RANGE)\n            plt.plot(indices, signal_i, color='r')\n            plt.plot(indices, prediction_i, color='g')\n            plt.plot(indices, actual_i, color='b')\n            \n            plt.show()\n            break","3774e4f9":"graphSignalChannelData(test, False)","bdf173ad":"test2 = test.copy()","6ecf03fd":"# REMOVE BATCH 0 DRIFT\nstart=500\na = 0; b = 100000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.\nstart=510\na = 100000; b = 200000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.\nstart=540\na = 400000; b = 500000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.","71401244":"# REMOVE BATCH 1 DRIFT\nstart=560\na = 600000; b = 700000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.\nstart=570\na = 700000; b = 800000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.\nstart=580\na = 800000; b = 900000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.","717cc77d":"# REMOVE BATCH 2 DRIFT\ndef f(x):\n    return -(0.00788)*(x-625)**2+2.345 +2.58\na = 1000000; b = 1500000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - f(test2.time[a:b].values)","103bfcd6":"res = 1000; let = ['A','B','C','D','E','F','G','H','I','J']\n\nplt.figure(figsize=(30,5))\nplt.plot(range(0, 2000000)[::1000], test['signal'][::1000], color='r')\nplt.plot(range(0, 2000000)[::1000], test2['signal'][::1000], color='g')\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'r:')\nfor k in range(4): plt.text(k*500000+250000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7.5,let[k],size=16)\nplt.show()","dc60dafa":"predictions = []\n\n# SUBSAMPLE A, Model v_low\ndata = generateFeatures(test2[\"signal\"].values[0:100000])\npredictions = np.concatenate((predictions, models[\"v_low\"].predict(data)))\n\n# SUBSAMPLE B, Model med\ndata = generateFeatures(test2[\"signal\"].values[100000:200000])\npredictions = np.concatenate((predictions, models[\"med\"].predict(data)))\n\n# SUBSAMPLE C, Model high\ndata = generateFeatures(test2[\"signal\"].values[200000:300000])\npredictions = np.concatenate((predictions, models[\"high\"].predict(data)))\n\n# SUBSAMPLE D, Model v_low\ndata = generateFeatures(test2[\"signal\"].values[300000:400000])\npredictions = np.concatenate((predictions, models[\"v_low\"].predict(data)))\n\n# SUBSAMPLE E, Model med\ndata = generateFeatures(test2[\"signal\"].values[400000:500000])\npredictions = np.concatenate((predictions, models[\"med\"].predict(data)))\n\n# SUBSAMPLE F, Model v_high\ndata = generateFeatures(test2[\"signal\"].values[500000:600000])\npredictions = np.concatenate((predictions, models[\"v_high\"].predict(data)))\n\n# SUBSAMPLE G, Model high\ndata = generateFeatures(test2[\"signal\"].values[600000:700000])\npredictions = np.concatenate((predictions, models[\"high\"].predict(data)))\n\n# SUBSAMPLE H, Model v_high\ndata = generateFeatures(test2[\"signal\"].values[700000:800000])\npredictions = np.concatenate((predictions, models[\"v_high\"].predict(data)))\n\n# SUBSAMPLE I, Model v_low\ndata = generateFeatures(test2[\"signal\"].values[800000:900000])\npredictions = np.concatenate((predictions, models[\"v_low\"].predict(data)))\n\n# SUBSAMPLE J, Model med\ndata = generateFeatures(test2[\"signal\"].values[900000:1000000])\npredictions = np.concatenate((predictions, models[\"med\"].predict(data)))\n\n# BATCHES 3 AND 4, Model v_low\ndata = generateFeatures(test2[\"signal\"].values[1000000:2000000])\npredictions = np.concatenate((predictions, models[\"v_low\"].predict(data)))\n\nplt.figure(figsize=(30,5))\nplt.plot(range(len(predictions))[::100], predictions[::100])\nplt.show()","0f0f3ef1":"sub = test.copy()\nsub.pop('signal')\nsub[\"open_channels\"] = [int(x) for x in predictions]\nsub.to_csv('submission.csv',index=False, float_format='%.4f')","73a95912":"# Now let's get rid of that drift\n\n### Based on this notebook: https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930#Remove-Training-Data-Drift","28f72fd9":"# We've gotta train some models now!","b004580d":"## Some notes on drift:\n#### For each of our batches it has the following drift \n\n* 0: No real drift\n* 1: Slant drift up to approx 600,000\n* 2: No real drift\n* 3: No real drift\n* 4: No real drift\n* 5: No real drift\n* 6: Lots of parabolic drift\n* 7: Lots of parabolic drift\n* 8: Lots of paraboilc drift\n* 9: Lots of parabolic drift","b1ec6c0f":"## Parabolic drift removal","6fd4e174":"# Visualization of training!!!","80d6238c":"## Our patterns are:\n\n* 0 & 1\n* 2 & 6\n* 3 & 7\n* 4 & 9\n* 5 & 8","6a3cdbbf":"# Lets take a look at some of our testing data","f7a0c293":"## Let's remove that drift!!!","3c035810":"# Ok, let's see some examples when the model predicts the wrong thing to maybe find some hints...","0b580e00":"# Going to look at some specific examples of signal change","4e4a2e0a":"### Note that there are patterns!!!"}}