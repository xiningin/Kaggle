{"cell_type":{"1f192f60":"code","bc67de97":"code","321e4721":"code","b264af95":"code","90562d60":"code","93ae0b48":"code","6bc739f8":"code","0ab33042":"code","eab1c3b1":"code","3f7bf3de":"code","b74c9a5b":"code","680a86ea":"code","04b8e673":"code","47d7f38b":"code","060bc4b0":"code","8bf5129e":"code","86dddefc":"code","91455e9e":"code","e4fdac7a":"code","ba435f6c":"code","8c5a8bc8":"code","5377b627":"code","349396e0":"code","3a7958ea":"code","9ff1c805":"code","02219512":"code","5d571f4e":"code","4ea70456":"code","d47508b7":"code","440b9206":"code","d7604e6a":"markdown","34681d24":"markdown","c3f6e2f7":"markdown","1684fa7e":"markdown","21ccadbc":"markdown","1f65eda6":"markdown","0646b1b1":"markdown","b36a53d2":"markdown","ec37dcc1":"markdown","0d134a1d":"markdown","63ecde23":"markdown","35ebd7d3":"markdown","df64d1ba":"markdown"},"source":{"1f192f60":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bc67de97":"from pathlib import Path\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","321e4721":"train = Path(\"\/kaggle\/input\/google-quest-challenge\/train.csv\")\ntest =  Path(\"\/kaggle\/input\/google-quest-challenge\/test.csv\")\nsample_sub = Path(\"\/kaggle\/input\/google-quest-challenge\/sample_submission.csv\")","b264af95":"train_df = pd.read_csv(train)\ntest_df = pd.read_csv(test)","90562d60":"train_df.shape, test_df.shape","93ae0b48":"train_targets = train_df.iloc[:, 11:]\ntrain_feats = train_df.iloc[:, 1:11]\n\ntest_targets = test_df.iloc[:, 11:]\ntest_feats = test_df.iloc[:, 1:11]","6bc739f8":"train_targets.columns, train_feats.columns","0ab33042":"question_target_cols = [col for col in train_targets.columns if col.split('_')[0] == 'question']\nanswer_target_cols = [col for col in train_targets.columns if col.split('_')[0] == 'answer']\n","eab1c3b1":"question_feat_cols = [col for col in train_feats.columns if col.split('_')[0] == 'question']\nanswer_feat_cols = [col for col in train_feats.columns if col.split('_')[0] == 'answer']","3f7bf3de":"train_df.head()","b74c9a5b":"train_df.columns","680a86ea":"print('Question:')\ntrain_df.iloc[0].question_title, train_df.iloc[0].question_body","04b8e673":"print('Answer:')\ntrain_df.iloc[0].answer","47d7f38b":"def trunc_text(text, n=102):\n    tokens = text.split()\n    return ' '.join(tokens[: n]) if len(tokens) > n else text","060bc4b0":"train_question_text = train_feats.question_title + ' ' + train_feats.question_body\ntest_question_text = test_feats.question_title + ' ' + test_feats.question_body\n\ntrain_answer_text = train_feats.answer\ntest_answer_text = test_feats.answer\n\nlens = train_question_text.apply(lambda x: len(x.split()))\nprint(lens.describe())\n\ntrain_question_text = train_question_text.apply(lambda x: trunc_text(x))\ntest_question_text = test_question_text.apply(lambda x: trunc_text(x))","8bf5129e":"train_targets['none_q'] = 1-train_targets[question_target_cols].max(axis=1)\ntrain_targets.describe()","86dddefc":"train_question_text.fillna(\"unknown\", inplace=True)\ntest_question_text.fillna(\"unknown\", inplace=True)","91455e9e":"import re, string\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","e4fdac7a":"n = train_df.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train_question_text)\ntest_term_doc = vec.transform(test_question_text)","ba435f6c":"vec_answers = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_answers_term_doc = vec_answers.fit_transform(train_answer_text)\ntest_answers_term_doc = vec_answers.transform(test_answer_text)","8c5a8bc8":"trn_term_doc, test_term_doc","5377b627":"train_cat_dummies = pd.get_dummies(train_feats['category']).values\ntest_cat_dummies = pd.get_dummies(test_feats['category']).values","349396e0":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) \/ ((y==y_i).sum()+1)","3a7958ea":"def get_mdl(y):\n    # Binarizing\n    y = y.gt(0.5).astype(int)\n    y = y.values\n    r = np.log(pr(1,y) \/ pr(0,y))\n    #m = LinearRegression()\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    x_nb_cat = np.concatenate([x_nb.toarray(), train_cat_dummies], axis=1)\n    return m.fit(x_nb_cat, y), r","9ff1c805":"x = trn_term_doc\ntest_x = test_term_doc\n\nquestion_preds = np.zeros((test_df.shape[0], len(question_target_cols)))\n\nfor i, j in enumerate(question_target_cols):\n    print('fitting', j)\n    m,r = get_mdl(train_targets[j])\n    test_x_nb_cat = np.concatenate([test_x.multiply(r).toarray(), test_cat_dummies], axis=1)\n    #question_preds[:,i] = np.clip(m.predict(), 0, 1)\n    print('predicting ...')\n    question_preds[:,i] = m.predict_proba(test_x_nb_cat)[:,1]\n    \nquestion_preds_df = pd.DataFrame(question_preds, columns=question_target_cols)","02219512":"x = trn_answers_term_doc\ntest_x = test_answers_term_doc\n\nanswer_preds = np.zeros((test_df.shape[0], len(answer_target_cols)))\n\nfor i, j in enumerate(answer_target_cols):\n    print('fit', j)\n    m,r = get_mdl(train_targets[j])\n    test_x_nb_cat = np.concatenate([test_x.multiply(r).toarray(), test_cat_dummies], axis=1)\n    #answer_preds[:,i] = np.clip(m.predict(test_x.multiply(r)), 0, 1)\n    print('predicting ...')\n    answer_preds[:,i] = m.predict_proba(test_x_nb_cat)[:,1]\n    \nanswer_preds_df = pd.DataFrame(answer_preds, columns=answer_target_cols)","5d571f4e":"preds_df = pd.concat([question_preds_df, answer_preds_df], axis=1)\npreds_df['qa_id'] = test_df.qa_id","4ea70456":"sub_df = pd.read_csv(sample_sub)\nsub_df_columns = sub_df.columns.values.tolist()\nsub_df = preds_df[sub_df_columns]","d47508b7":"sub_df","440b9206":"sub_df.to_csv(\"submission.csv\", index = False)","d7604e6a":"Naive Bayes feature equation:","34681d24":"Let's start with a simple implementation where the question features are purely the question title and body (truncated to median) and answer features are from the answer","c3f6e2f7":"We'll create a list of all the labels to predict, and we'll also create a 'none' label so we can see how many comments have no labels. We can then summarize the dataset.","1684fa7e":"Filling NaNs","21ccadbc":"## Create category dummy features","1f65eda6":"## Sample question and answer","0646b1b1":"## Introduction\n\nThis kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) on this dataset by training separate models for the question and answer targets, and then combining the predictions at submission.\n\nNBSVM was introduced by Sida Wang and Chris Manning in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic Classi\ufb01cation](https:\/\/nlp.stanford.edu\/pubs\/sidaw12_simple_sentiment.pdf)\n\nSpecial thanks to Jeremy Howard for this basis kernel from a previous [competition](https:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline). ","b36a53d2":"## Distinguish between features and targets about questions and answers","ec37dcc1":"## Train and predict w\/ answer model","0d134a1d":"## Submit predictions","63ecde23":"## Building the model\nWe'll start by creating a bag of words representation, as a term document matrix. We'll use ngrams, as suggested in the NBSVM paper.","35ebd7d3":"## Train and predict w\/ question model","df64d1ba":"# Training the questions model"}}