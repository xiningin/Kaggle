{"cell_type":{"735da53c":"code","a2d5c94d":"code","77da6960":"code","72ba4bb7":"code","cad68937":"code","5fcf43d2":"markdown","57d35d14":"markdown","8e9ab35b":"markdown","2c789531":"markdown","1c63a1b7":"markdown","9c514c30":"markdown"},"source":{"735da53c":"#MCDCGAN\n# By Ayush Agarwal , Electronics Engineering , IIT BHU Varanasi \n\n#The main code\n##############################################################################################\n# base image size would be 8x8 , as I don't have that much time and computational resources \n#importing the libraries\nimport numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom functools import partial\nimport matplotlib.pyplot as plt\n########################################################################\n#making the image \n#img1 = (np.eye(8)*255)\nimg1 = np.ones(shape=(8,8))\nimg2 = np.ones(shape=(8,8))\n# for i in range(0,8):\n#     for j in range(0,8,2):\n#         img1[i,j]=255\n# for j in range(0,8):\n#     for i in range(0,8,2):\n#         img2[i,j]=255\nimg1[:][2:4]=255\nimg2[:][4:6]=255\n#img1[:][3]=255\n#img2[4:8][:]=255\n#img2[5][:]=255\n# img1[1:4][1:4]=255\n# img2[5:8][5:8]=255\n######################################\n# printing the image\nprint()\nprint(img1)\nprint()\nprint(img2)\n#################################################\nimg1 = tf.reshape(img1, (-1, 8, 8, 1))\nimg2 = tf.reshape(img2, (-1, 8, 8, 1))\n# seed1 = [[1,0],[0,1]]\n# seed1 = tf.constant([[1,0,0]])\n# seed2 = tf.constant([[0,1], [1,0]])\n# the outputs of the discriminator I named seed \nseed1=tf.constant([[1,0,0]])\nseed2=tf.constant([[0,1,0]])\nimgs = tf.convert_to_tensor(np.array([img1,img2]))\nimgs = tf.reshape(imgs, (-1, 8, 8, 1))\nseeds = tf.constant([[1,0,0],[0,1,0]])\n#############################################################################################\n# The Discriminator model \nDiscriminator = keras.Sequential([\n    \n    layers.Conv2D(filters=25, kernel_size=2,# activation=\"relu\", #padding='same',\n                  input_shape=[8, 8, 1]),\n    layers.Conv2D(filters=50, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Conv2D(filters=75, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Conv2D(filters=50, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Conv2D(filters=25, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Conv2D(filters=1, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Flatten(),\n    layers.Dense(3, activation='softmax'),\n])\n# was facing the dying relu problem so used a leakyrelu here \nDiscriminator.summary()\nDiscriminator.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# training the discriminator \nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.01,\n    restore_best_weights=True,\n)\n\nhistory = Discriminator.fit(\n    imgs,seeds,\n    validation_data=(imgs,seeds),\n    epochs=25,\n    callbacks=[early_stopping]\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[1:, ['loss', 'val_loss']].plot()\nhistory_df.loc[1:, ['accuracy', 'val_accuracy']].plot()\nplt.show()\n\nprediction1 = Discriminator.predict(img1)\nprediction2 = Discriminator.predict(img2)\nprint(\"Prediction on class 1 by discriminator :\")\nprint(prediction1)\nprint()\nprint(\"Prediction on class 2 by discriminator :\")\nprint(prediction2)\n#####################################################################################\n# Making the Generator \n\nGenerator = keras.Sequential([\n    layers.Dense(4, use_bias=False, input_shape=(4,)),\n    layers.Reshape((2, 2,1)),\n  #  layers.Conv2DTranspose(25, (2, 2), strides=(1, 1), use_bias=False),# padding='same',\n    layers.Conv2DTranspose(500, (3, 3), strides=(1, 1), use_bias=False, activation=partial(tf.nn.leaky_relu, alpha=0.01)),# padding='same',\n    layers.AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'),\n    layers.Conv2DTranspose(1000, (1, 1), strides=(1, 1), use_bias=False, activation=partial(tf.nn.leaky_relu, alpha=0.01)),# padding='same',\n   # layers.Dropout(0.3),\n    layers.Conv2DTranspose(500, (3, 3), strides=(1, 1), use_bias=False, activation=partial(tf.nn.leaky_relu, alpha=0.01)),# padding='same',\n   # layers.Conv2DTranspose(200, (2, 2), strides=(1, 1), use_bias=False, activation=partial(tf.nn.leaky_relu, alpha=0.01)),# padding='same',\n    layers.AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'),\n   # layers.Dropout(0.3),\n    layers.Conv2DTranspose(500, (2, 2), strides=(1, 1), use_bias=False, activation=partial(tf.nn.leaky_relu, alpha=0.01)),# padding='same',\n    layers.Conv2DTranspose(1, (2, 2), strides=(1, 1), use_bias=False),# padding='same',\n    \n])\nGenerator.summary()\n# brush1=tf.constant([[1,0,0,0]])\n# brush2=tf.constant([[0,0,0,1]])\nbrushes = tf.constant([[255,1,255,1],[1,255,1,255]])\nprint(\"Generator output when it wasnt trained :\")\nuntrained_prediction1 = Generator.predict(brush1)\nplt.imshow(untrained_prediction1[0][:][:][:], cmap='gray')\n#print(prediction1)\n################################################################\n#Making the GAN \nDiscriminator.trainable = False\n#GAN = tf.keras.layers.Concatenate()([Generator, Discriminator])\n#GAN = tf.keras.Model(inputs=Generator.outputs, outputs=Discriminator.outputs)\nGAN = keras.Sequential([\n    Generator ,\n    Discriminator\n])\n    \nGAN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n########################################################################\n# Training the GAN \nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.01,\n    restore_best_weights=True,\n)\n\nhistory2 = GAN.fit(\n    brushes,seeds,\n    validation_data=(brushes,seeds),\n    epochs=100,\n    callbacks=[early_stopping]\n)\n\nhistory_df2 = pd.DataFrame(history2.history)\nhistory_df2.loc[1:, ['loss', 'val_loss']].plot()\nhistory_df2.loc[1:, ['accuracy', 'val_accuracy']].plot()\nplt.show()      \n\n##############################################################\nprint()\nprint(\"Painting using class 1 ( brush 1 )\")\ndrawing1 = Generator.predict(brush1)\nplt.imshow(drawing1[0][:][:][:], cmap='gray')\nplt.show()\nprint(\"Painting using class 2 ( brush 2 )\")\ndrawing2 = Generator.predict(brush2)\nplt.imshow(drawing2[0][:][:][:], cmap='gray')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n","a2d5c94d":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom functools import partial\nimport matplotlib.pyplot as plt\n\nGenerator = keras.Sequential([\n    layers.Dense(4, use_bias=False, input_shape=(4,)),\n    layers.Reshape((2, 2,1)),\n    layers.Conv2DTranspose(25, (2, 2), strides=(1, 1), use_bias=False),# padding='same',\n    layers.Conv2DTranspose(50, (2, 2), strides=(1, 1), use_bias=False),# padding='same',\n    layers.Conv2DTranspose(75, (2, 2), strides=(1, 1), use_bias=False),# padding='same',\n    layers.Conv2DTranspose(50, (2, 2), strides=(1, 1), use_bias=False),# padding='same',\n    layers.Conv2DTranspose(50, (2, 2), strides=(1, 1), use_bias=False),# padding='same',\n    layers.Conv2DTranspose(1, (2, 2), strides=(1, 1), use_bias=False),# padding='same',\n    \n])\nGenerator.summary()\nbrush1=tf.constant([[1,0,0,0]])\nuntrained_prediction1 = Generator.predict(brush1)\n\nplt.imshow(untrained_prediction1[0][:][:][:], cmap='gray')\n#print(prediction1)","77da6960":"# perfectly working code for a discriminator between horizontal and vertical image lines \n\n# base image size would be 8x8 , as I don't have that much time and computational resources \n\nimport numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom functools import partial\nimport matplotlib.pyplot as plt\nimport pickle\nimport dill\n#from sklearn.externals import joblib\n\nimg1 = np.zeros(shape=(8,8))\nimg2 = np.zeros(shape=(8,8))\nfor i in range(0,8):\n    for j in range(0,8,2):\n        img1[i,j]=255\nfor j in range(0,8):\n    for i in range(0,8,2):\n        img2[i,j]=255\nprint()\nprint(img1)\nprint()\nprint(img2)\nplt.imshow(img1, cmap='gray')\nplt.show()\nplt.imshow(img2, cmap='gray')\nplt.show()\nimg1 = tf.reshape(img1, (-1, 8, 8, 1))\nimg2 = tf.reshape(img2, (-1, 8, 8, 1))\n# seed1 = [[1,0],[0,1]]\n# seed1 = tf.constant([[1,0,0]])\n# seed2 = tf.constant([[0,1], [1,0]])\nseed1=tf.constant([[1,0,0]])\nseed2=tf.constant([[0,1,0]])\nimgs = tf.convert_to_tensor(np.array([img1,img2]))\nimgs = tf.reshape(imgs, (-1, 8, 8, 1))\n#imgs = tf.expand_dims(imgs, 1) \n#seeds=tf.convert_to_tensor(([seed1,seed2]))\nseeds = tf.constant([[1,0,0],[0,1,0]])\n\nDiscriminator = keras.Sequential([\n    \n    layers.Conv2D(filters=25, kernel_size=2,# activation=\"relu\", #padding='same',\n                  input_shape=[8, 8, 1]),\n    layers.Conv2D(filters=25, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Conv2D(filters=25, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Conv2D(filters=25, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Conv2D(filters=25, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Conv2D(filters=1, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Flatten(),\n    layers.Dense(3, activation='softmax'),\n])\n# was facing the dying relu problem so used a leakyrelu here \nDiscriminator.summary()\n\n#optimizer = tf.keras.optimizers.Adam(epsilon=0.01)\n# Discriminator.compile(\n#     #optimizer=optimizer,\n#     optimizer='adam',\n#     #loss='mae',\n#     loss='sparse_categorical_crossentropy',\n#     metrics=['accuracy'],\n# )\n\nDiscriminator.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.01,\n    restore_best_weights=True,\n)\n\nhistory = Discriminator.fit(\n    imgs,seeds,\n    validation_data=(imgs,seeds),\n    epochs=25,\n    callbacks=[early_stopping]\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[1:, ['loss', 'val_loss']].plot()\nhistory_df.loc[1:, ['accuracy', 'val_accuracy']].plot()\n\nprediction1 = Discriminator.predict(img1)\nprediction2 = Discriminator.predict(img2)\nprint(prediction1)\nprint()\nprint(prediction2)\n#tf.saved_model.save(Discriminator, mobilenet_save_path)\ntf.saved_model.save(Discriminator,'.\/')\n#Discriminator.save('Discriminator1.h5')\n#joblib.dump(Discriminator, 'Discriminator2.pkl')\n# dill.dump(Discriminator, open('Discriminator1.pkl', 'wb'))\n#pickle.dump(Discriminator, open('Discriminator2.pkl','wb'))\n\n\n\n\n\n\n\n\n\n\n\n\n","72ba4bb7":"pip install joblib","cad68937":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom functools import partial\nimport matplotlib.pyplot as plt\nimport pickle\nimport dill\n\nfrom keras.models import load_model\n\nimport keras\n\nDiscriminator = keras.Sequential([\n    \n    layers.Conv2D(filters=25, kernel_size=2,# activation=\"relu\", #padding='same',\n                  input_shape=[8, 8, 1]),\n    layers.Conv2D(filters=25, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Conv2D(filters=25, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Conv2D(filters=25, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Conv2D(filters=25, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Conv2D(filters=1, kernel_size=2, activation=partial(tf.nn.leaky_relu, alpha=0.01)#\"relu\"#, padding='same'\n                 ),\n    layers.Flatten(),\n    layers.Dense(3, activation='softmax'),\n])\n#Discriminator= tf.saved_model.load('..\/input\/discriminator4\/saved_model (1).pb')\n#loaded = tf.saved_model.load(mobilenet_save_path)\n#new_model.load_weights('CIFAR1006.h5')\n#Discriminator.load_weights('.\/Discriminator1.pkl')\n#model1 = load_model('.\/Discriminator1.h5')\nimg1 = np.zeros(shape=(8,8))\nimg2 = np.zeros(shape=(8,8))\nfor i in range(0,8):\n    for j in range(0,8,2):\n        img1[i,j]=255\nfor j in range(0,8):\n    for i in range(0,8,2):\n        img2[i,j]=255\n        \ny_pred1 = Discriminator.predict(img1)\ny_pred2 = Discriminator.predict(img2)\nprint(y_pred1)\nprint(y_pred2)","5fcf43d2":"The main code","57d35d14":"MCDCGAN (Multi Class Deep Convulution Generative Adversarial Networks )\n\nby Ayush Agarwal , Electronics Engineering , IIT BHU Varanasi \n\nA techniques invented by me , to make the GAN draw 2 different classes of drawings as per the input key given to it .","8e9ab35b":"generator code work here","2c789531":"Rest of the file is my rough","1c63a1b7":"Perfectly working code for discriminator \n\ndo \n\nnot \n\nchange","9c514c30":"As we can see , the white part is a bit up in the first image , whereas it is a bit below in the second image . These 2 different patterns it has absorbed during training from the 2 different images it was trained on . And more importantly , I could make these 2 drawings not randomly but by my own choice by using different inputs to the generator (which I have called brushes) . Since I have used only 1 picture of each class , it has absorbed very little . But I feel that when lots of images would be fed into the GAN and differently labelled , (say for eg one picture is in warm colours and other in cool colours ) , then I can select different modes\/keys\/(brushes as per my project ) and make different types of images from the GAN as per my own choice . This is all I wanted to show in my project . The results weren't personally satisfactory to me since I was expecting the GAN to give out perfect pictures , but alas , the dataset isn't big enough . Noteworthy it is that the Discriminator is quite well trained yet the Generator isn't.\n"}}