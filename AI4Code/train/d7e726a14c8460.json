{"cell_type":{"92086e42":"code","1d551065":"code","32a36423":"code","d142299b":"code","3f74eef4":"code","ef69efe1":"code","a4fb9556":"code","5617c945":"code","ca18a145":"code","954359ed":"code","1b2a450d":"code","cb730862":"code","54c15066":"code","0f1cf932":"code","4adde036":"code","8d77adce":"code","c30e5ad7":"code","ba1fbae4":"code","5ffa1b8a":"code","8918d910":"code","3c014587":"code","95764ca3":"code","982ba8cd":"code","8e05f58e":"code","4144a77a":"code","5d191e2d":"code","95422f84":"code","4a3916b0":"markdown","326da39f":"markdown","87c36c9b":"markdown","649b01f7":"markdown","20dd55e6":"markdown","016205c5":"markdown","8dd71651":"markdown","6e6a6f3d":"markdown","b571b6b7":"markdown","f7dd0e4e":"markdown","e242fbee":"markdown","0b97275c":"markdown","8fa1d45f":"markdown","7692dd52":"markdown","77e78acb":"markdown","517e5a1b":"markdown","70272d3c":"markdown","5bdb0f78":"markdown","e9237c3b":"markdown","1ad4f6df":"markdown","7b0f41b2":"markdown","a5a4c9a0":"markdown","eb4fcf0a":"markdown","e79ccdfe":"markdown","c4bb586c":"markdown","c5f948be":"markdown","8c892929":"markdown","5ada063a":"markdown","9e3506c9":"markdown","c09e3302":"markdown","5acfbe1f":"markdown","3a9b84fb":"markdown"},"source":{"92086e42":"# Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# set seed for reproducibility\nnp.random.seed(0)\n\n#Data\ntumor_data = pd.read_csv('..\/input\/data.csv')\ntumor_data.sample(5)","1d551065":"print(\"The dataset has %d rows and %d columns\" % (tumor_data.shape[0], tumor_data.shape[1]))","32a36423":"tumor_data.describe(include='all')","d142299b":"tumor_data.dtypes","3f74eef4":"tumor_data['Unnamed: 32'].sample(8)","ef69efe1":"missing_values = tumor_data['Unnamed: 32'].isnull().sum()\nnumber_of_rows = tumor_data['Unnamed: 32'].shape[0]\nif missing_values == number_of_rows:\n    print('The whole \\'Unnamed: 32\\' column has empty values.')\nelse:\n    print('There are non-empty values in the \\'Unnamed: 32\\' column.')","a4fb9556":"tumor_data.drop(['Unnamed: 32'], axis= 1, inplace = True)\ntumor_data.columns","5617c945":"tumor_data.isna().sum()","ca18a145":"tumor_data['diagnosis'].value_counts()","954359ed":"tumor_data['target'] = tumor_data['diagnosis'].replace({'B': 1, 'M': 0})\n# Let's show if the convertion was rightly done\ntumor_data[['id', 'diagnosis', 'target']].sample(5)","1b2a450d":"tumor_data.drop(['diagnosis'], axis = 1, inplace = True)\ntumor_data.columns","cb730862":"tumor_data.drop(['id'], axis = 1, inplace=True)\ntumor_data.columns","54c15066":"sns.pairplot(\n    tumor_data,\n    vars = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean'],\n    hue = 'target'\n)","0f1cf932":"# First, we make sure that the graphic is crearly visible\nplt.figure(figsize = (30, 20))\n# And now, draw the heatmap\nsns.heatmap(tumor_data.corr(), cmap = \"RdBu_r\")","4adde036":"# Independent variables\nX = tumor_data.drop(['target'], axis = 1)\n# Dependent variable\nY = tumor_data['target']","8d77adce":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)","c30e5ad7":"from sklearn.svm import SVC\nsvc_model = SVC()\nsvc_model.fit(X_train, Y_train)","ba1fbae4":"Y_predicted = svc_model.predict(X_test)\nY_predicted","5ffa1b8a":"cm = confusion_matrix(Y_test, Y_predicted)\nsns.heatmap(cm, annot = True, cmap=\"Blues\")","8918d910":"print(classification_report(Y_test, Y_predicted))","3c014587":"tumor_data.describe()","95764ca3":"min_train = X_train.min()\nrange_train = (X_train - min_train).max()\nX_train_scaled = (X_train - min_train)\/range_train\nX_train_scaled.describe()","982ba8cd":"fig = plt.figure(figsize = (20, 5))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\nax1.set_title('Values without normalization')\nax2.set_title('Values with normalization')\nsns.scatterplot(x = X_train['texture_mean'], y = X_train['area_mean'], hue = Y_train, ax = ax1)\nsns.scatterplot(x = X_train_scaled['texture_mean'], y = X_train_scaled['area_mean'], hue = Y_train, ax = ax2)","8e05f58e":"# Train the model again\nsvc_model.fit(X_train_scaled, Y_train)\n# Create scaled test data\nX_test_scaled = (X_test - X_test.min())\/(X_test - X_test.min()).max()\n# Calculate new predictions\nY_predicted = svc_model.predict(X_test_scaled)\n# Draw confusion matrix\ncm = confusion_matrix(Y_test, Y_predicted)\nsns.heatmap(cm, annot = True, cmap='Blues')","4144a77a":"print(classification_report(Y_test, Y_predicted))","5d191e2d":"# We can automate the refinement of C and gamma using the GridSearchCV library\nfrom sklearn.model_selection import GridSearchCV\nC_values =  [0.1, 1, 10, 100, 1000]\ngamma_values = [1, 0.1, 0.01, 0.001]\ngrid = GridSearchCV(SVC(), {'C': C_values, 'gamma': gamma_values, 'kernel': ['rbf']}, refit = True, verbose = 4)\n# Find best pair of C and gamma values\ngrid.fit(X_train_scaled, Y_train)\ngrid.best_params_","95422f84":"# We can use the optimized grid object directly to get predictions\ngrid_predicted = grid.predict(X_test_scaled)\n\ncm = confusion_matrix(Y_test, grid_predicted)\nsns.heatmap(cm, annot = True, cmap = 'Blues')\nprint(classification_report(Y_test, grid_predicted))","4a3916b0":"### Train\/test data\nNow we can split our data into test and train datasets.","326da39f":"After normalizing our features, we have all our values in the same scale (between zero and one). One thing to notice here is that, while the value ranges have changed, we shouldn't worry about altering our dataset because the relation between them is still the same, so the model will have the same behavior.","87c36c9b":"## Data Cleaning\nWe're seeing that there's one column (*Unnamed: 32*) with strange stats. Let see some of their values.","649b01f7":"As promised by the dataset description, we don't have missing values in any column.","20dd55e6":"Now, there's another curious thing. The column *diagnosis* doesn't seem to have numerical values.","016205c5":"From this heatmap we can see many things.\n\nFor example, take the *perimeter*, *radius* and *area* features and you'll see that the intersection between each combination of them has a dark red cell, meaning a strong correlation between them. This sounds kind of obvious, given that geometry states that the area is directly proportional to both perimeter and radius. Other red cells can be found in the intersection of *concavity*-*compactness*, *compactness*-*smoothness*, and even *fractal_dimension*-*smoothness*.\n\nOn the other hand, we can see that the *target* feature doesn't have red cells, which means it isn't strongly correlated to any other feature.","8dd71651":"## Data exploration\nLet's take a glance at our dataset","6e6a6f3d":"## Goal\nTry to predict if a breast tumor is bening or malignant, using its features.\n\n### Features\n* **id** ID number\n* **diagnosis** The diagnosis of breast tissues (M = malignant, B = benign)\n* **radius_mean** mean of distances from center to points on the perimeter\n* **texture_mean** standard deviation of gray-scale values\n* **perimeter_mean** mean size of the core tumor\n* **area_mean**\n* **smoothness_mean** mean of local variation in radius lengths\n* **compactness_mean** mean of perimeter^2 \/ area - 1.0\n* **concavity_mean** mean of severity of concave portions of the contour\n* **concave points_mean** mean for number of concave portions of the contour\n* **symmetry_mean**\n* **fractal_dimension_mean** mean for \"coastline approximation\" - 1\n* **radius_se** standard error for the mean of distances from center to points on the perimeter\n* **texture_se** standard error for standard deviation of gray-scale values\n* **perimeter_se**\n* **area_se**\n* **smoothness_se** standard error for local variation in radius lengths\n* **compactness_se** standard error for perimeter^2 \/ area - 1.0\n* **concavity_se** standard error for severity of concave portions of the contour\n* **concave points_se** standard error for number of concave portions of the contour\n* **symmetry_se**\n* **fractal_dimension_se** standard error for \"coastline approximation\" - 1\n* **radius_worst** \"worst\" or largest mean value for mean of distances from center to points on the perimeter\n* **texture_worst** \"worst\" or largest mean value for standard deviation of gray-scale values\n* **perimeter_worst**\n* **area_worst**\n* **smoothness_worst** \"worst\" or largest mean value for local variation in radius lengths\n* **compactness_worst** \"worst\" or largest mean value for perimeter^2 \/ area - 1.0\n* **concavity_worst** \"worst\" or largest mean value for severity of concave portions of the contour\n* **concave points_worst** \"worst\" or largest mean value for number of concave portions of the contour\n* **symmetry_worst**\n* **fractal_dimension_worst** \"worst\" or largest mean value for \"coastline approximation\" - 1","b571b6b7":"One of the things to notice here is that our features are not in the same scale: some ranges are in thousands, while others are between 0 and 1. In order to normalize our features, we are going to scale them using unity based normalization.","f7dd0e4e":"## What's next?\nSome ideas to implement after all this:\n* Train more models using different algorithms.\n* Apply k-fold cross-validation\n* Create more graphs to explain better what's happening in each step","e242fbee":"# Breast Cancer Classification","0b97275c":"## Model training\nNow that we've checked that our data is ready for computation, we can move on to train our model.\n### Independent and dependent variables\nLet's start by defining our main variables. We'll use *X* to store all features that will help us to classify a tumor, in other words, all columns from our dataframe except *target*, which is going to be the variable *Y*, our dependent variable.","8fa1d45f":"### SVM training\nIn this case, we're going to classify tumors using SVM.","7692dd52":"It's confirmed, the *diagnosis* columns has only 'B' and 'M' values. According to the dataset description, 'B' stands for 'benign' while 'M' stands for 'malignant'. Since those values are not computable for our model training, we need to convert them to numbers. We're going to create a **target** column with numerical values, where '1' will be used to indicate that the tumor is benign, and '0' if it's malignant.","77e78acb":"## Data visualization\nLet's see some graphics to give us an idea of the distributions within the dataset.","517e5a1b":"Now that we removed an empty column, why don't we check this dataset doesn't have missing values?","70272d3c":"Working with normalized features should cause an improvement in our model training. So, let's run the training again using the normalized data and test it with a normalized test dataset.","5bdb0f78":"Ok, so our classifier failed at classifying malignant tumors (row 0). Let's see precision, recall and f1-score for this model.","e9237c3b":"## Import libraries and data","1ad4f6df":"## Model evaluation\nNow that we have trained our model, let's see if it's a good classifier.","7b0f41b2":"This model look better than the previous one. From a f1-score of 0.5, we have now 0.95, approximately. Having said that, let's take into consideration those five cases were the target was actualy 0 but our model said they are 1. That means that those tumors were malignant but we classified them as bening. Given that we could be talking about human lives, those five people would be in serious risk and we wouldn't notice! We need to see if there's something else we can do to improve even more our model.\n### SVM parameter refinement\nAnother improvement we can make on our model is refining the parameters of our training algorithm. In the case of SVM, we have **C** and **gamma** parameters. Both of them can help us improve our model's accuracy and generalization.","a5a4c9a0":"We've succesfully converted the categorical values from *diagnosis* to number in *target*. Now we can remove *diagnosis* from the dataframe because we don't need it anymore for our classifier.","eb4fcf0a":"Good, we have slightly improve our metrics. And while the little value changes may sound insignificant, we can see that those 5 malignant tumors that we were not able to detect turned into 2, which could be very important for those people.\nWe can also see that now we have 1 tumor that is bening but was classified as malignant. That's not good (after all it's a wrong classification) but it's not as bad as the other classification error because in this case the person with the tumor might get more analysis and then they would realize that it's bening.","e79ccdfe":"Let's use a confusion matrix to compare the predicted values against our test dataset.","c4bb586c":"Now that we know that this column doesn't add any value, we can safely remove it.","c5f948be":"Those stats are not cool, notice that precision, recall and f1-score for malignant tumor are zero, wich means that failed completely. Actually, the trained model classified all as bening, so we need to review the training process to improve the model.\n\n*For more information about which metrics should be considered, take a look at this great article https:\/\/towardsdatascience.com\/beyond-accuracy-precision-and-recall-3da06bea9f6c *","8c892929":"## Model improvement\n### Normalization\nFirst, let's review our dataset's metrics:","5ada063a":"Apparently, this colum is completely empty. Let's make sure of it.","9e3506c9":"Finally, have you noticed that there is an **id** column? Should it stay or should it be removed? If we read to the dataset documentation, we'll see that this column stores the *ID number*. What does that mean?  Honestly, we don't know. It could mean the order in which the samples were taken; it could mean instead the order in which the records were added to the dataset; or it could mean anything else, even nothing! \n\nSince we don't know the meaning of the values in this column, we can't understand the difference between a record with id 842517 and another with id 84348301, right? Is it bigger, is it shorter? So, in order to proceed with the data analysis and training of our classifier, we're going to proceed without this arbitrary **id** column.","c09e3302":"Apparently, the best value for C and gamma is 1 for both of them. Let's validate it by running a new training and get a new prediction. ","5acfbe1f":"## Conclusions\nAfter all this analysis, we can conclude that:\n* SVM can run smoothly in a 569x32 dataset\n* The dataset is not quite balanced. In other words, it has more bening tumor samples than malignant tumor samples.\n* An SVM model can benefit from normalizing features and parameter refinement.\n* Using this dataset, we can obtain a trained model able to classify benign and malignant tumors with 97% of accuracy.","3a9b84fb":"We can see that, for each pair of parameters (at least, for the ones we chose to display using the *var* parameter), we could draw a line to separate the clases. In other words, if we pick two features, it seems to be possible separating between 'bening' and 'malignant' classes.\n\nNow, while these graphics give us some idea about the relations between each pair of features, we could more easily (in my opinion) see the correlation between all of them in a heatmap, so let's draw one."}}