{"cell_type":{"69b9f719":"code","aeca4d46":"code","8a9b4535":"code","53273e4c":"code","67c3c9e6":"code","5bd08d3d":"code","4e822edc":"code","7e1946d6":"code","ec979500":"code","e59b80d5":"code","0afd5460":"code","c0a59bb9":"code","96c706cd":"code","384bb9df":"code","bf2d0b60":"code","32d6204d":"code","9ea12641":"code","72b8f171":"code","61e35f08":"code","d8ddf148":"code","e6087237":"code","b083e819":"code","b392338f":"code","579b4540":"code","835212e3":"code","7e8de05b":"code","a6d28d6d":"code","06340eb2":"code","27d1fdfa":"code","c12a493f":"markdown","26420e3b":"markdown","a84f8566":"markdown","577952d2":"markdown","66fadb59":"markdown","63ca46a7":"markdown","dfff1e6e":"markdown","c67f4882":"markdown","6dab8a86":"markdown","00540583":"markdown","d881a6a0":"markdown","bfc0374c":"markdown","9cd37cca":"markdown","8081ee9b":"markdown","4d617ee0":"markdown","4ae1b9b8":"markdown","67b7c42e":"markdown","dcec47bc":"markdown","a4f965be":"markdown"},"source":{"69b9f719":"# Imports And Libraries\nimport time\nimport warnings\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport gc\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom functools import partial, wraps\nfrom datetime import datetime as dt\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","aeca4d46":"import itertools\n\ndef get_random_pairs(numbers):\n    pairs = list(itertools.combinations(numbers, 2))\n    return list(pairs)\ndef autobox(cols):\n    length = len(list(cols)) + 1\n    if length == 1:\n        m, n = 2, 1\n    else:\n        m = int(length**0.5)\n        n = int(length \/ m)\n    return n, m + 1\ndef crosstab_all(df,\n                 cat_cols,\n                 annot=True,\n                 cmap='viridis',\n                 figsize=(20, 15),\n                 subplot_grid=None,\n                 title='Paired Chi Square Matrices',\n                 **kwargs):\n    plt.figure(figsize=figsize)\n    plt.suptitle(title, x=0.5, y=0.9, size=20)\n    i = 0\n    pairs = get_random_pairs(cat_cols)\n    m, n = autobox(cat_cols)\n    factor = len(pairs)\n    m = int(factor \/ 3) + 2\n    if subplot_grid:\n        m = subplot_grid[0]\n        n = subplot_grid[1]\n    for pair in pairs:\n        ax = plt.subplot(m, n, i + 1)\n        ax.set_xticklabels(\n            ax.get_xticklabels(), rotation=60, ha=\"right\", fontsize=13)\n        x = df[pair[0]]\n        y = df[pair[1]]\n        sns.heatmap(\n            pd.crosstab(x, y).apply(lambda r: r \/ r.sum(), axis=1),\n            annot=annot,\n            cmap=cmap,\n            **kwargs)\n        i += 1","8a9b4535":"!ls ..\/input","53273e4c":"# Credits @Theol\ndtypes = {'MachineIdentifier': 'category', 'ProductName': 'category', 'EngineVersion': 'category', 'AppVersion': 'category', 'AvSigVersion': 'category', 'IsBeta': 'int8', 'RtpStateBitfield': 'float16', 'IsSxsPassiveMode': 'int8', 'DefaultBrowsersIdentifier': 'float16', 'AVProductStatesIdentifier': 'float32', 'AVProductsInstalled': 'float16', 'AVProductsEnabled': 'float16', 'HasTpm': 'int8', 'CountryIdentifier': 'int16', 'CityIdentifier': 'float32', 'OrganizationIdentifier': 'float16', 'GeoNameIdentifier': 'float16', 'LocaleEnglishNameIdentifier': 'int8', 'Platform': 'category', 'Processor': 'category', 'OsVer': 'category', 'OsBuild': 'int16', 'OsSuite': 'int16', 'OsPlatformSubRelease': 'category', 'OsBuildLab': 'category', 'SkuEdition': 'category', 'IsProtected': 'float16', 'AutoSampleOptIn': 'int8', 'PuaMode': 'category', 'SMode': 'float16', 'IeVerIdentifier': 'float16', 'SmartScreen': 'category', 'Firewall': 'float16', 'UacLuaenable': 'float32', 'Census_MDC2FormFactor': 'category', 'Census_DeviceFamily': 'category', 'Census_OEMNameIdentifier': 'float16', 'Census_OEMModelIdentifier': 'float32', 'Census_ProcessorCoreCount': 'float16', 'Census_ProcessorManufacturerIdentifier': 'float16', 'Census_ProcessorModelIdentifier': 'float16', 'Census_ProcessorClass': 'category', 'Census_PrimaryDiskTotalCapacity': 'float32', 'Census_PrimaryDiskTypeName': 'category', 'Census_SystemVolumeTotalCapacity': 'float32', 'Census_HasOpticalDiskDrive': 'int8', 'Census_TotalPhysicalRAM': 'float32', 'Census_ChassisTypeName': 'category', 'Census_InternalPrimaryDiagonalDisplaySizeInInches': 'float16', 'Census_InternalPrimaryDisplayResolutionHorizontal': 'float16', 'Census_InternalPrimaryDisplayResolutionVertical': 'float16', 'Census_PowerPlatformRoleName': 'category', 'Census_InternalBatteryType': 'category', 'Census_InternalBatteryNumberOfCharges': 'float32', 'Census_OSVersion': 'category', 'Census_OSArchitecture': 'category', 'Census_OSBranch': 'category', 'Census_OSBuildNumber': 'int16', 'Census_OSBuildRevision': 'int32', 'Census_OSEdition': 'category', 'Census_OSSkuName': 'category', 'Census_OSInstallTypeName': 'category', 'Census_OSInstallLanguageIdentifier': 'float16', 'Census_OSUILocaleIdentifier': 'int16', 'Census_OSWUAutoUpdateOptionsName': 'category', 'Census_IsPortableOperatingSystem': 'int8', 'Census_GenuineStateName': 'category', 'Census_ActivationChannel': 'category', 'Census_IsFlightingInternal': 'float16', 'Census_IsFlightsDisabled': 'float16', 'Census_FlightRing': 'category', 'Census_ThresholdOptIn': 'float16', 'Census_FirmwareManufacturerIdentifier': 'float16', 'Census_FirmwareVersionIdentifier': 'float32', 'Census_IsSecureBootEnabled': 'int8', 'Census_IsWIMBootEnabled': 'float16', 'Census_IsVirtualDevice': 'float16', 'Census_IsTouchEnabled': 'int8', 'Census_IsPenCapable': 'int8', 'Census_IsAlwaysOnAlwaysConnectedCapable': 'float16', 'Wdft_IsGamer': 'float16', 'Wdft_RegionIdentifier': 'float16', 'HasDetections': 'int8'}\n","67c3c9e6":"# Imports \ntrain_df = pd.read_csv(\"..\/input\/train.csv\", dtype= dtypes, nrows=50000)\ntest_df = pd.read_csv(\"..\/input\/test.csv\", dtype = dtypes, nrows=50000)\nprint(\"Train Shape : \", train_df.shape)\nprint(\"Test Shape : \", test_df.shape)","5bd08d3d":"train_df.head()","4e822edc":"def mr_inspect(df):\n    \"\"\"Returns a inspection dataframe\"\"\"\n    inspect_dataframe = pd.DataFrame({'DataType': df.dtypes, 'Unique values': df.nunique() ,\n                  'Number of missing values': df.isnull().sum() ,\n                  'Percentage missing': (df.isnull().sum() \/ len(df)) * 100,\n                                      'Memory Usage (MB)':round(df.memory_usage(index=False) \/ 1024, 2)\n                                     }).sort_values(by='Number of missing values', ascending = False)\n    inspect_dataframe['Variance'] = df[inspect_dataframe.index].var()\n    inspect_dataframe['Mean'] = df[inspect_dataframe.index].mean()\n    inspect_dataframe['Min'] = df[inspect_dataframe.index].min()\n    inspect_dataframe['Max'] = df[inspect_dataframe.index].max()\n    return inspect_dataframe","7e1946d6":"ins_train = mr_inspect(train_df)\nins_train.transpose()","ec979500":"mr_inspect(test_df).transpose()","e59b80d5":"target = train_df['HasDetections']\ncnt_srs = train_df['HasDetections'].value_counts()\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=cnt_srs.values,\n        colorscale = 'Picnic',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Target Count',\n    font=dict(size=18))\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"TargetCount\")","0afd5460":"labels = (np.array(cnt_srs.index))\nsizes = (np.array((cnt_srs \/ cnt_srs.sum())*100))\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Target distribution',\n    font=dict(size=18),\n    width=600,\n    height=600)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,)","c0a59bb9":"categorical_cols = list(ins_train[ins_train['DataType'] == 'category'].index)\nnumerical_cols = list(ins_train[ins_train['DataType'] != 'category'].index)\nbinary_cols = []\nfor col in (numerical_cols):\n    if train_df[col].nunique() == 2:\n        binary_cols.append(col)\n        numerical_cols.remove(col)\n\nfor col in (categorical_cols):\n    if train_df[col].nunique() == 2:\n        binary_cols.append(col)\n        categorical_cols.remove(col)\n        \nprint(\">> Categorical Columns: {}: \\n {} \\n\\n\".format(len(categorical_cols), categorical_cols))\nprint(\">> Numerical Columns: {}: \\n{}\\n\\n\".format(len(numerical_cols), numerical_cols))\nprint(\">> Binary Columns: {}:\\n{}\".format(len(binary_cols), binary_cols))","96c706cd":"params = {'objective': 'binary', 'boosting_type': 'gbdt', 'learning_rate': 0.02, 'max_depth': 8, 'num_leaves': 67, 'n_estimators': 1000, 'bagging_fraction': 0.4, 'feature_fraction': 0.5, 'bagging_freq': 5, 'bagging_seed': 2018, 'min_child_samples': 80, 'min_child_weight': 100.0, 'min_split_gain': 0.1, 'reg_alpha': 0.005, 'reg_lambda': 0.1, 'subsample_for_bin': 25000, 'min_data_per_group': 100, 'max_cat_to_onehot': 4, 'cat_l2': 25.0, 'cat_smooth': 2.0, 'max_cat_threshold': 32, 'random_state': 1, 'silent': True, 'metric': 'auc'}\n# Credit @jiazhuang\nTARGET = 'HasDetections'\nTARGET_INDEX = 'MachineIdentifier'\ndef modeling_cross_validation(params, X, y, folds=2):\n    clfs = list()\n    oof_preds = np.zeros(X.shape[0])\n    # Split data with kfold\n    kfolds = StratifiedKFold(n_splits=folds, shuffle=False, random_state=42)\n    for n_fold, (trn_idx, val_idx) in enumerate(kfolds.split(X, y)):\n        X_train, y_train = X.iloc[trn_idx], y.iloc[trn_idx]\n        X_valid, y_valid = X.iloc[val_idx], y.iloc[val_idx]\n\n        # LightGBM Regressor estimator\n        model = lgb.LGBMClassifier(**params)\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_valid, y_valid)],\n            verbose=-1, eval_metric='auc',\n            early_stopping_rounds=100\n        )\n\n        clfs.append(model)\n        oof_preds[val_idx] = model.predict(X_valid, num_iteration=model.best_iteration_)\n        \n    score = roc_auc_score(y, oof_preds)\n    print(score)\n    return clfs, score\n\ndef get_importances(clfs, feature_names):\n    importances = pd.DataFrame()\n    for i, model in enumerate(clfs, 1):\n        # Feature importance\n        imp_df = pd.DataFrame({\n                \"feature\": feature_names, \n                \"gain\": model.booster_.feature_importance(importance_type='gain'),\n                \"fold\": model.n_features_,\n                })\n        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n\n    importances['gain_log'] = importances['gain']\n    mean_gain = importances[['gain', 'feature']].groupby('feature').mean()\n    importances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n    importances.sort_values(by='gain', inplace=True, ascending=False)\n    importances.to_csv('feature_importances.csv', index=False)\n    return importances\n\ndef predict_cross_validation(test, clfs):\n    sub_preds = np.zeros(test.shape[0])\n    for i, model in enumerate(clfs, 1):    \n        test_preds = model.predict_proba(test, num_iteration=model.best_iteration_)\n        sub_preds += test_preds[:,1]\n\n    sub_preds = sub_preds \/ len(clfs)\n    ret = pd.Series(sub_preds, index=test.index)\n    ret.index.name = test.index.name\n    return ret\ndef predict_test_chunk(features, clfs, dtypes, filename='tmp.csv', chunks=100000):\n    \n    for i_c, df in enumerate(pd.read_csv('..\/input\/test.csv', \n                                         chunksize=chunks, \n                                         dtype=dtypes, \n                                         iterator=True)):\n        \n        df.set_index(TARGET_INDEX, inplace=True)\n        preds_df = predict_cross_validation(df[features], clfs)\n        preds_df = preds_df.to_frame(TARGET)\n\n        if i_c == 0:\n            preds_df.to_csv(filename, header=True, mode='a', index=True)\n        else:\n            preds_df.to_csv(filename, header=False, mode='a', index=True)\n        del preds_df\n        gc.collect()\ntrain_features = list()\ntrain = pd.read_csv('..\/input\/train.csv', nrows=50000, dtype=dtypes).set_index(TARGET_INDEX)\ntrain_features = [f for f in train.columns if f != TARGET]\nclfs, score = modeling_cross_validation(params, train[train_features], train[TARGET], folds=5)","384bb9df":"feature_importances = get_importances(clfs, train_features)\nfeature_importances.drop_duplicates(subset=['feature'], inplace=True)","bf2d0b60":"data = [go.Bar(\n            x= feature_importances.feature.values,\n            y= feature_importances.gain.values, marker = dict(\n          color = 'gold'\n        ), orientation = 'v'\n    )]\nlayout = go.Layout(\n    title='Feature Importances LGB Model')\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","32d6204d":"top40 = list(feature_importances['feature'][:40])\nprint(\">> Top 40 Features are:\\n\", top40)\nfeature_importances.head(40).set_index('feature').transpose()","9ea12641":"train_df[top40].head(20)","72b8f171":"crosstab_list = ['OsBuild', 'HasDetections', 'Census_OSVersion','ProductName']\nprint(\"CrossTab for:\", crosstab_list)\ncrosstab_all(train_df,crosstab_list, cmap='summer', figsize=(20,18), subplot_grid=(4,3), annot=False)","61e35f08":"crosstab_list = [ 'Census_OSVersion', 'SmartScreen','HasDetections']\nprint(\"CrossTab for:\", crosstab_list)\ncrosstab_all(train_df,crosstab_list, cmap='coolwarm', figsize=(27,15), subplot_grid=(2,3), annot=False)","d8ddf148":"crosstab_list = ['OsVer', 'Platform', 'Processor', 'HasDetections']\nprint(\"CrossTab for:\", crosstab_list)\ncrosstab_all(train_df,crosstab_list, cmap='winter', figsize=(20,18))","e6087237":"crosstab_list = ['Census_ProcessorClass', 'HasDetections', 'SmartScreen','ProductName']\nprint(\"CrossTab for:\", crosstab_list)\ncrosstab_all(train_df,crosstab_list, cmap='plasma', figsize=(20,18), subplot_grid=(4,3))","b083e819":"HasDetections = train_df[train_df.HasDetections == 1]\nNoDetections = train_df[train_df.HasDetections == 0]","b392338f":"def countPlot(key, title):\n    trace1 = go.Bar(\n        x= list(dict(HasDetections[key].value_counts()).keys()), \n        y= list(dict(HasDetections[key].value_counts()).values()),\n        name='HasDetections'\n    )\n    trace2 = go.Bar(\n        x= list(dict(NoDetections[key].value_counts()).keys()),\n        y= list(dict(NoDetections[key].value_counts()).values()),\n        name='NoDetections'\n    )\n\n    data = [trace1, trace2]\n    layout = go.Layout(\n        barmode='group', title=title\n    )\n    fig = go.Figure(data=data, layout=layout)\n    return py.iplot(fig)","579b4540":"train_df[top40].head(20)","835212e3":"countPlot('SmartScreen', \"CountPlot: SmartScreen\")","7e8de05b":"countPlot('Census_ChassisTypeName', \"CountPlot: Census_ChassisTypeName\")","a6d28d6d":"countPlot('AVProductsInstalled', \"CountPlot: AVProductsInstalled\")","06340eb2":"def multi_corr_heatmap(df,annot=True,\n                       cmap='viridis',\n                       figsize=(15, 10),\n                       title='Numerical Columns Correlation Heatmap',\n                       **kwargs):\n    plt.figure(figsize=figsize)\n    plt.suptitle(title, x=0.5, y=.94, size=20)\n    sns.heatmap(df.corr(), annot=annot, cmap=cmap, **kwargs)\nmulti_corr_heatmap(train_df[numerical_cols], cmap='Blues_r', annot=False, vmax=0.9)","27d1fdfa":"train_features = list(feature_importances.feature[:50])\nclfs, score = modeling_cross_validation(params, train[train_features], train[TARGET], folds=5)\nfilename = 'shaz13_submission_baseline.csv'\npredict_test_chunk(train_features, clfs, dtypes, filename=filename, chunks=100000)","c12a493f":"**More to come. Stay Tuned** \n\nI highly appreciate your feedbacks on this. Do let me know if you like more EDA or Baseline tuning from this point. Thanks! \n\nPeace. shaz13","26420e3b":"## Train Inspection Frame","a84f8566":"To make the analysis simple. I am taking only first **50,000** rows from the dataset","577952d2":"## Performing Chi2 Tests \n\nWe can plot the chi square matrix with simple crosstab function from pandas. Let us use this to make a custom crosstab_all function which takes a list of categorical colum and then generate chi2 matrices for all possible pairs. (Code Hidden In #2 Cell of Notebook)","66fadb59":"Great, now we have the challenge to build models so that we can detect Malware occurances for a particular specifications. To get started let us identify columns in our dataset","63ca46a7":"\n# Competition Information:\nThis is the second Microsoft Malware Hosted competition on Kaggle. The former can be found [here](https:\/\/www.kaggle.com\/c\/malware-classification). Let us see what's this competition is all about..\n\n![](https:\/\/zdnet1.cbsistatic.com\/hub\/i\/2014\/08\/27\/0d77a99a-2da9-11e4-9e6a-00505685119a\/1a84d511ed7039462265cda82825e113\/microsofts-logo-gets-a-makeover.jpg)\nThe malware industry continues to be a well-organized, well-funded market dedicated to evading traditional security measures. Once a computer is infected by malware, criminals can hurt consumers and enterprises in many ways.With more than one billion enterprise and consumer customers, Microsoft takes this problem very seriously and is deeply invested in improving security.\nAs one part of their overall strategy for doing so, Microsoft is challenging the data science community to develop techniques to predict if a machine will soon be hit with malware. As with their previous, Malware Challenge (2015), Microsoft is providing Kagglers with an unprecedented malware dataset to encourage open-source progress on effective techniques for predicting malware occurrences.\n\n\n\n## Goals \nThe goal of this competition is to predict a Windows machine\u2019s probability of getting infected by various families of malware, based on different properties of that machine. The telemetry data containing these properties and the machine infections was generated by combining heartbeat and threat reports collected by Microsoft's endpoint protection solution, Windows Defender. Each row in this dataset corresponds to a machine, uniquely identified by a MachineIdentifier. HasDetections is the ground truth and indicates that Malware was detected on the machine. Using the information and labels in train.csv, you must predict the value for HasDetections for each machine in test.csv.\n\n## Metric\n\nSubmissions are evaluated on [area under the ROC curve](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic) between the predicted probability and the observed label.\n","dfff1e6e":"Now, that we have clear picture of what's important, let us extract these and find their story. Priting Top 40 features that model discovered and head of train.csv with only important ones","c67f4882":"## Training With Top 50 Features For Submission","6dab8a86":"## Multicorrelation Matrix","00540583":"## Baseline Model And Feature Importances","d881a6a0":"Let's print out and take a look at the header frame","bfc0374c":"## Identifying And Grouping Columns","9cd37cca":"Let's check what tools and files we have been given to recipe our model..","8081ee9b":"## Target Analysis","4d617ee0":"## Test Inspection Frame","4ae1b9b8":"Huge datasets and many columns is always a bummer to eyes. But, statistics and pretty tabular conversion makes much sense. Let's study out such inspection of data types and values statistics for the features.\ngiven. The code above prints out information like;\n\n1. Data Type\n2. Unique Values\n3. Missing Values\n4. Percentage Of Missing\n5. Memory Usage (MB)\n\nAnd few statistical features ...\n\n6. Variance\n7. Mean\n8. Min\n9. Max","67b7c42e":"## HasDetection Vs. NoDetections","dcec47bc":"It occurs to me that the following features were well selective for a particular version release. \n\n**These features actually help us identify the software version\/ commits that might have introduced the vulnerability or loop holes in the OS**","a4f965be":"Rich information. Exactly what we need to make   robust machine learning models. Given the dimentionality of the dataset we will proceed analysis after taking importances by training a public baseline model"}}