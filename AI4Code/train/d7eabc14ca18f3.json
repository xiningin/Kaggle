{"cell_type":{"1078c57f":"code","3dc011b8":"code","2c041129":"code","13eac581":"code","4a54db98":"code","3e423a96":"code","34c9d0b8":"code","d5f2d920":"code","3d4774ef":"code","5e8bebd4":"code","83995c2b":"code","f37ffd6e":"code","d6f6885f":"code","056866f9":"code","cace8db7":"code","97552e5a":"code","3baa9a49":"code","a82f4df4":"code","34bb9428":"code","666d7ffb":"code","e7f31bc1":"code","49aadea2":"code","a9d7ffa7":"code","ea58059e":"code","371b9911":"code","9d93c1d1":"code","30f5a014":"code","4eda7853":"code","b6a90fc3":"code","b3b0c7b3":"code","8a44c440":"code","5f2d2372":"code","795e55e6":"code","6f7bd1d5":"code","8ee0f487":"code","ea557dfc":"code","f7aeb177":"code","4b174bac":"code","c54c08b0":"code","d62cbd49":"code","7ae7f731":"markdown","4d7209ea":"markdown","0a5f7f77":"markdown","bad7f04b":"markdown","5da8e217":"markdown","72a1655d":"markdown","126ed34b":"markdown","b47c49f0":"markdown","0f5c97a7":"markdown","4ea9fc5d":"markdown","806a33b4":"markdown","1c9bf375":"markdown","a5f2ba95":"markdown","2a6ecabc":"markdown","77f2f2c3":"markdown","552e2f83":"markdown","d8860808":"markdown","0c99a19d":"markdown","aa469147":"markdown","9f19c983":"markdown","02658f0b":"markdown","69884b8a":"markdown","639fb5f1":"markdown","2839d231":"markdown","4720035b":"markdown","e8dc7b6e":"markdown","c04e279c":"markdown","9c8433a1":"markdown","1a4f7b45":"markdown","c281c40d":"markdown","be5c07bd":"markdown","94a9d849":"markdown","dae4d4b4":"markdown","f29f14a3":"markdown","f6ae069f":"markdown","858df583":"markdown","0cbca39d":"markdown"},"source":{"1078c57f":"# Importing libraries\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom mlxtend.plotting import plot_decision_regions","3dc011b8":"df = pd.read_csv('..\/input\/drug-addiction-in-bangladesh-reasons\/drugAddiction.csv')\ndf.info()\ndf.describe()","2c041129":"df[df.duplicated()].count()","13eac581":"sns.countplot(x='Suicidal thoughts',hue='Gender',data=df,palette='RdBu_r')","4a54db98":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Education'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01)","3e423a96":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Gender'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01)","34c9d0b8":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['no. of friends'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01)","d5f2d920":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Family relationship'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01)","3d4774ef":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Age'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01)","5e8bebd4":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Financials of family'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01)","83995c2b":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Motive about drug'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01)","f37ffd6e":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Live with'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01)","d6f6885f":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Case in court'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01)","056866f9":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Living with drug user'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01[df['Gender']=='Male'])","cace8db7":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Living with drug user'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01[((df['Gender']==\"Female\"))])\n\n","97552e5a":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Suicidal thoughts'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01[((df['Gender']==\"Male\"))])","3baa9a49":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Suicidal thoughts'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01[((df['Gender']==\"Female\"))])","a82f4df4":"df01 = df.melt( id_vars=['Suicidal thoughts'],value_vars=['Education'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Suicidal thoughts\", hue='vals', data=df01)","34bb9428":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Smoking'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01)","666d7ffb":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Ever taken drug'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01)","e7f31bc1":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Spend most time'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01[((df['Gender']==\"Female\"))])","49aadea2":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Spend most time'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01[((df['Gender']==\"Male\"))])","a9d7ffa7":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Enjoyable with-'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01[((df['Gender']==\"Male\"))])","ea58059e":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Enjoyable with-'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01[((df['Gender']==\"Female\"))])","371b9911":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Satisfied with workplace'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01[((df['Gender']==\"Female\"))])","9d93c1d1":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Satisfied with workplace'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01[((df['Gender']==\"Male\"))])","30f5a014":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Friends influence'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01[((df['Gender']==\"Female\"))])","4eda7853":"df01 = df.melt( id_vars=['Frequency of drug usage'],value_vars=['Friends influence'], var_name='cols',  value_name='vals')\nplt.figure(figsize=(12,5))\ng =sns.countplot(x=\"Frequency of drug usage\", hue='vals', data=df01[((df['Gender']==\"Male\"))])","b6a90fc3":"dfclassification=df.copy()\ndfclassification['Suicidal thoughts']=dfclassification[['Suicidal thoughts']].apply(lambda x:x.map({'Yes':1,'No':0}))\ndfclassification['Gender']=dfclassification[['Gender']].apply(lambda x:x.map({'Male':1,'Female':0}))\ndfclassification['Failure in life']=dfclassification[['Failure in life']].apply(lambda x:x.map({'Yes':1,'No':0}))","b3b0c7b3":"mylist = list(dfclassification.select_dtypes(include=['object']).columns)\ndummy1=pd.get_dummies(dfclassification[mylist],drop_first=True)\ndfclassificationFinal = pd.concat([dummy1,dfclassification['Suicidal thoughts'],dfclassification['Gender'],dfclassification['Failure in life']], axis=1)\nfrom sklearn import preprocessing\nX = dfclassificationFinal.drop('Suicidal thoughts',axis=1).values\ny = dfclassificationFinal['Suicidal thoughts'].values","8a44c440":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.5,random_state=2,stratify=y)\n\ntrain_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\ntest_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\nprint('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))","5f2d2372":"model=SVC()\nmodel.fit(X_train,y_train)\nprint(f' Training Accuracy is:- {model.score(X_train,y_train)}')\nf'Test Accuracy is:- {model.score(X_test,y_test)}'","795e55e6":"\nk = range(1,20)\ntrainingAccuracy = []\ntestAccuracy=[]\nfor i in k:\n    knn = KNeighborsClassifier(n_neighbors=i,n_jobs=15,p=1,weights='distance')\n    knn.fit(X_train,y_train)\n    trainingacc = knn.score(X_train,y_train)\n    trainingAccuracy.append(trainingacc)\n    testAccuracy.append(knn.score(X_test,y_test))\n\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10,4))\nplt.xlabel(\"value of K\")\nplt.ylabel(\"Accuracy of test and training\")\nplt.title(\"Select best value of k\")\nplt.plot(k,trainingAccuracy)\nplt.plot(k,testAccuracy)\n  #axes[0].legend(['loss','val_loss'])\naxes.legend([\"Training Accurracy\",\"Test Accuracy\"])\nprint(\"\\n Best Test accuracy is:- \",max(testAccuracy))","6f7bd1d5":"k_range = list(range(1,50))\nweight_options = [\"uniform\", \"distance\"]\npe=[1,2]\n\nparam_grid = dict(n_neighbors = k_range, weights = weight_options,p=pe)\nknn = KNeighborsClassifier()\nknngrid = GridSearchCV(knn, param_grid, cv = 10, scoring = 'accuracy',n_jobs=15)\nknngrid.fit(X_train,y_train)\n\nprint (\"Best score on 10 folds split Data on Train split is :- \",knngrid.best_score_)\nprint (\"\\n Best Param:- \",knngrid.best_params_)\nprint (\"\\n Best KNN Metric:- \", knngrid.best_estimator_)\n\nprint(f' \\n Training Accuracy {knngrid.score(X_train,y_train)}')\nf'Test Accuracy {knngrid.score(X_test,y_test)}'","8ee0f487":"y_predicted = knngrid.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_predicted)\nimport seaborn as sn\nplt.figure(figsize = (10,7))\nsn.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","ea557dfc":"from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n\nlog_model.fit(X_train, y_train)\nprint(f' Training Accuracy {log_model.score(X_train,y_train)}')\nf'Test Accuracy {log_model.score(X_test,y_test)}'","f7aeb177":"folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4)\nparam_grid = [\n        {\n            'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n            'solver' : ['lbfgs', 'sgd', 'adam'],            \n        }\n       ]\nclf = GridSearchCV(MLPClassifier(), param_grid, cv=folds,\n                           scoring='accuracy',n_jobs=-1,verbose = 1,\n)\nclf.fit(X_train, y_train)\nprint(f' Training Accuracy {clf.score(X_train,y_train)}')\nf'Test Accuracy {clf.score(X_test,y_test)}'","4b174bac":"cv_results = pd.DataFrame(clf.cv_results_)\ncv_results[cv_results.rank_test_score<5]\n#print the optimum value of hyperparameters\nprint('Best hyperparameters: ', clf.best_params_)","c54c08b0":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('accuracy')>= 1.0):\n      print(\"\\nReached 100% accuracy so cancelling training!\")\n      self.model.stop_training = True\ncallbacks = myCallback()\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(800, activation=tf.nn.relu),\n  tf.keras.layers.Dense(256, activation=tf.nn.relu),\n  tf.keras.layers.Dense(256, activation=tf.nn.relu),\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n  \n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=50,batch_size=32,callbacks=[callbacks])\nprint(\"accuracy on test data is\", model.evaluate(X_test, y_test))","d62cbd49":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, RidgeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC,NuSVC\nfrom sklearn.neighbors import KNeighborsClassifier, NearestCentroid\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import precision_score\n\nmodels =[(\"SVC\", SVC()),\n         ('KNN',KNeighborsClassifier()),(\"DTC\", DecisionTreeClassifier()),\n         (\"GNB\", GaussianNB()),(\"SGDC\", SGDClassifier()),(\"Perc\", Perceptron()),\n         (\"NC\",NearestCentroid()),(\"Ridge\", RidgeClassifier()),\n         (\"BNB\", BernoulliNB()),('RF',RandomForestClassifier()),('ADA',AdaBoostClassifier()),\n         ('XGB',GradientBoostingClassifier()),('PAC',PassiveAggressiveClassifier())]\npred = []\nnames = []\nmodelsprecision = []\n\nfor name,model in models:\n    model.fit(X_train, y_train)\n    prediction = model.predict(X_test)\n    score = precision_score(y_test, prediction,average = 'macro')\n    pred.append(score)\n    names.append(name)\n    modelsprecision.append((name,score))\n    \nmodelsprecision.sort(key=lambda k:k[1],reverse=True)\n\nmodelsprecision","7ae7f731":"\n### <b>Model 02:- KNN without Hyper parameter tuning<\/b>","4d7209ea":"## Section 04:- Model Building","0a5f7f77":"* <h4> <b> From the above plot we see Regular usage is more in people who are regular\/everyday smokers, Intresting fact from the plot is if the person dont smoke he\/she may not get in to the Regular usage of drug <\/b><\/h4>","bad7f04b":"* <h4> <b> From the above plot we see Regularly usage of drug is more when person is having more than 5 friends.<\/b><\/h4>","5da8e217":"* <h4> <b> From the above plot we see Regular usage of drug in Female are those who are NOT influenced by their friends<\/b><\/h4>","72a1655d":"* <h4> <b> From the above plot we see Regular usage of drug in Female those who are Satisfied with their work in work place<\/b><\/h4>","126ed34b":"## Section 03:- Splitting the Data Equally in to X_train and X_test(Stratified logic)","b47c49f0":"* <h4> <b> From the above plot we see suicidal thoughts are more in Male compared to Female.<\/b><\/h4>","0f5c97a7":"* <h4> <b> From the above plot we see Suicidal Ideation is MORE if the person is Under graduate <\/b><\/h4>","4ea9fc5d":"* <h4> <b> From the above plot we see Regularly usage of drug is more in person whose Family relationship is average.<\/b><\/h4>","806a33b4":"* <h4> <b> From the above plot we see Regular usage of drug in Female spends most of time with Friends,One intresting fact to notice is if you spend most of time with Family\/Relatives you are most likely not get addicted <\/b><\/h4>","1c9bf375":"\n### <b>Model 01:- Support Vector Machine (SVM)<\/b>","a5f2ba95":"* <h4> <b> Regulary usage of Drug from the above plot is bit contradictory since few people have answered Ever taken drug to NO but still they have answered as they are regular users of Drug <\/b><\/h4>","2a6ecabc":"## Section 01:- Reading Data and Checking the Meta information","77f2f2c3":"* <h4> <b> From the above plot we see Regularly usage of drug is more in people belonging to Poor\/weak section of society<\/b><\/h4>","552e2f83":"* <h4> <b> From the above plot we see Regular usage of drug in Male are in to usage of multiple drugs <\/b><\/h4>","d8860808":"\n### <b>Model 03:- Logistic Regression<\/b>","0c99a19d":" * <h4> <b> From the above plot we see Regular usage of drug in Male are those who are NOT Satisfied with their work in work place<\/b><\/h4>","aa469147":"* <h4> <b> From the above plot we see Regularly usage of drug is more in people who are influenced by social trends<\/b><\/h4>","9f19c983":"\n### <b>Model 02:- KNN With Hyper parameter tuning<\/b>","02658f0b":"\n### <b>Model 04:- Neural Network<\/b>","69884b8a":"* <h4> <b> From the above plot we see Regularly usage of drug Same like Male's Female's are influenced if they are in toch with friend who is regularly using it.<\/b><\/h4>","639fb5f1":"* <h4> <b> From the above plot we see Regular usage of drug in Male are those who are influenced by their friends<\/b><\/h4>","2839d231":"* <h4> <b> From the above plot we see Suicidal Ideation is MORE in Female who are Regular users of drug <\/b><\/h4>","4720035b":"* <h4> <b> From the above plot we see Regularly usage of drug is more in MALE if their friends are used to the drugs<\/b><\/h4>","e8dc7b6e":"* <h4> <b> From the above plot we see Regularly usage of drug is more in Male compared to Female.<\/b><\/h4>","c04e279c":"* <h4> <b> From the above plot we see Regular usage of drug in Male spends most of time with Friends.One intresting fact to notice is if you spend most of time with Family\/Relatives you are most likely not get addicted  <\/b><\/h4>","9c8433a1":"\n### <b>Model 04:- Neural Network with Dense layers<\/b>","1a4f7b45":"## Conclusion:-\n* Logistic regression proved to be the good modal compared to others with 77% Accuracy on Test Data.\n\n## Future work:-\nThis data can be explored with clustering algorithms to find out the pattern among different clusters.","c281c40d":"* <h4> <b> From the above plot we see Suicidal Ideation is less in Male who are Regularly users of drug <\/b><\/h4>","be5c07bd":"### Spot Check Using Different Algorithms","94a9d849":"* <h4> <b> From the above plot we see Regular usage of drug in Female are in to usage of ONE drug <\/b><\/h4>","dae4d4b4":"* <h4> <b> From the above plot we see Regularly usage of drug is more in people who stay with their Family<\/b><\/h4>","f29f14a3":"## Section 02:- EDA","f6ae069f":"### One hot Encoding the columns","858df583":"* <h4> <b> From the above plot we see Regularly usage of drug is more between age group of 22 to 35<\/b><\/h4>","0cbca39d":"* <h4> <b> From the above plot we see Regular usage of Drug is more common in undergraduates than compared to others<\/b><\/h4>"}}