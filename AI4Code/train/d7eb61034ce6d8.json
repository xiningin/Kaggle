{"cell_type":{"dc7fc369":"code","99483b43":"code","0321e745":"code","982cc665":"code","ba64e230":"code","7f0fdd90":"code","ff25d821":"code","f0b6e48b":"code","1bda859b":"code","b68433e2":"code","44e82e1b":"code","bdd97069":"markdown","819f0130":"markdown","0394ddd1":"markdown"},"source":{"dc7fc369":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom sklearn.model_selection import KFold, RepeatedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\n\nimport os\nprint(os.listdir(\"..\/input\"))","99483b43":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","0321e745":"#Some basic features:\n\ntrain['question_len']   = train['question_text'].apply(lambda x: len(str(x)))\ntest['question_len']   = test['question_text'].apply(lambda x: len(str(x)))\n\ndf_all = pd.concat([train, test], axis=0)","982cc665":"print(train.shape)\nprint(train.head())\nprint(train.columns)\n","ba64e230":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ncorpus_3 = [\"one two three four \",\n           \"one two three  \",\n           \"one two \",\n           \"one\"]\nvectorizer = TfidfVectorizer(min_df=1)\nX = vectorizer.fit_transform(corpus_3)\nidf = vectorizer.idf_\nprint(dict(zip(vectorizer.get_feature_names(), idf)))","7f0fdd90":"print('Preprocessing text...')\ncols = [\"question_text\"]\nn_features = [\n    400, # Number of different features for project_title\n    4040, \n    400\n]\n\nfor c_i, c in tqdm(enumerate(cols)):\n    tfidf = TfidfVectorizer(max_features=n_features[c_i], min_df=3)\n    tfidf.fit(df_all[c])\n    tfidf_train = np.array(tfidf.transform(train[c]).todense(), dtype=np.float16) \n    tfidf_test = np.array(tfidf.transform(test[c]).todense(), dtype=np.float16)\n\n    for i in range(n_features[c_i]):\n        train[c + '_tfidf_' + str(i)] = tfidf_train[:, i]\n        test[c + '_tfidf_' + str(i)] = tfidf_test[:, i]\n        \n    del tfidf, tfidf_train, tfidf_test    \nprint('Done.')\ndel df_all\n","ff25d821":"train.reset_index(drop= True)\ntest.reset_index(drop= True)\n\npca_columns=train.columns[train.columns.str.contains(\"_tfidf_\")]\ntrain_pca = train[pca_columns]\ntest_pca = test[pca_columns]","f0b6e48b":"train = train.drop(pca_columns, axis=1, errors='ignore')\ntest = test.drop(pca_columns, axis=1, errors='ignore')\n\ntrain = train.merge(train_pca, right_index=True, left_index = True)\ntest = test.merge(test_pca, right_index=True, left_index = True)","1bda859b":"cols_to_drop = [\"qid\", \"question_text\", \"target\"]\n\nX = train.drop(cols_to_drop, axis=1, errors='ignore')\ny = train['target']\nX_test = test.drop(cols_to_drop, axis=1, errors='ignore')\nid_test = test['qid'].values\nfeature_names = list(X.columns)\nprint(X.shape, X_test.shape)\n\n\n# Build the model\ncnt = 0\np_buf = []\nn_splits = 5\nn_repeats = 1\nkf = RepeatedKFold(\n    n_splits=n_splits, \n    n_repeats=n_repeats, \n    random_state=0)\nauc_buf = []   \n\n\nfor train_index, valid_index in kf.split(X):\n    print('Fold {}\/{}'.format(cnt + 1, n_splits))\n    params = {\n          'objective': 'binary',\n          'metric': 'auc',\n          'boosting_type': 'dart',\n          'learning_rate': 0.08,\n          'max_bin': 15,\n          'max_depth': 10, #17\n          'num_leaves': 30, #63\n          'subsample': 0.8,\n          'subsample_freq': 5,\n          'colsample_bytree': 0.8,\n          'reg_lambda': 7,\n          'num_threads': 4}\n    \n    model = lgb.train(\n        params,\n        lgb.Dataset(X.loc[train_index], y.loc[train_index], feature_name=feature_names),\n        num_boost_round=10000,\n        valid_sets=[lgb.Dataset(X.loc[valid_index], y.loc[valid_index])],\n        early_stopping_rounds=100,\n        verbose_eval=100,\n    )\n\n    if cnt == 0:\n        importance = model.feature_importance()\n        model_fnames = model.feature_name()\n        tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]\n        tuples = [x for x in tuples if x[1] > 0]\n        print('Important features:')\n        print(tuples[:50])\n\n    p = model.predict(X.loc[valid_index], num_iteration=model.best_iteration)\n    auc = roc_auc_score(y.loc[valid_index], p)\n\n    print('{} AUC: {}'.format(cnt, auc))\n\n    p = model.predict(X_test, num_iteration=model.best_iteration)\n    if len(p_buf) == 0:\n        p_buf = np.array(p)\n    else:\n        p_buf += np.array(p)\n    auc_buf.append(auc)\n\n    cnt += 1\n    if cnt > 0: # Comment this to run several folds\n        break\n    \n    del model\n    gc.collect\n\nauc_mean = np.mean(auc_buf)\nauc_std = np.std(auc_buf)\nprint('AUC = {:.6f} +\/- {:.6f}'.format(auc_mean, auc_std))\n\npreds = p_buf\/cnt","b68433e2":"cols_to_drop = [\"qid\", \"question_text\", \"target\"]\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\ntrain = train.rename(columns = {'<lambda>':'lamda'})\ntest = test.rename(columns = {'<lambda>':'lamda'})\n\nX = train.drop(cols_to_drop, axis=1, errors='ignore')\ny = train['target']\nX_test = test.drop(cols_to_drop, axis=1, errors='ignore')\nid_test = test['qid'].values\nfeature_names = list(X.columns)\nprint(X.shape, X_test.shape)\n\n# Build the model\ncnt = 0\np_buf = []\nn_splits = 5\nn_repeats = 1\nkf = RepeatedKFold(\n    n_splits=n_splits, \n    n_repeats=n_repeats, \n    random_state=0)\nauc_buf = []   \n\nfor train_index, valid_index in kf.split(X):\n    print('Fold {}\/{}'.format(cnt + 1, n_splits))\n\n    \n    xlf = XGBRegressor(\n          objective= 'binary:logistic',\n          eval_metric= 'auc',\n          eta=0.01,\n          max_depth= 7,\n          subsample =0.8, \n          colsample_bytree= 0.4,\n          min_child_weight= 10,\n          gamma= 2)\n    \n    \n    #xlf.fit(X.loc[train_index], y.loc[train_index], eval_metric='rmse')\n    xlf.fit(X.loc[train_index], y.loc[train_index], eval_metric='rmse', verbose = True, eval_set = [(X.loc[train_index], y.loc[train_index]), (X.loc[valid_index], y.loc[valid_index])], early_stopping_rounds=80)\n    p = xlf.predict(X.loc[valid_index])\n","44e82e1b":"subm = pd.DataFrame()\nsubm['qid'] = id_test\nsubm['prediction'] = preds.round().astype(int)\nsubm.to_csv('submission.csv', index=False)","bdd97069":"Not so long ago there was this competition called DonorsChoose.org Application Screening. It was one of my firsts competitions in Kaggle and I learn a lot about text analysis loooking at the kernels from this competition. If you don't kno where to start I think that competition will be the first place to look up. Also you will find the kernel of the guy that won the competition, unfurtunately I haven read the code but If you are an eager learner as I am, you will use the same kernel to uderstand how and why it beat so many models in DonosChoose competition. Also I have to add that DonorsChoose is a very intuitive and dun competition so you will have agreat time exploring it! ","819f0130":"What TfidfVectorizer does?\nThis classs of sklearn allow us to compare words inside two texts and tell us how each of these words are realted within the texts. When we apply it for diferent number of texts the algorithm return a score that represents how strange is the word in relation to the other texts. In this example we can see that the word one is the most repeated and it appear in all the texts, then it must have the lowest possible value. In the other hand four only appear in the first text then it must have the higher poissible value.","0394ddd1":"PCA"}}