{"cell_type":{"bb71e318":"code","61ef3f5f":"code","e41347ff":"code","d4616ed2":"code","30879dec":"code","70bd2e96":"code","5454b5e7":"code","bb785cba":"markdown","bfd1e9b4":"markdown","65e74007":"markdown","b6074a41":"markdown","086045fe":"markdown","6cbf82a7":"markdown","453eeb74":"markdown"},"source":{"bb71e318":"import os\nos.chdir(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\")  \n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import uniform, randint, norm\nimport xgboost as xgb\n\nfrom sklearn.preprocessing import OneHotEncoder, scale, StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer, TransformedTargetRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nfrom math import sqrt      \n\n# Installing Microsoft's package for \"explanable boosting machine\"\n!pip install -U interpret\n\n# Set random seed\nnp.random.seed(123)\n\n# loading data\ndata = pd.read_csv(\"train.csv\")","61ef3f5f":"# droping (almost) perfectly correlated variables\ndata.drop([\"GarageArea\", \"1stFlrSF\", \"GrLivArea\"], axis=1)\n\n# replacing intended NAs\nNA_to_no = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \n            \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \n            \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]\n\nfor i in NA_to_no:\n  data[i] = data[i].fillna(\"N\")\n\n# Droping the two features with many missing values\ndata = data.drop([\"LotFrontage\", \"GarageYrBlt\"], axis = 1)\n\n#Dropping the outliers\ndata = data[data.GrLivArea<4000]\n\n# Splitting the features from the target, and the train and test sets\n\nX = data\nX = X.drop(\"SalePrice\", axis=1)\ny = data.loc[:,\"SalePrice\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=69)\n\n# identifying the categorical and numeric variables\n\ncategorical = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LotShape\", \"LandContour\", \"Utilities\", \"LotConfig\",\"LandSlope\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"ExterQual\", \"ExterCond\", \"Foundation\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"Heating\", \"HeatingQC\", \"CentralAir\", \"Electrical\", \"KitchenQual\", \"Functional\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PavedDrive\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"MoSold\", \"SaleType\", \"SaleCondition\", \"BedroomAbvGr\", \"KitchenAbvGr\"]\n\nnumeric = [\"LotArea\", \"OverallQual\", \"OverallCond\", \"YearBuilt\", \"YearRemodAdd\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"2ndFlrSF\", \"LowQualFinSF\", \"BsmtFullBath\", \"BsmtHalfBath\", \"FullBath\", \"HalfBath\", \"TotRmsAbvGrd\", \"Fireplaces\", \"GarageCars\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"MiscVal\", \"YrSold\"]","e41347ff":"# I use the log transformation for prediction\n\ndef log(x):\n    return np.log(x)\n\ndef exp(x):\n    return np.exp(x)\n\n# Setting up the preprocessor.\npreprocessor = make_column_transformer((make_pipeline(SimpleImputer(strategy=\"most_frequent\"), \n                                                      OneHotEncoder(handle_unknown=\"ignore\")), categorical), \n                                       (make_pipeline(SimpleImputer(strategy=\"median\"),\n                                                      StandardScaler()), numeric))\n\n# Instantiating the model\npipeline_linear = make_pipeline(preprocessor,\n                               TransformedTargetRegressor(LinearRegression(),\n                               func=log, inverse_func =exp))\n\n#Fitting the model and retrieving the prediction\npipeline_linear.fit(X_train, y_train)\nline_pred = pipeline_linear.predict(X_test)","d4616ed2":"pipeline_xgb = make_pipeline(preprocessor,\n                    TransformedTargetRegressor(xgb.XGBRegressor(objective ='reg:squarederror', nthread=-1), \n                                               func=log, inverse_func=exp))\n# Hyperparameters distributions\nparams = {\n    \"transformedtargetregressor__regressor__colsample_bytree\": uniform(0.7, 0.3),\n    \"transformedtargetregressor__regressor__gamma\": uniform(0, 0.5),\n    \"transformedtargetregressor__regressor__learning_rate\": uniform(0.03, 0.3),\n    \"transformedtargetregressor__regressor__max_depth\": randint(2, 6),\n    \"transformedtargetregressor__regressor__n_estimators\": randint(500, 1000),\n    \"transformedtargetregressor__regressor__subsample\": uniform(0.6, 0.4)\n}\n\n# Instantiating the xgboost model, with random-hyperparameter tuning\nxgb_model = RandomizedSearchCV(pipeline_xgb, param_distributions=params, random_state=123, \n                               n_iter=50, cv=5, n_jobs=-1)\n\n#Fitting the model and retrieving the predictions\nxgb_model.fit(X_train, y_train)\nxgb_pred = xgb_model.predict(X_test)","30879dec":"from interpret.glassbox import ExplainableBoostingRegressor\nfrom interpret import show\nfrom interpret.data import Marginal\n\n# Definition of the EBM preprocessor; I do not one hot encode, since EBM deals with categoricals\n\npreprocessor_ebm = make_column_transformer(\n    (SimpleImputer(strategy=\"most_frequent\"), categorical),\n    (SimpleImputer(strategy=\"median\"), numeric)\n    )\n\n# Instantiating the model\nebm = make_pipeline(preprocessor_ebm, \n                    TransformedTargetRegressor(ExplainableBoostingRegressor(random_state=123),\n                    func=log, inverse_func=exp))\n\n#Fitting the model and retrieving the predictions\nebm.fit(X_train, y_train)\nebm_pred = ebm.predict(X_test)","70bd2e96":"params = {\n    \"xgbregressor__colsample_bytree\": uniform(0.7, 0.3),\n    \"xgbregressor__gamma\": uniform(0, 0.5),\n    \"xgbregressor__learning_rate\": uniform(0.03, 0.3),\n    \"xgbregressor__max_depth\": randint(2, 6),\n    \"xgbregressor__n_estimators\": randint(500, 1000),\n    \"xgbregressor__subsample\": uniform(0.6, 0.4)\n}\n\npipeline_xgb2 = make_pipeline(preprocessor,\n                              xgb.XGBRegressor(objective ='reg:squarederror', nthread=-1))\n\nxgb_model_2 = RandomizedSearchCV(pipeline_xgb2, param_distributions=params, random_state=123,\n                                 n_iter=50, cv=5)\n\n# getting residual predictions from the train data\nebm_pred_train = ebm.predict(X_train)\nebm_residual_train = y_train - ebm_pred_train\n\n# training the xgb from the train data residual\nxgb_model_2.fit(X_train, ebm_residual_train)\nresidual_predicted = xgb_model_2.predict(X_test)\n\n# then we get our boosted ebm prediction\nebm_xgb_pred = ebm_pred + residual_predicted","5454b5e7":"# Getting performance \n\npredict = [line_pred, xgb_pred, ebm_pred, ebm_xgb_pred]\n\nmae = []\nmse = []\nrmse = []\n\nfor i in predict:\n    mae.append(mean_absolute_error(y_test, i))\n    mse.append(mean_squared_error(y_test, i))\n    rmse.append(sqrt(mean_squared_error(y_test, i)))\n\nscores = pd.DataFrame([mae, mse, rmse], \n                      columns=[\"line\", \"xgb\", \"ebm\", \"ebm + xgb\"],\n                      index = [\"mae\", \"mse\", \"rmse\"])\n\nscores[\"ebm + xgb over ebm\"] = (round((scores[\"ebm\"]\/scores[\"ebm + xgb\"] -1)*100, 2)\\\n                                .astype(str) +\" %\")\nscores[\"xgb over ebm + xgb\"] = (round((1- scores[\"xgb\"]\/scores[\"ebm + xgb\"])*100, 2)\\\n                                .astype(str) +\" %\")\n\nscores","bb785cba":"# Linear model","bfd1e9b4":"# Preprocessing\nSome features of this dataset are notoriously problematic. In a real situation, we would investigate the nature of each variable, especially for the reasons behind the missing data. We would also investigate about outliers. Since we do not wish to spend too much time on this dataset, we rely on its author's remarks and the previous works in the data science community.\n## (Almost) perfect correlation between features\nWe drop GarageArea, since it is almost perfectly correlated with GarageCars. Same for 1stFloorSF, TotalBsmtSF and GrLivArea, TotRmsAbvGrd. We keep the later in both cases.\n## Intended missing values\nMost NAs are tagged so voluntarily. For example, the data dictionnary indicates that the PoolQc variable is missing if the property has no pool. We will thus replace them by the string \"no\", which will not be interpreted as missing.\n## Other missing values\nLooking at the remaining missing values, we find that LotFrontage, that is the \"Linear feet of street connected to property\" has more than 15% of NAs. For now, we do not have an explanation for this. We will thus simply remove this feature. We do the same for the variable GarageYrBlt.\n\nThe three remaining features have less than one percent of NAs. We will deal with them in the preprocessing pipeline. The two numeric NAs will be changed for the median of the respective variable and the NA for the variable Electrical will take is most frequent value.\n## Outliers\nThere are five points that the author of the dataset identified as outliers. Three of them are partial sales that simply are another kind of transaction. We thus follow the recommendation of the author by removing all transactions with more than 4000 square feets of total living area (above ground). There are simply not such enough cases in the dataset to properly train a model.","65e74007":"# Introduction\n\nIn this notebook, we test a method that will provide the best possible tradeoff between interpretability and prediction. We first make predictions that rely on a very transparent and interpretable model. For this part, we use Microsoft's [\"explanable boosting machine\"](https:\/\/github.com\/interpretml\/interpret). We then use the xgboost library to reduce our model's error of prediction. One way to put it is that our model goes as far as it is humanly interpretable. From there, we use a model with higher predictive performances to reduce the prediction mistakes.\n\nIn short:\n\n$$(1)\\;\\;\\; y_i = f(X_i) $$\n\nWhere $y_i$ is the target variable, $X_i$ the features vector and $f$ is an unknown data generating process.\n\n$$(2)\\;\\;\\; y_i = \\hat{y}_i + \\lambda_i $$\n\nWhere $\\hat{y}_i$ is estimated with a glassbox model, $\\lambda_i$ the prediction's residual.\n\n$$(3)\\;\\;\\; y_i = \\hat{y}_i + \\hat{\\lambda}_i + \\sigma_i $$\n\nWhere $\\hat{\\lambda}_i$ is estimated with a blackbox model, and $\\sigma_i$ is the new residual. We hypothesize that $\\sum_{i=1}^{N}\\lambda_i^2 > \\sum_{i=1}^{N}\\sigma_i^2$.\n\nWe believe it is a better method than stacking an interpretable and blackbox models, or using ex-post sensitivity tests (like SHAP), since the additive structure sets clear boundaries between the interpretable and non-intepretable parts of each prediction. \n\nWe use the dataset on houses sale in Ames Iowa prepared by Dean De Cock (2011). It is a widely used and some high quality publicly available notebooks already did a good job at exploring it. We can thus save some time building upon them to test our method.\n# Environment & loading data","b6074a41":"# ebm + xgboost ","086045fe":"# xgboost","6cbf82a7":"# ebm","453eeb74":"# Comparing performances\nIt has been remarked in the past that ebm gives similar prediction performances than xgboost. Our method reaches performances that are in between the two."}}