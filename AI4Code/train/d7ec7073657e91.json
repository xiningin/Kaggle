{"cell_type":{"01d6fb7d":"code","e38eb3fb":"code","00b147a9":"code","86fe20ea":"code","82550995":"code","d3224cf7":"code","eff35ec0":"code","ce9b246b":"code","7b2a23ba":"code","cb4130f3":"code","cd49dbc3":"code","48125295":"code","96e62aa4":"code","2e5248ca":"code","f59f37bf":"code","7df78e0d":"code","19a5b586":"code","0e200dd1":"code","be97d2ff":"code","c4a77807":"code","ce8d34ee":"code","d57b2a2f":"code","360145a6":"code","18164e23":"code","934801b6":"code","ef81c857":"code","2541b9f2":"code","100e2080":"code","db1063f3":"code","80d07dce":"code","62048ad7":"code","ef5d3974":"code","fc56d6ee":"code","862acbd4":"code","483ea34e":"code","283df12a":"code","4b8db63c":"code","b3ccaa3f":"code","37b966b4":"code","5402c442":"code","814bec37":"code","246f063c":"code","35ac88f5":"code","6c5a06ce":"code","e0f3f105":"code","9bc89b2e":"code","95294b03":"code","028a14e8":"code","d4b57ff6":"code","6b33842e":"code","9f1a9f1d":"code","06c4ea61":"code","5bca0aee":"code","83ad5719":"code","f29d076b":"code","6f54182d":"code","246a65d3":"code","f36e6a48":"code","efbd3fd7":"markdown","b382e6f0":"markdown","043a3659":"markdown","0969dd9c":"markdown","95caf657":"markdown","b02c50c1":"markdown","67b1cb14":"markdown","72a27023":"markdown","76e596c1":"markdown","84835831":"markdown","38e7e023":"markdown","5a1ce266":"markdown","d1813533":"markdown","4d89d1ee":"markdown","ecccd572":"markdown","ad560e5a":"markdown"},"source":{"01d6fb7d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e38eb3fb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.utils import resample\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nimport multiprocessing as mp\nfrom bayes_opt import BayesianOptimization","00b147a9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.utils import resample\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier","86fe20ea":"train_df = pd.read_csv('\/kaggle\/input\/massp-health-insurance-prediction\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/massp-health-insurance-prediction\/test.csv')\ntrain_df[\"Response\"] = preprocessing.LabelEncoder().fit_transform(train_df[\"Response\"])","82550995":"train_df.shape, test_df.shape","d3224cf7":"train_df.info()","eff35ec0":"train_df.isnull().sum()","ce9b246b":"test_df.info()","7b2a23ba":"train_df['Response'].value_counts()","cb4130f3":"f, ax = plt.subplots(figsize=(6, 8))\nax = sns.countplot(x=\"Response\", data=train_df, palette=\"Set1\")\nplt.show()\n","cd49dbc3":"# Comment: So imbalanced--> resampling","48125295":"# RESAMPLING THE TRAINING DATA\nresampling = True\nif resampling:\n    df_0 = train_df[train_df[\"Response\"]==0] # MAJORITY\n    df_1 = train_df[train_df[\"Response\"]==1] # MINORITY\n\n    df_1 = resample(df_1, replace=True, n_samples=len(df_0))\n\n    train_df = pd.concat([df_0, df_1])\ntrain_df","96e62aa4":"train_df['Response'].value_counts()","2e5248ca":"f, ax = plt.subplots(figsize=(6, 8))\nax = sns.countplot(x=\"Response\", data=train_df, palette=\"Set1\")\nplt.show()\n","f59f37bf":"pipe = []\ndef composition(*pipe):\n    def helper(x):\n        out = x\n        for f in pipe:\n            out = f(out)\n        return out\n    return helper","7df78e0d":"def to_label(column):\n    \"\"\"\n    return a function that convert a column into label\n    \"\"\"\n    def helper(df):\n        if df[column].dtype == np.float32 or df[column].dtype == np.float64:\n            df[column] = df[column].astype(np.int64)\n        return df\n    return helper\ndef to_float(column):\n    \"\"\"\n    return a function that convert a column into number (float64)\n    \"\"\"\n    def helper(df):\n        df[column] = df[column].astype(np.float64)\n        return df\n    return helper","19a5b586":"# PLOT\ndef plot_label(column, df):\n    _, ax = plt.subplots(figsize = (12,6), ncols=2)\n    sns.countplot(x=column, data=df[df[\"Response\"] == 0], ax=ax[0])\n    ax[0].set_title(\"Response = 0\")\n    sns.countplot(x=column, data=df[df[\"Response\"] == 1], ax=ax[1])\n    ax[1].set_title(\"Response = 1\")\ndef plot_numeric(column, df):\n    _, ax = plt.subplots(figsize = (12,6), ncols=2)\n    sns.histplot(x=column, data=df[df[\"Response\"] == 0], ax=ax[0])\n    ax[0].set_title(\"Response = 0\")\n    sns.histplot(x=column, data=df[df[\"Response\"] == 1], ax=ax[1])\n    ax[1].set_title(\"Response = 1\")","0e200dd1":"# find categorical variables\n\ncategorical = ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage']\ntrain_df[categorical].info()","be97d2ff":"# check for labels in categorical variables\n\nfor var in categorical:\n    \n    print(var, ' contains ', len(train_df[var].unique()), ' labels')","c4a77807":"#Explore gender\npipe.append(to_label(\"Gender\"))\ntrain_df = pipe[-1](train_df)","ce8d34ee":"plot_label(\"Gender\", train_df)","d57b2a2f":"#Explore Driving_License\npipe.append(to_label(\"Driving_License\"))\ntrain_df = pipe[-1](train_df)\nplot_label(\"Driving_License\", train_df)","360145a6":"#Previously_Insured \npipe.append(to_label(\"Previously_Insured\"))\ntrain_df = pipe[-1](train_df)\nplot_label(\"Previously_Insured\", train_df)","18164e23":"#Vehicle_Age\npipe.append(to_label(\"Vehicle_Age\"))\ntrain_df = pipe[-1](train_df)\nplot_label(\"Vehicle_Age\", train_df)","934801b6":"#Vehicle_Damage\npipe.append(to_label(\"Vehicle_Damage\"))\ntrain_df = pipe[-1](train_df)\nplot_label(\"Vehicle_Damage\", train_df)","ef81c857":"categorical = ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage']","2541b9f2":"# find numerical variables\nnumerical = ['Age','Region_Code','Annual_Premium','Policy_Sales_Channel', 'Vintage']\nnumerical","100e2080":"train_df[numerical].info()","db1063f3":"#Age\npipe.append(to_label(\"Age\"))\ntrain_df = pipe[-1](train_df)\nplot_label(\"Age\", train_df)","80d07dce":"# comment: Yes t\u1eadp chung \u1edf \u0111\u1ed9 tu\u1ed5i 30-40, No t\u1eadp chung \u1edf \u0111\u1ed9 tu\u1ed5i 20\n","62048ad7":"#Region_Code\npipe.append(to_label(\"Region_Code\"))\ntrain_df = pipe[-1](train_df)\nplot_label(\"Region_Code\", train_df)","ef5d3974":"#Policy_Sales_Channel\npipe.append(to_label(\"Policy_Sales_Channel\"))\ntrain_df = pipe[-1](train_df)\nplot_label(\"Policy_Sales_Channel\", train_df)","fc56d6ee":"# Vintage\npipe.append(to_float(\"Vintage\"))\ntrain_df = pipe[-1](train_df)     \nplot_numeric(\"Vintage\", train_df)","862acbd4":"numerical = ['Age','Region_Code','Annual_Premium','Policy_Sales_Channel', 'Vintage']\n#Found outliers\n# view summary statistics in numerical variables\nprint(round(train_df[numerical].describe()))\n# Age, Ann, Poli","483ea34e":"_, ax = plt.subplots(figsize = (12,6))\nsns.heatmap(train_df.corr(), annot=True, linewidths=.5, ax=ax)","283df12a":"# Slightly corr--> Decided not drop any features.","4b8db63c":"def to_Xy(df, onehot=True):\n    \"\"\"one hot encoding \"\"\"\n    # X\n    columns = numerical +categorical\n    X = []\n    for col in columns:\n        if df[col].dtype != np.float64: \n            df[col] = preprocessing.LabelEncoder().fit_transform(df[col])\n            if onehot:\n                X.append(pd.get_dummies(df[col], prefix=col))\n            else:\n                X.append(df[col])\n        else:\n            X.append(df[col])\n            \n    X = pd.concat(X, axis=1)\n    \n    # y\n    if \"Response\" in df.columns:\n        y = df[\"Response\"]\n    else:\n        y = None\n    return X, y\n","b3ccaa3f":"target_feature = \"Response\"","37b966b4":"df = pd.concat([train_df, test_df])\nX, y = to_Xy(df, onehot = False)","5402c442":"for i in X:\n    X[i] = X[i].astype(float)","814bec37":"X_Train = X[~np.isnan(y)]\ny_Train = y[~np.isnan(y)]\nX_test = X[np.isnan(y)]","246f063c":"X_Train","35ac88f5":"y_Train","6c5a06ce":"def cross_validation(clf, features, k=10, random_state=0):\n    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n    \n    auc_valid_avg = []\n    for train_idx, valid_idx in kf.split(np.arange(len(y_Train))):\n        # resampling\n        X_ = []\n        y_ = []\n        for label in y_Train.iloc[train_idx].unique():\n            X_.append(X_Train.iloc[train_idx, :][y_Train.iloc[train_idx] == label])\n            y_.append(y_Train.iloc[train_idx][y_Train.iloc[train_idx] == label])\n        n_samples = max(yy.shape[0] for yy in y_)\n        for i in range(len(y_)):\n            if len(y_[i]) < n_samples:\n                X_[i], y_[i] = resample(X_[i], y_[i], n_samples=n_samples, replace=True)\n\n        X_train = pd.concat(X_)\n        y_train = pd.concat(y_)\n        X_valid = X_Train.iloc[valid_idx]\n        y_valid = y_Train.iloc[valid_idx]\n\n        # train model\n        clf.fit(X_train[features] , y_train)\n        y_train_pred = clf.predict_proba(X_train[features])[:, 1]\n        auc_train = roc_auc_score(y_train, y_train_pred)\n        y_valid_pred = clf.predict_proba(X_valid[features])[:, 1]\n        auc_valid = roc_auc_score(y_valid, y_valid_pred)\n\n        auc_valid_avg.append(auc_valid)\n\n    auc_valid_avg = sum(auc_valid_avg) \/ len(auc_valid_avg)\n    return auc_valid_avg","e0f3f105":"features = categorical + numerical ","9bc89b2e":"   \n\n# train\nclf = XGBClassifier(\n    n_estimators=1000,\n    random_state=0,\n    n_jobs=mp.cpu_count(),\n    eval_metric=\"auc\",\n)\nclf.fit(X_Train , y_Train)\n\ny_pred = clf.predict_proba(X_Train)[:, 1]\nauc = roc_auc_score(y_Train, y_pred)\nauc","95294b03":"feature_importance = clf.feature_importances_\n_, ax = plt.subplots()\nax.plot(np.arange(len(feature_importance)), feature_importance)","028a14e8":"feature_importance = clf.feature_importances_\n_, ax = plt.subplots()\nax.plot(np.arange(len(feature_importance)), np.sort(feature_importance))","d4b57ff6":"features = np.array(features)[np.argsort(feature_importance)[::-1]]\nfeatures","6b33842e":"def objective(num_features: int, n_estimators: int, max_depth: int, reg_alpha: float, reg_lambda: float):\n    chosen_features = features[0: int(round(num_features))]\n    auc = cross_validation(\n        clf=XGBClassifier(\n            n_estimators=int(round(n_estimators)),\n            max_depth=int(round(max_depth)),\n            reg_alpha=reg_alpha,\n            reg_lambda=reg_lambda,\n            random_state=0,\n            n_jobs=mp.cpu_count(),\n            eval_metric=\"error\",\n        ),\n        features=chosen_features,\n    )\n    return auc","9f1a9f1d":"\npbounds = {\n    \"num_features\": (1,10),\n    \"n_estimators\": (100, 200),\n    \"max_depth\": (len(features)\/\/8, len(features)\/\/4),\n    \"reg_alpha\": (1.0, 5.0),\n    \"reg_lambda\": (1.0, 5.0),\n}\n\noptimizer = BayesianOptimization(\n    f=objective,\n    pbounds=pbounds,\n    random_state=0,\n)\n\noptimizer.maximize(n_iter=50)","06c4ea61":"optimizer.max","5bca0aee":"chosen_features = features[0: int(round(10.0))]\nclf=XGBClassifier(\n    n_estimators=int(round(195.87756776788973)),\n    max_depth=int(round(2.0)),\n    reg_alpha=5.0,\n    reg_lambda=5.0,\n    random_state=0,\n    n_jobs=mp.cpu_count(),\n    eval_metric=\"error\",\n)\nclf.fit(X_Train[chosen_features], y_Train)","83ad5719":"X_test\n","f29d076b":"\ntest_df[\"Response\"] = clf.predict_proba(X_test[chosen_features])[:, 1]\ntest_df[[\"id\", \"Response\"]].to_csv(f\"\/kaggle\/working\/xg_n_{int(round(196))}.csv\", index=False)","6f54182d":"test_df[[\"id\", \"Response\"]]","246a65d3":"randomforest = False\nif randomforest:\n    # ENCODING FOR CATEGORICAL FEATURES\n    df = pd.concat([train_df, test_df])\n    X, y = to_Xy(df, onehot=False)\n    # SPLIT TRAIN - TEST\n    X_train = X[~np.isnan(y)]\n    y_train = y[~np.isnan(y)]\n    X_test = X[np.isnan(y)]\n    ##\n\n    X_t, X_v, y_t, y_v = train_test_split(X_train, y_train, test_size=0.2)\n    d = 15\n    clf = RandomForestClassifier(max_depth=d, random_state=0)\n    clf.fit(X_t, y_t)\n    auc_t = roc_auc_score(y_t, clf.predict_proba(X_t)[:, 1])\n    auc_v = roc_auc_score(y_v, clf.predict_proba(X_v)[:, 1])\n    print(d, auc_t, auc_v)\n\n    # all\n    # clf = make_pipeline(StandardScaler(), CatBoostClassifier(iterations=i, verbose=False))\n    # clf.fit(X_train, y_train)\n    test_df[\"Response\"] = clf.predict_proba(X_test)[:, 1]\n    test_df[[\"id\", \"Response\"]].to_csv(f\"\/kaggle\/working\/randomforest_depth_{d}.csv\", index=False)\n","f36e6a48":"xg = False\nif xg:\n    # ENCODING FOR CATEGORICAL FEATURES\n    df = pd.concat([train_df, test_df])\n    X, y = to_Xy(df, onehot=False)\n    # SPLIT TRAIN - TEST\n    X_train = X[~np.isnan(y)]\n    y_train = y[~np.isnan(y)]\n    X_test = X[np.isnan(y)]\n    y_test = y[np.isnan(y)]\n    ##\n    features  = ['Age','Annual_Premium','Policy_Sales_Channel','Gender', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage','Vintage']\n    X_t, X_v, y_t, y_v = train_test_split(X_train, y_train, test_size=0.2)\n    n = 1000\n    clf = XGBClassifier(n_jobs=mp.cpu_count(),n_estimators= n, learning_rate=0.1)\n    clf.fit(X_t[features], y_t)\n    auc_t = roc_auc_score(y_t, clf.predict_proba(X_t[features])[:, 1])\n    auc_v = roc_auc_score(y_v, clf.predict_proba(X_v[features])[:, 1])\n    print(n, auc_t, auc_v)\n\n    # all\n    # clf = make_pipeline(StandardScaler(), CatBoostClassifier(iterations=i, verbose=False))\n    # clf.fit(X_train, y_train)\n    test_df[\"Response\"] = clf.predict_proba(X_test)[:, 1]\n    test_df[[\"id\", \"Response\"]].to_csv(f\"\/kaggle\/working\/xg_n_{n}.csv\", index=False)","efbd3fd7":"# Feature Importance\n","b382e6f0":"def objective(num_features: int, n_estimators: int, max_depth: int, reg_alpha: float, reg_lambda: float):\n    chosen_features = features[0: int(round(num_features))]\n    auc = cross_validation(\n        clf=XGBClassifier(\n            n_estimators=int(round(n_estimators)),\n            max_depth=int(round(max_depth)),\n            reg_alpha=reg_alpha,\n            reg_lambda=reg_lambda,\n            random_state=0,\n            n_jobs=mp.cpu_count(),\n            eval_metric=\"error\",\n        ),\n        features=chosen_features,\n    )\n    return auc","043a3659":"auc","0969dd9c":"df = pd.concat([train_df, test_df])\n    X, y = to_Xy(df, onehot=False)\n    # SPLIT TRAIN - TEST\n    X_train = X[~np.isnan(y)]\n    y_train = y[~np.isnan(y)]\n    X_test = X[np.isnan(y)]\n    y_test = y[np.isnan(y)]\n    ##\n\n    X_t, X_v, y_t, y_v = train_test_split(X_train, y_train, test_size=0.2)\n    n = 1000\n    clf = XGBClassifier(\n            n_estimators=int(round(n_estimators)),\n            max_depth=int(round(max_depth)),\n            reg_alpha=reg_alpha,\n            reg_lambda=reg_lambda,\n            random_state=0,\n            n_jobs=mp.cpu_count(),\n            eval_metric=\"error\",\n        ),\n    clf.fit(X_t, y_t)\n    auc_t = roc_auc_score(y_t, clf.predict_proba(X_t)[:, 1])\n    auc_v = roc_auc_score(y_v, clf.predict_proba(X_v)[:, 1])","95caf657":"#comment: t\u1ec9 l\u1ec7 v\u1eabn v\u1eady, k thay \u0111\u1ed5i nhi\u1ec1u -> drop\ntrain_df.drop('Region_Code', axis=1, inplace = True)\ntest_df.drop('Region_Code', axis=1, inplace = True)","b02c50c1":"# # Explore 'Response'","67b1cb14":"# Heat Map","72a27023":"# Model training","76e596c1":"#comment: Nothing change --> drop\ntrain_df.drop('Driving_License', axis =1 ,inplace = True)\ntest_df.drop('Driving_License', axis =1 ,inplace = True)","84835831":"\nauc = cross_validation(\n    clf=XGBClassifier(\n        n_estimators=1000,\n        random_state=0,\n        n_jobs=mp.cpu_count(),\n        eval_metric=\"auc\",\n    ),\n    features=features,\n)\nauc","38e7e023":"clf\n","5a1ce266":"# Categorical analyst","d1813533":"pbounds = {\n    \"num_features\": (len(features)\/\/4, len(features)\/\/2),\n    \"n_estimators\": (100, 200),\n    \"max_depth\": (len(features)\/\/8, len(features)\/\/4),\n    \"reg_alpha\": (0.0, 1.0),\n    \"reg_lambda\": (0.0, 1.0),\n}\n\noptimizer = BayesianOptimization(\n    f=objective,\n    pbounds=pbounds,\n    random_state=0,\n)\n\noptimizer.maximize(n_iter=50)","4d89d1ee":"FEATURE IMPORTANCE FROM XG BOOST\n\n","ecccd572":"# Numerical analyst","ad560e5a":"# Check data"}}