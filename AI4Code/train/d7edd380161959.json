{"cell_type":{"0d632489":"code","e7adcad0":"code","58260eab":"code","c3268a1c":"code","6b7e618d":"code","dee2db11":"code","532627e8":"code","b0424fc9":"code","4c035d27":"code","5e7cce1c":"code","438f9bc6":"code","0df832e3":"code","6eb11257":"code","1bf34b0c":"code","cea0cf93":"code","08c074b6":"code","e9b3e791":"code","3babea55":"code","f626f42c":"code","e5d83518":"code","7c0f3e7d":"code","68dd470e":"code","151f105e":"code","64fab4a3":"code","0c03c2e8":"code","a5451a68":"code","fce8ff20":"code","0c1ea4e5":"code","430a2284":"code","964ae866":"markdown","7d853d56":"markdown","4dc4f812":"markdown","864ab336":"markdown","7db744db":"markdown","380ff010":"markdown","fb6b02b9":"markdown","f4bc4773":"markdown","54a13921":"markdown","ad847adf":"markdown","055af1a3":"markdown","c1169bee":"markdown","04a9b195":"markdown","00ed013e":"markdown","2b72c2b2":"markdown"},"source":{"0d632489":"#Libraries to be imported\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport warnings\nfrom sklearn.utils import check_array \nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)","e7adcad0":"df_sample_sub = pd.read_csv(\"..\/input\/ltfs-2020\/sample_submission_IIzFVsf.csv\")\ndf_test = pd.read_csv(\"..\/input\/ltfs-2020\/test_1eLl9Yf.csv\")\ndf_train = pd.read_csv(\"..\/input\/ltfs-2020\/train_fwYjLYX.csv\")","58260eab":"display(df_train.info())\ndisplay(df_train.head())\ndisplay(df_train.describe())","c3268a1c":"print('Minimum date from training set: {}'.format(pd.to_datetime(df_train.application_date.min()).date()))\nprint('Maximum date from training set: {}'.format(pd.to_datetime(df_train.application_date.max()).date()))","6b7e618d":"max_date_train = pd.to_datetime(df_train.application_date.max()).date()\nmax_date_test = pd.to_datetime(df_test.application_date.max()).date()\nlag_size = (max_date_test - max_date_train).days\nprint('Maximum date from training set: {}'.format(max_date_train))\nprint('Maximum date from test set: {}'.format(max_date_test))\nprint('Forecast Lag: {}'.format(lag_size))","dee2db11":"daily_cases_1 = df_train[df_train['segment'] == 1].groupby(['branch_id','state','zone','application_date'], as_index = False)['case_count'].sum()\ndaily_cases_2 = df_train[df_train['segment'] == 2].groupby(['state','application_date'], as_index = False)['case_count'].sum()","532627e8":"daily_cases_1_sc = []\nfor state in daily_cases_1['state'].unique():\n    current_daily_cases_1 = daily_cases_1[daily_cases_1['state'] == state]\n    daily_cases_1_sc.append(go.Scatter(x=current_daily_cases_1['application_date'], y=current_daily_cases_1['case_count'], name=('%s' % state)))\n\nlayout = go.Layout(title='Daily Case Count - Segment 1', xaxis=dict(title='Date'), yaxis=dict(title='Case Count'))\nfig = go.Figure(data=daily_cases_1_sc, layout=layout)\niplot(fig)","b0424fc9":"daily_cases_2_sc = []\nfor state in daily_cases_2['state'].unique():\n    current_daily_cases_2 = daily_cases_2[daily_cases_2['state'] == state]\n    daily_cases_2_sc.append(go.Scatter(x=current_daily_cases_2['application_date'], y=current_daily_cases_2['case_count'], name=('%s' % state)))\n\nlayout = go.Layout(title='Daily Case Count - Segment 2', xaxis=dict(title='Date'), yaxis=dict(title='Case Count'))\nfig = go.Figure(data=daily_cases_2_sc, layout=layout)\niplot(fig)","4c035d27":"df_train['application_date'] = pd.to_datetime(df_train['application_date'])\ndf_train = df_train.sort_values('application_date').groupby(['application_date','segment'], as_index=False)\ndf_train = df_train.agg({'case_count':['sum']})\ndf_train.columns = ['application_date','segment', 'case_count']\ndf_train.head()","5e7cce1c":"def series_to_supervised(data, window=1, lag=1, dropnan = True):\n    cols, names = list(), list()\n    #Input Sequence (t-n, ... t-1)\n    for i in range(window-1,0,-1):\n        cols.append(data.shift(i))\n        names+=[('%s(t-%d)' % (col,i)) for col in data.columns]\n    #Current Timestamp (t=0)\n    cols.append(data)\n    names+=[('%s(t)' % (col)) for col in data.columns]\n    #Target Timestamp (t=lag)\n    cols.append(data.shift(-lag))\n    names+=[('%s(t+%d)' %  (col,lag)) for col in data.columns]\n    #Put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # Drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","438f9bc6":"df_train_1 = df_train[df_train['segment'] == 1] #Segment 1 records\ndf_train_2 = df_train[df_train['segment'] == 2] #Segment 2 records","0df832e3":"window = 30 #use the last 30 days\nlag = 1 #predict the next day\nseries = series_to_supervised(df_train_1.drop(['application_date','segment'], axis=1), window=window, lag=lag)\nseries.head()","6eb11257":"df_train_1.case_count[:31]","1bf34b0c":"def train_test_split(data, n_test):\n    return data[:-n_test], data[-n_test:]","cea0cf93":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","08c074b6":"def model_fit(train, config):\n    # unpack config\n    n_input, n_nodes, n_epochs, n_batch = config\n    df = series_to_supervised(train, window=n_input)\n    data = df.to_numpy()\n    train_x, train_y = data[:, :-1], data[:, -1]\n    train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n    # define model\n    model = Sequential()\n    model.add(LSTM(n_nodes, activation='relu', input_shape=(n_input, 1)))\n    model.add(Dense(n_nodes, activation='relu'))\n    model.add(Dense(1))\n    model.compile(loss='mse', optimizer='adam')\n    # fit\n    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n    return model","e9b3e791":"def model_predict(model, history, config):\n    # unpack config\n    window, _, _, _ = config\n    x_input = np.array(history[-window:]).reshape((1, window, 1))\n    # forecast\n    yhat = model.predict(x_input, verbose=0)\n    return yhat[0]","3babea55":"#walk-forward validation for univariate data\ndef walk_forward_validation(data, n_test, cfg):\n    predictions = list()\n    # split dataset\n    train, test = train_test_split(data, n_test)\n    # fit model\n    model = model_fit(train, cfg)\n    # seed history with training dataset\n    history = [x for x in train.to_numpy()]\n    test = test.to_numpy()\n    # step over each time-step in the test set\n    for i in range(len(test)):\n        # fit model and make forecast for history\n        yhat = model_predict(model, history, cfg)\n        # store forecast in list of predictions\n        predictions.append(yhat)\n        # add actual observation to history for the next loop\n        history.append(test[i])\n    # estimate prediction error\n    error = mean_absolute_percentage_error(test, predictions)\n    print(' > %.3f' % error)\n    return error","f626f42c":"# repeat evaluation of a config\ndef repeat_evaluate(data, config, n_test, n_repeats=30):\n    # fit and evaluate the model n times\n    scores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n    return scores","e5d83518":"# summarize model performance\ndef summarize_scores(name, scores):\n    # print a summary\n    scores_m, score_std = np.mean(scores), np.std(scores)\n    print('%s: %.3f MAPE (+\/- %.3f)' % (name, scores_m, score_std))\n    # box and whisker plot\n    plt.boxplot(scores)\n    plt.show()","7c0f3e7d":"print('Date Range for Segment 1: {} days'.format((df_train_1.application_date.max() - df_train_1.application_date.min()).days))\nprint('Date Range for Segment 2: {} days'.format((df_train_2.application_date.max() - df_train_2.application_date.min()).days))","68dd470e":"config = [30, 50, 100, 100]","151f105e":"#Training segment 1 data\nn_test = 225\ndf_1 = df_train_1.drop(['segment','application_date'], axis=1)\nscores = repeat_evaluate(df_1, config, n_test)\nsummarize_scores('LSTM', scores)","64fab4a3":"#Training segment 2 data\nn_test = 243\ndf_2 = df_train_2.drop(['segment','application_date'], axis=1)\nscores = repeat_evaluate(df_2, config, n_test)\nsummarize_scores('LSTM', scores)","0c03c2e8":"def train_model(data, config):\n    n_input, n_nodes, n_epochs, n_batch = config\n    df = series_to_supervised(data, window=n_input)\n    data = df.to_numpy()\n    train_x, train_y = data[:, :-1], data[:, -1]\n    train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n    # define model\n    model = Sequential()\n    model.add(LSTM(n_nodes, activation='relu', input_shape=(window, 1)))\n    model.add(Dense(n_nodes, activation='relu'))\n    model.add(Dense(1))\n    model.compile(loss='mse', optimizer='adam')\n    # fit\n    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n    return model","a5451a68":"def time_series_forecasting(train, test, segment, config):\n    #Drop the unwanted columns\n    df = train.drop(['segment','application_date'], axis=1)\n    #Get the window\n    window = config[0]\n    #Train the model\n    model = train_model(df, config)\n    #Define the history to be taken into consideration for prediction\n    history = [x for x in df.to_numpy()]\n    #Get the records of the specified segment\n    test_seg = test[test['segment'] == segment]\n    #Define an empty case_count column to be inserted later on\n    cases = pd.Series([])\n    #One by one do prediction and append it to history and the series.\n    for i in range(test.shape[0]):\n        x_input = np.array(history[-window:]).reshape((1, window, 1))\n        y = model.predict(x_input, verbose=0)\n        history.append(y[0])\n        cases[i] = round((y[0][0]),0) #Since number of cases are supposed to be integer\n    #Add the calculated column to the dataset\n    test_seg.insert(loc=3, column='case_count', value=cases)\n    return test_seg","fce8ff20":"test1 = time_series_forecasting(df_train_1, df_test, 1, config)\ntest2 = time_series_forecasting(df_train_2, df_test, 2, config)","0c1ea4e5":"submit = pd.concat([test1, test2], ignore_index=True)","430a2284":"submit.to_csv('csv_to_submit.csv', index = False)","964ae866":"## Summarize Scores\nFinally, we need to summarize the performance of a model from the multiple repeats.\n\nWe will summarize the performance first using summary statistics, specifically the mean and the standard deviation.\n\nWe will also plot the distribution of model performance scores using a box and whisker plot to help get an idea of the spread of performance.\n\nThe summarize_scores() function below implements this, taking the name of the model that was evaluated and the list of scores from each repeated evaluation, printing the summary and showing a plot.","7d853d56":"## Repeat Evaluate\nNeural network models are stochastic.\n\nThis means that, given the same model configuration and the same training dataset, a different internal set of weights will result each time the model is trained that will in turn have a different performance.\n\nThis is a benefit, allowing the model to be adaptive and find high performing configurations to complex problems.\n\nIt is also a problem when evaluating the performance of a model and in choosing a final model to use to make predictions.\n\nTo address model evaluation, we will evaluate a model configuration multiple times via walk-forward validation and report the error as the average error across each evaluation.\n\nThis is not always possible for large neural networks and may only make sense for small networks that can be fit in minutes or hours.\n\nThe repeat_evaluate() function below implements this and allows the number of repeats to be specified as an optional parameter that defaults to 30 and returns a list of model performance scores: in this case, MAPE values.","4dc4f812":"Below are the first 31 values of case count column. In the above table it can be seen that the first row values are exactly similar. So, in a way we have rearranged the data so that the model learns to take in the input of the first 30 days and returns the output i.e for the 31st day. It can be taken as the label for the supervised learning problem.","864ab336":"# Data Preprocessing\nRearrange the dataset so that we can apply shift methods. Since, the prediction is to be done on country level, we just need to group by the data on date and segment and get rid of all other columns(state, branch_id, e.t.c). Also, the aggregation applied is sum so that we get the total number of cases throughout the country for that particular day and segment.","7db744db":"## Training the Model\nThe model_fit() function for fitting an LSTM model is provided below.\n\nThe model expects a list of four model hyperparameters; they are:\n\n- **n_input**: The number of lag observations to use as input to the model.\n- **n_nodes**: The number of LSTM units to use in the hidden layer.\n- **n_epochs**: The number of times to expose the model to the whole training dataset.\n- **n_batch**: The number of samples within an epoch after which the weights are updated.\nA single input must have the three-dimensional structure of samples, timesteps, and features, which in this case we only have 1 sample and 1 feature: [1, n_input, 1].","380ff010":"## Walk Forward Validation\nTime series forecasting models can be evaluated on a test set using walk-forward validation.\n\nWalk-forward validation is an approach where the model makes a forecast for each observation in the test dataset one at a time. After each forecast is made for a time step in the test dataset, the true observation for the forecast is added to the test dataset and made available to the model.\n\nFirst, the dataset is split into train and test sets. We will call the train_test_split() function to perform this split and pass in the pre-specified number of observations to use as the test data.\n\nA model will be fit once on the training dataset for a given configuration. Each time step of the test dataset is enumerated. A prediction is made using the fit model.\n\nThe prediction is added to a list of predictions and the true observation from the test set is added to a list of observations that was seeded with all observations from the training dataset. This list is built up during each step in the walk-forward validation, allowing the model to make a one-step prediction using the most recent history.\n\nAll of the predictions can then be compared to the true values in the test set and an error measure calculated.\n\nWe will calculate the mean absolute percentage error, or MAPE, between predictions and the true values.","fb6b02b9":"## Predicting the next day case count\nThe model_predict() function takes in the trained model, the history(last 30 days case counts in our case) and the configuration as arguments and returns the next day case count.","f4bc4773":"## This submission was done for a competition. I will keep on additing more models and even using Hyperparameter tuning to get better predictions on the test set. Please upvote if you liked the notebook.","54a13921":"> Plots for both the business segments: 1 & 2, have a seasonal component but no trend component.","ad847adf":"Here's a working example of the series_to_supervised() method. It will take in the counts of last 30 days as input <t-29, t-28, ..., t> and give the output for (t+1)th day.","055af1a3":"## Train-test split\nThe train_test_split() function below will split the series taking the raw observations and the number of observations to use in the test set as arguments","c1169bee":"# Time Series Forecasting using LSTMs\n![image.png](attachment:image.png)\n\nLTFS receives a lot of requests for its various finance offerings that include housing loan, two-wheeler loan, real estate financing and micro loans. The number of applications received is something that varies a lot with season. Going through these applications is a manual process and is tedious. Accurately forecasting the number of cases received can help with resource and manpower management resulting into quick response on applications and more efficient processing.\n\nOur task is to forecast daily cases for next 3 months for 2 different business segments aggregated at the country level keeping in consideration the following major Indian festivals (inclusive but not exhaustive list): Diwali, Dussehra, Ganesh Chaturthi, Navratri, Holi etc.","04a9b195":"## Convert to a Supervised Learning problem\nThis seems to be a *Univariate Time series forecasting* problem. Next, we need to be able to frame the univariate series of observations as a supervised learning problem so that we can train neural network models. A supervised learning framing of a series means that the data needs to be split into multiple examples that the model learn from and generalize across.Each sample must have both an input component and an output component.\n\nThe input component(window) will be some number of prior observations, such as 30 days or time steps. The output component will be the total case counts in the next day because we are interested in developing a model to make one-step forecasts.","00ed013e":"# EDA\nIn this section I have performed some basic exploratory data analysis on the training dataset using plotly library. The train data has been provided in the following way:\n- For business segment 1, historical data has been made available at branch ID level\n- For business segment 2, historical data has been made available at State level.","2b72c2b2":"## Evaluation Metric\nI will be using Mean Absolute Percentage Error(MAPE) as my metric for evaluation.\n![image.png](attachment:image.png)\nWhere At is the actual value and Ft is the forecast value."}}