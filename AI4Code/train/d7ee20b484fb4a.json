{"cell_type":{"33360c23":"code","8a80e75e":"code","12265913":"code","7d60fabd":"code","3d4100de":"code","e637e605":"code","57646643":"code","b9eafb19":"code","6ff849b8":"code","78499960":"code","5bdb2337":"code","75d841b5":"code","3526ecda":"code","36164a3f":"code","db723eaa":"code","14594d67":"code","59b4d82a":"code","38261228":"code","8374215a":"code","efa489a6":"code","8680e889":"code","b44afda7":"code","0e8c6d03":"code","afa5438c":"code","35aeb8ce":"markdown"},"source":{"33360c23":"!pip install utils","8a80e75e":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\n\nimport gc\ngc.collect()","12265913":"!pip install pretrainedmodels\n\n%matplotlib inline\n\n!pip install fastai==1.0.52\nimport fastai\n\nfrom fastai import *\n#from fastai.vision import *\nfrom fastai.text import *\n\n#from torchvision.models import *\nimport pretrainedmodels\n\nfrom utils import *\nimport sys\n\nfrom fastai.callbacks import *\nfrom fastai.callbacks.tracker import EarlyStoppingCallback\nfrom fastai.callbacks.tracker import SaveModelCallback","7d60fabd":"%%bash\npip install pytorch-pretrained-bert","3d4100de":"from pytorch_pretrained_bert import BertTokenizer\nbert_tok = BertTokenizer.from_pretrained(\n    \"bert-base-uncased\",\n)","e637e605":"train = pd.read_csv(\"..\/input\/idayhack\/IDay\/train.csv\")\ntest = pd.read_csv(\"..\/input\/idayhack\/IDay\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/idayhack\/IDay\/sample_submission.csv\")","57646643":"class FastAiBertTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length\"\"\"\n        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]","b9eafb19":"fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))","6ff849b8":"fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=256), pre_rules=[], post_rules=[])","78499960":"label_cols = [\"Computer Science\", \"Physics\", \"Mathematics\", \"Statistics\", \"Quantitative Biology\", \"Quantitative Finance\"]","5bdb2337":"class BertTokenizeProcessor(TokenizeProcessor):\n    def __init__(self, tokenizer):\n        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n\nclass BertNumericalizeProcessor(NumericalizeProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n\ndef get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n    \"\"\"\n    Constructing preprocessors for BERT\n    We remove sos\/eos tokens since we add that ourselves in the tokenizer.\n    We also use a custom vocabulary to match the numericalization with the original BERT model.\n    \"\"\"\n    return [BertTokenizeProcessor(tokenizer=tokenizer),\n            NumericalizeProcessor(vocab=vocab)]","75d841b5":"class BertDataBunch(TextDataBunch):\n    @classmethod\n    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n                label_cols:IntsOrStrs=0, label_delim:str=None, **kwargs) -> DataBunch:\n        \"Create a `TextDataBunch` from DataFrames.\"\n        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n        # use our custom processors while taking tokenizer and vocab as kwargs\n        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n        return src.databunch(**kwargs)","3526ecda":"path=Path('.')","36164a3f":"# this will produce a virtually identical databunch to the code above\n'''databunch_2 = BertDataBunch.from_df(\".\", train_df=train, valid_df=val,\n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  text_cols=[\"TITLE\", \"ABSTRACT\"],\n                  label_cols=label_cols,\n                  bs=32,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )'''","db723eaa":"from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification, BertForNextSentencePrediction, BertForMaskedLM\nbert_model_class = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)","14594d67":"loss_func = nn.BCEWithLogitsLoss()","59b4d82a":"acc_02 = partial(accuracy_thresh, thresh=0.25)","38261228":"model = bert_model_class","8374215a":"def bert_clas_split(self) -> List[nn.Module]:\n    \n    bert = model.bert\n    embedder = bert.embeddings\n    pooler = bert.pooler\n    encoder = bert.encoder\n    classifier = [model.dropout, model.classifier]\n    n = len(encoder.layer)\/\/3\n    print(n)\n    groups = [[embedder], list(encoder.layer[:n]), list(encoder.layer[n+1:2*n]), list(encoder.layer[(2*n)+1:]), [pooler], classifier]\n    return groups","efa489a6":"kf = KFold(n_splits = 5)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X = train)):\n  print(\"Fold #\", fold)\n  x_train, x_valid = train.loc[train_idx], train.loc[val_idx]\n\n  databunch_1 = TextDataBunch.from_df(\".\", x_train, x_valid, \n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=[\"TITLE\", \"ABSTRACT\"],\n                  label_cols=label_cols,\n                  bs=32,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n\n  learner = Learner(\n    databunch_1, model,\n    loss_func=loss_func, model_dir='\/content', metrics=acc_02,)\n  \n  x = bert_clas_split(model)\n  learner.split([x[0], x[1], x[2], x[3], x[5]])\n\n  #learner.lr_find()\n  #learner.recorder.plot()\n\n  gc.collect()\n  torch.cuda.empty_cache()\n\n  print(\"Fitting Base version\")\n  learner.fit_one_cycle(2, max_lr=slice(1e-5, 5e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))\n  learner.save(f'Base-Fold{fold}')\n  learner.load(f'Base-Fold{fold}')\n\n  gc.collect()\n  torch.cuda.empty_cache()\n\n  print(\"Running Top 2 layers unfreeze\")\n  learner.freeze_to(-2)\n  learner.fit_one_cycle(2, max_lr=slice(1e-5, 5e-4), \n                        moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))\n\n  learner.save(f'Head-2-Fold{fold}')\n  learner.load(f'Head-2-Fold{fold}')\n\n  gc.collect()\n  torch.cuda.empty_cache()\n\n  '''print(\"Learning all layers\")\n  learner.unfreeze()\n  learner.lr_find()\n  learner.recorder.plot(suggestion=True)'''\n\n  gc.collect()\n  torch.cuda.empty_cache()\n\n  print(\"Fittting all layers\")\n  learner.fit_one_cycle(2, slice(5e-6, 5e-5), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","8680e889":"gc.collect()\ntorch.cuda.empty_cache()","b44afda7":"preds = []\nfor i in range(len(test.ABSTRACT)):\n  pred = learner.predict(test.iloc[i][['TITLE', 'ABSTRACT']])[1].numpy()\n  preds.append(pred)","0e8c6d03":"submission[['Computer Science',\t'Physics',\t'Mathematics',\t'Statistics',\t'Quantitative Biology',\t'Quantitative Finance']] = preds","afa5438c":"submission.to_csv('sub.csv', index =False)","35aeb8ce":"This is my solution to Analytics Vidhya Hackathon - [Problem link](https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-independence-day-2020-ml-hackathon\/#LeaderBoard)\n\n**Topic Modeling for Research Articles**\n\nResearchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n\nGiven the abstract and title for a set of research articles, predict the topics for each article included in the test set. \n\nNote that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: \n\n1. Computer Science\n\n2. Physics\n\n3. Mathematics\n\n4. Statistics\n\n5. Quantitative Biology\n\n6. Quantitative Finance"}}