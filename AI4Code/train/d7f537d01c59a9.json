{"cell_type":{"96d4ed20":"code","94be4df7":"code","f388009c":"code","76532e19":"code","1842e41a":"code","89bac170":"code","50c7e4e7":"code","a6f6c8bc":"code","5d3b29eb":"code","b12786a7":"code","04bc9efc":"code","56811732":"code","1fdd8019":"code","4e8c08dc":"code","5addcb31":"code","c3e921f6":"code","4925be17":"code","4ce31518":"code","8fc7f001":"code","1daff1be":"code","371b46d5":"code","f529ecd4":"code","8c4b0a60":"code","03b872a8":"code","169f9fa7":"code","c53fb03b":"code","6e5fef0e":"markdown","d8db4c05":"markdown","29d23591":"markdown","e1969658":"markdown","d050077b":"markdown","e784d5ac":"markdown"},"source":{"96d4ed20":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport seaborn as sn\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport gc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nimport sklearn.model_selection as skt\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC  \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean\nfrom numpy import std\nfrom matplotlib import pyplot\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\n%matplotlib inline\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\npd.set_option('display.max_rows', 1100)\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","94be4df7":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest  = pd.read_csv('..\/input\/titanic\/test.csv')\ngender_submission  = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","f388009c":"train.head()","76532e19":"test.head()","1842e41a":"#Check if any of the passengers in the train set is present in the test set\n#No passengers are present both in train and test, so we have to exclude this column from both sets\ntrain['booking'] = train.Name.str.split(',',expand = True)[0]\ntest['booking'] = test.Name.str.split(',',expand = True)[0]\nprint('Number of passengers in both train and test sets ' + str(sum(train['PassengerId'].isin(test['PassengerId']))))\nprint('Number of bookings in both train and test sets '   + str(sum(train['booking'].isin(test['booking']))))","89bac170":"#Checking duplicates\nprint('Number of duplicated passengers in train set ' + str(sum(train['PassengerId'].duplicated())))\nprint('Number of duplicated passengers in test set ' + str(sum(test['PassengerId'].duplicated())))","50c7e4e7":"def age_cat(x):\n    if (x <= 15): return(0)\n    elif(x <= 30):return(1)\n    elif (x <= 40):return(2)\n    elif(x <= 50):return(3)\n    elif(x <= 60):return(4)\n    elif(x <= 80):return(5) #Max Age\n    else : return(6) #Age not known ","a6f6c8bc":"#We plot some charts to better understand the data\nfig, axes = plt.subplots(2, 3, sharex=False, sharey=True, figsize=(20, 10))\n\n#Survival Rate per Gender\nsurv_gender  = train.groupby(['Sex'],as_index = False).agg({'Survived':'sum','PassengerId':'count'})\nsurv_gender['surv_rate']  = surv_gender['Survived'] \/ surv_gender['PassengerId']\naxes[0,0].set_ylabel('Survival Rate')\naxes[0,0].set_title('Survival Rate per Gender')\naxes[0,0].bar(surv_gender['Sex'],surv_gender['surv_rate'],width = 0.5,color = ['pink','darkblue'])\n\n#Survival Rate per Ticket Class\nsurv_pclass  = train.groupby(['Pclass'],as_index = False).agg({'Survived':'sum','PassengerId':'count'})\nsurv_pclass['surv_rate']  = surv_pclass['Survived'] \/ surv_pclass['PassengerId']\nlabels = ['Upper','Middle','Lower']\naxes[0,1].set_title('Survival Rate per Ticket Class')\naxes[0,1].set_xticks([1,2,3])\naxes[0,1].set_xticklabels(labels)\naxes[0,1].bar(surv_pclass['Pclass'],surv_pclass['surv_rate'],width = 0.5,color = ['red','darkblue','green'])\n\n#Survival Rate per Age Category\ntrain['age_cat'] = train['Age'].apply(lambda x : age_cat(x))\nsurv_age  = train.groupby(['age_cat'],as_index = False).agg({'Survived':'sum','PassengerId':'count'})\nsurv_age['surv_rate']  = surv_age['Survived'] \/ surv_age['PassengerId']\naxes[0,2].set_title('Survival Rate per Age Category')\naxes[0,2].bar(surv_age['age_cat'],surv_age['surv_rate'],width = 0.5)\naxes[0,2].set_xticks([0,1,2,3,4,5,6])\naxes[0,2].set_xticklabels(['<15','<30','<40','<50','<60','<=80','NA',])\n\n#Survival Rate per Cabin Category\ntrain['Cabin'].fillna(\"\",inplace = True)\ntrain['cabin_cat']  = train['Cabin'].astype(str).str[0]\ntrain['cabin_cat'].fillna(\"NA\",inplace = True)\nsurv_cabin  = train.groupby(['cabin_cat'],as_index = False).agg({'Survived':'sum','PassengerId':'count'})\nsurv_cabin['surv_rate'] = surv_cabin['Survived'] \/ surv_cabin['PassengerId']\naxes[1,0].bar(surv_cabin['cabin_cat'],surv_cabin['surv_rate'],width = 0.5)\naxes[1,0].set_title('Survival Rate per Cabin Category')\n\n#Survival Rate per Port of Embarkation\nsurv_embark  = train.groupby(['Embarked'],as_index = False).agg({'Survived':'sum','PassengerId':'count'})\nsurv_embark['surv_rate']  = surv_embark['Survived']\/surv_embark['PassengerId']\naxes[1,1].set_title('Survival Rate per Port of Embarkation')\naxes[1,1].bar(surv_embark['Embarked'],surv_embark['surv_rate'],width = 0.5,color = ['red','darkblue','green'])\naxes[1,1].set_xticks(['C','Q','S'])\naxes[1,1].set_xticklabels(['Cherbourg','Queenstown','Southampton'])\n\n#Survival Rate per Ticket and Booking counts\nbooking_count = train.groupby(['booking'],as_index = False).agg({'PassengerId':'count'})\nbooking_count.rename(columns = {'PassengerId':'booking_cnt'},inplace = True)\nticket_count = train.groupby(['Ticket'],as_index = False).agg({'PassengerId':'count'})\nticket_count.rename(columns = {'PassengerId':'ticket_cnt'},inplace = True)\ntrain = pd.merge(train,booking_count, how = 'left', on = ['booking'])\ntrain = pd.merge(train,ticket_count, how = 'left', on = ['Ticket'])\nsurv_bsize = train.groupby(['booking_cnt'],as_index = False).agg({'Survived':'sum','PassengerId':'count'})\nsurv_bsize['surv_rate']  = surv_bsize['Survived']\/surv_bsize['PassengerId']\n#surv_tsize = train.groupby(['ticket_cnt'],as_index = False).agg({'Survived':'sum','PassengerId':'count'})\n#surv_tsize['surv_rate']  = surv_tsize['Survived']\/surv_tsize['PassengerId']\naxes[1,2].bar(surv_bsize['booking_cnt'],surv_bsize['surv_rate'],width = 0.5)\naxes[1,2].set_title('Survival Rate per Booking Size (#of Passengers)')\naxes[1,2].set_xticks([1,2,3,4,5,6,7,8,9])\n\nplt.subplots_adjust(wspace=0)\n\ntrain.drop(['booking_cnt','ticket_cnt'],axis =1, inplace = True)\ndel surv_gender,surv_pclass,surv_age,surv_cabin,surv_embark,surv_bsize\ngc.collect()","5d3b29eb":"#Corrletion Matrix\ncorrMatrix = train.corr()\nplt.subplots(figsize=(11, 9))\nsn.heatmap(corrMatrix, annot=True)\nplt.show()","b12786a7":"#Keep track of slice index\ntrain_idx = train.shape[0]\n#We concatenate train and test dataset to apply unique transformations\ndf_all = pd.concat([train,test],ignore_index = True)\n#Adding Basic Features\ndf_all['age_cat'] = df_all['Age'].apply(lambda x : age_cat(x))\ndf_all['Cabin'].fillna(\"\",inplace = True)\ndf_all['cabin_cat']  = df_all['Cabin'].astype(str).str[0]\ndf_all['cabin_cat'].fillna(\"NA\",inplace = True)\ndf_all['familly_size'] = df_all['SibSp'] + df_all['Parch']\n#Bin the fare column\nbins = [0,7,15,25,40,55,70,100,300,500]\ndf_all['fare_bin'] = np.digitize(df_all.Fare.values, bins=bins)\n#Group by booking.ticket to get group size\nbooking_count = df_all.groupby(['booking'],as_index = False).agg({'PassengerId':'count'})\nbooking_count.rename(columns = {'PassengerId':'booking_cnt'},inplace = True)\nticket_count = df_all.groupby(['Ticket'],as_index = False).agg({'PassengerId':'count'})\nticket_count.rename(columns = {'PassengerId':'ticket_cnt'},inplace = True)\ndf_all = pd.merge(df_all,booking_count, how = 'left', on = ['booking'])\ndf_all = pd.merge(df_all,ticket_count, how = 'left', on = ['Ticket'])","04bc9efc":"df_all['pclass_target'] = 0\ndf_all.columns.get_loc('pclass_target')\ndf_all.columns","56811732":"#Add some mean\/target\/liklyhood encodings\ndef cvloop_encoder(df,target,feature,cv = 5,nafill = 0):\n   \n    nafill = np.mean(df.groupby(feature)[target].mean())\n    df[feature +str('_target')] = 0\n    y_tr  = df[target].values\n    kf = skt.KFold(n_splits = cv, shuffle = False)\n    for tr_ind, val_ind in kf.split(y_tr):\n        X_tr  = df.iloc[tr_ind]\n        X_val = df.iloc[val_ind]\n        means = X_val[feature].map(X_tr.groupby(feature)[target].mean())\n        col_idx = df.columns.get_loc(feature +str('_target'))\n        df.iloc[val_ind,col_idx] = means\n\n    df[feature +str('_target')].fillna(nafill, inplace=True) \n    return(df)","1fdd8019":"#Mean Encoding with CV loop regularization\n\ndf_all = cvloop_encoder(df_all,'Survived','cabin_cat',cv = 8)\ndf_all = cvloop_encoder(df_all,'Survived','Pclass',cv = 8)\ndf_all = cvloop_encoder(df_all,'Survived','age_cat',cv = 8)\ndf_all = cvloop_encoder(df_all,'Survived','booking',cv = 8)\ndf_all = cvloop_encoder(df_all,'Survived','Embarked',cv = 8)\ndf_all = cvloop_encoder(df_all,'Survived','Sex',cv = 8)\ndf_all = cvloop_encoder(df_all,'Survived','Ticket',cv = 8)\ndf_all = cvloop_encoder(df_all,'Survived','familly_size',cv = 8)","4e8c08dc":"df_all['gender_pclass'] = df_all['Sex'] + df_all['Pclass'].astype(str)\ndf_all = cvloop_encoder(df_all,'Survived','gender_pclass',cv = 8)\ndf_all.drop('gender_pclass', axis = 1, inplace =True)\n\ndf_all['gender_cabin'] = df_all['Sex'] + df_all['cabin_cat'].astype(str)\ndf_all = cvloop_encoder(df_all,'Survived','gender_cabin',cv = 8)\ndf_all.drop('gender_cabin', axis = 1, inplace =True)\n\ndf_all.fillna(method = \"backfill\",inplace = True)","5addcb31":"#We encode categorical features by survival rate\ncabin_dict = {\"D\": 1,\"E\": 2,\"E\": 3,\"B\": 4,\"F\": 5,\"C\": 6,\"G\": 7,\"A\": 8,\"NA\": 9,\"T\" : 10}\ndf_all['cabin_cat'] = df_all.cabin_cat.apply(lambda x : cabin_dict[x])\n\ngender_dict ={\"female\":1,\"male\":2}\ndf_all['Sex'] = df_all.Sex.apply(lambda x : gender_dict[x])\n\nport_dict ={\"C\":1,\"Q\":2,\"S\":3}\ndf_all['Embarked'] = df_all.Embarked.apply(lambda x : port_dict[x])","c3e921f6":"#One_Hot Encoding , I choose to use it to be able to fit lenear models as well\ndrop_cols = ['PassengerId','Name','Age','Ticket','Fare','Cabin','booking','SibSp','Parch']\ndf_all.drop(drop_cols,axis =1,inplace = True)\n#df_all = pd.get_dummies(df_all)","4925be17":"#Split train validation and test sets\nX_train_org  = df_all[df_all.index < train_idx].drop('Survived',axis =1)\ny_train_org  = df_all[df_all.index < train_idx]['Survived']\nX_test   = df_all[df_all.index >= train_idx].drop('Survived',axis =1)\nX_train, X_val, y_train, y_val = train_test_split(X_train_org, y_train_org, random_state=1609)","4ce31518":"#Linear model are not good with categorical features, so we hot encode them\ndf_all_enc = df_all.copy()\nencoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\nencoded_cols = ['Pclass','Sex','Embarked','age_cat','cabin_cat','fare_bin']\n\nfor i in encoded_cols:\n    df_all_enc = pd.concat([df_all_enc,pd.get_dummies(df_all_enc[i],prefix=i)],axis=1)\n    df_all_enc.drop(i, axis = 1, inplace=True)","8fc7f001":"#Generate encoded version more suitable for linear models\nX_train_org_enc  = df_all_enc[df_all_enc.index < train_idx].drop('Survived',axis =1)\nX_test_enc   = df_all_enc[df_all_enc.index >= train_idx].drop('Survived',axis =1)\nX_train_enc, X_val_enc, y_train, y_val = train_test_split(X_train_org_enc, y_train_org, random_state=1609)","1daff1be":"#GridSearch\nrf_params = {'n_estimators': [100,200,300,400],\n             'max_depth':[5,6,7,8]}\nrf = RandomForestClassifier(random_state = 1609)\nrf_grid = GridSearchCV(rf,rf_params,scoring = 'accuracy',n_jobs = -1,verbose = False,cv = 10)\nrf_grid.fit(X_train,y_train)","371b46d5":"print(\"Train set score: {:.3f}\".format(rf_grid.score(X_train, y_train)))\nprint(\"Validation set score: {:.3f}\".format(rf_grid.score(X_val, y_val)))\nprint(\"Best parameters: {}\".format(rf_grid.best_params_))\nprint(\"Best cross-validation score: {:.3f}\".format(rf_grid.best_score_))","f529ecd4":"#SVM Model\npipe = Pipeline([\n        ('sc', StandardScaler()),     \n        ('svm', SVC())])\nsvm_params = {'svm__C': [0.1,1, 10, 100], 'svm__gamma': [1,0.1,0.01,0.001],'svm__kernel': ['rbf', 'poly', 'sigmoid']}\nsvm_grid = GridSearchCV(pipe,svm_params,verbose=2,cv = 10,n_jobs = -1)\nsvm_grid.fit(X_train_enc,y_train)\nprint(svm_grid.best_estimator_)    \nprint(\"Validation set score: {:.3f}\".format(svm_grid.score(X_val_enc, y_val)))\nprint(\"Best parameters: {}\".format(svm_grid.best_params_))\nprint(\"Best cross-validation score: {:.3f}\".format(svm_grid.best_score_))","8c4b0a60":"#KNN\npipe = Pipeline([\n        ('sc', StandardScaler()),     \n        ('knn', KNeighborsClassifier(algorithm='brute')) ])\nknn_params = {'knn__n_neighbors': [6,7,8,9,10,11,12,14,16,18,20,22],\n              'knn__weights' :    ['uniform', 'distance'],\n              'knn__leaf_size':    list(range(1,50,5))}\n\nknn_grid = GridSearchCV(pipe,knn_params,verbose=2,cv = 10,n_jobs = -1)\nknn_grid.fit(X_train_enc,y_train)\nprint(knn_grid.best_estimator_)    \nprint(\"Validation set score: {:.3f}\".format(knn_grid.score(X_val_enc, y_val)))\nprint(\"Best parameters: {}\".format(knn_grid.best_params_))\nprint(\"Best cross-validation score: {:.3f}\".format(knn_grid.best_score_))","03b872a8":"#Logistic Regression\npipe = Pipeline([\n        ('sc', StandardScaler()),     \n        ('logreg', LogisticRegression()) ])\nlog_params = {'logreg__C'      : np.logspace(-3,3,7),\n              'logreg__penalty':[\"l1\",\"l2\"]}\n\nlog_grid = GridSearchCV(pipe,log_params,verbose=2,cv = 10,n_jobs = -1)\nlog_grid.fit(X_train_enc,y_train)\nprint(log_grid.best_estimator_)    \nprint(\"Validation set score: {:.3f}\".format(log_grid.score(X_val_enc, y_val)))\nprint(\"Best parameters: {}\".format(log_grid.best_params_))\nprint(\"Best cross-validation score: {:.3f}\".format(log_grid.best_score_))","169f9fa7":"models = dict()\nmodels['rf']       = rf_grid\nmodels['svm']      = svm_grid\nmodels['knn']      = knn_grid\nmodels['log']      = log_grid\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    if (name == 'rf' ): \n        scores = cross_val_score(model, X_val, y_val, scoring='accuracy', n_jobs=-1, error_score='raise')\n    else : \n        scores = cross_val_score(model, X_val_enc, y_val, scoring='accuracy', n_jobs=-1, error_score='raise')\n\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\npyplot.subplots(figsize=(11, 9))\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","c53fb03b":"#Generate Predictions\n\n#Tree Model\npred_rf = rf_grid.predict(X_test).astype(int)\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': pred_rf})\noutput.to_csv('my_submission_RF.csv', index=False)","6e5fef0e":"# Model Training","d8db4c05":"**Mean Encoding**","29d23591":"# Exploratory Data Analysis","e1969658":"This is my second Notebook in Kaggle, and I prefered to start with the classical Titanic dataset. The used pipeline is :\n* EDA\n* Feature Engineering\n* Random Forrest Model Training\n* Other linear model training\n* Comparison and final submission.\n\n","d050077b":"# Features Engineering","e784d5ac":"**Basic Features**"}}