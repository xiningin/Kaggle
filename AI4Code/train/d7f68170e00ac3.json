{"cell_type":{"428c15c0":"code","1b16fb0b":"code","de28d337":"code","ebecc16f":"code","c1ff498f":"code","6b25a744":"code","4e656d69":"code","365eb905":"code","197910fd":"code","9d766760":"code","909402f5":"code","7b0b5627":"code","eb8cd8a9":"code","69bf0ead":"code","a95bcf93":"code","ce0c5e73":"code","ad366031":"code","7455dc94":"code","766eb57c":"code","09a512dc":"code","bf350cc1":"code","e43532fa":"code","79e47680":"code","ae7b2914":"code","9da11e6d":"code","9d3305f9":"code","acf249c8":"code","b80829d2":"code","9817d8c0":"code","2cd86332":"code","cbc93d2d":"code","7758d916":"code","1ecb053e":"code","3c24c7d8":"code","9bf5e157":"code","ba591f25":"code","46a9833c":"code","4357c01b":"code","518d9770":"code","15588bbe":"code","19ac339c":"code","85816e1e":"code","4d854035":"code","92d43739":"code","8f112316":"code","1f1500c9":"code","e59540e9":"code","f47805aa":"code","7dd88f01":"code","1fc44ebb":"code","171b62f6":"code","7d4a2456":"code","13f7ee59":"code","4fefe8bd":"code","2ecaeafa":"code","1939b1ed":"code","3a53defd":"code","8c822199":"code","8afd16a8":"code","502f46a7":"code","a2abba94":"code","d900a2bf":"code","c013412a":"code","6d4ecf27":"code","778078af":"code","774ae4fe":"code","f33d764b":"code","b53ea031":"code","38ccbe43":"code","4d4e5669":"code","e222e02e":"code","869bd92b":"code","b214d2d2":"code","540c45a0":"code","7ad518a7":"code","0a68cd34":"code","46d92859":"code","3513c4b7":"code","dd0ee90d":"code","0bd0b3c9":"code","8485ec44":"code","c3efb73d":"code","94985e4d":"code","a54850b9":"code","72e50e67":"code","52013e9b":"code","56dea1c0":"code","58525c1b":"code","634ffc86":"code","5108fc65":"code","14df4747":"code","191461e6":"code","ebaeb049":"code","1610b4b4":"code","1423cf4d":"code","2b7b5439":"code","3c5d7773":"code","585c5a5e":"code","404ef0ee":"code","b4116e54":"markdown","f029fd1f":"markdown","15e5f54d":"markdown","c192a468":"markdown","4bd96554":"markdown","b05ff515":"markdown","058f3907":"markdown","8d290df4":"markdown","26790fc0":"markdown","5933fc1d":"markdown","964513e8":"markdown","3a3e5331":"markdown","a552bf5e":"markdown","c6eaf2a7":"markdown","d86fcb8d":"markdown","35d8b1c7":"markdown","14f3cfd5":"markdown","20b7a3aa":"markdown","7513c931":"markdown","2f33f258":"markdown","e4e762bd":"markdown","3efb8307":"markdown","d2354fee":"markdown","30036d1f":"markdown","6526e2e1":"markdown","a708f1dc":"markdown","4d386667":"markdown","922e634d":"markdown","e57fc353":"markdown","1f752e89":"markdown","b91f3f87":"markdown","8b84df7c":"markdown","4896ce97":"markdown","8a5b7c69":"markdown","dcc8bf42":"markdown","2b3cfe5a":"markdown","d6dc919b":"markdown","f2f116a8":"markdown","124e576a":"markdown","50432815":"markdown","8bdeb973":"markdown","a47fdf5a":"markdown","3d380438":"markdown","160ec313":"markdown","ba5ed6be":"markdown","8d11a26a":"markdown","875abae3":"markdown","289d3a8c":"markdown","f5a98d97":"markdown","7f4972ef":"markdown","9289d99d":"markdown","67f056e7":"markdown","c86e3d89":"markdown","0d9098ab":"markdown","ec7d48f6":"markdown","16a4dc2d":"markdown","705d4a84":"markdown","2bcedebb":"markdown","6813147a":"markdown","3a97408d":"markdown","49997de2":"markdown","2c6db15f":"markdown","3a6181a5":"markdown","9e2f19e3":"markdown","01dc0081":"markdown","de2ddb53":"markdown","488a5b80":"markdown","8040aafb":"markdown","7b866950":"markdown","d393e681":"markdown","05c40465":"markdown","9987b627":"markdown","b38eb62d":"markdown","1199a630":"markdown","84a7eb90":"markdown","ba754646":"markdown","d6fdf8d7":"markdown","328f5f29":"markdown","c86772eb":"markdown","88a94c78":"markdown","aebb505d":"markdown","ccae9e24":"markdown","7b886290":"markdown","2c49d2e6":"markdown","d2e5c6a2":"markdown","4c363caf":"markdown","aaaf083c":"markdown","bac9ad77":"markdown"},"source":{"428c15c0":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\n%matplotlib inline ","1b16fb0b":"#Create a directory to save plots\n! mkdir plots","de28d337":"# Set the seed\nseed = 27912\nnp.random.seed(seed)","ebecc16f":"#Read data set\ndf = pd.read_csv(\"..\/input\/seattlecollisions\/Data-Collisions.csv\")","c1ff498f":"df.head()","6b25a744":"df.shape","4e656d69":"missing_data = df.isnull()\n\nfor column in missing_data.columns.values.tolist():\n    print(column)\n    print (missing_data[column].value_counts())\n    print(\"\")   ","365eb905":"df['ADDRTYPE'].value_counts()","197910fd":"df['ADDRTYPE'].value_counts().idxmax()","9d766760":"#replace the missing 'ADDRTYPE' values by the most frequent \ndf['ADDRTYPE'].replace(np.nan, 'Block', inplace=True)","909402f5":"df['ADDRTYPE'].value_counts()","7b0b5627":"#replace the missing 'COLLISIONTYPE' values by the most frequent\nmost_freq = df['COLLISIONTYPE'].value_counts().idxmax()\ndf['COLLISIONTYPE'].replace(np.nan, most_freq, inplace=True)","eb8cd8a9":"#replace the missing 'JUNCTIONTYPE' values by the most frequent\nmost_freq = df['JUNCTIONTYPE'].value_counts().idxmax()\ndf['JUNCTIONTYPE'].replace(np.nan, most_freq, inplace=True)","69bf0ead":"#replace the missing 'UNDERINFL' values by the most frequent\nmost_freq = df['UNDERINFL'].value_counts().idxmax()\ndf['UNDERINFL'].replace(np.nan, most_freq, inplace=True)","a95bcf93":"#replace the missing 'WEATHER' values by the most frequent\nmost_freq = df['WEATHER'].value_counts().idxmax()\ndf['WEATHER'].replace(np.nan, most_freq, inplace=True)","ce0c5e73":"#replace the missing 'ROADCOND' values by the most frequent\nmost_freq = df['ROADCOND'].value_counts().idxmax()\ndf['ROADCOND'].replace(np.nan, most_freq, inplace=True)","ad366031":"#replace the missing 'LIGHTCOND' values by the most frequent\nmost_freq = df['LIGHTCOND'].value_counts().idxmax()\ndf['LIGHTCOND'].replace(np.nan, most_freq, inplace=True)","7455dc94":"#drop the columns listed above\ndf.drop(['INTKEY','EXCEPTRSNCODE','EXCEPTRSNDESC','INATTENTIONIND','PEDROWNOTGRNT','SPEEDING','SDOTCOLNUM','LOCATION'], axis=1, inplace=True)","766eb57c":"df.columns","09a512dc":"df.shape","bf350cc1":"df['SEVERITYCODE'].equals(df['SEVERITYCODE.1'])","e43532fa":"to_drop = ['SEVERITYCODE.1']","79e47680":"to_study = ['OBJECTID', 'INCKEY', 'COLDETKEY', 'REPORTNO', 'SDOT_COLCODE', 'ST_COLCODE', 'SEGLANEKEY', 'CROSSWALKKEY']\n\nfor column in to_study:\n    print(column + \": {}\".format(len(df[column].unique())))","ae7b2914":"to_drop.extend(['OBJECTID', 'INCKEY', 'COLDETKEY', 'REPORTNO'])","9da11e6d":"df[['SEVERITYCODE','SEVERITYDESC']].value_counts()","9d3305f9":"df[['SDOT_COLCODE','SDOT_COLDESC']].value_counts(sort=False)","acf249c8":"df[['ST_COLCODE','ST_COLDESC']].value_counts(sort=False)","b80829d2":"to_drop.extend(['SEVERITYDESC','SDOT_COLDESC','ST_COLDESC'])","9817d8c0":"df[['INCDATE','INCDTTM']].sample(5)","2cd86332":"to_drop.extend(['INCDATE'])","cbc93d2d":"df.drop(to_drop, axis=1, inplace=True)","7758d916":"df.columns","1ecb053e":"df.dtypes","3c24c7d8":"df[['SEVERITYCODE']].value_counts()","9bf5e157":"df[['STATUS']].value_counts()","ba591f25":"df[['ADDRTYPE']].value_counts()","46a9833c":"df[['COLLISIONTYPE']].value_counts()","4357c01b":"df[['JUNCTIONTYPE']].value_counts()","518d9770":"#replace the missing 'JUNCTIONTYPE' values by the most frequent\nmost_freq = df['JUNCTIONTYPE'].value_counts().idxmax()\ndf['JUNCTIONTYPE'].replace('Unknown', most_freq, inplace=True)","15588bbe":"df[['UNDERINFL']].value_counts()","19ac339c":"df.loc[df.UNDERINFL == '0', 'UNDERINFL'] = \"N\"\ndf.loc[df.UNDERINFL == '1', 'UNDERINFL'] = \"Y\"","85816e1e":"df[['UNDERINFL']].value_counts()","4d854035":"df[['WEATHER']].value_counts()","92d43739":"#replace the missing 'WEATHER' values by the most frequent\nmost_freq = df['WEATHER'].value_counts().idxmax()\ndf['WEATHER'].replace('Unknown', most_freq, inplace=True)","8f112316":"df[['ROADCOND']].value_counts()","1f1500c9":"#replace the missing 'ROADCOND' values by the most frequent\nmost_freq = df['ROADCOND'].value_counts().idxmax()\ndf['ROADCOND'].replace('Unknown', most_freq, inplace=True)","e59540e9":"df[['LIGHTCOND']].value_counts()","f47805aa":"#replace the missing 'LIGHTCOND' values by the most frequent\nmost_freq = df['LIGHTCOND'].value_counts().idxmax()\ndf['LIGHTCOND'].replace('Unknown', most_freq, inplace=True)","7dd88f01":"df[['HITPARKEDCAR']].value_counts()","1fc44ebb":"to_cat = ['SEVERITYCODE', 'STATUS', 'ADDRTYPE', 'COLLISIONTYPE', 'JUNCTIONTYPE', 'UNDERINFL', 'WEATHER', 'ROADCOND', 'LIGHTCOND', 'HITPARKEDCAR']","171b62f6":"df[to_cat] = df[to_cat].astype('category') \ndf[['SDOT_COLCODE']] = df[['SDOT_COLCODE']].astype('object')\ndf[['INCDTTM']] = df[['INCDTTM']].astype('datetime64')","7d4a2456":"df.dtypes","13f7ee59":"df.columns[df.isna().any()].tolist()","4fefe8bd":"df.drop(['ST_COLCODE'], axis=1, inplace=True)","2ecaeafa":"# Split the dataset in training and test\n(df, df_test) = train_test_split(\n    df,\n    train_size=0.7, shuffle=True, random_state=seed)","1939b1ed":"#sns.set_theme(style='darkgrid')\nplt.figure(figsize=(8,6))\nsns.countplot(x ='SEVERITYCODE', palette='Set2', data = df) \nplt.title('Severity Code Count', fontsize=18)\nplt.xlabel('Severity Code', fontsize=16)\nplt.ylabel('Count', fontsize=16)\nplt.savefig('.\/plots\/1.png')","3a53defd":"df[\"SEVERITYCODE\"].value_counts(normalize=True)*100","8c822199":"fig, ax = plt.subplots(figsize=(20,5))\nsns.countplot(df['INCDTTM'].dt.year, palette='husl', ax=ax)\nax.set_xlabel('Year', fontsize=18)\nax.tick_params(axis='x', labelsize=15)\nax.tick_params(axis='y', labelsize=15)\nax.set_ylabel('Collision Count', fontsize=18)\nplt.title('Collision count through years', fontsize=18)\nplt.savefig('.\/plots\/2.png')","8afd16a8":"df['HOUR'] = df['INCDTTM'].dt.hour","502f46a7":"df[['HOUR', 'INCDTTM']].sample(5)","a2abba94":"bins = [0,4,8,12,16,20,24]\nlabels = ['Late Night', 'Early Morning','Morning','Noon','Eve','Night']\ndf['TIME'] = pd.cut(df['HOUR'], bins=bins, labels=labels, include_lowest=True)","d900a2bf":"df[['TIME','HOUR']].sample(5)","c013412a":"df.drop(['HOUR'], axis=1, inplace=True)","6d4ecf27":"def time_of_day_plot(df, title):\n    '''\n    Creates a countplot visualizing the data throughout the day \n    including the frequency.\n    \n        Parameters:\n            df(DataFrame): Data to be visualized\n            title(str): Title for the plot\n    '''\n    ncount = len(df['TIME'])\n    plt.figure(figsize=(12,8))\n    ax = sns.countplot(x='TIME', palette='Oranges', data=df)\n    plt.title(title, fontsize=18)\n    plt.xlabel('Time of the day', fontsize=18)\n\n    # Make twin axis\n    ax2=ax.twinx()\n\n    # Switch so count axis is on right, frequency on left\n    ax2.yaxis.tick_left()\n    ax.yaxis.tick_right()\n\n    # Also switch the labels over\n    ax.yaxis.set_label_position('right')\n    ax2.yaxis.set_label_position('left')\n\n    ax.set_ylabel('Count', fontsize=18)\n    ax2.set_ylabel('Frequency [%]', fontsize=18)\n    ax.tick_params(axis=\"x\", labelsize=15)\n\n    for p in ax.patches:\n        x=p.get_bbox().get_points()[:,0]\n        y=p.get_bbox().get_points()[1,1]\n        ax.annotate('{:.1f}%'.format(100.*y\/ncount), (x.mean(), y), \n                ha='center', va='bottom', fontsize=15) # set the alignment of the text\n\n    ax2.set_ylim(0,100*ax.get_ylim()[1]\/ncount)\n\n    # Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars\n    ax2.grid(None)","778078af":"time_of_day_plot(df, 'Distribution of Collisions throughout the day')\nplt.savefig('.\/plots\/3.png')","774ae4fe":"plt.figure(figsize=(8,6))\nsns.countplot(x ='UNDERINFL', palette='Set2', data = df) \nplt.title('Under Influence Count', fontsize=18)\nplt.xlabel('Under Influence', fontsize=16)\nplt.ylabel('Count', fontsize=16)\nplt.savefig('.\/plots\/4.png')","f33d764b":"#Filter the dataset with only the rows where UNDERINFL is Y\ninfluenced = df['UNDERINFL'] == 'Y'\ninfluenced = df[influenced]","b53ea031":"time_of_day_plot(influenced, 'Distribution of Collisions influenced by alcohol or drugs')\nplt.savefig('.\/plots\/5.png')","38ccbe43":"plt.figure(figsize=(8,6))\nsns.countplot(x ='HITPARKEDCAR', palette='Set2', data = df) \nplt.title('Hit Parked Car Count', fontsize=18)\nplt.xlabel('Hit Parked Car', fontsize=16)\nplt.ylabel('Count', fontsize=16)\nplt.savefig('.\/plots\/6.png')","4d4e5669":"#Filter the dataset with only the rows where HITPARKEDCAR is Y\nhit = df['HITPARKEDCAR'] == 'Y'\nhit = df[hit]","e222e02e":"time_of_day_plot(hit, 'Distribution of Collisions when a parked car is hit')\nplt.savefig('.\/plots\/7.png')","869bd92b":"sns.set_palette(sns.color_palette('magma_r'))\ntempdf = hit[(hit['UNDERINFL']=='Y')|(hit['UNDERINFL']=='N')]\nfig, ax = plt.subplots(figsize=(7,7))\nax.pie(tempdf['UNDERINFL'].value_counts(), textprops={'color':'white', 'fontsize': 14}, autopct='%1.0f%%', explode=[0,0.1])\n\nlgd = ax.legend(tempdf['UNDERINFL'].unique(),\n          title='Under Alcohol\/Drugs Influence',\n          loc='upper center',\n          bbox_to_anchor=(1, 0, 0.5, 1))\n#plt.savefig('.\/plots\/8.png')\nplt.savefig('.\/plots\/8.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\nplt.show()","b214d2d2":"#Filter the dataset with only the rows where HITPARKEDCAR is Y and UNDERINFL is Y\nhit_infl = (df['HITPARKEDCAR'] == 'Y') & (df['UNDERINFL'] == 'Y')\nhit_infl = df[hit_infl]","540c45a0":"time_of_day_plot(hit_infl, 'Distribution of Collisions when influenced by alcohol\/drugs and a parked car is hit')\nplt.savefig('.\/plots\/9.png')","7ad518a7":"plt.figure(figsize=(12,8))\nsns.countplot(y ='COLLISIONTYPE', palette='husl', data = df) \nplt.savefig('.\/plots\/10.png')","0a68cd34":"plt.figure(figsize=(12,8))\nsns.countplot(y ='JUNCTIONTYPE', palette='husl', data = df)\nplt.savefig('.\/plots\/11.png')","46d92859":"fig, axs = plt.subplots(nrows=3, figsize=(15,15))\nsns.countplot(y ='WEATHER', palette='husl', data = df, ax=axs[0]) \nsns.countplot(y ='ROADCOND', palette='husl', data = df, ax=axs[1]) \nsns.countplot(y ='LIGHTCOND', palette='husl', data = df, ax=axs[2])\nplt.savefig('.\/plots\/12.png')","3513c4b7":"from folium import plugins\n\n# simply drop whole row with NaN in \"price\" column\ndf_map = df.dropna(subset=['X','Y'], axis=0)\n\n# reset index, because we droped two rows\ndf_map.reset_index(drop=True, inplace=True)\n \nlatitude = df_map['Y'].mean()\nlongitude = df_map['X'].mean()\n\n# let's start with a clean copy of the map of Seattle\nseattle_map = folium.Map(location = [latitude, longitude], zoom_start = 12)\n\n# instantiate a mark cluster object for the collisions in the dataframe\ncollisions = plugins.MarkerCluster().add_to(seattle_map)\n\n# loop through the dataframe and add each data point to the mark cluster\nfor lat, lng in zip(df_map.Y, df_map.X):\n    folium.Marker(\n        location=[lat, lng],\n        icon=None\n    ).add_to(collisions)\n\n# display map\nseattle_map","dd0ee90d":"df.drop(['X','Y','INCDTTM'], axis=1, inplace=True)","0bd0b3c9":"df.shape","8485ec44":"(X, y) = (df.drop('SEVERITYCODE', axis=1), df['SEVERITYCODE'])","c3efb73d":"to_encode = ['STATUS', \n             'ADDRTYPE', \n             'COLLISIONTYPE',\n             'JUNCTIONTYPE',\n             'UNDERINFL',\n             'WEATHER',\n             'ROADCOND',\n             'LIGHTCOND',\n             'HITPARKEDCAR',\n             'TIME']\n\nle = LabelEncoder()\n\nfor feat in to_encode:\n    X[feat] = le.fit_transform(X[feat].astype(str))","94985e4d":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)","a54850b9":"tree_model = DecisionTreeClassifier(criterion='entropy', max_depth = 4)\ntree_model.fit(X_train, y_train)\npredTree = tree_model.predict(X_val)\nprint(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_val, predTree))","72e50e67":"fn=['STATUS','ADDRTYPE','COLLISIONTYPE','PERSONCOUNT','PEDCOUNT','PEDCYLCOUNT','VEHCOUNT','JUNCTIONTYPE','SDOT_COLDOE',\n   'UNDERINFL','WEATHER','ROADCOND','LIGHTCOND','SEGLANEKEY','CROSSWALKKEY','HITPARKEDCAR','TIME']\ncn=['prop damage', 'injury']\n\nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (40,20), dpi=300)\n\nout = tree.plot_tree(tree_model,\n               feature_names = fn, \n               class_names=cn,\n               fontsize=15,\n               filled = True);\n\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(3)\n\nplt.savefig('.\/plots\/13.png')","52013e9b":"Ks = 20\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\nConfustionMx = [];\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_val)\n    mean_acc[n-1] = metrics.accuracy_score(y_val, yhat)\n\n    \n    std_acc[n-1]=np.std(yhat==y_val)\/np.sqrt(yhat.shape[0])\n\nmean_acc","56dea1c0":"plt.figure(figsize=(10,6))\nplt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.40, color='aquamarine')\nplt.legend(('Accuracy ', '+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.savefig('.\/plots\/14.png')\nplt.show()","58525c1b":"print( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1) ","634ffc86":"kNN_model = KNeighborsClassifier(n_neighbors = 16)","5108fc65":"LR_model = LogisticRegression(C=0.01, solver='sag', max_iter=1000)\nLR_model.fit(X_train, y_train)\npredLR = LR_model.predict(X_val)\nprint(\"Logistic Regression's Accuracy: \", metrics.accuracy_score(y_val,predLR))","14df4747":"df_test['HOUR'] = df_test['INCDTTM'].dt.hour\nbins = [0,4,8,12,16,20,24]\nlabels = ['Late Night', 'Early Morning','Morning','Noon','Eve','Night']\ndf_test['TIME'] = pd.cut(df_test['HOUR'], bins=bins, labels=labels, include_lowest=True)\ndf_test.drop(['HOUR'], axis=1, inplace=True)\n\ndf_test.drop(['X','Y','INCDTTM'], axis=1, inplace=True)\n\n(X_test, y_test) = (df_test.drop('SEVERITYCODE', axis=1), df_test['SEVERITYCODE'])\n\nto_encode = ['STATUS', \n             'ADDRTYPE', \n             'COLLISIONTYPE',\n             'JUNCTIONTYPE',\n             'UNDERINFL',\n             'WEATHER',\n             'ROADCOND',\n             'LIGHTCOND',\n             'HITPARKEDCAR',\n             'TIME']\n\nle = LabelEncoder()\n\nfor feat in to_encode:\n    X_test[feat] = le.fit_transform(X_test[feat].astype(str))","191461e6":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print(\"Confusion matrix, without normalization\")\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=18)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label', fontsize=16)\n    plt.xlabel('Predicted label', fontsize=16)","ebaeb049":"#Train the model on the training data\ntree_model.fit(X, y)\n\n#Make predictions on the test data\nyhat_tree = tree_model.predict(X_test)\n\n#Compute the different metrics\njaccard_tree = metrics.jaccard_score(y_test, yhat_tree)\nf1_tree = metrics.f1_score(y_test, yhat_tree, average='weighted') \nacc_tree = metrics.accuracy_score(y_test, yhat_tree)\n\n#Print the results\nprint(\"Tree model Accuracy Score\", acc_tree)\nprint(\"Tree model Jaccard Score: \", jaccard_tree)\nprint(\"Tree model F1 Score: \", f1_tree)","1610b4b4":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat_tree, labels=[1,2])\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, yhat_tree))\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(8,6))\nplot_confusion_matrix(cnf_matrix, classes=['Prop damage(1)','Injury(2)'],normalize= False,  title='Confusion matrix')\nplt.savefig('.\/plots\/15.png')","1423cf4d":"#Train the model on the training data\nkNN_model.fit(X, y)\n\n#Make predictions on the test data\nyhat_kNN = kNN_model.predict(X_test)\n\n#Compute the different metrics\njaccard_kNN = metrics.jaccard_score(y_test, yhat_kNN)\nf1_kNN = metrics.f1_score(y_test, yhat_kNN, average='weighted') \nacc_kNN = metrics.accuracy_score(y_test, yhat_kNN)\n\n#Print the results\nprint(\"K Nearest Neighbors model Accuracy Score\", acc_kNN)\nprint(\"K Nearest Neighbors model Jaccard Score: \", jaccard_kNN)\nprint(\"K Nearest Neighbors model F1 Score: \", f1_kNN)","2b7b5439":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat_kNN, labels=[1,2])\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, yhat_tree))\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(8,6))\nplot_confusion_matrix(cnf_matrix, classes=['Prop damage(1)','Injury(2)'],normalize= False,  title='Confusion matrix')\nplt.savefig('.\/plots\/16.png')","3c5d7773":"#Train the model on the training data\nLR_model.fit(X, y)\n\n#Make predictions on the test data\nyhat_LR = LR_model.predict(X_test)\nyhat_LR_prob = LR_model.predict_proba(X_test)\n\n#Compute the different metrics\nacc_LR = metrics.accuracy_score(y_test, yhat_LR)\njaccard_LR = metrics.jaccard_score(y_test, yhat_LR)\nf1_LR = metrics.f1_score(y_test, yhat_LR, average='weighted') \nloss_LR = metrics.log_loss(y_test, yhat_LR_prob)\n\n#Print the results\nprint(\"Logistic Regression model Accuracy Score\", acc_LR)\nprint(\"Logistic Regression model Jaccard Score: \", jaccard_LR)\nprint(\"Logistic Regression model F1 Score: \", f1_LR)\nprint(\"Logistic Regression mode Log loss \", loss_LR)","585c5a5e":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat_LR, labels=[1,2])\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, yhat_tree))\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(8,6))\nplot_confusion_matrix(cnf_matrix, classes=['Prop damage(1)','Injury(2)'],normalize= False,  title='Confusion matrix')\nplt.savefig('.\/plots\/17.png')","404ef0ee":"#Create lists with values\nalgorithms = ['Decision Tree', 'K Nearest Neighbors', 'Logistic Regression']\nacc_total = [acc_tree, acc_kNN, acc_LR]\njaccard_total = [jaccard_tree, jaccard_kNN, jaccard_LR]\nf1_total = [f1_tree, f1_kNN, f1_LR]\nloss_total = ['','',loss_LR]\n\n#Create the dictionary\nd = {'Algorithm':algorithms, 'Accuracy':acc_total, 'Jaccard':jaccard_total, 'F1-score': f1_total, 'LogLoss': loss_total}\n\n#Create and visualize the DataFrame\nresults = pd.DataFrame(d)\nresults.set_index('Algorithm', inplace=True)\nresults","b4116e54":"Let's change the type of some columns:","f029fd1f":"## 5. Model Evaluation <a name=\"model-evaluation\"><\/a>","15e5f54d":"## 5. Modeling","c192a468":"## 3. Data Visualization <a name=\"data-visualization\"><\/a>","4bd96554":"Let's now study how the influence of alcohol and drugs affects collisions.","b05ff515":"Let's see the distribution of the `LIGHTCOND` column:","058f3907":"We are going to use all the data from X_train, y_train, X_val, y_val to train the models, therefore we are going to use the variables **X** and **y** which contains the training and validation data. After that we are going to test our models with the test data.","8d290df4":"Let's remember that our dataframe has **194673** rows. Looking at the output we can see that `OBJECTID`, `INCKEY`, `COLDETKEY` and `REPORTNO` have the same number of unique values as rows in the dataframe, therefore these variables do not add valuable information to the prediction.","26790fc0":"We are going to use the test dataset defined earlier: **df_test**\nFirst of all we need to split it into X and y, and apply the transformations that were applied to the training data after the split.","5933fc1d":"The model that I would choose is the **K Nearest Neighbors** model since it offers the most balanced predictions out of the three models proposed.","964513e8":"We can see that each code has a unique description, therefore the code columns already give us the required information so we can drop the description columns. \n\n*IMPORTANT: These columns could be useful for visualization purposes but in this case I have decided to drop them.*","3a3e5331":"Let's define the funtion that plots the confusion matrix we are going to use later:","a552bf5e":"Now, let's create a label for each time of the day:","c6eaf2a7":"We see that in the majority of the collisions that occur, the drivers where not under the influence of alcohol\/drugs. Let's now see the distribution of collisions inlfuenced by alcohol\/drugs throughout the day: ","d86fcb8d":"After that, we create X for the features and y for the class:","35d8b1c7":"# Seattle Traffic Collisions\nAuthor: Miguel Enrique J\u00e1tiva Jim\u00e9nez","14f3cfd5":"There are some columns that seem to have unique values for each row:\n\n* `OBJECTID`\n* `INCKEY`\n* `COLDETKEY`\n* `REPORTNO`\n* `SDOT_COLCODE`\n* `ST_COLCODE`\n* `SEGLANEKEY`\n* `CROSSWALKKEY`\n\nLet's study them.","20b7a3aa":"We can have a look to the shape of our dataframe:","7513c931":"Since they are equal we are going to drop one of them:","2f33f258":"Let's have a look at the columns left:","e4e762bd":"Let's start by replacing the missing values with the most frequent one:","3efb8307":"Let's study the columns regarding the date and time:\n\n* `INCDATE`\n* `INCDTTM`","d2354fee":"Let's see the distribution of the `COLLISIONTYPE` column:","30036d1f":"Let's see how the number of collisions has evolved through the years.","6526e2e1":"Let's see the distribution of the `STATUS` column:","a708f1dc":"Based on the summary above, there are 19 columns with missing values:\n\n* `X`: 5334 missing data\n* `Y`: 5334 missing data\n* `ADDRTYPE`: 1926 missing data\n* `INTKEY`: 129603 missing data\n* `LOCATION`: 2677 missing data\n* `EXCEPTRSNCODE`: 109862 missing data\n* `EXCEPTRSNDESC`: 189035 missing data\n* `COLLISIONTYPE`: 4904 missing data\n* `JUNCTIONTYPE`: 6329 missing data\n* `INATTENTIONIND`: 164868 missing data\n* `UNDERINFL`: 4884 missing data\n* `WEATHER`: 5081 missing data\n* `ROADCOND`: 5012 missing data\n* `LIGHTCOND`: 5170 missing data\n* `PEDROWNOTGRNT`: 190006 missing data\n* `SDOTCOLNUM`: 79737 missing data\n* `SPEEDING`: 185340 missing data\n* `ST_COLCODE`: 18 missing data\n* `ST_COLDESC`: 4904 missing data","4d386667":"Let's see the distribution of the `SEVERITYCODE` column:","922e634d":"We can see that most of the collisions occur in the center of Seattle.","e57fc353":"After that, let's take care of the columns to be dropped:","1f752e89":"Let's see the distribution of the `UNDERINFL` column:","b91f3f87":"We can see the changes where applied since there are more samples with the \"Block\" value on the ADDRTYPE variable. Let's apply the same methodology to the rest of the columns on the list.","8b84df7c":"Now we can plot the amount of collisions given the time of the day:","4896ce97":"## 4. Pre-processing <a name=\"pre-processing\"><\/a>","8a5b7c69":"## 6. Summary <a name=\"summary\"><\/a>","dcc8bf42":"Once the summary is printed it is time to decide how should we deal with these missing values. There are several options to be used depending on the situation.","2b3cfe5a":"We can see that our tree model does a good job classifying the **Prop damage** samples, **40506** (TN, if we take the Prop damage class as negative) but its behavior is not good when classifying **Injury** samples, it classifies **14160** samples as **Prop damage** when they really are **Injury** (FN).","d6dc919b":"### Decision Tree <a name=\"descision-tree\"><\/a>","f2f116a8":"We can plot the generated tree structure:","124e576a":"Let's study the distribution of collisions when there is influence of alcohol\/drugs and a parked car is hit.","50432815":"It is observed that most of the parked cars are hit by people with no influence of alcohol\/drugs.","8bdeb973":"## 1. Introduction <a name=\"introduction\"><\/a>","a47fdf5a":"Plot  model accuracy  for Different number of Neighbors:","3d380438":"We can see that `ST_COLCODE` has some missing values, we are going to drop it because the column `SDOT_COLCODE` gives us almost the same information. Regarding the `X` and `Y` columns we are going to use them for visualization purposes and after that they will be dropped.","160ec313":"We can see that both columns give us the same information but `INCDTTM` also stores the time of the collision so we are going to drop `INCDATE`.","ba5ed6be":"### K Nearest Neighbors <a name=\"kNN\"><\/a>","8d11a26a":"We can see that people under the influence of alcohol\/drugs tend to hit parked cars at **Late Night**.","875abae3":"Let's see the distribution of the `ADDRTYPE` column:","289d3a8c":"### K Nearest Neighbors <a name=\"kNN-eval\"><\/a>","f5a98d97":"We can see that the behavior is similar to the one that the tree model offers, but we can see that the kNN model does a better job classifying the **Injury** class.","7f4972ef":"We can see that most of the collisions occur when the **Weather** is *Clear* the **Road Condition** is *Dry* and the **Lighting Condition** is *Daylight*.","9289d99d":"If we use the *idxmax()* method along with *value_counts()* we get the most frequent value directly.","67f056e7":"Let's see the distribution of the `WEATHER` column:","c86e3d89":"We can see that 45% of the collisions when the influence of alcohol\/drugs is present are at **Late Night** and 23% are at **Night**. The behavior is as expected.","0d9098ab":"After that, let's study the amount of collisions throughout the day. To do that, let's extract the hour from the `INCDTTM` column:","ec7d48f6":"We are going to drop the columns `X` and `Y` as we said earlier. We are also going to drop `INCDTTM` because we have the `TIME` column created earlier:","16a4dc2d":"Let's now split the dataset into train and test.","705d4a84":"We can see that there is a value named **Unknown**. Let's change them with the most frequent value: ","2bcedebb":"Let's check if there are any columns left with NaN values:","6813147a":"Let's see the distribution of the `ROADCOND` column:","3a97408d":"### Logistic Regression <a name=\"LR-eval\"><\/a>","49997de2":"It is clearly seen that the time of the day when most collisions occur are **Late Night** and **Noon**.","2c6db15f":"This method gives us the count of each unique value so we can see which one is the most frequent.","3a6181a5":"Let's study now some variables that seem to be descriptions:\n\n* `SEVERITYDESC`\n* `SDOT_COLDESC`\n* `ST_COLDESC`","9e2f19e3":"# Table of contents\n1. [Introduction](#introduction)\n2. [Data Wrangling](#data-wrangling)\n3. [Data Visualization](#data-visualization)\n3. [Pre-processing](#pre-processing)\n3. [Modeling](#modeling)\n    1. [Decision Tree](#decision-tree)\n    1. [K Nearest Neighbors](#kNN)\n    1. [Logistic Regression](#LR)\n3. [Model Evaluation](#model-evaluation)\n    1. [Decision Tree](#decision-tree-eval)\n    1. [K Nearest Neighbors](#kNN-eval)\n    1. [Logistic Regression](#LR-eval)\n3. [Summary](#summary)\n\n","01dc0081":"Let's see the distribution of the `HITPARKEDCAR` column:","de2ddb53":"Here we can also see the **Unknown** value, we are going to do the same:","488a5b80":"We can see that there are four unique values but we only need two of them to represent yes or no. So we are going to convert **0 to N** and **1 to Y**.","8040aafb":"As we can see there are 194673 rows of data and 38 columns.","7b866950":"We can see that there are two columns with almost the same name. These are `SEVERITYCODE` and `SEVERITYCODE.1`. So let's check if both are equal:","d393e681":"It is seen that there are some columns with missing values. I am going to count missing values in each column (take into account that \"True\" represents a missing value):","05c40465":"We can see that the data is not balanced 70% of the collisions are type 1 which means prop damage, and 30% type 2 injury.","9987b627":"## 2. Data Wrangling <a name=\"data-wrangling\"><\/a>","b38eb62d":"### Decision Tree <a name=\"decision-tree-eval\"><\/a>","1199a630":"We can study if people influenced by alcohol\/drugs tend to hit parked cars more:","84a7eb90":"We can see that the number of collisions have been decreasing since 2015. We can also see that there is a small amount of collisions in 2020, this is probably due to the pandamic situation we are living. ","ba754646":"Let's see the distribution of the `JUNCTIONTYPE` column:","d6fdf8d7":"Let's study the distribution of the `HITPARKEDCAR` column:","328f5f29":"Then we split the dataset into train and validation:","c86772eb":"Let's create a map of Seattle to visualize where the collisions are placed.\n\n*IMPORTANT: the map creation has been commented because it adds several MB to the final notebook*","88a94c78":"Let's study them along with the corresponding \"code\" column:","aebb505d":"Let's create some more plots to see the behavior of the columns left.","ccae9e24":"### Logistic Regression <a name=\"LR\"><\/a>","7b886290":"Let's study the columns that are more likely to be categories:\n* `SEVERITYCODE`\n* `STATUS`\n* `ADRRTYPE`\n* `COLLISIONTYPE`\n* `JUNCTIONTYPE`\n* `UNDERINFL`\n* `WEATHER`\n* `ROADCOND`\n* `LIGHTCOND`\n* `HITPARKEDCAR`","2c49d2e6":"We can see that 20% of parked cars are hit in the **Morning**, this percentage decreases throughout the day but it suddenly increases in the **Late Night** maybe due to the influence of alcohol and drugs as we have seen.","d2e5c6a2":"Once everything is checked we can drop the `HOUR` column created earlier:","4c363caf":"There are few collisions when a parked car is hit. Let's study when more parked cars are hit during the day:","aaaf083c":"**Replace by frequency:**\n* `ADDRTYPE`: 1926 missing values. Looking at the Metadata document we can see that this variable has 3 unique values (Alley, Block, Intersection) so we are going to replace the missing values with the most frequent one\n* `COLLISIONTYPE`: 4904 missing values. We are going to replace the missing values with the most frequent Collision Type\n* `JUNCTIONTYPE`: 6329 missing values (the reason is the same as the previous one)\n* `UNDERINFL`: 4884 missing values (the reason is the same as the previous one)\n* `WEATHER`: 5081 missing values (the reason is the same as the previous one)\n* `ROADCOND`: 5012 missing values (the reason is the same as the previous one)\n* `LIGHTCOND`: 5170 missing values\n\n**Drop the whole column:**\n* `INTKEY`: there are 129603 missing values and in our dataframe there are in total 194673 rows, therefore more than half of the values are missing \n* `EXCEPTRSNCODE`: 109862 missing values.\n* `EXCEPTRSNDESC`: 189035 missing values.\n* `INATTENTIONIND`: 164868 missing values.\n* `PEDROWNOTGRNT`: 190006 missing values.\n* `SPEEDING`: 185340 missing values.\n* `SDOTCOLNUM`: 79737 missing values.\n* `LOCATION`: 2677 missing values. We are going to drop this column because its values may be redundant with the columns `X` and `Y` which are the coordinates of the collision.\n","bac9ad77":"We can see that the behavior is similar to the ones offered by the tree and kNN models but this model is the worst when classifying the **Injury** class."}}