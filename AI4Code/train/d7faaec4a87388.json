{"cell_type":{"4a54bdaa":"code","f8be22a3":"code","cf243b2f":"code","04d20587":"code","29fbecdd":"code","eba364e1":"code","a7346bfc":"code","15ecf442":"code","91c1ed1f":"code","88ab0795":"markdown","b9a843c5":"markdown","5bf43b96":"markdown","8ae85337":"markdown","2a7143fa":"markdown","495165e2":"markdown","42dbee98":"markdown"},"source":{"4a54bdaa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f8be22a3":"from keras.datasets import boston_housing\n\n(train_data, train_target), (test_data, test_target) = boston_housing.load_data()\n\nprint(train_data.shape)\nprint(test_data.shape)","cf243b2f":"mean = train_data.mean(axis=0)\nstd = train_data.std(axis=0)\n\ntrain_data -= mean\ntrain_data \/= std\ntest_data -= mean\ntest_data \/= std","04d20587":"from keras.models import Sequential\nfrom keras.layers import Dense\n\ndef build_model():\n    model = Sequential()\n    model.add(Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n    return model","29fbecdd":"k = 4\nnum_val_samples = train_data.shape[0] \/\/ k\nnum_epochs = 100\nall_scores = []\n\nfor i in range(k):\n    print('Processing Fold #', i)\n    val_data = train_data[i * num_val_samples : (i+1) * num_val_samples]\n    val_target = train_target[i * num_val_samples : (i+1) * num_val_samples]\n    \n    partial_train_data = np.concatenate([train_data[:i * num_val_samples], train_data[(i+1) * num_val_samples:]], axis=0)\n    partial_train_target = np.concatenate([train_target[:i * num_val_samples], train_target[(i+1) * num_val_samples:]], axis=0)\n    \n    model = build_model()\n    model.fit(partial_train_data, partial_train_target, epochs=num_epochs, batch_size=1, verbose=0)\n    val_mse, val_mae = model.evaluate(val_data, val_target, verbose=0)\n    all_scores.append(val_mae)\n\nprint(all_scores)\nprint(np.mean(all_scores))","eba364e1":"num_epochs = 500\nall_mae_histories = []\n\nfor i in range(k):\n    print('Processing Fold #', i+1)\n    val_data = train_data[i * num_val_samples : (i+1) * num_val_samples]\n    val_target = train_target[i * num_val_samples : (i+1) * num_val_samples]\n    \n    partial_train_data = np.concatenate([train_data[:i * num_val_samples], train_data[(i+1) * num_val_samples:]], axis=0)\n    partial_train_target = np.concatenate([train_target[:i * num_val_samples], train_target[(i+1) * num_val_samples:]], axis=0)\n    \n    model = build_model()\n    history = model.fit(partial_train_data, partial_train_target, epochs=num_epochs, batch_size=1, validation_data=(val_data, val_target), verbose=0)\n    mae_history = history.history['val_mae']\n    all_mae_histories.append(mae_history)\n    \navg_mae_histories = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]","a7346bfc":"import matplotlib.pyplot as plt\n\nplt.plot(range(1, len(avg_mae_histories)+1), avg_mae_histories)\nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\nplt.show()","15ecf442":"def smooth_curve(points, factor=0.9):\n    smoothed_points = []\n    for point in points:\n        if smoothed_points:\n            previous = smoothed_points[-1]\n            smoothed_points.append(previous * factor + point * (1-factor))\n        else:\n            smoothed_points.append(point)\n    return smoothed_points\n            \nsmoothed_mae_histories = smooth_curve(avg_mae_histories[10:])\n\nplt.plot(range(1, len(smoothed_mae_histories)+1), smoothed_mae_histories)\nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\nplt.show()","91c1ed1f":"model = build_model()\nmodel.fit(train_data, train_target, epochs=80, batch_size=16, verbose=0)\ntest_mse_score, test_mae_score = model.evaluate(test_data, test_target)\nprint(test_mse_score, test_mae_score)","88ab0795":"Between epoch 0 and 100 the validation MAE is the smallest at the bottom. Therefore, we should build a model where its epochs stop right there","b9a843c5":"Since the data is little, we use k-fold to improve model's performance. Here, we split the data into 4 folds","5bf43b96":"# Key Takeaway\n\n* Mean Squared Error (MSE) is a commonly used loss function for regression\n* Mean Absolute Error (MAE) is the common regression metrics to model's performance\n* Don't forget to scale the features in preprocessing step when they are in different range\n* K-fold is a great way to solve the problem of little data available\n* When data is little, using a small network with few hidden layers is preferable in order to avoid overfitting.","8ae85337":"# Scaling\n\nWe need to make every feature range from 0 to 1","2a7143fa":"We have a rather small dataset here","495165e2":"The first 10 MAE values kind of messed up the scale of whole chart. Also, replacing each point with an exponential average of the previous point can give us a smoother curve","42dbee98":"In this project I will build neural networks for regression to predict the housing price in Boston, a dataset from keras package. The main goal is not to explore and clean data since it has been done already, but to focus on building a regression model.\n\nAll the codes are originally from \"Deep Learning With Python\" by Fran\u00e7ois Chollet. This is a note to keep my study pregress of the book."}}