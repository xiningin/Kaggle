{"cell_type":{"87d11218":"code","264318ce":"code","667e34d1":"code","36cc2d3b":"code","89c2c8a3":"code","109c92ca":"code","977a5ed2":"code","ceaff1b4":"code","c25bb25a":"code","7e9e066a":"code","fa337943":"code","1c08798c":"code","1a0fd283":"code","78e1085d":"code","78f629df":"code","0ed3861d":"code","bbd8c619":"code","90a49315":"code","4b0b8d4f":"code","241f02ec":"code","ba45090c":"code","c78b3282":"code","907f7b06":"code","e96c7079":"code","c1d1ebf7":"code","29c0e18b":"code","ede97986":"markdown","8e401f28":"markdown","a3581d96":"markdown","5b57f559":"markdown","7f952f18":"markdown","145f2b5c":"markdown","6bc270f1":"markdown","a4a8005e":"markdown","674b7bf4":"markdown","d3ea3647":"markdown","a9ad393c":"markdown","d07d541b":"markdown","32125e9f":"markdown","0687edf9":"markdown","eb2095d6":"markdown","ef38d0e5":"markdown","3264a903":"markdown","e4f69d72":"markdown","8ef5a30b":"markdown","5f346fcb":"markdown","f54c69c0":"markdown","0b1fd6b3":"markdown","e92c2198":"markdown","94683bd6":"markdown","2465bc30":"markdown","adb893fc":"markdown","4a4caad6":"markdown","39247a32":"markdown","f54d5236":"markdown","91ff749a":"markdown","b06aedfe":"markdown","6327043a":"markdown","7a0cfe36":"markdown","c88a1401":"markdown","30cfe14b":"markdown","d8aeabce":"markdown"},"source":{"87d11218":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# for the notebook\n# %matplotlib inline\n# disable warnings\nimport warnings\nwarnings.filterwarnings('ignore')","264318ce":"# get the data and show basic information\ntrain_set = pd.read_csv(\"..\/input\/train.csv\")\ntrain_set.info()","667e34d1":"train_set.hist(bins=20, figsize=(16,8))\nplt.show()","36cc2d3b":"print(\"Age.mean =\", train_set[\"Age\"].mean())\nprint(\"Survived.mean =\", train_set[\"Survived\"].mean())","89c2c8a3":"# we make a new copy of the data to work with\nt = train_set.copy()\nattrs_family_related = [\"Family\", \"Parch\", \"SibSp\"]\nt[\"Family\"] = t[\"Parch\"] + t[\"SibSp\"]\nt[attrs_family_related].hist(bins=10, figsize=(16,8))\nplt.show()","109c92ca":"t[attrs_family_related].hist(bins=10, figsize=(16, 8), weights=t[\"Survived\"])\nplt.show()","977a5ed2":"def plot_survival_per_feature(data, feature):\n    grouped_by_survival = data[feature].groupby(data[\"Survived\"])\n    survival_per_feature = pd.DataFrame({\"Survived\": grouped_by_survival.get_group(1),\n                                        \"didnt_Survived\": grouped_by_survival.get_group(0),\n                                        })\n    hist = survival_per_feature.plot.hist(bins=20, alpha=0.6)\n    hist.set_xlabel(feature)\n    plt.show()","ceaff1b4":"plot_survival_per_feature(t, \"Family\")","c25bb25a":"plot_survival_per_feature(t, \"Age\")","7e9e066a":"plot_survival_per_feature(t, \"Pclass\")","fa337943":"t[\"Embarked\"].hist(by=t[\"Survived\"], sharey=True, figsize=(16,8))\nplt.show()","1c08798c":"t[\"Embarked\"].groupby(t[\"Pclass\"]).value_counts()","1a0fd283":"# Sex destribution\nt[\"Sex\"].value_counts().plot.pie(figsize=(8,8))\nplt.show()\n# What is the number of survival\/non survival in each of the two sex?\nt[\"Sex\"].hist(by=t[\"Survived\"], sharey=True, figsize=(16,8))\nplt.show()","78e1085d":"print(t[\"Cabin\"].dropna())","78f629df":"corr_matrix = t.corr()\ncorr_matrix[\"Fare\"]","0ed3861d":"plot_survival_per_feature(t, \"Fare\")","bbd8c619":"t[\"personal_fare\"] = t[\"Fare\"] \/ (t[\"Family\"] + 1)\nplot_survival_per_feature(t, \"personal_fare\")","90a49315":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nsex = t[\"Sex\"]\nsex_encoded = encoder.fit_transform(sex)\nt2 = t.copy()\nt2[\"Sex\"] = sex_encoded\nembarked = t[\"Embarked\"].fillna(\"C\")\nembarked_encoded = encoder.fit_transform(embarked)\nt2[\"Embarked\"] = embarked_encoded\nt2.corr()[\"Embarked\"]","4b0b8d4f":"labels = train_set[\"Survived\"]\nfeatures_data = train_set.drop(\"Survived\", axis=1)","241f02ec":"from sklearn.preprocessing import Imputer, LabelEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\n# store the columns for the learning_data\nCOLUMNS = None\n\nclass Dropper(BaseEstimator, TransformerMixin):\n    def __init__(self, to_drop=[\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"]):\n        self.to_drop = to_drop\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X.drop(self.to_drop, axis=1)\n    \n    \nclass AttributesExtension(BaseEstimator, TransformerMixin):\n    def __init__(self, family=True, personal_fare=True, is_child=True, is_child_and_sex=True):\n        self.family = family\n        self.personal_fare = personal_fare\n        self.is_child = is_child\n        self.is_child_and_sex = is_child_and_sex\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        if self.family:\n            family = X[\"Parch\"] + X[\"SibSp\"]\n            X[\"Family\"] = family\n        if self.personal_fare and self.family:\n            personal_fare = X[\"Fare\"] \/ (X[\"Family\"] + 1)\n            X[\"Personal_fare\"] = personal_fare\n        # is_child improved the model by 2% accuracy\n        if self.is_child:\n            X[\"is_child\"] = X[\"Age\"] <= 8\n        if self.is_child_and_sex:\n            X[\"is_child_and_sex\"] = X[\"Sex\"] * X[\"is_child\"]\n        \n            \n        #save columns\n        global COLUMNS\n        COLUMNS = X.columns.tolist()\n        return X\n    \n    \nclass AttributesEncoding(BaseEstimator, TransformerMixin):\n    def __init__(self, sex=True, embarked=True):\n        self.sex = sex\n        self.embarked = embarked\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        encoder = LabelEncoder()\n        if self.sex:\n            sex_encoded = encoder.fit_transform(X[\"Sex\"])\n            X[\"Sex\"] = sex_encoded\n        if self.embarked:\n            #impute with C\n            embarked_encoded = encoder.fit_transform(X[\"Embarked\"].fillna('C'))\n            X[\"Embarked\"] = embarked_encoded\n        return X","ba45090c":"from sklearn.impute import SimpleImputer\npipeline = Pipeline([\n    ('dropper', Dropper()),\n    ('encoder', AttributesEncoding()),\n    ('extender', AttributesExtension()),\n    ('imputer', SimpleImputer(strategy=\"mean\")),\n])\nlearning_data = pipeline.fit_transform(features_data)","c78b3282":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm.classes import SVC\nfrom sklearn.metrics import accuracy_score\n\nsvc = SVC()\nlog_reg = LogisticRegression()\n#log_reg.fit(learning_data, labels)\nrand_for = RandomForestClassifier()\n#rand_for.fit(learning_data, labels)\n\nmodels = {\n    \"Logistic Regression\": log_reg,\n    \"Random Forest\": rand_for,\n    \"SVM\": svc,\n}\n\nfor model in models.keys():\n    scores = cross_val_score(models[model], learning_data, labels, scoring=\"accuracy\", cv=10)\n    print(\"===\", model, \"===\")\n    print(\"scores = \", scores)\n    print(\"mean = \", scores.mean())\n    print(\"variance = \", scores.var())\n    models[model].fit(learning_data, labels)\n    print(\"score on the learning data = \", accuracy_score(models[model].predict(learning_data), labels))\n    print(\"\")","907f7b06":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nlog_reg.fit(learning_data, labels)\nperm_imp = PermutationImportance(log_reg, random_state=1).fit(learning_data, labels)\neli5.show_weights(perm_imp,feature_names=COLUMNS)","e96c7079":"test_set = pd.read_csv(\"..\/input\/test.csv\")\npred = pipeline.fit_transform(test_set)","c1d1ebf7":"log_reg.fit(learning_data, labels)\nsub = pd.DataFrame(test_set[\"PassengerId\"], columns=(\"PassengerId\", \"Survived\"))\nsub[\"Survived\"] = log_reg.predict(pred)","29c0e18b":"#write predicted data to submit it\n#sub.to_csv(\"..\/input\/sub.csv\", index=False)","ede97986":"So, the mean age is 29.69 and 38% of the passengers survived.\n\nSince the Parch\/SibSp features are not detailed we can't even think of \"parent tend to save their childs\" or \"husband will be the hero of his spouse\" so we will just try to merge them and see.","8e401f28":"Let us see how the number of Survived passengers is distributed according to our new Family feature","a3581d96":"The more a passenger have a high fare the more it has a chance to survive. The correlation with the Pclass make sense now, we already know that the more the class is low the more the passenger has a chance to survive, this is why the correlation value is negative.","5b57f559":"### Visualize the data and gain insights: think of new features","7f952f18":"### Fine-tune the model","145f2b5c":"##### Notes:\n- We can see here that we have a lot of passengers who are alone (Parch and SibSp)\n- More than 50% of the passengers are in the third class (Pclass)\n- The mean Age is around 18-35, we can check the exact value later \n- Appromximately 30% of the passengers only survived :\/, we can check the exact value later\n- The Fare is generally less than 50\n\nwe will now calculate the mean value for the Age and the Survived features","6bc270f1":"okaay, now we can see that passengers who were alone have not a big chance to survive, actually, less than 30% of the alone passengers survived. You should be aware of what you visualize, you might conclude something wrong! so be careful.","a4a8005e":"More than 500 passengers are alone!","674b7bf4":"For the cabin feature, I started googling for a map or something that can help me visualize what the letters and numbers mean in the cabin number, and I found a good [discussion](https:\/\/www.kaggle.com\/c\/titanic\/discussion\/4693) on Kaggle that was talking exactly about that. In summary, the cabin feature is closely related to the Pclass feature and the more the letter is close to the 'A' the more the passenger is close to the lifeboats.\n\nWe have two choices for the cabin feature:\n1. we can just keep the first letter and use the Pclass feature to impute it eg. maping 1st class to one of the letters A,B or C\n2. we can just get rid of it since it's closely related to the Pclass feature.","d3ea3647":"I was reading about how to select good feature from [here](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance?utm_medium=email&utm_source=mailchimp&utm_campaign=ml4insights) so I decided to try it now that I can't add features on myself, so let's do it.","a9ad393c":"There is a 3 features that correlate with the Fare: Survived, Pclass and Family (SibSp, Parch). We will check the correlation with the Survived with our magic function then we will see for the two other features.","d07d541b":"This part will be implemented soon","32125e9f":"### Prepare the data","0687edf9":"We will talk about the Fare now, but let's take a look at the correlation matrix first.","eb2095d6":"What about the Pclass feature?","ef38d0e5":"We will now build our pipeline of transormation that we will use to get prepared data. But we will first cut the label apart first","3264a903":"Wow! it's too obvious now that passengers who where alone has a greater chance to survive than families... wait, but we have seen that a lot of passengers were alone, so this histogram is not really representative... We should try something else","e4f69d72":"Passengers with greater personal_fare tend to survive more, it doesn't seem to be harmfull so we will think of adding it as a new feature.","8ef5a30b":"Let's now get this awesome new function to work on other features","5f346fcb":"So what if we use this function to visualize more precisely what's happening","f54c69c0":"There is much more male than female on the Titanic but what about the survivals? didn't guess that already? the survival\/non-survival ratio is much greater for the female than the male, isn't that obvious? like we say \"les femmes d'abord\". I think of encoding this feature into binary with value 1 indicating a female and 0 indicating a male.","0b1fd6b3":"Apparently, young passengers (~< 8) were the one who has the number of survivals greater than 50%.\nPassengers between 65 and 75 didn't survive at all but they were not that much, however a unique passenger who has 80 years survived. We may conclude that childs were rescued more than other passengers so we might add a feature like is_child which is set to the value 1 if the passenger have less than 8 years and the value 0 otherwise.","e92c2198":"# Titanic\n\nAs a begginer to ML and Data Science, I tried through this Kernel to practice what I learnt on an End to End project and see if I can make a good prediction.\n\nI learned how to deal with ML project from the [Hands-On Machine Learning with Sickit-learn and TensorFlow](https:\/\/github.com\/ageron\/handson-ml), my methodology is inspired from this book.\n\nI will go through each one of this steps:\n\n- [Prepare the workspace, load the data and take a quick look to it](#Prepare-the-workspace,-load-the-data-and-take-a-quick-look-to-it)\n- [Visualize the data and gain insights: think of new features](#Visualize-the-data-and-gain-insights:-think-of-new-features)\n- [Prepare the data](#Prepare-the-data)\n- [Select a model](#Select-a-model)\n- [Fine-tune the model](#Fine-tune-the-model)\n- [Run on test data](#Run-on-test-data)\n\nYou can find the problem description [here](https:\/\/www.kaggle.com\/c\/titanic).\n\nLet's begin","94683bd6":"Apparently, medium sized families have a greater chance so survive than alone passengers and big families. We can think of it like \"Alone passengers didn't survive because they haven't someone to help them and big families were unmanageable\".","2465bc30":"For the correlation with the family, I don't know if the food is accounted in the fare or the class or something else, I just assume that the more the family is bigger the more they consume, this is why the fare is high when a passenger has family, we may divide the Fare by the Family+1 (+1 for the person itself) to get the fare for one passenger and see if it correlate with other features.","adb893fc":"We are gonna implement a function that will be useful to visualize the survival relatively to a certain feature","4a4caad6":"We can see that passengers who embarked at Cherbourg have the highest survival\/non-survival ratio, I asked my mother who was next to me and she said with all simplicity, it's maybe because they embarked more on the 1st class so I checked that and ...\n\nC = Cherbourg, Q = Queenstown, S = Southampton","39247a32":"### Select a model","f54d5236":"### Prepare the workspace, load the data and take a quick look to it\n\nfirst things first, import the modules needed","91ff749a":"Looking at those results, I choosed the logitic regression because:\n\n- the others looks like they are overfiting\n- it has a good mean score\n- it has a low variance","b06aedfe":"### Run on test data","6327043a":"Hmm... the more we increase the class is high, the more the survival\/non-survival ratio decrease, it's surely because of the position of the passengers of each class, in the data description it's said :\n\n`\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n`\n\nIt's clear now that 1st class was closer to the lifeboats than the others.\n","7a0cfe36":"We can see that we have a lot of missing data in the 'Cabin' feature and some in the 'Age' one.\nWe are gonna visualize the data so we can think of a way to impute our data after that.","c88a1401":"Embarkation","30cfe14b":"Allright, the first thing that we conclude is: we should take advices from our mother seriously.","d8aeabce":"The features are ordered by impact on the model, so the Sex feature has the biggest impact on our model.\n\nI repeated this process many time and tried to combine features to end up with adding is_child_and_sex feature."}}