{"cell_type":{"d2d629db":"code","3cd01b0d":"code","b4968862":"code","64731284":"code","38ba20a8":"code","9dad88a4":"code","32b10c03":"code","f558a692":"code","053aaf7f":"markdown","17ef496d":"markdown","074e5016":"markdown","e46fe202":"markdown","3f6cd79e":"markdown","94de6395":"markdown"},"source":{"d2d629db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3cd01b0d":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice              \nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","b4968862":"X_train.head()","64731284":"from xgboost import XGBRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\n\ndef get_score(n_estimators):\n    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n    \n    Keyword argument:\n    n_estimators -- the number of trees in the forest\n    \"\"\"\n    # Replace this body with your own code\n    my_pipeline = Pipeline(steps=[\n        ('preprocessor', SimpleImputer()),\n        ('model', XGBRegressor(n_estimators=n_estimators,learning_rate=.05, random_state=0))\n        ])\n    scores = -1 * cross_val_score(my_pipeline, X_train, y_train,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n    return scores.mean()","38ba20a8":"import matplotlib.pyplot as plt\n\nresults = {}\nfor i in range(1,11):\n    results[100*i] = get_score(100*i)\n\n\n%matplotlib inline\n\nplt.plot(list(results.keys()), list(results.values()))\nplt.show()","9dad88a4":"model = XGBRegressor(n_estimators = 400, learning_rate=.05, random_state=0)\n\nmodel.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)","32b10c03":"from sklearn.metrics import mean_absolute_error\n\npredictions = model.predict(X_valid)\n\nmae = mean_absolute_error(predictions, y_valid)\n\nprint('The mean absolute error is: ', mae)","f558a692":"final_preds = model.predict(X_test)\n\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': final_preds})\noutput.to_csv('submission.csv', index=False)","053aaf7f":"Now we will make predictions on the validation data.","17ef496d":"We have determined that the optimal number of estimators for an XGB regressor with a learning rate of .05 is 400. Now we will fit this to the data.","074e5016":"Perform cross-validation to optimize parameters. ","e46fe202":"Begin by loading the training data and test data, dropping missing target data, and breaking off validation set from training data. ","3f6cd79e":"Now to make predictions with the test data.","94de6395":"Define a useful function for changing parameters. "}}