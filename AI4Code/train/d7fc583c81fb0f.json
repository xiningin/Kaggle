{"cell_type":{"8688b2ba":"code","ee740e29":"code","663fad8d":"code","b413db65":"code","9e71d30d":"code","5e8f5982":"code","b1a6cdab":"code","c63d107c":"code","4282f0b9":"code","ffceedf1":"code","0fcdf3d2":"code","2bc3cee2":"code","981c4c4b":"code","f6a69ff8":"code","c7c99522":"code","dc31f7b3":"code","7be33f5c":"code","02738e59":"code","abb96cd7":"code","89ecaf3c":"code","19af979f":"code","5792dc60":"code","af8f111a":"code","7e550ca3":"code","1ea1846b":"code","23782b65":"code","cd05d116":"code","7e353f55":"code","5fa14a70":"code","992a4135":"markdown","d79fcfd4":"markdown","ee646b51":"markdown","7b9a2311":"markdown","f0fd2bf4":"markdown","3f539898":"markdown","be47ec8f":"markdown","94ba667e":"markdown","430cc78a":"markdown","b55c7770":"markdown","133a09a6":"markdown","e01509c5":"markdown","ebad783a":"markdown","5b450695":"markdown","96f19aa7":"markdown","5076bb85":"markdown","e304ac56":"markdown","a56125fc":"markdown","fcad37b7":"markdown","babd2a76":"markdown","f072e8b5":"markdown","78f61b88":"markdown","5b839b11":"markdown","dba13896":"markdown","3f1e8978":"markdown","b2b43260":"markdown","5ffef254":"markdown","dd1b688b":"markdown"},"source":{"8688b2ba":"import sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')","ee740e29":"!pip install -q accelerate\n!pip install wandb --upgrade","663fad8d":"# Warning\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# Python\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport glob\npd.set_option('display.max_columns', None)\n\n# Visualizations\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\n# Image Augmentations\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n\n# Utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score\n\n# Pytorch for Deep Learning\nimport torch\nimport torchvision\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.cuda import amp\n\n\n# GPU \nfrom accelerate import Accelerator\naccelerator = Accelerator()\n\n# Weights and Biases Tool\nimport wandb","b413db65":"params = {\n    'seed': 42,\n    'model': 'swin_small_patch4_window7_224',\n    'size' : 224,\n    'inp_channels': 1,\n    'device': accelerator.device,\n    'lr': 1e-4,\n    'weight_decay': 1e-6,\n    'batch_size': 32,\n    'num_workers' : 0,\n    'epochs': 5,\n    'out_features': 1,\n    'name': 'CosineAnnealingLR',\n    'T_max': 10,\n    'min_lr': 1e-6,\n    'num_tta':1\n}","9e71d30d":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(params['seed'])","5e8f5982":"train_dir = ('..\/input\/seti-breakthrough-listen\/train')\ntest_dir = ('..\/input\/seti-breakthrough-listen\/test')\ntrain_df = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\ntest_df = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')","b1a6cdab":"def return_filpath(name, folder=train_dir):\n    path = os.path.join(folder, name[0], f'{name}.npy')\n    return path","c63d107c":"train_df['image_path'] = train_df['id'].apply(lambda x: return_filpath(x))\ntest_df['image_path'] = test_df['id'].apply(lambda x: return_filpath(x, folder=test_dir))\ntrain_df.head()","4282f0b9":"ax = plt.subplots(figsize=(12, 6))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='target', data=train_df);\nplt.ylabel(\"No. of Observations\", size=20);\nplt.xlabel(\"Target\", size=20);","ffceedf1":"def get_train_transforms():\n    return albumentations.Compose(\n        [\n            albumentations.Resize(params['size'],params['size']),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=180, p=0.7),\n            albumentations.RandomBrightness(limit=0.6, p=0.5),\n            albumentations.Cutout(\n                num_holes=10, max_h_size=12, max_w_size=12,\n                fill_value=0, always_apply=False, p=0.5\n            ),\n            albumentations.ShiftScaleRotate(\n                shift_limit=0.25, scale_limit=0.1, rotate_limit=0\n            ),\n            ToTensorV2(p=1.0),\n        ]\n    )\n\ndef get_valid_transforms():\n    return albumentations.Compose(\n        [\n            albumentations.Resize(params['size'],params['size']),\n            ToTensorV2(p=1.0)\n        ]\n    )\n\ndef get_test_transforms():\n        return albumentations.Compose(\n            [\n                albumentations.Resize(params['size'],params['size']),\n                ToTensorV2(p=1.0)\n            ]\n        )","0fcdf3d2":"class SETIDataset(Dataset):\n    def __init__(self, images_filepaths, targets, transform=None):\n        self.images_filepaths = images_filepaths\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = np.load(image_filepath).astype(np.float32)\n        image = np.vstack(image).transpose((1, 0))\n            \n        if self.transform is not None:\n            image = self.transform(image=image)[\"image\"]\n        else:\n            image = image[np.newaxis,:,:]\n            image = torch.from_numpy(image).float()\n        \n        label = torch.tensor(self.targets[idx]).float()\n        return image, label","2bc3cee2":"(X_train, X_valid, y_train, y_valid) = train_test_split(train_df['image_path'],\n                                                        train_df['target'],\n                                                        test_size=0.2,\n                                                        stratify=train_df['target'],\n                                                        shuffle=True,\n                                                        random_state=params['seed'])","981c4c4b":"train_dataset = SETIDataset(\n    images_filepaths=X_train.values,\n    targets=y_train.values,\n    transform=get_train_transforms()\n)\n\nvalid_dataset = SETIDataset(\n    images_filepaths=X_valid.values,\n    targets=y_valid.values,\n    transform=get_valid_transforms()\n)","f6a69ff8":"def rand_bbox(W, H, lam):\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    cx = np.random.randint(cut_w \/\/ 2, W - cut_w \/\/ 2)\n    cy = np.random.randint(cut_h \/\/ 2, H - cut_h \/\/ 2)\n\n    bbx1 = cx - cut_w \/\/ 2\n    bby1 = cy - cut_h \/\/ 2\n    bbx2 = cx + cut_w \/\/ 2\n    bby2 = cy + cut_h \/\/ 2\n\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix(x, y, alpha=1.0):\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(params['device'])\n\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size()[1], x.size()[2], lam)\n    x[:, bbx1:bbx2, bby1:bby2] = x[index, bbx1:bbx2, bby1:bby2]\n    y_a, y_b = y, y[index]\n    return x, y_a, y_b, lam\n\ndef cutmix_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)","c7c99522":"class MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] \/ metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"],\n                    float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )\n    \ndef use_roc_score(output, target):\n    try:\n        y_pred = torch.sigmoid(output).cpu()\n        y_pred = y_pred.detach().numpy()\n        target = target.cpu()\n\n        return roc_auc_score(target, y_pred)\n    except:\n        return 0.5","dc31f7b3":"class_counts = y_train.value_counts().to_list()\nnum_samples = sum(class_counts)\nlabels = y_train.to_list()\n\nclass_weights = [num_samples\/class_counts[i] for i in range(len(class_counts))]\nweights = [class_weights[labels[i]] for i in range(int(num_samples))]\nsampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))","7be33f5c":"train_loader = DataLoader(\n    train_dataset, batch_size=params['batch_size'], sampler = sampler,\n    num_workers=params['num_workers'], pin_memory=True)\n\nval_loader = DataLoader(\n    valid_dataset, batch_size=params['batch_size'], shuffle=False,\n    num_workers=params['num_workers'], pin_memory=True)","02738e59":"class SwinNet(nn.Module):\n    def __init__(self, model_name=params['model'], out_features=params['out_features'],\n                 inp_channels=params['inp_channels'], pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained,\n                                       in_chans=inp_channels)\n        n_features = self.model.head.in_features\n        self.model.head = nn.Linear(n_features, out_features, bias=True)    \n    \n    def forward(self, x):\n        x = self.model(x)\n        return x","abb96cd7":"model = SwinNet()\nmodel = model.to(params['device'])\ncriterion = nn.BCEWithLogitsLoss().to(params['device'])\noptimizer = torch.optim.Adam(model.parameters(), lr=params['lr'],\n                             weight_decay=params['weight_decay'],\n                             amsgrad=False)\n\nscheduler = CosineAnnealingLR(optimizer,\n                              T_max=params['T_max'],\n                              eta_min=params['min_lr'],\n                              last_epoch=-1)","89ecaf3c":"def train(train_loader, model, criterion, optimizer, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(train_loader)\n    scaler = amp.GradScaler()\n       \n    for i, (images, target) in enumerate(stream, start=1):\n\n        images = images.to(params['device'])\n        target = target.to(params['device']).float().view(-1, 1)\n        images, targets_a, targets_b, lam = cutmix(images, target.view(-1, 1))\n        \n        with amp.autocast(enabled=True):\n            output = model(images)\n            loss = cutmix_criterion(criterion, output, targets_a, targets_b, lam)\n            \n        accelerator.backward(scaler.scale(loss))\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n        \n        roc_score = use_roc_score(output, target)\n        metric_monitor.update('Loss', loss.item())\n        metric_monitor.update('ROC', roc_score)\n        wandb.log({\"Train Epoch\":epoch,\"Train loss\": loss.item(), \"Train ROC\":roc_score})\n        \n\n        stream.set_description(\n            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(\n                epoch=epoch,\n                metric_monitor=metric_monitor)\n        )","19af979f":"def validate(val_loader, model, criterion, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(val_loader)\n    final_targets = []\n    final_outputs = []\n    with torch.no_grad():\n        for i, (images, target) in enumerate(stream, start=1):\n            images = images.to(params['device'], non_blocking=True)\n            target = target.to(params['device'], non_blocking=True).float().view(-1, 1)\n            output = model(images)\n            loss = criterion(output, target)\n            roc_score = use_roc_score(output, target)\n            metric_monitor.update('Loss', loss.item())\n            metric_monitor.update('ROC', roc_score)\n            wandb.log({\"Valid Epoch\": epoch, \"Valid loss\": loss.item(), \"Valid ROC\":roc_score})\n            stream.set_description(\n                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(\n                    epoch=epoch,\n                    metric_monitor=metric_monitor)\n            )\n            \n            targets = target.detach().cpu().numpy().tolist()\n            outputs = output.detach().cpu().numpy().tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(outputs)\n    return final_outputs, final_targets","5792dc60":"best_roc = -np.inf\nbest_epoch = -np.inf\nbest_model_name = None\n\nfor epoch in range(1, params['epochs'] + 1):\n    \n    run = wandb.init(project='Seti-Swin', \n                 config=params, \n                 job_type='train',\n                 name = 'Swin Transformer')\n    \n    train(train_loader, model, criterion, optimizer, epoch, params)\n    predictions, valid_targets = validate(val_loader, model, criterion, epoch, params)\n    roc_auc = round(roc_auc_score(valid_targets, predictions), 3)\n    torch.save(model.state_dict(),f\"{params['model']}_{epoch}_epoch_{roc_auc}_roc_auc.pth\")\n    \n    if roc_auc > best_roc:\n        best_roc = roc_auc\n        best_epoch = epoch\n        best_model_name = f\"{params['model']}_{epoch}_epoch_{roc_auc}_roc_auc.pth\"\n        \n    scheduler.step()\n    ","af8f111a":"print(f'The best ROC: {best_roc} was achieved on epoch: {best_epoch}.')\nprint(f'The Best saved model is: {best_model_name}') ","7e550ca3":"model = SwinNet()\nmodel.load_state_dict(torch.load(best_model_name))\nmodel = model.to(params['device'])","1ea1846b":"model.eval()\npredicted_labels = None\nfor i in range(params['num_tta']):\n    test_dataset = SETIDataset(\n        images_filepaths = test_df['image_path'].values,\n        targets = test_df['target'].values,\n        transform = get_test_transforms()\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=params['batch_size'],\n        shuffle=False, num_workers=params['num_workers'],\n        pin_memory=True\n    )\n    \n    temp_preds = None\n    with torch.no_grad():\n        for (images, target) in tqdm(test_loader):\n            images = images.to(params['device'], non_blocking=True)\n            output = model(images)\n            predictions = torch.sigmoid(output).cpu().numpy()\n            if temp_preds is None:\n                temp_preds = predictions\n            else:\n                temp_preds = np.vstack((temp_preds, predictions))\n    \n    if predicted_labels is None:\n        predicted_labels = temp_preds\n    else:\n        predicted_labels += temp_preds\n        \npredicted_labels \/= params['num_tta']","23782b65":"torch.save(model.state_dict(), f\"{params['model']}_{best_epoch}epochs_weights.pth\")","cd05d116":"sub_df = pd.DataFrame()\nsub_df['id'] = test_df['id']\nsub_df['target'] = predicted_labels","7e353f55":"sub_df.head()","5fa14a70":"sub_df.to_csv('submission.csv', index=False)","992a4135":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Import Libraries<\/p>","d79fcfd4":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">That's it for today folks!<br> A huge shoutout to <a href = 'https:\/\/www.kaggle.com\/manabendrarout\/nfnet-pytorch-starter-lb-0-95'>manabendrarou <\/a>for his excellent Pytorch starter kernel<br>Go Give it some love!<\/p>\n\n","ee646b51":"<p p style = \"font-family: garamond; font-size:40px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">What are we discussing today? <\/p>\n <p p style = \"font-family: garamond; font-size:40px; font-style: normal;background-color: #f6f5f5; color :#006699; border-radius: 10px 10px; text-align:center\">  \n CutMix <br>\n AMP + Gradient Scaling <br>\n Weighted Random Sampler <br>\n HuggingFace Accelerate <br>\n Weights and Biases\n \n","7b9a2311":"![](https:\/\/miro.medium.com\/max\/4176\/1*IR3uTsclxKdzKIXDlTiVgg.png)","f0fd2bf4":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Train and Validation Data<\/p>","3f539898":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Configurations\/Parameters<\/p>","be47ec8f":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Swin Transformer<\/p>","94ba667e":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Test Time Augmentation<\/p>\n\n<p style = \"font-family: garamond; font-size: 25px; font-style: normal; border-radius: 10px 10px; text-align:center\">Similar to what Data Augmentation is doing to the training set, the purpose of Test Time Augmentation is to perform random modifications to the test images. Thus, instead of showing the regular, \u201cclean\u201d images, only once to the trained model, we will show it the augmented images several times. We will then average the predictions of each corresponding image and take that as our final guess.<br>\n    The reason why it works is that, by averaging our predictions, on randomly modified images, we are also averaging the errors. The error can be big in a single vector, leading to a wrong answer, but when averaged, only the correct answer stand out.<br><br> Here I'll be taking a TTA of 1 as I dont have enough Gpu hours left but feel free to experiment and evaluate the performance with different number of tta's<\/p>","430cc78a":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Loss Function, Optimizer and Scheduler<\/p>","b55c7770":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">CutMix<\/p>\n\n<p style = \"font-family: garamond; font-size: 25px; font-style: normal; border-radius: 10px 10px; text-align:center\">CutMix is an image data augmentation strategy. Instead of simply removing pixels as in Cutout, we replace the removed regions with a patch from another image. The ground truth labels are also mixed proportionally to the number of pixels of combined images. The added patches further enhance localization ability by requiring the model to identify the object from a partial view.<\/p>","133a09a6":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Running Train and Evaluation and Monitoring on Weights and Biases<\/p>","e01509c5":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">TIMM Pytorch Models<\/p>","ebad783a":"![](https:\/\/preview.ibb.co\/kH61v0\/pipeline.png)","5b450695":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Seed for Reproducibility<\/p>","96f19aa7":"<p style = \"font-family: garamond; font-size: 30px; font-style: normal; border-radius: 10px 10px; text-align:center\">Accelerate by HuggingFace \ud83e\udd17 <\/p><br> <p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Accelerate provides an easy API to make your scripts run with mixed precision and on any kind of distributed setting (multi-GPUs, TPUs etc.) while still letting you write your own training loop. The same code can then runs seamlessly on your local machine for debugging or your training environment. <br> In 5 Lines of code we can run our scripts on any distributed setting! <\/p>\n\n<p style = \"font-family: garamond; font-size: 30px; font-style: normal; border-radius: 10px 10px; text-align:center\">Weights & Biases<\/p><br> <p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Wandb is a developer tool for companies turn deep learning research projects into deployed software by helping teams track their models, visualize model performance and easily automate training and improving models.<br> We will use their tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues.  <\/p>","5076bb85":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spacing: 1px; background-color: #f6f5f5; color :#6666ff; border-radius: 200px 200px; text-align:center\">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows<\/h1>\n\n![Swin](http:\/\/raw.githubusercontent.com\/microsoft\/Swin-Transformer\/master\/figures\/teaser.png)\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting selfattention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer\nmake it compatible with a broad range of vision tasks, including image classification <\/p>\n\n","e304ac56":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Submission File<\/p>","a56125fc":"<p p style = \"font-family: garamond; font-size:40px; font-style: normal;background-color: #f6f5f5; color :#ff0066; border-radius: 10px 10px; text-align:center\">Upvote the kernel if you find it insightful!<\/p>","fcad37b7":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Image Augmentation<\/p>","babd2a76":"<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">The dataset is very imbalanced and we will see later how we use a sampler to handle it. <\/p>","f072e8b5":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">TOOLS<\/p>","78f61b88":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Weighted Random Sampler<\/p>\n\n<p style = \"font-family: garamond; font-size: 25px; font-style: normal; border-radius: 10px 10px; text-align:center\">Samples elements from [0 ,.., len(weights)-1] with given probabilities (weights).<\/p>","5b839b11":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Validation Results<\/p>\n\n<p style = \"font-family: garamond; font-size: 25px; font-style: normal; border-radius: 10px 10px; text-align:center\">We are able to achieve a ROC score of 96.2 in just 5 epochs!<br><br> Weights & Biases provides us with such easy to use interface and tools to keep a track of our Evaluation metrics like training and validation loss and Roc along with other resources like Gpu usage.<\/p>","dba13896":"<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">PyTorch Image Models (timm) is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders \/ augmentations, and reference training \/ validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.\nUsing timm we will create the Swin Transformer model for our problem statement. We will be using the swin small patch4 window7 224 pretrained model. <\/p>","3f1e8978":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Minimal EDA<\/p>","b2b43260":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Custom Class for Monitoring Loss and ROC<\/p>","5ffef254":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Custom Dataset<\/p>","dd1b688b":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Mixed Precision Training<\/p>\n\n<p style = \"font-family: garamond; font-size: 25px; font-style: normal; border-radius: 10px 10px; text-align:center\">\ud83d\udccd Automatic Mixed Precision <br><br> AMP provides convenience methods for mixed precision, where some operations use the torch.float32 (float) datatype and other operations use torch.float16 (half). <\/p>\n\n<p style = \"font-family: garamond; font-size: 25px; font-style: normal; border-radius: 10px 10px; text-align:center\">\ud83d\udccd Autocasting <br><br>Instances of autocast serve as context managers or decorators that allow regions of your script to run in mixed precision. autocast should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended.<\/p>\n\n<p style = \"font-family: garamond; font-size: 25px; font-style: normal; border-radius: 10px 10px; text-align:center\">\ud83d\udccd Gradient Scaling <br><br> If the forward pass for a particular op has float16 inputs, the backward pass for that op will produce float16 gradients. Gradient values with small magnitudes may not be representable in float16. These values will flush to zero (\u201cunderflow\u201d), so the update for the corresponding parameters will be lost. <br>\n    To prevent underflow, \u201cgradient scaling\u201d multiplies the network\u2019s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don\u2019t flush to zero.<\/p>\n"}}