{"cell_type":{"1a16b5de":"code","4aa8531e":"code","e9442f8f":"code","905aaf0f":"code","084f1d3a":"code","2d7e0d95":"code","e6ac087a":"code","721fb5a5":"code","a4288774":"code","92c49b9e":"code","c769ab69":"code","dcadd41c":"code","e841a239":"code","dd11cfe2":"code","30c1d7af":"code","b56c5e38":"code","0f319be7":"code","111d404c":"code","666b0b43":"code","43696254":"code","735f3f46":"code","66a00529":"code","8fbb9743":"code","cadc74a4":"code","5ff59c41":"code","a88dbfaf":"code","8bc5c966":"code","c9b83d24":"code","4b357c2e":"code","9cd42422":"code","f9b4fd39":"code","92438bc3":"code","30dfc765":"code","0dbc9952":"code","822441a6":"code","ab3b8bc6":"code","1f0cd9bf":"markdown","510b3d70":"markdown","c8b3c4f9":"markdown","3a54720a":"markdown","10be6581":"markdown","546d17e0":"markdown","edcaf21b":"markdown","fa15d394":"markdown"},"source":{"1a16b5de":"import os\nimport random\nimport glob\nimport shutil\nimport warnings\n\nimport cv2\nimport scipy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xml.etree.ElementTree as ET\nimport albumentations as A\nfrom sklearn.preprocessing import LabelEncoder\nfrom PIL import Image\nfrom albumentations.pytorch import ToTensor\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils import spectral_norm\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import make_grid, save_image\n\n\n%matplotlib inline\nwarnings.filterwarnings('ignore', category=FutureWarning)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","4aa8531e":"config = {'DataLoader': {'batch_size': 64,\n                         'shuffle': True},\n          'Generator': {'latent_dim': 120,\n                        'embed_dim': 32,\n                        'ch': 64,\n                        'num_classes': 120,\n                        'use_attn': True},\n          'Discriminator': {'ch': 64,\n                            'num_classes': 120,\n                            'use_attn': True},\n          'sample_latents': {'latent_dim': 120,\n                             'num_classes': 120},\n          'num_iterations': 50000,\n          'd_steps': 1,\n          'lr_G': 2e-4,\n          'lr_D': 8e-4,\n          'betas': (0.0, 0.999),\n          'margin': 1.0,\n          'gamma': 0.1,\n          'ema': 0.999,\n          'seed': 42}","e9442f8f":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(config['seed'])","905aaf0f":"root_images = '..\/input\/all-dogs\/all-dogs\/'\nroot_annots = '..\/input\/annotation\/Annotation\/'","084f1d3a":"all_files = os.listdir(root_images)","2d7e0d95":"breeds = glob.glob(root_annots+'*')\nannotations = []\nfor breed in breeds:\n    annotations += glob.glob(breed+'\/*')","e6ac087a":"breed_map = {}\nfor annotation in annotations:\n    breed = annotation.split('\/')[-2]\n    index = breed.split('-')[0]\n    breed_map.setdefault(index, breed)","721fb5a5":"all_labels = [breed_map[file.split('_')[0]] for file in all_files]\nle = LabelEncoder()\nall_labels = le.fit_transform(all_labels)","a4288774":"def load_bbox(file):\n    file = str(breed_map[file.split('_')[0]]) + '\/' + str(file.split('.')[0])\n    path = os.path.join(root_annots, file)\n    tree = ET.parse(path)\n    root = tree.getroot()\n    objects = root.findall('object')\n    for o in objects:\n        bndbox = o.find('bndbox')\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n    \n    return (xmin, ymin, xmax, ymax)","92c49b9e":"all_bboxes = [load_bbox(file) for file in all_files]","c769ab69":"def get_resized_bbox(height, width, bbox):\n    xmin, ymin, xmax, ymax = bbox\n    xlen = xmax - xmin\n    ylen = ymax - ymin\n    \n    if xlen > ylen:\n        diff = xlen - ylen\n        min_pad = min(ymin, diff\/\/2)\n        max_pad = min(height-ymax, diff-min_pad)\n        ymin = ymin - min_pad\n        ymax = ymax + max_pad\n\n    elif ylen > xlen:\n        diff = ylen - xlen\n        min_pad = min(xmin, diff\/\/2)\n        max_pad = min(width-xmax, diff-min_pad)\n        xmin = xmin - min_pad\n        xmax = xmax + max_pad\n    \n    return xmin, ymin, xmax, ymax","dcadd41c":"resized_bboxes = []\nfor file, bbox in zip(all_files, all_bboxes):\n    img = Image.open(os.path.join(root_images, file))\n    width, height = img.size\n    xmin, ymin, xmax, ymax = get_resized_bbox(height, width, bbox)\n    resized_bboxes.append((xmin, ymin, xmax, ymax))","e841a239":"def load_bboxcrop_resized_image(file, bbox):\n    img = cv2.imread(os.path.join(root_images, file))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    xmin, ymin, xmax, ymax = bbox\n    img = img[ymin:ymax,xmin:xmax]\n\n    transform = A.Compose([A.Resize(64, 64, interpolation=cv2.INTER_AREA),\n                           A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    img = transform(image=img)['image']\n    \n    return img","dd11cfe2":"all_images = [load_bboxcrop_resized_image(f, b) for f, b in zip(all_files, resized_bboxes)]\nall_images = np.array(all_images)","30c1d7af":"class DogDataset(Dataset):\n    def __init__(self, images, labels):\n        super().__init__()\n        self.images = images\n        self.labels = labels\n        self.transform = A.Compose([A.HorizontalFlip(p=0.5), ToTensor()])\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img, label = self.images[idx], self.labels[idx]\n        img = self.transform(image=img)['image']\n        label = torch.as_tensor(label, dtype=torch.long)\n\n        return img, label","b56c5e38":"def get_dataiterator(images, labels, dataloader_params, device='cpu'):\n    train_dataset = DogDataset(images, labels)\n    train_dataloader = DataLoader(train_dataset, **dataloader_params)\n\n    while True:\n        for imgs, labels in train_dataloader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            imgs += (1.0 \/ 128.0) * torch.rand_like(imgs)\n\n            yield imgs, labels","0f319be7":"class Attention(nn.Module):\n    def __init__(self, channels, reduction_attn=8, reduction_sc=2):\n        super().__init__()\n        self.channles_attn = channels \/\/ reduction_attn\n        self.channels_sc = channels \/\/ reduction_sc\n        \n        self.conv_query = spectral_norm(nn.Conv2d(channels, self.channles_attn, kernel_size=1, bias=False))\n        self.conv_key = spectral_norm(nn.Conv2d(channels, self.channles_attn, kernel_size=1, bias=False))\n        self.conv_value = spectral_norm(nn.Conv2d(channels, self.channels_sc, kernel_size=1, bias=False))\n        self.conv_attn = spectral_norm(nn.Conv2d(self.channels_sc, channels, kernel_size=1, bias=False))\n        self.gamma = nn.Parameter(torch.zeros(1))\n        \n        nn.init.orthogonal_(self.conv_query.weight.data)\n        nn.init.orthogonal_(self.conv_key.weight.data)\n        nn.init.orthogonal_(self.conv_value.weight.data)\n        nn.init.orthogonal_(self.conv_attn.weight.data)\n\n    def forward(self, x):\n        batch, _, h, w = x.size()\n        \n        proj_query = self.conv_query(x).view(batch, self.channles_attn, -1)\n        proj_key = F.max_pool2d(self.conv_key(x), 2).view(batch, self.channles_attn, -1)\n        \n        attn = torch.bmm(proj_key.permute(0,2,1), proj_query)\n        attn = F.softmax(attn, dim=1)\n        \n        proj_value = F.max_pool2d(self.conv_value(x), 2).view(batch, self.channels_sc, -1)\n        attn = torch.bmm(proj_value, attn)\n        attn = attn.view(batch, self.channels_sc, h, w)\n        attn = self.conv_attn(attn)\n        \n        out = self.gamma * attn + x\n        \n        return out","111d404c":"class CBN2d(nn.Module):\n    def __init__(self, num_features, num_conditions):\n        super().__init__()\n        self.bn = nn.BatchNorm2d(num_features, affine=False)\n        self.embed = spectral_norm(nn.Conv2d(num_conditions, num_features*2, kernel_size=1, bias=False))\n        \n        nn.init.orthogonal_(self.embed.weight.data)\n\n    def forward(self, x, y):\n        out = self.bn(x)\n        embed = self.embed(y.unsqueeze(2).unsqueeze(3))\n        gamma, beta = embed.chunk(2, dim=1)\n        out = (1.0 + gamma) * out + beta \n\n        return out","666b0b43":"class GBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, num_conditions, upsample=False):\n        super().__init__()\n        self.upsample = upsample\n        self.learnable_sc = in_channels != out_channels or upsample\n        \n        self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False))\n        self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False))\n        self.cbn1 = CBN2d(in_channels, num_conditions)\n        self.cbn2 = CBN2d(out_channels, num_conditions)\n        if self.learnable_sc:\n            self.conv_sc = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False))\n        self.relu = nn.ReLU()\n\n        nn.init.orthogonal_(self.conv1.weight.data)\n        nn.init.orthogonal_(self.conv2.weight.data)\n        if self.learnable_sc:\n            nn.init.orthogonal_(self.conv_sc.weight.data)\n    \n    def _upsample_conv(self, x, conv):\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n        x = conv(x)\n        \n        return x\n    \n    def _residual(self, x, y):\n        x = self.relu(self.cbn1(x, y))\n        x = self._upsample_conv(x, self.conv1) if self.upsample else self.conv1(x)\n        x = self.relu(self.cbn2(x, y))\n        x = self.conv2(x)\n        \n        return x\n    \n    def _shortcut(self, x):\n        if self.learnable_sc:\n            x = self._upsample_conv(x, self.conv_sc) if self.upsample else self.conv_sc(x)\n            \n        return x\n    \n    def forward(self, x, y):\n        return self._shortcut(x) + self._residual(x, y)","43696254":"class Generator(nn.Module):\n    def __init__(self, latent_dim, ch, num_classes, embed_dim, use_attn=False):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.ch = ch\n        self.num_classes = num_classes\n        self.embed_dim = embed_dim\n        self.use_attn = use_attn\n        self.num_chunk = 5\n        num_latents = self.__get_num_latents()\n        \n        self.embed = nn.Embedding(num_classes, embed_dim)\n        self.fc = spectral_norm(nn.Linear(num_latents[0], ch*8*4*4, bias=False))\n        self.block1 = GBlock(ch*8, ch*8, num_latents[1], upsample=True)\n        self.block2 = GBlock(ch*8, ch*4, num_latents[2], upsample=True)\n        self.block3 = GBlock(ch*4, ch*2, num_latents[3], upsample=True)\n        if use_attn:\n            self.attn = Attention(ch*2)\n        self.block4 = GBlock(ch*2, ch, num_latents[4], upsample=True)\n        self.bn = nn.BatchNorm2d(ch)\n        self.relu = nn.ReLU()\n        self.conv_last = spectral_norm(nn.Conv2d(ch, 3, kernel_size=3, padding=1, bias=False))\n        self.tanh = nn.Tanh()\n        \n        nn.init.orthogonal_(self.embed.weight.data)\n        nn.init.orthogonal_(self.fc.weight.data)\n        nn.init.orthogonal_(self.conv_last.weight.data)\n        nn.init.constant_(self.bn.weight.data, 1.0)\n        nn.init.constant_(self.bn.bias.data, 0.0)\n    \n    def __get_num_latents(self):\n        xs = torch.empty(self.latent_dim).chunk(self.num_chunk)\n        num_latents = [x.size(0) for x in xs]\n        for i in range(1, self.num_chunk):\n            num_latents[i] += self.embed_dim\n        \n        return num_latents\n    \n    def forward(self, x, y):\n        xs = x.chunk(self.num_chunk, dim=1)\n        y = self.embed(y)\n        \n        h = self.fc(xs[0])\n        h = h.view(h.size(0), self.ch*8, 4, 4)\n        h = self.block1(h, torch.cat([y, xs[1]], dim=1))\n        h = self.block2(h, torch.cat([y, xs[2]], dim=1))\n        h = self.block3(h, torch.cat([y, xs[3]], dim=1))\n        if self.use_attn:\n            h = self.attn(h)\n        h = self.block4(h, torch.cat([y, xs[4]], dim=1))\n        h = self.relu(self.bn(h))\n        out = self.tanh(self.conv_last(h))\n        \n        return out","735f3f46":"class DBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, downsample=False, optimized=False):\n        super().__init__()\n        self.downsample = downsample\n        self.optimized = optimized\n        self.learnable_sc = in_channels != out_channels or downsample\n        \n        self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False))\n        self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False))\n        if self.learnable_sc:\n            self.conv_sc = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False))\n        self.relu = nn.ReLU()\n        \n        nn.init.orthogonal_(self.conv1.weight.data)\n        nn.init.orthogonal_(self.conv2.weight.data)\n        if self.learnable_sc:\n            nn.init.orthogonal_(self.conv_sc.weight.data)\n    \n    def _residual(self, x):\n        if not self.optimized:\n            x = self.relu(x)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        if self.downsample:\n            x = F.avg_pool2d(x, 2)\n        \n        return x\n    \n    def _shortcut(self, x):\n        if self.learnable_sc:\n            if self.optimized:\n                x = self.conv_sc(F.avg_pool2d(x, 2)) if self.downsample else self.conv_sc(x)\n            else:\n                x = F.avg_pool2d(self.conv_sc(x), 2) if self.downsample else self.conv_sc(x)\n        \n        return x\n    \n    def forward(self, x):\n        return self._shortcut(x) + self._residual(x)","66a00529":"class Discriminator(nn.Module):\n    def __init__(self, ch, num_classes, use_attn=False):\n        super().__init__()\n        self.ch = ch\n        self.num_classes = num_classes\n        self.use_attn = use_attn\n        \n        self.block1 = DBlock(3, ch, downsample=True, optimized=True)\n        if use_attn:\n            self.attn = Attention(ch)\n        self.block2 = DBlock(ch, ch*2, downsample=True)\n        self.block3 = DBlock(ch*2, ch*4, downsample=True)\n        self.block4 = DBlock(ch*4, ch*8, downsample=True)\n        self.relu = nn.ReLU()\n        self.fc = spectral_norm(nn.Linear(ch*8, 1, bias=False))\n        self.embed = spectral_norm(nn.Embedding(num_classes, ch*8))\n        self.clf = spectral_norm(nn.Linear(ch*8, num_classes, bias=False))\n        \n        nn.init.orthogonal_(self.fc.weight.data)\n        nn.init.orthogonal_(self.embed.weight.data)\n        nn.init.orthogonal_(self.clf.weight.data)\n    \n    def forward(self, x, y):\n        h = self.block1(x)\n        if self.use_attn:\n            h = self.attn(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.relu(h)\n        h = torch.sum(h, dim=(2,3))\n        \n        out = self.fc(h)\n        out += torch.sum(self.embed(y)*h, dim=1, keepdim=True)\n        \n        ac = self.clf(h)\n        ac = F.log_softmax(ac, dim=1)\n        \n        return out, ac","8fbb9743":"train_dataiterator = get_dataiterator(all_images, all_labels, config['DataLoader'], device=device)","cadc74a4":"netG = Generator(**config['Generator']).to(device, torch.float32)\nnetD = Discriminator(**config['Discriminator']).to(device, torch.float32)","5ff59c41":"netGE = Generator(**config['Generator']).to(device, torch.float32)\nnetGE.load_state_dict(netG.state_dict());","a88dbfaf":"optim_G = Adam(params=netG.parameters(), lr=config['lr_G'], betas=config['betas'])\noptim_D = Adam(params=netD.parameters(), lr=config['lr_D'], betas=config['betas'])","8bc5c966":"def calc_advloss_D(real, fake, margin=1.0):\n    loss_real = torch.mean((real - fake.mean() - margin) ** 2)\n    loss_fake = torch.mean((fake - real.mean() + margin) ** 2)\n    loss = (loss_real + loss_fake) \/ 2\n    \n    return loss","c9b83d24":"def calc_advloss_G(real, fake, margin=1.0):\n    loss_real = torch.mean((real - fake.mean() + margin) ** 2)\n    loss_fake = torch.mean((fake - real.mean() - margin) ** 2)\n    loss = (loss_real + loss_fake) \/ 2\n    \n    return loss","4b357c2e":"criterion = nn.NLLLoss().to(device, torch.float32)","9cd42422":"def sample_latents(batch_size, latent_dim, num_classes):\n    latents = torch.randn((batch_size, latent_dim), dtype=torch.float32, device=device)\n    labels = torch.randint(0, num_classes, size=(batch_size,), dtype=torch.long, device=device)\n    \n    return latents, labels","f9b4fd39":"step = 1\n\nwhile True:\n    # Discriminator\n    for i in range(config['d_steps']):\n        for param in netD.parameters():\n            param.requires_grad_(True)\n    \n        optim_D.zero_grad()\n\n        real_imgs, real_labels = train_dataiterator.__next__()\n        batch_size = real_imgs.size(0)\n\n        latents, fake_labels = sample_latents(batch_size, **config['sample_latents'])\n        fake_imgs = netG(latents, fake_labels).detach()\n        \n        preds_real, preds_real_labels = netD(real_imgs, real_labels)\n        preds_fake, _ = netD(fake_imgs, fake_labels)\n\n        loss_D = calc_advloss_D(preds_real, preds_fake, config['margin'])\n        loss_D += config['gamma'] * criterion(preds_real_labels, real_labels)\n        loss_D.backward()\n        optim_D.step()\n\n    # Generator\n    for param in netD.parameters():\n        param.requires_grad_(False)\n\n    optim_G.zero_grad()\n    \n    real_imgs, real_labels = train_dataiterator.__next__()\n    batch_size = real_imgs.size(0)\n    \n    latents, fake_labels = sample_latents(batch_size, **config['sample_latents'])\n    fake_imgs = netG(latents, fake_labels)\n\n    preds_real, _ = netD(real_imgs, real_labels)\n    preds_fake, preds_fake_labels = netD(fake_imgs, fake_labels)\n\n    loss_G = calc_advloss_G(preds_real, preds_fake, config['margin'])\n    loss_G += config['gamma'] * criterion(preds_fake_labels, fake_labels)\n    loss_G.backward()\n    optim_G.step()\n    \n    # Update Generator Eval\n    for param_G, param_GE in zip(netG.parameters(), netGE.parameters()):\n        param_GE.data.mul_(config['ema']).add_((1-config['ema'])*param_G.data)\n    for buffer_G, buffer_GE in zip(netG.buffers(), netGE.buffers()):\n        buffer_GE.data.mul_(config['ema']).add_((1-config['ema'])*buffer_G.data)\n            \n    # stopping\n    if step < config['num_iterations']:\n        step += 1\n    else:\n        print('total step: {}'.format(step))\n        break","92438bc3":"def truncated_normal(size, threshold=2.0, dtype=torch.float32, device='cpu'):\n    x = scipy.stats.truncnorm.rvs(-threshold, threshold, size=size)\n    x = torch.from_numpy(x).to(device, dtype)\n\n    return x","30dfc765":"def generate_eval_samples(generator, batch_size, latent_dim, num_classes):\n    latents = truncated_normal((batch_size, latent_dim), dtype=torch.float32, device=device)\n    labels =  torch.randint(0, num_classes, size=(batch_size,), dtype=torch.long, device=device)\n    \n    with torch.no_grad():\n        imgs = (generator(latents, labels) + 1) \/ 2\n    \n    return imgs","0dbc9952":"def make_submissions(generator, user_images_unzipped_path, latent_dim, num_classes):\n    if not os.path.exists(user_images_unzipped_path):\n        os.mkdir(user_images_unzipped_path)\n    \n    sample_batch_size = 50\n    num_samples = 10000\n    \n    for i in range(0, num_samples, sample_batch_size):\n        imgs = generate_eval_samples(generator, sample_batch_size, latent_dim, num_classes)\n        for j, img in enumerate(imgs):\n            save_image(img, os.path.join(user_images_unzipped_path, f'image_{i+j:05d}.png'))\n    \n    shutil.make_archive('images', 'zip', user_images_unzipped_path)","822441a6":"user_images_unzipped_path = '..\/output_images'\nmake_submissions(netGE, user_images_unzipped_path, **config['sample_latents'])","ab3b8bc6":"torch.save(netGE.state_dict(), '.\/weight.pth')","1f0cd9bf":"# Data Processing","510b3d70":"# Train GANs","c8b3c4f9":"## Load Data","3a54720a":"## Generator","10be6581":"# Settings","546d17e0":"# Generate Samples","edcaf21b":"## Discriminator","fa15d394":"# Models"}}