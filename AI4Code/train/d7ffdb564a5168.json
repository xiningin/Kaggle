{"cell_type":{"955e182e":"code","4adfe005":"code","7cace68b":"code","9f871be9":"code","3c2d9b4c":"code","692467b6":"code","f1791ed4":"code","6de032ad":"code","dde1948d":"code","18d95a99":"code","55516a71":"code","fc69c23a":"code","dd0bfe15":"code","5c4a9c48":"code","08c3ef4e":"code","92c317e4":"code","f1ec31e2":"code","a001e043":"code","db75c86f":"code","01495ca7":"code","77507bbc":"code","1d60aa3c":"code","cd8ef396":"code","57dd192c":"code","10c36e05":"code","a971032b":"code","0621011d":"markdown","8bcf2564":"markdown","3f194e3b":"markdown","69a02293":"markdown","5a1697a4":"markdown","53213102":"markdown","250dd83c":"markdown","f8c158d8":"markdown","73bbd242":"markdown","888bbce6":"markdown","10a23d5d":"markdown","5bd88317":"markdown","e81ff855":"markdown","a2721754":"markdown","1336e359":"markdown","9b92b152":"markdown","c1f5e946":"markdown","a8ba03cd":"markdown","f012d219":"markdown","eb752258":"markdown","58e40a27":"markdown"},"source":{"955e182e":"# \nfrom scipy import stats\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight');\n\n# Sci-kit learn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import set_config; set_config(display='diagram')","4adfe005":"data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndata.head()","7cace68b":"data = data.drop([\"Name\", \"PassengerId\", \"Ticket\"], axis=1)","9f871be9":"size_before = len(data)\ndata = data.drop_duplicates()\nsize_after = len(data)\nprint(str(size_before - size_after) + \" duplicates were removed.\")","3c2d9b4c":"100 * data.isnull().sum().sort_values(ascending=False)\/len(data)","692467b6":"data = data.drop([\"Cabin\"], axis=1)","f1791ed4":"data.head()","6de032ad":"plt.bar(x=[\"Didn't survived\", \"Survived\"], height=data[\"Survived\"].value_counts());","dde1948d":"def plot_dist(series=data[\"Fare\"], title=\"Ticket Fare Distribution\"):\n    sns.histplot(series, kde=True, stat='density', discrete=True)\n    sns.despine()\n    plt.title(title);\n    plt.show()\nplot_dist()","18d95a99":"data = data[data[\"Fare\"].between(0, 100)]\nplot_dist(series=data[\"Fare\"])","55516a71":"plot_dist(series=data[\"Age\"], title=\"Age Distribution\")","fc69c23a":"data = data.drop([\"Embarked\", \"Parch\", \"SibSp\"], axis=1)\ndata.head()","dd0bfe15":"X_train = data.drop([\"Survived\"], axis=1).copy()\ny_train = data[\"Survived\"].copy()","5c4a9c48":"pipe_numeric = Pipeline([\n    ('imputer', SimpleImputer(strategy = 'mean'))\n])\npipe_numeric","08c3ef4e":"pipe_multiclass = Pipeline([\n    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n    ('encoder', OneHotEncoder(sparse=False))\n])\npipe_multiclass","92c317e4":"pipe_binary = Pipeline([\n    ('encoder', OneHotEncoder(sparse=False, drop='if_binary'))\n])\npipe_binary","f1ec31e2":"impute_and_encode = ColumnTransformer([\n    ('binary', pipe_binary, [\"Sex\"]),\n    ('numeric', pipe_numeric, [\"Age\", \"Fare\"]),\n    ('multiclass', pipe_multiclass, [\"Pclass\"])])\nimpute_and_encode","a001e043":"preprocessor = Pipeline([(\"preproc\", impute_and_encode), \n                         (\"scaler\", StandardScaler())])","db75c86f":"preprocessor.fit(X_train)","01495ca7":"output_pipe_columns = [\"Sex\", \"Age\", \"Fare\", \"Pclass1\", \"Pclass2\", \"Pclass3\"]","77507bbc":"X_train_scaled = pd.DataFrame(preprocessor.fit_transform(X_train), columns=output_pipe_columns)\nX_train_scaled.head()","1d60aa3c":"final_pipe = Pipeline([\n    (\"preprocessor\", preprocessor),\n    ('classifier', GradientBoostingClassifier())])\nfinal_pipe","cd8ef396":"cv_baseline_GBC = cross_validate(final_pipe, X_train, y_train, scoring= \"accuracy\", cv=15)\nprint(\"Baseline gradient boosting classifier model accuracy: \" + str(round(cv_baseline_GBC[\"test_score\"].mean()*100, 2)) + \"%\")","57dd192c":"grid_GBC = {'classifier__loss': ['exponential'],#, 'deviance'],\n            'classifier__learning_rate': stats.loguniform(0.01, 1),\n            'classifier__n_estimators': stats.randint(1, 500)\n            }\n\nsearch_GBC = RandomizedSearchCV(final_pipe,\n                                grid_GBC,\n                                scoring='accuracy',\n                                n_iter=100,\n                                cv=5,\n                                n_jobs=-1,\n                                verbose=True)\n\nsearch_GBC.fit(X_train, y_train);","10c36e05":"search_GBC.best_params_","a971032b":"print(\"Tuned gradient boosting classifier model accuracy: \" + str(round(search_GBC.best_score_*100, 2)) + \"%\")","0621011d":"<div style=\"font-weight:700\">As written at the beginning of the notebook, I remove some features with no further analyses.<\/div>","8bcf2564":"This workbook is the titanic passenger dataset analysis to predict whether or not a passenger will survive from RMS Titanic disaster.\n\nIt is a Kaggle competition available <a href=\"https:\/\/www.kaggle.com\/c\/titanic\" target=\"_blank\">HERE<\/a>.\n\n<i><blockquote><b>The Challenge<\/b><\/blockquote><\/i>\n    \n<blockquote>The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).<\/blockquote>\n<figcaption>Kaggle<\/figcaption>","3f194e3b":"<div style=\"font-weight:700\">Cabin feature has 74% missing value, we remove it.<\/div>","69a02293":"## Pipeline for binary features","5a1697a4":"# 4. Missing\n<div style=\"font-weight:700\">Droping features that have too many missing values<\/div>","53213102":"# 7. Baseline Accuracy for GradientBoostingClassifier","250dd83c":"## Pipeline for numeric features","f8c158d8":"<div style=\"font-weight:700\">Let's have an overview of the preprocessor pipeline output<\/div>","73bbd242":"<div style=\"font-weight:700\">Let's add a model to the pipeline: GradientBoostingClassifier<\/div>","888bbce6":"## Impute and encode pipeline combination","10a23d5d":"<div style=\"font-size:35pt; font-weight:700;margin-top:50px;margin-bottom:50px;color:royalblue; text-align:center;width:800px;line-height:20pt\">Kaggle Titanic Challenge<\/div>","5bd88317":"# 6. Pipelines","e81ff855":"<div style=\"font-weight:700\">The output columns will be in the order of appearance in the ColumnTranformer pipe<\/div>","a2721754":"# 8. RandomizedSearchCV for an optimized model","1336e359":"# 1. Import","9b92b152":"# 3. Duplicates","c1f5e946":"# 2. Dropping useless features\n<div style=\"font-weight:700\">These features can't be used in algorithms: Tickets seems to be random, Names and PassengerId will not be used <\/div>","a8ba03cd":"## Preprocessor pipeline","f012d219":"Note that this analysis is not extremely deep, it is intentionally simplified and some features are removed without further analysis for the sake of simplicity as I used the dataset to train at building a package and deploy an app online. The app is available <a href=\"https:\/\/titanic-survivor-vbnnt.herokuapp.com\/\" target=\"_blank\">HERE<\/a>.","eb752258":"# 5. Preparing model inputs","58e40a27":"## Pipeline for multiclass features"}}