{"cell_type":{"cbb80535":"code","6c8ee839":"code","79dd0b49":"code","801617e5":"code","23f6ca5c":"code","ea9702fd":"code","12f38a31":"code","1282e263":"code","e680a969":"code","27922707":"code","3f3ca518":"code","49289ad8":"code","b8fe33e2":"code","99b56cc6":"code","d282238f":"markdown"},"source":{"cbb80535":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c8ee839":"import pandas as pd\nimport numpy as np","79dd0b49":"df=pd.read_csv(\"..\/input\/bill_authentication\/bill_authentication.csv\")\ndf.head(5)","801617e5":"df[\"Class\"].value_counts()","23f6ca5c":"X = df.iloc[:, 1:-1].values\ny = df.iloc[:, -1].values","ea9702fd":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=42)","12f38a31":"import matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nDT=DecisionTreeClassifier()\nDT.fit(X_train,y_train)\npredict=DT.predict(X_test)","1282e263":"plt.figure(figsize=(15,10))\nfeature_names=['Variance','Skewness','Curtosis','Entropy']\ntree.plot_tree(DT,feature_names=feature_names,filled=True)","e680a969":"path = DT.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","27922707":"ccp_alphas","3f3ca518":"from sklearn.metrics import accuracy_score\naccuracy_train,accuracy_test=[],[]\nfor i in ccp_alphas:\n    DT_clf=DecisionTreeClassifier(ccp_alpha=i)\n    DT_clf.fit(X_train,y_train)\n    y_pred_train=DT_clf.predict(X_train)\n    y_pred_test=DT_clf.predict(X_test)\n    accuracy_train.append(accuracy_score(y_train,y_pred_train))\n    accuracy_test.append(accuracy_score(y_test,y_pred_test))","49289ad8":"import seaborn as sns\nsns.set()\nplt.figure(figsize=(14,7))\nsns.lineplot(y=accuracy_train,x=ccp_alphas,label=\"Training Accuracy\")\nsns.lineplot(y=accuracy_test,x=ccp_alphas,label=\"Testing Accuracy \")\nplt.show","b8fe33e2":"DT_clf=DecisionTreeClassifier(ccp_alpha=0.006,random_state=42)\nDT_clf.fit(X_train,y_train)\ny_pred_train=DT_clf.predict(X_train)\ny_pred_test=DT_clf.predict(X_test)\nprint(\"Training Accuracy=\",accuracy_score(y_train,y_pred_train),\"\\n Testing Accuracy=\",accuracy_score(y_test,y_pred_test))","99b56cc6":"plt.figure(figsize=(15,10))\ntree.plot_tree(DT_clf,feature_names=feature_names,filled=True)","d282238f":"### Post pruning decision trees with cost complexity pruning"}}