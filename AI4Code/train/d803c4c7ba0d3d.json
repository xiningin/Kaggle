{"cell_type":{"6932e499":"code","83a0bb37":"code","0a39b494":"code","b59bdeb1":"code","6cfd1e7d":"code","6c756d52":"code","b3ab9e83":"code","c9d4c174":"code","7cbd95e3":"code","6adfa491":"code","ac2267ac":"code","7929c6ed":"code","b00e029e":"code","4ef2f3f9":"code","4a0ee7b0":"code","5178c672":"code","491d030f":"code","2db16c88":"code","25fbe9e4":"code","de2b51f4":"code","698323de":"code","c3ff6a38":"code","5fad886d":"code","e641a8a2":"code","2c2b6a37":"code","9cd0877c":"code","5ee71417":"code","f4aa97d8":"code","fbfedbe5":"code","3fd3642c":"code","80ffd240":"code","3b05a1fc":"code","be6b4ab4":"code","7f53751e":"code","7ef7e92c":"code","8f9bac78":"code","bc05707c":"code","5a273393":"code","509d8c2d":"code","a01d5ef8":"code","061a4ba2":"code","407b86c0":"code","717b7272":"code","27830478":"code","6cdbe950":"code","d4b50764":"code","38cf6392":"code","2489717f":"code","70b64cac":"code","801cbf5c":"code","854733f4":"code","24df4fe7":"code","4536867a":"markdown","dc82bfd1":"markdown","87ca6ed9":"markdown","c1715905":"markdown","1235b9c0":"markdown","219be277":"markdown","178ef134":"markdown","c9342e67":"markdown","a404ffd3":"markdown","ba29187a":"markdown","df38e4c9":"markdown","ceac730e":"markdown","9202a76c":"markdown","a0d77da2":"markdown","29772b74":"markdown","d54a6d8b":"markdown","3d9fe16b":"markdown","0d3b0c20":"markdown","bce90ccb":"markdown","c46d618e":"markdown","36ccb1fe":"markdown","bdca0ae7":"markdown","5d527817":"markdown","a403ee7a":"markdown","810af53d":"markdown","c999ecc7":"markdown","3540f304":"markdown"},"source":{"6932e499":"import numpy as np\nimport pandas as pd\nimport os\nimport random, re, math\nimport tensorflow as tf, tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import optimizers\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.models import Sequential\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.applications import ResNet152V2, InceptionResNetV2, InceptionV3, Xception, VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D,GlobalMaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\nfrom keras import regularizers\n\nimport matplotlib.pyplot as plt\n\n!pip install efficientnet\nimport efficientnet.tfkeras as efn","83a0bb37":"AUTO = tf.data.experimental.AUTOTUNE\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n","0a39b494":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('ocular-disease-recognition-odir5k')","b59bdeb1":"train = pd.read_csv('..\/input\/new-df-csv-oc\/new_df_oc.csv')\ntrain_paths = train.filename.apply(lambda x: GCS_DS_PATH+ '\/ODIR-5K\/ODIR-5K\/Training Images\/' + x).values\ntrain_labels = train.target.values","6cfd1e7d":"train.head(10)","6c756d52":"train=train.drop(columns=['D','M','H','A','O'],axis=1)","b3ab9e83":"train=train[((train['N']== 1) | (train['C'] == 1)| (train['G'] == 1))]","c9d4c174":"train","7cbd95e3":"train,valid = train_test_split(train,test_size = 0.2,random_state = 42)","6adfa491":"BATCH_SIZE = 8* strategy.num_replicas_in_sync\nimg_size = 512\nEPOCHS = 20\nSEED = 42","ac2267ac":"def decode_image(filename, label=None, image_size=(img_size,img_size)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3) \n    image = tf.image.resize(image, image_size)\n    image = tf.cast(image, tf.float32)\n    image = tf.image.per_image_standardization(image)\n    if label is None:\n        return image\n    else:\n        return image, label\n    \ndef preprocess(df,test=False):\n    paths = df.filename.apply(lambda x: GCS_DS_PATH + '\/ODIR-5K\/ODIR-5K\/Training Images\/' + x).values\n    labels = df.loc[:, ['N', 'C', 'G']].values\n    if test==False:\n        return paths,labels\n    else:\n        return paths\n    \ndef data_augment(image, label=None, seed=SEED):\n    image = tf.image.random_flip_left_right(image, seed=seed)\n    image = tf.image.random_flip_up_down(image, seed=seed)\n           \n    if label is None:\n        return image\n    else:\n        return image, label","7929c6ed":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    rotation = math.pi * rotation \/ 180.\n    shear = math.pi * shear \/ 180.\n\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    \n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    zoom_matrix = tf.reshape( tf.concat([one\/height_zoom,zero,zero, zero,one\/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))","b00e029e":"def transform(image,label=None):\n    DIM = img_size\n    XDIM = DIM%2 \n    \n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    h_shift = 8. * tf.random.normal([1],dtype='float32') \n    w_shift = 8. * tf.random.normal([1],dtype='float32') \n  \n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    x = tf.repeat( tf.range(DIM\/\/2,-DIM\/\/2,-1), DIM )\n    y = tf.tile( tf.range(-DIM\/\/2,DIM\/\/2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM\/\/2+XDIM+1,DIM\/\/2)\n              \n    idx3 = tf.stack( [DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n    \n    if label is None:\n        return tf.reshape(d,[DIM,DIM,3])\n    else:\n        return tf.reshape(d,[DIM,DIM,3]),label","4ef2f3f9":"train_dataset = (tf.data.Dataset\n    .from_tensor_slices(preprocess(train))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .map(transform,num_parallel_calls=AUTO)\n    .shuffle(SEED)\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(AUTO))","4a0ee7b0":"test_dataset= (tf.data.Dataset\n    .from_tensor_slices(preprocess(valid))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO))","5178c672":"LR_START = 0.00001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","491d030f":"def categorical_focal_loss(gamma=2., alpha=.25):\n    def categorical_focal_loss_fixed(y_true, y_pred):\n        y_pred \/= K.sum(y_pred, axis=-1, keepdims=True)\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n        cross_entropy = -y_true * K.log(y_pred)\n        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n        return K.sum(loss, axis=1)\n    return categorical_focal_loss_fixed","2db16c88":"with strategy.scope():\n    enet = efn.EfficientNetB7(input_shape=(img_size, img_size, 3),weights='noisy-student',include_top=False)","25fbe9e4":"with strategy.scope():\n    for layer in  enet.layers:\n        layer.trainable = False\n\n    for i in range(-3,0):\n         enet.layers[i].trainable = True","de2b51f4":"with strategy.scope():\n    ef7 =tf.keras.Sequential()\n    ef7.add(enet)\n    ef7.add(tf.keras.layers.MaxPooling2D())\n    ef7.add(tf.keras.layers.Conv2D(4096,3,padding='same'))\n    ef7.add(tf.keras.layers.BatchNormalization())\n    ef7.add(tf.keras.layers.ReLU())\n    ef7.add(tf.keras.layers.GlobalAveragePooling2D())\n    ef7.add(tf.keras.layers.Dropout(0.35))\n    ef7.add(tf.keras.layers.Flatten())\n\n    ef7.add(tf.keras.layers.Dense(2048,activation='relu'))\n    ef7.add(tf.keras.layers.BatchNormalization())\n    ef7.add(tf.keras.layers.LeakyReLU())\n    ef7.add(tf.keras.layers.Dropout(0.35))\n\n    ef7.add(tf.keras.layers.Dense(1024,activation='relu'))\n    ef7.add(tf.keras.layers.BatchNormalization())\n    ef7.add(tf.keras.layers.LeakyReLU())\n    ef7.add(tf.keras.layers.Dropout(0.25))\n    ef7.add(tf.keras.layers.Dense(3,activation='softmax'))\n    ef7.compile(\n                optimizer=tf.optimizers.Adam(lr=0.0001),\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\n                metrics=['categorical_accuracy',\n                        tf.keras.metrics.Recall(),\n                        tf.keras.metrics.Precision(),   \n                        tf.keras.metrics.AUC(),\n                        tfa.metrics.F1Score(num_classes=3, average=\"macro\")\n                       ])\n","698323de":"h7=ef7.fit(\n    train_dataset,\n    steps_per_epoch=train_labels.shape[0] \/\/ BATCH_SIZE,\n    callbacks=[lr_callback],\n    epochs=EPOCHS)","c3ff6a38":"import seaborn as sns\nsns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(h7.epoch,h7.history['categorical_accuracy'], label = 'train')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(h7.epoch,h7.history['loss'], label = 'train')\nplt.title('Loss')\nplt.tight_layout()\nplt.show()","5fad886d":"ef7.evaluate(test_dataset)","e641a8a2":"from sklearn.metrics import confusion_matrix\nclasses=['N','C','G']\nY_pred = ef7.predict(test_dataset)\ntrue_classes = valid.loc[:, ['N','C','G']].values\nprint('Confusion Matrix')\ncm=confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\ncm","2c2b6a37":"import seaborn as sns\nsns.set_style(\"darkgrid\")\nimport itertools\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Oranges):\n    plt.figure(figsize=(6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        cm = np.around(cm, decimals=2)\n        cm[np.isnan(cm)] = 0.0\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","9cd0877c":"plot_confusion_matrix(cm,classes)","5ee71417":"def calculate_sensitivity_specificity(y_test, y_pred_test):\n    actual_pos = y_test == 1\n    actual_neg = y_test == 0\n    \n    true_pos = (y_pred_test == 1) & (actual_pos)\n    false_pos = (y_pred_test == 1) & (actual_neg)\n    true_neg = (y_pred_test == 0) & (actual_neg)\n    false_neg = (y_pred_test == 0) & (actual_pos)\n    \n    # Calculate sensitivity and specificity\n    sensitivity = np.sum(true_pos) \/ np.sum(actual_pos)\n    specificity = np.sum(true_neg) \/ np.sum(actual_neg)\n    \n    return sensitivity, specificity","f4aa97d8":"sensitivity, specificity= calculate_sensitivity_specificity(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\nprint ('Sensitivity:', sensitivity)\nprint ('Specificity:', specificity)","fbfedbe5":"with strategy.scope():\n    DenseNet201 = tf.keras.applications.DenseNet201(input_shape=(512, 512, 3), weights='imagenet', include_top=False)","3fd3642c":"with strategy.scope():\n    for layer in  DenseNet201.layers:\n        layer.trainable = False\n\n    for i in range(-3,0):\n         DenseNet201.layers[i].trainable = True","80ffd240":"with strategy.scope():\n    model_D201=tf.keras.Sequential()\n    model_D201.add(DenseNet201)\n    model_D201.add(tf.keras.layers.MaxPooling2D())\n    model_D201.add(tf.keras.layers.Conv2D(4096,3,padding='same'))\n    model_D201.add(tf.keras.layers.BatchNormalization())\n    model_D201.add(tf.keras.layers.ReLU())\n    model_D201.add(tf.keras.layers.GlobalAveragePooling2D())\n    model_D201.add(tf.keras.layers.Dropout(0.35))\n    model_D201.add(tf.keras.layers.Flatten())\n\n    model_D201.add(tf.keras.layers.Dense(2048,activation='relu'))\n    model_D201.add(tf.keras.layers.BatchNormalization())\n    model_D201.add(tf.keras.layers.LeakyReLU())\n    model_D201.add(tf.keras.layers.Dropout(0.35))\n    \n    model_D201.add(tf.keras.layers.Dense(1024,activation='relu'))\n    model_D201.add(tf.keras.layers.BatchNormalization())\n    model_D201.add(tf.keras.layers.LeakyReLU())\n    model_D201.add(tf.keras.layers.Dropout(0.25))\n    model_D201.add(tf.keras.layers.Dense(3,activation='softmax'))\n    model_D201.compile(\n                optimizer=tf.optimizers.Adam(lr=0.0001),\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\n                metrics=['categorical_accuracy',\n                        tf.keras.metrics.Recall(),\n                        tf.keras.metrics.Precision(),   \n                        tf.keras.metrics.AUC(),\n                        tfa.metrics.F1Score(num_classes=3, average=\"macro\")\n                       ])\n","3b05a1fc":"D_201=model_D201.fit(\n    train_dataset, \n    steps_per_epoch=train_labels.shape[0] \/\/ BATCH_SIZE,\n    callbacks=[lr_callback],\n    epochs=EPOCHS)\n    #validation_data=test_dataset)","be6b4ab4":"import seaborn as sns\nsns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(D_201.epoch,D_201.history['categorical_accuracy'], label = 'train')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(D_201.epoch,D_201.history['loss'], label = 'train')\nplt.title('Loss')\nplt.tight_layout()\nplt.show()","7f53751e":"model_D201.evaluate(test_dataset)","7ef7e92c":"from sklearn.metrics import confusion_matrix,classification_report\nY_pred = model_D201.predict(test_dataset)\ntrue_classes = valid.loc[:, ['N','C','G']].values\nprint('Confusion Matrix')\ncm=confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\ncm","8f9bac78":"plot_confusion_matrix(cm,classes)","bc05707c":"sensitivity, specificity= calculate_sensitivity_specificity(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\nprint ('Sensitivity:', sensitivity)\nprint ('Specificity:', specificity)","5a273393":"from keras.models import Sequential, load_model\nfrom keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D,concatenate,Concatenate,multiply, LocallyConnected2D, Lambda)\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import metrics\nfrom keras.optimizers import Adam \nimport keras\nfrom keras.models import Model\nfrom keras.activations import hard_sigmoid\nCFG = dict(\n    inp_size          = 512,\n    read_size         = 512, \n    crop_size         = 512,\n    net_size          = 512)\n","509d8c2d":"with strategy.scope():\n    in_lay = Input(shape=(CFG['inp_size'], CFG['inp_size'],3))\n    base_model = efn.EfficientNetB7(weights='noisy-student',\n        input_shape=(CFG['inp_size'], CFG['inp_size'],3),\n        include_top=False\n                       )\n    #base_model.load_weights(\"..\/input\/efficientnet-keras-weights-b0b5\/efficientnet-b5_imagenet_1000_notop.h5\")\n    pt_depth = base_model.get_output_shape_at(0)[-1]\n    pt_features = base_model(in_lay)\n    bn_features = BatchNormalization()(pt_features)\n    \n    # ici nous faisons un m\u00e9canisme d'attention pour activer et d\u00e9sactiver les pixels dans le GAP\n    # lidee est baser sur cette explication \n    #1-http:\/\/akosiorek.github.io\/ml\/2017\/10\/14\/visual-attention.html\n    #2-https:\/\/machinelearningmastery.com\/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks\/\n    \n    \n    attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(Dropout(0.5)(bn_features))\n    attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n    attn_layer = Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n    attn_layer = Conv2D(1, \n                        kernel_size = (1,1), \n                        padding = 'valid', \n                        activation = 'sigmoid')(attn_layer)\n    # diffusez sur toutes les cha\u00eenes\n    # kernel_size  d\u00e9termine les dimensions du noyau. Les dimensions courantes comprennent 1\u00d71, 3\u00d73, 5\u00d75 et 7\u00d77, qui peuvent \u00eatre pass\u00e9es en (1, 1), (3, 3), (5, 5) ou (7, 7) tuples.\n    # Il s'agit d'un nombre entier ou d'un tuple\/liste de 2 nombres entiers, sp\u00e9cifiant la hauteur et la largeur de la fen\u00eatre de convolution 2D.\n    #  Ce param\u00e8tre doit \u00eatre un nombre entier impair\n    # pour plus de details sur cette partie (mask et use_bias ... ) il ya  une bonne explication sur geekforgeeks\n    #https:\/\/www.geeksforgeeks.org\/keras-conv2d-class\/\n    \n    up_c2_w = np.ones((1, 1, 1, pt_depth))\n    up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n                   activation = 'linear', use_bias = False, weights = [up_c2_w])\n    up_c2.trainable = False\n    attn_layer = up_c2(attn_layer)\n\n    mask_features = multiply([attn_layer, bn_features])\n    gap_features = GlobalAveragePooling2D()(mask_features)\n    gap_mask = GlobalAveragePooling2D()(attn_layer)\n    \n    # pour tenir compte des valeurs manquantes du mod\u00e8le d'attention\n    # pour bien comprendre resaclegap il ya un bon exemple ici qui explique tellemnt bien cette partie \n    # https:\/\/codefellows.github.io\/sea-python-401d5\/lectures\/rescaling_data.html\n    \n    gap = Lambda(lambda x: x[0]\/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n    gap_dr = Dropout(0.25)(gap)\n    dr_steps = Dropout(0.25)(Dense(128, activation = 'relu')(gap_dr))\n    out_layer = Dense(3, activation = 'softmax')(dr_steps)\n    model = Model(inputs = [in_lay], outputs = [out_layer])  \n    model.compile(\n                optimizer=tf.optimizers.Adam(lr=0.0001),\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\n                metrics=['categorical_accuracy',\n                        tf.keras.metrics.Recall(),\n                        tf.keras.metrics.Precision(),   \n                        tf.keras.metrics.AUC(),\n                        tfa.metrics.F1Score(num_classes=3, average=\"macro\")\n                       ])","a01d5ef8":"ATT_EF7= model.fit(\n    train_dataset, \n    steps_per_epoch=train_labels.shape[0] \/\/ BATCH_SIZE,\n    callbacks=[lr_callback],\n    epochs=EPOCHS)","061a4ba2":"import seaborn as sns\nsns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(ATT_EF7.epoch,ATT_EF7.history['categorical_accuracy'], label = 'train')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(ATT_EF7.epoch,ATT_EF7.history['loss'], label = 'train')\nplt.title('Loss')\nplt.tight_layout()\nplt.show()","407b86c0":"model.evaluate(test_dataset)","717b7272":"from sklearn.metrics import confusion_matrix,classification_report\nY_pred = model.predict(test_dataset)\ntrue_classes = valid.loc[:, ['N','C','G']].values\nprint('Confusion Matrix')\ncm=confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\ncm","27830478":"plot_confusion_matrix(cm,classes)","6cdbe950":"sensitivity, specificity= calculate_sensitivity_specificity(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\nprint ('Sensitivity:', sensitivity)\nprint ('Specificity:', specificity)","d4b50764":"with strategy.scope():\n    in_lay = Input(shape=(CFG['inp_size'], CFG['inp_size'],3))\n    base_model = tf.keras.applications.DenseNet201(weights='imagenet',\n        input_shape=(CFG['inp_size'], CFG['inp_size'],3),\n        include_top=False\n                       )\n    #base_model.load_weights(\"..\/input\/efficientnet-keras-weights-b0b5\/efficientnet-b5_imagenet_1000_notop.h5\")\n    pt_depth = base_model.get_output_shape_at(0)[-1]\n    pt_features = base_model(in_lay)\n    bn_features = BatchNormalization()(pt_features)\n    \n    # ici nous faisons un m\u00e9canisme d'attention pour activer et d\u00e9sactiver les pixels dans le GAP\n    # lidee est baser sur cette explication \n    #1-http:\/\/akosiorek.github.io\/ml\/2017\/10\/14\/visual-attention.html\n    #2-https:\/\/machinelearningmastery.com\/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks\/\n    \n    \n    attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(Dropout(0.5)(bn_features))\n    attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n    attn_layer = Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n    attn_layer = Conv2D(1, \n                        kernel_size = (1,1), \n                        padding = 'valid', \n                        activation = 'sigmoid')(attn_layer)\n    # diffusez sur toutes les cha\u00eenes\n    # kernel_size  d\u00e9termine les dimensions du noyau. Les dimensions courantes comprennent 1\u00d71, 3\u00d73, 5\u00d75 et 7\u00d77, qui peuvent \u00eatre pass\u00e9es en (1, 1), (3, 3), (5, 5) ou (7, 7) tuples.\n    # Il s'agit d'un nombre entier ou d'un tuple\/liste de 2 nombres entiers, sp\u00e9cifiant la hauteur et la largeur de la fen\u00eatre de convolution 2D.\n    #  Ce param\u00e8tre doit \u00eatre un nombre entier impair\n    # pour plus de details sur cette partie (mask et use_bias ... ) il ya  une bonne explication sur geekforgeeks\n    #https:\/\/www.geeksforgeeks.org\/keras-conv2d-class\/\n    \n    up_c2_w = np.ones((1, 1, 1, pt_depth))\n    up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n                   activation = 'linear', use_bias = False, weights = [up_c2_w])\n    up_c2.trainable = False\n    attn_layer = up_c2(attn_layer)\n\n    mask_features = multiply([attn_layer, bn_features])\n    gap_features = GlobalAveragePooling2D()(mask_features)\n    gap_mask = GlobalAveragePooling2D()(attn_layer)\n    \n    # pour tenir compte des valeurs manquantes du mod\u00e8le d'attention\n    # pour bien comprendre resaclegap il ya un bon exemple ici qui explique tellemnt bien cette partie \n    # https:\/\/codefellows.github.io\/sea-python-401d5\/lectures\/rescaling_data.html\n    \n    gap = Lambda(lambda x: x[0]\/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n    gap_dr = Dropout(0.25)(gap)\n    dr_steps = Dropout(0.25)(Dense(128, activation = 'relu')(gap_dr))\n    out_layer = Dense(3, activation = 'softmax')(dr_steps)\n    model_d201 = Model(inputs = [in_lay], outputs = [out_layer])  \n    model_d201.compile(\n                optimizer=tf.optimizers.Adam(lr=0.0001),\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\n                metrics=['categorical_accuracy',\n                        tf.keras.metrics.Recall(),\n                        tf.keras.metrics.Precision(),   \n                        tf.keras.metrics.AUC(),\n                        tfa.metrics.F1Score(num_classes=3, average=\"macro\")\n                       ])","38cf6392":"d201= model_d201.fit(\n    train_dataset, \n    steps_per_epoch=train_labels.shape[0] \/\/ BATCH_SIZE,\n    callbacks=[lr_callback],\n    epochs=EPOCHS)","2489717f":"import seaborn as sns\nsns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(d201.epoch,d201.history['categorical_accuracy'], label = 'train')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(d201.epoch,d201.history['loss'], label = 'train')\nplt.title('Loss')\nplt.tight_layout()\nplt.show()","70b64cac":"model_d201.evaluate(test_dataset)","801cbf5c":"from sklearn.metrics import confusion_matrix,classification_report\nY_pred = model_d201.predict(test_dataset)\ntrue_classes = valid.loc[:, ['N','C','G']].values\nprint('Confusion Matrix')\ncm=confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\ncm","854733f4":"plot_confusion_matrix(cm,classes)","24df4fe7":"sensitivity, specificity= calculate_sensitivity_specificity(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\nprint ('Sensitivity:', sensitivity)\nprint ('Specificity:', specificity)","4536867a":"### 2.3 Normal Vs Cataract","dc82bfd1":"## 2. Pr\u00e9paration de la base de donn\u00e9es\n\n### 2.1 importer les biblioth\u00e8ques n\u00e9cessaires","87ca6ed9":"<center><img src='https:\/\/thecitymagazineelp.com\/wp-content\/uploads\/2019\/03\/AdobeStock_103569152-5BConverted5D-1000.jpg' height=350><\/center>\n<p>\n<h1><center> Normal VS Cataract VS Glaucoma <\/center><\/h1>","c1715905":"# 5. \u00c9tude comparative","1235b9c0":"## 8. Fonction du taux d'apprentissage","219be277":"## 3 Hyperparam\u00e8tre","178ef134":"| Methodes | Loss | Accuracy  | Precision | Recall  |F1_score  |specificity |sensitivity  |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| VGG16 |  0.9876 | 0.9884  | 0.9840|0.9874 |0.9991 |0.9991 |0.9991 |\n| VGG19 | 0.9662  | 0.9807| 0.9484 |0.9657 |1.0000 |0.9991 |0.9991 |\n|DenseNet201|0.9662  | 0.9807| 0.9484 |0.9657 |1.0000 |0.9991 |0.9991 |\n|EfficientNetB7 |0.9662  | 0.9807| 0.9484 |0.9657 |1.0000 |0.9991 |0.9991 |","c9342e67":"## 2.4 Diviser notre dataset en 80% l'entra\u00eenement et 20% pour le test","a404ffd3":"## 13. specificity et sensitivity","ba29187a":"## 2.2 Configuration de tpu","df38e4c9":"**l'\u00e9valuation des 4 mod\u00e8les (vgg16,vgg19,DenseNet-201,EfficientNetB7) est bas\u00e9 sur Accuracy,Pr\u00e9cision,Recall,F1_Score,AUC,Specificity,Sensitivity**\n\n**Positifs vrais (TP)** - Ce sont les valeurs positives correctement pr\u00e9dites, ce qui signifie que la valeur de la classe r\u00e9elle est oui et que la valeur de la classe pr\u00e9dite est \u00e9galement oui.\n\n**Vrais n\u00e9gatifs (TN)** - Il s'agit des valeurs n\u00e9gatives correctement pr\u00e9dites, ce qui signifie que la valeur de la classe r\u00e9elle est non et que la valeur de la classe pr\u00e9dite est \u00e9galement non.\n\n**Faux positifs (FP) et faux n\u00e9gatifs (FN)** , ces valeurs se produisent lorsque votre classe r\u00e9elle est en contradiction avec la classe pr\u00e9dite.\n\n**Accuracy** - La pr\u00e9cision est la mesure de performance la plus intuitive et il s'agit simplement d'un rapport entre l'observation correctement pr\u00e9dite et le total des observations.\n\n**Accuracy  = TP+TN\/TP+FP+FN+TN**\n\n**Pr\u00e9cision** - La pr\u00e9cision est le rapport entre les observations positives correctement pr\u00e9dites et le total des observations positives pr\u00e9dites. \n\n**Pr\u00e9cision = TP\/TP+FP**\n\n**Recall (sensibilit\u00e9)**- Le Recall est le rapport entre les observations positives correctement pr\u00e9dites et toutes les observations de la classe r\u00e9elle \n\n**Recall = TP\/TP+FN**\n\n**F1_Score** - Le score F1 est la moyenne pond\u00e9r\u00e9e de la pr\u00e9cision et du rappel. Par cons\u00e9quent, ce score tient compte \u00e0 la fois des faux positifs et des faux n\u00e9gatifs. \n\n**F1_Score = 2*(Rappel * Pr\u00e9cision) \/ (Rappel + Pr\u00e9cision)**\n\n**Sensitivity mesure la proportion de vrais positifs qui sont correctement identifi\u00e9s**\n**Specificity mesure la proportion de vrais n\u00e9gatifs qui sont correctement identifi\u00e9s comme non**\n","ceac730e":"# 10 Entra\u00eenement","9202a76c":"## 2.Affichage des courbes (acc,loss)","a0d77da2":"<p>\n<h1><center> DenseNet201 <\/center><\/h1>\n<center><img src='https:\/\/oi.readthedocs.io\/en\/latest\/_images\/cnn_vs_resnet_vs_densenet.png' height=20><\/center>\n<p>\n","29772b74":"## 11. Test et \u00e9valuation","d54a6d8b":"## 4. Matrice de confusion","3d9fe16b":"# 1. Entra\u00eenement","0d3b0c20":"## 7. Cr\u00e9ation d'un g\u00e9n\u00e9rateur pour l'ensemble de donn\u00e9es de test","bce90ccb":"## 4. Pr\u00e9traitement des donn\u00e9es","c46d618e":"<p>\n<h1><center> EfficientNetB7 <\/center><\/h1>\n<center><img src='https:\/\/1.bp.blogspot.com\/-DjZT_TLYZok\/XO3BYqpxCJI\/AAAAAAAAEKM\/BvV53klXaTUuQHCkOXZZGywRMdU9v9T_wCLcBGAs\/s1600\/image2.png' height=350><\/center>\n<p>","36ccb1fe":"## 6.Cr\u00e9ation d'un g\u00e9n\u00e9rateur pour l'ensemble de donn\u00e9es d'entra\u00eenement ","bdca0ae7":"## 11. Affichage des courbes (acc,loss)","5d527817":"## 9. Fonction de perte","a403ee7a":"## 1.Objectifs\n\n* L\u2019objectif ici est de d\u00e9velopper un r\u00e9seau CNN en se basant sur des architecture diff\u00e9rentes (vgg16,vgg19,EfficientNetB7,DenseNet201) pour la classification de deux maladie (Normal VS Cataract VS Glaucoma)","810af53d":"## 5. Augmentation","c999ecc7":"## 3. Test et \u00e9valuation","3540f304":"## 12. Matrice de confusion"}}