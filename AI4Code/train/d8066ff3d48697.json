{"cell_type":{"e415ce71":"code","23dda4d5":"code","0671c005":"code","a799df6c":"code","7a3ddfb0":"code","ef530541":"code","10fdbce3":"code","98c9bfe5":"code","192e4393":"code","79adaa5f":"code","d2dc2925":"code","58395210":"code","720ca698":"code","b46f1607":"code","9c28590b":"code","050657f3":"code","f2b9cb80":"code","5037c64e":"code","eea54ad9":"code","f39ee9c2":"code","c5fa218e":"code","de762cbe":"code","98970f35":"code","dbc52fa4":"code","6174fa35":"code","ee5b48b0":"code","402d4937":"code","7b1eb8e3":"code","53bbbd64":"code","33221686":"code","030b6a24":"code","5c0ea3b1":"code","9e0f0652":"code","1572683c":"code","374687a4":"code","914578eb":"code","0beaadeb":"code","2dbe9010":"code","8b18c069":"code","bdcc3c41":"code","9303d109":"code","bf1fc504":"code","494a1426":"code","558f2783":"code","499d94cc":"code","d452b630":"code","c93b7522":"code","e7ad3c87":"code","9172141c":"code","aa83f215":"code","c46eebfd":"code","ee92f52e":"code","2ec8fade":"code","8a00ec51":"code","637b1704":"code","1ffc900f":"code","f76488d1":"markdown","89131e62":"markdown","751469e1":"markdown","19913b2e":"markdown","2d874c1b":"markdown","c2e9216a":"markdown","21e35030":"markdown","4891395f":"markdown","bad6e180":"markdown","8b3b7db0":"markdown","ac04a885":"markdown","4238b2a0":"markdown","2f5006c9":"markdown","fc243667":"markdown","68884c02":"markdown","79bd19ac":"markdown","56f55f23":"markdown","d4b336d4":"markdown","679e58b7":"markdown","d7c08b74":"markdown","aa6612e6":"markdown","38d9d600":"markdown","4a7277e1":"markdown"},"source":{"e415ce71":"# importar pacotes necess\u00e1rios\nimport numpy as np\nimport pandas as pd","23dda4d5":"# definir par\u00e2metros extras\npd.set_option('precision', 4)\npd.set_option('display.max_columns', 100)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0671c005":"prefixo_arquivos = '\/kaggle\/input\/titanic\/'","a799df6c":"# carregar arquivo de dados de treino\ntrain_data = pd.read_csv(prefixo_arquivos + 'train.csv', index_col='PassengerId')","7a3ddfb0":"# carregar arquivo de dados de teste\ntest_data = pd.read_csv(prefixo_arquivos + 'test.csv', index_col='PassengerId')","ef530541":"# unir ambos os dados de treino e teste\ndata = pd.concat([train_data, test_data])\n\n# mostrar alguns exemplos de registros\ndata.head()","10fdbce3":"# extrair t\u00edtulos das pessoas a partir do nome\ndata['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# exibir rela\u00e7\u00e3o entre t\u00edtulo e sexo\npd.crosstab(data['Title'], data['Sex']).T","98c9bfe5":"# agregar t\u00edtulos incomuns\nreplacements = {\n    'Miss': ['Mlle', 'Ms'],\n    'Mrs': ['Mme'],\n    'Rare': ['Lady', 'Countess', 'Capt', 'Col', 'Don', \\\n             'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\n}\nfor k, v in replacements.items():\n    data['Title'] = data['Title'].replace(v, k)\n    \n# exibir rela\u00e7\u00e3o entre t\u00edtulo e sexo\npd.crosstab(data['Title'], data['Sex']).T","192e4393":"# categorizar os valores dos t\u00edtulos\ntitle_mapping = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}\ndata['Title'] = data['Title'].map(title_mapping)\ndata['Title'] = data['Title'].fillna(0)","79adaa5f":"# categorizar os valores dos sexos\ndata['Sex'] = data['Sex'].map({'female': 1, 'male': 0}).astype(int)","d2dc2925":"# preencher e categorizar os valores dos portos de embarque\ndata['Embarked'].fillna(data.Embarked.mode()[0], inplace=True)\ndata['Embarked'] = data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)","58395210":"# preencher os valores da passagem\ndata['Fare'].fillna(data.Fare.mean(), inplace=True)","720ca698":"# criar coluna com tamanho da fam\u00edlia\ndata['FSize'] = data['Parch'] + data['SibSp'] + 1","b46f1607":"# criar coluna indicando se estava sozinho\ndata['Alone'] = 0\ndata.loc[data.FSize == 1, 'Alone'] = 1","9c28590b":"# criar coluna contendo o deque\ndata['Deck'] = data['Cabin'].str[:1]\ndata['Deck'] = data['Deck'].fillna('N').astype('category')\ndata['Deck'] = data['Deck'].cat.codes","050657f3":"# criar coluna contendo o n\u00famero do quarto\ndata['Room'] = data['Cabin'].str.extract(\"([0-9]+)\", expand=False)\ndata['Room'] = data['Room'].fillna(0).astype(int)","f2b9cb80":"data.head()","5037c64e":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\ndef evaluate_regression_model(model, X, y):\n    kfold = KFold(n_splits=10, random_state=42)\n    results = cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error', verbose=1)\n    score = (-1) * results.mean()\n    stddev = results.std()\n    print(model, '\\nScore: %.2f (+\/- %.2f)' % (score, stddev))\n    return score, stddev","eea54ad9":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\nage_models = [\n#    ('LR', LinearRegression(n_jobs=-1, fit_intercept=True, normalize=True)),\n    ('GBR', GradientBoostingRegressor(random_state=42)),\n    ('RFR', RandomForestRegressor(random_state=42)),\n    ('XGB', XGBRegressor(random_state=42, objective='reg:squarederror')),\n#    ('MLP', MLPRegressor(random_state=42, max_iter=500, activation='tanh',\n#                         hidden_layer_sizes=(10,5,5), solver='lbfgs')),\n#    ('GPR', GaussianProcessRegressor(random_state=42, alpha=0.01, normalize_y=True))\n]","f39ee9c2":"# selecionar dados para o treino\n\ncols = ['Pclass', 'SibSp', 'Parch', 'Fare', 'Title', 'Age', 'Alone']\n\ndata_age = data[cols].dropna()\n\nX_age = data_age.drop(['Age'], axis=1)\ny_age = data_age['Age']\n\ndata_age.head()","c5fa218e":"data_age.corr()","de762cbe":"names = []\nscores = []\nlowest = 999\nbest_model = None\n\nfor name, model in age_models:\n    \n    score, stddev = evaluate_regression_model(model, X_age, y_age)\n    names.append(name)\n    scores.append(score)\n    \n    if score < lowest:\n        best_model = model\n        lowest = score","98970f35":"results = pd.DataFrame({'Age Model': names, 'Score': scores})\nresults.sort_values(by='Score', ascending=True)","dbc52fa4":"age_model = best_model\nage_model.fit(X_age, y_age)","6174fa35":"# preencher dados faltantes de idade a partir de uma regress\u00e3o\ndata['AgePred'] = age_model.predict(data[cols].drop('Age', axis=1))\ndata.loc[data.Age.isnull(), 'Age'] = data['AgePred']\ndata.drop('AgePred', axis=1, inplace=True)\ndata.head()","ee5b48b0":"# existem colunas com dados nulos?\ndata[data.columns[data.isnull().any()]].isnull().sum()","402d4937":"data.head()","7b1eb8e3":"# realizar normaliza\u00e7\u00e3o nos dados num\u00e9ricos cont\u00ednuos\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\ncols = ['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'FSize']\n\ndata.loc[:,cols] = scaler.fit_transform(data.loc[:,cols])","53bbbd64":"data.head()","33221686":"# importar os pacotes necess\u00e1rios para os algoritmos de classifica\u00e7\u00e3o\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier","030b6a24":"from datetime import datetime\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\n# avalia o desempenho do modelo, retornando o valor da precis\u00e3o\ndef evaluate_classification_model(model, X, y):\n    start = datetime.now()\n    kfold = KFold(n_splits=10, random_state=42)\n    results = cross_val_score(model, X, y, cv=kfold, scoring='accuracy', verbose=1)\n    end = datetime.now()\n    elapsed = int((end - start).total_seconds() * 1000)\n    score = 100.0 * results.mean()\n    stddev = 100.0 * results.std()\n    print(model, '\\nScore: %.2f (+\/- %.2f) [%5s ms]' % (score, stddev, elapsed))\n    return score, stddev, elapsed","5c0ea3b1":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\n\n# faz o ajuste fino do modelo, calculando os melhores hiperpar\u00e2metros\ndef fine_tune_model(model, params, X, y):\n    print('\\nFine Tuning Model:')\n    print(model, \"\\nparams:\", params)\n    kfold = KFold(n_splits=10, random_state=42)\n    grid = GridSearchCV(estimator=model, param_grid=params, scoring='accuracy', cv=kfold, verbose=1)\n    grid.fit(X, y)\n    print('\\nGrid Best Score: %.2f' % (grid.best_score_ * 100.0))\n    print('Best Params:', grid.best_params_)\n    return grid","9e0f0652":"# definir dados de treino\ntrain_data = data[data.Survived.isnull() == False]\n\n# selecionar atributos para o modelo\ncols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title', 'FSize', 'Alone', 'Deck']\n\nX_train = train_data[cols]\ny_train = train_data['Survived']\n\nprint('Forma dos dados de treino:', X_train.shape, y_train.shape)","1572683c":"train_data.corr()","374687a4":"# definir dados de teste\ntest_data = data[data.Survived.isnull()]\n\nX_test = test_data[cols]\n\nprint('Forma dos dados de teste:', X_test.shape)","914578eb":"names = []\nmodels = []\nscores = []\nstddevs = []\ntimes = []\n\ndef add_model_info(name, model, score, stddev, elapsed):\n    names.append(name)\n    models.append((name, model))\n    scores.append(score)\n    stddevs.append(stddev)\n    times.append(elapsed)","0beaadeb":"model = LogisticRegression(random_state=42, solver='newton-cg', C=0.1, multi_class='auto', max_iter=500)\n\nparams = dict(\n    solver=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n    C=np.logspace(-3, 3, 7)\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('LR', model, score, stddev, elapsed)","2dbe9010":"model = DecisionTreeClassifier(random_state=42, criterion='entropy', max_depth=6, min_samples_split=0.25)\n\n#criterion=\u2019mse\u2019, splitter=\u2019best\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1, \n#min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, \n#min_impurity_decrease=0.0, min_impurity_split=None, presort=False\n\nparams = dict(\n    criterion=['gini','entropy'],\n    max_depth=[4, 6, 8, 10, 12, 14],\n    min_samples_split=[0.25, 0.5, 0.75, 1.0]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('DT', model, score, stddev, elapsed)","8b18c069":"model = LinearDiscriminantAnalysis(solver='lsqr')\n\n#solver=\u2019svd\u2019, shrinkage=None, priors=None,\n#n_components=None, store_covariance=False, tol=0.0001\n\nparams = dict(\n    solver=['svd', 'lsqr'] #, 'eigen']\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('LDA', model, score, stddev, elapsed)","bdcc3c41":"model = GaussianNB(priors=None, var_smoothing=1e-8)\n\n#priors=None, var_smoothing=1e-09\n\nparams = dict(\n    priors=[None],\n    var_smoothing=[1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('NB', model, score, stddev, elapsed)","9303d109":"model = KNeighborsClassifier(n_neighbors=11, weights='uniform')\n\n#n_neighbors=5, weights=\u2019uniform\u2019, algorithm=\u2019auto\u2019, leaf_size=30, p=2, metric=\u2019minkowski\u2019,\n#metric_params=None, n_jobs=None\n\nparams = dict(\n    n_neighbors=[1, 3, 5, 7, 9, 11, 13],\n    weights=['uniform', 'distance']\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('KNN', model, score, stddev, elapsed)","bf1fc504":"model = SVC(random_state=42, C=10, gamma=0.1, kernel='rbf')\n\n#kernel=\u2019rbf\u2019, degree=3, gamma=\u2019auto_deprecated\u2019, coef0=0.0, tol=0.001, C=1.0, \n#epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1\n\nparams = dict(\n    C=[0.001, 0.01, 0.1, 1, 10, 100],\n    gamma=[0.001, 0.01, 0.1, 1, 10, 100],\n    kernel=['linear', 'rbf']\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('SVM', model, score, stddev, elapsed)","494a1426":"model = MLPClassifier(random_state=42, solver='lbfgs', alpha=1, hidden_layer_sizes=(100,), activation='logistic')\n                \n#hidden_layer_sizes=(100, ), activation=\u2019relu\u2019, solver=\u2019adam\u2019, alpha=0.0001, batch_size=\u2019auto\u2019, \n#learning_rate=\u2019constant\u2019, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, \n#random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n#early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10\n\nparams = dict(\n    alpha=[1,0.1,0.01,0.001,0.0001,0],\n    hidden_layer_sizes=[(100,), (50,), (50,2), (5,5,2), (10,5,2)],\n    activation=['identity', 'logistic', 'tanh', 'relu'],\n    solver=['lbfgs', 'sgd', 'adam']\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('MLP', model, score, stddev, elapsed)","558f2783":"model = AdaBoostClassifier(DecisionTreeClassifier(random_state=42), random_state=42, n_estimators=50)\n\n#base_estimator=None, n_estimators=50, learning_rate=1.0,\n#algorithm=\u2019SAMME.R\u2019, random_state=None\n\nparams = dict(\n    n_estimators=[10, 25, 50, 100]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('ABDT', model, score, stddev, elapsed)","499d94cc":"from sklearn.ensemble import BaggingClassifier\n\nmodel = BaggingClassifier(random_state=42, n_estimators=100,\n                          max_samples=0.25, max_features=0.8)\n\n#base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0,\n#bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False,\n#n_jobs=None, random_state=None, verbose=0\n\nparams = dict(\n    n_estimators=[10, 50, 100, 500],\n    max_samples=[0.25, 0.5, 0.75, 1.0],\n    max_features=[0.7, 0.8, 0.9, 1.0]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('BC', model, score, stddev, elapsed)","d452b630":"model = ExtraTreesClassifier(random_state=42, n_estimators=100, max_depth=7, \n                             min_samples_split=0.25, max_features='auto')\n\n#n_estimators=\u2019warn\u2019, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2,\n#min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, \n#max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n#bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0,\n#warm_start=False, class_weight=None\n\nparams = dict(\n    n_estimators=[10, 50, 100, 500],\n    max_depth=[None, 3, 7, 11],\n    min_samples_split=[0.25, 0.5],\n    max_features=['auto', 0.7, 0.85, 1.0]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('ET', model, score, stddev, elapsed)","c93b7522":"model = GradientBoostingClassifier(random_state=42, n_estimators=100, max_features=0.75,\n                                   max_depth=4, learning_rate=0.1, subsample=0.6)\n\n#loss=\u2019ls\u2019, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=\u2019friedman_mse\u2019, min_samples_split=2,\n#min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, \n#min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, \n#max_leaf_nodes=None, warm_start=False, presort=\u2019auto\u2019, validation_fraction=0.1, n_iter_no_change=None, \n#tol=0.0001\n\nparams = dict(\n    n_estimators=[100, 250, 500],\n    max_features=[0.75, 0.85, 1.0],\n    max_depth=[4, 6, 8, 10],\n    learning_rate=[0.05, 0.1, 0.15],\n    subsample=[0.4, 0.6, 0.8]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('GB', model, score, stddev, elapsed)","e7ad3c87":"model = RandomForestClassifier(random_state=42, n_estimators=100, max_features='auto', max_depth=5)\n\n#n_estimators=\u2019warn\u2019, criterion=\u2019mse\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1, \n#min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0, \n#min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, \n#verbose=0, warm_start=False\n\nparams = dict(\n    n_estimators=[10, 50, 100, 500],\n    max_features=['auto', 'sqrt', 'log2'],\n    max_depth=[None, 3, 5, 7, 9, 11, 13]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('RF', model, score, stddev, elapsed)","9172141c":"model = XGBClassifier(max_depth=3, n_estimators=100)\n\n#max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:squarederror',\n#booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, \n#colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, \n#base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain'\n\nparams = dict(\n    max_depth=[3, 5, 7, 9],\n    n_estimators=[50, 75, 100, 200]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('XGB', model, score, stddev, elapsed)","aa83f215":"estimators =  [\n    ('RF', RandomForestClassifier(random_state=42, n_estimators=100, max_features='auto', max_depth=5)),\n    ('BC', BaggingClassifier(random_state=42, n_estimators=100, max_samples=0.25, max_features=0.8)),\n    ('GB', GradientBoostingClassifier(random_state=42, max_depth=4, max_features=0.75,\n                                   n_estimators=100, learning_rate=0.1, subsample=0.6)),\n#    ('XGB', XGBClassifier(max_depth=3, n_estimators=100)),\n]\nmodel = VotingClassifier(estimators, n_jobs=-1, weights=(2,1,1))\n\n#estimators, weights=None, n_jobs=None\n\nparams = dict(\n    weights=[(1,1,1), (2,1,1), (3,1,1), (3,2,1), (2,2,1), (2,1,2), (5,4,3), (1,2,1), (1,1,2), ]\n)\n#fine_tune_model(model, params, X_train, y_train)\n\nscore, stddev, elapsed = evaluate_classification_model(model, X_train, y_train)\nadd_model_info('VC', model, score, stddev, elapsed)","c46eebfd":"model = RandomForestClassifier(random_state=42, max_features='auto', n_estimators=100)\nmodel.fit(X_train, y_train)\n\nimportances = pd.DataFrame({'feature': X_train.columns,\n                            'importance': np.round(model.feature_importances_, 3)})\nimportances = importances.sort_values('importance', ascending=False).set_index('feature')\nimportances.head(20)","ee92f52e":"importances.plot.bar()","2ec8fade":"results = pd.DataFrame({'Algorithm': names, 'Score': scores, 'Std Dev': stddevs, 'Time (ms)': times})\nresults.sort_values(by='Score', ascending=False)","8a00ec51":"# criar diret\u00f3rio para os arquivos de envio\n#!test -d submissions || mkdir submissions","637b1704":"prefixo_arquivo = 'titanic-submission'\nsufixo_arquivo = '06set'\n\nfor name, model in models:\n    print(model, '\\n')\n    \n    # treinar o modelo\n    model.fit(X_train, y_train)\n    \n    # executar previs\u00e3o usando o modelo\n    y_pred = model.predict(X_test)\n    y_pred_int = y_pred.astype(int)\n    #vfunc = np.vectorize(lambda x: 'yes' if x > 0 else 'no')\n\n    # gerar dados de envio (submiss\u00e3o)\n    submission = pd.DataFrame({\n      'PassengerId': X_test.index,\n      'Survived': y_pred_int #vfunc(y_pred)\n    })\n    submission.set_index('PassengerId', inplace=True)\n\n    # gerar arquivo CSV para o envio\n    filename = '%s-p-%s-%s.csv' % (prefixo_arquivo, sufixo_arquivo, name.lower())\n    submission.to_csv(filename)","1ffc900f":"!head titanic-*.csv","f76488d1":"## Carga dos dados","89131e62":"## Avaliar import\u00e2ncia dos atributos no modelo","751469e1":"#### XGBoost\n\n- https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn","19913b2e":"### Ensemble Methods","2d874c1b":"# transformar colunas textuais em categ\u00f3ricas\ndata['Survived'] = data['Survived'].map({'yes': 1, 'no': 0})","c2e9216a":"### Decision Trees","21e35030":"### Discriminant Analysis","4891395f":"# criar colunas adicionais\ndata['ageclass'] = data['age'] * data['pclass']\ndata['pfare'] = data['fare'] \/ data['fsize']","bad6e180":"## Avalia\u00e7\u00e3o e ajuste fino de cada modelo preditivo\n\n-  https:\/\/scikit-learn.org\/stable\/modules\/classes.html","8b3b7db0":"## Importa\u00e7\u00e3o dos pacotes","ac04a885":"### Support Vector Machines","4238b2a0":"### Ensemble Learning Model\n\n- https:\/\/towardsdatascience.com\/automate-stacking-in-python-fc3e7834772e\n- https:\/\/github.com\/vecxoz\/vecstack","2f5006c9":"### Na\u00efve Bayes","fc243667":"### Neural network models","68884c02":"### Inferir idades faltantes dos passageiros","79bd19ac":"## Compara\u00e7\u00e3o final entre os algoritmos","56f55f23":"## Modelagem preditiva","d4b336d4":"## Gerar arquivos com resultados","679e58b7":"### Generalized Linear Models","d7c08b74":"### Outros algoritmos","aa6612e6":"## Transforma\u00e7\u00f5es nos dados","38d9d600":"### Nearest Neighbors","4a7277e1":"# gerar \"one hot encoding\" em atributos categ\u00f3ricos\n#cols = ['pclass', 'sex', 'embarked']\ncols = ['embarked', 'pclass', 'title', 'deck']\ndata = pd.get_dummies(data, columns=cols)"}}