{"cell_type":{"8250dcc7":"code","2d9a574c":"code","df5c3819":"code","a8c36a3b":"code","c5d9bbf2":"code","2b11ae5d":"code","5706f102":"code","a79513e0":"code","ae74080e":"code","479b472e":"code","0b9c36c8":"markdown","cd3f7763":"markdown","1af36fba":"markdown","3d28843f":"markdown","89d3937b":"markdown","e6a3d539":"markdown"},"source":{"8250dcc7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly_express as px\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2d9a574c":"data = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","df5c3819":"data.head()","a8c36a3b":"sns.set(style=\"ticks\")\ng = sns.countplot(x=\"target\", data=data, palette=\"bwr\")\nsns.despine()\ng.figure.set_size_inches(12,7)\nplt.show()","c5d9bbf2":"train = data.copy()\ntrain_y = data['target']\ndel train['target']","2b11ae5d":"X, Xcv, y, ycv = train_test_split(train,train_y,test_size = 0.2,random_state=0)","5706f102":"from hyperopt import hp, fmin, tpe, STATUS_OK, Trials","a79513e0":"from sklearn.metrics import accuracy_score\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nimport numpy as np\ndef objective(space):\n    # Instantiate the classifier\n    clf = xgb.XGBClassifier(n_estimators =1000,colsample_bytree=space['colsample_bytree'],\n                           learning_rate = .3,\n                            max_depth = int(space['max_depth']),\n                            min_child_weight = space['min_child_weight'],\n                            subsample = space['subsample'],\n                           gamma = space['gamma'],\n                           reg_lambda = space['reg_lambda'])\n    \n    eval_set  = [( X, y), ( Xcv, ycv)]\n    \n    # Fit the classsifier\n    clf.fit(X, y,\n            eval_set=eval_set, eval_metric=\"rmse\",\n            early_stopping_rounds=10,verbose=False)\n    \n    # Predict on Cross Validation data\n    pred = clf.predict(Xcv)\n    \n    # Calculate our Metric - accuracy\n    accuracy = accuracy_score(ycv, pred>0.5)\n\n    # return needs to be in this below format. We use negative of accuracy since we want to maximize it.\n    return {'loss': -accuracy, 'status': STATUS_OK }","ae74080e":"space ={'max_depth': hp.quniform(\"x_max_depth\", 4, 16, 1),\n        'min_child_weight': hp.quniform ('x_min_child', 1, 10, 1),\n        'subsample': hp.uniform ('x_subsample', 0.7, 1),\n        'gamma' : hp.uniform ('x_gamma', 0.1,0.5),\n        'colsample_bytree' : hp.uniform ('x_colsample_bytree', 0.7,1),\n        'reg_lambda' : hp.uniform ('x_reg_lambda', 0,1)\n    }","479b472e":"trials = Trials()\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=100,\n            trials=trials)\nprint(best)","0b9c36c8":"# Running Hyperopt","cd3f7763":"## 4. Run Hyperopt","1af36fba":"## 1. Importing the Required Library for Hyperopt","3d28843f":"# Splitting the data into Cross-Validation and Train","89d3937b":"## 3. Create the Space for your classifier\n\nNow, we create the search space for hyperparameters for our classifier.\n\nTo do this we end up using many of hyperopt built in functions which define verious distributions. As you can see we use uniform distribution between 0.7 and 1 for our subsample hyperparameter. It is much better than defining a parameter value using ranges for sure. You can also define a lot of other distributions too. \n\n","e6a3d539":"## 2. Create the objective function\n\nHere we create an objective function which takes as input a hyperparameter space:\n- defines a classifier, in this case XGBoost. Just try to see how we take the parameters from the space. For example `space['max_depth']` \n- We fit the classifier to the train data\n- We predict on cross validation set\n- We calculate the required metric we want to maximize or minimize\n- Since we only minimize using `fmin` in hyperopt, if we want to minimize `logloss` we just send our metric as is. If we want to maximize accuracy we will try to minimize `-accuracy`"}}