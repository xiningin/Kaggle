{"cell_type":{"2bfe1f55":"code","82ae943e":"code","d02f0c5b":"code","a751fedd":"code","cc579ef7":"code","c58f078f":"code","75172841":"code","12779c2c":"code","bfc84754":"code","9d576025":"code","a9c82f60":"code","5b04c5cd":"code","831134a9":"code","b32e5a7f":"code","2a983fcf":"code","6b5b7c62":"code","42cba73b":"code","0fc11c87":"code","55fedcd7":"code","e6e18ac2":"code","aeff1597":"code","089e405a":"code","a2c54f2a":"code","59353131":"code","be26a3e6":"code","bab561ae":"code","bd3949ab":"code","85a1649c":"code","e44ab676":"code","6155daf4":"code","22e18d4a":"code","edbed503":"code","884d3ed0":"code","ec8139b5":"code","0763215a":"code","c0a38ed6":"code","11912eb3":"code","db20c7ed":"code","15817142":"code","47059b51":"code","47461dcb":"code","4a8d1bef":"code","9a2c552d":"code","43f18987":"markdown","812f4676":"markdown","f44f8e6d":"markdown","4ee795fa":"markdown","d7107442":"markdown","6d92432c":"markdown","49d7d083":"markdown","cb57581b":"markdown","d2b9c902":"markdown","fe830dd1":"markdown","88900687":"markdown","85eea1e9":"markdown","b5729db8":"markdown","cfc85902":"markdown","6f80408c":"markdown","55fa1027":"markdown","c752e377":"markdown","a3b00822":"markdown","44b186c1":"markdown","4204d834":"markdown","31cb3f04":"markdown","1d563a17":"markdown","fa1dce26":"markdown","ad16d47f":"markdown","6cfd3405":"markdown","a777a9ae":"markdown","236a1b2c":"markdown","6e12c952":"markdown","0cdf8eb9":"markdown","52dd2af2":"markdown","efa83039":"markdown","8da258f6":"markdown","71b3505a":"markdown","98b57f3e":"markdown","101f8ebd":"markdown","0a5f9cf5":"markdown","17348d4a":"markdown","a1852b19":"markdown","e077f6eb":"markdown","8f6d58ff":"markdown","ae5bed78":"markdown","6ec52058":"markdown","fcf98c91":"markdown","e5e7b87a":"markdown","f7a94389":"markdown","2bb7a798":"markdown","48447c95":"markdown","d9d98bc9":"markdown","ae4137ff":"markdown","8718cb83":"markdown"},"source":{"2bfe1f55":"# Python 3 environment comes with many helpful analytics libraries installed\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os","82ae943e":"# Read data to a pandas data frame\ndata=pd.read_csv('..\/input\/train.csv')\n# lets have a look on first few rows\ndisplay(data.head())\n# Checking shape of our data set\nprint('Shape of Data : ',data.shape)","d02f0c5b":"#checking for null value counts in each column\ndata.isnull().sum()","a751fedd":"f,ax=plt.subplots(1,2,figsize=(13,5))\ndata['Survived'].value_counts().plot.pie(explode=[0,0.05],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=data,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","cc579ef7":"f,ax=plt.subplots(1,3,figsize=(18,5))\ndata[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Fraction of Survival with respect to Sex')\nsns.countplot('Sex',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Survived vs Dead counts with respect to Sex')\nsns.barplot(x=\"Sex\", y=\"Survived\", data=data,ax=ax[2])\nax[2].set_title('Survival by Gender')\nplt.show()","c58f078f":"f,ax=plt.subplots(1,3,figsize=(18,5))\ndata['Pclass'].value_counts().plot.bar(color=['#BC8F8F','#F4A460','#DAA520'],ax=ax[0])\nax[0].set_title('Number Of Passengers with respect to Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Survived vs Dead counts with respect to Pclass')\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=data,ax=ax[2])\nax[2].set_title('Survival by Pclass')\nplt.show()","75172841":"# Plot\nplt.figure(figsize=(25,6))\nsns.barplot(data['Age'],data['Survived'], ci=None)\nplt.xticks(rotation=90);","12779c2c":"data['Initial']=0\nfor i in data:\n    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n\npd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap='summer_r') #Checking the Initials with the Sex","bfc84754":"data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)","9d576025":"data.groupby('Initial')['Age'].mean() #lets check the average age by Initials","a9c82f60":"## Assigning the NaN Values with the Ceil values of the mean ages\ndata.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\ndata.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\ndata.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\ndata.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\ndata.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46","5b04c5cd":"data.Age.isnull().any() #So no null values left finally ","831134a9":"f,ax=plt.subplots(1,2,figsize=(12,5))\nsns.countplot('Embarked',data=data,ax=ax[0])\nax[0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Embarked vs Survived')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","b32e5a7f":"f,ax=plt.subplots(1,2,figsize=(12,5))\nsns.countplot('Embarked',hue='Sex',data=data,ax=ax[0])\nax[0].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Pclass',data=data,ax=ax[1])\nax[1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","2a983fcf":"f,ax=plt.subplots(1,1,figsize=(6,5))\ndata['Embarked'].value_counts().plot.pie(explode=[0,0,0],autopct='%1.1f%%',ax=ax)\nplt.show()","6b5b7c62":"data['Embarked'].fillna('S',inplace=True)","42cba73b":"data.Embarked.isnull().any()","0fc11c87":"f,ax=plt.subplots(2,2,figsize=(15,10))\nsns.countplot('SibSp',hue='Survived',data=data,ax=ax[0,0])\nax[0,0].set_title('SibSp vs Survived')\nsns.barplot('SibSp','Survived',data=data,ax=ax[0,1])\nax[0,1].set_title('SibSp vs Survived')\n\nsns.countplot('Parch',hue='Survived',data=data,ax=ax[1,0])\nax[1,0].set_title('Parch vs Survived')\nsns.barplot('Parch','Survived',data=data,ax=ax[1,1])\nax[1,1].set_title('Parch vs Survived')\n\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","55fedcd7":"data['FamilySize'] = data['Parch'] + data['SibSp']\nf,ax=plt.subplots(1,2,figsize=(15,4.5))\nsns.countplot('FamilySize',hue='Survived',data=data,ax=ax[0])\nax[0].set_title('FamilySize vs Survived')\nsns.barplot('FamilySize','Survived',data=data,ax=ax[1])\nax[1].set_title('FamilySize vs Survived')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","e6e18ac2":"f,ax=plt.subplots(1,1,figsize=(20,5))\nsns.distplot(data.Fare,ax=ax)\nax.set_title('Distribution of Fares')\nplt.show()","aeff1597":"print('Highest Fare:',data['Fare'].max(),'   Lowest Fare:',data['Fare'].min(),'    Average Fare:',data['Fare'].mean())\ndata['Fare_Bin']=pd.qcut(data['Fare'],6)\ndata.groupby(['Fare_Bin'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","089e405a":"sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","a2c54f2a":"data['Sex'].replace(['male','female'],[0,1],inplace=True)\ndata['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ndata['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)","59353131":"print('Highest Age:',data['Age'].max(),'   Lowest Age:',data['Age'].min())","be26a3e6":"data['Age_cat']=0\ndata.loc[data['Age']<=16,'Age_cat']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32),'Age_cat']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48),'Age_cat']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64),'Age_cat']=3\ndata.loc[data['Age']>64,'Age_cat']=4","bab561ae":"data['Fare_cat']=0\ndata.loc[data['Fare']<=7.775,'Fare_cat']=0\ndata.loc[(data['Fare']>7.775)&(data['Fare']<=8.662),'Fare_cat']=1\ndata.loc[(data['Fare']>8.662)&(data['Fare']<=14.454),'Fare_cat']=2\ndata.loc[(data['Fare']>14.454)&(data['Fare']<=26.0),'Fare_cat']=3\ndata.loc[(data['Fare']>26.0)&(data['Fare']<=52.369),'Fare_cat']=4\ndata.loc[data['Fare']>52.369,'Fare_cat']=5","bd3949ab":"#data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)\ndata.drop(['Name','Age','Fare','Ticket','Cabin','Fare_Bin','SibSp','Parch','PassengerId'],axis=1,inplace=True)","85a1649c":"data.head(2)","e44ab676":"sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","6155daf4":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","22e18d4a":"#Lets prepare data sets for training. \ntrain,test=train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\ntest_X=test[test.columns[1:]]\ntest_Y=test[test.columns[:1]]\nX=data[data.columns[1:]]\nY=data['Survived']","edbed503":"data.head(2)","884d3ed0":"# Logistic Regression\nmodel = LogisticRegression(C=0.05,solver='liblinear')\nmodel.fit(train_X,train_Y.values.ravel())\nLR_prediction=model.predict(test_X)\nprint('The accuracy of the Logistic Regression model is \\t',metrics.accuracy_score(LR_prediction,test_Y))\n\n# Naive Bayes\nmodel=GaussianNB()\nmodel.fit(train_X,train_Y.values.ravel())\nNB_prediction=model.predict(test_X)\nprint('The accuracy of the NaiveBayes model is\\t\\t\\t',metrics.accuracy_score(NB_prediction,test_Y))\n\n# Decision Tree\nmodel=DecisionTreeClassifier()\nmodel.fit(train_X,train_Y)\nDT_prediction=model.predict(test_X)\nprint('The accuracy of the Decision Tree is \\t\\t\\t',metrics.accuracy_score(DT_prediction,test_Y))\n\n# Random Forest\nmodel=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_Y.values.ravel())\nRF_prediction=model.predict(test_X)\nprint('The accuracy of the Random Forests model is \\t\\t',metrics.accuracy_score(RF_prediction,test_Y))","ec8139b5":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Logistic Regression','Decision Tree','Naive Bayes','Random Forest']\nmodels=[LogisticRegression(solver='liblinear'),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2","0763215a":"f,ax=plt.subplots(2,2,figsize=(10,8))\ny_pred = cross_val_predict(LogisticRegression(C=0.05,solver='liblinear'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\nax[0,0].set_title('Matrix for Logistic Regression')\ny_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')\nax[0,1].set_title('Matrix for Decision Tree')\ny_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\nax[1,0].set_title('Matrix for Naive Bayes')\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')\nax[1,1].set_title('Matrix for Random-Forests')\nplt.subplots_adjust(hspace=0.2,wspace=0.2)\nplt.show()","c0a38ed6":"from sklearn.model_selection import GridSearchCV\nn_estimators=range(100,1000,100)\nhyper={'n_estimators':n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True,cv=10)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","11912eb3":"from sklearn.ensemble import VotingClassifier\nestimators=[('RFor',RandomForestClassifier(n_estimators=100,random_state=0)),\n            ('LR',LogisticRegression(C=0.05,solver='liblinear')),\n            ('DT',DecisionTreeClassifier()),\n            ('NB',GaussianNB())]\nensemble=VotingClassifier(estimators=estimators,voting='soft')\nensemble.fit(train_X,train_Y.values.ravel())\nprint('The accuracy for ensembled model is:',ensemble.score(test_X,test_Y))\ncross=cross_val_score(ensemble,X,Y, cv = 10,scoring = \"accuracy\")\nprint('The cross validated score is',cross.mean())","db20c7ed":"Ensemble_Model_For_Prediction=VotingClassifier(estimators=[\n                                       ('RFor',RandomForestClassifier(n_estimators=200,random_state=0)),\n                                       ('LR',LogisticRegression(C=0.05,solver='liblinear')),\n                                       ('DT',DecisionTreeClassifier(random_state=0)),\n                                       ('NB',GaussianNB())\n                                             ], \n                       voting='soft')\nEnsemble_Model_For_Prediction.fit(X,Y)","15817142":"test=pd.read_csv('..\/input\/test.csv')\nIDtest = test[\"PassengerId\"]\ntest.head(2)","47059b51":"test.isnull().sum()","47461dcb":"# Prepare Test Data set for feeding\n\n# Construct feature Initial\ntest['Initial']=0\nfor i in test:\n    test['Initial']=test.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n    \ntest['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don','Dona'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr','Other'],inplace=True)\n\n# Fill Null values in Age Column\ntest.loc[(test.Age.isnull())&(test.Initial=='Mr'),'Age']=33\ntest.loc[(test.Age.isnull())&(test.Initial=='Mrs'),'Age']=36\ntest.loc[(test.Age.isnull())&(test.Initial=='Master'),'Age']=5\ntest.loc[(test.Age.isnull())&(test.Initial=='Miss'),'Age']=22\ntest.loc[(test.Age.isnull())&(test.Initial=='Other'),'Age']=46\n\n# Fill Null values in Fare Column\ntest.loc[(test.Fare.isnull()) & (test['Pclass']==3),'Fare'] = 12.45\n\n# Construct feature Age_cat\ntest['Age_cat']=0\ntest.loc[test['Age']<=16,'Age_cat']=0\ntest.loc[(test['Age']>16)&(test['Age']<=32),'Age_cat']=1\ntest.loc[(test['Age']>32)&(test['Age']<=48),'Age_cat']=2\ntest.loc[(test['Age']>48)&(test['Age']<=64),'Age_cat']=3\ntest.loc[test['Age']>64,'Age_cat']=4\n\n# Construct feature Fare_cat\ntest['Fare_cat']=0\ntest.loc[test['Fare']<=7.775,'Fare_cat']=0\ntest.loc[(test['Fare']>7.775)&(test['Fare']<=8.662),'Fare_cat']=1\ntest.loc[(test['Fare']>8.662)&(test['Fare']<=14.454),'Fare_cat']=2\ntest.loc[(test['Fare']>14.454)&(test['Fare']<=26.0),'Fare_cat']=3\ntest.loc[(test['Fare']>26.0)&(test['Fare']<=52.369),'Fare_cat']=4\ntest.loc[test['Fare']>52.369,'Fare_cat']=5\n\n# Construct feature FamilySize\ntest['FamilySize'] = test['Parch'] + test['SibSp']\n\n# Drop unwanted features\ntest.drop(['Name','Age','Ticket','Cabin','SibSp','Parch','Fare','PassengerId'],axis=1,inplace=True)\n\n# Converting String Values into Numeric \ntest['Sex'].replace(['male','female'],[0,1],inplace=True)\ntest['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ntest['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)\n\ntest.head(2)","4a8d1bef":"# Predict\ntest_Survived = pd.Series(ensemble.predict(test), name=\"Survived\")\nresults = pd.concat([IDtest,test_Survived],axis=1)\nresults.to_csv(\"predictions.csv\",index=False)","9a2c552d":"f,ax=plt.subplots(1,1,figsize=(6,6))\nmodel=RandomForestClassifier(n_estimators=500,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax)\nax.set_title('Feature Importance in Random Forests')\nplt.show()","43f18987":"* Best Score for Random Forest is with n_estimators=100","812f4676":"#### Convert Age into a categorical feature by binning[^](#4_2)<a id=\"4_2\" ><\/a><br>","f44f8e6d":"* This looks interesting! looks like family sizes of 1-3 have better survival rates than others.","4ee795fa":"* The barplot and factorplot shows that if a passenger is alone onboard with no siblings, he have 34.5% survival rate. The graph roughly decreases if the number of siblings increase.","d7107442":"**If You Like the notebook and think that it helped you, please upvote to It keep motivate me**","6d92432c":"#### Converting String Values into Numeric[^](#4_1)<a id=\"4_1\" ><\/a><br>\nSince we cannot pass strings to a machine learning model, we need to convert features Sex, Embarked, etc into numeric values.","49d7d083":"### Ensembling[^](#5_4)<a id=\"5_4\" ><\/a><br>\n\nEnsembling is a way to increase performance of a model by combining several simple models to create a single powerful model.\nread more about ensembling [here](https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-for-ensemble-models\/).\nEnsembling can be done in ways like: Voting Classifier, Bagging, Boosting.\n\nI will use voting method in this kernal","cb57581b":"A confusion matrix is a table that is often used to describe the performance of a classification model. read more [here](https:\/\/www.dataschool.io\/simple-guide-to-confusion-matrix-terminology\/)","d2b9c902":"## **Basic Pipeline for predictive modeling problem**[^](#1)<a id=\"1\" ><\/a><br>\n\n**<left><span style=\"color:blue\">Exploratory Data Analysis<\/span> -> <span style=\"color:blue\">Feature Engineering and Data Preparation<\/span> -> <span style=\"color:blue\">Predictive Modeling<\/span><\/left>.**\n\n1. First we need to see what the data can tell us: We call this **<span style=\"color:blue\">Exploratory Data Analysis(EDA)<\/span>**. Here we look at data which is hidden in rows and column format and try to visualize, summarize and interprete it looking for information.\n1. Next we can **leverage domain knowledge** to boost machine learning model performance. We call this step, **<span style=\"color:blue\">Feature Engineering and Data Cleaning<\/span>**. In this step we might add few features, Remove redundant features, Converting features into suitable form for modeling.\n1. Then we can move on to the **<span style=\"color:blue\">Predictive Modeling<\/span>**. Here we try basic ML algorthms, cross validate, ensemble and Important feature Extraction.","fe830dd1":"### Hyper-Parameters Tuning[^](#5_3)<a id=\"5_3\" ><\/a><br>\n\nYou might have noticed there are few parameters for each model which defines how the model learns. We call these hyperparameters. These hyperparameters can be tuned to improve performance. Let's try this for Random Forest classifier.","88900687":"---\n\n## Exploratory Data Analysis (EDA)[^](#2)<a id=\"2\" ><\/a><br>\n\nWith the objective in mind that this kernal aims to explain the workflow of a predictive modelling problem for begginers, I will try to use simple easy to understand visualizations in the EDA section. Kernals with more advanced EDA sections will be mentioned at the end for you to learn more.","85eea1e9":"---\n#### Feature: Age[^](#2_2_3)<a id=\"2_2_3\" ><\/a><br>\n**Meaning :** Age in years","b5729db8":"#### Convert Fare into a categorical feature by binning[^](#4_3)<a id=\"4_3\" ><\/a><br>","cfc85902":"* By looking at above matrices we can say that, if we are more concerned on making less mistakes by predicting survived as dead, then Naive Bayes model does better.\n* If we are more concerned on making less mistakes by predicting dead as survived, then Decision Tree model does better.","6f80408c":"Lets combine above and analyse family size. ","55fa1027":"## Feature Importance[^](#6)<a id=\"6\" ><\/a><br>\n\nWell after we have trained a model to make predictions for us, we feel curiuos on how it works. What are the features model weights more when trying to make a prediction?. As humans we seek to understand how it works. Looking at feature importances of a trained model is one way we could explain the decisions it make. Lets visualize the feature importances of the Random forest model we used inside the ensemble above.","c752e377":"#### Feature: Sex[^](#3_2_1)<a id=\"2_2_1\" ><\/a><br>","a3b00822":"**Sex:** Survival chance for female is better than that for male.\n\n**Pclass:** Being a 1st class passenger gives you better chances of survival.\n\n**Age:** Age range 5-10 years have a high chance of survival.\n\n**Embarked:** Majority of passengers borded from Southampton.The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S. All most all Passengers at Q were from Pclass3.\n\n**Family Size:** looks like family sizes of 1-3 have better survival rates than others.\n\n**Fare:** As Fare Bins increase chances of survival increases\n\n","44b186c1":"#### Observations Summary[^](#2_3)<a id=\"2_3\" ><\/a><br>","4204d834":"---\n## Feature Engineering and Data Cleaning[^](#4)<a id=\"4\" ><\/a><br>\nNow what is Feature Engineering? Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work.\n\nIn this section we will be doing,\n1. Converting String Values into Numeric\n1. Convert Age into a categorical feature by binning\n1. Convert Fare into a categorical feature by binning\n1. Dropping Unwanted Features\n","31cb3f04":"---\n#### Feature: Embarked[^](#2_2_4)<a id=\"2_2_4\" ><\/a><br>\n**Meaning :** Port of Embarkation. C = Cherbourg, Q = Queenstown, S = Southampton","1d563a17":"* The Age, Cabin and Embarked have null values.","fa1dce26":"* We have 891 data points (rows); each data point has 12 columns.","ad16d47f":"Okay so there are some misspelled Initials like Mlle or Mme that stand for Miss. Lets replace them.","6cfd3405":"---\n#### Fare[^](#2_2_6)<a id=\"2_2_6\" ><\/a><br>\n**Meaning :** Passenger fare","a777a9ae":"---\n### Analyse features[^](#2_2)<a id=\"2_2\" ><\/a><br>","236a1b2c":"We need to  do some preprocessing to  this test data set before we can feed that to the trained model.","6e12c952":"* Survival rate for passenegers below Age 14(i.e children) looks to be good than others.\n* So Age seems an important feature too.\n* Rememer we had 177 null values in the Age feature. How are we gonna fill them?.","0cdf8eb9":"#### Dropping Unwanted Features[^](#4_4)<a id=\"4_4\" ><\/a><br>\n\nName--> We don't need name feature as it cannot be converted into any categorical value.\n\nAge--> We have the Age_cat feature, so no need of this.\n\nTicket--> It is any random string that cannot be categorised.\n\nFare--> We have the Fare_cat feature, so unneeded\n\nCabin--> A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.\n\nFare_Bin--> We have the fare_cat feature.\n\nPassengerId--> Cannot be categorised.\n\nSibsp & Parch --> We got FamilySize feature\n","52dd2af2":"* For Pclass 1 %survived is around 63%, for Pclass2 is around 48% and for Pclass2 is around 25%.\n* **So its clear that higher classes had higher priority while rescue.**\n* **Looks like Pclass is also an important feature.**","efa83039":"* Since 72.5% passengers are from Southampton, So lets fill missing 2 values using S (Southampton)","8da258f6":"* Sad Story! Only 38% have survived. That is roughly 340 out of 891. ","71b3505a":"* It is clear that as Fare Bins increase chances of survival increase too.","98b57f3e":"* While survival rate for female is around 75%, same for men is about 20%.\n* It looks like they have given priority to female passengers in the rescue.\n* **Looks like Sex is a good predictor on the survival.**","101f8ebd":"Now after data cleaning and feature engineering we are ready to train some classification algorithms that will make predictions for unseen data. We will first train few classification algorithms and see how they perform. Then we can look how an ensemble of classification algorithms perform on this data set.\nFollowing Machine Learning algorithms will be used in this kernal.\n\n* Logistic Regression Classifier\n* Naive Bayes Classifier\n* Decision Tree Classifier\n* Random Forest Classifier\n","0a5f9cf5":"---\n#### Feature: Pclass[^](#2_2_2)<a id=\"2_2_2\" ><\/a><br>\n**Meaning :** Ticket class : 1 = 1st, 2 = 2nd, 3 = 3rd","17348d4a":"### Cross Validation[^](#5_1)<a id=\"5_1\" ><\/a><br>\n\nAccuracy we get here higlhy depends on the train & test data split of the original data set. We can use cross validation to avoid such problems arising from dataset splitting.\nI am using K-fold cross validation here. Watch this short [vedio](https:\/\/www.youtube.com\/watch?v=TIgfjmp-4BA) to understand what it is.\n","a1852b19":"### Prediction[^](#5_5)<a id=\"5_5\" ><\/a><br>\n\nWe can see that ensemble model does better than individual models. lets use that for predictions.","e077f6eb":"#### Filling Age NaN\n\nWell there are many ways to do this. One can use the mean value or median .. etc.. But can we do better?. Seems yes. [EDA To Prediction(DieTanic)](https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic#EDA-To-Prediction-(DieTanic)) has used a wonderful method which I would use here too. There is a name feature. First lets extract the initials.\n","8f6d58ff":"#### Filling Embarked NaN","ae5bed78":"### Lets look at overall survival stats[^](#2_1)<a id=\"2_1\" ><\/a><br>","6ec52058":"#### Correlation Between The Features[^](#2_4)<a id=\"2_4\" ><\/a><br>","fcf98c91":"* We guessed correctly. higher % of 1st class passegers boarding from C might be the reason.","e5e7b87a":"---\n## Predictive Modeling[^](#5)<a id=\"5\" ><\/a><br>\n","f7a94389":"Now we have looked at cross validation accuracies to get an idea how those models work. There is more we can do to understand the performances of the models we tried ; let's have a look at confusion matrix for each model.","2bb7a798":"---\n#### Features: SibSip & Parch[^](#2_2_5)<a id=\"2_2_5\" ><\/a><br>\n**Meaning :**  \nSibSip -> Number of siblings \/ spouses aboard the Titanic\n\nParch -> Number of parents \/ children aboard the Titanic\n\nSibSip + Parch -> Family Size ","48447c95":"# Contents\n1. [Basic pipeline for a predictive modeling problem](#1)\n1. [Exploratory Data Analysis (EDA)](#2)\n    * [Overall survival stats](#2_1)\n    * [Analysis features](#2_2)\n        1. [Sex](#2_2_1)\n        1. [Pclass](#2_2_2)\n        1. [Age](#2_2_3)\n        1. [Embarked](#2_2_4)\n        1. [SibSip & Parch](#2_2_5)\n        1. [Fare](#2_2_6) \n    * [Observations Summary](#2_3)\n    * [Correlation Between The Features](#2_4)\n1. [Feature Engineering and Data Cleaning](#4)\n    * [Converting String Values into Numeric](#4_1)\n    * [Convert Age into a categorical feature by binning](#4_2)\n    * [Convert Fare into a categorical feature by binning](#4_3)\n    * [Dropping Unwanted Features](#4_4)\n1. [Predictive Modeling](#5)\n    * [Cross Validation](#5_1)\n    * [Confusion Matrix](#5_2)\n    * [Hyper-Parameters Tuning](#5_3)\n    * [Ensembling](#5_4)\n    * [Prediction](#5_5)\n1. [Feature Importance](#6)\n","d9d98bc9":"# Start with simplest problem\n\nI feel like clasification is the easiest problem catogory to start with.\nWe will start with simple clasification problem to predict survivals of  titanic https:\/\/www.kaggle.com\/c\/titanic","ae4137ff":"### Confusion Matrix[^](#5_2)<a id=\"5_2\" ><\/a><br>","8718cb83":"* Majority of passengers borded from Southampton\n* Survival counts looks better at C. Why?. Could there be an influence from sex and pclass features we already studied?. Let's find out "}}