{"cell_type":{"41b50180":"code","67f6172e":"code","8d581dc2":"code","7d06b7e0":"code","11b5c923":"code","3869ed6b":"code","7b4a0bf5":"code","46fe1a07":"code","b7325d7d":"code","8c3093b0":"code","2b1a18d6":"code","e45823c7":"code","fbff420c":"markdown","98a143d3":"markdown","ed860d81":"markdown","7508f5f7":"markdown","15c3e2a6":"markdown","3ed91543":"markdown","fa07c9bb":"markdown","55af2ac3":"markdown"},"source":{"41b50180":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nfrom tqdm import tqdm_notebook\nimport pickle\nimport gc\nfrom sklearn.model_selection import KFold\n\nfrom jigsaw_utils import *","67f6172e":"# setting parameters.\nset_seed(42)\n\ncrawl_embedding_path = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\nglove_embedding_path = '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\nnum_models = 2\nlstm_units = 128\ndense_hidden_units = 4 * lstm_units\nmax_len = 220\nembed_size = 300\nmax_features = 120000","8d581dc2":"load = True\nif not load:\n    train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n    train = df_parallelize_run(train, text_clean_wrapper)\n    test = df_parallelize_run(test, text_clean_wrapper)\n    train.to_csv('processed_train.csv', index=False)\n    test.to_csv('processed_test.csv', index=False)\nelse:\n    train = pd.read_csv('..\/input\/jigsaw-public-files\/train.csv')\n    test = pd.read_csv('..\/input\/jigsaw-public-files\/test.csv')\n    # after processing some of the texts are emply\n    train['comment_text'] = train['comment_text'].fillna('')\n    test['comment_text'] = test['comment_text'].fillna('')","7d06b7e0":"%%time\nglove_embed = load_embed(glove_embedding_path)\noovs = vocab_check_coverage(train, glove_embed)","11b5c923":"print(oovs[0]['oov_words'][:20])","3869ed6b":"if not load:\n    tokenizer = text.Tokenizer(lower=False, num_words=max_features)\n    tokenizer.fit_on_texts(list(train['comment_text']) + list(test['comment_text']))\n    \n    # by default tokenizer keeps all words, I leave only top max_features\n    sorted_by_word_count = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n    tokenizer.word_index = {}\n    i = 0\n    for word,count in sorted_by_word_count:\n        if i == max_features:\n            break\n        tokenizer.word_index[word] = i + 1    # <= because tokenizer is 1 indexed\n        i += 1\n    \n    with open(f'tokenizer_{max_features}.pickle', 'wb') as f:\n        pickle.dump(tokenizer, f)\nelse:\n    with open(f'..\/input\/jigsaw-public-files\/tokenizer_{max_features}.pickle', 'rb') as f:\n        tokenizer = pickle.load(f)\n    \nX_train = tokenizer.texts_to_sequences(train['comment_text'])\nX_test = tokenizer.texts_to_sequences(test['comment_text'])\nx_train_lens = [len(i) for i in X_train]\nx_test_lens  = [len(i) for i in X_test]","7b4a0bf5":"y_train = np.where(train['target'] >= 0.5, 1, 0)\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\nfinal_y_train = np.hstack([y_train[:, np.newaxis], y_aux_train])","46fe1a07":"crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, crawl_embedding_path, embed_size)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, glove_embedding_path, embed_size)\nprint('n unknown words (glove): ', len(unknown_words_glove))\n\nembedding_matrix = crawl_matrix * 0.5 +  glove_matrix * 0.5\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()","b7325d7d":"embedding_matrix_small = np.zeros((embedding_matrix.shape[0], 30))","8c3093b0":"# splits for training\nsplits = list(KFold(n_splits=5, shuffle=True, random_state=42).split(X_train, final_y_train))","2b1a18d6":"test_preds = train_on_folds(X_train, x_train_lens, final_y_train, X_test, x_test_lens,\n                            splits, embedding_matrix, embedding_matrix_small, n_epochs=2, validate=False, debug=True)","e45823c7":"submission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': test_preds.mean(1)\n})\n\nsubmission.to_csv('submission.csv', index=False)","fbff420c":"Creating a small embedding, which will be trainable.","98a143d3":"## Checking vocab coverage","ed860d81":"## Training model on folds","7508f5f7":"## Loading embeddings","15c3e2a6":"## Loading data\nI have saved the processed data to the dataset, so I can load it from there.","3ed91543":"Most of out of vocab words are names, so I suppose there is nothing to do about them.","fa07c9bb":"## General information\n\n**UPD:** It seems that I messed up: importing scripts is considered to be using other kernel output, which prevents submitting from this kernel. Also this script can't be removed from this kernel, so I can't fix it :( No fun.\n\nSo here is a new kernel where all the code is inside as usual: https:\/\/www.kaggle.com\/artgor\/pytorch-text-processing-and-other-things\n\n\nIn this kernel I wanted to create a full cycle of processing text and training model in Pytorch.\n\nSome time ago Kaggle launched a new feature: possibility to import scripts into kernels and I heavily use this feature in my kernel.\n\nAll functions for preprocessing and training model are defined in the script: https:\/\/www.kaggle.com\/artgor\/jigsaw-utils\n\nThat script is based on ideas from several kernels with my changes and improvements when possible, I want to acknowledge these great works:\n\nhttps:\/\/www.kaggle.com\/adityaecdrid\/public-version-text-cleaning-vocab-65\/\nhttps:\/\/www.kaggle.com\/bminixhofer\/simple-lstm-pytorch-version\nhttps:\/\/www.kaggle.com\/authman\/simple-lstm-pytorch-with-batch-loading\nhttps:\/\/www.kaggle.com\/gpreda\/jigsaw-fast-compact-solution\n\nIf I missed someone - write to me, I'll add it.\n\nSo my kernel and script contain the following:\n\n* preprocessing texts. Mostly based on adityaecdrid code with some changes. I have a json file with mappings here: https:\/\/www.kaggle.com\/artgor\/jigsaw-public-files\n* text dataset with collating for dynamic length change\n* neural net with two embeddings. The first embedding is fasttext and glove embeddings multiplied by weights. The second one is a small trainable embedding. The idea is that this embedding could get some important information while training model\n* training model on folds\n* to be done - competition metric calculation\n* to be done - weighting loss\n* to be done - saving model while training on folds","55af2ac3":"### Tokenizing"}}