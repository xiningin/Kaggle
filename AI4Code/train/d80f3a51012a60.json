{"cell_type":{"935a4379":"code","6a3fa5a9":"code","e06c6be4":"code","999d1038":"code","c69ace38":"code","804e1750":"code","18179769":"code","0ff3507c":"code","a3631ffd":"code","7d65dca9":"code","8b49d805":"code","cffa815c":"code","8efc8d63":"code","a7f30e86":"code","5e515b4c":"code","2994cd90":"code","774754b7":"code","25ccdd93":"code","9279a393":"code","c6509518":"code","e3c0ffc9":"code","0b5c5024":"code","2fad232f":"code","2c2550f8":"code","1d6ed6ea":"code","962b51b7":"code","f0094126":"code","61684a4e":"code","e5efd156":"code","3b27048f":"code","e68c3903":"code","99e03433":"code","55e9f311":"code","a81a14e0":"code","ffa1c21b":"code","0e8b1612":"code","a7fd8c17":"code","b56a412d":"code","e33d93f2":"code","f2c86f2c":"code","6b72123e":"code","0808044e":"code","416b1294":"code","4a778183":"code","30fa548b":"code","a6208b83":"code","b1d70460":"code","9da17ae0":"code","4e43d0a4":"code","c2b5a68f":"code","d2e44474":"code","cb0c18cc":"code","4f35ce01":"code","d3847d48":"code","008a04ff":"code","01395e1d":"code","d8392d03":"code","e982adde":"markdown","6ee30290":"markdown","083f18b9":"markdown","1672a6eb":"markdown","eba18a27":"markdown","4b8746dc":"markdown","0b404341":"markdown","5dae9340":"markdown","39b90127":"markdown","ab75b4d9":"markdown","60084351":"markdown","9802881c":"markdown","24040b2c":"markdown","deb18d79":"markdown","36969496":"markdown"},"source":{"935a4379":"! pip install tensorflow --upgrade\n!pip install tensorflow-history-plot\n\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn.model_selection\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras.layers import Dense\nfrom transformers import TFBertModel, BertConfig, BertTokenizerFast\n\nfrom zipfile import ZipFile\nfrom os.path import basename\n\nimport datetime\n\nimport re\nimport string\n\nprint(tf.__version__)","6a3fa5a9":"imdb_df = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")\nimdb_df","e06c6be4":"batch_size = 32\nseed = 42\n\nimdb_data_train, imdb_data_test = sklearn.model_selection.train_test_split(imdb_df, test_size=.2)\nimdb_data_train, imdb_data_val = sklearn.model_selection.train_test_split(imdb_data_train, test_size=.2)","999d1038":"def custom_standardization(input_data):\n  lowercase = tf.strings.lower(input_data)\n  stripped_html = tf.strings.regex_replace(lowercase, '<br \/>', ' ')\n  return tf.strings.regex_replace(\n      stripped_html,'[%s]' % re.escape(string.punctuation),\n                                  '')","c69ace38":"max_features = 10000\nsequence_length = 250\n\nvectorize_layer = tf.keras.layers.TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=max_features,\n    output_mode='int',\n    output_sequence_length=sequence_length)","804e1750":"target_col = ['review']\nx_imdb_train_data = imdb_data_train[target_col]\ny_imdb_train_data = imdb_data_train.drop(target_col, axis=1)\n\nvectorize_layer.adapt(x_imdb_train_data)\n\nprint(x_imdb_train_data.head())\nprint(\"\\n\")\nprint(y_imdb_train_data.head())","18179769":"def vectorize_text(text, label):\n  text = tf.expand_dims(text, -1)\n  return vectorize_layer(text), label","0ff3507c":"imdb_data_train","a3631ffd":"imdb_data_train.apply(lambda x: vectorize_text(x.review, x.sentiment), axis=1)","7d65dca9":"train_ds = vectorize_text(imdb_data_train['review'], imdb_data_train['sentiment'])\ntrain_ds","8b49d805":"val_ds = vectorize_text(imdb_data_val['review'], imdb_data_val['sentiment'])\nval_ds","cffa815c":"test_ds = vectorize_text(imdb_data_test['review'], imdb_data_test['sentiment'])\ntest_ds","8efc8d63":"# # Clear any logs from previous runs\n# !rm -rf .\/logs\/ \n# !mkdir .\/logs\/","a7f30e86":"# # Download Ngrok to tunnel the tensorboard port to an external port\n# !wget https:\/\/bin.equinox.io\/c\/4VmDzA7iaHb\/ngrok-stable-linux-amd64.zip\n# !unzip ngrok-stable-linux-amd64.zip\n\n# # Run tensorboard as well as Ngrox (for tunneling as non-blocking processes)\n# import os\n# import multiprocessing\n\n\n# pool = multiprocessing.Pool(processes = 10)\n# results_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n#                         for cmd in [\n#                         f\"tensorboard --logdir .\/logs\/ --host 0.0.0.0 --port 6006 &\",\n#                         \".\/ngrok http 6006 &\"\n#                         ]]","5e515b4c":"# ! curl -s http:\/\/localhost:4040\/api\/tunnels | python3 -c \\\n#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","2994cd90":"# # quick tensorboard vis\n# mnist = tf.keras.datasets.mnist\n\n# (x_train, y_train),(x_test, y_test) = mnist.load_data()\n# x_train, x_test = x_train \/ 255.0, x_test \/ 255.0\n\n# def create_model():\n#   return tf.keras.models.Sequential([\n#     tf.keras.layers.Flatten(input_shape=(28, 28)),\n#     tf.keras.layers.Dense(512, activation='relu'),\n#     tf.keras.layers.Dropout(0.2),\n#     tf.keras.layers.Dense(10, activation='softmax')\n#   ])\n\n# import datetime\n# model = create_model()\n# model.compile(optimizer='adam',\n#               loss='sparse_categorical_crossentropy',\n#               metrics=['accuracy'])\n\n# log_dir = \"logs\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n# model.fit(x=x_train, \n#           y=y_train, \n#           epochs=10, \n#           validation_data=(x_test, y_test), \n#           callbacks=[tensorboard_callback])","774754b7":"# load_existing_model = False\n# base_model_name = \"male\"\n# my_model_name = str(datetime.date.today()) + \"_\" + base_model_name\n# load_model_name = \"bert-char-gen\"","25ccdd93":"# df = pd.read_csv(\"..\/input\/charactergeneratortest\/turk-train-01.csv\")\n# df = pd.read_csv(\"..\/input\/characterdescriptionsbatch1\/2021-09-14_batch-1-output.csv\")\n# df = pd.read_csv(r\"data\/_model-ready\/2021-09-14_batch-1-output.csv\")\n# df = pd.read_csv(r\"data\/_model-ready\/batch-2-output.csv\")\n# df = pd.read_csv(r\"data\/_model-ready\/2021-10-14_batch-3-output-male.csv\")\n# df = pd.read_csv(r\"..\/input\/charactergeneratorbatch3\/2021-10-14_batch-3-output-male.csv\")\n# df = pd.read_csv(r\"..\/input\/charactergen\/2021-10-09_batch-2-1-output.csv\")\n# df = pd.read_csv(r\"..\/input\/d\/alinawithaface\/charactergen\/2021-09-14_batch-0-output.csv\")\n# df = pd.read_csv(r\"..\/input\/d\/alinawithaface\/charactergen\/2021-10-09_batch-1-output.csv\")","9279a393":"# cols = ['Answer.q1_Description', 'Answer.q2_Description', 'Answer.q3_Description']\n# df['Answer.description'] = df[cols].apply(lambda row: '.'.join(row.values.astype(str)), axis=1)\n# df.dropna()","c6509518":"# # data = df.drop(['Character-ID', 'Input.image_url'], axis=1)\n\n# data = df.drop(['Character-ID'], axis=1)\n# data.dropna()","e3c0ffc9":"# kept_cols = ['Answer.description', 'Hair', 'Gender', 'Height']\n# data = df[kept_cols]","0b5c5024":"# # Train\/Test\/Val Split\n\n# data_train, data_test = sklearn.model_selection.train_test_split(data, test_size=.2)\n# data_train, data_val = sklearn.model_selection.train_test_split(data_train, test_size=.2)\n\n# x_train_data = data_train[['Answer.description']]\n# y_train_data = data_train.drop(['Answer.description'], axis=1)","2fad232f":"model_name = 'bert-base-uncased'\nmax_token_len = 50\nconfig = BertConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\ntokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path=model_name, config=config)","2c2550f8":"\n# Build Output layers\noutput_layer_names = []\n# for col in y_train_data.columns:\nfor col in y_imdb_train_data.columns:\n    output_layer_names.append(col)\nprint(output_layer_names)\nlen(output_layer_names)","1d6ed6ea":"input_dim = 50\noutput_dim = 1\n# output_dim = len(output_layer_names)\ninput_layer_name = \"input_ids\"\n\n# Input Layer\ninput_layer = keras.Input(\n    shape=input_dim,\n    name=input_layer_name,\n    dtype='int32')\n\n# Bert Layer\ntransformer_model = TFBertModel.from_pretrained(\n    model_name,\n    config=config)\nbert = transformer_model.layers[0]\ntail = bert(input_layer)[1]\n\ntail = keras.layers.Dropout(config.hidden_dropout_prob)(tail)","962b51b7":"output_layers = []\nmy_loss = {}\nmy_metrics = {}\nfor layer_name in output_layer_names:\n    # Create output layers for every part of the data\n    #     output_dim = (data[layer_name].nunique())\n    #     print(str(layer_name) + \" : \" + str(output_dim) + \"\\n\")\n    layer = Dense(\n        units=output_dim,\n        name=layer_name)(tail)\n    output_layers.append(layer)\n    # Create a loss for every part of the data\n#     my_loss.update({layer_name: 'mse'})\n#     my_loss.update({losses.BinaryCrossentropy(from_logits=True)})\n    # Create a metric for every part of the data\n#     my_metrics.update({layer_name: tf.keras.metrics.RootMeanSquaredError()})\n#     my_metrics.update({tf.metrics.BinaryAccuracy(threshold=0.0)})","f0094126":"# in the darkness bind them\nmodel = keras.Model(input_layer, output_layers)\nmodel.summary()\n# plot_model(model, to_file='bert.png')","61684a4e":"# def tokenize_input(dataFrameCol):\n# #     for col in dataFrame:\n# #         print(dataFrame[col])\n#     tokenized_data = tokenizer(\n#         text=dataFrameCol.to_list(),\n#         add_special_tokens=True,\n#         max_length=max_token_len,\n#         truncation=True,\n#         padding=True,\n#         return_tensors='tf',\n#         return_token_type_ids=False,\n#         return_attention_mask=False,\n#         verbose=True)\n# #     temp_df[col] = tokenized_data['input_ids']\n#     return tokenized_data\n\n# dataframe_tokenized = tokenize_input(imdb_data_train['review'])\n# dataframe_tokenized","e5efd156":"# dataframe_tokenized","3b27048f":"# imdb_data_train","e68c3903":"# x_train_data","99e03433":"# tokenizer_input = x_train_data['Answer.description'].to_list()\nx_tokenized = tokenize_input(imdb_data_train['review'])\n# print(tokenizer_input)\n# x_tokenized\n# print(tokenized_x)\n# input_x = tokenized_x\n# input_x = {input_layer_name: tokenized_x['input_ids']}\ninput_x = x_tokenized\ninput_x","55e9f311":"# input_y = []\ny_tokenized = tokenize_input(imdb_data_train['sentiment'])\ninput_y = y_tokenized\n\n# for layer_name in output_layer_names:\n#     #     print(layer_name)\n# #     temp = y_train_data[layer_name]\n#     temp = y_imdb_train_data[layer_name]\n#     temp = np.array(temp)\n#     input_y.append(temp)\n#     print(temp)\n# input_y = {input_layer_name: tokenized_y['input_ids']}\ninput_y","a81a14e0":"x_val = tokenize_input(imdb_data_val['review'])\ny_val = tokenize_input(imdb_data_val['sentiment'])","ffa1c21b":"# Compile the model\nmodel.compile(\n    loss =losses.BinaryCrossentropy(from_logits=True),\n#     loss=my_loss,\n    #     loss='sparse_categorical_crossentropy',\n    #     loss=keras.losses.CategoricalCrossentropy(),\n#     metrics=my_metrics,\n    metrics=tf.metrics.BinaryAccuracy(threshold=0.0),\n    optimizer=keras.optimizers.Adam())","0e8b1612":"# # load_model_path = \"output\/\" + load_model_name\n# load_model_path = \"..\/input\/bertchargenv1\/bert-char-gen\"\n\n# if load_existing_model:\n#     model = tf.keras.models.load_model(load_model_path)","a7fd8c17":"log_dir = \"logs\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n# cd cd ","b56a412d":"# https:\/\/medium.com\/octavian-ai\/how-to-use-the-learning-rate-finder-in-tensorflow-126210de9489\ndecay_steps=100\ndecay_rate=1.30\nlearning_rate = 0\nglobal_step = 1\n\n# decay_learning_rate = learning_rate * decay_rate ^ (global_step \/ decay_steps)\n\n# learning_rate = tf.train.exponential_decay(1e-10, global_step=global_step, decay_steps=your_value, decay_rate=your_value)\n\n# So it can be seen in TensorBoard later\n# tf.summary.scalar('learning_rate', learning_rate)","e33d93f2":"train_ds","f2c86f2c":"sample_num = 14\n# label_dimension = len(output_layer_names)\n\n# input_x = np.zeros((sample_num, max_token_len))\n\n# input_y = np.zeros((sample_num,label_dimension))\n\nBATCH_SIZE = 50\nEPOCHS = 10\nINIT_LR = 1e-4\nMAX_LR = 1e-2\n\n\nhistory = model.fit(\n    x=train_ds['review'],\n    y=train_ds['sentiment'],\n    validation_data=val_ds,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS\n#     callbacks=[tensorboard_callback],\n#     verbose='auto'\n)","6b72123e":"# Ready test data\n# test_y_issue = to_categorical(data_test['Issue'])\n# test_y_product = to_categorical(data_test['Product'])\n# x_test_data = data_test[['Answer.description']]\nx_test_data = data_test[['Answer.description']]","0808044e":"# x_test_data.head()\n# x_test_data.shape","416b1294":"# test_tokenizer_input = x_test_data['Answer.description'].to_list()\ntest_tokenizer_input = x_test_data['Answer.description'].to_list()","4a778183":"# print(test_tokenizer_input)","30fa548b":"# test_x = tokenizer(\nx_test_tokenized = tokenizer(\n    #     text=data_test['Consumer complaint narrative'].to_list(),\n    text=test_tokenizer_input,\n    max_length=max_token_len,\n    padding='max_length',\n    add_special_tokens=True,\n    truncation=True,\n    return_tensors='tf',\n    return_token_type_ids=True,\n    return_attention_mask=False,\n    verbose=True\n)","a6208b83":"x_test_input = {input_layer_name: x_test_tokenized['input_ids']}","b1d70460":"# print(x_test_input)","9da17ae0":"# y_test_data = data_test.drop(['Answer.description'], axis=1)\ny_test_data = data_test.drop(['Answer.description'], axis=1)","4e43d0a4":"y_test_input = y_test_data","c2b5a68f":"model.evaluate(x_test_input, y_test_input, verbose=1)","d2e44474":"my_model_name = str(datetime.date.today()) + \"_\" + base_model_name\n\nsave_model_name = \"output\/\" + my_model_name\n\ntf.keras.models.save_model(model, save_model_name)","cb0c18cc":"zip_path = 'output\/' + my_model_name + '.zip'\n\n# create a ZipFile object\nwith ZipFile(zip_path, 'w') as zipObj:\n    # Iterate over all the files in directory\n    for folderName, subfolders, filenames in os.walk(save_model_name):\n        for filename in filenames:\n            #create complete filepath of file in directory\n            filePath = os.path.join(folderName, filename)\n            # Add file to zip\n            zipObj.write(filePath, basename(filePath))","4f35ce01":"model = tf.keras.models.load_model(save_model_name)","d3847d48":"# Create fresh dataframe\npredDF = pd.DataFrame()\nfor i in range(0, len(output_layer_names)):\n    predDF[output_layer_names[i]] = 0\npredDF","008a04ff":"predict_test_phrases = [\n    \"He has a big bushy blonde beard, a red shirt, green pants, and blue eyes\",\n    \"the beard with eye glasses show he was old man. the shoes and dress show youngest person. compare this both he was shown later young person.\",\n    \"He appears to be a white man with a trimmed beard and mustache. He appears to be above average height and a few pounds overweight\the is wearing a gray and red skull cap. He has on a green and orange sweat shirt. He has on gray pants and gray shoes. He has on a pair of purple glasses.\tHe appears to be a man in his 30s. He could be a basketball player or a football player. He is slightly facing his left.\",\n    \"The person is a white male of medium build with green hair and blue eyes. They are wearing  a grey hoody with grey shorts and black shoes along with head phones.\tThey seem like they're laid back, listening to music.\",\n    \"The man is a doctor.  He is wearing blue pants and a white lab coat.  He also has a stethoscope and a mask.\"\n]","01395e1d":"for predict_test_phrase in predict_test_phrases:\n    predict_tokenized = tokenizer(\n        text=predict_test_phrase,\n        max_length=max_token_len,\n        padding='max_length',\n        add_special_tokens=True,\n        truncation=True,\n        return_tensors='tf',\n        return_token_type_ids=True,\n        return_attention_mask=False,\n        verbose=True\n    )\n\n    predict_input = {input_layer_name: predict_tokenized['input_ids']}\n\n    prediction = model.predict(\n        x=predict_input\n    )\n\n    new_row = {}\n    for i in range(0, len(output_layer_names)):\n        new_row[output_layer_names[i]] = (prediction[i][0][0])\n    new_row['InputText'] = predict_test_phrase\n    predDF = predDF.append(new_row, ignore_index=True)\n\n    # print(predict_test_phrase)\n    # prediction\npredDF","d8392d03":"predDF.to_csv(\"output\/predictions.csv\", index=False)\n# csv.writer(csvfile, )","e982adde":"---\nBuild CSV From Prediction\n---","6ee30290":"---\nLoad Existing Model (optional)\n---","083f18b9":"---\nFinal Data Processing\n---","1672a6eb":"---\nIMDB Review Classification\n---\ntesting the architecture with a larger\/known dataset","eba18a27":"---\nBuild the Model\n---","4b8746dc":"---\nEvaluate the model\n---","0b404341":"---\nModel Edition\n---","5dae9340":"---\nSave the model\n---","39b90127":"---\n# Start Tensorboard for data Visualization","ab75b4d9":"---\nImport the Data\n---","60084351":"---\nZip the Model\n---","9802881c":"---\nPredict with the model\n---","24040b2c":"---\nBring in pretrained model for tokenizing\n---","deb18d79":"---\nTrain the Model\n---","36969496":"---\nLoad the model\n---"}}