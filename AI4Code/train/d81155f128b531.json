{"cell_type":{"b57753d5":"code","870f76f6":"code","b1f589da":"code","0c9c98d0":"code","ef549fa1":"code","2f77dbf0":"code","5405a770":"code","6941bc16":"code","fd959d18":"code","276ab85c":"code","194a5222":"code","c2d11a3b":"code","fb547dd3":"code","ce9e32f4":"code","927c7968":"code","e7f1ca79":"code","ff86deae":"code","ed17ba7c":"code","3859679c":"code","66b873f4":"code","dfe8ef68":"code","15aab90d":"code","9b3f5364":"code","4dccba7c":"code","46dcc5cc":"code","13a91863":"code","56bd7827":"code","a444c18d":"code","40e8adbd":"code","608f952e":"code","139dc049":"code","20b4fbcb":"code","e5e461c7":"markdown","e32eb316":"markdown","b0af001c":"markdown","d646dc88":"markdown","5d03a587":"markdown","6db37d3f":"markdown","ba3d4d7e":"markdown","54e225e3":"markdown","b8e8eb29":"markdown","b4a3c2ea":"markdown","cc3628d8":"markdown","06588f0f":"markdown","9537f5e5":"markdown","69519541":"markdown","f59cbda3":"markdown","4a75ef2a":"markdown","a880a4c6":"markdown","a5ca2e7a":"markdown","45120ec4":"markdown","483c4c71":"markdown","fccdbe3c":"markdown"},"source":{"b57753d5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n#model selction tools\nfrom sklearn.model_selection import train_test_split , GridSearchCV\\\n        , StratifiedKFold , TimeSeriesSplit,KFold,cross_val_score\n#metrics \nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error\n#models \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor,BaggingRegressor,GradientBoostingRegressor\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb\nimport catboost as cb\n#tools\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline","870f76f6":"#---------- configs\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\n%matplotlib inline","b1f589da":"#Dataset Paths\ntrain_path = r\"..\/input\/dry-beans-classification-iti-ai-pro-intake01\/train.csv\"\ntest_path = r\"..\/input\/dry-beans-classification-iti-ai-pro-intake01\/test.csv\"\n#Load Train Dataset\ndf = pd.read_csv(train_path,index_col='ID')\nprint(\"Shape:\",df.shape)\ndf.head()","0c9c98d0":"df.info()","ef549fa1":"df.describe()","2f77dbf0":"df.isna().sum()","5405a770":"df.nunique()","6941bc16":"# #Drop Unused columns\n# df.drop(columns=['ID'],inplace=True)","fd959d18":"#Factorize classes of the bean\ndf['y_factorized'],_ = pd.factorize(df['y'],sort = True)\ndf","276ab85c":"plt.figure(figsize=(14,8))\nsns.countplot(x=\"y\", data=df, palette=\"Set3\")\nplt.show()","194a5222":"def plot_boxenplots(df,x_cols=[],rows = 2,cell_size = 4):\n    size = len(x_cols)\n    cols = size \/\/ rows\n    fig,axes = plt.subplots(rows,cols,figsize=(cols * cell_size, cols * cell_size),sharey=True)\n    fig.suptitle(\"Distributions\")\n    for i,axe in enumerate(axes.flatten()):\n        if(i < size):\n            sns.boxenplot(data = df,x=x_cols[i],y='y',palette='Set2',ax=axe)\n        else:\n            print('subplots > n of columns, change n of rows')\n            break \n        axe.set_title(x_cols[i])\n    \n    plt.tight_layout()\n\n    plt.show()","c2d11a3b":"plot_boxenplots(df,df.iloc[:,:-2].columns,rows = 4,cell_size =4)","fb547dd3":"def plot_distribution(df,x_cols=[],rows = 2,cell_size = 4):\n    size = len(x_cols)\n    cols = size \/\/ rows\n    fig,axes = plt.subplots(rows,cols,figsize=(cols * cell_size, rows * cell_size))\n    fig.suptitle(\"Variable Distribution\",size= 'large')\n    for i,axe in enumerate(axes.flatten()):\n        if(i < size):\n            sns.distplot(df[x_cols[i]],ax=axe,rug=True)\n            median = df[x_cols[i]].median()\n            axe.set_title(x_cols[i] + f' ,Median : {median:0.1f}',size= 'large')\n            axe.axvline(median, color ='red',lw=2, alpha = 0.55)\n        else:\n            print('subplots > n of columns, change n of rows')\n            break \n    plt.tight_layout()\n    plt.show()","ce9e32f4":"plot_distribution(df,df.iloc[:,:-2].columns,rows=4,cell_size=5)","927c7968":"plt.figure(figsize=(10,6))\nsns.scatterplot(x=\"Area\" , y=\"Solidity\" , hue=\"y\" , data=df)\nplt.title('Area vs Solidity')\nplt.show()","e7f1ca79":"sns.pairplot(df, hue = 'y', palette='gist_rainbow')\nplt.show()","ff86deae":"#Correlation Map\ncorrmatrix = df.corr()\nfig,ax = plt.subplots(figsize=(12,12))\nsns.heatmap(corrmatrix,annot=True,fmt='.2f',cmap='YlGnBu',vmin=-1,vmax=1, square=True,ax=ax)\nplt.xticks(rotation=90)\nplt.title('Correlation Map')\nplt.show()","ed17ba7c":"def DERM_SIRA(df):\n    df[\"DERM_SIRA\"] = ( (np.exp(df['ShapeFactor4']) + np.log(df['ShapeFactor1'].mean())) **2 ) + ( (np.log(df['ConvexArea'])) + np.log(df['Perimeter']))\n    \ndef shape_cali(df):\n    df['shape_cali'] = (df['MajorAxisLength']  ) - (df['ShapeFactor4'] ) \ndef shape_barbunya(df):\n    df['shape_barbunya'] = (df['MinorAxisLength']  ) - (df['ShapeFactor1'] ) \n\n    \ndef shape_sira(df):\n    df[\"shape_sira\"] = ((df['Solidity']**2) + (df['ShapeFactor1']**2) + df['Eccentricity']   ) - ((df['Extent']**2) + (df['ShapeFactor4']**2) + df['MinorAxisLength'])\n\n\ndef shape_dermason(df):\n    df[\"shape_dermason\"] = df['ShapeFactor1'] - (df['Area'] \/ 10000)\n    \ndef shape_seker(df):\n    df['shape_seker'] = (df['Compactness']  ) - (df['AspectRation'] )     \n\n\ndef shape_horoz(df):\n    df['shape_horoz'] = (df['Eccentricity']) - (df['ShapeFactor3'] ) \n    \ndef shape_CALI(df):\n    df[\"shape_CALI\"] = 0.1 *( ((np.log(df['Area']))+(np.log(df['MajorAxisLength']))+df['AspectRation']+df['Eccentricity']+df['roundness']) - (df['Compactness']+df['ShapeFactor2']+df['ShapeFactor3']+ df['ShapeFactor4']) )\n    \ndef shape_CALI_2(df):\n    df[\"shape_CALI_2\"] = ( df['AspectRation']+df['Eccentricity']) - (df['Compactness']+df['ShapeFactor2']+df['ShapeFactor3'] )\n# print(df[\"shape_cali\"] = df['Area']*df['MajorAxisLength']*df['AspectRation']*df['Eccentricity']*df['roundness'] \/ (df['Compactness']*df['ShapeFactor2']*df['ShapeFactor3'*, df['ShapeFactor4']))\n\ndef shape_sira_2(df):\n    df[\"shape_sira_2\"] = 1000* ((df['shape_sira']**2)    ) \/ ((df['ShapeFactor4']**2)  )\n\ndef log_feature(df):\n    cols = df.drop(columns=['y']).columns\n    for col in cols:\n        df[col+'_log'] = np.log(df[col].mean())\n        \ndef standariseMinMax(df):\n    from sklearn.preprocessing import QuantileTransformer\n    quantile = QuantileTransformer()\n    from sklearn.preprocessing import PowerTransformer\n    power = PowerTransformer(method = 'box-cox')\n     #'' box-cox yeo-johnson\n    from sklearn.preprocessing import RobustScaler\n    robust = RobustScaler()\n    from sklearn.preprocessing import MinMaxScaler\n    minmax = MinMaxScaler()\n        \n    \n    columns = [ 'Area' , 'ShapeFactor4', 'EquivDiameter', \"ConvexArea\",\"AspectRation\",\n       'Solidity', 'roundness',  'ShapeFactor1', 'ShapeFactor2',  'Perimeter','MajorAxisLength','MinorAxisLength'\n              ,'Eccentricity','Extent','Solidity','Compactness', 'ShapeFactor3', 'ShapeFactor4']\n    \n    \n    for col in columns:\n#         df[col] =robust.fit_transform(df[col].to_numpy().reshape((df[col].shape[0],1)))\n        df[col] =quantile.fit_transform(df[col].to_numpy().reshape((df[col].shape[0],1)))\n\n        \n    columns2 = [\"shape_horoz\",\"shape_seker\",\"shape_dermason\",\"shape_sira\" ,\"shape_barbunya\",\"shape_cali\"\n               ,'shape_CALI', 'shape_CALI_2', 'shape_sira_2','DERM_SIRA']\n    for col in columns2:\n#         df[col] =robust.fit_transform(df[col].to_numpy().reshape((df[col].shape[0],1)))\n        df[col] =quantile.fit_transform(df[col].to_numpy().reshape((df[col].shape[0],1)))\n\n        ","3859679c":"def processing(df):\n    shape_horoz(df)\n    shape_seker(df)\n    shape_dermason(df)\n    shape_sira(df)\n    shape_barbunya(df)\n    shape_cali(df)\n\n    shape_CALI(df)\n    shape_CALI_2(df)\n    shape_sira_2(df)\n    \n    DERM_SIRA(df)\n    standariseMinMax(df)\n    \nprocessing(df)\n\n\n# df_ldaD , lda = to_lda_Train(df)","66b873f4":"col_1 = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength']\ncol_2 = ['AspectRation', 'Eccentricity', 'ConvexArea', 'EquivDiameter']\ncol_3 = ['Extent','Solidity', 'roundness', 'Compactness']\ncol_4 = ['ShapeFactor1', 'ShapeFactor2','ShapeFactor3', 'ShapeFactor4']\ncol_5 = [\"shape_horoz\",\"shape_seker\",\"shape_dermason\",\"shape_sira\" ,\"shape_barbunya\",\"shape_cali\"]\n\n\ndef feature_distribution(cols):\n    classes = ['HOROZ', 'SEKER', 'DERMASON', 'SIRA', 'BARBUNYA', 'CALI']\n    fig,axes = plt.subplots(6,int(len(classes)\/6),figsize=(18, 30), sharey=False)\n    fig.suptitle(\"Distributions\")\n#     \/=for clas_ in classes:\n    all_medians = []\n    for i,axe in enumerate(axes.flatten()):\n        medians = []\n        axe.set_title(classes[i],size= 'large')\n        for c in range(len(cols)):\n            if(i <= len(classes)):\n                sns.distplot( df[df[\"y\"] == classes[i]  ][cols[c]]  ,ax=axe,rug=True , hist=False )\n                median = df[df[\"y\"] == classes[i]  ][cols[c]].median()\n#                 axe.set_title(classes[i] + f' ,Median : {median:0.1f}',size= 'large')\n                axe.axvline(median, color ='red',lw=2, alpha = 0.55)\n                medians.append((cols[c],median))\n        all_medians.append((classes[i],medians))\n    fig.legend(labels= cols)\n    plt.tight_layout()\n    plt.show() \n    return all_medians\nall_medians = feature_distribution(col_1+col_2+col_3+col_4+col_5)","dfe8ef68":"Features = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength',\n       'AspectRation',  'ConvexArea', 'EquivDiameter', 'Extent',\n       'Solidity', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2',\n        'ShapeFactor4', 'shape_horoz', 'shape_seker',\n       'shape_dermason', 'shape_sira', 'shape_barbunya', 'shape_cali',\n       'shape_CALI', 'shape_sira_2','DERM_SIRA']\n\nX =  df[Features]\nY = df['y']\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=132)","15aab90d":"# knn - k-nearest neighbours\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel1 = KNeighborsClassifier()\nmodel1.fit(x_train, y_train)\n# print metric to get performance\nprint(\"Accuracy: \",model1.score(x_test, y_test) * 100)\ny_pred = model1.predict(x_test)\nf1 = f1_score(y_test, y_pred, average='micro')\nprint(\"f1_score: \",f1)","9b3f5364":"# decision tree\nfrom sklearn.tree import DecisionTreeClassifier\nmodel2 = DecisionTreeClassifier(random_state=42)\nmodel2.fit(x_train, y_train)\n# print metric to get performance\nprint(\"Accuracy: \",model2.score(x_test, y_test) * 100)\ny_pred = model2.predict(x_test)\nf1 = f1_score(y_test, y_pred, average='micro')\nprint(\"f1_score: \",f1)","4dccba7c":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbag_clf = BaggingClassifier(\n    DecisionTreeClassifier(), n_estimators=500,\n    max_samples=100, bootstrap=True, random_state=42)\nbag_clf.fit(x_train, y_train)\ny_pred = bag_clf.predict(x_test)\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, y_pred))","46dcc5cc":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.ensemble import VotingClassifier\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.svm import SVC\n\n# log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n# rnd_clf = RandomForestClassifier(random_state=42)\n# svm_clf = SVC(random_state=42)\n# knn_clf = KNeighborsClassifier(n_neighbors=8)\n# mlp_clf = MLPClassifier(random_state=42)\n# dt_clf  = DecisionTreeClassifier(random_state=42)\n# cb_clf  = cb.CatBoostClassifier(loss_function='MultiClass',depth= 2,iterations= 1000,learning_rate= 0.05,  silent=True)\n# bag_clf = BaggingClassifier(\n#     DecisionTreeClassifier(), n_estimators=1000,\n#     max_samples=200, bootstrap=True, random_state=42)\n\n# voting_clf = VotingClassifier(\n#     estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf),('knn',knn_clf),('bag',bag_clf),('mlp',mlp_clf)],\n#     voting='hard')","13a91863":"# voting_clf.fit(x_train, y_train)","56bd7827":"# from sklearn.metrics import accuracy_score\n\n# for clf in (log_clf, rnd_clf, svm_clf,knn_clf,bag_clf,mlp_clf ,voting_clf):\n#     clf.fit(x_train, y_train)\n#     y_pred = clf.predict(x_test)\n#     print(f\"{clf}\", accuracy_score(y_test, y_pred))\n#     f1 = f1_score(y_test, y_pred, average='micro')\n#     print(\"f1_score: \",f1)\n","a444c18d":"# # make a prediction with a stacking ensemble\n# from sklearn.datasets import make_classification\n# from sklearn.ensemble import StackingClassifier\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.svm import SVC\n# from sklearn.naive_bayes import GaussianNB\n# from xgboost import XGBClassifier\n\n# # define the base models\n# level0 = list()\n# level0.append(('rnd', RandomForestClassifier(random_state=42)))\n# level0.append(('cat', cb.CatBoostClassifier(loss_function='MultiClass',depth= 2,iterations= 1000,learning_rate= 0.05,  silent=True)))\n# level0.append(('cart', DecisionTreeClassifier(random_state=42)))\n# level0.append(('svm', SVC(random_state=42,C= 5, gamma= 0.1, kernel= 'rbf',probability=True)))\n# level0.append(('bag', BaggingClassifier(DecisionTreeClassifier(), n_estimators=1000,max_samples=200, bootstrap=True, random_state=42)))\n# level0.append(('mlp', MLPClassifier(random_state=42)))\n# level0.append(('knn', KNeighborsClassifier(n_neighbors=8)))\n# level0.append(('xgb', XGBClassifier()))\n# # define meta learner model\n# level1 = cb.CatBoostClassifier(iterations=1500,  learning_rate=0.05, verbose=0)\n# # define the stacking ensemble\n# model = StackingClassifier(estimators=level0, final_estimator=level1, cv=10)\n# # fit the model on all available data\n# model.fit(x_train, y_train)\n# # make a prediction for one example\n# y_pred = model.predict(x_test)\n# f1 = f1_score(y_test, y_pred, average='micro')\n# print(\"f1_score: \",f1)","40e8adbd":"model = cb.CatBoostClassifier(loss_function='MultiClass',depth=2,iterations= 1000,\n                    learning_rate= 0.05,l2_leaf_reg=0.002, silent=True)\nmodel.fit(x_train, y_train)\ny_predcb = model.predict(x_test)\nprint(\"Accuracy: \",model.score(x_test, y_test) * 100)\nf1 = f1_score(y_test, y_predcb, average='micro')\nprint(\"f1_score: \",f1)","608f952e":"df_test = pd.read_csv(test_path)\ndf_test.head()\nprocessing(df_test)","139dc049":"X_test =  df_test[Features]\npred = model.predict(X_test)\n\n# add y column to the test data\ndf_test['y'] = pred\n\ndf_test.head()","20b4fbcb":"df_test[['ID', 'y']].to_csv('\/kaggle\/working\/submission.csv', index=False)","e5e461c7":"# Importing Libraries","e32eb316":"## Get information about Non-Null values & Dtype of columns","b0af001c":"## Get statistical info(mean,std,min,max,etc..)","d646dc88":"## Notes on ('Area','Perimeter','MajorAxisLength', 'MinorAxisLength') Distributions : \n>* the 4 features share common informations\n   >>* for each bean class , the Area and Perimeter has the same distinct distribution from the other classes.\n   >>* for bombay beans has the largest [Area,Perimeter] , for the dermason has the smallest [Area,Perimeter].\n   >>* Cali And Barbunya share semi equal distributions for the 4 features plotted above.\n   \n## Notes on ('AspectRation', 'Compactness', 'roundness', 'EquivDiameter') Distributions :\n\n>* Aspect Ratio \n   >>* Give idea on the Length\/width , for example from the visual images we expect the horoz to have the highest aspect ration and seker to be closer to ~1, and this is correct according to the distribution plot. (the rest share close to similar distributions).\n   \n>* Compactness\n   >>*  Measures the roundness of an object by the following equation: ~ an equivalent circle diameter \/ Length\n   >>*  Seker is the most compact meaning it's close to be a circle , on the contrary horoz is the least because the aspect ratio described above.\n   >>*  Cali is in the lower end also due to its aspect ratio.\n    \n>* Roundness\n   >>*  Measures the roundness of an object by the following equation: ~  (4pi * Area)\/(perimeter^2)\n   >>*  Seker is the most round followed by dermason ... only one problem here , the outliers, huge skewness.\n   >>*  it's not obvious to me the validity of the equation used in the calculation where depending on pixel values for (area , perimeter) it can lead to misleading info depending on the bean orientation in the 3d space.\n   \n>* EquivDiameter\n   >>*  The diameter of a circle having the same area as a bean seed area. \n   >>*  Has similar distribution characterstic to the area, perimeter, major and minor lengths.\n\n## Notes on ('Eccentricity', 'ConvexArea', 'Extent','Solidity') Distributions : \n\n>* Eccentricity\n   >>* Gives idea of how much the seed is elongated , ~e=1 means a very stretched(on length) seed, ~e=0 means a circular seed.\n   \n>* ConvexArea\n   >>*  Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\n   >>*  Similar to the Area Distribution.\n    \n>* Extent\n   >>*  The ratio of the pixels in the bounding box to the bean area.\n   >>* ~Extent = 1 , when the bean is squarish (filling the bounding box).\n   >>* one note , the bimodal distributions , this is clear indication of the 3D orientation for example the horoz seed is the most -elongated- seed it has bimodal distribution due to the fact of different orientations will give different extent ratio .. on the contrary the seker seed is closer to a be circlular giving close to normal distrinution..\n   >>* don't know how to benefit from this, to be investigated.\n   \n>* Solidity\n   >>*  Also known as convexity. The ratio of the pixels in the convex shell to those found in beans\n   >>*  doesn't give me clear variance between the classes. to be investigated later..\n   \n## Notes on ('ShapeFactor1', 'ShapeFactor2','ShapeFactor3', 'ShapeFactor4') Distributions : \n\n>* no details were given , maybe in the paper they have the reasoning behind the 4 features.\n>* there is unique variance in the distributions for 'ShapeFactor1', 'ShapeFactor2'.\n>* shape factor 3 is similar to shape factor 2 ,shape factor 4 has similar characteristics with Solidity attribute.","5d03a587":"# Training Model","6db37d3f":"![13.PNG](attachment:13.PNG)","ba3d4d7e":"## Visualization","54e225e3":"The dataset is imbalanced","b8e8eb29":"# Test Dataset","b4a3c2ea":"# $$Precision_{micro} = \\frac{\\sum_{k \\in C} TP_k}{\\sum_{k \\in C} TP_k + FP_k}$$ $$Recall_{micro} = \\frac{\\sum_{k \\in C} TP_k}{\\sum_{k \\in C} TP_k + FN_k}$$ \n# F1-score is the harmonic mean of precision and recall \n$$MeanFScore = F1_{micro}= \\frac{2 Precision_{micro} Recall_{micro}}{Precision_{micro} + Recall_{micro}}$$","cc3628d8":"# Bivariate Analysis","06588f0f":"# 1-EDA (Exploratory Data Analysis)","9537f5e5":"### Check for Balancing in dataset","69519541":"#### mostly skewed multi-modal distributions with possible big percentage of outliers.","f59cbda3":"# Features","4a75ef2a":"## Check for Number of Unique Values","a880a4c6":"# Data fields\n* ID - an ID for this instance\n* Area - (A), The area of a bean zone and the number of pixels within its boundaries.\n* Perimeter - (P), Bean circumference is defined as the length of its border.\n* MajorAxisLength - (L), The distance between the ends of the longest line that can be drawn from a bean.\n* MinorAxisLength - (l), The longest line that can be drawn from the bean while standing perpendicular to the main axis.\n* AspectRatio - (K), Defines the relationship between L and l.\n* Eccentricity - (Ec), Eccentricity of the ellipse having the same moments as the region.\n* ConvexArea - (C), Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\n* EquivDiameter - (Ed), The diameter of a circle having the same area as a bean seed area.\n* Extent - (Ex), The ratio of the pixels in the bounding box to the bean area.\n* Solidity - (S), Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.\n* Roundness - (R), Calculated with the following formula: (4piA)\/(P^2)\n* Compactness - (CO), Measures the roundness of an object: Ed\/L\n* ShapeFactor1 - (SF1)\n* ShapeFactor2 - (SF2)\n* ShapeFactor3 - (SF3)\n* ShapeFactor4 - (SF4)\n* y - the class of the bean. It can be any of BARBUNYA, SIRA, HOROZ, DERMASON, CALI, BOMBAY, and SEKER.","a5ca2e7a":"# Univariate Analysis","45120ec4":"There is no missing values in the dataset","483c4c71":"## Reading the Data","fccdbe3c":"## Check for Missing Values"}}