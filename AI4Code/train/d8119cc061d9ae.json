{"cell_type":{"5d29a633":"code","1f0b8390":"code","cb0773d2":"code","a2729d43":"code","4fe8e39c":"code","e345da20":"code","fe5ac6ad":"code","b0849ced":"code","8a2416e3":"code","4c613cc3":"code","92b7ef8f":"code","5ac06d19":"code","c684f712":"code","6c0ffb14":"code","6cc67367":"code","6f0a8eb4":"code","ce936f57":"markdown","be64e335":"markdown","daa122e5":"markdown","2a3c7e43":"markdown","6c8e79f5":"markdown","b7e10e2c":"markdown","b6b636b8":"markdown","f02ee19f":"markdown","0b495910":"markdown","447a75fb":"markdown","b7abf93f":"markdown"},"source":{"5d29a633":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1f0b8390":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv', index_col='id')\ntrain_df.describe()","cb0773d2":"y = train_df['target']\nX = train_df.drop('target', axis='columns')","a2729d43":"normal_X = (X - X.mean()) \/ X.std()\nnormal_X.describe()","4fe8e39c":"X_train, X_test, y_train, y_test = train_test_split(normal_X, y, random_state=42)\ninput_shape = [X_train.shape[1]]","e345da20":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.losses import BinaryCrossentropy\n\nlayer_size = 64\n\nmodel = keras.Sequential([\n    layers.Dense(layer_size, activation='swish', input_shape=input_shape),\n    layers.Dropout(0.25),\n    layers.Dense(layer_size, activation='swish'),\n    layers.Dropout(0.25),\n    layers.Dense(1, activation='sigmoid'),\n])\n\nbce = BinaryCrossentropy(label_smoothing=0.1)\n\nmodel.compile(\n    optimizer='rmsprop',\n    loss=bce,  # 'binary_crossentropy',\n    metrics=['AUC', 'accuracy']\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.001,\n    restore_best_weights=True,\n)","fe5ac6ad":"history = model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    batch_size=512,\n    epochs=500,\n    callbacks=[early_stopping]\n)\n\nhistory_df = pd.DataFrame(history.history)\nprint(\"Best Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\nprint(\"Best Validation Accuracy: {:0.4f}\".format(history_df['val_accuracy'].max()))\nprint(\"Best Validation ROC AUC: {:0.4f}\".format(history_df['val_auc'].max()))","b0849ced":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14, 4))\naxes[0].plot(history_df.index, history_df[['loss', 'val_loss']])\naxes[0].set_title(\"Cross-entropy\")\n\naxes[1].plot(history_df.index, history_df[['accuracy', 'val_accuracy']])\naxes[1].set_title(\"Accuracy\")\n\naxes[2].plot(history_df.index, history_df[['auc', 'val_auc']])\naxes[2].set_title(\"AUC\")\nfig.tight_layout()","8a2416e3":"y_pred = model.predict(X_test)\ny_pred = np.squeeze(y_pred)","4c613cc3":"predictions_df = pd.DataFrame({'prediction': y_pred, 'target': y_test})\ntarget_0 = predictions_df[predictions_df['target'] == 0]\ntarget_1 = predictions_df[predictions_df['target'] == 1]","92b7ef8f":"plt.figure(figsize=(16,6))\nsns.kdeplot(data=target_0['prediction'], label=\"Target=0 predictions\", shade=True)\nsns.kdeplot(data=target_1['prediction'], label=\"Target=1 predictions\", shade=True)\nplt.title(\"Distribution of Predictions by target label\")\nplt.legend()","5ac06d19":"test_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv', index_col='id')\nnormal_test_df = (test_df - X.mean()) \/ X.std() # use X mean and stddev to avoid leakage","c684f712":"model.fit(\n    normal_X, y,\n    batch_size=512,\n    epochs=25\n)","6c0ffb14":"# Make predictions on the normalized test data set.\ny_pred = model.predict(normal_test_df)\ny_pred = np.squeeze(y_pred)","6cc67367":"submission_df = pd.DataFrame({'id': test_df.index, 'target': y_pred})\nsubmission_df.to_csv('submission.csv', index=False)","6f0a8eb4":"plt.figure(figsize=(15,8))\nsns.histplot(x=y_pred, kde=True)\nplt.title(\"Predictions Distribution\")\nplt.xlabel(\"Prediction\")\nplt.show()","ce936f57":"### Load and normalize the test set","be64e335":"### Create and compile the model\n\nHere I created a simple neural network with two hidden layers. In early tests, I noticed that the model was overfitting after a very small number of epochs, so I added a small amount of dropout to each layer. Since I didn't know how many epochs the model would need to train, I added an early stopping callback as well.","daa122e5":"### Plot the predictions to see their distribution","2a3c7e43":"### Normalize and split the training data\n\nAll of the columns are numeric, and none of the columns have missing values, so we can dive right into normalization. The features in the data set are in many different ranges of values. Normalization will put them all in the similar ranges with a mean of 0 and standard deviation of 1.","6c8e79f5":"This is a good distribution for a first attempt at a binary classifier, so I'll use this as my first submission. With a few tweaks to the model, this could be a high-ranking model in the competition.","b7e10e2c":"### Load the training data set","b6b636b8":"### Fit the model, plot the loss, accuracy, and AUC","f02ee19f":"### Retrain the model on all of the data for (hopefully) better generalization\n\nThe model started to overfit the validation data after about 25 epochs during training, so we'll stop at 25 when fitting the model to the complete set of training data.","0b495910":"### Plot predictions vs. actual target values","447a75fb":"This shows that most of our predictions in the test set are in the right range, but there's still a sizable spike of incorrect predictions.","b7abf93f":"### Create the submission file"}}