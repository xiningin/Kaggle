{"cell_type":{"4bd06953":"code","e523b260":"code","4caf01dd":"code","f3565f56":"code","bc67efba":"code","a53e231a":"code","27e33b1b":"code","3e562ab0":"code","bfd883b2":"code","53be63d7":"code","8aa68275":"code","7aca91fa":"code","83ee6f61":"code","69efa0b7":"code","10100d08":"code","e5d53410":"code","ba329ffa":"code","0bbcdd93":"code","5e86b7b5":"markdown","c4c30b2e":"markdown","008f1a9e":"markdown","4f394872":"markdown","f9b55625":"markdown","e4ed659a":"markdown","12950c55":"markdown","d8924e4f":"markdown","bc080b9d":"markdown","1af8189a":"markdown","e484b11b":"markdown","9fb6254f":"markdown","b0c312de":"markdown"},"source":{"4bd06953":"from sklearn.datasets import make_regression\n\nn_samples = 100\nn_features = 4\nn_informative = 2\n\nX, y = make_regression(\n    n_samples=n_samples,\n    n_features=n_features,\n    n_informative=n_informative,\n    random_state=0,\n)","e523b260":"from sklearn.linear_model import Ridge, Lasso\n\nridge = Ridge()\nridge.fit(X, y)\nprint(f'Ridge coef: {ridge.coef_}')\n\nlasso = Lasso()\nlasso.fit(X, y)\nprint(f'Lasso coef: {lasso.coef_}')","4caf01dd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f3565f56":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain_df.head()","bc67efba":"train_df = train_df.drop('Id', axis=1)","a53e231a":"target = 'SalePrice'\ncategorical_features = []\nnumeric_features = []\nfeatures = train_df.columns.values.tolist()\nfor col in features:\n    if train_df[col].dtype != 'object': \n        if col != target:\n            numeric_features.append(col)\n    else:\n        categorical_features.append(col)\n        \nfor col in numeric_features:\n    mean = train_df[col].mean()\n    train_df[col] = train_df[col].fillna(mean)\n    \nfor col in categorical_features:\n    train_df[col] = train_df[col].fillna('None')","27e33b1b":"train_df['SalePrice'] = np.log1p(train_df['SalePrice'])","3e562ab0":"from scipy.stats import skew\nskewed_feats = train_df[numeric_features].apply(lambda x: skew(x)) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\ntrain_df[skewed_feats] = np.log1p(train_df[skewed_feats])","bfd883b2":"from sklearn.preprocessing import LabelEncoder\n# Encoding categorical features\nfor col in categorical_features:\n    le = LabelEncoder()\n    le.fit(list(train_df[col].astype(str).values))\n    train_df[col] = le.transform(list(train_df[col].astype(str).values))","53be63d7":"y = train_df['SalePrice']\nX = train_df.drop('SalePrice', axis=1)","8aa68275":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\n\nalpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\nmax_iter = [1000, 10000, 100000]\nlasso_gscv = GridSearchCV(estimator=Lasso(), \n                                param_grid={'alpha': alpha,\n                                        'max_iter': max_iter},\n                                scoring='neg_mean_absolute_error',\n                                cv=5,\n                                refit=False)","7aca91fa":"lasso_gscv.fit(X, y)\nlasso_gscv.best_params_","83ee6f61":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","69efa0b7":"lasso = Lasso(alpha = lasso_gscv.best_params_['alpha'], \n                        max_iter = lasso_gscv.best_params_['max_iter'])\nlasso.fit(X_train, y_train)","10100d08":"print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train))) \nprint(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test))) ","e5d53410":"coef = pd.Series(lasso.coef_, index = X_train.columns)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","ba329ffa":"important_features = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])","0bbcdd93":"important_features.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","5e86b7b5":"Let us try to understand this by generating a random regression problem.","c4c30b2e":"## Build Lasso Regression model","008f1a9e":"Lasso Regression is quite similar to Ridge Regression but with a difference. \n\nBoth add penalty to non-zero coefficients.\n\n\nRidge Regression penalizes sum of squared coefficients - ${theslope}^2$\n\nLasso Regression penalizes the sum of their absolute values - $|{theslope}|$ ","4f394872":"# Lasso Regression","f9b55625":"This regression problem contains 4 features, with only 2 features being informative, other 2 features being noise.","e4ed659a":"If we have a large amount of features and expect only a few of them to be important, Lasso Regression can help by excluding useless features.","12950c55":"${\\lambda}$ controls the strength of the penalty. \n\n${\\lambda}$ = 0 , Lasso Regression = Linear Regression\n\nAs ${\\lambda}$ increases, more and more coefficients are set to zero and eliminated (theoretically, when ${\\lambda}$ = ${\\infty}$, all coefficients are eliminated).","d8924e4f":"## Apply Lasso Regression on House Price Dataset","bc080b9d":"Ridge Regression tries to shrink the coefficients of non-informative features close to zero, but Lasso Regression reduces them to exactly zero.\n\nHaving some coefficients be exactly zero often makes a model easier to interpret, and can reveal the most important features of the model.","1af8189a":"# Summary","e484b11b":"${L}_{ridge}({\\hat\\beta}) = \\sum\\limits_{i=1}^{n}({Y}_i - \\hat{Y}_i)^2 + {\\lambda} * \\sum\\limits_{j=1}^{m}{\\hat\\beta}_j^2$","9fb6254f":"Lasso (Least Absolute Shrinkage and Selection Operator) is a regression analysis method that performs both feature selection and regularization in order to enhance the prediction accuracy and interpretability of the model.","b0c312de":"${L}_{lasso}({\\hat\\beta}) = \\sum\\limits_{i=1}^{n}({Y}_i - \\hat{Y}_i)^2 + {\\lambda} * \\sum\\limits_{j=1}^{m}|{\\hat\\beta}_j|$"}}