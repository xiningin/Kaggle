{"cell_type":{"3be00f83":"code","9a8bcdbb":"code","3856f2e0":"code","10a9468e":"code","a02cc0eb":"code","34262479":"code","036a1561":"code","9cd6800a":"code","3e8feac3":"code","7e92d261":"code","08ebcf4b":"code","159de7f9":"code","7d6e54bc":"markdown","f641e6f4":"markdown","c033adc0":"markdown","d452a932":"markdown","462ecbaa":"markdown","7940497f":"markdown","a03f481f":"markdown","d3a50b59":"markdown","a13d6e6c":"markdown","74c47188":"markdown","e0ebbec0":"markdown"},"source":{"3be00f83":"import pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom nltk.stem.snowball import SnowballStemmer \nfrom nltk.corpus import stopwords\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport eli5\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nimport gensim\nfrom sklearn import manifold\nfrom gensim.models.word2vec import Word2Vec\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D","9a8bcdbb":"test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\nX = train['text']\ny = train['target']\ntest = test['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\nX_train, X_test, y_train, y_test = list(X_train), list(X_test), list(y_train), list(y_test)","3856f2e0":"stop = stopwords.words('english')\ndef tokenizer(text):\n    tokenized = []\n    for string in text:\n        string = re.sub('[^a-z\\sA-Z]', '', string)\n        string = re.sub('http\\S+', '', string)\n        tokenized.append([w for w in string.split() if w not in stop])\n    return tokenized\n\nsnow_stemmer = SnowballStemmer(language='english') \ndef stemmer(text):\n    stem_string = []\n    for string in text: \n        stem_string.append([snow_stemmer.stem(word) for word in string])\n    return stem_string \n\nX_train = tokenizer(X_train)\nX_train = stemmer(X_train)\ntest = tokenizer(test)\ntest = stemmer(test)\nX_test = tokenizer(X_test)\nX_test = stemmer(X_test)\n\nX_train_corrected = [\" \".join(x) for x in X_train]\nX_test_corrected = [\" \".join(x) for x in X_test]\ntest_corrected = [\" \".join(x) for x in test]\n","10a9468e":"tfidf = TfidfVectorizer(lowercase=False, stop_words='english', preprocessor=None)\nX_tfidf_train = tfidf.fit_transform(X_train_corrected)\nX_tfidf_test = tfidf.transform(X_test_corrected )\ntest = tfidf.transform(test_corrected)\n\nX_tfidf_train.shape, X_tfidf_test.shape","a02cc0eb":"logit = LogisticRegression(random_state=0)\nparam = {'C': [0.1, 1, 2, 3],\n        'solver': ['lbfgs', 'liblinear']}\nlogit_grid = GridSearchCV(estimator = logit, param_grid = param, \n                          scoring = 'f1', n_jobs = -1,)\nlogit_grid.fit(X_tfidf_train, y_train)\nlogit_grid.best_params_","34262479":"logit = LogisticRegression(C = 4, solver = 'lbfgs',random_state=0)\nlogit.fit(X_tfidf_train, y_train)","036a1561":"y_pred = logit.predict(X_tfidf_test)\nprint('F1 = ', f1_score(y_true = y_test, y_pred = y_pred))\nprint('Accuracy = ', precision_score(y_true = y_test, y_pred = y_pred))","9cd6800a":"eli5.show_weights(estimator=logit, feature_names = list(tfidf.get_feature_names()),\n                 top=(20, 20))","3e8feac3":"corpus = X_train\nnlp = gensim.models.word2vec.Word2Vec(corpus, size=200,   \n            window=6, min_count=1, sg=1, iter=30)","7e92d261":"nlp.most_similar(\"fire\")","08ebcf4b":"## choose a word 'fire' to compute its weights\nfig = plt.figure()\nword = \"fire\"\ntot_words = [word] + [tupla[0] for tupla in \n                 nlp.most_similar(word, topn=20)]\nX = nlp[tot_words]\n\n## pca to reduce dimensionality from 300 to 3\npca = manifold.TSNE(perplexity=40, n_components=3, init='pca')\nX = pca.fit_transform(X)\n\n## create data frame with 3 axis\ndtf_ = pd.DataFrame(X, index=tot_words, columns=[\"x\",\"y\",\"z\"])\ndtf_[\"input\"] = 0\ndtf_[\"input\"].iloc[0:1] = 1\n\n# plot 3d\nfig = plt.figure(figsize = (10,8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(dtf_[dtf_[\"input\"]==0]['x'], \n           dtf_[dtf_[\"input\"]==0]['y'], \n           dtf_[dtf_[\"input\"]==0]['z'], c=\"black\")\nax.scatter(dtf_[dtf_[\"input\"]==1]['x'], \n           dtf_[dtf_[\"input\"]==1]['y'], \n           dtf_[dtf_[\"input\"]==1]['z'], c=\"red\")\nax.set(xlabel=None, ylabel=None, zlabel=None, xticklabels=[], \n       yticklabels=[], zticklabels=[])\nfor label, row in dtf_[[\"x\",\"y\",\"z\"]].iterrows():\n    x, y, z = row\n    ax.text(x, y, z, s=label)\nplt.show()","159de7f9":"y_pred = logit.predict(test)\nnew_id = pd.read_csv('..\/input\/nlp-getting-started\/test.csv') \nids = list(new_id.id)\nfinal_submission = pd.DataFrame({'id': ids, 'target':y_pred})\nfinal_submission.to_csv('final_submission.csv', index = False) ","7d6e54bc":"### Here we just reduce dimension of vector embeddings from 200 to 3. These 3 dimensions are our new axis for 3D scatter plot. ","f641e6f4":"###  Interestingly that a word 'california' has quite big weight.\n### Now we will try to create words vectors using word2vec and vizualize it with 3D scatter plot","c033adc0":"## Load the data and split it","d452a932":"## Find params for logistic regression based on Tfid features","462ecbaa":"## Compute Tfid ","7940497f":"### First of all, let`s see how well our model understood the text and what words it considers to be close by meaning to the word 'fire'.","a03f481f":"## Now let`s show weights of words importance ","d3a50b59":"It seems to me that our vector embeddings reflects the meaning of words not bad if we take into account that it doesn`t have configured parameters","a13d6e6c":"## Show F1 score and accuracy","74c47188":"## Make tokenizer and stemmer for cleaning data","e0ebbec0":"### Not very good. But it still makes sense because it also shows 'forest', 'wild'. Model could work better if we better cleaned text and have played with parameters of word2vec. "}}