{"cell_type":{"da2d699d":"code","a78037ef":"code","42ccc3cc":"code","0569495b":"code","51604ba8":"code","f1724179":"code","12599710":"code","096e0762":"code","384577b6":"code","dcda3c3f":"code","5c3f0d0e":"code","67a2e0a2":"code","530a60ac":"code","60fc9022":"code","d06aa86a":"code","a92e2c4f":"code","64c6e582":"code","560fb88b":"code","3d3d077c":"code","d927e07b":"code","0327cd1d":"code","57569c3d":"code","ac1069c4":"code","80a7bc15":"code","eae64541":"code","50d93eb6":"code","ce804eef":"code","6237dbc3":"code","cc895289":"code","cd0bac86":"code","8869886d":"code","039a411f":"code","b114482f":"code","0223599e":"code","ebc523af":"code","b45bfa02":"code","af573fe1":"code","e08392da":"code","f04c77e6":"code","8a0f4cda":"code","c859b220":"code","2343c01f":"code","5efe7481":"code","55f4696f":"code","a6e16441":"code","45cd646e":"markdown","897fd200":"markdown","618ebfc6":"markdown","b5e4f26b":"markdown","cba8f491":"markdown","060b6732":"markdown","6b520760":"markdown","982225d7":"markdown","4566f47c":"markdown","5718f870":"markdown","a9d94c42":"markdown","29c69b39":"markdown","9f42f06e":"markdown","9444a68f":"markdown","6455747f":"markdown","00b923ac":"markdown","3f8cbfb9":"markdown","de4bbe76":"markdown","499c06a8":"markdown","58c31460":"markdown","1a4e4632":"markdown","c0fccec6":"markdown","f8a2e690":"markdown"},"source":{"da2d699d":"!pip install --quiet datatable\n!pip install --quiet stumpy\n!pip install --quiet yfinance","a78037ef":"import datatable as dt\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport stumpy\nimport yfinance\nimport matplotlib\nimport matplotlib.cm as cm\nfrom scipy.cluster import hierarchy as hc\nfrom scipy.stats import spearmanr\nimport missingno\n\nplt.style.use('bmh')\nplt.rcParams[\"figure.figsize\"] = [16, 5]  # width, height","42ccc3cc":"%%time\n\ntrain_data_datatable = dt.fread('..\/input\/jane-street-market-prediction\/train.csv')","0569495b":"%%time\n\ntrain_data = train_data_datatable.to_pandas()","51604ba8":"train_data.info()","f1724179":"train_data = train_data.query('weight > 0')","12599710":"train_data.info()","096e0762":"train_data.head()","384577b6":"cnt = train_data[['date', 'weight']].groupby('date').agg(['count'])\ncnt_mean = cnt.mean().values[0]\ncnt.plot(legend=False, title='Trades per Day (train)');\nplt.axhline(cnt_mean, linestyle='--', alpha=0.85, c='r');","dcda3c3f":"train_data['resp'].mean()","5c3f0d0e":"daily_avg_ret = train_data[['date', 'resp']].groupby('date').agg(['mean'])\ndaily_avg_ret.plot(\n    title=f'Daily mean market return of {daily_avg_ret.mean().values[0]}'\n);","67a2e0a2":"cumu_series = (1 + daily_avg_ret).cumprod()\ncumu_series.plot(legend=False, title='\"Market\" return over train period');","530a60ac":"raw_data = yfinance.download('SPY', start='2002-12-01', threads=True) \nraw_data.head()","60fc9022":"spy = raw_data['Adj Close']","d06aa86a":"spy.plot()","a92e2c4f":"spy_ret = spy.pct_change()","64c6e582":"distance_profile = stumpy.core.mass(daily_avg_ret.iloc[:, 0], spy_ret)","560fb88b":"idx = np.argmin(distance_profile)\n\nprint(f'The nearest neighbor to our \"market\" is located at index {idx} in SPY')\nprint(f'This is the date starting {spy_ret.index[idx]}')","3d3d077c":"spy_series = spy.iloc[idx:(idx+499)].reset_index(drop=True)\/spy.iloc[idx]\n\nplt.plot(spy_series)\nplt.plot(cumu_series)","d927e07b":"all_columns = train_data.columns\ncolumns = all_columns[train_data.columns.str.contains('feature')]","0327cd1d":"%%time\ncardinality = train_data[columns].nunique()","57569c3d":"cardinality.sort_values()","ac1069c4":"train_data['feature_43'].hist(bins=50)","80a7bc15":"train_data['feature_43']","eae64541":"\nnsamples = len(train_data)\ntrain_cut = int(nsamples*0.80)\n\nthreshold = 0.0004  # this is like de-meaning\n\ny = (train_data['resp'].values > threshold).astype(int)\n\ny_valid = y[(train_cut+1):]\n\ndTrain = lgb.Dataset(train_data[list(columns)][0:train_cut], y[0:train_cut])\nvalidationSet = lgb.Dataset(train_data[list(columns)][(train_cut+1):], y[(train_cut+1):])\n\nparams = {\n    'min_child_samples': 100,\n    'objective': 'binary',\n    'max_depth': 8,\n    'learning_rate': 0.05,\n    'boosting_type': \"gbdt\",\n    'subsample_freq': 10,\n    'subsample': 0.33,\n    'bagging_seed': 42,\n    'metric': 'binary_logloss',\n    'feature_fraction': 0.80,\n    'extra_trees': False    # can't get this to work; it's in version 3.0+; we are less than that\n}","50d93eb6":"%%time\nmodel = lgb.train(\n    params,\n    dTrain,\n    500,\n    early_stopping_rounds = 100,\n    valid_sets = [validationSet],\n    verbose_eval = 50\n)","ce804eef":"# pick a random sample of days\nsample_days = np.random.choice(np.arange(499), 50)","6237dbc3":"%%time\ncorr = np.round(spearmanr(train_data.query('date in @sample_days').fillna(-999)).correlation, 4)","cc895289":"plt.matshow(corr);","cd0bac86":"corr[np.isnan(corr)] = 0\ncorr_condensed = hc.distance.squareform(1-corr, checks=True)\nz = hc.linkage(corr_condensed, method='average')","8869886d":"tags = pd.read_csv('..\/input\/jane-street-market-prediction\/features.csv')\ntag_names = tags.copy()\n\nfor index, row in tags.iterrows():\n    for i in range(29):\n        if row[i+1] == True:\n            tag_names.iloc[index, i+1] = row.index[i+1]\n        else:\n            tag_names.iloc[index, i+1] = np.nan\n\nndf = pd.DataFrame([r.str.cat(sep=\",\") for i, r in tag_names.iloc[:,0:].iterrows()])\nflat_list = ['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp'] + \\\n    [item for sublist in ndf.values for item in sublist] + \\\n    ['ts_id']\nflat_list = [sub.replace(',', ' ') for sub in flat_list]","039a411f":"importances = model.feature_importance()\nminima = min(importances) - 0.20*(max(importances) - min(importances))\nmaxima = max(importances)\n\nnorm = matplotlib.colors.Normalize(vmin=minima, vmax=maxima, clip=True)\nmapper = cm.ScalarMappable(norm=norm, cmap=cm.Greys)","b114482f":"ignore = ['resp', 'resp_1', 'resp_4', 'resp_3', 'resp_2', 'weight', 'ts_id', 'date']\npruned = [x for x in flat_list if x not in ignore]","0223599e":"fig = plt.figure(figsize=(12,30))\ndendrogram = hc.dendrogram(z, labels=flat_list, orientation='left', leaf_font_size=10)\n\nax = plt.gca()\nylbls = ax.get_ymajorticklabels()\nfor lbl in ylbls:\n    try:\n        imp = importances[pruned.index(lbl.get_text())]\n        lbl.set_color(mapper.to_rgba(imp))\n    except:\n        lbl.set_color('r')\nplt.title('Features Clustered; Tags Noted; Label Color is f(lgb feature importance)')\nplt.show()","ebc523af":"feat_1 = 'feature_42'\nfeat_2 = 'feature_43'\nplt.scatter(train_data[feat_1], train_data[feat_2]);","b45bfa02":"feat_1 = 'feature_60'\nfeat_2 = 'feature_61'\nplt.scatter(train_data[feat_1], train_data[feat_2]);","af573fe1":"feat_1 = 'feature_65'\nfeat_2 = 'feature_66'\nplt.scatter(train_data[feat_1], train_data[feat_2]);","e08392da":"feat_1 = 'feature_83'\nfeat_2 = 'feature_77'\n\nplt.scatter(train_data[feat_1], train_data[feat_2]);\n","f04c77e6":"feat_1 = 'feature_7'\nfeat_2 = 'feature_8'\n\nplt.scatter(train_data[feat_1], train_data[feat_2]);","8a0f4cda":"feat_1 = 'feature_27'\nfeat_2 = 'feature_28'\n\nplt.scatter(train_data[feat_1], train_data[feat_2]);","c859b220":"feat_1 = 'feature_57'\nfeat_2 = 'feature_58'\n\nplt.scatter(train_data[feat_1], train_data[feat_2]);","2343c01f":"feat_1 = 'feature_122'\nfeat_2 = 'feature_128'\n\nplt.scatter(train_data[feat_1], train_data[feat_2]);","5efe7481":"feat_2 = 'feature_51'\nfeat_1 = 'weight'\n\nplt.scatter(train_data[feat_1], train_data[feat_2]);\nplt.xlabel(feat_1)\nplt.ylabel(feat_2);","55f4696f":"import seaborn as sns\ntrain_data['target'] = ((train_data['resp']*train_data['weight']) > 0).astype(int)\nsns.pairplot(\n    train_data.query('date in @sample_days')[[f'feature_{x}' for x in [42, 43]] + ['target']],\n    plot_kws={'alpha': 0.1},\n    #hue='target'\n);","a6e16441":"feat_1 = 'weight'\nfeat_2 = 'resp'\nplt.scatter(train_data[feat_1], (train_data[feat_2]));\nplt.xlabel(feat_1)\nplt.ylabel(feat_2);","45cd646e":"I am going to sample days. Importantly, not sampling rows -- we need to keep the days together, I believe, to preserve the full cross section of the securities where available.","897fd200":"Reminder: log loss of 0.693 is random guessing. https:\/\/towardsdatascience.com\/estimate-model-performance-with-log-loss-like-a-pro-9f47d13c8865","618ebfc6":"\n## The relationship between weight and resp\n\nThis is interesting. Lower weight trades have a much higher dispersion in resp. **This implies that weight is some kind of (valid) predictor of future return volatility**. ","b5e4f26b":"Here we get the distance profile between the two return time series. Then we find the overlap where the distance is the smallest.","cba8f491":"Lots of kernels plot a corr matrix; not that helpful IMHO :-).","060b6732":"## Feature Clustering with Tags\n\n[This insightful comment from the host ](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/198965#1088878) leads me to believe we should be able to figure out which tags are days in history and which will allow us to do some sensible feature combination.\n\nThe first steps to this are to cluster features. We use **spearman rank correlation**. Generally, Spearman is the best thing to use in financial data problems. Also using a rank statistic is consistent with decision tree based learners which are simply splitting and are invariant to range. Then we plot the heirarchy of the spearman rank correlation matrix.","6b520760":"# Jane Street Ongoing EDA\n\n<img src=\"https:\/\/images.wsj.net\/im-64845?width=300\" \/>\n","982225d7":"We assume (?) these are US equities and the market is proxied by S&P500 (SPY)","4566f47c":"This is not super convincing. It's unlikely we are being given days from 2012... to be continued. I think rather these are just samples of days and stocks fron many years of history.","5718f870":"I've read in the discussion that the time periods are disjoint, so cumulating this may not make sense; but, let's see anyhow.","a9d94c42":"There is probably some better pandas magic to do this, but here we go. I make a label string of the feature name and the associated tags.","29c69b39":"**The darker labels represent features that have higher feature importance in the LightGBM model.**\n\nWhat do we do with this?\n\n![image.png](attachment:image.png)\n\n\n- we can drop a feature where we see a feature pair which implies there is redundant information. This can speed training and inference in tree models, make a model more robust to overfitting, and reduce collinearity in linear models.\n\nFor example, the relationship and inclusion of **feature_61** and **feature_60** should be investigated.\n\n- also we can get some intution on what the tags might be and use that to make additional features. Based on the clustering above, I am going to speculate that tags 0, 1, 2, 3, 4 relate to historical time periods. Why? Let's imagine that we have a feature like the trailing standard deviation of returns (a very common financial feature; aka historic volatility of a stock\/security); let's call this function `hist_vol(n_days_history)`. The rank correlations of `hist_vol(30 days)`, `hist_vol(15 days)`, and `hist_vol(10 days)` will be very high because the days overlap. The tags for the set of anonymized `hist_vol` features would look something like:\n    - feature 1 (which is `hist_vol(30)`): tag A, tag J\n    - feature 2 (which is `hist_vol(15)`): tag B, tag J\n    - feature 3 (which is `hist_vol(10)`): tag C, tag J\n\nAs such, we will see one tag repeated (in this case \"J\") to indicate this is a volatility feature. Then we would see tags indicating the 10, 15, 30 day histories (in this case, \"A\", \"B\" \"C\"). If tags A, B, C represent different historic periods, then we should see A,B,C again for other time series features.\n","9f42f06e":"The smallest cardinality is > 20000 after feature_0. So it appears that none of these features, except, feature_0, is categorical. I was expecting more -- like sector, subsector, etc. Just checking....","9444a68f":"## Can we match this market to a particular point in time?\n\nTLDR: not really.\n\nThis is admitedly a little goofy, but let's try anyway.\n\n![image.png](attachment:image.png)\n\nWe can use `stumpy` for this (https:\/\/stumpy.readthedocs.io\/en\/latest\/Tutorial_Pattern_Searching.html).","6455747f":"## How many trades per day?\n\nA lot! The US equity market has the most liquid stocks of any market in the world. If the time horizon is daily, and this were a single market you would not expect to see more than ~3000 line items per day. We see much more than that.","00b923ac":"However, what can be helpful is to cluster this matrix **and to show the feature importance**.","3f8cbfb9":"Trades are generally positive (mean) by about 4bps. \n\nTo me this means we should be doing some **target engineering**. In train we are in an upward trending market. Who knows if that will be the case in the out-of-sample period. Perhaps de-meaning `resp` before binarizing it.","de4bbe76":"Comments and questions welcome. Thank you for reading.","499c06a8":"## What kind of market regime are we in in train?\n\nWe can make an index-like return series by averaging the daily returns and cumulating them. Perhaps it makes sense to use `weight` as an index weight. I am not doing that here though. I am assuming that the days are contiguous but there is reason to believe that they are not.","58c31460":"## Feature Cardinality\n\nWhich features are categorical?\n\nTLDR.... just feature_0","1a4e4632":"So, no categoricals.","c0fccec6":"Since we don't care about situations when weight==0, I am dropping all those. This is a topic of debate. ","f8a2e690":"First we need some feature importances. I will get them from LightGBM."}}