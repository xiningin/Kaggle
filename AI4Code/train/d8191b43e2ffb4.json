{"cell_type":{"0fb77983":"code","3f053865":"code","5da90b38":"code","267a7739":"code","9df656f2":"code","0cbe3c78":"code","f261f8f8":"code","1513c892":"code","27d2db2b":"code","1de68756":"code","ecbfcb0f":"code","37b68864":"code","631673bb":"code","5da2f895":"code","4bf9cc8f":"code","d3a43508":"code","52ab7fd9":"code","6ee321d8":"code","77b0937e":"code","b842b595":"code","f9f3d190":"code","cc4cd373":"code","16c74b53":"code","4276b0b8":"code","ec9f2dba":"code","8a9dc8f3":"code","78ebeec5":"code","479a6f11":"code","9eb49520":"code","6569f4cf":"code","038af997":"code","43467665":"code","2b013c11":"code","b1a9d03f":"code","7e93d134":"code","4baa21e5":"code","9cdf9e11":"code","9192a5c6":"code","16c4e804":"code","65de1d6f":"markdown","320483c9":"markdown","18ac9a9f":"markdown","827a5c5b":"markdown","10910841":"markdown","f1a088db":"markdown","62de132b":"markdown","b225d00b":"markdown","2b76f6b8":"markdown"},"source":{"0fb77983":"#import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.pandas.set_option('display.max_rows',None) \npd.pandas.set_option('display.max_columns',None) ","3f053865":"#load dataset\ndf = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","5da90b38":"#datafame shape\nprint(df.shape)","267a7739":"#observing the central tendency and other essentials of the dataset\ndf.describe()","9df656f2":"#unnamed 32 is not essential so dropping it\ndf = df.drop(['Unnamed: 32', 'id'], axis=1)\ndf.head()","0cbe3c78":"#counting the number of Malignant and Benign in diagnosis column\ndf['diagnosis'].value_counts()","f261f8f8":"#encoding the the target feature\ndf['diagnosis']= df['diagnosis'].replace('M', 1)\ndf['diagnosis']= df['diagnosis'].replace('B', 0)","1513c892":"df.head()","27d2db2b":"#plotting the corellation matrix\ncorr = df.corr()\nplt.figure(figsize=(18,18))\nsns.heatmap(corr, cmap='coolwarm', annot = True)\nplt.show()","1de68756":"#finding out the positively corelated feature\ncc=corr[abs(corr['diagnosis']) > 0.5].index\nprint('- Number of most correlated features = ', len(cc))\nprint('--------------------------------------------------')\nprint('- Most correlated features is: \\n ',cc)","ecbfcb0f":"acc=df[df.columns[:]].corr()['diagnosis']\nprint('All features  with thier correlations is: \\n',acc)","37b68864":"#finding out the negatively corelated feature\ncc2=corr[abs(corr['diagnosis']) <= 0.5].index\nprint('- Number of Least correlated features = ', len(cc2))\nprint('--------------------------------------------------')\nprint('- Least correlated features is: \\n ',cc2)","631673bb":"#selecting 20 most essential features\ndf = df[['diagnosis', 'radius_mean', 'area_mean',\n       'compactness_mean', 'concavity_mean', 'concave points_mean',\n       'perimeter_worst', 'area_worst', 'compactness_worst',\n       'concavity_worst', 'concave points_worst','texture_mean', \n         'smoothness_mean', 'symmetry_mean','area_se','fractal_dimension_se',\n         'texture_worst', 'smoothness_worst', 'symmetry_worst',\n       'fractal_dimension_worst']]\ndf.head()","5da2f895":"print(df.shape)\ndf.tail(10)","4bf9cc8f":"# create hist and kde plots to observe the data distribution\nfig, ax = plt.subplots(ncols=5, nrows=4, figsize=(30,20))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df.items():\n    col_dist = sns.histplot(value, ax=ax[index], color=\"blue\", label=\"100% Equities\", kde=True, stat=\"density\", linewidth=0)\n    col_dist.set_xlabel(col,fontsize=18)\n    col_dist.set_ylabel('density',fontsize=18)\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)","d3a43508":"# selecting only the skewed columns to transform it in gaussian distribution\ndf_new = df[['radius_mean', 'area_mean',\n       'compactness_mean', 'concavity_mean', 'concave points_mean',\n       'area_worst', 'compactness_worst',\n       'concavity_worst', 'area_se','fractal_dimension_se',\n         'symmetry_worst', 'fractal_dimension_worst']].copy()\nprint(df_new.shape)\ndf_new.head()","52ab7fd9":"#using standard deviation method to treat the skewness\nfor feature in df_new:\n    upper_limit = df_new[feature].mean() + 3*df_new[feature].std()\n    lower_limit = df_new[feature].mean() - 3*df_new[feature].std()\n    df_new[feature] = np.where(df_new[feature]>upper_limit, upper_limit,\n                    np.where( df_new[feature]<lower_limit, lower_limit,\n                    df_new[feature]))","6ee321d8":"# create hist and kde plots to observe the data distribution\nfig, ax = plt.subplots(ncols=4, nrows=3, figsize=(20,10))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df_new.items():\n    scol_dist = sns.histplot(value, ax=ax[index], color=\"blue\", label=\"100% Equities\", kde=True, stat=\"density\", linewidth=0)\n    col_dist.set_xlabel(col,fontsize=18)\n    col_dist.set_ylabel('density',fontsize=18)\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)","77b0937e":"#merging the normalized and the remaining datframes into one for making the model\ndf_to_be_added = df[['diagnosis', 'perimeter_worst', 'concave points_worst','texture_mean', \n         'smoothness_mean', 'symmetry_mean', 'texture_worst', 'smoothness_worst']].copy()\n\nframes = [df_to_be_added, df_new]\n\ndf_final = pd.concat(frames, axis=1)\ndf_final.head()","b842b595":"df_final.shape","f9f3d190":"# create hist and kde plots to observe the data distribution\nfig, ax = plt.subplots(ncols=5, nrows=4, figsize=(30,20))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df_final.items():\n    col_dist = sns.histplot(value, ax=ax[index], color=\"blue\", label=\"100% Equities\", kde=True, stat=\"density\", linewidth=0)\n    col_dist.set_xlabel(col,fontsize=18)\n    col_dist.set_ylabel('density',fontsize=18)\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)","cc4cd373":"#plotting the corellation matrix\ncorr = df_final.corr()\nplt.figure(figsize=(12,6))\nsns.heatmap(corr, cmap='coolwarm', annot = True)\nplt.show()","16c74b53":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics","4276b0b8":"#dividing the dataframe into training and target features\nx = df_final.drop(['diagnosis'], axis=1)\ny = df_final['diagnosis']","ec9f2dba":"x.shape, y.shape","8a9dc8f3":"#splitting the dataframe and keeping 80% of the data for training and rest 20% for testing\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify = y)","78ebeec5":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","479a6f11":"#training the data using Logistic Regression Classifier\nfrom sklearn.linear_model import LogisticRegression\nclassifier_lr = LogisticRegression()\nclassifier_lr.fit(x_train, y_train)\n\nprediction_lr = classifier_lr.predict(x_test)\nprint(classification_report(y_test, prediction_lr, \n                            target_names = ['Malignant(Class 0)','Benign (Class 1)']))\nmetrics.plot_roc_curve(classifier_lr, x_test, y_test)","9eb49520":"#plotting the confusion matrix for test set\nfrom sklearn.metrics import plot_confusion_matrix, accuracy_score, classification_report\nplot_confusion_matrix(classifier_lr,x_test,y_test)\nplt.show()  ","6569f4cf":"#printing the accuracy for test set\nprint('Accuracy of Logistic Regression model is {}'.format(accuracy_score(y_test,prediction_lr)*100))","038af997":"#training the data using Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier()\ntree.fit(x_train, y_train)\nprediction_dt = tree.predict(x_test)\nprint(classification_report(y_test, prediction_dt))\nmetrics.plot_roc_curve(tree, x_test, y_test)","43467665":"#plotting the confusion matrix for test set\nfrom sklearn.metrics import confusion_matrix\nplot_confusion_matrix(tree,x_test,y_test)\nplt.show()","2b013c11":"#printing the accuracy for test set\nprint('Accuracy of Decision Tree model is {}'.format(accuracy_score(y_test,prediction_dt)*100))","b1a9d03f":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier()\nforest.fit(x_train, y_train)\nprediction_rf = forest.predict(x_test)\nprint(classification_report(y_test, prediction_rf))\nmetrics.plot_roc_curve(forest, x_test, y_test)","7e93d134":"#plotting the confusion matrix for test set\nplot_confusion_matrix(forest,x_test,y_test)\nplt.show()","4baa21e5":"#printing the accuracy for test set\nprint('Accuracy of Random Forest model is {}'.format(accuracy_score(y_test,prediction_rf)*100))","9cdf9e11":"import xgboost\nxgb = xgboost.XGBClassifier(earning_rate=0.009, n_estimators=350, subsample=0.8, max_depth=7)\nxgb.fit(x_train,y_train)\nprediction_xgb = xgb.predict(x_test)\nprint(classification_report(y_test, prediction_xgb, \n                            target_names = ['Malignant(Class 0)','Benign (Class 1)']))\nmetrics.plot_roc_curve(xgb, x_test, y_test)","9192a5c6":"#plotting the confusion matrix for test set\nplot_confusion_matrix(xgb,x_test,y_test)\nplt.show()","16c4e804":"#printing the accuracy for test set\nprint('Accuracy of XGB model is {}'.format(accuracy_score(y_test,prediction_xgb)*100))","65de1d6f":"# Read Data","320483c9":"# PREPROCESS","18ac9a9f":"# DECISION TREE","827a5c5b":"# XGBOOST","10910841":"Great news! The data has no null values. Also all variables except *diagnosis* are in float form.","f1a088db":"# RANDOM FOREST","62de132b":"There are two diagnosis results and they are in object form. I will replace M with 1 and B with 0. So if the *diagnosis* is 1, which means it is cancerous.\n* M, malignant: 1, Cancerous\n* B, benign: 0, Not Cancerous","b225d00b":"We will not be needing these.","2b76f6b8":"# LOGISTIC REGRESSION"}}