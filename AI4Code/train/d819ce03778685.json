{"cell_type":{"5e052ae7":"code","d3382bf9":"code","b4f86593":"code","a8fd5bff":"code","cd2486cc":"code","19c3ff69":"code","b6e86172":"code","16a61ad8":"code","e5fae38e":"code","f2e339b2":"code","4aebe3f9":"code","22809ccd":"code","c3d34232":"code","2fc14d14":"code","4418c810":"code","64d548fb":"code","01d83d4d":"code","2be2cd66":"code","b27f8919":"code","370971e2":"code","31b90185":"code","c93e80f2":"code","aa0598b9":"code","9a58317b":"code","c16df96f":"code","b6dbae09":"code","5fc19442":"code","0a336440":"code","ce21ea96":"code","8bc272f0":"code","1e92bc17":"code","59cf82eb":"code","a9a90473":"code","b9f76a72":"code","13119baf":"code","b7d33d57":"code","2d526961":"code","5922bc2d":"code","0ca60f2e":"code","93bd7930":"code","c85e0fd5":"code","c493e6a2":"code","e14a3bdb":"code","806cdab3":"markdown","fd3eb836":"markdown","ccf79a64":"markdown","3374b17e":"markdown","58db176d":"markdown","d4f3b3f7":"markdown","c3ab28f5":"markdown","1a20de43":"markdown","0f6d11ac":"markdown","0b1f6f33":"markdown","c8f1da35":"markdown","2d368fc3":"markdown","69cd960d":"markdown","1057a7c0":"markdown","ae63003e":"markdown","3a913d87":"markdown","3e3f8da1":"markdown","0534cc83":"markdown","e242c89f":"markdown","36ce8d92":"markdown","a5b7706a":"markdown","b1a1aa3f":"markdown","a0737f16":"markdown"},"source":{"5e052ae7":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","d3382bf9":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b4f86593":"stroke_data = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")\nstroke_data.head()","a8fd5bff":"stroke_data.shape","cd2486cc":"# Total Null Values\nstroke_data.isna().sum()","19c3ff69":"stroke_data.describe()","b6e86172":"stroke_data[\"bmi\"].fillna(stroke_data[\"bmi\"].mean(), inplace=True)","16a61ad8":"stroke_data.isna().sum()","e5fae38e":"# lets look at data once more so we can see that nan values in bmi has been changed to mean value.\nstroke_data.head()","f2e339b2":"# As the id column has no use for us so we will remove that\nstroke_data.drop(columns={'id'}, inplace=True)","4aebe3f9":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns","22809ccd":"rcParams[\"figure.figsize\"] = 12, 12","c3d34232":"sns.countplot(stroke_data[\"heart_disease\"])","2fc14d14":"plt.figure(figsize=(15, 15))\nfig, axs = plt.subplots(1, 3)\naxs[0].boxplot(stroke_data[\"age\"])\naxs[0].set_title('Age', size=20)\naxs[1].boxplot(stroke_data[\"avg_glucose_level\"])\naxs[1].set_title('Glucose Level', size=20)\naxs[2].boxplot(stroke_data[\"bmi\"])\naxs[2].set_title(\"BMI\", size=20)","4418c810":"sns.distplot(stroke_data[\"avg_glucose_level\"], color=\"blue\", label=\"Glucose Level\")","64d548fb":"sns.distplot(stroke_data[\"bmi\"], color=\"red\", label=\"Body Mass Index\")","01d83d4d":"# Finding the all outliers inside the glucose column\n\ndef FindOutliers(data):\n    outliers = []\n\n    Q1, Q3 = data.quantile([0.25, 0.75])\n\n    IQR = Q3 - Q1\n\n    upper_range = Q3 + IQR*(1.5)\n    lower_range = Q1 - IQR*(1.5)\n    \n    for x in data:\n        if x > upper_range or x < lower_range:\n            outliers.append(x)\n            \n    return outliers, upper_range, lower_range","2be2cd66":"# Outliers for the column avg_glucose_level\noutliers_glucose_level, upper_glucose_lev, lower_glucose_lev = FindOutliers(stroke_data[\"avg_glucose_level\"])","b27f8919":"# Outliers for the column bmi\noutliers_bmi, upper_bmi, lower_bmi = FindOutliers(stroke_data[\"bmi\"])","370971e2":"# Total number of outliers in these two columns\nprint(len(outliers_glucose_level), len(outliers_bmi))","31b90185":"# Applying capping for the glucose level column\nstroke_data[\"avg_glucose_level\"] = np.where(stroke_data[\"avg_glucose_level\"] < lower_glucose_lev, lower_glucose_lev, stroke_data[\"avg_glucose_level\"])\nstroke_data[\"avg_glucose_level\"] = np.where(stroke_data[\"avg_glucose_level\"] > upper_glucose_lev, upper_glucose_lev, stroke_data[\"avg_glucose_level\"])","c93e80f2":"# Performing Capping for the Bmi column\nstroke_data[\"bmi\"] = np.where(stroke_data[\"bmi\"] < lower_bmi, lower_bmi, stroke_data[\"bmi\"])\nstroke_data[\"bmi\"] = np.where(stroke_data[\"bmi\"] > upper_bmi, upper_bmi, stroke_data[\"bmi\"])","aa0598b9":"stroke_data.describe()","9a58317b":"plt.figure(figsize=(15, 15))\nfig, axs = plt.subplots(1, 2)\naxs[0].boxplot(stroke_data[\"avg_glucose_level\"])\naxs[0].set_title('Glucose Level', size=20)\naxs[1].boxplot(stroke_data[\"bmi\"])\naxs[1].set_title(\"BMI\", size=20)","c16df96f":"from sklearn.preprocessing import LabelEncoder","b6dbae09":"le = LabelEncoder()\n\nen_stroke_data = stroke_data.apply(le.fit_transform)  ## en_ here is simply to remind that this data is encoded and will be used mostly now on","5fc19442":"en_stroke_data.head()","0a336440":"# Dependent(Response) variable Y and Independent(Predictor) variable X.\nX = en_stroke_data.iloc[:, : -1]\ny = en_stroke_data.iloc[:, -1]","ce21ea96":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 30)","8bc272f0":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbag_clf = BaggingClassifier(\n    DecisionTreeClassifier(),\n    n_estimators = 500,       ## Total Number of decision tree that will be used to train an ensemble is 2\n    max_samples = 100,         ## each trained on 100 training instances randomly sampled from the training set with replacement\n    bootstrap = True,         ## Bootstrap = True means use bagging method, if this option is set to False then it will be Pasting method that we didn't mention here.\n    n_jobs = -1               ## n_jobs means how many cores will be used to train the ensemble and -1 here means all of them\n)\n\nbag_clf.fit(x_train, y_train)","1e92bc17":"# Making predictions\ny_pred_bagging = bag_clf.predict(x_test)","59cf82eb":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_pred_bagging)","a9a90473":"(1468\/len(y_test))*100","b9f76a72":"#  Building a randomForest model\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest_clf = RandomForestClassifier(\n    n_estimators=350,         ## Training the ensemble model using 350 decision trees, we can use any number there depends on the speed of our machine\n    max_leaf_nodes = 15,      ## Each tree will have a maximum number of 10 leaf nodes\n    n_jobs = -1,\n)\n\nrandom_forest_clf.fit(x_train, y_train)","13119baf":"y_pred_rf = random_forest_clf.predict(x_test)","b7d33d57":"from sklearn.metrics import accuracy_score\n\nacc = (accuracy_score(y_test, y_pred_rf))*100\nprint(f\"{round(acc, 2)}% of Accuracy\")","2d526961":"# Building our Adaboost ensemble model\nfrom sklearn.ensemble import AdaBoostClassifier\n\nadaboost_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(),\n    n_estimators = 400,\n    learning_rate = 0.6   \n)\n\nadaboost_clf.fit(x_train, y_train)","5922bc2d":"adaboost_pred = adaboost_clf.predict(x_test)\n\nconfusion_matrix(y_test, adaboost_pred)","0ca60f2e":"acc_boost = (accuracy_score(y_test, adaboost_pred))*100\nprint(f\"{round(acc_boost, 2)}% Accuracy achieved\")","93bd7930":"from sklearn.ensemble import GradientBoostingClassifier\n\ngradient_clf = GradientBoostingClassifier(\n    n_estimators=2000,\n    learning_rate=0.5\n)\n\ngradient_clf.fit(x_train, y_train)","c85e0fd5":"grad_pred = gradient_clf.predict(x_test)","c493e6a2":"confusion_matrix(y_test, grad_pred)","e14a3bdb":"grad_acc = (accuracy_score(y_test, grad_pred))*100\nprint(f\"{round(grad_acc, 2)}% Accuracy\")","806cdab3":"## Categorical Data\n\n### For the categorical data we can use dummy variable or i can use labelEncoder but I prefer using labelEncoding as it will be easy to decode a particular label back later after predicting if needed.","fd3eb836":"Total number of outliers are 627 out of almost 5000 records so if we remove we might lost a lots of information so lets choose another method for handling outliers. I will do capping(Replacing the larger outliers with uppers range and smaller outliers with lower range)","ccf79a64":"#### So far we have achieved 95.7% of accuracy in this.","3374b17e":"## Gradient Boost:\nFinally our last(in this notebook) algorithm is gradient boost that we will use here. Gradient Boost is a popular boosting algorithm, just like AdaBoost Gradient Boost works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every\niteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.","58db176d":"## Column BMI has 201 nan values\n\n#### We have multiple ways to handle nan values either we can remove or we can perform imputation such as filling nan values with mean or the median value. In this case i am going to fill nan values with mean.","d4f3b3f7":"Just a little description about why I use inplace=True there. If inplace=True is not given then the function will return a back the dataframe that we need to store again. Default value for inplace is false.\n\ndf = stroke_data.drop(columns={'id'}, inplace=False) \n\n>will return a dataframe wihout the column id and put it in the df.\n\nwhere as,\n\nstroke_data.drop(columns={'id'}, inplace=True)\n\n>will remove the id and put data back to the stroke_data.","c3ab28f5":"### End note:\nEnsemble techniques can boost the performance of the model and give a better result. The ensemble techniques that we have used here boosted also by passing more parameters. We used Classifiers of each of these algorithms they also have a regression version and those work with the numerical data. Those can be used from the same sklearn library, simply import GradientBoostingRegressor, AdaBoostRegressor, etc. We have lot more Ensemble techniques but these are some of the main ones, we can explore further more ensemble techniques.","1a20de43":"### Code for Bagging:\n","0f6d11ac":"### Accuracy test of our model using confusion matrix","0b1f6f33":"Finally we have seen that AdaBoost is classifying all the data into both of the categories. The algorithms used prior were giving definitely a better accuracy but they were not being able to classify both the categories.","c8f1da35":"### Now we have handled the outliers. Lets handle categorical data.","2d368fc3":"### Visualize Data","69cd960d":"### Machine Learning algorithms.\n\nAs we say in the beginning about ensemble technique, it is time to apply one by one all of them and see which one gives better result.","1057a7c0":"### Train Test Split:\nLets split our data into train and test sets. As we have almost 5000 records we will use 70-30 split.","ae63003e":"## Boosting:\nBoosting (originally called hypothesis boosting) refers to any Ensemble method that can combine several weak learners into a strong learner. The idea of boosting is to train predictors sequentially and each of them is trying to correct predecessor. The Boosting mehtods that we will use are going to be AdaBoost and Gradient Boost","3a913d87":"## RandomForest\nThe second algorithms that we will use is going to be random forest. Here Forest means we will have n number of trees. The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, generally yielding an overall better model.","3e3f8da1":"# Ensemble Techniques\n\n> All the machine learning algorithms that we have seen have their advantages and disadvantages and there are certain situations where these algorithms are going to give us better result and sometimes they won't. So ensemble learning uses the group of predictors in order to increase accuracy, reduce bias, etc. In ensemble learning combiles the predictions of several base estimators build with a given learning algorithm in order to increase accuracy.\n\n## The Ensemble techniques that we are going to use here are:\n> Bagging\n\n> RandomForest\n\n> Boosting (AdaBoost and Gradient Boost)","0534cc83":"#### avg_glucose has more higest number of outliers. We can check out the outliers also in python using IQR Method","e242c89f":"#### Seems like we got the same accuracy here.","36ce8d92":"### AdaBoost:\nEverytime we get errors by focusing on those wrongly predicted can help to increase the accuracy. That is how Adaboost works, the first base classifier is trained and predictions are made from training set. The relative weight of all misclassified training instances is increased. A second classifier is then trained using the updated weights and again it makes predictions and again the weights are increased for misclassified instances and so on. It continues until we get the best accuracy.\n\nEach instances will get boosted weights for the misclassified(by predecessor) records and improve accordingly. ","a5b7706a":"### Before going with the the machine learning algorithms, lets perform some EDA (Exploratory Data Analysis)","b1a1aa3f":"Heart patients visualization using countplot shows that there are few number of heart patients.","a0737f16":"## Bagging.\nThe first algorithm that we will use is bagging. Bagging is a short name for the bootstrap aggregation is a machine learning ensemble meta-algorithm designed to improve the accuracy and stability of machine learning algorithms. Bootstrap is a sampling technique where out of n samples avaible k samples are choosen with replacement. We then run our algorithm(i.e: Decision Tree Classifier) on each of these samples. The point is to make sampling truly random. Aggregation here means the predictions of all the models is combined to make final predictions."}}