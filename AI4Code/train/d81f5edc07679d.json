{"cell_type":{"cbc4672f":"code","627d80c1":"code","6935dd55":"code","b8ef9ed9":"code","1db714f3":"code","651c0d6c":"code","2385b9de":"code","bb5dec00":"code","9ad1419b":"code","d5e263be":"code","53ca8c13":"code","cc785e6f":"code","1b962833":"code","269d67a1":"code","b2c96a87":"markdown","8056992e":"markdown","494a686e":"markdown","2d1e71b1":"markdown","558815c6":"markdown","8628d962":"markdown","64155315":"markdown","4dc3dda2":"markdown","0e27d8c9":"markdown","b3cfeeae":"markdown","47dfbbac":"markdown","c03e4d09":"markdown"},"source":{"cbc4672f":"# Import some necessary libraries\nfrom IPython.display import display, Markdown, Latex\nimport json\nimport pandas as pd\nimport re","627d80c1":"raw_data_file = \"\/kaggle\/input\/netherlands-rent-properties\/properties.json\"\n\ndef load_raw_data(filepath):\n    raw_data = []\n    for line in open(filepath, 'r'):\n        raw_data.append(json.loads(line))\n    \n    return raw_data\n    \nraw_data = load_raw_data(raw_data_file)\n\nMarkdown(f\"\"\"\nSuccessfully imported {len(raw_data)} properties from the dataset.\n\"\"\")","6935dd55":"df = pd.DataFrame(raw_data)\n\nMarkdown(f\"\"\"\nSuccessfully created DataFrame with shape: {df.shape}.\n\"\"\")","b8ef9ed9":"df.info()","1db714f3":"# Define all columns that need to be flatten and the property to extract\nflatten_mapper = {\n    \"_id\": \"$oid\",\n    \"crawledAt\": \"$date\",\n    \"firstSeenAt\": \"$date\",\n    \"lastSeenAt\": \"$date\",\n    \"detailsCrawledAt\": \"$date\",\n}\n\n# Function to do all the work of flattening the columns using the mapper\ndef flatten_columns(df, mapper):\n    \n    # Iterate all columns from the mapper\n    for column in flatten_mapper:\n        prop = flatten_mapper[column]\n        raw_column_name = f\"{column}_raw\"\n        \n        # Check if the raw column is already there\n        if raw_column_name in df.columns:\n            # Drop the generated one\n            df.drop(columns=[column], inplace=True)\n            \n            # Rename the raw back to the original\n            df.rename(columns={ raw_column_name: column }, inplace=True)        \n    \n        # To avoid conflicts if re-run, we will rename the columns we will change\n        df.rename(columns={\n            column: raw_column_name,\n        }, inplace=True)\n\n        # Get the value inside the dictionary\n        df[column] = df[raw_column_name].apply(lambda obj: obj[prop])\n        \n    return df\n        ","651c0d6c":"df = df.pipe(flatten_columns, mapper=flatten_mapper)","2385b9de":"def rename_columns(df):\n    # Store a dictionary to be able to rename later\n    rename_mapper = {}\n    \n    # snake_case REGEX pattern\n    pattern = re.compile(r'(?<!^)(?=[A-Z])')\n    \n    # Iterate the DF's columns\n    for column in df.columns:\n        rename_mapper[column] = pattern.sub('_', column).lower()\n        \n    # Rename the columns using the mapper\n    df.rename(columns=rename_mapper, inplace=True)\n    \n    return df","bb5dec00":"df = df.pipe(rename_columns)","9ad1419b":"def parse_types(df):\n    \n    df[\"crawled_at\"] = pd.to_datetime(df[\"crawled_at\"])\n    df[\"first_seen_at\"] = pd.to_datetime(df[\"first_seen_at\"])\n    df[\"last_seen_at\"] = pd.to_datetime(df[\"last_seen_at\"])\n    df[\"details_crawled_at\"] = pd.to_datetime(df[\"details_crawled_at\"])\n    df[\"latitude\"] = pd.to_numeric(df[\"latitude\"])\n    df[\"longitude\"] = pd.to_numeric(df[\"longitude\"])\n    \n    return df","d5e263be":"df = df.pipe(parse_types)","53ca8c13":"raw_data = load_raw_data(raw_data_file)\ndf = pd.DataFrame(raw_data)\ndf = (df\n      .pipe(flatten_columns, mapper=flatten_mapper)\n      .pipe(rename_columns)\n      .pipe(parse_types)\n     )","cc785e6f":"# Flatten column with list of objects\ndef flatten_col_list(lst):\n    return list(map(lambda obj: obj[\"$date\"], lst))\n\n# Transform the DF into a time series\ndef to_timeseries(df, dates_column=\"dates_published\"):\n    # Get a list of columns without the target column\n    columns = df.columns.values.tolist()\n    columns.remove(dates_column)\n    \n    # Create a DF with all the dates\n    dates_df = pd.DataFrame(df[dates_column].apply(flatten_col_list).tolist())\n    \n    # Create a wide representation of our DF\n    wide = pd.concat([df, dates_df], axis=1).drop(dates_column, axis=1)\n    \n    # Melt the dataframe\n    ts = pd.melt(wide, id_vars=columns, value_name='date')\n    \n    # [WARNING] Drop columns with missing date\n    ts.dropna(inplace=True, subset=[\"date\"])\n    \n    # Parse the date column\n    ts[\"date\"] = pd.to_datetime(ts[\"date\"])\n    \n    # Offset the date column to account for timezone differences\n    ts[\"date\"] = ts[\"date\"] + pd.DateOffset(hours=3)\n    \n    return ts","1b962833":"ts = df[:100].pipe(to_timeseries)","269d67a1":"# Get a random property to show the time series\ntarget_external_id = ts[\"external_id\"].sample().iloc[0]\nts[ts[\"external_id\"] == target_external_id][[\"date\", \"external_id\", \"city\"]].head(10)","b2c96a87":"## Introduction\n\nThis is an example notebook for exploring the [Netherlands Rent Properties](https:\/\/www.kaggle.com\/juangesino\/netherlands-rent-properties) dataset. The goal is to show how this dataset can be further used for analysis.","8056992e":"## Import & Read Data\n\nThe data is in a JSON file. To start with, we will import the file as a python dictionary.","494a686e":"## Handle Types\n\nNow we can start parsing the appropiate data types for our columns","2d1e71b1":"## Flatten Columns\n\nBecause the source of these data was a JSON file (coming from MongoDB), there are a few columns that have some nested JSONs and\/or lists.\n\nIn general, how to handle these will depend on the analysis that will be performed, but there are a few that are useful to handle from the start. For example, the `_id` and some date columns (`crawledAt`, `firstSeenAt`, `lastSeenAt`, `detailsCrawledAt`) are represented as JSON objects because they include the MongoDB type (for compatibility reasons). We can get rid of those and flatten the columns.","558815c6":"## Next Steps\n\nHere are a few suggestions of next steps to clean this data further:\n\n* Use the `latitude` and `longitude` columns to generate geomery points (for example using [GeoPandas](https:\/\/geopandas.readthedocs.io\/en\/latest\/gallery\/create_geopandas_from_pandas.html))\n* Parse the `posted_ago` column to determine how long ago the property was published (this might not be needed considering that the column `dates_published` has the entire daily history)\n* Perform NLP on the details and descriptions\n* Handle categorical variables better (`internet`, `pets`, `kitchen`, etc)\n* Parse match values (`matchAge`, `matchCapacity`) into numeric values\n* Combine with public datasets to get more features (distance to POI, distance to public transport, neighbourhood data, finacial data)\n\n**Note**:\n\nI tried as much as I could to use re-usable functions to wrangle the data to make it easier to reproduce. Simply import my functions and pipe them to the data:","8628d962":"## Research Ideas\n\nFinally, here are some ideas on how to use this dataset, some are more obvious than others:\n\n* Can we predict the rental price of a property in The Netherlands?\n* Is there a real-estate bubble in Amsterdam (or any other city)?\n* What factors determine the price of a property?\n* Can we detect high profitability opportunities for rental businesses?\n* Can we find any advice\/insights for people who are looking for accomodation?","64155315":"This makes it super easy to extend the functions or add more pipelines.","4dc3dda2":"## Rename Columns\n\nThis step is entirely optional, but I prefer all columns to have a more \"pythonic\" name. The current column names come from JavaScript's camelCase conventions, we'll create a function to rename all columns into snake_case.","0e27d8c9":"Feel free to parse other columns or modify how we parse these.\n\nSuggestions:\n* `user_last_logged_on` can also be parsed as date\n* `user_member_since` can also be parsed as date\n* `roommates` can also be parsed as numeric, but we need to handle some text values","b3cfeeae":"## Bonus: Time Series\n\nAs a bonus, this is one approach to handle these data as a time series.\n\n**Warning**: This blows up the dataset. It can take a while to finish, and RAM might be an issue. In this example I only performm it with a subset of the data (100 properties).","47dfbbac":"Now we can create a Pandas' DataFrame from this list","c03e4d09":"Note that we haven't dealt with the `datesPublished` column. At the moment this column contains a list of all the days the property has been published on the website. We can later use this to generate a time series."}}