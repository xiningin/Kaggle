{"cell_type":{"80a75a64":"code","4722e499":"code","29b72624":"code","89aa9bb6":"code","2d063729":"code","ce33ac34":"code","8cfcda7b":"code","2141201b":"code","580d07e6":"code","17a9d3cf":"code","933c5e2f":"code","1dd080c8":"code","49245256":"code","c1573872":"code","f4370320":"code","9ca0027b":"code","016134c6":"code","2a2057fd":"code","fb6d82f1":"code","7334c6b8":"markdown","a24037b2":"markdown","fef0526e":"markdown","3ae2de43":"markdown","291991a2":"markdown","fdb5f7c3":"markdown","4a384a4a":"markdown","bf1037bf":"markdown","da7a3c09":"markdown","58cecf4d":"markdown","d2104498":"markdown","06507a8e":"markdown","22e46c07":"markdown","3a1dc31c":"markdown","3c9763a8":"markdown","5ca39ad7":"markdown","4f19df16":"markdown"},"source":{"80a75a64":"import numpy as np \nimport pandas as pd \nimport spacy\n\nimport plotly.graph_objs as go\n# these two lines allow your code to show up in a notebook\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode()\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport collections\n\ndata=pd.read_csv(\"..\/input\/neuroscience_articles.csv\")\ndata['citation_online_date']=pd.to_datetime(data['citation_online_date'],format=\"%Y\/%M\/%d\")\ndata['citation_year']=data['citation_online_date'].dt.year\n\n# Replace empty abstract (' ') with Na\ndata['citation_abstract']=data['citation_abstract'].fillna(' ')\ndata=data.drop(['citation_doi', 'citation_pdf_url', 'dc.identifier', 'citation_author_email', 'citation_language', 'citation_publisher'], 1)\ndata=data.dropna()","4722e499":"# Find the time range of the articles\ntime = data['citation_online_date'].sort_values()\narticles = data.shape[0]\n\n\"Number of articles: {0} Newest: {1} Oldest: {2}\".format(articles, time.iloc[-1], time.iloc[0])","29b72624":"data['citation_country'] = data['citation_author_institution'].apply(lambda x: x.split(',')[-1].strip())\ndata['citation_country'] = data['citation_country'].replace({'USA': 'United States', 'UK': 'United Kingdom'})\n\n# Number of neurosceince papers published in each country\njournal_type_freq = data['citation_country'].value_counts().head(10).plot.bar()\n","89aa9bb6":"# Get a list of top 11 countries\ncountries = data['citation_country'].value_counts().head(11)\n\n# Get research category and country counts. Reset index to have a research category for every row.\ncountries_journal_types = data.groupby([\"citation_journal_title\", 'citation_country']).size().reset_index()\n\n# Get the rows containing the top n countries\ncountries_journal_types = countries_journal_types.loc[countries_journal_types['citation_country'].isin(countries.index)].reset_index(drop=True)\ncountries_journal_types.set_axis(['citation_journal_title', 'citation_country', 'count'], axis='columns', inplace=True)\n\ncountries_journal_types = countries_journal_types.pivot(index='citation_journal_title', columns='citation_country', values='count').fillna(0)\n\ntrace = go.Heatmap(z=countries_journal_types.values.transpose(),\n                   y=list(countries_journal_types.columns.values),\n                   x=list(countries_journal_types.index), colorscale='Viridis')\n\nlayout = go.Layout(\n    margin = dict(t=25,r=20,b=170,l=115),\n)\n\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig, filename='datetime-heatmap')","2d063729":"import sklearn.feature_extraction.text as text\ndef get_imp(bow,mf,ngram):\n    tfidf=text.CountVectorizer(bow,ngram_range=(ngram,ngram),max_features=mf,stop_words='english')\n    matrix=tfidf.fit_transform(bow)\n    return pd.Series(np.array(matrix.sum(axis=0))[0],index=tfidf.get_feature_names()).sort_values(ascending=False).head(150)\n\n# Global trends\nbow=data['citation_abstract'].tolist()\ntotal_data=get_imp(bow,mf=5000,ngram=1)\ntotal_data_bigram=get_imp(bow=bow,mf=5000,ngram=2)\ntotal_data_trigram=get_imp(bow=bow,mf=5000,ngram=3)\n\n# Yearly trends\nyearly_unigram_dict ={}\nfor y in data['citation_year'].unique():\n    bow=data[data['citation_year']==y]['citation_abstract'].tolist()\n    yearly_unigram_dict[y]=get_imp(bow,mf=5000,ngram=1)\n    \nyearly_bigram_dict={}\nfor y in data['citation_year'].unique():\n    bow=data[data['citation_year']==y]['citation_abstract'].tolist()\n    yearly_bigram_dict[y]=get_imp(bow,mf=5000,ngram=2)\n    \nyearly_trigram_dict={}\nfor y in data['citation_year'].unique():\n    bow=data[data['citation_year']==y]['citation_abstract'].tolist()\n    yearly_trigram_dict[y]=get_imp(bow,mf=5000,ngram=3)","ce33ac34":"import matplotlib.pyplot as plt\nplt.subplot(1,2,1)\ntotal_data_bigram.head(30).plot(kind=\"bar\",figsize=(25,10),colormap='Set2')\nplt.title(\"Bigrams\",fontsize=30)\nplt.yticks([])\nplt.xticks(size=15)\nplt.subplot(1,2,2)\ntotal_data_trigram.head(30).plot(kind=\"bar\",figsize=(25,10),colormap='Set2')\nplt.title(\"Trigrams\",fontsize=30)\nplt.yticks([])\nplt.xticks(size=15)","8cfcda7b":"for i in range(10,19,1):\n    plt.subplot(9,2,i)\n    yearly_bigram_dict[2000+i].head(10).plot(kind=\"barh\",figsize=(20,25),colormap='Set1')\n    plt.title(2000+i,fontsize=20)\n    plt.xticks([])\n    plt.yticks(size=10,rotation=5)","2141201b":"keywords_analysis = data.copy()\nkeywords_analysis['citation_keywords'] = keywords_analysis['citation_keywords'].apply(lambda x: [a.strip() for a in x.lower().split(';')])\nall_keywords = (list([a for b in keywords_analysis.citation_keywords.tolist() for a in b]))\n\ncounter = collections.Counter(all_keywords)\nall_keywords = sorted( all_keywords, key=counter.get, reverse=True )\n\n[[a, counter.get(a)] for a in list(collections.OrderedDict.fromkeys(all_keywords))][:15]","580d07e6":"keywords = ['alzheimer\u2019s disease', 'parkinson\u2019s disease', 'schizophrenia', 'depression', 'epilepsy']\n\n# Create a list of keyword, and a pandas series of all the rows that mention that keyword\nindeces = [[keyword, data['citation_abstract'].str.contains(keyword,case=False)] for keyword in keywords]\n\nfor keyword in indeces:\n    print(\"{0} has been mentioned in {1} abstracts\".format(keyword[0], np.sum(keyword[1])))","17a9d3cf":"counts = []\nfor keyword in indeces:\n    counts.append(pd.DataFrame(data['citation_year'].loc[keyword[1]].value_counts()))\n    \nn_r=pd.concat(counts,axis=1)\nn_r.columns=keywords\nn_r.plot(figsize=(10,10), kind='line', colormap= 'Set1', grid=True)\nplt.title(\"Mentions of Mental Health Keywords in Citation Abstracts over time\",fontsize=20)\n","933c5e2f":"alz_index= data['citation_abstract'].str.contains(\"alzheimer's\",case=False)\ndata_alz=data.loc[alz_index].copy()\ntext=\" \".join(data_alz['citation_abstract'].tolist())\n\nfrom wordcloud import WordCloud,STOPWORDS\nwc = WordCloud(max_words=500,width=5000,height=2500,background_color=\"white\",stopwords=STOPWORDS).generate(text)\nplt.figure( figsize=(30,15) )\nplt.imshow(wc)\nplt.yticks([])\nplt.xticks([])\nplt.axis(\"off\")","1dd080c8":"alz_index= data['citation_abstract'].str.contains(\"schizophrenia\",case=False)\ndata_alz=data.loc[alz_index].copy()\ntext=\" \".join(data_alz['citation_abstract'].tolist())\n\nfrom wordcloud import WordCloud,STOPWORDS\nwc = WordCloud(max_words=500,width=5000,height=2500,background_color=\"white\",stopwords=STOPWORDS).generate(text)\nplt.figure( figsize=(30,15) )\nplt.imshow(wc)\nplt.yticks([])\nplt.xticks([])\nplt.axis(\"off\")","49245256":"word_counts_years = {}\n\nfor i in range(9):\n    year = 2010 + i\n    series = yearly_trigram_dict[year]\n    \n    # Create data frame from pandas Series\n    word_counts_years[year] = pd.DataFrame({'Word':series.index, 'Count':series, 'Year': year}).reset_index(drop=True)\n\n# Stack the frames on top of each other\ndf_final = pd.concat([df for df in word_counts_years.values()], ignore_index = True)\n\n# Get the Rows containing word that are in the top 20 most common words\ndf_final = df_final.loc[df_final['Word'].isin(total_data_trigram.head(10).index)]\n\n# Sort common words together, year acesding\ndf_final = df_final.sort_values(['Word', 'Year'])\n\n# Set indeces and calculate percent change in count, https:\/\/stackoverflow.com\/questions\/41453325\/python-pandas-groupby-calculate-change\ndf_final = df_final.set_index(['Year', 'Word']).Count\ndf_final = df_final.groupby('Word').pct_change()\n\ndf_final.fillna(0).add(1).unstack().plot(figsize=(15,15), kind='bar', colormap= 'Spectral', grid=True)\nplt.title(\"Percentage growth of trigram mentions in articles\",fontsize=20)\nplt.ylabel(\"Percent change\")","c1573872":"import re\n\nkeywords = ['bci', 'brain computer', 'neocortex', 'EEG', 'electroencephalography', 'brain-machine']\n\n# Create a list of keyword, and a pandas series of all the rows that mention that keyword\nindeces = [[keyword, data['citation_abstract'].str.contains(keyword,case=False)] for keyword in keywords]\n\nfor keyword in indeces:\n    print(\"{0} has been mentioned in {1} citation abstracts\".format(keyword[0], np.sum(keyword[1])))","f4370320":"import re\n\nkey = ['bci', 'brain computer interface','brain-machine interface']\npattern = '|'.join(map(re.escape, key))\n\nbci_index= data['citation_abstract'].str.contains(pattern,case=False)\ndata=data.loc[bci_index].copy()\ntext=\" \".join(data['citation_abstract'].tolist())\n\nfrom wordcloud import WordCloud,STOPWORDS\nwc = WordCloud(max_words=500,width=5000,height=2500,background_color=\"white\",stopwords=STOPWORDS).generate(text)\nplt.figure( figsize=(30,15) )\nplt.imshow(wc)\nplt.yticks([])\nplt.xticks([])\nplt.axis(\"off\")","9ca0027b":"import sklearn.feature_extraction.text as text\ndef get_imp(bow,mf,ngram):\n    tfidf=text.CountVectorizer(bow,ngram_range=(ngram,ngram),max_features=mf,stop_words='english')\n    matrix=tfidf.fit_transform(bow)\n    return pd.Series(np.array(matrix.sum(axis=0))[0],index=tfidf.get_feature_names()).sort_values(ascending=False).head(150)\n\n# Global trends\nbow=data['citation_abstract'].tolist()\ntotal_data=get_imp(bow,mf=5000,ngram=1)\ntotal_data_bigram=get_imp(bow=bow,mf=5000,ngram=2)\ntotal_data_trigram=get_imp(bow=bow,mf=5000,ngram=3)\n\n# Yearly trends\nyearly_unigram_dict ={}\nfor y in data['citation_year'].unique():\n    bow=data[data['citation_year']==y]['citation_abstract'].tolist()\n    yearly_unigram_dict[y]=get_imp(bow,mf=5000,ngram=1)\n    \nyearly_bigram_dict={}\nfor y in data['citation_year'].unique():\n    bow=data[data['citation_year']==y]['citation_abstract'].tolist()\n    yearly_bigram_dict[y]=get_imp(bow,mf=5000,ngram=2)\n    \nyearly_trigram_dict={}\nfor y in data['citation_year'].unique():\n    bow=data[data['citation_year']==y]['citation_abstract'].tolist()\n    yearly_trigram_dict[y]=get_imp(bow,mf=5000,ngram=3)","016134c6":"import matplotlib.pyplot as plt\n\nplt.subplot(1,2,1)\ntotal_data_bigram.head(30).plot(kind=\"bar\",figsize=(25,10),colormap='Set2')\nplt.title(\"Bigrams\",fontsize=30)\nplt.yticks([])\nplt.xticks(size=15)\nplt.subplot(1,2,2)\ntotal_data_trigram.head(30).plot(kind=\"bar\",figsize=(25,10),colormap='Set2')\nplt.title(\"Trigrams\",fontsize=30)\nplt.yticks([])\nplt.xticks(size=15)","2a2057fd":"for i in range(10,19,1):\n    plt.subplot(9,2,i)\n    yearly_bigram_dict[2000+i][3:14].plot(kind=\"barh\",figsize=(20,25),colormap='Set1')\n    plt.title(2000+i,fontsize=20)\n    plt.xticks([])\n    plt.yticks(size=10,rotation=5)","fb6d82f1":"word_counts_years = {}\n\nfor i in range(9):\n    year = 2010 + i\n    series = yearly_bigram_dict[year]\n    \n    # Create data frame from pandas Series\n    word_counts_years[year] = pd.DataFrame({'Word':series.index, 'Count':series, 'Year': year}).reset_index(drop=True)\n    \n# Stack the frames on top of each other\ndf_final = pd.concat([df for df in word_counts_years.values()], ignore_index = True)\n\n# Get the Rows containing word that are in the top 20 most common words\ndf_final = df_final.loc[df_final['Word'].isin(total_data_bigram[:12].index)]\n\n# Pivot Dataframe so words become columns, and can set a word not present in a year to 0\ndf_final = df_final.pivot_table('Count', ['Year'], 'Word').fillna(0)\ndf_final = df_final.pct_change()\n\n# # Stack again\ndf_final = df_final.stack().rename_axis((['Year', 'Word'])).reset_index(name='Change')\n\n# Set indeces and calculate percent change in count\ndf_final = df_final.set_index(['Year', 'Word']).Change\n\ndf_final.fillna(0).add(1).unstack().plot(figsize=(15,15), kind='bar', colormap= 'Spectral', grid=True)\nplt.title(\"Percentage growth of Bigram mentions in citation abstracts over time\",fontsize=20)\nplt.ylabel(\"Percent change\")","7334c6b8":"Plot of most common bigrams over the years. Can see shifts in research focus, for exmaple basal ganglia to prefrontal cortex.","a24037b2":"Percent change in BCI specific article trigram mentions over time.","fef0526e":"Analyze mentions of mental health keywords in citation abstracts over time.","3ae2de43":"Number of articles published in each country.","291991a2":"# Overview","fdb5f7c3":"Trigrams over the years for BCI specific articles.","4a384a4a":"Remove uneccesary columns and delete any rows with missing data. 8% of articles are missing an abstract, these article rows are dropped.","bf1037bf":"Unigram, bigram, trigram analysis for BCI specific articles.","da7a3c09":"Heatmap of journals in the Frontiers in Neuroscience series topis, countries, and number of articles.","58cecf4d":"Word cloud of BCI terms. Searching multiple strings [Pandas regex string match](https:\/\/stackoverflow.com\/questions\/22623977\/searching-multiple-strings-in-pandas-without-predefining-number-of-strings-to-us).\nKeep only BCI spefific journals, drop the rest","d2104498":"Only keep articles that have citation absracts that contain specific keywords","06507a8e":"# Persistent Themes\nTo get an idea of what are some of the common research topics, I followed a general approach of doing frequency counts, of unigrams, bigrams and trigrams for all of the articles as well as for each year. Can see that BCI is one of the popular ones. Will investegate the popularity of BCI's later.","22e46c07":"Find most common citation keywords.","3a1dc31c":"Plot the percentage change of top 10 most common trigrams over the years. Can see which years there was a spike in a specific research theme.","3c9763a8":"Word clouds for citation abstracts containing mental health keywords.","5ca39ad7":"Bigrams and trigrams for all of the articles in the Frontiers in Neuroscience journal series.","4f19df16":"# BCI Trends"}}