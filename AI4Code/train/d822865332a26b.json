{"cell_type":{"e78e66bd":"code","716f1379":"code","d55c6bb2":"code","517fb213":"code","92256680":"code","bbcf4981":"code","272df921":"code","29ad30a2":"code","b3c654df":"code","37aff18f":"code","593778bd":"code","7b11e047":"code","75eaffbe":"code","eb1d6686":"code","f7f7ba5d":"code","ddd9dd2f":"code","9f64f95f":"code","54fcd5bf":"code","fdcf75bf":"code","6c42461c":"code","13e4a87c":"code","e9e372eb":"code","6bfe67fc":"code","8c84d3bb":"code","486a1687":"code","1c56463e":"code","f3f4271d":"code","dccc1b7e":"code","69b7ccd2":"code","6b5a7e7a":"code","149e8f51":"code","9ea1a2fb":"code","e1af6cff":"code","0a642a93":"code","3a35ef64":"code","21b74276":"code","2605f021":"code","946d2e0f":"code","945b0dfc":"code","997f8bc3":"code","b0233c35":"code","e98064aa":"code","ae33d35a":"code","3aff0c2c":"code","e5251425":"code","7f84ac1d":"code","19d0590b":"code","39c6ad71":"code","6b6d4f34":"markdown","143a4fe9":"markdown","7b80b9e1":"markdown","8032caf0":"markdown","1b17d6fb":"markdown","2b1b2588":"markdown","34f3ace5":"markdown","2ebc195e":"markdown","9d917316":"markdown","104348b7":"markdown","79704559":"markdown","344062a0":"markdown","aa3b8742":"markdown","60a31fe8":"markdown","3a093d89":"markdown","daf2a372":"markdown","0dd0de1a":"markdown","be344c91":"markdown","addae3dc":"markdown"},"source":{"e78e66bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# print( os.walk('\/kaggle\/input'))\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     print(dirname, _, filenames)\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","716f1379":"train=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\");\ntest=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\");","d55c6bb2":"train_test_data=[train,test]","517fb213":"train.shape,test.shape","92256680":"train.head(80)","bbcf4981":"train['SalePrice'].describe()","272df921":"#histogram\nsns.distplot(train['SalePrice']);","29ad30a2":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","b3c654df":"#scatter plot grlivarea\/saleprice  train['GrLivArea']\nvar = 'GrLivArea'\ndata = pd.concat([ train['SalePrice'],  train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","37aff18f":"#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([ train['SalePrice'],  train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","593778bd":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([train['SalePrice'],  train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","7b11e047":"var = 'YearBuilt'\ndata = pd.concat([ train['SalePrice'],  train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","75eaffbe":"#correlation matrix\ncorrmat =  train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","eb1d6686":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef( train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","f7f7ba5d":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot( train[cols], size = 2.5)\nplt.show();","ddd9dd2f":"train.isnull().sum().loc[lambda x : x>0].sort_values(ascending=False)\n \n# train.isnull().sum()","9f64f95f":"def analysis(feature):\n    print(\"*********** group by value ,count ***************\")\n    print(feature.value_counts())\n    print(\"*********** total null value count ***************\")\n    print(feature.isnull().value_counts())\n    \n    ","54fcd5bf":"\ncol_obj = train.select_dtypes([np.object]).columns.values.tolist()\n# for feature in [\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\",\"FireplaceQu\",\"GarageType\",\"GarageFinish\",\"GarageQual\"]:\nfor feature in col_obj:\n    for dataset in train_test_data:\n        dataset[feature].fillna(\"NA\", inplace=True)","fdcf75bf":"# LotFrontage    259\n# GarageYrBlt     81\n# MasVnrArea  \ntest.info()\n# col_int_float = train.select_dtypes([np.float64,np.int64,np.uint8]).columns.values.tolist()\ncol_int_float = train.select_dtypes([np.float64,np.int64,np.uint8]).columns.values.tolist()\ncol_int_float.remove(\"SalePrice\")\nfor feature in col_int_float:\n    for dataset in train_test_data:\n        dataset[feature].fillna(0, inplace=True)","6c42461c":"# # analysis(train[\"SalePrice\"])\n# # analysis(test[\"PoolQC\"])\n# for dataset in train_test_data:\n#     dataset[\"PoolQC\"].fillna(\"NA\", inplace=True)\n \n# analysis(train[\"GarageFinish\"])\n# analysis(test[\"PoolQC\"])\n \n\n\n# # use pd.concat to join the new columns with your original dataframe\n# train = pd.concat([train,pd.get_dummies(train ,columns=['PoolQC' ], prefix=['PoolQC' ])],axis=1)\n# test = pd.concat([test,pd.get_dummies(train ,columns=['PoolQC' ], prefix=['PoolQC' ])],axis=1)\n# # now drop the original 'country' column (you don't need it anymore)\n# train.drop(['PoolQC' ],axis=1, inplace=True)\n# test.drop(['PoolQC' ],axis=1, inplace=True)\n","13e4a87c":"# fill missing LotFrontage with median LotFrontage for each MSZoning (RL,RM ,FV  ,RH  ,C (all) )\n# train[\"LotFrontage\"].fillna(train.groupby(\"MSZoning\")[\"LotFrontage\"].transform(\"median\"), inplace=True)","e9e372eb":"import pandas as pd\n# train = pd.DataFrame(data = [['a', 123, 'ab'], ['b', 234, 'bc']],\n#                      columns=['col1', 'col2', 'col3'])\n# test = pd.DataFrame(data = [['c', 345, 'ab'], ['b', 456, 'ab']],\n#                      columns=['col1', 'col2', 'col3'])\n# train_objs_num = len(train)\n# dataset = pd.concat(objs=[train, test], axis=0,sort=False)\n# dataset_preprocessed = pd.get_dummies(dataset)\n# train_preprocessed = dataset_preprocessed[:train_objs_num]\n# test_preprocessed = dataset_preprocessed[train_objs_num:]","6bfe67fc":"# # Categorical boolean mask\n# categorical_feature_mask = train.dtypes==object\n# # filter categorical columns using mask and turn it into a list\n# categorical_cols = train.columns[categorical_feature_mask].tolist()\n\n# # import labelencoder\n# from sklearn.preprocessing import LabelEncoder\n# # instantiate labelencoder object\n# le = LabelEncoder()\n\n# # apply le on categorical feature columns\n# train[categorical_cols] = train[categorical_cols].apply(lambda col: le.fit_transform(col))\n# train[categorical_cols].head(10)\n\n# # import OneHotEncoder\n# from sklearn.preprocessing import OneHotEncoder\n# # instantiate OneHotEncoder\n# ohe = OneHotEncoder(categorical_features = categorical_feature_mask, sparse=False ) \n\n# # categorical_features = boolean mask for categorical columns\n# # sparse = False output an array not sparse matrix\n\n# # apply OneHotEncoder on categorical feature columns\n# X_ohe = ohe.fit_transform(train) # It returns an numpy array\n","8c84d3bb":"# Encoding categorical data\n# Encoding the Independent Variable\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n# labelencoder_X = LabelEncoder()\n# train['MSZoning']=labelencoder_X.fit_transform(train['MSZoning' ])\n# list(labelencoder_X.inverse_transform(train['MSZoning']))\n\n# onehotencoder = OneHotEncoder(categorical_features = [0])\n# onehotencoder.fit_transform(train)\n# pd.get_dummies(df, prefix=['A', 'D'], columns=['A', 'D'])\n\n\n\n# # use pd.concat to join the new columns with your original dataframe\n# train = pd.concat([train,pd.get_dummies(train ,columns=['MSZoning', 'Street'], prefix=['MSZoning', 'Street'])],axis=1)\n# test = pd.concat([test,pd.get_dummies(train ,columns=['MSZoning', 'Street'], prefix=['MSZoning', 'Street'])],axis=1)\n# # now drop the original 'country' column (you don't need it anymore)\n# train.drop(['MSZoning', 'Street'],axis=1, inplace=True)\n# test.drop(['MSZoning', 'Street'],axis=1, inplace=True)\n\n\ncol_obj = train.select_dtypes([np.object]).columns.values.tolist()\n\n\n# col_obj.values\ntrain_objs_num = len(train)\ntest['SalePrice']=-1;\ndataset = pd.concat(objs=[train, test], axis=0,sort=False)\ndataset_preprocessed = pd.get_dummies(dataset ,columns=col_obj, prefix=col_obj)\n# dataset_preprocessed = pd.concat([dataset,pd.get_dummies(dataset ,columns=col_obj, prefix=col_obj)],axis=1)\ndataset_preprocessed = pd.concat([dataset[col_obj],pd.get_dummies(dataset ,columns=col_obj, prefix=col_obj)],axis=1)\ndataset_preprocessed.drop(col_obj,axis=1, inplace=True)\ntrain = dataset_preprocessed[:train_objs_num]\ntest = dataset_preprocessed[train_objs_num:]\n\ntest.drop(['SalePrice'],axis=1, inplace=True)\n# dataset_preprocessed['SaleType_NA']\ndataset_preprocessed.to_csv('1.csv', index=False)\n\n# # use pd.concat to join the new columns with your original dataframe\n# train = pd.concat([train,pd.get_dummies(train ,columns=col_obj, prefix=col_obj)],axis=1)\n# test = pd.concat([test,pd.get_dummies(test ,columns=col_obj, prefix=col_obj)],axis=1)\n\n\n# # now drop the original 'country' column (you don't need it anymore)\n# train.drop(col_obj,axis=1, inplace=True)\n# test.drop(col_obj,axis=1, inplace=True)\n\n\n\n\n","486a1687":"# for dataset in train_test_data:\n#     dataset = pd.concat([dataset,pd.get_dummies(dataset['MSZoning'], prefix='MSZoning')],axis=1)\n#     dataset.drop(['MSZoning'],axis=1, inplace=True)\n   \n   ","1c56463e":"print(train.shape,test.shape)\ntrain.isnull().sum().loc[lambda x : x>0].sort_values(ascending=False)\ntest.isnull().sum().loc[lambda x : x>0].sort_values(ascending=False)\n \ntrain.isnull().sum()\n\n# train['MSZoning'].isnull().sum()\n# train['MSZoning'].value_counts()\n\n# MSZoning=train.groupby('MSZoning')\n# MSZoning.first()","f3f4271d":"# train['Street'].value_counts()\n\n# street_mapping={\n#     'Pave':1,\n#     'Grvl':2\n# }\n\n# for dataset in train_test_data:\n#     dataset['Street']=dataset['Street'].map(street_mapping)\n   \n\n# train.head()\n# test.head( )","dccc1b7e":"train.info()","69b7ccd2":"%%javascript\nIPython.OutputArea.auto_scroll_threshold = 9999;","6b5a7e7a":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set() # setting seaborn default for plots","149e8f51":"def bar_chart(feature):\n    survived = train[train['SalePrice']==1][feature].value_counts()\n    dead = train[train['SalePrice']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['SalePrice','SalePrice']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))","9ea1a2fb":"# bar_chart(train['SaleType_NA'])","e1af6cff":"facet = sns.FacetGrid(train, hue=\"SaleType_NA\",aspect=4)\nfacet.map(sns.kdeplot,'SalePrice',shade= True)\nfacet.set(xlim=(0, train['SalePrice'].max()))\nfacet.add_legend()\n \nplt.show() ","0a642a93":"train = train.drop(['Id'], axis=1)\n# test_data = test.drop(\"Id\", axis=1).copy()\ntest_data = test.drop(['Id'], axis=1)\n","3a35ef64":"train_data = train.drop('SalePrice', axis=1)\ntarget = train['SalePrice']\n# train_data.columns-train.columns\ntrain.shape,train_data.shape, target.shape,  test.shape\n# train_data['SalePrice']\n\n\n","21b74276":"train_data.head(10)","2605f021":"# Importing Classifier Modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nimport numpy as np","946d2e0f":"train.info()","945b0dfc":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","997f8bc3":"#Fitting multiple layer regression to the training set\n# type(target)\n# target.reshape((-1, 1))\nfrom sklearn.linear_model  import LinearRegression\nregressor=LinearRegression()\nregressor.fit(train_data,target.T)\n\ntrain_data.shape,target.shape,target.T.shape\n\ntrain_data.to_csv('train_data.csv', index=False)\ntarget.to_csv('target.csv', index=False)\ntest_data.to_csv('test1.csv', index=False)\n#predict\ntest.info()\ntest_pred=regressor.predict(test_data)","b0233c35":"scores = cross_val_score(regressor,train_data, target, cv = 3)\nprint(scores)\nprint(round(np.mean(scores)*100,2))","e98064aa":"submission = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": test_pred\n    })\n\nsubmission.to_csv('submission.csv', index=False)","ae33d35a":"# from sklearn import cross_validation, linear_model\n\n# loo = cross_val_score.LeaveOneOut(len(target))\n\n# regr = linear_model.LinearRegression()\n\n# scores = cross_validation.cross_val_score(regressor, train_data, target, scoring='mean_squared_error', cv=loo)\n\n# # This will print the mean of the list of errors that were output and \n# # provide your metric for evaluation\n# print scores.mean()","3aff0c2c":"\n# scoring = 'accuracy'\n# score = cross_val_score(regressor, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\n# print(score)","e5251425":"# # Visualising the Linear Regression results\n# plt.scatter(train_data, target.T, color = 'red')\n# plt.plot(train_data, regressor.predict(train_data), color = 'blue')\n# plt.title('Truth or Bluff (Linear Regression)')\n# plt.xlabel('Position level')\n# plt.ylabel('Salary')\n# plt.show()","7f84ac1d":"clf = SVC(gamma='auto')\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100,2)","19d0590b":"clf = GaussianNB()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","39c6ad71":"# Naive Bayes Score\nround(np.mean(score)*100, 2)","6b6d4f34":"**1. Collecting data**\n ","143a4fe9":"### SVM","7b80b9e1":"# **3. Feature engineering**\n","8032caf0":"\n**2. Data analysing**\n","1b17d6fb":"* PoolQC          1453\n* MiscFeature     1406\n* Alley           1369\n* Fence           1179\n* FireplaceQu      690\n* LotFrontage      259\n* GarageYrBlt       81\n* GarageType        81\n* GarageFinish      81\n* GarageQual        81\n* GarageCond        81\n* BsmtFinType2      38\n* BsmtExposure      38\n* BsmtFinType1      37\n* BsmtCond          37\n* BsmtQual          37\n* MasVnrArea         8\n* MasVnrType         8\n* Electrical         1","2b1b2588":"# Relationship with categorical features","34f3ace5":"Those feature are String, NULL replace with \"NA\"****","2ebc195e":"**5. Testing**","9d917316":"# Relationship with numerical variables","104348b7":"# # LinearRegression","79704559":"**LabelEncoder & OneHotEncoder**","344062a0":"# House price\n\n**Steps:**\n1. Collecting data\n2. Data analysing\n3. Feature engineering\n4. Modelling \n5. Testing\n\n\n\n","aa3b8742":"# Correlation matrix (heatmap style)","60a31fe8":"# **4. Modelling **\n","3a093d89":"# Scatter plots between 'SalePrice' and correlated variables (move like Jagger style)","daf2a372":"### Cross Validation (K-fold)","0dd0de1a":"# 'SalePrice' correlation matrix (zoomed heatmap style)","be344c91":"Data import by pandas","addae3dc":"# Naive Bayes"}}