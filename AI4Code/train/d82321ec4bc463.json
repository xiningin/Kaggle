{"cell_type":{"c4298154":"code","dfe134bf":"code","45816f8c":"code","5fbf33fa":"code","b9b1b589":"code","1b5728ba":"code","9d81ca80":"code","9f6208de":"code","1bd249e2":"code","828bcf6e":"code","44fe207e":"code","7a3b1ede":"code","5bd1e4fd":"code","6c596c15":"code","7d7ac8f5":"code","55568991":"code","6d51ec15":"code","a6f53f53":"code","56a886d0":"code","7fa5f366":"code","382a4a9e":"code","f94b7883":"code","a2bf05d6":"code","66fbb460":"code","9e0e8c8c":"code","8774737e":"code","e102646c":"code","d3939d94":"code","0ae5becd":"code","c2fb74c2":"code","05f3b342":"code","439b7890":"code","4793205d":"code","11e38acc":"code","3ca103e5":"code","a6a558d5":"code","c378a895":"code","efc94b5b":"code","a13e3a37":"code","8fa31d95":"code","a7bfc59c":"code","0a411ee2":"code","2def5c18":"markdown","c79e1625":"markdown","47f392c7":"markdown","bc38077d":"markdown","ec5fb87a":"markdown","dda39aa0":"markdown","882ef838":"markdown","59b38ac2":"markdown","b839453c":"markdown","06095c71":"markdown","ff934889":"markdown","e489784b":"markdown","132a2157":"markdown","93f8443b":"markdown","c1e3585c":"markdown","3ddc5a32":"markdown","17099017":"markdown","d0c06f0b":"markdown","0b2839e9":"markdown","2ba6aa93":"markdown","e2436952":"markdown","5f136ec0":"markdown"},"source":{"c4298154":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dfe134bf":"import category_encoders as c_e\nimport lightgbm as lgb\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport pickle\nimport seaborn as sns\nimport shap\nimport time\n\n\nfrom calendar import monthrange\nfrom datetime import date, timedelta\nfrom itertools import product\nfrom sklearn import preprocessing\nfrom xgboost import plot_importance\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler","45816f8c":"def prepare_sales_monthly(sales):\n    \"\"\"\n    Aggregates sales data montly\n    :arg sales (pandas df) - sales df from sales_train.csv\n    :return sales_train_montly_df (pandas df) - sales aggregated montly\n    \"\"\"\n    \n    sales_train_df_no_dupl_shops = adjust_duplicated_shops(sales)\n\n    sales_train_montly_df = sales_train_df_no_dupl_shops.groupby(\n        ['date_block_num', 'shop_id', 'item_id'])['item_price', 'item_cnt_day', 'date'].agg(\n        {'item_price': 'mean',\n         'date': 'min',\n         'item_cnt_day': 'sum'})\n    sales_train_montly_df = sales_train_montly_df.reset_index()\n\n    colnames = ['date_block_num', 'shop_id', 'item_id', 'item_price_avg', 'date_min', 'item_cnt_month']\n    sales_train_montly_df.columns = colnames\n\n    print('sales_train_montly_df has shape ', sales_train_montly_df.shape)\n    return sales_train_montly_df\n\n\ndef adjust_duplicated_shops(df):\n    'Function that combines duplicated shop names'\n    # from https:\/\/www.kaggle.com\/taranenkodaria\/predict-future-sales-the-russian-forecast\n    \n    df.loc[df['shop_id'] == 0, 'shop_id'] = 57\n    df.loc[df['shop_id'] == 1, 'shop_id'] = 58\n    df.loc[df['shop_id'] == 11, 'shop_id'] = 10\n    df.loc[df['shop_id'] == 40, 'shop_id'] = 39\n    df.loc[df['shop_id'] == 23, 'shop_id'] = 24\n\n    return df\n\n\ndef remove_outliers(df):\n    'Removes duplicates from sales df'\n    return df[(df[\"item_price\"] < np.percentile(df[\"item_price\"], q=99))\n              & (df[\"item_price\"] > 0)\n              & (df[\"item_cnt_day\"] >= 0)\n              & (df[\"item_cnt_day\"] < np.percentile(df[\"item_cnt_day\"], q=99))]\n\n\ndef prepare_sales(use_cache):\n    \"\"\"\n    Function that applies preprocessing steps to the original sales df (from sales_train.csv) \n    :arg use_cache (Bool) if True the resulting data is loaded from already created pickle file (to save time), if False \n                          then all processing steps are performed\n    :return sales (pandas df) - sales df ready to be used for modeling\n    \"\"\"\n    if use_cache:\n        try:\n            infile = open(\"sales_df.pickle.dat\", \"rb\")\n            sales = pickle.load(infile)\n            infile.close()\n            return sales\n        except (OSError, IOError) as e:\n            pass\n    \n    print('prepare_sales')\n    # load and preprocess sales\n    sales = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\n    sales = remove_outliers(sales)\n    sales = prepare_sales_monthly(sales)\n    sales = add_zero_sales(sales, None)\n\n    first_items_sales = sales.groupby('item_id')['date_block_num'].min()\n    sales['when_first_sold'] = sales['item_id'].map(first_items_sales)\n    sales['when_first_sold'] = sales['date_block_num'] - sales['when_first_sold']\n\n\n    print('prepare_sales has shape ', sales.shape)\n    if use_cache:\n        pickle.dump(sales, open(\"sales_df.pickle.dat\", \"wb\"))\n    return sales\n\n\ndef create_matrix(df):\n    'creates shop_id\/item_id matrix based on sales to match the test set distribution'\n    # adopted from https:\/\/www.kaggle.com\/gordotron85\/future-sales-xgboost-top-3\n    x = date(2013, 1, 1)\n    matrix = []\n    cols = [\"date_block_num\", \"shop_id\", \"item_id\", \"Year\", \"Month\"]\n    for i in range(df.date_block_num.max() + 1):\n        try:\n            sales = df[df.date_block_num == i]\n            matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique(), [x.year], [x.month])), dtype=np.int16))\n            x = x.replace(month=x.month+1)\n        except ValueError:\n            if x.month == 12:\n                x = x.replace(year=x.year+1, month=1)\n            else:\n                # next month is too short to have \"same date\"\n                # pick your own heuristic, or re-raise the exception:\n                raise\n\n    matrix = pd.DataFrame(np.vstack(matrix), columns = cols )\n    matrix[\"date_block_num\"] = matrix[\"date_block_num\"].astype(np.int8)\n    matrix[\"shop_id\"] = matrix[\"shop_id\"].astype(np.int8)\n    matrix[\"item_id\"] = matrix[\"item_id\"].astype(np.int16)\n    matrix.sort_values(cols, inplace=True)\n\n    return matrix\n\n\ndef add_zero_sales(df, matrix):\n    'Combines original sales df and matrix'\n    if matrix is None:\n        matrix = create_matrix(df)\n\n    df = pd.merge(matrix, df, how='left',\n                  left_on=['date_block_num', 'shop_id', 'item_id'],\n                  right_on=['date_block_num', 'shop_id', 'item_id'])\n\n    return df","5fbf33fa":"def extract_shop_type(df):\n    'Extracts type of the shop and creates the shop_type_1 and shop_type_2 columns'\n\n    df.loc[df['shop_name'].str.contains('\u0422\u0426'),'shop_type_1'] = 'type_1'\n    df.loc[df['shop_name'].str.contains('\u0422\u041a'),'shop_type_1'] = 'type_2'\n    df.loc[df['shop_name'].str.contains('\u0422\u0420\u0426'),'shop_type_1'] = 'type_3'\n    df.loc[df['shop_name'].str.contains('\u0422\u0420\u041a'),'shop_type_1'] = 'type_4'\n\n    df.loc[(df['shop_name'].str.contains('\u0422\u0426')) |\n           (df['shop_name'].str.contains('\u0422\u041a')),'shop_type_2'] = 'type_1'\n    df.loc[(df['shop_name'].str.contains('\u0422\u0420\u0426')) |\n           (df['shop_name'].str.contains('\u0422\u0420\u041a')),'shop_type_2'] = 'type_2'\n\n    df.shop_type_1 = df.shop_type_1.fillna('NONE')\n    df.shop_type_2 = df.shop_type_2.fillna('NONE')\n\n    le_1 = preprocessing.OrdinalEncoder(dtype=np.int32)\n    df['shop_type_1'] = le_1.fit_transform(df[['shop_type_1']])\n    le_2 = preprocessing.OrdinalEncoder(dtype=np.int32)\n    df['shop_type_2'] = le_2.fit_transform(df[['shop_type_2']])\n\n    return df\n\n\ndef extract_shop_city(df):\n    'Extracts shop city name and city type and creates two new columns'\n\n    # City type: 1 if city is Moscow or Sankt Petersburg (they are quite different from the rest of Russia)\n    df['shop_city_type'] = 0\n\n    df['shop_city'] = df['shop_name'].str.split(' ').str[0]\n    df.drop(columns=['shop_name'], inplace=True)\n\n    df.loc[df['shop_city'].isin(['\u041c\u043e\u0441\u043a\u0432\u0430', '\u0421\u041f\u0431']), 'shop_city_type'] = 1\n\n    le = preprocessing.OrdinalEncoder(dtype=np.int32)\n    df['shop_city'] = le.fit_transform(df[['shop_city']])\n\n    return df\n\n\ndef fix_shops(shops_df):\n    \"\"\"\n    This function modifies the shops df inplace.\n    It correct's 3 shops that we have found to be 'duplicates'\n    and also creates a few more features: extracts the city and encodes it using OrdinalEncoder\n    \"\"\"\n\n    shops_df = shops_df.loc[~shops_df['shop_id'].isin([0, 1, 11, 40, 23])]\n    shops_df = extract_shop_type(shops_df)\n    shops_df = extract_shop_city(shops_df)\n\n    return shops_df","b9b1b589":"def correct_item_category_name(df):\n    'adjusts the format of the \"item_category_name\" column'\n    df.loc[df['item_category_name'] == '\u0411\u0438\u043b\u0435\u0442\u044b (\u0426\u0438\u0444\u0440\u0430)','item_category_name'] = '\u0411\u0438\u043b\u0435\u0442\u044b - \u0426\u0438\u0444\u0440\u0430'\n    df.loc[df['item_category_name'] == '\u0414\u043e\u0441\u0442\u0430\u0432\u043a\u0430 \u0442\u043e\u0432\u0430\u0440\u0430','item_category_name'] = '\u0414\u043e\u0441\u0442\u0430\u0432\u043a\u0430 \u0442\u043e\u0432\u0430\u0440\u0430 - service'\n    df.loc[df['item_category_name'] == '\u041a\u0430\u0440\u0442\u044b \u043e\u043f\u043b\u0430\u0442\u044b (\u041a\u0438\u043d\u043e, \u041c\u0443\u0437\u044b\u043a\u0430, \u0418\u0433\u0440\u044b)',\n           'item_category_name'] = '\u041a\u0430\u0440\u0442\u044b \u043e\u043f\u043b\u0430\u0442\u044b - \u041a\u0438\u043d\u043e, \u041c\u0443\u0437\u044b\u043a\u0430, \u0418\u0433\u0440\u044b'\n    df.loc[df['item_category_name'] == '\u0421\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435','item_category_name'] = '\u0421\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435 - none'\n    df.loc[df['item_category_name'] == '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u043f\u0438\u043b\u044c)','item_category_name'] = '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 - \u0448\u043f\u0438\u043b\u044c'\n    df.loc[df['item_category_name'] == '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u0442\u0443\u0447\u043d\u044b\u0435)','item_category_name'] = '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 - \u0448\u0442\u0443\u0447\u043d\u044b\u0435'\n    df.loc[df['item_category_name'] == '\u042d\u043b\u0435\u043c\u0435\u043d\u0442\u044b \u043f\u0438\u0442\u0430\u043d\u0438\u044f','item_category_name'] = '\u042d\u043b\u0435\u043c\u0435\u043d\u0442\u044b \u043f\u0438\u0442\u0430\u043d\u0438\u044f - none'\n\n    return df\n\n\ndef extract_main_category(df):\n    'extracts the first part of the category_name to build a feature'\n    df['item_category_main'] = df['item_category_name'].str.split(' - ').str[0]\n\n    le = preprocessing.OrdinalEncoder(dtype=np.int32)\n    df['item_category_main'] = le.fit_transform(df[['item_category_main']])\n\n    return df\n\n\ndef extract_whether_digital(df):\n    'extracts a feature whether item_category_name contains a word \"Digit\"'\n    df['is_category_digital'] = 0\n    df.loc[df['item_category_name'].str.contains('\u0426\u0438\u0444\u0440\u0430'),'is_category_digital'] = 1\n\n    return df\n\n\ndef extract_ps_related(df):\n    'extracts a feature whether item_category_name contains a word \"PS\"'\n    df['is_category_ps_related'] = 0\n    df.loc[df['item_category_name'].str.contains('PS', case=False), 'is_category_ps_related'] = 1\n\n    return df\n\n\ndef fix_item_category(df):\n    'Applies all above fritten functions to item_category df'\n    df = correct_item_category_name(df)\n    df = extract_main_category(df)\n    df = extract_whether_digital(df)\n    df = extract_ps_related(df)\n    df.drop(columns = ['item_category_name'], inplace = True)\n\n    return df","1b5728ba":"def create_df(use_cache=True):\n    'Function that creates the train df'\n\n    sales = prepare_sales(use_cache)\n\n    shops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\n    shops = fix_shops(shops)  # fix the shops as we have seen before\n\n    items_category = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\n    items_category = fix_item_category(items_category)\n\n    items = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\n    items.drop(columns=['item_name'], inplace=True)\n\n    merged_df = sales.merge(shops, on = 'shop_id', how = 'left')\n    print('df size with zero sales ', merged_df.shape)\n\n    items_to_merge = items.merge(items_category, on = 'item_category_id')\n    merged_df = merged_df.merge(items_to_merge, on = 'item_id', how = 'left')\n    \n    merged_df['item_cnt_month'] = merged_df['item_cnt_month'].fillna(0)\n    \n    return merged_df\n\n\ndef calculate_missing_prices_for_train_set(df):\n    'Imputes the missing price into train set based on the median value of month\/item_id combination'\n    \n    sales_raw = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\n    \n    price_stat = pd.DataFrame(sales_raw.groupby(['date_block_num', 'item_id'])['item_price'].median()).reset_index()\n\n    df_with_price = df.merge(price_stat, on=['date_block_num', 'item_id'], how='left')\n    df_with_price['temp_col'] = df_with_price['item_price_avg'].fillna(df_with_price['item_price'])\n    df_with_price.drop(['item_price_avg', 'item_price'], axis=1, inplace=True)\n    df_with_price.rename(columns={'temp_col': 'item_price_avg'}, inplace=True)\n\n    return df_with_price\n\n\ndef downcast_dtypes(df):\n    \"\"\"\n        Changes column types in the dataframe:\n                `float64` type to `float32`\n                `int64`   type to `int32`\n    \"\"\"\n\n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype == \"int64\"]\n\n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int32)\n\n    return df\n\n\ndef construct_lag(df, original_df, colname, target_var = 'item_cnt_month'):\n    '''\n    function that constructs lag\n    :arg \n    df (pandas df) - train df after all transformation after date_block_num filter\n    original_df (pandas df) - train df after all transformation withou filtering\n    colname (list of str) - list of columns to build lag onto (example: ['shop_id'] or ['shop_id','item_id'])\n    target_var (str) - name of the parameter to calculate with lag\n    :return: df with additional lag columns\n    '''\n\n    # calculate montly statistics over lag columns\n    stat = pd.DataFrame(original_df.groupby(['date_block_num', *colname])[target_var].sum()).reset_index()\n    stat[target_var] = stat[target_var].round(2)\n    stat = downcast_dtypes(stat)\n\n    for month_shift in [1,2,3,12]:\n        # rename a resulting column in the copy of stats for further merge\n        stat_copy = stat.copy()\n        new_colname = 'lag_sum_{}_{}_{}'.format(target_var,'_'.join(colname), month_shift)\n        stat_copy.rename(columns={target_var: new_colname}, inplace=True)\n\n        # merge a lagged column with original dataset\n        df['temp_col'] = df['date_block_num'] - month_shift\n        df = df.merge(stat_copy, left_on=['temp_col', *colname],\n                      right_on=['date_block_num', *colname], how='left')\n\n        # perform some final cleaning steps\n        df.drop(columns=['date_block_num_y', 'temp_col'], inplace=True)\n        df.rename(columns={'date_block_num_x': 'date_block_num'}, inplace=True)\n        df[new_colname].fillna(0, inplace=True)\n\n        print(target_var, colname, month_shift, df.shape)\n\n    return df\n\n\ndef add_lag(all_data_df, df_to_add_lag, target_var = 'item_cnt_month'):\n    \"\"\" adds lag columns to original dataframe\n    :arg df (pandas df) train df after all modifications\n    :return df (pandas df) original df with lag columns. Important (!) the 2013 data is deleted\n    \"\"\"\n\n    lag_columns_list = [\n        ['shop_id','item_id'],\n        ['shop_id','item_category_id'],\n        ['shop_id'],\n        ['item_id'],\n        ['item_category_id'],\n        ['item_category_main']\n    ]\n\n    df_with_lag = df_to_add_lag.loc[df_to_add_lag['date_block_num'] >= 12]\n    for colname in lag_columns_list:\n        df_with_lag = construct_lag(df_with_lag, all_data_df, colname, target_var)\n\n    return df_with_lag\n\n\ndef calculate_number_of_particular_days(day_of_week):\n    \"\"\"\n    Function that calculates how many particular days were in each months of 2013-2015\n                    (like how many mondays were in 02.2014)\n    :arg day_of_week (int) number of weekday (0 - Monday, 6 - Sunday)\n    :return: day_of_week_series (pandas series) index - month_number, value - number of particular day\n    \"\"\"\n\n    def checkio(from_date, to_date, day_of_week):\n        return len([1 for d in range((to_date - from_date).days + 1)\n                    if (from_date + timedelta(d)).weekday() in [day_of_week]])\n\n    month_number = []\n    number_of_weekend_days = []\n\n    counter = 0\n    for year in [2013, 2014, 2015]:\n        for month in range(1, 13):\n            start_m, end_m = monthrange(year, month)\n            weekend_days_in_month = checkio(date(year, month, 1), date(year, month, end_m), day_of_week)\n            month_number.append(counter)\n            number_of_weekend_days.append(weekend_days_in_month)\n            counter += 1\n    day_of_week_series = pd.Series(index=month_number, data=number_of_weekend_days)\n\n    return day_of_week_series\n\n\ndef add_days_stat(df):\n    df['number_of_saturdays'] = df['date_block_num'].map(calculate_number_of_particular_days(5)).astype(np.int8)\n    df['number_of_sundays'] = df['date_block_num'].map(calculate_number_of_particular_days(6)).astype(np.int8)\n    df['number_of_mondays'] = df['date_block_num'].map(calculate_number_of_particular_days(0)).astype(np.int8)\n    df['number_of_days_in_month'] = df['Month'].map(\n        pd.Series([0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31])).astype(np.int8)\n\n    return df\n\n\ndef create_feature_names_list(df):\n    \"\"\"\n    Function that builds a list of all features required to train\/test model\n    :arg df (pandas df) - train set df\n    :return features (list) - list of feature names\n    \"\"\"\n    features = ['date_block_num', 'shop_id', 'item_id', 'Year', 'Month', 'shop_type_1',\n                'shop_type_2', 'shop_city_type', 'shop_city', 'item_category_id',\n                'item_category_main', 'is_category_digital', 'is_category_ps_related', 'item_price_avg',\n                'when_first_sold',\n                'number_of_mondays', 'number_of_saturdays', 'number_of_sundays', 'number_of_days_in_month']\n    lag_cols = [x for x in df.columns if 'lag' in x]\n    features = features + lag_cols\n\n    return features\n\n\ndef create_train_set(addition_to_filename = ''):\n    \"\"\"\n    Function that creates a train set and saves it in pickle file for further use\n    :arg addition_to_filename (str) - any additional information appended to filename\n    :return pickle file with train set without mean encoding\n    \"\"\"\n    df = create_df()\n    print('original df size is ', df.shape)\n    print('original df columns ', df.columns)\n\n    df = calculate_missing_prices_for_train_set(df)\n    print('df size after averaging price ', df.shape)\n    df = add_lag(all_data_df = df, df_to_add_lag= df)\n    df['revenue'] = df['item_price_avg'] * df['item_cnt_month']\n    #df = add_lag(df, df, 'revenue')\n    \n    df = add_days_stat(df)\n    df = downcast_dtypes(df)\n    \n    print(df.columns)\n\n    # df = h.add_holidays(df)\n    # print('df size with holidays ', df.shape)\n\n    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n    pickle.dump(df, open(\"train.pickle.dat\", \"wb\"))\n\n    # save feature names for further use\n    features_list = create_feature_names_list(df)\n    pickle.dump(features_list, open(\"features.pickle.dat\", \"wb\"))\n    \n    return df, features_list\n\n\ndef load_train_set_and_features_list(version):\n    \"\"\"\n    Function that loads train set and features list from corresponding pickle files\n    :param version: (str) part of the pickle filename (for example \"20201224-121548_first_ver\")\n    :return: df (pandas df) train set\n             features (list) feature names\n    \"\"\"\n    filename_train_set = f'{version}_train.pickle.dat'\n    filename_features_list = f'{version}_features.pickle.dat'\n\n    if os.path.exists(filename_train_set) and os.path.exists(filename_features_list):\n\n        infile = open(filename_train_set, \"rb\")\n        df = pickle.load(infile)\n        infile.close()\n\n        infile = open(filename_features_list, \"rb\")\n        features = pickle.load(infile)\n        infile.close()\n\n        return df, features\n\n    else:\n        raise ValueError('Files do not exist!')\n\n        \n# TEST SET PREPROCESSING\n\ndef add_when_first_sold_to_test(test):\n    \"\"\"ads a 'when_first_sold column to test df'\"\"\"\n    sales = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\n    sales = remove_outliers(sales)\n    first_items_sales = sales.groupby('item_id')['date_block_num'].min()\n    test['when_first_sold'] = test['item_id'].map(first_items_sales)\n    test['when_first_sold'] = test['date_block_num'] - test['when_first_sold']\n    test['when_first_sold'].fillna(0, inplace=True)\n\n    return test\n\n\ndef create_test_df(train_df):\n\n    test = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\n    test['date_block_num'] = 34\n    test['Year'] = 2015\n    test['Month'] = 11\n\n    # load shops and preprocess it\n    shops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\n    shops = fix_shops(shops)  # fix the shops as we have seen before\n\n    # load item_category and preprocess it\n    items_category = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\n    items_category = fix_item_category(items_category)\n\n    # load items\n    items = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\n    items.drop(columns = ['item_name'], inplace = True)\n\n    # merge data\n    items_to_merge = items.merge(items_category, on = 'item_category_id')\n    test_merged = test.merge(shops, on = 'shop_id', how = 'left')\n    test_merged = test_merged.merge(items_to_merge, on = 'item_id', how = 'left')\n    \n    return test_merged\n\n\ndef add_price_col_to_test(test, regime = 'test'):\n\n    # Algorithm:\n    # 1. take the last price for the shop\/item_id\n    # 2. take the average price for the item_id\n    # 3. take the median price for the category\n\n    items = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\n    sales_raw = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\n    sales_raw = adjust_duplicated_shops(sales_raw)\n    if regime == 'val':\n        sales_raw = sales_raw.loc[sales_raw['date_block_num'] < 33]\n\n    last_price = sales_raw.sort_values(['shop_id','date_block_num'], ascending = [True, True])\n    last_price = last_price.drop_duplicates(subset = ['shop_id','item_id'], keep = 'last')\n    last_price = last_price[['shop_id','item_id','item_price']]\n    last_price.columns = ['shop_id','item_id','item_price_avg']\n    test_last_price = test.merge(last_price, on = ['shop_id','item_id'], how = 'left').dropna(subset = ['item_price_avg'])\n    test_last_price_rest = test.loc[~test['ID'].isin(test_last_price['ID'])]\n\n    mean_item_price = pd.DataFrame(sales_raw.groupby('item_id')['item_price'].median()).reset_index()\n    mean_item_price.columns = ['item_id','item_price_avg']\n    test_mean_item = test_last_price_rest.merge(mean_item_price, on = 'item_id', how = 'left').dropna(subset = ['item_price_avg'])\n    id_list = list(test_mean_item['ID']) + list(test_last_price['ID'])\n    test_mean_item_rest = test.loc[~test['ID'].isin(id_list)]\n\n    sales_items = sales_raw.merge(items[['item_id','item_category_id']], on = 'item_id')\n\n    median_item_cat_price = pd.DataFrame(sales_items.groupby('item_category_id')['item_price'].median()).reset_index()\n    median_item_cat_price.columns = ['item_category_id','item_price_avg']\n\n    test_category_price = test_mean_item_rest.merge(median_item_cat_price, on = 'item_category_id', how = 'left')\n\n    test_with_price = pd.concat([test_last_price,test_mean_item,test_category_price], axis = 0)\n\n    test_with_price.sort_values('ID', inplace = True)\n\n    test_with_price.set_index('ID', inplace = True)\n\n    return test_with_price\n\n\ndef perform_target_encoding(train, test):\n\n    cat_cols = ['shop_id',\n                'Year',\n                'Month',\n                'shop_type_1',\n                'shop_type_2',\n                'shop_city_type',\n                'shop_city',\n                'item_category_id',\n                'item_category_main',\n                'is_category_digital',\n                'is_category_ps_related']\n\n    X_train = train.drop('item_cnt_month', axis=1)\n    Y_train = train['item_cnt_month']\n    enc = c_e.TargetEncoder(cols = cat_cols, smoothing=100)\n\n    X_train = enc.fit_transform(X_train, Y_train)\n    train = X_train.copy()\n    train['item_cnt_month'] = Y_train\n    test = enc.transform(test)\n\n    return train, test\n\n\ndef create_test_set(train, features):\n    \"\"\"\n    Function that creates test set\n    :param train: (pandas df) train set from pickle file\n    :param features: (list) - list of feature names\n    :return: test_with_price (pandas df) - test set\n    \"\"\"\n    test = create_test_df(train)\n    print('df base size ', test.shape)\n\n    test = add_when_first_sold_to_test(test)\n\n    test = add_lag(all_data_df = train, df_to_add_lag= test)\n    \n    #test = add_lag(train, test, 'revenue')\n\n    test = add_days_stat(test)\n\n    test_with_price = add_price_col_to_test(test)\n\n    test_with_price = test_with_price[features]\n\n    test_with_price = downcast_dtypes(test_with_price)\n    \n    pickle.dump(test_with_price, open(f\"test.pickle.dat\", \"wb\"))\n\n    return test_with_price\n","9d81ca80":"def create_train_val_split(df, features):\n    \"\"\"\n    Function that splits the df into train \/ val splits\n    :param df: (pandas df) all training data loaded from pickle file\n    :param features: (list) oll feature names loaded from pickle file\n    :return: train\/val splits\n    \"\"\"\n    print('Number of features Train_set: ', len(features))\n    print('Features: ', features)\n\n    target = ['item_cnt_month']\n\n    train = df[(df[\"date_block_num\"] < 33)]\n    val = df[(df[\"date_block_num\"] == 33)]\n    #test = df[(df[\"date_block_num\"] == 33)]\n\n\n    # Adjust price of the val according to test set\n    val = val.drop('item_price_avg', axis = 1)\n    val = val.reset_index().rename(columns={'index':'ID'})\n    val = add_price_col_to_test(val, regime='val')\n\n    X_train = train[features]\n    X_val = val[features]\n\n    Y_train = train[target]\n    Y_val = val[target]\n\n    X_train['Year'] = X_train['Year'].astype(int)\n    X_train['Month'] = X_train['Month'].astype(int)\n    X_val['Year'] = X_val['Year'].astype(int)\n    X_val['Month'] = X_val['Month'].astype(int)\n\n    return X_train, Y_train, X_val, Y_val\n\n\n\ndef build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features):\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_val = lgb.Dataset(X_val, y_val)\n    model = lgb.train(params=params, train_set=lgb_train, valid_sets=(lgb_train, lgb_val), verbose_eval=10,\n                     categorical_feature=cat_features)\n    return model\n\n\ndef build_lgb_model_test(params, X_train, y_train, cat_features):\n    lgb_train = lgb.Dataset(X_train, y_train)\n    model = lgb.train(params=params, train_set=lgb_train, valid_sets=(lgb_train), verbose_eval=10,\n                     categorical_feature=cat_features)\n    return model\n","9f6208de":"def apply_0_to_not_sold_categories(df):\n    'Optional function that applies 0 to submission file rows with item_categories that were not sold in particular shops'\n    items = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\n    sales_raw = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\n    sales_raw = adjust_duplicated_shops(sales_raw)\n    merged_df = sales_raw.merge(items[['item_id','item_category_id']], on = 'item_id')\n\n    all_item_categories = list(merged_df['item_category_id'].unique())\n\n    not_available_categories_per_shop = {}\n\n    for shop_id in merged_df['shop_id'].unique():\n        shop_item_categories = list(merged_df.loc[merged_df['shop_id'] == shop_id,'item_category_id'].unique())\n        not_available_categories_per_shop[shop_id] = list(set(all_item_categories) - set(shop_item_categories))\n\n    counter = 0\n    for key, value in not_available_categories_per_shop.items():\n        counter += len(df.loc[(df['shop_id'] == key) & (df['item_category_id'].isin(value)), 'item_cnt_month'])\n        df.loc[(df['shop_id'] == key) & (df['item_category_id'].isin(value)), 'item_cnt_month'] = 0\n\n    print(counter)\n    return df\n\n\ndef correct_submission_for_not_sold_items(test_to_correct):\n    \"\"\"\n    function that checks whether particular item_id is still sold in shops.\n    arg: path_to_submission_to_improve (str) - path to submission file\n         path_to_sales_train (str) - path to sales_train.csv file\n    return: corrected_submission (pandas df) - an adjusted submission\n    \"\"\"\n\n    shop_sales = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\n\n    items_not_sold_in_last_3_months = {}\n\n    test = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\n\n    all_test_items = test.item_id.unique()\n\n    for shop_id in test['shop_id'].unique():\n        shop = shop_sales.loc[(shop_sales['shop_id'] == shop_id) & (shop_sales['item_id'].isin(all_test_items))]\n        shop_items = list(shop['item_id'].unique())\n        shop_items_3_months = list(shop.loc[shop['date_block_num'] > 30, 'item_id'].unique())\n\n        items_not_sold_in_last_3_months[shop_id] = list(set(shop_items) - set(shop_items_3_months))\n\n    test_work = test[['ID', 'shop_id', 'item_id']]\n\n    test_work_done = pd.DataFrame()\n\n    for key, value in items_not_sold_in_last_3_months.items():\n        test_df = test_work.loc[test_work['shop_id'] == key]\n        test_df['coeff'] = 1\n        test_df.loc[test_df['item_id'].isin(value),'coeff'] = 0\n        test_work_done = pd.concat([test_work_done, test_df], axis = 0)\n\n    test_to_correct_1 = test_to_correct.merge(test_work_done, on = 'ID', how = 'left')\n\n    # test_to_correct_1.fillna(1, inplace=True)\n\n    test_to_correct_1['item_cnt_month'] = test_to_correct_1['item_cnt_month'] * test_to_correct_1['coeff']\n\n    corrected_submission = test_to_correct_1[['ID','item_cnt_month']]\n\n    return corrected_submission\n\n\ndef create_submission_file(test, model=None, y_pred=None, name=None):\n    \"\"\"\n    Function that creates and saves a submission-ready file\n    :param test: (pandas df) test set\n    :param model: (xgboost model) trained model\n    :return: saved submission file\n    \"\"\"\n    if model:\n        y_pred = model.predict(test)\n\n    submission = test.copy()\n    submission['item_cnt_month'] = y_pred.clip(0, 20)\n    \n    submission = apply_0_to_not_sold_categories(submission)\n    submission = submission[['item_cnt_month']].reset_index()\n    submission.rename(columns={\"index\": \"ID\"}, inplace=True)\n    #print('apply_0_to_not_sold_categories ', submission.shape)\n\n    # This did not change much\n    #submission = correct_submission_for_not_sold_items(submission)\n\n    print('Expect (214200, 2)')\n    print('Actual ', submission.shape)\n    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n    submission.to_csv(timestr+'solution_{}.csv'.format(name), index=False)\n    print(\"Your submission was successfully saved!\")\n    \n    return","1bd249e2":"# Prepare training set and save it to pickle for reuse\n#train_set_identifier = 'first_ver'\n#df, features_list = create_train_set(train_set_identifier)\n\n# Uncomment if pickle files are available\ndf = pickle.load(open('.\/train.pickle.dat', \"rb\"))\nfeatures_list = pickle.load(open('.\/features.pickle.dat',\"rb\"))","828bcf6e":"# Prepare test set\n#test = create_test_set(df, features_list)\n\n# Uncomment if pickle files are available\ntest = pickle.load(open('.\/test.pickle.dat', \"rb\"))","44fe207e":"shops_df = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\n\ndf_mean_revenue_per_shop_month = df.groupby(['shop_id','date_block_num'])['revenue'].mean().reset_index()\ndf_mean_revenue_per_shop = df_mean_revenue_per_shop_month.groupby('shop_id')['revenue'].mean().reset_index()\ndf_mean_revenue_per_shop = df_mean_revenue_per_shop.merge(shops_df, on = 'shop_id', \n                                                          how = 'left').sort_values('revenue', ascending = False)","7a3b1ede":"# mean revenue per shop\nfig, axes = plt.subplots(figsize = (15,6))\nsns.barplot(x = df_mean_revenue_per_shop['shop_id'], y = df_mean_revenue_per_shop['revenue'], ax = axes)\nprint('Top 10 shops by revenue: ')\nprint(df_mean_revenue_per_shop.head(10))\nprint('\\nShops from Moscow have by far the highest sales!')","5bd1e4fd":"df_mean_revenue_per_shop.head(10)['shop_id']","6c596c15":"df_mean_revenue_per_shop_month_top_10 = df_mean_revenue_per_shop_month.loc[\n    df_mean_revenue_per_shop_month['shop_id'].isin(df_mean_revenue_per_shop.head(10)['shop_id'])]\n\nfig, axes = plt.subplots(figsize = (22,5), ncols = 3) \nsns.pointplot(x = 'date_block_num', y = 'revenue', data = df_mean_revenue_per_shop_month, ax = axes[0])\nsns.pointplot(x = 'date_block_num', y = 'revenue', data = df_mean_revenue_per_shop_month_top_10, ax = axes[1])\nsns.pointplot(x = 'date_block_num', y = 'revenue', data = df_mean_revenue_per_shop_month_top_10, hue = 'shop_id', ax = axes[2])\n\naxes[0].set_ylim(0,1300)\naxes[1].set_ylim(0,1300)\naxes[2].set_ylim(0,2000)\n\naxes[0].set_ylabel('mean revenue per month')\naxes[1].set_ylabel('mean revenue per month')\naxes[2].set_ylabel('mean revenue per month')\n\naxes[0].set_title('All Shops', fontsize = 16)\naxes[1].set_title('Top 10 Shops by revenue', fontsize = 16)\naxes[2].set_title('Top 10 Shops by revenue', fontsize = 16)\n\naxes[2].legend(bbox_to_anchor=(1.02, 1.05))\n\nprint('December sales are predominant')\nprint('November sales (test set) are expected to be higher than those of October (validation set)')","7d7ac8f5":"import gc\ndel df#, test\ngc.collect()","55568991":"X_train, Y_train, X_val, Y_val = create_train_val_split(df, features_list)","6d51ec15":"params = {\n    'objective': 'rmse',\n    'metric': 'rmse',\n    'max_depth':10,\n    'num_leaves': 1023,\n    'min_data_in_leaf':100, \n    'feature_fraction':0.7, # ratio of features to be randomly selected at each iteration\n    'learning_rate': 0.03,\n    'num_rounds': 1000,\n    'early_stopping_rounds': 10,\n    'seed': 42,\n    'device': 'gpu',\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n\n}\n#designating the categorical features which should be focused on\n\ncat_features = ['shop_id',\n 'Year',\n 'Month',\n 'shop_type_1',\n 'shop_type_2',\n 'shop_city_type',\n 'shop_city',\n 'item_category_id',\n 'item_category_main',\n 'is_category_digital',\n 'is_category_ps_related']","a6f53f53":"lgb_model = build_lgb_model(params, X_train, X_val, Y_train, Y_val, cat_features)","56a886d0":"fig, axes = plt.subplots(figsize = (8,15))\nlgb.plot_importance(lgb_model, ax = axes)","7fa5f366":"sample_df = X_train.sample(5000)\nshap_values = shap.TreeExplainer(lgb_model).shap_values(sample_df)\nshap.summary_plot(shap_values, sample_df)","382a4a9e":"# Train Final Light GBM Model on concatenated train and val sets\n\nparams = {\n    'objective': 'rmse',\n    'metric': 'rmse',\n    'max_depth':10,\n    'num_leaves': 1023,\n    'min_data_in_leaf':100, \n    'feature_fraction':0.7, # ratio of features to be randomly selected at each iteration\n    'learning_rate': 0.03,\n    'num_rounds': 170,\n    'early_stopping_rounds': 10,\n    'seed': 42,\n    'device': 'gpu',\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n\n}\n#designating the categorical features which should be focused on\n\ncat_features = ['shop_id',\n 'Year',\n 'Month',\n 'shop_type_1',\n 'shop_type_2',\n 'shop_city_type',\n 'shop_city',\n 'item_category_id',\n 'item_category_main',\n 'is_category_digital',\n 'is_category_ps_related']\n\nlgb_model = build_lgb_model_test(params, pd.concat([X_train, X_val]), pd.concat([Y_train, Y_val]), cat_features)\n","f94b7883":"pickle.dump(lgb_model, open(\"lgb_model.pickle.dat\", \"wb\"))","a2bf05d6":"# RF parameters tuning was done by my teammate Tullio Coppottelli\n\nts = time.time()\nrf = RandomForestRegressor(bootstrap=0.7, criterion='mse', max_depth=10,\n                           max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0,\n                           min_impurity_split=None, min_samples_leaf=1,\n                           min_samples_split=2, min_weight_fraction_leaf=0.0,\n                           n_estimators=50, n_jobs=-1, oob_score=False, random_state=42,\n                           verbose=1, warm_start=False)\nrf.fit(X_train,Y_train)\nprint(f\"Time required for RF: {time.time() - ts}\")","66fbb460":"Y_val_rf = rf.predict(X_val)\nprint('Val rmse:', np.sqrt(mean_squared_error(Y_val_rf, Y_val)))","9e0e8c8c":"sample_df = X_train.sample(5000)\nshap_values = shap.TreeExplainer(rf).shap_values(sample_df)\nshap.summary_plot(shap_values, sample_df)","8774737e":"# Training on the whole dataset\nts = time.time()\n\nrf_compl = RandomForestRegressor(bootstrap=0.7, criterion='mse', max_depth=10,\n                           max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0,\n                           min_impurity_split=None, min_samples_leaf=1,\n                           min_samples_split=2, min_weight_fraction_leaf=0.0,\n                           n_estimators=50, n_jobs=-1, oob_score=False, random_state=42,\n                           verbose=1, warm_start=False)\nrf_compl.fit(pd.concat([X_train, X_val]), pd.concat([Y_train, Y_val]))\nprint(f\"Time required for RF with combined train\/val set: {time.time() - ts}\")","e102646c":"pickle.dump(rf_compl, open(f\"rf_model.pickle.dat\", \"wb\"))","d3939d94":"# Scale the features to be able to train the linear model\nlr_scaler = MinMaxScaler()\nlr_scaler.fit(X_train)\nlr_train = lr_scaler.transform(X_train)\nlr_val = lr_scaler.transform(X_val)\nlr_test = lr_scaler.transform(test)","0ae5becd":"ts = time.time()\nlr_model = LinearRegression(n_jobs=-1)\nlr_model.fit(lr_train, Y_train)\nprint(f\"Time required for LR with combined train\/val set: {time.time() - ts}\")\n\nY_val_lr = lr_model.predict(lr_val)\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_val_lr, Y_val)))","c2fb74c2":"# Train LR on the whole dataset\nts = time.time()\nlr_train_compl = lr_scaler.transform(pd.concat([X_train, X_val]))\nlr_model_compl = LinearRegression(n_jobs=-1)\nlr_model_compl.fit(lr_train_compl, pd.concat([Y_train, Y_val]))\nprint(f\"Time required for LR with combined train\/val set: {time.time() - ts}\")","05f3b342":"pickle.dump(lr_model_compl, open(\"lr_model.pickle.dat\", \"wb\"))","439b7890":"# If needed the load the models from pickle\nlgb_model = pickle.load(open(f\"lgb_model.pickle.dat\", \"rb\"))\nrf_compl = pickle.load(open(f\"rf_model.pickle.dat\", \"rb\"))\nlr_model_compl = pickle.load(open(f\"lr_model.pickle.dat\", \"rb\"))","4793205d":"# Perform predictions on val and test sets\nlgb_pred_val = lgb_model.predict(X_val)\nrf_pred_val = rf_compl.predict(X_val)\nlr_pred_val = lr_model_compl.predict(lr_val)\n\nlgb_pred_test = lgb_model.predict(test)\nrf_pred_test = rf_compl.predict(test)\nlr_pred_test = lr_model_compl.predict(lr_test)","11e38acc":"# Create tables with prediction results\n\nfirst_level_df = pd.DataFrame({\n    'lgbm':lgb_pred_val,\n    'rf':rf_pred_val,\n    'lr':lr_pred_val[:,0],\n    'y_val':Y_val['item_cnt_month'].values\n})\n\nfirst_level_test = pd.DataFrame({\n    'lgbm':lgb_pred_test,\n    'rf':rf_pred_test,\n    'lr':lr_pred_test[:,0],\n})","3ca103e5":"# Train a LR metamodel to fit the prediction of Level1 models\nmeta_model = LinearRegression(n_jobs=-1)\nmeta_model.fit(first_level_df.iloc[:,:-1], first_level_df.iloc[:,-1])\n\nensemble_pred = meta_model.predict(first_level_df.iloc[:,:-1])\nprint('Train rmse for meta model:', np.sqrt(mean_squared_error(ensemble_pred, Y_val)))","a6a558d5":"# Create submission\ny_pred = meta_model.predict(first_level_test)\ncreate_submission_file(test, None, y_pred, name = 'stacking')","c378a895":"#FROM https:\/\/www.kaggle.com\/dromosys\/gpu-accelerated-lightgbm-full","efc94b5b":"!rm -r \/opt\/conda\/lib\/python3.6\/site-packages\/lightgbm\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM","a13e3a37":"!apt-get install -y -qq libboost-all-dev","8fa31d95":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","a7bfc59c":"!cd LightGBM\/python-package\/;python3 setup.py install --precompile","0a411ee2":"!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","2def5c18":"### Train LightGBM Model","c79e1625":"## Functions necessary to create submission file","47f392c7":"Interestingly SHAP analysis and LGBM Feature Importance do not 100% match. This is probably to the the random subset that I provided to the SHAP. ","bc38077d":"### Stacking","ec5fb87a":"# SETUP LIGHTGBM GPU (Optional)","dda39aa0":"## Create Datasets","882ef838":"## Data Preprocessing Functions","59b38ac2":"An approach is:\n1. Train LighGBM + RandomForest + LinearRegression using train\/val splits (Level 1)\n2. Evaluate models and tune parameters\n3. Make predictions on val set for all three models\n4. Combine all predictions and Y_val into one table\n5. Train Linear Regression on the table from step 4 in order to predict Y_val from 3 models (Level 2)\n6. Predict Test Set by models from level 1 and then by level 2 model.\n7. Make a submission","b839453c":"> ### Functions to work with sales data","06095c71":"## Some EDA","ff934889":"### Functions to work with item_category data","e489784b":"### Linear Regression","132a2157":"# Necessary Libraries","93f8443b":"## ML Predictions using Stacking --> LinearRegression(LightGBM + RandomForest + LinearRegression)","c1e3585c":"## Modeling Functions","3ddc5a32":"### Functions to work with shops data","17099017":"### Split the data into train\/val","d0c06f0b":"### General functions for train_df and test_df","0b2839e9":"# Functions","2ba6aa93":"### Random Forest Model","e2436952":"# SKRIPT","5f136ec0":"## Clean memory"}}