{"cell_type":{"fb148fcf":"code","aa01c297":"code","8c128ca9":"code","d1230b49":"code","e50636cd":"code","c48ab28e":"code","f6cdf50a":"code","165d3bec":"code","1d9bd45a":"code","a4d89a87":"code","93042121":"code","7d53e56a":"code","67ef7a60":"code","499236fb":"code","6cb71b2f":"code","c74cbda3":"code","1c11fed7":"code","3a037901":"code","b4d74a99":"code","18269b56":"code","09e85813":"code","11e42ad1":"code","b8769d7f":"code","f9f2f260":"code","913d8bc9":"code","bf91d3a9":"markdown","0834c52e":"markdown","224ca4f4":"markdown","63acc8a6":"markdown","f5848a7b":"markdown","c0ad7e64":"markdown","3ff764c0":"markdown","a2b2b294":"markdown","bdfec52a":"markdown","58264de5":"markdown","fd6f4a7a":"markdown","667285a0":"markdown","f47aa7f2":"markdown"},"source":{"fb148fcf":"!nvidia-smi","aa01c297":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.image as mpimg\nimport itertools\nimport seaborn as sns\nfrom IPython.display import Image\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, Flatten, MaxPool2D, BatchNormalization\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom keras.utils import plot_model\n\nsns.set(style='white', context='notebook', palette='deep')\nnp.random.seed(2)","8c128ca9":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","d1230b49":"y_train = train[\"label\"]\nX_train = train.drop(labels=[\"label\"], axis = 1)","e50636cd":"sns.countplot(y_train)","c48ab28e":"X_train.isnull().any().describe()","f6cdf50a":"test.isnull().any().describe()","165d3bec":"from keras.datasets import mnist\n(X_train1, y_train1), (X_test1, y_test1) = mnist.load_data()\nX_train1 = np.concatenate([X_train1, X_test1], axis=0)\ny_train1 = np.concatenate([y_train1, y_test1], axis=0)\n\nX_train1 = X_train1.reshape(-1, 28*28)","1d9bd45a":"X_train = X_train\/255.\nX_train1 = X_train1\/255.\ntest = test\/255.","a4d89a87":"X_train = np.concatenate((X_train.values, X_train1))\ny_train = np.concatenate((y_train, y_train1))","93042121":"X_train = X_train.reshape(-1, 28, 28, 1)\ntest = test.values.reshape(-1, 28, 28, 1)","7d53e56a":"y_train = to_categorical(y_train, num_classes = 10)","67ef7a60":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = 2)","499236fb":"print(f\"Training shape {X_train.shape}\\nValidation shape {X_val.shape}\")","6cb71b2f":"g = plt.imshow(X_train[0][:,:,0])","c74cbda3":"model = Sequential()\n\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(256, kernel_size=(3, 3),\n                 activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(512, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Dense(128))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation='softmax'))","1c11fed7":"plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\nImage('model.png')","3a037901":"optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\nmodel.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","b4d74a99":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n                                           patience=3,\n                                           verbose=1,\n                                           factor=0.2,\n                                           min_lr=0.00001)\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\ncheckpoint = ModelCheckpoint(filepath='model.h5', monitor='val_loss', mode='min', save_best_only=True, save_weights_only=True)","18269b56":"datagen = ImageDataGenerator(\n                                                    featurewise_center=False,\n                                                    samplewise_center=False,\n                                                    featurewise_std_normalization=False,\n                                                    samplewise_std_normalization=False,\n                                                    zca_whitening=False,\n                                                    rotation_range=10,\n                                                    zoom_range=0.1,\n                                                    width_shift_range=0.1,\n                                                    horizontal_flip=False,\n                                                    vertical_flip=False)\n\ndatagen.fit(X_train)","09e85813":"epochs = 50\nbatch_size = 128","11e42ad1":"history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n                             epochs=epochs,\n                             validation_data=(X_val, y_val),\n                             verbose=2,\n                             steps_per_epoch=X_train.shape[0]\/\/batch_size,\n                             callbacks=[learning_rate_reduction, es, checkpoint])","b8769d7f":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Train\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Train\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation\")\nlegend = ax[1].legend(loc='best', shadow=True)","f9f2f260":"# predict results\nresults = model.predict(test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")","913d8bc9":"# Save the final result in cnn_mnist_submission.csv\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"submission.csv\",index=False)","bf91d3a9":"## Data Preparation\n### Load Data","0834c52e":"### Set the optimizer and callbacks","224ca4f4":"### Split training and validation set","63acc8a6":"### Normalization","f5848a7b":"### Evaluate the Model","c0ad7e64":"### Check for null and missing values","3ff764c0":"### Label Encoding","a2b2b294":"### Data Augmentation","bdfec52a":"## CNN Model\n### Define the model","58264de5":"## Import Necessary Libraries","fd6f4a7a":"## Submission","667285a0":"### Add more data from mnist","f47aa7f2":"### Reshape"}}