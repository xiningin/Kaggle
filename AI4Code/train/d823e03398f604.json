{"cell_type":{"3c802770":"code","43bac727":"code","12cc726e":"code","7e290bc0":"code","1579b92b":"code","5cca8850":"code","f17a9597":"code","788e542c":"code","31e79a1f":"code","fb0b39a0":"code","1f73522e":"code","5c108d7a":"code","60d675a1":"code","919f8dcc":"code","af65ded6":"code","df22d3e5":"code","7308cd80":"code","17bfa41f":"code","5d401dd6":"code","8037d05b":"code","8c299284":"code","7268b7d8":"code","b178e51d":"code","b240621c":"code","25548a5d":"code","561a505c":"code","027a9991":"code","d282622c":"code","6466525d":"code","0b9b0b2c":"code","78341c82":"code","7bf499de":"code","5d2b6b52":"code","5da0ef5a":"code","d645818e":"code","e1389c16":"code","841ac979":"code","abf315a5":"code","b1bebd60":"code","59c274fb":"code","076bfe12":"code","4cd09d33":"code","d69cc876":"code","06410050":"markdown","1de4e79b":"markdown"},"source":{"3c802770":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","43bac727":"import os\nfrom tqdm.auto import tqdm\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\n\npd.set_option('max_column', None)","12cc726e":"pip install kds","7e290bc0":"defaultcard = pd.read_csv('\/kaggle\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv')\ndefaultcard.drop(columns='ID', inplace=True)\ndefaultcard.head()","1579b92b":"defaultcard.rename(columns={'default.payment.next.month' : 'target' , 'PAY_0' : 'PAY_1'}, inplace=True)\ndefaultcard.head(20)","5cca8850":"defaultcard.shape","f17a9597":"bill_amt = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\npay_x = ['PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']\npay_x","788e542c":"a = pd.Series(pay_x)\nprint(a[0])\ndefaultcard[defaultcard[a] == -2]","31e79a1f":"\nbill_statement_sum = defaultcard[bill_amt].sum(axis=1)\npayment = defaultcard[pay_x].sum(axis=1)\nprint(f\"number of observation with zero total bill amount and no consumption for credit card {sum(bill_statement_sum == 0) }\")\n#print(f\"number of observation no consumption for credit card {sum(payment == -12) }\")\n#defaultcard = defaultcard[defaultcard.PAY_1 != -2 and defaultcard.PAY_2 != -2 and defaultcard.PAY_3 != -2 and defaultcard.PAY_4 != -2 and defaultcard.PAY_5 != -2 and defaultcard.PAY_6 != -2  and defaultcard.BILL_AMT1 == 0 and defaultcard.BILL_AMT2 == 0 and defaultcard.BILL_AMT3 == 0 and defaultcard.BILL_AMT4 == 0 and defaultcard.BILL_AMT5 == 0 and defaultcard.BILL_AMT6 == 0 ]","fb0b39a0":"\nbill_amt = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\npay_x = ['PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']\nbill_statement_sum = defaultcard[bill_amt].sum(axis=1)\npayment = defaultcard[pay_x].sum(axis=1)\ns = pd.concat([bill_statement_sum,payment],axis=1)\nd=s[s[0]==0][s[1]==-12]\ni=d.index.array\nx=defaultcard.drop(i)\nx.reset_index()","1f73522e":"payment","5c108d7a":"#bill_statement_sum_index = bill_statement_sum.loc[bill_statement_sum > 0].index\n#defaultcard = defaultcard.loc[bill_statement_sum_index]\ndefaultcard.shape","60d675a1":"data = defaultcard.copy()","919f8dcc":"data = data.drop(data[(data['PAY_1']== -2) & (data['PAY_2']== -2)& (data['PAY_3']== -2)&(data['PAY_4']== -2)&(data['PAY_5']== -2)& (data['PAY_6']== -2) & (data['BILL_AMT1']== 0) & (data['BILL_AMT2']== 0) & (data['BILL_AMT3']== 0) & (data['BILL_AMT4']== 0) & (data['BILL_AMT5']== 0) & (data['BILL_AMT6']== 0) ].index)\ndata.shape","af65ded6":"print (data.duplicated().value_counts())\ndata.drop_duplicates(inplace=True)","df22d3e5":"data.shape","7308cd80":"data.loc[:,'EDUCATION'] = data.loc[:,'EDUCATION'].replace(0,5)\ndata.loc[:,'EDUCATION'] = data.loc[:,'EDUCATION'].replace(6,5)\ndata.loc[:,'EDUCATION'] = data.loc[:,'EDUCATION'].replace(5,4)","17bfa41f":"features_col = [col for col in data.columns if col != 'target']\nfeatures = data[features_col]\ntarget = data['target']","5d401dd6":"features_col","8037d05b":"features.head(20)","8c299284":"SEED=42\nX, X_test , y , y_test = train_test_split(features,\n                                         target,\n                                         test_size=0.2,\n                                         shuffle = True,\n                                         random_state = SEED,\n                                         stratify = target\n                                         )","7268b7d8":"y.shape[0]","b178e51d":"unique_train , count_train = np.unique(y , return_counts=True)\nprint(f\"y train value counts: \\n {np.asanyarray((unique_train, count_train\/y.shape[0])).T}\")\nprint('\\n')\nunique_test, count_test = np.unique(y_test , return_counts=True)\nprint(f\"y test vakue counts: \\n {np.asanyarray((unique_test , count_test\/y_test.shape[0])).T}\")","b240621c":"scaler = MinMaxScaler() \nscaler.fit(X)\nscaled_x = scaler.transform(X)\nscaled_test = scaler.transform(X_test)\nX.loc[:,features_col] = scaled_x\nX_test.loc[:,features_col] = scaled_test","25548a5d":"knn = KNeighborsClassifier(n_neighbors=10, algorithm='kd_tree', n_jobs=-1)\nknn.fit(X,y)\ny_pred = knn.predict(X_test)\nerror_rate_train = 1 - knn.score(X,y)\nerror_rate_valid = 1 - knn.score(X_test, y_test)\nprint(f\"KNN error rate on train set: {error_rate_train}\\n\")\nprint(f\"KNN error rate on test (valid) set: {error_rate_valid}\\n\") ","561a505c":"%%time\nfrom sklearn.metrics import accuracy_score\n\nNSPLITS = 5\nSHUFFLE = True\n\noof_label = np.zeros((X.shape[0]))\noof_proba = np.zeros((X.shape[0], 2))\ntest_proba = np.zeros((X_test.shape[0], 2))\n\nscores_valid = []\n\nskf = StratifiedKFold(n_splits=NSPLITS, shuffle=SHUFFLE, random_state=SEED)\n\nknn = KNeighborsClassifier(n_neighbors=20, algorithm='kd_tree', n_jobs=-1)\n\ncounter = 1\n\nfor trn_idx, vld_idx in skf.split(X, y):\n    print(f\"CV{counter}\")\n    xtrain, xvalid  = X.iloc[trn_idx].values, X.iloc[vld_idx].values\n    ytrain, yvalid  = y.iloc[trn_idx].values, y.iloc[vld_idx].values\n    \n    knn.fit(xtrain, ytrain)\n    \n    oof_pred_label = knn.predict(xvalid)\n    oof_pred_proba = knn.predict_proba(xvalid) # return probability estimates for the test data X\n    oof_label[vld_idx] = oof_pred_label\n    oof_proba[vld_idx] = oof_pred_proba\n    \n    \n    \n    y_pred_test = knn.predict_proba(X_test)\n    score_valid = 1 - accuracy_score(yvalid , oof_pred_label)\n    print(f\"error fold {counter} rate score {scores_valid}\")\n    scores_valid.append(score_valid)\n    test_proba += y_pred_test \/ NSPLITS\n    counter += 1 \n    print(\"\\n\")\n\ny_pred_label = test_proba.argmax(1)\nprint(f\"error rate cv score : {np.mean(np.array(scores_valid))}\")\nprint(f\"error rate test validation score : {1 - accuracy_score(y_test , y_pred_label)}\")","027a9991":"import kds\nkds.metrics.plot_cumulative_gain(y_test, test_proba[:,1])","d282622c":"#pip install -U scikit-learn","6466525d":"# logistic regression without cv","0b9b0b2c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report , confusion_matrix\nfrom sklearn.model_selection import GridSearchCV , RepeatedStratifiedKFold","78341c82":"\n#pip install -U scikit-learn","7bf499de":"#NSPLITS = 5\n#SHUFFLE = True\n#SEED=42\n\n#Logistic = LogisticRegression()\n#param_grid = [    \n#    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n#    'C' : np.logspace(-4, 4, 20),\n#    'solver' : ['newton-cg','liblinear','sag','saga'],\n#    'max_iter' : [100, 1000,2500, 5000]\n#    }\n#]\n#cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n#clf = GridSearchCV(Logistic, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n#best_clf = clf.fit(X,y)\n#best_clf.best_estimator_\n# summarize result\n#print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n#print (f'Accuracy - : {best_clf.score(X,y):.3f}')","5d2b6b52":"#logModel = LogisticRegression(n_jobs = -1)\n#param_grid ={\n#    'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n#    'solver' : ['newton-cg','liblinear','sag','saga'],\n#    'max_iter' : [100, 1000,2500, 5000]\n#}\n\n#model = GridSearchCV(\n#    estimator=logModel,\n#    param_grid=param_grid,\n#    scoring=\"accuracy\",\n#    verbose=10,\n#    n_jobs=1,\n#    cv=5\n#)\n\n#model.fit(X,y)\n#print(model.best_score_)\n#print(model.best_estimator_.get_params())\n\n######################################################\"\"\n\n#from sklearn.model_selection import GridSearchCV\n#clf = GridSearchCV(logModel, param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)\n#best_clf = clf.fit(X,y)\n#best_clf.best_estimator_\n#print (f'Accuracy - : {best_clf.score(X,y):.3f}')","5da0ef5a":"Logistic = LogisticRegression(solver='saga')\nLogistic.fit(X, y)\n\ny_pred = Logistic.predict(X_test)\nerror_rate_train_LR = 1 - Logistic.score(X,y)\nerror_rate_valid_LR = 1 - Logistic.score(X_test, y_test)\n\nprint(classification_report(y_pred, y_test))\nprint(confusion_matrix(y_pred, y_test))\n\nprint(f\"Logistic Regression error rate on train set:{error_rate_train_LR}\\n\" )\nprint(f\"Logistic Regression error rate on test set:{error_rate_valid_LR}\\n\" )","d645818e":"#hyperparameter tuning\n","e1389c16":"# logistic regression with cv","841ac979":"%%time\nfrom sklearn.metrics import accuracy_score\n\nNSPLITS = 5\nSHUFFLE = True\n\noof_label = np.zeros((X.shape[0]))\noof_proba = np.zeros((X.shape[0], 2))\ntest_proba = np.zeros((X_test.shape[0], 2))\n\nscores_valid = []\n\nskf = StratifiedKFold(n_splits=NSPLITS, shuffle=SHUFFLE, random_state=SEED)\n\nLogistic = LogisticRegression(solver='liblinear')\n\ncounter = 1\n\nfor trn_idx, vld_idx in skf.split(X, y):\n    print(f\"CV{counter}\")\n    xtrain, xvalid  = X.iloc[trn_idx].values, X.iloc[vld_idx].values\n    ytrain, yvalid  = y.iloc[trn_idx].values, y.iloc[vld_idx].values\n    \n    Logistic.fit(xtrain, ytrain)\n    \n    oof_pred_label = Logistic.predict(xvalid)\n    oof_pred_proba = Logistic.predict_proba(xvalid) # return probability estimates for the test data X\n    oof_label[vld_idx] = oof_pred_label\n    oof_proba[vld_idx] = oof_pred_proba\n    \n    \n    \n    y_pred_test = Logistic.predict_proba(X_test)\n    score_valid = 1 - accuracy_score(yvalid , oof_pred_label)\n    print(f\"error fold {counter} rate score {scores_valid}\")\n    scores_valid.append(score_valid)\n    test_proba += y_pred_test \/ NSPLITS\n    counter += 1 \n    print(\"\\n\")\n\ny_pred_label = test_proba.argmax(1)\nprint(f\"error rate cv score : {np.mean(np.array(scores_valid))}\")\nprint(f\"error rate test validation score : {1 - accuracy_score(y_test , y_pred_label)}\")","abf315a5":"# randomized search for the best C parameter\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\nlogistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,random_state=42)\ndistributions = dict(C = np.logspace(-4, 4, 20), penalty=['l1', 'l2'],max_iter = [100, 1000,2500, 5000] )\nclf = RandomizedSearchCV(logistic, distributions, random_state=42)\n\nlr_best= clf.fit(X, y)   \n\nprint(lr_best.best_params_)","b1bebd60":"%%time\nfrom sklearn.metrics import accuracy_score\n\nNSPLITS = 5\nSHUFFLE = True\n\noof_label = np.zeros((X.shape[0]))\noof_proba = np.zeros((X.shape[0], 2))\ntest_proba = np.zeros((X_test.shape[0], 2))\n\nscores_valid = []\n\nskf = StratifiedKFold(n_splits=NSPLITS, shuffle=SHUFFLE, random_state=SEED)\n\nLogistic = LogisticRegression(penalty =\"l1\",solver='liblinear',C= 3.329770563201687,max_iter=1000)\n\ncounter = 1\n\nfor trn_idx, vld_idx in skf.split(X, y):\n    print(f\"CV{counter}\")\n    xtrain, xvalid  = X.iloc[trn_idx].values, X.iloc[vld_idx].values\n    ytrain, yvalid  = y.iloc[trn_idx].values, y.iloc[vld_idx].values\n    \n    Logistic.fit(xtrain, ytrain)\n    \n    oof_pred_label = Logistic.predict(xvalid)\n    oof_pred_proba = Logistic.predict_proba(xvalid) # return probability estimates for the test data X\n    oof_label[vld_idx] = oof_pred_label\n    oof_proba[vld_idx] = oof_pred_proba\n    \n    \n    \n    y_pred_test = Logistic.predict_proba(X_test)\n    score_valid = 1 - accuracy_score(yvalid , oof_pred_label)\n    print(f\"error fold {counter} rate score {scores_valid}\")\n    scores_valid.append(score_valid)\n    test_proba += y_pred_test \/ NSPLITS\n    counter += 1 \n    print(\"\\n\")\n\ny_pred_label = test_proba.argmax(1)\nprint(f\"error rate cv score : {np.mean(np.array(scores_valid))}\")\nprint(f\"error rate test validation score : {1 - accuracy_score(y_test , y_pred_label)}\")","59c274fb":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import plot_confusion_matrix\n\nTree_Classifier = DecisionTreeClassifier(criterion='entropy', max_depth=10)\n\nTree_Classifier.fit(X, y)\n\nTree_Classifier_acc = Tree_Classifier.score(X_test, y_test)*100\n\nprint('Tree_Classifier Accuracy score is :', Tree_Classifier_acc)\n\nprint( classification_report(y_test, Tree_Classifier.predict(X_test)) )\n\nplot_confusion_matrix(Tree_Classifier, X_test, y_test, cmap=\"Blues_r\")\n\nplt.show()","076bfe12":"from sklearn.ensemble import RandomForestClassifier # Model 2\nrfc = RandomForestClassifier(max_depth=10)\n\nrfc.fit(X, y)\n\nrfc_acc = rfc.score(X_test, y_test)*100\n\nprint('Model3 Accuracy score is :', rfc_acc)\n\nprint( classification_report(y_test, rfc.predict(X_test)) )\n\nplot_confusion_matrix(rfc, X_test, y_test, cmap=\"Blues_r\")\n\nplt.show()","4cd09d33":"rfc = RandomForestClassifier(max_depth=10)\nparam_grid = {\n    \"n_estimators\" : [100, 200, 300, 400],\n    \"max_depth\" : [1, 3, 5, 7],\n    \"criterion\" : [\"gini\",\"entropy\"],\n}\n\nmodel = GridSearchCV(\n    estimator=rfc,\n    param_grid=param_grid,\n    scoring=\"accuracy\",\n    verbose=10,\n    n_jobs=1,\n    cv=5,\n)\nmodel.fit(X ,y)\nprint(model.best_score_)\nprint(model.best_estimator_.get_params())","d69cc876":"from sklearn.ensemble import RandomForestClassifier # Model 2\nrfc = RandomForestClassifier(max_depth=7,n_estimators=400,criterion='gini',verbose=0)\n\nrfc.fit(X, y)\n\nrfc_acc = rfc.score(X_test, y_test)*100\n\nprint('Model3 Accuracy score is :', rfc_acc)\n\nprint( classification_report(y_test, rfc.predict(X_test)) )\n\nplot_confusion_matrix(rfc, X_test, y_test, cmap=\"Blues_r\")\n\nplt.show()","06410050":"### Pr\u00e9paration des donn\u00e9es :","1de4e79b":"### predict without CV"}}