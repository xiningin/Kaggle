{"cell_type":{"5905d332":"code","ed00b849":"code","72481809":"code","56d4c56f":"code","5f1826da":"code","552d8221":"code","56cd12d0":"code","38f5e6c6":"code","5f712c50":"code","6d58038c":"code","503cb5c9":"code","9980ea32":"code","efb8def8":"code","39beefed":"code","7a845850":"code","410321eb":"code","2a7f3180":"code","1a76e464":"code","d5341260":"code","7cf0f4c4":"code","61e88c90":"code","372b6008":"code","f8d86473":"code","0ce471d7":"code","7e39765b":"code","a0405874":"code","0e8ceb36":"code","513a5698":"code","00d5f8f3":"code","ee0025cb":"code","56b7780c":"code","622b0f10":"code","99b54abb":"code","84c2daf4":"code","42c8b51f":"code","54773b7c":"code","17500897":"code","486c8ff2":"code","cbcddf05":"code","656e0c5b":"code","70d5f053":"code","34968106":"code","7c90e916":"code","a21dac16":"code","d99b46c4":"code","411050c1":"code","01c6c840":"code","bc3e8d0a":"code","1819bed9":"code","fd04c42d":"code","c2996227":"code","acb98f80":"code","22b35c74":"code","302509cf":"code","f9743549":"code","3ceba27d":"code","08561294":"code","255d7563":"code","f87c9ead":"code","04d1fbfa":"code","273b9fca":"code","ae6e0bf9":"code","14e28e5d":"code","a5633583":"code","873ecffe":"code","a92c8858":"code","b6b1b37d":"code","fd9c9427":"code","046d8757":"code","6f8c6a99":"code","74385a84":"code","5fa9d853":"code","d75d5c80":"code","eda9340d":"code","ca121ba0":"code","b0b21739":"code","3a17c525":"code","21265259":"code","1643aaa3":"code","e08ff58a":"code","a7810d90":"code","544e6096":"markdown","57f92838":"markdown","9e4fc70b":"markdown","0e2efd76":"markdown","5347cf7b":"markdown","53db68ac":"markdown","3288f62f":"markdown","8def0b3e":"markdown","e852f456":"markdown","edc6c04a":"markdown","3d6b23f9":"markdown","645755c2":"markdown","69ad823f":"markdown","2b819e7f":"markdown","305eb78d":"markdown","94455c13":"markdown","c4324c3d":"markdown","efb8bb0e":"markdown","6cf08abd":"markdown","b938893f":"markdown","483d47e5":"markdown","42d8c256":"markdown","2df7efd4":"markdown","a313d01a":"markdown","279d4d2a":"markdown","d23dd146":"markdown","519b92b7":"markdown","b5c7303c":"markdown"},"source":{"5905d332":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n","ed00b849":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72481809":"cust_data = pd.read_csv('\/kaggle\/input\/ccdata\/CC GENERAL.csv')","56d4c56f":"cust_data.tail()","5f1826da":"cust_data.info()","552d8221":"#convert column names to lower case for easy interpretation\ncust_data.columns = cust_data.columns.str.lower()","56cd12d0":"cust_data.columns","38f5e6c6":"cust_data.head()","5f712c50":"## Create important derived variables\n\ndef month_avg_purchase(prch, tenure):\n    return prch\/tenure\n\ndef month_cash_advance(cash, tenure):\n    return cash\/tenure\n\ndef monthly_usage(blnc, limit):\n    return blnc\/limit\n\ndef prch_type(x, y):\n    if ((x <= 0) & (y <= 0)):\n        return 'none'\n    elif((x > 0) & (y <= 0)):\n        return 'one_off'\n    elif((x <= 0) & (y > 0)):\n        return 'installments'\n    elif((x > 0) & (y > 0)):\n        return 'both'\n        ","6d58038c":"cust_data['monthly_avg_purchase'] = cust_data.apply(lambda x : month_avg_purchase(x['purchases'], x['tenure']), axis = 1)\ncust_data['monthly_cash_advance'] = cust_data.apply(lambda x : month_cash_advance(x['cash_advance'], x['tenure']), axis = 1)\ncust_data['monthly_usage'] = cust_data.apply(lambda x : monthly_usage(x['balance'], x['credit_limit']), axis = 1)\ncust_data['purchase_type'] = cust_data.apply(lambda x : prch_type(x['oneoff_purchases'], x['installments_purchases']), axis = 1)\n","503cb5c9":"cust_data.head()","9980ea32":"##Renaming the columns for better undersatnding\n#purchases_frequency - freq of months wth atleast 1 purcase\n#balance_frequency - balance in last 12months\/ balance\n\ncust_data.rename(columns = {'balance' : 'avg_monthly_balance'}, inplace=True)","efb8def8":"cust_data.head()","39beefed":"import pandas_profiling\ncust = cust_data.profile_report()\n#cust.to_file(output_file = 'cust_segmentation_profile.html')","7a845850":"cust","410321eb":"cust_data.info()","2a7f3180":"numeric_var_names=[key for key in dict(cust_data.dtypes) if dict(cust_data.dtypes)[key] in ['float64', 'int64', 'float32', 'int32']]\ncat_var_names=[key for key in dict(cust_data.dtypes) if dict(cust_data.dtypes)[key] in ['object']]\nprint(numeric_var_names)\nprint(cat_var_names)","1a76e464":"cust_data.isna().sum()","d5341260":"def continous_var_summary(x):\n    return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),x.std(),\n                      x.var(), x.min(), x.quantile(0.01), x.quantile(0.05), x.quantile(0.10), x.quantile(0.25),\n                      x.quantile(0.50), x.quantile(0.75), x.quantile(0.90), x.quantile(0.95), \n                      x.quantile(0.99), x.max()],\n                    index = ['N', 'NMiss', 'Sum', 'Mean', 'Median', 'SD', 'Var', 'Min', 'P1', 'P5', 'P10','P25',\n                            'P50', 'P75', 'P90', 'P95', 'P99', 'Max'])","7cf0f4c4":"def categorical_var_summary(x):\n    Mode = x.value_counts().sort_values(ascending = False)[0:1].reset_index()\n    return pd.Series([x.count(), x.isnull().sum(), Mode.iloc[0, 0], Mode.iloc[0,1],\n                     round((Mode.iloc[0,1]*100)\/x.count(), 2)],\n                     index = ['N', 'NMiss', 'Mode', 'Freq', 'Percent'])","61e88c90":"cust_data[numeric_var_names].apply(lambda x : continous_var_summary(x)).T.round(1)","372b6008":"cust_data[cat_var_names].apply(lambda x : categorical_var_summary(x)).T.round(1)","f8d86473":"#Handling Outliers - at 99%tile or 95%tile if required \ndef outlier_capping(x):\n    x = x.clip(upper=x.quantile(0.95))\n    x = x.clip(lower=x.quantile(0.05))\n    return x","0ce471d7":"cust_data[numeric_var_names]=cust_data[numeric_var_names].apply(lambda x: outlier_capping(x))","7e39765b":"#Handling missings \ndef Missing_imputation(x):\n    x = x.fillna(x.mean())\n    return x","a0405874":"cust_data[numeric_var_names]=cust_data[numeric_var_names].apply(lambda x: Missing_imputation(x))","0e8ceb36":"cust_data[numeric_var_names].apply(lambda x : continous_var_summary(x)).round(3).T","513a5698":"### Correlation Matrix","00d5f8f3":"cust_corr = cust_data.corr()\n#cust_corr.to_excel('cust_corr.xlsx')\nsns.heatmap(cust_corr)","ee0025cb":"#function for creating dummy variable\ndef create_dummies(df, colname):\n    col_dummies = pd.get_dummies(df[colname], prefix = colname, drop_first= True)\n    df = pd.concat([df, col_dummies], axis = 1)\n    df.drop(colname, axis = 1, inplace= True)\n    return df","56b7780c":"cat_var_names = cust_data[cat_var_names].columns.difference(['cust_id'])\ncat_var = cust_data[cat_var_names]\ncat_var.head()","622b0f10":"for c_feature in cat_var_names:\n    cat_var[c_feature] = cat_var[c_feature].astype('category')\n    cat_var = create_dummies(cat_var, c_feature)","99b54abb":"cat_var.head()","84c2daf4":"#As cust_id is unique and have no variance so it may add unecessary noise to our data. Hence we need to drop it\n\ncust_data.drop(columns=['cust_id'], inplace=True)","42c8b51f":"data_final = pd.concat([cust_data[numeric_var_names], cat_var], axis = 1)","54773b7c":"data_final.head()","17500897":"from sklearn.preprocessing import StandardScaler","486c8ff2":"data_final.columns","cbcddf05":"#prescreening of variables to remove less useful vraiable for segmentation\ndata_feature = data_final.drop(columns=['purchases', 'cash_advance'], axis = 1)","656e0c5b":"sc = StandardScaler()","70d5f053":"data_final_scaled = pd.DataFrame(sc.fit_transform(data_feature))","34968106":"data_final_scaled.head()","7c90e916":"from sklearn.decomposition import PCA","a21dac16":"pca = PCA(n_components=21)\npca.fit(data_final_scaled)","d99b46c4":"pca.explained_variance_ ","411050c1":"#The amount of variance that each PC explains\nvar = pca.explained_variance_ratio_\nvar","01c6c840":"#cummilative var explained\nvar1 = np.cumsum(np.round(pca.explained_variance_ratio_ , decimals=4)*100)\nvar1","bc3e8d0a":"pd.DataFrame({'Eigen Values' : pca.explained_variance_, 'Cumulative Variance' : var1}, index=range(1,22))","1819bed9":"pca_final = PCA(n_components=7).fit(data_final_scaled)","fd04c42d":"pca_final.explained_variance_","c2996227":"reduced_cr = pca_final.fit_transform(data_final_scaled)","acb98f80":"dimensions = pd.DataFrame(reduced_cr)\ndimensions.columns = [\"C1\", \"C2\", \"C3\", \"C4\", \"C5\",\"C6\", \"C7\"]","22b35c74":"print(dimensions.shape)\nprint(dimensions.head())","302509cf":"from sklearn.cluster import KMeans","f9743549":"km_3 = KMeans(n_clusters=3, random_state=123)\nkm_3.fit(dimensions)","3ceba27d":"km_4 = KMeans(n_clusters=4, random_state=123).fit(dimensions)\nkm_5 = KMeans(n_clusters=5, random_state=123).fit(dimensions)\nkm_6 = KMeans(n_clusters=6, random_state=123).fit(dimensions)\nkm_7 = KMeans(n_clusters=7, random_state=123).fit(dimensions)\nkm_8 = KMeans(n_clusters=8, random_state=123).fit(dimensions)\nkm_9 = KMeans(n_clusters=9, random_state=123).fit(dimensions)","08561294":"# save the cluster labels and sort by cluster\ndata_final['cluster_3'] = km_3.labels_\ndata_final['cluster_4'] = km_4.labels_\ndata_final['cluster_5'] = km_5.labels_\ndata_final['cluster_6'] = km_6.labels_\ndata_final['cluster_7'] = km_7.labels_\ndata_final['cluster_8'] = km_8.labels_\ndata_final['cluster_9'] = km_9.labels_","255d7563":"data_final.head(10)","f87c9ead":"data_final['cluster_3'].value_counts()","04d1fbfa":"data_final['cluster_3'].value_counts()\/sum(data_final['cluster_3'].value_counts())","273b9fca":"data_final['cluster_4'].value_counts()\/sum(data_final['cluster_4'].value_counts())","ae6e0bf9":"data_final['cluster_5'].value_counts()\/sum(data_final['cluster_5'].value_counts())","14e28e5d":"data_final['cluster_6'].value_counts()\/sum(data_final['cluster_6'].value_counts())","a5633583":"from sklearn import  metrics","873ecffe":"metrics.silhouette_score(dimensions, labels=km_3.labels_)","a92c8858":"k_range = range(2, 16)\nscores = []\nfor k in k_range:\n    km = KMeans(n_clusters=k, random_state=123)\n    km.fit(dimensions)\n    scores.append(metrics.silhouette_score(dimensions, labels=km.labels_))","b6b1b37d":"scores","fd9c9427":"plt.plot( k_range, scores)\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score')\nplt.grid('True')","046d8757":"cluster_range= range(2, 20)\nerrors = []\n\nfor num_clusters in cluster_range:\n    clusters = KMeans( num_clusters )\n    clusters.fit(dimensions)\n    errors.append(clusters.inertia_)","6f8c6a99":"errors","74385a84":"clusters_df = pd.DataFrame({'Cluster_no' : range(2, 20), 'Unexpalined_variance' : errors})\nclusters_df[0:10]","5fa9d853":"%matplotlib inline\nplt.plot(clusters_df.Cluster_no, clusters_df.Unexpalined_variance, marker = 'o')\nplt.xlabel('No of clusters')\nplt.ylabel('Unexplained Variance(error)')\nplt.grid('True')\n","d75d5c80":"data_final.head()","eda9340d":"size = pd.concat([pd.Series(data_final.cluster_3.size), pd.Series(data_final.cluster_3.value_counts()).sort_index(), pd.Series(data_final.cluster_4.value_counts()).sort_index(),\n          pd.Series(data_final.cluster_5.value_counts()).sort_index(), pd.Series(data_final.cluster_6.value_counts()).sort_index(), pd.Series(data_final.cluster_7.value_counts()).sort_index(),\n          pd.Series(data_final.cluster_8.value_counts()).sort_index(), pd.Series(data_final.cluster_9.value_counts()).sort_index()])\nsize","ca121ba0":"Seg_size = pd.DataFrame(size, columns=['seg_size'])\nSeg_pct = pd.DataFrame(size\/data_final.cluster_3.size, columns= ['Seg_pct'])","b0b21739":"pd.concat([Seg_size.T, Seg_pct.T], axis = 0)","3a17c525":"# Mean value gives a good indication of the distribution of data. So we are finding mean value for each variable for each cluster\nProfiling_output = pd.concat([data_final.apply(lambda x: x.mean()).T, data_final.groupby('cluster_3').apply(lambda x : x.mean()).T,\n                             data_final.groupby('cluster_4').apply(lambda x : x.mean()).T, data_final.groupby('cluster_5').apply(lambda x : x.mean()).T, \n                             data_final.groupby('cluster_6').apply(lambda x : x.mean()).T, data_final.groupby('cluster_7').apply(lambda x : x.mean()).T,\n                             data_final.groupby('cluster_8').apply(lambda x : x.mean()).T, data_final.groupby('cluster_9').apply(lambda x : x.mean()).T], axis =1)","21265259":"Profiling_output","1643aaa3":"Profiling_output_final=pd.concat([Seg_size.T, Seg_pct.T, Profiling_output], axis=0)\nProfiling_output_final.columns = ['Overall', 'KM3_1', 'KM3_2', 'KM3_3',\n                                'KM4_1', 'KM4_2', 'KM4_3', 'KM4_4',\n                                'KM5_1', 'KM5_2', 'KM5_3', 'KM5_4', 'KM5_5',\n                                'KM6_1', 'KM6_2', 'KM6_3', 'KM6_4', 'KM6_5','KM6_6',\n                                'KM7_1', 'KM7_2', 'KM7_3', 'KM7_4', 'KM7_5','KM7_6','KM7_7',\n                                'KM8_1', 'KM8_2', 'KM8_3', 'KM8_4', 'KM8_5','KM8_6','KM8_7','KM8_8',\n                                'KM9_1', 'KM9_2', 'KM9_3', 'KM9_4', 'KM9_5','KM9_6','KM9_7','KM9_8', 'KM9_9']","e08ff58a":"Profiling_output_final","a7810d90":"Profiling_output_final.to_csv('Profiling_output1.csv')","544e6096":"# Standardizing the data","57f92838":"#### 1. Outlier treatment","9e4fc70b":"1. purchases with one off & monthly_avg_purchase\n2. one-off with monthly_avg_purchase\n3. cahs_adv wth monthly_cash advance","0e2efd76":"# Clustering model(k-means)","5347cf7b":"# Qualitative Analysis(Profiling)","53db68ac":"We can drop purchases column and cash_adv based on our finding from correlation matrix","3288f62f":"# Data Cleaning","8def0b3e":"From here we can see that Purchases is highly correlated with one-off purchases variable","e852f456":"#### 4. Dropping unecessary variables","edc6c04a":"### 1. Silhouette Coefficient(Higher the better)","3d6b23f9":"## Profiling","645755c2":"# Business Problem","69ad823f":"Here after cluster 7 the incremental decrease in error is almost constant","2b819e7f":"Here 5 cluter seems optimal solution as the sc score is highest around it.","305eb78d":"either 7 or 8 seems a perfect candidate for no of componenets in our clustering model","94455c13":"#### 3. Dummy variable creation","c4324c3d":"Here we can choose 5-6 cluster solution as optimum solution","efb8bb0e":"# Quantitative Evaluation of model","6cf08abd":"### Combining numeric and categorical data","b938893f":"This case requires to develop a customer segmentation to define marketing strategy. The sample dataset summarizes the usage behavior of about 9000 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables.","483d47e5":"### Segment Size check","42d8c256":"# Applying PCA to reduce the variables","2df7efd4":"From profiling we conclude that *__8 cluster solution__* seems the best one. Below is the detailed characteristic description of each cluster for future marketing strategy-\n\n**Cluster 0** - These include customers with average credit limit who are mostly involved in one off type of purchases with their credit cards. They dont prefer cash transactions on their cards. They maybe targeted for offers on different partner outlets.\n\n**Cluster 1** - These are the customers with high credit limit who spend alot on purchases of both installment and one-off type. Amount and number of transactions are quite high for these card holders. As a result the balance is quite low for them.\n\n**Cluster 2** -  This cluster targets a group of customers who have a high balance and cash advances with low purchase frequency. We can assume that this customer segment uses their credit cards as a loan facility.\n\n**Cluster 3** - This cluster includes uninvolved customers which rarely use their cards and that also on small amount of purchases. Hence they have low minimun payments inspite of decent credit limit. We may target them to diffrenet market strategies like emi\/installments purchases.\n\n**Cluster 4** - These customers are similar to cluster no. 2 but with lower balance and lower credit limit.\n\n**Cluster 5** - These customers purchase frequently with highest amount of installment purchases contrast of a lower cash advance percentage. They have lower credit limit maybe that is the reason for not spending on other type of services. Also they pay their bill on time compared to other customers.\n\n**Cluster 6** - These are the customers who frequently use all the services with high amount whether it be any kind of purchase or cash transactions. They have the highest credit limit and minimun payment. In short these are the involved customers.\n\n**Cluster 7** -  These customers are almost similar to cluster no. 5 but with higher minimum payment and they don't pay their bill on time.","a313d01a":"#### 2. Missing value imputation","279d4d2a":"### 2. Elbow Analysis","d23dd146":"## Data audit report","519b92b7":"# Data Inspection","b5c7303c":"### Identify Categorical and continous variables"}}