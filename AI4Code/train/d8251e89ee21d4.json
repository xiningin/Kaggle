{"cell_type":{"ab0b8fc0":"code","0b2e857d":"code","1dc1aaea":"code","e401e932":"code","bf3c3d1a":"code","04260816":"code","9705fb89":"code","db4cc7c8":"code","161834f8":"code","a9787d31":"code","29e7c334":"code","4c1b0ee3":"code","21ecf26e":"code","fffa2d2d":"code","d53fb378":"code","619fb397":"code","76d33184":"code","bd4bfcfd":"code","b46d36d5":"code","5281ad93":"code","1fe44316":"code","d9172c48":"code","f8ced1b8":"code","98807ac8":"markdown"},"source":{"ab0b8fc0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0b2e857d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","1dc1aaea":"df = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf","e401e932":"df.info()","bf3c3d1a":"df['output'].value_counts()","04260816":"sns.countplot(x='output',data=df)","9705fb89":"df.corr()","db4cc7c8":"sns.pairplot(df)","161834f8":"df.corrwith(df['output'])","a9787d31":"sns.displot(df['age'])","29e7c334":"sns.distplot(df['output'])","4c1b0ee3":"for i in df.columns:\n    sns.displot(df[i])","21ecf26e":"X = df.drop(['output'],axis=1)\nX","fffa2d2d":"y = df['output']\ny","d53fb378":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2)","619fb397":"X_train","76d33184":"from sklearn.linear_model import LogisticRegression\nlogregressor = LogisticRegression()\nlogregressor.fit(X_train,y_train)","bd4bfcfd":"predictions = logregressor.predict(X_test)","b46d36d5":"predictions","5281ad93":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))","1fe44316":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(n_estimators=100)\nforest.fit(X_train,y_train)","d9172c48":"predictions_forest = forest.predict(X_test)","f8ced1b8":"print(classification_report(y_test,predictions_forest))\nprint(confusion_matrix(y_test,predictions_forest))","98807ac8":"This proves that Logistic Regression gives the best possible accuracy due to it's sigmoid function and on the other hand ensemble algorithms seems to be found wanting"}}