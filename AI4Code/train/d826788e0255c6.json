{"cell_type":{"b501bbd9":"code","06ed86d3":"code","dc3022bb":"code","be40175f":"code","fa6c6db7":"code","a717f2e2":"code","d245b324":"code","c21e4a00":"code","33f59297":"code","4486e619":"code","839f962a":"code","f7f83a23":"code","3f2428b7":"code","9efb28e3":"code","22f395ad":"markdown","a095ef0d":"markdown","6cf3a934":"markdown","9b0f97fb":"markdown","c4ae050c":"markdown","72413325":"markdown","7a360f00":"markdown"},"source":{"b501bbd9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import Callback\n\nfrom sklearn.model_selection import train_test_split\n\nimport string\n\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\n\nimport plotly\nplotly.offline.init_notebook_mode(connected=True)\nimport plotly.graph_objects as go\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","06ed86d3":"legit, fake = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv'), pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')\nlegit.sample(10)","dc3022bb":"legit['target'] = 1\nfake['target'] = 0\ndata = pd.concat([legit, fake], axis=0)\ndata.sample(10)","be40175f":"data.isnull().sum()","fa6c6db7":"# Hyperparameters for title and text\nvocab_size = 100000\nembedding_dim_title = 128\nmax_length_title = 40\nembedding_dim_text = 500\nmax_length_text = 500\ntrunc_type = 'post'\npadding_type = 'post'\noov_tok = '<OOV>'\ntest_ratio = .3\nembedding_dim = 500\nmax_length_text = 500","a717f2e2":"# detect and init the TPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n\n    # instantiate a distribution strategy\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    TPU_EXIST = True\nexcept Exception as e:\n    print(e)\n    TPU_EXIST = False","d245b324":"# Text cleaning\ndef clean(text):\n    #1. Remove punctuation\n    translator1 = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n    text = text.translate(translator1)\n    \n    #2. Convert to lowercase characters\n    text = text.lower()\n    \n    #3. Remove stopwords\n    text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n    \n    return text","c21e4a00":"# Apply cleaning to title and text in dataset\ndata['title'] = data['title'].apply(clean)\ndata['text'] = data['text'].apply(clean)\ndata.sample(10)","33f59297":"def preprocessing(data, dependent_column=None, target='target', max_len=40):\n    train_X, test_X, train_y, test_y = train_test_split(data[dependent_column], data[target], test_size=test_ratio)\n    tokenizer = Tokenizer(num_words=vocab_size,\n                          oov_token=oov_tok)\n    tokenizer.fit_on_texts(train_X)\n    train_sequences = tokenizer.texts_to_sequences(train_X)\n    train_padded = pad_sequences(train_sequences, maxlen=max_len,\n                                padding=padding_type,\n                                truncating=trunc_type)\n    test_sequences = tokenizer.texts_to_sequences(test_X)\n    test_padded = pad_sequences(test_sequences, maxlen=max_len,\n                               padding=padding_type,\n                               truncating=trunc_type)\n    return train_padded, test_padded, train_y, test_y","4486e619":"# Create the model\ndef model_creation(vocab_size=vocab_size, embedding_dim=128):\n    if TPU_EXIST:\n        with tpu_strategy.scope():\n            model = tf.keras.Sequential()\n            model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim))\n            model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)))\n            model.add(tf.keras.layers.Dense(embedding_dim, activation='relu'))\n            model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n            model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n    else:\n        model = tf.keras.Sequential()\n        model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim))\n        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)))\n        model.add(tf.keras.layers.Dense(embedding_dim, activation='relu'))\n        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n    return model","839f962a":"def train_model(model, train_X, train_Y, test_X, test_Y, epochs):\n    class CustomCallback(Callback):\n        def on_epoch_end(self, epoch, logs={}):\n            if logs.get('acc') > 0.99:\n                print(f'Accuracy reached {logs.get(\"acc\")*100:0.2f}. Stopping the training')\n                self.model.stop_training = True\n\n    history = model.fit(train_X, train_Y,\n                       epochs=epochs,\n                       batch_size=64,\n                       validation_data=[test_X, test_Y],\n                       callbacks=[CustomCallback()])\n    return history","f7f83a23":"train_padded, test_padded, train_y, test_y = preprocessing(data, dependent_column='title', max_len=max_length_title)\nmodel = model_creation(embedding_dim=embedding_dim_title)\nhistory_title = train_model(model, train_padded, train_y, test_padded, test_y, 15)","3f2428b7":"train_padded, test_padded, train_y, test_y = preprocessing(data, dependent_column='text', max_len=max_length_title)\nmodel = model_creation(embedding_dim=embedding_dim_text)\nhistory_text = train_model(model, train_padded, train_y, test_padded, test_y, 15)","9efb28e3":"title_max_acc = max(history_title.history.get('acc'))\ntext_max_acc = max(history_text.history.get('acc'))\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=['Title', 'Text'],\n                        y=[title_max_acc,\n                          text_max_acc],\n                        mode='lines+markers',\n                        name='Accuracies of Models'))\nfig.update_layout(title='Accuracies Differences',\n                 xaxis_title='Case Name',\n                 yaxis_title='Accuracy of Model')\nfig.show()","22f395ad":"## Plotting accuracies of above two cases","a095ef0d":"## Case 1: Using news title to train and validate predictor model\n\nIn this case, we will use news title as depended text to predict the variable target, which denotes the legitimacy of the news article.","6cf3a934":"## Data Cleaning\n\nData cleaning techniques consist of applying various transfromations to data, in our case textual data. Some of the procedures include:\n* Removing unwanted characters which include punctuation marks.\n* Converting all characters to lowercase\n* Removing stopwords from the text","9b0f97fb":"## Case 2: Using news text to train and validate predictor model\n\nIn other case, we will use news title as depended text to predict the variable target, which denotes the legitimacy of the news article.","c4ae050c":"### Long Short Term Memory model\n\nLong Short-Term Memory networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. \n\n> Hence standard RNNs fail to learn in presence of time lags greater than 5-10 discrete time steps between relevant input events and target signals. The vanishing error problem casts doubt on whether standard RNNs can indeed exhibit significant practical advantages over time window-based feedforward networks. A recent model, \u201cLong Short-Term Memory\u201d (LSTM), is not affected by this problem. LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error flow through \u201cconstant error carrousels\u201d (CECs) within special units, called cells.\n\n- Felix A. Gers, et al., ([Learning to Forget: Continual Prediction with LSTM](http:\/\/www.mitpressjournals.org\/doi\/abs\/10.1162\/089976600300015015), 2000\n","72413325":"## Data preprocessing, model creation and testing functions","7a360f00":"### Tokenization and sequence padding\n\nTokenization is the process of splitting sentences into tokens which represent specific words. Furthermore, the length of sentences is different for different texts which need to be padded to same length sequences. Machine learning models are designed to work on static length of input data and hence padding method is used for input sequences to model."}}