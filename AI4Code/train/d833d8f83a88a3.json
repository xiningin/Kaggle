{"cell_type":{"a36e5231":"code","a4b307c6":"code","c786812f":"code","275c99c6":"code","314e99d9":"code","3d0eaadc":"code","0dc6ef14":"code","6babeee4":"code","3a04a8e5":"code","dc94b7a4":"code","4ecdd5ef":"code","100ccbe0":"code","8bb08c3f":"code","336d181b":"code","b3db4349":"code","f9bbe941":"code","b94ec9be":"code","d9e5440e":"code","f4bd6495":"code","5f578b26":"code","427d2bb0":"markdown","dc00495e":"markdown","75fdf899":"markdown","d53057f0":"markdown","df2a582a":"markdown","6ff860a6":"markdown","fb66361a":"markdown","74934cc0":"markdown","50e7dd08":"markdown","6ab8ac59":"markdown","16498136":"markdown","608750a2":"markdown","17ad4cd5":"markdown","4977645d":"markdown","559338af":"markdown"},"source":{"a36e5231":"%load_ext cython","a4b307c6":"import glob\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import accuracy_score","c786812f":"def download():\n    \n    import os, sys\n\n    print('Create folder')\n    path = \".\/Fonts\"\n\n    os.mkdir( path )\n    path = \".\/Fonts\/fonts\"\n\n    os.mkdir( path )\n    os.chdir(path)\n\n    import urllib.request\n\n    print('Download Dataset')\n\n    url = 'https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00417\/fonts.zip'  \n    urllib.request.urlretrieve(url, 'fonts.zip')\n    \n    import zipfile\n    zip_ref = zipfile.ZipFile('.\/fonts.zip', 'r')\n    zip_ref.extractall('.\/')\n    zip_ref.close()\n    os.chdir('..\/..')\n    \n    print('Finished!')\n    ","275c99c6":"def allinone():\n    path =r'.\/Fonts\/fonts' \n    allFiles = glob.glob(path + \"\/*.csv\")\n    first=allFiles.pop(0)\n    df = pd.read_csv(first,index_col=None, header=0) \n    #label=df.iloc[:,0].values\n    #feature=df.iloc[:,3:411].values.astype(np.float16)\n    #arr=np.concatenate((label,feature),axis=1)\n    #arr=df.values\n    arry=df.iloc[:,0].values\n    arrX=df.iloc[:,3:411].values.astype(np.float16)\n    for file in allFiles:\n        df= pd.read_csv(file,index_col=None, header=0,low_memory=True)\n        #arr=np.concatenate((arr,df.values.astype(np.float16)),axis=0)\n        arry=np.concatenate((arry,df.iloc[:,0].values),axis=0)\n        arrX=np.concatenate((arrX,df.iloc[:,3:411].values.astype(np.float16)),axis=0)\n        #label=df.iloc[:,0].values\n        #feature=df.iloc[:,3:411].values.astype(np.float16)\n        #font=np.concatenate((label,feature),axis=1)\n        #arr=np.concatenate((arr,font),axis=0)\n    np.save('allX',arrX)\n    np.save('ally',arry)\n    #np.save('all',arr)\n    print('Data was saved into allX, ally.')","314e99d9":"#download()\n#allinone()","3d0eaadc":"y=np.load('.\/ally.npy')\nX=np.load('.\/allX.npy')\nprint('Data Loaded!')","0dc6ef14":"'''\nfor i in range(X.shape[1]):\n    q1= np.percentile(X[:,i], 25)\n    q3= np.percentile(X[:,i], 75)\n    iqr=q3-q1\n    upperbound=q3+(1.5*iqr)\n    lowerbound=q1-(1.5*iqr)\n    X = X[~((X[:,i]< lowerbound) |(X[:,i] > upperbound))]\nX.shape'''\nprint('output size: (8,408)!!!')","6babeee4":"lb=LabelBinarizer()\nbinlabels=lb.fit_transform(y)\nprint(binlabels.shape)","3a04a8e5":"classname,classid = np.unique(y, return_inverse=True)\nprint(classid.shape)","dc94b7a4":"trainX, testX, trainy, testy = train_test_split(X, binlabels, test_size=0.2)\nX=None\nY=None\n\nprint(testX.shape)\nprint(trainX.shape)\n\nprint(testy.shape)\nprint(trainy.shape)\n","4ecdd5ef":"trainknnX, testknnX, trainknny, testknny = train_test_split(X, classid, test_size=0.2)\nX=None\nY=None\n\nprint(testknnX.shape)\nprint(trainknnX.shape)\n\nprint(testknny.shape)\nprint(trainknny.shape)","100ccbe0":"%%cython\ncimport numpy as np\nimport numpy as np\nfrom collections import Counter\nfrom operator import itemgetter\n\ncdef class knn():\n    cpdef double dist(self,double[:] x):\n        return np.linalg.norm(x)\n    \n    cpdef set neighbors(self,double[:,:] trainX,double[:] test):\n        cdef dists=[]\n        cdef int i=0\n        for i in range(trainX.shape[0]):\n            #print(trainX.shape[1])\n            dists.append((i,self.dist(np.subtract(test,trainX[i,:]))))\n        #print(sorted(dist))\n        return set(dists)\n    \n    cpdef int vote(self,list arr,double[:] trainY):\n    \n        cdef ys=[]\n        cdef int i=0\n        for i in range(len(arr)):\n            tup=arr[i]\n            ys.append(trainY[tup[0]])\n        cdef cnt = Counter(ys)\n        #print(cnt)\n        #return(cnt)\n        return(cnt.most_common()[0][0] )\n\n    cpdef int fit(self,double[:,:] trainX,double[:] trainY,double[:] test,int k):\n        cdef a=self.neighbors(trainX,test)\n        #print(a)\n        cdef arr=sorted(a,key=itemgetter(1))\n        #print(arr)\n        #print(arr[0:k])\n        #print(arr[0:k])\n        arr=arr[0:k]\n        #rint(arr[0][:])\n        return(self.vote(arr,trainY))","8bb08c3f":"def mainknn(trainknnX,testknnX,trainknny,testknny):\n    y_pred=[]\n    k=5\n    a=knn()\n    trainknnX=trainknnX.astype(np.double)\n    testknnX=testknnX.astype(np.double)\n    trainknny=trainknny.astype(np.double)\n    testknny=testknny.astype(np.double)\n\n    for i in range(testX.shape[0]):\n        feature=testknnX[i,:]\n        #print(feature.shape)\n        #print()\n        #if (feature.shape!=trainX.shape[1]):\n        #    print('Oh')\n        y_pred.append(a.fit(trainknnX,trainknny,feature,k))\n\n    y=np.asarray(y_pred)\n    #h=np.concatenate((testY,y),axis=1)\n\n    #for i in range(testY.shape[0]):\n    #    print(testY[i],' : ',y[i])\n\n    print(accuracy_score(testknny, y))","336d181b":"mainknn(trainknnX,testknnX,trainknny,testknny)","b3db4349":"%%cython\ncimport cython\ncimport numpy as np\nimport numpy as np\ncdef class LogisticRegression(): \n    \n    @cython.returns(cython.double[:])\n    cpdef  sigmoid(self,double[:,:] z):\n        z=np.array(z,dtype=np.double)\n        return (1\/(1+(np.exp(np.multiply(-1,z)))))\n    \n    @cython.returns(cython.double)\n    @cython.locals(h=\"double\",y=\"double\")\n    cpdef cost(self, h, y):\n        n=h.shape[0]\n        return (((-y*np.log(h)-(1-y)*np.log(1-h))\/n))\n    \n    @cython.returns(cython.double[:])\n    cpdef predict(self,double[:,:] feature,double[:,:] weights):\n        fw=np.dot(feature,weights)\n        sig=self.sigmoid(fw)\n        return(sig)\n\n    @cython.returns(np.double)\n    #@cython.locals(trainX=\"double[:,:]\",trainY=\"double[:]\")\n    #cpdef fit(self,trainX,trainY,int iteration=1000,float learningrate=0.001,int batchsize=10,long regularizationrate=10000000):\n    cpdef fit(self,double[:, :] trainX,double[:] trainY,int iteration=1000,float learningrate=0.001,int batchsize=10,long regularizationrate=10000000):\n        weights=np.array(np.random.rand(trainX.shape[1],1),dtype=np.double)\n        #print(weights.shape)\n        #print('OOO')\n        for i in range(iteration):\n            choice=np.random.choice(trainX.shape[0],size=batchsize)\n            #print('OOO')#####\n            batchX=[]\n            batchY=[]\n            for j in range(batchsize):\n                index=choice[j]\n                batchX.append(trainX[index][:])\n                batchY.append(trainY[index])\n\n                #batchX=np.concatenate((batchX,trainX[index][:]))\n                #batchY=np.concatenate((batchY,trainY[index]))\n            #print('OOO')######\n            batchX=np.asarray(batchX)\n            batchY=np.asarray(batchY)\n            y_pred=self.predict(batchX,weights)\n            '''\n            gradient = np.dot(batchX.T,  (y_pred - batchY))+(np.linalg.norm(weights)\/(regularizationrate))\n            #print(gradient)\n            #################################\n            gradient=np.mean(gradient,axis=1)\n            gradient=np.reshape(gradient,(4,1))'''\n            gradient=np.reshape((np.mean((np.dot(batchX.T,  (y_pred - batchY))+(np.linalg.norm(weights)\/(regularizationrate))),axis=1)),(weights.shape[0],1))\n            lg=learningrate * gradient \n            #weights = weights-lg #+\/- ???\n            weights=np.array(np.subtract(weights,lg),dtype=np.double)\n\n            #################################\n        return weights","f9bbe941":"def mainlogreg(trainX,trainy,testX,testy):\n    \n    trainX=trainX.astype(np.double)\n    trainy=trainy.astype(np.double)\n    classifier=[]\n    for j in range(testy.shape[1]):\n        a=LogisticRegression()\n        w=a.fit(trainX,trainy[:,j],iteration=5000, learningrate=0.01, batchsize=10, regularizationrate=1000)\n        classifier.append(w)\n        \n    #print('fit')\n    #ys1=[]\n    #ys2=[]\n    #ys3=[]\n    y_pred=np.zeros((1,testy.shape[1]))\n    b=LogisticRegression()\n    \n    for i in range(testy.shape[0]):\n        feature=np.reshape(testX[i][:],(1,testX.shape[1]))\n        feature=feature.astype(np.double)\n        #print(np.round(b.predict(feature,classifier[0])),' , ',np.round(b.predict(feature,classifier[1])),' , ',np.round(b.predict(feature,classifier[2])),' -> ',testY[i])    \n        #ys1.append(np.round(b.predict(feature,classifier[0])))\n        #ys2.append(np.round(b.predict(feature,classifier[1])))\n        #ys3.append(np.round(b.predict(feature,classifier[2])))\n        y_temp=[]\n        for j in range(len(classifier)):\n            pred=np.round(b.predict(feature,classifier[j]))\n            y_temp.append(pred)\n        y_temp=np.asarray(y_temp)\n        y_temp=y_temp.reshape((1,testy.shape[1]))\n        y_pred=np.concatenate((y_pred,y_temp),axis=0)\n    \n    y_pred=np.delete(y_pred,(0),axis=0)  \n    for i in range(testy.shape[1]):\n        print(accuracy_score(testy[:,i],y_pred[:,i]))","b94ec9be":"mainlogreg(trainX,trainy,testX,testy)","d9e5440e":"%%cython\nimport numpy as np\ncimport numpy as np\ncdef class perceptron():\n    \n    cpdef int predict(self,double[:]feature,double[:]w):\n        return( np.where((np.dot(feature,w[1:])+w[0])>0,1,-1))\n\n    cpdef double[:] train(self,double[:,:] features,double[:] labels,double lr=0.01,long iteration=10,int batchsize=10):\n        w=np.zeros(features.shape[1]+1)\n        for _ in range(iteration):\n            '''for feature,label in zip(features,labels):\n                y_pred=self.predict(feature,w)\n                #w=w-np.dot(feature.T,(label-predict(feature,w))*lr)\n                #w=w+lr*(label-predict(feature,w))*feature\n                #print(y_pred.dtype)\n                #print(label.dtype)\n                e=np.array(np.subtract(y_pred,label))#,casting='unsafe'\n                #print(e.dtype)\n                #w[1:] -= lr*(e)*feature#label-y_pred\n                #w[0] -= lr*(e)\n                w[1:]=np.subtract(w[1:],(e*lr*feature))\n                w[0]=np.subtract(w[0],(e*lr))'''\n            choice=np.random.choice(features.shape[0],size=batchsize)\n            batchX=[]\n            batchY=[]\n            for j in range(batchsize):\n                index=choice[j]\n\n                batchX.append(features[index][:])\n                batchY.append(labels[index])\n\n                #batchX=np.concatenate((batchX,trainX[index][:]))\n                #batchY=np.concatenate((batchY,trainY[index]))\n\n            batchX=np.asarray(batchX)\n            batchY=np.asarray(batchY)\n            #y_pred=self.predict(batchX,w)\n            for feature,label in zip(batchX,batchY): \n                y_pred=self.predict(feature,w)\n                e=y_pred-label#np.array(np.subtract(y_pred,label))\n                w[1:]=np.subtract(w[1:],(e*lr*feature))\n                w[0]=np.subtract(w[0],(e*lr))\n        return w","f4bd6495":"def mainper(trainX,trainy,testX,testy):\n    trainX=trainX.astype(np.double)\n    trainy=trainy.astype(np.double)\n    \n    classifier=[]\n    for j in range(trainy.shape[1]):\n        a=perceptron()\n        w=a.train(trainX,trainy[:,j],lr=0.01,iteration=5000)\n        classifier.append(w)\n    \n    b=perceptron()\n    y_pred=np.zeros((1,testy.shape[1]))\n    for i in range(testy.shape[0]):\n        \n        y_temp=[]\n        for j in range(len(classifier)):\n            feature=testX[i,:]\n            feature=feature.astype(np.double)\n            pred=np.round(b.predict(feature,classifier[j]))\n            y_temp.append(pred)\n        y_temp=np.asarray(y_temp)\n        y_temp=y_temp.reshape((1,testy.shape[1]))\n        y_pred=np.concatenate((y_pred,y_temp),axis=0)\n    \n    y_pred=np.delete(y_pred,(0),axis=0)  \n    for i in range(testy.shape[1]):\n        print(accuracy_score(testy[:,i],y_pred[:,i]))\n    print('----')","5f578b26":"mainper(trainX,trainy,testX,testy)","427d2bb0":"# Classification: #","dc00495e":"## Perceptron: ##","75fdf899":"# Split data into train, test:# \n### (our classificaton methods does'nt need validation) ###","d53057f0":"My first project!!!\nClassification with three method:\n* KNN\n* Logistic Regression \n* Perceptron\n\non [Character Font Images Data Set](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Character+Font+Images)\n\nUse [Google colab](colab.research.google.com\/) For more speed\n","df2a582a":"## there is no Missing Values \ud83d\ude0b ##  ","6ff860a6":"# or load Numpy arrays:#","fb66361a":"# All data in one Numpy array: #","74934cc0":"# At first Download Dataset:#","50e7dd08":"## Logistic Regression: ##","6ab8ac59":"# binarization labels: #","16498136":"# Run each function you need: #","608750a2":"## load cython compiler to compile classification method into c code: ##\nuseful resource: \"Cython: A Guide for Python Programmers\" by Kurt Smith\n\nif don't use colab install Cython: ``pip3 install cython``","17ad4cd5":"## Knn: ##","4977645d":"# Delete outliers: #\n(Doesn't work)","559338af":"## Class name to id (for knn): ##"}}