{"cell_type":{"ef16a1de":"code","97d3f803":"code","536ba469":"code","6c220a8d":"code","1c16dc2d":"code","cb06bb52":"code","8fc4abac":"code","2f60d05b":"code","76a02481":"code","62c174d4":"code","fb600a5f":"code","b399c7e9":"code","324d7bb2":"code","d7173912":"code","11d9d5a8":"code","b531b9c0":"code","e350c5b4":"code","f7fea171":"code","790cdaac":"code","f3f3b686":"code","64ed259a":"code","e3f8c972":"code","d693ad9c":"code","57ee485d":"code","055b0954":"code","16d77e30":"code","a2441565":"code","a1a95cfa":"markdown","89472a18":"markdown","bb6ee308":"markdown","f4ae3abb":"markdown"},"source":{"ef16a1de":"Import any library that we use in this project","97d3f803":"import os\nimport sys\n\nfrom copy import deepcopy\nfrom collections import Counter\nfrom nltk.tag import CRFTagger\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import classification_report\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimport re\nimport unicodedata\nimport nltk\n\nnltk.download('punkt')\n","536ba469":"Defining the function for extracting the dataset","6c220a8d":"def get_raw_data_from_bio_file(fpath):\n    \"\"\"A simple function to read in from a one-word-per-line BIO\n    (Beginning, Inside, Outside) tagged corpus, tab separated\n    and each example sentence\/text separated with a blank line.\n    The data is already tokenized in a simple way.\n    e.g.:                     \n    \n    O\ta\n    O\tgreat\n    O\tlunch\n    O\tspot\n    O\tbut\n    B-Hours\topen\n    I-Hours\ttill\n    I-Hours\t2\n    I-Hours\ta\n    I-Hours\tm\n    B-Restaurant_Name\tpassims\n    I-Restaurant_Name\tkitchen\n    \n    returns a list of lists of tuples of (word, tag) tuples\n    \"\"\"\n    f = open(fpath)\n    data = []  # the data, a list of lists of (word, tag) tuples\n    current_sent = []  # data for current sentence\/example\n    for line in f:\n        if line == \"\\n\":  # each instance has a blank line separating it from next one\n            # solution\n            data.append(current_sent)\n            current_sent = []\n            continue\n        line_data = line.strip(\"\\n\").split(\"\\t\")\n        current_sent.append((line_data[1], line_data[0]))\n    f.close()\n    return data","1c16dc2d":"raw_training_data = get_raw_data_from_bio_file(\"engtrain.bio.txt\") ","cb06bb52":"# have a look at the first example\nprint(raw_training_data[0])","8fc4abac":"print(len(raw_training_data), \"instances\")\nprint(sum([len(sent) for sent in raw_training_data]), \"words\")","2f60d05b":"def preProcess(example):\n    \"\"\"Function takes in list of (word, bio-tag) pairs, e.g.:\n        [('what', 'O'), ('movies', 'O'), ('star', 'O'), ('bruce', 'B-ACTOR'), ('willis', 'I-ACTOR')]\n    returns new (token, bio-tag) pairs with preprocessing applied to the words\"\"\"\n       \n    preprocessed_example = example\n    return preprocessed_example","76a02481":"training_data = [preProcess(example) for example in raw_training_data]","62c174d4":"# check the effect of pre-processing\nprint(training_data[0])","fb600a5f":"Defining the function to do feature engineering by using NLP method","b399c7e9":"_pattern = re.compile(r\"\\d\")  # to recognize numbers\/digits\n\n# This is the 'out-of-the-box' get_features function from the nltk CRF tagger\ndef get_features(tokens, idx):\n    \"\"\"\n    Extract basic features about this word including\n         - Current Word\n         - Is Capitalized ?\n         - Has Punctuation ?\n         - Has Number ?\n         - Suffixes up to length 3\n    Note that : we might include feature over previous word, next word ect.\n\n    :return : a list which contains the features\n    :rtype : list(str)\n\n    \"\"\"\n    token = tokens[idx]\n    feature_list = []\n\n    if not token:\n        return feature_list\n\n    # Capitalization\n    if token[0].isupper():\n        feature_list.append(\"CAPITALIZATION\")\n\n    # Number\n    if re.search(_pattern, token) is not None:\n        feature_list.append(\"HAS_NUM\")\n\n    # Punctuation\n    punc_cat = set([\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"])\n    if all(unicodedata.category(x) in punc_cat for x in token):\n        feature_list.append(\"PUNCTUATION\")\n\n    # Suffix up to length 3\n    if len(token) > 1:\n        feature_list.append(\"SUF_\" + token[-1:])\n    if len(token) > 2:\n        feature_list.append(\"SUF_\" + token[-2:])\n    if len(token) > 3:\n        feature_list.append(\"SUF_\" + token[-3:])\n        \n    feature_list.append(\"WORD_\" + token)\n    #print(feature_list)\n    return feature_list","324d7bb2":"pip install python-crfsuite","d7173912":"Implement CRF Tagging to train the model","11d9d5a8":"# Train the CRF BIO-tag tagger\nTAGGER_PATH = \"crf_nlu.tagger\"  # path to the tagger- it will save\/access the model from here\nct = CRFTagger(feature_func=get_features)  # initialize tagger with get_features function\n\nprint(\"training tagger...\")\nct.train(training_data, TAGGER_PATH)\nprint(\"done\")","b531b9c0":"# load tagger from saved file\nct = CRFTagger(feature_func=get_features)  # initialize tagger\nct.set_model_file(TAGGER_PATH)  # load model from file\n\n#\u00a0prepare the test data:\nraw_test_data = get_raw_data_from_bio_file(\"engtest.bio.txt\") \ntest_data = [preProcess(example) for example in raw_test_data]\nprint(len(test_data), \"instances\")\nprint(sum([len(sent) for sent in test_data]), \"words\")","e350c5b4":"print(\"testing tagger...\")\npreds = []\ny_test = []\nfor sent in test_data:\n    sent_preds = [x[1] for x in ct.tag([s[0] for s in sent])]\n    sent_true = [s[1] for s in sent]\n    preds.extend(sent_preds)\n    y_test.extend(sent_true)\nprint(\"done\")","f7fea171":"# Output the classification report (which you should save each time for comparing your models)\nprint(classification_report(y_test, preds))","790cdaac":"def confusion_matrix_heatmap(y_test, preds):\n    \"\"\"Function to plot a confusion matrix\"\"\"\n    labels = list(set(y_test))   # get the labels in the y_test\n    # print(labels)\n    cm = confusion_matrix(y_test, preds, labels)\n    fig = plt.figure(figsize=(20,20))\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(cm)\n    plt.title('Confusion matrix of the classifier')\n    fig.colorbar(cax)\n    ax.set_xticks(np.arange(len(labels)))\n    ax.set_yticks(np.arange(len(labels)))\n    ax.set_xticklabels( labels, rotation=45)\n    ax.set_yticklabels( labels)\n\n    for i in range(len(cm)):\n        for j in range(len(cm)):\n            text = ax.text(j, i, cm[i, j],\n                           ha=\"center\", va=\"center\", color=\"w\")\n\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    #fig.tight_layout()\n    plt.show()","f3f3b686":"confusion_matrix_heatmap(y_test, preds)","64ed259a":"\npercentage = 0.8 # separate the data in 80% of training data and 20% of testing_data\ndata_sample = len(training_data) # find the length of training_data\ntraining_sample = int((percentage*data_sample)) # create the number of training data\ntrain_dataset = training_data[:training_sample] # create the list which store train_data\ntest_dataset = training_data[training_sample:] # create the list which store test_data\n    \n  \n\n\n","e3f8c972":"print(\"testing tagger...\")\n# we will train the data by using CRFTagger()\npredict_result = [] #create list\ny_predict = []\nsent_predict_result = []\nsent_y_predict = []  \nfor i in test_dataset:\n\n      # we use the previous code the train the data\n      sent_predict_result = [k[1] for k in ct.tag([j[0] for j in i])] # we will tag the word(j) by using ct.tag() and store BIO- tag in sent_predict_result\n      sent_y_predict = [j[1] for j in i] # we store all BIO-tag, which has one line, in sent_y_predict\n      predict_result.extend(sent_predict_result) # add the data in predict_result \n      y_predict.extend(sent_y_predict) # add the data in y_predict    \n\n\nprint(\"done\")","d693ad9c":"print(classification_report(y_predict,predict_result))","57ee485d":"import pandas as pd \ndef classification_report_table(table): # this function will convert classification_report to csv file\n   data_table = []\n   column_data = table.split('\\n') # split the data and consider in single-line\n   for i in column_data[2:-5]: # iterate the data which consider only the data (not white space). So we use index from 2 until the last five lines\n       row_classification = {} # create dictionary\n       row_data = ' '.join(i.split()) # split the data by all white space and join the data again by one white space\n       row_data = row_data.split(' ') # split the data again in order to visualise easily, P.S. we use this method to distiguish the column\n       row_classification['class'] = row_data[0] # given data in class\n       row_classification['precision'] = ' '.join(map(str, row_data[1:2])) #given data in precision\n       row_classification['recall'] = ' '.join(map(str, row_data[2:3])) #given data in recall\n       row_classification['f1-score'] = ' '.join(map(str, row_data[3:4])) #given data in f1-score\n       row_classification['support'] = row_data[-1] #given data in support\n       data_table.append(row_classification) # add the all data in single line to data_table\n   df = pd.DataFrame.from_dict(data_table) # use this data to create csv file\n   df.to_csv('classification_report.csv' ,index = False)\n   \nx = classification_report(y_predict, predict_result) #store classification_report in x\nclassification_report_table(x) # execute function","055b0954":"y = pd.read_csv('classification_report.csv') # read classification_report.csv\nprint(y)","16d77e30":"y_precision = y.sort_values(by = ['precision']) # sort value by 'precision' from lowest to highest\ny_precision[0:5] # visualise the five lowest precision to illustrate 5 cases in false postive\n","a2441565":"y_recall = y.sort_values(by = ['recall']) #sort value by 'recall' from lowest to highest\ny_recall[0:5] #visualise the five lowest precision to illustrate 5 cases in false postive","a1a95cfa":"# CRF sequence tagging for Movie Queries","89472a18":"# Split the training data into 80% training, 20% development set \nWe need to separate the data from the engtrain.bio.txt- 80% for training data, 20% for developing the model ","bb6ee308":"# Error analysis 2: False negatives \n\nFor the 5 classes which have the lowest recall, according to the results table from your 20% development data,, print out all the sentences where there is a false negative for that label (i.e. the label is present in the ground truth label for a given word, but that label is not predicted for that word by the tagger). \n\nAlso, we should examine the 5 lowest recall of the classes, according to the 20% development data, finding the false negative for that label. For example,the label is present in the ground truth label for a given word, but that label is not predicted for that word by the tagger  \n","f4ae3abb":"# Error analysis 1: False positives \n\nwe can perform error analysis for improving NLP application. So, we can illustrate 5 lowest precision classes, according to the result of 20% development data, showing the false positive for that class. For example,  the label is predicted in the predicted label for a given word by the tagger, but this is not present in the corresponding ground truth label for that word "}}