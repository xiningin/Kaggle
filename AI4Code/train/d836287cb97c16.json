{"cell_type":{"2ee1b37a":"code","738b39d6":"code","d86083e4":"code","175034aa":"code","3b55dec3":"code","c106614a":"code","f8a20176":"code","3680e87f":"code","7a39f6fa":"code","4ce52332":"code","c03dd940":"code","35323ca0":"code","9a429fef":"code","c058fecd":"code","0c30bfdf":"code","158d7b25":"code","bf0a6c33":"code","217496de":"code","12fba441":"code","cf932dff":"code","52000949":"code","3d00ccb5":"code","276a6112":"code","40e4827d":"code","1a3616d5":"code","a6f3e88a":"code","e0679bd8":"code","b849e962":"code","f2b10f10":"code","e5da111b":"code","38f88528":"code","83056cff":"code","f14ff5a8":"code","4e9d29b0":"code","1ce36dd6":"markdown","e50ff0c9":"markdown","1a511dde":"markdown","5e6d8720":"markdown","dc0989c8":"markdown","d8a75c47":"markdown","7060d52a":"markdown","e0e04ef1":"markdown","9c1f2bb6":"markdown","c8b506ea":"markdown","fa3968fa":"markdown","a5fc5e29":"markdown","14e14746":"markdown","108161a3":"markdown","b22e5815":"markdown","cc72fe01":"markdown"},"source":{"2ee1b37a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\nimport random\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split","738b39d6":"# Load the training data\n\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","d86083e4":"test.info(memory_usage=\"deep\")","175034aa":"train.head(10)","3b55dec3":"test.head(10)","c106614a":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","f8a20176":"# Comparing the datasets length\nfig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","3680e87f":"# Statistical description of the train dataset\ntrain.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]).T","7a39f6fa":"# Checking if there are missing values in the datasets\ntrain.isna().sum().sum(), test.isna().sum().sum()","4ce52332":"fig, ax = plt.subplots(figsize=(16, 8))\n\nbars = ax.hist(train[\"target\"],\n               bins=100,\n               color=\"palevioletred\",\n               edgecolor=\"black\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","c03dd940":"print(f\"{(train['target'] < 5).sum() \/ len(train) * 100:.3f}% of the target values are less than 5\")","35323ca0":"# Lists of categorical and numerical feature columns\ncat_features = [\"cat\" + str(i) for i in range(10)]\nnum_features = [\"cont\" + str(i) for i in range(14)]","9a429fef":"# Combined dataframe containing numerical features only\ndf = pd.concat([train[num_features], test[num_features]], axis=0)\ncolumns = df.columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n\n# Plots counter\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n            # Train data histogram\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            # Test data histogram\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n# plt.suptitle(\"Numerical feature values distribution in both datasets\", y=0.99)\nplt.show();","c058fecd":"# Combined dataframe containing categorical features only\ndf = pd.concat([train[cat_features], test[cat_features]], axis=0)\ncolumns = df.columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.2, wspace=0.25)\n\n# Plots counter\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(cat_features): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n\n            values = df[cat_features[i]].value_counts().sort_index(ascending=False).index\n            bars_pos = np.arange(0, len(values))\n            if len(values)<4:\n                height=0.1\n            else:\n                height=0.3\n\n            bars1 = axs[r, c].barh(bars_pos+height\/2,\n                                   [train[train[cat_features[i]]==x][cat_features[i]].count() for x in values],\n                                   height=height,\n                                   color=\"teal\",\n                                   edgecolor=\"black\",\n                                   label=\"Train Dataset\")\n            bars2 = axs[r, c].barh(bars_pos-height\/2,\n                                   [test[test[cat_features[i]]==x][cat_features[i]].count() for x in values],\n                                   height=height,\n                                   color=\"salmon\",\n                                   edgecolor=\"black\",\n                                   label=\"Test Dataset\")\n            y_labels = [str(x) for x in values]\n\n            axs[r, c].set_title(cat_features[i], fontsize=14, pad=1)\n            axs[r, c].set_xlim(0, len(train[\"id\"])+50)\n            axs[r, c].set_yticks(bars_pos)\n            axs[r, c].set_yticklabels(y_labels)\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"x\")\n            axs[r, c].legend(fontsize=12)\n            axs[r, c].margins(0.1, 0.02)\n                                  \n        i+=1\n\n#plt.suptitle(\"Categorical feature values distribution in both datasets\", y=0.99)\nplt.show();","0c30bfdf":"# Bars position should be numerical because there will be arithmetical operations with them\nbars_pos = np.arange(len(cat_features))\n\nwidth=0.3\nfig, ax = plt.subplots(figsize=(14, 6))\n# Making two bar objects. One is on the left from bar position and the other one is on the right\nbars1 = ax.bar(bars_pos-width\/2,\n               train[cat_features].nunique().values,\n               width=width,\n               color=\"darkorange\", edgecolor=\"black\")\nbars2 = ax.bar(bars_pos+width\/2,\n               train[cat_features].nunique().values,\n               width=width,\n               color=\"steelblue\", edgecolor=\"black\")\nax.set_title(\"Amount of values in categorical features\", fontsize=20, pad=15)\nax.set_xlabel(\"Categorical feature\", fontsize=15, labelpad=15)\nax.set_ylabel(\"Amount of values\", fontsize=15, labelpad=15)\nax.set_xticks(bars_pos)\nax.set_xticklabels(cat_features, fontsize=12)\nax.tick_params(axis=\"y\", labelsize=12)\nax.grid(axis=\"y\")\nplt.margins(0.01, 0.05)","158d7b25":"# Checking if test data doesn't contain categories that are not present in the train dataset\nfor col in cat_features:\n    print(set(train[col].value_counts().index) == set(test[col].value_counts().index))","bf0a6c33":"# Plot dataframe\ndf = train.drop(\"id\", axis=1)\n\n# Encoding categorical features with OrdinalEncoder\nfor col in cat_features:\n    encoder = OrdinalEncoder()\n    df[col] = encoder.fit_transform(np.array(df[col]).reshape(-1, 1))\n\n# Calculatin correlation values\ndf = df.corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(14,14))\nax = sns.heatmap(df, annot=True, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"normal\", \"fontsize\":9})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","217496de":"columns = train.drop([\"id\", \"target\"], axis=1).columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 4\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            scatter = axs[r, c].scatter(train[columns[i]].values,\n                                        train[\"target\"],\n                                        color=random.choice(colors))\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n                                  \n        i+=1\n# plt.suptitle(\"Features vs target\", y=0.99)\nplt.show();","12fba441":"# Remove rows with missing target, separate target from predictors. \n\n# Separate target from features\ny = train['target']\nfeatures = train.drop([\"id\",'target'], axis=1)\nTest = test.drop([\"id\"], axis=1)\n\n# List of features for later use\nfeature_list = list(features.columns)\n\n# Preview features\nfeatures.head()","cf932dff":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = Test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\n# Preview the ordinal-encoded features\nX.head()","52000949":"X_test.head()","3d00ccb5":"X.columns","276a6112":"from sklearn.model_selection import train_test_split\n# Splitting the train dataset \nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)","40e4827d":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor()\nmodel.fit(X_train, y_train)\npreds_valid = model.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","1a3616d5":"# Feature Importance\npd.Series(data=model.feature_importances_,\n         index=X_train.columns).sort_values(ascending=False).plot.bar(color='darkorange')","a6f3e88a":"# Get numerical feature importances\nimportances = list(model.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# List of features sorted from most to least important\nsorted_importances = [importance[1] for importance in feature_importances]\nsorted_features = [importance[0] for importance in feature_importances]\n# Cumulative importances\ncumulative_importances = np.cumsum(sorted_importances)\n# list of x locations for plotting\nx_values = list(range(len(importances)))\n# Make a line graph\nplt.plot(x_values, cumulative_importances, 'g-')\n# Draw line at 95% of importance retained\nplt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n# Format x ticks and labels\nplt.xticks(x_values, sorted_features, rotation = 'vertical')\n# Axis labels and title\nplt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');","e0679bd8":"# Add 1 because Python is zero-indexed\nprint('Number of features for 95% importance:', np.where(cumulative_importances > 0.95)[0][0] + 1)","b849e962":"# Extract the names of the most important features\nimportant_feature_names = [feature[0] for feature in feature_importances[0:17]]\n# Create training and testing sets with only the important features\nimportant_train_features = X_train[important_feature_names]\nimportant_test_features = X_valid[important_feature_names]\n# Check\nprint('Important train features shape:', important_train_features.shape)\nprint('Important test features shape:', important_test_features.shape)","f2b10f10":"# Train the expanded model on only the important features\nmodel.fit(important_train_features, y_train)\npreds_valid = model.predict(important_test_features)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","e5da111b":"# #Hyperparameter Optimization - Played with parameters to provide an optimum result in XGBoost","38f88528":"# Here,I tuned 13 of the hyperparameters that potentially would have a big impact on performance\nhyperparameters_grid = {'objective': 'reg:squarederror',\n              'n_estimators': 10000,\n              'learning_rate': 0.036,\n              'subsample': 0.926,\n              'colsample_bytree': 0.118,\n              'grow_policy':'lossguide',\n              'max_depth': 3,\n              'booster': 'gbtree', \n              'reg_lambda': 45.1,\n              'reg_alpha': 34.9,\n              'random_state': 42,\n              'reg_lambda': 0.00087,\n              'reg_alpha': 23.132,\n              'n_jobs': 4}\n\nmodel_2 = XGBRegressor(**hyperparameters_grid)\nmodel_2.fit(important_train_features, y_train)\npreds_valid = model_2.predict(important_test_features)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","83056cff":"#Define the grid of hyperparameters to search \nhyperparameters_grid2 = {'objective': 'reg:squarederror',\n              'n_estimators': 5000,\n              'learning_rate': 0.036,\n              'subsample': 0.926,\n              'colsample_bytree': 0.118,\n              'grow_policy':'lossguide',\n              'max_depth': 3,\n              'booster': 'gbtree', \n              'random_state': 0,\n              'reg_lambda': 0.00087,\n              'reg_alpha': 23.132,\n              'n_jobs': 4}\n\nmodel_3 = XGBRegressor(**hyperparameters_grid2)\nmodel_3.fit(important_train_features, y_train)\npreds_valid = model_3.predict(important_test_features)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","f14ff5a8":"# Extract the names of the most important features from X_test\nimportant_feature_names = [feature[0] for feature in feature_importances[0:17]]\nimportant_X_test_features = X_test[important_feature_names]\nimportant_X_test_features","4e9d29b0":"# Use the model to generate predictions\npredictions = model_3.predict(important_X_test_features)\n\n# Save the predictions to a CSV file\nsample_submission.target = predictions\nsample_submission.to_csv(\"submission.csv\", index=False)","1ce36dd6":"# **Feature Importance**","e50ff0c9":"As you can see, target column is very weakly correlated with all features.\n\nLet's visualize each feature vs target.","1a511dde":"There are no missing value in the both datasets.\n\nLet's check target distribution.","5e6d8720":"Retraining Model with only the 18 most important features and further conduct parameters tuning","dc0989c8":"# **EDA**","d8a75c47":"The dataset contains categorical and numerical values. Let's see values distribution for these categories.","7060d52a":"# **Data preprocessing\/Feature Selection**","e0e04ef1":"Cumulative Feature Importance","9c1f2bb6":"Training and Tuning an XGBoost model","c8b506ea":"# **Data import**","fa3968fa":"EDA forked from https:\/\/www.kaggle.com\/rishirajacharya\/30-days-xgboost-with-5-folds-eda-gpu","a5fc5e29":"# **Predictions submission**","14e14746":"# **Model training**","108161a3":"So the datasets are pretty well balanced. Let's look at feature correlation.","b22e5815":"Let's check if the datasets have different amount of categories in categorical features.","cc72fe01":"Features required for 95% of cumulative importance"}}