{"cell_type":{"1b5b3e57":"code","22122d77":"code","c0ae18ad":"code","5c499fe1":"code","acd17f10":"code","1f5701ec":"code","0b544641":"code","f27d3e67":"code","11be2c68":"code","70a7dc76":"code","37163cd0":"code","8a7ce014":"code","c78d1a1b":"code","687a7a82":"code","4b36af7c":"code","c89dd211":"code","f78da87d":"code","a813b43a":"code","5644973c":"code","c09e6ef4":"code","662f2f73":"code","58d1f503":"code","2e533e85":"code","5647c309":"code","be21368d":"code","b4727b77":"code","1791a8bd":"code","bb1ad1a4":"code","a851afab":"code","0652430a":"code","2acc7d23":"code","6ef907f4":"code","cec994ae":"code","a3b9f2ff":"code","7d32b19d":"code","159afb57":"code","c1e243d8":"code","e5879965":"code","bd02e626":"markdown","c1aebf6d":"markdown","3eab77f7":"markdown","e2469639":"markdown","372856eb":"markdown","32635f7f":"markdown","ac8074bf":"markdown","2de6a6e5":"markdown","7d59f7dd":"markdown","8e62eab6":"markdown","e3f2e417":"markdown","23fc34f8":"markdown","582a2db7":"markdown","945fde12":"markdown","614e8542":"markdown","a86457ce":"markdown","cee9922c":"markdown","16da8f6b":"markdown","b3945a5a":"markdown","9a919507":"markdown","4aa5638f":"markdown","5a1cb23e":"markdown","aa2f62a0":"markdown","dc53a20e":"markdown","42f53544":"markdown","4172735a":"markdown","0597e602":"markdown","1f93b96e":"markdown","e28c2563":"markdown","bfe16eeb":"markdown","413eea76":"markdown","f5df9c41":"markdown","46c13571":"markdown","96728e8e":"markdown","e72536f6":"markdown","54b522f2":"markdown"},"source":{"1b5b3e57":"%%capture\n\n!pip install contractions\n!pip install gensim\n\nimport html\nimport re\nimport unicodedata\nimport string\nimport spacy\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nimport contractions\nimport numpy as np\nimport os\nimport datetime\nimport seaborn as sns\nimport multiprocessing\nimport nltk\nimport matplotlib.pyplot as plt\nimport pyLDAvis\nimport pyLDAvis.gensim  \n\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import word_tokenize\nfrom nltk.corpus import wordnet as wn\nfrom nltk import pos_tag\nfrom requests import get\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.cluster.hierarchy import dendrogram, linkage, ward, cut_tree\nfrom gensim import corpora, models\nfrom gensim.models import word2vec\nfrom gensim.models import CoherenceModel\nfrom gensim.models.phrases import Phrases, Phraser\nfrom sklearn.neighbors import NearestNeighbors\nfrom collections import defaultdict\nfrom collections import Counter\nfrom sklearn.manifold import TSNE\n\npd.set_option('display.max_rows', 10)\n\nnltk.download('stopwords')\nspacy.cli.download(\"en\")\n\nsns.set(style=\"darkgrid\")\n%matplotlib inline","22122d77":"url = 'http:\/\/www.gutenberg.org\/files\/25525\/25525-h\/25525-h.htm#2150link2H_4_0003'\nresponse = get(url)\nresponse.encoding = 'utf-8'\n\ntext = re.sub(r\"\\r|\\n\", \"\", response.text)\nbook1 = re.search(r\"<h2[^<]*?THE UNPARALLELED ADVENTURES.*She was dead!.*?p>\", text).group(0)\nbook2 = re.search(r\"<h2[^<]*?THE PURLOINED LETTER.*unto Eleonora.*?p>\", text).group(0)\nbook3 = re.search(r\"<h2[^<]*?LIGEIA.*opinion upon that.*?p>\", text).group(0)\nbook4 = re.search(r\"<h2[^<]*?THE DEVIL IN THE BELFRY.*departed friends.*?p>\", text).group(0)\nbook5 = re.search(r\"<h2[^<]*?PHILOSOPHY OF FURNITURE.*it would have been Lilies without, roses within.*?p>\", text).group(0)\n\nbooks = book1 + book2 + book3 + book4 + book5\nbooks = html.unescape(books)\nbooks = re.sub(r\"\\s+\", \" \", books)","c0ae18ad":"titles = re.findall(r\"<h2.*?h2>\", books)\ntitles = [re.sub(r\"<.*?>\", \"\", x).strip() for x in titles]\ntitles = [re.sub(r\"Footnotes.*|Notes.*|[.]|\\(.*?\\)\", \"\",x, flags=re.I) for x in titles]\ntitles = [x.strip() for x in titles if x != \"\"]\nsort_index = np.argsort(titles)\ntitles = np.array(titles)[sort_index]\npd.DataFrame({\"Titles\":titles}).head(5)","5c499fe1":"stories = books.split(\"<h2>\")\nstories = [re.sub(r\"^.*<\/h2>\",\"\", x) for x in stories]\nstories = [re.sub(r\"<.*?>\",\"\", x).strip() for x in stories]\nstories = [i for i in stories if i != \"\" and re.match(r\"\\(\\*1\\)\", i) is None]\nstories = np.array(stories)[sort_index]\nprint(\"Number of stories : %s, Number of titles : %s\" %(len(stories),len(titles)))","acd17f10":"tables = pd.read_html(\"https:\/\/en.wikipedia.org\/wiki\/Edgar_Allan_Poe_bibliography\", header=0)\ntable = tables[2].sort_values(by=\"Title\")\ntable.Notes =  [re.sub(r'\\[.*\\]', \"\", x) for x in table.Notes.values]\ntable[:5]","1f5701ec":"titles2 = [x.upper() for x in table.Title.values.tolist()]\ntitles2 = np.array([re.sub(\"[^A-Z]\", \"\", x) for x in titles2])\ntitles1 = np.array([re.sub(\"[^A-Z]\", \"\", x) for x in titles])\ntable_info_subs = [True if x in titles1 else False for x in titles2]\nsubs = np.array([True if x in titles2 else False for x in titles1])\ntable = table[table_info_subs]\ntitles2 = titles2[table_info_subs]\nsort_index = np.argsort(titles2)\ntable = table.iloc[sort_index,:]\ntitles2 = titles2[sort_index]\n\ntitles_not_in_wiki = titles[~subs]\nstories_not_in_wiki = stories[~subs]\ntitles = titles[subs]\nstories = stories[subs]\nsort_index = np.argsort(titles)\ntitles = np.array(titles)[sort_index]\nstories = stories[sort_index]\n\n\ntitles = np.concatenate((titles, titles_not_in_wiki), axis = 0)\nstories = np.concatenate((stories, stories_not_in_wiki), axis = 0)\n","0b544641":"data = pd.DataFrame({\"title\": titles, \"stories\":stories})\ntable = table.reset_index(drop= True)\ndata = data.join(table, how=\"left\")\ndata.columns = [\"title\", \"text\", \"wikipedia_title\", \"publication_date\", \"first_published_in\", \"classification\", \"notes\"]\ndata.fillna(\"\", inplace = True)\ndata[:5]","f27d3e67":"# PHILOSOPHY OF FURNITURE\n## https:\/\/en.wikipedia.org\/wiki\/The_Philosophy_of_Furniture\ndata.at[62,'classification']= 'Essay'\ndata.at[62,'publication_date']= 'May 1840'\n\n# MAAELZEL\u2019S CHESS-PLAYER\n## https:\/\/en.wikipedia.org\/wiki\/Maelzel%27s_Chess_Player\ndata.at[63,'classification']= 'Essay'\ndata.at[63,'publication_date']= 'April 1836'\n\n# OLD ENGLISH POETRY\ndata.at[64,'classification']= 'Essay'\ndata.at[64,'publication_date']= \"?\"\n\n# THE BALLOON-HOAX\n## https:\/\/en.wikipedia.org\/wiki\/The_Balloon-Hoax\ndata.at[65,'classification']= 'Hoax \/ Fiction'\ndata.at[65,'publication_date']= 'April 13, 1844'\n\n# THE MYSTERY OF MARIE ROGET\n## https:\/\/en.wikipedia.org\/wiki\/The_Mystery_of_Marie_Rog%C3%AAt\n## taking the date of publication of the first part\ndata.at[66,'classification']= 'Detective fiction'\ndata.at[66,'publication_date']= 'November 1842'\n\n# THE POETIC PRINCIPLE\n## http:\/\/www.thepoeblog.org\/the-poetic-principle-a-rich-intellectual-treat\/\n## taking the date when the the work was known with Poe alive (published posthumously)\ndata.at[67,'classification']= 'Essay'\ndata.at[67,'publication_date']= 'August 17, 1849' \n\n# THE UNPARALLELED ADVENTURES OF ONE HANS PFAAL\n## https:\/\/en.wikipedia.org\/wiki\/The_Unparalleled_Adventure_of_One_Hans_Pfaall\ndata.at[68,'classification']= 'Hoax \/ Science fiction'\ndata.at[68,'publication_date']= 'June 1835' \n\n# X-ING A PARAGRAPH\ndata.at[69,'classification']= 'Satire'\ndata.at[69,'publication_date']= 'May 12, 1849' \n\n# Some fixes \n## The Purloined Letter\n## https:\/\/en.wikipedia.org\/wiki\/The_Purloined_Letter\ndata.at[51,'publication_date']= \"December, 1844\"\n\n## Silence - a Fable\n## https:\/\/en.wikipedia.org\/wiki\/Poems_by_Edgar_Allan_Poe\ndata.at[23,'publication_date']= \"January 4, 1840\"\n\n## https:\/\/www.goodreads.com\/book\/show\/8498298-why-the-little-frenchman-wears-his-hand-in-a-sling\ndata.at[60,'publication_date']= \"August 17, 1839\"\n\n# Eleonora\n## Unknown month, published in \"the Gift\" as \"The Pit and the Pendulum\"\ndata.at[6,'publication_date']= \"? 1841\"\n\n# The Pit and the Pendulum\ndata.at[48,'publication_date']= \"? 1843\"","11be2c68":"year = [re.sub(r\"(.*)(\\d{4})\", \"\\\\2\", x) for x in  data.publication_date.values]\nmonth = [re.sub(r\"[^a-zA-Z]\", \"\", x) for x in  data.publication_date.values]\nmonth[6] = \"?\"\nmonth[48] = \"?\"\ndata[\"normalized_date\"] = [\"%s %s\" %(x, y) for x,y in zip(month,year)]","70a7dc76":"data[\"wikipedia_title\"] = [re.sub(r\"[\\\"]\", \"\", x) for x in data.iloc[:,2].values]","37163cd0":"data.classification = [re.sub(\",\", \"\/\", x) for x in data.classification]\ndata.classification = [re.sub(r\"(\\w)( +)(\/)\", \"\\\\1\/\", x) for x in data.classification]\ndata.classification = [re.sub(r\"(\/)( +)(\\w)\", \"\/\\\\3\", x) for x in data.classification]\ndata.classification = [re.sub(\" +\", \"_\", x) for x in data.classification]\n\ntokens = [x.split(\"\/\") for x in data.classification.values]\ntokens = [sorted(x) for x in tokens]\ntokens = [\",\".join(x) for x in tokens]\n\ndata.classification = tokens\n\nnp.unique(tokens)","8a7ce014":"data[data == \"\"] = \"?\"","c78d1a1b":"data.to_csv(\"preprocessed_data.csv\", index=False)\nwith pd.option_context('display.max_rows', 100):\n    display(data)","687a7a82":"\n\"\"\"\nText normalization\n\"\"\"\n\ndef text_normalizer(text, stemming = False, lemmatize = True,\n                    find_bigrams = True, min_count = 2, min_token_length = 2):\n  \n    # remove dialog marks\n    text = re.sub(\"\u2014+\", \" \", text)\n    # remove accents\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8') \n \n    # fix contractions\n    text = contractions.fix(text) \n  \n    # fix case\n    text = text.lower() \n  \n    # remove extra newlines\n    text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', text) \n  \n    # remove special characters and digits\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text) \n  \n    # remove extra whitespace\n    text = re.sub(' +', ' ', text)\n    \n    tokens = word_tokenize(text)\n  \n    stopwords_list = nltk.corpus.stopwords.words('english')\n    stopwords_list = stopwords_list + list(string.punctuation)\n    stopwords_list = set(stopwords_list)\n    tokens = [token.strip() for token in tokens]\n    tokens = [token for token in tokens if token not in stopwords_list]\n\n    if stemming:\n        stemmer = SnowballStemmer(\"english\")\n        tokens = [stemmer.stem(token) for token in tokens]\n    \n    if lemmatize:\n        tag_map = defaultdict(lambda : wn.NOUN)\n        tag_map['J'] = wn.ADJ\n        tag_map['V'] = wn.VERB\n        tag_map['R'] = wn.ADV\n    \n        wordnet_lemmatizer = WordNetLemmatizer()\n        tokens = [wordnet_lemmatizer.lemmatize(token,  tag_map[tag[0]]) for token,tag in pos_tag(tokens)]\n\n    tokens = [token for token in tokens if len(token) > min_token_length]\n  \n    return tokens\n\n\n\"\"\"\nComputation of Word2vec average vectors\n\"\"\"\n\ndef average_word_vectors(words, model, vocabulary, num_features):\n    feature_vector = np.zeros((num_features, ), dtype = \"float64\")\n    nwords = 0.\n    for word in words:\n        if word in vocabulary:\n            nwords = nwords + 1.\n            feature_vector = np.add(feature_vector, model.wv[word])\n    if nwords:\n        feature_vector = np.divide(feature_vector, nwords)\n    return feature_vector\n  \n    \ndef averaged_word_vectorizer(corpus, model, num_features):\n    vocabulary = set(model.wv.index2word)\n    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features) for tokenized_sentence in corpus]\n    return np.array(features)\n\n\"\"\"\nLDA Coherence computation\n\"\"\"\n\ndef compute_coherence_values(model, dictionary, corpus, texts, limit, start=2, step=2):\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        print(\"Modeling = \" + str(num_topics))\n        model = models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word = dictionary, \n                                         passes=20, random_state = 100, \n                                         alpha='auto', eta='auto', update_every=1, chunksize=100)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence = coherencemodel.get_coherence()\n        coherence_values.append(coherence)\n        print(\"Coherence: \" + str(coherence))\n    return model_list, coherence_values\n\n\ndef plot_coherence(coherence_values, start, limit, step):\n    x = range(start, limit, step)\n    plt.plot(x, coherence_values)\n    plt.xlabel(\"Num Topics\")\n    plt.ylabel(\"Coherence score\")\n    plt.legend((\"coherence_values\"), loc='best')\n    plt.show()\n    \n    \n\"\"\"\nGenererate a table output with the LDA results\n\"\"\"\n    \ndef format_topics_sentences(ldamodel, corpus, texts):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), \n                                                                  round(prop_topic,4), topic_keywords]), \n                                                       ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)","4b36af7c":"normalized_stories = [text_normalizer(i) for i in data.text.values]","c89dd211":"sns.distplot([len(x) for x in normalized_stories], axlabel=\"# words\")","f78da87d":"# Set values for various parameters\nfeature_size = 300 # Word vector dimensionality\nwindow_context = 20 # Context window size\nmin_word_count = 3 # Minimum word count\nsample = 6e-5 # Downsample setting for frequent words\ncores = multiprocessing.cpu_count() # Count the number of cores in a computer\nnegative = 10\nalpha = 0.03\nepochs = 300\nmin_alpha =  0.03 \/ epochs #alpha - (min_alpha * epochs) ~ 0.00\n\nbigram = Phrases(normalized_stories, min_count=5, delimiter=b' ')\nbigram_text = [bigram[line] for line in normalized_stories]\n\nw2v_model = word2vec.Word2Vec(size=feature_size, window=window_context, min_count=min_word_count, \n                              sample=sample, iter=epochs, workers=cores-1, negative = negative, \n                              alpha = alpha, min_alpha = min_alpha)\n\nw2v_model.build_vocab(bigram_text)\n\nw2v_model.train(normalized_stories, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)\n","a813b43a":"w2v_model.corpus_total_words","5644973c":"w2v_feature_array = averaged_word_vectorizer(corpus=normalized_stories, model=w2v_model, num_features=feature_size)\ncorpus_df = pd.DataFrame(w2v_feature_array)","c09e6ef4":"w2v_model.most_similar(positive=[\"dupin\"])","662f2f73":"w2v_model.most_similar(positive=[\"clock\"])","58d1f503":"w2v_model.most_similar(positive=[\"poetry\"])","2e533e85":"similarity_matrix = cosine_similarity(w2v_feature_array)\nsimilarity_df = pd.DataFrame(similarity_matrix)\nsimilarity_df\nZ = linkage(similarity_matrix, 'ward')\n\nlabels = data.classification.values\n\nclasses=np.unique(labels)\nclasses = [re.sub(\".*Horror.*\", \"Horror\", x) for x in classes]\nclasses = [re.sub(\".*Humor.*\", \"Humor\", x) for x in classes]\nclasses = [re.sub(\".*Parody.*\", \"Humor\", x) for x in classes]\nclasses = [re.sub(\".*Satire.*\", \"Humor\", x) for x in classes]\nclasses = [re.sub(\".*Science fiction.*\", \"Science fiction\", x) for x in classes]\nclasses = [re.sub(\".*Detective fiction.*\", \"Detective fiction\", x) for x in classes]\nclasses = [re.sub(\".*Satire.*\", \"Satire\", x) for x in classes]\nclasses = np.unique(classes)\n\nlabels = [re.sub(\".*Horror.*\", \"Horror\", x) for x in labels]\nlabels = [re.sub(\".*Humor.*\", \"Humor\", x) for x in labels]\nlabels = [re.sub(\".*Parody.*\", \"Humor\", x) for x in labels]\nlabels = [re.sub(\".*Satire.*\", \"Humor\", x) for x in labels]\nlabels = [re.sub(\".*Science fiction.*\", \"Science fiction\", x) for x in labels]\nlabels = [re.sub(\".*Detective fiction.*\", \"Detective fiction\", x) for x in labels]\nlabels = [re.sub(\".*Satire.*\", \"Satire\", x) for x in labels]\n\nindex= [int(np.where(val == classes)[0]) for i, val in enumerate(labels)] ","5647c309":"w2v_feature_array.shape\ntsne_model = TSNE(perplexity=20, n_components=2, init='pca', n_iter=2500, random_state=6)\nX_2d = tsne_model.fit_transform(w2v_feature_array)\n\nplt.subplots(figsize=(20,10))\nfrom sklearn.preprocessing import MinMaxScaler\nX_2d = MinMaxScaler().fit_transform(X_2d)\ndf= pd.DataFrame({\"x\":X_2d[:, 0], \"y\":X_2d[:, 1], \"labels\":labels, \"titles\":titles})\n\nmarkers = ('v', 'o', '^', 'X', '>', '8', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', '<')\nax=sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"labels\", style=\"labels\", s=170, markers = markers)\n#For each point, we add a text inside the bubble\nfor line in range(0,df.shape[0]):\n     np.random.seed(line + 2)\n     ax.text(df.x[line] + 0.01, df.y[line] + np.random.uniform(-0.03, 0.03), \n             df.titles[line], size='small', color='black', weight=540, ha=\"left\", va=\"top\")\nax.legend(loc='right', bbox_to_anchor=(1.25, 0.5), ncol=1)\nplt.show()\n","be21368d":"info = [\"%s   (%s)\" %(x,y) for x,y in zip(data.title.values, data.classification.values + \" - \" + data.normalized_date.values)]\ninfo[:5]","b4727b77":"\npal=sns.color_palette(\"dark\", n_colors=len(classes))\npal=pal.as_hex()\npal = [pal[i] for i in index]\n\nfig,ax=plt.subplots(figsize=(12, 18))\ndd= dendrogram(Z,  labels=info, orientation = \"left\", leaf_font_size = 10, color_threshold = 4)\npal = [pal[i] for i in dd[\"leaves\"]]\nax = plt.gca()\nxlbls = ax.get_ymajorticklabels()\nnum=0\nfor lbl in xlbls:\n    lbl.set_color(pal[num])\n    num+=1\n\nax.set_title('Hierarchical Clustering Dendrogram')\nax.set_xlabel('sample index')\nax.set_ylabel('distance (Ward)')\nplt.show()","1791a8bd":"x = ward(Z)\nclusters = cut_tree(Z, n_clusters=[2, 4, 6])\ndata[[\"level_0\", \"level_1\", \"level_2\"]] = clusters","bb1ad1a4":"taged_words = [pos_tag(x, \"universal\") for x in normalized_stories]\nword_type = [Counter([y for x,y in z]) for z in taged_words ]\nword_classification = pd.DataFrame(word_type, index = titles)\n#word_classification[\"total\"] = word_classification.sum(axis=1)\nword_classification.fillna(0, inplace=True)\nword_classification = word_classification.apply(lambda x: x\/x.sum(), axis = 1)\nword_classification \n","a851afab":"sns.clustermap(word_classification, standard_scale=1, yticklabels=True, robust=True, metric = \"euclidean\", method = \"ward\", cmap=\"vlag\", figsize = (13,13))","0652430a":"# LDA \n\n# filtering grammatical structure\n\n#VERB - verbs (all tenses and modes)\n#NOUN - nouns (common and proper)\n#PRON - pronouns\n#ADJ - adjectives\n#ADV - adverbs\n#ADP - adpositions (prepositions and postpositions)\n#CONJ - conjunctions\n#DET - determiners\n#NUM - cardinal numbers\n#PRT - particles or other function words\n#X - other: foreign words, typos, abbreviations\n    \nfiltered = []\nfor x in normalized_stories:\n    tmp = []\n    for y,z in pos_tag(x, \"universal\"):\n        if z not in  [\"PRON\", \"ADP\", \"CONJ\", \"DET\", \"PRT\", \"X\"]:\n            tmp.append(y)\n    filtered.append(tmp)\n\n\n# Create a dictionary representation of the documents.\ndictionary = corpora.Dictionary(filtered)\n\n# Filter out words that occur less than 5 documents, or more than 50% of the documents.\ndictionary.filter_extremes(no_below=5, no_above=0.5)\n\ncorpus = [dictionary.doc2bow(text) for text in  filtered]\n\nldamodel = models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20, random_state = 5,  alpha='auto', eta = 'auto', update_every=1, chunksize=100)","2acc7d23":"# Can take a long time to run.\nmodel_list, coherence_values = compute_coherence_values(model = ldamodel, dictionary=dictionary, corpus=corpus, texts=filtered, start=2, limit=14, step=1)","6ef907f4":"# Show graph\nplot_coherence(coherence_values, 2, 14, 1)","cec994ae":"#optimal_model = model_list[np.argmax(coherence_values)]\noptimal_model = model_list[6] # I am choosing K = 8, it is a stable point as shown by the plot, and with an interesting diversity of topics\nmodel_topics = optimal_model.show_topics(formatted=False)\nfor i in optimal_model.print_topics(num_words=10):\n    print(i)","a3b9f2ff":"pyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(optimal_model, corpus, dictionary)\nvis","7d32b19d":"df_topic = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=info)\ndata[[\"Dominant_Topic\", \"Perc_Contribution\", \"Topic_Keywords\"]] = df_topic[[\"Dominant_Topic\", \"Perc_Contribution\", \"Topic_Keywords\"]]\ndata.to_csv(\"final_data.csv\", index=False)\ndata","159afb57":"neigh = NearestNeighbors(n_neighbors=4, metric='cosine')\nneigh.fit(w2v_feature_array)\nA = neigh.kneighbors_graph(w2v_feature_array).toarray()\nnp.fill_diagonal(A, 0)","c1e243d8":"titles = np.array(titles)\nout = []\nfor i in range(len(titles)):\n  out.append(np.array(titles)[A[i].astype(\"bool\")])","e5879965":"\nrecommended = pd.DataFrame(out, index= titles)\nrecommended.sort_index()\nrecommended = pd.DataFrame(recommended.values, info)\nrecommended.to_csv(\"recommended.csv\", index=True)\nwith pd.option_context('display.max_rows', 100):\n    display(recommended)\n","bd02e626":"### One of the clusters seems to be enriched with adjectives, while the other with verbs. I will be removing some structures to perform LDA.","c1aebf6d":"## Basic optimization, evaluating the coherence through K potential topics, from 2 to 14","3eab77f7":"## Obtaining the titles","e2469639":"## Let' cluster the stories, to detect interrelations based on text. I am grouping similar genres (e.g., humor = humor + satire + parody) for visualization purposes","372856eb":"## Wikipedia title normalization","32635f7f":"## TSNE","ac8074bf":"## Get document-level embeddings","2de6a6e5":"## Word count","7d59f7dd":"## Representing the data semantics with a hierarchical diagram","8e62eab6":"## NaN fill with \"?\"","e3f2e417":"# 5. Topic modeling via Latent Dirichlet Allocation","23fc34f8":"# 2. Basic functions","582a2db7":"## Completing missing data","945fde12":"## Final dataset: 62 annotated documents + 8 unannotated documents","614e8542":"## Parts of speech","a86457ce":"## And what happens with \"poetry\"?","cee9922c":"## Normalizing the classification column to generate unique, sorted categories","16da8f6b":"## Adding three levels of clustering to the input data","b3945a5a":"## Words \"similar\" to \"Dupin\"?","9a919507":"### A note for the untagged stories: Maelzel's Chess Player (https:\/\/en.wikipedia.org\/wiki\/Maelzel%27s_Chess_Player,  https:\/\/www.eapoe.org\/works\/essays\/maelzel.htm), practically a detective work from Poe, is clustered with some detective fiction stories, and the other untagged works (colored in green), are clustered with sketch stories (https:\/\/www.britannica.com\/art\/literary-sketch).\n","4aa5638f":"## Gathering bibliographic information from Wikipedia","5a1cb23e":"## 10 main words per topic","aa2f62a0":"## The final output","dc53a20e":"Notes\n\n<sup> 1 <\/sup> This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.  You may copy it, give it away or\nre-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org","42f53544":"## Sorting the stories using the title order","4172735a":"## jump to <span style=\"color:blue\">Section 2<\/span> in case you are only interested in the analysis! The goal of <span style=\"color:blue\">Section 1<\/span> is to make the workflow fully reproducible","0597e602":"\n# Clustering documents, modeling topics and generating text recommendations:\n\n\n## A Natural Language Processing approach using the Edgar Allan Poe's corpus of short stories\n\n\n<img src=\" https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F893618%2F1516276%2Fpoe.png?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1601537101&Signature=Jd9JyOxwEkZQMpd7bRUXaimp4UJqcgARpWX0QmjHWYcqsVbr%2BJvLOq1Wcir6315AshMBWMjM9SU0710qBBZgLgZNIx1C4nNCl97ZhNdZnJZe6TEUjPGkuKyOarqpBzOx9wDGgu7zya6%2BsmMlI6pjvxoILQOq%2B%2FXfJru2BprUrMuij52b6dua6rMCGD9EmNBZGYqpzCR21PCuRgHxmcWqE0y16ewQaZgHg2fjj6L%2B9womOLJfKuY1yperlmxV56OLl%2BpqNmhsucQgt8esaW5I5OiLgQgptRAF380v%2B%2Brh3iYFmlpWDhNHa3szGs1w2bWgzo78fnUds0Xnf8PVW6dP8w%3D%3D\" width=\"400px\">\n","1f93b96e":"## Recommendations (three per story)","e28c2563":"# 6. KNN clustering of the Word2vec matrix","bfe16eeb":"## Date normalization","413eea76":"\n## <ins> Poe's corpus in context <\/ins>\n\nEdgar Allan Poe was an American short story writer, poet and critic, born in Boston in 1809. His short stories encompassed several topics: detective fiction (he is the creator of the modern detective fiction genre), science fiction, horror and humor. Poe's narratives are often envolved with the characteristic aesthetic of the Romantic movement (present in classical books of the same era, such as Frankenstein or Dracula).\n\n\n## <ins> Methods <\/ins>\n\nIn this notebook, I will explore the Poe's corpus of 70 short stories to perform several actions:\n\n\n- To find hidden relations among elements of the corpus, based on semantics and style <span style=\"color:green\">-> Word2vec <\/span>\n\n- To elaborate a document level (i.e., \"story-level\") classification <span style=\"color:green\">-> cosine-similarity of the Word2vec matrix + clustering <\/span>\n\n- To tag each document with 5-10 main topics  <span style=\"color:green\">-> Latent Dirichlet Allocation, mediated by some optimization <\/span>\n\n- To generate a simple recommender system. For example, if I read one of the stories and I liked it... which story could I pick next based on this preference? <span style=\"color:green\">-> Analyzing with KNN the Word2vec matrix <\/span>\n\n\n\n## <ins>Data source<\/ins>\n\n- Edgar Allan Poe's corpus of short stories, available as raw HTML at Project Gutenberg<sup>1<\/sup>:\nhttp:\/\/www.gutenberg.org\/files\/25525\/25525-h\/25525-h.htm#2150link2H_4_0003\n\n### -> You can find a processed version of the generated dataset attached to this kernel for your own experiments! <- \n[](http:\/\/)\n\n- A table from Wikipedia with information about the stories: https:\/\/en.wikipedia.org\/wiki\/Edgar_Allan_Poe_bibliography \nThe information was merged with the Project Gutenberg data.\n","f5df9c41":"# 3. Text normalization and creation of word embeddings ","46c13571":"# 4. Visualization of results","96728e8e":"## Wikipedia information formatting. Matching this table with the titles of the data downloaded from Project Gutenberg. For each title, extract genre and date from the table of Wikipedia","e72536f6":"# 1. Preprocessing the downloaded webpage with Poe's stories \/ curating the Wikipedia information","54b522f2":"## And what about \"clock\"?"}}