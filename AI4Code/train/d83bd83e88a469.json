{"cell_type":{"2b83ca1f":"code","44328bdd":"code","9a42e264":"code","0d45bcfe":"code","04b24acd":"code","1005a63a":"code","0323a7bd":"code","4bfc4d1d":"code","a8ca0605":"code","ddcce748":"code","457a4780":"code","2ab5691f":"code","c13ebf5f":"code","d7445322":"code","4446c244":"code","4f2d14aa":"code","b796af72":"code","4ec85659":"code","e088295e":"code","fc0bc426":"code","1c4e0351":"code","0be41344":"code","00ad8a6c":"code","ed34bc14":"code","a1c0b821":"code","337e0cd6":"code","87b27a92":"markdown","62f52f5d":"markdown","ba098e81":"markdown","1dabb6bf":"markdown","52d3064c":"markdown","4c3d00a0":"markdown"},"source":{"2b83ca1f":"import os\nimport sys\nsys.path.append('..\/input\/torchez\/')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport math\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport torchez as ez\n\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AdamW\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight","44328bdd":"df = pd.read_csv('..\/input\/tweet-mental-health-classification\/train.csv')","9a42e264":"df.head()","0d45bcfe":"all_lens = [len(t) for t in df['tweets'].values]\ndf['lens'] = all_lens","04b24acd":"sns.set(rc={\n    'axes.facecolor':'#ffeacb', \n    'figure.facecolor':'white', \n    'axes.edgecolor':'#ffeacb', \n    \"axes.grid\":False,\n    'image.cmap':'rocket'\n    }\n)","1005a63a":"sns.stripplot(x=\"labels\", y=\"lens\", data=df)\nsns.despine(left=True, bottom=True)","0323a7bd":"sns.boxplot(data=df, x='labels', y='lens')\nplt.show()","4bfc4d1d":"le = LabelEncoder()\ndf['labels'] = le.fit_transform(df['labels'].values)","a8ca0605":"traindf, validdf = train_test_split(df, test_size=0.2, shuffle=True, random_state=2022)","ddcce748":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","457a4780":"cv = TfidfVectorizer(stop_words='english', max_features=200000)\ncv.fit_transform(list(traindf['tweets'].values) + list(validdf['tweets'].values))","2ab5691f":"xtrain = cv.transform(traindf['tweets'].values)\nxvalid = cv.transform(validdf['tweets'].values)","c13ebf5f":"ytrain = traindf['labels'].values\nyvalid = validdf['labels'].values","d7445322":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(C= 10, multi_class='multinomial', solver='sag')\n\nlr.fit(xtrain, ytrain)\nypred = lr.predict(xvalid)\n\naccuracy_score(yvalid, ypred)","4446c244":"tfidf = TfidfVectorizer(stop_words='english', ngram_range=(3,5), max_features=200000)\ntfidf.fit_transform(list(traindf['tweets'].values) + list(validdf['tweets'].values))","4f2d14aa":"xtrain = tfidf.transform(traindf['tweets'].values)\nxvalid = tfidf.transform(validdf['tweets'].values)","b796af72":"ytrain = traindf['labels'].values\nyvalid = validdf['labels'].values","4ec85659":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(C=5, multi_class='multinomial', solver='sag')\n\nlr.fit(xtrain, ytrain)\nypred = lr.predict(xvalid)\n\naccuracy_score(yvalid, ypred)","e088295e":"class Config:\n    batch_size = 64\n    test_batch_size = 128\n    lr = 1e-5\n    num_classes = 4\n    max_len = 128\n    model = 'vinai\/bertweet-base'\n    decay = 0.001\n    load_model = None\n    class_weight_compute = 'batch'\n    device = 'cuda'","fc0bc426":"class TweetModel(ez.Model):\n    def __init__(self):\n        super(TweetModel, self).__init__()\n        self.config = AutoConfig.from_pretrained(args.model)\n        self.roberta = AutoModel.from_pretrained(args.model, config=self.config)\n        self.out = nn.Linear(self.config.hidden_size, args.num_classes)\n        self.dropout = nn.Dropout(0.3)\n        \n    def forward(self, input_ids, attention_mask):\n        x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        x = self.dropout(x[1]) ## [CLS] : Taking the classifier token\n        out = self.out(x)\n        return out\n    \n    def loss_fn(self, output, mask, target, num_labels, class_weight=None):\n        loss = nn.CrossEntropyLoss(weight=class_weight)(output.view(-1, num_labels), target.view(-1))\n        return {'loss':loss}\n    \n    def get_optimizer(self):\n        optimizer = AdamW(self.parameters(), lr=args.lr, weight_decay=args.decay)\n        return optimizer\n    \n    def training_step(self, input_ids, attention_mask, target):\n        outputs = self(input_ids, attention_mask)\n        if args.class_weight_compute == 'batch':\n            class_weight = compute_class_weight(classes=n_y, y=target.cpu().detach().numpy().flatten(), class_weight='balanced')\n            class_weight = torch.tensor(class_weight, dtype=torch.float).to(args.device)\n        loss = self.loss_fn(outputs, attention_mask, target, args.num_classes, class_weight)\n        metric = self.metrics(outputs, target)\n\n        return {**loss, **metric}\n    \n    def validation_step(self, input_ids, attention_mask, target):\n        outputs = self(input_ids, attention_mask)\n        loss = self.loss_fn(outputs, attention_mask, target, args.num_classes)\n        metric = self.metrics(outputs, target)\n        \n        return {**loss, **metric}\n    \n    def prediction_step(self, input_ids, attention_mask):\n        outputs = self(input_ids, attention_mask)\n        outputs = torch.argmax(torch.softmax(outputs, dim=1), axis=1)\n        return outputs\n    \n    def metrics(self, outputs, targets):\n        outputs = torch.argmax(torch.softmax(outputs, dim=1), axis=1).cpu().detach().numpy()\n        targets = targets.cpu().detach().numpy().flatten()\n        diff = accuracy_score(outputs, targets)\n        return {'accuracy': diff}","1c4e0351":"class TweetDataset:\n    def __init__(self, texts, labels=None):\n        self.texts = texts\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        if self.labels is not None:\n            labels = self.labels[idx]\n        \n        tokenized_text = tokenizer.encode_plus(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=args.max_len\n        )\n        \n        input_ids = tokenized_text['input_ids']\n        attention_mask = tokenized_text['attention_mask']\n        \n        if self.labels is not None:\n            return {\n                'input_ids': torch.tensor(input_ids, dtype=torch.long),\n                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n                'target': torch.tensor(labels, dtype=torch.long)\n            }\n        return {\n            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n        }","0be41344":"train_dataset = TweetDataset(traindf['tweets'].values, traindf['labels'].values)\nvalid_dataset = TweetDataset(validdf['tweets'].values, validdf['labels'].values)","00ad8a6c":"n_y = traindf['labels'].unique()","ed34bc14":"testdf = pd.read_csv('..\/input\/tweet-mental-health-classification\/test.csv')\ntest_dataset = TweetDataset(testdf['tweets'].values)","a1c0b821":"args = Config()\nmodel = TweetModel()\ntokenizer = AutoTokenizer.from_pretrained(args.model, add_prefix_space=True)\n\nif args.class_weight_compute == 'global':\n    class_weight = compute_class_weight(classes=n_y, y=traindf['labels'].values, class_weight='balanced')\n    class_weight = torch.tensor(class_weight, dtype=torch.float).to(args.device)\n\nif args.load_model is None:\n    model.fit(\n        train_dataset=train_dataset,\n        train_batch_size=args.batch_size,\n        valid_dataset=valid_dataset,\n        valid_batch_size=args.batch_size*2,\n        device='cuda',\n        epochs=10,\n        save=True,\n        es=True,\n        es_monitor='valid_accuracy',\n        es_epochs=1,\n        es_mode='max',\n        model_path='tweetmodel.bin'\n    )\n    \n    preds = model.predict(test_dataset, device='cuda', batch_size=args.test_batch_size)\n    \n    \nelse:\n    preds = model.predict(test_dataset, device='cuda', batch_size=args.test_batch_size, model_path=args.load_model)","337e0cd6":"preds = le.inverse_transform(preds)\nsubdf = testdf.copy()\nsubdf['labels'] = preds\nsubdf['id'] = subdf.index\nsubdf[['id', 'labels']].to_csv('submission.csv', index=False)","87b27a92":"# RoBERTa Baseline","62f52f5d":"### TF-IDF Vectorizer","ba098e81":"### If you like the notebook please UPVOTE!. Every upvote is appreciated. Thanks!","1dabb6bf":"# Logistic Regression ","52d3064c":"### Count Vectorizer","4c3d00a0":"# Data Preparation"}}