{"cell_type":{"2293a546":"code","c5aca8f7":"code","6d0aa270":"code","1ddad3fe":"code","0e8ab26e":"code","aa06aabc":"code","6bd54edb":"code","c378bf02":"code","2a197c98":"code","fe8acff3":"code","0f747275":"code","9fd7ad59":"code","15f6b5f3":"markdown","a4746316":"markdown","0234fe8e":"markdown","76e53b55":"markdown","b4ce6463":"markdown","5c18fcdb":"markdown","f8efd7a9":"markdown","8532f508":"markdown","3b49cc73":"markdown","6c4dcafb":"markdown","0de26d79":"markdown"},"source":{"2293a546":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c5aca8f7":"import keras\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom keras.applications import VGG19\nfrom keras.models import Sequential,Model\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing.image import ImageDataGenerator,img_to_array\nfrom keras.layers import Flatten,Dense,Dropout","6d0aa270":"img_size = [224,224]\nvgg_model = VGG19(include_top = False,weights = 'imagenet',input_shape = img_size+[3])\nfor i in vgg_model.layers:\n    i.trainable = False","1ddad3fe":"x = Flatten()(vgg_model.output)\nx = Dense(1024,activation = 'relu',kernel_initializer='he_uniform')(x)\nx = Dropout(0.5)(x)\nx = Dense(512,activation = 'relu',kernel_initializer='he_uniform')(x)\nprediction = Dense(1,activation = 'sigmoid')(x)\nmodel = Model(inputs = vgg_model.input,outputs = prediction)\n","0e8ab26e":"model.summary()","aa06aabc":"data_gen = ImageDataGenerator(rescale= 1.0\/255.0,validation_split=0.3)\n\ntrain =data_gen.flow_from_directory('..\/input\/cell-images-for-detecting-malaria\/cell_images\/cell_images',\n                                    target_size = (224,224),class_mode = 'binary',\n                                    batch_size = 64,subset = 'training'\n                                    )\n\nvalidation = data_gen.flow_from_directory('..\/input\/cell-images-for-detecting-malaria\/cell_images\/cell_images',\n                                          target_size = (224,224),class_mode = 'binary',\n                                    batch_size = 64,subset = 'validation')\n\nclasses = ['Parasitized', 'Uninfected']","6bd54edb":"model.compile('adam',loss = 'binary_crossentropy',metrics = ['accuracy'])\nes = EarlyStopping(monitor='val_loss',patience=3)\ndata = model.fit(train,epochs = 20,callbacks = [es],validation_data = validation)","c378bf02":"ypred = model.predict(validation,verbose = 1)\nperformance =model.evaluate_generator(validation)\nprint(f\"loss: {performance[0]}\")\nprint(f\"accuracy: {performance[1]}\")","2a197c98":"img = keras.preprocessing.image.load_img('..\/input\/cell-images-for-detecting-malaria\/cell_images\/Parasitized\/C100P61ThinF_IMG_20150918_144104_cell_162.png',target_size=(224,224))\nimg = img_to_array(img)\nimg =  img\/255.0\nimg = np.array([img])\n(model.predict(img)>0.5).astype(int)","fe8acff3":"img = keras.preprocessing.image.load_img('..\/input\/cell-images-for-detecting-malaria\/cell_images\/Uninfected\/C100P61ThinF_IMG_20150918_144348_cell_108.png',target_size=(224,224))\nimg = img_to_array(img)\nimg =  img\/255.0\nimg = np.array([img])\n(model.predict(img)>0.5).astype(int)","0f747275":"loss_train = data.history['loss']\nloss_val = data.history['val_loss']\nepochs = range(1,12)\nplt.plot(epochs,loss_train,'g',label = 'training loss')\nplt.plot(epochs,loss_val,'b',label = 'validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","9fd7ad59":"loss_train = data.history['accuracy']\nloss_val = data.history['val_accuracy']\nepochs = range(1,12)\nplt.plot(epochs,loss_train,'g',label = 'training accuracy')\nplt.plot(epochs,loss_val,'b',label = 'validation accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","15f6b5f3":"### **Testing the model on random images from the dataset**","a4746316":"### **Training vs Validation Accuracy**","0234fe8e":"### **Training the Model**","76e53b55":"### **Predicting and Evaluating the model**","b4ce6463":"### **Training vs Validation Loss**","5c18fcdb":"### **Importing the required libraries**","f8efd7a9":"### **Calling the Pretrained model VGG19**","8532f508":"### **Visualizing The Loss and Accuracy of the Model**\n\n\n","3b49cc73":"### **Summary of the model**","6c4dcafb":"### **Adding the Output layer to the model**","0de26d79":"### **Data Augmentation**"}}