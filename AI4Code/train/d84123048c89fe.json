{"cell_type":{"fce4dac1":"code","ad257099":"code","a9b88648":"code","192bb43b":"code","296d2b08":"code","c6ff7eb6":"code","42f32323":"code","40b69c2c":"code","192fd840":"code","bedb9354":"code","989bafdb":"code","99cf12a7":"code","e36b0d4f":"code","71588eca":"code","8f10a242":"code","754d8643":"code","224c16e5":"code","ecea381e":"code","30b61f76":"code","3a4a4240":"code","683fff45":"code","a6737747":"code","0119a873":"code","1a81e8f7":"code","6ebbad68":"code","f8ef32dc":"code","ca5cc8c2":"code","4f4e299e":"code","e5b19d5c":"code","6f5039b0":"code","7e225ebf":"code","e7e496ee":"code","2ba5284d":"code","dd387e62":"code","4a2dcb02":"code","48cc458c":"code","254be553":"code","5eaa48ef":"code","4a4c7735":"code","af9a5785":"code","55daeef1":"code","86f93ce6":"code","ad5be31f":"code","cf6efc93":"code","4f55bbbb":"code","5747c28a":"code","8a351bbf":"code","c29c0b9a":"code","89d2af5b":"code","6540d832":"code","3c2a0f59":"code","f4ead462":"code","f6a64203":"code","63293f52":"code","794134a8":"code","afeda83b":"code","7fe69ba3":"code","4298dacc":"code","1847b782":"code","f28839d3":"code","67b8db28":"code","83b84b8e":"code","c224ff19":"code","8ff3b48f":"code","a6e5892b":"code","3ff2d125":"code","540c3e71":"code","1ff070a5":"code","76c53eac":"code","5aa0bd6e":"code","96e102f3":"code","dce79f11":"code","f726da50":"code","45ce6873":"code","504da2eb":"code","49798f2b":"code","13a83642":"code","f3698ae6":"code","e54c77b8":"code","501c048e":"code","c1151f1a":"code","835bf6be":"code","12324a13":"code","648a84ff":"code","27922aec":"code","41a82748":"code","df9b0e2a":"code","c4c7f8e8":"code","e0d21826":"code","044d277e":"code","5d31f01f":"code","9763bcdd":"code","55b6a8da":"code","881e7b9d":"code","b893977b":"code","0c9c84f6":"code","4c37b516":"code","ddd238d0":"code","abe8f40f":"code","07b43807":"code","57884a29":"code","ba195265":"code","4e60fe6c":"code","7786e27b":"code","c78f8d97":"code","7a0928e5":"code","2d6ab76d":"code","63827f4c":"code","82e56e3e":"code","520a3970":"code","2b11ed00":"code","5487b4d7":"code","cdf0a803":"code","4471b785":"code","179cda68":"code","ea216b6c":"code","8168f4eb":"code","7657d8c3":"code","356323a8":"code","2a3d943c":"code","d75587b6":"code","42f2d612":"code","33185682":"code","0e57887d":"code","9f3cd474":"code","951bcc5d":"code","23e21c98":"code","7aebb5fb":"code","92820979":"code","61972fc1":"code","12065a2f":"code","6b6b1440":"code","07095b37":"code","7ee427af":"code","1e9c27d8":"code","dad508a9":"code","b9073f92":"code","8bc433b2":"code","679295a5":"code","2e24eedc":"code","ef9eadff":"code","cdb309e1":"code","f19c2175":"code","449428fc":"code","46d45b23":"code","49ab6c7e":"code","a82d2919":"code","728e526c":"code","d8e46a84":"markdown","04ef168f":"markdown","0755fb88":"markdown","2055c0df":"markdown","f69c1bab":"markdown","f03a5b39":"markdown","372569b4":"markdown","601d859e":"markdown","6e365bd2":"markdown","0f8d150d":"markdown","5b3ac1ba":"markdown","74e73d3f":"markdown","150ef952":"markdown","0b687b4e":"markdown","bd528d87":"markdown","7cb5daac":"markdown","21a69bc3":"markdown","25ee0d99":"markdown","2f65552c":"markdown","6ea23e25":"markdown","07717a40":"markdown","a4a4e127":"markdown","21bb9aff":"markdown","8068a981":"markdown","8aa82aac":"markdown","03db40bc":"markdown","5f44c58e":"markdown","db13068b":"markdown","b9546b18":"markdown","d8af0e63":"markdown","f56b50f6":"markdown","959b45f8":"markdown","6e0ba462":"markdown","c9c265ea":"markdown","5defb649":"markdown","d17a8c79":"markdown","20ad05a7":"markdown","5f86cc34":"markdown","5aecd47e":"markdown","3f0b44ce":"markdown","5d4811a7":"markdown","284a6c12":"markdown","78708d44":"markdown","e16ec613":"markdown","ea3a9978":"markdown","eb3a62c5":"markdown","2e648ace":"markdown","8a33755f":"markdown","a744d21b":"markdown","fee36934":"markdown","7e120bc4":"markdown","2956aef3":"markdown","1ecf98cd":"markdown","d1f679c3":"markdown","a8ce21af":"markdown","34fe7638":"markdown","2ed9bd87":"markdown","251cc8a5":"markdown","ee9d5580":"markdown","67116fb5":"markdown","9c18eda4":"markdown","a0278466":"markdown","25806270":"markdown","a6673725":"markdown","8d674592":"markdown","97c1dc89":"markdown","4285f8c0":"markdown","7e3d6286":"markdown","4ef3b4d9":"markdown","73b63709":"markdown","35b73039":"markdown","75aeb5ed":"markdown","daf954cb":"markdown","a722e5f0":"markdown","7ed7865e":"markdown","879dcd4b":"markdown","fec797e7":"markdown","c0ff98d3":"markdown","b8a1bdb4":"markdown","3cdbe22d":"markdown","6ef54366":"markdown","1576e9ad":"markdown","a35cae7c":"markdown","9c6631cb":"markdown","020f2410":"markdown","5e72510f":"markdown","0b1fc8be":"markdown","ce19119a":"markdown","9bd717ce":"markdown","38bca8cc":"markdown","a73db4fa":"markdown","d2e7f59a":"markdown","f1411ae6":"markdown","dbcdde5e":"markdown","f05fe5d2":"markdown","f4ab1ca6":"markdown","733e0370":"markdown","b50565f8":"markdown","6c4bb6a2":"markdown","45630733":"markdown","396e7f4c":"markdown","03d5c2a7":"markdown","cf1d6b01":"markdown","29c60ec4":"markdown","24f5d845":"markdown","f73feeba":"markdown","c13f09a5":"markdown","49fd3d04":"markdown","aa933f1f":"markdown","243a8780":"markdown","dd583481":"markdown","99593838":"markdown","14f682cb":"markdown","897d47ea":"markdown","2ae140ce":"markdown","6614a5a7":"markdown"},"source":{"fce4dac1":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os\n\n# from plotly import plotly\nimport plotly.offline as offline\nimport plotly.graph_objs as go\noffline.init_notebook_mode()\nfrom collections import Counter\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(\"DONE ---------------------------------------\")","ad257099":"project_data = pd.read_csv('..\/input\/train_data.csv') \nresource_data = pd.read_csv('..\/input\/resources.csv')\nprint(\"Done\")","a9b88648":"project_data.shape","192bb43b":"project_data.columns.values","296d2b08":"prefixlist=project_data['teacher_prefix'].values\nprefixlist=list(prefixlist)\ncleanedPrefixList = [x for x in project_data['teacher_prefix'] if x != float('nan')] ## Cleaning the NULL Values in the list -> https:\/\/stackoverflow.com\/a\/50297200\/4433839\n\nlen(cleanedPrefixList)\n# print(len(prefixlist))","c6ff7eb6":"## Converting to Nan and Droping -> https:\/\/stackoverflow.com\/a\/29314880\/4433839\n\n# df[df['B'].str.strip().astype(bool)] \/\/ for deleting EMPTY STRINGS.\nproject_data.dropna(subset=['teacher_prefix'], inplace=True)\nproject_data.shape","42f32323":"print(\"Number of data points in train data\", resource_data.shape)\nprint(resource_data.columns.values)\nresource_data.head(2)","40b69c2c":"catogories = list(project_data['project_subject_categories'].values)\n# remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\ncat_list = []\nfor i in catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_') # we are replacing the & value into \n    cat_list.append(temp.strip())\n    \nproject_data['clean_categories'] = cat_list\nproject_data.drop(['project_subject_categories'], axis=1, inplace=True)\n\nfrom collections import Counter\nmy_counter = Counter()\nfor word in project_data['clean_categories'].values:\n    my_counter.update(word.split())\n\ncat_dict = dict(my_counter)\nsorted_cat_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")\n","192fd840":"sub_catogories = list(project_data['project_subject_subcategories'].values)\n# remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\n\nsub_cat_list = []\nfor i in sub_catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp +=j.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_')\n    sub_cat_list.append(temp.strip())\n\nproject_data['clean_subcategories'] = sub_cat_list\nproject_data.drop(['project_subject_subcategories'], axis=1, inplace=True)\n\n# count of all the words in corpus python: https:\/\/stackoverflow.com\/a\/22898595\/4084039\nmy_counter = Counter()\nfor word in project_data['clean_subcategories'].values:\n    my_counter.update(word.split())\n    \nsub_cat_dict = dict(my_counter)\nsorted_sub_cat_dict = dict(sorted(sub_cat_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")","bedb9354":"print(stop)","989bafdb":"project_data['project_grade_category'].values","99cf12a7":"# this code removes \" \" and \"-\". ie Grades 3-5 -> grage3to5\n#  remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\nclean_grades=[]\nfor project_grade in project_data['project_grade_category'].values:\n    project_grade=str(project_grade).lower().strip().replace(' ','').replace('-','to')\n    clean_grades.append(project_grade.strip())","e36b0d4f":"len(clean_grades)","71588eca":"project_data['clean_project_grade_category']=clean_grades","8f10a242":"project_data.columns","754d8643":"project_data['clean_project_grade_category'].values","224c16e5":"project_data['clean_project_grade_category']=clean_grades\nproject_data.drop(['project_grade_category'],axis=1,inplace=True)","ecea381e":"my_counter = Counter()\nfor word in project_data['clean_project_grade_category'].values:\n    my_counter.update(word.split())","30b61f76":"    \ngrade_dict = dict(my_counter)\nsorted_project_grade_cat_dict = dict(sorted(grade_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")","3a4a4240":"# merge two column text dataframe: \nproject_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n                        project_data[\"project_essay_2\"].map(str) + \\\n                        project_data[\"project_essay_3\"].map(str) + \\\n                        project_data[\"project_essay_4\"].map(str)","683fff45":"project_data.head(2)","a6737747":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","0119a873":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","1a81e8f7":"# Combining all the above stundents \nfrom tqdm import tqdm\npreprocessed_essays = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(project_data['essay'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n    preprocessed_essays.append(sent.lower().strip())","6ebbad68":"project_data.columns","f8ef32dc":"## new column added as PreProcessed_Essays and older unProcessed essays column is deleted\nproject_data['preprocessed_essays'] = preprocessed_essays\nproject_data.drop(['essay'], axis=1, inplace=True)","ca5cc8c2":"project_data.columns","4f4e299e":"# after preprocesing\npreprocessed_essays[20000]","e5b19d5c":"# similarly you can preprocess the titles also\n# similarly you can preprocess the titles also\n# similarly you can preprocess the titles also\nfrom tqdm import tqdm\npreprocessed_titles = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(project_data['project_title'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n    preprocessed_titles.append(sent.lower().strip())","6f5039b0":"#https:\/\/stackoverflow.com\/questions\/26666919\/add-column-in-dataframe-from-list\/3849072\nproject_data['preprocessed_titles'] = preprocessed_titles\n# project_data.drop(['project_title'], axis=1, inplace=True)","7e225ebf":"project_data.columns","e7e496ee":"project_data.shape","2ba5284d":"price_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nprice_data.head(2)","dd387e62":"project_data = pd.merge(project_data, price_data, on='id', how='left')","4a2dcb02":"project_data.shape","48cc458c":"project_bkp=project_data.copy()","254be553":"project_bkp.shape","5eaa48ef":"## taking random samples of 100k datapoints\nproject_data = project_bkp.sample(n = 2000) \n# resource_data = pd.read_csv('..\/resources.csv')\n\nproject_data.shape\n\n# y_value_counts = row1['project_is_approved'].value_counts()\ny_value_counts = project_data['project_is_approved'].value_counts()\nprint(\"Number of projects thar are approved for funding:     \", y_value_counts[1],\" -> \",round(y_value_counts[1]\/(y_value_counts[1]+y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects thar are not approved for funding: \", y_value_counts[0],\" -> \",round(y_value_counts[0]\/(y_value_counts[1]+y_value_counts[0])*100,2),\"%\")","4a4c7735":"# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n# from sklearn.model_selection import train_test_split\n# x_train,x_test,y_train,y_test=train_test_split(\n#     project_data.drop('project_is_approved', axis=1),\n#     project_data['project_is_approved'].values,\n#     test_size=0.3,\n#     random_state=42,\n#     stratify=project_data[['project_is_approved']])\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(\n    project_data,\n    project_data['project_is_approved'],\n    test_size=0.2,\n    random_state=42,\n    stratify=project_data[['project_is_approved']])\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test : \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test : \",y_test.shape)","af9a5785":"x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size=0.25, stratify=y_train)\n\nprint(\"x_train: \",x_train.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"x_cv   : \",x_cv.shape)\nprint(\"x_cv   : \",y_cv.shape)\nprint(\"x_test : \",x_test.shape)\nprint(\"y_test : \",y_test.shape)","55daeef1":"# y_value_counts = row1['project_is_approved'].value_counts()\nprint(\"X_TRAIN-------------------------\")\nx_train_y_value_counts = x_train['project_is_approved'].value_counts()\nprint(\"Number of projects that are approved for funding    \", x_train_y_value_counts[1],\" -> \",round(x_train_y_value_counts[1]\/(x_train_y_value_counts[1]+x_train_y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects that are not approved for funding \",x_train_y_value_counts[0],\" -> \",round(x_train_y_value_counts[0]\/(x_train_y_value_counts[1]+x_train_y_value_counts[0])*100,2),\"%\")\nprint(\"\\n\")\n# y_value_counts = row1['project_is_approved'].value_counts()\nprint(\"X_TEST--------------------------\")\nx_test_y_value_counts = x_test['project_is_approved'].value_counts()\nprint(\"Number of projects that are approved for funding    \", x_test_y_value_counts[1],\" -> \",round(x_test_y_value_counts[1]\/(x_test_y_value_counts[1]+x_test_y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects that are not approved for funding \",x_test_y_value_counts[0],\" -> \",round(x_test_y_value_counts[0]\/(x_test_y_value_counts[1]+x_test_y_value_counts[0])*100,2),\"%\")\nprint(\"\\n\")\n# y_value_counts = row1['project_is_approved'].value_counts()\nprint(\"X_CV----------------------------\")\nx_cv_y_value_counts = x_cv['project_is_approved'].value_counts()\nprint(\"Number of projects that are approved for funding    \", x_cv_y_value_counts[1],\" -> \",round(x_cv_y_value_counts[1]\/(x_cv_y_value_counts[1]+x_cv_y_value_counts[0])*100),2,\"%\")\nprint(\"Number of projects that are not approved for funding \",x_cv_y_value_counts[0],\" -> \",round(x_cv_y_value_counts[0]\/(x_cv_y_value_counts[1]+x_cv_y_value_counts[0])*100),2,\"%\")\nprint(\"\\n\")","86f93ce6":"from sklearn.utils import resample\n\n## splitting x_train in their respective classes\nx_train_majority=x_train[x_train.project_is_approved==1]\nx_train_minority=x_train[x_train.project_is_approved==0]\n\nprint(\"No. of points in the Training Dataset : \", x_train.shape)\nprint(\"No. of points in the majority class 1 : \",len(x_train_majority))\nprint(\"No. of points in the minority class 0 : \",len(x_train_minority))\n\nprint(x_train_majority.shape)\nprint(x_train_minority.shape)\n\n## Resampling with replacement\nx_train_minority_upsampled=resample(\n    x_train_minority,\n    replace=True,\n\n    n_samples=len(x_train_majority),\n    random_state=123)\n\nprint(\"Resampled Minority class details\")\nprint(\"Type:  \",type(x_train_minority_upsampled))\nprint(\"Shape: \",x_train_minority_upsampled.shape)\nprint(\"\\n\")\n## Concatinating our Upsampled Minority class with the existing Majority class\nx_train_upsampled=pd.concat([x_train_majority,x_train_minority_upsampled])\n\nprint(\"Upsampled Training data\")\nprint(\"Total number of Class labels\")\nprint(x_train_upsampled.project_is_approved.value_counts())\nprint(\"\\n\")\nprint(\"Old Training IMBALANCED Dataset Shape         : \", x_train.shape)\nprint(\"New Training BALANCED Upsampled Dataset Shape : \",x_train_upsampled.shape)\n\nx_train_upsampled.to_csv ('x_train_upsampled_csv.csv',index=False)","ad5be31f":"yy_train=x_train_upsampled['project_is_approved'].copy()","cf6efc93":"yy_train.shape","4f55bbbb":"x_train_upsampled.shape","5747c28a":"x_test.shape","8a351bbf":"x_cv.shape","c29c0b9a":"project_data.columns","89d2af5b":"# we use count vectorizer to convert the values into one \nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=list(sorted_cat_dict.keys()), lowercase=False, binary=True)\n\nvectorizer.fit(x_train_upsampled['clean_categories'].values)\n\nx_train_categories_one_hot = vectorizer.transform(x_train_upsampled['clean_categories'].values)\nx_cv_categories_one_hot    = vectorizer.transform(x_cv['clean_categories'].values)\nx_test_categories_one_hot  = vectorizer.transform(x_test['clean_categories'].values)\n\n\nprint(vectorizer.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> categories: x_train: \",x_train_categories_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> categories: x_cv   : \",x_cv_categories_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> categories: x_test : \",x_test_categories_one_hot.shape)","6540d832":"# we use count vectorizer to convert the values into one \nvectorizer = CountVectorizer(vocabulary=list(sorted_sub_cat_dict.keys()), lowercase=False, binary=True)\n\nvectorizer.fit(x_train_upsampled['clean_subcategories'].values)\n\nx_train_sub_categories_one_hot = vectorizer.transform(x_train_upsampled['clean_subcategories'].values)\nx_cv_sub_categories_one_hot    = vectorizer.transform(x_cv['clean_subcategories'].values)\nx_test_sub_categories_one_hot  = vectorizer.transform(x_test['clean_subcategories'].values)\n\nprint(vectorizer.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> sub_categories: x_train: \",x_train_sub_categories_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> sub_categories: x_cv   : \",x_cv_sub_categories_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> sub_categories: x_test : \",x_test_sub_categories_one_hot.shape)","3c2a0f59":"my_counter = Counter()\nfor state in project_data['school_state'].values:\n    my_counter.update(state.split())","f4ead462":"school_state_cat_dict = dict(my_counter)\nsorted_school_state_cat_dict = dict(sorted(school_state_cat_dict.items(), key=lambda kv: kv[1]))","f6a64203":"from scipy import sparse ## Exporting Sparse Matrix to NPZ File -> https:\/\/stackoverflow.com\/questions\/8955448\/save-load-scipy-sparse-csr-matrix-in-portable-data-format\nstatelist=list(project_data['school_state'].values)\nvectorizer = CountVectorizer(vocabulary=set(statelist), lowercase=False, binary=True)\n\nvectorizer.fit(x_train_upsampled['school_state'])\n\nx_train_school_state_one_hot = vectorizer.transform(x_train_upsampled['school_state'].values)\nx_cv_school_state_one_hot    = vectorizer.transform(x_cv['school_state'].values)\nx_test_school_state_one_hot  = vectorizer.transform(x_test['school_state'].values)\n\nprint(vectorizer.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> school_state: x_train: \",x_train_school_state_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> school_state: x_cv   : \",x_cv_school_state_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> school_state: x_test : \",x_test_school_state_one_hot.shape)\n# school_one_hot = vectorizer.transform(statelist)\n# print(\"Shape of matrix after one hot encodig \",school_one_hot.shape)\n# print(type(school_one_hot))\n# sparse.save_npz(\"school_one_hot_export.npz\", school_one_hot) \n# print(school_one_hot.toarray())","63293f52":"my_counter = Counter()\nfor teacher_prefix in project_data['teacher_prefix'].values:\n    teacher_prefix = str(teacher_prefix).lower().replace('.','').strip()\n    \n    my_counter.update(teacher_prefix.split())\nteacher_prefix_cat_dict = dict(my_counter)\nsorted_teacher_prefix_cat_dict = dict(sorted(teacher_prefix_cat_dict.items(), key=lambda kv: kv[1]))","794134a8":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=list(sorted_teacher_prefix_cat_dict.keys()), lowercase=False, binary=True)\n\nvectorizer.fit(x_train_upsampled['teacher_prefix'].values)\n\nx_train_prefix_one_hot = vectorizer.transform(x_train_upsampled['teacher_prefix'].values)\nx_cv_prefix_one_hot    = vectorizer.transform(x_cv['teacher_prefix'].values)\nx_test_prefix_one_hot  = vectorizer.transform(x_test['teacher_prefix'].values)\n\nprint(vectorizer.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> prefix: x_train: \",x_train_prefix_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> prefix: x_test : \",x_test_prefix_one_hot.shape)","afeda83b":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=list(sorted_project_grade_cat_dict.keys()), lowercase=False, binary=True)\n\nvectorizer.fit(x_train_upsampled['clean_project_grade_category'].values)\n\nx_train_grade_category_one_hot = vectorizer.transform(x_train_upsampled['clean_project_grade_category'].values)\nx_cv_grade_category_one_hot    = vectorizer.transform(x_cv['clean_project_grade_category'].values)\nx_test_grade_category_one_hot  = vectorizer.transform(x_test['clean_project_grade_category'].values)\n\nprint(vectorizer.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> project_grade: x_train : \",x_train_grade_category_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> project_grade: x_cv    : \",x_cv_grade_category_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> project_grade: x_test  : \",x_test_grade_category_one_hot.shape)\n","7fe69ba3":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\nfrom sklearn.preprocessing import StandardScaler\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n\nprice_scalar = StandardScaler()\n\nprice_scalar.fit(x_train_upsampled['price'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\nprint(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nx_train_price_standardized = price_scalar.transform(x_train_upsampled['price'].values.reshape(-1, 1))\nx_test_price_standardized  = price_scalar.transform(x_test['price'].values.reshape(-1, 1))\nx_cv_price_standardized    = price_scalar.transform(x_cv['price'].values.reshape(-1, 1))\n\nprint(\"Shape of matrix after standardization -> Teacher Prefix: x_train: \",x_train_prefix_one_hot.shape)\nprint(\"Shape of matrix after standardization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\nprint(\"Shape of matrix after standardization -> Teacher Prefix: x_test : \",x_test_prefix_one_hot.shape)","4298dacc":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\nfrom sklearn.preprocessing import StandardScaler\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n\nteacher_previous_proj_scalar = StandardScaler()\n\nteacher_previous_proj_scalar.fit(x_train_upsampled['teacher_number_of_previously_posted_projects'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\nprint(f\"Mean : {teacher_previous_proj_scalar.mean_[0]}, Standard deviation : {np.sqrt(teacher_previous_proj_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nx_train_teacher_previous_proj_standardized = teacher_previous_proj_scalar.transform(x_train_upsampled['teacher_number_of_previously_posted_projects'].values.reshape(-1, 1))\nx_test_teacher_previous_proj_standardized  = teacher_previous_proj_scalar.transform(x_test['teacher_number_of_previously_posted_projects'].values.reshape(-1, 1))\nx_cv_teacher_previous_proj_standardized    = teacher_previous_proj_scalar.transform(x_cv['teacher_number_of_previously_posted_projects'].values.reshape(-1, 1))\n\nprint(\"Shape of matrix after standardization -> Teachers Previous Projects: x_train:  \",x_train_prefix_one_hot.shape)\nprint(\"Shape of matrix after standardization -> Teachers Previous Projects: x_cv   :  \",x_cv_prefix_one_hot.shape)\nprint(\"Shape of matrix after standardization -> Teachers Previous Projects: x_test :  \",x_test_prefix_one_hot.shape)\n","1847b782":"# We are considering only the words which appeared in at least 10 documents(rows or projects).\nvectorizer = CountVectorizer(min_df=10)\n\nvectorizer.fit(x_train_upsampled['preprocessed_essays'])\n\nx_train_essays_bow = vectorizer.transform(x_train_upsampled['preprocessed_essays'])\nx_cv_essays_bow    = vectorizer.transform(x_cv['preprocessed_essays'])\nx_test_essays_bow  = vectorizer.transform(x_test['preprocessed_essays'])\n\nprint(\"Shape of matrix after BOW -> Essays: x_train: \",x_train_essays_bow.shape)\nprint(\"Shape of matrix after BOW -> Essays: x_cv   : \",x_cv_essays_bow.shape)\nprint(\"Shape of matrix after BOW -> Essays: x_test : \",x_test_essays_bow.shape)","f28839d3":"vectorizer = CountVectorizer(min_df=10)\n\nvectorizer.fit(x_train_upsampled['preprocessed_titles'])\n\nx_train_titles_bow = vectorizer.transform(x_train_upsampled['preprocessed_titles'])\nx_cv_titles_bow    = vectorizer.transform(x_cv['preprocessed_titles'])\nx_test_titles_bow  = vectorizer.transform(x_test['preprocessed_titles'])\n\nprint(\"Shape of matrix after BOW -> Title: x_train: \",x_train_titles_bow.shape)\nprint(\"Shape of matrix after BOW -> Title: x_cv   : \",x_cv_titles_bow.shape)\nprint(\"Shape of matrix after BOW -> Title: x_test : \",x_test_titles_bow.shape)","67b8db28":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\n\nvectorizer.fit(x_train_upsampled['preprocessed_essays'])\n\nx_train_essays_tfidf = vectorizer.transform(x_train_upsampled['preprocessed_essays'])\nx_cv_essays_tfidf    = vectorizer.transform(x_cv['preprocessed_essays'])\nx_test_essays_tfidf  = vectorizer.transform(x_test['preprocessed_essays'])\n\nprint(\"Shape of matrix after TF-IDF -> Essay: x_train: \",x_train_essays_tfidf.shape)\nprint(\"Shape of matrix after TF-IDF -> Essay: x_cv   : \",x_cv_essays_tfidf.shape)\nprint(\"Shape of matrix after TF-IDF -> Essay: x_test : \",x_test_essays_tfidf.shape)","83b84b8e":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\n\nvectorizer.fit(x_train_upsampled['preprocessed_titles'])\n\nx_train_titles_tfidf = vectorizer.transform(x_train_upsampled['preprocessed_titles'])\nx_cv_titles_tfidf    = vectorizer.transform(x_cv['preprocessed_titles'])\nx_test_titles_tfidf  = vectorizer.transform(x_test['preprocessed_titles'])\n\nprint(\"Shape of matrix after TF-IDF -> Title: x_train: \",x_train_titles_tfidf.shape)\nprint(\"Shape of matrix after TF-IDF -> Title: x_cv   : \",x_cv_titles_tfidf.shape)\nprint(\"Shape of matrix after TF-IDF -> Title: x_test : \",x_test_titles_tfidf.shape)\n\n# Code for testing and checking the generated vectors\n# v1 = vectorizer.transform([preprocessed_titles[0]]).toarray()[0]\n# text_title_tfidf=pd.DataFrame(v1)\n# text_title_tfidf.to_csv('text_title_tfidf.csv', index=None,header=None)","c224ff19":"'''\n# Reading glove vectors in python: https:\/\/stackoverflow.com\/a\/38230349\/4084039\ndef loadGloveModel(gloveFile):\n    print (\"Loading Glove Model\")\n    f = open(gloveFile,'r', encoding=\"utf8\")\n    model = {}\n    for line in tqdm(f):\n        splitLine = line.split()\n        word = splitLine[0]\n        embedding = np.array([float(val) for val in splitLine[1:]])\n        model[word] = embedding\n    print (\"Done.\",len(model),\" words loaded!\")\n    return model\nmodel = loadGloveModel('glove.42B.300d.txt')\n\n# ============================\nOutput:\n    \nLoading Glove Model\n1917495it [06:32, 4879.69it\/s]\nDone. 1917495  words loaded!\n\n# ============================\n\nwords = []\nfor i in preproced_texts:\n    words.extend(i.split(' '))\n\nfor i in preproced_titles:\n    words.extend(i.split(' '))\nprint(\"all the words in the coupus\", len(words))\nwords = set(words)\nprint(\"the unique words in the coupus\", len(words))\n\ninter_words = set(model.keys()).intersection(words)\nprint(\"The number of words that are present in both glove vectors and our coupus\", \\\n      len(inter_words),\"(\",np.round(len(inter_words)\/len(words)*100,3),\"%)\")\n\nwords_courpus = {}\nwords_glove = set(model.keys())\nfor i in words:\n    if i in words_glove:\n        words_courpus[i] = model[i]\nprint(\"word 2 vec length\", len(words_courpus))\n\n\n# stronging variables into pickle files python: http:\/\/www.jessicayung.com\/how-to-use-pickle-to-save-and-load-variables-in-python\/\n\nimport pickle\nwith open('glove_vectors', 'wb') as f:\n    pickle.dump(words_courpus, f)\n\n\n'''\n\n\n# words_train_essays = []\n# for i in preprocessed_essays_train :\n#     words_train_essays.extend(i.split(' '))\n\n# ## Find the total number of words in the Train data of Essays.\n# print(\"all the words in the corpus\", len(words_train_essays))\n\n# ## Find the unique words in this set of words\n# words_train_essay = set(words_train_essays)\n# print(\"the unique words in the corpus\", len(words_train_essay))\n\n# ## Find the words present in both Glove Vectors as well as our corpus.\n# inter_words = set(model.keys()).intersection(words_train_essay)\n# print(\"The number of words that are present in both glove vectors and our corpus are {} which is nearly {}% \".format(len(inter_words), np.round((float(len(inter_words))\/len(words_train_essay))*100)))\n\n# words_corpus_train_essay = {}\n# words_glove = set(model.keys())\n# for i in words_train_essay:\n#     if i in words_glove:\n#         words_corpus_train_essay[i] = model[i]\n# print(\"word 2 vec length\", len(words_corpus_train_essay))","8ff3b48f":"# stronging variables into pickle files python: http:\/\/www.jessicayung.com\/how-to-use-pickle-to-save-and-load-variables-in-python\/\n# make sure you have the glove_vectors file\nwith open('..\/input\/glove_vectors\/glove_vectors', 'rb') as f:\n    model = pickle.load(f)\n    glove_words =  set(model.keys())","a6e5892b":"# average Word2Vec\n# compute average word2vec for each review.\nx_train_essays_avg_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sentence in tqdm(x_train_upsampled['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    x_train_essays_avg_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : AVG-W2V -> Essay: x_train: \",len(x_train_essays_avg_w2v_vectors))\nprint(\"Length of a Single vector: AVG-W2V -> Essay: x_train: \",len(x_train_essays_avg_w2v_vectors[0]))","3ff2d125":"x_cv_essays_avg_w2v_vectors = [];\nfor sentence in tqdm(x_cv['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    x_cv_essays_avg_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : AVG-W2V -> Essay: x_cv   : \",len(x_cv_essays_avg_w2v_vectors))\nprint(\"Length of a Single vector: AVG-W2V -> Essay: x_cv   : \",len(x_cv_essays_avg_w2v_vectors[0]))","540c3e71":"x_test_essays_avg_w2v_vectors = [];\nfor sentence in tqdm(x_test['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    x_test_essays_avg_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : AVG-W2V -> Essay: x_test : \",len(x_test_essays_avg_w2v_vectors))\nprint(\"Length of a Single vector: AVG-W2V -> Essay: x_test : \",len(x_test_essays_avg_w2v_vectors[0]))","1ff070a5":"x_train_titles_avg_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sentence in tqdm(x_train_upsampled['preprocessed_titles']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    x_train_titles_avg_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : AVG-W2V -> Titles: x_train: \",len(x_train_titles_avg_w2v_vectors))\nprint(\"Length of a Single vector: AVG-W2V -> Titles: x_train: \",len(x_train_titles_avg_w2v_vectors[0]))","76c53eac":"x_cv_titles_avg_w2v_vectors = [];\nfor sentence in tqdm(x_cv['preprocessed_titles']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    x_cv_titles_avg_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : AVG-W2V -> Titles: x_cv   : \",len(x_cv_titles_avg_w2v_vectors))\nprint(\"Length of a Single vector: AVG-W2V -> Titles: x_cv   : \",len(x_cv_titles_avg_w2v_vectors[0]))","5aa0bd6e":"x_test_titles_avg_w2v_vectors = [];\nfor sentence in tqdm(x_test['preprocessed_titles']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    x_test_titles_avg_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : AVG-W2V -> Titles: x_test : \",len(x_test_titles_avg_w2v_vectors))\nprint(\"Length of a Single vector: AVG-W2V -> Titles: x_test : \",len(x_test_titles_avg_w2v_vectors[0]))","96e102f3":"# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\ntfidf_model = TfidfVectorizer()\ntfidf_model.fit(x_train_upsampled['preprocessed_essays'])\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\ntfidf_words = set(tfidf_model.get_feature_names())","dce79f11":"# average Word2Vec\n# compute average word2vec for each review.\nx_train_essays_tfidf_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sentence in tqdm(x_train_upsampled['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if (word in glove_words) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector \/= tf_idf_weight\n    x_train_essays_tfidf_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Essays: x_train : \",len(x_train_essays_tfidf_w2v_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Essays: x_train : \",len(x_train_essays_tfidf_w2v_vectors[0]))","f726da50":"x_cv_essays_tfidf_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sentence in tqdm(x_cv['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if (word in glove_words) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector \/= tf_idf_weight\n    x_cv_essays_tfidf_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Essays: x_cv : \",len(x_cv_essays_tfidf_w2v_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Essays: x_cv : \",len(x_cv_essays_tfidf_w2v_vectors[0]))","45ce6873":"x_test_essays_tfidf_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sentence in tqdm(x_test['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if (word in glove_words) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector \/= tf_idf_weight\n    x_test_essays_tfidf_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Essays: x_test : \",len(x_test_essays_tfidf_w2v_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Essays: x_test : \",len(x_test_essays_tfidf_w2v_vectors[0]))","504da2eb":"# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\ntfidf_title_model = TfidfVectorizer()\ntfidf_title_model.fit(x_train_upsampled['preprocessed_titles'])\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_title_model.get_feature_names(), list(tfidf_title_model.idf_)))\ntfidf_title_words = set(tfidf_title_model.get_feature_names())","49798f2b":"# average Word2Vec\n# compute average word2vec for each title.\nx_train_tfidf_w2v_title_vectors = []; # the avg-w2v for each title is stored in this list\nfor sentence in x_train_upsampled['preprocessed_titles']: # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_title_weight =0; # num of words with a valid vector in the title\n    for word in sentence.split(): # for each word in a title\n        if (word in glove_words) and (word in tfidf_title_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_title_weight += tf_idf\n    if tf_idf_title_weight != 0:\n        vector \/= tf_idf_title_weight\n    x_train_tfidf_w2v_title_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Titles: x_train : \",len(x_train_tfidf_w2v_title_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Titles: x_train : \",len(x_train_tfidf_w2v_title_vectors[0]))","13a83642":"# average Word2Vec\n# compute average word2vec for each title.\nx_cv_tfidf_w2v_title_vectors = []; # the avg-w2v for each title is stored in this list\nfor sentence in x_cv['preprocessed_titles']: # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_title_weight =0; # num of words with a valid vector in the title\n    for word in sentence.split(): # for each word in a title\n        if (word in glove_words) and (word in tfidf_title_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_title_weight += tf_idf\n    if tf_idf_title_weight != 0:\n        vector \/= tf_idf_title_weight\n    x_cv_tfidf_w2v_title_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Titles: x_cv : \",len(x_cv_tfidf_w2v_title_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Titles: x_cv : \",len(x_cv_tfidf_w2v_title_vectors[0]))","f3698ae6":"# average Word2Vec\n# compute average word2vec for each title.\nx_test_tfidf_w2v_title_vectors = []; # the avg-w2v for each title is stored in this list\nfor sentence in x_test['preprocessed_titles']: # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_title_weight =0; # num of words with a valid vector in the title\n    for word in sentence.split(): # for each word in a title\n        if (word in glove_words) and (word in tfidf_title_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_title_weight += tf_idf\n    if tf_idf_title_weight != 0:\n        vector \/= tf_idf_title_weight\n    x_test_tfidf_w2v_title_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Titles: x_test : \",len(x_test_tfidf_w2v_title_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Titles: x_test : \",len(x_test_tfidf_w2v_title_vectors[0]))","e54c77b8":"from scipy.sparse import hstack\n\nx_train_onehot = hstack((x_train_categories_one_hot, x_train_sub_categories_one_hot, x_train_school_state_one_hot, x_train_prefix_one_hot, x_train_grade_category_one_hot, x_train_price_standardized, x_train_teacher_previous_proj_standardized))\nx_cv_onehot    = hstack((x_cv_categories_one_hot, x_cv_sub_categories_one_hot, x_cv_school_state_one_hot, x_cv_prefix_one_hot, x_cv_grade_category_one_hot, x_test_price_standardized, x_test_teacher_previous_proj_standardized))\nx_test_onehot  = hstack((x_test_categories_one_hot, x_test_sub_categories_one_hot, x_test_school_state_one_hot, x_test_prefix_one_hot, x_test_grade_category_one_hot, x_cv_price_standardized, x_cv_teacher_previous_proj_standardized))\n\nprint(\"Type -> One Hot -> x_train_cv_test: \",type(x_train_onehot))\nprint(\"Type -> One Hot -> cv             : \",type(x_test_onehot))\nprint(\"Type -> One Hot -> x_test         : \",type(x_cv_onehot))\nprint(\"\\n\")\nprint(\"Shape -> One Hot -> x_train_cv_test: \",x_train_onehot.shape)\nprint(\"Shape -> One Hot -> cv             : \",x_test_onehot.shape)\nprint(\"Shape -> One Hot -> x_test         : \",x_cv_onehot.shape)","501c048e":"x_train_onehot.shape\n","c1151f1a":"x_train_onehot_bow = hstack((x_train_onehot,x_train_titles_bow,x_train_essays_bow)).tocsr()### Merging all ONE HOT features\nx_cv_onehot_bow    = hstack((x_cv_onehot, x_cv_titles_bow, x_cv_essays_bow)).tocsr()### Merging all ONE HOT features\nx_test_onehot_bow  = hstack((x_test_onehot, x_test_titles_bow, x_test_essays_bow)).tocsr()### Merging all ONE HOT features\nprint(\"Type -> One Hot BOW -> x_train_cv_test: \",type(x_train_onehot_bow))\nprint(\"Type -> One Hot BOW -> cv             : \",type(x_cv_onehot_bow))\nprint(\"Type -> One Hot BOW -> x_test         : \",type(x_test_onehot_bow))\nprint(\"\\n\")\nprint(\"Shape -> One Hot BOW -> x_train_cv_test: \",x_train_onehot_bow.shape)\nprint(\"Shape -> One Hot BOW -> cv             : \",x_cv_onehot_bow.shape)\nprint(\"Shape -> One Hot BOW -> x_test         : \",x_test_onehot_bow.shape)","835bf6be":"x_train_onehot_tfidf = hstack((x_train_onehot,x_train_titles_tfidf, x_train_essays_tfidf)).tocsr()\nx_cv_onehot_tfidf    = hstack((x_cv_onehot,x_cv_titles_tfidf, x_cv_essays_tfidf)).tocsr()\nx_test_onehot_tfidf  = hstack((x_test_onehot,x_test_titles_tfidf, x_test_essays_tfidf)).tocsr()\nprint(\"Type -> One Hot TFIDF -> x_train_cv_test: \",type(x_train_onehot_tfidf))\nprint(\"Type -> One Hot TFIDF -> cv             : \",type(x_cv_onehot_tfidf))\nprint(\"Type -> One Hot TFIDF -> x_test         : \",type(x_test_onehot_tfidf))\nprint(\"\\n\")\nprint(\"Shape -> One Hot TFIDF -> x_train_cv_test: \",x_train_onehot_tfidf.shape)\nprint(\"Shape -> One Hot TFIDF -> cv             : \",x_cv_onehot_tfidf.shape)\nprint(\"Shape -> One Hot TFIDF -> x_test         : \",x_test_onehot_tfidf.shape)### SET 1:  Merging ONE HOT with BOW (Title and Essay) features","12324a13":"x_train_onehot_avg_w2v = hstack((x_train_onehot, x_train_titles_avg_w2v_vectors, x_train_essays_avg_w2v_vectors)).tocsr()\nx_cv_onehot_avg_w2v    = hstack((x_cv_onehot, x_cv_titles_avg_w2v_vectors, x_cv_essays_avg_w2v_vectors)).tocsr()\nx_test_onehot_avg_w2v  = hstack((x_test_onehot, x_test_titles_avg_w2v_vectors, x_test_essays_avg_w2v_vectors)).tocsr()\nprint(\"Type -> One Hot AVG W2V-> x_train_cv_test: \",type(x_train_onehot_avg_w2v))\nprint(\"Type -> One Hot AVG W2V-> cv             : \",type(x_cv_onehot_avg_w2v))\nprint(\"Type -> One Hot AVG W2V-> x_test         : \",type(x_test_onehot_avg_w2v))\nprint(\"\\n\")\nprint(\"Shape -> One Hot AVG W2V-> x_train_cv_test: \",x_train_onehot_avg_w2v.shape)\nprint(\"Shape -> One Hot AVG W2V-> cv             : \",x_cv_onehot_avg_w2v.shape)\nprint(\"Shape -> One Hot AVG W2V-> x_test         : \",x_test_onehot_avg_w2v.shape)### SET 1:  Merging ONE HOT with BOW (Title and Essay) features","648a84ff":"x_train_onehot_tfidf_w2v = hstack((x_train_onehot, x_train_tfidf_w2v_title_vectors, x_train_essays_tfidf_w2v_vectors)).tocsr()\nx_cv_onehot_tfidf_w2v    = hstack((x_cv_onehot, x_cv_tfidf_w2v_title_vectors, x_cv_essays_tfidf_w2v_vectors)).tocsr()\nx_test_onehot_tfidf_w2v  = hstack((x_test_onehot, x_test_tfidf_w2v_title_vectors, x_test_essays_tfidf_w2v_vectors)).tocsr()\nprint(\"Type -> One Hot TFIDF W2V -> x_train_cv_test: \",type(x_train_onehot_tfidf_w2v))\nprint(\"Type -> One Hot TFIDF W2V -> cv             : \",type(x_cv_onehot_tfidf_w2v))\nprint(\"Type -> One Hot TFIDF W2V -> x_test         : \",type(x_test_onehot_tfidf_w2v))\nprint(\"\\n\")\nprint(\"Shape -> One Hot TFIDF W2V -> x_train_cv_test: \",x_train_onehot_tfidf_w2v.shape)\nprint(\"Shape -> One Hot TFIDF W2V -> cv             : \",x_cv_onehot_tfidf_w2v.shape)\nprint(\"Shape -> One Hot TFIDF W2V -> x_test         : \",x_test_onehot_tfidf_w2v.shape)### SET 1:  Merging ONE HOT with BOW (Title and Essay) features","27922aec":"# print(khallas)","41a82748":"def batch_predict(clf, data):\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs\n\n    y_data_pred = []\n    tr_loop = data.shape[0] - data.shape[0]%1000\n    # consider you X_tr shape is 49041, then your tr_loop will be 49041 - 49041%1000 = 49000\n    # in this for loop we will iterate unti the last 1000 multiplier\n    for i in range(0, tr_loop, 1000):\n        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])\n    # we will be predicting for the last data points\n    if data.shape[0]%1000 !=0:\n        y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])\n    \n    return y_data_pred","df9b0e2a":"print(\"DONE TILL HERE\")","c4c7f8e8":"import matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_auc_score\n\"\"\"\ny_true : array, shape = [n_samples] or [n_samples, n_classes]\nTrue binary labels or binary label indicators.\n\ny_score : array, shape = [n_samples] or [n_samples, n_classes]\nTarget scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of\ndecisions (as returned by \u201cdecision_function\u201d on some classifiers). \nFor binary y_true, y_score is supposed to be the score of the class with greater label.\n\n\"\"\"\n# x_train_onehot_bow\n# x_cv_onehot_bow\n# x_test_onehot_bow\nplt.close()\ntrain_auc = []\ncv_auc = []\nK = [3, 5,7,9,29,31,35,37,61,67,71,91,97,101,103,107,109]\n# K=[1, 5, 10, 15, 21, 31, 41, 51, 65, 71]\nfor i in tqdm(K):\n    neigh = KNeighborsClassifier(n_neighbors=i, n_jobs=-1)\n    neigh.fit(x_train_onehot_bow, yy_train)\n\n    y_train_pred = batch_predict(neigh, x_train_onehot_bow)    \n    y_cv_pred = batch_predict(neigh, x_cv_onehot_bow)\n\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs        \n    train_auc.append(roc_auc_score(yy_train,y_train_pred))\n    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))\nax = plt.subplot()\n# ax.plot(x, y)\n# plt.plot(K, train_auc, label='Train AUC')\n# plt.plot(K, cv_auc, label='CV AUC')\nax.plot(K, train_auc, label='Train AUC')\nax.plot(K, cv_auc, label='CV AUC')\n\nplt.scatter(K, train_auc, label='Train AUC points')\nplt.scatter(K, cv_auc, label='CV AUC points')\n\nplt.legend()\nplt.xlabel(\"K: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid(b=True, which='major', color='k', linestyle=':') ## ->https:\/\/matplotlib.org\/3.1.1\/api\/_as_gen\/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_color\nax.set_facecolor(\"white\") ## ->https:\/\/stackoverflow.com\/a\/43391123\/4433839\n# plt.set_facecolor(\"red\")\n# fig = plt.figure(facecolor=\"w\")\nplt.show()","e0d21826":"# # https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n# import matplotlib.pyplot as plt\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.metrics import roc_auc_score\n# from sklearn.model_selection import GridSearchCV\n# from scipy.stats import randint as sp_randint\n# from sklearn.model_selection import RandomizedSearchCV\n\n# neigh = KNeighborsClassifier(n_jobs=-1)\n# parameters = {'n_neighbors':sp_randint(30, 100)}\n# clf = RandomizedSearchCV(neigh, parameters, cv=x_cv_onehot, scoring='roc_auc')\n# clf.fit(x_train_onehot, yy_train)\n\n# results = pd.DataFrame.from_dict(clf.cv_results_)\n# results = results.sort_values(['param_n_neighbors'])\n\n# train_auc= results['mean_train_score']\n# train_auc_std= results['std_train_score']\n# cv_auc = results['mean_test_score'] \n# cv_auc_std= results['std_test_score']\n# K =  results['param_n_neighbors']\n\n# ax = plt.subplot()\n# # plt.plot(K, train_auc, label='Train AUC')\n# ax.plot(K, train_auc, label='Train AUC')\n# # this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\n# # plt.gca().fill_between(K, train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n\n# # plt.plot(K, cv_auc, label='CV AUC')\n# ax.plot(K, cv_auc, label='CV AUC')\n# # this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\n# # plt.gca().fill_between(K, cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n\n# plt.scatter(K, train_auc, label='Train AUC points')\n# plt.scatter(K, cv_auc, label='CV AUC points')\n\n\n# plt.legend()\n# plt.xlabel(\"K: hyperparameter\")\n# plt.ylabel(\"AUC\")\n# plt.title(\"Hyper parameter Vs AUC plot\")\n# plt.grid(b=True, which='major', color='k', linestyle=':') ## ->https:\/\/matplotlib.org\/3.1.1\/api\/_as_gen\/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_color\n# ax.set_facecolor(\"white\") ## ->https:\/\/stackoverflow.com\/a\/43391123\/4433839\n# plt.show()\n\n# results.head(10)","044d277e":"# results.head(10)","5d31f01f":"# print(\"Best Value of k         : \",clf.best_estimator_.get_params()['n_neighbors']) ##-> https:\/\/chrisalbon.com\/machine_learning\/model_selection\/hyperparameter_tuning_using_random_search\/\n# print(\"Best Score for the k is : \",clf.best_score_)\n# # clf.best_params_ ->## https:\/\/www.arunprakash.org\/2017\/04\/gridsearchcv-vs-randomizedsearchcv-for.html","9763bcdd":"# print(STOP)","55b6a8da":"# from the error plot we choose K such that, we will have maximum AUC on cv data and gap between the train and cv is less\n# Note: based on the method you use you might get different hyperparameter values as best one\n# so, you choose according to the method you choose, you use gridsearch if you are having more computing power and note it will take more time\n# if you increase the cv values in the GridSearchCV you will get more rebust results.\n\n#here we are choosing the best_k based on forloop results\nbest_k_bow = 71\n# best_k = clf.best_estimator_.get_params()['n_neighbors']\n# print(\"Value of k considered for Testing is : \", clf.best_estimator_.get_params()['n_neighbors'])","881e7b9d":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\n\nneigh = KNeighborsClassifier(n_neighbors=best_k_bow, n_jobs=-1)\nneigh.fit(x_train_onehot_bow, yy_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\ny_train_pred = batch_predict(neigh, x_train_onehot_bow)    \ny_test_pred = batch_predict(neigh, x_test_onehot_bow)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(yy_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\ntrain_auc_bow = auc(train_fpr, train_tpr)\ntest_auc_bow  = auc(test_fpr, test_tpr)\n\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.plot([0,1],[0,1],'r--')\nplt.legend()\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC\")\nplt.grid()\nplt.show()","b893977b":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(fpr*(1-tpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","0c9c84f6":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(yy_train, predict(y_train_pred, tr_thresholds, train_fpr, train_fpr)))","4c37b516":"conf_matr_df_train = pd.DataFrame(confusion_matrix(yy_train, predict(y_train_pred, tr_thresholds,train_fpr, train_fpr)), range(2),range(2))","ddd238d0":"## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","abe8f40f":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_fpr)))","07b43807":"conf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_fpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","57884a29":"import matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_auc_score\n\"\"\"\ny_true : array, shape = [n_samples] or [n_samples, n_classes]\nTrue binary labels or binary label indicators.\n\ny_score : array, shape = [n_samples] or [n_samples, n_classes]\nTarget scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of\ndecisions (as returned by \u201cdecision_function\u201d on some classifiers). \nFor binary y_true, y_score is supposed to be the score of the class with greater label.\n\n\"\"\"\n# x_train_onehot_bow\n# x_cv_onehot_bow\n# x_test_onehot_bow\nplt.close()\ntrain_auc = []\ncv_auc = []\n# K = [3, 5,7,9,29,31,35,37,61,67,71,91,97,101,103,107,109]\n# K=[1, 5, 10, 15, 21, 31, 41, 51, 65, 71]\nK = [37,61,67,71,91,97,101,103,107,109,111,121,131,141,147,151]\nfor i in tqdm(K):\n    neigh = KNeighborsClassifier(n_neighbors=i, n_jobs=-1)\n    neigh.fit(x_train_onehot_tfidf, yy_train)\n\n    y_train_pred = batch_predict(neigh, x_train_onehot_tfidf)    \n    y_cv_pred = batch_predict(neigh, x_cv_onehot_tfidf)\n\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs        \n    train_auc.append(roc_auc_score(yy_train,y_train_pred))\n    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))\nax = plt.subplot()\n# ax.plot(x, y)\n# plt.plot(K, train_auc, label='Train AUC')\n# plt.plot(K, cv_auc, label='CV AUC')\nax.plot(K, train_auc, label='Train AUC')\nax.plot(K, cv_auc, label='CV AUC')\n\nplt.scatter(K, train_auc, label='Train AUC points')\nplt.scatter(K, cv_auc, label='CV AUC points')\n\nplt.legend()\nplt.xlabel(\"K: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid(b=True, which='major', color='k', linestyle=':') ## ->https:\/\/matplotlib.org\/3.1.1\/api\/_as_gen\/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_color\nax.set_facecolor(\"white\") ## ->https:\/\/stackoverflow.com\/a\/43391123\/4433839\n# plt.set_facecolor(\"red\")\n# fig = plt.figure(facecolor=\"w\")\nplt.show()","ba195265":"# print(\"Best Value of k         : \",clf.best_estimator_.get_params()['n_neighbors']) ##-> https:\/\/chrisalbon.com\/machine_learning\/model_selection\/hyperparameter_tuning_using_random_search\/\n# print(\"Best Score for the k is : \",clf.best_score_)\n# # clf.best_params_ ->## https:\/\/www.arunprakash.org\/2017\/04\/gridsearchcv-vs-randomizedsearchcv-for.html","4e60fe6c":"# print(STOP)","7786e27b":"# from the error plot we choose K such that, we will have maximum AUC on cv data and gap between the train and cv is less\n# Note: based on the method you use you might get different hyperparameter values as best one\n# so, you choose according to the method you choose, you use gridsearch if you are having more computing power and note it will take more time\n# if you increase the cv values in the GridSearchCV you will get more rebust results.\n\n#here we are choosing the best_k based on forloop results\nbest_k_tfidf = 147\n# best_k = clf.best_estimator_.get_params()['n_neighbors']\n# print(\"Value of k considered for Testing is : \", clf.best_estimator_.get_params()['n_neighbors'])","c78f8d97":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\n\nneigh = KNeighborsClassifier(n_neighbors=best_k_tfidf, n_jobs=-1)\nneigh.fit(x_train_onehot_tfidf, yy_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\ny_train_pred = batch_predict(neigh, x_train_onehot_tfidf)    \ny_test_pred = batch_predict(neigh, x_test_onehot_tfidf)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(yy_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\ntrain_auc_tfidf = auc(train_fpr, train_tpr)\ntest_auc_tfidf  = auc(test_fpr, test_tpr)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.plot([0,1],[0,1],'r--')\nplt.legend()\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC\")\nplt.grid()\nplt.show()","7a0928e5":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(fpr*(1-tpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","2d6ab76d":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(yy_train, predict(y_train_pred, tr_thresholds, train_fpr, train_fpr)))","63827f4c":"conf_matr_df_train = pd.DataFrame(confusion_matrix(yy_train, predict(y_train_pred, tr_thresholds,train_fpr, train_fpr)), range(2),range(2))","82e56e3e":"## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","520a3970":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_fpr)))","2b11ed00":"conf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_fpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","5487b4d7":"import matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_auc_score\n\"\"\"\ny_true : array, shape = [n_samples] or [n_samples, n_classes]\nTrue binary labels or binary label indicators.\n\ny_score : array, shape = [n_samples] or [n_samples, n_classes]\nTarget scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of\ndecisions (as returned by \u201cdecision_function\u201d on some classifiers). \nFor binary y_true, y_score is supposed to be the score of the class with greater label.\n\n\"\"\"\n# x_train_onehot_bow\n# x_cv_onehot_bow\n# x_test_onehot_bow\nplt.close()\ntrain_auc = []\ncv_auc = []\nK = [3, 5,7,9,29,31,35,37,61,67,71,91,97,101,103,107,109,111,121,131,141,147,151]\n# K=[1, 5, 10, 15, 21, 31, 41, 51, 65, 71]\nfor i in tqdm(K):\n    neigh = KNeighborsClassifier(n_neighbors=i, n_jobs=-1)\n    neigh.fit(x_train_onehot_avg_w2v, yy_train)\n\n    y_train_pred = batch_predict(neigh, x_train_onehot_avg_w2v)    \n    y_cv_pred = batch_predict(neigh, x_cv_onehot_avg_w2v)\n\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs        \n    train_auc.append(roc_auc_score(yy_train,y_train_pred))\n    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))\nax = plt.subplot()\n# ax.plot(x, y)\n# plt.plot(K, train_auc, label='Train AUC')\n# plt.plot(K, cv_auc, label='CV AUC')\nax.plot(K, train_auc, label='Train AUC')\nax.plot(K, cv_auc, label='CV AUC')\n\nplt.scatter(K, train_auc, label='Train AUC points')\nplt.scatter(K, cv_auc, label='CV AUC points')\n\nplt.legend()\nplt.xlabel(\"K: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid(b=True, which='major', color='k', linestyle=':') ## ->https:\/\/matplotlib.org\/3.1.1\/api\/_as_gen\/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_color\nax.set_facecolor(\"white\") ## ->https:\/\/stackoverflow.com\/a\/43391123\/4433839\n# plt.set_facecolor(\"red\")\n# fig = plt.figure(facecolor=\"w\")\nplt.show()","cdf0a803":"# print(\"Best Value of k         : \",clf.best_estimator_.get_params()['n_neighbors']) ##-> https:\/\/chrisalbon.com\/machine_learning\/model_selection\/hyperparameter_tuning_using_random_search\/\n# print(\"Best Score for the k is : \",clf.best_score_)\n# # clf.best_params_ ->## https:\/\/www.arunprakash.org\/2017\/04\/gridsearchcv-vs-randomizedsearchcv-for.html","4471b785":"# print(STOP)","179cda68":"# from the error plot we choose K such that, we will have maximum AUC on cv data and gap between the train and cv is less\n# Note: based on the method you use you might get different hyperparameter values as best one\n# so, you choose according to the method you choose, you use gridsearch if you are having more computing power and note it will take more time\n# if you increase the cv values in the GridSearchCV you will get more rebust results.\n\n#here we are choosing the best_k based on forloop results\nbest_k_avg_w2v = 151\n# best_k = clf.best_estimator_.get_params()['n_neighbors']\n# print(\"Value of k considered for Testing is : \", clf.best_estimator_.get_params()['n_neighbors'])","ea216b6c":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\n\nneigh = KNeighborsClassifier(n_neighbors=best_k_avg_w2v, n_jobs=-1)\nneigh.fit(x_train_onehot_avg_w2v, yy_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\ny_train_pred = batch_predict(neigh, x_train_onehot_avg_w2v)    \ny_test_pred = batch_predict(neigh, x_test_onehot_avg_w2v)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(yy_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\ntrain_auc_avg_w2v = auc(train_fpr, train_tpr)\ntest_auc_avg_w2v  = auc(test_fpr, test_tpr)\n\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.plot([0,1],[0,1],'r--')\nplt.legend()\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC\")\nplt.grid()\nplt.show()","8168f4eb":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(fpr*(1-tpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","7657d8c3":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(yy_train, predict(y_train_pred, tr_thresholds, train_fpr, train_fpr)))","356323a8":"conf_matr_df_train = pd.DataFrame(confusion_matrix(yy_train, predict(y_train_pred, tr_thresholds,train_fpr, train_fpr)), range(2),range(2))","2a3d943c":"## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","d75587b6":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_fpr)))","42f2d612":"conf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_fpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","33185682":"# Please write all the code with proper documentation","0e57887d":"import matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_auc_score\n\"\"\"\ny_true : array, shape = [n_samples] or [n_samples, n_classes]\nTrue binary labels or binary label indicators.\n\ny_score : array, shape = [n_samples] or [n_samples, n_classes]\nTarget scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of\ndecisions (as returned by \u201cdecision_function\u201d on some classifiers). \nFor binary y_true, y_score is supposed to be the score of the class with greater label.\n\n\"\"\"\n# x_train_onehot_bow\n# x_cv_onehot_bow\n# x_test_onehot_bow\nplt.close()\ntrain_auc = []\ncv_auc = []\nK = [3, 5,7,9,29,31,35,37,61,67,71,91,97,101,103,107,109,111,121,131,141,147,151]\n# K=[1, 5, 10, 15, 21, 31, 41, 51, 65, 71]\nfor i in tqdm(K):\n    neigh = KNeighborsClassifier(n_neighbors=i, n_jobs=-1)\n    neigh.fit(x_train_onehot_tfidf_w2v, yy_train)\n\n    y_train_pred = batch_predict(neigh, x_train_onehot_tfidf_w2v)    \n    y_cv_pred = batch_predict(neigh, x_cv_onehot_tfidf_w2v)\n\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs        \n    train_auc.append(roc_auc_score(yy_train,y_train_pred))\n    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))\nax = plt.subplot()\n# ax.plot(x, y)\n# plt.plot(K, train_auc, label='Train AUC')\n# plt.plot(K, cv_auc, label='CV AUC')\nax.plot(K, train_auc, label='Train AUC')\nax.plot(K, cv_auc, label='CV AUC')\n\nplt.scatter(K, train_auc, label='Train AUC points')\nplt.scatter(K, cv_auc, label='CV AUC points')\n\nplt.legend()\nplt.xlabel(\"K: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid(b=True, which='major', color='k', linestyle=':') ## ->https:\/\/matplotlib.org\/3.1.1\/api\/_as_gen\/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_color\nax.set_facecolor(\"white\") ## ->https:\/\/stackoverflow.com\/a\/43391123\/4433839\n# plt.set_facecolor(\"red\")\n# fig = plt.figure(facecolor=\"w\")\nplt.show()","9f3cd474":"# print(\"Best Value of k         : \",clf.best_estimator_.get_params()['n_neighbors']) ##-> https:\/\/chrisalbon.com\/machine_learning\/model_selection\/hyperparameter_tuning_using_random_search\/\n# print(\"Best Score for the k is : \",clf.best_score_)\n# # clf.best_params_ ->## https:\/\/www.arunprakash.org\/2017\/04\/gridsearchcv-vs-randomizedsearchcv-for.html","951bcc5d":"# print(STOP)","23e21c98":"# from the error plot we choose K such that, we will have maximum AUC on cv data and gap between the train and cv is less\n# Note: based on the method you use you might get different hyperparameter values as best one\n# so, you choose according to the method you choose, you use gridsearch if you are having more computing power and note it will take more time\n# if you increase the cv values in the GridSearchCV you will get more rebust results.\n\n#here we are choosing the best_k based on forloop results\nbest_k_tfidf_w2v = 71\n# best_k = clf.best_estimator_.get_params()['n_neighbors']\n# print(\"Value of k considered for Testing is : \", clf.best_estimator_.get_params()['n_neighbors'])","7aebb5fb":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\n\nneigh = KNeighborsClassifier(n_neighbors=best_k_tfidf_w2v, n_jobs=-1)\nneigh.fit(x_train_onehot_tfidf_w2v, yy_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\ny_train_pred = batch_predict(neigh, x_train_onehot_tfidf_w2v)    \ny_test_pred = batch_predict(neigh, x_test_onehot_tfidf_w2v)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(yy_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\ntrain_auc_tfidf_w2v = auc(train_fpr, train_tpr)\ntest_auc_tfidf_w2v  = auc(test_fpr, test_tpr)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.plot([0,1],[0,1],'r--')\nplt.legend()\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC\")\nplt.grid()\nplt.show()","92820979":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(fpr*(1-tpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","61972fc1":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(yy_train, predict(y_train_pred, tr_thresholds, train_fpr, train_fpr)))","12065a2f":"conf_matr_df_train = pd.DataFrame(confusion_matrix(yy_train, predict(y_train_pred, tr_thresholds,train_fpr, train_fpr)), range(2),range(2))","6b6b1440":"## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","07095b37":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_fpr)))","7ee427af":"conf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_fpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","1e9c27d8":"# Please write all the code with proper documentation","dad508a9":"#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.f_classif\n#https:\/\/stackoverflow.com\/questions\/49300193\/feature-selection-f-classif-scikit-learn\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.feature_selection import f_classif\n\n## -> https:\/\/stackoverflow.com\/a\/46929321\/4433839\n\nselector = SelectKBest(f_classif, k=1000)\nselector.fit(x_train_onehot_tfidf, yy_train)\nx_train_tfidf_2000 =selector.transform(x_train_onehot_tfidf)\nx_cv_tfidf_2000 = selector.transform(x_cv_onehot_tfidf)\nx_test_tfidf_2000 = selector.transform(x_test_onehot_tfidf)\nprint(x_train_tfidf_2000.shape)\nprint(x_cv_tfidf_2000.shape)\nprint(x_test_tfidf_2000.shape)","b9073f92":"import matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_auc_score\n\"\"\"\ny_true : array, shape = [n_samples] or [n_samples, n_classes]\nTrue binary labels or binary label indicators.\n\ny_score : array, shape = [n_samples] or [n_samples, n_classes]\nTarget scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of\ndecisions (as returned by \u201cdecision_function\u201d on some classifiers). \nFor binary y_true, y_score is supposed to be the score of the class with greater label.\n\n\"\"\"\n# x_train_onehot_bow\n# x_cv_onehot_bow\n# x_test_onehot_bow\nplt.close()\ntrain_auc = []\ncv_auc = []\nK = [3, 5,7,9,29,31,35,37,61,67,71,91,97,101,103,107,109,111,121,131,141,147,151]\n# K=[1, 5, 10, 15, 21, 31, 41, 51, 65, 71]\nfor i in tqdm(K):\n    neigh = KNeighborsClassifier(n_neighbors=i, n_jobs=-1)\n    neigh.fit(x_train_tfidf_2000, yy_train)\n\n    y_train_pred = batch_predict(neigh, x_train_tfidf_2000)    \n    y_cv_pred = batch_predict(neigh, x_cv_tfidf_2000)\n\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs        \n    train_auc.append(roc_auc_score(yy_train,y_train_pred))\n    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))\nax = plt.subplot()\n# ax.plot(x, y)\n# plt.plot(K, train_auc, label='Train AUC')\n# plt.plot(K, cv_auc, label='CV AUC')\nax.plot(K, train_auc, label='Train AUC')\nax.plot(K, cv_auc, label='CV AUC')\n\nplt.scatter(K, train_auc, label='Train AUC points')\nplt.scatter(K, cv_auc, label='CV AUC points')\n\nplt.legend()\nplt.xlabel(\"K: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid(b=True, which='major', color='k', linestyle=':') ## ->https:\/\/matplotlib.org\/3.1.1\/api\/_as_gen\/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_color\nax.set_facecolor(\"white\") ## ->https:\/\/stackoverflow.com\/a\/43391123\/4433839\n# plt.set_facecolor(\"red\")\n# fig = plt.figure(facecolor=\"w\")\nplt.show()","8bc433b2":"# print(\"Best Value of k         : \",clf.best_estimator_.get_params()['n_neighbors']) ##-> https:\/\/chrisalbon.com\/machine_learning\/model_selection\/hyperparameter_tuning_using_random_search\/\n# print(\"Best Score for the k is : \",clf.best_score_)\n# # clf.best_params_ ->## https:\/\/www.arunprakash.org\/2017\/04\/gridsearchcv-vs-randomizedsearchcv-for.html","679295a5":"# print(STOP)","2e24eedc":"# from the error plot we choose K such that, we will have maximum AUC on cv data and gap between the train and cv is less\n# Note: based on the method you use you might get different hyperparameter values as best one\n# so, you choose according to the method you choose, you use gridsearch if you are having more computing power and note it will take more time\n# if you increase the cv values in the GridSearchCV you will get more rebust results.\n\n#here we are choosing the best_k based on forloop results\nbest_k_2000 = 141\n# best_k = clf.best_estimator_.get_params()['n_neighbors']\n# print(\"Value of k considered for Testing is : \", clf.best_estimator_.get_params()['n_neighbors'])","ef9eadff":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\n\nneigh = KNeighborsClassifier(n_neighbors=best_k_2000, n_jobs=-1)\nneigh.fit(x_train_tfidf_2000, yy_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\ny_train_pred = batch_predict(neigh, x_train_tfidf_2000)    \ny_test_pred = batch_predict(neigh, x_test_tfidf_2000)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(yy_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\ntrain_auc_2000 = auc(train_fpr, train_tpr)\ntest_auc_2000 = auc(test_fpr, test_tpr)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.plot([0,1],[0,1],'r--')\nplt.legend()\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC\")\nplt.grid()\nplt.show()","cdb309e1":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(fpr*(1-tpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","f19c2175":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(yy_train, predict(y_train_pred, tr_thresholds, train_fpr, train_fpr)))","449428fc":"conf_matr_df_train = pd.DataFrame(confusion_matrix(yy_train, predict(y_train_pred, tr_thresholds,train_fpr, train_fpr)), range(2),range(2))","46d45b23":"## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","49ab6c7e":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_fpr)))","a82d2919":"conf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_fpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","728e526c":"from prettytable import PrettyTable\n    \nx = PrettyTable()\n\nx.field_names = [\"Vectorizer\", \"Model\", \"Hyperparameter\", \"Train AUC\", \"Test AUC\"]\n\nx.add_row([\"BOW\", \"Brute\", best_k_bow, round(train_auc_bow,2),round(test_auc_bow,2)])\nx.add_row([\"BOW-2000\", \"Brute\", best_k_2000, round(train_auc_2000,2),round(test_auc_2000,2)])\nx.add_row([\"TF-IDF\", \"Brute\", best_k_tfidf, round(train_auc_tfidf,2),round(test_auc_tfidf,2)])\nx.add_row([\"AVG W2v\", \"Brute\", best_k_avg_w2v, round(train_auc_avg_w2v,2),round(test_auc_avg_w2v,2)])\nx.add_row([\"TF-IDF W2V\", \"Brute\", best_k_tfidf_w2v, round(train_auc_tfidf_w2v,2),round(test_auc_tfidf_w2v,2)])\n\nprint(x)","d8e46a84":"###   ->->-> 6.3.4.2. B: TF-IDF W2V: Title -> CV","04ef168f":"# 10. CONCLUSION","0755fb88":"### ->->-> 8.3.3.2: <font color='red'> SET 3<\/font> Confusion Matrix: Test","2055c0df":"###  ->->->6.3.3.1. B:  Avg W2V: Essays -> CV","f69c1bab":"# 9. Feature selection with `SelectKBest`","f03a5b39":"## ->-> 8.4.3: <font color='red'> SET 4<\/font> Confusion Matrix","372569b4":"# 2: PRE-PROCESSING","601d859e":"## -> 2.3: Preprocessing: Project Grade Category","6e365bd2":"According to Andrew Ng, in the Coursera MOOC on Introduction to Machine Learning, the general rule of thumb is to partition the data set into the ratio of ***3:1:1 (60:20:20)*** for training, validation and testing respectively.","0f8d150d":"###  ->->->6.3.3.1. C: Avg W2V: Essays -> Test","5b3ac1ba":"## -> 7.2: SET 1:  Merging All ONE HOT with BOW (Title and Essay) features","74e73d3f":"## -> 3.2: Text Preprocessing: Title","150ef952":"### ->->-> 8.4.3.2: <font color='red'> SET 4<\/font> Confusion Matrix: Test","0b687b4e":"## ->-> 6.3.4.1: ESSAYS","bd528d87":"# -> 8.4:<font color='red'> SET 4<\/font>  Applying KNN brute force on TF-IDF W2V.","7cb5daac":"## -> 3.1: Text Preprocessing: Essays","21a69bc3":"## -> 7.3: SET 2:  Merging All ONE HOT with TF-IDF (Title and Essay) features","25ee0d99":"## ->-> 8.4.2: <font color='red'> SET 4<\/font> TESTING the performance of the model on test data, plotting ROC Curves","2f65552c":"## ->-> 6.2.2: Standarizing Numerical data: Teacher's Previous Projects","6ea23e25":"### <font color='red'>SelectKBest SET 2<\/font> : Best Value of 'k' and Score","07717a40":"## ->-> 6.1.5 Vectorizing Categorical data: Project Grade","a4a4e127":"## ->-> 6.1.1: Vectorizing Categorical data: Clean Subject Categories","21bb9aff":"## ->-> 8.3.1: <font color='red'> SET 3<\/font> Hyper parameter tuning to find best K","8068a981":"###   ->->->6.3.3.2. C: Avg W2V: Title -> Test","8aa82aac":"## -> 9.3: <font color='red'>SelectKBest: SET 2<\/font> Confusion Matrix","03db40bc":"###    -> -> 4.2.1: Splitting the FULL DATASET into Training and Test sets. (80:20)","5f44c58e":"## ->-> 8.3.2: <font color='red'> SET 3<\/font> TESTING the performance of the model on test data, plotting ROC Curves","db13068b":"# B. OBJECTIVE\nThe primary objective is to implement the k-Nearest Neighbor Algo on the DonorChoose Dataset and measure the accuracy on the Test dataset.","b9546b18":"# -> 8.3:<font color='red'> SET 3<\/font>  Applying KNN brute force on Avg W2V.","d8af0e63":"### <font color='red'> SET 4<\/font> Best Value of 'k' and Score","f56b50f6":"**Observation:**\n1. The proportion of Majority class of 85% and Minority class of 15% is maintained in Training, CV and Testing dataset.","959b45f8":"# -> 6.3.3:  AVG Word2Vec","6e0ba462":"1. ### ->-> 9.3.1: <font color='red'>SelectKBest: SET 2<\/font> Confusion Matrix: Train","c9c265ea":"# 5. UPSAMPLING THE MINORITY CLASS WITH REPLACEMENT STRATEGY","5defb649":"## ->-> 8.2.2: <font color='red'> SET 2<\/font> TESTING the performance of the model on test data, plotting ROC Curves","d17a8c79":"# ->-> 6.3.2: TF-IDF","20ad05a7":"# -> 8.1:<font color='red'> SET 1<\/font>  Applying KNN brute force on BOW.","5f86cc34":"###   ->->-> 6.3.4.1. B: TF-IDF W2V: Essays -> CV","5aecd47e":"#### Text PreProcessing Functions","3f0b44ce":"## ->-> 8.2.3: <font color='red'> SET 2<\/font> Confusion Matrix","5d4811a7":"###   ->->->6.3.3.2. B: Avg W2V: Title -> CV","284a6c12":"### -> -> 4.2.2: Splitting the TRAINING Dataset further into Training and CV sets.","78708d44":"## ->-> 6.2.1: Standarizing Numerical data: Price","e16ec613":"**Conclusion:** Now the number of rows reduced from 109248 to 109245 in project_data.","ea3a9978":"## ->-> 6.3.4.2 TITLE","eb3a62c5":"###   ->->->6.3.3.2. A: Avg W2V: Title -> Train","2e648ace":"**Observation:** 3 Rows had NaN values and they are not considered, thus the number of rows reduced from 109248 to 109245. <br>\n**Action:** We can safely delete these, as 3 is very very small and its deletion wont impact as the original dataset is very large. <br>\nStep1: Convert all the empty strings with Nan \/\/ Not required as its NaN not empty string <br> \nStep2: Drop rows having NaN values","8a33755f":"## -> 1.1: REMOVING NaN:<br>\n**As it is clearly metioned in the dataset details that TEACHER_PREFIX has NaN values, we need to handle this at the very beginning to avoid any problems in our future analysis.**","a744d21b":"## ->-> 8.4.1: <font color='red'> SET 4<\/font> Hyper parameter tuning to find best K","fee36934":"#### -> Merging Price with Project Data","7e120bc4":"## ->-> 6.3.3.2: TITLE","2956aef3":"### <font color='red'> SET 2<\/font> : Best Value of 'k' and Score","1ecf98cd":"## A. DATA INFORMATION \n### <br>About the DonorsChoose Data Set\n\nThe `train.csv` data set provided by DonorsChoose contains the following features:\n\nFeature | Description \n----------|---------------\n**`project_id`** | A unique identifier for the proposed project. **Example:** `p036502`   \n**`project_title`**    | Title of the project. **Examples:**<br><ul><li><code>Art Will Make You Happy!<\/code><\/li><li><code>First Grade Fun<\/code><\/li><\/ul> \n**`project_grade_category`** | Grade level of students for which the project is targeted. One of the following enumerated values: <br\/><ul><li><code>Grades PreK-2<\/code><\/li><li><code>Grades 3-5<\/code><\/li><li><code>Grades 6-8<\/code><\/li><li><code>Grades 9-12<\/code><\/li><\/ul>  \n **`project_subject_categories`** | One or more (comma-separated) subject categories for the project from the following enumerated list of values:  <br\/><ul><li><code>Applied Learning<\/code><\/li><li><code>Care &amp; Hunger<\/code><\/li><li><code>Health &amp; Sports<\/code><\/li><li><code>History &amp; Civics<\/code><\/li><li><code>Literacy &amp; Language<\/code><\/li><li><code>Math &amp; Science<\/code><\/li><li><code>Music &amp; The Arts<\/code><\/li><li><code>Special Needs<\/code><\/li><li><code>Warmth<\/code><\/li><\/ul><br\/> **Examples:** <br\/><ul><li><code>Music &amp; The Arts<\/code><\/li><li><code>Literacy &amp; Language, Math &amp; Science<\/code><\/li>  \n  **`school_state`** | State where school is located ([Two-letter U.S. postal code](https:\/\/en.wikipedia.org\/wiki\/List_of_U.S._state_abbreviations#Postal_codes)). **Example:** `WY`\n**`project_subject_subcategories`** | One or more (comma-separated) subject subcategories for the project. **Examples:** <br\/><ul><li><code>Literacy<\/code><\/li><li><code>Literature &amp; Writing, Social Sciences<\/code><\/li><\/ul> \n**`project_resource_summary`** | An explanation of the resources needed for the project. **Example:** <br\/><ul><li><code>My students need hands on literacy materials to manage sensory needs!<\/code<\/li><\/ul> \n**`project_essay_1`**    | First application essay<sup>*<\/sup>  \n**`project_essay_2`**    | Second application essay<sup>*<\/sup> \n**`project_essay_3`**    | Third application essay<sup>*<\/sup> \n**`project_essay_4`**    | Fourth application essay<sup>*<\/sup> \n**`project_submitted_datetime`** | Datetime when project application was submitted. **Example:** `2016-04-28 12:43:56.245`   \n**`teacher_id`** | A unique identifier for the teacher of the proposed project. **Example:** `bdf8baa8fedef6bfeec7ae4ff1c15c56`  \n**`teacher_prefix`** | Teacher's title. One of the following enumerated values: <br\/><ul><li><code>nan<\/code><\/li><li><code>Dr.<\/code><\/li><li><code>Mr.<\/code><\/li><li><code>Mrs.<\/code><\/li><li><code>Ms.<\/code><\/li><li><code>Teacher.<\/code><\/li><\/ul>  \n**`teacher_number_of_previously_posted_projects`** | Number of project applications previously submitted by the same teacher. **Example:** `2` \n\n<sup>*<\/sup> See the section <b>Notes on the Essay Data<\/b> for more details about these features.\n\nAdditionally, the `resources.csv` data set provides more data about the resources required for each project. Each line in this file represents a resource required by a project:\n\nFeature | Description \n----------|---------------\n**`id`** | A `project_id` value from the `train.csv` file.  **Example:** `p036502`   \n**`description`** | Desciption of the resource. **Example:** `Tenor Saxophone Reeds, Box of 25`   \n**`quantity`** | Quantity of the resource required. **Example:** `3`   \n**`price`** | Price of the resource required. **Example:** `9.95`   \n\n**Note:** Many projects require multiple resources. The `id` value corresponds to a `project_id` in train.csv, so you use it as a key to retrieve all resources needed for a project:\n\nThe data set contains the following label (the value you will attempt to predict):\n\nLabel | Description\n----------|---------------\n`project_is_approved` | A binary flag indicating whether DonorsChoose approved the project. A value of `0` indicates the project was not approved, and a value of `1` indicates the project was approved.","d1f679c3":"## -> 2.2: Preprocessing: Project Subject Sub Categories","a8ce21af":"## -> 4.2: Splitting the dataset into Train, CV and Test datasets. (60:20:20)","34fe7638":"## ->-> 6.3.3.1: Avg W2v: ESSAYS","2ed9bd87":"### <font color='red'> SET 1<\/font> Best Value of 'k' and Score","251cc8a5":"# 8. K Nearest Neighbor","ee9d5580":"## ->->-> 6.3.2.2: TF-IDF: Title (Train, CV, Test)","67116fb5":"## 1. READING DATA","9c18eda4":"## ->-> 8.3.3: <font color='red'> SET 3<\/font> Confusion Matrix","a0278466":"## -> 9.2: <font color='red'>SelectKBest: SET 2<\/font> TESTING the performance of the model on test data, plotting ROC Curves","25806270":"## ->-> 8.2.1: <font color='red'> SET 2<\/font> Hyper parameter tuning to find best K","a6673725":"###   ->->-> 6.3.4.1. C: TF-IDF W2V: Essays -> Test","8d674592":"# 6. PREPARING DATA FOR MODELS","97c1dc89":"## -> 2.1: Preprocessing: Project Subject Categories","4285f8c0":"### -> 4.3: Details of our Training, CV and Test datasets.","7e3d6286":"### <font color='red'> SET 3<\/font> Best Value of 'k' and Score","4ef3b4d9":"## -> 7.5: SET 4:  Merging All ONE HOT with TF-IDF W2V (Title and Essay) features","73b63709":"# NOTE: Only 2000 datapoints are considered as Kaggle Kernel is getting disconnected after few hours !","35b73039":"#### Merging Project Essays 1 2 3 4 into Essays","75aeb5ed":"## -> 7.1: Merging all ONE HOT features","daf954cb":"## ->-> 6.1.4 Vectorizing Categorical data: Teacher Prefix","a722e5f0":"# -> 6.3: VECTORIZING TEXT DATA","7ed7865e":"## ->-> 6.1.2: Vectorizing Categorical data: Clean Subject Sub-Categories","879dcd4b":"# ->6.3.4: TFIDF - W2V","fec797e7":"## ->-> 6.1.3 Vectorizing Categorical data: School State","c0ff98d3":"# k-Nearest Neighbors: DonorsChoose Dataset\n# NOTE: Only 2000 points are considered in the sample as Kaggle Kernel is frequently getting disconnected in the middle of the processing.\n# Dataset is Splitted RANDOMLY into Train , CV, Test.\n### A. DATA INFORMATION\n### B. OBJECTIVE\n## 1. READING THE DATASET\n- **1.1 Removing Nan**\n\n## 2. PREPROCESSING \n- **2.1 Preprocessing: Project Subject Categories**\n- **2.2 Preprocessing: Project Subject Sub Categories**\n- **2.3 Preprocessing: Project Grade**\n\n## 3. TEXT PROCESSING\n- **3.1 Text Preprocessing: Essays**\n- **3.2 Text Preprocessing: Title**\n\n## 4. SAMPLING\n- **4.1 Taking Sample from the complete dataset.**\n- **4.2 Splitting the dataset into Train, CV and Test datasets. (60:20:20)**\n    - 4.2.1: Splitting the FULL DATASET into Training and Test sets. (80:20)\n    - 4.2.2: Splitting the TRAINING Dataset further into Training and CV sets.\n- **4.3 Details of our Training, CV and Test datasets.**\n\n## 5. UPSAMPLING THE MINORITY CLASS WITH REPLACEMENT STRATEGY\n\n## 6. PREPARING DATA FOR MODELS\n- **6.1: VECTORIZING CATEGORICAL DATA**\n    - 6.1.1: Vectorizing Categorical data: Clean Subject Categories.\n    - 6.1.2: Vectorizing Categorical data: Clean Subject Sub-Categories.\n    - 6.1.3: Vectorizing Categorical data: School State.\n    - 6.1.4: Vectorizing Categorical data: Teacher Prefix.\n    - 6.1.5: Vectorizing Categorical data: Project Grade\n        \n- **6.2: VECTORIZING NUMERICAL DATA**\n    - 6.2.1: Standarizing Numerical data: Price\n    - 6.2.2: Standarizing Numerical data: Teacher's Previous Projects\n        \n- **6.3: VECTORIZING TEXT DATA**\n    - **6.3.1: BOW**\n        - 6.3.1.1: BOW: Essays (Train, CV, Test)\n        - 6.3.1.2: BOW: Title (Train, CV, Test)\n            \n    - **6.3.2: TF-IDF**\n        - 6.3.2.1: TF-IDF: Essays (Train, CV, Test)\n        - 6.3.2.2: TF-IDF: Title (Train, CV, Test)\n            \n    - **6.3.3: AVG Word2Vec**\n        - 6.3.3.1: Avg W2V: ESSAYS\n            - 6.3.3.1. A: Avg W2V: Essays -> Train\n            - 6.3.3.1. B: Avg W2V: Essays -> CV\n            - 6.3.3.1. C: Avg W2V: Essays -> Test\n                \n        - 6.3.3.2: TITLE\n            - 6.3.3.2. A: Avg W2V: Title -> Train\n            - 6.3.3.2. B: Avg W2V: Title -> CV\n            - 6.3.3.2. C: Avg W2V: Title -> Test\n                \n    - **6.3.4: TFIDF - W2V**\n        - 6.3.4.1: ESSAYS\n            - 6.3.4.1. A: TF-IDF W2V: Essays -> Train\n            - 6.3.4.1. B: TF-IDF W2V: Essays -> CV\n            - 6.3.4.1. C: TF-IDF W2V: Essays -> Test\n                \n        - 6.3.4.2 TITLE\n            - 6.3.4.2. A: TF-IDF W2V: Title -> Train\n            - 6.3.4.2. B: TF-IDF W2V: Title -> CV\n            - 6.3.4.2. C: TF-IDF W2V: Title -> Test\n                \n## 7. MERGING FEATURES\n- 7.1: Merging all ONE HOT features.\n- 7.2: SET 1: Merging All ONE HOT with BOW (Title and Essay) features.\n- 7.3: SET 2: Merging All ONE HOT with TF-IDF (Title and Essay) features.\n- 7.4: SET 3: Merging All ONE HOT with AVG W2V (Title and Essay) features.\n- 7.5: SET 4: Merging All ONE HOT with TF-IDF W2V (Title and Essay) features.\n    \n## 8. K-NEAREST NEIGHBORS\n- **8.1: SET 1 Applying KNN brute force on BOW.**\n    - 8.1.1: SET 1 Hyper parameter tuning to find best K.\n    - 8.1.2: SET 1 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.1.3: SET 1 Confusion Matrix\n        - 8.1.3.1: SET 1 Confusion Matrix: Train\n        - 8.1.3.2: SET 1 Confusion Matrix: Test\n            \n- **8.2: SET 2 Applying KNN brute force on TF-IDF.**\n    - 8.2.1: SET 2 Hyper parameter tuning to find best K.\n    - 8.2.2: SET 2 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.2.3: SET 2 Confusion Matrix\n        - 8.2.3.1: SET 2 Confusion Matrix: Train\n        - 8.2.3.2: SET 2 Confusion Matrix: Test\n\n- **8.3: SET 3 Applying KNN brute force on AVG W2V.**\n    - 8.3.1: SET 3 Hyper parameter tuning to find best K.\n    - 8.3.2: SET31 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.3.3: SET 3 Confusion Matrix\n        - 8.3.3.1: SET 3 Confusion Matrix: Train\n        - 8.3.3.2: SET 3 Confusion Matrix: Test\n        \n- **8.4: SET 4 Applying KNN brute force on TF-IDF W2V.**\n    - 8.4.1: SET 4 Hyper parameter tuning to find best K.\n    - 8.4.2: SET 4 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.4.3: SET 4 Confusion Matrix\n        - 8.4.3.1: SET 4 Confusion Matrix: Train\n        - 8.4.3.2: SET 4 Confusion Matrix: Test\n    \n## 9. FEATURE SELECTION WITH SelectKBest\n- **9.1: SelectkBest: SET 4 Applying KNN brute force on TF-IDF W2V.**\n    - 9.1.1: SelectkBest: SET 4 Hyper parameter tuning to find best K.\n    - 9.1.2: SelectkBest: SET 4 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 9.1.3: SelectkBest: SET 4 Confusion Matrix\n        - 9.1.3.1: SelectkBest: SET 4 Confusion Matrix: Train\n        - 9.1.3.2: SelectkBest: SET 4 Confusion Matrix: Test\n\n## 10. CONCLUSION <br>\n### --------------------------------------------------------------------------------","b8a1bdb4":"# 7. MERGING FEATURES","3cdbe22d":"# ->-> 6.3.1: BOW","6ef54366":"**Observation:** \n1. 3 Rows had NaN values and they are not considered, thus the number of rows reduced from 109248 to 109245.","1576e9ad":"### ->->-> 8.2.3.2: <font color='red'> SET 2<\/font> Confusion Matrix: Test","a35cae7c":"###  ->->->6.3.3.1. A:  Avg W2V: Essays -> Train","9c6631cb":"### Notes on the Essay Data\n\n<ul>\nPrior to May 17, 2016, the prompts for the essays were as follows:\n<li>__project_essay_1:__ \"Introduce us to your classroom\"<\/li>\n<li>__project_essay_2:__ \"Tell us more about your students\"<\/li>\n<li>__project_essay_3:__ \"Describe how your students will use the materials you're requesting\"<\/li>\n<li>__project_essay_3:__ \"Close by sharing why your project will make a difference\"<\/li>\n<\/ul>\n\n\n<ul>\nStarting on May 17, 2016, the number of essays was reduced from 4 to 2, and the prompts for the first 2 essays were changed to the following:<br>\n<li>__project_essay_1:__ \"Describe your students: What makes your students special? Specific details about their background, your neighborhood, and your school are all helpful.\"<\/li>\n<li>__project_essay_2:__ \"About your project: How will these materials make a difference in your students' learning and improve their school lives?\"<\/li>\n<br>For all projects with project_submitted_datetime of 2016-05-17 and later, the values of project_essay_3 and project_essay_4 will be NaN.\n<\/ul>\n","020f2410":"## ->-> 8.1.1: <font color='red'> SET 1<\/font> Hyper parameter tuning to find best K","5e72510f":"Reference: https:\/\/elitedatascience.com\/imbalanced-classes","0b1fc8be":"### ->->-> 8.3.3.1: <font color='red'> SET 3<\/font> Confusion Matrix: Train","ce19119a":"**Observation:**\n1. Dataset is highly **IMBALANCED**.\n1. Approved Class (1) is the Majority class. And the Majority class portion in our sampled dataset: ~85%\n1. Unapproved class (0) is the Minority class. And the Minority class portion in our sampled dataset: ~15%","9bd717ce":"###   ->->-> 6.3.4.2. A: TF-IDF W2V: Title -> Train","38bca8cc":"## -> 9.1: <font color='red'>SelectKBest: SET 2<\/font> Hyper parameter tuning to find best K","a73db4fa":"# 4. SAMPLING\n## -> 4.1: Taking Sample from the complete dataset","d2e7f59a":"## -> 7.4: SET 3:  Merging All ONE HOT with AVG W2V (Title and Essay) features","f1411ae6":"- https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/handling-categorical-and-numerical-features\/","dbcdde5e":"**Teacher Prefix has NAN values, that needs to be cleaned.\nRef: https:\/\/stackoverflow.com\/a\/50297200\/4433839**","f05fe5d2":"# 3. TEXT PROCESSING","f4ab1ca6":"**Conclusion:**\n1. Resampling is performed on the Training data.\n1. Training data in now **BALANCED**.","733e0370":"### ->->-> 8.2.3.1: <font color='red'> SET 2<\/font> Confusion Matrix: Train","b50565f8":"# -> 6.2: VECTORIZING NUMERICAL DATA","6c4bb6a2":"## ->-> 8.1.3: <font color='red'> SET 1<\/font> Confusion Matrix","45630733":"**Conlusion**\n1. **UPSAMPLING** needs to be done on the Minority class to avoid problems related to Imbalanced dataset.\n1. Upsampling will be done by _**\"Resample with replacement strategy\"**_","396e7f4c":"## ->->-> 6.3.1.1: BOW: Essays (Train, CV, Test)","03d5c2a7":"### ->->-> 8.4.3.1: <font color='red'> SET 4<\/font> Confusion Matrix: Train","cf1d6b01":"**ALL VECTOR NAMES**<br>\nx_train_categories_one_hot\nx_cv_categories_one_hot\nx_test_categories_one_hot\n\nx_train_sub_categories_one_hot\nx_cv_sub_categories_one_hot\nx_test_sub_categories_one_hot\n\nx_train_school_state_one_hot\nx_cv_school_state_one_hot\nx_test_school_state_one_hot\n\nx_train_prefix_one_hot\nx_cv_prefix_one_hot\nx_test_prefix_one_hot\n\nx_train_grade_category_one_hot\nx_cv_grade_category_one_hot\nx_test_grade_category_one_hot\n\nx_train_essays_bow\nx_cv_essays_bow\nx_test_essays_bow\n\nx_train_titles_bow\nx_cv_titles_bow\nx_test_titles_bow\n\nx_train_price_standardized\nx_test_price_standardized\nx_cv_price_standardized\n\nx_train_teacher_previous_proj_standardized\nx_test_teacher_previous_proj_standardized\nx_cv_teacher_previous_proj_standardized\n\nx_train_essays_tfidf\nx_cv_essays_tfidf\nx_test_essays_tfidf\n\nx_train_titles_tfidf\nx_cv_titles_tfidf\nx_test_titles_tfidf\n\nx_train_essays_avg_w2v_vectors\nx_cv_essays_avg_w2v_vectors\nx_test_essays_avg_w2v_vectors\n\nx_train_titles_avg_w2v_vectors\nx_cv_titles_avg_w2v_vectors\nx_test_titles_avg_w2v_vectors\n\nx_train_essays_tfidf_w2v_vectors\nx_cv_essays_tfidf_w2v_vectors\nx_test_essays_tfidf_w2v_vectors\n\nx_train_tfidf_w2v_title_vectors\nx_cv_tfidf_w2v_title_vectors\nx_test_tfidf_w2v_title_vectors","29c60ec4":"###   ->->-> 6.3.4.2. C: TF-IDF W2V: Title -> Test","24f5d845":"###   ->->-> 6.3.4.1. A: TF-IDF W2V: Essays -> Train","f73feeba":"# -> 6.1: VECTORIZING CATEGORICAL DATA","c13f09a5":"## ->->-> 6.3.1.2: BOW: Title (Train, CV, Test)","49fd3d04":"### ->-> 9.3.2: <font color='red'>SelectKBest: SET 2<\/font> Confusion Matrix: Test","aa933f1f":"#### Checking total number of enteries with NaN values","243a8780":"# -> 8.2:<font color='red'> SET 2<\/font>  Applying KNN brute force on TF-IDF.","dd583481":"## ->-> 8.1.2: <font color='red'> SET 1<\/font> TESTING the performance of the model on test data, plotting ROC Curves","99593838":"we are going to consider\n\n       - school_state : categorical data\n       - clean_categories : categorical data\n       - clean_subcategories : categorical data\n       - project_grade_category : categorical data\n       - teacher_prefix : categorical data\n       \n       - project_title : text data\n       - text : text data\n       - project_resource_summary: text data (optinal)\n       \n       - quantity : numerical (optinal)\n       - teacher_number_of_previously_posted_projects : numerical\n       - price : numerical","14f682cb":"- we need to merge all the numerical vectors i.e catogorical, text, numerical vectors","897d47ea":"### ->->-> 8.1.3.1: <font color='red'> SET 1<\/font> Confusion Matrix: Train","2ae140ce":"## ->->-> 6.3.2.1: TF-IDF: Essays (Train, CV, Test)","6614a5a7":"### ->->-> 8.1.3.2: <font color='red'> SET 1<\/font> Confusion Matrix: Test"}}