{"cell_type":{"7b541e50":"code","d47a4907":"code","bb237721":"code","e8d47e9f":"code","d24ce38d":"code","25f9616c":"code","366d19d8":"code","3979a046":"code","ffc8fcea":"code","83989c3a":"code","edda31fd":"code","3d8aad76":"code","9899caba":"code","623e883a":"code","06876f76":"code","3202b87a":"code","c4a7e333":"markdown","4965fefa":"markdown","0af64856":"markdown","a8d318ef":"markdown","c9f35860":"markdown","9ddc645a":"markdown","da652bb0":"markdown","afbd208b":"markdown","e21d04c2":"markdown","b41be0e3":"markdown","8651f6ad":"markdown","816cb9dc":"markdown","8ec36b21":"markdown","d9ddb9c2":"markdown"},"source":{"7b541e50":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA","d47a4907":"creditcard = pd.read_csv('..\/input\/ccdata\/CC GENERAL.csv')\ncreditcard.head()","bb237721":"creditcard.describe()","e8d47e9f":"creditcard.isnull().sum()","d24ce38d":"creditcard['CREDIT_LIMIT'].fillna(creditcard['CREDIT_LIMIT'].mean(), inplace=True)\ncreditcard['MINIMUM_PAYMENTS'].fillna(creditcard['MINIMUM_PAYMENTS'].mean(), inplace=True)\n\ncreditcard.isnull().sum()","25f9616c":"duplicatesN = creditcard[creditcard.duplicated()]['CUST_ID'].count()\nprint(f\"Number of dupplicated entries: {duplicatesN}\")","366d19d8":"columnsName = creditcard.columns[1:]  # CUST_ID column is not considered\nplt.figure(figsize=(12,55))\nfor ii, columnName in enumerate(columnsName): \n    plt.subplot(len(columnsName), 1, ii+1)\n    plt.hist(creditcard[columnName], alpha=.4, bins=30)\n    plt.title(columnName)\n    \nplt.tight_layout()","3979a046":"plt.figure(figsize=(12,12))\nsns.heatmap(creditcard.corr(), annot=True)","ffc8fcea":"# As customer ID is not important for this analysis, I will drop this from the dataframe\ncreditcard_df = creditcard.copy()\ncreditcard_df = creditcard_df.drop(columns='CUST_ID')","83989c3a":"scaler = StandardScaler()\ncreditcard_scaled = scaler.fit_transform(creditcard_df)","edda31fd":"inertiaValue = []\n\nfor ii in range(1, 25):\n    model = KMeans( n_clusters = ii )\n    model.fit(creditcard_scaled)\n    inertiaValue.append(model.inertia_)\n    \nplt.plot(inertiaValue,'bs-')","3d8aad76":"kmeans = KMeans(7)\nkmeans.fit(creditcard_scaled)\nlabels = kmeans.labels_\ny_kmeans = kmeans.fit_predict(creditcard_scaled)","9899caba":"# New column including the cluster for each sample\ncreditcard_df_cluster = pd.concat([creditcard_df, pd.DataFrame({'cluster':labels})], axis = 1)\ncreditcard_df_cluster.head()","623e883a":"columnsName = creditcard_df_cluster.columns\nfor ii, columnName in enumerate(columnsName): \n    plt.figure(figsize=(35,5))\n    \n    for jj in range(7):\n        plt.subplot(1, 7, jj+1)    \n        plot_df = creditcard_df_cluster[creditcard_df_cluster['cluster'] == jj]    \n        plt.hist(plot_df[columnName], alpha=.4, bins=30)\n        plt.title('{}    \\nCluster {} '.format(columnName, jj))\n    \nplt.tight_layout()","06876f76":"pca = PCA(n_components=2)\nprincipal_comp = pca.fit_transform(creditcard_scaled)\n\n# Create a dataframe with the two components\npca_df = pd.DataFrame(data = principal_comp, columns =['PCA 1','PCA 2'])\npca_df['Cluster'] = labels  \npca_df.head()","3202b87a":"plt.figure(figsize=(10,10))\nsns.scatterplot(x=\"PCA 1\", y=\"PCA 2\", hue = \"Cluster\", data = pca_df, palette =['red','green','blue','pink','yellow','gray','purple'])\nplt.xlabel('PCA 1')\nplt.xlabel('PCA 2')\nplt.title('Principal Component Analysis (PCA) by Cluster')","c4a7e333":"* **Correlation matrix** - helps visualizing how the different variables are correlated. ","4965fefa":"* Checking for __null values__ --> MINIMUM_PAYMENTS and CREDIT_LIMIT COLUMNS present null values","0af64856":"<a id='eda'><\/a>\n## Exploratory Data Analysis (EDA)\n\nTo better understand the data it is important to explore the dataset. First a statistical description of the dataset is obtained. Then null values are replaced with the mean of the column. Next, histograms and correlation plots are done.\n\n* __Describe -__ Main statistical information is obtained: minimum, maximum values as well as standard deviations and quarters.\n\n","a8d318ef":"[BACK](https:\/\/www.slopezza.com\/playground)\n<br> <br>\n[GitHub repository](https:\/\/github.com\/smlopezza\/CustomerSegmentation)\n<br>\n[Kaggle](https:\/\/www.kaggle.com\/smlopezza\/customer-segmentation-credit-card-transactions)\n\n# Customer Segmentation from Credit Card Transactions\n\n*Customer segmentation* divides costumers into groups based on common characteristics. This is useful for companies to be able to strategize their marketing estrategies in a more effective way. One of the main goals is to identify ways to improve products or new product or service opportunities.\n\n[Clustering](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#k-means) is an *Unsupervised Learning* methodology, useful to develop customer segmentation analysis. One of the most known algorithms is KMeans. KMeans requires the number of clusters to be specified and clusters data by trying to separate samples in n groups of equal variance, minimizing inertia. KMeans scales well to large number of samples.\n\n\n## Objective\nIn this playground project, a customer segmentation using KMeans is developed. The dataset sumarizes the behaviour of 8950 clients and includes information such as balance, purchases, cash advance, among others.\n\nIn this notebook I am practicing my skills in:\n* __Pandas__ to handling the data\n* __Clustering__ (KMeans --> Sklearn)\n\n\n\n## Resources\n* The data for this notebook can be found at [Kaggle (data source)](https:\/\/www.kaggle.com\/arjunbhasin2013\/ccdata) <br>\n* A tutorial from [Ryan Ahmed](https:\/\/www.coursera.org\/instructor\/~48777395) using this dataset can be found as a [Coursera Guided Project](https:\/\/www.coursera.org\/projects\/machine-learning-for-customer-segmentation)\n\n\n## Graphical Summary of Results\n* 7 Clusters were used for customer segmentation\n![PCA.png](attachment:PCA.png)\n\n\n\n## Contents\n0. <a href='#eda'> Exploratory Data Analysis (EDA) <\/a>\n1. <a href='#clustering'> Clustering: K-Means <\/a>\n2. <a href='#pca'> Principal Component Analysis (PCA) <\/a>\n3. <a href='#conclusions'> Conclusions <\/a>\n\n\n## Initial Setup\n* Import the packages needed for the notebook","c9f35860":"* Check for **duplicates**","9ddc645a":"### Visualization\n\nAll the varialbes are plote according to the cluster assigned by k-Means. However, it is not possible to visualize all the 17 features. In this case a Principal Component Analysis is needed.","da652bb0":"* **Histogram Plots** - help to understand how the values on the different parameters are distributed","afbd208b":"### Elbow method\nTo decide how many clusters are needed, [elbow method](https:\/\/en.wikipedia.org\/wiki\/Elbow_method_(clustering)#:~:text=In%20cluster%20analysis%2C%20the%20elbow,number%20of%20clusters%20to%20use.) can be used. The idea is to find the proper number of clusters accounting for both, inertia and computer time. Here we selected 7 as the optimum value.","e21d04c2":"## Load Data","b41be0e3":"<a id='clustering'><\/a>\n## Clustering: k-Means\n\n[KMeans clustering](https:\/\/en.wikipedia.org\/wiki\/K-means_clustering) was originated for signal processing. It splits the data into *k* clusters in which each observation belongs to the cluster with the nearest mean. More information about K-Means [here](https:\/\/towardsdatascience.com\/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a).\n\n\n### Scale\/standardize\nThe first step to apply k-Means is to scale\/standarize the features. It is not convinient to have features at different scale. Here I am using a standar scaler.\n","8651f6ad":"### Applying k-Means","816cb9dc":"<a id='pca'><\/a>\n## Principal Component Analysis (PCA)\n\n[PCA](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis) is a dimensional reduction technique. The idea is to be able to visualize the different clusters by reducing to 2 dimensions.","8ec36b21":"* There are different options to work with null values. One would be to drop them, but in this case I am filling them with the mean value of the column","d9ddb9c2":"<a id='conclusions'><\/a>\n## Conclusions\n* It was possible to use k-Means to perform customer segmentation of 8950 credit card users.\n* 7 Clusters can explain the customers behaviors\n* PCA can be used to visualize the clustering"}}