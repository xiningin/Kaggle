{"cell_type":{"789315f6":"code","1e9a7629":"code","dab36200":"code","a8cd6e02":"code","8054aca1":"code","1455d7df":"code","fa0aa800":"code","c4444f09":"code","5bf44d23":"code","7458d376":"code","e35a72aa":"code","99db9273":"code","20cfdc37":"code","a4591baf":"code","ba5ded97":"code","0b7bad9d":"code","6906fa03":"code","f4330ce9":"code","94f1b831":"code","50892c93":"code","79ecb84b":"code","aae60091":"code","3f9aaf94":"code","7cb7437f":"code","012b5ffb":"code","e8366e3e":"code","132f9cbf":"markdown","b135ccd0":"markdown","f16667ec":"markdown","8d38eee7":"markdown","fd41963c":"markdown","0002cfdc":"markdown","28d03d46":"markdown","e567c7fd":"markdown","ec687fd1":"markdown","f28dd067":"markdown","58bdd8cf":"markdown","12c14ed5":"markdown","567ad371":"markdown","589564cf":"markdown","ff1a42f0":"markdown","d30d6a54":"markdown","370c9f31":"markdown","3f90a962":"markdown","9c1d765b":"markdown","e52ff6f9":"markdown","34ccd5fc":"markdown","cdc0d4f5":"markdown"},"source":{"789315f6":"# importing Pytorch modules\n\nimport torch\n\nx = torch.empty(5,3)       #torch.empty generates random values\nprint(x)\n\ny= torch.ones(5,3)         #torch.ones generates number of ones\nprint(y)\n\na = torch.tensor([[0,1,2],[3,4,5]])  #it helps in saving in matrix format\nprint(a)","1e9a7629":"# importing pytorch modules\nimport torch\nimport torch.nn as nn                            # to access built-in functions to make a NN\nimport torch.optim as optim                      # optimizers\nfrom torchvision import datasets, transforms     # to access the MNIST dataset\nimport torch.nn.functional as F   # to access activation functions\n\n# libraries you may know already\nimport numpy as np\nimport matplotlib.pyplot as plt # for plotting\n%matplotlib inline\nimport seaborn as sns\nimport sklearn.metrics\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","dab36200":"# Load in the data from torchvision datasets \n# train=True to access training images and train=False to access test images\n# We also transform to Tensors the images\nmnist_train = datasets.MNIST('data', train = True, download = True, transform=transforms.ToTensor())\nmnist_test = datasets.MNIST('data', train = False, download = True, transform=transforms.ToTensor())","a8cd6e02":"# How the object looks:\nprint('Structure of train data:', mnist_train, '\\n')\nprint('Structure of test data:', mnist_test, '\\n')\nprint('Image on index 0 shape:', list(mnist_train)[0][0].shape)\nprint('Image on index 0 label:', list(mnist_train)[0][1])","8054aca1":"# Visualize sample of images:\nsample = datasets.MNIST('data', train=True, download=True)\n\nplt.figure(figsize = (16, 3))\nfor i,(image, label) in enumerate(sample):\n    if i>=16:\n        break\n    plt.subplot(2,8,i+1)\n    plt.imshow(image)","1455d7df":"#creating the network\nclass MNISTClassifier(nn.Module):                  # nn.Module is a subclass from which we inherit\n    def __init__(self):                            #defining structure\n        super(MNISTClassifier, self).__init__() \n        self.layers = nn.Sequential(nn.Linear(28*28, 50),   # adding first layer: 784 neurons to 50\n                                    nn.ReLU(),               # calling activation function\n                                    nn.Linear(50, 20),      # adding second layer: 50 neurons to 20\n                                    nn.ReLU(),               # activation function\n                                    nn.Linear(20, 10)        # output layer having 10 neurons\n                                   )\n        #output layer contains number of classes we want to predict\n        \n    def forward(self, image, prints=False):\n        if prints: print(\"Image shape:\", image.shape)\n        image = image.view(-1,28*28)                   # Flatten image: from [1, 28, 28] to [784]\n        if prints: print('Image reshaped:', image.shape)\n        out = self.layers(image)                            # Create Log Probabilities\n        if prints: print('Out shape:', out.shape)\n        \n        return out","fa0aa800":"torch.manual_seed(1) # set the manual seed\nnp.random.seed(1)  # set random seed in numpy\n\n# Selecting 1 image with its label\nimage_ex, label_ex = mnist_train[0]\nprint(\"Image shape:\", image_ex.shape,\"\\n\")\nprint(\"Label:\", label_ex,\"\\n\")\n\n# creating an instance of model\nmodel_example = MNISTClassifier()\nprint(model_example,\"\\n\")\n\n# creating log probabilities\nout = model_example(image_ex, prints=True)\nprint(\"out:\",out,\"\\n\")\n\n# Choose maximum probability and then select only the label (not the prob number)\nprediction = out.max(dim=1)[1]\nprint(\"Prediction:\", prediction)","c4444f09":"# LOSS and Optimizer instances\n\n# Loss is the function that calculates how far is the prediction from the true value\ncriterion = nn.CrossEntropyLoss()\nprint(\"Criterion:\", criterion,\"\\n\")\n\n# Using this loss the Optimizer computes the gradients of each neuron and updates the weights\noptimizer = optim.SGD(model_example.parameters(), lr = 0.004, momentum=0.9)   #stochastic gradient descent\nprint(\"Optimizer:\",optimizer,\"\\n\")","5bf44d23":"# Let's also look at how many parameters (weights and biases) are updating during 1 single backpropagation\n# Parameter Understanding\nfor i in range(6):\n    print(i+1,\":\",list(model_example.parameters())[i].shape)","7458d376":"torch.manual_seed(1)  # set random seed\nnp.random.seed(1)  # set random seed for numpy\n\nprint(\"Log Probabilities:\",out,\"\\n\")\nprint(\"Actual Value:\", torch.tensor(label_ex).reshape(-1))\n\n# clear gradients: needs to be done before applying backpropagation\noptimizer.zero_grad()\n# compute loss\nloss = criterion(out, torch.tensor(label_ex).reshape(-1))\nprint(\"Loss:\", loss,\"\\n\")\n\n#compute gradients\nloss.backward()\n\n# update weights\noptimizer.step()\n\n# After this 1 iteration the weights have updated once","e35a72aa":"# Create trainloaders for train and test data\n# We put shuffle=True so the images shuffle after every epoch\ntrain_loader = torch.utils.data.DataLoader(mnist_train, batch_size=60, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(mnist_test, batch_size=60, shuffle=True)\n\n# inspect Trainloader\nprint(\"Train loader:\",train_loader,\"\\n\")\n\n# select first batch\nimgs, labels = next(iter(train_loader))\n\nprint('Object shape:', imgs.shape)      # [60, 1, 28, 28]: 60 images of size [1, 28, 28]\nprint('Label values:', labels)          # actual labels for the 60 images\nprint('Total Images:', labels.shape)    # 60 labels in total","99db9273":"# setting up seed\ntorch.manual_seed(1)\nnp.random.seed(1)\n\nfor i, (images, labels) in enumerate(train_loader):\n    # stop after three iterations\n    if i>=3:\n        break\n        \n    print(\"-----Batch \",i,\"-----\")\n    #prediction\n    out = model_example(images)\n    print(\"out shape:\", out.shape)\n    \n    # update weights\n    loss = criterion(out, labels)\n    print(\"loss:\", loss)\n    \n    print(\"Optimizing:\")\n    # Clears the gradients of all optimized\n    optimizer.zero_grad()\n    # computes optimizer\n    loss.backward()\n    #performs a single optimization step\n    optimizer.step()\n    \n    print(\"Done.\\n\")","20cfdc37":"# Instantiate 2 variables for total cases and correct cases\ncorrect_cases = 0\ntotal_cases = 0\n\n# Sets the module in evaluation mode (VERY IMPORTANT)\nmodel_example.eval()\n\nfor k, (images, labels) in enumerate(train_loader):\n    # Just show first 3 batches accuracy\n    if k >= 3: break\n    \n    print('==========', k, ':')\n    out = model_example(images)\n    print('Out:', out.shape)\n    \n    # Choose maximum probability and then select only the label (not the prob number)\n    prediction = out.max(dim = 1)[1]\n    print('Prediction:', prediction.shape)\n    \n    # Number of correct cases - we first see how many are correct in the batch\n            # then we sum, then convert to integer (not tensor)\n    correct_cases += (prediction == labels).sum().item()\n    print('Correct:', correct_cases)\n    \n    # Total cases\n    total_cases += images.shape[0]\n    print('Total:', total_cases)\n    \n    \n    if k < 2: print('\\n')\n        \n\nprint('Average Accuracy after 3 iterations:', correct_cases\/total_cases)","a4591baf":"def get_accuracy(model, data, batchSize = 20):\n    '''Iterates through data and returnes average accuracy per batch.'''\n    # Sets the model in evaluation mode\n    model.eval()\n    \n    # Creates the dataloader\n    data_loader = torch.utils.data.DataLoader(data, batch_size=batchSize)\n    \n    correct_cases = 0\n    total_cases = 0\n    \n    for (images, labels) in iter(data_loader):\n        # Is formed by 20 images (by default) with 10 probabilities each\n        out = model(images)\n        # Choose maximum probability and then select only the label (not the prob number)\n        prediction = out.max(dim = 1)[1]\n        # First check how many are correct in the batch, then we sum then convert to integer (not tensor)\n        correct_cases += (prediction == labels).sum().item()\n        # Total cases\n        total_cases += images.shape[0]\n    \n    return correct_cases \/ total_cases","ba5ded97":"get_accuracy(model_example,mnist_train,20)","0b7bad9d":"def train_network(model, train_data, test_data, batchSize=20, num_epochs=1, learning_rate=0.01, weight_decay=0,\n                 show_plot = True, show_acc = True):\n    \n    '''Trains the model and computes the average accuracy for train and test data.\n    If enabled, it also shows the loss and accuracy over the iterations.'''\n    \n    print('Get data ready...')\n    # Create dataloader for training dataset - so we can train on multiple batches\n    # Shuffle after every epoch\n    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batchSize, shuffle=True)\n    \n    # Create criterion and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n    \n    # Losses & Iterations: to keep all losses during training (for plotting)\n    losses = []\n    iterations = []\n    # Train and test accuracies: to keep their values also (for plotting)\n    train_acc = []\n    test_acc = []\n    \n    print('Training started...')\n    iteration = 0\n    # Train the data multiple times\n    for epoch in range(num_epochs):\n        \n        for images, labels in iter(train_loader):\n            # Set model in training mode:\n            model.train()\n            \n            # Create log probabilities\n            out = model(images)\n            # Clears the gradients from previous iteration\n            optimizer.zero_grad()\n            # Computes loss: how far is the prediction from the actual?\n            loss = criterion(out, labels)\n            # Computes gradients for neurons\n            loss.backward()\n            # Updates the weights\n            optimizer.step()\n            \n            # Save information after this iteration\n            iterations.append(iteration)\n            iteration += 1\n            losses.append(loss)\n            # Compute accuracy after this epoch and save\n            train_acc.append(get_accuracy(model, train_data))\n            test_acc.append(get_accuracy(model, test_data))\n            \n    \n    # Show Accuracies\n    # Show the last accuracy registered\n    if show_acc:\n        print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n        print(\"Final Testing Accuracy: {}\".format(test_acc[-1]))\n    \n    # Create plots\n    if show_plot:\n        plt.figure(figsize=(10,4))\n        plt.subplot(1,2,1)\n        plt.title(\"Loss Curve\")\n        plt.plot(iterations[::20], losses[::20], label=\"Train\", linewidth=4, color='#008C76FF')\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        plt.subplot(1,2,2)\n        plt.title(\"Accuracy Curve\")\n        plt.plot(iterations[::20], train_acc[::20], label=\"Train\", linewidth=4, color='#9ED9CCFF')\n        plt.plot(iterations[::20], test_acc[::20], label=\"Test\", linewidth=4, color='#FAA094FF')\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend(loc='best')\n        plt.show()","6906fa03":"mnist_data = datasets.MNIST(\"data\", train=True, download=True, transform = transforms.ToTensor())\nmnist_data = list(mnist_data)\n\nmnist_train = mnist_data[:600]   # 600 images for training \nmnist_test = mnist_data[600:1000]  # 400 images for testing\n\nmodel = MNISTClassifier()\n\n# train the model using above function(predefined)\ntrain_network(model, mnist_train, mnist_test, num_epochs=200)","f4330ce9":"# Predefined Function that shows 20 images\ndef show_image(data, title=\"Default\"):\n    plt.figure(figsize=(10,2))\n    for i,(image, label) in enumerate(data):\n        if i>=20:\n            break\n        plt.subplot(2,10,i+1)\n        plt.imshow(image)\n        plt.suptitle(title, fontsize=15)","94f1b831":"# creating original and rotated images\noriginal_images = datasets.MNIST(\"data\", train=True, download=True)\nrotated_images = datasets.MNIST(\"data\", train=True, \n                                download=True, transform = transforms.RandomRotation(35,fill=(0,)))\n# show images\nshow_image(original_images,\"Original\")\nshow_image(rotated_images, \"Rotated\")","50892c93":"# transform\nmytransform = transforms.Compose([transforms.RandomRotation(35, fill=(0,)),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize([0.5], [0.5])])\n# import mnist data and apply transformations\nmnist_data_aug = datasets.MNIST(\"data\", train=True, download=True, transform=mytransform)\nmnist_data_aug = list(mnist_data_aug)\n\n# training data\nmnist_train_aug = mnist_data_aug[:600]  # 600 augmented images for training\n\n# ------ Training the model ------\n# Create Model Instance\nmodel_aug = MNISTClassifier()\n\n# train the network\ntrain_network(model_aug, mnist_train_aug, mnist_test, num_epochs=200)","79ecb84b":"model2 = MNISTClassifier()\n\n#train the network\ntrain_network(model2, mnist_train, mnist_test, num_epochs=220, learning_rate=0.001, weight_decay=0.0005)","aae60091":"class MNIST_Classifier_Improved(nn.Module):\n    def __init__(self, layer1_size=50, layer2_size=20, dropout=0.4):    #structure of FNN\n        super(MNIST_Classifier_Improved, self).__init__()\n        \n        self.layers = nn.Sequential(nn.Dropout(p= dropout),       #dropout for first layer\n                      nn.Linear(28*28, layer1_size),              # 784 neurons to 50\n                      nn.ReLU(),                                  # activation functino\n                      nn.Dropout(p=dropout),                      # Dropout for second layer\n                      nn.Linear(layer1_size, layer2_size),        # Second layer: 50 neurons to 20\n                      nn.ReLU(),                                  # activation function\n                      nn.Dropout(p=dropout),                      # dropout for output layer\n                      nn.Linear(layer2_size, 10))                 # output layer\n         \n    def forward(self, image):              # taking image through NN                       \n        image= image.view(-1, 28*28)       # Flatten image (Matrix to Vector)\n        out = self.layers(image)           #  log probabilities output\n        \n        return out","3f9aaf94":"# Training Improved model now:\nmodel_improved = MNIST_Classifier_Improved(layer1_size=80, layer2_size=50, dropout=0.5)\nprint(model_improved,\"\\n\")\n\ntrain_network(model_improved, mnist_train, mnist_test,num_epochs=220,learning_rate=0.001,weight_decay=0.0005)","7cb7437f":"get_accuracy(model_improved, mnist_train,64)","012b5ffb":"def get_confusion_matrix(model, test_data):\n    # First we make sure we disable Gradient Computing\n    torch.no_grad()\n    \n    # Model in Evaluation Mode\n    model.eval()\n    \n    preds, actuals = [], []\n\n    for image, label in mnist_test:\n        # Add 1 more dimension for batching\n        image = image.unsqueeze(0)\n        out = model_improved(image)\n\n        prediction = torch.max(out, dim=1)[1].item()\n        preds.append(prediction)\n        actuals.append(label)\n    \n    return sklearn.metrics.confusion_matrix(preds, actuals)","e8366e3e":"plt.figure(figsize=(16,6))\nsns.heatmap(get_confusion_matrix(model_improved, mnist_test), cmap=\"icefire\", annot=True, linewidths=0.1)\nplt.title(\"Confusion Matrix\", fontsize=14)","132f9cbf":"### 7.1.1 Training on Augmented Data:\n\n* `transforms.Normalize()`: means to scale the input features of a neural network, so that all features are scaled similarly. For images is not really necessary, as they all have the same structure, but I threw it here just for reference.","b135ccd0":"Great!! You just made it through this!","f16667ec":"If you have any questions, please do not hesitate to ask. This notebook is made to bring more clear understanding of concepts and coding, so this would also help me add, modify and improve it. \n\n<div class=\"alert alert-block alert-warning\"> \n<p>If you liked this, upvote!<\/p>\n<p>Cheers!<\/p>\n<\/div>","8d38eee7":"## 5.2 Accuracy of the Classifier\nDuring Training, we would usually want to check for the accuracy of the model, to see how good or how bad is performing.\n\n<div class=\"alert alert-block alert-info\"> \n<strong>Note<\/strong>: During <strong>training<\/strong>, it is highly important to set the model into training mode by calling <code>your_model.train()<\/code>. This enables gradients training, the Dropout() function etc. When you <strong>evaluate<\/strong> the model call <code>your_model.eval()<\/code>. This disables the gradients, Dropout() function etc and sets the model in evaluation mode.\n<\/div>","fd41963c":"# 2. Neural Networks\n\n## 2.1 Youtube videos are the best if you want to grasp more and save your valuable time.\nHere are some links for video lectures:\n<div class=\"alert alert-block alert-info\">\n<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/ba\/3B1B_Logo.png' width='50' align='left'><\/img>\n<p><a href='https:\/\/www.youtube.com\/watch?v=aircAruvnKk&t=1007s'>What are Neural Networks?<\/a><\/p>\n<p><a href='https:\/\/www.youtube.com\/watch?v=IHZwWFHWa-w'>How do Neural Networks learn?<\/a><\/p>\n\nWatch above videos for visualizing neural networks and how it works.\n<\/div>\n\n## 2.2 Perceptron\nA **Perceptron** is a single layer neural network, while a **Multi Layer Perceptron** is called a Neural Network. \n\nI **highly suggest** reading [this blog post](https:\/\/towardsdatascience.com\/what-the-hell-is-perceptron-626217814f53) for some very good explanations.\n\n<img src = 'https:\/\/i.imgur.com\/IHgw2au.png' width='400'>\n\n## 2.3 Deep vs Shallow Networks\nPlain vanilla Neural Networks (or Feed Forward Neural Networks or FNNs) have the most simple architecture in the Neural Networks realm, but their basics will help you understand much more compicated stuff ahead.\n\nIn Feed Foward Neural Nets, the hidden layers gradually \/increase\/decrease in hidden size (number of neurons) so more and more **details** of the input (images, text etc.) can be grasped.\n\n\n<img src='https:\/\/i.imgur.com\/D8QhLWM.png' width=350>\n\nIt is known that Deep Neural Nets (thin and tall) are **better** than Shallow ones (fat and short). This happens because the deep ones can learn more and more abstract representations the *deeper* you go. Also, the number of parameters is smaller so the training is faster.\n\n... Let's start programming.\n\n# 3. The Data - MNIST\n\nWe'll be working in MNIST Dataset, which is usually the go-to dataset when starting in Neural Networks. Nevertheless, you can apply the following principles on any datasets (images, text, tabular data, audio data), as **all data can be represented in numbers**. Cuz this is just it: numbers.","0002cfdc":"## 7.2 Weight Decay and Learning Rate\n\n**Weight Decay**: The idea of weight decay is to *penalize large weights*. Large weights mean that the prediction relies heavily on the content of one or multiple pixels. So, we penalize them by adding and extra term to the `criterion` function.\n\n**Learning Rate**: This one is probably not new. In FNNs we train using gradient descent to update the weights. The learning rate *controls* [how much to change the model in response to the estimated error each time the model weights are updated](https:\/\/machinelearningmastery.com\/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks\/). If we choose a lr too large we might overshoot the local minima, while using a lr too small we might wait longer for the model to train, as the steps are tinier.\n\n<img src='https:\/\/srdas.github.io\/DLBook\/DL_images\/TNN2.png' width='400'>","28d03d46":"# 8. Confusion Matrix","e567c7fd":"## 4.1 Activation Functions\nAn activation function is a fancy way of saying that we are making the output of each neuron *nonlinear*, because we WANT to learn non-linear relationships between the input and the output.\n\nThere are maaany types of activation functions, but some of them are:\n\n### Rectifier Linear Unit (ReLu)\nThe function is linear when the activation is above zero, and is equal to zero otherwise.\n<img src=\"https:\/\/miro.medium.com\/max\/1026\/0*g9ypL5M3k-f7EW85.png\" width=\"350\">\n\n### Sigmoid\nThe sigmoid function has a tilted \"S\" shape, and its output is always between 0 and 1. They are *interpreted as probabilities* (probability of input to be digit 1, probability of input to be digit 2 etc.).\n<img src=\"https:\/\/miro.medium.com\/max\/4000\/1*JHWL_71qml0kP_Imyx4zBg.png\" width=\"350\">\n\n### Tanh\nA variation of the Sigmoid, but it outputs values between -1 and 1.\n<img src=\"https:\/\/mathworld.wolfram.com\/images\/interactive\/TanhReal.gif\" width=\"300\">\n\nThese Activation Functions squish the neuron's output between the 2 values, preventing big numbers becoming much bigger and small numbers becoming much smaller.\n\n## 4.2 Making a Forward Pass\n\nA forward pass is when you take the images one by one (or batch by batch, we'll come back to this) and we put them through the neural network, which outputs for each a log probability (10 in out case).\n\nLet's look at 1 example:\n\n<img src='https:\/\/i.imgur.com\/ywMFtDz.png' width='600'>","ec687fd1":"# 1. Introduction\nThis notebook is focused on Deep learning in baby steps. For a beginner, who has no idea about neural networks and how they work can be frustrating sometimes. But don't worry, I'll try to make it simpler and try to make you understand the concept.\n\nI will also be providing video lectures and articles that helped me.\n<div class=\"alert alert-block alert-warning\"> \n<strong>Note<\/strong>: Deep learning coding is VERY different in structure than the usual <em>sklearn<\/em> for machine learning. In addition, it usually works with <em>images<\/em> and <em>text<\/em>, while <em>ML<\/em> usually works with <em>tabular<\/em> data. So please, be patient with yourself and if you don't understand something right away, continue reading\/ coding and it will all make sense in the end.\n<\/div>\n\n<img src='https:\/\/miro.medium.com\/max\/1200\/1*4br4WmxNo0jkcsY796jGDQ.jpeg' width=200>\n\n**Pytorch** is a library that has many advantages over *Keras* and is widely used nowadays. It has different structure as compared to *sklearn*.\n\n**Tensors**: Instead of working with tabular data or numpy arrays, we'll be working with tensors. A tensor is a container which can house data in *N dimensions*. Although, Tensor is similar to numpy arrays, the difference is that it supports better GPU support so they are faster as compared to numpy arrays","f28dd067":"# 4. Vanilla FNN Neural Network\nif we were working with sklearn library then, we just have to call the object of the model e.g: let's say of `RandomForestClassifier()` and then hypertune their parameters for which there is already a predefined class which we don't have to create every time. But while dealing or working with neural networks, we certainly have to define our own class.\n\nFor Neural Networks is different: they can be so volatile, depending on the structure of your input (eg. an image of shape `[3, 500, 250]`), number of `hidden layers`, number of `neurons` in each hidden layer, whether or not you want to call the `Dropout()` functions etc. You can also build multiple neural networks and then combine them in another one (for example in a Sequence2Sequence RNN).\n\n>Note:`super()` function is there because the `MNISTClassifier` class inherits attributes from it's parent class `nn.Module`. By calling this function we make this possible. Removing it would lead to an *error*.","58bdd8cf":"## 5.3 Iterations vs Epochs:\n\n**Iterations**: number of iterations is the number of times we *update* the weights (parameters) of the FNN. For example, above we did 3 iterations.\n\n**Epoch**: number of times *all* training data was used once to update the parameters. This is used because, in general, we would want to train the network for longer. Until now in this notebook we haven't completed yet a full epoch.\n\n## 5.4 Predefined Functions: Accuracy and Training Loop\nNow let's create some functions, so our trainin process will become easier:\n\n### 5.4.1 Predefined Accuracy Function:","12c14ed5":"### 4.3.3 Do 1 BackPropagation : Obtain loss and update weights","567ad371":"### 4.3.2 MNISTClassifier trainable parameters:\n* 1 : torch.Size([50, 784]) - 50 weights (or parameters) for each 28x28 neurons (28x28x50 weights in total)\n* 2 : torch.Size([50]) - 50 biases\n* 3 : torch.Size([20, 50]) - 20 weights (or parameters) for each 50 neurons (50x20 weights in total)\n* 4 : torch.Size([20]) - 20 biases\n* 5 : torch.Size([10, 20]) - 10 weights (or parameters) for each 20 neurons (10x20 weights in total)\n* 6 : torch.Size([10]) - value of the final neurons (the log probabilities)","589564cf":"<div class=\"alert alert-block alert-info\"> \nPrediction is wrong because we have not yet trained the model.\nDon't worry we are going to get correct predictions soon\ud83d\ude09.\n<\/div>\n\n## 4.3 Backpropagation\n\nSo, the purpose is to UPDATE the weights and biases in the neural network so it *learns* to recognize the digits and accurately classify them. This is done during backpropagation, when the model literally goes back and updates the parameters (weights) a little bit. Before going any further, I highly recommend watching the following video which explains the concept of Backpropagation.\n\n<div class=\"alert alert-block alert-info\">\n<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/ba\/3B1B_Logo.png' width='50' align='left'><\/img>\n<p><a href='https:\/\/www.youtube.com\/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3'>What is Backpropagation really doing?<\/a><\/p>\n<p>Cheers again to 3Blue1Brown for his amazing structured videos.<\/p>\n<\/div>\n\n### 4.3.1 Loss and Optimizer Functions:\n\nThese 2 are like brother and sister: work hand in hand during the neural network training. They change by the case, but their main purpose is the same:\n\n**Loss Function (`criterion`): given an output and an actual, it computes the difference between them**\n* Regressive loss functions:\n    * MAE: `torch.nn.L1Loss()`\n    * MSE: `torch.nn.MSELoss()` etc.\n* Classification loss functions:\n    * Cross Entropy Loss: `torch.nn.CrossEntropyLoss()`\n    * Binary Cross Entropy Loss: `torch.nn.BCELoss()` etc.\n* Embedding Loss functions (whether 2 inputs are similar or not):\n    * Hinge Loss: `torch.nn.HingeEmbeddingLoss()`\n    * Cosine Loss: `torch.nn.CosineEmbeddingLoss()` etc.\n\n**Optimizer Function (`torch.optim`): updates the weights and biases to REDUCE the loss**\n* Examples:\n    * Stochastic Gradient Descent: `SGD()`\n    * Adam: `Adam()`   (widely used)\n    * Adagrad: `Adagrad()`\n    \nDifferent neural networks and purposes can require different loss and optimizer functions. Click [here](https:\/\/pytorch.org\/docs\/stable\/nn.html) to check all of them.","ff1a42f0":"## 7.3 Dropout() and Layer Optimization\n\n### 7.3.1 Dropout() Function\nThis technique builds *many* models and then averages their prediction at test time (this is why it is very important to call `model.eval()` when we want to evaluate).\n\nFor each model we **dropout** (drop out, zero out, remove etc.) a portion of neurons from each training iteration. Hence, in different iterations of training, we will drop out a different set of neurons.\n\nThis way we prevent the weight from being overly dependent on eachother: for example for one weight to be unnecessarily large to compensate for another unnecessarily large weight with the opposite sign. In other words, weights are encouraged to be *strong and independent*.\n<img src='https:\/\/miro.medium.com\/max\/1200\/1*iWQzxhVlvadk6VAJjsgXgg.png' width=400>\n\n\n### 7.3.2 Layer Optimization\nOur `MNISTClassifier()` had until now 3 layers with a fixed number on neurons in each layer. We can change that by making it changable during training, so eventually we can apply `Grid Search` and find the best combination possible.\n\n### 7.3.3 Changing the Structure of our MNISTClassifier()\nNow let's change our Neural Net a bit:\n* `nn.Dropout(p=0.4)`: each neuron has 40% chance of being dropped\n* `layer1_size`: size of the first hidden layer\n* `layer2_size`: size of the second hidden layer","d30d6a54":"### 5.1.1 Training the example network on a batch instead of image by image","370c9f31":"# 7. Overfitting\n\nAs any other Machine Learning Model, Neural Nets can suffer from overfitting. Overfitting is when a neural network model learns about the quirks of the training data, rather than information that is generalizable to the task at hand.\n\n## 7.1 Data Augmentation\nWhy try to collect more data when you can create some on your own? *Data Augmentation* generates more data points from our existing data set by:\n* Flipping each image horizontally or vertically (won't work for digit recognition, but might for other tasks)\n* Shifting each pixel a little to the left or right\n* Rotating the images a little\n* Adding noise to the image\n\n<img src='https:\/\/www.kdnuggets.com\/wp-content\/uploads\/cats-data-augmentation.jpg' width='400'>\n\nFor our example we'll rotate the images randomly up to 35 degrees.","3f90a962":"### 6.4.2 Predefined Training Function\n\n<img src='https:\/\/i.imgur.com\/S1miUl0.png' width=600>","9c1d765b":"Until now we:\n1. Created a Vanilla FNN\n2. Took 1 image through the network and create prediction\n3. Look at the prediction vs actual and computed the loss\n4. Using the loss we updated the weights and biases\n\nThis is called training. The next chapters will be dedicated to training the network and improving it.\n\n# 5. Training the Neural Network\nOur purpose now that we have the structure in place and the data is to make the Vanilla FNN perform well.\n\n## 5.1 Batches\nWith an artificial neural network, we may want to use more than one image at one time. That way, we can compute the *average* loss across a **mini-batch** of **multiple** images, and take a step to optimize the **average** loss. The average loss across multiple training inputs is going to be less \"noisy\" than the loss for a single input, and is less likely to provide \"bad information\" because of a \"bad\" input.\n\nBatches can have different sizes:\n* one extreme is `batch_size` = 1: meaning that we compute the loss and update after EACH image (so we have 60,000 batches of size 1)\n* a `batch_size` = 60: means that, for 60,000 training images, we'll have 1000 batches of size 60\n* the other extreme is `batch_size` = 60,000: when we input ALL images and do 1 backpropagation (we have 1 batch of size 60,000 images)\n\nThe actual batch size that we choose depends on many things. We want our batch size to be large enough to not be too \"noisy\", but not so large as to make each iteration too expensive to run.\n\n<img src='https:\/\/i.imgur.com\/M6ZkRXa.png' width='400'>\n\nIn the above example, instead of having 70 noisy losses we'll have just 7 averaged losses.","e52ff6f9":"#### Improved Model Structure:\n<img src='https:\/\/i.imgur.com\/m22zEqN.png' width='600'>","34ccd5fc":"# References:\n* [Andrada Olteanu](https:\/\/www.kaggle.com\/andradaolteanu)\n* [Create your own FNN](http:\/\/alexlenail.me\/NN-SVG\/index.html)\n* [WTF is a Tensor?](https:\/\/www.kdnuggets.com\/2018\/05\/wtf-tensor.html)\n* [What the hell is a Perceptron?](https:\/\/towardsdatascience.com\/what-the-hell-is-perceptron-626217814f53)\n* 3Blue1Brown videos:\n    * [But what is a Neural Network?](https:\/\/www.youtube.com\/watch?v=aircAruvnKk&t=1007s)\n    * [What is Backpropagation really doing?](https:\/\/www.youtube.com\/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3)\n    * [Gradient Descent, how Neural Networks learn](https:\/\/www.youtube.com\/watch?v=IHZwWFHWa-w)\n* [All `torch.` functions (including loss & optimizer functions)](https:\/\/pytorch.org\/docs\/stable\/nn.html)\n* [Impact of Learning Rate in NNs](https:\/\/machinelearningmastery.com\/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks\/)","cdc0d4f5":"# 6. Model Evaluation\n\nNow that we have our functions ready, we can start training on the ENTIRE dataset.\n\nBut first, to make the training faster, we will:\n* select 600 training images and 400 testing images\n* `batch_size` will be by default set to 20 images\/batch\n* we'll iterate through the data 200 times (`num_epochs`=200)"}}