{"cell_type":{"b6cefe01":"code","3dfe2db1":"code","1195a178":"code","e9ba75ed":"code","1cb51a8a":"code","4cbcae0f":"code","2193762b":"code","b8d62760":"code","5edd8dbe":"code","a679f18d":"code","9340333e":"code","25f741fb":"code","bf2dc504":"code","0a540730":"code","47d63c6b":"code","1209dbc7":"code","4eb078e2":"code","186c2954":"markdown","4a8108e8":"markdown","7b145024":"markdown","582d9c78":"markdown","f0d97107":"markdown","daa510e0":"markdown","57458314":"markdown","6c658ec1":"markdown","d00e35cd":"markdown","43b7b52c":"markdown","55fd5e51":"markdown","74fd5109":"markdown","3c5280bb":"markdown","7d3a856e":"markdown","fc6646ea":"markdown","734798ed":"markdown","1c3d774c":"markdown","827187e4":"markdown","26f26604":"markdown"},"source":{"b6cefe01":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn import datasets\n\nfrom sklearn.decomposition import (PCA, IncrementalPCA,\n                                   KernelPCA, TruncatedSVD,\n                                   FastICA, MiniBatchDictionaryLearning,\n                                   SparsePCA)\n\nfrom sklearn.manifold import (Isomap,\n                              LocallyLinearEmbedding)\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.random_projection import (GaussianRandomProjection,\n                                       SparseRandomProjection)\n\nfrom sklearn.neighbors import (KNeighborsClassifier,\n                               NeighborhoodComponentsAnalysis)\n                               \nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","3dfe2db1":"# Load Digits dataset\ndigits = datasets.load_digits()\nX, y = digits.data, digits.target\n\n# Parameters\ndim = len(X[0])\nn_classes = len(np.unique(y))\nn_neighbors = 3\nrandom_state = 0\n\n# Split into train\/test\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.5, stratify=y,\n                     random_state=random_state)","1195a178":"pca = make_pipeline(StandardScaler(),\n                    PCA(n_components=2,\n                        random_state=random_state))","e9ba75ed":"inc_pca = make_pipeline(StandardScaler(),\n                        IncrementalPCA(n_components=2))","1cb51a8a":"# kernel : \u201clinear\u201d | \u201cpoly\u201d | \u201crbf\u201d | \u201csigmoid\u201d | \u201ccosine\u201d | \u201cprecomputed\u201d\nkpca = make_pipeline(StandardScaler(),\n                     KernelPCA(kernel=\"cosine\",\n                               n_components=2,\n                               gamma=None,\n                               fit_inverse_transform=True,\n                               random_state=random_state,\n                               n_jobs=1))","4cbcae0f":"sparsepca = make_pipeline(StandardScaler(),\n                          SparsePCA(n_components=2,\n                                    alpha=0.0001,\n                                    random_state=random_state,\n                                    n_jobs=-1))","2193762b":"SVD = make_pipeline(StandardScaler(),\n                    TruncatedSVD(n_components=2,\n                                 algorithm='randomized',\n                                 random_state=random_state,\n                                 n_iter=5))","b8d62760":"GRP = make_pipeline(StandardScaler(),\n                    GaussianRandomProjection(n_components=2,\n                                             eps = 0.5,\n                                             random_state=random_state))","5edd8dbe":"lda = make_pipeline(StandardScaler(),\n                    LinearDiscriminantAnalysis(n_components=2))","a679f18d":"nca = make_pipeline(StandardScaler(),\n                    NeighborhoodComponentsAnalysis(n_components=2,\n                                                   random_state=random_state))","9340333e":"SRP = make_pipeline(StandardScaler(),\n                    SparseRandomProjection(n_components=2,\n                                           density = 'auto',\n                                           eps = 0.5,\n                                           random_state=random_state,\n                                           dense_output = False))","25f741fb":"isomap = make_pipeline(StandardScaler(),\n                       Isomap(n_components=2,\n                              n_jobs = 4,\n                              n_neighbors = 5))","bf2dc504":"miniBatchDictLearning = make_pipeline(StandardScaler(),\n                                      MiniBatchDictionaryLearning(n_components=2,\n                                                                  batch_size = 200,\n                                                                  alpha = 1,\n                                                                  n_iter = 25,\n                                                                  random_state=random_state))","0a540730":"FastICA = make_pipeline(StandardScaler(),\n                        FastICA(n_components=2,\n                                algorithm = 'parallel',\n                                whiten = True,\n                                max_iter = 100,\n                                random_state=random_state))","47d63c6b":"lle = make_pipeline(StandardScaler(),\n                    LocallyLinearEmbedding(n_components=2,\n                                           n_neighbors = 10,\n                                           method = 'modified',\n                                           n_jobs = 4,\n                                           random_state=random_state))","1209dbc7":"knn = KNeighborsClassifier(n_neighbors=n_neighbors)","4eb078e2":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Make a list of the methods to be compared\ndim_reduction_methods = {'PCA': pca, \n                         'LDA': lda, \n                         'NCA': nca, \n                         'INC PCA': inc_pca, \n                         'KPCA':kpca, \n                         'Sparced PCA': sparsepca, \n                         'SVD': SVD, \n                         'GRP' : GRP, \n                         'SRP': SRP, \n                         'IsoMap': isomap, \n                         'MBD': miniBatchDictLearning, \n                         'ICA': FastICA, \n                         'LLE': lle}\n\n\nplt.figure(figsize=(24, 36))\n\nfor j,(name, model) in enumerate(dim_reduction_methods.items()):\n    plt.subplot(5, 3, j + 1, aspect='auto')\n\n    # Fit the method's model\n    model.fit(X_train, y_train)\n\n    # Fit a nearest neighbor classifier on the embedded training set\n    knn.fit(model.transform(X_train), y_train)\n\n    # Compute the nearest neighbor accuracy on the embedded test set\n    acc_knn = knn.score(model.transform(X_test), y_test)\n\n    # Fit the methons using the fitted model\n    X_embedded = model.transform(X)\n    \n    # Creating a dataframe to easily plot the sample label\n    df = pd.DataFrame(np.concatenate((X_embedded, np.reshape(y, (-1, 1))), axis=1))\n\n    # Plot the projected points and show the evaluation score\n    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, s=20, cmap='Set1')\n    plt.title(\"{}, KNN (k={})\\nTest accuracy = {:.2f}\".format(name,\n                                                              n_neighbors,\n                                                              acc_knn))\n    plt.colorbar()\n    \n    # Label the data distributions\n    for i, number in enumerate(y_test):\n        plt.annotate(number,\n                     df.loc[df[2]==number,[0,1]].mean(),\n                     horizontalalignment='center',\n                     verticalalignment='center',\n                     weight='bold',\n                     size='20')\n    \nplt.show()","186c2954":"### Singular Value Decomposition\n\n**SVD** can be applied even on rectangular matrices; whereas, eigenvalues are defined only for square matrices. The equivalent of eigenvalues obtained through the SVD method are called singular values, and vectors obtained equivalent to eigenvectors are known as singular vectors. However, as they are rectangular in nature, we need to have left singular vectors and right singular vectors respectively for their dimensions.","4a8108e8":"## Use a nearest neighbor classifier to evaluate the methods","7b145024":"# Dimensionality Reduction in\u00a0Python\nThis article is derived from my work with surface electromyography (sEMG) signal.\n\nData weight has a significant impact on the time of implementation of an algorithm, and in some data, like most natural signals (sounds, images\u00a0, biological signals) the information has a relatively dilute form, a large amount of data contains a small amount of information.\n\nMoreover, nowadays,  most datasets have a large number of variables. In other words, they have a high number of dimensions along which the data is distributed. Visually exploring the data can then become challenging and most of the time even practically impossible to do manually.\n\nThereat, dimensionality reduction is the process of reducing the total number of features in our feature set using strategies like feature selection or feature extraction.\nThe techniques presented here will be implemented with python, so be sure to have python installed on your machine.","582d9c78":"### Kernel PCA\n\nKPCA makes it possible to perform complex nonlinear projections for dimensionality reduction.","f0d97107":"## Creating the ploting for the data\n\nEach algorithm will have it own chart with the feature distribution and the test accuracy of the KNN","daa510e0":"## Load the dataset, define the initial parameters and split the train\/test set","57458314":"### Sparse PCA\n\n**Sparse PCA** uses the links between the ACP and the SVD to extract the main components by solving a lower-order matrix approximation problem.","6c658ec1":"### ISOMAP\n\nIt is a nonlinear dimensionality reduction method based on spectral theory that attempts to preserve geodetic distances in the lower dimension.","d00e35cd":"### Neighborhood Components Analysis\n\n**Neighborhood Component Analysis (NCA)** is a machine learning algorithm for metric learning. It learns a linear transformation in a supervised fashion to improve the classification accuracy of a stochastic nearest neighbors rule in the transformed space.","43b7b52c":"### Gaussian Random Projection\n\nIn the random projection, data with a very large dimension (d) are projected\nin a two-dimensional space (kd) with a random matrix.","55fd5e51":"## Importing the libraries","74fd5109":"### Sparse Random Projection\n\nThe fragmented random matrix is an alternative to the dense random projection matrix with traditional methods of dimension reduction. It ensures similar embedding quality while maximizing memory efficiency and allowing faster calculation of projected data.","3c5280bb":"### Independent Component Analysis\n\n**Independent component analysis** is a method primarily used for signal processing to linearly separate mixed data.","7d3a856e":"## Define the dimensionality reduction algorithms to be used\n\nLet's use 2 dimensions, so we can create a chart later","fc6646ea":"### MiniBatch Dictionary Learning\n\nDictionary-based learning solves a problem of matrix factorization which amounts to finding a dictionary that can give good results under the condition of parsimony of the code.","734798ed":"### Locally Linear Embedding\n\n**LLE** works by first measuring how each training instance linearly relates to its closest neighbors (c.n.), and then looking for a low-dimensional representation of the training set where these local relationships are best preserved (more details shortly). This makes it particularly good at unrolling twisted manifolds, especially when there is not too much noise.","1c3d774c":"### Incremental PCA\n\nThe **incremental principal component analysis** is a variant of the ACP It only keeps the most significant singular vectors to project the data into a space to\nreduced size.","827187e4":"### Linear Discriminant Analysis\n\n**LDA** is a classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.","26f26604":"### PCA\n\n**Principal component analysis** is a statistical method that uses the process of linear, orthogonal transformation to transform a higher-dimensional set of features that could be possibly correlated into a lower-dimensional set of linearly uncorrelated features. These transformed and newly created features are also known as Principal Components or PCs. In any PCA transformation, the total number of PCs is always less than or equal to the initial number of features. The first principal component tries to capture the maximum variance of the original set of features."}}