{"cell_type":{"f2f66aa9":"code","a9fdb4a2":"code","4b1336d8":"code","029bf268":"code","0a8b42b0":"code","9df3fd3d":"code","21102e86":"markdown","ef8ef547":"markdown","740eec94":"markdown","309b9796":"markdown","cfd389d4":"markdown","60ed7401":"markdown","75e30cca":"markdown"},"source":{"f2f66aa9":"# Erstmal lesen wir die Daten ein, ja ja, mit Pandas ist das alles\n# einfacher, aber machen wir ruhig mal alles \"zu Fu\u00df\" :)\n\ndata = [\n ('Deadline?','Party?   ','faul?    ','Aktivit\u00e4t'),\n ('Dringend','Ja','Ja','Party'),\n ('Dringend','Nein','Ja','Lernen'),\n ('Bald','Ja','Ja','Party'),\n ('Nein','Ja','Nein','Party'),\n ('Nein','Nein','Ja','Kneipe'),\n ('Nein','Ja','Nein','Party'),\n ('Bald','Nein','Nein','Lernen'),\n ('Bald','Nein','Ja','TV'),\n ('Bald','Ja','Ja','Party'),\n ('Dringend','Nein','Nein','Lernen')\n]\n\n## Read the data\ndef attribute_values(data,pos):\n    '''\n        Collect all different values that occur in\n        a given column and return them as a list.\n        Note that duplicates are left out.\n    '''\n    vl = [] # we could use sets instead, but we want to index it later\n    for d in data:\n        if not (d[pos] in vl):\n            vl.append(d[pos])\n    return vl\n\n# Strip the header of and keep it (as a copy)\nattribute_names = data[0][:]\ndata = data[1:]\n\n# Assume that the last attribute is the class to predict\nclasspos   = len(data[0])-1\nclasses    = attribute_values(data,classpos)\nattvalues  = []\nattributes = list(range(0,len(data[0])-1)) # assume non-empty data \nfor att in attributes: \n    attvalues.append(attribute_values(data,att))\n\nprint(\"Domains of the attributes: \")\nfor i,att in enumerate(attvalues):\n    print(\"   \",attribute_names[i],\": \",att)\n\nprint(\"\\nClasses: \\n   \",attribute_names[classpos],\": \",classes,\"\\n\")","a9fdb4a2":"def build_named_P_from_data(data,classpos,classes,correct=False):\n    '''\n    Computes the distribution of probabilities for the given classes.\n    DON'T CALL WITH EMPTY data list, as this makes no sense and will\n    produce a runtime error anyway (and rightly so)\n    '''\n    # print(data,classpos,classes)\n    p = {}  # List of probabilities, for each element in classes this will\n            # contain a probability in the same position as the class in\n            # the classes list\n    count = {}\n    n = float(len(data)) # Works also in Python 2\n    for c in classes:\n        count[c] = 0\n    for d in data:\n        count[d[classpos]] += 1\n    for c in classes:\n        if not correct:\n            p[c] = count[c]\/n\n        else:\n            p[c] = (count[c]+1)\/(n+len(classes))\n    return p\n\n\nprint(\"Prior probability distribution for the classes: \\n\")   \nP_named_classes = build_named_P_from_data(data,classpos,classes)\nprint(\"\\n\",P_named_classes)","4b1336d8":"def show_data(data):\n    for d in data:\n        print(\"  \",d)\n\n# Select tuples from some given data\ndef select_data(data,attribute,value):\n    return [d for d in data if d[attribute] == value]\n\nS_party_ja = select_data(data,classpos,'Party')\nshow_data(S_party_ja)\nprint(\"\\nLikelihoods of  over shown dataset for Attribute\",attribute_names[0],\":\",\n      build_named_P_from_data(S_party_ja,0,attvalues[0]))\n\nprint(\"\\nNow we want to correct it as done in the text above:\")\nprint(\"Corrected Likelihoods of for Attribute\",attribute_names[0],\":\",\n      build_named_P_from_data(S_party_ja,0,attvalues[0],correct=True))\n\nP_attribute_given_class = {}\n\nprint(\"\\nAll likelihoods: \\n\")\n# Compute it for all classes\nfor a in attributes:\n    P_attribute_given_class[attribute_names[a]] = {}\n    print(\"\\n\",attribute_names[a])\n    for c in classes:\n        p = build_named_P_from_data(select_data(data,classpos,c),a,attvalues[a],correct=True)           \n        P_attribute_given_class[attribute_names[a]][c] = p\n        print(\"  \",c,\":\",p)    ","029bf268":"## Now, let's predict!\n\ndef P_class_given_sample(s):    \n    global P_classes # has been determined already, see above\n    p = {}\n    p_result = []\n    # P(Attribute=value|class) * ...\n    for i,v in enumerate(s):\n        for c in classes:\n            p1 = P_attribute_given_class[attribute_names[i]][c][v]\n            if not c in p: \n                p[c] = p1\n            else:\n                p[c] *= p1\n                \n    # ... * P(class)\n    for c in classes:\n        p[c] *= P_named_classes[c]\n        p_result.append(p[c])\n    return p_result # or return p for named results (as a dictionary)\n\ns1 = ('Dringend','Nein','Nein')\ns2 = ('Dringend','Ja','Nein')\n\nprint(classes)\nprint(s1,\"-->\",P_class_given_sample(s1))\nprint(s2,\"-->\",P_class_given_sample(s2))\n\n# Argmax delivers in our case the index of the maximum entry in a list\n# one could also use argmax from numpy ... or do it manually or\n# with a shorthand requiring two passes: lst.index(max(lst))\ndef argmax(iterable):\n    return max(enumerate(iterable), key=lambda x: x[1])[0]\n\n# predict gives us ONE OF (it should be the first in the list)\n# the classes (as an index value pointing into \"classes\")\n# with the maximum probability.\n# NOTE: It may be useful sometime to deliver a normalized\n# probability distribution instead of a single class\ndef predict(cond_prob,debug=True):\n    # Look for a maximum entry\n    if debug: print(\"   \",cond_prob)\n    return classes[argmax(cond_prob)]\n\nprint(\"\\nNow let's make prediction for all samples we know already:\")\nfor d in data:\n    print(d,\"->\",predict(P_class_given_sample(d[0:classpos])))","0a8b42b0":"## Wenden wir das auf die noch nie gesehenen unklassifizierten Samples an:\nunknown_samples = [\n    ('Dringend','Ja','Nein',None),\n    ('Bald','Ja','Nein',None),\n    ('Nein','Ja','Ja',None),\n    ('Nein','Nein','Nein',None)\n]\n\nprint(classes)\nfor d in unknown_samples:\n    print(d,\"->\",predict(P_class_given_sample(d[0:classpos])))","9df3fd3d":"## ignore\nfrom IPython.display import display, Markdown, Latex\ndisplay(Markdown('*some markdown* $\\phi$'))","21102e86":"## Naive Bayes aus Sicht von Tom Mitchell\n\nSchalten wir aber einen Gang zur\u00fcck und schauen uns einmal an, was Tom Mitchell in seinem [neuen Buch zu Naive Bayes](http:\/\/www.cs.cmu.edu\/~tom\/mlbook\/NBayesLogReg.pdf) sagen wird.\n\nGegeben sei ein **supervised learning problem**: wir m\u00f6chten einen unbekannte **Target Function** $f: X \\rightarrow Y$ approximieren bzw., \u00e4quivalent, $P(Y|X)$. Hinweis: mit dem $P$ ist hier eine Wahrscheinlichkeitsverteilung gemeint. Ehrlich gesagt, br\u00e4uchte die behauptete \u00c4quivalenz von Funktion und Verteilung noch eine wenig Erl\u00e4uterung, denn eine Funktion sollte einen Wert der m\u00f6glichen Werte f\u00fcr $Y$ ausw\u00e4hlen, eine Verteilung erzeugt aber eine Wahrscheinlichkeit f\u00fcr jeden Wert und wird erst mit einem $argmax$, also einer Entscheidungsregel, zu einer Funktion. Nun gut, weiter im Text:\n\nWir nehmen zun\u00e4chst folgendes an:\n   * $Y$ ist eine boole'sche Zufallsvariable (also mit den Auspr\u00e4gungen _True_ und _False_).\n   * $X$ ist ein Vektor aus $n$ boole'schen Attributen, d.h. $X = (X_1,\\dots,X_n)$ und die $X_i$ ist die boole'sche Zufallsvariable, die das $i$-te Attribut von $X$ darstellt.\n   * $P(Y = y_i \\mid X)$ l\u00e4\u00dft sich wie folgt darstellen\n   \n   \\begin{equation}\n   P(Y=y_i \\mid X=x_k) = \\frac{P(X=x_k \\mid Y=y_i) P(Y=y_i)}{\\sum_j P(X=x_k \\mid Y=y_j)P(Y=y_j)}\n   \\end{equation}\n\nHier ist $y_m$ der $m$-te m\u00f6gliche Wert von $Y$ (bei einer boole'schen Zufallsvariable gibt es nur $2$ Werte, klar), $x_k$ ist der $k$-te m\u00f6gliche **(Vektor)-Wert** des Vektors $X$, und die Summe l\u00e4uft f\u00fcr dieses $x_k$ \u00fcber alle m\u00f6glichen Werte von $Y$.\n\nNoch eine kleine Erkl\u00e4uterung zu $x_k$. Nehmen wir ein Beispiel, $X$ sei ein Vektor, der aus 3 Komponenten besteht, d.h. $X = (X_1,X_2,X_3)$ und die $X_i$ sind hier boole'sche Zufallsvariablen, die jeweils das $i$-te Attribut von $X$ darstellen. Dann k\u00f6nnen wir eine Anordnung der m\u00f6glichen Werte von $X$ finden, z.B. so $[x_0=(0,0,0),x_1=(0,0,1),\\dots,x_7=(1,1,1)]$. Hier repr\u00e4sentierten wir mit $0$ False und mit $1$ True. Das $x_k$ oben l\u00e4uft nun \u00fcber die Anordnung, es wird also jeder m\u00f6gliche (Vektor-)Wert der Variable $X$ in die Hand genommen, jeder bestimmt eine Verteilung \u00fcber den Auspr\u00e4gungen f\u00fcr $Y$. Nat\u00fcrlich kann man das Subskript von $x_k$ aus der Position in der Sequenz folgern, man braucht also nicht $x_0$,... usw. hinzuschreiben, falls man mal eine Anordnung der m\u00f6glichen Werte angeben wollen w\u00fcrde.\n\nEin Weg, um nun $P(Y \\mid X)$ zu lernen, ist es, die Trainingsdaten zu verwenden, um $P(X \\mid Y)$ und $P(Y)$ zu sch\u00e4tzen. F\u00fcr jedes neue $x_k$ k\u00f6nnen wir dann mit _Bayes_ (s. die Formel eben) $P(Y \\mid X=x_k)$ sch\u00e4tzen.\n\nEine wichtige erste Frage hier ist: **wieviel Trainingsdaten ben\u00f6tigen wir, um die beiden Verteilungen 'zuverl\u00e4ssig' sch\u00e4tzen zu k\u00f6nnen**?\n\nNehmen wir an, wir ziehen die Trainingsbeispiele zuf\u00e4llig entsprechend einer zugrunde liegenden Verteilung $P(X)$ und erlauben dann einem Experten (Teacher), die Beispiele zu labeln (Ihnen also einen Wert f\u00fcr $Y$ zuzuordnen).\n\nIn der Regel reichen etwa 100 solcher gelabelten Samples, um f\u00fcr eine boole'sche Zufallsvariable ein _maximum likelihood estimate_ von $P(Y)$ zu finden, das einen Fehler im Rahmen weniger Prozent aufweist (s. z.B. Kapitel 5 der ersten Auflage von Tom Mitchells _Machine Learning_-Buch). Es erfordert aber in aller Regel wesentlich mehr Beispiele, um $P(X \\mid Y)$ zu sch\u00e4tzen:\n\nWelche und wieviele Parameter $\\theta$ m\u00fcssen wir sch\u00e4tzen, wenn, wie angenommen, $Y$ und die $X_i$ boole'sche Zufallsvariablen sind?\n\nWir m\u00fcssen diese Parameter sch\u00e4tzen:\n\\begin{equation}\n\\theta_{ij} \\equiv P(X=x_i \\mid Y=y_j)\n\\end{equation}\n\nWieviele sind das? Der Index $i$ l\u00e4uft \u00fcber $2^n$ m\u00f6gliche Kombinationen boole'scher Werte f\u00fcr unsere $n$ Variablen in $X$, $j$ nimmt zwei Werte an, insgesamt m\u00fcssen wir also ann\u00e4hernd $2^{n+1}$ Parameter sch\u00e4tzen (das stimmt nicht exakt, weil f\u00fcr jede Fixierung von $j$ die Summe der $\\theta_{ij}$ \u00fcber $i$ $1$ sein muss, wir k\u00f6nnen also f\u00fcr jede Auspr\u00e4gung von $Y$ genau einen Wert sparen, exakt sind es also $2^{n+1} - 2$ zu sch\u00e4tzende Parameter).\n\nWir ben\u00f6tigen also f\u00fcr jede denkbare Instanz aus dem _instance space_ von $X$ ziemlich genau zwei Parametersch\u00e4tzungen - und um diese erhalten zu k\u00f6nnen, m\u00fcssen wir diese Instanzen (mit beiden Auspr\u00e4gungen von $Y$!) vielfach sehen! Das ist, f\u00fcr die meisten praktischen Lern-Probleme, eine unrealistische Annahme. \n\nMachen wir uns die Dimension noch einmal klar: wenn sie auf diese Art z.B. das Kategorisieren von speziellen Texten, die aus nur h\u00f6chstens  30 signifikanten Worte bestehen, lernen wollen, und jede m\u00f6gliche Instanz eines Textes als 30-stelligen boole'schen Vektor modellieren w\u00fcrden, dann m\u00fc\u00dften wir mehr als 2 Milliarden Parameter sch\u00e4tzen, autsch!\n\nMit der (bedingten) Unabh\u00e4ngigkeitsannahme, die wir jetzt formalisieren, ben\u00f6tigt _Naive Bayes_ nur noch $2n$ Parameter! (nat\u00fcrlich ist die Annahme nicht immer korrekt - vorsichtig formuliert ;)\n\n**Definition of Conditional Indepencence**: Given three sets of random variables $X$, $Y$ and $Z$, we say $X$\nis conditionally independent of $Y$ given $Z$, if and only if the probability distribution governing $X$ is independent of the value of $Y$ given\n$Z$; that is\n\\begin{equation}\nP(X = x_i \\mid Y = y_j, Z = z_k) = P(X = x_i \\mid Z = z_k) \\;\\; \\text{f\u00fcr alle}\\;\\; i,j,k\n\\end{equation}\n\nAchtung, das ist noch nicht Naive Bayes! Die Definition deckt jede Form von bedingter Unabh\u00e4ngigkeit ab, denn sie bezieht sich auf Mengen $X$, $Y$, $Z$ von Zufallsvariablen.\n\nDer **Naive Bayes Algorithmus** ist ein Klassifikationsalgorithmus, der Bayes und eine Menge von bedingten Unabh\u00e4ngigskeitsannahmen verwendet. Wir wollen $P(Y \\mid X)$ lernen, wobei $X = (X_1,\\dots,X_n)$. Wir nehmen nun an, dass jedes $X_i$ bedingt unabh\u00e4ngig (conditionally independent) von **jedem** anderen $X_k$ ist, gegeben $Y$, ebenso von jeder Teilmenge der anderen $X_k$s gegeben $Y$.\n\nZur Illustration nehmen wir an, dass $X = (X_1,X_2)$ ist. Wir k\u00f6nnen $P(X \\mid Y)$ dann wie folgt bestimmen:\n\n\\begin{eqnarray}\nP(X \\mid Y) & = & P(X_1,X_2 \\mid Y) & (1) \\\\\n                   & = & P(X_1 \\mid X_2,Y) * P(X_2 \\mid Y) & (2) \\\\\n                   & = & P(X_1 \\mid Y) * P(X_2 \\mid Y) & (3) \n\\end{eqnarray}","ef8ef547":"Schauen wir uns zur Erinnerung den \u00dcbergang von (1) zu (2) nochmal genauer an (Annahme $P(Y)$ ist f\u00fcr kein $Y=y_j$ gleich $0$):\n\n\\begin{eqnarray}\nP(X1,X2 \\mid Y) & = & P(X1,X2,Y) \/ P(Y) & \\quad \\text{Definition bedingte Wahrscheinlichkeit} \\\\\n                           & = & P(X1 | X2, Y) * P(X2, Y) \/ P(Y) & \\quad \\text{ebenso} \\\\\n                           & = & P(X1 | X2, Y) * P(X2 \\mid Y) * P(Y) \/ P(Y) & \\quad \\text{ebenso} \\\\\n                           & = & P(X1 | X2, Y) * P(X2 \\mid Y) & \\quad \\text{K\u00fcrzen}\n\\end{eqnarray}                       \n\nDer \u00dcbergang von (2) zu (3) ist einfach unsere bedingte Unabh\u00e4ngigkeitsannahme.\n\nVerallgemeinern wir das auf $X$ mit $n$ Attributen und nehmen an, dass $X$ unsere oben formulierte Unabh\u00e4ngigkeitsannahme f\u00fcr Naive Bayes erf\u00fcllt:\n\n\\begin{equation}\nP(X_1,\\dots,X_n \\mid Y) = \\prod_{i=1}^{n} P(X_i \\mid Y)\n\\end{equation}\n\nJetzt m\u00fcssen wir f\u00fcr boole'sche $X$s und $Y$ nur noch $2n$ Parameter sch\u00e4tzen, um $P(X_i = x_{ik} \\mid Y = y_j)$ f\u00fcr die notwenidgen $i,j,k$ zu bestimmen.\n\n### Herleitung Naive Bayes\n\nSie $Y$ eine Zufallsvariable mit diskreten Werten und die Attribute $X_i$ seien diskret oder kontinuierlich. Wir suchen einen Klassifizierer, der f\u00fcr jede (neue) Instanz von $X$ eine Wahrscheinlichkeitsverteilung \u00fcber den Werten f\u00fcr $Y$ angibt:\n  \n   \\begin{equation}\n   P(Y=y_i \\mid X_1,\\dots,X_n) = \\frac{P(Y=y_k)P(X1,\\dots,X_n \\mid Y=y_k)}{\\sum_j P(Y=y_j)P(X1,\\dots,X_n \\mid Y=y_j)} \\quad\\quad (1)\n   \\end{equation}\n\nHier l\u00e4uft die Summe im Nenner \u00fcber alle m\u00f6glichen Werte f\u00fcr $Y$. Mit unsere Unabh\u00e4ngigkeitsannahme folgt:\n\n   \\begin{equation}\n   P(Y=y_i \\mid X_1,\\dots,X_n) = \\frac{P(Y=y_k) \\prod_i P(X_i \\mid Y=y_k)}{\\sum_j P(Y=y_j)\\prod_i P(X_i  \\mid Y=y_j)} \\quad\\quad (2)\n   \\end{equation}\n   \n   Mit Hilfe dieser Formel k\u00f6nnen wir nun, vorausgesetzt, wir haben Sch\u00e4tzungen f\u00fcr die verwendeten Verteilungen parat, zu jeder Instanz aus dem _instance space_ $X$ eine Wahrscheinlichkeitsverteilung \u00fcber $Y$ bestimmen - oder auch den Wert $y^*$ von $Y$ bestimmen, der am wahrscheinlichsten ist:\n   \n   \\begin{equation}\n    y^* \\leftarrow \\operatorname*{arg\\,max}_{y_k} P(Y=y_i \\mid X_1,\\dots,X_n) = \\frac{P(Y=y_k) \\prod_i P(X_i \\mid Y=y_k)}{\\sum_j P(Y=y_j)\\prod_i P(X_i  \\mid Y=y_j)} \n   \\end{equation}\n   \n   bzw. (der Nenner ist f\u00fcr alle $y_k$ gleich):\n           \n   \\begin{equation}\n    y^* \\leftarrow \\operatorname*{arg\\,max}_{y_k} P(Y=y_i \\mid X_1,\\dots,X_n) = P(Y=y_k) \\prod_i P(X_i \\mid Y=y_k) \\quad\\quad (3)\n   \\end{equation}\n   \n   Anmerkung: Mitchell verwendet hier die Notation $Y \\leftarrow$, was ich nicht sch\u00f6n finde, denn wir finden hier einen konkreten Wert **aus** $Y$ (und nicht f\u00fcr $Y$). Das $Y$ sieht 'abstrakter' aus, wie in der abstrakten Funktionnotation $f: X \\rightarrow Y$, aber wir sind hier gar nicht abstrakt, sondern geben die konkrete Definition der Funktion an, in Mathe also das \"definiert durch: $y = ... $\". Vermutlich wollte uns Mitchell daran erinnern, dass wir hier die genannte Funktion $f$ approximieren (s. ganz oben in diesem Unterkapitel). Oder er hat sich etwas Anderes dabei gedacht, das ich gerade nicht durchschaue ;)\n   \n### Konkretisierung der Parametersch\u00e4tzung f\u00fcr diskrete $X$\n\nWir haben $n$ Eingabeattribute (unabh\u00e4ngige Attribute, Variablen, ...) $X_i$. Jedes $X_i$ kann einen von (endlich vielen) $J$ Werten annehmen (die $X$ sind also diskret, \u00fcbrigens k\u00f6nnten wir auch jedem $X_i$ ein eigenes $J$, n\u00e4mlich $J_i$ verpassen, das kann nat\u00fcrlich praktisch erforderlich sein, aber das macht die Formeln unten komplizierter und \u00e4ndert nichts Substantielles, deshalb \u00fcberlassen wir das als Finger\u00fcbung dem geneigten Leser).\n\nUnsere _learning task_ ist es, zwei Parametermengen zu sch\u00e4tzen. Zun\u00e4chst tun wir das f\u00fcr die Likelihoods:\n\n\\begin{equation}\n\\theta_{ijk} = P(X_i = x_{ij} \\mid Y = y_k) \\quad\\text{f\u00fcr alle} \\; X_i \\; \\text{f\u00fcr alle ihre m\u00f6glichen Werte} \\; x_{ij} \\; \\text{f\u00fcr jeden m\u00f6glichen Wert} \\; y_k \\;\\text{von} \\; Y\n\\end{equation}\n\nEs gibt also $nJK$ solche Parameter, allerdings ist nur die Sch\u00e4tzung von $n(J-1)K$ dieser Parameter erforderlich, denn ein jeweils letzter kann f\u00fcr jedes Paar $i,k$ aus den anderen Parametern berechnet werden, denn es muss gelten $1 = \\sum_j \\theta_{ijk}$.\n\nZudem m\u00fcssen wir noch die Prior probabilities f\u00fcr die Auspr\u00e4gungen von $Y$ sch\u00e4tzen:\n\\begin{equation}\n\\pi_k \\equiv P(Y = y_k)\n\\end{equation}\n\nEs gibt $K$ dieser Parameter, $(K-1)$ davon m\u00fcssen wir sch\u00e4tzen, den letzten k\u00f6nnen wir, wie gehabt, berechnen (1-Summe der anderen Wahrscheinlichkeiten).\n\nWir k\u00f6nnen die Parameter auf zwei Arten sch\u00e4tzen (N\u00e4heres dazu s. unten):\n  * Maximum Likelihood \u00fcber die relativen H\u00e4ufigkeiten (relative frequencies) in den Daten\n  * Maximum a posteriori, in dem wir die beobachteten Daten mit einer A-priori Verteilung \u00fcber den Werten dieser Parameter anreichern (das haben wir oben schon so gemacht, um das $0$-Problem zu vermeiden).\n\nMaximum-Likelihood-Sch\u00e4tzungen f\u00fcr eine Trainingsmenge $D$ erhalten wir wie folgt:\n\n\\begin{equation}\n\\hat{\\theta}_{ijk} = \\hat{P}(X_i = x_{ij}  \\mid Y = y_k ) = \\frac{\\#D\\{X_i = x_{ij} \\wedge Y=y_k\\}}{\\#D\\{Y = y_k\\}}\n\\end{equation}\n\nHier ist $\\#D\\{x\\}$ ein Operator, der die Anzahl an Elementen in der Menge $D$ zur\u00fcckgibt, die die Bedingung $x$ erf\u00fcllen. \n\nEin Risiko dieser Art zu sch\u00e4tzen ist, dass wir ab und an $0$ sch\u00e4tzen (falls wir die Bedingung im Z\u00e4hler nicht erf\u00fcllen k\u00f6nnen, weil wir noch nicht genug Daten gesehen haben - das auch der Z\u00e4hler mal $0$ sein kann, schlie\u00dfen wir per Annahme einfach mal aus (sonst k\u00f6nnte man das auch per Konvention l\u00f6sen, in dem man $0$ als Sch\u00e4tzung annimmt...etwas unsch\u00f6n, soviel Daten sollten wir schon haben, um den Nenner $>0$ zu haben!).\n\nUm das Problem zu vermeiden, ist es \u00fcblich, ein \"gegl\u00e4ttetes\" Sch\u00e4tzen durchzuf\u00fchren, dass eine gewisse Anzahl (unten $lJ$) \"halluzinierter\" Beispiele hinzuf\u00fcgt und annimmt, dass diese gleichm\u00e4\u00dfig \u00fcber alle m\u00f6glichen Werte f\u00fcr $X_i$ verteilt aufgetreten sind ($l$ pro Auspr\u00e4gung von $X_i$):\n\n\\begin{equation}\n\\hat{\\theta}_{ijk} = \\hat{P}(X_i = x_{ij}  \\mid Y = y_k) = \\frac{\\#D\\{X_i = x_{ij} \\wedge Y=y_k\\} +l}{\\#D\\{Y = y_k\\} +lJ}\n\\end{equation}\n\nDies entspricht einem MAP-estimate der Parameter, wenn wir eine (symmetrische) prior [Dirichlet distribution](https:\/\/en.wikipedia.org\/wiki\/Dirichlet_distribution) \u00fcber den $\\theta_{ijk}$-Parametern annehmen, mit gleichwertigen Parametern (deshalb symmetrisch).\n\nF\u00fcr $l=1$ (eine h\u00e4ufige Wahl) wird dies _Laplace smoothing_ genannt.\n\nDie MLE (maximum likelihood estimates) f\u00fcr $\\pi_k$ sind:\n\\begin{equation}\n\\hat{\\pi}_k = \\hat{P}(Y = y_k) = \\frac{\\#D\\{Y=y_k\\}}{\\mathopen|D\\mathclose|}\n\\end{equation}\n\n$\\mathopen|D\\mathclose|$ ist nat\u00fcrlich die M\u00e4chtigkeit des Sets $D$, im Endlichen also die Anzahl seiner Elemente.\n\nAuch k\u00f6nnten wir wieder eine MAP-Sch\u00e4tzung durchf\u00fchren (wieder mit angenommenem symmetrischem Dirichlet prior):\n\\begin{equation}\n\\hat{\\pi}_k = \\hat{P}(Y = y_k) = \\frac{\\#D\\{Y=y_k\\} + l}{\\mathopen|D\\mathclose| + lK}\n\\end{equation}\n\nHier ist $K$ die Anzahl m\u00f6glicher Werte von $Y$. Machen wir uns nochmals klar, das $l$ hier die St\u00e4rke der A-Priori-Annahme (prior assumption) relativ zu den beobachteten Daten bestimmt (f\u00fcr wachsende Trainingsdatenmengen wird der Einfluss des Prior zunehmend verschwinden).\n\nF\u00fcr kontinuierliche Inputs l\u00e4\u00dft sich eine analoge Ableitung angeben (die durchaus interessant ist). Mit logistischer Regession kann man direkt $P(Y|X)$ sch\u00e4tzen. Man kann einen interessanten Zusammenhang zwischen Naive Bayes und der logistischen Regression zeigen. Erl\u00e4uterungen zu diesen Aspekten und weitere Details findet man im genannten [Kapitel 3 von Tom Mitchell](http:\/\/www.cs.cmu.edu\/~tom\/mlbook\/NBayesLogReg.pdf). Erg\u00e4nzende, detaillierte Ausf\u00fchrungen zur MLE- bzw. MAP-Parametersch\u00e4tzung findet man in [Kapitel 2](http:\/\/www.cs.cmu.edu\/~tom\/mlbook\/Joint_MLE_MAP.pdf). Beides zusammen kann ein guter Einstieg sein, Klassifikation (und weitere ML-Methoden) aus Bayes'scher Perspektive einordnen und verstehen zu lernen: wann macht man was und was w\u00e4re eigentlich optimal?\n\nVergessen Sie nicht, die Aufgabe im Text zu l\u00f6sen (in der Mitte)! Es kann auch nicht schaden, die beiden Beispiele nachzurechnen.\n  \nPS: Wenn Sie Fehler finden oder Fragen haben, dann senden Sie mir bitte eine Email, danke!","740eec94":"## Ein Problem mit Bayes\n\nSehen wir uns unsere Daten aus dem Entscheidungsbaumkapitel nochmal an - hier mit neuem Kontext: ein Einbrecher beobachtet Sie. Er hat Wege gefunden, gewisse Informationen \u00fcber ihre Lebensumst\u00e4nde zu sammeln (vermutlich ist er selbst Informatik-Student ;) und m\u00f6chte gerne lernen vorherzusagen, wie Sie sich verhalten werden, wenn gewisse Umst\u00e4nde eintreten. Nat\u00fcrlich, um in Ruhe bei Ihnen einbrechen zu k\u00f6nnen, wenn Sie nicht daheim sind. Hier sind die Daten, die er gesammelt hat:\n\n| Deadline? | Party heute abend?  | erholungsbed\u00fcrftig?  | Aktivit\u00e4t |\n|:---------:|:-----------:|:-----------:|:-----------:|\n| Dringend  | Ja  | Ja | Party |\n| Dringend | Nein | Ja | Lernen |\n| Bald | Ja | Ja | Party |\n| Nein | Ja | Nein | Party |\n| Nein | Nein | Ja | Kneipe |\n| Nein | Ja | Nein | Party |\n| Bald | Nein | Nein | Lernen |\n| Bald | Nein | Ja | TV |\n| Bald | Ja | Ja | Party |\n| Dringend | Nein | Nein | Lernen |\n\nHier sind die Attribute \"Deadline?\", \"Party heute abend?\" (kurz: Party?) und \"erholungsbed\u00fcrftig?\" (kurz: faul?) die Attribute, die er vorab erheben kann. \"Aktivit\u00e4t\" ist das Attribut, das er vorhersagen m\u00f6chte.\n\nAus den Daten k\u00f6nnen wir einige Likelihoods sch\u00e4tzen, z.B. w\u00e4re $P(Dringend=Bald,Party=Ja,faul=Ja \\mid Party) = 2\/5$, f\u00fcr alle anderen Aktivit\u00e4ten, z.B. $Lernen$ w\u00e4re die Likelihood $P(Dringend=Bald,Party=Ja,faul=Ja \\mid Lernen) = 0$.\n\nWenn wir jetzt die Beobachtung $E = (Dringend=Bald,Party=Ja,faul=Ja)$ machen w\u00fcrden, k\u00f6nnten wir die Maximum-a-posteriori-Hypothese (MAP-Hypothese), $h^*$ bestimmen. Unsere Hypothesen sind $h_P = \\text{Aktivit\u00e4t Party}$, $h_L = \\text{Aktivit\u00e4t  Lernen}$, $h_K = \\text{Aktivit\u00e4t Kneipe}$, $h_T = \\text{Aktivit\u00e4t TV}$ mit den Priors $P(h_P) = 1\/2$, $P(h_L) = 3\/10$, $P(h_K) = 1\/10$, $P(h_T) = 1\/10$.\n\n\\begin{equation}\nh^* = \\operatorname*{arg\\,max}_h P(E \\mid h) * P(h)\n\\end{equation}\n\nBestimmen wir die rechts stehenden Werte f\u00fcr unsere Aktivit\u00e4ten:\n\n$P(E \\mid h_P) * P(h_P) = 2\/5 * 1\/2 = 1\/5$, $P(E \\mid h_L) * P(h_L) = 0 * 3\/10 = 0$, $P(E \\mid h_K) * P(h_K) = 0 * 1\/10 = 0$, $P(E \\mid h_T) * P(h_T) = 0 * 1\/10 = 0$\n\nDie MAP-Klasse $h^*$ wird also durch die Hypothese $h_P$ repr\u00e4sentiert, wir sagen also die Aktivit\u00e4t $Party$ voraus.\n\nOffensichtlich werden wir auf analoge Art **f\u00fcr alle bisher gesehenen Samples die** - gegeben die bisherigen Daten - **korrekte Klasse vorhersagen**. Aber was geschieht, wenn wir eine bisher noch nicht vorgekommene Kombination sehen, z.B. \n\n$E = (Deadline=Dringend,Party=Ja,faul=Nein)$?\n\nJetzt sind die zugeh\u00f6rigen Likelihoods f\u00fcr alle Klassen $0$ (dann wird auch $P(E)$ 0, also Vorsicht!), alle Z\u00e4hler der A-Posteriori-Berechnung werden demnach $0$ und wir sagen irgendeine der Klassen vollkommen zuf\u00e4llig voraus (alle erf\u00fcllen die Maximum-Bedingung!).\n\nWir k\u00f6nnten, analog zu Vorgehen oben beim Naive Bayes auf Texten, f\u00fcr jede denkbare Kombination der bereits gesehenen Attributauspr\u00e4gungen zun\u00e4chst einmal annehmen, sie w\u00e4ren in jeder Klasse schon vorhanden gewesen. Wir haben 3 Auspr\u00e4gungen beim Attribut $Deadline$, 2 bei $Party$ und 2 bei $Faul$, insgesamt gibt es also $3*2*2 = 12$ m\u00f6gliche Kombinationen, unser Sample-Space $\\cal{S}$ hat also 12 Elemente. Wenn wir so tun, als w\u00e4re jedes dieser Elemente in jeder Klasse \"zu Beginn\" schon einmal vorhanden, dann liesse sich auch $P(E)$ f\u00fcr jedes $E$ aus $\\cal{S}$ unser wieder bestimmen. Bestimmen wir einmal die Likelihood, den Prior und den Z\u00e4hler des A-Posteriori-Berechnung f\u00fcr unser $E = (Dringend=Dringend,Party=Ja,faul=Nein)\\;$ f\u00fcr alle Klassen:\n   * $h_P: P(E \\mid h_P) = 1\/(5+12)$ mit $P(h_P) = (5+12)\/(10+48) = 17\/58$ f\u00fchrt zu $1\/17 * 17\/58 = 1\/58$\n   * $h_L: P(E \\mid h_L) = 1\/(3+12)$ mit $P(h_P) = (3+12)\/(10+48) = 15\/58$ f\u00fchrt zu $1\/15 * 15\/58 = 1\/58$\n   * $h_K: P(E \\mid h_K) = 1\/(1+12)$ mit $P(h_K) = (1+12)\/(10+48) = 13\/58$ f\u00fchrt zu $1\/13 * 13\/58 = 1\/58$\n   * $h_T: P(E \\mid h_T) = 1\/(1+12)$ mit $P(h_T) = (1+12)\/(10+48) = 13\/58$ f\u00fchrt zu $1\/13 * 13\/58 =  1\/58$\n   \nJetzt k\u00f6nnen wir auch mit Bayes eine Wahrscheinlichkeit bestimmen: $0.25$ f\u00fcr jede der Aktivit\u00e4ten, Damit sind wir f\u00fcr eine Wahl der MAP-Hypothese nicht weitergekommen. Wenn man ein bisschen dr\u00fcber nachdenkt, macht das ja auch Sinn: wenn wir unterstellen, dass die Kombination aller unabh\u00e4ngigen Attributauspr\u00e4gungen bedeutsam f\u00fcr die Klasse des Samples ist und wir zu dem Sample nichts \"differenzierendes\" wissen, dann m\u00fcssen wir halt w\u00fcrfeln.\n \n Wenn wir sehr viele Daten sehen w\u00fcrden, idealerweise mit reichlich Samples zu jeder m\u00f6glichen Kombination, dann k\u00f6nnten wir nat\u00fcrlich \"gute\" Likelihoods und Priors sch\u00e4tzen und dann direkt Bayes zur MAP-Hypothesenbestimmung verwenden - aber eigentlich nicht im Sinne von: wir generalisieren, in dem wir ein Modell bilden, sondern im Sinne von: wir schlagen nach, wie es in der Vergangenheit war. Das ist ja nicht per se schlecht, aber im Regelfall haben wir soviele Daten gar nicht zur Verf\u00fcgung! \n \n Aber, klar, wir k\u00f6nnten mit dem \"Alles-ist-\u00dcberall-drin\"-Initialisieren sicher stellen, dass wir immer rechnen k\u00f6nnen (solange, bis wir eine neue Auspr\u00e4gung sehen w\u00fcrden, z.B. $sehr\\;dringend$, dann w\u00fcrden wir kurz nerv\u00f6s und m\u00fc\u00dften unser Modell aufblasen) und auch jederzeit auf neue Daten reagieren k\u00f6nnen, in dem wir die Likelihoods und die Priors anpassen. Das macht auch dann Sinn, wenn wir sowieso erwarten, dass jede Kombination mindestens einige Male auftreten wird (nicht in jeder Klasse, sondern \u00fcberhaupt). Dann haben wir halt ein richtig **fettes** Problem (mit kombinatorisch vielen Daten relativ zur Attributanzahl). Oft sind unsere Daten aber eher _sparse_, wir sehen im Laufe der Zeit in der Regel nur einen Bruchteil der naiv denkbaren Kombinationsm\u00f6glichkeiten. Dann macht es nur so halb Sinn, \"Bayes'sch\" zu denken (geht aber oft immer noch, in dem man die noch nicht gesehenen Daten nur in den Berechnungen ber\u00fccksichtigt und z.B. die Parametrisierung, wie etwa $\\cal{S}$ sch\u00e4tzt und im Bedarfsfall korrigiert, egal).\n \n Wenn wir eine maximale Unabh\u00e4ngigsannahme treffen, wie bei **Naive-Bayes**, dann wird es leicht zu generalisieren - aber eben nicht unbedingt richtig. Probieren wir das, schauen wir dazu nochmals auf die Daten:\n \n| Deadline? | Party heute abend?  | erholungsbed\u00fcrftig?  | Aktivit\u00e4t |\n|:---------:|:-----------:|:-----------:|:-----------:|\n| Dringend  | Ja  | Ja | Party |\n| Dringend | Nein | Ja | Lernen |\n| Bald | Ja | Ja | Party |\n| Nein | Ja | Nein | Party |\n| Nein | Nein | Ja | Kneipe |\n| Nein | Ja | Nein | Party |\n| Bald | Nein | Nein | Lernen |\n| Bald | Nein | Ja | TV |\n| Bald | Ja | Ja | Party |\n| Dringend | Nein | Nein | Lernen |\n\nUnsere Daten (zur Erinnerung)\n\nPriors: $P(h_P) = 1\/2$, $P(h_L) = 3\/10$, $P(h_K) = 1\/10$, $P(h_T) = 1\/10$.\n\nJetzt unterstellen wir, dass jede Auspr\u00e4gung eines Attributs im \"Attributtopf\" je Klasse bereits enthalten ist - und welche durch die beobachteten Samples hinzu kommen. Damit k\u00f6nnen wir die Likelihoods bestimmen:\n   * $P(Deadline=Dringend \\mid h_P) = (1+1)\/(5+3) = 2\/8\\;$, $P(Deadline=Dringend \\mid h_L) = (2+1)\/(3+3) = 3\/6 = 1\/2$, ...\n   * $P(Deadline=Bald \\mid h_P) = (2+1)\/(5+3) = 3\/8\\;$, $P(Deadline=Bald \\mid h_L) = (1+1)\/(3+3) = 2\/6 = 2\/6$, ...\n   * $P(Deadline=Nein \\mid h_P) = (2+1)\/(5+3) = 3\/8\\;$, $P(Deadline=Nein \\mid h_L) = (0+1)\/(3+3) = 1\/6 = 1\/6$, ...\n   * $P(Party=Ja \\mid h_P) = (5+1)\/(5+2) = 6\/7$, ...\n   * $P(faul=Nein \\mid h_P) = (2+1)\/(5+2) = 3\/7$, ...\n \nDas ist schon ganz sch\u00f6n aufw\u00e4ndig, wir brauchen jetzt $(3 + 2 + 2) * 4$ Likelihoods - das ist aber vergleichsweise harmlos, denn es explodiert ja nicht kombinatorisch! Weitere Attributauspr\u00e4gungen oder Attribute gehen zun\u00e4chst nur additiv ein und werden dann einmal multipliziert mit der Klassenzahl. \n \n Wenn wir jetzt eine (unklassifizierte) Beobachtung $E$ klassifizieren wollen, k\u00f6nnen wir die MAP-Hypothese mittels **(Naive) Bayes** finden, in dem wir die Unabh\u00e4ngigkeitsanahme nutzen: \n\n\\begin{equation}\nP(E = (e_{Dringend},e_{Party},e_{faul}) \\mid h_x) = P(e_{Dringend}\\mid h_x) * P(e_{Party} \\mid h_x) * P(e_{faul} \\mid h_x).\n\\end{equation}\n\nSei nun $E = (Deadline=Dringend,Party=Ja,faul=Nein)\\;$, welches ist dann die MAP-Hypothese $h^*$?\n* $h_P: 2\/8 * 6\/7 * 3\/7 * 1\/2 = 9 \/ 392 \\sim 0.046$\n* $h_L: 1\/2 * 1\/5 * 3\/5 * 3\/10 = 9 \/ 500  = 0.018$\n* $h_K: 1\/4 * 1\/3 * 1\/3 * 1\/10 = 1 \/ 360 \\sim 0.00278$\n* $h_T:  1\/4 * 1\/3 * 1\/3 * 1\/10 = 1 \/ 360 \\sim 0.00278$\n\nDie MAP-Hypothese ist $h_P$, wir sagen also den Gang zur Party voraus. Wir k\u00f6nnten nun alle Vorhersagen bestimmen, auch f\u00fcr die Beispiele, die wir bereits gesehen haben (und damit ein Gef\u00fchl f\u00fcr die Qualit\u00e4t unseres Modells erhalten, s. unten - wir sollten aber im Kopf behalten, dass wir bisher wahnsinnig wenige Daten gesehen haben, und deshalb der Einfluss unserer \"Korrekturen\" der Sch\u00e4tzungen der Likelihoods ziemlich gro\u00df sein kann). Dazu noch eine Anmerkung:\n\nWir haben gerade die Likelihoods \"einfach so\" korrigiert durch die Annahme, alles sei in jeder Klasse schon einmal vorgekommen - ohne, dass wir wirklich weitere Samples angegeben h\u00e4tte, die diese Auspr\u00e4gungen in den Klassen auch enthalten w\u00fcrden, deshalb hat das auch keinen Eingang in unsere Bestimmung des Priors gefunden. Ein wenig nat\u00fcrlicher w\u00e4re es m\u00f6glicherweise, wenn wir f\u00fcr das Sch\u00e4tzen der Likelihoods und Priors wieder annehmen w\u00fcrden, dass jede m\u00f6gliche der 48 Kombinationen in jeder Klasse schon einmal aufgetaucht w\u00e4re. Dann k\u00f6nnten wir, analog zum Modell bei der Textklassifikation, durch Z\u00e4hlen sch\u00e4tzen. Aber das ersparen wir uns hier, ist auch nicht sooo super wichtig, weil die Annahme \"In allen ist alles schon drin\" nat\u00fcrlich auch nicht \"objektiv korrekt\" ist - und die Fehler beider Vorgehensweisen durch das Beobachten vieler bereits klassifizierter Samples sowieso ausgemerzt werden (und ohne die Annahme, ausreichend viele Samples zu sehen, ist schwer zu sagen, welches Vorgehen gerechtfertigter zu sein scheint).\n\nRechnen wir einmal nach, was wir gerade \"zu Fu\u00df\" bestimmt haben:","309b9796":"Ohne die Korrektur w\u00fcrde \u00fcbrigens das Sample aus der Klasse $TV$ korrekt klassifiziert (klar, das liegt dann an den $0$-Werten, die in den anderen Klassen auftreten). Nun gut, wir wissen jetzt also, dass wir unsere (klassifizierten) Trainingssamples nicht mit maximaler Accuracy klassifizieren. \n\nSchauen wir mal, ob wir f\u00fcr das Sample $E = (Deadline=Dringend,Party=Ja,faul=Nein)\\;$ die gleichen Werte bestimmt haben, wie oben.\n   * Berechnet: $[0.04591836734693877, 0.018, 0.002777777777777778, 0.002777777777777778]$\n   * Oben per Hand bestimmt: $[\\sim 0.046, 0.018, \\sim 0.00278, \\sim 0.00278]$\n\nScheint geklappt zu haben - oder aber den selben systematischen Fehler zu enthalten ;)\n\nJetzt haben wir einen Klassifizierer gebaut, der jederzeit auch aus weiteren klassifizierten Beispielen lernen k\u00f6nnte (in dem wir die Likelihoods und Priors entsprechen korrigieren, am besten inkrementel). Schon nett.","cfd389d4":"### Weitere Unabh\u00e4ngigkeitsstrukturen\n \nWas wir auch versuchen k\u00f6nnten, w\u00e4re, ein Bayes'sches Netz zu finden, das m\u00f6glichst kompakt und mit m\u00f6glichst kleinem Fehler die Wahrscheinlichkeitsstruktur unserer bisher gesehenen Daten wiedergibt und das wir dann anwenden k\u00f6nnen, um noch nicht gesehene Daten zu klassifizieren. Nicht trivial ist allerdings, dieses Netz dann an neu gesehene, klassifizierte Daten anzupassen: reicht eine Korrektur der (Conditional) Probability Tables oder muss man die Netzstruktur und die Tables \u00e4ndern? Das ist aber insgesamt ein spannendes Feld - und nat\u00fcrlich auch praktisch relevant, wenn man Bayes und Bayes'sche Netze zum Herleiten von Wahrscheinlichkeiten verwenden m\u00f6chte, denn dann muss man aus den Daten ein \"m\u00f6glichst plausibles\" Modell ableiten, das auch generalisieren kann (das geht nicht ohne einen Bias, also eine gewisse Voreingenommenheit: wir glauben, dass dieses und jenes Vorgehen Sinn macht, deshalb nehmen wir es) - solange dies n\u00f6tig ist, denn auf (sehr) lange Sicht gesehen lernen wir, wenn wir es richtig machen, die wahren Joint Probability Distribution und k\u00f6nnen dann jede Frage durch Rechnen beantworten (dann wird es wieder interessant, wenn die Verteilungen sich \u00fcber die Zeit \u00e4ndern...).","60ed7401":"Jetzt bestimmen wir die Likelihoods:","75e30cca":"## Einf\u00fchrendes Beispiel\n\n**Beispiel 1**: Wir wollen Nachrichtentexte klassifizieren. Hier sind unsere Daten:\n\n| | docID | W\u00f6rter im Dokument | in Klasse \"Japan\"\n| ------------- |:-------------:| -----:|-----:|\n| Trainingsmenge | 1 | Kyoto Osaka Taiwan | Yes\n| | 2  | Japan Kyoto  | Yes\n| | 3  | Taipei Taiwan  | No\n| | 4  | Macao Taiwan Shanghai  | No\n| | 5  | London  | No\n| Testmenge  | 6  | Taiwan Taiwan Kyoto  | ?\n\nUnsere Beispieldaten, die aus vorverarbeiteten Kurznachrichten aus dem asiatischen Raum entstanden sein k\u00f6nnten.\n\nAus den kurzen Newstexten wurden (vermeintlich) \u00fcberfl\u00fcssige Worte bereits entfernt (z.B. 'und'). Die Dokumente mit den _docIDs_ 1 bis 5 sind bereits klassifiziert, das sind also unsere \"Gold-Standard-Daten\", die wir zum Training unseres Klassifikators verwenden k\u00f6nnen. Das Dokument 6 wollen wir klassifizieren.\n\nUm das tun zu k\u00f6nnen, lernen wir jetzt das etwas schr\u00e4ge **Handtaschen-Modell der Texterzeugung** kennen:\n\nIhre Freundin sammelt W\u00f6rter auf Zetteln in ihrer Handtasche (bag). Auf jedem Zettel steht genau ein Wort. Die Handtasche hat zwei F\u00e4cher, das vordere, in dem sie W\u00f6rter f\u00fcr $Liebesbriefe$ sammelt und das hintere, in dem sie W\u00f6rter f\u00fcr ein $Trennungsschreiben$ sammelt. Wann immer sie ein gut passendes Wort f\u00fcr eines der beiden F\u00e4cher findet, schreibt sie es auf einen Zettel und tut es ins entsprechende Fach. Manche W\u00f6rter finden sich mehrfach in den F\u00e4chern, diese scheinen besonders wichtig zu sein, z.B. gibt es schon 57 Zettel mit dem Wort _liebe_ im Fach $Liebesbrief$ und 23 Zettel _liebe_ im  Fach $Trennungsschreiben$ (gefunden in S\u00e4tzen wie: ich liebe dich nicht mehr...). Sie hat schon eine recht gro\u00dfe Menge W\u00f6rter gesammelt.\n\nHeute morgen vorm Fr\u00fchst\u00fcck haben sie ihre Freundin leider etwas ge\u00e4rgert ('wer hat schon wieder die Zahnpastatube aufgelassen???').  daraufhin \"schreibt\" ihre Freundin ihnen einen Brief! Dazu greift sie 10mal in eines der F\u00e4cher ihrer Handtasche (immer in das gleiche) und zieht jeweils einen einzelnen Zettel heraus. Sie schreibt das Wort auf dem Zettel auf einen neuen Zettel und legt diesen dann in einen kleinen schwarzen Stoffbeutel.  Den gezogenen Zettel legt sie zur\u00fcck ins Fach. \n\nAls sie fertig ist, kommt sie in die K\u00fcche und gibt ihnen den \"Brief\". Sie sch\u00fctten ihn aus, drehen alle W\u00f6rter nach oben, und sehen dann einen ungeordneten Haufen von W\u00f6rtern vor sich, dessen Bedeutung sich Ihnen aber dennoch erschlie\u00dft, weil sie \u00fcber die Verteilung der W\u00f6rter in den F\u00e4chern Bescheid wissen (sie sind n\u00e4mlich ganz sch\u00f6n neugierig), zudem (etwas naiv) annehmen, beide Briefsorten seien a priori gleich wahrscheinlich, und deshalb das wahrscheinlichere Fach mit dem Satz von Bayes und etwas Rechnerei bestimmen k\u00f6nnen! Dann k\u00f6nnen sie die Frage beantworten, ob sie das Haus verlassen m\u00fcssen oder ob ihre Freundin sie doch noch lieb hat...\n\nDieses _Handtaschen-Modell der Texterzeugung_ hat den Vorteil, dass wir es ziemlich leicht formalisieren k\u00f6nnen. Wir nehmen allerdings noch eine Annahme hinzu, nennen wir sie die Vollst\u00e4ndigkeitsannahme: ihre Freundin legt jedes Wort, das sie zum ersten Mal findet, zun\u00e4chst in beide F\u00e4cher ('ah, sch\u00f6nes Wort, man wei\u00df ja nie, wof\u00fcr man es noch mal gebrauchen kann'), und dann ein weiteres Mal in das Fach, in das es im Moment am besten zu passen scheint (als sie das erste Mal das Wort _liebe_ fand, war sie romantisch gestimmt, und hat es deshalb zun\u00e4chst in beide F\u00e4cher getan und dann ein zweites Mal ins Fach $Liebesbriefe$).\n\nJetzt wollen wir das Modell auf unsere Daten von oben anwenden, dazu gehen wir wie folgt vor:\n\nWir bestimmen das Vokabular, das alle W\u00f6rter genau einmal enth\u00e4lt, die in einem der Trainingsdokumente vorkommen.\n\nVokabular $V = \\{$ Kyoto, Osaka, Taiwan, Japan, Taipei, Macao, Shanghai, London $\\}$\n\nDas Vokabular enth\u00e4lt acht W\u00f6rter: $|V| = 8$.\n\nWir bauen nun f\u00fcr jede Klasse, also die Klassen $Japan$ und $\\overline{Japan}$ (Nicht-Japan), eine **Bag-of-Words** (also ein Fach in unserer Handtasche), in der alle W\u00f6rter vorkommen (gegebenenfalls auch mehrfach), die in den Dokumenten der jeweiligen Klasse auftauchen **plus** alle W\u00f6rter aus dem Vokabular. Dieses Hinzuf\u00fcgen der W\u00f6rter aus dem Vokabular beugt einem Problem vor, das auftauchen w\u00fcrde, wenn im Testdokument ein Wort verwendet wird, das aus dem Vokabular stammt, aber nicht in jeder Klasse auftaucht (im Handtaschenmodell vermeiden wir genau das automatisch, in dem wir die Vollst\u00e4ndigkeitsannahme hinzugenommen haben: jedes Wort taucht mindestens einmal in beiden F\u00e4chern auf, das ist Laplace-Smoothing mit $k=1$, s. unten die Ausf\u00fchrungen von Tom Mitchell bzw. die NaiveBayes-Pr\u00e4sentation von Klein\/Abdeel):\n\n   * $D_{Japan} = \\{$ Kyoto, Osaka, Taiwan,  Japan, Kyoto  $\\}$ $+$ $\\{$ Kyoto, Osaka, Taiwan, Japan, Taipei, Macao, Shanghai, London $\\}$\n   * $D_{\\overline{Japan}} = \\{$ Taipei, Taiwan,  Macao, Taiwan, Shanghai, London $\\}$ $+$ $\\{$ Kyoto, Osaka, Taiwan, Japan, Taipei, Macao, Shanghai, London $\\}$\n\nDie geschweiften Klammern stehen hier nicht f\u00fcr Mengen, sondern f\u00fcr sogenannte _Bags_ (Multi-Mengen), letztlich Sequenzen oder Listen ohne Ordnung. Das $+$ f\u00fcgt die Bags zusammen.\n\nWir nehmen nun an, s. unser Modell oben, dass jemand, der eine Nachricht zum Thema Japan schreibt, sich in der zugeh\u00f6rigen Bag-of-Words bedient und wir das **Erzeugen (Generierung) eines Dokumentes als Prozess** beschreiben k\u00f6nnen, bei dem er zuerst w\u00fcrfelt, wie lang das Dokument werden soll, nennen wir das Ergebnis $n$, und dann die  $n$ W\u00f6rter durch $n$ _voneinander unabh\u00e4ngige_ Zufallswahlen bestimmt, bei denen er ein Wort entsprechend der Verteilung \u00fcber den Worten in der zugeh\u00f6rigen Menge $D$ w\u00e4hlt. Durch die Reihenfolge der Wahlen erzeugt er an sich eine geordnete Sequenz, aber das ignorieren wir hier, wir interessieren uns nur f\u00fcr die Bag-of-Words, die entsteht, die Reihenfolge der Worte interessiert uns nicht.\n\nDie durch die $D$s bestimmten (auf die Klasse bedingten) Verteilungen sch\u00e4tzen wir empirisch wie folgt:\n   * $P(Kyoto \\mid Japan) =$ Anzahl Vorkommen 'Kyoto' in $D_{Japan}$ \/ Anzahl W\u00f6rter in $D_{Japan}$ $= 3\/13$\n   * $P(Osaka \\mid Japan) = 2\/13$\n   * $P(Taiwan \\mid Japan) = 2\/13$\n   * $P(Japan \\mid Japan) = 2\/13$\n   * $P(Taipei \\mid Japan) = 1\/13$\n   * $P(Macao \\mid Japan) = 1\/13$\n   * $P(Shanghai \\mid Japan) = 1\/13$\n   * $P(London \\mid Japan) = 1\/13$\n   * $P(Kyoto \\mid \\overline{Japan}) =$ Anzahl Vorkommen 'Kyoto' in $D_{\\overline{Japan}}$ \/ Anzahl W\u00f6rter in $D_{\\overline{Japan}}$ $= 1\/14$\n   * $P(Osaka \\mid \\overline{Japan}) = 1\/14$\n   * $P(Taiwan \\mid \\overline{Japan}) = 3\/14$\n   * $P(Japan \\mid \\overline{Japan}) = 2\/14$\n   * $P(Taipei \\mid \\overline{Japan}) = 1\/14$\n   * $P(Macao \\mid \\overline{Japan}) = 1\/14$\n   * $P(Shanghai \\mid \\overline{Japan}) = 1\/14$\n   * $P(London \\mid \\overline{Japan}) = 1\/14$\n\nEs gibt zwei Hypothesen f\u00fcr das Testdokument;\n   * $h_1 =$ \"Dokument ist aus der Klasse $Japan$\"\n   * $h_2 =$ \"Dokument ist _nicht_ aus der Klasse Japan, also aus der Klasse $\\overline{Japan}$\"\n   \nDer Prior f\u00fcr die Hypothesen ergibt sich \u00fcber den Anteil der jeweiligen Klasse an den Trainingsdokumentklassen, also\n   * $P(h_1) = 2\/5$\n   * $P(h_2) = 3\/5$\n   \n Hinweis: wenn wir ausreichend viele Trainingsdokumente h\u00e4tten, k\u00f6nnten wir auch noch die L\u00e4nge des Dokuments in den Prior eingehen lassen, vielleicht sind lange Briefe ja h\u00e4ufig eher Trennungsschreiben... (ja, ja, wir unterstellen eine On-Off-Beziehung, sonst w\u00fcrden wir ja nur genau ein Trennungsschreiben erhalten und das h\u00e4tte einen gesch\u00e4tzten Prior von $0$, wenn wir es nicht korrigieren w\u00fcrden, s. unten).\n   \nUnsere Evidence $E$ ist das ge\u00e4u\u00dferte Dokument 6, also \"Taiwan Taiwan Kyoto\", als Bag-of-Words interpretiert, also $\\{Taiwan, Taiwan, Kyoto \\}$.\n\nWir fragen nach den Wahrscheinlichkeiten f\u00fcr die Hypothesen gegeben die Evidence (und, implizit, die Trainingsdaten, das lassen wir aus den Formeln, denn diese Menge ver\u00e4ndert sich in diesem Beispiel nicht), $P(h_x \\mid E)$ mit $x \\in \\{1,2\\}$.\n\nDiese k\u00f6nnen wir mit Bayes bestimmen (Reihenfolgen und Zusammenh\u00e4nge ignorieren wir hierbei - zurecht, denn in unserem Handtaschenmodell haben wir diese Informationen auch gar nicht!):\n\n\\begin{equation}\nP(h_1 \\mid E) = \\frac{P(E \\mid h_1) * P(h_1)}{P(E)} \\quad \\text{und} \\quad\nP(h_2 \\mid E) = \\frac{P(E \\mid h_2) * P(h_2)}{P(E)}\n\\end{equation}\n\n(1) $P(E)$ k\u00f6nnen wir ignorieren, es reicht, die wahrscheinlichere Klasse zu finden, dazu ben\u00f6tigen wir nur den Z\u00e4hler.\n\n(2) $P(E \\mid h_x)$ l\u00e4\u00dft sich \u00fcber unsere Unabh\u00e4ngigkeitsannahmen zerlegen in $P(w_1 \\mid h_x)* \\dots * P(w_k \\mid h_x)$, hierbei sei $E = \\{ w_1,\\dots,w_k\\}$ eben eine Bag-of-Words, d.h. es kann gelten $w_i = w_j$ f\u00fcr beliebige $i,j$, $i \\neq j$, und die Position ist irrelvant (d.h. zwei Bag-of-Words sind gleich, wenn die jedes vorkommende Wort in der gleichen Anzahl enthalten).\n\nWir k\u00f6nnen also wie folgt rechnen:\n\n\\begin{eqnarray}\nP(E \\mid h_1) * P(h_1) & = & P(\\{Taiwan, Taiwan, Kyoto\\} \\mid h_1) * P(h_1) \\\\ & = & P(Taiwan \\mid h_1) * P(Taiwan \\mid h_1) * P(Kyoto \\mid h_1)  * P(h_1) \\\\  & = & 2\/13 *  2\/13 * 3\/13 * 2\/5 = 0.00218479745 \\\\\n\\end{eqnarray}\n\\begin{eqnarray}\nP(E \\mid h_2) * P(h_2) & = & P(\\{Taiwan, Taiwan, Kyoto\\} \\mid h_2) * P(h_2) \\\\ & = &  P(Taiwan \\mid h_2) * P(Taiwan \\mid h_2) * P(Kyoto \\mid h_2)  * P(h_2)  \\\\  & = & 3\/14 *  3\/14 * 1\/14 * 3\/5 = 0.00196793002\n\\end{eqnarray}\n\nDie \"passendere\" Klasse ist also die Klasse $Japan$. Praktisch rechnen kann man das \u00fcbrigens auch mit **Logarithmen**, dann kann man summieren, statt zu multiplizieren und bleibt generell beherrschbarer in den Gr\u00f6\u00dfen (sonst hat man schnell einen [Underflow](https:\/\/en.wikipedia.org\/wiki\/Arithmetic_underflow), weil die Zahlen sehr sehr klein werden). Aber das ist f\u00fcr die Klausur nicht wichtig.\n\nWir k\u00f6nnten jetzt auch leicht die \"echten\" Wahrscheinlichkeiten bestimmen:\n   * $P(h_1 \\mid E) = 0.00218479745 \/ (0.00218479745 + 0.00196793002) \\sim 0.526$\n   * $P(h_2 \\mid E) = 0.00196793002 \/ (0.00218479745 + 0.00196793002) \\sim 0.474$\n\nAber sp\u00e4testens hier sollten wir sehr vorsichtig werden: diese tats\u00e4chlich sehr **naiven** Bayes'schen Modelle (dies hei\u00dft so, weil wir naiverweise annehmen, dass die Reihenfolge und die Zusammenstellung der W\u00f6rte keinen signifikanten Einfluss auf die Bedeutung, also hier die Klasse, hat) arbeiten oft \"\u00fcberraschend gut\" (s. z.B. Hinrich Sch\u00fctze), weil die Ordnungsrelation \u00fcber den Wahrscheinlichkeiten h\u00e4ufig \"passt\", die \"absoluten\" Wahrscheinlichkeiten sind aber in der Regel weit weg von den Wahrscheinlichkeiten, die man mit echten Bayes'schen Modellen finden w\u00fcrde.\n\nWir haben jetzt ein **[naives Bayes'sches Modell](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier)** f\u00fcr die Klassifikation in einem Bag-of-Words-Modell verwendet. Und zwar haben wir nur zwischen zwei Klassen unterschieden ([binary or binomial classification](https:\/\/en.wikipedia.org\/wiki\/Binary_classification)), es w\u00e4re aber direkt erweiterbar auf mehr als zwei Klassen (multinomial or multiclass classification).\n\n**Aufgabe**: Rechnen Sie das Modell f\u00fcr die folgende einfache Aufgabe so nach, wie wir es oben getan haben:\n\n| | docID | W\u00f6rter im Dokument | in Klasse $China$\n| ------------- |:-------------:| -----:|-----:|\n| Trainingsmenge | 1 | Chinese Beijing Chinese | Yes\n| | 2  | Chinese Chinese Shanghai   | Yes\n| | 3  | Chinese Macao  | Yes\n| | 4  | Tokyo Japan Chinese | No\n| Testmenge  | 5  | Chinese Chinese Chinese Tokyo Japan  | ?\n\nDaten zu unserer Aufgabe. Es gibt 2 Klassen, $China$ und $\\overline{China}$. Das \"No\"-Dokument ist in der Klasse $\\overline{China}$.\n\nZu welcher der beiden Klassen $China$ und $\\overline{China}$ geh\u00f6rt das Dokument 5 in einem naiven Bayes'schen Modell? \n\n_Hinweis 1_: sie brauchen nur die Wahrscheinlichkeiten bestimmen, die f\u00fcr eine Entscheidung n\u00f6tig sind. Was ist damit gemeint? Hm, brauchen Sie z.B. $P(Macao \\mid China)$? Nein!\n\n_Hinweis 2_: rechnen sie ausreichend genau, behalten sie 4 signifikante Ziffern, z.B. k\u00f6nnen sie aus $0,003455667$ $0,003455$ durch Abschneiden erhalten und damit weiterrechnen. 4 signifikante Ziffern meint hier: die erste von 0 verschiedene Ziffer von rechts plus 3 weitere Ziffern. Mit dieser Vorgehensweise k\u00f6nnen sie auch in der Klausur rechnen. Sie d\u00fcrfen das f\u00fcr abgeschnittene Zahlen das normale Gleichheitszeichen verwenden.\n\nDas finale Beispiel hier stammt aus einer [Pr\u00e4sentation von Hinrich Sch\u00fctze](http:\/\/www.cis.uni-muenchen.de\/~hs\/teach\/14s\/ir\/pdf\/13bayes.flat.pdf#page=32) aus einem Information-Retrieval-Kontext, dort k\u00f6nnen sie auch eine formale Herleitung der Vorgehensweise und Formeln finden. Alles, was f\u00fcr das Rechnen der Klausuraufgabe erforderlich ist, finden sie aber oben stehend."}}