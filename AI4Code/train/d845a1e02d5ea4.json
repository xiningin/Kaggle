{"cell_type":{"21649408":"code","4b865cec":"code","8d75516a":"code","d9fee4f2":"code","8c50e27f":"code","0e7122da":"code","b5893959":"code","7739aae3":"code","50253d0b":"code","c5458848":"code","277244da":"code","95fb538c":"code","c60ceaea":"code","1b8ba831":"code","8f239362":"code","3d8094e5":"code","c2fad7fe":"code","59246d0f":"code","0c223fce":"code","ca1dd7d0":"code","a62858f7":"code","017539cf":"code","0a880f5c":"code","d0ffc13c":"code","3cf19372":"code","b4072e1c":"code","693d0c32":"code","4b14cb88":"code","89e542ec":"code","fda7cb67":"code","c2c80f22":"code","1c166803":"code","1fa1e53f":"code","7a490213":"code","f540bf94":"code","bd306074":"code","21c436b4":"code","1f7b0982":"markdown","c2e69eea":"markdown","080b50cc":"markdown","4b60576a":"markdown","66d465ad":"markdown","203bc665":"markdown","168f66a5":"markdown","53cb9628":"markdown","01a7eb6b":"markdown","ca647e95":"markdown","ae6c969e":"markdown","f4726ad1":"markdown","63a3ad7d":"markdown","caba2292":"markdown","c34e19d6":"markdown","b01df61b":"markdown","50540e33":"markdown","b6c6c77a":"markdown","1c5ddff9":"markdown","31d57290":"markdown","cd661552":"markdown","ab693a6a":"markdown","24e0e811":"markdown","f29535fa":"markdown","1c223e4c":"markdown","0d511f2c":"markdown","d2c50c29":"markdown","b9cbaff6":"markdown"},"source":{"21649408":"from IPython.display import Image\nImage('..\/input\/iris-measurement\/iris_measurements.png')","4b865cec":"# importing required modules\nimport numpy as np\nimport pandas as pd\n# importing visualization maodules\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom mpl_toolkits import mplot3d\nfrom sklearn.tree import plot_tree\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score,KFold,StratifiedKFold\n# importing MachineLearning Algorithms\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler,StandardScaler\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso\nfrom sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB\nfrom sklearn.linear_model import LogisticRegression,PassiveAggressiveClassifier\nfrom sklearn.svm import SVC,SVR\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n# importing ensembling algorithms\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier,BaggingClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor,RadiusNeighborsClassifier\nfrom sklearn.decomposition import PCA,TruncatedSVD\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8d75516a":"# overview of the data\n### Reading the iris dataset\ndata=pd.read_csv('..\/input\/iris\/Iris.csv')\ndata.head(3)","d9fee4f2":"# checking whether our dataset has any null values\nprint(data.isnull().sum())","8c50e27f":"sns.pairplot(data.drop(['Id'],axis=\"columns\"),hue='Species')\nplt.show()","0e7122da":"fig=plt.figure(figsize=(20,20))\ndata.drop(['Id'],axis=\"columns\").plot(kind=\"kde\",subplots=True,ax=plt.gca())\nplt.show()","b5893959":"fig=plt.figure(figsize=(15,15))\ndata.drop(['Id'],axis=\"columns\").plot(kind=\"hist\",subplots=True,ax=plt.gca())\nplt.title(' histogram plotting of all the data')\nplt.show()","7739aae3":"fig=plt.figure(figsize=(10,10))\nax=plt.gca()\ndata.drop(['Id'],axis=\"columns\").hist(edgecolor='black', linewidth=1.2,ax=ax)\nplt.title('histogram plotting of numerical data in iris dataset')\nplt.show()","50253d0b":"def plot_scatter(feature1,feature2,title):\n    fig=px.scatter(data,x=feature1,y=feature2,color='Species',title=title,hover_data=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm',\n       'Species'],template=\"plotly_dark\")\n    fig.show()","c5458848":"plot_scatter(feature1='SepalLengthCm',feature2='SepalWidthCm',title='SepalLengthCm vs SepalWidthCm')\nplot_scatter(feature1='PetalLengthCm',feature2='PetalWidthCm',title='PetalLengthCm vs PetalWidthCm')","277244da":"plot_scatter(feature1='SepalLengthCm',feature2='PetalLengthCm',title='SepalLengthCm vs PetalLengthCm')\nplot_scatter(feature1='SepalWidthCm',feature2='PetalWidthCm',title='SepalWidthCm vs PetalWidthCm')","95fb538c":"def plot_bar(feature1,feature2,title):\n    fig=px.bar(data,x=feature1,y=feature2,color='Species',title=title,hover_data=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm',\n       'Species'],template=\"plotly_dark\")\n    fig.show()","c60ceaea":"plot_bar(feature1='SepalLengthCm',feature2='PetalWidthCm',title='SepalLengthCm vs PetalWidthCm')\nplot_bar(feature1='SepalWidthCm',feature2='PetalLengthCm',title='SepalWidthCm vs PetalLengthCm')","1b8ba831":"plot_bar(feature1='PetalLengthCm',feature2='SepalWidthCm',title='PetalLengthCm vs SepalWidthCm')\nplot_bar(feature1='PetalWidthCm',feature2='SepalLengthCm',title='PetalWidthCm vs SepalLengthCm')","8f239362":"def pie_chart_rep(values,names,title):\n    fig = px.pie(data, values=values, names=names, title=title,template=\"plotly_dark\")\n    fig.update_layout()\n    fig.show()","3d8094e5":"pie_chart_rep(values='SepalLengthCm',names='Species',title='pie representation of SepalLengthCm and Species')\npie_chart_rep(values='SepalWidthCm',names='Species',title='pie representation of SepalWidthCm and Species')","c2fad7fe":"pie_chart_rep(values='PetalLengthCm',names='Species',title='pie representation of PetalLengthCm and Species')\npie_chart_rep(values='PetalWidthCm',names='Species',title='pie representation of PetalWidthCm and Species')","59246d0f":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=data.Species, y=data.SepalLengthCm,mode='markers', name='SepalLengthCm'))\nfig.add_trace(go.Scatter(x=data.Species, y=data.SepalWidthCm,mode='markers', name='SepalWidthCm'))\nfig.add_trace(go.Scatter(x=data.Species, y=data.PetalLengthCm,mode='markers', name='PetalLengthCm'))\nfig.add_trace(go.Scatter(x=data.Species, y=data.PetalWidthCm,mode='markers', name='PetalWidthCm'))\nfig.update_layout(template=\"plotly_dark\",title='specifying the species based on SepalLengthCm')\nfig.show()               ","0c223fce":"fig=plt.figure(figsize=(10,10))\nplt.subplot(2,2,1)\nsns.swarmplot(x='Species',y='PetalLengthCm',data=data)\nplt.legend()\nplt.subplot(2,2,2)\nsns.swarmplot(x='Species',y='PetalWidthCm',data=data)\nplt.legend()\nplt.subplot(2,2,3)\nsns.swarmplot(x='Species',y='SepalLengthCm',data=data)\nplt.legend()\nplt.subplot(2,2,4)\nsns.swarmplot(x='Species',y='SepalWidthCm',data=data)\nplt.legend()\nplt.show()","ca1dd7d0":"# we convert the categorial data into numerical data using LabelEncoder()\nle=LabelEncoder()\ndata['SpeciesCategory']=le.fit_transform(data['Species'])\nlabel={0:'Iris-setosa',1:'Iris-versicolor',2:'Iris-virginica'}","a62858f7":"# we choose feature_names and target values from the data and split the data into train and test data\nX=data.drop(['Species','SpeciesCategory'],axis='columns')\ny=data['SpeciesCategory']\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0,test_size=0.3)","017539cf":"#Classification using KNeighborsClassifier and we will take nearest neighbors as 5\nknn_clf=KNeighborsClassifier(n_neighbors=5)\nknn_clf.fit(X_train,y_train)\nprint(\"the predicted values of X_test are {}\".format(knn_clf.predict(X_test)))\nprint(\"the accuracy score of trained data for KNN classifier is {}\".format(knn_clf.score(X_train,y_train)))\nprint(\"the accuracy score of test and predicted data for KNN classifier is {}\".format(knn_clf.score(X_test,knn_clf.predict(X_test))))","0a880f5c":"# plotting KNN for iris dataset\nfig=px.scatter(data,x='PetalLengthCm',y='PetalWidthCm',color='Species',title='PetalLengthCm vs PetalWidthCm in KNN Classifier',size='PetalWidthCm',hover_data=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm','Species'],template=\"plotly_dark\")\nfig.show()","d0ffc13c":"# plotting accuracy for a range of numbers \naccuracy=[]\nn_neighbors_range=range(1,10)\nfor i in n_neighbors_range:\n    knn_clf=KNeighborsClassifier(n_neighbors=i)\n    knn_clf.fit(X_train,y_train)\n    accuracy.append(knn_clf.score(X_train,y_train))\npx.line(n_neighbors_range,accuracy,title='n_neighbors_range vs accuracy',template='plotly_dark')","3cf19372":"radius_clf=RadiusNeighborsClassifier(radius=10)\nradius_clf.fit(X_train,y_train)\nprint(\"the predicted values of X_test are {}\".format(radius_clf.predict(X_test)))\nprint(\"the accuracy score of trained data for radius neighbors classifier is {}\".format(radius_clf.score(X_train,y_train)))\nprint(\"the accuracy score of test and predicted data for radius neighbors  classifier is {}\".format(radius_clf.score(X_test,radius_clf.predict(X_test))))","b4072e1c":"accuracy=[]\nradius_range=range(1,20)\nfor i in radius_range:\n    radius_clf=RadiusNeighborsClassifier(radius=i)\n    radius_clf.fit(X_train,y_train)\n    accuracy.append(radius_clf.score(X_train,y_train))\npx.line(radius_range,accuracy,title='radius vs accuracy',template='plotly_dark')","693d0c32":"svm_clf=SVC()\nsvm_clf.fit(X_train,y_train)\nprint(\"the predicted values of X_test are {}\".format(svm_clf.predict(X_test)))\nprint(\"the accuracy score of trained data for SVM is {}\".format(svm_clf.score(X_train,y_train)))\nprint(\"the accuracy score of test and predicted data for SVM is {}\".format(svm_clf.score(X_test,svm_clf.predict(X_test))))","4b14cb88":"# plotting accuracy for a range of numbers \naccuracy=[]\nc=range(1,10)\nfor i in c:\n    svm_clf=KNeighborsClassifier(n_neighbors=i)\n    svm_clf.fit(X_train,y_train)\n    accuracy.append(svm_clf.score(X_train,y_train))\npx.line(n_neighbors_range,accuracy,title='Regularization factor(C) vs accuracy',template='plotly_dark')","89e542ec":"mnb=MultinomialNB()\nbnb=BernoulliNB()\ngnb=GaussianNB()\nmnb.fit(X_train,y_train)\nbnb.fit(X_train,y_train)\ngnb.fit(X_train,y_train)\nprint(\"the accuracy score of trained data for MultinomialNB is {}\".format(mnb.score(X_train,y_train)))\nprint(\"the accuracy score of test and predicted data for MultinomialNB is {}\".format(mnb.score(X_test,mnb.predict(X_test))))\nprint(\"the accuracy score of trained data for BernoulliNB is {}\".format(bnb.score(X_train,y_train)))\nprint(\"the accuracy score of test and predicted data for BernoulliNB is {}\".format(bnb.score(X_test,bnb.predict(X_test))))\nprint(\"the accuracy score of trained data for GaussianNB is {}\".format(gnb.score(X_train,y_train)))\nprint(\"the accuracy score of test and predicted data for GaussianNB is {}\".format(gnb.score(X_test,gnb.predict(X_test))))","fda7cb67":"dec_tree=DecisionTreeClassifier(max_depth=10)\ndec_tree.fit(X_train,y_train)\nprint(\"predicted data is:\",svm_clf.predict(X_test))\nprint(\"the accuracy score of trained data for DecisionTreeClassifier is {}\".format(svm_clf.score(X_train,y_train)))\nprint(\"the accuracy score of test and predicted data for DecisionTreeClassifier is {}\".format(svm_clf.score(X_test,svm_clf.predict(X_test))))","c2c80f22":"from sklearn import tree\nfig=plt.figure(figsize=(10,10))\n_=tree.plot_tree(dec_tree,feature_names=data.PetalLengthCm,class_names=data.Species,filled=True)\nplt.title('DecisionTreeClassifier for PetalLengthCm vs Species')\nfig.show()","1c166803":"# plotting accuracy for a range of numbers \naccuracy=[]\nmax_depth=range(1,10)\nfor i in max_depth:\n    dec_tree=KNeighborsClassifier(n_neighbors=i)\n    dec_tree.fit(X_train,y_train)\n    accuracy.append(dec_tree.score(X_test,svm_clf.predict(X_test)))\npx.line(max_depth,accuracy,title='max_depth of the tree vs accuracy',template='plotly_dark')","1fa1e53f":"forest_clf=RandomForestClassifier(n_estimators=5)\nforest_clf.fit(X_train,y_train)\nprint(\"the predicted values are:\",forest_clf.predict(X_test))\nprint(\"the accuracy score of trained data for DecisionTreeClassifier is\",forest_clf.score(X_train,y_train))\nprint(\"the accuracy score of tested and predicted data for DecisionTreeClassifier is\",forest_clf.score(X_test,forest_clf.predict(X_test)))","7a490213":"fig=plt.figure(figsize=(10,10))\n_=tree.plot_tree(forest_clf.estimators_[3],feature_names=data.PetalLengthCm,class_names=data.Species)\nplt.title(\"random forest tree PetalLengthCm vs PetalLengthCm\")\nplt.show()","f540bf94":"# plotting accuracy for a range of numbers \naccuracy=[]\nn_estimators=range(1,10)\nfor i in n_estimators:\n    forest_clf=RandomForestClassifier(n_estimators=5)\n    forest_clf.fit(X_train,y_train)\n    accuracy.append(forest_clf.score(X_test,forest_clf.predict(X_test)))\npx.line(n_estimators,accuracy,title='n_estimators of the random forest classifier vs accuracy',template='plotly_dark')","bd306074":"log_reg=LogisticRegression(C=100)\nlog_reg.fit(X_train,y_train)\nprint(\"the predicted values are:\",log_reg.predict(X_test))\nprint(\"the accuracy score of trained data for DecisionTreeClassifier is\",log_reg.score(X_train,y_train))\nprint(\"the accuracy score of tested and predicted data for DecisionTreeClassifier is\",log_reg.score(X_test,log_reg.predict(X_test)))","21c436b4":"C_range=range(1,200)\naccuracy=[]\nfor n in C_range:\n    log_reg=LogisticRegression(C=n)\n    log_reg.fit(X_train,y_train)\n    accuracy.append(log_reg.score(X_test,log_reg.predict(X_test)))\npx.line(C_range,accuracy,title='Regularization factor(C) of the tree vs accuracy',template='plotly_dark')","1f7b0982":"### Label Encoding for Species target variable","c2e69eea":"# 2. Exploring the iris datset\n","080b50cc":"# SVM Classifier\n-  \u201cSupport Vector Machine\u201d (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.\n\n","4b60576a":"###### also after plotting n_estimators to accuracy shows that for any value of n_estimators we are getting an accuracy of 1.","66d465ad":"###### also after plotting n_neighbors_range to accuracy shows that for any value of n_neighbors we are getting an accuracy of 1.","203bc665":"# Topics\n1. About Iris dataset\n2. Exploring the Iris dataset\n3. iris data visualization\n4. Supervised learning on Iris dataset","168f66a5":"<h1><font color='brown'>                   Thank you!!!!!!!!!!!!!!!!","53cb9628":"# LogisticRegression\n- Types of Logistic Regression\n    - Generally, logistic regression means binary logistic regression having binary target variables, but there can be two more categories of target variables that can be predicted by it. Based on those number of categories, Logistic regression can be divided into following types:\n\n    - **Binary or Binomial:** In such a kind of classification, a dependent variable will have only two possible types either 1 and 0. \n    - **Multinomial:** In such a kind of classification, dependent variable can have 3 or more possible unordered types or the types having no quantitative significance.\n    - **Ordinal:** In such a kind of classification, dependent variable can have 3 or more possible ordered types or the types having a quantitative significance. For example, these variables may represent \u201cpoor\u201d or \u201cgood\u201d, \u201cvery good\u201d, \u201cExcellent\u201d and each category can have the scores like 0,1,2,3.\n ### For this iris dataset we will use Multinomial types of LogisticRegression","01a7eb6b":"- step1: we will first split the data into training and testing data and will divide in 7:3 ratio\n- step2: And now we will train the model using X_train and y_train with fit() function\n- step3: we will predict the X_test data using predict() function \n- step4: and then we'll find the accuracy of trained and predicted data using score() function","ca647e95":"- we will look at some of the most used machine learning supervised classifier algorithms such as\n    - KNN Classifier\n    - RadiusNeighborsClassifier\n    - SVM Classifier\n    - Naive-Bayes Classifier\n    - DecisionTreeClassifier\n    - RandomForestClassifier\n    - LogisticRegressor","ae6c969e":"# 1. About  iris Dataset","f4726ad1":"# Naive-Bayes Classifier\n- we apply Naive-Bayes Classifier for three types of Naive-Bayes:\n\n    - Gaussian: It is used in classification and it assumes that features follow a normal distribution.\n    - Multinomial: It is used for discrete counts. \n    - Bernoulli: The binomial model is useful if your feature vectors are binary (i.e. zeros and ones).\n- Naive Bayes is a probabilistic machine learning model which is used as a classifier. It comes under Supervised learning algorithms.\n- The intuition behind this algorithm is Bayes theorem. Bayes theorem tells the probability of an event occurring given the probability of another event that has been already occurred. Below is the mathematical equation of Bayes theorem.","63a3ad7d":"######  if we see plotting PetalLengthCm vs PetalWidthCm gives some good visualization we could easily identify our species based on PetalLengthCm vs PetalWidthCm through visualization also there is no outliers in those features","caba2292":"######  Above Decision tree Classifies data based on MSE(Mean Squared Error) and 'gini' value","c34e19d6":"# RadiusNeighborsClassifier\n- **RadiusNeighborsClassifier:** RadiusNeighborsClassifier implements learning based on the number of neighbors within a fixed radius 'r' of each training point, where 'r' is a floating-point value specified by the user.\n- It is very much similar to KNN classifier only difference we commonly consider is, in KNN Classifier we use n_neighbors as a key factor but in RadiusNeighborsClassifier we use radius as a key factor it classifier the data based on a radius value given. ","b01df61b":"# KNN Classifier\n- K-Nearest Neighbors (KNN) is one of the simplest algorithms used in Machine Learning for regression and classification problem. KNN algorithms use data and classify new data points based on similarity measures (e.g. distance function). Classification is done by a majority vote to its neighbors. The data is assigned to the class which has the nearest neighbors. As you increase the number of nearest neighbors, the value of k, accuracy might increase.","50540e33":"<h1>***********  Please <font color='red'>UPVOTE<\/font color> if you like my work  ****************","b6c6c77a":"- The iris dataset contains the following data\n     - 50 samples of 3 different species of iris (150 samples total)\n- Measurements: sepal length, sepal width, petal length, petal width\n     - The format for the data: (sepal length, sepal width, petal length, petal width)","1c5ddff9":"# DecisionTreeClassifer\n- **Decision Tree :** Decision tree is the most powerful and popular tool for classification and prediction. A Decision tree is a flowchart like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label.\n- The strengths of decision tree methods are:\n   - Decision trees are able to generate understandable rules.\n   - Decision trees perform classification without requiring much computation.\n   - Decision trees are able to handle both continuous and categorical variables.\n   - Decision trees provide a clear indication of which fields are most important for prediction or classification.\n\n","31d57290":"###### we could see the accuracy values changes with radius value","cd661552":"# 3. iris data visualization\n1. We will visualize the data using seaborn,matplotlib,plotly","ab693a6a":"######  also after plotting max_depth to accuracy shows that for any value of n_neighbors we are getting an accuracy of 1.","24e0e811":"# 4. Supervised learning on Iris dataset\n **Supervised learning:** Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples.Supervised learning is divided into Regression and Classification.\n-  **Regression:** Regression is the process of finding a model or function for distinguishing the data into continuous real values instead of using classes or discrete values. It can also identify the distribution movement depending on the historical data. Because a regression predictive model predicts a quantity, therefore, the skill of the model must be reported as an error in those predictions\n-   **Classification:** Classification is the process of finding or discovering a model or function which helps in     -separating the data into multiple categorical classes i.e. discrete values. In classification, data is categorized under different labels according to some parameters given in input and then the labels are predicted for the data.\n\n\n##### Before we go into Supervised learning we must know something called feature engineering:\n- Feature Selection is one of the core concepts in machine learning which hugely impacts the performance of your model. The data features that you use to train your machine learning models have a huge influence on the performance you can achieve.\n- Irrelevant or partially relevant features can negatively impact model performance.\n- Feature selection and Data cleaning should be the first and most important step of your model designing.\n\nHow to select features and what are Benefits of performing feature selection before modeling your data?\n\n1. Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n\n2. Improves Accuracy: Less misleading data means modeling accuracy improves.\n\n3. Reduces Training Time: fewer data points reduce algorithm complexity and algorithms train faster.\n\n\nAlso, we must know two terms (\nfeature_names ---> feature_names are variables that are used to determine the classification process in this iris dataset we use SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm as feature_names\nand \ntarget ---> the target variables are the output variable in this dataset we use Species as target\n)","f29535fa":"###### also after plotting Regularization factor(C) to accuracy shows that for any value of n_neighbors we are getting an accuracy of 1.","1c223e4c":"# To process supervised learning algorithms","0d511f2c":"###### also after plotting n_neighbors_range to accuracy shows that for any value of n_neighbors we are getting an accuracy of 1.","d2c50c29":"# RandomForestClassifier\n**RandomForestClassifier:** Random forest is a supervised learning algorithm which is used for both classification as well as regression. But however, it is mainly used for classification problems. As we know that a forest is made up of trees and more trees means more robust forest. Similarly, random forest algorithm creates decision trees on data samples and then gets the prediction from each of them and finally selects the best solution by means of voting. It is an ensemble method which is better than a single decision tree because it reduces the over-fitting by averaging the result.\n- We can understand the working of Random Forest algorithm with the help of following steps \u2212\n\n   - Step 1 \u2212 First, start with the selection of random samples from a given dataset.\n\n   - Step 2 \u2212 Next, this algorithm will construct a decision tree for every sample. Then it will get the prediction result from every decision tree.\n\n   - Step 3 \u2212 In this step, voting will be performed for every predicted result.\n\n   - Step 4 \u2212 At last, select the most voted prediction result as the final prediction result.","b9cbaff6":"- Above Decision tree Classifies data based on MSE(Mean Squared Error) and 'gini' value\n- A tree can be \u201clearned\u201d by splitting the source set into subsets based on an attribute value test. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node all has the same value of the target variable, or when splitting no longer adds value to the predictions. "}}