{"cell_type":{"e5fd19eb":"code","dc7687ca":"code","77ac8cb2":"code","8fc6aad3":"code","a88e4d95":"code","29d7676f":"code","c56c6c7b":"code","d72ffb06":"code","6f10b0bc":"code","c8afcc58":"code","9efe37d0":"code","2921532f":"code","b2f8d01d":"code","4a41b7f0":"code","487ef958":"code","404b9708":"code","4d855911":"code","cec5143a":"code","85a1161d":"code","5c4114ea":"code","634c8299":"code","2d0148e7":"code","0023c757":"code","0a9a1970":"code","25a54435":"code","889ffb98":"code","783a922d":"markdown","7ed7e17d":"markdown","b10d6fff":"markdown","a861a025":"markdown","d3c8f6ed":"markdown","4e136f28":"markdown","99cad0d4":"markdown","a3cc4c81":"markdown","c414172d":"markdown","3dda2ac2":"markdown","84ffa89f":"markdown","a11dd9ff":"markdown"},"source":{"e5fd19eb":"#### This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dc7687ca":"df_real = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/True.csv\")\ndf_fake = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv\")","77ac8cb2":"df_real.head()","8fc6aad3":"df_fake.head()","a88e4d95":"print(\"Total number of real news records : \"+str(len(df_real)))\nprint(\"Total number of fake news records : \"+str(len(df_fake)))","29d7676f":"real_list = [0 for x in range(len(df_real))]\nfake_list = [1 for x in range(len(df_fake))]","c56c6c7b":"df_real['Target'] = real_list\ndf_fake['Target'] = fake_list","d72ffb06":"df_real.head()","6f10b0bc":"df_fake.head()","c8afcc58":"df = pd.concat([df_real, df_fake], axis = 0)\nprint(\"Total number of news records : \"+str(len(df)))","9efe37d0":"df.head()","2921532f":"#shuffle the dataframe\ndf = df.sample(frac = 1)\ndf.head()","b2f8d01d":"final_df = df.drop(['text','subject','date'],axis=1)\nfinal_df.head()","4a41b7f0":"final_df.to_csv(\".\/dataframe.csv\")","487ef958":"y = final_df['Target']\nX = final_df['title']","404b9708":"type(X),type(y)","4d855911":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3, random_state = 100)","cec5143a":"X_train","85a1161d":"from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(stop_words='english')","5c4114ea":"vect.fit(X_train)","634c8299":"#printing the vocabulary size.\nlen(vect.vocabulary_.keys())","2d0148e7":"X_train_trasnformed = vect.transform(X_train)\nX_test_trasnformed = vect.transform(X_test)","0023c757":"from sklearn.naive_bayes import MultinomialNB\n\nmnb = MultinomialNB()\n\nmnb.fit(X_train_trasnformed,y_train)\n\ny_pred = mnb.predict(X_test_trasnformed)","0a9a1970":"from sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred)","25a54435":"from sklearn.naive_bayes import BernoulliNB\n\nbnb = BernoulliNB()\n\nbnb.fit(X_train_trasnformed,y_train)\n\ny_pred = bnb.predict(X_test_trasnformed)","889ffb98":"from sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred)","783a922d":"## TFIDF Vectorizer :","7ed7e17d":"We are able to get a slightly better accuracy on Bernoulli Naive Bayes compared to Multinomial Naive Bayes.","b10d6fff":"# **Model Building and evaluation :**","a861a025":"Concat both the dataframes into a single dataframe:","d3c8f6ed":"Splitting the dataframe into train and test dataset.","4e136f28":"Vectorising the title values since ML models dont accept strings as input directly:","99cad0d4":"Note: Ensure X and y are pandas Series here.","a3cc4c81":"# **Data Extraction and preprocessing:**","c414172d":"Based on the above count, we see the datapoints are well distributed.","3dda2ac2":"*Multinomial Naive Bayes:*","84ffa89f":"We will only consider title and target column for classification:","a11dd9ff":"*Bernoulli Naive Bayes:*"}}