{"cell_type":{"d7c74498":"code","bd3f43f8":"code","5edaa96a":"code","11f997da":"code","e055eba2":"code","d97a9e36":"code","7b9259e4":"code","8036a470":"code","4552874c":"code","3364f4e8":"code","2923b510":"code","8ad49cf8":"code","aba62185":"code","b7ba87d0":"code","9a2f6bd6":"code","42365350":"code","db54e481":"code","ef442aa2":"code","84408927":"code","7602e367":"code","60e559b9":"code","b2ea1ccd":"code","3c6a4e02":"code","af7b734a":"code","38b4b69e":"code","74e8a24f":"code","c0b7d33c":"code","a3057728":"markdown","27fd09a1":"markdown","4cf7ee9f":"markdown","fb0cffc8":"markdown","ddfa05f4":"markdown","4e9b4150":"markdown","00e161a0":"markdown","7aa0630d":"markdown","170b0fb1":"markdown","09404193":"markdown"},"source":{"d7c74498":"## Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() \n","bd3f43f8":"### Creating a Raw Data\nx_train = np.linspace(-10, 10, 1000) \ny_train = 2 * x_train + np.random.randn(*x_train.shape) * 0.33\n \nplt.scatter(x_train, y_train)  \nplt.show() \n","5edaa96a":"#Set up fake data that we will use to find a best fit line \n\nlearning_rate = 0.01  \ntraining_epochs = 100\n \nx_train = np.linspace(-1, 1, 101)  \ny_train = 2 * x_train + np.random.randn(*x_train.shape) * 0.33","11f997da":"#Set up the input and output nodes as placeholders since the value will be injected by x_train and y_train.\n\nX = tf.placeholder(tf.float32) \nY = tf.placeholder(tf.float32)","e055eba2":"#Define the model as y = w*x \n\ndef model(X, w):  \n    return tf.multiply(X, w)   ","d97a9e36":"#Set up the weights variable \n\nw = tf.Variable(0.0, name=\"weights\")\n \ny_model = model(X, w)  \ncost = tf.square(Y-y_model)\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) \nsess = tf.Session()\ninit = tf.global_variables_initializer()  \nsess.run(init)","7b9259e4":"#Set up a session and initialize all variables \n#Loop through the dataset multiple times \n\nfor epoch in range(training_epochs):  \n    for (x, y) in zip(x_train, y_train):  \n        sess.run(train_op, feed_dict={X: x, Y: y})","8036a470":"#Update the model parameter(s) to try to minimize the cost function \n#Close the session \n#Plot the original data \n#Plot the best fit line \n\nw_val = sess.run(w)\nsess.close() \nplt.scatter(x_train, y_train) \ny_learned = x_train*w_val  \nplt.plot(x_train, y_learned, 'r')  \nplt.show()","4552874c":"#Set up some fake raw input data\nlearning_rate = 0.01  \ntraining_epochs = 40 \n \ntrX = np.linspace(-1, 1, 101)","3364f4e8":"#Set up raw output data based on a degree 5 polynomial \nnum_coeffs = 6\ntrY_coeffs = [1, 2, 3, 4, 5, 6]  \ntrY = 0","2923b510":"for i in range(num_coeffs):\n    trY += trY_coeffs[i] * np.power(trX, i) ","8ad49cf8":"#add some noise\ntrY += np.random.randn(*trX.shape) * 1.5","aba62185":"#Plot\n\nplt.scatter(trX, trY)\nplt.show()","b7ba87d0":"#Define the nodes to hold values for input\/output pairs \nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)","9a2f6bd6":"#Define Model\ndef model(X, w):\n    terms = []\n    for i in range(num_coeffs):\n        term = tf.multiply(w[i], tf.pow(X, i))\n        terms.append(term) \n    return tf.add_n(terms)","42365350":"#Set up the parameter vector to all zero\nw = tf.Variable([0.] * num_coeffs, name=\"parameters\")  \ny_model = model(X, w)","db54e481":"#Define Cost function\ncost = (tf.pow(Y-y_model, 2))  \ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)","ef442aa2":"#Set up sesion and run learning algorithm\nsess = tf.Session() \ninit = tf.global_variables_initializer() \nsess.run(init)\n","84408927":"for epoch in range(training_epochs):\n    for (x, y) in zip(trX, trY): \n        sess.run(train_op, feed_dict={X: x, Y: y})\nw_val = sess.run(w)\nprint(w_val)\n","7602e367":"#Close Session\nsess.close()","60e559b9":"plt.scatter(trX, trY)\ntrY2 = 0 \nfor i in range(num_coeffs): \n    trY2 += w_val[i] * np.power(trX, i) \n \nplt.plot(trX, trY2, 'r')\nplt.show()","b2ea1ccd":"# Split into test and train data\n\ndef split_dataset(x_dataset, y_dataset, ratio):\n    arr = np.arange(x_dataset.size)  \n    np.random.shuffle(arr)  \n    num_train = int(ratio * x_dataset.size)  \n    x_train = x_dataset[arr[0:num_train]]  \n    y_train = y_dataset[arr[0:num_train]]  \n    x_test = x_dataset[arr[num_train:x_dataset.size]]   \n    y_test = y_dataset[arr[num_train:x_dataset.size]]  \n    return x_train, x_test, y_train, y_test\n\n","3c6a4e02":"learning_rate = 0.001  \ntraining_epochs = 1000  \nreg_lambda = 0.\n \nx_dataset = np.linspace(-1, 1, 100)\nnum_coeffs = 9  \ny_dataset_params = [0.] * num_coeffs  \ny_dataset_params[2] = 1  \ny_dataset = 0  \nfor i in range(num_coeffs):  \n    y_dataset += y_dataset_params[i] * np.power(x_dataset, i)  \ny_dataset += np.random.randn(*x_dataset.shape) * 0.3\n    ","af7b734a":"(x_train, x_test, y_train, y_test) = split_dataset(x_dataset, y_dataset, 0.7) \n \nX = tf.placeholder(tf.float32)   \nY = tf.placeholder(tf.float32)\n\ndef model(X, w):  \n    terms = []\n    for i in range(num_coeffs):\n        term = tf.multiply(w[i], tf.pow(X, i))  \n        terms.append(term)  \n    return tf.add_n(terms)\n ","38b4b69e":"w = tf.Variable([0.] * num_coeffs, name=\"parameters\")  \ny_model = model(X, w)  \ny_model\n\ncost = tf.div(tf.add(tf.reduce_sum(tf.square(Y-y_model)),\n                     tf.multiply(reg_lambda, tf.reduce_sum(tf.square(w)))),\n              2*x_train.size)\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)","74e8a24f":"sess = tf.Session()\ninit = tf.global_variables_initializer() \nsess.run(init)","c0b7d33c":"for reg_lambda in np.linspace(0,1,100):\n    for epoch in range(training_epochs): \n        sess.run(train_op, feed_dict={X: x_train, Y: y_train})  \n    final_cost = sess.run(cost, feed_dict={X: x_test, Y:y_test})  \n    print('reg lambda', reg_lambda)  \n    print('final cost', final_cost)\n \nsess.close()","a3057728":"What if the regression pattern is not as straught line but other trajecteries ?\nTo make the linear regression more flexible , polynomial model can be used. \n\nPolynomial pattern can be represented as :\n\n![image.png](attachment:image.png)","27fd09a1":"![image.png](attachment:image.png)","4cf7ee9f":"Linear Regression works on the hypothesis of Equation of Line.\ny = ax + b\n\nAnd the graphical representations of the predections looks like \n\n![image.png](attachment:image.png)","fb0cffc8":"Contents :\n\n\n1.\tFitting a Line over Data set points\n2.\tFitting arbitrary curves to data points\n3.\tTesting performance of these regression algorithms\n4.\tApplying regression in real-world data.\n","ddfa05f4":"Lets see the linear regression Model using tensor flow and its visualization.","4e9b4150":"A regression algorithm is meant for continous output. Flow is as below :\n\n![image.png](attachment:image.png)\n\nHere output is given only as continous, because the discrete valued outputs are better handled by classification models.","00e161a0":"Polynomial curve can be represented as :\n\n![image.png](attachment:image.png)","7aa0630d":"During realtime data , the above model can give the problem of overfitting.\nTo overcome from overfitting , we need to use the regularization techniques. Regularization technique can be mathematically represented as below :\n![image.png](attachment:image.png)\n\nHere how good the model fit depends on the value of lambda. \nWe need to check with various lambda values and see which one is fitting best.","170b0fb1":"Reference :\nWiki\nMachine Learning with Tensor Flow","09404193":"Why Tensor Flow ?\n\n1. Good performance when selecting High End Computational Data.\n2. When need to use GPU for computational purpose.\n"}}