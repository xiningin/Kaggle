{"cell_type":{"eeb20e4f":"code","9043739a":"code","9963af3d":"code","aaf1180f":"code","8f62b46a":"code","7ddceee9":"code","a0bd5b8f":"code","9685f4f7":"code","288e82e3":"code","be68a136":"code","2d600401":"code","78afff69":"code","aea510d7":"code","a50140fd":"code","3a5caff9":"code","e43e5d24":"code","989fb5e9":"code","3f0fd8c9":"code","44b7fbf8":"code","d190a08b":"code","1cc55777":"code","d2ad11d3":"markdown","59f01034":"markdown","a0e37951":"markdown","1a15d452":"markdown","d4a1828a":"markdown","6c031622":"markdown","4fb4e36c":"markdown","b37f7bff":"markdown","58a93437":"markdown","d4126d94":"markdown","6d11416c":"markdown","8a231b92":"markdown","d3b99c4f":"markdown","057de85a":"markdown","56aad050":"markdown","3077c393":"markdown","69aa9c17":"markdown"},"source":{"eeb20e4f":"import warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n\nimport os\nimport re\nimport json\nimport glob\nfrom collections import defaultdict\nfrom textblob import TextBlob\nfrom functools import partial\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\n\nimport nltk\nimport spacy\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nnlp.max_length = 4000000\nfrom nltk.probability import FreqDist\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom tqdm.autonotebook import tqdm\nimport string\n\n%matplotlib inline\n\nos.listdir('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/')\n\n\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\n \n        \ndef random_colours(number_of_colors):\n    '''\n    Simple function for random colours generation.\n    Input:\n        number_of_colors - integer value indicating the number of colours which are going to be generated.\n    Output:\n        Color in the following format: ['#E86DA4'] .\n    '''\n    colors = []\n    for i in range(number_of_colors):\n        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n    return colors","9043739a":"train = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nsample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","9963af3d":"train.head()\n","aaf1180f":"train.info()\n","8f62b46a":"[print(f\"{col}:{len(train[col].unique())}\") for col in train.columns]   #finding unique values in each column","7ddceee9":"def read_append_return(filename, train_files_path=train_files_path, output='text'):\n    \"\"\"\n    Function to read json file and then return the text data from them and append to the dataframe\n    \"\"\"\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","a0bd5b8f":"%%time\ntqdm.pandas()   #tqdm is used to show any code running with a progress bar. \ntrain['text'] = train['Id'].progress_apply(read_append_return)","9685f4f7":"train.head()\n","288e82e3":"%%time\ntqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_files_path))","be68a136":"sample_sub.head()\n","2d600401":"train.describe()\n","78afff69":"temp = train.groupby('cleaned_label').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","aea510d7":"fig = go.Figure(go.Funnelarea(\n    text =temp.cleaned_label,\n    values = temp.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of cleaned_label Distribution\"}\n    ))\nfig.show()","a50140fd":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n#     text = re.sub(\"\/'+\/g\", ' ', text)\n    \n    return text","3a5caff9":"%%time\ntqdm.pandas()\ntrain['text'] = train['text'].progress_apply(text_cleaning)","e43e5d24":"train.head()\n","989fb5e9":"train['temp_list'] = train['cleaned_label'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","3f0fd8c9":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(train['cleaned_label'])","44b7fbf8":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in cleaned_label', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","d190a08b":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(train['text'])","1cc55777":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","d2ad11d3":"So, now we have our text appended in our train dataframe.","59f01034":"Lets look at the distribution of cleaned_data in the train set\n\n","a0e37951":"Also, we have the text of for the sample_submission file","1a15d452":"Let's load the datasets\n\n","d4a1828a":"You can move your mouse on each color to see details!","6c031622":"A quick look at the train data\n\n","4fb4e36c":"# Text analysis","b37f7bff":"Hi every body","58a93437":"**EDA**","d4126d94":"And for text","6d11416c":"Extract text from json file and plus its column to train csv file","8a231b92":"**Most Common words\n**","d3b99c4f":"**Cleaning the data**","057de85a":"There are no null Values in the train set\n\n","56aad050":"Import library and modules","3077c393":"Let's draw a Funnel-Chart for visualization\n\n","69aa9c17":"Special thanks to helper notebooks \ud83d\ude4f\ud83c\udffb:\n1. \ud83d\udcddColeridge Initiative-EDA\ud83d\udcda& Baseline Model\ud83c\udfaf\n2. Twitter sentiment Extaction-Analysis,EDA and Model"}}