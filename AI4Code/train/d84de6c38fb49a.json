{"cell_type":{"e91f61f9":"code","73e06615":"code","9abf8451":"code","132a1047":"code","cb371a23":"code","21c6553d":"code","aa9e3071":"code","ea8f6c4f":"code","a22bd239":"code","8b3b016c":"code","9d3a2560":"code","798612ac":"code","a14d0502":"code","92b6a17c":"code","6280031b":"code","ad9ae9d3":"code","e0f958f6":"code","2406e465":"code","1c259057":"code","ba740736":"code","04dea8ee":"code","f39f280f":"code","6f065b76":"code","62ada543":"code","1af85b1b":"code","2e204312":"code","04e8b2c3":"code","e8171dcf":"code","8585b868":"code","0a7da8e0":"markdown","618cc19b":"markdown","edfa668f":"markdown","ccf57a55":"markdown","5b22be2e":"markdown"},"source":{"e91f61f9":"from sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error, r2_score, get_scorer, make_scorer, mean_absolute_error\nfrom sklearn.model_selection import cross_val_predict, train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler,RobustScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.kernel_ridge import KernelRidge\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n%matplotlib inline","73e06615":"import tensorflow as tf\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras import layers\nfrom keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\nfrom keras.models import Model, load_model\nfrom matplotlib.pyplot import imshow\nfrom keras.optimizers import Adam, Adadelta, Adagrad, SGD\nimport keras.backend as K\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping","9abf8451":"# Setting GPU as the primary Device\n\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n# from keras import backend as K\n# K.tensorflow_backend._get_available_gpus()","132a1047":"# Reading Test and Train Datasets\n\ndf = pd.read_csv(\"..\/input\/mengary-revenue-prediction\/train.csv\", index_col='id')\ndf_test = pd.read_csv(\"..\/input\/mengary-revenue-prediction\/test.csv\", index_col='id')\ndf.head()","cb371a23":"# Extracting Delivery time as a new feature\n\ndf['delivery date'] = pd.to_datetime(df['delivery date'])\ndf['placement date'] = pd.to_datetime(df['placement date'])\ndf_test['delivery date'] = pd.to_datetime(df_test['delivery date'])\ndf_test['placement date'] = pd.to_datetime(df_test['placement date'])\ndf['delivery time'] = (df['delivery date']-df['placement date']).dt.days+1\ndf_test['delivery time'] = (df_test['delivery date']-df_test['placement date']).dt.days+1\ndf['delivery time'].head()","21c6553d":"print(df['profit'].max(), df['profit'].min())\ndf.describe()","aa9e3071":"# def outier_remover(arr, alpha):\n#     q1 = arr.quantile(alpha)\n#     q3 = arr.quantile(1-alpha)\n#     iqr = q3-q1\n#     lb = q1-1.5*iqr\n#     ub = q3+1.5*iqr\n#     return lb,ub\n\n# lb,ub = outier_remover(df['price'], 0.05)\n# df.drop(df[(df['price']<lb) | (df['price']>ub)].index, inplace=True)\n# print(df.shape)\n\n# lb,ub = outier_remover(df['profit'], 0.01)\n# df.drop(df[(df['profit']<lb) | (df['profit']>ub)].index, inplace=True)\n# print(df.shape)\n\ndf.describe()","ea8f6c4f":"# Forming labels for train-test-spliting\n\nprint(len(df.location.unique()),len(df['class'].unique()), len(df.segment.unique()), len(df['sub-class'].unique()))\n# df.segment.unique()\nlabel = df['sub-class'] #df.location+df.segment\nle = LabelEncoder()\nlabel = le.fit_transform(label)\nlabel","a22bd239":"# Droping the un-necessary features \ntry:\n    df = df.drop(['departure city','RID','delivery date','address code','placement date','departure state'], axis=1)\n    df_test = df_test.drop(['departure city','RID','delivery date','address code','placement date','departure state'], axis=1)\nexcept:\n    pass\ndf.head()","8b3b016c":"# print(len(df['departure state'].unique()))\nprint('df shape:', df.shape, '; df_test shape:', df_test.shape)\ncategorical = ['location', 'class', 'segment', 'sub-class', 'delivery type']","9d3a2560":"# Filling the missing values with Mode(as categorical feature have missing values)\n\ndf = df.fillna(df.mode().iloc[0])\ndf_test = df_test.fillna(df_test.mode().iloc[0])\nprint(df.shape, df_test.shape)","798612ac":"# Onehot encoding the categorical variables\n\nenc = OneHotEncoder(drop='first')\ncoded = enc.fit_transform(df[categorical]).toarray()\ntest_coded = enc.transform(df_test[categorical]).toarray()\nprint(enc.categories_)\ncoded","a14d0502":"print(coded.shape, test_coded.shape)","92b6a17c":"# droping the categorical features from the dataframe\n\ndf1 = df.drop((categorical), axis = 1)\ntest_df1 = df_test.drop((categorical), axis = 1)\nprint(df1.head(), type(df1), sep='\\n')\nx = df1.drop(['profit'], axis=1)\ny = df1['profit']","6280031b":"x.dtypes","ad9ae9d3":"# Normalising the dataset(which only consists of numerical data)\n\nscaler = RobustScaler()\nx = scaler.fit_transform(x)\ntest_x = scaler.transform(test_df1)","e0f958f6":"# Merging numerical and onehot encoded categorical features\n\ndata = np.concatenate((x, coded), axis = 1)\ntest_data = np.concatenate((test_x, test_coded), axis = 1)\nprint(data.shape)\nprint(test_data.shape)\ndata[:2]","2406e465":"corr = pd.DataFrame(data).corr()\nimport seaborn as sns\nsns.heatmap(corr)","1c259057":"# Spliting the data\nseed = 2019\nx_train, x_test, y_train, y_test = train_test_split(data,y.values,test_size=0.25, stratify = label, random_state=seed)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","ba740736":"def coeff_determination(y_true, y_pred):\n    from keras import backend as K\n    SS_res =  K.sum(K.square( y_true-y_pred ))\n    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n    return ( 1 - SS_res\/(SS_tot + K.epsilon()) )","04dea8ee":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\n\nopt = Adadelta(learning_rate=0.05)\n\n# Initialising the ANN\nclassifier = Sequential()\n \n# Adding the input layer and the first hidden layer\n#classifier.add(Dropout(0.2, input_shape=(76,)))\nclassifier.add(Dense(units = 2048, kernel_initializer = 'glorot_uniform', activation = 'relu',input_dim=x_train.shape[1]))\nclassifier.add(Dense(units = 1024, kernel_initializer = 'glorot_uniform', activation = 'relu'))\n# classifier.add(Dense(units = 1024, kernel_initializer = 'normal', activation = 'relu'))\n# classifier.add(Dropout(0.2))\nclassifier.add(Dense(units = 512, kernel_initializer = 'glorot_uniform', activation = 'relu'))\n \n# Adding the second hidden layer\nclassifier.add(Dense(units = 256, kernel_initializer = 'glorot_uniform', activation = 'relu'))\nclassifier.add(Dense(units = 128, kernel_initializer = 'glorot_uniform', activation = 'relu'))\nclassifier.add(Dropout(0.1))\n# Adding the output layer\nclassifier.add(Dense(units =1, kernel_initializer = 'glorot_uniform', activation = 'linear'))\n \n# Compiling the ANN\nclassifier.compile(optimizer = opt, loss = 'mse', metrics = [coeff_determination])\n \n# Fitting the ANN to the Training set\nclassifier.summary()","f39f280f":"from keras.callbacks import ModelCheckpoint\ntry:\n    os.mkdir('.\/weights')\nexcept:\n    print('weights directory already exist :)')\n\ncheckpoint_name = \".\/weights\/best-weights.hdf5\"\ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='min')","6f065b76":"def scheduler(epoch, lr):\n    if epoch>200 and epoch<250:\n        return lr * tf.math.exp(-0.05)\n    else:\n        return lr\nlr_sche = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n\ncallbacks_list = [checkpoint]","62ada543":"history = classifier.fit(x_train, y_train, validation_split=0.2, batch_size = 512, epochs = 300, callbacks=callbacks_list)","1af85b1b":"classifier.load_weights(\".\/weights\/best-weights.hdf5\")\nclassifier.compile(optimizer = opt, loss = 'mse', metrics = [coeff_determination])","2e204312":"## Getting prediction on the test dataset\n\ny_pred = classifier.predict(x_test)\ny_pred","04e8b2c3":"# classifier.optimizer.learning_rate","e8171dcf":"## Checking the performance of the model on test dataset\n\nimport sklearn.metrics\nimport math\nr2_score=sklearn.metrics.r2_score(y_test,y_pred)\nmeansqerr =math.sqrt(sklearn.metrics.mean_squared_error(y_test, y_pred))\nprint(\"R2_score= %f\" %(r2_score))\nprint(\"Root_Mean Square error= %f\" %(meansqerr))","8585b868":"y_predict = classifier.predict(test_data)\nresult = pd.DataFrame(y_predict, index = df_test.index, columns=['profit'])\nresult.to_csv('result_ANN_1.csv')\nresult.head()","0a7da8e0":"## Predicting the profit values for the test.csv file for submission","618cc19b":"### Fitting the model with the dataset","edfa668f":"### Creating a file for saving the best weights","ccf57a55":"### Loading the weights of the best run   ","5b22be2e":"## Creating model"}}