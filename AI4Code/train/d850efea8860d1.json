{"cell_type":{"98a20e91":"code","21284329":"code","d017548f":"code","4dbd2dfd":"code","9cddcb79":"code","6590883d":"code","8930f109":"code","86838faf":"code","7350c537":"code","aa6b6022":"code","3a28878c":"code","c424755c":"code","ca1055ca":"code","c5c29d03":"code","83aaa361":"code","18c53690":"code","094c92fe":"code","887a0c45":"code","ac840f97":"code","87a51859":"code","67aca622":"code","6aad34bc":"code","c95d0cbf":"code","1a401911":"code","1b56234d":"code","49f8b4e6":"code","cb4cc830":"code","a700acd6":"code","fbf7e455":"code","ebe58a9d":"code","b3da00ff":"code","96aa150c":"code","34c5fbab":"code","be27077e":"code","8366646b":"code","ccdeaf42":"code","729dd0a6":"code","f670de64":"code","d1985b27":"code","1014748b":"code","543e35cb":"code","06f56443":"code","690a88cb":"code","51ed8228":"code","1dc9dae0":"code","37eb0511":"code","805c3c7f":"code","672dbf88":"code","b49ce83d":"code","823214a0":"code","1c751e4c":"markdown","ee362c9a":"markdown","272530ac":"markdown","e6ca7a6c":"markdown","1ef24323":"markdown","7329c03a":"markdown","dc50dee2":"markdown","bab8aa05":"markdown","d028e4ab":"markdown","2b13e0b2":"markdown","bb8190d6":"markdown","245dbf11":"markdown","66dd459f":"markdown","6f5c0512":"markdown","ca0a4476":"markdown","ce80e5b0":"markdown","9f1461fc":"markdown","2907da02":"markdown","ebf0ce20":"markdown","af70e270":"markdown","4e33b9c4":"markdown","a3dad52b":"markdown","55c8040d":"markdown","8386c3eb":"markdown","a0af4a60":"markdown","91ece99a":"markdown","a2244bff":"markdown","2363aa0e":"markdown","dcf45e7c":"markdown","29d47ba1":"markdown","4ec47de7":"markdown","17d759a0":"markdown","f810c203":"markdown","e88de69b":"markdown","681e2dde":"markdown","762d1c1e":"markdown"},"source":{"98a20e91":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","21284329":"#import data \ndf = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\ndf.head()","d017548f":"df.info()","4dbd2dfd":"print(\"shape:{} and size:{} of the data\".format(df.shape , df.size))","9cddcb79":"df.describe()","6590883d":"df.columns","8930f109":"print(\"If any Missing Value is present in the data\")\ndf.isnull().sum()","86838faf":"df.DEATH_EVENT.describe()","7350c537":"fig,ax = plt.subplots(1,2,figsize=(10,5))\nsns.countplot(data = df , x= \"DEATH_EVENT\" ,palette = \"Set3\" ,ax=ax[0])\nplt.title(\"Death_event\")\ndf.DEATH_EVENT.value_counts().plot.pie(explode =[0.1,0] ,autopct = \"%0.2f%%\" ,shadow = True ,ax = ax[1])\nplt.show()","aa6b6022":"def univarient(data , feature):\n    plt.figure(figsize = (10,10))\n    sns.distplot(data[feature])\n    plt.show()","3a28878c":"feature = [\"age\" , \"creatinine_phosphokinase\" ,\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\",\"time\"]\nfor var in feature:\n    univarient(df,var)","c424755c":"def outlier(data ,feature):\n    plt.figure(figsize = (10,10))\n    sns.boxplot(data[feature])\n    plt.show()\n    ","ca1055ca":"feature = [\"age\" , \"creatinine_phosphokinase\" ,\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\",\"time\"]\nfor var in feature:\n    outlier(df,var)","c5c29d03":"def univarient_cat(data ,feature):\n    plt.figure(figsize = (10,10))\n    sns.countplot(x = feature ,hue =\"DEATH_EVENT\" , data = data , palette = \"rainbow\" )\n    plt.show()","83aaa361":"feature_b = [\"anaemia\"  ,\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\"]\nfor var in feature_b:\n    univarient_cat(df ,var)","18c53690":"def bivarient(data ,feature):\n    plt.figure(figsize = (10,10))\n    sns.boxplot(y = feature ,x =\"DEATH_EVENT\" , data = data , palette = \"rainbow\" )\n    plt.show()","094c92fe":"feature = [\"age\" , \"creatinine_phosphokinase\" ,\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\",\"time\"]\nfor var in feature:\n    bivarient(df,var)","887a0c45":"def bivarient_conti(data , feature):\n    plt.figure(figsize =(10,10))\n    sns.lineplot(x = \"age\" , y = feature , hue = \"DEATH_EVENT\" , data = data)\n    plt.title(\"Relationship between Age and Hue =DEATH_EVENT\")\n    plt.show()","ac840f97":"feature_c = [ \"creatinine_phosphokinase\" ,\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\"]\nfor var in feature_c:\n    bivarient_conti(df,var)","87a51859":"plt.figure(figsize = (10,10))\nplt.title(\"Relation between death based on Sex and Diabetes \")\nsns.catplot(kind = \"count\",x = \"sex\" ,hue = \"diabetes\" ,col = \"DEATH_EVENT\" ,data = df,palette = \"rainbow\")\n","67aca622":"plt.figure(figsize = (10,10))\nplt.title(\"Relation between death based on Sex and High_blood_pressure \")\nx = sns.catplot(kind = \"count\",x = \"sex\" ,hue = \"high_blood_pressure\" ,col = \"DEATH_EVENT\" ,data = df,palette = \"rainbow\")\nx","6aad34bc":"plt.figure(figsize=(10,15))\nsns.heatmap(df.corr() , annot = True ,cmap = \"Blues\" )\nplt.show()","c95d0cbf":"sns.pairplot(df)\nplt.show()","1a401911":"#outlier calculation for Extreme and Nominal\ndef IQR_CAL(data , feature):\n    IQR = data[feature].quantile(0.75) - data[feature].quantile(0.25)\n    E_upper = data[feature].quantile(0.75) + (3 * IQR)\n    E_lower = data[feature].quantile (0.25)- (3 * IQR)\n    N_upper = data[feature].quantile(0.75) + (1.5 * IQR) # apply Nominal outlier\n    N_lower = data[feature].quantile(0.25) + (1.5 * IQR)\n    print(\"Inter Quantile Range  {}:{}\".format(feature,IQR))\n    print(\"Extreme outlier for Upper Boundary  {}:{}\".format(feature,E_upper))\n    print(\"Extreme outlier for Lower Boundary  {}:{}\".format(feature,E_lower))\n    print(\"Nominal outlier for Upper Boundary  {}:{}\".format(feature,N_upper))\n    print(\"Nominal outlier for Lower Boundary  {}:{}\".format(feature,N_lower))\n    \n    ","1b56234d":"outlier_f = [ \"creatinine_phosphokinase\" ,\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\",\"time\"]\nfor var in outlier_f:\n    IQR_CAL(df,var)","49f8b4e6":"#outlier removal\ndf.loc[df[\"serum_sodium\"] < 125 ,  \"serum_sodium\" ] = 125.0\ndf.loc[df[\"serum_creatinine\"] > 2.14 ,\"serum_creatinine\" ] = 2.14\n","cb4cc830":"df.loc[df[\"platelets\"] > 440000 , \"platelets\"] =440000  #nominal outlier\ndf.loc[df[\"ejection_fraction\"] > 67.5 , \"ejection_fraction\"] =  67.5   #nominal outlier\ndf.loc[df[\"creatinine_phosphokinase\"] > 1280.25 , \"creatinine_phosphokinase\"] = 1280.25 #nominal outlier\n","a700acd6":"def outlier_removal_f(data,var):\n    plt.figure(figsize = (10,10))\n    sns.distplot(data[var],color=\"y\")\n    plt.title(var)\n    plt.show()","fbf7e455":"outlier_f = [ \"creatinine_phosphokinase\" ,\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\",\"time\"]\nfor var in outlier_f:\n    outlier_removal_f(df,var)","ebe58a9d":"df.head()","b3da00ff":"x = df.iloc[:,:-1]\ny = df.iloc[:,-1]\n","96aa150c":"y.head()","34c5fbab":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix,classification_report, precision_score,accuracy_score\n","be27077e":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 0)\nprint(\"shape of x_train:{} and x_test:{}\".format(x_train.shape,x_test.shape))\nprint(\"shape of y_train:{} and y_test:{}\".format(y_train.shape,y_test.shape))","8366646b":"x_train_std = StandardScaler().fit_transform(x_train)\nx_test_std = StandardScaler().fit_transform(x_test)\n","ccdeaf42":"lr = LogisticRegression(penalty = \"l2\" , fit_intercept=True,verbose = 2 ,n_jobs = -1)\nlr.fit(x_train,y_train)\npred = lr.predict(x_test)\ncon = confusion_matrix(y_test,pred)\nprint(con)\nplt.title(\"confusion_matrix for Logistic Regression\")\nsns.heatmap(con , annot = True ,cmap = \"Blues\")\nprint(\"accuracy Score for Logistic Regression Before appling Standardisation:{}\".format(accuracy_score(y_test,pred) * 100))","729dd0a6":"lr = LogisticRegression(penalty = \"l2\" , fit_intercept=True)\nlr.fit(x_train_std,y_train)\npred = lr.predict(x_test_std)\ncon = confusion_matrix(y_test,pred)\nprint(con)\nplt.title(\"confusion_matrix for Logistic Regression\")\nsns.heatmap(con , annot = True ,cmap = \"Blues\")\nprint(\"accuracy Score for Logistic Regression after appling Standardisation:{}\".format(accuracy_score(y_test,pred) * 100))","f670de64":"rf = RandomForestClassifier()\nrf.fit(x_train,y_train)\npred = rf.predict(x_test)\ncon = confusion_matrix(y_test,pred)\nprint(con)\nplt.title(\"confusion_matrix for RandomForestClassifier\")\nsns.heatmap(con , annot = True ,cmap = \"Blues\")\nprint(\"accuracy Score for RandomClassifier:{}\".format(accuracy_score(y_test,pred) * 100 ,\"%\"))\nprint(\"Classification_report for Random_forest\")\nprint(classification_report(y_test,pred))","d1985b27":"gb = GradientBoostingClassifier(n_estimators = 50 ,max_depth = 10 ,criterion = \"mse\",random_state = 3)\ngb.fit(x_train,y_train)\npred = rf.predict(x_test)\ncon = confusion_matrix(y_test,pred)\nprint(con)\nplt.title(\"confusion_matrix for GradientBoostinClassifier\")\nsns.heatmap(con , annot = True ,cmap = \"bone\")\nprint(\"accuracy Score for GradientBoosting:{}\".format(accuracy_score(y_test,pred) * 100 ,\"%\"))\nprint(\"Classification_report for GradientBoosting\")\nprint(classification_report(y_test,pred))","1014748b":"Knn = KNeighborsClassifier(n_neighbors=100, weights='uniform')\nKnn.fit(x_train,y_train)\nknnpred = Knn.predict(x_test)\ncon = confusion_matrix(y_test,knnpred)\nprint(con)\nplt.title(\"confusion_matrix for KneighborsClassifier\")\nsns.heatmap(con , annot = True ,cmap = \"YlGnBu\")\nprint(\"accuracy Score for KneighborsClassifier:{}\".format(accuracy_score(y_test,knnpred) * 100 ))\nprint(\"Classification_report for KneighborsClassifier\")\nprint(classification_report(y_test,knnpred))\n","543e35cb":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(x_train,y_train)\nsvcpred = svc.predict(x_test)\ncon = confusion_matrix(y_test,svcpred)\nprint(con)\nplt.title(\"confusion_matrix for Support Vector Classifier\")\nsns.heatmap(con , annot = True ,cmap = \"Pastel2\")\nprint(\"accuracy Score for Support Vector Classifier:{}\".format(accuracy_score(y_test,svcpred) * 100 ))\nprint(\"Classification_report for Support Vector Classifier\")\nprint(classification_report(y_test,svcpred))\n\n","06f56443":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(x_train_std,y_train)\nsvcpred = svc.predict(x_test_std)\ncon = confusion_matrix(y_test,svcpred)\nprint(con)\nplt.title(\"confusion_matrix for Support Vector Classifier\")\nsns.heatmap(con , annot = True ,cmap = \"YlGnBu\")\nprint(\"accuracy Score for Support Vector Classifier Standardized:{}\".format(accuracy_score(y_test,svcpred) * 100 ))\nprint(\"Classification_report for Support Vector Classifier\")\nprint(classification_report(y_test,svcpred))\n\n","690a88cb":"dt = DecisionTreeClassifier(max_depth = 10 ,criterion = \"entropy\",splitter = \"best\")\ndt.fit(x_train,y_train)\ndtpred = dt.predict(x_test)\ncon = confusion_matrix(y_test,dtpred)\nprint(con)\nplt.title(\"confusion_matrix for Decision Tree Classifier\")\nsns.heatmap(con , annot = True ,cmap = \"YlGnBu\")\nprint(\"accuracy Score for Decision Tree Classifier:{}\".format(accuracy_score(y_test,dtpred) * 100 ))\nprint(\"Classification_report for Decision Tree Classifier\")\nprint(classification_report(y_test,dtpred))\n\n","51ed8228":"rf = RandomForestClassifier()\ncr = cross_val_score(rf,x,y,cv = 10)\nprint(\"Cross Value Score Random Forest:{}\".format(cr.mean()))","1dc9dae0":"rf=RandomForestClassifier(n_estimators=300,criterion='entropy',\n                             max_features='sqrt',min_samples_leaf=10,random_state=100)\nrf.fit(x_train,y_train)\nrfpred = rf.predict(x_test)\ncon = confusion_matrix(y_test,rfpred)\nprint(con)\nplt.title(\"confusion_matrix for RandomForest Classifier\")\nsns.heatmap(con , annot = True ,cmap = \"Pastel1\")\nprint(\"accuracy Score for RandomForest Classifier:{}\".format(accuracy_score(y_test,rfpred) * 100 ))\nprint(\"Classification_report for RandomForest Classifier\")\nprint(classification_report(y_test,rfpred))","37eb0511":"from sklearn.model_selection import RandomizedSearchCV\nn_estimators = [int(x) for x in np.linspace(start = 2, stop = 2000, num = 1000)]\nmax_features = ['auto', 'sqrt','log2']\nmax_depth = [int(x) for x in np.linspace(1, 1000,500)]\nmin_samples_split = [2, 5, 10,14]\nmin_samples_leaf = [1, 2, 4,6,8]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n              'criterion':['entropy','gini']}\nprint(random_grid)","805c3c7f":"rf=RandomForestClassifier()\nrf_randomcv=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,\n                               random_state=100,n_jobs=-1)\nrf_randomcv.fit(x_train,y_train)","672dbf88":"rf_randomcv.best_params_","b49ce83d":"rbest = rf_randomcv.best_estimator_\nrbest","823214a0":"rfpred = rbest.predict(x_test)\nconfusion = confusion_matrix(y_test , rfpred)\nprint(confusion)\nprint(\"Accuray Score HyperTuning Random forest:{}\".format(accuracy_score(y_test,rfpred) * 100))\nsns.heatmap(confusion ,annot = True , cmap = \"rainbow\")\nplt.title(\"Confusion_matrix for HyperTuning Random forest\")\nprint(\"Classification Report HyperTuning Random forest:{}\".format(classification_report(y_test , rfpred)))\n","1c751e4c":"1. Logistic Regression\n1. Random Forest Classfier \n1. Gradient Boosting \n1. KNeighbours \n1. SVC \n1. Decision Tree Classifier\n1. Random Forest Classifier Hyper parameter tuning","ee362c9a":"# Manual Hypertuning  Random Forest","272530ac":"# Extreme Outlier\nExtreme outliers are any data values which lie more than 3.0 times the interquartile range below the first quartile or above the third quartile","e6ca7a6c":"# Heart Failure Prediction \nIn this Notebook we discuss various Machine learning model to Classify Heart Failure Prediction","1ef24323":"# X-y Selection","7329c03a":"# Bivarient Analysis","dc50dee2":"# Conclusion\nApplying All Algorithm RandomforestClassifier and Gradient Boosting Perform Good Accuracy nearly 85.00","bab8aa05":"# visuvalize the Target Feature","d028e4ab":"# Gradient Boosting Classifier","2b13e0b2":"# After removing outlier ","bb8190d6":"# importing libraries","245dbf11":"# Discuss Algorithms","66dd459f":"In the above code we will calculate the outlier using IQR with Nominal and Extreme Outlier\n","6f5c0512":"#  Decision Tree Classifier","ca0a4476":"# Apply Standardised Logistic Regression","ce80e5b0":"# Cross_val_score Random Forest","9f1461fc":"# Train_Test_spliting","2907da02":"# Check the Missing value","ebf0ce20":"# Categorical Visualization","af70e270":"## Support Vector Classifier","4e33b9c4":"# Random Forest Classifier","a3dad52b":"# KNearest Neighbors Classifier","55c8040d":"# Random Forest HuperParem Tuning","8386c3eb":"# Describe and visualize independent Variable","a0af4a60":"# play with Hyper Tuning","91ece99a":"# Support Vector Classifier Standardized","a2244bff":"# Import Data","2363aa0e":"# Nominal Outlier\nNominal outliers are any data values which lie more than 1.5 times the interquartile range below the first quartile or above the third quartile","dcf45e7c":"# univarient Analysis and Detect outlier","29d47ba1":"> ","4ec47de7":"# Logistic Regression","17d759a0":"# Data cleaning and visualization","f810c203":"# Correlation about the feature","e88de69b":"#  Apply Standardisation","681e2dde":"# Model Comparision\n1.   Logistic Regression - 76.666\n1.   Logistic Regression(std) - 81.66\n1.   RandomForestClassifier - 85.00\n1.   GradientBoosting -85.00\n1.   Support vector Classifeir - 61.66               \n1.   SVC (std)                 - 80.5\n1.   KNN Classifier            - 61.66\n1.   Decision Tree Classifier  - 83.5\n1.   Random Forest Classifier( Manual hyper parem) - 85.0\n1.   Random Forest Classifier(hyper Parem) - 85.00 \n","762d1c1e":"# Outlier Handling"}}