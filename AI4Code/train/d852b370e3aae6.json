{"cell_type":{"814d6de2":"code","7834c2d1":"code","7288b2ae":"code","5a1c5bf9":"code","dc43ccf3":"code","fa361b8e":"code","ea342124":"code","70578c5d":"code","ef7c5b34":"code","ec55e31b":"code","5af505a0":"code","8c735e6b":"code","381df7db":"code","e0ba387a":"code","f5ed9091":"code","52f582aa":"code","c83f8e50":"code","7688b7e4":"code","f1b242a8":"code","472c10ca":"code","f529e4ba":"code","864fbce8":"code","f2ed91f2":"code","3c20ba75":"code","6619d4e8":"code","9325e0e2":"code","91c38129":"code","395b5b42":"code","93689b0c":"code","35f72ea9":"code","94eb7ecd":"code","a1a25c34":"code","85511432":"code","0f69de3f":"code","a0b4e048":"code","a3952949":"code","563e9620":"code","1550c47c":"code","31666138":"code","109c4a38":"code","96b51fee":"code","29488044":"code","ae3e8524":"code","a10cc7a9":"code","d4bd1c21":"code","f219b5e3":"code","34b5a0bc":"code","1b04f76b":"code","8faaf6ab":"code","32e9d97d":"code","6c883512":"code","9c960384":"code","9307a69c":"code","ea7fe465":"code","9e5dc6b6":"code","5c2d8c62":"markdown","f3282717":"markdown","b4492e42":"markdown","770bcbd4":"markdown","b3e63c70":"markdown","3c7ced79":"markdown","09289f9f":"markdown","2c49c09a":"markdown","451f4b0c":"markdown","54b005de":"markdown","6afe5f90":"markdown","1b451182":"markdown","0dee8305":"markdown","82b55d1b":"markdown","2b8dad68":"markdown","5d4b5b7c":"markdown","dddfe33d":"markdown","829641af":"markdown","cd1cabca":"markdown","45da6178":"markdown","79010b22":"markdown","c65a4b21":"markdown","da56b66d":"markdown","a177eda0":"markdown","e13ebe92":"markdown","a51a94dd":"markdown","406a4d9a":"markdown","13d351a8":"markdown","597885ca":"markdown","ee45c430":"markdown","deb44184":"markdown","36e5eb1f":"markdown","45841b9f":"markdown","44584e7c":"markdown","fdd0feac":"markdown","f3807b9f":"markdown","3cc5aea1":"markdown","32edcd0d":"markdown","852373f5":"markdown","7a5f37fc":"markdown","99aa32e2":"markdown"},"source":{"814d6de2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7834c2d1":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-1\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-1\/test.csv\")\nsample_submission_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-1\/sample_submission.csv\")","7288b2ae":"sample_submission_data.head(5)","5a1c5bf9":"train_data.shape","dc43ccf3":"train_data.head(10)","fa361b8e":"train_data.columns","ea342124":"train_data.dtypes","70578c5d":"train_data.describe()","ef7c5b34":"train_data.info()","ec55e31b":"train_data[\"Start Date\"] = pd.to_datetime(train_data[\"Start Date\"])","5af505a0":"train_data.head(-5)","8c735e6b":"train_data['Age Group'].value_counts()","381df7db":"train_data['Race and Hispanic Origin Group'].value_counts()","e0ba387a":"date_sorted = train_data.copy()\ndate_sorted.sort_values(by=\"Start Date\")","f5ed9091":"feature_subset = [\"Start Date\", \"HHS Region\", \"Race and Hispanic Origin Group\", \"Age Group\", \"Total Deaths\", \"COVID-19 Deaths\", \"Group\", \"MMWR Week\"]\ntraining_subset = train_data[feature_subset]\n\ntraining_subset = training_subset[training_subset[\"HHS Region\"] == \"United States\"]\ntraining_subset = training_subset[training_subset[\"Group\"] == \"By Week\"]\n\ntraining_subset = training_subset.drop(\"HHS Region\", axis=1)\ntraining_subset = training_subset.drop(\"Group\", axis=1)","52f582aa":"training_subset.head()","c83f8e50":"training_subset.info()","7688b7e4":"training_subset.hist(bins=50, figsize=(20, 10), log=True)\nplt.show()","f1b242a8":"race_categories = train_data[\"Race and Hispanic Origin Group\"].value_counts().index\nrace_categories = list(race_categories)\nrace_categories","472c10ca":"age_categories = train_data[\"Age Group\"].value_counts().index\nage_categories = list(age_categories)\nage_categories","f529e4ba":"race = \"Race and Hispanic Origin Group\"\nage = \"Age Group\"\ncovid_deaths = \"COVID-19 Deaths\"","864fbce8":"training_subset.plot(\"Start Date\", covid_deaths, figsize=(18, 13))\ntraining_subset.plot(\"Start Date\", \"Total Deaths\", figsize=(18, 13))\n\nplt.show()","f2ed91f2":"dates = pd.unique(training_subset[\"Start Date\"])\ni = 0\nfor date in dates:\n    datetime = pd.to_datetime(date)\n    training_subset.loc[training_subset[\"Start Date\"] == date, \"Week\"] = i\n    i += 1    \n    \ntraining_subset.head()","3c20ba75":"correlation_matrix = pd.get_dummies(training_subset).corr()\ncorrelation_matrix[covid_deaths].sort_values(ascending=False)","6619d4e8":"training_subset.describe()","9325e0e2":"def upperLowerOutliers(data_set, feature):\n        q1 = np.nanpercentile(data_set[feature], 25)\n        q3 = np.nanpercentile(data_set[feature], 50)\n        IQR = q3 - q1\n        upper = q3 + 1.5 * IQR \n        lower = q1 + 1.5 * IQR\n        \n        return upper, lower\n    \ndef getPrintOutlierBounds(data_set):\n    total_upper, total_lower = upperLowerOutliers(data_set, \"Total Deaths\")\n    covid_upper, covid_lower = upperLowerOutliers(data_set, covid_deaths)\n\n    print(\"Total Deaths upper: \", total_upper, \" lower: \", total_lower)\n    print(\"Covid Deaths upper: \", covid_upper, \" lower: \", covid_lower)","91c38129":"getPrintOutlierBounds(training_subset)","395b5b42":"test_data.head(-10)","93689b0c":"test_data.info()","35f72ea9":"test_data[\"Group\"].value_counts()","94eb7ecd":"test_data[\"HHS Region\"].value_counts()","a1a25c34":"test_data[\"Race and Hispanic Origin Group\"].value_counts()","85511432":"test_data[\"Age Group\"].value_counts()","0f69de3f":"test_data.describe()","a0b4e048":"test_data.shape","a3952949":"X = training_subset\nX = X.drop(covid_deaths, axis=1)\nX = X.drop(\"Start Date\", axis=1)\nX = X.drop(\"MMWR Week\", axis=1)\nX = pd.get_dummies(X)\n\ny = training_subset[covid_deaths]","563e9620":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nlin_reg = LinearRegression()\nridge = RidgeCV(scoring = \"neg_mean_squared_error\", cv=5)\nlasso = Lasso(max_iter=10000)\nelastic = ElasticNet()\n\nmodels = [lin_reg, ridge, lasso, elastic]","1550c47c":"def getCrossValScores(models, X_train, y_train, cv):\n    model_scores = []\n\n    for model in models:\n        model_score = cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=cv)\n        cross_scores = -model_score\n        cross_scores = cross_scores ** (0.5)\n\n        model_scores.append(cross_scores)\n    \n    return model_scores","31666138":"def predictAndScore(model, X_train, X_test, y_train, y_test):\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    MSE = mean_squared_error(y_test, y_pred)\n    RMSE = MSE ** (0.5)\n        \n    return RMSE","109c4a38":"from sklearn.model_selection import TimeSeriesSplit\n\ndef timeSeriesValidation(models, X, y, start_split, stop_split):\n    final_scores = []\n    lin_reg = []\n    ridge = []\n    lasso = []\n    elastic = []\n    \n    for split in range(start_split, stop_split):\n        \n        tscv = TimeSeriesSplit(n_splits = split)\n        \n        for train_index, test_index in tscv.split(X):\n            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n            lin_reg.append(predictAndScore(models[0], X_train, X_test, y_train, y_test))\n            ridge.append(predictAndScore(models[1], X_train, X_test, y_train, y_test))\n            lasso.append(predictAndScore(models[2], X_train, X_test, y_train, y_test))\n            elastic.append(predictAndScore(models[3], X_train, X_test, y_train, y_test))\n                \n    final_scores.append(lin_reg)\n    final_scores.append(ridge)\n    final_scores.append(lasso)\n    final_scores.append(elastic)\n    \n    return final_scores","96b51fee":"def printScores(scores):\n    print(\"lin_Reg mean  \", np.mean(scores[0]))\n    print(\"ridge mean    \", np.mean(scores[1]))\n    print(\"lasso mean    \", np.mean(scores[2]))\n    print(\"elastic mean  \", np.mean(scores[3]))","29488044":"full_data_scores = timeSeriesValidation(models, X, y, 5, 10)\nprintScores(full_data_scores)","ae3e8524":"plt.hist(full_data_scores, bins=70)\nplt.show()","a10cc7a9":"linreg_scores_df = pd.DataFrame(full_data_scores[0])\nlinreg_scores_df.describe()","d4bd1c21":"ridge_scores_df = pd.DataFrame(full_data_scores[1])\nridge_scores_df.describe()","f219b5e3":"lasso_scores_df = pd.DataFrame(full_data_scores[2])\nlasso_scores_df.describe()","34b5a0bc":"elastic_scores_df = pd.DataFrame(full_data_scores[3])\nelastic_scores_df.describe()","1b04f76b":"training_subset.head()","8faaf6ab":"# for i_race in race_categories:\n#     for j_age in age_categories:\n#         mask = (training_subset[\"Race and Hispanic Origin Group\"] == i_race) & (training_subset[\"Age Group\"] == j_age)\n\n#         indicies = training_subset.index[mask]\n        \n#         subset = training_subset[mask]\n#         diff_deaths = subset[covid_deaths].diff()\n#         subset.loc[:, \"diff_deaths\"] = diff_deaths\n#         subset.fillna(0)\n\n#         subset_diff = subset[\"diff_deaths\"]\n#         subset_diff = list(subset_diff)\n        \n#         for index in indicies:\n#             i = 0\n#             for index in indicies:\n#                 training_subset.loc[index, \"diff_deaths\"] = subset_diff[i] \n#                 i += 1","32e9d97d":"# training_subset = training_subset.fillna(0)\n# training_subset.head(-10)","6c883512":"# training_subset.plot(\"Start Date\", \"diff_deaths\", figsize=(18, 13))","9c960384":"dates = pd.unique(test_data[\"Start Date\"])\ni = 0\nfor date in dates:\n    datetime = pd.to_datetime(date)\n    test_data.loc[test_data[\"Start Date\"] == date, \"Week\"] = i\n#     test_data.loc[test_data[\"Start Date\"] == date, \"Month\"] = datetime.month\n    i += 1 \n    \n    \ntest_data.head()","9307a69c":"# for i_race in race_categories:\n#     for j_age in age_categories:\n#         mask = (test_data[\"Race and Hispanic Origin Group\"] == i_race) & (test_data[\"Age Group\"] == j_age)\n\n#         indicies = test_data.index[mask]\n        \n#         subset = test_data[mask]\n#         diff_deaths = subset[covid_deaths].diff()\n#         subset.loc[:, \"diff_deaths\"] = diff_deaths\n#         subset.fillna(0)\n\n#         subset_diff = subset[\"diff_deaths\"]\n#         subset_diff = list(subset_diff)\n        \n#         for index in indicies:\n#             i = 0\n#             for index in indicies:\n#                 test_data.loc[index, \"diff_deaths\"] = subset_diff[i] \n#                 i += 1","ea7fe465":"predict_test_data = test_data[[\"Week\", \"Race and Hispanic Origin Group\", \"Age Group\", \"Total Deaths\"]]\npredict_test_data = pd.get_dummies(predict_test_data)\npredict_test_data.head()\npredict_test_data.shape","9e5dc6b6":"predictions = lasso.predict(predict_test_data)\noutput = pd.DataFrame({'id': test_data.id, 'COVID-19 Deaths': predictions})\noutput.to_csv(\"submission.csv\", index=False)\nprint(output.to_string())","5c2d8c62":"# Handling Missing Data\n\nLooks like there aren't any missing values to actually deal with with the reduced subset! Lucky me!\n\nRegardless, now we can start building the models and evaluating them on this form of the dataset\n\n# Building the Models\n\nCreating the train test split:","f3282717":"## Plotting Time-Series Data","b4492e42":"### From the Future to the Past (Returning to this segment to perform outlier detection)\n\nI'd like to take a quick look at the data for COVID-19 Deaths and Total Deaths to determine the location of outliers (if any). \n\n# A Detour: Outlier Detection\n\nSince Total Deaths is the closest thing to a numerical metric off which we can base our model fitting, removing it of outliers will surely improve model performance. This also follows basic EDA.\n\nLets start by using .describe() and finding the median.","770bcbd4":"Looks like we dont have any missing values with this subset!\n\nTaking a look at the histogram plots","b3e63c70":"Importing and creating baseline models","3c7ced79":"Looking at the distributions and the preliminary submission scores so far, I am going to pick Lasso.\n\nRandomizedSearchCV was not implemented as I had hoped it would be, so I have elected to remove it from this notebook.","09289f9f":"Function to retrieve predcions and scores quickly","2c49c09a":"Printing the head of this new \"finalized\" subset:","451f4b0c":"# Reading in Data Drom CSV File","54b005de":"Linear Regression analysis","6afe5f90":"Elastic Net Regression analysis","1b451182":"# Identifying Missing Data\n\n### Converting Date-related Objects to DateTime Objects\n\nAnd... in order to extract any amount of useful data from the date, i'll need to convert them to datetime objects.\n\nThere is missing data from: \n* Month\n* MMWR Week\n* Week-Ending Date\n* COVID-19 Deaths (shame on this one, since its the target variable)\n* Total Deaths (another shame, probably will correlate with COVID deaths)\n* and Footnote (whatever that does)\n\n\nI am only going to convert Start Date, as it is the only useful one out of the 4 date related features.","0dee8305":"Ridge Regression analysis","82b55d1b":"For some reason, this version does not reflect the final score I had gotten on the leaderboard, which is weird. However, the notebook does represent where I roughly \"finished\".","2b8dad68":"This should be a correlation of all features I am considering for training the models. Although some of them clearly do not have much impact at all on the outcome of covid deaths, working with them all should be worthwhile.\n\nThis list is also subject to change as future feature engineering will be needed in order to further optimize the models. \nPossible feature combinations include:\n* Pairing Region with each Race independently\n* Pairing Region with each Age independently\n* Pairing Race with each Age independently\n    \n  \nFor now though, I am going to transform the \"Start Date\" values into integers","5d4b5b7c":"Function to print scores concisely","dddfe33d":"This doesnt work since the training data doesnt have covid deaths in it. I later learned that differencing is actually more for seeing if there is seasonality in some sort of way. It will be left in, but commented out.","829641af":"Function to quickly retrieve cross validation scores","cd1cabca":"Looks like there are a good number of values to consider for sure!","45da6178":"Checking for null values on this set:","79010b22":"Well isnt all this interesting? It makes me wonder what the distributions would be like if we culled the data before covid actually hit. Lets do some quick math on the data set as is, and then see what it looks like if we remove a month or two from the front of the dataset","c65a4b21":"Lots of objects, specifically date related information.","da56b66d":"Well at least those values both correspond with each other. No discrepency there.\n\n\nThe data is organized as such:\n* By Group\n* ----Region\n* --------Race\n* ------------Age Group\n                \n","a177eda0":"\n\nJust confirming my suspicions here. It seems like we are covering a years worth of time, but some columns are broken into further columns, *and* something tells me, there are going to be very specific tells as to who is more likely to get COVID\/die from it than others such as: age group, race, and even region","e13ebe92":"# _None of this SH!T is Working_\n## Couldn't Actually Get this Working Before The Deadline, but Will Keep it in Regardless\n\nSo here is what I am going to do. The data is roughly as prepared as it can be, as far as raw data goes. I need to manipulate it further though in order to removed most of its seasonal components, in order to fit a _linear_ model to it. The problem with seasonal data is that a single line can't be used to accurately predict the data, so it struggles.\n\nI believe this is one of the big hints that Dr. H was trying to get across, if not the biggest hint of our last lecture. Seasonality is the bane of this assignment, so how do we deal with that?\nFrom what I am reading, I need to strip the data of its seasonality by using _differencing_ so the data is \"stationary\", this will provide the foundation for the model to accurately predict covid deaths over time. I dont currently know how that will translate though to the test data and making predictions, but I will figure that out.\n\n## Differencing (Removing Seasonality)\n\nStarting with going back to my original training_subset, just to get a fresh start. It already has weeks created and filled out. Weeks is important as it gives the model an integer to actually base weeks off of, since models struggle with datetime objects.\n\nWhat I hadn't considered about this new Weeks category, is that it provides a fantastic base for differencing the data (\"stabalizing\" or \"removing seasonality\") in order to make it stationary. _AND_ to top it off we have categories that further break it down. So we can adjust it based not only on the total covid deaths, but we can actually difference it based on race and age as well! WOO! It's pretty fuckin' cool if you ask me.","a51a94dd":"#### Exploring the Trio-Features (Region, Race, Age)","406a4d9a":"Lasso Regression analysis","13d351a8":"So it looks like there are outliers by this detection method. I don't think removing them will be too impactful on the final results of the model though. So I am going to leave them as be.\n\n## Taking a Quick Look at the Test Data\n\nJust to see how the test data looks, so I know how much I will have to work on\/select to better suit the test data, I am going to take a really simple preliminary look at its layout","597885ca":"Creating these for future use","ee45c430":"We have 9 age groups","deb44184":"# Prepping Test Data and Submitting","36e5eb1f":"Looking at the correlation matrix","45841b9f":"### Exploring the 4 Features \n\nAt this point, I have a good grasp of what is actually needed to solve the problem (also from looking ahead at the test data). So I am going to quickly make a more finalized training subset to work off of.\n\nThis subset will be based off the \"By Week\" and \"United States\" categories.","44584e7c":"Taking a look at how these catagories interact may be enlightening as well. It is worth noting that these *are* some of our most important features to work with, or rather really our only features to work with.\n\nCreating quick variables for ease of access later","fdd0feac":"So, target is number of COVID-19 Deaths. This is a regression problem.\n\n## Training Data Exploration\n\n### Super Starting Exploration\n\nTest data and training data *shoud* follow the same format roughly. There probably will be some things missing in the test data, but that will be dealt with\/explored later.\n\nBasic summary stats follow:","f3807b9f":"# Basic Exploratory Data Analysis\n\nStarting with the sample_submission, just to figure the target variable is, as well as the submission format (will be important later)","3cc5aea1":"Kyle Crisp","32edcd0d":"And 8 different race\/hispanic origin groups","852373f5":"# Understanding How the Data Is Broken Down\n\nOk, lets a get a good idea of how each columns category actually breaks down. Something tells me that while there definitely is a lot of data, it isnt actually as much as far as time-span goes. More so a lot of dense data in a condensed time period","7a5f37fc":"A lot of data entries, which is nice","99aa32e2":"Distribution of scores via hist-plot"}}