{"cell_type":{"99145fe5":"code","e7a87f89":"code","91384278":"code","163e181e":"code","82c761ad":"code","95fccb0f":"code","8791d35d":"code","783dd944":"code","5c2667d2":"code","09a52518":"code","65754eaa":"code","71c75425":"code","b18a5ab2":"code","f3e5f4b4":"code","c1f0e5e6":"code","660a98f3":"code","3c8bd71e":"code","cdd7872c":"code","e9ae3356":"code","d753c37b":"code","21f9cfac":"code","0fa73abe":"code","59d5f676":"code","63a3e2dc":"code","39fd07c4":"code","0cd74876":"code","8a590f2a":"code","858d07eb":"code","7567fe25":"code","8dcb0e1b":"code","440a280b":"code","b797bad3":"code","117a570c":"code","93fa8536":"code","a5a89f71":"code","653c9334":"code","c86a98d7":"code","34b76652":"code","4e7a063d":"code","3db2adad":"code","2c073336":"code","7e21dc64":"code","68dc2b9f":"code","daf7cac0":"code","e2908df6":"code","39ae2784":"code","6e2632cc":"code","2524b256":"code","51640868":"code","b3c5f583":"code","43232885":"code","6023d4ff":"code","90fa2dd9":"code","e4f72168":"code","9c75867d":"code","b0bdd442":"code","ef275c9e":"code","03d3a6ff":"code","e544b9f7":"code","91f805f4":"code","762291ca":"code","b989ffa1":"code","5b3c758e":"code","9e0e2576":"code","305a64cc":"code","2ec2e0b3":"code","cc034c17":"markdown","c0df6c06":"markdown","d88c5e16":"markdown","99cb34a3":"markdown","686e316a":"markdown","d44e2d3d":"markdown","1189eb8a":"markdown","7feb6e97":"markdown","e8a0d326":"markdown","aecfed61":"markdown","7626f46b":"markdown","571cc7b1":"markdown","446f5b09":"markdown","a47126df":"markdown","ff143ea5":"markdown","9df8175e":"markdown","d06200e5":"markdown","eb972250":"markdown","5ec76803":"markdown","e402ef63":"markdown","bf425286":"markdown","e8c6a86f":"markdown","a62b1590":"markdown","6c3963c0":"markdown","599d0a02":"markdown","f9f6f135":"markdown"},"source":{"99145fe5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e7a87f89":"# import libraries \nimport pandas as pd # Import Pandas for data manipulation using dataframes\nimport numpy as np # Import Numpy for data statistical analysis \nimport matplotlib.pyplot as plt # Import matplotlib for data visualisation\nimport seaborn as sns # Statistical data visualization\n# %matplotlib inline","91384278":"# Import Cancer data drom the Sklearn library\n\nfrom sklearn.datasets import load_breast_cancer\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_breast_cancer.html\n# https:\/\/scikit-learn.org\/0.16\/datasets\/index.html\n# https:\/\/scikit-learn.org\/stable\/datasets\/toy_dataset.html#breast-cancer-wisconsin-diagnostic-dataset\n# https:\/\/scikit-learn.org\/stable\/datasets.html#:~:text=The%20sklearn.,from%20the%20'real%20world'.\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_breast_cancer.html#examples-using-sklearn-datasets-load-breast-cancer\n\ncancer = load_breast_cancer()","163e181e":"cancer\n# cancer.items()","82c761ad":"# What dictionaries we have\ncancer.keys()","95fccb0f":"cancer.values()","8791d35d":"# print them one by one\nprint(cancer['DESCR']) #descriptions of the dataset\n","783dd944":"print(cancer['target'])","5c2667d2":"print(cancer['target_names'])","09a52518":"print(cancer['feature_names'])","65754eaa":"len(cancer['feature_names'])","71c75425":"print(cancer['data'])","b18a5ab2":"cancer['data'].shape","f3e5f4b4":"df_cancer = pd.DataFrame(np.c_[cancer['data'], cancer['target']], columns = np.append(cancer['feature_names'], ['target']))\ndf_cancer.head()","c1f0e5e6":"df_cancer.tail(5)","660a98f3":"sns.pairplot(df_cancer,vars= ['mean radius','mean texture', 'mean area', 'mean perimeter', 'mean smoothness'])","3c8bd71e":"sns.pairplot(df_cancer,hue = 'target', vars= ['mean radius','mean texture', 'mean area', 'mean perimeter', 'mean smoothness'])","cdd7872c":"sns.countplot(df_cancer['target'])","e9ae3356":"sns.scatterplot(x='mean area', y='mean smoothness', hue='target', data=df_cancer)","d753c37b":"df_cancer.corr()","21f9cfac":"sns.heatmap(df_cancer.corr())","0fa73abe":"plt.figure(figsize=(20,20))\nsns.heatmap(df_cancer.corr(), annot=True)","59d5f676":"# Let's drop the target label coloumns\nx = df_cancer.drop(['target'],axis=1)\nx","63a3e2dc":"x.shape","39fd07c4":"y = df_cancer['target']\ny","0cd74876":"y.shape","8a590f2a":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state= 5)\n\nx_train","858d07eb":"print(len(x_train))\nprint(len(x_test))","7567fe25":"print(len(x_train)\/len(x)) # so 80% Training set and 20% test set","8dcb0e1b":"x_train.shape","440a280b":"x_test","b797bad3":"x_test.shape","117a570c":"y_train","93fa8536":"y_train.shape","a5a89f71":"y_test","653c9334":"y_test.shape","c86a98d7":"from sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix","34b76652":"svc_model = SVC()","4e7a063d":"svc_model.fit(x_train, y_train)","3db2adad":"y_predict = svc_model.predict(x_test)\ny_predict","2c073336":"cm = confusion_matrix(y_test, y_predict)","7e21dc64":"sns.heatmap(cm, annot=True)","68dc2b9f":"print(classification_report(y_test, y_predict))","daf7cac0":"x_train","e2908df6":"x_train.min","39ae2784":"min_train = x_train.min()\nmin_train","6e2632cc":"range_train = (x_train - min_train).max()\nrange_train","2524b256":"x_train_scaled = (x_train - min_train)\/range_train\nx_train_scaled","51640868":"sns.scatterplot(x = x_train['mean area'], y= x_train['mean smoothness'], hue= y_train)","b3c5f583":"sns.scatterplot(x= x_train_scaled['mean area'], y= x_train_scaled['mean smoothness'], hue= y_train)","43232885":"min_test = x_test.min()\nrange_test = (x_test - min_test).max()\nx_test_scaled = (x_test - min_test)\/ range_test","6023d4ff":"svc_model.fit(x_train_scaled, y_train)","90fa2dd9":"y_predict = svc_model.predict(x_test_scaled)","e4f72168":"cm = confusion_matrix(y_test, y_predict)\nsns.heatmap(cm, annot=True, fmt = 'd')","9c75867d":"print(classification_report(y_test, y_predict))","b0bdd442":"param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']} \nparam_grid\n# print(type(param_grid))","ef275c9e":"# Example of GridSearchCV\n\n# x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=50)\n# xgb=XGBClassifier()\n# ----------------------------------------------------------------------\n# from sklearn.model_selection import GridSearchCV\n# parameters=[{'learning_rate':[0.1,0.2,0.3,0.4],'max_depth':[3,4,5,6,7,8],'colsample_bytree':[0.5,0.6,0.7,0.8,0.9]}]\n            \n# gscv=GridSearchCV(xgb,parameters,scoring='accuracy',n_jobs=-1,cv=10)\n# grid_search=gscv.fit(x,y)\n# grid_search.best_params_\n# -----------------------------------------------------------------------\n# x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=50)\n# xgb=XGBClassifier(colsample_bytree=0.8, learning_rate=0.4, max_depth=4)\n# xgb.fit(x,y)\n# pred=xgb.predict(x_test)\n# print('Accuracy=  ',accuracy_score(y_test,pred))\n# -----------------------------------------------------------------------\n# #Cross validating (for classification) the model and checking the cross_val_score,model giving highest score will be choosen as final model.\n# from sklearn.model_selection import cross_val_predict\n# xgb=XGBClassifier(colsample_bytree=0.8, learning_rate=0.4, max_depth=4)\n# cvs=cross_val_score(xgb,x,y,scoring='accuracy',cv=10)\n# print('cross_val_scores=  ',cvs.mean())\n# y_pred=cross_val_predict(xgb,x,y,cv=10)\n# conf_mat=confusion_matrix(y_pred,y)\n# conf_mat\n# ---------------------------------------------------------------------------\n# #Cross validating(for regression) the model and checking the cross_val_score,model giving highest score will be choosen as final model.\n# gbm=GradientBoostingRegressor(max_depth=7,min_samples_leaf=1,n_estimators=100)\n# cvs=cross_val_score(xgb,x,y,scoring='r2',cv=5)\n# print('cross_val_scores=  ',cvs.mean())\n# -------------------------------------------------------------------------------\n# #parameters\n# #xgboost:-\n# parameters=[{'learning_rate':[0.1,0.2,0.3,0.4],'max_depth':[3,4,5,6,7,8],'colsample_bytree':[0.5,0.6,0.7,0.8,0.9]}]\n# #random forest\n# parameters=[{'max_depth':[5,7,9,10],'min_samples_leaf':[1,2],'n_estimators':[100,250,500]}]\n# #gradientboost\n# parameters=[{'max_depth':[5,7,9,10],'min_samples_leaf':[1,2],'n_estimators':[100,250,500]}]\n# #kneighbors\n# parameters={'n_neighbors':[5,6,8,10,12,14,15]}\n# #logistic regression\n# parameters={'penalty':['l1','l2'],'C':[1,2,3,4,5]}\n# #gaussiannb\n# parameters={'var_smoothing': np.logspace(0,-9, num=100)}\n# #SVC\n# parameters=[{'C':[0.1,0.5,1,2,3],'kernel':['rbf','poly']}]\n# #adaboost\n# parameters=[{'base_estimator':[lr],'learning_rate':[1,0.1,0.001],'n_estimators':[100,150,250]}]\n# #decesion tree\n# parameters=[{'criterion':['gini','entropy'],'max_depth':[5,7,9,10],'min_samples_leaf':[1,2]}]\n","03d3a6ff":"from sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(SVC(), param_grid, refit= True, verbose= 4)","e544b9f7":"param_grid","91f805f4":"grid.fit(x_train_scaled, y_train)","762291ca":"grid.best_params_","b989ffa1":"grid.best_estimator_","5b3c758e":"grid_prediction = grid.predict(x_test_scaled)\ngrid_prediction","9e0e2576":"cm = confusion_matrix(y_test, grid_prediction)\ncm","305a64cc":"sns.heatmap(cm, annot=True)","2ec2e0b3":"print(classification_report(y_test,grid_prediction ))","cc034c17":"**We take one of these slide graphs and see how can we play.**","c0df6c06":"## **PROBLEM IN MACHINE LEARNING VOCABULARY**\n\n*Input:* 30 features \n* Radius\n* Texture\n* Perimeter\n* Area\n* Smoothness\n* ...\n\n*Target Class:* 2 \n* Malignant\n* Benign\n\n*How many datasets we have? :* \n* Number of Instances: 569\n* Class Distribution: 212 Malignant, 357 Benign\n\n*Data source:*\n* [Breast Cancer Wisconsin(Diagnostic)](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+(Diagnostic)\n* [Breast Cancer Detection with Reduced Feature Set](https:\/\/www.researchgate.net\/publication\/271907638_Breast_Cancer_Detection_with_Reduced_Feature_Set)\n\n\nWe're going to say: if you look at all these features then indicate that cancer which is denoted by a zero, is malignant in this case.\n\nAnd then, If we look at the 30 features that may be classified as one, this cancer type is kind of benign.\n\nSo it's a kind of binary detection indicating zero or one for malignant or benign.\n\n---\n**SUPPORT VECTOR MACHINE CLASSIFIER**\n\nNear the maximum Margin Hyperplane, we don't know whether this cancer is malignant or benign. \n\nThat's why the support vector machine classifier is very unique in this sense. It simply uses the points or the support vectors that are on the boundary to draw the boundary out to classify the classes.\n\nSupport vector machines are really powerful techniques.\nWhy? Because it's kind of an extreme algorithm.\nIt just focuses on supporting the support vectors or the points on the boundary and separating them somehow.\n","d88c5e16":"**The objective of a Linear SVC (Support Vector Classifier) is to fit to the data you provide, returning a \"best fit\" hyperplane that divides, or categorizes, your data. From there, after getting the hyperplane, you can then feed some features to your classifier to see what the \"predicted\" class is.**\n\nhttps:\/\/pythonprogramming.net\/linear-svc-example-scikit-learn-svm-python\/#:~:text=The%20objective%20of%20a%20Linear,the%20%22predicted%22%20class%20is.\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html","99cb34a3":"****EVALUATING THE MODEL****\n\nWe're talking about the testing data which has data that has never seen before. ","686e316a":"**VISUALIZING THE DATA**","d44e2d3d":"* **A Classification report is used to measure the quality of predictions from a classification algorithm. ... The report shows the main classification metrics precision, recall and f1-score on a per-class basis. The metrics are calculated by using true and false positives, true and false negatives.**\n\nhttps:\/\/muthu.co\/understanding-the-classification-report-in-sklearn\/#:~:text=A%20Classification%20report%20is%20used,predictions%20from%20a%20classification%20algorithm.&text=The%20report%20shows%20the%20main,positives%2C%20true%20and%20false%20negatives.\n\nhttps:\/\/medium.com\/@kohlishivam5522\/understanding-a-classification-report-for-your-machine-learning-model-88815e2ce397\n\n","1189eb8a":"**IMPORTING DATA**","7feb6e97":"https:\/\/medium.com\/@szabo.bibor\/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e\n\nHow to Create a Seaborn Correlation Heatmap in Python?","e8a0d326":"# **IMPROVING THE MODEL**","aecfed61":"**We're going to plot a confusion matrix.  We need to specify compare our true value versus the predicted that.**","7626f46b":"## **BREAST CANCER CLASSIFICATION**\n### **Via SuperDataScience Team**\n\n*   Breast cancer is the most common cancer among women worldwide accounting for 25% of all cancer cases and infected 2.1 million people in 2015.\n*   Early diagnosis significantly increase the chances of survival. \n*   The key challenge in cancer detection is how to classify tumors into *malignant* or *benign* ; therefore, machine learning techniques can dramatically improve the accuracy of diagnosis.\n*   Research indicates that most experienced physicians can diagnose cancer with maximum 79% accuracy \n\n\n**First stage**: Any process which is simply extracting some of the cells out of the tumor\n\nWhen we say benign that means the tumor is kind of not spreading across the body so the patient is safe; somehow,  if it's malignant that means it's a cancerous.\nThat means we need to intervene and actually stopping cancer growth. \n\n---\n\n\n\nWhat we do here in the machine learning aspect. \n* We execute all these images and \n* We wanted to specify if that cancer out of these images is malignant or benign.\n\nSo what we do with that, we extract out of these images some features. When we see features that mean some characteristics out of the image such as \n* radius\n* cells\n* texture\n* perimeter\n* area\n* smoothness\n\nWe feed all these features into kind of our machine learning model.\n\n**MAIN PART:**  We want to teach the machine how to basically classify images or classify data and tell us if it's malignant or benign.\n","571cc7b1":"SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n\n    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n    \n    probability=False, random_state=None, shrinking=True, tol=0.001,\n    verbose=False)","446f5b09":"**What is GridSearchCV?**\n\nGridSearchCV is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters.\n\nhttps:\/\/towardsdatascience.com\/grid-search-for-hyperparameter-tuning-9f63945e8fec#:~:text=What%20is%20GridSearchCV%3F,parameters%20from%20the%20listed%20hyperparameters.\n\nhttps:\/\/medium.datadriveninvestor.com\/an-introduction-to-grid-search-ff57adcc0998","a47126df":"**Pairplot is a module of seaborn library which provides a high-level interface for drawing attractive and informative statistical graphics.**\n\nhttps:\/\/medium.com\/analytics-vidhya\/pairplot-visualization-16325cd725e6#:~:text=Pairplot%20is%20a%20module%20of,attractive%20and%20informative%20statistical%20graphics.\n\nhttps:\/\/towardsdatascience.com\/visualizing-data-with-pair-plots-in-python-f228cf529166","ff143ea5":"**There are four ways to check if the predictions are right or wrong:**\n\n**TN \/ True Negative**: the case was negative and predicted negative\n\n**TP \/ True Positive**: the case was positive and predicted positive\n\n**FN \/ False Negative**: the case was positive but predicted negative\n\n**FP \/ False Positive**: the case was negative but predicted positive\n\n\n**Precision \u2014 What percent of your predictions were correct?**\nPrecision is the ability of a classifier not to label an instance positive that is actually negative. For each class, it is defined as the ratio of true positives to the sum of a true positive and false positive.\n\n**Precision:- Accuracy of positive predictions.\nPrecision = TP\/(TP + FP)**\n\n**Recall \u2014 What percent of the positive cases did you catch?**\nRecall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives.\n\n**Recall:- Fraction of positives that were correctly identified.\nRecall = TP\/(TP+FN)**\n\n**F1 score \u2014 What percent of positive predictions were correct?**\nThe F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. F1 scores are lower than accuracy measures as they embed precision and recall into their computation. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.\n\n**F1 Score = 2*(Recall * Precision) \/ (Recall + Precision)**\n\n**Support**\nSupport is the number of actual occurrences of the class in the specified dataset. Imbalanced support in the training data may indicate structural weaknesses in the reported scores of the classifier and could indicate the need for stratified sampling or rebalancing. Support doesn\u2019t change between models but instead diagnoses the evaluation process.","9df8175e":"**Now we are going to divide our data into Train and Test set**\n\ntherefore, we aretraining on the Train set and checking the validation of the chosen model on test set (here we omit the idea of Validation set which are used before test set since the data is very small compared to Big data)","d06200e5":"# **MODEL TRAINING (FINDING A PROBLEM SOLUTION)**","eb972250":"https:\/\/datascience.stackexchange.com\/questions\/21877\/how-to-use-the-output-of-gridsearch\n\nThe .best_estimator_ attribute is an instance of the specified model type, which has the 'best' combination of given parameters from the param_grid. Whether or not this instance is useful depends on whether the refit parameter is set to True (it is by default).","5ec76803":"**Let's check the correlation between the variables**","e402ef63":"# **IMPROVING THE MODEL - PART 2**","bf425286":"**But the only problem is that doesn't show the target class. It doesn't show actual which one of these samples is malignant or which one of them is benign.**","e8c6a86f":"# **CONCLUSION**\n* Machine Learning techniques (SVM) was able to classify tumors into Malignant \/ Benign with 97% accuracy.\n* The technique can rapidly evaluate breast masses and classify them in an automated fashion. \n* Early breast cancer can dramatically save lives especially in the developing world\n* The technique can be further improved by combining Computer Vision\/ ML techniques to directly classify cancer using tissue images.","a62b1590":"Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation.\n\n\nhttps:\/\/machinelearningmastery.com\/k-fold-cross-validation\/\n\nhttps:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics)\n\nhttps:\/\/towardsdatascience.com\/why-and-how-to-cross-validate-a-model-d6424b45261f","6c3963c0":"The blue points in here that's the malignant case. The orange points in here that's the benign case.","599d0a02":"What is grid search used for?\n\nGrid-search is used to find the **optimal hyperparameters of a model** which results in the most 'accurate' predictions\n\nhttps:\/\/towardsdatascience.com\/grid-search-for-model-tuning-3319b259367e\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html","f9f6f135":"numpy.c_ = <numpy.lib.index_tricks.CClass object>\nTranslates slice objects to concatenation along the second axis.\n\nhttps:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.c_.html#:~:text=c_-,numpy.,because%20of%20its%20common%20occurrence."}}