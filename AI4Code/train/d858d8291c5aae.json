{"cell_type":{"a50a406d":"code","4ffb1006":"code","2ef99d5f":"code","51f0ef36":"code","455daf4e":"code","31466f69":"code","c98047b2":"code","3ff5edf8":"code","a3b53c49":"code","ccc2d377":"code","db729b9d":"code","a7353e91":"code","94bb8e52":"code","5cf54112":"code","96ba2024":"code","c4b4954e":"code","90211d74":"code","5a2bcede":"code","426cb6b8":"code","067a441e":"code","fedddde7":"code","80e502c9":"code","12431abf":"code","701fe2f2":"code","307e35fb":"code","84f2c5f0":"markdown","785f5883":"markdown","9e2cd50e":"markdown","7505fe70":"markdown","05ac430c":"markdown","921e17e8":"markdown","8f2db5d7":"markdown","acd79774":"markdown"},"source":{"a50a406d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n!pip install torchsummary\nfrom torchsummary import summary\n\nimport os\nimport math\nimport copy\nimport time\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torchvision import models\nfrom sklearn import preprocessing\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4ffb1006":"# Download google fonts for chinese characters in plots\n!wget \"https:\/\/fonts.google.com\/download?family=Noto%20Sans%20SC\" -O custom_font.zip\n\n# Unzip downloaded zip file\n!unzip custom_font.zip\n\n# Remove downloaded zip file\n!rm custom_font.zip","2ef99d5f":"import matplotlib as mpl\nimport matplotlib.font_manager as fm\n\nfonts = fm.findSystemFonts(fontpaths=\"\/kaggle\/working\")\nfor font in fonts:\n    fm.fontManager.addfont(font)\n\nfont_name = fm.FontProperties(fname=fonts[0]).get_name()\nmpl.rc('font', family=font_name)\nprint('Applied new font family: ', plt.rcParams['font.family'])","51f0ef36":"FILE_PATH = '\/kaggle\/input\/chinese-mnist-digit-recognizer\/chineseMNIST.csv'\n\ndf = pd.read_csv(FILE_PATH)\ndf.head()","455daf4e":"df['new_label'] = df['label'].astype(str) + ' - ' + df['character']\ndata = df.drop(['label', 'character'], axis=1).copy()\ndata.head()","31466f69":"# Data imbalance check\ndata['new_label'].value_counts()","c98047b2":"classes = list(set(data['new_label']))\nclasses.sort(key=lambda x: int(x.split(' ')[0]))\nclasses","3ff5edf8":"# Get labels in terms of indices of classes\nlabels = list(data['new_label'])\nfor i in range(len(labels)):\n    labels[i] = classes.index(labels[i])","a3b53c49":"# Get data from dataframe, drop new label\nX = data.drop(['new_label'], axis=1).copy().to_numpy()\nX.shape","ccc2d377":"# Normalize from (0,255) to (0,1)\nX = np.float32(X\/255)\nprint(X.min(), X.max())","db729b9d":"from sklearn.model_selection import train_test_split\n\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, labels, test_size=0.15, random_state=12, stratify=labels)\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=12, stratify=y_train_val)\n\nprint(f'Number of training samples: {len(y_train)}')\nprint(f'Distribution of labels in training samples: {np.unique(y_train, return_counts=True, axis=0)[1]}\\n')\n\nprint(f'Number of validation samples: {len(y_val)}')\nprint(f'Distribution of labels in validation samples: {np.unique(y_val, return_counts=True, axis=0)[1]}\\n')\n\nprint(f'Number of test samples: {len(y_test)}')\nprint(f'Distribution of labels in test samples: {np.unique(y_test, return_counts=True, axis=0)[1]}')","a7353e91":"X_train = X_train.reshape(-1,64,64)\nX_val = X_val.reshape(-1,64,64)\nX_test = X_test.reshape(-1,64,64)\nX_test.shape","94bb8e52":"class _Dataset(Dataset):\n    def __init__(self, X, y, transform=None):\n        self.x = X\n        self.y = y\n        self.num_samples = len(X)\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        if self.transform:\n            transformed_img = self.transform(self.x[index])\n        \n        return transformed_img, self.y[index]\n    \n    def __len__(self):\n        return self.num_samples\n    \n    \ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.RandomAffine(degrees=(-20, 20), translate=(0.1, 0.2), scale=(0.8, 1.2)),\n        transforms.ToTensor()\n    ]),\n    'val': transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.ToTensor()\n    ])\n}","5cf54112":"training_dataset = _Dataset(X_train, y_train, transform=data_transforms['train'])\nvalidation_dataset = _Dataset(X_val, y_val, transform=data_transforms['val'])\ntest_dataset = _Dataset(X_test, y_test, transform=data_transforms['val'])","96ba2024":"num_cols = 10\nnum_rows = 100\/\/num_cols +1\n\nfig = plt.figure(figsize=(2*num_cols, 2*num_rows+1), dpi=80)\nfor i in range(100):\n    ax = fig.add_subplot(num_rows, num_cols, i+1)\n    image, label = training_dataset[i]\n    ax.imshow(image.permute(1,2,0), cmap='gray')\n    ax.axis('off')\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_title(f'{classes[label]}', fontsize=15)","c4b4954e":"BATCH_SIZE=64\n\ntrain_loader = torch.utils.data.DataLoader(dataset=training_dataset, shuffle=True, batch_size=BATCH_SIZE)\nval_loader = torch.utils.data.DataLoader(dataset=validation_dataset, shuffle=False, batch_size=BATCH_SIZE)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, shuffle=False, batch_size=BATCH_SIZE)","90211d74":"# Device config\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using {device}')","5a2bcede":"def train_model(model, train_loader, val_loader, criterion, optimizer, num_epoch, scheduler=None):\n    since = time.time()\n    \n    # Plotting stats\n    training_accuracies, training_losses, val_accuracies, val_losses = [], [], [], []\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    phases = ['Training', 'Validation']\n \n    for epoch in range(num_epoch):\n        print('=' * 70)\n        print('Epoch {}\/{}, Learning Rate = {:.1}'.format(epoch+1, num_epoch, optimizer.param_groups[0]['lr']))\n        print('=' * 70)\n        \n        # For epoch stats. Resets every epoch\n        train_running_loss = 0.0\n        train_running_corrects = 0\n        val_running_loss = 0.0\n        val_running_corrects = 0\n\n        for phase in phases:\n            if phase == 'Training':\n                model.train()  \n                dataloaders = train_loader\n                dataset_sizes = len(training_dataset)\n            else:\n                model.eval()  \n                dataloaders = val_loader\n                dataset_sizes = len(validation_dataset)\n\n            for batch_idx, (inputs, labels) in enumerate(dataloaders):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase == 'Training'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    if phase == 'Training':\n                        # Calculate loss and accuracy for mini-batch. Add these stats to epoch stats\n                        train_loss = loss.item() \n                        train_acc = torch.sum(preds == labels.data) \/ len(labels)\n                        train_running_loss += train_loss\n                        train_running_corrects += train_acc\n                        \n                        loss.backward()\n                        optimizer.step()\n                    else:\n                        # Calculate loss and accuracy for entire validation data. Add these stats to epoch stats\n                        val_loss = loss.item() \n                        val_acc = torch.sum(preds == labels.data) \/ len(labels)\n                        val_running_loss += val_loss\n                        val_running_corrects += val_acc\n\n                # Print mini-batch stats\n                if batch_idx % 30 == 0 and phase == 'Training':\n                    print('Training: [{}\/{} ({:.0f}%)]\\t Accuracy: {:.6f} Loss: {:.6f}'.format(batch_idx, len(dataloaders), 100. * batch_idx \/ len(dataloaders), train_acc, train_loss))\n                \n            # Update learning rate if applicable             \n            if phase == 'Training' and scheduler is not None:\n                scheduler.step()\n                          \n            # Save model weights that give best val acc\n            if phase == 'Validation' and val_running_corrects\/len(val_loader) > best_acc:\n                best_acc = val_running_corrects\/len(val_loader)\n                best_model_wts = copy.deepcopy(model.state_dict())\n        \n        training_accuracies.append(train_running_corrects\/len(train_loader))\n        training_losses.append(train_running_loss\/len(train_loader))\n        val_accuracies.append(val_running_corrects\/len(val_loader))\n        val_losses.append(val_running_loss\/len(val_loader))\n        \n        \n        print(\n            f'Epoch Training Accuracy: {train_running_corrects\/len(train_loader):.4f} ' \n            f'Loss: {train_running_loss\/len(train_loader):.4f}'\n        )\n        print(\n            f'Epoch Validation Accuracy: {val_running_corrects\/len(val_loader):.4f} '\n            f'Loss: {val_running_loss\/len(val_loader):.4f}\\n'\n        )\n                                          \n    time_elapsed = time.time() - since\n    print('Training completed in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n               \n    return model, training_accuracies, training_losses, val_accuracies, val_losses\n    \n\ndef test(model, test_loader, criterion):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    preds_list = []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            output = model(inputs)\n            predictions = torch.argmax(output, dim=1)\n            preds_list.extend(prediction.item() for prediction in predictions)\n            pred = output.max(1, keepdim=True)[1] \n            correct += pred.eq(labels.view_as(pred)).sum().item()\n            \n    print(f'Test Accuracy {correct}\/{len(test_loader.dataset)}, {100. * correct \/ len(test_loader.dataset):.3f}%')\n\n    return preds_list","426cb6b8":"from collections import OrderedDict\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.layers = nn.Sequential(OrderedDict([\n            ('conv1', nn.Conv2d(1, 64, kernel_size=5)), # (64-5)\/1 + 1 = 60x60\n            ('relu1', nn.ReLU(inplace=True)),\n            ('pool1', nn.MaxPool2d(2, 2)), # (60-2)\/2 + 1 = 30x30\n            \n            ('conv2', nn.Conv2d(64, 256, kernel_size=3)), # 28x28\n            ('relu2', nn.ReLU()),\n            ('drop2', nn.Dropout(p=0.25, inplace=True)),\n            ('pool2', nn.MaxPool2d(2, 2)), # 14x14\n            \n            ('conv3', nn.Conv2d(256, 128, kernel_size=3)), # 12x12\n            ('relu3', nn.ReLU()),\n            ('drop3', nn.Dropout(p=0.25, inplace=True)),\n            ('pool3', nn.MaxPool2d(2, 2)), # 6x6\n            \n            ('global_avg_pool', nn.AdaptiveAvgPool2d((1,1))),\n            ('flatten', nn.Flatten()),\n            ('fc1', nn.Linear(128, len(classes)))\n        ]))\n\n    def forward(self, x):\n        x = self.layers(x)\n        return x","067a441e":"model = Net().to(device)\n\n# Number of Epochs\nnum_epoch = 40\n\n# Initial Learning Rate\nlearning_rate = 0.01\n\n# Optimizer\noptimizer_ft = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Decay LR by a factor of 0.1 every 10 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n\ncriterion = nn.CrossEntropyLoss()\n\nprint(model)\nprint(summary(model, (1,64,64)))","fedddde7":"model_ft, training_accuracies, training_losses, val_accuracies, val_losses = train_model(model=model,\n                                                                                         train_loader= train_loader,\n                                                                                         val_loader=val_loader, \n                                                                                         criterion=criterion, \n                                                                                         optimizer=optimizer_ft, \n                                                                                         num_epoch=num_epoch, \n                                                                                         scheduler=exp_lr_scheduler)","80e502c9":"import plotly.graph_objects as go\nimport plotly.offline as pyo\n\npyo.init_notebook_mode()\n\n# create figure\nfig = go.Figure()\n\n# Add trace\nfig.add_trace(\n    go.Scatter(\n        x=[*range(len(training_accuracies))],\n        y=torch.tensor(training_accuracies).cpu(),\n        name='Training Accuracy',\n        hoverinfo = 'skip',\n        hovertemplate ='<br><b>Epoch<\/b>: %{x}<br><i>Training Accuracy <\/i>:%{y}'\n    )\n)\n\n# Add trace\nfig.add_trace(\n    go.Scatter(\n        x=[*range(len(val_accuracies))],\n        y=torch.tensor(val_accuracies).cpu(),\n        name='Validation Accuracy',\n        hovertemplate ='<br><b>Epoch<\/b>: %{x}<br><i>Validation Accuracy <\/i>:%{y}'\n    )\n)\n\n# Add trace\nfig.add_trace(\n    go.Scatter(\n        x=[*range(len(training_losses))],\n        y=torch.tensor(training_losses).cpu(),\n        name='Training Loss',\n        hovertemplate ='<br><b>Epoch<\/b>: %{x}<br><i>Training Loss <\/i>:%{y}',\n        visible=False\n    )\n)\n\n# Add trace\nfig.add_trace(\n    go.Scatter(\n        x=[*range(len(val_losses))],\n        y=torch.tensor(val_losses).cpu(),\n        name='Validation Loss',\n        hovertemplate ='<br><b>Epoch<\/b>: %{x}<br><i>Validation Loss <\/i>:%{y}',\n        visible=False\n    )\n)\n\n\n# Update plot sizing\nfig.update_layout(\n    width=800,\n    height=600,\n    autosize=False,\n    margin=dict(t=0, b=0, l=0, r=0)\n)\n\nfig.update_xaxes(title_text='Epoch')\nfig.update_yaxes(title_text='Accuracy')\n    \n\n# Add dropdown\nfig.update_layout(\n    updatemenus=[\n        dict(\n            type = \"buttons\",\n            direction = \"left\",\n            buttons=list([\n                dict(\n                    label=\"Accuracy\",\n                    method=\"update\",\n                    args = [\n                        {'visible': [True, True, False, False]},\n                        {\"yaxis\": {\"title\": \"Accuracy\"}}\n                    ]\n                ),\n                dict(\n                    label=\"Loss\",\n                    method=\"update\",\n                    args = [\n                        {'visible': [False, False, True, True]},\n                        {\"yaxis\": {\"title\": \"Loss\"}}\n                    ]\n                )\n            ]),\n            pad={\"r\": 10, \"t\": 10},\n            showactive=True,\n            x=0.00,\n            xanchor=\"left\",\n            y=1.1,\n            yanchor=\"top\"\n        ),\n    ],\n    hovermode='x',\n    xaxis_range=[-2,len(val_losses)+3]\n)\n\nfig.show()","12431abf":"y_pred = test(model, test_loader, criterion)","701fe2f2":"idx = np.where(np.array(y_pred)!=y_test)[0]\nnum_cols = 11\nnum_rows = 26\/\/num_cols+1\n\nfig = plt.figure(figsize=(2*num_cols+1, 2*num_rows+1))\nfor i in range(26):\n    ax = fig.add_subplot(num_rows, num_cols, i+1)\n    ax.imshow(X_test[idx[i]], cmap='gray')\n    ax.axis('off')\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_title(f'Predicted: {classes[y_pred[idx[i]]]}\\n Actual: {classes[y_test[idx[i]]]}', fontsize=15)\n\nplt.tight_layout(h_pad=2)","307e35fb":"test_idx = np.random.randint(0, high=len(test_dataset), size=1)[0]\ntest_image, test_label = test_dataset[test_idx]\n\nimage = torch.unsqueeze(test_image, 0)\nimage = image.to(device)\n\nwith torch.no_grad():\n    output = model(image)\n    probs = F.softmax(output, dim=1)[0].cpu().detach().numpy()\n    \nclrs = ['grey' if (x < max(probs)) else 'red' for x in probs]\n\nfig, ax = plt.subplots(1, 2, figsize=(10,5), dpi=80)\n\nax[0].imshow(test_image.permute(1,2,0), cmap='gray')\nax[0].axis('off')\n\nax[1] = sns.barplot(x=classes, y=probs, palette=clrs)\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90, ha=\"center\")\n\nplt.suptitle(f'Predicted: {classes[np.argmax(probs)]}\\nActual: {classes[test_label]}', fontsize=15)\nplt.tight_layout()\nplt.show()","84f2c5f0":"# 2. Datasets Creation","785f5883":"# 1. Data Processing","9e2cd50e":"Run cell below to generate random test index and see probabilities of predictions.","7505fe70":"### Incorrect Test Predictions","05ac430c":"### Plots","921e17e8":"# 3. Model and Training","8f2db5d7":"Combine characters and labels","acd79774":"# 4. Predictions"}}