{"cell_type":{"c9978bb3":"code","111de897":"code","46770355":"code","046a0adb":"code","6ed38f41":"code","c7f588b7":"code","c9d40b24":"code","301b82b1":"code","3ada1cf4":"code","7444e467":"code","9555b99f":"code","2e6c0627":"code","a45833b3":"code","3e3a2c7f":"code","8861151f":"code","6925451a":"code","2c05dfa0":"code","6ca0c37f":"code","215e8d16":"code","8fa2cc2b":"code","1dd9a815":"code","cbf908dd":"code","b7fa0626":"code","101b6900":"code","3e24167d":"code","f5a5a9cc":"code","c3625235":"code","bcd08963":"code","e03ef563":"code","a58f80b7":"code","4006f2f3":"code","cf0f4eb1":"code","7b5fffaf":"code","37a32d73":"code","b4f9a14e":"code","36036946":"code","9eabfdf0":"code","1ef2d8e9":"code","c441b371":"code","a9bccc8d":"code","02c668bc":"code","e04bf5d1":"code","431d99d6":"code","9586f69d":"code","c6daec28":"code","bec85463":"code","e19e69ef":"code","55279fb7":"code","282089de":"code","518e7552":"code","3ede5b6f":"code","1a868418":"code","a548569e":"code","f03993fc":"code","24b95ac1":"code","ea2fbec8":"code","7bdb8c6e":"code","f19dd47b":"code","da7bfadf":"code","e41f9c2a":"code","af0621e5":"code","56e866e3":"code","a064f8a8":"code","f73ca7a0":"code","64bbb71d":"code","220f5e71":"code","55fff527":"code","d780d5dc":"markdown","6733a766":"markdown","a795337f":"markdown","1c06a688":"markdown","a09d02c9":"markdown","6ce20fb8":"markdown","f0290588":"markdown","54ce0864":"markdown","7d232030":"markdown","fda1cb6c":"markdown","82cef19a":"markdown","21431343":"markdown","ee2bb5bf":"markdown","53f56f16":"markdown","92093cdc":"markdown","71452370":"markdown","7ab73e31":"markdown","64a5bf70":"markdown","6196711b":"markdown","47a41525":"markdown","a67824a2":"markdown","647d47ec":"markdown","b7cfcdf6":"markdown","626fb9c3":"markdown","839b0b4d":"markdown","db0764b7":"markdown"},"source":{"c9978bb3":"!pip -q install pytorch-pfn-extras==0.4.2\n!pip -q install icecream\n!pip -q install wandb\n!pip -q install rezero\n!pip -q install madgrad\n!pip install -Uq scikit-learn","111de897":"COMP_NAME = 'shigglecup-2nd'","46770355":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_key\")\n\nimport wandb\napi_key = secret_value_0\n!wandb login $api_key\nwandb.init(project=COMP_NAME, name='Transformer_v4')","046a0adb":"import os\nimport gc\nimport copy\nimport yaml\nimport random\nimport shutil\nimport typing as tp\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, StratifiedGroupKFold\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.cuda import amp\n\nimport pytorch_pfn_extras as ppe\nfrom pytorch_pfn_extras.config import Config\nfrom pytorch_pfn_extras.training import extensions as ppe_exts, triggers as ppe_triggers\n\nimport wandb\n\nfrom tqdm.auto import tqdm as tqdmp\nfrom tqdm.autonotebook import tqdm as tqdm\ntqdmp.pandas()\n\nfrom icecream import ic","6ed38f41":"import warnings\nwarnings.simplefilter('ignore')","c7f588b7":"import sys\nsys.path.append('..\/input\/sam-optimizer')\n\nimport sam\n\nimport madgrad","c9d40b24":"pre_eval_cfg = yaml.safe_load(\n\"\"\"\nglobals:\n  seed: 28\n  val_fold: null  # indicate when training\n  output_path: null # indicate when training\n  device: cuda\n  enable_amp: False\n  max_epoch: 300\n  cudnn_benchmark: False\n  no_last_aug: True\n  no_aug_epoch: 150\n  label_smooth: False\n  mixup_or_label: False\n  mixup_lamda_05: False\n\nmodel:\n  type: RezeroTransformer\n#  type: SkipTransformer\n#  type: SimpleTransformer\n#  type: RezeroSkipTransformer\n#  type: DoubleRezeroTransformer\n#  type: SkipLearnTransformer\n#  num_features: 36\n#  num_features: 33\n#  num_features: 54\n#  num_features: 106\n  num_features: 95\n  num_hidden: 128\n#  dim_feedforward: 2048\n  dim_feedforward: 1024\n  num_layers: 12\n#  num_layers: 6\n  num_heads: 8\n  activation: gelu\n  drop_rate: 0.5\n\ndataset:\n  mixup: {enabled: False, alpha: 1.0}\n  train:\n    type: PokemonMixupDataset_v3\n    paths: null  # set by lazy_init\n    labels: null  # set by lazy_init\n    transform: null\n    use_shuffle: True\n    use_swap: True\n    base_df: null  # set by lazy_init\n  val:\n    type: PokemonDataset\n    paths: null  # set by lazy_init\n    labels: null  # set by lazy_init\n    transform: null\n  test:\n    type: PokemonDataset\n    paths: null  # set by lazy_init\n    labels: null  # set by lazy_init\n    transform: \"@\/dataset\/val\/transform\"\n  no_aug_train:\n    type: PokemonMixupDataset_v3\n    paths: null  # set by lazy_init\n    labels: null  # set by lazy_init\n    transform: \"@\/dataset\/val\/transform\"\n    use_shuffle: True\n    use_swap: False\n    base_df: null  # set by lazy_init\n\nloader:\n  train: {type: DataLoader, dataset: \"@\/dataset\/train\",\n    batch_size: 64, num_workers: 2, shuffle: True, pin_memory: True, drop_last: True}\n  val: {type: DataLoader, dataset: \"@\/dataset\/val\",\n    batch_size: 128, num_workers: 2, shuffle: False, pin_memory: True, drop_last: False}\n  test: {type: DataLoader, dataset: \"@\/dataset\/test\",\n    batch_size: 128, num_workers: 2, shuffle: False, pin_memory: True, drop_last: False}\n  no_aug_train: {type: DataLoader, dataset: \"@\/dataset\/no_aug_train\",\n    batch_size: 64, num_workers: 2, shuffle: True, pin_memory: True, drop_last: True}\n\noptimizer:\n  type: AdamW\n  params: {type: method_call, obj: \"@\/model\", method: parameters}\n#   lr: 1.0e-06\n  lr: 1.0e-03\n#  lr: 1.0e-02\n  weight_decay: 1.0e-02\n\nscheduler:\n#  type: OneCycleLR\n#  optimizer: \"@\/optimizer\"\n#  epochs: \"@\/globals\/max_epoch\"\n#  steps_per_epoch: {type: __len__, obj: \"@\/loader\/train\"}\n#  max_lr: 1.0e-3\n#  pct_start: 0.1\n#  anneal_strategy: cos\n#  div_factor: 1.0e+3\n#  final_div_factor: 1.0e+3\n  type: CosineAnnealingWarmRestarts\n  optimizer: \"@\/optimizer\"\n  T_0: 100\n  T_mult: 1\n\n\nloss: {type: BCEWithLogitsLoss}\n# loss: {type: BCEFocalLoss, \"alpha\":1.0}\n\neval:\n  - type: micro_average\n    metric_func: {type: BCEWithLogitsLoss}\n    report_name: loss\n  - type: calc_across_all_batchs\n    metric_func: {type: ROCAUC}\n    report_name: metric\n\nmanager:\n  type: ExtensionsManager\n  models: \"@\/model\"\n  optimizers: \"@\/optimizer\"\n  max_epochs: \"@\/globals\/max_epoch\"\n  iters_per_epoch: {type: __len__, obj: \"@\/loader\/train\"}\n  out_dir: \"@\/globals\/output_path\"\n  # stop_trgiger: {type: EarlyStoppingTrigger,\n  #   monitor: val\/metric, mode: max, patience: 5, verbose: True,\n  #   check_trigger: [1, epoch], max_trigger: [\"@\/globals\/max_epoch\", epoch]}\n\nextensions:\n  # # log\n  - {type: observe_lr, optimizer: \"@\/optimizer\"}\n  - {type: LogReport}\n  - {type: PlotReport, y_keys: lr, x_key: epoch, filename: lr.png}\n  - {type: PlotReport, y_keys: [train\/loss, val\/loss], x_key: epoch, filename: loss.png}\n  - {type: PlotReport, y_keys: val\/metric, x_key: epoch, filename: metric.png}\n  - {type: PrintReport, entries: [\n      epoch, iteration, lr, train\/loss, val\/loss, val\/metric, elapsed_time]}\n  - {type: ProgressBarNotebook, update_interval: 20}\n  - {type: SendWandB, entries: [\n      lr, train\/loss, val\/loss, val\/metric, elapsed_time], \"fold_id\":None}\n\n  # snapshot\n#   - extension: {type: snapshot, target: \"@\/model\", filename: \"snapshot_by_metric_epoch_{.epoch}.pth\"}\n#     trigger: {type: MaxValueTrigger, key: \"val\/metric\", trigger: [1, epoch]}\n  - extension: {type: snapshot, target: \"@\/model\", filename: \"snapshot_by_metric.pth\"}\n    trigger: {type: MaxValueTrigger, key: \"val\/metric\", trigger: [1, epoch]}\n  # lr scheduler\n  - {type: LRScheduler, scheduler: \"@\/scheduler\", trigger: [1,  iteration]}\n\"\"\"\n)","301b82b1":"wandb.config.update(pre_eval_cfg)\n\ntry:\n    wandb.config.update({'transforms':[c['type'] for c in pre_eval_cfg['dataset']['train']['transform']['transforms']]})\nexcept:\n    pass","3ada1cf4":"ROOT = Path.cwd().parent\n# ROOT = Path.cwd()\nINPUT = ROOT \/ \"input\"\nOUTPUT = ROOT \/ \"working\"\nDATA = INPUT \/ COMP_NAME\nTRAIN = DATA \/ \"train\"\nTEST = DATA \/ \"test\"\n\nTMP = OUTPUT \/ \"tmp\"\nTMP.mkdir(exist_ok=True)\n\nRANDAM_SEED = 28\nCLASSES = [\"target\",]\nN_CLASSES = len(CLASSES)\nFOLDS = [0, 1, 2, 3, 4]\nN_FOLDS = len(FOLDS)","7444e467":"def set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n    \nset_random_seed(RANDAM_SEED)","9555b99f":"df_train = pd.read_csv(DATA \/ \"train.csv\")\ndf_test = pd.read_csv(DATA \/ \"test.csv\")\nsmpl_sub = pd.read_csv(DATA \/ \"sample_submission.csv\")\ndf_team_id = pd.read_csv(DATA \/ \"team_id.csv\")\ndf_pokemon = pd.read_csv(DATA \/ \"pokemon.csv\")\ndf_typetable = pd.read_csv(DATA \/ \"typetable.csv\")","2e6c0627":"ic(df_train.shape)\nic(df_test.shape)\nic(df_team_id.shape)\nic(df_pokemon.shape)\nic(df_typetable.shape)","a45833b3":"df_train.head()","3e3a2c7f":"df_team_id.head()","8861151f":"df_pokemon.head()","6925451a":"df_typetable.head()","2c05dfa0":"train = df_train.copy()\ntest = df_test.copy()","6ca0c37f":"# CV = GroupKFold(n_splits=N_FOLDS)\nCV = StratifiedGroupKFold(n_splits=N_FOLDS)\ntrain[\"fold\"] = -1\nfor fold_id, (tr_idx, val_idx) in enumerate(CV.split(train[\"first\"], train[\"target\"], train[\"first\"])):\n    print(val_idx)\n    train.loc[val_idx, \"fold\"] = fold_id","215e8d16":"train.head()","8fa2cc2b":"train.groupby(\"fold\").nunique()['first']","1dd9a815":"train.groupby(\"fold\").count()['first']","cbf908dd":"df_pokemon_feat = df_pokemon.copy()","b7fa0626":"tmp = pd.get_dummies(df_pokemon['Type_1']) + pd.get_dummies(df_pokemon['Type_2'])\ntmp = tmp.add_prefix('Type_OH_')\ndf_pokemon_feat = pd.concat([df_pokemon_feat, tmp], axis=1)\n# df_pokemon_feat.head()","101b6900":"df_pokemon_feat['Total'] = df_pokemon_feat[['HP', 'Attack', 'Defense', 'Sp_Atk', 'Sp_Def', 'Speed']].sum(axis=1)\n# df_pokemon_feat.head()","3e24167d":"df_pokemon_feat[['HP', 'Attack', 'Defense', 'Sp_Atk', 'Sp_Def', 'Speed', 'Total']] \/= df_pokemon_feat[['HP', 'Attack', 'Defense', 'Sp_Atk', 'Sp_Def', 'Speed', 'Total']].max(axis=0)","f5a5a9cc":"tmp = pd.get_dummies(df_pokemon['Generation']).add_prefix('Generation_OH_')\ndf_pokemon_feat = pd.concat([df_pokemon_feat, tmp], axis=1)\n# df_pokemon_feat.head()","c3625235":"df_pokemon_feat['Legendary'] = df_pokemon_feat['Legendary'].astype('int')","bcd08963":"df_pokemon_feat.head().T","e03ef563":"feat_cols = ['HP', 'Attack', 'Defense', 'Sp_Atk', 'Sp_Def', 'Speed', 'Total', 'Legendary',\n            'Type_OH_Bug', 'Type_OH_Dark', 'Type_OH_Dragon', 'Type_OH_Electric', 'Type_OH_Fairy',\n            'Type_OH_Fighting', 'Type_OH_Fire', 'Type_OH_Flying', 'Type_OH_Ghost','Type_OH_Grass',\n            'Type_OH_Ground', 'Type_OH_Ice', 'Type_OH_Normal', 'Type_OH_Poison', 'Type_OH_Psychic',\n            'Type_OH_Rock', 'Type_OH_Steel', 'Type_OH_Water',\n            'Generation_OH_1', 'Generation_OH_2', 'Generation_OH_3', 'Generation_OH_4', 'Generation_OH_5', 'Generation_OH_6']\n\ndef get_feat(target, battle, is_first):\n    target_team = df_team_id[df_team_id['team_id']==target]\n    battle_team = df_team_id[df_team_id['team_id']==battle]\n\n    feats = []\n    for i in range(1,7):\n        target_col = f'pokemon_id_{i}'\n        target_poke = target_team[target_col].values[0]\n        target_types = df_pokemon[['Type_1', 'Type_2']][df_pokemon['pokemon_id']==target_poke].dropna(axis=1).values[0]\n\n        type_matchs = []\n        for j in range(1,7):\n            battle_col = f'pokemon_id_{j}'\n            battle_poke = battle_team[battle_col].values[0]\n            battle_types = df_pokemon[['Type_1', 'Type_2']][df_pokemon['pokemon_id']==battle_poke].dropna(axis=1).values[0]\n\n            for t in target_types:\n                tmp = []\n                for b in battle_types:\n                    tmp.append(df_typetable[b][df_typetable['atck']==t].values)\n                type_matchs.append(np.prod(tmp))\n        feats.append(np.concatenate([\n                            df_pokemon_feat[feat_cols][df_pokemon['pokemon_id']==target_poke].values[0],\n                           np.mean(type_matchs).reshape(-1),\n                           np.min(type_matchs).reshape(-1),\n                           np.max(type_matchs).reshape(-1),\n                            np.reshape(int(is_first),-1)\n                          ],axis=0))\n    return np.array(feats)","a58f80b7":"%%time\nget_feat(1,2, True), get_feat(1,2, False)","4006f2f3":"get_feat(1,2, False).shape","cf0f4eb1":"# battle_feat = []\n# for teams in tqdm(train[['first', 'second']].values):\n#     battle_feat.append(np.concatenate([get_feat(teams[0],teams[1], True), get_feat(teams[1],teams[0], False)], axis=0))\n# np.array(battle_feat).shape","7b5fffaf":"# battle_feat = np.array(battle_feat)","37a32d73":"# np.save('battle_feat.npy', battle_feat)\n\nfeat_num = pre_eval_cfg['model']['num_features']\nif feat_num==33:\n    battle_feat = np.load('..\/input\/shigglecup-2nd-feature-notype\/battle_feat_noType.npy')\nelif feat_num==36:\n    battle_feat = np.load('..\/input\/shigglecup2nd-battle-feature\/battle_feat.npy')\nelif feat_num==54:\n    battle_feat = np.load('..\/input\/shigglecup-2nd-feat-v2\/battle_feat_v2.npy')\nelif feat_num==48:\n    battle_feat = np.load('..\/input\/shigglecup-2nd-feat-v2\/battle_feat_v2_noGen.npy')\nelif feat_num==106:\n    battle_feat = np.load('..\/input\/shigglecup-2nd-feat-v2\/battle_feat_v3.npy', allow_pickle=True)\nelif feat_num==95:\n    battle_feat = np.load('..\/input\/shigglecup-2nd-feat-v2\/battle_feat_v4.npy', allow_pickle=True)\nbattle_feat.shape","b4f9a14e":"# battle_feat_test = []\n# for teams in tqdm(test[['first', 'second']].values):\n#     battle_feat_test.append(np.concatenate([get_feat(teams[0],teams[1], True), get_feat(teams[1],teams[0], False)], axis=0))\n# battle_feat_test = np.array(battle_feat_test)\n# battle_feat_test.shape","36036946":"# np.save('battle_feat_test.npy', battle_feat)\n\nif feat_num==33:\n    battle_feat_test = np.load('..\/input\/shigglecup-2nd-feature-notype\/battle_feat_noType_test.npy')\nelif feat_num==36:\n    battle_feat_test = np.load('..\/input\/shigglecup2nd-battle-feature\/battle_feat_test.npy')\nelif feat_num==54:\n    battle_feat_test = np.load('..\/input\/shigglecup-2nd-feat-v2\/battle_feat_v2_test.npy')\nelif feat_num==48:\n    battle_feat_test = np.load('..\/input\/shigglecup-2nd-feat-v2\/battle_feat_v2_noGen_test.npy')\nelif feat_num==106:\n    battle_feat_test = np.load('..\/input\/shigglecup-2nd-feat-v2\/battle_feat_test_v3.npy', allow_pickle=True)\nelif feat_num==95:\n    battle_feat_test = np.load('..\/input\/shigglecup-2nd-feat-v2\/battle_feat_test_v4.npy', allow_pickle=True)\nbattle_feat_test.shape","9eabfdf0":"class SimpleTransformer(nn.Module):\n    def __init__(self,\n                 num_features: int = 36,\n                 num_hidden: int = 128,\n                 dim_feedforward: int = 2048,\n                 num_layers: int = 6,\n                 num_heads: int = 8,\n                 activation: str = 'relu',\n                 drop_rate: float = 0.1,\n                 num_classes: int = 1):\n        super(SimpleTransformer, self).__init__()\n\n        self.seq_linear = nn.Sequential(\n            nn.Linear(num_features, num_hidden),\n            nn.LayerNorm(num_hidden),\n            nn.Dropout(drop_rate),\n            nn.ReLU(),\n        )\n        encoder_layer = nn.TransformerEncoderLayer(d_model=num_hidden, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=drop_rate, activation=activation)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n        # self.linear = nn.Linear(num_features, num_classes)\n        self.head = nn.Sequential(\n            nn.Linear(num_hidden*2, num_hidden),\n            nn.LayerNorm(num_hidden),\n            nn.ReLU(),\n            nn.Dropout(drop_rate),\n            nn.Linear(num_hidden, num_classes),\n        )\n\n    def forward(self, x):\n        # print(x.shape) # BATCH, NUM_POKEMON, NUM_FEATUERS\n        # print(x.dtype)\n        x = self.seq_linear(x)\n        # print(x.shape) # BATCH, NUM_POKEMON, NUM_HIDDEN\n        x = x.permute(1,0,2) # [NUM_POKEMON, BATCH, NUM_HIDDEN]\n        x = self.transformer_encoder(x) # [NUM_POKEMON, BATCH, NUM_HIDDEN]\n        x = x.permute(1,0,2) # [BATCH, NUM_POKEMON, NUM_FEATUERS]\n        # print(x.shape)\n        f_x = torch.mean(x[:,:6,], axis=1) # [BATCH, NUM_FEATUERS]\n        s_x = torch.mean(x[:,6:,], axis=1) # [BATCH, NUM_FEATUERS]\n        x = torch.cat([f_x, s_x], axis=1) # [BATCH, NUM_FEATUERS*2]\n        # print(x.shape)\n        x = self.head(x)  # [BATCH, NUM_CLASSES]\n\n        return x","1ef2d8e9":"from rezero.transformer import RZTXEncoderLayer\n\nclass RezeroTransformer(nn.Module):\n    def __init__(self,\n                 num_features: int = 36,\n                 num_hidden: int = 128,\n                 dim_feedforward: int = 2048,\n                 num_layers: int = 6,\n                 num_heads: int = 8,\n                 activation: str = 'relu',\n                 drop_rate: float = 0.1,\n                 num_classes: int = 1):\n        super(RezeroTransformer, self).__init__()\n\n        self.seq_linear = nn.Sequential(\n            nn.Linear(num_features, num_hidden),\n            nn.LayerNorm(num_hidden),\n            nn.Dropout(drop_rate),\n            nn.ReLU(),\n        )\n        encoder_layer = RZTXEncoderLayer(d_model=num_hidden, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=drop_rate, activation=activation)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.head = nn.Sequential(\n            nn.Linear(num_hidden*2, num_hidden),\n            nn.LayerNorm(num_hidden),\n            nn.ReLU(),\n            nn.Dropout(drop_rate),\n            nn.Linear(num_hidden, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.seq_linear(x)\n        x = x.permute(1,0,2) # [NUM_POKEMON, BATCH, NUM_HIDDEN]\n        x = self.transformer_encoder(x) # [NUM_POKEMON, BATCH, NUM_HIDDEN]\n        x = x.permute(1,0,2) # [BATCH, NUM_POKEMON, NUM_FEATUERS]\n        f_x = torch.mean(x[:,:6,], axis=1) # [BATCH, NUM_FEATUERS]\n        s_x = torch.mean(x[:,6:,], axis=1) # [BATCH, NUM_FEATUERS]\n        x = torch.cat([f_x, s_x], axis=1) # [BATCH, NUM_FEATUERS*2]\n        x = self.head(x)  # [BATCH, NUM_CLASSES]\n\n        return x","c441b371":"def _get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n\nclass SkipTransformer(nn.Module):\n    def __init__(self,\n                 num_features: int = 36,\n                 num_hidden: int = 128,\n                 dim_feedforward: int = 2048,\n                 num_layers: int = 6,\n                 num_heads: int = 8,\n                 activation: str = 'relu',\n                 drop_rate: float = 0.1,\n                 num_classes: int = 1,\n                 alpha: float = 0.7):\n        super(SkipTransformer, self).__init__()\n\n        self.seq_linear = nn.Sequential(\n            nn.Linear(num_features, num_hidden),\n            nn.LayerNorm(num_hidden),\n            nn.Dropout(drop_rate),\n            nn.ReLU(),\n        )\n        encoder_layer = nn.TransformerEncoderLayer(d_model=num_hidden, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=drop_rate, activation=activation)\n        self.transformer_encoder_layers = _get_clones(encoder_layer, num_layers)\n        self.head = nn.Sequential(\n            nn.Linear(num_hidden*2, num_hidden),\n            nn.LayerNorm(num_hidden),\n            nn.ReLU(),\n            nn.Dropout(drop_rate),\n            nn.Linear(num_hidden, num_classes),\n        )\n        self.alpha = alpha\n\n    def forward(self, x):\n        x = self.seq_linear(x)\n        x = x.permute(1,0,2) # [NUM_POKEMON, BATCH, NUM_HIDDEN]\n        for layer in self.transformer_encoder_layers:\n            x_old = x\n            x = layer(x) # [NUM_POKEMON, BATCH, NUM_HIDDEN]\n            x = self.alpha*x + (1-self.alpha)*x_old\n        x = x.permute(1,0,2) # [BATCH, NUM_POKEMON, NUM_FEATUERS]\n        f_x = torch.mean(x[:,:6,], axis=1) # [BATCH, NUM_FEATUERS]\n        s_x = torch.mean(x[:,6:,], axis=1) # [BATCH, NUM_FEATUERS]\n        x = torch.cat([f_x, s_x], axis=1) # [BATCH, NUM_FEATUERS*2]\n        x = self.head(x)  # [BATCH, NUM_CLASSES]\n\n        return x","a9bccc8d":"from rezero.transformer import RZTXEncoderLayer\n\ndef _get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n\nclass RezeroSkipTransformer(nn.Module):\n    def __init__(self,\n                 num_features: int = 36,\n                 num_hidden: int = 128,\n                 dim_feedforward: int = 2048,\n                 num_layers: int = 6,\n                 num_heads: int = 8,\n                 activation: str = 'relu',\n                 drop_rate: float = 0.1,\n                 num_classes: int = 1,\n                 alpha: float = 0.7):\n        super(RezeroSkipTransformer, self).__init__()\n\n        self.seq_linear = nn.Sequential(\n            nn.Linear(num_features, num_hidden),\n            nn.LayerNorm(num_hidden),\n            nn.Dropout(drop_rate),\n            nn.ReLU(),\n        )\n        encoder_layer = RZTXEncoderLayer(d_model=num_hidden, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=drop_rate, activation=activation)\n        self.transformer_encoder_layers = _get_clones(encoder_layer, num_layers)\n        self.head = nn.Sequential(\n            nn.Linear(num_hidden*2, num_hidden),\n            nn.LayerNorm(num_hidden),\n            nn.ReLU(),\n            nn.Dropout(drop_rate),\n            nn.Linear(num_hidden, num_classes),\n        )\n        self.alpha = alpha\n\n    def forward(self, x):\n        x = self.seq_linear(x)\n        x = x.permute(1,0,2) # [NUM_POKEMON, BATCH, NUM_HIDDEN]\n        for layer in self.transformer_encoder_layers:\n            x_old = x\n            x = layer(x) # [NUM_POKEMON, BATCH, NUM_HIDDEN]\n            x = self.alpha*x + (1-self.alpha)*x_old\n        x = x.permute(1,0,2) # [BATCH, NUM_POKEMON, NUM_FEATUERS]\n        f_x = torch.mean(x[:,:6,], axis=1) # [BATCH, NUM_FEATUERS]\n        s_x = torch.mean(x[:,6:,], axis=1) # [BATCH, NUM_FEATUERS]\n        x = torch.cat([f_x, s_x], axis=1) # [BATCH, NUM_FEATUERS*2]\n        x = self.head(x)  # [BATCH, NUM_CLASSES]\n\n        return x","02c668bc":"from rezero.transformer import RZTXEncoderLayer\n\ndef _get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n\nclass DoubleRezeroTransformer(nn.Module):\n    def __init__(self,\n                 num_features: int = 36,\n                 num_hidden: int = 128,\n                 dim_feedforward: int = 2048,\n                 num_layers: int = 6,\n                 num_heads: int = 8,\n                 activation: str = 'relu',\n                 drop_rate: float = 0.1,\n                 num_classes: int = 1,\n                 alpha: float = 0.7):\n        super(DoubleRezeroTransformer, self).__init__()\n\n        self.seq_linear = nn.Sequential(\n            nn.Linear(num_features, num_hidden),\n            nn.LayerNorm(num_hidden),\n            nn.Dropout(drop_rate),\n            nn.ReLU(),\n        )\n        encoder_layer = RZTXEncoderLayer(d_model=num_hidden, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=drop_rate, activation=activation)\n        self.transformer_encoder_layers = _get_clones(encoder_layer, num_layers)\n        self.resweights = nn.ParameterList([nn.Parameter(torch.Tensor([0])) for i in range(num_layers)])\n        self.head = nn.Sequential(\n            nn.Linear(num_hidden*2, num_hidden),\n            nn.LayerNorm(num_hidden),\n            nn.ReLU(),\n            nn.Dropout(drop_rate),\n            nn.Linear(num_hidden, num_classes),\n        )\n        self.alpha = alpha\n\n    def forward(self, x):\n        x = self.seq_linear(x)\n        x = x.permute(1,0,2) # [NUM_POKEMON, BATCH, NUM_HIDDEN]\n        for layer, resweight in zip(self.transformer_encoder_layers, self.resweights):\n            x_old = x\n            x = layer(x) # [NUM_POKEMON, BATCH, NUM_HIDDEN]\n            x = x*resweight + x_old\n        x = x.permute(1,0,2) # [BATCH, NUM_POKEMON, NUM_FEATUERS]\n        f_x = torch.mean(x[:,:6,], axis=1) # [BATCH, NUM_FEATUERS]\n        s_x = torch.mean(x[:,6:,], axis=1) # [BATCH, NUM_FEATUERS]\n        x = torch.cat([f_x, s_x], axis=1) # [BATCH, NUM_FEATUERS*2]\n        x = self.head(x)  # [BATCH, NUM_CLASSES]\n\n        return x","e04bf5d1":"def _get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n\nclass SkipLearnTransformer(nn.Module):\n    def __init__(self,\n                 num_features: int = 36,\n                 num_hidden: int = 128,\n                 dim_feedforward: int = 2048,\n                 num_layers: int = 6,\n                 num_heads: int = 8,\n                 activation: str = 'relu',\n                 drop_rate: float = 0.1,\n                 num_classes: int = 1,\n                 alpha: float = 0.7):\n        super(SkipLearnTransformer, self).__init__()\n\n        self.seq_linear = nn.Sequential(\n            nn.Linear(num_features, num_hidden),\n            nn.LayerNorm(num_hidden),\n            nn.Dropout(drop_rate),\n            nn.ReLU(),\n        )\n        encoder_layer = nn.TransformerEncoderLayer(d_model=num_hidden, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=drop_rate, activation=activation)\n        self.transformer_encoder_layers = _get_clones(encoder_layer, num_layers)\n        self.resweights = nn.ParameterList([nn.Parameter(torch.Tensor([0])) for i in range(num_layers)])\n        self.head = nn.Sequential(\n            nn.Linear(num_hidden*2, num_hidden),\n            nn.LayerNorm(num_hidden),\n            nn.ReLU(),\n            nn.Dropout(drop_rate),\n            nn.Linear(num_hidden, num_classes),\n        )\n        self.alpha = alpha\n\n    def forward(self, x):\n        x = self.seq_linear(x)\n        x = x.permute(1,0,2) # [NUM_POKEMON, BATCH, NUM_HIDDEN]\n        for layer, resweight in zip(self.transformer_encoder_layers, self.resweights):\n            x_old = x\n            x = layer(x) # [NUM_POKEMON, BATCH, NUM_HIDDEN]\n            x = x*resweight + x_old\n        x = x.permute(1,0,2) # [BATCH, NUM_POKEMON, NUM_FEATUERS]\n        f_x = torch.mean(x[:,:6,], axis=1) # [BATCH, NUM_FEATUERS]\n        s_x = torch.mean(x[:,6:,], axis=1) # [BATCH, NUM_FEATUERS]\n        x = torch.cat([f_x, s_x], axis=1) # [BATCH, NUM_FEATUERS*2]\n        x = self.head(x)  # [BATCH, NUM_CLASSES]\n\n        return x","431d99d6":"tmp = np.concatenate([get_feat(1,2, False), get_feat(1,2, True)], axis=0)\ntmp = tmp[np.newaxis,::]\ntmp.shape","9586f69d":"model = SkipTransformer()\nmodel.forward(torch.Tensor(tmp))","c6daec28":"FilePath = tp.Union[str, Path]\nLabel = tp.Union[int, float, np.ndarray]\n\n\nclass PokemonDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset using 6 channels by stacking them along time-axis\n\n    Attributes\n    ----------\n    paths : tp.Sequence[FilePath]\n        Sequence of path to cadence snippet file\n    labels : tp.Sequence[Label]\n        Sequence of label for cadence snippet file\n    transform: albumentations.Compose\n        composed data augmentations for data\n    \"\"\"\n\n    def __init__(\n        self,\n        paths: tp.Sequence[FilePath],\n        labels: tp.Sequence[Label],\n        transform,\n    ):\n        \"\"\"Initialize\"\"\"\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"Return num of cadence snippets\"\"\"\n        return len(self.paths)\n\n    def __getitem__(self, index: int):\n        \"\"\"Return transformed image and label for given index.\"\"\"\n        # teams, label = self.paths[index], self.labels[index]\n        \n        # feat = np.concatenate([get_feat(teams[0],teams[1], True),\n        #                       get_feat(teams[1],teams[0], False)], axis=0)\n        # feat = feat.astype(np.float32)\n        feat, label = self.paths[index], self.labels[index]\n        feat = feat.astype(np.float32)\n        return {\"features\": feat, \"target\": label}\n\n    def lazy_init(self, paths=None, labels=None, transform=None):\n        \"\"\"Reset Members\"\"\"\n        if paths is not None:\n            self.paths = paths\n        if labels is not None:\n            self.labels = labels\n        if transform is not None:\n            self.transform = transform","bec85463":"FilePath = tp.Union[str, Path]\nLabel = tp.Union[int, float, np.ndarray]\n\n\nclass PokemonMixupDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset using 6 channels by stacking them along time-axis\n\n    Attributes\n    ----------\n    paths : tp.Sequence[FilePath]\n        Sequence of path to cadence snippet file\n    labels : tp.Sequence[Label]\n        Sequence of label for cadence snippet file\n    transform: albumentations.Compose\n        composed data augmentations for data\n    \"\"\"\n\n    def __init__(\n        self,\n        paths: tp.Sequence[FilePath],\n        labels: tp.Sequence[Label],\n        transform,\n        use_shuffle=True,\n        use_mix=True,\n        base_df: pd.DataFrame=None\n    ):\n        \"\"\"Initialize\"\"\"\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n        self.use_shuffle = use_shuffle\n        self.use_mix = use_mix\n        self.base_df = base_df\n\n    def __len__(self):\n        \"\"\"Return num of cadence snippets\"\"\"\n        return len(self.paths)\n\n    def __getitem__(self, index: int):\n        \"\"\"Return transformed image and label for given index.\"\"\"\n        feat, label = self.paths[index], self.labels[index]\n        feat = feat.astype(np.float32)\n        if self.use_mix:\n            if np.random.rand() > 0.7:\n                # first\u5171\u901a\u3059\u308b\u5bfe\u6226\u76f8\u624b\u3068\u30dd\u30b1\u30e2\u30f3mix\n                base_team = self.base_df.at[index, 'first']\n                index_list = np.array(self.base_df[self.base_df['first']==base_team].index)\n                use_index = np.random.permutation(index_list)[0]\n                mix_num = np.random.randint(1,2)\n                rep_loc = np.random.choice(5, mix_num, replace=False)\n                feat[rep_loc+6] = self.paths[use_index][rep_loc+6]            \n                label = label*(6-mix_num)\/6 + self.labels[use_index]*mix_num\/6\n        if self.use_shuffle:\n            feat[:6] = np.random.permutation(feat[:6])\n            feat[6:] = np.random.permutation(feat[6:])\n        return {\"features\": feat, \"target\": label}\n\n    def lazy_init(self, paths=None, labels=None, transform=None, base_df=None):\n        \"\"\"Reset Members\"\"\"\n        if paths is not None:\n            self.paths = paths\n        if labels is not None:\n            self.labels = labels\n        if transform is not None:\n            self.transform = transform\n        if base_df is not None:\n            self.base_df = base_df","e19e69ef":"FilePath = tp.Union[str, Path]\nLabel = tp.Union[int, float, np.ndarray]\n\n\nclass PokemonMixupDataset_v2(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset using 6 channels by stacking them along time-axis\n\n    Attributes\n    ----------\n    paths : tp.Sequence[FilePath]\n        Sequence of path to cadence snippet file\n    labels : tp.Sequence[Label]\n        Sequence of label for cadence snippet file\n    transform: albumentations.Compose\n        composed data augmentations for data\n    \"\"\"\n\n    def __init__(\n        self,\n        paths: tp.Sequence[FilePath],\n        labels: tp.Sequence[Label],\n        transform,\n        use_shuffle=True,\n        use_mix=True,\n        base_df: pd.DataFrame=None\n    ):\n        \"\"\"Initialize\"\"\"\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n        self.use_shuffle = use_shuffle\n        self.use_mix = use_mix\n        self.base_df = base_df\n\n    def __len__(self):\n        \"\"\"Return num of cadence snippets\"\"\"\n        return len(self.paths)\n\n    def __getitem__(self, index: int):\n        \"\"\"Return transformed image and label for given index.\"\"\"\n        feat, label = self.paths[index], self.labels[index]\n        feat = feat.astype(np.float32)\n        if self.use_mix:\n            if np.random.rand() > 0.7:\n                # first\u3068Second\u306e\u30dd\u30b1\u30e2\u30f3mix\n                mix_num = np.random.randint(1,3)\n                rep_loc = np.random.choice(5, mix_num, replace=False)\n                feat_old = feat.copy()\n                feat[rep_loc] = feat_old[rep_loc+6]\n                for loc in rep_loc:\n                    feat[loc][-1] = 1\n                feat[rep_loc+6] = feat_old[rep_loc]\n                for loc in rep_loc:\n                    feat[loc+6][-1] = 0\n                if label == 1:\n                    label = label-(mix_num\/6)\n                else:\n                    label = label+(mix_num\/6)\n        if self.use_shuffle:\n            feat[:6] = np.random.permutation(feat[:6])\n            feat[6:] = np.random.permutation(feat[6:])\n        return {\"features\": feat, \"target\": label}\n\n    def lazy_init(self, paths=None, labels=None, transform=None, base_df=None):\n        \"\"\"Reset Members\"\"\"\n        if paths is not None:\n            self.paths = paths\n        if labels is not None:\n            self.labels = labels\n        if transform is not None:\n            self.transform = transform\n        if base_df is not None:\n            self.base_df = base_df","55279fb7":"FilePath = tp.Union[str, Path]\nLabel = tp.Union[int, float, np.ndarray]\n\n\nclass PokemonMixupDataset_v3(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset using 6 channels by stacking them along time-axis\n\n    Attributes\n    ----------\n    paths : tp.Sequence[FilePath]\n        Sequence of path to cadence snippet file\n    labels : tp.Sequence[Label]\n        Sequence of label for cadence snippet file\n    transform: albumentations.Compose\n        composed data augmentations for data\n    \"\"\"\n\n    def __init__(\n        self,\n        paths: tp.Sequence[FilePath],\n        labels: tp.Sequence[Label],\n        transform,\n        use_shuffle=True,\n        use_mix=False,\n        use_swap=True,\n        base_df: pd.DataFrame=None\n    ):\n        \"\"\"Initialize\"\"\"\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n        self.use_shuffle = use_shuffle\n        self.use_mix = use_mix\n        self.use_swap = use_swap\n        self.base_df = base_df\n\n    def __len__(self):\n        \"\"\"Return num of cadence snippets\"\"\"\n        return len(self.paths)\n\n    def __getitem__(self, index: int):\n        \"\"\"Return transformed image and label for given index.\"\"\"\n        feat, label = self.paths[index], self.labels[index]\n        if self.use_mix:\n            if np.random.rand() > 0.7:\n                # first\u5171\u901a\u3059\u308b\u5bfe\u6226\u76f8\u624b\u3068\u30dd\u30b1\u30e2\u30f3mix\n                base_team = self.base_df.at[index, 'first']\n                index_list = np.array(self.base_df[self.base_df['first']==base_team].index)\n                use_index = np.random.permutation(index_list)[0]\n                mix_num = np.random.randint(1,3)\n                rep_loc = np.random.choice(5, mix_num, replace=False)\n                feat[rep_loc+6] = self.paths[use_index][rep_loc+6]\n                label = label*(6-mix_num)\/6 + self.labels[use_index]*mix_num\/6\n        if self.use_swap:\n            if np.random.rand() > 0.5:\n                feat_old = feat.copy()\n                feat[:6] = feat_old[6:]\n                feat[6:] = feat_old[:6]\n                for loc in range(6):\n                    feat[loc][-1] = 1\n                    feat[loc+6][-1] = 0\n                if label == 1:\n                    label = np.zeros(1)\n                else:\n                    label = np.ones(1)\n        if self.use_shuffle:\n            feat[:6] = np.random.permutation(feat[:6])\n            feat[6:] = np.random.permutation(feat[6:])\n        feat = feat.astype(np.float32)\n        return {\"features\": feat, \"target\": label}\n\n    def lazy_init(self, paths=None, labels=None, transform=None, base_df=None):\n        \"\"\"Reset Members\"\"\"\n        if paths is not None:\n            self.paths = paths\n        if labels is not None:\n            self.labels = labels\n        if transform is not None:\n            self.transform = transform\n        if base_df is not None:\n            self.base_df = base_df","282089de":"FilePath = tp.Union[str, Path]\nLabel = tp.Union[int, float, np.ndarray]\n\n\nclass PokemonMixupDataset_v4(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset using 6 channels by stacking them along time-axis\n\n    Attributes\n    ----------\n    paths : tp.Sequence[FilePath]\n        Sequence of path to cadence snippet file\n    labels : tp.Sequence[Label]\n        Sequence of label for cadence snippet file\n    transform: albumentations.Compose\n        composed data augmentations for data\n    \"\"\"\n\n    def __init__(\n        self,\n        paths: tp.Sequence[FilePath],\n        labels: tp.Sequence[Label],\n        transform,\n        use_shuffle=True,\n        use_swap=True,\n        base_df: pd.DataFrame=None\n    ):\n        \"\"\"Initialize\"\"\"\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n        self.use_shuffle = use_shuffle\n        self.use_swap = use_swap\n        self.base_df = base_df\n\n    def __len__(self):\n        \"\"\"Return num of cadence snippets\"\"\"\n        return len(self.paths)\n\n    def __getitem__(self, index: int):\n        \"\"\"Return transformed image and label for given index.\"\"\"\n        feat, label = self.paths[index], self.labels[index]\n        if self.use_swap:\n            if np.random.rand() > 0.5:\n                prob = np.random.rand()\n                if prob > 0.3:\n                    feat_old = feat.copy()\n                    feat[:6] = feat_old[6:]\n                    feat[6:] = feat_old[:6]\n                    for loc in range(6):\n                        feat[loc][-1] = 1\n                        feat[loc+6][-1] = 0\n                    if label == 1:\n                        label = np.zeros(1)\n                    else:\n                        label = np.ones(1)\n                elif prob > 0.1:\n                    if label == 1:\n                        feat_old = feat.copy()\n                        feat[:6] = feat_old[6:]\n                        feat[6:] = feat_old[:6]\n                        for loc in range(6):\n                            feat[loc][-1] = 1\n                            feat[loc+6][-1] = 0\n                        label = np.ones(1)\n        if self.use_shuffle:\n            feat[:6] = np.random.permutation(feat[:6])\n            feat[6:] = np.random.permutation(feat[6:])\n        feat = feat.astype(np.float32)\n        return {\"features\": feat, \"target\": label}\n\n    def lazy_init(self, paths=None, labels=None, transform=None, base_df=None):\n        \"\"\"Reset Members\"\"\"\n        if paths is not None:\n            self.paths = paths\n        if labels is not None:\n            self.labels = labels\n        if transform is not None:\n            self.transform = transform\n        if base_df is not None:\n            self.base_df = base_df","518e7552":"Batch = tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]]\nModelOut = tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor], torch.Tensor]\n\n\nclass RMSE(nn.Module):\n    \"\"\"RMSE score\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize.\"\"\"\n        super(RMSE, self).__init__()\n\n    def forward(self, y, t) -> float:\n        \"\"\"Forward.\"\"\"\n        if isinstance(y, torch.Tensor):\n            y = y.detach().cpu().numpy()\n        if isinstance(t, torch.Tensor):\n            t = t.detach().cpu().numpy()\n\n        return mean_squared_error(t, y, squared=False)\n    \n    \nclass ROCAUC(nn.Module):\n    \"\"\"ROC AUC score\"\"\"\n\n    def __init__(self, average=\"macro\") -> None:\n        \"\"\"Initialize.\"\"\"\n        self.average = average\n        super(ROCAUC, self).__init__()\n\n    def forward(self, y, t) -> float:\n        \"\"\"Forward.\"\"\"\n        if isinstance(y, torch.Tensor):\n            y = y.detach().cpu().numpy()\n        if isinstance(t, torch.Tensor):\n            t = t.detach().cpu().numpy()\n\n        return roc_auc_score(t, y, average=self.average)\n\n    \ndef micro_average(\n    metric_func: nn.Module,\n    report_name: str, prefix=\"val\",\n    pred_index: int=-1, label_index: int=-1,\n    pred_key: str=\"logit\", label_key: str=\"target\",\n) -> tp.Callable:\n    \"\"\"Return Metric Wrapper for Simple Mean Metric\"\"\"\n    metric_sum = [0.]\n    n_examples = [0]\n    \n    def wrapper(batch: Batch, model_output: ModelOut, is_last_batch: bool):\n        \"\"\"Wrapping metric function for evaluation\"\"\"\n        if isinstance(batch, tuple): \n            t = batch[label_index]\n        elif isinstance(batch, dict):\n            t = batch[label_key]\n        else:\n            raise NotImplementedError\n\n        if isinstance(model_output, tuple):\n            y = model_output[pred_index]\n        elif isinstance(model_output, dict):\n            y = model_output[pred_key]\n        else:\n            y = model_output\n\n        metric = metric_func(y, t).item()\n        metric_sum[0] += metric * y.shape[0]\n        n_examples[0] += y.shape[0]\n\n        if is_last_batch:\n            final_metric = metric_sum[0] \/ n_examples[0]\n            ppe.reporting.report({f\"{prefix}\/{report_name}\": final_metric})\n            # # reset state\n            metric_sum[0] = 0.\n            n_examples[0] = 0\n\n    return wrapper\n\n\ndef calc_across_all_batchs(\n    metric_func: nn.Module,\n    report_name: str, prefix=\"val\",\n    pred_index: int=-1, label_index: int=-1,\n    pred_key: str=\"logit\", label_key: str=\"target\",\n) -> tp.Callable:\n    \"\"\"\n    Return Metric Wrapper for Metrics caluculated on all data\n    \n    storing predictions and labels of evry batch, finally calculating metric on them.\n    \"\"\"\n    pred_list = []\n    label_list = []\n    \n    def wrapper(batch: Batch, model_output: ModelOut, is_last_batch: bool):\n        \"\"\"Wrapping metric function for evaluation\"\"\"\n        if isinstance(batch, tuple):\n            t = batch[label_index]\n        elif isinstance(batch, dict):\n            t = batch[label_key]\n        else:\n            raise NotImplementedError\n\n        if isinstance(model_output, tuple):\n            y = model_output[pred_index]\n        elif isinstance(model_output, dict):\n            y = model_output[pred_key]\n        else:\n            y = model_output\n\n        pred_list.append(y.numpy())\n        label_list.append(t.numpy())\n\n        if is_last_batch:\n            pred = np.concatenate(pred_list, axis=0)\n            label = np.concatenate(label_list, axis=0)\n            final_metric = metric_func(pred, label)\n            ppe.reporting.report({f\"{prefix}\/{report_name}\": final_metric})\n            # # reset state\n            pred_list[:] = []\n            label_list[:] = []\n\n    return wrapper","3ede5b6f":"class SendWandB(ppe.training.extensions.PrintReport):\n#     def __init__(self, entries=None, log_report='LogReport', wandb=None):\n    def __init__(self, entries=None, log_report='LogReport', wandb=wandb, fold_id=None):\n        super().__init__(entries, log_report, None)\n        self.wandb = wandb\n        self.entries = entries\n        self.fold_id = fold_id\n\n    def __call__(self, manager):\n        log_report = self.get_log_report(manager)\n        log = log_report.log\n        log_len = self._log_len\n        while len(log) > log_len:\n            # self.wandb.log(log[log_len], step=log_len)\n            if str(self.fold_id) != 'None':\n                send_log = {}\n                for k,v in log[log_len].items():\n                    if k in self.entries:\n                        send_log[f'{k}\/fold_{self.fold_id}'] = v\n                    elif k == 'epoch':\n                        send_log[k] = v\n                self.wandb.log(send_log)\n            else:\n                self.wandb.log(log[log_len])\n            log_len += 1\n        self._log_len = log_len","1a868418":"import inspect\n\ndef SAM(params, base_optimizer, rho=0.05, **kwargs):\n    optimizers = [name[0] for name in inspect.getmembers(optim) if name[0][0] != '_']\n    if base_optimizer in optimizers:\n        base_optimizer = eval(f'optim.{base_optimizer}')\n    else:\n        raise ValueError(f'{base_optimizer} is None in torch.optim')\n    return sam.SAM(params=params, base_optimizer=base_optimizer, rho=rho, **kwargs)","a548569e":"CONFIG_TYPES = {\n    # # utils\n    \"__len__\": lambda obj: len(obj),\n    \"method_call\": lambda obj, method: getattr(obj, method)(),\n\n    # # Dataset, DataLoader\n    \"PokemonDataset\": PokemonDataset,\n    \"PokemonMixupDataset\": PokemonMixupDataset,\n    \"PokemonMixupDataset_v2\": PokemonMixupDataset_v2,\n    \"PokemonMixupDataset_v3\": PokemonMixupDataset_v3,\n    \"PokemonMixupDataset_v4\": PokemonMixupDataset_v4,\n    \"DataLoader\": torch.utils.data.DataLoader,\n\n    # # Model\n    \"SimpleTransformer\": SimpleTransformer,\n    \"RezeroTransformer\": RezeroTransformer,\n    \"SkipTransformer\": SkipTransformer,\n    \"RezeroSkipTransformer\": RezeroSkipTransformer,\n    \"DoubleRezeroTransformer\": DoubleRezeroTransformer,\n    \"SkipLearnTransformer\": SkipLearnTransformer,\n\n    # # Optimizer\n    \"AdamW\": optim.AdamW,\n\n    # # Scheduler\n    \"OneCycleLR\": lr_scheduler.OneCycleLR,\n    \"CosineAnnealingWarmRestarts\": lr_scheduler.CosineAnnealingWarmRestarts,\n\n    # # Loss,Metric\n    \"MSELoss\": nn.MSELoss,\n    \"RMSE\": RMSE,\n    \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss,\n    \"ROCAUC\": ROCAUC,\n\n    # # Metric Wrapper\n    \"micro_average\": micro_average,\n    \"calc_across_all_batchs\": calc_across_all_batchs,\n\n    # # PPE Extensions\n    \"ExtensionsManager\": ppe.training.ExtensionsManager,\n\n    \"observe_lr\": ppe_exts.observe_lr,\n    \"LogReport\": ppe_exts.LogReport,\n    \"PlotReport\": ppe_exts.PlotReport,\n    \"PrintReport\": ppe_exts.PrintReport,\n    \"PrintReportNotebook\": ppe_exts.PrintReportNotebook,\n    \"ProgressBar\": ppe_exts.ProgressBar,\n    \"ProgressBarNotebook\": ppe_exts.ProgressBarNotebook,\n    \"snapshot\": ppe_exts.snapshot,\n    \"LRScheduler\": ppe_exts.LRScheduler, \n    \"SendWandB\":SendWandB,\n\n    \"MinValueTrigger\": ppe_triggers.MinValueTrigger,\n    \"MaxValueTrigger\": ppe_triggers.MaxValueTrigger,\n    \"EarlyStoppingTrigger\": ppe_triggers.EarlyStoppingTrigger,\n}","f03993fc":"def set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n\n\ndef to_device(\n    tensors: tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]],\n    device: torch.device, *args, **kwargs\n):\n    if isinstance(tensors, tuple):\n        return (t.to(device, *args, **kwargs) for t in tensors)\n    elif isinstance(tensors, dict):\n        return {\n            k: t.to(device, *args, **kwargs) for k, t in tensors.items()}\n    else:\n        return tensors.to(device, *args, **kwargs)","24b95ac1":"def get_path_label(cfg: Config, train_all: pd.DataFrame):\n    \"\"\"Get file path and target info.\"\"\"\n    use_fold = cfg[\"\/globals\/val_fold\"]\n\n    train_df = train_all[train_all[\"fold\"] != use_fold]\n    val_df = train_all[train_all[\"fold\"] == use_fold]\n    \n    train_path_label = {\n        # \"paths\": train_df[['first', 'second']].values,\n        \"paths\": battle_feat[train_all[\"fold\"] != use_fold],\n        \"labels\": train_df[CLASSES].values.astype(\"f\"),\n        \"base_df\": train_df.reset_index(drop=True)\n    }\n    val_path_label = {\n        # \"paths\": val_df[['first', 'second']].values,\n        \"paths\": battle_feat[train_all[\"fold\"] == use_fold],\n        \"labels\": val_df[CLASSES].values.astype(\"f\")\n    }\n    return train_path_label, val_path_label\n\n\ndef get_eval_func(cfg, model, device):\n    \n    def eval_func(**batch):\n        \"\"\"Run evaliation for val or test. This function is applied to each batch.\"\"\"\n        batch = to_device(batch, device)\n        x = batch[\"features\"]\n        with amp.autocast(cfg[\"\/globals\/enable_amp\"]): \n            y = model(x)\n        return y.detach().cpu().to(torch.float32)  # input of metrics\n\n    return eval_func\n\n\ndef mixup_data(use_mixup, x, t, alpha=1.0, use_cuda=True):\n    '''Returns mixed inputs, pairs of targets, and lambda'''\n    if not use_mixup:\n        return x, t, None, None\n    \n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    t_a, t_b = t, t[index]\n    return mixed_x, t_a, t_b, lam\n\n\ndef get_criterion(use_mixup, loss_func, label_smooth=False):\n\n    def mixup_criterion(pred, t_a, t_b, lam):\n        return lam * loss_func(pred, t_a) + (1 - lam) * loss_func(pred, t_b)\n\n    def mixup_smooth_criterion(pred, t_a, t_b, lam):\n        alpha = 1e-3\n        return lam * loss_func(pred, torch.clamp(t_a, alpha, 1.0-alpha)) + (1 - lam) * loss_func(pred, torch.clamp(t_b, alpha, 1.0-alpha))\n\n    def single_criterion(pred, t_a, t_b, lam):\n        return loss_func(pred, t_a)\n\n    def single_smooth_criterion(pred, t_a, t_b, lam):\n        alpha = 1e-3\n        return loss_func(pred, torch.clamp(t_a, alpha, 1.0-alpha))\n    \n    if use_mixup:\n        if label_smooth:\n            return mixup_smooth_criterion\n        else:\n            return mixup_criterion\n    else:\n        if label_smooth:\n            return single_smooth_criterion\n        else:\n            return single_criterion","ea2fbec8":"def train_one_fold(cfg, train_all):\n    \"\"\"Main\"\"\"\n    torch.backends.cudnn.benchmark = True\n    set_random_seed(cfg[\"\/globals\/seed\"], deterministic=True)\n    device = torch.device(cfg[\"\/globals\/device\"])\n    \n    if cfg['\/scheduler\/type'] == 'CosineAnnealingWarmRestarts':\n        train_path_label, val_path_label = get_path_label(cfg, train_all)   \n        cfg[\"\/dataset\/train\"].lazy_init(**train_path_label)\n        train_loader = cfg[\"\/loader\/train\"]\n\n        num_train_steps = int(len(train_loader) * cfg['\/scheduler\/T_0'])\n        pre_eval_cfg['scheduler']['T_0'] = num_train_steps\n        cfg = Config(pre_eval_cfg, types=CONFIG_TYPES)\n    \n    train_path_label, val_path_label = get_path_label(cfg, train_all)\n    print(\"train: {}, val: {}\".format(len(train_path_label[\"paths\"]), len(val_path_label[\"paths\"])))\n   \n    cfg[\"\/dataset\/train\"].lazy_init(**train_path_label)\n    cfg[\"\/dataset\/val\"].lazy_init(**val_path_label)\n    train_loader = cfg[\"\/loader\/train\"]\n    val_loader = cfg[\"\/loader\/val\"]\n\n    model = cfg[\"\/model\"]\n    model.to(device)\n    optimizer = cfg[\"\/optimizer\"]\n    loss_func = cfg[\"\/loss\"]\n    loss_func.to(device)\n    \n    manager = cfg[\"\/manager\"]\n    for ext in cfg[\"\/extensions\"]:\n        if isinstance(ext, dict):\n            manager.extend(**ext)\n        else:\n            manager.extend(ext)\n\n    evaluator = ppe_exts.Evaluator(\n        val_loader, model, eval_func=get_eval_func(cfg, model, device),\n        metrics=cfg[\"\/eval\"], progress_bar=False)\n    manager.extend(evaluator, trigger=(1, \"epoch\"))\n\n    use_amp = cfg[\"\/globals\/enable_amp\"]\n    scaler = amp.GradScaler(enabled=use_amp)\n    use_mixup = cfg[\"\/dataset\/mixup\/enabled\"]\n    mixup_alpha = cfg[\"\/dataset\/mixup\/alpha\"]\n    \n    max_epoch = cfg[\"\/globals\/max_epoch\"]\n    try:\n        no_last_aug = cfg[\"\/globals\/no_last_aug\"]\n        no_aug_epoch = cfg[\"\/globals\/no_aug_epoch\"]\n        cfg[\"\/dataset\/no_aug_train\"].lazy_init(**train_path_label)\n        no_aug_train_loader = cfg[\"\/loader\/no_aug_train\"]\n    except:\n        no_last_aug = False\n        no_aug_epoch = 0\n\n    try:\n        label_smooth = cfg[\"\/globals\/label_smooth\"]\n    except:\n        label_smooth = False\n    \n    while not manager.stop_trigger:\n        model.train()\n        if no_last_aug & (manager.epoch+1 > (max_epoch-no_aug_epoch)):\n            print(f'Epoch {manager.epoch+1} : No Augmentation')\n            for batch in no_aug_train_loader:\n                with manager.run_iteration():\n                    batch = to_device(batch, device)\n                    x, t = batch[\"features\"], batch[\"target\"]\n                    # # for mixup\n                    mixed_x, t_a, t_b, lam = mixup_data(use_mixup, x, t, mixup_alpha)\n                    criterion = get_criterion(use_mixup, loss_func, label_smooth)\n    \n                    if cfg['\/optimizer\/type'] == 'SAM':\n                        optimizer.zero_grad()\n                        def closure():\n                            with amp.autocast(use_amp):\n                                y = model(mixed_x)\n                                loss = criterion(y, t_a, t_b, lam)\n                            scaler.scale(loss).backward()\n                            return loss\n                        \n                        with amp.autocast(use_amp):\n                            y = model(mixed_x)\n                            loss = criterion(y, t_a, t_b, lam)\n                        scaler.scale(loss).backward()\n                        scaler.step(optimizer, closure=closure)\n                        scaler.update()\n                    else:\n                        optimizer.zero_grad()\n                        with amp.autocast(use_amp):\n                            y = model(mixed_x)\n                            loss = criterion(y, t_a, t_b, lam)\n                        scaler.scale(loss).backward()\n                        scaler.step(optimizer)\n                        scaler.update()\n\n                    ppe.reporting.report({'train\/loss': loss.item()})\n                \n        else:\n            for batch in train_loader:\n                with manager.run_iteration():\n                    batch = to_device(batch, device)\n                    x, t = batch[\"features\"], batch[\"target\"]\n                    # # for mixup\n                    mixed_x, t_a, t_b, lam = mixup_data(use_mixup, x, t, mixup_alpha)\n                    criterion = get_criterion(use_mixup, loss_func, label_smooth)\n\n                    if cfg['\/optimizer\/type'] == 'SAM':\n                        optimizer.zero_grad()\n                        def closure():\n                            with amp.autocast(use_amp):\n                                y = model(mixed_x)\n                                loss = criterion(y, t_a, t_b, lam)\n                            scaler.scale(loss).backward()\n                            return loss\n                        \n                        with amp.autocast(use_amp):\n                            y = model(mixed_x)\n                            loss = criterion(y, t_a, t_b, lam)\n                        scaler.scale(loss).backward()\n                        scaler.step(optimizer, closure=closure)\n                        scaler.update()\n                    else:\n                        optimizer.zero_grad()\n                        with amp.autocast(use_amp):\n                            y = model(mixed_x)\n                            loss = criterion(y, t_a, t_b, lam)\n                        scaler.scale(loss).backward()\n                        scaler.step(optimizer)\n                        scaler.update()\n\n                    ppe.reporting.report({'train\/loss': loss.item()})","7bdb8c6e":"def get_fold_config(cfg, fold_id):\n    try:\n        cfg_ = cfg.copy()\n        for i, c in enumerate(cfg_['extensions']):\n            if c['type']=='SendWandB':\n                cfg_['extensions'][i]['fold_id'] = fold_id\n                return cfg_\n    except:\n        return cfg","f19dd47b":"pre_eval_cfg_list = []\nfor fold_id in FOLDS:\n    # tmp_cfg = copy.deepcopy(pre_eval_cfg)\n    tmp_cfg = copy.deepcopy(get_fold_config(pre_eval_cfg, fold_id))\n    tmp_cfg[\"globals\"][\"val_fold\"] = fold_id\n    tmp_cfg[\"globals\"][\"output_path\"] = str(TMP \/ f\"fold{fold_id}\")\n    pre_eval_cfg_list.append(tmp_cfg)","da7bfadf":"for pre_eval_cfg in pre_eval_cfg_list:\n    cfg = Config(pre_eval_cfg, types=CONFIG_TYPES)\n    print(f\"\\nfold:\", cfg[\"\/globals\/val_fold\"])\n    train_one_fold(cfg, train)\n\n    fold_n = cfg['\/globals\/val_fold']\n    exp_dir_path = TMP \/ f\"fold{fold_n}\"\n    log = pd.read_json(exp_dir_path \/ \"log\")\n    best_log = log.iloc[[log[\"val\/metric\"].idxmax()],]\n    best_epoch = best_log.epoch.values[0]\n    print(f\"fold{fold_n}  best epoch {best_epoch}  best metric  {np.round(best_log['val\/metric'], 6)}\")\n    del cfg\n    torch.cuda.empty_cache()\n    gc.collect()","e41f9c2a":"best_log_list = []\nfor pre_eval_cfg, fold_id in zip(pre_eval_cfg_list, FOLDS):\n    exp_dir_path = TMP \/ f\"fold{fold_id}\"\n    log = pd.read_json(exp_dir_path \/ \"log\")\n    best_log = log.iloc[[log[\"val\/metric\"].idxmax()],]\n    best_epoch = best_log.epoch.values[0]\n    best_log_list.append(best_log)\n    \n    # best_model_path = exp_dir_path \/ f\"snapshot_by_metric_epoch_{best_epoch}.pth\"\n    best_model_path = exp_dir_path \/ f\"snapshot_by_metric.pth\"\n    copy_to = f\"{OUTPUT}\/best_metric_model_fold{fold_id}.pth\"\n    shutil.copy(best_model_path, copy_to)\n    \n    for p in exp_dir_path.glob(\"*.pth\"):\n        p.unlink()\n    \n    shutil.copytree(exp_dir_path, f\"{OUTPUT}\/fold{fold_id}\")\n    \n    with open(f\"{OUTPUT}\/fold{fold_id}\/config.yml\", \"w\") as fw:\n        yaml.dump(pre_eval_cfg, fw)\n    \npd.concat(best_log_list, axis=0, ignore_index=True)","af0621e5":"def run_inference_loop(cfg, model, loader, device):\n    model.to(device)\n    model.eval()\n    pred_list = []\n    with torch.no_grad():\n        for batch in tqdm(loader):\n            x = to_device(batch[\"features\"], device)\n            y = model(x)\n            # pred_list.append(y.detach().cpu().numpy())\n            pred_list.append(y.sigmoid().detach().cpu().numpy())\n        \n    pred_arr = np.concatenate(pred_list)\n    del pred_list\n    return pred_arr","56e866e3":"label_arr = train[CLASSES].values\noof_pred_arr = np.zeros((len(train), N_CLASSES))\nscore_list = []\ntest_pred_arr = np.zeros((N_FOLDS, len(smpl_sub), N_CLASSES))\ntest_path_label = {\n    \"paths\": battle_feat_test,\n    \"labels\": smpl_sub[CLASSES].values.astype(\"f\")\n}\n\nfor fold_id in range(N_FOLDS):\n    print(f\"[fold {fold_id}]\")\n    tmp_dir = Path(f\"{OUTPUT}\/fold{fold_id}\")\n    with open(tmp_dir \/ \"config.yml\", \"r\") as fr:\n        cfg = Config(yaml.safe_load(fr), types=CONFIG_TYPES)\n    device = torch.device(cfg[\"\/globals\/device\"])\n    val_idx = train.query(\"fold == @fold_id\").index.values\n\n    # # get_dataloader\n    _, val_path_label = get_path_label(cfg, train)\n    cfg[\"\/dataset\/val\"].lazy_init(**val_path_label)\n    cfg[\"\/dataset\/test\"].lazy_init(**test_path_label)\n    val_loader = cfg[\"\/loader\/val\"]\n    test_loader = cfg[\"\/loader\/test\"]\n    \n    # # get model\n    model_path = f\"{OUTPUT}\/best_metric_model_fold{fold_id}.pth\"\n    model = cfg[\"\/model\"]\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    \n    # # inference\n    val_pred = run_inference_loop(cfg, model, val_loader, device)\n    val_score = roc_auc_score(label_arr[val_idx], val_pred)\n    oof_pred_arr[val_idx] = val_pred\n    score_list.append([fold_id, val_score])\n    \n    test_pred_arr[fold_id] = run_inference_loop(cfg, model, test_loader, device)\n    \n    del cfg, val_idx, val_path_label\n    del model, val_loader, test_loader\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(f\"val score: {val_score:.4f}\")","a064f8a8":"oof_score = roc_auc_score(label_arr, oof_pred_arr)\nscore_list.append([\"oof\", oof_score])\npd.DataFrame(score_list, columns=[\"fold\", \"metric\"])","f73ca7a0":"oof_df = train.copy()\noof_df[CLASSES] = oof_pred_arr\noof_df.to_csv(f\"{OUTPUT}\/oof_prediction_0_{str(oof_score)[2:7]}.csv\", index=False)","64bbb71d":"sub_df = smpl_sub.copy()\nsub_df[CLASSES] = test_pred_arr.mean(axis=0)\nsub_df.to_csv(f\"{OUTPUT}\/submission_0_{str(oof_score)[2:7]}.csv\", index=False)","220f5e71":"sub_df.head()","55fff527":"wandb.log({'CV':oof_score})","d780d5dc":"# Prapere","6733a766":"# Train","a795337f":"## Make Feature","1c06a688":"## Inference OOF & Test","a09d02c9":"### Model","6ce20fb8":"## Copy best models","f0290588":"### Dataset","54ce0864":"type OneHot","7d232030":"\u7a2e\u65cf\u5024","fda1cb6c":"generation OneHot","82cef19a":"\u4f5c\u6210\u306b20\u5206\u307b\u3069\u304b\u304b\u308b\u305f\u3081\u3001\u4e8b\u524d\u306b\u7528\u610f\u3057\u3066\u307e\u3059","21431343":"## run train","ee2bb5bf":"\u5404\u30b9\u30c6\u30fc\u30bf\u30b9\u306e\u6570\u5024\u3092\u6b63\u898f\u5316","53f56f16":"## Make submission","92093cdc":"Legendary int\u5316","71452370":"## Log Setting","7ab73e31":"## Make fold","64a5bf70":"- \u76f8\u6027\u3000min,max,mean\n- type  OneHot\n- first, second\u30d5\u30e9\u30b0\n- \u7a2e\u65cf\u5024\n- \u5404\u30b9\u30c6\u30fc\u30bf\u30b9\u306e\u6570\u5024\u3092\u6b63\u898f\u5316\n- generation OneHot","6196711b":"## Read Data, Split folds","47a41525":"## functions for training","a67824a2":"## Import","647d47ec":"### Metric","b7cfcdf6":"## config_types for evaluating configuration\n\nI use [pytorch-pfn-extras](https:\/\/github.com\/pfnet\/pytorch-pfn-extras) for training NNs. This library has useful config systems but requires some preparation.\n\nFor more details, see [docs](https:\/\/github.com\/pfnet\/pytorch-pfn-extras\/blob\/master\/docs\/config.md).","626fb9c3":"## configration","839b0b4d":"# Inference","db0764b7":"## Definition of Model, Dataset, Metric"}}