{"cell_type":{"15b469ec":"code","f8e4030f":"code","1b9e8548":"code","75171174":"code","c803b94f":"code","5eaf61e1":"code","33c0556a":"code","bbe2251f":"code","37a54c10":"code","92300c72":"code","1635ca41":"code","75c7b0aa":"code","4db25d21":"code","bb361f54":"code","d376b98a":"code","0650d284":"code","980f8aa8":"code","46d79b21":"code","0945a6e1":"code","be6e555e":"code","ae05e104":"code","686c7c50":"code","28fda979":"code","5805c39b":"code","beaf4d39":"code","90e19a28":"code","2c61ec3b":"code","a1f6988c":"code","01e991ac":"code","c2e21f06":"code","0b5acafe":"code","cb036b57":"code","12fff7c9":"code","2f69f24e":"code","95bdaaa8":"code","1eeafe66":"code","3e16215d":"code","5780d5ee":"code","c5e69471":"code","56421c41":"code","48851911":"code","466b3e0b":"code","44217af7":"code","36b47c25":"code","bfb3bfdb":"code","d6a7e07d":"code","4bd76d3e":"code","bd89e891":"code","9fa675be":"markdown"},"source":{"15b469ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f8e4030f":"import numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom kaggle.competitions import twosigmanews\nimport matplotlib.pyplot as plt\nimport random\nfrom datetime import datetime, date\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nimport time\nimport math\nimport gc\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport inspect, sys\nfrom multiprocessing import Pool","1b9e8548":"dic_assetName={}\ndic_assetNameSNo={}\ndic_assetCodes={}\ndic_assetCode={}\ndic_assetCodeSNo={}\ndic_rng = {}\ndic_min = {}\nset_sector = {\"TECH\",\"ENER\",\"BMAT\",\"INDS\",\"CYCS\",\"NCYC\",\"FINS\",\"HECA\",\"TECH\",\"TCOM\",\"UTIL\"}\ndic_sector_code_name = {\"ENER\":\"Energy\", \"BMAT\":\"Basic Materials\", \"INDS\":\"Industrials\", \"CYCS\":\"Cyclical Consumer Goods & Services\", \\\n     \"NCYC\":\"Non Cyclical Consumer Goods & Services\", \"FINS\": \"Financials\", \"HECA\":\"Healthcare\",\"TECH\":\"Technology\", \\\n     \"TCOM\":\"Telecommunication Services\", \"UTIL\":\"Utilities\"}\nsector_col = list(set_sector)\nsector_col.append(\"assetNameSNo\")\ndic_sectorSNo = {v: k for v,k in enumerate(list(set_sector))}\ndic_sector = {k: v for v,k in enumerate(list(set_sector))}\ndic_sectorSNo[\"-1\"] = \"Unknown\"\ndic_sector[\"Unknown\"] = -1\ndic_asset_sector = {}\n\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\ntotal_market_obs_df = []\ntotal_news_obs_df = []\nreturn_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10',\"SenSR\"] #'open','close',]\nsector_return_features = [\"weighted_SenSR\"]\nn_lag = [3,7,14]\ndebug_func = set()\nprv_mkt_data = []\nprv_news_data = []","75171174":"# official way to get the data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\ndays = env.get_prediction_days()","c803b94f":"(market_train_df, news_train_df) = env.get_training_data()\n#market_train_df = market_train_df[(market_train_df[\"assetName\"]==\"Apple Inc\") | (market_train_df[\"assetName\"] == \"Chevron Corp\")]\n#news_train_df = news_train_df[(news_train_df[\"assetName\"]==\"Apple Inc\") | (news_train_df[\"assetName\"]==\"Chevron Corp\")]","5eaf61e1":"if 1==2:\n    prv_time_list = market_train_df[\"time\"].dt.date.unique()[-14:]\n    prv_time_list.sort()\n    prv_time_list\n    for prv_time in prv_time_list:\n        prv_mkt_data.append(market_train_df[market_train_df[\"time\"].dt.date == prv_time])\n        prv_news_data.append(news_train_df[news_train_df[\"time\"].dt.date == prv_time])","33c0556a":"#market_train_df[market_train_df[\"assetName\"].str.contains(\"Morgan\")]","bbe2251f":"def plotdata(ticker):\n    tickerdata = market_train_df[market_train_df[\"assetCode\"]==ticker]\n    tickerdata.set_index([\"time\"],inplace=True)\n    fig,ax = plt.subplots(nrows=2,ncols=2, figsize=(27,9))\n    tickerdata[\"close\"].plot(ax=ax[0,0])\n    tickerdata[\"returnsOpenNextMktres10\"].plot(ax=ax[0,1])\n    result =plot_acf(tickerdata[\"close\"].dropna(), ax=ax[1,0])\n    result =plot_acf(tickerdata[\"returnsOpenNextMktres10\"].dropna(), ax=ax[1,1])\n    #Partial auto corelation sudden drop indicated auto regressive model, gradual drop indicates moving average model\n    #Most data exhibit sudden drop\n    #result =plot_pacf(tickerdata[\"close\"].dropna(), ax=ax[2,0])\n    #result =plot_pacf(tickerdata[\"returnsOpenNextMktres10\"].dropna(), ax=ax[2,1])\n    plt.tight_layout()\n                                               \n\n#CVX.N\n#AAPL.O","37a54c10":"def fixassetopenclose(in_asset_df):\n    #print(in_asset_df[\"assetCode\"].unique())\n    in_asset_df[\"final_close\"] = in_asset_df[\"close\"]\n    in_asset_df[\"final_open\"] = in_asset_df[\"open\"]\n    in_asset_df[\"adjusted_close\"] = in_asset_df[\"close\"].shift(-1)\/(1 + in_asset_df[\"returnsClosePrevRaw1\"].shift(-1))\n    in_asset_df[\"ratio\"] = in_asset_df[\"close\"]\/in_asset_df[\"adjusted_close\"]\n    idx_list = in_asset_df[(in_asset_df[\"ratio\"].round(1) <0.9) | (in_asset_df[\"ratio\"].round(1) >1.1)].index\n    for idx in idx_list: \n        affected_idx = in_asset_df[in_asset_df.index <= idx].index\n        ratio = in_asset_df.loc[idx,\"ratio\"]\n        #print(ratio)\n        in_asset_df.loc[affected_idx,[\"final_close\",\"final_open\"]] =  in_asset_df.loc[affected_idx,[\"final_close\",\"final_open\"]] \/ ratio\n        del affected_idx\n    changed_recs = in_asset_df[in_asset_df[\"final_close\"] != in_asset_df[\"close\"]].index\n    return in_asset_df.loc[changed_recs,[\"final_close\",\"final_open\"]]\n        \ndef fixopenclose(in_mkt_df):\n    assetlist = []\n    changedlist = []\n    df_group = in_mkt_df.groupby([\"assetCode\"])\n    for group in df_group:\n        assetlist.append(group[1])\n    \n    pool = Pool(4)\n    changedlist = pool.map(fixassetopenclose, assetlist)\n    pool.close()\n    \n    changed_df = pd.concat(changedlist)\n    changed_df.dropna(inplace=True)\n   \n    in_mkt_df.loc[changed_df.index,[\"close\"]] = list(changed_df[\"final_close\"])\n    in_mkt_df.loc[changed_df.index,[\"open\"]] = list(changed_df[\"final_open\"])\n       \nfixopenclose(market_train_df)","92300c72":"plotdata(\"AAPL.O\")\nplotdata(\"CVX.N\")\nplotdata(\"JPM.N\")\n","1635ca41":"news_train_df.head(1)","75c7b0aa":"def findnulls(in_mkt_df, in_news_df):\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_mkt_cols = set(in_mkt_df.columns)\n    org_news_cols = set(in_news_df.columns)\n    #----------------------------------\n    for col in in_mkt_df:\n        cols = in_mkt_df[col]\n        if fname in debug_func:\n            print(\"Market df: Num nulls in \" + col + \" is \" + str(cols[cols.isna()].shape[0]) + \" out of \" + str(cols.shape[0]))\n    for col in in_news_df:\n        cols = in_news_df[col]\n        if fname in debug_func:\n            print(\"Market df: Num nulls in \" + col + \" is \" + str(cols[cols.isna()].shape[0]) + \" out of \" + str(cols.shape[0]))\n    #---------------------------------\n    if fname in debug_func:\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")","4db25d21":"#findnulls(market_train_df, news_train_df)\n#print(\"As we can see only columns that are null are the prev columns!Let us keep them in the dataset for now.\")","bb361f54":"dic_time = {}\ndef fix_time_col(in_mkt_df, in_news_df):\n    global dic_time\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_mkt_cols = set(in_mkt_df.columns)\n    org_news_cols = set(in_news_df.columns)\n    #----------------------------------\n    if \"datetime\" in str(in_mkt_df[\"time\"].dtype):\n        in_mkt_df[\"time\"] = in_mkt_df[\"time\"].dt.date\n    if \"datetime\" in str(in_news_df[\"time\"].dtype):\n        in_news_df[\"time\"] = in_news_df[\"time\"].dt.date\n        \n    set_time = set(in_mkt_df[\"time\"].unique()) | set(in_news_df[\"time\"].unique())\n    lst_time = list(set_time)  \n    if len(dic_time) == 0:\n        dic_time= {k:v for v,k in enumerate(lst_time)}\n    else: #dictionary already exists, find only for those elemnts that are extra\n        set_time_new = set_time - set(dic_time.keys()) \n        if fname in debug_func:\n            print(\"Need to get sno for \" + str(len(set_time_new)) + \" news time.\")\n        time_max_SNo = len(dic_time)\n        dic_time_new = {k:v+time_max_SNo for v,k in enumerate(list(set_time_new))}\n        dic_time.update(dic_time_new)\n    \n    in_mkt_df[\"timeSNo\"] = in_mkt_df[\"time\"].map(dic_time)\n    in_news_df[\"timeSNo\"] = in_news_df[\"time\"].map(dic_time)\n    #-------------------------------------\n    if fname in debug_func:\n        print(\"mkt cols dropped: \" + str(org_mkt_cols - set(in_mkt_df.columns)))\n        print(\"new mkt cols added: \" + str(set(in_mkt_df.columns) - org_mkt_cols))\n        print(\"news cols dropped: \" + str(org_news_cols - set(in_news_df.columns)))\n        print(\"new news cols added: \" + str(set(in_news_df.columns) - org_news_cols))\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")\n    \ndef clean_assetName_col(in_mkt_df, in_news_df):\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_mkt_cols = set(in_mkt_df.columns)\n    org_news_cols = set(in_news_df.columns)\n    #----------------------------------\n    in_mkt_df[\"assetName\"] = in_mkt_df[\"assetName\"].str.lower()\n    in_mkt_df[\"assetName\"].replace({\"inc\":\"\",\"llc\":\"\",\"ltd\":\"\"}, inplace=True)\n    in_mkt_df[\"assetName\"] = in_mkt_df[\"assetName\"].str.strip()\n\n    in_news_df[\"assetName\"] = in_news_df[\"assetName\"].str.lower()\n    in_news_df[\"assetName\"].replace({\"inc\":\"\",\"llc\":\"\",\"ltd\":\"\"}, inplace=True)\n    in_news_df[\"assetName\"] = in_news_df[\"assetName\"].str.strip()\n   #-------------------------------------\n    if fname in debug_func:\n        print(\"mkt cols dropped: \" + str(org_mkt_cols - set(in_mkt_df.columns)))\n        print(\"new mkt cols added: \" + str(set(in_mkt_df.columns) - org_mkt_cols))\n        print(\"news cols dropped: \" + str(org_news_cols - set(in_news_df.columns)))\n        print(\"new news cols added: \" + str(set(in_news_df.columns) - org_news_cols))\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")","d376b98a":"\ndef set_assetNameCodes_SNo(in_mkt_df, in_news_df,dic_assetName, dic_assetNameSNo, dic_assetCodes, dic_assetCode, dic_assetCodeSNo):\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_mkt_cols = set(in_mkt_df.columns)\n    org_news_cols = set(in_news_df.columns)\n    #--------------------------------------\n    set_mkt_asset = set(in_mkt_df[\"assetName\"].unique())\n    set_news_asset = set(in_news_df[\"assetName\"].unique())\n    if fname in debug_func:\n        print(\"Additional asset names in mkt df:\" + str(len(set_mkt_asset - set_news_asset)) + \" out of a total of \" + str(len(set_mkt_asset)) + \" assets\")\n        print(\"Additional asset names in news df:\" + str(len(set_news_asset - set_mkt_asset)) + \" out of a total of \" + str(len(set_news_asset)) + \" assets\")\n\n    set_asset = set_news_asset | set_mkt_asset\n    set_codes = set(in_news_df[\"assetCodes\"].unique())\n    set_code = set(in_mkt_df[\"assetCode\"].unique())\n    list_asset = list(set_asset)\n    list_codes = list(set_codes)\n    list_code = list(set_code)\n    \n    if len(dic_assetName) == 0:\n        dic_assetName= {k:v for v,k in enumerate(list_asset)}\n        dic_assetNameSNo= {v:k for v,k in enumerate(list_asset)}\n        dic_assetCodes= {k:v for v,k in enumerate(list_codes)}\n        dic_assetCode= {k:v for v,k in enumerate(list_code)}\n    else: #dictionary already exists, find only for those elemnts that are extra\n        set_asset_new = set_asset - set(dic_assetName.keys()) \n        if fname in debug_func:\n            print(\"Need to get sno for \" + str(len(set_asset_new)) + \" news assts.\")\n        asset_max_SNo = len(dic_assetName)\n        dic_assetName_new = {k:v+asset_max_SNo for v,k in enumerate(list(set_asset_new))}\n        dic_assetNameSNo_new = {v+asset_max_SNo:k for v,k in enumerate(list(set_asset_new))}\n        dic_assetName.update(dic_assetName_new)\n        dic_assetNameSNo.update(dic_assetNameSNo_new)\n        \n       \n        set_assetCode_new = set_code - set(dic_assetCode.keys()) \n        if fname in debug_func:\n            print(\"Need to get sno for \" + str(len(set_assetCode_new)) + \" news assts codes.\")\n        assetCode_max_SNo = len(dic_assetCode)\n        dic_assetCode_new = {k:v+assetCode_max_SNo for v,k in enumerate(list(set_assetCode_new))}\n        dic_assetCodeSNo_new = {v+assetCode_max_SNo:k for v,k in enumerate(list(set_assetCode_new))}\n        dic_assetCode.update(dic_assetCode_new)\n        dic_assetCodeSNo.update(dic_assetCodeSNo_new)\n        \n        \n        set_codes_new = set_codes - set(dic_assetCodes.keys())\n        codes_max_SNo = len(dic_assetCodes)\n        dic_assetCodes_new = {k:v+codes_max_SNo for v,k in enumerate(list(set_codes_new))}\n        dic_assetCodes.update(dic_assetCodes_new)\n        \n    in_news_df[\"assetNameSNo\"] = in_news_df[\"assetName\"].map(dic_assetName)\n    in_mkt_df[\"assetNameSNo\"] = in_mkt_df[\"assetName\"].map(dic_assetName)\n    in_news_df[\"assetCodesSNo\"] = in_news_df[\"assetCodes\"].map(dic_assetCodes)\n    in_mkt_df[\"assetCodeSNo\"] = in_mkt_df[\"assetCode\"].map(dic_assetCode)\n    if fname in debug_func:\n        print(\"Unique Name SNo in mkt data: \" + str(len(in_mkt_df[\"assetNameSNo\"].unique())))\n        print(\"Unique Code SNo in mkt data: \" + str(len(in_mkt_df[\"assetCodeSNo\"].unique())))\n        print(\"Unique Name SNo in news data: \" + str(len(in_news_df[\"assetNameSNo\"].unique())))\n   \n#---------------------------------\n    #-------------------------------------\n    if fname in debug_func:\n        print(\"mkt cols dropped: \" + str(org_mkt_cols - set(in_mkt_df.columns)))\n        print(\"new mkt cols added: \" + str(set(in_mkt_df.columns) - org_mkt_cols))\n        print(\"news cols dropped: \" + str(org_news_cols - set(in_news_df.columns)))\n        print(\"new news cols added: \" + str(set(in_news_df.columns) - org_news_cols))\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")\n    return dic_assetName, dic_assetNameSNo, dic_assetCodes, dic_assetCode, dic_assetCodeSNo","0650d284":"def get_final_assetName(row):\n    min_mkt_assetNameSNo = row[\"min_mkt_assetNameSNo\"]\n    max_mkt_assetNameSNo = row[\"max_mkt_assetNameSNo\"]\n    min_assetNameSNo = row[\"min_assetNameSNo\"]\n    max_assetNameSNo = row[\"max_assetNameSNo\"]\n    if ((not math.isnan(min_mkt_assetNameSNo)) & (math.isnan(max_mkt_assetNameSNo))):\n        return min_assetNameSNo\n    elif ((not math.isnan(max_mkt_assetNameSNo)) & (math.isnan(min_mkt_assetNameSNo))):\n        return max_assetNameSNo\n    elif ((math.isnan(max_mkt_assetNameSNo)) & (math.isnan(min_mkt_assetNameSNo))):\n        return max_assetNameSNo\n    else:\n        return -1\n    \ndef get_replace_assetName(row):\n    min_mkt_assetNameSNo = row[\"min_mkt_assetNameSNo\"]\n    max_mkt_assetNameSNo = row[\"max_mkt_assetNameSNo\"]\n    min_assetNameSNo = row[\"min_assetNameSNo\"]\n    max_assetNameSNo = row[\"max_assetNameSNo\"]\n    if ((math.isnan(min_mkt_assetNameSNo)) & (not math.isnan(max_mkt_assetNameSNo))):\n        return min_assetNameSNo\n    elif ((math.isnan(max_mkt_assetNameSNo)) & (not math.isnan(min_mkt_assetNameSNo))):\n        return max_assetNameSNo\n    elif ((math.isnan(max_mkt_assetNameSNo)) & (math.isnan(min_mkt_assetNameSNo))):\n        return min_assetNameSNo\n    else:\n        return -1\n      \ndef replace_assetName(in_mkt_df, in_news_df):\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_mkt_cols = set(in_mkt_df.columns)\n    org_news_cols = set(in_news_df.columns)\n    #--------------------------------------\n    \n    in_news_df[\"num_rows\"] = 1\n    assetName_df = in_news_df[[\"assetCodesSNo\",\"assetNameSNo\",\"num_rows\"]].groupby(\\\n                                                [\"assetCodesSNo\",\"assetNameSNo\"], as_index=False).count()\n    if fname in debug_func:\n        print(\"Distinct assetcodes, assetname combination: \" + str(assetName_df.shape[0]) + \" out of \" + str(in_news_df.shape[0]) )\n    assetCode_df = assetName_df[[\"assetCodesSNo\", \\\n                                 \"assetNameSNo\",\"num_rows\"]].groupby([\"assetCodesSNo\"], \\\n                                 as_index=False).agg({\"assetNameSNo\": [\"max\",\"min\"], \"num_rows\": \"sum\"})\n\n    assetCode_df.columns = [\"assetCodesSNo\", \"max_assetNameSNo\", \"min_assetNameSNo\", \"count_num_rows\"]\n\n    assetCode_df = assetCode_df[assetCode_df[\"max_assetNameSNo\"] != assetCode_df[\"min_assetNameSNo\"]]\n    num_with_duplicate_assetName = assetCode_df.shape[0]\n    if fname in debug_func:\n        print(\"Rows with different assetname for same assetcode : \" + str(num_with_duplicate_assetName) + \" out of \" + str(assetName_df.shape[0]) )\n    in_news_df.drop([\"num_rows\"], axis=1, inplace=True)\n    \n    if assetCode_df.shape[0] > 0:\n        assetCode_df[\"max_assetName\"] = assetCode_df[\"max_assetNameSNo\"].map(dic_assetNameSNo)\n        assetCode_df[\"min_assetName\"] = assetCode_df[\"min_assetNameSNo\"].map(dic_assetNameSNo)\n        list_mkt_asset = list(in_mkt_df[\"assetName\"].unique())\n        dic_mkt_assetName= {k:v for v,k in enumerate(list_mkt_asset)}\n        assetCode_df[\"min_mkt_assetNameSNo\"] = assetCode_df[\"min_assetName\"].map(dic_mkt_assetName)\n        assetCode_df[\"max_mkt_assetNameSNo\"] = assetCode_df[\"max_assetName\"].map(dic_mkt_assetName)\n\n\n        assetCode_df[\"final_assetNameSNo\"] = assetCode_df.apply(get_final_assetName, axis=1)\n        assetCode_df[\"replace_assetNameSNo\"] = assetCode_df.apply(get_replace_assetName, axis=1)\n        missing_in_mkt = assetCode_df[assetCode_df[\"final_assetNameSNo\"]==-1].shape[0]\n        if fname in debug_func:\n            print(\"None of the asset names exist in market df for \" + str(missing_in_mkt) + \" entries\")\n        if num_with_duplicate_assetName == missing_in_mkt:\n            df1 = assetCode_df[[\"assetCodesSNo\",\"max_assetNameSNo\"]]\n            df1.columns = [\"assetCodesSNo\",\"assetNameSNo\"]\n            df2 = assetCode_df[[\"assetCodesSNo\",\"min_assetNameSNo\"]]\n            df2.columns = [\"assetCodesSNo\",\"assetNameSNo\"]\n            tot_df = pd.concat([df1,df2])\n            rows_to_be_dropped = pd.merge(in_news_df.reset_index(), tot_df, how=\"inner\", on=[\"assetCodesSNo\", \"assetNameSNo\"])\n            print(rows_to_be_dropped.index.unique())\n            print(\"dropping rows:\" + str(rows_to_be_dropped.shape[0]))\n            in_news_df.drop(rows_to_be_dropped[\"index\"].unique(), axis=0, inplace=True)\n        \n        assetCode_df[\"final_assetName\"] = assetCode_df[\"final_assetNameSNo\"].map(dic_assetNameSNo)\n        assetCode_df = assetCode_df[assetCode_df[\"final_assetNameSNo\"]!=-1]\n        df_temp = pd.merge(in_news_df.reset_index()[[\"index\",\"assetCodesSNo\",\"assetNameSNo\"]], \\\n                           assetCode_df[[\"replace_assetNameSNo\", \"final_assetNameSNo\",\"assetCodesSNo\",\"final_assetName\"]], \\\n                           how=\"inner\", left_on=[\"assetCodesSNo\",\"assetNameSNo\"], right_on=[\"assetCodesSNo\",\"replace_assetNameSNo\"], left_index=True).set_index(\"index\")\n        in_news_df.loc[df_temp.index, \"assetNameSNo\"] = list(df_temp[\"final_assetNameSNo\"])\n        in_news_df.loc[df_temp.index, \"assetName\"] = list(df_temp[\"final_assetName\"])\n        \n        in_news_df[\"num_rows\"] = 1\n        assetName_df = in_news_df[[\"assetCodesSNo\",\"assetNameSNo\",\"num_rows\"]].groupby(\\\n                                                    [\"assetCodesSNo\",\"assetNameSNo\"], as_index=False).count()\n        if fname in debug_func:\n            print(\"After replacement, Distinct assetcodes, assetname combination: \" + str(assetName_df.shape[0]) + \" out of \" + str(in_news_df.shape[0]) )\n       \n        assetCode_df = assetName_df[[\"assetCodesSNo\", \\\n                                     \"assetNameSNo\",\"num_rows\"]].groupby([\"assetCodesSNo\"], \\\n                                     as_index=False).agg({\"assetNameSNo\": [\"max\",\"min\"], \"num_rows\": \"sum\"})\n\n        assetCode_df.columns = [\"assetCodesSNo\", \"max_assetNameSNo\", \"min_assetNameSNo\", \"count_num_rows\"]\n\n        assetCode_df = assetCode_df[assetCode_df[\"max_assetNameSNo\"] != assetCode_df[\"min_assetNameSNo\"]]\n        if fname in debug_func:\n            print(\"After replacement, Rows with different assetname for same assetcode : \" + str(assetCode_df.shape[0]) + \" out of \" + str(assetName_df.shape[0]) )\n        \n        in_news_df.drop([\"num_rows\"], axis=1, inplace=True)\n    in_news_df.drop([\"assetCodes\",\"assetCodesSNo\"], inplace=True, axis=1)\n#-------------------------------------\n    if fname in debug_func:\n        print(\"mkt cols dropped: \" + str(org_mkt_cols - set(in_mkt_df.columns)))\n        print(\"new mkt cols added: \" + str(set(in_mkt_df.columns) - org_mkt_cols))\n        print(\"news cols dropped: \" + str(org_news_cols - set(in_news_df.columns)))\n        print(\"new news cols added: \" + str(set(in_news_df.columns) - org_news_cols))\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")\n        \n        ","980f8aa8":"\ndef scale_col(in_news_df, col):\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_news_cols = set(in_news_df.columns)\n    #----------------------------------\n    global dic_rng\n    global dic_min\n    if col not in dic_rng.keys(): \n        col_max = in_news_df[col].max()\n        col_min = in_news_df[col].min()\n        col_rng = col_max - col_min\n        dic_rng[col] = col_rng\n        dic_min[col] = col_min\n    else:\n        col_min = dic_min[col] \n        col_rng = dic_rng[col]\n\n    in_news_df[col] = (in_news_df[col]-col_min)\/col_rng\n    if col_min < 0:\n        in_news_df[col] = 2*in_news_df[col] - 1\n    print(col, in_news_df[col].min(), in_news_df[col].max())\n    #-------------------------------------\n    if fname in debug_func:\n        print(\"news cols dropped: \" + str(org_news_cols - set(in_news_df.columns)))\n        print(\"new news cols added: \" + str(set(in_news_df.columns) - org_news_cols))\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")\n        \ndef drop_extra_news_cols(in_news_df):\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_news_cols = set(in_news_df.columns)\n    #----------------------------------\n    news_drop_cols = [\"sourceTimestamp\",\"firstCreated\",\"sourceId\",\"headline\",\"urgency\",\"takeSequence\",\"provider\",\"audiences\",\"bodySize\",\\\n                      \"headlineTag\",\"sentenceCount\",\"wordCount\",\"firstMentionSentence\",\"sentimentClass\",\"sentimentWordCount\",\"companyCount\", \"marketCommentary\",\n                      \"volumeCounts12H\",\"volumeCounts24H\",\"volumeCounts3D\",\"volumeCounts5D\",\"volumeCounts7D\"]\n\n    news_drop_cols = [col  for col in news_drop_cols if col in in_news_df.columns ]\n    in_news_df.drop(news_drop_cols, axis=1, inplace=True)\n    #-------------------------------------\n    if fname in debug_func:\n        print(\"news cols dropped: \" + str(org_news_cols - set(in_news_df.columns)))\n        print(\"new news cols added: \" + str(set(in_news_df.columns) - org_news_cols))\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")\n    \n    \ndef drop_irrelevant_news(in_news_df):\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_news_cols = set(in_news_df.columns)\n    #----------------------------------\n    if \"relevance\" in in_news_df.columns:\n        irrelevant_news = in_news_df[in_news_df[\"relevance\"] < 0.3].index\n        print(\"Num low relevance news:\", len(irrelevant_news))\n        in_news_df.drop(irrelevant_news, axis=0, inplace=True)\n        del irrelevant_news\n        gc.collect()\n    #-------------------------------------\n    if fname in debug_func:\n        print(\"news cols dropped: \" + str(org_news_cols - set(in_news_df.columns)))\n        print(\"new news cols added: \" + str(set(in_news_df.columns) - org_news_cols))\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")\n \n\ndef drop_neutral_news(in_news_df):\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_news_cols = set(in_news_df.columns)\n    #----------------------------------\n    if \"sentimentPositive\" in in_news_df.columns:\n        in_news_df[\"netSentimentPositive\"] = (in_news_df[\"sentimentPositive\"] - in_news_df[\"sentimentNegative\"]) \n        neutral_news = in_news_df[in_news_df[\"netSentimentPositive\"].abs() < 0.05].index\n        in_news_df.drop(neutral_news, axis=0, inplace=True)\n        in_news_df[\"netSentimentPositive\"] = in_news_df[\"netSentimentPositive\"] * (1 - in_news_df[\"sentimentNeutral\"])\n        print(\"Num neutral news:\", len(neutral_news))\n        del neutral_news\n        gc.collect()\n        in_news_df.drop([\"sentimentNegative\",\"sentimentNeutral\",\"sentimentPositive\"], axis=1, inplace=True)\n   #-------------------------------------\n    if fname in debug_func:\n        print(\"news cols dropped: \" + str(org_news_cols - set(in_news_df.columns)))\n        print(\"new news cols added: \" + str(set(in_news_df.columns) - org_news_cols))\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")\n\n    \ndef gen_netnovelty_col(in_news_df):\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_news_cols = set(in_news_df.columns)\n    #----------------------------------\n    # if there is a news in 5 to 7 days then count will be more than 1. This news should have less importance as it is an old news\n    if \"noveltyCount12H\" in in_news_df.columns:\n        arr_novelty = np.array(in_news_df[[\"noveltyCount12H\",\"noveltyCount24H\",\"noveltyCount3D\",\"noveltyCount5D\",\"noveltyCount7D\"]])\n\n        in_news_df[\"inv_netNovelty\"] = list(1\/(np.argmax(arr_novelty, axis=1)+2))\n        col = \"inv_netNovelty\"\n        scale_col(in_news_df, col)\n        in_news_df.drop([\"noveltyCount12H\",\"noveltyCount24H\",\"noveltyCount3D\",\"noveltyCount5D\",\"noveltyCount7D\"], axis=1, inplace=True)\n        in_news_df[\"SenSR\"] = in_news_df[\"netSentimentPositive\"] * in_news_df[\"relevance\"] * (in_news_df[\"inv_netNovelty\"])\n        in_news_df.drop([\"relevance\",\"netSentimentPositive\",\"inv_netNovelty\"],axis=1, inplace=True)\n  #-------------------------------------\n    if fname in debug_func:\n        print(\"news cols dropped: \" + str(org_news_cols - set(in_news_df.columns)))\n        print(\"new news cols added: \" + str(set(in_news_df.columns) - org_news_cols))\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")\n \n\n","46d79b21":"\ndef create_sector(in_asset_df):\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_news_cols = set(in_asset_df.columns)\n    #----------------------------------\n    global dic_asset_sector\n    global set_sector \n    global sector_col\n    global dic_sectorSNo\n    \n    in_asset_df[\"sector\"] = in_asset_df[\"subjects\"].apply(lambda x: str(eval(x) & set_sector))\n    for col in set_sector:\n        #print(col)\n        in_asset_df[col] = in_asset_df[\"sector\"].apply(lambda x: col in eval(x))\n\n    asset_df = in_asset_df[sector_col].groupby([\"assetNameSNo\"]).sum()\n    asset_df1 = asset_df.sum(axis=1)\n    asset_unknown_sector = list(asset_df1[asset_df1==0].index)\n    if len(asset_unknown_sector) > 0:\n        print(\"Could not find sector of \", len(asset_unknown_sector) , \" assets\")\n    #asset_df.drop(asset_unknown_sector, axis=0, inplace=True)\n    asset_df[\"sectorNameSNo\"] = asset_df.values.argmax(axis=1)\n    asset_df[\"sectorName\"] = asset_df[\"sectorNameSNo\"].map(dic_sectorSNo)\n    asset_df.loc[asset_unknown_sector,[\"sectorNameSNo\",\"sectorName\"]] = [-1,\"Unknown\"]\n    asset_df = asset_df[[\"sectorName\",\"sectorNameSNo\"]]\n    asset_df.reset_index(inplace=True)\n    #-------------------------------------\n    if fname in debug_func:\n        print(\"news cols dropped: \" + str(org_news_cols - set(in_asset_df.columns)))\n        print(\"new news cols added: \" + str(set(in_asset_df.columns) - org_news_cols))\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")\n    return asset_df\n        \n\ndef get_sector_name(in_news_df, forpred=False):\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_news_cols = set(in_news_df.columns)\n    #--------------------------------------\n    global dic_asset_sector\n    if \"sectorName\" not in in_news_df.columns:\n        in_news_df[\"sectorName\"] = \"\"\n        in_news_df[\"sectorNameSNo\"] = -1\n    if len(dic_asset_sector) > 0:\n        in_news_df[\"sectorName\"] = in_news_df[\"assetNameSNo\"].map(dic_asset_sector)\n        #if forpred==True:\n        #    in_news_df[\"sectorName\"].fillna(\"Unknown\", inplace=True)\n        #else:\n        in_news_df[\"sectorName\"].fillna(\"\", inplace=True)\n        in_news_df[\"sectorNameSNo\"] = in_news_df[\"sectorName\"].map(dic_sector)  \n    if 1==1: #forpred==False:  \n        rec_index = in_news_df[in_news_df[\"sectorName\"] == \"\"].index\n        lst_subjects = in_news_df.loc[rec_index][\"subjects\"].unique()\n        dic_subjects = {k:v for v,k in enumerate(list(lst_subjects))}\n        dic_subjectsSNo = {v:k for v,k in enumerate(list(lst_subjects))}\n        in_news_df[\"subjectsSNo\"] = -1\n        in_news_df.loc[rec_index,[\"subjectsSNo\"]] = in_news_df.loc[rec_index][\"subjects\"].map(dic_subjects)\n        in_asset_df = in_news_df.loc[rec_index]\n    \n        in_asset_df[\"tempCol\"] = 1\n        in_asset_df = in_asset_df[[\"assetNameSNo\",\"subjectsSNo\",\"tempCol\"]].groupby([\"assetNameSNo\",\"subjectsSNo\"], as_index=False).first()\n        in_asset_df.drop([\"tempCol\"], inplace=True, axis=1)\n        in_asset_df[\"subjects\"] = in_asset_df[\"subjectsSNo\"].map(dic_subjectsSNo)\n    \n        if fname in debug_func:\n            print(\"New asset records in news : \" + str(in_asset_df.shape[0]) + \" out of \" + str(in_news_df.shape[0]) )\n            print(\"***************************\")\n        all_news = []\n        if in_asset_df.shape[0] > 0:\n            if forpred==False:\n                maxSNo = in_asset_df[\"assetNameSNo\"].max()\n                partition_size = (maxSNo \/\/ 4) + 1\n\n                for i in range(4):\n                    minr = i*partition_size\n                    maxr = (i+1)*partition_size\n                    all_news.append(in_asset_df[(in_asset_df[\"assetNameSNo\"] >= minr) & (in_asset_df[\"assetNameSNo\"] < maxr)])\n\n                pool = Pool(4)\n                all_df = pool.map(create_sector, all_news)\n\n                asset_df = pd.concat(all_df)  \n                pool.close()\n            else:\n                asset_df = create_sector(in_asset_df)\n\n            asset_df1 = asset_df.set_index([\"assetNameSNo\"])[[\"sectorName\"]]\n            new_dict = asset_df1.to_dict()[\"sectorName\"]\n            dic_asset_sector.update(new_dict)\n          \n            #print(dic_asset_sector)\n\n            merged_df = pd.merge(in_news_df.reset_index()[[\"assetNameSNo\",\"index\"]], asset_df[[\"assetNameSNo\",\"sectorName\",\"sectorNameSNo\"]], \\\n                                 on=[\"assetNameSNo\"], how=\"left\").set_index(\"index\")\n            #print(merged_df.head(1).T)\n            in_news_df.loc[merged_df.index,[\"sectorName\",\"sectorNameSNo\"]] = merged_df.loc[merged_df.index,[\"sectorName\",\"sectorNameSNo\"]]\n    if 1==1:  \n        if \"subjects\" in in_news_df.columns:\n            in_news_df.drop(\"subjects\", axis=1, inplace=True)\n        \n    #print(in_news_df[\"sectorNameSNo\"].unique())\n    #print(in_news_df[\"sectorName\"].unique())\n    #-------------------------------------\n    if fname in debug_func:\n        print(\"news cols dropped: \" + str(org_news_cols - set(in_news_df.columns)))\n        print(\"new news cols added: \" + str(set(in_news_df.columns) - org_news_cols))\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")","0945a6e1":"\ndef merge_mkt_news(in_mkt_df, in_news_df):\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_mkt_cols = set(in_mkt_df.columns)\n    org_news_cols = set(in_news_df.columns)\n    #----------------------------------\n    global dic_asset_sector\n    in_mkt_df[\"sectorName\"] = in_mkt_df[\"assetNameSNo\"].map(dic_asset_sector)\n    df_unknown_sector = in_mkt_df[in_mkt_df[\"sectorName\"].isna()]\n    in_mkt_df[\"sectorName\"].fillna(\"Unknown\", inplace=True)\n    print(\"no of market recs with unresolved sector: \" + str(len(df_unknown_sector[\"assetNameSNo\"].unique())))\n    #in_mkt_df.drop(df_unknown_sector.index, inplace=True)\n    \n    in_mkt_df[\"sectorNameSNo\"] = in_mkt_df[\"sectorName\"].map(dic_sector)\n    \n    in_mkt_df[\"mkt_cap\"] = ((in_mkt_df[\"close\"] + in_mkt_df[\"open\"])\/2)  * in_mkt_df[\"volume\"] \n    \n    \n    in_news_df1 = in_news_df[[\"timeSNo\",\"assetNameSNo\",\"SenSR\"]].groupby([\"assetNameSNo\",\"timeSNo\"], as_index=False).sum()\n    if \"SenSR\" in in_mkt_df.columns:\n        in_mkt_df.drop([\"SenSR\"], axis=1, inplace=True)\n    merged_df = pd.merge(in_mkt_df.reset_index(), in_news_df1,  on= [\"assetNameSNo\",\"timeSNo\"], how=\"inner\").set_index(\"index\")\n    in_mkt_df[\"SenSR\"] = 0\n    in_mkt_df.loc[merged_df.index, [\"SenSR\"]] = merged_df.loc[merged_df.index, [\"SenSR\"]]\n\n    in_mkt_df[\"weighted_SenSR\"] = in_mkt_df[\"SenSR\"] * in_mkt_df[\"mkt_cap\"]\n    #---------------------------------------------------\n    if fname in debug_func:\n        print(\"mkt cols dropped: \" + str(org_mkt_cols - set(in_mkt_df.columns)))\n        print(\"new mkt cols added: \" + str(set(in_mkt_df.columns) - org_mkt_cols))\n        print(\"news cols dropped: \" + str(org_news_cols - set(in_news_df.columns)))\n        print(\"new news cols added: \" + str(set(in_news_df.columns) - org_news_cols))\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")","be6e555e":"market_train_df.head(1)","ae05e104":"def create_lag(df_code,n_lag=[3,7,14,],shift_size=1):\n    code = df_code['assetCodeSNo'].unique()\n    \n    for col in return_features:\n        for window in n_lag:\n            rolled = df_code[col].shift(shift_size).rolling(window=window)\n            lag_mean = rolled.mean()\n            lag_max = rolled.max()\n            lag_min = rolled.min()\n            lag_std = rolled.std()\n            df_code['%s_lag_%s_mean'%(col,window)] = lag_mean\n            df_code['%s_lag_%s_max'%(col,window)] = lag_max\n            df_code['%s_lag_%s_min'%(col,window)] = lag_min\n#             df_code['%s_lag_%s_std'%(col,window)] = lag_std\n    return df_code.fillna(-1)\n\ndef generate_lag_features(df,n_lag = [3,7,14]):\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_mkt_cols = set(df.columns)\n    #----------------------------------\n    all_df = []\n    df_codes = df.groupby('assetCodeSNo')\n    df_codes = [df_code[1][['timeSNo','assetCodeSNo']+return_features] for df_code in df_codes]\n    \n    \n    pool = Pool(4)\n    all_df = pool.map(create_lag, df_codes)\n    \n    new_df = pd.concat(all_df)  \n    new_df.drop(return_features,axis=1,inplace=True)\n    pool.close()\n     #-------------------------------------\n    if fname in debug_func:\n        print(\"mkt cols dropped: \" + str(org_mkt_cols - set(df.columns)))\n        print(\"new mkt cols added: \" + str(set(df.columns) - org_mkt_cols))\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")\n    return new_df","686c7c50":"def create_sector_lag(df_code,n_lag=[3,7,14,],shift_size=1):\n    for col in sector_return_features:\n        for window in n_lag:\n            if df_code[\"sectorNameSNo\"].unique()[0] == -1:\n                df_code['%s_lag_%s_mean'%(col,window)] = -1\n                df_code['%s_lag_%s_max'%(col,window)] = -1\n                df_code['%s_lag_%s_min'%(col,window)] = -1\n            else:\n                rolled = df_code[[col,\"mkt_cap\"]].shift(shift_size).rolling(window=window)\n                lag_mean = rolled[col].sum()\/ rolled[\"mkt_cap\"].sum()\n                lag_max = rolled[col].max()\/ rolled[\"mkt_cap\"].sum()\n                lag_min = rolled[col].mean()\/ rolled[\"mkt_cap\"].sum()\n                df_code['%s_lag_%s_mean'%(col,window)] = lag_mean\n                df_code['%s_lag_%s_max'%(col,window)] = lag_max\n                df_code['%s_lag_%s_min'%(col,window)] = lag_min\n    #             df_code['%s_lag_%s_std'%(col,window)] = lag_std\n    return df_code.fillna(-1)\n\ndef generate_sector_lag_features(df,n_lag = [3,7,14]):\n    fname = sys._getframe().f_code.co_name\n    if fname in debug_func:\n        print(\"Start \" + fname)\n    start = datetime.now()\n    org_mkt_cols = set(df.columns)\n    #----------------------------------\n    all_df = []\n    df_codes = df[['timeSNo','sectorNameSNo',\"mkt_cap\"] + sector_return_features].groupby([\"timeSNo\",'sectorNameSNo'], as_index=False).sum()\n    df_codes = df_codes.groupby('sectorNameSNo')\n    df_codes = [df_code[1][['timeSNo','sectorNameSNo',\"mkt_cap\"] + sector_return_features] for df_code in df_codes]\n    #print('total %s df'%len(df_codes))\n    \n    pool = Pool(4)\n    all_df = pool.map(create_sector_lag, df_codes)\n    \n    new_df = pd.concat(all_df)  \n    new_df.drop(sector_return_features + [\"mkt_cap\"],axis=1,inplace=True)\n    pool.close()\n    #-------------------------------------\n    if fname in debug_func:\n        print(\"mkt cols dropped: \" + str(org_mkt_cols - set(df.columns)))\n        print(\"new mkt cols added: \" + str(set(df.columns) - org_mkt_cols))\n        end = datetime.now()\n        print(\"End \" + fname + \": \" + str(end-start) + \"---------------------\")\n    return new_df.reset_index()","28fda979":"\nfix_time_col(market_train_df, news_train_df)\nmarket_train_df = market_train_df.loc[market_train_df['time']>=date(2010, 1, 1)]\nnews_train_df = news_train_df.loc[news_train_df['time']>=date(2010, 1, 1)]\nclean_assetName_col(market_train_df, news_train_df)\ndic_assetName, dic_assetNameSNo, dic_assetCodes, dic_assetCode, dic_assetCodeSNo = set_assetNameCodes_SNo(market_train_df, news_train_df,dic_assetName, \\\n                                                                                                          dic_assetNameSNo, dic_assetCodes, dic_assetCode, dic_assetCodeSNo)\n\nreplace_assetName(market_train_df, news_train_df)\ndrop_extra_news_cols(news_train_df)\ndrop_irrelevant_news(news_train_df)\ndrop_neutral_news(news_train_df)\ngen_netnovelty_col(news_train_df)\n\n\n","5805c39b":"get_sector_name(news_train_df)\nmerge_mkt_news(market_train_df, news_train_df)\n","beaf4d39":"# return_features = ['close']\n# new_df = generate_lag_features(market_train_df,n_lag = 5)\n# market_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])","90e19a28":"import gc\ndel news_train_df\ngc.collect()","2c61ec3b":"market_train_df.tail(1)","a1f6988c":"\nnew_df = generate_lag_features(market_train_df,n_lag=n_lag)\nmarket_train_df = pd.merge(market_train_df,new_df,how='left',on=['timeSNo','assetCodeSNo'])\n","01e991ac":"new_df = generate_sector_lag_features(market_train_df,n_lag=n_lag)\nmarket_train_df = pd.merge(market_train_df,new_df,how='left',on=['timeSNo','sectorNameSNo'])\n","c2e21f06":"def mis_impute(data):\n    for i in data.columns:\n        if data[i].dtype == \"object\":\n            data[i] = data[i].fillna(\"other\")\n        elif (data[i].dtype == \"int64\" or data[i].dtype == \"float64\"):\n            data[i] = data[i].fillna(data[i].mean())\n        else:\n            pass\n    return data\n\nmarket_train_df = mis_impute(market_train_df)","0b5acafe":"for col in market_train_df:\n    cols = market_train_df[col]\n    print(\"Market df: Num nulls in \" + col + \" is \" + str(cols[cols.isna()].shape[0]) + \" out of \" + str(cols.shape[0]))","cb036b57":"market_train_df.head(10)","12fff7c9":"from sklearn.preprocessing import LabelEncoder\n\nup = market_train_df['returnsOpenNextMktres10'] >= 0\n\n\nuniverse = market_train_df['universe'].values\nd = market_train_df['time']\n\nfcol = [c for c in market_train_df if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'audiences', \n                                             'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider', \n                                             'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', \"timeSNo\",\n                                                'universe','sourceTimestamp', \"sectorName\",\"index\",\"mkt_cap\",\"open\",\"close\"]]\n\n\nX = market_train_df[fcol].values\nup = up.values\nr = market_train_df.returnsOpenNextMktres10.values\ndel market_train_df\ngc.collect()\n\n# Scaling of X values\n# It is good to keep these scaling values for later\nmins = np.min(X, axis=0)\nmaxs = np.max(X, axis=0)\nrng = maxs - mins\nX = 1 - ((maxs - X) \/ rng)\n\n# Sanity check\nassert X.shape[0] == up.shape[0] == r.shape[0]\n\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nimport time\n\nX_train, X_test, up_train, up_test, r_train, r_test,u_train,u_test,d_train,d_test = \\\nmodel_selection.train_test_split(X, up, r,universe,d,test_size=0.25, random_state=99)\nprint(X_train.shape)\n\ntrain_data = lgb.Dataset(X, label=up.astype(int), feature_name=fcol, categorical_feature=[\"assetCodeSNo\",\"assetNameSNo\",\"sectorNameSNo\"], free_raw_data=False)\ntest_data = lgb.Dataset(X_test, label=up_test.astype(int), feature_name=fcol, categorical_feature=[\"assetCodeSNo\",\"assetNameSNo\",\"sectorNameSNo\"], free_raw_data=False)","2f69f24e":"del X, X_train\ngc.collect()","95bdaaa8":"fcol","1eeafe66":"#for var, obj in locals().items():\n#    print (var, sys.getsizeof(obj))","3e16215d":"# these are tuned params I found\nx_1 = [0.19000424246380565, 2452, 212, 328, 202]\nx_2 = [0.19016805202090095, 2583, 213, 312, 220]\n\ndef exp_loss(p,y):\n    y = y.get_label()\n#     p = p.get_label()\n    grad = -y*(1.0-1.0\/(1.0+np.exp(-y*p)))\n    hess = -(np.exp(y*p)*(y*p-1)-1)\/((np.exp(y*p)+1)**2)\n    \n    return grad,hess\n\nparams_1 = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n#         'objective': 'regression',\n        'learning_rate': x_1[0],\n        'num_leaves': x_1[1],\n        'min_data_in_leaf': x_1[2],\n#         'num_iteration': x_1[3],\n        'num_iteration': 239,\n        'max_bin': x_1[4],\n        'verbose': 1\n    }\n\nparams_2 = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n#         'objective': 'regression',\n        'learning_rate': x_2[0],\n        'num_leaves': x_2[1],\n        'min_data_in_leaf': x_2[2],\n#         'num_iteration': x_2[3],\n        'num_iteration': 272,\n        'max_bin': x_2[4],\n        'verbose': 1\n    }\n\ngbm_1 = lgb.train(params_1,\n        train_data,\n        num_boost_round=100,\n        valid_sets=test_data,\n        early_stopping_rounds=5,\n#         fobj=exp_loss,\n        )\n\ngbm_2 = lgb.train(params_2,\n        train_data,\n        num_boost_round=100,\n        valid_sets=test_data,\n        early_stopping_rounds=5,\n#         fobj=exp_loss,\n        )\n\n","5780d5ee":"lgb.plot_importance(gbm_1, importance_type='split', max_num_features=20)\n","c5e69471":"lgb.plot_importance(gbm_1, importance_type='gain', max_num_features=20)","56421c41":"confidence_test = (gbm_1.predict(X_test) + gbm_2.predict(X_test))\/2\nconfidence_test = (confidence_test-confidence_test.min())\/(confidence_test.max()-confidence_test.min())\nconfidence_test = confidence_test*2-1\nprint(max(confidence_test),min(confidence_test))\n\n# calculation of actual metric that is used to calculate final score\nr_test = r_test.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_test * r_test * u_test\ndata = {'day' : d_test, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_test = mean \/ std\nprint(score_test)\n","48851911":"del X_test, d_train\ngc.collect()","466b3e0b":"def process_mkt_news_data(market_obs_df, news_obs_df):   \n    global dic_assetName, dic_assetNameSNo, dic_assetCodes, dic_assetCode, dic_assetCodeSNo\n    fix_time_col(market_obs_df, news_obs_df)\n    curtime = market_obs_df[\"time\"].unique()\n    print(curtime)\n    found=1\n    if len(total_market_obs_df)>1:\n        while found==1:\n            prev_df = total_market_obs_df[-1]\n            if prev_df[\"time\"].unique() == curtime:\n                print(\"removing------------------------\")\n                total_market_obs_df.pop(-1)\n                total_news_obs_df.pop(-1)\n            else:\n                found=0\n    clean_assetName_col(market_obs_df, news_obs_df)\n    dic_assetName, dic_assetNameSNo, dic_assetCodes, dic_assetCode, dic_assetCodeSNo = set_assetNameCodes_SNo(market_obs_df, news_obs_df,dic_assetName, dic_assetNameSNo, \\\n                                                                                            dic_assetCodes,dic_assetCode, dic_assetCodeSNo)\n    drop_extra_news_cols(news_obs_df)\n    drop_irrelevant_news(news_obs_df)\n    drop_neutral_news(news_obs_df)\n    gen_netnovelty_col(news_obs_df)\n    get_sector_name(news_obs_df, True)\n    \n    return dic_assetName, dic_assetNameSNo, dic_assetCodes, dic_assetCode, dic_assetCodeSNo, curtime","44217af7":"\nif len(total_market_obs_df) == 0:\n    for i in range(len(prv_mkt_data)):\n        mkt_data = prv_mkt_data[i]\n        news_data = prv_news_data[i]\n        mkt_data.drop([\"universe\",\"returnsOpenNextMktres10\"], inplace=True, axis=1)\n        dic_assetName, dic_assetNameSNo, dic_assetCodes, dic_assetCode, dic_assetCodeSNo, curtime = process_mkt_news_data(mkt_data, news_data)\n        total_market_obs_df.append(mkt_data)\n        total_news_obs_df.append(news_data)\ndef make_random_predictions_new(market_obs_df, news_obs_df,predictions_template_df):  \n    global n_days\n    global prep_time\n    global prediction_time\n    global packaging_time\n    global total_market_obs_df\n    global total_news_obs_df\n    global dic_assetName, dic_assetNameSNo, dic_assetCodes, dic_assetCode, dic_assetCodeSNo\n    n_days +=1\n    if (n_days%50==0):\n        print(n_days,end=' ')\n    t = time.time()\n\n    dic_assetName, dic_assetNameSNo, dic_assetCodes, dic_assetCode, dic_assetCodeSNo, curtime = process_mkt_news_data(market_obs_df, news_obs_df)\n\n    total_market_obs_df.append(market_obs_df)\n    total_news_obs_df.append(news_obs_df)\n    if len(total_market_obs_df)==1:\n        history_df = total_market_obs_df[0]\n        history_news_df = total_news_obs_df[0]\n    else:\n        history_df = pd.concat(total_market_obs_df[-(np.max(n_lag)+1):], sort=False)\n        history_news_df = pd.concat(total_news_obs_df[-(np.max(n_lag)+1):], sort=False)\n    history_df.sort_values([\"timeSNo\"], inplace=True)\n    history_df.index = list(range(history_df.shape[0]))\n    history_news_df.sort_values([\"timeSNo\"], inplace=True)\n    history_news_df.index = list(range(history_news_df.shape[0]))\n    fixopenclose(history_df)\n    merge_mkt_news(history_df, history_news_df)\n    \n    new_df = generate_lag_features(history_df,n_lag=[3,7,14])\n    history_df = pd.merge(history_df,new_df,how='left',on=['timeSNo','assetCodeSNo'])\n    new_df = generate_sector_lag_features(history_df,n_lag=[3,7,14])\n    history_df = pd.merge(history_df,new_df,how='left',on=['timeSNo','sectorNameSNo'])\n    history_df = mis_impute(history_df)\n    history_df.dropna(inplace=True)\n    market_obs_df = history_df[history_df[\"time\"].isin(curtime)]\n    X_live = market_obs_df[fcol].values\n    X_live = 1 - ((maxs - X_live) \/ rng)\n   \n    prep_time += time.time() - t\n    \n    t = time.time()\n    lp = (gbm_1.predict(X_live) + gbm_2.predict(X_live))\/2\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    \n    confidence = lp\n    confidence = (confidence-confidence.min())\/(confidence.max()-confidence.min())\n    confidence = confidence * 2 - 1\n    \n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n    return market_obs_df","36b47c25":"market_obs_df, news_obs_df, predictions_template_df = next(days)","bfb3bfdb":"\nmarket_obs_df = make_random_predictions_new(market_obs_df, news_obs_df,predictions_template_df)","d6a7e07d":"market_obs_df.head(1)","4bd76d3e":"\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    market_obs_df = make_random_predictions_new(market_obs_df, news_obs_df,predictions_template_df)\n    \nprint('Done!')","bd89e891":"#prediction\n\n    \n    \nenv.write_submission_file()\nsub  = pd.read_csv(\"submission.csv\")","9fa675be":"1. Remove assetCodeSNo as a feature\n2. Specify sectorSNo as a feature\n3. Specify sectorSNo as categorical\n4. Drop records with unidentified sector\n5. Fix open close for test data as well\n6. Remove lag features for open and close\nTODO: Add assetCode as categorical feature"}}