{"cell_type":{"9480de9c":"code","d3949ff2":"code","21e48af1":"code","b6b1dc8a":"code","d3b0315a":"code","df02f98c":"code","76b451e2":"code","1f729718":"code","55a3724d":"code","9e96e135":"code","5f0b1f31":"code","3e62caed":"code","f803f070":"code","e55e6b94":"code","5b8822e5":"code","bbe6f519":"code","09b8a867":"code","d13f43ec":"code","2ce00eb4":"code","f9fe8d93":"code","a1d0e7cd":"code","5ba69ff9":"code","17f313de":"code","eafd27ac":"code","ffba35da":"code","00c8c2bd":"code","d393b208":"code","7ed35949":"code","05172524":"code","bac37a65":"code","67477413":"code","515bf873":"code","fcc4550d":"code","467d03d7":"code","0212cf85":"code","a2e58b1b":"code","5dfdc139":"code","b0d05b30":"code","372f2562":"code","0bf4787e":"code","f7e1ac9a":"code","1874f0cc":"code","3cd542bc":"code","113f79a9":"code","011ee894":"code","437ee34a":"code","d39c03bf":"code","2259bbd7":"code","b99767ce":"code","08ba6d68":"code","1e10aba6":"code","d424467f":"code","ecdc5666":"code","8e5ab94f":"code","632dc0ae":"code","d628db2e":"code","45056a0d":"code","e7572687":"code","32110f4e":"code","389e17cb":"code","86a1b2df":"code","780f831a":"code","addf8188":"code","f80705a3":"code","15c9159f":"code","773f1b22":"code","4f174769":"code","1bf026d1":"code","d37d46f1":"markdown","4ca0463c":"markdown","88f8d30b":"markdown","0ec80a6b":"markdown","2c404372":"markdown","c48ebfe0":"markdown","2b84f346":"markdown","e3cb669a":"markdown","0790da00":"markdown","ca408459":"markdown","8faa4b0e":"markdown","71d2fe07":"markdown","a95eb007":"markdown","c6a52853":"markdown","bc40adba":"markdown","42adfe6d":"markdown","23c958ed":"markdown","14aeb05f":"markdown","55e98bc0":"markdown","e1e5410b":"markdown"},"source":{"9480de9c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (12, 8)\n\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sklearn.base as skb\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nimport sklearn.linear_model as sklm\nimport random\nseed = 12\nnp.random.seed(seed)\n\nfrom datetime import date","d3949ff2":"# important funtions\ndef datasetShape(df):\n    rows, cols = df.shape\n    print(\"The dataframe has\",rows,\"rows and\",cols,\"columns.\")\n    \n# select numerical and categorical features\ndef divideFeatures(df):\n    numerical_features = df.select_dtypes(include=[np.number])\n    categorical_features = df.select_dtypes(include=[np.object])\n    return numerical_features, categorical_features","21e48af1":"base = '\/kaggle\/input\/hackerearth-machine-learning-exhibit-art\/'\ndata_file = base + \"train.csv\"\ndf = pd.read_csv(data_file)\ndf.head()","b6b1dc8a":"data_file = base + \"test.csv\"\ndf_test = pd.read_csv(data_file)\ndf_test.head()","d3b0315a":"# set target feature\ntargetFeature='Cost'","df02f98c":"# check dataset shape\ndatasetShape(df)","76b451e2":"df.drop(['Customer Id', 'Artist Name'], inplace=True, axis=1)\ndf_test.drop(['Artist Name'], inplace=True, axis=1)","1f729718":"# check for duplicates\nprint(df.shape)\ndf.drop_duplicates(inplace=True)\nprint(df.shape)","55a3724d":"df.info()","9e96e135":"df_test.info()","5f0b1f31":"cont_features, cat_features = divideFeatures(df)\ncat_features","3e62caed":"# boxplots of numerical features for outlier detection\n\nfig = plt.figure(figsize=(16,16))\nfor i in range(len(cont_features.columns)):\n    fig.add_subplot(3, 4, i+1)\n    sns.boxplot(y=cont_features.iloc[:,i])\nplt.tight_layout()\nplt.show()","f803f070":"# distplots for categorical data\n\nfig = plt.figure(figsize=(16,30))\nfor i in range(len(cat_features.columns[:-3])):\n    fig.add_subplot(9, 5, i+1)\n    cat_features.iloc[:,i].hist()\n    plt.xlabel(cat_features.columns[i])\nplt.tight_layout()\nplt.show()","e55e6b94":"sns.pairplot(df)\nplt.show()","5b8822e5":"# correlation heatmap for all features\ncorr = df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask = mask, annot=True)\nplt.show()","bbe6f519":"# plot sample skewed feature\nplt.figure(figsize=(10,4))\nsns.distplot(df[targetFeature])\nplt.show()","09b8a867":"skewed_features = cont_features.apply(lambda x: x.skew()).sort_values(ascending=False)\nskewed_features","d13f43ec":"# plot missing values\n\ndef calc_missing(df):\n    missing = df.isna().sum().sort_values(ascending=False)\n    missing = missing[missing != 0]\n    missing_perc = missing\/df.shape[0]*100\n    return missing, missing_perc\n\nif df.isna().any().sum()>0:\n    missing, missing_perc = calc_missing(df)\n    missing.plot(kind='bar',figsize=(14,5))\n    plt.title('Missing Values')\n    plt.show()\nelse:\n    print(\"No Missing Values\")","2ce00eb4":"# remove all columns having no values\ndf.dropna(axis=1, how=\"all\", inplace=True)\ndf.dropna(axis=0, how=\"all\", inplace=True)\ndatasetShape(df)","f9fe8d93":"df.describe()","a1d0e7cd":"def fillNan(df, col, value):\n    df[col].fillna(value, inplace=True)","5ba69ff9":"fillNan(df, 'Artist Reputation', df['Artist Reputation'].median())\nfillNan(df_test, 'Artist Reputation', df['Artist Reputation'].median())\ndf['Artist Reputation'].isna().any()","17f313de":"fillNan(df, 'Height', df['Height'].median())\nfillNan(df_test, 'Height', df['Height'].median())\ndf['Height'].isna().any()","eafd27ac":"fillNan(df, 'Width', df['Width'].median())\nfillNan(df_test, 'Width', df['Width'].median())\ndf['Width'].isna().any()","ffba35da":"fillNan(df, 'Weight', df['Weight'].median())\nfillNan(df_test, 'Weight', df['Weight'].median())\ndf['Weight'].isna().any()","00c8c2bd":"fillNan(df, 'Transport', df['Transport'].mode()[0])\nfillNan(df_test, 'Transport', df['Transport'].mode()[0])\ndf['Transport'].isna().any()","d393b208":"fillNan(df, 'Remote Location', df['Remote Location'].mode()[0])\nfillNan(df_test, 'Remote Location', df['Remote Location'].mode()[0])\ndf['Remote Location'].isna().any()","7ed35949":"fillNan(df, 'Material', df['Material'].mode()[0])\nfillNan(df_test, 'Material', df['Material'].mode()[0])\ndf['Material'].isna().any()","05172524":"df.isna().any().sum()","bac37a65":"# score addition\ndf['state'] = df['Customer Location'].map(lambda x:x.split()[-2])\ndf.drop('Customer Location', inplace=True, axis=1)\ndf_test['state'] = df_test['Customer Location'].map(lambda x:x.split()[-2])\ndf_test.drop('Customer Location', inplace=True, axis=1)\ndf_test.head()","67477413":"df['Scheduled Date'] = pd.to_datetime(df['Scheduled Date'])\ndf['Delivery Date'] = pd.to_datetime(df['Delivery Date'])\ndf['scheduleDiff'] = (df['Delivery Date'] - df['Scheduled Date']).map(lambda x:str(x).split()[0])\ndf['scheduleDiff'] = pd.to_numeric(df['scheduleDiff'])\n\ndf_test['Scheduled Date'] = pd.to_datetime(df_test['Scheduled Date'])\ndf_test['Delivery Date'] = pd.to_datetime(df_test['Delivery Date'])\ndf_test['scheduleDiff'] = (df_test['Delivery Date'] - df_test['Scheduled Date']).map(lambda x:str(x).split()[0])\ndf_test['scheduleDiff'] = pd.to_numeric(df_test['scheduleDiff'])\n\ndf.head()","515bf873":"df['dday'] = df['Delivery Date'].dt.day\ndf['dmonth'] = df['Delivery Date'].dt.month\ndf['dyear'] = df['Delivery Date'].dt.year\ndf['ddayofweek'] = df['Delivery Date'].dt.dayofweek\n\ndf_test['dday'] = df_test['Delivery Date'].dt.day\ndf_test['dmonth'] = df_test['Delivery Date'].dt.month\ndf_test['dyear'] = df_test['Delivery Date'].dt.year\ndf_test['ddayofweek'] = df_test['Delivery Date'].dt.dayofweek\n\ndf_test.head()","fcc4550d":"df.drop(['Delivery Date', 'Scheduled Date'], inplace=True, axis=1)\ndf_test.drop(['Delivery Date', 'Scheduled Date'], inplace=True, axis=1)\ndf.head()","467d03d7":"cont_features, cat_features = divideFeatures(df)\ncat_features","0212cf85":"# label encoding\ndef mapFeature(data, f, data_test=None):\n    feat = data[f].unique()\n    feat_idx = [x for x in range(len(feat))]\n\n    data[f].replace(feat, feat_idx, inplace=True)\n    if data_test is not None:\n        data_test[f].replace(feat, feat_idx, inplace=True)","a2e58b1b":"for col in cat_features.columns:\n    mapFeature(df, col, df_test)\ndf_test.head()","5dfdc139":"# create dummy features\ncustom_feat = ['Transport']\nfor feat in cat_features.columns:\n    if len(df[feat].unique()) > 2 and feat in custom_feat:\n        dummyVars = pd.get_dummies(df[feat], drop_first=True, prefix=feat+\"_\")\n        df = pd.concat([df, dummyVars], axis=1)\n        df.drop(feat, axis=1, inplace=True)\ndatasetShape(df)\n\ndf.head()","b0d05b30":"# create dummy features\ncustom_feat = ['Transport']\nfor feat in cat_features.columns:\n    if len(df_test[feat].unique()) > 2 and feat in custom_feat:\n        dummyVars = pd.get_dummies(df_test[feat], drop_first=True, prefix=feat+\"_\")\n        df_test = pd.concat([df_test, dummyVars], axis=1)\n        df_test.drop(feat, axis=1, inplace=True)\ndatasetShape(df_test)\n\ndf_test.head()","372f2562":"df.describe()","0bf4787e":"def log1p(vec):\n    return np.log1p(abs(vec))\n\ndef expm1(x):\n    return np.expm1(x)\n\ndef clipExp(vec):\n    return np.clip(expm1(vec), 0, None)\n\ndef getRmse(y_train, y_train_pred):\n    print(skm.mean_squared_error(y_train, y_train_pred))","f7e1ac9a":"# apply log on target feature with abs as it has many negative values\ndf[targetFeature] = log1p(df[targetFeature])\ndf[targetFeature].min()","1874f0cc":"# shuffle samples\ndf_shuffle = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\ndf_y = df_shuffle.pop(targetFeature)\ndf_X = df_shuffle\n\n# split into train dev and test\nX_train, X_test, y_train, y_test = skms.train_test_split(df_X, df_y, train_size=0.8, random_state=seed)\nprint(f\"Train set has {X_train.shape[0]} records out of {len(df_shuffle)} which is {round(X_train.shape[0]\/len(df_shuffle)*100)}%\")\nprint(f\"Test set has {X_test.shape[0]} records out of {len(df_shuffle)} which is {round(X_test.shape[0]\/len(df_shuffle)*100)}%\")","3cd542bc":"import sklearn.linear_model as sklm","113f79a9":"# scaler = skp.RobustScaler()\nscaler = skp.MinMaxScaler()\n# scaler = skp.StandardScaler()\n\n# apply scaling to all numerical variables except dummy variables as they are already between 0 and 1\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n\n# scale test data with transform()\nX_test = pd.DataFrame(scaler.transform(X_test), columns=X_train.columns)\n\n# view sample data\nX_train.describe()","011ee894":"X_train_small = X_train.sample(frac=0.3)\ny_train_small = y_train.iloc[X_train_small.index.tolist()]\nX_train_small.shape","437ee34a":"ridge_model = sklm.RidgeCV(scoring = \"neg_mean_squared_error\", \n                           alphas = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1, 1.0, 10], \n                           cv=5)\nridge_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = ridge_model.predict(X_train)\ny_test_pred = ridge_model.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","d39c03bf":"enet_model = sklm.ElasticNetCV(l1_ratio = [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],\n                    alphas = [1, 0.1, 0.01, 0.001, 0.0005], cv=5)\nenet_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = enet_model.predict(X_train)\ny_test_pred = enet_model.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","2259bbd7":"import sklearn.ensemble as ske","b99767ce":"# Grid used\nparam_test1 = {\n    'n_estimators': [10, 50, 100, 500, 1000],\n    'max_depth': np.arange(2, 12, 2)\n}\nextra_cv1 = skms.GridSearchCV(estimator = ske.ExtraTreesRegressor(criterion='mse', random_state=seed), \n                             param_grid = param_test1, scoring='neg_mean_squared_error', n_jobs=-1, \n                             iid=False, cv=5, verbose=1)\n# extra_cv1.fit(X_train_small, y_train_small)\nextra_cv1.fit(X_train, y_train)\nprint(extra_cv1.best_params_, extra_cv1.best_score_)\n# n_estimators = 1000\n# max_depth = 10","08ba6d68":"# Grid used\nparam_test2 = {\n    'min_samples_split': np.arange(5, 18, 3),\n    'min_samples_leaf': np.arange(1, 10, 2)\n}\nextra_cv2 = skms.GridSearchCV(estimator = ske.ExtraTreesRegressor(criterion='mse', random_state=seed,\n                                                                 n_estimators=1000,\n                                                                 max_depth=10), \n                             param_grid = param_test2, scoring='neg_mean_squared_error', n_jobs=-1, \n                             iid=False, cv=5, verbose=1)\nextra_cv2.fit(X_train, y_train)\nprint(extra_cv2.best_params_, extra_cv2.best_score_)\nprint(extra_cv2.best_estimator_)\n# min_samples_split = 5\n# min_samples_leaf = 1","1e10aba6":"extra_model = ske.ExtraTreesRegressor(criterion='mse', random_state=seed, n_jobs=-1, \n                                min_samples_leaf=1, max_depth=10, \n                                min_samples_split=5, n_estimators=100\n                               )\n# extra_model = ske.ExtraTreesRegressor(criterion='mse', random_state=seed, n_jobs=-1)\n\nextra_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = extra_model.predict(X_train)\ny_test_pred = extra_model.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","d424467f":"# Grid used\nparam_test1 = {\n    'n_estimators': [10, 50, 100, 500, 1000],\n    'max_depth': np.arange(2, 12, 2)\n}\nrf_cv1 = skms.GridSearchCV(estimator = ske.RandomForestRegressor(criterion='mse', random_state=seed), \n                             param_grid = param_test1, scoring='neg_mean_squared_error', n_jobs=-1, \n                             iid=False, cv=5, verbose=1)\n# rf_cv1.fit(X_train_small, y_train_small)\nrf_cv1.fit(X_train, y_train)\nprint(rf_cv1.best_params_, rf_cv1.best_score_)\n# n_estimators = 1000\n# max_depth = 10","ecdc5666":"# Grid used\nparam_test2 = {\n    'min_samples_split': np.arange(2, 12, 3),\n    'min_samples_leaf': np.arange(1, 10, 3)\n}\nrf_cv2 = skms.GridSearchCV(estimator = ske.RandomForestRegressor(criterion='mse', random_state=seed,\n                                                                 n_estimators=1000,\n                                                                 max_depth=10), \n                             param_grid = param_test2, scoring='neg_mean_squared_error', n_jobs=-1, \n                             iid=False, cv=5, verbose=1)\nrf_cv2.fit(X_train, y_train)\nprint(rf_cv2.best_params_, rf_cv2.best_score_)\nprint(rf_cv2.best_estimator_)\n# min_samples_split = 2\n# min_samples_leaf = 4","8e5ab94f":"rf_model = ske.RandomForestRegressor(criterion='mse', random_state=seed,\n                                    max_depth=10, n_estimators=1000, \n                                    min_samples_leaf=4, min_samples_split=2\n                                    )\n# rf_model = ske.RandomForestRegressor(criterion='mse', random_state=seed)\n\nrf_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = rf_model.predict(X_train)\ny_test_pred = rf_model.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","632dc0ae":"gb_model = ske.GradientBoostingRegressor(criterion='mse', random_state=seed,\n                                         max_depth=10, n_estimators=100\n                                    )\n\ngb_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = gb_model.predict(X_train)\ny_test_pred = gb_model.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","d628db2e":"import tensorflow as tf\nprint(\"TF version:-\", tf.__version__)\nimport keras as k\ntf.random.set_seed(seed)","45056a0d":"THRESHOLD = 0\nbestModelPath = '.\/best_model.hdf5'\n\nclass myCallback(k.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('mse') < THRESHOLD):\n            print(\"\\n\\nStopping training as we have reached our goal.\")   \n            self.model.stop_training = True\n\nmycb = myCallback()\ncheckpoint = k.callbacks.ModelCheckpoint(filepath=bestModelPath, monitor='val_loss', verbose=1, save_best_only=True)\n\ncallbacks_list = [mycb,\n                  checkpoint,\n                  k.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5)\n                 ]\n            \ndef plotHistory(history):\n    print(\"Min. Validation MSE\",min(history.history[\"val_mse\"]))\n    pd.DataFrame(history.history).plot(figsize=(12,6))\n    plt.show()","e7572687":"epochs = 150\n\nmodel_1 = k.models.Sequential([\n    k.layers.Dense(1024, activation='relu', input_shape=(X_train.shape[1],)),\n    k.layers.BatchNormalization(),\n    k.layers.Dropout(0.3),\n    \n    k.layers.Dense(512, activation='relu'),\n    k.layers.BatchNormalization(),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(256, activation='relu'),\n    k.layers.BatchNormalization(),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(128, activation='relu'),\n#     k.layers.BatchNormalization(),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(1, activation='linear'),\n])\nprint(model_1.summary())\n\nmodel_1.compile(optimizer=k.optimizers.Adam(0.001),\n              loss='mse',\n              metrics='mse'\n)\nhistory = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs,\n                 callbacks=[callbacks_list], batch_size=512)\n","32110f4e":"plotHistory(history)","389e17cb":"def rmse_cv(model):\n    '''\n    Use this function to get quickly the rmse score over a cv\n    '''\n    rmse = np.sqrt(-skms.cross_val_score(model, X_train, y_train, \n                                         scoring=\"neg_mean_squared_error\", cv = 5, n_jobs=-1))\n    return rmse\n\nclass MixModel(skb.BaseEstimator, skb.RegressorMixin, skb.TransformerMixin):\n    '''\n    Here we will get a set of models as parameter already trained and \n    will calculate the mean of the predictions for using each model predictions\n    '''\n    def __init__(self, algs):\n        self.algs = algs\n\n    # Define clones of parameters models\n    def fit(self, X, y):\n        self.algs_ = [skb.clone(x) for x in self.algs]\n        \n        # Train cloned base models\n        for alg in self.algs_:\n            alg.fit(X, y)\n\n        return self\n    \n    # Average predictions of all cloned models\n    def predict(self, X):\n        predictions = np.column_stack([\n            stacked_model.predict(X) for stacked_model in self.algs_\n        ])\n        return np.mean(predictions, axis=1) ","86a1b2df":"mixed_model = MixModel(algs = [\n#     ridge_model, \n#     enet_model, \n    extra_model, \n    rf_model,\n    gb_model\n])\n# score = rmse_cv(mixed_model)\n# print(\"\\nAveraged base algs score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nmixed_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = mixed_model.predict(X_train)\ny_test_pred = mixed_model.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","780f831a":"def getTestResults(model=None, roundOff=False):\n    df_final = df.sample(frac=1, random_state=1).reset_index(drop=True)\n    test_cols = [x for x in df_final.columns if targetFeature not in x]\n    df_final_test = df_test[test_cols]\n    df_y = df_final.pop(targetFeature)\n    df_X = df_final\n\n#     scaler = skp.RobustScaler()\n    scaler = skp.MinMaxScaler()\n#     scaler = skp.StandardScaler()\n\n    df_X = pd.DataFrame(scaler.fit_transform(df_X), columns=df_X.columns)\n    X_test = pd.DataFrame(scaler.transform(df_final_test), columns=df_X.columns)\n    \n    # for machine learning models, else neural network will work\n    if model is None:\n        model = MixModel(algs = [\n#             ridge_model, \n#             enet_model, \n            extra_model, \n            rf_model,\n            gb_model\n        ])\n\n        model.fit(df_X, df_y)\n\n    # predict\n    y_train_pred = model.predict(df_X)\n    y_test_pred = model.predict(X_test)\n    \n    if roundOff:\n        y_train_pred = np.round(y_train_pred)\n        y_test_pred = np.round(y_test_pred)\n                \n    if type(y_test_pred[0]) == np.ndarray:\n        y_test_pred = np.ravel(y_test_pred)\n\n    getRmse(df_y, y_train_pred)\n    return clipExp(y_test_pred)","addf8188":"# ML models\nresults = getTestResults(roundOff=False)","f80705a3":"submission = pd.DataFrame({\n    'Customer Id': df_test['Customer Id'],\n    targetFeature: results,\n})\nsubmission.head()","15c9159f":"submission.to_csv('.\/submission_Ensemble.csv', index=False)","773f1b22":"# Neural Network model\nresults = getTestResults(k.models.load_model(bestModelPath))","4f174769":"submission = pd.DataFrame({\n    'Customer Id': df_test['Customer Id'],\n    targetFeature: results,\n})\nsubmission.head()","1bf026d1":"submission.to_csv('.\/submission_NN.csv', index=False)","d37d46f1":"## Derive Features","4ca0463c":"## Generate Ensembles","88f8d30b":"# Test Evaluation & Submission","0ec80a6b":"### Ridge","2c404372":"### ElasticNet","c48ebfe0":"# Step 4: Data Modelling\n\n### Split Train-Test Data","2b84f346":"### Univariate Analysis","e3cb669a":"## Create Dummy Features","0790da00":"### END NOTES\n\nTry more different models and HP tuning, and probably some more better feature engineering, and you can achieve more better results.\n\n`Ridge -> 28.38\nElasticNet -> 27.18\nExtraTrees -> 91.66\nRandomForest -> 90.81\nGradientBoosting -> 92.14\nNN -> 60 - try with more parameters to train for better LB.`\n\n**Ensemble(ET,RF,GB) -> 92.19** - can be increased more with more improved models.","ca408459":"### Extra Trees","8faa4b0e":"## Model Building","71d2fe07":"# Step 2: EDA","a95eb007":"# HackerEarth Machine Learning Challenge - Exhibit A(rt)","c6a52853":"### Feature Scaling","bc40adba":"## Deep Learning Model","42adfe6d":"### RandomForest","23c958ed":"# Step 3: Data Preparation","14aeb05f":"### Gradient Boosting","55e98bc0":"# Step 1: Reading and Understanding the Data","e1e5410b":"## Remove Missing"}}