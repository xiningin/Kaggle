{"cell_type":{"c97f9649":"code","94e4cb40":"code","cc6dd759":"code","6de9415e":"code","4209f354":"code","ece93dca":"code","10c76515":"code","f4afd9c3":"code","40d748d0":"code","bdae21be":"code","b8f7d85f":"code","30d33ed3":"code","ac7ea110":"code","77d8fc67":"code","02067d59":"code","3b1b0e29":"code","8d7e891a":"code","613bf5ad":"code","f1a2afc6":"code","c30575b0":"code","7ff18e8a":"code","daac880b":"code","8972e8ad":"code","c10d65e9":"code","f4bbd458":"code","d49c5ff8":"code","3c6da777":"code","5f0b32cb":"code","57900277":"code","ab059bff":"code","e892ac64":"code","3c33f2be":"code","fcff7bbd":"code","14a2c128":"code","9699a745":"code","7d19d005":"code","cea9a35e":"code","381f9182":"code","37fb37c2":"code","dbddda6f":"code","309d28f0":"code","ad81ec29":"code","9c3bf50b":"code","f203a97d":"code","f3be568d":"code","4d25a09e":"code","9817e47d":"code","9d5e535d":"code","40809be4":"code","ff7f4906":"code","4dc3dbca":"code","c1b7b5d6":"code","77e21723":"code","d2e80091":"code","4ea8e2d3":"code","108c6ced":"code","dde638cf":"code","1b114fc1":"code","9f54a027":"code","194bf68f":"markdown","d3637a9e":"markdown","20599f58":"markdown","2584d3f1":"markdown","80df7917":"markdown","6267fa1a":"markdown","d51f22d0":"markdown","71d06cc7":"markdown","af4b4a68":"markdown","c3e87c4f":"markdown","141474a1":"markdown","1cfe9c53":"markdown","8d4732de":"markdown","ef8e102f":"markdown","cef03402":"markdown","4dfe3bb0":"markdown","73de033a":"markdown","a819599b":"markdown","833c81c9":"markdown","6417b9ad":"markdown","52ca5fa5":"markdown","2f916ca5":"markdown","b6f97aa4":"markdown","7ab590ef":"markdown","96e4d2de":"markdown","970fe430":"markdown","10caabe3":"markdown","60f8f5f5":"markdown","e59cd00b":"markdown","afa0454e":"markdown","1124e865":"markdown","f2e7c358":"markdown","27b5ac8d":"markdown","37e0d221":"markdown","4b7ad9bb":"markdown","5cc47240":"markdown","df697674":"markdown","a90bfea2":"markdown","3d9e9443":"markdown","dcd6a8fe":"markdown","225f49c2":"markdown","b53288b8":"markdown","7a8af99c":"markdown","7ba22837":"markdown","254dffcf":"markdown","86b7ce5d":"markdown","1108b2d5":"markdown","81bc01e5":"markdown","7c1b2060":"markdown"},"source":{"c97f9649":"# prepare the notebook\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","94e4cb40":"# import necessary libraries\nimport numpy as np\nfrom fastai.text import *\nfrom pathlib import Path","cc6dd759":"# View current working directory\nprint(f\"Current directory: {Path.cwd()}\")\nprint(f\"Home directory: {Path.home()}\")\npath=Path.cwd()\npath=path","6de9415e":"path","4209f354":"# Download Dataset\n#import kaggle\n#kaggle.api.authenticate()\n#kaggle.api.dataset_download_files('crowdflower\/twitter-airline-sentiment', path=path, unzip=True)","ece93dca":"#Prepare Dataframe\ndf_org = pd.read_csv('..\/input\/twitter-airline-sentiment\/Tweets.csv')\ndf_org.rename(columns={'airline_sentiment':'label'},inplace=True)               \ndf=df_org[['label','text']]\nnp.random.seed(2020)\ndf['is_valid']=np.random.choice([True, False], len(df_org), p=[0.9,0.1 ]) # Seperate 10% for test\ndf.head()","10c76515":"df.info()","f4afd9c3":"# Save to clean version CSV\ndf[['label','text','is_valid']].to_csv(path\/'Tweets.csv',index=False)","40d748d0":"#Show the first text item \ndf['text'][1]","bdae21be":"import seaborn as sns\ndf.label.value_counts().plot(kind='pie',autopct='%1.0f')","b8f7d85f":"# Sentiment distribution\nsns.countplot(x='label',data=df,palette='viridis')","30d33ed3":"#sentiment distribution over airelines \nplt.figure(figsize=(12,7))\nsns.countplot(x='airline',hue='label',data=df_org,palette='rainbow')","ac7ea110":"# selecting bunch size depends on the memory size of your PC\nbs=48","77d8fc67":"data_lm = (TextList.from_csv(path, 'Tweets.csv', cols='text') \n            .split_by_rand_pct(0.1,seed=2020)\n           #We randomly split and keep 10% (10,000 reviews) for validation\n            .label_for_lm()           \n           #We want to do a language model so we label accordingly\n            .databunch(bs=bs))","02067d59":"#Save DataBunch object\ndata_lm.save('data_lm.pkl')","3b1b0e29":"data_lm = load_data(path, 'data_lm.pkl', bs=bs)","8d7e891a":"# Lets have a look at the first item of the training set\ndata_lm.train_ds[0][0]","613bf5ad":"data_lm.train_ds[0][0].data[:10]","f1a2afc6":"data_lm.show_batch()","c30575b0":"data_lm.vocab.itos[:20]","7ff18e8a":"# Slanted triangular learning rates (STLR), which first linearly increases the learning rate and then linearly decays it\nlearn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n","daac880b":"learn.lr_find()","8972e8ad":"learn.recorder.plot(skip_end=15)","c10d65e9":"learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7)) # lr should be 4*1e-2 at the stiffest slope\n#The momentum is the first beta in Adam (or the momentum in SGD\/RMSProp). When you pass along (0.8,0.7) it means going from 0.8 to0.7 during the warmup then from 0.8 to 0.7 in the annealing, but it only changes the first beta in Adam\n#fit_one_cycle equivalent to the Adam optimizer\u2019s (beta_2, beta_1) (notice the order) parameters, where beta_1 is the decay rate for the first moment, and beta_2 for the second","f4bbd458":"learn.save('fit_head')","d49c5ff8":"learn.load('fit_head');","3c6da777":"learn.unfreeze()","5f0b32cb":"learn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7))","57900277":"learn.save('fine_tuned')","ab059bff":"learn.load('fine_tuned');","e892ac64":"TEXT = \"I liked this airline because\"\nN_WORDS = 40\nN_SENTENCES = 2","3c33f2be":"print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))","fcff7bbd":"learn.save_encoder('fine_tuned_enc')","14a2c128":"data_clas=TextClasDataBunch.from_csv(path, 'Tweets.csv',vocab=data_lm.vocab)\n","9699a745":"data_clas.save('data_clas.pkl')","7d19d005":"data_clas = load_data(path, 'data_clas.pkl', bs=bs)","cea9a35e":"data_clas.show_batch()","381f9182":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n","37fb37c2":"#Show the learner structure \nlearn","dbddda6f":"# Transfer learned encoder from previous language model\nlearn.load_encoder('fine_tuned_enc')","309d28f0":"learn.lr_find()","ad81ec29":"learn.recorder.plot()","9c3bf50b":"learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7)) ","f203a97d":"learn.save('first')","f3be568d":"learn.load('first')","4d25a09e":"learn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2\/(2.6**4),1e-2), moms=(0.8,0.7))   #?? why 1e-2\/(2.6**4)","9817e47d":"learn.save('second')","9d5e535d":"learn.load('second');","40809be4":"learn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3\/(2.6**4),5e-3), moms=(0.8,0.7))","ff7f4906":"learn.save('third')","4dc3dbca":"learn.load('third');","c1b7b5d6":"learn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7))","77e21723":"learn.save('Fourth')","d2e80091":"learn.load('Fourth');","4ea8e2d3":"learn.recorder.plot_losses()","108c6ced":"learn.recorder.plot_metrics()","dde638cf":"learn.predict(\"I really loved that airline, it was awesome!\")","1b114fc1":"# Prepare Interpreter\ninterp = ClassificationInterpretation.from_learner(learn)","9f54a027":"# Confusion Matrix\ninterp.plot_confusion_matrix()","194bf68f":"   d.  Feature Selection\n-----------------","d3637a9e":"\n\nWe're not going to train a model that classifies the reviews from scratch. Like in computer vision, we'll use a model pretrained on a bigger dataset (a cleaned subset of wikipedia called [wikitext-103](https:\/\/einstein.ai\/research\/blog\/the-wikitext-long-term-dependency-language-modeling-dataset)). That model has been trained to guess what the next word is, its input being all the previous words. It has a recurrent structure and a hidden state that is updated each time it sees a new word. This hidden state thus contains information about the sentence up to that point.\n\nWe are going to use that 'knowledge' of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because the English of the reviews left by people on IMDB isn't the same as the English of wikipedia, we'll need to adjust the parameters of our model by a little bit. Plus there might be some words that would be extremely common in the reviews dataset but would be barely present in wikipedia, and therefore might not be part of the vocabulary the model was trained on.","20599f58":"2- Introduction\n============\n\nTaking a view of the advantages of pretraining we would be able to do better than initializing arbitrarily the remaining parameters of our models. However, finetuning inductive transfer was ineffective for NLP. Language model (LM) fine-tuning requires millions of in-domain documents to achieve good results, which significantly limits its applicability. Universal Language Model Fine-tuning (ULMFiT) solves these issues and facilitates stable, inductive learning transfers for any NLP function.","2584d3f1":"### Numericalization","80df7917":"### i- Discriminative fine-tuning\n\nBecause different layers\u2019 capture various information types, they should be fine-tuned to a different degree. Instead of using the same learning rate for all layers of the model, Discriminative fine-tuning allows one to apply specific learning levels to each layer. \nIt was empirically found that it performed well to select the last layer's Alpha^L learning rate first by fine-tuning only the last layer and using Alpha^(L-1) = (Alpha^L)\/2.6 as the lesser layer learning rate. For L, the order of the layer in the model.\n","6267fa1a":"It is clear that the losses plunged just after the second epoch and the  accuracy reached 0.824. ","d51f22d0":"   b. Problem Description\n-------------------\n\nSentiment analysis of the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets. The problem under our interest is to create a classification model that can identify the sentiment of the text written by the client as positive, negative or neutral.  ","71d06cc7":"How good is our model? Well let's try to see what it predicts after a few given words.","af4b4a68":"###    iv-  Transfer Learning Classifier Model","c3e87c4f":"To see what the tokenizer had done behind the scenes, let's have a look at a few texts in a batch.","141474a1":"Once we have extracted tokens from our texts, we convert to integers by creating a list of all the words used. We only keep the ones that appear at least twice with a maximum vocabulary size of 60,000 (by default) and replace the ones that don't make the cut by the unknown token `UNK`.\n\nThe correspondance from ids to tokens is stored in the `vocab` attribute of our datasets, in a dictionary called `itos` (for int to string).","1cfe9c53":"5-  Conclusion\n==========","8d4732de":"It only contains one csv file, let's have a look at it.","ef8e102f":"### ii- Slanted triangular learning rates\n\nTo adjust its parameters to task-specific features, at the beginning of the training, make the model converge quickly into an acceptable region of the parameter space, and then refine its parameters. Slanted triangular learning rates (STLR), first increase the learning rate linearly and then decay it linearly. Finally, the learning rate at the stiffest slop will be chosen.","cef03402":"But the underlying data is all numbers","4dfe3bb0":"###     ii- Learning rate selection","73de033a":"We can then put this in a learner object very easily with a model loaded with the pretrained weights. They'll be downloaded the first time you'll execute the following line and stored in `~\/.fastai\/models\/` (or elsewhere if you specified different paths in your config file).","a819599b":"First let's download the dataset we are going to study. ","833c81c9":"###    v-   Learning rate selection","6417b9ad":"Feature selection is the process of selecting what we think is worthwhile in our documents, and what can be ignored. Rejected features are those that act like noise, thus when fed to the model with the training set, the classification accuracy will decrease.\nIn most NLP literature work, stop words, punctuations and non-formal vocabs are deprecated from the training set. Furthermore, most work applies word stemming to return the used words to their lemma. However, we believe that all the words in the text field are important and there no reason to deprecate them. Besides, this notebook takes into consideration only the text field as the dependent variable where we leave the process of exploring the effectiveness of other features for future optimization.\n","52ca5fa5":"The first step of processing we make the texts go through is to split the raw sentences into words, or more exactly tokens. The easiest way to do this would be to split the string on spaces, we can further:\n\n-  take care of punctuation\n- some words are contractions of two different words, like isn't or don't\n- we may need to clean some parts of our texts, if there's HTML code for instance\n\n","2f916ca5":"###    vii-  Prediction and Results","b6f97aa4":"This is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let's create our DataBunch object.","7ab590ef":"###    iii-  Fine-tuning","96e4d2de":"# <center>Universal Language Model Fine-tuning for Text Classification<\/center>\n## <center>NLP Sentimental Analysis on Twitter US Airlines Dataset<\/center>\n### <center>By Ahmad Abboud<\/center>\n\n\n","970fe430":"#### Preparing and Download the data","10caabe3":"To have an idea about the distribution of the sentiment labels we will present the frequency distribution over the pie chart and histograms.","60f8f5f5":"4-  Application\n===========\n\n   a.  Dataset Description\n-------------------\n\nThe dataset is published on [Kaggle](https:\/\/www.kaggle.com\/crowdflower\/twitter-airline-sentiment) [3], and it analyzes how travellers in February 2015 expressed their feelings on Twitter. It contains 14640 records, which was semantically analysed. ","e59cd00b":"3- Model Description\n=================\n\nThe model used is the state-of-the-art language model AWD-LSTM \\[2\\], a\nstandard LSTM with various tuned dropout hyper-parameters (with no\ninput, short-cut links, or other sophisticated additions).\n\nULMFiT consists of three stages (Figure 1):\n\n> a\\) The LM is trained on a **general-domain** corpus to capture\n> general features of the language in different layers.\n>\n> b\\) The full LM is **fine-tuned** on target task data using\n> discriminative fine-tuning ('Discr') and slanted triangular learning\n> rates (STLR) to learn task-specific features.\n>\n> c\\) The classifier is fine-tuned on the target task using **gradual\n> unfreezing**, 'Discr', and STLR to preserve low-level representations\n> and adapt high-level ones (shaded: unfreezing stages; black: frozen).\n\n<img src=\"..\/input\/images\/ULMFit.png\" alt=\"Figure 1. ULMFiT Model Structure (source [1])s\" title=\"ULMFiT Model Structure\" \/>\n<i> Figure 1. ULMFiT Model Structure (source [1])<\/i>\n","afa0454e":"By the conclusion, we have applied a ULMFiT on classification job for US airlines sentimental analysis where the obtained results are promising. Using transfer learning and AWD-LSTM pre-trained network we reach an accuracy of more than 82% with few learning epochs, which is pretty good compared to the literature results. Moreover, the results can be improved by pre-train the model with text chats from social networks e.g. Twitter, Facebook, where nonformal language can more accurately fit this dataset compared to Wikitext-103, which in most cases contain a formal and scientific language. Besides, exploring the effect of another independent variable in the dataset could also improve the results especially the field \u201cnegativereason\u201d .","1124e865":"To complete the fine-tuning, we can then unfeeze and launch a new training.","f2e7c358":"We can then create a model to classify those reviews and load the encoder we saved before.","27b5ac8d":"   a) General-domain LM pretraining \n-----------------------------\n\nThe model was pre-trained using Wikitext-103 which is consisting of\n28,595 preprocessed Wikipedia documents and 103 million words [2].\n","37e0d221":"### Tokenization","4b7ad9bb":"We have to save not only the model, but also its encoder, the part that's responsible for creating and updating the hidden state. For the next part, we don't care about the part that tries to guess the next word.","5cc47240":"   c.  Exploratory Analysis\n--------------------\n\n\n\n","df697674":"### Data Visualization","a90bfea2":"\n   b) Target task LM fine-tuning \n--------------------------\n\nRegardless of how complex the general-domain data used for pre-training is, the target task data would typically come from a different source. Therefore, we fine-tune the language model on target task results. This stage converges more quickly, provided a pre-trained general-domain LM, as it only needs to adjust to the idiosyncrasies of the target data, and it enables us to train a robust language model even for small datasets.\n","3d9e9443":"   c) Target task classifier fine-tuning\n----------------------------------\n\nThe pre-trained language model with two additional linear blocks was augmented to fine-tune the classifier. Following common practice for Computer Vision (CV) classifiers, each block uses batch normalization and dropout, with intermediate layer ReLU activations, and a Softmax activation that outputs a probability distribution at the last layer over target classes. Remember that the only parameters learned from scratch are the parameters in these task-specific classifier strata. The first linear layer takes the last hidden layer being pooled as the input state.\n","dcd6a8fe":"Now, we'll create a new data object that only grabs the labelled data and keeps those labels. Again, this line takes a bit of time.","225f49c2":"6-  References\n==========\n\n\\[1\\] J. Howard and S. Ruder, \"Universal language model fine-tuning for\ntext classification,\" in *ACL 2018 - 56th Annual Meeting of the\nAssociation for Computational Linguistics, Proceedings of the Conference\n(Long Papers)*, 2018. <br>\n\\[2\\] S. Merity, N. S. Keskar, and R. Socher,\n\"Regularizing and optimizing LSTM language models,\" in *6th\nInternational Conference on Learning Representations, ICLR 2018 -\nConference Track Proceedings*, 2018. <br>\n\\[3\\] \"Twitter US Airline Sentiment\n\\| Kaggle.\" \\[Online\\]. Available:\nhttps:\/\/www.kaggle.com\/crowdflower\/twitter-airline-sentiment.\n\\[Accessed: 13-Jul-2020\\].\n","b53288b8":"1- Abstract\n========\n\nTraining on inductive transfer has significantly influenced computer vision, but current NLP methods often need task-specific modifications and training from scratch. This notebook discusses the performance of Universal Language Model Fine-tuning (ULMFiT) [1], and efficient transfer learning approach that can be applied to any NLP function, and implementing techniques that are essential to fine-tune a language model. Further, empirical results had been introduced after applying ULMFit for NLP sentimental analysis on Twitter US airlines dataset.","7a8af99c":"\n###    i-Creating Data Bunches","7ba22837":"   e.  Data Preprocessing\n------------------\n","254dffcf":"A text is composed of words, and we can't apply mathematical functions to them directly. We first have to convert them to numbers. This is done in two differents steps: tokenization and numericalization. A `TextDataBunch` does all of that behind the scenes for you.\n","86b7ce5d":"   f.  Modelling\n--------\n\n###    i-   Pre-trained Learning","1108b2d5":"###    vi-  Fine-tuning","81bc01e5":"We have to use a special kind of `TextDataBunch` for the language model, that ignores the labels (that's why we put 0 everywhere), will shuffle the texts at each epoch before concatenating them all together (only for training, we don't shuffle for the validation set) and will send batches that read that text in order with targets that are the next word in the sentence.\n\nThe line before being a bit long, we want to load quickly the final ids by using the following cell.","7c1b2060":"And if we look at what a what's in our datasets, we'll see the tokenized text as a representation:"}}