{"cell_type":{"6a9bc868":"code","d61b7a20":"code","ef977923":"code","871eed74":"code","dffbec13":"code","60fb2cfb":"code","e19bb25c":"code","c2639a83":"code","2d3a59b0":"code","4f46d997":"code","5f0ad71d":"code","42ad7f8a":"code","c14a5897":"code","d61f4307":"code","f9a84c15":"code","4ec78d39":"code","3619fd08":"code","cf5a4a93":"code","2e8a722d":"code","8ccd84a9":"code","5606241b":"code","5cb007ba":"code","9f59fe36":"code","f442069e":"code","f8cc1dc5":"code","f4e89dbf":"code","dc2a0106":"code","1fa16c84":"code","92a05d6b":"code","aa02b4e2":"code","21cde2fd":"code","e47de3c2":"code","8083f2d5":"code","8a53f52a":"code","2e8c17b4":"code","0b0584b0":"code","d02a9edf":"code","e63d0b46":"code","b9b358fd":"code","b1671c94":"code","29c757c3":"code","a3dd7e89":"code","ff142a17":"code","98cc356f":"code","d5bde794":"code","bfa66021":"code","1e319be6":"code","fd4f3155":"code","de9b1ff9":"code","2f37998e":"code","5586dddb":"code","8263e85b":"code","be444ab2":"code","58582b7e":"code","1c3eefd2":"code","790d84f7":"code","e8da0792":"code","2c02d0fa":"code","0249bf4c":"code","fe391dbe":"code","188eb381":"code","b78f7380":"code","8af54fc9":"code","f84cfc93":"markdown","b4443289":"markdown","a173d8f9":"markdown","a4c46f69":"markdown","344bc339":"markdown","5d6227f7":"markdown","c175b13c":"markdown","27dd9b86":"markdown","411a5a19":"markdown","53002713":"markdown","54b684cd":"markdown","01b8aadc":"markdown","00459073":"markdown","5eb09fbd":"markdown","a807c22d":"markdown","fa3646fb":"markdown","129d6092":"markdown","d837e117":"markdown","14246540":"markdown","5eb9f192":"markdown","6a567eae":"markdown","fb35a8dc":"markdown","03112d38":"markdown","3f8686f5":"markdown","7f1fa294":"markdown","88667228":"markdown","346dd134":"markdown","cc87d407":"markdown","1c4103fc":"markdown","29b77498":"markdown","0f956503":"markdown","1d975541":"markdown","3ff6e79d":"markdown","ec971a10":"markdown"},"source":{"6a9bc868":"# Importing libraries\n\n# Overall libraries\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n# Plotting libraries\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# scikit learnit\nimport sklearn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","d61b7a20":"# Setting our environment, we want to be able to see all the rows and all the columns, specially for the correlation matrix we will build\n\npd.set_option(\"max_columns\", None)\npd.set_option(\"max_rows\", None)","ef977923":"# Importing DataSet\n\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_train.head()","871eed74":"# Quick view of the DataFrame's description, specially total rows, the type of each variable and the number of non-null values.\n\ndf_train.info()","dffbec13":"# Creating a null rank\nnull_count = df_train.isnull().sum().sort_values(ascending=False)\nnull_percentage = null_count \/ len(df_train)\nnull_rank = pd.DataFrame(data=[null_count, null_percentage],index=['null_count', 'null_ratio']).T\nnull_rank","60fb2cfb":"df_train.PoolArea.value_counts()","e19bb25c":"# dropping columns with over 70% missing values and PoolArea, which doesn't have actual values, most of it is zero.\n\ndf_drop = df_train.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'PoolArea'], axis = 1)\ndf_drop.head()","c2639a83":"# Let's check the numeric attributes of this dataframe\n\ndf_drop.describe().round(2)","2d3a59b0":"# Let's check how the attributes distance themselves on the x axis\ndf_drop.hist(bins = 50, figsize = (20,15))\nplt.show()","4f46d997":"# Creating a copy of the original dataframe\n\nhousing = df_drop.copy()","5f0ad71d":"# Creating a correlation matrix\n\ncorr_matrix = housing.corr()","42ad7f8a":"# Checking the correlation to our target attribute, sale price\n\ncorr_matrix['SalePrice'].sort_values(ascending = False)","c14a5897":"# Let's remove the variables with less than 10% correlation\n\nhousing = housing.drop(['MoSold', '3SsnPorch', 'BsmtFinSF2', 'BsmtHalfBath', 'MiscVal', 'Id', 'LowQualFinSF', 'YrSold',\n                        'OverallCond', 'MSSubClass'], axis = 1)","d61f4307":"# Creating a new correlation matrix, since we dropped a few columns\n\ncorr_matrix = housing.corr()","f9a84c15":"# Checking the correlation to our target, the Sale Price\n\ncorr_matrix['SalePrice'].sort_values(ascending = False)","4ec78d39":"# Creating a list of numeric columns and categorical columns\n\nnum_list = list(housing.select_dtypes(include = [np.int, np.float]).columns)\ncat_list = list(housing.select_dtypes(include = [object]).columns)","3619fd08":"# Creating a numeric DataFrame\n\ndf_num = housing.drop(cat_list, axis = 1)\ndf_num.shape","cf5a4a93":"# Defining the imputer\n\nimputer = SimpleImputer(strategy='mean')","2e8a722d":"# Fitting and transforming the imputer\n\nhousing_num = pd.DataFrame(imputer.fit_transform(df_num), columns = num_list)","8ccd84a9":"# Checking if there were any missing values behind\n\nhousing_num.isnull().any().sum()","5606241b":"# describing number variables\n\nhousing_num.describe().round(2)","5cb007ba":"# Creating a categorical DataFrame\n\ndf_cat = housing.drop(num_list, axis = 1)","9f59fe36":"# Creating dummies for all the categorical variables.\n\ncat_dummies = pd.get_dummies(df_cat)\ncat_dummies.shape","f442069e":"# Creating the X variable\n\nX = pd.concat([housing_num, cat_dummies], axis = 1, join = 'outer')\nX.isnull().any().sum()","f8cc1dc5":"# Removing SalePrice and creating our target variable\n\ny = X['SalePrice']\nX = X.drop(['SalePrice'], axis = 1)","f4e89dbf":"# Creating the scaler\n\nscaler = StandardScaler()","dc2a0106":"# Let's scale X\n\nX_scaled = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\nX_scaled.describe().round(2)","1fa16c84":"# Let's log y the variables\n\ny = np.log(y)","92a05d6b":"# Let's see some statistic magic\n\nX1 = sm.add_constant(X_scaled)\nest = sm.OLS(y, X1)\nest1 = est.fit()\nprint(est1.summary())","aa02b4e2":"# Let's define an y_pred to calculate what a probable mse and r\u00b2 would be\n\ny_pred = est1.predict(X1)\n","21cde2fd":"# Let's calculate the MSE\n\nmse = sklearn.metrics.mean_squared_error(y, y_pred)\nmse","e47de3c2":"# Let's calculate the RMSE\n\nrmse = mse**(1\/2)\nrmse","8083f2d5":"# Let's calculate the r\u00b2\n\nsklearn.metrics.r2_score(y, y_pred)","8a53f52a":"# Creating the train  test set\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled,y, test_size = 0.3, random_state = 42)","2e8c17b4":"# Creating the Linear Regression and fitting it\n\nlr = LinearRegression(fit_intercept = False)\nlr.fit(X_train, y_train)","0b0584b0":"# Creating the prediction of y\n\ny_pred = lr.predict(X_test)","d02a9edf":"# Checking the adherence of the model so far\n\n_ = plt.figure(figsize=(8,8))\n_ = sns.regplot(x = y_test, y = y_pred)","e63d0b46":"# Let's calculate the MSE\n\nmse = sklearn.metrics.mean_squared_error(y_test, y_pred)\nmse","b9b358fd":"# Let's calculate the RMSE\n\nrmse = mse**(1\/2)\nrmse","b1671c94":"# Let's calculate the r\u00b2\n\nsklearn.metrics.r2_score(y_test, y_pred)","29c757c3":"# Creating an unified base\n\nunified = pd.concat([X, y], axis = 1)","a3dd7e89":"# Creating a correlation matrix\n\ncorr_matrix = unified.corr()","ff142a17":"# Checking the correlation to SalePrice\n\ncorr_matrix['SalePrice'].sort_values(ascending = False)","98cc356f":"list_to_drop = ['RoofStyle_Hip', 'Neighborhood_StoneBr', 'Neighborhood_Somerst', 'LotConfig_CulDSac', 'BsmtExposure_Av', 'Neighborhood_Timber',\n                'BsmtCond_TA', 'Heating_GasA', 'Functional_Typ', 'BldgType_1Fam', 'ExterCond_TA', 'LotShape_IR2', 'BsmtFinType2_Unf', \n                'ScreenPorch', 'MSZoning_FV', 'RoofMatl_WdShngl', 'Condition1_Norm', 'Neighborhood_CollgCr', 'LandContour_HLS', 'BsmtCond_Gd',\n                'Exterior1st_CemntBd', 'Exterior2nd_CmentBd', 'Neighborhood_Crawfor', 'Neighborhood_Gilbert', 'Neighborhood_ClearCr', \n                'Neighborhood_Veenker', 'Condition1_PosN', 'Neighborhood_NWAmes', 'Street_Pave', 'RoofMatl_WdShake', 'Condition1_PosA',\n                'BsmtExposure_Mn', 'GarageQual_Gd', 'Condition2_Norm', 'Exterior2nd_ImStucc', 'Condition2_PosA', 'Condition2_PosN', \n                'SaleType_Con', 'Exterior2nd_Other', 'BsmtFinType2_ALQ', 'Exterior1st_Stone', 'Neighborhood_Blmngtn', 'LandContour_Low',\n                'LotShape_IR3', 'Neighborhood_SawyerW', 'HouseStyle_2.5Fin', 'Exterior1st_BrkFace', 'Exterior1st_ImStucc', 'LandSlope_Mod',\n                'RoofStyle_Shed', 'BldgType_TwnhsE', 'LandSlope_Sev', 'RoofMatl_Membran', 'RoofStyle_Flat', 'SaleType_CWD', 'Condition1_RRNn',\n                'LotConfig_FR3', 'GarageQual_Ex', 'Condition1_RRAn', 'Exterior2nd_BrkFace', 'Utilities_AllPub', 'Condition1_RRNe',\n                'Exterior1st_Plywood', 'ExterCond_Ex', 'RoofMatl_Tar&Grv', 'Foundation_Wood', 'Condition2_RRAe', 'RoofStyle_Mansard', \n                'GarageCond_Gd', 'RoofMatl_Metal', 'LotConfig_FR2', 'LotConfig_Corner', 'SaleType_ConLI', 'BsmtFinType2_GLQ', \n                'RoofMatl_ClyTile', 'FireplaceQu_Fa', 'LandContour_Lvl','HouseStyle_SLvl', 'Utilities_NoSeWa', 'RoofMatl_Roll',\n                'Condition2_RRAn', 'Foundation_Stone', 'Functional_Sev', 'Neighborhood_Blueste', 'Exterior2nd_Stone', 'GarageType_2Types', 'BsmtFinType2_LwQ',\n                'Exterior2nd_Plywood', 'Exterior2nd_AsphShn', 'SaleCondition_Alloca', 'HouseStyle_2.5Unf', 'Heating_GasW', 'Heating_OthW', 'GarageCond_Ex',\n                'BsmtFinType2_Rec', 'Exterior2nd_CBlock', 'Exterior1st_CBlock', 'GarageType_Basment', 'Neighborhood_NPkVill', 'Exterior1st_AsphShn', 'LandSlope_Gtl',\n                'SaleType_ConLw', 'SaleType_Oth', 'Functional_Maj1', 'Neighborhood_Mitchel', 'Condition2_Artery', 'Functional_Mod', 'HeatingQC_Po', 'MasVnrType_BrkCmn',\n                'Exterior1st_Stucco', 'Condition1_RRAe', 'SaleCondition_Family', 'ExterCond_Gd', 'RoofStyle_Gambrel', 'SaleType_ConLD', 'Exterior2nd_HdBoard',\n                'ExterCond_Po', 'BsmtFinType2_BLQ', 'Heating_Floor', 'Condition2_RRNn', 'Condition2_Feedr', 'Street_Grvl', 'Exterior2nd_Stucco',\n                'Functional_Min1', 'Electrical_Mix', 'Exterior1st_WdShing', 'Neighborhood_SWISU', 'GarageQual_Po', 'SaleCondition_AdjLand', 'Electrical_FuseP', 'Functional_Min2',\n                'MSZoning_RH', 'BsmtFinType1_ALQ', 'HouseStyle_1Story', 'Exterior1st_HdBoard', 'Heating_Wall', 'GarageCond_Po', 'Exterior1st_BrkComm', 'BsmtFinType1_LwQ', 'FireplaceQu_Po',\n                'SaleType_COD', 'GarageType_CarPort', 'BsmtCond_Po', 'LotConfig_Inside', 'RoofMatl_CompShg', 'PavedDrive_P', 'HouseStyle_SFoyer', 'BsmtFinType1_Unf', 'SaleCondition_Normal',\n                'Functional_Maj2', 'HouseStyle_1.5Unf', 'BldgType_Twnhs', 'BldgType_2fmCon', 'LandContour_Bnk', 'BldgType_Duplex', 'Neighborhood_Sawyer', 'Condition1_Feedr', 'Neighborhood_BrDale',\n                'HeatingQC_Gd', 'Condition1_Artery', 'BsmtFinType1_BLQ', 'Exterior2nd_AsbShng', 'BsmtFinType1_Rec', 'Exterior1st_AsbShng', 'KitchenAbvGr', 'EnclosedPorch', 'Heating_Grav',\n                'Neighborhood_MeadowV', 'Foundation_Slab', 'BsmtQual_Fa', 'SaleCondition_Abnorml', 'GarageQual_Fa', 'Electrical_FuseF', 'Neighborhood_NAmes', 'BsmtCond_Fa', 'GarageCond_Fa',\n                'Exterior2nd_MetalSd', 'Exterior1st_MetalSd', 'Neighborhood_BrkSide', 'ExterQual_Fa', 'HeatingQC_Fa', 'HouseStyle_1.5Fin', 'RoofStyle_Gable', 'ExterCond_Fa', 'Exterior2nd_Wd Shng',\n                'Exterior2nd_Brk Cmn', 'Exterior2nd_Wd Sdng', 'Exterior1st_Wd Sdng', 'MSZoning_C (all)']","d5bde794":"# Let's drop the selected variables\n\nunified = unified.drop(list_to_drop, axis = 1)","bfa66021":"# Creating a correlation matrix, since we dropped a few columns\n\ncorr_matrix = unified.corr()","1e319be6":"# Checking the correlations again\n\ncorr_matrix['SalePrice'].sort_values(ascending = False)","fd4f3155":"#unified[~unified.isin([np.nan, np.inf, -np.inf]).any(1)]","de9b1ff9":"# Let's separate X and y again\n\ny = unified['SalePrice']\nX = unified.drop('SalePrice', axis = 1)","2f37998e":"# Creating the train  test set\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","5586dddb":"# Creating the Linear Regression and fitting it\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)","8263e85b":"# Creating the prediction of y\n\ny_pred = lr.predict(X_test)","be444ab2":"# Checking the adherence of the model so far\n\n_ = plt.figure(figsize=(8,8))\n_ = sns.regplot(x = y_test, y = y_pred)","58582b7e":"# Let's calculate the MSE\n\nmse = sklearn.metrics.mean_squared_error(y_test, y_pred)\nmse","1c3eefd2":"# Let's calculate the RMSE\n\nrmse = mse**(1\/2)\nrmse","790d84f7":"# Let's calculate the r\u00b2\n\nsklearn.metrics.r2_score(y_test, y_pred)","e8da0792":"# Getting the X columns as a list\n\nx_columns = list(X.columns)","2c02d0fa":"# Reading the test data\n\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_test.head()","0249bf4c":"# Let's check the shape of the test DataFrame\n\ndf_test.shape","fe391dbe":"# Let's make the X the same size of the test DataFrame\n\nX = X[:1459]","188eb381":"# Let's create a DataFrame with the ID and the SalePrice in log values\n\nsubmit = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': lr.predict(X)})","b78f7380":"# Let's take a sneak peak of the submit\n\nsubmit.head()","8af54fc9":"# Creating the csv file, remember to use index false or it will create 3 columns\n\nsubmit.to_csv('output.csv', index = False)","f84cfc93":"Let's see the scaling between the number variables. If they are way off we will have a problem when we run our model.","b4443289":"First let's separate the numeric variables and the categoric variables.","a173d8f9":"Remember when we saw the description of the numeric variables and they were not scaled? We will scale them now, because we have the categorical variables transformed into numbers as well.","a4c46f69":"We now have to treat all the categorical variables. We will do that by transforming them into numbers, through the function get_dummies. That will increase a lot of our columns here, but it will also deal with our missing values.","344bc339":"Let's remove the SalePrice, it is our target, so we'll use the opportunity to create our y variable.","5d6227f7":"Let's check now the performance of our model.","c175b13c":"# 8. Create an output to submit for the competition\n![](https:\/\/static1.squarespace.com\/static\/5a25e15d8fd4d23cd9c25d86\/5b738490562fa79aabf9897a\/5b7384b989858325de8e283d\/1534319552670\/)","27dd9b86":"We can see from these graphs that some of the variables are not well distributed in the x axis, which make sense in some cases, like year sold. number of car slots in the garage, etc, that creates very separated columns. Others have a very skewed curve, like thee sale price.","411a5a19":"Let's check some statistic with the statsmodel library. This will help us prepare the data even more for the model we want to implement. What we want to check for now on the output is the adjusted r\u00b2, as it averages the multiple variables. We will check the P > |t|, as to see the extremeness of the result, for it checks the probability agains the standard error. We will also calculate a mockup mse, rmse and r\u00b2.","53002713":"<h1>Goals<\/h1>\nOn this notebook I will try to work through the dataset available on the <a href='https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques'>House Prices: Advanced Regression Techniques<\/a>, a fairly simple exercise. My goal here is to use knowledge that I know so far, and then look at other people's work to improve in a second go. However, if I am being completely honest, I will be usint O'Reilly's book on Machine Learning for Scikit-Learn and TensorFlow. Besides that, I am trying to get a scholarship for a data science course and the test is a linear regression exercise, so I thought I could practice here.<br><br>\n<h1>Steps<\/h1>\n<ol><li>Check the overall problem<\/li>\n    <li>Gather data<\/li>\n    <li>Discover and visualize data to gather information<\/li>\n    <li>Prepare data for Machine Learning algorithms<\/li>\n    <li>Select and train a model (spoilers: we will go with linear regression)<\/li>\n    <li>Adjust the model<\/li>\n    <li>Show the solution<\/li>\n    <li>Create an output to submit for the competition<\/li><\/ol>\n<h1>Questions<\/h1>\nIf you have any questions or don't understand any of the steps I have taken in this, please feel free to comment and I will respond as soon as possible.","54b684cd":"Our results weren't bad, and since it is a linear regression, there are only a few hyperparameters we can tweak with. However, the first thing we will do is thin out the variables through correlation and see the results, and then work on the parameters.","01b8aadc":"# 1. Check the overall problem\n![](http:\/\/images.virgula.com.br\/2017\/03\/nazare-gif.gif)","00459073":"Now that we have looked over our data, let's create a copy of our dataframe and analyse it a bit further.","5eb09fbd":"As said before on this notebook, we will solve this problem with a linear regression. So let's divide the team on train and test and see how it goes,","a807c22d":"The variables **PoolQC, MiscFeature, Alley** and **Fence** have over 70% of their data missing. Let's check on the data dictionary what they mean.<br>\n* **PoolQC:** Pool quality<br>\nMy first guess is that most houses don't have a pool. We will check that next.\n* **MiscFeature:** Miscellaneous feature not covered in other categories<br>\nThis category having a lot of it's data missing makes sense, because it would only be filled if the other categories didn't cover the features of the house, and, considering that we are dealing with 78 other featuers, it doesn't leave room for a lot of different features.\n* **Alley:** Type of alley access<br>\nI believe most of the houses have a direct entrance to the street, so it's not needed to fill this column.\n* **Fence:** Fence quality<br>\nMany houses may not have fences, so that might also help for the number being very high.","fa3646fb":"It is finally time to create the csv to submit for the competition! We will simply use our submit DataFrame and transform it into a csv file.","129d6092":"<h2>Missing Values<\/h2>\nWe can see that there are several variables with null values, some with most of it's values being null, for example PoolQCand MiscFeature. It will be of no use to fill those missing values and there isn't really much they will tell us. I'll create a rank of how much of the variable's content is null. if it is over 70%, I will drop that variable from the analysis, but checking which variables they were and if they could indeed be of any value for our sale price.<br>\nI will create this rank by counting how many missing values there are and seeing how representative they are to the entire variable, and then sort them in a descending order.","d837e117":"# 6. Adjust the model\n![](https:\/\/i.makeagif.com\/media\/8-18-2016\/sqtcmZ.gif)","14246540":"After all of this, let's glue all of the variables together in our X variable.","5eb9f192":"The results are very reasonable, the MSE and RSME are very small and the r\u00b2 is close to 1.","6a567eae":"# 3. Discover and visualize data to gather information\n![](https:\/\/blog.revolutionanalytics.com\/downloads\/datasaurus.gif)","fb35a8dc":"We can see that only seven of the pools have areas different than 0, so we could even disconsider that column as well.","03112d38":"# 4. Prepare data for Machine Learning algorithms\n![](https:\/\/i.imgur.com\/5CAJxry.gif)","3f8686f5":"The values are not scaled, so we will have to scale them later on.","7f1fa294":"Each of these lines is a house, because each ID will have a sale price in the end. Besides the SalePrice and the ID, we do have 79 variables, summing up to 81 columns.","88667228":"# 2. Gather data\n![](https:\/\/media1.giphy.com\/media\/LXHcQx5bvwd2\/source.gif)","346dd134":"Let's take out the variables with less than 20% correlation.","cc87d407":"# Welcome to House Pricing\n![](https:\/\/media.giphy.com\/media\/mLGeXwx1h2J8Y\/giphy.gif)\n","1c4103fc":"# 5. Select and train a model\n![](https:\/\/cdna.artstation.com\/p\/assets\/images\/images\/007\/962\/444\/original\/burak-cinar-jellos1.gif?1509585352)","29b77498":"Let's get the columns for the X variable, so we can replicate the model on the test set.","0f956503":"# 7. Show the solution\n![](https:\/\/gifimage.net\/wp-content\/uploads\/2017\/11\/grand-entrance-gif.gif)","1d975541":"I will remove all the variables that have less than 10% correlation with the sale price. Those variables are:\n* MoSold\n* 3SsnPorch\n* BsmtFinSF2\n* BsmtHalfBath\n* MiscVal\n* Id\n* LowQualFinSF\n* YrSold\n* OverallCond\n* MSSubClass","3ff6e79d":"Let's **FINALLY** deal with those missing values - at least for the variables that are numeric.","ec971a10":"This dataset provides you with 79 variables describing almost every aspect of residential homes in Ames, Iowa. We have to <b>predict the final price of each home<\/b>, so, in order to do that, we have to find out which of these 79 variables. So we will spend most of the time on this dataset actually on the feature engineering, speacially because the linear regression is an easy model to apply. All the results will be <b>evaluated by the root-mean-squared-error or RMSE<\/b> between the logarithm of the predicted value and the logarithm of the observed value. So, when we check our final scores we have to double check that as well. Finally, the submission must be on a file that contains ID and SalePrice as a CSV.<br>"}}