{"cell_type":{"54bda7b0":"code","b399a173":"code","c18f0e01":"code","82940505":"code","d4fe07b0":"code","9826ff49":"code","a04d0cc5":"code","7bfe5366":"markdown","2200ae7b":"markdown","cef1f229":"markdown","ec1e1f54":"markdown","39c6ab00":"markdown","bf015087":"markdown"},"source":{"54bda7b0":"import os\nimport numpy as np\nimport pandas as pd\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","b399a173":"# show all columns name\nprint(train.columns.values)","c18f0e01":"cat_features = [\n    \"cat0\", \"cat1\", \"cat2\", \"cat3\", \"cat4\", \"cat5\", \"cat6\", \"cat7\", \n    \"cat8\", \"cat9\"\n]\n\ncont_features = [\n    \"cont0\", \"cont1\", \"cont2\", \"cont3\", \"cont4\",\n    \"cont5\", \"cont6\", \"cont7\", \"cont8\", \"cont9\", \"cont10\", \n    \"cont11\", \"cont12\", \"cont13\"\n]","82940505":"from sklearn.preprocessing import LabelEncoder\n\nnew_cat_features = []\n\ndef label_encode(train_df, test_df, column):\n    le = LabelEncoder()\n    new_feature = \"{}_le\".format(column)\n    le.fit(train_df[column])\n    train_df[new_feature] = le.transform(train_df[column])\n    test_df[new_feature] = le.transform(test_df[column])\n    return new_feature\n\nfor feature in cat_features:\n    new_cat_features.append(label_encode(train, test, feature))\n\ntrain","d4fe07b0":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\nn_folds = 10\n\nskf = KFold(n_splits=n_folds, random_state=2021, shuffle=True)\n\ntrain_oof = np.zeros((300000,))\ntest_preds = 0\n\nfull_features = []\nfull_features.extend(new_cat_features)\nfull_features.extend(cont_features)\n\nlgbm_params = {\n    \"random_state\": 2021,\n    \"metric\": \"rmse\",\n    \"n_jobs\": 6,\n    \"early_stopping_round\": 200,\n    \"cat_features\": [x for x in range(len(new_cat_features))],\n    \"reg_alpha\": 9.03513073170552,\n    \"reg_lambda\": 0.024555737897445917,\n    \"colsample_bytree\": 0.2185112060137363,\n    \"learning_rate\": 0.003049106861273527,\n    \"max_depth\": 65,\n    \"num_leaves\": 51,\n    \"min_child_samples\": 177,\n    \"n_estimators\": 1600000,\n    \"cat_smooth\": 93.60968300634175,\n    \"max_bin\": 537,\n    \"min_data_per_group\": 117,\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": 0.6709049555262285,\n    \"cat_l2\": 7.5586732660804445,\n}\n\nfor fold, (train_index, test_index) in enumerate(skf.split(train, train[\"target\"])):\n    print(\"-------> Fold {} <--------\".format(fold + 1))\n    x_train, x_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[test_index])\n    y_train, y_valid = train[\"target\"].iloc[train_index], train[\"target\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[full_features])\n    x_valid_features = pd.DataFrame(x_valid[full_features])\n\n    model = LGBMRegressor(\n        **lgbm_params\n    )\n    model.fit(\n        x_train_features[full_features], \n        y_train,\n        eval_set=[(x_valid_features[full_features], y_valid)],\n        verbose=100,\n    )\n    oof_preds = model.predict(x_valid_features[full_features])\n    test_preds += model.predict(test[full_features]) \/ n_folds\n    train_oof[test_index] = oof_preds\n    print(\"\")\n    \nprint(\"--> Overall results for out of fold predictions\")\nprint(\": RMSE = {}\".format(mean_squared_error(train_oof, train[\"target\"], squared=False)))","9826ff49":"test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\npreds = test_preds.tolist()\ntest_ids = test['id'].tolist()\n\nsubmission = pd.DataFrame({\"id\": test_ids, \"target\": preds})\n\n","a04d0cc5":"submission.to_csv(\"submission_LGBM_tuned.csv\", index=False)","7bfe5366":"# Introduction\n\nA simple LightGBM model using LabelEncoder for categorical values. Further tuned by hand to find a good balance between fit and LB score. Note that the model is very likely overfit. This model may be most useful if combined with other ensemble methods. Note that no significant feature engineering was employed by this model","2200ae7b":"Categories must be converted to ```int``` types for LightGBM to use them as categorical. We'll use ```LabelEncoder``` to do this.\n\n","cef1f229":"If you find this kernel useful, please upvote!","ec1e1f54":"# Feature Definitions\n\n","39c6ab00":"# Generate Results\nThe test predictions were generated from each fold. Collect them here and build submission file.","bf015087":"# Generate Model\nHere we'll use KFold cross validation, but produce predictions out-of-fold for each fold."}}