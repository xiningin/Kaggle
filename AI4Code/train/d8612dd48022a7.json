{"cell_type":{"69c0ffab":"code","806b8867":"code","5fcf5873":"code","f38f7180":"code","86bcd00b":"code","48a736b0":"code","a065e1e3":"code","4cbb6de5":"code","ed714f76":"code","2dacda97":"code","d6113ae3":"code","59623b5d":"code","6b560c7f":"code","12c715eb":"code","a608ced6":"markdown","d12c2a35":"markdown","2187a5ca":"markdown","e18df003":"markdown"},"source":{"69c0ffab":"import matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\nfrom sklearn.metrics import roc_auc_score\nfrom bayes_opt import BayesianOptimization\nimport pandas as pd, numpy as np, os\n\ndir = '..\/input\/e6-oof-prediction\/\/'\nnp.random.seed(42)\n\n# exp 1 ___________\noof_one  = pd.read_csv(dir + 'oof_e6_exp1_seed_42.csv') \ntest_one = pd.read_csv(dir + 's_e6_exp1_seed_42.csv')   \noof_one  = oof_one.sort_values(by=['image_name'],  \n                               ascending=True).reset_index(drop=True)\ntest_one = test_one.sort_values(by=['image_name'],  \n                                ascending=True).reset_index(drop=True)\n    \n# exp 2 ___________\noof_two   = pd.read_csv(dir + 'oof_e6_exp2_seed_42.csv') \ntest_two  = pd.read_csv(dir + 's_e6_exp2_seed_42.csv')   \noof_two   = oof_two.sort_values(by=['image_name'],  \n                                ascending=True).reset_index(drop=True)\ntest_two  = test_two.sort_values(by=['image_name'],  \n                                 ascending=True).reset_index(drop=True)\n\n# exp 3 ___________\noof_three  = pd.read_csv(dir + 'oof_e6_exp3_seed_42.csv') \ntest_three = pd.read_csv(dir + 's_e6_exp3_seed_42.csv')   \noof_three  = oof_three.sort_values(by=['image_name'],  \n                                   ascending=True).reset_index(drop=True)\ntest_three = test_three.sort_values(by=['image_name'],  \n                                    ascending=True).reset_index(drop=True)\n\n# exp 4 ___________\noof_four  = pd.read_csv(dir + 'oof_e6_exp4_seed_42.csv') \ntest_four = pd.read_csv(dir + 's_e6_exp4_seed_42.csv')    \noof_four  = oof_four.sort_values(by=['image_name'],  \n                                 ascending=True).reset_index(drop=True)\ntest_four = test_four.sort_values(by=['image_name'],  \n                                  ascending=True).reset_index(drop=True)","806b8867":"oof_one.fold.value_counts()","5fcf5873":"oof_one.head()","f38f7180":"test_one.head()","86bcd00b":"blend_train = []\nblend_test = []\n\n# out of fold prediction\nblend_train.append(oof_one.pred)\nblend_train.append(oof_two.pred)\nblend_train.append(oof_three.pred)\nblend_train.append(oof_four.pred)\nblend_train = np.array(blend_train)\n\n# submission scores\nblend_test.append(test_one.target)\nblend_test.append(test_two.target)\nblend_test.append(test_three.target)\nblend_test.append(test_four.target)\nblend_test = np.array(blend_test)","48a736b0":"def roc_min_func(weights):\n    final_prediction = 0\n    for weight, prediction in zip(weights, blend_train):\n        final_prediction += weight * prediction\n    return roc_auc_score(np.array(oof_one.target), final_prediction)\n\nprint('\\n Finding Blending Weights ...')\nres_list = []\nweights_list = []\n\nfor k in range(100):\n    starting_values = np.random.uniform(size=len(blend_train))\n    bounds = [(0, 1)] * len(blend_train)\n    \n    res = minimize(roc_min_func,\n                   starting_values,\n                   method='L-BFGS-B',\n                   bounds=bounds,\n                   options={'disp': False,\n                            'maxiter': 100000}) \n    \n    res_list.append(res['fun'])\n    weights_list.append(res['x'])\n    \n    print('{iter}\\tScore: {score}\\tWeights: {weights}'.format(\n        iter=(k + 1),\n        score=res['fun'],\n        weights='\\t'.join([str(item) for item in res['x']])))\n\n    \nbestSC   = np.max(res_list)\nbestWght = weights_list[np.argmax(res_list)]\nweights  = bestWght\nblend_score = round(bestSC, 6)","a065e1e3":"print('\\n Ensemble Score: {best_score}'.format(best_score=bestSC))\nprint('\\n Best Weights: {weights}'.format(weights=bestWght))\n\ntrain_prices = np.zeros(len(blend_train[0]))\ntest_prices  = np.zeros(len(blend_test[0]))\n\nprint('\\n Your final model:')\nfor k in range(len(blend_test)):\n    print(' %.6f * model-%d' % (weights[k], (k + 1)))\n    test_prices += blend_test[k] * weights[k]\n\nfor k in range(len(blend_train)):\n    train_prices += blend_train[k] * weights[k]","4cbb6de5":"test_one.target = (test_one.target.values*bestWght[0]  + \n                   test_two.target.values*bestWght[1]  + \n                   test_three.target.values*bestWght[2]+ \n                   test_four.target.values*bestWght[3])\/max(bestWght)","ed714f76":"plt.hist(test_one.target,bins=100)\nplt.ylim((0,100))\nplt.show()","2dacda97":"train = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')","d6113ae3":"dirname = '\/kaggle\/input\/e6-oof-prediction'\nprefix = ['exp1', 'exp2', 'exp3', 'exp4', ]\n\nfor model in range(4):\n    _oof = pd.read_csv(os.path.join(dirname, f\"oof_e6_{prefix[model]}_seed_42.csv\"))\n    score = roc_auc_score(_oof['target'], _oof['pred'])\n    print(f\"{prefix[model]}: OOF auc:{score:.4}\")\n\n    _oof = _oof.rename(columns={\"pred\":prefix[model]}).drop([\"target\"],axis=1)\n    \n    if \"fold\" in _oof.columns:\n        _oof = _oof.drop([\"fold\"],axis=1)\n\n    train = train.merge(_oof, on=\"image_name\")   \n\n    _sub = pd.read_csv(os.path.join(dirname, f\"s_e6_{prefix[model]}_seed_42.csv\"))\n    _sub.columns = [\"image_name\", prefix[model]]    \n    test = test.merge(_sub, on=\"image_name\")  ","59623b5d":"def dim_optimizer (df_oof, features, init_points = 20, n_iter = 100 ):\n    pbounds = {'c0': (0.0, 1.0), \n               'c1': (0.0, 1.0), \n               'c2': (0.0, 1.0), \n               'c3': (0.0, 1.0)}\n    \n    features = features\n\n    def dim_opt (df_oof, c0,c1,c2,c3):\n\n        x = (c0*df_oof[features[0]] + \n             c1*df_oof[features[1]] + \n             c2*df_oof[features[2]] + \n             c3*df_oof[features[3]])\n        \n        return roc_auc_score(df_oof['target'], x)\n\n\n\n    def q (c0,c1,c2,c3):\n        return dim_opt  ( df_oof, c0,c1,c2,c3)\n\n    optimizer = BayesianOptimization(\n        f=q,\n        pbounds=pbounds,\n        random_state=42,\n    )\n\n\n    optimizer.maximize(\n        init_points=init_points,\n        n_iter=n_iter,\n    )\n\n    c0 = optimizer.max[\"params\"][\"c0\"]\n    c1 = optimizer.max[\"params\"][\"c1\"]\n    c2 = optimizer.max[\"params\"][\"c2\"]\n    c3 = optimizer.max[\"params\"][\"c3\"]\n    t  = optimizer.max[\"target\"]\n    \n    print ( f'bo auc:{t}, c0:{c0}, c1:{c1}, c2:{c2}, c3:{c3}' )\n    \n    return c0, c1, c2, c3\n\n\nc0, c1, c2, c3 = dim_optimizer (train, prefix, \n                                init_points = 40, \n                                n_iter = 40  )\n\nprint(' ')\nprint (prefix[0],c0)\nprint (prefix[1],c1)\nprint (prefix[2],c2)\nprint (prefix[3],c3)","6b560c7f":"def bo_pred (df):\n    x = (c0*df[ prefix[0] ] + \n         c1*df[ prefix[1] ] + \n         c2*df[ prefix[2] ] + \n         c3*df[ prefix[3] ])\n    \n    return x\n\ntrain[\"pred\"] = bo_pred (train)\nscore = roc_auc_score(train['target'], train['pred'])\nprint(f\"auc bo:{score}\")","12c715eb":"plt.hist(train.pred,bins=100)\nplt.ylim((0,100))\nplt.show()","a608ced6":"**Update**\n\nWe've included `Bayesian Optimization` from [this notebook](https:\/\/www.kaggle.com\/steubk\/simple-oof-ensembling-methods-for-classification). Thanks to [steubk](https:\/\/www.kaggle.com\/steubk). Aim was to compare with `scipy.optimize` function. \n\n---\n\nIn this notebook, we attempt to find the best weight vector for ensembling via trying to maximize **OOF CV** scores. The piece of code actually came from [this work](https:\/\/www.kaggle.com\/tilii7\/cross-validation-weighted-linear-blending-errors). \n\n\n**Disclaimer**\n\nAll the input scripts are designed for demonstratio purpose and also the procedure tha we've presented here. Please, use it to your own risk. Also note, in order to use it, you Have-To-Have **N** times `oof.csv` and `test.csv` that trained one the **same validation fold**. Thanks.\n\n---\n\nSet Up:\n\n```\nModel: E6\nSeed : 42\nExp  : 5 Fold Training, Total 4 experiment [same fold trainig]\n```","d12c2a35":"# Bayesian Optimization\n\nNext, Bayesian. ","2187a5ca":"# Scipy Optimizer\n\nLet's try first optmization function from `Scipy`, usng `L-BFGS-B` method. We've tried other methods, but this one gave best.","e18df003":"|  Method | CV  |  \n|---|---|\n| L-BFGS-B  | 0.9316942291023538  |   \n| Bayesian-Op  | 0.9316718240316467  |"}}