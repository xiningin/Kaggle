{"cell_type":{"86290b07":"code","0e2a96e1":"code","96d08fe2":"code","401ff5d6":"code","59e86fdf":"code","9ef62da0":"code","52bd363e":"code","1f88d6f4":"code","2c0c77b7":"code","f2123fc6":"code","4cd776cc":"code","a889bbba":"code","bfc785a7":"code","bf6264d7":"code","bd9a9139":"code","3a697019":"code","99831c42":"code","db47b282":"code","9f1a1d37":"code","4c1a9fb4":"code","76d0edfd":"code","5f5b0754":"code","251fe35c":"code","c5e356e4":"code","bfb152f4":"code","f6f74a45":"code","a04241e2":"code","c1fd0298":"code","cce45a73":"code","6b48c313":"code","ae348830":"code","d351f42e":"code","92ecebf1":"code","10959da0":"code","54219aac":"code","d3f86eee":"code","4c32b92c":"code","67545bcd":"code","06976770":"code","7a656b7f":"code","82a74658":"code","d5c6da8b":"code","7e65cfb4":"code","b8a38b70":"code","bb4bafa4":"code","0bbdff2e":"code","90b608a2":"code","43460e1a":"code","1ec02687":"code","2f52b267":"code","2d9a8c62":"code","9fd3436b":"code","2b3c0e50":"code","9ca095c1":"code","22044c83":"code","02b06072":"code","4df3b13c":"code","12824bbc":"code","7537231c":"code","288003f3":"code","7578cca7":"code","925ed83e":"code","d54703f5":"code","f390e62a":"code","fc45229d":"code","2507e9c7":"code","3c8ae92b":"code","d5087bac":"code","b639d944":"code","2d4e42af":"code","e56037cf":"code","ebb377b9":"code","711f162e":"code","6810bcd6":"code","8197e1b3":"code","aa3d4b8e":"code","1a1a088f":"code","50611e6c":"code","91ac4c5c":"code","7d841281":"code","86c410d0":"code","938e4e6a":"code","f41bcd45":"code","2ba7c012":"code","9a906a58":"code","2858dbaf":"code","2afe6479":"code","fe07d01f":"code","214014ef":"code","a36540a0":"code","a13da3fd":"code","444961bd":"code","c73d631e":"code","3460dd69":"code","1810cf21":"code","e69e5c9c":"code","b9c04963":"code","eadf80a4":"code","75edcb62":"code","a75def07":"code","12ec8220":"code","e0c0ed5c":"code","117983c4":"code","a7859301":"code","9e840ab0":"code","7583dff2":"code","34016785":"code","2ba5daad":"markdown","4f0db8ca":"markdown","c06ab6ad":"markdown","b3cb6149":"markdown","e737262f":"markdown","1688c88f":"markdown","e7624690":"markdown","3297484d":"markdown","383c4827":"markdown","cb3e1b4b":"markdown","5a22fdf1":"markdown","ee04abf5":"markdown","afaf5ce4":"markdown","8085d39e":"markdown","49ef6c66":"markdown","9c4e2a32":"markdown","cfd7772e":"markdown","9a16176a":"markdown","094b509a":"markdown","c93745b4":"markdown","f1987e67":"markdown","34a4b78d":"markdown","8c820e3c":"markdown","639c0396":"markdown","bf140e1f":"markdown","909566d0":"markdown","93b61539":"markdown","e9957627":"markdown","0359a889":"markdown","3e2b669f":"markdown","78fa2342":"markdown","c7b958ee":"markdown","e1f8502c":"markdown","a1ba0af2":"markdown","332dd3d5":"markdown","131a7f3f":"markdown","9a527553":"markdown","6d784c58":"markdown","2d9f3376":"markdown","c74b16d9":"markdown","91fed728":"markdown","926c769b":"markdown","e9d419f7":"markdown","259656f0":"markdown","d6c4e211":"markdown","b8a3e5d5":"markdown","4be015ef":"markdown","ca87a4de":"markdown","ac80bc81":"markdown","cf2e9ffb":"markdown","34aaf5f5":"markdown","95742014":"markdown","ba45de2a":"markdown","cfbd66d6":"markdown","c4cf90ca":"markdown","0039789f":"markdown","0dfe6e79":"markdown","e46dd4a1":"markdown","2b9bfd03":"markdown","19807339":"markdown","6da1de83":"markdown","e93d5e4e":"markdown","43501d4f":"markdown","741c00fa":"markdown","b7472827":"markdown","dbfb1f26":"markdown","2f64011a":"markdown","09f7d594":"markdown","44493393":"markdown","45fe8ea9":"markdown","c07cafd9":"markdown","1ae87419":"markdown","639c1035":"markdown","6dd0b2e9":"markdown","767706fb":"markdown","adc5cf61":"markdown","b3013d5c":"markdown","ee2b30c7":"markdown","d0d73739":"markdown","e766569a":"markdown","5a66c66f":"markdown","57bedfd1":"markdown","4aacb362":"markdown","29e9db7a":"markdown","69a28f16":"markdown","093d3def":"markdown","003bf6fb":"markdown","44e24649":"markdown","1124c711":"markdown","78fa309c":"markdown","f3de6f44":"markdown","76f54d6d":"markdown","79cf5d4b":"markdown","dca5a6ec":"markdown","35f9aeb3":"markdown","b77150ec":"markdown","02e4c04b":"markdown","2a59ac3d":"markdown","81b50f92":"markdown","9944d740":"markdown","f636557f":"markdown","08146de3":"markdown","6d0bdac7":"markdown","7e988281":"markdown","e8ff3524":"markdown","4f27c91e":"markdown","5c677cfe":"markdown","eb1a8d2d":"markdown","f8558298":"markdown","94902a14":"markdown","f7239998":"markdown","40873b49":"markdown","47cac685":"markdown","eeea0e79":"markdown","664a3522":"markdown","1b07ff91":"markdown","9da5a1a4":"markdown","25526a47":"markdown","02e469a0":"markdown","b724152f":"markdown","210405c5":"markdown","8ffc52ce":"markdown","bd9fe0fe":"markdown","0fba5c87":"markdown","8e0fc5b7":"markdown","e408d4a2":"markdown","d6d2024f":"markdown","1a3ae64a":"markdown"},"source":{"86290b07":"!pip install linearmodels","0e2a96e1":"import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport glob\nimport random\nimport math\nimport re\nimport collections\nimport statistics\nimport os\nimport time\n\n## Stats\nimport scipy.stats  as stats\n\n## Scikit Learn\nimport sklearn\nfrom sklearn import linear_model\nfrom sklearn import svm\nfrom sklearn import neighbors\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import SVR\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.pipeline import make_pipeline\n\n\n## Linear Models (OLS)\nimport linearmodels\nfrom linearmodels import PanelOLS\nfrom linearmodels import RandomEffects\nfrom linearmodels import IV2SLS, IVLIML, IVGMM, IVGMMCUE\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\n## Plots\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nimport seaborn as sb  # for statistic plots","96d08fe2":"def add_ISO3Year(df):\n    ''' this adds the ISO3Year column to a dataframe.\n    nothing is returned - the passed data frame is changed.\n    consider it as inplace=True.\n    '''\n    #numrows = df.shape[0]\n    dummy_list = []\n    #for i in range(numrows):\n    for (idx,row) in df.iterrows():\n        #append_val = df['ISO3'].iloc[i] + str(df['Year'].iloc[i])\n        append_val = row['ISO3'] + str(row['Year'])\n        dummy_list.append(append_val)\n    df['ISO3Year'] = dummy_list\n\n#def get_country_dict(fn='CountryName_ISO3.csv'):\ndef get_country_dict(fn='\/kaggle\/input\/co2emissions\/\/CountryName_ISO3.csv'):\n    '''returns a dictionary {[ISO3]:[CountryName], ...} '''\n    df = pd.read_csv(fn,sep=';')\n    country_dict = {}\n    for (idx,row) in df.iterrows():\n        country_dict[row['ISO3']] = row['CountryName']\n    return country_dict\n\n\ndef data_not_avl(df,year_min,year_max,col_list,print_to_console=False):\n    ''' returns a dictionary containing country ids and names of countries,\n    that do not have all data available \n    df ... pd.dataframe\n    year_min, year_max ... required time frame\n    col_list ... list of required column names\n    '''\n    df_temp = df[(df['Year'] >= year_min) & (df['Year'] <= year_max)]\n    countries = set(df['ISO3'].to_list())\n    name_dict = get_country_dict()\n    incomplete_dict = {}\n    for col in col_list:\n        for cou in countries:\n            complete = True\n            avl = data_avl(df_temp,cou,col)\n            if (not(avl['data_complete'])\n                or (not(avl['first_year'] == year_min))\n                or (not(avl['last_year'] == year_max))):\n                complete = False\n                incomplete_dict[cou] = name_dict[cou]\n            if print_to_console:\n                print('country: {}, complete: {},'.format(cou,complete))               \n    return incomplete_dict\n\ndef data_avl(df,country,column):\n    ''' will return the earliest and latest year and completeness of \n    data availability for a given country and column\n    '''\n    df_temp = df[(df['ISO3'] == country)][[column,'Year']].copy()\n    # now drop all na's\n    df_temp.dropna(inplace=True)\n    \n    # now get list of remaining years    \n    list_years = df_temp['Year'].to_list()\n    try:\n        minn = min(list_years)\n        maxx = max(list_years)\n    except:\n        minn = -1\n        maxx = -1\n    complete = (len(list_years) == (maxx - minn + 1))\n\n    return {'first_year':minn, 'last_year':maxx, 'data_complete':complete}\n\n\ndef avl_report(df,countries,column):\n    ''' prints data availability report to the screen'''\n    ISO3_list = []\n    name_list = []\n    first_year_list = []\n    last_year_list = []\n    completeness_list = []\n    \n    country_dict = get_country_dict()\n    \n    for c in countries:\n        avl_dict = data_avl(df,c,column)\n        ISO3_list.append(c)\n        name_list.append(country_dict[c])        \n        first_year_list.append(avl_dict['first_year'])        \n        last_year_list.append(avl_dict['last_year'])        \n        completeness_list.append(avl_dict['data_complete'])        \n        \n    df_out = pd.DataFrame()\n    df_out['ISO3'] = ISO3_list\n    df_out['CountryName'] = name_list\n    df_out['first_year'] = first_year_list\n    df_out['last_year'] = last_year_list\n    df_out['completeness'] = completeness_list\n    return df_out\n\ndef country_list_share(df,country_list,column,year=2016):\n    ''' will return the share of countries in list \n    of a column at a certain year and a list of countries, that\n    were in the passed list, but were not found'''\n    \n    df_year = df[df['Year'] == year]\n    total = df_year[column].sum()\n    temp_sum = 0\n    not_found = []\n    for c in country_list:\n        try:\n            add_this = float(df_year[df_year['ISO3'] == c][column])\n            add_this = np.nan_to_num(add_this)\n            temp_sum = temp_sum + add_this\n        except:\n            not_found.append(c)\n    share = temp_sum \/ total\n    return share, not_found\n\ndef drop_ISO3_list(df,ISO3_list):\n    ''' will drop a list of countries given by ISO3_list from df\n    no return value. df is changed (inplace=True)'''\n    for c in ISO3_list:\n        row_select = df[df['ISO3'] == c].index\n        df.drop(row_select,inplace=True)\n\n        \ndef get_bins_dict(df,column,year=2014,\n                    quantiles=[0,.2,.4,.6,.8,1],\n                    labels=[1,2,3,4,5]):\n    ''' this will return a dictionary of the shape\n    {label1: [iso11, iso12, ...], label2: [iso21, iso22, ...], ...}\n    every bin label is mapped to the list of ISO3 codes belonging\n    to the corresponding quantile\n    per default 5 equal quantiles are applied with the labels 1,2,3,4,5\n    '''\n    df_tmp = df[df['Year'] == year].copy()\n    bins = []\n    for q in quantiles:\n        bins.append(df_tmp[column].quantile(q))\n    df_tmp['bins'] = pd.cut(df_tmp[column],bins=bins,labels=labels)\n    bins_dict = {}\n    for label in labels:\n        iso_list = df_tmp[df_tmp['bins'] == label]['ISO3'].to_list()\n        bins_dict[label] = iso_list\n    \n    # for some reason the country with the smalles value is not in a bin\n    # therefore assign bin manually\n    minidx = df_tmp[column].idxmin()\n    iso3_min = df_tmp.loc[minidx]['ISO3']\n    bins_dict[labels[0]].append(iso3_min)\n    \n    return bins_dict\n        \n        \ndef add_bins_col(df,column,bins_col_name=None,year=2014,\n                 quantiles=[0,.2,.4,.6,.8,1],\n                 labels=[1,2,3,4,5]):\n    ''' this will add a column to a dataframe that holds quantile information\n    with respect to a passed column (in which group is the country)\n    per default 5 equal quantiles labeled 1,2,3,4,5\n    df ... DataFrame\n    column ... name of column, that defines the bins\n    year ... df is reduced to that year to define the bins\n    bins_col_name ... will default to column+' bins' if nothing is passed\n    labels ... names of bins that will end up in the bins column\n    '''\n    if not(bins_col_name):\n        bins_col_name = column + ' bins'\n        \n    df[bins_col_name] = np.nan \n    bins_dict = get_bins_dict(df,column,year,quantiles,labels)\n\n    for key in bins_dict:\n        for c in bins_dict[key]:\n            index_list = df[df['ISO3']==c].index\n            df.loc[index_list,bins_col_name] = key\n\n\ndef agg_over_bins_df(df,column,bins_column,mode='sum'):\n    df_agg = pd.DataFrame()\n    ''' this will return a dataframe that aggregates ('sum' or 'mean') over\n    quantile bins. The bins need to be defined in a passed bins_column\n    df ... DataFrame\n    column ... name of column over witch to aggregate\n    bins_column ... name of the column holding the bins\n    mode ... 'sum' or 'mean'\n    '''\n    labels = list(set(df[bins_column].to_list()))\n    years = list(set(df['Year'].to_list()))\n\n    for year in years:\n        for label in labels:            \n            indexer = df[((df['Year']==year) \n                            & (df[bins_column]==label))].index            \n            if mode == 'sum':\n                agg_val = df.loc[indexer,column].sum()\n            if mode == 'mean':\n                agg_val = df.loc[indexer,column].mean()\n            df_agg.loc[year,label] = agg_val\n\n    return df_agg\n\n\ndef get_shares(country_list,df,print_to_screen=True):\n    ''' this will print the share of countries country_list of \n    global co2,population and gdp to the screen\n    '''\n    share_pop = country_list_share(df,country_list,'population')[0]\n    share_co2 = country_list_share(df,country_list,'co2')[0]\n    share_gdp = country_list_share(df,country_list,'gdp')[0]\n    shares_dict={}\n    shares_dict['pop'] = share_pop\n    shares_dict['co2'] = share_co2\n    shares_dict['gpd'] = share_gdp\n    if print_to_screen:\n        print('number of countries and territories:',len(country_list))\n        print('share on global population is {:.2f} %'.format(100*share_pop))\n        print('share on global CO2 is {:.2f} %'.format(100*share_co2))\n        print('share on global GDP is {:.2f} %'.format(100*share_gdp))\n    return shares_dict\n        ","401ff5d6":"def pie_chart(ax,df,country_dict,column,year=2014,colors=None,explode=None):\n    ''' this will plot the ratios of the countries in country_dict of the\n    total value of column\n    e.g. shares of gdp-quantile bins in terms of global co2 emissions\n    ax ... plt.Axes object to draw into\n    df ... dataframe\n    country_dict ... {key1: country_list1, key2:country_list2, ...}\n    column ... name of column, we are interested in (e.g. 'co2')\n    year ... optional integer\n    '''\n    ratio_list = []\n    labels = []\n    if colors: \n        tmp = []\n        for key in country_dict:\n            tmp.append(colors[key])\n        colors = tmp\n\n    for key in country_dict:\n        ratio = country_list_share(df,country_dict[key],column,year=year)[0]\n        ratio_list.append(ratio)\n        labels.append(key)\n    ax.pie(ratio_list,labels=labels,colors=colors,explode=explode)\n\n        \ndef lineplot(ax,\n             x_values,\n             line_dict,\n             legend_pos=2,\n             col=None,\n             sorting=None):\n    ''' this will draw the elements of line_dict in a stacked line plot.\n    x_values ... an iterable with the x-Values\n    line_dict ... y-values\n                 {'name1': pd.Series1, 'name2': pd.Series2, ...}\n                 the names will show in the legend of the plot\n    legend_pos ... position of the legend in the plot (default: top left)\n    col ... an optional list of strings for the colors of the areas\n    '''\n    if not(col):\n        col = {}\n        for key in line_dict:\n            col[key] = None\n    if sorting:\n        keys = sorting\n    else:\n        keys = list(line_dict)            \n    #for key in line_dict:\n    for key in keys:\n        #plt.plot(x_values.to_list(),\n        ax.plot(x_values.to_list(),\n                 line_dict[key].to_list(),\n                 label=key,\n                 color = col[key])\n    ax.legend()\n    ax.grid()\n    #plt.legend()\n    #plt.grid()\n\n\ndef my_stackplot(ax, x_values, line_dict,legend_pos=2,col=None,ylab=None,\n                 sorting=None):\n    ''' this will draw the elements of line_dict in a stacked line plot.\n    x_values ... an iterable with the x-Values\n    line_dict ... y-values\n                 {'name1': pd.Series1, 'name2': pd.Series2, ...}\n                 the names will show in the legend of the plot\n    legend_pos ... position of the legend in the plot (default: top left)\n    col ... an optional dict colors - must have the same keys as line_dict\n    '''\n    stack_array = np.zeros([len(line_dict),len(x_values)])\n\n    if sorting:\n        keys = sorting\n        labels = sorting\n    else: \n        keys = list(line_dict)\n        labels = list(line_dict)\n\n    if col: \n        tmp = []\n        for key in keys:\n            tmp.append(col[key])\n        col = tmp\n    \n    index = 0\n    for key in keys:\n        stack_array[index,:] = line_dict[key].to_numpy()\n        index = index + 1\n    if ylab:\n        plt.ylabel(ylab)\n\n    ax.stackplot(x_values,stack_array,labels=labels,\n                  colors=col)\n    ax.legend(loc=legend_pos)\n    ax.grid()\n    #plt.stackplot(x_values,stack_array,labels=list(line_dict),\n    #              colors=col)\n    #plt.legend(loc=legend_pos)\n    #plt.grid()\n\n\ndef draw_global1(size=(8,6)):\n    ''' this will draw the stacked lineplot co2 sources\/sinks over time'''\n    \n    df1_infile = '\/kaggle\/working\/ID1_clean.csv'\n    df = pd.read_csv(df1_infile,sep=';')\n    df.set_index('Year',inplace=True)\n    \n    fossil_list = ['Coal', 'Oil', 'Gas', 'Flaring', 'Cement']\n    sources_list = ['fossil fuel and industry', 'land-use change emissions']\n    sinks_list = ['ocean sink', 'land sink', 'atmospheric growth', 'budget imbalance']\n    fossil_list = ['Coal', 'Oil', 'Gas', 'Flaring', 'Cement']\n\n    \n    # https:\/\/matplotlib.org\/3.1.0\/gallery\/color\/named_colors.html    \n    sources_colors = {'fossil fuel and industry':'indianred',\n                      'land-use change emissions': 'orange'}\n    sinks_colors = {'ocean sink':'darkslateblue',\n                    'land sink':'forestgreen',\n                    'atmospheric growth':'cornflowerblue',\n                    'budget imbalance':'lightgrey'}\n\n    fossil_dict = {}    \n    for key in fossil_list:\n        fossil_dict[key] = df[key]\n\n    sources_dict = {}    \n    for key in sources_list:\n        sources_dict[key] = df[key]\n\n    sinks_dict = {}    \n    for key in sinks_list:\n        # make sinks negative for display in plot\n        sinks_dict[key] = -df[key]\n\n    fig = plt.figure(figsize=size)\n    ax = fig.add_axes([0.15,0.15,.85,.85])\n\n    my_stackplot(ax,df.index,sources_dict,col=sources_colors)\n    my_stackplot(ax,df.index,sinks_dict,col=sinks_colors)\n    plt.ylabel('million tonnes of CO2')\n    plt.grid()\n\ndef draw_global2(size=(8,6)):\n    ''' this will draw the stacked lineplot of fossil fuels'''\n    \n    df1_infile = '\/kaggle\/working\/ID1_clean.csv'\n    df = pd.read_csv(df1_infile,sep=';')\n    df.set_index('Year',inplace=True)\n    \n    fossil_list = ['Coal', 'Oil', 'Gas', 'Flaring', 'Cement']\n    fossil_cols = {'Coal':'black',\n                   'Oil': 'red', \n                   'Gas': 'green', \n                   'Flaring': 'orange', \n                   'Cement':'grey'}    \n    fossil_dict = {}    \n    for key in fossil_list:\n        fossil_dict[key] = df[key]\n\n    fig = plt.figure(figsize=size)\n    ax = fig.add_axes([0.15,0.15,.8,.8])\n    lineplot(ax,df.index,fossil_dict,col=fossil_cols)\n    plt.ylabel('million tonnes of CO2')\n\n\ndef draw_co2_countries1(size=(16,6),\n                        column='co2',\n                        ylabel='million tonnes of CO2',\n                       omit_stack=False):\n    ''' this will draw the total co2 countries grouped by gdp\/capita over time\n    '''    \n    file = '\/kaggle\/working\/df_joined.csv'\n    df = pd.read_csv(file,sep=';')\n    df['co2 per capita'] = df['co2'] \/ df['population']\n        \n    # make gdp\/capita column\n    df['gdp per capita'] = df['Gross Domestic Product (GDP)'] \/ df['population']\n    gdp_labels = ['lowest gdp\/cap',\n                  'low gdp\/cap',\n                  'medium gdp\/cap',\n                  'high gdp\/cap',\n                  'highest gdp\/cap']    \n\n    # make bins (baskets) of gdp\/capita categories (default: 5 categories)\n    add_bins_col(df,'gdp per capita',labels=gdp_labels)\n\n    # create a dataframe with the data to plot (sum co2 over the created bins)\n    df_agg_co2 = agg_over_bins_df(df,column,'gdp per capita bins',mode='sum')\n\n\n    # color dictionary is optional\n    col_dict  = {'lowest gdp\/cap': 'firebrick',\n                 'low gdp\/cap': 'forestgreen',\n                 'medium gdp\/cap': 'orange',\n                 'high gdp\/cap': 'thistle',\n                 'highest gdp\/cap': 'slateblue'}\n\n    # make the dictionary of the (stacked) lines and plot\n    line_dict = {}\n    for col in df_agg_co2.columns:\n        line_dict[col] = df_agg_co2[col]\n        \n\n    if omit_stack:\n        fig = plt.figure(figsize=size)\n        ax = fig.add_axes([0.15,0.15,.8,.8])\n        lineplot(ax,df_agg_co2.index,line_dict,col=col_dict,sorting=gdp_labels)\n        plt.ylabel('million tonnes of CO2')\n    else:\n        # plotting - make figure and axes object        \n        #fig = plt.figure(figsize=size)\n        fig,(ax1,ax2) = plt.subplots(1,2,figsize=size)\n        #ax = fig.add_axes([0.12,0.12,.85,.85]) # position in the figure window\n\n        # create plot (by passing the axes object to the plot function)\n        ax1.set(ylabel=ylabel)\n        ax2.set(ylabel=ylabel)\n        my_stackplot(ax1,df_agg_co2.index,line_dict,col=col_dict,sorting=gdp_labels)\n        lineplot(ax2,df_agg_co2.index,line_dict,col=col_dict,sorting=gdp_labels)\n    \n    \ndef draw_pop_gdp_share1(size=(15,6)):\n    '''this will draw two pie-charts showing share of the bins with \n    respect to global population and global co2 emissions\n    '''\n    file='\/kaggle\/working\/df_joined.csv'\n    df = pd.read_csv(file,sep=';')\n    df['gdp\/capita'] = df['Gross Domestic Product (GDP)'] \/ df['population']   \n    \n    gdp_labels = ['lowest gdp\/cap',\n                  'low gdp\/cap',\n                  'medium gdp\/cap',\n                  'high gdp\/cap',\n                  'highest gdp\/cap']    \n    bins_dict = get_bins_dict(df,'gdp\/capita',labels=gdp_labels)\n        \n    df_global = pd.read_csv('\/kaggle\/working\/ID1_clean.csv',sep=';')\n    df_global.set_index('Year',inplace=True)\n        \n    col_dict  = {'lowest gdp\/cap': 'firebrick',\n                 'low gdp\/cap': 'forestgreen',\n                 'medium gdp\/cap': 'orange',\n                 'high gdp\/cap': 'thistle',\n                 'highest gdp\/cap': 'slateblue'}\n    \n    fig,(ax1,ax2) = plt.subplots(1,2,figsize=size)\n    pie_chart(ax1,df,bins_dict,'population',\n              colors=col_dict,explode=[0,0,.1,0,0])\n    ax1.set_title('Global population share')\n    pie_chart(ax2,df,bins_dict,'Gross Domestic Product (GDP)',\n              colors=col_dict,explode=[0,0,.1,0,0])\n    ax2.set_title('Global GDP share')\n\n\ndef draw_bin3_shares(size=(16,6)):\n    ''' this will draw a pie-chart, showing that the medium gdp\/capita\n    bin is dominated by china\n    '''\n    file = '\/kaggle\/working\/df_joined.csv'\n    df = pd.read_csv(file,sep=';')\n\n    df['gdp\/capita'] = df['Gross Domestic Product (GDP)'] \/ df['population']\n    gdp_labels = ['lowest gdp\/cap',\n                  'low gdp\/cap',\n                  'medium gdp\/cap',\n                  'high gdp\/cap',\n                  'highest gdp\/cap']    \n\n    bins_dict = get_bins_dict(df,'gdp\/capita',labels=gdp_labels)\n    \n    year = 2014\n    # get a dataframe that only holds the countries of bin3\n    df_bin3 = df[df['ISO3'].isin(bins_dict['medium gdp\/cap'])].copy()\n    df_bin3 = df_bin3[df_bin3['Year']==year].copy()\n\n    cols=['ISO3','CountryName','co2']\n    df_bin3 = df_bin3[cols].copy()\n    df_bin3.set_index('ISO3',inplace=True)\n    shares = []\n    bin3_share = country_list_share(df,bins_dict['medium gdp\/cap'],'co2')[0]\n    for c in bins_dict['medium gdp\/cap']:\n        c_share = country_list_share(df,[c],'co2')[0] \/ bin3_share\n        shares.append(c_share)\n    \n    df_bin3['bins'] = 'others'\n    df_bin3.loc['CHN','bins'] = 'China'\n    df_bin3.loc['ZAF','bins'] = 'South Africa'\n    df_bin3.loc['THA','bins'] = 'Thailand'\n\n    bins = set(df_bin3['bins'].to_list())\n    color_dict = {'China': 'orange',\n                  'Thailand': 'lightcoral',\n                  'South Africa': 'chocolate',\n                  'others': 'grey' }\n    colors = []\n    sums_dict = {}\n    countries = []\n    sums = []\n    for mybin in bins:\n        indexer = df_bin3[df_bin3['bins'] == mybin].index\n        agg_val = df_bin3.loc[indexer,'co2'].sum()\n        countries.append(mybin)\n        sums.append(agg_val)\n        sums_dict[mybin] = agg_val\n        colors.append(color_dict[mybin])\n    \n    fig = plt.figure(figsize=size)\n    #ax = fig.add_axes([0.15,0.15,.85,.85])\n    ax = fig.add_axes([0,0,1,.9])\n    ax.pie(sums,labels=countries,colors=colors)\n    ax.set_title('medium gdp\/capita countries share on CO2 emissions')\n\ndef draw_co2_prod_cons(size=(16,6)):\n    file = '\/kaggle\/working\/ID2a_clean.csv'\n    df_prod = pd.read_csv(file,sep=';')\n    df_cons = df_prod.copy()\n    df_cons.drop('co2',axis=1,inplace=True)\n    df_cons['co2'] = df_cons['consumption_co2']\n    \n    gdp_labels = ['lowest gdp\/cap',\n                  'low gdp\/cap',\n                  'medium gdp\/cap',\n                  'high gdp\/cap',\n                  'highest gdp\/cap']    \n\n    # make bins (baskets) of gdp\/capita categories (default: 5 categories)\n    add_bins_col(df_prod,'gdp per capita',labels=gdp_labels)\n    add_bins_col(df_cons,'gdp per capita',labels=gdp_labels)\n\n    # create a dataframe with the data to plot (sum co2 over the created bins)\n    df_agg_co2_prod = agg_over_bins_df(df_prod,'co2','gdp per capita bins',mode='sum')\n    df_agg_co2_cons = agg_over_bins_df(df_cons,'co2','gdp per capita bins',mode='sum')\n\n    # color dictionary is optional\n    col_dict  = {'lowest gdp\/cap': 'firebrick',\n                 'low gdp\/cap': 'forestgreen',\n                 'medium gdp\/cap': 'orange',\n                 'high gdp\/cap': 'thistle',\n                 'highest gdp\/cap': 'slateblue'}\n\n    # make the dictionary of the (stacked) lines and plot\n    line_dict_prod = {}\n    line_dict_cons = {}\n    for col in df_agg_co2_prod.columns:\n        line_dict_prod[col] = df_agg_co2_prod[col]\n        line_dict_cons[col] = df_agg_co2_cons[col]\n        \n        \n    # plotting - make figure and axes object        \n    #fig = plt.figure(figsize=size)\n    fig,(ax_prod,ax_cons) = plt.subplots(1,2,figsize=size)\n    #ax = fig.add_axes([0.12,0.12,.85,.85]) # position in the figure window\n\n    # create plot (by passing the axes object to the plot function)\n    ax_prod.set(ylabel='million tonnes of CO2 (production based)')\n    ax_cons.set(ylabel='million tonnes of CO2 (consumption based)')\n\n    ax_cons.set_title('Consumption based CO2')\n    ax_prod.set_title('Production based CO2')\n    \n    lineplot(ax_prod,df_agg_co2_prod.index,line_dict_prod,col=col_dict,sorting=gdp_labels)\n    ax_cons.set_ylim(ax_prod.get_ylim())\n    lineplot(ax_cons,df_agg_co2_cons.index,line_dict_cons,col=col_dict,sorting=gdp_labels)\n\n    \ndef draw_top_ctr_bar(df,column,add_china=False,size=(8,6)):\n    ''' this will draw a bar plot of the top 10 countries in a certain aspect\n    optinally china can be included manually\n    '''       \n    df.sort_values(column,ascending=False,inplace=True)\n    x = df['CountryName'][:10].to_list()\n    y = df[column][:10].to_list()\n    \n    if add_china:\n        x.append('China')\n        index_ch = df[df['ISO3']=='CHN'].index\n        val_ch = float(df.loc[index_ch][column])\n        y.append(val_ch)\n    fig = plt.figure(figsize=size)\n    ax = fig.add_axes([0.2,0.3,.6,.6])\n    plt.xticks(rotation=90)\n    ax.bar(x,y)\n\ndef draw_top_acuco2pc(year=2014,size=(8,6)):\n    ''' draw top 10 countries ito acumulated co2 per capita plot '''\n    df_ID2a = pd.read_csv('\/kaggle\/working\/ID2a_clean.csv',sep=';')    \n    df_ID2a2014 = df_ID2a[df_ID2a['Year'] == 2014].copy()\n    df_ID2a2014['cumulative_co2 per capita'] = df_ID2a2014['cumulative_co2'] \/ df_ID2a2014['population']\n    draw_top_ctr_bar(df_ID2a2014,'cumulative_co2 per capita',add_china=True)\n    plt.ylabel('million tonnes of CO2 per capita')\n    plt.title('Top countries in terms of acumulated CO2 emissions per capita')\n\ndef draw_top_acuco2(year=2014,size=(8,6)):\n    ''' draw top 10 countries ito acumulated co2 plot '''\n    df_ID2a = pd.read_csv('\/kaggle\/working\/ID2a_clean.csv',sep=';')    \n    df_ID2a2014 = df_ID2a[df_ID2a['Year'] == 2014].copy()\n    draw_top_ctr_bar(df_ID2a2014,'cumulative_co2')\n    plt.ylabel('million tonnes of CO2 per capita')\n    plt.title('Top countries in terms of acumulated CO2 emissions')","59e86fdf":"## Load current ISO3 definition and add ISO3 Definition for no longer exisiting states\n## Results:\n## DataFrame with with rows 'CountryName' and 'ISO3'\n## csv file of this data frame\n\n\n# read ISO3 country codes from html \n# and return iso3 DataFrame with rows 'CountryName' and 'ISO3'\ndef load_iso3_countries(path):\n    \n    # download from, save as html\n    # https:\/\/github.com\/lukes\/ISO-3166-Countries-with-Regional-Codes\/blob\/master\/all\/all.csv\n    fn = path + 'github_iso3.htm'\n      \n    # iso3c_df=pd.read_fwf(fn, header=None, )\n    list_df=pd.read_html(fn)\n\n    # get first dataframe from list of dataframes    \n    df=list_df[0]\n\n    # select and copy needed rows\n    cut_df=df.loc[:,['name', 'alpha-3']]\n\n    # rename column names    \n    iso3c_df= cut_df.rename(columns={\"name\": \"CountryName\", \"alpha-3\": \"ISO3\"})\n    \n    \n    # add no longer existing countries\n    dict_old_countries = { 'USSR'           : 'SUN' ,\n                           'Czechoslovakia' : 'CSK' , \n                           'Yugoslavia'     : 'YUG' ,\n                           'Vietnam, N.'    : 'VDR' ,                 \n                           'Vietnam, S.'    : 'SVI' ,                 \n                           'Germany, W. '   : 'DEU' ,\n                           'Germany, E. '   : 'DDR' ,\n                           'Serbia and Montenegro' : 'SCG',\n                           'Zaire'          : 'ZAR' ,\n                           'Burma'          : 'BUR'\n                         }\n    \n    co=[\"CountryName\", \"ISO3\" ] \n    \n    cn_=pd.Series([  'USSR'           ,\n                     'Czechoslovakia' , \n                     'Yugoslavia'     ,\n                     'Vietnam, N.'    ,                 \n                     'Vietnam, S.'    ,                 \n                     'Germany, W. '   ,\n                     'Germany, E. '   ,\n                     'Serbia and Montenegro' ,\n                     'Zaire'          ,\n                     'Burma'                        \n                  ])\n    \n    \n    iso3_=pd.Series(['SUN' ,\n                     'CSK' , \n                     'YUG' ,\n                     'VDR' ,                 \n                     'SVI' ,                 \n                     'DEU' ,\n                     'DDR' ,\n                     'SCG',\n                     'ZAR' ,\n                     'BUR'\n                     ])\n    \n    \n    rl = pd.DataFrame({co[0] : cn_,\n                       co[1] : iso3_ })\n    \n    # add new rows to resulting DataFrame \n    iso3df_ = iso3c_df.append(rl, ignore_index=True)\n\n    # set index to 'CountryName'\n    iso3df= iso3c_df.set_index('CountryName')\n\n\n    \n    return iso3df_\n\n\n\n############################################################\n\n# Resulting iso3df with columnes 'CountryName' 'ISO3':\niso3df =  load_iso3_countries('\/kaggle\/input\/co2emissions\/')\n\n# Write ISO3 Dataframe to csv file\niso3df.to_csv('\/kaggle\/working\/CountryName_ISO3.csv',sep=';')","9ef62da0":"def ID1_preproc(write_csv=False, \n                file_in='\/kaggle\/input\/co2emissions\/Global_Carbon_Budget_2019v1.0.xls',\n                file_out='\/kaggle\/working\/ID1_clean.csv'):\n    ''' this will preprocess the Global Carbon Project dataset'''\n    \n    \n    df_budget = pd.read_excel(file_in, \n                              sheet_name=\"Global Carbon Budget\",\n                              header=18)\n\n    df_fossil = pd.read_excel(file_in, \n                              sheet_name=\"Fossil Emissions by Fuel Type\",\n                              header=12)\n    df_budget.drop('Unnamed: 0',axis=1,inplace=True)\n    df_fossil.drop('Total',axis=1,inplace=True)\n    \n    # drop empty lines that openpyxl created in its wisdom\n    df_budget.dropna(axis=1,how='all',inplace=True)   \n    df_budget.dropna(axis=0,how='all',inplace=True)        \n    df_fossil.dropna(axis=1,how='all',inplace=True)        \n    df_fossil.dropna(axis=0,how='all',inplace=True)\n    \n    # use year as index\n    df_budget.set_index('Year',inplace=True)\n    df_fossil.set_index('Year',inplace=True)\n    \n\n    # scaling\n    # ---------\n    # df_budget is in billion tonnes of carbon per year\n    # rescale it to million tonnes of co2 per year    \n    # 1 million tonnes of carbon = 3.664 million tonnes of CO2\n    factor = 3.664\n    \n    # df_budget is in billion tons of carbon\n    df_budget = 1000*factor * df_budget\n    # df_fossil is in million tons of carbon\n    df_fossil = factor * df_fossil\n    \n    df_joined = pd.concat([df_budget, df_fossil],\n                           join='outer',\n                           axis=1,\n                           sort=False)\n    if write_csv:\n        df_joined.to_csv(file_out,sep=';')\n    return df_joined","52bd363e":"def ID2_preproc(write_csv=False, include_gdp=False, print_test=False,\n                file_in='\/kaggle\/input\/co2emissions\/owid-co2-data.csv',\n                file_out='\/kaggle\/working\/ID2_clean.csv'):\n    ''' this will save the OWID data to 'ID2_clean.csv'\n    '''\n    df = pd.read_csv(file_in,sep=',')\n\n    #rename columns\n    #rename_map = {'year':'Year',\n    #              'country':'CountryName',\n    #              'iso_code':'ISO3'}\n    #df.rename(rename_map,axis=1,inplace=True)\n\n    # kick out lines without ISO3 code\n    #row_list = df[df['ISO3'].isna()].index.to_list()\n    \n    # kick out lines with iso3-codes OWID_WRL and OWID_KOS\n    #idx_world = df.index[df['ISO3'] == 'OWID_WRL'].to_list()\n    #idx_kos = df.index[df['ISO3'] == 'OWID_KOS'].to_list()\n    #df.drop(index=idx_world,inplace=True)\n    #df.drop(index=idx_kos,inplace=True)\n    \n    #df.drop(row_list,inplace=True)\n    \n    ID2_preproc_format_only(df)\n    \n    \n    add_ISO3Year(df)\n\n    # only keep co2 and population\n    keeper_list = ['ISO3Year','ISO3','Year','CountryName','co2','population']\n    if include_gdp:\n        keeper_list = ['ISO3Year','ISO3','Year','CountryName','co2','population','gdp']\n    df = df[keeper_list].copy()\n\n    # drop all countries where co2 or population is not completely\n    # available between 1972 and 2016\n    drop_list = list(data_not_avl(df,1972,2016,['co2','population']))\n    for c in drop_list:\n        idx_drop = df.index[df['ISO3'] == c].to_list()\n        df.drop(index=idx_drop,inplace=True)\n    dropped_countries = drop_list\n    \n    # drop years before 1972 and after 2016\n    idx_drop = df.index[df['Year'] < 1972].to_list()\n    df.drop(index=idx_drop,inplace=True)\n    idx_drop = df.index[df['Year'] > 2016].to_list()\n    df.drop(index=idx_drop,inplace=True)\n\n    # deal with gdp\n    if include_gdp:\n        # kick out countries with no gdp info\n        loc = list(set(df['ISO3'].to_list()))\n        df_avl_gdp = avl_report(df,loc,'gdp')\n        ltk = df_avl_gdp[df_avl_gdp['first_year'] == -1]['ISO3'].to_list()\n        for c in ltk:\n            idx_drop = df.index[df['ISO3'] == c].to_list()\n            df.drop(index=idx_drop,inplace=True)\n        # kick out slovakia and u.arab.emirates (not enough gdp info)\n        ltk = ['SVK','ARE']\n        for c in ltk:\n            idx_drop = df.index[df['ISO3'] == c].to_list()\n            df.drop(index=idx_drop,inplace=True)\n        dropped_countries = dropped_countries + ltk\n    \n    # make sure the data is complete\n    if print_test:\n        if df.isna().any().any():\n            print('Error: there are still NaN values in the dataframe')\n        else:\n            print('no NaN values left in df')\n        num_years = len(set(df['Year'].to_list()))\n        num_countries = len(set(df['ISO3'].to_list()))\n        if not(num_years * num_countries == df.shape[0]):\n            print('Error: countries x years is not number of rows!')\n        else:\n            print('df data is complete')\n    \n    if write_csv:\n        df.to_csv(file_out,sep=';')\n\n    return df\n    #return df,dropped_countries\n\ndef ID2_preproc_format_only(df):    \n    #rename columns\n    rename_map = {'year':'Year',\n                  'country':'CountryName',\n                  'iso_code':'ISO3'}\n    df.rename(rename_map,axis=1,inplace=True)\n\n    # kick out lines without ISO3 code\n    row_list = df[df['ISO3'].isna()].index.to_list()\n    df.drop(row_list,inplace=True)\n    \n    # kick out lines with iso3-codes OWID_WRL and OWID_KOS\n    idx_world = df.index[df['ISO3'] == 'OWID_WRL'].to_list()\n    idx_kos = df.index[df['ISO3'] == 'OWID_KOS'].to_list()\n    df.drop(index=idx_world,inplace=True)\n    df.drop(index=idx_kos,inplace=True) \n    \n    \ndef get_ISO3_list(file='\/kaggle\/working\/df_joined.csv'):\n    ''' returns the list of country-ISO codes in the joined dataframe'''\n    df = pd.read_csv(file,sep=';')\n    return set(df['ISO3'].to_list())\n\n\ndef ID2a_preproc(write_csv=False, \n                 file_in='\/kaggle\/input\/co2emissions\/owid-co2-data.csv',\n                 joined_df_file = '\/kaggle\/working\/co2emissions\/df_joined.csv',\n                 file_out='\/kaggle\/working\/ID2a_clean.csv'):\n    ''' this will return the dataframe with reduced countries (58) and reduced\n    time period (1990-2014) but extended number of columns'\n    it will also optionally write out a csv file\n    '''\n    df_ID2a = pd.read_csv(file_in,sep=',')\n    ID2_preproc_format_only(df_ID2a)\n    add_ISO3Year(df_ID2a)\n    #df_ID2_alt = ID2_preproc()[0]\n    \n    # drop years before 1990 and after 2014\n    idx_drop = df_ID2a.index[df_ID2a['Year'] < 1990].to_list()\n    df_ID2a.drop(index=idx_drop,inplace=True)\n    idx_drop = df_ID2a.index[df_ID2a['Year'] > 2014].to_list()\n    df_ID2a.drop(index=idx_drop,inplace=True)\n    \n    col_list = ['co2',\n                'gdp',\n                'population',\n                'consumption_co2',\n                'cumulative_co2',\n                #'co2_per_unit_energy',\n                'cement_co2',\n                'coal_co2', \n                #'flaring_co2', \n                'gas_co2', \n                'oil_co2',\n                'primary_energy_consumption']\n\n    # --------------------------------------------------------------------\n    # get the list of countries, that need to be dropped by assembling\n    # data availability reports for each required column and then dropping\n    # rows, that show incomplete data\n    # --------------------------------------------------------------------\n    iso3_list = get_ISO3_list()    \n    df_list = []\n    for col in col_list:\n        df_tmp = avl_report(df_ID2a,iso3_list,col)\n        df_tmp.set_index('ISO3',inplace=True)\n        df_list.append(df_tmp)\n\n    # join the availability reports\n    df_joined = pd.concat(df_list,\n                          join='outer',\n                          axis=1,\n                          sort=False)\n\n    # drop countries that have at least one -1\n    drop_list1 = []\n    for (idx,row) in df_joined.iterrows():\n        if row.isin([-1]).any():\n            drop_list1.append(idx)        \n    df_joined.drop(index=drop_list1,inplace=True)\n    \n    # drop countries that do not go from 1990 to 2014\n    drop_list2 = []\n    for (idx,row) in df_joined.iterrows():\n        if row.isin(range(1991,2014)).any():\n            drop_list2.append(idx)        \n    df_joined.drop(index=drop_list2,inplace=True)\n\n    # drop countries with gaps in the data\n    drop_list3 = []\n    for (idx,row) in df_joined.iterrows():\n        if row.isin([False]).any():\n            drop_list3.append(idx)        \n    df_joined.drop(index=drop_list3,inplace=True)\n\n    # remaining countries\n    rem_countries = df_joined.index.to_list()\n    \n    # drop countries from dataframe\n    all_countries = set(df_ID2a['ISO3'].to_list())\n    drop_list = list(all_countries - set(rem_countries))\n    drop_ISO3_list(df_ID2a,drop_list)\n    \n    \n    keeper_list = ['ISO3Year', 'ISO3','Year','CountryName'] + col_list\n    df_ID2a = df_ID2a[keeper_list].copy()\n    df_ID2a.set_index('ISO3Year',inplace=True)\n\n    df_ID2a['gdp per capita'] = df_ID2a['gdp'] \/ df_ID2a['population']\n    df_ID2a['co2 per capita'] = df_ID2a['co2'] \/ df_ID2a['population']\n    df_ID2a['co2 per gdp'] = df_ID2a['co2'] \/ df_ID2a['gdp']\n\n    if write_csv:\n        df_ID2a.to_csv(file_out,sep=';')        \n        \n    return df_ID2a    ","1f88d6f4":"def ID3_prep(file, write_csv=True):\n    '''\n    Reading and modifying the data provided by the UN for the national accounts of all the countries available.\n    '''\n    ## load data\n    data = pd.read_excel(file, sheet_name=\"Download-GDPconstant-USD-countr\", skiprows=2)\n    iso3_un = pd.read_excel(\"\/kaggle\/input\/co2emissions\/iso3_un.xlsx\", sheet_name=\"Correspondence\")\n\n    ## Drop some dimensions\n    data = data[~data.IndicatorName.isin([\"Household consumption expenditure (including Non-profit institutions serving households)\",\n                                                   \"General government final consumption expenditure\",\n                                                   \"Gross capital formation\",\n                                                   \"Gross fixed capital formation (including Acquisitions less disposals of valuables)\",\n                                                   \"Changes in inventories\"])]\n\n    ## merge with iso-code\n    data = data.merge(iso3_un[['UN-Code', 'ISO3']], left_on=\"CountryID\", right_on=\"UN-Code\")\n\n    ## drop columns\n    data = data.drop(['CountryID', \"UN-Code\", \"Country\"], axis=1)\n\n    ## declare soviet union and yugoslavia\n    ussr = [\"ARM\", \"AZE\", \"BLR\", \"EST\", \"GEO\", \"KAZ\", \"KGZ\", \"LVA\", \"LTU\", \"MDA\", \"RUS\", \"TJK\", \"TKM\", \"UKR\", \"UZB\"]\n    yugo = [\"BIH\", \"HRV\", \"CZE\", \"UNK\", \"MNE\", \"MKD\", \"SRB\", \"SVK\", \"SVN\"]\n\n    ## missing values for USSR and Yugoslavia\n    for ind in data.IndicatorName.unique():\n        sum_region = \\\n            data.loc[(data[\"ISO3\"] == \"SUN\") & (data[\"IndicatorName\"] == ind)][1990]\n        if (sum_region.empty== True):\n            sum_region = float(\"NAN\")\n        else:\n            sum_region = sum_region.values[0]\n\n        data.loc[(data[\"ISO3\"].isin(ussr)) & (data[\"IndicatorName\"] == ind), \"help_share\"] = \\\n            data.loc[(data[\"ISO3\"].isin(ussr)) & (data[\"IndicatorName\"] == ind), 1990] \/ sum_region\n\n        sum_region = \\\n            data.loc[(data[\"ISO3\"] == \"YUG\") & (data[\"IndicatorName\"] == ind)][1990]\n\n        if (sum_region.empty== True):\n            sum_region = float(\"NAN\")\n        else:\n            sum_region = sum_region.values[0]\n\n        data.loc[(data[\"ISO3\"].isin(yugo)) & (data[\"IndicatorName\"] == ind), \"help_share\"] = \\\n            data.loc[(data[\"ISO3\"].isin(yugo)) & (data[\"IndicatorName\"] == ind), 1990] \/ sum_region\n\n        for year in range(1970, 1990):\n\n            sum_region = \\\n                data.loc[(data[\"ISO3\"] == \"SUN\") & (data[\"IndicatorName\"] == ind)][year]\n            if (sum_region.empty == True):\n                sum_region = float(\"NAN\")\n            else:\n                sum_region = sum_region.values[0]\n            data.loc[(data[\"ISO3\"].isin(ussr)) & (data[\"IndicatorName\"] == ind), year] = \\\n                data.loc[(data[\"ISO3\"].isin(ussr)) & (data[\"IndicatorName\"] == ind), \"help_share\"] * \\\n                sum_region\n\n            sum_region = \\\n                data.loc[(data[\"ISO3\"] == \"YUG\") & (data[\"IndicatorName\"] == ind)][year]\n            if (sum_region.empty == True):\n                sum_region = float(\"NAN\")\n            else:\n                sum_region = sum_region.values[0]\n            data.loc[(data[\"ISO3\"].isin(yugo)) & (data[\"IndicatorName\"] == ind), year] = \\\n                data.loc[(data[\"ISO3\"].isin(yugo)) & (data[\"IndicatorName\"] == ind), \"help_share\"] * \\\n                sum_region\n\n    del(ind, year)\n    data = data.drop(['help_share'], axis=1)\n\n    ## drop Countries\n    data = data[~data.ISO3.isin(['SDN', \"ETH\", \"YEM\", \"YUG\", \"SUN\"])]\n\n    ## reshape data\n    data = data.melt(id_vars=[\"ISO3\", \"IndicatorName\"], var_name=\"year\")\n    data = data.pivot(index=['year', 'ISO3'], columns=\"IndicatorName\", values=\"value\")\n\n    ## Index\n    data = data.sort_values(by=['ISO3', \"year\"])\n    data = data.reset_index()\n    data[\"ISO3Year\"] = data[\"ISO3\"] + data[\"year\"].astype(str)\n    data = data.set_index([\"ISO3Year\"])\n\n    ## extrapolate indicators with gdp\n    for cy in data['ISO3'].unique():\n        df = data.loc[(data[\"ISO3\"] == cy)]\n        for ind in df.columns.unique()[list(range(2, 5)) + list(range(7, 14))]:\n            missing = df[ind].isna()\n            # data['Gross Domestic Product (GDP)'].pct_change()\n            if (any(missing) and all(~(df['Gross Domestic Product (GDP)'].isna()))):\n                missing = missing.shift(fill_value=True)\n                df_gdp = df.loc[missing, 'Gross Domestic Product (GDP)']\n                oldest = len(df_gdp.index) - 1\n                df_gdp = df_gdp \/ df_gdp.iloc[oldest]\n                df_new = df.iloc[oldest][ind] * df_gdp\n                df = df.copy()\n                df.loc[missing, ind] = df_new\n        data.loc[(data[\"ISO3\"] == cy)] = df\n\n    del (missing, df, oldest, df_new, df_gdp, ind, cy)\n\n    ## drop NAs\n    data = data.dropna()\n\n    # data.columns\n\n    ## Calculate new variables (shares and abs)\n    # sum of VA for shares\n    data['VA_sum'] = (data['Agriculture, hunting, forestry, fishing (ISIC A-B)'] +\n                      data['Mining, Manufacturing, Utilities (ISIC C-E)'] +\n                      data['Construction (ISIC F)'] +\n                      data['Wholesale, retail trade, restaurants and hotels (ISIC G-H)'] +\n                      data['Transport, storage and communication (ISIC I)'] +\n                      data['Other Activities (ISIC J-P)'])\n\n    data['share_prim'] = data['Agriculture, hunting, forestry, fishing (ISIC A-B)'] \/ data['VA_sum']\n    data['share_C_E'] = data['Mining, Manufacturing, Utilities (ISIC C-E)'] \/ data['VA_sum']\n    data['share_D'] = data['Manufacturing (ISIC D)'] \/ data['VA_sum']\n    data['share_F'] = data['Construction (ISIC F)'] \/ data['VA_sum']\n    data['share_G_H'] = data['Wholesale, retail trade, restaurants and hotels (ISIC G-H)'] \/ data['VA_sum']\n    data['share_I'] = data['Transport, storage and communication (ISIC I)'] \/ data['VA_sum']\n    data['share_J_P'] = data['Other Activities (ISIC J-P)'] \/ data['VA_sum']\n\n    data['abs_sec'] = (data['Mining, Manufacturing, Utilities (ISIC C-E)'] +\n                       data['Construction (ISIC F)'])\n\n    data['share_sec'] = data['abs_sec'] \/ data['VA_sum']\n\n    data['abs_tert'] = (data['Wholesale, retail trade, restaurants and hotels (ISIC G-H)'] +\n                        data['Transport, storage and communication (ISIC I)'] +\n                        data['Other Activities (ISIC J-P)'])\n\n    data['share_tert'] = data['abs_tert'] \/ data['VA_sum']\n\n    data['import_share'] = data['Imports of goods and services'] \/ data['Gross Domestic Product (GDP)']\n    data['export_share'] = data['Exports of goods and services'] \/ data['Gross Domestic Product (GDP)']\n\n    data = data.rename(columns={'Agriculture, hunting, forestry, fishing (ISIC A-B)': 'abs_prim'})\n\n    data = data.reindex(data.columns[[0, 1, 6, 5, 7, 4, 26, 27, 11, 14, 2, 22, 24, 15, 23, 25, 9, 8, 3, 13, 12, 10, 16, 17, 18, 19, 20, 21]], axis=1)\n\n    ## rename\n    data = data.rename(columns={\"year\": \"Year\"})\n\n    ## save\n    data.to_csv(\"\/kaggle\/working\/ID03_gdp_sector.csv\", sep=\";\")\n\n    return data","2c0c77b7":"## ID7 Country Ratings:\n## generate DataFrame and csv file with columns:\n## Year Countrname PR CL Status ISO3\n\n\n# read ISO3 country codes from html \n# and return iso3 data frame with rows 'CountryName' and 'ISO3'\ndef load_iso3_countries(path):\n    \n    # download from, save as html\n    # https:\/\/github.com\/lukes\/ISO-3166-Countries-with-Regional-Codes\/blob\/master\/all\/all.csv\n    fn = path + 'github_iso3.htm'\n      \n    # iso3c_df=pd.read_fwf(fn, header=None, )\n    list_df=pd.read_html(fn )\n\n    # get first dataframe from list of dataframes    \n    df=list_df[0]\n\n    # select and copy needed rows\n    cut_df=df.loc[:,['name', 'alpha-3']]\n\n    # rename column names    \n    iso3c_df= cut_df.rename(columns={\"name\": \"CountryName\", \"alpha-3\": \"ISO3\"})\n    \n    # set index to 'CountryName'\n    iso3df= iso3c_df.set_index('CountryName')\n    \n    return iso3df\n\n\n# read ISO3 country codes from preprocessed csv file\n# and return iso3 data frame with rows 'CountryName' and 'ISO3'\ndef load_iso3_csv(path):\n\n    \n    # original source data:\n    # https:\/\/github.com\/lukes\/ISO-3166-Countries-with-Regional-Codes\/blob\/master\/all\/all.csv\n    # already converted to csv file\n    fn = path + 'CountryName_ISO3.csv'\n      \n    # iso3c_df=pd.read_fwf(fn, header=None, )\n    iso3c_df=pd.read_csv(fn, sep=';' )\n    \n    # set index to 'CountryName', preparation for join\n    iso3df= iso3c_df.set_index('CountryName')\n\n\n    return iso3df\n\n    \n    \n# read ID 7 Global Country Ratings\ndef load_country_rating_data(path):\n\n    fn = path +'2020_Country_and_Territory_Ratings_and_Statuses_FIW1973-2020.xlsx'\n\n\n    crdf = pd.read_excel(fn,sheet_name='Country Ratings, Statuses ')\n\n    \n    return crdf\n\n\n# process missing data in country political ratings\ndef manage_missing_id7( crdf ):\n\n    # for merge and split of countries backward fill appears reasonable\n    \n    # mark missing data as na\n    \n    \n    # perfrom backward fill : Problem Germany gets vlalues of Ghana\n    # crfilldf=crdf.fillna(method='bfill')\n\n    # crfilldf=crdf.replace(to_replace='-',method='ffill')\n\n    # strategy for filling missing data country reunion \/ country splits\n    # perfrom backward fill : Problem Germany, W gets vlalues of Ghana\n    # in general values of adjacent different country\n    \n    # Missing Data Splits: USSR, Yugoslavia, Czechsolovakia\n    #         Only Reunion : Germany\n    # Andorra:        1977 - 1992\n    # Angola:         1972 - 1974\n    # Antigua:        1972 - 1980\n    # Armenia:        1972 - 1990\n    # Azerbaijan:     1972 - 1990\n    # Bahamas:        1972\n    # Belarus:        1972 - 1990\n    # Belize:         1972 - 1980\n    # Bosnia:         1972 - 1991\n    # Brunei:         1977 - 1983\n    # Cabo Verde:     1972 - 1974\n    # Croatia:        1972 - 1990\n    # Czech Republic: 1972 - 1992\n    # Czechoslovakia  1993 - 2019\n    # Eritrea:        1972 - 1992\n    # Estonia:        1972 - 1990\n    # Georgia:        1972 - 1990\n    # Germany:        1972 - 1989\n    \n\n    # mark missing as nan    \n    nandf=crdf.replace(to_replace='-',value=np.nan)\n    \n    cols=nandf.columns\n\n    # empty dataframe for extrapolated results\n    extdf=pd.DataFrame(columns=cols)    \n\n    # special processing for USSR\n    # country list, later countrynnames of former USSR soviet republics before 1989\n    countrylist=[ 'Armenia', 'Azerbaijan', 'Belarus', 'Estonia', 'Latvia',  'Tajikistan', 'Lithuania', 'Kazakhstan' , 'Kyrgyzstan', 'Ukraine', 'Uzbekistan'  ]\n\n    # polictical rating of USSR druing cold war\n    ussrpr = 7\n    ussrcl = 7\n    ussrStatus = 'NF'\n\n    # loop over countries of former USSR\n    for co in countrylist:\n        \n        # get index range of country co\n        idx = nandf['CountryName']==co\n        \n        # dataframe of current country\n        cdf = nandf[idx]\n        # fill in USSR cold war political values\n        cdf = cdf.copy()\n        cdf['PR'] = cdf['PR'].fillna(value=ussrpr)\n        cdf['CL'] = cdf['CL'].fillna(value=ussrcl)\n        cdf['Status'] = cdf['Status'].fillna(value=ussrStatus)\n        \n        nandf[idx] = cdf \n        \n        # dataframe holding current country\n        # nandf[idx] = nandf[idx].fillna(method='ffill')\n        # nandf[idx] = nandf[idx].fillna(method='bfill')\n        \n        pass\n    \n    \n    # fill holes in still existing countries\n    countrylist = list(nandf['CountryName'])\n    countrylist = list(np.unique(countrylist))\n    \n    \n    # loop over all still exsiting countries to fill holes over years\n    for co in countrylist:\n        \n        # get index range of country co\n        idx = nandf['CountryName']==co\n        \n        # dataframe holding current country\n        nandf[idx] = nandf[idx].fillna(method='ffill')\n        nandf[idx] = nandf[idx].fillna(method='bfill')\n        \n        pass\n      \n    \n    # for remaining no longer exisitng countries e.g. USSR, DDR, St. Vincent etc.  \n    # and all small countries not identified in ISO3 column drop all entries\n    \n\n    crfilldf=nandf.dropna()\n      \n    return crfilldf\n\n\n# fix_fromat_id7 : fix id7 formatting issues generated after handling missing values\n# replace 1.0 -> 1, 2.0 -> 2, 3.0 -> 3, 4.0 -> 4, 5.0 -> 5, 6.0 -> 6, 7.0 -> 7\n# replace 2(5) -> 4, 3(6) -> 5\n# replace (NF) -> PF\t\n# replace F (NF) -> PF\n#\t\ndef fix_formating_id7(rdf4):\n    rdf5 = rdf4\n    rdf5 = rdf5.replace(to_replace='(NF)',value='PF')\t\n    rdf5 = rdf5.replace(to_replace='F (NF)',value='PF')\t   \n    rdf5 = rdf5.replace(to_replace='2(5)',value=int(4))\t\n    rdf5 = rdf5.replace(to_replace='3(6)',value=int(5))\t\n    rdf5 = rdf5.replace(to_replace=1.0,value=int(1))\t\n    rdf5 = rdf5.replace(to_replace=2.0,value=int(2))\t\n    rdf5 = rdf5.replace(to_replace=3.0,value=int(3))\t\n    rdf5 = rdf5.replace(to_replace=4.0,value=int(4))\t\n    rdf5 = rdf5.replace(to_replace=5.0,value=int(5))\t\n    rdf5 = rdf5.replace(to_replace=6.0,value=int(6))\t\n    rdf5 = rdf5.replace(to_replace=7.0,value=int(7))\t\n    pass\n    return rdf5\n\t\n\t\n# def aggregate_format_id7( crdf, iso3df):\n# function to aggregate country indicators\n# PR political rights (range: 1..7)\n# CL civil liberties  (range: 1..7)\n# Status              { F, PF, NF }    \n# Resulting returned DataFrame Format ------------\n# Year CountryName ISO3 PR CL Status\ndef aggregate_format_id7( crdf, iso3df):\n        \n    # Align Country Names with \n    # CountryNames from tempdf 'Russia' and 'United States' do not conform with \n    # ISO3         'Russian Federation' and 'United States of America'\n\n    # crdf1=crdf.replace('USSR', 'Russian Federation')\n\n    crdf1=crdf.replace('United States', 'United States of America')\n\n\n\n\n    s=crdf.shape    # 207 rows (countries) , 142 columns ( 3 columns per year)\n    nr=s[0]\n    nc=s[1]\n    \n    # Input data layout\n    # crdf.iloc[0,:]    ... years\n    # crdf.iloc[2,:]    ... first country ratings (Albania)\n    # ...\n    # crdf.iloc[206,:]  ... last country rating (Zimbawe) \n    \n    \n    # Align column entries in years in row 1 crdf.iloc[0,:]\n    # for c in range (1,nc,3):\n    #    y=crdf1.iloc(0,c)     # get year under review\n    #    crdf1.iloc(0,c+1)=y   # duplicate excel entry over 3 columns\n    #    crdf1.iloc(0,c+2)=y\n    \n    \n    # Result Dataframe\n    \n    # prcdf    ... country political rights (PR)        scale 1...7\n    # clcdf    ... country civil liberties rating (CL)  scale 1...7\n    # statusdf ... Status F ... Free, PF ... partly freem NF ... not free\n    \n    # Resulting DataFrame Format\n    # Year CountryName ISO3 PR CL Status\n    \n    co=['Year', 'CountryName', 'PR', 'CL', 'Status' ] \n    values = [0, 'AAACountry', 'AAA', 1, 1, 'FF' ]          \n    \n    years=pd.Series([0])\n    cous=pd.Series(['AAACountry'])\n    ps=pd.Series([0])\n    cs=pd.Series([0])\n    stats=pd.Series(['FF'])\n    \n    rdf = pd.DataFrame(columns=co)\n    \n    rl = pd.DataFrame({co[0] : years,\n                       co[1] : cous,\n                       co[2] : ps,\n                       co[3] : cs,\n                       co[4] : stats})\n    \n    # different review cylce from 1981 to 1989 -> special_years\n    # years 1981 is reviewed over 1.6years  \n    # simple solution mapping with dict\n    # Jan. 1981 - Aug. 1982 -> 1981 \n    # Aug. 1983 - Nov. 1983 -> 1983\n   \n    special_years = {'Jan.1981-Aug. 1982' : 1981,\n                     'Aug.1982-Nov.1983'  : 1983,\n                     'Nov.1983-Nov.1984'  : 1984,\n                     'Nov.1984-Nov.1985'  : 1985,\n                     'Nov.1985-Nov.1986'  : 1986,\n                     'Nov.1986-Nov.1987'  : 1987,\n                     'Nov.1987-Nov.1988'  : 1988,\n                     'Nov.1988-Dec.1989'  : 1989\n                    }\n\n    #dictionary to align with ISO3 CountryNames\n\n    iso3_dict = { 'Bolivia' : 'Bolivia (Plurinational State of)',\n                  'Brunei'  : 'Brunei Darussalam',\n                  'Congo (Brazzaville)' : 'Congo',\n                  'Congo (Kinshasa)' : 'Congo, Democratic Republic of the',\n                  'Czech Republic' : 'Czechia',\n                  # 'Czechoslovakia' : 'Czechia',\n                  # 'Germany, W. ' : 'Germany',\n                  'Iran' : 'Iran (Islamic Republic of)',\n                  'Russia' : 'Russian Federation' ,\n                  'Syria' : 'Syrian Arab Republic', \n                  'Tanzania' : 'Tanzania, United Republic of' ,\n                  'United Kingdom' : 'United Kingdom of Great Britain and Northern Ireland',\n                  'Venezuela' : 'Venezuela (Bolivarian Republic of)',\n                  'Vietnam, N.' : 'Viet Nam',                 \n                  'Lao Peoples Democratic Republic': 'Lao People\\'s Democratic Republic',\n                  'North Korea': 'Korea (Democratic People\\'s Republic of)',\n                  'South Korea': 'Korea, Republic of',\n                  'Taiwan': 'Taiwan, Province of China'\n                 }\n  \n    # align country names with ISO3 country names\n    \n    \n    \n    resr = 0 \n        \n    for r in range (2,nr):   # loop over rows (countries) ---------\n        # current country\n        country_ = crdf1.iloc[r,0]  \n        \n        # align for join with \n        if country_ in iso3_dict:\n            country_ = iso3_dict[country_]\n\n        for c in range (1,nc,3):     # loop over columns (years) ----------\n            # current year\n            year_ = crdf1.iloc[0,c]   \n\n            # current year status\n            pr_      = crdf1.iloc[r,c]\n            cl_      = crdf1.iloc[r,c+1]\n            status_  =  crdf1.iloc[r,c+2]\n\n            # special treatment years 1981 to 1989\n            if year_ in special_years:\n                year_ = special_years[year_]\n          \n            # add new row to resulting DataFrame rdf\n            rdf = rdf.append(rl, ignore_index=True)\n            \n            # write last row of dataframe rdf\n            rdf.iloc[resr,0] = year_\n            rdf.iloc[resr,1] = country_\n            rdf.iloc[resr,2] = pr_\n            rdf.iloc[resr,3] = cl_\n            rdf.iloc[resr,4] = status_\n           \n            resr = resr + 1\n\n            # special treatment for 1981, copy 1981 to 1982\n            if year_ == 1981:\n                year_ = 1982\n                # add new row to resulting DataFrame rdf\n                rdf = rdf.append(rl, ignore_index=True)\n            \n                # write last row of dataframe rdf\n                rdf.iloc[resr,0] = year_\n                rdf.iloc[resr,1] = country_\n                rdf.iloc[resr,2] = pr_\n                rdf.iloc[resr,3] = cl_\n                rdf.iloc[resr,4] = status_\n           \n                resr = resr + 1\n                \n            pass\n\n        pass\n    \n    # open content problems:\n\n    # missing data for changing country names or merge \/ split of countries\n\n\n\n    # Note ISO3 Names are the current ones   \n    # rdf2 = rdf.join(iso3df)\n    rdf2=rdf.join(iso3df,on='CountryName')\n\n    # manage missing data in country political ratings\n    # \n    rdf3 = manage_missing_id7(rdf2)\n    \n    \n    # generate consecutive index again\n    rdf4 = rdf3.reset_index()\n\n\t\n\t# fix float and ambigious format entries in dataframes\n\t\n    rdf4 = fix_formating_id7(rdf4)\n\t\n\t\n    # add column\n    \n    def add_ISO3Year(df):\n        ''' this adds the ISO3Year column to a dataframe.\n        nothing is returned - the passed data frame is changed.\n        consider it as inplace=True.\n        '''\n        \n        numrows = df.shape[0]\n        dummy_list = []\n        for i in range(numrows):\n            append_val = df['ISO3'].iloc[i] + str(df['Year'].iloc[i])\n            dummy_list.append(append_val)\n            \n        dummy_series = pd.Series(dummy_list)\n        df['ISO3Year'] = dummy_series\n    \n        return df\n\n    # add column ISO3year\n    rdf5 = add_ISO3Year(rdf4)\n\n    # remove columns not needed\n    # rdf5 = rdf5.drop('Unnamed: 0', axis=1) \n    \n    rdf5 = rdf5.drop('index', axis=1) \n    \n\n    return rdf5\n\n\n\n############################################################\n\ndef id7_preprocess (inpath, outpath, id7outfile):\n\n       \n    id7outfn = outpath + id7outfile\n    \n    # read iso3 and countrynames\n    \n    iso3_df =  load_iso3_countries(inpath)\n    \n    # plain read excel sheet country political rating\n    crdf = load_country_rating_data(inpath)\n    \n    # format and aggregate output dataframe country political rating\n    id7_df = aggregate_format_id7( crdf, iso3_df)\n    \n    id7fn = outpath + id7outfile\n    # write csv file with ID 7 policitcal ratings\n    id7_df.to_csv(id7fn,sep=';')    \n    \n    \n    \n    return id7_df, iso3_df\n    \n\n\n###############################################################    \n\n# check id7 countries for coverage over years\ndef check_id7_countries ( id7_df ):\n\n    cols=id7_df.columns\n\n    countrylist = list(id7_df['CountryName'])\n    countrylist = list(np.unique(countrylist))\n\n    # empty dataframe for extrapolated results\n    extdf=pd.DataFrame(columns=cols)    \n\n    # lenght list, one entry per country\n    lenl=[]    \n\n    # country list, countrynnams\n    countryl=[]\n\n    for co in countrylist:\n        \n        # get index range of country co\n        idx = id7_df['CountryName']==co\n        \n        tcdf = id7_df[idx] \n \n        l = tcdf.shape[0]\n    \n        lenl.append(l) \n        countryl.append(co)\n        \n        pass\n\n    return countryl, lenl\n\n\n\n# id7_df, iso3_df = id7_preprocess ('\/kaggle\/input\/co2emissions\/', '\/kaggle\/working\/', 'ID7_Politics_CountryName_PR_CL_Status.csv')  \n\n# check for nans\n# id7_df.info()\n# iso3_df.info()\n\n# check id 7 countries for complete year coverage\n# countl, lenl = check_id7_countries ( id7_df )\n\n\n","f2123fc6":"def ID8_preproc(path_to_folder):\n\n    # read ID 8 Education and Forest area data\n    def load_data(path):\n    \n        fn = path +'Data_Extract_From_World_Development_Indicators.xlsx'\n    \n    \n        df = pd.read_excel(fn)\n    \n        \n        return df\n    \n    #from google.colab import drive\n    #drive.mount('\/content\/drive')\n    \n    df = load_data(path_to_folder)\n    \n    first_row_with_all_NaN = df[df.isnull().all(axis=1) == True].index.tolist()[0]\n    \n    df = df.loc[0:first_row_with_all_NaN-1]\n    \n    cols=['Year', 'CountryName', 'ISO3', 'Forest area (% of land area)', 'Government expenditure on education, total (% of GDP)', 'Surface area (sq. km)' ] \n    \n    year = pd.Series([0])\n    country = pd.Series([''])\n    iso = pd.Series([''])\n    forest = pd.Series([0])\n    edu = pd.Series([0])\n    surface = pd.Series([0])\n    \n    temp = pd.DataFrame({cols[0] : year,\n                       cols[1] : country,\n                       cols[2] : iso,\n                       cols[3] : forest,\n                       cols[4] : edu,\n                       cols[5] : surface})\n    \n    df2 = pd.DataFrame(columns = cols)\n    \n    sh = df.shape\n    last_row = 0\n    for i in range(0, 651, 3):\n        for y in range(4, sh[1]):\n            country = df.iloc[i, 0]\n            iso =  df.iloc[i, 1]\n            year = int(df.columns[y][0:5])\n            forest = df.iloc[i, y]\n            surface = df.iloc[i+2, y]\n            edu = df.iloc[i+1, y]\n    \n            df2 = df2.append(temp, ignore_index=True)\n            #print(df2.shape)\n    \n            df2.iloc[last_row,0] = year\n            df2.iloc[last_row,1] = country\n            df2.iloc[last_row,2] = iso\n            df2.iloc[last_row,3] = forest\n            df2.iloc[last_row,4] = edu\n            df2.iloc[last_row,5] = surface\n    \n            last_row = last_row + 1\n    \n    df2.dtypes\n    \n    def add_ISO3Year(df):\n        ''' this adds the ISO3Year column to a dataframe.\n        nothing is returned - the passed data frame is changed.\n        consider it as inplace=True.\n        '''\n        \n        numrows = df.shape[0]\n        dummy_list = []\n        for i in range(numrows):\n            append_val = str(df['ISO3'].iloc[i]) + str(df['Year'].iloc[i])\n            dummy_list.append(append_val)\n            \n        dummy_series = pd.Series(dummy_list)\n        df['ISO3Year'] = dummy_series\n    \n        return df\n    \n    df3 = add_ISO3Year(df2)\n    df3.replace('..', np.NaN, inplace = True)\n    df3.replace('', np.NaN, inplace = True)\n    \n    pd.to_numeric(df3['Surface area (sq. km)'])\n    pd.to_numeric(df3['Forest area (% of land area)'])\n    pd.to_numeric(df3['Government expenditure on education, total (% of GDP)'])\n    \n    #df3['Surface area (sq. km)'].replace(\"..\", np.NaN, inplace = True)\n    i = 0\n    num_years = 47\n    while i < df3.shape[0]:\n      df3.iloc[i: i+num_years, 3] = df3.iloc[i: i+num_years, 3].interpolate(method='linear', limit_direction= 'both')\n      df3.iloc[i: i+num_years, 4:6] = df3.iloc[i: i+num_years, 4:6].interpolate(method='linear')\n      df3.iloc[i: i+num_years, 4:6] = df3.iloc[i: i+num_years, 4:6].fillna(method = 'bfill')\n      df3.iloc[i: i+num_years, 4:6] = df3.iloc[i: i+num_years, 4:6].fillna(method = 'ffill')\n      i += num_years\n    \n    df3.drop(['Year', 'CountryName', 'ISO3'], axis='columns', inplace=True)\n    \n    df2.dtypes\n    \n    #df3.to_csv('ID8_Education&Forest&Surface.csv', sep=';', index=False)\n    \n    df_hdi = pd.read_csv(path_to_folder + 'human-development-index.csv')\n    \n    df_hdi.rename(columns={\"Code\": \"ISO3\"}, inplace = True)\n    \n    df_hdi2 = add_ISO3Year(df_hdi)\n    #df_hdi.dtypes\n    \n    df_hdi2.drop(['Entity', 'Year', 'ISO3'], axis='columns', inplace=True)\n    \n    df3.dtypes\n    \n    df_hdi2.dtypes\n    \n    df3 = df3.merge(df_hdi2, on = 'ISO3Year', how = 'left')\n    \n    i = 0\n    while i < df3.shape[0]:\n      df3.iloc[i: i+num_years, 4] = df3.iloc[i: i+num_years, 4].interpolate(method='linear', limit_direction= 'both')\n      i += num_years\n    \n    df3.dropna(0, inplace = True)\n    df3.to_csv('\/kaggle\/working\/ID8_Edu_For_Surf_HDI.csv', sep = ';',  index=False)\n\n    return df3","4cd776cc":"## ID9a Average Temperature of All Countries:\n## generate DataFrame and csv file with columns:\n## Year Countryname AverageTemperature ISO3\n\n\n\n# In[1]:\n\n\n# Note: The only imports allowed are those contained in Python's standard library, pandas, numpy, scipy and matplotlib\n\n\n\n# In[2]:\n\n\n# read ISO3 country codes from preprocessed csv file\n# and return iso3 data frame with rows 'CountryName' and 'ISO3'\ndef load_iso3_csv(path):\n\n    \n    # original source data:\n    # https:\/\/github.com\/lukes\/ISO-3166-Countries-with-Regional-Codes\/blob\/master\/all\/all.csv\n    # already converted to csv file\n    fn = path + 'CountryName_ISO3.csv'\n      \n    # iso3c_df=pd.read_fwf(fn, header=None, )\n    iso3c_df=pd.read_csv(fn, sep=';' )\n    \n    # set index to 'CountryName', preparation for join\n    iso3df= iso3c_df.set_index('CountryName')\n\n\n    return iso3df\n\n\n\n\n\n# read ID 9a Global Temperatures by state csv file in data frame\ndef load_temp_data(path):\n\n    fn = path +'GlobalLandTemperatures_GlobalLandTemperaturesByCountry.csv'\n\n    # tdf=pd.read_csv(fn)\n    \n    # stackoverflow \n    # https:\/\/stackoverflow.com\/questions\/48248239\/pandas-how-to-convert-rangeindex-into-datetimeindex\n    tdf=pd.read_csv(fn, parse_dates=['dt'], index_col=['dt'])\n\n    # tdf=pd.read_csv(fn, parse_dates=['dt'])\n    \n    return tdf\n\n\n# process missing data in country temperatures\ndef manage_missing_id9(tdf):\n\n    \n    # mark missing data as na\n    \n    \n    # perfrom backward fill : Problem Germany gets vlalues of Ghana\n    # crfilldf=crdf.fillna(method='bfill')\n\n    # crfilldf=crdf.replace(to_replace='-',method='ffill')\n\n    \n    # Overall best strategy: drop entries with missing data\n    # this means in general country did not exist in this form at this year\n    \n    nandf=tdf.replace(to_replace='-',value=np.nan)\n    \n    tfilldf=nandf.dropna()\n    \n    return tfilldf\n\n\n# function to exatrapolate years from 2013 up to endyear\ndef extrapolate_years(ydf, endyear):\n    \n    cols=ydf.columns\n\n    countrylist = list(ydf['CountryName'])\n    countrylist = list(np.unique(countrylist))\n\n    # empty dataframe for extrapolated results\n    extdf=pd.DataFrame(columns=cols)    \n\n    for co in countrylist:\n        \n        # get index range of country co\n        idx = ydf['CountryName']==co\n        \n        tcdf = ydf[idx] \n        \n        # extend extdf till endyear\n        \n        # get mean temperature of n years before 2013 for this year\n        n = 5\n        yearendrec = 2013\n        byear   = yearendrec  - n\n                \n        lastyears = tcdf[tcdf['Year'] > byear]\n        \n        temps = np.array(lastyears['AverageTemperature'])\n        years = np.array(lastyears['Year'])\n\n\n        \n        meantemp = np.mean(lastyears['AverageTemperature'])\n        \n        # get last yer of recoreds for append extrapolation\n        lastyear = tcdf[tcdf['Year'] == yearendrec ]\n        \n        lastyear = lastyear.copy()\n        # generate new rows for extrapolated years and \n        for y in range ( yearendrec+1, endyear+1 ):\n           lastyear['AverageTemperature'] = meantemp\n           lastyear['Year'] = y\n           tcdf = tcdf.append(lastyear, ignore_index=True)\n           pass \n\n\n        extdf = extdf.append(tcdf)\n        pass\n    \n    return extdf\n\n\n\n# function to aggregate and format temperature data frame\n# \ndef aggregate_format_id9_temp( tempdf, iso3df):\n    \n    \n    # \n    # CountryNames from tempdf 'Russia' and 'United States' do not conform with \n    # ISO3         'Russian Federation' and 'United States of America'\n\n    # tempdf1=tempdf.replace('Russia', 'Russian Federation')\n\n    tempdf1a=tempdf.replace('United States', 'United States of America')\n\n    tempdf1b=tempdf1a.replace('United Kingdom (Europe)', 'United Kingdom of Great Britain and Northern Ireland')\n    \n    tempdf1c=tempdf1b.replace('Russia', 'Russian Federation')\n    \n    \n    #rename column Country to CountryName\n    tempdf2= tempdf1c.rename(columns={'Country' : 'CountryName'})\n    \n\n    # aggregate country temperature per month\n    ymdf=tempdf2.groupby(['CountryName', 'dt']).mean()   # multiindex time country\n\n    # turn multindex componente dt in to column dt, only country as index in cdf\n    cdf=ymdf.reset_index(1)\n      \n       \n    \n    agg_tempdf = cdf\n\n    # generated datetime series yc\n    yc=pd.to_datetime(agg_tempdf['dt'])\n \n    # extract years from datetime sries\n    yco=yc.dt.year\n    \n    # cast to dataframe with year entries only\n    ydf=pd.DataFrame(yco)\n    \n    #rename column dt to year and \n    ydf2= ydf.rename(columns={'dt': 'Year'})\n    \n    # add year dataframe as columne to original dataframe\n    agg_tempdf['Year'] = ydf2\n    \n    # drop column not needed\n    agg_tempdf=agg_tempdf.drop(['AverageTemperatureUncertainty'], axis=1)\n\n    # cydf has country as index and columns year and Average Temperature\n    cydf=agg_tempdf.groupby(['CountryName', 'Year']).mean() \n\n    # get Year back as column\n    cy0=cydf.reset_index()\n\n    ################################################################\n    # extrapolate years \n    \n    cy1 = extrapolate_years(cy0, 2018)\n    \n\n    \n    ###############################################################\n    # merge with common iso3 country codes, merge on CountryName\n    cydf2 = cy1.join(iso3df, on='CountryName')\n\n    # create additional idx \n    # s=cydf2.shape\n    # nr=s[0]\n    # idx=pd.Series(range(1,nr+1))\n    # cydf2.insert(0,'idx',range(1,s[0]+1))\n\n    # this would drop index and \n    # t=cydf2.reset_index(0,drop=False)\n    \n    \n    # \n    def add_ISO3Year(df):\n        ''' this adds the ISO3Year column to a dataframe.\n        nothing is returned - the passed data frame is changed.\n        consider it as inplace=True.\n        '''\n        \n        numrows = df.shape[0]\n        dummy_list = []\n        for i in range(numrows):\n            append_val = df['ISO3'].iloc[i] + str(df['Year'].iloc[i])\n            dummy_list.append(append_val)\n            \n        dummy_series = pd.Series(dummy_list)\n        df['ISO3Year'] = dummy_series\n    \n        return df\n    \n    \n    # drop missing to allow generation of valid column ISO3Year \n\n\n\n    # process missing data\n    cydf3 = manage_missing_id9(cydf2)\n    \n    # \n    cydf4=cydf3.reset_index()\n    \n    \n    # add column ISO3Year\n    t = add_ISO3Year(cydf4)\n\n    # drop rows not needed\n    # t=t.drop('Unnamed: 0', axis=1)\n    t=t.drop('index', axis=1)\n\n\n    return t\n\n#############################################################\n\n\ndef id9_preprocess (inpath, outpath, id9outfile):\n\n    # read ISO3 country codes from html \n    # and return iso3 data frame with rows 'CountryName' and 'ISO3'\n    def load_iso3_countries(path):\n    \n        # download from, save as html\n        # https:\/\/github.com\/lukes\/ISO-3166-Countries-with-Regional-Codes\/blob\/master\/all\/all.csv\n        fn = path + 'github_iso3.htm'\n      \n        # iso3c_df=pd.read_fwf(fn, header=None, )\n        list_df=pd.read_html(fn )\n\n        # get first dataframe from list of dataframes    \n        df=list_df[0]\n\n        # select and copy needed rows\n        cut_df=df.loc[:,['name', 'alpha-3']]\n\n        # rename column names    \n        iso3c_df= cut_df.rename(columns={\"name\": \"CountryName\", \"alpha-3\": \"ISO3\"})\n    \n        # set index to 'CountryName'\n        iso3df= iso3c_df.set_index('CountryName')\n    \n        return iso3df\n\n    \n    tdf = load_temp_data(inpath)\n\n    iso3_df =  load_iso3_countries(inpath)\n\n\n    id9_df = aggregate_format_id9_temp(tdf, iso3_df )\n\n\n    id9fn = outpath + id9outfile\n\n    # write csv file with ID 9 Temperature date\n    id9_df.to_csv( id9fn ,sep=';')\n    \n    \n    return id9_df, iso3_df\n\n\n############################################################\n\n\n# id9_df, iso3_df = id9_preprocess ('\/kaggle\/input\/co2emissions\/', '\/kaggle\/working\/', 'ID9a_Temp_Countries_CountryName_AvgTemp.csv')\n\n# check for nans\n#id9_df.info()\n#iso3_df.info()\n\n\n\n\n","a889bbba":"def add_country_name(df):\n    '''adds a CountryName column to a dataframe based on the ISO3 column\n    Function does not return anything but changes the passed dataframe'''    \n    name_col = []\n    country_dict = get_country_dict()\n    for (idx,row) in df.iterrows():\n        name_col.append(country_dict[row['ISO3']])\n    df['CountryName'] = name_col\n\ndef reconstruct_ISO3_Year(df):\n    '''reconstructs the columns ISO3 and Year from the ISO3Year index\n    and adds these columns to the dataframe.\n    Function does not return anything but changes the passed dataframe'''    \n    iso3_col = []\n    year_col = []\n    for (idx,row) in df.iterrows():\n        iso3_col.append(idx[:3])\n        year_col.append(int(idx[3:]))\n    df['Year'] = year_col\n    df['ISO3'] = iso3_col\n\n\ndef join_dfs(files,path='\/kaggle\/working\/'):\n    drop_list = ['Unnamed: 0', 'ISO3', 'CountryName', 'Year']\n    dfs = []\n    for i in range(len(files)):\n        fn = path + files[i]\n        append_df = pd.read_csv(fn,sep=';')\n        append_df.set_index('ISO3Year',inplace=True)\n\n        for col in drop_list:\n            try:\n                append_df.drop(col,axis=1,inplace=True)\n            except:\n                pass\n        dfs.append(append_df)\n        \n\n    df_joined_outer = pd.concat(dfs,\n                                join='outer',\n                                axis=1,\n                                sort=False)\n\n    df_joined_inner = pd.concat(dfs,\n                                join='inner',\n                                axis=1,\n                                sort=False)\n\n    #inner_filename = 'df_joined_inner.csv'\n    #outer_filename = 'df_joined_outer.csv'\n\n    #df_joined_inner.to_csv(inner_filename,sep=';')\n    #df_joined_outer.to_csv(outer_filename,sep=';')\n\n    #outer_from_file = pd.read_csv(outer_filename,sep=';')\n    #inner_from_file = pd.read_csv(inner_filename,sep=';')\n\n    #return inner_from_file\n    reconstruct_ISO3_Year(df_joined_inner)\n    add_country_name(df_joined_inner)\n    return df_joined_inner\n\n\ndef preprocess():    \n    df_dict = {}\n    df_dict['ID1'] = ID1_preproc(write_csv=True)    \n    df_dict['ID2'] = ID2_preproc(write_csv=True)\n    df_dict['ID3'] = ID3_prep(file=\"\/kaggle\/input\/co2emissions\/ama_gdp_breakdown.xlsx\")\n    df_dict['ID7'] = id7_preprocess ('\/kaggle\/input\/co2emissions\/', '\/kaggle\/working\/', 'ID7_Politics_CountryName_PR_CL_Status.csv')\n    df_dict['ID8'] = ID8_preproc('\/kaggle\/input\/co2emissions\/')\n    df_dict['ID9'] = id9_preprocess ('\/kaggle\/input\/co2emissions\/', '\/kaggle\/working\/', 'ID9a_Temp_Countries_CountryName_AvgTemp.csv')[0]\n    return df_dict\n\ndef dropped_countries(df):\n    ''' will return a dictionary of the countries that are in the global\n    country dictionary obtained by get_country_dict() but are not in the\n    passed dataframe df'''\n    country_dict = get_country_dict()\n    rem_countries = set(df['ISO3'].to_list())\n    dropped_countries = set(list(country_dict)) - rem_countries\n    dropped_dict = {}\n    for c in dropped_countries:\n        dropped_dict[c] = country_dict[c]    \n    return dropped_dict\n","bfc785a7":"prep_data = True\njoin_data = True\nfile_out = '\/kaggle\/working\/df_joined.csv'\n\nif prep_data:\n    start = time.time()\n    df_dict = preprocess()\n    print('Preprocessing time elapsed in seconds = {:.1f}'.format(time.time()-start))\n\nif join_data:\n    start = time.time()\n    file_list = ['ID2_clean.csv',\n                 'ID03_gdp_sector.csv',\n                 'ID7_Politics_CountryName_PR_CL_Status.csv',\n                 'ID8_Edu_For_Surf_HDI.csv',\n                 'ID9a_Temp_Countries_CountryName_AvgTemp.csv']\n\n    df = join_dfs(file_list)\n    df.to_csv(file_out,sep=';')\n    print('Joining and writing file time elapsed in seconds = {:.1f}'.format(time.time()-start))\n\nif prep_data:\n    # this preprocessing relies on a joined dataframe\n    df_dict['ID2a'] = ID2a_preproc(write_csv=True)","bf6264d7":"# analyse how representative our reduced datasets are for the golbal situation\nfile_joined = '\/kaggle\/working\/df_joined.csv'\ndf_joined = pd.read_csv(file_joined,sep=';')\nclist_joined = list(set(df_joined['ISO3'].to_list()))\nfile_ID2a = '\/kaggle\/working\/ID2a_clean.csv'\ndf_ID2a = pd.read_csv(file_ID2a,sep=';')\nclist_ID2a = list(set(df_ID2a['ISO3'].to_list()))\n\nbig_file='\/kaggle\/input\/co2emissions\/owid-co2-data.csv'\ndf_big = pd.read_csv(big_file,sep=',')\nID2_preproc_format_only(df_big)\nprint('Joined DataFrame with combined data from many sources. Time span: 1972-2016')\nprint('---------------------------------------------------------------------------')\nget_shares(clist_joined,df_big)\nprint('\\nReduced DataFrame with data on consumption based CO2 and energy sources. Time span: 1990-2014')\nprint('---------------------------------------------------------------------------------------------')\nget_shares(clist_ID2a,df_big)\n\ndel df_joined\ndel df_ID2a\ndel df_big","bd9a9139":"# check joined dataframe for outliers and report potential outlier data\ndef check_outliers( indf, dbg_on ):\n    '''\n    check joined dataframe for outliers\n    INPUT:  joined dataframe\n            dbg_on   ... 1: debug output showing outliers\n    RETURN: cnt_outl ... number detected outlier\n            extdf    ... dataframe holding exteme values with year, country, \n                         ['Year', 'Country', 'maxv' 'minv', 'Ext_Categories'] \n            outdf    ... dataframe with outliers limited to plausible range \n    '''\n    \n    start_y = 1972    \n\n    perc_ = 0.08          # min percentile, max percentile 1-perc_\n    \n    dev_fact = 0.4        # outliers outside percentile * (1+-dev_fact) * percentile(0.08|0.92)\n    \n    mean_frac = 0.01      # outliers outside mean +- mean_frac * mean\n    \n    #get column names\n    cols = indf.columns\n    \n    # indices for col selection for numerical checks\n    numselidx = list( range(1,29))\n    # numselidx.append([32, 33, 34, 35, 36)\n    numselidx.append(32)\n    numselidx.append(33)\n    numselidx.append(34)\n    numselidx.append(35)\n    # numselidx.append(36)   #AverageTemperature, sometimes negative\n    # add CountryName\n    numselidx.append(39)\n    \n    # numerical columns of dataframe with series over time\n    numcols=[cols[x] for x in numselidx]\n    \n    # set limits for outliers in terms of stddev\n    cnt_numcols = len(numcols)-1\n        \n    # set limits for outliers in in fraction of mean\n    mfrac = np.array([ mean_frac for x in range(0,cnt_numcols)])\n    \n    max_dev_fact = np.array([ (1.0 + dev_fact) for x in range(0,cnt_numcols)])\n    min_dev_fact = np.array([ (1.0 - dev_fact) for x in range(0,cnt_numcols)])\n    \n    # get dataframe with all numerical columns\n    indf.copy()\n    numdf = indf[numcols]\n    numcoln = (numdf.columns)\n    numcolname = numcoln[0:(len(numcoln)-1)]\n    \n    # create list of all countrynames\n    countrylist = list(indf['CountryName'])\n    countrylist = list(np.unique(countrylist))\n     \n    # ----------------------------------------------------\n    # loop over all still countries \n    # calculate descriptive statistics for each country\n    \n    cnt_outl = 0     # count number of coutliers\n    \n    # create empty data for exteme values over time     \n    olist=['Year', 'Country', 'maxv' , 'minv', 'Ext_Categories']\n    extdf = pd.DataFrame(columns=olist)\n    \n    \n    for co in countrylist:\n        \n        # get index range of country co\n        idx = numdf['CountryName']==co\n        \n        # dataframe holding current country\n        cdf = pd.DataFrame(numdf[idx]).copy()\n        \n        \n        # calculate descriptive statistics        \n        mean_ = np.array(cdf.mean())\n        std_  = np.array(cdf.std()) \n        minperc_  = np.array(cdf.quantile(perc_))\n        maxperc_  = np.array(cdf.quantile(1.0-perc_))\n        min_  = np.array(cdf.min())\n        max_  = np.array(cdf.max())\n        \n        \n        # set limits for outlier from mean by percentiles * dev_fact and fraction of mean\n        # for this country\n        maxl = maxperc_ * max_dev_fact + mfrac * mean_\n        minl = minperc_ * min_dev_fact - mfrac * mean_\n        \n        # check for outliers iterating over years of this country\n            \n        lencdf = cdf.shape[0]\n        for y in range(0,lencdf):\n            print(\"\")\n            cy = np.array(cdf.iloc[y,0:(cnt_numcols)])   # entries of current year\n        \n            upidx = cy > maxl\n            \n            lowidx = cy < minl\n            \n            # count number of outliers\n            up_cnt  = sum(bool(x) for x in upidx)\n            low_cnt = sum(bool(x) for x in lowidx)\n            extdf.copy\n            # debug output outlier \n            if (up_cnt > 0):\n                # append new maximum\n                new_row = len(extdf)\n                extdf.copy\n                extdf.loc[new_row,'Year'] = y + start_y\n                extdf.loc[new_row,'Country'] = co\n                extdf.loc[new_row,'maxv'] = up_cnt\n                extdf.loc[new_row,'minv'] = 0\n                cstr = str(numcolname[upidx])\n                cstr = cstr[6:-17]\n                extdf.loc[new_row,'Ext_Categories'] = cstr\n                if (dbg_on > 0) :\n                    print(co,' up :',y+start_y,': ', up_cnt) \n                    print('      ',numcolname[upidx])\n\n            if (low_cnt >0):\n                # append new minimum\n                new_row = len(extdf)\n                extdf.copy\n                extdf.loc[new_row,'Year'] = y + start_y\n                extdf.loc[new_row,'Country'] = co\n                extdf.loc[new_row,'maxv'] = 0\n                extdf.loc[new_row,'minv'] = low_cnt\n                cstr = str(numcolname[lowidx])\n                cstr = cstr[6:-17]\n                extdf.loc[new_row,'Ext_Categories'] = cstr\n                \n                if (dbg_on > 0):\n                    print(co,' low :',y+start_y,': ', low_cnt)                \n                    print('      ',numcolname[lowidx])\n                \n\n            cnt_outl = cnt_outl + up_cnt + low_cnt\n        \n            # limit outliers to [1.05*minl, 0.95*maxl] for all numeric entries\n            cy[upidx]  = 0.97*maxl[upidx]\n            cy[lowidx] = 1.03*minl[lowidx]\n        \n            # write back to current country df\n            cdf.iloc[y,0:(cnt_numcols)] = cy.copy()            \n        \n        pass\n        \n        # write back numeric columns of this country\n        # numdf[idx] = cdf\n        numdf.copy()\n        numdf.loc[idx,:] = cdf.copy()\n    \n    pass\n    \n    # write numerical columns with limitations to outdf\n    outdf = indf.copy()  \n    outdf[numcols] = numdf.copy()\n\n    return cnt_outl, extdf, outdf\n\n","3a697019":"\n############################################\n# Test Driver code check outliers, example integration   \noutlier_analysis = False\nif outlier_analysis: \n    fn='\/kaggle\/working\/df_joined.csv'\n    jdf=pd.read_csv(fn, sep=';')    \n\n    # check for outliers and limit outliers\n    # cntoutl    ... number of detected outliers\n    # limited_df ... datframe with outliers limited \n    debug=0\n    cntoutl, extdf, limited_df = check_outliers(jdf, debug)\n    \n    # show found extreme values over years and countries\n    extdf[['Year','Country']]","99831c42":"if outlier_analysis:\n    pd.set_option('display.max_rows', extdf.shape[0]+1)\n    print(extdf[['Year','Country','Ext_Categories']])","db47b282":"# joined dataframe debug output\ndf","9f1a1d37":"df.isna().sum()","4c1a9fb4":"country_list = list(set(df['ISO3'].to_list()))\nyear_list = list(set(df['Year'].to_list()))\nnum_rows = df.shape[0]\n\nprint('country_list x year_list = num_rows')\nprint(len(country_list),'x',len(year_list),'=',len(country_list)*len(year_list))\nprint('num_rows = ',num_rows)","76d0edfd":"print(avl_report(df,country_list,'co2').to_string())","5f5b0754":"dropped_ctr = dropped_countries(df)\ndropped_ctr","251fe35c":"file_owid = '\/kaggle\/input\/co2emissions\/owid-co2-data.csv'\ndf_owid = pd.read_csv(file_owid,sep=',')\nID2_preproc_format_only(df_owid)\ndrop_share_co2,not_found = country_list_share(df_owid,list(dropped_ctr),'co2',year=2016)\ndrop_share_population,not_found = country_list_share(df_owid,list(dropped_ctr),'population',year=2016)\ndrop_share_gdp,not_found = country_list_share(df_owid,list(dropped_ctr),'gdp',year=2016)\n\nprint('number of dropped countries: {}'.format(len(dropped_ctr)))\nprint('share of co2 of dropped countries (2016): {:.5f}'.format(drop_share_co2))\nprint('share of population of dropped countries (2016): {:.5f}'.format(drop_share_population))\nprint('share of gdp of dropped countries (2016): {:.5f}'.format(drop_share_gdp))\n","c5e356e4":"print('these countries are dropped, but were not found in the owid dataset:')\nprint('--------------------------------------------------------------------')\ncountry_dict = get_country_dict()\nfor c in not_found:\n    print(country_dict[c])","bfb152f4":"# analyse how representative our reduced datasets are for the golbal situation\nfile_joined = '\/kaggle\/working\/df_joined.csv'\ndf_joined = pd.read_csv(file_joined,sep=';')\nclist_joined = list(set(df_joined['ISO3'].to_list()))\nfile_ID2a = '\/kaggle\/working\/ID2a_clean.csv'\ndf_ID2a = pd.read_csv(file_ID2a,sep=';')\nclist_ID2a = list(set(df_ID2a['ISO3'].to_list()))\n\nbig_file='\/kaggle\/input\/co2emissions\/owid-co2-data.csv'\ndf_big = pd.read_csv(big_file,sep=',')\nID2_preproc_format_only(df_big)\nprint('Joined DataFrame with combined data from many sources. Time span: 1972-2016')\nprint('---------------------------------------------------------------------------')\nget_shares(clist_joined,df_big)\nprint('\\nReduced DataFrame with data on consumption based CO2 and energy sources. Time span: 1990-2014')\nprint('---------------------------------------------------------------------------------------------')\nget_shares(clist_ID2a,df_big)\n\ndel df_joined\ndel df_ID2a\ndel df_big","f6f74a45":"# fix_fromat_id7 : fix id7 formatting issues generated after handling missing values\n# replace 1.0 -> 1, 2.0 -> 2, 3.0 -> 3, 4.0 -> 4, 5.0 -> 5, 6.0 -> 6, 7.0 -> 7\n# replace 2(5) -> 4, 3(6) -> 5\n# replace (NF) -> PF\t\n# replace F (NF) -> PF\n#\t\ndef fix_formating_id7__(rdf4):\n    rdf5 = rdf4.copy()\n    rdf5 = rdf5.replace(to_replace='(NF)',value='PF')\t\n    rdf5 = rdf5.replace(to_replace='F (NF)',value='PF')\t   \n    rdf5 = rdf5.replace(to_replace='2(5)',value=int(4))\t\n    rdf5 = rdf5.replace(to_replace='3(6)',value=int(5))\t\n    \n    pr = np.array(rdf5['PR'], dtype=float)\n    cl = np.array(rdf5['CL'], dtype=float)\n    \n    rdf5['PR'] = pr\n    rdf5['CL'] = cl\n        \n    rdf5 = rdf5.copy()\n    pass\n    return rdf5\n\n################################################\n# get most significant correlations\ndef ex3_correlations(jdf):\n    '''\n    ex3_correlations\n    INPUT: joinded dataframe\n    RETURN : corm_co2c ... correlation matrix CO2\/Capita, most significant correlations\n             corm_co2g ... correlation matrix CO2\/GDP, most significant correlations \n\n    '''\n    # add normalized co2\n    \n    jdf['CO2\/capita'] = jdf['co2']*1000000\/jdf['population']\n    \n    jdf['CO2\/GDP'] = jdf['co2']*1000000\/jdf['Gross Domestic Product (GDP)']\n    \n    cols = jdf.columns\n    \n    \n    \n    col_select = [ 'CO2\/capita', 'CO2\/GDP', 'Human Development Index (UNDP)', 'share_sec', 'share_C_E ', 'share_JP', 'export_share', 'AverageTemperature', 'CL', 'PR', 'Forest area (% of land area)', 'Government expenditure on education, total (% of GDP)',  'share_prim', 'share_tert', 'import_share', 'export_share', 'CL', 'Forest area (% of land area)', 'share_G_H', 'share_J_P', 'AverageTemperature', 'share_prim' ]    \n    \n    # jdf_sel = jdf [col_select]\n    jdf_sel = jdf\n    \n    # check for significant correlation of all columns in data_merged \n    corm = jdf_sel.corr()\n    \n    ########################################################################\n    # most significant correlation CO2\/capita\n    co2c = corm['CO2\/capita']\n    co2c_sort= co2c.sort_values(axis=0,ascending=False)\n    \n    '''\n    5 Most significant positive Correlations for CO2\/capita\n    Human Development Index (UNDP)                                0.549114\n    share_sec                                                     0.327387\n    share_C_E                                                     0.321727\n    share_J_P                                                     0.305712\n    export_share                                                  0.301150\n    \n    5 Most significant negative Correlations for CO2\/capita\n    CL                                                           -0.145735\n    Forest area (% of land area)                                 -0.163653\n    share_G_H                                                    -0.221765\n    AverageTemperature                                           -0.316142\n    share_prim                                                   -0.467255\n    \n    '''\n    ########################################################################\n    # most significant correlation CO2\/GDP\n    co2g = corm['CO2\/GDP']\n    co2g_sort= co2g.sort_values(axis=0,ascending=False)\n    \n    '''\n    5 Most significant positive Correlations for CO2\/GDP\n    CL                                                            0.348924\n    PR                                                            0.317880\n    share_D                                                       0.297515\n    share_sec                                                     0.274091\n    share_C_E                                                     0.246327\n    \n    \n    5 Most significant negative Correlations for CO2\/GDP\n    Forest area (% of land area)                                 -0.181013\n    share_J_P                                                    -0.216413\n    share_G_H                                                    -0.228718\n    share_tert                                                   -0.231218\n    AverageTemperature                                           -0.337516\n    '''\n    #################################################################\n    # Co2\/capita: largest correlations\n    # visualize correlation matrix over all parameters\n    colc_select = [ 'co2', 'CO2\/capita', 'CO2\/GDP', 'Human Development Index (UNDP)', 'share_sec', 'share_C_E', 'share_J_P', 'export_share', 'CL', 'Forest area (% of land area)', 'share_G_H', 'AverageTemperature', 'share_prim' ]    \n    jdfc_sel = jdf[colc_select]\n    corm_co2c = jdfc_sel.corr()\n    \n    # sb.heatmap(corm_co2c)\n    # plt.title('Correlation heatmap CO2\/Capita')\n    # plt.savefig('Correlation_heatmap.svg',pad_inches=2.5,paper_type='a4')\n    \n    #################################################################\n    # Co2\/GDP: largest correlations\n    # visualize correlation matrix over all parameters\n    colg_select = [ 'co2', 'CO2\/capita', 'CO2\/GDP', 'CL', 'PR', 'share_D', 'share_sec', 'share_C_E', 'Forest area (% of land area)', 'share_J_P', 'share_G_H', 'share_tert', 'AverageTemperature' ]    \n    jdfg_sel = jdf[colg_select]\n    corm_co2g = jdfg_sel.corr()\n    \n    # sb.heatmap(corm_co2g)\n    # plt.title('Correlation heatmap CO2\/GDP')\n    # plt.savefig('Correlation_heatmap.svg',pad_inches=2.5,paper_type='a4')\n\n    pass\n    return corm_co2c, corm_co2g, colc_select, colg_select, jdfc_sel, jdfg_sel\n\n##############################################################\n# calculate descriptive statistics\ndef ex3_calc_descriptive_statistics(jdf):\n    '''\n    ex3_calc_descriptive_statistics\n    INPUT :    jdf             ... joined dataframe\n    RETURN:    jdf_abs_norm    ... joined dataframe, cols absolute values normalized to mean\n               jdf_rel,        ... joined dataframe, cols relative values normalized to 1\n               descr_norm,     ... df descriptive statistic for absolute normalized values\n               descr_rel,      ... df descriptive statistic for relative values\n               abs_norm        ... mean value used for normalization of abolute values\n\n    '''\n    jdf['CO2\/capita'] = jdf['co2']*1000000\/jdf['population']   \n    jdf['CO2\/GDP'] = jdf['co2']*1000000\/jdf['Gross Domestic Product (GDP)']\n    \n    # normalize percentage to 1\n    jdf['Forest area (% of land area)'] = jdf['Forest area (% of land area)'] \/ 100\n    jdf['Government expenditure on education, total (% of GDP)'] = jdf['Government expenditure on education, total (% of GDP)'] \/ 100\n    \n    \n    cols= jdf.columns\n    \n    mean_ = jdf.mean()\n    std_  = jdf.std()\n    \n    # data normalized to mean\n    jdf_norm = jdf\/mean_\n    \n    # columns with absolute values\n    col_abs=['co2', 'population', 'Gross Domestic Product (GDP)',\n           'Final consumption expenditure', 'Imports of goods and services',\n           'Exports of goods and services', \n           'Total Value Added', 'VA_sum', 'abs_prim', 'abs_sec', 'abs_tert',\n           'Surface area (sq. km)', \n           'AverageTemperature', 'Year', 'CO2\/capita',\n           'CO2\/GDP']\n    \n    # columns with relative values, normalized to 1\n    col_rel=['import_share', 'export_share',\n           'share_prim', 'share_sec', 'share_tert',\n           'share_C_E', 'share_D', 'share_F',\n           'share_G_H', 'share_I', 'share_J_P', \n           'Human Development Index (UNDP)',\n           'Forest area (% of land area)',\n           'Government expenditure on education, total (% of GDP)']\n    \n    \n    # abs_norm : absolute values are normalized to mean\n    \n    jdf_abs_norm=jdf_norm[col_abs]\n    \n    abs_norm = jdf[col_abs].mean() \n    \n    \n    jdf_rel=jdf[col_rel]\n    \n    descr_rel=jdf_rel.describe()\n    \n    descr_norm=jdf_abs_norm.describe()\n    \n    abs_norm = jdf[col_abs].mean() \n    \n    \n    return jdf_abs_norm, jdf_rel, descr_norm, descr_rel, abs_norm\n\n\n##############################################################\n# Boxplots for descriptive statistics, supressing outliers\n# absolute values distributions normalized to mean\ndef ex3_plot_descriptive_stats(jdf_abs_norm, jdf_rel):\n    fig, (ax1, ax2) = plt.subplots(2,1,figsize=(10,10))\n    \n    flierprops = dict(markerfacecolor='0.75', markersize=5,\n                  linestyle='none')\n    \n    plt.title(\"Box Plot of Distribution normalized\" )\n    ax1 = sb.boxplot(data=jdf_abs_norm,orient=\"h\",flierprops=flierprops,showfliers=False, ax=ax1)\n    \n    ax2 = sb.boxplot(data=jdf_rel,orient=\"h\",flierprops=flierprops,showfliers=False, ax=ax2)\n    # plt.title(\"Box Plot of Distribution of relative values\")\n    \n    plt.show()\n    \n    pass\n\n","a04241e2":"fn='\/kaggle\/working\/df_joined.csv'\njdf0=pd.read_csv(fn, sep=';')\n# convert ID7 entries PR, CL to float\njdf=fix_formating_id7__(jdf0)\n\n# calculate correlation matrices for most significant correlations\n# CO2\/Capita and CO2\/GDP\ncorm_co2c, corm_co2g, colc_select, colg_select, jdfc_sel, jdfg_sel = ex3_correlations(jdf)\n\ncorm_co2c\n\n","c1fd0298":"corm_co2g","cce45a73":"# calculate descriptive statistics for all columns in joined dataframe\njdf_abs_norm, jdf_rel, descr_norm, descr_rel, abs_norm = ex3_calc_descriptive_statistics(jdf)","6b48c313":"descr_norm","ae348830":"descr_rel","d351f42e":"# plot distribution using boxplots\nex3_plot_descriptive_stats(jdf_abs_norm, jdf_rel)","92ecebf1":"# load global data\n#df1_infile = '\/kaggle\/working\/ID1_clean.csv'\n#df_id1 = pd.read_csv(df1_infile,sep=';')\n#df_id1.set_index('Year',inplace=True)","10959da0":"# set pyplot font size\nplt.rcParams.update({'font.size': 12})","54219aac":"draw_global1(size=(8,6))","d3f86eee":"draw_global2(size=(8,6))","4c32b92c":"draw_co2_countries1(size=(16,6))","67545bcd":"draw_pop_gdp_share1(size=(15,6))","06976770":"draw_bin3_shares(size=(8,6))","7a656b7f":"draw_co2_prod_cons(size=(16,6))","82a74658":"    draw_co2_countries1(size=(8,6),\n                        column='co2 per capita',\n                        ylabel='million tonnes of CO2 per capita',\n                       omit_stack=True)","d5c6da8b":"draw_top_acuco2()","7e65cfb4":"draw_top_acuco2pc()","b8a38b70":"def add_percapita_cols(df):\n    '''Adds co2\/capita, GDP\/capita'''\n    df['co2\/Capita'] = (df['co2'] \/df['population']) * 10**6\n    df['GDP\/Capita'] = (df['Gross Domestic Product (GDP)'] \/df['population'])\n    df['co2\/GDP'] = (df['co2'] \/df['Gross Domestic Product (GDP)']) * 10**9\n    \ndef top_10_bar_plot(df, column, year):\n    df_temp = df.loc[df['Year'] == year]\n    df_temp = df_temp.nlargest(10, column)\n    \n    # Shorten some names to make the 2016 x-axis readable\n    df_temp.replace('United States of America', 'USA', inplace = True)\n    df_temp.replace('Russian Federation', 'Russia', inplace = True)\n    df_temp.replace('Germany, W. ', 'Germany', inplace = True)\n    df_temp.replace('Saudi Arabia', 'Saudi A.', inplace = True)\n\n    ax = df_temp.plot.bar(x = 'CountryName', y='co2', rot=45, label = 'CO2 Emissions')\n    ax.set_title('Top 10 CO2 Emitter Countries in 2016')\n    ax.set_xlabel(\"Country Name\")\n    ax.set_ylabel(\"Million tons\")\n\nadd_percapita_cols(df)\ntop_10_bar_plot(df, 'co2', 2016)","bb4bafa4":"def top_ten_co2_percapita(df):\n    df_temp = df.loc[df['Year'] == 2016]\n    df_temp = df_temp.nlargest(10, 'co2\/Capita')\n\n    # Shorten some names to make the 2016 x-axis readable\n    df_temp.replace('United States of America', 'USA', inplace = True)\n    df_temp.replace('Russian Federation', 'Russia', inplace = True)\n    df_temp.replace('Germany, W. ', 'Germany', inplace = True)\n    df_temp.replace('Saudi Arabia', 'Saudi A.', inplace = True)\n\n    ax = df_temp.plot.bar(x = 'CountryName', y='co2\/Capita', rot=45, label = 'CO2 per capita Emissions')\n    ax.set_title('Top 10 CO2\/capita Emitter Countries in 2016')\n    ax.set_xlabel(\"Country Name\")\n    ax.set_ylabel(\"tons per person\")\n    \ntop_ten_co2_percapita(df)","0bbdff2e":"def mean_values(df):\n    '''Function takes in a dataframe and returns one with\n    corresponding mean values over the span \n    of available years for each country\n    \n    For the column Status - most frequent value is considered\n    the mean'''\n    # Create a new dataframe for storing means\n    countries = df.ISO3.unique()\n    out = df.head(0).reset_index().drop(['ISO3Year', 'Year',], axis = 1)\n    \n    #Define columns to be grouped\n    cols = list(df.columns)\n    rm_list = ['Year', 'ISO3', 'CountryName', 'Status']\n    for i in rm_list:\n        #print(i)\n        cols.remove(i)\n\n    for country in countries:\n        df_means = df.head(0).reset_index().drop(['ISO3Year', 'Year', 'ISO3', 'CountryName', 'Status'], axis = 1)\n        df_temp = df.loc[df['ISO3'] == country]\n        df_ = df_temp.mean()\n        \n        for col in cols:\n            df_means.loc[0, col] = df_[col]\n            df_means['Avg annual change of ' +col] =(df_temp.loc[country+'2016', col] - df_temp.loc[country+'1972', col])\/(df_temp.loc[country+'2016', 'Year'] - df_temp.loc[country+'1972', 'Year'])\n        df_means['ISO3'] = df_temp['ISO3'].iloc[0]\n        df_means['CountryName'] = df_temp['CountryName'].iloc[0]\n        df_means['Status'] =  df_temp['Status'].mode()\n        out = out.append(df_means, sort = True)\n    \n    return out\n\ndf2 = mean_values(df)","90b608a2":"def print_correlations():\n    '''Print the strongest correlations with co2\/capita and co2\/GDP'''\n    pd.set_option('display.max_rows', 100)\n    corr_matrix = df2.corr()\n\n    print('Largest positive correlations with co2\/Capita')\n    print(50* '-')\n    print(corr_matrix['co2\/Capita'].sort_values(ascending=False)[1:6])\n    print(50* '-')\n    print('Largest negative correlations with co2\/Capita')\n    print(corr_matrix['co2\/Capita'].sort_values(ascending=True)[0:5])\n\n    print(50* '*')\n    print('Largest positive correlations with co2\/GDP')\n    print(50* '-')\n    print(corr_matrix['co2\/GDP'].sort_values(ascending=False)[1:6])\n    print(50* '-')\n    print('Largest negative correlations with co2\/GDP')\n    print(50* '-')\n    print(corr_matrix['co2\/GDP'].sort_values(ascending=True)[0:5])\n    \nprint_correlations()\n","43460e1a":"def top_ten_co2_per_gdp(df):\n    df_temp = df.loc[df['Year'] == 2016]\n    df_temp = df_temp.nlargest(10, 'co2\/GDP')\n\n    # Shorten some names to make the 2016 x-axis readable\n    df_temp.replace('United States of America', 'USA', inplace = True)\n    df_temp.replace('Russian Federation', 'Russia', inplace = True)\n    df_temp.replace('Germany, W. ', 'Germany', inplace = True)\n    df_temp.replace('Saudi Arabia', 'Saudi A.', inplace = True)\n\n    ax = df_temp.plot.bar(x = 'CountryName', y='co2\/GDP', rot=45, label = 'kg(CO2) per $ GDP')\n    ax.set_title('Top 10 CO2\/$ Intensive Countries in 2016')\n    ax.set_xlabel(\"Country Name\")\n    ax.set_ylabel(\"kg(CO2) per $GDP\")\n    \ntop_ten_co2_per_gdp(df)","1ec02687":"def assign_basket(df):\n    '''Function takes in dataframe and a list of columns for\n    which the user want the values assigned to baskets 1-5.\n    1 = Very Low\n    2 = Low\n    3 = Moderate\n    4 = High\n    5 = Very High'''\n    \n    def func(x):\n        if x > 0.8:\n            return 5\n        elif x > 0.6:\n            return 4\n        elif x > 0.4:\n            return 3\n        elif x > 0.2:\n            return 2\n        else:\n            return 1\n        \n    def func2(x):\n        if x > 0.8:\n            return 1\n        elif x > 0.6:\n            return 2\n        elif x > 0.4:\n            return 3\n        elif x > 0.2:\n            return 4\n        else:\n            return 5\n    \n    df_temp = df.copy()\n    #Define columns to be grouped\n    cols = list(df_temp.columns)\n    rm_list = [ 'CountryName', 'CL', 'PR', 'Status', 'co2', 'population', 'Gross Domestic Product (GDP)',\n              'co2\/Capita', 'ISO3']\n    for i in rm_list:\n        #print(i)\n        cols.remove(i)\n        \n    df_temp[cols] = df_temp[cols].rank(pct = True).applymap(func)\n    df_temp['CL'] = df_temp['CL'].rank(pct = True).apply(func2)\n    df_temp['PR'] = df_temp['PR'].rank(pct = True).apply(func2)\n\n    return df_temp\n\n#Example usage\ndf_bask = assign_basket(df2)","2f52b267":"def barplot_by_groups(df_means, df_baskets, column, metric, ytitle):\n    '''Input :  1) df with a single value per country\n                2) df with countries sorted in baskets\n                3) column as a criteria for baskets\n                \n       Output: Barplot contrasting average values for baskets'''\n    groups = ['Lowest',  'Low', 'Moderate', 'High', 'Highest']\n    \n    \n    fig, axes = plt.subplots(figsize=(15,10), nrows=2, ncols=2)\n    fig.tight_layout(h_pad=10, w_pad = 8)\n\n    \n    def group_avgs(column):\n        group_avg = np.zeros(5)\n        for i in range(1,6):\n            df_temp = df_baskets.loc[df_baskets[column] == i].ISO3\n            for iso in df_temp:\n                group_avg[i-1] += df_means.loc[df2['ISO3'] == iso, metric]\n            group_avg[i-1] \/= df_temp.count()\n         \n        return {'Groups':groups,'Group Average':group_avg}\n    \n    df_0 = pd.DataFrame(group_avgs(column[0]))\n    df_1 = pd.DataFrame(group_avgs(column[1]))\n    df_2 = pd.DataFrame(group_avgs(column[2]))\n    df_3 = pd.DataFrame(group_avgs(column[3]))\n\n\n    \n    df_0.plot(ax = axes[0,0], kind = 'bar', x = 'Groups', y='Group Average', rot=45, label = column[0])\n    axes[0, 0].set_title(column[0])\n    axes[0, 0].set_xlabel('Bins')\n    axes[0, 0].set_ylabel(ytitle)\n    \n    df_1.plot(ax = axes[0,1], kind = 'bar', x = 'Groups', y='Group Average', rot=45, label = column[1])\n    axes[0,1].set_title(column[1])\n    axes[0,1].set_xlabel('Bins')\n    axes[0,1].set_ylabel(ytitle)\n    \n    if (column[2] == 'Human Development Index (UNDP)'):\n        df_2.plot(ax = axes[1,0], kind = 'bar', x = 'Groups', y='Group Average', rot=45, label = 'HDI')\n    else:\n        df_2.plot(ax = axes[1,0], kind = 'bar', x = 'Groups', y='Group Average', rot=45, label = column[2])\n\n        \n    \n    axes[1,0].set_title(column[2] )\n    axes[1,0].set_xlabel('Bins')\n    axes[1,0].set_ylabel(ytitle)\n    \n    title = ''\n    if (column[3] == 'share_prim'):\n        title = 'Share of Primary Sector in the Economy'\n    else: \n        title = column[3] \n    df_3.plot(ax = axes[1,1], kind = 'bar', x = 'Groups', y='Group Average', rot=45, label = column[3])\n    axes[1,1].set_title(title)\n    axes[1,1].set_xlabel('Bins')\n    axes[1,1].set_ylabel(ytitle)\n    \n\n#Example usage\ncols = ['GDP\/Capita', 'Avg annual change of co2\/Capita', 'Human Development Index (UNDP)', 'share_prim']\nbarplot_by_groups(df2, df_bask, cols, 'co2\/Capita', \"Tons of co2\/person\")","2d9a8c62":"cols2 = ['Avg annual change of AverageTemperature', 'AverageTemperature', 'CL', 'Avg annual change of co2\/GDP']\nbarplot_by_groups(df2, df_bask, cols2, 'co2\/GDP', \"kg of co2\/$\")","9fd3436b":"def co2_pcap_histograms(df):\n    ''' Take a dataframe containing co2\/capita data and displays histogram for 1994, 2004 and 2014'''\n    fig, axes = plt.subplots(nrows=1, ncols=3)\n    fig.set_figheight(5)\n    fig.set_figwidth(20)\n\n    max_co2 = int(df.loc[df['Year'] == 1994, 'co2\/Capita'].max())\n    ticks = (range(0, max_co2 + 5, int(max_co2\/16) ))\n    plt.setp(axes, xticks=ticks);\n    df.loc[df['Year'] == 1994, 'co2\/Capita'].hist(ax =axes[0], bins = ticks)  \n \n    df.loc[df['Year'] == 2004, 'co2\/Capita'].hist(ax=axes[1], bins = ticks)\n    df.loc[df['Year'] == 2014, 'co2\/Capita'].hist(ax=axes[2], bins = ticks)\n\n    fig.suptitle('CO2 per Capita Overview ', fontsize=14)\n    #axes[0].set_xlabel('Bins based on CO2 tons\/person')\n    axes[0].set_title('Year 1994')\n    axes[0].set_ylabel('Number of Countries')\n    axes[1].set_xlabel('Bins based on CO2 tons\/person')\n    axes[1].set_title('Year 2004')\n    axes[1].set_ylabel('Number of Countries')\n    #axes[2].set_xlabel('Bins based on CO2 tons\/person')\n    axes[2].set_title('Year 2014')\n    axes[2].set_ylabel('Number of Countries')\n\nco2_pcap_histograms(df)","2b3c0e50":"def assign_basketfloatup_evol(df, col_basket, col_listall):\n    '''\n    assign_basketfloatup_evol\n    INPUT: df           ... source data frame\n           col_basket   ... column name to setup 20% percentile baskets\n                            percentiels rising > 80% ... 5\n           col_listall  ... list of all columns used\n    RETURN: out_df      ... dataframe with col_listall columns, \n                            col_basket       \n                          \n    Function takes in dataframe and a list of columns for\n    which the user want the values assigned to baskets 1-5.\n    1 = Lowest\n    2 = Low\n    3 = Moderate\n    4 = High\n    5 = Highest'''\n    \n    def func(x):\n        if x > 0.8:\n            return 5\n        elif x > 0.6:\n            return 4\n        elif x > 0.4:\n            return 3\n        elif x > 0.2:\n            return 2\n        else:\n            return 1\n        \n    \n    df2 = df.copy()\n    df2['CO2\/capita'] = df['co2']*1000000\/df['population']\n    df2['CO2\/GDP'] = df['co2']*1000000\/df['Gross Domestic Product (GDP)']\n    \n    #Define columns to be analysed\n    cols = list(df2.columns)\n        \n    df3 = df2[col_listall]\n    df3 = df3.copy()\n    \n    df3[col_basket] = df3[col_basket].rank(pct = True).apply(func)\n\n    return df3\n\n\ndef assign_basketpol_evol(df, col_basket, col_listall):\n    '''\n    assign_basketpol_evol\n    INPUT: df           ... source data frame\n           col_basket   ... column name to setup 20% percentile baskets\n                            percentiles falling > 80% ... 1\n           col_listall  ... list of all columns used\n    RETURN: out_df      ... dataframe with col_listall columns, \n                            col_basket       \n                          \n    Function takes in dataframe and a list of columns for\n    which the user want the values assigned to baskets 1-7.\n    '''\n\n    def func4(x):\n        if x > 6.5\/7:\n            return 7\n        elif x > 5.5\/7:\n            return 6\n        elif x > 4.5\/7:\n            return 5\n        elif x > 3.5\/7:\n            return 4\n        elif x > 2.5\/7:\n            return 3\n        elif x > 1.5\/7:\n            return 2\n        else:\n            return 1\n    \n    df2 = df.copy()\n    df2['CO2\/capita'] = df['co2']*1000000\/df['population']\n    df2['CO2\/GDP'] = df['co2']*1000000\/df['Gross Domestic Product (GDP)']\n    \n    #Define columns to be analysed\n    cols = list(df2.columns)\n\n    df3 = df2[col_listall]\n    df3 = df3.copy()\n    \n    # df3[col_basket] = df3[col_basket].rank(pct = True).apply(func3)\n    df3[col_basket] = df3[col_basket].rank(pct = True).apply(func4)\n\n\n    return df3\n\n\n# fix_fromat_id7 : fix id7 formatting issues generated after handling missing values\n# replace 1.0 -> 1, 2.0 -> 2, 3.0 -> 3, 4.0 -> 4, 5.0 -> 5, 6.0 -> 6, 7.0 -> 7\n# replace 2(5) -> 4, 3(6) -> 5\n# replace (NF) -> PF\t\n# replace F (NF) -> PF\n#\t\ndef fix_formating_id7_(rdf4):\n    rdf5 = rdf4.copy()\n    rdf5 = rdf5.replace(to_replace='(NF)',value='PF')\t\n    rdf5 = rdf5.replace(to_replace='F (NF)',value='PF')\t   \n    rdf5 = rdf5.replace(to_replace='2(5)',value=int(4))\t\n    rdf5 = rdf5.replace(to_replace='3(6)',value=int(5))\t\n    \n    pr = np.array(rdf5['PR'], dtype=float)\n    cl = np.array(rdf5['CL'], dtype=float)\n    \n    rdf5['PR'] = pr\n    rdf5['CL'] = cl\n    \n    rdf5 = rdf5.copy()\n    pass\n    return rdf5\n\n\n\ndef plot_evol_baskets( indf ):\n    '''\n    plot_evol_baskets( indf )\n    plots time evolution of col[2] over years in col[0] for 5 percentiles col[1]\n    INPUT:  indf     ... dataframe [ 'Year', 'Basket [1..n] ', 'CO2\/capita' ]\n    RETURN: fig, ax        \n    '''\n    cols = indf.columns\n\n    basket = cols[1]    \n    \n    # plotting - make figure and axes object        \n    #fig = plt.figure(figsize=size)\n    # \n    # fig = plt.figure() \n    fig, ax  = plt.subplots()\n    # fig = plt.subplots(1,1)\n    #ax = fig.add_axes([0.12,0.12,.85,.85]) # position in the figure window\n\n    # create plot (by passing the axes object to the plot function)\n    # ax1.set(ylabel=' CO2 tons \/Capita ')\n    # ax1.set(xlabel=cols[0])\n    ax.set(title=basket + '  Quantiles')\n    # ax.set(ylabel=' CO2\/Capita ')\n    ystr = cols[2]\n    ax.set(ylabel= ystr)\n    ax.set(xlabel=cols[0])\n    # loop over baskets and plot baskets\n    legstr=['lowest', 'low',  'medium',  'high', 'highest']\n    for np in range(1,6):\n        fnp = float(np)       \n        idx=indf[basket]==np\n        df=indf[idx]\n        \n        # plt.plot(df[cols[0]], df[cols[2]], fig)\n        \n        lstr = legstr[np-1]\n        \n        plt.plot(df[cols[0]], df[cols[2]],label = lstr)\n        plt.legend()\n        pass\n    \n    plt.show()\n\n    pass\n    return fig, ax\n\n\ndef plot_evol7_baskets( indf ):\n    '''\n    plot_evol_baskets( indf )\n    \n    INPUT:  indf     ... dataframe [ 'Year', 'Basket [1..n] ', 'CO2\/capita' ]\n    RETURN: fig, ax   \n    '''\n    cols = indf.columns\n\n    basket = cols[1]    \n    \n    # plotting - make figure and axes object        \n    #fig = plt.figure(figsize=size)\n    # \n    # fig = plt.figure() \n    fig, ax  = plt.subplots()\n    # fig = plt.subplots(1,1)\n    #ax = fig.add_axes([0.12,0.12,.85,.85]) # position in the figure window\n\n    # create plot (by passing the axes object to the plot function)\n    # ax1.set(ylabel=' CO2 tons \/Capita ')\n    # ax1.set(xlabel=cols[0])\n    ax.set(title=basket + '  Quantiles')\n    # ax.set(ylabel=' CO2\/Capita ')\n    ystr = cols[2]\n    ax.set(ylabel= ystr)\n\n    ax.set(xlabel=cols[0])\n    # loop over baskets and plot baskets\n    legstr=['1\/7_free', '2\/7',  '3\/7',  '4\/7', '5\/7', '6\/7', '7\/7']\n    for np in range(7,0,-1):\n        fnp = float(np)       \n        idx=indf[basket]==np\n        df=indf[idx]\n        \n        # plt.plot(df[cols[0]], df[cols[2]], fig)\n        \n        lstr = legstr[np-1]\n        \n        plt.plot(df[cols[0]], df[cols[2]],label = lstr)\n        plt.legend()\n        pass\n    \n    plt.show()\n\n    pass\n    return fig, ax\n\n\n# calcualte co2\/capita baskets time evolution         \ndef co2pop_evol_baskets( indf ):\n    '''\n    co2pop_evol_baskets( indf )\n    \n    INPUT:  indf     ... joined dataframe\n    RETURN:              dataframes with time evolution co2\/cpaita over baskets\n                         Temperatures, HDI, Population, Civil Liberties\n                         Political Rights, Education\n\t'''\n    \n    #get column names\n    cols = indf.columns\n\n    col_listall = [ 'Year', 'CountryName', 'co2', 'CO2\/capita', 'CO2\/GDP', 'population', 'Human Development Index (UNDP)', 'AverageTemperature', 'CL', 'PR', 'Government expenditure on education, total (% of GDP)' ]    \n\n    # co2\/cpapita time evolution in baskets sorted by temperature\n    col_basket = 'AverageTemperature' \n    df_Tempbask = assign_basketfloatup_evol(indf, col_basket, col_listall)\n\n    # co2\/capita time evolution in baskets sorted by HDI\n    col_basket = 'Human Development Index (UNDP)' \n    df_HDIbask = assign_basketfloatup_evol(indf, col_basket, col_listall)\n\n    # co2\/cpapita time evolution in baskets sorted by population\n    col_basket = 'population' \n    df_Popbask = assign_basketfloatup_evol(indf, col_basket, col_listall)\n\n    # co2\/capita time evolution in baskets sorted by civil liberties\n    col_basket = 'CL' \n    df_CLbask = assign_basketpol_evol(indf, col_basket, col_listall)\n\n    # co2\/capita time evolution in baskets sorted by political rights\n    col_basket = 'PR' \n    df_PRbask = assign_basketpol_evol(indf, col_basket, col_listall)    \n\n    # co2\/capita time evolution in baskets sorted by Education expenditures\n    col_basket = 'Government expenditure on education, total (% of GDP)' \n    df_Edubask = assign_basketfloatup_evol(indf, col_basket, col_listall)    \n\n    \n    ##############################################################\n    # group co2\/capita means over baskets time evolution\n    \n    df_HDIgroup=df_HDIbask.groupby(['Year', 'Human Development Index (UNDP)'], group_keys=False).mean()\n    \n    df_Tempgroup=df_Tempbask.groupby(['Year', 'AverageTemperature'], group_keys=False).mean()\n    \n    df_Popgroup=df_Popbask.groupby(['Year', 'population'], group_keys=False).mean()\n    \n    df_CLgroup=df_CLbask.groupby(['Year', 'CL'], group_keys=False).mean()\n    \n    df_PRgroup=df_PRbask.groupby(['Year', 'PR'], group_keys=False).mean()\n\n    df_Edugroup=df_Edubask.groupby(['Year', 'Government expenditure on education, total (% of GDP)'], group_keys=False).mean()\n\n    \n    ##############################################################\n    # remove multi-index\n    \n    df_HDIgroup=df_HDIgroup.reset_index()\n    \n    df_Tempgroup=df_Tempgroup.reset_index()\n    \n    df_Popgroup=df_Popgroup.reset_index()\n    \n    df_CLgroup=df_CLgroup.reset_index()\n    \n    df_PRgroup=df_PRgroup.reset_index()\n\n    df_Edugroup=df_Edugroup.reset_index()\n\n    \n    ##############################################################\n    # drop columns not needed\n    \n    df_HDIgroup  = df_HDIgroup[['Year', 'Human Development Index (UNDP)', 'CO2\/capita']]\n    \n    df_Tempgroup = df_Tempgroup[['Year', 'AverageTemperature' , 'CO2\/capita']]\n    \n    df_Popgroup  = df_Popgroup[['Year', 'population', 'CO2\/capita']]\n    \n    df_CLgroup   = df_CLgroup[['Year', 'CL', 'CO2\/capita']]\n                      \n    df_PRgroup   = df_PRgroup[['Year', 'PR', 'CO2\/capita']]\n\n    df_Edugroup   = df_Edugroup[['Year', 'Government expenditure on education, total (% of GDP)', 'CO2\/capita']]\n\n    pass\n    return df_HDIgroup, df_Tempgroup, df_Popgroup, df_CLgroup, df_PRgroup, df_Edugroup\n\t\n\t\n# calcualte co2\/gdp baskets time evolution         \ndef co2gdp_evol_baskets( indf ):\n    '''\n    co2gdp_evol_baskets( indf )\n    \n    INPUT:  indf     ... joined dataframe\n    RETURN:              dataframes with time evolution co2\/gdp over baskets\n                         Temperatures, HDI, Population, Civil Liberties\n                         Political Rights, Education\n\t'''\n    \n    #get column names\n    cols = indf.columns\n\n    col_listall = [ 'Year', 'CountryName', 'co2', 'CO2\/capita', 'CO2\/GDP', 'population', 'Human Development Index (UNDP)', 'AverageTemperature', 'CL', 'PR', 'Government expenditure on education, total (% of GDP)' ]    \n\n    # co2\/cpapita time evolution in baskets sorted by temperature\n    col_basket = 'AverageTemperature' \n    df_Tempbask = assign_basketfloatup_evol(indf, col_basket, col_listall)\n\n    # co2\/capita time evolution in baskets sorted by HDI\n    col_basket = 'Human Development Index (UNDP)' \n    df_HDIbask = assign_basketfloatup_evol(indf, col_basket, col_listall)\n\n    # co2\/cpapita time evolution in baskets sorted by population\n    col_basket = 'population' \n    df_Popbask = assign_basketfloatup_evol(indf, col_basket, col_listall)\n\n    # co2\/capita time evolution in baskets sorted by civil liberties\n    col_basket = 'CL' \n    df_CLbask = assign_basketpol_evol(indf, col_basket, col_listall)\n\n    # co2\/capita time evolution in baskets sorted by political rights\n    col_basket = 'PR' \n    df_PRbask = assign_basketpol_evol(indf, col_basket, col_listall)    \n\n    # co2\/capita time evolution in baskets sorted by Education expenditures\n    col_basket = 'Government expenditure on education, total (% of GDP)' \n    df_Edubask = assign_basketfloatup_evol(indf, col_basket, col_listall)    \n\n    \n    ##############################################################\n    # group co2\/capita means over baskets time evolution\n    \n    df_HDIgroup=df_HDIbask.groupby(['Year', 'Human Development Index (UNDP)'], group_keys=False).mean()\n    \n    df_Tempgroup=df_Tempbask.groupby(['Year', 'AverageTemperature'], group_keys=False).mean()\n    \n    df_Popgroup=df_Popbask.groupby(['Year', 'population'], group_keys=False).mean()\n    \n    df_CLgroup=df_CLbask.groupby(['Year', 'CL'], group_keys=False).mean()\n    \n    df_PRgroup=df_PRbask.groupby(['Year', 'PR'], group_keys=False).mean()\n\n    df_Edugroup=df_Edubask.groupby(['Year', 'Government expenditure on education, total (% of GDP)'], group_keys=False).mean()\n\n    \n    ##############################################################\n    # remove multi-index\n    \n    df_HDIgroup=df_HDIgroup.reset_index()\n    \n    df_Tempgroup=df_Tempgroup.reset_index()\n    \n    df_Popgroup=df_Popgroup.reset_index()\n    \n    df_CLgroup=df_CLgroup.reset_index()\n    \n    df_PRgroup=df_PRgroup.reset_index()\n\n    df_Edugroup=df_Edugroup.reset_index()\n\n    \n    ##############################################################\n    # drop columns not needed\n    \n    df_HDIgroup  = df_HDIgroup[['Year', 'Human Development Index (UNDP)', 'CO2\/GDP']]\n    \n    df_Tempgroup = df_Tempgroup[['Year', 'AverageTemperature' , 'CO2\/GDP']]\n    \n    df_Popgroup  = df_Popgroup[['Year', 'population', 'CO2\/GDP']]\n    \n    df_CLgroup   = df_CLgroup[['Year', 'CL', 'CO2\/GDP']]\n                      \n    df_PRgroup   = df_PRgroup[['Year', 'PR', 'CO2\/GDP']]\n\n    df_Edugroup   = df_Edugroup[['Year', 'Government expenditure on education, total (% of GDP)', 'CO2\/GDP']]\n\n    pass\n    return df_HDIgroup, df_Tempgroup, df_Popgroup, df_CLgroup, df_PRgroup, df_Edugroup\n\n\n","9ca095c1":"# Example usage assign basket for time evolution\nfn='\/kaggle\/working\/df_joined.csv'\njdf0=pd.read_csv(fn, sep=';')\n# convert ID7 entries PR, CL to float\njdf=fix_formating_id7_(jdf0)\n\ncol_listall = [ 'Year', 'CountryName', 'co2', 'CO2\/capita', 'population', 'Human Development Index (UNDP)', 'AverageTemperature', 'CL', 'PR', 'Government expenditure on education, total (% of GDP)' ]    \n\n###########################################################################\n# generate dataframes with time evolution CO2\/Capita in baskets (percentiles)\ndfc_HDI, dfc_Temp, dfc_Pop, dfc_CL, dfc_PR, dfc_Edu = co2pop_evol_baskets( jdf )","22044c83":"fig01,ax01 = plot_evol_baskets( dfc_HDI )","02b06072":"fig02,ax02 = plot_evol_baskets( dfc_Temp )","4df3b13c":"fig03,ax03 = plot_evol_baskets( dfc_Pop )","12824bbc":"fig04,ax04 = plot_evol7_baskets( dfc_CL )","7537231c":"fig05,ax05 = plot_evol7_baskets( dfc_PR )","288003f3":"fig06,ax06 = plot_evol_baskets( dfc_Edu )","7578cca7":"# generate dataframes with time evolution CO2\/GDP in baskets (percentiles)\ndfg_HDI, dfg_Temp, dfg_Pop, dfg_CL, dfg_PR, dfg_Edu = co2gdp_evol_baskets( jdf )","925ed83e":"##############################################################\n# plot time evolution of baskets CO2\/GDP\n\nfig07,ax07 = plot_evol_baskets( dfg_HDI )\n\n","d54703f5":"fig08,ax08 = plot_evol_baskets( dfg_Temp )","f390e62a":"fig09,ax09 = plot_evol_baskets( dfg_Pop )","fc45229d":"fig10,ax10 = plot_evol7_baskets( dfg_CL )","2507e9c7":"fig11,ax11 = plot_evol7_baskets( dfg_PR )","3c8ae92b":"fig12,ax12 = plot_evol_baskets( dfg_Edu )","d5087bac":"def scatter (data, x_axis, y_axis, degree=1, group=False):\n    \n    '''\n    Scatter Plot:\n    Draws a scatter plot for two variables and regression lines according to the number of degrees chosen\n    e.g. 2 degrees plots 2 regression lines (of a 1 degree polynomial and a 2 degree polynomial regression)\n    Score and RMSE are given in the brackets of each plot \n    If a group argument is given, points are grouped by colour\n    '''\n    \n    # generate points used to plot\n    df_plot = data[[x_axis, y_axis]].sort_values(by=[x_axis])\n    x = np.array(df_plot[x_axis])\n    X = x[:, np.newaxis]\n    y = np.array(df_plot[y_axis])\n\n    # set theme and plot\n    # sb.set_theme(style=\"whitegrid\")\n    sb.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n    plt.subplots(figsize=(12, 8))\n    \n    ## Scater plot\n    if (group!=False):\n        sb.scatterplot(data=data, x=x_axis, y=y_axis,\n                                  s=20, hue=group, palette=\"muted\")\n    else:\n        sb.scatterplot(data=data, x=x_axis, y=y_axis, color='navy',\n                                  s=20, label=\"training points\")\n\n    ## Get the polynomials\n    # iterate over the number of degrees\n    for count, degree in enumerate(range(1,degree+1)):\n        model = make_pipeline(PolynomialFeatures(degree), linear_model.LinearRegression())\n        model.fit(X, y)\n        # prediction\n        y_plot = model.predict(X)\n        # Score\n        score = model.score(X, y)\n        # RMSE\n        RMSE = np.sqrt(mean_squared_error(y, model.predict(X)))\n        # Plot regression line\n        plt.plot(X, y_plot, color=sb.color_palette(\"muted\")[count+1], linewidth=2,\n                 label=\"degree {} (score: {:.3f} \/ RMSE: {})\".format(degree, score, int(RMSE)))\n\n    plt.title(\"Relatopnship between \" + y_axis + \" & \" + x_axis)\n    plt.legend(loc='upper left')\n    plt.tight_layout()\n    plt.show()\n\n    \ndef pred_graph(y_val, y_pred, title=\"\"):\n    \n    '''\n    Actual vs. predicted values:\n    Plots a scatter plot of actual vs predicted values with a 1\/1 line\n    '''\n    \n    # set theme and plot\n    # sb.set_theme(style=\"whitegrid\")\n    sb.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n    plt.subplots(figsize=(6, 6))\n\n    scatter = sb.scatterplot(x=y_val, y=y_pred, s=20, palette=\"muted\")\n\n    scatter.set_title(title + \"Actual vs. predicted values\")\n    scatter.set_ylabel(\"Predicted values\")\n    scatter.set_xlabel(\"Actual values\")\n\n    # plot 1\/1-line\n    x_vals = np.array(scatter.get_xlim())\n    plt.plot(x_vals, x_vals, linewidth=1.5, color=sb.color_palette(\"muted\")[3], alpha=.5)\n    \n    scatter.figure.tight_layout()\n    plt.show()\n    \n\ndef heatmap(df, cols):\n    \n    '''\n    Creates a Heatmap for the specified columns \n    '''\n    \n    ## Correlation Matrix\n    corr = df[cols_num].corr()\n    corr = corr.reindex(cols_dict.values(), axis=1)\n    corr = corr.loc[cols_num, cols_num]\n    \n    ## Plot Parameters\n    plt.rcParams['font.size'] = '10'\n    plt.subplots(figsize=(12, 12))\n    \n    # Generate Color Map\n    colormap = sb.diverging_palette(220, 10, as_cmap=True)\n    # Generate Heat Map, allow annotations and place floats in map\n    ax = sb.heatmap(corr, cmap=colormap, fmt=\".1f\", linewidths=0.1, annot=True, annot_kws={\"size\": 8},\n                     xticklabels=True, yticklabels=True)\n    \n    ## Axis\n    ax.set_xticklabels(\n        ax.get_xticklabels(),\n        rotation=90,\n        horizontalalignment='center',\n    )\n    \n    ax.figure.tight_layout()\n    plt.show()\n    \n'''\nAdvanced Correlation Plot\nTo get a good first overview\n'''\ndef sign_cor(sign):\n    '''\n    returns the sign of a given significance \n    '''\n    if (sign) < 0.001:\n        sig = '***'\n    elif (sign) < 0.01:\n        sig = '**'\n    elif (sign) < 0.05:\n        sig = '*'\n    elif (sign) < 0.1:\n        sig = '+'\n    else: sig =''\n    return sig\n\ndef layout_switch(row, col, col_len, data, fontsize=8):\n    '''\n    Switches the layout of the correlation plot according to the given coordinates in the plot:\n    from top left to bottom right\n    '''\n    x_axis = data.iloc[:,col].name\n    x_axis = x_axis.replace('_','\\n')\n    x_axis = x_axis.replace(' ','\\n')\n    y_axis = data.iloc[:,row].name\n    y_axis = y_axis.replace('_','\\n')\n    y_axis = y_axis.replace(' ','\\n')\n    if (row==0 and col==0):\n        #top left\n        layout = [plt.tick_params(\n            axis='both',\n            which='both',\n            bottom=False,\n            top=False,\n            left=True,\n            labelbottom=False,\n            labelleft=True),\n            plt.title(x_axis, size=fontsize, fontweight='bold'),\n            plt.ylabel(y_axis, size=fontsize, fontweight='bold'),\n            plt.xlabel(x_axis, size=fontsize, fontweight='bold')]\n    elif (row==0):\n        #top\n        layout = [plt.tick_params(\n            axis='both',\n            which='both',\n            bottom=False,\n            top=False,\n            left=False,\n            labelbottom=False,\n            labelleft=False),\n            plt.title(x_axis, size=fontsize, fontweight='bold'),\n            plt.ylabel(\"\"),\n            plt.xlabel(\"\")]\n        #bottom left\n    elif (row==col_len-1 and col==0):\n        layout = [plt.tick_params(\n            axis='both',\n            which='both',\n            bottom=False,\n            top=False,\n            left=True,\n            labelbottom=True,\n            labelleft=True),\n            plt.title(\"\"),\n            plt.ylabel(y_axis, size=fontsize, fontweight='bold'),\n            plt.xlabel(x_axis, size=fontsize, fontweight='bold')]\n        # bottom\n    elif (row==col_len-1 and col!=0):\n        layout = [plt.tick_params(\n            axis='both',\n            which='both',\n            bottom=True,\n            top=False,\n            left=False,\n            labelbottom=True,\n            labelleft=False),\n            plt.title(\"\"),\n            plt.ylabel(\"\"),\n            plt.xlabel(x_axis, size=fontsize, fontweight='bold')]\n        # left\n    elif (col==0):\n        layout = [plt.tick_params(\n            axis='both',\n            which='both',\n            bottom=False,\n            top=False,\n            left=True,\n            labelbottom=False,\n            labelleft=True),\n            plt.title(\"\"),\n            plt.ylabel(y_axis, size=fontsize, fontweight='bold'),\n            plt.xlabel(\"\")]\n    else:\n        layout = [plt.tick_params(\n            axis='both',\n            which='both',\n            bottom=False,\n            top=False,\n            left=False,\n            labelbottom=False,\n            labelleft=False),\n            plt.title(\"\"),\n            plt.ylabel(\"\"),\n            plt.xlabel(\"\")]\n    return layout\n\n\n\ndef grid_plot_overview(data, fontsize):\n    '''\n    Prints a correlation plot with scatter plots in the lower half,\n    correlations (amount and significance) (colour coded as well) in the upper half\n    and histograms on the diagonal  \n    '''\n\n    ## Parameters\n    # sb.set_theme(style=\"white\")\n    col_len = data.columns.size\n    width = [1]*col_len\n    width.append(0.6)\n    plt.rcParams['font.size'] = str(fontsize)\n    plt.rcParams['axes.axisbelow'] = True\n    colormap = sb.diverging_palette(220, 10, as_cmap=True)\n    scalarmappaple = plt.cm.ScalarMappable(cmap=colormap)\n    scalarmappaple.set_array([-1, 1])\n\n    fig = plt.figure(0, figsize=(16,16), dpi= 120, constrained_layout=False)\n    \n    # add an extra column to the grid for the legend\n    grid = fig.add_gridspec(col_len, col_len+1, wspace=0.0, hspace=0.0,\n                            width_ratios=width)\n    # rows\n    for row in range(0, col_len):\n        y = data.iloc[:,row]\n    # columns\n        for col in range(0, col_len):\n            x = data.iloc[:,col]\n        # diagonal: histograms\n            if (row==col):\n                # sb.set_theme(style=\"white\")\n                ax = fig.add_subplot(grid[row, col])\n                # grid\n                plt.grid(linestyle=\"-\", alpha=0.5)\n                # histogram\n                ax.hist(x, bins=15, color=sb.color_palette(\"colorblind\")[0])\n                ax.set(xlabel='', ylabel='',\n                       title='')\n                layout_switch(row, col, col_len, data, fontsize)\n                \n        # Scatter (lower half)\n            if (row>col):\n                # sb.set_theme(style=\"white\")\n                ax = fig.add_subplot(grid[row, col])\n                plt.grid(linestyle=\"-\", alpha=0.5)\n                # scatter\n                ax.scatter(x,y, s=3, color=sb.color_palette(\"colorblind\")[0])\n                # fitted line\n                m, b = np.polyfit(x, y, 1)\n                ax.plot(x, m*x + b, color=sb.color_palette(\"colorblind\")[3], linewidth=1)\n                ax.set(xlabel='', ylabel='',\n                       title='')\n                layout_switch(row, col, col_len, data, fontsize)\n                \n        # Correlation (upper half)\n            if (row<col):\n                # sb.set_theme(style=\"white\")\n                ax = fig.add_subplot(grid[row, col])\n                # correlation and significance\n                s = stats.pearsonr(y=y, x=x)\n                ax.text(0.5, 0.5, '{:.2f}{}'.format(s[0], sign_cor(s[1])),\n                   ha=\"center\", va=\"center\", fontsize=int(fontsize*1.4))\n                ## Backround colour\n                background_col = int(((s[0]+1)\/2)*255)\n                background_col = colormap(background_col)\n                ax.set_facecolor(background_col)\n                layout_switch(row, col, col_len, data, fontsize)\n    \n    # add subplots\n    ax = fig.add_subplot(grid[:, col_len])\n    ax.axis('off')\n    \n    # add legend on the right\n    cbaxes = inset_axes(ax, width=\"40%\", height=\"100%\", loc='center left')\n    plt.colorbar(scalarmappaple, orientation=\"vertical\", cax=cbaxes)\n    \n    # show plot\n    # plt.tight_layout()\n    plt.show()\n    \n    # clean parameters \n    plt.cla()\n    plt.clf()\n    \ndef barplots_var():\n    '''\n    Produces barplots for a set of dependent and independent variables\n    '''\n    fig, axes = plt.subplots(1, len(end_var), figsize=(9, 6))\n    for i, col in enumerate(end_var):\n        ax = sb.boxplot(y=df_reg[col], ax=axes.flatten()[i])\n        ax.set_ylim(df_reg[col].min(), df_reg[col].max())\n        ax.set_ylabel(col)\n    fig.suptitle('Outputs', fontsize=18)\n    plt.tight_layout()\n    plt.show()\n\n    ## Independent Vars\n    fig, axes = plt.subplots(2, int(len(exog_vars)\/2), figsize=(14, 8))\n    for i, col in enumerate(exog_vars):\n        ax = sb.boxplot(y=df_reg[col], ax=axes.flatten()[i])\n        ax.set_ylim(df_reg[col].min(), df_reg[col].max())\n        ax.set_ylabel(col)\n    fig.suptitle('Inputs', fontsize=18)\n    plt.tight_layout()\n    plt.show()","b639d944":"## short column names\ncols_dict = {\n 'ISO3Year':\"ISO3Year\",\n 'ISO3':\"ISO3\",\n 'Year':\"Year\",\n 'CountryName':\"Country\",\n 'co2':\"CO2\",\n 'CO2_capita':\"CO2_capita\",\n 'CO2_gdp':\"CO2_gdp\",\n 'CO2_cum':\"CO2_cum\",\n 'CO2_cum_capita':'CO2_cum_capita',\n 'CO2_cum_gdp':'CO2_cum_gdp',\n 'population':\"Population\",\n 'Surface area (sq. km)':\"Surface\",\n 'AverageTemperature':\"Av. Temp.\",\n 'Forest area (% of land area)':\"Forest area\",\n 'CL': \"CL\",\n 'PR':\"PR\",\n 'Status':\"Status\",\n 'Human Development Index (UNDP)':\"HDI\",\n 'Government expenditure on education, total (% of GDP)':\"Education\",\n 'Gross Domestic Product (GDP)':\"GDP\",\n 'GDP_capita':'GDP_capita',\n 'Final consumption expenditure':\"Final consumption\",\n 'Total Value Added':\"VA\",\n 'VA_sum':\"VA_sum\",\n 'abs_prim':\"Primary\",\n 'abs_sec':\"Secondary\",\n 'abs_tert':\"Tertiary\",\n 'Mining, Manufacturing, Utilities (ISIC C-E)':\"C-E\",\n 'Manufacturing (ISIC D)':\"D\",\n 'Construction (ISIC F)':\"F\",\n 'Wholesale, retail trade, restaurants and hotels (ISIC G-H)':\"G-H\",\n 'Transport, storage and communication (ISIC I)':\"I\",\n 'Other Activities (ISIC J-P)':\"J-P\",\n 'share_prim':\"Share Primary\",\n 'share_sec':\"Share Secondary\",\n 'share_tert':\"Share Tertiary\",\n 'share_C_E':\"Share C-E\",\n 'share_D':\"Share D\",\n 'share_F':\"Share F\",\n 'share_G_H':\"Share G-H\",\n 'share_I':\"Share I\",\n 'share_J_P':\"Share J-P\",\n 'Exports of goods and services':\"Export\",\n 'Imports of goods and services':\"Imports\",\n 'export_share':\"Export share\",\n 'import_share':\"Import share\"\n}","2d4e42af":"## load and modify data\ndef regression_data(path, test_length=3, split=0.8):\n    '''\n    Function to read data, modify it accordingly and split it into train, validatin and test sets\n    Two types of splits are given:\n    one split according to years to obtain splits of full years (each country is present in a specific year in train\/val\/test)\n    the other were samples are randomly drawn from the DF and full years are not guaranteed.\n    test_length: number of years in the test set\n    '''\n    # set seed\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # read the data\n    df = pd.read_csv(path, sep=\";\")\n\n    # modify variables\n    vars_c02(df)\n\n    ## rename and reindex\n    df = df.reindex(list(cols_dict.keys()), axis=1)\n    df = df.rename(columns=cols_dict)\n\n    ## Year\n    year = pd.Categorical(df.Year)\n    ISO3 = pd.Categorical(df.ISO3)\n    df = df.set_index([\"ISO3\", \"Year\"])\n    df[\"year\"] = year\n    df[\"ISO3\"] = ISO3\n    del (ISO3, year)\n\n    ## Years of each split\n    years = df['year'].unique()\n    test_years = years[range(len(years) - test_length, len(years))]\n    reg_years = sorted(list(set(list(years)) - set(test_years)))\n    train_years = list(years[range(0, round(split * len(reg_years)))])\n    val_years = sorted(list(set(list(reg_years)) - set(train_years)))\n\n    ## Dataframe with information about the splits\n    years_split = pd.DataFrame(years)\n    years_split[\"split\"] = \"train\"\n    years_split.loc[years_split[0].isin(val_years), \"split\"] = \"validation\"\n    years_split.loc[years_split[0].isin(test_years), \"split\"] = \"test\"\n    years_split.set_index(0, inplace=True)\n\n    ## Acces second Row \/ Slice\n    # idx = pd.IndexSlice\n    # df.loc[idx[:,test_years],]\n    ## or\n    # df.loc[(slice(None), test_years),:]\n\n    # split data by years\n    df_reg = df.loc[(slice(None), reg_years), :]\n    n_all = len(df.index)\n    n_reg = len(df_reg.index)\n    n_train = len(df_reg.loc[(slice(None), train_years), :].index)\n    train_years_i = list(range(0, n_train))\n    val_years_i = list(set(range(0, n_reg)) - set(train_years_i))\n    test_i = list(set(range(0, n_all)) - set(train_years_i + val_years_i))\n\n    # split data randomly\n    train_i = random.sample(range(0, n_reg), round(split * n_reg))\n    val_i = list(set(range(0, n_reg)) - set(train_i))\n\n    ## returns: new DF, regression DF (new DF without the test set), information about the splits and the respective\n    ## two sets of split: by (1) years and (2) randomly, splits are given as indices (n) of the DF\n    return df, df_reg, years_split, train_years_i, val_years_i, train_i, val_i, test_i\n\n\ndef vars_c02(df):\n    '''\n    Define new variables and\n    rescale some variables to obtain better results and make them easier to interpret\n    '''\n    ## kg per capita\n    df['CO2_capita'] = df['co2']*1000000000\/df['population']\n    ## kg per $ GDP\n    df['CO2_gdp'] = df['co2']*1000000000\/df['Gross Domestic Product (GDP)']\n    ## Million tons\n    df['CO2_cum'] = df.groupby(['ISO3'])['co2'].cumsum()\n    ## kg per capita\n    df['CO2_cum_capita'] = df.groupby(['ISO3'])['CO2_cum'].cumsum()\n    ## kg per $ GDP\n    df['CO2_cum_gdp'] = df.groupby(['ISO3'])['CO2_gdp'].cumsum()\n    # $ per capita\n    df['GDP_capita'] = df['Gross Domestic Product (GDP)'] \/ df['population']\/1000\n    ## Billion $\n    df['Gross Domestic Product (GDP)'] = df['Gross Domestic Product (GDP)']\/1000000000\n    ## Thousand people\n    df['population'] = df['population'] \/ 1000000\n\n    \ndef scale_data(data, scale=None):\n    '''\n    scale the data according to a give scale:\n    e.g. preprocessing.StandardScaler() or sklearn.preprocessing.MinMaxScaler\n    '''\n    columns = data.columns\n\n    ## scaling of the given data by the given scaler\n    if (scale != None):\n        scaler = scale\n        scaler = scaler.fit(data)\n        data = scaler.transform(data)\n        data = pd.DataFrame(data)\n        data.columns = columns\n    else:\n        scaler = None\n\n    return data, scaler\n\ndef data_mod(data, X_var, y_var, split=None, scale=None):\n    '''\n    Modify data: separately:\n    (1) split and scale the data according to the given split and scale: for train and validation of models\n    (2) full data: for cross validation\n    These tasks are handled separately since otherwise the train set would include information about the validation set\n    Split needs to be list of 2 Elements:[train, validation]\n    '''\n    y_mod = data.loc[:, y_var]\n    X_mod = data.loc[:, X_var]\n\n    # split data into train and validation set\n    if (split!=None):\n        train = split[0]\n        val = split[1]\n        y_train = y_mod.iloc[train]\n        X_train = X_mod.iloc[train]\n        y_val = y_mod.iloc[val]\n        X_val = X_mod.iloc[val]\n\n        # scale train set according to function\n        X_train, scaler = scale_data(X_train, scale)\n\n        # scale full data according to function\n        X_mod, _ = scale_data(X_mod, scale)\n        # transform validation set with scaler of train set if scaler was provided\n        if (scaler != None):\n            X_val = pd.DataFrame(scaler.transform(X_val))\n            X_val.columns = X_var\n\n        return X_mod, y_mod, X_train, y_train, X_val, y_val, scaler\n\n    # If No split, but scaling is present\n    # scale full data according to function\n    elif (scaler!=None):\n        X_mod, scaler = scale_data(X_mod, scale)\n\n    ## If no split: train and val are the same\n    return X_mod, y_mod","e56037cf":"# overview function\ndef knn(X_mod, y_mod, X_train, y_train, X_val, y_val, folds):\n    '''\n    KNN\n    Performs two types of KNN-regressions with neighbours ranging from 1 to 100 to find the optimal number of neighbours.\n    (1) with a given train and validation data\n    (2) Cross validation with the full dataframe\n    and shows the results in a graph: neighbours vs. score\n    '''\n\n    ## Dictionary for the results\n    dic = {'KNN': [], 'Score': [], 'Score_CV': [], 'RMSE': []}\n    for i in range(1, 100, 1):\n        # set up and fit model\n        knn = neighbors.KNeighborsRegressor(n_neighbors=i)\n        knn.fit(X_train, y_train)\n\n        # Score, Prediction, MSE & RMSE\n        score = knn.score(X_val, y_val)\n        pred = knn.predict(X_val)\n        MSE = mean_squared_error(y_val, pred)\n        RMSE = np.sqrt(MSE)\n\n        ## Cross validate the model\n        kfold = KFold(n_splits=folds)\n        scores = model_selection.cross_val_score(knn, X_mod, y_mod, cv=kfold)\n\n        # Append results to the dictionary\n        # print(\"KNN({:d}): {:.3f} (CV5: {})\".format(i, score, scores.mean()))\n        dic['KNN'].append(i)\n        dic['Score'].append(score)\n        dic['Score_CV'].append(scores.mean())\n        dic['RMSE'].append(RMSE)\n\n    # Transform dictionary into dataframe\n    df_knn_res = pd.DataFrame(data=dic)\n\n    # extract best performance\n    max_knn = df_knn_res['Score'].max()\n    max_knncv = df_knn_res['Score_CV'].max()\n    max_knn_n = int(df_knn_res.loc[df_knn_res['Score'] == max_knn].KNN)\n    max_knncv_n = int(df_knn_res.loc[df_knn_res['Score_CV'] == max_knncv].KNN)\n\n    ## Plot knn results in a line plot neighbours vs. score\n    # sb.set_theme(style=\"whitegrid\")\n    # sb.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n    # plt.rcParams['font.size'] = '12'\n    # plt.subplots(figsize=(8, 8))\n    # sb.lineplot(data = df_knn_res, x='KNN', y='Score')\n    # plt.xlabel(\"Neighbors\")\n    # plt.ylabel(\"Accuracy\")\n    # plt.title(\"KNN\")\n    # plt.tight_layout()\n    # plt.show()\n\n    ## Plot knn-cv results in a line plot neighbours vs. score\n    # plt.subplots(figsize=(8, 8))\n    # sb.lineplot(data = df_knn_res, x='KNN', y='Score_CV')\n    # plt.xlabel(\"Neighbors\")\n    # plt.ylabel(\"Accuracy\")\n    # plt.title(\"KNN-CV\")\n    # plt.show()\n\n    # Print results\n    print(\"Best predictions:\")\n    print(\"KNN: {:.3f} with {:2d} neighbours.\".format(max_knn, max_knn_n))\n    print(\"KNN CV: {:.3f} with {:2d} neighbours.\".format(max_knncv, max_knncv_n))\n    print('------------------------------------------------------------------------------------\\n\\n')\n    return df_knn_res\n\ndef linear_regression(X_mod, y_mod, X_train, y_train, X_val, y_val, folds):\n    '''\n    Linear Regression\n    Set up a linear regression model with test\/val-split, extract the results and print in an output\n    these results depict detailed information about the regression\n    Perform a cross validation on the defined model to measure\/validate its performance and accuracy\n    '''\n\n    # set up and fit model\n    model = linear_model.LinearRegression()\n    model.fit(X=X_train, y=y_train)\n    # prediction, parameters, MSE & RMSE\n    pred = model.predict(X_val)\n    params = np.append(model.intercept_, model.coef_)\n    MSE = mean_squared_error(y_val, pred)\n    RMSE = np.sqrt(MSE)\n    lin_score = r2_score(y_val, pred)\n\n    # produce output\n    output = regression_output(X_train, params, MSE, list(X_mod.columns))\n\n    # Linear Regression: CV\n    # Cross validate regression\n    model = linear_model.LinearRegression()\n    kfold = KFold(n_splits=folds)\n    cv = cross_validate(model, X_mod, y_mod, cv=kfold)\n    lincv_score = cv['test_score'].mean()\n    model.fit(X=X_mod, y=y_mod)\n    RMSE_cv = np.sqrt(mean_squared_error(y_mod, model.predict(X_mod)))\n\n    # display output\n    display(output)\n    # The coefficient of determination: R2\n    print('R_squared: {:.3f}'.format(lin_score))\n    print('RMSE: {:.3f}'.format(RMSE))\n    print('\\n\\nCross Validation\\n')\n    print('R_squared: {:.3f}'.format(lincv_score))\n    print('------------------------------------------------------------------------------------\\n\\n')\n    return lin_score, lincv_score, RMSE, RMSE_cv\n\n\ndef regression_output(X, params, MSE, columns):\n    '''\n    Extracts detailed about the coefficients from a regression and gathers them in an output:\n    Std. Errors, t values and propabilities (p-value)\n    '''\n\n    newX = np.append(np.ones((len(X), 1)), X, axis=1)\n    # variance\n    var_b = MSE * (np.linalg.inv(np.dot(newX.T, newX)).diagonal())\n    # std. errors\n    sd_b = np.empty_like(var_b)\n    sd_b.fill(np.nan)\n    sd_b = np.sqrt(var_b, where=var_b>=0)\n    # t values\n    ts_b = params \/ sd_b\n    # p values\n    p_values = [2 * (1 - stats.t.cdf(np.abs(i), (len(newX) - len(newX[0])))) for i in ts_b]\n\n    # format values\n    sd_b = np.round(sd_b, 3)\n    ts_b = np.round(ts_b, 3)\n    p_values = np.round(p_values, 3)\n    params = np.round(params, 4)\n\n    # produce output\n    output = pd.DataFrame()\n    output[\"Coefficients\"], output[\"Standard Errors\"], output[\"t values\"], output[\"Probabilities\"] = [params, sd_b, ts_b, p_values]\n    var = ['Intercept'] + columns\n    output['var'] = var\n    output.set_index('var', inplace=True)\n\n    return output\n\ndef lasso_regression(X_mod, y_mod, folds):\n    '''\n    LASSO\n    Performs a cross validated Lasso regression (LARS) on a given data set\n    to see which variables\/regressors are of (most) importance.\n    In accordance to this variables can be selected to reduce the size of the model\n    and make it easier to understand.\n    Two plots are printed:\n    (1) Showing the importance of the variables of the model\n    (2) Showing the lasso-path of the variables, indicating there importance in accordance to they penalization term (alpha)\n    '''\n\n    eps = 1e-4\n\n    ## LARS\n    kfold = KFold(n_splits=folds)\n    lasso = sklearn.linear_model.LassoLarsCV(cv=kfold, max_iter=10000, eps=eps).fit(X_mod, y_mod)\n    lasso.fit(X=X_mod, y=y_mod)\n    lasso_score = lasso.score(X_mod, y_mod)\n    RMSE = np.sqrt(mean_squared_error(y_mod, lasso.predict(X_mod)))\n\n    ## Classic Lasso\n    # Alpha reduced since it explodes and gives unusable results\n    # alpha_range = range(1, 5000)\n    # lasso = sklearn.linear_model.LassoCV(cv=folds, max_iter=10000, alphas=alpha_range, random_state=seed, eps=eps).fit(X_mod, y_mod)\n\n    # Penalization term\n    # print('Alpha (penalization term): {:3f}'.format(lasso.alpha_))\n\n    lasso_imp_df = pd.Series(lasso.coef_)\n    lasso_imp_df.index = X_mod.columns\n\n    ## Code for Prediction\n    # lasso_score = lasso.score(X_mod, y_mod)\n    # RMSE = np.sqrt(mean_squared_error(y_mod, lasso.predict(X_mod)))\n    # print('R_squared: {:.3f}'.format(lasso_score))\n    # print('RMSE: {:.3f}'.format(RMSE))\n\n    ## Setting up the Lasso (LARS) Path\n    _, _, coefs_lars = linear_model.lars_path(np.array(X_mod), np.array(y_mod), method='lasso', verbose=True, eps=eps)\n\n    # Importance of each variable for the range of alphas gathered in a dataframe\n    # so one can easily obtains information about the best model\n    xx = np.sum(np.abs(coefs_lars.T), axis=1)\n    xx \/= xx[-1]\n    lasso_df = pd.DataFrame(coefs_lars.T)\n    lasso_df.columns = X_mod.columns\n    lasso_df['x'] = xx\n    lasso_df = pd.melt(lasso_df, id_vars=\"x\", value_vars=lasso_df.columns[range(0, len(lasso_df.columns) - 1)])\n\n    # Classic Lasso Path\n    # alphas_lasso, coefs_lasso, _ = linear_model.lasso_path(np.array(X_mod), np.array(y_mod), eps=eps, alphas=alpha_range, fit_intercept=False)\n    # lasso_df = pd.DataFrame(coefs_lasso.T)\n    # lasso_df.columns = X_mod.columns\n    # lasso_imp_df = lasso_df.iloc[::-1].reset_index().iloc[lasso.alpha_-1]\n    # lasso_imp_df.drop(\"index\", inplace=True)\n    # lasso_df['x'] = -np.log10(alphas_lasso) ## =xx\n    # lasso_df = pd.melt(lasso_df, id_vars=\"x\", value_vars=lasso_df.columns[range(0, len(lasso_df.columns) - 1)])\n\n    ## Bar Plot\n    # depicting the importance of each variable at for the best alpha (chosen by cv)\n    # sb.set_theme(style=\"whitegrid\")\n    sb.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n    plt.subplots(figsize=(8, 6))\n    ax = sb.barplot(x = lasso_imp_df.index, y=lasso_imp_df)\n    # label each bar in barplot with the actual value\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(x=p.get_x() + (p.get_width() \/ 2), y=height + height\/100,\n                s=\"{: .0f}\".format(height), ha=\"center\", size=9)\n    ax.set_xticklabels(\n        ax.get_xticklabels(),\n        rotation=45,\n        horizontalalignment='right'\n    )\n    plt.tight_layout()\n    ax.set_title(\"Importance of Variables (LassoLars CV)\")\n    plt.show()\n\n    ## LASSO Path Plot\n    # plt.subplots(figsize=(8, 6))\n    # sb.lineplot(data=lasso_df, x='x', y=\"value\", hue=\"variable\", style=\"variable\", palette=\"muted\", linewidth=1.5)\n    # ymin, ymax = plt.ylim()\n    # plt.vlines(xx, ymin, ymax, linewidth=1, alpha=0.5)\n    ## Classic Lasso\n    # plt.xlabel('-Log(alpha)')\n    ## LARS\n    # plt.xlabel('|coef| \/ max|coef|')\n    # plt.ylabel('Coefficients')\n    # plt.title('LASSO Path')\n    # plt.axis('tight')\n    # plt.legend(loc='center left')\n    # plt.show()\n\n\n    print('------------------------------------------------------------------------------------\\n\\n')\n    return lasso_imp_df, lasso_score, RMSE\n\ndef model_overview(data, X_var, y_var, split=None, scale=None, folds=10):\n    '''\n    Model Overview\n    Gathers several different models and their specific outputs\n    according to the provided variables\n    (1) KNN\n    (2) (Full) linear model (+CV) with all the given variables\n    (3) Lasso (LARS) CV for variable reduction\n    (4) (Reduced) linear model with the (important) variables derived from the Lasso regression in (3)\n    Displays the outputs for a comparison in the end\n    ! RMSE & Score are not comparable between CV models and non CV models !\n    '''\n    # Modulation of data\n    X_mod, y_mod, X_train, y_train, X_val, y_val, _ = data_mod(data, X_var, y_var, split, scale)\n\n    print('\\nPresenting different Models:\\n')\n    print('Inputs: {}'.format(list(X_var)))\n    print('Output: {}'.format(y_var))\n    print('CV: {:d}, scale: {}\\n\\n'.format(folds, scale))\n\n    # cont. KNN\n    print('Continuous KNN\\n')\n    df_knn_res = knn(X_mod, y_mod, X_train, y_train, X_val, y_val, folds)\n\n    # Full Modell: Linear Regression\n    print('Full Model: Linear Regression\\n')\n    lin_score, lincv_score, lin_RMSE, lin_RMSE_cv = linear_regression(X_mod, y_mod, X_train, y_train, X_val, y_val, folds)\n\n    # Lasso: Variable and Model Selection\n    print('Lasso\\n')\n    lasso_imp_df, lasso_score, lasso_rmse = lasso_regression(X_mod, y_mod, folds)\n\n    # Reduced Model\n    reduced_var = list(lasso_imp_df.loc[abs(lasso_imp_df)>lasso_imp_df.max()\/100].index)\n    print('Reduced Model: Linear Regression\\n')\n    print('With variables: {}\\n'.format(reduced_var))\n    lin_red_score, lincv_red_score, lin_red_RMSE, lin_red_RMSE_cv = linear_regression(X_mod[reduced_var], y_mod, X_train[reduced_var], y_train, X_val[reduced_var], y_val, folds)\n\n    # Overview\n    print('Overview')\n    scores = {'Method': [], 'Score': [], 'RMSE' :[]}\n    scores['Method'] = ['KNN', 'KNN (CV)', 'Linear model', 'Linear model (CV)', 'Lasso (CV)', 'Reduced linear model', 'Reduced linear model (CV)']\n    scores['Score'] = [df_knn_res['Score'].max(), df_knn_res['Score_CV'].max(), lin_score, lincv_score, lasso_score, lin_red_score,\n                       lincv_red_score]\n    scores['RMSE'] = [float(df_knn_res[df_knn_res['Score'] == df_knn_res['Score'].max()].RMSE),\n    float(df_knn_res[df_knn_res['Score_CV'] == df_knn_res['Score_CV'].max()].RMSE),\n                      lin_RMSE, lasso_rmse, lin_RMSE_cv, lin_red_RMSE,\n                       lin_red_RMSE_cv]\n    scores = pd.DataFrame(data=scores)\n    scores.set_index('Method', inplace=True)\n    display(scores)\n    print('------------------------------------------------------------------------------------\\n\\n')","ebb377b9":"## polynomial regression\ndef polynomialReg(data, formula, model, split, scale, X_var=None, y_var=None, reduce=False):\n    '''\n    Polynomial regression:\n    Defines and runs a polynomial regression either according to (1) a formula as elements of list:\n    or (2) a number of degrees of a given data with a given model (e.g. Linear, Ridge etc.).\n    Formula (list): first element is depend variable, consecutive variables are independent variables)\n    If only number of degrees is given additional y and X variables have to be provided,\n    to declare on which variables a full polynomial model is to be calculated.\n    If reduce=True: Also produces a LassoLARS to perform variable selection.\n    returns output with detailed information about the polynomial regression and the Lasso regression respectively.\n    '''\n\n    ## Produce polynomials\n    data_poly, new_formula, y_var = polynomial(data, formula, X_var, y_var)\n\n    ## Modify data: split and scale\n    X_mod, y_mod, X_train, y_train, X_val, y_val, _ = data_mod(data_poly, new_formula, y_var, split, scale)\n\n\n    ## Model\n    poly_model = model\n    poly_model.fit(X_train, y_train)\n    pred = poly_model.predict(X_val)\n    ## parameters\n    params = np.append(poly_model.intercept_, poly_model.coef_)\n\n    ## MSE & RMSE\n    MSE = mean_squared_error(y_val, pred)\n    RMSE = np.sqrt(MSE)\n    lin_score = r2_score(y_val, pred)\n\n    # get output\n    output = regression_output(X_train, params, MSE, list(X_mod.columns))\n\n    ## Output\n    ' + '.join(new_formula)\n    print(\"\")\n    display(output)\n    print(\"\")\n    # MSE\n    # print('Mean squared error: {:.3f}'.format(mean_squared_error(y_val, pred)))\n    # The coefficient of determination: R2\n    print('R_squared: {:.3f}'.format(lin_score))\n    print('RMSE: {:.3f}'.format(RMSE))\n\n    ## lasso LARS\n    if (reduce == True):\n        lasso_lars = sklearn.linear_model.LassoLars(max_iter=10000)\n        lasso_lars.fit(X_train, y_train)\n        # lasso_lars.score(X_train_poly, y_train)\n        # pred = lasso_lars.predict(X_val)\n\n        ## Lasso Importance Table\n        lasso_imp_df = pd.Series(lasso_lars.coef_)\n        lasso_imp_df.index = X_train.columns\n\n        ## Active Variables\n        # act_variables = list(lasso_imp_df[lasso_imp_df!=0].index)\n        ## Bar Plot\n        # sb.set_theme(style=\"whitegrid\")\n        sb.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n        plt.subplots(figsize=(8, 8))\n        ax = sb.barplot(x=lasso_imp_df.index, y=lasso_imp_df)\n        # label each bar in barplot\n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(x=p.get_x() + (p.get_width() \/ 2), y=height + height\/100,\n                    s=\"{: .0f}\".format(height), ha=\"center\", size=12)\n        ax.set_xticklabels(\n            ax.get_xticklabels(),\n            rotation=90,\n            horizontalalignment='right'\n        )\n        plt.tight_layout()\n        ax.set_title(\"Importance of Variables (LassoLars)\")\n        plt.show()\n\n        return (output, lasso_imp_df)\n\n    else:\n        return output\n\n\ndef polynomial(data, formula, X_var=None, y_var=None):\n    '''\n    Produces polynomials either according to (1) a formula as elements of list:\n    or (2) a number of degrees of a given data.\n    Formula (list): first element is depend variable, consecutive variables are independent variables)\n    If only number of degrees is given additional y and X variables have to be provided,\n    to declare on which variables full polynomials have to be calculated.\n    Returns: Polynomials of the data, the y variable and the new formula.\n    '''\n\n    if (type(formula) == int and (X_var == None or y_var == None)):\n        print(\n            \"If no formula, but only number of degrees is provided, please also provide a set of dependent and independent variables!\")\n        raise\n\n    ## Declare variables\n    all_varnames = list()\n    unique_varnames = list()\n    max_degree = 1\n\n    ## Formula and variables, if formula is given\n    ## Get number of max degrees\n    if (type(formula) == list):\n\n        X_var = list()\n        new_formula = formula.copy()\n\n        ## find vars with degree\n        degrees = [f for f in new_formula if re.search(r'\\^', f)]\n        for f in degrees:\n            degree = int(f.split(\"^\", 1)[1][0:1])\n            if (degree > max_degree):\n                max_degree = degree\n\n        # Get unique variable names\n        y_var = formula[0]\n        # remove dependent var\n        new_formula.remove(y_var)\n        for i in new_formula:\n            ## for interaction terms\n            if (i.find(' * ') != -1):\n                for j in i.split(\" * \"):\n                    unique_varnames.append(j.split(\"^\", 1)[0])\n            else:\n                unique_varnames.append(i.split(\"^\", 1)[0])\n        ## not changing the order; otherwise: list(set())\n        for x in unique_varnames:\n            # check if exists in unique_list or not\n            if x not in X_var:\n                X_var.append(x)\n        max_degree = max_degree+1\n\n    # if only degree is given\n    elif (type(formula) == int):\n        max_degree = formula\n\n\n    # with pipeline\n    # lasso_lars = make_pipeline(PolynomialFeatures(degree), Lasso(max_iter=10000))\n    # lasso_lars.fit(X_train, y_train)\n    # lasso_lars.score(X_train, y_train)\n\n    ## Create Polynomials\n    poly = PolynomialFeatures(max_degree)\n    data_poly = pd.DataFrame(poly.fit_transform(data[X_var]))\n\n    ## Variable names for the final model\n    for i in poly.powers_:\n        powers = i\n        var = \"\"\n        for j in range(0, len(X_var)):\n            power = powers[j]\n            if (var == \"\" and power == 1):\n                var = str(X_var[j])\n            elif (var == \"\" and power > 1):\n                var = str(X_var[j]) + \"^\" + str(power)\n            elif (power == 1):\n                var = var + \" * \" + str(X_var[j])\n            elif (power > 1):\n                var = var + \" * \" + str(X_var[j]) + \"^\" + str(power)\n        all_varnames.append(var)\n\n    ## Name DataFrame columns\n    data_poly.columns = all_varnames\n\n    ## Select variables by formula\n    if (type(formula) == int):\n        all_varnames.remove(\"\")\n        new_formula = all_varnames\n    data_poly = data_poly[new_formula]\n\n    ## Concatinate: X and y\n    data_poly = pd.concat([data[y_var].reset_index(drop=True), data_poly.reset_index(drop=True)], axis=1)\n\n    ## Obtain index\n    data_poly.index = data.index\n\n    return data_poly, new_formula, y_var","711f162e":"## Set seed\nseed = 111\n## Load data\ndf, df_reg, years_split, train_years_i, val_years_i, train_i, val_i, test_i = regression_data(\"\/kaggle\/working\/df_joined.csv\")\n# split data\nsplit = [train_i, val_i]","6810bcd6":"df_reg","8197e1b3":"cols_num = list(df.columns)\nfor i in [\"ISO3Year\",\"year\",\"ISO3\",\"Country\",\"CL\",\"PR\",\"Status\"]:\n    cols_num.remove(i)\nheatmap(df_reg, cols_num)","aa3d4b8e":"## declare important exogenous\/independent variables\n## and endogenous\/dependent variables\nend_var = ['CO2_capita', 'CO2_gdp', 'CO2']\nexog_vars = [\"Population\",\n             \"GDP_capita\", \"GDP\", \"Share Secondary\", \"Share Tertiary\",\n             \"Import share\", \"Export share\",\n             \"Av. Temp.\", \"Surface\", \"Forest area\",\n             \"HDI\", \"Education\"]","1a1a088f":"barplots_var()","50611e6c":"grid_plot_overview(df_reg[end_var + exog_vars], 8)","91ac4c5c":"model_overview(df_reg, exog_vars, 'CO2_capita', split, None, 10)","7d841281":"model_overview(df_reg, exog_vars, 'CO2_gdp', split, None, 10)","86c410d0":"model_overview(df_reg, exog_vars, 'CO2', split, None, 10)","938e4e6a":"model_overview(df_reg, exog_vars, 'CO2_capita', split, preprocessing.StandardScaler(), 10)","f41bcd45":"model_overview(df_reg, exog_vars, 'CO2_capita', split, preprocessing.MinMaxScaler(), 10)","2ba7c012":"def other_mods(X_train, y_train, X_val, y_val):\n    ## Elastic Net\n    elastic_netcv = sklearn.linear_model.ElasticNetCV(cv=10, max_iter=10000, alphas=list(np.linspace(0.1,100,1000)), l1_ratio=[.1, .5, .7, .9, .95, .99, 1], random_state=seed, eps = 1e-4).fit(X_mod, y_mod)\n    elastic_netcv.alpha_\n    elastic_netcv.l1_ratio_\n\n    elastic_net = sklearn.linear_model.ElasticNet(alpha=elastic_netcv.alpha_, l1_ratio=elastic_netcv.l1_ratio_).fit(X_train, y_train)\n    pred = elastic_net.predict(X_val)\n    print(\"\\nElastic Net\")\n    print(\"Score: {:.3f}\".format(elastic_netcv.score(X_val, y_val)))\n    print(\"RMSE: {:.3f}\".format(np.sqrt(mean_squared_error(y_val, elastic_net.predict(X_val)))))\n    pred_graph(y_val, pred, \"Elastic Net: \")\n\n    ## Bayesian Ridge\n    brr = linear_model.BayesianRidge()\n    brr.fit(X_train, y_train)\n    pred = brr.predict(X_val)\n    print(\"\\nBayesian Ridge\")\n    print(\"Score: {:.3f}\".format(brr.score(X_val, y_val)))\n    print(\"RMSE: {:.3f}\".format(np.sqrt(mean_squared_error(y_val, brr.predict(X_val)))))\n    pred_graph(y_val, pred, \"Bayesian Ridge: \")\n    \n    ## ARD\n    ard = linear_model.ARDRegression()\n    ard.fit(X_train, y_train)\n    pred = ard.predict(X_val)\n    print(\"\\nBayesian ARD regression\")\n    print(\"Score: {:.3f}\".format(ard.score(X_val, y_val)))\n    print(\"RMSE: {:.3f}\".format(np.sqrt(mean_squared_error(y_val,ard.predict(X_val)))))\n    pred_graph(y_val, pred, \"ARD \")\n    \n    ## GLM\n    glm_freq = linear_model.TweedieRegressor(max_iter=100000)\n    glm_freq.fit(X_train, y_train)\n    pred = glm_freq.predict(X_val)\n    print(\"\\nGLM\")\n    print(\"Score: {:.3f}\".format(glm_freq.score(X_val, y_val)))\n    print(\"RMSE: {:.3f}\".format(np.sqrt(mean_squared_error(y_val,glm_freq.predict(X_val)))))\n    pred_graph(y_val, pred, \"GLM: \")\n\n    ## SVM - not good\n    svr = sklearn.svm.SVR()\n    svr.fit(X_train, y_train)\n    pred = svr.predict(X_val)\n    print(\"\\nSVM\")\n    print(\"Score: {:.3f}\".format(svr.score(X_val, y_val)))\n    print(\"RMSE: {:.3f}\".format(np.sqrt(mean_squared_error(y_val,svr.predict(X_val)))))\n    pred_graph(y_val, pred, \"SVM: \")\n    \n    ## MLP\n    # solver: lbfgs\n    # activation: relu\n    # alpha: for normalized data: smaller, for unnorm: 1\n\n    alpha = 8.385e-06\n    alpha = 5.5e-06\n    mlp = MLPRegressor(random_state=seed, max_iter=400000, solver=\"lbfgs\", activation=\"relu\", alpha=alpha).fit(X_train, y_train)\n    mlp.fit(X_train, y_train)\n    pred = mlp.predict(X_val)\n    print(\"\\nMLP\")\n    print(\"Score: {:.3f}\".format(mlp.score(X_val, y_val)))\n    print(\"RMSE: {:.3f}\".format(np.sqrt(mean_squared_error(y_val, mlp.predict(X_val)))))\n    pred_graph(y_val, pred, \"MLP: \")\n    \n    etr = sklearn.ensemble.ExtraTreesRegressor(n_estimators=50, ccp_alpha=1, min_weight_fraction_leaf=0, min_samples_split = 2, random_state=1)\n    etr.fit(X_train, y_train)\n    pred = etr.predict(X_val)\n    print(\"\\nETR\")\n    print(\"Score: {:.3f}\".format(etr.score(X_val, y_val)))\n    print(\"RMSE: {:.3f}\".format(np.sqrt(mean_squared_error(y_val,etr.predict(X_val)))))\n    pred_graph(y_val, pred, \"Elastic Net: \")","9a906a58":"X_mod, y_mod, X_train, y_train, X_val, y_val, scaler_st = data_mod(df_reg, exog_vars, 'CO2_capita', split, preprocessing.StandardScaler())\nother_mods(X_train, y_train, X_val, y_val)","2858dbaf":"X_mod, y_mod, X_train, y_train, X_val, y_val, scaler_min = data_mod(df_reg, exog_vars, 'CO2_capita', split, preprocessing.MinMaxScaler())\nother_mods(X_train, y_train, X_val, y_val)","2afe6479":"# Perform a grid search over a handful of parameters to obtain the best estimators.\n# The range of each paramater has been chosen through a prior testings.\ndef etr_grid():\n    etr = sklearn.ensemble.ExtraTreesRegressor()\n    etr.get_params()\n    ## grid search\n    param_grid_etr = [\n        {'n_estimators': [10,50,100,500,1000],\n         'min_samples_split': [2,3],\n         'min_samples_leaf': [0.5, 1,2],\n         'ccp_alpha': [0,0.5,1],\n         'min_weight_fraction_leaf': [0, 0.25, 0.5]}\n    ]\n    etr_gscv = sklearn.model_selection.GridSearchCV(etr, param_grid_etr, cv=10)\n    etr_gscv.fit(X_train, y_train)\n    # etr_gscv.cv_results_\n    # etr_grid_results = pd.DataFrame.from_dict(etr_gscv.cv_results_)\n    # etr_gscv.best_estimator_\n    # etr_gscv.best_score_\n    \n    return etr_gscv","fe07d01f":"# Fit and predict with ETR\nX_mod, y_mod, X_train, y_train, X_val, y_val, scaler_min = data_mod(df_reg, exog_vars, 'CO2_capita', split, preprocessing.MinMaxScaler())\netr = sklearn.ensemble.ExtraTreesRegressor(n_estimators=50, ccp_alpha=1, min_weight_fraction_leaf=0, min_samples_split = 3, random_state=111)\netr.fit(X_train, y_train)\npred = etr.predict(X_val)\nprint(etr.score(X_val, y_val))\nprint(math.sqrt(mean_squared_error(y_val,etr.predict(X_val))))\npred_graph(y_val, pred, \"ETR: \")","214014ef":"print(\"ETR: \")\nscores = sklearn.model_selection.cross_validate(etr, X_mod, y_mod, cv=10, scoring=['explained_variance','neg_root_mean_squared_error'])\nprint(\"Score: {:.3f}\".format(scores['test_explained_variance'].mean()))\nprint(\"RMSE: {:.3f}\".format(abs(scores['test_neg_root_mean_squared_error'].mean())))\n\nprint(\"\\nKNN: \")\nknn = neighbors.KNeighborsRegressor(n_neighbors=2, weights=\"uniform\")\nscores = sklearn.model_selection.cross_validate(knn, X_mod, y_mod, cv=10, scoring=['explained_variance','neg_root_mean_squared_error'])\nprint(\"Score: {:.3f}\".format(scores['test_explained_variance'].mean()))\nprint(\"RMSE: {:.3f}\".format(abs(scores['test_neg_root_mean_squared_error'].mean())))","a36540a0":"def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    From the sci-kit module\n    function to plot the learning curve\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt\n\n## learning curve\ndef learning_curve_plot():\n    cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=seed)\n\n    # fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n    plot_learning_curve(estimator = sklearn.ensemble.ExtraTreesRegressor(n_estimators=50, ccp_alpha=1, min_weight_fraction_leaf=0),\n                        title = \"Test\", X=X_mod, y=y_mod, axes=None, cv=cv, n_jobs=4)\n    \n    plt.show()\n\ndef validation_curve():\n    param_range=range(10,101,10)\n    train_scores, test_scores = sklearn.model_selection.validation_curve(sklearn.ensemble.ExtraTreesRegressor(ccp_alpha=1, min_weight_fraction_leaf=0),\n                     np.array(X_mod), np.array(y_mod), param_name=\"n_estimators\", param_range=param_range, cv=10)\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    plt.title(\"Validation Curve with ETR\")\n    plt.xlabel(r\"$\\gamma$\")\n    plt.ylabel(\"Score\")\n    # plt.ylim(0.0, 1.1)\n    lw = 2\n    plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n                 color=\"darkorange\", lw=lw)\n    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.2,\n                     color=\"darkorange\", lw=lw)\n    plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n                 color=\"navy\", lw=lw)\n    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.2,\n                     color=\"navy\", lw=lw)\n    plt.legend(loc=\"best\")\n    plt.show()","a13da3fd":"learning_curve_plot()","444961bd":"X_mod, y_mod, X_train, y_train, X_val, y_val, scaler_min = data_mod(df_reg, exog_vars, 'CO2_capita', split, preprocessing.MinMaxScaler())\nX_test = pd.DataFrame(scaler_min.transform(df[exog_vars].iloc[test_i]))\nX_test.columns = exog_vars\ny_test = df['CO2_capita'].iloc[test_i]\n\netr = sklearn.ensemble.ExtraTreesRegressor(n_estimators=50, ccp_alpha=1, min_weight_fraction_leaf=0, min_samples_split = 3)\netr.fit(X_mod, y_mod)\npred = etr.predict(X_test)\nprint(\"\\nETR\")\nprint(\"Score: {:.3f}\".format(etr.score(X_test, y_test)))\nprint(\"RMSE: {:.3f}\".format(np.sqrt(mean_squared_error(y_test,etr.predict(X_test)))))\npred_graph(y_test, pred, \"ETR: \")","c73d631e":"# Reduced set of variables\nexog_vars_red = [\"GDP_capita\", \"Share Secondary\", \"Share Tertiary\",\n             \"Import share\", \"Export share\",\n             \"HDI\"]","3460dd69":"grid_plot_overview(df_reg[['CO2_capita'] + exog_vars_red], 10)","1810cf21":"for i in exog_vars_red:\n    scatter(data=df_reg, x_axis=i, y_axis='CO2_capita', degree=4)","e69e5c9c":"scatter(data=df_reg, x_axis='GDP', y_axis='CO2', degree=4)\nscatter(data=df_reg.loc[df_reg[\"GDP\"]<2500], x_axis='GDP', y_axis='CO2', degree=4)","b9c04963":"## Full polynomial model to the degree of 3.\nmodel = linear_model.LinearRegression()\ndf_poly, active_variables = polynomialReg(df_reg, 3, model, split, None, sorted(exog_vars_red), ['CO2_capita'], True)","eadf80a4":"exog_vars_red_poly = list(active_variables.loc[abs(active_variables)>active_variables.max()\/100].index)\nexog_vars_red_poly","75edcb62":"## rearrange, since formula is sensitive to sorting (of interrelation terms)\nexog_vars_red_poly = ['GDP_capita','Export share * Import share',\n 'Export share * Share Secondary^2',\n 'Export share * Share Secondary * Share Tertiary',\n 'Share Secondary * HDI^2',\n 'Import share * Share Tertiary * HDI',\n 'Import share^2 * Share Secondary',\n 'Import share * Share Secondary^2',\n 'Share Secondary^3',\n 'Share Secondary * Share Tertiary^2',\n 'Share Tertiary^3']","a75def07":"df_poly, active_variables = polynomialReg(df_reg, ['CO2_capita'] + list(exog_vars_red_poly), model, split, None, None, None, True)","12ec8220":"formula_co2_cap = ['CO2_capita', \"GDP_capita\" , \"Share Secondary\", \"Share Secondary^2\",\n                   \"Share Tertiary\", \"Share Tertiary^2\", \"Share Tertiary^3\",\n                   \"Import share\", \"HDI\"]\ndf_poly, active_variables = polynomialReg(df_reg, formula_co2_cap, model, split, None, None, None, True)","e0c0ed5c":"df_poly, active_variables = polynomialReg(df_reg, 3, model, split, None, ['GDP'] + exog_vars_red, ['CO2'], True)","117983c4":"formula_co2_red = ['CO2'] + list(active_variables[active_variables>0.5].index)\ndf_poly, active_variables = polynomialReg(df_reg, formula_co2_red, model, split, None, ['GDP'] + formula_co2_red, ['CO2'], True)","a7859301":"scatter(data=df_reg, x_axis='Secondary', y_axis='CO2', degree=2)","9e840ab0":"data_poly, new_formula, y_var = polynomial(df_reg, ['CO2_capita'] + exog_vars_red_poly)\nmod_ols = PanelOLS(df_reg['CO2_capita'], data_poly[new_formula], entity_effects=True, time_effects=True)\nres_ols = mod_ols.fit(cov_type='robust', debiased=True)\nprint(res_ols)","7583dff2":"data_poly, new_formula, y_var = polynomial(df_reg, formula_co2_cap)\nmod_ols = PanelOLS(df_reg['CO2_capita'], data_poly[new_formula], entity_effects=True, time_effects=True)\nres_ols = mod_ols.fit(cov_type='robust', debiased=True)\nprint(res_ols)","34016785":"data_poly, new_formula, y_var = polynomial(df_reg, formula_co2_red)\nmod_ols = PanelOLS(df_reg['CO2'], data_poly[new_formula], entity_effects=True, time_effects=True)\nres_ols = mod_ols.fit(cov_type='robust', debiased=True)\nprint(res_ols)","2ba5daad":"**Interpretation of above data, Check for Outliers**:\n\nExtreme values are classified as minima when they lie 40% below 8 %percentile and are classified as maxima when they are 40% above 92% percentile - This heuristic has been empricially choosen and such values are not expected for normal evolution of economic and political or climate indicators.\n\nWhen applying these checks, such extrema are visibile in economical data. This occurs never in large developed industrial countries like the G7 or G20, but appears in developing countries mainly in central Africa, or former Soviet Union Republics or in the middle east. By checking the chronologic occurrence, some patterns are visible:\n\n1972 -1973: Chile: Crisis and Military coup against democratic government around Allende\n\n1972-1975: Oil crisis: Extrema in some OPEC countries: Iraq, Kuwait, Sudi Arabia, Libya\n\n1991: Gulf war around Kuwait: Kuwait economic data has extremes\n\n1989 - 1993: Collapse of the Soviet Union: Georgia, Latvia, Estonia, Lithuania, Tajikistan, Armenia, Ukraine\n\n1995: Balkan War; Serbia\n\n2010 - 2016: Libya\n\nOther extrema are visible in developing countries exposed to crisis like: Benin, Burund, Botswana, Central African Republic, Chad, Congo, Djibut, Liberia, Mali, Nepal, Rwanda, Sierra Leone, Zimbawe\n\nHence these extreme values occur when there is a sort of economic crisis or war e.g. the collapse of the Soviet Union, wars in near east contries or civil wars and crisis in central Africa.\n\n**Conclusion:**\n\nFrom these observation it is concluded that there are no real outliers due to wrong data entry or data processing in the data and the collected data is not modified for further processing.\n","4f0db8ca":"<div id=\"helper\"><\/div>\n\n# Helper Functions","c06ab6ad":"Since our predictive model performs very good, we now we turn to the descriptive modeling. At first we reduce our independent variables to the ones the overview regression has shown to be significant.","b3cb6149":"The strongest individual relationship exists between CO2 per capita and GDP per capita with a score of almost 50%. Altough all the variables do not show a polynomial relationship as explicit as the two examples below, we still see deviations from the linear regression in the form of an inverted U or a N shape.","e737262f":"The model for absolute CO2 emission is not as intuitiv as the others - what is more: the full and the reduced model are quite different (jumping directions of signs) as well, which indicates that the model is not very stable. This is why accurate statements about the significant factors, that drive CO2 emissions, cannot be given at this stage. We also see that with the omission of GDP in the reduced model the R squared drops from almost 90% down to less than 10%. This means that GDP pretty acuratly describes a huge part of the variance of the emission of CO2.\n\nTo narrow down the further analysis, we will reduce the exogenous variables to the set of inputs, that prior models have shown to be significant. This way we can on the one hand verify their individual effects and on the other hand the stability of the effect of the other inputs.\n\nIn further stages we will also concentrate on the effects of this inputs on CO2\/capita due to the above given reasons.","1688c88f":"<div id=\"descr\"><\/div>\n\n## Descreptive model","e7624690":"The panel analysis shows that the size of the secondary sector indeed correlates linearly and quadratically with the emission of CO2 (p values are below an alpha of 0.01) and can explain almost 80% of the variance.","3297484d":"Analysing the composition of the medium gdp\/capita group, it turns out, that about 84% of its CO2 emissions are caused by China. So it can be concluded, that the significant increase of CO2 emissions that started in the early 2000s is caused by China. ","383c4827":"# Table of contents\n* [**Introduction**](#introduction)\n* [Imports](#imports)\n* [Helper Functions](#helper)\n    * [General](#helper_general)\n    * [Plotting](#helper_plotting)\n* [**Data Preparation**](#data_prep)\n    * [Import and Preprocess Data](#data_imp)\n        * [Common ISO3 and Country](#iso3) \n        * [CO2](#co2) \n        * [National Accounts](#na) \n        * [Political Data](#polit) \n        * [HDI, Education, Forest Area & Surface](#id8) \n        * [Temperature](#temp) \n        * [Common Processing and Joining DataFrames](#join) \n    * [Data Preparation Script](#data_preparation_script) \n    * [Report on Preprocessing](#preprocessing_report)\n* [**Descriptive Statistics**](#Descriptive_Statistics)\n    * [Total CO2 emissions](#total_co2)\n    * [Grouping Countries](#grouping)\n    * [CO2 per capita histograms](#histograms)\n* [**Modeling**](#model)\n    * [Load and modify data for regression](*lad)\n    * [Introduction](#intro)\n    * [Overview](#overview)\n        * [Heatmap](#heat)\n        * [Complex correlation plot](#corplot)\n        * [Overview: Regression on CO2 per capita without scaling](#o_reg_cap)\n        * [Overview: Regression on CO2 per Dollar GDP without scaling](#o_reg_gdp)\n        * [Overview: Regression on CO2 without scaling](#o_reg_co)\n    * [Predictive Model](#pred)\n        * [Overview: Regression on CO2 per capita with standard scaling](#o_reg_st)\n        * [Overview: Regression on CO2 per capita with min-max scaling](#o_reg_min)\n        * [Other Models](#others)\n        * [Extra Trees Regressor](#etr)\n    * [Descriptive Model](#descr)\n        * [Overview](#overview_desc)\n        * [Polynomial Relationships](#poly_rel)\n        * [Polynomial Model](#poly)\n        * [Panel OLS](#ols)","cb3e1b4b":"#### Learning Curve","5a22fdf1":"<div id=\"ols\"><\/div>\n\n### Panel OLS","ee04abf5":"Political Rights and expenditures on education does not have a clear impact on CO2 emssions.","afaf5ce4":"Since literature suggests that the relation between CO2\/capita and its regressors is not linear, but polynomial (especially quadratic and cubic) and we want to further deepen\/enhance our descpriptive model as well, we will add basis functions to it. So the task in the following section is to find a polynomial model that is performant as performant as possible and yet easy to interpret (since an overload of polynomials may affect its readability).\n\nFor this we will first have a look at the polynomial relationship between individual variables and the output.","8085d39e":"Allmost all inputs have a significant effect on our output variables (With some minor exceptions). The goal of the descriptive regression is therefore to find the decisive drivers behind this multicorrelation. This is very important, since most of our inputs also tend to intercorrelate, meaning that e.g. an increase in HDI correlates with an increase in GDP\/capita as well as in CO2\/capita and it is not clear in the first place, wether an increase of HDI or GDP lead to a higher CO2\/capita. Since we cannot answer the question of causality, we restrict ourselves to the question of correlation. To disentangle these interrelations, we have to create a model with all the possible variables and look at the results\/significance of the individual inputs.","49ef6c66":"<div id=\"histograms\"><\/div>\n\n## CO2 per capita histograms","9c4e2a32":"These results look really promising: a RMSE of below 1300 and a score (coverage of variablity) of almost 97%. Since the model parameters were obtained through cross validation, the cross validated results mirror the ones for the initial train\/val split and are compared to the KNN model significantly better.","cfd7772e":"Calculate Correlations over Columns","9a16176a":"Furthermore, to have a brief insight into the parameters that could characterize the countries' carbon emissions, countries were described by mean values over the time period (1972-2016) of the considered parameters. Obviously, population has a major impact on the absolute carbon dioxide emitted. Therefore, we first look into the correlations with the co2\/Capita column. To get an idea of the countries that dominate this parameter, we first present a plot of top 10 countries.","094b509a":"<div id=\"corplot\"><\/div>\n\n### Complex correlation plot","c93745b4":"The barplots delivers a brief summary over the output and input variables. Here we clearly see, that the three variables measured in absolute terms (CO2, population and GDP) are very wide spread are more difficult to regress with. The other variables are nicely spread and therefore - as mentioned above - better suited for regression.","f1987e67":"The full polynomial model for absolute CO2 emissions covers - with an R squared of around .98 - almost all the variance. Yet, the drawback of this model is its amount of inputs. Via Lasso we only filter two that are important in explaining the emission of CO2: the absolute size of the secondary sector (GDP * secondary sector) and its squared size. But since both variables have a positive sign the existence of the EKC (for which we have given evidence beforehand) has to be questioned again. Furthermore, if we look at the corresponding scatter plot, we'll see that the quadtratic relationship has only a slight bending.","34a4b78d":"<div id=\"data_imp\"><\/div>\n\n## Import and Preprocess Data","8c820e3c":"<div id=\"id8\"><\/div>\n\n### HDI, Education, Forest Area & Surface","639c0396":"<div id=\"co2\"><\/div>\n\n### CO2 ","bf140e1f":"Above Plot shows the mean CO2\/capita time evolution over 5 baskets of countries with similar Human Developemnt Index (HDI). CO2 consumption is strongly correlated with HDI, but for higehr HDI groups the CO2\/capita emission is decreasing since the late 90s.","909566d0":"Overall countries with less polulation have higher CO2\/capita emission.","93b61539":"#### Final test for the model","e9957627":"<div id=\"heatmap\"><\/div>\n\n### Heatmap","0359a889":"#### Scatter Plots CO2\/capita","3e2b669f":"<div id=\"lad\"><\/div>\n\n## Load and modify data for regression","78fa2342":"Through PanelOLS with fixed entity and time effects we can check whether the observed correlations are time and entity invariant or are actually specific to an individual country or period of time.\n\nAlthough random effects are efficient and should be used if certain assumptions are satisfied, they can be biased if theses criterias are not met. For random effects it is necessary that the time and country effects are uncorrelated to the other inputs of the model. To test this assumption one can run a Hausman test, which is unfortunately not included in the linearmodels package. So we rely on the fixed effects estimator, since it is always correct, but might be inefficient (leading to a higher variance). \nFurthermore in a case like ours (with a low number of entities and periods) normally one would use a system GMM to calculate the regression, which is also not present in the package. \nStill the results would not be all to different, only a bit more accurate.","c7b958ee":"<div id=\"overview_desc\"><\/div>\n\n### Overview","e1f8502c":"All models perform on average (comparable to our linear model with slightly overpredicting values) except for the MLP and the ETR (Extra Trees Regressor) with the clear advantage for the ETR with a RMSE of 1342 and a score of 0.97. Yet it performs not much better compared to the the simple KNN model, which outplays all the other models. This ones more confirms that in the end (after all the CPU and GPU heavy work) a simple KNN might deliver the best results. But in our case ETR predicts a bit more acurate, that is why in the following we will concentrate on further enhancing this model.","a1ba0af2":"## Analysis of time evolution \n\nFor this analysis, in each year over the observation range from 1972 to 2016, a certain parameter e.g. Human Development Index (HDI) is put into baskets, which mean a 20% or 14% percentile. Over all these baskets the mean value of the CO2 parameter over all countries belonging to  these baskets is calculated and its evolution over time is plotted. Hence this mean value of the CO2 parameter is common for a basket of countries with a similar development index.\n\nBy applying this method some countries evolve over time to a different development stage, which means the membership of countries to baskets can vary over observation time. This occurs typically for emerging countries like e.g. China.\n\nThe Parametes observed over time are:\n- CO2\/Capita\n- CO2\/GDP","332dd3d5":"For our reduced model the R Squared within drops even further to below 1% (overall R squared still lies around 0.4). Yet, we get a clearer picture about the relationship between the sector shares and the CO2\/capita:\n\n* The share of the secondary sector (linearly) has a positive effect on the emissions per capita: The larger the share of the secondary sector in an economy the more CO2 it emits per capita.\n* While the share of the tertiary sector has a resversed U shaped effect on it: declining - rising - declining again. This means, that the share of the tertiary sector has to a certain point a positive effect on CO2\/capita, but once it reaches this point, its effect on CO2\/capita is negative: service based economies tend to emit less CO2\/capita, since services are less energy intensive than the secondary sector. A result we initially expected and is widely shown in literature. So while the EKC could not be demonstrated for the total emissions of CO2 it shows up again in the case of CO2 emissions per capita.\n\nStill, the relationship between CO2\/capita and the country specific characteristics remain vague and are not as evident as for the total emission of CO2. Our results therefore suggest, that the amount of CO2 emissions per capita is to a big extend specific to the individual country and not driven by transnational correlations.","131a7f3f":"Allmost all of the inputs turn out to be insignificant, except some of the relational terms. Also the R squared within is very low, meaning that the model only acounts for around 6% of the variance within the countries. The overall R squared is still pretty good.","9a527553":"To test the findings above, we will add these polynomial terms to our regression and see which relationships are considered the strongest and drive the CO2 emission per capita (and in absolute terms) the most.","6d784c58":"CO2\/GDP emission is highest for the industrial countries, but the impact is decreasing.","2d9f3376":"### Scope of the Data\n - global data is taken from Global Carbon Project and represents all countries of the world\n - a Pandas.DataFrame that combines the datasets mentioned in the [Introduction](#Introduction) on countries and territories has been compiled. In order to achieve a high completeness over a time span of 1972-2016, several countries had to be removed.\n - a Pandas.DataFrame that is a reduced version of an Our-World-in-Data dataset that has additional information on consumption based CO2 and energy sources \n\nAs mentioned, some countries had to be dropped. However, the remaining data is still highly representative for global carbon emissions, as is shown here:","c74b16d9":"### Plot time evolution of baskets CO2\/Capita","91fed728":"#### Overview Function","926c769b":"The learning and the validation curve provide information about fitting of the model on the one hand and about about the scalabilty of the model on the other.\nIf CV curve is much below the training curve the model is heavily overfitting, which is not present in our case, since at around 5000 training examples cv and training curve are pretty close. Also the scalability of our model is pretty impressive, indicating that with more observations our model could perform even better.","e9d419f7":"A strong link between both high development and immense economy with the co2 per capita emissions did not come as a surprise. More GDP dollars(USD) signal more co2 emissions. Interesting to see here is a moderate negative correlation with the average annual change in co2\/Capita emission. This shows the countries that were on average the most carbon intense, showed the largest reduction in co2\/Capita emissions from 1972 to 2016. Furthermore, we looked into correlations with co2 intensity of each USD of the countries' GDPs. This will be referred to as - co2 emission per USD. Building on the same idea as mentioned above, there is a strong negative correlation between carbon emissions per USD and their average annual change. In other words - countries emitting, on average, the most co2 per each USD of their GDP, showed the smallest increase (or the largest decrease) in this category over the considered timespan. Interestingly, average temperature and its average annual change (averaged over the timespan 1972-2016) seem to indicate a connection with the co2 emission per USD. Additionally, there seems to be a relationship between political freedoms and the co2 emission per USD. In the following graph we present the top 10 countries with respect to the co2 emissions per USD. However, further analysis is needed to make any substantial conclusions. ","259656f0":"#### Plots","d6c4e211":"#### Acumulated CO2\nCumulative emissions of CO2 from 1751 through to 2014, measured in million tonnes. Calculated by Our World in Data based on Carbon Dioxide Information Analysis Center (CDIAC) and Global Carbon Project","b8a3e5d5":"It can be seen that while the countries with highest GDP per capita show a decrease in CO2 emissions per capita, most other groups are more or less stagnating. The dramatic increase of CO2 emissions of the medium GDP per capita quantile is barely visible in this plot.  ","4be015ef":"#### Barplots","ca87a4de":"#### CO2\/capita","ac80bc81":"<div id=\"model\"><\/div>\n\n# Modeling","cf2e9ffb":"The full polynomial model for CO2\/capita is quite acurate (R squared of almost 0.8), but comes only at the price of a huge amount of inputs. Although we can slim down the model via Lasso it is still very complex and unlike the reduced polynomial model for absolute CO2 emissions not very intuitiv (and still looses more than 0.2 in the score). This indicates that CO2 per capita is to the most extend not unversally influenced by a handful of parameters, but rather by a complex interplay between country specifc dimensions. This hypothesis can be tested via PanelOLS (see below) to extract time and country specific effects.","34aaf5f5":"<div id=\"imports\"><\/div>\n\n# Imports","95742014":"### Consumption based CO2\nThe above figures indicated, that China is mostly responsible for the increase of CO2 emissions. However this is partly due to the fact, that a lot of industrial production has moved from developed countries to China. The produced goods are however often still consumed in developed countries. The Our-World-in-Data dataset therefore offers data on \"Consumption based CO2\": Emissions are being associated with the country, where goods are being consumed rather than where they are being produced.","ba45de2a":"<div id=\"introduction\"><\/div>\n\n# Introduction\n\nThis notebook was created as a project in the Data Oriented Programming Paradigms course at TU Wien during winter semester 2020\/21. The goal of the notebook is to answer these questions (Question 19):\n- How is the level of carbon emission in the world evolving?  \n- Which countries are emitting and absorbing the most carbon? \n- Are there di\ufb00erences when considering all carbon ever emitted by a country?  \n- Are there characteristics of countries that correlate with the changes in the carbon emissions?  \n\nFor this, data from seveal sources is being collected and processed: \n- Data on global carbon sources and sinks (Global Carbon Project)\n- Data on carbon emissions by country (Our World in Data)\n- National Accounts (United Nations) \n- Political Regime (Our World in Data)\n- Education (United Nations Development Programme)\n- Temperature by Country (Kaggle)\n\nThe approach is to use descriptive statistics on that data as well as creating a predictive model. \n\n\n### Authors\nGroup 24:\n - Josip Bobinac - e11932458\n - Fabian Gabelberger - 0806306\n - Simon Hinterseer - e9925802 \n - Horst J\u00fcrgen Kratschmann - e8627141\n","cfbd66d6":"<div id=\"o_reg_min\"><\/div>\n\n### Overview: Regression on CO2 per capita with min-max scaling","c4cf90ca":"<div id=\"o_reg_co\"><\/div>\n\n### Overview: Regression on CO2 without scaling","0039789f":"<div id=\"temp\"><\/div>\n\n### Temperature","0dfe6e79":"<div id=\"total_co2\"><\/div>\n\n## Total CO2 emissions","e46dd4a1":"\n\nCalculate Descriptive Statistics parameters for all columns\n\n","2b9bfd03":"# Descriptive Statistics","19807339":"#### Other models: Standard Scaler","6da1de83":"<div id=\"grouping\"><\/div>\n\n## Grouping Countries","e93d5e4e":"In terms of total acumulated CO2 emissions, China has taken second place behind the USA. However this does not take into account the size of China.","43501d4f":"<div id=\"poly\"><\/div>\n\n### Polynomial Model","741c00fa":"In the predictive part we will soley concentrate on prediciting the CO2 emissions per capita. We have seen above, that the KNN delivers great results for our split as well as for cross validation with scores above 90% and an RMSE below 1300. To further enhance our model we will first try different types of scaling before we try other types of models.","b7472827":"Again CO2\/GDP emission is higher in countries with cooler climate.","dbfb1f26":"<div id=\"join\"><\/div>\n\n### Common Processing and Joining DataFrames","2f64011a":"#### Other models: min-max Scaler","09f7d594":"#### CO2\/capita","44493393":"<div id=\"o_reg_cap\"><\/div>\n\n### Overview: Regression on CO2 per capita without scaling","45fe8ea9":"This plot analyses the co2 emissions from fossil fuel and industry over time. It can be seen, that ...  \nThe kink in the early 2000s is visible here as a significant sudden increase in the use of coal.","c07cafd9":"### Help functions","1ae87419":"### Outliers","639c1035":"# DOPP Exercise Group 24 \/ Question 19 Common Jupyter Notebook","6dd0b2e9":"#### CO2","767706fb":"When looking at acumulated CO2 emissions per capita, China it is not in the top ranks.","adc5cf61":"#### CO2 - polynomial","b3013d5c":"Civil Liberties and political rights have a less significant impact on CO2\/GDP emissions","ee2b30c7":"The simple KNN method delivers a pretty good model, that (also cross validated) covers around 90% of the variance and has quite a low RMSE. These values can probably be enhanced by some scaling, which is done later. Yet, the drawback, that is also mentioned earlier, of a KNN is its intransparency: Though it predicts the outcame very good, we don't know, which of the variables are important in this process. To obtain a more descriptive regression, linear models are very good, since they provide more information about the nature of the individual inputs. The first overview shows that although almost all variables are significant (except GDP) in explaining the CO2\/capita, the shares of the secondary and tertiary sector, import and export shares as well as HDI are of most importance (Lasso):\n\n* The share of the GDP per capita, the secondary sector share, the export and import shares, and the HDI have a positive effect on CO2_capita: Meaning that an increase in these inputs lead to a higher CO2\/capita.\n* Whereas the share of the tertiary sector has a negative effect on the output.\n\nThese findings are to some extent in accordance with literature and seem to be very plausible: The secondary sector includes all the manufactoring, constructing and energy intensive sectors, which produce a lot of CO2 - whereas the tertiary sector includes all the services, which traditionally have a low CO2 impact. More over it is well known, that GDP per capita leads (on average) to a higher emission of CO2\/capita due to an intensified production - although we will see, how at a certain point this trend is inverted, where a higher GDP\/capita leads to a drop in CO2 emissions per capita (due to a quadratic or cubic relationship). \n\nBut at this stage of our analysis there is no evidence for pollution haven hypothesis: industrialized and service based economies tend to outsource energy intensive productions, since these are mostly relient on ressources and labor, because they make up the largest share in value addedd. The cheapest options are most of the time developing countries with cheap labor force and ressources, but also with less stringent environmental regulations. So an economy that imports a lot tends to import a lot of CO2 intensive goods, whereas an economy that exports a lot tends to emit a lot of CO2. This is why in a political context not only place of production is of importance, but also place of consumption. Since it seems to be very bigot to point the finger at the producer while one actually consumes those products the other one is blamed for.\n\nYet, in our first model import as well as export shares have a positive sign, meaning they both have a positive effect on CO2 emissions per capita. Therefore in a later section we have to further establish these theses with PanelOLS).\n\nHDI has a positive effect on GDP\/capita, meaning a higher humanly developed country tends to emit more CO2 per capita.\n\nThese aforementioned results are to some extend mirrored in the regression on CO2_gdp: The sector and trading shares have the same signs (higher secondary share -> higher CO2 per GDP, whereas higher tertiary share -> lower CO2 per GDP), but GDP\/capit has a negative effect on the CO2\/GDP: meaning in econmocially more advanced countries (measured in GDP\/capita) the CO2 emission per Dollar of GDP declines. This result is quite plausible and can be interpreted easily: higher economically produtive countries tend to emit fewer CO2 per Dollar GDP produced: economic productivity goes hand in hand with environemtal productivity.\nIn the CO2\/GDP scenario we also find that temperature has a negative effect, but which is almost negligible.\n\nThe overall R squared of more then 50%, resp. 70% for the two full models is very good and shows that our selected variables can predict the outcome very well.","d0d73739":"About the data:\n - global data is taken from Global Carbon Project and represents all countries of the world\n - a pd.DataFrame that combines several datasets on countries and territories has been compiled. In order to achieve a high completeness over a time span of 1972-2016, several countries had to be removed.\n - a pd.DataFrame that is a reduced version of an Our-World-in-Data dataset that has additional information on consumption based CO2 and energy sources  \n\nThe representativeness of the latter two DataFrames is being discussed below.","e766569a":"For countries grouped by Civil Liberties the correlation with CO2 emission is not so significant","5a66c66f":"#### Load and modify data","57bedfd1":"As an addition, we made bins with the CO2\/Capita emissions as a criterium with the interest of determining the number of countries belonging to each group. The bins were made with the sustainable target of 3 tons per capita per year. The bin on the very left of each plot is the sustainable bin. It is visible that the vast majority of countries fall into it. These are predominately developing countries with smaller shares of industry in their economies. It is visible that the sustainable bin's count has decreased over time.","4aacb362":"<div id=\"data_preparation_script\"><\/div>\n\n## Data Preparation Script","29e9db7a":"The random split delivered better and more reliable results then the period (yearly) based approach. That is why this split is chosen throughout the rest of the course.","69a28f16":"<div id=\"compl\"><\/div>\n\n## Checking Completness of the Data","093d3def":"<div id=\"Descriptive_Statistics\"><\/div>\n\n# Descriptive Statistics","003bf6fb":"#### Short Colnames","44e24649":"#### Scatter Plots CO2","1124c711":"Descriptive statistics for relative variable values normalized to their mean","78fa309c":"#### Polynomials","f3de6f44":"### Data on Global Scale","76f54d6d":"CO2\/GDP emissions are particular high in countries with medium to high development stages, while it is remarkable low for the countries on the highest development stages.","79cf5d4b":"The grouping of these countries is uneven in terms of GDP and population. It is only even in the way, that each group has the same number of countries in it. The next figure shows the shares of these groups in terms of global population and global gdp. Here the focus is on the medium gdp\/capita group that shows an overproportional share on the global population while having a reasonable share of global GDP.","dca5a6ec":"<div id=\"iso3\"><\/div>\n\n### Common ISO3 and Country names","35f9aeb3":"Countries with low average temperatures have higher CO2\/capita emission. \nIt is assumed that this is due to the fact that most industrialized countries have cooler climate.","b77150ec":"The predictions of our model are really good with a RMSE of around 300 and a R square of almost 1. The graph shows, how well our model performs with very accurate predictions.","02e4c04b":"<div id=\"intro\"><\/div>\n\n## Introduction\n\nTo get a first good overview for the following regressions we should check for individual correlations between the variables at hand. This information can be gathered by a simple heatmap, which displays the correlations between variables. Since we have a lot of valid candidates it is good practice to pre filter our inputs accordingly. What is more a lot of our inputs are (by virtue of their definition alone) highly correlated and should therefore not be mixed in the regression, since they lead to interdependencies in our model.\n\nAs expected the absolute output of CO2 in a country is mostly driven by its economic size (and not its mere population): The correlation between CO2 and GDP is around 0.9. What is more, all the absolute variables from the National Accounts (from GDP to J-P) are highly (inter-)correlated, since they are mostly driven by the countries economic size, respectively its GDP. That is why all the economic variables are highly correlated with the output of CO2 as well. Since this does not come as a surprise and was already shown in our notebook, the regression part will mostly take a look at shares of the individual variables. So the goal is not only to validate the well established hypothesis, that the absolute CO2 output is mostly driven by a countries economic size and especially it's secondary sector, but to take a closer look what drives the CO2 per capita and per Dollar of GDP.\nFurthermore, cumulative CO2 (wheter absolute or as share) is quite unintuitiv to regress on, since in contrast to the regressors the dependent variable continously grows. Our tests also showed, that results are mostly insignificant, not plausible or show once again that the output is simply effected by economic related indicators. We therefore decided to limit our models on three outputs:\n\n* CO2\/capita\n* CO2\/gdp\n* CO2\n\nFor CO2 per capita our analysis will be most detailed, followed by total CO2 emissions. The analysis of absolute CO2 emissions will for the most part try to find the type of relationship between CO2 emissions and economic structure - since literature has shown that economic size (GDP\/etc.) does not only linearly correlate with CO2, but quadratically (Environmental Kutznet Curve (EKC)) or cubically (n-shaped).\n\nOn the input side we will also focuse on share rather than absolute values, since they are easier to interpret, better adoptable to a countries situation and not simply driven by the size of a country. E.g. instead of saying that a higher GDP leads to more CO2 output per capita, our goal is to find out which economical, political and sociological structure leads to a higher output in CO2 (per capita) - independently of a countries size.\n\nBy looking at the Heatmap we can identify several variables, which can function as valid regressors (variables which are also in accordance with literature):\nGDP_capita, Share Primary, Share Secondary, Import share, Export share, Av. Temp., Forest area, HDI, Education.\n\nWe will still include three absolute values to check if these have an effect on the CO2\/capita and CO2\/GDP, respectively:\nGDP, Population, Surface.\n\nThe regression part will be twofold:\n\nPredictive regression\n(1) Find the model that best predicts our defined outputs. \nYet, the drawback of such a model might be, that it is more or less a black box, meaning that it predicts the output very precise, but does not give any valid information about which variables are important in this process. Which means that we might end up with a very good predictive model (e.g. KNN\/Tree\/etc.), but still don't know, which variables fuel CO2 per capita.\n\nDescriptive regression\n(2) To meet this task, we try to obtain a descriptive regression model, that tells us, which variables are significant in explaining the output. We will start with a linear regression model to get a good overview of the importance of the variables, move forward to a polynomial regression to further enhance our model (also testing the widely known (and also heavily criticised inverted U-shape correlation between CO2 and gdp\/capita (environmental Kuznets curve): after countries reach a certain point of gdp per capita their env. footprint declines again). This polynomial model is then tested with a panelOLS, to see if the measured effects and importance\/significance of variables are only country and time specific or if they hold across countries throughout time.","2a59ac3d":"<div id=\"data_prep\"><\/div>\n\n# Data Preparation","81b50f92":"These plots compare the usual production based CO2 emissions witht the described consumption based CO2 emissions. This is based on the reduced dataset (see section on dataset description) over 58 countries, that account for 76% of global poppulation and 86% of global CO2 Emissions. In this dataset both China and India are in the lowest gdp per capita quantile.  \nIt can be seen that the lowest quantile shows somewhat lower emissions while the highest quantile shows higher emissions, when shifting to consumption based CO2.","9944d740":"To clean things a bit up and make results easier to interpret we came up with a slimmer and more intuitiv model (formula_co2_cap). This model still explains over half of the variance in CO2 per capita. All the variables have a significante effect on the output. Here we also see a complex relationship between the sector shares and GDP per capita:\n\n* The second share has at first a negative effect on GDP\/capita (probably if the tertiary sector is on the other hand very high), but is overruled at a certain point by its quadratic relationship, meaning that economies with a very high share of the secondary sector emit more CO2 per capita as well.\n* For the tertiary sector we find exactly the opposite case: At first it has a positive effect on CO2\/capita (corresponding with a very high share in the secondary sector), declines at a certain point (service based economies) and increases once again.","f636557f":"<div id=\"overview\"><\/div>\n\n## Overview","08146de3":"We then sort the countries into 5 bins based on their percentiles ranging from the 'Lowest' to the 'Highest' in each parameter and visualize the average value of each bin. The plots only created for the parameters that exhibited significant relationships with co2 emissions above.","6d0bdac7":"This stacked plot shows the sources (positive) and the sinks (negative) for CO2 on a global scale. Every emitted CO2 molecule is either absorbed by oceans or land area or it contributes to the atmospheric CO2, where it contributes to the greenhouse effect and global warming. The budged imbalance term is a measure of the inaccuracy of the data and smotthes the sinks to match the sources.\n\nIt can be seen, that every year there is significant and over time increasing atmospheric growth of CO2. It can also be seen, that while emissions due to land-use change (deforestation etc.) used to be a large part of the global co2-emissions in the 1960s, fossil fuel and industry has become the dominant driver in global CO2 emissions in modern day and age.  \nThere is also a visible kink in fossil fuel and industry emissions in the early 2000s, when the emissions start to increase at a higher rate. This kink will be examined in more detail.","7e988281":"<div id=\"na\"><\/div>\n\n### National Accounts","e8ff3524":"<div id=\"preprocessing_report\"><\/div>\n\n## Report on Preprocessing\nIn this section the result of the preprocessing is being presented. These topics will be discussed:\n- scope of the data (countries and timespan)\n- filling in missing data\n- outliers\n","4f27c91e":"Since scaling lowers the divergence between the model fitted with the train\/validation split and the cross validated one and therefore the risk of overfitting. We will proceed only with the scaled data. In the following part the RMSE and score are calculated for a handful models for both standard scaling and min-max scaling. These results can then be compared to our initial KNN-model.\n(All models have been optimized for their purpose.)","5c677cfe":"<div id=\"o_reg_gdp\"><\/div>\n\n### Overview: Regression on CO2 per Dollar GDP without scaling","eb1a8d2d":"Government expenditures on education have a decreasing impact over time on CO2\/GDP emissions.","f8558298":"<div id=\"helper_plotting\"><\/div>\n\n## Plotting","94902a14":"These scatter plots (with different scopes) clearly depict the inverted U relationship between absolute CO2 emissions and GDP, that is widely discussed in literature under the name of Environmental Kutznet Curve: CO2 emissions increase parallel with GDP to a certain point, from which on the tend to fall again with an increasing GDP. ","f7239998":"<div id=\"etr\"><\/div>\n\n### Extra Trees Regressor","40873b49":"Descriptive statistics for absolute variable values normalized to their mean","47cac685":"Show Distribution of all Variables in Boxploats.\nNote: Absolute values are normalized to Mean.","eeea0e79":"<div id=\"drop\"><\/div>\n\n## Analysing Dropped Countries","664a3522":"<div id=\"polit\"><\/div>\n\n### Political Data","1b07ff91":"<div id=\"poly_rel\"><\/div>\n\n### Polynomial Relationships","9da5a1a4":"<div id=\"helper_general\"><\/div>\n\n## General","25526a47":"To have a good overview at the beginning we constructed a function that runs several types of regressions: KNN, linear with all the given inputs, Lasso and a linear with a reduced set of variables, that we obtain through the lasso regression.","02e469a0":"## Calc Descriptive statistics \n\n","b724152f":"### CO2 Emissions per Capita\nThe above figures do not take into account the different populations of the country bins. So the comparison of total CO2 emissions could be considered biased, because the lowest two of five quantiles seem to account for roughly two thirds of world population. The picture shifts, if CO2 emissions per capita are being considered.","210405c5":"In these plots countries are grouped in five categories by their GDP\/capita in 2014. The left plot shows once again, that the overall growth of CO2 emissions has a kink in the early 2000s. In the right plot becomes obvious, that this kink is due to countries in the medium gdp\/capita category.","8ffc52ce":"<div id=\"o_reg_st\"><\/div>\n\n### Overview: Regression on CO2 per capita with standard scaling","bd9fe0fe":"<div id=\"others\"><\/div>\n\n### Other models","0fba5c87":"### Plot time evolution of baskets CO2\/GDP","8e0fc5b7":"The results clearly show, that neither with the scaled nor the unscaled data predictions with our linear model are very accurate. The RMSE for the simple KNN is around only a third and the score is almost twice as large. \nThe KNN with the standard scaling (zero mean, variance of one) delivers the most promising results: Though the RMSE for our train\/val-split is a bit higher compared to the unscaled KNN-model, the cross validated results better align with the initial model, meaning it does not really overfit.","e408d4a2":"As seen above, in the bar plots on the left, HDI and GDP\/Capita are strongly correlated to the co2\/capita emissions. The bottom right plot indicate the same connection since the countries with the smallest share of primary sector are the highly developed ones. The plot in the top right corner shows an interesting link between the dominant emitters and the reduction. In other words - highly developed and developed countries which are cumulatively the largest emitters are in the process of transition to less co2 intensive economies. Next, we shift our attention to the co2 emitted per dollar GDP. The most significant correlations are visualized in the four barplots below. The top left plot shows that the countries with the highest average temperature change have significantly more co2 inefficient economy. Additionally, the top right plot shows the co2 inefficiency of the countries with the coolest climates. However, it is hard to make statements based on this as the lowest AverageTemperature basket is composed, with a couple of exceptions, of developed western world. Looking at the bottom left plot, there are indications of a connection between the civil liberties and the co2 economy efficiency. Namely, the countries with the lowest civil liberties are significantly more co2 inefficient. This could be due to a slower adoption of modern technologies in these less democratic countries. Lastly, the bottom-right figure, similarly to the co2\/Capita analyisis above, points to the trend that the most co2 inefficient economies are the ones that show the smallest increase of co2\/GDP. This seems like a positive trend, but by looking into the countries belonging to this bin , the fact of the matter is that the energy sectors of these countries are consistently fossil fuel based, making little room for becoming more inefficient.","d6d2024f":"<div id=\"pred\"><\/div>\n\n## Predictive Model","1a3ae64a":"### Filling in missing data\n\n**Data on global carbon sources and sinks**\nThis dataset was complete and nothing had to be filled in.\n\n**Data on carbon emissions by country**\nPopulation and CO2 emissions of the remaining countries and territories were almost complete only for few countries the data either started only in 1973 or 1974 or ended in 2015. These few years were filled in by linear extrapolation. \n\n**National Accounts**\n...please fill in...\n\n**Political Regime**\n\nThe Input Data for political Regime ( Political Rights, Civil Liberties and Status Free, Partly Free or not free) is bound to the existence of a certain state. \n\nChanges in existence of states and their political properties:\n\nDuring the observation periode states like Soviet Union and Yugoslavia have been split to several states, while Germany has been united again after end of cold war.\nThe assignment of the political data is on the exisiting states today. Following this approach the existence of the fromer Soviet Republics is filled back to the years when they were part of the Soviet Union with parameter sets for single political party communist regime.\nFor Germany the values from Western Germany are filled back, as the majority of people lived in the former BRD.\n\nReporting interval for Political data is once per year, with some changes to 1.75 in 1981 and then to 1.25 years in 1983. For these speacial years the year 1981 data is duplicated to 1982. \n\n**Education**\n...please fill in...\n\n**Temperature by Country**\n\nTemperature Data was complete form 1972 up to 2013. As Economic and other data for this analyiss was given up to 2016, the average temperature from 2009 to 2013 for each country was copied to the years 2014 to 2016 for each country to be able to perform the analysis up to 2016.\n\n\n"}}