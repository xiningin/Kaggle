{"cell_type":{"f50e3f62":"code","bcfb318b":"code","0d2a14b1":"code","4ab966f8":"code","89c55e08":"code","525236a9":"code","1a6adecc":"code","bfdcaceb":"code","bc261ff9":"code","3e6cf2b2":"code","f5fa3307":"code","b30e6696":"code","663c9787":"code","984fadd7":"code","605ccce0":"code","046c9bee":"code","befcd7a4":"code","9d4d56f3":"code","fcd336c0":"code","23cd5a1e":"markdown","a863f41e":"markdown","16adc708":"markdown","641ed7e7":"markdown","2e33640e":"markdown","534f9530":"markdown","1cae2e3a":"markdown","f166b430":"markdown","65c87e61":"markdown","09d36690":"markdown","4fdf4a8d":"markdown","fc0fdce7":"markdown","dc586412":"markdown","bfbb0943":"markdown","23e65579":"markdown","bdd6bf70":"markdown","950e8e0a":"markdown","988392e2":"markdown","6489eed7":"markdown","58364f1d":"markdown","ce799adf":"markdown","b8bf9c7d":"markdown","6dfb887b":"markdown","3f9fe078":"markdown","da7d7fca":"markdown","f8d2d989":"markdown"},"source":{"f50e3f62":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dropout, Flatten ,BatchNormalization , MaxPool2D\nfrom keras.layers.convolutional import Conv2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nimport os","bcfb318b":"ds_train=pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\nds_test=pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","0d2a14b1":"print(\"Train dataset :\",ds_train.shape)\n#print(\"Test dataset :\",ds_test.shape)","4ab966f8":"X=ds_train.drop(['label'],axis=1)\ny=ds_train['label']\nprint(\"done!\")","89c55e08":"sns.countplot(y)","525236a9":"X=X\/255.0\nds_test=ds_test\/255.0","1a6adecc":"X = X.values.reshape(-1,28,28,1)\nds_test = ds_test.values.reshape(-1,28,28,1)","bfdcaceb":"fig,axs=plt.subplots(1,5,figsize=(20,5))\nfig.tight_layout()\n\nfor i in range(5):\n    axs[i].imshow(X[i].reshape(28,28))\n    axs[i].axis('off')\n    axs[i].set_title(y[i])\nplt.show()","bc261ff9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.12, random_state=4)","3e6cf2b2":"len(X_train) #we will set batch size to 56 and step per epouch to 660","f5fa3307":"dataGen= ImageDataGenerator(width_shift_range=0.1,   \n                            height_shift_range=0.1,\n                            zoom_range=0.2,  \n                            shear_range=0.1, \n                            rotation_range=10)  \ndataGen.fit(X_train)","b30e6696":"batches= dataGen.flow(X_train,y_train,batch_size=20)\nX_batch,y_batch = next(batches)","663c9787":"fig,axs=plt.subplots(1,5,figsize=(20,5))\nfig.tight_layout()\n\nfor i in range(5):\n    axs[i].imshow(X_batch[i].reshape(28,28))\n    axs[i].axis('off')\n    axs[i].set_title(y_batch[i])\nplt.show()","984fadd7":"y_train = to_categorical(y_train,10)\ny_test = to_categorical(y_test,10)","605ccce0":"model = Sequential()\n\n#First\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3) ,activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 56, kernel_size = (3,3),activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\n#Second\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),activation ='relu'))\nmodel.add(Conv2D(filters = 48, kernel_size = (3,3),activation ='relu'))\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3),activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\n#Third\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dense(128, activation = \"relu\"))\nmodel.add(Dense(64, activation = \"relu\"))\nmodel.add(Dropout(0.4))\n\n#Output\nmodel.add(Dense(10, activation = \"softmax\"))\n\n\nmodel.compile(Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])","046c9bee":"print(model.summary())","befcd7a4":"#you can set more than 10 epochs to get more accuracy \nhistory = model.fit_generator(dataGen.flow(X_train,y_train, batch_size=56),\n                              epochs = 10, validation_data = (X_test,y_test),\n                              verbose = 2, steps_per_epoch=660)\n\n\n# For 10 epochs we get \n#  loss: 0.0614 \n#  accuracy: 0.9838 \n#  val_loss: 0.0364 \n#  val_accuracy: 0.9921","9d4d56f3":"plt.figure()\nfig,(ax1, ax2)=plt.subplots(1,2,figsize=(19,7))\nax1.plot(history.history['loss'])\nax1.plot(history.history['val_loss'])\nax1.legend(['training','validation'])\nax1.set_title('loss')\nax1.set_xlabel('epoch')\n\nax2.plot(history.history['accuracy'])\nax2.plot(history.history['val_accuracy'])\nax2.legend(['training','validation'])\nax2.set_title('Acurracy')\nax2.set_xlabel('epoch')\n\n\n\nscore =model.evaluate(X_test,y_test,verbose=0)\nprint('Test Score:',score[0])\nprint('Test Accuracy:',score[1])","fcd336c0":"results = model.predict(ds_test)\n\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"CNN_Digit_Recognizer.csv\",index=False)","23cd5a1e":"## 2.6 Split to train and test","a863f41e":"## 4.2 Build the Network ","16adc708":"# 4. CNN","641ed7e7":"Everything look fine !","2e33640e":"## 2.5 check the Dataset","534f9530":"As you can see we have the count of all label are almost in the save interval [3500,5000], that's good for the training, for example if we had just 1000 samples of label 1 that will be a problem , the model will find difficulties to detect label 1\"less accuracy \", so that's not going to happend everything look fine .","1cae2e3a":"## 2.3 Normalization","f166b430":"Cool!!","65c87e61":"From 1D vectors to 28\u00d728\u00d71 (3D)","09d36690":"# 5. Let's Predict","4fdf4a8d":"# 2. Data Preparation\n## 2.1 Import libraries","fc0fdce7":"As you can see when we only used 10 epoch the model look great and we tried to avoid Overfitting and Underfitting","dc586412":"## 2.2 Load Data","bfbb0943":"# 3. Data Augmentation","23e65579":"We will use data augmentation is called in-place data augmentation, the process is input batch of images to the IDG(ImageDataGenerator), and transforms each image in the batch using random translation (given parameter),at the end the transformed batch returned to the calling function.\nThe idea here is to avoid Over-fitting , we won't let the model train with the same pictures for example the model should train using different pictures of label 1.","bdd6bf70":"Before we move forward we need to check our dataset first if there anything wrong and the label are correct.","950e8e0a":"![](http:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2019\/07\/keras_data_augmentation_in_place.png)","988392e2":"* **1. Introduction**   \n* **2. Data Preparation**\n   * 2.1 Import Libaries\n   * 2.2 Load Data\n   * 2.3 Normalization\n   * 2.4 Reshape\n   * 2.5 Check the Data\n   * 2.6 Split to train and test\n* **3. Data Augmentation**\n* **4. CNN**\n   * 4.1 Label encoding\n   * 4.2 Build the Network\n   * 4.3 Evaluate the Model\n* **5. Let's Predict**","6489eed7":"# 1. Introduction\n![](https:\/\/miro.medium.com\/max\/3744\/1*SGPGG7oeSvVlV5sOSQ2iZw.png)\n\nA Convolutional Neural Network (ConvNet\/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects\/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters\/characteristics.\n","58364f1d":"## 4.3 Evaluate The Model","ce799adf":"### After Data Augmentation","b8bf9c7d":"## 2.4 Reshape","6dfb887b":"## 4.1 Label encoding","3f9fe078":"We will use Grayscale normalization the model will work faster if the interval of data between [0,1] instead of [0,255]","da7d7fca":"###  If you like my work, please hit upvote since it will keep me motivated","f8d2d989":"### Before the trip begin\n\n#####  If you like my work, please hit upvote since it will keep me motivated"}}