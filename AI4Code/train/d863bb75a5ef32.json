{"cell_type":{"eb28586e":"code","b4ba4870":"code","c9d06ec6":"code","ddcf38d4":"code","fa651745":"code","796fefa7":"code","7a31ae14":"code","4f134e0b":"code","dab97109":"code","71d4c05f":"code","ad338b03":"code","7cd26cc8":"code","9eccc4f7":"code","fabe2aaa":"code","f9f2a763":"code","639c5b10":"code","bb1f9db1":"code","5594e070":"code","586a1803":"markdown"},"source":{"eb28586e":"#Importing the libraries\nimport numpy as np\nimport pandas as pd\nfrom fastai.text.all import *\nimport re","b4ba4870":"# Loading the training and testing data into dataframes\ndir_path = \"\/kaggle\/input\/nlp-getting-started\/\"\ntrain_df = pd.read_csv(dir_path + \"train.csv\")\ntest_df = pd.read_csv(dir_path + \"test.csv\")","c9d06ec6":"# Keep only the text and target columnds\ntraind_df = train_df.drop(columns = [\"id\", \"keyword\", \"location\"])\ntraind_df[\"target\"].value_counts()","ddcf38d4":"# Cleaning the text data: removing URLs, html code and emoji\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_URL)\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_html)\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_emoji)\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_URL)\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_html)\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_emoji)","fa651745":"train_df[\"text\"].apply(lambda x:len(x.split())).plot(kind=\"hist\")","796fefa7":"from transformers import AutoTokenizer, AutoModelForSequenceClassification","7a31ae14":"# Instantiate a tokenizer based on the Bert case sensitive model \ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")","4f134e0b":"# Convert text sequences to numerical tokens (vector of numbers which can be fed into the model)\n\ntrain_tensor = tokenizer(list(train_df[\"text\"]), padding=\"max_length\", \n                         truncation=True, max_length=30, \n                         return_tensors=\"pt\")[\"input_ids\"]","dab97109":"# Create a custom class to prepare the training data to be \n# in model input form (tuple of tokenized text sequence and tensor of target)\n\nclass TweetDataset:\n    def __init__(self, tensors, targ, ids):\n        self.text = tensors[ids]\n        self.targ = targ[ids].reset_index(drop=True)\n    \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, idx):\n        t = self.text[idx]\n        y = self.targ[idx]\n        return t, tensor(y)","71d4c05f":"# Split the data in a trainig and validation set\ntrain_ids, valid_ids = RandomSplitter()(train_df)\n\n# Separate the y \/ target into a variable\ntarget = train_df[\"target\"]\n\n# create the input dataset based on the randomsplitter ids and utiliing the pre-processed tokens \ntrain_ds = TweetDataset(train_tensor, target, train_ids)\nvalid_ds = TweetDataset(train_tensor, target, valid_ids)\n\ntrain_dl = DataLoader(train_ds, bs=64)\nvalid_dl = DataLoader(valid_ds, bs=512)\n\ndls = DataLoaders(train_dl, valid_dl).to(\"cuda\")","ad338b03":"# Instantiate model object (Bert model with classification output)\n\nbert = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\").train().to(\"cuda\")\n\nclassifier = nn.Sequential(\n    nn.Linear(768, 300),\n    nn.ReLU(),\n    nn.BatchNorm1d(300),\n    nn.Dropout(0.5),\n    nn.Linear(300, 2)\n)\n\nbert.classifier = classifier\n\nclass BertClassifier(Module):\n    def __init__(self, bert):\n        self.bert = bert\n    def forward(self, x):\n        return self.bert(x).logits\n\nmodel = BertClassifier(bert)\n    ","7cd26cc8":"# Set up the fastai learner using the model insantiated in previous step and find optimal learning rate\n\nlearn = Learner(dls, model, loss_func=nn.CrossEntropyLoss(), metrics=[accuracy, F1Score()])\nlearn.lr_find()","9eccc4f7":"# Fit the model\nlearn.fit_one_cycle(4, lr_max=5e-5, wd=0.8)","fabe2aaa":"# Check f1 scores if threshold for for the probability of the logit \n# with the highest probability is above a minimum threshold \n# (if logits is below this threshold, I believe the prediction is zero: \"not a disaster tweet\") \n\nfrom sklearn.metrics import f1_score\n\npreds, targs = learn.get_preds()\n\nmin_threshold = None\nmax_f1 = -float(\"inf\")\nthresholds = np.linspace(0.3, 0.7, 50)\nfor threshold in thresholds:\n    f1 = f1_score(targs, F.softmax(preds, dim=1)[:, 1]>threshold)\n    if f1 > max_f1:\n        min_threshold = threshold\n        min_f1 = f1\n    print(f\"thresholds:{threshold:.4f} - f1:{f1:.4f}\")","f9f2a763":"# Convert text sequences to numerical tokens (vector of numbers which can be fed into the model)\n\ntest_tensor = tokenizer(list(test_df[\"text\"]),\n                        padding=\"max_length\",\n                        truncation=True,\n                        max_length=30,\n                        return_tensors=\"pt\")[\"input_ids\"]","639c5b10":"# Create a custom class to prepare the test data in the input form required for the model: a tuple\n# of text sequence and a tensor of zero\n\nclass TestDS:\n    def __init__(self, tensors):\n        self.tensors = tensors\n    \n    def __len__(self):\n        return len(self.tensors)\n    \n    def __getitem__(self, idx):\n        t = self.tensors[idx]\n        return t, tensor(0)\n\ntest_dl = DataLoader(TestDS(test_tensor), bs=128)","bb1f9db1":"# Get test prediction from the learner model\n\ntest_preds = learn.get_preds(dl=test_dl)","5594e070":"# Submit output to Kaggle\nprediction = (F.softmax(test_preds[0], dim=1)[:, 1]>min_threshold).int()\nsub = pd.read_csv(dir_path + \"sample_submission.csv\")\nsub[\"target\"] = prediction\nsub.to_csv(\"submission.csv\", index=False)","586a1803":"This notebook is a code-along note book that follows the following notebook by Pei-Yi Hong with additional comments to clarify each step for beginner learners like me :-)\n\nhttps:\/\/www.kaggle.com\/hongpeiyi\/bert-with-pytorch-and-fastai "}}