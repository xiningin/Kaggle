{"cell_type":{"416fb5b2":"code","2dcd5743":"code","95c2b511":"code","b4925c4e":"code","2aec56c2":"code","d2bba08e":"code","87ba6982":"code","c1587d05":"code","1f015229":"code","bfec278e":"code","26a1c4ae":"code","6c0f2357":"code","d4c3b53f":"code","5bc65c22":"code","7590c1bc":"code","3a74a4e5":"code","6be9edf3":"code","d86caf82":"code","a69a52ac":"code","4d54b25c":"code","45726f8f":"code","48572122":"code","c49ce025":"code","89abb4e0":"code","886f52f2":"code","0a0e49c1":"code","92c382d8":"code","0b2b9a5a":"code","6e21a75e":"code","491d7fd6":"code","67b2aa0b":"code","225fe6de":"code","d3086a6f":"code","0f9f413f":"code","2d03c78d":"code","96a8bb66":"code","9c3f1861":"code","7bf059eb":"code","25e74ca0":"markdown","b5cb7982":"markdown","f24f16db":"markdown","43f9a5c8":"markdown","6cd55fa7":"markdown","4483aef1":"markdown","af132723":"markdown","daefdc5d":"markdown","67acaf41":"markdown","c4b2a73f":"markdown","bf194875":"markdown","d8085fd3":"markdown"},"source":{"416fb5b2":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n\nimport xgboost\n\nfrom scipy.stats import zscore, pearsonr\n\nfrom joblib import dump","2dcd5743":"# Pandas config\ndef pandas_config():\n    # display 10 rows and all the columns\n    pd.set_option('display.max_rows', 10)\n    pd.set_option('display.max_columns', None)\n    \npandas_config()","95c2b511":"# Loading the dataset\nfile_path = '\/kaggle\/input\/housesalesprediction\/kc_house_data.csv'\ndf = pd.read_csv(file_path)\ndf.sample(5)","b4925c4e":"df.info()","2aec56c2":"# Drop df column\ndef drop_df_column(df, column_name, inplace=True):\n    return df.drop([column_name], axis='columns', inplace=inplace)","d2bba08e":"drop_df_column(df, 'id')","87ba6982":"print(df.select_dtypes('object').columns.tolist())\ndf.drop(['date'], axis='columns', inplace=True)","c1587d05":"def plot_corr(df, figsize=(16, 12)):\n    # the `corr` method uses pearson correaltion\n    corr = df.corr()\n    \n    _, ax = plt.subplots(1, 1, figsize=figsize)\n    g = sns.heatmap(corr, ax=ax, annot=True, cmap=sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True))\n\n    for _ax in g.get_xticklabels():\n        _ax.set_rotation(75)\n    \n    \nplot_corr(df)","1f015229":"# Get columns names in as sentence (instead of getting long list \n# of column names)\ndef get_column_names(df):\n    for column_name in df.columns.tolist():\n        print(f'{column_name} | ', end='')","bfec278e":"get_column_names(df)","26a1c4ae":"def plot_base_relation(df, figsize=(20, 200)):\n    columns = df.columns.tolist()\n    _, axs = plt.subplots(len(columns), 4, figsize=figsize)\n    \n    for idx, column in enumerate(columns):\n        # To get distribution of data\n        sns.histplot(\n            x=df[column],\n            kde=False,\n            color='#65b87b', alpha=.7,\n            ax=axs[idx][0]\n        )\n\n        # To get knowledge about outliers\n        sns.boxplot(\n            x=df[column],\n            color='#6fb9bd',\n            ax=axs[idx][1]\n        )\n\n        # To get its realtion with price\n        sns.scatterplot(\n            x=column, y='price', data=df,\n            color='#706dbd', alpha=.7, s=80,\n            ax=axs[idx][2]\n        )\n        \n        # To get count plot for `column`\n        sns.countplot(\n            x=column, data=df,\n            color='#42b0f5', alpha=.7,\n            ax=axs[idx][3]\n        )\n        \n        \nplot_base_relation(df, figsize=(20, 70))","6c0f2357":"# Removing outliers using IQR method\ndef rm_outliers_in_col_using_iqr(df, col):\n    # col here is df.column_name (or df[column_name])\n\n    Q1 = col.quantile(0.25)\n    Q3 = col.quantile(0.75)\n    IQR = Q3 - Q1\n\n    outliers_row_idx = col.loc[\n        (col < (Q1 - 1.5 * IQR)) | (col > (Q3 + 1.5 * IQR))\n    ].index.tolist()\n\n    df = df.drop(outliers_row_idx, axis='rows')\n    return (outliers_row_idx, df)\n\n\n# Removing outliers using the Zscore method\ndef rm_outliers_in_col_using_zscore(df, col, column_name):\n    # col here is df.column_name (or df[column_name])\n    \n    zscores_df = pd.DataFrame({\n        f'{column_name}': col.to_numpy()\n    }, df.index.tolist())\n    \n    zscores_df['zscores'] = zscores_df.apply(lambda x: zscore(x))\n    outliers_row_idx = zscores_df[np.abs(zscores_df.zscores) > 3].index.tolist()\n\n    df = df.drop(outliers_row_idx, axis='rows')\n    return (outliers_row_idx, df)\n\n\n# Remove outliers of a column using iqr & zscore methods\ndef remove_outliers_of_a_column(df, column_name):\n    rm_idxs = []\n    for _ in range(10):\n        outliers_row_idx, df = rm_outliers_in_col_using_iqr(df, df[column_name])\n        rm_idxs.extend(outliers_row_idx)\n        \n        outliers_row_idx, df = rm_outliers_in_col_using_zscore(df, df[column_name], column_name)\n        rm_idxs.extend(outliers_row_idx)\n    return rm_idxs, df\n\n\n# Remove outliers of a df using iqr & zscore methods\ndef remove_outliers_of_df(df):\n    rm_rows_idxs = []\n    for column in df.columns.tolist():\n        if column == 'price':\n            # As we don't want to do anything with `price`\n            continue\n            \n        rm_idxs, df = remove_outliers_of_a_column(df, column)\n        rm_rows_idxs.extend(rm_idxs)\n    return rm_rows_idxs, df","d4c3b53f":"'''\n    If you removed all the outliers in `continuous_df` then only 1\/10th of the data is remaining.\n    So instead of going through all the columns in df at once, we will go through each column at \n    a time and if the columns has 2% of outliners then we drop that column. Keeping the threshold \n    as 2% because if keep threshold higher then collectivetly a lot of rows will drop which in turn\n    reduces our dataset \n    \n    eg. if threshold is 20% then column1 (if there are 15% outliers then remove the rows) then \n    column2 (if there are 10% outliers then remove the rows) so in total we end up dropping \n    15% + 10% = 30% of our rows.\n    \n    So to avoid this we are keeping threshold as 2%\n'''\n\n# To do the above thing we can just modify the `remove_outliers_of_df` func\ndef remove_outliers_of_df_with_threshold(df, threshold=2):\n    # `threshold` here is the percent above which the entire \n    # column will be dropped \n\n    rm_rows_idxs = []\n    for column in df.columns.tolist():\n        if column == 'price':\n            # As we don't want to do anything with `price`\n            continue\n            \n        rm_idxs, tmp_df = remove_outliers_of_a_column(df, column)\n\n        if round(len(rm_idxs) \/ len(df), 2) * 100 > threshold:\n            drop_df_column(df, column)\n        else:\n            df = tmp_df.copy()\n            del tmp_df\n            rm_rows_idxs.extend(rm_idxs)\n   \n    return rm_rows_idxs, df","5bc65c22":"print(f'Dataset size before removing outliers: {len(df)}')\n\nwith np.errstate(divide='ignore', invalid='ignore'):\n    RM_ROWS_IDXS, df = remove_outliers_of_df_with_threshold(df, threshold=4)\n\nprint(f'Dataset size after removing outliers: {len(df)}')","7590c1bc":"print(f'{len(RM_ROWS_IDXS)} columns are dropped while removing outliers')","3a74a4e5":"plot_base_relation(df, (20, 38))","6be9edf3":"# Remove columns which have only one unique value as they won't be useful\ndrop_df_column(df, 'waterfront')","d86caf82":"plot_corr(df)","a69a52ac":"drop_df_column(df, 'yr_renovated')\ndrop_df_column(df, 'zipcode')\ndrop_df_column(df, 'lat')\ndrop_df_column(df, 'long')","4d54b25c":"df.head()","45726f8f":"def plot_scatterplot(x, y, ax=None):\n    sns.scatterplot(\n        x=x, y=y,\n        color='#706dbd', alpha=.7, s=80,\n        ax=ax\n    )\n    \n    \ndef plot_boxplot(x, ax=None):\n    sns.boxplot(x=x, color='#6fb9bd', ax=ax)\n    \n    \ndef plot_barplot(x, y, ax=None):\n    sns.barplot(x=x, y=y, data=df, palette='rocket', ax=ax)","48572122":"_, ax = plt.subplots(2, 2, figsize=(16, 8))\n\n\nplot_barplot(df.bedrooms, df.price, ax=ax[0][0])\nplot_barplot(df.condition, df.price, ax=ax[0][1])\nplot_barplot(df.bathrooms, df.price, ax=ax[1][0])\nplot_barplot(df.floors, df.price, ax=ax[1][1])","c49ce025":"_, ax = plt.subplots(2, 3, figsize=(16, 8))\n\nplot_scatterplot(df.sqft_living, df.price, ax=ax[0][0])\nplot_scatterplot(df.sqft_above, df.price, ax=ax[0][1])\nplot_scatterplot(df.sqft_basement, df.price, ax=ax[0][2])\nplot_scatterplot(df.yr_built, df.price, ax=ax[1][0])\nplot_scatterplot(df.sqft_living15, df.price, ax=ax[1][1])","89abb4e0":"plot_corr(df, figsize=(14, 8))","886f52f2":"drop_df_column(df, 'sqft_above')\ndrop_df_column(df, 'sqft_living15')","0a0e49c1":"tmp_df = df[['yr_built', 'price']].sort_values(by=['yr_built'])\n\ngroup = tmp_df.groupby(['yr_built'])['price'].mean()\navg_price_of_the_year = [avg_price for avg_price in group]\n\nplt.plot(tmp_df.yr_built.unique(), avg_price_of_the_year, linestyle='solid')\nplt.xticks(rotation=16)","92c382d8":"# Scaling int & float dtype column\ndef standard_scaler(column):\n    # Bumping up the ndim by np.newaxis as column.values is 1D & fit_transform needs 2D\n    return StandardScaler().fit_transform(column.values[:, np.newaxis])\n\n\n# Scaling all int & float dtype columns \ndef scaling_df(df):\n    # Selecting columns which have number dtype\n    numbers_df = df.select_dtypes(include=[np.int64, np.float64])\n\n    for column_name in numbers_df.columns.tolist():\n        df[column_name] = standard_scaler(df[column_name])\n    return df\n\n\n# Scaling `continuous_df` for EDA \nscaling_df(df)\n\ndf.sample(5)","0b2b9a5a":"columns = df.columns.tolist()\ncolumns.remove('price')\n\nx = df[columns]\ny = df['price']","6e21a75e":"# Splitting the dataset\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=6)","491d7fd6":"# Cross validation\n\nkf = KFold(n_splits=10)\n\nscore = cross_val_score(Ridge(), x_train, y_train, cv=kf)\nprint(score.mean())\n\npr = PolynomialFeatures(degree=4)\nx_train_pr = pr.fit_transform(x_train)\nx_test_pr = pr.fit_transform(x_test)\n\nscore = cross_val_score(Ridge(), x_train_pr, y_train, cv=kf)\nprint(score.mean())","67b2aa0b":"# Using XGBoost\n\nxgb = xgboost.XGBRegressor()\ntry:\n    xgb.fit(x_train_pr, y_train)\nexcept KeyError:\n    pass","225fe6de":"# Predictions\nxgb_y_test_pred = xgb.predict(x_test_pr)","d3086a6f":"rms_error = mean_squared_error(y_test, xgb_y_test_pred, squared=False)\nr2_score_value = r2_score(y_test, xgb_y_test_pred)\n\nprint(f\"Root mean squared error: {rms_error}\")\nprint(f\"R2-score: {r2_score_value}\")","0f9f413f":"# Creating a pipeline\n\nscaling = ('scale', StandardScaler())\nploy = ('ploy', PolynomialFeatures(degree=4))\nmodel = ('model', xgboost.XGBRegressor())\n\n# Steps in the pipeline\nsteps = [scaling, ploy, model]\n\npipe = Pipeline(steps=steps)\n\n# Fiitting the model\nmodel = pipe.fit(x_train, y_train)\n\n# Out-Of-Sample Forecast\ny_test_pred = model.predict(x_test)\n\n# Evaluation\nrms_error = mean_squared_error(y_test, y_test_pred, squared=False)\nr2_score_value = r2_score(y_test, y_test_pred)\n\nprint(f\"Root mean squared error: {rms_error}\")\nprint(f\"R2-score: {r2_score_value}\")","2d03c78d":"# Saving the model\ndump(model, 'model.joblib')","96a8bb66":"f, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\n\nax1.plot(np.arange(len(y_test)), y_test, label='Actual')\nax2.plot(np.arange(len(y_test_pred)), y_test_pred, label='Prediction')\n\nax1.legend()\nax2.legend()\n\nf, ax3 = plt.subplots(nrows=1, ncols=1, figsize=(20, 6))\n\nax3.plot(np.arange(len(y_test)), y_test, label='Actual')\nax3.plot(np.arange(len(y_test_pred)), y_test_pred, label='Prediction')\n\nax3.legend()","9c3f1861":"def plot_result(start, end):\n    f, ax3 = plt.subplots(nrows=1, ncols=1, figsize=(13, 5))\n\n    ax3.plot(np.arange(len(y_test[start:end+1])), y_test[start:end+1], label='Actual')\n    ax3.plot(np.arange(len(y_test_pred[start:end+1])), y_test_pred[start:end+1], label='Prediction')\n\n    ax3.set_title(f'{start} - {end}')\n    ax3.legend()","7bf059eb":"for i in range(0, 4480, 100):\n    start = i\n    end = start + 100\n    plot_result(start, end)","25e74ca0":"### Visualizing entire prediction vs actual value","b5cb7982":"`sqft_above` has strong positive correlation with `sqft_living` and moderate positive correlation with `sqft_living15` and `sqft_living` has positive correlation with `sqft_living15`. In short there is `multi-collinearity` issue here, so dropping any 2 columns out of 3. ","f24f16db":"## Modelling","43f9a5c8":"# House Sales in King Country, USA\n\nHere [House Sales in King County, USA](https:\/\/www.kaggle.com\/harlfoxem\/housesalesprediction) dataset by [harlfoxem](https:\/\/www.kaggle.com\/harlfoxem) is used to perform `EDA` on housing prices and creating `machine lerning model` to predict house prices.\n\n**About data source**: This dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between `May 2014 and May 2015`.\n\n![](https:\/\/media.giphy.com\/media\/3o6Mba1qerHR51rl9C\/giphy.gif)","6cd55fa7":"### Dealing with outliers\n\nA lot of columns have issue of outliers. Using `IQR` & `Zscores` method to deal with it.","4483aef1":"## Evaluation","af132723":"No missing data","daefdc5d":"## Data preparation","67acaf41":"### Visualizing prediction vs actual values in interval of 100","c4b2a73f":"## Visualizing our prediction against actual values","bf194875":"---\n\nI'll wrap things up there. If you want to find some other answers then go ahead `edit` this kernel. If you have any `questions` then do let me know.\n\nIf this kernel helped you then don't forget to \ud83d\udd3c `upvote` and share your \ud83c\udf99 `feedback` on improvements of the kernel.\n\n![](https:\/\/media.giphy.com\/media\/cp7bUxkodNBHW\/giphy.gif)\n\n---","d8085fd3":"## Exploratory Data Analysis"}}