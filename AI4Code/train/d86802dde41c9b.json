{"cell_type":{"8c5af8c9":"code","22c5fe12":"code","cd8bf8bd":"code","4174605a":"code","112a4b11":"code","108c9917":"code","b9c37984":"code","8a150801":"code","fb4a1f72":"code","392f953b":"code","3b4c028c":"code","31bb7069":"code","f0ec216c":"code","9a68f862":"code","48215cc7":"code","32a85ac1":"code","5c4e244a":"code","58ea1ba3":"code","e8215b29":"code","21cb0087":"code","5db34e79":"code","7dbff3ee":"code","db54b790":"code","b67c4dbe":"code","f63cf7e5":"code","11b91526":"code","66608d0c":"code","3c8dc24c":"code","04c82f18":"code","727056ce":"markdown","a0b5a4e4":"markdown","450984c5":"markdown","b3f27464":"markdown","b0b06477":"markdown","37214a74":"markdown","facdf359":"markdown","3b6c31d9":"markdown","76c4e518":"markdown","a083aae6":"markdown","0bb6eda8":"markdown","cf7ffb89":"markdown","4dce0cc1":"markdown","a431a061":"markdown","fcfe274f":"markdown","0bee6b82":"markdown","2bb0ce03":"markdown","d25a93b3":"markdown","277c836a":"markdown","d9525b1b":"markdown","9f04946b":"markdown","c45b5a6f":"markdown","183daf09":"markdown","130b1e4d":"markdown","53d5c449":"markdown","8e8066e5":"markdown","fa44c7cc":"markdown","3f0f5fcc":"markdown","b6b60c20":"markdown","38f84caf":"markdown","59fbba5c":"markdown","0601f248":"markdown","33a02f5a":"markdown","2b613aaf":"markdown","468f4805":"markdown","bee1b97d":"markdown"},"source":{"8c5af8c9":"def create_submission(test, preds_test, file_name):\n    predictions = []\n    for pred in preds_test:\n        if pred == 1:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    submission = pd.concat([test_orig['PassengerId'], pd.Series(predictions).astype(\"int\")], axis=1)\n    submission.columns = ['PassengerId', 'Survived']\n    submission['Survived'] = submission['Survived'].astype(\"int\")\n    # Not here since we do not submit it.\n    submission.to_csv(file_name, index = False)\n\n# Returns a fitted and tuned model. Will also create predictions. \ndef run_model(train, test, cv_grid_params,file_name = \"submission.csv\", regression = False, gauss_proc = False):\n    from sklearn.model_selection import cross_val_score, GridSearchCV\n    from sklearn.metrics import confusion_matrix\n    \n    X_train = train.drop(['Survived'], axis = 1)\n    y_train = train['Survived']\n    X_test = test.drop(['Survived'], axis = 1)\n    y_test = test['Survived']\n\n    grid_search = GridSearchCV(**cv_grid_params)\n    grid_search.fit(X_train,y_train)\n    \n    model = cv_grid_params['estimator']\n    \n    model.set_params(**grid_search.best_params_)\n    model.fit(X_train,y_train)\n    cv_scores = cross_val_score(model, X_train, y_train, cv = 10)\n    print(\"Cross validation scores:\" + str(cv_scores))\n    print(\"Mean score: \" + str(cv_scores.mean()))\n    test_preds = model.predict(X_test)\n    if regression == False:\n        print(\"Train confusion matrix:\" )\n        print(confusion_matrix(y_train.astype(\"int\"), model.predict(X_train).astype(\"int\")))\n        print(\"Predicting and creating submission. \")\n        create_submission(X_test, test_preds.astype(\"float\"), file_name)\n    else:\n        train_preds = model.predict(X_train)\n        \n        preds_test = []\n        preds_train = []\n        for i in range(train_preds.shape[0]):\n            if train_preds[i] < 0.5:\n                preds_train.append(0)\n            else:\n                preds_train.append(1)\n        print(\"Train confusion matrix:\" )\n        print(confusion_matrix(y_train, np.array(preds_train)))\n        for i in range(test_preds.shape[0]):\n            if test_preds[i] < 0.5:\n                preds_test.append(0)\n            else:\n                preds_test.append(1)\n        print(\"Predicting and creating ridge submission. \")\n        create_submission(X_test, preds_test, file_name)\n    \n    return test_preds, model\n        \n    ","22c5fe12":"import numpy as np\nimport pandas as pd\n\n# Set seed to always be used\nseed = 123\n\ntrain_orig = pd.read_csv(\"..\/input\/train.csv\")\ntest_orig = pd.read_csv(\"..\/input\/test.csv\")\ndata = pd.concat([test_orig,train_orig], sort = False)\ndata = data.reset_index()\n\nclass_weights = {\n    0: (train_orig['Survived'] == 0).sum()\/train_orig.shape[0],\n    1: (train_orig['Survived'] == 1).sum()\/train_orig.shape[0]\n}\n\ndata.describe()","cd8bf8bd":"data.isna().sum()","4174605a":"# Fill the missing one. \ndata['Fare'] = data['Fare'].fillna(data['Fare'].dropna().median())","112a4b11":"print(data['Cabin'].isnull().sum())\ndata['Cabin_dep'] = [cabin_no[0] for cabin_no in data['Cabin'].astype(\"str\")]\ndata['Cabin_dep'] = data['Cabin_dep'].astype(\"category\")","108c9917":"from matplotlib.pyplot import hist\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import gaussian_kde\nhist(data['Age'].dropna(), density = True)\nfrom scipy.stats import poisson\n\ndens0 = gaussian_kde(data.Age[data['Survived'] == 0].dropna())\ndens1 = gaussian_kde(data.Age[data['Survived'] == 1].dropna())\ndensNA = gaussian_kde(data.Age[data['Survived'].isna()].dropna())\n\nx0 = np.arange(0,data.Age[data['Survived'] == 0].dropna().max())\nx1 = np.arange(0,data.Age[data['Survived'] == 1].dropna().max())\nxNA = np.arange(0,data.Age[data['Survived'].isna()].dropna().max())\n\nplt.plot(x0, dens0.evaluate(x0), 'r')\nplt.plot(x1, dens1.evaluate(x1), 'g')\nplt.plot(xNA, densNA.evaluate(xNA), 'b')\nplt.legend([\"Died\", \"Survived\", \"Test dist\"])\nplt.title(\"Density slightly different, but not very. Still worth to separate sampling though. \")\n\ndist0 = dens0.evaluate(x0)\n# Normalize\ndist0 = np.divide(dist0,np.sum(dist0))\ndist0 = dens0.evaluate(x0)\n# Normalize\ndist0 = np.divide(dist0,np.sum(dist0))\n\ndist1 = dens1.evaluate(x1)\n# Normalize\ndist1 = np.divide(dist1,np.sum(dist1))\ndist1 = dens1.evaluate(x1)\n# Normalize\ndist1 = np.divide(dist1,np.sum(dist1))\n\ndistNA = densNA.evaluate(xNA)\n# Normalize\ndistNA = np.divide(distNA,np.sum(distNA))\ndistNA = dens1.evaluate(xNA)\n# Normalize\ndistNA = np.divide(distNA,np.sum(distNA))\n\n\n# We should sample out of this distribution to compensate. \nnp.random.seed(seed)\nnan_ages0 = np.random.choice(x0, p = dist0, size = data['Age'].isnull().sum())\nnan_ages1 = np.random.choice(x1, p = dist1, size = data['Age'].isnull().sum())\nnan_agesNA = np.random.choice(xNA, p = distNA, size = data['Age'].isnull().sum())\n\ncount = 0\nfor i in list(data.Age[data['Survived'] == 0].index[np.where(data.Age[data['Survived'] == 0].isna())]):\n    data['Age'][i] = nan_ages0[count]\n    count += 1\n        \ncount = 0\nfor i in list(data.Age[data['Survived'] == 1].index[np.where(data.Age[data['Survived'] == 1].isna())]):\n    data['Age'][i] = nan_ages1[count]\n    count += 1\n\ncount = 0\nfor i in list(data.Age[data['Survived'].isna()].index[np.where(data.Age[data['Survived'].isna()].isna())]):\n    data['Age'][i] = nan_agesNA[count]\n    count += 1","b9c37984":"from scipy.stats import zscore\n\nindices_age = data.Age[data['Age'].isnull() == False].index\nz_scores_age = np.abs(zscore(data.Age[data['Age'].isnull() == False]))\n\n\nz_scores = pd.DataFrame(columns=['Age','SibSp','Parch','Fare'])\nz_scores['SibSp'] = zscore(data.SibSp)\nz_scores['Parch'] = zscore(data.Parch)\nz_scores['Fare'] = zscore(data.Fare)\nz_scores['Age'] = zscore(data.Age)\n    \noutlier_rows, outlier_cols = np.where(z_scores > 3)\n\noutliers = {}\n\nfor i,col in enumerate(outlier_rows):\n    if outlier_rows[i] not in outliers:\n        outliers[outlier_rows[i]] = [outlier_cols[i]]\n    else:\n        outliers[outlier_rows[i]].append(outlier_cols[i])\n\n        \n# Remove all being outlier in 2 or more columns\nto_del = []\nfor outlier in outliers:\n    if len(outliers[outlier]) >= 2 and outlier >= test_orig.shape[0]:\n        to_del.append(outlier)\n\nprint(len(to_del))\n# Now delete all in list to_del\nfor i in to_del:\n    data.drop(data.index[i], inplace=True)","8a150801":"data.isnull().sum()","fb4a1f72":"data['Sex'].isnull().sum()\ndata['Sex'] = data['Sex'].astype('str')\ndata['Sex'] = data['Sex'].replace({'male':1, 'female':-1})\nx=np.arange(1,4,2)\nplt.bar(x, data.Sex[data['Survived'] == 1].value_counts().sort_index(), width = 0.5)\nplt.title(\"Gender distributions of people dying and surviving\")\nplt.xticks(x - 0.25,[\"Female\", \"Male\"])\nplt.bar(x-0.5, data.Sex[data['Survived'] == 0].value_counts().sort_index(), width = 0.5, color = \"r\")\nplt.legend([\"Survived\",\"Died\"])","392f953b":"plt.plot(data.Fare[data['Survived'] == 1], 'ro')\nplt.plot(data.Fare[data['Survived'] == 0], 'bo')\nplt.legend([\"Survived\", \"Died\"])\nprint(\"Survived: Mean: \"+str(np.mean(data.Fare[data['Survived'] == 1]))+ \", sd: \" + str(np.std(data.Fare[data['Survived'] == 1])))\nprint(\"Died: Mean: \"+str(np.mean(data.Fare[data['Survived'] == 0]))+ \", sd: \" + str(np.std(data.Fare[data['Survived'] == 0])))","3b4c028c":"x = np.arange(0,14,2)\nplt.bar(x - 0.25, data.Parch[data['Survived'] == 0].value_counts().sort_index(), width = 0.5)\nplt.bar(2*data.Parch[data['Survived'] == 1].value_counts().sort_index().index + 0.25, data.Parch[data['Survived'] == 1].value_counts().sort_index(), color = 'r', width = 0.5)\nplt.xticks(x,np.arange(0,7,1))\nplt.legend(['Survived', 'Died'])\nplt.show()","31bb7069":"data.loc[data['Parch'] >= 3, 'Parch'] = 3\nprint(data.Parch.value_counts().sort_index())\nx = np.arange(0,8,2)\nplt.bar(x - 0.25, data.Parch[data['Survived'] == 0].value_counts().sort_index(), width = 0.5, color = \"r\")\nplt.bar(2*data.Parch[data['Survived'] == 1].value_counts().sort_index().index + 0.25, data.Parch[data['Survived'] == 1].value_counts().sort_index(), color = 'b', width = 0.5)\nplt.xticks(x,np.arange(0,7,1))\nplt.legend(['Died', 'Survived'])","f0ec216c":"x = np.array([0,1,2,3,4,5,8])\n\nplt.bar(np.array([0,2,4,6,8]) - 0.25,data.SibSp[data['Survived'] == 1].value_counts().sort_index(), color=\"b\", width=0.5)\nplt.bar(2*np.array([0,1,2,3,4,5,8]) + 0.25,data.SibSp[data['Survived'] == 0].value_counts().sort_index(), color = \"r\", width=0.5)\nplt.legend([\"Survived\", \"Died\"])\nplt.xticks(np.arange(0,18,2),np.arange(0,9))\nplt.show()","9a68f862":"data.loc[data['SibSp'] >= 4, 'SibSp'] = 4\n\nx = np.arange(0,10,2)\n\nplt.bar(x - 0.25,data.SibSp[data['Survived'] == 1].value_counts().sort_index(),width=0.5, color=\"b\")\nplt.bar(x + 0.25,data.SibSp[data['Survived'] == 0].value_counts().sort_index(),width=0.5, color=\"r\")\nplt.legend(['Survived', 'Died'])\nplt.xticks(x,x\/2)\nplt.show()","48215cc7":"data.Name.head()","32a85ac1":"#print(['yes' for x in [x.lower() for x in list(data['Name'].values)] if 'master' in x])\ndef create_title(name_col):\n    import re\n    for name in name_col:\n        matches = re.findall(\"\\w+\\.\",name)\n        yield matches[0]\n\ndata['Title'] = list(create_title(data['Name']))\n#print(data['Title'].value_counts())\ndata['Title'] = data['Title'].replace({'Mlle.':'Miss.','Ms.':'Miss.','Jonkheer.':\"Mr.\",\"Major.\":\"Mr.\",\"Countess.\":\"Mrs.\",\"Don.\":\"Mr.\", \"Dona.\":\"Mrs.\",\"Sir.\":\"Mr.\",\"Mme.\":\"Miss.\", \"Col.\":\"Military\", \"Major.\":\"Military\",\"Capt.\":\"Military\", \"Lady.\":\"Mrs.\"})\nprint(data['Title'].value_counts())\n\nx = np.arange(0,14,2)\n\nplt.bar(x - 0.25, data.Title[data['Survived'] == 0].value_counts().sort_index(), color = \"r\", width = 0.5)\nplt.bar(x[:6] + 0.25, data.Title[data['Survived'] == 1].value_counts().sort_index(), color = \"b\", width = 0.5)\nplt.xticks(x,[\"Dr.\", \"Master.\", \"Military\",\"Miss.\",\"Mr.\",\"Mrs.\",\"Revenant\"])\nplt.legend([\"Died\",\"Survived\"])\nplt.show()\n","5c4e244a":"data.Ticket.head(20)\ndata.Ticket.loc[20]\n\nticket_type = []\n\nimport re\n\n#print(data.Ticket)\nfor ticket in data.Ticket.values:\n    if ticket.isdigit():\n        ticket_type.append(\"Numbered\")\n    else:\n        ticket_type.append([re.sub('[^a-zA-Z_]','', x)[0:1] for x in ticket.split(\" \")][0])\n        \ndata['Ticket_type'] = ticket_type\nx = np.arange(0,16,2)\nplt.bar(x - 0.25,data.Ticket_type[data['Survived'] == 0].value_counts().sort_index(), color = \"r\", width = 0.5)\nplt.bar(x + 0.25, data.Ticket_type[data['Survived'] == 1].value_counts().sort_index(), color = \"b\", width = 0.5)\nplt.legend(['Died', 'Survived'])\nplt.title(\"Ticket type dying vs \")\nplt.xticks(x,data.Ticket_type[data['Survived'] == 0].value_counts().sort_index().index)\nplt.show()","58ea1ba3":"data = pd.get_dummies(data.drop(['Cabin','Ticket','Name'], axis = 1))\ndata.head()","e8215b29":"train, test = [x for _, x in data.groupby(data['Survived'].isnull())]\ntrain = train.drop(['index', 'PassengerId'], axis = 1)\ntest = test.drop(['index', 'PassengerId'], axis = 1)\n\"\"\"\n# The upsampling, which later was chosen not to use. \nn_dying = train[train['Survived'] == 0].shape[0]\nn_surviving = train[train['Survived'] == 1].shape[0]\nn_to_sample = n_dying - n_surviving\nprint(\"Resampling \"+str(n_to_sample)+\" samples from training set. \")\n\nresamples = train[train['Survived'] == 1].sample(n_to_sample, axis = 0)\n\ntrain = pd.concat([train, resamples])\"\"\"\n\ntrain.Survived.value_counts()","21cb0087":"X_train = train.drop(['Survived'], axis = 1)\ny_train = train['Survived']\nX_test = test.drop(['Survived'], axis = 1)","5db34e79":"from sklearn.kernel_ridge import KernelRidge\n\nkern_ridge = KernelRidge()\n\nparams = {'alpha': [0.01,0.1, 1.0],\n 'coef0': [0,0.1,1],\n 'degree': [1,2,3],\n 'gamma': [0.01,1,10],\n 'kernel': ['rbf']\n}\n\ngrid_params = {\n    'estimator':kern_ridge,\n    'param_grid':params,\n    'n_jobs':5,\n    'iid':False,\n    'verbose':True,\n    'scoring':'neg_mean_squared_error',\n    'cv':10\n}\n\ny_preds_ridge, ridge_model = run_model(train, test, grid_params, file_name = \"submission_ridge.csv\", regression = True)","7dbff3ee":"from sklearn.linear_model import LogisticRegression\n\nlog_reg_model = LogisticRegression()\n\nlog_reg_params = {'C': [0.1,1.0,10,100], \n                  'class_weight': [None], \n                  'dual': [None], \n                  'fit_intercept': [True,False], \n                  'max_iter': [10000], \n                  'multi_class': ['ovr'],\n                  'class_weight':[class_weights],\n                  'n_jobs': [1],\n                  'penalty': ['l1','l2'],\n                  'random_state': [123],  \n                  'tol': [0.0001, 0.0005,0.001], \n                  'solver':['saga'],\n                  'warm_start': [False]}\ngrid_params['estimator'] = log_reg_model\ngrid_params['param_grid'] = log_reg_params\ngrid_params['scoring'] = 'accuracy'\n\ntest_preds_log_reg, log_reg_model = run_model(train, test, grid_params, file_name = \"submission_log_reg.csv\")","db54b790":"from xgboost import XGBClassifier\n\nxgb_model = XGBClassifier()\n\nparams_xgb = {\n    'base_score':[0.3,0.5],\n    'colsample_bytree':[0.4,0.7],\n    'gamma':[0.01,0.5,0.9],\n    'min_child_weight':[1,3],\n    'learning_rate':[0.01,0.1,1],\n    'max_depth':[3,4,5],\n    'n_estimators':[500],\n    'reg_alpha':[1e-5, 0.1],\n    'reg_lambda':[1e-5, 0.1],\n    'subsample':[0.8]\n}\n\ngrid_params['estimator'] = xgb_model\ngrid_params['param_grid'] = params_xgb\ngrid_params['n_jobs'] = 5\n\ntest_preds_xgb, xgb_model = run_model(train, test, grid_params, file_name = \"submission_xgb.csv\")\n","b67c4dbe":"from sklearn.preprocessing import scale\n# Scale the data \ntest_gp = test.copy()\ntrain_gp = train.copy()\ndata_gp = pd.concat([test_gp,train_gp])\ndata_gp[['Age','Fare','SibSp','Parch']] = scale(data_gp[['Age','Fare','SibSp','Parch']])\n\n\nprint(data_gp.head())\n\ndata_gp['Survived'].replace({1:1, 0:-1})\n\nfrom sklearn.preprocessing import scale\n# Time to scale the numerical variables!\ntrain_gp, test_gp = [x for _, x in data_gp.groupby(data_gp['Survived'].isnull())]\n\nprint(train_gp.shape)","f63cf7e5":"from sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nparams = {'copy_X_train': [True],\n 'kernel': [RBF(1.0),RBF(0.01),RBF(0.001),RBF(10), RBF(5), RBF(3), RBF(2)],\n 'max_iter_predict': [100],\n 'multi_class': ['one_vs_rest'],\n 'n_jobs': [3],\n 'n_restarts_optimizer': [0],\n 'optimizer': ['fmin_l_bfgs_b'],\n 'random_state': [123],\n 'warm_start': [False]\n         }\n\ngrid_params['estimator'] = GaussianProcessClassifier()\ngrid_params['param_grid'] = params\n\ntest_preds_gauss, gp_model = run_model(train_gp, test_gp, grid_params, file_name = \"submission_gauss_process.csv\")","11b91526":"from sklearn.svm import SVC\n\nparams = {'C': [0.01,0.1,0.5,1.0,10,50],\n 'class_weight': [class_weights],\n 'coef0': [0.0,0.1,0.5,1],\n 'decision_function_shape': ['ovr'],\n 'degree': [2,3],\n 'gamma': ['auto_deprecated'],\n 'kernel': ['rbf','poly'],\n 'max_iter': [-1],\n 'probability': [False],\n 'random_state': [123],\n 'shrinking': [True],\n 'tol': [0.001,0.003,0.005,0.01]}\n\ngrid_params['estimator'] = SVC()\ngrid_params['param_grid'] = params\n\ntest_preds_svm, svm_model = run_model(train_gp, test_gp, grid_params, file_name = \"submission_gauss_process.csv\")","66608d0c":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier()\n\nparams_rf = {\n     'bootstrap': [True,False],\n     'class_weight': [None],\n     'criterion': ['gini','entropy'],\n     'max_depth': [None],\n     'max_features': ['auto'],\n     'max_leaf_nodes': [None],\n     'min_impurity_decrease': [0.0,0.01,0.1,1],\n     'min_impurity_split': [None],\n     'min_samples_leaf': [1,5,10,15],\n     'min_samples_split': [2,5,10],\n     'min_weight_fraction_leaf': [0.0],\n     'n_estimators': [5,10,20,100],\n     'n_jobs': [None],\n     'oob_score': [False],\n     'random_state': [123]\n            }\n\ngrid_params['estimator'] = rf_model\ngrid_params['param_grid'] = params_rf\n\ntest_preds_rf, rf_model = run_model(train, test, grid_params, file_name = \"submission_rf.csv\")","3c8dc24c":"preds = pd.concat([pd.Series(test_preds_rf),pd.Series(test_preds_gauss),pd.Series(test_preds_xgb),pd.Series(test_preds_svm)], axis = 1)\n\npreds.columns = ['Random Forest', 'Gaussian Process', 'XGBoost', 'SVM']\npreds.corr()\n\nimport seaborn as sns\n\nsns.heatmap(preds.corr(), xticklabels=preds.columns, yticklabels=preds.columns, annot=True)\n","04c82f18":"from sklearn.ensemble import VotingClassifier\nsvm_model.set_params(probability = True)\nvc_model = VotingClassifier([('rf',rf_model),('xgb',xgb_model),('gp',gp_model),('svm',svm_model)], voting = 'soft')\n\nX_train = train_gp.drop(['Survived'], axis = 1)\ny_train = train_gp['Survived']\nX_test = test_gp.drop(['Survived'],axis = 1)\n\nvc_model.fit(X_train, y_train)\n\npreds_train = vc_model.predict(X_train)\n\nfrom sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_train, preds_train))\ny_preds_test = vc_model.predict(X_test)\n\ncreate_submission(test_gp,y_preds_test, \"submission_vc_soft.csv\")","727056ce":"### Name feature\n\nThe name feature is interesting, from which we can draw some other features perhaps. ","a0b5a4e4":"## Define useful global functions\n\nCollecting global functions to be used throughout here. ","450984c5":"Okay, we only removed 1. Hopefully this will help the model. ","b3f27464":"## Gaussian process\n\nLet's try gaussian processes instead. We use the same features as previously, i.e. X_train, y_train and X_test. However, we need to scale the numerical variables. \n\nAlso, in gaussian processes, output $y$ is assumed to have mean 0. Since we have a balanced dataset through upsampling, ","b0b06477":"## Okay, let's go for an ensemble model\n\nSome of the models has worked quite well, but none has been perfect. Let's combine them into an ensemble model and see how good that one will be. \n\nI will not combine ridge regression, as it require modifications of the data for a result. However, I will combine the others. Logistic regression is probably not very good though, so I might exclude that one. \n\nI will also not include logistic regression due to its poor results previously. \n\nAlso noteworthy is that the scaled data will be used. Hopefully, it will not hurt the accuracy of the models that did not use scaled data for their best results. \n\nHowever, let's first check out how the predictions of the test set correlates. ","37214a74":"## Start with processing data, feature fixing etc. ","facdf359":"### SibSp feature\n\nThe SibSp indicates how many siblings and spouses the person had onboard. Again, this probably indicates whether they had to go back for someone or not. \n\n","3b6c31d9":"Let's reduce the variance on this, and let 3 be \"3+\", as these indicate outliers. Also, we only have 2 that have the value 6, which is weird. This probably gives a better, more general model input. ","76c4e518":"### Fare variable\n\nWhat can be seen with the fare variable? \n\nThere is one missing, in the test set. Just replace this one with the mean to compensate for it. ","a083aae6":"It seems like many has titles in their names. ","0bb6eda8":"### Sex variable\n\nThe sex feature does not have any null values. However, we clearly see that men died to a much larger extent than women. Women clearly seem to have been prioritised. ","cf7ffb89":"# Titanic prediction - competition\n","4dce0cc1":"### Check for null values\n\nOkay, so we need to fix age, Fare, Cabin and Embarked. Survived's null values is the test set. ","a431a061":"### Split into training and test set\n\nWe should also upsample to balance the class weights. This does however make the training set's confusion matrix not necessarily representative. \n\nIt can be done with upsampling, which was tried. However, I will use the class weights for those models where this is available instead, to induce a loss function. ","fcfe274f":"### XGBoost \n\nLet's try XGBoost instead. The score, without tuning hyperparameters or any cross-validation, resulted in 0.77, quite a good score. \n\n#### Tuning the parameters of XGBoost\n\nLet's tune the parameters for XGBoost and see how well it performs. ","0bee6b82":"## Remove outliers \n\nFirst of all, I discovered while doing the things below, that there are many outliers in the dataset. To do this, I use the Z-score as in [this article from Towards Data Science](https:\/\/towardsdatascience.com\/ways-to-detect-and-remove-the-outliers-404d16608dba). This is only done on the numerical features. \n\nA critique towards this is that we assume a normal distribution of these, which might be far from the case. However, we'll do it, and see if it works well. ","2bb0ce03":"Let's use the best model to predict and submit it. Tuning as above led to an improvement of 2 %. Not much, but still something. ","d25a93b3":"## Ridge kernel regression\n\nRidge kernel regression might give a nice result. Although it is not classification, this has interestingly worked well before. Important to note that the cross-validation scores here does not reflect the accuracy of the estimates. ","277c836a":"## Fixing age variable\n\nAs we saw, we have many ages that are NaN. To compensate for this, we simulate the distribution and obtain new samples. Make sure to use seed.  \n\nWe sample for the training set and test set individually. For the test set, we just sample out of the distribution it has. For the training and test set, we sample out of densities for each class. \n\nThe test distribution is, as seen, similar to the ones who died, which *might* indicate that we will rather have more people dying in the test set. ","d9525b1b":"## Logistic regression\n\nNow lets try logistic regression. ","9f04946b":"So Logistic regression performed worse. Let's choose the Ridge Regression for the classification. ","c45b5a6f":"#### Create Title feature\n\nFirst of all, everyone seems to have a title \"Mr.\", \"Mrs.\", \"Master.\" or \"Miss.\". Thus, we create a new one with their title. ","183daf09":"## Data processing done - time for modelling\n\nFirst off, we simply try a ridge regression with an rbf kernel and see its performance. This is kind of unchristly as it is actually regression, but it might actually perform well. ","130b1e4d":"Hmm, this one yielded pretty good test results of about 0.78. ","53d5c449":"### Parch feature\n\nThe parch feature apparently indicates how many parents and children aboard the passenger in question had. This variable might give a good indication - a person with no family probably did not have to collect any family members. It is also quite probable that big, poor families were travelling. ","8e8066e5":"We have clear outliers. I think we will do 4+ on this one. ","fa44c7cc":"There actually does seem to be some kind of useful information here. P seems to have survived to larger extent. However, the other seem to, unfortunately, follow the same distribution as the whole training set more or less. ","3f0f5fcc":"## Ticket feature\nLet us look at the ticket feature. \n\nTickets seem to be a number or with letters and a number, indicating some sort of special ticket. Let's categorize them into into \"Numbered\" and their letters. Since we have so many different types, I will just base them based on the first letter. ","b6b60c20":"## Critique, questions and problems \n\nIn this work, I did some feature analysis, feature engineering and tried a few different models. \n\n- I do introduce a bias by drawing from the classes' respective distributions in age. This might be questionable, and other methods might be more appropriate. For example, just filling in with the median, but I don't think that is the way to go either. \n- Upsampling is, to be honest, probably not completely needed here. The data set is already *quite* balanced, and introducing the bias through upsampling of people surviving might actually hurt the results. I did it first, but decided not to, which actually improved my scores. \n- Feature selection needs to be improved. Currently, I have 36 columns. While this is still ~20 times less than the number of training samples, many of these matrices are sparse. \n- How I choose my parameters for my gridsearch for the models can still be improved. I would love to receive feedback on the grid input parameters for the models. \n- Although I did remove some outliers, I am not sure as to how much these actually affect the models that I specifically chose to implement, apart from logistic regression, which is sensitive to outliers. \n\nSo far, the RandomForest, the Gaussian Process and XGBoost have yielded the best scores. I think there might be some feature engineering left to do. ","38f84caf":"The SVM yielded a test score result of about 0.78468, in other words, not too shabby. ","59fbba5c":"### SVM classifier \n\nLet's use an SVM aswell to classify, with *scaled* numerical features. ","0601f248":"### Random Forest\n\nLet us try a random forest","33a02f5a":"This actually yielded some pretty good results; this made me pass above the 0.8 line on the test set. ","2b613aaf":"Interesting is that *all* the revenants died. **Military** is a collection of military title. I did find many noble title, and that might have been interesting to separate. ","468f4805":"## Fixing Cabin department out of Cabin feature\n\nWe can get which department people lived, as well as their enumeration. The most probable is that the letter accounted for different sections, ","bee1b97d":"### Fix dummy variables for categories that we that"}}