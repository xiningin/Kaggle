{"cell_type":{"72601599":"code","e6f8c955":"code","04ad2013":"code","fc868b7b":"code","01a9f0d3":"code","481703f0":"code","9e3c1b13":"code","8115dc45":"code","2346bd09":"code","92242ab8":"code","afbd89f7":"code","cbe49e51":"code","d4351f8c":"code","92a16ac1":"code","30195889":"code","d99efe54":"code","10753f13":"code","4f649b73":"code","16878b18":"code","81dc5163":"code","0b57a8a1":"code","f3367ec1":"code","7852b688":"code","9dbc079a":"code","f1ab337c":"code","50dec590":"code","a063a6bc":"code","64601f0e":"code","72b5945c":"code","35cc826f":"code","895e0348":"code","595a2028":"code","487d2f71":"code","500ac36b":"code","5c60624b":"code","021f0cd2":"code","c22a64c8":"code","328b5553":"code","2f2c58a1":"code","08d18da9":"code","8511adab":"code","de7db18b":"code","5e7597c5":"code","d5ec40e3":"code","67f3da4b":"code","35ddacac":"code","a895af2c":"code","f17a5e6a":"code","a0861fb1":"code","7b1143ce":"code","36aff33f":"code","988d1546":"code","c7fa8996":"code","9e38474d":"code","457fe7e9":"code","fb4c0af9":"code","81130de6":"code","a83cffb4":"code","6ae23001":"code","eb1e9473":"code","55721de3":"code","712267e7":"code","908e46af":"code","ec31bc53":"code","231df54c":"code","23057f65":"code","18f295c7":"code","9cb8ff34":"code","25646947":"code","3f2667e3":"code","a34c23f7":"code","2b230610":"code","8f09cc49":"code","5a73e862":"code","69a70626":"code","4bcbb20a":"code","22d1ec82":"code","eaaf1766":"code","3300558c":"code","3ca9afef":"code","bbf38f85":"code","85d0c10e":"markdown","cfaea53c":"markdown","3bcfed0f":"markdown","504813b9":"markdown","53c5e8d3":"markdown","79fcfa05":"markdown","4a1dc8a8":"markdown","5ab70fff":"markdown","8eee9f84":"markdown","3c0e7962":"markdown","48595799":"markdown","835cb0eb":"markdown","086dac2a":"markdown","c527dde1":"markdown","4b4e3526":"markdown","34596aeb":"markdown","2b35d65f":"markdown","2d86056c":"markdown","96891678":"markdown","78544e92":"markdown","ffb0f33c":"markdown","e8613a94":"markdown","483dc63a":"markdown","294ed1a7":"markdown","4839a7c1":"markdown","3f0de3e1":"markdown","d2e42eb3":"markdown","2f78ef47":"markdown","79cc044a":"markdown","a172f5b4":"markdown","5fa8e535":"markdown","986640ff":"markdown","4ecac390":"markdown","79f18a61":"markdown","14df97a7":"markdown","173142dc":"markdown","980886ad":"markdown","1ad1c9cd":"markdown","545a54d7":"markdown","567bab72":"markdown","b9a79b1d":"markdown","f2ae9577":"markdown","d1627825":"markdown","bde4c48d":"markdown","99ee86d9":"markdown","d887639c":"markdown","79762b4a":"markdown","9d2cb5fe":"markdown","ff3a9fd5":"markdown","4716ebfe":"markdown","14f3024a":"markdown","e65280ab":"markdown","1d6e96ca":"markdown","904b67fe":"markdown","717d699e":"markdown","adda62c0":"markdown"},"source":{"72601599":"#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","e6f8c955":"import tensorflow_datasets as tfds\nimport tensorflow as tf\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt","04ad2013":"# examples, metadata = tfds.load('ted_hrlr_translate\/pt_to_en', with_info=True,\n#                                as_supervised=True)\n# train_examples, val_examples = examples['train'], examples['validation']","fc868b7b":"# tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n#     (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n\n# tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n#     (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)","01a9f0d3":"# sample_string = 'Transformer is awesome.'\n\n# tokenized_string = tokenizer_en.encode(sample_string)\n# print ('Tokenized string is {}'.format(tokenized_string))\n\n# original_string = tokenizer_en.decode(tokenized_string)\n# print ('The original string: {}'.format(original_string))\n\n# assert original_string == sample_string","481703f0":"# for ts in tokenized_string:\n#   print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))","9e3c1b13":"# BUFFER_SIZE = 20000\n# BATCH_SIZE = 64","8115dc45":"# def encode(lang1, lang2):\n#   lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n#       lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n\n#   lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n#       lang2.numpy()) + [tokenizer_en.vocab_size+1]\n  \n#   return lang1, lang2","2346bd09":"# MAX_LENGTH = 40","92242ab8":"# def filter_max_length(x, y, max_length=MAX_LENGTH):\n#   return tf.logical_and(tf.size(x) <= max_length,\n#                         tf.size(y) <= max_length)","afbd89f7":"# def tf_encode(pt, en):\n#   result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n#   result_pt.set_shape([None])\n#   result_en.set_shape([None])\n\n#   return result_pt, result_en","cbe49e51":"# train_dataset = train_examples.map(tf_encode)\n# train_dataset = train_dataset.filter(filter_max_length)\n# # \u5c06\u6570\u636e\u96c6\u7f13\u5b58\u5230\u5185\u5b58\u4e2d\u4ee5\u52a0\u5feb\u8bfb\u53d6\u901f\u5ea6\u3002\n# train_dataset = train_dataset.cache()\n# train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n# train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n\n# val_dataset = val_examples.map(tf_encode)\n# val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)","d4351f8c":"# pt_batch, en_batch = next(iter(val_dataset))\n# pt_batch, en_batch","92a16ac1":"# en_batch.shape","30195889":"# test = en_batch.numpy()[0]\n# test.shape","d99efe54":"import os\nimport time\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport logging\nfrom pprint import pprint\nfrom IPython.display import clear_output\n\nprint(tf.__version__)\nlogging.basicConfig(level=\"ERROR\")  # change the log level to error\nnp.set_printoptions(suppress=True)  # let the numpy do not print out as the scientific pattern\n\n# setting the file location and variables\noutput_dir = \"..\/input\/trans13hckpts\/nmt\"  # where the generated dictionary will locate\nen_vocab_file = os.path.join(output_dir, \"en_vocab\")  # join the two sub-directories\nzh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\ncheckpoint_path = os.path.join(output_dir, \"checkpoints\")\nlog_dir = os.path.join(output_dir, 'logs')\ndownload_dir = \"tensorflow-datasets\/downloads\"  # where the dataset will locate in the project.\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\ntmp_builder = tfds.builder(\"wmt19_translate\/zh-en\")  # check what are the dataset inside the WMT2019\n# pprint(tmp_builder.subsets)\n\n# download the dataset\nconfig = tfds.translate.wmt.WmtConfig(\n    version=tfds.core.Version('0.0.3'),\n    language_pair=(\"zh\", \"en\"),\n    subsets={\n        tfds.Split.TRAIN: [\"newscommentary_v14\"]  # select the news comment train dataset to be the dataset,\n    }\n)\nbuilder = tfds.builder(\"wmt_translate\", config=config)  # fetch the dataset\nbuilder.download_and_prepare(download_dir=download_dir)  # download it, and write it to disk\nclear_output()\n","10753f13":"train_examples, val_examples= builder.as_dataset(split=\"train[:90%]\", as_supervised=True),builder.as_dataset(split=\"train[90%:]\", as_supervised=True)","4f649b73":"# output some data as example\nsample_examples = []\nfor en, zh in train_examples.take(3):\n    en = en.numpy().decode(\"utf-8\")  # use numpy().decode the string into utf-8 format\n    zh = zh.numpy().decode(\"utf-8\")\n\n    # print(en)\n    # print(zh)\n    # print('-' * 10)\n\n    sample_examples.append((en, zh))  # append a tuple into a list\nsample_examples","16878b18":"# construct the dictionary of Chinese and English\n# tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n#     (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n\n# tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n#     (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)\nstart = time.process_time()\ntry:\n    subword_encoder_en = tfds.features.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n    print(f\"\u8f09\u5165\u5df2\u5efa\u7acb\u7684\u82f1\u6587\u5b57\u5178\uff1a {en_vocab_file}\")\nexcept:\n    print(\"\u6c92\u6709\u5df2\u5efa\u7acb\u7684\u82f1\u6587\u5b57\u5178\uff0c\u5f9e\u982d\u5efa\u7acb\u3002\")\n    subword_encoder_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n        (en.numpy() for en, _ in train_examples),\n        target_vocab_size=2 ** 13)  # \u6709\u9700\u8981\u53ef\u4ee5\u8abf\u6574\u5b57\u5178\u5927\u5c0f\n\n    # \u5c07\u5b57\u5178\u6a94\u6848\u5b58\u4e0b\u4ee5\u65b9\u4fbf\u4e0b\u6b21 warmstart\n    subword_encoder_en.save_to_file(en_vocab_file)\n\nprint(f\"\u82f1\u6587\u5b57\u5178\u5927\u5c0f\uff1a{subword_encoder_en.vocab_size}\")\n# print(f\"\u524d 10 \u500b subwords\uff1a{subword_encoder_en.subwords[:10]}\")\n# print()\nend = time.process_time()\nprint(end - start)\n\n# test with a sentence\nsample_string = 'Guangzhou is beautiful.'\nindices = subword_encoder_en.encode(sample_string)\nprint(\"sample string: \", sample_string, \"index of sample string: \", indices)\nprint(100 * '-')\n\n# recover from the indices\nfor index in indices:\n    print(index, 5 * ' ', subword_encoder_en.decode([index]))\n\n# construct a Chinese dictionary\nstart = time.process_time()\ntry:\n    subword_encoder_zh = tfds.features.text.SubwordTextEncoder.load_from_file(zh_vocab_file)\n    print(f\"\u8f09\u5165\u5df2\u5efa\u7acb\u7684\u4e2d\u6587\u5b57\u5178\uff1a {zh_vocab_file}\")\nexcept:\n    print(\"\u6c92\u6709\u5df2\u5efa\u7acb\u7684\u4e2d\u6587\u5b57\u5178\uff0c\u5f9e\u982d\u5efa\u7acb\u3002\")\n    subword_encoder_zh = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n        (zh.numpy() for _, zh in train_examples),  # here should be _, zh, as the pair in training_set is like en-zh\n        target_vocab_size=2 ** 13, max_subword_length=1)  # \u6709\u9700\u8981\u53ef\u4ee5\u8abf\u6574\u5b57\u5178\u5927\u5c0f, \u6bcf\u4e00\u4e2a\u4e2d\u6587\u5b57\u662f\u4e00\u4e2a\u5355\u4f4d\n\n    # \u5c07\u5b57\u5178\u6a94\u6848\u5b58\u4e0b\u4ee5\u65b9\u4fbf\u4e0b\u6b21 warmstart\n    subword_encoder_zh.save_to_file(zh_vocab_file)\n\nprint(f\"\u4e2d\u6587\u5b57\u5178\u5927\u5c0f\uff1a{subword_encoder_zh.vocab_size}\")\n# print(f\"\u524d 10 \u500b subwords\uff1a{subword_encoder_en.subwords[:10]}\")\n# print()\nend = time.process_time()\nprint(\"\u8017\u65f6\uff1a\", end - start)\n\nstring = sample_examples[0]\nzh_string = string[1]\nprint(\"each in sample_example:\", string, 10 * '-', \"\\nthe Chinese part: \", zh_string, 10 * '-',\n      \"\\nis the item in sample_example a tuple?\", isinstance(string, tuple))\nsample_string = sample_examples[0][1]\nindices = subword_encoder_zh.encode(sample_string)\nprint(\"index of the string: \", indices)\n\nfor index in indices:\n    print(index, 5 * ' ', subword_encoder_zh.decode([index]))\n\nen = \"The eurozone\u2019s collapse forces a major realignment of European politics.\"\nzh = \"\u6b27\u5143\u533a\u7684\u74e6\u89e3\u5f3a\u8feb\u6b27\u6d32\u653f\u6cbb\u8fdb\u884c\u4e00\u6b21\u91cd\u5927\u6539\u7ec4\u3002\"\n\n# \u5c07\u6587\u5b57\u8f49\u6210\u70ba subword indices\nen_indices = subword_encoder_en.encode(en)\nzh_indices = subword_encoder_zh.encode(zh)\n\nprint(\"[\u82f1\u4e2d\u539f\u6587]\uff08\u8f49\u63db\u524d\uff09\")\nprint(en)\nprint(zh)\nprint()\nprint('-' * 20)\nprint()\nprint(\"[\u82f1\u4e2d\u5e8f\u5217]\uff08\u8f49\u63db\u5f8c\uff09\")\nprint(en_indices)\nprint(zh_indices)\nprint(100 * '-')\n\n\n# pre-process:\n# insert a special token in both the beginning and the end of seq:\ndef encode(en_t, zh_t):  # now the en_t,zh_t are eager tensor\n    # \u56e0\u70ba\u5b57\u5178\u7684\u7d22\u5f15\u5f9e 0 \u958b\u59cb\uff0c\n    # \u6211\u5011\u53ef\u4ee5\u4f7f\u7528 subword_encoder_en.vocab_size \u9019\u500b\u503c\u4f5c\u70ba BOS \u7684\u7d22\u5f15\u503c\n    # \u7528 subword_encoder_en.vocab_size + 1 \u4f5c\u70ba EOS \u7684\u7d22\u5f15\u503c\n    en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n        en_t.numpy()) + [subword_encoder_en.vocab_size + 1]\n    # \u540c\u7406\uff0c\u4e0d\u904e\u662f\u4f7f\u7528\u4e2d\u6587\u5b57\u5178\u7684\u6700\u5f8c\u4e00\u500b\u7d22\u5f15 + 1\n    zh_indices = [subword_encoder_zh.vocab_size] + subword_encoder_zh.encode(\n        zh_t.numpy()) + [subword_encoder_zh.vocab_size + 1]\n\n    return en_indices, zh_indices\n\n\nen, zh = next(iter(\n    train_examples))  # here en,zh are just Tensor:<tf.Tensor: id=248, shape=(), dtype=string, numpy=b'Making Do With More'>\nen_t, zh_t = encode(en, zh)\npprint((en, zh))\nprint(\"after pre-process:\")\npprint((en_t, zh_t))\n\n\ndef tf_encode(en_t,\n              zh_t):  # because in the dataset.map(), which is run in Graph mode instead of eager mode, so the en_t, zh_t are not eager tensor, which do not contain the .numpy()\n    return tf.py_function(encode, [en_t, zh_t], [tf.int64,\n                                                 tf.int64])  # this will wrap the encode() into a eager mode enabled function in Graph mode when do the map() later on.\n\n\ntrain_dataset = train_examples.map(tf_encode)\nprint(100 * '-')\nprint(\"after pre-processed the whole trainning dataset: (take one pair example)\")\nen_indices, zh_indices = next(iter(train_dataset))\npprint((en_indices.numpy(), zh_indices.numpy()))\nprint(100*'-')\n\n\n# filter the sequences that more than 40 tokens.\nprint(\"filter the sequences that more than 40: \")\nMAX_LENGTH = 40\n\ndef filter_max_length(en, zh, max_length=MAX_LENGTH):\n    return tf.logical_and(tf.size(en) <= max_length,\n                          tf.size(zh) <= max_length)\n\n\ntrain_dataset = train_dataset.filter(filter_max_length)\n\n# check after the filter\nnum_of_data = 0\nnum_of_invaild = 0\nfor each in train_dataset:\n    en,zh = each\n    if tf.size(en) <= MAX_LENGTH and tf.size(zh) <= MAX_LENGTH:\n        num_of_data+=1\n    else:\n        num_of_invaild+=1\n\nprint(f\"the train_dateset has {num_of_invaild} invalid data, and total {num_of_data} remained valid data\")\nprint(100*'-')\n\n# construct a batch\n# when constructing a batch, the length of each sequence need to be padded, so that there are in the same shape\nprint(\"after batch and pad: \")\nBATCH_SIZE = 64\ntrain_dataset = train_dataset.padded_batch(BATCH_SIZE,padded_shapes=([-1],[-1]))  # padded_shapes: when None and -1, that means the shape of each element before batch will be padded 0 utill reach the maximun length of element in the batch\nen_batch,zh_batch = next(iter(train_dataset))\nprint(\"English batch:\")\nprint(en_batch)\nprint(20*'-')\nprint(\"Chinese batch:\")\nprint(zh_batch)","81dc5163":"def get_angles(pos, i, d_model):\n  angle_rates = 1 \/ np.power(10000, (2 * (i\/\/2)) \/ np.float32(d_model))\n  return pos * angle_rates","0b57a8a1":"def positional_encoding(position, d_model):\n  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                          np.arange(d_model)[np.newaxis, :],\n                          d_model)\n  \n  # \u5c06 sin \u5e94\u7528\u4e8e\u6570\u7ec4\u4e2d\u7684\u5076\u6570\u7d22\u5f15\uff08indices\uff09\uff1b2i\n  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n  \n  # \u5c06 cos \u5e94\u7528\u4e8e\u6570\u7ec4\u4e2d\u7684\u5947\u6570\u7d22\u5f15\uff1b2i+1\n  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    \n  pos_encoding = angle_rads[np.newaxis, ...]\n    \n  return tf.cast(pos_encoding, dtype=tf.float32)","f3367ec1":"pos_encoding = positional_encoding(50, 512)\nprint (pos_encoding.shape)\n\nplt.pcolormesh(pos_encoding[0], cmap='RdBu')\nplt.xlabel('Depth')\nplt.xlim((0, 512))\nplt.ylabel('Position')\nplt.colorbar()\nplt.show()","7852b688":"def create_padding_mask(seq):\n  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n  \n  # \u6dfb\u52a0\u989d\u5916\u7684\u7ef4\u5ea6\u6765\u5c06\u586b\u5145\u52a0\u5230\n  # \u6ce8\u610f\u529b\u5bf9\u6570\uff08logits\uff09\u3002\n  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)","9dbc079a":"x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\ncreate_padding_mask(x)","f1ab337c":"def create_look_ahead_mask(size):\n  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n  return mask  # (seq_len, seq_len)","50dec590":"x = tf.random.uniform((1, 3))\ntemp = create_look_ahead_mask(x.shape[1])\ntemp","a063a6bc":"def scaled_dot_product_attention(q, k, v, mask):\n  \"\"\"\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd\u3002\n  q, k, v \u5fc5\u987b\u5177\u6709\u5339\u914d\u7684\u524d\u7f6e\u7ef4\u5ea6\u3002\n  k, v \u5fc5\u987b\u6709\u5339\u914d\u7684\u5012\u6570\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\uff0c\u4f8b\u5982\uff1aseq_len_k = seq_len_v\u3002\n  \u867d\u7136 mask \u6839\u636e\u5176\u7c7b\u578b\uff08\u586b\u5145\u6216\u524d\u77bb\uff09\u6709\u4e0d\u540c\u7684\u5f62\u72b6\uff0c\n  \u4f46\u662f mask \u5fc5\u987b\u80fd\u8fdb\u884c\u5e7f\u64ad\u8f6c\u6362\u4ee5\u4fbf\u6c42\u548c\u3002\n  \n  \u53c2\u6570:\n    q: \u8bf7\u6c42\u7684\u5f62\u72b6 == (..., seq_len_q, depth)\n    k: \u4e3b\u952e\u7684\u5f62\u72b6 == (..., seq_len_k, depth)\n    v: \u6570\u503c\u7684\u5f62\u72b6 == (..., seq_len_v, depth_v)\n    mask: Float \u5f20\u91cf\uff0c\u5176\u5f62\u72b6\u80fd\u8f6c\u6362\u6210\n          (..., seq_len_q, seq_len_k)\u3002\u9ed8\u8ba4\u4e3aNone\u3002\n    \n  \u8fd4\u56de\u503c:\n    \u8f93\u51fa\uff0c\u6ce8\u610f\u529b\u6743\u91cd\n  \"\"\"\n\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n  \n  # \u7f29\u653e matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n\n  # \u5c06 mask \u52a0\u5165\u5230\u7f29\u653e\u7684\u5f20\u91cf\u4e0a\u3002\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)  \n\n  # softmax \u5728\u6700\u540e\u4e00\u4e2a\u8f74\uff08seq_len_k\uff09\u4e0a\u5f52\u4e00\u5316\uff0c\u56e0\u6b64\u5206\u6570\n  # \u76f8\u52a0\u7b49\u4e8e1\u3002\n  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output, attention_weights","64601f0e":"def print_out(q, k, v):\n  temp_out, temp_attn = scaled_dot_product_attention(\n      q, k, v, None)\n  print ('Attention weights are:')\n  print (temp_attn)\n  print ('Output is:')\n  print (temp_out)","72b5945c":"np.set_printoptions(suppress=True)\n\ntemp_k = tf.constant([[10,0,0],\n                      [0,10,0],\n                      [0,0,10],\n                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n\ntemp_v = tf.constant([[   1,0],\n                      [  10,0],\n                      [ 100,5],\n                      [1000,6]], dtype=tf.float32)  # (4, 2)\n\n# \u8fd9\u6761 `\u8bf7\u6c42\uff08query\uff09\u7b26\u5408\u7b2c\u4e8c\u4e2a`\u4e3b\u952e\uff08key\uff09`\uff0c\n# \u56e0\u6b64\u8fd4\u56de\u4e86\u7b2c\u4e8c\u4e2a`\u6570\u503c\uff08value\uff09`\u3002\ntemp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\nprint_out(temp_q, temp_k, temp_v)","35cc826f":"# \u8fd9\u6761\u8bf7\u6c42\u7b26\u5408\u91cd\u590d\u51fa\u73b0\u7684\u4e3b\u952e\uff08\u7b2c\u4e09\u7b2c\u56db\u4e2a\uff09\uff0c\n# \u56e0\u6b64\uff0c\u5bf9\u6240\u6709\u7684\u76f8\u5173\u6570\u503c\u53d6\u4e86\u5e73\u5747\u3002\ntemp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\nprint_out(temp_q, temp_k, temp_v)","895e0348":"# \u8fd9\u6761\u8bf7\u6c42\u7b26\u5408\u7b2c\u4e00\u548c\u7b2c\u4e8c\u6761\u4e3b\u952e\uff0c\n# \u56e0\u6b64\uff0c\u5bf9\u5b83\u4eec\u7684\u6570\u503c\u53bb\u4e86\u5e73\u5747\u3002\ntemp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\nprint_out(temp_q, temp_k, temp_v)","595a2028":"temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\nprint_out(temp_q, temp_k, temp_v)","487d2f71":"class MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n    \n    assert d_model % self.num_heads == 0\n    \n    self.depth = d_model \/\/ self.num_heads\n    \n    self.wq = tf.keras.layers.Dense(d_model)\n    self.wk = tf.keras.layers.Dense(d_model)\n    self.wv = tf.keras.layers.Dense(d_model)\n    \n    self.dense = tf.keras.layers.Dense(d_model)\n        \n  def split_heads(self, x, batch_size):\n    \"\"\"\u5206\u62c6\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u5230 (num_heads, depth).\n    \u8f6c\u7f6e\u7ed3\u679c\u4f7f\u5f97\u5f62\u72b6\u4e3a (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n  def call(self, v, k, q, mask):\n    batch_size = tf.shape(q)[0]\n    \n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n    \n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n    \n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n    \n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention, \n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n        \n    return output, attention_weights","500ac36b":"temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\ny = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\nout, attn = temp_mha(y, k=y, q=y, mask=None)\nout.shape, attn.shape","5c60624b":"def point_wise_feed_forward_network(d_model, dff):\n  return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])","021f0cd2":"sample_ffn = point_wise_feed_forward_network(512, 2048)\nsample_ffn(tf.random.uniform((64, 50, 512))).shape","c22a64c8":"class EncoderLayer(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(EncoderLayer, self).__init__()\n\n    self.mha = MultiHeadAttention(d_model, num_heads)\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n  def call(self, x, training, mask):\n\n    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n    attn_output = self.dropout1(attn_output, training=training)\n    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n    \n    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n    ffn_output = self.dropout2(ffn_output, training=training)\n    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n    \n    return out2","328b5553":"sample_encoder_layer = EncoderLayer(512, 8, 2048)\n\nsample_encoder_layer_output = sample_encoder_layer(\n    tf.random.uniform((64, 43, 512)), False, None)\n\nsample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)","2f2c58a1":"class DecoderLayer(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(DecoderLayer, self).__init__()\n\n    self.mha1 = MultiHeadAttention(d_model, num_heads)\n    self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n \n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n    self.dropout3 = tf.keras.layers.Dropout(rate)\n    \n    \n  def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n    # enc_output.shape == (batch_size, input_seq_len, d_model)\n\n    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n    attn1 = self.dropout1(attn1, training=training)\n    out1 = self.layernorm1(attn1 + x)\n    \n    attn2, attn_weights_block2 = self.mha2(\n        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n    attn2 = self.dropout2(attn2, training=training)\n    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n    \n    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n    ffn_output = self.dropout3(ffn_output, training=training)\n    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n    \n    return out3, attn_weights_block1, attn_weights_block2","08d18da9":"sample_decoder_layer = DecoderLayer(512, 8, 2048)\n\nsample_decoder_layer_output, _, _ = sample_decoder_layer(\n    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n    False, None, None)\n\nsample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)","8511adab":"class Encoder(tf.keras.layers.Layer):\n  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n               maximum_position_encoding, rate=0.1):\n    super(Encoder, self).__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n    \n    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n    self.pos_encoding = positional_encoding(maximum_position_encoding, \n                                            self.d_model)\n    \n    \n    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n                       for _ in range(num_layers)]\n  \n    self.dropout = tf.keras.layers.Dropout(rate)\n        \n  def call(self, x, training, mask):\n\n    seq_len = tf.shape(x)[1]\n    \n    # \u5c06\u5d4c\u5165\u548c\u4f4d\u7f6e\u7f16\u7801\u76f8\u52a0\u3002\n    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    x += self.pos_encoding[:, :seq_len, :]\n\n    x = self.dropout(x, training=training)\n    \n    for i in range(self.num_layers):\n      x = self.enc_layers[i](x, training, mask)\n    \n    return x  # (batch_size, input_seq_len, d_model)","de7db18b":"sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n                         dff=2048, input_vocab_size=8500,\n                         maximum_position_encoding=10000)\n\nsample_encoder_output = sample_encoder(tf.random.uniform((64, 62)), \n                                       training=False, mask=None)\n\nprint (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)","5e7597c5":"class Decoder(tf.keras.layers.Layer):\n  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n               maximum_position_encoding, rate=0.1):\n    super(Decoder, self).__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n    \n    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n    \n    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n                       for _ in range(num_layers)]\n    self.dropout = tf.keras.layers.Dropout(rate)\n    \n  def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n\n    seq_len = tf.shape(x)[1]\n    attention_weights = {}\n    \n    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    x += self.pos_encoding[:, :seq_len, :]\n    \n    x = self.dropout(x, training=training)\n\n    for i in range(self.num_layers):\n      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                             look_ahead_mask, padding_mask)\n      \n      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n    \n    # x.shape == (batch_size, target_seq_len, d_model)\n    return x, attention_weights","d5ec40e3":"sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n                         dff=2048, target_vocab_size=8000,\n                         maximum_position_encoding=5000)\n\noutput, attn = sample_decoder(tf.random.uniform((64, 26)), \n                              enc_output=sample_encoder_output, \n                              training=False, look_ahead_mask=None, \n                              padding_mask=None)\n\noutput.shape, attn['decoder_layer2_block2'].shape","67f3da4b":"class Transformer(tf.keras.Model):\n  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n               target_vocab_size, pe_input, pe_target, rate=0.1):\n    super(Transformer, self).__init__()\n\n    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n                           input_vocab_size, pe_input, rate)\n\n    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n                           target_vocab_size, pe_target, rate)\n\n    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n    \n  def call(self, inp, tar, training, enc_padding_mask, \n           look_ahead_mask, dec_padding_mask):\n\n    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n    \n    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n    dec_output, attention_weights = self.decoder(\n        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n    \n    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n    \n    return final_output, attention_weights","35ddacac":"sample_transformer = Transformer(\n    num_layers=2, d_model=512, num_heads=8, dff=2048, \n    input_vocab_size=8500, target_vocab_size=8000, \n    pe_input=10000, pe_target=6000)\n\ntemp_input = tf.random.uniform((64, 62))\ntemp_target = tf.random.uniform((64, 26))\n\nfn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n                               enc_padding_mask=None, \n                               look_ahead_mask=None,\n                               dec_padding_mask=None)\n\nfn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)","a895af2c":"num_layers = 6\nd_model = 512\ndff = 2048\nnum_heads = 8\n\n# input_vocab_size = subword_encoder_zh.vocab_size + 2\ninput_vocab_size = 2 ** 13 + 2\n# target_vocab_size = subword_encoder_en.vocab_size + 2\ntarget_vocab_size = 2 ** 13 + 2\ndropout_rate = 0.1","f17a5e6a":"subword_encoder_zh.vocab_size","a0861fb1":"class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n  def __init__(self, d_model, warmup_steps=4000):\n    super(CustomSchedule, self).__init__()\n    \n    self.d_model = d_model\n    self.d_model = tf.cast(self.d_model, tf.float32)\n\n    self.warmup_steps = warmup_steps\n    \n  def __call__(self, step):\n    arg1 = tf.math.rsqrt(step)\n    arg2 = step * (self.warmup_steps ** -1.5)\n    \n    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)","7b1143ce":"learning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n                                     epsilon=1e-9)","36aff33f":"temp_learning_rate_schedule = CustomSchedule(d_model)\n\nplt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\nplt.ylabel(\"Learning Rate\")\nplt.xlabel(\"Train Step\")","988d1546":"loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')","c7fa8996":"def loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n  \n  return tf.reduce_mean(loss_)","9e38474d":"train_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n    name='train_accuracy')","457fe7e9":"transformer = Transformer(num_layers, d_model, num_heads, dff,\n                          input_vocab_size, target_vocab_size, \n                          pe_input=input_vocab_size, \n                          pe_target=target_vocab_size,\n                          rate=dropout_rate)","fb4c0af9":"def create_masks(inp, tar):\n  # \u7f16\u7801\u5668\u586b\u5145\u906e\u6321\n  enc_padding_mask = create_padding_mask(inp)\n  \n  # \u5728\u89e3\u7801\u5668\u7684\u7b2c\u4e8c\u4e2a\u6ce8\u610f\u529b\u6a21\u5757\u4f7f\u7528\u3002\n  # \u8be5\u586b\u5145\u906e\u6321\u7528\u4e8e\u906e\u6321\u7f16\u7801\u5668\u7684\u8f93\u51fa\u3002\n  dec_padding_mask = create_padding_mask(inp)\n  \n  # \u5728\u89e3\u7801\u5668\u7684\u7b2c\u4e00\u4e2a\u6ce8\u610f\u529b\u6a21\u5757\u4f7f\u7528\u3002\n  # \u7528\u4e8e\u586b\u5145\uff08pad\uff09\u548c\u906e\u6321\uff08mask\uff09\u89e3\u7801\u5668\u83b7\u53d6\u5230\u7684\u8f93\u5165\u7684\u540e\u7eed\u6807\u8bb0\uff08future tokens\uff09\u3002\n  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n  dec_target_padding_mask = create_padding_mask(tar)\n  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n  \n  return enc_padding_mask, combined_mask, dec_padding_mask","81130de6":"# \u5148\u52a0\u8f7d\u6743\u91cd\ncheckpoint_path = \"..\/input\/trans13hckpts\/checkpoints\/train\"\n\nckpt = tf.train.Checkpoint(transformer=transformer,\n                           optimizer=optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# \u5982\u679c\u68c0\u67e5\u70b9\u5b58\u5728\uff0c\u5219\u6062\u590d\u6700\u65b0\u7684\u68c0\u67e5\u70b9\u3002\nif ckpt_manager.latest_checkpoint:\n  ckpt.restore(ckpt_manager.latest_checkpoint)\n  print ('Latest checkpoint restored!!')\n# \u518d\u6b63\u5e38\u5199\u5165\n\ncheckpoint_path = \".\/checkpoints\/train\"\n\nckpt = tf.train.Checkpoint(transformer=transformer,\n                           optimizer=optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","a83cffb4":"EPOCHS = 80","6ae23001":"# \u8be5 @tf.function \u5c06\u8ffd\u8e2a-\u7f16\u8bd1 train_step \u5230 TF \u56fe\u4e2d\uff0c\u4ee5\u4fbf\u66f4\u5feb\u5730\n# \u6267\u884c\u3002\u8be5\u51fd\u6570\u4e13\u7528\u4e8e\u53c2\u6570\u5f20\u91cf\u7684\u7cbe\u786e\u5f62\u72b6\u3002\u4e3a\u4e86\u907f\u514d\u7531\u4e8e\u53ef\u53d8\u5e8f\u5217\u957f\u5ea6\u6216\u53ef\u53d8\n# \u6279\u6b21\u5927\u5c0f\uff08\u6700\u540e\u4e00\u6279\u6b21\u8f83\u5c0f\uff09\u5bfc\u81f4\u7684\u518d\u8ffd\u8e2a\uff0c\u4f7f\u7528 input_signature \u6307\u5b9a\n# \u66f4\u591a\u7684\u901a\u7528\u5f62\u72b6\u3002\n\ntrain_step_signature = [\n    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n]\n\n@tf.function(input_signature=train_step_signature)\ndef train_step(inp, tar):\n  tar_inp = tar[:, :-1]\n  tar_real = tar[:, 1:]\n  \n  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n  \n  with tf.GradientTape() as tape:\n    predictions, _ = transformer(inp, tar_inp, \n                                 True, \n                                 enc_padding_mask, \n                                 combined_mask, \n                                 dec_padding_mask)\n    loss = loss_function(tar_real, predictions)\n\n  gradients = tape.gradient(loss, transformer.trainable_variables)    \n  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n  \n  train_loss(loss)\n  train_accuracy(tar_real, predictions)","eb1e9473":"for epoch in range(EPOCHS):\n  start = time.time()\n  \n  train_loss.reset_states()\n  train_accuracy.reset_states()\n  \n  # inp -> zh, tar -> en\n  for (batch, (inp, tar)) in enumerate(train_dataset):\n    train_step(inp, tar)\n    \n    if batch % 50 == 0:\n      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n      \n  if (epoch + 1) % 5 == 0:\n    ckpt_save_path = ckpt_manager.save()\n    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n                                                         ckpt_save_path))\n    \n  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n                                                train_loss.result(), \n                                                train_accuracy.result()))\n\n  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))","55721de3":"def evaluate(inp_sentence):\n  start_token = [subword_encoder_zh.vocab_size]\n  end_token = [subword_encoder_zh.vocab_size + 1]\n  \n  # \u8f93\u5165\u8bed\u53e5\u662fzh\uff0c\u589e\u52a0\u5f00\u59cb\u548c\u7ed3\u675f\u6807\u8bb0\n  inp_sentence = start_token + subword_encoder_zh.encode(inp_sentence) + end_token\n  encoder_input = tf.expand_dims(inp_sentence, 0)\n  \n  # \u56e0\u4e3a\u76ee\u6807\u662f\u82f1\u8bed\uff0c\u8f93\u5165 transformer \u7684\u7b2c\u4e00\u4e2a\u8bcd\u5e94\u8be5\u662f\n  # \u82f1\u8bed\u7684\u5f00\u59cb\u6807\u8bb0\u3002\n  decoder_input = [subword_encoder_en.vocab_size]\n  output = tf.expand_dims(decoder_input, 0)\n    \n  for i in range(MAX_LENGTH):\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n        encoder_input, output)\n  \n    # predictions.shape == (batch_size, seq_len, vocab_size)\n    predictions, attention_weights = transformer(encoder_input, \n                                                 output,\n                                                 False,\n                                                 enc_padding_mask,\n                                                 combined_mask,\n                                                 dec_padding_mask)\n    \n    # \u4ece seq_len \u7ef4\u5ea6\u9009\u62e9\u6700\u540e\u4e00\u4e2a\u8bcd\n    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n\n    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n    \n    # \u5982\u679c predicted_id \u7b49\u4e8e\u7ed3\u675f\u6807\u8bb0\uff0c\u5c31\u8fd4\u56de\u7ed3\u679c\n    if predicted_id == subword_encoder_en.vocab_size+1:\n      return tf.squeeze(output, axis=0), attention_weights\n    \n    # \u8fde\u63a5 predicted_id \u4e0e\u8f93\u51fa\uff0c\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u8f93\u5165\u4f20\u9012\u5230\u89e3\u7801\u5668\u3002\n    output = tf.concat([output, predicted_id], axis=-1)\n\n  return tf.squeeze(output, axis=0), attention_weights","712267e7":"import matplotlib\nimport matplotlib.pyplot as plt\nmyfont =matplotlib.font_manager.FontProperties(fname=r'..\/input\/china-font\/simfang.ttf')\nplt.title('dasdasdad',fontproperties=myfont,size=20)","908e46af":"def plot_attention_weights(attention, sentence, result, layer):\n    fig = plt.figure(figsize=(16, 8))\n    sentence = subword_encoder_zh.encode(sentence)\n    attention = tf.squeeze(attention[layer], axis=0)\n    for head in range(attention.shape[0]):\n        ax = fig.add_subplot(2, 4, head+1)\n\n        # \u753b\u51fa\u6ce8\u610f\u529b\u6743\u91cd\n        ax.matshow(attention[head][:-1, :], cmap='viridis')\n        # \u8bbe\u7f6e\u5b57\u4f53\u5c5e\u6027\n        fontdict = {'fontsize':10,\"fontproperties\":myfont}\n        # \u8bbe\u7f6e\u523b\u5ea6\n        ax.set_xticks(range(len(sentence)+2))\n        ###\n        '''\n        \u8fd9\u91cc\u6709\u4e00\u4e2a\u5751\uff01\uff01\uff01result\u957f\u8fd9\u6837\uff1a\n        [8173 4849 4849 4849 4849 4849 4849 4849 4849 4849 4849 4849 4849 4849\n         4849 4849 4849 4849 4849 4849 4849 4849 4849 4849 4849 4849 4849 4849\n         4849 4849 4849 4849 4849 4849 4849 4849 4849 4849 4849 4849 4849], shape=(41,), dtype=int32)\n         \u53d1\u73b0\u4e86\u5417\uff0c\u5934\u4e00\u4e2a\u5143\u7d20\u662f\u201c\u8d77\u59cb\u7b26\u201d\uff0c\u4ed6\u4e0d\u4f1a\u88ab\u7f16\u7801\u6210\u6709\u6548\u7684\u5b57\u7b26\uff0c\u6240\u4ee5\uff0c\n         \u7531\u8fd9\u4e32\u5411\u91cf\u7f16\u7801\u51fa\u6765\u7684\u7684\u5b57\u7b26\u548c\u5b83\u672c\u8eab\u7684\u957f\u5ea6\u4e0d\u7b49\uff01\uff01\uff01\uff01\uff01\n         \u6240\u4ee5\u8fd9\u91cc\u8981\u51cf\u4e00\uff01\uff01\uff01\uff01\uff01\n        '''\n        ######\n        ax.set_yticks(range(len(result) - 1))# \u539f\u672c\u8fd9\u91cc\u7684\u957f\u5ea6\u662f41\uff0c\u73b0\u5728\u662f40\n        # \u8bbe\u7f6ey\u7684\u533a\u95f4\n        ax.set_ylim(len(result)-1.5, -0.5)\n        # \u8bbe\u7f6e\u523b\u5ea6\u6807\u7b7e\n        ax.set_xticklabels(\n            ['<start>']+[subword_encoder_zh.decode([i]) for i in sentence]+['<end>'], \n            fontdict=fontdict, rotation=90)\n\n        ax.set_yticklabels([subword_encoder_en.decode([i]) for i in result \n                            if i < subword_encoder_en.vocab_size], \n                           fontdict=fontdict)# \u8fd9\u91cc\u7684\u957f\u5ea6\u662f40\n        # \u8bbe\u7f6e\u5750\u6807\u6807\u7b7e\n        ax.set_xlabel('Head {}'.format(head+1))\n        \n    plt.tight_layout()\n    plt.show()","ec31bc53":"# def plot_attention_weights(attention, sentence, result, layer):\n#     fig = plt.figure(figsize=(16, 8))\n#     sentence = subword_encoder_zh.encode(sentence)\n#     attention = tf.squeeze(attention[layer], axis=0)\n#     print(\"yticks\u7684\u957f\u5ea6\", len(result))\n#     fenci = [subword_encoder_en.decode([i]) for i in result \n#                              if i < subword_encoder_en.vocab_size]\n#     print(\"ytlables\u7684\u957f\u5ea6\",len(fenci))\n#     print(\"\u7ffb\u8bd1\u7ed3\u679c\u8bcd\u5411\u91cf\",result)\n#     print(\"\u7ffb\u8bd1\u7ed3\u6784\", fenci)\n# #     for head in range(attention.shape[0]):\n# #         ax = fig.add_subplot(2, 4, head+1)\n\n# #         # \u753b\u51fa\u6ce8\u610f\u529b\u6743\u91cd\n# #         ax.matshow(attention[head][:-1, :], cmap='viridis')\n# #         # \u8bbe\u7f6e\u5b57\u4f53\u5c5e\u6027\n# #         fontdict = {'fontsize':10,\"fontproperties\":myfont}\n# #         # \u8bbe\u7f6e\u523b\u5ea6\n# #         ax.set_xticks(range(len(sentence)+2))\n# #         ax.set_yticks(range(len(result)))\n# #         # \u8bbe\u7f6ey\u7684\u533a\u95f4\n# #         ax.set_ylim(len(result)-1.5, -0.5)\n# #         # \u8bbe\u7f6e\u523b\u5ea6\u6807\u7b7e\n# #         ax.set_xticklabels(\n# #             ['<start>']+[subword_encoder_zh.decode([i]) for i in sentence]+['<end>'], \n# #             fontdict=fontdict, rotation=90)\n\n# #         ax.set_yticklabels([subword_encoder_en.decode([i]) for i in result \n# #                             if i < subword_encoder_en.vocab_size], \n# #                            fontdict=fontdict)\n# #         # \u8bbe\u7f6e\u5750\u6807\u6807\u7b7e\n# #         ax.set_xlabel('Head {}'.format(head+1))\n# #     print(range(len(result)), [subword_encoder_en.decode([i]) for i in result \n# #                             if i < subword_encoder_en.vocab_size])\n# #     plt.tight_layout()\n# #     plt.show()","231df54c":"def translate(sentence, plot=''):\n  result, attention_weights = evaluate(sentence)\n  \n  predicted_sentence = subword_encoder_en.decode([i for i in result \n                                            if i < subword_encoder_en.vocab_size])  \n\n  print('Input: {}'.format(sentence))\n  print('Predicted translation: {}'.format(predicted_sentence))\n#   print(\"'Predicted translation: In his talk, Karpathy talked about some of the things Tesla has been doing over the past few months\")\n  \n  if plot:\n    plot_attention_weights(attention_weights, sentence, result, plot)","23057f65":"translate(\"\u4f60\u597d.\", plot='decoder_layer1_block2')\nprint (\"Hello.\")","18f295c7":"translate(\"\u4f60\u597d.\", plot='decoder_layer4_block2')\nprint (\"Hello.\")","9cb8ff34":"translate(\"\u4f60\u597d,\u6211\u6765\u81ea\u4e2d\u56fd\", plot='decoder_layer1_block2')\nprint (\"Hello.I come from China.\")","25646947":"translate(\"\u4f60\u597d,\u6211\u6765\u81ea\u4e2d\u56fd\", plot='decoder_layer2_block2')\nprint (\"Hello.I come from China.\")","3f2667e3":"translate(\"\u4f60\u597d,\u6211\u6765\u81ea\u4e2d\u56fd\", plot='decoder_layer3_block2')\nprint (\"Hello.I come from China.\")","a34c23f7":"translate(\"\u4f60\u597d,\u6211\u6765\u81ea\u4e2d\u56fd\", plot='decoder_layer4_block2')\nprint (\"Hello.I come from China.\")","2b230610":"translate(\"\u5728\u6f14\u8bb2\u4e2d\uff0cKarpathy \u8c08\u5230\u4e86\u7279\u65af\u62c9\u5728\u8fc7\u53bb\u51e0\u4e2a\u6708\u4e2d\u6240\u505a\u7684\u4e00\u4e9b\u4e8b\u60c5\", plot='decoder_layer1_block2')\nprint (\"In his talk, Karpathy talked about some of the things Tesla has been doing over the past few months\")","8f09cc49":"translate(\"\u5728\u6f14\u8bb2\u4e2d\uff0cKarpathy \u8c08\u5230\u4e86\u7279\u65af\u62c9\u5728\u8fc7\u53bb\u51e0\u4e2a\u6708\u4e2d\u6240\u505a\u7684\u4e00\u4e9b\u4e8b\u60c5\", plot='decoder_layer2_block2')\nprint (\"In his talk, Karpathy talked about some of the things Tesla has been doing over the past few months\")","5a73e862":"translate(\"\u5728\u6f14\u8bb2\u4e2d\uff0cKarpathy \u8c08\u5230\u4e86\u7279\u65af\u62c9\u5728\u8fc7\u53bb\u51e0\u4e2a\u6708\u4e2d\u6240\u505a\u7684\u4e00\u4e9b\u4e8b\u60c5\", plot='decoder_layer3_block2')\nprint (\"In his talk, Karpathy talked about some of the things Tesla has been doing over the past few months\")","69a70626":"translate(\"\u5728\u6f14\u8bb2\u4e2d\uff0cKarpathy \u8c08\u5230\u4e86\u7279\u65af\u62c9\u5728\u8fc7\u53bb\u51e0\u4e2a\u6708\u4e2d\u6240\u505a\u7684\u4e00\u4e9b\u4e8b\u60c5\", plot='decoder_layer4_block2')\nprint (\"In his talk, Karpathy talked about some of the things Tesla has been doing over the past few months\")","4bcbb20a":"translate(\"\u81ea\u7136\u8bed\u8a00\u5904\u7406\u662f\u4e00\u95e8\u5f88\u6709\u6f5c\u529b\u7684\u5b66\u79d1.\")\nprint (\"Real translation: Natural language processing is a promising subject.\")","22d1ec82":"translate(\"\u5f52\u6839\u7ed3\u5e95\u662f\u4eba\u672c\u8eab\u5e76\u4e0d\u64c5\u957f\u9a7e\u9a76\uff0c\u4f1a\u9020\u6210\u4e00\u4e9b\u4ea4\u901a\u4e8b\u6545\u3002\u6211\u4eec\u5e0c\u671b\u5b9e\u73b0\u4ea4\u901a\u81ea\u52a8\u5316\uff0c\u5e76\u8ba9\u6574\u4e2a\u4eba\u7c7b\u793e\u4f1a\u53d7\u76ca\u3002.\")\nprint (\"In the final analysis, people themselves are not good at driving, will cause some traffic accidents.We want to automate transportation for the benefit of all human society.\")","eaaf1766":"translate(\"\u4e00\u822c\u800c\u8a00\uff0c\u4f53\u578b\u8d8a\u5c0f\u7684\u52a8\u7269\uff0c\u4ee3\u8c22\u7387\u8d8a\u9ad8\u3002\")\nprint (\"In general, the smaller the animal, the higher the metabolic rate.\")","3300558c":"translate(\"\u53f8\u6cd5\u62cd\u5356\u603b\u6709\u732b\u3001\u70c2\u5c3e\u697c\u3001\u77f3\u5934\u3001\u6728\u5934\u3001\u77f3\u72ee\u5b50\u3001\u9a6c\u6876\u3001\u5c0f\u5305\u7eb8\u5dfe\u3001\u98de\u673a\u51a0\u540d\u6743\u3001\u8d5d\u54c1\u53e4\u8463\u3001\u73a9\u5177\u7b49\u4e00\u4e9b\u5947\u602a\u4e1c\u897f\u3002\")\nprint (\"Judicial auctions have always included cats, broken buildings, stones, wood, stone lions, toilets, small packages of tissues, naming rights to airplanes, fake antiques, toys and other odd items.\")","3ca9afef":"translate(\"\u8fd9\u79cd\u6050\u60e7\u662f\u771f\u5b9e\u800c\u5185\u5728\u7684\u3002 \u5ffd\u89c6\u5b83\u7684\u653f\u6cbb\u5bb6\u4eec\u524d\u9014\u582a\u5fe7\")","bbf38f85":"translate(\"\u54c8\u54c8\u54c8\u54c8\u54c8\")","85d0c10e":"Transformer \u5305\u62ec\u7f16\u7801\u5668\uff0c\u89e3\u7801\u5668\u548c\u6700\u540e\u7684\u7ebf\u6027\u5c42\u3002\u89e3\u7801\u5668\u7684\u8f93\u51fa\u662f\u7ebf\u6027\u5c42\u7684\u8f93\u5165\uff0c\u8fd4\u56de\u7ebf\u6027\u5c42\u7684\u8f93\u51fa\u3002","cfaea53c":"\u7531\u4e8e\u76ee\u6807\u5e8f\u5217\u662f\u586b\u5145\uff08padded\uff09\u8fc7\u7684\uff0c\u56e0\u6b64\u5728\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u65f6\uff0c\u5e94\u7528\u586b\u5145\u906e\u6321\u975e\u5e38\u91cd\u8981\u3002","3bcfed0f":"## \u914d\u7f6e\u8d85\u53c2\u6570\uff08hyperparameters\uff09","504813b9":"# \u8bad\u7ec3\u96c6\u5212\u5206","53c5e8d3":"# \u4e0b\u8f7d\u6570\u636e","79fcfa05":"\u521b\u5efa\u4e00\u4e2a `MultiHeadAttention` \u5c42\u8fdb\u884c\u5c1d\u8bd5\u3002\u5728\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u4f4d\u7f6e `y`\uff0c`MultiHeadAttention` \u5728\u5e8f\u5217\u4e2d\u7684\u6240\u6709\u5176\u4ed6\u4f4d\u7f6e\u8fd0\u884c\u6240\u67098\u4e2a\u6ce8\u610f\u529b\u5934\uff0c\u5728\u6bcf\u4e2a\u4f4d\u7f6ey\uff0c\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u540c\u6837\u957f\u5ea6\u7684\u5411\u91cf\u3002","4a1dc8a8":"<img src=\"https:\/\/tensorflow.google.cn\/images\/tutorials\/transformer\/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n\nTransformer \u4f7f\u7528\u7684\u6ce8\u610f\u529b\u51fd\u6570\u6709\u4e09\u4e2a\u8f93\u5165\uff1aQ\uff08\u8bf7\u6c42\uff08query\uff09\uff09\u3001K\uff08\u4e3b\u952e\uff08key\uff09\uff09\u3001V\uff08\u6570\u503c\uff08value\uff09\uff09\u3002\u7528\u4e8e\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd\u7684\u7b49\u5f0f\u4e3a\uff1a\n\n$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n\n\u70b9\u79ef\u6ce8\u610f\u529b\u88ab\u7f29\u5c0f\u4e86\u6df1\u5ea6\u7684\u5e73\u65b9\u6839\u500d\u3002\u8fd9\u6837\u505a\u662f\u56e0\u4e3a\u5bf9\u4e8e\u8f83\u5927\u7684\u6df1\u5ea6\u503c\uff0c\u70b9\u79ef\u7684\u5927\u5c0f\u4f1a\u589e\u5927\uff0c\u4ece\u800c\u63a8\u52a8 softmax \u51fd\u6570\u5f80\u4ec5\u6709\u5f88\u5c0f\u7684\u68af\u5ea6\u7684\u65b9\u5411\u9760\u62e2\uff0c\u5bfc\u81f4\u4e86\u4e00\u79cd\u5f88\u786c\u7684\uff08hard\uff09softmax\u3002\n\n\u4f8b\u5982\uff0c\u5047\u8bbe `Q` \u548c `K` \u7684\u5747\u503c\u4e3a0\uff0c\u65b9\u5dee\u4e3a1\u3002\u5b83\u4eec\u7684\u77e9\u9635\u4e58\u79ef\u5c06\u6709\u5747\u503c\u4e3a0\uff0c\u65b9\u5dee\u4e3a `dk`\u3002\u56e0\u6b64\uff0c*`dk` \u7684\u5e73\u65b9\u6839*\u88ab\u7528\u4e8e\u7f29\u653e\uff08\u800c\u975e\u5176\u4ed6\u6570\u503c\uff09\uff0c\u56e0\u4e3a\uff0c`Q` \u548c `K` \u7684\u77e9\u9635\u4e58\u79ef\u7684\u5747\u503c\u672c\u5e94\u8be5\u4e3a 0\uff0c\u65b9\u5dee\u672c\u5e94\u8be5\u4e3a1\uff0c\u8fd9\u6837\u4f1a\u83b7\u5f97\u4e00\u4e2a\u66f4\u5e73\u7f13\u7684 softmax\u3002\n\n\u906e\u6321\uff08mask\uff09\u4e0e -1e9\uff08\u63a5\u8fd1\u4e8e\u8d1f\u65e0\u7a77\uff09\u76f8\u4e58\u3002\u8fd9\u6837\u505a\u662f\u56e0\u4e3a\u906e\u6321\u4e0e\u7f29\u653e\u7684 Q \u548c K \u7684\u77e9\u9635\u4e58\u79ef\u76f8\u52a0\uff0c\u5e76\u5728 softmax \u4e4b\u524d\u7acb\u5373\u5e94\u7528\u3002\u76ee\u6807\u662f\u5c06\u8fd9\u4e9b\u5355\u5143\u5f52\u96f6\uff0c\u56e0\u4e3a softmax \u7684\u8f83\u5927\u8d1f\u6570\u8f93\u5165\u5728\u8f93\u51fa\u4e2d\u63a5\u8fd1\u4e8e\u96f6\u3002","5ab70fff":"## \u603b\u7ed3","8eee9f84":"\u6bcf\u4e2a\u591a\u5934\u6ce8\u610f\u529b\u5757\u6709\u4e09\u4e2a\u8f93\u5165\uff1aQ\uff08\u8bf7\u6c42\uff09\u3001K\uff08\u4e3b\u952e\uff09\u3001V\uff08\u6570\u503c\uff09\u3002\u8fd9\u4e9b\u8f93\u5165\u7ecf\u8fc7\u7ebf\u6027\uff08Dense\uff09\u5c42\uff0c\u5e76\u5206\u62c6\u6210\u591a\u5934\u3002 \n\n\u5c06\u4e0a\u9762\u5b9a\u4e49\u7684 `scaled_dot_product_attention` \u51fd\u6570\u5e94\u7528\u4e8e\u6bcf\u4e2a\u5934\uff08\u8fdb\u884c\u4e86\u5e7f\u64ad\uff08broadcasted\uff09\u4ee5\u63d0\u9ad8\u6548\u7387\uff09\u3002\u6ce8\u610f\u529b\u8fd9\u6b65\u5fc5\u987b\u4f7f\u7528\u4e00\u4e2a\u6070\u5f53\u7684 mask\u3002\u7136\u540e\u5c06\u6bcf\u4e2a\u5934\u7684\u6ce8\u610f\u529b\u8f93\u51fa\u8fde\u63a5\u8d77\u6765\uff08\u7528`tf.transpose` \u548c `tf.reshape`\uff09\uff0c\u5e76\u653e\u5165\u6700\u540e\u7684 `Dense` \u5c42\u3002\n\nQ\u3001K\u3001\u548c V \u88ab\u62c6\u5206\u5230\u4e86\u591a\u4e2a\u5934\uff0c\u800c\u975e\u5355\u4e2a\u7684\u6ce8\u610f\u529b\u5934\uff0c\u56e0\u4e3a\u591a\u5934\u5141\u8bb8\u6a21\u578b\u5171\u540c\u6ce8\u610f\u6765\u81ea\u4e0d\u540c\u8868\u793a\u7a7a\u95f4\u7684\u4e0d\u540c\u4f4d\u7f6e\u7684\u4fe1\u606f\u3002\u5728\u5206\u62c6\u540e\uff0c\u6bcf\u4e2a\u5934\u90e8\u7684\u7ef4\u5ea6\u51cf\u5c11\uff0c\u56e0\u6b64\u603b\u7684\u8ba1\u7b97\u6210\u672c\u4e0e\u6709\u7740\u5168\u90e8\u7ef4\u5ea6\u7684\u5355\u4e2a\u6ce8\u610f\u529b\u5934\u76f8\u540c\u3002","3c0e7962":"## \u4f4d\u7f6e\u7f16\u7801\uff08Positional encoding\uff09\n\n\u56e0\u4e3a\u8be5\u6a21\u578b\u5e76\u4e0d\u5305\u62ec\u4efb\u4f55\u7684\u5faa\u73af\uff08recurrence\uff09\u6216\u5377\u79ef\uff0c\u6240\u4ee5\u6a21\u578b\u6dfb\u52a0\u4e86\u4f4d\u7f6e\u7f16\u7801\uff0c\u4e3a\u6a21\u578b\u63d0\u4f9b\u4e00\u4e9b\u5173\u4e8e\u5355\u8bcd\u5728\u53e5\u5b50\u4e2d\u76f8\u5bf9\u4f4d\u7f6e\u7684\u4fe1\u606f\u3002\n\n\u4f4d\u7f6e\u7f16\u7801\u5411\u91cf\u88ab\u52a0\u5230\u5d4c\u5165\uff08embedding\uff09\u5411\u91cf\u4e2d\u3002\u5d4c\u5165\u8868\u793a\u4e00\u4e2a d \u7ef4\u7a7a\u95f4\u7684\u6807\u8bb0\uff0c\u5728 d \u7ef4\u7a7a\u95f4\u4e2d\u6709\u7740\u76f8\u4f3c\u542b\u4e49\u7684\u6807\u8bb0\u4f1a\u79bb\u5f7c\u6b64\u66f4\u8fd1\u3002\u4f46\u662f\uff0c\u5d4c\u5165\u5e76\u6ca1\u6709\u5bf9\u5728\u4e00\u53e5\u8bdd\u4e2d\u7684\u8bcd\u7684\u76f8\u5bf9\u4f4d\u7f6e\u8fdb\u884c\u7f16\u7801\u3002\u56e0\u6b64\uff0c\u5f53\u52a0\u4e0a\u4f4d\u7f6e\u7f16\u7801\u540e\uff0c\u8bcd\u5c06\u57fa\u4e8e*\u5b83\u4eec\u542b\u4e49\u7684\u76f8\u4f3c\u5ea6\u4ee5\u53ca\u5b83\u4eec\u5728\u53e5\u5b50\u4e2d\u7684\u4f4d\u7f6e*\uff0c\u5728 d \u7ef4\u7a7a\u95f4\u4e2d\u79bb\u5f7c\u6b64\u66f4\u8fd1\u3002\n\n\u53c2\u770b [\u4f4d\u7f6e\u7f16\u7801](https:\/\/github.com\/tensorflow\/examples\/blob\/master\/community\/en\/position_encoding.ipynb) \u7684 notebook \u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u3002\u8ba1\u7b97\u4f4d\u7f6e\u7f16\u7801\u7684\u516c\u5f0f\u5982\u4e0b\uff1a\n\n$$\\Large{PE_{(pos, 2i)} = sin(pos \/ 10000^{2i \/ d_{model}})} $$\n$$\\Large{PE_{(pos, 2i+1)} = cos(pos \/ 10000^{2i \/ d_{model}})} $$","48595799":"Note: \u6211\u4eec\u7684 TensorFlow \u793e\u533a\u7ffb\u8bd1\u4e86\u8fd9\u4e9b\u6587\u6863\u3002\u56e0\u4e3a\u793e\u533a\u7ffb\u8bd1\u662f\u5c3d\u529b\u800c\u4e3a\uff0c \u6240\u4ee5\u65e0\u6cd5\u4fdd\u8bc1\u5b83\u4eec\u662f\u6700\u51c6\u786e\u7684\uff0c\u5e76\u4e14\u53cd\u6620\u4e86\u6700\u65b0\u7684\n[\u5b98\u65b9\u82f1\u6587\u6587\u6863](https:\/\/tensorflow.google.cn\/?hl=en)\u3002\u5982\u679c\u60a8\u6709\u6539\u8fdb\u6b64\u7ffb\u8bd1\u7684\u5efa\u8bae\uff0c \u8bf7\u63d0\u4ea4 pull request \u5230\n[tensorflow\/docs](https:\/\/github.com\/tensorflow\/docs) GitHub \u4ed3\u5e93\u3002\u8981\u5fd7\u613f\u5730\u64b0\u5199\u6216\u8005\u5ba1\u6838\u8bd1\u6587\uff0c\u8bf7\u52a0\u5165\n[docs-zh-cn@tensorflow.org Google Group](https:\/\/groups.google.com\/a\/tensorflow.org\/forum\/#!forum\/docs-zh-cn)","835cb0eb":"\u4ece\u8bad\u7ec3\u6570\u636e\u96c6\u521b\u5efa\u81ea\u5b9a\u4e49\u5b50\u8bcd\u5206\u8bcd\u5668\uff08subwords tokenizer\uff09\u3002","086dac2a":"\u6c49\u8bed\u4f5c\u4e3a\u8f93\u5165\u8bed\u8a00\uff0c\u82f1\u8bed\u4e3a\u76ee\u6807\u8bed\u8a00\u3002","c527dde1":"# \u89e3\u51b3\u663e\u793a\u4e2d\u6587\u95ee\u9898","4b4e3526":"\u672c\u6559\u7a0b\u8bad\u7ec3\u4e86\u4e00\u4e2a <a href=\"https:\/\/arxiv.org\/abs\/1706.03762\" class=\"external\">Transformer \u6a21\u578b<\/a> \u7528\u4e8e\u5c06\u8461\u8404\u7259\u8bed\u7ffb\u8bd1\u6210\u82f1\u8bed\u3002\u8fd9\u662f\u4e00\u4e2a\u9ad8\u7ea7\u793a\u4f8b\uff0c\u5047\u5b9a\u60a8\u5177\u5907[\u6587\u672c\u751f\u6210\uff08text generation\uff09](text_generation.ipynb)\u548c [\u6ce8\u610f\u529b\u673a\u5236\uff08attention\uff09](nmt_with_attention.ipynb) \u7684\u77e5\u8bc6\u3002\n\nTransformer \u6a21\u578b\u7684\u6838\u5fc3\u601d\u60f3\u662f*\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff08self-attention\uff09*\u2014\u2014\u80fd\u6ce8\u610f\u8f93\u5165\u5e8f\u5217\u7684\u4e0d\u540c\u4f4d\u7f6e\u4ee5\u8ba1\u7b97\u8be5\u5e8f\u5217\u7684\u8868\u793a\u7684\u80fd\u529b\u3002Transformer \u521b\u5efa\u4e86\u591a\u5c42\u81ea\u6ce8\u610f\u529b\u5c42\uff08self-attetion layers\uff09\u7ec4\u6210\u7684\u5806\u6808\uff0c\u4e0b\u6587\u7684*\u6309\u6bd4\u7f29\u653e\u7684\u70b9\u79ef\u6ce8\u610f\u529b\uff08Scaled dot product attention\uff09*\u548c*\u591a\u5934\u6ce8\u610f\u529b\uff08Multi-head attention\uff09*\u90e8\u5206\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e\u3002\n\n\u4e00\u4e2a transformer \u6a21\u578b\u7528\u81ea\u6ce8\u610f\u529b\u5c42\u800c\u975e [RNNs](text_classification_rnn.ipynb) \u6216 [CNNs](..\/images\/intro_to_cnns.ipynb) \u6765\u5904\u7406\u53d8\u957f\u7684\u8f93\u5165\u3002\u8fd9\u79cd\u901a\u7528\u67b6\u6784\u6709\u4e00\u7cfb\u5217\u7684\u4f18\u52bf\uff1a\n\n* \u5b83\u4e0d\u5bf9\u6570\u636e\u95f4\u7684\u65f6\u95f4\/\u7a7a\u95f4\u5173\u7cfb\u505a\u4efb\u4f55\u5047\u8bbe\u3002\u8fd9\u662f\u5904\u7406\u4e00\u7ec4\u5bf9\u8c61\uff08objects\uff09\u7684\u7406\u60f3\u9009\u62e9\uff08\u4f8b\u5982\uff0c[\u661f\u9645\u4e89\u9738\u5355\u4f4d\uff08StarCraft units\uff09](https:\/\/deepmind.com\/blog\/alphastar-mastering-real-time-strategy-game-starcraft-ii\/#block-8)\uff09\u3002\n* \u5c42\u8f93\u51fa\u53ef\u4ee5\u5e76\u884c\u8ba1\u7b97\uff0c\u800c\u975e\u50cf RNN \u8fd9\u6837\u7684\u5e8f\u5217\u8ba1\u7b97\u3002\n* \u8fdc\u8ddd\u79bb\u9879\u53ef\u4ee5\u5f71\u54cd\u5f7c\u6b64\u7684\u8f93\u51fa\uff0c\u800c\u65e0\u9700\u7ecf\u8fc7\u8bb8\u591a RNN \u6b65\u9aa4\u6216\u5377\u79ef\u5c42\uff08\u4f8b\u5982\uff0c\u53c2\u89c1[\u573a\u666f\u8bb0\u5fc6 Transformer\uff08Scene Memory Transformer\uff09](https:\/\/arxiv.org\/pdf\/1903.03878.pdf)\uff09\n* \u5b83\u80fd\u5b66\u4e60\u957f\u8ddd\u79bb\u7684\u4f9d\u8d56\u3002\u5728\u8bb8\u591a\u5e8f\u5217\u4efb\u52a1\u4e2d\uff0c\u8fd9\u662f\u4e00\u9879\u6311\u6218\u3002\n\n\u8be5\u67b6\u6784\u7684\u7f3a\u70b9\u662f\uff1a\n\n* \u5bf9\u4e8e\u65f6\u95f4\u5e8f\u5217\uff0c\u4e00\u4e2a\u5355\u4f4d\u65f6\u95f4\u7684\u8f93\u51fa\u662f\u4ece*\u6574\u4e2a\u5386\u53f2\u8bb0\u5f55*\u8ba1\u7b97\u7684\uff0c\u800c\u975e\u4ec5\u4ece\u8f93\u5165\u548c\u5f53\u524d\u7684\u9690\u542b\u72b6\u6001\u8ba1\u7b97\u5f97\u5230\u3002\u8fd9*\u53ef\u80fd*\u6548\u7387\u8f83\u4f4e\u3002   \n* \u5982\u679c\u8f93\u5165*\u786e\u5b9e*\u6709\u65f6\u95f4\/\u7a7a\u95f4\u7684\u5173\u7cfb\uff0c\u50cf\u6587\u672c\uff0c\u5219\u5fc5\u987b\u52a0\u5165\u4e00\u4e9b\u4f4d\u7f6e\u7f16\u7801\uff0c\u5426\u5219\u6a21\u578b\u5c06\u6709\u6548\u5730\u770b\u5230\u4e00\u5806\u5355\u8bcd\u3002\n\n\u5728\u6b64 notebook \u4e2d\u8bad\u7ec3\u5b8c\u6a21\u578b\u540e\uff0c\u60a8\u5c06\u80fd\u8f93\u5165\u8461\u8404\u7259\u8bed\u53e5\u5b50\uff0c\u5f97\u5230\u5176\u82f1\u6587\u7ffb\u8bd1\u3002\n\n<img src=\"https:\/\/tensorflow.google.cn\/images\/tutorials\/transformer\/attention_map_portuguese.png\" width=\"800\" alt=\"Attention heatmap\">","34596aeb":"## \u521b\u5efa Transformer","2b35d65f":"Note\uff1a\u4e3a\u4e86\u4f7f\u672c\u793a\u4f8b\u8f83\u5c0f\u4e14\u76f8\u5bf9\u8f83\u5feb\uff0c\u5220\u9664\u957f\u5ea6\u5927\u4e8e40\u4e2a\u6807\u8bb0\u7684\u6837\u672c\u3002","2d86056c":"`\u89e3\u7801\u5668`\u5305\u62ec\uff1a\n1.   \u8f93\u51fa\u5d4c\u5165\uff08Output Embedding\uff09\n2.   \u4f4d\u7f6e\u7f16\u7801\uff08Positional Encoding\uff09\n3.   N \u4e2a\u89e3\u7801\u5668\u5c42\uff08decoder layers\uff09\n\n\u76ee\u6807\uff08target\uff09\u7ecf\u8fc7\u4e00\u4e2a\u5d4c\u5165\u540e\uff0c\u8be5\u5d4c\u5165\u548c\u4f4d\u7f6e\u7f16\u7801\u76f8\u52a0\u3002\u8be5\u52a0\u6cd5\u7ed3\u679c\u662f\u89e3\u7801\u5668\u5c42\u7684\u8f93\u5165\u3002\u89e3\u7801\u5668\u7684\u8f93\u51fa\u662f\u6700\u540e\u7684\u7ebf\u6027\u5c42\u7684\u8f93\u5165\u3002","96891678":"## \u906e\u6321\uff08Masking\uff09","78544e92":"\u5982\u679c\u5355\u8bcd\u4e0d\u5728\u8bcd\u5178\u4e2d\uff0c\u5219\u5206\u8bcd\u5668\uff08tokenizer\uff09\u901a\u8fc7\u5c06\u5355\u8bcd\u5206\u89e3\u4e3a\u5b50\u8bcd\u6765\u5bf9\u5b57\u7b26\u4e32\u8fdb\u884c\u7f16\u7801\u3002","ffb0f33c":"### \u89e3\u7801\u5668\uff08Decoder\uff09","e8613a94":"\u4f7f\u7528 [TFDS](https:\/\/tensorflow.google.cn\/datasets) \u6765\u5bfc\u5165 [\u8461\u8404\u7259\u8bed-\u82f1\u8bed\u7ffb\u8bd1\u6570\u636e\u96c6](https:\/\/github.com\/neulab\/word-embeddings-for-nmt)\uff0c\u8be5\u6570\u636e\u96c6\u6765\u81ea\u4e8e [TED \u6f14\u8bb2\u5f00\u653e\u7ffb\u8bd1\u9879\u76ee](https:\/\/www.ted.com\/participate\/translate).\n\n\u8be5\u6570\u636e\u96c6\u5305\u542b\u6765\u7ea6 50000 \u6761\u8bad\u7ec3\u6837\u672c\uff0c1100 \u6761\u9a8c\u8bc1\u6837\u672c\uff0c\u4ee5\u53ca 2000 \u6761\u6d4b\u8bd5\u6837\u672c\u3002","483dc63a":"## \u4f18\u5316\u5668\uff08Optimizer\uff09","294ed1a7":"## \u591a\u5934\u6ce8\u610f\u529b\uff08Multi-head attention\uff09","4839a7c1":"## \u70b9\u5f0f\u524d\u9988\u7f51\u7edc\uff08Point wise feed forward network\uff09","3f0de3e1":"\u5c06\u6240\u6709\u8bf7\u6c42\u4e00\u8d77*\u4f20\u9012*\u3002","d2e42eb3":"## \u8bad\u7ec3\u4e0e\u68c0\u67e5\u70b9\uff08Training and checkpointing\uff09","2f78ef47":"# \u7406\u89e3\u8bed\u8a00\u7684 Transformer \u6a21\u578b","79cc044a":"\u521b\u5efa\u68c0\u67e5\u70b9\u7684\u8def\u5f84\u548c\u68c0\u67e5\u70b9\u7ba1\u7406\u5668\uff08manager\uff09\u3002\u8fd9\u5c06\u7528\u4e8e\u5728\u6bcf `n` \u4e2a\u5468\u671f\uff08epochs\uff09\u4fdd\u5b58\u68c0\u67e5\u70b9\u3002","a172f5b4":"\u4ee5\u4e0b\u6b65\u9aa4\u7528\u4e8e\u8bc4\u4f30\uff1a\n\n* \u7528\u8461\u8404\u7259\u8bed\u5206\u8bcd\u5668\uff08`tokenizer_pt`\uff09\u7f16\u7801\u8f93\u5165\u8bed\u53e5\u3002\u6b64\u5916\uff0c\u6dfb\u52a0\u5f00\u59cb\u548c\u7ed3\u675f\u6807\u8bb0\uff0c\u8fd9\u6837\u8f93\u5165\u5c31\u4e0e\u6a21\u578b\u8bad\u7ec3\u7684\u5185\u5bb9\u76f8\u540c\u3002\u8fd9\u662f\u7f16\u7801\u5668\u8f93\u5165\u3002\n* \u89e3\u7801\u5668\u8f93\u5165\u4e3a `start token == tokenizer_en.vocab_size`\u3002\n* \u8ba1\u7b97\u586b\u5145\u906e\u6321\u548c\u524d\u77bb\u906e\u6321\u3002\n* `\u89e3\u7801\u5668`\u901a\u8fc7\u67e5\u770b`\u7f16\u7801\u5668\u8f93\u51fa`\u548c\u5b83\u81ea\u8eab\u7684\u8f93\u51fa\uff08\u81ea\u6ce8\u610f\u529b\uff09\u7ed9\u51fa\u9884\u6d4b\u3002\n* \u9009\u62e9\u6700\u540e\u4e00\u4e2a\u8bcd\u5e76\u8ba1\u7b97\u5b83\u7684 argmax\u3002\n* \u5c06\u9884\u6d4b\u7684\u8bcd\u8fde\u63a5\u5230\u89e3\u7801\u5668\u8f93\u5165\uff0c\u7136\u540e\u4f20\u9012\u7ed9\u89e3\u7801\u5668\u3002\n* \u5728\u8fd9\u79cd\u65b9\u6cd5\u4e2d\uff0c\u89e3\u7801\u5668\u6839\u636e\u5b83\u9884\u6d4b\u7684\u4e4b\u524d\u7684\u8bcd\u9884\u6d4b\u4e0b\u4e00\u4e2a\u3002\n\nNote\uff1a\u8fd9\u91cc\u4f7f\u7528\u7684\u6a21\u578b\u5177\u6709\u8f83\u5c0f\u7684\u80fd\u529b\u4ee5\u4fdd\u6301\u76f8\u5bf9\u8f83\u5feb\uff0c\u56e0\u6b64\u9884\u6d4b\u53ef\u80fd\u4e0d\u592a\u6b63\u786e\u3002\u8981\u590d\u73b0\u8bba\u6587\u4e2d\u7684\u7ed3\u679c\uff0c\u8bf7\u4f7f\u7528\u5168\u90e8\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4fee\u6539\u4e0a\u8ff0\u8d85\u53c2\u6570\u6765\u4f7f\u7528\u57fa\u7840 transformer \u6a21\u578b\u6216\u8005 transformer XL\u3002","5fa8e535":"# \u6253\u5370\u6570\u636e","986640ff":"\u4e3a\u4e86\u8ba9\u672c\u793a\u4f8b\u5c0f\u4e14\u76f8\u5bf9\u8f83\u5feb\uff0c\u5df2\u7ecf\u51cf\u5c0f\u4e86*num_layers\u3001 d_model \u548c  dff* \u7684\u503c\u3002 \n\nTransformer \u7684\u57fa\u7840\u6a21\u578b\u4f7f\u7528\u7684\u6570\u503c\u4e3a\uff1a*num_layers=6*\uff0c*d_model = 512*\uff0c*dff = 2048*\u3002\u5173\u4e8e\u6240\u6709\u5176\u4ed6\u7248\u672c\u7684 Transformer\uff0c\u8bf7\u67e5\u9605[\u8bba\u6587](https:\/\/arxiv.org\/abs\/1706.03762)\u3002\n\nNote\uff1a\u901a\u8fc7\u6539\u53d8\u4ee5\u4e0b\u6570\u503c\uff0c\u60a8\u53ef\u4ee5\u83b7\u5f97\u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u7684\u6a21\u578b\u3002","4ecac390":"### \u7f16\u7801\u5668\u5c42\uff08Encoder layer\uff09\n\n\u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u5305\u62ec\u4ee5\u4e0b\u5b50\u5c42\uff1a\n\n1.   \u591a\u5934\u6ce8\u610f\u529b\uff08\u6709\u586b\u5145\u906e\u6321\uff09\n2.   \u70b9\u5f0f\u524d\u9988\u7f51\u7edc\uff08Point wise feed forward networks\uff09\u3002\n\n\u6bcf\u4e2a\u5b50\u5c42\u5728\u5176\u5468\u56f4\u6709\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5\uff0c\u7136\u540e\u8fdb\u884c\u5c42\u5f52\u4e00\u5316\u3002\u6b8b\u5dee\u8fde\u63a5\u6709\u52a9\u4e8e\u907f\u514d\u6df1\u5ea6\u7f51\u7edc\u4e2d\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002\n\n\u6bcf\u4e2a\u5b50\u5c42\u7684\u8f93\u51fa\u662f `LayerNorm(x + Sublayer(x))`\u3002\u5f52\u4e00\u5316\u662f\u5728 `d_model`\uff08\u6700\u540e\u4e00\u4e2a\uff09\u7ef4\u5ea6\u5b8c\u6210\u7684\u3002Transformer \u4e2d\u6709 N \u4e2a\u7f16\u7801\u5668\u5c42\u3002","79f18a61":"### \u89e3\u7801\u5668\u5c42\uff08Decoder layer\uff09\n\n\u6bcf\u4e2a\u89e3\u7801\u5668\u5c42\u5305\u62ec\u4ee5\u4e0b\u5b50\u5c42\uff1a\n\n1.   \u906e\u6321\u7684\u591a\u5934\u6ce8\u610f\u529b\uff08\u524d\u77bb\u906e\u6321\u548c\u586b\u5145\u906e\u6321\uff09\n2.   \u591a\u5934\u6ce8\u610f\u529b\uff08\u7528\u586b\u5145\u906e\u6321\uff09\u3002V\uff08\u6570\u503c\uff09\u548c K\uff08\u4e3b\u952e\uff09\u63a5\u6536*\u7f16\u7801\u5668\u8f93\u51fa*\u4f5c\u4e3a\u8f93\u5165\u3002Q\uff08\u8bf7\u6c42\uff09\u63a5\u6536*\u906e\u6321\u7684\u591a\u5934\u6ce8\u610f\u529b\u5b50\u5c42\u7684\u8f93\u51fa*\u3002\n3.   \u70b9\u5f0f\u524d\u9988\u7f51\u7edc\n\n\u6bcf\u4e2a\u5b50\u5c42\u5728\u5176\u5468\u56f4\u6709\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5\uff0c\u7136\u540e\u8fdb\u884c\u5c42\u5f52\u4e00\u5316\u3002\u6bcf\u4e2a\u5b50\u5c42\u7684\u8f93\u51fa\u662f `LayerNorm(x + Sublayer(x))`\u3002\u5f52\u4e00\u5316\u662f\u5728 `d_model`\uff08\u6700\u540e\u4e00\u4e2a\uff09\u7ef4\u5ea6\u5b8c\u6210\u7684\u3002\n\nTransformer \u4e2d\u5171\u6709 N \u4e2a\u89e3\u7801\u5668\u5c42\u3002\n\n\u5f53 Q \u63a5\u6536\u5230\u89e3\u7801\u5668\u7684\u7b2c\u4e00\u4e2a\u6ce8\u610f\u529b\u5757\u7684\u8f93\u51fa\uff0c\u5e76\u4e14 K \u63a5\u6536\u5230\u7f16\u7801\u5668\u7684\u8f93\u51fa\u65f6\uff0c\u6ce8\u610f\u529b\u6743\u91cd\u8868\u793a\u6839\u636e\u7f16\u7801\u5668\u7684\u8f93\u51fa\u8d4b\u4e88\u89e3\u7801\u5668\u8f93\u5165\u7684\u91cd\u8981\u6027\u3002\u6362\u4e00\u79cd\u8bf4\u6cd5\uff0c\u89e3\u7801\u5668\u901a\u8fc7\u67e5\u770b\u7f16\u7801\u5668\u8f93\u51fa\u548c\u5bf9\u5176\u81ea\u8eab\u8f93\u51fa\u7684\u81ea\u6ce8\u610f\u529b\uff0c\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u3002\u53c2\u770b\u6309\u6bd4\u7f29\u653e\u7684\u70b9\u79ef\u6ce8\u610f\u529b\u90e8\u5206\u7684\u6f14\u793a\u3002","14df97a7":"\u5f53 softmax \u5728 K \u4e0a\u8fdb\u884c\u5f52\u4e00\u5316\u540e\uff0c\u5b83\u7684\u503c\u51b3\u5b9a\u4e86\u5206\u914d\u5230 Q \u7684\u91cd\u8981\u7a0b\u5ea6\u3002\n\n\u8f93\u51fa\u8868\u793a\u6ce8\u610f\u529b\u6743\u91cd\u548c V\uff08\u6570\u503c\uff09\u5411\u91cf\u7684\u4e58\u79ef\u3002\u8fd9\u786e\u4fdd\u4e86\u8981\u5173\u6ce8\u7684\u8bcd\u4fdd\u6301\u539f\u6837\uff0c\u800c\u65e0\u5173\u7684\u8bcd\u5c06\u88ab\u6e05\u9664\u6389\u3002","173142dc":"## \u8bc4\u4f30\uff08Evaluate\uff09","980886ad":"##### Copyright 2019 The TensorFlow Authors.","1ad1c9cd":"<img src=\"https:\/\/tensorflow.google.cn\/images\/tutorials\/transformer\/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n\n\n\u591a\u5934\u6ce8\u610f\u529b\u7531\u56db\u90e8\u5206\u7ec4\u6210\uff1a\n*    \u7ebf\u6027\u5c42\u5e76\u5206\u62c6\u6210\u591a\u5934\u3002\n*    \u6309\u6bd4\u7f29\u653e\u7684\u70b9\u79ef\u6ce8\u610f\u529b\u3002\n*    \u591a\u5934\u53ca\u8054\u3002\n*    \u6700\u540e\u4e00\u5c42\u7ebf\u6027\u5c42\u3002","545a54d7":"## \u6309\u6bd4\u7f29\u653e\u7684\u70b9\u79ef\u6ce8\u610f\u529b\uff08Scaled dot product attention\uff09","567bab72":"## \u8bbe\u7f6e\u8f93\u5165\u6d41\u6c34\u7ebf\uff08input pipeline\uff09","b9a79b1d":"## \u7f16\u7801\u4e0e\u89e3\u7801\uff08Encoder and decoder\uff09","f2ae9577":"\u5c06\u5f00\u59cb\u548c\u7ed3\u675f\u6807\u8bb0\uff08token\uff09\u6dfb\u52a0\u5230\u8f93\u5165\u548c\u76ee\u6807\u3002","d1627825":"\u906e\u6321\u4e00\u6279\u5e8f\u5217\u4e2d\u6240\u6709\u7684\u586b\u5145\u6807\u8bb0\uff08pad tokens\uff09\u3002\u8fd9\u786e\u4fdd\u4e86\u6a21\u578b\u4e0d\u4f1a\u5c06\u586b\u5145\u4f5c\u4e3a\u8f93\u5165\u3002\u8be5 mask \u8868\u660e\u586b\u5145\u503c `0` \u51fa\u73b0\u7684\u4f4d\u7f6e\uff1a\u5728\u8fd9\u4e9b\u4f4d\u7f6e mask \u8f93\u51fa `1`\uff0c\u5426\u5219\u8f93\u51fa `0`\u3002","bde4c48d":"<table class=\"tfo-notebook-buttons\" align=\"left\">\n  <td>\n    <a target=\"_blank\" href=\"https:\/\/tensorflow.google.cn\/tutorials\/text\/transformer\">\n    <img src=\"https:\/\/tensorflow.google.cn\/images\/tf_logo_32px.png\" \/>\n    \u5728 tensorflow.google.cn \u4e0a\u67e5\u770b<\/a>\n  <\/td>\n  <td>\n    <a target=\"_blank\" href=\"https:\/\/colab.research.google.com\/github\/tensorflow\/docs-l10n\/blob\/master\/site\/zh-cn\/tutorials\/text\/transformer.ipynb\">\n    <img src=\"https:\/\/tensorflow.google.cn\/images\/colab_logo_32px.png\" \/>\n    \u5728 Google Colab \u8fd0\u884c<\/a>\n  <\/td>\n  <td>\n    <a target=\"_blank\" href=\"https:\/\/github.com\/tensorflow\/docs-l10n\/blob\/master\/site\/zh-cn\/tutorials\/text\/transformer.ipynb\">\n    <img src=\"https:\/\/tensorflow.google.cn\/images\/GitHub-Mark-32px.png\" \/>\n    \u5728 Github \u4e0a\u67e5\u770b\u6e90\u4ee3\u7801<\/a>\n  <\/td>\n  <td>\n    <a href=\"https:\/\/storage.googleapis.com\/tensorflow_docs\/docs-l10n\/site\/zh-cn\/tutorials\/text\/transformer.ipynb\"><img src=\"https:\/\/tensorflow.google.cn\/images\/download_logo_32px.png\" \/>\u4e0b\u8f7d\u6b64 notebook<\/a>\n  <\/td>\n<\/table>","99ee86d9":"\u524d\u77bb\u906e\u6321\uff08look-ahead mask\uff09\u7528\u4e8e\u906e\u6321\u4e00\u4e2a\u5e8f\u5217\u4e2d\u7684\u540e\u7eed\u6807\u8bb0\uff08future tokens\uff09\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u8be5 mask \u8868\u660e\u4e86\u4e0d\u5e94\u8be5\u4f7f\u7528\u7684\u6761\u76ee\u3002\n\n\u8fd9\u610f\u5473\u7740\u8981\u9884\u6d4b\u7b2c\u4e09\u4e2a\u8bcd\uff0c\u5c06\u4ec5\u4f7f\u7528\u7b2c\u4e00\u4e2a\u548c\u7b2c\u4e8c\u4e2a\u8bcd\u3002\u4e0e\u6b64\u7c7b\u4f3c\uff0c\u9884\u6d4b\u7b2c\u56db\u4e2a\u8bcd\uff0c\u4ec5\u4f7f\u7528\u7b2c\u4e00\u4e2a\uff0c\u7b2c\u4e8c\u4e2a\u548c\u7b2c\u4e09\u4e2a\u8bcd\uff0c\u4f9d\u6b64\u7c7b\u63a8\u3002 ","d887639c":"### \u7f16\u7801\u5668\uff08Encoder\uff09\n\n`\u7f16\u7801\u5668` \u5305\u62ec\uff1a\n1.   \u8f93\u5165\u5d4c\u5165\uff08Input Embedding\uff09\n2.   \u4f4d\u7f6e\u7f16\u7801\uff08Positional Encoding\uff09\n3.   N \u4e2a\u7f16\u7801\u5668\u5c42\uff08encoder layers\uff09\n\n\u8f93\u5165\u7ecf\u8fc7\u5d4c\u5165\uff08embedding\uff09\u540e\uff0c\u8be5\u5d4c\u5165\u4e0e\u4f4d\u7f6e\u7f16\u7801\u76f8\u52a0\u3002\u8be5\u52a0\u6cd5\u7ed3\u679c\u7684\u8f93\u51fa\u662f\u7f16\u7801\u5668\u5c42\u7684\u8f93\u5165\u3002\u7f16\u7801\u5668\u7684\u8f93\u51fa\u662f\u89e3\u7801\u5668\u7684\u8f93\u5165\u3002","79762b4a":"`.map()` \u5185\u90e8\u7684\u64cd\u4f5c\u4ee5\u56fe\u6a21\u5f0f\uff08graph mode\uff09\u8fd0\u884c\uff0c`.map()` \u63a5\u6536\u4e00\u4e2a\u4e0d\u5177\u6709 numpy \u5c5e\u6027\u7684\u56fe\u5f20\u91cf\uff08graph tensor\uff09\u3002\u8be5`\u5206\u8bcd\u5668\uff08tokenizer\uff09`\u9700\u8981\u5c06\u4e00\u4e2a\u5b57\u7b26\u4e32\u6216 Unicode \u7b26\u53f7\uff0c\u7f16\u7801\u6210\u6574\u6570\u3002\u56e0\u6b64\uff0c\u60a8\u9700\u8981\u5728 `tf.py_function` \u5185\u90e8\u8fd0\u884c\u7f16\u7801\u8fc7\u7a0b\uff0c`tf.py_function` \u63a5\u6536\u4e00\u4e2a eager \u5f20\u91cf\uff0c\u8be5 eager \u5f20\u91cf\u6709\u4e00\u4e2a\u5305\u542b\u5b57\u7b26\u4e32\u503c\u7684 numpy \u5c5e\u6027\u3002","9d2cb5fe":"\u70b9\u5f0f\u524d\u9988\u7f51\u7edc\u7531\u4e24\u5c42\u5168\u8054\u63a5\u5c42\u7ec4\u6210\uff0c\u4e24\u5c42\u4e4b\u95f4\u6709\u4e00\u4e2a ReLU \u6fc0\u6d3b\u51fd\u6570\u3002","ff3a9fd5":"<img src=\"https:\/\/tensorflow.google.cn\/images\/tutorials\/transformer\/transformer.png\" width=\"600\" alt=\"transformer\">","4716ebfe":"# \u521b\u5efa\u8f93\u5165\u6d41","14f3024a":"\u76ee\u6807\uff08target\uff09\u88ab\u5206\u6210\u4e86 tar_inp \u548c tar_real\u3002tar_inp \u4f5c\u4e3a\u8f93\u5165\u4f20\u9012\u5230\u89e3\u7801\u5668\u3002`tar_real` \u662f\u4f4d\u79fb\u4e86 1 \u7684\u540c\u4e00\u4e2a\u8f93\u5165\uff1a\u5728 `tar_inp` \u4e2d\u7684\u6bcf\u4e2a\u4f4d\u7f6e\uff0c`tar_real` \u5305\u542b\u4e86\u5e94\u8be5\u88ab\u9884\u6d4b\u5230\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\uff08token\uff09\u3002\n\n\u4f8b\u5982\uff0c`sentence` = \"SOS A lion in the jungle is sleeping EOS\"\n\n`tar_inp` =  \"SOS A lion in the jungle is sleeping\"\n\n`tar_real` = \"A lion in the jungle is sleeping EOS\"\n\nTransformer \u662f\u4e00\u4e2a\u81ea\u56de\u5f52\uff08auto-regressive\uff09\u6a21\u578b\uff1a\u5b83\u4e00\u6b21\u4f5c\u4e00\u4e2a\u90e8\u5206\u7684\u9884\u6d4b\uff0c\u7136\u540e\u4f7f\u7528\u5230\u76ee\u524d\u4e3a\u6b62\u7684\u81ea\u8eab\u7684\u8f93\u51fa\u6765\u51b3\u5b9a\u4e0b\u4e00\u6b65\u8981\u505a\u4ec0\u4e48\u3002\n\n\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u672c\u793a\u4f8b\u4f7f\u7528\u4e86 teacher-forcing \u7684\u65b9\u6cd5\uff08\u5c31\u50cf[\u6587\u672c\u751f\u6210\u6559\u7a0b](.\/text_generation.ipynb)\u4e2d\u4e00\u6837\uff09\u3002\u65e0\u8bba\u6a21\u578b\u5728\u5f53\u524d\u65f6\u95f4\u6b65\u9aa4\u4e0b\u9884\u6d4b\u51fa\u4ec0\u4e48\uff0cteacher-forcing \u65b9\u6cd5\u90fd\u4f1a\u5c06\u771f\u5b9e\u7684\u8f93\u51fa\u4f20\u9012\u5230\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e0a\u3002\n\n\u5f53 transformer \u9884\u6d4b\u6bcf\u4e2a\u8bcd\u65f6\uff0c*\u81ea\u6ce8\u610f\u529b\uff08self-attention\uff09*\u529f\u80fd\u4f7f\u5b83\u80fd\u591f\u67e5\u770b\u8f93\u5165\u5e8f\u5217\u4e2d\u524d\u9762\u7684\u5355\u8bcd\uff0c\u4ece\u800c\u66f4\u597d\u5730\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd\u3002\n\n\u4e3a\u4e86\u9632\u6b62\u6a21\u578b\u5728\u671f\u671b\u7684\u8f93\u51fa\u4e0a\u8fbe\u5230\u5cf0\u503c\uff0c\u6a21\u578b\u4f7f\u7528\u4e86\u524d\u77bb\u906e\u6321\uff08look-ahead mask\uff09\u3002","e65280ab":"# \u4e2d\u82f1\u7ffb\u8bd1 \u9884\u5904\u7406en_batch","1d6e96ca":"\u6839\u636e[\u8bba\u6587](https:\/\/arxiv.org\/abs\/1706.03762)\u4e2d\u7684\u516c\u5f0f\uff0c\u5c06 Adam \u4f18\u5316\u5668\u4e0e\u81ea\u5b9a\u4e49\u7684\u5b66\u4e60\u901f\u7387\u8c03\u5ea6\u7a0b\u5e8f\uff08scheduler\uff09\u914d\u5408\u4f7f\u7528\u3002\n\n$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n","904b67fe":"Transformer \u6a21\u578b\u4e0e\u6807\u51c6\u7684[\u5177\u6709\u6ce8\u610f\u529b\u673a\u5236\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\uff08sequence to sequence with attention model\uff09](nmt_with_attention.ipynb)\uff0c\u9075\u5faa\u76f8\u540c\u7684\u4e00\u822c\u6a21\u5f0f\u3002\n\n* \u8f93\u5165\u8bed\u53e5\u7ecf\u8fc7 `N` \u4e2a\u7f16\u7801\u5668\u5c42\uff0c\u4e3a\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u8bcd\/\u6807\u8bb0\u751f\u6210\u4e00\u4e2a\u8f93\u51fa\u3002\n* \u89e3\u7801\u5668\u5173\u6ce8\u7f16\u7801\u5668\u7684\u8f93\u51fa\u4ee5\u53ca\u5b83\u81ea\u8eab\u7684\u8f93\u5165\uff08\u81ea\u6ce8\u610f\u529b\uff09\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u3002","717d699e":"## \u635f\u5931\u51fd\u6570\u4e0e\u6307\u6807\uff08Loss and metrics\uff09","adda62c0":" `plot` \u53c2\u6570\u4f20\u9012\u4e0d\u540c\u7684\u5c42\u548c\u89e3\u7801\u5668\u7684\u6ce8\u610f\u529b\u6a21\u5757\u3002"}}