{"cell_type":{"a6133e28":"code","0fe3b719":"code","e707d6fc":"code","8b82e2e7":"code","e35e0a4d":"code","9cd15661":"code","ee0eabe8":"code","562c73e8":"code","941396f5":"code","32a3af57":"code","13553401":"code","368c1c75":"code","1fe43816":"code","b0eb60ce":"code","1ec1a890":"code","0dd1ac28":"code","45eecb29":"code","6d3682a0":"code","c5a13044":"code","82a95339":"code","1d25507c":"code","fe002461":"code","7ef75c21":"code","164566a5":"code","f0f4c960":"code","bd271ec2":"code","d930b973":"code","b30004f9":"code","2c7dddef":"code","d289155a":"code","f17a608d":"code","986fa118":"code","9d92016a":"code","48debb76":"code","faf214db":"markdown","03d00dd3":"markdown","9592b2fa":"markdown","8bb2e8e3":"markdown","f37e6fa3":"markdown","7c3a19ba":"markdown","305a1366":"markdown","dcb16f4d":"markdown","fa9510dc":"markdown","881564e8":"markdown","8d867921":"markdown","f6a13837":"markdown","a7857fae":"markdown","526c159f":"markdown","84c9c597":"markdown","dd582201":"markdown","d0a71571":"markdown","94d4e965":"markdown","212e804c":"markdown","b2e6ecc3":"markdown","21d43a7d":"markdown","b8116b8a":"markdown","efb3c566":"markdown","59035871":"markdown","4c2f9a0b":"markdown","2bb3b235":"markdown","34549446":"markdown","8399f559":"markdown","7444daa4":"markdown","1b8df75b":"markdown","f02146d4":"markdown","3d179f1d":"markdown","b3547b6d":"markdown"},"source":{"a6133e28":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline \nfrom wordcloud import WordCloud, STOPWORDS\nfrom joblib import Parallel, delayed\nimport tqdm\nimport jieba\nimport time\nimport matplotlib.font_manager as fm","0fe3b719":"INPUT_PATH = \"..\/input\"\nDATA_PATH = os.path.join(INPUT_PATH, os.listdir(INPUT_PATH)[0])\nnews_data_df = pd.read_csv(os.path.join(DATA_PATH, \"news_collection.csv\"), low_memory=False)","e707d6fc":"news_data_df.head()","8b82e2e7":"news_data_df.info()","e35e0a4d":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","9cd15661":"missing_data(news_data_df)","ee0eabe8":"def unique_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    uniques = []\n    for col in data.columns:\n        unique = data[col].nunique()\n        uniques.append(unique)\n    tt['Uniques'] = uniques\n    return(np.transpose(tt))","562c73e8":"unique_values(news_data_df)","941396f5":"def most_frequent_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    items = []\n    vals = []\n    for col in data.columns:\n        itm = data[col].value_counts().index[0]\n        val = data[col].value_counts().values[0]\n        items.append(itm)\n        vals.append(val)\n    tt['Most frequent item'] = items\n    tt['Frequence'] = vals\n    tt['Percent from total'] = np.round(vals \/ total * 100, 3)\n    return(np.transpose(tt))","32a3af57":"most_frequent_values(news_data_df)","13553401":"news_data_df['year'] = news_data_df['date'].apply(lambda x: str(x)[0:4])\nnews_data_df['month'] = news_data_df['date'].apply(lambda x: str(x)[4:6])\nnews_data_df['day'] = news_data_df['date'].apply(lambda x: str(x)[6:8])","368c1c75":"news_data_df[['title', 'date', 'year', 'month', 'day']].head()","1fe43816":"def jieba_tokens(x, sep=' ', cut_all_flag=False):\n    '''\n    input: x - text in Chines to cut\n    input: sep - separator to use in the output\n    input: cut_all_flag - cut in individual ideograms rather than in concepts (groups of ideograms). \n    function: cut the text in Chinese in group of ideograms (or individual ideograms)\n    output: the text cut in group of ideograms (or ideograms)\n    '''\n    try:\n        return sep.join(jieba.cut(x, cut_all=cut_all_flag))\n    except:\n        return None","b0eb60ce":"start_time = time.time()\nnews_data_df['proc_title'] = Parallel(n_jobs=4)(delayed(jieba_tokens)(x) for x in tqdm.tqdm_notebook(news_data_df['title'].values))\nprint(f\"Total processing time: {round(time.time()-start_time,2)} sec.\")","1ec1a890":"start_time = time.time()\nnews_data_df['proc_desc'] = Parallel(n_jobs=4)(delayed(jieba_tokens)(x) for x in tqdm.tqdm_notebook(news_data_df['desc'].values))\nprint(f\"Total processing time: {round(time.time()-start_time,2)} sec.\")","0dd1ac28":"start_time = time.time()\nnews_data_df['main_url'] = news_data_df['url'].apply(lambda x: x.split('\/')[2])\nprint(f\"Total processing time: {round(time.time()-start_time,2)} sec.\")","45eecb29":"news_data_df[['main_url', 'url']].head()","6d3682a0":"def get_main_image(x):\n    try:\n        return x.split('\/')[2]\n    except:\n        return x\n\nstart_time = time.time()\nnews_data_df['main_image'] = news_data_df['image'].apply(lambda x: get_main_image(x))\nprint(f\"Total processing time: {round(time.time()-start_time,2)} sec.\")","c5a13044":"news_data_df[['main_image', 'image', 'main_url', 'url']].head()","82a95339":"!wget https:\/\/github.com\/adobe-fonts\/source-han-sans\/raw\/release\/SubsetOTF\/SourceHanSansCN.zip\n!unzip -j \"SourceHanSansCN.zip\" \"SourceHanSansCN\/SourceHanSansCN-Regular.otf\" -d \".\"\n!rm SourceHanSansCN.zip\n!ls","1d25507c":"font_path = '.\/SourceHanSansCN-Regular.otf'\nfont_prop = fm.FontProperties(fname=font_path)","fe002461":"def plot_count(feature, title, df, font_prop=font_prop, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:25], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    ax.set_xticklabels(ax.get_xticklabels(), fontproperties=font_prop);\n    plt.show()    ","7ef75c21":"plot_count('main_image', 'Most frequent images sources (first 25 from all data)', news_data_df, size=4)","164566a5":"plot_count('main_url', 'Most frequent main pages of news sites (first 25 from all data)', news_data_df, size=4)","f0f4c960":"plot_count('source', 'Most frequent sources (first 25 from all data)', news_data_df, size=4)","bd271ec2":"plot_count('year', 'Year', news_data_df, size=1)","d930b973":"plot_count('month', 'Month', news_data_df, size=2)","b30004f9":"plot_count('day', 'Day', news_data_df, size=4)","2c7dddef":"def most_frequent_texts(feature, df):\n    total = float(len(df))\n    dd = pd.DataFrame(df[feature].value_counts().index[:10], columns = ['Item'])\n    dd['Frequency'] = df[feature].value_counts().values[:10]\n    dd['Source'] = df['source']\n    dd['Landing page'] = df['main_url']\n    return dd","d289155a":"most_frequent_texts('title', news_data_df)","f17a608d":"most_frequent_texts('desc', news_data_df)","986fa118":"prop = fm.FontProperties(fname=font_path, size=20)\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, font_path=font_path, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        font_path=font_path,\n        max_words=50,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        prop = fm.FontProperties(fname=font_path)\n        fig.suptitle(title, fontsize=40, fontproperties=prop)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","9d92016a":"show_wordcloud(news_data_df['proc_title'], font_path, title = 'Prevalent words in title, all data')","48debb76":"show_wordcloud(news_data_df['proc_desc'], font_path, title = 'Prevalent words in desc, all data')","faf214db":"## Most frequent values","03d00dd3":"Let's check the new values.","9592b2fa":"### Title wordcloud","8bb2e8e3":"All data types are of type `object` besides the date, which is encoded as an `int64`. 3.83% of the descriptions `desc` are missing and 0.3% of images.","f37e6fa3":"# Data exploration","7c3a19ba":"We process in the same time the description `desc`. ","305a1366":"# References  \n\n[1] Jiazhen Xi, Chinese News WordCloud EDA, https:\/\/www.kaggle.com\/johnfarrell\/chinese-news-wordcloud-eda\n[2] Xuan Cao, Chinese News WordCloud EDA - Sometimes Naive!, https:\/\/www.kaggle.com\/naivelamb\/chinese-news-wordcloud-eda-sometimes-naive","dcb16f4d":"## Load data","fa9510dc":"## Extract image main page\n\nLet's extract in a similar way the location of main page for the image.","881564e8":"### Title\n\nFor title and description we will use another mode of presentation, we will just show a table with frequency and text.","8d867921":"# Introduction   \n\n\nWe will explore a Chinese news dataset (with Chinese traditional, Chinese simplified as well as Western characters text) to highlight what is specific to Chinese when we are doing text exploration.  \n\nWe will introduce the use of a special library for cutting Chinese text in `words` i.e. group of ideograms that define together a concept.\n\nMain credits for the utility functions for processing Chinese text goes to <a href=\"https:\/\/www.kaggle.com\/johnfarrell\">Jiazhen Xi<\/a> and <a href=\"https:\/\/www.kaggle.com\/naivelamb\">Xuan Cao<\/a>, from which I got the function to represent Chinese characters with Seaborn plots and the idea to use `jieba` for cutting Chinese texts in `words`.\n\nIn the reference section of the Kernel are given the respective Kernels links.","f6a13837":"# Prepare the data analysis\n\n\n## Load packages","a7857fae":"## Date extraction\n\n\nLet's extract year, month, day from date.","526c159f":"### Month","84c9c597":"### Source","dd582201":"We are using a special character set to represent the chinese categories or words. We download the ","d0a71571":"## Unique values","94d4e965":"### Year","212e804c":"## Frequent tokens\n\nWe continue now with exploring the frequent tokens extracted from the titles and descriptions, currently stored in `proc_title` and `proc_desc`.","b2e6ecc3":"## Extract landing page\n\nLet's extract the main homepage for each article.","21d43a7d":"## Values distribution","b8116b8a":"## Missing data","efb3c566":"### Main pages of news sites","59035871":"Let's start with the title.","4c2f9a0b":"## Glimpse the data","2bb3b235":"### Desc wordcloud","34549446":"The most frequent title is the main page of `SGSME.SG` (and the article is actually a link to the landing page of this news source). Most frequent description is `...` and most frequent source is `\u828b\u50b3\u5a92` (Taro Media)). As for date, the most frequent date is March 13, 2019, with 756 entries.","8399f559":"## Chinese tokens extraction\n\nIn order to extraxt chinese tokens, we will use the `jieba` library.","7444daa4":"<h1>Chinese News Text Exploration<\/h1>\n\n# Content\n\n* Introduction\n* Prepare for data analysis  \n    * Load packages  \n    * Load data  \n* Data exploration  \n    * Glimpse the data\n    * Missing data\n    * Unique values  \n    * Date extraction  \n    * Chinese tokens extraction  \n    * Extract landing page  \n    * Extract image main page\n    * Data visualization  \n    * Frequent tokens  \n* References ","1b8df75b":"We observe that not only `title` but also some of the texts descriptions (`desc`) are not unique (there are duplicates). Also `image` and `url` are in a small proportion not uniques. As for `source` and `date` are like categories.\n\nLet's see first the most frequent values for each category.","f02146d4":"### Main pages of image source","3d179f1d":"### Desc (description)","b3547b6d":"### Day"}}