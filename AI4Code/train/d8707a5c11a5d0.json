{"cell_type":{"48b54bb8":"code","01af54d6":"code","5ee6d190":"code","4a9e76c0":"code","f17cb6dc":"code","cfe4031d":"code","29ba364b":"code","2c3ee983":"code","fa9a819b":"code","ec58dff9":"code","875ee503":"code","efa30bd5":"code","fe503a43":"code","20d4c526":"code","bc0f3e6d":"code","8d061809":"code","e079fac8":"code","e28ebe52":"code","99e47471":"code","ede22ee5":"code","3af879db":"code","7f5dcb13":"code","011f0aa3":"code","8f32e9f2":"code","65039938":"code","548f19d7":"code","3540123f":"code","b594d190":"code","76187c6c":"code","7c0bdf70":"code","78a5fc6c":"code","8aad942d":"code","885debb6":"code","9147b542":"code","8c6d0d0c":"code","786e3b91":"code","6a5122c4":"code","a6b7af76":"code","27b3d404":"code","d300a512":"code","e1eebad3":"code","69f9d649":"code","9171908e":"code","6dc67382":"code","5f92f646":"code","450aef4d":"markdown","7ca8d0aa":"markdown","ecd9bab6":"markdown","8f8a007c":"markdown","1917979b":"markdown","a6fb3179":"markdown","bbb89053":"markdown","8ad1feed":"markdown","62b2fb93":"markdown","7f0f3731":"markdown","af581f22":"markdown"},"source":{"48b54bb8":"#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points","01af54d6":"#Now let's import and put the train and test datasets in  pandas dataframe\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","5ee6d190":"##display the first five rows of the train dataset.\ntrain.head(5)","4a9e76c0":"##display the first five rows of the test dataset.\ntest.head(5)","f17cb6dc":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","cfe4031d":"#SalePrice is the variable we need to predict. So let's do some analysis on this variable first.\nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","29ba364b":"#The target variable is right skewed. As (linear) models love normally distributed data , \n#we need to transform this variable and make it more normally distributed.\n#Log-transformation of the target variable\n#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","2c3ee983":"#Outliers\n#Documentation for the Ames Housing Data indicates that there are outliers present in the training data\ntrain.describe().transpose()","fa9a819b":"#Let's explore these outliers\n\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","ec58dff9":"#We can see at the bottom right two with extremely large GrLivArea that are of a low price. \n#These values are huge oultliers. Therefore, we can safely delete them.\n#Deleting outliers\n\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","875ee503":"#let's first concatenate the train and test data in the same dataframe\n\nntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\ndf_combined = pd.concat((train, test)).reset_index(drop=True)\ndf_combined.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"df_combined size is : {}\".format(df_combined.shape))","efa30bd5":"#Handling Missing Data\n\ndf_combined_na = (df_combined.isnull().sum() \/ len(df_combined)) * 100\ndf_combined_na = df_combined_na.drop(df_combined_na[df_combined_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :df_combined_na})\nmissing_data.head(30)","fe503a43":"# plotting the missing data \n\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=df_combined_na.index, y=df_combined_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","20d4c526":"#Correlation map to see how features are correlated with SalePrice\n\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)\nplt.show()","bc0f3e6d":"#Imputing missing values\n# Using for loop to fill each category based data description\n\nfor col in ('PoolQC','MiscFeature','Alley','Fence','FireplaceQu','GarageType','GarageFinish',\n           'GarageQual','GarageCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n           'MasVnrType','MSSubClass'):\n        df_combined[col].fillna('NA', inplace=True) ","8d061809":"for col in ('GarageYrBlt','GarageArea','GarageCars','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n             'TotalBsmtSF','BsmtFullBath','BsmtHalfBath','MasVnrArea'):\n        df_combined[col].fillna(0, inplace=True) ","e079fac8":"for col in ('MSZoning','Electrical','KitchenQual','Exterior1st','Exterior2nd','SaleType','Functional','Utilities'):\n        df_combined[col].fillna(df_combined[col].mode()[0], inplace=True) ","e28ebe52":"# Using .groupby and lambda expression to fill 'LotFrontage' column using 'Neighborhood \ndf_combined['LotFrontage'] = df_combined.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","99e47471":"#Check remaining missing values if any \ndf_combined_na = (df_combined.isnull().sum() \/ len(df_combined)) * 100\ndf_combined_na = df_combined_na.drop(df_combined_na[df_combined_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :df_combined_na})\nmissing_data.head(30)","ede22ee5":"#Transforming some numerical variables that are really categorical\n#MSSubClass = The building class\ndf_combined['MSSubClass'] = df_combined['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ndf_combined['OverallCond'] = df_combined['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ndf_combined['YrSold'] = df_combined['YrSold'].astype(str)\ndf_combined['MoSold'] = df_combined['MoSold'].astype(str)","3af879db":"#Label Encoding some categorical variables that may contain information in their ordering set\nfrom sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(df_combined[c].values)) \n    df_combined[c] = lbl.transform(list(df_combined[c].values))\n\n# shape        \nprint('Shape df_combined: {}'.format(df_combined.shape))","7f5dcb13":"# Adding total sqfootage feature \ndf_combined['TotalSF'] = df_combined['TotalBsmtSF'] + df_combined['1stFlrSF'] + df_combined['2ndFlrSF']","011f0aa3":"#\nnumeric_feats = df_combined.dtypes[df_combined.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = df_combined[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","8f32e9f2":"# Drawing  \nf, (ax1,ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10) = plt.subplots(10,figsize=(7,36))\nsns.distplot(df_combined['MiscVal'], fit=norm, ax=ax1)\nsns.distplot(df_combined['PoolArea'], fit=norm, ax=ax2)\nsns.distplot(df_combined['LotArea'], fit=norm, ax=ax3)\nsns.distplot(df_combined['LowQualFinSF'], fit=norm, ax=ax4)\nsns.distplot(df_combined['3SsnPorch'], fit=norm, ax=ax5)\nsns.distplot(df_combined['LandSlope'], fit=norm, ax=ax6)\nsns.distplot(df_combined['KitchenAbvGr'], fit=norm, ax=ax7)\nsns.distplot(df_combined['BsmtFinSF2'], fit=norm, ax=ax8)\nsns.distplot(df_combined['EnclosedPorch'], fit=norm, ax=ax9)\nsns.distplot(df_combined['ScreenPorch'], fit=norm, ax=ax10)","65039938":"#Box Cox Transformation of (highly) skewed features\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    df_combined[feat] += 1\n    df_combined[feat] = boxcox1p(df_combined[feat], lam)\n    \ndf_combined[skewed_features] = np.log1p(df_combined[skewed_features])","548f19d7":"#Getting dummy categorical features \ndf_combined = pd.get_dummies(df_combined, drop_first=True)\nprint(df_combined.shape)","3540123f":"#Getting the new train and test sets.\ndf_train = df_combined[:ntrain]\ndf_test = df_combined[ntrain:]","b594d190":"#Import librairies\nfrom sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\nimport time\nfrom sklearn.neighbors import KNeighborsRegressor","76187c6c":"X_train = df_train.values","7c0bdf70":"X_test = df_test.values","78a5fc6c":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train)\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","8aad942d":"#LASSO Regression \n#This model may be very sensitive to outliers. So we need to made it more robust on them. \n#For that we use the sklearn's Robustscaler() method on pipeline\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","885debb6":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","9147b542":"#ENet Regression\n# Again made robust to outliers\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","8c6d0d0c":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","786e3b91":"score","6a5122c4":"#define a rmsle evaluation function\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","a6b7af76":"lasso.fit(X_train, y_train)\nlasso_train_pred = lasso.predict(X_train)\nlasso_pred = np.expm1(lasso.predict(X_test))\nprint(rmsle(y_train, lasso_train_pred))","27b3d404":"#Test Submission\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = lasso_pred\nsub.to_csv('submissionL.csv',index=False)","d300a512":"#Random Forest Regression using RandomizedSearchCV\nrfr = RandomForestRegressor(criterion='mse', bootstrap=True, random_state=0, n_jobs=2)\n\nparam_dist = dict(n_estimators=list(range(1,100)),\n                  max_depth=list(range(1,100)),\n                  min_samples_leaf=list(range(1,10)))\n\nrand = RandomizedSearchCV(rfr, param_dist, cv=10, verbose=1, n_iter=30)\nrand.fit(X_train,y_train)","e1eebad3":"rand.best_score_ # RandomizedSearchCV","69f9d649":"best_rfr = rand.best_estimator_","9171908e":"best_rfr.score(X_train,y_train) # RandomForestRegressor","6dc67382":"rfr_train_pred = best_rfr.predict(X_train)\nrfr_pred = np.expm1(best_rfr.predict(X_test))\nprint(rmsle(y_train, rfr_train_pred))","5f92f646":"#Test Submission\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = rfr_pred\nsub.to_csv('submissionRR.csv',index=False)","450aef4d":"## Modelling ","7ca8d0aa":"### Features engineering","ecd9bab6":"## Boston house price dictionary\n|Feature|Score|\n|---|---|\n|**LASSO Regression**|*0.1151*|\n|**Elastic Net**|0.1150|\n|**RandomForestRegressor**|*0.9666*|\n|**RandomizedSearchCV**|*0.8779567*|\n","8f8a007c":"### Note :\nOutliers removal is not always safe. We decided to delete these two as they are very huge and really bad ( extremely large areas for very low prices).\nThere are probably others outliers in the training data. However, removing all them may affect badly our models if ever there were also outliers in the test data. That's why , instead of removing them all, we will just manage to make some of our models robust on them. You can refer to the modelling part of this notebook for that.","1917979b":"### Team Members:\n\nAsmaa Alrefae, Faisal Al shuraym, Mansour Aljuaid","a6fb3179":"## Fine_Tuned Models","bbb89053":"The Kaggle Score for this Model is 0.122","8ad1feed":"### Team Name: \nFAM\n","62b2fb93":"## Data Processing ","7f0f3731":"## Base Models","af581f22":"## Problem Statement \n\nPredict the sales price for each house. For each Id in the test set, predict the value of the SalePrice variable. "}}