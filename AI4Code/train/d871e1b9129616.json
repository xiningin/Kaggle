{"cell_type":{"28d9e6c9":"code","dc152bc3":"code","13b33562":"code","1dc9232d":"code","f2194027":"code","5dc8b36e":"code","b9639c96":"code","9981149b":"code","910a5d99":"code","d08f4e73":"code","5873a824":"code","8196201f":"code","a60353ae":"code","2e441ec3":"code","0f1ec984":"code","3c7db147":"code","1b73e11b":"code","fd02ef7a":"code","6add0d46":"code","fb6ae289":"code","0befbf1e":"code","4eed7f74":"code","9f57d26c":"code","9a113d60":"code","7179695d":"code","80462997":"code","764c9716":"code","96e7e82a":"code","459c2cef":"code","b44e5f20":"code","0345648d":"code","dfacd00a":"code","aeacc99c":"code","06b8f776":"code","63b66261":"code","af889973":"code","15af50c9":"code","11e40ab4":"code","896138e8":"code","57a92d51":"code","fa27e06e":"code","cccd9b16":"code","ba180553":"code","fac391b2":"code","759bc6df":"code","bb5ba2e2":"code","6db79cce":"code","095ac629":"code","51ce4972":"code","7e9c649a":"code","68398c9a":"code","96208cbf":"code","10b11693":"code","f18fbf96":"code","24e18fe9":"code","40daeaec":"code","ee4b8b5c":"code","b756f4fd":"code","97350f03":"code","447c9a43":"code","b3baa671":"code","d7405980":"code","7718a5f4":"code","eea29867":"code","e386baac":"code","622c3b5f":"markdown","2d7a7ef6":"markdown","8d4fa611":"markdown","815576b2":"markdown","e9d6fa47":"markdown","cb4a8158":"markdown","dbdc4fde":"markdown","24be8508":"markdown","1c02da10":"markdown","9f0a72fe":"markdown","ec5665f9":"markdown","c754ae8b":"markdown","c0a9469c":"markdown","8185817f":"markdown","e6ba5a72":"markdown","877ab912":"markdown","6b0cee95":"markdown","adbfe2eb":"markdown","1e670dd1":"markdown","7eeb3bf7":"markdown","bebf9df6":"markdown","bc0e70cc":"markdown","3498b4e0":"markdown","d5b0833d":"markdown","50b1b84a":"markdown","8fb8880d":"markdown","54ec1baf":"markdown","c157fd01":"markdown","0df53682":"markdown","04c1d6f3":"markdown","39b9bd7a":"markdown","7e5a696c":"markdown","99dd82f4":"markdown","ceffb024":"markdown","32388d89":"markdown","e02d64b5":"markdown","13429d80":"markdown"},"source":{"28d9e6c9":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport os\nfrom scipy.optimize import fmin as scip_fmin\n\n# Visialisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\n# Machine Learning\n\n# Utils\nfrom sklearn.model_selection import StratifiedKFold, cross_validate\nfrom sklearn.model_selection import cross_val_score, train_test_split, KFold\nfrom sklearn import preprocessing\nimport category_encoders as ce\n\n#Feature Selection\nfrom sklearn.feature_selection import chi2, f_classif, f_regression\nfrom sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, VarianceThreshold\n# Models\nimport xgboost as xgb\n\n#Metrics\nfrom sklearn.metrics import roc_auc_score","dc152bc3":"data_dir = '..\/input\/tabular-playground-series-mar-2021'\n\ntrain_file_path = os.path.join(data_dir, 'train.csv')\ntest_file_path = os.path.join(data_dir, 'test.csv')\nsample_sub_file_path = os.path.join(data_dir, 'sample_submission.csv')\n\nprint(f'Train file: {train_file_path}')\nprint(f'Train file: {test_file_path}')\nprint(f'Train file: {sample_sub_file_path}')","13b33562":"RANDOM_SEED = 42","1dc9232d":"def seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)","f2194027":"seed_everything()","5dc8b36e":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)\nsub_df = pd.read_csv(sample_sub_file_path)","b9639c96":"train_df.sample(10)","9981149b":"train_df.columns","910a5d99":"train_df.describe().T","d08f4e73":"train_df.isnull().sum()","5873a824":"test_df.isnull().sum()","8196201f":"ax = plt.subplots(figsize=(12, 6))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='target', data=train_df);\nplt.ylabel(\"No. of Observations\", size=20);\nplt.xlabel(\"Targets\", size=20);","a60353ae":"def plot_cat_distribution(cat, train_df=train_df):\n    ax = plt.subplots(figsize=(12, 6))\n    sns.set_style('whitegrid')\n    sns.countplot(x=cat, data=train_df);\n    plt.ylabel('No. of Observations', size=20);\n    plt.xlabel(cat+' Count', size=20);\n    plt.show()\n    \ndef plot_cat_response(cat, train_df=train_df):\n    ax = plt.subplots(figsize=(8, 5))\n    sns.set_style('whitegrid')\n    sns.countplot(x=cat, hue='target', data=train_df);\n    plt.show()","2e441ec3":"plot_cat_distribution('cat0')","0f1ec984":"plot_cat_response('cat0')","3c7db147":"plot_cat_distribution('cat1')","1b73e11b":"plot_cat_response('cat1')","fd02ef7a":"plot_cat_distribution('cat2')","6add0d46":"plot_cat_response('cat2')","fb6ae289":"plot_cat_distribution('cat3')","0befbf1e":"plot_cat_response('cat3')","4eed7f74":"plot_cat_distribution('cat4')","9f57d26c":"plot_cat_response('cat4')","9a113d60":"train_df['cat5'].value_counts()","7179695d":"plot_cat_distribution('cat6')","80462997":"plot_cat_response('cat6')","764c9716":"train_df['cat7'].value_counts()","96e7e82a":"train_df['cat8'].value_counts()","459c2cef":"plot_cat_distribution('cat9')","b44e5f20":"plot_cat_response('cat9')","0345648d":"train_df['cat10'].value_counts()","dfacd00a":"plot_cat_distribution('cat11')","aeacc99c":"plot_cat_response('cat11')","06b8f776":"plot_cat_distribution('cat12')","63b66261":"plot_cat_response('cat12')","af889973":"plot_cat_distribution('cat13')","15af50c9":"plot_cat_response('cat13')","11e40ab4":"plot_cat_distribution('cat14')","896138e8":"plot_cat_response('cat14')","57a92d51":"plot_cat_distribution('cat15')","fa27e06e":"plot_cat_response('cat15')","cccd9b16":"plot_cat_distribution('cat16')","ba180553":"plot_cat_response('cat16')","fac391b2":"plot_cat_distribution('cat17')","759bc6df":"plot_cat_response('cat17')","bb5ba2e2":"plot_cat_distribution('cat18')","6db79cce":"plot_cat_response('cat18')","095ac629":"not_features = ['id', 'target']\nfeatures = [feat for feat in train_df.columns if feat not in not_features]\nnumerical_features = []\ncategorical_features = []\n\nfor feat in features:\n    if train_df[feat].dtype == 'object':\n        categorical_features.append(feat)\n    else:\n        numerical_features.append(feat)\n\nprint(f'Numerical Features: {numerical_features}')\nprint(f'Categorical Features: {categorical_features}')","51ce4972":"g = sns.pairplot(train_df[numerical_features + ['target']], hue='target')\ng.map_lower(sns.kdeplot, levels=4, color=\".2\")\ng.fig.set_size_inches(20,20)","7e9c649a":"train_df_cor_spear = train_df[numerical_features].corr(method='pearson')\nplt.figure(figsize=(10,10))\nsns.heatmap(train_df_cor_spear, square=True, cmap='coolwarm', annot=True);","68398c9a":"plt.figure(figsize=(10,8))\nsns.boxplot(data=train_df[numerical_features], orient=\"h\", palette=\"Set2\");","96208cbf":"# From https:\/\/github.com\/abhishekkrthakur\/approachingalmost\nNUM_SPLITS = 5\n\ntrain_df[\"kfold\"] = -1\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ny = train_df.target.values\nkf = StratifiedKFold(n_splits=NUM_SPLITS)\nfor f, (t_, v_) in enumerate(kf.split(X=train_df, y=y)):\n    train_df.loc[v_, 'kfold'] = f\n    \ntrain_df.head()","10b11693":"train_df[categorical_features] = train_df[categorical_features].apply(lambda x: x.mask(x.map(x.value_counts())< (0.001*train_df.shape[0]), 'RARE'))\n\nfor col in categorical_features:\n    vals = list(train_df[col].unique())\n    test_df[col] = test_df[col].apply(lambda x: 'RARE' if x not in vals else x)","f18fbf96":"def catboost_enc(train_df, test_df, features):\n    cb_enc = ce.CatBoostEncoder(cols=features)\n    cb_enc.fit(train_df[features], train_df['target'])\n    \n    train_df = train_df.join(cb_enc.transform(train_df[features]).add_suffix('_cb'))\n    test_df = test_df.join(cb_enc.transform(test_df[features]).add_suffix('_cb'))\n    \n    train_df = train_df.drop(features, axis=1)\n    test_df = test_df.drop(features, axis=1)\n    \n    return train_df, test_df","24e18fe9":"low_cardinality_cols = []\nhigh_cardinality_cols = []\n\nfor feat in categorical_features:\n    if train_df[feat].nunique() < 5:\n        low_cardinality_cols.append(feat)\n    else:\n        high_cardinality_cols.append(feat)\n        \nprint(f'Low Cardinality Cols: {low_cardinality_cols}')\nprint(f'High Cardinality Cols: {high_cardinality_cols}')","40daeaec":"train_df, test_df = catboost_enc(train_df, test_df, low_cardinality_cols)","ee4b8b5c":"train_df, test_df = catboost_enc(train_df, test_df, high_cardinality_cols)","b756f4fd":"cols_for_log = ['cont8', 'cont9', 'cont10']\n\nfor col in cols_for_log:\n    train_df[col] = np.log(train_df[col])\n    test_df[col] = np.log(test_df[col])","97350f03":"target = ['target']\nnot_features = ['id', 'target', 'kfold']\ncols = list(train_df.columns)\nfeatures = [feat for feat in cols if feat not in not_features]","447c9a43":"# From https:\/\/github.com\/abhishekkrthakur\/approachingalmost\nclass UnivariateFeatureSelction:\n    def __init__(self, n_features, problem_type, scoring, return_cols=True):\n        \"\"\"\n        Custom univariate feature selection wrapper on\n        different univariate feature selection models from\n        scikit-learn.\n        :param n_features: SelectPercentile if float else SelectKBest\n        :param problem_type: classification or regression\n        :param scoring: scoring function, string\n        \"\"\"\n        self.n_features = n_features\n        \n        if problem_type == \"classification\":\n            valid_scoring = {\n                \"f_classif\": f_classif,\n                \"chi2\": chi2,\n                \"mutual_info_classif\": mutual_info_classif\n            }\n        else:\n            valid_scoring = {\n                \"f_regression\": f_regression,\n                \"mutual_info_regression\": mutual_info_regression\n            }\n        if scoring not in valid_scoring:\n            raise Exception(\"Invalid scoring function\")\n            \n        if isinstance(n_features, int):\n            self.selection = SelectKBest(\n                valid_scoring[scoring],\n                k=n_features\n            )\n        elif isinstance(n_features, float):\n            self.selection = SelectPercentile(\n                valid_scoring[scoring],\n                percentile=int(n_features * 100)\n            )\n        else:\n            raise Exception(\"Invalid type of feature\")\n    \n    def fit(self, X, y):\n        return self.selection.fit(X, y)\n    \n    def transform(self, X):\n        return self.selection.transform(X)\n    \n    def fit_transform(self, X, y):\n        return self.selection.fit_transform(X, y)\n    \n    def return_cols(self, X):\n        if isinstance(self.n_features, int):\n            mask = SelectKBest.get_support(self.selection)\n            selected_features = []\n            features = list(X.columns)\n            for bool, feature in zip(mask, features):\n                if bool:\n                    selected_features.append(feature)\n                    \n        elif isinstance(self.n_features, float):\n            mask = SelectPercentile.get_support(self.selection)\n            selected_features = []\n            features = list(X.columns)\n            for bool, feature in zip(mask, features):\n                if bool:\n                    selected_features.append(feature)\n        else:\n            raise Exception(\"Invalid type of feature\")\n        \n        return selected_features","b3baa671":"ufs = UnivariateFeatureSelction(\n    n_features=0.9,\n    problem_type=\"classification\",\n    scoring=\"f_classif\"\n)\n\nufs.fit(train_df[features], train_df[target].values.ravel())\nselected_features = ufs.return_cols(train_df[features])","d7405980":"def rate_model(clf, x, cv = StratifiedKFold(n_splits=NUM_SPLITS),\n               features=selected_features, target=target):\n    '''\n    Prints out various evaluation metrics for a classification task. Like:-\n    1. Classification Accuracy\n    2. ROC-AUC Score\n    3. Precision\n    4. Recall\n    5. F1 Score\n    All score are calculated in base format. No averaging is performed.\n    \n    clf - Classifier\n    x - Input features\n    cv - Cross Validation criteria\n    features - Feature column names\n    target - Target column name\n    '''\n    \n    scoring = {'acc' : 'accuracy', 'roc' : 'roc_auc', 'precision' : 'precision', 'recall' : 'recall', 'f1' : 'f1'}\n    scores = cross_validate(clf, x[features], x[target].values.ravel(), scoring=scoring, cv=cv, return_train_score=False)\n    roc = np.mean(scores['test_roc'])\n    acc = np.mean(scores['test_acc'])\n    prec = np.mean(scores['test_precision'])\n    rec = np.mean(scores['test_recall'])\n    f1 = np.mean(scores['test_f1'])\n    print(f'ROC: {roc}')\n    print(f'Accuracy: {acc}')\n    print(f'Precision: {prec}')\n    print(f'Recall: {rec}')\n    print(f'F-Score: {f1}')","7718a5f4":"def prepare_submission_one_model(clf, train_df=train_df, test_df=test_df,\n                                 features=selected_features, target=target):\n    '''\n    Prepared the submission.csv file for submitting to the competition.\n    \n    Inputs:-\n    clf - Classifier\n    train_df - Training Dataset\n    test_df - Test Dataset\n    features - Feature column names\n    target - Target column name\n    '''\n    \n    clf.fit(train_df[features], train_df[target].values.ravel())\n    preds = clf.predict_proba(test_df[features])[:, 1]\n    output = pd.DataFrame({'id': test_df['id'],\n                           'target': preds})\n    output.to_csv('submission.csv', index=False)\n    print('Prediction file saved. All the Best!')","eea29867":"NUM_BOOSTERS = 5000\n\nparams = {\n    'booster': 'gbtree',\n    'eval_metric': 'auc',\n    'random_state': RANDOM_SEED,\n    'use_label_encoder': False,\n    'tree_method': 'gpu_hist',\n    'max_depth': 8,\n    'learning_rate': 0.01,\n    'n_estimators': NUM_BOOSTERS,\n    'min_child_weight': 20,\n    'gamma': 0.1,\n    'alpha': 0.2,\n    'lambda': 9,\n    'colsample_bytree': 0.2,\n    'subsample': 0.8,\n    'nthread': -1\n}\n\nclf = xgb.XGBClassifier(**params)\nrate_model(clf, train_df)","e386baac":"NUM_BOOSTERS = 5000\n\nparams = {\n    'booster': 'gbtree',\n    'eval_metric': 'auc',\n    'random_state': RANDOM_SEED,\n    'use_label_encoder': False,\n    'tree_method': 'gpu_hist',\n    'max_depth': 8,\n    'learning_rate': 0.01,\n    'n_estimators': NUM_BOOSTERS,\n    'min_child_weight': 20,\n    'gamma': 0.1,\n    'alpha': 0.2,\n    'lambda': 9,\n    'colsample_bytree': 0.2,\n    'subsample': 0.8,\n    'nthread': -1\n}\n\nclf = xgb.XGBClassifier(**params)\nprepare_submission_one_model(clf)","622c3b5f":"Let's look at the distribution, correlation and target relationship of all the numerical features through a pair plot.","2d7a7ef6":"## Categorical Value Encoding\nCategorical values are of datatype object, and in most of the scenarios those can not be fed directly to any machine learning algorithm. So the process of converting these objects to numbers is called encoding.  \nThere can be two types of categorical features:-\n1. Ordinal - Have order associated with object name (Hot, Cold, Warm)\n2. Nominal - Do not have any order associated (Cake, Bread, Burger)  \n\nAs the ordinality of the features are unknown in our case we will assume that all of them are nominal and proceed accordingly.  \nThere can be two scenarios in categorical varibles (as we know from the EDA):-\n* High Cardinality Features (very high number of rare values)\n* Low Cardinality Features (low number of rare values)\n\nBut despite that, there are some encoders like Target and Catboost encoders which work well with both. We will be using Catboost Encoder here. \n\n### Define Encoders","8d4fa611":"# Approach:-\nMy advice to beginners will be to follow the following workflow for approaching any tabular data for competition:-  \n1. Read the Problem Statement and understand the requirements carefull.\n2. READ THE PROBLEM STATEMENT AND REQUIREMENTS AGAIN.\n3. Look carefully at the Data and do EDA (It will help largely during feature engineering and Inference). It is worth the time.\n4. Decide the poroblem category (Classification\/Regression).\n5. Spit data into Kfolds before doing Feature engineering. Because it's very easy to leak data\/contaminate the validation set during Feature Engineering. Then the validation set is no longer representative of real data.\n6. Build Basic model and record the performance.\n7. Try to improve the performance by Feature engineering, better encoding, synthetic feature creation etc.\n8. Do feature selection to only use the important\/relevant features.\n9. If that saturates or starts to drop, go back and try using other models and see whichone works better.\n10. Tune the hyperparameters for the models which seem to work good as per your observations.\n11. Select some of the best models from previous step and do an ensemble\/stacking-blending.\n12. Submit to the leaderboard and gaze the difference in CV and LB. If it is huge, then most likely there was some overfitting and leakage across folds. Try to identify and rectify the same.\n13. When done, resubmit and you should see a close result.\n14. Not satisfied with result\/Trying to get better rank? Head over to the Discussion Forum and read through interesting discussions and see what others are trying to do. If happy, implement them and gradually start improving your scores and skills.\n\n**Happy Kaggling!**\n\n# Why this Competition?\nThis competition provides an unique oppertunity for Data Science begiiners to participate in a Hackathon style challenge for Data Science. It also provides the unique oppertunities for beginners to get their hands dirty and indulge is practical application of ML and do one of the basic tasks machine learning algorithms are capable of doing:- **Classification**.  \n\nThis competition has the right mix to Catergorical and Numerical features we might expect in a practical problem and this helps us know how to leverage both of thhem in conjugation for a Classification task.\n\n# Problem Statement\nThe goal of this competition is to provide a fun, and approachable for anyone, tabular dataset. These competition will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.  \n\nThe dataset used for this competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.  \n\nSo we are sort of dealing with a variation of actual real-world data and here as Data Scientists are expected to predict the Binary Classification based on these features.\n\n## Data Description:-\nWe have 19 Categorical Features and 11 Continuous Features in the data.\n\n## Expected Outcome:-\n* Build a model to predict the probability of binary class given the information above.\n* Grading Metric: **ROC_AUC_SCORE**\n\n## Problem Category:-\nFrom the data and objective its is evident that this is a **Binary Classification Problem** in the **Tabular Data** format.\n\nSo without further ado, let's now start with some basic imports to take us through this journey:-","815576b2":"* B is a very rare category in this feature.\n* But category B has a very strong correlation with the Target class 1.\n\n## 16. cat14","e9d6fa47":"* There doesnot seem to be a very high definitive relationship between the numeric values despite them having good numbers of correlation coefficient.\n* There are some continuous features which have a long tail, so we can take log of them to diminish the effects of outliers.\n\nThis is close to the basic EDA we could do given that we do not know any feature names and have no idea regarding their relationship for trying bi-variate EDA at this point in time.  \nHaving said that, we can always iterate over unknown features as well to establish\/discover any relationships between them. But we will skip that for now. And back to it later given when we want to further improve our model and need to create new features.","cb4a8158":"* Category B has a strong response rate to Target 1 despite 1 being the lesser class.\n\n## 14. cat12","dbdc4fde":"# Utils  \nBefore creating models, let's create some helper functions which will help us rate the models and not do trivial repetative codes again and again. This is a good practice for keeping your code clean and reusable.","24be8508":"* We can see there are certain categories where the data is very rare and we do not necessarily learn anything new from them.\n* Also there are categories like A which are almost exclusively representing target=0.\n* Even though in the overall dataset there is a imbalance in favour of Target 0 but in certain categories here like L, H and G both the targets are similarly represented.  \n\n## 4. cat2","1c02da10":"* Categtory A is very rare in nature in this feature.\n\n## 21. Numerical Features","9f0a72fe":"# Model Building\nHere we will be building a simple single XGBoost Classifier. As this is a starter notebook, I believe this is enough to get anyone started. You can build upon the same layout and add your own models and evaluate the performance in same way.  \nIn the end after selecting some goof models, you can try doing an ensemble of some best performing models on the dataset.","ec5665f9":"* We can see there are certain categories where the data is very rare and we do not necessarily learn much from them.\n* Category A is the dominant category by far in this feature.\n* Category O gives a good response rate for Target value 1.  \n\n## 5. cat3","c754ae8b":"Another good sign for us \ud83d\ude04.  \nThere are no Null values in either train or test dataset. Loess work for us as we do not have to handle for missing values and do imputation.  \n\n## 1. Class Imbalance","c0a9469c":"## 10. cat8","8185817f":"# EDA\nLet's have a basic look around the data we have at hand first","e6ba5a72":"* There are some very rare categories in this feature.  \n\n## 6. cat4","877ab912":"Let's see what columns we have in the training data.","6b0cee95":"# Features Selection\nWe need to select only the important features for better performance of the model. As unnecessary in best case scenario will not add to any productive calculation of the algorithm or in wrst case scenario 'confuse' the model.  \n\nAlso there is something called the curse of dimensionality associated with mathematical models, which says **\"The curse of dimensionality, indicates that the number of samples needed to estimate an arbitrary function with a given level of accuracy grows exponentially with respect to the number of input variables (i.e., dimensionality) of the function.\"** Hence removing unnecessay\/low-contributing features can actually help the algorithm make better predictions.\n\nTo do the same let's create a wrapper class that has all the built in statistical tests required to perform feature selection and takes some basic inputs from user and spits out the required features.","adbfe2eb":"# Submission","1e670dd1":"## 19. cat17","7eeb3bf7":"# Feature Engineering\nAs we have seen in EDA some features also have very rare ctagories. As a rule we will select all rare classes having representation <0.1% of population and group them together under a new label called 'RARE'. So let's do that first...","bebf9df6":"* Category A is very rare in nature for this feature.\n\n## 20. cat18","bc0e70cc":"# KFold Splits  \nBefore we move on to feature engineering, it is always a good idea to perform cross validation splits. In that way, we will not risk any data leakage and would be more certain of the validation set being aptly represenative of the real world unknown data.","3498b4e0":"* There are some very rare categories in this feature.  \n\n## 11. cat9","d5b0833d":"* There are some rare categories in this feature.  \n\n## 9. cat7","50b1b84a":"* This is a very highly cardinal feature.\n* There are certain categories which are very rare in nature.\n* BI is the most dominant category by far.\n\n## 8. cat6","8fb8880d":"We can see that all of the continuous features are almost on a similar scale of magnitude. Makes job easy for us, as we do not need to do any sort scaling later. Let's see if we have any null values in the dataset...","54ec1baf":"* Category A has a strong response towards Target 0.\n\n## 15. cat15","c157fd01":"## Diminishing Outliers  \nAs seen in EDA, there is a long tail for some numerical values. So, let's take log of the same to diminish the effects of outliers.","0df53682":"* There are some very rare categories in this feature.  \n\n## 13. cat11","04c1d6f3":"The naming scheme makes it obvious which column has what datatype.","39b9bd7a":"* We can say that when cat0 has value A it has a significant response rate as compared to class B.  \n\n## 3. cat1","7e5a696c":"* Category C is very rare in nature.\n* Category B and C have a strong response towards target 0.  \n\n## 18. cat16","99dd82f4":"This was a quick overview\/implementation example of a basic model for tabular data.  \nHope you learnt something from this notebook.  \nI will always keep updating and adding new things to this notebook as and when I come across more algorithms\/Feature engineering\/models worth sharing. So come back for more if you liked this one...\n\n***Also if you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. This keeps me motivated to write and share similar such starter kernels for beginners.***","ceffb024":"* There are some very rare categories in this feature.  \n\n## 7. cat5","32388d89":"Ok, so this is a imbalanced dataset. We have to keep this in mind while developing our models later.  \nNow let's move on to understanding each individual feature through EDA and some visualizations...\n\n## 2. cat0","e02d64b5":"* Category A is by far the dominant category by far.\n* There are some very rare categories in this feature.\n\n## 12. cat10","13429d80":"## 15. cat13"}}