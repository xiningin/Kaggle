{"cell_type":{"c1c07382":"code","955dc6dd":"code","d67c2a94":"code","e3ce0302":"code","7417add5":"code","17adeb5f":"code","d25c3559":"code","45c4b1fc":"code","4f552105":"code","5d987a0b":"code","74d69c01":"code","e2bb23ef":"code","88e952e7":"code","59f2e30c":"code","748b99d0":"code","f779224e":"code","13ffc388":"code","62e74b21":"code","56e09c08":"code","afe4660d":"code","c36defeb":"code","34cc7250":"code","5039453a":"code","7ea31d23":"code","f835e862":"markdown"},"source":{"c1c07382":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tarfile # this is to extract the data from that .tgz file\nimport string\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","955dc6dd":"# get all of the data out of that .tgz\namazon_reviews = tarfile.open('\/kaggle\/input\/amazon-reviews\/amazon_review_polarity_csv.tgz')\namazon_reviews.extractall('data')\namazon_reviews.close()","d67c2a94":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e3ce0302":"import os\nfor dirname, _, filenames in os.walk('.\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7417add5":"# check out what the data looks like before you get started\n# look at the training data set\ntrain_df = pd.read_csv('.\/data\/amazon_review_polarity_csv\/train.csv', header=None)","17adeb5f":"# look at the test data set\ntest_df = pd.read_csv('.\/data\/amazon_review_polarity_csv\/test.csv', header=None)","d25c3559":"train_len=50000\ntest_len=5000\n\ntrain_df = pd.concat([train_df.loc[train_df[0] == 1].sample(train_len\/\/2, random_state=42),\n                      train_df.loc[train_df[0] == 2].sample(train_len\/\/2, random_state=42)]).reset_index(drop=True)\ntest_df = pd.concat([test_df.loc[test_df[0] == 1].sample(test_len\/\/2, random_state=42),\n                      test_df.loc[test_df[0] == 2].sample(test_len\/\/2, random_state=42)]).reset_index(drop=True)","45c4b1fc":"train_df.rename(columns = {0:'class'}, inplace = True)\ntrain_df.rename(columns = {1:'title'}, inplace = True)\ntrain_df.rename(columns = {2:'review'}, inplace = True)","4f552105":"test_df.rename(columns = {0:'class'}, inplace = True)\ntest_df.rename(columns = {1:'title'}, inplace = True)\ntest_df.rename(columns = {2:'review'}, inplace = True)","5d987a0b":"train_df['text']=train_df['title']+' '+train_df['review']\ntest_df['text']=test_df['title']+' '+test_df['review']","74d69c01":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","e2bb23ef":"stop_words = set(stopwords.words('english'))\nREPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))","88e952e7":"def clean_text_data(text):\n    text = text.lower()\n    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n    text = BAD_SYMBOLS_RE.sub('', text)\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n    return text","59f2e30c":"train_df['review'] = train_df['review'].apply(clean_text_data)\ntest_df['review'] = test_df['review'].apply(clean_text_data)","748b99d0":"max_words = 50000\nmax_len = 250\ntokenizer = Tokenizer(num_words=max_words, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(train_df['review'].values)\nword_index = tokenizer.word_index","f779224e":"def tokenize_data(data, max_len):\n    X = tokenizer.texts_to_sequences(data.values)\n    X = pad_sequences(X,maxlen=max_len)\n    return X","13ffc388":"X_train = tokenize_data(train_df['review'], max_len)\nX_test = tokenize_data(test_df['review'], max_len)\n\nY_train = pd.get_dummies(train_df['class']).values\nY_test = pd.get_dummies(test_df['class']).values\n","62e74b21":"embeddings_index = {}\nf = open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","56e09c08":"EMBEDDING_DIM=100\nembedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","afe4660d":"def model(max_features, shape):\n    model = Sequential()\n    model.add(Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=shape,\n                            trainable=False))\n    model.add(Dropout(0.1))\n    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(64,activation='relu'))\n    model.add(Dropout(0.2))\n#model.add(Dense(32,activation='relu'))\n    model.add(Dense(2,activation='softmax'))\n    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n    print(model.summary())\n    return model","c36defeb":"model = model(max_words, X_train.shape[1])","34cc7250":"batch_size = 64\nhistory = model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 1, validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])","5039453a":"res = model.evaluate(X_test, Y_test, verbose = 2)","7ea31d23":"model.predict(tokenize_data(pd.Series([\"Amazing\"]), max_len))","f835e862":"### Starter code to extract data from .tgz file to begin with EDA, buliding NLP models"}}