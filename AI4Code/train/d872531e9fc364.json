{"cell_type":{"fc517fb2":"code","64e49a3c":"code","915df0a2":"code","e9dd4255":"code","c2b77379":"code","d065d94a":"code","8546c5aa":"code","acfc5668":"code","c5077864":"code","73e2167c":"code","227802fb":"code","5ea6126e":"code","c66eef71":"code","7be8ca02":"code","02cc3a03":"code","a12c5d0b":"code","01617960":"code","4d06646f":"code","d15df3d0":"code","45247374":"code","f27943a4":"code","055e88af":"code","35b2bd62":"code","6905da7d":"code","a57219d8":"code","fe70f7d4":"code","8b19a6fe":"markdown","c2c4e274":"markdown","c9e9ff9a":"markdown","ee6f9399":"markdown","670dd944":"markdown","66efda37":"markdown","bc4c5d09":"markdown","583b5621":"markdown","a387050e":"markdown","08201882":"markdown","ebd8cbef":"markdown","3af4c6b8":"markdown","fde687e6":"markdown","1f1f4f59":"markdown","8052561f":"markdown","30000e76":"markdown","e6f3abde":"markdown","d60da05d":"markdown","a0101bb8":"markdown","a775c867":"markdown","85e9de6a":"markdown","1a8f4817":"markdown","99fd7c07":"markdown"},"source":{"fc517fb2":"import warnings\nwarnings.filterwarnings(\"ignore\")","64e49a3c":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\ncolumn_transformer = ColumnTransformer(\"One_Hot_Encoder\",OneHotEncoder(),[0,1,2],remainder=\"passthrough\")","915df0a2":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold,train_test_split\n\n%matplotlib inline\nplt.style.use(\"ggplot\")","e9dd4255":"train_data = pd.read_csv(\"..\/input\/adult-census-income\/adult.csv\")\ntrain = train_data.copy()","c2b77379":"print(\"Train Data Shape : {}\".format(train.shape))","d065d94a":"train.head(2)","8546c5aa":"# remove dot(.) from column names and insert '_'\nfor col in train.columns:\n    new_name = col.replace(\".\",\"_\")\n    train.rename(columns={col:new_name},inplace=True)","acfc5668":"#train data statistics\ntrain_stats = pd.DataFrame()\ntrain_stats[\"Columns\"] = train.columns\ntrain_stats[\"Missing_Values\"] = train.isna().sum().values\ntrain_stats[\"Unique_values\"] = [train[x].nunique() for x in train.columns]\ntrain_stats[\"Column_Type\"] = [train[x].dtypes for x in train.columns]\nskewness = []\nfor col in train.columns:\n    try:\n        skew = train[col].skew()\n        skewness.append(skew)\n    except:\n        skewness.append(\"NA\")\ntrain_stats[\"Skewness\"] = skewness\ntrain_stats","c5077864":"#visualizations\ntrain.hist(figsize=(12,9))\nplt.tight_layout()\nplt.show()","73e2167c":"## Categorical plots for inference. Uncomment and run.\n# categorical = [\"workclass\",\"education\",\"marital_status\",\"occupation\",\"relationship\",\"race\",\"sex\"]\n# cnt = 0\n# for cat in categorical:\n#     plt.figure(figsize=(12,7))\n#     sns.countplot(cat,hue=\"income\",data=train)\n#     plt.show()","227802fb":"#mapping target variable to make it binary\nincome_map = {\"<=50K\":0,\">50K\":1}\ntrain.income = train.income.map(income_map)","5ea6126e":"df = train[[\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education_num\",\"occupation\",\"sex\",\"capital_gain\",\"capital_loss\",\"hours_per_week\",\"income\"]].copy()","c66eef71":"most_frequent = df.capital_gain.value_counts(ascending=False).index[0]\nfreq = df.capital_gain.value_counts(ascending=False)[0]\nprint(\"Percentage of frequency of {} value in \/capital_gain\/ column = {}%\".format(\n    most_frequent,round((freq\/df.capital_gain.value_counts().sum())*100,2)))\n\nmost_frequent = df.capital_loss.value_counts(ascending=False).index[0]\nfreq = df.capital_loss.value_counts(ascending=False)[0]\nprint(\"Percentage of frequency of {} value in \/capital_loss\/ column = {}%\".format(\n    most_frequent,round((freq\/df.capital_loss.value_counts().sum())*100,2)))","7be8ca02":"df = df.drop([\"capital_gain\",\"capital_loss\"], axis=1)","02cc3a03":"X = df.drop(\"income\",axis=1)\ny = df.income","a12c5d0b":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,stratify=y,random_state=3)","01617960":"X_train.reset_index(drop=True,inplace=True)\nX_test.reset_index(drop=True,inplace=True)","4d06646f":"from sklearn.base import BaseEstimator, TransformerMixin\nclass transformation(BaseEstimator, TransformerMixin):\n    def __init__(self,method=\"log2\"):\n        self.method = method\n    def fit(self,X,y=None):\n        #In this case the fit function is empty.\n        return self\n    def transform(self,X):\n        X_ = X.copy()\n        for col in X_.columns:\n            if self.method == \"log2\":\n                X_[col] = np.log2(X_[col])\n            elif self.method == \"log10\":\n                X_[col] = np.log10(X_[col])\n            elif self.method == \"sqrt\":\n                X_[col] = np.sqrt(X_[col])\n            elif self.method == \"cbrt\":\n                X_[col] = np.cbrt(X_[col])\n        return X_","d15df3d0":"# # code for testing the skewness when respective transformations are done on each column\n# for col in [\"age\",\"fnlwgt\",\"education_num\",\"capital_gain\",\"capital_loss\",\"hours_per_week\"]:\n#     print(\"\\nOriginal Skewness for {} is {}\".format(col,df[col].skew()))\n#     for method in [\"log2\",\"log10\",\"sqrt\",\"cbrt\"]:\n#         trans = transformation(method)\n#         trans.fit(df[col])\n#         print(\"\\nSkewness after {} transformation = {}\".format(method,trans.transform(df[col]).skew()))\n#     print(\"\\n\")","45247374":"col_trans = make_column_transformer(\n            (transformation(\"sqrt\"),[\"age\",\"fnlwgt\"]),\n            (OneHotEncoder(sparse=False),[\"workclass\",\"education\",\"occupation\",\"sex\"]),\n            (StandardScaler(),[\"age\",\"fnlwgt\",\"education_num\",\"hours_per_week\"]),\n            remainder = \"passthrough\"\n            )","f27943a4":"#cross folds\ncv = RepeatedStratifiedKFold(n_splits=5,n_repeats=2,random_state=3)\n#model\nlogreg = LogisticRegression(solver=\"lbfgs\")","055e88af":"#defining the steps for the pipeline\nsteps = [\n    (\"Col_trans\",col_trans),\n    (\"Model\",logreg)    \n]\npipe = Pipeline(steps)","35b2bd62":"score_with_pipeline = cross_val_score(pipe,X,y,cv=cv,scoring=\"accuracy\")","6905da7d":"print(\"Mean Accuracy: {:.2f}%\".format(score_with_pipeline.mean()*100))\nprint(\"Standard Deviation among Accuracy scores: {:.6f}\".format(score_with_pipeline.std()))","a57219d8":"#fitting the pipeline\npipe.fit(X_train,y_train)","fe70f7d4":"#predicting on the test data\ny_pred = pipe.predict(X_test[[\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education_num\",\"occupation\",\"sex\",\n                              \"hours_per_week\"]])\nprint(\"Accuracy on the Test Data: {:.2f}%\".format(accuracy_score(y_test,y_pred)*100))","8b19a6fe":"### Did you observe that we didn't actually do any pre-processing on the test data?!\n### The pipeline we made was directly used on the test data without any pre-processing from our side. It was performed by the pipeline itself before predicting the target on the test data. This is a big relief and saves a lot of time as well as makes the implementation look neat.","c2c4e274":"### Transformers and Pipeline can simplify the process of dealing with data, preprocessing it and automating the entire process.\n### A typical approach of dealing with data includes cleaning, scaling, removing and adding features, normalizing and clipping values on the **train data** and then applying the same transfomation on the **test data** before passing it to the trained model.\n### This entire task of performing the same transformations again and again could make the process look messy and vulnerable to commiting errors.\n## This is where Transformers and Pipeline kicks in!\nAll thanks to the good old Skicit-learn, implementing a transformer could be as simple as :","c9e9ff9a":"Obviously the **EDA** could be much better! Real-life data holds a plenty of hidden truths in them. Believe me! I had to force myself not to bring all those up as it would divert us from the **main topic** in hand which is **Transformers and Pipeline**!","ee6f9399":"### It's common to see a dataset with variety of data. Some columns could be nominal, others could be ordinal and a few of them maybe in need of some normalization. If you want to know more about **nominal-ordinal data** or **implementation** of different kinds of **encoding** used [Click Here](https:\/\/www.kaggle.com\/hkalita\/feature-encoding\/notebook). \n#### Hence, we need something that applies the respective transformations only to particular columns.\n### Here comes the concept of Column Transformer.","670dd944":"* Did you observe that the '**fit**' function has a '**return self**' statement?\nIt is the basic layout in any class function of Scikit-learn. It helps in **chaining**. We do observe codes where data is first **fitted** using '**.fit()**' and then **transformed** using '**.transform()**'. This chaining is possible due to the return self in the fit function.\n\n* BaseEstimator and TransformerMixin helps in making sure that the transformer performs well in he pipeline. Moreover, BaseEstimator aids the use of GridSearchCV while tunning the hyperparameters.\n\nYou can check out their source codes here : [BaseEstimator](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/sklearn\/base.py#L151) and [TransformerMixin](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/0fb307bf3\/sklearn\/base.py#L660)","66efda37":"![8314064918_ba4d742e0c_b.jpg](attachment:8314064918_ba4d742e0c_b.jpg)","bc4c5d09":"The **remainder = \"passthrough\"** ensures that only the mentioned columns undergo through the particular transformer and the remaining columns are left as it is.","583b5621":"![disappoint.jpeg](attachment:disappoint.jpeg)","a387050e":"## Do we always have to depend on Scikit-learn for the transformers we plan to use? What if we want a certain transformation in our data which is not present in there? Pack up?!\n\n## Nope! You can customize your own transformer too.\n### However, while constructing the customized transformer, one must keep in mind the format followed by the classes in Scikit-learn so that the transformer can perform as desired.\n\n#### Filling up diesel in a petrol-engine car would definitely result in the malfunctioning of the vehicle right?!\n#### [ DISCLAIMER : Please don't try the above stunt! I am not responsible for any kind of loss, accidents or explosions! ]","08201882":"# IMPORTING LIBRARIES & DATASET","ebd8cbef":"### WHOAA! I like TRANSFORMERS too but NO! I ain't talking about Autobots or Decepticons!\n![WhatsApp%20Image%202020-10-15%20at%2015.30.16.jpeg](attachment:WhatsApp%20Image%202020-10-15%20at%2015.30.16.jpeg)","3af4c6b8":"# TRANSFORMERS & PIPELINES","fde687e6":"Here we will be looking at the construction of a basic transformer performing log2, log10, square root or cube root operations on the data according to the parameter passed.","1f1f4f59":"## If you enjoyed the Kernel do Upvote for support!\n## As this was a basic implementation, in the next kernels I will try to perform Hyperparameter tuning using GridSearchCV on a pipeline.\n\n## HAPPY LEARNING\n\n### If you have any queries regarding the kernel or views on Data Science do reach out to me on [LinkedIn](https:\/\/www.linkedin.com\/in\/hrishikesh-kalita-1aa746173\/).\n\n-Everthing Teaches Us Something","8052561f":"# CUSTOM TRANSFORMER","30000e76":"# DATA CLEANING\/ENCODING\/MODIFICATION","e6f3abde":"# PLOTS","d60da05d":"### Let's explore the **adult-cencus-income** dataset to determine if a person earns more than 50K.","a0101bb8":"Transformers are being widely used in the field of Natural Language Processing (NLP). We will be seeing a small demo of the use of Transformers and Pipeline in this kernel.","a775c867":"### We observed that there is majority issue here. As over 90% of the data in both columns are the same, they won't be adding much information to the model. Hence, we remove them.","85e9de6a":"The above table is just a compilation of a few details about the columns in the dataset. It's a personal taste as I like to view those details in the form of a dataframe.","1a8f4817":"A short breakdown of the code above:\n\n* **\"One_Hot_Encoder\"** : Name of the Transformer. Depends on the user.\n\n* **OneHotEncoder()** : The Transformer Class object to be used.\n\n* **[0,1,2]** : The array indices for transformation. Feature\/column names can also be used.\n\n* **remainder=\"passthrough\"** : To ensure that the transformation is done only to the specified columns and the remaining                           columns are left as it is.","99fd7c07":"# DATA STATISTICS"}}