{"cell_type":{"18053844":"code","23adfbd8":"code","9966fc12":"code","76a5936a":"code","78357bf1":"markdown","7802855d":"markdown","e23950f0":"markdown","516a3ecd":"markdown","7688efbc":"markdown","d19f987b":"markdown","c48753bd":"markdown"},"source":{"18053844":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read Data\ndata = pd.read_csv('..\/input\/melb_data.csv')\ncols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = data[cols_to_use]\ny = data.Price\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\n\n","23adfbd8":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\n\nmy_pipeline = make_pipeline(Imputer(), RandomForestRegressor())","9966fc12":"my_pipeline.fit(train_X, train_y)\npredictions = my_pipeline.predict(test_X)","76a5936a":"my_imputer = Imputer()\nmy_model = RandomForestRegressor()\n\nimputed_train_X = my_imputer.fit_transform(train_X)\nimputed_test_X = my_imputer.transform(test_X)\nmy_model.fit(imputed_train_X, train_y)\npredictions = my_model.predict(imputed_test_X)","78357bf1":"# Understanding Pipelines\nMost scikit-learn objects are either **transformers** or **models.** \n\n**Transformers** are for pre-processing before modeling.  The Imputer class (for filling in missing values) is an example of a transformer.  Over time, you will learn many more transformers, and you will frequently use multiple transformers sequentially. \n\n**Models** are used to make predictions. You will usually preprocess your data (with transformers) before putting it in a model.  \n\nYou can tell if an object is a transformer or a model by how you apply it.  After fitting a transformer, you apply it with the *transform* command.  After fitting a model, you apply it with the *predict* command. Your pipeline must start with transformer steps and end with a model.  This is what you'd want anyway.\n\nEventually you will want to apply more transformers and combine them more flexibly.  We will cover this later in an Advanced Pipelines tutorial.\n\n# Your Turn\nTake your modeling code and convert it to use pipelines.  For now, you'll need to do one-hot encoding of categorical variables outside of the pipeline (i.e. before putting the data in the pipeline).","7802855d":"\nYou have a modeling process that uses an Imputer to fill in missing values, followed by a RandomForestRegressor to make predictions.  These can be bundled together with the **make_pipeline** function as shown below.","e23950f0":"You can now fit and predict using this pipeline as a fused whole.","516a3ecd":"For comparison, here is the code to do the same thing without pipelines","7688efbc":"*This tutorial is part of the [Learn Machine Learning](https:\/\/www.kaggle.com\/learn\/machine-learning) series. In this step, you will learn how and why to use pipelines to clean up your modeling code.* \n\n# What Are Pipelines\n\nPipelines are a simple way to keep your data processing and modeling code organized.  Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.\n\nMany data scientists hack together models without pipelines, but Pipelines have some important benefits. Those include:\n1. **Cleaner Code:** You won't need to keep track of your training (and validation) data at each step of processing.  Accounting for data at each step of processing can get messy.  With a pipeline, you don't need to manually keep track of each step.\n2. **Fewer Bugs:** There are fewer opportunities to mis-apply a step or forget a pre-processing step.\n3. **Easier to Productionize:** It can be surprisingly hard to transition a model from a prototype to something deployable at scale.  We won't go into the many related concerns here, but pipelines can help.\n4. **More Options For Model Testing:** You will see an example in the next tutorial, which covers cross-validation.\n\n---","d19f987b":"# Example\n\nWe won't focus on the data loading. For now, you can imagine you are at a point where you already have train_X, test_X, train_y and test_y. ","c48753bd":"This particular pipeline was only a small improvement in code elegance. But pipelines become increasingly valuable as your data processing becomes increasingly sophisticated."}}