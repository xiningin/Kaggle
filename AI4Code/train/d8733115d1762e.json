{"cell_type":{"016ffdce":"code","08d399b3":"code","eba8222e":"code","0eac4819":"code","b3328d87":"code","fe6d624f":"code","6c7157bb":"code","cfeb816b":"code","892b345f":"code","539dd9a8":"code","b61b8144":"code","a556c084":"code","c5049d13":"code","e2496ad6":"code","b252c18a":"code","492ddc1c":"code","ce91ac1a":"code","b33361da":"code","3a1b2791":"code","3b128ca0":"code","34b5f6eb":"code","d54085fc":"code","ddb6d841":"code","1adbedb9":"code","c76be1f7":"code","e72309b5":"code","e4e58508":"code","f388d101":"code","02959ded":"code","3b615592":"code","25107ad2":"code","d8c3af02":"code","e6c2e405":"code","95dc4729":"code","74c62d47":"code","c76f02ef":"code","ca3b8534":"code","c3728fac":"code","32df0e75":"code","71a8a396":"code","8221f79d":"code","aefdb27c":"code","7627e5e0":"code","8cd447c5":"code","fe7c1d49":"markdown","0a6f67bc":"markdown","77df51ee":"markdown","ae9098c1":"markdown","b78c8796":"markdown"},"source":{"016ffdce":"!pip install turicreate --ignore-installed\n!pip install turicreate --user\n!pip install turicreate","08d399b3":"from __future__ import division\nimport turicreate ","eba8222e":"import numpy as np\nimport pandas as pd\n#import turicreate\nfrom sklearn.preprocessing import OneHotEncoder","0eac4819":"train = turicreate.SFrame.read_csv('..\/input\/alc-datathon-2021\/covid_mental_health_train.csv')","b3328d87":"test = turicreate.SFrame.read_csv('..\/input\/alc-datathon-2021\/covid_mental_health_test.csv')\nlen(test)","fe6d624f":"train_data, validation_data = train.random_split(.6, seed=2)\n","6c7157bb":"import numpy as np\n\ndef get_numpy_data(data_sframe, features, label):\n    data_sframe['intercept'] = 1\n    features = ['intercept'] + features\n    features_sframe = data_sframe[features]\n    feature_matrix = features_sframe.to_numpy()\n    label_sarray = data_sframe[label]\n    label_array = label_sarray.to_numpy()\n    return(feature_matrix, label_array)","cfeb816b":"important_words = ['current_mental', 'past_mental', 'past_physical', 'current_physical', 'optimism', 'deterioration_interact', 'frustration', 'difficulty_work', 'difficulty_living', 'deterioration_economy', 'healthy_sleep']\nfeature_matrix_train, sentiment_train = get_numpy_data(train, important_words, 'depression')\nfeature_matrix_valid, sentiment_valid = get_numpy_data(validation_data, important_words, 'depression')\n","892b345f":"import math","539dd9a8":"def predict_probability(feature_matrix, coefficients):\n    w_h_x = np.dot(feature_matrix, coefficients)\n    predictions = 1\/(1 + np.exp(-w_h_x))\n    return predictions","b61b8144":"def feature_derivative_with_L2(errors, feature, coefficient, l2_penalty, feature_is_constant):\n    derivative = np.dot(errors, feature)\n    if not feature_is_constant:\n        derivative = derivative - 2*l2_penalty*coefficient\n    return derivative","a556c084":"def compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty):\n    indicator = (sentiment==+1)\n    scores = np.dot(feature_matrix, coefficients)\n    \n    lp = np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores))) - l2_penalty*np.sum(coefficients[1:]**2)\n    \n    return lp","c5049d13":"from math import sqrt\n\ndef logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter):\n    coefficients = np.array(initial_coefficients)\n    for itr in range(max_iter):\n        predictions = predict_probability(feature_matrix, coefficients)\n        indicator = (sentiment==+1)\n        errors = indicator - predictions\n        for j in range(len(coefficients)):\n            is_intercept = (j == 0)\n            derivative = feature_derivative_with_L2(errors, feature_matrix[:,j], coefficients[j], l2_penalty, is_intercept)\n            coefficients[j] = coefficients[j] + step_size*derivative\n        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n            lp = compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty)\n            print('iteration %*d: log likelihood of observed labels = %.8f' % \\\n                (int(np.ceil(np.log10(max_iter))), itr, lp))\n    return coefficients","e2496ad6":"coefficients_4_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n                                                     initial_coefficients=np.zeros(12),\n                                                     step_size=5e-6, l2_penalty=4, max_iter=501)","b252c18a":"coefficients_4_penalty","492ddc1c":"prediction = predict_probability(feature_matrix_valid, coefficients_4_penalty)\n","ce91ac1a":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\ndef accuracy_plots(y_score, plot_name, y_true=[int(i) for i in sentiment_valid]):\n    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n    close_zero = np.argmin(np.abs(thresholds))\n    roc_auc = roc_auc_score(y_true, y_score)\n    precision, recall, thresholds_pr = precision_recall_curve(y_true, y_score)\n    close_zero_pr = np.argmin(np.abs(thresholds_pr))\n    \n    fig, ax = plt.subplots(2, 1, figsize=(12,14), sharex=True)\n    plt.tight_layout()\n    plt.subplots_adjust(hspace=0.15, top=0.93)\n    fig.suptitle(plot_name, fontsize=14)\n    ax[0].plot(fpr, tpr, 'b', label='AUC %0.2f' % roc_auc)\n    ax[0].plot(fpr[close_zero], tpr[close_zero], 'o', markersize=12, label=\"threshold zero\", fillstyle=\"none\", c='r', mew=2)\n    ax[0].plot([(0,0),(1,1)], c='r', linestyle='--')\n    ax[0].set_ylabel('True Positive Rate')\n    ax[0].set_xlabel('False Positive Rate')\n    ax[0].set_xlim([0,1])\n    ax[0].set_ylim([0,1])\n    ax[0].legend(loc=4)\n    ax[0].set_title('ROC Curve')\n    ax[1].plot(precision, recall, 'b')\n    ax[1].plot(precision[close_zero_pr], recall[close_zero_pr], 'o', markersize=12, label=\"threshold zero\", fillstyle=\"none\", c='r', mew=2)\n    ax[1].set_ylabel('Recall')\n    ax[1].set_xlabel('Precision')\n    ax[1].set_title('Precision Recall Curve')\n    ax[1].legend(loc=4)\n    plt.show()","b33361da":"\nsubmission = turicreate.SFrame({'Id':validation_data['id'],'depression':prediction})","3a1b2791":"s_c = submission.export_csv('26_Newbies_attempt_LogisticRegression_consideringManyFeatures.csv')","3b128ca0":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_roc_curve\nprint(roc_auc_score(sentiment_valid, prediction))","34b5f6eb":"def get_numpy_data_test(data_sframe, features):\n    data_sframe['intercept'] = 1\n    features = ['intercept'] + features\n    features_sframe = data_sframe[features]\n    feature_matrix = features_sframe.to_numpy()\n    return feature_matrix","d54085fc":"feature_test_matrix = get_numpy_data_test(test, important_words)","ddb6d841":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_roc_curve","1adbedb9":"def get_numpy_data_pred_train(data_sframe, label):\n    label_sarray = data_sframe[label]\n    label_array = label_sarray.to_numpy()\n    return label_array","c76be1f7":"depression_data = turicreate.SFrame.read_csv('..\/input\/alc-datathon-2021\/covid_mental_health_train.csv')","e72309b5":"features = ['current_mental', 'past_mental', 'past_physical', 'current_physical', 'optimism', 'deterioration_interact', 'frustration', 'difficulty_work', 'difficulty_living', 'deterioration_economy', 'healthy_sleep']\ntarget = 'depression'\ndepression_data = depression_data[features + [target]]","e4e58508":"no_dep_raw = depression_data[depression_data[target] == 0]\ndep_raw = depression_data[depression_data[target] == 1]","f388d101":"percentage = len(dep_raw)\/float(len(no_dep_raw))\nno_dep = no_dep_raw.sample(percentage, seed = 1)\ndep = dep_raw\ndep_data = dep.append(no_dep)\n","02959ded":"for feature in features:\n    dep_data_one_hot_encoded = dep_data[feature].apply(lambda x: {x: 1})    \n    dep_data_unpacked = dep_data_one_hot_encoded.unpack(column_name_prefix=feature)\n    \n   \n    for column in dep_data_unpacked.column_names():\n        dep_data_unpacked[column] = dep_data_unpacked[column].fillna(0)\n\n    dep_data = dep_data.remove_column(feature)\n    dep_data = dep_data.add_columns(dep_data_unpacked)\n","3b615592":"features = dep_data.column_names()\nfeatures.remove('depression')  # Remove the response variable\ntrain_data, test_data = dep_data.random_split(.8, seed=1)","25107ad2":"def reached_minimum_node_size(data, min_node_size):\n    if len(data) <= min_node_size:\n        return True\n    else:\n        return False","d8c3af02":"def error_reduction(error_before_split, error_after_split):\n    return (error_before_split - error_after_split)","e6c2e405":"def intermediate_node_num_mistakes(labels_in_node):\n    # Corner case: If labels_in_node is empty, return 0\n    c_pos=0\n    c_neg=0\n    if len(labels_in_node) == 0:\n        return 0\n    for i in range(0, len(labels_in_node)):\n        if labels_in_node[i]==1:\n            c_pos = c_pos +1\n        if labels_in_node[i]==0:\n            c_neg = c_neg +1\n    if c_pos < c_neg:\n        mistakes = c_pos\n    else:\n        mistakes = c_neg\n    return mistakes","95dc4729":"def best_splitting_feature(data, features, target):\n    \n    best_feature = None \n    num_data_points = float(len(data))  \n    best_error = 100\n    \n    for feature in features:\n        \n        left_split = data[data[feature] == 0]\n\n        right_split =  data[data[feature] == 1]\n            \n        left_mistakes = intermediate_node_num_mistakes(left_split[target])            \n        \n        right_mistakes = intermediate_node_num_mistakes(right_split[target])\n        \n        error = (left_mistakes + right_mistakes)\/(num_data_points)\n\n        if error < best_error:\n            best_feature = feature\n            best_error = error\n        \n    \n    return best_feature ","74c62d47":"def create_leaf(target_values):\n    \n    leaf = {'splitting_feature' : None,\n            'left' : None,\n            'right' : None,\n            'is_leaf': True}  \n    num_ones = len(target_values[target_values == +1])\n    num_minus_ones = len(target_values[target_values == 0])\n    \n \n    if num_ones > num_minus_ones:\n        leaf['prediction'] = 1         \n    else:\n        leaf['prediction'] = 0        \n               \n    return leaf ","c76f02ef":"def decision_tree_create(data, features, target, current_depth = 0, \n                         max_depth = 10, min_node_size=1, \n                         min_error_reduction=0.0):\n    remaining_features = features[:] # Make a copy of the features.\n    \n    target_values = data[target]\n    if intermediate_node_num_mistakes(target_values)==0:\n        print('Stopping condition 1 is reached')\n        return create_leaf(target_values)\n    if remaining_features==0:\n        print('Stopping condition 2 is reached')\n        return create_leaf(target_values)\n    \n    # Early stopping condition 1: Reached max depth limit.\n    if current_depth>=max_depth:\n        print('Early stopping condition 1 is reached')\n        return create_leaf(target_values)\n    \n    # Early stopping condition 2: Reached the minimum node size.\n    if reached_minimum_node_size(data, min_node_size):\n        print(\"Early stopping condition 2 reached. Reached minimum node size.\")\n        return create_leaf(target_values)\n    \n    splitting_feature = best_splitting_feature(data, features, target)\n    \n    left_split = data[data[splitting_feature] == 0]\n    right_split = data[data[splitting_feature] == 1]\n    \n    # Early stopping condition 3: Minimum error reduction\n    error_before_split = intermediate_node_num_mistakes(target_values)\/float(len(data))\n    left_mistakes = intermediate_node_num_mistakes(left_split[target])\n    right_mistakes = intermediate_node_num_mistakes(right_split[target])\n    error_after_split = (left_mistakes + right_mistakes)\/float(len(data))\n    if (error_reduction(error_before_split, error_after_split)) <= min_error_reduction:\n        print(\"Early stopping condition 3 reached. Minimum error reduction.\")\n        return create_leaf(target_values)\n    \n    remaining_features.remove(splitting_feature)\n    print(\"Split on feature %s. (%s, %s)\" % (\\\n                      splitting_feature, len(left_split), len(right_split)))\n    \n    left_tree = decision_tree_create(left_split, remaining_features, target, \n                                     current_depth + 1, max_depth, min_node_size, min_error_reduction)        \n    \n    right_tree = decision_tree_create(right_split, remaining_features, target, \n                                     current_depth + 1, max_depth, min_node_size, min_error_reduction)   \n    \n    return {'is_leaf'          : False, \n            'prediction'       : None,\n            'splitting_feature': splitting_feature,\n            'left'             : left_tree, \n            'right'            : right_tree}","ca3b8534":"def count_nodes(tree):\n    if tree['is_leaf']:\n        return 1\n    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])","c3728fac":"def classify(tree, x, annotate = False):\n    if tree['is_leaf']:\n        if annotate: \n            print(\"At leaf, predicting %s\" % tree['prediction'])\n        return tree['prediction']\n    else:\n        split_feature_value = x[tree['splitting_feature']]\n        if annotate: \n            print(\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n        if split_feature_value == 0:\n            return classify(tree['left'], x, annotate)\n        if split_feature_value == 1:\n            \n            return classify(tree['right'], x, annotate)","32df0e75":"my_decision_tree_new = decision_tree_create(train_data, features, 'depression', max_depth = 20, \n                                min_node_size = 10, min_error_reduction=0.0)","71a8a396":"classify(my_decision_tree_new, test_data[0], annotate = True)","8221f79d":"def evaluate_classification_error(tree, data, target):\n    # Apply the classify(tree, x) to each row in your data\n    prediction = data.apply(lambda x: classify(tree, x))\n    #print(prediction)\n    mistakes = intermediate_node_num_mistakes(prediction)\n    classification_error = mistakes\/len(data)\n    return classification_error","aefdb27c":"evaluate_classification_error(my_decision_tree_new, test_data, target)","7627e5e0":"sentiment_train = get_numpy_data_pred_train(train, 'depression')\nprediction_test = predict_probability(feature_test_matrix, coefficients_4_penalty)\nval_pred = predict_probability(feature_matrix_valid, coefficients_4_penalty)\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_roc_curve\nprint(roc_auc_score(sentiment_valid, val_pred))","8cd447c5":"submission_last = pd.DataFrame({'id':test['id'], 'depression':prediction_test})","fe7c1d49":"# Conclusion:","0a6f67bc":"# Hard Code of Decision Tree:","77df51ee":"We have participated in the competition Ada Lovelace Datathon 2021, we managed to score 11 th position in the leaderboard, we are publishing our attempts in the series of notebooks, rest of the notebook links are-\n\n* [EDA and Data Visualization](https:\/\/www.kaggle.com\/nawshadbintanizam\/eda-allfeatures)\n* [Handling Data Imbalance](https:\/\/www.kaggle.com\/erabaka\/classifier-experiments-with-resample)\n* [Classification Using Pre-built Models](https:\/\/www.kaggle.com\/tasnimnishatislam\/26-na-experimentwithclassifiers?fbclid=IwAR15jk4-zEoAPXxQXXgMun-8Ne--LGk5gMalllyGjpcC9BNY_vMFWKR-0Qo)","ae9098c1":"**Hard code of Decision Tree cannot be used to predict probability type output. It can only determine binary classification.**","b78c8796":"# Hard Code of Logistic Regression:"}}