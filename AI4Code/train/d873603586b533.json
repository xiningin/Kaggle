{"cell_type":{"376a3044":"code","bb956659":"code","e07613ec":"code","240a468b":"code","c8ed7722":"code","3f57aa76":"code","7f7924fc":"code","5485ef64":"code","4a9e5b54":"code","56611064":"code","ff94e283":"code","2be5a842":"code","d62024fd":"code","13942f68":"code","d26c7fd5":"code","2409b34d":"code","d5ed0c2e":"code","36f4eb30":"code","bc4a21f7":"code","14a1c64a":"code","78ead183":"code","cc27471b":"code","45ae8ca4":"code","da5d0865":"code","09867a2c":"code","0c969e5b":"code","d8f8dc46":"code","80323a26":"code","056a0e3e":"code","01a2a865":"markdown","4692235f":"markdown","4ba69c3e":"markdown","0306b370":"markdown","7bd41c15":"markdown","bb9cc108":"markdown","a2817dbe":"markdown","2f63feae":"markdown","7170504e":"markdown","66666781":"markdown","63879a36":"markdown","9c84135b":"markdown","1c75b9c0":"markdown","cb687fb3":"markdown","446f4dec":"markdown","5759a8c0":"markdown","a12edb43":"markdown","6ff1fc22":"markdown","8cb7f7fd":"markdown"},"source":{"376a3044":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bb956659":"# Importando arquivo csv para o dataframe\ndf = pd.read_csv('\/kaggle\/input\/hmeq-data\/hmeq.csv')\n\n# Conferindo os tipos de dados \ndf.info()","e07613ec":"# Traduzindo o nome das colunas\ndf.rename(columns={'BAD': 'INADIMPLENTE',\n                  'LOAN': 'VALOR_EMPRESTIMO',\n                  'MORTDUE': 'VALOR_HIPOTECA',\n                  'VALUE': 'VALOR_PROPRIEDADE',\n                  'REASON': 'MOTIVO',\n                  'JOB': 'AREA_OCUPACAO',\n                  'YOJ': 'ANOS_TRABALHO',\n                  'DEROG': 'QTD_PROTESTOS',\n                  'DELINQ': 'QTD_CALOTES',\n                  'CLAGE': 'QTD_MESES_PRIM_EMPRES',\n                  'NINQ': 'QTD_LINHAS_CRED_REC',\n                  'CLNO': 'QTD_LINHAS_CRED',\n                  'DEBTINC': 'IND_COMPROMET_SAL'}, inplace = True)\n\n","240a468b":"# Olhando os dados aleat\u00f3riamente\ndf.sample(15).T","c8ed7722":"# Importando bibliotecas gr\u00e1ficas\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize'] = (11,7)\n\n# Gerando gr\u00e1fico para an\u00e1lise de distribui\u00e7\u00e3o da vari\u00e1vel 'VALOR_EMPR\u00c9STIMO'\nf, ax = plt.subplots(figsize=(15,6))\nsns.distplot(df['VALOR_EMPRESTIMO'], hist = True, kde = True, label='Valor Empr\u00e9stimo')\n\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('Distribui\u00e7\u00e3o de Valores do Empr\u00e9stimo')\nplt.xlabel('Valor em US$')\nplt.ylabel('Frequ\u00eancia')","3f57aa76":"df[df.columns[1:2]].describe().style.format(\"{:.2f}\")","7f7924fc":"# Gerando gr\u00e1fico para an\u00e1lise de distribui\u00e7\u00e3o das vari\u00e1veis 'VALOR_HIPOTECA' e 'VALOR_PROPRIEDADE'\nf, ax = plt.subplots(figsize=(15,6))\nsns.distplot(df['VALOR_HIPOTECA'], hist = True, kde = True, label='Valor Hipoteca')\nsns.distplot(df['VALOR_PROPRIEDADE'], hist = True, kde = True, label='Valor da Propriedade')\n\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('Distribui\u00e7\u00e3o de Vari\u00e1veis')\nplt.xlabel('Valor em US$')\nplt.ylabel('Frequ\u00eancia')","5485ef64":"df_rel_hip_prop = df[df.columns[2:4]].describe()\n\ndf_rel_hip_prop['rel_hip_prop'] = df_rel_hip_prop['VALOR_HIPOTECA'] \/ df_rel_hip_prop['VALOR_PROPRIEDADE']\n\ndf_rel_hip_prop.style.format(\"{:.2f}\")","4a9e5b54":"# Boxplot para an\u00e1lise da rela\u00e7\u00e3o entre o valor do emprestimo e o bom e mal pagador.\nsns.set_style(\"whitegrid\") \n\nsns.boxplot(x = 'INADIMPLENTE', y = 'VALOR_EMPRESTIMO', data = df)","56611064":"df_agrup_inad = df[['VALOR_EMPRESTIMO']].groupby(df['INADIMPLENTE'])\ndf_agrup_inad.describe().T","ff94e283":"df_agrup_inad = pd.crosstab(df.AREA_OCUPACAO, df.INADIMPLENTE)\n\ndf_agrup_inad.div(df_agrup_inad.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True,\n                                                                 title='\u00c1rea de Ocupa\u00e7\u00e3o X Tipo de Pagadores',\n                                                                 figsize=(10,5))\n","2be5a842":"df_prof_inad = pd.crosstab(df['INADIMPLENTE'],df['AREA_OCUPACAO'])\ndf_prof_inad = df_prof_inad.T\ndf_prof_inad = df_prof_inad[1]\n\n\nplt.pie(df_prof_inad, colors=['b', 'g', 'r', 'c', 'm', 'y'], \n        labels= df_prof_inad.index,explode=(0, 0, 0.2, 0.2, 0, 0),\n        autopct='%1.1f%%',\n        counterclock=False, shadow=True)\n\nplt.title('Propor\u00e7\u00e3o de Maus Pagadores por \u00c1rea de Ocupa\u00e7\u00e3o')\nplt.legend(df_prof_inad.index,loc=3)\nplt.show()","d62024fd":"# Explorando a correla\u00e7\u00e3o entre as vari\u00e1veis\n\nf, ax = plt.subplots(figsize=(15,6))\nsns.heatmap(df.corr(), annot=True, fmt='.2f', linecolor='white', ax=ax, lw=.7)","13942f68":"# Gerando gr\u00e1fico para an\u00e1lise de distribui\u00e7\u00e3o da vari\u00e1vel 'INADIMPLENTE'\n\nax = sns.countplot(y=\"INADIMPLENTE\", data=df)\nplt.title('An\u00e1lise da quantidade de inadimplentes')\nplt.xlabel('QUANTIDADE')\n\ntotal = len(df['INADIMPLENTE'])\nfor p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_width()\/total)\n        x = p.get_x() + p.get_width() + 0.02\n        y = p.get_y() + p.get_height()\/2\n        ax.annotate(percentage, (x, y))\n\nplt.show()","d26c7fd5":"df_inad_1 = df[df['INADIMPLENTE'] == 1]\ndf_inad_2 = df[df['INADIMPLENTE'] == 0]\n\nmed_1 = pd.DataFrame(df_inad_1.mean())\nmed_1.rename(columns = {0:'Mau Pagador'}, inplace=True)\nmed_2 = pd.DataFrame(df_inad_2.mean())\nmed_2.rename(columns = {0:'Bom Pagador'}, inplace=True)\n\ndf_med_inad = pd.concat([med_1, med_2], axis=1, join='inner')\n\n\ndf_med_inad[\"rel_mau_bom\"] = df_med_inad['Mau Pagador'] \/ df_med_inad['Bom Pagador']\n\ndf_med_inad","2409b34d":"# Substituindo valores nulos\n\ndf_subst_na = df.copy()\n\ndf_subst_na['VALOR_HIPOTECA'] = df_subst_na['VALOR_HIPOTECA'].fillna(0)\ndf_subst_na['VALOR_PROPRIEDADE'] = df_subst_na['VALOR_PROPRIEDADE'].fillna(0)\ndf_subst_na['ANOS_TRABALHO'] = df_subst_na['ANOS_TRABALHO'].fillna(0)\ndf_subst_na['QTD_PROTESTOS'] = df_subst_na['QTD_PROTESTOS'].fillna(0)\ndf_subst_na['QTD_CALOTES'] = df_subst_na['QTD_CALOTES'].fillna(0)\ndf_subst_na['QTD_MESES_PRIM_EMPRES'] = df_subst_na['QTD_MESES_PRIM_EMPRES'].fillna(0)\ndf_subst_na['QTD_LINHAS_CRED_REC'] = df_subst_na['QTD_LINHAS_CRED_REC'].fillna(0)\ndf_subst_na['QTD_LINHAS_CRED'] = df_subst_na['QTD_LINHAS_CRED'].fillna(0)\ndf_subst_na['IND_COMPROMET_SAL'] = df_subst_na['IND_COMPROMET_SAL'].fillna(0)\n\ndf_subst_na['MOTIVO'] = df_subst_na['MOTIVO'].fillna('Not filled')\ndf_subst_na['AREA_OCUPACAO'] = df_subst_na['AREA_OCUPACAO'].fillna('Not filled')\n\n\n# Convertendo colunas de valores para float\ndf_subst_na['VALOR_EMPRESTIMO'] = df_subst_na['VALOR_EMPRESTIMO'].astype(float)\n\n# Convertendo colunas para inteiro\ndf_subst_na['QTD_PROTESTOS'] = df_subst_na['QTD_PROTESTOS'].astype(int)\ndf_subst_na['QTD_CALOTES'] = df_subst_na['QTD_CALOTES'].astype(int)\ndf_subst_na['QTD_LINHAS_CRED_REC'] = df_subst_na['QTD_LINHAS_CRED_REC'].astype(int)\ndf_subst_na['QTD_LINHAS_CRED'] = df_subst_na['QTD_LINHAS_CRED'].astype(int)","d5ed0c2e":"# Identificando valores NA\ndf_subst_na.isna().sum()","36f4eb30":"df_subst_na.info()","bc4a21f7":"# Convertendo as colunas categ\u00f3ricas em dummies\ndf_subst_na = pd.get_dummies(df_subst_na, columns=['MOTIVO','AREA_OCUPACAO'])\n\ndf_subst_na.info()","14a1c64a":"# Separando o dataframe em train e test\n\n# Importando o train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# primeiro, train e test\ntrain, test = train_test_split(df_subst_na, test_size=0.2, random_state=42)\n\n\ntrain.shape, test.shape","78ead183":"# definindo as colunas de entrada\n\nfeats = [c for c in df_subst_na.columns if c not in ['INADIMPLENTE']]","cc27471b":"# Importando bibliotecas\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Instanciando o modelo\nrf = RandomForestClassifier(n_estimators=150, random_state=42)\n\n# treinar o modelo\nrf.fit(train[feats], train['INADIMPLENTE'])\n\n# Verificar a acur\u00e1cia com a massa de teste\nresultado = accuracy_score(test['INADIMPLENTE'], rf.predict(test[feats]))\n\n# Guardando resultado no dataframe\nresultados = pd.DataFrame([{'tipo':'rf', 'modo':'single','resultado':resultado}])\n\n\n\n# Feature Importance com RF\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()\n\n","45ae8ca4":"# Importando bibliotecas\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbm = GradientBoostingClassifier(n_estimators=200, learning_rate=1.0, max_depth=1, random_state=42)\ngbm.fit(train[feats], train['INADIMPLENTE'])\n\n# Verificando a acur\u00e1cia com a massa de teste\nresultado = accuracy_score(test['INADIMPLENTE'], gbm.predict(test[feats]))\n\n# Guardando resultado no dataframe\nresultados.loc[1] = ['gbm', 'single',resultado]\n\n\n\n# Feature Importance com GBM\npd.Series(gbm.feature_importances_, index=feats).sort_values().plot.barh()\n\n","da5d0865":"# Importando Bibliotecas\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier(n_estimators=200, learning_rate=0.09, random_state=42)\nxgb.fit(train[feats], train['INADIMPLENTE'])\n\n# Verificando a acur\u00e1cia com a massa de teste\nresultado = accuracy_score(test['INADIMPLENTE'], xgb.predict(test[feats]))\n\n# Guardando resultado no dataframe\nresultados.loc[2] = ['xgb','single',resultado]\n\n\n# Feature Importance com XGBoost\npd.Series(xgb.feature_importances_, index=feats).sort_values().plot.barh()\n","09867a2c":"# Importando bibliotecas\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(rf, train[feats], train['INADIMPLENTE'], n_jobs=-1, cv=8)\n\n# Guardando resultado no dataframe\nresultados.loc[3] = ['rf','cross-validation',scores.mean()]\n\n# Feature Importance com RF\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","0c969e5b":"scores = cross_val_score(gbm, train[feats], train['INADIMPLENTE'], n_jobs=-1, cv=7)\n\n# Guardando resultado no dataframe\nresultados.loc[4] = ['gbm','cross-validation',scores.mean()]\n\n\n# Feature Importance com GBM\npd.Series(gbm.feature_importances_, index=feats).sort_values().plot.barh()","d8f8dc46":"scores = cross_val_score(xgb, train[feats], train['INADIMPLENTE'], n_jobs=-1, cv=9)\n\n# Guardando resultado no dataframe\nresultados.loc[5] = ['xgb','cross-validation',scores.mean()]\n\n\n# Feature Importance com XGBoost\npd.Series(xgb.feature_importances_, index=feats).sort_values().plot.barh()\n","80323a26":"resultados.sort_values('resultado', ascending=False)","056a0e3e":"# importando a biblioteca para plotar o gr\u00e1fico de Matriz de Confus\u00e3o\nimport scikitplot as skplt\n\npreds_test = xgb.predict(test[feats])\n\n# Matriz de confus\u00e3o - Dados de test\nskplt.metrics.plot_confusion_matrix(test['INADIMPLENTE'], preds_test)","01a2a865":"### Resultados\n\nO melhor modelo de predi\u00e7\u00e3o dos dados para os testes realizados acima foi o XGB com mais de 92% de acur\u00e1cia. Ainda assim o matriz de confus\u00e3o mostra que a propor\u00e7\u00e3o de erros para o mal pagador foi de aproximadamente 12%. A vari\u00e1vel que mais contribuiu para todas as an\u00e1lises foi a 'IND_COMPROMET_SAL', indicando que \u00e9 determinante para os bons e maus pagadores quanto dinheiro n\u00e3o comprometido ele disp\u00f5e mensalmente.","4692235f":"#### Featuring Engineering\n\nPara o preenchimento dos valores nulos, decidi adotar o valor zero (0), para os campos num\u00e9ricos, tendo como premissa que estas linhas s\u00e3o importantes para o resultado da an\u00e1lise e supondo que os valores em branco n\u00e3o foram declarados devido ao cliente n\u00e3o possuir informa\u00e7\u00e3o \u00e0 declarar.\n\nPara as vari\u00e1veis categ\u00f3ricas, optei por utilizar o classificador 'Not Filled', (n\u00e3o preenchido) para que o modelo tamb\u00e9m possa fazer previs\u00f5es para este tipo de situa\u00e7\u00e3o. Em seguida, transformei estas colunas em *dummies* para adequa\u00e7\u00e3o aos modelos de predi\u00e7\u00e3o.","4ba69c3e":"# **Data Mining e Machine Learning II**\n\n\n## *Trabalho Final de Disciplina *\n\n\n*Aluno*: Diogo Mattioli Neiva\n\n","0306b370":"#### Cross Validation - GBM\n\nNeste modelo, optei por dividir os dados de treino em 8 conjuntos. A vari\u00e1vel 'IND_COMPROMET_SAL' foi respons\u00e1vel por mais de 80% para o resultado do modelo.\n","7bd41c15":"#### Identificando o perfil do mau pagador\n\nEm m\u00e9dia, at\u00e9 75% dos maus pagadores pegaram empr\u00e9stimos de at\u00e9 US$21,700.00. Este valor \u00e9 ligeiramente menor que o valor requerido pelos bons pagadores. No boxplot abaixo \u00e9 poss\u00edvel verificar que n\u00e3o h\u00e1 uma diferen\u00e7a muito grande com rela\u00e7\u00e3o aos valores solicitados no empr\u00e9stimo para os bons e maus pagadores. ","bb9cc108":"#### Verificando a correla\u00e7\u00e3o entre as vari\u00e1veis\n\nAqui foi poss\u00edvel confirmar a rela\u00e7\u00e3o entre o valor do empr\u00e9stimo e o valor da hipoteca conforme mensionado na  an\u00e1lise de distribui\u00e7\u00e3o. H\u00e1 tamb\u00e9m uma correla\u00e7\u00e3o leve entre as colunas 'QTD_PROTESTOS' e 'INADIMPLENTE', e 'QTD_CALOTES' e 'INADIMPLENTE'. S\u00e3o sinais de que o hist\u00f3rico do pagador tem certa relev\u00e2ncia na identifica\u00e7\u00e3o dos maus pagadores.","a2817dbe":"Ao analisar a \u00c1rea de Ocupa\u00e7\u00e3o dos maus pagadores, ficou claro um ter\u00e7o dos profissionais da \u00e1rea de vendas (Sales) s\u00e3o maus pagadores. Embora este n\u00famero seja relevante, a maioria absoluta dos maus pagadores s\u00e3o gerentes e profissionais de outras \u00e1reas correspondendo a 60% do total de caloteiros.","2f63feae":"#### Separando o dataframe\n\nO data frame foi dividido em duas por\u00e7\u00f5es, uma para treinamento e outra para teste. A propor\u00e7\u00e3o entre as bases ficou na rela\u00e7\u00e3o 80\/20 para treino e teste respectivamente.","7170504e":"#### Cross Validation - Random Forest\n\nNeste modelo, optei por dividir os dados de treino em 8 conjuntos. A vari\u00e1vel 'IND_COMPROMET_SAL' foi respons\u00e1vel por mais de 25% para o resultado do modelo.","66666781":"#### Cross Validation - XGBoost\n\nNeste modelo, optei por dividir os dados de treino em 8 conjuntos. A vari\u00e1vel 'IND_COMPROMET_SAL' foi respons\u00e1vel por mais de 30% para o resultado do modelo.","63879a36":"## 1. Introdu\u00e7\u00e3o\n\nCom este trabalho busca-se identificar o melhor modelo para predi\u00e7\u00e3o de bons e maus pagadores com base em um hist\u00f3rico de clientes de um banco que empresta dinheiro tendo como garantia a propriedade do contratante (hipot\u00e9ca).\n\nSer\u00e3o utilizadas t\u00e9cnicas para melhorar a predi\u00e7\u00e3o, tais como:\n\n* Featuring Engeneering\n* Machine Learning\n* Cross Validation\n\nAo final ser\u00e1 poss\u00edvel saber qual o melhor modelo e configura\u00e7\u00e3o de par\u00e2metros obteve o melhor resultado .\n","9c84135b":"## Predi\u00e7\u00e3o\n\n### Ajustando dados\n\nA an\u00e1lise da vari\u00e1vel resposta mostra que a massa de dados est\u00e1 desbalanceada com uma propor\u00e7\u00e3o de 80\/20. As vari\u00e1veis 'QTD_PROTESTOS', 'QTD_CALOTES', 'QTD_LINHAS_CRED', e 'IND_COMPROMET_SAL possuem m\u00e9dias bem diferentes, o que pode possivelmente ajudar a identificar os maus pagadores. Para as demais vari\u00e1veis, a m\u00e9dia dos valores est\u00e3o parecidas e s\u00e3o suficientes para a an\u00e1lise.\n\n","1c75b9c0":"### An\u00e1lises de Distribui\u00e7\u00e3o\n\n#### Valor do Empr\u00e9stimo\n\nA an\u00e1lise de distribui\u00e7\u00e3o da vari\u00e1vel 'VALOR_EMPRESTIMO' revela que a m\u00e9dia do valor emprestado gira em torno de U$S18,600.00, embora hajam empr\u00e9stimos de at\u00e9 quase US$90,000.00. Entretanto, tr\u00eas quartos dos empr\u00e9stimos giram em torno de R$23,000.00. ","cb687fb3":"#### GBM\n\nNeste modelo foram escolhidos os seguintes par\u00e2metros:\n* n_estimators=200\n* learnin_rate=1.0\n* max_depth=1\n\nEstes par\u00e2metros resultaram em um bom desempenho e a acur\u00e1cia do modelo tamb\u00e9m ficou razo\u00e1vel. A vari\u00e1vel mais importante para o modelo foi a 'IND_COMPROMET_SAL' com mais de 80% de contribui\u00e7\u00e3o.","446f4dec":"#### Random Forest\n\nAp\u00f3s alguns testes, o n\u00famero adequado para o par\u00e2metro 'n_estimators' foi 150. A vari\u00e1vel mais significativa para este modelo foi a 'IND_COMPROMET_SAL', indicando que a disponibilidade de recursos financeiros dos clientes \u00e9 determinante para quase 30% dos casos.","5759a8c0":"#### XGBoost\n\nNeste modelo foram escolhidos os seguintes par\u00e2metros:\n* n_estimators=200\n* learnin_rate=0.9\n* max_depth=1\n\nEstes par\u00e2metros resultaram em um bom desempenho e a acur\u00e1cia do modelo tamb\u00e9m ficou razo\u00e1vel. A vari\u00e1vel mais importante para o modelo foi a 'IND_COMPROMET_SAL' com mais de 30% de contribui\u00e7\u00e3o.","a12edb43":"#### Valor da Hipoteca e Valor da Propriedade\n\nAnalisando estas vari\u00e1veis, foi poss\u00edvel confirmar que em m\u00e9dia, o valor da hipoteca \u00e9 72% do valor das propriedades. Metade dos solicitantes de empr\u00e9stimo t\u00eam propriedades que valem at\u00e9 US$90,000.00","6ff1fc22":"## 2. An\u00e1lises Explorat\u00f3rias\n\n### Conhecendo os dados\n\nPara criar familiaridade dom os dados decidi traduzir o cabe\u00e7alho das colunas. Isso facilita o entendimento do material a se trabalhar. Em seguida, foram identificados em v\u00e1rias colunas valores nulos que precisar\u00e3o ser tratados posteriormente. ","8cb7f7fd":"## Conclus\u00e3o\n\nAo utilizar os diversos modelos e diversas configura\u00e7\u00f5es dos par\u00e2metros, foi poss\u00edvel identificar a varia\u00e7\u00e3o do resultado de cada configura\u00e7\u00e3o. Acredito que com utilizando outras t\u00e9cnicas para um melhor balanceamento da massa de dados tais como *resample* ou outras t\u00e9cnicas possa melhorar a capacidade de predi\u00e7\u00e3o."}}