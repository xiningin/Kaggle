{"cell_type":{"e65d37e3":"code","7ddce556":"code","315b5c0a":"code","32ccb557":"code","1b473136":"code","9572ef92":"code","0e2266bf":"code","49d87778":"code","c1ec60e1":"code","1ec54dd2":"code","670e4ffb":"markdown","b773d1c5":"markdown","c345d221":"markdown","a1676526":"markdown","470df67a":"markdown","f78c6b61":"markdown","9dd62182":"markdown","0e364b9b":"markdown","04dbb4ee":"markdown","8c05be08":"markdown","1608fac8":"markdown"},"source":{"e65d37e3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader as DL\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils import weight_norm as WN\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, f1_score\n\nfrom time import time\nimport random as r\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nn_folds = 4\n\nsc_X = StandardScaler()","7ddce556":"def breaker():\n    print(\"\\n\" + 30*\"-\" + \"\\n\")\n\ndef head(x, no_of_ele=5):\n    breaker()\n    print(x[:no_of_ele])\n    breaker()","315b5c0a":"dataset = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\n\nbreaker()\nprint(dataset.shape)\nbreaker()\n\nX = dataset.iloc[:, :-1].copy().values\ny = dataset.iloc[:, -1].copy().values\n\nX = sc_X.fit_transform(X)\nnum_features = X.shape[1]\nnum_obs_test = X.shape[0]\n\ndel dataset","32ccb557":"class DS(Dataset):\n    def __init__(this, X=None, y=None, mode=\"train\"):\n        this.X = X\n        this.mode = mode\n        if mode == \"train\":\n            this.y = y\n        \n    def __len__(this):\n        return this.X.shape[0]\n    \n    def __getitem__(this, idx):\n        if this.mode == \"train\":\n            return torch.FloatTensor(this.X[idx]), torch.FloatTensor(this.y[idx])\n        else:\n            return torch.FloatTensor(this.X[idx])","1b473136":"class ANN_CFG():\n    batch_size = 64\n    epochs = 50\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    IL = num_features\n    HL = [128, 128]\n    OL = 1\n    \ncfg = ANN_CFG()","9572ef92":"class ANN(nn.Module):\n    def __init__(this, IL=None, HL=None, OL=None):\n        super(ANN, this).__init__()\n        \n        this.DP_1 = nn.Dropout(p=0.2)\n        this.DP_2 = nn.Dropout(p=0.5)\n        \n        this.BN1 = nn.BatchNorm1d(IL)\n        this.FC1 = WN(nn.Linear(IL, HL[0]))\n        \n        this.BN2 = nn.BatchNorm1d(HL[0])\n        this.FC2 = WN(nn.Linear(HL[0], HL[1]))\n        \n        this.BN3 = nn.BatchNorm1d(HL[1])\n        this.FC3 = WN(nn.Linear(HL[1], OL))\n    \n    def getOptimizer(this):\n        return optim.Adam(this.parameters(), lr=1e-3, weight_decay=0)\n    \n    def forward(this, x):\n        x = this.BN1(x)\n        x = F.relu(this.FC1(x))\n        x = this.BN2(x)\n        x = F.relu(this.FC2(x))\n        x = this.BN3(x)\n        x = torch.sigmoid(this.FC3(x))\n        return x","0e2266bf":"def train_fn(X=None, y=None, n_folds=None):\n    breaker()\n    print(\"Training...\")\n    LP = []\n    name_getter = []\n    fold = 0\n    bestLoss = {\"train\" : np.inf, \"valid\" : np.inf}\n    \n    start_time = time()\n    breaker()\n    for tr_idx, va_idx in KFold(n_splits=n_folds, shuffle=True, random_state=0).split(X, y):\n        print(\"Processing Fold {fold} ...\".format(fold=fold+1))\n        X_train, X_valid, y_train, y_valid = X[tr_idx], X[va_idx], y[tr_idx], y[va_idx]\n        \n        tr_data_setup = DS(X_train, y_train.reshape(-1,1))\n        va_data_setup = DS(X_valid, y_valid.reshape(-1,1))\n        \n        dataloaders = {\"train\" : DL(tr_data_setup, batch_size=cfg.batch_size, shuffle=True, generator=torch.manual_seed(0)),\n                       \"valid\" : DL(va_data_setup, batch_size=cfg.batch_size, shuffle=False)\n                      }\n        \n        torch.manual_seed(0)\n        model = ANN(cfg.IL, cfg.HL, cfg.OL)\n        model.to(cfg.device)\n        \n        optimizer = model.getOptimizer()\n        \n        for e in range(cfg.epochs):\n            epochLoss = {\"train\" : 0, \"valid\" : 0}\n            for phase in [\"train\", \"valid\"]:\n                if phase == \"train\":\n                    model.train()\n                else:\n                    model.eval()\n                lossPerPass = 0\n                \n                for feat, lbls in dataloaders[phase]:\n                    feat, lbls = feat.to(cfg.device), lbls.to(cfg.device)\n                    \n                    optimizer.zero_grad()\n                    with torch.set_grad_enabled(phase == \"train\"):\n                        output = model(feat)\n                        loss   = nn.BCELoss()(output, lbls)\n                        if phase == \"train\":\n                            loss.backward()\n                            optimizer.step()\n                    lossPerPass += (loss.item()\/lbls.shape[0])\n                epochLoss[phase] = lossPerPass\n            LP.append(epochLoss)\n            if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n                bestLoss = epochLoss\n                name = \".\/Model_Fold_{fold}.pt\".format(fold=fold)\n                name_getter.append(name)\n                torch.save(model.state_dict(), name)\n        fold += 1\n    \n    breaker()\n    print(\"Time taken to train {fold} folds for {e} epochs : {:.2f} minutes\".format((time()-start_time)\/60, fold=n_folds, e=cfg.epochs))\n    breaker()\n    print(\"Best Loss :\", repr(bestLoss))\n    breaker()\n    print(\"Training Complete\")\n    breaker()\n    \n    return LP, name_getter, model\n\ndef eval_fn(model=None, names=None, dataloader=None):\n    final_Pred = np.zeros((num_obs_test, 1))\n    \n    for name in names:\n        Pred = torch.zeros(cfg.batch_size, 1).to(cfg.device)\n        model.load_state_dict(torch.load(name))\n        model.eval()\n        for X, y in dataloader:\n            X = X.to(cfg.device)\n            with torch.no_grad():\n                Prob = model(X)\n            Pred = torch.cat((Pred, Prob), dim=0)\n        Pred = Pred[cfg.batch_size:]\n        Pred = Pred.cpu().numpy()\n        final_Pred = np.add(final_Pred, Pred)\n        \n    final_Pred = np.divide(final_Pred, len(names))\n    final_Pred[np.argwhere(final_Pred > 0.5)[:, 0]]  = int(1)\n    final_Pred[np.argwhere(final_Pred <= 0.5)[:, 0]] = int(0)\n    return final_Pred.reshape(-1)","49d87778":"LP, Names, Network = train_fn(X=X, y=y, n_folds=n_folds)","c1ec60e1":"LPV = []\nLPT = []\nfor i in range(len(LP)):\n  LPT.append(LP[i][\"train\"])\n  LPV.append(LP[i][\"valid\"])\n\nxAxis = [i+1 for i in range(cfg.epochs)]\nplt.figure(figsize=(20, 25))\nfor fold in range(n_folds):\n    plt.subplot(n_folds, 1, fold+1)\n    plt.plot(xAxis, LPT[fold*cfg.epochs:(fold+1)*cfg.epochs], \"b\", label=\"Training Loss\")\n    plt.plot(xAxis, LPV[fold*cfg.epochs:(fold+1)*cfg.epochs], \"r--\", label=\"Validation Loss\")\n    plt.legend()\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Fold {fold}\".format(fold=fold+1))\nplt.show()","1ec54dd2":"ts_data_setup = DS(X, y.reshape(-1,1))\nts_data = DL(ts_data_setup, batch_size=cfg.batch_size, shuffle=False)\n\ny_pred = eval_fn(Network, Names, ts_data)\n\nbreaker()\nprint(\"Accuracy : {:.5f} %\".format(accuracy_score(y, y_pred) * 100))\nbreaker()","670e4ffb":"# Helper Functions","b773d1c5":"**Dataset Template**","c345d221":"**Setup**","a1676526":"# Data Handling","470df67a":"**ANN Helpers**","f78c6b61":"**Evaluation**","9dd62182":"# Library Imports","0e364b9b":"**Config**","04dbb4ee":"**Training**","8c05be08":"**Plots**","1608fac8":"# ANN"}}