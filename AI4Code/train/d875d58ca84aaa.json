{"cell_type":{"0f95a13a":"code","b587c287":"code","339e35c2":"code","a04cfca6":"code","50c1d36b":"code","a92cc2ac":"code","3292e922":"code","8f089fa0":"code","056d5421":"code","978ac474":"code","c0b6b146":"code","debfef04":"code","b0d55d50":"code","cffe3975":"code","2af26788":"code","203b8729":"code","08e54abc":"code","8fca68bb":"code","10ad5f5b":"code","47b15e6e":"code","a27b0c0a":"code","812233a3":"code","3b588ebf":"code","0b910a5b":"code","910b9719":"code","c72c7fbd":"code","1d84f193":"code","1b962d28":"code","ae33f476":"code","05b7c3fc":"code","c1b42f6b":"code","63fd7042":"code","a74593c7":"code","3f0de123":"code","5acb5da8":"code","4650c031":"code","f3663316":"code","33bedbaf":"code","5bc17408":"code","d650f4ad":"code","3f98c733":"code","e64b6e6d":"code","c1c578fd":"code","678162f0":"code","a2db9102":"code","67182944":"code","e7e22a4e":"code","3204e5b8":"code","9f2e9d03":"code","d5739c42":"code","f273237e":"code","05817605":"code","233c813a":"code","8ca292f4":"code","21ef344c":"code","ac2c406a":"markdown","c8804d5e":"markdown","2746423f":"markdown","fa06f69e":"markdown","488f52ea":"markdown","dbea12ce":"markdown","e442ec2a":"markdown","14fafb56":"markdown","9eae883e":"markdown","91953082":"markdown","63040816":"markdown","de332206":"markdown","a5b1aca5":"markdown","4fa19823":"markdown","a21a386c":"markdown","5ec03799":"markdown","fec9d481":"markdown","6c5eeda8":"markdown","d51d1c9a":"markdown","e29b41c0":"markdown","0e317f01":"markdown","2e15379c":"markdown","1bfb1510":"markdown","299e30df":"markdown","bfa50847":"markdown","9417f033":"markdown","4578fa5b":"markdown","2061bc0e":"markdown","f20e4b35":"markdown","c0925bea":"markdown","e35e939b":"markdown","d09a3aaa":"markdown","fdbca681":"markdown","3e768a6f":"markdown","a65b674a":"markdown","d5d48d8a":"markdown","a875acf8":"markdown","86c21e4a":"markdown","7aaa3405":"markdown","7dca3e00":"markdown","f8b935c7":"markdown","d622b9ed":"markdown","f3c3972e":"markdown","972ee81b":"markdown","43647e80":"markdown","4ac24ca2":"markdown","ad5feadb":"markdown","d3b5bc2d":"markdown","d262dfc8":"markdown","af77dc50":"markdown","215a086a":"markdown","8657c4ec":"markdown","de953658":"markdown","6d76e744":"markdown","37a767ce":"markdown","6a02bef3":"markdown","9f1de853":"markdown","ab1b7e22":"markdown","d4dda8ab":"markdown","8ad4667b":"markdown","efb8d293":"markdown","915f1797":"markdown","fd994175":"markdown","7d4e5240":"markdown"},"source":{"0f95a13a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b587c287":"import os\nimport numpy as np \nimport pandas as pd \nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.express as px\nplotly.offline.init_notebook_mode (connected = True)\nwarnings.filterwarnings(\"ignore\")","339e35c2":"df = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf.head(10).style.set_table_styles(\n[{'selector': 'th',\n  'props': [('background', '#2CE1B0 '), \n            ('color', 'white'),\n            ('font-size', 150),\n            ('font-family', 'verdana')]},\n \n {'selector': 'td',\n  'props': [('font-family', 'verdana')]},\n\n {'selector': 'tr:nth-of-type(odd)',\n  'props': [('background', '#DCDCDC')]}, \n \n {'selector': 'tr:nth-of-type(even)',\n  'props': [('background', 'white')]},\n \n {'selector': 'tr:hover',\n  'props': [('background-color', '#ED8B16 ')]}\n \n]\n).hide_index()\n\n","a04cfca6":"df.shape","50c1d36b":"df.describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","a92cc2ac":"df.dtypes","3292e922":"# Convert features with object data type into string\nstring_col = df.select_dtypes(include=\"object\").columns\ndf[string_col]=df[string_col].astype(\"string\")\n","8f089fa0":"df.dtypes","056d5421":"df.isnull().sum()","978ac474":"categorical = ['Sex', 'ChestPainType', 'FastingBS', 'RestingECG', 'ExerciseAngina', \n               'ST_Slope', 'HeartDisease']\nplt.style.use('ggplot')\nsns.countplot(x=categorical[0],data=df)","c0b6b146":"sns.countplot(x=categorical[1],data=df, order=df[categorical[1]].value_counts().index)","debfef04":"sns.countplot(x=categorical[2],data=df)\n","b0d55d50":"sns.countplot(x=categorical[3],data=df, order=df[categorical[3]].value_counts().index)","cffe3975":"sns.countplot(x=categorical[4],data=df)","2af26788":"sns.countplot(x=categorical[5],data=df, order=df[categorical[-2]].value_counts().index)","203b8729":"sns.countplot(x=categorical[6],data=df)","08e54abc":"numerical = df.columns.drop(categorical)\nnumerical","8fca68bb":"def plot_histograms(df, features, rows, cols):\n    fig=plt.figure(figsize=(20,20))\n    for i, feature in enumerate(features):\n        ax=fig.add_subplot(rows,cols,i+1)\n        df[feature].hist(bins=20,ax=ax ,facecolor='midnightblue')\n        ax.set_title(feature+\" Distribution\")\n        \n    fig.tight_layout()  \n    plt.show()\nplot_histograms(df, numerical, 2, 3)","10ad5f5b":"sns.pairplot(df,hue=\"HeartDisease\")","47b15e6e":"corr = df.corr()\nplt.figure(figsize=(7,7))\nsns.heatmap(corr, annot=True, cmap=\"YlGnBu\");\nplt.title('Correlation of the features')","a27b0c0a":"fig = px.box(df,y=\"Age\",x=\"HeartDisease\",title=f\"Distrubution of Age\")\nfig.show()","812233a3":"fig = px.box(df,y=\"RestingBP\",x=\"HeartDisease\",title=f\"Distrubution of RestingBP\")\nfig.show()","3b588ebf":"fig = px.box(df,y=\"Cholesterol\",x=\"HeartDisease\",title=f\"Distrubution of Cholesterol\")\nfig.show()","0b910a5b":"fig = px.box(df,y=\"Oldpeak\",x=\"HeartDisease\",title=f\"Distrubution of Oldpeak\")\nfig.show()","910b9719":"fig = px.box(df,y=\"MaxHR\",x=\"HeartDisease\",title=f\"Distrubution of MaxHR\")\nfig.show()","c72c7fbd":"df.loc[(df['RestingBP']==0)].shape[0]","1d84f193":"# We can simply just drop this single row that cotains zero value for RestingBP\ndf = df.loc[~(df['RestingBP']==0)]","1b962d28":"df.loc[df['Cholesterol']==0].shape[0]","ae33f476":"df.loc[df['Cholesterol']==0, 'Cholesterol'] = np.nan\ndf.isnull().sum()","05b7c3fc":"df_dummies=pd.get_dummies(df,columns=string_col,drop_first=True)\ndf_dummies.reset_index(drop=True, inplace=True)","c1b42f6b":"# Before imputing, I need to do train-test split\n# then fit and transform the umputing method on training data and only transform on the test set to avoid data leakage","63fd7042":"from sklearn.model_selection import train_test_split\nx = df_dummies.drop(columns='HeartDisease', axis=1)\ny=df_dummies['HeartDisease']\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.20,random_state=42, stratify=y)","a74593c7":"train_set = pd.concat([x_train, y_train], axis=1)\ntest_set = pd.concat([x_test, y_test], axis=1)","3f0de123":"# Iterative imputer to impute missing values\nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.impute import IterativeImputer\nimpute_it = IterativeImputer()\nimpute_it = impute_it.fit(train_set)\nimputed_train_set = pd.DataFrame(impute_it.transform(train_set), columns=train_set.columns)\nimputed_test_set = pd.DataFrame(impute_it.transform(test_set), columns=test_set.columns)","5acb5da8":"from sklearn.preprocessing import RobustScaler\nfeatures_to_scale = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\nro_scaler=RobustScaler()\nro_scaler.fit(imputed_train_set[features_to_scale])\nimputed_train_set[features_to_scale]=ro_scaler.transform(imputed_train_set[features_to_scale])\nimputed_test_set[features_to_scale]=ro_scaler.transform(imputed_test_set[features_to_scale])\n\n","4650c031":"cols = imputed_train_set.columns.drop('HeartDisease')\nimputed_y_train = imputed_train_set['HeartDisease']\nimputed_x_train = imputed_train_set[cols]","f3663316":"imputed_y_test = imputed_test_set['HeartDisease']\nimputed_x_test = imputed_test_set[cols]","33bedbaf":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nlogis=LogisticRegression()\nlogis.fit(imputed_x_train,imputed_y_train)\nlogis_y_pred=logis.predict(imputed_x_test)\nprint(f'accuracy score: {accuracy_score(imputed_y_test,logis_y_pred)}')\nprint(classification_report(y_test, logis_y_pred))\n\n\n","5bc17408":"from sklearn.metrics import confusion_matrix, roc_curve, accuracy_score\ncm=confusion_matrix(imputed_y_test,logis_y_pred)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d')\n","d650f4ad":"logis_proba = logis.predict_proba(imputed_x_test)\ny_pred_prob_df=pd.DataFrame(data=logis_proba, columns=['Prob of 0', 'Prob of 1'])\ny_pred_prob_df.head()","3f98c733":"fpr, tpr, thresholds = roc_curve(imputed_y_test, y_pred_prob_df['Prob of 1'])\nfig, (ax1, ax2) = plt.subplots(ncols=2)\n#fig = matplotlib.pyplot.gcf()\nfig.set_size_inches(14, 5)\nax1.set_xlim([0.0, 1.0])\nax1.set_ylim([0.0, 1.0])\nax1.set_title('ROC curve for Heart disease classifier')\nax1.set_xlabel('False positive rate (1-Specificity)')\nax1.set_ylabel('True positive rate (Sensitivity)')\nax1.plot(fpr,tpr, label='Tradeoff between FPR and TPR line')\nax1.legend(loc = 'best')\n\nax2.set_xlim([0.0, 1.0])\nax2.set_ylim([0.0, 1.0])\nax2.set_title('Thresholds vs True positive rate')\nax2.set_xlabel('Thresholds')\nax2.set_ylabel('True positive rate (Sensitivity)')\nax2.plot(thresholds,tpr, color='blue', label='Thresholds vs TPR line')\nplt.grid(True)\nax2.legend(loc = 'best')\nplt.show()","e64b6e6d":"from sklearn.metrics import roc_auc_score\nroc_auc_score(imputed_y_test, y_pred_prob_df['Prob of 1'])","c1c578fd":"from sklearn.svm import SVC\nkernels = [\"poly\", \"rbf\", \"sigmoid\"]\nC = [1, 10, 20]\nsvm_acc_score = {}\nfor kernel in kernels:\n    for Cval in C:\n        clf = SVC(kernel=kernel, C=Cval)\n        clf.fit(imputed_x_train,imputed_y_train)\n        y_pred = clf.predict(imputed_x_test)\n        acc = accuracy_score(imputed_y_test, y_pred)\n        svm_acc_score.update({f'{kernel} and {Cval}': round(acc, 4)})\n        \n        \n","678162f0":"{k: v for k, v in reversed(sorted(svm_acc_score.items(), key=lambda item: item[1]))}","a2db9102":"SVM = SVC(kernel='rbf', C=1)\nSVM.fit(imputed_x_train,imputed_y_train)\nSVM_y_pred = SVM.predict(imputed_x_test)\nprint(classification_report(y_test, SVM_y_pred))\ncm=confusion_matrix(imputed_y_test,SVM_y_pred)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d', cmap=\"Greens\")","67182944":"from sklearn.neighbors import KNeighborsClassifier\nknn_acc_score = {}\nknn_auc_score = {}\nfor i in range(1, 30):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(imputed_x_train,imputed_y_train)\n    knn_y_pred = knn.predict(imputed_x_test)\n    knn_proba = knn.predict_proba(imputed_x_test)\n    acc = accuracy_score(imputed_y_test, knn_y_pred)\n    knn_acc_score.update({f'n_neighbors={i}': round(acc, 3)})\n    auc = roc_auc_score(imputed_y_test, knn_proba[ :, 1])\n    knn_auc_score.update({f'n_neighbors={i}': round(auc, 3)})\n    \n    \n    ","e7e22a4e":"fig, (ax1, ax2) = plt.subplots(ncols=2)\nfig.set_size_inches(15.5, 8)\nax1.plot(range(1, 30), knn_acc_score.values(), linestyle='--', marker='o', color='#34eb7a', label='accuracy score')\nax2.plot(range(1, 30), knn_auc_score.values(), linestyle='--', marker='o', color='#34dceb', label='roc_auc score')\n\nax1.legend()\nax1.set_title('k values and accuracy score')\nax1.set_xlabel('k values')\nax1.set_ylabel('accuracy rate')\n\nax2.legend()\nax2.set_title('k values and roc_auc score')\nax2.set_xlabel('k values')\nax2.set_ylabel('roc_auc rate')\n\n\n","3204e5b8":"knn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(imputed_x_train,imputed_y_train)\nknn_y_pred = knn.predict(imputed_x_test)\nknn_proba = knn.predict_proba(imputed_x_test)","9f2e9d03":"print(classification_report(y_test, knn_y_pred))\ncm=confusion_matrix(imputed_y_test,knn_y_pred)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d', cmap=\"Blues\")","d5739c42":"fpr, tpr, thresholds = roc_curve(imputed_y_test, knn_proba[:,1])\nfig, (ax1, ax2) = plt.subplots(ncols=2)\n#fig = matplotlib.pyplot.gcf()\nfig.set_size_inches(14, 5)\nax1.set_xlim([0.0, 1.0])\nax1.set_ylim([0.0, 1.0])\nax1.set_title('ROC curve for Heart disease classifier')\nax1.set_xlabel('False positive rate (1-Specificity)')\nax1.set_ylabel('True positive rate (Sensitivity)')\nax1.plot(fpr,tpr, label='Tradeoff between FPR and TPR line')\nax1.legend(loc = 'best')\n\nax2.set_xlim([0.0, 1.0])\nax2.set_ylim([0.0, 1.0])\nax2.set_title('Thresholds vs True positive rate')\nax2.set_xlabel('Thresholds')\nax2.set_ylabel('True positive rate (Sensitivity)')\nax2.plot(thresholds,tpr, color='blue', label='Thresholds vs TPR line')\nplt.grid(True)\nax2.legend(loc = 'best')\nplt.show()","f273237e":"roc_auc_score(imputed_y_test, knn_proba[:, 1])","05817605":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier()\nrandom_forest.fit(imputed_x_train, imputed_y_train)\nRF_y_pred = random_forest.predict(imputed_x_test)\nRF_proba =  random_forest.predict_proba(imputed_x_test)","233c813a":"print(classification_report(y_test, RF_y_pred))\ncm=confusion_matrix(imputed_y_test,RF_y_pred)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d', cmap=\"Reds\")","8ca292f4":"fpr, tpr, thresholds = roc_curve(imputed_y_test, RF_proba[:,1])\nfig, (ax1, ax2) = plt.subplots(ncols=2)\n#fig = matplotlib.pyplot.gcf()\nfig.set_size_inches(14, 5)\nax1.set_xlim([0.0, 1.0])\nax1.set_ylim([0.0, 1.0])\nax1.set_title('ROC curve for Heart disease classifier')\nax1.set_xlabel('False positive rate (1-Specificity)')\nax1.set_ylabel('True positive rate (Sensitivity)')\nax1.plot(fpr,tpr, label='Tradeoff between FPR and TPR line')\nax1.legend(loc = 'best')\n\nax2.set_xlim([0.0, 1.0])\nax2.set_ylim([0.0, 1.0])\nax2.set_title('Thresholds vs True positive rate')\nax2.set_xlabel('Thresholds')\nax2.set_ylabel('True positive rate (Sensitivity)')\nax2.plot(thresholds,tpr, color='blue', label='Thresholds vs TPR line')\nplt.grid(True)\nax2.legend(loc = 'best')\nplt.show()","21ef344c":"roc_auc_score(imputed_y_test, RF_proba[:, 1])","ac2c406a":"As observed in the correlation matrix, there is a negative correlation between Cholesterol and HeartDisease, which is odd on its own. Now we observe many high values of cholesterol (e.g 603) and 0 cholesterol.","c8804d5e":"> KNN started to perform really well at k=5. But as k increases, the accuracy rate goes down and fluctuate a lot while roc_auc score only increases slightly\n\n**We'll use k=5**","2746423f":"## Importing libraries","fa06f69e":"![Random forest](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/02\/rfc_vs_dt1.png)","488f52ea":"**Flat has the most occurences while there is less than 100 occurences of down in the dataset.**","dbea12ce":"## Imputing missing values ","e442ec2a":"There are many rows contain zero value for Cholesterol.","14fafb56":"## K-nearest neighbors (KNN)","9eae883e":"Using seaborn to to draw pairplots of the features of dataset. Pairplot is a great way to do multivariate analysis. With pairplot, we can observe the correlation of each pair of features in the dataset.","91953082":"## Support vector machine (SVM)","63040816":"> The accuracy score of our model is approx. 0.89\n\n> Positive predictive value and Negative predictive value are 0.89\n\n> The sensitivity (the percentage of people with heartdisease who were correctly identified) is 0.91\n\n> The specificity (the percentage of people without heartdisease who were correctly identified) is 0.87","de332206":">Age: age of the patient (years)\n\n>Sex: sex of the patient (M: Male, F: Female)\n\n>There are four types of chest pains:\n-  TA (Typical Angina)\n-  ATA (ATypical Angina)\n-  NAP (Non-Anginal Pain)\n-  ASY (Asymptomatic)\n\n>RestingBP (resting blood pressure): In this case, It is Systolic blood pressure (the pressure when the heart beats) and unit of measure is mmHg\n\n>Cholesterol (a waxy, fat-like substance that's found in all the cells in your body), unit of measure is mm\/dl\n\n>FastingBS: fasting blood sugar (1: if FastingBS > 120 mg\/dl, 0: otherwise)\n\n>RestingECG: resting electrocardiogram results (Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria)\n\n>MaxHR: highest number of beats per minute your heart can pump under maximum stress, a common estimation methods is 220-age.\n\n>ExerciseAngina: exercise-induced angina (Y: Yes, N: No)\n\n>Oldpeak: ST depression induced by exercise relative to rest. ST depression refers to a finding on an electrocardiogram, wherein the trace in the ST segment is abnormally low below the baseline.\n\n>ST_Slope: the slope of the peak exercise ST segment (Up: upsloping, Flat: flat, Down: downsloping)\n\n>HeartDisease: output class (1: heart disease, 0: Normal)","a5b1aca5":"**ASY is the most common chest pain type while the least common is TA**","4fa19823":"Oldpeak values for people with heart disease vary a lot more and the median is also higher than people without heart disease ","a21a386c":"## Countplots of categorical features","5ec03799":"![KNN](https:\/\/ongxuanhong.files.wordpress.com\/2015\/07\/knn-concept-e1438068127480.png)","fec9d481":"- Older people has a higher probability of incurring heart disease\n- The youngest person who has heart disease in the data is 31 years old","6c5eeda8":"## Histograms of numerical features","d51d1c9a":"# <p style=\"background-color: #E0ED16 ; font-family:Verdana; font-size:120%; text-align:center; border-radius: 15px 50px; height:50px\">Preprocessing<\/p>\n","e29b41c0":"We will use robust scaling to reduce the effect of outliers\n\nThis can be achieved by calculating the median (50th percentile) and the 25th and 75th percentiles. The values of each variable then have their median subtracted and are divided by the interquartile range (IQR) which is the difference between the 75th and 25th percentiles.\n\n**value = (value \u2013 median) \/ (p75 \u2013 p25)**\nThe resulting variable has a zero mean and median and a standard deviation of 1, although not skewed by outliers and the outliers are still present with the same relative relationships to other values.","0e317f01":"## Feature scaling","2e15379c":"# <p style=\"background-color:#ED5416 ; font-family:Verdana; font-size:120%; text-align:center; border-radius: 15px 50px; height:50px\">Build models<\/p>","1bfb1510":"## RestingBP and Cholesterol","299e30df":"![gif](https:\/\/media3.giphy.com\/media\/3oEdva9BUHPIs2SkGk\/giphy.gif?cid=ecf05e4769llkt31hqv9lm4p0iwgbfe8qv5mp7yxgkdwspwy&rid=giphy.gif&ct=g)","bfa50847":"> The accuracy score of our model is approx. 0.88\n\n> Positive predictive value and Negative predictive value are 0.88\n\n> The sensitivity (the percentage of people with heartdisease who were correctly identified) is 0.91\n\n> The specificity (the percentage of people without heartdisease who were correctly identified) is 0.84","9417f033":"## Pairplots","4578fa5b":"**The distributions of MaxHR and Age resembles a normal distribution. Also there are a lot of 0 value in Cholesterol histogram**","2061bc0e":"## To detect outliers in the dataset, we will use boxplots in this case.","f20e4b35":"![Heart disease](https:\/\/media4.giphy.com\/media\/kIS1NBIphAbrmDwAva\/giphy.gif?cid=ecf05e47fx7999om8enwmg4irh1avl8hv9gxwamxmhfzp40a&rid=giphy.gif&ct=g)","c0925bea":"# <p style=\"background-color: #ED16D0 ; font-family:Verdana; font-size:120%; text-align:center; border-radius: 15px 50px; height:50px\">Outliers<\/p>","e35e939b":"## Random forest","d09a3aaa":"RestingBP of 0 seems really absurb, so we'll remove this out of dataset","fdbca681":"## Handling missing values","3e768a6f":"**Cardiovascular diseases (CVDs) are the leading cause of death globally, taking an estimated 17.9 million lives each year. CVDs are a group of disorders of the heart and blood vessels and include coronary heart disease, cerebrovascular disease, rheumatic heart disease and other conditions. More than four out of five CVD deaths are due to heart attacks and strokes, and one third of these deaths occur prematurely in people under 70 years of age.**","a65b674a":"**df.describe() is really helpful to get a quick understanding of the numerical data.**\n- The majority of people seems to be middle-aged (40-60) or older, the mean and median is close to each other\n- the min RestingBP is 0, but 75 percent of people show 120 and above or maybe it is null value of the dataframe\n- the min Cholesterol is 0, but 75 percent of people show 173.25 and above or maybe it is null value of the dataframe. Also Cholesterol numbers of people recored have high standard deviation\n- FastingBs is binary feature and seems like most records are 0\n- Oldpeak has negative value and high standard deviation\n- More than half of people recored have heart disease","d5d48d8a":"## Correlation matrix","a875acf8":"The ROC curve for Heart disease classifier linegraph shows the tradeoffs between TPR and FPR. As threshold decrease, we can predict more people with heart d\u00edsease but at the expense of classifying more people without heart disease as having heart disease.","86c21e4a":"## Summary \u00f2f all variables","7aaa3405":"However, simple imputing approaches are univariate, which means they take into account only 1 chosen variable. More advanced imputing methods can be used instead to take various variables into accounts in order to impute the missing data with better predictions\n\n* [Iterative imputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html): This method uses the linear regression concept to impute the missing values\n* [KNNimputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.KNNImputer.html): This method uses the k-nearest neigbor concept to impute the missing values\nFootnote: these methods just work with numerical variables","7dca3e00":"**More than 500 people show normal result in Resting ECG**","f8b935c7":"![Imputing](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTqDeqICaJrg3Thuz7OQ-TkNpk6XXUAfCB9yFGW_74JuZBqg_QZTwyuX5SGiE2khmK7u2o&usqp=CAU)","d622b9ed":"MaxHR median of people with heart disease is lower than that of people without heart disease","f3c3972e":"**Correlation matrix is a good tool to check the correlation between features:**\n- Correlation between any two independent features (multicollinearity) should not be too high (threshold is around 90%) because it might affect the performance of non tree ML models. Here we don't see any multicolinearity.\n- The two features that have the highest correlation with HeartDisease are MaxHR (negative relationship) and Oldpeak (positive relationship). Also cholesterol has negative correlation with HeartDisease. ","972ee81b":"**Around 75 to 85% records are from male individuals**","43647e80":"**More people answered No to ExceriseAngina than YES**","4ac24ca2":"![Logistic regression](https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/logistic-regression-in-machine-learning.png)","ad5feadb":"![SVM](https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/support-vector-machine-algorithm.png)","d3b5bc2d":"**There is no null values in our dataset**","d262dfc8":"# <p style=\"background-color: #16D9ED; font-family:Verdana; font-size:120%; text-align:center; border-radius: 15px 50px; height:50px\">Exploratory data analysis<\/p>","af77dc50":"![EDA](https:\/\/media4.giphy.com\/media\/FSzLVme5Y3n3LMOiqP\/giphy.gif?cid=ecf05e47yf92ev61m0sytpu1ot37ljm061pxyj5cny1upo46&rid=giphy.gif&ct=g)","215a086a":"> The accuracy score of our model is approx. 0.89\n\n> Positive predictive value is 0.89 while Negative predictive value is 0.88\n\n> The sensitivity (the percentage of people with heartdisease who were correctly identified) is 0.90\n\n> The specificity (the percentage of people without heartdisease who were correctly identified) is 0.87\n\n**Our model did a good job in classifying people with heart disease and people witout heart disease at the 0.5 threshold (the default threshold of the model)**","8657c4ec":"## Heart Disease  ","de953658":"Imputation is a technique used for replacing the missing data with some substitute value to retain most of the data\/information of the dataset. These techniques are used because removing the data from the dataset every time is not feasible and can lead to a reduction in the size of the dataset to a large extend, which not only raises concerns for biasing the dataset but also leads to incorrect analysis.\n\nFirst of all, there are several simple imputing methods that can be employed using [sklearn.imput.SimpleImputer](https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html):\n* The mean approach: replace missing values using the mean along each column. Can only be used with numeric data.\n\n* The median approach: replace missing values using the median along each column. Can only be used with numeric data.\n\n* The mode\/most frequent approach: replace missing using the most frequent value along each column. Can be used with strings or numeric data. If there is more than one such value, only the smallest is returned.\n\n* The constant approach: replace missing values with fill_value. Can be used with strings or numeric data.","6d76e744":"**A higher percentage of with heartdisease who are correctly identified can be obtain if we try to lower the threshold even though it may lead to increase in percentage of people without heart disease predicted as having heart disease (False positive)**","37a767ce":"# <p style=\"background-color:#CCED16; font-family:Verdana; font-size:120%; text-align:center; border-radius: 15px 50px; height:50px\">Quick look at the dataset<\/p>","6a02bef3":"## Logistic regression","9f1de853":"**The best SVM models are either with rbf kernel and C=1 or with poly kerne and C=1**","ab1b7e22":"## Thanks a lot for spending time reading my notebook, if you find it helpful, pls give me an upvote. I would love to receive your feedbacks and advices to learn more","d4dda8ab":"**Around 400 people don't have heart disease while slightly more than 500 people have heart disease**","8ad4667b":"## Get dummies","efb8d293":"**More than 75% of people are labeled 0 in FastingBS**","915f1797":"> The accuracy score of our model is approx. 0.89\n\n> Positive predictive value is 0.9 while Negative predictive value is 0.87\n\n> The sensitivity (the percentage of people with heartdisease who were correctly identified) is 0.89\n\n> The specificity (the percentage of people without heartdisease who were correctly identified) is 0.88","fd994175":"**The dataframe shows the record of 918 people and 11 of features (independent variables) that help predict heart disease and 1 target variable named 'HeartDisease' that has 2 values: 1 for having heart disease and 0 for not having heart disease.**","7d4e5240":"### You can learn more from\n- https:\/\/www.kaggle.com\/durgancegaur\/a-guide-to-any-classification-problem\n- https:\/\/www.kaggle.com\/neisha\/heart-disease-prediction-using-logistic-regression"}}