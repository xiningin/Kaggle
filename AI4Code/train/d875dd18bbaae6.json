{"cell_type":{"009fd75d":"code","8ede7bd0":"code","a309a522":"code","4c399e96":"code","39ba5ba3":"code","fb128a3a":"code","828a0bec":"code","030ee87d":"code","cf30720a":"code","21601a8e":"code","cafa20d6":"code","1e6c7434":"code","a2c34792":"code","f958b81b":"code","3d8c6b12":"code","f68dda31":"code","f399bc29":"code","148c94cf":"code","7c58ffb5":"code","39a2f09f":"code","195a8b79":"code","324e6aea":"code","f60489eb":"code","992c422b":"code","2df4dca7":"code","15e65601":"code","5074370c":"code","46af5f39":"code","3f4fc0af":"code","4615b3c6":"code","3ded323e":"code","3ec1698f":"code","4af1f80c":"code","3846f725":"code","880be106":"code","e8d602ba":"code","6a2594b6":"code","ad0e9ee3":"code","b6ae7bff":"code","93443147":"code","0ce3e2e9":"code","6b4e0288":"code","4cd02702":"code","9870d1ad":"code","5fc7d95c":"code","9b7e993c":"code","9e020953":"code","d87f803b":"markdown","956f9d61":"markdown","792184da":"markdown","ecaca4bd":"markdown","4dbda3dd":"markdown","76f663d1":"markdown","61f4dee1":"markdown","fcb170fd":"markdown","b5d34c50":"markdown","63f511ae":"markdown","6983b7dc":"markdown","983002fd":"markdown","62bdb277":"markdown"},"source":{"009fd75d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8ede7bd0":"import pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import naive_bayes\nfrom sklearn.metrics import roc_auc_score","a309a522":"#nltk.download()","4c399e96":"df= pd.read_csv(\"..\/input\/sms_spam.csv\")","39ba5ba3":"df.head()","fb128a3a":"df.type.replace('spam', 1, inplace=True)","828a0bec":"df.type.replace('ham', 0, inplace=True)","030ee87d":"df.head()","cf30720a":"df.shape","21601a8e":"##Our dependent variable will be 'spam' or 'ham' \ny = df.type","cafa20d6":"df.text","1e6c7434":"#TFIDF Vectorizer\nstopset = set(stopwords.words('english'))\nvectorizer = TfidfVectorizer(use_idf=True, lowercase=True, strip_accents='ascii', stop_words=stopset)","a2c34792":"#Convert df.txt from text to features\nX = vectorizer.fit_transform(df.text)","f958b81b":"X.shape","3d8c6b12":"X.data","f68dda31":"df.text[0]","f399bc29":"## Spliting the SMS to separate the text into individual words\nsplt_txt1=df.text[0].split()\nprint(splt_txt1)","148c94cf":"## Count the number of words in the first SMS\nlen(splt_txt1)","7c58ffb5":"X[0]","39a2f09f":"print(X[0])","195a8b79":"vectorizer.get_feature_names()[8585]## 4316 is the position of the word jurong","324e6aea":"## Spliting the SMS to separate the text into individual words\nsplt_txt2=df.text[1].split()\nprint(splt_txt2)","f60489eb":"len(splt_txt2)","992c422b":"X[1]## Second SMS","2df4dca7":"print (X[1])","15e65601":"## Finding the most frequent word appearing in the second SMS\nmax(splt_txt2)","5074370c":"## Last word in the vocabulary\nmax(vectorizer.get_feature_names())","46af5f39":"len(vectorizer.get_feature_names())","3f4fc0af":"print (y.shape)\nprint (X.shape)","4615b3c6":"##Split the test and train\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","3ded323e":"X_train","3ec1698f":"##Train Naive Bayes Classifier\n## Fast (One pass)\n## Not affected by sparse data, so most of the 8605 words dont occur in a single observation\nclf = naive_bayes.MultinomialNB()\nmodel=clf.fit(X_train, y_train)","4af1f80c":"clf.feature_log_prob_","3846f725":"clf.coef_","880be106":"predicted_class=model.predict(X_test)\nprint(predicted_class)","e8d602ba":"print(y_test)","6a2594b6":"df.loc[[19]]","ad0e9ee3":"predicted_class[19]## This SMS(SMS no. 19) has been classified as Ham but Actually it's SPAM","b6ae7bff":"prd=model.predict_proba(X_test)","93443147":"prd","0ce3e2e9":"clf.predict_proba(X_test)[:,0]","6b4e0288":"roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])","4cd02702":"##Check model's accuracy\nroc_auc_score(y_test, clf.predict_proba(X_test)[:,1])","9870d1ad":"clf.coef_","5fc7d95c":"def get_most_important_features(vectorizer, model, n=5):\n    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n    \n    # loop for each class\n    classes ={}\n    for class_index in range(model.coef_.shape[0]):\n        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n        bottom = sorted_coeff[-n:]\n        classes[class_index] = {\n            'tops':tops,\n            'bottom':bottom\n        }\n    return classes\n\nimportance = get_most_important_features(vectorizer, clf, 20)","9b7e993c":"print (importance)","9e020953":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndef plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n    y_pos = np.arange(len(top_words))\n    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n    \n    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n    \n    top_words = [a[0] for a in top_pairs]\n    top_scores = [a[1] for a in top_pairs]\n    \n    bottom_words = [a[0] for a in bottom_pairs]\n    bottom_scores = [a[1] for a in bottom_pairs]\n    fig = plt.figure(figsize=(10, 10))  \n\n    plt.subplot(121)\n    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n    plt.title('Ham', fontsize=20)\n    plt.yticks(y_pos, bottom_words, fontsize=14)\n    plt.suptitle('Key words', fontsize=16)\n    plt.xlabel('Importance', fontsize=20)\n    \n    plt.subplot(122)\n    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n    plt.title('Spam', fontsize=20)\n    plt.yticks(y_pos, top_words, fontsize=14)\n    plt.suptitle(name, fontsize=16)\n    plt.xlabel('Importance', fontsize=20)\n    \n    plt.subplots_adjust(wspace=0.8)\n    plt.show()\ntop_scores = [a[0] for a in importance[0]['tops']]\ntop_words = [a[1] for a in importance[0]['tops']]\nbottom_scores = [a[0] for a in importance[0]['bottom']]\nbottom_words = [a[1] for a in importance[0]['bottom']]\n\nplot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")","d87f803b":"## Goal: To classify future SMS messages as either spam or ham with a Naive Bayes model.\n\nSteps:\n\n1.  Convert the words ham and spam to a binary indicator variable(0\/1)\n\n2.  Convert the txt to a sparse matrix of TFIDF vectors\n\n3.  Fit a Naive Bayes Classifier\n\n4.  Measure your success using roc_auc_score\n\n","956f9d61":"Identify the words which are the most important in classifying a message as spam","792184da":"### Find the probability of assigning a SMS to a specific class","ecaca4bd":"### TF(t) = (Number of times term t appears in a document) \/ (Total number of terms in the document)\n\n### IDF(t) = log_e(Total number of documents \/ Number of documents with term t in it).\n\n## tf-idf score=TF(t)*IDF(t)","4dbda3dd":"### With the model, the success rate is ~98.60%","76f663d1":"## Second SMS","61f4dee1":"#### Convert the spam and ham to 1 and 0 values respectively for probability testing","fcb170fd":"### From the above in the 2nd SMS there are 6 words  & out of which only 5 elements have been taken, that's why\n### we'll get only 5 tf-idf values for the 2nd the SMS.Likewise elements or words of all other SMSes are taken into consideration","b5d34c50":"## 0 is the first SMS,3536,4316 etc are the positions of the elements or the words & 0.15,0.34,0.27 are the tf_idf value of the words . Like wise we can find the next SMSes & the tf-idf value of the words of the SMSes","63f511ae":"### It means in the first SMS there are 20(len(splt_txt1)) words & out of which only 14 elements have been taken, that;s why we'll get only 14 tf-idf values for the first the SMS.Likewise elements or words of all other SMSes are taken into consideration","6983b7dc":"# END**","983002fd":"### First 3 SMSes are correctly assigned to Ham(0) based on the tf-idf scores of the words given in the SMSes","62bdb277":"#### Train the classifier if it is spam or ham based on the text"}}