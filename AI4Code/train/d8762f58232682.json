{"cell_type":{"2a0c24e6":"code","15fdbd83":"code","9b6505a8":"code","4609ef42":"code","964fb6a9":"code","fa3cf08d":"code","c949d32b":"code","22463a9b":"code","c9a93fc0":"code","75a6faf1":"code","6cac932a":"code","34b64315":"code","6e78ba8d":"code","45bca39f":"code","d2967936":"code","086b6600":"code","0791e994":"code","95675dc4":"code","d549b537":"code","99f53650":"code","bfa75606":"code","278c0bc1":"code","90300ac9":"code","55ccbc7e":"code","a5511fee":"code","3457b9dc":"code","0da6cb01":"code","19e2ce7a":"code","3f85594f":"code","3d918703":"code","6df899c0":"code","d9ce267c":"code","0b82f5d2":"markdown","3e674ec8":"markdown","c3666dde":"markdown","21222df0":"markdown","f3cf38b9":"markdown","49560635":"markdown","1ba0dcf6":"markdown","8a7798a1":"markdown","6341ea51":"markdown","4db4bf9c":"markdown","70d16eac":"markdown","ea49fd3d":"markdown","e49d401f":"markdown","2e9aec61":"markdown"},"source":{"2a0c24e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","15fdbd83":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\n\nfrom matplotlib import pyplot as plt","9b6505a8":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain_data_copy1 = train_data.copy(deep = True)\ndata_combined = [train_data_copy1, test_data]\ntrain_data.info()","4609ef42":"train_data.sample(10)","964fb6a9":"print('Train columns with null values:\\n', train_data.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test\/Validation columns with null values:\\n', test_data.isnull().sum())\nprint(\"-\"*10)\n\ntrain_data.describe(include = 'all')","fa3cf08d":"for dataset in data_combined:\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0],inplace = True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)    ","c949d32b":"drop_column = ['PassengerId', 'Cabin', 'Ticket']\ntrain_data_copy1.drop(drop_column, axis =1, inplace = True)\nprint(train_data_copy1.isnull().sum())\nprint('*'*10)\nprint(test_data.isnull().sum())","22463a9b":"fig = px.histogram(train_data_copy1, x = 'Age', title = \"Age Distribution to Gender\", color = 'Sex',color_discrete_sequence = px.colors.qualitative.Safe, nbins = 15)\nfig.show()\n","c9a93fc0":"sns.boxplot(data = train_data_copy1, x = 'Age')","75a6faf1":"fig = px.histogram(train_data_copy1, x = 'Age', title = \"Age Distribution to Survived Passenger\", color = 'Survived',color_discrete_sequence = px.colors.qualitative.Set2, nbins = 15)\nfig.show()","6cac932a":"fig = px.pie(train_data_copy1, values = 'Survived', names= 'Sex', title = \"Pie Chart: Gender wise survival\")\nfig.show()","34b64315":"fig = px.pie(train_data_copy1, values = 'Survived', names= 'Pclass', title = \"Pie Chart: Class wise survival\",color_discrete_sequence=px.colors.sequential.RdBu)\nfig.show()","6e78ba8d":"fig = px.pie(train_data_copy1, values = 'Survived', names= 'Embarked', title = \"Pie Chart: Survival according to Embarked Location\",color_discrete_sequence=px.colors.qualitative.Bold)\nfig.show()","45bca39f":"plt.figure(figsize = (10,10))\nplt.title('Swarm Plot: Survived to Age and Gender', fontsize = 15)\nsns.swarmplot(x = 'Survived', y = 'Age',hue = 'Sex',data = train_data_copy1, size = 5)\n","d2967936":"sns.pairplot(train_data_copy1, vars = ['Age','Fare','Pclass'], hue = 'Survived', palette = 'Set1')\n\n","086b6600":"for dataset in data_combined:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 #(Including self as 1)\n    dataset['IsAlone']= 1\n    dataset['IsAlone'].loc[dataset['FamilySize']>1] = 0 # Assign 0 if familysize greater than 1\n    \n    dataset['Title'] = dataset['Name'].str.split(',',expand = True)[1].str.split(\".\",expand = True)[0]\n    dataset['FareBin'] = pd.qcut(dataset['Fare'],4)\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int),5)\n    \ntitle_count = 10\ntitle_names = (train_data_copy1['Title'].value_counts()<title_count)\n\n\ntrain_data_copy1['Title'] = train_data_copy1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)        \nprint(train_data_copy1['Title'].value_counts())\n\nprint('*'*10)\ntrain_data_copy1.info()\ntest_data.info()\ntrain_data_copy1.sample(10)\n\n    \n       ","0791e994":"plt.figure(figsize = (10,8))\nplt.title('Count Plot: Survived to IsAlone', fontsize = 15)\nsns.countplot(data = train_data_copy1,x = 'Survived', hue = 'IsAlone', palette = 'Set1')\n\n\n","95675dc4":"plt.figure(figsize = (10,8))\nplt.title('Count Plot: Survived to IsAlone', fontsize = 15)\nsns.countplot(data = train_data_copy1,x = 'FamilySize', hue = 'Survived', palette = 'muted')\n\n\n","d549b537":"la = LabelEncoder()\nfor dataset in data_combined:\n    \n    dataset['Sex_Code'] = la.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = la.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = la.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = la.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = la.fit_transform(dataset['FareBin'])\nTarget = ['Survived']\n\n\n#define x variables for original features aka feature selection\ntrain_data_copy1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name\/values for charts\ntrain_data_copy1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\ntrain_data_copy1_xy =  Target + train_data_copy1_x\nprint('Original X Y: ', train_data_copy1_xy, '\\n')\n\n\n#define x variables for original w\/bin features to remove continuous variables\ntrain_data_copy1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ntrain_data_copy1_xy_bin = Target + train_data_copy1_x_bin\nprint('Bin X Y: ', train_data_copy1_xy_bin, '\\n')\n\n\n#define x and y variables for dummy features original\ntrain_data_copy1_dummy = pd.get_dummies(train_data_copy1[train_data_copy1_x])\ntrain_data_copy1_x_dummy = train_data_copy1_dummy.columns.tolist()\ntrain_data_copy1_xy_dummy = Target + train_data_copy1_x_dummy\nprint('Dummy X Y: ', train_data_copy1_xy_dummy, '\\n')\n\n\n\ntrain_data_copy1_dummy.head()\n\n","99f53650":"print('Train columns with null values: \\n', train_data_copy1.isnull().sum())\nprint(\"-\"*10)\nprint (train_data_copy1.info())\nprint(\"-\"*10)\n\nprint('Test\/Validation columns with null values: \\n', test_data.isnull().sum())\nprint(\"-\"*10)\nprint (test_data.info())\nprint(\"-\"*10)\n\ntrain_data.describe(include = 'all')","bfa75606":"Y_train = train_data_copy1[\"Survived\"]\nX_train = pd.get_dummies(train_data_copy1[train_data_copy1_x_bin])\nX_test = pd.get_dummies(test_data[train_data_copy1_x_bin])\n\nY_train.shape, X_train.shape, X_test.shape\n                         ","278c0bc1":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(\"Logistic Regression Score:\",acc_log)","90300ac9":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint(\"Naive-Bayes accuracy : \",acc_gaussian)","55ccbc7e":"svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","a5511fee":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint (\"KNeighbors accuracy score : \",acc_knn)","3457b9dc":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(data_train, target_train.values.ravel())\npred = random_forest.predict(data_test)\nacc_random_forest =accuracy_score(target_test, pred)\nprint (\"Random Forest accuracy score : \",acc_random_forest)","0da6cb01":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","19e2ce7a":"sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nprint (\"Stochastic accuracy score : \",acc_sgd)","3f85594f":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint (\"Decision Tree accuracy score : \", acc_decision_tree)\n","3d918703":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint (\"Random Forest accuracy score : \",acc_random_forest)","6df899c0":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, \n              acc_sgd, acc_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)\n\n\n","d9ce267c":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': Y_pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","0b82f5d2":"# Stochastic Gradient Descent","3e674ec8":"# Data Visualization","c3666dde":"# Random Forest","21222df0":"# Gaussian NB","f3cf38b9":"# KNN Classifier","49560635":"# Linear SVC\n","1ba0dcf6":"## Feature Engineering \n1. Creating new column Name FamilySize with the help of Parch, SibSp\n2. Removing Unnecessary characters from Name\n3. Create New Column IsAlone from Newly created FamilySize\n4. Change continuous data into Range\n","8a7798a1":"# Random Forest","6341ea51":"# Data Modelling","4db4bf9c":"# Logistic Regression","70d16eac":"# Support Vector Machines","ea49fd3d":"# Label Encoding\nConvert string based data to numeric data using label encoder\n\n","e49d401f":"# Data Exploration","2e9aec61":"# Decision Tree"}}