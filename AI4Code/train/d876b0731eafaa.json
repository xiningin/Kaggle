{"cell_type":{"8eba985a":"code","ea4fdcfc":"code","6000149e":"code","ad7d3ea5":"code","5d671ab2":"code","c95cee5f":"code","b66690e8":"code","1bd38fe5":"code","be003e10":"code","a1ca96a9":"code","f8b38544":"code","579b0244":"code","fab2e94a":"code","12668a21":"code","f18b7f34":"code","2d0ee9ef":"markdown","db311789":"markdown"},"source":{"8eba985a":"import mxnet as mx, sys, os, time\nfrom mxnet import autograd, nd, init, gluon\nfrom mxnet.gluon import nn, data as gdata, loss as gloss","ea4fdcfc":"def try_gpu():\n    try:\n        ctx = mx.gpu()\n        _ = nd.array([0], ctx=ctx)\n    except mx.base.MXNetError:\n        ctx = mx.cpu()\n    return ctx","6000149e":"def load_data_fashion_mnist(batch_size, resize=None, root='mnist'):\n    transformer = []\n    if resize:\n        transformer += [gdata.vision.transforms.Resize(resize)]\n    transformer += [gdata.vision.transforms.ToTensor()]\n    transformer = gdata.vision.transforms.Compose(transformer)\n    mnist_train = gdata.vision.ImageFolderDataset(os.path.join(root, 'train'))\n    mnist_test = gdata.vision.ImageFolderDataset(os.path.join(root, 'test'))\n    num_workers = 0 if sys.platform.startswith('win32') else 4\n    train_iter = gdata.DataLoader(mnist_train.transform_first(transformer), batch_size, True, num_workers=num_workers)\n    test_iter = gdata.DataLoader(mnist_test.transform_first(transformer), batch_size, False, num_workers=num_workers)\n    return train_iter, test_iter","ad7d3ea5":"def evaluate_acc(data_iter, net, ctx):\n    test_acc_sum, n = nd.array([0], ctx=ctx), 0\n    for X, y in data_iter:\n        X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n        test_acc_sum += (net(X).argmax(axis=1) == y.astype('float32')).sum()\n        n += y.size\n    return test_acc_sum.asscalar() \/ n","5d671ab2":"def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n    if not autograd.is_training():\n        X_hat = (X - moving_mean) \/ nd.sqrt(moving_var + eps)\n    else:\n        assert len(X.shape) in (2, 4)\n        if len(X.shape) == 2:\n            mean = X.mean(axis=0)\n            var = ((X - mean) ** 2).mean(axis=0)\n        else:\n            mean = X.mean(axis=(0, 2, 3), keepdims=True)\n            var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)\n        X_hat = (X - mean) \/ nd.sqrt(var + eps)\n        moving_mean = moving_mean * momentum + (1 - momentum) * mean\n        moving_var = moving_var * momentum + (1 - momentum) * var\n    Y = gamma * X_hat + beta\n    return Y, moving_mean, moving_var","c95cee5f":"class BatchNorm(nn.Block):\n    def __init__(self, num_features, num_dims, **kwargs):\n        super(BatchNorm, self).__init__(**kwargs)\n        if num_dims == 2:\n            shape = (1, num_features)\n        else:\n            shape = (1, num_features, 1, 1)\n        self.gamma = self.params.get('gamma', shape=shape, init=init.One())\n        self.beta = self.params.get('beta', shape=shape, init=init.Zero())\n        self.moving_mean = nd.zeros(shape=shape)\n        self.moving_var = nd.zeros(shape=shape)\n    def forward(self, x):\n        if self.moving_mean.context != x.context:\n            self.moving_mean = self.moving_mean.as_in_context(x.context)\n            self.moving_var = self.moving_var.as_in_context(x.context)\n        Y, self.moving_mean, self.moving_var = batch_norm(x, self.gamma.data(), self.beta.data(), self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9)\n        return Y","b66690e8":"def conv_block(num_features, num_channels):\n    blk = nn.Sequential()\n    blk.add(BatchNorm(num_features, 4), nn.Activation('relu'),\n            nn.Conv2D(num_channels, kernel_size=3, padding=1))\n    return blk","1bd38fe5":"class DenseBlock(nn.Block):\n    def __init__(self, num_convs, input_channels, num_channels, **kwargs):\n        super(DenseBlock, self).__init__(**kwargs)\n        self.net = nn.Sequential()\n        num_features = input_channels\n        for _ in range(num_convs):\n            self.net.add(conv_block(num_features, num_channels))\n            num_features += num_channels\n    def forward(self, x):\n        for blk in self.net:\n            Y = blk(x)\n            x = nd.concat(x, Y, dim=1)\n        return x","be003e10":"X = nd.random.uniform(shape=(4, 3, 8, 8))\nblk = DenseBlock(2, X.shape[1], 10)\nblk.initialize()\nY = blk(X)\nY.shape","a1ca96a9":"def transition_block(num_features, num_channels):\n    blk = nn.Sequential()\n    blk.add(BatchNorm(num_features, 4), nn.Activation('relu'),\n            nn.Conv2D(num_channels, kernel_size=1), nn.AvgPool2D(pool_size=2, strides=2))\n    return blk","f8b38544":"blk = transition_block(Y.shape[1], 10)\nblk.initialize()\nblk(Y).shape","579b0244":"net = nn.Sequential()\nnet.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n        BatchNorm(64, 4), nn.Activation('relu'),\n        nn.MaxPool2D(pool_size=3, strides=2, padding=1))","fab2e94a":"num_channels, growth_rate = 64, 32\nnum_convs_in_dense_blocks = [4, 4, 4, 4]\nfor i, num_convs in enumerate(num_convs_in_dense_blocks):\n    net.add(DenseBlock(num_convs, num_channels, growth_rate))\n    num_channels += num_convs * growth_rate\n    if i != len(num_convs_in_dense_blocks) - 1:\n        input_size = num_channels\n        num_channels \/\/= 2\n        net.add(transition_block(input_size, num_channels))\nnet.add(BatchNorm(num_channels, 4), nn.Activation('relu'), nn.GlobalAvgPool2D(), nn.Dense(10))","12668a21":"def train(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs):\n    print('Computing on', ctx)\n    loss = gloss.SoftmaxCrossEntropyLoss()\n    for epoch in range(num_epochs):\n        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n        for X, y in train_iter:\n            X, y = X.as_in_context(ctx), y.as_in_context(ctx).astype('float32')\n            with autograd.record():\n                y_hat = net(X)\n                l = loss(y_hat, y).sum()\n            l.backward()\n            trainer.step(batch_size)\n            train_l_sum += l.asscalar()\n            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n            n += y.size\n        test_acc = evaluate_acc(test_iter, net, ctx)\n        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n                 % (epoch+1, train_l_sum \/ n, train_acc_sum \/ n, test_acc, time.time() - start))","f18b7f34":"lr, num_epochs, batch_size, ctx = 0.1, 5, 256, try_gpu()\nnet.initialize(ctx=ctx, init=init.Xavier())\ntrainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\ntrain_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96, root='\/kaggle\/input\/fashionmnist\/data')\ntrain(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)","2d0ee9ef":"\u7a20\u5bc6\u5757\uff1a\u5b9a\u4e49\u8f93\u5165\u548c\u8f93\u51fa\u7684\u8fde\u63a5","db311789":"\u8fc7\u6e21\u5c42\uff1a\u7a20\u5bc6\u5757\u4f1a\u5e26\u6765\u901a\u9053\u6570\u7684\u589e\u52a0\uff0c\u4f7f\u7528\u8fc7\u591a\u4f1a\u589e\u52a0\u6a21\u578b\u7684\u590d\u6742\u5ea6\u3002\u8fc7\u6e21\u5c42\u7528\u6765\u63a7\u5236\u6a21\u578b\u7684\u590d\u6742\u5ea6\u3002"}}