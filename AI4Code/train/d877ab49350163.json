{"cell_type":{"c8c3a26c":"code","895b9281":"code","5676f56f":"code","1d732aff":"code","32236db2":"code","0f6b1519":"code","2f18dd6c":"code","0ef75531":"code","d866bb66":"code","3d968f11":"code","88761cd7":"code","83c717d6":"code","58e499af":"code","58ebe74a":"code","8dca7b95":"code","6e3909fb":"code","fdca4685":"code","74b06768":"code","6871b7bd":"code","b8d375c3":"code","1ef54013":"code","ba3ea7e1":"code","5afdaa31":"code","d6756bd9":"code","1020220b":"code","b59f40f0":"code","bb386365":"code","b12edb82":"code","c059d3c3":"code","0402a326":"code","c19ea2c8":"code","455c0c89":"code","c3f67b84":"code","b485a8ae":"code","b79d2918":"code","589f570d":"code","83c041c8":"code","c41e7f48":"code","571f9aa8":"code","df81ec8f":"code","eb22f524":"code","3b07f7c9":"code","c4574c22":"code","14a48c23":"markdown","079bf986":"markdown","e7a84486":"markdown","144354d5":"markdown","0d5ba878":"markdown","cf527c56":"markdown","867944ff":"markdown","528d1253":"markdown","10b6806c":"markdown","e87abc37":"markdown","24e3c5dc":"markdown","0f803146":"markdown","66429210":"markdown","8000806f":"markdown","43f016ef":"markdown","f574410c":"markdown","d114da7c":"markdown","14f3e743":"markdown","dc054bbc":"markdown","a44019cc":"markdown","8e9f9329":"markdown","78f341cd":"markdown","4dcab422":"markdown","ba8fdee4":"markdown","c644b2c4":"markdown","bf089dcd":"markdown","baad7da3":"markdown","aaef6206":"markdown","f7456ebe":"markdown"},"source":{"c8c3a26c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n        \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.options.mode.chained_assignment = None\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","895b9281":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","5676f56f":"train.head()","1d732aff":"test.head()","32236db2":"train.shape, test.shape","0f6b1519":"train.info()","2f18dd6c":"test.info()","0ef75531":"train = train.drop(['Id','Alley','PoolQC','Fence','MiscFeature'],axis=1)\ntestID = test.Id\n#print(testID)\ntest = test.drop(['Id','Alley','PoolQC','Fence','MiscFeature'],axis=1)","d866bb66":"corr = train.corr()\nf, ax = plt.subplots(figsize=(16, 12))\nax = sns.heatmap(corr,linewidths=.5,annot=True)","3d968f11":"corr = train.corr()\nstrong_corr = corr.index[abs(corr[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\nsns.heatmap(train[strong_corr].corr(),annot=True,cmap=\"RdYlGn\")","88761cd7":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n#Have not chosen them all, as you can see from above that some columns are highly correlatd with one another such as GarageCars and GarageArea, so will only use one of them.\nsns.pairplot(train[cols], height = 2.5)","83c717d6":"sns.histplot(train.SalePrice,kde=True)","58e499af":"from scipy.stats import skew\nskew(train.SalePrice)","58ebe74a":"#LOG OF SALE PRICE\n#DROP SALE PRICE BUT SAVE IT \n#CONCATENATE TWO DATAFRAMES BUT ADD COLUMN TRAIN\ntrain['train']  = 1\ntest['train']  = 0\nall_data = pd.concat([train, test], axis=0,sort=False)\n\ntrain.SalePrice = np.log1p(train.SalePrice)\n#print(all_data.head())","8dca7b95":"#Lets have a look at sale price now\nsns.histplot(train.SalePrice,kde=True)","6e3909fb":"sns.barplot(x=all_data.OverallQual,y=all_data.SalePrice)","fdca4685":"#does the neighbourhood effect price\nplt.figure(figsize=(10,10))\nplt.xticks(rotation = 45)\nsns.barplot(x=all_data.Neighborhood,y=all_data.SalePrice)","74b06768":"#GrLiving area is the area above ground in square feet.\nsns.scatterplot(x=all_data.GrLivArea,y=all_data.SalePrice)","6871b7bd":"#are modern houses going for more or less?\nsns.scatterplot(x=all_data.YearBuilt,y=all_data.SalePrice)","b8d375c3":"#number of bathrooms imply bigger house?\nsns.stripplot(x=all_data.FullBath,y=all_data.SalePrice)","1ef54013":"#Dropping but saving the SalePrice for later Use as it is the dependent variable\nsalePrice = train.SalePrice\nall_data = all_data.drop(['SalePrice'],axis=1)\n\ncat = all_data.select_dtypes(include=['object'])\nnum = all_data.select_dtypes(exclude=['object'])\n#print(all_data)\n#print(cat.columns)","ba3ea7e1":"#Lets See how many they are\nprint(\"There are \" + str(num.shape[1]) + \" numerical values\")\nprint(\"There are \" + str(cat.shape[1]) + \" categorical values\")","5afdaa31":"#for i in cat.columns:\n#    plt.figure()\n#    sns.countplot(x=cat[i])\n\n#After carrying out the previous code it is clear that some of the variables have an extremely low variance - and should be dropped from the dataset, these columns are:\n\n#'Heating','RoofMatl','Condition2','Street','Utilities'\n\ncols = ['Heating','RoofMatl','Condition2','Street','Utilities']\nfor i in cols:\n    plt.figure()\n    sns.countplot(x=cat[i])","d6756bd9":"cat = cat.drop(['Heating','RoofMatl','Condition2','Street','Utilities'],axis=1)","1020220b":"print(num.columns)\nprint(num.shape[1])","b59f40f0":"print(len(strong_corr))\n\n#plotting the top correlated values, only doing first 10 as the last one is sale price which has now been removed.\nfor i in range(10):\n    plt.figure()\n    sns.histplot(num[strong_corr[i]],kde=True)","bb386365":"#Lets look at Categorical First\n#cat.info()\n#Lets look at how many null values there are\nfor i in cat.columns:\n    print(str(i) + ' has ' + str(cat[i].isnull().sum()) + ' null values')","b12edb82":"#From looking at the data description we can fill some of these values in already.\n\n#If null then it has no fireplace\n#MasVnrType and Functional have the following values if null from data description\ncat['FireplaceQu'] = cat['FireplaceQu'].fillna('None')\ncat['MasVnrType'] = cat['MasVnrType'].fillna('None')\ncat['Functional'] = cat['Functional'].fillna('Typ')\n\n#If null then no garage\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        cat[col] = cat[col].fillna('None')\n\n#If null then no basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        cat[col] = cat[col].fillna('None')\n\n\n# Replace the missing values in each of the columns below with their mode\ncat['Electrical'] = cat['Electrical'].fillna(\"SBrkr\")\ncat['KitchenQual'] = cat['KitchenQual'].fillna(\"TA\")\ncat['Exterior1st'] = cat['Exterior1st'].fillna(cat['Exterior1st'].mode()[0])\ncat['Exterior2nd'] = cat['Exterior2nd'].fillna(cat['Exterior2nd'].mode()[0])\ncat['SaleType'] = cat['SaleType'].fillna(cat['SaleType'].mode()[0])\ncat['MSZoning'] = cat['MSZoning'].fillna(cat['MSZoning'].mode()[0])","c059d3c3":"#Now lets see how many null values remain.\nfor i in cat.columns:\n    print(str(i) + ' has ' + str(cat[i].isnull().sum()) + ' null values')","0402a326":"#Now for filling in numerical data\n#num.info()\n#Lets look at how many null values there are\nfor i in num.columns:\n    print(str(i) + ' has ' + str(num[i].isnull().sum()) + ' null values')","c19ea2c8":"#no garage then 0\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        num[col] = num[col].fillna(0)\n\n#plt.hist(num.LotFrontage)\n#Plotting lot frontage to see whehter to fill with mean or median\n#As it is skewed we are going to replace it with the median.\nnum['LotFrontage'] = num['LotFrontage'].fillna(num['LotFrontage'].median())\n\n#fill the rests with 0\nfor col in ('BsmtHalfBath', 'BsmtFullBath','TotalBsmtSF','BsmtUnfSF','BsmtFinSF2','BsmtFinSF1','MasVnrArea'):\n    num[col] = num[col].fillna(0)\n","455c0c89":"#Lets see how many null values remain\nfor i in num.columns:\n    print(str(i) + ' has ' + str(num[i].isnull().sum()) + ' null values')","c3f67b84":"#total surface area = TotalbsmtSF + 1stFlrSF + 2ndFlrSf\n#Total bnathrooms = fullbath + 0.5*halfbath + bsmtfullbath + 0.5*bsmthalfbath\n#Overall = overallqual + overallCond\nnum['TotalSF'] = num['TotalBsmtSF'] + num['1stFlrSF'] + num['2ndFlrSF']\nnum['Total_Home_Quality'] = num['OverallQual'] + num['OverallCond']\nnum['Total_Bathrooms'] = (num['FullBath'] + (0.5 * num['HalfBath']) +\n                               num['BsmtFullBath'] + (0.5 * num['BsmtHalfBath']))\n\nnum['haspool'] = num['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nnum['has2ndfloor'] = num['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nnum['hasgarage'] = num['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nnum['hasbsmt'] = num['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nnum['hasfireplace'] = num['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","b485a8ae":"cat = pd.get_dummies(cat)\n#print(cat)\nprint(cat.shape)","b79d2918":"#Lets look at how skewed our data is on a box plot.\n#original plot is too skewed to see so we will scale it down!\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=num , orient=\"h\", palette=\"Set1\")","589f570d":"#Lets look at the skew values\nskew_features = num.apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skew_features.apply(lambda x: abs(x) > 0.5)\nhigh_skew = high_skew[high_skew == True]\nprint(high_skew)","83c041c8":"#dont want to transform our new feature egineered cols that fall under being highly skewed\ntransform_cols = [x for x in high_skew.index if x not in ['hasgarage','hasbsmt','haspool']]\n#print(transform_cols)\n\nfor i in transform_cols:\n    num[i] = np.log1p(num[i])\n\nprint(num.head())\n","c41e7f48":"#Lets put our numerical and categorical data back together\nnum.shape,cat.shape\ndata = pd.concat([num, cat], axis=1,sort=False)\ndata.shape,num.shape,cat.shape\ndata.shape\nprint(data.head())","571f9aa8":"#Split these into our test and train\ntrain_Y = salePrice\ntrain_X = data[data.train == 1]\ntest_X = data[data.train == 0]\n#test_X.shape\ntrain_X = train_X.drop(['train'],axis=1)\ntest_X = test_X.drop(['train'],axis=1)\n#drop train column in both","df81ec8f":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=8,#only changing this to 8 got a score of 0.12272\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)","eb22f524":"print(train_X)\ngbr.fit(train_X, train_Y)\nrf.fit(train_X, train_Y)","3b07f7c9":"predictions = gbr.predict(test_X)\npredictions = np.expm1(predictions) \npredictions = predictions.round(0)\nprint(predictions)\n\n#predictions2 = rf.predict(test_X)\n#predictions2 = np.expm1(predictions2)\n#predictions2 = predictions2.round(0)\n#print(predictions2)","c4574c22":"#Submission\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.Id = testID\nsubmission.SalePrice = predictions\nsubmission.head()\nsubmission.to_csv(\"submission.csv\", index=False)","14a48c23":"# Fixing skewed data","079bf986":"**Now we split our data intio numerical and categorical data to understand and pre-process both independently, before combining to use in our model.**","e7a84486":"**Here I use a gradient boosting regressor and a random forrest**\n**Tuning the hyper paramters was something I have had trouble understanding 100% but after reading and watching content on it and playing around with them for a while it seems to be good.**\n**However could be improved.**","144354d5":"**Now lets encoide our cateogrical data**","0d5ba878":"# **The steps that I have taken to for this task:**\n\n# 1. EDA\n# 2. Feature Egineering\n# 3. Fitting the Models","cf527c56":"**Here we are going to use are list of strongly correlated features from earlier to plot and get and idea of some of our numerical variables**","867944ff":"**We now repeat this process for numerical data**","528d1253":"# Lets Investigate are target variable : Sale Price","10b6806c":"**We can now see how many nulls are left**","e87abc37":"**As we can see, sale price is not normally distributed, the models that we will be using here will work better with niormally distributed data, so in iorder to fix this we will use numpy to log transform this variable**","24e3c5dc":"# We are now going to plot some graphs to understand more about our dataset","0f803146":"Can see very linear pattern between these two variables - this visualsation could have been predicted from the strong correlation value we were given earlier.","66429210":"# Putting back together and making Test and Train","8000806f":"# **MODEL**","43f016ef":"**Now lets look at our numerical data**.","f574410c":"**We see here that some of these are skewed, we will fix this in feature egineering.**\n\n# First lets fix null values","d114da7c":"Now lets look into skewed data and fix that!","14f3e743":"**Now it looks more normally distributed!**","dc054bbc":"**Looking at the data description provided allows us to know how to handle null values for certain features**","a44019cc":"# Feature egineering time!","8e9f9329":"**Now lets look at some of these categorical variables**","78f341cd":"# Now Lets take a look at correlations!\n**Looking at correlations will help us see what features are going to be most impoortant in our model.**","4dcab422":"**Can see clearly that in both the training and test datsets have a very large amount of null values in the columns - PoolQC, Fence, MiscFeature. As over 50% of their values are missing. I am going to drop these straight away as filling these with values will not help us.**\n\n**I am also going do drop the ID from both as we do not need them right now, however will require the test ID at the end for submission.**","ba8fdee4":"# Predicting House Prices - Using Gradient Boosting and Random Forrest Classifiers","c644b2c4":"**Now lets refine this graph so we can see it clearer and look at correlations with a strong correlation with sale price.**","bf089dcd":"**Below you will also see me combine both testing and training data, I add a new column to help differentiate this**","baad7da3":"**The first part of our EDA will be too look at the data we have, see the data types and see how many null values we have**","aaef6206":"**As we can see by lookoing above, some variables themselves are highly correlated, such as GarageCars and Garage Area - this can be detrimnetal to our model if two features are correlated themselves.**\n\n**Below I will plot some of these so we can get a better look into how they are correlated, some have been left out for the reasons stated above**","f7456ebe":"**We do not need the columns above due to their extremley high variance - these could affect our model as it may weigh certain features higher due to coincidences, as the majority of these features take on the same value.**"}}