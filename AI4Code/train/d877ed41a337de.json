{"cell_type":{"ba247bd3":"code","ebe59d59":"code","6cc2960c":"code","fab90e28":"code","1ba97c7a":"code","a6f28988":"code","4d926a63":"code","4d6e354c":"code","0e5c4b53":"code","93c1eeb6":"code","8d53fc55":"code","1ee53a06":"code","f16b25b0":"code","29549f16":"code","f980e63e":"code","de019438":"code","2a52c191":"code","41d798d1":"code","ba074ce8":"code","dde62d34":"code","749e7741":"code","5a5267c9":"code","92e875db":"code","240b8684":"code","e7ad8a69":"code","69c8a2d4":"code","2653f060":"code","1171d35c":"code","369900f9":"code","be8ab9dc":"code","04a52bb5":"code","cf50c4da":"code","b93b22a7":"code","7e75ade8":"code","3f993542":"code","e92d239f":"code","3371f23d":"code","9047fa28":"code","56526e2f":"code","4d1f04d3":"code","7ea01cca":"code","1000930b":"code","5eb5d0b9":"code","34d3e4b4":"markdown","9b5c52e6":"markdown","e1951d3e":"markdown","82ee640f":"markdown","02330163":"markdown","82da0513":"markdown","6e82fe60":"markdown","f214efca":"markdown","7987a51f":"markdown","85020519":"markdown","b8d41239":"markdown","ad19b0a5":"markdown","36746752":"markdown","3a5825ca":"markdown","af41e1c7":"markdown","60cda66a":"markdown","8c3da371":"markdown","57903322":"markdown","3fc9217a":"markdown","cb0e3d06":"markdown","ff2223cb":"markdown","88809c81":"markdown","7828c866":"markdown","d3ad9022":"markdown","9090fddb":"markdown","f29063c5":"markdown","24e01f33":"markdown","617cc46f":"markdown","45b09b24":"markdown","27e82c48":"markdown","eb638aed":"markdown","3c4a5479":"markdown","9e65b86f":"markdown","db4cc190":"markdown","df0bd72c":"markdown","195a4eb9":"markdown","98d18ee2":"markdown","d292ec04":"markdown","84e96be3":"markdown","a9833cf9":"markdown","7a1e5e15":"markdown","ddc3bb28":"markdown","872d1891":"markdown","3aa76b63":"markdown","d4a4b973":"markdown","568b35e4":"markdown","821d89b6":"markdown","42c94845":"markdown","925dcf41":"markdown","5d96722d":"markdown","d3fef564":"markdown","5f524d8d":"markdown","9c4c6861":"markdown","b434e3cc":"markdown","91bce1c8":"markdown"},"source":{"ba247bd3":"# Importing essential starter libraries\nimport numpy as np      # vectors and matrices || Linear Algebra\nimport pandas as pd     # tables and data manipulations \n\n# For Dates\nimport datetime\nfrom dateutil.relativedelta import relativedelta    # working with dates with style\nfrom scipy.optimize import minimize                 # for function minimization\n\n# Sklearn imports\nfrom sklearn import metrics\nfrom sklearn import datasets\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LogisticRegression # For this particular model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# statistics and econometrics\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Visualization Libraries\nimport matplotlib.pyplot as plt # plots\nimport seaborn as sns # more plots\nplt.style.use('fivethirtyeight')\n# Above is a special style template for matplotlib, highly useful for visualizing time series data\n\n# some useful functions\nfrom itertools import product\nfrom tqdm import tqdm_notebook\nimport warnings # `do not disturb` mode\nwarnings.filterwarnings('ignore')\n\n# To have graphs embedded in the notebook\n%matplotlib inline\n\n# Print the graphs as PDF and PNG\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('png', 'pdf')","ebe59d59":"# Load the dataset as a dataframe\nclaims = pd.read_csv('..\/input\/insurance-data\/insurance.csv')","6cc2960c":"# Check the first 5 entries\nclaims.head()","fab90e28":"# Check the number of rows and columns\nprint(\"There are \", claims.shape[0], \"rows and\", claims.shape[1], \"features.\")","1ba97c7a":"# Check if there's missing data\n# Turn it into a dataframe\npd.DataFrame(claims.isnull().sum(), columns=[\"Number of Missing Values\"]).T","a6f28988":"# Describe the statistical elements of the dataset\nclaims.describe().T#.T","4d926a63":"claims.info()","4d6e354c":"plt.style.use('seaborn')\nplt.figure(figsize=(10, 18))\nplt.subplot(2,2,1)\n# AGE Regression Plot\nsns.regplot(x='age', y='charges', data=claims, \n            scatter_kws = {'color': 'red', 'alpha':0.2}, \n            line_kws = {'color': 'black', 'alpha':1, 'lw':6})\nsns.kdeplot( claims['age'], claims['charges'], cmap=\"rainbow\")\nplt.title(\"Age vs. Charges\", fontname ='Times New Roman', size = 25, color ='red')\nplt.xlabel(\"Age\", fontname ='Times New Roman', size = 15, color ='blue')\nplt.ylabel(\"Charges\", fontname ='Times New Roman', size = 15, color ='blue')\nplt.tight_layout()\nplt.subplot(2,2,2)\n# BMI Regression Plot\nsns.regplot(x='bmi', y='charges', data=claims, \n            scatter_kws = {'color': 'magenta', 'alpha':0.2}, \n            line_kws = {'color': 'blue', 'alpha':1, 'lw':2})\nsns.kdeplot( claims['bmi'], claims['charges'], cmap=\"rainbow\")\nplt.title(\"BMI vs. Charges\", fontname ='Times New Roman', size = 25, color ='red')\nplt.xlabel(\"BMI\", fontname ='Times New Roman', size = 15, color ='blue')\nplt.ylabel(\"Charges\", fontname ='Times New Roman', size = 15, color ='blue')\nsns.set_style('whitegrid')\nplt.show()","0e5c4b53":"plt.figure(figsize=(10, 19))\nplt.style.use('seaborn')\nplt.tight_layout()\n# Age Plot\nplt.subplot(2,2,1)\nsns.scatterplot(x='age', y='charges', hue=claims['smoker'], size=claims['smoker'], style = claims['smoker'], data=claims)\nplt.title(\"Age vs. Charges with respect to Smoking Status\", fontname ='Times New Roman', size = 14, color ='red')\nplt.xlabel(\"Age\", fontname ='Times New Roman', size = 15, color ='blue')\nplt.ylabel(\"Charges\", fontname ='Times New Roman', size = 15, color ='blue')\n\n# BMI Plot\nplt.subplot(2,2,2)\nsns.scatterplot(x='bmi', y='charges', hue=claims['smoker'], size=claims['smoker'], data=claims)\nplt.title(\"BMI vs. Charges with respect to Smoking Status\", fontname ='Times New Roman', size = 14, color ='red')\nplt.xlabel(\"BMI\", fontname ='Times New Roman', size = 15, color ='blue')\nplt.ylabel(\"Charges\", fontname ='Times New Roman', size = 15, color ='blue')","93c1eeb6":"plt.figure(figsize=(20, 20))\nplt.tight_layout()\nplt.suptitle(\"Categorical Features vs Charges Box Plots\", fontname ='Times New Roman', size = 50, color ='purple')\n# Gender Boxplots\nplt.subplot(3, 3, 1)\nsns.boxplot(x = 'sex', y = 'charges', data = claims)\nplt.title(\"Gender vs Charges\", size = 20, color ='blue')\nplt.xlabel(\"Gender\")\n\n# Smoking Status\nplt.subplot(3,3,2)\nsns.boxplot(x = 'smoker', y = 'charges', data = claims)\nplt.title(\"Smoking Status vs Charges\", size = 20, color ='blue')\nplt.xlabel(\"Smoking Status\")\n\n# Region\nplt.subplot(3,3,3)\nsns.boxplot(x = 'region', y = 'charges', data = claims)\nplt.title(\"Region vs Charges\", size = 20, color ='blue')\nplt.xlabel(\"Region\")","8d53fc55":"gender = pd.DataFrame(claims['sex'].value_counts())\n\n# visualize fule type\n#sex = pd.DataFrame(claims['sex'].value_counts()).reset_index()\nplt.style.use('seaborn')\nexplode = (0.01, 0.01)\nvarient_colors = ['blue', 'deeppink']\ngender.plot.pie(y='sex', figsize=(8,8), autopct='%1.0f%%', explode=explode, startangle=90, shadow=True, \n                wedgeprops={'edgecolor': 'red'},  colors = varient_colors)\nplt.title(\"Gender Distribution Percentages\", fontname ='Times New Roman', size = 30, color ='purple')\nplt.show()\ngender.T","1ee53a06":"smoker = pd.DataFrame(claims['smoker'].value_counts())\n\nexplode = (0.01, 0.01)\nvarient_colors = ['blue', 'red']\nsmoker.plot.pie(y='smoker', figsize=(8,8), colors = varient_colors, autopct='%1.0f%%', explode=explode, startangle=90, shadow=True, wedgeprops={'edgecolor': 'red'})\nplt.title(\"Smoking Status Distribution Percentages\", fontname ='Times New Roman', size = 30, color ='purple')\nplt.show()\nsmoker.T","f16b25b0":"child = pd.DataFrame(claims['children'].value_counts())\n##########################################################################################\n# Set figure size\nplt.style.use('seaborn')\nplt.figure(figsize=(15,15))\nplt.tight_layout()\n\n# Set Plot Title for both subplots\nplt.suptitle(\"Dependents vs. Charges\", fontname ='Times New Roman', size = 30, color ='purple')\n\n# box Plots\nplt.subplot(3,3,1)\nsns.boxplot(x = 'children', y = 'charges', data = claims, palette=('rocket'))\n\n# Count \/Bar Plots\nplt.subplot(3,3,2)\nsns.countplot(claims.children, palette=(\"rocket\"))\n\n# Pie Chart\nplt.subplot(3,3,3)\nexplode = (0.1, 0.1, 0.1, 0.1, 0.1, 0.4)\nlabels=[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\npie = plt.pie(claims.children.value_counts(),\n              labels=labels,\n              autopct='%1.0f%%', \n              explode=explode,\n              #palette = ('rocket'),\n              startangle=90, shadow=True, \n              wedgeprops={'edgecolor': 'red'})\nplt.show()\nchild.T","29549f16":"plt.figure(figsize=(15,20))\nplt.style.use('seaborn')\nsns.pairplot(claims[['bmi', 'charges', 'sex', 'smoker']], hue=\"smoker\", \n             hue_order=None, palette = 'cubehelix');","f980e63e":"df = claims\n# Minimum price of the data\nminimum_price = np.min(df[\"charges\"])\n# Alternative using pandas\n# minimum_price = prices.min()\n\n# Maximum price of the data\nmaximum_price = np.max(df[\"charges\"])\n# Alternative using pandas\n# maximum_price = prices.max()\n\n# Mean price of the data\nmean_price = np.mean(df[\"charges\"])\n# Alternative using pandas\n# mean_price = prices.mean()\n\n# Median price of the data\nmedian_price = np.median(df[\"charges\"])\n# Alternative using pandas\n# median_price = prices.median()\n\n# Standard deviation of prices of the data\nstd_price = np.std(df[\"charges\"])\n# Alternative using pandas \n# std_price = prices.std(ddof=0)\n\n# There are other statistics you can calculate too like quartiles\nfirst_quartile = np.percentile(df[\"charges\"], 25)\nthird_quartile = np.percentile(df[\"charges\"], 75)\ninter_quartile = third_quartile - first_quartile\n\n# We show the calculated statistics\nprint(\"\\tStatistics for charges:\")\nprint('-'*42)\nprint(\"Minimum charges:                ${:,.2f}\".format(minimum_price))\nprint(\"Maximum charges:                ${:,.2f}\".format(maximum_price))\nprint(\"Average charges:                ${:,.2f}\".format(mean_price))\nprint(\"Median charge:                  ${:,.2f}\".format(median_price))\nprint(\"Standard deviation of charges:  ${:,.2f}\".format(std_price))\nprint(\"First quartile of charges:      ${:,.2f}\".format(first_quartile))\nprint(\"Second quartile of charges:     ${:,.2f}\".format(third_quartile))\nprint(\"Interquartile (IQR) of charges: ${:,.2f}\".format(inter_quartile))\nprint('-'*42)","de019438":"# Outlier Analysis\nplt.figure(figsize = [15,5])\nsns.boxplot(data=claims['charges'], orient=\"h\", palette=\"Set1\", color=\"red\")\nplt.title(\"Charges Distribution\", fontsize = 20, fontweight = 'bold')\nplt.xlabel(\"Price Range\", fontsize = 15, fontweight= 'bold')\nplt.ylabel(\"Continuous Variable\", fontsize = 15, fontweight= 'bold')\nplt.show()","2a52c191":"# Label Encoding for Sex and Smoker columns\n# importing label encoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# creating a label encode\nle = LabelEncoder()\n\n# Lvabel encoding for sex: 0 for females and 1 --> Males\nclaims['sex'] = le.fit_transform(claims['sex'])\n\n# Label encoding for smoker: 0 for Smoker; 1 for non-smoker\nclaims['smoker'] = le.fit_transform(claims['smoker'])\n\n# Label encoding for Region\nclaims['region'] = le.fit_transform(claims['region'])\n\n# check if the changes took effect\nclaims.head()","41d798d1":"data = claims\nfrom math import pi\n\n# Set data\ndf = pd.DataFrame({'group': [i for i in range(0, 1338)],\n                   'Charges': data['charges'],\n                   'Age': data['age'],\n                   'Children': data['children'],\n                   'Region':data['region'],\n                   'Gender':data['sex'],\n                   'BMI': data['bmi'],\n                   'Smoker':data['smoker']})\n \n# number of variable\ncategories = list(df)[1:]\nN = len(categories)\n \n# We are going to plot the first line of the data frame.\n# But we need to repeat the first value to close the circular graph:\nvalues=df.loc[0].drop('group').values.flatten().tolist()\nvalues += values[:1]\nvalues\n \n# What will be the angle of each axis in the plot? (we divide the plot \/ number of variable)\nangles = [n \/ float(N) * 2 * pi for n in range(N)]\nangles += angles[:1]\n \n# Initialise the spider plot\nax = plt.subplot(111, polar=True)\n \n# Draw one axe per variable + add labels labels yet\nplt.xticks(angles[:-1], categories, color='red', size=10)\n \n# Draw ylabels\nax.set_rlabel_position(0)\nplt.yticks([10,20,30], [\"10\",\"20\",\"30\"], color=\"blue\", size=10)\nplt.ylim(0,40)\n \n# Plot data\nax.plot(angles, values, linewidth=1, linestyle='solid')\nplt.title('Radar Chart for determing Importances of Features', fontsize = 20, fontname=\"Times New Roman\") \n\n# Fill area\nax.fill(angles, values, 'b', alpha=.3)","ba074ce8":"# splitting the dependent and independent variables\n#X = claim.drop('charges', axis = 1)\ninsure_data = claims[['age', 'bmi', 'children', 'sex', 'smoker', 'charges']]\n\nX = insure_data.drop(['charges'], axis = 1)\ny = insure_data.charges","dde62d34":"# check X\nX.head()","749e7741":"# check y\ny.head()","5a5267c9":"# Split Dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split as tts\n\nX_train, X_test, y_train, y_test = tts(X, y, test_size = 0.2, random_state = 30)","92e875db":"#print('-'*48)\nprint('X_train has', X_train.shape[0], 'rows, y_train also has', y_train.shape[0], 'rows')\nprint('-'*49)\nprint('X_test has', X_test.shape[0], 'rows, y_test also has', y_test.shape[0], 'rows')\nprint('-'*49)","240b8684":"# Import StandardScaler and rename it SS\nfrom sklearn.preprocessing import StandardScaler as SS\n\n# Feeding independent sets into the standardscaler \nX_train = SS().fit_transform(X_train)\nX_test = SS().fit_transform(X_test)","e7ad8a69":"# Create a linear regressor\nlin_reg_mod = LinearRegression()\n\n# Train the model using the X_train  and y_train sets\nlin_reg_mod.fit(X_train, y_train)","69c8a2d4":"# Model prediction on test data\nlin_reg_y_test_pred = lin_reg_mod.predict(X_test)","2653f060":"# Evaluation Metrics\n# RSquare\nfrom sklearn.metrics import r2_score\n\n# Print RSquare Score for test data\nprint(\"Test data R-squared score for Linear Regression Model is {}\".format(r2_score(y_test, lin_reg_y_test_pred)))","1171d35c":"from sklearn.tree import DecisionTreeRegressor\n# Assign DecisionTreeRegressor to the variable \"dt_regressor\"\ndt_regressor = DecisionTreeRegressor(random_state=0)\n# Fit the training data to the model\ndt_regressor.fit(X_train, y_train)","369900f9":"# Predict the results\ndt_y_test_pred = dt_regressor.predict(X_test)","be8ab9dc":"# Evaluate the Model\ndt_regressor.score(X_test, y_test)\n\n# Evaluate the model\nprint(\"Test dataset df_regressor score for Decision Tree Regression Model is {}\".format(dt_regressor.score(X_test, y_test)))","04a52bb5":"# import the regressor\nfrom sklearn.ensemble import RandomForestRegressor\n# Assign the RandomForestRegressor to rf\nrf_regressor = RandomForestRegressor(n_estimators = 10,\n                           criterion = 'mse', \n                           random_state = 0,\n                           n_jobs = -1)\n# Fit the Model with training data\nrf_regressor.fit(X_train, y_train)","cf50c4da":"# Model Prediction\nrf_y_test_pred = rf_regressor.predict(X_test)","b93b22a7":"# Evaluate the model\nprint(\"Test dataset R-squared score for Random Forest Regressor Model is {}\".format(rf_regressor.score(X_test, y_test)))","7e75ade8":"print(\"Linear Regression RSquared Score is {}\".format(r2_score(y_test, lin_reg_y_test_pred)))\nprint(\"Decision Tree Regression   Score is {}\".format(dt_regressor.score(X_test, y_test)))\nprint(\"Random Forest Regressor    Score is {}\".format(rf_regressor.score(X_test, y_test)))","3f993542":"lr_df = pd.DataFrame({'Real Values': y_test, 'Predicted Values': lin_reg_y_test_pred})\ndt_df = pd.DataFrame({'Real Values': y_test, 'Predicted Values': dt_y_test_pred})\nrf_df = pd.DataFrame({'Real Values': y_test, 'Predicted Values': rf_y_test_pred})","e92d239f":"plt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(20, 6))\n# Linear Reg Model Predictions \nsns.lineplot(x = y_test.index, y = y_test, label = \"Actual\", color = \"blue\", ax = ax)\nsns.lineplot(x = y_test.index, y = lin_reg_y_test_pred, label = \"Predictions\", color = \"red\", ax = ax)\nax.set_title(\"Price: Actual vs Predicted values for Linear Reg Model\", fontname ='Times New Roman', size = 30, color ='purple')\nax.set_xlabel(\"Index\")\nax.set_ylabel(\"Price\")","3371f23d":"# Model Prediction on Test Data\nLinPrediction = lin_reg_mod.predict(X_test)\n\n# Turn future_forecast into a Dataframe\nfuture_forecast = pd.DataFrame(LinPrediction, index = y_test.index, columns=['Prediction'])\n\n# Bring it together with the original\/real dataset\nforecast_fig = pd.concat([y_test,future_forecast], axis=1)\n\n# Graph the forecast vs real data\nfigsize = (30,5)\nforecast_fig.head(20).plot(kind='bar')\nplt.title('Actual vs Predicted values', size = 35, fontname = \"Times New Roman\")","9047fa28":"plt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(20, 5))\n#\nsns.lineplot(x = y_test.index, y = y_test, label = \"Actual\", color = \"blue\", ax = ax)\nsns.lineplot(x = y_test.index, y = dt_y_test_pred, label = \"Predictions\", color = \"red\", ax = ax)\nax.set_title(\"Charges: Actual vs Predicted values for Decision Tree Model\", fontname ='Times New Roman', size = 30, color ='purple')\nax.set_xlabel(\"Index\")\nax.set_ylabel(\"Charges\")","56526e2f":"# Model Prediction on Test Data\nDtPrediction = dt_regressor.predict(X_test)\n\n# Turn future_forecast into a Dataframe\nfuture_forecast = pd.DataFrame(DtPrediction, index = y_test.index, columns=['Prediction'])\n\n# Bring it together with the original\/real dataset\nforecast_fig = pd.concat([y_test,future_forecast], axis=1)\n\n# Graph the forecast vs real data\nfigsize = (15,5)\nforecast_fig.head(20).plot(kind='bar')\nplt.title('Actual vs Predicted values', size = 35, fontname = \"Times New Roman\")","4d1f04d3":"plt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(20, 5))\n\nsns.lineplot(x = y_test.index, y = y_test, label = \"Actual\", color = \"blue\", ax = ax)\nsns.lineplot(x = y_test.index, y = rf_y_test_pred, label = \"Predictions\", color = \"red\", ax = ax)\nax.set_title(\"Price: Actual vs Predicted values for Random Forest Model\", fontname ='Times New Roman', size = 30, color ='purple')\nax.set_xlabel(\"Index\")\nax.set_ylabel(\"Price\")","7ea01cca":"# Model Prediction on Test Data\nRFPrediction = rf_regressor.predict(X_test)\n\n# Turn future_forecast into a Dataframe\nfuture_forecast = pd.DataFrame(RFPrediction, index = y_test.index, columns=['Prediction'])\n\n# Bring it together with the original\/real dataset\nforecast_fig = pd.concat([y_test,future_forecast], axis=1)\n\n# Graph the forecast vs real data\nfigsize = (15,5)\nforecast_fig.head(20).plot(kind='bar')\nplt.title('Actual vs Predicted values', size = 35, fontname = \"Times New Roman\")","1000930b":"quote = rf_regressor.predict(np.asarray([20, 30, 5, 1, 1]).reshape(-1,5))\n\n# Print Quote\nprint(\"Quote based on Random Forest Model is $\",round(float(quote), 2))","5eb5d0b9":"quote1 = rf_regressor.predict(np.asarray([20, 30, 5, 1, 0]).reshape(-1,5))\n\n# Print Quote\nprint(\"Quote based on Random Forest Model is $\",round(float(quote1), 2))","34d3e4b4":">> ### e) Dependents Distribution","9b5c52e6":"> ### b) Model Prediction on Test dataset\n    > - Here we predict the results of the test set \n    > - We use `lin_reg_mod.predict()` function and assign it to the variable \"$lin$_$reg$_$y$_$test$_$pred$\".","e1951d3e":">###  b). Model Prediction: Predicting the Results\n    > - Here we predict the results of the test set with which the model trained on the training set values; \n        > - We use `dt_regressor.predict()` function and assign it to the variable \"***dtr_y_test_pred***\".","82ee640f":"<h2>7.1 Summary Model Evaluations<\/h2>\n\nWe use $\ud835\udc45^2$ to evaluate and find the best model.\n\n> - $\ud835\udc45^2$ : is a measure of the linear relationship between $X$ and $y$. It represents the goodness-of-fit of a regression model. Goodness of fit implies how better the regression model is fitted to the data points. The ideal value for $R^2$ is 1. The closer the value of $R^2$ to 1, the better is the model fitted.\n    \n> - One big limitation of $R^2$ is that its value always increases or remains the same as new variables are added to the model, without detecting the significance of this newly added variable (i.e value of $R^2$ never decreases on addition of new attributes to the model). As a result, non-significant attributes can also be added to the model with an increase in $R^2$ value; this can lead to overfitting of the model if there are large number of variables.","02330163":"### $Observation$\n- It looks like our dataset has no missing values.\n\n>> ### 3.1.4 Statistical Description of the dataset\n- **Looking at only Numerical Columns, what are some basic statistical observations?**\n","82da0513":"### $Analysis$\n- Indeed, smokers do get charged more as we can see from our charts above.\n- The older one is, $AND$ a smoker, the higher the charges, and the higher the BMI one has, $AND$ is a smoker, the higher the charges.\n- It all makes sense, smokers pose the highest risk, now on top of that, the age and BMI play a role- adding more risk if older or heavier.\n\n>> ### b) Categorical Features Analysis\n>> - #### Categorical Features vs Charges","6e82fe60":"<h1>Data Modeling<\/h1>\n<h2> 5.1 Split the data into features and label<\/h2>\n\n- Here we split our original dataset into two: features (that will be used in the model) and label (our target variable).","f214efca":"> ## 4.1 Label Encoding for non-numerical columns\nLet us change entries that are non-numerical in our dataset","7987a51f":"<h1> Data Collection<\/h1>\nThe first step in the Machine Learning process is getting data.\n- We will use data from the ever-famous internet respository site, [Kaggle](https:\/\/www.kaggle.com\/mirichoi0218\/insurance). \n\n> ## 2.1 Import Neccessary Libraries","85020519":"> ## 4.2 Feature Selection\nThis is the process of selecting features which are most relevant in predicting the output variable.\n> - <font color='red'>$\\implies$<\/font> It helps reduce data dimensionality and \n> - <font color='red'>$\\implies$<\/font> Ensures that models' accuracy can be trusted when those features are out. Learn more [here](https:\/\/en.wikipedia.org\/wiki\/Feature_selection).\nAfter exploratory data analysis, we saw that some features do not affect insurance charges that much. We saw that \n> * * `Sex` and \n> * * `Region` \nhave an insignificant impact on the charges.\n\n> Such features can be dropped. But before we do, let us create a radar chart that shall prove to us that thos features do no affect charges.\n>> ### 4.2.1 Radar Chart\n>> - A radar chart is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point (Source: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Radar_chart))","b8d41239":"> ### b). Model Prediction: Predicting the Results\n> - Here we predict the results of the test set with which the model trained on the training set values; \n> - We use `regressor.predict()` function and assign it to the variable \"***rf_y_test_pred***\".","ad19b0a5":"> ## 2.2 Data Ingestion","36746752":"<h1>Conclusion<\/h1>","3a5825ca":"### $Analysis$\n- We can see the upward trend in the $Age$ $vs.$ $Charges$: The older the policyholder, the more the charges.\n\n- We also see the upward trend in the $BMI$ $vs.$ $Charges$: The higher the policyholder's BMI, the more the charges.","af41e1c7":"<h2>7.2 Comparing Real Values $vs.$ Predicted Values<\/h2>\n<li> Let's display the values of y_test as \"Real Values\" and lin_reg_y_test_pred as Predicted Values in a Pandas DataFrame.<\/li>","60cda66a":"### $Observation$\n- We have $1338$ entries (rows) and $7$ features (columns). This is not a big dataset.\n\n>> ### 3.1.3 Missing Data: \n- **Do we have missing data under each feature?**\n","8c3da371":">> ### 3.2.2 Univariate Analysis: \n>>> #### 3.2.2.1 Focusing on Charges\nLet's put our focus only on the `charges` column, so we have a general idea of how much people paid in premiums, taking into account statistics like min, max prices, std dev, quatiles and so on. ","57903322":"### Random Forest Model","3fc9217a":">> ### c) Gender Distribution\n>> - #### How many males and females in the dataset?","cb0e3d06":"<h2>6.3 Random Forest Regressor<\/h2>","ff2223cb":"> ## 3.2 Exploratory Data Analysis\nExploratory Data Analysis (EDA) is the philosophy of analyzing datasets to to summerize their main characteristics, usually employing graphs and data vizualization techniques[ [1] ](https:\/\/en.wikipedia.org\/wiki\/Exploratory_data_analysis).\n\n>This is the critical first step in analyzing the data; here are some reasons for using EDA:\n  >* Detection of mistakes\n  >* Determining relationships among the features\n  >* Assessing assumptions on which statistical inference will be based, etc. \n  \n>It is worth noting that EDA is not really a formal process with rigid set of rules or path to follow, it is waht one makes it. The aim is to uncover whatever may be hidden in the data, so one should feel free to investigate whatever idea that comes to mind. Of course some ideas will yield some positive outcomes, others not so much. To successfully perform data cleaning, we'll need to deploy EDA tools such as visualisation, transformation and modelling. \n\n>> ### 3.2.1 Multivariate Analysis:\n> - Let us dig deeper to see how feature affects charges\n>> ### a) Age, BMI $vs.$ Charges\n> - #### Do Charges increase with Age?","88809c81":">> ### 3.1.2 Size of the dataset: How many entries are in our dataset?","7828c866":"### $Analysis$\n- We see that for this particular individual they'd be charged $59020.95 in premiums.\n\n\n---\n> Now you may be wondering, \"how realible is this model?\". Remember in our Exploratory Data Anaysis we did say that Smokers get charged more. Since the above individual is a smoker, what if you were to present me with the data of a non-smoker- would we get lower quote? Let's find out- this is to further validate our model.\n\n> We will use same data provided but change one thing- smoking status (to No).","d3ad9022":"<h3>5.2.1 Check if shapes of split data match: for train sets and test sets<\/h3>","9090fddb":"### $Observation$\n- We can see that we do have outliers in this dataset, and we need to handle them.\n### Handling Outliers\n- We will use **IQR (Interquartile Range Method)** to deal with outliers:\n#### How it works\n* IQR is calculated as the difference between the $25th$ and the $75th$ percentile of the data. The percentiles can be calculated by sorting the selecting values at specific indices. The IQR is used to identify outliers by defining limits on the sample values that are a factor k of the IQR. The common value for the factor k is the value $1.5$.\n\n","f29063c5":"#### Standard Scailing. ","24e01f33":"\n<h1>Feature Engineering<\/h1>\n\n#### What is it?\nThis is the process of extracting features that can help improve the Machine Learning algorithms.\n","617cc46f":"<li>We see that the best model amongst the three we used is Random Forest Regressor\n    <\/li>\n<li>We also see some illustrations of actual charges $vs$ predicted charges- this is to paint a better picture so as to get a sense of the accuracy of the models, even without the scores.\n    \n<h3>\n    Understanding Predictions better\n    <\/h3>\n    <li>\n    What if you were to give me details of an individual and you want to know how much they'd be charged? \n    <\/li>\n    \n> #### Let's say you have the following details about that someone:\n- Age, \n- BMI, \n- Number of children, \n- Sex (gender), \n- Smoking status\n\n<li>Using the best model from the above - Random Forest- I should be able to give you a quote.<\/li>\n\n><font color='red'> Let's assume the individual has the following details:<\/font> \n\n>>- $\\implies$ Age: 34\n>>- $\\implies$ BMI: 30\n>>- $\\implies$ Number of Children: 5\n>>- $\\implies$ Gender: 1 (male)\n>>- $\\implies$ Smoking Status: 1 (Yes, a smoker)\n\n>> Then the array will be [34, 30, 5, 1, 1]","45b09b24":"<h2>6.2 Decision Tree Regressor<\/h2>","27e82c48":"> ###  c).  Model Evaluation","eb638aed":"### $Analysis$\n- The number of males is almost equalto the number of available females: 51% to 49% respectively.\n\n>> ### d) Smoking Status Distribution\n>> - #### How many policy holders smoke in the dataset?","3c4a5479":">### a). Training the Decision Tree Regression Model on the training dataset\n    > - Assign `RandomForestRegressor` to the variable **\"rf_regressor\"**\n        > - Then we fit the ***$X$_$train$*** and ***$y$_$train$*** data to the model using `rf_regressor.fit()` function.","9e65b86f":"<h1> Insurance Claims Prediction Project<\/h1>\n\n>## 1.1 Problem Statement\n- The objective is to predict individual's insurance premium based on allocated features that are taken into consideration for underwriting purposes; we also want to (along the way) identify premium holders who pose a risk of making making claims agains their premiums in the near future. \n\n>> ### 1.1.1 Project Goal\n- To predict insurance charges based on given features. \n\n>> ### 1.1.2 Possible Questions\n- What features affect insurance pricing\n- Does Data Visualization offer better insights into the data?\n- What hidden patterns can we reveal from the dataset?\n- Data segnmentation Analysis: Which group seems to be charged more? $e.t.c.$\n\n>## 1.2 About The Dataset\nThis data was scrped from [Kaggle](https:\/\/www.kaggle.com\/mirichoi0218\/insurance).\nWhile we will dive deeper into the features involved in our data, below are a list and definitions of them:\n- **Charges**: Total medical expense charged to the plan for the calendar year\n- **Age**: Insurance policyholder\u2019s age, ranging from 18 to 64\n- **Sex**: Insurance policyholder\u2019s gender identity- male or female\n- **BMI**: The Body Mass Index, used to vary premim costs\n- **Children**: Number of dependants\/children on parents' insurance plan\n- **Smoker**: Smoking status of an insurance policyholder\n- **Region**: The beneficiary\u2019s regional residential area within the US:Northeast, Southeast, Northwest, Southwest\n\n\n> ## 1.3 Methodology\n- **Step 1- Data collection:** This will involve scapping of structured datafrom Kaggle.\n- **Step 2- Data Preprocessing:** In this phase, the data is prepared for the analysis purpose which contains relevant information. Pre-processing and cleaning of data are one of the most important tasks that must be one before dataset can be used for machine learning. The real-world data is noisy, incomplete and inconsistent. So, it is required to be cleaned.\n- **Step 3- Extraction of Feature Set\/Training Data:** Feature set or training data can be prepared from the cleaned data by using any of the available techniques. The feature sets and training set that has obtained by using any method will be used for the implementation of machine learning algorithms.\n- **Step 4- Implementation of Machine Learning Algorithm on Feature Set\/Training Data:** \n- **Step 5: Testing of Data:** Testing of data is done based on training model which is classified using supervised learning algorithm.\n\n> ## 1.4 Experimental Design\n- We will use static data- dataset already available rather than real-time data from an IoT system.\nLinks to the Datasets\n- [Kaggle](https:\/\/www.kaggle.com\/mirichoi0218\/insurance)\n- [Enigma](https:\/\/public.enigma.com\/?gclid=EAIaIQobChMIyf35qdXm2gIV0AQqCh19iQYuEAMYASAAEgJduvD_BwE)\n\n> ## 1.5 Software tools & Hardware Requirements: \n- Jupyter Notebook, \n- Python3 Liraries such as [NumPy](https:\/\/numpy.org\/doc\/stable\/user\/quickstart.html), [Pandas](https:\/\/pandas.pydata.org\/docs\/), [Matplotlib](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/visualization.html) and [Seaborn](https:\/\/seaborn.pydata.org\/) \n- Supervised Learning libraries such [Scikit-Learn](https:\/\/scikit-learn.org\/stable\/)\n  ... will be exploited for the development and experimentation of the project. ","db4cc190":"<h2> 5.2 Train-Test Split: split the data into Training and Testing Sets<\/h2>","df0bd72c":"<h1>Data Preprocessing & Exploratory Data Analysis<\/h1>\n\n**What is it?**\n\n- According to [Techopedia](https:\/\/www.techopedia.com\/definition\/14650\/data-preprocessing), **Data Preprocessing** is a Data Mining technique that involves transforming raw data into an understandable format.\n\n**Why Do We Need Data Preprocessing?**\n- Since real-world data tends to be incomplete, noisy or inconsistent, we need this process in order to eliminate such to avoid poor quality models we'll build later. \n- Data Preprocessing provides operations which can organise the data into a proper form for better understanding in data mining process.\n\n**Stages of Data Preprocessing**\nData preprocessing is divided into four stages: \n    1. Data cleaning, \n    2. Data integration, \n    3. Data reduction, and \n    4. Data transformation.\n    \n> ## 3.1 Data Cleaning & Inspection\n**What is Data Cleaning?**\n\nIn Data Science, data cleaning can be described in many ways, one of them being: <font color=red>***the process of fixing and\/or removing incorrect, corrupted, wrongly-formatted, incomplete or duplicate data within a dataset***.<\/font>\n\nQuite mouthful and yet comprehensive. We want to ensure that our dataset has no duplicate data or does not contain any corrupted entries that will otherwise lead to wrong\/less useful models. \n\nAs we make use of multiple data sources in data analysis, the chances of duplicating data are very high, and so are those of mislabeling the data. It goes without saying that incorrect data leads to unreliable algorithms and predictions or outcomes. While there are no sure steps of going about data cleaning due to the nature of different datasets, it is however vital to build some sort of a framework or template for data cleaning process for future references so as to at least be close to doing it right each time.\n\n>> ### 3.1.1 Exploring the Dataset: **Understanding the data**\n With the data fed into our notebook, it's time we take a look at it: we need to spend some time exploring it in order to understand what the features represent in each column. We want to avoid or at least minimize mistakes in the data analysis and the modeling process. \nWithout any further ado, let's dive right into our loaded dataset.\n","195a4eb9":"### Decision Tree","98d18ee2":"> ### a). Training the Decision Tree Regression Model\n   > - Assign `DecisionTreeRegressor` to the variable **\"dt_regressor\"**\n   > - Then we fit the ***$X$_$train$*** and ***$y$_$train$*** data to the model using `dt_regressor.fit()` function.","d292ec04":"### Show Real Charges vs Predicted Charges for the first 20 entries","84e96be3":"> ### c). Model Evaluation","a9833cf9":">> ### a) Age, BMI $vs.$ Charges with respect to Smoking\n>>- #### Do smokers get charged more?","7a1e5e15":"### $Analysis$\n- While the drop is huge, it goes to magnify the abilities of our model. Yes, we indeed see that smokers get charged more and non-smokers not so much. \n\n\nI do hope that this short offers a better insight into analysing insurance data and that it offers an overview of how we can make the most out of the simplest Machine Learning Models in the insurance industry for underwriting and many other purposes. ]\n","ddc3bb28":"## Visualizing Models Results\n### Linear Regression","872d1891":"### $Analysis$\n- <font color='red'>$\\implies$<\/font>Most (80%) of the policyholders do not smoke.","3aa76b63":"### $Observation$\n\nThe above returns important statistical summaries for our dataset.\n- **Age**: Average age of our policyholders is 39, the youngest being $18$ and the olders aged $64$.\n- **BMI:** Average BMI is $30.7$; $25$% of our policyholders had a BMI of $34.7$.\n- **Children:** Most policyholders had on average 1 child on thier policies, with only $25$% having 2 children, and $75$% with no children.\n\n>> ### 3.1.5 Data Types \n    >> * **What Data Types Do we have in the dataset?**","d4a4b973":"> ### a) Train the Linear Regression Model \n> - Assign `LinearRegresson` to the variable **\"lin_reg-mod\"**\n   > - Then we fit the ***$X$_$train$*** and ***$y$_$train$*** data to the model using `lin_reg_mod.fit()` function.","568b35e4":"#### $Analysis$\n   1. - <font color='red'>$\\implies$<\/font> **Gender**: Charges seem to not rise based on gender. \n   2. -  <font color='red'>$\\implies$<\/font>**Sming Status**: Smoking does have an impact on charges.\n   3. -  <font color='red'>$\\implies$<\/font>**Region**: Region does not seem to affect charges.","821d89b6":">> ### f) Bivariate Pairplots","42c94845":"### $Analysis$\n- We see that most policyholders did not have dependents (children)","925dcf41":"### Show Real Charges vs Predicted Charges for the first 20 entries","5d96722d":"### Show Real Charges vs Predicted Charges for the first 20 entries","d3fef564":"<h1>Summary Model Evalautions & Predictions<\/h1>","5f524d8d":"> ### c) Linear Regression Model Evaluation","9c4c6861":"\n<h1>Machine Learning Models Implementation<\/h1>\n<h2>6.1 Linear Regression Model<\/h2>","b434e3cc":"### $Analysis$\n- There we have it, just like we say\n        - Region\n        - Children and\n        - Sex \nfeatures do not have a significant impact on charges and can be dropped accordingly.","91bce1c8":">>> #### 3.2.2.2 Outlier Analysis in Charges\n![Outliers_illustrated](attachment:image.png)"}}