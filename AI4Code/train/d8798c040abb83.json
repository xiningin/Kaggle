{"cell_type":{"a8f261a3":"code","3c1a2832":"code","f3765af8":"code","50b8867b":"code","015a3de1":"code","e3559c22":"code","3db008bc":"code","20cf6c7c":"code","d62c2ba2":"code","9859ddaf":"code","78bd5497":"code","01f803fd":"code","15684da2":"code","cb65ca54":"code","2643b6c4":"code","d5770607":"code","6ffc9332":"code","a166d96d":"code","f2670f02":"code","9b16bca0":"code","4f6dd505":"code","2dda3e18":"code","6f15b41d":"markdown"},"source":{"a8f261a3":"#https:\/\/www.kaggle.com\/dmitryuarov\/ventilator-pressure-eda-lstm-0-189\/notebook\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.preprocessing import RobustScaler\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\npd.set_option('display.max_columns', None)","3c1a2832":"train = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\nss = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')","f3765af8":"train.info()","50b8867b":"test.info()","015a3de1":"def prepare_set(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_1st_derivative'] = (df['u_in'].diff().fillna(0) \/ df['time_step'].diff().fillna(0)).fillna(0)\n    #df['u_in_1st_der_cumsum'] =  (df['u_in_1st_derivative']).groupby(df['breath_id']).cumsum()\n    df['expand_mean_1sr_der'] = df.groupby('breath_id')['u_in_1st_derivative'].expanding(2).mean().reset_index(level=0,drop=True)\n    #df['expand_max_1sr_der'] = df.groupby('breath_id')['u_in_1st_derivative'].expanding(2).max().reset_index(level=0,drop=True) \n    #df['expand_std_1sr_der'] = df.groupby('breath_id')['u_in_1st_derivative'].expanding(2).std().reset_index(level=0,drop=True)\n    #df['u_in_1st_der_mean5'] = df.groupby('breath_id')['u_in_1st_derivative'].rolling(window=5, min_periods=1).mean().reset_index(level=0,drop=True)\n    df['u_in_1st_der_mean10'] = df.groupby('breath_id')['u_in_1st_derivative'].rolling(window=10, min_periods=1).mean().reset_index(level=0,drop=True)\n                \n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2).fillna(0).reset_index(level=0,drop=True)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4).fillna(0).reset_index(level=0,drop=True)\n    df['u_in_lag-2'] = df.groupby('breath_id')['u_in'].shift(-2).fillna(0).reset_index(level=0,drop=True)\n    df['u_in_lag-4'] = df.groupby('breath_id')['u_in'].shift(-4).fillna(0).reset_index(level=0,drop=True)  \n        \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df = pd.get_dummies(df)\n       \n    df['ewm_u_in_mean'] = df.groupby('breath_id')['u_in'].ewm(halflife=10).mean().reset_index(level=0,drop=True)\n    df['ewm_u_in_std'] = df.groupby('breath_id')['u_in'].ewm(halflife=10).std().reset_index(level=0,drop=True)\n    df['ewm_u_in_corr'] = df.groupby('breath_id')['u_in'].ewm(halflife=10).corr().reset_index(level=0,drop=True)\n    \n    #df['rolling_5_mean'] = df.groupby('breath_id')['u_in'].rolling(window=5, min_periods=1).mean().reset_index(level=0,drop=True)\n    #df['rolling_5_max'] = df.groupby('breath_id')['u_in'].rolling(window=5, min_periods=1).max().reset_index(level=0,drop=True)\n    #df['rolling_5_std'] = df.groupby('breath_id')['u_in'].rolling(window=5, min_periods=1).std().reset_index(level=0,drop=True)\n    \n    df['rolling_10_mean'] = df.groupby('breath_id')['u_in'].rolling(window=10, min_periods=1).mean().reset_index(level=0,drop=True)\n    df['rolling_10_max'] = df.groupby('breath_id')['u_in'].rolling(window=10, min_periods=1).max().reset_index(level=0,drop=True)\n    df['rolling_10_std'] = df.groupby('breath_id')['u_in'].rolling(window=10, min_periods=1).std().reset_index(level=0,drop=True)\n             \n    df['expand_mean'] = df.groupby('breath_id')['u_in'].expanding(2).mean().reset_index(level=0,drop=True)\n    df['expand_max'] = df.groupby('breath_id')['u_in'].expanding(2).max().reset_index(level=0,drop=True)\n    df['expand_std'] = df.groupby('breath_id')['u_in'].expanding(2).std().reset_index(level=0,drop=True)\n    \n    df['delta_u_in'] = abs(df.groupby(df['breath_id'])['u_in'].diff().fillna(0)).reset_index(level=0,drop=True)\n    df['delta_u_in_exp'] = df.groupby(df['breath_id'])['delta_u_in'].rolling(window=10, min_periods=1).mean().reset_index(level=0,drop=True)\n    df['delta_rolling_10_mean'] = df.groupby('breath_id')['delta_u_in'].rolling(window=10, min_periods=1).mean().reset_index(level=0,drop=True)\n    df['delta_rolling_10_max'] = df.groupby('breath_id')['delta_u_in'].rolling(window=10, min_periods=1).max().reset_index(level=0,drop=True)\n    #df['delta_rolling_10_std'] = df.groupby('breath_id')['delta_u_in'].rolling(window=10, min_periods=1).std().reset_index(level=0,drop=True)\n    \n    #df['area_10_mean_exp']=(df['rolling_10_mean']*df['time_step']).expanding(2).mean().reset_index(level=0,drop=True)     \n   \n    df['work']=((df['u_in'] + df['u_in'].shift(1).fillna(0))\/2 * df['time_step'].diff().fillna(0)).clip(0,)\n    df['work_roll_10']=df.groupby(df['breath_id'])['work'].rolling(window=10, min_periods=1).sum().reset_index(level=0,drop=True)\n    df['work_roll_15']=df.groupby(df['breath_id'])['work'].rolling(window=15, min_periods=1).sum().reset_index(level=0,drop=True)\n      \n    df['u_in_rol_q0.1'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.1).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.25'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.25).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.5'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.5).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.75'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.75).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.9'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.9).reset_index(level=0,drop=True)  \n        \n    df = df.fillna(0)\n    \n    return df","e3559c22":"train=prepare_set(train)\ntrain.shape","3db008bc":"test=prepare_set(test)\ntest.shape","20cf6c7c":"#plt.figure(figsize=(20,20))\n#sns.heatmap(train.drop([ 'id', 'breath_id'], axis=1).corr(), annot=True)","d62c2ba2":"train.drop([ 'id', 'breath_id'], axis=1).corr().pressure.sort_values(ascending=False)","9859ddaf":"sample=train.sample(4)\nbreath_id=list(sample['breath_id']) \n\nfor ID in breath_id:\n    case = train[train.breath_id == ID]\n      \n    fig = make_subplots(rows=1, cols=1, x_title = \"Time\", subplot_titles=[f'Breath id: {ID}'])\n    fig.add_trace(go.Scatter(x=case['time_step'], y=case['u_in'], name='u_in'), row=1, col=1)\n    fig.add_trace(go.Scatter(x=case['time_step'], y=case['pressure'], name='pressure'), row=1, col=1)\n    fig.add_trace(go.Scatter(x=case['time_step'], y=case['u_in_rol_q0.75'], name='u_in_rol_q0.75'), row=1, col=1) \n    fig.add_trace(go.Scatter(x=case['time_step'], y=case['rolling_10_mean'], name='rolling_10_mean'), row=1, col=1)\n    fig.add_trace(go.Scatter(x=case['time_step'], y=case['work_roll_15'], name='work_roll_15'), row=1, col=1)\n   \n    fig.show()","78bd5497":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['pressure', 'id', 'breath_id'], axis = 1, inplace = True)\ntest = test.drop(['id', 'breath_id'], axis = 1)","01f803fd":"RS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)","15684da2":"#from sklearn.decomposition import PCA\n#pca=PCA(n_components=0.995)\n#train = pca.fit_transform(train)\n#test = pca.transform(test)","cb65ca54":"train = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","2643b6c4":"#in case of TPU run\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","d5770607":"#LSTM\/GRU ensemble\ndef get_model():       \n    inputs = keras.Input(shape = train.shape[-2:])\n    \n    x = layers.Bidirectional(layers.GRU(256, return_sequences = True))(inputs)\n    x = layers.Bidirectional(layers.GRU(192, return_sequences = True))(x)\n    x = layers.Bidirectional(layers.GRU(128, return_sequences = True))(x) \n    x = layers.Bidirectional(layers.GRU(96, return_sequences = True))(x) \n    \n    y = layers.Bidirectional(layers.LSTM(256, return_sequences = True))(inputs)\n    y = layers.Bidirectional(layers.LSTM(192, return_sequences = True))(y)\n    y = layers.Bidirectional(layers.LSTM(128, return_sequences = True))(y) \n    y = layers.Bidirectional(layers.LSTM(96, return_sequences = True))(y) \n    \n    combined = layers.Concatenate()([x,y])\n    \n    out = layers.Dense(128, activation = 'swish')(combined)\n    out = layers.Dense(1)(out)\n    \n    model = keras.Model(inputs, out)\n    \n    model.compile(optimizer = \"adam\", loss = \"mae\")\n           \n    return model  ","6ffc9332":"def get_model():   \n    \n    model = tf.keras.models.Sequential([\n            layers.Input(shape = train.shape[-2:]),\n            layers.Bidirectional(layers.GRU(512, return_sequences = True)),\n            layers.Bidirectional(layers.GRU(256, return_sequences = True)),\n            layers.Bidirectional(layers.GRU(192, return_sequences = True)),\n            layers.Bidirectional(layers.GRU(128, return_sequences = True)),\n            layers.Dense(64, activation = 'selu'),\n            layers.Dense(1),\n        ])\n    model.compile(optimizer = \"adam\", loss = \"mae\")\n    \n    return model  ","a166d96d":"def get_model():       \n    inputs = keras.Input(shape = train.shape[-2:])\n    \n    x = layers.Bidirectional(layers.GRU(512, return_sequences = True))(inputs)\n    x = layers.Bidirectional(layers.GRU(384, return_sequences = True))(x)\n        \n    y = layers.Bidirectional(layers.GRU(256, return_sequences = True))(x) \n    \n    z = layers.Bidirectional(layers.GRU(192, return_sequences = True))(y) \n        \n    combined = layers.Concatenate()([x,y,z])\n    \n    out = layers.Dense(128, activation = 'swish')(combined)\n    out = layers.Dense(1)(out)\n    \n    model = keras.Model(inputs, out)\n    \n    model.compile(optimizer = \"adam\", loss = \"mae\")\n           \n    return model  ","f2670f02":"get_model().summary()","9b16bca0":"tf.keras.utils.plot_model(get_model(), show_shapes=True)","4f6dd505":"EPOCH = 300\nBATCH_SIZE = 2048\nn_folds=5\n\nwith strategy.scope():\n    kf = KFold(n_splits = n_folds, shuffle = True, random_state = 0)\n    test_preds = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        print('-'*15, '>', f'Fold {fold+1}\/{n_folds}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        model = get_model()\n\n        estop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, verbose=0, mode='min',restore_best_weights=True)\n        scheduler = keras.optimizers.schedules.ExponentialDecay(3e-3, 40*((len(train))\/BATCH_SIZE), 1e-4)\n        lr = keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)\n\n        model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = EPOCH, batch_size = BATCH_SIZE, callbacks = [lr, estop])\n\n        test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())","2dda3e18":"ss['pressure'] = sum(test_preds) \/ 5\nss.to_csv('submission.csv', index = False)","6f15b41d":"#### Based on this notebook:<br>\nhttps:\/\/www.kaggle.com\/dmitryuarov\/ventilator-pressure-eda-lstm-0-189\/"}}