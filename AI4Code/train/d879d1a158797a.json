{"cell_type":{"c9478bec":"code","150da309":"code","70652f4c":"code","b3037ae0":"code","36668fd0":"code","28855295":"code","4bf21796":"code","8e3cc3ed":"code","e07ac1df":"code","2a855bf7":"code","57929aec":"code","4dda35d4":"code","fc42a57d":"code","abcd6f31":"code","b59c1621":"code","763d06e4":"code","1a0a7440":"code","ebada41d":"markdown","f2bf045b":"markdown","a382cb0c":"markdown","a85e4f1a":"markdown","7c202a8a":"markdown","03c14333":"markdown","b63b0aa7":"markdown","484c09f3":"markdown","31aa28e0":"markdown","defffdde":"markdown","77f5f662":"markdown","383eb890":"markdown","40e4ab74":"markdown"},"source":{"c9478bec":"### Necessary Imports\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import train_test_split","150da309":"samp = pd.read_csv('..\/input\/predicting-bank-telemarketing\/samp_submission.csv')\ntrain = pd.read_csv('..\/input\/predicting-bank-telemarketing\/bank-train.csv')\ntest = pd.read_csv('..\/input\/predicting-bank-telemarketing\/bank-test.csv')\ntrain.drop(columns = 'duration')\n","70652f4c":"train.head()","b3037ae0":"test.head()","36668fd0":"### This gives us the probability of each occurance\ntrain['y'].value_counts(1)","28855295":"### In the sample data (which only has the client id), we randomly assign success values based off the probabilities shown above\nsamp.Predicted = np.random.choice(range(2), size = samp.shape[0], p = [train['y'].value_counts(1)[0], train['y'].value_counts(1)[1]])","4bf21796":"samp.to_csv('first_test.csv', index = False)","8e3cc3ed":"print(train.columns)\ntrain.dropna()","e07ac1df":"train = pd.get_dummies(train)\ntest = pd.get_dummies(test)\ntrain.head()","2a855bf7":"X = train.drop(columns = 'y')\nX = X.drop(columns = 'id')\nX = X.drop(columns = 'duration')\nY = train['y']\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2)\n\ntest = test.drop(columns = 'id')\ntest = test.drop(columns = 'duration')\ntest['default_yes'] = 0","57929aec":"## Fitting the tree to our testing data \ntree = DecisionTreeClassifier(max_depth = 5)\ntree.fit(X_train,Y_train)","4dda35d4":"## Running the tree on our training and testing data\nprint(\"Training accuracy:\", tree.score(X_train, Y_train))\nprint(\"Testing accuracy:\", tree.score(X_test, Y_test))","fc42a57d":"pd.DataFrame({'Gain': tree.feature_importances_}, index = X_train.columns).sort_values('Gain', ascending = False)","abcd6f31":"## Running bagging classifier on our original decision tree\nbag_model = BaggingClassifier(base_estimator=tree, n_estimators=100,bootstrap=True)\nbag_model = bag_model.fit(X_train,Y_train)\ny_pred = bag_model.predict(X_test)\nprint(\"Training accuracy: \", bag_model.score(X_train,Y_train))\nprint(\"Testing accuracy: \", bag_model.score(X_test,Y_test))\n","b59c1621":"\nfeature_importances = np.mean([\n    tree.feature_importances_ for tree in bag_model.estimators_\n], axis=0)\n\npd.DataFrame({'Gain': tree.feature_importances_}, index = X_train.columns).sort_values('Gain', ascending = False)","763d06e4":"predictions = pd.DataFrame(bag_model.predict(test))\n","1a0a7440":"samp['Predicted'] = predictions\nsamp.to_csv('second_test.csv', index=False)","ebada41d":"### Creating Dummy Variables\n- Some of our columns contain categorical variables, like job status and marital status. Unfortunately, the decision tree can't handle categorical variables\n- By creating dummy variables, we create a new column for each category in the original column\n    - For example, the **marital** column would be broken up into 3 new columns: Single, Married, and Divorced. A '1' in the Single column represents that the person is single, a '0' in the Married column represents that the person is not married, etc.\n- I added an extra column to the testing data. This is because there was no 'default_yes' value in the original default column. Because of this, I make default_yes a column of zeros","f2bf045b":"### Creating the tree\n- I created the tree with a maximum depth of 5. This is because with more than 20 features (columns), our tree would otherwise grow very large. Having a large tree not only slows down the algorithm and becomes confusing, but it can cause overfitting","a382cb0c":"# A Solution to Bank Telemarketing Predictions\n\n## Introduction\n\nThis notebook was completed as a response to\n<a href=\"https:\/\/www.kaggle.com\/c\/predicting-bank-telemarketing\/overview\">this<\/a> Kaggle competition\n\n### Goal\n\nThe focus of this Kaggle competition is to target clients through telemarketing to sell long-term deposits. The data, which was collected from 2008 to 2013, contains demographic and personal information about each client. Our goal is to correctly guess whether or not a client will buy long-term deposits given this information \n\n### Data\n\n#### Necessary Import Statements","a85e4f1a":"#### Probbilistic Distributions\n\nFrom the head of the training file, we can see that none of the first 5 clients purchased deposits. Here, I will look at the percentage of clients who declined the offer, and the percentage that accepted.\n- **Failure (0) :** 88.76%\n- **Success (1) :** 11.24%\n\nClearly, not many people liked the offer. We can use this information to infer that most of the time, a client is most likely to say no.","7c202a8a":"### Most \"Important\" Features\n- We will sort the features based off of their **gain**, or the weight given to each feature\n- Features with higher gain more heavily impact the mode\n- We can see that these features are similar to, but not exactly the same as, the original decision tree","03c14333":"## Decision Tree with Bagging Classifier\n- Now that we've created our decision tree, we can run a bagging classifier on the tree\n- The classifier will run on a model n_estimators times. Each time the classifier runs, it selects a percentage of the original data points, with replacement. All of these attempts are then averaged together\n- For more information about bagging classifiers, look here: \n- Our classifier improved our model from **90.51%** to **90.64%**","b63b0aa7":"### Testing the Tree\n- Below we can see how well our model ran on the training and testing data we partitioned\n- About 90% isn't bad!","484c09f3":"## Exporting Data\n- To export the data, we're running the bagging model on our ORIGINAL testing set. Then, we're saving the results to a csv file","31aa28e0":"## Decision Tree","defffdde":"### Removing NULL Rows\n- Rows with NULL values can screw up our decision tree. Here, will will remove any rows that have NULL values","77f5f662":"- The data was given in the form of training, testing, and a sample sumbission (as an example).\n    - The training data has the column \"duration,\" which indicates the duration of a call with the client. As we will not know the duration of a call **before** calling a client, this data is not included in our testing data. Because of this, we will remove it from the training set\n    - Below, we can see that the training data has a column labeled 'y' with the result. 1 indicates a success (the client purchased the deposit), and 0 indicates a failute (the client did not purchase the deposit)\n    \n#### Reading in the Data","383eb890":"## Random Guess\n\nIf you were to guess randomly using these distributions, this is how you would do it:\n- This attempt yielded me a \"success score\" of around 0.8. This seems pretty good, but it's only because it is very easy to guess a failure. If the failure rate is 0.88%, and I guess \"failure\" 88% of the time, then I would correctly theoretically predict 77.44% of the failures simply by guessing\n\nHere is my code for determining the random predictions:","40e4ab74":"### Most \"Important\" Features\n- We will sort the features based off of their **gain**, or the weight given to each feature\n- Features with higher gain more heavily impact the model"}}