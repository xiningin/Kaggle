{"cell_type":{"307a588f":"code","87d4a6b9":"code","36850219":"code","10d65cb5":"code","81b540d4":"code","875eff56":"code","bb0aab99":"code","3c8ce7c1":"code","a0cfbd21":"code","ccb2d35d":"code","c795855a":"code","24dee18d":"code","757fa665":"code","4ac245cb":"code","bdcee3b3":"code","0bc60536":"code","02e8ee7b":"code","c4c977bb":"code","78313317":"code","b1f3aa19":"code","bd584be5":"code","eccb52d4":"code","ac3223af":"code","fe20d205":"code","46815ebb":"code","e4949520":"code","ef471a08":"code","f4fc3d79":"code","299f8ceb":"code","63e44750":"code","83302a97":"code","dc496589":"code","bae7a85f":"code","e193d28f":"code","20c70e83":"markdown","a9e723b6":"markdown","3661190f":"markdown","01865ad0":"markdown","40e8df32":"markdown","4e0d30e8":"markdown","e5d7437f":"markdown","e15cd41c":"markdown","fa968130":"markdown","59578173":"markdown"},"source":{"307a588f":"import numpy as np \nimport pandas as pd \nimport os\nfrom sklearn.utils import shuffle\nimport string\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","87d4a6b9":"import tensorflow as tf\ntf.__version__ # newest version","36850219":"import matplotlib.pyplot as plt\n\ndef plot_graphs(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])","10d65cb5":"data_path = '..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv'\ndf = pd.read_csv(data_path)","81b540d4":"df.head()","875eff56":"df.drop(df.columns[0],inplace=True, axis=1)\ndf = df[['Title', 'Review Text', 'Division Name', 'Department Name', 'Class Name', 'Recommended IND']]\n\n# see if label has any null values, and it doesn't\ndf['Recommended IND'].isnull().values.any()\n\n# fill the nan in features with ''(empty string)\ndf = df.fillna('')\n\n# concatenate\ndf['Reviews'] = df['Title'] + ' ' + df['Review Text'] + ' ' + df['Division Name'] + ' ' + df['Department Name'] + ' ' + df['Class Name']\n\n# remove the title review text, division name, department name and class name columns\ndf = df[['Reviews', 'Recommended IND']]\n\n# shuffle the data frame\ndf = shuffle(df, random_state=2021)\n\n# remove punctuation\ndf[\"Reviews\"] = df['Reviews'].str.replace('[{}]'.format(string.punctuation), '')\n\n# lower-case everything\ndf['Reviews'] = df['Reviews'].str.lower()","bb0aab99":"df.head()","3c8ce7c1":"# get the entire dataset's unique words and its frequency\ntotal_words = df['Reviews'].str.split()\n\ntotal_words.head()","a0cfbd21":"total_word_set = set()\ntotal_words.apply(total_word_set.update)","ccb2d35d":"# total_word_set","c795855a":"# word frequency distribution\nfrom collections import Counter\n\ncount_dict = Counter(total_word_set)\nVOCAB_SIZE = len(count_dict)","24dee18d":"# vectorize text\n# sequence_length = 100\n\nencoder = TextVectorization(max_tokens = VOCAB_SIZE)","757fa665":"dataset = tf.data.Dataset.from_tensor_slices(\n           ( tf.cast(df['Reviews'].values, tf.string),\n            tf.cast(df['Recommended IND'].values, tf.int32)))","4ac245cb":"dataset.element_spec","bdcee3b3":"# print out an instance in the dataset\nfor example, label in dataset.take(1):\n  print('text: ', example.numpy())\n  print('label: ', label.numpy())","0bc60536":"TRAIN_SIZE = int(len(dataset)*0.7)\n\ntrain_dataset = dataset.take(TRAIN_SIZE)\ntest_dataset = dataset.skip(TRAIN_SIZE) ","02e8ee7b":"# print out a training example and a test example\n\nprint('========== TRAINING EXAMPLE','='*50)\nfor sentence, label in train_dataset.take(1):\n    print('text: ', sentence.numpy())\n    print('label: ', label.numpy())\nprint()    \nprint('========== TEST EXAMPLE', '='*54)\nfor sentence, label in test_dataset.take(1):\n    print('text: ', sentence.numpy())\n    print('label: ', label.numpy())\n    ","c4c977bb":"# tuning the tring and test dataset\n\n    # previous buffer_size hyperparameter BUFFER_SIZE = 10000\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 16\n\ntrain_dataset = train_dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)","78313317":"#since we set the batch_size=16, when we take out 1, which means 1 batch (16 obs\/rows)\n# features, labels\nfor example, label in train_dataset.take(1):\n  print('texts: ', example.numpy()[:3])\n  print()\n  print('labels: ', label.numpy()[:3])","b1f3aa19":"import tensorflow as tf\n\nVOCAB_SIZE = 1000\nencoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n    max_tokens=VOCAB_SIZE)\n# We only need to convert features (NOT label) ot int\nencoder.adapt(train_dataset.map(lambda text, label: text))","bd584be5":"# the first 20 tokens\nnp.array(encoder.get_vocabulary())[:20]","eccb52d4":"embedding_layer = tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),\n                                            output_dim=64,\n                                            mask_zero=True)","ac3223af":"embedding_dim=16\n\nmodel1 = tf.keras.Sequential([\n    encoder,\n    embedding_layer,\n    GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n])","fe20d205":"tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")","46815ebb":"model1.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","e4949520":"model1.fit(\n    train_dataset,\n    validation_data=test_dataset, \n    epochs=15,\n    callbacks=[tensorboard_callback])","ef471a08":"test_loss, test_acc = model1.evaluate(test_dataset)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))\n\n# The accuracy of this model is only 18%, which is not great at all. ","f4fc3d79":"model2 = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=64,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n])","299f8ceb":"model2.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])","63e44750":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='accuracy', mode='max', patience=3)","83302a97":"history2 = model2.fit(train_dataset, epochs=10,\n                    validation_data=test_dataset, \n                    validation_steps=30,\n                    callbacks = [early_stop])","dc496589":"test_loss2, test_acc2 = model2.evaluate(test_dataset)\n\nprint(f'Test Loss: {round(test_loss2,2)}')\nprint(f'Test Accuracy: {round(test_acc2,4)*100}%')","bae7a85f":"plt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplot_graphs(history2, 'accuracy')\nplt.ylim(None,1)\nplt.subplot(1,2,2)\nplot_graphs(history2, 'loss')\nplt.ylim(0,None)","e193d28f":"def make_prediction(text):\n    prediction = model2.predict(np.array([text]))\n    if prediction >= 0.0:\n        return 'a positive review.'\n    else:\n        return 'a negative review.'\n\n\n# run a few validation predictions:\nreview_1 = ('The shirt is cool. The print on the shirt '\n               'is so much fun. I would recommend this product.') #true label = 1, positive review\n\nreview_2 = ('The pattern is hedious, and the fit is weird. '\n                 'I would not recommend this to anyone.') #true label = 0, negative review\n \nreview_3 = ('So happy! I order size M because I have my belly and it works perfect, '\n                 'the waist is wide helping to control.') # true label = 1, positive review\n\nreview_4 = ('True to size and comfy! The inner lining is soft and dry-fit while the outside is a bit more like a windbreaker material. '\n                 'I like that they are super lightweight and not so thin that you can see your underwear') # true label = 1, positive review\n\ntext_list = [review_1, review_2, review_3, review_4]\n\n\n\ncounter = 0\nfor review in text_list:\n    counter += 1\n    print(f'Review {counter}: {make_prediction(review)}')\n    print()\n    \n# model2 got every review correct. ","20c70e83":"## Convert the df(data frame) to a tf dataset","a9e723b6":"### Run a test with some new reviews on model2\n\n- if the prediction is >= 0.0, it is positive reveiw, otherwise, negative","3661190f":"## Create, train and complie the base model","01865ad0":"# Build a baseline model of Word embeddings to forecast the Recommended IND based on Reviews using deep learning.","40e8df32":"## Create the text encoder\n\nThe raw text needs to be processed before it can be used in the model. I use the ***experimental.preprocessing.TextVectorization*** layer. ","4e0d30e8":"## Create embedding layer","e5d7437f":"# Build a second model of RNN using a bidirectional LSTM to forecast the Recommended IND based on Reviews","e15cd41c":"# Concatenate the Title, Review Text, Division Name, Department Name and Class Name as a new feature of Reviews.","fa968130":"## Train Test Split","59578173":"# Load the data, Womens Clothing E-Commerce Reviews.csv, into memory."}}