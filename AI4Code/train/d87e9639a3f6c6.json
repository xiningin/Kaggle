{"cell_type":{"051933a4":"code","e5808987":"code","515ba49d":"code","f8867f6a":"code","8a5ba813":"code","195cdd98":"code","ab01f83f":"code","7a69cb60":"code","5e169979":"code","d076bbec":"code","19b17ad4":"code","82c91188":"code","80102a44":"code","76b3ae3f":"code","308cd1ea":"markdown","fa320ee4":"markdown","c8a16c11":"markdown","2619dded":"markdown","0eadd589":"markdown"},"source":{"051933a4":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix","e5808987":"class Logistic_Regression():\n\n\n  # declaring learning rate & number of iterations (Hyperparametes)\n    def __init__(self, learning_rate, no_of_iterations):\n\n        self.learning_rate = learning_rate\n        self.no_of_iterations = no_of_iterations\n\n\n\n      # fit function to train the model with dataset\n    def fit(self, X, Y):\n\n        # number of data points in the dataset (number of rows)  -->  m\n        # number of input features in the dataset (number of columns)  --> n\n        self.m, self.n = X.shape\n\n\n        #initiating weight & bias value\n\n        self.w = np.zeros(self.n)\n\n        self.b = 0\n\n        self.X = X\n\n        self.Y = Y\n\n\n        # implementing Gradient Descent for Optimization\n\n        for i in range(self.no_of_iterations):\n              self.update_weights()\n\n\n\n    def update_weights(self):\n\n        # Y_hat formula (sigmoid function)\n\n        Y_hat = 1 \/ (1 + np.exp( - (self.X.dot(self.w) + self.b ) ))    \n\n\n        # derivaties\n\n        dw = (1\/self.m)*np.dot(self.X.T, (Y_hat - self.Y))\n\n        db = (1\/self.m)*np.sum(Y_hat - self.Y)\n\n\n        # updating the weights & bias using gradient descent\n\n        self.w = self.w - self.learning_rate * dw\n\n        self.b = self.b - self.learning_rate * db\n\n\n      # Sigmoid Equation & Decision Boundary\n\n    def predict(self, X):\n\n        Y_pred = 1 \/ (1 + np.exp( - (X.dot(self.w) + self.b ) )) \n        Y_pred = np.where( Y_pred > 0.5, 1, 0)\n        return Y_pred","515ba49d":"df = pd.read_csv(\"..\/input\/diabetes-data-set\/diabetes.csv\")","f8867f6a":"df.head()","8a5ba813":"df.isna().sum()","195cdd98":"X = df.drop([\"Outcome\"], axis = \"columns\")\ny = df[\"Outcome\"]","ab01f83f":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)","7a69cb60":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 2)","5e169979":"model = Logistic_Regression(learning_rate = 0.01, no_of_iterations = 1000)","d076bbec":"model.fit(X_train, y_train)","19b17ad4":"y_preds = model.predict(X_test)","82c91188":"y_test","80102a44":"y_preds.shape","76b3ae3f":"accuracy_score(y_test,y_preds)","308cd1ea":"![image.png](attachment:907c48e5-8a25-4e62-a704-ce7c09c9b06d.png)","fa320ee4":"**Learning Rate:**\n\nLearning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.","c8a16c11":"Gradient Descent:\n\nGradient Descent is an optimization algorithm used for minimizing the loss function in various machine learning algorithms. It is used for updating the parameters of the learning model.\n\nw = w - \u03b1*dw\n\nb = b - \u03b1*db","2619dded":"Y_hat --> predicted value\n\nX --> Input Variable\n\nw --> weight\n\nb --> bias","0eadd589":"![image.png](attachment:8830c259-5b52-4d74-b582-a914e86de4b6.png)"}}