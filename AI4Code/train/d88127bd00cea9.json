{"cell_type":{"ab4848e3":"code","588a5ef0":"code","ba521c40":"code","7dbf8280":"code","3df4de4e":"code","c42e8396":"code","a6f437ec":"code","65578c60":"code","ab71c58d":"code","468310be":"code","7a71f194":"code","13d8eb9c":"code","66ece0eb":"code","9a8407c5":"code","ba084afb":"code","f4b168b2":"code","0c7c7352":"code","6087f043":"code","4259ce2d":"code","a050c29b":"code","69b83c66":"code","b9ac1c4d":"code","41c27e5b":"code","43ccd237":"code","4939022a":"code","ae54893a":"code","d153bbfb":"code","ef6ef738":"code","7a3c6bd9":"code","759ade55":"code","36d9e85f":"code","bd489db3":"code","b7e90ac4":"code","920c4c90":"code","133cd011":"code","e4d104c8":"code","812cf04c":"code","dff7b0e5":"code","1cdb4c94":"code","60707bd7":"code","755bfc4f":"code","85c5b148":"code","34657a77":"code","24f27df3":"code","086b4cee":"code","b7d8edad":"code","d0fcf0a5":"code","073d7ebf":"code","07b52446":"code","34f2d6f9":"code","3cd98021":"code","f843b641":"code","cb7a4a29":"code","4105c7ec":"code","2af0e248":"code","834f5beb":"code","0ed5f79c":"code","08923208":"code","06b68ecf":"code","26e17b6d":"code","fe664c91":"code","97bcdef6":"code","1021d353":"code","311e9e55":"code","0f1a33a9":"code","3355a695":"code","2a72c4dd":"code","ef61e00d":"code","7a6babb8":"code","e97459bf":"code","7340d1f1":"code","f3b63866":"code","cf84910b":"code","b770c732":"code","0cd36ca6":"code","80a46028":"code","d49747a0":"code","f859999f":"code","88557d72":"code","c04f8986":"code","88ef672d":"code","b75b79e7":"code","b62339b9":"code","5286f69f":"code","f28055e2":"code","b22941ca":"code","cb5e7102":"code","e54c9357":"code","a2a95d3e":"code","04b0856b":"code","af817cf7":"code","6396ff4b":"code","09353071":"code","937dc891":"code","56e9affe":"markdown","40aeaa85":"markdown","48dd3fb3":"markdown","223db20f":"markdown","b6bd96f6":"markdown","a6e70f5d":"markdown","8f152df1":"markdown","ac012171":"markdown","9da0736e":"markdown","8b0ec671":"markdown","0949b616":"markdown","f5ecf5e3":"markdown","614098a4":"markdown"},"source":{"ab4848e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","588a5ef0":"traindf = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n","ba521c40":"traindf.head()","7dbf8280":"#Let's see the shape of the dataset\ntraindf.shape","3df4de4e":"traindf.isnull().sum()","c42e8396":"def display_null_proportion_by_field(traindf):\n    \n    nullval = traindf.isnull().sum()\n    percentval = traindf.isnull().sum()\/traindf.shape[0] * 100\n    nulldf = pd.concat([nullval,percentval],axis = 1)\n    return nulldf","a6f437ec":"null_data_prop = display_null_proportion_by_field(traindf)","65578c60":"null_data_prop","ab71c58d":"#Lets drop them based on percentage of null values criteria\nfor index,per in zip(null_data_prop.index,null_data_prop[1]):\n    if(per >= 40.0):\n        traindf.drop(index,axis = 1,inplace = True)\n    ","468310be":"traindf.head()","7a71f194":"traindf.info()","13d8eb9c":"traindf.shape","66ece0eb":"traindf['Street'].value_counts(normalize = True) * 100","9a8407c5":"traindf['Condition2'].value_counts(normalize = True) * 100","ba084afb":"traindf['Heating'].value_counts(normalize = True) * 100","f4b168b2":"traindf['Utilities'].value_counts(normalize = True) * 100","0c7c7352":"traindf['RoofMatl'].value_counts(normalize = True) * 100","6087f043":"traindf['SaleCondition'].value_counts(normalize = True) * 100","4259ce2d":"#Lets drop them\ntraindf.drop(['Street','Utilities','LandSlope','Condition2','RoofMatl','Heating','GarageCond','GarageQual'],axis = 1,inplace = True)","a050c29b":"traindf.shape","69b83c66":"#['LotFrontage','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Electrical','GarageType','GarageYrBlt','GarageFinish']","b9ac1c4d":"traindf[['LotFrontage','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Electrical','GarageType','GarageYrBlt','GarageFinish']].info()","41c27e5b":"#I will be using sklearn's SimpleImputer\ntraindf['GarageFinish'].isnull().values.any()","43ccd237":"from sklearn.impute import SimpleImputer\n","4939022a":"traindf['BsmtQual'].dtype == 'object'","ae54893a":"for i in traindf.columns:\n    if(traindf[i].isnull().values.any()):\n        \n        if((traindf[i].dtype == 'float64') or (traindf[i].dtype == 'int64')):\n            floatimputer = SimpleImputer(missing_values = np.nan,strategy = 'mean')\n            traindf[i] = floatimputer.fit_transform(traindf[[i]])\n        \n        elif(traindf[i].dtype == 'object'):\n            objectimputer = SimpleImputer(missing_values = np.nan,strategy = 'most_frequent')\n            traindf[i] = objectimputer.fit_transform(traindf[[i]])\n            \n        else:\n            print('Unsupported Type')\n            \n    else:\n        continue","d153bbfb":"traindf.isnull().sum()\n    \n    ","ef6ef738":"traindf.columns","7a3c6bd9":"traindf[['YearBuilt','YearRemodAdd']].info()","759ade55":"#Other columns that we could remove\ntraindf.drop(['Id','YearRemodAdd','GarageYrBlt'],axis = 1,inplace = True)","36d9e85f":"traindf.shape","bd489db3":"testdf = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","b7e90ac4":"testdf.shape","920c4c90":"set(testdf.columns) - set(traindf.columns)","133cd011":"testdf.drop(['Alley','Condition2','Fence','GarageCond','GarageQual','GarageYrBlt','Heating','Id','LandSlope','MiscFeature','PoolQC','RoofMatl','Street','Utilities',\n 'YearRemodAdd'],axis = 1,inplace = True)","e4d104c8":"testdf.drop('FireplaceQu',axis = 1,inplace = True)","812cf04c":"testdf.shape","dff7b0e5":"pd.DataFrame([traindf.columns,testdf.columns])","1cdb4c94":"testdf.isnull().sum()","60707bd7":"def Imputer(df):\n    for i in df.columns:\n        if(df[i].isnull().values.any()):\n        \n            if((df[i].dtype == 'float64') or (df[i].dtype == 'int64')):\n                floatimputer = SimpleImputer(missing_values = np.nan,strategy = 'mean')\n                df[i] = floatimputer.fit_transform(df[[i]])\n        \n            elif(df[i].dtype == 'object'):\n                objectimputer = SimpleImputer(missing_values = np.nan,strategy = 'most_frequent')\n                df[i] = objectimputer.fit_transform(df[[i]])\n            \n            else:\n                print('Unsupported Type')\n            \n        else:\n            continue\n    ","755bfc4f":"Imputer(testdf)","85c5b148":"testdf.shape","34657a77":"testdf.isnull().values.any()","24f27df3":"import seaborn as sns\nimport matplotlib.pyplot as plt\n","086b4cee":"figure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data = traindf,x = 'OverallQual',y='SalePrice',ax = ax[0])\nsns.violinplot(data = traindf,x = 'OverallQual',y='SalePrice',ax = ax[1])\nsns.boxplot(data = traindf,x = 'OverallQual',y='SalePrice',ax = ax[2])","b7d8edad":"plt.figure(figsize = (10,5))\nsns.barplot(x='YrSold', y=\"SalePrice\", data = traindf, estimator = np.median)\nplt.title('Median of Sale Price by Year', fontsize = 13)\nplt.xlabel('Selling Year', fontsize = 12)\nplt.ylabel('Median of Price in $', fontsize = 12)\nplt.show()","d0fcf0a5":"traindf.shape","073d7ebf":"testdf.shape","07b52446":"del traintemp\ndel testtemp","34f2d6f9":"traindf.select_dtypes('object').shape","3cd98021":"testdf.select_dtypes('object').shape","f843b641":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\nonehot = OneHotEncoder()\n\nencoderesults = onehot.fit_transform(traindf.select_dtypes('object'))\ntrnewdf = pd.DataFrame(encoderesults.toarray(),columns = onehot.get_feature_names())\ntraindf = traindf.join(trnewdf)\n\ntestencoderesults = onehot.transform(testdf.select_dtypes('object'))\ntenewdf = pd.DataFrame(testencoderesults.toarray(),columns = onehot.get_feature_names())\ntestdf = testdf.join(tenewdf)\n        \n    \n        \n\n    #if('SalePrice' in df.columns):\n    #    print('True')\n    #    numcol = list(df.select_dtypes(['int64','float64']).drop('SalePrice',axis = 1).columns)\n    #else:\n    #    print('False')\n    #    numcol = list(df.select_dtypes(['int64','float64']).columns)\n    #scaler = StandardScaler()\n    \n    #for i in numcol:\n    #    df[i] = scaler.fit_transform(df[[i]])\n    \n\n    ","cb7a4a29":"removecols = traindf.select_dtypes('object').columns.tolist()","4105c7ec":"traindf.drop(removecols,axis = 1,inplace = True)\ntestdf.drop(removecols,axis = 1,inplace = True)","2af0e248":"traindf.head()","834f5beb":"traindf.shape","0ed5f79c":"testdf.head()","08923208":"testdf.shape","06b68ecf":"def normalize(df):\n    \n    if('SalePrice' in df.columns):\n        print('True')\n        numcol = list(df.select_dtypes(['int64','float64']).drop('SalePrice',axis = 1).columns)\n    else:\n        print('False')\n        numcol = list(df.select_dtypes(['int64','float64']).columns)\n    \n    scaler = StandardScaler()\n    \n    for i in numcol:\n        df[i] = scaler.fit_transform(df[[i]])\n        \n    return df\n    ","26e17b6d":"traindf = normalize(traindf)\ntestdf = normalize(testdf)","fe664c91":"traindf.head()","97bcdef6":"testdf.head()","1021d353":"traindf.shape,testdf.shape","311e9e55":"import statsmodels.api as sm\nimport scipy.stats as stats\nfrom scipy.stats import norm","0f1a33a9":"target = traindf['SalePrice']","3355a695":"# SalePrice before transformation\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\"qq-plot & distribution SalePrice\", fontsize= 15)\n\nsm.qqplot(target, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\n\nsns.distplot(target, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","2a72c4dd":"#SalePrice after transformation\ntarget_log = np.log1p(target)\n\nfig,ax = plt.subplots(1,2,figsize= (15,5))\nfig.suptitle(\"qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target_log,stats.t,distargs=(4,),fit=True, line=\"45\", ax = ax[0])\nsns.distplot(target_log,kde = True,hist=True,fit = norm, ax= ax[1])\nplt.show()","ef61e00d":"import shap\nimport xgboost as xgb\nfrom catboost import Pool\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error","7a6babb8":"newtest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","e97459bf":"testdf['Id'] = newtest['Id']","7340d1f1":"testdf.head()","f3b63866":"trainX = traindf.drop('SalePrice',axis = 1)\ntrainY = target_log","cf84910b":"from sklearn.model_selection import train_test_split","b770c732":"X_train,X_val,y_train,y_val = train_test_split(trainX,trainY,test_size = 0.1,random_state=42)","0cd36ca6":"baseline_models = ['LinearRegression','BayesianRidge','LGBMRegressor','SVR',\n                   'DecisionTreeRegressor','RandomForestRegressor', 'GradientBoostingRegressor',\n                   'CatBoostRegressor']","80a46028":"from sklearn.metrics import mean_squared_error","d49747a0":"rmsevalues = []","f859999f":"def fit_and_predict(model,X_train,X_val,y_train,y_val):\n    modelfit = model.fit(X_train,y_train)\n    y_predict = modelfit.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, y_predict))\n    print(rmse)\n    rmsevalues.append(rmse)\n    ","88557d72":"lr = LinearRegression()\nbrr = BayesianRidge(compute_score=True)\nlgbm = LGBMRegressor(objective='regression')\nsvr = SVR()\ndtr = DecisionTreeRegressor()\nrfr = RandomForestRegressor()\ngbr = GradientBoostingRegressor()\ncatb = CatBoostRegressor()\n\nmodellist = [lr,brr,lgbm,svr,dtr,rfr,gbr,catb]\n\nfor model in modellist:\n    fit_and_predict(model,X_train,X_val,y_train,y_val)\n    \n\n\n","c04f8986":"finalscore = pd.DataFrame(baseline_models,columns = ['Regressors'])","88ef672d":"finalscore['RMSE'] = rmsevalues","b75b79e7":"finalscore","b62339b9":"cat = CatBoostRegressor()\ncat_model = cat.fit(X_train,y_train,eval_set = (X_val,y_val),plot=True,verbose = 0)\ncat_pred = cat_model.predict(X_val)\ncat_score = np.sqrt(mean_squared_error(y_val, cat_pred))\ncat_score\n","5286f69f":"feat_imp = cat_model.get_feature_importance(prettified=True)\nfeat_imp","f28055e2":"sum(feat_imp['Importances'])","b22941ca":"# Plotting top 20 features' importance\n\nplt.figure(figsize = (12,8))\nsns.barplot(feat_imp['Importances'][:20],feat_imp['Feature Id'][:20], orient = 'h')\nplt.show()","cb5e7102":"testdf.head()","e54c9357":"test_id = testdf['Id']","a2a95d3e":"test_pred = cat.predict(testdf.drop('Id',axis = 1))\nsubmission = pd.DataFrame(test_id, columns = ['Id'])\ntest_pred = np.expm1(test_pred)\nsubmission['SalePrice'] = test_pred \nsubmission.head()","04b0856b":"sub = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","af817cf7":"sub.head()","6396ff4b":"filname = 'submissions.csv'\nsubmission.to_csv(filname,index=False)\nprint('Saved file: ' + filename)","09353071":"sub1 = pd.read_csv('submissions.csv')","937dc891":"sub1","56e9affe":"#### Looks like there are several null values. Let's find out !","40aeaa85":"### Yipeee!!!! No more null data.","48dd3fb3":"### EDA","223db20f":"### Model building","b6bd96f6":"### CatBoostRegressor is clear winner. Let's use it on our test dataset","a6e70f5d":"### Encoding and Normalization (traindf)","8f152df1":"### Feature Importance","ac012171":"### Preprocessing test set","9da0736e":"### We can drop ['Street','Utilities','LandSlope','Condition2','RoofMatl','Heating','GarageCond','GarageQual'] features too as the variability of the feature is almost the same. I mean it has one category 'Pave' appearing almost all the time.Similarly with other features have a category dominating(>99%). See below code that justifies the same. Hence not a good predictor for our house prices. So we can drop them.","8b0ec671":"### The columns which have most of the nulls could be removed. The columns are ['Alley','MasVnrType','MasVnrArea','FireplaceQu','PoolQC','Fence','MiscFeature']","0949b616":"### There are 16 columns which we need to drop from test set as those features were dropped in train set","f5ecf5e3":"## It's time to Submit!!","614098a4":"### We still have features with null values. Since we just have 1460 records we cannot afford removing the rows for null records. Hence let's impute the features based on their data type. Let's Go!!"}}