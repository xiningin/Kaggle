{"cell_type":{"609d6582":"code","d8ebb468":"code","13250549":"code","e44047f9":"code","e95c992a":"code","5ec644f3":"code","7aeef7c1":"code","ab3c01a6":"code","032ddb42":"code","6fe72fd2":"code","c6b706fc":"code","9c79a6c8":"code","98d15f66":"code","9c4bfaa8":"code","6cd9f697":"code","a82079a0":"code","358d07f1":"code","617cb31f":"code","47a8dcfb":"code","c93740e9":"code","3c827b10":"code","452338ad":"code","6d266764":"code","abc503f2":"code","9ce24a2c":"code","a732b725":"code","898510e4":"code","a22bca86":"code","006938ec":"code","09c46f1f":"code","82944381":"code","af353837":"code","3035c60f":"code","1e1058a3":"markdown","d18d6194":"markdown","d7f8f719":"markdown","203e4609":"markdown","63ae60e3":"markdown","e6c92785":"markdown","bf1983b7":"markdown","83021f8b":"markdown","360e0fe4":"markdown","bf95ab10":"markdown","192e5515":"markdown","bf858cce":"markdown","a1bb17a1":"markdown","998f9c16":"markdown","32b0600c":"markdown","7c0c3ce6":"markdown","8ce68b77":"markdown","49c83df0":"markdown","c9c6e56d":"markdown","a3c38888":"markdown","add79fee":"markdown","d6472bf0":"markdown","2c2424a7":"markdown","4d5183d3":"markdown","18ac639a":"markdown","ce05d086":"markdown"},"source":{"609d6582":"import pandas as pd\nimport plotly.express as px\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset_path = \"..\/input\/real-time-advertisers-auction\/Dataset.csv\"\nascendeum_data = pd.read_csv(dataset_path)\nascendeum_data.head()","d8ebb468":"X_orig = ascendeum_data.copy()\nascendeum_data.describe()","13250549":"ascendeum_data.isnull().sum()","e44047f9":"ascendeum_data.nunique()","e95c992a":"ascendeum_data.info()","5ec644f3":"def CPM(revenue, impressions):\n    return revenue \/ impressions if impressions else 0\n\nascendeum_data['CPM'] = ascendeum_data.apply(lambda x: CPM(((x['total_revenue']*100)),x['measurable_impressions'])*1000 , axis=1)\nascendeum_data['CPM'].describe()","7aeef7c1":"corr = ascendeum_data.corr()\nplt.figure(figsize=(18,9))\nsns.heatmap(data=corr,vmin=0, vmax=1, cmap=\"RdYlGn\",square=True, annot=True)\nplt.show()","ab3c01a6":"ascendeum_data= ascendeum_data.drop(['integration_type_id', 'revenue_share_percent', \n                                     'measurable_impressions', 'total_revenue'], axis = 1)\nascendeum_data.info()","032ddb42":"corr = ascendeum_data.corr()\nplt.figure(figsize=(14,8))\nsns.heatmap(data=corr,vmin=0, vmax=1, cmap=\"RdYlGn\",square=True, annot=True)\nplt.show()","6fe72fd2":"sns.distplot(ascendeum_data[\"CPM\"])","c6b706fc":"ascendeum_data = ascendeum_data[ascendeum_data['CPM'].between(ascendeum_data['CPM'].quantile(.05), ascendeum_data['CPM'].quantile(.95))]\nsns.boxplot(ascendeum_data[\"CPM\"],color=\"green\")","9c79a6c8":"sns.distplot(ascendeum_data[\"CPM\"])","98d15f66":"ascendeum_data.shape","9c4bfaa8":"ascendeum_data['date'] =  pd.to_datetime(ascendeum_data['date'])\nascendeum_data['weekday'] = ascendeum_data['date'].dt.dayofweek","6cd9f697":"y = ascendeum_data.CPM\nX = ascendeum_data.drop(['CPM', 'date'], axis = 1)","a82079a0":"from sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)","358d07f1":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nDTR_model = DecisionTreeRegressor( max_leaf_nodes =1000, random_state=0)\n\nDTR_model.fit(train_X, train_y)\n\nval_predictions = DTR_model.predict(val_X)\nprint(\"MAE:\", mean_absolute_error(val_y, val_predictions))\nprint(\"MSE:\", mean_squared_error(val_y, val_predictions))","617cb31f":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    mse = mean_squared_error(val_y, preds_val)\n    return(mae, mse)\n\nfor max_leaf_nodes in [5, 50, 500, 1000, 2000, 5000]:\n    my_mae, my_mse = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d \\t\\t MAE: %d \\t\\t MSE: %d\" %(max_leaf_nodes, my_mae, my_mse))","47a8dcfb":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(train_X, train_y)\nfor_preds = forest_model.predict(val_X)\n\nprint(\"MAE:\", mean_absolute_error(val_y, for_preds))\nprint(\"MSE:\", mean_squared_error(val_y, for_preds))","c93740e9":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    mse = mean_squared_error(val_y, preds_val)\n    return(mae, mse)\n\nfor max_leaf_nodes in [5, 50, 500, 1000, 2000, 5000]:\n    my_mae, my_mse = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d \\t\\t MAE: %d \\t\\t MSE: %d\" %(max_leaf_nodes, my_mae, my_mse))","3c827b10":"import xgboost as xgb\nfrom xgboost import plot_importance\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor as cbr\n\nmodel_lgb = lgb.LGBMRegressor(num_leaves=41, n_estimators=200)\nmodel_lgb.fit(train_X, train_y)\nlgb_preds = model_lgb.predict(val_X)\n\nprint(\"MAE:\", mean_absolute_error(val_y, lgb_preds))\nprint(\"MSE:\", mean_squared_error(val_y, lgb_preds))","452338ad":"model_xgb = xgb.XGBRegressor(objective='reg:squarederror')\nmodel_xgb.fit(train_X, train_y)\nxgb_preds = model_xgb.predict(val_X)\n\nprint(\"MAE:\", mean_absolute_error(val_y, xgb_preds))\nprint(\"MSE:\", mean_squared_error(val_y, xgb_preds))","6d266764":"model_cbr = cbr(random_seed=242, verbose=0, early_stopping_rounds=10)\nmodel_cbr.fit(train_X, train_y)\ncbr_preds = model_cbr.predict(val_X)\n\nprint(\"MAE:\", mean_absolute_error(val_y, cbr_preds))\nprint(\"MSE:\", mean_squared_error(val_y, cbr_preds))","abc503f2":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nnumerical_cols = ['geo_id', 'order_id', 'ad_unit_id', 'total_impressions', 'viewable_impressions']\ncategorical_cols = ['site_id', 'ad_type_id', 'device_category_id', 'advertiser_id', 'line_item_type_id', \n                    'os_id','monetization_channel_id', 'weekday']\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols),\n                                               ('cat', categorical_transformer, categorical_cols)])\n\nmodel = cbr(random_seed=242, verbose=0, early_stopping_rounds=10)\n\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\nmy_pipeline.fit(train_X, train_y)\npreds = my_pipeline.predict(val_X)\n\n# Evaluate the model\nprint('MAE:', mean_absolute_error(val_y, preds))\nprint('MSE:', mean_squared_error(val_y, preds))","9ce24a2c":"from sklearn.model_selection import cross_val_score\n\nmy_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n                              ('model', cbr(random_seed=242, verbose=0, early_stopping_rounds=10))])\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X, y, cv=5, scoring='neg_mean_absolute_error')\n\nprint(\"MAE scores:\", scores)\n\nprint(\"Average MAE score (across experiments):\", scores.mean())","a732b725":"from sklearn.model_selection import cross_val_score\n\nmy_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n                              ('model', cbr(random_seed=242, verbose=0, early_stopping_rounds=10))])\n\n# Multiply by -1 since sklearn calculates *negative* MSE\nscores = -1 * cross_val_score(my_pipeline, X, y, cv=5, scoring='neg_mean_squared_error')\n\nprint(\"MSE scores:\", scores)\n\nprint(\"Average MSE score (across experiments):\", scores.mean())","898510e4":"from sklearn.ensemble import VotingRegressor\nfrom sklearn import model_selection\nfrom sklearn.metrics import confusion_matrix\n\nestimators = []\n\nmodel1 = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\nestimators.append(('logistic1', model1))\nmodel2 = RandomForestRegressor(random_state=1)\nestimators.append(('logistic2', model2))\nmodel3 = lgb.LGBMRegressor(num_leaves=41, n_estimators=200)\nestimators.append(('logistic3', model3))\nmodel4 = xgb.XGBRegressor(objective='reg:squarederror')\nestimators.append(('logistic4', model4))\nmodel5 = cbr(random_seed=242, verbose=0, early_stopping_rounds=10)\nestimators.append(('logistic5', model5))\n\n# Defining the ensemble model\nensemble = VotingRegressor(estimators)\nensemble.fit(train_X, train_y)\ny_pred = ensemble.predict(val_X)\n\n# Evaluate the model\nprint('MAE:', mean_absolute_error(val_y, y_pred))\nprint('MSE:', mean_squared_error(val_y, y_pred))","a22bca86":"boost_df= pd.DataFrame({})\n\nboost_df['Actual_CPM']= val_y\n\nboost_df['Pred_LGB_CPM']= lgb_preds\nboost_df['Pred_XGB_CPM']= xgb_preds\nboost_df['Pred_CBR_CPM']= cbr_preds\nboost_df['Pred_DTR_CPM']= val_predictions\nboost_df['Pred_RFR_CPM']= for_preds\nboost_df['Pred_Voting_CPM'] = y_pred\nboost_df.sample(n=10)","006938ec":"boost_df.describe()","09c46f1f":"revenue_df = pd.DataFrame({'Actual_Impressions': val_X['total_impressions'].values,  'Actual_CPM': val_y, \n                           'Pred_Voting_CPM': boost_df['Pred_Voting_CPM'].values})\n\nrevenue_df['Pred_Revenue'] = revenue_df['Pred_Voting_CPM'] * revenue_df['Actual_Impressions'] \/ (1000 * 100)\nrevenue_df['Pred_Revenue'] = revenue_df['Pred_Revenue'].clip(lower=0)\nrevenue_df.sample(n=10)","82944381":"revenue_df.describe()","af353837":"print('Average revenue of june month:', np.round(X_orig[\"total_revenue\"].mean(),2))\nprint('Predicted approximate revenue for july month:', np.round(revenue_df[\"Pred_Revenue\"].mean(),2))","3035c60f":"print('Reserve price of june month:', np.round(boost_df[\"Actual_CPM\"].max(),2))\nprint('Predicted approximate Reserve price for july month:',np.round(boost_df[\"Pred_Voting_CPM\"].max(),2))","1e1058a3":"#### Defining Hybrid Ensemble Learning Model to increase prediction efficiency","d18d6194":"# 1. Loading Data","d7f8f719":"----------------------------------------------------------------------------------------------------------------\n### Inferences from correlation analysis:\n     1. 'integration_type_id' and 'revenue_share_percent' can be dropped as they have constant values through out\n     2. 'measurable_impressions' and 'total_revenue' can be dropped as they are highly correlated with 'total_impressions'","203e4609":"### Finding Correlation (HeatMap)","63ae60e3":"### 4.2 Random Forest Regressor","e6c92785":"### 4.5 Cat Boost Regressor","bf1983b7":"----------------------------------------------------------------------------------------------------------------\n### Summary:\nMSE of 5 Regressor models evaluated:\n    1. Decision Tree Regressor : 2537.40\n    2. Random Forest Regressor : 2355.67\n    3. LGBM Regressor          : 2362.56\n    4. XGB Regressor           : 2373.63\n    5. Cat Boost Regressor     : 2347.23\n    \n#### Cat Boost Regressor is considered for further investigation as it is having the lowest MSE.","83021f8b":"### Building ML Pipeline\n\nCreating pipeline. \nSplitting categorical and numerical columns.\nPerforming One Hot encoder on categorical columns.\nModel evaluation.","360e0fe4":"Created new column \"dayofweek\" from date to include the effect of date.","bf95ab10":"# 3. Feature Engineering","192e5515":"----------------------------------------------------------------------------------------------------------------\n### Solution 1: Approximately our publisher in July can make revenue in the range of 0.05 to 0.07.\n----------------------------------------------------------------------------------------------------------------","bf858cce":"----------------------------------------------------------------------------------------------------------------\n### Handling Outliers","a1bb17a1":"# 2. Analyse Data","998f9c16":"### 4.4 XGB Regressor","32b0600c":"#### Iterating through various \"Max leaf nodes\" values for lowest MSE and MAE.","7c0c3ce6":"### Cross-validation to validate the model for over-fitting","8ce68b77":"----------------------------------------------------------------------------------------------------------------\n### Solution 2: Predicted reserve prices one can set in the range of 522.15 to 526.92.\n----------------------------------------------------------------------------------------------------------------","49c83df0":"# 6. Questions\n\n## 1. What is the potential revenue range our publisher can make in July?","c9c6e56d":"Remove the extremes\/outliers from CPM. 95% of the data is within 2 standard deviations.","a3c38888":"# 5. Predictions and Evaluation","add79fee":"### 4.1 Decision Tree Regressor","d6472bf0":"## 2. What is the reserve prices that he\/she can set ?","2c2424a7":"### 4.3 LGBM Regressor","4d5183d3":"# 4. Modelling\n\n### Split Dataset (Train, Validation)","18ac639a":"### CPM calculation and injesting to the dataset\n\nCPM \u2013 cost per Mille. It is Calculated as revenue\/impressions * 1000. 'bids' and 'price' are measured in terms of CPM.","ce05d086":"----------------------------------------------------------------------------------------------------------------\n### Inferences from analysing input data:\n     1. There are 17 columns - 1 date time column, 15 Integer columns and 1 Float column\n     2. Nearly 8 coulmns are having less than 10 unique items, so they can be considered as categorical columns\n     3. There are no missign values in any of the given columns\n     4. On average, there is a 0.069 Revenue generate for 33.67 Impressions\n     5. Target, which is CPM need to calculated"}}