{"cell_type":{"4b0c9bd1":"code","b0049bad":"code","fa0ea3d4":"code","0e1d72c3":"code","0a3d26ad":"code","1a6ff8d1":"code","fba175f1":"code","af2f1ed9":"code","5452cd97":"code","4029ea81":"code","a68dfe2e":"code","cd600cd0":"code","a26cdf0b":"code","5f86e67c":"code","cdd7ebd7":"code","8a289418":"code","2d620a29":"code","4baf9c08":"code","eb2cd697":"code","8976ed52":"code","3b77e196":"code","cdcd979c":"code","31c44d0b":"code","fa87b2cc":"code","059bd632":"code","cdfbe796":"code","dd1d34b5":"code","a49e4216":"code","3534ac02":"code","a3d28593":"code","f5e29d52":"code","a7f8920f":"code","cb937275":"code","406a0934":"code","e07f8169":"code","1cd83d55":"code","661bc373":"code","8ed7a79c":"code","e9913059":"code","7cdba725":"code","c0d36fd4":"code","f5330eb7":"code","a7d170a8":"code","9c55996c":"code","c82d72c3":"code","9ebd64bb":"code","61b50a6b":"code","16791d37":"code","10e0aeb1":"code","015c804f":"code","0b325667":"code","439947b7":"code","7cf78375":"code","94677451":"code","699f2870":"code","120c1abb":"code","2eb317d5":"code","b2187b85":"code","c75ed466":"code","1fcb6dfc":"code","feb10b46":"code","7a6e5ae8":"code","fb162b00":"code","416ec445":"code","e32831c4":"code","e2c71819":"code","9798f11c":"code","70707347":"code","61ea38a0":"code","9ed5026b":"code","868e0d61":"code","ceb0ec6b":"code","be107709":"code","afa5c0b1":"code","b94316db":"code","e0fac3b2":"code","4765d849":"code","a2116d23":"code","82425a8b":"code","3584f025":"code","d17fb6fb":"code","b503a310":"markdown","c5282abe":"markdown","ae3a18c5":"markdown","9467c73f":"markdown","81391999":"markdown","e9920878":"markdown","2c756780":"markdown","2b7c9237":"markdown","55f0b324":"markdown","be28e879":"markdown","24a7a5fa":"markdown","7074c4b1":"markdown","65ff33e7":"markdown","a65ee9f0":"markdown","641f5ebd":"markdown","8e714731":"markdown","d3b3b89c":"markdown","538ce675":"markdown","a628d947":"markdown","5b6a0645":"markdown","3394f4ad":"markdown","4860eb1d":"markdown","72323682":"markdown","317dbd6a":"markdown","e81d5a6f":"markdown","7f959787":"markdown","06515f90":"markdown","abd038c7":"markdown","9025563d":"markdown","6da39bde":"markdown","779dae7b":"markdown","e35c0696":"markdown","688b7cca":"markdown","ac51b45c":"markdown","7655bbc2":"markdown","a6e8d022":"markdown","6cce861f":"markdown","04ad6899":"markdown","e91b767d":"markdown","bac0133f":"markdown","4cb91bbe":"markdown","a40b86d2":"markdown","a648c14d":"markdown","af059171":"markdown","0ec3de40":"markdown","fb44ac74":"markdown","eb636577":"markdown","0937e4fb":"markdown","eef086ff":"markdown","a0ec7568":"markdown","bfc8f199":"markdown","125fc544":"markdown","963fd40e":"markdown","d17166a1":"markdown","c18a83dd":"markdown","900ad677":"markdown","5cf2ff79":"markdown","ad94aa2f":"markdown","22691468":"markdown","7a75b9a6":"markdown","a3ee8f02":"markdown","e9556a9e":"markdown","1a26bb4f":"markdown","033b181d":"markdown","218aded6":"markdown","0eecc124":"markdown","5c41c2f7":"markdown","4e82b558":"markdown","40bc0953":"markdown"},"source":{"4b0c9bd1":"import pandas as pd \nimport re \nimport math\nfrom nltk.stem.snowball import SnowballStemmer # k\u1ef9 thu\u1eadt stemmer \u0111\u01b0a v\u1ec1 t\u1eeb g\u1ed1c\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\nimport lxml\nimport re\nimport nltk\nfrom nltk.corpus import stopwords # c\u00e1c stopwords (the, or, and, ...)\nfrom nltk.metrics import edit_distance\nfrom string import punctuation\nfrom collections import Counter\nimport numpy as np ","b0049bad":"#Gi\u1ea3i n\u00e9n t\u1eadp tin\n!unzip ..\/input\/home-depot-product-search-relevance\/attributes.csv.zip\n!unzip ..\/input\/home-depot-product-search-relevance\/product_descriptions.csv.zip\n!unzip ..\/input\/home-depot-product-search-relevance\/test.csv.zip\n!unzip ..\/input\/home-depot-product-search-relevance\/train.csv.zip","fa0ea3d4":"#\u0110\u1ecdc d\u1eef li\u1ec7u\ntrain_data = pd.read_csv(\"train.csv\", encoding=\"ISO-8859-1\")\ntest_data = pd.read_csv(\"test.csv\", encoding=\"ISO-8859-1\")\nattribute_data = pd.read_csv('attributes.csv')\ndescriptions = pd.read_csv('product_descriptions.csv')","0e1d72c3":"#T\u1eadp train \ntrain_data.head(10)","0a3d26ad":"train_data.shape","1a6ff8d1":"train_data.info()","fba175f1":"# Th\u1ed1ng k\u00ea s\u1ed1 l\u01b0\u1ee3ng m\u1ee9c \u0111\u1ed9 li\u00ean quan trong t\u1eadp train \ndef count_relevance():\n    high = [i for i in train_data['relevance'] if i >= 3.00]\n    medium = [i for i in train_data['relevance'] if i > 1.00 and i < 3.00]\n    low = [i for i in train_data['relevance'] if i == 1.00]\n    return [len(high), len(medium) , len(low)]","af2f1ed9":"from matplotlib import pyplot as plt","5452cd97":"\nplt.bar(['high', 'medium', 'low'], count_relevance(), label='Relevance score', color='red')\nplt.xlabel('name')\nplt.ylabel('numbers')\nplt.title('Bar')\nplt.legend()\nplt.show()","4029ea81":"test_data.shape","a68dfe2e":"test_data.head(10)","cd600cd0":"test_data.info()","a26cdf0b":"attribute_data.shape","5f86e67c":"attribute_data.head(50)","cdd7ebd7":"attribute_data.info()","8a289418":"attribute_counts = attribute_data.name.value_counts()\nprint(attribute_counts)\nprint(attribute_data['name'].unique())\nprint(len(attribute_data['name'].unique()))","2d620a29":"descriptions.shape","4baf9c08":"descriptions.head(10)","eb2cd697":"descriptions.info()","8976ed52":"#train \nprint(\"train :\" ,train_data[train_data.duplicated(keep='first')].shape )\nprint(\"test : \", test_data[test_data.duplicated(keep='first')].shape )\nprint(\"attribute : \", attribute_data[attribute_data.duplicated(keep='first')].shape )\nprint(\"descriptions : \", descriptions[descriptions.duplicated(keep='first')].shape )\n","3b77e196":"#Ki\u1ec3m tra c\u00e1c h\u00e0ng tr\u00f9ng nhau\nattribute_data[attribute_data.duplicated(keep='first')]","cdcd979c":"# Lo\u1ea1i b\u1ecf c\u00e1c h\u00e0ng tr\u00f9ng nhau v\u00e0 ki\u1ec3m tra l\u1ea1i \nattribute_data = attribute_data.drop_duplicates()\ndescriptions[descriptions.duplicated(keep='first')].shape ","31c44d0b":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm  \n\ntrain_data.relevance.plot(kind='hist', density=True)\nmu, std = norm.fit(train_data.relevance)\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\ntitle = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\nplt.title(title)\nplt.show()","fa87b2cc":"# T\u00e1ch t\u1eadp d\u1eef li\u1ec7u brand t\u1eeb t\u1eadp Attribute\ndf_brand = attribute_data[attribute_data.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"})\ndf_brand.head(5)","059bd632":"df_attr_stripped = attribute_data\ndf_attr_stripped['name'] = df_attr_stripped['name'].astype(str)\ndf_attr_stripped['name'] = df_attr_stripped['name'].apply(lambda s: re.sub(r\"Bullet([0-9]+)\", \"\", s))\ndf_attr_stripped.head(10)","cdfbe796":"df_attr_stripped['attribute'] = df_attr_stripped['name'] + \" \" + df_attr_stripped['value']\ndf_attr_stripped.head(10)","dd1d34b5":"df_attr_final = df_attr_stripped.groupby('product_uid').agg({'attribute': lambda s : ' '.join(s.astype(str))}).reset_index()\ndf_attr_final.head(20)","a49e4216":"data_all = pd.concat([train_data, test_data], axis=0, ignore_index=True) \ndata_all.head(5)","3534ac02":"data_all = pd.merge(data_all, descriptions, how = 'left', on = 'product_uid')","a3d28593":"data_all = pd.merge(data_all, df_brand, how = 'left', on = 'product_uid')","f5e29d52":"data_all = pd.merge(data_all, df_attr_final, how = 'left', on = 'product_uid')","a7f8920f":"data_all.head(10)","cb937275":"def string_edit(s:str): \n    if isinstance(s, str):\n        s = re.sub(r\"(\\w)\\.([A-Z])\", r\"\\1 \\2\", s) \n        s = s.lower()\n        s = s.replace(\"  \",\" \")\n        s = s.replace(\",\",\"\") \n        s = s.replace(\"$\",\" \")\n        s = s.replace(\"?\",\" \")\n        s = s.replace(\"-\",\" \")\n        s = s.replace(\"\/\/\",\"\/\")\n        s = s.replace(\"..\",\".\")\n        s = s.replace(\" \/ \",\" \")\n        s = s.replace(\" \\\\ \",\" \")\n        s = s.replace(\".\",\" . \")\n        s = re.sub(r\"(^\\.|\/)\", r\"\", s)\n        s = re.sub(r\"(\\.|\/)$\", r\"\", s)\n        s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n        s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n        s = s.replace(\" x \",\" xbi \")\n        s = re.sub(r\"([a-z])( *)\\.( *)([a-z])\", r\"\\1 \\4\", s)\n        s = re.sub(r\"([a-z])( *)\/( *)([a-z])\", r\"\\1 \\4\", s)\n        s = s.replace(\"*\",\" xbi \")\n        s = s.replace(\" by \",\" xbi \")\n        s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n        \n        # Consolidate variations of equivalent unit terms \n        s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n        s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n        s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1cu.ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n        s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n        s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n        s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n        s = s.replace(\"\u00b0\",\" degrees \")\n        s = re.sub(r\"([0-9]+)( *)(degrees|degree)\\.?\", r\"\\1deg. \", s)\n        s = s.replace(\" v \",\" volts \")\n        s = re.sub(r\"([0-9]+)( *)(volts|volt)\\.?\", r\"\\1volt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(watts|watt)\\.?\", r\"\\1watt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1amp. \", s)\n        s = s.replace(\"  \",\" \")\n        s = s.replace(\" . \",\" \")\n        \n        # Handling numeric instances with common identifiers\n        s = re.sub(r\"zero\\.?\", r\"0 \", s)\n        s = re.sub(r\"one\\.?\", r\"1 \", s)\n        s = re.sub(r\"two\\.?\", r\"2 \", s)\n        s = re.sub(r\"three\\.?\", r\"3 \", s)\n        s = re.sub(r\"four\\.?\", r\"4 \", s)\n        s = re.sub(r\"five\\.?\", r\"5 \", s)\n        s = re.sub(r\"six\\.?\", r\"6 \", s)\n        s = re.sub(r\"seven\\.?\", r\"7 \", s)\n        s = re.sub(r\"eight\\.?\", r\"8 \", s)\n        s = re.sub(r\"nine\\.?\", r\"9 \", s)\n        \n        return s\n    else:\n        # Return a \"null\" string if the parameter supplied is not a string\n        return \"null\"","406a0934":"import time\n\nstart_time = time.time()\n\ndata_all['search_term'] = data_all['search_term'].apply(str).apply(string_edit)\ndata_all['product_title'] = data_all['product_title'].apply(str).apply(string_edit)\ndata_all['product_description'] = data_all['product_description'].apply(str).apply(string_edit)\ndata_all['attribute'] = data_all['attribute'].apply(str).apply(string_edit)\nprint(data_all.head())\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","e07f8169":"data_all['brand'] = data_all['brand'].apply(str).apply(string_edit)","1cd83d55":"data_all.head(10)","661bc373":"from bs4 import BeautifulSoup\nimport lxml\nimport re\nimport nltk\nfrom nltk.corpus import stopwords # Import the stop word list\nfrom nltk.metrics import edit_distance\nfrom string import punctuation\nfrom collections import Counter\n\ndef str_stemmer(doc):\n    # th\u00eam kho\u1ea3ng tr\u1eafng \u0111\u1ec3 t\u00e1ch c\u00e1c t\u1eeb\n    tokens = doc.split()\n    # lo\u1ea1i b\u1ecf c\u00e1c d\u1ea5u ch\u1ea5m c\u00e2u\n    table = str.maketrans('', '', punctuation)\n    tokens = [w.translate(table) for w in tokens]\n    # lo\u1ea1i b\u1ecf c\u00e1c t\u1eeb kh\u00f4ng n\u1eb1m trong b\u1ea3ng ch\u1eef c\u00e1i\n    tokens = [word for word in tokens if word.isalpha()]\n    # l\u1ecdc c\u00e1c t\u1eeb l\u00e0 stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [w for w in tokens if not w in stop_words]\n    # l\u1ecdc ra c\u00e1c t\u1eeb ng\u1eafn \n    tokens = [word for word in tokens if len(word) > 1]\n    return ' '.join(tokens)\n\ndef str_stemmer_title(s):\n    return \" \".join(map(stemmer.stem, s.lower().split()))\n\ndef str_common_word(str1, str2):\n    whole_set = set(str1.split())\n    return sum(int(str2.find(word)>=0) for word in whole_set)\n\ndef get_shared_words(row_data):\n    return np.sum([str_common_word(*row_data[:-1]), str_common_word(*row_data[1:])])","8ed7a79c":"data_all['search_term'] = pd.Series(data_all['search_term'].map(lambda x:str_stemmer(str(x))))\ndata_all['product_title'] = pd.Series(data_all['product_title'].map(lambda x:str_stemmer(str(x))))\ndata_all['product_description'] = pd.Series(data_all['product_description'].map(lambda x:str_stemmer(str(x))))\ndata_all['attribute'] = pd.Series(data_all['attribute'].map(lambda x:str_stemmer(str(x))))","e9913059":"data_all.head(10)","7cdba725":"print('total data has html tags in',descriptions.product_description.str.count('<br$').values.sum())","c0d36fd4":"def remove_html_tag(text):\n    soup = BeautifulSoup(text, 'lxml')\n    text = soup.get_text().replace('Click here to review our return policy for additional information regarding returns', '')\n    return text","f5330eb7":"data_all['search_term'] = pd.Series(data_all['search_term'].map(lambda x:remove_html_tag(str(x))))\ndata_all['product_title'] = pd.Series(data_all['product_title'].map(lambda x:remove_html_tag(str(x))))\ndata_all['product_description'] = pd.Series(data_all['product_description'].map(lambda x:remove_html_tag(str(x))))","a7d170a8":"data_all.head(10)","9c55996c":"data_all['search_term_tokens'] = data_all.search_term.str.lower().str.split()\ndata_all['product_title_tokens'] = data_all.product_title.str.lower().str.split()\ndata_all['product_description_tokens'] = data_all.product_description.str.lower().str.split()","c82d72c3":"data_all.head(10)","9ebd64bb":"data_all[\"edistance_sprot\"] = [edit_distance(word1, word2) for word1, word2 in\n                                    data_all[[\"search_term\",\"product_title\"]].values.tolist()]\ndata_all[\"edistance_sd\"] = [edit_distance(word1, word2) for word1, word2 in\n                                    data_all[[\"search_term\",\"product_description\"]].values.tolist()]","61b50a6b":"data_all.head(10)","16791d37":"def get_jaccard_sim(columns): \n    str1, str2 = columns[0], columns[1]\n    a = set(str1) \n    b = set(str2)\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","10e0aeb1":"data_all['j_dis_sqt'] = [get_jaccard_sim(rows) for rows in data_all[[\"search_term_tokens\",\"product_title_tokens\"]].values]\ndata_all['j_dis_sqd'] = [get_jaccard_sim(rows) for rows in data_all[[\"search_term_tokens\",\"product_description_tokens\"]].values]","015c804f":"data_all.head(10)","0b325667":"data_all['search_query_length'] = data_all.search_term.str.len()\ndata_all['number_of_words_in_descr'] = data_all.product_description.str.count(\"\\\\w+\")","439947b7":"data_all['number_of_words_in_search'] = data_all['search_term_tokens'].map(lambda x: len(x))\ndata_all['number_of_words_in_title'] = data_all['product_title_tokens'].map(lambda x: len(x))\ndata_all['length_product_description'] = data_all.product_description.str.len()\ndata_all['length_product_title'] = data_all.product_title.str.len()\ndata_all['length_attribute'] = data_all.attribute.str.len()","7cf78375":"data_all['len_of_attribute'] = data_all['attribute'].map(lambda x:len(str(x).split())).astype(np.int64)\ndata_all['len_of_brand'] = data_all['brand'].map(lambda x:len(str(x).split())).astype(np.int64)","94677451":"data_all.head(10)","699f2870":"def cosineSim(v1, v2):\n    sumxx, sumxy, sumyy = 0, 0, 0\n    for i in range (len(v1)):\n        x = v1[i]\n        y = v2[i]\n        sumxx += x*x\n        sumyy += y*y\n        sumxy += x*y\n        \n    return sumxy\/math.sqrt(sumxx*sumyy)","120c1abb":"from sklearn.feature_extraction.text import TfidfVectorizer\narr1 = []\narr2 = []\narr3 = []\nfor i in range (len(data_all)):\n    product_title_i = data_all['product_title'][i]\n    description_i = data_all['product_description'][i]\n    attribute_i = data_all['attribute'][i]\n    search_term_i = data_all['search_term'][i]\n    \n    TfidfVectorizer1  = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n    TfidfVectorizer2 = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n    TfidfVectorizer3 = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n    \n    tf_idf1 = TfidfVectorizer1.fit_transform([product_title_i, search_term_i])\n    tf_idf2 = TfidfVectorizer2.fit_transform([description_i, search_term_i])\n    tf_idf3 = TfidfVectorizer3.fit_transform([attribute_i, search_term_i])\n    \n    # tinh cosine similarrities\n    \n    cosineSim_title_search = cosineSim(tf_idf1.toarray()[0], tf_idf1.toarray()[1])\n    arr1.append(cosineSim_title_search)\n    cosineSim_description = cosineSim(tf_idf2.toarray()[0], tf_idf2.toarray()[1])\n    arr2.append(cosineSim_description)\n    cosineSim_attribute = cosineSim(tf_idf3.toarray()[0], tf_idf3.toarray()[1])\n    arr3.append(cosineSim_attribute)","2eb317d5":"data_all['tfidf_cosineSim_search_title'] = arr1\ndata_all['tfidf_cosineSim_search_description'] = arr2\ndata_all['tfidf_cosineSim_search_attribute'] = arr3","b2187b85":"data_all['tfidf_cosineSim_search_title'] = data_all['tfidf_cosineSim_search_title'].fillna(data_all['tfidf_cosineSim_search_title'].sum()\/len(data_all))\ndata_all['tfidf_cosineSim_search_description'] = data_all['tfidf_cosineSim_search_description'].fillna(data_all['tfidf_cosineSim_search_description'].sum()\/len(data_all))","c75ed466":"data_all.head(10)","1fcb6dfc":"from sklearn.feature_extraction.text import CountVectorizer\n\narr1 = []\narr2 = []\narr3 = []\nfor i in range (len(data_all)):\n    product_title_i = data_all['product_title'][i]\n    description_i = data_all['product_description'][i]\n    attribute_i = data_all['attribute'][i]\n    search_term_i = data_all['search_term'][i]\n    \n    vectorizer1 = CountVectorizer()\n    vectorizer2 = CountVectorizer()\n    vectorizer3 = CountVectorizer()\n    \n    tf1 = vectorizer1.fit_transform([product_title_i, search_term_i])\n    tf2 = vectorizer2.fit_transform([description_i, search_term_i])\n    tf3 = vectorizer3.fit_transform([attribute_i, search_term_i])\n\n    \n    # tinh cosine similarrities\n    \n    cosineSim_title_search = cosineSim(tf1.toarray()[0], tf1.toarray()[1])\n    arr1.append(cosineSim_title_search)\n    cosineSim_description = cosineSim(tf2.toarray()[0], tf2.toarray()[1])\n    arr2.append(cosineSim_description)\n    cosineSim_attribute = cosineSim(tf3.toarray()[0], tf3.toarray()[1])\n    arr3.append(cosineSim_attribute)","feb10b46":"data_all['cosineSim_search_title'] = arr1\ndata_all['cosineSim_search_description'] = arr2\ndata_all['cosineSim_search_attribute'] = arr3","7a6e5ae8":"data_all.head(10)","fb162b00":"data_all['product_info'] = data_all['search_term']+'\\t'+data_all['product_title']+'\\t'+data_all['product_description']+'\\t'+data_all['attribute']\ndata_all['word_in_title'] = data_all['product_info'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\ndata_all['word_in_description'] = data_all['product_info'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[2]))\ndata_all['word_in_attributes'] = data_all['product_info'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[3]))\ndata_all['ratio_title'] = data_all['word_in_title']\/data_all['number_of_words_in_search']\ndata_all['ratio_description'] = data_all['word_in_description']\/data_all['number_of_words_in_search']\ndata_all['ratio_attributes'] = data_all['word_in_attributes']\/data_all['number_of_words_in_search']\ndata_all['title_length'] = data_all.product_title.str.len()","416ec445":"data_all['attr'] = str(data_all['search_term'])+\"\\t\"+str(data_all['brand'])\ndata_all['brand_in_search'] = data_all['attr'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\ndata_all['ratio_brand'] = data_all['brand_in_search']\/data_all['len_of_brand']","e32831c4":"data_all['ratio_title'] = data_all['ratio_title'].fillna(data_all['ratio_title'].sum()\/len(data_all))\ndata_all['ratio_description'] = data_all['ratio_description'].fillna(data_all['ratio_description'].sum()\/len(data_all))\ndata_all['ratio_attributes'] = data_all['ratio_attributes'].fillna(data_all['ratio_attributes'].sum()\/len(data_all))","e2c71819":"data_all.head(10)","9798f11c":"from fuzzywuzzy import fuzz\ndef fuzzy_partial_ratio(string_1 , string_2):\n    return fuzz.partial_ratio(string_1, string_2)","70707347":"def fuzzy_token_sort_ratio(string_1,string_2):\n    return fuzz.token_sort_ratio(string_1,string_2)","61ea38a0":"data_all['fuzzy_ratio_in_title'] = data_all['product_info'].map(lambda x:fuzzy_partial_ratio(x.split('\\t')[0],x.split('\\t')[1]))\ndata_all['fuzzy_ratio_in_description'] = data_all['product_info'].map(lambda x:fuzzy_partial_ratio(x.split('\\t')[0],x.split('\\t')[2]))\ndata_all['fuzzy_ratio_in_attribute'] = data_all['product_info'].map(lambda x:fuzzy_partial_ratio(x.split('\\t')[0],x.split('\\t')[3]))","9ed5026b":"data_all['fuzzy_token_sort_ratio_in_title'] = data_all['product_info'].map(lambda x:fuzzy_token_sort_ratio(x.split('\\t')[0],x.split('\\t')[1]))\ndata_all['fuzzy_token_sort_ratio_in_description'] = data_all['product_info'].map(lambda x:fuzzy_token_sort_ratio(x.split('\\t')[0],x.split('\\t')[2]))\ndata_all['fuzzy_token_sort_ratio_in_attribute'] = data_all['product_info'].map(lambda x:fuzzy_token_sort_ratio(x.split('\\t')[0],x.split('\\t')[3]))","868e0d61":"data_all.head(10)","ceb0ec6b":"tf = TfidfVectorizer()\narr1 =[]\narr2 = []\narr3 = []\narr4 = []\nfor i in range (len(data_all)):\n    arr1.append(tf.fit_transform([data_all['product_description'][i]]).mean())\n    arr2.append(tf.fit_transform([data_all['product_title'][i]]).mean())\n    arr3.append(tf.fit_transform([data_all['attribute'][i]]).mean())\ndata_all['mean_product_des'] = arr1\ndata_all['mean_product_title'] = arr2\ndata_all['mean_attribute'] = arr3","be107709":"data_all.head()","afa5c0b1":"data_all = data_all.drop(['product_title', 'search_term', 'product_description', 'search_term_tokens', 'product_title_tokens', 'product_description_tokens', 'product_info', 'attr', 'attribute', 'brand'], axis = 1)","b94316db":"data_all.head(10)","e0fac3b2":"train = data_all[:74067]\ntest = data_all[74067:]","4765d849":"y_train = train['relevance'].values\nX_train = train.drop(['id', 'relevance'], axis = 1).values \nid_test = test['id']\nX_test = test.drop(['id', 'relevance'], axis = 1).values","a2116d23":"import xgboost as xgb\nfrom matplotlib import pyplot\nfrom xgboost import plot_importance\nbst = xgb.XGBRegressor(max_depth = 6,\n                    n_estimators = 130).fit(X_train , y_train)\ny_pred = bst.predict(X_test)\npyplot.bar(range(len(bst.feature_importances_)), bst.feature_importances_)\npyplot.show()\nlen(bst.feature_importances_)","82425a8b":"from xgboost import XGBRegressor\nimport sklearn\nxgb = XGBRegressor()\nparam_grid = {'max_depth':[5, 6], \n              'n_estimators': [130, 150, 170], \n              'learning_rate' : [0.1]}\nmodel_xgb = sklearn.model_selection.GridSearchCV(estimator = xgb, param_grid = param_grid, n_jobs = -1)\nmodel_xgb.fit(X_train, y_train)\n\ny_pred = model_xgb.predict(X_test)","3584f025":"for i in range(len(y_pred)):\n    if y_pred[i] > 3:\n        y_pred[i] = 3","d17fb6fb":"pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)","b503a310":"- Fuzzy Seach (t\u00ecm ki\u1ebfm \"m\u1edd\"), hay c\u00f2n hay \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 Approximate Search (t\u00ecm ki\u1ebfm \"x\u1ea5p x\u1ec9\") l\u00e0 kh\u00e1i ni\u1ec7m \u0111\u1ec3 ch\u1ec9 k\u1ef9 thu\u1eadt \u0111\u1ec3 t\u00ecm ki\u1ebfm m\u1ed9t x\u00e2u \"g\u1ea7n gi\u1ed1ng\" (thay v\u00ec \"gi\u1ed1ng h\u1ec7t\") so v\u1edbi m\u1ed9t x\u00e2u cho tr\u01b0\u1edbc. V\u00ed d\u1ee5: ta nh\u1eadp t\u1eeb t\u00ecm ki\u1ebfm l\u00e0 \"tran thanh long \" c\u00f3 th\u1ec3 t\u00ecm ki\u1ebfm ra t\u1eeb \"trn thnh log\" v\u00ec 2 t\u1eeb g\u1ea7n gi\u1ed1ng nhau ","c5282abe":"## 3.1 Ki\u1ec3m tra d\u1eef li\u1ec7u tr\u00f9ng nhau","ae3a18c5":"- Sau b\u01b0\u1edbc tr\u00ean ta thu \u0111\u01b0\u1ee3c 2 \u0111\u1eb7c tr\u01b0ng m\u1edbi:\n    - edistance_sprot l\u00e0 kho\u1ea3ng c\u00e1ch edit_distance gi\u1eefa search_term v\u00e0 product_title\n    - edistance_sd l\u00e0 kho\u1ea3ng c\u00e1ch edit_distance gi\u1eefa search_term v\u00e0 product_description","9467c73f":"- \u0110\u00e3 lo\u1ea1i b\u1ecf xong c\u00e1c h\u00e0ng tr\u00f9ng nhau","81391999":"## 2.1 Khai b\u00e1o c\u00e1c th\u01b0 vi\u1ec7n","e9920878":"- M\u00f4 t\u1ea3 d\u1eef li\u1ec7u:\n    -\n- C\u00e1c file data :  \n    - `Train.csv` : t\u1eadp train g\u1ed3m s\u1ea3n ph\u1ea9m (product), t\u00ecm ki\u1ebfm (search) v\u00e0 m\u1ee9c \u0111\u1ed9 li\u00ean quan (relevance)\n    - `Test.csv`: t\u1eadp test g\u1ed3m s\u1ea3n ph\u1ea9m (product) , t\u00ecm ki\u1ebfm (search) v\u00e0 d\u1ef1a v\u00e0o t\u1eadp test n\u00e0y ch\u00fang ta ph\u1ea3i d\u1ef1 \u0111o\u00e1n \u0111\u01b0\u1ee3c m\u1ee9c \u0111\u1ed9 li\u00ean quan (relevance)\n    - `Product_descriptions.csv`: ch\u1ee9a \u0111o\u1ea1n text m\u00f4 t\u1ea3 s\u1ea3n ph\u1ea9m\n    - `Attributes.csv`: ch\u1ee9a th\u00f4ng s\u1ed1 thu\u1ed9c t\u00ednh c\u1ee7a s\u1ea3n ph\u1ea9m, m\u1ed9t s\u1ed1 s\u1ea3n ph\u1ea9m kh\u00f4ng c\u00f3 thu\u1ed9c t\u00ednh\n- C\u00e1c tr\u01b0\u1eddng data :\n    - `Id`: id c\u1ee7a m\u1ed9t c\u1eb7p search_term v\u00e0 s\u1ea3n ph\u1ea9m.\n    - `Product_uid`: id c\u1ee7a s\u1ea3n ph\u1ea9m\n    - `Product_title` : ti\u00eau \u0111\u1ec1 c\u1ee7a s\u1ea3n ph\u1ea9m \n    - `Product_description` : m\u00f4 t\u1ea3 s\u1ea3n ph\u1ea9m \n    - `Search_term`: \u0111o\u1ea1n text \u0111\u01b0\u1ee3c ghi l\u1ea1i\n    - `Relevance` : m\u1ee9c \u0111\u1ed9 li\u00ean quan search v\u00e0 k\u1ebft qu\u1ea3 t\u00ecm ki\u1ebfm \n    - `Name`: t\u00ean thu\u1ed9c t\u00ednh\n    - `Value`: gi\u00e1 tr\u1ecb thu\u1ed9c t\u00ednh\n\n","2c756780":"- M\u1ed9t s\u1ed1 gi\u00e1 tr\u1ecb d\u1ef1 \u0111o\u00e1n v\u01b0\u1ee3t qu\u00e1 3 s\u1ebd \u0111\u01b0\u1ee3c x\u00e9t l\u1ea1i l\u00e0 3","2b7c9237":"- Stopword l\u00e0 nh\u1eefng t\u1eeb v\u00ed d\u1ee5 nh\u01b0 the , in , on , at , which , what , .....\n- S\u1eed d\u1ee5ng h\u00e0m stopwords trong th\u01b0 vi\u1ec7n nltk \u0111\u1ec3 x\u1eed l\u00fd\n- \u0110\u1ecbnh ngh\u0129a h\u00e0m str_steamer \u0111\u1ec3 lo\u1ea1i b\u1ecf stopword","55f0b324":"## 2.2 Ph\u00e2n t\u00edch d\u1eef li\u1ec7u","be28e879":"- Nh\u01b0 ta nh\u1eadn th\u1ea5y c\u00e1c h\u00e0ng tr\u00f9ng nhau \u0111\u1ec1u c\u00f3 gi\u00e1 tr\u1ecb NaN v\u00ec th\u1ebf gi\u00e1 tr\u1ecb n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c lo\u1ea1i b\u1ecf ","24a7a5fa":"### 5.1.2 Chia data_all th\u00e0nh 2 t\u1eadp train v\u00e0 test ","7074c4b1":"- Chu\u1ea9n h\u00f3a x\u00e2u : \n    - Thay th\u1ebf c\u00e1c k\u00ed t\u1ef1, chu\u1ed7i kh\u00f4ng c\u1ea7n thi\u1ebft b\u1eb1ng kho\u1ea3ng tr\u1eafng\n    - Chuy\u1ec3n ch\u1eef th\u00e0nh s\u1ed1 v\u00ed d\u1ee5 chuy\u1ec3n fivexfive->5x5\n    - Chuy\u1ec3n ch\u1eef hoa th\u00e0nh ch\u1eef th\u01b0\u1eddng","65ff33e7":"## 4.3 T\u00ednh Jaccard simulator\n\n- Jaccard simulator d\u00f9ng \u0111\u1ec3 t\u00ednh \u0111\u1ed9 t\u01b0\u01a1ng \u0111\u1ed3ng gi\u1eefa 2 \u0111o\u1ea1n v\u0103n","a65ee9f0":"# 4. Tr\u00edch ch\u1ecdn \u0111\u1eb7c tr\u01b0ng  ","641f5ebd":"### 5.1.3 Chia t\u1eadp d\u1eef li\u1ec7u th\u00e0nh X_train, y_train, X_test, id_test ","8e714731":"- T\u1eadp attribute ch\u1ee9a 2044803 h\u00e0ng v\u00e0 3 c\u1ed9t , g\u1ed3m c\u00e1c tr\u01b0\u1eddng `product_uid`, `name` v\u00e0 `value`\n- V\u1edbi m\u1ed7i `product_uid` c\u00f3 m\u1ed9t name kh\u00e1c nhau th\u00ec value c\u0169ng kh\u00e1c nhau tuy nhi\u00ean c\u00e1c name kh\u00e1c nhau ch\u1ec9 m\u1ed9t ho\u1eb7c hai k\u00fd t\u1ef1.","d3b3b89c":"### 3.4.3 Lo\u1ea1i b\u1ecf c\u00e1c th\u1ebb HTML","538ce675":"- Thu \u0111\u01b0\u1ee3c 3 \u0111\u1eb7c tr\u01b0ng m\u1edbi` mean_product_des`, `mean_product_title`, `mean_attribute`:","a628d947":"# 2. Ph\u00e2n t\u00edch d\u1eef li\u1ec7u","5b6a0645":"**Tr\u1ea7n Th\u00e0nh Long \n19021323**","3394f4ad":"## 3.3 T\u1ea1o t\u1eadp data m\u1edbi ","4860eb1d":"### 3.4.1 Ti\u1ec1n x\u1eed l\u00fd\n\n","72323682":"* CountVectorizer \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 chuy\u1ec3n \u0111\u1ed5i m\u1ed9t b\u1ed9 s\u01b0u t\u1eadp c\u00e1c t\u00e0i li\u1ec7u v\u0103n b\u1ea3n th\u00e0nh m\u1ed9t vect\u01a1 c\u00f3 s\u1ed1 l\u01b0\u1ee3ng thu\u1eadt ng\u1eef \/ m\u00e3 th\u00f4ng b\u00e1o. N\u00f3 c\u0169ng cho ph\u00e9p x\u1eed l\u00fd tr\u01b0\u1edbc d\u1eef li\u1ec7u v\u0103n b\u1ea3n tr\u01b0\u1edbc khi t\u1ea1o bi\u1ec3u di\u1ec5n vect\u01a1. Ch\u1ee9c n\u0103ng n\u00e0y l\u00e0m cho n\u00f3 tr\u1edf th\u00e0nh m\u1ed9t m\u00f4-\u0111un bi\u1ec3u di\u1ec5n t\u00ednh n\u0103ng r\u1ea5t linh ho\u1ea1t cho v\u0103n b\u1ea3n.\n* Ta s\u1eed d\u1ee5ng countvectorize \u0111\u1ec3 bi\u1ebfn text th\u00e0nh c\u00e1c ma tr\u1eadn th\u01b0a r\u1ed3i d\u00f9ng cosinesimilary \u0111\u1ec3 so s\u00e1nh ch\u00fang","317dbd6a":"## 4.7 T\u00ednh ratio\n\n\n### 4.7.1. T\u00ednh ratio theo c\u00f4ng th\u1ee9c \n* H\u00e0m str_common_word d\u00f9ng \u0111\u1ec3 t\u00ecm t\u1eeb chung gi\u1eefa 2 \u0111o\u1ea1n v\u0103n. Ta s\u1ebd t\u00ecm t\u1eeb chung c\u1ee7a 2 c\u1eb7p (search_term, product_title) v\u00e0 (search_term, product_description)\n* T\u00ednh 2 \u0111\u1eb7c tr\u01b0ng m\u1edbi theo c\u00f4ng th\u1ee9c sau: ratio =  $\\frac{s\u1ed1-t\u1eeb-chung}{num-of-word-in-search}$","e81d5a6f":"**X\u00e2y d\u1ef1ng attribute_data m\u1edbi**\n\n- Hi\u1ec7n ta mu\u1ed1n x\u00e2y d\u1ef1ng m\u1ed9t tr\u01b0\u1eddng attribute_data m\u1edbi, ch\u1ec9 g\u1ed3m 2 tr\u01b0\u1eddng l\u00e0 product_uid v\u00e0 attribute (text)\n- Th\u00f4ng tin \"Bullet\" l\u00e0 v\u00f4 ngh\u0129a v\u00e0 s\u1ebd x\u00f3a ch\u00fang.\n- Ta t\u1ea1o attribute_data_stripped m\u1edbi (\u0111\u1ec3 tr\u00e1nh thay \u0111\u1ed5i d\u1eef li\u1ec7u g\u1ed1c), v\u1edbi h\u00e0m re.sub \u0111\u1ec3 x\u00f3a \u0111i c\u00e1c \"Bullet\", thay b\u1eb1ng x\u00e2u r\u1ed7ng","7f959787":"- Sau c\u00e1c b\u01b0\u1edbc tr\u00ean thu \u0111\u01b0\u1ee3c 3 \u0111\u1eb7c tr\u01b0ng m\u1edbi:\n    - `tfidf_cosineSim_search_title` l\u00e0 kho\u1ea3ng c\u00e1ch cosine similary gi\u1eefa 2 vector thu \u0111\u01b0\u1ee3c sau khi th\u1ef1c hi\u1ec7n tf-idf search_term v\u00e0 product_title\n    - `tfidf_cosineSim_search_description` l\u00e0 kho\u1ea3ng c\u00e1ch cosine similary gi\u1eefa 2 vector thu \u0111\u01b0\u1ee3c sau khi th\u1ef1c hi\u1ec7n tf-idf search_term v\u00e0 product_description\n    - `tfidf_cosineSim_search_attribute` l\u00e0 kho\u1ea3ng c\u00e1ch cosine similary gi\u1eefa 2 vector thu \u0111\u01b0\u1ee3c sau khi th\u1ef1c hi\u1ec7n tf-idf search_term v\u00e0 attribute\n","06515f90":"- Thu \u0111\u01b0\u1ee3c 2 \u0111\u1eb7c tr\u01b0ng m\u1edbi:\n\n    - j_dis_sqt l\u00e0 jaccard simulator gi\u1eefa search_term_tokens v\u00e0 product_title_tokens\n    - j_dis_sqd l\u00e0 jaccard simulator gi\u1eefa search_term_tokens v\u00e0 product_description_tokens","abd038c7":"- T\u1eadp train g\u1ed3m 74067 h\u00e0ng v\u00e0 5 c\u1ed9t trong \u0111\u00f3 ch\u1ee9a c\u00e1c tr\u01b0\u1eddng : `product_uid` , `product_tile` , `search_term` v\u00e0 `relevance`\n \n","9025563d":"- Thu \u0111\u01b0\u1ee3c c\u00e1c \u0111\u1eb7c tr\u01b0ng m\u1edbi sau:\n    * `word_in_title` l\u00e0 s\u1ed1 t\u1eeb chung gi\u1eefa search_term v\u00e0 product_title \n    * `word_in_description` l\u00e0 s\u1ed1 t\u1eeb chung gi\u1eefa search_term v\u00e0 product_description\n    * `word_in_attributes` l\u00e0 s\u1ed1 t\u1eeb chung gi\u1eefa search_term v\u00e0 attribute\n    * `ratio_title` l\u00e0 t\u1ec9 l\u1ec7 s\u1ed1 t\u1eeb chung gi\u1eefa search v\u00e0 title chia cho s\u1ed1 t\u1eeb search \n    * `ratio_description` l\u00e0 t\u1ec9 l\u1ec7 s\u1ed1 t\u1eeb chung gi\u1eefa search v\u00e0 desceiption chia cho s\u1ed1 t\u1eeb search \n    * `ratio_attributes` t\u1ec9 l\u1ec7 s\u1ed1 t\u1eeb chung gi\u1eefa search v\u00e0 attribute chia cho s\u1ed1 t\u1eeb search ","6da39bde":"### 2.2.3 Attribute_dataset","779dae7b":"- Nh\u01b0 v\u1eady ngo\u00e0i t\u1eadp Attribute c\u00f3 ch\u1ee9a 154 h\u00e0ng tr\u00f9ng nhau ra, c\u00e1c t\u1eadp c\u00f2n l\u1ea1i kh\u00f4ng c\u00f3 d\u1eef li\u1ec7u b\u1ecb tr\u00f9ng nhau","e35c0696":"## 4.2 T\u00ednh edit_distance\n\n- Edit_distance l\u00e0 s\u1ed1 b\u01b0\u1edbc c\u1ea7n th\u1ef1c hi\u1ec7n \u0111\u1ec3 chuy\u1ec3n t\u1eeb chu\u1ed7i s1 -> s2. V\u00ed d\u1ee5 \u0111\u1ec3 chuy\u1ec3n rain -> shine c\u1ea7n 3 b\u01b0\u1edbc:\"rain\u201d -> \u201csain\u201d -> \u201cshin\u201d -> \u201cshine\u201d => edit_distance = 3. Ta s\u1ebd d\u00f9ng h\u00e0m edit_distance c\u1ee7a th\u01b0 vi\u1ec7n nltk \u0111\u1ec3 t\u00ednh ra \u0111\u1eb7c tr\u01b0ng edit_distance gi\u1eefa 2 c\u1eb7p","688b7cca":"- Nh\u01b0 ta th\u1ea5y c\u00f3 173 tag HTML trong c\u00e1c \u0111o\u1ea1n text\n- Trong qu\u00e1 tr\u00ecnh thu th\u1eadp d\u1eef li\u1ec7u t\u1eeb c\u00e1c trang web th\u00ec c\u00f2n s\u00f3t l\u1ea1i c\u00e1c tag HTML m\u00e0 ch\u00fang kh\u00f4ng c\u1ea7n thi\u1ebft cho qu\u00e1 tr\u00ecnh ph\u00e2n t\u00edch d\u1eef li\u1ec7u hay hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh n\u00ean ta c\u1ea7n xem x\u00e9t v\u00e0 lo\u1ea1i b\u1ecf\n- S\u1eed d\u1ee5ng BeautifulSoup \u0111\u1ec3 lo\u1ea1i b\u1ecf tag html","ac51b45c":"- Ta x\u00e2y d\u1ef1ng m\u1ed9t h\u00e0m v\u1edbi input l\u00e0 1 x\u00e2u \u0111\u1ea7u v\u00e0o , k\u1ebft qu\u1ea3 tr\u1ea3 v\u1ec1 m\u1ed9t x\u00e2u \u0111\u00e3 \u0111\u01b0\u1ee3c chu\u1ea9n h\u00f3a ","7655bbc2":"# 5. Hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh","a6e8d022":"# 1. M\u00f4 t\u1ea3 b\u00e0i to\u00e1n","6cce861f":"- Nh\u01b0 \u0111\u00e3 n\u00f3i trong ph\u1ea7n ph\u00e2n t\u00edch Attribute_data , MFG l\u00e0 m\u1ed9t th\u00f4ng tin h\u1eefu \u00edch\n- Ta mu\u1ed1n x\u00e2y d\u1ef1ng m\u1ed9t dataFrame kh\u00e1c ch\u1ec9 d\u1ef1a tr\u00ean th\u00f4ng tin n\u00e0y, b\u1eb1ng vi\u1ec7c ch\u1ecdn nh\u1eefng h\u00e0ng \u1edf attribute_data c\u00f3 attribute_data.name == \"MFG Brand Name\", l\u1ea5y product_uid v\u00e0 \"value\", v\u00e0 tr\u01b0\u1eddng value \u0111\u01b0\u1ee3c thay b\u1eb1ng th\u00f4ng tin m\u1edbi: brand - t\u00ean nh\u00e3n hi\u1ec7u","04ad6899":"## 5.2 Hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh\n\n- S\u1eed d\u1ee5ng m\u00f4 h\u00ecnh XGBRegression (eXtreme Gradient Boosting)\n- \u00dd t\u01b0\u1edfng c\u1ee7a Boosting :\n    - X\u00e2y d\u1ef1ng m\u1ed9t l\u01b0\u1ee3ng l\u1edbn c\u00e1c model (th\u01b0\u1eddng l\u00e0 c\u00f9ng lo\u1ea1i). M\u1ed7i model sau s\u1ebd h\u1ecdc c\u00e1ch s\u1eeda l\u1ed7i c\u1ee7a model tr\u01b0\u1edbc \u0111\u1ec3 c\u1ea3i thi\u1ec7n ch\u1ea5t l\u01b0\u1ee3ng c\u1ee7a m\u00f4 h\u00ecnh (c\u1ee5 th\u1ec3 ta s\u1ebd gi\u1ea3m tr\u1ecdng s\u1ed1 khi d\u1eef li\u1ec7u \u0111\u00fang v\u00e0 t\u0103ng tr\u1ecdng s\u1ed1 khi d\u1eef li\u1ec7u sai)\n- XGBRegression : \n    - L\u00e0 thu\u1eadt to\u00e1n m\u1edf r\u1ed9ng c\u1ee7a gradient boost\n    - \u0110\u01b0\u1ee3c s\u1eed d\u1ee5ng r\u1ed9ng d\u00e3i trong c\u00e1c cu\u1ed9c thi kaggle\n- \u01afu \u0111i\u1ec3m : \n    - D\u1ec5 s\u1eed d\u1ee5ng, c\u00e0i \u0111\u1eb7t\n    - Cho \u0111\u1ed9 ch\u00ednh x\u00e1c cao\n    - C\u00e1c th\u00f4ng s\u1ed1 c\u00f3 th\u1ec3 \u0111i\u1ec1u ch\u1ec9nh \u0111\u01b0\u1ee3c\n    - D\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o kh\u00f4ng b\u1ecb b\u00f3 bu\u1ed9c, c\u00f3 th\u1ec3 ch\u1ee9a c\u00e1c gi\u00e1 tr\u1ecb nh\u01b0 NaN, text,....\n- Gi\u1ea3i th\u00edch c\u00e1c tham s\u1ed1 s\u1eed d\u1ee5ng trong m\u00f4 h\u00ecnh :\n    * max_depth: \u0111\u1ed9 s\u00e2u c\u1ee7a c\u00e2y quy\u1ebft \u0111\u1ecbnh \n    * n_estimators: s\u1ed1 l\u01b0\u1ee3ng c\u00e2y trong m\u00f4 h\u00ecnh \n    * learning_rate: h\u1ec7 s\u1ed1 h\u1ecdc, h\u1ec7 s\u1ed1 n\u00e0y d\u00f9ng \u0111\u1ec3 nh\u00e2n v\u1edbi k\u1ebft qu\u1ea3 m\u00f4 h\u00ecnh tr\u01b0\u1edbc\n- Ta s\u1eed d\u1ee5ng feature_importances c\u1ee7a XGBoost v\u00e0 matplotlib \u0111\u1ec3 bi\u1ec3u th\u1ecb \u0111\u1ed9 quan tr\u1ecdng c\u1ee7a c\u00e1c feature: \n","e91b767d":"- T\u01b0\u01a1ng t\u1ef1 nh\u01b0 t\u00ednh cosine th\u00ec t\u00ednh ratio c\u0169ng sinh ra m\u1ed9t s\u1ed1 gi\u00e1 tr\u1ecb NaN n\u00ean ph\u1ea3i x\u00e9t gi\u00e1 tr\u1ecb cho nh\u1eefng gi\u00e1 tr\u1ecb \u0111\u00f3 l\u00e0 trung b\u00ecnh.","bac0133f":"- T\u1eeb \u0111\u1ed3 th\u1ecb ta th\u1ea5y \u0111\u01b0\u1ee3c \u0111i\u1ec3m `relevance` ph\u00e2n b\u1ed5 ch\u1ee7 y\u1ebfu t\u1eeb 2-3\n- \u0110\u1ed3 th\u1ecb tr\u00ean kh\u00f4ng ph\u1ea3i l\u00e0 ph\u00e2n b\u1ed1 chu\u1ea9n ","4cb91bbe":"## 5.1 T\u1ea1o d\u1eef li\u1ec7u\n\n### 5.1.1 Lo\u1ea1i b\u1ecf c\u00e1c d\u1eef li\u1ec7u d\u1ea1ng ch\u1eef, ch\u1ec9 l\u1ea5y d\u1eef li\u1ec7u d\u1ea1ng s\u1ed1 \u0111\u1ec3 hu\u1ea5n luy\u1ec7n ","a40b86d2":"## 4.6 Countvectorize v\u00e0 cosinesimilary","a648c14d":"- Sau b\u01b0\u1edbc tr\u00ean, ta v\u1eabn th\u1ea5y c\u00f3 v\u1ea5n \u0111\u1ec1, \u0111\u00f3 l\u00e0 m\u1ed9t product_uid ch\u1ee9a nhi\u1ec1u attribute.\n- Do \u0111\u00f3, ta nh\u00f3m h\u1ebft c\u00e1c d\u1eef li\u1ec7u b\u1eb1ng h\u00e0m groupby('product_uid'), sau \u0111\u00f3 c\u1ed9ng x\u00e2u ch\u00fang l\u1ea1i b\u1eb1ng h\u00e0m ' '.join(...) v\u1edbi agg(), ph\u00e2n t\u00e1ch b\u1edfi d\u1ea5u c\u00e1ch.\n- K\u1ebft qu\u1ea3 l\u00e0 attribute_data_new, v\u1edbi 2 tr\u01b0\u1eddng l\u00e0 product_uid v\u00e0 attribute, s\u1eb5n s\u00e0ng cho vi\u1ec7c g\u1ed9p v\u00e0o d\u1eef li\u1ec7u ch\u00ednh all_data nh\u01b0 brand_data v\u00e0 descriptions.","af059171":"### 4.7.2. T\u00ednh ratio b\u1eb1ng h\u00e0m fuzzy","0ec3de40":"## 4.8. T\u00ednh gi\u00e1 tr\u1ecb Mean c\u1ee7a product_description, product_title, attribute","fb44ac74":"- Do khi t\u00ednh cosine, m\u1eabu = 0 n\u00ean m\u1ed9t s\u1ed1 gi\u00e1 tr\u1ecb s\u1ebd l\u00e0 NaN khi\u1ebfn cho t\u1eadp d\u1eef li\u1ec7u kh\u00f4ng th\u1ec3 \u0111\u01b0a v\u00e0o hu\u1ea5n luy\u1ec7n \u0111\u01b0\u1ee3c. V\u00ec v\u1eady ta s\u1ebd thay c\u00e1c gi\u00e1 tr\u1ecb NaN \u0111\u00f3 b\u1eb1ng gi\u00e1 tr\u1ecb trung b\u00ecnh c\u00e1c cosine t\u00ednh \u0111\u01b0\u1ee3c","eb636577":"# 3.T\u1ea1o l\u1ea1i d\u1eef li\u1ec7u v\u00e0 Clean d\u1eef li\u1ec7u","0937e4fb":"## 3.2 Ph\u00e2n b\u1ed5 relevance ","eef086ff":"### 2.2.2 Test_dataset ","a0ec7568":"- Nh\u00ecn v\u00e0i k\u1ebft qu\u1ea3, ta th\u1ea5y nhi\u1ec1u t\u00ean thu\u1ed9c t\u00ednh l\u00e0 Bullet, kh\u00f4ng mang \u00fd ngh\u0129a g\u00ec. Tuy nhi\u00ean l\u1ea1i c\u00f3 nh\u1eefng th\u00f4ng tin mang ng\u1eef ngh\u0129a.\n- C\u00f3 m\u1ed9t tr\u01b0\u1eddng th\u00f4ng tin l\u00e0 MFG Brand Name xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t, ng\u1eef ngh\u0129a c\u1ee7a n\u00f3 l\u00e0 tr\u01b0\u1eddng mang th\u00f4ng tin nh\u00e0 s\u1ea3n xu\u1ea5t s\u1ea3n ph\u1ea9m \u0111\u00f3 (Manufacturing Brand Name) \n- C\u00f3 5411 tr\u01b0\u1eddng th\u00f4ng tin kh\u00e1c nhau, tr\u00ean t\u1ed5ng s\u1ed1 2 tri\u1ec7u m\u1eabu, t\u1ee9c l\u00e0 ta ho\u00e0n to\u00e0n c\u00f3 kh\u1ea3 n\u0103ng d\u1ef1a v\u00e0o th\u00f4ng tin n\u00e0y \u0111\u1ec3 l\u1ea5y \u0111\u1eb7c tr\u01b0ng","bfc8f199":"- S\u1eed d\u1ee5ng tf-idf \u0111\u1ec3 bi\u1ebfn d\u1eef li\u1ec7u d\u1ea1ng text th\u00e0nh vector sau \u0111\u00f3 t\u00ednh gi\u00e1 tr\u1ecb trung b\u00ecnh c\u1ee7a c\u00e1c vector \u0111\u00f3 \u0111\u1ec3 chuy\u1ec3n sang d\u1ea1ng s\u1ed1. Gi\u00e1 tr\u1ecb trung b\u00ecnh t\u00ednh theo c\u00f4ng th\u1ee9c sau: mean = $\\frac {sum}{len(vector)}$. \u1ede \u0111\u00e2y ta d\u00f9ng h\u00e0m np.mean()","125fc544":"## 4.5 TF-IDF v\u00e0 CosineSimilary\n\n- **TF-IDF** l\u00e0 vi\u1ebft t\u1eaft c\u1ee7a **Term Frequency-Inverse Document Frequency**. Hi\u1ec3u m\u1ed9 c\u00e1ch \u0111\u01a1n gi\u1ea3n n\u00f3 l\u00e0 s\u1ef1 k\u1ebft h\u1ee3p c\u1ee7a t\u1ea7n s\u1ed1 xu\u1ea5t hi\u1ec7n c\u1ee7a m\u1ed9t t\u1eeb trong m\u1ed9t m\u1eabu v\u00e0 ngh\u1ecbch \u0111\u1ea3o c\u1ee7a t\u1ea7n s\u1ed1 c\u1ee7a t\u1eeb \u0111\u00f3 trong to\u00e0n b\u1ed9 t\u1eadp d\u1eef li\u1ec7u. K\u1ef9 thu\u1eadt n\u00e0y \u0111\u01b0\u1ee3c ph\u00e1t tri\u1ec3n \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 k\u1ebft qu\u1ea3 cho c\u00e1c truy v\u1ea5n trong c\u00f4ng c\u1ee5 t\u00ecm ki\u1ebfm v\u00e0 hi\u1ec7n t\u1ea1i n\u00f3 l\u00e0 m\u1ed9t ph\u1ea7n kh\u00f4ng th\u1ec3 thi\u1ebfu trong x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean\n\n- **TF: Term Frequency(T\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb) l\u00e0 s\u1ed1 l\u1ea7n t\u1eeb xu\u1ea5t hi\u1ec7n trong v\u0103n b\u1ea3n :**\n    - tf(t,d) \u0111\u1ea1i di\u1ec7n cho t\u1ea7n s\u1ed1 c\u1ee7a t\u1eeb t xu\u1ea5t hi\u1ec7n trong m\u1eabu d :\n    - f(t, d): S\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb t trong v\u0103n b\u1ea3n d\n    - max({f(w, d) : w \u2208 d}): S\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb c\u00f3 s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t trong v\u0103n b\u1ea3n d    \n     \n![](https:\/\/nguyenvanhieu.vn\/wp-content\/uploads\/2019\/01\/tf.png)     \n     \n     \n        \n        \n - **IDF: Inverse Document Frequency(Ngh\u1ecbch \u0111\u1ea3o t\u1ea7n su\u1ea5t c\u1ee7a v\u0103n b\u1ea3n), gi\u00fap \u0111\u00e1nh gi\u00e1 t\u1ea7m quan tr\u1ecdng c\u1ee7a m\u1ed9t t\u1eeb :**\n    - idf(t, D): gi\u00e1 tr\u1ecb idf c\u1ee7a t\u1eeb t trong t\u1eadp v\u0103n b\u1ea3n\n    - |D|: T\u1ed5ng s\u1ed1 v\u0103n b\u1ea3n trong t\u1eadp D\n    - |{d \u2208 D : t \u2208 d}|: th\u1ec3 hi\u1ec7n s\u1ed1 v\u0103n b\u1ea3n trong t\u1eadp D c\u00f3 ch\u1ee9a t\u1eeb t.\n    \n    \n![](https:\/\/nguyenvanhieu.vn\/wp-content\/uploads\/2019\/01\/idf.png)    \n\n\n\n- **Cosine Similarity (hay \u0111\u1ed9 t\u01b0\u01a1ng t\u1ef1 c\u1ee7a v\u0103n b\u1ea3n)** l\u00e0 qu\u00e1 tr\u00ecnh s\u1eed d\u1ee5ng s\u1ed1 li\u1ec7u d\u1ef1a tr\u00ean kho\u1ea3ng c\u00e1ch ho\u1eb7c \u0111\u1ed9 t\u01b0\u01a1ng t\u1ef1 c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng \u0111\u1ec3 x\u00e1c \u0111\u1ecbnh m\u1ee9c \u0111\u1ed9 t\u01b0\u01a1ng \u0111\u01b0\u01a1ng c\u1ee7a m\u1ed9t v\u0103n b\u1ea3n v\u1edbi b\u1ea5t k\u1ef3 v\u0103n b\u1ea3n n\u00e0o kh\u00e1c d\u1ef1a tr\u00ean c\u00e1c \u0111\u1eb7c tr\u01b0ng \u0111\u01b0\u1ee3c tr\u00edch xu\u1ea5t ra t\u1eeb **tf-idf**. V\u1ec1 c\u01a1 b\u1ea3n, kho\u1ea3ng c\u00e1ch cosin cung c\u1ea5p cho ch\u00fang ta m\u1ed9t s\u1ed1 li\u1ec7u bi\u1ec3u th\u1ecb g\u00f3c gi\u1eefa 2 vecto \u0111\u1eb7c tr\u01b0ng t\u01b0\u01a1ng \u1ee9ng c\u1ee7a t\u1eebng m\u1eabu. Khi g\u00f3c b\u1eb1ng 0 th\u00ec cosine = 1 khi \u0111\u00f3 th\u00ec \u0111\u1ed9 t\u01b0\u01a1ng \u0111\u1ed3ng l\u00e0 l\u1edbn nh\u1ea5t. \n    - Ta s\u1eed d\u1ee5ng h\u00e0m TfidfVectorizer c\u1ee7a th\u01b0 vi\u1ec7n sklearn \u0111\u1ec3 t\u1ea1o vector v\u00e0 t\u00ednh tf-idf cho 2 c\u1eb7p (search_term, product_title) v\u00e0 (search_term v\u00e0 product_description). Sau \u0111\u00f3 t\u00ednh ra 2 \u0111\u1eb7c tr\u01b0ng m\u1edbi l\u00e0 cosineSim_title_search v\u00e0 cosineSim_description.","963fd40e":"- Sau b\u01b0\u1edbc b\u00ean tr\u00ean, gi\u1edd attribute s\u1ebd c\u1ea7n mang d\u1eef li\u1ec7u t\u1eeb c\u1ea3 name v\u00e0 value, do \u0111\u00f3 ta c\u1ed9ng x\u00e2u l\u1ea1i.\n- Ta x\u00e2y d\u1ef1ng tr\u01b0\u1eddng attribute b\u1eb1ng vi\u1ec7c c\u1ed9ng x\u00e2u, v\u1edbi kho\u1ea3ng tr\u1eafng \u1edf gi\u1eefa, gi\u1eefa t\u00ean attribute v\u00e0 tr\u01b0\u1eddng value.","d17166a1":"## 4.4 T\u00ednh s\u1ed1 t\u1eeb v\u00e0 s\u1ed1 ch\u1eef trong description_product, search_term, product_title, attribute","c18a83dd":"- \u0110\u1eb7t v\u1ea5n \u0111\u1ec1 \n    -\n>     Home Depot l\u00e0 m\u1ed9t website b\u00e1n h\u00e0ng , kh\u00e1ch h\u00e0ng khi v\u00e0o web site c\u00f3 th\u1ec3 nh\u1eadp t\u00ean s\u1ea3n ph\u1ea9m mong mu\u1ed1n v\u00e0o \u00f4 search \u0111\u1ec3 t\u00ecm. B\u00e0i to\u00e1n \u0111\u1eb7t ra trong cu\u1ed9c thi \u0111\u00f3 ch\u00ednh l\u00e0 c\u1ea3i thi\u1ec7n tr\u1ea3i nghi\u1ec7m mua s\u1eafm c\u1ee7a kh\u00e1ch h\u00e0ng b\u1eb1ng c\u00e1ch ph\u00e1t tri\u00ean m\u1ed9t m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n v\u00e0 t\u1ed1i \u01b0u h\u00f3a \u0111\u1ea7u ra l\u00e0 `Search_relevance` \u0111\u1ec3 c\u00f3 th\u1ec3 g\u1ee3i \u00fd s\u1ea3n ph\u1ea9m sao cho li\u00ean quan nh\u1ea5t v\u1edbi nh\u1eefng g\u00ec kh\u00e1ch h\u00e0ng mong mu\u1ed1n t\u00ecm ki\u1ebfm nh\u1eb1m ti\u1ebft ki\u1ec7m th\u1eddi gian t\u00ecm ki\u1ebfm c\u1ee7a kh\u00e1ch h\u00e0ng","900ad677":"- Sau \u0111\u00f3 ta n\u1ed1i c\u00e1c d\u1eef li\u1ec7u kh\u00e1c nh\u01b0 descriptions , attribute , brand v\u00e0o `data_all`","5cf2ff79":"### 3.4.2 Lo\u1ea1i b\u1ecf c\u00e1c stopwords","ad94aa2f":"- Ta thu \u0111\u01b0\u1ee3c th\u00eam 3 \u0111\u1eb7c tr\u01b0ng m\u1edbi \u0111o l\u00e0 `search_term_tokens`, `product_title_tokens`, `product_description_tokens`","22691468":"### 2.2.1 Train_dataset","7a75b9a6":"- Nh\u01b0 v\u1eady vi\u1ec7c t\u1ea1o data m\u1edbi \u0111\u00e3 xong gi\u1edd ta s\u1ebd l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u \u0111\u1ec3 model c\u00f3 th\u1ec3 h\u1ecdc 1 c\u00e1ch t\u1ed1i \u01b0u","a3ee8f02":"### 2.2.4 Descriptions_dataset","e9556a9e":"- Xu\u1ea5t c\u00e1c gi\u00e1 tr\u1ecb d\u1ef1 \u0111o\u00e1n t\u1eadp test th\u00e0nh file submission.csv","1a26bb4f":"## 4.1 C\u00e1c d\u1eef li\u1ec7u d\u1ea1ng v\u0103n b\u1ea3n\n\n- T\u00e1ch c\u00e1c \u0111o\u1ea1n v\u0103n th\u00e0nh c\u00e1c t\u1eeb","033b181d":"- D\u00f9ng h\u00e0m get_jaccard_sim \u0111\u1ec3 sinh ra 2 \u0111\u1eb7c tr\u01b0ng m\u1edbi l\u00e0 jaccard simulator gi\u1eefa (search_term, product_title) v\u00e0 (search_term v\u00e0 product_description)","218aded6":"- T\u1eadp test c\u00f3 166693 h\u00e0ng v\u00e0 4 c\u1ed9t \n- Ch\u1ee9a c\u00e1c tr\u01b0\u1eddng `product_uid` , `product_tile` , `search_term` \n- D\u1ef1a v\u00e0o t\u1eadp test n\u00e0y \u0111\u1ec3 m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n `relevance`","0eecc124":"## 3.4 Clean d\u1eef li\u1ec7u","5c41c2f7":"- D\u00f9ng h\u00e0m chu\u1ea9n h\u00f3a \u0111\u00e3 \u0111\u1ecbnh ngh\u0129a \u1edf tr\u00ean v\u00e0 chu\u1ea9n h\u00f3a `all_data` chuy\u1ec3n h\u1ebft ch\u1eef hoa th\u00e0nh ch\u1eef th\u01b0\u1eddng, ch\u1eef th\u00e0nh s\u1ed1 ","4e82b558":"- T\u1eadp descriptions ch\u1ee9a 124428 h\u00e0ng v\u00e0 2 c\u1ed9t, g\u1ed3m c\u00e1c tr\u01b0\u1eddng `product_uid` v\u00e0 `product_description` \n- Tr\u01b0\u1eddng `product_description` \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 search n\u00ean tr\u01b0\u1eddng n\u00e0y kh\u00e1 quan tr\u1ecdng \n- Tr\u01b0\u1eddng m\u00f4 t\u1ea3 l\u00e0 text c\u00f3 ch\u1ee9a c\u1ea3 ch\u1eef s\u1ed1 , c\u00f3 c\u00e1c k\u00fd t\u1ef1 b\u1ecb vi\u1ebft t\u1eaft v\u00e0 \u0111\u1eb7c bi\u1ec7t kh\u00e1 d\u00e0i","40bc0953":"> Ta th\u1ea5y \u0111\u01b0\u1ee3c t\u1eadp Train v\u00e0 Test \u0111\u1ec1u c\u00f3 nh\u1eefng tr\u01b0\u1eddng gi\u1ed1ng nhau v\u00e0 ch\u1ec9 kh\u00e1c nhau \u1edf vi\u1ec7c Test kh\u00f4ng c\u00f3 tr\u01b0\u1eddng relevance \nDo \u0111\u00f3 ta gh\u00e9p ch\u00fang v\u1edbi nhau \u0111\u1ec3 d\u1ec5 d\u00e0ng x\u1eed l\u00fd "}}