{"cell_type":{"8552ba81":"code","9ff8cc12":"code","43d7405c":"code","718a9aa0":"code","3280668c":"code","e96c70ab":"code","2bc8d823":"code","c9d94203":"code","334aa523":"code","059b2fbb":"code","c13b9749":"code","2ce100ee":"code","1c91920d":"code","7da25a41":"code","b32bcc64":"code","5c9237a9":"code","493e0616":"code","5a473096":"code","517ec2bb":"code","ff22ec2b":"code","41be9c8c":"code","66403c63":"code","6f9dd8b7":"code","91f71295":"code","d9055739":"code","98d215cd":"code","d7300d8a":"code","e2cb32f1":"code","ad663f5d":"code","d25320d7":"code","1cf2f83e":"code","4f679130":"code","462e00c1":"code","9400e2ab":"code","197f1cc9":"code","e9d7d5c6":"code","48eb2962":"markdown","a466c0eb":"markdown","e1361bd8":"markdown","dcaa31fe":"markdown","30c37973":"markdown","521e7c16":"markdown","f20c1c68":"markdown","0332135c":"markdown","89d916fe":"markdown"},"source":{"8552ba81":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ff8cc12":"%matplotlib inline","43d7405c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.models import Model, load_model\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom keras.layers import Input, Embedding, SpatialDropout1D, Bidirectional, GRU, GlobalMaxPooling1D, Dense","718a9aa0":"#DATA_DIR = '..\/input\/jigsaw-toxic-comment-classification-challenge\/'\nFASTTEXT_FILE_PATH = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'","3280668c":"#!ls {DATA_DIR}","e96c70ab":"train_df = pd.read_csv('\/kaggle\/input\/brbndata-dir\/encoded_train.csv')\ntrain_df.head()","2bc8d823":"test_df = pd.read_csv('\/kaggle\/input\/brbndata-dir\/Test_BNBR.csv')\ntest_df.head()","c9d94203":"train_df.shape, test_df.shape","334aa523":"# Noise Removal (in train_df)\n# And conversion of upper_case text to Lower case\n#train_df['text'] = train_df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n#train_df['text'].head()","059b2fbb":"# Noise Removal (in test_df)\n# And conversion of upper_case text to Lower case\n#test_df['text'] = test_df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n#test_df['text'].head()","c13b9749":"#Remove punctuation(in train_df)\n#train_df['text'] = train_df['text'].str.replace('[^\\w\\s]','')\n#train_df['text'].head()","2ce100ee":"#Remove punctuation(in test_df)\n#test_df['text'] = test_df['text'].str.replace('[^\\w\\s]','')\n#test_df['text'].head()","1c91920d":"train_word_count = train_df['text'].str.split().apply(lambda x: len(x))\ntest_word_count = test_df['text'].str.split().apply(lambda x: len(x))","7da25a41":"train_word_count.hist(bins=10, rwidth=0.9)","b32bcc64":"test_word_count.hist(bins=10, rwidth=0.9)","5c9237a9":"maxlen = 175\nprint(train_word_count[train_word_count < maxlen].count()\/train_word_count.count())\nprint(test_word_count[test_word_count < maxlen].count()\/test_word_count.count())","493e0616":"train_df.isnull().sum()","5a473096":"test_df.isnull().sum()","517ec2bb":"max_features = 850\ntrain_text = train_df['text']\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(train_text.values)\n\ntrain_sequences = tokenizer.texts_to_sequences(train_text)\ntrain_data = pad_sequences(train_sequences, maxlen=maxlen)","ff22ec2b":"label_names = train_df.columns[2:].values; label_names","41be9c8c":"target = train_df[label_names]\ntarget.shape","66403c63":"val_count = 123\n\nx_val = train_data[:val_count]\nx_train = train_data[val_count:]\ny_val = target[:val_count]\ny_train = target[val_count:]","6f9dd8b7":"print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)","91f71295":"class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, batch_size=1024, verbose=1)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))","d9055739":"roc_auc = RocAucEvaluation(validation_data=(x_val, y_val), interval=1)","98d215cd":"class WordEmbeddingsProcessor:\n    \n    def __init__(self, file_path, max_features, emb_sz, toknzr):\n        self.file_path = file_path\n        self.max_features = max_features\n        self.emb_sz = emb_sz\n        self.toknzr = toknzr\n        self.embeddings_index = {}\n        \n    def generate_embeddings_index(self):\n        with open(self.file_path, encoding='utf8') as f:\n            for line in f:\n                values = line.rstrip().rsplit(' ')\n                word = values[0]\n                coefs = np.asarray(values[1:], dtype='float32')\n                self.embeddings_index[word] = coefs\n                \n    def get_embedding_matrix(self):\n        self.generate_embeddings_index()\n        \n        word_index = self.toknzr.word_index\n        num_words = min(self.max_features, len(word_index) + 1)\n        embedding_matrix = np.zeros((num_words, self.emb_sz))\n        for word, i in word_index.items():\n            if i >= self.max_features:\n                continue\n            embedding_vector = self.embeddings_index.get(word)\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n                \n        return embedding_matrix","d7300d8a":"emb_sz = 300\n\nfastTextProcessor = WordEmbeddingsProcessor(FASTTEXT_FILE_PATH,\n                                           max_features=max_features,\n                                           emb_sz=emb_sz,\n                                           toknzr=tokenizer)\nemb_matrix = fastTextProcessor.get_embedding_matrix()","e2cb32f1":"def build_model():\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, emb_sz, weights=[emb_matrix], trainable = False)(inp)\n    x = SpatialDropout1D(0.3)(x)\n    x = Bidirectional(GRU(128, dropout=0.3, recurrent_dropout=0.5,  return_sequences=True))(x)\n    x = GlobalMaxPooling1D()(x)\n    x = Dense(4, activation=\"sigmoid\")(x)\n    \n    model = Model(inputs=inp, outputs=x)\n    return model\n\n\nmodel = build_model()\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()","ad663f5d":"best_weights_path = 'weights_base.best.hdf5'\nval_loss_checkpoint = ModelCheckpoint(best_weights_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', patience=4)","d25320d7":"# takes too long to run in kernel environment, so I commented out this section.\n# Instead I will load pretrained weights.\n\nmodel.fit(x_train, y_train,\n          epochs=20,\n          batch_size=1024,\n          validation_data=(x_val, y_val),\n          callbacks=[roc_auc, val_loss_checkpoint, early_stop], verbose=1)","1cf2f83e":"# loading pretrained weights\n# make sure you comment out this section when you train the model\n# After running 19 epochs, I got validation loss of 0.03867 and ROC-AUC 0.989902 on validation set\n\n#pretrained_weights_path = '..\/input\/toxic-pretrained-gru-weights\/pretrained.best.hdf5'\n#model.load_weights(pretrained_weights_path)","4f679130":"val_preds = model.predict(x_val, batch_size=1024, verbose=1)","462e00c1":"roc_auc_score(y_val, val_preds)","9400e2ab":"test_sequences = tokenizer.texts_to_sequences(test_df['text'])\nx_test = pad_sequences(test_sequences, maxlen=maxlen)","197f1cc9":"# uncomment to make test set predictions\ntest_preds = model.predict(x_test, batch_size=1024, verbose=1)","e9d7d5c6":"# once you make test set predictions, uncomment this section to create submission file\n\nsub_df = pd.DataFrame(test_preds, columns=label_names)\nsub_df.insert(0, 'ID', test_df['ID'])\nsub_df.head()","48eb2962":"## Tokenize & train-valid split","a466c0eb":"## First look at data","e1361bd8":"## Predictions on test data","dcaa31fe":"## Submission","30c37973":" ## Get Pretrained Embeddings Matrix","521e7c16":"## Training","f20c1c68":"## RocAucEvaluation Callback","0332135c":"## Building model","89d916fe":"## Imports"}}