{"cell_type":{"05af709b":"code","48835577":"code","810019f9":"code","1092334d":"code","0ca07178":"code","bc26a3b9":"code","15eda949":"code","76a73b33":"code","74810f59":"code","f38dc1bf":"code","d908ad3b":"code","1aba86ed":"code","80c0a111":"code","9ce3910f":"code","15db7a98":"code","86e9de8b":"code","2e64de12":"code","47db56a3":"code","e6a58835":"code","9f1593aa":"code","f96de509":"code","bdafbd30":"code","847eabc8":"code","90b34118":"code","6c19bc39":"code","b52a87cd":"code","fd4e803b":"code","d83be218":"code","227b7c12":"code","1feae87c":"code","0065a814":"markdown","96acc14e":"markdown","e7eb9894":"markdown","e2abbec0":"markdown","77387f2f":"markdown","042d7079":"markdown","a4f35b8e":"markdown","862e0f17":"markdown","f6f6592b":"markdown","49c5b2f7":"markdown","de3ad551":"markdown","15abefc9":"markdown","f765a9fd":"markdown","570129ec":"markdown","4d114771":"markdown","0c10e6e2":"markdown","bd8b54f3":"markdown","5686de21":"markdown","d4f6414a":"markdown","69654f6d":"markdown","08a1e4ab":"markdown","06769fd8":"markdown","b1f9fc1a":"markdown","a2f3360d":"markdown"},"source":{"05af709b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import set_config\nfrom sklearn.metrics import mean_absolute_error\n\nfrom yellowbrick.regressor import ResidualsPlot\nfrom yellowbrick.regressor import PredictionError\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","48835577":"df = pd.read_csv('..\/input\/spotify-dataset-19212020-160k-tracks\/tracks.csv')\ndf.info()","810019f9":"df.head(10)","1092334d":"def encode_artist_popularity(df, num_quantiles=10):\n    \"\"\"\n    Select the main artist for each song (first name when list of artists)\n    Rank main artists by popularity \n    Replace the 'artists' column by quantiles of popularity for each song\n    Return a dataframe with 'artist_ranking' quantile instead of initial 'artists' names.\n    \n    \"\"\"\n    df['artists'] = df['artists'].apply(eval)\n    artists_dict = df.artists.to_dict()\n\n    for i in range(len(artists_dict)):\n        artists_dict[i] = artists_dict[i][0]\n    \n    df['artists'] = pd.DataFrame.from_dict(artists_dict, orient='index')\n    \n    artists_df = df[['artists', 'popularity']].groupby('artists').sum().sort_values(by='popularity', ascending=False)\n    artists_df['artist_ranking'] = pd.qcut(artists_df['popularity'], num_quantiles, labels=False)\n    \n    data_artists = artists_df.loc[df['artists']].reset_index()\n    df['artists'] = data_artists['artist_ranking']\n    \n    return df","0ca07178":"#df = encode_artist_popularity(df, 10)\n#df.head(10)","bc26a3b9":"df = df.sample(frac=1).reset_index(drop=True)\ndf[['popularity']].plot.hist(bins=100)","15eda949":"df.drop(df[df['popularity']<3].index, inplace=True)\ndf[['popularity']].plot.hist(bins=100)","76a73b33":"df.drop(df[(df['tempo']==0) & (df['speechiness']==0)].index, inplace=True)\ndf.drop(df[df['time_signature']<2].index, inplace=True)","74810f59":"X = df[['explicit', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']]\nX.describe()","f38dc1bf":"sns.pairplot(X)","d908ad3b":"y = df.popularity.values","1aba86ed":"X.loc[:, ['explicit', 'key', 'mode', 'time_signature']] = X.loc[:, ['explicit', 'key', 'mode', 'time_signature']].astype('category')","80c0a111":"numerical_columns_selector = selector(dtype_exclude='category')\ncategorical_columns_selector = selector(dtype_include='category')\n\nnumerical_columns = numerical_columns_selector(X)\ncategorical_columns = categorical_columns_selector(X)","9ce3910f":"numerical_columns","15db7a98":"categorical_columns","86e9de8b":"X.dtypes","2e64de12":"categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\nnumerical_preprocessor = StandardScaler()","47db56a3":"preprocessor = ColumnTransformer([\n    ('One-Hot-Encoder', categorical_preprocessor, categorical_columns),\n    ('Standard-Scaler', numerical_preprocessor, numerical_columns)])","e6a58835":"model = make_pipeline(preprocessor, LinearRegression())\nset_config(display='diagram')\nmodel","9f1593aa":"data_train, data_test, target_train, target_test = train_test_split(X, y, test_size=0.2)","f96de509":"_ = model.fit(data_train, target_train)","bdafbd30":"pred_train = model.predict(data_train)\npred_test = model.predict(data_test)","847eabc8":"predictions = pd.DataFrame(pred_test, columns=['predictions'])\npredictions.plot.hist(bins=100)","90b34118":"predictions.describe()","6c19bc39":"mean_absolute_error(target_train, pred_train)","b52a87cd":"mean_absolute_error(target_test, pred_test)","fd4e803b":"visualizer = ResidualsPlot(model, hist=True, qqplot=False)\nvisualizer.fit(data_train, target_train)\nvisualizer.score(data_test, target_test)\nvisualizer.show()","d83be218":"visualizer = PredictionError(model)\nvisualizer.fit(data_train, target_train)\nvisualizer.score(data_test, target_test)\nvisualizer.show()","227b7c12":"err = abs(target_test - pred_test)\nerror = pd.DataFrame(err, columns = ['error'])\nerror.plot.hist(bins=100)","1feae87c":"error.describe()","0065a814":"Let's plot residuals and observe their Gaussian distributions centered around 0, meaning that a larger portion of predictions are accurate. As expected for the test set, the distribution is more spread out. We can see some negative predicted values due to linear regression extrapolation.","96acc14e":"The purpose of the following function is to replace artists names by their popularity quantiles. It was developped for future usage only. This encoding method introduces the concept of artist bias when evaluating a song: a given song will generally be rated higher when the artist is famous.\n\nWe will not make use of this function due to lack of detailed information from the dataset author about rating methods and given the purpose of this study (predicting the popularity of a song, based on its acoustic features only).","e7eb9894":"It seems like a significant amount of songs were not rated, or very rarely\/poorly rated (no information about rating methods was provided). Consequently, our distribution appears to be skewed by these outliers. in order to preserve a more Gaussian-like distribution, let's get rid of them so we can learn relevant patterns from the rest of our data:","e2abbec0":"Let's define our model (default number of iteration for training = 1000 epochs) and let's visualize our pipeline architecture:","77387f2f":"Now let's define our X input matrix and y target vector:","042d7079":"The ColumnTransformer splits the columns of the original dataset based on column names, transforms each subset  by calling the predefined fit_transform and finally concatenates the transformed datasets into a single dataset for training. The same steps will be applied when calling the predict() method.","a4f35b8e":"# EDA","862e0f17":"# Training & prediction analysis","f6f6592b":"Making predictions on the train set and the test set, and getting to know our prediction distribution for the test set:","49c5b2f7":"Let's split the data into train and test sets with a 80\/20 ratio:","de3ad551":"Now let's define the transformers (preprocessors) for our sklearn pipeline: OneHotEncoder() for categorical columns and StandardScaler() for numerical columns.","15abefc9":"# Spotify Data Analysis: Song Popularity Predictions.","f765a9fd":"Let's use Seaborn capabilities to derive insights from feature correlation. The following plots show no significant correlation between features. Straight parallel lines belong to discrete\/categorical features (integer values) that will be processed as such later on.","570129ec":"Let's have a look at the absolute error in further detail. Mean is approx. 12 and standard deviation is approx. 8.","4d114771":"# Conclusion","0c10e6e2":"Now let's cast the categorical columns type as 'category' and then use the sklearn selector() method to create a list of numerical columns and a list of categorical columns:","bd8b54f3":"The dataset has 20 features (numerical and categorical) and has nearly 600 000 songs. Let's check for NaN values. The column 'name' is the only column with NaN values and it will be dropped later on as it is of no use for our model:","5686de21":"# Preprocessing","d4f6414a":"Some songs might have badly encoded\/skewed features: getting rid of songs that have 'tempo'==0 (which is technically impossible) and making sure these outliers are not simply spoken files by checking their 'speechiness'.\nAlso getting rid of songs which have a 'time_signature' of 0 or 1 (which is technically impossible).","69654f6d":"Performance metrics for training and test sets: the MAE metric is quite similar for both sets.","08a1e4ab":"Now let's visualize the predictions error and the R-squared metric. The latter is a ratio between the variance explained by the model and the total variance, e.g approximately 18% of the data can be explained by the model. Our data still contains a significant amount fo randomness. In addition, comparing the fitted line and the identity line shows that higher values of popularity (y) have lower prediction scores (y hat), e.g our model has a hard time predicting hits. This makes sense as the training data contains much fewer hit songs (see df['popularity'] distribution above).","06769fd8":"The objective of this notebook is to predict the popularity of a song using a 600k song dataset from Kaggle:\nhttps:\/\/www.kaggle.com\/yamaerenay\/spotify-dataset-19212020-160k-tracks\n\nWe will use a mix of categorical and numerical features from 'tracks.csv' to perform a multi-linear regression using Scikit-Learn Pipelines. The objective is to minimize the \"mean_absolute_error\" metric of a numerical target feature.","b1f9fc1a":"Let's shuffle our dataset and visualize the distribution of the 'popularity' target feature (100 bins for 0-100 ratings):","a2f3360d":"It is no surprise that music hits cannot be accurately predicted before they reach the market, art is also about randomness, right? However this model shows that nearly 20% of their acoustic features can be correlated to 'popularity'. In consequence, if I were the manager of a music production label, I wouldn't be able to use this model to predict the next big hit, but I would certainly use it to get a trend for future success of my next productions."}}