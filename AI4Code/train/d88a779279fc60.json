{"cell_type":{"a4af8423":"code","69b9ac74":"code","cc03b746":"code","c80ff91b":"code","869116d8":"code","f1833357":"code","7314b958":"code","059d6282":"code","fb4fd6fc":"code","5cbb47ac":"code","f44028ab":"code","e5142646":"code","21ca3e27":"code","328b8903":"code","8423fba1":"code","6ad89f70":"code","46e039d9":"code","e6dea44e":"code","6a6cf3fa":"code","30314ea4":"code","8621fb59":"code","35d03b23":"code","7f8eebfe":"code","bbb34fda":"code","041303ba":"code","1f220ecb":"code","12cba102":"code","d1fed615":"code","381a9a0b":"code","043ae1de":"code","bef86a5e":"code","c8f06d7e":"code","b2bcee3c":"code","66170314":"code","05faa764":"code","0d0b4b3e":"code","76b6bca2":"code","58018a3d":"markdown","fd394572":"markdown","f27f2b17":"markdown","37ab93e5":"markdown","b8bea6c6":"markdown","be3e0d2a":"markdown","1ec3c6a3":"markdown","3a628b4c":"markdown","7b08e83d":"markdown"},"source":{"a4af8423":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv', \n                    parse_dates=[\"date_time\"])\ntrain = train.set_index('date_time')\ntarget = train[['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']]\ntrain = train.drop(['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'], axis=1)\ntest = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv',\n                  parse_dates=[\"date_time\"])\ntest = test.set_index('date_time')","69b9ac74":"train.deg_C = train.deg_C + 273.15\ntest.deg_C = test.deg_C + 273.15","cc03b746":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(train, target, test_size=0.2)\ncolumn_names, tr_index, val_index = X_train.columns, X_train.index, X_val.index","c80ff91b":"from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='box-cox')\nX_train_sc = pd.DataFrame(data = pt.fit_transform(X_train), columns=column_names, \n                          index=tr_index)\nX_val_sc = pd.DataFrame(data = pt.transform(X_val), columns=column_names, \n                          index=val_index)\n\ntrain_sc = pd.DataFrame(data = pt.fit_transform(train), columns=column_names, \n                          index=train.index)\ntest_sc = pd.DataFrame(data = pt.transform(test), columns=column_names, \n                          index=test.index)\n\nfig = X_train_sc.hist(figsize=(100, 100), bins=30)\n[x.title.set_size(80) for x in fig.ravel()]\nplt.show()","869116d8":"from sklearn.preprocessing import MinMaxScaler\n\nminmax = MinMaxScaler()\nX_train_m = pd.DataFrame(data = minmax.fit_transform(X_train), columns=X_train.columns, \n                          index=X_train.index)\nX_val_m = pd.DataFrame(data = minmax.transform(X_val), columns=X_val.columns, \n                          index=X_val.index)\n\ntrain_m = pd.DataFrame(data = minmax.fit_transform(train), columns=train.columns, \n                          index=train.index)\ntest_m = pd.DataFrame(data = minmax.transform(test), columns=test.columns, \n                          index=test.index)\n\nfig = X_train_m.hist(figsize=(100, 100), bins=30)\n[x.title.set_size(80) for x in fig.ravel()]\n[x.tick_params(axis='both', which='major', labelsize=80) for x in fig.ravel()]\nplt.show()","f1833357":"y_train_log = np.log1p(y_train)\ny_val_log = np.log1p(y_val)","7314b958":"fig = y_train_log.hist(figsize=(100, 100), bins=30)\n[x.title.set_size(80) for x in fig.ravel()]\nplt.show()","059d6282":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nimport keras.backend as K\nfrom keras.callbacks import EarlyStopping\n\nDROPOUT = 0.3\nNSIZE = 512\n\nwith tf.device('\/gpu:0'):\n    tf.random.set_seed(0)\n    model = keras.Sequential([\n        layers.BatchNormalization(input_shape=[X_train.shape[1]]),\n        \n        layers.Dense(4096, activation='relu'),\n        layers.Reshape((256,-1)),\n        \n        layers.Conv1D(filters=512, kernel_size=5, strides=1, padding='same',\n                    data_format='channels_first', groups=16,\n                    activation='relu'),\n        layers.AveragePooling1D(pool_size=2,\n                    strides=1, padding='same'),\n        \n        layers.Conv1D(filters=16, kernel_size=5, strides=1, padding='same',\n                    data_format='channels_last',\n                    activation='relu'),\n        \n        layers.MaxPooling1D(pool_size=2,\n                    strides=2, padding='same', data_format='channels_first'),\n        \n        layers.Flatten(),\n        \n        layers.Dense(NSIZE),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.Dropout(DROPOUT),\n        \n        layers.Dense(NSIZE),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.Dropout(DROPOUT),\n        \n        layers.Dense(3, activation='relu')\n    ])\n\n    def RMSLE(y_true, y_pred):\n        msle = keras.losses.MeanSquaredLogarithmicError()\n        return K.sqrt(msle(y_true, y_pred))\n\n    model.compile(\n        optimizer='adam',\n        loss='MeanSquaredLogarithmicError',\n        metrics=[RMSLE]\n    )\n\n    earlyStopping = EarlyStopping(\n        min_delta=0.0001, \n        patience=15, \n        verbose=1,\n        restore_best_weights=True\n    )\n\n    history = model.fit(X_train_sc, y_train, validation_data=(X_val_sc, y_val),\n             epochs=400,\n             batch_size=256,\n             callbacks=[earlyStopping]\n    )","fb4fd6fc":"history_df = pd.DataFrame(history.history)\nfig, ax = plt.subplots(2, figsize=(50,20))\nhistory_df.loc[:,['loss', 'val_loss']].plot(ax = ax[0])\nhistory_df.loc[:,['RMSLE', 'val_RMSLE']].plot(ax = ax[1])\nfor i in range(2):\n    ax[i].title.set_fontsize(30)\n    ax[i].legend(fontsize=30)\n    ax[i].tick_params(axis='both', which='major', labelsize=30)\nplt.show()","5cbb47ac":"from sklearn.metrics import mean_squared_log_error\n\npreds = model.predict(X_val_sc)\nerr = np.sqrt(mean_squared_log_error(y_val, preds))\nprint('Validation error:', '{0:.4f}'.format(err))","f44028ab":"from random import sample, seed\n\nseed(0)\ntrain_len = X_train_sc.shape[0]\nPARTITIONS = 3\nsampling = np.array(sample(range(train_len), train_len)).reshape((PARTITIONS,-1))\nsampling = X_train_sc.index[sampling]\n\nX_train_sc_part = {}\ny_train_part = {}\nfor i in range(PARTITIONS):\n    X_train_sc_part[i] = X_train_sc[X_train_sc.index.isin(sampling[i])]\n    y_train_part[i] = y_train[y_train.index.isin(sampling[i])]","e5142646":"models = {}\nfor i in range(PARTITIONS):\n    models[i] = keras.models.clone_model(model)\n    models[i].compile(\n            optimizer='adam',\n            loss='MeanSquaredLogarithmicError',\n            metrics=[RMSLE, 'MeanSquaredError']\n        )","21ca3e27":"history = {}\nclass myCallback(keras.callbacks.Callback):\n    def __init__(self, partition):\n        super().__init__()\n        self.curr_part = partition\n        \n    def on_train_begin(self, logs=None):\n        print('Currently on partition:', self.curr_part)\n        \n    def on_train_end(self, logs=None):\n        print('Loss:', logs.get('loss'), 'val_loss:', logs.get('val_loss'))\n        \n        \nwith tf.device('\/gpu:0'):\n    tf.random.set_seed(0)\n    for i in range(PARTITIONS):\n        callback = myCallback(i)\n        history[i] = models[i].fit(X_train_sc_part[i], y_train_part[i],\n                 validation_data=(X_val_sc, y_val),\n                 epochs=400,\n                 batch_size=128, verbose=0,\n                 callbacks=[earlyStopping, callback]\n        )","328b8903":"fig, ax = plt.subplots(PARTITIONS,3, figsize=(50,20))\nfor j in range(PARTITIONS):\n    history_df = pd.DataFrame(history[j].history)\n    history_df.loc[:,['loss', 'val_loss']].plot(ax = ax[j,0])\n    history_df.loc[:,['RMSLE', 'val_RMSLE']].plot(ax = ax[j,1])\n    history_df.loc[:,['mean_squared_error', 'val_mean_squared_error']].plot(ax = ax[j,2])\n    for i in range(3):\n        ax[j,i].title.set_fontsize(30)\n        ax[j,i].legend(fontsize=30)\n        ax[j,i].tick_params(axis='both', which='major', labelsize=30)\nplt.show()","8423fba1":"preds = np.zeros(y_val.shape)\nfor i in range(PARTITIONS):\n    preds += models[i].predict(X_val_sc)\npreds = preds \/ PARTITIONS\nerr = np.sqrt(mean_squared_log_error(y_val, preds))\nprint('Validation error:', '{0:.4f}'.format(err))","6ad89f70":"preds = model.predict(test_sc)\npreds = pd.DataFrame(data=preds, columns=target.columns, index=test_sc.index)","46e039d9":"## Ensemble\npreds = np.zeros((test_sc.shape[0],3))\nfor i in range(PARTITIONS):\n    preds += models[i].predict(test_sc)\npreds = preds \/ PARTITIONS\npreds = pd.DataFrame(data=preds, columns=target.columns, index=test_sc.index)","e6dea44e":"fig, ax = plt.subplots(3,1, figsize=(50,20))\ntarget.target_benzene.plot(ax=ax[0])\npreds.target_benzene.plot(ax=ax[0])\n\ntarget.target_carbon_monoxide.plot(ax=ax[1])\npreds.target_carbon_monoxide.plot(ax=ax[1])\n\ntarget.target_nitrogen_oxides.plot(ax=ax[2])\npreds.target_nitrogen_oxides.plot(ax=ax[2])\nplt.show()","6a6cf3fa":"preds.reset_index().to_csv('submission.csv', index=False)","30314ea4":"# Basic ResNet Building Block\ndef resnet_layer(inputs, nsize, dropout, activation):\n    dense = layers.Dense(nsize)\n    batch = layers.BatchNormalization()\n    drop = layers.Dropout(dropout)\n    act = layers.Activation(activation)\n  \n    x = inputs\n    x = dense(x)\n    x = batch(x)\n    x = drop(x)\n    if activation is not None:\n        x = act(x)\n    return x","8621fb59":"# ResNet Model\ndef resnet(input_shape, depth, nsize, dropout, activation):\n      \n    if (depth - 2) % 9 != 0:\n        raise ValueError('depth should be 9n + 2 (eg 56 or 110 in [b])')\n    # Start model definition.\n    num_res_blocks = int((depth - 2) \/ 9)\n  \n    inputs = layers.Input(shape = input_shape)\n    x = resnet_layer(inputs=inputs, nsize=nsize, dropout=dropout, activation=activation)\n    # Instantiate the stack of residual units\n    for stack in range(3):\n        for res_block in range(num_res_blocks):           \n            y = resnet_layer(inputs=x, nsize=nsize, dropout=dropout, activation=activation)\n            y = resnet_layer(inputs=y, nsize=nsize, dropout=dropout, activation=activation)\n            y = resnet_layer(inputs=y, nsize=nsize, dropout=dropout, activation=None)\n            x = keras.layers.add([x, y])\n            x = layers.Activation(activation)(x)\n  \n    # Add regressor on top.\n    outputs = layers.Dense(3, activation ='relu')(x)\n  \n    # Instantiate model.\n    model = keras.models.Model(inputs = inputs, outputs = outputs)\n    return model","35d03b23":"tf.random.set_seed(0)\nresNetModel = resnet([X_train_sc.shape[1]], 110, 512, 0.2, 'relu')\n\nresNetModel.compile(\n    optimizer='adam',\n    loss='MeanSquaredLogarithmicError',\n    metrics=[RMSLE]\n)","7f8eebfe":"with tf.device('\/gpu:0'):\n    history = resNetModel.fit(X_train_sc, y_train, validation_data=(X_val_sc, y_val),\n             epochs=400,\n             batch_size=256,\n             callbacks=[earlyStopping]\n    )","bbb34fda":"fig, ax = plt.subplots(2, figsize=(50,20))\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:,['loss', 'val_loss']].plot(ax = ax[0])\nhistory_df.loc[:,['RMSLE', 'val_RMSLE']].plot(ax = ax[1])\nfor i in range(2):\n    ax[i].title.set_fontsize(30)\n    ax[i].legend(fontsize=30)\n    ax[i].tick_params(axis='both', which='major', labelsize=30)\nplt.show()","041303ba":"preds = resNetModel.predict(X_val_sc)\nerr = np.sqrt(mean_squared_log_error(y_val, preds))\nprint('Validation error:', '{0:.4f}'.format(err))","1f220ecb":"preds = resNetModel.predict(test_sc)\npreds = pd.DataFrame(data=preds, columns=target.columns, index=test_sc.index)","12cba102":"fig, ax = plt.subplots(3,1, figsize=(50,20))\ntarget.target_benzene.plot(ax=ax[0])\npreds.target_benzene.plot(ax=ax[0])\n\ntarget.target_carbon_monoxide.plot(ax=ax[1])\npreds.target_carbon_monoxide.plot(ax=ax[1])\n\ntarget.target_nitrogen_oxides.plot(ax=ax[2])\npreds.target_nitrogen_oxides.plot(ax=ax[2])\nplt.show()","d1fed615":"preds.reset_index().to_csv('submission.csv', index=False)","381a9a0b":"# Basic ResNet Building Block\ndef resnet_layer_conv(inputs, num_filters = 16, kernel_size = 3, strides = 1, \n                      activation ='relu', batch_normalization = True):\n    \n    conv = layers.Conv1D(num_filters,\n                  kernel_size = kernel_size,\n                  strides = strides,\n                  padding ='same',\n                  kernel_initializer ='he_normal',\n                  kernel_regularizer = keras.regularizers.l2(1e-4))\n    x = inputs\n    if batch_normalization:\n        x = layers.BatchNormalization()(x)\n        x = layers.Dropout(0.2)(x)\n    if activation is not None:\n        x = layers.Activation(activation)(x)\n    x = conv(x)\n    return x","043ae1de":"# ResNet Model\ndef resnetConv(input_shape, depth, activation):\n      \n    if (depth - 2) % 9 != 0:\n        raise ValueError('depth should be 9n + 2 (eg 56 or 110 in [b])')\n    # Start model definition.\n    num_filters_in = 16\n    num_res_blocks = int((depth - 2) \/ 9)\n  \n    inputs = layers.Input(shape = input_shape)\n    x = layers.BatchNormalization(input_shape=input_shape)(inputs)\n    x = layers.Dense(4096, activation='relu')(x)\n    x = layers.Reshape((256,-1))(x)\n        \n    x = resnet_layer_conv(inputs = x)\n    # Instantiate the stack of residual units\n    for stage in range(3):\n        for res_block in range(num_res_blocks):           \n            strides = 1\n            if stage == 0:\n                num_filters_out = num_filters_in * 4\n                if res_block == 0:  # first layer and first stage\n                    activation = None\n                    batch_normalization = False\n            else:\n                num_filters_out = num_filters_in * 2\n                if res_block == 0:  # first layer but not first stage\n                    strides = 2    # downsample\n  \n            # bottleneck residual unit\n            y = resnet_layer_conv(inputs = x,\n                             num_filters = num_filters_in,\n                             kernel_size = 1,\n                             strides = strides,\n                             activation = activation,\n                             batch_normalization = batch_normalization)\n            y = resnet_layer_conv(inputs = y,\n                             num_filters = num_filters_in)\n            y = resnet_layer_conv(inputs = y,\n                             num_filters = num_filters_out,\n                             kernel_size = 1)\n            if res_block == 0:\n                # linear projection residual shortcut connection to match\n                # changed dims\n                x = resnet_layer_conv(inputs = x,\n                                 num_filters = num_filters_out,\n                                 kernel_size = 1,\n                                 strides = strides,\n                                 activation = None,\n                                 batch_normalization = False)\n            x = keras.layers.add([x, y])\n  \n    # Add regressor on top.\n    x = layers.MaxPooling1D(pool_size = 8)(x)\n    x = layers.Flatten()(x)\n    outputs = layers.Dense(3, activation ='relu')(x)\n  \n    # Instantiate model.\n    model = keras.models.Model(inputs = inputs, outputs = outputs)\n    return model","bef86a5e":"tf.random.set_seed(0)\nresNetModelConv = resnetConv([X_train_sc.shape[1]], 110, 'relu')\n\nresNetModelConv.compile(\n    optimizer='adam',\n    loss='MeanSquaredLogarithmicError',\n    metrics=[RMSLE]\n)","c8f06d7e":"with tf.device('\/gpu:0'):\n    history = resNetModelConv.fit(X_train_sc, y_train, validation_data=(X_val_sc, y_val),\n             epochs=400,\n             batch_size=256,\n             callbacks=[earlyStopping]\n    )","b2bcee3c":"fig, ax = plt.subplots(2, figsize=(50,20))\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:,['loss', 'val_loss']].plot(ax = ax[0])\nhistory_df.loc[:,['RMSLE', 'val_RMSLE']].plot(ax = ax[1])\nfor i in range(2):\n    ax[i].title.set_fontsize(30)\n    ax[i].legend(fontsize=30)\n    ax[i].tick_params(axis='both', which='major', labelsize=30)\nplt.show()","66170314":"preds = resNetModelConv.predict(X_val_sc)\nerr = np.sqrt(mean_squared_log_error(y_val, preds))\nprint('Validation error:', '{0:.4f}'.format(err))","05faa764":"preds = resNetModelConv.predict(test_sc)\npreds = pd.DataFrame(data=preds, columns=target.columns, index=test_sc.index)","0d0b4b3e":"fig, ax = plt.subplots(3,1, figsize=(50,20))\ntarget.target_benzene.plot(ax=ax[0])\npreds.target_benzene.plot(ax=ax[0])\n\ntarget.target_carbon_monoxide.plot(ax=ax[1])\npreds.target_carbon_monoxide.plot(ax=ax[1])\n\ntarget.target_nitrogen_oxides.plot(ax=ax[2])\npreds.target_nitrogen_oxides.plot(ax=ax[2])\nplt.show()","76b6bca2":"preds.reset_index().to_csv('submission.csv', index=False)","58018a3d":"## ResNet Convolutional","fd394572":"## ResNet version","f27f2b17":"## Ensemble version\n\nWe partition the data into three equal datasets and train the same model three times.","37ab93e5":"## Neural Network","b8bea6c6":"In order for the deg_C column to be positive I just convert the value to Kelvin.","be3e0d2a":"## Preprocessing\n\nI try two types of standardization, the box-cox transformation and the MinMax scaling to the interval $[0,1]$. It seems that the box-cox is the better in the models.","1ec3c6a3":"It seems that the target is skewed so I also try the logarithmic transformation. However it doesn't change very much the result.","3a628b4c":"# Tabular Playground\n## Data loading and preprocessing\n\nFollowing the same steps as the other notebook I will standardize the data.","7b08e83d":"## Prediction\n\nIt isn't my best."}}