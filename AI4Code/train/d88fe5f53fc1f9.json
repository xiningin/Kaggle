{"cell_type":{"84a055b8":"code","23bd67b8":"code","2dc92977":"code","22dd7e0c":"code","e2dcf0d0":"code","33caa28f":"code","89f12244":"code","d238703a":"code","c452e187":"code","53cd28bf":"code","9d7c94be":"code","2fad4d64":"code","10f0c10b":"code","640286c7":"code","947d0c68":"code","86c59f48":"code","d4c1e763":"code","b249dadc":"code","7c815742":"code","3e8017c8":"markdown","53411f65":"markdown","95b2f708":"markdown","a2609dce":"markdown","75823061":"markdown"},"source":{"84a055b8":"!pip install fastai2 ","23bd67b8":"\n## Kaggle paths ##########################################\n\npath = \"\/kaggle\/input\/m5-forecasting-accuracy\"\npath_fe =\"\/kaggle\/input\/m5-simple-fe-eval\"\npath_lag =\"\/kaggle\/input\/m5-lags-features-eval\"\npath_o = '..\/output\/tabular_pred'\n\n#PATHS for Features\nORIGINAL = path\n\n\n#test_path  = f'{path}\/dm_files'\nBASE     = f'{path_fe}\/grid_part_1.pkl'\nPRICE    = f'{path_fe}\/grid_part_2.pkl'\nCALENDAR = f'{path_fe}\/grid_part_3.pkl'\nLAGS     = f'{path_lag}\/lags_df_28.pkl'\nCS   = f'{path_lag}\/cumsum.pkl'\n\n#########################################################\n\n# AUX(pretrained) Models paths\nAUX_MODELS = path","2dc92977":"%reload_ext autoreload\n%autoreload 2","22dd7e0c":"import fastai2\nfrom fastai2.tabular.all import *\nfrom fastai2.basics import *\nfrom fastai2.callback.all import *\n\n# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, time, warnings, pickle, psutil, random\n\n# custom imports\nfrom multiprocessing import Pool        # Multiprocess Runs\n\nwarnings.filterwarnings('ignore')","e2dcf0d0":"def seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n","33caa28f":"# Read data\ndef get_data_by_store(store, START_TRAIN):\n    print('Start train at Day ', START_TRAIN, '; End train at Day ', END_TRAIN)\n    # Read and contact basic feature\n    df = pd.concat([pd.read_pickle(BASE),\n                    pd.read_pickle(PRICE).iloc[:,2:],\n                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n                    axis=1)\n    \n    # Leave only relevant store\n    df = df[df['store_id']==store]\n    df = df[df['d']>=START_TRAIN]\n\n\n\n    # keep 'd' to select by days\n    lag_feat = ['sales_lag_28', 'sales_lag_56', 'sales_lag_84', 'sales_lag_168', 'sales_lag_364',\n              'roll_mean_lag_28_7', 'roll_mean_lag_28_14', 'roll_mean_lag_28_28', \n            'roll_mean_lag_56_7', 'roll_mean_lag_56_14', 'roll_mean_lag_56_28', \n            'roll_mean_lag_84_7', 'roll_mean_lag_84_14', 'roll_mean_lag_84_28', \n            'roll_mean_lag_168_7', 'roll_mean_lag_168_14', 'roll_mean_lag_168_28',\n            'roll_mean_lag_364_7', 'roll_mean_lag_364_14', 'roll_mean_lag_364_28',\n            ]\n\n    #df3 = pd.read_pickle(LAGS).iloc[:, 3:]\n    df3 = pd.read_pickle(LAGS)[lag_feat]\n    df3 = df3[df3.index.isin(df.index)]\n\n  \n    df = pd.concat([df, df3], axis=1)\n    del df3 # to not reach memory limit \n    gc.collect()\n    \n    \n    cum_feat = [ 'sales_lag_28_cum']  # 'price_sales',\n    df4 = pd.read_pickle(CS)[cum_feat]\n    #df4 = pd.read_pickle(CS).iloc[:, 3:]\n    df4 = df4[df4.index.isin(df.index)]\n\n  \n    df = pd.concat([df, df4], axis=1)\n    del df4 # to not reach memory limit \n    gc.collect()\n    \n\n    # Create features list\n    features = [col for col in list(df) if col not in remove_features]\n    #df = df[['id','d',TARGET]+features]\n    df = df[features]\n    \n    # Skipping first n rows\n    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n    \n    return df, features","89f12244":"def create_dbunch(train_df):\n    global df_test, submit_store, y_max\n\n    dep_var = TARGET\n\n\n    print('max of sales_lag_28_cum=',  train_df.sales_lag_28_cum.max() ) # train_df.ps_lag_1.max()\n\n\n    feats = list(train_df)\n    for feat in feats:\n      if '_lag_' in feat:\n        train_df[feat]=train_df[feat].fillna(0.0)  #inplace can't work\n\n    #train_df['log_sales']= np.log1p(train_df.sales.values)  #convert to log then can fit\n    #train_df[TARGET]= train_df.sales.values  #try no Log\n    train_df[TARGET]= train_df['sales']*train_df['sell_price']  #tgt=price*sales\n    \n    print('max, Target; max, min Sales ',train_df[TARGET].max(), train_df['sales'].max(), train_df['sales'].min())\n    y_max = 1.1 * train_df[TARGET].max()\n    #print('train_df null=', train_df.isnull().sum())\n\n    all_vars = train_df.columns.tolist()\n    all_vars.remove(dep_var)\n    #all_vars.remove('weekday')\n\n    cat_vars = ['item_id', 'dept_id', 'cat_id', 'd', 'price_nunique', 'item_nunique',  \n                'event_name_1', 'event_name_2', 'event', 'snap_CA', 'snap_TX', 'snap_WI', 'release',\n                  'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end',  ]\n\n    xtra_vars = ['sales', 'id' ] # move 'id' here, else embedding ERROR\n    cont_vars = [col for col in all_vars if col not in cat_vars+xtra_vars]\n\n\n    train_mask = train_df['d']<= END_TRAIN  # all data b4 end of trg set\n    df = train_df[train_mask][cat_vars + cont_vars + [dep_var]].copy()\n\n    preds_mask = train_df['d']> (START_PRED - 400) # need 364+28days b4 predict to calc lag_roll\n    df_test = train_df[preds_mask][cat_vars + cont_vars + [dep_var]].copy()  #add Target to test for recursive predict\n\n    submit_mask = train_df['d']== START_PRED  #mask 1-store all 3049 products, for only 1 day\n    submit_store = train_df[submit_mask][['id']].copy()\n\n    #procs=[FillMissing, Categorify, Normalize]\n    procs=[Categorify, Normalize]\n\n    cut = df['d'][(df['d'] >= (END_TRAIN - P_HORIZON) )].index.min() # find smallest index\n    last = df['d'][(df['d'] == END_TRAIN )].index.max() #find biggest trg index\n\n    valid_idx = list(range(cut, last))  \n    print(cut, last)\n    #print (valid_idx)\n\n\n    dls = TabularDataLoaders.from_df(df, path=path, procs=procs, cat_names=cat_vars, cont_names=cont_vars, \n                   y_names=TARGET, valid_idx=valid_idx, bs=2048)\n\n    dls.show_batch()\n    return dls","d238703a":"def create_learner(dls):\n     \n    #y_max = 800.0  #use 1.2*y_max\n    print('set y_max at ', y_max)\n\n    learn = tabular_learner(dls,  loss_func= nn.PoissonNLLLoss(log_input=False), layers=[500, 100], ps=[0.001, 0.01], \n                            emb_drop=0.04, y_range=[0.0, y_max], path=path_o) #define path for Kaggle \n    \n    \n    print('Loss fn= ', learn.loss_func)\n    #learn.model\n\n    learn.lr_find(end_lr=8)\n    learn.fit_one_cycle(10, max_lr=5e-3, wd=0.01,  cbs=SaveModelCallback() ) \n    print('show sample result:')\n    learn.show_results()\n      \n    return learn","c452e187":"def predict_store(learn):\n    global preds, tgt\n\n    dl = learn.dls.test_dl(df_test) #can provide Tgt y or not\n    preds, tgt = learn.get_preds(dl=dl)\n    #test_preds = (np.expm1(preds)).numpy().squeeze()  #inv log(x)-1.0\n    test_preds = preds.numpy().squeeze() \n    \n    #df_test['sales_p']=test_preds\n    df_test['pricesales_p']=test_preds\n    df_test['sales_p']= df_test['pricesales_p'] \/ df_test['sell_price']\n\n    for day_id in range(1, 29):\n    #for day_id in range(29, 57):\n      submit_mask = df_test['d']== (START_PRED -1 + day_id) # 1942 - 1\n      submit_store[f'F{day_id}'] = df_test[submit_mask][['sales_p']].values\n\n    submit_store.to_pickle(f'{path_o}\/{store_id}_pred.pkl')\n\n    return","53cd28bf":"VER = 1                          # Our model version\nSEED = 42                        # We want all things\nseed_everything(SEED)            # to be as deterministic \n#lgb_params['seed'] = SEED        # as possible\nN_CORES = psutil.cpu_count()     # Available CPU cores\n\n#TARGET      = 'sales'            # Our target 'sales'\nTARGET      = 'pricesales'\n\n#remove_features = ['id','state_id','store_id', 'date','wm_yr_wk','d', TARGET]\nremove_features = ['state_id','store_id', 'date', 'wm_yr_wk', ]\n                        \n\n#STORES_IDS = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\nSTORES_IDS = ['CA_1',  ]\n\n\n#LIMITS and const\nSTART_TRAIN = 1200                  # We can skip some rows (Nans\/faster training)\nEND_TRAIN   = 1941               # End day of our train set\nSTART_PRED  = 1942        # sid --> Decouple start_pred & end_train; Gap !!\nP_HORIZON   = 28                 # Prediction horizon\nUSE_AUX     = False               # Use or not pretrained models\n\n","9d7c94be":"for store_id in STORES_IDS:\n    print('Training Store ', store_id)\n    \n    # Get grid for current store\n    grid_df, features_columns = get_data_by_store(store_id, START_TRAIN)\n    \n    \n    ## Create databunch\n    dbunch = create_dbunch(grid_df)\n\n    del grid_df\n    gc.collect()\n\n    # Launch seeder again to make training 100% deterministic\n    seed_everything(SEED)\n\n    learner = create_learner(dbunch)\n\n    predict_store(learner)\n      ","2fad4d64":"#df_test\ndf_test[(df_test['d']>=1907) & (df_test['item_id']=='FOODS_3_827')]  #.iloc[:, 20:]","10f0c10b":"tst_feat = list(df_test)\nlen(tst_feat), tst_feat","640286c7":"#path_o = f'{path}\/tabular_pred'\nall_preds = pd.DataFrame()\n\nfor store_id in STORES_IDS:\n  temp_df = pd.read_pickle(f'{path_o}\/{store_id}_pred.pkl')\n  if 'id' in list(all_preds):\n    #all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n    all_preds = pd.concat([all_preds, temp_df], axis=0, sort=False)\n  else:\n    all_preds = temp_df.copy()\n\n  del temp_df\n    \nall_preds = all_preds.reset_index(drop=True)\nall_preds","947d0c68":"feats = list(all_preds)\nfeats.remove('id')\nfor feat in feats :\n    all_preds[feat] = np.round(all_preds[feat].values * 1.00, 4)    ","86c59f48":"sample = pd.read_csv(ORIGINAL+'\/sample_submission.csv')\nsubm_eval = sample[sample['id'].str.contains(\"validation\")].copy()  #validation is now dummy\nsubmission = pd.concat([subm_eval, all_preds ], axis=0, sort=False) \nsubmission.to_csv('submission.csv', index=False)\n","d4c1e763":"submission.id.nunique()","b249dadc":"# no rows = 3049 x # stores + 30490 dummy stores\nsubmission","7c815742":"submission[submission['id'].str.contains(\"evaluation\")]","3e8017c8":"# Train Models + Predict by Store (n*lag_28)","53411f65":"# magic number 1.00 (None)","95b2f708":"# Submission","a2609dce":"# Run for start_train=700, 950, 1200; 1350, 1500 then Average\n## Loss fn = Poisson; Magic No *1.00 (None)","75823061":"# Create fast.ai databunch + Learner"}}