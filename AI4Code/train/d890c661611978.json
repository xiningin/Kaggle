{"cell_type":{"aab3ed39":"code","0ab43414":"code","d18001e0":"code","4308d389":"code","2b641675":"code","4d807576":"code","1897de5c":"code","8383d645":"code","dafe0a30":"code","a12f96b4":"code","a5fc5862":"code","1592b712":"code","74958d02":"code","762a7d34":"code","506a6c3b":"code","2b12f150":"code","a9b69cb1":"code","cf0b025b":"code","ea0a5669":"code","15c52b8d":"code","c31057e8":"code","68f1b148":"code","e4fbd9b5":"code","c02731fc":"code","7bab8947":"code","47ecfbce":"code","b8d126bc":"code","9d94dac3":"code","70b54e4c":"code","dc9a191e":"code","4002607f":"code","3e07c155":"code","001196e3":"code","3d0f51fd":"code","24720f7a":"code","e73323f1":"markdown","015c2ea0":"markdown","9b474c37":"markdown","b01ae7c8":"markdown"},"source":{"aab3ed39":"import pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","0ab43414":"df = pd.read_csv('..\/input\/train.tsv',sep=\"\\t\")","d18001e0":"df.head()","4308d389":"df.set_index('train_id',inplace=True)","2b641675":"# quick check on missing data\ndf_na = (df.isnull().sum() \/ len(df)) * 100\ndf_na = df_na.drop(df_na[df_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :df_na})\nmissing_data.head(20)","4d807576":"len(df[df['price'] == 0])","1897de5c":"df = df[df['price']>0]","8383d645":"# split target and features\ny = df['price']\ndf = df.drop('price',axis=1)","dafe0a30":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom scipy.stats import norm\n\nsns.distplot(y , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(y)\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Price distribution')\nplt.show()","a12f96b4":"import numpy as np\ny_log = np.log1p(y)","a5fc5862":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom scipy.stats import norm\n\nsns.distplot(y_log , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(y_log)\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Price distribution')\nplt.show()","1592b712":"df.dtypes","74958d02":"# replace missing values with 'missing'\ndf['brand_name'] = df['brand_name'].fillna('missing')\ndf['category_name'] = df['category_name'].fillna('missing')\ndf['item_description'] = df['item_description'].fillna('missing')","762a7d34":"df.head()","506a6c3b":"# changing the column types for categorical features\ndf['category_name'] = df['category_name'].astype('category')\ndf['brand_name'] = df['brand_name'].astype('category')\ndf['item_condition_id'] = df['item_condition_id'].astype('category')","2b12f150":"# clean up text based features before tf-idf \ndef clean_text(col):\n    # remove non alpha characters\n    col = col.str.replace(\"[\\W]\", \" \") #a-zA-Z1234567890\n    # all lowercase\n    col = col.apply(lambda x: x.lower())\n    return col\n\ndf['name']=clean_text(df['name'])\ndf['category_name']=clean_text(df['category_name'])\ndf['item_description']=clean_text(df['item_description'])","a9b69cb1":"df.head()","cf0b025b":"# create feature matrix for name and category_name\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(min_df=10,max_df=0.1, stop_words='english')\nX_name = cv.fit_transform(df['name'])\ncv = CountVectorizer()\nX_category = cv.fit_transform(df['category_name'])","ea0a5669":"X_name.shape","15c52b8d":"X_category.shape","c31057e8":"# Feature matrix for item description\ncv = CountVectorizer(min_df=10,max_df=0.1, stop_words='english')\nX_item_description = cv.fit_transform(df['item_description'])","68f1b148":"X_item_description.shape","e4fbd9b5":"# feature matrix for brand\nfrom sklearn.preprocessing import LabelBinarizer\nlb = LabelBinarizer(sparse_output=True)\nX_brand = lb.fit_transform(df['brand_name'])\n","c02731fc":"X_brand.shape","7bab8947":"# feature matrix for item condition and shipping\nfrom scipy.sparse import csr_matrix\nX_condition_shipping = csr_matrix(pd.get_dummies(df[['item_condition_id', 'shipping']], sparse=True).values)","47ecfbce":"X_condition_shipping.shape","b8d126bc":"# create the complete feature matrix\nfrom scipy.sparse import hstack\n\nX_all = hstack((X_brand, X_category, X_name, X_item_description, X_condition_shipping)).tocsr()","9d94dac3":"X_all.shape","70b54e4c":"# reduce the feature columns by removing all features with a document frequency smaller than 1\nmask = np.array(np.clip(X_all.getnnz(axis=0) - 1, 0, 1), dtype=bool)\nX_all = X_all[:, mask]","dc9a191e":"X_all.shape","4002607f":"# split into test and train samples\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_log, random_state=42, train_size=0.1, test_size=0.02)","3e07c155":"X_train.shape","001196e3":"X_test.shape","3d0f51fd":"from sklearn.model_selection import KFold, cross_val_score\ndef score_model(model):\n    kf = KFold(3, shuffle=True, random_state=42).get_n_splits(X_train)\n    model_score = np.mean(cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf, n_jobs=-1))\n    return((type(model).__name__,model_score))","24720f7a":"# get a baseline for a few regression models\n\nimport time\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\nmodel_scores = pd.DataFrame(columns=['model','NMSE'])\nreg_model = [Ridge(),Lasso(), GradientBoostingRegressor(),XGBRegressor()]\nfor model in reg_model:\n    start = time.time()\n    sc = score_model(model)\n    total = time.time() - start\n    print(\"done with {}, ({}s)\".format(sc[0],total))\n    model_scores = model_scores.append({'model':sc[0],'NMSE':sc[1]},ignore_index=True)    \n\n# print results\nmodel_scores.sort_values('NMSE',ascending=False)","e73323f1":"let's continue with Ridge and XGB as both train relatively fast and had the highest scores.","015c2ea0":"## Training","9b474c37":"## Target variable (Item price)\nDrop samples with price = 0 and fix the skewness of the target variable by log transformation","b01ae7c8":"## Features"}}