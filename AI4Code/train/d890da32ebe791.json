{"cell_type":{"eef646c7":"code","d27cf899":"code","713951b0":"code","91506d60":"code","c7ad3178":"code","51e0eeb3":"code","470a539c":"code","5f0864e7":"code","e2912c29":"code","35d0fd76":"code","2926c398":"code","ffb31101":"code","b71f4c78":"code","2f6bf5a8":"code","02eded27":"code","66465ef1":"code","5a5d67ba":"code","492383e4":"code","5504a563":"code","79a74672":"code","43d1bb09":"code","17c358ad":"code","6f13784c":"code","bee68951":"code","b60645c7":"code","6b2989c9":"code","f2f53b40":"code","68ca0e99":"code","13905bcd":"code","91e18c97":"code","9e272a00":"code","bf7bf7a6":"code","8d8cdbe0":"code","dba20c12":"code","2a0a40fc":"code","674ddd91":"code","2dde0bdd":"markdown","82c2ea36":"markdown","67889754":"markdown","0a1582d8":"markdown","79083c35":"markdown","5148ed5b":"markdown","41467ef1":"markdown","b61bb9ca":"markdown","50ef003d":"markdown","4c477266":"markdown","2cf4cc9b":"markdown"},"source":{"eef646c7":"# WandB \u2013 Install the W&B library\n!pip install --upgrade wandb\nimport wandb","d27cf899":"!wandb login b77985991c4b2c51ccc6af56fdfd2515dac6016a","713951b0":"# WandB \u2013 Initialize a new run\nwandb.init(project=\"pix2code\")","91506d60":"import requests\n\nurl = \"https:\/\/www.floydhub.com\/api\/v1\/resources\/YrepYFsJ8om77SivtHpVHH?content=true&download=true&rename=floydhub-datasets-pix2code-1\"\ntarget_path = 'xlrd-0.9.4.tar.gz'\n\nresponse = requests.get(url, stream=True)\nif response.status_code == 200:\n    with open(target_path, 'wb') as f:\n        f.write(response.raw.read())","c7ad3178":"!mkdir data\nimport gc\ndel response ; gc.collect()","51e0eeb3":"import tarfile\nmy_tar = tarfile.open('xlrd-0.9.4.tar.gz')\nmy_tar.extractall('data') # specify which folder to extract to\nmy_tar.close()","470a539c":"import os\nimport cv2\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import SubsetRandomSampler\nfrom torchvision import transforms\nimport torch.optim as optim\nfrom PIL import Image","5f0864e7":"del my_tar ; gc.collect()","e2912c29":"image_files = []\nhtml_files = []\nTransforms =transforms.Compose([\n                    transforms.ToTensor() ,\n                    transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225])\n                ])\n\ndef load_doc(file_path):\n    file = open(file_path , 'r')\n    text = file.read()\n    file.close()\n    return text\n\ndef blue_evaluation(text_data , gui_data ):\n    html_seq =[]\n    gui_seq= []\n    for html_file in text_data :\n        html_seq.append(text_process(html_file))\n        \n    for gui_file in gui_data :\n        img_tensor = Transforms(gui_file)\n        gui_seq.append(img_tensor)\n    \n    gui_seq = torch.stack(gui_seq)\n        \n    return html_seq , gui_seq\n    \n\ndef text_process(file):\n    \"\"\"\n    text = load_doc(file_path)\n    text = '<START> ' + text + ' <END>'\n    syntax = \" \".join(text.split())\n    #replace the commas with ' ,' syntax\n    syntax = syntax.replace(',', ' ,')\n    \"\"\"\n    return file.split()\n\ndef image_process(img_path):\n    img_bgr = cv2.imread(img_path)\n    img_rgb = cv2.cvtColor(img_bgr , cv2.COLOR_BGR2RGB)\n    return img_rgb","35d0fd76":"#data paths\nDS_TRAIN_PATH = \"data\/train\/\"\nDS_VAL_PATH = \"data\/eval\/\"","2926c398":"import matplotlib.pyplot as plt\nimport os\nimport numpy as np\n\ndef show_img(im, figsize=None, ax=None):\n    if not ax: fig,ax = plt.subplots(figsize=figsize)\n    ax.imshow(im)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    return ax\n\n# Read a file and return a string\ndef load_doc(filename):\n    file = open(filename, 'r')\n    text = file.read()\n    file.close()\n    return text\n\ndef load_data(data_dir):\n    text = []\n    images = []\n    # Load all the files and order them\n    all_filenames = os.listdir(data_dir)\n    all_filenames.sort()\n    for filename in (all_filenames):\n        if filename[-3:] == \"npz\":\n            # Load the images already prepared in arrays\n            image = np.load(data_dir+filename)\n            images.append(image['features'])\n        else:\n            # Load the boostrap tokens and rap them in a start and end tag\n            syntax = '<START> ' + load_doc(data_dir+filename) + ' <END>'\n            # Seperate all the words with a single space\n            syntax = ' '.join(syntax.split())\n            # Add a space after each comma\n            syntax = syntax.replace(',', ' ,')\n            text.append(syntax)\n    images = np.array(images, dtype=float)\n    return images, text\n\n# Get images and text\ntrain_features, train_texts = load_data(DS_TRAIN_PATH)\nvalid_features , valid_texts = load_data(DS_VAL_PATH)","ffb31101":"import random\nimport numpy as np\nidx_list = list(np.arange(len(train_features)))\nrand_idx  = np.random.choice(idx_list)\nblue_gui_list = train_features[rand_idx:rand_idx+2]\nblue_html_list = train_texts[rand_idx:rand_idx+2]","b71f4c78":"blue_html_seq , blue_gui_seq = blue_evaluation(blue_html_list , blue_gui_list)","2f6bf5a8":"#define the vocabulary\nwords=[]\nword_count=[]\nfor html_path in train_texts :\n    syntax = text_process(html_path)\n    sen_len = len(syntax)\n    words.extend(syntax)\n    word_count.append(sen_len)","02eded27":"#define the couunter object \nfrom collections import Counter\nfrom collections import OrderedDict\nword_counter = Counter(words)\nword_count_sorted = sorted(word_counter.items() , key=lambda x : x[1] , reverse=True)\nordered_dict = OrderedDict(word_count_sorted)\nordered_dict[\"<PAD>\"]=1\nordered_dict[\"<UNK>\"]=1","66465ef1":"from torchtext.vocab import Vocab\n\nhtml_vocab = Vocab(ordered_dict, min_freq=0 , specials=(\"<PAD>\" ,\"<UNK>\") , specials_first=True)\nvocab = html_vocab.itos","5a5d67ba":"word_2_idx = {word : idx  for idx , word in enumerate(vocab)}\nidx_2_word = {idx : word  for idx , word in enumerate(vocab)}","492383e4":"import numpy as np\nmax_seq_len = max(word_count)\ndef pad_seqence(sequence , max_len):\n    seq=[]\n    for word in sequence :\n        idx = word_2_idx[word]\n        seq.append(idx)\n    \n    pad_seq = np.zeros(max_len , dtype=np.uint8)\n    pad_seq[-len(seq):] = seq\n    \n    return pad_seq","5504a563":"#main data reader\nmax_len = max(word_count)\nMAX_LEN = 48\ndef token_target(text):\n    outter_token =[]\n    outter_target =[]\n    for html_pth in (text):\n        \"\"\"\n        1 step each html sentence create the token list\n        2 step each toekn create the input sequence with token list and target syntax\n        3 step for each token and target syntax take the approriate image file name\n\n        \"\"\"\n        inner_token =[]\n        inner_target=[]\n        syntax = text_process(html_pth)\n        for i_step in range(1, len(syntax)):\n            in_seq  , out_seq = syntax[:i_step] , syntax[i_step]\n            pad_in_seq = pad_seqence(in_seq , max_len)\n            out_seq = word_2_idx[out_seq]\n            pad_in_seq = pad_in_seq[-(MAX_LEN):]\n            inner_token.append(pad_in_seq)\n            inner_target.append(out_seq)\n            \n        outter_token.append(inner_token)\n        outter_target.append(inner_target)\n        \n    return outter_token , outter_target\n        \ntrain_token_set , train_target_set = token_target(train_texts)\neval_token_set , eval_target_set  = token_target(valid_texts)","79a74672":"train_token_set = np.array(train_token_set)\ntrain_target_set = np.array(train_target_set)\neval_token_set = np.array(eval_token_set)\neval_target_set = np.array(eval_target_set)","43d1bb09":"from torch.utils.data import TensorDataset\ndef my_collate(batch):\n    \n    gui_set = batch[0]['gui']\n    token_set = batch[0]['token']\n    target_set = batch[0]['target']\n\n    html_token_data=   [ data.clone() for data in    token_set  ]\n    html_target_data = [ torch.tensor(data)  for data in   target_set ]\n    seq_len = len(html_target_data)\n    gui_rgb = [ gui_set.clone() ]\n    \n    batch_32 = int(np.ceil(seq_len\/32))\n    \n    #gui_rgb = [ torch.stack(gui_rgb[i:i+16])  for i in range(batch_32) ]\n    html_token_data = [ torch.stack(html_token_data[i:i+32]) for i in range(batch_32) ]\n    html_target_data = [ torch.stack(html_target_data[i:i+32]) for i in range(batch_32) ]   \n    \n    gui_rgb =torch.stack(gui_rgb)\n    #html_token_data =torch.stack(html_token_data)\n    #html_target_data=torch.stack(html_target_data)\n    \n    \n    \n    sample ={\n        'gui':gui_rgb ,\n        'token': html_token_data ,\n        'target': html_target_data\n        }\n \n\n    return sample","17c358ad":"class Pix2CodeData(Dataset):\n    def __init__(self , html_token_set , html_target_set , gui_path_set):\n        self.html_token_set = html_token_set\n        self.html_target_set = html_target_set\n        self.gui_path_set = gui_path_set\n        self.transforms =transforms.Compose([\n                            transforms.ToTensor() ,\n                            transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225])\n                            ])\n        \n    def __len__(self):\n        return len(self.html_target_set)\n    def __getitem__(self , index):\n        \"\"\"\n        1 . take the html token data\n        2 . take the html target data\n        3 . take the gui path and take the rgb image\n        \"\"\"\n        html_token_data = self.html_token_set[index]\n        html_target_data = self.html_target_set[index]\n        gui_rgb = self.gui_path_set[index]\n        \n        #transform the image\n        gui_rgb = self.transforms(gui_rgb)\n        html_token_data = torch.LongTensor(html_token_data)\n        \n        sample ={\n            'gui':gui_rgb ,\n            'token': html_token_data ,\n            'target': html_target_data\n        }\n        \n        return sample","6f13784c":"#define the data loaders\n\ntrain_data_set = Pix2CodeData(train_token_set , train_target_set  , train_features)\neval_data_set = Pix2CodeData(eval_token_set , eval_target_set , valid_features)\n\ntrain_loader = DataLoader(train_data_set , batch_size=1 , shuffle=True , collate_fn=my_collate)\nvalid_loader = DataLoader(eval_data_set , batch_size=1 , shuffle=True  , collate_fn=my_collate)","bee68951":"del train_features ; gc.collect()\ndel valid_features ; gc.collect()\ndel train_texts ; gc.collect()\ndel valid_texts ; gc.collect()","b60645c7":"data = next(iter(train_loader))","6b2989c9":"print(\"Sample GUI tensor shape : \", data['gui'][0].shape)\nprint(\"Sample Token tensor shape : \", data['token'][0].shape)\nprint(\"Sample Target tensor shape : \",data['target'][0].shape)","f2f53b40":"import matplotlib.pyplot as plt\nmean = [0.485, 0.456, 0.406] \nstd = [0.229, 0.224, 0.225]\ndef viz_samples(gui):\n    \"\"\"\n    1 step convert to numpy array\n    2 step convert the channels\n    3 renormalize the gui\n    \"\"\"\n    gui = gui.numpy()\n    gui = np.transpose(gui , (1,2,0))\n    gui = gui*std + mean\n    plt.imshow(gui)\n    plt.title(\"GUI interface\")\n    plt.show()","68ca0e99":"gui_data = data['gui'][0]\nviz_samples(gui_data)","13905bcd":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","91e18c97":"device","9e272a00":"del data ; gc.collect()","bf7bf7a6":"class Pix2Code(nn.Module):\n    def __init__(self , in_channels , out_dim , dropout ,hidden_token , hidden_decoder , lstm_layers , MAX_LEN):\n        super(Pix2Code , self).__init__()\n        self.in_channels = in_channels\n        self.out_dim = out_dim\n        self.dropout = dropout\n        self.hidden_token = hidden_token\n        self.hidden_decoder = hidden_decoder\n        self.lstm_layers= lstm_layers\n        self.MAX_LEN = MAX_LEN\n        \n        self.conv_32 = nn.Conv2d(in_channels=in_channels , out_channels=32 , \n                                 kernel_size=3 , stride=1 , padding=1)\n        self.bn_32 = nn.BatchNorm2d(32)\n        self.conv_32_down = nn.Conv2d(32 , 32 , kernel_size=3 , stride=2 , padding=1)\n        self.bn_32_down = nn.BatchNorm2d(32)\n        \n        self.conv_64 = nn.Conv2d(32 , 64 , kernel_size=3 , stride=1 , padding=1)\n        self.bn_64 = nn.BatchNorm2d(64)\n        self.conv_64_down = nn.Conv2d(64 , 64 , kernel_size=3 , stride=2 , padding=1)\n        self.bn_64_down = nn.BatchNorm2d(64)\n        \n        self.conv_128 = nn.Conv2d(64 , 128 , kernel_size=3 , stride=1 , padding=1)\n        self.bn_128 = nn.BatchNorm2d(128)\n        self.dense_size = 128 * 64 * 64\n        \n        self.fc_gui1 = nn.Linear(self.dense_size , 1024)\n        self.fc_gui2 = nn.Linear(1024 , 1024)\n        \n        #****************************** Token Encoder *********************************************************\n        self.token_embed = nn.Embedding(len(vocab) , 100)\n        self.token_lstm = nn.LSTM(100 , self.hidden_token , num_layers=self.lstm_layers , \n                                  dropout = self.dropout , batch_first = True)\n        \n        #**************************** Decoder ******************************************************************\n        self.decoder_in = 1024 + self.hidden_token\n        self.decoder_lstm = nn.LSTM(self.decoder_in , self.hidden_decoder , num_layers=self.lstm_layers ,\n                                    dropout= self.dropout , batch_first=True)\n        \n        self.fc_decoder = nn.Linear(self.hidden_decoder , len(vocab))\n        self.softmax = nn.Softmax(dim=-1)\n        \n        \n    def forward(self , gui , token ):\n        \n        hidden_token = self.init_token_hidden(token.shape[0])\n        hidden_decoder = self.init_decoder_hidden(token.shape[0])\n        \n        # gui feature extraction \n        gui_x = F.relu(self.bn_32(self.conv_32(gui)))\n        gui_x = F.relu(self.bn_32_down(self.conv_32_down(gui_x)))\n        gui_x = F.relu(self.bn_64(self.conv_64(gui_x)))\n        gui_x = F.relu(self.bn_64_down(self.conv_64_down(gui_x)))\n        gui_x = F.relu(self.bn_128(self.conv_128(gui_x)))\n        \n        gui_x = gui_x.view(-1 , self.dense_size)\n        gui_x = F.relu(self.fc_gui1(gui_x))\n        gui_x = F.dropout(gui_x ,p=0.4)\n        gui_x = F.relu(self.fc_gui2(gui_x))\n        gui_x = gui_x.view(1 , 1024)\n        \n        gui_x = gui_x.repeat(token.shape[0] , 1 )\n        \n        gui_x = F.dropout(gui_x , p=0.4)\n        \n        #repeat the vector\n        gui_x = gui_x.unsqueeze(1)\n        gui_repeat = gui_x.repeat(1 , self.MAX_LEN , 1)      # (batch size , 1024) --> (batch size , MAX LEN , 1024)\n        \n        # encoder of the token features using embeddings and lstms\n\n        token_embed = self.token_embed(token.data)\n        token_lstm , _ = self.token_lstm(token_embed , hidden_token)\n\n        decoder_in = torch.cat([token_lstm , gui_repeat] , dim=-1)\n     \n        # decoder output prediction\n        decoder_lstm, _ = self.decoder_lstm(decoder_in , hidden_decoder)\n        decoder_lstm = decoder_lstm[:,-1,:]\n        decoder_fc  = decoder_lstm.contiguous().view(-1,self.hidden_decoder)\n       \n        decoder_fc = F.dropout(decoder_fc , p=0.5)\n        decoder_out = self.fc_decoder(decoder_fc)\n        \n        return decoder_out\n        \n        \n        \n    def init_token_hidden(self , batch_size):\n        weight = next(self.parameters()).data\n        \n        hidden =(weight.new(self.lstm_layers , batch_size , self.hidden_token).zero_().to(device) , \n                 weight.new(self.lstm_layers , batch_size , self.hidden_token).zero_().to(device) )\n        \n        return hidden\n    \n    def init_decoder_hidden(self,batch_size):\n        weight = next(self.parameters()).data\n        \n        hidden = ( weight.new(self.lstm_layers , batch_size , self.hidden_decoder).zero_().to(device) , \n                   weight.new(self.lstm_layers , batch_size , self.hidden_decoder).zero_().to(device)  )\n        \n        return hidden","8d8cdbe0":"def word_into_idx(text):\n    #split the text\n    words = text.split()\n        \n    return words\n\n# generate a description for an image\ndef generate_desc(model, photo, max_length):\n    photo = photo.unsqueeze(0).float().to(device)\n    # seed the generation process\n    in_text = '<START> '\n    # iterate over the whole length of the sequence\n    print('\\nPrediction---->\\n\\n<START> ', end='')\n    for i in range(150):\n        # integer encode input sequence\n        sequence = word_into_idx(in_text)\n        if(len(sequence)>=max_length):\n            sequence = sequence[-(max_length):]\n        #pad sequence\n        sequence = pad_seqence(sequence , max_length)\n        seq_tensor = torch.tensor(sequence).unsqueeze(0).to(device)\n        seq_tensor = seq_tensor.long()\n       \n        # predict next word\n        with torch.no_grad():\n            yhat = model.forward(photo, seq_tensor)\n        yhat_softmax = F.softmax(yhat , dim=-1)\n        yhat_max = torch.argmax(yhat_softmax)\n\n        # map integer to word\n        word = idx_2_word[yhat_max.item()]\n        # stop if we cannot map the word\n        if word is None:\n            break\n        # append as input for generating the next word\n        in_text += word + ' '\n        # stop if we predict the end of the sequence\n        print(word + ' ', end='')\n        if word == '<END>':\n            break\n    return in_text","dba20c12":"from nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\nsmoothie = SmoothingFunction().method4\n# Evaluate the skill of the model\ndef evaluate_model(model, texts, photos, max_length):\n    actual, predicted = list(), list()\n    # step over the whole set\n    for i in range(len(texts)):\n        viz_samples(photos[i])\n        yhat = generate_desc(model , photos[i], max_length)\n        # store actual and predicted\n        text = \" \".join(texts[i])\n        print('\\n\\nReal---->\\n\\n' + text)\n        actual.append(text)\n        predicted.append(yhat.split())\n    # calculate BLEU score\n    bleu = corpus_bleu(actual, predicted , smoothing_function=smoothie )\n    return bleu, actual, predicted","2a0a40fc":"#keep track of the configurations\nconfig = wandb.config  \nconfig.in_channels =3 \nconfig.out_dim =len(vocab) \nconfig.dropout = 0.3 \nconfig.hidden_token = 128\nconfig.hidden_decoder =512\nconfig.lstm_layers =2\nconfig.MAX_LEN = 48\nconfig.EPOCHS=20\nconfig.grad_clip = 3.0\nconfig.learning_rate = 0.0001\n\npix2code =  Pix2Code(in_channels= config.in_channels , out_dim=config.out_dim , \n                    dropout=config.dropout , hidden_token=config.hidden_token , \n                    hidden_decoder=config.hidden_decoder , lstm_layers=config.lstm_layers , MAX_LEN=config.MAX_LEN)\n\npix2code.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.RMSprop(pix2code.parameters() , lr=config.learning_rate)","674ddd91":"from collections import deque\nimport gc\n\nmean_train_loss = deque(maxlen=100)\nmean_val_loss = deque(maxlen=100)\ntotal_train_loss = []\ntotal_val_loss = []\nprint_every = 250\nblue_score =0\n#define the training loop and wandb logging\n#wandb.watch(pix2code, log=\"all\")\nstep = 0\nfor i_epoch in range(config.EPOCHS) :\n    pix2code.train()\n    epoch_train_loss = 0\n    epoch_val_loss = 0\n    \n    \n    for idx , data in enumerate(train_loader) :\n        \n        gui = data['gui']\n        token = data['token']\n        target = data['target']\n        \n        for i_batch in range(len(token)):\n        \n            gui_i = gui.float().to(device)\n            token_i = token[i_batch].to(device)\n            target_i = target[i_batch].to(device)\n\n            target_pred = pix2code(gui_i , token_i)\n            loss = criterion(target_pred , target_i)\n\n            #reset the optimizer\n            optimizer.zero_grad()\n\n            #backprop the loss\n            loss.backward()\n\n            #clip the exploiding gradinets\n            torch.nn.utils.clip_grad_norm_(pix2code.parameters(), config.grad_clip)\n\n            #optimize the model\n            optimizer.step()\n\n            epoch_train_loss += loss.to('cpu').detach().item()\n\n            mean_train_loss.append(loss.to('cpu').detach().item())\n            total_train_loss.append(loss.to('cpu').detach().item())\n        \n\n        if((idx+1)%print_every==0):\n            step += 1\n            print(\"Epoch {} Step {} train loss {:.6f}\".format(i_epoch ,step , np.mean(mean_train_loss)))\n            wandb.log({\n                \"Steps\":step ,\n                \"Train Loss\":np.mean(mean_train_loss)})\n\n        \n    pix2code.eval()\n    with torch.no_grad():\n        for idx , data in enumerate(valid_loader) :\n    \n            gui = data['gui']\n            token = data['token']\n            target = data['target']\n            \n            for i_batch in range(len(token)):\n    \n                gui_i = gui.float().to(device)\n                token_i = token[i_batch].to(device)\n                target_i = target[i_batch].to(device)\n\n                target_pred = pix2code(gui_i , token_i)\n                loss = criterion(target_pred , target_i)\n\n                epoch_val_loss += loss.to('cpu').detach().item()\n\n                mean_val_loss.append(loss.to('cpu').detach().item())\n                total_val_loss.append(loss.to('cpu').detach().item())\n                \n         \n        print(\"Epoch {} Val loss {:.6f}\".format(i_epoch , np.mean(mean_val_loss)))\n        wandb.log({\n            \"Epoch\":i_epoch ,\n            \"Val Loss\":np.mean(mean_train_loss)})\n     \n    #do some model evaluation based on the belu scores\n    # Eval on the first 10 samples\n    \n    if((i_epoch +1 )%8==0):\n        bleu, actual, predicted = evaluate_model( pix2code , blue_html_seq, blue_gui_seq, MAX_LEN)\n        print(\"BLUE score: \", bleu)\n        if (bleu > blue_score):\n            # Save model to wandb\n            #torch.save(pix2code.state_dict(), os.path.join(wandb.run.dir, 'pix2code.pt'))\n            blue_score = bleu\n    \n    wandb.log({\n        \"Train Loss\":np.mean(mean_train_loss) ,\n        \"Test Loss\": np.mean(mean_val_loss) ,\n        \"Blue Score\": blue_score})","2dde0bdd":"> # Model  , Loss functions and Optimizers initialization","82c2ea36":"> # Model Arciture from the paper","67889754":"> # Data preprocessing \n* Load the data from the data folders\n* Create the HTML tags with 'START'+  Token + 'END' structure","0a1582d8":"# Pix2code paper implementation in Pytorch\n\n### Below the implementation Arcitecture\n![](https:\/\/blog.thinkwik.com\/wp-content\/uploads\/2017\/12\/Training.png)","79083c35":"> # Define the Collate function and the pytorch data set custom class","5148ed5b":"> Obtained the data set from the floydhub because it's preprocesses and easy to use","41467ef1":"> # Training Loop","b61bb9ca":"> # Evaluation stratergy","50ef003d":"## Wandb great visualization tool \ninstall the wandb package and initialize the tool , create your own project their and give credentials to the project below","4c477266":"> # Create the data format for feed the model\n* As our input for the encoder contain two types of features one from image features and other from text token features\n* Define out token html sequence with max len =48 \n* Define output sequence for each token html sequnece which is next to predict","2cf4cc9b":"> # Create the vocabulary for the HTML tokanization"}}