{"cell_type":{"fed902bd":"code","1c79db47":"code","cb61606a":"code","af6f96d6":"code","10ce172b":"code","44d10258":"code","7818d1d2":"code","38d68077":"code","c04dae04":"code","b1b5d3fa":"code","795c4001":"code","f64b9b5e":"code","fab2d95a":"code","590f6177":"code","24513368":"code","9b167816":"code","fd371722":"code","d01f132a":"code","35c5f85c":"code","7defe4ea":"code","cfef1ead":"code","97699444":"code","6deb8dc8":"code","dfef3f48":"code","0cf52a67":"code","e0966eab":"markdown","9e6d7b1c":"markdown","0fc131ba":"markdown"},"source":{"fed902bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c79db47":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns #for visualization\nimport matplotlib.pyplot as plt #\n\nfrom sklearn import metrics # For metrics such as R2 and explained variance score\nfrom tqdm import tqdm # Progress bar\nfrom sklearn.ensemble import RandomForestRegressor #simple model","cb61606a":"#data loading\ndata = pd.read_csv('\/kaggle\/input\/pro-data-2020\/train.csv', header = 0, sep = ',')\ntest_data = pd.read_csv('\/kaggle\/input\/pro-data-2020\/test.csv', header = 0, sep = ',')\nprint(data.shape[0])\nprint(test_data.shape[0])\n#join two data for preprocessing\ndata = data.append(test_data)\nprint(data.shape[0])","af6f96d6":"#Count all categorical values\ndata.select_dtypes(include=['object']).T.apply(lambda x: x.nunique(), axis=1)[:12]","10ce172b":"def one_hot_encode_pipeline(df: pd.DataFrame, \n                            train_shape: int, \n                            test_shape: int) -> [pd.DataFrame, pd.DataFrame]:\n    \n    #one hot encoding for categorical values\n    def get_one_hot_dataset(df, exception: list) -> pd.DataFrame:\n        all_features = []\n        for column in tqdm(df.columns):\n            if column not in exception:\n                one_hot = pd.get_dummies(df[column])\n                df = df.drop(columns=[column])\n                all_features.append(one_hot)\n            else:\n                continue\n        features = pd.concat(all_features, axis=1)\n        return pd.concat([df, features], axis=1)\n    \n    # Imputing missing values\n    df[df.iloc[:, 12:].columns] = df.iloc[:, 12:].fillna(0)  #filling zero for one hot values\n    df = df.interpolate(method='pad', inplace=False) #imputing numerical values\n    \n    exceptions = df.select_dtypes(exclude=['object']).columns #filtering non - categorical values\n    df = get_one_hot_dataset(df=df, exception=exceptions) #transforming categorical into one_hot \n    \n    #return train, test\n    return df.head(train_shape), df.tail(test_shape)","44d10258":"train, test = one_hot_encode_pipeline(df=data, \n                                      train_shape=data.shape[0]-test_data.shape[0], \n                                      test_shape=test_data.shape[0])","7818d1d2":"print(train.shape[0])\nprint(test.shape[0])","38d68077":"train.isnull().sum()[train.isnull().sum()>0]","c04dae04":"train['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'] = np.where(train['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'] >  100000,  100000, train['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'])","b1b5d3fa":"train['\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b'] = np.where(train['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'] <=  train['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'].median(),  1, 0)\ntest['\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b'] = np.where(test['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'] <=  train['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'].median(),  1, 0)","795c4001":"train_high = train.loc[train['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'] > train['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'].median()]\ntrain_low = train.loc[train['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'] <= train['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'].median()]\ntest_high = test.loc[test['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'] > train['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'].median()]\ntest_low = test.loc[test['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'] <= train['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'].median()]","f64b9b5e":"train_labels = (train['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445']).values\ntrain_data = train.drop(['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445', '\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b'], axis = 1)\ntrain_labels_high = (train_high['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445']).values\ntrain_data_high = train_high.drop(['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445', '\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b'], axis = 1)\ntrain_labels_low = (train_low['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445']).values\ntrain_data_low = train_low.drop(['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445', '\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b'], axis = 1)\ntrain_labels_class = (train['\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b']).values\ntrain_data_class = train.drop(['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445', '\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b'], axis = 1)","fab2d95a":"test_labels = (test['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445']).values\ntest_data = test.drop(['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445', '\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b'], axis = 1)\ntest_labels_high = (test_high['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445']).values\ntest_data_high = test_high.drop(['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445', '\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b'], axis = 1)\ntest_labels_low = (test_low['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445']).values\ntest_data_low = test_low.drop(['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445', '\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b'], axis = 1)\ntest_labels_class = (test['\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b']).values\ntest_data_class = test.drop(['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445', '\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b'], axis = 1)","590f6177":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.linear_model import LassoCV\nfrom sklearn import model_selection, linear_model, metrics, pipeline, preprocessing\nfrom sklearn.metrics import r2_score\nfrom sklearn import ensemble, model_selection, metrics \nimport xgboost as xgb\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom xgboost import XGBClassifier","24513368":"#define model\nmodel_rf = RandomForestRegressor(n_estimators=1500, \n                              max_depth=1000, \n                              max_leaf_nodes=300, \n                              max_features='sqrt', \n                              n_jobs=-1, \n                             bootstrap=True)\nmodel_abr = AdaBoostRegressor()\nmodel_xgb = xgb.XGBRegressor(booster = 'gbtree', \n                             max_depth = 6, \n                             subsample = 1,\n                             eta = 0.1,\n                             min_child_weight = 8)\nmodel_votr = VotingRegressor(estimators=[('xgb', model_xgb), ('rf', model_rf)],\n                            n_jobs = -1)\nmodel_gbr = GradientBoostingRegressor()\nmodel_extree = ExtraTreesRegressor()\nmodel_xgb_cl = XGBClassifier()","9b167816":"from sklearn.ensemble import StackingRegressor\nestimators = [('rf', model_rf), ('extree', model_extree)]\nmodel_stack = StackingRegressor(estimators=estimators, n_jobs = -1)","fd371722":"#\u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043c\u0430\u0448\u0438\u043d \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f\nmodel_xgb_cl.fit(train_data_class.values, train_labels_class)\n#\u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043c\u0430\u0448\u0438\u043d \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f\nmodel_stack.fit(train_data.values, train_labels)\nmodel_xgb.fit(train_data.values, train_labels)\n#\u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0434\u043e\u0440\u043e\u0433\u0438\u0445 \u043c\u0430\u0448\u0438\u043d\nmodel_extree.fit(train_data_high.values, train_labels_high)\n#\u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0434\u0435\u0448\u0451\u0432\u044b\u0445 \u043c\u0430\u0448\u0438\u043d\nmodel_votr.fit(train_data_low.values, train_labels_low)","d01f132a":"y_xgb_cl = model_xgb_cl.predict(test_data_class.values)\ny_stack = model_stack.predict(test_data.values)\ny_xgb = model_xgb.predict(test_data.values)\ny_extree = model_extree.predict(test_data_high.values)\ny_votr = model_votr.predict(test_data_low.values)","35c5f85c":"print(\"R2: \", metrics.r2_score(test_labels_class, y_xgb_cl))\nprint(\"R2: \", metrics.r2_score(test_labels, y_stack))\nprint(\"R2: \", metrics.r2_score(test_labels, y_xgb))\nprint(\"R2: \", metrics.r2_score(test_labels_high, y_extree))\nprint(\"R2: \", metrics.r2_score(test_labels_low, y_votr))","7defe4ea":"df_predict = pd.DataFrame({'\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b': model_xgb_cl.predict(test_data_class.values),\n                           '\u0414\u043e\u0440\u043e\u0433\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c': model_extree.predict(test_data.values),\n                           '\u0414\u0435\u0448\u0451\u0432\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c':  model_votr.predict(test_data.values),\n                           '\u0421\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c Stack': model_stack.predict(test_data.values),\n                           '\u0421\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c XGB': model_xgb.predict(test_data.values),\n                            '\u0424\u0430\u043a\u0442':test_labels})","cfef1ead":"#\u043f\u043e\u0434\u0431\u043e\u0440 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0446\u0435\u043d\u0442\u043e\u0432 \u0434\u043b\u044f \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f R2\nn_xgbs = [float(x) for x in np.linspace(start = 0.00, stop = 1.0, num = 11)]\nn_stacks = [float(x) for x in np.linspace(start = 0.00, stop = 1.0, num = 11)]\nn_lows = [float(x) for x in np.linspace(start = 0.00, stop = 1.0, num = 11)]\nn_highs = [float(x) for x in np.linspace(start = 0.00, stop = 1.0, num = 11)]\nfor n_xgb in n_xgbs:\n    for n_stack in n_stacks:\n        for n_low in n_lows:\n            for n_high in n_highs:\n                df_predict['Itog'] = np.where(df_predict['\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b'] == 1, \n                              n_low*df_predict['\u0414\u0435\u0448\u0451\u0432\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c'] + n_xgb*df_predict['\u0421\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c XGB'], \n                              n_high*df_predict['\u0414\u043e\u0440\u043e\u0433\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c'] + n_stack*df_predict['\u0421\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c Stack'])\n                if r2_score(test_labels, df_predict['Itog']) > r2_scor:\n                    r2_scor = r2_score(test_labels, df_predict['Itog'])\n                    n_xgb_opt = n_xgb\n                    n_stack_opt = n_stack\n                    n_low_opt = n_low\n                    n_high_opt = n_high\nprint (str(n_xgb_opt)+\" \"+\n        str(n_stack_opt)+\" \"+\n        str(n_low_opt)+\" \"+\n           str(n_high_opt)+\" \"+\n            str(r2_scor)) ","97699444":"df_predict['\u0418\u0442\u043e\u0433\u043e\u0432\u044b\u0439 \u043f\u0440\u043e\u0433\u043d\u043e\u0437'] = np.where(df_predict['\u041c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u044b'] == 1, \n                              n_low_opt*df_predict['\u0414\u0435\u0448\u0451\u0432\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c'] + n_xgb_opt*df_predict['\u0421\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c XGB'], \n                              n_high_opt*df_predict['\u0414\u043e\u0440\u043e\u0433\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c'] + n_stack_opt*df_predict['\u0421\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c Stack'])","6deb8dc8":"metrics.r2_score(test_labels, df_predict['\u0418\u0442\u043e\u0433\u043e\u0432\u044b\u0439 \u043f\u0440\u043e\u0433\u043d\u043e\u0437'])","dfef3f48":"submission = pd.DataFrame({\n    'Id':np.arange(df_predict['\u0418\u0442\u043e\u0433\u043e\u0432\u044b\u0439 \u043f\u0440\u043e\u0433\u043d\u043e\u0437'].shape[0]),  #Id is necessary \n    'Predicted':df_predict['\u0418\u0442\u043e\u0433\u043e\u0432\u044b\u0439 \u043f\u0440\u043e\u0433\u043d\u043e\u0437']  # Predicted  is also necessary\n})","0cf52a67":"submission.to_csv(\"submission.csv\", index=False) #always index=False","e0966eab":"## Modelling","9e6d7b1c":"## Pipeline","0fc131ba":"## Preparing submission"}}