{"cell_type":{"37bd34b1":"code","cffeda10":"code","1dc8b7ae":"code","664b5332":"code","c5db08ca":"code","ae92c280":"code","ef2129b8":"code","44f538bc":"code","af01ccf3":"code","6dc8bdc4":"code","7b7c06a7":"code","c789c843":"code","05879d2d":"code","b4c34350":"code","b0cc2f03":"code","01f581c1":"code","8104decd":"code","f189de2e":"markdown","824efb83":"markdown","fd3af18f":"markdown","efb013d9":"markdown","0a523916":"markdown","641ac1f2":"markdown","9612adaa":"markdown","6a2d6392":"markdown","bb47ac9f":"markdown","d67b4f5f":"markdown","15ca36c9":"markdown","7910c2be":"markdown","14138f44":"markdown","843bb3e6":"markdown","a03055d1":"markdown","b76c9446":"markdown","40e296b7":"markdown","e85b7b9f":"markdown"},"source":{"37bd34b1":"!pip install pmdarima","cffeda10":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n\nfrom fbprophet import Prophet\nfrom matplotlib import pyplot as plt\nfrom pmdarima import auto_arima\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nmyfavouritenumber = 13\nseed = myfavouritenumber\nnp.random.seed(seed)","1dc8b7ae":"df = pd.read_csv(\"\/kaggle\/input\/nifty50-stock-market-data\/BAJAJFINSV.csv\")\ndf.set_index(\"Date\", drop=False, inplace=True)\ndf.head()","664b5332":"df.VWAP.plot(figsize=(14, 7))","c5db08ca":"df.reset_index(drop=True, inplace=True)\nlag_features = [\"High\", \"Low\", \"Volume\", \"Turnover\", \"Trades\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index().astype(np.float32)\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index().astype(np.float32)\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index().astype(np.float32)\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index().astype(np.float32)\n\nfor feature in lag_features:\n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf.fillna(df.mean(), inplace=True)\n\ndf.set_index(\"Date\", drop=False, inplace=True)\ndf.head()","ae92c280":"df.Date = pd.to_datetime(df.Date, format=\"%Y-%m-%d\")\ndf[\"month\"] = df.Date.dt.month\ndf[\"week\"] = df.Date.dt.week\ndf[\"day\"] = df.Date.dt.day\ndf[\"day_of_week\"] = df.Date.dt.dayofweek\ndf.head()","ef2129b8":"df_train = df[df.Date < \"2019\"]\ndf_valid = df[df.Date >= \"2019\"]\n\nexogenous_features = [\"High_mean_lag3\", \"High_std_lag3\", \"Low_mean_lag3\", \"Low_std_lag3\",\n                      \"Volume_mean_lag3\", \"Volume_std_lag3\", \"Turnover_mean_lag3\",\n                      \"Turnover_std_lag3\", \"Trades_mean_lag3\", \"Trades_std_lag3\",\n                      \"High_mean_lag7\", \"High_std_lag7\", \"Low_mean_lag7\", \"Low_std_lag7\",\n                      \"Volume_mean_lag7\", \"Volume_std_lag7\", \"Turnover_mean_lag7\",\n                      \"Turnover_std_lag7\", \"Trades_mean_lag7\", \"Trades_std_lag7\",\n                      \"High_mean_lag30\", \"High_std_lag30\", \"Low_mean_lag30\", \"Low_std_lag30\",\n                      \"Volume_mean_lag30\", \"Volume_std_lag30\", \"Turnover_mean_lag30\",\n                      \"Turnover_std_lag30\", \"Trades_mean_lag30\", \"Trades_std_lag30\",\n                      \"month\", \"week\", \"day\", \"day_of_week\"]","44f538bc":"model = auto_arima(df_train.VWAP, exogenous=df_train[exogenous_features], trace=True, error_action=\"ignore\", suppress_warnings=True)\nmodel.fit(df_train.VWAP, exogenous=df_train[exogenous_features])\n\nforecast = model.predict(n_periods=len(df_valid), exogenous=df_valid[exogenous_features])\ndf_valid[\"Forecast_ARIMAX\"] = forecast","af01ccf3":"df_valid[[\"VWAP\", \"Forecast_ARIMAX\"]].plot(figsize=(14, 7))","6dc8bdc4":"print(\"RMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Forecast_ARIMAX)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.VWAP, df_valid.Forecast_ARIMAX))","7b7c06a7":"model_fbp = Prophet()\nfor feature in exogenous_features:\n    model_fbp.add_regressor(feature)\n\nmodel_fbp.fit(df_train[[\"Date\", \"VWAP\"] + exogenous_features].rename(columns={\"Date\": \"ds\", \"VWAP\": \"y\"}))\n\nforecast = model_fbp.predict(df_valid[[\"Date\", \"VWAP\"] + exogenous_features].rename(columns={\"Date\": \"ds\"}))\ndf_valid[\"Forecast_Prophet\"] = forecast.yhat.values","c789c843":"model_fbp.plot_components(forecast)","05879d2d":"df_valid[[\"VWAP\", \"Forecast_ARIMAX\", \"Forecast_Prophet\"]].plot(figsize=(14, 7))","b4c34350":"print(\"RMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Forecast_ARIMAX)))\nprint(\"RMSE of Prophet:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Forecast_Prophet)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.VWAP, df_valid.Forecast_ARIMAX))\nprint(\"MAE of Prophet:\", mean_absolute_error(df_valid.VWAP, df_valid.Forecast_Prophet))","b0cc2f03":"params = {\"objective\": \"regression\"}\n\ndtrain = lgb.Dataset(df_train[exogenous_features], label=df_train.VWAP.values)\ndvalid = lgb.Dataset(df_valid[exogenous_features])\n\nmodel_lgb = lgb.train(params, train_set=dtrain)\n\nforecast = model_lgb.predict(df_valid[exogenous_features])\ndf_valid[\"Forecast_LightGBM\"] = forecast","01f581c1":"df_valid[[\"VWAP\", \"Forecast_ARIMAX\", \"Forecast_Prophet\", \"Forecast_LightGBM\"]].plot(figsize=(14, 7))","8104decd":"print(\"RMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Forecast_ARIMAX)))\nprint(\"RMSE of Prophet:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Forecast_Prophet)))\nprint(\"RMSE of LightGBM:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Forecast_LightGBM)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.VWAP, df_valid.Forecast_ARIMAX))\nprint(\"MAE of Prophet:\", mean_absolute_error(df_valid.VWAP, df_valid.Forecast_Prophet))\nprint(\"MAE of LightGBM:\", mean_absolute_error(df_valid.VWAP, df_valid.Forecast_LightGBM))","f189de2e":"## Nifty-50 Stock Market Data\nThe [dataset](https:\/\/www.kaggle.com\/rohanrao\/nifty50-stock-market-data) used is stock market data of the Nifty-50 index from NSE (National Stock Exchange) India over the last 20 years (2000 - 2019)\n\nThe historic **VWAP (Volume Weighted Average Price)** is the target variable to predict. VWAP is a trading benchmark used by traders that gives the average price the stock has traded at throughout the day, based on both volume and price.   \nRead more about the dataset: https:\/\/www.kaggle.com\/rohanrao\/nifty50-stock-market-data\n\nThe stock used is **BAJAJFINSV**.","824efb83":"The Auto ARIMAX model seems to do a fairly good job in predicting the stock price given data till the previous day. Can other models beat this benchmark?","fd3af18f":"## LightGBM\nTime series problems are popularly converted into a tabular i.i.d. structure and fed into boosting models like [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/) and [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/).\n\nThere is loss of information in terms of knowing the order of data points in the time series but it can be circumvented by the datetime features to capture this information to some extent.\n\nNote that the default parameters are used for LightGBM. They can be tuned to improve the results.","efb013d9":"## Feature Engineering\nAlmost every time series problem will have some external features or some internal feature engineering to help the model.\n\nLet's add some basic features like lag values of available numeric features that are widely used for time series problems. Since we need to predict the price of the stock for a day, we cannot use the feature values of the same day since they will be unavailable at actual inference time. We need to use statistics like mean, standard deviation of their lagged values.\n\nWe will use three sets of lagged values, one previous day, one looking back 7 days and another looking back 30 days as a proxy for last week and last month metrics.","0a523916":"## Conclusions and Tips\n* Auto ARIMAX is a great baseline model but newer algorithms like Facebook's Prophet are extremely powerful and are getting cleverer by the day. Don't feel afraid to try out new techniques.\n* Setting up an appropriate validation framework is extremely important. It enables you to try and experiment various models and objectively compare them.\n* Lag-based features are very useful in providing trends information about the time series data. Rolling statistics are a common way of generating these.\n* Exogenous regressors help in providing external information about the time series. They tend to be very important in most models.\n* Boosting models like LightGBM are constrained to predict within the range of values of the target variable in the training data and don't extrapolate when there is strong trend.\n* Converting a time series to stationary and then modelling is a common approach for building solutions and can significantly improve results.","641ac1f2":"## Auto ARIMAX\nARIMA (Auto Regressive Integrated Moving Average) models explain a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values.\n\nARIMA models require certain input parameters: p for the AR(p) part, q for the MA(q) part and d for the I(d) part. Thankfully, there is an automatic process by which these parameters can be chosen which is called Auto ARIMA.\n\nWhen exogenous regressors are used with ARIMA it is commonly called ARIMAX.\n\nRead more about ARIMA: https:\/\/en.wikipedia.org\/wiki\/Autoregressive_integrated_moving_average","9612adaa":"The additional features supplied to time series problems are called exogenous regressors.","6a2d6392":"Plotting the target variable **VWAP** over time","bb47ac9f":"The auto_arima module from *pmdarima* package is not directly available on Kaggle so we'll install it using pip.","d67b4f5f":"Auto ARIMAX performs better than Prophet in this case.","15ca36c9":"The best ARIMA model is ARIMA(2, 0, 1) which has the lowest [AIC](https:\/\/en.wikipedia.org\/wiki\/Akaike_information_criterion).","7910c2be":"For boosting models, it is very useful to add datetime features like hour, day, month, as applicable to provide the model information about the time component in the data. For time series models it is not explicitly required to pass this information but we could do so and we will in this notebook so that all models are compared on the exact same set of features.","14138f44":"One method of improving i.i.d. based models is to model the differential target variable after removing the trend and making the time series stationary. We will look at this process along with an example of using a deep learning model (LSTM) to build a time series solution.\n\nWill be updated soon.","843bb3e6":"## Data Preparation\nReading the market data of BAJAJFINSV stock and preparing a training dataset and validation dataset.","a03055d1":"## Facebook Prophet\nProphet is an open-source time series model developed by Facebook. It was released in early 2017. An exerpt from the homepage:\n\n> Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n\nRead more about Prophet: https:\/\/facebook.github.io\/prophet\/\n\nI also shared a starter code [Prophet's Prophecy](https:\/\/www.kaggle.com\/rohanrao\/ashrae-prophet-s-prophecy) for using Prophet in the ASHRAE competition on Kaggle.\n\nNote that the default parameters are used for Prophet. They can be tuned to improve the results.","b76c9446":"## (A modern) Time Series tutorial\n![ts.jpg](attachment:ts.jpg)\n\n**Time Series** is a class of data science problems where the primary values of interest are a series of data points measured over a period of time. This notebook aims to provide the basic building blocks of some of the **more modern algorithms \/ techniques (and data!)** for solving these types of problems.\n\nIs **ARIMA** the first thing you think of when you hear about time series? It might be time to explore other ventures and methodologies.\nThere is a lot of new innovation and modern techniques being actively developed and some of them are outperforming the traditional ARIMA models. We'll look at some of these models and try to apply them on stock market data to predict price.\n\nModels explored in this notebook:\n* **Auto ARIMAX**\n* **Facebook Prophet**\n* **LightGBM**\n* **LSTM (coming soon)** ","40e296b7":"LightGBM performs terribly! This is a very important aspect of using boosting models for time series. Remember that boosting models are constrained to predict within the range of target values appearing in the training data. The maximum price value in the training data is ~ 7100 and hence LGBM is unable to predict values beyond 7100.\n\nThen why are boosting methods still so popular? Well, they fail only in cases where the trend component is extremely strong and there are a wide variety of use cases where the trend is weak and the expected forecasts are within the values of the past. Stock prices is an example that generally has strong trend components especially when measured over years.","e85b7b9f":"Splitting the data into train and validation along with features.     \n* **train:** Data from 26th May, 2008 to 31st December, 2018.\n* **valid:** Data from 1st January, 2019 to 31st December, 2019."}}