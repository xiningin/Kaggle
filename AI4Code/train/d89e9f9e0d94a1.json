{"cell_type":{"006c3450":"code","f330a540":"code","42cdc2b5":"code","3f0d940d":"code","ea562a26":"code","df1cc3b6":"code","aaa1a2e6":"code","1d3e8a38":"code","f7c7e344":"code","0ec64ce0":"code","10884cfd":"code","3e7b66d6":"code","3e29e7d2":"code","b9cc49e8":"code","a981d3a2":"code","7c70c55b":"code","f6128c77":"code","3598e5db":"code","be538ff9":"code","7ddc1453":"code","5cbcd124":"code","18f0572c":"code","fec4aacd":"markdown","2e13f513":"markdown","1ec5e50a":"markdown","000c9018":"markdown","00cdffa9":"markdown","50dbdce3":"markdown"},"source":{"006c3450":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","f330a540":"data = pd.read_csv('..\/input\/airways-tweets\/Tweets.csv')\ndata.head()","42cdc2b5":"df = data[['airline_sentiment', 'text']]\ndf.head()","3f0d940d":"df['airline_sentiment'].unique()","ea562a26":"df['airline_sentiment'].value_counts()","df1cc3b6":"df_pos = df[df['airline_sentiment'] == 'positive']\ndf_neg = df[df['airline_sentiment'] == 'negative'].iloc[:len(df_pos)]\nlen(df_pos), len(df_neg)","aaa1a2e6":"df = pd.concat([df_pos, df_neg])\ndf","1d3e8a38":"df = df.sample(len(df))\ndf","f7c7e344":"df['review'] = (df['airline_sentiment'] == 'positive').astype('int')\ndel df['airline_sentiment']\ndf","0ec64ce0":"import re\ntoken = re.compile('[A-Za-z]+|[,.!?()]')\n\ndef regular_text(text):\n    new_text = token.findall(text)\n    new_text = [word.lower() for word in new_text]\n    return new_text","10884cfd":"df['text'] = df['text'].apply(regular_text)\ndf","3e7b66d6":"word_set = set()\nfor text in df['text']:\n    for word in text:\n        word_set.add(word)\nword_set","3e29e7d2":"len(word_set)","b9cc49e8":"word_list = list(word_set)\n# Use 0 to pad the texts, so the index should start at 1.\nto_index = dict((word_list[i], i + 1) for i in range(7100))\nto_index","a981d3a2":"# return 0 if the word concerned is not in word_list\ntrain_data = df['text'].apply(lambda x:[to_index.get(word, 0) for word in x])","7c70c55b":"max(len(x) for x in train_data)","f6128c77":"max_len = 40\nmax_words = 7100 + 1\n\ntrain_data = tf.keras.preprocessing.sequence.pad_sequences(train_data.values, maxlen=max_len)\ntrain_data.shape","3598e5db":"model = keras.Sequential()\n\nmodel.add(layers.Embedding(max_words, 50, input_length=max_len)) # input_dim, output_dim ...\nmodel.add(layers.LSTM(64))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()","be538ff9":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])","7ddc1453":"history = model.fit(train_data, df.review.values, epochs=10, batch_size=128, validation_split=0.2)","5cbcd124":"acc = history.history['acc']\nval_acc = history.history['val_acc']\n\nplt.figure()\nplt.plot(range(10), acc, 'b', label='acc')\nplt.plot(range(10), val_acc, 'r', label='val_acc')\n\n\n\nplt.title('Training & Validation Acc')\nplt.xlabel('Epoch')\nplt.ylabel('Acc')\n\nplt.legend()\nplt.show()","18f0572c":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure()\nplt.plot(range(10), loss, 'b', label='loss')\nplt.plot(range(10), val_loss, 'r', label='val_loss')\n\n\nplt.title('Training & Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\nplt.legend()\nplt.show()","fec4aacd":"## Use of Token to Regularize Texts","2e13f513":"## Building Model\n\n> Use of Embedding: to Map Texts onto Dense Vectors","1ec5e50a":"## Compile & Run","000c9018":"*Reached a good acc after a few epochs, but resulted in overfitting,*","00cdffa9":"## Padding Texts to Max_Len","50dbdce3":"## Preprocessing"}}