{"cell_type":{"988e79f3":"code","9fc1e4f2":"code","81d6900f":"code","9a4f3bb8":"code","fa07748e":"code","6b40d089":"code","155335e2":"code","f8b645d3":"code","dd038d7d":"code","0144b2d5":"code","97d37910":"code","4522d798":"code","7ac99c95":"markdown"},"source":{"988e79f3":"!pip install transformers@git+https:\/\/github.com\/monuminu\/transformers.git\n!pip install seqeval","9fc1e4f2":"!pip install pyyaml>=5.1, bs4","81d6900f":"import numpy as np\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import LayoutLMv2Tokenizer, LayoutLMv2ForTokenClassification, LayoutLMv2Config\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import Dataset, DataLoader\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport warnings \nwarnings.filterwarnings('ignore')\nfrom PIL import Image\n\n\n\ndef normalize_box(box, width, height):\n    width = int(width)\n    height = int(height)\n    return [\n         int(1000 * (box[0] \/ width)),\n         int(1000 * (box[1] \/ height)),\n         int(1000 * (box[2] \/ width)),\n         int(1000 * (box[3] \/ height)),\n     ]\n\ndef resize_and_align_bounding_box(bbox, original_image, target_size):\n    x_, y_ = original_image.size\n    x_scale = target_size \/ x_ \n    y_scale = target_size \/ y_\n    origLeft, origTop, origRight, origBottom = tuple(bbox)\n    x = int(np.round(origLeft * x_scale))\n    y = int(np.round(origTop * y_scale))\n    xmax = int(np.round(origRight * x_scale))\n    ymax = int(np.round(origBottom * y_scale)) \n    return [x-0.5, y-0.5, xmax+0.5, ymax+0.5]\n\nclass InvoiceDataSet(Dataset):\n    \"\"\"LayoutLM dataset with visual features.\"\"\"\n\n    def __init__(self, df, tokenizer, max_length, target_size, train=True):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_length\n        self.target_size = target_size\n        self.pad_token_box = [0, 0, 0, 0]\n        self.train = train\n\n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, idx):\n        item = self.df.iloc[idx,:].to_dict()        \n        #base_path = data_config.base_image_path\n        original_image = Image.open(os.path.join(base_path , item[\"imageFilename\"])).convert(\"RGB\")\n        # resize to target size (to be provided to the pre-trained backbone)\n        resized_image = original_image.resize((self.target_size, self.target_size))\n        # first, read in annotations at word-level (words, bounding boxes, labels)\n        words = item[\"words\"]\n        unnormalized_word_boxes = item[\"bbox\"]\n        word_labels = item[\"label\"]\n        width = item[\"imageWidth\"]\n        height = item[\"imageHeight\"]\n        normalized_word_boxes = [normalize_box(bbox, width, height) for bbox in unnormalized_word_boxes]\n        assert len(words) == len(normalized_word_boxes)\n\n        # next, transform to token-level (input_ids, attention_mask, token_type_ids, bbox, labels)\n        token_boxes = []\n        unnormalized_token_boxes = []\n        token_labels = []\n        for word, unnormalized_box, box, label in zip(words, unnormalized_word_boxes, normalized_word_boxes, word_labels):\n            word_tokens = self.tokenizer.tokenize(word)\n            unnormalized_token_boxes.extend(unnormalized_box for _ in range(len(word_tokens)))\n            token_boxes.extend(box for _ in range(len(word_tokens)))\n            # label first token as B-label (beginning), label all remaining tokens as I-label (inside)\n            for i in range(len(word_tokens)):\n                if i == 0:\n                    token_labels.extend(['B-' + label])\n                else:\n                    token_labels.extend(['I-' + label])\n        \n        # Truncation of token_boxes + token_labels\n        special_tokens_count = 2 \n        if len(token_boxes) > self.max_seq_length - special_tokens_count:\n            token_boxes = token_boxes[: (self.max_seq_length - special_tokens_count)]\n            unnormalized_token_boxes = unnormalized_token_boxes[: (self.max_seq_length - special_tokens_count)]\n            token_labels = token_labels[: (self.max_seq_length - special_tokens_count)]\n        \n        # add bounding boxes and labels of cls + sep tokens\n        token_boxes = [self.pad_token_box] + token_boxes + [[1000, 1000, 1000, 1000]]\n        unnormalized_token_boxes = [self.pad_token_box] + unnormalized_token_boxes + [[1000, 1000, 1000, 1000]]\n        token_labels = [-100] + token_labels + [-100]\n        \n        encoding = self.tokenizer(' '.join(words), padding='max_length', truncation=True)\n        # Padding of token_boxes up the bounding boxes to the sequence length.\n        input_ids = self.tokenizer(' '.join(words), truncation=True)[\"input_ids\"]\n        padding_length = self.max_seq_length - len(input_ids)\n        token_boxes += [self.pad_token_box] * padding_length\n        unnormalized_token_boxes += [self.pad_token_box] * padding_length\n        token_labels += [-100] * padding_length\n        encoding['bbox'] = token_boxes\n        encoding['labels'] = token_labels\n\n        assert len(encoding['input_ids']) == self.max_seq_length\n        assert len(encoding['attention_mask']) == self.max_seq_length\n        assert len(encoding['token_type_ids']) == self.max_seq_length\n        assert len(encoding['bbox']) == self.max_seq_length\n        assert len(encoding['labels']) == self.max_seq_length\n\n        encoding['resized_image'] = ToTensor()(resized_image)\n        # rescale and align the bounding boxes to match the resized image size (typically 224x224) \n        encoding['resized_and_aligned_bounding_boxes'] = [resize_and_align_bounding_box(bbox, original_image, self.target_size) for bbox in unnormalized_token_boxes]\n        #encoding['unnormalized_token_boxes'] = unnormalized_token_boxes\n        \n        # finally, convert everything to PyTorch tensors \n        for k,v in encoding.items():\n            if k == 'labels':\n                label_indices = []\n                # convert labels from string to indices\n                for label in encoding[k]:\n                    if label != -100:\n                        label_indices.append(data_config.label2id[label])\n                    else:\n                        label_indices.append(label)\n                encoding[k] = label_indices\n            encoding[k] = torch.as_tensor(encoding[k])\n        return encoding","9a4f3bb8":"!wget -O a.zip https:\/\/public.bn.files.1drv.com\/y4mSquuayMrAT271sQVjs1QtBXPBuB0beDzG8_vny1gii4Gaui_vewF5ouw-wBF0tXswCRzAJtfPepvuUpQ4sR-wWIXcQmeKwuDYVhaDOSepxBh4pGhiNRZlkBV_Rkd3ZT6V4ZvCTDgVMq1P7YV6-0HJXVASSStayku2inAYr36FCipdGCNcWl9nmb5IKWwWvUV7d_nrJxpRiOKr45ZTPuIhBG8wnoDDqVRFPb_R98vS8o?access_token=EwAYA61DBAAUmcDj0azQ5tf1lkBfAvHLBzXl5ugAAdEIFwMFcTejmPWay44FcktbpeOdPyjg2EM2Av7T5s%2btU7wCA2oUhl8iBoVl7bbFjG3VHDRDFP2UXLUu2TDQ9IZDCTEIEHJCevSZHUuoGReXzdfRrH3P4Ouy1CW3DM4h1mrZvlkqpZNNSOyhGcrr4W3Ys%2bhSrYmEE93vC34537iJKVlTLOBaAVEdorY3zLR9mnbRhX1\/akIadw12JswgPInfSJoUgrmV9DO9eSEF%2bHtiN%2bRjcbRyLwnm3nSv%2bLJdH4IVQs2N17mAqaYiacIZx3kQ1b43PK7W23GB\/dfsWpoosL9%2byrP7DpJipOWAuFoPMl6idHPburHM5byHAKzglrsDZgAACCccOFX\/04c86AFhflFZdqavTdCxtOuo2bDXq56Hk4EVlRmhBAdGQsS98CifFLwvnmHIWxGZht0Cz1hOSfIF67j2TrP\/%2bSWc6LKLuw3MB38tBNqvgGf7VlNYeOJaarQYJ3LhUsjG994Hx4T9Ro%2bTjTaVbQqrTwqR8XoDwZwftgXXbxHBH9q%2bIpAWTnYOtiwZr7m5531plcdhcMUqSmxqC\/vOLO9IlY\/KkkvOtuLyO\/HiPaBz1EUHkqhxA\/EdKksmQX2MsJNB9yvfMqNM0fCrbhNRRYYpgFU2K7AlBDjthu3vv6IlabxC74WqsLxqET4DQUUm0CVx9TSAMZgsBuKqxZTnK77Rrd86Uajjo5rgLiKQ7K4DVrAV9TlH85\/BORivWCoM99zHq1o\/HxuThDnVseZ9FpRIhJ27hMOukIMgNHie240stssaqNjXffETyBhG7wxF8Zc\/21nBu7CrQpXQWQ8oWpRYJ3yV\/cGHVLy\/rNuhod\/Xn4a\/fJjEjAtT00MxXrQFeYTAH6v4JIvyceUjlTZXgUTu\/LhBKIy%2bXBLGkQy%2bkP1Xu2MVh3Xlfus6g8Q4na0nSABYbAQvUYYSpi9IHvd64pDi\/sTSMXZZxlsKRVi\/FIN7OrAcJ4Ec8nZTYGt6i1HIHRhIU%2b9aFjEfzgaJmaCsHTAC","fa07748e":"!ls \/kaggle\/input\/text-extraction-for-ocr\/imageAndXML_Data","6b40d089":"from bs4 import BeautifulSoup, element\nimport pandas as pd\nimport operator \n\ndef get_get_bbox(bbox):\n    items = bbox.split(\",\")\n    x1 = int(float(items[0]))\n    y1 = int(float(items[1].split(\" \")[0]))\n    x2 = int(float(items[1].split(\" \")[1]))\n    y2 = int(float(items[-1]))\n    return [x1, y1, x2, y2]\n\ndef get_label_bbox(gt_xml_path):\n    with open(gt_xml_path, encoding=\"utf8\")as f:\n        xml_data = f.read()\n    soup = BeautifulSoup(xml_data,'xml')\n    word_list = []\n    words = soup.find_all('TextRegion')\n    word_list = []\n    for word in words:\n        word_dict = {}\n        for content in word.contents:\n            if isinstance(content,element.Tag):\n                word_dict.update(content.attrs)\n        word_dict[\"bbox\"] = get_get_bbox(word_dict[\"points\"])\n        word_dict.pop(\"points\")\n        word_list.append(word_dict)\n    return sorted(word_list, key=lambda x : [x[\"bbox\"][1], x[\"bbox\"][0]])\n\n\ndef get_words_bbox(xml_file_path):\n    with open(xml_file_path, encoding=\"utf8\")as f:\n        xml_data = f.read()\n    soup = BeautifulSoup(xml_data,'xml')\n    word_list = []\n    page = soup.find_all('Page')\n    words = soup.find_all('Word')\n    page_attrs = page[0].attrs\n    for word in words:\n        word_dict = {}\n        for content in word.contents:\n            word_dict.update({\"text\" : word.find(\"Unicode\").get_text()})\n            if isinstance(content,element.Tag):\n                word_dict.update(content.attrs)\n        word_dict[\"bbox\"] = get_get_bbox(word_dict[\"points\"])\n        word_dict.pop(\"points\")\n        word_list.append(word_dict)\n    return page_attrs, sorted(word_list, key=lambda x : [x[\"bbox\"][1], x[\"bbox\"][0]])\n\ndef is_word_bbox_in_label_bbox(word_bbox, label_bbox):\n    x1_w,y1_w,x2_w,y2_w = word_bbox\n    x1_l,y1_l,x2_l,y2_l = label_bbox\n    if x1_w > x1_l and x2_w < x2_l and y1_w > y1_l and y2_w < y2_l:\n        return True\n    else:\n        return False\n\ndef assign_lable_to_word(words_bbox_list, word_label_list):\n    df_label = pd.DataFrame(word_label_list)\n    df_words = pd.DataFrame(words_bbox_list)\n    lst_output = []\n    for index_word, row_word in df_words.iterrows():\n        for index_label, row_label in df_label.iterrows():\n            if is_word_bbox_in_label_bbox(row_word[\"bbox\"], row_label[\"bbox\"]):\n                row_dict = row_word.to_dict()\n                row_dict[\"label\"] = row_label[\"value\"]\n                lst_output.append(row_dict)    \n    return pd.DataFrame(lst_output)\n\n\nif __name__ == \"__main__\":\n    import glob\n    lst_output = []\n    for file in glob.glob(\"\/kaggle\/input\/text-extraction-for-ocr\/ImageAndXML_Data\/*.tif\"):\n        try:\n            ocr_xml_file = file.replace(\".tif\", \"_ocr.xml\")\n            page_attrs, words_bbox_list = get_words_bbox(ocr_xml_file)\n            label_xml_file = file.replace(\".tif\", \"_gt.xml\")\n            word_label_list = get_label_bbox(label_xml_file)\n            df_word_lable = assign_lable_to_word(words_bbox_list, word_label_list)\n            page_attrs.update({\"words\" : df_word_lable.text.tolist(), \"bbox\" : df_word_lable.bbox.tolist(), \"label\" : df_word_lable.label.tolist()})\n            lst_output.append(page_attrs)\n        except:\n            print(label_xml_file)\n    df = pd.DataFrame(lst_output)[[\"imageFilename\",\"imageHeight\", \"imageWidth\", \"words\", \"bbox\", \"label\"]]\n    df.to_pickle(\"\/kaggle\/working\/data.pkl\")","155335e2":"base_path = \"\/kaggle\/input\/text-extraction-for-ocr\/ImageAndXML_Data\"\ndata = pd.read_pickle(\"\/kaggle\/working\/data.pkl\")\ndata.head()","f8b645d3":"class data_config:\n    labels = np.unique([item for sublist in data.label for item in sublist]).tolist()\n    labels = sum([[\"B-\" + item, \"I-\" + item] for item in np.unique(labels)], [])\n    num_labels = len(labels)\n    id2label = {v: k for v, k in enumerate(labels)}\n    label2id = {k: v for v, k in enumerate(labels)}","dd038d7d":"model_path = 'microsoft\/layoutlmv2-base-uncased'\nconfig = LayoutLMv2Config.from_pretrained(model_path, num_labels=data_config.num_labels, id2label = data_config.id2label, label2id = data_config.label2id)\ntokenizer = LayoutLMv2Tokenizer.from_pretrained(model_path)\nmodel = LayoutLMv2ForTokenClassification.from_pretrained(model_path, config = config)\nmodel.to(device)","0144b2d5":"from sklearn.model_selection import train_test_split\ntrain, valid = train_test_split(data, test_size = 0.2)\n\ntrain_dataset = InvoiceDataSet(df = train, tokenizer = tokenizer, max_length = 512, target_size = 224, train=True)\ntrain_dataloader = DataLoader(train_dataset, batch_size=5)\n\nvalid_dataset = InvoiceDataSet(df = valid, tokenizer = tokenizer, max_length = 512, target_size = 224, train=False)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=5)","97d37910":"from transformers import AdamW\nfrom tqdm.notebook import tqdm\nimport numpy as np\nfrom seqeval.metrics import (\n    classification_report,\n    f1_score,\n    precision_score,\n    recall_score,\n)\nimport torch\n\ndef train_fn(train_dataloader, model, optimizer):\n    tk0 = tqdm(train_dataloader, total = len(train_dataloader))\n    for bi, batch in enumerate(tk0):\n        input_ids=batch['input_ids'].to(device)\n        bbox=batch['bbox'].to(device)\n        attention_mask=batch['attention_mask'].to(device)\n        token_type_ids=batch['token_type_ids'].to(device)\n        labels=batch['labels'].to(device)\n        resized_images = batch['resized_image'].to(device) \n        resized_and_aligned_bounding_boxes = batch['resized_and_aligned_bounding_boxes'].to(device) \n        outputs = model(image = resized_images,input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \ndef eval_fn(eval_dataloader, model):\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    preds = None\n    out_label_ids = None\n    model.eval()\n    tk0 = tqdm(eval_dataloader, total = len(eval_dataloader))\n    for bi, batch in enumerate(tk0):\n        with torch.no_grad():\n            input_ids=batch['input_ids'].to(device)\n            bbox=batch['bbox'].to(device)\n            attention_mask=batch['attention_mask'].to(device)\n            token_type_ids=batch['token_type_ids'].to(device)\n            labels=batch['labels'].to(device)\n            resized_images = batch['resized_image'].to(device) \n            resized_and_aligned_bounding_boxes = batch['resized_and_aligned_bounding_boxes'].to(device)\n            outputs = model(image = resized_images,input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,labels=labels)\n            tmp_eval_loss = outputs.loss\n            logits = outputs.logits\n            eval_loss += tmp_eval_loss.item()\n            nb_eval_steps += 1\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = labels.detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(\n                    out_label_ids, labels.detach().cpu().numpy(), axis=0\n                )\n    eval_loss = eval_loss \/ nb_eval_steps\n    preds = np.argmax(preds, axis=2)\n    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n    for i in range(out_label_ids.shape[0]):\n        for j in range(out_label_ids.shape[1]):\n            if out_label_ids[i, j] != -100:\n                out_label_list[i].append(config.id2label[out_label_ids[i][j]])\n                preds_list[i].append(config.id2label[preds[i][j]])\n\n    results = {\n        \"loss\": eval_loss,\n        \"precision\": precision_score(out_label_list, preds_list),\n        \"recall\": recall_score(out_label_list, preds_list),\n        \"f1\": f1_score(out_label_list, preds_list),\n    }\n    return results","4522d798":"MODEL_PATH =\"\/kaggle\/working\/pytorch_model.bin\"\noptimizer = AdamW(model.parameters(), lr=5e-5)\nglobal_step = 0\nbest_f1_score = 0\nfor epoch in range(5):\n    train_fn(train_dataloader, model, optimizer)\n    current_f1_score = eval_fn(valid_dataloader, model)\n    if current_f1_score[\"f1\"] > best_f1_score:\n        torch.save(model.state_dict(), MODEL_PATH)\n        best_f1_score = current_f1_score[\"f1\"]\n    print(\"best_f1_score :\", best_f1_score)","7ac99c95":"<a href=\"https:\/\/colab.research.google.com\/github\/monuminu\/ocr_extract\/blob\/master\/LayoutLMv2.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>"}}