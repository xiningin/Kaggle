{"cell_type":{"9fb5c3a6":"code","aec43e6e":"code","a786a74e":"code","07a6fc26":"code","f17c9453":"code","769a1899":"code","b6987d65":"code","90184cb5":"code","030305af":"code","7fbba146":"code","32e8870b":"code","ce5c1cdf":"code","f2f77d7d":"code","a3e0add7":"code","70e3d7c3":"code","dc389540":"code","919d21cf":"code","1300a0bb":"code","37f894af":"code","66c4ada5":"code","2ccadd8b":"code","774f1f47":"code","2b735260":"code","872fde4a":"code","88418717":"code","38d12c92":"code","eba3da65":"code","738dec81":"code","8cf81f75":"code","7156ef93":"code","3bd654a1":"code","20d98761":"code","9ef7b9d9":"code","59c43a9d":"code","1148d56c":"code","8f3cd062":"markdown","8eb5394a":"markdown","9b3e296c":"markdown","7b626114":"markdown","059451d6":"markdown","bf5067fc":"markdown","a5368330":"markdown","30f6fc22":"markdown","19ea4255":"markdown","d4191ac2":"markdown","e3cec6d3":"markdown","e638a81b":"markdown","c5af6f8f":"markdown","4f5e4c53":"markdown","24a2352d":"markdown","f4f2002c":"markdown","cf60f2d9":"markdown","f6bd454a":"markdown","41ad8461":"markdown","ab8a321d":"markdown","e1898c7a":"markdown","cdc2b77f":"markdown","01a6ff6f":"markdown","3f2065bb":"markdown","33222218":"markdown","bbbc523e":"markdown","a43eb6d4":"markdown","bae7e241":"markdown","c578c9f3":"markdown","3ac55ee8":"markdown","f02df894":"markdown","6e7f7617":"markdown","3e45c722":"markdown","9d8d3699":"markdown","295fce92":"markdown","a78c43da":"markdown","d2805f54":"markdown","05ff3cbf":"markdown","7d4c5e26":"markdown","46717087":"markdown","9773098c":"markdown","3daf60be":"markdown","1912f7ca":"markdown"},"source":{"9fb5c3a6":"# Imports and setup\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #biblioteca gr\u00e1fica para mostrar os gr\u00e1ficos\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nDF = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","aec43e6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#import networkx as nx\n\n#import plotly.express as px\n#import plotly.figure_factory as ff\n#from plotly.graph_objs import graph_objs\n#import plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n#import seaborn as sns\n\nimport itertools\nimport time\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, Lasso, LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n#from xgboost.sklearn import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom mlxtend.plotting import plot_decision_regions\n#from sklearn.gaussian_process import GaussianProcessClassifier\n#from sklearn.gaussian_process.kernels import RBF\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# Any results you write to the current directory are saved as output.","a786a74e":"for c,d in zip(DF.columns, DF.dtypes):\n    print(\"Column {}, type {}\".format(c,d))\nprint(\"Number of missing values: {}.\".format(DF.isna().sum().sum()))\nprint(\"Number of duplicated rows: {}.\".format(DF.shape[0] - DF.drop_duplicates().shape[0]))\nDF.drop_duplicates(inplace=True)\nDF.describe()","07a6fc26":"# Analysing the features distribution on graphic way\n\ndef histogram(data, title, ax): #index\n    n_bins = 30\n    ax.hist(data, n_bins, density=True, histtype='bar')\n    ax.legend(prop={'size': 8})\n    ax.set_title(title)\n\nfig, axes = plt.subplots(nrows=4, ncols=3, figsize=(15,15))\nfor i in range(4):\n    for j in range(3):\n        idx_col = i*3+j\n        if(idx_col >= DF.shape[1]):\n            continue\n        col = list(DF.columns)[idx_col]\n        print(col)\n        #axs[i][j] = histogram(DF[col])\n        ax = axes[i][j]\n        histogram(DF[col], col, ax)\n\nfig.tight_layout()\nplt.show()","f17c9453":"corr = DF.corr()\n#Plot Correlation Matrix using Matplotlib\nplt.figure(figsize=(7, 5))\nplt.imshow(corr, cmap='YlOrBr', interpolation='none', aspect='auto')\nplt.colorbar()\nplt.xticks(range(len(corr)), corr.columns, rotation='vertical')\nplt.yticks(range(len(corr)), corr.columns);\nplt.suptitle('Correlation between variables', fontsize=15, fontweight='bold')\nplt.grid(False)\nplt.show()","769a1899":"def correlation_pairs(df, threshold, sort=False):\n    \"\"\"\n        Function to filter pair of features of a given DataFrame :df:\n        that area correlated at least at :threshold:\n    \"\"\"\n    pairs = []\n    corr = df.corr()\n    corr = corr.reset_index()\n    for i in range(corr.shape[0]):\n        for j in range(corr.shape[1]):\n            if(j < i):\n                col = corr.columns[j+1]\n                corr_val = corr.loc[i][col]\n                if(abs(corr_val) > threshold):\n                    #print(i, j, corr.loc[i]['index'], col, corr_val)\n                    pairs.append((corr.loc[i]['index'], col, corr_val))\n    return pairs\n\ncorrelation_pairs(DF, 0.3)","b6987d65":"DF_qlt7 = DF[DF['quality'] < 7]\nDF_qge7 = DF[DF['quality'] >= 7]","90184cb5":"print(DF_qlt7.shape)\nDF_qlt7.describe()","030305af":"print(DF_qge7.shape)\nDF_qge7.describe()","7fbba146":"qlt7_stats = DF_qlt7.describe().loc[['mean', 'std']]\nqge7_stats = DF_qge7.describe().loc[['mean', 'std']]","32e8870b":"round(((qlt7_stats - qge7_stats) \/ qge7_stats) * 100, 2)","ce5c1cdf":"DF_qlt7_eq = DF_qlt7.sample(n=DF_qge7.shape[0], random_state=1)\nqlt7_eq_stats = DF_qlt7_eq.describe().loc[['mean', 'std']]\nround(((qlt7_eq_stats - qge7_stats) \/ qge7_stats) * 100, 2)","f2f77d7d":"def boxplot(data, title, ax): #index\n    green_diamond = dict(markerfacecolor='g', marker='D')\n    ax.set_title(title)\n    ax.boxplot(data, flierprops=green_diamond)\n\nfig, axes = plt.subplots(nrows=4, ncols=3, figsize=(15,15))\nfor i in range(4):\n    for j in range(3):\n        idx_col = i*3+j\n        if(idx_col >= DF.shape[1]):\n            continue\n        col = list(DF.columns)[idx_col]\n        print(col)\n        #axs[i][j] = histogram(DF[col])\n        ax = axes[i][j]\n        boxplot(DF[col], col, ax)\n\nfig.tight_layout()\nplt.show()","a3e0add7":"# In order to detect and remove outliers we will use the zscore function of scipy.stats package\nfrom scipy.stats import zscore\nfrom functools import reduce\n\nz = np.abs(zscore(DF))\nthreshold = 3 # our threshold will be 3 * std_dev\nzmask = abs(z) > 3\nzmask_per_line = [reduce(lambda curr, res : curr or res, zmask[i]) for i in range(len(zmask))]\nprint(\"Using Z-score to remove outliers we would remove ~ {} % of our data.\".format(round(sum(zmask_per_line) \/ DF.shape[0] * 100), 2))\n\nQ1 = DF.quantile(0.25)\nQ3 = DF.quantile(0.75)\nIQR = Q3 - Q1\niqrmask = (DF < (Q1 - 1.5 * IQR)) |(DF > (Q3 + 1.5 * IQR))\niqrmask_per_line = list(iqrmask.apply(lambda row : reduce(lambda curr, res : curr or res, row), axis=1))\nprint(\"Using IQR to remove outliers we would remove ~ {} % of our data.\".format(round(sum(iqrmask_per_line) \/ DF.shape[0] * 100), 2))\n\nzmask_per_line = [not z for z in zmask_per_line]\n\n# So we use Z-score to reduce the data loss\nDF = DF[zmask_per_line]","70e3d7c3":"# First of all, we need to split data into two sets:\n# X -> With all dependent variables\n# y -> With target-feature\n\nX = DF[DF.columns[:-1]]\ny = DF[DF.columns[-1]]","dc389540":"from sklearn.preprocessing import StandardScaler\n\ncolumns = X.columns\n\n# Prepate the transformation function\nscaler = StandardScaler().fit(X)\n# Standardize data (mean=0, variance = 1)\nX = pd.DataFrame(scaler.transform(X), columns=columns)","919d21cf":"X.describe()","1300a0bb":"pca = PCA()\npca_result = pca.fit_transform(X)\nvar_exp = pca.explained_variance_ratio_\n\nattr_x_var_exp = sorted(list(zip(X.columns, var_exp)), key=lambda x: x[1])\nimportances = [var_exp for _, var_exp in attr_x_var_exp]\nattr_rank = [attr for attr, _ in attr_x_var_exp]\n\nfor attr, var_exp in attr_x_var_exp:\n    print(attr, var_exp)\n\nplt.title('Feature Importances')\nplt.tight_layout()\nplt.barh(range(len(importances)), importances, color='b', align='center')\nplt.yticks(range(len(importances)), attr_rank, fontsize=25)\nplt.xlabel('Relative Importance',fontsize=25)\nplt.xticks(color='k', size=15)\nplt.yticks(color='k', size=15)\nplt.xlim([0.0, 1])\nplt.show()","37f894af":"pca = PCA().fit(X)\nplt.figure(figsize=(8,5))\nncomp = np.arange(1, np.shape(X)[1]+1)\nplt.plot(ncomp, np.cumsum(pca.explained_variance_ratio_), 'ro-')\nplt.xlabel('number of components', fontsize=15)\nplt.ylabel('cumulative explained variance', fontsize=15);\nplt.xticks(color='k', size=15)\nplt.yticks(color='k', size=15)\nplt.grid(True)\nplt.show(True)","66c4ada5":"features_names = ['fixed acidity', 'volatile acidity']\nX_ = X[features_names]\nclass_labels = np.unique(y)\n#Plotting\nall_colors = ['red', 'blue', 'orange', 'purple', 'green', 'yellow', 'black']\ncolors = all_colors[:len(class_labels)]\nfor i, c in enumerate(class_labels):\n    ind = np.where(y == c)\n    # mostra os grupos com diferentes cores\n    plt.scatter(X_.loc[ind][X.columns[0]], X_.loc[ind][X.columns[1]], color = colors[i], label = c)\nplt.legend()\nplt.show()","2ccadd8b":"def test_models(models, cv, X, y, scoring=None):\n    for i in range(len(models)):\n        print(\"Testing \", models[i]['name'])\n        \n        #if('multiclassTransformation' in models[i] and models[i]['multiclassTransformation']):\n            # This line is required when using classification models for multi-class classification\n        #    y_ = preprocessing.label_binarize(y, classes=list(y.unique()))\n        #else:\n        #    y_ = y\n\n        y_ = y\n        model = models[i]['model']\n                 \n        if('multiclassClassifier' in models[i]):\n            multiclassClassifier = models[i]['multiclassClassifier']\n            if(multiclassClassifier != None):\n                #print(multiclassClassifier)\n                model = multiclassClassifier(models[i]['model'])\n        \n        clf = GridSearchCV(model, models[i]['params'], cv=cv, scoring=scoring, verbose=2, n_jobs=-1)\n        clf.fit(X, y_)\n        models[i]['exec_time'] = (sum(clf.cv_results_['mean_fit_time']) * cv)\n        models[i]['best_params'] = clf.best_params_\n        models[i]['best_model'] = clf.best_estimator_ \n        models[i]['best_score'] = clf.best_score_","774f1f47":"lb = preprocessing.LabelBinarizer()\ny_encoded = lb.fit_transform(y)","2b735260":"X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size = 0.2, random_state = 42)","872fde4a":"# ALL\n# ---------------------------------------------------------------------\nknn_params = {\n    'n_neighbors' : list(range(6,50)),\n    'weights' : ['uniform', 'distance'],\n    'p' : [1, 2]\n}\n# ---------------------------------------------------------------------\nsvc_params = {\n    'estimator__C' : [0.01, 0.1, 1, 10],\n    'estimator__gamma' : ['auto', 'scale'],\n    'estimator__class_weight' : [None, 'balanced'],\n}\n# ---------------------------------------------------------------------\ndt_params = {\n    'max_depth' : [1, 3, 5, 8, 13, 21, 34],\n    'criterion' : ['gini', 'entropy'],\n    'splitter' : ['best', 'random']\n}\n# ---------------------------------------------------------------------\ngnb_params = {}\n# ---------------------------------------------------------------------\nrf_params = {\n    'max_depth' : [1, 3, 5, 7, 11, 21],\n    'n_estimators' : [3, 10, 20, 50, 100, 200],\n    'max_features' : [2, 3, 5, 7, 9]\n}\n# ---------------------------------------------------------------------\n\nModels = [\n    {'name': \"Dummy\", 'model' : DummyClassifier(strategy='most_frequent', random_state=0), 'params' : {}, 'multiclassTransformation' : True, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"KNN\", 'model' : KNeighborsClassifier(), 'params' : knn_params, 'multiclassTransformation' : True, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"DecisionTreeClassifier\", 'model' : DecisionTreeClassifier(), 'params' : dt_params, 'multiclassTransformation' : True, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"GaussianNB\", 'model' : GaussianNB(), 'params' : gnb_params, 'multiclassTransformation' : True, 'multiclassClassifier' : OneVsRestClassifier, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"SVC\", 'model' : SVC(), 'params' : svc_params, 'multiclassTransformation' : True, 'multiclassClassifier' : OneVsRestClassifier, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"RandomForestClassifier\", 'model' : RandomForestClassifier(), 'params' : rf_params, 'multiclassTransformation' : True, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n]\n\ntest_models(Models, 10, X_train, y_train, scoring=make_scorer(accuracy_score))\nModels","88418717":"from sklearn.metrics import confusion_matrix, classification_report\n\nfor model in Models:\n    clf = model['best_model']\n    name = model['name']\n    \n    predictions_clf = clf.predict(X_test)\n    predictions_clf_decoded = lb.inverse_transform(predictions_clf)\n    y_test_decoded = lb.inverse_transform(y_test)\n    print(name ,classification_report(y_test_decoded, predictions_clf_decoded))","38d12c92":"features_names = ['fixed acidity', 'volatile acidity']\nX_ = X[features_names]\n\nclassifiers = [x['best_model'] for x in Models]\nmodel_names = [x['name'] for x in Models]\n\n# We save this variable to restore later\nmax_features = Models[5]['best_model'].max_features\n# changing the best model of RandomForests to match with new number of features\nModels[5]['best_model'].max_features = 2\n\nfor model in Models:\n    clf = model['best_model']\n    name = model['name']\n    \n    clf.fit(X_, y)\n    \n    plot_decision_regions(np.array(X_), np.array(y), clf=clf, legend=2)\n    \n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.title('Decision Regions for ' + name)\n    \n    plt.show()","eba3da65":"rf_params = {\n    'max_depth' : [1, 3, 5, 7, 11, 21],\n    'n_estimators' : [3, 10, 20, 50, 100, 200],\n    'max_features' : [2, 3, 5, 7, 9]\n}\n# ---------------------------------------------------------------------\n\nRFModel = [\n    {'name': \"RandomForestClassifier\", 'model' : RandomForestClassifier(), 'params' : rf_params, 'multiclassTransformation' : True, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n]\n\ntest_models(RFModel, 10, X, y, scoring=make_scorer(accuracy_score))","738dec81":"features_names = DF.columns\nimportances = RFModel[0]['best_model'].feature_importances_\nindices = np.argsort(importances)\nlmeas_order = []\nfor i in indices:\n    lmeas_order.append(features_names[i])\nplt.figure(figsize=(10,6))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), lmeas_order, fontsize=15)\nplt.xlabel('Relative Importance',fontsize=15)\nplt.xticks(color='k', size=15)\nplt.yticks(color='k', size=15)\nplt.show()","8cf81f75":"# =====================================================================================================\nX = DF[DF.columns[:-1]]\ny = DF[DF.columns[-1]].copy()\n\n# =====================================================================================================\n\ncolumns = X.columns\nscaler = StandardScaler().fit(X)\nX = pd.DataFrame(scaler.transform(X), columns=columns)\n\n# =====================================================================================================\n\ny[DF[DF.columns[-1]] > 6.5] = 1\ny[DF[DF.columns[-1]] <= 6.5] = 0\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# =====================================================================================================\n\n# ALL\n# ---------------------------------------------------------------------\nknn_params = {\n    'n_neighbors' : list(range(6,50)),\n    'weights' : ['uniform', 'distance'],\n    'p' : [1, 2]\n}\n# ---------------------------------------------------------------------\nsvc_params = {\n    'estimator__C' : [0.01, 0.1, 1, 10],\n    'estimator__gamma' : ['auto', 'scale'],\n    'estimator__class_weight' : [None, 'balanced'],\n}\n# ---------------------------------------------------------------------\ndt_params = {\n    'max_depth' : [1, 3, 5, 8, 13, 21, 34],\n    'criterion' : ['gini', 'entropy'],\n    'splitter' : ['best', 'random']\n}\n# ---------------------------------------------------------------------\ngnb_params = {}\n# ---------------------------------------------------------------------\nrf_params = {\n    'max_depth' : [1, 3, 5, 7, 11, 21],\n    'n_estimators' : [3, 10, 20, 50, 100, 200],\n    'max_features' : [2, 3, 5, 7, 9]\n}\n# ---------------------------------------------------------------------\n\nModels = [\n    {'name': \"Dummy\", 'model' : DummyClassifier(strategy='most_frequent', random_state=0), 'params' : {}, 'multiclassTransformation' : True, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"KNN\", 'model' : KNeighborsClassifier(), 'params' : knn_params, 'multiclassTransformation' : True, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"DecisionTreeClassifier\", 'model' : DecisionTreeClassifier(), 'params' : dt_params, 'multiclassTransformation' : True, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"GaussianNB\", 'model' : GaussianNB(), 'params' : gnb_params, 'multiclassTransformation' : True, 'multiclassClassifier' : OneVsRestClassifier, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"SVC\", 'model' : SVC(), 'params' : svc_params, 'multiclassTransformation' : True, 'multiclassClassifier' : OneVsRestClassifier, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"RandomForestClassifier\", 'model' : RandomForestClassifier(), 'params' : rf_params, 'multiclassTransformation' : True, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n]\n\ntest_models(Models, 10, X_train, y_train, scoring=make_scorer(accuracy_score))\n\n# =====================================================================================================\n\nfor model in Models:\n    clf = model['best_model']\n    name = model['name']\n    \n    predictions_clf = clf.predict(X_test)\n    print(name ,classification_report(y_test, predictions_clf))\n","7156ef93":"features_names = ['fixed acidity', 'volatile acidity']\nX_ = X[features_names]\n\nclassifiers = [x['best_model'] for x in Models]\nmodel_names = [x['name'] for x in Models]\n\n# We save this variable to restore later\nmax_features = Models[5]['best_model'].max_features\n# changing the best model of RandomForests to match with new number of features\nModels[5]['best_model'].max_features = 2\n\nfor model in Models:\n    clf = model['best_model']\n    name = model['name']\n    \n    clf.fit(X_, y)\n    \n    plot_decision_regions(np.array(X_), np.array(y), clf=clf, legend=2)\n    \n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.title('Decision Regions for ' + name)\n    \n    plt.show()","3bd654a1":"rf_params = {\n    'max_depth' : [1, 3, 5, 7, 11, 21],\n    'n_estimators' : [3, 10, 20, 50, 100, 200],\n    'max_features' : [2, 3, 5, 7, 9]\n}\n# ---------------------------------------------------------------------\n\nRFModel = [\n    {'name': \"RandomForestClassifier\", 'model' : RandomForestClassifier(), 'params' : rf_params, 'multiclassTransformation' : True, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n]\n\ntest_models(RFModel, 10, X, y, scoring=make_scorer(accuracy_score))\n\n# =====================================================================================================\n\nfeatures_names = DF.columns\nimportances = RFModel[0]['best_model'].feature_importances_\nindices = np.argsort(importances)\nlmeas_order = []\nfor i in indices:\n    lmeas_order.append(features_names[i])\nplt.figure(figsize=(10,6))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), lmeas_order, fontsize=15)\nplt.xlabel('Relative Importance',fontsize=15)\nplt.xticks(color='k', size=15)\nplt.yticks(color='k', size=15)\nplt.show()","20d98761":"from sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.metrics import mean_squared_error\n\nX_for_regression = DF[list(DF.columns[:10]) + [DF.columns[-1]]]\n\ncolumns = X_for_regression.columns\n\nscaler = StandardScaler().fit(X)\nX_for_regression = pd.DataFrame(scaler.transform(X), columns=columns)\ny_for_regression = DF[DF.columns[10]]\n\nfrom sklearn.model_selection import train_test_split\n# Split the data into training and testing sets\np = 0.3 # fracao de elementos no conjunto de teste\nx_train, x_test, y_train, y_test = train_test_split(X_for_regression, y_for_regression, test_size = p, random_state = 42)","9ef7b9d9":"lm = LinearRegression()\nlm.fit(x_train, y_train)\n\ny_pred = lm.predict(x_test)\n\nfrom sklearn.metrics import r2_score\nR2 = r2_score(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nprint('R2 Coefficient: {} and MSE: {}', round(R2,2), round(mse,2))","59c43a9d":"import matplotlib.pyplot as plt\n\nfig = plt.figure()\nl = plt.plot(y_pred, y_test, 'bo')\nplt.setp(l, markersize=10)\nplt.setp(l, markerfacecolor='C0')\n\nplt.ylabel(\"y\", fontsize=15)\nplt.xlabel(\"Prediction\", fontsize=15)\n\n# show original and predicted values\nxl = np.arange(min(y_test), 1.2*max(y_test),(max(y_test)-min(y_test))\/10)\nyl = xl\nplt.plot(xl, yl, 'r--')\n\nplt.show(True)","1148d56c":"np.random.seed(42)\n\nvRDGRmse = []\nvLASRmse = []\nvalpha = []\n# varying values of alpha\nfor alpha in np.arange(1,30,1):\n    \n    ridge = Ridge(alpha = alpha, random_state=101, normalize=True)\n    ridge.fit(x_train, y_train)             # Fit a ridge regression on the training data\n    y_pred = ridge.predict(x_test)           # Use this model to predict the test data\n    rmse = mean_squared_error(y_test, y_pred)\n    vRDGRmse.append(rmse)\n    \n    lasso = Lasso(alpha = alpha, random_state=101, normalize=True) # normalize=True\n    lasso.fit(x_train, y_train)             # Fit a lasso regression on the training data\n    y_pred = lasso.predict(x_test)           # Use this model to predict the test data\n    rmse = mean_squared_error(y_test, y_pred)\n    vLASRmse.append(rmse)\n    \n    valpha.append(alpha)\n    \nplt.plot(valpha, vRDGRmse, '-ro')\nplt.plot(valpha, vLASRmse, '-bo')\nplt.xlabel(\"alpha\", fontsize=15)\nplt.ylabel(\"Mean Squared Error\", fontsize=15)\nplt.legend(['Ridge', 'Lasso'])\nplt.show(True)","8f3cd062":"And again, showing the feature importance for Random Forest algorithm:","8eb5394a":"Now we can see what are the most correlated variables and confirm some of our initial toughts (citric acid - fixed acidity - volatile acidity and total sulfur dioxide - free sulfur dioxide correlations), and see new and perhaps more interesting correlations like: **quality - volatile acidity - alcohol**","9b3e296c":"Now, in order to know better the dataset's columns we could take more information on Kaggle:\n\n**fixed acidity:** most acids involved with wine or fixed or nonvolatile (do not evaporate readily)\n\n**volatile acidity:** the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste\n\n**citric acid:** found in small quantities, citric acid can add 'freshness' and flavor to wines\n\n**residual sugar:** the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram\/liter and wines with greater than 45 grams\/liter are considered sweet\n\n**chlorides:** the amount of salt in the wine\n\n**free sulfur dioxide:** the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine\n\n**total sulfur dioxide:** amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine\n\n**density:** the density of water is close to that of water depending on the percent alcohol and sugar content\n\n**pH:** describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale\n\n**sulphates:** a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant\n\n**alcohol:** the percent alcohol content of the wine\n\n**quality:** output variable (based on sensory data, score between 0 and 10)","7b626114":"In previous analysis we could not distinguish very well the correlation of the variable, so in order to take a better look, we will approach in a text-based solution:","059451d6":"In order to eliminate the different number of samples effect, we may take a sample of the grater set with same size of the other set.","bf5067fc":"In our project we already use one tool of data cleasing (drop_duplicates) to delete duplicated rows on our dataset. Now we gonna use detection and treatment of outliers (if present).\n\nAfter that, we gonna normalize data, to reduce the effect of different scales between features. \n\nThe other techniquies does'nt contribute too much for this project so we will not gonna use.","a5368330":"This work is divided as follow:\n\n<a href=\"#intro\">1. Introduction<\/a>\n\n<a href=\"#eda\">2. Exploratory Data Analysis<\/a>\n\n<a href=\"#data-cleansing\">3. Data Cleansing and Data Treatment<\/a>\n\n<a href=\"#ml-models\">4. Machine Learning Models<\/a>\n\n- <a href=\"#classification\">4.1. Classification<\/a>\n\n    - - <a href=\"#scenario1\">4.1.1. Scenario 1 - Multi-class problem<\/a>\n    \n    - - <a href=\"#scenario2\">4.1.2. Scenario 2 - 2-classes problem<\/a>\n    \n- <a href=\"#regression\">4.2. Regression<\/a>\n    \n<a href=\"#conclusions\">5. Conclusions<\/a>\n\n<a href=\"#references\">6. References<\/a>\n","30f6fc22":"As we could see, the f1-score improved a lot!\n\nSo, train a model to classify 2 classes is way better, simple and faster than multi-class classification.\n\nAs in, this particular case, we can do the two classifications, we gonna choose by the 2-classes classification.","19ea4255":"We gonna train separated only the RandomForest Algorithm and plot the feature importance for the best choosen model.","d4191ac2":"We can also analyze distribution of data in a graphic way:","e3cec6d3":"Now using the two most relevant attributes to plot data:","e638a81b":"By looking at the graph above, we may lead to conclude that as we increase alpha *ridge regression* have a worse performance, and lasso improve, but not considerably.","c5af6f8f":"Let's see now, how our dataset was after data treatment:","4f5e4c53":"To test some concepts of regression, we gonna try to predict the values of alcohol.\n\nTo do that, we gonna compare the models Ridge and Lasso Regression","24a2352d":"## <a id='ml-models'>4. Machine Learning Models<\/a>","f4f2002c":"## <a id='references'>6. References:<\/a>\n\n\n<a id='1'><\/a>\n[1. P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0167923609001377?via%3Dihub)\n\n<a id='2'><\/a>\n[2. Master Business Administration in Data Science - USP - ICMC](http:\/\/cemeai.icmc.usp.br\/MBA\/)\n\n<a id='3'><\/a>\n[3. http:\/\/www3.dsi.uminho.pt\/pcortez\/wine\/ accessed on: March 03 - 2020](http:\/\/www3.dsi.uminho.pt\/pcortez\/wine\/)\n\nOther very helpfull links:\n\n- [Matplotlib - docs](https:\/\/scikit-learn.org\/stable\/index.html)\n\n- [Scikit-Learn - docs](https:\/\/matplotlib.org\/)\n\n- [Towards Data Science - Removing outliers](https:\/\/towardsdatascience.com\/ways-to-detect-and-remove-the-outliers-404d16608dba)\n\n### Acknowledges\n\nI would like to thank all people that have contribuited to this present work.\n\nPaulo Cortez, Ant\u00f3nio Cerdeira, Fernando Almeida, Telmo Matos, Jos\u00e9 Reis (authors of Modeling wine preferences by data mining from physicochemical properties) who gave us the database and provided me a copy of they amazing paper!\n\nThe professors, tutors and classmates for all concepts, help and tips.","cf60f2d9":"As we could see, all features are numeric (?), and that is no line with null values (all columns have same number of values, which is the number of rows in dataset).\n\nWe also could see, that apparently some features are correlated, as we see in groups like: (fixed acidity, volatile acidity and citric acid) and (free sulfur dioxide, total sulfur dioxide and sulphates).\n\nOne last thing we could see is that Wine Quality is defined as a integer number varying from 3 to 8.","f6bd454a":"Now we gonna use our custom-made function test_models to test each model and compare them.\nTo test each model we gonna use a method called Grid Search Cross Validation that will tune models on selected parameters (using a method named all-versus-all) and evaluate them using the Cross Validation method.\nAfter that, the best model parameters will be selected for each model.\nAnd to visualize the adjusted models we gonna to plot the decision boundaries for each model.","41ad8461":"The most popular and efficient models are (but not limited to):\n\n- KNN (K-Nearest Neighboors)\n- Decision Trees\n- Naive-Bayes\n- SVM (Support Vector Machines)\n- Random Forests\n\nAnd we gonna use, tunning and compare them in order to predict the *target-feature:* ***'quality'***\n\nAnd, in order to test some concepts we also gonna use **Linear Regression Models** like **Lasso** and **Ridge** Regressions to predict the alcohol percentage.","ab8a321d":"## <a id='conclusions'>5. Conclusions<\/a>","e1898c7a":"## <a id='data-cleansing'>3. Data Cleansing and Data Treatment<\/a>","cdc2b77f":"In this first scenario, we gonna show, detailed way, how we train, test and evaluate our models (with all extra-dificulties included by having a multi-class problem)","01a6ff6f":"One last thing we may do, just to test, is to consider the separation of wines in a binary way like the author comment:\n\n```\n$quality$ > 6.5 => \"good\"\nTRUE => \"bad\"\n```\n\nconsidering this, we have:","3f2065bb":"Some of the most efficient techniquies of Data Cleansing include:\n\n    Manual feature deletion (Delete features that do not contribute with model understanding. e.g.: Name of Wine)\n    Data Integration (When you have multiple sources of data you may want to merge them in some way in order to enrich your data)\n    Data Sampling (When you have too much samples, you may want to not use some samples to redece the model complexity, and thus the model overfitting) <- review\n    Dimensionality Reduction (the same ideia behind Data Sampling, but now applied to columns)\n    unbalanced samples or to balance data\n    Data Balance (In Classification, when you have much more samples of one class than the ther, you have unballanced data. This could lead to difficulties to classification, because, even a dummy classifier (Classifier which always classify a sample by the most frequent class in dataset) will have a high accurate score. So Balance data means the process of take same amout of samples by class).\n    Data Cleansing (Aims to resolve some commum problems with data, like: inconsistence, redundancy, missing values, outliers and etc.).\n    Data Transformation (process to change data in order to equal scale(e.g.: standartization, min-max saclling, softmax), ?(e.g.: translating), ?(e.g.: enconding), etc).","33222218":"# Red Wine Quality\n\nThis DataSet is provided by [Cortez et al., 2009](https:\/\/doi.org\/10.1016\/j.dss.2009.05.016) and is a part of the work ***Modeling wine preferences by data mining from physicochemical properties*** <a href=\"#1\">1<\/a> uploaded in [Kaggle Plataform](https:\/\/www.kaggle.com\/uciml\/red-wine-quality-cortez-et-al-2009).\n\nIn the present work we gonna use this DataSet to apply the concepts of syllabus of the course Introduction to Data Science provided by **ICMC - USP** as a part of the **Master in Business Administration in Data Science Program** <a href=\"#2\">2<\/a>.","bbbc523e":"After reading the description of each feature we could now confirm or deny some of our first thoughts about the dataset:\n\nYes, the features are indeed correlated with each other as we see that, for instance, sulfur and sulphates are related with SO2 gas.\n\nThe Wine Quality measure can vary in a scale from 0 up to 10, although the samples in this dataset vary from 3 up to 8.\n\nAnd one last interesting thing: Quality Evaluation is a subjective matter (although the effort of Quality Classification of Enologists), but nonetheless, it seems like that are some underlying rules in Wine Quality Classifications, like:\n\n- volatile acidity can't be *too high* or the wine will taste unpleasant;\n- citric acids in the *correct amout* add some 'freshness' and flavor to wines (which is a **very distinct characteristic** of this wine, as we could see in citation on the author's academic page: *\"Vinho verde is a unique product from the Minho (northwest) region of Portugal. Medium in alcohol, is it particularly appreciated due to its freshness (specially in the summer)\"* <a href='#3'>3<\/a>\n- total sulfur dioxide can become evident in concentrations over 50 ppm and this may interfer in wine taste and aroma\n\nGiven the rules above, one first hypothesis we could think of is:\n\nThese underlying rules mentioned could be built by a Decision Tree, because this type of algorithm uses a non-balanced tree on which each node is a feature-criterium pair and determines, in the end, the class of each sample.\n\nSo **Hipothesys 1**: *\"Decision Tree will perform well in the task of classification this dataset, by identifing the underlying rules that determines wine quality\"*","a43eb6d4":"## <a id=\"intro\">1. Introduction<\/a>","bae7e241":"Let's analyze features for two different groups to see if there is a pattern that differ:\n\n(*Obs.: The following division was firstly proposed by dataset's publisher on Kaggle.*","c578c9f3":"Now plotting the decision boundaries we shall see the difference between each classifier:","3ac55ee8":"In this section we will initiate our analysis with Machine Learning Models.\nA Machine Learning Model is a function that try to predict a independent value y by a set of dependent values $X_{i}$ times some adjusted coeficients $\\Theta$$_{i}$ with some error $\\epsilon$\n\nor:\n\n\nf(X) = $\\Theta$$_{i}$ $X_{i}$ + $\\Theta$$_{0}$+ $\\epsilon$ = y\n\nWhen the domain of our indenpendant variable is discrete (Naturals) we say that our function is a **classifier function**.\n\nWhen the domain of our indenpendant variable is continuos (Reals) we say that our function is a **regression function**.","f02df894":"So these are the results of classifiers (considering **f1-score**):\n\n- KNN (k = 8, using Euclidean distance): 55%\n- Decision Tree (criterium = 'entropy'): 49%\n- Naive-Bayes (kernel='gaussian'): 51%\n- SVC (C = 10): 57%\n- Random Forest (# estimators = 100): 56%\n\nSo the best classifier is SVC(C=10)","6e7f7617":"Another interesting analysis is about data correlation. By data correlation we can determine which variable influences each other and take some decision, like, exclude two redundant features, or in case of correlation with target-feature, look better for a feature.","3e45c722":"But first, let's see how data is distributed in the classes.\n\nIn order to see this we will need to plot a 2-dimensional graph, and to choose which variables we will use in this 2-dimension graph we will use PCA (Principal Component Analysis).","9d8d3699":"Apparently, that are some features that really differs (high mean difference with a low std percentuals) between the two groups:\n\n- citric acid;\n- total sulfur dioxide;\n- alcohol;\n\nThese features, may be a good point to differentiate a bad wine to good ones.\n\nSo **Hyphoteses 2:** *As features: 'citric acid', 'total sulfur dioxide' and 'alcohol' have a relative high difference between considerable bad wines and good ones, they will have a high relevance for classifiers.*","295fce92":"### <a id='scenario1'>4.1.1. Scenario 1 - Multi-classe problem<\/a>","a78c43da":"Taking a closer look to percentual difference between mean and standard deviation of each feature of each set, we have:","d2805f54":"## <a id='eda'>2. Exploratory Data Analysis<\/a>","05ff3cbf":"We could also see, the relation between the number of components and how much variance that components can explain.","7d4c5e26":"So, this work was very interesting to use almost all class concepts.\nAs I could observe by some hypotheses made, I reach these conclusions:\n\n- It was not simple and easy to classify the multi-class dataset of wines. Observing the 2-D plots, we could not see a easy division between data and the classifiers confirm that (with the best classifier with a f1-score of 56%). The task of classify the 2-classes dataset was a lot easier and simpler (confirmed by run times of algorithms) and way more assertive (with the two best classifiers with a f1-score of 89%).\n- My Hypothese 1 was wrong, as we saw in two different scenarios. This may be explained by the fact of the dataset doesn't follow simple set of rules, as I expected.\n- My Hypothese 2 was not so wrong, in the end, as we can see in the plot of feature importance in one of the best models of classification (in both scenarios) the Random Forest Algorithm.\n- The one last challenge of try to predict the alcohol levels by the other features was very interesting too, mostly because I could test some regression techniquies.","46717087":"As defined by the author of dataset on Kaggle:\n*\"This datasets is related to red variants of the Portuguese \"Vinho Verde\" wine. For more details, consult the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\nThe datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).\"*\n\nThis dataset presents data about Wine Quality and the main goal of this work is to explore it, draw some hipoteses and by using some tecniquies reach conclusions about what and how features affect the Wine Quality.\n\nIn Kaggle, this DataSet is present as a sole DataSet, meaning that is no competition behind it.\nSo it's up to the person taken this dataset the task of divide it into test, train and validations sub-datasets.","9773098c":"### <a id='classification'>4.1. Classification<\/a>","3daf60be":"### <a id='regression'>4.2. Regression<\/a>","1912f7ca":"### <a id='scenario2'>4.1.2. Scenario 2 - 2-classes problem<\/a>"}}