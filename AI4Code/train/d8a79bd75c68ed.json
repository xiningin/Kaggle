{"cell_type":{"dea84d54":"code","36469ded":"code","1faf9022":"code","a85ffe1d":"code","b4dcef68":"code","28cb98b4":"code","93d41f6e":"code","04a0c7c9":"code","e42c4e0e":"code","54b1115a":"code","9b9e643c":"code","90d49403":"code","811f42fa":"code","0d5aba7a":"code","20b9476d":"code","eb7a1863":"code","5f30f88e":"code","dee33a5b":"code","c863f4a4":"code","022522af":"code","64538ea8":"code","280524be":"code","d6c074f2":"code","a4ff89c6":"code","92e0ba2d":"code","90d0527f":"code","3ed3e34d":"markdown","cc31515f":"markdown","0f332bed":"markdown","8df482d8":"markdown","d63968f4":"markdown","dca7a933":"markdown","793f071b":"markdown","059eebaf":"markdown","3269828a":"markdown","f7e0d713":"markdown","e6151f5a":"markdown","7b3fbe7f":"markdown","64260de4":"markdown","a040f686":"markdown","5f8233ce":"markdown","1e9065a4":"markdown","30a2417e":"markdown","f1ad1a55":"markdown","bad268c0":"markdown","b2cb48f4":"markdown","e4ad43e3":"markdown","b0ef48a2":"markdown"},"source":{"dea84d54":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.decomposition import PCA\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","36469ded":"path = '\/kaggle\/input\/tabular-playground-series-jan-2021\/'\nos.listdir(path)","1faf9022":"train_data = pd.read_csv(path+'train.csv')\ntest_data = pd.read_csv(path+'test.csv')\nsamp_subm = pd.read_csv(path+'sample_submission.csv')","a85ffe1d":"print('Number train samples:', len(train_data.index))\nprint('Number test samples:', len(test_data.index))\nprint('Number features:', len(train_data.columns))","b4dcef68":"print('Missing values on the train data:', train_data.isnull().sum().sum())\nprint('Missing values on the test data:', test_data.isnull().sum().sum())","28cb98b4":"corr = train_data[train_data.columns[1:]].corr()\ncorr.style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)","93d41f6e":"pca = PCA().fit(train_data[train_data.columns[1:-1]])\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('No of components')\nplt.ylabel('Cumulative explained variance')\nplt.grid()\nplt.show()","04a0c7c9":"train_data.boxplot(column = list(train_data.columns[1:-1]), figsize=(12,5))\nplt.show()","e42c4e0e":"test_data.boxplot(column = list(test_data.columns[1:]), figsize=(12,5))\nplt.show()","54b1115a":"features = ['cont'+str(i) for i in range(1, 15)]\nno_features = ['id', 'target']","9b9e643c":"train_data['mean'] = train_data[features].mean(axis=1)\ntrain_data['std'] = train_data[features].std(axis=1)\ntrain_data['max'] = train_data[features].max(axis=1)\ntrain_data['min'] = train_data[features].min(axis=1)\ntrain_data['sum'] = train_data[features].sum(axis=1)\n\ntest_data['mean'] = test_data[features].mean(axis=1)\ntest_data['std'] = test_data[features].std(axis=1)\ntest_data['max'] = test_data[features].max(axis=1)\ntest_data['min'] = test_data[features].min(axis=1)\ntest_data['sum'] = test_data[features].sum(axis=1)","90d49403":"train_data.head()","811f42fa":"X = train_data[train_data.columns.difference(no_features)]\ny = train_data['target']\nX_test = test_data[test_data.columns.difference(no_features)]","0d5aba7a":"mean = X.mean()\nX = X-mean\nstd = X.std()\nX = X\/std\nX_test = (X_test-mean)\/std","20b9476d":"iso = IsolationForest(contamination=0.01)\nyhat = iso.fit_predict(train_data[train_data.columns.difference(no_features)])\nmask = yhat != -1\n#X, y = X[mask], y[mask]","eb7a1863":"print('Number of outliers:', len(train_data)-mask.sum())","5f30f88e":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=2021)","dee33a5b":"print('Train shape:', X_train.shape)\nprint('Val shape:', X_val.shape)\nprint('Test shape:', X_test.shape)","c863f4a4":"model = XGBRegressor(objective='reg:squarederror',\n                     booster = \"gbtree\",\n                     eval_metric = \"rmse\",\n                     tree_method = \"gpu_hist\",\n                     n_estimators = 600,\n                     learning_rate = 0.04,\n                     eta = 0.1,\n                     max_depth = 7,\n                     subsample=0.85,\n                     colsample_bytree = 0.85,\n                     colsample_bylevel = 0.8,\n                     alpha = 0,\n                     random_state = 2021)\nmodel.fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\nprint('Score validation data:', np.sqrt(mean_squared_error(y_val, y_val_pred)))","022522af":"model","64538ea8":"importance = model.feature_importances_\nfig = plt.figure(figsize=(10, 6))\nx = list(train_data[train_data.columns[1:-1]])\nplt.barh(x, 100*importance, color='orange')\nplt.title('Feature Importance', loc='left')\nplt.xlabel('Percentage')\nplt.grid()\nplt.show()","280524be":"y_train_pred = model.predict(X_train)\ny_val_pred = model.predict(X_val)\n\nfig, axs = plt.subplots(1, 2, figsize=(22, 6))\nfig.subplots_adjust(hspace = .5, wspace=.5)\naxs = axs.ravel()\naxs[0].plot(y_train, y_train_pred, 'ro')\naxs[0].plot(y_train, y_train, 'blue')\naxs[1].plot(y_val, y_val_pred, 'ro')\naxs[1].plot(y_val, y_val, 'blue')\nfor i in range(2):\n    axs[i].grid()\n    axs[i].set_xlabel('true')\n    axs[i].set_ylabel('pred')\naxs[0].set_title('train')\naxs[1].set_title('val')\nplt.show()","d6c074f2":"y_test = model.predict(X_test)","a4ff89c6":"output = samp_subm.copy()\noutput['target'] = y_test","92e0ba2d":"output.to_csv('submission.csv', index=False)","90d0527f":"output.head()","3ed3e34d":"# Outlier Detection\n(Actually not active)","cc31515f":"# Predict Test Data","0f332bed":"# Define Train And Val","8df482d8":"# Libraries","d63968f4":"# EDA","dca7a933":"# Path","793f071b":"# Load Data","059eebaf":"Scale Data:","3269828a":"# Model","f7e0d713":"# Analyse Training","e6151f5a":"# Set X And y","7b3fbe7f":"As we can see the distribution of the values in the train and test sets are similiar. Especially in terms of the outliers of feature cont7 and cont9.","64260de4":"Box plots for every columns to visualize the distribution of the values and identify outliers\nTrain data:","a040f686":"Correlation matrix to identify dependencies:","5f8233ce":"# Feature Eningeering\nWe create statistical features like mean, max and min for every sample on the train and test data.","1e9065a4":"Feature importance:","30a2417e":"# Intro\nWelcome to the monthly Kaggle experiment in 2021. This is [january](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2021). \n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/24673\/logos\/header.png)\n\n<span style=\"color: royalblue;\">Please vote the notebook up if it helps you. Feel free to leave a comment above the notebook. Thank you. <\/span>","f1ad1a55":"Test data","bad268c0":"Visualization of the error: ","b2cb48f4":"# Overview","e4ad43e3":"Principal Component Analysis (PCA) is used to reduce the dimension of the dataset. For details we recommend [this tutorial](https:\/\/www.kaggle.com\/drcapa\/iris-species-pca).\n\nIf we try to reduce the dimension we could lost accurancy. So we decide not to do that:","b0ef48a2":"# Write Output"}}