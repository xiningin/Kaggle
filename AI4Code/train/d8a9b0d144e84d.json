{"cell_type":{"c4242714":"code","8102add5":"code","3ad3fac8":"code","4b5d6eb0":"code","4b4e7224":"code","77d4016b":"code","48194729":"code","9f2063a8":"code","e80f668c":"code","7ec97ef7":"code","0d6926fd":"code","31f690ec":"code","a49ccc3d":"code","8b46f0b7":"code","62dd20ba":"code","cee85246":"code","bc32f9cf":"code","74f9ef57":"code","ad05b929":"code","7603972c":"code","086d5944":"code","cde4a04e":"markdown","23f8d61e":"markdown","c600303b":"markdown","b74c0ac0":"markdown","b433c3a1":"markdown","0d54369c":"markdown","b32ce5ea":"markdown","3c8458de":"markdown","e18d6ede":"markdown","66c9a481":"markdown"},"source":{"c4242714":"# Importing basic libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle","8102add5":"# Import Dataset\ndataset = pd.read_csv(\"..\/input\/kc_house_data.csv\")","3ad3fac8":"dataset.head()","4b5d6eb0":"dataset.describe()\n\n#taking a close look at dataset to analyze it.","4b4e7224":"# Checking missing data\ndataset.isnull().sum()","77d4016b":"# Dropping unnecessary columns\ndataset.drop(['id', 'date', 'waterfront', 'view', 'zipcode', 'long', 'sqft_basement', 'yr_renovated'], axis=1, inplace = True)","48194729":"dataset.head()","9f2063a8":"dataset['grade'].value_counts()","e80f668c":"# Normalize dataset\n\ndataset =(dataset-dataset.mean())\/dataset.std()","7ec97ef7":"x = dataset.iloc[:, 1:].values\ny = dataset.iloc[:, :1].values","0d6926fd":"# Splitting dataset into training and testing part\n\nfrom sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size = 0.3)","31f690ec":"N, D = x.shape                # Dimentions of data\nK = 10                        # No. of hidden units","a49ccc3d":"# Initializing random weights and bias for our neural network\n\nw1 = np.random.randn(D,K)\/np.sqrt(D+K)\nb1 = np.zeros(K)\nw2 = np.random.randn(K,1)\/np.sqrt(K+1)\nb2 = 0","8b46f0b7":"# Feed forward function for ANN\ndef feed_forward(x, w1, b1, w2, b2):\n    zout = np.maximum((x.dot(w1)+b1), 0)          # relu nonlinearity\n    return (zout.dot(w2)+b2), zout                # we'll return both of them.","62dd20ba":"# Differentiation of relu will be required.\nd_relu = lambda x: (x>0).astype(x.dtype)","cee85246":"# Describing Cost function for regression.\ndef cost(T, Y):\n    return  ((T-Y)*(T-Y)).sum()\/N\n","bc32f9cf":"# Empty list, cost will be appended in backpropagation \ntrain_cost = []\ntest_cost = []\n\nlr =  0.01                            # Learning Rate\nreg = 0                               # Regularization ","74f9ef57":"# For batch gradient descent\niter = 1000                                         # No. of iteration\nbatch_size = 300                                    # Batch Size\nn_batches = np.round(len(xtrain)\/batch_size)        # No. of batches\nn_batches = n_batches.astype(int)","ad05b929":"# I am using batch gradient descent, so I'll comment code for full gradient descent\n\n# FULL GRADIENT DESCENT\n#for i in range(10000):\n#    ypred, ztrain = feed_forward(xtrain, w1, b1, w2, b2)\n#    trainCost = cost(ytrain, ypred)\n    \n#    ytest_pred, ztest = feed_forward(xtest, w1, b1, w2, b2)\n#    testCost = cost(ytest, ytest_pred)\n    \n#    if i%1000==0:\n#        print(trainCost)\n    \n#    E = ypred-ytrain\n#    w2-= lr*(ztrain.T.dot(E)\/len(xtrain) + reg*w2)\n#    b2-= lr*(E.sum()\/len(xtrain) + reg*b2)\n    \n#    dz = E.dot(w2.T)*d_relu(ztrain)\n#    w1-= lr*(xtrain.T.dot(dz)\/ len(xtrain) + reg*w1)\n#    b1-= lr*(dz.sum()\/len(xtrain) + reg*b1)\n    \n#    train_cost.append(trainCost)\n#    test_cost.append(testCost)\n    \n\n# BATCH GRADIENT DESCENT\nfor i in range(iter):\n    tempX, tempY = shuffle(xtrain, ytrain)\n\n    for j in range(n_batches):\n        batchX = tempX[j*batch_size:(j*batch_size + batch_size), :]\n        batchY = tempY[j*batch_size:(j*batch_size + batch_size), :]\n        ypred, ztrain = feed_forward(batchX, w1, b1, w2, b2)\n        \n    \n        trainCost = cost(batchY, ypred)\n    \n        ytest_pred, ztest = feed_forward(xtest, w1, b1, w2, b2)\n        testCost = cost(ytest, ytest_pred)\n    \n        if i%50 ==0 and j%50==0:\n            print(testCost)\n    \n        E = ypred-batchY\n        w2-= lr*(ztrain.T.dot(E) + reg*w2)\/len(batchX)\n        b2-= lr*(E.sum() + reg*b2)\/len(batchX)\n    \n        dz = E.dot(w2.T)*d_relu(ztrain)\n        w1-= lr*(batchX.T.dot(dz) + reg*w1)\/ len(batchX)\n        b1-= lr*(dz.sum() + reg*b1)\/len(batchX)\n    \n        train_cost.append(trainCost)\n        test_cost.append(testCost)","7603972c":"# Plotting train and test cost.\n\nplt.plot(train_cost, 'k') #black\nplt.plot(test_cost, 'b') #blue","086d5944":"ytest_pred, ztest = feed_forward(xtest, w1, b1, w2, b2)         # Our prediction\n\nd1 = ytest-ytest_pred\nd2 = ytest-ytest.mean()\nr2 = 1-d1.T.dot(d1)\/d2.T.dot(d2)                                # R-Squared value\n\nprint(r2)","cde4a04e":"*there is not any missing data.*","23f8d61e":"Our dataset is almost cleaned, but before feeding it to neural network we'll have to normalize it.","c600303b":"## Cleaning Data\nOur data is not cleaned, there are many unnecessary columns which we can't feed into neural network.","b74c0ac0":"*Since we are using batch gradient descent so graph for train cost is for 300(batch size) different batches*","b433c3a1":"Now data is ready to feed into neural network","0d54369c":"## Backpropagation","b32ce5ea":"**There are many unnecessary columns in dataset which doesn't affect our target variable, so we'll drop those columns**","3c8458de":"## How good is our model ?\n**Determining by R-squared**","e18d6ede":"# ANN for regression from scratch\nIn [Neural Nerwork from scratch](https:\/\/www.kaggle.com\/abhishekdobhal\/neural-network-from-scratch-no-libraries), I have shown how neural network works for classification problem without using any deep learning library. In this kernel I have built ANN from scratch to solve a regression problem.\n\n**** Please upvote if found useful****","66c9a481":"**We can use full or batch gradient descent**"}}