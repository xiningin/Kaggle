{"cell_type":{"b4d1e011":"code","c39c1ed2":"code","bf2397d6":"code","045ee281":"code","72c4c069":"code","3ddf95e5":"code","63b52d78":"code","3dc265c2":"code","1febd193":"code","1eca16d1":"code","7468c3ac":"code","235d2368":"code","b859a017":"code","aaf570a7":"code","7c1c3be1":"code","0e1d66a8":"code","4eb543ba":"code","3cd86beb":"code","4d24e226":"code","0b167da5":"code","b139c4ef":"code","47159dfc":"code","c94da42d":"code","7ead910a":"code","e37885f7":"code","e6a77eab":"code","d72ab34e":"code","e1b4d9ac":"code","b9676b61":"code","901614c2":"markdown","379ccd54":"markdown","0a691d81":"markdown","3f853eb7":"markdown","c7189d7f":"markdown","3c02f9f1":"markdown","337c297f":"markdown","46cf38b2":"markdown","4a8736d2":"markdown","aa754cfb":"markdown","4da8963d":"markdown"},"source":{"b4d1e011":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))|{'i','im','you','youre','they','theyre','he','hes','she','shes','we','our','us','were','arent',\\\n      'can','cant','could','couldnt','will','wont','would','wouldnt','should','shouldnt','may',\\\n       'dont','didnt','doesnt'}\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom wordcloud import WordCloud\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom gensim.models import word2vec\nfrom janome.tokenizer import Tokenizer\n\npd.set_option(\"display.max_colwidth\", 200)","c39c1ed2":"tweet=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntweet.head(3)","bf2397d6":"df=pd.concat([tweet,test],sort=False)","045ee281":"#Reference : https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\ndf['text']=df['text'].apply(lambda x:remove_URL(x))\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ndf['text']=df['text'].apply(lambda x : remove_html(x))\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\ndf['text']=df['text'].apply(lambda x : remove_punct(x))\n\n","72c4c069":"tweet=df[:len(tweet)]\ntest=df[len(tweet):]\ntweet['target']=tweet['target'].apply(lambda x:int(x))\ntweet.head()","3ddf95e5":"disaster=tweet[tweet['target']==1]\nnot_disaster=tweet[tweet['target']==0]\ndisaster.head(10)","63b52d78":"def CountWord(S):\n    cntD,cntN=0,0\n    for s in disaster['text']:\n        isok=False\n        for c in s.split():\n            if c.lower()==S:\n                isok=True\n                break\n        if isok:\n            cntD+=1\n            \n    for s in not_disaster['text']:\n        isok=False\n        for c in s.split():\n            if c.lower()==S:\n                isok=True\n                break\n        if isok:\n            cntN+=1\n    return cntD,cntN","3dc265c2":"def words_list(df):\n    words=[]\n    for x in tweet[tweet['target']==df]['text'].str.split():\n        for s in x:\n            if not s.lower() in stop:\n                words.append(s.lower())\n    return words","1febd193":"dwords=words_list(1)\nd=defaultdict(int)\nfor word in dwords:\n    d[word]+=1\ntop=sorted(d.items(),key=lambda x:x[1],reverse=True)[:30]","1eca16d1":"fig = plt.figure(figsize=(8.0, 8.0))\nx,y=zip(*top)\nplt.barh(x,y)\nplt.title('Common words in disaster tweets')","7468c3ac":"d=defaultdict(int)\nfor word in dwords:\n    d[word]+=1\ntop=sorted(d.items(),key=lambda x:x[1],reverse=True)[:300]\n\nLI=[]\nfor w,c in top:\n    cntD,cntN=CountWord(w)\n    LI.append((w,cntD,cntN,(cntD\/(cntD+cntN))))\nLI.sort(key=lambda x:x[3],reverse=True)\n\nfor i,x in enumerate(LI):\n    if i>=30:\n        break\n    w,a,b,p=x\n    print(w)\n    print('disaster:{:.5f}%'.format(p*100))\n    print('non-disaster:{:.5f}%'.format(100-p*100))\n    print('{0}\/{1}'.format(a,a+b))\n    print()","235d2368":"nwords=words_list(0)\nn_d=defaultdict(int)\nfor word in nwords:\n    n_d[word]+=1\ntop=sorted(n_d.items(),key=lambda x:x[1],reverse=True)[:30]","b859a017":"fig=plt.figure(figsize=(8.0, 8.0))\nx,y=zip(*top)\nplt.barh(x,y)\nplt.title('Common words in non-disaster tweets')","aaf570a7":"d=defaultdict(int)\nfor word in nwords:\n    d[word]+=1\ntop=sorted(d.items(),key=lambda x:x[1],reverse=True)[:300]","7c1c3be1":"LI=[]\nfor w,c in top:\n    cntD,cntN=CountWord(w)\n    LI.append((w,cntD,cntN,(cntD\/(cntD+cntN))))\nLI.sort(key=lambda x:x[3])\n\nfor i,x in enumerate(LI):\n    if i>=30:\n        break\n    w,a,b,p=x\n    print(w)\n    print('disaster:{:.5f}%'.format(p*100))\n    print('non-disaster:{:.5f}%'.format(100-p*100))\n    print('{0}\/{1}'.format(a,a+b))\n    print()","0e1d66a8":"#target==df\u3067W\u3092\u542b\u3080\u30c4\u30a4\u30fc\u30c8\u3092\u5168\u3066\u8868\u793a\n\ndef view(W,df):\n    for s in tweet[tweet['target']==df]['text']:\n        for c in s.split():\n            if c.lower()==W:\n                print(s)\n                break","4eb543ba":"view('happy',1)","3cd86beb":"view('apocalypse',0)","4d24e226":"def create_wordcloud(text,stop,df):\n    wordcloud=WordCloud(background_color=\"white\" if df==0 else \"black\",width=900,height=500,\\\n                       stopwords=stop).generate(text)\n    plt.figure(figsize=(15,12))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()","0b167da5":"create_wordcloud(' '.join(dwords),stop,1)","b139c4ef":"create_wordcloud(' '.join(nwords),stop,0)","47159dfc":"Disaster_Corpus=[]\nNon_disaster_Corpus=[]","c94da42d":"for s in tweet[tweet['target']==1]['text'].str.split():\n    arr=[]\n    for c in s:\n        arr.append(c.lower())\n    Disaster_Corpus.append(arr.copy())","7ead910a":"for s in tweet[tweet['target']==0]['text'].str.split():\n    arr=[]\n    for c in s:\n        arr.append(c.lower())\n    Non_disaster_Corpus.append(arr.copy())","e37885f7":"Disaster_Corpus[:10]","e6a77eab":"Non_disaster_Corpus[:10]","d72ab34e":"Corpus=Disaster_Corpus+Non_disaster_Corpus","e1b4d9ac":"model=word2vec.Word2Vec(Corpus,window=10,min_count=3)","b9676b61":"model.wv.doesnt_match([\"murder\",\"terrorism\",\"youtube\"])","901614c2":"### \u707d\u5bb3\u30c4\u30a4\u30fc\u30c8","379ccd54":"# Text Cleaning","0a691d81":"### \u666e\u901a\u306e\u30c4\u30a4\u30fc\u30c8","3f853eb7":"# Corpus","c7189d7f":"> \u4e8b\u4ef6\u30fb\u707d\u5bb3\u306e\u8a18\u4e8b\u3078\u306e\u30ea\u30f3\u30af\u304c\u7e70\u308a\u8fd4\u3057\u30c4\u30a4\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\u3084\u3064\n\n* **\"16yr\", \"pkk\"**\n\nhttps:\/\/twitter.com\/charles_lister\/status\/628275514332041216\n\n\n* **\"mh370\", \"debris\":**\n\nhttps:\/\/www.bbc.com\/news\/world-asia-34145127\n\n\n* **\"northern\", \"wildfire\", \"california\"**\n\nhttps:\/\/www.latimes.com\/california\/story\/2020-08-26\/california-fires-burn-more-than-1-600-structures-but-total-losses-could-top-3-000-officials-say\n\n* **\"legionnaires\"**\n\nhttps:\/\/www.bbc.com\/news\/uk-scotland-edinburgh-east-fife-33803309\n\n* **\"projected\", \"refugio\"**\n\nhttps:\/\/www.independent.com\/2015\/08\/05\/refugio-oil-spill-likely-far-larger-than-projected\/\n\n* **\"helicopter\"**\n\nhttps:\/\/www.reuters.com\/article\/us-pakistan-crash\/twelve-feared-killed-in-pakistani-air-ambulance-helicopter-crash-idUSKCN0QB1WN20150807","3c02f9f1":"### \u666e\u901a\u306e\u30c4\u30a4\u30fc\u30c8","337c297f":"# Word Analysis","46cf38b2":"### \u707d\u5bb3\u30c4\u30a4\u30fc\u30c8","4a8736d2":"> **\u5404\u5358\u8a9e\u304c\u51fa\u73fe\u3059\u308b\u6587\u7ae0\u306e\u707d\u5bb3\u30c4\u30a4\u30fc\u30c8\u7387**","aa754cfb":"# WordCloud","4da8963d":"# word2vec"}}