{"cell_type":{"d8b3f8a4":"code","489a9caa":"code","cad4d785":"code","f8c3d9b3":"code","d10f64fd":"code","ad0016be":"code","ea960324":"code","06096f1d":"code","e1be37d7":"code","6313cb38":"code","4443d32a":"code","c225f033":"code","0a124bea":"code","0115e113":"code","4a99da9d":"code","e1317acf":"code","c32a7c47":"code","82f8cfaf":"code","ba7f0943":"code","32156d6c":"code","fb50589b":"code","2d029131":"code","736fa517":"code","68251edd":"code","9ce99393":"code","b70c3a7b":"code","cfaac4e3":"code","27a181c9":"code","72090190":"code","14477faa":"code","7a4e0c8c":"code","a341e87b":"code","e13dd444":"code","5cdabe14":"code","08e81790":"code","a896d121":"code","6ffec3e7":"code","36e53989":"code","e08a1fd6":"code","906be4b4":"code","21fa3e95":"code","e83a1c8a":"code","b7d1d43d":"code","60ee72d1":"code","9d4755f0":"code","5096891e":"code","d0084507":"code","50d35409":"code","3239e267":"code","ff4e207d":"code","ba91909e":"code","1b927c82":"code","ebdf25f7":"code","e17b945b":"code","dca2856a":"code","90142c49":"code","7ee2ba7f":"code","96cf17a1":"code","e08025cf":"code","087159dc":"code","a880b4a2":"code","002291a9":"code","0ff2ee3a":"code","c9d55149":"code","3c6e6750":"code","97285ee4":"code","82d53b79":"code","b19fb3bd":"code","6399ca7d":"code","ca30540e":"code","0dbd1713":"code","f53b7d12":"code","4f0aace0":"code","9654cf8b":"code","f14dac62":"code","abad9ef4":"code","b0cc004f":"code","8c030cf3":"code","c9ef590b":"code","cb4664a1":"code","d5b17859":"code","f2a6b806":"code","f817f178":"code","20b9ab67":"code","4b1bb1ec":"code","1034a801":"code","1536730c":"code","fb84164d":"code","b21e00f0":"code","c95bd5cd":"code","d4d842e9":"code","8652c09f":"code","00e46847":"code","a6594df1":"code","3530e73b":"code","902e4480":"code","d5233640":"code","2b9e093d":"code","840ebb6c":"code","ddb91c91":"code","1efc5a1c":"code","55247f06":"code","2ae1bc85":"code","5efe5d5c":"code","5eb1379d":"code","b251304e":"code","44ab720f":"code","726e0e64":"code","7cae6421":"code","e2d79c29":"code","15ee7ec5":"code","8f06d1a8":"markdown","870661c0":"markdown","cadccc3b":"markdown","0e8db1f1":"markdown","dcff7b74":"markdown","bde5252c":"markdown","6e7da17c":"markdown","b13250a8":"markdown","8ad25fc2":"markdown","dbbcedd9":"markdown","eec14e7d":"markdown","dcf19d46":"markdown","ba93dcdb":"markdown","df4cf11f":"markdown","67c29e47":"markdown","448d927a":"markdown","0fecbbc2":"markdown","0acc2135":"markdown"},"source":{"d8b3f8a4":"import numpy as np\nimport pandas as pd\nfrom IPython.display import display\nfrom tqdm import tqdm\nfrom collections import Counter\nimport ast\n\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport seaborn as sb\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom textblob import TextBlob\nimport scipy.stats as stats\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\n\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\noutput_notebook()\n\n%matplotlib inline","489a9caa":"npr = pd.read_csv('..\/input\/topicmodelling\/npr.csv')\n","cad4d785":"npr=npr.head(50)","f8c3d9b3":"npr.iloc[0,1] ","d10f64fd":"reindexed_data = npr['Article']","ad0016be":"# Define helper functions\ndef get_top_n_words(n_top_words, count_vectorizer, text_data):\n    '''\n    returns a tuple of the top n words in a sample and their \n    accompanying counts, given a CountVectorizer object and text sample\n    '''\n    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n    vectorized_total = np.sum(vectorized_headlines, axis=0)\n    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n    \n    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n    for i in range(n_top_words):\n        word_vectors[i,word_indices[0,i]] = 1\n\n    words = [word[0].encode('ascii').decode('utf-8') for \n             word in count_vectorizer.inverse_transform(word_vectors)]\n\n    return (words, word_values[0,:n_top_words].tolist()[0])","ea960324":"count_vectorizer = CountVectorizer(stop_words='english')\nwords, word_values = get_top_n_words(n_top_words=25,\n                                     count_vectorizer=count_vectorizer, \n                                     text_data=reindexed_data)\n\nfig, ax = plt.subplots(figsize=(16,8))\nax.bar(range(len(words)), word_values);\nax.set_xticks(range(len(words)));\nax.set_xticklabels(words, rotation='vertical');\nax.set_title('Top words in headlines dataset (excluding stop words)');\nax.set_xlabel('Word');\nax.set_ylabel('Number of occurences');\nplt.show()","06096f1d":"from sklearn.feature_extraction.text import CountVectorizer","e1be37d7":"cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')","6313cb38":"cv","4443d32a":"dtm = cv.fit_transform(npr['Article'])","c225f033":"dtm","0a124bea":"from sklearn.decomposition import LatentDirichletAllocation","0115e113":"LDA = LatentDirichletAllocation(n_components=7,random_state=42)","4a99da9d":"# This can take a while as we are dealing with a large amount of documents!\nLDA.fit(dtm)","e1317acf":"lda_topic_matrix = LDA.fit_transform(dtm)","c32a7c47":"len(cv.get_feature_names())","82f8cfaf":"import random","ba7f0943":"for i in range(10):\n    random_word_id = random.randint(0,500)\n    print(cv.get_feature_names()[random_word_id])","32156d6c":"len(LDA.components_)","fb50589b":"LDA.components_","2d029131":"len(LDA.components_[0])","736fa517":"single_topic = LDA.components_[0]","68251edd":"# Returns the indices that would sort this array.\nsingle_topic.argsort()","9ce99393":"# Word least representative of this topic\nsingle_topic[100]","b70c3a7b":"# Word most representative of this topic\nsingle_topic[1294]","cfaac4e3":"# Top 10 words for this topic:\nsingle_topic.argsort()[-10:]","27a181c9":"top_word_indices = single_topic.argsort()[-10:]","72090190":"for index in top_word_indices:\n    print(cv.get_feature_names()[index])","14477faa":"for index,topic in enumerate(LDA.components_):\n    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n    print([cv.get_feature_names()[i] for i in topic.argsort()[-60:]])\n    print('\\n')","7a4e0c8c":"dtm","a341e87b":"dtm.shape","e13dd444":"len(npr)","5cdabe14":"topic_results = LDA.transform(dtm)","08e81790":"topic_results.shape","a896d121":"topic_results[0]","6ffec3e7":"topic_results[0].round(2)","36e53989":"topic_results[0].argmax()","e08a1fd6":"npr.head()","906be4b4":"topic_results.argmax(axis=1)","21fa3e95":"npr['Topic'] = topic_results.argmax(axis=1)","e83a1c8a":"npr.head(10)","b7d1d43d":"tf_feature_names = cv.get_feature_names()","60ee72d1":"first_topic = LDA.components_[0]\nsecond_topic = LDA.components_[1]\nthird_topic = LDA.components_[2]\nfourth_topic = LDA.components_[3]","9d4755f0":"first_topic.shape","5096891e":"first_topic_words = [tf_feature_names[i] for i in first_topic.argsort()[:-50 - 1 :-1]]\nsecond_topic_words = [tf_feature_names[i] for i in second_topic.argsort()[:-50 - 1 :-1]]\nthird_topic_words = [tf_feature_names[i] for i in third_topic.argsort()[:-50 - 1 :-1]]\nfourth_topic_words = [tf_feature_names[i] for i in fourth_topic.argsort()[:-50 - 1 :-1]]","d0084507":"type(first_topic_words)","50d35409":"first_topic_words","3239e267":"x=list(first_topic_words)\nx","ff4e207d":"# Generating the wordcloud with the values under the category dataframe\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfirstcloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(second_topic_words))\nplt.imshow(firstcloud)\nplt.axis('off')\nplt.show()","ba91909e":"npr_article=npr[(npr[\"Topic\"] == 1) ][[\"Article\"]]\n","1b927c82":"from collections import Counter\nresult = Counter(\" \".join(npr_article['Article'].values).split(\" \")).items()\nresult","ebdf25f7":"npr_article1=npr\nnpr_article1","e17b945b":"npr_article1['Topic_join'] = npr_article1.apply(lambda x: ','.join([x['Article']] ), axis=1)\nnpr_article2=npr_article1.groupby('Topic')['Topic_join'].apply(list)\n\nnpr_article2","dca2856a":"a = pd.Series.to_frame(npr_article2)\n\na['Topic_id'] = list(a.index)","90142c49":"a","7ee2ba7f":"sent= pd.Series.to_frame(a[(a['Topic_id'] ==3)][\"Topic_join\"])","96cf17a1":"sent=sent.iat[0,0]","e08025cf":"sent3=''.join(sent)","087159dc":"sent3 = sent3.replace(\"'\", \"\")\nsent3 = sent3.replace('\"', \"\")\n\nsent3=sent3.replace(\"\u201d\",\"\")\nsent3=sent3.replace(\"\u2019\",\"\")","a880b4a2":"import spacy\nimport string\nnlp = spacy.load('en')    \nsent1 = sent3\nimport re\n\n\nclean = re.sub(r\"\"\"\n               [,.;@#?!&$]+  # Accept one or more copies of punctuation\n               \\ *           # plus zero or more copies of a space,\n               \"\"\",\n               \" \",          # and replace it with a single space\n               sent1, flags=re.VERBOSE)\nsent=clean\n\ndoc=nlp(sent)\n\nmy_list = sent.split()\n\nsub_toks = [tok for tok in doc if ((tok.dep_ == \"nsubj\") )]\nprint(sub_toks)\n\nnc= [x for x in doc.noun_chunks]\nprint(nc)\nx=0\nfor i,token in enumerate(doc):\n\n    if token.pos_ in ('PROPN'):\n        x=i\n    if token.pos_ in ('PRON'):\n        y=i\n        \n        try:\n            my_list[y]=my_list[x]\n        except:\n            print(i)\n\n        \nprint(my_list)\n\n\nprint(x)\nmy_new_string = \" \".join(my_list)\nprint(my_new_string)","002291a9":"from nltk.corpus import stopwords\nmy_new_string = ' '.join([word for word in my_new_string.split() if word not in (stopwords.words('english'))])","0ff2ee3a":"my_new_string","c9d55149":"from collections import Counter\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import word_tokenize\ncount= Counter([j for i,j in pos_tag(word_tokenize(my_new_string))])\nprint (count)","3c6e6750":"import spacy\nfrom collections import Counter\nnlp = spacy.load('en')\ndoc = nlp(my_new_string)\n\n# all tokens that arent stop words or punctuations\nwords = [token.lemma_ for token in doc if token.is_stop != True and token.is_punct != True]\n# five most common tokens\nword_freq = Counter(words)\ncommon_words = word_freq.most_common(5)\n\n\n# noun tokens that arent stop words or punctuations\nnouns = [token.lemma_ for token in doc if token.is_stop != True and token.is_punct != True and token.pos_ == \"NOUN\"]\n# five most common noun tokens\nnoun_freq = Counter(nouns)\ncommon_nouns = noun_freq.most_common(5)\n\n\npropnouns = [token.lemma_ for token in doc if token.is_stop != True and token.is_punct != True and token.pos_ == \"PROPN\"]\npropnoun_freq = Counter(propnouns)\ncommon_propnouns = propnoun_freq.most_common(5)\n\nadjs = [token.lemma_ for token in doc if token.is_stop != True and token.is_punct != True and token.pos_ == \"ADJ\"]\nadjs_freq = Counter(adjs)\ncommon_adjs = adjs_freq.most_common(5)\n\nprint(common_words)\nprint(common_nouns)\nprint(common_propnouns)\nprint(common_adjs)","97285ee4":"common_propnouns[0][0]","82d53b79":"common_adjs[0][0]","b19fb3bd":"from collections import defaultdict, Counter\npos_counts = defaultdict(Counter)\nfor token in doc:\n    pos_counts[token.pos][token.orth] += 1\n\nfor pos_id, counts in sorted(pos_counts.items()):\n    pos = doc.vocab.strings[pos_id]\n    for orth_id, count in counts.most_common():\n        print(pos, count, doc.vocab.strings[orth_id])","6399ca7d":"import string\nTitle= common_adjs[0][0]  + \" \" + common_propnouns[0][0]\nprint(Title.upper())","ca30540e":"from nltk import ne_chunk, pos_tag, word_tokenize\nfrom nltk.tree import Tree\n \ndef get_continuous_chunks(text):\n     chunked = ne_chunk(pos_tag(word_tokenize(text)))\n     continuous_chunk = []\n     current_chunk = []\n     for i in chunked:\n             if type(i) == Tree:\n                     current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n             elif current_chunk:\n                     named_entity = \" \".join(current_chunk)\n                     if named_entity not in continuous_chunk:\n                             continuous_chunk.append(named_entity)\n                             current_chunk = []\n             else:\n                     continue\n     return continuous_chunk\n\nget_continuous_chunks(my_new_string)\n","0dbd1713":"import nltk\nfor sent in nltk.sent_tokenize(my_new_string):\n   for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n      if hasattr(chunk, 'label'):\n         print(chunk.label(), ' '.join(c[0] for c in chunk))","f53b7d12":"import spacy\nnlp = spacy.load('en',disable=['parser', 'tagger','ner'])\n\nnlp.max_length = 1198623","4f0aace0":"def separate_punc(doc_text):\n    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-\/:;<=>?@[\\\\]^_`{|}~\\t\\n ']","9654cf8b":"d = my_new_string\ntokens = separate_punc(d)","f14dac62":"tokens","abad9ef4":"len(tokens)","b0cc004f":"# organize into sequences of tokens\ntrain_len = 25+1 # 50 training words , then one target word\n\n# Empty list of sequences\ntext_sequences = []\n\nfor i in range(train_len, len(tokens)):\n    \n    # Grab train_len# amount of characters\n    seq = tokens[i-train_len:i]\n    \n    # Add to list of sequences\n    text_sequences.append(seq)","8c030cf3":"' '.join(text_sequences[0])","c9ef590b":"' '.join(text_sequences[1])","cb4664a1":"' '.join(text_sequences[2])","d5b17859":"len(text_sequences)","f2a6b806":"from keras.preprocessing.text import Tokenizer","f817f178":"# integer encode sequences of words\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(text_sequences)\nsequences = tokenizer.texts_to_sequences(text_sequences)","20b9ab67":"sequences[0]","4b1bb1ec":"tokenizer.index_word","1034a801":"for i in sequences[0]:\n    print(f'{i} : {tokenizer.index_word[i]}')","1536730c":"tokenizer.word_counts","fb84164d":"vocabulary_size = len(tokenizer.word_counts)","b21e00f0":"import numpy as np\nsequences = np.array(sequences)\nsequences","c95bd5cd":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM,Embedding","d4d842e9":"def create_model(vocabulary_size, seq_len):\n    model = Sequential()\n    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n    model.add(LSTM(150, return_sequences=True))\n    model.add(LSTM(150))\n    model.add(Dense(150, activation='relu'))\n\n    model.add(Dense(vocabulary_size, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n   \n    model.summary()\n    \n    return model","8652c09f":"from keras.utils import to_categorical","00e46847":"sequences","a6594df1":"# First 49 words\nsequences[:,:-1]","3530e73b":"# last Word\nsequences[:,-1]","902e4480":"X = sequences[:,:-1]\ny = sequences[:,-1]\ny = to_categorical(y, num_classes=vocabulary_size+1)\nseq_len = X.shape[1]\nseq_len","d5233640":"# define model\nmodel = create_model(vocabulary_size+1, seq_len)","2b9e093d":"from pickle import dump,load","840ebb6c":"# fit model\nmodel.fit(X, y, batch_size=128, epochs=100,verbose=1)","ddb91c91":"import os\nprint(os.listdir('..\/'))","1efc5a1c":"# save the model to file\nmodel.save('..\/working\/epochBIG.h5')","55247f06":"\n# save the tokenizer\ndump(tokenizer, open('epochBIG', 'wb'))","2ae1bc85":"from random import randint\nfrom pickle import load\nfrom keras.models import load_model\nfrom keras.preprocessing.sequence import pad_sequences","5efe5d5c":"def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n    '''\n    INPUTS:\n    model : model that was trained on text data\n    tokenizer : tokenizer that was fit on text data\n    seq_len : length of training sequence\n    seed_text : raw string text to serve as the seed\n    num_gen_words : number of words to be generated by model\n    '''\n    \n    # Final Output\n    output_text = []\n    \n    # Intial Seed Sequence\n    input_text = seed_text\n    \n    # Create num_gen_words\n    for i in range(num_gen_words):\n        \n        # Take the input text string and encode it to a sequence\n        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n        \n        # Pad sequences to our trained rate (50 words in the video)\n        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n        \n        # Predict Class Probabilities for each word\n        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n        \n        # Grab word\n        pred_word = tokenizer.index_word[pred_word_ind] \n        \n        # Update the sequence of input text (shifting one over with the new word)\n        input_text += ' ' + pred_word\n        \n        output_text.append(pred_word)\n        \n    # Make it look like a sentence.\n    return ' '.join(output_text)","5eb1379d":"import random\nrandom.seed(101)\nrandom_pick = random.randint(0,len(text_sequences))\nrandom_seed_text = text_sequences[random_pick]\nrandom_seed_text","b251304e":"seed_text = ' '.join(random_seed_text)\nseed_text","44ab720f":"generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)","726e0e64":"seed_text=common_adjs[0][0]  + \" \" + common_propnouns[0][0]","7cae6421":"seed_text","e2d79c29":"generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=4)","15ee7ec5":"Title=seed_text + \" \" + generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=4)\nTitle.upper()","8f06d1a8":"# Methodologies - 3\n<a id=\"112\"><\/a> <br>","870661c0":"## Preprocessing\n<a id=\"102\"><\/a> <br>","cadccc3b":"## LDA - Topic Modelling\n<a id=\"104\"><\/a> <br>","0e8db1f1":"# Methodologies - 2\n<a id=\"111\"><\/a> <br>","dcff7b74":"### Showing Top Words Per Topic","bde5252c":"### Attaching Discovered Topic Labels to Original Articles\n<a id=\"107\"><\/a> <br>","6e7da17c":"## Showing Stored Words\n<a id=\"105\"><\/a> <br>","b13250a8":"## Defining Libraries\n<a id=\"100\"><\/a> <br>","8ad25fc2":"This means that our model thinks that the first article belongs to topic #1.","dbbcedd9":"<a id=\"0\"><\/a> <br>","eec14e7d":"# CONTENT \n1. [All Steps](#0)\n     * [Defining Libraries](#100)\n     * [Dataset Loaded](#101)\n     * [Preprocessing](#102)\n     * [LDA - Topic Modelling](#104)\n     * [Showing Stored Words](#105)\n     * [Attaching Discovered Topic Labels to Original Articles](#107)\n     * [Combining with Original Data](#108)\n    \n1. [Methodologies  1](#110)\n1. [Methodologies  2](#111)    \n1. [Methodologies  3](#112)\n1. [Methodologies  4](#114)","dcf19d46":"### Combining with Original Data\n<a id=\"108\"><\/a> <br>","ba93dcdb":"**`max_df`**` : float in range [0.0, 1.0] or int, default=1.0`<br>\nWhen building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n\n**`min_df`**` : float in range [0.0, 1.0] or int, default=1`<br>\nWhen building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n\nIn CountVectorizer we only count the number of times a word appears in the document which results in biasing in favour of most frequent words. this ends up in ignoring rare words which could have helped is in processing our data more efficiently.\n\nTo overcome this , we use TfidfVectorizer .\n\nIn TfidfVectorizer we consider overall document weightage of a word. It helps us in dealing with most frequent words. Using it we can penalize them. TfidfVectorizer weights the word counts by a measure of how often they appear in the documents.","df4cf11f":"# Methodologies  - 1\n<a id=\"110\"><\/a> <br>","67c29e47":"# Introduction\n\n**Methodology 1:**\n\nhttps:\/\/www.researchgate.net\/publication\/312452399_Automated_title_generation_in_english_language_using_NLP\n\n\u2022\tOnce Topic modelling is done, then each sentence  \/ paragraph will be mapped to the one of the topic.\n\u2022\tFor each topic, we will merge the sentences \/ paragraph corresponding to that topic and plan to provide a Title\n\u2022\tReplace all the pronoun with the respective noun \n\u2022\tFind out top noun , adjective and n-grams and other part of speeches.\n\n\u2022 **Combing those words to frame the title ( max 3 words) based on the above noun , adjective and n-grams.  ** \n\n\n**Methodology 2:**\n\n\u2022\tOnce Topic modelling is done, then each sentence  \/ paragraph will be mapped to the one of the topic.\n\u2022\tFor each topic, we will merge the sentences \/ paragraph corresponding to that topic and plan to provide a Title\n\u2022\tReplace all the pronoun with the respective noun \n\u2022\tFind out top noun , adjective and n-grams and other part of speeches.\n\n\u2022\t**For each noun  \/ pronoun \u2013 find out the named entity recognition \u2013 Then frame the title with NER + Top Noun**\n\n**Methodology 3:**\n\nhttps:\/\/spacy.io\/usage\/training#ner\n\n\u2022\tOnce Topic modelling is done, then each sentence  \/ paragraph will be mapped to the one of the topic.\n\u2022\tFor each topic, we will merge the sentences \/ paragraph corresponding to that topic and plan to provide a Title\n\u2022\tReplace all the pronoun with the respective noun \n\u2022\tFind out top noun , adjective and n-grams and other part of speeches.\n\n\u2022\t**If we need a detailed level title \u2013 we need to collect the data through API \/ prepare by ourselves to train a model.**\n\n**Methodology 4:**\n\nhttps:\/\/spacy.io\/usage\/training#ner\n\n\u2022\tOnce Topic modelling is done, then each sentence  \/ paragraph will be mapped to the one of the topic.\n\u2022\tFor each topic, we will merge the sentences \/ paragraph corresponding to that topic and plan to provide a Title\n\u2022\tReplace all the pronoun with the respective noun \n\u2022\tFind out top noun , adjective and n-grams and other part of speeches.\n\n\u2022\t**Next word generator - from the highest noun**","448d927a":"These look like business articles perhaps... Let's confirm by using .transform() on our vectorized articles to attach a label number. But first, let's view all the 10 topics found.","0fecbbc2":"# Methodologies - 4\n<a id=\"114\"><\/a> <br>","0acc2135":"## Dataset Loaded\n<a id=\"101\"><\/a> <br>"}}