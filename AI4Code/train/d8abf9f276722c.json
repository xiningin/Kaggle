{"cell_type":{"cb2a565a":"code","a8f7244e":"code","35415944":"code","d8f5836a":"code","1f161ac4":"code","20b1a3c8":"code","ccf7d01a":"code","307ffb14":"code","5576d4ac":"code","6fcbef9a":"code","b63e238a":"code","44a89d0e":"code","de5b0068":"code","860cb559":"code","89960084":"code","e50d5e11":"markdown","0f8f7876":"markdown","c28893ec":"markdown","a1b119a9":"markdown","1fdab497":"markdown","e4cf283c":"markdown","bca5da62":"markdown","b949702e":"markdown","64e88883":"markdown","4c61836a":"markdown","0f0d3b82":"markdown","ba326458":"markdown","cfcbcf2f":"markdown","890f163d":"markdown"},"source":{"cb2a565a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn import svm","a8f7244e":"df = pd.read_csv('https:\/\/raw.githubusercontent.com\/satishgunjal\/datasets\/master\/admission_basedon_exam_scores.csv')\nprint('Shape of data= ', df.shape)\ndf.head()","35415944":"df_admitted = df[df['Admission status'] == 1]\nprint('Training examples with admission status 1 are = ', df_admitted.shape[0])\ndf_admitted.head(3)","d8f5836a":"df_notadmitted = df[df['Admission status'] == 0]\nprint('Training examples with admission status 0 are = ', df_notadmitted.shape[0])\ndf_notadmitted.head(3)","1f161ac4":"def plot_data(title):    \n    plt.figure(figsize=(10,6))\n    plt.scatter(df_admitted['Exam 1 marks'], df_admitted['Exam 2 marks'], color= 'green', label= 'Admitted Applicants')\n    plt.scatter(df_notadmitted['Exam 1 marks'], df_notadmitted['Exam 2 marks'], color= 'red', label= 'Not Admitted Applicants')\n    plt.xlabel('Exam 1 Marks')\n    plt.ylabel('Exam 2 Marks')\n    plt.title(title)\n    plt.legend()\n \nplot_data(title = 'Admitted Vs Not Admitted Applicants')","20b1a3c8":"#Lets create feature matrix X and label vector y\nX = df[['Exam 1 marks', 'Exam 2 marks']]\ny = df['Admission status']\n\nprint('Shape of X= ', X.shape)\nprint('Shape of y= ', y.shape)","ccf7d01a":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state= 1)\n\nprint('X_train dimension= ', X_train.shape)\nprint('X_test dimension= ', X_test.shape)\nprint('y_train dimension= ', y_train.shape)\nprint('y_train dimension= ', y_test.shape)","307ffb14":"# Note here we are using default SVC parameters\nclf = svm.SVC()\nclf.fit(X_train, y_train)\nprint('Model score using default parameters is = ', clf.score(X_test, y_test))","5576d4ac":"def plot_support_vector(classifier):\n    \"\"\"\n    To plot decsion boundary and margin. Code taken from Sklearn documentation.\n\n    I\/P\n    ----------\n    classifier : SVC object for each type of kernel\n\n    O\/P\n    -------\n    Plot\n    \n    \"\"\"\n    clf =classifier\n    # plot the decision function\n    ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    # create grid to evaluate model\n    xx = np.linspace(xlim[0], xlim[1], 30)\n    yy = np.linspace(ylim[0], ylim[1], 30)\n    YY, XX = np.meshgrid(yy, xx)\n    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n    Z = clf.decision_function(xy).reshape(XX.shape)\n\n    # plot decision boundary and margins\n    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n               linestyles=['--', '-', '--'])\n    # plot support vectors\n    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n               linewidth=1, facecolors='none', edgecolors='k')  ","6fcbef9a":"plot_data(title = 'SVM Classifier With Default Parameters')\nplot_support_vector(clf)  ","b63e238a":"def svm_params(X_train, y_train, X_test, y_test):\n    \"\"\"\n    Finds the best choice of Regularization parameter (C) and gamma for given choice of kernel and returns the SVC object for each type of kernel\n\n    I\/P\n    ----------\n    X_train : ndarray\n        Training samples\n    y_train : ndarray\n        Labels for training set\n    X_test : ndarray\n        Test data samples\n    y_test : ndarray\n        Labels for test set.\n\n    O\/P\n    -------\n    classifiers : SVC object for each type of kernel\n    \n    \"\"\"\n    C_values = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 40]\n    gamma_values = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 40]\n    kernel_types = ['linear', 'poly', 'rbf']\n    classifiers = {}\n    max_score = -1\n    C_final = -1\n    gamma_final = -1\n    for kernel in kernel_types:                    \n        for C in C_values:\n            for gamma in gamma_values:\n                clf = svm.SVC(C=C, kernel= kernel, gamma=gamma)\n                clf.fit(X_train, y_train)\n                score = clf.score(X_test, y_test)\n                #print('C = %s, gamma= %s, score= %s' %(C, gamma, score))\n                if score > max_score:\n                    max_score = score\n                    C_final = C\n                    gamma_final = gamma\n                    classifiers[kernel] = clf        \n        print('kernel = %s, C = %s, gamma = %s, score = %s' %(kernel, C_final, gamma_final, max_score))\n    return classifiers","44a89d0e":"classifiers = svm_params(X_train, y_train, X_test, y_test)","de5b0068":"plot_data(title = 'SVM Classifier With Parameters ' + str(classifiers['linear']))\nplot_support_vector(classifiers['linear'])","860cb559":"plot_data(title = 'SVM Classifier With Parameters ' + str(classifiers['rbf']))\nplot_support_vector(classifiers['rbf'])","89960084":"plot_data(title = 'SVM Classifier With Parameters ' + str(classifiers['poly']))\nplot_support_vector(classifiers['poly'])","e50d5e11":"## Build Machine Learning Model","0f8f7876":"### Create Test And Train Dataset\n* We will split the dataset, so that we can use one set of data for training the model and one set of data for testing the model\n* We will keep 20% of data for testing and 80% of data for training the model\n* If you want to learn more about it, please refer [Train Test Split tutorial](https:\/\/satishgunjal.com\/train_test_split\/)","c28893ec":"In order to visualize the results better lets create a function to plot SVM Classifier decision boundary with margin","a1b119a9":"# Example: Classification Problem\nNow we will implement the SVM algorithm using sklearn library and build a classification model that estimates an applicant\u2019s probability of admission based on Exam 1 and Exam 2 scores. Note- I have also used the same dataset in [Logistic Regression From Scratch With Python](https:\/\/satishgunjal.github.io\/binary_lr\/)","1fdab497":"## Load Data\n* We are going to use \u2018admission_basedon_exam_scores.csv\u2019 CSV file\n* File contains three columns Exam 1 marks, Exam 2 marks and Admission status","e4cf283c":"## Data Understanding\n* There are total 100 training examples (m= 100 or 100 no of rows)\n* There are two features Exam 1 marks and Exam 2 marks\n* Label column contains application status. Where \u20181\u2019 means admitted and \u20180\u2019 means not admitted\n\n### Data Visualization\nTo plot the data of admitted and not admitted applicants, we need to first create separate data frame for each class(admitted\/not-admitted)","bca5da62":"![SVM_Header](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/SVM_Header.png)\n\nSupport vector machines (SVM) is one of the most powerful 'Black Box' machine learning algorithm. It belongs to the family of supervised learning algorithm. Used to solve classification as well as regression problems. Using threshold to classify the different groups is not very accurate and may lead to the wrong predictions. SVM classifier also use the threshold to classify the data, but it uses midpoint of the observations on the edge of each group as threshold. In order to avoid the classification error, SVM also defines the safety margin on both the side of threshold. So this safety margin will provide robustness to SVM and that's why it is called as 'Large Margin Classifier'. SVM algorithm can be used to classify linear as well as non-linear data. So the secret sauce of the SVM is the way it finds the threshold and define the safety margin on either side of threshold to avoid the classification errors.\n\n# How SVM Classifier Work\nOutliers in the data can affect the threshold value and lead to wrong predictions. Consider below example\n\n![Datapoint_With_Outlier_And_Threshold](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Datapoint_With_Outlier_And_Threshold.png)\n\nSo from above example its clear that we can't just choose data point on edge to draw a decision boundary. In order to avoid this, SVM use cross validation technique to identify the data points to draw the decision boundary. The data points on the edge and within the boundary are called support vectors. Data points inside the margin are also called as 'misclassified observations'. So SVM using cross validation tries multiple combination of support vectors to find the best decision boundary and finally select the best possible support vector which provides the larger margin. Consider below example where SVM has ignored the few observations in order to find more robust threshold and safety margin.\n\n![Datapoint_With_Outlier_And_SVM_Classifier](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Datapoint_With_Outlier_And_SVM_Classifier.png)\n\n\n\n## Types Of SVM Classifiers\nSince SVM can be used to classify linear as well as non-linear data, we can have support vector classifiers from a point to hyperplane. Hyperplane in this context is tool that separates the data space into one less dimension for easier classification. Actually every SVM classifier is hyperplane of dimension n - 1 where, n is the dimension of given data. \n\nFor 1 dimensional data, hyperplane is a point, and we can classify the new observation based on the which side of the point they are\n\n![1D_SVM_Classifier](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/1D_SVM_Classifier.png)\n\nFor 2 dimensional data, hyperplane is a line, and we can classify the new observation based on the which side of the line they are\n\n![2D_SVM_Classifier](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/2D_SVM_Classifier.png)\n\nFor 3 dimensional data, hyperplane is a plane, and we can classify the new observation based on the which side of the plane they are\n\n![3D_SVM_Classifier](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/3D_SVM_Classifier.png)\n\nFor n dimensional data, hyperplane is  n-1 dimensional plane\n\nSince we can visualize the data up to 3 dimensions, we refer the hyperplane by more friendly names like line, plane etc and for more than 3 dimensions we just call it hyperplane.\n\n# Kernels\nKernel is the technique used by SVM to classify the non-linear data. Kernel functions are used to increase the dimension of the data, so that SVM can fit the optimum hyperplane to separate the data. Consider below example where 1D data points are randomly grouped. Using kernel function(here 2nd degree polynomial) we can convert 1D data points to 2D data points and fit a line to separate the data into two groups.\n\n![SVM_Polynomial_Kernel](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/SVM_Polynomial_Kernel.png)\n\nSo kernel functions help SVM to transform the lower dimension data to higher dimension but in order to figure out which kernel function to use, to transform the data SVM uses cross validation. Using cross validation SVM tries multiple combination of Kernel functions like polynomial kernel and choose the one which results in the best classification.","b949702e":"Now lets plot the scatter plot for admitted and not admitted students","64e88883":"## Import Libraries\n* pandas: Used for data manipulation and analysis\n* numpy : Numpy is the core library for scientific computing in Python. It is used for working with arrays and matrices.\n* matplotlib : It\u2019s plotting library, and we are going to use it for data visualization\n* model_selection: Here we are going to use model_selection.train_test_split() for splitting the data\n* svm: Sklearn support vector machine model","4c61836a":"# Conclusion\nRemember that, our data is 2D so hyperplane will be a line. But if you observe the data closely there is no clear separation between classes that's why straight line is not a good fit, which is obvious from above plots. Though the accuracy of poly kernel is less than rbf, but still its best choice for our data. \n\n","0f0d3b82":"Lets call the svm_params() function to get the best parameters for each type of kernel","ba326458":"Now lets train the model using SVM classifier","cfcbcf2f":"# Advantages\n* Effective in high dimensional input data\n* Effective when number of dimensions are higher than number of samples\n* Uses a subset of training points to find the support vector classifier, so it is also memory efficient\n* Prediction accuracy is higher when there is a clear separation between classes\n\n# Disadvantages\n* Large data sets- takes lots of time to separate the data\n* Data with lots of error- Since SVM separates the two groups of data based on nearest points, if these points have errors in them, then it will affect entire model performance\n* Choose the wrong kernel- If we choose the wrong separation plane then it will affect the model performance","890f163d":"## SVM Parameters\n* Gamma: In case of high value of Gamma decision boundary is dependent on observations close to it, where in case of low value of Gamma, SVM will consider the far away points also while deciding the decision boundary\n* Regularization parameter(C): Large C will result in overfitting and which will lead to lower bias and high variance. Small C will result in underfitting and which will lead to higher bias and low variance. For more details about it please refer [Underfitting & Overfitting](https:\/\/satishgunjal.github.io\/underfitting_overfitting\/)\n\nSo regularization parameter C and gamma parameters plays an important role in order to find the best fit model. Let's create a function which will try multiple such values and return the best value of C and gamma for our choice of the kernel. At the end we will plot the decision boundary with margin using the best choice of SVM parameters for each type of kernel."}}