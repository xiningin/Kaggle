{"cell_type":{"5206c2dc":"code","140f31ec":"code","380c5526":"code","74aeb53c":"code","de19735c":"code","36e5479e":"code","2fdca4ad":"code","e8cecf6a":"code","699d1ed8":"code","cff20814":"code","a078dcb9":"code","2b509634":"code","c539f816":"code","df1801d5":"code","ad2457ed":"code","5af00404":"code","a6e999bc":"code","041e5bc5":"code","7547a129":"code","e38b8a98":"code","f92d58d7":"code","6e9f5395":"code","de243168":"code","75ab3c25":"code","4e2e68b7":"code","ee509f22":"code","677801f7":"code","6818da7f":"code","26bb4fbc":"code","df476367":"code","bcbf5da1":"code","aee6d272":"code","cafa8395":"code","4085dc1c":"code","f3dc0817":"code","491609ed":"code","9b807f42":"code","4ec04cee":"code","0f3adce6":"code","dd0e308f":"code","d92f7ff2":"code","bf8dc9ba":"code","e03bdf2d":"code","81658690":"code","6c8b7a31":"code","a8c15d73":"code","df0691db":"code","72faf334":"code","6fbb4f66":"code","da0f931c":"code","e222dc3a":"code","bce8d16a":"code","31c1c17e":"code","cac3728d":"code","d926253c":"code","68527575":"code","4a4dc0f2":"code","d0d44f88":"code","8e5b7d30":"code","6e09686a":"code","2b176925":"code","19943661":"markdown","121fc52f":"markdown","9f14db46":"markdown","e9ae1f89":"markdown","ca54ce78":"markdown","7e7d9590":"markdown","894dc51d":"markdown","374df817":"markdown","a6ccaeb9":"markdown","00c8437d":"markdown","368e60c3":"markdown","d1107634":"markdown","d8a6d737":"markdown","83254891":"markdown","20e0b253":"markdown","ec245587":"markdown","f6143c40":"markdown","5b7dfb26":"markdown","293acb1e":"markdown","ecf38942":"markdown","5fe6de7a":"markdown","8a895167":"markdown","13d599a8":"markdown","629b7efe":"markdown","ec3a6a54":"markdown","89d1bb63":"markdown","e7671bb4":"markdown","496e52aa":"markdown","0743ef9f":"markdown","fb5459b3":"markdown","090714c7":"markdown","3b248d10":"markdown","81c89bdd":"markdown","b3ae8a5d":"markdown","828b7275":"markdown","53b04700":"markdown","7cd27348":"markdown","1a9f75a4":"markdown","9ade7d81":"markdown","af52a56a":"markdown","62161ef0":"markdown","0c0f4dc3":"markdown","2dec270a":"markdown"},"source":{"5206c2dc":"# data analysis and wrangling\nimport numpy as np \nimport pandas as pd \nimport random as rnd\nimport math\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n#Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn\n\n# machine learning\nimport sklearn\nimport tensorflow as tf\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score","140f31ec":"train_df = pd.read_csv('..\/input\/titanic\/train.csv') \ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]","380c5526":"display(train_df.head())\ndisplay(train_df.tail())","74aeb53c":"train_df.info()\nprint('_'*40)\ntest_df.info()","de19735c":"train_df.describe()","36e5479e":"train_df.describe(include=['O'])","2fdca4ad":"display(train_df.groupby('Pclass')['Survived'].mean().sort_values(ascending=False))","e8cecf6a":"display(train_df.groupby('Sex')['Survived'].mean().sort_values(ascending=False))","699d1ed8":"display(train_df.groupby('SibSp')['Survived'].mean().sort_values(ascending=False))","cff20814":"display(train_df.groupby('Parch')['Survived'].mean().sort_values(ascending=False))","a078dcb9":"fig, (ax1, ax2) = plt.subplots(1, 2)\nfig.suptitle('Horizontally stacked subplots')\nax1.hist(train_df['Age'][train_df['Survived'] == 0], bins = 20)\nax2.hist(train_df['Age'][train_df['Survived'] == 1], bins = 20)\nax1.set_title('Survived = 0')\nax2.set_title('Survived = 1')\nax1.set_ylim(0, 60)\nax2.set_ylim(0, 60)","2b509634":"fig, (ax1, ax2) = plt.subplots(1, 2)\nfig.suptitle('Horizontally stacked subplots')\nax1.hist(train_df['Age'][(train_df['Pclass'] == 1) & (train_df['Survived'] == 0)], bins = 20)\nax1.set_ylim(0, 40)\nax1.set_title('PClass = 1 | Survived = 0')\nax2.hist(train_df['Age'][(train_df['Pclass'] == 1) & (train_df['Survived'] == 1)], bins = 20)\nax2.set_ylim(0, 40)\nax2.set_title('PClass = 1 | Survived = 1')\n\nfig, (ax3, ax4) = plt.subplots(1, 2)\nfig.suptitle('Horizontally stacked subplots')\nax3.hist(train_df['Age'][(train_df['Pclass'] == 2) & (train_df['Survived'] == 0)], bins = 20)\nax3.set_ylim(0, 40)\nax3.set_title('PClass = 2 | Survived = 0')\nax4.hist(train_df['Age'][(train_df['Pclass'] == 2) & (train_df['Survived'] == 1)], bins = 20)\nax4.set_ylim(0, 40)\nax4.set_title('PClass = 2 | Survived = 1')\n\nfig, (ax3, ax4) = plt.subplots(1, 2)\nfig.suptitle('Horizontally stacked subplots')\nax3.hist(train_df['Age'][(train_df['Pclass'] == 3) & (train_df['Survived'] == 0)], bins = 20)\nax3.set_ylim(0, 40)\nax3.set_title('PClass = 3 | Survived = 0')\nax4.hist(train_df['Age'][(train_df['Pclass'] == 3) & (train_df['Survived'] == 1)], bins = 20)\nax4.set_ylim(0, 40)\nax4.set_title('PClass = 3 | Survived = 1')","c539f816":"train_df[['Pclass', 'Sex', 'Embarked', 'Survived']].groupby('Embarked')['Survived'].mean()\nfirst_male = train_df[(train_df['Sex'] == 'male') & (train_df['Pclass'] == 1)].groupby('Embarked')['Survived'].mean()\nsecond_male = train_df[(train_df['Sex'] == 'male') & (train_df['Pclass'] == 2)].groupby('Embarked')['Survived'].mean()   \nthird_male = train_df[(train_df['Sex'] == 'male') & (train_df['Pclass'] == 3)].groupby('Embarked')['Survived'].mean()\nfirst_female = train_df[(train_df['Sex'] == 'female') & (train_df['Pclass'] == 1)].groupby('Embarked')['Survived'].mean()\nsecond_female = train_df[(train_df['Sex'] == 'female') & (train_df['Pclass'] == 2)].groupby('Embarked')['Survived'].mean()   \nthird_female = train_df[(train_df['Sex'] == 'female') & (train_df['Pclass'] == 3)].groupby('Embarked')['Survived'].mean()\nplt.plot([1, 2, 3], [first_male[2], second_male[2], third_male[2]], label = 'male')\nplt.plot([1, 2, 3], [first_female[2], second_female[2], third_female[2]], label = 'female')\nplt.ylabel('Survived')\nplt.xlabel('Pclass')\nplt.ylim(0, 1)\nplt.legend()\nplt.title('Embarked = S')\nplt.show()\nplt.plot([1, 2, 3], [first_male[1], second_male[1], third_male[1]], label = 'male')\nplt.plot([1, 2, 3], [first_female[1], second_female[1], third_female[1]], label = 'female')\nplt.ylabel('Survived')\nplt.xlabel('Pclass')\nplt.ylim(-0.1, 1.1)\nplt.legend()\nplt.title('Embarked = Q')\nplt.show()\nplt.plot([1, 2, 3], [first_male[0], second_male[0], third_male[0]], label = 'male')\nplt.plot([1, 2, 3], [first_female[0], second_female[0], third_female[0]], label = 'female')\nplt.ylabel('Survived')\nplt.xlabel('Pclass')\nplt.ylim(-0.1, 1.1)\nplt.legend()\nplt.title('Embarked = C')","df1801d5":"grid = seaborn.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(seaborn.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","ad2457ed":"print(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\nprint(\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)","5af00404":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","a6e999bc":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","041e5bc5":"old_name = ['Master', 'Miss', 'Mr', 'Mrs', 'Rare']\nnew_name = [1, 2, 3, 4, 5]\nfor dataset in combine:\n    for i in range(5):\n        dataset['Title'] = dataset['Title'].replace(old_name[i], new_name[i])\n        \ndisplay(train_df.head())\ndisplay(test_df.head())","7547a129":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","e38b8a98":"le = LabelEncoder()\ntrain_df['Sex'] = le.fit_transform(train_df['Sex'])\ntest_df['Sex'] = le.transform(test_df['Sex'])","f92d58d7":"le.classes_","6e9f5395":"display(train_df.head(10))\ntest_df.head()","de243168":"grid = seaborn.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","75ab3c25":"guess_ages = np.zeros((2,3))\nguess_ages","4e2e68b7":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n            age_mean = guess_df.mean()\n            age_std = guess_df.std()\n            age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n            \n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int(age_guess\/0.5 + 0.5) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head(10)","ee509f22":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","677801f7":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()","6818da7f":"train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","26bb4fbc":"train_df.isnull().sum()\n#test_df.isnull().sum()","df476367":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","bcbf5da1":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","aee6d272":"train_df = train_df.drop(['Parch', 'SibSp'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","cafa8395":"for dataset in combine:\n    dataset['Age*Class'] = dataset['Age'] * dataset['Pclass']\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","4085dc1c":"freq_port = train_df.Embarked.dropna().mode()[0]\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","f3dc0817":"le_embarked = LabelEncoder()\ntrain_df['Embarked'] = le_embarked.fit_transform(train_df['Embarked'])\ntest_df['Embarked'] = le_embarked.transform(test_df['Embarked'])\n\ntrain_df.head()","491609ed":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","9b807f42":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","4ec04cee":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","0f3adce6":"train_df = train_df.drop(['FamilySize'], axis=1)\ntest_df = test_df.drop(['FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","dd0e308f":"X_train = train_df.drop(\"Survived\", axis=1)\ny_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","d92f7ff2":"sc = StandardScaler()\nX_train_prep = sc.fit_transform(X_train)\nX_test_prep = sc.transform(X_test)","bf8dc9ba":"log_reg = LogisticRegression(C = 0.7)\nlog_reg.fit(X_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nprint(confusion_matrix(y_train, log_reg.predict(X_train)))\nprint(accuracy_score(y_train, log_reg.predict(X_train)))\n\nfrom sklearn.model_selection import GridSearchCV\nparameters_log_reg = [{'C': [0.4, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1]}]\ngrid_search_log_reg = GridSearchCV(estimator = log_reg,\n                           param_grid = parameters_log_reg,\n                           scoring = 'accuracy',\n                           cv = 4)\ngrid_search_log_reg.fit(X_train, y_train)\nbest_accuracy_log_reg = grid_search_log_reg.best_score_\nbest_parameters_log_reg = grid_search_log_reg.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy_log_reg*100))\nprint(\"Best Parameters:\", best_parameters_log_reg)","e03bdf2d":"knn = KNeighborsClassifier(n_neighbors = 3, metric = 'minkowski', p = 2)\nknn.fit(X_train_prep, y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nprint(confusion_matrix(y_train, knn.predict(X_train_prep)))\nprint(accuracy_score(y_train, knn.predict(X_train_prep)))\n\nfrom sklearn.model_selection import GridSearchCV\nparameters_knn = [{'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}]\ngrid_search_knn = GridSearchCV(estimator = knn,\n                           param_grid = parameters_knn,\n                           scoring = 'accuracy',\n                           cv = 4)\ngrid_search_knn.fit(X_train_prep, y_train)\nbest_accuracy_knn = grid_search_knn.best_score_\nbest_parameters_knn = grid_search_knn.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy_knn*100))\nprint(\"Best Parameters:\", best_parameters_knn)","81658690":"svc = SVC(kernel = 'rbf', gamma = 0.02, C = 8, random_state = 0, probability=True)\nsvc.fit(X_train_prep, y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nprint(confusion_matrix(y_train, svc.predict(X_train_prep)))\nprint(accuracy_score(y_train, svc.predict(X_train_prep)))\n\nfrom sklearn.model_selection import GridSearchCV\nparameters_svc = [{'C': [1, 3, 5, 7, 8, 9, 10], 'kernel': ['rbf'], 'gamma': [0.01, 0.015, 0.02, 0.03, 0.04, 0.05]}]\ngrid_search_svc = GridSearchCV(estimator = svc,\n                           param_grid = parameters_svc,\n                           scoring = 'accuracy',\n                           cv = 4)\ngrid_search_svc.fit(X_train_prep, y_train)\nbest_accuracy_svc = grid_search_svc.best_score_\nbest_parameters_svc = grid_search_svc.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy_svc*100))\nprint(\"Best Parameters:\", best_parameters_svc)","6c8b7a31":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train_prep, y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nprint(confusion_matrix(y_train, nb.predict(X_train_prep)))\nprint(accuracy_score(y_train, nb.predict(X_train_prep)))\n\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = nb, X = X_train_prep, y = y_train, cv = 4)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\ndisplay(accuracies)","a8c15d73":"rf = RandomForestClassifier(n_estimators = 100, max_depth = 4, criterion = 'gini', random_state = 0)\nrf.fit(X_train_prep, y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nprint(confusion_matrix(y_train, rf.predict(X_train_prep)))\nprint(accuracy_score(y_train, rf.predict(X_train_prep)))\n\nfrom sklearn.model_selection import GridSearchCV\nparameters_rf = [{'n_estimators': [50, 100], \n                  'max_depth': [2, 4, 5],\n                  'criterion': ['gini', 'entropy']}]\ngrid_search_rf = GridSearchCV(estimator = rf,\n                           param_grid = parameters_rf,\n                           scoring = 'accuracy',\n                           cv = 10)\ngrid_search_rf.fit(X_train_prep, y_train)\nbest_accuracy_rf = grid_search_rf.best_score_\nbest_parameters_rf = grid_search_rf.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy_rf*100))\nprint(\"Best Parameters:\", best_parameters_rf)","df0691db":"from xgboost import XGBClassifier\nboost = XGBClassifier(n_estimators = 19, max_depth = 4, booster = 'gbtree', eta = 0.01, subsample = 0.75, use_label_encoder=False, eval_metric = 'logloss', random_state = 0)\nboost.fit(X_train_prep, y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nprint(confusion_matrix(y_train, boost.predict(X_train_prep)))\nprint(accuracy_score(y_train, boost.predict(X_train_prep)))\n\nfrom sklearn.model_selection import GridSearchCV\nparameters_boost = [{'n_estimators': [19, 20, 21, 22, 100], \n                     'max_depth': [2, 3, 4],\n                    'subsample': [0.75, 0.8, 0.85]}]\ngrid_search_boost = GridSearchCV(estimator = boost,\n                           param_grid = parameters_boost,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           verbose = 1)\ngrid_search_boost.fit(X_train_prep, y_train)\nbest_accuracy_boost = grid_search_boost.best_score_\nbest_parameters_boost = grid_search_boost.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy_boost*100))\nprint(\"Best Parameters:\", best_parameters_boost)","72faf334":"data = [[best_accuracy_log_reg,\n       best_accuracy_knn,\n       best_accuracy_svc,\n       best_accuracy_rf,\n       best_accuracy_boost]]\n\ncolumns = ['logistic_reg',\n          'knn',\n          'svm',\n          'rf',\n          'boost']\n\ndisplay(pd.DataFrame(data=data, columns=columns))","6fbb4f66":"from sklearn.ensemble import VotingClassifier\n\nvoting_clf = VotingClassifier(\n    estimators=[('svc', svc), ('rf', rf), ('boost', boost)],\n    voting='hard',\n    weights=[1, 2, 1])\nvoting_clf.fit(X_train_prep, y_train)\n\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = voting_clf, X = X_train_prep, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\ndisplay(accuracies)","da0f931c":"weights_df_3 = pd.DataFrame(columns=('w1', 'w2', 'w3', 'mean', 'std'))\n\ni = 0\nfor w1 in range(1,4):\n    for w2 in range(1,4):\n        for w3 in range(1,4):\n\n                    if len(set((w1,w2,w3))) == 1: # skip if all weights are equal\n                        continue\n\n                    voting_clf = VotingClassifier(\n                        estimators=[('svc', svc), ('rf', rf), ('boost', boost)],\n                        voting='hard',\n                        weights=[w1, w2, w3])\n                    scores = cross_val_score(\n                                                    estimator=voting_clf,\n                                                    X=X_train_prep,\n                                                    y=y_train,\n                                                    cv=10,\n                                                    scoring='accuracy',\n                                                    n_jobs=1,\n                                                    verbose = 1)\n\n                    weights_df_3.loc[i] = [w1, w2, w3, scores.mean(), scores.std()]\n                    i += 1\n                \ndisplay(weights_df_3.sort_values(by=['mean'], ascending=False).head(5))","e222dc3a":"from mlxtend.classifier import EnsembleVoteClassifier\nclf1 = SVC(kernel = 'rbf', gamma = 0.02, C = 8, random_state = 0, probability=True)\nclf2 = RandomForestClassifier(n_estimators = 100, max_depth = 6, criterion = 'gini', random_state = 0)\nclf3 = XGBClassifier(n_estimators = 20, max_depth = 3, booster = 'gbtree', eta = 0.01, subsample = 0.75, use_label_encoder=False, eval_metric = 'logloss', random_state = 0)\n\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='hard')\n\nparams = {'svc__C': [0.02, 0.1],\n          'svc__gamma': [1, 8],\n          'randomforestclassifier__n_estimators': [20, 100],\n          'randomforestclassifier__max_depth': [4, 6],\n          'xgbclassifier__n_estimators' : [20, 50],\n          'xgbclassifier__max_depth' : [2, 3]\n         }\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=10, verbose = 1)\ngrid.fit(X_train_prep, y_train)\n\nbest_accuracy = grid.best_score_\nbest_parameters = grid.best_params_\nprint(\"Best Accuracy: {:.3f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","bce8d16a":"clf1 = SVC(kernel='rbf', C=1.35, gamma = 0.12, probability=True, random_state = 0)\nclf2 = RandomForestClassifier(n_estimators = 26, random_state = 0, max_depth = 2)\nclf3 = XGBClassifier(n_estimators = 19, max_depth = 2, use_label_encoder=False, eval_metric = 'logloss', random_state = 0)\n\nweights_df_3_new = pd.DataFrame(columns=('w1', 'w2', 'w3', 'mean', 'std'))\n\ni = 0\nfor w1 in range(1,4):\n    for w2 in range(1,4):\n        for w3 in range(1,4):\n\n                    if len(set((w1,w2,w3))) == 1: # skip if all weights are equal\n                        continue\n\n                    eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], \n                                                  voting='hard',\n                                                  weights=[w1, w2, w3])\n                    scores = cross_val_score(\n                                                    estimator=eclf,\n                                                    X=X_train_prep,\n                                                    y=y_train,\n                                                    cv=5,\n                                                    scoring='accuracy',\n                                                    verbose = 1)\n\n                    weights_df_3_new.loc[i] = [w1, w2, w3, scores.mean(), scores.std()]\n                    i += 1\n                \ndisplay(weights_df_3_new.sort_values(by=['mean'], ascending=False).head(15))","31c1c17e":"from sklearn.ensemble import VotingClassifier\n\nclf1 = SVC(kernel = 'rbf', gamma = 0.02, C = 8, random_state = 0, probability=True)\nclf2 = RandomForestClassifier(n_estimators = 100, max_depth = 4, criterion = 'gini', random_state = 0)\nclf3 = XGBClassifier(n_estimators = 19, max_depth = 3, booster = 'gbtree', eta = 0.01, subsample = 0.75, use_label_encoder=False, eval_metric = 'logloss', random_state = 0)\n\nvoting_clf_final = VotingClassifier(\n    estimators=[('svc', clf1), ('rf', clf2), ('boost', clf3)],\n    voting='hard',\n    weights = [1, 3, 2])\nvoting_clf_final.fit(X_train_prep, y_train)\n\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = voting_clf_final, X = X_train_prep, y = y_train, cv = 10)\nprint(\"Accuracy: {:.4f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.4f} %\".format(accuracies.std()*100))\ndisplay(accuracies)","cac3728d":"print(confusion_matrix(y_train, boost.predict(X_train_prep)))\nprint('training set accuracy, xgboost:', accuracy_score(y_train, boost.predict(X_train_prep)))\nprint('')\nprint(confusion_matrix(y_train, voting_clf_final.predict(X_train_prep)))\nprint('training set accuracy, voting:', accuracy_score(y_train, voting_clf_final.predict(X_train_prep)))","d926253c":"y_pred = voting_clf_final.predict(X_test_prep)","68527575":"y_pred_rf = rf.predict(X_test_prep) ","4a4dc0f2":"y_pred_boost = boost.predict(X_test_prep) ","d0d44f88":"len(y_pred)","8e5b7d30":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('\/kaggle\/working\/submission_voting_one_more_feature.csv', index=False) ","6e09686a":"submission_rf = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": y_pred_rf\n    })\nsubmission_rf.to_csv('\/kaggle\/working\/submission_rf_with_family_size.csv', index=False) ","2b176925":"submission_boost = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": y_pred_boost\n    })\nsubmission_boost.to_csv('\/kaggle\/working\/submission_boost.csv', index=False) ","19943661":"**Convert the Fare features to ordinal values based on the FareBand**","121fc52f":"### **<font color = 'green'>3.1 Logistic regression<\/font>**","9f14db46":"**In the following code we extract Title feature using regular expressions. The RegEx pattern (\\w+\\\\.) matches the first word which ends with a dot character within Name feature. The expand=False flag returns a DataFrame.**","e9ae1f89":"**Final model for prediction is voting classifier**","ca54ce78":"### **<font color = 'green'>3.7 Final accuracies on the validation<\/font>**","7e7d9590":"**We can convert the categorical titles to ordinal**","894dc51d":"**Let us create Age bands and determine correlations with Survived.**","374df817":"**Observations.**\n\n* Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\n* Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\n* Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\n* Pclass varies in terms of Age distribution of passengers.\n\n**Decisions.**\n\n* Consider Pclass for model training.","a6ccaeb9":"### Voting classifier without tuning","00c8437d":"### Tuning weights with the hyperparameters from the previous step (3 models)","368e60c3":"### **<font color = 'green'>3.2 KNN<\/font>**","d1107634":"# Analyze by visualizing data","d8a6d737":"**Observations.**\n\n* Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\n* Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).\n\n**Decisions.**\n\n* Consider banding Fare feature.","83254891":"## Creating new features combinign existing features","20e0b253":"### **<font color = 'green'>3.6 Gradient Boosting<\/font>**","ec245587":"**Now we can drop the Name feature, we also don't need PassengerId**","f6143c40":"**We can also create an artificial feature combining Pclass and Age**","5b7dfb26":"### Creating new feature extracting from existing","293acb1e":"**Let us drop PArch and SibSp**","ecf38942":"# **<font color = 'red'>3. Training the models and hyperparameter tuning<\/font>**\n","5fe6de7a":"### **<font color = 'green'>3.5 Random Forest<\/font>**","8a895167":"* **Pclass** We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\n* **Sex** We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\n* **SibSp** and Parch These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1).","13d599a8":"**We dropped the 'Ticket' and 'Cabin' columns**\n","629b7efe":"**Let us replace Age with ordinals based on these bands.**\n\n","ec3a6a54":"**Observations.**\n\n* Female passengers had much better survival rate than males. Confirms classifying (#1).\n* Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n* Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\n* Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\n\n**Decisions.**\n\n* Add Sex feature to model training.\n* Complete and add Embarked feature to model training.","89d1bb63":"**Fill missing Fare value in the test data**","e7671bb4":"# **<font color = 'blue'>4. Ensembling the models<\/font>**\n","496e52aa":"**Creating FareBand**","0743ef9f":"### Tuning weights with the new hyperparameters(3 models)","fb5459b3":"### Tuning hyperparameters (3 models)","090714c7":"**Completing a categorical feature**","3b248d10":"**We can replace many title with a more common name or classify them as Rare**","81c89bdd":"# Data Preprocessing","b3ae8a5d":"# Assumtions based on data analysis\nWe arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n\n**Correlating.**\n\nWe want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n\n**Completing.**\n\n1. We may want to complete Age feature as it is definitely correlated to survival.\n2. We may want to complete the Embarked feature as it may also correlate with survival or another important feature.\n\n**Correcting.**\n\n1. Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\n2. Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\n3. PassengerId may be dropped from training dataset as it does not contribute to survival.\n4. Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.\n\n**Creating.**\n\n1. We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\n2. We may want to engineer the Name feature to extract Title as a new feature.\n3. We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\n4. We may also want to create a Fare range feature if it helps our analysis.\n\n**Classifying.**\n\nWe may also add to our assumptions based on the problem description noted earlier.\n\n1. Women (Sex=female) were more likely to have survived.\n1. Children (Age<?) were more likely to have survived.\n1. The upper-class passengers (Pclass=1) were more likely to have survived.","828b7275":"# Analyzing by privoting features","53b04700":"### Final Voting Classifier","7cd27348":"### **<font color = 'green'>3.3 SVM<\/font>**","1a9f75a4":"### **<font color = 'green'>3.4 Naive-Bayes<\/font>**","9ade7d81":"# **1. Looking at the data**","af52a56a":"### Completing a numerical continuous feature\n\nNow we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\n1. A simple way is to generate random numbers between mean and standard deviation.\n\n1. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using median values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n\n1. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2.","62161ef0":"**Converting a categorical feature**","0c0f4dc3":"**Observations.**\n\n* Infants (Age <=4) had high survival rate.\n* Oldest passengers (Age = 80) survived.\n* Large number of 15-25 year olds did not survive.\n* Most passengers are in 15-35 age range.\n\n**Decisions.**\n\nThis simple analysis confirms our assumptions as decisions for subsequent workflow stages.\n\n* We should consider Age (our assumption classifying #2) in our model training.\n* Complete the Age feature for null values (completing #1).\n* We should band age groups (creating #3).","2dec270a":"**We can create anothe feature calles IsAlone**"}}