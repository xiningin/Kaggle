{"cell_type":{"c61a5142":"code","f5fb59ea":"code","2535fefe":"code","e4133cff":"code","98eba7e3":"code","ef5e8938":"code","2268dafc":"code","528401f5":"code","7afbcc2b":"code","47a4d694":"code","3d8f67ad":"code","3ad0da20":"code","3f375043":"code","a7d67304":"code","297b0319":"code","35e36e69":"code","f2f7ad50":"code","82a7b1b4":"code","2cb87d1d":"code","8592ad4a":"code","96c8b33e":"markdown","9b4d949b":"markdown","1ba625e9":"markdown","af267066":"markdown","733c0b4e":"markdown","adc79157":"markdown","b9648266":"markdown","5f7f91ed":"markdown","a3612fcd":"markdown","da9f7ba4":"markdown","f40d459e":"markdown","b9632031":"markdown","9af384e3":"markdown","87e4630f":"markdown","e58666e0":"markdown","c8f9fb1d":"markdown","41f605a2":"markdown","2f823557":"markdown","780b39d3":"markdown","9f4026ec":"markdown","c9d0dfbc":"markdown","e9730cd3":"markdown","98da618d":"markdown","a8da5016":"markdown","f7b889b4":"markdown","51ab6de4":"markdown","cb294f36":"markdown","4ccfc1b0":"markdown","c73b1c18":"markdown"},"source":{"c61a5142":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","f5fb59ea":"nba_df = pd.read_csv('..\/input\/nba-2k-ratings-with-real-nba-stats\/nba_rankings_2014-2020.csv')\nnba_df.drop(['Unnamed: 0'], axis=1, inplace=True)\nnba_df.head()","2535fefe":"nba_df.groupby(['SEASON']).agg({'rankings': ['mean', 'std']})","e4133cff":"import seaborn as sns\nplt.figure(figsize=(10,8))\nsns.distplot(nba_df['rankings'], bins=20)\nplt.xlabel('Ratings')\nplt.ylabel('Frequency')\nplt.title('Distribution of 2K Ratings')\nplt.tight_layout()","98eba7e3":"nba_df.sort_values(by=['rankings'], ascending=False).loc[:, ['PLAYER', 'TEAM', 'AGE', 'SEASON', 'PTS', 'FG%','3P%',\n                                                             'FT%', 'REB', 'AST', 'TOV', 'STL', 'BLK', 'PF', 'FP',\n                                                             'DD2', 'TD3', '+\/-', 'rankings']]","ef5e8938":"years = ['2014-15', '2015-16', '2016-17', '2017-18', '2018-19', '2019-20']\ntop_players = pd.DataFrame(columns=nba_df.columns)\nfor year in years:\n    season = nba_df[nba_df['SEASON'] == year]\n    ranks = season[season['rankings'] == season['rankings'].max()]\n    top_players = top_players.append(ranks)\ntop_players","2268dafc":"nba_df.corr()","528401f5":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor","7afbcc2b":"X = nba_df.drop(['PLAYER', 'TEAM', 'SEASON', 'rankings', 'FP'], axis=1)\ny = nba_df['rankings']","47a4d694":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","3d8f67ad":"rfe = RFECV(GradientBoostingRegressor(random_state=0, max_depth=3, subsample=0.8), cv=KFold(10), scoring='r2')\nrfe.fit(X_train, y_train)\nprint('Number of Features Selected from RFECV:',  rfe.n_features_)\nplt.plot(range(1, len(rfe.grid_scores_) + 1), rfe.grid_scores_)\nplt.ylabel('R2')\nplt.xlabel('# of Features')\nplt.title('RFECV Gradient Boosting')\nplt.tight_layout()","3ad0da20":"# subset feature set to RFE features\nX_train = X_train.loc[:, rfe.support_]\nX_test = X_test.loc[:, rfe.support_]\n# drop FGM\nX_train.drop(['FGM'], axis=1, inplace=True)\nX_test.drop(['FGM'], axis=1, inplace=True)\n# X_train.drop(['PTS'], axis=1, inplace=True)\n# X_test.drop(['PTS'], axis=1, inplace=True)\n# X_train, X_test, y_train, y_test = train_test_split(X_subset, y, random_state=0)","3f375043":"xgb_param_grid = {\"learning_rate\": [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n                  \"max_depth\": [ 3, 4, 5, 6, 8, 10, 12, 15],\n                  \"min_child_weight\": [ 1, 3, 5, 7 ], 'subsample': [0.6, 0.7, 0.8, 0.9],\n                  'colsample_bytree': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n                  \"gamma\": [ 0.0, 0.1, 0.2 , 0.3, 0.4 ]}\nxgb_pipe = (XGBRegressor(random_state=0))\nxgb_grid = RandomizedSearchCV(xgb_pipe, xgb_param_grid, scoring='r2')\nxgb_grid.fit(X_train, y_train)\nprint('XGB best params: ', xgb_grid.best_params_)\nprint('R2: ', xgb_grid.score(X_test, y_test))\ny_pred_xgb = xgb_grid.predict(X_test)","a7d67304":"series = pd.Series(xgb_grid.best_estimator_.feature_importances_, X_train.columns)\nseries.sort_values(ascending=False, inplace=True)\nplt.figure(figsize=(10,8))\nplt.barh(series.index, series.values)\nplt.xlabel('Feature Importances')\nplt.ylabel('Features')\nplt.title('Feature Importaces for XGBRegressor')\nplt.tight_layout()","297b0319":"from sklearn.inspection import permutation_importance\nseries = pd.Series(permutation_importance(xgb_grid, X_train, y_train).importances_mean\n, X_train.columns)\nseries.sort_values(ascending=False, inplace=True)\nplt.figure(figsize=(10,8))\nplt.barh(series.index, series.values)\nplt.xlabel('Permutation Importances')\nplt.ylabel('Features')\nplt.title('Permutation Importaces for XGBRegressor')\nplt.tight_layout()","35e36e69":"import shap\nexplainer = shap.TreeExplainer(xgb_grid.best_estimator_)\nshap_values = explainer.shap_values(X_train)\nshap.summary_plot(shap_values, features=X_train, feature_names=X_train.columns, max_display=X_train.shape[1])","f2f7ad50":"from sklearn import metrics\nfrom sklearn.linear_model import Ridge\npipe = make_pipeline(StandardScaler(), Ridge())\nridge_grid = {'ridge__alpha': [10, 5, 1,0.1,0.01,0.001,0.0001]}\ngrid_ridge = GridSearchCV(pipe, ridge_grid, scoring='r2')\ngrid_ridge.fit(X_train, y_train)\nprint(grid_ridge.best_estimator_)\ny_pred = grid_ridge.predict(X_test)\ncoef = zip(X_train.columns, grid_ridge.best_estimator_['ridge'].coef_)\nsorted(coef, key = lambda x: np.abs(x[1]), reverse=True)","82a7b1b4":"xgb_r2 = metrics.r2_score(y_test, y_pred_xgb)\nridge_r2 = metrics.r2_score(y_test, y_pred)\nxgb_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred_xgb))\nridge_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\npd.DataFrame({'model': ['xgb', 'ridge'], 'r2': [xgb_r2, ridge_r2], 'rmse':[xgb_rmse, ridge_rmse ]})","2cb87d1d":"np.sqrt(metrics.mean_squared_error(y_test, (0.4*y_pred+0.6*y_pred_xgb)))","8592ad4a":"print('Mean Absolute Error (combining predictions): ', \n      metrics.mean_absolute_error(y_test, (0.6*y_pred+0.4*y_pred_xgb)))","96c8b33e":"Let's see how a linear model performs with these features. We can't use Linear Regression here however, unless we do something with the features, since there is multicollinearity involved. For that reason, let's try Ridge Regression.  We'll standardize the data and also grid search for the right value of `alpha`, which is the regularization parameter.","9b4d949b":"## Introduction","1ba625e9":"Below, we see that LeBron James headlines the rankings leaderboards in every 2K game in the past five years; he's the top highest rated player in all five, with two ties in NBA 2K21 with Giannis and NBA 2K20 with Kawhi Leonard. ","af267066":"## Conclusion","733c0b4e":"RFECV returned us 18 optimal features, and we can see in our graph that adding more features will not give us any improvement in R<sup>2<\/sup>. Perhaps we could have done less than 18 features as well, as the R<sup>2<\/sup> does not seem to be increasing past 15 features. ","adc79157":"We can see a general linear trend between player statistics and player ratings. However, once we get to the higher ranks, these rankings seem to be a little bit more arbitray. \n\nLeBron was the highest rated player with a rating 98, but his production did not match other high ranking players. Players who were ranked 96 had a much higher PTS average, and players who were ranked 97 had a higher rebound average. The last column, `+\/-`, is a metric used to indicate a player's impact on the game. A negative plus-minus shows that a player has an adverse impact on the game when he's on the floor, and ad positive means the opposite. More about that here: (https:\/\/www.basketball-reference.com\/about\/bpm2.html). \n\nLeBron's `+\/-` is significantly lower than any of the high rated players, with only a +1.30. A value of +1.30 means the team is only 1.30 points per 100 possessions better than the other team on the floor. These aren't production numbers you would expect from a player with such a high ranking.\n\nBut we would be remiss in our analysis if we did not take into account that LeBron is the sole player representing the 98 rating. We mustn't make the mistake of comparing averages to a single year production. However, if we were to compare averages, it seems that even the 96 OVR players have higher statistical production than the 97 OVR players! \n\nIt seems that the difference between these one or two rating points lie not within statistics but in context; LeBron was the cover athlete for 2K19, and perhaps 2K felt the need to boost his ranking a little bit more to push their cover athlete forward. Or LeBron might've gotten the nod due to his renown as a clutch performer.","b9648266":"We see that average NBA 2K rank is around 75-76, with the lowest rank being 62 with Sean Kilpatrick in 2K16 and the highest being 98 with Lebron in 2K19. The distribution does display a somewhat bell-shaped curve, so we can say that 2K does try to maintain a normal distribution.","5f7f91ed":"After loading in the data, we can view the distribution of NBA 2K ratings, as well as look at the top rated players and the lowest rated players.","a3612fcd":"But there are a lot of things inherently wrong with the base feature importance in gradient boosting. Let's look at permutation importances as well, which will permute one feature at a time (keeping distribution the same but just interchange rows) and see the impact it has on the model. We see that the feature importance disparity becomes even more apparent, with `PTS` being the most important feature by far.","da9f7ba4":"If we combine the two predictions, you can see that **our overall predictions are around the actual rankings, plus or minus 1.4.** \n\nThat is, on average, our predictions will miss the true rating by plus-minus 1.4. Not bad!","f40d459e":"### Resources\n- https:\/\/www.complex.com\/sports\/2017\/10\/how-nba-2k-determines-player-rankings\n- https:\/\/www.basketball-reference.com\/about\/bpm2.html\n- https:\/\/hoopshype.com\/2017\/08\/20\/nba-2k-ratings-how-they-are-determined-and-why-players-care-so-much-about-them\/\n- https:\/\/www.reddit.com\/r\/NBA2k\/comments\/5zl48j\/the_secrets_of_overall\/","b9632031":"# NBA Stats and NBA 2K Analysis\nWilliam Yu","9af384e3":"Now, what we really want to see is how these rankings reflect real life statistics. Does a player with a 98 OVR really have better IRL basketball stats than someone with a 95 OVR?\n\nBelow you will see an interactive visualization of the rankings and their respective real life stats. We've taken the average of all players in each rank and reflected it in the visualization. There are no players ranked 63 OVR in the past 5 games hence why the graph is omitted when the slider is at 63. If it's not visible, you can click [here](https:\/\/nba2k-stats.herokuapp.com\/).\n\n**Move the slider below the graph to see how NBA player stats change with the given rating**.","87e4630f":"We can start by using Recursive Feature Elimination (RFE), which will find the optimal number of features by iteratively testing each combination of features. RFE will start by selecting all the features in the dataset, then iteratively remove the weakest feature in terms of the model's feature importances. RFE is a wrapper feature selection method, meaning we can choose a model we would like to use to evaluate the feature importances. We use Gradient Boosting, as we will use this for modeling. Feature importance here is calculated by weight, or the number of times the feature has been used as a split point for all the trees. RFECV uses RFE with cross validation, which will fit n models for the n features in our model, for each of k splits. Whichever of the n models has the best average performance over the k splits, we will use that model with those features.","e58666e0":"Let's see how a linear model performs with these features. We can't use Linear Regression here however, unless we do something with the features, since there is multicollinearity involved. For that reason, let's try Ridge Regression.  We'll standardize the data and also grid search for the right value of `alpha`, which is the regularization parameter.Let's try Linear Regression. We fit the model and look at the coefficients below:","c8f9fb1d":"Most basketball fans have heard of the game NBA 2K; hailed for its realistic graphics, gameplay, and attention to detail, there's no other basketball game quite like it. But how does the game compare to the real thing? More specifically, how does it capture the skill gap between NBA superstars and the 10-day contract players?\n\nThe answer lies in NBA 2K's rating system. This system assigns a player (their counterpart in the game to be exact) a ranking ranging anywhere from 60 to the high 90's. You can have multiple players be assigned the same rank. With these ratings, you can expect the player with the higher rating to be the better player in the game. So you won't have Zylan Cheatham, who has a 67 rating, dominating Lebron James, who's a 97. \n\nOf course, we're only talking about overall rating for each player, and there are still areas in which a player with a lower rating than another might be better in one category. For instance, Giannis Antetokounmpo might have a higher rating than Steph Curry in NBA 2K21, but Curry's 3PT rating is much higher than Giannis'. For simplicity sake, we'll be sticking with **overall (OVR) rating**.\n","41f605a2":"So we've learned a few things: NBA box scores and 2K rankings do seem to have a **positive correlation**. If a player is more productive on the court (in terms of stats), then we should expect that player to have a higher ranking. But once we progress into the higher ranking players, these ranking assignments become more or less arbitrary and may depend on looking at film. \n\nIt seems that the model does tend to value offensive production a lot more. However, in the case of a player such as 'Rudy Gobert', the model still does seem to be able to predict his rating with suprising accuracy.\n\nWe did try a model not including `PTS` (not included here), and the model performance is slightly worse, but does use other features to determine ratings. \n\nStats that seem like they would be good predictors in rankings, such as **three point percentages, have almost no bearing on the result**. This backs up 2K's claim of not just looking at the whole picture. Quoted from Complex's interview with 2K, 2K stated: \n\n<em>\"If a player has a 37 percentage for his 3-point shot, we're interested in 'how',\" Stauffer said. \"Not every 3-point shot is the same. There's always a variable. For example, if Steph Curry has to run across the whole court through multiple screens to find an open 3\u2014or has to shoot from far back to get open\u2014does his percentage tell the whole story?\"<\/em>\n\nAnd this makes sense. Digging around, I managed to find that Joe Harris, whose career 3P% is on par with Steph Curry's, has not only a much lower OVR ranking than Curry, but also a much lower 3P shot ranking in 2K. Joe Harris shoots most of his three's wide open off screens, where Curry will usually see double teams everywhere he goes around the perimter..","2f823557":"## Load Data\/EDA\n\nWe can start by loading the data. \n\nNBA 2K, in anticipation of the new season, will release a new version of their game every fall. Not only does this allow 2K to keep up with any offseason roster changes, but it gives them a chance to update their ranking database. 2K does change their rankings quite a bit during the season however, as it does keep up with real life stats. But the rankings before the season serve as a prediction to how 2K thinks each player will perform.\n\nSo what we're primarily interested in is how 2K generates **pre-season rankings**, and not so much how they change the rankings throughout the season. \n\nWe'll pull the last 5 seasons of the NBA (2014-15 to 2019-20), and match them with the past 5 NBA 2K games. In the table below, you will see that the first 30 columns reflect the actual player and his stats in the NBA, and the last column, `rankings`, is their NBA 2K rating.","780b39d3":"We again see that `PTS` is the most important feature, where a high value for `PTS` corresponds with a high value for rating. This makes sense with what we saw with the slider, but as mentioned, becomes quite arbitrary once we get to the high 90 OVR ratings.","9f4026ec":"We can then look at what our model determines as the most important features. `PTS`, `FTA`, `MIN`, `DREB` seem to be the most important.","c9d0dfbc":"We can actually try to get a more robust estimate of our predictions by aggregating the two model predictions:","e9730cd3":"This concludes the analysis in NBA 2K rankings. Thanks for reading!","98da618d":"After retrieving the optimal features, we can run a RandomizedSearch on our model (XGBRegressor) to tune the optimal hyperparameters to achieve the best R<sup>2<\/sup>. RandomizedSearch acts similarly to GridSearch, but reduces much of the search space by sampling random parameter combinations through a uniform distribution.","a8da5016":"We can also look at SHAP values, which are feature importance values derived from game theory. SHAP differs in that it offers not only feature importances, but the magnitude of those importances. In the graph below, the location of the feature on the y-axis indicates the ranking of the feature. The higher, the better. The color indicates whether that value was high or not. The location of data on x-axis shows whether the effect of that value is associated with a higher or lower prediction. For instance, a high value for `PTS` has an impact on a higher rating (positively correlated). Whereas the feature `L` is negatively correlated.\n","f7b889b4":"Let's compare the two approaches below. We see that Ridge seems to slightly outperform XGB here, which is quite suprising. Although this may be attributed to the number of randomized searches we performed. The difference is quite miniscule however.","51ab6de4":"## Modeling","cb294f36":"Now we can move on to some modeling. We want to see what are the key metrics 2K uses to weigh their ratings. Although 2K has admitted that it's somewhat of a plug and chug model (see here: https:\/\/hoopshype.com\/2017\/08\/20\/nba-2k-ratings-how-they-are-determined-and-why-players-care-so-much-about-them\/), they also do look at film to determine a player's contribution on the floor. \n\nNow of course 2K doesn't directly pull from NBA stats to make their ratings, they first encode the stats into individual ratings for each category, then run their model to get the OVR rating. But this rating encoding should still correspond to the actual stats. If you want to take a look at how 2K determines rankings from their own categories, I recommend this really informative Reddit post: https:\/\/www.reddit.com\/r\/NBA2k\/comments\/5zl48j\/the_secrets_of_overall\/ . \n","4ccfc1b0":"<iframe src=\"https:\/\/nba2k-stats.herokuapp.com\/\" height=\"400\" width=\"100%\" frameBorder=\"0\"><\/iframe>","c73b1c18":"So, now that we've learned a bit about 2K's rating system, we can ask some questions: what are the key factors in deciding a basketball player's 2K rating? Is it just something that people plug into a model and get the rating back? Or is it more nuanced, and depends on context and player potential?\n\nBelow you will find a short exploratory data analysis, as well as some data fitting and modeling on the dataset. I scraped from stats.nba.com and hoopshype.com to pull together the data. The scraper is available in this repository as the \"nba_scrape\" notebook.\n\nLet's get started."}}