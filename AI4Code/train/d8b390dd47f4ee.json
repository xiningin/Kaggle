{"cell_type":{"550b00b5":"code","c9666151":"code","62e9e0a8":"code","4dfc20db":"code","cc467773":"code","2b7a85a1":"code","f89dea1e":"code","0dae27c8":"code","88a71a78":"code","1ba4d9f6":"code","c1d669aa":"code","2b0ebe69":"code","735221fc":"code","997648cc":"code","b4bab1d9":"code","aa3e6530":"code","f1161142":"code","4430ead8":"code","8c196832":"code","e8e46aa7":"code","fadcb5ca":"code","e3383942":"code","95886519":"code","5f0792ad":"code","856854a4":"code","01532a3d":"code","9686246b":"code","686908ff":"code","61eabcc5":"code","8d4917ba":"code","9ff83fb8":"code","970ad379":"markdown","94730632":"markdown","61140ea8":"markdown","92c8b050":"markdown","7efd9cf5":"markdown","42fc020a":"markdown","bbad7d98":"markdown","7e5288ac":"markdown","2d589620":"markdown","e578a1e0":"markdown","a4bd2717":"markdown","d8db58e3":"markdown","2876cdc6":"markdown","3ca6a9f8":"markdown","b78cec08":"markdown","e9fe2ba4":"markdown","4847e8ae":"markdown","53f92126":"markdown","a20058c1":"markdown","6b1654ea":"markdown","bb438946":"markdown","5ea42c4c":"markdown","fd0c1c68":"markdown","a0001c76":"markdown","43459c44":"markdown"},"source":{"550b00b5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport missingno as msno\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.svm import SVC\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline","c9666151":"# Loading the data\ndf_train = pd.read_csv('\/kaggle\/input\/mobile-price-classification\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/mobile-price-classification\/test.csv')\ndf_train.head()","62e9e0a8":"print('Train set shape:', df_train.shape)\nprint('Test set shape:', df_test.shape)","4dfc20db":"df_train.info()","cc467773":"df_train.describe().T","2b7a85a1":"msno.bar(df_train)\nplt.show()","f89dea1e":"sns.pairplot(data=df_train, hue='price_range')\nplt.show()","0dae27c8":"sns.set(rc={'figure.figsize':(10,7)})\n\nsns.stripplot(x=\"price_range\", y=\"ram\", data=df_train, dodge=True, palette='dark')\nplt.show()","88a71a78":"sns.swarmplot(x=\"fc\", y=\"ram\", hue=\"price_range\", data=df_train, dodge=True, palette='deep')\nplt.show()","1ba4d9f6":"sns.scatterplot(x=\"ram\", y=\"battery_power\", hue=\"price_range\", data=df_train, palette='deep')\nplt.show()","c1d669aa":"sns.countplot(x = 'price_range' , data = df_train)\nplt.show()","2b0ebe69":"df_train['price_range'].value_counts()","735221fc":"corr = df_train.corr()\nsns.heatmap(corr, cmap=\"YlGnBu\", linewidths=.5)\nplt.show()","997648cc":"corr.sort_values(by=[\"price_range\"],\n                 ascending=False).iloc[0].sort_values(ascending=False)","b4bab1d9":"x = df_train.drop([\"price_range\"],axis=1)\ny = df_train[\"price_range\"].values\n\nx_train, x_val, y_train, y_val = train_test_split(x,y,test_size = 0.2,random_state=420)\n\nmin_max_scaling = preprocessing.MinMaxScaler()\nx_train = min_max_scaling.fit_transform(x_train)\nx_val = min_max_scaling.transform(x_val)","aa3e6530":"from sklearn.svm import SVC\nsvm_model = SVC(random_state=420)\nsvm_model.fit(x_train,y_train)\nprint(\"train accuracy:\",svm_model.score(x_train,y_train))\nprint(\"val accuracy:\",svm_model.score(x_val,y_val))","f1161142":"from sklearn.feature_selection import SelectKBest, f_classif\n\ntrain_accuracy = []\n\nk = np.arange(1,21,1)\n\nfor i in k:\n    selector = SelectKBest(f_classif, k=i)\n    x_train_new = selector.fit_transform(x_train, y_train)\n    svm_model.fit(x_train_new,y_train)\n    train_accuracy.append(svm_model.score(x_train_new,y_train))\n    \nplt.plot(k,train_accuracy,color=\"blue\",label=\"train\")\nplt.xlabel(\"k values\")\nplt.ylabel(\"train accuracy\")\nplt.legend()\nplt.show()","4430ead8":"result = pd.DataFrame(data= {'k_best_features': k,\n                             'train_accuracy': train_accuracy})\nresult","8c196832":"print(result[result.train_accuracy == result.train_accuracy.max()])","e8e46aa7":"selector = SelectKBest(f_classif, k = 13)\n\nx_train_new = selector.fit_transform(x_train, y_train)\nx_val_new = selector.transform(x_val)\n\ntop_features = x.columns.values[selector.get_support()].tolist()\nprint(\"Top features:\",top_features)","fadcb5ca":"svm_model = SVC(random_state=420)\nsvm_model.fit(x_train_new,y_train)\nprint(\"train accuracy:\",svm_model.score(x_train_new,y_train))\nprint(\"val accuracy:\",svm_model.score(x_val_new,y_val))","e3383942":"# Preparing the train data\n\nx = np.array(df_train[top_features])\ny = np.array(df_train[\"price_range\"])","95886519":"# Hyperparameter tuning and saving the models\n\nimport optuna\nimport pickle\n\ndef objective(trial):\n    \n    params = {\n        'C':trial.suggest_loguniform('C', 1e-10, 1e10),\n        'kernel':trial.suggest_categorical('kernel',[\"linear\",\"rbf\"]),\n        'gamma':trial.suggest_categorical('gamma',[\"auto\",\"scale\"]),\n        'decision_function_shape':trial.suggest_categorical(\n            'decision_function_shape',[\"ovo\",\"ovr\"]\n        )\n    }\n    \n    svm_model = SVC(**params)\n    \n    skf = StratifiedKFold(n_splits=10)\n    accuracy = []\n    \n    for fold, (train_index, val_index) in enumerate(skf.split(x,y)):\n        x_train, y_train, = x[train_index], y[train_index]\n        x_val, y_val = x[val_index], y[val_index]\n        \n        scaler = preprocessing.MinMaxScaler()\n        scaler.fit(x_train)\n        x_train = scaler.transform(x_train)\n        x_val = scaler.transform(x_val)\n        \n        SCALER_PATH = f'scaler-t{trial.number}-f{fold}.pickle'\n        pickle.dump(scaler, open(SCALER_PATH,'wb'))\n        \n        svm_model.fit(x_train,y_train)\n        score = svm_model.score(x_val,y_val)\n        accuracy.append(score)\n        \n        MODEL_PATH = f'model-t{trial.number}-f{fold}.pickle'\n        pickle.dump(svm_model, open(MODEL_PATH,'wb'))\n\n    print(f'Trial done: Accuracy scores values on folds: {accuracy}')\n    accuracy_on_folds = np.mean(accuracy)\n    \n    return accuracy_on_folds","5f0792ad":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","856854a4":"ls","01532a3d":"num_folds = 10\n\npredictions_from_folds = []\n\nfor i in range(num_folds):\n    \n    x_test = np.array(df_test[top_features])\n    \n    SCALER_FILE = f'scaler-t{study.best_trial.number}-f{i}.pickle'\n    scaler = pickle.load(open(SCALER_FILE, 'rb'))\n    x_test = scaler.transform(x_test)\n    \n    MODEL_FILE = f'model-t{study.best_trial.number}-f{i}.pickle'\n    model = pickle.load(open(MODEL_FILE, 'rb'))\n    y_test_preds = model.predict(x_test)\n    predictions_from_folds.append(y_test_preds)\n    \npredictions_from_folds = np.array(predictions_from_folds)","9686246b":"predictions_df = pd.DataFrame(predictions_from_folds)\npredictions_df","686908ff":"final_predictions = predictions_df.mode(axis=0)\nfinal_predictions","61eabcc5":"final_predictions.isnull().sum(axis=1)","8d4917ba":"results = final_predictions.T\nresults.drop(results.columns[[1]], axis = 1, inplace = True)\nresults","9ff83fb8":"results.to_csv('results.csv', header=False, index=False)","970ad379":"Once we have calculated the mode from pandas module we see there are two rows in the output and most of the values of second row are NaN. The second row here, is for the second mode if it exists, i.e when two values have equal count. So lets find out if we have any.","94730632":"After finding out the trail that gives best mean accuracy. We will use the models and transformations from the trial to predict the price category for the test data.\n\nNote: There are 10 different transformation states and 10 model states which we have saved from the best trial.","61140ea8":"We can see form the above countplot that the prices are uniformly distributed. \n  \n  Now lets see how our feature variables are correlated using corelation matrix.","92c8b050":"Now lets plugin those features into the model and check for both train and validation accura","7efd9cf5":"## **Creating base line for our model**\n\nLets go ahead and fit our model to standard SVC to the train data.","42fc020a":"We notice the val accuraccy has also improved from 0.85 to 0.89 which is good","bbad7d98":"Now as the training is finished. Lets look at the results and see which trial gave us the best mean accuracy and what are the parameter values for the algorithm.\n\nWe have also saved the model states and transformation states of each trail and folds during training.","7e5288ac":"We will now calculate mode for each item in the test data as the final prediction.","2d589620":"Now we know the model outputs best train accuracy of 0.98 which is higher than the previously obtained 0.97 with its top 13 features selected. Now lets look what are those features.","e578a1e0":"## **Hyper Tuning Model with selected top features using optuna**\n\nFor more details: https:\/\/optuna.readthedocs.io\/en\/stable\/index.html\n\nAs of now we have a knowledge of what features to select for our model. Now its time to improve on our model by tuning the parameters. In doing so we will be using 10 fold cross validation and calculate mean accuracy.\n\n**TWO THINGS TO SPECIFY** : number of trials (20) and number of folds (10) for each trial.\n\n**STEPS FROM HERE:**\n1. For each trial and fold we will be saving both the scaler which transforms the data and the models used to calculate accuracy.\n\n2. After finishing the trials. We will select the *best trial* i.e. which gives us the best mean accuracy of Out Of Fold predictions.\n\n3. Next step would be to transform the test data with the scalers and get predictions from the models that we have saved from the best trial.\n\n4. Finally we will use mode to select the most appropriate category from the predictions.","a4bd2717":"From the above plot we notice the price range is increasing with increase in ram.","d8db58e3":"## **Creating train and validation sets for training**  \n\nLets break down our train data into two parts one used for training and other for validation. Note we would also like to scale our data before sending as input to our model. For this we will use Min-Max Scaler.\n\n*Note: It is important to fit scaler only on train data and then transform train and test data to prevent any data leak.*","2876cdc6":"We can define the `SelectKBest` class to use the `f_classif()` function and select the features based on highest `selector.scores_` values (higher the better) and then we plot a graph of accuracy score by incrementing one feature everytime to decide how many top features we need for a better score.","3ca6a9f8":"Now with all the features selected we get train acc of 0.97 and val acc of 0.85. Lets see if we could do any better by selecting only important features and dropping those which are not so important","b78cec08":"Now we have predictions from 10 different models lets have a look how it looks like. Across columns we have the test sample data and across rows we have 10 models.","e9fe2ba4":"Lets look at some summary statistics of train data.","4847e8ae":"References:\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html  \nhttps:\/\/machinelearningmastery.com\/","53f92126":"# Mobile Price Range Classification\n\nHi, In this notebook I've used C-Support Vector Classification algorithm to classify mobile price from the [mobile-price-classification](https:\/\/www.kaggle.com\/iabhishekofficial\/mobile-price-classification) dataset. I've mainly focused on model building, feature selection, hyperparameter optimization and using K-fold cross validation techique to obtain a high accuracy results using a single model.\n\n1. Building a base line model with SVC\n2. Selecting useful features and removing redundant ones.\n3. Finding right set of parameters using optuna and 10 fold cross validation.\n4. Selecting the best trial making predictions and averaging it from folds.\n\n*Note*: This dataset is comparatively small. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using `LinearSVC` or `SGDClassifier` instead, possibly after a `Nystroem` transformer.","a20058c1":"## **Basic Descriptive statistics and EDA**","6b1654ea":"From the graph we notice that we get higher accuracy when we fit the model with somewhere around 12-14 features. Lets obtain the right value for the features by inspecting the results.","bb438946":"We notice Ram has highest correlation with price.","5ea42c4c":"## **Feature Selection and Fine tuning Model**\n\nThe scikit-learn library provides a bunch of functions we can use for selecting the best features based on univariate statistical tests.\n\n+ For regression: f_regression, mutual_info_regression\n+ For classification: chi2, f_classif, mutual_info_classif\n\nANOVA a.k.a \u201canalysis of variance\u201d and is a parametric statistical hypothesis test for determining whether the means from two or more samples of data (often three or more) come from the same distribution or not. \n  \nF-test, is a class of statistical tests that calculate the ratio between variances values, such as the variance from two different samples or the explained and unexplained variance by a statistical test, like ANOVA. \n  \nThe ANOVA method is a type of F-statistic referred to here as an ANOVA f-test. ANOVA is used when one variable is numeric and one is categorical, such as numerical input variables and a classification target variable in a classification task. The results of this test can be used for feature selection where those features that are independent of the target variable can be removed from the dataset.\n\n\n`SelectKBest` Removes all but the *k* highest scoring features.  \n`f_classif` Compute the ANOVA F-value for the provided sample","fd0c1c68":"There are no missing values in data. \n  \nLets take a look at pairwise relationships between features.","a0001c76":"## **Useful imports and data loading**","43459c44":"Finally we save the prediction results to a csv file"}}