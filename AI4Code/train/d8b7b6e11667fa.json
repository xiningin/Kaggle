{"cell_type":{"a814b1b2":"code","e132064b":"code","8baaf26a":"code","368e1479":"code","b70e7c88":"code","b50aa6d0":"code","44aca99c":"code","d18a9f03":"code","3396b671":"code","4cca1404":"code","6cc85dcd":"code","880eab4a":"code","20e3b39c":"code","3163c1b1":"code","3284d91f":"markdown","bb1229ed":"markdown","82148e71":"markdown","b8cbeb18":"markdown","e3b78a74":"markdown","eec6cfd3":"markdown","e8f5948d":"markdown","10cf9185":"markdown","ffb2078e":"markdown","33cc2b04":"markdown","f6d90391":"markdown","46c885d5":"markdown","4b0cc54c":"markdown","dc69f287":"markdown","f3106b66":"markdown","d8fdbe0b":"markdown"},"source":{"a814b1b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e132064b":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import Support Vector Classifier\nfrom sklearn.svm import SVC\n\n# for splitting the data into train and test sets\nfrom sklearn.model_selection import train_test_split\n\n# to evaluate the model\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\n\n# I will keep the resulting plots\n%matplotlib inline\n\n# Enable Jupyter Notebook's intellisense\n%config IPCompleter.greedy=True","8baaf26a":"# Load the data set\nbreast_cancer = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\n\n# Display first 5 rows of the DataFrame\ndisplay(breast_cancer.head())\n\n# Display the statistics\ndisplay(breast_cancer.describe())\n\n# Print info\nprint(breast_cancer.info())\n","368e1479":"# Drop the columns\nbreast_cancer.drop([\"Unnamed: 32\",\"id\"], axis=1, inplace=True)","b70e7c88":"# Print the counts\nprint(breast_cancer[\"diagnosis\"].value_counts())\n\n# Visualize the counts\nsns.countplot(breast_cancer[\"diagnosis\"])\nplt.show()","b50aa6d0":"# Visualize some 2-D features to see patterns\ndef make_scatterplot(x,y):\n    sns.scatterplot(x,y,data=breast_cancer,hue='diagnosis')\n    plt.title(y + \" vs \" + x)\n    plt.show()\n    \nmake_scatterplot('radius_mean', 'texture_mean')\nmake_scatterplot('perimeter_mean', 'area_mean')\nmake_scatterplot('smoothness_mean', 'smoothness_se')\nmake_scatterplot('concavity_mean', 'compactness_mean')\nmake_scatterplot('fractal_dimension_mean', 'perimeter_se')\nmake_scatterplot('symmetry_worst', 'concave points_worst')","44aca99c":"# Print the correlation matrix\nprint(breast_cancer.corr())","d18a9f03":"# Visualize with a heatmap\nfigure, ax = plt.subplots(figsize=(20,20))\nmask = np.triu(np.ones_like(breast_cancer.corr(), dtype=np.bool))\nsns.heatmap(breast_cancer.corr(), mask=mask, annot=True)\nplt.show()","3396b671":"# Plot the histograms\ndef plot_histogram(column):\n    sns.distplot(breast_cancer[column])\n    plt.title(column)\n    plt.show()\n\n\nplot_histogram(\"radius_mean\")\nplot_histogram(\"texture_mean\")\nplot_histogram(\"perimeter_mean\")\nplot_histogram(\"area_mean\")\nplot_histogram(\"smoothness_mean\")\nplot_histogram(\"compactness_mean\")\nplot_histogram(\"concavity_mean\")\nplot_histogram(\"concave points_mean\")\nplot_histogram(\"symmetry_mean\")\nplot_histogram(\"fractal_dimension_mean\")\n","4cca1404":"# import TSNE\nfrom sklearn.manifold import TSNE\n\n# fit and transform the TSNE model\ntsne = TSNE(learning_rate =  50)\ntsne_f = tsne.fit_transform(breast_cancer.drop(\"diagnosis\", axis=1))\n\n# Create a new DataFrame to store reduced features\ndf = pd.DataFrame({'x':tsne_f[:,0],'y':tsne_f[:,1]})\n\nprint(\"Before:\",breast_cancer.shape)\nprint(\"After\",df.shape)\n\ndisplay(df.head())\nsns.scatterplot(x='x', y='y', hue=breast_cancer['diagnosis'],data=df)\nplt.title(\"After Dimensionality Reduction\")\nplt.show()","6cc85dcd":"# Get features and the target\nX = breast_cancer.drop(\"diagnosis\", axis=1)\ny = breast_cancer[\"diagnosis\"]","880eab4a":"# Split the data as 30% test and 80% training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=34)","20e3b39c":"# Initialize the Support vector classifier\nsvc = SVC(kernel=\"linear\")\n\n# Fit the SVC with training sets\nsvc.fit(X_train, y_train)\n\nscores = cross_val_score(svc, X_train, y_train, cv=10, scoring='f1_macro')\n\nprint(scores)\n","3163c1b1":"# Make predictions on test set\ny_pred = svc.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\",acc)\n\nprint(\"\\n Classification Report\\n\")\nprint(classification_report(y_test, y_pred))","3284d91f":"Let's drop the **Unnamed: 32** and the **id** columns","bb1229ed":"Now, it gives us better understanding of the features","82148e71":"Look at the distributions of some features.","b8cbeb18":"Count the labels.","e3b78a74":"Let's look at the correlations.","eec6cfd3":"Select the features and the target.","e8f5948d":"Let's apply dimensionality reduction with t-SNE (t-distributed stochastic neighbor embedding) to see whole picture in 2-D space","10cf9185":"Now we can build our model and make predictions on test set","ffb2078e":"It looks like our model works pretty well! <br>\nThat's the end of the notebook. I am hoping that it will be helpful to understand basics of SVM.\n","33cc2b04":"# Introduction\nThe data set includes observations labeled as malignant (harmful) or benign (not harmful) cancer type. We'll try to classify labels using support vector machine. \n\n> Originally published [here](https:\/\/github.com\/Bhasfe\/ml-algorithms\/tree\/master\/SVC)\n\n### What is SVM ?\nSupport Vector Machines are mainly used for classification problems in Machine Learning. They can also be used for regression problems. A support vector machine tries to find best \"hyperplane\" which separates different classes. Following figure shows that a hyperplane whose margin is maximized.\n\n<img src=\"https:\/\/github.com\/Bhasfe\/ml-algorithms\/blob\/master\/SVC\/svm.png?raw=true\" width=\"300px\" height=\"300px\" align=\"left\" \/>\n<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\nIf the classes are not linearly separable, the kernel trick can be applied to make hyperplane. Kernel trick stands for applying the different kernel functions for multidimensional space to get desired dimensions (for example 2D to 3D ). The most populer kernels are RBF (radial basis function), polynomial and linear. Following figure shows us how kernel trick can be applied for 2D space to 3D space transformation\n\n<img src=\"https:\/\/github.com\/Bhasfe\/ml-algorithms\/blob\/master\/SVC\/kernel-trick.png?raw=true\" width=\"500px\" height=\"500px\" align=\"left\" \/>\n<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n","f6d90391":"Notes:\n   * There are 569 observations and 33 features.\n   * There is no missing values. \n   * **Unnamed: 32** and **id** columns should be dropped","46c885d5":"Let's look at the accuracy score and classification report","4b0cc54c":"Now it's time to build a machine learning model. First of all, we'll split the data into training and test sets.","dc69f287":"# 2. Machine Learning","f3106b66":"# 1. EDA + FE\n\nFirstly, we need to import necessary libraries\/packages","d8fdbe0b":"Secondly, load the breast cancer dataset and start to explore it."}}