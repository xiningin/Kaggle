{"cell_type":{"6839582d":"code","8a629e46":"code","fd474776":"code","270d679d":"code","5a3b7c88":"code","5cd18317":"code","527701b3":"code","eeb85b6e":"code","72670d23":"code","d3689089":"code","76fab096":"code","3cec5000":"code","02b8cb50":"code","d5ad6dfb":"code","7b825e69":"code","33a4c0ef":"code","f492e7df":"code","d0937953":"code","cc6715a3":"code","265c2cff":"code","01b906f9":"code","86064f3e":"code","fac33e95":"code","51722e70":"code","31fc4376":"code","d1d780ea":"code","348605b5":"code","b992e3eb":"code","89ee1825":"code","439dcb1e":"code","c8ff63af":"code","a92316d3":"code","af42d379":"code","d986b7c6":"code","1f79db84":"code","7c8a3e68":"code","c070adca":"code","88acc988":"code","e9663f03":"code","bae3da65":"code","0edffe45":"code","0c76c807":"code","70b3f8b6":"code","a01e9b9e":"code","6bb2490f":"code","4758ee2d":"code","8cb0dbcb":"code","e51389b7":"code","0458f953":"code","0e2185e0":"code","f536d18a":"code","b0a74f05":"code","0ed9caec":"code","a34fc785":"code","a1c70498":"code","93416f61":"code","b4fd7f14":"code","ed218a4a":"code","ae3fefb4":"code","94e0de75":"code","f703a4a8":"code","f4448bd1":"code","8ed76f74":"code","faed9444":"code","b4930a35":"code","9c1a18a5":"code","11722273":"code","d58fc125":"code","dcf0f43e":"code","24c5a9e5":"code","166e26af":"code","d278f982":"code","0b204763":"code","6d321836":"code","16bfda4f":"code","6dcf85eb":"code","e0fe3d3d":"code","6e3b2343":"code","cc102a2d":"code","06f19944":"code","7c790eb0":"code","b636f198":"code","d2b1742a":"code","86df4aa6":"code","b1f88a52":"code","c97233df":"code","594bd331":"code","ffbeaaf6":"code","9db58a54":"code","041887f9":"code","7c9abd12":"code","4c443151":"code","3ec520a1":"code","0dc9d82d":"code","a8903650":"code","3915d74d":"code","e1672b80":"code","83d5e41b":"code","d9aa5bb5":"code","9dc36680":"code","c307b76e":"code","ff89b162":"code","bcb415a1":"code","fb23b902":"code","bca86105":"code","0c5e884e":"code","7349a085":"code","7d0a9cde":"code","dc854f32":"code","1d33538f":"code","fed9c937":"code","7fc294f0":"code","67335c6e":"code","6467dc88":"code","4d4c8240":"code","b1a4888a":"code","71b8058d":"code","604661d6":"markdown","a2ee9b73":"markdown","4d79236c":"markdown","71891a53":"markdown","92beb4b9":"markdown","2c8f1141":"markdown","b84652fb":"markdown","11e442cc":"markdown","a03b717e":"markdown","b0f46d7c":"markdown","dfb9604c":"markdown","48d1d36e":"markdown","b84a936e":"markdown","74f534e0":"markdown","df4b3047":"markdown","a720feb4":"markdown","6b816ac9":"markdown","51f1d5e4":"markdown","36f929c7":"markdown","c80f9e21":"markdown","22af5533":"markdown","7fd4690a":"markdown","0dea0a28":"markdown","0b6f80eb":"markdown","a4f5de4b":"markdown","8e7206d9":"markdown","e97b86e7":"markdown","b11f740a":"markdown","a60add67":"markdown","cdf62bf5":"markdown","8d383508":"markdown","52bac3fa":"markdown","65bcb13c":"markdown","46e022cb":"markdown","f70b216b":"markdown","2dcaa34b":"markdown","84a64a0f":"markdown","bc7bf35d":"markdown","31267fcc":"markdown","09dbcb61":"markdown","a97b737d":"markdown","49ab8f4d":"markdown","d8d50646":"markdown","4f356a50":"markdown","b46bc8a8":"markdown","f68f0c06":"markdown","3635b220":"markdown","5cf2fb02":"markdown","54ae9bff":"markdown","e0c7f3e5":"markdown","4a4418f1":"markdown","c4682fcf":"markdown","25247c53":"markdown","1a45410c":"markdown","3a0e1c75":"markdown","9b19fc95":"markdown","ed70f9c5":"markdown","adb71d37":"markdown","edfc591e":"markdown","db9b501d":"markdown","2df2b889":"markdown","dbfd6bc6":"markdown","377818ea":"markdown","12ef21ab":"markdown","d919ae33":"markdown","0efdbc0d":"markdown","0b2891fe":"markdown","694c59fe":"markdown","f7842d18":"markdown","2ab01824":"markdown","d62b84c8":"markdown","6c841ff8":"markdown","36ee2c21":"markdown","f8e71610":"markdown","c74c1133":"markdown","3a9e88ee":"markdown","2e18d44c":"markdown","146d8c11":"markdown","c63df453":"markdown","0b11ce89":"markdown","e3cad3c9":"markdown","1bd9e645":"markdown","7421dff2":"markdown","f62d694c":"markdown","240fbf16":"markdown","988feb2d":"markdown","0c854a56":"markdown","f0f731f0":"markdown","6237cf3e":"markdown","221e7a52":"markdown","5bcf5d00":"markdown","b7207910":"markdown","7e67872e":"markdown","0367dba9":"markdown","2b107230":"markdown","b99feff0":"markdown","0bf0d947":"markdown","44348205":"markdown","6d3aa172":"markdown","7906aabb":"markdown","1eb5f41c":"markdown","e4392949":"markdown","56122819":"markdown","d12d5497":"markdown","bbdd7ebc":"markdown","f0048bdd":"markdown","b17e9986":"markdown","ff017bfc":"markdown","daca4b8d":"markdown","d7e66e2f":"markdown","1930e869":"markdown","6e9e5e19":"markdown","65212266":"markdown","5e4205a5":"markdown","0c73e684":"markdown","2f2c5b73":"markdown","3f29c066":"markdown","3862ca84":"markdown","09fdad54":"markdown","c445cadd":"markdown","31807f9c":"markdown","fb50f970":"markdown","2f45e113":"markdown","db838e2a":"markdown","6d331ad7":"markdown","4209b3b5":"markdown"},"source":{"6839582d":"# install comet\n# !pip install comet_ml","8a629e46":"# importing Experiment from comet\n# from comet_ml import Experiment","fd474776":"# # Linking our current workspace to comet by creating an experiment with our api key:\n# experiment = Experiment(\n#     api_key=\"YBjEZqF3vM9CQLf2Lx7GeSw0C\",\n#     project_name=\"general\",\n#     workspace=\"mpho-mokhokane\",\n# )","270d679d":"# downloads\n#!pip install parfit\n!pip install scikit-plot\n#!pip install contractions\n\n\n# imports for Natural Language  Processing\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport re\nfrom sklearn.pipeline import Pipeline\nimport pickle\n\n\n# feature extractioin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Preprocessing\n#import contractions\nfrom collections import Counter\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.model_selection import train_test_split\n\n# classification models\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Hyperparameter tunning methods\n#import parfit.parfit as pf\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import ParameterGrid\n\n# metrics\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n# imports for data visualisation\nimport seaborn as sns\nfrom PIL import Image\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom nltk.probability import FreqDist\nfrom sklearn.metrics import plot_roc_curve\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom scikitplot.metrics import plot_roc, plot_confusion_matrix\n%matplotlib inline","5a3b7c88":"# Importing the train & test data sets\ntrain = pd.read_csv('..\/input\/climate-change-edsa2020-21\/train.csv')\ntest = pd.read_csv('..\/input\/climate-change-edsa2020-21\/test.csv')\n\n#Create a copy for EDA\ntrain_eda = train.copy()\n\n# Create copies for modeling\ntrain_data = train.copy()\ntest_data = test.copy()","5cd18317":"train.head()","527701b3":"test.head()","eeb85b6e":"#Cheching if there are missing values in the Train dataset\ntrain.isna().info()","72670d23":"#Cheching if there are missing values in the Test dataset\ntest.isna().info()","d3689089":"# Part of Speech for modeling\ndef POS(word):\n    \"\"\"\n    This function gets the part of speech\n    \"\"\"\n    pos_counts = Counter()\n    probable_part_of_speech = wordnet.synsets(word)\n    pos_counts[\"n\"] = len([i for i in probable_part_of_speech if i.pos()==\"n\"])\n    pos_counts[\"v\"] = len([i for i in probable_part_of_speech if i.pos()==\"v\"])\n    pos_counts[\"a\"] = len([i for i in probable_part_of_speech if i.pos()==\"a\"])\n    pos_counts[\"r\"] = len([i for i in probable_part_of_speech if i.pos()==\"r\"])\n    part_of_speech = pos_counts.most_common(1)[0][0]\n    return part_of_speech","76fab096":"# Extract mentions\n\ndef extractor(df):\n    # your code here\n    \"\"\"\" \n        Returns a DataFrame with three additional columns: \n        \"hashtags\", \"mentions\" and \"url\"\n         Args:\n             DataFrame: DateFrame with Data,Index and Colums\n        Return:\n             DataFrame: A DataFrame with additional columns with data\n        Egs:\n             df['new colum'] = col_name\n    \"\"\"\n    # Extract hashtags\n    tweets = df['message']\n    df['hashtags'] = df.message. str.lower().str.findall(r'#.*?(?=\\s|$)')\n    htags = df['hashtags']\n    df['hashtags'] = htags.apply(lambda x: np.nan if len(x) == 0 else x)\n    \n    # Extract mentions\n    df['mentions'] = df.message. str.lower().str.findall(r'@\\w*')\n    mtags = df['mentions']\n    df['mentions'] = mtags.apply(lambda x: np.nan if len(x) == 0 else x)\n    \n    # Extract url\n    df['url'] = df.message. str.lower().str.findall(r'http\\S+|www.\\S+')\n    urltags = df['url']\n    df['url'] = urltags.apply(lambda x: np.nan if len(x) == 0 else x)\n    \n    return df\n","3cec5000":"extractor(train)","02b8cb50":"# Remove URLs\ntrain['message'] = train['message'].str.replace('http\\S+|www.\\S+', '', case=False)\n\n# Remove mentions\ntrain['message'] = train['message'].str.replace('@\\w*', '', case=False)\n\n# Removal hashtags\ntrain['message'] = train['message'].str.replace('#.*?(?=\\s|$)', '', case=False)\n\n# Remove 'RT'\ntrain['message'] = train['message'].str.replace('RT', '', case=False)\n\n# Remove stopwords\ntrain.head()","d5ad6dfb":"#train['message'] = train['message'].apply(lambda x: contractions.fix(x))","7b825e69":"# Clean tweets\ndef clean_tweets(df):\n    '''\n    This function cleans the tweets by tokenizing, removing punctuation, \n    removing digits and removing 1 character tokens\n    \n    '''\n\n    # tokenizing the tweets\n    clean_tweets = df['message'].apply(TweetTokenizer().tokenize) ## first we tokenize\n\n    # remove punctuation\n    clean_tweets = clean_tweets.apply(lambda x : [token for token in x if token not in string.punctuation])\n\n    # removing digits from the tweets\n    clean_tweets = clean_tweets.apply(lambda x: [token for token in x if token not in list(string.digits)])\n\n    # lastly we remove all one character tokens\n    clean_tweets = clean_tweets.apply(lambda x: [token for token in x if len(token) > 1])\n    \n    df['cleaned_tweets'] = clean_tweets\n    \n    return df['cleaned_tweets']","33a4c0ef":"clean_tweets(train)","f492e7df":"from nltk.corpus import stopwords\nnltk.download('stopwords')\n\nstop = stopwords.words('english')\n\n# Convert to lower case\ntrain['cleaned_tweets'] = train['cleaned_tweets'].apply(lambda x: [word.lower() for word in x])\n\n# Remove stopwords\ntrain['no_stopwords'] = train['cleaned_tweets'].apply(lambda x: [item for item in x if item not in stop])\n\ntrain.head()","d0937953":"collection_words = ['climatechange', 'climate', 'change']\ntrain['no_colwords'] = [[w for w in word if not w in collection_words]\n                 for word in train['no_stopwords']]\n\ntrain.head()","cc6715a3":"from collections import Counter\ncnt = Counter()\nfor message in train['no_stopwords'].values:\n    for word in message:\n        cnt[word] += 1\n        \n# cnt.most_common()\n","265c2cff":"train['sent_labels']  = train['sentiment'].map({-1: 'Anti',0:'Neutral', 1:'Pro', 2:'News'})\ntrain_eda['sent_labels']  = train['sentiment'].map({-1: 'Anti',0:'Neutral', 1:'Pro', 2:'News'})","01b906f9":"train['text_length'] = train_eda['message'].apply(lambda x: len(x))","86064f3e":"# Dataframe for neutral tweets\nneutral_df = train[train['sentiment'] == 0]\nneutral_df.head()","fac33e95":"# Dataframe for pro tweets\npro_df = train[train['sentiment'] == 1]\npro_df.head()","51722e70":"# Dataframe for anti tweets\nanti_df = train[train['sentiment'] == -1]\nanti_df.head()","31fc4376":"# Dataframe for news tweets\nnews_df = train[train['sentiment'] == 2]\nnews_df.head()","d1d780ea":"# Display target distribution\n\ndisplay(print(\"Percentage contribution:\\n\",train.sent_labels.value_counts(normalize=True)*100))\n\nfig, axes = plt.subplots(ncols=2, \n                         nrows=1, \n                         figsize=(20, 10), \n                         dpi=100)\n\nsns.countplot(train['sent_labels'], ax=axes[0])\n\ncode_labels=['Pro', 'News', 'Neutral', 'Anti']\naxes[1].pie(train['sent_labels'].value_counts(),\n            labels= code_labels,\n            autopct='%1.0f%%',\n            startangle=90,\n            explode = (0.1, 0.1, 0.1, 0.1))\n\nfig.suptitle('Count for each sentiment class', fontsize=20)\nplt.show()","348605b5":"from pylab import rcParams\nrcParams['figure.figsize'] = 12, 5\n\nsns.boxplot(x=train['sent_labels'], y=train['text_length'], \n            data=train_eda, width = 0.9, color = 'orange')\nplt.ylabel('Lenght of the message')\nplt.xlabel('Sentiment class')\nplt.title('Message length for each sentiment class')\nplt.show()","b992e3eb":"#Compare the sentiment analysis from raw data\n\nfrom textblob import TextBlob\ntrain_eda['polarity']=train_eda['message'].apply(lambda x:\n                                             TextBlob(x).sentiment.polarity)\n\nplt.figure(figsize=[10, 8])\nplt.hist(train_eda['polarity'], bins=25, linewidth=0)\nplt.gca().set(title='Message polarity of raw data',\n              ylabel='Frequency', xlabel = 'Sentiment value')\nplt.axvline(train_eda['polarity'].mean(), color='black',\n            linestyle='dashed', linewidth=1)\nplt.axvline(train_eda['polarity'].median(), color='orange',\n            linestyle='dashed', linewidth=1)","89ee1825":"#Compare the sentiment analysis from cleaned data\n\n# Convert the cleaned message from list to string\ntrain_eda['clean_msg'] = train['no_colwords'].str.join(' ')\n\ntrain_eda['polarity'] = train_eda['clean_msg'].apply(lambda x:\n                                             TextBlob(x).sentiment.polarity)\n\nplt.figure(figsize=[10, 8])\nplt.hist(train_eda['polarity'], bins=25, linewidth=0)\nplt.gca().set(title='Message polarity of cleaned data',\n              ylabel='Frequency', xlabel = 'Sentiment value')\nplt.axvline(train_eda['polarity'].mean(), color='black',\n            linestyle='dashed', linewidth=1)\nplt.axvline(train_eda['polarity'].median(), color='orange',\n            linestyle='dashed', linewidth=1)","439dcb1e":"# Download the image to mask the wordcloud\n\n#mask = np.array(Image.open('10wmt-superJumbo-v4.jpg'))\n\ntrain_word = train[\"no_colwords\"].str.join(' ')\n#wordcloud = WordCloud(width=1000, height=1000, mask = mask, background_color=\n                      #'white').generate(str(mostcommon))\nwordcloud = WordCloud(width=1000, height=1000, background_color=\n                      'white').generate(str(train_word))\nfig = plt.figure(figsize=(30,10), facecolor='white')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","c8ff63af":"# Convert list to strings for each sentiment class\nnews = news_df[\"no_colwords\"].str.join(' ')\nneutral = neutral_df[\"no_colwords\"].str.join(' ')\npro = pro_df[\"no_colwords\"].str.join(' ')\nanti = anti_df[\"no_colwords\"].str.join(' ')\n\n#Visualize each sentiment class\n\nfig, axis = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\n\nnews_wordcloud = WordCloud(width=900, height=600, background_color='black', colormap='winter').generate(str(news))\naxis[0, 0].imshow(news_wordcloud)\naxis[0, 0].set_title('News Class',fontsize=14)\naxis[0, 0].axis(\"off\") \n\nneutral_wordcloud = WordCloud(width=900, height=600, background_color='black', colormap='winter', min_font_size=10).generate(str(neutral))\naxis[1, 0].imshow(neutral_wordcloud)\naxis[1, 0].set_title('Neutral Class',fontsize=14)\naxis[1, 0].axis(\"off\") \n\npro_wordcloud = WordCloud(width=900, height=600, background_color='black', colormap='winter', min_font_size=10).generate(str(pro))\naxis[0, 1].imshow(pro_wordcloud)\naxis[0, 1].set_title('Pro Class',fontsize=14)\naxis[0, 1].axis(\"off\") \n\nanti_wordcloud = WordCloud(width=900, height=600, background_color='black', colormap='winter', min_font_size=10).generate(str(anti))\naxis[1, 1].imshow(anti_wordcloud)\naxis[1, 1].set_title('Anti Class',fontsize=14)\naxis[1, 1].axis(\"off\") \n\nplt.show()","a92316d3":"# Create hashtag df and remove nulls \ndef hashtags(df):\n    '''\n    This function takes in a dataframe as input.\n    It returns dataframe with no null values.\n    '''\n    df = df[['hashtags', 'sent_labels']]\n    df = df[df['hashtags'].notnull()]\n    return df","af42d379":"hashtags(news_df).head()","d986b7c6":"def hashtag_count(df):\n    '''\n    This function takes a dataframe as input.\n    It returns a dataframe with count for hashtags.\n    '''\n    hashtag = sum(df['hashtags'], [])\n    count = nltk.FreqDist(hashtag)  \n    hashtag = pd.DataFrame({'hashtags': list(count.keys()),\n                           'count': list(count.values())})\n    hashtag = hashtag.nlargest(20, columns=\"count\")\n\n    return hashtag","1f79db84":"hashtag_count(hashtags(news_df)).head()","7c8a3e68":"# Visualize the count for each class\ndef hastag_plot(df, x):\n    '''\n    This function plots a barplot for an iput dataframe, \n    given x as the title of the plot.\n    '''\n    plt.figure(figsize=(12, 6))\n    ax = sns.barplot(data=df, y =df['hashtags'], x =df['count'], orient='h', color = 'blue')\n    plt.title(x,fontsize=14)\n    plt.xlabel('Hashtag Count')\n    plt.ylabel('Hashtags Used')\n    return plt.show()","c070adca":"hastag_plot(hashtag_count(hashtags(train)), x = 'Top 20 hashtags from the whole data')","88acc988":"hastag_plot(hashtag_count(hashtags(news_df)), x = 'Top 20 hashtags for News Sentiment')","e9663f03":"hastag_plot(hashtag_count(hashtags(pro_df)), x = 'Top 20 hashtags for Pro Sentiment')","bae3da65":"hastag_plot(hashtag_count(hashtags(neutral_df)), x = 'Top 20 hashtags for neutral Sentiment')","0edffe45":"hastag_plot(hashtag_count(hashtags(anti_df)), x = 'Top 20 hashtags for anti Sentiment')","0c76c807":"# Data preprocessing for model building\n\ntrain = train_data.copy()\ntest = test_data.copy()\nnormalizer = WordNetLemmatizer()\nnltk.download('wordnet')\n\ndef replace_sentiments(df):\n    \"\"\"\n    replace keywords in the tweets  with associated sentiments\n    \n    'global' ----> 'negative'\n    'climate' ----> 'positive'\n    'MAGA'----> 'negative'\n  \n    \"\"\"\n    df['message'] = df['message'].apply(lambda x: x.replace('global', 'negative'))\n    df['message'] = df['message'].apply(lambda x: x.replace('climate', 'positive'))\n    df['message'] = df['message'].apply(lambda x: x.replace('MAGA', 'negative')) \n\n    return df['message']\n\ntrain['message'] = replace_sentiments(train)\ntest['message'] = replace_sentiments(test)\n\n# cleaning tweets\ntrain['clean_tweets'] = clean_tweets(train)\ntest['clean_tweets'] = clean_tweets(test)\n\n\n# lemmatize\ntrain['clean_tweets'] = train['clean_tweets'].apply(lambda x: [normalizer.lemmatize(token, POS(token)) for token in x])\ntest['clean_tweets'] = test['clean_tweets'].apply(lambda x: [normalizer.lemmatize(token, POS(token)) for token in x])","70b3f8b6":"X = train['clean_tweets']\ny = train['sentiment']\nX_test = test['clean_tweets']","a01e9b9e":"# Splitting the data into 90% train and 10% validation set\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state = 42)","6bb2490f":"X_train = list(X_train.apply(' '.join))\nX_val = list(X_val.apply(' '.join))\nX_test = list(X_test.apply(' '.join))","4758ee2d":"# DecisionTreeClassifier Pipeline\ntree_tfidf = Pipeline([('tfidf', TfidfVectorizer()),('tree', DecisionTreeClassifier()),])\ntree_count = Pipeline([('CountVec',  CountVectorizer(analyzer = 'word', \n                             tokenizer = None, \n                             preprocessor = None, \n                             stop_words = None, \n                             max_features = 180000,\n                             min_df = 1,\n                             ngram_range = (1,2)\n                            )),('tree', DecisionTreeClassifier()),])\n\n\n# RandomForestClassifier Pipeline\nrfc_tfidf = Pipeline([('tfidf', TfidfVectorizer()), ('rfc', RandomForestClassifier())])\nrfc_count = Pipeline([('CountVec',  CountVectorizer(analyzer = 'word', \n                             tokenizer = None, \n                             preprocessor = None, \n                             stop_words = None, \n                             max_features = 180000,\n                             min_df = 1,\n                             ngram_range = (1,2)\n                            )),('rfc', RandomForestClassifier()),])\n\n\n# LinearSVC Pipeline\nLsvc_tfidf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('scv', LinearSVC()),])\nLsvc_count = Pipeline([('CountVec',  CountVectorizer(analyzer = 'word', \n                             tokenizer = None, \n                             preprocessor = None, \n                             stop_words = None, \n                             max_features = 180000,\n                             min_df = 1,\n                             ngram_range = (1,2)\n                            )),('svc', LinearSVC()),])\n\n\n\n# Logistic Regression pipeline\nlogreg_tfidf = Pipeline([('tfidf', TfidfVectorizer()),('logistic', LogisticRegression()),])\nlogreg_count = Pipeline([('CountVec',  CountVectorizer(analyzer = 'word', \n                             tokenizer = None, \n                             preprocessor = None, \n                             stop_words = None, \n                             max_features = 180000,\n                             min_df = 1,\n                             ngram_range = (1,2)\n                            )),('logistic', LogisticRegression()),])\n\n\n\n# SGD Classifier pipeline\nSGD_tfidf = Pipeline([('tfidf', TfidfVectorizer()), ('SGD', SGDClassifier())])\nSGD_count = Pipeline([('CountVec',  CountVectorizer(analyzer = 'word', \n                             tokenizer = None, \n                             preprocessor = None, \n                             stop_words = None, \n                             max_features = 180000,\n                             min_df = 1,\n                             ngram_range = (1,2)\n                            )),('SGD', SGDClassifier()),])\n\n\n\n\n# Support Vector Classifier Pipeline\nsvc_tfidf = Pipeline([('tfidf', TfidfVectorizer()), ('SVC', SVC())])\nsvc_count = Pipeline([('CountVec',  CountVectorizer(analyzer = 'word', \n                             tokenizer = None, \n                             preprocessor = None, \n                             stop_words = None, \n                             max_features = 180000,\n                             min_df = 1,\n                             ngram_range = (1,2)\n                            )),('SVC', SVC()),])\n\n\nridge_tfidf = Pipeline([('tfidf', TfidfVectorizer()), ('Ridge', RidgeClassifier())])\nridge_count = Pipeline([('CountVec',  CountVectorizer(analyzer = 'word', \n                             tokenizer = None, \n                             preprocessor = None, \n                             stop_words = None, \n                             max_features = 180000,\n                             min_df = 1,\n                             ngram_range = (1,2)\n                            )),('Ridge', RidgeClassifier()),])","8cb0dbcb":"# training the decision tree pipeline\ntree_tfidf.fit(X_train, y_train)\ntree_count.fit(X_train, y_train)\n\n# training the RandomForest pipleline\nrfc_tfidf.fit(X_train, y_train)\nrfc_count.fit(X_train, y_train)\n\n# training the LinearSVC pipeline\nLsvc_tfidf.fit(X_train, y_train)\nLsvc_count.fit(X_train, y_train)\n\n# training the logistic regression pipeline\nlogreg_tfidf.fit(X_train, y_train)\nlogreg_count.fit(X_train, y_train)\n\n# training the SGD Classifier\nSGD_tfidf.fit(X_train, y_train)\nSGD_count.fit(X_train, y_train)\n\n# training the support vector classifier\nsvc_tfidf.fit(X_train, y_train)\nsvc_count.fit(X_train, y_train)\n\n# training the Ridge classifier\nridge_tfidf.fit(X_train, y_train)\nridge_count.fit(X_train, y_train)","e51389b7":"# calculating average scores for the TFIDF\ntree_acc =  accuracy_score(y_val,tree_tfidf.predict(X_val))\nrfc_acc = accuracy_score(y_val,rfc_tfidf.predict(X_val))\nlsvc_acc = accuracy_score(y_val,Lsvc_tfidf.predict(X_val))\nlog_acc = accuracy_score(y_val, logreg_tfidf.predict(X_val))\nsgd_acc = accuracy_score(y_val, SGD_tfidf.predict(X_val))\nsvc_acc = accuracy_score(y_val, svc_tfidf.predict(X_val))\nridge_acc = accuracy_score(y_val, ridge_tfidf.predict(X_val))\n\ntfidf_avg_accuracy = round(np.mean([tree_acc,rfc_acc,lsvc_acc,log_acc,sgd_acc,svc_acc,ridge_acc]),4)","0458f953":"tree_acc =  accuracy_score(y_val,tree_count.predict(X_val))\nrfc_acc = accuracy_score(y_val,rfc_count.predict(X_val))\nlsvc_acc = accuracy_score(y_val,Lsvc_count.predict(X_val))\nlog_acc = accuracy_score(y_val, logreg_count.predict(X_val))\nsgd_acc = accuracy_score(y_val, SGD_count.predict(X_val))\nsvc_acc = accuracy_score(y_val, svc_count.predict(X_val))\nridge_acc = accuracy_score(y_val, ridge_count.predict(X_val))\n\nCountVec = round(np.mean([tree_acc,rfc_acc,lsvc_acc,log_acc,sgd_acc,svc_acc, ridge_acc]),4)","0e2185e0":"accuracy_dict = {'TFIDF':[tfidf_avg_accuracy], 'CountVec':[CountVec]}\naccuracy_df = pd.DataFrame(accuracy_dict)\naccuracy_df.T.plot(kind='barh', figsize = (8,8),legend = False)\nplt.xlabel('Avg Accuracy Score')\nplt.ylabel('Feature Engineering Method')\nplt.yticks(rotation = 40)\nplt.title('Avg accuracy score per method')\nplt.show()","f536d18a":"# Making validations set predicions\n\ntree_prediction = tree_count.predict(X_val) # DecisionTreeClassifier predictions\nrfc_prediction = rfc_count.predict(X_val) # RandomForestClassifier predictions\nLsvc_prediction = Lsvc_count.predict(X_val) # LinearSVClassifier Predictions\nlogreg_prediction = logreg_count.predict(X_val) # Logistic regression predictions\nSGD_prediction = SGD_count.predict(X_val) # SGD Classifier predictions\nSVC_prediction = svc_count.predict(X_val) # Support vector machine predictions\nridge_prediction = ridge_count.predict(X_val) # Ridge predictions","b0a74f05":"print(confusion_matrix(y_val, tree_prediction))","0ed9caec":"print('\\nDecision Tree\\n', classification_report(y_val, tree_prediction))\nplot_confusion_matrix(y_val, tree_prediction, normalize=True,figsize=(8,8),cmap='Blues')\nplt.title('Decision Tree Confusion Matrix')\nplt.show()","a34fc785":"# # Visual represetation of of the f1 score for each class\nreport_tree = classification_report(y_val, tree_prediction, output_dict=True)\ndf_tree = pd.DataFrame(report_tree).transpose()\ndf_tree.drop(['accuracy'], inplace = True)\ndf_tree.sort_values(by=['f1-score'],ascending=True, inplace = True)\ndf_tree.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\nplt.xlabel('f1-score')\nplt.ylabel('sentiment class')\nplt.yticks(rotation = 40)\nplt.title('f1-score per sentiment class for Decision Tree Classiffier')\nplt.show()","a1c70498":"# Print the overall accuracy\ndecison_tree_acc = round(accuracy_score(y_val, tree_prediction),4)\nprint(f'\\nOverall accuracy score for Decision Tree : {decison_tree_acc}')\ndecision_tree_f1 = round(f1_score(y_val, tree_prediction, average=\"weighted\"),4)\nprint(f'\\nWeighted avg f1 score Decision Tree {decision_tree_f1}' )\n","93416f61":"print('\\nRandomForestClassifier\\n', confusion_matrix(y_val, rfc_prediction))\n","b4fd7f14":"print('\\nRandomForestClassifier\\n', classification_report(y_val, rfc_prediction))\nplot_confusion_matrix(y_val, rfc_prediction, normalize=True,figsize=(8,8),cmap='Blues')\nplt.title('Random Forest Classification')\nplt.show()","ed218a4a":"# # Visual represetation of of the f1 score for each class\nreport_rfc = classification_report(y_val, rfc_prediction, output_dict=True)\ndf_rfc = pd.DataFrame(report_rfc).transpose()\ndf_rfc.drop(['accuracy'], inplace = True)\ndf_rfc.sort_values(by=['f1-score'],ascending=True, inplace = True)\ndf_rfc.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\nplt.xlabel('f1-score')\nplt.ylabel('sentiment class')\nplt.yticks(rotation = 40)\nplt.title('f1-score per sentiment class for Random Forest Classiffier')\nplt.show()","ae3fefb4":"random_forest_acc = round(accuracy_score(y_val, rfc_prediction),4)\nprint(f'\\nOveral accuracy score for RandomForestClassifier :{random_forest_acc}')\nrandom_forest_f1 = round(f1_score(y_val, rfc_prediction, average=\"weighted\"),4)\nprint(f'\\nWeighted f1 score for RandomForestClassifier : {random_forest_f1}')","94e0de75":"print('\\nLinearSVC Model\\n', confusion_matrix(y_val, Lsvc_prediction))","f703a4a8":"print('\\nLinearSVC Model\\n', classification_report(y_val, Lsvc_prediction))\nplot_confusion_matrix(y_val, Lsvc_prediction, normalize=True,figsize=(8,8),cmap='Blues')\nplt.title('LinearSCV Classification')\nplt.show()","f4448bd1":"# # Visual represetation of of the f1 score for each class\nreport_svc = classification_report(y_val, Lsvc_prediction, output_dict=True)\ndf_svc = pd.DataFrame(report_svc).transpose()\ndf_svc.drop(['accuracy'], inplace = True)\ndf_svc.sort_values(by=['f1-score'],ascending=True, inplace = True)\ndf_svc.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\nplt.xlabel('f1-score')\nplt.ylabel('sentiment class')\nplt.yticks(rotation = 40)\nplt.title('f1-score per sentiment class for LinearSVC ')\nplt.show()","8ed76f74":"linearSVC_acc = round(accuracy_score(y_val, Lsvc_prediction),4)\nprint(f'\\nOverall accuracy score for LinearSVC Model : {linearSVC_acc}')\nlinearSVC_f1 = round(f1_score(y_val, Lsvc_prediction, average=\"weighted\"),4)\nprint(f'\\nWeighted avg f1 score for LinearSVC Model : {linearSVC_f1}')","faed9444":"# Report the confusion matrix\nprint('\\nLogistic Regression\\n', confusion_matrix(y_val, logreg_prediction))","b4930a35":"# Print a classification report\n\nprint('\\nLogistic Regression\\n', classification_report(y_val, logreg_prediction))\nplot_confusion_matrix(y_val, logreg_prediction, normalize=True,figsize=(8,8),cmap='Blues')\nplt.title('Logistic Regression Classification')\nplt.show()\n\n","9c1a18a5":"# # Visual represetation of of the f1 score for each class\nreport_logreg = classification_report(y_val, logreg_prediction, output_dict=True)\ndf_logreg = pd.DataFrame(report_logreg).transpose()\ndf_logreg.drop(['accuracy'], inplace = True)\ndf_logreg.sort_values(by=['f1-score'],ascending=True, inplace = True)\ndf_logreg.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\nplt.xlabel('f1-score')\nplt.ylabel('sentiment class')\nplt.yticks(rotation = 40)\nplt.title('f1 score per sentiment class for Logistic Regression')\nplt.show()","11722273":"# Print the overall accuracy\nlogistic_reg_acc = round(accuracy_score(y_val, logreg_prediction),4)\nprint('\\nLogistic Regression accuracy Score\\n', logistic_reg_acc)\nlogistic_reg_f1 = round(f1_score(y_val, logreg_prediction, average=\"weighted\"),4)\nprint('\\nLogistic Regression weighted f1 score\\n', logistic_reg_f1)\n","d58fc125":"# Confusion matrix for the random forest classifier\nprint('\\nSGD Classifier\\n', confusion_matrix(y_val, SGD_prediction))","dcf0f43e":"# The classification report \nprint('\\nSGD Classifier  Classification report :\\n', classification_report(y_val, SGD_prediction))\nplot_confusion_matrix(y_val, SGD_prediction, normalize=True,figsize=(8,8),cmap='Blues')\nplt.title('Logistic Regression Classification')\nplt.show()\n","24c5a9e5":"# # Visual represetation of of the f1 score for each class\nreport_sgd = classification_report(y_val, SGD_prediction, output_dict=True)\ndf_sgd = pd.DataFrame(report_sgd).transpose()\ndf_sgd.drop(['accuracy'], inplace = True)\ndf_sgd.sort_values(by=['f1-score'],ascending=True, inplace = True)\ndf_sgd.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\nplt.xlabel('f1-score')\nplt.ylabel('sentiment class')\nplt.yticks(rotation = 40)\nplt.title('f1-score per sentiment class for SGD')\nplt.show()","166e26af":"# Checking the accuracy score\nsgd_acc = round(accuracy_score(y_val, SGD_prediction),4)\nprint('\\nSGD Classifier accuracy Score :\\n', sgd_acc)\n\n# Checking the f1_score report for the decison tree model\nsgd_f1 = round(f1_score(y_val, SGD_prediction, average=\"weighted\"),4)\nprint('\\nSGD weighted avg f1_score :\\n', sgd_f1)","d278f982":"# Confusion matrix for the random forest classifier\nprint('\\nSupport Vector Classifier\\n', confusion_matrix(y_val, SVC_prediction))","0b204763":"# The classification report \nprint('\\nSupport Vector Classifier  Classification report :\\n', classification_report(y_val, SVC_prediction))\nplot_confusion_matrix(y_val, SVC_prediction, normalize=True,figsize=(8,8),cmap='Blues')\nplt.title('Support Vector Classification')\nplt.show()\n","6d321836":"# # Visual represetation of of the f1 score for each class\nreport_svc = classification_report(y_val, SVC_prediction, output_dict=True)\ndf_SVC = pd.DataFrame(report_svc).transpose()\ndf_SVC.drop(['accuracy'], inplace = True)\ndf_SVC.sort_values(by=['f1-score'],ascending=True, inplace = True)\ndf_SVC.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\nplt.xlabel('f1-score')\nplt.ylabel('sentiment class')\nplt.yticks(rotation = 40)\nplt.title('f1-score per sentiment class for SVC')\nplt.show()","16bfda4f":"# Checking the accuracy score\nsvc_acc = round(accuracy_score(y_val, SVC_prediction),4)\nprint(f'\\nSupport Vector Classifier accuracy Score : {svc_acc}')\nsvc_f1 = round(f1_score(y_val, SVC_prediction, average=\"weighted\"),4)\nprint(f'\\nSupport Vector Classifier weighted avg f1 score : {svc_f1}')","6dcf85eb":"# Confusion matrix for the Ridge Classifier\nprint('\\nRidge Classifier\\n', confusion_matrix(y_val, ridge_prediction))","e0fe3d3d":"# The classification report \nprint('\\nRidge Classifier  Classification report :\\n', classification_report(y_val, ridge_prediction))\nplot_confusion_matrix(y_val, ridge_prediction, normalize=True,figsize=(8,8),cmap='Blues')\nplt.title('Ridge Classification')\nplt.show()","6e3b2343":"# # Visual represetation of of the f1 score for each class\nreport_ridge = classification_report(y_val, ridge_prediction, output_dict=True)\ndf_ridge = pd.DataFrame(report_ridge).transpose()\ndf_ridge.drop(['accuracy'], inplace = True)\ndf_ridge.sort_values(by=['f1-score'],ascending=True, inplace = True)\ndf_ridge.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\nplt.xlabel('f1-score')\nplt.ylabel('sentiment class')\nplt.yticks(rotation = 40)\nplt.title('f1-score per sentiment class for Ridge Classifer')\nplt.show()","cc102a2d":"# Checking the accuracy score\nridge_acc = round(accuracy_score(y_val, ridge_prediction),4)\nprint(f'\\nRidge Classifier accuracy Score : {ridge_acc}')\nridge_f1 = round(f1_score(y_val, ridge_prediction, average=\"weighted\"),4)\nprint(f'\\nRidge Classifier weighted avg f1 score : {ridge_f1}')","06f19944":"# Creating a dataframe with our models and their performances metrics\nclassifier_scores = {'Classifiers':['Decision Tree', 'Random Forest','LinearSVC',\n                                    'Logistic Regression','Stochastic Gradient Descent',\n                                    'Support Vector Classifier','Ridge Classifier'],\n                    'Accuracy':[decison_tree_acc,random_forest_acc,\n                                linearSVC_acc,logistic_reg_acc, sgd_acc, svc_f1, ridge_acc],\n                     'Weighted avg f1 Score':[decision_tree_f1,random_forest_f1,\n                                       linearSVC_f1,logistic_reg_f1, sgd_f1, svc_f1, ridge_f1]}\ndf= pd.DataFrame(classifier_scores)\ndf.sort_values(by=['Accuracy'],ascending=True, inplace = True)\ndf","7c790eb0":"df.set_index(df['Classifiers'], inplace = True)\ndf.drop(['Classifiers'],axis = 1)\ndf.plot(kind='barh', figsize = (8,8),colormap='winter')\nplt.xlabel('Score')\nplt.yticks(rotation = 45)\nplt.title('Classifier Perfomance')\nplt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\nplt.show()","b636f198":"# Define the models which we'll include in our ensemble. \n# We pass a list of tuples, which each have a string identifier for the\n# model (arbitrary choice), along the actual instantiated sklearn model.  \nmodels = [(\"LinearSVC\",Lsvc_count),(\"Logistic Regression\",logreg_count),(\"Ridge Classified\",ridge_count)]\n\n# Specify weights for weighted model averaging\nmodel_weightings = np.array([0.1,0.3,0.6])\n\n# building the voting classifier\nVoting_classifier = VotingClassifier(estimators=models,weights=model_weightings)\n","d2b1742a":"# training the voting classifier\nVoting_classifier.fit(X_train, y_train)","86df4aa6":"voting_prediction = Voting_classifier.predict(X_val) # Voting Classifier predictions","b1f88a52":"print('\\nVoting Classifier  Classification report :\\n', classification_report(y_val, voting_prediction))","c97233df":"# Checking the accuracy score\nvoting_acc = round(accuracy_score(y_val, voting_prediction),4)\nprint(f'\\nOverall accuracy for the Voting Classifier : {voting_acc}')\nvoting_f1 = round(f1_score(y_val, voting_prediction, average=\"weighted\"),4)\nprint(f'\\nWeighted avg f1 score for the Voting Classifier : {voting_f1}')","594bd331":"# We have to to prepare our data again since we wont be using the pipelines\n\ntrain = train_data.copy()\ntest = test_data.copy()\n\ntrain['message'] = replace_sentiments(train)\ntest['message'] = replace_sentiments(test)\ntrain['clean_tweets'] = clean_tweets(train)\ntest['clean_tweets'] = clean_tweets(test)\n\nnormalizer = WordNetLemmatizer()\ntrain['clean_tweets'] = train['clean_tweets'].apply(lambda x: [normalizer.lemmatize(token, POS(token)) for token in x])\ntest['clean_tweets'] = test['clean_tweets'].apply(lambda x: [normalizer.lemmatize(token, POS(token)) for token in x])\n\n\n\nX = train['clean_tweets']\ny = train['sentiment']\nX_test = test['clean_tweets']\n\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state = 42)\n\nX_train = list(X_train.apply(' '.join))\nX_val = list(X_val.apply(' '.join))\nX_test = list(X_test.apply(' '.join))","ffbeaaf6":"vectorizer = CountVectorizer(analyzer = 'word', \n                             tokenizer = None, \n                             preprocessor = None, \n                             stop_words = None, \n                             max_features = 180000,\n                             min_df = 1,\n                             ngram_range = (1,2)\n                            )\nvectorizer.fit(X_train)\nX_train = vectorizer.transform(X_train)\nX_val = vectorizer.transform(X_val)\nX_test = vectorizer.transform(X_test)","9db58a54":"# Tunning LinearSVC using parfit\n\n#grid = {'C': np.logspace(-4, 4, 50)}\n#paramGrid = ParameterGrid(grid)\n#bestModel, bestScore, allModels, allScores = pf.bestFit(LinearSVC, paramGrid,\n#           X_train, y_train, X_val, y_val, \n#           metric = accuracy_score,\n#           scoreLabel = \"AUC\")\n#print(bestModel, '\\n',bestScore)","041887f9":"# Tuning LinearSVC using GridSearchCV\n\n# The hyperprarameter gridsearch for the LinearSVC is purposefully commented out because \n# it perfoms 5 folds for each of the 9 candidates totalling in 45 fits, this GridSearch code took  2 hours to complete\n\n#  we have saved the resulting model as a pickle file for convinience\n\n\n'''\nparam_grid = {'C': np.logspace(-4, 4, 50)}\n# grid_lsvc = GridSearchCV(LinearSVC(),param_grid,refit=True,verbose=2)\n# grid_lsvc.fit(X_train_new,y_train)\n\ngrid_lsvc = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)  \n\ngrid_lsvc.fit(X_train_new,y_train)'''\n\n\n# # Saving the model\n# import pickle\n# model_save_path = 'LinearSVC.pkl'\n# with open(model_save_path, 'wb') as file:\n#     pickle.dump(grid_lsvc, file)\n    \n \n#  # loading the saved Logistic Regression model\n# model_load_path = 'LinearSVC.pkl'\n# with open(model_load_path, 'rb') as file:\n#     grid_lsvc=pickle.load(file)\n\n# The GridSearch Best parameter\nbest_param = {'C': 7.9060432109076855}\n\n# retrain the model\ngrid_lsvc = LinearSVC(C= 7.9060432109076855)\ngrid_lsvc.fit(X_train, y_train)","7c9abd12":"tuned_lsvc_predictions = grid_lsvc.predict(X_val)","4c443151":"# Checking the accuracy score\ntuned_lsvc_acc = round(accuracy_score(y_val, tuned_lsvc_predictions),4)\nprint(f'\\nOverall accuracy score for LinearSVC : {tuned_lsvc_acc}')","3ec520a1":"# We now retrain the LinearSVC with the best parameters we got from the tuning method that performed the best\n\nfinal_lsvc = LinearSVC(C=0.3906939937054613, class_weight=None, dual=True,\n          fit_intercept=True, intercept_scaling=1, loss='squared_hinge',\n          max_iter=1000, multi_class='ovr', penalty='l2', random_state=None,\n          tol=0.0001, verbose=0) \nfinal_lsvc.fit(X_train, y_train)\nLinearSVC_prediction = final_lsvc.predict(X_val)\nfinal_svc_acc = accuracy_score(y_val, LinearSVC_prediction)\nprint(f'\\nOverall accuracy score for LinearSVC : {final_svc_acc}')\n\n\n'''\n# Saving the model\nimport pickle\nmodel_save_path = 'linear_svc.pkl'\nwith open(model_save_path, 'wb') as file:\n    pickle.dump(final_lsvc, file)\n    \n  \n # loading the saved Logistic Regression model\nmodel_load_path = 'linear_svc.pkl'\nwith open(model_load_path, 'rb') as file:\n    Logistic_reg_tuned=pickle.load(file)\n \n'''","0dc9d82d":"# Saving the model\nimport pickle\nmodel_save_path = 'final_lsvc.pkl'\nwith open(model_save_path, 'wb') as file:\n  pickle.dump(final_lsvc, file)","a8903650":"Final_lsvc_acc = accuracy_score(y_val, LinearSVC_prediction)\nFinal_lsvc_f1 = f1_score(y_val, LinearSVC_prediction, average = 'weighted')","3915d74d":"# Tunning Logistic Regression using parfit\n\n#grid = [\n#    {'penalty' : [ 'l2'],\n#    'C' : np.logspace(-4, 4, 50),\n#    'solver' : ['lbfgs']},]\n#\n#paramGrid = ParameterGrid(grid)\n#bestModel, bestScore, allModels, allScores = pf.bestFit(LogisticRegression, paramGrid,\n#           X_train, y_train, X_val, y_val, \n#           metric = accuracy_score,\n#           scoreLabel = \"AUC\")\n#print(bestModel, '\\n\\n',bestScore)","e1672b80":"# Tuning Logistic Regression using GridSearchCV\n\n# The hyperprarameter gridsearch for the logistic regression model is purposefully commented out because \n# it perfoms 5 folds for each of the 50 candidates totalling in 250 fits, this GridSearch  code took 15 minutes to complete\n#  we have saved the resulting model as a pickle file\n\n'''\nparam_grid = [\n    {'penalty' : [ 'l2'],\n    'C' : np.logspace(-4, 4, 50),\n    'solver' : ['lbfgs']},]\n\nLogistic_reg_grid = GridSearchCV(LogisticRegression(),param_grid,refit=True,verbose=2)\nLogistic_reg_grid.fit(X_train_new, y_train)\n\n\n# Saving the model\nimport pickle\nmodel_save_path = 'LogisticReg.pkl'\nwith open(model_save_path, 'wb') as file:\n    pickle.dump(logistic_grid, file)\n    \n  \n # loading the saved Logistic Regression model\nmodel_load_path = 'LogisticReg.pkl'\nwith open(model_load_path, 'rb') as file:\n    Logistic_reg_tuned=pickle.load(file)\n    \n\nbest_params = {'C': 3.727593720314938, 'penalty': 'l2', 'solver': 'lbfgs'}\n    \n'''\n\n\n# Retraining the Logistic Regresion model with best parameters                     \ngrid_Log_reg = LogisticRegression(C = 3.727593720314938, penalty = 'l2',solver='lbfgs', random_state=42)\ngrid_Log_reg.fit(X_train, y_train)\n\ntunned_logreg_acc = round(accuracy_score(y_val, grid_Log_reg.predict(X_val)),4)\nprint(f'The accuracy score score for tuned Logistic Regression Classifier : {tunned_logreg_acc}')","83d5e41b":"# Saving the model\nimport pickle\nmodel_save_path = 'Logistic_regression.pkl'\nwith open(model_save_path, 'wb') as file:\n  pickle.dump(grid_Log_reg, file)","d9aa5bb5":"# We now retrain Logistic Regression with the best parameters we got from the tuning method that performed the best\n\nfinal_logreg = LogisticRegression(C = 3.727593720314938, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\nfinal_logreg.fit(X_train, y_train)\nfinal_log_reg_prediction= final_logreg.predict(X_val) \nfinal_logreg_acc = round(accuracy_score(y_val, final_log_reg_prediction),4)\nfinal_logreg_acc","9dc36680":"final_logreg_f1 = round(f1_score(y_val, final_log_reg_prediction, average = 'weighted'),4)\nfinal_logreg_f1","c307b76e":"## Tuning Ridge classifier using parfit\n#grid = {\n#    'alpha':  np.logspace(-4, 4, 50), # learning rate\n#}\n#paramGrid = ParameterGrid(grid)\n#\n#bestModel, bestScore, allModels, allScores = pf.bestFit(RidgeClassifier, paramGrid,\n#           X_train, y_train, X_val, y_val, \n#           metric = accuracy_score,\n#           scoreLabel = \"AUC\")\n#\n#print(bestModel,'\\n\\n' ,bestScore)","ff89b162":"# Tunning Ridge Classifier using GridSearchCV\n\n# The hyperprarameter gridsearch for the logistic regression model is purposefully commented out because \n# it perfoms 5 folds for each of the 50 candidates totalling in 250 fits, this GridSearch  code took 57 minutes to complete\n#  we have saved the resulting model as a pickle file\n\n'''\nparam_grid = {'alpha':np.logspace(-4, 4, 50)}\ngrid = GridSearchCV(RidgeClassifier(),param_grid,refit=True,verbose=2)\ngrid.fit(X_train,y_train)\nprint(accuracy_score(y_val, grid.predict(X_val)))\n'''\nbest_params = {'alpha': 11.513953993264458}\n# retrain the model with best params\nridge_grid = RidgeClassifier(alpha=11.513953993264458)\nridge_grid.fit(X_train, y_train)","bcb415a1":"# accuracy of the ridge gridsearch model\nprint(f1_score(y_val, ridge_grid.predict(X_val), average = 'weighted'))","fb23b902":"# retraining the best Ridge Classifer model we have\nridge_tuned = RidgeClassifier(alpha=5.428675439323859, class_weight=None, copy_X=True,\n                fit_intercept=True, max_iter=None, normalize=False,\n                random_state=None, solver='auto', tol=0.001) \nridge_tuned.fit(X_train, y_train)","bca86105":"# Checking the accuracy score\nridge_prediction_tuned =ridge_tuned.predict(X_val)\n\nridge_acc_2 = round(accuracy_score(y_val, ridge_prediction_tuned),4)\nprint(f'\\nOverall accuracy score for Ridge Classifier accuracy Score : {ridge_acc}')\nridge_f1_2 = round(f1_score(y_val, ridge_prediction_tuned, average=\"weighted\"),4)\nprint(f'\\nWeighted avg f1 score for Ridge Classifier : {ridge_f1}')","0c5e884e":"# Saving the model\nimport pickle\nmodel_save_path = 'Ridgeclfr.pkl'\nwith open(model_save_path, 'wb') as file:\n  pickle.dump(ridge_tuned, file)\n  ","7349a085":"# Creating a dataframe with our models and their performances metrics\nclassifier_scores = {'Classifiers':['Decision Tree', 'Random Forest','LinearSVC'\n                                    ,'Logistic Regression','Stochastic Gradient Descent',\n                                    'Support Vector Classifier', 'Voting Classifer',\n                                    'Tunned_LinearSVC','Tunned LogisticReg','Ridge Classifier', 'Tuned Ridge Classifier'],\n                    'Accuracy':[decison_tree_acc,random_forest_acc,\n                                linearSVC_acc,logistic_reg_acc, sgd_acc, svc_f1, voting_acc\n                                ,Final_lsvc_acc ,final_logreg_acc,\n                                 ridge_acc, ridge_acc_2],\n                     'Weighted avg f1 Score':[decision_tree_f1,random_forest_f1,\n                                       linearSVC_f1,logistic_reg_f1, sgd_f1, svc_f1,\n                                              voting_f1, Final_lsvc_f1 ,final_logreg_f1, \n                                              ridge_f1, ridge_f1_2]}\n\n\n\n\ndf= pd.DataFrame(classifier_scores)\ndf.sort_values(by=['Accuracy'],ascending=True, inplace = True)\ndf\n\n\ndf= pd.DataFrame(classifier_scores)\ndf.sort_values(by=['Accuracy'],ascending=True, inplace = True)\ndf","7d0a9cde":"df.set_index(df['Classifiers'], inplace = True)\ndf.drop(['Classifiers'],axis = 1)\ndf.plot(kind='barh', figsize = (8,8),colormap='winter')\nplt.xlabel('Score')\nplt.yticks(rotation = 45)\nplt.title('Classifier Perfomance')\nplt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\nplt.show()","dc854f32":"print('classfication report for our best model\\n',classification_report(y_val, ridge_tuned.predict(X_val)))","1d33538f":"# # Visual represetation of of the f1 score for each class\nreport_ridge = classification_report(y_val, ridge_prediction_tuned, output_dict=True)\ndf_ridge = pd.DataFrame(report_ridge).transpose()\ndf_ridge.drop(['accuracy'], inplace = True)\ndf_ridge.sort_values(by=['f1-score'],ascending=True, inplace = True)\ndf_ridge.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\nplt.xlabel('f1-score')\nplt.ylabel('Classes')\nplt.yticks(rotation = 40)\nplt.title('f1-score per sentiment class for tunned SVC')\nplt.show()","fed9c937":"OneVsRest =  OneVsRestClassifier(RidgeClassifier(alpha=5.428675439323859, class_weight=None, copy_X=True,\n                fit_intercept=True, max_iter=None, normalize=False,\n                random_state=None, solver='auto', tol=0.001))\ny_train_binarized = label_binarize(y_train, classes=[-1, 0, 1, 2])\ny_val_binarized = label_binarize(y_val, classes=[-1, 0, 1, 2])\nOneVsRest.fit(X_train, y_train_binarized)\ny_prob = OneVsRest.decision_function(X_val)\nplot_roc(y_val, y_prob,figsize=(10,10),cmap='cool')\nplt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\nplt.show()","7fc294f0":"y_pred = ridge_tuned.predict(X_test)","67335c6e":"y_pred","6467dc88":"test_data['sentiment'] = y_pred","4d4c8240":"test_data[['tweetid','sentiment']].to_csv('Final Ridge Submission.csv', index=False)","b1a4888a":"test_data[['tweetid','sentiment']]","71b8058d":"# close the experiment\n# experiment.end()\n# experiment.display()","604661d6":"*  Decision Tree Classifier\n*  RandomForest Classifier\n*  LinearSVC(Support Vector Classifier)\n*  Support Vector Classifier\n*  Logistic Regression\n*  Stochastic Gradient Descent (SGD)\n*  Ridge Classiffier\n","a2ee9b73":"**Key Observations**\n\nA Classification report is used to measure the quality of predictions from a classification algorithm.\n\nThe confusion matrix heatmap shows the model's  ability to classify positive samples, each class achieving a recall score of:\n\n+ Anti Climate Change : 0.46\n+ Neutral : 0.5\n+ Pro : 0.88\n+ News : 0.81\n\nThe major concern here is that the Ridge classification classified 40% of of `neutral` tweets as `Pro` climate change tweets\n","4d79236c":"<a id='Pipelines'><\/a>\n# Pipelines\n![cover.png](https:\/\/www.houseofbots.com\/images\/news\/11939\/cover.png)\n`Pipeline`  by definition is a tool that sequentially applies a list of transforms and a final estimator. Intermediate steps of pipeline implement fit and transform methods and the final estimator only needs to implement fit. In our case pipelines will help us tranform the train, validation and test data as well as train our models.\n\nSince our models can only process numerical data our first step is to build a pipeline that converts text data into numeric data, In this notebook we will be focusing on two methods of feature engineering, which we will use to convert text data to numeric data, namely `TfidfVectorizer` and the `CountVectorizer`, then we will train our models within these pipelines\n\nWe will be building pipelines with features generated using both `tfidfVectorizer` and the `CountVectorizer`","71891a53":"The `parfit` method peformed 50 fits and with the hyperparameter C=C=339.3221771895323, achieved the accuracy score of 0.7914 which is a slight improvement from the accuracy score we had for the Logistic Regression\n\nNow we want to see how the GridSearch method tunes the `Logistic Regression` with the same param grid we used for the `partfit` method","92beb4b9":"The overall accuracy and weighted f1 score for the Random Forest Classifier is better compared to that of the Decision Tree classifier, making the Random Forest classifier the best model this far","2c8f1141":"**Key observations**\n\nThe above bar graph shows the f1 score for each sentiment class using \nStochastic Gradient Descent classifier\n- The SGD classifier is just as good at classifying `Pro` sentiment classs as the LinearSVC both achieving an f1 score of 0.84 however falls short in classifying the rest of the sentiment classes","b84652fb":"**Separate Datframes of Tweets for each Sentiment**","11e442cc":"<a id='Target'><\/a>\n### Explore the target variable\n\nWe want to understand the behaviour of our target variable for our respective categories...let's dig in \u26cf!","a03b717e":"Model comparison by accuracy and macro f1_score","b0f46d7c":"**Key Observations**\n\nA Classification report is used to measure the quality of predictions from a classification algorithm.\n\nThe confusion matrix heatmap shows the model's  ability to classify positive samples, each class achieving a recall score of:\n\n+ Anti Climate Change : 0.52\n+ Neutral : 0.52\n+ Pro : 0.87\n+ News : 0.82\n\nWe see that that the LinearSVC did a very job at classifying positive samples, the biggest concern here is that 32% and 39% of `anti` and `neutral` sentiment classes respectively were incorrectly classified as `Pro` sentiment class","dfb9604c":"The LinearSVC  is the best we've seen this far achieving an accuracy score of 0.7876  and a weighted F1 score of 0.7817","48d1d36e":"**Key Observations**\nThe above bar graph shows the f1 score for each sentiment class using the Decision Tree classifier\n- We see that the decision tree model did a very good job at classifiying `Pro` sentiment class, followed by `News` and `Neutral` respectively.\n- The Decision Tree classifier did a poor job at classifiying `Anti climate Change` tweets with an f1 score that is below 0.4.\n- Poor classification of `Anti climate change` tweets is expected given the imbalance in our train data where we see that `Anti climate change` tweets only account for 8% of all tweets in the train dataset.","b84a936e":"Identifying Missing Values and Check for Data Types","74f534e0":"**Key Observations**\n\nA Classification report is used to measure the quality of predictions from a classification algorithm.\n\nThe confusion matrix heatmap shows the model's  ability to classify positive samples, each class achieving a recall score of:\n\n+ Anti Climate Change : 0.52\n+ Neutral : 0.5\n+ Pro : 0.88\n+ News : 0.82\n\nJust like the LinearSVC we see that Logistic Regression does a very good job at classifying positive classes\n","df4b3047":"**Key Observations**\n\nA Classification report is used to measure the quality of predictions from a classification algorithm.\n\nThe confusion matrix heatmap shows the model's  ability to classify positive samples, each class achieving a recall score of:\n\n+ Anti Climate Change : 0.54\n+ Neutral : 0.53\n+ Pro : 0.85\n+ News : 0.84\n\nSGD classifier scored the highest in classification of positive classes for `anti` and `neutral` sentiment classes despite incorretly classsifying `anti` and `neutral` sentiment classes as `Pro` sentiment class 35% and 42% of the time respectively\n","a720feb4":"**Key observations**\n\nThe above bar graph shows the f1 score for each sentiment class using the LinearSVC\n- We see that the LinearSVC model did a far better job at classifiying `Pro` and `News` sentiment classes compared to `Decision Tree` and `RandomForest` models  with both classes achieving an f1 score of 0.85 and 0.81 respectively\n- The LinearSVC model also did a far better job at classifying `Anti` sentiment class comapred to both the Decision tree and the Randrom Forest.\n- There was a slight improvement in the classification of `neutral` tweets with the LinearSVC, which is by far overshadowed by the improvements we see in other sentiments classes\n- The LinearSVC has done a better job overall in classifying the sentiments, we see that `Anti` and `Neutral` sentiments have almost the same score, same applies with `Pro` and `News` sentiments which is consistent with the distribution of the data between the sentiment classes","6b816ac9":"\n|Overall |News|Pro|Neutral|Anti|\n|:-----|:----:|----:|----:|----:|\n|#climate |#climate |#climate |#climate |#maga |\n|#climatechange |#climatechange |#beforetheflood |#climatechange |#climate |\n|#beforetheflood |#environment |#climatechange |#trump |#trump |\n|#environment |#news |#imvotingbecause |#beforetheflood |#climatechange |\n|#trump |#trump |#cop22 |#amreading |#globalwarming |\n","51f1d5e4":"The AUC plot shows the accuracy score for each value fo the C parameter that the model is fitted with","36f929c7":"### Ridge Classifier","c80f9e21":"## Logistic Regression","22af5533":"The resulting model from the Gridsearch achieved an accuracy score of 0.766 which is just under 0.02 lower compared to the accuracy of the model we got using the parfit method","7fd4690a":"**Extract Useful Data**","0dea0a28":"###### Observation \n- Positively skewed sentiments, the sample mean is greater than median (orange dashed-line).\n- Majority of the tweets have a neutral sentiment on the topic.\n- Cleaned data has a smaller mean than raw data; however, the central tendency show a similar trend of being positively skewed indicating that we did not dilute our data while cleaning it. Therefore, cleaned data can be representative of the raw data.","0b6f80eb":"<a id='build'><\/a>\n## Building classification  pipelines\n","a4f5de4b":"## Stochastic Gradient Descent","8e7206d9":"<a id='conclusion'><\/a>\n# Conclusion","e97b86e7":"### Evaluation of SGD Classifier","b11f740a":"**Key observations**","a60add67":"## Tuning LinearSVC","cdf62bf5":"**Remove Unnecessary Information on Message**","8d383508":"**Compare the sentiments analysis of raw and cleaned tweets**","52bac3fa":"<a id='Sentiment'><\/a>\n## Investigate the feature variable\n\nAt this stage we want to understand the message and its contents much better, to draw fruitful insghts about our target variable. \n","65bcb13c":"**Key Observations**\nThe above bar graph shows the f1 score for each sentiment class using the Random Forest classifier\n- We see that the Random Forest model did a better job at classifiying `Pro` sentiment class  and  `News`  comapred to the Decision tree model, with both the `Pro` and `News` sentimetents with an f1 score of  0.80 and 76 respectively.  \n- However even though the Random Forest classifier did a better job with `Pro` and `News` sentiment classes, the classifer perfomred poorly on `Neutral` and `Anti` sentiments with both sentiments having an f1 score below 0.4","46e022cb":"The caveat of using pipelines to build our models is that we can't easily get the parameters for our models as such to perfom hyperparameter tunning and obtain the best parameters for our models we wont be using the pipelines, this means we will convert raw text data to numeric data independently from building the models.","f70b216b":"**Key Observations**\n\nA Classification report is used to measure the quality of predictions from a classification algorithm.\n\nThe confusion matrix heatmap shows the model's  ability to classify positive samples, each class achieving a recall score of:\n\n+ Anti Climate Change : 0.33\n+ Neutral : 0.37\n+ Pro : 0.77\n+ News : 0.71\nWe see that  most of the tweets are incorrectly classified as `pro` sentiment classes, with 44% and 48% of `anti` and `neutral` sentiment classes respectively incorrectly classified to belong to `Pro` sentiment class","2dcaa34b":"**Get the length of each raw tweet**","84a64a0f":"**Key Observations**\n\nA Classification report is used to measure the quality of predictions from a classification algorithm.\n\nThe confusion matrix heatmap shows the model's  ability to classify positive samples, each class achieving a recall score of:\n\n+ Anti Climate Change : 0.29\n+ Neutral : 0.4\n+ Pro : 0.85\n+ News : 0.85\n\nThe Support Vector Classifier incorrectly classfied over 50% of `neutral` tweets as `Pro` climate change tweets and 49% of `anti` sentiment class tweets as `Pro` sentiment class tweets","bc7bf35d":"# Closing the comet experiment","31267fcc":"### Evalution of DecisionTreeClassifier","09dbcb61":"### Support Vector Classfifier","a97b737d":"To evaluate the base models we first start with making predictions for the validation set","49ab8f4d":"<a id='Hashtags'><\/a>\n### Hashtags for sentiment classes\n`Hashtags` are the best way used on twitter to link the conversions of similar content. Hashtags encourage social media users to explore content that catches their eye. Companies can use hashtags to reach their target audience and to help members filter information. It is for this reason that we will then collect most sed hashtags for each sentiment class. ","d8d50646":"**Key observations**\n\nThe above bar graph shows the f1 score for each sentiment class using the Support Vector Classifier(SVC)\n- Much like the `LinearSVC` we see that the  the `SVC` does a really good job at classifying `Pro` sentiment class with a score of 0.81, followed by the `News` sentiment class with an f1 score of over 0.77.\n- Unlike most of the models we've build this far, the Support Vector Classifier struggle more with classifying the `Anti`sentiment class","4f356a50":"<a id='model_eval'><\/a>\n# Modelling and Evaluation","b46bc8a8":"<a id='ensemble'><\/a>\n# Ensemble Methods\n\nEnsemble learning in machine learning is the practice of combining multiple models to try and achieve higher overall model performance. In general, ensembles consist of multiple **heterogeneous or homogeneous** models trained on the same dataset. Each of these models is used to make predictions on the same input, then these predictions are aggregated across all models in some way (e.g. by taking the mean) to produce the final output.","f68f0c06":"Climate change describes a change in the average conditions such as temperature and rainfall in a region over a long period.\nGlobal climate change refers to the average long term changes over the entire earth, These include warming temperatures and changes in the precipitation as well as the effects of earth's warming such as\n\n * Rissing sea levels\n * Shrinking mountain glaciers \n * Ice melting at a faster rate than usual in Greenland and Artic \n * Wildfires\n * Floods\n * Droughts\n \nThere are lots of factors that contribute to Earth\u2019s climate. Scientists agree that Earth has been getting warmer in the past 50 to 100 years due to human activities, This is a statement that sparks debate among people, as we will see when exploring the tweets, There are two stands that people take on the man made climate change issue\n *  There are people that believe that humans contribute climate change, that is the `Pro` sentiment class\n *  There are people that dont believe humans contribute to climate change, that is the `Anti` sentiment class,  This may be because they either dont believe there is such as thing as climate change or they believe there is climate change but is humans don't contibute to climate change\n\n \nEarth\u2019s climate has constantly been changing, even long before humans came into the picture. However, scientists have observed unusual changes recently. For example, Earth\u2019s average temperature has been increasing much more quickly than they would expect over the past 150 years.\nAccording the scientists the main human activities that contribute to climate change over the last century is the burning of fossil fuels like coal and oil as well as carbon pollution , which has increased the concentration of atmospheric carbon dioxide(CO2).\n\n<!-- The diagram below shows average global temperature and carbon emission between 1880 and 2019 \n![2020CO2Peak_Temps_en_title_lg.jpg](https:\/\/ccimgs-2020.s3.amazonaws.com\/2020CO2Peak\/2020CO2Peak_Temps_en_title_lg.jpg)\n -->\n \nMany companies are built around lessening one\u2019s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product\/service may be received.\n\nStatistics show that 90% of the world's data has been generated over the past 2 years, this explosion of information that is known as `big data` most of this data is text data generated through socia media posts and comments. \n\nTwitter is one of the most popular social media sites and has become a huge data source of text data, generating over 12 Terabytes of data per day, The explosion in text data came with a growing demand for orginizations to analyse and gather insights from text data, which is known as text analytics.\n\n`Text analytics` is the automated process of translating large volumes of unstructured text into quantitative data to uncover insights, trends, and patterns. combined with data visualization tools, this technique enables companies to understand the story behind the numbers and make better decisions. In this notebook we will look into a concept called sentiment analysis using tweets.\n `Sentiment analysis` (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information\n \n![twitter-bird-animated-logo.gif?w=300&zoom=2](https:\/\/dropnerblog.files.wordpress.com\/2019\/12\/twitter-bird-animated-logo.gif?w=300&zoom=2)","3635b220":"### Evalution of RandomForestClassifier","5cf2fb02":"<a id='final'><\/a>\n# Final model selection","54ae9bff":"The overall accuracy score of the SGD classifier is 0.7826  and the wighted f1 score of 0.7763","e0c7f3e5":"The GridSearch method achieved the best accuracy score of 0.7857 which is slightly less comapared to the accuracy score from the parfit method","4a4418f1":"**Key Observations**","c4682fcf":"<a id='Top'><\/a>\n# CLASSIFICATION CHALLENGE\n","25247c53":"## Support Vector Classification(LinearSVC)\nSVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes as seen in the diagram below\n\n![1*dh0lzq0QNCOyRlX1Ot4Vow.jpeg](https:\/\/miro.medium.com\/max\/963\/1*dh0lzq0QNCOyRlX1Ot4Vow.jpeg)\n\nTo better explain the concept of `SVM` we will look at a case of two classes.\n\n**To find the best line seperating the classes**\n\nThe `SVM` algorithm finds the points closest to the line from both the classes.These points are called support vectors, then it compute the distance between the line and the support vectors, This distance is called the margin. Our goal is to maximize the margin.\n\nIn a case for more than two classes the goal is to find the the best hyperplane that seperates the classes.\nThe hyperplane for which the margin is maximum is the optimal hyperplane.\n\nBelow is a visual representation of how `SVMs` work\n\n![1*06GSco3ItM3gwW2scY6Tmg.png](https:\/\/miro.medium.com\/max\/963\/1*06GSco3ItM3gwW2scY6Tmg.png)\n\nWe wil be looking at two Support Vector Classifer models namely SVC and LinearSVC, the main differences between these two are as follows\n- By default scaling, LinearSVC minimizes the squared hinge loss while SVC minimizes the regular hinge loss.\n- LinearSVC uses the One-vs-All (also known as One-vs-Rest) multiclass reduction while SVCuses the One-vs-One multiclass reduction.","1a45410c":"### Ridge Classifier","3a0e1c75":"<a id='tunning'><\/a>\n# Hyperparameter Tunning\nWe will look at two methods of hyperparameter tunning, namely `GridSearchCV` and `Parfit`\n\n* Models we will perform hyperparameter tunning on\n  * LinearSVC\n  * Logistic Regression\n  * Ridge Classifier\n ","9b19fc95":"The AUC plot shows the accuracy score for each value fo the C parameter that the model is fitted with","ed70f9c5":"**Continue cleaning \ud83e\uddfc\ud83e\uddfd....**","adb71d37":"<a id='EDA'><\/a>\n# Exploratory Data Analysis \n\nIn this section we want to apply a variety of techniques to maximize specific insights into a dataset, reveal underlying structure, extract significant variables, detect outliers and anomalies, test assumptions, develop models, and determine best parameters for estimations. In essence, we want to dig deeper into our dataset to more insights about its behaviour!","edfc591e":"In this notebook we have succesfully build 10 machine learning models to classify whether not a person believes in man made climate change based on their novel tweet data, Even though our models struggled with classifying the `anti` man made climate change sentiment class they did a very good job as classifying the `pro` man made climate change sentiment class. Our best model is the Ridge Classification model  achieving an accuracy score and the weighted f1 score of 0.7845 and 0.7743 respectively based on the validation dataset.\n\nThe Ridge classifier model achieved an F1 score of 0.76043 on unseen\/test data.\n\nText classification problems tend to be quite high dimensionality and high dimensionality problems are likely to be linearly separable, So linear classifiers, whether ridge classification or SVM with a linear kernel, are likely to do well. In both cases, the ridge parameter or C for the SVM control the complexity of the classifier and help to avoid over-fitting by separating the patterns of each class by large margins as we can see in our final model comparision, our best models are linear classifiers","db9b501d":"<a id='models'><\/a>\n# Classification Models\nWe're going to look at the following models:","2df2b889":"**Expand Contractions**","dbfd6bc6":"The above bar graph shows the f1 score for each sentiment class our best model\n- The Ridge Classifier is our best performing model, achieving f1 score of 0.85 for `Pro climate change` sentiment class, followed by `News` and `Anti` Climate sentiment classes with f1 scores of 0.79 and 0.60 respectively, which is quite impressive given that all our models perfomed poorly when it comes to classifying `anti climate change` sentiment class\n","377818ea":"We see that on average  the models build using the `CountVectorizer` performed the best and for the remainder of this notebook we will generate our features using the `CountVectorizer`","12ef21ab":"<a id='Frequency'><\/a>\n### Word Frequency in each class\nCounting the frequency of specific words in the list can provide illustrative data to understand the buzz words used for respective sentiment classes","d919ae33":"Overal the Logistic Regression classifier achieved the highest accuracy of 0.7838 and the highest wighted f1 score of 0.7764 which makes it the best model we've tried this far","0efdbc0d":"<a id='train'><\/a>\n## Training models\nEach model is trained using it's custom pipeline which will take raw text data turn it into numeric data and initial the classifier with default parameters","0b2891fe":"<a id='DC'><\/a>\n### Decision Tree Classifier\n\n![1*bcLAJfWN2GpVQNTVOCrrvw.png](https:\/\/miro.medium.com\/max\/688\/1*bcLAJfWN2GpVQNTVOCrrvw.png)\nDecision Trees (DTs) are  non-parametric supervised learning methods used for classification and regression. Decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.\n\nDecision tree builds classification or regression models in the form of a tree structure. It breaks down data by partitioning it into subsets after each decision while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches. Leaf node represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.\n\nVisual representation of a `Decision Tree`\n![1*WerHJ14JQAd3j8ASaVjAhw.jpeg](https:\/\/miro.medium.com\/max\/963\/1*WerHJ14JQAd3j8ASaVjAhw.jpeg)\n\n\nDecision trees are prone to overfitting. Overfitting happens when the learning algorithm continues to develop hypotheses that reduce training set error at the cost of an increased test set error; One method to tackle overfitting in decision trees is by **prunning**.\nThere are several approaches used to avoid overfitting in building decision trees namely, \t\t\n- Pre-pruning that stops growing the tree earlier, before it perfectly classifies the training set.\n- Post-pruning that allows the tree to perfectly classify the training set, and then post prune the tree. \nPractically, the second approach of post-pruning overfit trees is more successful because it is not easy to precisely estimate when to stop growing the tree.\n\nDecision Trees are building blocks for the next machine learning method we will look into, which is the **Random Forest Classifier**","694c59fe":"#### Quick observations:\n* There is a strong imbalance for our sentiment classes\n* Sentiment class '1' (Pro) dominates the chart with over 50% contribution, while class '-1' (Anti) lags behind with 8%.\n* The text length is dependent on the character limit of each tweet on Twitter. Character limit used to be 140, but it increased to 280 characters in late 2017.\n* There are evident outliers in all classes, except for 'neutral' sentiment where all the data is taken in.\n* It is evident that the 'pro' class had a lot to say to express their opinion, as shown by more lenghty message in the outliers. ","f7842d18":"# Outline\n\n- [Introduction](#Introduction)\n- [Import necessary libraries](#Import)\n- [Loading Datasets](#Load)\n- [Data Preprocessing](#section-four)\n- [Exploratory Data Analysis](#EDA)\n   * [Target variable analysis](#Target)\n   * [Hashtags analysis](#Hashtags)\n   * [Sentiment Analysis](#Sentiment)\n   * [Word Frequency per class](#Frequency)\n   * [Mentions for sentiment class](#Mentions)\n  \n- [Classification Models](#models)  \n- [Feature Egineering](#feature_engineering)\n  * [TF-IDF](#tfidf)\n  * [CountVectorizer](#count) \n- [Pipelines](#Pipelines)\n  * [Building Classification Pipelines](#classifiers)\n  * [Building Models](#build)\n  * [Training Models](#training)\n- [Modelling and Evaluation](#model_eval)\n- [Model Comparison](#compare)\n- [Ensemble Method](#ensemble)\n- [Hyperparameter tunning](#tunning)\n- [Final model selection](#final)\n- [Predictions](#predictions)\n- [Submission](#submission)  \n- [Conclusion](#conclusion)\n- [Appendix](#appendix)\n- [References](#references)\n    \n    \n","2ab01824":"## ROC Curves and AUC","d62b84c8":"<a id='ensemble'><\/a>\n## Heterogeneous Ensembel Method\nThis type of ensemble consists of different types of models, so it can add pretty much any classification model we want, however in our case we're only going to add our top 3 best perfoming models which are, `LinearSVC, Stochastic Gradient Descent, Logistic Regression, `.\n\nThe Heterogeneous ensemble method we're going to look at is the `Voting classifier`\n","6c841ff8":"\n**Key observations**\n\nThe above bar graph shows the f1 score for each sentiment class using the Classification\n- Much like the `LinearSVC` we see that the  the `Ridge classifier` does a really good job at classifying `Pro` sentiment class with a score of 0.85, followed by the `News` sentiment class with an f1 score of over 0.79.\n- Just like the support Vector Classifier, we see that Ridge Classifier does very good job at classifying the `anti` and `neutral` sentiment class","36ee2c21":"<a id='references'><\/a>\n# References","f8e71610":"* Overall, #climatechange and #climate are at the top of the charts as expected, they are the colection words used to identify tweets that are of climate change content. \n* #BeforeTheFlood was trending in the year 2016 following the documentray by Actor Leonardo DiCaprio with scientists, activists and world leaders to discuss the dangers of climate change and possible solutions.\n* In the same year; 2016, the outgoing president of U.S.A was canvassing for presidency and he had made his stand clear on Climate Change clear to the public describing it as a \"hoax\". That accounts for his name appearing across all sentiment classes.\n* In his campaign, he used the slogan #MAGA which stands for \"Make America Great Again\". This appears to have attracted more tweets for tweets in the 'anti' class, making it to the top spot.\n* It is for this reason that #iamvotingbecause was at the top for 'pro' class as it was election year in the United States of America.\n* We can notice that #cop22 also made it to top 5 in the 'pro' class. COP22 (Conference of the Parties) represents the United Nations Climate Change Conference in 2016.","c74c1133":"## Tuning Logistic Regression","3a9e88ee":"# Creating our X and y Metrics","2e18d44c":"The Support Vector Classifer achieves an accuracy score of 0.74 and the f1 score of 0.72 ","146d8c11":"**Key Observations**\n\nA Classification report is used to measure the quality of predictions from a classification algorithm.\n\nThe confusion matrix heatmap shows the model's  ability to classify positive samples, each class achieving a recall score of:\n\n+ Anti Climate Change : 0.2\n+ Neutral : 0.28\n+ Pro : 0.92\n+ News : 0.72\n\nRandom Forest classifier did a bad job at correctly classifying sentiment class `anti` and `neutral` incorrectly classifying them as `pro` sentiment class 66% and 68% of the time respectively.\n\n","c63df453":"Most Frequent Words","0b11ce89":"<a id=\"comet\"><\/a>\n# Starting a Comet experiment\n<img src=\"https:\/\/www.comet.ml\/images\/logo_comet_light.png\" width=\"350\" alt=\"Drawing\" style=\"width: 350px;\"\/>\nWe will be using Comet as a form of version control throughout the development of our model","e3cad3c9":"**Stochastic Gradient Descent (SGD)** is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning.\n\nin Stochastic Gradient Descent, a few samples are selected randomly instead of the whole data set for each iteration.\n\n![GD-v-SGD.png](http:\/\/pythonmachinelearning.pro\/wp-content\/uploads\/2017\/09\/GD-v-SGD.png)\n\nThe advantages of Stochastic Gradient Descent are:\n* Efficiency.\n* Ease of implementation (lots of opportunities for code tuning).\n\n\nThe disadvantages of Stochastic Gradient Descent include:\n* SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n* SGD is sensitive to feature scaling.","1bd9e645":"### Evaluation of LinearSVClassifier","7421dff2":"Overal the Decision Tree classifier did a poor job at classifying the sentiments, achieving the accuracy score of 0.6669 and a weighted F1 score of 0.6593","f62d694c":"<a id='tree'><\/a>\n## Tree-based Models for Classification","240fbf16":"## Final evaluation of our best model","988feb2d":"ROC curves show the trade-off between sensitivity\/recall known as the ability of a model to detect positive samples  and specificity also known as the `True Negative` rate.\nclassifiers that produce curves that are closer to the top left corner, that is closer to the 100% True positive rate are classifiers that are considerd to have better perfomance","0c854a56":"## Tuning Ridge Classifier","f0f731f0":"# Data Preprocessing","6237cf3e":"From the above bar graph we see comparison of all the 7 models we've attempted thus far based on their `accuracy score` and associated `wighted f1 score`\n\n- We see that our top 3 best performing models are `LinearSVC`,`Logistic Regression` and `Ridge Classification` respectively, theres are the models will use in ensemble methods to try and improve our results\n- The `Decision Tree` classifer is the worst  at classifying the tweets with the lowest accuracy and wighted f1 scores of 0.68 and 0.61 respectivey\n\n**LinearSVC is the best performing model out of all 7 models that we've tried thus far with an accuracy score of 0.7876 and a weighted f1 score of 0.7871**","221e7a52":"<a id='count'><\/a>\n## CountVectorizer\n\nThe `CountVectorizer` provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary by creating a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix.\n","5bcf5d00":"\n## Infiltrate text analysis\nFrom here, we want to dig deeper into our text and investigate each word individually and its frequency of use. \n\n![alt text](https:\/\/cdn2.iconfinder.com\/data\/icons\/business-management-1-6\/66\/14-512.png)","b7207910":"<a id=\"top\"><\/a>\n# <font color='#2874A6'>Team <font color='#3498DB'>3 <font color = '#5DADE2'>July <font color='#1B4F72'>Classification Predict\n* [Notebook repo](https:\/\/github.com\/JosiasSekhebesa\/classification-predict-streamlit-template\/blob\/master\/Team3_ClassificationPredict_Updated.ipynb)\n* [Streamlit repo](https:\/\/github.com\/CharlesMaponya\/classification-predict-streamlit-template)\n* [Trello board](https:\/\/trello.com\/b\/Jxi6rVUR\/team3classification) ","7e67872e":"Comparing all the models we've build so far to choose the best performing one","0367dba9":"**Remove Stopwords**","2b107230":"**Key observations**\nThe above bar graph shows the f1 score for each sentiment class using Logistic Regression\n- The Random Forest Classifier performed almost as good as the LinearSVC at classifying each sentiment class with `Pro` and `News` sentiment class achieving f1 scores of 84 and 81 respetively","b99feff0":"<a id='predictions'><\/a>\n# Final prediction","0bf0d947":"Companies are constructed around lessening ones environment impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. ","44348205":"In order to decide which feature extraction method we will use between the `TF-IDF` and the `CountVectorizer` for the remainder of the notebook, we will build pipelines using both methods and check which approach ahs the highest accuracy on average","6d3aa172":"### Sentiment Analysis\n\nSentiment analysis is a technique that detects the underlying sentiment in a piece of text. It is the process of classifying text as either positive, negative, or neutral. The values range from 1 to -1; with positivity decreasing from 1 to -1, having a neutral sentiment at 0.\n\n![Alt_text](https:\/\/miro.medium.com\/max\/687\/1*FRd4BsrZ2VxKLbvVYJQC6w.png)\n","7906aabb":"<a id='tfidf'><\/a>\n##  TFIDF\n\n`TF-IDF` stands for Term Frequency \u2014 Inverse Document Frequency and is a statistic that aims to better define how important a word is for a document, while also taking into account the relation to other documents from the same corpus.\nThis is performed by looking at how many times a word appears into a document while also paying attention to how many times the same word appears in other documents in the corpus.\n`vocabulary_` Is a dictionary that converts each word in the text to feature index in the matrix, each unique token gets a feature index.\n ","1eb5f41c":"### Voting classifer \nVoting involves combining individual model outputs through a kind of \"[majority rule](https:\/\/en.wikipedia.org\/wiki\/Majority_rule)\" paradigm.\nThe diagram below shows how the `Voting Classifier` works\n![ud382N9.png](https:\/\/iq.opengenus.org\/content\/images\/2020\/01\/ud382N9.png)","e4392949":"# Introduction","56122819":"The `parfit` method peformed 50 fits and with the hyperparameter C = 0.3906939937054613, achieved the accuracy score of 0.79 which is an improvement of 0.0025\n\nNow we want to see how the GridSearch method tunes the `LinearSVC` with the same param grid we used for the `partfit` method","d12d5497":"This Ridge classifier first converts the target values into {-1, 1} and then treats the problem as a regression task (multi-output regression in the multiclass case) \n\nRidge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When\nmulticollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from\nthe true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.\nIt is hoped that the net effect will be to give estimates that are more reliable. Another biased regression technique,\nprincipal components regression, is also available in NCSS. Ridge regression is the more popular of the two\nmethods.","bbdd7ebc":"[Back to top \u2191](#Top)","f0048bdd":"### Evaluation of  Logistic Regression ","b17e9986":"**Map the sentiment class by name**","ff017bfc":"**Remove collection words**\nCollection words are the words that you used to query your data from Twitter. In our case 'climatechange', 'climate', 'change' are the query words.","daca4b8d":"<a id='feature_engineering'><\/a>\n# Feature Engineering\nWe will be looking at two methods of generating features namely, `tfidfVectorizer` and `CountVectorizer`\n","d7e66e2f":"The resulting best accuracy score for the GridSearch method is 0.78 which is significatly low compared to the one we got using the parfit method","1930e869":"Create a Natural Language Processing model to classify whether or not a person believes in climate change, based on their novel tweet data. ","6e9e5e19":"<a id='model_eval'><\/a>\n## Model evalution","65212266":"**Problen Statement**","5e4205a5":"##### Quick Insights:\n`Top 5 Hashtags Summary:`","0c73e684":"The Voting Classifier achieved the accuracy and weighted f1 score of 0.7819 and 0.7736 respectively, which is not much of an improvement from our best performing model that achieved the accuracy of 0.7876 and 0.7817","2f2c5b73":"<a id='compare'><\/a>\n## Model Comparision","3f29c066":"# Splitting data\n\nSeparating data into training and validation sets is an important part of evaluating our models. \nIn our case we will randomly split the train data into 90% train and 10% validation. \nAfter our model is trained with the train data we then use it to make predictions for the target using the validation set,Because the data in the validation set already contains known values for the target variable this will make it easy  for us to asses our model's accuracy.","3862ca84":"<a id='Load'><\/a>\n# Load and view the data","09fdad54":"We have build a total of 11 models in this notebook out of all the models we've build, We see that the best performing model is the tunned LinearSVC with the best accuracy score of 0.79 and the best weighted f1 score  of 78 based on the validation dataset, however for the unseen\/test dataset the Ridge Classifier achieved the best score on Kaggle.\n\nWe will be using the **Ridge Regression** to make the final prediction.","c445cadd":"### Performance Metrics for model evaluation\n\nWe will evaluate our models using the the F1 Score which is the number of true instances for each label.\n\n#### Precision\n\nPrecision is the ratio of correctly predicted positive observations to the total predicted positive observations\n\n$$ Precision = \\frac{TP}{TP \\space + FP} = \\frac{TP}{Total \\space Predicted \\space Positive} $$\n\n#### Recall\n\nThe recall is intuitively the ability of the classifier to find all the positive samples\n\n$$ Recall = \\frac{TP}{TP \\space + FN} = \\frac{TP}{Total \\space Actual \\space Positive}$$\n\n#### F1 Score\n\nWeighted average of precision and recall. \n\n$$F_1 = 2 \\times \\frac {Precision \\space \\times \\space Recall }{Precision \\space + \\space Recall }$$","31807f9c":"<a id='random'><\/a>\n### Random Forest Classifier\n`Random forest` is a supervised learning algorithm that can be used both for classification and regression. A forest is comprised of a number of individual trees. It is said that the more trees it has, the more robust a forest is, unlike decision trees `Random Forest`  prevents overfitting by creating trees on random subsets  \n\nThe Random forest algorithm works in four steps\n\n 1. Selects a number of random samples from a given dataset\n 2. Construct a decision tree for each sample and get a prediction result from each decision tree\n 3. Perform a vote for each predicted result.\n 4. Select the prediction result with the most votes as the final prediction.\n\na visual representation of a Random Forest classifier is seen in the diagram below\n![voting_dnjweq.jpg](https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1526467744\/voting_dnjweq.jpg)\n\n\n\n","fb50f970":"What is climate change : https:\/\/climatekids.nasa.gov\/climate-change-meaning\/\n\nTF-IDF Explained And Python Sklearn Implementation :\nhttps:\/\/towardsdatascience.com\/tf-idf-explained-and-python-sklearn-implementation-b020c5e83275\n\nDecision Trees Explained Easily :\nhttps:\/\/medium.com\/@chiragsehra42\/decision-trees-explained-easily-28f23241248\n\nUnderstanding Random Forests Classifiers in Python :\nhttps:\/\/www.datacamp.com\/community\/tutorials\/random-forests-classifier-python\n\nWhat is LightGBM, How to implement it? How to fine tune the parameters?\nhttps:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n\nUsing word2vec embeddings as featrues :\nhttps:\/\/www.kaggle.com\/vladislavkisin\/word2vec-in-supervised-nlp-tasks-shortcut\n\nA hands-on intuitive approach to Deep Learning Methods for Text Data \u2014 Word2Vec, GloVe and FastText\nhttps:\/\/towardsdatascience.com\/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa\n\nHow to make SGD Classifier perform as well as Logistic Regression\nhttps:\/\/towardsdatascience.com\/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4\n\nBig data and what it means: https:\/\/www.uschamberfoundation.org\/bhq\/big-data-and-what-it-means\n\nCO2 and the climate curve : https:\/\/medialibrary.climatecentral.org\/resources\/co2-and-the-climate-curve\n\nEffects of Climate change : https:\/\/climate.nasa.gov\/effects\/\n\nPast a point of no return': Reducing greenhouse gas emissions to zero still won't stop global warming, study says: https:\/\/www.usatoday.com\/story\/news\/nation\/2020\/11\/12\/reducing-greenhouse-gas-emissions-stop-climate-change-study\/3761882001\/\n\nText classification with Ridge Classifier : https:\/\/stats.stackexchange.com\/questions\/17711\/why-does-ridge-regression-classifier-work-quite-well-for-text-classification","2f45e113":"**Logistic regression** is a statistical method for predicting binary classes. The outcome or target variable is dichotomous in nature. Dichotomous means there are only two possible classes. For example, it can be used for cancer detection problems. It computes the probability of an event occurrence.\n\nLogistic Regression uses the probability of a data point to belonging to a certain class to classify each datapoint to it's best estimated class\n\nLogistic regression has been rated as the best performing model for linearly separable data especially if it's predicting binary data(Yes & NO or 1 & 0), and performs better when there's no class imbalance.\n\nThe figure below is the sigmoid function logistic regression models use to make predictions:\n![1*QY3CSyA4BzAU6sEPFwp9ZQ.png](http:\/\/miro.medium.com\/max\/725\/1*QY3CSyA4BzAU6sEPFwp9ZQ.png)\n\n\n\nAdvantages\n* Convenient probability scores for observations (probability of each outcome is transformed into a classification);\n* Not a major issue if there is collinearity among features (much worse with linear regression).\n\nDisadvantages\n* Can overfit when data is unbalanced (i.e.: we have far more observations in one class than the other).\n* Doesn't handle large number of categorical variables well.","db838e2a":"<a href=\"https:\/\/colab.research.google.com\/github\/JosiasSekhebesa\/classification-predict-streamlit-template\/blob\/master\/Team3_ClassificationPredict.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","6d331ad7":"The voting  classifer did a good job at classifying the sentiment classes which 'Neutral' sentiment class being the poorly classified one\n- achieving the f1 score of\n- `Pro` sentiment class : 0.85\n- `News` sentiment class : 0.79\n- `Anti` sentiment class: 0.57\n- `Neutral` sentiment class : 0.57","4209b3b5":"<a id='Import'><\/a>\n# Import necessary libraries"}}