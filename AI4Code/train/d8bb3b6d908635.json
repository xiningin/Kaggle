{"cell_type":{"95aa45d2":"code","ab9a9576":"code","7052144f":"code","c3d667f3":"code","65a8e7e2":"code","2a62179f":"code","196537e6":"code","84dda546":"code","98a86738":"code","b6fae9b1":"code","569161d8":"code","6fd2fb6d":"code","32a60d02":"code","5b7f26a6":"code","c111529c":"code","6b96673c":"code","41ff77d2":"code","5512eb92":"code","2e8e79b3":"code","e472b38f":"code","eec3c344":"code","4aa85b1a":"code","4b336fc9":"code","75c2091c":"code","308bb35a":"code","cc178135":"code","b04d5e38":"code","ad2dd00e":"code","1443baf4":"code","e9234045":"code","a0ce03b3":"code","53ffe345":"code","c157c966":"code","adb5a8c3":"code","b0c63620":"code","b8ba1ff8":"code","4f9dd45d":"code","c1b5ce94":"code","1f829062":"code","c1805eb7":"code","19a8bdd9":"code","7a97e99c":"code","ef70b8cb":"code","6e266c01":"code","d4af4a06":"code","cc0785fd":"code","fedff1c6":"code","24f12a83":"code","97106354":"code","742ac047":"code","22a0f95b":"markdown","c1127d0d":"markdown","7e63f33c":"markdown","7a2ded1e":"markdown","c469a0e5":"markdown","6e2b3c41":"markdown","38e62539":"markdown","3abe6918":"markdown","bedda880":"markdown","743a4993":"markdown","5c545ada":"markdown","467f182d":"markdown","ee651546":"markdown","f491a13c":"markdown","078b6bfc":"markdown","91277160":"markdown","946ff5cf":"markdown","9c980df8":"markdown","76bcc8a3":"markdown","d9da44d8":"markdown","33104f28":"markdown","9151138e":"markdown","09fdf5eb":"markdown","a738c546":"markdown","a73379e5":"markdown","f2fc6a40":"markdown","e3510cae":"markdown","00e7aa58":"markdown","05b2d2f7":"markdown","b768c606":"markdown","e8cdb523":"markdown","c49182eb":"markdown","a381bf28":"markdown","c096976f":"markdown","6477317f":"markdown","2ad8b28f":"markdown","64ba5837":"markdown","0443afef":"markdown","1a43336a":"markdown","cf4a0362":"markdown","2e04bc3f":"markdown","62329d7f":"markdown","ce3ac650":"markdown","b2ce9531":"markdown","19e34898":"markdown","c1e07cf5":"markdown","e29ce2c2":"markdown","e66619e0":"markdown","ef1ee73e":"markdown","a3a1b9ab":"markdown","de36d482":"markdown","5ca7acd4":"markdown","586195d2":"markdown","ac5c2734":"markdown","34f6842c":"markdown","d72ca082":"markdown","8582f84d":"markdown","705d3eb6":"markdown","2d1d5eaa":"markdown","eae270c5":"markdown","d0f01f19":"markdown","451a2193":"markdown","856d0ff9":"markdown","5e94a07b":"markdown","951efe32":"markdown","d4315ac7":"markdown","d294bdfa":"markdown","c2f148ee":"markdown","ba64fd72":"markdown","ae2389aa":"markdown","23892eb9":"markdown","eb626678":"markdown","5287aa57":"markdown","3d49c67d":"markdown","62651c4d":"markdown","7e7d3187":"markdown","c5592835":"markdown","b7aa16cf":"markdown","6bdbed3b":"markdown","31344c30":"markdown","77dbf530":"markdown","4ce11b4a":"markdown","c4aa2407":"markdown","e5521dfd":"markdown","b06dea34":"markdown","a8af7b0e":"markdown","984dc74c":"markdown","413cc399":"markdown","0d804cd6":"markdown","c734c15d":"markdown","0213d930":"markdown","da94cc06":"markdown","3c07e5d4":"markdown","f12fa2da":"markdown","2b9075d7":"markdown","9c75af27":"markdown","0931bb18":"markdown","e94168b9":"markdown","0d40ea45":"markdown","8e3c3ff4":"markdown","7bb97680":"markdown","d32b1d0f":"markdown","26915799":"markdown","f3754c9e":"markdown","72cdb0b2":"markdown","9202922b":"markdown","8f7fd134":"markdown","077d25e0":"markdown","987142b8":"markdown","1646f8b5":"markdown","a5a1a100":"markdown"},"source":{"95aa45d2":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \n\n\n\nfrom sklearn.model_selection import KFold,cross_val_score, RepeatedStratifiedKFold,StratifiedKFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler,PowerTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.dummy import DummyClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport plotly \nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ab9a9576":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf=df.drop('id', axis=1)\ndf.head()","7052144f":"print (f' We have {df.shape[0]} instances with the {df.shape[1]-1} features and 1 output variable')","c3d667f3":"df.duplicated().sum()","65a8e7e2":"df.info()","2a62179f":"y = df['stroke']\nprint(f'Percentage of patient had a stroke: % {round(y.value_counts(normalize=True)[1]*100,2)} --> ({y.value_counts()[1]} patient)\\nPercentage of patient did not have a stroke: % {round(y.value_counts(normalize=True)[0]*100,2)} --> ({y.value_counts()[0]} patient)')","196537e6":"fig = px.histogram(df, x=\"stroke\", title='Stroke', width=400, height=400)\nfig.show()","84dda546":"def missing (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values\n\nmissing(df)","98a86738":"msno.bar(df);","b6fae9b1":"msno.matrix(df);","569161d8":"categorical = ['gender', 'hypertension', 'heart_disease', 'ever_married',\n'work_type', 'Residence_type', 'smoking_status']\n\nnumerical = ['age','avg_glucose_level', 'bmi']","6fd2fb6d":"df[numerical].describe()","32a60d02":"df[numerical].skew()","5b7f26a6":"df[numerical].hist(figsize=(8,6));","c111529c":"print (f'{round(df[\"gender\"].value_counts(normalize=True)*100,2)}')\nfig = px.histogram(df, x=\"gender\", title='Gender', width=400, height=400)\nfig.show()","6b96673c":"print (f'{round(df[\"hypertension\"].value_counts(normalize=True)*100,2)}')\nfig = px.histogram(df, x=\"hypertension\", title='hypertension', width=400, height=400)\nfig.show()","41ff77d2":"print (f'{round(df[\"heart_disease\"].value_counts(normalize=True)*100,2)}')\nfig = px.histogram(df, x=\"heart_disease\", title='heart_disease', width=400, height=400)\nfig.show()","5512eb92":"print (f'{round(df[\"ever_married\"].value_counts(normalize=True)*100,2)}')\nfig = px.histogram(df, x=\"ever_married\", title='ever_married', width=400, height=400)\nfig.show()","2e8e79b3":"print (f'{round(df[\"work_type\"].value_counts(normalize=True)*100,2)}')\nfig = px.histogram(df, x=\"work_type\", title='work_type', width=400, height=400)\nfig.update_layout(xaxis={'categoryorder':'total descending'})\nfig.show()","e472b38f":"print (f'{round(df[\"Residence_type\"].value_counts(normalize=True)*100,2)}')\nfig = px.histogram(df, x=\"Residence_type\", title='Residence_type', width=400, height=400)\nfig.show()","eec3c344":"print (f'{round(df[\"smoking_status\"].value_counts(normalize=True)*100,2)}')\nfig = px.histogram(df, x=\"smoking_status\", title='smoking_status', width=400, height=400)\nfig.update_layout(xaxis={'categoryorder':'total descending'})\nfig.show()","4aa85b1a":"print (f'A person with hypertension has a probability of {round(df[df[\"hypertension\"]==1][\"stroke\"].mean()*100,2)} % get a stroke')\n\nprint()\n\nprint (f'A person without hypertension has a probability of  {round(df[df[\"hypertension\"]==0][\"stroke\"].mean()*100,2)} % get a stroke')\n\n","4b336fc9":"fig = px.histogram(df, x=\"hypertension\", color=\"stroke\",width=400, height=400)\nfig.show()","75c2091c":"print (f'A female person has a probability of {round(df[df[\"gender\"]==\"Female\"][\"stroke\"].mean()*100,2)} % get a stroke')\n\nprint()\n\nprint (f'A male person has a probability of {round(df[df[\"gender\"]==\"Male\"][\"stroke\"].mean()*100,2)} % get a stroke')\n\nprint()\n\nprint (f'A person from  the other category of gender has a probability of {round(df[df[\"gender\"]==\"Other\"][\"stroke\"].mean()*100,2)} % get a stroke')\n","308bb35a":"fig = px.histogram(df, x=\"gender\", color=\"stroke\",width=400, height=400)\nfig.show()","cc178135":"print (f'A person with heart disease has a probability of {round(df[df[\"heart_disease\"]==1][\"stroke\"].mean()*100,2)} % get a stroke')\n\nprint()\n\nprint (f'A person without heart disease has a probability of {round(df[df[\"heart_disease\"]==0][\"stroke\"].mean()*100,2)} % get a stroke')\n","b04d5e38":"fig = px.histogram(df, x=\"heart_disease\", color=\"stroke\",width=400, height=400)\nfig.show()","ad2dd00e":"print (f'A person married (or married before) has a probability of {round(df[df[\"ever_married\"]==\"Yes\"][\"stroke\"].mean()*100,2)} % get a stroke')\n\nprint()\n\nprint (f'A person never married has a probability of {round(df[df[\"ever_married\"]==\"No\"][\"stroke\"].mean()*100,2)} % get a stroke')\n","1443baf4":"fig = px.histogram(df, x=\"ever_married\", color=\"stroke\",width=400, height=400)\nfig.show()","e9234045":"print (f'A person with private work type has a probability of {round(df[df[\"work_type\"]==\"Private\"][\"stroke\"].mean()*100,2)} % get a stroke')\n\nprint()\n\nprint (f'Self-employed person has a probability of {round(df[df[\"work_type\"]==\"Self-employed\"][\"stroke\"].mean()*100,2)} % get a stroke')\n\nprint()\n\nprint (f'A person with a goverment job has a probability of {round(df[df[\"work_type\"]==\"Govt_job\"][\"stroke\"].mean()*100,2)} % get a stroke')\n\nprint()\n\nprint (f'A child has a probability of {round(df[df[\"work_type\"]==\"children\"][\"stroke\"].mean()*100,2)} % get a stroke')\n\nprint()\n\nprint (f'A person never worked has a probability of {round(df[df[\"work_type\"]==\"Never_worked\"][\"stroke\"].mean()*100,2)} % get a stroke')","a0ce03b3":"fig = px.histogram(df, x=\"work_type\", color=\"stroke\",width=600, height=600)\nfig.show()","53ffe345":"print (f'A person, who lives in urban area, has a probability of {round(df[df[\"Residence_type\"]==\"Urban\"][\"stroke\"].mean()*100,2)} %  get a stroke')\n\nprint()\n\nprint (f'A person, who lives in rural area, has a probability of {round(df[df[\"Residence_type\"]==\"Rural\"][\"stroke\"].mean()*100,2)} % get a stroke')\n","c157c966":"fig = px.histogram(df, x=\"Residence_type\", color=\"stroke\",width=400, height=400)\nfig.show()","adb5a8c3":"print (f'A formerly smoked person has a probability of {round(df[df[\"smoking_status\"]==\"formerly smoked\"][\"stroke\"].mean()*100,2)} % get a stroke')\n\nprint()\n\nprint (f'A person never smoked has a probability of {round(df[df[\"smoking_status\"]==\"never smoked\"][\"stroke\"].mean()*100,2)} % get a stroke')\n\nprint()\n\nprint (f'A person smokes has a probability of {round(df[df[\"smoking_status\"]==\"smokes\"][\"stroke\"].mean()*100,2)} % get a stroke')\n\nprint()\n\nprint (f'A person whom smoking history is not known,has a probability of {round(df[df[\"smoking_status\"]==\"Unknown\"][\"stroke\"].mean()*100,2)} % get a stroke')\n\nprint()","b0c63620":"fig = px.histogram(df, x=\"smoking_status\", color=\"stroke\",width=600, height=600)\nfig.show()","b8ba1ff8":"from sklearn.metrics import mutual_info_score\ndef cat_mut_inf(series):\n    return mutual_info_score(series, df['stroke']) \n\ndf_cat = df[categorical].apply(cat_mut_inf) \ndf_cat = df_cat.sort_values(ascending=False).to_frame(name='mutual_info_score') \ndf_cat","4f9dd45d":"df[numerical].corr()","c1b5ce94":"df.groupby('stroke')[numerical].mean()","1f829062":"df[['age','avg_glucose_level','bmi','stroke']].corr()","c1805eb7":"fig = px.scatter(df, x='age', y='bmi', title='Age & BMI ',color='stroke', hover_data = df[['stroke']])\nfig.show()","19a8bdd9":"fig = px.scatter(df, y='avg_glucose_level', x='age', title='Age & Average Glucose Level',color='stroke', hover_data = df[['stroke']])\nfig.show()","7a97e99c":"fig = px.scatter(df, y='avg_glucose_level', x='bmi', title='Average Glucose Level & BMI ',color='stroke', hover_data = df[['stroke']])\nfig.show()","ef70b8cb":"#def load_data ():\n #   df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\n  #  df=df.drop('id', axis=1)\n   # categorical = [ 'hypertension', 'heart_disease', 'ever_married','work_type', 'Residence_type', 'smoking_status']\n   # numerical = ['avg_glucose_level', 'bmi','age']\n   # y= df['stroke']\n   # X = df.drop('stroke', axis=1)\n   # return X,y,categorical, numerical","6e266c01":"# def evaluate_model(X, y, model):\n  #  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n   # scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    # return scores\n\n","d4af4a06":"#def load_data ():\n #   df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\n #   df=df.drop('id', axis=1)\n #   categorical = [ 'hypertension', 'heart_disease', 'ever_married','work_type', 'Residence_type', 'smoking_status']\n #   numerical = ['avg_glucose_level', 'bmi','age']\n #   y= df['stroke']\n #   X = df.drop('stroke', axis=1)\n #   return X,y,categorical, numerical\n\n\n#def baseline_model(X, y, model):\n#    transformer = ColumnTransformer(transformers=[('imp',SimpleImputer(strategy='median'),numerical),('o',OneHotEncoder(),categorical)])\n#    pipeline = Pipeline(steps=[('t', transformer),('p',PowerTransformer(method='yeo-johnson')),('m', model)])    \n#    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n#    scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n#    return scores\n\n# X,y,categorical, numerical= load_data()\n# model = DummyClassifier(strategy='constant', constant=1)\n# scores = baseline_model(X, y, model)\n# print('Mean roc_auc: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))","cc0785fd":"#def get_models():\n #   models, names = list(), list()\n  #  models.append(LogisticRegression(solver='liblinear'))    \n   # names.append('LR')\n   # models.append(LinearDiscriminantAnalysis())\n   # names.append('LDA')\n   # models.append(SVC(gamma='scale'))\n   # names.append('SVM')\n   # return models, names","fedff1c6":"# for i in range(len(models)):\n #   transformer = ColumnTransformer(transformers=[('imp',SimpleImputer(strategy='median'),numerical),('o',OneHotEncoder(),categorical)])\n #   pipeline = Pipeline(steps=[('t', transformer),('p',PowerTransformer(method='yeo-johnson')),('m', models[i])])    \n #   scores = evaluate_model(X, y, pipeline)\n   # results.append(scores)","24f12a83":"# plt.boxplot(results, labels=names, showmeans=True)\n# plt.show()","97106354":"def load_data ():\n    df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\n    df=df.drop('id', axis=1)\n    categorical = [ 'hypertension', 'heart_disease', 'ever_married','work_type', 'Residence_type', 'smoking_status']\n    numerical = ['avg_glucose_level', 'bmi','age']\n    y= df['stroke']\n    X = df.drop('stroke', axis=1)\n    return X,y,categorical, numerical\n\n\ndef evaluate_model(X, y, model):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n    scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    return scores\n\ndef get_models():\n    models, names = list(), list()\n    models.append(LogisticRegression(solver='liblinear'))    \n    names.append('LR')\n    models.append(LinearDiscriminantAnalysis())\n    names.append('LDA')\n    models.append(SVC(gamma='scale'))\n    names.append('SVM')\n    return models, names\n\n\nX,y,categorical, numerical= load_data()\nprint(X.shape, y.shape)\n\nmodels, names = get_models()\nresults = list()\n\nfor i in range(len(models)):\n    transformer = ColumnTransformer(transformers=[('imp',SimpleImputer(strategy='median'),numerical),('o',OneHotEncoder(),categorical)])\n    pipeline = Pipeline(steps=[('t', transformer),('p',PowerTransformer(method='yeo-johnson')),('m', models[i])])    \n    scores = evaluate_model(X, y, pipeline)\n    results.append(scores)\n    print('>%s %.3f (%.3f)' % (names[i], np.mean(scores), np.std(scores)))\n# plot the results\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","742ac047":"from imblearn.pipeline import Pipeline\n\ndef load_data ():\n    df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\n    df=df.drop('id', axis=1)\n    categorical = [ 'hypertension', 'heart_disease', 'ever_married','work_type', 'Residence_type', 'smoking_status']\n    numerical = ['avg_glucose_level', 'bmi','age']\n    y= df['stroke']\n    X = df.drop('stroke', axis=1)\n    return X,y,categorical, numerical\n\n\ndef evaluate_model(X, y, model):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n    scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    return scores\n\ndef get_models():\n    models, names = list(), list()\n    models.append(LogisticRegression(solver='liblinear'))    \n    names.append('LR')\n    models.append(LinearDiscriminantAnalysis())\n    names.append('LDA')\n    models.append(SVC(gamma='scale'))\n    names.append('SVM')\n    return models, names\n\n\nX,y,categorical, numerical= load_data()\nprint(X.shape, y.shape)\n\nmodels, names = get_models()\nresults = list()\n\nfor i in range(len(models)):\n    transformer = ColumnTransformer(transformers=[('imp',SimpleImputer(strategy='median'),numerical),('o',OneHotEncoder(),categorical)])\n    pipeline = Pipeline(steps=[('t', transformer),('p',PowerTransformer(method='yeo-johnson', standardize=True)),('over', SMOTE()), ('m', models[i])])    \n    scores = evaluate_model(X, y, pipeline)\n    results.append(scores)\n    print('>%s %.3f (%.3f)' % (names[i], np.mean(scores), np.std(scores)))\n# plot the results\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","22a0f95b":"<a id=\"3\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>What Problem We Have?<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","c1127d0d":"<a id=\"15\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Baseline Model<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","7e63f33c":"![](https:\/\/expertchikitsa.com\/wp-content\/uploads\/2017\/05\/stroke.jpg)","7a2ded1e":"#### Heart Disease","c469a0e5":"- The results suggest that logistic regression achieves a slightly better score than Linear Discriminant Analysis (.839 & .833). \n- The distribution between the two top-performing models appears roughly equivalent, LR gets a slightly bigger mean score than LDA.\n-  A box and whisker plot is created summarizing the distribution of results. \n- All methods sshow skill on the problem., \n- SVM gave the worst performance (.649), but let's see in our second model, how it performs?","6e2b3c41":"- We have 276 patient with heart disease which is 5.4 % of the sample.","38e62539":"#### Hi all.  \ud83d\ude4b\u200d\u2642\ufe0f \n\n#### I have recently published [Beginner Friendly Detailed Explained EDAs \u2013 For anyone at the beginnings of DS\/ML journey](https:\/\/www.kaggle.com\/general\/253911#1393015) series.\n\n#### After getting positive feedback and requests for Beginner-Intermediate Friendly Machine Learning series, I started to publish the Machine Learning Basic Series, which would help anyone who wants to learn or refresh the basics of ML.\n\n#### What we have covered: \n\n#### [BIAS & VARIANCE TRADEOFF](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-bias-variance-tradeoff) \u2714\ufe0f\n\n#### [LINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-linear-algorithms)  \u2714\ufe0f\n\n#### [NONLINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/nonlinear-algorithms)  \u2714\ufe0f\n\n#### [The Most Used Methods to Deal with MISSING VALUES](https:\/\/www.kaggle.com\/kaanboke\/the-most-used-methods-to-deal-with-missing-values)  \u2714\ufe0f\n\n#### By this post, we are starting to implement machine learning algorithms.\n\n#### In this notebook we will step by step implement end to end machine learning project.\n\n#### We will use **classification algorithms with imbalanced data**, which will be very fun. **Enjoy** \ud83e\udd18\n\n","3abe6918":"- As seen above, most of the categorical variables, almost no effect on the target variable.\n- It would be good exercise to see the differences on the model between with and without the some of the categorical variables. For the sake of the first study, we will keep the categorical variables in the model.","bedda880":"- Instances across classes are imbalanced, like in our dataset, we have imbalance data.\n\n- The problem is, most of the machine learning algorithm do not work well with the imbalanced data.\n\n- Some of the metrics (like accuracy) give us misleading results.\n\n- Most of the time in classification problems our interest is to get better predict on the minority class.\n\n- In our example: People had a stroke is minority class.\n\n- Otherwise our machine learning algorithm falsely predicts majority class.\n\n- In our example: No stroke is majority class.","743a4993":"- Almost %95 of the instances of our target variable is 'No stroke'\n-  4861 patient does not have a stroke\n- %5  of the instances of our target variable is 'Stroke'\n- 249 patient have a stroke.\n\n- We have imbalanced data.","5c545ada":"- If you have never worked, you will not die by stroke !!! Just kidding.\n\n- Self employed person has more probability to get stroke than other work type.\n\n- Person with private job and goverment job almost has same probability to get stroke.","467f182d":"- We will go step by step.\n  - We will load the data\n  - Then first we decide our evaluation method\n  - Then we will make baseline algorithm to give us lower limit\n  - Then we define our models\n  - We make our pipeline\n  - Analyze the models by evaluation results and boxplot.\n  - Decide which model perform the best in the evaluated models.\n  - We try to improve oour models.\n\n\n- Let's start.\n","ee651546":"- As name states, this our baseline model. \n- Any model gets better score than baseline has skill on our problem.\n- Any model gets lower score than baseline model, does not have skill on our problem.\n- In this study, our baseline model gives us (Mean roc_auc: 0.500 (0.000)).","f491a13c":"<a id=\"8\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Numerical Features<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","078b6bfc":"![](https:\/\/media.springernature.com\/m685\/springer-static\/image\/art%3A10.1038%2Fs42003-019-0440-4\/MediaObjects\/42003_2019_440_Fig1_HTML.png)","91277160":"<a id=\"toc\"><\/a>\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n    \n* [Data](#0)\n* [Exploratory Data Analysis](#1)\n    * [Insights from the First Glance](#2)\n    * [What Problem We Have?](#3)\n    * [Target Variable](#4)\n    * [Imbalance Data](#5)\n    * [Decide the Metric](#6)\n    * [Missing Values](#7)\n    * [Numerical Features](#8)\n    * [Categorical Features](#9)    \n    * [Bivariate Analysis](#10)\n    * [Insights from the Exploratory Data Analysis](#11)\n    \n    \n* [Model Selection](#12)    \n    * [Load the Data](#13)\n    * [Our Evaluation Model](#14)\n    * [Baseline Model](#15)\n    * [Our Models](#16)\n    * [Analyze the Models](#17)    \n    * [Visualize the Results](#18)\n    * [Our First Model](#19)    \n    * [Our Second Model with SMOTE](#20)\n\n\n* [Conclusion](#21)\n\n* [References & Further Reading](#22)\n","946ff5cf":"- We have 498 patient with hypertension which represents at raound 10 % of the sample.","9c980df8":"- After getting results, we will look at each Machine Learning algorithm by using box and whisker plots.","76bcc8a3":"- One of the first steps of exploratory data analysis should always be to look at what the values of y look like.","d9da44d8":"- As we have seen, stroke probability for those who has hypertension are quite different than for those who don't.\n- %13.2 and %3.9 respectively\n- It means that **person with hypertension are almost 3.3 time more likely to get stroke than the ones who don't have hypertension**.","33104f28":"<a id=\"4\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Target Variable<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","9151138e":"- As we have seen, stroke probability for those who has marriage history are quite different than for those who don't.\n- % 6.56 and % 1.65 respectively\n- It means that **person is married(or married before) are 5.7 times more likely to get stroke than the ones who don't have marriage history**.","09fdf5eb":"#### By the way, when you like the topic, you can show it by supporting \ud83d\udc4d\n\n####  **Feel free to leave a comment**. \n\n#### All the best \ud83e\udd18","a738c546":"- We have 2994 female and 2115 male and 1 other gender people.","a73379e5":"#### **Gender & Stroke**","f2fc6a40":"#### **Gender**","e3510cae":"- We will design and test several models, for that reason, each time we will start fresh with the data load.\n- First we drop the 'id' column, which is not useful for our model.\n- We defined, categorical and numerical columns.\n- We defined our target variable and our features.\n- Function returns our target variable and features.","00e7aa58":"- 2925 people work in the private sector.\n- 819 people are self-employed\n- 657 people work at the government job.\n","05b2d2f7":"- We have three numerical features in our dataset. \n\n- All of our numerical features are measured in different scales.\n\n- Many machine learning algorithms perform better standard range scaled numerical variables (such as Linear models,artificial neural networks, K-nearest Neighbors,support vector machines, etc.) \n\n- Tree models (such as, decision trees,random forest, etc.) work fine with different range numerical features.\n\n- Based on the mean & median score differences, we can expect\n- Slightly left skew on the 'age' (mean: 43.22 & median: 45)\n- Slightly right skew on the 'bmi' (mean: 28.89 & median: 28.10)\n- And right skew distribution on the 'avg_glucose_level' (mean: 106.14 & median: 91.88)\n\n- Let's see the skewness.","b768c606":"<a id=\"1\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Exploratory Data Analysis<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","e8cdb523":"- As we have mentioned before, we have imbalanced data and majority class (no stroke) oversamples the examples.\n- To deal with we used stratified method to get percentages same in the examples (5% & 95%)\n- Synthetic Minority Oversampling Technique (SMOTE) deals with the imbalanced data by adding random minority class instances ( a stroke) by using k nearest (a stroke) neighbor.\n- In our second model we used SMOTE technique and increased the minority class representation to the same level with the majority class (no stroke)","c49182eb":"<a id=\"22\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>References & Further Reading<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\n\n[Machine Learning - Beginner &Intermediate-Friendly BOOKS](https:\/\/www.kaggle.com\/general\/255972)","a381bf28":"#### Residence Type","c096976f":"<a id=\"13\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Load the Data<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","6477317f":"- We have developed model to predict classification problem.\n\n- First, we  made the detailed exploratory analysis.\n- We have decided which metric to use.\n- We analyzed both target and features in detail.\n- We transform categorical variables into numeric so we can use them in the model.\n- We transform numerical variables to reduce skewness and get close to normal  distribution.\n- We define our functions to use in our model.\n- We use cross validation model to evaluate our models.\n- We use pipeline to avoid data leakage.\n- We looked at the results of the each model and selected the best one for the problem in hand.\n- We have seen two different methods to use when developing model on the imbalanced data.\n\n- After this point it is up to you to develop and improve the models.  **Enjoy** \ud83e\udd18","2ad8b28f":"#### **Work Type & Stroke**","64ba5837":"#### **Hypertension & Stroke**","0443afef":"<a id=\"16\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Our Models<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","1a43336a":"<a id=\"7\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Missing Values<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","cf4a0362":"<a id=\"18\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Visualize the Results<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","2e04bc3f":"- Age and target variable weak positive relationship (almost .25).\n- Average glucose level's mean scores on the target have differences between a person who has a stroke or not. But this differences are small.\n- BMI does not have any significant relationship with the target variable. \n- **A person with hypertension are almost 3.3 time more likely to get stroke than the ones who don't have hypertension**.\n- Male compare to female are more likelyto get stroke, but difference between female and male is very small.\n- **A person with heart diease are 4.07 times more likely to get stroke than the ones who don't have heart disease.**\n- **A person is married(or married before) are 5.7 times more likely to get stroke than the ones who don't have marriage history**.\n- Self employed person has more probability to get stroke than other work type. Be carefull !!!\n- Person who lives in rural area slightly has more probablity to get sroke than a person who lives in rural area. Difference is small.\n- It is smal difference between who smokes and who does not smoke in regard to probability of getting stroke.","62329d7f":"#### Work Type","ce3ac650":"#### Below code-snippets are mostly generated through https:\/\/machinelearningmastery.com. I have made changes and modified them to adjust to the problem at hand.\n\nReference : https:\/\/machinelearningmastery.com\n\n","b2ce9531":"- When age increases, also the mean score on the stroke is increases.\n- Average glucose level's mean scores have differences between a person who has a stroke or not.\n- Bmi mean scores are close to each other.\n- Correlations with the target variable are very small. ","19e34898":"image credit: https:\/\/giphy.com\/","c1e07cf5":"### Correlation Matrix & Scatter Plots","e29ce2c2":"- As seen, there is not much difference between person residence type.\n- Person who lives in rural area slightly has more probablity to get sroke than a person who lives in rural area. Difference is small.","e66619e0":"- 3353 people have been married and 1757 people are not married before.","ef1ee73e":"<a id=\"17\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Analyze the Models<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","a3a1b9ab":"- Our  stroke dataset is an example of a so-called imbalanced dataset.\n- There are 19 times more people who didn\u2019t have stroke in our data than who had, and we say that the non-stroke class dominates the stroke class.\n- We can clearly see that: the stroke rate in our data is 0.048\n- Which is a strong indicator of class imbalance","de36d482":"- The degree of dependency between a categorical variable and the target variable can be measured by using mutual info score.\n- Higher values of mutual information mean a higher degree of dependence.\n- Higher the mutual info score implies that we can get the better prediction on the target variable.","5ca7acd4":"#### **Heart Disease & Stroke**","586195d2":"#### **Age & BMI** ","ac5c2734":"<a id=\"12\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>MODEL SELECTION<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","34f6842c":"<a id=\"10\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Bivariate Analysis<\/b><\/font>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","d72ca082":"#### Smoking","8582f84d":"- 2596 people live in the urban area\n- 2514 people live in the rural area","705d3eb6":"<a id=\"20\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b> Our Second Model with SMOTE <\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","2d1d5eaa":"<a id=\"6\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Decide the Metric<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","eae270c5":"#### **Metrics of Importance**","d0f01f19":"<a id=\"2\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Insights from the First Glance<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","451a2193":"![](https:\/\/miro.medium.com\/max\/1400\/1*FUZS9K4JPqzfXDcC83BQTw.png)","856d0ff9":"![](https:\/\/media.giphy.com\/media\/tkYpAbKdWj4TS\/giphy.gif)","5e94a07b":"Image Credit: https:\/\/miro.medium.com\/","951efe32":"#### Married","d4315ac7":"<a id=\"0\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Data<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","d294bdfa":"#### **Smoking & Stroke**","c2f148ee":"- First we have two list to contain models and their names\n- We decide to use several different Machine Learning classification algorithms.\n- We have defined them","ba64fd72":"-Let's see our numerical and categorical features seperately.","ae2389aa":"- There is very small positive correlation between numerical features.","23892eb9":"- There is a significant jump on the SVM (from .649 to .76). \n- Very small changes on the LR and LDA \n- All methods still show skill on the problem. \n","eb626678":"- In order to evaluate each model, we use for loop.\n- As  we have seen in the exploratory data analysis part, our dataset has missing values.\n- Most of the machine learning algorithms do not work with the missing values.\n- We have decided to use median imputation.\n- Also we need to change categorical variable to numerical ones by using one hot encoder.\n- Also as we have seen in the exploratory analysis part, we have skewed distibution on the numerical features.\n- For that reason first we standardize the raw data and then applied Yoe-Johnson transform method.\n- To avoid data leakage we have done this by using pipeline.\n- By using pipleline, model first fit to the training data and then repeat the process with the test data.\n- By using evaluate model function we have defined before, we evaluate each model and save their scores into score list.\n- By evaluating each model we will find out which algorithm works well with the problem in hand.\n- After finding the best algorithm, we will look deeper on that algorithm. ","5287aa57":"![](https:\/\/media.giphy.com\/media\/xT5LMzIK1AdZJ4cYW4\/giphy.gif)","3d49c67d":"- As seen in both skewness result and histograms, numerical features have skewness in different degrees.\n- We will deal with different scale and skewness during the modeling by using standardization (Standard scaler) and power transform (Yoe-Johnson).","62651c4d":"- We have missing values on the 'bmi', which is at around %4. \n- It seem that we have random missing values.\n- We will handle it by using pipeline during the modeling.","7e7d3187":"### **Univariate Analysis**","c5592835":"#### **Residence Type & Stroke**","b7aa16cf":"### **Skewness**","6bdbed3b":"- This is the first step when approaching a machine learning problem: decide the metric!\n\n- The choice of the wrong metric can mean choosing the wrong algorithm.\n\n- We see that the target is skewed and thus the best metric for this binary classification problem would be Area Under the ROC Curve (AUC). \n- We can use precision and recall too, but AUC combines these two metrics.\n\n- We have already seen the label\/target distribution, and we know that it is a binary classification problem with skewed targets. Thus, we will be using StratifiedKFold to split the data\n\n- Just for further info, it is not advisable to use accuracy as an evaluation metric, when dealing with higly imbalanced data. ","31344c30":"image credit : https:\/\/expertchikitsa.com","77dbf530":"<a id=\"21\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>Conclusion<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","4ce11b4a":"#### Stroke Prediction Dataset","c4aa2407":"<a id=\"9\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Categorical Features<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","e5521dfd":"![](https:\/\/media.giphy.com\/media\/6pUbJKgj2nI3GDytPN\/giphy.gif)","b06dea34":"#### **Married & Stroke**","a8af7b0e":"**Context**\n\n- According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths. This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n\n**Attribute Information**\n1. **id**: unique identifier\n\n2. **gender**: \"Male\", \"Female\" or \"Other\"\n\n3. **age**: age of the patient\n\n4. **hypertension**: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n\n5. **heart_disease**: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n\n6. **ever_married**: \"No\" or \"Yes\"\n\n7. **work_type**: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n\n8. **Residence_type**: \"Rural\" or \"Urban\"\n\n9. **avg_glucose_level**: average glucose level in blood\n\n10. **bmi**: body mass index\n\n11. **smoking_status**: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n\n12. **stroke**: 1 if the patient had a stroke or 0 if not\n\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient\n\nReference: https:\/\/www.kaggle.com\/fedesoriano\/stroke-prediction-dataset ","984dc74c":"<a id=\"14\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Our Evaluation Model<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","413cc399":"- There are differeneces based on the smoking habits.\n- A formerly smoked person has a probability to get stroke 1.66 times more than person never smoked.\n- A person smokes has a a probability to get stroke 1.11 times more than person never smoked.\n- It is smal difference between who smokes and who does not smoke in regard to probability of getting stroke.","0d804cd6":"image credit: https:\/\/www.nature.com","c734c15d":"- Based on the result, age has slight left skew, other two features have right tail, positively skewed shape distribution.","0213d930":"- K fold repeated stratified cross validation will be our evaluation tool for our proposed models.\n- 10 fold (n_splits=10) will be hold our instances.\n- Each fold will hold sample size \/ number of fold (5110 \/ 10) 511 instances.\n- Stratified version of k fold would be used to ensure both stroke and no stroke would be represented as their representation in the sample.\n- Each fold would have 5 % stroke and 95 % no stroke instances.\n- Better capture the variance of the chosen model, k fold evaluation process would be repeated 3 times. (n_repeats=3)\n- In a more detailed explanation: Each model will be evaluated n_splits * n_repeats time (10*3 =30) and the mean and standard deviation of these repeatations will be reported.\n- Models will be  evaluated and compared by using the area under ROC Curve or ROC AUC.","da94cc06":"#### Hypertension","3c07e5d4":"- Let' see their mean scores with the target variable","f12fa2da":"#### **Age & Average Glucose Level**","2b9075d7":"- In our dataset, we have both numerical and categorical variables.\n- It is essential to see whether columns are correctly inferred.\n- The most important one to look for is our target variable 'stroke'\n- 'Stroke' is detected as an integer, not as an object.\n- Target variable is coded as 1 for positive cases (has a stroke) and 0 for negative cases (does not have a stroke)\n- Both 'Hypertension' and 'heart disease\" are detected as an integer, not as an object. \n- Just remember from the data definition part, they are coded as 1 for the positive cases(has hypertension\/heart disease) \n- And 0 for the negative cases (does not have hypertension\/heart disease)\n- We don't need to change them, but it is good to see and be aware of it.\n- In addition to them, we have 3 categorical variables, which we have to encode as numerical.","9c75af27":"- **Male compare to female are more likelyto get stroke, but difference between female and male is very small.**","0931bb18":"<a id=\"11\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Insights from the Exploratory Data Analysis<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","e94168b9":"- Look for the detailed info on the missing values  ---> **[The Most Used Methods to Deal with MISSING VALUES](https:\/\/www.kaggle.com\/kaanboke\/the-most-used-methods-to-deal-with-missing-values)**","0d40ea45":"#### **Average Glucose Level & BMI**","8e3c3ff4":"- Finally let's see the scatter plots","7bb97680":"gif credit: https:\/\/giphy.com","d32b1d0f":"<a id=\"19\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b> Our First Model <\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","26915799":"- Read csv and see the top 5 instances of the data","f3754c9e":"#### **By the way, when you like the topic, you can show it by supporting** \ud83d\udc4d\n\n####  **Feel free to leave a comment in the notebook**. \n\n\n#### All the best \ud83e\udd18","72cdb0b2":"- 1892 people are never smoked\n- 789 people smoke","9202922b":"- And the correlation with the target variable","8f7fd134":"- We have binary classification problem.\n- We make prection on the target variable **STROKE**\n- And we will build a model to get best prediction on the stroke variable.","077d25e0":"- Let's import libraries","987142b8":"<a id=\"5\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Imbalance Data<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","1646f8b5":"> ","a5a1a100":"- As we have seen, stroke probability for those who has heart disease are quite different than for those who don't.\n- % 17.03 and % 4.18 respectively\n- It means that **person with heart diease are 4.07 times more likely to get stroke than the ones who don't have heart disease.**"}}