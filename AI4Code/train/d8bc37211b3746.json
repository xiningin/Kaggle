{"cell_type":{"d9542639":"code","3941052e":"code","1e652964":"code","144a0513":"code","0f4b5fac":"code","523bb8ad":"code","014853d2":"code","df87c519":"code","bdc65b9d":"code","5c776e61":"markdown","fbe1cee5":"markdown","9accdbfa":"markdown","70a32a1b":"markdown","8922f802":"markdown","df4d19b1":"markdown","9919e94d":"markdown","59588563":"markdown","8ce5de91":"markdown","9c0024e9":"markdown","6e306246":"markdown","e4cba17f":"markdown","8c912449":"markdown","88d8e258":"markdown"},"source":{"d9542639":"# Importing required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","3941052e":"ad  = pd.read_csv(\"..\/input\/advertising.csv\/Advertising.csv\")","1e652964":"ad.drop(\"Unnamed: 0\", axis=1, inplace=True)\nad.head()","144a0513":"# Fitting Linear Regression \nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(ad.TV.values.reshape(-1,1), ad.sales)","0f4b5fac":"plt.figure(figsize=(8,5))\nplt.subplot(111, facecolor='black')\nplt.scatter(ad.TV, ad.sales, color='gold')\nplt.plot(ad.TV,model.predict(ad.TV.values.reshape(-1,1)),color='red',linewidth=4)\nplt.text(0,22,\"Restrictive Model\\nHigh Bias\\nUnderfitting\", color='white',size=15)\nplt.text(220,1,\"@DataScienceWithSan\", color='white')","523bb8ad":"# Generating random data\nnp.random.seed(0)\nx = np.random.normal(0,1,100)\ny = 30 + 4*x - 2*(x**2) + 3*(x**3) + np.random.normal(0,1,100)\nplt.figure(figsize=(12,4))\n\nplt.subplot(121)\nplt.ylim(10,60)\nplt.xlim(-1,2)\nplt.scatter(x,y,s=20,c='seagreen')\nplt.plot(x, 27+12*x, color='orange', linewidth=1)\n\nplt.subplot(122)\nplt.ylim(10,60)\nplt.xlim(-1,2)\nplt.scatter(x,y,s=20,c='seagreen')\nx2 = np.linspace(-3,3,50)\nplt.plot(x2,30 + 4*x2 - 2*(x2**2) + 3*(x2**3), c='orange', linewidth=2)","014853d2":"from sklearn import neighbors\nknn = neighbors.KNeighborsRegressor(n_neighbors=1, weights='distance')\nknn.fit(ad.TV.values.reshape(-1,1), ad.sales)\n\nx_points_ad = np.linspace(0,300,100)\ny_knn_ad = knn.predict(x_points_ad.reshape(-1,1))\n\nplt.figure(figsize=(8,5))\nplt.subplot(111, facecolor='black')\nplt.scatter(ad.TV, ad.sales, color='gold')\nplt.plot(x_points_ad, y_knn_ad, color='red',linewidth=4)\nplt.text(0,22,\"Complex Model\\nHigh Variance\\nOverfitting\", color='white', size=15)\nplt.text(220,1,\"@DataScienceWithSan\", color='white')","df87c519":"## Making Random Data\n\nnp.random.seed(0)\nx = np.random.normal(0,10,50)\ny = 0.1*x + 0.01*(x**2) + 0.01*(x**3) + np.random.normal(0,10,50)\n\nx = np.array(x).reshape(-1,1)\ny = np.array(y).reshape(-1,1)","bdc65b9d":"plt.figure(figsize=(14,8))\n\n## Fitting and plotting Linear Regression\nregression_model = LinearRegression()\nregression_model.fit(x,y)\nplt.subplot(221)\nplt.scatter(x, y, edgecolor='skyblue',color='royalblue')\nplt.plot(x,regression_model.predict(x),color='orange',linewidth=1)\nplt.title(\"Figure 1 - Linear Regression\")\n\n#############################################################################\n\n## Fitting and plotting polynomial regression\nx_points = np.linspace(-25,25,100)\ny2 =  0.1*x_points + 0.01*(x_points**2) + 0.01*(x_points**3)\nplt.ylim(-150,150)\nplt.subplot(222)\nplt.scatter(x, y, edgecolor='skyblue',color='royalblue')\nplt.plot(x_points,y2,color='orange',linewidth=2)\nplt.title(\"Figure 2 - Polynomial Regression\")\n\n#############################################################################\n\n## Fitting and plotting KNN with high k\nfrom sklearn import neighbors\nknn = neighbors.KNeighborsRegressor(n_neighbors=9, weights='distance')\nknn.fit(x, y)\ny_knn_h = knn.predict(x_points.reshape(-1,1))\nplt.subplot(223)\nplt.scatter(x, y, edgecolor='skyblue', color='royalblue')\nplt.plot(x_points, y_knn_h, color='orange',linewidth=2)\nplt.title(\"Figure 3 - KNN with 9 nearest neighbors\")\n\n#############################################################################\n\n## Fitting and plotting KNN with k=1\nfrom sklearn import neighbors\nknn = neighbors.KNeighborsRegressor(n_neighbors=1, weights='distance')\nknn.fit(x, y)\ny_knn_h = knn.predict(x_points.reshape(-1,1))\n#plt.subplot(111,facecolor='navy')\nplt.subplot(224)\nplt.scatter(x, y, edgecolor='skyblue',color='royalblue' )\nplt.plot(x_points, y_knn_h, color='orange', linewidth=2)\nplt.title(\"Figure 4 - KNN with 1 nearest neighbor\")","5c776e61":"Let\u2019s summarise the key points about bias:\n* Bias is introduced when restrictive (inflexible) models are used to solve complex problems\n* As the flexibility of a model increases, the bias starts decreasing for training data.\n* Bias can cause underfitting, which further leads to poor performance and predictions.","fbe1cee5":"It is important to keep in mind that some bias and some variance will always be there while building a Machine Learning model. Both bias and variance add to the overall error in the model.\n\nTo minimize the reducible error (bias+variance), we must find the sweet spot between the two, where both bias and variance coexist with minimum possible values. This is called Bias-Variance Trade-off. It\u2019s more of a trade-off between Prediction Accuracy (Variance) and Model Interpretability (Bias).\n\nFind the full article here : https:\/\/towardsdatascience.com\/bias-variance-tradeoff-7ca56ba182a","9accdbfa":"What is Variance?\n\n***Variance is the amount by which our model will have to change if it were to make predictions on a different dataset.***\n\nLet\u2019s simplify the above statement \u2192 Our model should not yield high errors if we use it to estimate the outputs of unseen data. That is if the model shows good results on the training dataset, but poor results on testing, it is said to have high variance.","70a32a1b":"**We will use Linear Regression to display bias or underfitting**","8922f802":"**Performing KNN on above data to show high variance or overfitting**","df4d19b1":"Simple Linear Regression is an inflexible model that assumes a linear relationship between input and output variables. This assumption, approximation, and restriction introduce **bias** to this model.\n\n*Hence bias refers to the error which is observed while approximating a complex problem using a simple (or restrictive) model.*","9919e94d":"Hence, building a very complex model can come at the cost of overfitting. One must understand that too much learning can bring high variance.\n\nLet\u2019s see some of the key points about Variance.\n* Variance is the amount by which a model needs changing if it were to make predictions on unseen data.\n* High variance is equivalent to the overfitting of the model.\n* Restrictive models such as Linear Regression show low variance, whereas more complex and flexible models can introduce high variance.","59588563":"The plot on the right is quite more flexible than the one on the left. It fits more smoothly with the data. On the other hand, the plot on the left represents a poorly fitted model, which assumes a linear relationship in data. This poor-fitting due to high bias is also known as ***underfitting***. Underfitting results in poor performance and low accuracies and can be rectified if needed by using more flexible models.","8ce5de91":"Now take a few seconds to observe these plots and see how increasing the complexity of our model to train on the same data reduces bias and underfitting, thus reducing the training error.\n\n* Figure 1 shows the most restrictive model that is the linear model as we saw earlier. The bias in this model is certainly the highest.\n* Figure 2, the polynomial regression, is a smoother curve and reduces the training error further, hence showing a somewhat lesser bias than the linear model.\n* Figure 3 shows the K-Nearest Neighbors (KNN) model with K=9, which is able to classify the data more accurately than the previous two models.\n* Figure 4, the KNN model with K=1, closely follows the data and hardly misclassifies any samples. It gives the least amount of bias and underfitting as compared to all the previous models, but it shows very high variance and overfitting.","9c0024e9":"In Machine Learning, when a model performs so well on the training dataset, that it almost memorizes every outcome, it is likely to perform quite badly when running for testing dataset.\n\nThis phenomenon is known as ***overfitting*** and is generally observed while building very complex and flexible models.","6e306246":"## The Variance","e4cba17f":"## Bias Variance trade-off","8c912449":"You can download the data from here - http:\/\/faculty.marshall.usc.edu\/gareth-james\/ISL\/data.html ","88d8e258":"## The Bias"}}