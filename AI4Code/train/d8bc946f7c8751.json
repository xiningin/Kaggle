{"cell_type":{"bc78491b":"code","250e7cdc":"code","9cf44e37":"code","3bdc82d9":"code","d7a6a126":"code","d37cce1a":"code","65b33c34":"code","760b99d6":"code","fd84ee92":"code","35389a9c":"code","394f94b0":"code","d389098e":"code","efc5f737":"code","79d85f53":"code","ca94cbac":"code","6a7e6bab":"code","7c93e2a7":"code","a7b891cf":"code","75096c30":"code","4756995e":"code","0ce6535a":"code","93fbe060":"code","97ddb993":"code","02855abb":"code","b78855a3":"code","ec08edb0":"code","dfe61434":"code","e3437380":"code","78f35d60":"code","184600c9":"code","038d5774":"markdown","1b975e61":"markdown","0f138ca7":"markdown","49a472cc":"markdown","f0e7e701":"markdown","c0ccc27d":"markdown","e6425af7":"markdown","5947d30f":"markdown","d224b4b3":"markdown","956ecc5a":"markdown","6f2e1c34":"markdown","9ede31a6":"markdown","efb8b7d4":"markdown","96bc3381":"markdown","08ebfc20":"markdown"},"source":{"bc78491b":"\"\"\"\nImporting Libraries\n\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mp","250e7cdc":"\"\"\"\nImporting Machine Learning Libraries\n\"\"\"\nfrom sklearn.metrics import cohen_kappa_score as MC\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.metrics import f1_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm, datasets\nfrom sklearn import linear_model\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import metrics ","9cf44e37":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","3bdc82d9":"train_data.head() # first five columns of training dataset","d7a6a126":"plt.figure(figsize = (10,5))\n\ncorrMatrix = train_data.corr()\n\nsns.heatmap(corrMatrix, annot=True,cmap=\"Reds\")\nplt.show()","d37cce1a":"# Extract Title from Name, store in column and plot barplot\nimport re\n\ntrain_data['Title'] = train_data.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\nsns.countplot(x='Title', data=train_data);\nplt.xticks(rotation=45);\n\n\"\"\"\nReplacing Title Feature with smaller amount of titles\n\"\"\"\ntrain_data['Title'] = train_data['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'})\ntrain_data['Title'] = train_data['Title'].replace(['Don', 'Dona', 'Rev', 'Dr',\n                                            'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess', 'Jonkheer'],'Special')\nsns.countplot(x='Title', data=train_data);\nplt.xticks(rotation=45);","65b33c34":"# Did they have a Cabin?\ntrain_data['Has_Cabin'] = ~train_data.Cabin.isnull()","760b99d6":"\"\"\"\ndropping categorical column that is not needed\n\"\"\"\nclasses = train_data['Survived']\n\n\"\"\"\nConvert features into numeric values\n\"\"\"\ntrain_data['Sex'] = pd.factorize(train_data['Sex'])[0]\ntrain_data['Title'] = pd.factorize(train_data['Title'])[0]\ntrain_data['Embarked'] = pd.factorize(train_data['Embarked'])[0]\n\n    \nnew_data = train_data.drop(['Cabin','Ticket','Survived','Name'],axis = 1)\n\nnew_data #showing the data after deletion of columns","fd84ee92":"from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nnew_data = my_imputer.fit_transform(new_data)","35389a9c":"\"\"\"\nSplitting the Data for Testing and Training\n\"\"\"\nX_train, X_test, y_train, y_test = train_test_split(new_data, classes, test_size=0.25, stratify=classes,random_state=5)","394f94b0":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","d389098e":"\"\"\"\nLinear Support Vector Machines \n\"\"\"\nsvc = Pipeline([(\"scaler\",StandardScaler()),(\"linear_svc\",LinearSVC(C=1, loss= 'hinge'))])\n\nsvc.fit(X_train,y_train) # fitting linear SVM to dataset\n\ny_pred_svc = svc.predict(X_test) # making predictions with linear SVM\n\nprint(classification_report(y_test,y_pred_svc)) ","efc5f737":"\"\"\"\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nprint(random_grid)\n\n\"\"\"","79d85f53":"\"\"\"\n# Using the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available scores\n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(X_train,y_train)\n\"\"\"","ca94cbac":"\"\"\"\n\n# view the best parameters from fitting the random search\n\nrf_random.best_params_\n\"\"\"","6a7e6bab":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [70, 80, 90, 100],\n    'max_features': [2, 3,'auto','sqrt'],\n    'min_samples_leaf': [1, 2, 3],\n    'min_samples_split': [1, 2, 3],\n    'n_estimators': [600, 800, 1000, 1500]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train,y_train)\n\ngrid_search.best_params_\n\"\"\"","7c93e2a7":"\"\"\"\nRandom Forest Classifier\n\"\"\"\nfrom sklearn.ensemble import RandomForestClassifier \n\nrf = RandomForestClassifier(max_depth=70,n_estimators = 600,min_samples_split = 3,min_samples_leaf = 2,\n                           max_features = 'sqrt',bootstrap = True)\nrf.fit(X_train,y_train)\ny_pred_rf = rf.predict(X_test)\n\nprint(classification_report(y_test,y_pred_rf))","a7b891cf":"\"\"\"\nGradient Boosting Classifier\n\"\"\"\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier(random_state=0)\ngbc.fit(X_train,y_train)\ny_pred_gbc = gbc.predict(X_test)\n\nprint(classification_report(y_test,y_pred_gbc))","75096c30":"\"\"\"\nXGBoost\n\"\"\"\nimport xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\nxgb_model.fit(X_train, y_train)\n\ny_pred_xgb = xgb_model.predict(X_test)\n\nprint(classification_report(y_test,y_pred_xgb))","4756995e":"\"\"\"\nk-Nearest Neighbor\n\"\"\"\nfrom sklearn.neighbors import KNeighborsClassifier \n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn = knn.fit(X_train,y_train)\ny_pred_knn = knn.predict(X_test)\n\nprint(classification_report(y_test,y_pred_knn))","0ce6535a":"\"\"\"\nGaussian Naive Bayes\n\"\"\"\ngnb = GaussianNB()\ngnb.fit(X_train,y_train)\ny_pred_gnb = gnb.predict(X_test)\n\nprint(classification_report(y_test,y_pred_gnb))","93fbe060":"\"\"\"\nLinear Discriminant Analysis\n\"\"\"\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA \n\nlda=LDA(n_components=None)\nfit = lda.fit(X_train,y_train) # fitting LDA to dataset\ny_pred_lda=lda.predict(X_test) # predicting with LDA \n\nprint(classification_report(y_test,y_pred_lda))","97ddb993":"\"\"\"\nDecision Tree\n\"\"\"\ntrees = tree.DecisionTreeClassifier(max_depth = 3)\ntrees = trees.fit(X_train, y_train)\ny_pred_tree = trees.predict(X_test) \n\nprint(classification_report(y_test,y_pred_tree))","02855abb":"test_data.head() # first 5 rows of test dataset","b78855a3":"test_data['Title'] = test_data.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\nsns.countplot(x='Title', data=test_data);\nplt.xticks(rotation=45);\n\n\"\"\"\nReplacing Title Feature with smaller amount of titles\n\"\"\"\ntest_data['Title'] = test_data['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'})\ntest_data['Title'] = test_data['Title'].replace(['Don', 'Dona', 'Rev', 'Dr',\n                                            'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess', 'Jonkheer'],'Special')\nsns.countplot(x='Title', data=test_data);\nplt.xticks(rotation=45);","ec08edb0":"\"\"\"\nConvert features into numeric values\n\"\"\"\ntest_data['Sex'] = pd.factorize(test_data['Sex'])[0]\ntest_data['Title'] = pd.factorize(test_data['Title'])[0]\ntest_data['Embarked'] = pd.factorize(test_data['Embarked'])[0]\n\n# Did they have a Cabin?\ntest_data['Has_Cabin'] = ~test_data.Cabin.isnull()\n\nnew_test_data = test_data.drop(['Cabin','Ticket','Name'],axis = 1)\n\nnew_test_data # Output Dataframe","dfe61434":"from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nnew_test_data = my_imputer.fit_transform(new_test_data)","e3437380":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nnew_test_data = sc.fit_transform(new_test_data)","78f35d60":"y_pred_final = rf.predict(new_test_data)","184600c9":"submission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": y_pred_final\n    })\n\n# submission.to_csv('submission8.csv', index=False)","038d5774":"# Applying Preferred Classifier to Test Dataset","1b975e61":"**Scaling the Testing Data for Classification Model**","0f138ca7":"**GridSearch with Cross Validation (This is commented out)**","49a472cc":"**Seeing if each passenger on the Titanic had a Cabin or not**","f0e7e701":"**Making prediction on Test dataset**","c0ccc27d":"# **Feature Engineering**","e6425af7":"**Removing Missing Values by Imputing Values**","5947d30f":"**Removing columns that are not needed**","d224b4b3":"# **Feature Engineering**","956ecc5a":"**Scaling the Training Data for Classification Models**","6f2e1c34":"# Applying Standard Machine Learning Classification Approaches on Training Set#","9ede31a6":"**Random Hyperparameter Grid (This is commented out)**","efb8b7d4":"**Submitting Predictions**","96bc3381":"**Applying Imputation to Test Dataset**","08ebfc20":"**So multiple machine learning algorithms will be compared here and the one that performs best on the test set of the training data will be used for the testing data**"}}