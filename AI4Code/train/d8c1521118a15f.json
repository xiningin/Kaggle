{"cell_type":{"d946a470":"code","da74b6e8":"code","7de98811":"code","ba30e6ca":"code","f99c09a7":"code","8e677079":"code","d53ab697":"code","219f92ee":"code","accdfc7d":"code","9f309f7d":"code","88226ee0":"code","483e194f":"code","f80f042e":"code","9f5148f1":"code","c6fb544d":"code","b9c2c2fd":"code","65a33b1c":"markdown","c56cb839":"markdown","5b812cfe":"markdown","72f97206":"markdown","a7505ff0":"markdown","d90fdcbf":"markdown","faf196bd":"markdown","b2cdced5":"markdown","254bb4b5":"markdown","0980af80":"markdown","3ee3f751":"markdown","670d6ec4":"markdown","11c631f9":"markdown"},"source":{"d946a470":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da74b6e8":"def LeTweet(df):\n    \n    #Le as tabelas\n    tweets = pd.read_csv(\"Dataset\/Tweets\/\"+str(df)+\"_tweets.csv\")\n    tweets_vectorized_media = pd.read_csv(\"Dataset\/Tweets\/\"+str(df)+\"_tweets_vectorized_media.csv\")\n    tweets_vectorized_text = pd.read_csv(\"Dataset\/Tweets\/\"+str(df)+\"_tweets_vectorized_text.csv\")\n    \n    #Calcula o n\u00famero de imagens (m\u00eddia) que tem no tweet\n    number_of_images = tweets_vectorized_media.groupby(\"tweet_id\").size().reset_index()\n    number_of_images.columns = [\"tweet_id\", \"num_of_images\"]\n\n    #Aqueles tweets que n\u00e3o tem m\u00eddia vou colocar o num_of_image = 0\n    for tweet in tweets.tweet_id.unique(): \n        if tweet not in number_of_images.tweet_id.values:\n            df = pd.DataFrame({\"tweet_id\":tweet,\n                              \"num_of_images\":0}, index = [\"0\"])\n            number_of_images = pd.concat([number_of_images, df], axis = 0, ignore_index = True)\n            \n    #Junta as tabelas\n    tweets = tweets.merge(number_of_images, on = \"tweet_id\", how = \"left\")\n    tweets = tweets.merge(tweets_vectorized_text, on = \"tweet_id\", how = \"left\")\n    \n    return tweets","7de98811":"#Le as bases de treino e teste dos tweets\ntrain_tweets = LeTweet(\"train\")\ntest_tweets = LeTweet(\"test\")","ba30e6ca":"#Le as tabelas\nusers = pd.read_csv(\"Dataset\/Users\/users.csv\")\nuser_vectorized_descriptions = pd.read_csv(\"Dataset\/Users\/user_vectorized_descriptions.csv\")\nuser_vectorized_profile_images = pd.read_csv(\"Dataset\/Users\/user_vectorized_profile_images.csv\")\n\n#Junta as tabelas\nusers = users.merge(user_vectorized_descriptions, on = \"user_id\", how = \"left\")\nusers = users.merge(user_vectorized_profile_images, on = \"user_id\", how = \"left\")","f99c09a7":"train = pd.merge(train_tweets, users, how='left', left_on='tweet_user_id', right_on='user_id')\n\n\ntest = pd.merge(test_tweets, users, how='left', left_on='tweet_user_id', right_on='user_id')\ntestId = test.loc[:, \"tweet_id\"]","8e677079":"#Fun\u00e7\u00e3o para calcular a distribui\u00e7\u00e3o dos valores de um vetor\ndef distribution(arr):\n    uniq = np.sort(np.unique(arr))\n    freq = []\n    for v in uniq:\n        freq.append(np.sum(arr == v))\n    freq=np.array(freq)\n    return uniq, freq\/len(arr)","d53ab697":"#Pega os valores da vari\u00e1vel \"user_id\" para cada viralidade\nlevels = {1:[], 2:[], 3:[], 4:[], 5:[]}\n    \nfor i in range(train.shape[0]):\n    if not pd.isna(train.loc[i,\"user_id\"]):\n        topics = np.unique(train.loc[i,\"user_id\"]).astype(int)\n        for t in sorted(train.user_id.unique()):\n            if t in topics:\n                levels[train.loc[i,'virality']].append(t)\n                    \n    \n#Plota a distribui\u00e7\u00e3o para cada viralidade\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(18,12))\nax[1,2].remove()\n\nlvl1, plvl1 = distribution(levels[1])\nlvl2, plvl2 = distribution(levels[2])\nlvl3, plvl3 = distribution(levels[3])\nlvl4, plvl4 = distribution(levels[4])\nlvl5, plvl5 = distribution(levels[5])\n\nax[0,0].bar(lvl1, plvl1)\nax[0,0].set_title('Virality = 1', fontsize=24)\n\nax[0,1].bar(lvl2, plvl2)\nax[0,1].set_title('Virality = 2', fontsize=24)\n\nax[0,2].bar(lvl3, plvl3)\nax[0,2].set_title('Virality = 3', fontsize=24)\n\nax[1,0].bar(lvl4, plvl4)\nax[1,0].set_title('Virality = 4', fontsize=24)\n\nax[1,1].bar(lvl5, plvl5)\nax[1,1].set_title('Virality = 5', fontsize=24)\n\nfig.suptitle(' \"user_id\" por Viralidade', fontsize=24)\nplt.show()","219f92ee":"# Detecta quais ids de t\u00f3picos existem no conjunto\ntopics_num = np.array([])\nfor i in range(train.shape[0]):\n    if not pd.isna(train.loc[i,'tweet_topic_ids']):\n        topics = np.unique(eval(train.loc[i,'tweet_topic_ids'])).astype(int)\n        topics_num = np.concatenate([topics_num, topics])\nunique_topics = np.unique(topics_num).astype(int)  \n\n#Pega os valores da vari\u00e1vel \"tweet_topic_ids\" para cada viralidade\nlevels = {1:[], 2:[], 3:[], 4:[], 5:[]}\nfor i in range(train.shape[0]):\n    if not pd.isna(train.loc[i,'tweet_topic_ids']):\n        topics = np.unique(eval(train_tweets.loc[i,'tweet_topic_ids'])).astype(int)\n        for t in unique_topics:\n            if t in topics:\n                levels[train_tweets.loc[i,'virality']].append(t)\n    \n#Plota a distribui\u00e7\u00e3o para cada viralidade\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(18,12))\nax[1,2].remove()\n\nlvl1, plvl1 = distribution(levels[1])\nlvl2, plvl2 = distribution(levels[2])\nlvl3, plvl3 = distribution(levels[3])\nlvl4, plvl4 = distribution(levels[4])\nlvl5, plvl5 = distribution(levels[5])\n\nax[0,0].bar(lvl1, plvl1)\nax[0,0].set_title('Virality = 1', fontsize=24)\n\nax[0,1].bar(lvl2, plvl2)\nax[0,1].set_title('Virality = 2', fontsize=24)\n\nax[0,2].bar(lvl3, plvl3)\nax[0,2].set_title('Virality = 3', fontsize=24)\n\nax[1,0].bar(lvl4, plvl4)\nax[1,0].set_title('Virality = 4', fontsize=24)\n\nax[1,1].bar(lvl5, plvl5)\nax[1,1].set_title('Virality = 5', fontsize=24)\n\nfig.suptitle(' \"tweet_topic_ids\" por Viralidade', fontsize=24)\nplt.show()\n","accdfc7d":"#Remove as colunas de ID para n\u00e3o influenciar nos modelos\ntrain.drop([\"tweet_id\", \"tweet_user_id\"], axis = 1, inplace = True)\ntest.drop([\"tweet_id\", \"tweet_user_id\"], axis = 1, inplace = True)\n\n#Remove a coluna tweet_topic_ids, pois vimos que n\u00e3o \u00e9 relevante na An\u00e1lise explorat\u00f3ria de dados\ntrain.drop(\"tweet_topic_ids\", axis = 1, inplace = True)\ntest.drop(\"tweet_topic_ids\", axis = 1, inplace = True)","9f309f7d":"#Converte os valores das vari\u00e1veis categ\u00f3ricas para num\u00e9rico\n#  Exemplo para tweet_attachment_class: (C -> 0, A -> 3, etc.)\n\n#Fazemos isso pois h\u00e1 alguns modelos que necessitam que as covari\u00e1veis\n#  sejam num\u00e9ricas e n\u00e3o texto.\n\nfrom sklearn.preprocessing import LabelEncoder\nlbl = LabelEncoder()\nfor col in train.columns.values:\n    if train.loc[:,col].dtype == \"object\":\n        lbl.fit(train.loc[:,col].astype(str))\n        train.loc[:,col] = lbl.transform(train.loc[:,col].astype(str))\n        test.loc[:,col] = lbl.transform(test.loc[:,col].astype(str))","88226ee0":"from sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\n\ndf = train.copy()\n\ncolumns = df.columns.values\ntarget = \"virality\"\ny_columns = [target]\nx_columns = [x for x in columns if x != target]\n\nX = df[x_columns]\ny = df[y_columns]\n\n#PCA\npca = PCA(n_components = 50)\npca.fit(X)\n\nX = pca.transform(X)\ntest = pca.transform(test)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2, stratify = y)","483e194f":"#Note que puxamos muitas bibliotecas que n\u00e3o foram\/ser\u00e3o usadas no c\u00f3digo\n#  isso se deve porque testamos v\u00e1rias combina\u00e7\u00f5es de modelo nos dados de treino\n#  e acabamos selecionando a combina\u00e7\u00e3o mais simples que gerou um resultado satisfat\u00f3rio.\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import StackingClassifier\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\nfrom sklearn.pipeline import make_pipeline","f80f042e":"%%time\n\ncat = CatBoostClassifier()\ncat.fit(X_train, y_train.to_numpy().ravel())\n\ny_pred = cat.predict(X_test)\nprint(\"Acc. Score: {}\".format(accuracy_score(y_test, y_pred)))\n\n\ncat.fit(X, y.to_numpy().ravel())\ny_pred = cat.predict(test).reshape(-1)\n\npredicoes = pd.DataFrame(data = {\"tweet_id\":testId.values, \"virality\":y_pred})\npredicoes.to_csv(\"resposta_cat.csv\",index = False)","9f5148f1":"%%time\n\n\ngb = XGBClassifier()\ngb.fit(X_train, y_train.to_numpy().ravel())\n\ny_pred = gb.predict(X_test)\nprint(\"Acc. Score: {}\".format(accuracy_score(y_test, y_pred)))\n\ngb.fit(X, y.to_numpy().ravel())\ny_pred = gb.predict(test)\n\nprint(y_pred)","c6fb544d":"def get_stacking():\n    \n    level0 = list()\n    level0.append(('lr', LogisticRegression()))\n    level0.append(('knn', KNeighborsClassifier()))\n    level0.append(('cart', DecisionTreeClassifier()))\n    level0.append(('bayes', GaussianNB()))\n    \n    level0.append(('gb', GradientBoostingClassifier()))\n    level0.append(('xgb', XGBClassifier()))\n    level0.append(('cat', CatBoostClassifier()))\n    \n    level1 = LogisticRegression()\n    \n    model = StackingClassifier(estimators=level0, final_estimator = level1, stack_method = \"predict\",\n                               cv=5)\n    return model\n\ndef get_models():\n    models = dict()\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    models['bayes'] = GaussianNB()\n    \n    models['gb'] = GradientBoostingClassifier()\n    models['xgb'] = XGBClassifier()\n    models['cat'] = CatBoostClassifier()\n    \n    models['stacking'] = get_stacking()\n    return models\n\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores","b9c2c2fd":"%%time\n#Aplica os modelos\n# e verifica a acur\u00e1cia deles na valida\u00e7\u00e3o cruzada\nmodels = get_models()\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X_train, y_train)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n    \nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()\n\n\n#Aplica os modelos com StandardScaler()\n# e verifica a acur\u00e1cia deles na valida\u00e7\u00e3o cruzada\nmodels = get_models()\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(make_pipeline(StandardScaler(), model), X_train, y_train)\n    results.append(scores)\n    names.append(\"std_\"+name)\n    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n    \nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","65a33b1c":"![image.png](attachment:05b7bc8b-8e4b-4a3f-865b-2154b04c5d10.png)","c56cb839":"Stacking de modelos \n\nOBS: demora MUITO pra rodar, demora tanto que n\u00e3o conseguimos terminar de rodar ele para verificar a acur\u00e1cia...","5b812cfe":"## Combina\u00e7\u00e3o dos dados de tweet e usu\u00e1rios","72f97206":"OBS: no final do notebook tem a imagem da submiss\u00e3o no site da competi\u00e7\u00e3o. O modelo a seguir foi respons\u00e1vel pela submiss\u00e3o com um quadrado vermelho em volta, que gerou o score de 67.56301","a7505ff0":"# PCA e Label Encoder","d90fdcbf":"# User\n\nLeitura das bases de usu\u00e1rios","faf196bd":"# Conclus\u00e3o\n\nComo j\u00e1 conseguimos uma acur\u00e1cia de 67.5% nos dados p\u00fablicos da competi\u00e7\u00e3o, achamos melhor n\u00e3o tentarmos aplicar outras t\u00e9cnicas de pre processamento, modelos ou qualquer outra coisa para evitar overfit.","b2cdced5":"# An\u00e1lise Explorat\u00f3ria","254bb4b5":"### Outros modelos\/combina\u00e7\u00f5es de modelos que acabaram n\u00e3o sendo utilizadas","0980af80":"XGBoost (n\u00e3o demora tanto para rodar)","3ee3f751":"# Tweet\n\nLeitura dos dados sobre os tweets.","670d6ec4":"### Modelo final (usado para as respostas na competi\u00e7\u00e3o)","11c631f9":"# Modelagem"}}