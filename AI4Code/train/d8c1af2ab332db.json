{"cell_type":{"ec23dcca":"code","06e96409":"code","fb0ad3b4":"code","bd02863a":"code","b1f3f013":"code","23da2188":"code","c6a78e73":"code","3dbf9d7b":"code","21d8aedb":"code","b51b773b":"code","879967fa":"code","e4f9851e":"code","03c92479":"code","47e5c444":"code","5b381f96":"code","48021956":"code","2b32c829":"code","615477bf":"code","02bac926":"code","0c2008cb":"code","8d0bf868":"code","dfaf80b6":"code","173b4338":"code","aa5b37c7":"code","05410c4e":"code","bd246306":"code","8f8ccc82":"code","57497a4a":"code","078cfaa2":"code","ae891612":"markdown","396d3c44":"markdown","eec7cca3":"markdown","f5b699e7":"markdown","a8cbf932":"markdown","33fe2f08":"markdown","ea55bf7b":"markdown","9684d008":"markdown","9670dff7":"markdown","9c1239a9":"markdown","579428c6":"markdown","6ca488a3":"markdown","8422c4c6":"markdown","dbb4274f":"markdown"},"source":{"ec23dcca":"from PIL import Image \nImage.open(\"..\/input\/hitters\/hitters.jfif\")","06e96409":"# installlation required\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport missingno as msno\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n!pip install catboost\n!pip install lightgbm\n!pip install xgboost\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.width', 500)","fb0ad3b4":"# Load data into pandas dataframe..\n\ndf = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\ndf.head()","bd02863a":"df.info(),\ndf.shape  # There are 322 observation and 20 variables.","b1f3f013":"# Descriptive Analysis\n# When I examine the dataset, we see that there is a difference between the mean and median values of the Assist variable.\n# This difference is also supported by the standard deviation.\n\ndf.describe([0.05, 0.25, 0.50, 0.75, 0.95, 0.99]).T","23da2188":"# Returns cat, num and cat but car cols\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    # print(f\"Observations: {dataframe.shape[0]}\")\n    # print(f\"Variables: {dataframe.shape[1]}\")\n    # print(f'cat_cols: {len(cat_cols)}')\n    # print(f'num_cols: {len(num_cols)}')\n    # print(f'cat_but_car: {len(cat_but_car)}')\n    # print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","c6a78e73":"# Percentage values of categorical variables\ndef cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show(block=True)\n        \n# examination of categorical variables\n# plot shows there are around 140 count for Player's of League A and around 125 for League\nfor col in cat_cols:\n    cat_summary(df, col, plot=True)","3dbf9d7b":"# Quantiles of numeric variables\ndef num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n\n    if plot:\n        dataframe[numerical_col].hist(bins=20)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show(block=True)\n        \nfor col in num_cols:\n    num_summary(df, col, plot=True)        ","21d8aedb":"# Correlation analysis of numerical variables was performed\ncorrelation_matrix = df.corr().round(3)\nthreshold=0.95\nfiltre=np.abs(correlation_matrix['Salary']) > 0.35\ncorr_features=correlation_matrix.columns[filtre].tolist()\nsns.clustermap(df[corr_features].corr(),annot=True,fmt=\".2f\")\nplt.show()","b51b773b":"# Examination of categorical variables with Target\ndef target_summary_with_cat(dataframe, target, categorical_col):\n    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")\n    \nfor col in cat_cols:\n    target_summary_with_cat(df, \"Salary\", col)","879967fa":"# Missing Value Understanding\nmsno.bar(df)","e4f9851e":"def outlier_thresholds(dataframe, col_name, q1=0.10, q3=0.90):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\n\ndef check_outlier(dataframe, col_name, q1=0.10, q3=0.90):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name, q1, q3)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False","03c92479":"outlier_thresholds(df, df.columns)","47e5c444":"# Here, 3 outlier observations in all variables in quartiles of 0.1 and 0.9 are accessed\n# As we can see below, there are outliers in the dataset\nfor col in num_cols:\n    print(col, check_outlier(df, col))","5b381f96":"# replace with thresholds\nfor col in num_cols:\n    replace_with_thresholds(df, col)","48021956":"# Examined detailed outlier analysis for the target variable\n#It is seen that there is a jump between 85 and 95 values. We will be using boxplot to examine the details of the values included.\ndf[\"Salary\"].describe([0.05, 0.25, 0.45, 0.50, 0.65, 0.85, 0.95, 0.99]).T","2b32c829":"sns.boxplot(x=df[\"Salary\"])\nplt.show()\n\n# remove salary bigger than up limit\nq3 = 0.90\nsalary_up = int(df[\"Salary\"].quantile(q3))\ndf = df[(df[\"Salary\"] < salary_up)]","615477bf":"# Result of operation with Q3\nsns.boxplot(x=df[\"Salary\"])\nplt.show()","02bac926":"for col in cat_cols:\n    cat_summary(df, col)","0c2008cb":"df.dropna(inplace=True)","8d0bf868":"df[\"new_Hits\/CHits\"] = df[\"Hits\"] \/ df[\"CHits\"]\ndf[\"new_OrtCHits\"] = df[\"CHits\"] \/ df[\"Years\"]\ndf[\"new_OrtCHmRun\"] = df[\"CHmRun\"] \/ df[\"Years\"]\ndf[\"new_OrtCruns\"] = df[\"CRuns\"] \/ df[\"Years\"]\ndf[\"new_OrtCRBI\"] = df[\"CRBI\"] \/ df[\"Years\"]\ndf[\"new_OrtCWalks\"] = df[\"CWalks\"] \/ df[\"Years\"]\n\n\ndf[\"New_Average\"] = df[\"Hits\"] \/ df[\"AtBat\"]\ndf['new_PutOutsYears'] = df['PutOuts'] * df['Years']\ndf[\"new_RBIWalksRatio\"] = df[\"RBI\"] \/ df[\"Walks\"]\ndf[\"New_CHmRunCAtBatRatio\"] = df[\"CHmRun\"] \/ df[\"CAtBat\"]\ndf[\"New_BattingAverage\"] = df[\"CHits\"] \/ df[\"CAtBat\"]\n\n###########################################\n# Variables that I added but have low feature importance\n\n# df[\"new_CWalks*CRuns\"] = df[\"CWalks\"] * df[\"CRuns\"]\n# df[\"new_RBIWalks\"] = df[\"RBI\"] * df[\"Walks\"]\n# df[\"new_CRunsCHmRun\"] = df[\"CHmRun\"] \/ df[\"CRuns\"]\n# df[\"New_CRunsWalks\"] = df[\"CRuns\"] * df[\"Walks\"]\n# df['New_PutOutsAssists'] = df['PutOuts'] * df['Assists']\n###########################################","dfaf80b6":"# First, we will proceed by deleting the missing values.\ndf= df.dropna()","173b4338":"# Binary Encoding\n# label encoding of categorical features (League, Division, NewLeague) with two class\nbinary_cols = [col for col in df.columns if df[col].dtype not in [int, float] and df[col].nunique() == 2]\nfor col in binary_cols:\n    labelencoder = LabelEncoder()\n    df[col] = labelencoder.fit_transform(df[col])\n\n# One-Hot Encoding\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=False)\n    return dataframe\n\ndf = one_hot_encoder(df, cat_cols)","aa5b37c7":"y = df[\"Salary\"]\nX = df.drop([\"Salary\"], axis=1)\n\n# list feature importances for a regressor model like LGBM\npre_model = LGBMRegressor().fit(X, y)\nfeature_imp = pd.DataFrame({'Feature': X.columns, 'Value': pre_model.feature_importances_})\nfeature_imp.sort_values(\"Value\", ascending=False)","05410c4e":"# Standart Scaler \nscaler = StandardScaler()\nX = scaler.fit_transform(X)","bd246306":"models = [('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('KNN', KNeighborsRegressor()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          ('SVR', SVR()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor()),\n          # (\"CatBoost\", CatBoostRegressor(verbose=False))\n          ]\n\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n","8f8ccc82":"\ncart_params = {'max_depth': range(1, 20), \n               \"min_samples_split\": range(2, 30)}\n\nrf_params = {\"max_depth\": [5, 8, 15, None],\n             \"max_features\": [5, 7, \"auto\"],\n             \"min_samples_split\": [3, 5, 8, 15, 20],\n             \"n_estimators\": [600, 650, 1000]}\n\nxgboost_params = {\"learning_rate\": [0.1, 0.01, 0.01],\n                  \"max_depth\": [5, 8, 12, 20],\n                  \"n_estimators\": [100, 200, 300, 500],\n                  \"colsample_bytree\": [0.5, 0.8, 1]}\n\nlightgbm_params = {\"learning_rate\": [0.001, 0.01, 0.1, 0.001],\n                   \"n_estimators\": [250, 300, 500, 1500, 2500,3000],\n                   \"colsample_bytree\": [0.1, 0.3, 0.5, 0.7, 1]}\n\nregressors = [(\"CART\", DecisionTreeRegressor(), cart_params),\n              (\"RF\", RandomForestRegressor(), rf_params),\n              ('XGBoost', XGBRegressor(objective='reg:squarederror'), xgboost_params),\n              ('LightGBM', LGBMRegressor(), lightgbm_params)]\n\nbest_models = {}\n\nfor name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n\n    gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n\n    final_model = regressor.set_params(**gs_best.best_params_)\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n    best_models[name] = final_model\n","57497a4a":"voting_reg = VotingRegressor(estimators=[('RF', best_models[\"RF\"]),('LightGBM', best_models[\"LightGBM\"])])\n\nvoting_reg.fit(X, y)\nnp.mean(np.sqrt(-cross_val_score(voting_reg,\n                                 X, y,\n                                 cv=10,\n                                 scoring=\"neg_mean_squared_error\")))","078cfaa2":"def hitters_data_prep(dataframe):\n\n    ############ Specifying variable types ############\n\n    cat_cols, num_cols, cat_but_car = grab_col_names(dataframe, cat_th=5, car_th=20)\n\n    ############ We replace with thresholds ############\n\n    for col in num_cols:\n        replace_with_thresholds(dataframe, col)\n\n    ############ remove salary bigger than up limit ############\n\n    q3 = 0.90\n    salary_up = int(dataframe[\"Salary\"].quantile(q3))\n    dataframe = dataframe[(dataframe[\"Salary\"] < salary_up)]\n\n    ############ Feature engineering  ############\n\n    # New variables were created with the most appropriate variables according to their proportions.\n    dataframe[\"new_Hits\/CHits\"] = dataframe[\"Hits\"] \/ dataframe[\"CHits\"]\n    dataframe[\"new_OrtCHits\"] = dataframe[\"CHits\"] \/ dataframe[\"Years\"]\n    dataframe[\"new_OrtCHmRun\"] = dataframe[\"CHmRun\"] \/ dataframe[\"Years\"]\n    dataframe[\"new_OrtCruns\"] = dataframe[\"CRuns\"] \/ dataframe[\"Years\"]\n    dataframe[\"new_OrtCRBI\"] = dataframe[\"CRBI\"] \/ dataframe[\"Years\"]\n    dataframe[\"new_OrtCWalks\"] = dataframe[\"CWalks\"] \/ dataframe[\"Years\"]\n\n    dataframe[\"New_Average\"] = dataframe[\"Hits\"] \/ dataframe[\"AtBat\"]\n    dataframe['new_PutOutsYears'] = dataframe['PutOuts'] * dataframe['Years']\n    dataframe[\"new_RBIWalksRatio\"] = dataframe[\"RBI\"] \/ dataframe[\"Walks\"]\n    dataframe[\"New_CHmRunCAtBatRatio\"] = dataframe[\"CHmRun\"] \/ dataframe[\"CAtBat\"]\n    dataframe[\"New_BattingAverage\"] = dataframe[\"CHits\"] \/ dataframe[\"CAtBat\"]\n    dataframe.dropna(inplace=True)\n\n    ############ Binary Encoding ############\n    # label encoding of categorical features (League, Division, NewLeague) with two class\n    binary_cols = [col for col in dataframe.columns if dataframe[col].dtype not in\n                   [int, float] and dataframe[col].nunique() == 2]\n\n    for col in binary_cols:\n        labelencoder = LabelEncoder()\n        dataframe[col] = labelencoder.fit_transform(dataframe[col])\n\n    ############ One-Hot Encoding ############\n    cat_cols, num_cols, cat_but_car = grab_col_names(dataframe)\n    dataframe = one_hot_encoder(dataframe, cat_cols)\n\n    ############ MODEL ############\n\n    y = dataframe[\"Salary\"]\n    X = dataframe.drop([\"Salary\"], axis=1)\n\n\n    ############ Scaler ############\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n\n    return X, y\n\nX, y = hitters_data_prep(df)\n","ae891612":"# Feature Engineering\nNew variables were created with the most appropriate variables according to their importance","396d3c44":"# Dataset Story\n* This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n* This is part of the data that was used in the 1988 ASA Graphics Section Poster Session.\n* The salary data were originally from Sports Illustrated,from The 1987 Baseball Encyclopedia Update published by\n* Collier Books, Macmillan Publishing Company, New York.","eec7cca3":"**Thank you for taking the time.**","f5b699e7":"# Variables\n* A data frame with 322 observations of major league players on the following 20 variables.\n* AtBat: Number of times at bat in 1986\n* Hits: Number of hits in 1986\n* HmRun: Number of home runs in 1986\n* Runs: Number of runs in 1986\n* RBI: Number of runs batted in in 1986\n* Walks: Number of walks in 1986\n* Years: Number of years in the major leagues\n* CAtBat: Number of times at bat during his career\n* CHits: Number of hits during his career\n* CHmRun: Number of home runs during his career\n* CRuns: Number of runs during his career\n* CRBI: Number of runs batted in during his career\n* CWalks: Number of walks during his career\n* League: A factor with levels A and N indicating player's league at the end of 1986\n* Division: A factor with levels E and W indicating player's division at the end of 1986\n* PutOuts: Number of put outs in 1986\n* Assists: Number of assists in 1986\n* Errors: Number of errors in 1986\n* Salary: 1987 annual salary on opening day in thousands of dollars\n* NewLeague: A factor with levels A and N indicating player's league at the beginning of 1987","a8cbf932":"# Functionalization","33fe2f08":"# Data Understanding","ea55bf7b":"# Automated Hyperparameter Optimization","9684d008":"# Base Models","9670dff7":"# Business Problem \nEstimating the salaries of players whose properties are included in the dataset\n","9c1239a9":"# MODELS\n## Feature Importance","579428c6":"# Data Preprocessing","6ca488a3":"# REPORTING\n\n**1) Hitters Data Set was read**\n\n**2) With Exploratory Data Analysis:**\n* Structural information of the dataset was checked.There are 322 observation and 20 variables.\n* Types of variables in data set were examined.There are missing values in Salary columns.\n* The size information of the data set has been accessed.\n* Correlation of variables in dataset was examined.We can make observations such as, as AtBat have high correlation with Runs\n* The number of missing observations from which variable in the data set was accessed. It was observed that there were 59 missing observations only in \"Salary\" which was dependent variable.Plot shows there are around 140 count for Player's of League A and around 125 for League\n* Descriptive statistics of the data set were examined. When I examine the dataset, we see that there is a difference between the mean and median values of the Assist variable.\n\n**3) In Data PreProcessing:**\n* New variables were created with the most appropriate variables according to their proportions\n* label encoding of categorical features (League, Division, NewLeague) with two class\n* We have removed the salary values that are higher than the up limit.\n\n**4) During the Model Building phase:**\n* Using the Linear, Ridge, Lasso, ElasticNet, LightGBM, XGBoost, RandomForestRegressor machine learning models, RMSE values representing the difference between actual values and predicted values were calculated. Later, hyperparameter optimizations were applied for Ridge, Lasso and ElasticNet to further reduce the error value.\n\n**Conclusion**\n* When the model created as a result of Elastic Net Hyperparameter optimization was applied to the  Data Frame, the lowest RMSE was obtained. (153.5421)","8422c4c6":"# Stacking & Ensemble Learning","dbb4274f":"# ENCODING"}}