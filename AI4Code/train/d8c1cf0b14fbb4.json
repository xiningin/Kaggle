{"cell_type":{"9fc5d042":"code","39501658":"code","c4c474ab":"code","3ca92983":"code","4b45567d":"code","9ab47579":"code","af63d439":"code","c045d936":"code","5744acf0":"code","33c6b8b9":"code","a6ffdaff":"code","1cd81e37":"code","3c960ffa":"code","b91e4720":"code","e747b444":"code","900b4435":"code","cae8d738":"code","ffcbf02e":"code","0130393a":"code","aca586d4":"code","c81dcdef":"code","25378dfd":"markdown","f011c529":"markdown","e628156c":"markdown","32530443":"markdown"},"source":{"9fc5d042":"import xgboost as xgb\nimport numpy as np\nimport pandas as pd\nimport optuna\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","39501658":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nsample = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","c4c474ab":"train.head()","3ca92983":"data = pd.concat([train, test], sort=False)","4b45567d":"data['Sex'].replace(['male','female'], [0, 1], inplace=True)\ndata['Embarked'].fillna(('S'), inplace=True)\ndata['Embarked'] = data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ndata['Fare'].fillna(np.mean(data['Fare']), inplace=True)\ndata['fare_value']=data['Fare']\/50\nage_avg = data['Age'].mean()\nage_std = data['Age'].std()\ndata['Age'].fillna(np.random.randint(age_avg - age_std, age_avg + age_std), inplace=True)\ndata['age_value']=data['Age']\/50\ndata['family'] = (data['SibSp'] + data['Parch'])\/5 \ndata['isAlone'] = 0\ndata.loc[data['family'] > 0, 'isAlone'] = 1\ndelete_columns = ['Name','PassengerId','SibSp','Parch','Ticket','Cabin','Age','Fare']\ndata.drop(delete_columns, axis=1, inplace=True)","9ab47579":"train = data[:len(train)]\ntest = data[len(train):]","af63d439":"target = train['Survived'].astype(int)\ndata = train.drop('Survived',axis=1)\ntest = test.drop('Survived',axis=1)","c045d936":"columns=data.columns.to_list()\ncolumns","5744acf0":"def objective(trial,data=data,target=target):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2,random_state=42)\n    param = {\n        'objective': trial.suggest_categorical('objective',['rank:pairwise','binary:hinge','reg:logistic']), \n        'tree_method': trial.suggest_categorical('tree_method',['gpu_hist']),  # using the GPU\n        'lambda': trial.suggest_loguniform('lambda',1e-3,10.0),\n        'alpha': trial.suggest_loguniform('alpha',1e-3,10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.01,0.012,0.014,0.016,0.018,0.02]),\n        'n_estimators': trial.suggest_categorical('n_estimators', [1000,2000,4000,8000]),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24,48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1,300),\n        'use_label_encoder': trial.suggest_categorical('use_label_encoder',[False])\n    }\n    model = xgb.XGBClassifier(**param)      ###\n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    preds = model.predict(test_x)\n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","33c6b8b9":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","a6ffdaff":"study.trials_dataframe()","1cd81e37":"# shows the scores from all trials\noptuna.visualization.plot_optimization_history(study)","3c960ffa":"# interactively visualizes the hyperparameters and scores\noptuna.visualization.plot_parallel_coordinate(study)","b91e4720":"# shows the evolution of the search\noptuna.visualization.plot_slice(study)","e747b444":"# parameter interactions on an interactive chart.\noptuna.visualization.plot_contour(study, params=['lambda','learning_rate'])","900b4435":"# Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","cae8d738":"# Visualize empirical distribution function\noptuna.visualization.plot_edf(study)","ffcbf02e":"Best_trial= study.best_trial.params\nprint(Best_trial)","0130393a":"preds = np.zeros(test.shape[0])\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\n\nfor trn_idx, test_idx in kf.split(train[columns],train['Survived']):\n    X_tr,X_val=train[columns].iloc[trn_idx],train[columns].iloc[test_idx]\n    y_tr,y_val=train['Survived'].iloc[trn_idx],train['Survived'].iloc[test_idx]\n    model = xgb.XGBClassifier(**Best_trial)    ###\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    preds+=model.predict(test[columns])\/kf.n_splits\n    rmse=mean_squared_error(y_val, model.predict(X_val),squared=False)\n    print(rmse)","aca586d4":"model","c81dcdef":"submit=sample\n#submit['Survived']=np.where(preds<0.5,0,1)\nsubmit['Survived']=preds.astype(int)\nsubmit.to_csv('submission.csv', index=False)\nsubmit","25378dfd":"#### from error message\n## Objective candidate for XGBClassifier\n* Objective candidate: survival:aft\n* Objective candidate: binary:hinge\n* Objective candidate: multi:softmax\n* Objective candidate: multi:softprob\n* Objective candidate: rank:pairwise\n* Objective candidate: rank:ndcg\n* Objective candidate: rank:map\n* Objective candidate: reg:squarederror\n* Objective candidate: reg:squaredlogerror\n* Objective candidate: reg:logistic\n* Objective candidate: reg:pseudohubererror\n* Objective candidate: binary:logistic\n* Objective candidate: binary:logitraw\n* Objective candidate: reg:linear\n* Objective candidate: count:poisson\n* Objective candidate: survival:cox\n* Objective candidate: reg:gamma\n* Objective candidate: reg:tweedie","f011c529":"# XGBoost with Optuna tuning\n* doc: \nhttps:\/\/github.com\/optuna\/optuna\n* thanks to: \nhttps:\/\/www.kaggle.com\/miklgr500\/optuna-xgbclassifier-parameters-optimize\n","e628156c":"By including the fixed parameters in the optuna search item, every parameter is included in study.best_trial.params, so you can simply write \u2018model = xgb.XGBClassifier (** Best_trial)\u2019 for the final learning.","32530443":"#### from error message\n* Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n* The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]."}}