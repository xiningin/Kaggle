{"cell_type":{"4bf20ded":"code","e409dc31":"code","693d218b":"code","51fc9306":"code","4506ebc7":"code","dd29cd9e":"code","16629009":"code","77e9a2bb":"code","9ab1954f":"code","2d8384b2":"markdown","9cfcad1f":"markdown","b374b169":"markdown","41e12624":"markdown","b3427e5a":"markdown","5074695c":"markdown","eb99ca56":"markdown","5b35ecbe":"markdown","d008579a":"markdown","68e40853":"markdown","20d3ec3a":"markdown","3f7ab073":"markdown","dc336469":"markdown","fe297e3b":"markdown","035ef911":"markdown","03a2fefa":"markdown","908710e3":"markdown","b4d35da4":"markdown","7bf3123e":"markdown"},"source":{"4bf20ded":"#Numpy is used so that we can deal with array's, which are necessary for any linear algebra\n# that takes place \"under-the-hood\" for any of these algorithms.\nimport numpy as np\n\n#Pandas is used so that we can create dataframes, which is particularly useful when\n# reading or writing from a CSV.\nimport pandas as pd\n\n#Matplotlib is used to generate graphs in just a few lines of code.\nimport matplotlib.pyplot as plt\n\n#Import the classes we need to test linear, ridge, and lasso to compare\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV\n\n#Need these for selecting the best model\nfrom sklearn.model_selection import cross_val_score\n\n#These will be our main evaluation metrics \nfrom sklearn.metrics import confusion_matrix\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\n\n# Will use this to encode our categorical data.\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n#import xgboost\nfrom xgboost import XGBClassifier","e409dc31":"#read the data from csv\ndataset = pd.read_csv('..\/input\/churn-modeling\/Churn_Modelling.csv')\n\n#take a look at our dataset.  head() gives the first 5 lines. \ndataset.head()","693d218b":"#Grab X, ignoring row number, customer ID, and surname\nX = dataset.iloc[:, 3:13].values\n\n#grab the output variable\ny = dataset.iloc[:, 13].values\n\n#take a look\nX[0:10]","51fc9306":"# Transform the Geograpy Column.\nct = ColumnTransformer([(\"Geography\", OneHotEncoder(), [2])], remainder = 'passthrough')\nX = ct.fit_transform(X)\n#These were tacked onto the front, so we need to exclude one to avoid the \"Dummy Variable Trap\"\nX = X[:, 1:]\n\n#repeat this for Gender\nct = ColumnTransformer([(\"Gender\", OneHotEncoder(), [2])], remainder = 'passthrough')\nX = ct.fit_transform(X)\nX = X[:, 1:]\n\n#take a look\nX[0,:]","4506ebc7":"#split the datasets, leaving 20% for testing.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","dd29cd9e":"#create an object of the XGBoost Class\nclassifier = XGBClassifier()\n#fit it to the data.\nclassifier.fit(X_train, y_train)","16629009":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)","77e9a2bb":"cm = confusion_matrix(y_test, y_pred)\ncm","9ab1954f":"accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"MEAN ACCURACY: {:.4f}\".format(accuracies.mean()))\nprint(\"ACCURACY STD: {:.2f}\".format(accuracies.std()))\n","2d8384b2":"> # Enough to be Dangeous: XGBoost \n\n> ### This is the 7th notebook of my **\"Enough to be Dangeous\"** notebook series\n\nSee the other notebooks here:\n\n[Simple Linear regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangeous-simple-linear-regression)\n\n[Multiple Linear Regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-multiple-linear-regression)\n\n[Polynomial Regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-polynomial-regression)\n\n[Decision Tree](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-decision-tree-regression)\n\n[Random Forest](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-random-forest-regression)\n\n[Lasso and Ridge Regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-lasso-and-ridge-regression)","9cfcad1f":"This continues until there is no more improvement, or a maximum number of iterations is reached.  \n\nOverall, XGBoost provides state-of-the-art performance in terms of speed and accuracy, making it a go-to algorithm in real-world applications. This is especially true for very large datasets, as XGBoost has some tricks which it uses to keep computation time down.","b374b169":"Next we will need to load our data.\n\nTo see the power of XGBoost, we need a more complex dataset, so I will use a dataset on customer retention that has many predictor variables. ","41e12624":"XGBoost begins with a default prediction and calculates the \u201cresiduals\u201d between the prediction and the actual values.\n\n![image.png](attachment:image.png)","b3427e5a":"> ## This notebook is separated into two parts:\n\n### **1) Conceptual Overview:**  I will introduce the topic in 200 words or less.\n\n### **2) Implementation:**  I will implement the algorithm in as few lines as possible.","5074695c":"We use the residuals to calculate \u201csimilarity score\u201d, which also introduces a \u201cregularization parameter\u201d to prevent overfitting.\n\n![image.png](attachment:image.png)","eb99ca56":"The remaining splits are used to calculate new predictions for each subset of the data, which are scaled by a learning rate, and added the previous predictions.\n\n![image.png](attachment:image.png)\n","5b35ecbe":"We split the data into a decision tree, and calculate the similarity scores for each and compare them to the root node to compute a \u201cgain score\u201d.\n\n![image.png](attachment:image.png)","d008579a":"To evaluate the results, we can get an idea with a confusion matrix first. \n\nThe diagonals tell us how much we got right and wrong, and the type of error.  ","68e40853":"Now we can apply this to predict the results. ","20d3ec3a":"We can see that we have some categorical and continuous variables here, and also a few columns we won't need.\n\nSo we will have to do some \"OneHotEncoding\" to make this data work for XGBoost.\n\nBut first, we'll grab the columns we want for our X a nd Y variables. ","3f7ab073":"We continue splitting the data until we reach a maximum tree depth, then apply \u201cpruning\u201d to remove any split that did not provide sufficient gain.\n\n![image.png](attachment:image.png)","dc336469":"With our categorical variables handled, we can now split our dataset into training and test sets. ","fe297e3b":"From this we can see that we had a mean accuracy of 85.29% with only a few lines of code! ","035ef911":"## Implementation\n\nIn this section I will implement the code in its **simplest verison** so that it is understandable if you are brand new to machine learning. \n\nBelow we will predict salary based on the current role someone is in, and then make some predictions on start ups.  I will also include side by side examples showing how it performs compared to decision trees. \n\nThe first step is to start with \"imports\".","03a2fefa":"This meanas that we got 1491 correct when the outcome was expected to be Did not Exit, and 218 correct when the outcome should have been \"Exited\".\n\nWe made 187 mistakes by predicting \"Exited\" when it should have been \"Did Not Exit\".\n\nWe made 104 mistakes by predicting \"Did not Exit\" when it should have been \"Exit\".\n\nWe can get a better idea of accuracy by using cross validation. ","908710e3":"Now we can create our model.","b4d35da4":"Now we need to handle our categorical variables.  These are basically the ones that are non numeric.  We have to translate them into a numeric value that the system can understand. ","7bf3123e":"## Conceptual Overview\n\nXGBoost is a super-charged algorithm built from decision trees.  Its power comes from hardware and algorithm optimizations which make it significantly faster and more accurate than other algorithms.\n\n![image.png](attachment:image.png)"}}