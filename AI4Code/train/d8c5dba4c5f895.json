{"cell_type":{"6a5d3d47":"code","2049382e":"code","499e798c":"code","2e740a5e":"code","fcc8183d":"code","c8ae22dc":"code","94019d40":"code","c545cff8":"code","d2a888eb":"code","f2cab010":"code","756087c2":"code","f1327467":"code","dda0b211":"code","2b4f28ea":"code","f4ee5307":"code","6ffd31d6":"code","c98228f2":"code","c86adadc":"code","f509e69e":"code","22c456ff":"code","a504d6e1":"code","7e14b87b":"code","f3320c0e":"code","6580858d":"code","727758bd":"code","668b4efc":"code","69b79d30":"code","0437f091":"code","e5c4b0b3":"code","d2138504":"code","ea6d7db5":"code","4ac7a02f":"code","3402b859":"markdown","861ca716":"markdown","35dbd906":"markdown","d6db6cb1":"markdown","03fb63be":"markdown","c0105b1a":"markdown","bba8b2a5":"markdown","f572271c":"markdown","653372b0":"markdown","f5784364":"markdown","13a6a2af":"markdown","24ea03e9":"markdown","b3a94c76":"markdown","035097cc":"markdown","88a2c35a":"markdown","e0ac11ea":"markdown","56079040":"markdown","007f50f7":"markdown","f7ffb24e":"markdown","1bcf44da":"markdown","16509b66":"markdown","3ab5893e":"markdown","3f023e0c":"markdown","9db2e6ea":"markdown","fa897390":"markdown","05810bb9":"markdown","fb58b2b8":"markdown","9ab1f259":"markdown","fdb93c8b":"markdown","b1384879":"markdown","b07f0d9c":"markdown"},"source":{"6a5d3d47":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nfrom sklearn.model_selection import train_test_split, ShuffleSplit, cross_val_score\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nPATH_TO_DATA = '..\/input\/'\n\n","2049382e":"#Importing initial training dataset with targets and test dataset\n\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_features.csv'), \n                                    index_col='match_id_hash')\ntrain_targets = pd.read_csv(os.path.join(PATH_TO_DATA, \n                                            'train_targets.csv'), \n                                   index_col='match_id_hash')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, \n                                            'test_features.csv'), \n                                   index_col='match_id_hash')\nprint(train_df.shape, train_targets.shape, test_df.shape)","499e798c":"train_targets.head()","2e740a5e":"%%time\ny_train = train_targets.radiant_win #extract the target variable\n\n#Now make a train-test split, we'll see that results on the holdout set correlate with CV results.\n\nX_train_part, X_valid, y_train_part, y_valid = train_test_split(train_df, y_train, test_size = 0.3, random_state=0) #fixing random_state\n\n#Settling a CV scheme.\ncv = ShuffleSplit(n_splits=5, random_state=1) #using a shuffle split for CV \n\n#Implement RF with just 100 estimators not to wait too long.\nrf = RandomForestClassifier(n_estimators=100, random_state=1)\nrf.fit(X_train_part, y_train_part)\n\n#Count CV scoring and houldout scoring: \nholdout_score = roc_auc_score(y_valid, rf.predict_proba(X_valid)[:,1])\ncv_score = cross_val_score(rf, train_df, y_train, cv=cv, scoring = 'roc_auc') \n","fcc8183d":"#Let's look at the results.\nprint('CV scores: ', cv_score)\nprint('CV mean: ', cv_score.mean())\nprint('Holdout score: ', holdout_score)","c8ae22dc":"train_df.head()","94019d40":"idx_split = train_df.shape[0]\nfull_df = pd.concat((train_df, test_df))\n\nprint(train_df.shape, test_df.shape, full_df.shape)","c545cff8":"cols = [] \nfor i in full_df.columns[5:29]: #list of columns for r1 player\n    if i[3:] != 'hero_id' and i[3:] != 'firstblood_claimed':\n        cols.append(i[3:]) #drop r1_\nprint(cols)","d2a888eb":"def substract_numeric_features (df, feature_suffixes):\n    col_names=[]\n    df_out = df.copy()\n    for feat_suff in feature_suffixes:\n        for index in range(1,6):\n            df_out[f'{index}_{feat_suff}_substract'] = df[f'r{index}_{feat_suff}'] - df[f'd{index}_{feat_suff}'] # e.g. r1_kills - d1_kills\n            col_names.append(f'd{index}_{feat_suff}')\n            col_names.append(f'r{index}_{feat_suff}')\n    df_out.drop(columns = col_names, inplace=True)\n    return df_out\n\n#Run the function\nfull_df_mod = substract_numeric_features(full_df, cols)\nfull_df_mod.head()","f2cab010":"def combine_sub_features (df_out, feature_suffixes):\n    for feat_suff in feature_suffixes:\n            player_col_names = [f'{i}_{feat_suff}_substract' for i in range(1,6)] # e.g. 1_gold_substract\n            \n            df_out[f'{feat_suff}_max_substract'] = df_out[player_col_names].max(axis=1) # e.g. gold_max_substract\n            \n            df_out[f'{feat_suff}_min_substract'] = df_out[player_col_names].min(axis=1) # e.g. gold_min_substract\n            \n            df_out[f'{feat_suff}_sum_substract'] = df_out[player_col_names].sum(axis=1) # e.g. gold_sum_substract\n\n            \n            df_out.drop(columns=player_col_names, inplace=True) # remove teammembers' substract features from the dataset\n    return df_out\n\n#Run the function. Suffixes remain the same\nfull_df_mod = combine_sub_features(full_df_mod, cols)\nfull_df_mod.head()","756087c2":"%%time\n#Remember we need to use only training part of the full set\nX_train_part_1, X_valid_1, y_train_part_1, y_valid_1 = train_test_split(full_df_mod[:idx_split], y_train, test_size = 0.3, random_state=0) #fixing random_state\n\nrf = RandomForestClassifier(n_estimators=100, random_state=1)\nrf.fit(X_train_part_1, y_train_part_1)\n\n#Count CV scoring and houldout scoring: \nholdout_score_1 = roc_auc_score(y_valid_1, rf.predict_proba(X_valid_1)[:,1])\ncv_score_1 = cross_val_score(rf, full_df_mod[:idx_split], y_train, cv=cv, scoring = 'roc_auc') ","f1327467":"#New results.\nprint('CV scores: ', cv_score_1)\nprint('CV mean: ', cv_score_1.mean())\nprint('CV std:', cv_score_1.std())\nprint('Holdout score: ', holdout_score_1)\nprint('Better results on CV: ', cv_score_1>cv_score)","dda0b211":"def herotype_approach(df):\n    r_heroes = ['r%s_hero_id' %i for i in range(1,6)] # e.g. r1_hero_id...\n    d_heroes = ['d%s_hero_id' %i for i in range(1,6)] # e.g. d1_hero_id...\n    r_herotypes = ['r%s_hero_type' %i for i in range(1,6)] # e.g. r1_hero_type...\n    d_herotypes = ['d%s_hero_type' %i for i in range(1,6)] # e.g. d1_hero_type...\n\n    df['r_hero_invar_sum'] = np.log(df[r_heroes]).sum(axis=1) #sum of logs of hero ids for the team r\n    df['d_hero_invar_sum'] = np.log(df[d_heroes]).sum(axis=1) #sum of logs of hero ids for the team d\n    df['hero_invar_sum_diff'] = df['r_hero_invar_sum'] - df['d_hero_invar_sum'] #their difference (don't try to find the meaning)\n    \n    df[r_herotypes] = df[r_heroes].apply(lambda x: (x\/\/40)+1) #hero types like 1,2,3 supposing there's about equal number of heroes of each type\n    df[d_herotypes] = df[d_heroes].apply(lambda x: (x\/\/40)+1)\n    \n    df['r_invar_herotype_sum'] = np.log(df[r_herotypes]).sum(axis=1).astype(str) # findning an invariant sum to treat as categorial\n    df['d_invar_herotype_sum'] = np.log(df[d_herotypes]).sum(axis=1).astype(str)\n    \n    return df\n\nfull_df_mod = herotype_approach(full_df_mod)","2b4f28ea":"def hero_approach(df):\n    for team in 'r', 'd':\n        players = [f'{team}{i}' for i in range(1, 6)]\n        hero_columns = [f'{player}_hero_id' for player in players]\n\n        d = pd.get_dummies(df[hero_columns[0]])\n        for c in hero_columns[1:]:\n            d += pd.get_dummies(df[c])\n        df = pd.concat([df, d.add_prefix(f'{team}_hero_')], axis=1)\n        df.drop(columns=hero_columns, inplace=True)\n    return df\n\nfull_df_mod = hero_approach(full_df_mod)","f4ee5307":"r_firstblood = ['r%s_firstblood_claimed' %i for i in range(1,6)] \nd_firstblood = ['d%s_firstblood_claimed' %i for i in range(1,6)] \nr_herotypes = ['r%s_hero_type' %i for i in range(1,6)]\nd_herotypes = ['d%s_hero_type' %i for i in range(1,6)]\n\nfull_df_dum = pd.get_dummies(full_df_mod, columns = ['r_invar_herotype_sum', 'd_invar_herotype_sum'] + r_firstblood + d_firstblood)\nfull_df_dum.head()","6ffd31d6":"%%time\n#Remember we need to use only training part of the full set\nX_train_part_2, X_valid_2, y_train_part_2, y_valid_2 = train_test_split(full_df_dum[:idx_split], y_train, test_size = 0.3, random_state=0) #fixing random_state\n\nrf = RandomForestClassifier(n_estimators=100, random_state=1)\nrf.fit(X_train_part_2, y_train_part_2)\n\n#Count CV scoring and houldout scoring: \nholdout_score_2 = roc_auc_score(y_valid_2, rf.predict_proba(X_valid_2)[:,1])\ncv_score_2 = cross_val_score(rf, full_df_dum[:idx_split], y_train, cv=cv, scoring = 'roc_auc') ","c98228f2":"#New results.\nprint('CV scores: ', cv_score_2)\nprint('CV mean: ', cv_score_2.mean())\nprint('CV std:', cv_score_2.std())\nprint('Holdout score: ', holdout_score_2)\nprint('Better results on CV: ', cv_score_2>cv_score_1)","c86adadc":"%%time\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nfull_df_scaled = scaler.fit_transform(full_df_dum) #scalling the full dataset, that's not correct but it saves time\n\nX_train_part_lr, X_valid_lr, y_train_part_lr, y_valid_lr = train_test_split(full_df_scaled[:idx_split], \n                                                                        y_train, test_size = 0.3, random_state=0) #fixing random_state\nlr = LogisticRegression(random_state=0, solver='liblinear')\nlr.fit(X_train_part_lr, y_train_part_lr)\n\nlr_ho_score =  roc_auc_score(y_valid_lr, lr.predict_proba(X_valid_lr)[:,1])\nlr_cv_score = cross_val_score(lr, full_df_scaled[:idx_split], y_train, cv=cv, scoring = 'roc_auc') \n\ndel full_df_dum","f509e69e":"#Logistic regression results.\nprint('CV scores LR: ', lr_cv_score)\nprint('CV mean LR: ', lr_cv_score.mean())\nprint('CV std LR:', lr_cv_score.std())\nprint('Holdout score LR: ', lr_ho_score)","22c456ff":"rf_cv_score, rf_ho_score = cv_score_2, holdout_score_2 \n\nprint('CV scores RF: ', rf_cv_score)\nprint('CV mean RF: ', rf_cv_score.mean())\nprint('CV std RF:', rf_cv_score.std())\nprint('Holdout score RF: ', rf_ho_score)\n","a504d6e1":"%%time\nfrom catboost import CatBoostClassifier\n#We'll use full_df_mod without dummies and mark categorial vars\nX_train_part_ctb, X_valid_ctb, y_train_part_ctb, y_valid_ctb = train_test_split(full_df_mod[:idx_split], \n                                                                        y_train, test_size = 0.3, random_state=0) #fixing random_state\ncat_vars = ['r_invar_herotype_sum', 'd_invar_herotype_sum'] + r_firstblood + d_firstblood #all the vars that we got dummies of\n\n#Let it train for 200 iterations not to wait too long\nctb = CatBoostClassifier(iterations = 200, random_state=1, verbose=False, task_type='GPU', eval_metric='AUC', cat_features=cat_vars)\n\n#We'll look at an online validation plot\nctb.fit(X_train_part_ctb, y_train_part_ctb.astype(float), eval_set=(X_valid_ctb, y_valid_ctb.astype(float)), plot=True)\n\nctb_ho_score =  roc_auc_score(y_valid_ctb.astype(float), ctb.predict_proba(X_valid_ctb)[:,1])\nctb_cv_score = cross_val_score(ctb, full_df_mod[:idx_split], y_train.astype(float), cv=cv, scoring = 'roc_auc') \n","7e14b87b":"print('CV scores CTB: ', ctb_cv_score)\nprint('CV mean CTB: ', ctb_cv_score.mean())\nprint('CV std CTB:', ctb_cv_score.std())\nprint('Holdout score CTB: ', ctb_ho_score)","f3320c0e":"#!pip install keras\n#!pip install tensorflow","6580858d":"from keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport tensorflow as tf\n\nfrom keras import backend as K\nfrom keras import regularizers\nfrom keras import optimizers\n\n#Defining ROC AUC in Keras for evaluation.\ndef auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc","727758bd":"# to find a number of input dimensions\nfull_df_scaled.shape","668b4efc":"def model_function():\n    model = Sequential()\n    model.add(Dense(50, input_dim = 396, kernel_initializer='normal', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n    return model","69b79d30":"keras_net = KerasClassifier(build_fn=model_function, epochs=10, batch_size=32, verbose=1) #it's an sklearn wrapper for model\nkeras_net.fit(X_train_part_lr, y_train_part_lr.astype(float)) #we'll use scaled training part like for LR\nnn_ho_score =  roc_auc_score(y_valid_lr.astype(float), keras_net.predict_proba(X_valid_lr)[:,1])","0437f091":"keras_net = KerasClassifier(build_fn=model_function, epochs=10, batch_size=32, verbose=False) #turn off the verbose\n\nnn_cv_score = cross_val_score(keras_net, full_df_scaled[:idx_split], y_train.astype(float), cv=cv, scoring = 'roc_auc') ","e5c4b0b3":"print('CV scores nn: ', nn_cv_score)\nprint('CV mean nn: ', nn_cv_score.mean())\nprint('CV std nn:', nn_cv_score.std())\nprint('Holdout score nn: ', nn_ho_score)","d2138504":"import json \n\n#Collect needed columns names\nwith open(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')) as fin:\n        for i in range(150):\n            first_line = fin.readline()\n            data_json = json.loads(first_line)\ndata_json.keys()\nkey = []\nfor i in data_json['players'][9].keys():\n    if i not in cols: #remember we've settled columns from full dataset\n        key.append(i)","ea6d7db5":"import collections\nfrom tqdm import tqdm_notebook\ndef read_matches(matches_file):\n    \n    MATCHES_COUNT = {\n        'test_matches.jsonl': 10000,\n        'train_matches.jsonl': 39675,\n    }\n    _, filename = os.path.split(matches_file)\n    total_matches = MATCHES_COUNT.get(filename)\n    \n    with open(matches_file) as fin:\n        for line in tqdm_notebook(fin, total=total_matches):\n            yield json.loads(line)\n\n#Extracting function\n\ndef extract_features_csv(match, keys):\n    row = [\n        ('match_id_hash', match['match_id_hash']),\n    ]\n        \n    for slot, player in enumerate(match['players']):\n        if slot < 5:\n            player_name = 'r%d' % (slot + 1)\n        else:\n            player_name = 'd%d' % (slot - 4)\n# The main idea: if we have int or float or bool - return it, else - return the length of the item\n        for field in keys:\n            if (type(player[field]) == int) or (type(player[field]) == float) or (type(player[field]) == bool): \n                column_name = '%s_%s' % (player_name, field)\n                row.append((column_name, player[field]))\n            else:\n                column_name = '%s_%s' % (player_name, field)\n                row.append((column_name, len(player[field])))\n    return collections.OrderedDict(row)","4ac7a02f":"\n#df_new_features = []\n#for match in read_matches(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')):\n#    match_id_hash = match['match_id_hash']\n#    features = extract_features_csv(match, key)\n\n#    df_new_features.append(features)\n#df_new_features = pd.DataFrame.from_records(df_new_features).set_index('match_id_hash')\n#df_new_features.head()","3402b859":"While combimimg features we might choose almost any aggregating operation that we think might be useful.","861ca716":"We see great number of features probably divided by teammembers and they don't really mean anything to me (as fat as I'm a sports simulators fan). Let's try to find some description on the internet. \nWe can [find](https:\/\/www.quora.com\/What-is-DOTA-What-are-its-rules) that this is a team game where they have to tear apart some buildings or something like that.\n\nSo let's take such an approach: find the difference between each teammember's qualities and combine them to team ones as in [this](https:\/\/www.kaggle.com\/daemonis\/combine-hero-features-into-team-ones-basic) beautiful kernel.\n\nBeware, we won't combine *hero ids* (for now) and *firstblood* claimed columns because the first ones are definitely categorial and the second ones are binary.\n\nAlso let's concatenate training and test set, it won't spoil the thing greatly but it'll be more convenient to work.","35dbd906":"Display the head of the training targets.","d6db6cb1":" # Dota 2 Winner Prediction: Tips&Tricks, Approaches, Features, Models and etc.\n ![](https:\/\/cdn-st1.rtr-vesti.ru\/vh\/pictures\/hdr\/164\/936\/0.jpg)\n \n \n In this kernel we will calmly walk through some of the approaches and ideas that maybe useful for the competition. Don't even need to fasten your seatbelts, it's okay :)\n \n **We'll cover the following:**\n - Base Estimator Setting\n - Teammembers Substraction And Combining Approach\n - Approach To The Categorial Vars\n - Model Comparison (Logistic Regression, Random Forest, CatBoost Classifier, FF NN with Keras)\n - *Special Gift*","03fb63be":"I won't perform it here just to save the time and kernel memory. But you can use it as a template for further investigations.","c0105b1a":"Now let's see what we've got.","bba8b2a5":"# Please, be fair and feel free to upvote this kernel if you've found it somehow useful :)\n![](http:\/\/critterbabies.com\/wp-content\/gallery\/kittens\/803864926_1375572583.jpg)","f572271c":"We use 1 hidden layer with 50 neurons and RELU, also with use Batch Normalization layer and output layer has 1 neuron with a sigmoid.","653372b0":"Since now, I suppose, you have all necessary power :) \n\nYou're welcome!\n\nNow come and research... and research :)\n\n**P.S. I'm not claiming that everything here is a silver bullet (or at least something), I've just shared my own ideas and will be happy to discuss any possible issues in comments.**\n\n**P.S.S. I'd pleased to get teaming up suggestions. I'm located in Minsk, Belarus, but distant cooperation is not a problem, I suppose :) **\n\n\n\n\nSincerely yours\n\n**slack: Vlad Kisin**\n","f5784364":"**3. CatBoost Classifier**\n\nIt's famous for working well out-of-box and also can use GPU and can run much faster.\n\nAlso it provides unique algos to work with categorial vars (did you know that 'Cat' in CatBoost refers to Categorial? :) )","13a6a2af":"As you see it's hard to decide which model with parameter tuning will get the best result. 'No Free Lunch' in action. And once again it's up to you to decide which way to move. Still don't think that NNs are bad for this task. We didn't tune anything and learned just a little.","24ea03e9":"I'll show a way how to improve Yorko's approach (function) and extract players' data from raw json.","b3a94c76":"# Special Gift\n\n**Raw JSON extraction**","035097cc":"**4. FF NN witn Keras**\n\nLet's implement a simple FF NN with the help of Keras.","88a2c35a":"We'll also use an approach from [this](https:\/\/www.kaggle.com\/utapyngo\/dota-2-how-to-make-use-of-hero-ids) wonderful kernel, I really like that idea!","e0ac11ea":"Now let's build a simple model with just one hidden layer.","56079040":"Getting dummies to a separate dataframe so that we can further use raw *full_df_mod* for CatBoost.","007f50f7":"[](http:\/\/)From now let's consider hero ids. You can check that there are 120 unique values (1-120). I've looked at a number of DOTA 2 heroes [pictures](https:\/\/cyberpowerpc.files.wordpress.com\/2016\/03\/dota2-heroes-view-on-pc-gaming-console.png?w=700&h=391) and found that there're generally 3 categories of heroes with almost the same number of heroes in them.\n\nSo the next approach is totally made out of a hope.\n\nFor heroes we find their hero types (hoping there's something in it). Then we also make and kind of an invariant sum for the team (using logarithm). These features will be treated as categorial and we can just get dummies.\n\nThe same is done for hero ids but we gonna treat them as numerical because there're too many combinations (I understand that there is no meaning but still).","f7ffb24e":"Extracting needed columns names.","1bcf44da":"Alright. I've described a number of approaches and some of them sometimes seem to be misunderstading or illogical but it's up to you evaluate them, decide and move forward!\n\nWhat can be useful:\n- Finding different approaches to numerical features\n- Feature selection\n- Chosing the right way to treat categorial vars\n- Researching... and then again researching :)","16509b66":"To start accounting from somewhere we need to settle a base estimator's score for the raw data we have. We'll use a Random Forest model without tuning, just to see, whether our approaches are good, bad or maybe require further analysis.","3ab5893e":"**2. Random Forest**\n\nRecall our RF results.","3f023e0c":"Loading the data that we have from organizers.\n\nP.S. Many data scientists on the internet claim checking datasets' shape to be a good habit, I also like it, so sorry if it makes you furious.","9db2e6ea":"# Approach To The Categorial Vars","fa897390":" # Teammembers Substraction And Combining Approach","05810bb9":"**1. Logistic Regression**\n\nTo implement a Logistic Regression model we just need to scale the data and here we go. We won't tune the models, we'll evaluate them out-of-box.","fb58b2b8":"Wonderful! We've made less features to compute, got better results and actually we haven't even dealed with categorial and binary features! (Once again we see a correlation between CV and holdout scores :)) ","9ab1f259":"At the very first step we need to import all necessary frameworks.","fdb93c8b":"And finally let's see what we've got.","b1384879":"P.S. Notice that results of CV and holdout are really close :)\n\nAnd now let's look at the training set to understand the way to move.","b07f0d9c":"# Model Comparison\n\nHere we'll compare some models' perfomances on the dataset we've created. \nAs it's a binary classification task we'll use some familiar algos like Logistic Regression and Random Forest (we've actually already used), also we'll try boosting with CatBoost (LightGBM is a nightmare for me after Flight Delays competition :D) and implement a FF Neural Net with Keras.\n\n**Models to compare:**\n- Logistic Regression\n- Random Forest\n- CatBoost\n- FF NN with Keras"}}