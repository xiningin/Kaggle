{"cell_type":{"65fcd60c":"code","c6eccdeb":"code","cedd69a5":"code","f37408bb":"code","9c8c9ab5":"code","9488868d":"code","0fe4204c":"code","4c1ebada":"code","2c793077":"code","f437dacd":"code","a30b50ab":"code","3d3ea718":"code","1a800e8a":"code","8250ffe8":"code","fc8385ab":"code","a13c2d4d":"code","d7fdba01":"code","fd37e2c6":"code","106d2111":"code","e4646ca7":"code","edf90967":"code","364292e3":"code","b539c9cf":"code","79461adf":"code","9f50b39c":"code","fa261543":"code","f769ed4b":"code","0befab40":"code","c607cb25":"code","c421b03e":"code","aa945730":"markdown","e5e7a15a":"markdown","0db05033":"markdown"},"source":{"65fcd60c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt; \n# Importing sklearn libraries\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport pickle\n# Importing Keras libraries\nfrom keras.models import Sequential\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom keras import optimizers, losses, activations, models\nfrom keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Concatenate\nfrom keras import applications\nfrom keras.applications import imagenet_utils\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\n \nimport warnings\nwarnings.filterwarnings('ignore')\nimport cv2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c6eccdeb":"%cd \/kaggle\/working\/\n!ls","cedd69a5":"!wget https:\/\/www.dropbox.com\/s\/sh5yt160xzqjkk0\/Food-11.zip?dl=1","f37408bb":"!mv Food-11.zip?dl=1 Food_11.zip","9c8c9ab5":"!unzip Food_11.zip","9488868d":"!ls","0fe4204c":"!rm -rf Food_11.zip","4c1ebada":"train = [os.path.join(\"training\",img) for img in os.listdir(\"training\")]\nval = [os.path.join(\"validation\",img) for img in os.listdir(\"validation\")]\ntest = [os.path.join(\"evaluation\",img) for img in os.listdir(\"evaluation\")]\nlen(train),len(val),len(test)","2c793077":"train_y = np.array([int(img.split(\"\/\")[-1].split(\"_\")[0]) for img in train])\nval_y = np.array([int(img.split(\"\/\")[-1].split(\"_\")[0]) for img in val])\ntest_y = [int(img.split(\"\/\")[-1].split(\"_\")[0]) for img in test]\nnum_classes = 11\n# Convert class labels in one hot encoded vector\ny_train = []\nfor x in train_y:\n    a = np.array([0]*num_classes)\n    a[x] = 1\n    y_train.append(a)\ny_val = []\nfor x in val_y:\n    a = np.array([0]*num_classes)\n    a[x] = 1\n    y_val.append(a)\ny_test = []\nfor x in test_y:\n    a = np.array([0]*num_classes)\n    a[x] = 1\n    y_test.append(a)\n    \n#len(y_train),len(y_val),len(y_test)\ny_train = np.array(y_train)\ny_val = np.array(y_val)\ny_test = np.array(y_test)\ny_train.shape,y_val.shape,y_test.shape","f437dacd":"# y_train = []\n# y_val = []\n# y_test = []\n# train_y = []\n# val_y = []\n# test_y = []\n# train = []\n# val = []\n# test = []","a30b50ab":"import pickle\n# with open(\"test_op.pkl\",\"wb\") as file:\n#     pickle.dump(y_test,file)\n# with open(\"train_op.pkl\",\"wb\") as file:\n#     pickle.dump(y_train,file)\n# with open(\"val_op.pkl\",\"wb\") as file:\n#     pickle.dump(y_val,file)","3d3ea718":"print(\"Reading train images..\")\nX_train = np.array([cv2.resize(cv2.imread(x), dsize=(224,224), interpolation=cv2.INTER_AREA) for x in train])\nprint(\"Done.\")\nX_train.shape","1a800e8a":"print(\"Reading val images..\")\n# outs = []\n# X_train = []\nX_val = np.array([cv2.resize(cv2.imread(x), dsize=(224,224), interpolation = cv2.INTER_AREA) for x in val])\nprint(\"Done.\")\nX_val.shape","8250ffe8":"ROWS = 224\nCOLS = 224\nnclass = 11\nprint(X_train.shape,y_train.shape)\nprint(X_val.shape,y_val.shape)\ncheckpointer = ModelCheckpoint(filepath='transfermodel_best.hdf5',\n                               verbose=1,save_best_only=True)\nbase_model = applications.InceptionV3(weights='imagenet', \n                                include_top=False, \n                                input_shape=(ROWS, COLS,3))\nbase_model.trainable = True\nadd_model = Sequential()\nadd_model.add(base_model)\nadd_model.add(GlobalAveragePooling2D())\nadd_model.add(Dropout(0.25))\nadd_model.add(Dense(200, \n                    activation='relu'))\nadd_model.add(Dense(nclass, \n                    activation='softmax'))\nmodel = add_model\nmodel.compile(loss='categorical_crossentropy', \n              optimizer='adam',\n              metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size=64, epochs=30,\n          validation_data=(X_val, y_val), callbacks=[checkpointer],\n          verbose=1, shuffle=True)","fc8385ab":"def plot_acc_loss(history):\n    fig = plt.figure(figsize=(10,5))\n    plt.subplot(1, 2, 1)\n#     plt.plot(history.history['acc'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n \n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper right')\n    plt.show()\n \nplot_acc_loss(history)\n","a13c2d4d":"model.load_weights(\"transfermodel_best.hdf5\")\npreds = np.argmax(model.predict(X_val), axis=1)\nprint(\"\\nAccuracy of Transfer model (softmax) on validation Data: \", accuracy_score(val_y, preds))\nprint(\"\\nNumber of correctly identified imgaes: \",\n      accuracy_score(val_y, preds, normalize=False),\"\\n\")\nconfusion_matrix(val_y, preds, labels=range(0,11))","d7fdba01":"print(\"Reading test images..\")\nX_test = np.array([cv2.resize(cv2.imread(x), dsize=(224,224), interpolation = cv2.INTER_AREA) for x in test])\nprint(\"Done.\")","fd37e2c6":"model.summary()","106d2111":"intermediate_layer_model = Model(inputs=model.input,\n                                 outputs=model.get_layer(\"dense_1\").output)\ntrain_features = intermediate_layer_model.predict(np.array(X_train))\n# X_train = []\nval_features = intermediate_layer_model.predict(np.array(X_val))\n# X_val = []\ntest_features = intermediate_layer_model.predict(np.array(X_test))\n# X_test = []","e4646ca7":"train_features.shape, val_features.shape, test_features.shape","edf90967":"from sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n# clf = RandomForestClassifier(max_depth=28,n_estimators=150,random_state=0) --> 78.83 val acc\n# clf = RandomForestClassifier(max_depth=28,n_estimators=150,random_state=170) # --> 78.65\n# clf = RandomForestClassifier(max_depth=28,n_estimators=150,random_state=180) # --> 78.74\n# clf = RandomForestClassifier(max_depth=28,n_estimators=150,random_state=190) # --> 78.77\n# clf = RandomForestClassifier(max_depth=28,n_estimators=150,random_state=200) # --> 78.86\n# clf = RandomForestClassifier(max_depth=28,n_estimators=150,random_state=210) # --> 78.54\n# clf = RandomForestClassifier(max_depth=28,n_estimators=150,random_state=220) # --> 78.51\n# clf = RandomForestClassifier(max_depth=28,n_estimators=150,random_state=230) # --> 78.89\n# clf = RandomForestClassifier(max_depth=28,n_estimators=200,random_state=230) # --> 78.92\n# clf = RandomForestClassifier(max_depth=28,n_estimators=220,random_state=230) # --> 78.95\n# clf = RandomForestClassifier(max_depth=30,n_estimators=220,random_state=230) # --> 78.97\n# clf = RandomForestClassifier(max_depth=45,n_estimators=220,random_state=230) # --> 79.06\nclf = RandomForestClassifier(max_depth=45,n_estimators=220,random_state=230)\nclf.fit(train_features,np.array(train_y))","364292e3":"from sklearn.svm import SVC\nsvc = SVC(kernel='rbf',gamma='scale',decision_function_shape='ovo',probability=True)\nsvc.fit(train_features,np.array(train_y))","b539c9cf":"SVM_val_outputs = svc.predict(val_features)\nSVM_test_outputs = svc.predict(test_features)\nSVM_val_outputs.shape, SVM_test_outputs.shape\nprint(\"SVM accuracies:\")\nprint(\"val:\",accuracy_score(val_y,SVM_val_outputs))\nprint(\"test:\",accuracy_score(test_y,SVM_test_outputs))","79461adf":"RF_val_outputs = clf.predict(val_features)\nRF_test_outputs = clf.predict(test_features)\nRF_val_outputs.shape, RF_test_outputs.shape\nprint(\"RF accuracies:\")\nprint(\"val:\",accuracy_score(val_y,RF_val_outputs))\nprint(\"test:\",accuracy_score(test_y,RF_test_outputs))","9f50b39c":"model.load_weights(\"transfermodel_best.hdf5\")\ntm_val = model.predict(X_val)\ntm_test = model.predict(X_test)\npreds = np.argmax(tm_val, axis=1)\npreds2 = np.argmax(tm_test, axis=1)\nprint(\"Transfer Model Accuracies:\")\nprint(\"val:\",accuracy_score(val_y,preds))\nprint(\"test:\",accuracy_score(test_y,preds2))","fa261543":"SVM_val_outputs = svc.predict_proba(val_features)\nSVM_test_outputs = svc.predict_proba(test_features)\nSVM_val_outputs.shape, SVM_test_outputs.shape","f769ed4b":"RF_val_outputs = clf.predict_proba(val_features)\nRF_test_outputs = clf.predict_proba(test_features)\nRF_val_outputs.shape, RF_test_outputs.shape","0befab40":"# SVM accuracies:\n# val: 0.7137026239067056\n# test: 0.7367792052584404\n\n# RF accuracies:\n# val: 0.7160349854227406\n# test: 0.7511204063340304\n\n# Transfer Model Accuracies:\n# val: 0.5323615160349854\n# test: 0.5602031670152375","c607cb25":"w1 = 3.8; w2 = 4.3; w3 = 0# 79\nfinprobs = []\nfor i in range(3430):\n    p1 = SVM_val_outputs[i].argsort()[-5:][::-1]\n    p2 = RF_val_outputs[i].argsort()[-5:][::-1]\n    p3 = tm_val[i].argsort()[-5:][::-1]\n    p1_scores = sorted(SVM_val_outputs[i])[-5:][::-1]\n    p2_scores = sorted(RF_val_outputs[i])[-5:][::-1]\n    p3_scores = sorted(tm_val[i])[-5:][::-1]\n    probs = [0]*11\n    for k in range(5):\n        if p1[k]==p2[k] and p1[k] == p3[k]:\n            probs[p1[k]] += (w1*p1_scores[k]) + (w2*p2_scores[k]) + (w3*p3_scores[k])\n        elif p1[k]==p2[k]:\n            probs[p1[k]] += (w1*p1_scores[k]) + (w2*p2_scores[k])\n            probs[p3[k]] += (w3*p3_scores[k])\n        elif p2[k]==p3[k]:\n            probs[p2[k]] += (w2*p2_scores[k]) + (w3*p3_scores[k])\n            probs[p1[k]] += (w1*p1_scores[k])\n        elif p1[k]==p3[k]:\n            probs[p1[k]] += (w1*p1_scores[k]) + (w3*p3_scores[k])\n            probs[p2[k]] += (w2*p2_scores[k])\n        else:\n            probs[p1[k]] += (w1*p1_scores[k])\n            probs[p2[k]] += (w2*p2_scores[k])\n            probs[p3[k]] += (w3*p3_scores[k])\n    probs = np.array(probs).argsort()[-5:][::-1]\n    finprobs.append(probs[0])\n# print(\"ensembled!\",len(finprobs),len(val_y))\nprint(\"val:\",accuracy_score(val_y,finprobs))","c421b03e":"w1 = 2; w2 = 2; w3 = 1.05# 79\nfinprobs = []\nfor i in range(3347):\n    p1 = SVM_test_outputs[i].argsort()[-5:][::-1]\n    p2 = RF_test_outputs[i].argsort()[-5:][::-1]\n    p3 = tm_test[i].argsort()[-5:][::-1]\n    p1_scores = sorted(SVM_test_outputs[i])[-5:][::-1]\n    p2_scores = sorted(RF_test_outputs[i])[-5:][::-1]\n    p3_scores = sorted(tm_test[i])[-5:][::-1]\n    probs = [0]*11\n    for k in range(5):\n        if p1[k]==p2[k] and p1[k] == p3[k]:\n            probs[p1[k]] += (w1*p1_scores[k]) + (w2*p2_scores[k]) + (w3*p3_scores[k])\n        elif p1[k]==p2[k]:\n            probs[p1[k]] += (w1*p1_scores[k]) + (w2*p2_scores[k])\n            probs[p3[k]] += (w3*p3_scores[k])\n        elif p2[k]==p3[k]:\n            probs[p2[k]] += (w2*p2_scores[k]) + (w3*p3_scores[k])\n            probs[p1[k]] += (w1*p1_scores[k])\n        elif p1[k]==p3[k]:\n            probs[p1[k]] += (w1*p1_scores[k]) + (w3*p3_scores[k])\n            probs[p2[k]] += (w2*p2_scores[k])\n        else:\n            probs[p1[k]] += (w1*p1_scores[k])\n            probs[p2[k]] += (w2*p2_scores[k])\n            probs[p3[k]] += (w3*p3_scores[k])\n\n    probs = np.array(probs).argsort()[-5:][::-1]\n    finprobs.append(probs[0])\nprint(\"ensembled!\",len(finprobs),len(test_y))\nprint(\"test:\",accuracy_score(test_y,finprobs))","aa945730":"### Ensembled Validation outputs","e5e7a15a":"## Ensembled test outputs","0db05033":"## Accuracies\n### SVM val: 0.7880466472303207\n### RF val: 0.79067055393586\n### Transfer model val: 0.7740524781341108\n### Ensemble model val: 0.7909620991253644\nw1 = 2; w2 = 2; w3 = 1.05 --> test: 0.8096803107260233"}}