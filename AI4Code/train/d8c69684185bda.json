{"cell_type":{"883666e3":"code","d10c8c7a":"code","b52c8310":"code","300048f3":"code","38622f97":"code","8213fd82":"code","85985f34":"code","0f044a01":"code","6110e7c1":"code","3e4781da":"code","7b095820":"code","de27c097":"code","bae574c7":"code","c226bd6a":"code","80a69075":"code","e2b4e490":"code","4b45ccfd":"code","d39f7cae":"code","7431d03a":"code","247462d1":"code","ce1d4691":"code","9422dae8":"markdown","9eb00340":"markdown","728e573e":"markdown","cd764385":"markdown","0bb704bc":"markdown"},"source":{"883666e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d10c8c7a":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n","b52c8310":"data = pd.read_csv('\/kaggle\/input\/spam-mails-dataset\/spam_ham_dataset.csv')\ndata","300048f3":"print(\"\\n\"+'-'* 30)\nprint(\"shape of data:\",data.shape) \nprint(\"\\n\"+'-'* 30)\nprint(\"\\nno dimensions of data:\",data.ndim)\nprint(\"\\n\"+'-'* 30)\nprint(\"\\nsize of data:\",data.size)\nprint(\"\\n\"+'-'*30)\nprint(\"\\nSum fo all null values:\\n\",data.isnull().sum())\nprint(\"\\n\"+'-'*30)","38622f97":"display(\"Top 5 rows :\",data.head())\ndisplay(\"Last 5 Rows :\",data.tail())","8213fd82":"Vis = data['label_num'].value_counts()\nVis.plot(kind=\"bar\")\nplt.xticks(np.arange(2), ('Non spam', 'spam'),rotation=0)\nplt.show()","85985f34":"# label spam mail as 0;  ham mail as 1;\n\ndata.loc[data['label'] == 'spam', 'Category',] = 0\ndata.loc[data['label'] == 'ham', 'Category',] = 1","0f044a01":"# separating the data as texts and label\n\nX = data['text']\n\nY = data['label']","6110e7c1":"print(X)","3e4781da":"print(Y)","7b095820":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=3)","de27c097":"print(X.shape)\nprint(X_train.shape)\nprint(X_test.shape)","bae574c7":"# transform the text data to feature vectors that can be used as input to the Logistic regression\n\nfeature_extraction = TfidfVectorizer(min_df = 1, stop_words='english', lowercase='True')\n\nX_train_transformed = feature_extraction.fit_transform(X_train)\nX_test_transformed = feature_extraction.transform(X_test)\n\n","c226bd6a":"print(X_train)","80a69075":"print(X_train_transformed)","e2b4e490":"model = LogisticRegression()","4b45ccfd":"# training the Logistic Regression model with the training data\nmodel.fit(X_train_transformed, Y_train)","d39f7cae":"# prediction on training data\n\nprediction_on_training_data = model.predict(X_train_transformed)\naccuracy_on_training_data = accuracy_score(Y_train, prediction_on_training_data)","7431d03a":"print('Accuracy on training data : ', accuracy_on_training_data *100)","247462d1":"# prediction on test data\n\nprediction_on_test_data = model.predict(X_test_transformed)\naccuracy_on_test_data = accuracy_score(Y_test, prediction_on_test_data)","ce1d4691":"print('Accuracy on test data : ', accuracy_on_test_data*100)","9422dae8":"# ****Logistic Regression****","9eb00340":"# ****Importing Libraries and reading data****","728e573e":"# ****Feature Extraction****","cd764385":"# ****data Visualizations****","0bb704bc":"# ****Splitting the data into training data & test data****"}}