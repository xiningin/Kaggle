{"cell_type":{"0f94fbfe":"code","b641dcb7":"code","d9dddee6":"code","5d445476":"code","c0c77b71":"code","a9f9aefb":"code","bbf28ae2":"code","238f46a5":"code","63a5eaba":"code","9793adde":"code","17dd90d4":"code","937959af":"code","e38fabb7":"code","1d430db8":"code","818d3109":"code","131fb520":"code","2f66a7b3":"code","58a4b49f":"code","2a273f54":"code","12148523":"code","e42c26db":"code","0e6b38bb":"code","427cad4c":"code","617978ad":"code","0b62de4b":"code","82d227a6":"code","45079c44":"code","ea62fe77":"code","95189578":"code","40c6f31c":"code","72f556b5":"code","4b28e626":"code","231ab277":"code","70a5a836":"code","3d24e319":"code","058c48e0":"markdown","0f0d771d":"markdown","02fe5a93":"markdown","8b2a41b8":"markdown","9d308e5d":"markdown","816b75bc":"markdown","32aa4c79":"markdown","fbcd2ccb":"markdown","400a7721":"markdown","ba8f76da":"markdown","31415113":"markdown","49cc24ac":"markdown","a3f12c5d":"markdown","9aed6363":"markdown","bb2535b2":"markdown","c6bad171":"markdown","a4a4e3e4":"markdown","6cc5a1f4":"markdown","d413d11d":"markdown","e02e9332":"markdown","38dfe84a":"markdown","0a95fd33":"markdown","f1524eb3":"markdown","40e44950":"markdown","23e11b62":"markdown","3266c39b":"markdown","62c521d1":"markdown"},"source":{"0f94fbfe":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","b641dcb7":"import torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d9dddee6":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import DataLoader\n\n\nimport torchvision\nfrom torchvision import transforms\n\nfrom tqdm.notebook import tqdm","5d445476":"device = torch.device('cpu')\n# device = xm.xla_device()\nt1 = torch.randn(2, 2).to(device)\nt2 = torch.randn(2, 2).to(device)\nprint(t1+t2)","c0c77b71":"input = torch.randn(10,device=device)\nlinear = torch.nn.Linear(10, 20).to(device)\noutput = linear(input)\nprint(output)","a9f9aefb":"print(output.shape)","bbf28ae2":"transform = transforms.ToTensor()\n\ndataset = torchvision.datasets.MNIST(root='.\/data', train=True,\n                                        download=True, transform=transform)\n\ntrain_loader = DataLoader(dataset,batch_size=256)\n\nclass MNIST(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n\n    def forward(self, xb):\n        xb = xb.view(-1, 1, 28, 28)\n        xb = F.relu(self.conv1(xb))\n        xb = F.relu(self.conv2(xb))\n        xb = F.relu(self.conv3(xb))\n        xb = F.avg_pool2d(xb, 4)\n        return xb.view(-1, xb.size(1))\n\n    \n\n\ndef train_loop():\n    model = MNIST().train().to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n    for data, target in tqdm(train_loader,total=len(train_loader)):\n        optimizer.zero_grad()\n        data = data.to(device)\n        target = target.to(device)\n        output = model(data)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    print(f'Final Loss: {loss}')","238f46a5":"#train_loop()","63a5eaba":"import os\nos.environ['XLA_USE_BF16']=\"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\nimport torch\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom collections import OrderedDict, namedtuple\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport joblib\n\nimport logging\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule, XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaConfig\nimport sys\nfrom sklearn import metrics, model_selection","9793adde":"class AverageMeter:\n    \"\"\"\n    A helper class to compute and store the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","17dd90d4":"class ArrayDataset(torch.utils.data.Dataset):\n    def __init__(self,*arrays):\n        assert all(arrays[0].shape[0] == array.shape[0] for array in arrays)\n        self.arrays = arrays\n    \n    def __getitem__(self, index):\n        return tuple(torch.from_numpy(np.array(array[index])) for array in self.arrays)\n    \n    def __len__(self):\n        return self.arrays[0].shape[0]","937959af":"class CustomRoberta(nn.Module):\n    def __init__(self):\n        super(CustomRoberta, self).__init__()\n        self.num_labels = 1\n        self.hid_mix = 3\n        self.roberta = transformers.XLMRobertaModel.from_pretrained(\"xlm-roberta-large\", output_hidden_states=True, num_labels=1)\n        self.dropout = nn.Dropout(p=0.2)\n        self.feats = self.roberta.pooler.dense.out_features\n        self.classifier = nn.Linear(self.feats, self.num_labels)\n\n    def forward(self,\n                input_ids=None,\n                attention_mask=None,\n                position_ids=None,\n                head_mask=None,\n                inputs_embeds=None):\n\n        outputs = self.roberta(input_ids,\n                               attention_mask=attention_mask,\n                               position_ids=position_ids,\n                               head_mask=head_mask,\n                               inputs_embeds=inputs_embeds)\n\n        hidden_states = outputs[2]\n        hmix = []\n        for i in range(1, self.hid_mix + 1):\n            hmix.append(hidden_states[-i][:, 0].reshape((-1, 1, self.feats)))\n        hmix_tensor = torch.cat(hmix, 1)\n        mean_tensor = torch.mean(hmix_tensor, 1)\n        pool_tensor = self.dropout(mean_tensor)\n        return self.classifier(pool_tensor)","e38fabb7":"mx =CustomRoberta()","1d430db8":"tokenized_path = '..\/input\/xlm-r-large-tokenize-xhlulu-dataset\/'","818d3109":"x_train = np.load(tokenized_path+'x_train.npy',mmap_mode='r')\ntrain_toxic = np.load(tokenized_path+'df_train_toxic.npy',mmap_mode='r')\n\nx_valid = np.load(tokenized_path+'x_valid.npy',mmap_mode='r')\nvalid_toxic = np.load(tokenized_path+'df_valid_toxic.npy',mmap_mode='r')","131fb520":"x_train.shape, x_valid.shape","2f66a7b3":"train_dataset = ArrayDataset(x_train, train_toxic)\nvalid_dataset = ArrayDataset(x_valid, valid_toxic)","58a4b49f":"del x_train, x_valid\nimport gc;gc.collect()","2a273f54":"gc.collect()","12148523":"def loss_fn(outputs, targets):\n    # pass in outputs and targets, return loss function\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))","e42c26db":"def reduce_fn(vals):\n    # take average\n    return sum(vals) \/ len(vals)","0e6b38bb":"def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train() # put model in training mode\n    for bi, d in enumerate(data_loader): # enumerate through the dataloader\n\n        ids = d[0] # obtain the ids\n        targets = d[1] # obtain the targets\n\n        # put tensors onto desired device, in this case the TPU core\n        ids = ids.to(device, dtype=torch.long) \n        targets = targets.to(device, dtype=torch.float)\n\n        # pass ids to model\n        optimizer.zero_grad()\n        outputs = model(\n            input_ids=ids,\n        )\n        # calculate loss\n        loss = loss_fn(outputs, targets)\n        if bi % 50 == 0:\n            # since the loss is on all 8 cores, reduce the loss values and print the average (as defined in reduce_fn)\n            loss_reduced = xm.mesh_reduce('loss_reduce',loss,reduce_fn) \n            # master_print will only print once (not from all 8 cores)\n            xm.master_print(f'bi={bi}, loss={loss_reduced}')\n            \n        # backpropagate\n        loss.backward()\n        \n        # Use PyTorch XLA optimizer stepping\n        xm.optimizer_step(optimizer)\n        if scheduler is not None:\n            scheduler.step()\n        \n    model.eval() # put model in eval mode for later use\n    \ndef eval_loop_fn(data_loader, model, device):\n    fin_targets = []\n    fin_outputs = []\n    for bi, d in enumerate(data_loader): # enumerate through dataloader\n\n        ids = d[0] # obtain the ids\n        targets = d[1] # obtain the targets\n\n        # put tensors onto desired device, in this case the TPU core\n        ids = ids.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n\n        # pass ids to model\n        outputs = model(\n            input_ids=ids,\n        )\n\n        # Add the outputs and targets to a list \n        targets_np = targets.cpu().detach().numpy().tolist()\n        outputs_np = outputs.cpu().detach().numpy().tolist()\n        fin_targets.extend(targets_np)\n        fin_outputs.extend(outputs_np)    \n        del targets_np, outputs_np\n        gc.collect() # delete for memory conservation\n        \n    return fin_outputs, fin_targets","427cad4c":"def _run():\n    # Define training params \n    MAX_LEN = 192 # maximum text length in the batch (cannot have too high due to memory constraints)\n    BATCH_SIZE = 16 # batch size (cannot have too high due to memory constraints)\n    EPOCHS = 2 # number of epochs\n\n    # defining data samplers and loaders \n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_dataset,\n          num_replicas=xm.xrt_world_size(), # tell PyTorch how many devices (TPU cores) we are using for training\n          rank=xm.get_ordinal(), # tell PyTorch which device (core) we are on currently\n          shuffle=True)\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=0,\n    )\n        \n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n          valid_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=0\n    )\n    \n\n    device = xm.xla_device() # our device (single TPU core)\n    model = mx.to(device) # put model onto the TPU core\n    xm.master_print('done loading model')\n\n    param_optimizer = list(model.named_parameters()) # model parameters to optimize\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    # apply to weight decay\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    xm.master_print('training on train dataset')\n    \n    lr = 0.5e-5 * xm.xrt_world_size() # scale the learning rate\n    # calculate the total number of training steps\n    num_train_steps = int(len(train_dataset) \/ BATCH_SIZE \/ xm.xrt_world_size() * EPOCHS) \n    \n    optimizer = AdamW(optimizer_grouped_parameters, lr=lr) # define our optimizer\n    \n    # a scheduler can be used if desired\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n    xm.master_print(f'num_training_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n\n    # Let's start training on the train set!\n    for epoch in range(EPOCHS):\n        gc.collect() # I use a lot of gc.collect() statement to hopefully prevent OOM problems\n        # We use ParallelLoader (provided by PyTorch XLA) for TPU-core-specific dataloading:\n        para_loader = pl.ParallelLoader(train_data_loader, [device]) \n        xm.master_print('parallel loader created... training now')\n        gc.collect()\n        # call training loop:\n        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)\n        del para_loader\n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        gc.collect()\n        # call evaluation loop\n        o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n        del para_loader\n        gc.collect()\n        # report AUC at the end\n        auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n        auc_reduced = xm.mesh_reduce('auc_reduce',auc,reduce_fn)\n        xm.master_print(f'AUC = {auc_reduced}')\n        gc.collect()\n    \n    # We can also repeat the process on the validation set as demonstrated by @xhlulu\n    \n    xm.master_print('training on validation set')\n    \n    lr = 1.5e-5 * xm.xrt_world_size()\n    \n    num_train_steps = int(len(valid_dataset) \/ BATCH_SIZE \/ xm.xrt_world_size() * EPOCHS)\n    \n    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0.1*num_train_steps,\n        num_training_steps=num_train_steps\n    )\n    xm.master_print(f'num_training_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n\n\n    for epoch in range(EPOCHS):\n        gc.collect()\n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        xm.master_print('parallel loader created... training now')\n        gc.collect()\n        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)\n        del para_loader\n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        gc.collect()\n        o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n        del para_loader\n        gc.collect()\n        auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n        auc_reduced = xm.mesh_reduce('auc_reduce',auc,reduce_fn)\n        xm.master_print(f'AUC = {auc_reduced}')\n        gc.collect()\n        \n        \n    # save our model\n    xm.save(model.state_dict(), \"xlm_roberta_model.bin\")\n","617978ad":"import time\n\n# Start training processes\ndef _mp_fn(rank, flags):\n    a = _run()\n\nFLAGS={}\nstart_time = time.time()\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","0b62de4b":"print('Time taken for training: ',time.time()-start_time)","82d227a6":"gc.collect()","45079c44":"x_test = np.load(tokenized_path+'x_test.npy',mmap_mode='r')\ntest_ids = np.load(tokenized_path+'test_df_ids.npy',mmap_mode='r')","ea62fe77":"test_dataset = ArrayDataset(test_ids,x_test)","95189578":"trained_state_dict = torch.load(\"xlm_roberta_model.bin\")\nmx.load_state_dict(trained_state_dict)\ndel trained_state_dict\ngc.collect()","40c6f31c":"def gather_submissions():\n    # Given the saved results from each core, this function will consolidate them into a single submission.\n    ids = []\n    outputs = []\n    for core in range(8):\n        ids.extend(np.load(f'ids_core{core}.npy'))\n        outputs.extend(np.load(f'outputs_core{core}.npy'))\n    submission_df = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')\n    print(len(ids))\n    submission_df['toxic'].iloc[ids] = np.array(outputs)[:,0]\n    submission_df.to_csv('submission.csv',index=False)","72f556b5":"def test_loop_fn(data_loader, model, device):\n    \n    fin_id = []\n    fin_outputs = []\n    for bi, d in enumerate(data_loader): # enumerate through dataloader\n\n        text_id = d[0] # obtain the text id\n        ids = d[1] # obtain the tokenized ids\n\n        # put tensors onto desired device, in this case the TPU core\n        ids = ids.to(device, dtype=torch.long)\n\n        # pass ids to model\n        with torch.no_grad():\n            outputs = model(\n                input_ids=ids,\n            )\n\n        # Add the outputs and targets to a list \n        outputs_cpu = outputs.data.cpu()\n        fin_id.append(text_id)\n        fin_outputs.append(outputs_cpu)    \n        del text_id, ids, outputs_cpu # delete for memory conservation\n        gc.collect() \n    xm.master_print('done with inference, saving...')\n    fin_id_cat = torch.cat(fin_id)\n    fin_outputs_cat = torch.cat(fin_outputs)\n    np.save(f'ids_core{xm.get_ordinal()}.npy',fin_id_cat.cpu().numpy())\n    np.save(f'outputs_core{xm.get_ordinal()}.npy',fin_outputs_cat.numpy())    \n","4b28e626":"def _run_test():\n    BATCH_SIZE=64 # A higher learning rate can be used for inference\n    # defining data samplers and loaders \n    test_sampler = torch.utils.data.distributed.DistributedSampler(\n          test_dataset,\n          num_replicas=xm.xrt_world_size(), # tell PyTorch how many devices (TPU cores) we are using for training\n          rank=xm.get_ordinal(), # tell PyTorch which device (core) we are on currently\n          shuffle=True)\n\n    test_data_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=test_sampler,\n        drop_last=False,\n        num_workers=0,\n    )\n    device = xm.xla_device() # our device (single TPU core)\n    model = mx.to(device) # put model onto the TPU core\n    xm.master_print('done loading model')\n    \n    para_loader = pl.ParallelLoader(test_data_loader, [device])\n    test_loop_fn(para_loader.per_device_loader(device),model,device)\n","231ab277":"import time\nimport gc\n# Start inference processes\ndef _mp_fn(rank, flags):\n    a = _run_test()\n\nFLAGS={}\nstart_time = time.time()\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')\nprint('Time taken for inference: ',time.time()-start_time)\ngather_submissions()","70a5a836":"gc.collect()","3d24e319":"sub = pd.read_csv('submission.csv')\ntest = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\n\nsub.loc[test[\"lang\"] == \"es\", \"toxic\"] *= 1.06\nsub.loc[test[\"lang\"] == \"fr\", \"toxic\"] *= 1.04\nsub.loc[test[\"lang\"] == \"it\", \"toxic\"] *= 0.97\nsub.loc[test[\"lang\"] == \"pt\", \"toxic\"] *= 0.96\nsub.loc[test[\"lang\"] == \"tr\", \"toxic\"] *= 0.98\n\nsub.toxic -= sub.toxic.min()\nsub.toxic \/= sub.toxic.max()\nsub.toxic.hist(bins=100, log=True, alpha=0.6)\nsub.to_csv('submission.csv', index=False)","058c48e0":"In order to save memory, make sure to delete any unused variables:","0f0d771d":"We now define our training loop and evaluation loop functions. Much of the code is inspired by @abhishek's [kernel](https:\/\/www.kaggle.com\/abhishek\/bert-multi-lingual-tpu-training-8-cores). As you will see later, these functions are run on each of the 8 cores.\n\nTo get the loss of a batch, since the data is spread across the 8 cores, we have to _reduce_ the loss. Now the `loss_reduced` will be the same on all the cores (average of the 8 losses). `xm.master_print` can be used to print this value once from only one core.\n\nPyTorch XLA requires that the optimizer be stepped using their own function `xm.optimizer_step(optimizer)`.","02fe5a93":"Based on this property, it means we can **train models on a single XLA device**. In the case of TPUs, as we mentioned, there are multiple XLA devices (with caveats discussed later). But for now, we can train on a single core _by just changing the device_.\n\nHere is a typical training loop for a single epoch (using MNIST):","8b2a41b8":"# Running PyTorch model training on 8-core TPUs\n## First XLM-R kernel running on Kaggle 8-core TPUs\n","9d308e5d":"### Dataset and Model","816b75bc":"#### NOTE: Initialize model outside of training function \n\nLet's initialize our model. If you check the [code snipppet](https:\/\/pytorch.org\/xla\/master\/#running-on-multiple-xla-devices-with-multiprocessing) in the PyTorch XLA documentation, you will see that the recommended setup is to define the model in the function that is run on each of the 8 cores. But doing so will lead to high VM memory usage (since it needs to make 8 copies). Therefore, instead we define a single model and use them on the 8 cores. This is the setup used for low-memory VMs.","32aa4c79":"We finally define our main function (which itself calls the above functions) that will be spawned by PyTorch XLA multiprocessing. This function will be run on each of the 8 cores. There are several steps for 8-core training:\n1. We need to use a `DistributedSampler` that will appropriately distribute the dataset across the 8 cores.\n2. We are using `num_workers=0` as that decreases memory usage (only master process loading data). On higher memory VMs, this could be increased to speed up the training.\n3. The learning rate is scaled by the number of TPU cores (`xm.xrt_world_size()`)\n4. We put the model onto the TPU\n5. We use `ParallelLoader` which is a PyTorch XLA-specific DataLoader for loading data onto the TPU.\n6. We save the model at the end of training with `xm.model_save`","fbcd2ccb":"# XLA Tensors are like PyTorch Tensors\nThis section inspired by [this section](http:\/\/pytorch.org\/xla\/release\/1.5\/index.html#xla-tensors-are-pytorch-tensors) in the Docs.\n\nLet's explore the first idea that XLA tensors are like PyTorch tensors.\n\nBasically, `xm.xla_device()` is a PyTorch device in its own way and can be used as such. In the interactive session, please uncomment the `xm.xla_device()` if you want to experiment.\n","400a7721":"#### NOTE: Pre-tokenize the dataset\n\nThe trick also used in xhlulu's kernel is to pre-tokenize the dataset. This is done [over here](https:\/\/www.kaggle.com\/tanlikesmath\/xlm-r-large-tokenize-dataset). It uses the same `regular_encode` function defined in xhlulu's kernel. We now load it over here. \n\nAn additional trick used that may help memory usage is to load the dataset as a memory-mapped dataset (`mmap_mode='r'`). This way, the whole dataset isn't in the RAM at the same time.","ba8f76da":"### Training","31415113":"# PyTorch XLA Imports\n\nThe code cell below will install PyTorch XLA.","49cc24ac":"# Exploration of PyTorch XLA\n\nOkay, now we have set up PyTorch XLA to be used in this kernel. Let's explore how to use PyTorch XLA.\n\nThe major ideas behind PyTorch XLA and running PyTorch on TPUs are the following:\n1. You can use Tensors on XLA devices just like how you work with regular PyTorch Tensors on CPUs\/GPUs\n2. TPU devices (such as the one included in Kaggle Kernels, TPU v3-8) are composed of numerous cores (8 cores to be specific). Each core is itself an XLA device.\n3. PyTorch XLA provides both the interface for working with a single XLA device, and also the interface for working with multiple XLA devices, which is needed to work with the multiple TPU cores\n\nIt is because of the last two points that much of the code for running on 8-core TPUs might look familiar with you have worked with distributed\/multi-GPU set-ups in PyTorch.\n\nWe will now explore each of these three points and work our way up to training large models on 8-core TPUs, like an XLM-R model for the Jigsaw competition.","a3f12c5d":"# Acknowledgments:\n- Based on data from [Abhishek's code](https:\/\/www.kaggle.com\/abhishek\/bert-multi-lingual-tpu-training-8-cores-w-valid)\n- Model based on [xhlulu's code](https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-xlm-roberta)\n- Original attempt from [Aditya's code](https:\/\/www.kaggle.com\/adityaecdrid\/simple-xlmr-tpu-pytorch)\n- Discussion with Davide Libenzi and Daniel Sohn (PyTorch XLA team) - [code](https:\/\/www.kaggle.com\/davidelibenzi\/simple-xlmr-tpu-pytorch)\n- Fruitful discussions with Abhishek and Aditya\n\n\n# Fin\n\nIf you have any questions or suggestions, please drop a comment! :)","9aed6363":"That's it! Now we can submit the results of our XLM-R model!\n\n### Post-processing:\n\nLet's try @christofhenkel's post-processing trick from [here](https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/discussion\/160980)","bb2535b2":"Here is the XLM-RoBERTa model definition inspired by the model used in [xhlulu's kernel](https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-xlm-roberta).","c6bad171":"Let's create our dataset! Now the `ArrayDataset` defined earlier can be used.","a4a4e3e4":"Let's define some helper functions (loss function and reducing function)","6cc5a1f4":"# TPUs - what are they?\n\nTPUs, or tensor processing units, are special processors (ASICs) designed for accelerating deep learning (both training and inference). They have been used since 2015, and made publically available since 2018. There have been several TPU products:\n\n1. First generation TPUs (v1)\n2. Second generation TPUs (v2)\n3. Third generation TPUs (v3)\n4. Edge TPUs\n\nOriginally, the v1 TPUs accelerated neural network inference. As far as I can tell, thee TPUs are currently not available. \n\nThe Edge TPUs are also meant only for accelerating inference, but for edge applications. It only supports 8-bit math, and only TensorFlow Lite models can currently be deployed. They are currently sold under the Coral brand. You can learn more about Edge TPUs over at the [Coral website](https:\/\/coral.ai\/).\n\nGiven that most of the available TPUs for model training are TPU v2 and TPU v3, I will focus my discussion on those.\n\nIn a single device configuration, the TPU consists of 4 chips, each with 2 cores. Each TPU core has scalar, vector, and matrix units (MXU). The MXU provides the bulk of the compute power in a TPU chip. Each of the cores can run computations independently. A good demonstration of this is @abhishek's [8-core simultaneous fold training](https:\/\/www.kaggle.com\/abhishek\/super-duper-fast-pytorch-tpu-kernel). It uses the single device API we just discussed, along with standard Python joblib functionality to train 8 separate fold models on each of the 8 cores.\n\nThe TPU v2 will have 8 GB of high-bandwidth memory (HBM) and one MXU  per core. The TPU v3 will have 16 GB of HBM and two MXUs per core.\n\n![TPU system architecture](https:\/\/cloud.google.com\/tpu\/docs\/images\/tpu--sys-arch4.png)\n\nA single device configuration will basically have 8 cores (v2-8 and v3-8) as shown in the diagram above. This leads to a total memory of 64 GB for the v2 and 128 GB for the v3. However, note that this memory is not shared between cores.\n\nA v2-8 is provided for free on [Google Colab](https:\/\/colab.research.google.com\/) while a v3-8 is provided for free (30 h\/week limit) in a Kaggle Kernel.\n\nNote that [TPU pod](https:\/\/cloud.google.com\/tpu\/docs\/system-architecture#pod) configurations with many more cores are also available on Google Cloud Platform. A TPU-v2 pod can have up to 512 total TPU cores and 4 TiB of total memory, while a TPU-v3 pod can have up to 2048 total TPU cores and 32 TiB of total memory.\n\nSo now we have a better understanding of what TPUs actually are. Let's now try to figure out how to train models on a single device 8-core configuration of a TPU-v2\/v3.","d413d11d":"Now let's see how to train a large model like XLM-RoBERTa on multiple cores. You will see various optimizations and techniques used to get XLM-R to work on the dataset in the kernel environment.","e02e9332":"Here we import all the PyTorch XLA-specific modules.","38dfe84a":"## Inference\n\nI also have inference in the same kernel. We can divide up the dataset for each of the cores to perform inference, save each core's predictions, and then consolidate them to generate the final submission CSV.","0a95fd33":"Here is a simple class to create datasets from numpy arrays. I think this dataset class could likely be further optimized for decreased memory usage.","f1524eb3":"## Start training!\n\nIn order to train the model, we need to _spawn_ the training processes on each of the TPU cores. Let's spawn the `_mp_fn` and start training!","40e44950":"### Imports:\n\nHere we import the necessary modules. Note that we also set environment variables like `XLA_USE_BF16` in order to use [bfloat16](https:\/\/cloud.google.com\/tpu\/docs\/bfloat16), which is a more efficient lower precision floating-point format designed for machine learning.\n\nWe use traditional libraries like PyTorch, pandas, numpy, etc. For this NLP problem, we use Huggingface transformers.","23e11b62":"How simple was that? \n\n**NOTE:** I uncommented the `xm.xla_device()` definition of the device since that would tell the environment that we only want single device\/core training, when later we will see 8-core training. You can experiment with the code in interactive mode.\n\nNow let's move on to training on multiple XLA devices, which means training on 8-core TPUs. First, we need to better understand what TPUs *are*!","3266c39b":"# PyTorch imports","62c521d1":"\n![image.png](attachment:image.png)\n# The Ultimate PyTorch XLA\/TPU Tutorial (Jigsaw XLM-R)\ud83d\udd25\n## If you found this helpful, please give it an upvote!\n\nNOTE: This was meant to be released a month ago but classes took up a lot of time. Even though the competition is essentially over, I hope this is helpful to others.\n\n# Introduction\n\n[PyTorch XLA](https:\/\/pytorch.org\/xla\/) is a PyTorch library for XLA support. XLA (Accelerated Linear Algebra) is a domain-specific compiler that was originally meant for compiling and accelerating TensorFlow models. However, other packages, like [JAX](https:\/\/github.com\/google\/jax) and now PyTorch XLA can compile program with XLA to accelerate code. TPUs can be programmed with XLA programs and PyTorch XLA provides this interface with TPUs by compiling our PyTorch code as XLA programs to run on TPU devices. \n\nIn this kernel, I will demonstrate how to properly train a somewhat large model with [PyTorch XLA](https:\/\/pytorch.org\/xla). PyTorch XLA allows one to train PyTorch models on [Google's tensor processing units (TPUs)](https:\/\/cloud.google.com\/tpu). Kaggle provides 30 hours of free TPU compute.\n\nFirst let's install PyTorch XLA and get started!"}}