{"cell_type":{"e750c013":"code","38a11358":"code","c874fb15":"code","e877ae25":"code","0a8fb412":"code","9972f60b":"code","61ed8ef6":"code","3a91f054":"code","075ec27e":"code","767cc620":"code","b7694b66":"code","41af2940":"code","3da8d038":"code","c16da8c4":"code","4cd7d0bf":"code","6a84886b":"code","f0c4aabb":"code","7e5a62c0":"code","37fcce62":"code","fd63a955":"code","10234207":"code","49bee9eb":"markdown","9db3c2bc":"markdown","43d9411d":"markdown","6ea33558":"markdown","8070a20e":"markdown","321e2bb7":"markdown","ff386a83":"markdown","742ee6d6":"markdown","3302fac3":"markdown","3cc0defe":"markdown","f888533d":"markdown","4de823f7":"markdown","16c80541":"markdown","f655f6aa":"markdown"},"source":{"e750c013":"import pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom bs4 import BeautifulSoup as soup\nfrom urllib.request import urlopen as uReq\nimport re","38a11358":"list_of_orders = pd.read_csv('\/kaggle\/input\/ecommerce-data\/List of Orders.csv')\norder_details = pd.read_csv('\/kaggle\/input\/ecommerce-data\/Order Details.csv')\nsales_target = pd.read_csv('\/kaggle\/input\/ecommerce-data\/Sales target.csv')","c874fb15":"list_of_orders.isna().sum()\norder_details.isna().sum()\nsales_target.isna().sum()\n\nlist_of_orders = list_of_orders.dropna()","e877ae25":"type(sales_target['Month of Order Date'][0])\ntype(list_of_orders['Order Date'][0])\n\nsales_target['Month of Order Date'] = pd.to_datetime(sales_target['Month of Order Date'], format = '%b-%y')\nsales_target['Month of Order Date'] = pd.to_datetime(sales_target['Month of Order Date']).dt.to_period('M')\nlist_of_orders['Order Date'] = pd.to_datetime(list_of_orders['Order Date'])","0a8fb412":"temp = pd.merge(list_of_orders[['Order ID','Order Date']],\n                  order_details[['Order ID','Amount','Category']],\n                  on = 'Order ID')\ntemp['Month of Order Date'] = pd.to_datetime(temp['Order Date']).dt.to_period('M')\ntemp = temp.groupby(['Month of Order Date','Category']).sum().reset_index()","9972f60b":"target = pd.merge(sales_target, temp, on = 'Month of Order Date')\ntarget = target[target['Category_x'] == target['Category_y']].reset_index(drop=True)\ntarget = target.drop(columns = 'Category_y')\ntarget = target.rename(columns = {'Category_x':'Category'})\ntarget['Month of Order Date'] = target['Month of Order Date'].astype(str)\ntarget.head()","61ed8ef6":"target[target['Amount'] >= target['Target']]","3a91f054":"target['difference'] = target['Amount'] - target['Target']\n\nfurniture = target[target['Category'] == 'Furniture'].drop(columns = 'Category').set_index('Month of Order Date')\nelectronics = target[target['Category'] == 'Electronics'].drop(columns = 'Category').set_index('Month of Order Date')\nclothing = target[target['Category'] == 'Clothing'].drop(columns = 'Category').set_index('Month of Order Date')\nsummation = target.groupby(['Month of Order Date']).sum()","075ec27e":"plt.figure(figsize = (10,7))\n\nplt.plot(furniture['difference'], marker = '.', label = 'furniture')\nplt.plot(electronics['difference'], marker = '.', label = 'electronics')\nplt.plot(clothing['difference'], marker = '.', label = 'clothing')\nplt.plot(summation['difference'],linewidth = .5, label = 'summation')\nplt.legend()\n\nplt.axhline(y = 0, color = 'black', linewidth = .5)\nplt.xticks(summation.index[::2])\nplt.xlabel('Date', fontsize = 14)\nplt.ylabel('($)', fontsize = 14)\nplt.title('Difference between the real sales and the target sales', fontdict={'fontweight':'bold', 'fontsize':18})\n\nplt.show()","767cc620":"filename = 'Cities.csv'\nheaders = 'City,Latitude,Longitude\\n'\n\nf = open(filename, \"w\")\nf.write(headers)\n\nfor j in range(1,9):    \n    page_url = f'https:\/\/www.latlong.net\/category\/cities-102-15-{j}.html'\n\n    uClient = uReq(page_url)\n    page_soup = soup(uClient.read(), \"html.parser\")\n    uClient.close()\n\n    rows = page_soup.findAll('tr')\n    rows = rows[1:]\n    \n    for row in rows:\n\n        cell = row.findAll('td')\n\n        City = cell[0].text\n        Latitude = cell[1].text\n        Longitude = cell[2].text\n\n        f.write(City.replace(',', '|') + ',' + Latitude + ',' + Longitude + '\\n')\n\nf.close()","b7694b66":"cities = pd.read_csv('\/kaggle\/working\/Cities.csv')\ncities['Latitude'].astype(float)\ncities['Longitude'].astype(float)\ncities['City'] = cities['City'].str.split('|',expand=False).loc[:]\ncities['City'] = cities['City'].str[0]","41af2940":"toiter = pd.DataFrame(list_of_orders['City'].unique())\nfor index, row in toiter[0].iteritems():\n    if row in cities['City'].tolist():\n        pass\n    else:\n        print(row)","3da8d038":"my_cities = pd.DataFrame(list_of_orders.groupby(['City']).count()['Order ID'])\ncity_map = my_cities.merge(cities[['City','Longitude','Latitude']], how='left', on='City')\ncity_map.set_index('City',inplace = True)","c16da8c4":"city_map.at['Allahabad','Latitude'] = 25.435113\ncity_map.at['Allahabad','Longitude'] = 81.845084\ncity_map.at['Goa','Latitude'] = 15.331737\ncity_map.at['Goa','Longitude'] = 74.126248\ncity_map.at['Kashmir','Latitude'] = 33.925603\ncity_map.at['Kashmir','Longitude'] = 76.137685\ncity_map.at['Kohima','Latitude'] = 25.674477\ncity_map.at['Kohima','Longitude'] = 94.108727\ncity_map.at['Simla','Latitude'] = 31.111610\ncity_map.at['Simla','Longitude'] = 77.169854","4cd7d0bf":"city_map = gpd.GeoDataFrame(city_map, \n                            geometry=gpd.points_from_xy(city_map.Longitude, city_map.Latitude))\ncity_map.drop(columns = ['Longitude', 'Latitude','Order ID'], inplace = True)\ncity_map.reset_index(inplace = True)","6a84886b":"ind_dist = gpd.read_file(r'\/kaggle\/input\/ind-states\/IND_adm1.shp')\nind_dist = ind_dist[['NAME_1','geometry']]\nind_dist.rename(columns = {'NAME_1' : 'State'}, inplace = True)","f0c4aabb":"toiter = pd.DataFrame(list_of_orders['State'].unique())\nfor index, row in toiter[0].iteritems():\n    if row in ind_dist['State'].tolist():\n        pass\n    else:\n        print(row)","7e5a62c0":"list_of_orders.replace('Kerala ','Kerala',inplace = True)","37fcce62":"orders = list_of_orders.groupby(['State']).count()['Order ID']\nind_dist = ind_dist.merge(orders, how = 'left', on='State')\nind_dist.rename(columns={'Order ID':'Orders'},inplace=True)\nind_dist['Orders'] = ind_dist['Orders'].fillna(0)","fd63a955":"fig, ax = plt.subplots(figsize = (10,10))\n\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(position = 'right', size = '7%', pad = 0.1)\n\nind_dist.plot(column = 'Orders', legend = True, cmap = 'Oranges_r', ax = ax, cax = cax,\n             legend_kwds = {'label':'# of orders'}, alpha=0.8, edgecolor='k', linewidth=0.2)\n\ncity_map.plot(ax=ax, marker='o', color='r', markersize=45, edgecolor = 'k')\n\nax.set_title('# of orders by states in India', fontsize = 18)\nax.get_xaxis().set_visible(False)\nax.get_yaxis().set_visible(False)\n\nplt.show()","10234207":"ind_dist[['State','Orders']].sort_values('Orders', ascending = False).head()","49bee9eb":"## Online spending habits by state\nWe have a lot of geographical data in the list_of_orders table, so we could summarize this type of data.\nFirstly, we construct a new table, called Cities. This will contain the latitude and longitude parameters, which are missing from our database. The source: https:\/\/www.latlong.net\/category\/cities-102-15.html","9db3c2bc":"Check if our states from list_of_orders are compatible with the states from ind_dist.","43d9411d":"Now we can find the months, where the sales reached the sales target.","6ea33558":"As we can see, there are 5 missing items. We could search for extra sources, or we could make up for by hand. Because 5 is a really small amount of data, we can complete the table with Google Maps.","8070a20e":"# E-Commerce Data\n### Dataset: https:\/\/www.kaggle.com\/benroshan\/ecommerce-data\nWhat's inside:\n1. List of Orders-This dataset contains purchase information. The information includes ID, Date of Purchase and customer details\n2. Order Details- This dataset contains order ID, with the order price, quantity,profit, category and subcategory of product\n3. Sales target-This dataset contains sales target amount and date for each product category\n\nFirstly, we should import the required libraries, and our data.","321e2bb7":"## Which months did they hit the sales target?\nWe have the data about the sales target in our sales_target table, and the data about the sales in the order_details and list_of_orders tables. If we bring them together, we can get the answer.","ff386a83":"Interestingly, we can see a periodicity in the data. After 2018 June, overestimation and underestimation alternate, which can be seen not only by the summation line, but also by the categories. The periodicity is only 2 month, so it can't be due to big holidays. The reason is maybe, that the products are not so lasting, so they have to be replaced after a few weeks. The customers don't have an ID, so we can't examine how frequently do they order.","742ee6d6":"After a few adjustment we can plot our data to a map to observ, where do people spend online the most.","3302fac3":"We should make further examination on the months. How big was the difference? For more insight, we will break down the data by the category of products.","3cc0defe":"Based on this dataset, we can see that the most cities are located on the northwest side of the country, but the substantial majority of orders by states came from Madhya Pradesh and Maharashtra.","f888533d":"Before we start to analyse the data we should view that are there empty cells, and are the columns in the expected type. After that, do correction, if needed.","4de823f7":"As we can see, there is one state in our data, that missing from the imported data about the states. But if we examine by hand, we can find Kerala in both table. Maybe there is a typo. We can see, that Kerala from list_of_orders contains an extra space at the end: 'Kerala '. We should correct it.","16c80541":"Secondly, we get the required data for the states. The source: http:\/\/www.diva-gis.org\/gdata","f655f6aa":"After we constructed the Cities table, we should examine, how accurate is it."}}