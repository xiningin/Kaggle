{"cell_type":{"f5cc5535":"code","d933da1e":"code","b27295be":"code","26e77f0c":"code","1c3cc05c":"code","f875fab3":"code","6ad291ad":"code","113b6b90":"code","4490f28e":"code","74f3df6c":"code","3360a7b0":"code","5cd1bb17":"code","685991bf":"code","1d3db32b":"code","f1867db7":"code","9312ca34":"code","334f3fce":"code","1f423d63":"code","3a67f8de":"code","ee487dd4":"code","4a3461d7":"code","1f7c40ca":"code","418bc313":"code","bfcf8b65":"code","8ac2251c":"code","fe3a360c":"code","08b63d3f":"code","3e2327ab":"code","6be93060":"code","3a96cf41":"code","79da27b1":"code","82dd4cc2":"code","d12b57fa":"code","ebb974eb":"code","97ce405e":"code","62f9c9d0":"markdown","193f4612":"markdown","b93fb254":"markdown","228a6df2":"markdown","02db415c":"markdown","2e66e4ce":"markdown","8caed207":"markdown","8ac98749":"markdown","5f12320f":"markdown","383781f0":"markdown","80a9c705":"markdown","81dc5cf7":"markdown","dff6591e":"markdown"},"source":{"f5cc5535":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree;\nimport matplotlib.pyplot as plt\n","d933da1e":"df = pd.read_csv(r'..\/input\/water-potability\/water_potability.csv')\ndf.head()","b27295be":"df.shape","26e77f0c":"for i in df.columns:\n    if df[i].dtype!='O':\n        sns.displot(df[i], kde = True)","1c3cc05c":"# Dropping all the Nan's\ndf.dropna(inplace = True)\ndf.reset_index(drop= True, inplace = True)","f875fab3":"df.columns\n","6ad291ad":"df.shape","113b6b90":"sns.pairplot(df)","4490f28e":"df['weights'] = 1\/df.shape[0]\ndf","74f3df6c":"dtree = DecisionTreeClassifier(max_depth=1)\n","3360a7b0":"X = df.iloc[:,0:9].values\ny = df.iloc[:,9].values","5cd1bb17":"dtree.fit(X,y)\n","685991bf":"\nplot_tree(dtree)","1d3db32b":"df['predicted_y'] = dtree.predict(X)\ndf","f1867db7":"\ndef model_weight(error):\n\n  return 0.5*np.log((1-error)\/(error))\n\n","9312ca34":"alpha = model_weight(np.round(len(df[df['Potability']!=df['predicted_y']])\/len(df),2))","334f3fce":"def revising_weights(df,alpha=alpha):\n  if df['Potability'] == df['predicted_y']:\n    return df['weights'] * np.exp(-alpha)\n  else:\n    return df['weights'] * np.exp(alpha)","1f423d63":"df['new_weights'] = df.apply(revising_weights,axis=1)\ndf","3a67f8de":"df['new_weights'].sum()\n","ee487dd4":"df['correcting_weights'] = df['new_weights']\/df['new_weights'].sum()\ndf['correcting_weights'].sum()\n","4a3461d7":"df","1f7c40ca":"df['upper'] = np.cumsum(df['correcting_weights'])\ndf['lower'] = df['upper'] - df['correcting_weights']\ndf","418bc313":"def dataset_for_next_stump(df):\n\n  indices = []\n\n  for i in range(df.shape[0]):\n    a = np.random.random()\n    for index,row in df.iterrows():\n      if row['upper'] > a and a > row['lower']:\n        indices.append(index)\n  return indices\n\n\n\n","bfcf8b65":"index_values = dataset_for_next_stump(df)\n\nnew_df = df.iloc[index_values, [0,1,2,3,4,5,6,7,8,9,10]]\nnew_df\n","8ac2251c":"index_values = dataset_for_next_stump(df)\n\nnew_df = df.iloc[index_values, [0,1,2,3,4,5,6,7,8,9,10]]\nnew_df\n","fe3a360c":"dtree2 = DecisionTreeClassifier(max_depth=1)\n\nX = new_df.iloc[:,0:9].values\ny = new_df.iloc[:,9].values\n\ndtree2.fit(X,y)\n","08b63d3f":"plot_tree(dtree2)\n","3e2327ab":"new_df['predicted_y'] = dtree2.predict(X)\nnew_df","6be93060":"alpha1 = model_weight(np.round(len(new_df[new_df['Potability']!=new_df['predicted_y']])\/len(new_df),2))\nalpha1","3a96cf41":"alpha1","79da27b1":"def update_row_weights(row,alpha=1.09):\n  if row['Potability'] == row['predicted_y']:\n    return row['weights'] * np.exp(-alpha)\n  else:\n    return row['weights'] * np.exp(alpha)\n\nnew_df.apply(update_row_weights,axis=1)","82dd4cc2":"def revising_weights(df,alpha=alpha1):\n  if df['Potability'] == df['predicted_y']:\n    return df['weights'] * np.exp(-alpha)\n  else:\n    return df['weights'] * np.exp(alpha)\n\nnew_df['new_weights'] = new_df.apply(revising_weights,axis=1)\nnew_df","d12b57fa":"new_df['new_weights'].sum()","ebb974eb":"new_df['correcting_weights'] = new_df['new_weights']\/new_df['new_weights'].sum()\nnew_df['correcting_weights'].sum()\n","97ce405e":"new_df.reset_index(drop=True, inplace = True)","62f9c9d0":"# Weights assigned to each row","193f4612":"> **Notice that some predicted_y are different from actual y i.e Portability. Now we will try to upsample these rows by increasing the weights of these rows so that the next decision stump can improve its prediction on these rows.**","b93fb254":"> **That's the best we can reach**","228a6df2":"> **It can be observed for incorrect predictions the weight has increased and for correct predictions it has decreaed.**","02db415c":"**We use decision stump in AdaBoost i.e decision trees of height 1.**","2e66e4ce":"# <p style=\"background-color:#FFA500;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Implementing Boosting From Scratch \u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1<\/p>","8caed207":"**Now we need to do binning, bins would be bigger for wrong predictions so that they when choosing randomly, chances of these rows being selected increases.**","8ac98749":"# The process would be repeated till the time, model is not underfitting anynmore. Hope you would like the implmentation of AdaBoost without Sklearn. \n\n\n# Please upvote if you like my work.","5f12320f":"> **In AdaBoost, every row is assigned the same weight.**","383781f0":">**Sum of weights has to be 1**","80a9c705":"> **New weights are increased for those value which were predicted wrong and decreased for those which were predicted right. So that when we sample them again, the rows with higher weights are drawn majority of the times and new decision stump can revise the prediction in a better way.**","81dc5cf7":"> **New dataset: this would would have those rows higher and repeated multiple times where the prediction went wrong.**","dff6591e":"**Here I am passing the row count where predictions do not match with actual. Because these rows would be accounting for the error.**"}}