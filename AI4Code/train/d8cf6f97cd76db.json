{"cell_type":{"90a2407e":"code","e73981af":"code","a55e267c":"code","74d0df9b":"code","883811f2":"code","e7cbc5f8":"code","f6f7313d":"code","12520eb6":"code","9965a449":"code","1daa860a":"code","6015a840":"code","47015356":"code","7c3c8efb":"code","16d9ea0a":"code","e97eace5":"code","d69c17c0":"code","9256a83b":"markdown","20dd86ea":"markdown","3b8ff9ea":"markdown","72a996b0":"markdown"},"source":{"90a2407e":"import pandas as pd\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nmilk_powder = pd.read_csv('..\/input\/milk-powder.csv')","e73981af":"milk_powder.head()","a55e267c":"y = milk_powder.values[:, 1].astype('uint8')\nX = milk_powder.values[:, 2:]\n\nlda = LDA(n_components=2)\nXlda = lda.fit_transform(X, y)","74d0df9b":"import matplotlib.pyplot as plt","883811f2":"# Define the labels for the plot legend\nlabplot = [f'Milk {i*10}% ' for i in range(11)]","e7cbc5f8":"# Scatter plot\nunique = list(set(y))","f6f7313d":"unique","12520eb6":"import numpy as np\ncolors = [plt.cm.jet(float(i)\/max(unique)) for i in unique]\nplt.figure(figsize=(10, 10))\nwith plt.style.context('ggplot'):\n    for i, u in enumerate(unique):\n        col = np.expand_dims(np.array(colors[i]), axis=0)\n        xi = [Xlda[j, 0] for j in range(len(Xlda[:, 0])) if y[j] == u]\n        yi = [Xlda[j, 1] for j in range(len(Xlda[:, 1])) if y[j] == u]\n        plt.scatter(xi, yi, c = col, s=60, edgecolors='k', label=str(u))\n    plt.xlabel('F1')\n    plt.ylabel('F2')\n    plt.legend(labplot, loc = 'lower right')\n    plt.title('LDA')\n    plt.show()","9965a449":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2)\n# If you want it to be repeatable (for instance if you want to check the performance of the classifier on the same split by changing some other parameter)","1daa860a":"lda = LDA()\nlda.fit(X_train, y_train)\ny_pred = lda.predict(X_test)\nprint(lda.score(X_test, y_test))","6015a840":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(LDA(), X, y, cv=4)","47015356":"print(scores)\nprint(\"Accuracy: %0.4f (+\/- %0.4f)\" % (scores.mean(), scores.std()*3))\nprint(\"Accuracy confidence interval (3*sigma) [%0.4f, %0.4f]\" % (scores.mean()-scores.std()*3, min([scores.mean()+scores.std()*3, 1])))","7c3c8efb":"from sklearn.decomposition import PCA","16d9ea0a":"pca = PCA(n_components=15)\nXpc = pca.fit_transform(X)\nscores = cross_val_score(LDA(), Xpc, y, cv = 4)","e97eace5":"pca.explained_variance_ratio_","d69c17c0":"print(scores)\nprint(\"Accuracy: %0.4f (+\/- %0.4f)\" % (scores.mean(), scores.std()*3))\nprint(\"Accuracy confidence interval (3*sigma) [%0.4f, %0.4f]\" % (scores.mean()-scores.std()*3, min([scores.mean()+scores.std()*3, 1])))","9256a83b":"## Dealing with colinearity\nCollinearity means tha tthe value of the spectra (the \"samples\" in machine learing lingo) at different wavelengths (the \"features\") are not indepenedent, but highly correlated. This is the reason for warning.\n\nTherefore, PCA is one of the ways to remove collinearity problems. The individual principal components are always independent from one another, and by choosing a handful of PCA in our decomposition we are guaranteed that collinearity problem is eliminated.\n\n## Combining PCA and LDA\nPCA and LDA are two different beasts (they are not alternatives). So we can combine them.\nStep 1: Extract a handful of PCA components\nStep 2: Use them for classification","20dd86ea":"# Classificaiton of NIR spectra by Linear Discriminant Analysis in Python\nLearned from: https:\/\/nirpyresearch.com\/classification-nir-spectra-linear-discriminant-analysis-python\/\nProducts or raw materials identification is one of the staples of NIR analysis in industrial processing. Identification of a product or substance - or detection of anomalies over the expected range - are usually accomplished by separating NIR spectra into different classes. In this post we'll work through exmaple of classification of NIR spectrea by LDA in Python.\n\n## What is LDA?\nPCA vs. LDA\n- PCA is unsupervised method, LDA is supervised method\n- PCA is to maximize the variances in the data, LDA has class labels and maximize the distance between those groups \nThis is a good picture showing the difference:\n<img src=\"https:\/\/nirpyresearch.com\/wp-content\/uploads\/2018\/11\/PCAvsLDA-1024x467.png\"><\/img>","3b8ff9ea":"## LDA usage:\nThe use of LDA is not much since we have known the labels. Therefore, it is used more in classification of a new instances.\n","72a996b0":"# Cross validation scores\nJust to make sure that the accuracy is not due to chances, we will rule out this by using cross_val_score function.\nIn this case we don't have to specify a test-train split. This is done automatically by specifying the number of \"folds\", that is the number of splits in our data. For instance, by specifying (cv = 4) we are dividing our data in 4 ports, train the classificer on three of them and use the last part for test. The entire procedure is then repeated by choosing a different test set.\n\nIn this way we can get an average of more than one run, which is guaranteed to give a better etimateion of the accuracy of our classifier."}}