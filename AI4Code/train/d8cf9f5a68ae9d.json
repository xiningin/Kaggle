{"cell_type":{"91479f9d":"code","3f1ee318":"code","9007b328":"code","4289e717":"code","daf9c2d9":"code","80426c7d":"code","f026a7b9":"code","a72df513":"code","b972d362":"code","3a14e45e":"code","6d167fe0":"code","9b1067b3":"code","4b1bac1e":"code","c8508fde":"code","31861ed5":"code","8fef1c0d":"code","d3ba063f":"code","af71f731":"code","48579f5d":"code","1e1bbcef":"code","ea744849":"code","bc5bbe87":"code","4563be44":"code","4336e454":"code","c3e414f6":"code","e07d96fb":"code","ff7c2bcb":"code","f9177c8f":"code","c077157e":"code","22f2facf":"code","f5157b28":"markdown","8f6d2891":"markdown","e7b311b8":"markdown","3d4d1223":"markdown","6f6a7299":"markdown","e69d5fd1":"markdown","1b656a35":"markdown","b8ba5ea2":"markdown","695333f5":"markdown"},"source":{"91479f9d":"# installing the libraries needed (in my case they were already installed)\n\n!pip install rank_bm25 nltk","3f1ee318":"# importing libraries\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path, PurePath\nimport pandas as pd\nimport requests\nfrom requests.exceptions import HTTPError, ConnectionError\nfrom ipywidgets import interact\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom rank_bm25 import BM25Okapi\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\"punkt\")\nnltk.download(\"stopwords\")\nimport re\nimport plotly.express as px","9007b328":"# adjusting the size of columns and rows\n# I left the rows to 29500 because I want all the results to be shown\n\ndef set_column_width(ColumnWidth, MaxRows):\n    pd.options.display.max_colwidth = ColumnWidth\n    pd.options.display.max_rows = MaxRows\n    print('Set pandas dataframe column width to', ColumnWidth, 'and max rows to', MaxRows)\n    \ninteract(set_column_width, \n         ColumnWidth=widgets.IntSlider(min=50, max=400, step=50, value=200),\n         MaxRows=widgets.IntSlider(min=50, max=29500, step=100, value=29500));","4289e717":"# Where are all the files located\n\ninput_dir = PurePath('..\/input\/CORD-19-research-challenge')\n\nlist(Path(input_dir).glob('*'))","daf9c2d9":"# import metadata\n\nmetadata_path = input_dir \/ 'metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str\n})\nmeta_df.head()","80426c7d":"# Convert the doi to a url\n\ndef doi_url(d): return f'http:\/\/{d}' if d.startswith('doi.org') else f'http:\/\/doi.org\/{d}'\nmeta_df.doi = meta_df.doi.fillna('').apply(doi_url)","f026a7b9":"# Set the abstract to the paper title if it is null\n\nmeta_df.abstract = meta_df.abstract.fillna(meta_df.title)","a72df513":"# checking how many entries there are in the DataFrame\n\nlen(meta_df)","b972d362":"# Removing duplicated papers\n\nduplicate_paper = ~(meta_df.title.isnull() | meta_df.abstract.isnull()) & (meta_df.duplicated(subset=['title', 'abstract']))\nmeta_df = meta_df[~duplicate_paper].reset_index(drop=True)","3a14e45e":"# checking how many entries there are in the DataFrame\n\nlen(meta_df)","6d167fe0":"# Creating a function to get the requests from an url\n\ndef get(url, timeout=6):\n    try:\n        r = requests.get(url, timeout=timeout)\n        return r.text\n    except ConnectionError:\n        print(f'Cannot connect to {url}')\n        print(f'Remember to turn Internet ON in the Kaggle notebook settings')\n    except HTTPError:\n        print('Got http error', r.status, r.text)","9b1067b3":"# Creating a wrapper for a DataFrame with useful functions for notebooks\n\nclass DataHolder:    \n    def __init__(self, data: pd.DataFrame):\n        self.data = data        \n        \n    def __len__(self): return len(self.data)\n    \n    def __getitem__(self, item): return self.data.loc[item]\n    \n    def head(self, n:int): return DataHolder(self.data.head(n).copy())\n    \n    def tail(self, n:int): return DataHolder(self.data.tail(n).copy())\n    \n    def _repr_html_(self): return self.data._repr_html_()\n    \n    def __repr__(self): return self.data.__repr__()","4b1bac1e":"# Creating a wrapper for the entire dataset and provides useful functions to navigate through it\n\nclass ResearchPapers:\n    \n    def __init__(self, metadata: pd.DataFrame):\n        self.metadata = metadata\n        \n    def __getitem__(self, item):\n        return Paper(self.metadata.iloc[item])\n    \n    def __len__(self):\n        return len(self.metadata)\n    \n    def head(self, n):\n        return ResearchPapers(self.metadata.head(n).copy().reset_index(drop=True))\n    \n    def tail(self, n):\n        return ResearchPapers(self.metadata.tail(n).copy().reset_index(drop=True))\n    \n    def abstracts(self):\n        return self.metadata.abstract.dropna()\n    \n    def titles(self):\n        return self.metadata.title.dropna()\n        \n    def _repr_html_(self):\n        return self.metadata._repr_html_()","c8508fde":"# Creating a wrapper for each research paper\n\nclass Paper:\n    \n    # single research paper \n    \n    def __init__(self, item):\n        self.paper = item.to_frame().fillna('')\n        self.paper.columns = ['Value']\n    \n    def doi(self):\n        return self.paper.loc['doi'].values[0]\n    \n    def html(self):\n        \n        # loads the paper from doi.org and displays it as HTML. Needs internet on\n        \n        text = get(self.doi())\n        return widgets.HTML(text)\n    \n    def text(self):\n        \n        # loads the paper from doi.org and display as text. Needs internet on\n        \n        text = get(self.doi())\n        return text\n    \n    def abstract(self):\n        return self.paper.loc['abstract'].values[0]\n    \n    def title(self):\n        return self.paper.loc['title'].values[0]\n    \n    def authors(self, split=False):\n        \n        # gets a list of authors\n        \n        authors = self.paper.loc['authors'].values[0]\n        if not authors:\n            return []\n        if not split:\n            return authors\n        if authors.startswith('['):\n            authors = authors.lstrip('[').rstrip(']')\n            return [a.strip().replace(\"\\'\", \"\") for a in authors.split(\"\\',\")]\n        \n        # Todo: Handle cases where author names are separated by \",\"\n        return [a.strip() for a in authors.split(';')]\n        \n    def _repr_html_(self):\n        return self.paper._repr_html_()\n    \n\npapers = ResearchPapers(meta_df)","31861ed5":"abstracts = papers.head(29500).abstracts()\ntype(abstracts)","8fef1c0d":"# Creating a list of stopwords in english\n\nenglish_stopwords = list(set(stopwords.words('english')))","d3ba063f":"# Creating a function that cleans text of special characters\n\ndef strip_characters(text):\n    t = re.sub('\\(|\\)|:|,|;|\\.|\u2019|\u201d|\u201c|\\?|%|>|<', '', text)\n    t = re.sub('\/', ' ', t)\n    t = t.replace(\"'\",'')\n    return t","af71f731":"# Creating a function that makes text lowercase and uses the function created above\n\ndef clean(text):\n    t = text.lower()\n    t = strip_characters(t)\n    return t","48579f5d":"# Tokenize into individual tokens - words mostly\n\ndef tokenize(text):\n    words = nltk.word_tokenize(text)\n    return list(set([word for word in words \n                     if len(word) > 1\n                     and not word in english_stopwords\n                     and not (word.isnumeric() and len(word) is not 4)\n                     and (not word.isnumeric() or word.isalpha())] )\n               )","1e1bbcef":"# Creating a function that cleans and tokenize texts\n\ndef preprocess(text):\n    t = clean(text)\n    tokens = tokenize(t)\n    return tokens","ea744849":"# Creating a wrapper for the search results\n\nclass SearchResults:\n    \n    def __init__(self, \n                 data: pd.DataFrame,\n                 columns = None):\n        self.results = data\n        if columns:\n            self.results = self.results[columns]\n            \n    def __getitem__(self, item):\n        return Paper(self.results.loc[item])\n    \n    def __len__(self):\n        return len(self.results)\n        \n    def _repr_html_(self):\n        return self.results._repr_html_()","bc5bbe87":"# Defining column names of the search display\n\nSEARCH_DISPLAY_COLUMNS = ['title', 'abstract', 'doi', 'authors', 'journal']","4563be44":"# Creating a wrapper for the word tokens which will be searched\n\nclass WordTokenIndex:\n    \n    def __init__(self, \n                 corpus: pd.DataFrame, \n                 columns=SEARCH_DISPLAY_COLUMNS):\n        self.corpus = corpus\n        raw_search_str = self.corpus.abstract.fillna('') + ' ' + self.corpus.title.fillna('')\n        self.index = raw_search_str.apply(preprocess).to_frame()\n        self.index.columns = ['terms']\n        self.index.index = self.corpus.index\n        self.columns = columns\n    \n    def search(self, search_string):\n        search_terms = preprocess(search_string)\n        result_index = self.index.terms.apply(lambda terms: any(i in terms for i in search_terms))\n        results = self.corpus[result_index].copy().reset_index().rename(columns={'index':'paper'})\n        return SearchResults(results, self.columns + ['paper'])","4336e454":"# Creating the search index class\n\nclass RankBM25Index(WordTokenIndex):\n    \n    def __init__(self, corpus: pd.DataFrame, columns=SEARCH_DISPLAY_COLUMNS):\n        super().__init__(corpus, columns)\n        self.bm25 = BM25Okapi(self.index.terms.tolist())\n        \n    def search(self, search_string, n=4):\n        search_terms = preprocess(search_string)\n        doc_scores = self.bm25.get_scores(search_terms)\n        ind = np.argsort(doc_scores)[::-1][:n]\n        results = self.corpus.iloc[ind][self.columns]\n        results['Score'] = doc_scores[ind]\n        results = results[results.Score > 0]\n        return SearchResults(results.reset_index(), self.columns + ['Score'])","c3e414f6":"# Creating the search index\n\nbm25_index = RankBM25Index(meta_df)","e07d96fb":"# Creating a list for the tasks in the challenge and it's keywords\n\ntasks = [('What is known about transmission, incubation, and environmental stability?', 'transmission periods asymptomatic shedding persistance stability substrate surfaces diagnostics disease models tools immune immunity effectiveness strategy movement health care community PPE seasonality incubation environmental stability coronavirus'),\n         ('What do we know about COVID-19 risk factors?', 'COVID-19 risk factors coronavirus smoking pre-existing pulmonary disease neonates pregnant women severity transmission dynamics susceptibility of populations public health mitigation measures that could be effective for control'),\n         ('What do we know about virus genetics, origin, and evolution?', 'virus genetics origin evolution coronavirus real-time tracking genome dissemination diagnostics geographic temporal genomic strain nagoya protocol livestock field surveillance genetic sequencing receptor binding farmers wildlife SARS-CoV-2 animal host socioeconomic behavioral reduction risk'),\n         ('Sample task with sample submission', 'sample submission geographic variation variations mutation evidence'),\n         ('What do we know about vaccines and therapeutics?', 'drugs clinical bench trials less common viral inhabitors naproxen clarithromycin minocyclinethat Antibody-Dependent Enhancement (ADE) in vaccine recipients animal model predictive value therapeutic alternative model prioritize distribution expanding production universal vaccine standardize prophylaxis clinical studies enhanced disease immune response'),\n         ('What do we know about non-pharmaceutical interventions?', 'guidance scale up NPI funding infrastructure authorities support authoritative collaboration health care delivery system capacity respond increase case design execution experiment DHS center excelence assessment school closure travel ban mass gathering social distancing spread communities barriers compliance intervention cost benefit race income disability age geographic location immigration status housing employment health insurance compliance underserved advice pandemicpolicy programmatic mitigate government service food distribuition supplies food household diagnose treatment'),\n         ('What has been published about ethical and social science considerations?', 'effort articulate translate existing ethical principle standard salient issues COVID-19 thematic novel duplicate oversight sustained education access capacity WHO multidisciplinary research operational platform global network social science qualitative assessment framework local barrier enabler adherence public health measure prevention control surgical mask SRH school closure outbreak physical physiological underlying driver fear anxiety stigma misinformation rumor social media'),\n         ('What do we know about diagnostics and surveillance?', 'widespread exposure immediate policy recommentation mitigation measure denominator testing mechanism sharing information demographics sampling asymptomatic disease serosurvey convalescent screening neutralizing antibodies ELISA diagnostic surveillance recruitment legal ethical communication public health official national guidance guidelines best practices state universities private laboratories tradeoff speed accuracy accessibility PCR specific entity assay private sector evolution genetic drift mutation reagent latency pathogen cytokines progression therapeutical policies roadmap coalition epidemic preparedness inovation CRISPR genomics rapid sequencing bioinformatic wildlife domestic risk'),\n         ('What has been published about medical care?', 'nursing long term care medical staff shortage overwhelmed communities age-ajusted mortality Acute Respiratory Distress Syndrome ARDS organ failure viral etiology Extracorporeal membrane oxygenation ECMO mechanical ventilation frequency manifestation cardiomyopathy cardiac arrest regulatory standard EUA CLIA crisis care level elastomeric respirator N95 mask telemedicine barriers facilitators specific action state boundaries guidance oral medication AI real-time health care delivery valuate interventions risk factors outcomes best practices critical challenges innovative solutions technologies flow organization workforce protection allocation community-based resources payment supply chain natural history interventions steroids high flow oxygen'),\n         ('What has been published about information sharing and inter-sectoral collaboration?', 'data-gathering standardized nomenclature sharing response information with planners providers mitigating barriers of information-sharing recruit support coordinate local expertise capacity public health emergency response integration federal state local public health surveillance systems investment baseline infrastructure preparedness high-risk population elderly health care workers guidelines easy understand risk disease population group misunderstanding containment mitigation action plan surveillance treatment marginalized disadvantaged data system incarcareted COVID-19 information prevention diagnosis coverage policies')]","ff7c2bcb":"# Transforming the list into a DataFrame\n\ntasks = pd.DataFrame(tasks, columns=['Task', 'Keywords'])","f9177c8f":"# Creating a dropdown menu for each task\n\ndef show_task(Task):\n    print(Task)\n    keywords = tasks[tasks.Task == Task].Keywords.values[0]\n    search_results = bm25_index.search(keywords, n=1000)\n    display(search_results)\n    return search_results\n    \nresults = interact(show_task, Task = tasks.Task.tolist());","c077157e":"# Creating autocomplete search bar\n\ndef search_papers(SearchTerms: str):\n    search_results = bm25_index.search(SearchTerms, n=1000)\n    if len(search_results) > 0:\n        display(search_results) \n    return search_results","22f2facf":"# Autocomplete search bar\n\nsearchbar = widgets.interactive(search_papers, SearchTerms='ethic')\nsearchbar","f5157b28":"<h2>Creating a dropdown menu for the kaggle tasks<\/h2><br>\nIn this dropdown menu you can select the task and it will return all the papers related to the keywords associated with that task. The keywords were created based on the specific requirement of each task.","8f6d2891":"![](https:\/\/media3.giphy.com\/media\/3o7TKMlJrVQ5ket3hu\/giphy.gif?cid=790b76116b55efb8ce288d0f1c58ee050f5810df1d6368af&rid=giphy.gif)","e7b311b8":"<h2>Creating an autocomplete search bar<\/h2><br>\nWhy go through the trouble of typing everything when this search engine can do everything for you?","3d4d1223":"This uses IPywidgets interactive rendering of a TextBox.","6f6a7299":"![](https:\/\/media1.giphy.com\/media\/Ebys71phzQQx2\/giphy.gif?cid=790b7611a3c7952b8ed20b09249ca2cd3774beef22dd1560&rid=giphy.gif)","e69d5fd1":"<h2>Treating the dataset<\/h2><br>\nBesides treating all the dataset, we will create means, functions and utilities to work with the data and have an autofilling search index and a dropdown menu for the tasks in kaggle.","1b656a35":"![](https:\/\/media2.giphy.com\/media\/l2YWmdzX0DkNHk3ew\/giphy.gif?cid=790b76114fe3a6219b32e6f19c686f11f629724e736bcc5e&rid=giphy.gif)","b8ba5ea2":"# COVID-19 Open Research Dataset Challenge (CORD-19)\n![](https:\/\/altaonline.typepad.com\/.a\/6a0192ac343706970d025d9b3673bb200c-800wi)","695333f5":"<h2>Goal<\/h2><br>\n    This is my first response to the call to action to the artificial intelligence experts (if I can be called one) to ddevelop text and data mining tools that can help the medical community develop answers to high priority scientific questions. For that I will use the CORD-19 dataset, which represents the most extensive machine-readable coronavirus literature collection available for data mining to date. Bellow are the current tasks for this challenge, which will be completed by the creation of a search index on top of the all_sources_metadata file, which will work independently of the metadata limitations. There are around 29500 papers in the dataset. These are listed in the all_sources_metadata file. Some of the papers in the metadata are also in JSON files. The eventual goal is to connect the metadata with the JSON data.<br>\n    <h2>Tasks<\/h2>\n    <ul>\n    <li>What is known about transmission, incubation, and environmental stability?<\/li>\n    <li>What do we know about COVID-19 risk factors?<\/li>\n    <li>What do we know about virus genetics, origin, and evolution?<\/li>\n    <li>Sample task with sample submission<\/li>\n    <li>What do we know about vaccines and therapeutics?<\/li>\n    <li>What do we know about non-pharmaceutical interventions?<\/li>\n    <li>What has been published about ethical and social science considerations?<\/li>\n    <li>What do we know about diagnostics and surveillance?<\/li>\n    <li>What has been published about medical care?<\/li>\n    <li>What has been published about information sharing and inter-sectoral collaboration?<\/li>\n    <\/ul>\n    <h2>Citations, ups and downs<\/h2><br>\n    I used the <a href='https:\/\/www.kaggle.com\/dgunning\/browsing-research-papers-with-a-bm25-search-engine'>Browsing research papers with a BM25 search engine<\/a> as a reference for making this notebook. I was looking forward to testing different NLP solutions and the one presented here was very interesting. On the plus side, it offers a very dynamic online research index for all the papers, with a return that looks beautiful. On the down side, it needs you to have your internet working, and depending on the size of the dataframe, it can take a while to load (mine took a few minutes). Besides that I am still working on a way to split authors when they are separated by comma or by semicolon.<br>\n    <h2>Features of this notebook<\/h2>\n<ol><li>Viewing the papers in the metdata csv as a dataframe<\/li>\n    <li>Selecting individual papers<\/li>\n    <li>Search using a simple search index using RankBM25<\/li>\n    <li>Autocomplete search bar<\/li><\/ol>\n    <h2>Turn your internet on!<\/h2><br>\n    For this notebook to work your internet must be on."}}