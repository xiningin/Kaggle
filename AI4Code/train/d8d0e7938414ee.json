{"cell_type":{"919f7830":"code","ecbd6b6e":"code","acc816cf":"code","d096f1bb":"code","495748b4":"code","8743b4b0":"code","e7c1da51":"code","755f1543":"code","7ac75160":"code","8c216ec8":"code","ed513a12":"code","e3ca4d0a":"code","deb96da9":"code","1f2b9c6a":"code","103168fe":"code","b337502f":"code","4611fef7":"code","52ed0793":"code","b8e50cfb":"code","afa2d1d3":"code","490a6c94":"code","a79823b0":"code","21c35f9a":"code","422379e1":"code","e89032ff":"code","a72f1a38":"code","35c71627":"code","3cab389f":"code","b382b8bc":"code","501b34fe":"code","80e0915a":"code","069db9cc":"code","3d00c5e9":"code","b861a3f4":"markdown","94aeedec":"markdown","9f2887d5":"markdown","c564c9a8":"markdown","4cea5d05":"markdown","9f785e34":"markdown","b1ac51e3":"markdown","ebfe4809":"markdown"},"source":{"919f7830":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ecbd6b6e":"#!pip install keras","acc816cf":"#import required libraries\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\n\nprint('setup completed!')","d096f1bb":"#load the train and test data\ntrain_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col='Id')\ntest_data =  pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\", index_col='Id')\n\ntrain_data.head()\n","495748b4":"test_data.head()","8743b4b0":"#check the shape of the data\n\nprint(\"========sahpe of the data======\")\ntrain_data.shape, test_data.shape","e7c1da51":"print(\"======train data info=======\")\nprint(train_data.info())\nprint(\"======test data info========\")\nprint(test_data.info())","755f1543":"#NaN counts \n\nprint('======= train NaNs=======')\nprint(train_data.isnull().sum().sum())\nprint('======= test NaNs=======')\nprint(test_data.isnull().sum().sum())\n","7ac75160":"#let's plot null values graphically for each columns\n\nplt.figure(figsize=(30,70))\nplt.subplot(211)\nplt.barh(train_data.columns, train_data.isnull().sum())\nplt.title('Train and Test data NaN counts')\nplt.xlabel('features')\nplt.ylabel('counts')\nplt.subplot(212)\nplt.barh(test_data.columns, test_data.isnull().sum())\nplt.xlabel('features')\nplt.ylabel('counts')\nplt.show()","8c216ec8":"#Nan percentage\nlist_train=[]\ntrain_name=[]\nlist_test=[]\ntest_name=[]\n#train data NaN percentage counts\nfor col in train_data.columns:\n    if train_data[col].isnull().any():\n        train_name.append(col)\n        list_train.append((train_data[col].isnull().sum()\/len(train_data[col]))*100)\n        \ndf_train_null = pd.DataFrame({\n    'columns': train_name,\n    'percentage' : list_train\n})\n\n#Test data\nfor col in test_data.columns:\n    if test_data[col].isnull().any():\n        test_name.append(col)\n        list_test.append((test_data[col].isnull().sum()\/len(test_data[col]))*100)\n        \ndf_test_null = pd.DataFrame({\n    'columns': test_name,\n    'percentage' : list_test\n})\n\n    ","ed513a12":"df_train_null.head()","e3ca4d0a":"df_test_null.head()","deb96da9":"plt.figure(figsize=(50,50))\nplt.subplot(211)\nplt.title('Train and Test data NaN percentages')\nplt.bar(df_train_null['columns'], df_train_null.percentage)\n\nplt.subplot(212)\nplt.bar(df_test_null['columns'], df_test_null.percentage)\n\n\nplt.show()","1f2b9c6a":"#Let's sort the null counts of both data in descending order.\n\nprint('====Train Nulls======')\nprint(train_data.isnull().sum().sort_values(ascending=False))\nprint('====Test Nulls======')\nprint(test_data.isnull().sum().sort_values(ascending=False))","103168fe":"#let's separate target and predictors in train data and then we'll split the categorical and numerical columns.\n\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice','PoolQC', 'MiscFeature' , 'Alley', 'Fence'] ,axis=1)\n\n\n\n#separate the categorical and numerical columns\n\nnum_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n\ncateg_cols = [col for col in X.columns if X[col].dtype == 'object']\n\n","b337502f":"#Let's impute the remaining NaN values using the SimpleImputer\n#preprocessing numerical cols\nimputer = SimpleImputer(strategy = 'mean')\n\nX_num = pd.DataFrame(imputer.fit_transform(X[num_cols]))\nX_tnum= pd.DataFrame(imputer.transform(test_data[num_cols]))\n\nX_num.columns=num_cols\nX_tnum.columns=num_cols\n\n#preprocessing categorical cols\nimputer = SimpleImputer(strategy='most_frequent')\n\nX_cat = pd.DataFrame(imputer.fit_transform(X[categ_cols]))\nX_tcat= pd.DataFrame(imputer.transform(test_data[categ_cols]))\n\nX_cat.columns=categ_cols\nX_tcat.columns=categ_cols\n\n\n","4611fef7":"#Check for NaN again\nX_num.isnull().sum().sum(), X_tnum.isnull().sum().sum(),X_cat.isnull().sum().sum(),X_tcat.isnull().sum().sum()","52ed0793":"#Let's use One Hot Encoder to transform the categorical columns\n\nencoder = OneHotEncoder(handle_unknown = 'ignore', sparse=False)\n\nOH_X_cat = pd.DataFrame(encoder.fit_transform(X_cat))\nOH_X_tcat = pd.DataFrame(encoder.transform(X_tcat))\n\nOH_X_cat.index = X_cat.index\nOH_X_tcat.index = X_tcat.index\n\n","b8e50cfb":"#Now combine the numerical and  categorical columns together as single datasets\n\nX_full = pd.concat([X_num, OH_X_cat], axis=1)\nX_test = pd.concat([X_tnum, OH_X_tcat], axis=1)","afa2d1d3":"X_full.head()","490a6c94":"X_test.head()","a79823b0":"#check the shape\nX_full.shape , X_test.shape\nprint(\"The number of columns in both datasets : \", str(X_full.shape[1]))","21c35f9a":"#recheck the NaN existence\nX_full.isnull().sum().sum() , X_full.isnull().sum().sum()","422379e1":"print('All columns have been transformed into float64')\nX_full.info() , X_test.info()","e89032ff":"X_train, X_valid, y_train, y_valid = train_test_split(X_full, y, train_size=0.8 , test_size=0.2, random_state=0)","a72f1a38":"#lets define the number of inputs which is same as the column numbers\n\ninput_cols = X_train.shape[1]","35c71627":"#build the model\n\nmodel = keras.Sequential([\n        #hidden layers\n        layers.Dense(200, activation = 'relu', input_shape=[input_cols]),\n        layers.Dense(100, activation = 'relu'),\n        layers.Dense(100, activation = 'relu'),\n        layers.Dense(50, activation = 'relu'),\n        layers.Dense(10, activation = 'relu'),\n        #output layer\n        layers.Dense(1),\n])    \n\nmodel.compile(optimizer='adam', loss='mean_squared_error')","3cab389f":"#train the model\n\nmodel.fit(X_train, y_train, epochs = 200, batch_size=10)","b382b8bc":"#predict the validation data\npred= model.predict(X_valid)","501b34fe":"print(\"RMSE error : \")\nprint(mean_squared_error(y_valid, pred , squared=False))","80e0915a":"#Now fit and predict the test data\n\nmodel.fit(X_full, y , epochs=200 , batch_size=10)","069db9cc":"#Now predict the test data\n\npred_test = model.predict(X_test)","3d00c5e9":"# Save test predictions to file\noutput = pd.DataFrame({'Id': test_data.index,\n                       'SalePrice': pred_test[:,0]})\noutput.to_csv('submission_04.csv', index=False)","b861a3f4":"#### From the plots above we can see there are 4 columns with maximum numbr of NaN values. Now, we shall calculate the percentage of the NaN counts from the columns.","94aeedec":"### Now, we shal build our Deep Learning model ","9f2887d5":"### Now, we shall split our train data into train and validation","c564c9a8":" We see from plot that only 4 columns have more that 80% NaN values in it. We can ignore these 4 columns before preprocessing.","4cea5d05":"#### Lets plot the columns containing NaN values","9f785e34":"## House Price Prediction using Deep Learning","b1ac51e3":"#### Now we have imputed the values to replacwe NaN values, we need to encode the categorical data as numerical as the  models won't work if we let the categorical data as input. ","ebfe4809":"We see that columns : PoolQC, MiscFeature , Alley and Fence have over 80 % NaN values. So we'll be dropping this column before we begin with preprocessing our data."}}