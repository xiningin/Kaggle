{"cell_type":{"78487494":"code","25822bad":"code","467020f3":"code","b98b289b":"code","5d3f6af9":"code","dbd3d794":"code","a2cceffe":"code","9f9f56b7":"code","fb669397":"code","abf9fa03":"code","6e287cb5":"code","e879edfe":"code","762638c9":"code","6a1d88fd":"code","1ef015c6":"code","aa3701b9":"code","95b96684":"code","f20a0946":"markdown","94bd5dbe":"markdown","0c0f3086":"markdown","810c201a":"markdown","1f02f42d":"markdown"},"source":{"78487494":"import numpy as np\nimport pandas as pd\n\nfrom datetime import date, datetime\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom imblearn.pipeline import make_pipeline as make_pipeline_imb # To do our transformation in a unique time\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.metrics import classification_report_imbalanced\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, KFold, GridSearchCV\nfrom collections import Counter\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import precision_score, recall_score, fbeta_score, confusion_matrix, precision_recall_curve, accuracy_score, mean_squared_error\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)","25822bad":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_hist_trans = pd.read_csv('..\/input\/historical_transactions.csv')\ndf_new_merchant_trans = pd.read_csv('..\/input\/new_merchant_transactions.csv')","467020f3":"df_train.head()","b98b289b":"for df in [df_hist_trans,df_new_merchant_trans]:\n    df['category_2'].fillna(1.0,inplace=True)\n    df['category_3'].fillna('A',inplace=True)\n    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)","5d3f6af9":"def get_new_columns(name,aggs):\n    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]","dbd3d794":"for df in [df_hist_trans,df_new_merchant_trans]:\n    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n    df['year'] = df['purchase_date'].dt.year\n    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n    df['month'] = df['purchase_date'].dt.month\n    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n    df['hour'] = df['purchase_date'].dt.hour\n    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n    df['month_diff'] = ((datetime.today() - df['purchase_date']).dt.days)\/\/30\n    df['month_diff'] += df['month_lag']","a2cceffe":"aggs = {}\nfor col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n    aggs[col] = ['nunique']\n\naggs['purchase_amount'] = ['sum','max','min','mean','var']\naggs['installments'] = ['sum','max','min','mean','var']\naggs['purchase_date'] = ['max','min']\naggs['month_lag'] = ['max','min','mean','var']\naggs['month_diff'] = ['mean']\naggs['authorized_flag'] = ['sum', 'mean']\naggs['weekend'] = ['sum', 'mean']\naggs['category_1'] = ['sum', 'mean']\naggs['card_id'] = ['size']\n\nfor col in ['category_2','category_3']:\n    df_hist_trans[col+'_mean'] = df_hist_trans.groupby([col])['purchase_amount'].transform('mean')\n    aggs[col+'_mean'] = ['mean']    \n\nnew_columns = get_new_columns('hist',aggs)\ndf_hist_trans_group = df_hist_trans.groupby('card_id').agg(aggs)\ndf_hist_trans_group.columns = new_columns\ndf_hist_trans_group.reset_index(drop=False,inplace=True)\ndf_hist_trans_group['hist_purchase_date_diff'] = (df_hist_trans_group['hist_purchase_date_max'] - df_hist_trans_group['hist_purchase_date_min']).dt.days\ndf_hist_trans_group['hist_purchase_date_average'] = df_hist_trans_group['hist_purchase_date_diff']\/df_hist_trans_group['hist_card_id_size']\ndf_hist_trans_group['hist_purchase_date_uptonow'] = (datetime.today() - df_hist_trans_group['hist_purchase_date_max']).dt.days\n\ndf_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n\ndel df_hist_trans_group;\ngc.collect()","9f9f56b7":"aggs = {}\nfor col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n    aggs[col] = ['nunique']\naggs['purchase_amount'] = ['sum','max','min','mean','var']\naggs['installments'] = ['sum','max','min','mean','var']\naggs['purchase_date'] = ['max','min']\naggs['month_lag'] = ['max','min','mean','var']\naggs['month_diff'] = ['mean']\naggs['weekend'] = ['sum', 'mean']\naggs['category_1'] = ['sum', 'mean']\naggs['card_id'] = ['size']\n\nfor col in ['category_2','category_3']:\n    df_new_merchant_trans[col+'_mean'] = df_new_merchant_trans.groupby([col])['purchase_amount'].transform('mean')\n    aggs[col+'_mean'] = ['mean']\n    \nnew_columns = get_new_columns('new_hist',aggs)\ndf_hist_trans_group = df_new_merchant_trans.groupby('card_id').agg(aggs)\ndf_hist_trans_group.columns = new_columns\ndf_hist_trans_group.reset_index(drop=False,inplace=True)\ndf_hist_trans_group['new_hist_purchase_date_diff'] = (df_hist_trans_group['new_hist_purchase_date_max'] - df_hist_trans_group['new_hist_purchase_date_min']).dt.days\ndf_hist_trans_group['new_hist_purchase_date_average'] = df_hist_trans_group['new_hist_purchase_date_diff']\/df_hist_trans_group['new_hist_card_id_size']\ndf_hist_trans_group['new_hist_purchase_date_uptonow'] = (datetime.today() - df_hist_trans_group['new_hist_purchase_date_max']).dt.days\n\ndf_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n\ndel df_hist_trans_group;\ngc.collect()","fb669397":"del df_hist_trans;\ngc.collect()\n\ndel df_new_merchant_trans;\ngc.collect()\n\ndf_train.head()","abf9fa03":"df_train['first_active_month'] = pd.to_datetime(df_train['first_active_month'])\ndf_train['dayofweek'] = df_train['first_active_month'].dt.dayofweek\ndf_train['weekofyear'] = df_train['first_active_month'].dt.weekofyear\ndf_train['month'] = df_train['first_active_month'].dt.month\ndf_train['elapsed_time'] = (datetime.today() - df_train['first_active_month']).dt.days\ndf_train['hist_first_buy'] = (df_train['hist_purchase_date_min'] - df_train['first_active_month']).dt.days\ndf_train['new_hist_first_buy'] = (df_train['new_hist_purchase_date_min'] - df_train['first_active_month']).dt.days\nfor f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n                    'new_hist_purchase_date_min']:\n    df_train[f] = df_train[f].astype(np.int64) * 1e-9\ndf_train['card_id_total'] = df_train['new_hist_card_id_size']+df_train['hist_card_id_size']\ndf_train['purchase_amount_total'] = df_train['new_hist_purchase_amount_sum'] + df_train['hist_purchase_amount_sum']\ndf_train = pd.get_dummies(df_train, columns=['feature_1', 'feature_2'])\n\ndf_train.head()","6e287cb5":"df_train['outliers_target'] = 0\ndf_train.loc[df_train['target'] < -30, 'outliers_target'] = 1\ndf_train['outliers_target'].value_counts()","e879edfe":"df_train.columns[df_train.isna().any()]","762638c9":"df_train.drop(['new_hist_month_nunique', 'new_hist_hour_nunique',\n       'new_hist_weekofyear_nunique', 'new_hist_dayofweek_nunique',\n       'new_hist_year_nunique', 'new_hist_subsector_id_nunique',\n       'new_hist_merchant_id_nunique', 'new_hist_merchant_category_id_nunique',\n       'new_hist_purchase_amount_sum', 'new_hist_purchase_amount_max',\n       'new_hist_purchase_amount_min', 'new_hist_purchase_amount_mean',\n       'new_hist_purchase_amount_var', 'new_hist_installments_sum',\n       'new_hist_installments_max', 'new_hist_installments_min',\n       'new_hist_installments_mean', 'new_hist_installments_var',\n       'new_hist_month_lag_max', 'new_hist_month_lag_min',\n       'new_hist_month_lag_mean', 'new_hist_month_lag_var',\n       'new_hist_month_diff_mean', 'new_hist_weekend_sum',\n       'new_hist_weekend_mean', 'new_hist_category_1_sum',\n       'new_hist_category_1_mean', 'new_hist_card_id_size',\n       'new_hist_category_2_mean_mean', 'new_hist_category_3_mean_mean',\n       'new_hist_purchase_date_diff', 'new_hist_purchase_date_average',\n       'new_hist_purchase_date_uptonow', 'new_hist_first_buy', 'card_id_total',\n       'purchase_amount_total'], axis=1,inplace=True)","6a1d88fd":"X = df_train.drop(['target','card_id','first_active_month','outliers_target'],axis=1)\ny = df_train['outliers_target']","1ef015c6":"def print_results(headline, true_value, pred):\n    print(headline)\n    print(\"accuracy: {}\".format(accuracy_score(true_value, pred)))\n    print(\"precision: {}\".format(precision_score(true_value, pred)))\n    print(\"recall: {}\".format(recall_score(true_value, pred)))\n    print(\"f2: {}\".format(fbeta_score(true_value, pred, beta=2)))","aa3701b9":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.20)\n\nclassifier = RandomForestClassifier\n\n# build model with SMOTE imblearn\nsmote_pipeline = make_pipeline_imb(SMOTE(random_state=42), \\\n                                   classifier(random_state=42))\n\nsmote_model = smote_pipeline.fit(X_train, y_train)\nsmote_prediction = smote_model.predict(X_test)\n\n\nprint(\"normal data distribution: {}\".format(Counter(y)))\n\nX_smote, y_smote = SMOTE().fit_sample(X, y)\n\nprint(\"SMOTE data distribution: {}\".format(Counter(y_smote)))","95b96684":"print(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test, smote_prediction))\n\nprint('\\nSMOTE Pipeline Score {}'.format(smote_pipeline.score(X_test, y_test)))\n\nprint_results(\"\\nSMOTE + RandomForest classification\", y_test, smote_prediction)","f20a0946":"### Next steps:: \n#### * Tune the hyperparmeters to get a better Recall on the 'outlier_targets'\n#### * See how the performance of this classifier ends up affecting the performance of the actual model on the validation set \n\n### So this was my first effort at trying to account for the outliers in the training set. Any comments\/clarifications\/corrections are more than welcome! \n#### To be continued... ","94bd5dbe":"### So if we consider the outliers to be the target variable (for binary classification), we have a rather imbalanced dataset (which we will then balance out via Smote)\n### Smote will likely have issues with nans, so remove all those columns for now. Later we may use subtler methods like sklearn imputer etc. ","0c0f3086":"## As is evident from multiple discussions: \n* https:\/\/www.kaggle.com\/c\/elo-merchant-category-recommendation\/discussion\/73571\n* https:\/\/www.kaggle.com\/c\/elo-merchant-category-recommendation\/discussion\/73922\n* https:\/\/www.kaggle.com\/c\/elo-merchant-category-recommendation\/discussion\/73024\n\n## The 2207 rows with target values < -33 do seem to have an important role in this synthesized dataset. I wanted to do a little trial where I wanted to check what kind of accuracy the model was getting in predicting these outliers on the training set. So we have ourselves an imbalanced binary classification task, that I will balance out via SMOTE. \n\n## The longer term idea is that I want to use the model that predicts these outliers best (for the test set predictions)\n\n## Upvote the kernel if you like it, and please do point out any conceptual errors or flaws that you notice! \n","810c201a":"## The ideas for SMOTE come from https:\/\/www.kaggle.com\/kabure\/credit-card-fraud-prediction-rf-smote","1f02f42d":"### So we see quite a high number of False Negatives, which drags the Recall down, and implies that we aren't getting as many of the outliers as we should be. A lot of this can, of course, be improved by parameter tuning."}}