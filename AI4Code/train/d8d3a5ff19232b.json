{"cell_type":{"a99fc32c":"code","00951d26":"code","f49577c4":"code","b315a340":"code","c5fbab0d":"code","15801ca4":"code","cc06a362":"code","848fd114":"code","a22ee66b":"code","586334b3":"code","093b8fbf":"code","7b343886":"code","6184b101":"code","00ca4dd9":"code","ea6d7fe8":"code","90703933":"code","46171a1d":"code","df1b185b":"code","69152074":"code","83a06ed3":"code","134f9de5":"code","2ef8ca85":"code","2b87eb8c":"code","4e0055c9":"code","2cd7de88":"code","13b003d6":"code","57128c98":"code","662cbed8":"code","b9fb984a":"code","582bce4e":"code","d5287064":"code","1348da86":"code","111b5fa1":"code","5b14eb8d":"code","4b9e0438":"code","71e19142":"code","403b1c09":"code","8ef1f2f5":"code","e836aa15":"code","4f2ce88d":"code","27232175":"code","97b605e7":"code","726663c5":"code","7767567c":"code","843eaf3b":"code","b41ee60f":"code","706316ec":"code","05c8fa4b":"code","48849fc7":"code","a6daf29a":"code","d4c98cd7":"code","106fda3d":"code","05f58daa":"code","59c077e9":"code","2880b347":"code","a308adf4":"code","88135536":"code","8fb4a69e":"code","b3b4f673":"markdown","3ec80ed8":"markdown","a86824b8":"markdown","3da267a2":"markdown","33e25ed7":"markdown","e0434871":"markdown","d5883afa":"markdown","26f9c52a":"markdown","515cf62d":"markdown","e737a18d":"markdown","819bb53d":"markdown","16f96df4":"markdown","c1a627d0":"markdown","405148de":"markdown","7ca2136e":"markdown","c8ec403a":"markdown","e94272e0":"markdown","f80e99d4":"markdown","03824511":"markdown","1a58ed4d":"markdown","3c950f0a":"markdown","68403463":"markdown","18656eef":"markdown","044f8745":"markdown","07454974":"markdown","284b8619":"markdown","3b56468e":"markdown","bcf1e741":"markdown","26de6b67":"markdown","6a12d936":"markdown","ea6a2b5b":"markdown","8f0fcc38":"markdown","13a3aa11":"markdown"},"source":{"a99fc32c":"# We can use the pandas library in python to read in the csv file.\nimport pandas as pd\n#for numerical computaions we can use numpy library\nimport numpy as np","00951d26":"# This creates a pandas dataframe and assigns it to the titanic variable.\ntitanic = pd.read_csv(\"..\/input\/train.csv\")\n# Print the first 5 rows of the dataframe.\ntitanic.head()","f49577c4":"titanic_test = pd.read_csv(\"..\/input\/test.csv\")\n#transpose\ntitanic_test.head().T\n#note their is no Survived column here which is our target varible we are trying to predict","b315a340":"#shape command will give number of rows\/samples\/examples and number of columns\/features\/predictors in dataset\n#(rows,columns)\ntitanic.shape","c5fbab0d":"#Describe gives statistical information about numerical columns in the dataset\ntitanic.describe()\n#you can check from count if there are missing vales in columns, here age has got missing values","15801ca4":"#info method provides information about dataset like \n#total values in each column, null\/not null, datatype, memory occupied etc\ntitanic.info()","cc06a362":"#lets see if there are any more columns with missing values \nnull_columns=titanic.columns[titanic.isnull().any()]\ntitanic.isnull().sum()","848fd114":"#how about test set??\ntitanic_test.isnull().sum()","a22ee66b":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1)\n\npd.options.display.mpl_style = 'default'\nlabels = []\nvalues = []\nfor col in null_columns:\n    labels.append(col)\n    values.append(titanic[col].isnull().sum())\nind = np.arange(len(labels))\nwidth=0.6\nfig, ax = plt.subplots(figsize=(6,5))\nrects = ax.barh(ind, np.array(values), color='purple')\nax.set_yticks(ind+((width)\/2.))\nax.set_yticklabels(labels, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_ylabel(\"Column Names\")\nax.set_title(\"Variables with missing values\");","586334b3":"titanic.hist(bins=10,figsize=(9,7),grid=False);","093b8fbf":"g = sns.FacetGrid(titanic, col=\"Sex\", row=\"Survived\", margin_titles=True)\ng.map(plt.hist, \"Age\",color=\"purple\");","7b343886":"g = sns.FacetGrid(titanic, hue=\"Survived\", col=\"Pclass\", margin_titles=True,\n                  palette={1:\"seagreen\", 0:\"gray\"})\ng=g.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend();","6184b101":"g = sns.FacetGrid(titanic, hue=\"Survived\", col=\"Sex\", margin_titles=True,\n                palette=\"Set1\",hue_kws=dict(marker=[\"^\", \"v\"]))\ng.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend()\nplt.subplots_adjust(top=0.8)\ng.fig.suptitle('Survival by Gender , Age and Fare');","00ca4dd9":"titanic.Embarked.value_counts().plot(kind='bar', alpha=0.55)\nplt.title(\"Passengers per boarding location\");","ea6d7fe8":"sns.factorplot(x = 'Embarked',y=\"Survived\", data = titanic,color=\"r\");","90703933":"sns.set(font_scale=1)\ng = sns.factorplot(x=\"Sex\", y=\"Survived\", col=\"Pclass\",\n                    data=titanic, saturation=.5,\n                    kind=\"bar\", ci=None, aspect=.6)\n(g.set_axis_labels(\"\", \"Survival Rate\")\n    .set_xticklabels([\"Men\", \"Women\"])\n    .set_titles(\"{col_name} {col_var}\")\n    .set(ylim=(0, 1))\n    .despine(left=True))  \nplt.subplots_adjust(top=0.8)\ng.fig.suptitle('How many Men and Women Survived by Passenger Class');","46171a1d":"ax = sns.boxplot(x=\"Survived\", y=\"Age\", \n                data=titanic)\nax = sns.stripplot(x=\"Survived\", y=\"Age\",\n                   data=titanic, jitter=True,\n                   edgecolor=\"gray\")\nsns.plt.title(\"Survival by Age\",fontsize=12);","df1b185b":"titanic.Age[titanic.Pclass == 1].plot(kind='kde')    \ntitanic.Age[titanic.Pclass == 2].plot(kind='kde')\ntitanic.Age[titanic.Pclass == 3].plot(kind='kde')\n # plots an axis lable\nplt.xlabel(\"Age\")    \nplt.title(\"Age Distribution within classes\")\n# sets our legend for our graph.\nplt.legend(('1st Class', '2nd Class','3rd Class'),loc='best') ;","69152074":"corr=titanic.corr()#[\"Survived\"]\nplt.figure(figsize=(10, 10))\n\nsns.heatmap(corr, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\nplt.title('Correlation between features');","83a06ed3":"#correlation of features with target variable\ntitanic.corr()[\"Survived\"]","134f9de5":"g = sns.factorplot(x=\"Age\", y=\"Embarked\",\n                    hue=\"Sex\", row=\"Pclass\",\n                    data=titanic[titanic.Embarked.notnull()],\n                    orient=\"h\", size=2, aspect=3.5, \n                   palette={'male':\"purple\", 'female':\"blue\"},\n                    kind=\"violin\", split=True, cut=0, bw=.2);","2ef8ca85":"#Lets check which rows have null Embarked column\ntitanic[titanic['Embarked'].isnull()]","2b87eb8c":"sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=titanic);","4e0055c9":"titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna('C')","2cd7de88":"#there is an empty fare column in test set\ntitanic_test.describe()","13b003d6":"titanic_test[titanic_test['Fare'].isnull()]","57128c98":"#we can replace missing value in fare by taking median of all fares of those passengers \n#who share 3rd Passenger class and Embarked from 'S' \ndef fill_missing_fare(df):\n    median_fare=df[(df['Pclass'] == 3) & (df['Embarked'] == 'S')]['Fare'].median()\n#'S'\n       #print(median_fare)\n    df[\"Fare\"] = df[\"Fare\"].fillna(median_fare)\n    return df\n\ntitanic_test=fill_missing_fare(titanic_test)","662cbed8":"titanic[\"Deck\"]=titanic.Cabin.str[0]\ntitanic_test[\"Deck\"]=titanic_test.Cabin.str[0]\ntitanic[\"Deck\"].unique() # 0 is for null values","b9fb984a":"g = sns.factorplot(\"Survived\", col=\"Deck\", col_wrap=4,\n                    data=titanic[titanic.Deck.notnull()],\n                    kind=\"count\", size=2.5, aspect=.8);","582bce4e":"titanic = titanic.assign(Deck=titanic.Deck.astype(object)).sort(\"Deck\")\ng = sns.FacetGrid(titanic, col=\"Pclass\", sharex=False,\n                  gridspec_kws={\"width_ratios\": [5, 3, 3]})\ng.map(sns.boxplot, \"Deck\", \"Age\");","d5287064":"titanic.Deck.fillna('Z', inplace=True)\ntitanic_test.Deck.fillna('Z', inplace=True)\ntitanic[\"Deck\"].unique() # Z is for null values","1348da86":"# Create a family size variable including the passenger themselves\ntitanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]+1\ntitanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]+1\nprint(titanic[\"FamilySize\"].value_counts())","111b5fa1":"# Discretize family size\ntitanic.loc[titanic[\"FamilySize\"] == 1, \"FsizeD\"] = 'singleton'\ntitanic.loc[(titanic[\"FamilySize\"] > 1)  &  (titanic[\"FamilySize\"] < 5) , \"FsizeD\"] = 'small'\ntitanic.loc[titanic[\"FamilySize\"] >4, \"FsizeD\"] = 'large'\n\ntitanic_test.loc[titanic_test[\"FamilySize\"] == 1, \"FsizeD\"] = 'singleton'\ntitanic_test.loc[(titanic_test[\"FamilySize\"] >1) & (titanic_test[\"FamilySize\"] <5) , \"FsizeD\"] = 'small'\ntitanic_test.loc[titanic_test[\"FamilySize\"] >4, \"FsizeD\"] = 'large'\nprint(titanic[\"FsizeD\"].unique())\nprint(titanic[\"FsizeD\"].value_counts())","5b14eb8d":"sns.factorplot(x=\"FsizeD\", y=\"Survived\", data=titanic);","4b9e0438":"#Create feture for length of name \n# The .apply method generates a new series\ntitanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))\n\ntitanic_test[\"NameLength\"] = titanic_test[\"Name\"].apply(lambda x: len(x))\n#print(titanic[\"NameLength\"].value_counts())\n\nbins = [0, 20, 40, 57, 85]\ngroup_names = ['short', 'okay', 'good', 'long']\ntitanic['NlengthD'] = pd.cut(titanic['NameLength'], bins, labels=group_names)\ntitanic_test['NlengthD'] = pd.cut(titanic_test['NameLength'], bins, labels=group_names)\n\nsns.factorplot(x=\"NlengthD\", y=\"Survived\", data=titanic)\nprint(titanic[\"NlengthD\"].unique())","71e19142":"import re\n\n#A function to get the title from a name.\ndef get_title(name):\n    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    #If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n#Get all the titles and print how often each one occurs.\ntitles = titanic[\"Name\"].apply(get_title)\nprint(pd.value_counts(titles))\n\n\n#Add in the title column.\ntitanic[\"Title\"] = titles\n\n# Titles with very low cell counts to be combined to \"rare\" level\nrare_title = ['Dona', 'Lady', 'Countess','Capt', 'Col', 'Don', \n                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']\n\n# Also reassign mlle, ms, and mme accordingly\ntitanic.loc[titanic[\"Title\"] == \"Mlle\", \"Title\"] = 'Miss'\ntitanic.loc[titanic[\"Title\"] == \"Ms\", \"Title\"] = 'Miss'\ntitanic.loc[titanic[\"Title\"] == \"Mme\", \"Title\"] = 'Mrs'\ntitanic.loc[titanic[\"Title\"] == \"Dona\", \"Title\"] = 'Rare Title'\ntitanic.loc[titanic[\"Title\"] == \"Lady\", \"Title\"] = 'Rare Title'\ntitanic.loc[titanic[\"Title\"] == \"Countess\", \"Title\"] = 'Rare Title'\ntitanic.loc[titanic[\"Title\"] == \"Capt\", \"Title\"] = 'Rare Title'\ntitanic.loc[titanic[\"Title\"] == \"Col\", \"Title\"] = 'Rare Title'\ntitanic.loc[titanic[\"Title\"] == \"Don\", \"Title\"] = 'Rare Title'\ntitanic.loc[titanic[\"Title\"] == \"Major\", \"Title\"] = 'Rare Title'\ntitanic.loc[titanic[\"Title\"] == \"Rev\", \"Title\"] = 'Rare Title'\ntitanic.loc[titanic[\"Title\"] == \"Sir\", \"Title\"] = 'Rare Title'\ntitanic.loc[titanic[\"Title\"] == \"Jonkheer\", \"Title\"] = 'Rare Title'\ntitanic.loc[titanic[\"Title\"] == \"Dr\", \"Title\"] = 'Rare Title'\n\n#titanic.loc[titanic[\"Title\"].isin(['Dona', 'Lady', 'Countess','Capt', 'Col', 'Don', \n#                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']), \"Title\"] = 'Rare Title'\n\n#titanic[titanic['Title'].isin(['Dona', 'Lady', 'Countess'])]\n#titanic.query(\"Title in ('Dona', 'Lady', 'Countess')\")\n\ntitanic[\"Title\"].value_counts()\n\n\ntitles = titanic_test[\"Name\"].apply(get_title)\nprint(pd.value_counts(titles))\n\n#Add in the title column.\ntitanic_test[\"Title\"] = titles\n\n# Titles with very low cell counts to be combined to \"rare\" level\nrare_title = ['Dona', 'Lady', 'Countess','Capt', 'Col', 'Don', \n                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']\n\n# Also reassign mlle, ms, and mme accordingly\ntitanic_test.loc[titanic_test[\"Title\"] == \"Mlle\", \"Title\"] = 'Miss'\ntitanic_test.loc[titanic_test[\"Title\"] == \"Ms\", \"Title\"] = 'Miss'\ntitanic_test.loc[titanic_test[\"Title\"] == \"Mme\", \"Title\"] = 'Mrs'\ntitanic_test.loc[titanic_test[\"Title\"] == \"Dona\", \"Title\"] = 'Rare Title'\ntitanic_test.loc[titanic_test[\"Title\"] == \"Lady\", \"Title\"] = 'Rare Title'\ntitanic_test.loc[titanic_test[\"Title\"] == \"Countess\", \"Title\"] = 'Rare Title'\ntitanic_test.loc[titanic_test[\"Title\"] == \"Capt\", \"Title\"] = 'Rare Title'\ntitanic_test.loc[titanic_test[\"Title\"] == \"Col\", \"Title\"] = 'Rare Title'\ntitanic_test.loc[titanic_test[\"Title\"] == \"Don\", \"Title\"] = 'Rare Title'\ntitanic_test.loc[titanic_test[\"Title\"] == \"Major\", \"Title\"] = 'Rare Title'\ntitanic_test.loc[titanic_test[\"Title\"] == \"Rev\", \"Title\"] = 'Rare Title'\ntitanic_test.loc[titanic_test[\"Title\"] == \"Sir\", \"Title\"] = 'Rare Title'\ntitanic_test.loc[titanic_test[\"Title\"] == \"Jonkheer\", \"Title\"] = 'Rare Title'\ntitanic_test.loc[titanic_test[\"Title\"] == \"Dr\", \"Title\"] = 'Rare Title'\n\ntitanic_test[\"Title\"].value_counts()","403b1c09":"titanic[\"Ticket\"].tail()","8ef1f2f5":"titanic[\"TicketNumber\"] = titanic[\"Ticket\"].str.extract('(\\d{2,})', expand=True)\ntitanic[\"TicketNumber\"] = titanic[\"TicketNumber\"].apply(pd.to_numeric)\n\n\ntitanic_test[\"TicketNumber\"] = titanic_test[\"Ticket\"].str.extract('(\\d{2,})', expand=True)\ntitanic_test[\"TicketNumber\"] = titanic_test[\"TicketNumber\"].apply(pd.to_numeric)","e836aa15":"#some rows in ticket column dont have numeric value so we got NaN there\ntitanic[titanic[\"TicketNumber\"].isnull()]","4f2ce88d":"titanic.TicketNumber.fillna(titanic[\"TicketNumber\"].median(), inplace=True)\ntitanic_test.TicketNumber.fillna(titanic_test[\"TicketNumber\"].median(), inplace=True)","27232175":"from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n\nlabelEnc=LabelEncoder()\n\ncat_vars=['Embarked','Sex',\"Title\",\"FsizeD\",\"NlengthD\",'Deck']\nfor col in cat_vars:\n    titanic[col]=labelEnc.fit_transform(titanic[col])\n    titanic_test[col]=labelEnc.fit_transform(titanic_test[col])\n\ntitanic.head()","97b605e7":"with sns.plotting_context(\"notebook\",font_scale=1.5):\n    sns.set_style(\"whitegrid\")\n    sns.distplot(titanic[\"Age\"].dropna(),\n                 bins=80,\n                 kde=False,\n                 color=\"red\")\n    sns.plt.title(\"Age Distribution\")\n    plt.ylabel(\"Count\");","726663c5":"from sklearn.ensemble import RandomForestRegressor\n#predicting missing values in age using Random Forest\ndef fill_missing_age(df):\n    \n    #Feature set\n    age_df = df[['Age','Embarked','Fare', 'Parch', 'SibSp',\n                 'TicketNumber', 'Title','Pclass','FamilySize',\n                 'FsizeD','NameLength',\"NlengthD\",'Deck']]\n    # Split sets into train and test\n    train  = age_df.loc[ (df.Age.notnull()) ]# known Age values\n    test = age_df.loc[ (df.Age.isnull()) ]# null Ages\n    \n    # All age values are stored in a target array\n    y = train.values[:, 0]\n    \n    # All the other values are stored in the feature array\n    X = train.values[:, 1::]\n    \n    # Create and fit a model\n    rtr = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\n    rtr.fit(X, y)\n    \n    # Use the fitted model to predict the missing values\n    predictedAges = rtr.predict(test.values[:, 1::])\n    \n    # Assign those predictions to the full data set\n    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges \n    \n    return df","7767567c":"titanic=fill_missing_age(titanic)\ntitanic_test=fill_missing_age(titanic_test)","843eaf3b":"with sns.plotting_context(\"notebook\",font_scale=1.5):\n    sns.set_style(\"whitegrid\")\n    sns.distplot(titanic[\"Age\"].dropna(),\n                 bins=80,\n                 kde=False,\n                 color=\"tomato\")\n    sns.plt.title(\"Age Distribution\")\n    plt.ylabel(\"Count\")\n    plt.xlim((15,100));","b41ee60f":"from sklearn import preprocessing\n\nstd_scale = preprocessing.StandardScaler().fit(titanic[['Age', 'Fare']])\ntitanic[['Age', 'Fare']] = std_scale.transform(titanic[['Age', 'Fare']])\n\n\nstd_scale = preprocessing.StandardScaler().fit(titanic_test[['Age', 'Fare']])\ntitanic_test[['Age', 'Fare']] = std_scale.transform(titanic_test[['Age', 'Fare']])","706316ec":"titanic.corr()[\"Survived\"]","05c8fa4b":"# Import the linear regression class\nfrom sklearn.linear_model import LinearRegression\n# Sklearn also has a helper that makes it easy to do cross validation\nfrom sklearn.cross_validation import KFold\n\n# The columns we'll use to predict the target\npredictors = [\"Pclass\", \"Sex\", \"Age\",\"SibSp\", \"Parch\", \"Fare\",\n              \"Embarked\",\"NlengthD\", \"FsizeD\", \"Title\",\"Deck\"]\ntarget=\"Survived\"\n# Initialize our algorithm class\nalg = LinearRegression()\n\n# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n# We set random_state to ensure we get the same splits every time we run this.\nkf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n\npredictions = []","48849fc7":"for train, test in kf:\n    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n    train_predictors = (titanic[predictors].iloc[train,:])\n    # The target we're using to train the algorithm.\n    train_target = titanic[target].iloc[train]\n    # Training the algorithm using the predictors and target.\n    alg.fit(train_predictors, train_target)\n    # We can now make predictions on the test fold\n    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n    predictions.append(test_predictions)","a6daf29a":"predictions = np.concatenate(predictions, axis=0)\n# Map predictions to outcomes (only possible outcomes are 1 and 0)\npredictions[predictions > .5] = 1\npredictions[predictions <=.5] = 0\n\n\naccuracy=sum(titanic[\"Survived\"]==predictions)\/len(titanic[\"Survived\"])\naccuracy","d4c98cd7":"from sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\n\npredictors = [\"Pclass\", \"Sex\", \"Fare\", \"Embarked\",\"Deck\",\"Age\",\n              \"FsizeD\", \"NlengthD\",\"Title\",\"Parch\"]\n\n# Initialize our algorithm\nlr = LogisticRegression(random_state=1)\n# Compute the accuracy score for all the cross validation folds.\ncv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=50)\n\nscores = cross_val_score(lr, titanic[predictors], \n                                          titanic[\"Survived\"],scoring='f1', cv=cv)\n# Take the mean of the scores (because we have one for each fold)\nprint(scores.mean())","106fda3d":"from sklearn import cross_validation\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cross_validation import KFold\nfrom sklearn.model_selection import cross_val_predict\n\nimport numpy as np\npredictors = [\"Pclass\", \"Sex\", \"Age\",\n              \"Fare\",\"NlengthD\",\"NameLength\", \"FsizeD\", \"Title\",\"Deck\"]\n\n# Initialize our algorithm with the default paramters\n# n_estimators is the number of trees we want to make\n# min_samples_split is the minimum number of rows we need to make a split\n# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\nrf = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, \n                            min_samples_leaf=1)\nkf = KFold(titanic.shape[0], n_folds=5, random_state=1)\ncv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=50)\n\npredictions = cross_validation.cross_val_predict(rf, titanic[predictors],titanic[\"Survived\"],cv=kf)\npredictions = pd.Series(predictions)\nscores = cross_val_score(rf, titanic[predictors], titanic[\"Survived\"],\n                                          scoring='f1', cv=kf)\n# Take the mean of the scores (because we have one for each fold)\nprint(scores.mean())","05f58daa":"predictors = [\"Pclass\", \"Sex\", \"Age\",\n              \"Fare\",\"NlengthD\",\"NameLength\", \"FsizeD\", \"Title\",\"Deck\",\"TicketNumber\"]\nrf = RandomForestClassifier(random_state=1, n_estimators=50, max_depth=9,min_samples_split=6, min_samples_leaf=4)\nrf.fit(titanic[predictors],titanic[\"Survived\"])\nkf = KFold(titanic.shape[0], n_folds=5, random_state=1)\npredictions = cross_validation.cross_val_predict(rf, titanic[predictors],titanic[\"Survived\"],cv=kf)\npredictions = pd.Series(predictions)\nscores = cross_val_score(rf, titanic[predictors], titanic[\"Survived\"],scoring='f1', cv=kf)\n# Take the mean of the scores (because we have one for each fold)\nprint(scores.mean())","59c077e9":"importances=rf.feature_importances_\nstd = np.std([rf.feature_importances_ for tree in rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\nsorted_important_features=[]\nfor i in indices:\n    sorted_important_features.append(predictors[i])\n#predictors=titanic.columns\nplt.figure()\nplt.title(\"Feature Importances By Random Forest Model\")\nplt.bar(range(np.size(predictors)), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(np.size(predictors)), sorted_important_features, rotation='vertical')\n\nplt.xlim([-1, np.size(predictors)]);","2880b347":"import numpy as np\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.cross_validation import KFold\n%matplotlib inline\nimport matplotlib.pyplot as plt\n#predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\",\n #             \"FsizeD\", \"Embarked\", \"NlengthD\",\"Deck\",\"TicketNumber\"]\npredictors = [\"Pclass\", \"Sex\", \"Age\",\n              \"Fare\",\"NlengthD\", \"FsizeD\",\"NameLength\",\"Deck\",\"Embarked\"]\n# Perform feature selection\nselector = SelectKBest(f_classif, k=5)\nselector.fit(titanic[predictors], titanic[\"Survived\"])\n\n# Get the raw p-values for each feature, and transform from p-values into scores\nscores = -np.log10(selector.pvalues_)\n\nindices = np.argsort(scores)[::-1]\n\nsorted_important_features=[]\nfor i in indices:\n    sorted_important_features.append(predictors[i])\n\nplt.figure()\nplt.title(\"Feature Importances By SelectKBest\")\nplt.bar(range(np.size(predictors)), scores[indices],\n       color=\"seagreen\", yerr=std[indices], align=\"center\")\nplt.xticks(range(np.size(predictors)), sorted_important_features, rotation='vertical')\n\nplt.xlim([-1, np.size(predictors)]);","a308adf4":"from sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\npredictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\",\"NlengthD\",\n              \"FsizeD\", \"Title\",\"Deck\"]\n\n# Initialize our algorithm\nlr = LogisticRegression(random_state=1)\n# Compute the accuracy score for all the cross validation folds.  \ncv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=50)\nscores = cross_val_score(lr, titanic[predictors], titanic[\"Survived\"], scoring='f1',cv=cv)\nprint(scores.mean())","88135536":"from sklearn.ensemble import AdaBoostClassifier\npredictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\",\"NlengthD\",\n              \"FsizeD\", \"Title\",\"Deck\",\"TicketNumber\"]\nadb=AdaBoostClassifier()\nadb.fit(titanic[predictors],titanic[\"Survived\"])\ncv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=50)\nscores = cross_val_score(adb, titanic[predictors], titanic[\"Survived\"], scoring='f1',cv=cv)\nprint(scores.mean())","8fb4a69e":"predictions=[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\",\"NlengthD\",\n              \"FsizeD\", \"Title\",\"Deck\",\"NameLength\",\"TicketNumber\"]\nfrom sklearn.ensemble import VotingClassifier\neclf1 = VotingClassifier(estimators=[\n        ('lr', lr), ('rf', rf), ('adb', adb)], voting='soft')\neclf1 = eclf1.fit(titanic[predictors], titanic[\"Survived\"])\npredictions=eclf1.predict(titanic[predictors])\npredictions\n\ntest_predictions=eclf1.predict(titanic_test[predictors])\n\ntest_predictions=test_predictions.astype(int)\nsubmission = pd.DataFrame({\n        \"PassengerId\": titanic_test[\"PassengerId\"],\n        \"Survived\": test_predictions\n    })\n\nsubmission.to_csv(\"titanic_submission.csv\", index=False)","b3b4f673":"**yes even Embarked and cabin has missing values.**","3ec80ed8":"Load train & test data\n======================","a86824b8":"## Introduction\n\n**_Poonam Ligade_**\n\n*27th Dec 2016*\n\nI am are trying to find out how many people on titanic survived from disaster.\n\nHere goes Titanic Survival Prediction End to End ML Pipeline  \n\n 1) **Introduction**\n\n 1. Import Libraries\n 2. Load data\n 3. Run Statistical summeries\n 4. Figure out missing value columns\n\n \n \n2) **Visualizations**\n\n 1. Correlation with target variable\n\n\n3) **Missing values imputation**\n\n 1. train data Missing columns- Embarked,Age,Cabin\n 2. test data Missing columns- Age and Fare\n \n\n4) **Feature Engineering**\n\n 1. Calculate total family size\n 2. Get title from name\n 3. Find out which deck passenger belonged to\n 4. Dealing with Categorical Variables\n     * Label encoding\n 5. Feature Scaling\n\n\n5) **Prediction**\n\n 1. Split into training & test sets\n 2. Build the model\n 3. Feature importance\n 4. Predictions\n 5. Ensemble : Majority voting\n\n6) **Submission**","3da267a2":"**Age, Fare and cabin has missing values.\nwe will see how to fill missing values next.**","33e25ed7":"**we can see that Age and Fare are measured on very different scaling. So we need to do feature scaling before predictions.**","e0434871":"***Fare Column***","d5883afa":"Correlation of features with target \n=======================","26f9c52a":"***Deck- Where exactly were passenger on the ship?***","515cf62d":"***Hope you find it useful. :)please upvote***","e737a18d":"***Ticket column***","819bb53d":"Important features\n==================","16f96df4":"*Random Forest *\n-------------------","c1a627d0":"We can see that for ***1st class*** median line is coming around ***fare $80*** for ***embarked*** value ***'C'***.\nSo we can replace NA values in Embarked column with 'C'","405148de":"***To do: stacking!. Watch this space\u2026***","7ca2136e":"*Linear Regression*\n-------------------","c8ec403a":"***How Big is your family?***","e94272e0":"**PassengerId 62 and 830** have missing embarked values\n\nBoth have ***Passenger class 1*** and ***fare $80.***\n\nLets plot a graph to visualize and try to guess from where they embarked","f80e99d4":"***Age Column***\n\nAge seems to be promising feature.\nSo it doesnt make sense to simply fill null values out with median\/mean\/mode.\n\nWe will use ***Random Forest*** algorithm to predict ages. ","03824511":"**Feature Scaling**\n===============\n\nWe can see that Age, Fare are measured on different scales, so we need to do Feature Scaling first before we proceed with predictions.","1a58ed4d":"**Embarked Column**","3c950f0a":"***Whats in the name?***","68403463":"Import libraries\n================","18656eef":"Missing Value Imputation\n========================\n\n**Its important to fill missing values, because some machine learning algorithms can't accept them eg SVM.**\n\n*But filling missing values with mean\/median\/mode is also a prediction which may not be 100% accurate, instead you can use models like Decision Trees and Random Forest which handle missing values very well.*","044f8745":"**Visualizations**\n==============","07454974":"Feature Engineering\n===================","284b8619":"*Logistic Regression*\n-------------------","3b56468e":"**Looks like Pclass has got highest negative correlation with \"Survived\" followed by Fare, Parch and Age** ","bcf1e741":"Convert Categorical variables into Numerical ones\n=================================================","26de6b67":"*AdaBoost *\n--------------------","6a12d936":"***Do you have longer names?***","ea6a2b5b":"Predict Survival\n================","8f0fcc38":"Maximum Voting ensemble and Submission\n=======","13a3aa11":"*Gradient Boosting*\n-------------------"}}