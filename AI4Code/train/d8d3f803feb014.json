{"cell_type":{"87ec80aa":"code","c870721f":"code","e5a830ea":"code","ca15ff27":"code","6e114561":"code","2a7cd52c":"code","60322b8d":"code","4dfea5ce":"code","64afe0a7":"code","5766a3b1":"code","cf53dc8d":"code","72052b37":"code","9c66a8cb":"code","f4a76739":"code","9d32dbe2":"code","66fe978e":"code","d2c1cff0":"markdown","a7fed8d2":"markdown"},"source":{"87ec80aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c870721f":"train=pd.read_csv('\/kaggle\/input\/encoded_train.csv')\ntest=pd.read_csv('\/kaggle\/input\/Test_health.csv')\nsub = pd.read_csv('\/kaggle\/input\/ss_health.csv')","e5a830ea":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Any results you write to the current directory are saved as output.\nimport pandas as pd\nimport re\n","ca15ff27":"# Replace all numeric with 'n'\nreplace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n\nCOMMENT_COL = 'text'\nID_COL = 'ID'\ninput_dir = '..\/input\/'\noutput_dir = '..\/input\/'\n\n# redundancy words and their right formats\nredundancy_rightFormat = {\n    'ckckck': 'cock',\n    'fuckfuck': 'fuck',\n    'lolol': 'lol',\n    'lollol': 'lol',\n    'pussyfuck':'fuck',\n    'gaygay': 'gay',\n    'haha': 'ha',\n    'sucksuck': 'suck'}\n\nredundancy = set(redundancy_rightFormat.keys())","6e114561":"# all the words below are included in glove dictionary\n# combine these toxic indicators with 'CommProcess.revise_triple_and_more_letters'\nDepression_indicator_words = ['sad','depression','alone','hopeless','Sadness','hatred','situation','stressed',\n    'emotionally', 'psycologically','down','sad','lonely','low', 'bad', 'feeling',\n    'challenges','unworthy', 'wasted','Challenging']\n\nDepression_indicator_words_sets = set(Depression_indicator_words)\n\n\ndef _get_DepressionIndicator_transformers():\n    DepressionIndicator_transformers = dict()\n    for word in Depression_indicator_words:\n        tmp_1 = []\n        for c in word:\n            if len(tmp_1) > 0:\n                tmp_2 = []\n                for pre in tmp_1:\n                    tmp_2.append(pre + c)\n                    tmp_2.append(pre + c + c)\n                tmp_1 = tmp_2\n            else:\n                tmp_1.append(c)\n                tmp_1.append(c + c)\n        DepressionIndicator_transformers[word] = tmp_1\n    return DepressionIndicator_transformers\n\n\nDepressionIndicator_transformers = _get_DepressionIndicator_transformers()\n","2a7cd52c":"deny_origin = {\n    \"you're\": ['you', 'are'],\n    \"i'm\": ['i', 'am'],\n    \"he's\": ['he', 'is'],\n    \"she's\": ['she', 'is'],\n    \"it's\": ['it', 'is'],\n    \"they're\": ['they', 'are'],\n    \"can't\": ['can', 'not'],\n    \"couldn't\": ['could', 'not'],\n    \"don't\": ['do', 'not'],\n    \"don;t\": ['do', 'not'],\n    \"didn't\": ['did', 'not'],\n    \"doesn't\": ['does', 'not'],\n    \"isn't\": ['is', 'not'],\n    \"wasn't\": ['was', 'not'],\n    \"aren't\": ['are', 'not'],\n    \"weren't\": ['were', 'not'],\n    \"won't\": ['will', 'not'],\n    \"wouldn't\": ['would', 'not'],\n    \"hasn't\": ['has', 'not'],\n    \"haven't\": ['have', 'not'],\n    \"what's\": ['what', 'is'],\n    \"that's\": ['that', 'is'],\n}\ndenies = set(deny_origin.keys())","60322b8d":"class CommProcess(object):\n    @staticmethod\n    def clean_text(t):\n        t = re.sub(r\"[^A-Za-z0-9,!?*.;\u2019\u00b4'\\\/]\", \" \", t)\n        t = replace_numbers.sub(\" \", t)\n        t = t.lower()\n        t = re.sub(r\",\", \" \", t)\n        t = re.sub(r\"\u2019\", \"'\", t)\n        t = re.sub(r\"\u00b4\", \"'\", t)\n        t = re.sub(r\"\\.\", \" \", t)\n        t = re.sub(r\"!\", \" ! \", t)\n        t = re.sub(r\"\\?\", \" ? \", t)\n        t = re.sub(r\"\\\/\", \" \", t)\n        return t\n\n    @staticmethod\n    def revise_deny(t):\n        ret = []\n        for word in t.split():\n            if word in denies:\n                ret.append(deny_origin[word][0])\n                ret.append(deny_origin[word][1])\n            else:\n                ret.append(word)\n        ret = ' '.join(ret)\n        ret = re.sub(\"'\", \" \", ret)\n        ret = re.sub(r\";\", \" \", ret)\n        return ret\n\n    @staticmethod\n    def revise_star(t):\n        ret = []\n        for word in t.split():\n            if ('*' in word) and (re.sub('\\*', '', word) in Depression_indicator_words_sets):\n                word = re.sub('\\*', '', word)\n            ret.append(word)\n        ret = re.sub('\\*', ' ', ' '.join(ret))\n        return ret\n\n    @staticmethod\n    def revise_triple_and_more_letters(t):\n        for letter in 'abcdefghijklmnopqrstuvwxyz':\n            reg = letter + \"{2,}\"\n            t = re.sub(reg, letter + letter, t)\n        return t\n\n    @staticmethod\n    def revise_redundancy_words(t):\n        ret = []\n        for word in t.split(' '):\n            for redu in redundancy:\n                if redu in word:\n                    word = redundancy_rightFormat[redu]\n                    break\n            ret.append(word)\n        return ' '.join(ret)\n\n    @staticmethod\n    def fill_na(t):\n        if t.strip() == '':\n            return 'NA'\n        return t\n\n\ndef execute_comm_process(df):\n    comm_process_pipeline = [\n        CommProcess.clean_text,\n        CommProcess.revise_deny,\n        CommProcess.revise_star,\n        CommProcess.revise_triple_and_more_letters,\n        CommProcess.revise_redundancy_words,\n        CommProcess.fill_na,\n    ]\n    for cp in comm_process_pipeline:\n        df[COMMENT_COL] = df[COMMENT_COL].apply(cp)\n    return df","4dfea5ce":"# Process whole train data\nprint('Comm has processed whole train data')\ndf_train = pd.read_csv('\/kaggle\/input\/encoded_train.csv')\ndf_train = execute_comm_process(df_train)\ndf_train.to_csv('train_processed.csv', index=False)\n\n# Process test data\nprint('Comm has processed whole test data')\ndf_test = pd.read_csv('\/kaggle\/input\/Test_health.csv')\ndf_test = execute_comm_process(df_test)\ndf_test.to_csv('test_processed.csv', index=False)","64afe0a7":"import numpy as np\nnp.random.seed(42)\nimport pandas as pd\nimport string\nimport re\n\nimport gensim\nfrom collections import Counter\nimport pickle\n\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout, Conv1D, Embedding, SpatialDropout1D, concatenate\nfrom keras.layers import GRU, LSTM,Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.layers import CuDNNLSTM, CuDNNGRU\nfrom keras.preprocessing import text, sequence\n\nfrom keras.callbacks import Callback\nfrom keras import optimizers\nfrom keras.layers import Lambda\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom nltk.corpus import stopwords\n\nimport os\nos.environ['OMP_NUM_THREADS'] = '4'\n\nimport gc\nfrom keras import backend as K\nfrom sklearn.model_selection import KFold\n\nfrom unidecode import unidecode\n\nimport time\n\neng_stopwords = set(stopwords.words(\"english\"))","5766a3b1":"# 1. preprocessing\ntrain = pd.read_csv(\"train_processed.csv\")\ntest = pd.read_csv(\"test_processed.csv\")","cf53dc8d":"#2.  remove non-ascii\n\nspecial_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\ndef clean_text(x):\n    x_ascii = unidecode(x)\n    x_clean = special_character_removal.sub('',x_ascii)\n    return x_clean\n\ntrain['clean_text'] = train['text'].apply(lambda x: clean_text(str(x)))\ntest['clean_text'] = test['text'].apply(lambda x: clean_text(str(x)))\n\nX_train = train['clean_text'].fillna(\"something\").values\ny_train = train[[\"Depression\", \"Alcohol\", \"Suicide\", \"Drugs\"]].values\nX_test = test['clean_text'].fillna(\"something\").values\n","72052b37":"def add_features(df):\n    \n    df['text'] = df['text'].apply(lambda x:str(x))\n    df['total_length'] = df['text'].apply(len)\n    df['capitals'] = df['text'].apply(lambda text: sum(1 for c in text if c.isupper()))\n    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])\/float(row['total_length']),\n                                axis=1)\n    df['num_words'] = df.text.str.count('\\S+')\n    df['num_unique_words'] = df['text'].apply(lambda text: len(set(w for w in text.split())))\n    df['words_vs_unique'] = df['num_unique_words'] \/ df['num_words']  \n\n    return df\n\ntrain = add_features(train)\ntest = add_features(test)\n\nfeatures = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\ntest_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n\nss = StandardScaler()\nss.fit(np.vstack((features, test_features)))\nfeatures = ss.transform(features)\ntest_features = ss.transform(test_features)","9c66a8cb":"# For best score (Public: 9869, Private: 9865), change to max_features = 283759, maxlen = 900\nmax_features = 1050\nmaxlen = 128\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nX_train_sequence = tokenizer.texts_to_sequences(X_train)\nX_test_sequence = tokenizer.texts_to_sequences(X_test)\n\nx_train = sequence.pad_sequences(X_train_sequence, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test_sequence, maxlen=maxlen)\nprint(len(tokenizer.word_index))","f4a76739":"class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n        self.max_score = 0\n        self.not_better_count = 0\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=1)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n            if (score > self.max_score):\n                print(\"*** New High Score (previous: %.6f) \\n\" % self.max_score)\n                model.save_weights(\"best_weights.h5\")\n                self.max_score=score\n                self.not_better_count = 0\n            else:\n                self.not_better_count += 1\n                if self.not_better_count > 3:\n                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n                    self.model.stop_training = True","9d32dbe2":"def get_model(features,clipvalue=1.,num_filters=40,dropout=0.5,embed_size=501):\n    features_input = Input(shape=(features.shape[1],))\n    inp = Input(shape=(maxlen, ))\n    \n    # Layer 1: concatenated fasttext and glove twitter embeddings.\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    \n    # Uncomment for best result\n    # Layer 2: SpatialDropout1D(0.5)\n    #x = SpatialDropout1D(dropout)(x)\n    \n    # Uncomment for best result\n    # Layer 3: Bidirectional CuDNNLSTM\n    #x = Bidirectional(LSTM(num_filters, return_sequences=True))(x)\n\n\n    # Layer 4: Bidirectional CuDNNGRU\n    x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True))(x)  \n    \n    # Layer 5: A concatenation of the last state, maximum pool, average pool and \n    # two features: \"Unique words rate\" and \"Rate of all-caps words\"\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    \n    x = concatenate([avg_pool, x_h, max_pool,features_input])\n    \n    # Layer 6: output dense layer.\n    outp = Dense(4, activation=\"sigmoid\")(x)\n\n    model = Model(inputs=[inp,features_input], outputs=outp)\n    adam = optimizers.adam(clipvalue=clipvalue)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=adam,\n                  metrics=['accuracy'])\n    return model","66fe978e":"model = get_model(features)\n\nbatch_size = 32\n\n# Used epochs=100 with early exiting for best score.\nepochs = 100\ngc.collect()\nK.clear_session()\n\n# Change to 10\nnum_folds = 10 #number of folds\n\npredict = np.zeros((test.shape[0],4))\n\n# Uncomment for out-of-fold predictions\n#scores = []\n#oof_predict = np.zeros((train.shape[0],6))\n\nkf = KFold(n_splits=num_folds, shuffle=True, random_state=239)\n\nfor train_index, test_index in kf.split(x_train):\n    \n    kfold_y_train,kfold_y_test = y_train[train_index], y_train[test_index]\n    kfold_X_train = x_train[train_index]\n    kfold_X_features = features[train_index]\n    kfold_X_valid = x_train[test_index]\n    kfold_X_valid_features = features[test_index] \n    \n    gc.collect()\n    K.clear_session()\n    \n    model = get_model(features)\n    \n    ra_val = RocAucEvaluation(validation_data=([kfold_X_valid,kfold_X_valid_features], kfold_y_test), interval = 1)\n    \n    model.fit([kfold_X_train,kfold_X_features], kfold_y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n             callbacks = [ra_val])\n    gc.collect()\n    \n    #model.load_weights(bst_model_path)\n    model.load_weights(\"best_weights.h5\")\n    \n    predict += model.predict([x_test,test_features], batch_size=batch_size,verbose=1) \/ num_folds\n    \n    #gc.collect()\n    # uncomment for out of fold predictions\n    #oof_predict[test_index] = model.predict([kfold_X_valid, kfold_X_valid_features],batch_size=batch_size, verbose=1)\n    #cv_score = roc_auc_score(kfold_y_test, oof_predict[test_index])\n    \n    #scores.append(cv_score)\n    #print('score: ',cv_score)\n\nprint(\"Done\")\n#print('Total CV score is {}'.format(np.mean(scores)))    \n\n\nsub = pd.read_csv('\/kaggle\/input\/ss_health.csv')\nclass_names = ['Depression', 'Alcohol', 'Suicide', 'Drugs']\nsub[class_names] = predict\nsub.to_csv('model_9872_baseline_submission.csv',index=False)\n\n# uncomment for out of fold predictions\n#oof = pd.DataFrame.from_dict({'id': train['id']})\n#for c in class_names:\n#    oof[c] = np.zeros(len(train))\n#    \n#oof[class_names] = oof_predict\n#for c in class_names:\n#    oof['prediction_' +c] = oof[c]\n#oof.to_csv('oof-model_9872_baseline_submission.csv', index=False)","d2c1cff0":"# Loading back preprocessed train and test datasets","a7fed8d2":"# Advanced Preprocessing of Train and Test datasets"}}