{"cell_type":{"68fdd4df":"code","ab94595f":"code","443358b0":"code","3fa0ee5d":"code","20c8ded3":"code","1ed4e37f":"code","54cef8eb":"code","35345bd5":"code","ad7ddb62":"code","5d7b3a47":"code","ca7ae568":"code","1e5594e7":"markdown","ca4ee790":"markdown","ae871750":"markdown","a655c772":"markdown","d8560f42":"markdown","78126241":"markdown","34cdc760":"markdown","a7423736":"markdown","31ceb170":"markdown"},"source":{"68fdd4df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ab94595f":"# Load Training Images:\n\nim_train = np.loadtxt('..\/input\/train.csv', delimiter=',', dtype=int, skiprows=1)\ntrain_labels = im_train[:, 0]\nim_train = im_train[:, 1:]\nnclasses = len(np.unique(train_labels))\nnfeatures = np.size(im_train, axis=1)\nclass_indexes = []\nfor i in range(nclasses):\n    class_indexes.append(np.argwhere(train_labels == i))","443358b0":"im_train.shape","3fa0ee5d":"# Initializing needed variables\nclass_means, other_class_means = np.empty((nclasses, nfeatures)), np.empty((nclasses, nfeatures))\nother_class = []\nSW_one, SW_two, SW = np.zeros((nclasses, nfeatures, nfeatures)), np.zeros((nclasses, nfeatures, nfeatures)), np.zeros((nclasses, nfeatures, nfeatures))\nW = np.zeros((nclasses, nfeatures, 1))\nW0 = np.zeros((nclasses))","20c8ded3":"# Calculating SW, W & W0 #\nfor i in range(nclasses):\n    class_means[i] = np.mean(im_train[class_indexes[i]], axis=0)\n    other_class.append(np.delete(im_train, class_indexes[i], axis=0)) # one-versus-the-rest approach\n    other_class_means[i] = np.mean(other_class[i], axis=0)\n    between_class1 = np.subtract(im_train[class_indexes[i]].reshape(-1, nfeatures), \n                                 class_means[i])\n    SW_one[i] = between_class1.T.dot(between_class1)\n    between_class2 = np.subtract(other_class[i], other_class_means[i])\n    SW_two[i] = between_class2.T.dot(between_class2)\n    SW[i] = SW_one[i] + SW_two[i]\n    W[i] = np.dot(np.linalg.pinv(SW[i]), \n                  np.subtract(other_class_means[i], \n                              class_means[i]).reshape(-1, 1))\n    W0[i] = -0.5 * np.dot(W[i].T, (class_means[i] + other_class_means[i]))\n","1ed4e37f":"print(SW.shape)\nprint(W.shape)\nprint(W0.shape)","54cef8eb":"im_test = np.loadtxt('..\/input\/test.csv', delimiter=',', dtype=int, skiprows=1)\nim_test.shape","35345bd5":"Y = np.zeros((len(im_test), nclasses))\npredict = np.zeros((len(im_test)), dtype=int)\nfor j in range(len(im_test)):\n    for i in range(nclasses):\n        Y[j, i] = np.dot(W[i].T,  im_test[j]) + W0[i]\n    predict[j] = np.argmin(Y[j])","ad7ddb62":"predict[:10]","5d7b3a47":"for i in range(10):\n    plt.subplot(1, 10, i+1) # plot index can not be 0\n    plt.imshow(im_test[i].reshape(28, 28))\n    plt.axis('off')\nplt.show()","ca7ae568":"submission = pd.DataFrame({\"ImageId\": np.arange(1, len(im_test)+1), \"Label\": predict})\nsubmission.to_csv('submission.csv', index=False)","1e5594e7":"SW, the within-class covariance matrix equation is as below (shouldn't be hard to understand if you break it down)\nxn are the data points, m1 is the mean of class 1 and m2 is the mean of class 2 (all other classes combined) \n<img src=\"https:\/\/drive.google.com\/uc?id=1QrpVfz8g_i0aWIHSXbU9ssdyusdCxPUv\">","ca4ee790":"Using some vectorization and matrix properties we can avoid all the looping on the dataset to create SW_one and SW_two and end up with a model that does not actually take a long time to train.","ae871750":"Sounds good, let's try to plot the first 10 testing images too.","a655c772":"So apparently it made 9 correct decision out of 10. This is very good for a linear discriminant.","d8560f42":"The weights vector, w, which is orthogonal to the decision boundy and w0 (the bias) are equal to:\n\n<img src=\"https:\/\/drive.google.com\/uc?id=1hJlV-v4nU0drdIA6VSmWga_qlBVkNmBd\">\n","78126241":"**Load the test data**:\n","34cdc760":"**What are we trying to do?**\n\nWe will be building a Fisher's Linear Discriminant from scratch, The accuracy score is not bad for a linear classifier.\nThis classifier works by trying to find the best decision boundry given that it would maximize separation between classes means while minimizing the within-class variance. \n\nThe image on the left shows a bad decision boundry while the one on the right shows a good one. Discriminant function performs dimensionality reduction, we are trying to find the line which if the data was projected on, would give us the maximum separation between classes and smallest within class variance.\n\n<img src=\"https:\/\/drive.google.com\/uc?id=14RIvA5W5rE0rfJFapE37pM_xTH9VH5de\">\n\nWe will be using the One-versus-the-rest approach for class decisions.","a7423736":"**Classification**:\n<img src=\"https:\/\/drive.google.com\/uc?id=1kMuH5FEGLuGR41-OGxmvEmsff1gCqj0B\">\n\nWe would calculate Y for every image we want to classify, every Y is a 1D array having length = number of possible classes.\nThe prediction is simply the argmin of Y (index of smallest Y)","31ceb170":"**Load the data**:\n\nJust separating the labels from the features, count the classes and features.\nStoring the indexes of every class for later use.\n"}}