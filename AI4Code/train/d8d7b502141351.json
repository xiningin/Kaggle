{"cell_type":{"58e2976b":"code","a453ec07":"code","99186304":"code","e2b1ea9a":"code","8f973efb":"code","368f4045":"code","4cbe4093":"code","2e43a1b0":"code","df552a96":"code","b3a116a5":"markdown","38f4507b":"markdown","b36f2e90":"markdown","7e267490":"markdown","3cd848ea":"markdown","7c6884ea":"markdown","2fdf3f66":"markdown","9ebf2e7e":"markdown","40ef4ea1":"markdown"},"source":{"58e2976b":"import pandas as pd\nfrom pathlib import Path\nimport os\nimport numpy as np\nfrom sklearn.model_selection import KFold\nimport gc\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input, backend as K\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Dropout\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nfrom transformers import TFBertModel, BertConfig, BertTokenizerFast","a453ec07":"class CFG:\n    \n    model_name = 'best_model'\n\n    data_dir = Path('..\/input\/commonlitreadabilityprize')\n    train_file = data_dir \/ 'train.csv'\n    test_file = data_dir \/ 'test.csv'\n    sample_file = data_dir \/ 'sample_submission.csv'\n\n    build_dir = Path('.\/build\/')\n    output_dir = build_dir \/ model_name\n    trn_encoded_file = output_dir \/ 'trn.enc.joblib'\n    val_predict_file = output_dir \/ f'{model_name}.val.txt'\n    submission_file = 'submission.csv'\n\n    pretrained_dir = '..\/input\/tfbert-large-uncased'\n\n    id_col = 'id'\n    target_col = 'target'\n    text_col = 'excerpt'\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    max_len = 205\n    n_fold = 5\n    n_est = 10\n    n_stop = 2\n    batch_size = 8\n    seed = 42","99186304":"class Tokenize:\n    \n    def load_tokenizer():\n        \n        if not os.path.exists(CFG.pretrained_dir + '\/vocab.txt'):\n            Path(CFG.pretrained_dir).mkdir(parents=True, exist_ok=True)\n            tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-uncased\")\n            tokenizer.save_pretrained(CFG.pretrained_dir)\n        else:\n            print('loading the saved pretrained tokenizer')\n            tokenizer = BertTokenizerFast.from_pretrained(CFG.pretrained_dir)\n\n        model_config = BertConfig.from_pretrained(CFG.pretrained_dir)\n        model_config.output_hidden_states = True\n        \n        return tokenizer, model_config","e2b1ea9a":"class BERT:\n    \n    def load_bert(config):\n        \n        if not os.path.exists(CFG.pretrained_dir + '\/tf_model.h5'):\n            Path(CFG.pretrained_dir).mkdir(parents=True, exist_ok=True)\n            bert_model = TFBertModel.from_pretrained(\"bert-large-uncased\", config=config)\n            bert_model.save_pretrained(CFG.pretrained_dir)\n        else:\n            print('loading the saved pretrained model')\n            bert_model = TFBertModel.from_pretrained(CFG.pretrained_dir, config=config)\n        return bert_model\n\n    def bert_encode(texts, tokenizer, max_len=CFG.max_len):\n        \n        input_ids = []\n        token_type_ids = []\n        attention_mask = []\n\n        for text in texts:\n            token = tokenizer(text, max_length=max_len, truncation=True, padding='max_length',\n                             add_special_tokens=True)\n            input_ids.append(token['input_ids'])\n            token_type_ids.append(token['token_type_ids'])\n            attention_mask.append(token['attention_mask'])\n\n        return np.array(input_ids), np.array(token_type_ids), np.array(attention_mask)","8f973efb":"class model:\n\n    def build_model(bert_model, max_len=CFG.max_len):    \n        \n        input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n        token_type_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"token_type_ids\")\n        attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\n        sequence_output = bert_model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n        clf_output = sequence_output[:, 0, :]\n        clf_output = Dropout(.1)(clf_output)\n        out = Dense(1, activation='linear')(clf_output)\n\n        model = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=out)\n        model.compile(Adam(lr=1e-5), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n\n        return model\n    \n    def scheduler(epoch, lr, warmup=5, decay_start=10):\n        \n        if epoch <= warmup:\n            return lr \/ (warmup - epoch + 1)\n        elif warmup < epoch <= decay_start:\n            return lr\n        else:\n            return lr * tf.math.exp(-.1)","368f4045":"tokenizer, bert_config = Tokenize.load_tokenizer()\n\ntest_df = pd.read_csv(CFG.test_file, index_col=CFG.id_col)\nX_test = BERT.bert_encode(test_df[CFG.text_col].values, tokenizer, max_len=CFG.max_len)","4cbe4093":"class Infer:\n    \n    def infer():\n        infer_result = np.zeros((X_test[0].shape[0], ), dtype=float)\n        cv = KFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n\n        for i, (i_trn, i_val) in enumerate(cv.split(X_test[0]), 1):\n\n            bert_model = BERT.load_bert(bert_config)\n            clf = model.build_model(bert_model, max_len=CFG.max_len)\n\n            clf.load_weights(Path(f'..\/input\/tf-bert-baseline-lb-0-646-training\/bert_v13_cv{i}.h5'))\n\n            infer_result += clf.predict(X_test).flatten() \/ CFG.n_fold\n            \n        return infer_result","2e43a1b0":"infer_result = Infer.infer()","df552a96":"sub = pd.read_csv(CFG.sample_file, index_col=CFG.id_col)\nsub[CFG.target_col] = infer_result\nsub.to_csv(CFG.submission_file)\nsub.head()","b3a116a5":"# Configs","38f4507b":"# Building Model","b36f2e90":"# Inference","7e267490":"This notebook is in continuation of my previous notebook on **training** Tf BERT based baseline model here -> https:\/\/www.kaggle.com\/prvnkmr\/tf-bert-baseline-lb-0-646-training. \n\nPlease go through that if not already done so.","3cd848ea":"<h1 style=\"border:2px solid Purple;text-align:center\">Introduction \ud83c\udf96<\/h1>","7c6884ea":"# Submission","2fdf3f66":"# Imports","9ebf2e7e":"**As always if you like the content, please do remember to upvote !! \ud83d\ude03\ud83d\ude03**","40ef4ea1":"This notebook focuses on inferencing on test data, by the model trained in our training notebook.\nRemember that we trained Multiple CV models, so we will go over all those models and take the mean output (sort of like what we do in bagging)."}}