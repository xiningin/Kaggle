{"cell_type":{"7f9d638c":"code","59c032b5":"code","ac4afb2d":"code","2f0becaa":"code","4ac9c7e1":"code","fbca341d":"code","10b4e318":"code","916bc4c3":"code","4305a58b":"code","b6d47bbf":"code","6a1a2f80":"code","f6650e47":"code","5dbb9b35":"code","303e27ca":"code","b7e49633":"code","b3045cdf":"code","100df582":"markdown","4b63a599":"markdown"},"source":{"7f9d638c":"!conda remove -y greenlet\n!pip install pytorch-pretrained-bert\n!pip install allennlp\n!pip install https:\/\/github.com\/ceshine\/pytorch_helper_bot\/archive\/master.zip","59c032b5":"!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-development.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-test.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-validation.tsv -q","ac4afb2d":"import os\n\n# This variable is used by helperbot to make the training deterministic\nos.environ[\"SEED\"] = \"19543\"\n\nimport logging\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\nfrom allennlp.modules.span_extractors import SelfAttentiveSpanExtractor\n\nfrom helperbot import BaseBot, TriangularLR","2f0becaa":"BERT_MODEL = 'bert-base-uncased'\nCASED = False","4ac9c7e1":"class Head(nn.Module):\n    \"\"\"The MLP submodule\"\"\"\n    def __init__(self, bert_hidden_size: int):\n        super().__init__()\n        self.bert_hidden_size = bert_hidden_size\n        self.span_extractor = SelfAttentiveSpanExtractor(bert_hidden_size)\n        self.fc = nn.Sequential(\n            nn.BatchNorm1d(bert_hidden_size * 3),\n            nn.Dropout(0.1),             \n            nn.Linear(bert_hidden_size * 3, 64),\n            nn.ReLU(),\n            nn.BatchNorm1d(64),\n            nn.Dropout(0.5),          \n            nn.Linear(64, 3)\n        )\n        for i, module in enumerate(self.fc):\n            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                nn.init.constant_(module.weight, 1)\n                nn.init.constant_(module.bias, 0)\n                print(\"Initing batchnorm\")\n            elif isinstance(module, nn.Linear):\n                if getattr(module, \"weight_v\", None) is not None:\n                    nn.init.uniform_(module.weight_g, 0, 1)\n                    nn.init.kaiming_normal_(module.weight_v)\n                    print(\"Initing linear with weight normalization\")\n                    assert model[i].weight_g is not None\n                else:\n                    nn.init.kaiming_normal_(module.weight)\n                    print(\"Initing linear\")\n                nn.init.constant_(module.bias, 0)\n                \n    def forward(self, bert_outputs, offsets):\n        assert bert_outputs.size(2) == self.bert_hidden_size\n        spans_contexts = self.span_extractor(\n            bert_outputs, \n            offsets[:, :4].reshape(-1, 2, 2)\n        ).reshape(-1, 2 * self.bert_hidden_size)\n        return self.fc(torch.cat([\n            spans_contexts,\n            torch.gather(\n                bert_outputs, 1,\n                offsets[:, 4:].unsqueeze(2).expand(-1, -1, self.bert_hidden_size)\n            ).squeeze(1)\n        ], dim=1))","fbca341d":"def tokenize(row, tokenizer):\n    break_points = sorted(\n        [\n            (\"A\", row[\"A-offset\"], row[\"A\"]),\n            (\"B\", row[\"B-offset\"], row[\"B\"]),\n            (\"P\", row[\"Pronoun-offset\"], row[\"Pronoun\"]),\n        ], key=lambda x: x[0]\n    )\n    tokens, spans, current_pos = [], {}, 0\n    for name, offset, text in break_points:\n        tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n        # Make sure we do not get it wrong\n        assert row[\"Text\"][offset:offset+len(text)] == text\n        # Tokenize the target\n        tmp_tokens = tokenizer.tokenize(row[\"Text\"][offset:offset+len(text)])\n        spans[name] = [len(tokens), len(tokens) + len(tmp_tokens) - 1] # inclusive\n        tokens.extend(tmp_tokens)\n        current_pos = offset + len(text)\n    tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n    assert spans[\"P\"][0] == spans[\"P\"][1]\n    return tokens, (spans[\"A\"] + spans[\"B\"] + [spans[\"P\"][0]])\n\n\nclass GAPDataset(Dataset):\n    \"\"\"Custom GAP Dataset class\"\"\"\n    def __init__(self, df, tokenizer, labeled=True):\n        self.labeled = labeled\n        if labeled:\n            tmp = df[[\"A-coref\", \"B-coref\"]].copy()\n            tmp[\"target\"] = 0\n            tmp.loc[tmp['B-coref']  == 1, \"target\"] = 1\n            tmp.loc[~(tmp['A-coref'] | tmp['B-coref']), \"target\"] = 2\n            self.y = tmp.target.values.astype(\"uint8\")\n        \n        self.offsets, self.tokens = [], []\n        for _, row in df.iterrows():\n            tokens, offsets = tokenize(row, tokenizer)\n            self.offsets.append(offsets)\n            self.tokens.append(tokenizer.convert_tokens_to_ids(\n                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n        \n    def __len__(self):\n        return len(self.tokens)\n\n    def __getitem__(self, idx):\n        if self.labeled:\n            return self.tokens[idx], self.offsets[idx], self.y[idx]\n        return self.tokens[idx], self.offsets[idx], None\n\n    \ndef collate_examples(batch, truncate_len=490):\n    \"\"\"Batch preparation.\n    \n    1. Pad the sequences\n    2. Transform the target.\n    \"\"\"    \n    transposed = list(zip(*batch))\n    max_len = min(\n        max((len(x) for x in transposed[0])),\n        truncate_len\n    )\n    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n    for i, row in enumerate(transposed[0]):\n        row = np.array(row[:truncate_len])\n        tokens[i, :len(row)] = row\n    token_tensor = torch.from_numpy(tokens)\n    # Offsets\n    offsets = torch.stack([\n        torch.LongTensor(x) for x in transposed[1]\n    ], dim=0) + 1 # Account for the [CLS] token\n    # Labels\n    if len(transposed) == 2:\n        return token_tensor, offsets, None\n    labels = torch.LongTensor(transposed[2])\n    return token_tensor, offsets, labels\n\n    \nclass GAPModel(nn.Module):\n    \"\"\"The main model.\"\"\"\n    def __init__(self, bert_model: str, device: torch.device):\n        super().__init__()\n        self.device = device\n        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n            self.bert_hidden_size = 768\n        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n            self.bert_hidden_size = 1024\n        else:\n            raise ValueError(\"Unsupported BERT model.\")\n        self.bert = BertModel.from_pretrained(bert_model).to(device)\n        self.head = Head(self.bert_hidden_size).to(device)\n    \n    def forward(self, token_tensor, offsets):\n        token_tensor = token_tensor.to(self.device)\n        bert_outputs, _ =  self.bert(\n            token_tensor, attention_mask=(token_tensor > 0).long(), \n            token_type_ids=None, output_all_encoded_layers=False)\n        head_outputs = self.head(bert_outputs, offsets.to(self.device))\n        return head_outputs            \n\n\n# Adapted from fast.ai library\ndef children(m):\n    return m if isinstance(m, (list, tuple)) else list(m.children())\n\n\ndef set_trainable_attr(m, b):\n    m.trainable = b\n    for p in m.parameters():\n        p.requires_grad = b\n\n\ndef apply_leaf(m, f):\n    c = children(m)\n    if isinstance(m, nn.Module):\n        f(m)\n    if len(c) > 0:\n        for l in c:\n            apply_leaf(l, f)\n\n            \ndef set_trainable(l, b):\n    apply_leaf(l, lambda m: set_trainable_attr(m, b))\n    \n    \nclass GAPBot(BaseBot):\n    def __init__(self, model, train_loader, val_loader, *, optimizer, clip_grad=0,\n        avg_window=100, log_dir=\".\/cache\/logs\/\", log_level=logging.INFO,\n        checkpoint_dir=\".\/cache\/model_cache\/\", batch_idx=0, echo=False,\n        device=\"cuda:0\", use_tensorboard=False):\n        super().__init__(\n            model, train_loader, val_loader, \n            optimizer=optimizer, clip_grad=clip_grad,\n            log_dir=log_dir, checkpoint_dir=checkpoint_dir, \n            batch_idx=batch_idx, echo=echo,\n            device=device, use_tensorboard=use_tensorboard\n        )\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.loss_format = \"%.4f\"\n        \n    def extract_prediction(self, tensor):\n        return tensor\n    \n    def snapshot(self):\n        \"\"\"Override the snapshot method because Kaggle kernel has limited local disk space.\"\"\"\n        loss = self.eval(self.val_loader)\n        loss_str = self.loss_format % loss\n        self.logger.info(\"Snapshot loss %s\", loss_str)\n        self.logger.tb_scalars(\n            \"losses\", {\"val\": loss},  self.step)\n        target_path = (\n            self.checkpoint_dir \/ \"best.pth\")        \n        if not self.best_performers or (self.best_performers[0][0] > loss):\n            torch.save(self.model.state_dict(), target_path)\n            self.best_performers = [(loss, target_path, self.step)]\n        self.logger.info(\"Saving checkpoint %s...\", target_path)\n        assert Path(target_path).exists()\n        return loss","10b4e318":"df_train = pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\")\ndf_val = pd.read_csv(\"gap-validation.tsv\", delimiter=\"\\t\")\ndf_test = pd.read_csv(\"gap-development.tsv\", delimiter=\"\\t\")\nsample_sub = pd.read_csv(\"..\/input\/sample_submission_stage_1.csv\")\nassert sample_sub.shape[0] == df_test.shape[0]","916bc4c3":"tokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL,\n    do_lower_case=CASED,\n    never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")\n)","4305a58b":"train_ds = GAPDataset(df_train, tokenizer)\nval_ds = GAPDataset(df_val, tokenizer)\ntest_ds = GAPDataset(df_test, tokenizer)\ntrain_loader = DataLoader(\n    train_ds,\n    collate_fn = collate_examples,\n    batch_size=32,\n    num_workers=4,\n    pin_memory=True,\n    shuffle=True,\n    drop_last=True\n)\nval_loader = DataLoader(\n    val_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)\ntest_loader = DataLoader(\n    test_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)","b6d47bbf":"model = GAPModel(BERT_MODEL, torch.device(\"cuda:0\"))\n# You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\nset_trainable(model.bert, False)\nset_trainable(model.head, True)","6a1a2f80":"optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\nbot = GAPBot(\n    model, train_loader, val_loader,\n    optimizer=optimizer, echo=True,\n    avg_window=25\n)","f6650e47":"steps_per_epoch = len(train_loader) \nn_steps = steps_per_epoch * 20\nbot.train(\n    n_steps,\n    log_interval=steps_per_epoch \/\/ 4,\n    snapshot_interval=steps_per_epoch,\n    scheduler=TriangularLR(\n        optimizer, 20, ratio=2, steps_per_cycle=n_steps)\n)","5dbb9b35":"# Load the best checkpoint\nbot.load_model(bot.best_performers[0][1])","303e27ca":"# Evaluate on the test dataset\nbot.eval(test_loader)","b7e49633":"# Extract predictions to the test dataset\npreds = bot.predict(test_loader)","b3045cdf":"# Create submission file\ndf_sub = pd.DataFrame(torch.softmax(preds, -1).cpu().numpy().clip(1e-3, 1-1e-3), columns=[\"A\", \"B\", \"NEITHER\"])\ndf_sub[\"ID\"] = df_test.ID\ndf_sub.to_csv(\"cache\/submission.csv\", index=False)\ndf_sub.head()","100df582":"This is based on [this previous kernel of mine](https:\/\/www.kaggle.com\/ceshine\/pytorch-bert-baseline-public-score-0-54). In the previous kernel only the context vectors corresponding to the first token of each name candidate were extracted and fed to the fully connected layer.\n\nHere we use [`SelfAttentiveSpanExtractor` from AllenNLP](https:\/\/github.com\/allenai\/allennlp\/blob\/580dc8b0e2c6491d4d75b54c3b15b34b462e0c67\/allennlp\/modules\/span_extractors\/self_attentive_span_extractor.py) to utilize the full span (and their context vectors) of each name candidate. (Note that the target pronouns are always tokenized into a single token, so we only need to extract one context vector per pronoun.)\n\nI have yet to observe any significant gains in accuracies from using the full spans, though (it's worse most of the time, in fact). Hyper-parameters even seem to become tricker to tune.","4b63a599":"\"pytorch_helper_bot\" is a thin abstraction of some common PyTorch training routines. It can easily be replaced, so you can mostly ignore it and focus on the preprocessing and model definition instead."}}