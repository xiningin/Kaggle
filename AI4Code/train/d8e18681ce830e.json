{"cell_type":{"88c861bc":"code","170440df":"code","a717d55f":"code","4f997dfd":"code","0107597d":"code","a97c7b37":"code","a7f449c2":"code","5ce89996":"code","606ef018":"code","e32d8387":"code","84c4c585":"code","406b13c3":"code","4c6dd74d":"code","3ca1852e":"code","f8374f5d":"code","6d52d3fc":"code","1780b6d3":"code","4027a0f2":"code","93f2f824":"code","5f3d6c39":"code","3d1cc61a":"code","b8dc77f7":"code","368d15ca":"code","76ddf674":"code","3568632d":"code","7f4188ed":"code","223837f5":"code","a9d5711d":"markdown","d6b0f721":"markdown","1281e74c":"markdown","eed33962":"markdown","90b22213":"markdown","50850807":"markdown","1904d1e3":"markdown","41482807":"markdown","9df42c0d":"markdown","80ec43a7":"markdown","2cec929d":"markdown","dfa741bb":"markdown"},"source":{"88c861bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","170440df":"#read the data\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col = 'Id')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col = 'Id')\n#show the first 5 columns of the train data\ntrain.head()","a717d55f":"# take a look of the columns of this dataframe\ntrain.columns","4f997dfd":"# assign the SalePrice \nX = train.copy()\ny = X.pop('SalePrice')","0107597d":"concat = pd.concat([X, test])","a97c7b37":"# Let's take a look at the basic information of those missing values\nmissing_value = concat.isnull().sum()\nmissing_value[0:10]\ncol_with_missing = [col for col in concat.columns if concat[col].isnull().any()]\nprint('There are ' + str(len(col_with_missing)) + ' cols with missing value.')\nprint('There are ' + str(len(concat.columns)) + ' cols at all.\\n')\nmissing_value_count = missing_value.sum()\nprint('There are '+ str(missing_value_count) + ' missing values at all.')\ntotal_count = np.product(concat.shape)\nprint('It takes ' + str(missing_value_count\/total_count) +  ' percentage of all the values.')","a7f449c2":"# Take a look at the percentage of missing values in each columns\nmissing_percentage = missing_value\/ len(concat) *100\nmissing_percentage = missing_percentage.drop(missing_percentage[missing_percentage == 0].index).sort_values(ascending = False)\nmissing_data = pd.DataFrame({'Missing ratio':missing_percentage})\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize = (15, 10))\nsns.barplot(y = missing_data.index, x = missing_data['Missing ratio'])","5ce89996":"concat[col_with_missing].isnull().sum()","606ef018":"# By seeing through all the documentation of the feature, I find that\n# For most of data, features Alley, PoolQC, Fence and Miscfeature are missing, we could drop it for simplicity\ndrop_col = ['Alley', 'PoolQC', 'Fence', 'MiscFeature']\n# For feature MasVnrType, we just fill the NA with None, because they share the same meaning\nfill_None = ['MasVnrType']\n# For feature MasVnrArea, we just fill the 0 with None, because they share the same meaning\nfill_zero = ['MasVnrArea']\n# For features LotFrontage, BsmtQuan, BsmtCond, BsmtExposure, BsmtFintype1, BsmtFintype2, Eletrical\n# GarageType, GarageYrBlt, GarageFinish, GarageQual, GarageCond, we could fillna with the most frequent value\nfill_most_frequent = ['LotFrontage', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n                      'BsmtFinType2', 'Electrical', 'GarageType', 'GarageYrBlt', 'GarageFinish',\n                      'GarageQual', 'GarageCond','FireplaceQu', 'MSZoning', 'Utilities',\n                     'Exterior1st','Exterior2nd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n                     'BsmtFullBath', 'BsmtHalfBath', 'Functional', 'GarageCars',\n                     'GarageArea', 'SaleType', 'TotalBsmtSF', 'KitchenQual']","e32d8387":"X_drop = concat.drop(drop_col, axis =1)\nX_drop[fill_None] = X_drop[fill_None].fillna('None')\nX_drop[fill_zero] = X_drop[fill_zero].fillna(0)\nX_drop[fill_most_frequent] = X_drop[fill_most_frequent].fillna(X_drop[fill_most_frequent].mode().iloc[0])\nX_train_copy = X_drop.loc[train.index,:]\nX_test_copy = X_drop.loc[test.index,:]","84c4c585":"#label encoding\n#the effect of the function .factorize() can be checked from disscussion here https:\/\/www.kaggle.com\/questions-and-answers\/241462 \nX_copy = X_drop.copy()\nfor colname in X_copy.select_dtypes(\"object\"):\n    X_copy[colname], _ = X_copy[colname].factorize()\n# All discrete features should now have intger dtypes(double check it before using MI)\ndiscrete= X_copy.dtypes == int\nX_copy","406b13c3":"X_train = X_copy.loc[train.index,:]\nX_test = X_copy.loc[test.index,:]","4c6dd74d":"# Next we import two mutual information metric from sklearn's feature_selection model\n# one for real value target(mutual_info_regression), and one for categorical target(mutual_info_classif)\n# here we define a function to show the MI score in a better way\nfrom sklearn.feature_selection import mutual_info_regression\ndef make_mi_score_real(X, y, discrete_feature):\n    mi_scores = mutual_info_regression(X,y, discrete_features = discrete_feature)\n    mi_scores = pd.Series(mi_scores, name = 'MI scores', index = X.columns)\n    mi_scores = mi_scores.sort_values(ascending = False)\n    return mi_scores\nmi_scores_real = make_mi_score_real(X_train,y,discrete)\nmi_scores_real.round(decimals=4)\nmi_scores_real[:20] ","3ca1852e":"# Add a barplot to make comparison easy\nmi_scores_df = pd.DataFrame({'mi_scores':mi_scores_real})\nplt.figure(figsize = (20,20))\nsns.barplot(y = mi_scores_df.index, x = mi_scores_df['mi_scores'])","f8374f5d":"# As we could imagine, Oveall Qual should have strong relationship with SalePrice\nsns.scatterplot(x = X_train['OverallQual'], y = y)","6d52d3fc":"# So does Neighbour\nsns.scatterplot(x = X_train['Neighborhood'], y = y)","1780b6d3":"# So we keep those features with mi scores higher than 0.1\nremain_feature = list(mi_scores_real[mi_scores_real>0.1].index)\nX_train = X_train_copy[remain_feature]\nX_test = X_test_copy[remain_feature]\nnew_concat = pd.concat([X_train,X_test], axis = 0)\nnew_concat","4027a0f2":"# Find all colmns with type object\ns = (new_concat.dtypes == 'object')\nobj_cols = s[s].index\nobj_cols","93f2f824":"#Next get all columns with numerical value\nnum_cols = list(set(new_concat.columns) - set(obj_cols))\nnum_cols","5f3d6c39":"# Next let's investgate the cardinality of those obj_col\n# Get the number of unique values in a columns with categorical value\nobj_unique = list(map(lambda col: concat[col].nunique(), obj_cols))\nd = dict(zip(obj_cols, obj_unique))\n# Print it in a ascending order\nsorted(d.items(), key = lambda x:x[1])","3d1cc61a":"obj_unique","b8dc77f7":"# Categorize it in two different list\nhigh_cardinality = [col for col in obj_cols if new_concat[col].nunique() > 10]\nlow_cardinality = [col for col in obj_cols if new_concat[col].nunique() <= 10]","368d15ca":"# Import OrdinalEncoder to process low cardinalities data and apply get_dummies \n#to those high_cardinalities data\nfrom sklearn.preprocessing import OrdinalEncoder\nencoder = new_concat[low_cardinality]\ndummies = new_concat[high_cardinality]\nnumerical = new_concat[num_cols]\n\nordinal_encoder = OrdinalEncoder()\nencoder[low_cardinality] = ordinal_encoder.fit_transform(encoder)\ndummies = pd.get_dummies(dummies)\nprint('All set')","76ddf674":"new_concat = pd.concat([numerical, encoder, dummies], axis = 1)\nprint('All set')\nnew_concat","3568632d":"X_train_new = new_concat.loc[train.index,:]\nX_test_new = new_concat.loc[test.index, :]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_train_new, y, train_size = 0.8, test_size = 0.2,\n                                                   random_state = 0)","7f4188ed":"from xgboost import XGBRegressor\nl = []\n\nmodel = XGBRegressor(n_estimators = 2200, learning_rate = 0.05, colsample_bytree=0.4603, \n                    gamma=0.0468, max_depth=3, min_child_weight=1.7817, reg_alpha=0.4640, \n                reg_lambda=0.8571, subsample=0.5213, silent=1,random_state =7, nthread = -1)\nmodel.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_test, y_test)])\nprint(model.score(X_test, y_test))\n","223837f5":"model = XGBRegressor(n_estimators = 2200, learning_rate = 0.05, colsample_bytree=0.4603, \n                    gamma=0.0468, max_depth=3, min_child_weight=1.7817, reg_alpha=0.4640, \n                reg_lambda=0.8571, subsample=0.5213, silent=1,random_state =7, nthread = -1)\nmodel.fit(X_train_new, y,early_stopping_rounds=5, eval_set=[(X_test, y_test)])\npredict = model.predict(X_test_new)\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test_new.index,\n                       'SalePrice': predict})\noutput.to_csv('submission.csv', index=False)","a9d5711d":"**Drop some columns and fillna with the previous thoughts**","d6b0f721":"# 1.  Working on missing values","1281e74c":"**Get some basic information of this datase**t","eed33962":"**Final last step, do encoding process to get the data prepared for fitting**","90b22213":"**Use mutual_info_regression to deal with real value target**","50850807":"# 2. Features Engineering ","1904d1e3":"**Draw a barplot to get the brief idea**","41482807":"# 3. Prepare the data for model-fitting ","9df42c0d":"**Categorize different columns after checking the documentation**","80ec43a7":"**Draw a barplot to have direct comparison**","2cec929d":"# 4. Assign back the data to get prediction ","dfa741bb":"This time we just simply set n_estimators = 80"}}