{"cell_type":{"dbbc1c1c":"code","aa09ece1":"code","31918644":"code","8b8c13b3":"code","d9d9389a":"code","33d73478":"code","51494d50":"code","89c9f97f":"code","55759272":"code","9cbe1eed":"code","e8b160dd":"code","711e0494":"code","e6255b5d":"code","8aa653a0":"code","4c354f85":"code","93d40fbc":"code","749b53a3":"code","6e81f932":"markdown","14243da9":"markdown","7a924500":"markdown","a449026c":"markdown","8236f0dd":"markdown","9223e4a5":"markdown","abcde3df":"markdown","94521888":"markdown","44b1590c":"markdown","e0a4ac2a":"markdown","d142ecac":"markdown","3477e3e4":"markdown","1ab2c4b4":"markdown"},"source":{"dbbc1c1c":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\n\ndef load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df\n\ndf_train = load_df()\ndf_test = load_df(\"..\/input\/test.csv\")","aa09ece1":"#Lets have a look at the data\ndf_train.head(3)","31918644":"df_train.columns\nprint('Is there more than one transaction by VisitorId in train dataset?',\n      len(df_train['fullVisitorId'])!=df_train['fullVisitorId'].nunique())\nprint('Is there more than one transaction by VisitorId in test dataset?',\n      len(df_test['fullVisitorId'])!=df_test['fullVisitorId'].nunique())\n#Confirming that we have more rows than unique Visitors Id's in both train and test datasets.","8b8c13b3":"dropcols = [c for c in df_train.columns if df_train[c].nunique()==1]\ndropcols_test = [c for c in df_test.columns if df_test[c].nunique()==1]\ndf_train = df_train.drop(dropcols,axis=1)\ndf_test = df_test.drop(dropcols_test,axis=1)","d9d9389a":"#Date Information: Lets split the information in Years\/Months\/Days and split the analisys.\ndf_train['year']= df_train['date'].astype(str).str[:4]\ndf_test['year']= df_test['date'].astype(str).str[:4]\ndf_train['month']= df_train['date'].astype(str).str[4:6]\ndf_test['month']= df_test['date'].astype(str).str[4:6]\ndf_train['day']= df_train['date'].astype(str).str[6:8]\ndf_test['day']= df_test['date'].astype(str).str[6:8]\ndf_train.drop('date',axis=1,inplace=True)\ndf_test.drop('date',axis=1,inplace=True)","33d73478":"# As stated in previous kernel analisys, this information does very little for us, lets drop it.\ndf_train.drop('year',axis=1,inplace=True)\ndf_test.drop('year',axis=1,inplace=True)","51494d50":"#Missing Data on Train Dataset\nprint('Any different features between Train and Test datasets?',\n      False in df_train.drop('totals.transactionRevenue',axis=1).columns == df_test.columns)\n#So, apart from the target values, there are no features difference between these datasets.\n#The 'sessionId' and 'visitId' features are not going to be usefull for us, so we can drop it.\ndf_train.drop(['sessionId','visitId'],axis=1,inplace=True)\ndf_test.drop(['sessionId','visitId'],axis=1,inplace=True)","89c9f97f":"df_train[\"totals.transactionRevenue\"].fillna(0, inplace=True)# Impute 0 for missing target values\ny_train = df_train[\"totals.transactionRevenue\"]\n#df_train.drop(\"totals.transactionRevenue\",axis=1,inplace=True)\ntrain_id = df_train[\"fullVisitorId\"]\npred_id = df_test[\"fullVisitorId\"]","55759272":"df_merge = pd.concat([df_train.drop(\"totals.transactionRevenue\",axis=1),df_test],axis=0)\n\ndf_merge['day'] = df_merge['day'].astype(float)\ndf_merge['month'] = df_merge['month'].astype(float)\n\n\ndf_merge['trafficSource.adwordsClickInfo.page'] = df_merge['trafficSource.adwordsClickInfo.page'].astype(str)\ndf_merge['trafficSource.adwordsClickInfo.page'].fillna('None',inplace=True)\n\ndf_merge['totals.pageviews'] = df_merge['totals.pageviews'].astype(str)\ndf_merge['totals.pageviews'].fillna('None',inplace=True)\n\nmergeId = df_merge['fullVisitorId']\ndf_merge.drop('fullVisitorId',axis=1,inplace=True)\n\nqualitative_features = [f for f in df_merge.dropna().columns \n                        if df_merge.dropna().dtypes[f] == 'object' or 'bool'] #Lista de Features Qualitativas.\nquantitative_features = [f for f in df_merge.dropna().columns \n                         if df_merge.dropna().dtypes[f] != 'object' or 'bool'] #Lista de Features Qualitativas.\n\n\ndef missingData(df,features):\n    total = df[features].isnull().sum().sort_values(ascending=False)\n    percent = (df[features].isnull().sum()\/df[features].isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data","9cbe1eed":"#Missing data qualitative feature\nmissing_data_quali = missingData(df_merge,qualitative_features)\nmissing_data_quali.head(20)","e8b160dd":"#Missing data quantitative feature\nmissing_data_quanti = missingData(df_merge,quantitative_features)\nmissing_data_quanti.head(20)","711e0494":"for i in qualitative_features:\n    if df_merge[i].isnull().any():\n        df_merge[i].fillna('None',inplace=True)\nprint('\\nIs there any NaN value  in the dataset after Imputing?:',df_merge.isnull().sum().any())\n\n# Encoding the variable\nfrom collections import defaultdict\nfrom sklearn.preprocessing import LabelEncoder\nd = defaultdict(LabelEncoder)\n# Encoding the variable\nfit = df_merge[qualitative_features].apply(lambda x: d[x.name].fit_transform(x))\ndf_merge[qualitative_features] = fit\n\n#Restore datraframes df_train and df_test\ndf_train = df_merge[:len(df_train)]\ndf_train[\"totals.transactionRevenue\"] = y_train.tolist()\ndf_test = df_merge[len(df_train):]","e6255b5d":"from sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nsize_test = 0.3\ndf_train = shuffle(df_train) #shuffle data before division\ntrain_target = np.log1p(df_train[\"totals.transactionRevenue\"].astype(float).tolist()) # Just for code readibility\npredictors = df_train.drop(\"totals.transactionRevenue\", axis=1)\nX_train, X_val, y_train, y_val = train_test_split(predictors, \n                                                    train_target,\n                                                    train_size=1-size_test, \n                                                    test_size=size_test, \n                                                    random_state=0)\nX_pred = df_test","8aa653a0":"train_target","4c354f85":"from xgboost import XGBRegressor\nxgb_model = XGBRegressor(n_estimators=500, learning_rate=0.01)\n\nxgb_model.fit(X_train.values, y_train, early_stopping_rounds=5, \n             eval_set=[(X_val.values, y_val)], verbose=False)\n\n\n# Training the model #\npred_test = xgb_model.predict(X_pred.values)","93d40fbc":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(xgb_model, random_state=1).fit(X_val, y_val)\neli5.show_weights(perm, feature_names = X_val.columns.tolist())\n\n#Looks like totals.pageviews is the most important feature, followed by totals.hits","749b53a3":"sub_df = pd.DataFrame({\"fullVisitorId\":pred_id})\n#Round to Zero the negative predictions\npred_test[pred_test<0] = 0\nsub_df[\"PredictedLogRevenue\"] = np.expm1(pred_test)\n#sub_df[\"PredictedLogRevenue\"] = pred_test\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"xgb_predictions.csv\", index=False)\nsub_df.head()","6e81f932":"## Permutation Importante\n\nThere is a[ great kernel](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance?utm_medium=email&utm_source=mailchimp&utm_campaign=ml4insights) written by[ DanB](https:\/\/www.kaggle.com\/dansbecker) to helps us get insighsts about the feature importance in our model, and we will use it here.","14243da9":"**Dropping columns with constant values**\n\nSome of the features imported in the json-csv process have only one unique value, which doesn't give us any information, and it would be a problem for any ML model. So we should just drop them.","7a924500":"## Imputation with 'None' and LabelEncoder","a449026c":"## Missing Data per features type","8236f0dd":"**Reshaping the given dataset**\n\nAs mentioned in the challenge description, some fields of the datasets train.csv and test.csv are in json format, and for better data manipulation with DataFrames, we should make some type conversions first. For this task, we already have a [pretty nice kernel](https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\/notebook) built by [Juli\u00e1n Peller](https:\/\/www.kaggle.com\/julian3833).","9223e4a5":"**Datasets Analisys**","abcde3df":"## Verify if features in both train and test datasets are equal","94521888":"## Building Date Features","44b1590c":"\n## Division between X_train,X_val,y_train,y_val ","e0a4ac2a":"**Kernel Approach**\n\nThis kernel is a quick ML solution built based on XGBoost, and a Feature Importance Analisys using [Permutation Importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance?utm_medium=email&utm_source=mailchimp&utm_campaign=ml4insights). Further feature analisys were done in my previous kernel [GA Challenge - DAta Analisys](https:\/\/www.kaggle.com\/wesleyjr01\/google-analytics-challenge-data-analisys).","d142ecac":"## XGBoost Model\n\nIf you want to start with XGBoost, [this kernel](https:\/\/www.kaggle.com\/dansbecker\/xgboost) written by [DanB](https:\/\/www.kaggle.com\/dansbecker) might help you.","3477e3e4":"## Store and Drops Id's from datasets, and Target from Train Dataset","1ab2c4b4":"## Build the solution DataFrame"}}