{"cell_type":{"9ce48b5a":"code","f2a5a528":"code","57cdcb65":"code","b1cc2e17":"code","c12f652a":"code","934b60cf":"code","e4433269":"code","ecb96793":"code","4978c73c":"code","01044cf1":"code","205d34e6":"code","792aa7fd":"code","0a1beffa":"code","9c5e5beb":"code","5e2574d2":"code","861b28b7":"code","51be38ba":"code","026041aa":"code","da6e3482":"markdown","ab0214d5":"markdown","0a6fe5ce":"markdown","9ffc43fa":"markdown","d395193d":"markdown","6581d1ad":"markdown"},"source":{"9ce48b5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2a5a528":"import tensorflow as tf\nimport matplotlib.pyplot as plt","57cdcb65":"data = pd.read_csv(\"..\/input\/amazon-fine-food-reviews\/Reviews.csv\")\nreview_data = data[:10000]","b1cc2e17":"# Remove duplicated data\nreview_data = review_data[['UserId', 'ProductId', 'Score']]\nreview_data = review_data[~review_data[['UserId', 'ProductId']].duplicated(keep='first')]","c12f652a":"input_data = review_data.pivot(index='UserId', columns='ProductId', values='Score')\ninput_data = input_data.fillna(0) \/ 5.0\ninput_data.head()","934b60cf":"from sklearn.model_selection import train_test_split\ntrain_data, test_data = train_test_split(input_data, train_size=0.7, test_size=0.3)","e4433269":"# Normolization\ntrX = train_data.values\ntrX","ecb96793":"# Build RBM model\nhiddenUnits = 20\nvisibleUnits =  len(input_data.columns)\n\n\n\nv0 = tf.zeros([visibleUnits], tf.float32)\ndef hidden_layer(v0_state, W, hb):\n    h0_prob = tf.nn.sigmoid(tf.matmul([v0_state], W) + hb)  #probabilities of the hidden units\n    h0_state = tf.nn.relu(tf.sign(h0_prob - tf.random.uniform(tf.shape(h0_prob)))) #sample_h_given_X\n    return h0_state\n\ndef reconstructed_output(h0_state, W, vb):\n    v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb) \n    v1_state = tf.nn.relu(tf.sign(v1_prob - tf.random.uniform(tf.shape(v1_prob)))) #sample_v_given_h\n    return v1_state[0]\n\ndef error(v0_state, v1_state):\n    return tf.reduce_mean(tf.square(v0_state - v1_state))\n\nvb = tf.Variable(tf.zeros([visibleUnits]), tf.float32) \nhb = tf.Variable(tf.zeros([hiddenUnits]), tf.float32) \nW = tf.Variable(tf.zeros([visibleUnits, hiddenUnits]), tf.float32)\nh0 = hidden_layer(v0, W, hb)\nv1 = reconstructed_output(h0, W, vb)\nerr = tf.reduce_mean(tf.square(v0 - v1))\nv0 = tf.zeros([visibleUnits], tf.float32)","4978c73c":"# Train RBM model\nepochs = 15\nbatchsize = 500\nerrors = []\nweights = []\nK=1\nalpha = 0.1\n\n#creating datasets\ntrain_ds = \\\n    tf.data.Dataset.from_tensor_slices((np.float32(trX))).batch(batchsize)\n\nv0_state=v0\nfor epoch in range(epochs):\n    batch_number = 0\n    for batch_x in train_ds:\n\n        for i_sample in range(len(batch_x)):           \n            for k in range(K):\n                v0_state = batch_x[i_sample]\n                h0_state = hidden_layer(v0_state, W, hb)\n                v1_state = reconstructed_output(h0_state, W, vb)\n                h1_state = hidden_layer(v1_state, W, hb)\n\n                delta_W = tf.matmul(tf.transpose([v0_state]), h0_state) - tf.matmul(tf.transpose([v1_state]), h1_state)\n                W = W + alpha * delta_W\n\n                vb = vb + alpha * tf.reduce_mean(v0_state - v1_state, 0)\n                hb = hb + alpha * tf.reduce_mean(h0_state - h1_state, 0) \n\n                v0_state = v1_state\n\n            if i_sample == len(batch_x)-1:\n                err = error(batch_x[i_sample], v1_state)\n                errors.append(err)\n                weights.append(W)\n                print ( 'Epoch: %d' % (epoch + 1), \n                       \"batch #: %i \" % batch_number, \"of %i\" % (len(trX)\/batchsize), \n                       \"sample #: %i\" % i_sample,\n                       'reconstruction error: %f' % err)\n        batch_number += 1\n\n\n\n\nplt.plot(errors)\nplt.ylabel('Error')\nplt.xlabel('Epoch')\nplt.show()","01044cf1":"# test user\nuser_input = test_data.iloc[0:1]\nuser_input","205d34e6":"# Transfer tensor\nv0 = pd.DataFrame(columns = input_data.columns)\nv0","792aa7fd":"v0 = v0.append(user_input)\nv0","0a1beffa":"v0 = v0.values\nv0 = v0.reshape((v0.shape[1],))\nv0 = tf.convert_to_tensor(v0, 'float32')\nv0","9c5e5beb":"# Predict\nhh0 = tf.nn.sigmoid(tf.matmul([v0], W) + hb)\nvv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\nrec = vv1","5e2574d2":"# Recommendation scores\nscored_food_data = pd.DataFrame({'ProductId':input_data.columns})\nscored_food_data['Test_User_Scores'] = v0*5\nscored_food_data = scored_food_data.assign(RecommendationScore = rec[0])\nrecommendations = scored_food_data.sort_values([\"RecommendationScore\"], ascending=False).head(10)\nrecommendations","861b28b7":"recommendations[recommendations['ProductId'].isin(user_input.index)].sort_values([\"RecommendationScore\"], ascending=False).head(10)","51be38ba":"# User Haven't buy\nfinal_result = scored_food_data[-scored_food_data['ProductId'].isin(user_input.index)]\nfinal_result.sort_values([\"RecommendationScore\"], ascending=False).head(20)","026041aa":"testX = test_data.values\ntest_err = 0\ntest_ds = \\\n    tf.data.Dataset.from_tensor_slices((np.float32(testX))).batch(batchsize)\nfor batch_x in test_ds:\n        for i_sample in range(len(batch_x)):           \n            for k in range(K):\n                v0_state = batch_x[i_sample]\n                h0_state = hidden_layer(v0_state, W, hb)\n                v1_state = reconstructed_output(h0_state, W, vb)\n                h1_state = hidden_layer(v1_state, W, hb)\n\n                delta_W = tf.matmul(tf.transpose([v0_state]), h0_state) - tf.matmul(tf.transpose([v1_state]), h1_state)\n                W = W + alpha * delta_W\n\n                vb = vb + alpha * tf.reduce_mean(v0_state - v1_state, 0)\n                hb = hb + alpha * tf.reduce_mean(h0_state - h1_state, 0) \n\n                v0_state = v1_state\n\n            if i_sample == len(batch_x)-1:\n                err = error(batch_x[i_sample], v1_state)\n                test_err += err.numpy()\n                weights.append(W)\n                print ( 'Epoch: %d' % (epoch + 1), \n                       \"batch #: %i \" % batch_number, \"of %i\" % (len(trX)\/batchsize), \n                       \"sample #: %i\" % i_sample,\n                       'reconstruction error: %f' % err)\n        batch_number += 1\nprint(\"total err\", test_err \/ (len(testX) \/\/ batchsize + 1))","da6e3482":"1. Use the same dataset as your Team Project (preferably) if applicable or find a dataset from here or from the Stanford University datasets. Make sure to confirm the licensing agreement for the dataset being used.\n\nI used amazon fine food reviews' database.","ab0214d5":"3. Use one of the test cases to test the trained recommendation system.","0a6fe5ce":"2. Use the notebook from HOS08 to build and train a recommendation system for your dataset. Make sure to leave out a couple of subjects to use as the test dataset.\n\nI can't seperate test data due to the limitation of RBM model and dataset","9ffc43fa":"The following code indicates that the recommendation score of items that the user has scored. According to the results, the recommendations is not related to the test subject.","d395193d":"OPTIONAL: Can you think of a quantitative measure to assess the performance of the recommendation system? \n\nWe can try to use absolutely mean to evaluate our model.","6581d1ad":"4. Sort the recommendation scores in descending order and choose the top ten recommendations. Do you think these recommendations are related to the test subject? Justify your answer."}}