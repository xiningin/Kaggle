{"cell_type":{"c370d627":"code","124b7204":"code","daa79262":"code","2200b02d":"code","b62ce519":"code","de3d49b9":"code","5aaf5351":"code","4fef2ba1":"code","bbc8803c":"code","85bb0bff":"code","8903737c":"code","6673d600":"code","b2674bc7":"code","345f0787":"code","27a5ca45":"code","dde5b6a8":"code","b1efa34c":"code","8ee9368c":"code","913d32d9":"code","e67cd174":"code","91e9a433":"code","733b32c5":"code","640103dc":"code","6e8c91b2":"code","82128242":"code","f9cd113c":"code","ba5eacdf":"code","0453ec25":"code","094731d4":"code","44e00d26":"code","014c870f":"code","d00859e9":"code","937b4f3e":"code","1381f2f1":"code","8954d3c4":"code","2c4fd641":"code","bb07d6fb":"markdown","f1b77d59":"markdown","38c9eb7e":"markdown","2058db56":"markdown","e4511c01":"markdown","7ec17f20":"markdown","9109544d":"markdown","a29a31bb":"markdown","b0f7fee0":"markdown","db47b2c4":"markdown"},"source":{"c370d627":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","124b7204":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model, metrics","daa79262":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","2200b02d":"train.head()","b62ce519":"print(train.shape)\nprint(test.shape)","de3d49b9":"(train.isna().sum() \/ len(train) * 100).sort_values(ascending=False).head(10)","5aaf5351":"(test.isna().sum() \/ len(test) * 100).sort_values(ascending=False).head(10)","4fef2ba1":"for col in train.columns:\n    if (train[col].isna().sum() \/ len(train[col])) > 0.80:\n        train.drop(columns = col, inplace = True)\n        test.drop(columns = col, inplace = True)","bbc8803c":"train.isna().sum().sort_values(ascending=False).head(15)","85bb0bff":"test.isna().sum().sort_values(ascending=False).head(29)","8903737c":"train.info()","6673d600":"for col in test.columns:\n    train[col].fillna(train[col].mode, inplace = True)\n    test[col].fillna(train[col].mode, inplace = True)","b2674bc7":"train.isna().sum().sort_values(ascending=False).head(15)","345f0787":"test.isna().sum().sort_values(ascending=False).head(29)","27a5ca45":"for col in train.columns:\n    if ((train[col].value_counts()\/len(train[col])) > 0.7).any() == True:\n        train.drop(columns = col, inplace = True)\n        test.drop(columns = col, inplace = True)","dde5b6a8":"train.corr().iloc[-1:,:].T.sort_values(by='SalePrice', ascending=False)","b1efa34c":"train.SalePrice.hist()","8ee9368c":"np.log(train.SalePrice).hist()","913d32d9":"#train.SalePrice = np.log(train.SalePrice)\n\ntrain.OverallQual = np.exp(train.OverallQual)\ntest.OverallQual = np.exp(test.OverallQual)","e67cd174":"sns.set_style(\"whitegrid\")\n#sns.set_context(\"poster\")\nsns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 1})\nsns.boxplot(train.OverallQual, train.SalePrice)","91e9a433":"vars_object = list()\n\nfor col in train.columns:\n    if train[col].dtype == 'object':\n        vars_object.append(col)\n        \nvars_object","733b32c5":"for col in train.columns:\n    if train[col].dtype.name == \"object\":\n        train.drop(col, axis = 1, inplace = True)\n        test.drop(col, axis = 1, inplace = True)\n    elif col == 'SalePrice':\n        continue\n    elif test[col].dtype.name == \"object\":\n        train.drop(col, axis = 1, inplace = True)\n        test.drop(col, axis = 1, inplace = True)","640103dc":"for col in test.columns:\n    if test[col].dtype.name == \"object\":\n        print(col)","6e8c91b2":"from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(train.drop(train.columns[-1],axis=1), train.iloc[:,-1:], test_size=0.2, \n                                                    random_state=0)","82128242":"reg = linear_model.LinearRegression()","f9cd113c":"reg.fit(X_train, y_train)","ba5eacdf":"print('Coefficients: \\n', reg.coef_)\nprint('Variance score: {}'.format(reg.score(X_test, y_test)))","0453ec25":"plt.style.use('fivethirtyeight')","094731d4":"plt.scatter(reg.predict(X_train), reg.predict(X_train) - y_train, \n            color = \"green\", s = 10, label = 'Train data') ","44e00d26":"plt.scatter(reg.predict(X_test), reg.predict(X_test) - y_test, \n            color = \"blue\", s = 10, label = 'Test data')","014c870f":"test.head()","d00859e9":"test.info()","937b4f3e":"test_pred = reg.predict(test)","1381f2f1":"test_pred","8954d3c4":"my_submission = pd.DataFrame(data={'Id': test['Id'].values, 'SalePrice': test_pred.reshape(-1, )})\nmy_submission.head()\n\nmy_submission.to_csv(r'submission.csv', index=False)","2c4fd641":"my_submission.head()","bb07d6fb":"We check for nulls in our dataset.","f1b77d59":"We remove the features which has null higher than 80% of the observations. They are the same for both train and test set.","38c9eb7e":"# Exploratory Data Analysis","2058db56":"There are still null features. We will fill those values with modes for both numeric and categorical variables.","e4511c01":"# Train Test Split","7ec17f20":"# Linear Model","9109544d":"Now that it looks more like normal distribution, we change the SalePrice in our dataset to log of SalePrice.","a29a31bb":"We drop the features which have the same level more than 70% of the time.","b0f7fee0":"Correlation for numeric features.","db47b2c4":"To make the distribution normal, we are performing a log transformation."}}