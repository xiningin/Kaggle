{"cell_type":{"fa7cdbd6":"code","454de633":"code","11197bde":"code","7bd2008a":"code","59cbba2f":"code","8de3e390":"code","0b76576c":"code","463ccb7f":"code","1e60ffcf":"code","7cf3ceb9":"code","6b2fd013":"code","d52ce1c7":"code","28b7a61e":"code","af83edd7":"code","2380c33e":"code","e999a45a":"code","67f2f75c":"code","e1a2c3d8":"code","09d7dea9":"code","83fa1087":"code","7a5890e5":"code","b342ac5b":"code","65ad6f11":"code","60ab0f10":"code","67941411":"code","69b0f98f":"code","0c534442":"code","2bd6a73c":"code","d552ff5b":"code","633fabb4":"code","ece7468b":"code","4fa3e484":"code","cc929e53":"code","b542aafb":"code","ec596f74":"code","881ab151":"code","10182906":"code","b4b39895":"code","9fb2f519":"code","a97ce02d":"code","e76b6289":"code","32d97dd3":"code","a2d816f4":"code","491ca5c5":"code","a3127c32":"code","b1bdb76f":"code","bf66d50e":"code","16ffaa1a":"code","b08ea91e":"code","3385d866":"code","d7653595":"code","93c91b32":"code","be8931a1":"code","c5e738e4":"code","cb69469a":"code","a25a4786":"code","8e8c6ebc":"code","4650f4a5":"code","539ff7ae":"code","277cb29b":"code","be923286":"code","667185a1":"code","99081c62":"code","693f9637":"code","afaec5c0":"code","d77d0fa3":"code","ed563acf":"code","cf29612c":"code","e6ba5dc5":"code","d49f8ea9":"code","dc411c19":"code","07d27bf6":"code","ea8fa364":"code","c4ddc8e6":"code","ed9eaad0":"code","592906c0":"code","8fb99ecc":"code","affea325":"code","ba41295c":"code","290a9240":"code","fa502ad2":"code","7ed44cd8":"code","4bcffa51":"code","cba1f4d0":"code","d0a7b211":"code","97ff845c":"code","2f0f1195":"code","b26ff2b3":"code","16eb8970":"code","09470ffa":"code","08f2358b":"code","33634a7f":"code","053b5af6":"code","0575bb38":"code","3fb6f264":"code","60dd8cca":"code","7a328c02":"code","414745f3":"code","00c5640d":"code","7a587e50":"code","b7e47e58":"code","995bab74":"code","308d68b2":"code","9fb82169":"code","421fb224":"code","803965dd":"code","d3e0bbe8":"code","f9c9b2b4":"code","89da24ba":"code","25ef7de7":"code","735755c2":"code","1ca754a4":"code","7c74608d":"code","5caa8c64":"code","9ddbeecd":"code","6663f67e":"code","04d3b2b1":"code","ecf1c701":"code","6c359482":"code","356a9839":"code","cbef58dc":"code","5c21ce8b":"code","c8ec6f8b":"code","a694bf50":"code","884acab3":"code","18762793":"code","3cc6c6c4":"code","9060e52c":"code","94a04747":"code","12a67fb6":"code","49f638d3":"code","51fcf1c7":"code","dafe9893":"code","dfef8d99":"code","86381eae":"code","62cea938":"code","0817dca8":"code","185f764a":"code","93657e86":"code","c0f08ae7":"code","82d09d3c":"code","3904177f":"code","0b6ef1f6":"code","26e14803":"code","5ad45ced":"code","f21e2c07":"code","c54bc1a8":"code","6774a22d":"code","369c9d4f":"code","14e61872":"code","ccf49809":"code","6bb4c12e":"markdown","215b871f":"markdown","a2d16c94":"markdown","2d6ef256":"markdown","ec2dceeb":"markdown","f91e8c27":"markdown","3570f15e":"markdown","7e893d62":"markdown","0563c836":"markdown","cdd6ee01":"markdown","e4c6191a":"markdown","f9e10661":"markdown","c20336a0":"markdown"},"source":{"fa7cdbd6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.tabular import *\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim\n\nimport gc\ngc.collect()\n\nimport re\nimport os\nimport re\nimport gc\nimport pickle  \nimport random\nimport keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom bayes_opt import BayesianOptimization\nfrom lightgbm import LGBMRegressor\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.stem.snowball import EnglishStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom functools import lru_cache\nfrom tqdm import tqdm as tqdm","454de633":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 999\nseed_everything(SEED)","11197bde":"train = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\")\ntest = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\")\nsub = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")","7bd2008a":"train.shape, test.shape, sub.shape","59cbba2f":"train.head()","8de3e390":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\n', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df","0b76576c":"target_cols_questions = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written']\n\ntarget_cols_answers = ['answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']\n\ntargets = target_cols_questions + target_cols_answers","463ccb7f":"train = clean_data(train, ['answer', 'question_body', 'question_title'])\ntest = clean_data(test, ['answer', 'question_body', 'question_title'])","1e60ffcf":"find = re.compile(r\"^[^.]*\")\n\ntrain['netloc_1'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_1'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ntrain['netloc_2'] = train['question_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_2'] = test['question_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ntrain['netloc_3'] = train['answer_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_3'] = test['answer_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n","7cf3ceb9":"train.head(3)","6b2fd013":"train.host.value_counts()","d52ce1c7":"train.netloc_1.value_counts()","28b7a61e":"train.shape, test.shape","af83edd7":"# train_tfidf = train.copy()\n# test_tfidf = test.copy()","2380c33e":"# import gensim\n\n# w2v_model = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/word2vec-google\/GoogleNews-vectors-negative300.bin', \n#                                                             binary=True, unicode_errors='ignore')","e999a45a":"# #https:\/\/www.kaggle.com\/sediment\/a-gentle-introduction-eda-tfidf-word2vec\/data#Benchmark-with-Word2Vec\n\n# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n# TFIDF_SVD_WORDVEC_DIM = 300\n\n# def get_text_feats(df, col):\n\n#     def tokenize_downcase_filtering(x):\n#         words = TOKENIZER.tokenize(x)\n#         lower_case = map(lambda w: w.lower(), words)\n#         content_words = filter(lambda w: w not in STOPWORDS, lower_case)\n#         return ' '.join(content_words)\n\n#     rows = df[col].map(tokenize_downcase_filtering).values.tolist()\n#     tfidf = TfidfVectorizer(tokenizer=lambda x: x.split(' '))  # dont use sklearn default tokenization tool \n#     tfidf_weights = tfidf.fit_transform(rows)\n#     svd = TruncatedSVD(n_components=TFIDF_SVD_WORDVEC_DIM, n_iter=10)  # reduce dimensionality\n#     dense_tfidf_repr_mat = svd.fit_transform(tfidf_weights)\n    \n#     word2vec_repr_mat = np.zeros((len(df), w2v_model.vector_size))\n#     for i, row in enumerate(rows):\n#         word2vec_accum = np.zeros((w2v_model.vector_size, ))\n#         word_cnt = 0\n#         for w in row.split(' '):\n#             if w in w2v_model.wv:\n#                 word2vec_accum += w2v_model.wv[w]\n#                 word_cnt += 1\n\n#         # compute the average for the wordvec of each non-sptop word\n#         if word_cnt != 0:\n#             word2vec_repr_mat[i] = word2vec_accum \/ word_cnt\n\n#     return  np.concatenate([word2vec_repr_mat, dense_tfidf_repr_mat], axis=1)  # word2vec + tfidf","67f2f75c":"# from nltk.tokenize import RegexpTokenizer\n# from nltk.corpus import stopwords\n\n# TOKENIZER = RegexpTokenizer(r'\\w+')\n# STOPWORDS = set(stopwords.words('english'))","e1a2c3d8":"# # let's build features\n# df_all = pd.concat((train_tfidf, test_tfidf))\n# df_all['question_title_len'] = df_all['question_title'].map(lambda x: len(TOKENIZER.tokenize(x)))\n# df_all['question_body_len'] = df_all['question_body'].map(lambda x: len(TOKENIZER.tokenize(x)))\n# df_all['answer_len'] = df_all['answer'].map(lambda x: len(TOKENIZER.tokenize(x)))","09d7dea9":"# text_cols = [\n#     'question_title',\n#     'question_body',\n#     'answer'\n# ]\n\n# text_len_cols = ['question_title_len', 'question_body_len', 'answer_len']","83fa1087":"# from sklearn.decomposition import TruncatedSVD\n\n# data = []\n# for col in text_cols:\n#     data.append(get_text_feats(df_all, col))\n\n# data.append(df_all[text_len_cols].values)\n# data = np.concatenate(data, axis=1)\n\n# train_feats = data[:len(train_tfidf)]\n# test_feats = data[len(train_tfidf):]\n\n\n# print(train_feats.shape)","7a5890e5":"# print(test_feats.shape)","b342ac5b":"# del w2v_model\n# gc.collect()","65ad6f11":"# train_wordvec = pd.DataFrame(data = train_feats)\n# train_wordvec.columns = [str(col) + '_col' for col in train_wordvec.columns]\n# print(train_wordvec.shape)\n# train_wordvec.head()","60ab0f10":"# test_wordvec = pd.DataFrame(data = test_feats)\n# test_wordvec.columns = [str(col) + '_col' for col in test_wordvec.columns]\n# print(test_wordvec.shape)\n# test_wordvec.head()","67941411":"# tabular_cols = ['question_user_name', 'answer_user_name', \n#                'netloc_1', 'netloc_2', 'netloc_3',\n#                'category', 'host']\n\n# train_select = train[tabular_cols + targets]\n# test_select = test[tabular_cols]\n\n# train_tfidf_final = pd.concat([train_wordvec, train_select], axis=1)\n\n# test_tfidf_final = pd.concat([test_wordvec, test_select], axis=1)\n\n# gc.collect()","69b0f98f":"# valid_sz = 2000\n# valid_idx = range(len(train_wordvec)-valid_sz, len(train_wordvec))\n# valid_idx","0c534442":"# cont_names = train_wordvec.columns\n# cat_names = tabular_cols\n# dep_var = targets\n# procs = [FillMissing, Categorify, Normalize]\n\n# test_tab = TabularList.from_df(test_tfidf_final, cat_names=cat_names, cont_names=cont_names, procs=procs)\n\n# data = (TabularList.from_df(train_tfidf_final, procs = procs, cont_names=cont_names, cat_names=cat_names)\n#         .split_by_idx(valid_idx)\n#         .label_from_df(cols=dep_var)\n#         .add_test(test_tab)\n#         .databunch(bs=32))","2bd6a73c":"# from fastai.callbacks import *\n\n# auroc = AUROC()\n\n# learn_wordvec = tabular_learner(data, layers=[800, 400, 200, 100], \n#                         ps=[0.5, 0.5, 0.25, 0.25], emb_drop=0.5)\n# learn_wordvec.lr_find()\n# learn_wordvec.recorder.plot(suggestion=True)","d552ff5b":"# lr = 5e-2\n# learn_wordvec.fit_one_cycle(7, max_lr=lr,  pct_start=0.5, wd = 0.75)","633fabb4":"# learn_wordvec.lr_find()\n# learn_wordvec.recorder.plot(suggestion=True)","ece7468b":"# lr = 1e-4\n# learn_wordvec.fit_one_cycle(7, max_lr=lr,  pct_start=0.5, wd = 0.75)","4fa3e484":"# learn_wordvec.lr_find()\n# learn_wordvec.recorder.plot(suggestion=True)","cc929e53":"# lr = 1e-5\n# learn_wordvec.fit_one_cycle(7, max_lr=lr,  pct_start=0.5, wd = 1.)","b542aafb":"# pred_test_wordvec, lbl_test_wordvec = learn_wordvec.get_preds(ds_type=DatasetType.Test)\n# print(pred_test_wordvec.shape)\n# pred_test_wordvec","ec596f74":"# pred_test_wordvec = np.clip(pred_test_wordvec, 0.00001, 0.999999)\n# pred_test_wordvec.shape","881ab151":"# del df_all, train_tfidf_final, train_tfidf, test_tfidf, test_tfidf_final, train_wordvec, test_wordvec\n# gc.collect()","10182906":"train_tfidf = train.copy()\ntest_tfidf = test.copy()","b4b39895":"stemmer = EnglishStemmer()\n\n@lru_cache(30000)\ndef stem_word(text):\n    return stemmer.stem(text)\n\n\nlemmatizer = WordNetLemmatizer()\n\n@lru_cache(30000)\ndef lemmatize_word(text):\n    return lemmatizer.lemmatize(text)\n\n\ndef reduce_text(conversion, text):\n    return \" \".join(map(conversion, wordpunct_tokenize(text.lower())))\n\n\ndef reduce_texts(conversion, texts):\n    return [reduce_text(conversion, str(text))\n            for text in tqdm(texts)]","9fb2f519":"train_tfidf['question_body'] = reduce_texts(stem_word, train_tfidf['question_body'])\ntest_tfidf['question_body'] = reduce_texts(stem_word, test_tfidf['question_body'])\n\ntrain_tfidf['question_title'] = reduce_texts(stem_word, train_tfidf['question_title'])\ntest_tfidf['question_title'] = reduce_texts(stem_word, test_tfidf['question_title'])\n\ntrain_tfidf['answer'] = reduce_texts(stem_word, train_tfidf['answer'])\ntest_tfidf['answer'] = reduce_texts(stem_word, test_tfidf['answer'])","a97ce02d":"train_text_1 = train_tfidf['question_body']\ntest_text_1 = test_tfidf['question_body']\nall_text_1 = pd.concat([train_text_1, test_text_1])\n\ntrain_text_2 = train_tfidf['answer']\ntest_text_2 = test_tfidf['answer']\nall_text_2 = pd.concat([train_text_2, test_text_2])\n\ntrain_text_3 = train_tfidf['question_title']\ntest_text_3 = test_tfidf['question_title']\nall_text_3 = pd.concat([train_text_3, test_text_3])","e76b6289":"import re, string\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","32d97dd3":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nimport scipy\nfrom sklearn.metrics import log_loss\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import hstack\nfrom sklearn.decomposition import TruncatedSVD\n\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 2),\n    max_features=80000,\n    tokenizer=tokenize)\nword_vectorizer.fit(all_text_1)\n\ntrain_word_features_1 = word_vectorizer.transform(train_text_1)\ntest_word_features_1 = word_vectorizer.transform(test_text_1)\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 2),\n    max_features=80000,\n    tokenizer=tokenize)\nword_vectorizer.fit(all_text_2)\n\ntrain_word_features_2 = word_vectorizer.transform(train_text_2)\ntest_word_features_2 = word_vectorizer.transform(test_text_2)\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 2),\n    max_features=80000,\n    tokenizer=tokenize)\nword_vectorizer.fit(all_text_3)\n\ntrain_word_features_3 = word_vectorizer.transform(train_text_3)\ntest_word_features_3 = word_vectorizer.transform(test_text_3)\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    token_pattern=r'\\w{1,}',\n    ngram_range=(1, 4),\n    max_features=50000,\n    tokenizer=tokenize)\nchar_vectorizer.fit(all_text_1)\n\ntrain_char_features_1 = char_vectorizer.transform(train_text_1)\ntest_char_features_1 = char_vectorizer.transform(test_text_1)\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    token_pattern=r'\\w{1,}',\n    ngram_range=(1, 4),\n    max_features=50000,\n    tokenizer=tokenize)\nchar_vectorizer.fit(all_text_2)\n\ntrain_char_features_2 = char_vectorizer.transform(train_text_2)\ntest_char_features_2 = char_vectorizer.transform(test_text_2)\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(1, 4),\n    max_features=50000,\n    tokenizer=tokenize)\nchar_vectorizer.fit(all_text_3)\n\ntrain_char_features_3 = char_vectorizer.transform(train_text_3)\ntest_char_features_3 = char_vectorizer.transform(test_text_3)\n\ntrain_features = hstack([train_char_features_1, train_word_features_1, train_char_features_2, train_word_features_2,train_char_features_3, train_word_features_3])\ntest_features = hstack([test_char_features_1, test_word_features_1, test_char_features_2, test_word_features_2,test_char_features_3, test_word_features_3])\n\npca = TruncatedSVD(n_components=300, n_iter=10)\ntf_idf_text_train = pca.fit_transform(train_features)\ntf_idf_text_test = pca.fit_transform(test_features)","a2d816f4":"train_tfidf = pd.DataFrame(data = tf_idf_text_train)\ntrain_tfidf.columns = [str(col) + '_col' for col in train_tfidf.columns]\nprint(train_tfidf.shape)\ntrain_tfidf.head()","491ca5c5":"test_tfidf = pd.DataFrame(data = tf_idf_text_test)\ntest_tfidf.columns = [str(col) + '_col' for col in test_tfidf.columns]\nprint(test_tfidf.shape)\ntest_tfidf.head()","a3127c32":"tabular_cols = ['question_user_name', 'answer_user_name', \n               'netloc_1', 'netloc_2', 'netloc_3',\n               'category', 'host']\n\ntrain_select = train[tabular_cols + targets]\ntest_select = test[tabular_cols]\n\ntrain_tfidf_final = pd.concat([train_tfidf, train_select], axis=1)\n\ntest_tfidf_final = pd.concat([test_tfidf, test_select], axis=1)\n\ngc.collect()","b1bdb76f":"valid_sz = 2000\nvalid_idx = range(len(train_tfidf)-valid_sz, len(train_tfidf))\nvalid_idx","bf66d50e":"train_tfidf.columns","16ffaa1a":"cont_names = train_tfidf.columns\ncat_names = tabular_cols\ndep_var = targets\nprocs = [FillMissing, Categorify, Normalize]\n\ntest_tab = TabularList.from_df(test_tfidf_final, cat_names=cat_names, cont_names=cont_names, procs=procs)\n\ndata = (TabularList.from_df(train_tfidf_final, procs = procs, cont_names=cont_names, cat_names=cat_names)\n        .split_by_idx(valid_idx)\n        .label_from_df(cols=dep_var)\n        .add_test(test_tab)\n        .databunch(bs=32))","b08ea91e":"from fastai.callbacks import *\n\nauroc = AUROC()\n\nlearn_tfidf = tabular_learner(data, layers=[800, 400, 200, 100], \n                        ps=[0.5, 0.5, 0.25, 0.25], emb_drop=0.5)\nlearn_tfidf.lr_find()\nlearn_tfidf.recorder.plot(suggestion=True)","3385d866":"lr = 5e-2\nlearn_tfidf.fit_one_cycle(7, max_lr=lr,  pct_start=0.5, wd = 0.75)","d7653595":"learn_tfidf.lr_find()\nlearn_tfidf.recorder.plot(suggestion=True)","93c91b32":"lr=1e-4\nlearn_tfidf.fit_one_cycle(7, max_lr=lr,  pct_start=0.5, wd = 1)","be8931a1":"learn_tfidf.lr_find()\nlearn_tfidf.recorder.plot(suggestion=True)","c5e738e4":"lr=1e-5\nlearn_tfidf.fit_one_cycle(7, max_lr=lr,  pct_start=0.5, wd = 1.)","cb69469a":"pred_test_tfidf, lbl_test_tfidf = learn_tfidf.get_preds(ds_type=DatasetType.Test)","a25a4786":"pred_test_tfidf = np.clip(pred_test_tfidf, 0.00001, 0.999999)\npred_test_tfidf.shape","8e8c6ebc":"pred_test_tfidf","4650f4a5":"X, y  = train_tfidf_final.iloc[:, :-30], train_tfidf_final.iloc[:, -30:]","539ff7ae":"y.head()","277cb29b":"# Categorical boolean mask\ncategorical_feature_mask = X.dtypes==object\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = X.columns[categorical_feature_mask].tolist()\n\n# import labelencoder\nfrom sklearn.preprocessing import LabelEncoder\n# instantiate labelencoder object\nle = LabelEncoder()\n\n# apply le on categorical feature columns\nX[categorical_cols] = X[categorical_cols].apply(lambda col: le.fit_transform(col))\nX[categorical_cols].head(10)","be923286":"import numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\n\n\nregr_multirf = MultiOutputRegressor(LGBMRegressor(boosting_type='gbdt', num_leaves=31, max_depth=5, learning_rate=0.1, \n                                                  n_estimators=100, min_child_samples=20, subsample=0.8, \n                                                  subsample_freq=0, colsample_bytree=0.8, \n                                                  reg_alpha=1., reg_lambda=1., random_state=42, silent=False))\n\nregr_multirf.fit(X, y)","667185a1":"X = test_tfidf_final\nX.head()","99081c62":"# Categorical boolean mask\ncategorical_feature_mask = X.dtypes==object\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = X.columns[categorical_feature_mask].tolist()\n\n# import labelencoder\nfrom sklearn.preprocessing import LabelEncoder\n# instantiate labelencoder object\nle = LabelEncoder()\n\n# apply le on categorical feature columns\nX[categorical_cols] = X[categorical_cols].apply(lambda col: le.fit_transform(col))\nX[categorical_cols].head(10)","693f9637":"pred_test_tree = regr_multirf.predict(X)","afaec5c0":"pred_test_tree = np.clip(pred_test_tree, 0.00001, 0.999999)\npred_test_tree.shape","d77d0fa3":"pred_test_tree","ed563acf":"train, val = train_test_split(train, test_size=0.2, shuffle=True)","cf29612c":"train.shape, val.shape","e6ba5dc5":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ntext = train.question_title.values\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","d49f8ea9":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ntext = train.question_body.values\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","dc411c19":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ntext = train.answer.values\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","07d27bf6":"# who asked the most questions\n\nqn_asker = train.question_user_name.value_counts()\nqn_asker","ea8fa364":"import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport matplotlib.style as style\nstyle.use('seaborn-poster')\nstyle.use('ggplot')","c4ddc8e6":"qn_asker.loc[qn_asker>10].sort_values().plot(kind = 'barh', figsize=(15,15)).legend(loc='best')","ed9eaad0":"qn_answerer = train.answer_user_name.value_counts()\nqn_answerer","592906c0":"qn_answerer.loc[qn_answerer>10].sort_values().plot(kind = 'barh', figsize=(15,15)).legend(loc='best')","8fb99ecc":"category = train.category.value_counts()\ncategory","affea325":"## lets see some distributions of questions targets\nplt.figure(figsize=(20, 5))\n\nsns.distplot(train[target_cols_questions[0]], hist= False , rug= False ,kde=True, label =target_cols_questions[0],axlabel =False )\nsns.distplot(train[target_cols_questions[1]], hist= False , kde=True, rug= False,label =target_cols_questions[1],axlabel =False)\nsns.distplot(train[target_cols_questions[2]], hist= False , kde=True, rug= False,label =target_cols_questions[2],axlabel =False)\nsns.distplot(train[target_cols_questions[3]], hist= False , kde=True, rug= False,label =target_cols_questions[3],axlabel =False)\nsns.distplot(train[target_cols_questions[4]], hist= False , kde=True, rug= False,label =target_cols_questions[4],axlabel =False)\nplt.show()","ba41295c":"## lets see some distributions of questions targets\nplt.figure(figsize=(20, 5))\n\nsns.distplot(train[target_cols_answers[0]], hist= False , rug= False ,kde=True, label =target_cols_answers[0],axlabel =False )\nsns.distplot(train[target_cols_answers[1]], hist= False , kde=True, rug= False,label =target_cols_answers[1],axlabel =False)\nsns.distplot(train[target_cols_answers[2]], hist= False , kde=True, rug= False,label =target_cols_answers[2],axlabel =False)\nsns.distplot(train[target_cols_answers[3]], hist= False , kde=True, rug= False,label =target_cols_answers[3],axlabel =False)\nsns.distplot(train[target_cols_answers[4]], hist= False , kde=True, rug= False,label =target_cols_answers[4],axlabel =False)\nplt.show()","290a9240":"bs, bptt = 32, 80\n\ndata_lm = TextLMDataBunch.from_df('.', train, val, test,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=['question_title', 'question_body', 'answer'],\n                  label_cols=targets,\n                  bs=bs,\n                  mark_fields=True,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n\ndata_lm.save('data_lm.pkl')","fa502ad2":"# src_lm = ItemLists(path, TextList.from_df(train, path=\".\", cols = [ 'question_title', \"question_body\", 'answer']), \n#                    TextList.from_df(val, path=\".\", cols = [ 'question_title', \"question_body\", 'answer']))","7ed44cd8":"# data_lm = src_lm.label_for_lm().databunch(bs=32)","4bcffa51":"path = \".\"\ndata_lm = load_data(path, 'data_lm.pkl', bs=bs, bptt=bptt)","cba1f4d0":"path = \".\"\ndata_bwd = load_data(path, 'data_lm.pkl', bs=bs, bptt = bptt, backwards=True)","d0a7b211":"data_lm.show_batch()","97ff845c":"data_bwd.show_batch()","2f0f1195":"awd_lstm_lm_config = dict( emb_sz=400, n_hid=1150, n_layers=3, pad_token=1, qrnn=False, bidir=False, output_p=0.1,\n                          hidden_p=0.15, input_p=0.25, embed_p=0.02, weight_p=0.2, tie_weights=True, out_bias=True)","b26ff2b3":"awd_lstm_clas_config = dict(emb_sz=400, n_hid=1150, n_layers=3, pad_token=1, qrnn=False, bidir=False, output_p=0.4,\n                       hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5)","16eb8970":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5,\n                               config=awd_lstm_lm_config, pretrained = False)\nlearn = learn.to_fp16(clip=0.1)","09470ffa":"fnames = ['..\/input\/awd-lstm\/lstm_wt103.pth','..\/input\/awd-lstm\/itos_wt103.pkl']\nlearn.load_pretrained(*fnames, strict=False)\nlearn.freeze()","08f2358b":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","33634a7f":"learn.fit_one_cycle(2, max_lr=slice(5e-3, 5e-2), moms=(0.8, 0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3))","053b5af6":"learn.save('fit_head')","0575bb38":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","3fb6f264":"learn.fit_one_cycle(10, max_lr = slice(1e-4, 5e-2), moms=(0.8, 0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4,  1e-2))","60dd8cca":"learn.recorder.plot_losses()","7a328c02":"learn.save('fine-tuned')\nlearn.load('fine-tuned')\nlearn.save_encoder('fine-tuned-fwd')","414745f3":"print(learn.model[0].encoder)","00c5640d":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(learn.model[0].encoder.weight.data)\nembedding_weights = pca.transform(learn.model[0].encoder.weight.data)","7a587e50":"plt.figure(figsize=(15,15))\nplt.scatter(embedding_weights[:, 0], embedding_weights[:, 1])\n\nfor i, word in enumerate(data_lm.vocab.itos[:50]):\n    plt.annotate(word, xy=(embedding_weights[i, 0], embedding_weights[i, 1]))\nplt.show()","b7e47e58":"learn = language_model_learner(data_bwd, AWD_LSTM, drop_mult=0.5,\n                               config=awd_lstm_lm_config, pretrained = False)\nlearn = learn.to_fp16(clip=0.1)","995bab74":"fnames = ['..\/input\/awd-lstm\/lstm_wt103.pth','..\/input\/awd-lstm\/itos_wt103.pkl']\nlearn.load_pretrained(*fnames, strict=False)\nlearn.freeze()","308d68b2":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","9fb82169":"learn.fit_one_cycle(2, max_lr=slice(5e-2, 1e-1), moms=(0.8, 0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3))","421fb224":"learn.save('fit_head-bwd')","803965dd":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","d3e0bbe8":"learn.fit_one_cycle(10, max_lr = slice(1e-4, 1e-3), moms=(0.8, 0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4,  1e-2))","f9c9b2b4":"learn.recorder.plot_losses()","89da24ba":"learn.save('fine-tuned-bwd')\nlearn.load('fine-tuned-bwd')\nlearn.save_encoder('fine-tuned-bwd')","25ef7de7":"text_cols = ['question_title', \"question_body\", 'answer']","735755c2":"data_cls = TextClasDataBunch.from_df('.', train, val, test, vocab = data_lm.vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=text_cols,\n                  label_cols=targets,\n                  bs=bs,\n                  mark_fields=True,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n\ndata_cls.save('data_cls.pkl')","1ca754a4":"data_cls = load_data(path, 'data_cls.pkl', bs=bs)","7c74608d":"data_cls.show_batch()","5caa8c64":"data_cls_bwd = load_data(path, 'data_cls.pkl', bs=bs, backwards=True)","9ddbeecd":"data_cls_bwd.show_batch()","6663f67e":"class L1LossFlat(nn.MSELoss):\n    def forward(self, input:Tensor, target:Tensor) -> Rank0Tensor:\n        return super().forward(input.view(-1), target.view(-1))","04d3b2b1":"learn = text_classifier_learner(data_cls, AWD_LSTM, drop_mult=0.5,config=awd_lstm_clas_config, pretrained = False)\nlearn.load_encoder('fine-tuned-fwd')\nlearn = learn.to_fp16(clip=0.1)\n#learn.loss_func = L1LossFlat()\nfnames = ['..\/input\/awd-lstm\/lstm_wt103.pth','..\/input\/awd-lstm\/itos_wt103.pkl']\nlearn.load_pretrained(*fnames, strict=False)\nlearn.freeze()","ecf1c701":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","6c359482":"learn.fit_one_cycle(2, max_lr=slice(1e-2, 1e-1), moms=(0.8, 0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","356a9839":"learn.save('first-head')\nlearn.load('first-head')","cbef58dc":"learn.freeze_to(-2)\nlearn.fit_one_cycle(2, slice(1e-1\/(2.6**4),1e-1), moms=(0.8,0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","5c21ce8b":"learn.save('second')\nlearn.load('second')","c8ec6f8b":"learn.freeze_to(-3)\nlearn.fit_one_cycle(2, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","a694bf50":"learn.save('third')\nlearn.load('third')","884acab3":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","18762793":"learn.fit_one_cycle(5, slice(1e-5\/(2.6**4),1e-4), moms=(0.8,0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","3cc6c6c4":"learn.recorder.plot_losses()","9060e52c":"learn.save('fwd-cls')","94a04747":"learn_bwd = text_classifier_learner(data_cls_bwd, AWD_LSTM, drop_mult=0.5, config=awd_lstm_clas_config, pretrained = False)\nlearn_bwd.load_encoder('fine-tuned-bwd')\nlearn_bwd = learn_bwd.to_fp16(clip=0.1)","12a67fb6":"fnames = ['..\/input\/awd-lstm\/lstm_wt103.pth','..\/input\/awd-lstm\/itos_wt103.pkl']\nlearn_bwd.load_pretrained(*fnames, strict=False)\nlearn_bwd.freeze()","49f638d3":"learn_bwd.lr_find()\nlearn_bwd.recorder.plot(suggestion=True)","51fcf1c7":"learn_bwd.fit_one_cycle(2, max_lr=slice(5e-2, 1e-1), moms=(0.8, 0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","dafe9893":"learn_bwd.save('first-head-bwd')\nlearn_bwd.load('first-head-bwd')","dfef8d99":"learn_bwd.freeze_to(-2)\nlearn_bwd.fit_one_cycle(2, slice(1e-1\/(2.6**4),1e-1), moms=(0.8,0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","86381eae":"learn_bwd.save('second-bwd')\nlearn_bwd.load('second-bwd')","62cea938":"learn_bwd.freeze_to(-3)\nlearn_bwd.fit_one_cycle(2, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","0817dca8":"learn_bwd.save('third-bwd')\nlearn_bwd.load('third-bwd')","185f764a":"learn_bwd.unfreeze()\nlearn_bwd.lr_find()\nlearn_bwd.recorder.plot(suggestion=True)","93657e86":"learn_bwd.fit_one_cycle(5, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","c0f08ae7":"learn_bwd.recorder.plot_losses()","82d09d3c":"learn_bwd.save('bwd-cls')","3904177f":"pred_fwd_val, lbl_fwd_val = learn.get_preds(ds_type=DatasetType.Valid,ordered=True)\npred_bwd_val, lbl_bwd_val = learn_bwd.get_preds(ds_type=DatasetType.Valid,ordered=True)","0b6ef1f6":"pred_fwd_test, lbl_fwd_test = learn.get_preds(ds_type=DatasetType.Test,ordered=True)\npred_bwd_test, lbl_bwd_test = learn_bwd.get_preds(ds_type=DatasetType.Test,ordered=True)","26e14803":"pred_test_tree = torch.from_numpy(pred_test_tree)\nfinal_preds_test = (0.30 * pred_fwd_test + 0.30 * pred_bwd_test + 0.30 * pred_test_tfidf  + 0.10* pred_test_tree)","5ad45ced":"# def get_ordered_preds(learn, ds_type, preds):\n#   np.random.seed(42)\n#   sampler = [i for i in learn.data.dl(ds_type).sampler]\n#   reverse_sampler = np.argsort(sampler)\n#   preds = [p[reverse_sampler] for p in preds]\n#   return preds","f21e2c07":"# val_raw_preds = learn.get_preds(ds_type=DatasetType.Valid)\n# val_preds_fwd = get_ordered_preds(learn, DatasetType.Valid, val_raw_preds)\n\n# val_raw_preds = learn_bwd.get_preds(ds_type=DatasetType.Valid)\n# val_preds_bwd = get_ordered_preds(learn_bwd, DatasetType.Valid, val_raw_preds)","c54bc1a8":"# final_preds = (pred_fwd + pred_bwd)\/2","6774a22d":"# from scipy.stats import spearmanr\n# score = 0\n# for i in range(30):\n#     score += np.nan_to_num(spearmanr(val[targets].values[:, i], final_preds_val[:, i]).correlation) \/ 30\n# score","369c9d4f":"# test_raw_preds = learn.get_preds(ds_type=DatasetType.Test)\n# test_preds_fwd = get_ordered_preds(learn, DatasetType.Test, test_raw_preds)\n\n# test_raw_preds = learn_bwd.get_preds(ds_type=DatasetType.Test)\n# test_preds_bwd = get_ordered_preds(learn_bwd, DatasetType.Test, test_raw_preds)","14e61872":"sub.head()\n\nsub.iloc[:, 1:] = final_preds_test.numpy()\nsub.to_csv('submission.csv', index=False)\nsub.head()","ccf49809":"fig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 20)\n\nfor i, col in enumerate(targets):\n    ax = axes[i]\n    sns.distplot(train[col], label=col, bins=bins, ax=ax, color='blue')\n    sns.distplot(sub[col], label=col, bins=bins, ax=ax, color='orange')\n    # ax.set_title(col)\n    ax.set_xlim([0, 1])\nplt.tight_layout()\nplt.show()\nplt.close()","6bb4c12e":"# WordVec with Fastai Tabular","215b871f":"# TFIDF - Tabular","a2d16c94":"## Backward Training","2d6ef256":"# Tree Regressor","ec2dceeb":"# Language Model","f91e8c27":"# Prediction","3570f15e":"# Fastai NLP Modelling","7e893d62":"## Forward Training","0563c836":"## Forward Training","cdd6ee01":"# Regression Model","e4c6191a":"## Backward Training","f9e10661":"# EDA","c20336a0":"# Train-Val split"}}