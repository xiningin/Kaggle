{"cell_type":{"f20db8c6":"code","d07db401":"code","326306b6":"code","7e46f044":"code","f285c6b1":"code","e8234eec":"code","7e336d95":"code","d53d69ec":"code","7dfb9223":"code","68035de7":"code","de55e9c1":"code","4572a550":"code","5b693045":"code","3ccff2cb":"code","2a1d368b":"code","86e5fb02":"code","7b67ef4a":"code","55886c82":"code","4de0738b":"code","2333029c":"code","3a9102aa":"code","0ae5de70":"code","68056d69":"code","5dd6242c":"code","4fbde919":"code","13dc1cfa":"code","f0b1292b":"code","c31b2c85":"code","cc1f342c":"code","6ab1eb1e":"code","f02a3111":"code","09f43a56":"code","0548c395":"code","afb733d5":"code","4fe865f0":"code","5146f0f5":"code","aac3dcfd":"code","e7452f23":"code","a8772640":"code","915e7e22":"code","5a533cdd":"code","769d5911":"code","df7e45e9":"code","ca0823e4":"code","a4ccb482":"code","41aa6e26":"code","13498e88":"code","208cbd78":"code","06b210b6":"code","b642e2bf":"markdown","46dcfe6e":"markdown","525d886b":"markdown","b00c6abd":"markdown","769a888d":"markdown","71384d8f":"markdown","123edc1b":"markdown","2de08d6a":"markdown","41fede47":"markdown","3ce02e78":"markdown","7368cbc0":"markdown","5ee6aab2":"markdown","798aafeb":"markdown","c79fce67":"markdown","73e44ca8":"markdown","980d6343":"markdown","25379c6a":"markdown","e1dfe526":"markdown","5faaa9d5":"markdown","ac210052":"markdown","67beaf1d":"markdown"},"source":{"f20db8c6":"# Basic Libraries\nimport numpy as np\nimport pandas as pd\nfrom warnings import filterwarnings\nfrom collections import Counter\n\n# Visualizations Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.offline as pyo\nimport plotly.express as px\nimport plotly.graph_objs as go\npyo.init_notebook_mode()\nimport plotly.figure_factory as ff\nimport missingno as msno\n\n# Data Pre-processing Libraries\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\n# Modelling Libraries\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier, plot_importance\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier,SGDClassifier,PassiveAggressiveClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.svm import SVC,LinearSVC,NuSVC\nfrom sklearn.neighbors import KNeighborsClassifier,NearestCentroid\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB\nfrom sklearn.ensemble import VotingClassifier\n\n# Evaluation & CV Libraries\nfrom sklearn.metrics import precision_score,accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV,RepeatedStratifiedKFold\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","d07db401":"df=pd.read_csv('..\/input\/water-potability\/water_potability.csv')","326306b6":"df.info()","7e46f044":"df[['ph','Hardness','Solids','Chloramines','Sulfate','Conductivity','Organic_carbon','Trihalomethanes','Turbidity',\"Potability\"]].groupby([\"Potability\"], as_index = False).mean().sort_values(by = \"Potability\").style.background_gradient(\"Wistia\")\n","f285c6b1":"target = 'Potability'\nfeatures_list = list(df.columns)\nfeatures_list.remove(target)","e8234eec":"df[features_list].hist(bins=40, edgecolor='b', linewidth=1.0,\n                          xlabelsize=8, ylabelsize=8, grid=False, \n                          figsize=(16,6), color='red')    \nplt.tight_layout(rect=(0, 0, 1.2, 1.2))   \nplt.suptitle('Water Potability', x=0.65, y=1.25, fontsize=14);  ","7e336d95":"# for visualizing correlations\nf, ax = plt.subplots(figsize=(10, 6))\ncorr = df.corr()\nhm = sns.heatmap(round(corr,2), annot=True, ax=ax, cmap=\"Reds\",fmt='.2f',\n            linewidths=.05)\nf.subplots_adjust(top=0.93)\nt= f.suptitle('Water Attributes Correlation Heatmap', fontsize=14)","d53d69ec":"print(\"Do we have data with null in columns?\")\ndf.columns[df.isnull().any()]","7dfb9223":"df.isnull().sum()","68035de7":"fig = msno.matrix(df,color=(0,0.5,0.5))","de55e9c1":"def replace_nan_by_mean(info):\n    for col in info.columns:\n        info[col].fillna(np.mean(info[col]),inplace=True)\n    return info\ndf=replace_nan_by_mean(df)","4572a550":"fig = msno.matrix(df,color=(0,0.5,0.5))","5b693045":"df['ph'].fillna(value=df['ph'].median(),inplace=True)\ndf['Sulfate'].fillna(value=df['Sulfate'].median(),inplace=True)\ndf['Trihalomethanes'].fillna(value=df['Trihalomethanes'].median(),inplace=True)","3ccff2cb":"df.isnull().sum()","2a1d368b":"X = df.drop('Potability',axis=1).values\ny = df['Potability'].values","86e5fb02":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","7b67ef4a":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","55886c82":"lr = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs')\nlr.fit(X_train, y_train)","4de0738b":"# accuracy score\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nlr_acc = accuracy_score(y_test, lr.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, lr.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {lr_acc}\\n\")","2333029c":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)","3a9102aa":"# accuracy score\n\nknn_acc = accuracy_score(y_test, knn.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, knn.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {knn_acc}\\n\")","0ae5de70":"svc = SVC()\nsvc.fit(X_train, y_train)","68056d69":"# accuracy score\n\nsvc_acc = accuracy_score(y_test, svc.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, svc.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {svc_acc}\\n\")","5dd6242c":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nparameters = {\n    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1],\n    'loss' : ['hinge', 'log'],\n    'penalty' : ['l1', 'l2']\n}\n\ngrid_search = GridSearchCV(sgd, parameters, cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)","4fbde919":"# best parameter and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","13dc1cfa":"sgd = SGDClassifier(alpha = 0.001, loss = 'log', penalty = 'l1')\nsgd.fit(X_train, y_train)\n\ny_pred = sgd.predict(X_test)\n\nprint(accuracy_score(y_train, sgd.predict(X_train)))\n\nsgd_acc = accuracy_score(y_test, sgd.predict(X_test))\nprint(sgd_acc)","f0b1292b":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)","c31b2c85":"# accuracy score\n\ndtc_acc = accuracy_score(y_test, dtc.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, dtc.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {dtc_acc}\\n\")","cc1f342c":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)","6ab1eb1e":"# accuracy score\n\nrf_acc = accuracy_score(y_test, rf.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, rf.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {rf_acc}\\n\")","f02a3111":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator = dtc)\nada.fit(X_train, y_train)","09f43a56":"# accuracy score\n\nada_acc = accuracy_score(y_test, ada.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, ada.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {ada_acc}\\n\")","0548c395":"# hyper parameter tuning using grid search cv\n\ngrid_param = {\n    'n_estimators' : [40, 50, 70, 80, 100],\n    'learning_rate' : [0.01, 0.1, 0.05, 0.5, 1, 10],\n    'algorithm' : ['SAMME', 'SAMME.R']\n}\n\ngrid_search = GridSearchCV(ada, grid_param, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, y_train)","afb733d5":"# best parameters and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","4fe865f0":"ada = AdaBoostClassifier(base_estimator = ada, algorithm = 'SAMME.R', learning_rate = 0.5, n_estimators = 50)\nada.fit(X_train, y_train)","5146f0f5":"# accuracy score\n\nada_acc = accuracy_score(y_test, ada.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, ada.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {ada_acc}\\n\")","aac3dcfd":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)","e7452f23":"# accuracy score\n\ngb_acc = accuracy_score(y_test, gb.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, gb.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {gb_acc}\\n\")","a8772640":"sgb = GradientBoostingClassifier(subsample = 0.9, max_features = 0.8)\nsgb.fit(X_train, y_train)","915e7e22":"# accuracy score\n\nsgb_acc = accuracy_score(y_test, sgb.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, sgb.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {sgb_acc}\\n\")","5a533cdd":"xgb = XGBClassifier(learning_rate = 0.1, loss = 'deviance', n_estimators = 100)\nxgb.fit(X_train, y_train)","769d5911":"# accuracy score\n\nxgb_acc = accuracy_score(y_test, xgb.predict(X_test))\n\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, xgb.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {xgb_acc}\\n\")","df7e45e9":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier()\nlgbm.fit(X_train, y_train)\n\nlgbm_acc = accuracy_score(y_test, lgbm.predict(X_test))\n\nprint(f\"Training Accuracy of Decision Tree Classifier is {accuracy_score(y_train, lgbm.predict(X_train))}\")\nprint(f\"Test Accuracy of Decision Tree Classifier is {lgbm_acc} \\n\")\n","ca0823e4":"from catboost import CatBoostClassifier\n\ncat = CatBoostClassifier()\ncat.fit(X_train, y_train)","a4ccb482":"cat_acc = accuracy_score(y_test, cat.predict(X_test))\n\nprint(f\"Training Accuracy of Decision Tree Classifier is {accuracy_score(y_train, cat.predict(X_train))}\")\nprint(f\"Test Accuracy of Decision Tree Classifier is {cat_acc} \\n\")","41aa6e26":"from sklearn.ensemble import ExtraTreesClassifier\n\netc = ExtraTreesClassifier()\netc.fit(X_train, y_train)","13498e88":"etc_acc = accuracy_score(y_test, etc.predict(X_test))\n\nprint(f\"Training Accuracy of Decision Tree Classifier is {accuracy_score(y_train, etc.predict(X_train))}\")\nprint(f\"Test Accuracy of Decision Tree Classifier is {etc_acc} \\n\")","208cbd78":"models = pd.DataFrame({\n    'Model' : ['Logistic Regression', 'KNN', 'SVC', 'SGD',  'Decision Tree', 'Random Forest','Ada Boost',\n             'Gradient Boosting', 'SGB', 'XgBoost', 'LGBM', 'Cat Boost', 'Extra Tree'],\n    'Score' : [lr_acc, knn_acc, svc_acc, sgd_acc, dtc_acc, rf_acc, ada_acc, gb_acc, sgb_acc, xgb_acc, lgbm_acc, cat_acc, etc_acc]\n})\n\n\nmodels.sort_values(by = 'Score', ascending = False)","06b210b6":"plt.figure(figsize = (20, 8))\n\nsns.barplot(x = 'Model', y = 'Score', data = models)\nplt.ylim(0.45, 0.75)\nplt.show()","b642e2bf":"# Feature Description\n\n1) ph: pH of 1. water (0 to 14).\n\n2) Hardness: Capacity of water to precipitate soap in mg\/L.\n\n3) Solids: Total dissolved solids in ppm.\n\n4) Chloramines: Amount of Chloramines in ppm.\n\n5) Sulfate: Amount of Sulfates dissolved in mg\/L.\n\n6) Conductivity: Electrical conductivity of water in \u03bcS\/cm.\n\n7) Organic_carbon: Amount of organic carbon in ppm.\n\n8) Trihalomethanes: Amount of Trihalomethanes in \u03bcg\/L.\n\n9) Turbidity: Measure of light emiting property of water in NTU.\n\n10) Potability: Indicates if water is safe for human consumption. Potable - 1 and Not potable - 0\n\n","46dcfe6e":"# SGD Classifier","525d886b":"# XgBoost","b00c6abd":"# SVC","769a888d":"# Extra Trees Classifier","71384d8f":"# Light Gradient Boosting Classifier","123edc1b":"# Replacing missing value by mean of all values in respective column","2de08d6a":"# Gradient Boosting Classifier","41fede47":"# \u0130mport The DataSet","3ce02e78":"# Random Forest","7368cbc0":"# Stochastic Gradient Boosting (SGB)","5ee6aab2":"# KNN","798aafeb":"# If you like my kernel, please do upvote :)","c79fce67":"# Cat Boost Classifier","73e44ca8":"### We will take a look at if there is any missing data in our data. If there are, we will try to eliminate them.","980d6343":"# Decision Tree","25379c6a":"Replacing missing value by mean of all values in respective column\ndef replace_nan_by_mean(info):\n    for col in info.columns:\n        info[col].fillna(np.mean(info[col]),inplace=True)\n    return info\ndata=replace_nan_by_mean(data)","e1dfe526":"# Logistic Regression","5faaa9d5":"# Correlation Matrix","ac210052":"# Ada Boost Classifier","67beaf1d":"# Features distributions\n  * Univariate Analysis (features and target 'Potability')\n"}}