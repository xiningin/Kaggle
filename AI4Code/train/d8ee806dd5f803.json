{"cell_type":{"06921f3d":"code","76773f0f":"code","79e997ef":"code","894e1595":"code","31255529":"code","dc46f273":"code","fedcc9a4":"code","1e1357ae":"code","cc354798":"code","126577df":"code","00b96eb4":"code","71668ff5":"code","4e69b2a5":"code","20216735":"code","70b99e99":"code","1f4cce10":"code","36d01d67":"code","86cf9eee":"code","89b92abf":"code","ed2279f3":"code","0a45a6dc":"code","fa2a07a1":"code","45db5922":"code","16eab0a7":"code","ecb4bf85":"code","2ff8585f":"code","489a5f6e":"code","8f3a099b":"code","e066b49f":"code","eac1dc23":"code","f7b63083":"code","ca1c0c8f":"code","5002ce9d":"code","2a9d1a16":"code","83ccd34e":"code","431e5dbc":"code","83e6fba5":"code","1767a16d":"code","e9dbb6fc":"code","bfa3a6b7":"code","1df44142":"code","65770e5f":"code","b413c514":"code","7c5d47d4":"code","2d7c1612":"code","bef487b2":"code","3f1d07e5":"code","9fda38d4":"code","a705c866":"code","7625353c":"code","d9e566f3":"code","dfd6ee98":"code","2ee9573f":"code","3eef30e0":"code","f826fa68":"code","c8b006b6":"code","dcd7e44f":"code","c39cbe38":"code","2c0cbd9a":"code","e7bd5df8":"code","fcc37559":"code","f8729000":"code","3498fa98":"code","c351f592":"code","b84179db":"code","a7945017":"code","f3128d14":"code","ab4f0518":"code","d2aa48ca":"code","427af28b":"code","6bdbc9db":"code","a2a4f8f4":"code","bc3e256f":"code","31c37b58":"code","ce8c144d":"code","dcccbcc6":"code","b6da01b7":"code","f9d2c7a7":"code","060f8c89":"code","e2d6ee7d":"code","727db849":"code","173ed924":"code","6c80de97":"code","8c05b5d6":"code","d65ce154":"markdown","178f64f6":"markdown","58af161e":"markdown","b5bb9425":"markdown","72c1b48c":"markdown","93ea8f1b":"markdown","4412417d":"markdown","d3fd2cdd":"markdown","797ceb29":"markdown","4d8f1ef4":"markdown","b1c7c1a8":"markdown","36740aba":"markdown","0d9d901c":"markdown"},"source":{"06921f3d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","76773f0f":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","79e997ef":"train.head()","894e1595":"test.head()","31255529":"train.shape, test.shape","dc46f273":"train.info(), test.info()","fedcc9a4":"train.columns","1e1357ae":"train.isnull().sum()","cc354798":"test.isnull().sum()","126577df":"# The Column Age contains almost 20% missing values and therefore we would need to impute them\ntrain['Age'].isnull().sum()\/len(train) * 100","00b96eb4":"# The Coulumn Cabin contains 77% missing values therefore, will simply drop this column\ntrain['Cabin'].isnull().sum()\/len(train) * 100","71668ff5":"# Dropping the columns - Cabin and Ticket Number. The ticket no is of no use in model building.\ntrain.drop(['Cabin','Ticket'], axis=1, inplace=True)\ntest.drop(['Cabin','Ticket'], axis=1, inplace=True)","4e69b2a5":"# Finding the survived percentage of people belonging to different Passenger Classes\n# First Class people survived in large numbers\n\ntrain[['Survived','Pclass']].groupby(['Pclass'], as_index=False).mean()","20216735":"# Looking at different ages and the number of people survived of that age\n\ntrain[['Age','Survived']].groupby(['Age'], as_index=False).sum()","70b99e99":"# Survived percentage of females was 74% while of males was close to 19%\n\ntrain[['Sex','Survived']].groupby(['Sex'], as_index=False).mean()","1f4cce10":"# People travelling with one siblings or spouse have higher survival percentage\n\ntrain[['SibSp','Survived']].groupby(['SibSp'], as_index=False).mean()","36d01d67":"# Looking at passengers travelling with Parents or Children and their survival rate\n\ntrain[['Parch','Survived']].groupby(['Parch'], as_index=False).mean()","86cf9eee":"# The survival rate of people embarking from Cherbourg was highest\n\ntrain[['Embarked','Survived']].groupby(['Embarked'], as_index=False).mean()","89b92abf":"# Count of People who survived and who didn't\n\nsns.countplot(x=\"Survived\", data=train)","ed2279f3":"# Count of Gender of People who survived and who didn't. Clearly, survival rate of males was very low.\n\nsns.countplot(x=\"Survived\", hue=\"Sex\", data=train)","0a45a6dc":"# Distribution of Ages. Most passengers belonging to 20-40 age bracket.\n\ntrain['Age'].plot.hist()","fa2a07a1":"# Plotting Ages with Survival\n\ng = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","45db5922":"# Found that the passengers in third class consisted of youth\n\nsns.boxplot(x=\"Pclass\", y=\"Age\", data=train)","16eab0a7":"# Survival rate of Third Class people was very low\n\nsns.countplot(x=\"Survived\", hue=\"Pclass\", data=train)","ecb4bf85":"# The Fare is rightly-skewed. Falling mostly in the 0-100 bucket.\n\ntrain['Fare'].plot.hist(bins = 20, figsize=(10,5))","2ff8585f":"# Most of the passengers were not travelling with Siblings or Spouse\n\nsns.countplot(x=\"SibSp\", data=train)","489a5f6e":"# Most of the passengers were not travelling with Parents or Children\n\nsns.countplot(x=\"Parch\", data=train)","8f3a099b":"# Age has 177 null values while Embarked contains 2\n\ntrain.isnull().sum()","e066b49f":"# Heatmap is also a way of finding null values. Its mostly used in case of a large dataset.\n\nsns.heatmap(train.isnull(), yticklabels=False)","eac1dc23":"test.isnull().sum()","f7b63083":"# Count of passengers from different port of Embarkation\n# C = Cherbourg, Q = Queenstown, S = Southampton\n\ntrain['Embarked'].value_counts()","ca1c0c8f":"# Filling the two missing values with the mode of the column, S = Southampton\n\ntrain['Embarked'] = train['Embarked'].fillna('S')","5002ce9d":"train.isnull().sum()","2a9d1a16":"train['Age']","83ccd34e":"train['Age'] = np.round(train['Age'],0)","431e5dbc":"# Mean ages of people travelling in different Passenger Classes\n\ntrain[['Pclass','Age']].groupby(['Pclass'], as_index=False).mean()","83e6fba5":"# Since Mean age is different for people travelling in different Classes\n# Imputing the null values in Age in accordance with the PClass they are travelling in\n\ntrain.loc[(train['Age'].isnull()) & (train['Pclass'] == 1), 'Age'] = 38\ntrain.loc[(train['Age'].isnull()) & (train['Pclass'] == 2), 'Age'] = 30\ntrain.loc[(train['Age'].isnull()) & (train['Pclass'] == 3), 'Age'] = 25","1767a16d":"# Similarly for Test Dataset, the mean ages of people travelling in different PClasses is different\n\ntest[['Pclass','Age']].groupby(['Pclass'], as_index=False).mean()","e9dbb6fc":"# Imputation done in a similar way\n\ntest.loc[(train['Age'].isnull()) & (test['Pclass'] == 1), 'Age'] = 41\ntest.loc[(train['Age'].isnull()) & (test['Pclass'] == 2), 'Age'] = 29\ntest.loc[(train['Age'].isnull()) & (test['Pclass'] == 3), 'Age'] = 24","bfa3a6b7":"# There was one missing record of Fare in Test data, filled it using the mean of the column\n\ntest.fillna(test['Fare'].mean(), inplace = True)","1df44142":"train.isnull().sum()","65770e5f":"test.isnull().sum()","b413c514":"# ALl the null values are treated","7c5d47d4":"# Doing One-hot encoding for categorical columns Sex and Embarked on both train and test data\n\nfrom sklearn.preprocessing import LabelEncoder \nle = LabelEncoder() \n  \ntrain['Sex'] = le.fit_transform(train['Sex']) #M = 1 and f = 0\ntrain['Embarked'] = le.fit_transform(train['Embarked']) #S = 2, C = 0 and Q = 1\ntest['Sex'] = le.fit_transform(test['Sex']) #M = 1 and f = 0\ntest['Embarked'] = le.fit_transform(test['Embarked']) #S = 2, C = 0 and Q = 1","2d7c1612":"train.head()","bef487b2":"test.head()","3f1d07e5":"# Extracting Titles from Names\n\ntrain['Title']=train['Name'].str.extract(' ([A-Za-z]+)\\.', expand = False)","9fda38d4":"# Extracting Titles from Names\n\ntest['Title']=test['Name'].str.extract(' ([A-Za-z]+)\\.', expand = False)","a705c866":"train.head()","7625353c":"# Counting the titles\n\ntrain['Title'].value_counts()","d9e566f3":"# Mapping the less frequent titles with other\n\nTitle_mapping = {'Dr':'Others','Rev':'Others','Mile':'Others','Col':'Others','Major':'Others','Capt':'Others','Don':'Others','Lady':'Others','Jonkheer':'Others','Mme':'Others','Countess':'Others','Sir':'Others','Ms':'Others','Mlle':'Others','Dona':'Others'}\ntrain['Title'] = train['Title'].replace(Title_mapping)\ntest['Title'] = test['Title'].replace(Title_mapping)","dfd6ee98":"# Performed one-hot encoding on Title column as well\n\ntrain['Title']= le.fit_transform(train['Title'])\ntest['Title']= le.fit_transform(test['Title'])","2ee9573f":"train.head()","3eef30e0":"# Dropped the Name column as it is of no use now\n\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)","f826fa68":"train.head()","c8b006b6":"# Most passengers onboard are of age 25\n\ntrain['Age'].value_counts()","dcd7e44f":"# Converting the Ages into age groups so as to make it easier for the model to make predictions\n\ntrain['Age group'] = pd.cut(train['Age'], 5)\ntest['Age group'] = pd.cut(test['Age'], 5)","c39cbe38":"# Most passengers belong in the age group of 16-32\n\ntrain['Age group'].value_counts()","2c0cbd9a":"# One hot encoding done on Age groups so that it becomes simplified\n# Stored the encoded values in the original Age column\n\ntrain['Age']= le.fit_transform(train['Age group'])\ntest['Age']= le.fit_transform(test['Age group'])","e7bd5df8":"train['Age'].value_counts()","fcc37559":"train.head()","f8729000":"# Dropping the Age Group column as our work is done\n\ntrain.drop('Age group', axis=1, inplace=True)","3498fa98":"test.drop('Age group', axis=1, inplace=True)","c351f592":"# Also making buckets of the fares. Grouping them so as to have a simplified approach.\n\ntrain['Fare group'] = pd.cut(train['Fare'], 4)\ntest['Fare group'] = pd.cut(test['Fare'], 4)","b84179db":"# Passengers in higher fare groups have higher chances of survival\n\ntrain[['Fare group','Survived']].groupby(['Fare group'], as_index=False).mean()","a7945017":"# Similarly, did one-hot encoding on the fare groups and stored the encoded values in the original column\n\ntrain['Fare']= le.fit_transform(train['Fare group'])\ntest['Fare']= le.fit_transform(test['Fare group'])","f3128d14":"train[['Fare','Survived']].groupby(['Fare'], as_index=False).mean()","ab4f0518":"train.head()","d2aa48ca":"# Dropping the Fare group column as our work is done\n\ntrain.drop('Fare group', axis=1, inplace=True)\ntest.drop('Fare group', axis=1, inplace=True)","427af28b":"train.drop('PassengerId', axis=1, inplace=True)\ntest.drop('PassengerId', axis=1, inplace=True)","6bdbc9db":"train.head()","a2a4f8f4":"test.head()","bc3e256f":"train_X = train.drop('Survived', axis = 1)  # Independent set of variables\ntrain_y = train['Survived']  # Dependent Variable","31c37b58":"train_X.shape, train_y.shape","ce8c144d":"from sklearn.linear_model import LogisticRegression\nlg_model = LogisticRegression()\nlg_model.fit(train_X,train_y)","dcccbcc6":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(lg_model,train_X, train_y,cv=5)\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","b6da01b7":"preds = lg_model.predict(test)\npreds","f9d2c7a7":"from sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(train_X, train_y)\nY_pred = decision_tree.predict(test)\nacc_decision_tree = round(decision_tree.score(train_X, train_y) * 100, 2)\ntest_decision_tree = round(decision_tree.score(test, Y_pred) * 100, 2)\nprint(acc_decision_tree)","060f8c89":"Y_pred","e2d6ee7d":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(train_X, train_y)\nY_pred = random_forest.predict(test)\nrandom_forest.score(train_X, train_y)\nacc_random_forest = round(random_forest.score(train_X, train_y) * 100, 2)\nacc_random_forest","727db849":"Y_pred","173ed924":"passIDs = pd.read_csv(\"..\/input\/titanic\/test.csv\")[[\"PassengerId\"]].values","6c80de97":"df_for_submission = {'PassengerId': passIDs.ravel(), 'Survived': Y_pred}\ndf_submission_predictions = pd.DataFrame(df_for_submission).set_index(['PassengerId'])\ndf_submission_predictions.head(10)","8c05b5d6":"df_submission_predictions.to_csv('MySubmission.csv')","d65ce154":"# Feature Extraction","178f64f6":"# EDA - Exploring the given Data","58af161e":"#### Importing Python Libraries that we will be using:","b5bb9425":"### Logistic Regression","72c1b48c":"### Random Forest","93ea8f1b":"### Decision Tree","4412417d":"# Scores given by Models\n\n### Logistic Regression = 79.46\n### Decision Trees = 87.43\n### Random Forest = 87.43\n\n### Therefore, going with Random Forest","d3fd2cdd":"**Some Variable Notes:**\n\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","797ceb29":"### Challenge:\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n#### Approach:\n\n1. EDA - We will do some Exploratory Data Analysis to explore the given dataset and will try to derive information as to which variables are important and which are not.\n\n2. Visualization - We will them be using Seaborn and Matplotlib to plot relationships of various parameters with the ultimate Survival. \n\n3. Data Cleaning - After that, we will see if there are any missing values in both train and test datasets and will try to omit\/impute them.\n\n4. Feature Engineering - This is the point where we will draw features from the dataset to make sure our model gives out the best results. This is one of the most important steps carried out in any model development. It can make or break your model.\n\n5. Model - Finally, we will be building our Model and will make predictions on the test dataset.","4d8f1ef4":"**Reading the Datasets that we will be working on:**\n\nTrain data consists of 891 records while test consists of 418 records","b1c7c1a8":"# Modeling","36740aba":"# Visualization","0d9d901c":"# Data Cleaning: Handling Null Values"}}