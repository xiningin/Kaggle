{"cell_type":{"6391c4d4":"code","61f61d4b":"code","f0bee999":"code","d481bafa":"code","152eb0d6":"code","5f262256":"code","5da5b538":"code","86458add":"code","02aaa53f":"code","9bbbc434":"code","4a0e3c6d":"code","76229938":"code","19e8e132":"code","49b8a5df":"code","d17c3d68":"code","b2f4c2fe":"code","1d6600c7":"code","15a2f687":"code","8146be47":"code","2003ece6":"code","9e3f4e75":"code","c0b393cb":"code","bdb7c206":"code","33484e9c":"code","3d961018":"code","f63d1e97":"code","648db2ed":"code","363c0983":"code","b30a6c39":"code","53c425cf":"code","71144c0e":"code","6119eff8":"code","71ea441e":"code","767c694d":"code","feefede5":"code","f39ffebb":"code","f27b01d5":"code","16d759ee":"code","545a8e0b":"code","50b63459":"code","68bcb9e3":"code","303878b3":"code","a6685e45":"code","645eb533":"code","5880b534":"code","663ed951":"code","d3943573":"code","fc3897ef":"code","6ece058c":"code","3859e7d6":"code","b125d703":"code","ba876743":"code","40fc0ce9":"code","45cd17c3":"code","a3678303":"code","3fc36ede":"code","7bfc3239":"code","0cdd93c5":"code","e92ae1b6":"code","92f9e67e":"code","138cdd48":"code","5f2b056c":"code","63e895bc":"code","51e244bd":"code","43f58a92":"code","613eafa2":"code","37b2af7e":"code","356707de":"markdown","556cd99e":"markdown","b141e29c":"markdown","b79717a5":"markdown","3584f227":"markdown"},"source":{"6391c4d4":"import pandas as pd\npd.options.mode.chained_assignment = None\ndf = pd.read_csv('\/kaggle\/input\/scl-2021-ds\/train.csv')","61f61d4b":"from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df, test_size=0.15, random_state=75)","f0bee999":"# train on full dataset\ntrain_df = pd.read_csv('\/kaggle\/input\/scl-2021-ds\/train.csv')","d481bafa":"len(train_df), len(test_df)","152eb0d6":"from string import punctuation\nimport re\n\ndef clean(s):\n    res = re.sub(r'(\\w)(\\()(\\w)', '\\g<1> \\g<2>\\g<3>', s)\n    res = re.sub(r'(\\w)([),.:;]+)(\\w)', '\\g<1>\\g<2> \\g<3>', res)\n    res = re.sub(r'(\\w)(\\.\\()(\\w)', '\\g<1>. (\\g<3>', res)\n    res = re.sub(r'\\s+', ' ', res)\n    res = res.strip()\n    return res\n\ndef stripclean(arr):\n    return [s.strip().strip(punctuation) for s in arr]\n\ndef dummy(x):\n    # stupid workaround to deep copy array cause i couldn't get it to work properly\n    return [s for s in x]","5f262256":"train_df['raw_address'] = train_df['raw_address'].apply(lambda x: x.strip())\ntrain_df['POI'] = train_df['POI\/street'].str.split('\/').str[0].apply(clean).str.split().apply(stripclean)\ntrain_df['STR'] = train_df['POI\/street'].str.split('\/').str[1].apply(clean).str.split().apply(stripclean)\ntrain_df['tokens'] = train_df['raw_address'].apply(clean).str.split()\ntrain_df['strip_tokens'] = train_df['tokens'].apply(stripclean)\ntrain_df['full_tokens'] = train_df['tokens'].apply(dummy)\ntrain_df['labels'] = train_df['tokens'].apply(lambda x: ['O'] * len(x))\ntrain_df['pos_poi'] = train_df['tokens'].apply(lambda x: [-1, -1])\ntrain_df['pos_str'] = train_df['tokens'].apply(lambda x: [-1, -1])","5da5b538":"train_df.head()","86458add":"test_df['raw_address'] = test_df['raw_address'].apply(lambda x: x.strip())\ntest_df['tokens'] = test_df['raw_address'].apply(clean).str.split()","02aaa53f":"test_df.head()","9bbbc434":"wordlist_raw = {}\nPOI_ERR_IDX = []\nSTR_ERR_IDX = []\nSHORTEN_IDX = []\nOVERLAP_IDX = set()","4a0e3c6d":"from tqdm import tqdm\n\nfor idx in tqdm(range(len(train_df))):\n    row = train_df.iloc[idx]\n    found_poi, found_str, shorten = False, False, False\n    for i in range(len(row['strip_tokens'])):\n        if row['strip_tokens'][i] == '': continue\n        if len(row['POI']) > 0 and row['POI'][0].startswith(row['strip_tokens'][i]):\n            ok = True\n            for j in range(len(row['POI'])):\n                if i + j >= len(row['strip_tokens']) or not row['POI'][j].startswith(row['strip_tokens'][i + j]):\n                    ok = False\n                    break\n            if ok:\n                found_poi = True\n                row['pos_poi'][0] = i\n                row['pos_poi'][1] = i + len(row['POI']) - 1\n                for j in range(len(row['POI'])):\n                    if row['labels'][i + j] != 'O':\n                        OVERLAP_IDX.add(row['id'])\n                    if len(row['POI']) == 1:       row['labels'][i + j] = 'S-POI'\n                    elif j == 0:                   row['labels'][i + j] = 'B-POI'\n                    elif j == len(row['POI']) - 1: row['labels'][i + j] = 'E-POI'\n                    else:                          row['labels'][i + j] = 'I-POI'\n                    if row['strip_tokens'][i + j] != row['POI'][j]:\n                        row['full_tokens'][i + j] = row['full_tokens'][i + j].replace(row['strip_tokens'][i + j], row['POI'][j])\n                        row['labels'][i + j] += '-SHORT'\n                        shorten = True\n                        if not row['strip_tokens'][i + j] in wordlist_raw: wordlist_raw[row['strip_tokens'][i + j]] = {}\n                        if not row['POI'][j] in wordlist_raw[row['strip_tokens'][i + j]]: wordlist_raw[row['strip_tokens'][i + j]][row['POI'][j]] = 0\n                        wordlist_raw[row['strip_tokens'][i + j]][row['POI'][j]] += 1\n        \n        if len(row['STR']) > 0 and row['STR'][0].startswith(row['strip_tokens'][i]):\n            ok = True\n            for j in range(len(row['STR'])):\n                if i + j >= len(row['strip_tokens']) or not row['STR'][j].startswith(row['strip_tokens'][i + j]):\n                    ok = False\n                    break\n            if ok:\n                found_str = True\n                row['pos_str'][0] = i\n                row['pos_str'][1] = i + len(row['STR']) - 1\n                for j in range(len(row['STR'])):\n                    if row['labels'][i + j] != 'O':\n                        OVERLAP_IDX.add(row['id'])\n                    if len(row['STR']) == 1:       row['labels'][i + j] = 'S-STR'\n                    elif j == 0:                   row['labels'][i + j] = 'B-STR'\n                    elif j == len(row['STR']) - 1: row['labels'][i + j] = 'E-STR'\n                    else:                          row['labels'][i + j] = 'I-STR'\n                    if row['strip_tokens'][i + j] != row['STR'][j]:\n                        row['full_tokens'][i + j] = row['full_tokens'][i + j].replace(row['strip_tokens'][i + j], row['STR'][j])\n                        row['labels'][i + j] += '-SHORT'\n                        shorten = True\n                        if not row['strip_tokens'][i + j] in wordlist_raw: wordlist_raw[row['strip_tokens'][i + j]] = {}\n                        if not row['STR'][j] in wordlist_raw[row['strip_tokens'][i + j]]: wordlist_raw[row['strip_tokens'][i + j]][row['STR'][j]] = 0\n                        wordlist_raw[row['strip_tokens'][i + j]][row['STR'][j]] += 1\n    \n    if len(row['POI']) > 0 and not found_poi:\n        POI_ERR_IDX.append(row['id'])\n    if len(row['STR']) > 0 and not found_str:\n        STR_ERR_IDX.append(row['id'])\n    if shorten:\n        SHORTEN_IDX.append(row['id'])","76229938":"len(wordlist_raw), len(POI_ERR_IDX), len(STR_ERR_IDX), len(SHORTEN_IDX), len(OVERLAP_IDX)","19e8e132":"train_df[train_df['id'].isin(SHORTEN_IDX[:10])]","49b8a5df":"ERR_IDX = set(POI_ERR_IDX + STR_ERR_IDX + list(OVERLAP_IDX))\nlen(ERR_IDX)","d17c3d68":"train_df = train_df[~train_df['id'].isin(ERR_IDX)]","b2f4c2fe":"def cleanshort(arr):\n    return [s.replace('-SHORT', '') for s in arr]\n\nnew_train_df = train_df[train_df['id'].isin(SHORTEN_IDX)].copy(deep=True)\nnew_train_df['tokens'] = new_train_df['full_tokens'].apply(dummy)\nnew_train_df['labels'] = new_train_df['labels'].apply(cleanshort)","1d6600c7":"new_train_df.head()","15a2f687":"train_df = train_df.append(new_train_df, ignore_index=True)","8146be47":"from tqdm import tqdm\n\nswap_parts = []\nswap_tokens = []\nswap_labels = []","2003ece6":"for idx in tqdm(range(len(train_df))):\n    old_row = train_df.iloc[idx]\n    if old_row['pos_poi'][0] == -1 or old_row['pos_str'][0] == -1: continue\n    \n    start_poi, end_poi = old_row['pos_poi']\n    start_str, end_str = old_row['pos_str']\n    if end_poi < start_str:\n        swap_tokens.append(old_row['tokens'][:start_poi] + \\\n                           old_row['tokens'][start_str:end_str + 1] + \\\n                           old_row['tokens'][end_poi + 1:start_str] + \\\n                           old_row['tokens'][start_poi:end_poi + 1] + \\\n                           old_row['tokens'][end_str + 1:])\n        swap_labels.append(old_row['labels'][:start_poi] + \\\n                           old_row['labels'][start_str:end_str + 1] + \\\n                           old_row['labels'][end_poi + 1:start_str] + \\\n                           old_row['labels'][start_poi:end_poi + 1] + \\\n                           old_row['labels'][end_str + 1:])\n        swap_parts.append((0, \\\n                           old_row['tokens'][:start_poi], \\\n                           old_row['tokens'][start_str:end_str + 1], \\\n                           old_row['tokens'][end_poi + 1:start_str], \\\n                           old_row['tokens'][start_poi:end_poi + 1], \\\n                           old_row['tokens'][end_str + 1:], \\\n                           old_row['labels'][:start_poi], \\\n                           old_row['labels'][start_str:end_str + 1], \\\n                           old_row['labels'][end_poi + 1:start_str], \\\n                           old_row['labels'][start_poi:end_poi + 1], \\\n                           old_row['labels'][end_str + 1:]))\n    else:\n        swap_tokens.append(old_row['tokens'][:start_str] + \\\n                           old_row['tokens'][start_poi:end_poi + 1] + \\\n                           old_row['tokens'][end_str + 1:start_poi] + \\\n                           old_row['tokens'][start_str:end_str + 1] + \\\n                           old_row['tokens'][end_poi + 1:])\n        swap_labels.append(old_row['labels'][:start_str] + \\\n                           old_row['labels'][start_poi:end_poi + 1] + \\\n                           old_row['labels'][end_str + 1:start_poi] + \\\n                           old_row['labels'][start_str:end_str + 1] + \\\n                           old_row['labels'][end_poi + 1:])\n        swap_parts.append((1, \\\n                           old_row['tokens'][:start_str], \\\n                           old_row['tokens'][start_poi:end_poi + 1], \\\n                           old_row['tokens'][end_str + 1:start_poi], \\\n                           old_row['tokens'][start_str:end_str + 1], \\\n                           old_row['tokens'][end_poi + 1:], \\\n                           old_row['labels'][:start_str], \\\n                           old_row['labels'][start_poi:end_poi + 1], \\\n                           old_row['labels'][end_str + 1:start_poi], \\\n                           old_row['labels'][start_str:end_str + 1], \\\n                           old_row['labels'][end_poi + 1:]))","9e3f4e75":"import random\nswap_idx = list(range(len(swap_parts)))\nrandom.Random(4).shuffle(swap_idx)","c0b393cb":"for i in tqdm(range(len(swap_parts))):\n    if i == swap_idx[i]: continue\n    j = swap_idx[i]\n    \n    swap_tokens.append(swap_parts[i][1] + swap_parts[j][2] + swap_parts[i][3] + swap_parts[j][4] + swap_parts[i][5])\n    swap_labels.append(swap_parts[i][6] + swap_parts[j][7] + swap_parts[i][8] + swap_parts[j][9] + swap_parts[i][10])","bdb7c206":"swap_train_df = pd.DataFrame(columns=['tokens', 'labels'], data={'tokens': swap_tokens, 'labels': swap_labels})\nswap_train_df.head()","33484e9c":"train_df.drop(columns=['id', 'raw_address', 'POI\/street', 'POI', 'STR', 'strip_tokens', 'full_tokens', 'pos_poi', 'pos_str'], inplace=True)","3d961018":"train_df = train_df.append(swap_train_df, ignore_index=True)","f63d1e97":"train_df.head()","648db2ed":"len(train_df)","363c0983":"train_df.to_csv('train.csv', index=False)","b30a6c39":"import json\n\nwith open('wordlist_raw.json', 'w') as fp:\n    json.dump(wordlist_raw, fp)","53c425cf":"len(wordlist_raw)","71144c0e":"def get_list(raw, p, lmt):\n    res = {}\n    for word, stats in raw.items():\n        best = max(stats, key=stats.get)\n        best_cnt = stats[best]\n        total = sum(stats.values())\n        frac = best_cnt \/ total\n        if total >= lmt and best_cnt \/ total >= p: \n            res[word] = best\n    return res","6119eff8":"wordlist = get_list(wordlist_raw, 0, 1)","71ea441e":"len(wordlist)","767c694d":"import json\n\nwith open('wordlist.json', 'w') as fp:\n    json.dump(wordlist, fp)","feefede5":"!pip install -q ohmeow-blurr","f39ffebb":"# turn off multithreading to avoid deadlock\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","f27b01d5":"import ast\n\nfrom transformers import *\nfrom fastai.text.all import *\n\nfrom blurr.data.all import *\nfrom blurr.modeling.all import *\n\nSEED = 42\nset_seed(SEED)","16d759ee":"labels = sorted(list(set([lbls for sublist in train_df.labels.tolist() for lbls in sublist])))\nprint(labels)","545a8e0b":"task = HF_TASKS_AUTO.TokenClassification\n# pretrained_model_name = 'bert-base-multilingual-uncased'\npretrained_model_name = 'indobenchmark\/indobert-large-p1'\nconfig = AutoConfig.from_pretrained(pretrained_model_name)\nconfig.num_labels = len(labels)\n\nhf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n                                                                               task=task, \n                                                                               config=config)\nhf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)","50b63459":"before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model,\n                                                     is_split_into_words=True, \n                                                     tok_kwargs={ 'return_special_tokens_mask': True })\n\nblocks = (\n    HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput), \n    HF_TokenCategoryBlock(vocab=labels)\n)\n\ndef get_y(inp): return [(label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels)]","68bcb9e3":"db = DataBlock(blocks=blocks, \n               splitter=RandomSplitter(seed=42),\n               get_x=ColReader('tokens'),\n               get_y=get_y)","303878b3":"dls = db.dataloaders(train_df, bs=128)\ndls.show_batch(dataloaders=dls)","a6685e45":"# Cell\n@delegates()\nclass TokenCrossEntropyLossFlat(BaseLoss):\n    \"Same as `CrossEntropyLossFlat`, but for mutiple tokens output\"\n    y_int = True\n    @use_kwargs_dict(keep=True, weight=None, ignore_index=-100, reduction='mean')\n    def __init__(self, *args, axis=-1, **kwargs): super().__init__(nn.CrossEntropyLoss, *args, axis=axis, **kwargs)\n    def decodes(self, x):    return L([ i.argmax(dim=self.axis) for i in x ])\n    def activation(self, x): return L([ F.softmax(i, dim=self.axis) for i in x ])","645eb533":"model = HF_BaseModelWrapper(hf_model)\nloss_func = TokenCrossEntropyLossFlat()\nopt_func = partial(Adam)\nlearn_cbs = [HF_BaseModelCallback]\nfit_cbs = [HF_TokenClassMetricsCallback()]\nsplitter = hf_splitter","5880b534":"learn = Learner(dls, model, loss_func=loss_func, opt_func=opt_func, splitter=splitter, cbs=learn_cbs).to_fp16()","663ed951":"learn.create_opt()","d3943573":"learn.unfreeze()","fc3897ef":"learn.fit_one_cycle(5, 1e-4, moms=(0.8, 0.7, 0.8), cbs=fit_cbs)","6ece058c":"learn.recorder.plot_loss()","3859e7d6":"print(learn.token_classification_report)","b125d703":"learn.save('model')","ba876743":"@patch\ndef blurr_predict(self:Learner, items, rm_type_tfms=None):\n    hf_before_batch_tfm = get_blurr_tfm(self.dls.before_batch)\n    is_split_str = hf_before_batch_tfm.is_split_into_words and isinstance(items[0], str)\n    is_df = isinstance(items, pd.DataFrame)\n    if (not is_df and (is_split_str or not is_listy(items))): items = [items]\n    dl = self.dls.test_dl(items, rm_type_tfms=rm_type_tfms, num_workers=0)\n    with self.no_bar(): probs, _, decoded_preds = self.get_preds(dl=dl, with_input=False, with_decoded=True)\n    trg_tfms = self.dls.tfms[self.dls.n_inp:]\n    outs = []\n    probs, decoded_preds = L(probs), L(decoded_preds)\n    for i in range(len(items)):\n        item_probs = [probs[i]]\n        item_dec_preds = [decoded_preds[i]]\n        item_dec_labels = tuplify([tfm.decode(item_dec_preds[tfm_idx]) for tfm_idx, tfm in enumerate(trg_tfms)])\n        outs.append((item_dec_labels, item_dec_preds, item_probs))\n    return outs","40fc0ce9":"from string import punctuation\n\ndef reconstruct(num, pred, raw_tokens, raw_address):\n    def complete_word(x):\n        y = x.strip().strip(punctuation)\n        if y != '' and y in wordlist:\n            x = x.replace(y, wordlist[y])\n        return x\n    \n    def normalize_bracket(x):\n        if '(' in x and ')' not in x:\n            x = x + ')'\n        elif ')' in x and '(' not in x:\n            x = '(' + x\n        return x\n    \n    ans = ['\/'] * num\n    for idx in range(num):\n        res = pred[idx]\n        start_poi, end_poi = -1, -1\n        start_str, end_str = -1, -1\n        for i in range(len(res[0])):\n            if 'POI' in res[1][i]:\n                if start_poi == -1: start_poi = i\n                end_poi = i\n            if 'STR' in res[1][i]:\n                if start_str == -1: start_str = i\n                end_str = i\n        \n        if start_poi != -1:\n            txt1 = raw_address[idx]\n            for i in range(start_poi):\n                txt1 = txt1[len(raw_tokens[idx][i]):].strip()\n            for i in range(len(raw_tokens[idx]) - 1, end_poi, -1):\n                txt1 = txt1[:-len(raw_tokens[idx][i])].strip()\n            \n            txt1_check = ''.join(raw_tokens[idx][start_poi:end_poi + 1]).replace(' ', '')\n            assert txt1.replace(' ', '') == txt1_check\n            \n            last = len(txt1)\n            for i in range(end_poi, start_poi - 1, -1):\n                while last > 0 and txt1[last - 1] == ' ':\n                    last -= 1\n                assert last >= len(raw_tokens[idx][i])\n                last -= len(raw_tokens[idx][i])\n                if 'SHORT' in res[1][i]:\n                    txt1 = txt1[:last] + complete_word(raw_tokens[idx][i]) + txt1[last + len(raw_tokens[idx][i]):]\n        else:\n            txt1 = ''\n        \n        if start_str != -1:\n            txt2 = raw_address[idx]\n            for i in range(start_str):\n                txt2 = txt2[len(raw_tokens[idx][i]):].strip()\n            for i in range(len(raw_tokens[idx]) - 1, end_str, -1):\n                txt2 = txt2[:-len(raw_tokens[idx][i])].strip()\n            \n            txt2_check = ''.join(raw_tokens[idx][start_str:end_str + 1]).replace(' ', '')\n            assert txt2.replace(' ', '') == txt2_check\n            \n            last = len(txt2)\n            for i in range(end_str, start_str - 1, -1):\n                while last > 0 and txt2[last - 1] == ' ':\n                    last -= 1\n                assert last >= len(raw_tokens[idx][i])\n                last -= len(raw_tokens[idx][i])\n                if 'SHORT' in res[1][i]:\n                    txt2 = txt2[:last] + complete_word(raw_tokens[idx][i]) + txt2[last + len(raw_tokens[idx][i]):]\n        else:\n            txt2 = ''\n        \n        txt1 = txt1.strip(punctuation)\n        txt2 = txt2.strip(punctuation)\n        txt1 = normalize_bracket(txt1)\n        txt2 = normalize_bracket(txt2)\n        \n        ans[idx] = (txt1 + '\/' + txt2)\n    \n    return ans","45cd17c3":"def show_diff(df):\n    MAX_ROWS = 50\n    CNT = 0\n    for idx in range(len(df)):\n        if CNT == MAX_ROWS: break\n        row = df.iloc[idx]\n        if row['POI\/street'] != row['pred']:\n            CNT += 1\n            print(idx, row['id'], row['POI\/street'], 'vs', row['pred'])","a3678303":"def calc_acc(df):\n    return df.loc[test_df['pred'] == df['POI\/street'], 'id'].count() \/ len(df)","3fc36ede":"raw_tokens = list(test_df['tokens'])\nraw_address = list(test_df['raw_address'])","7bfc3239":"raw_pred = learn.blurr_predict_tokens(raw_tokens)","0cdd93c5":"pred = reconstruct(len(test_df), raw_pred, raw_tokens, raw_address)","e92ae1b6":"test_df['pred'] = pred\ntest_df.head()","92f9e67e":"calc_acc(test_df)","138cdd48":"show_diff(test_df)","5f2b056c":"real_test_df = pd.read_csv('\/kaggle\/input\/scl-2021-ds\/test.csv')\nreal_test_df['raw_address'] = real_test_df['raw_address'].apply(lambda x: x.strip())\nreal_test_df['tokens'] = real_test_df['raw_address'].apply(clean).str.split()\nreal_test_df.head()","63e895bc":"raw_tokens = list(real_test_df['tokens'])\nraw_address = list(real_test_df['raw_address'])","51e244bd":"raw_pred = learn.blurr_predict_tokens(raw_tokens)","43f58a92":"real_test_df['POI\/street'] = reconstruct(len(real_test_df), raw_pred, raw_tokens, raw_address)","613eafa2":"real_test_df.drop(columns=['raw_address', 'tokens'], inplace=True)\nreal_test_df.head()","37b2af7e":"real_test_df.to_csv('submission.csv', index=False)","356707de":"# Build word list and token labels","556cd99e":"# Training\n- Fine tune a pretrained BERT model using `fastai` and `blurr`\n- **Ensembling** is not included in this notebook due to Kaggle kernel time limit\n- Our 1st place solution used ensembling of 2 models `bert-base-multilingual-uncased` and `indobenchmark\/indobert-large-p1` by taking the **average of the prediction probabilities for each word**, feel free to explore on your own :)","b141e29c":"# Prepare data\n- Format the data into **tokens** and **labels** for **Named Entity Recognition (NER)** task\n- Labels are heavily categorised to give the most useful data to the model: ['B-POI', 'B-POI-SHORT', 'B-STR', 'B-STR-SHORT', 'E-POI', 'E-POI-SHORT', 'E-STR', 'E-STR-SHORT', 'I-POI', 'I-POI-SHORT', 'I-STR', 'I-STR-SHORT', 'O', 'S-POI', 'S-POI-SHORT', 'S-STR', 'S-STR-SHORT']\n- **IOBES** tagging scheme is used for both **POI** and **STR**\n- An additional tag **SHORT** is used to label words that are **incomplete** and need to be fixed using a **prebuilt one-one dictionary wordlist** \n- Simple data augmentation is also used to increase the size of the training data (completing, swapping, etc.)","b79717a5":"# Evaluation\n- This is only relevant during model selection and testing\n- For the final training, full dataset is used so the accuracy below doesn't really reflect the power of the model.","3584f227":"# Submission"}}