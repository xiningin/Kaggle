{"cell_type":{"847fb8de":"code","95491700":"code","be89a412":"code","71aa2ff5":"code","c0834829":"code","f2754661":"code","8765cfd5":"code","963b5dc6":"code","cc80acf9":"code","f75a4987":"code","8f3ce1b2":"code","644f9e78":"code","61f0e5c7":"code","1a529b92":"code","fa158efd":"code","0d2fe33a":"code","92c01a18":"code","6464efa5":"code","ed9e56f4":"code","0be2aa5a":"code","02f68bd4":"code","6b649f43":"code","73edb391":"code","1a7a5fc6":"code","58ccc2b4":"code","b4807424":"code","08bf33f6":"code","548c696a":"code","297404c7":"code","cedba1bb":"code","ac3184d2":"code","8fcf28ad":"code","3e3a310f":"code","96ae808a":"code","8cdfebfb":"code","69f55b55":"code","8a8df812":"code","dd2a84c4":"code","7766391e":"code","5fb090c9":"code","c9bb860e":"code","01e1f3b4":"code","b2260976":"code","e17f85f2":"code","c4e85f39":"code","29627711":"markdown","e1cb9ac7":"markdown","3bdde733":"markdown","02adadb1":"markdown","d7cb7cc5":"markdown","96f937e5":"markdown","6b8075f7":"markdown","69b45c85":"markdown","f2f395c6":"markdown","8afc5816":"markdown","85f6c89f":"markdown","8552ad8a":"markdown","ce98ed5b":"markdown","cc5f4b46":"markdown","3b7d931a":"markdown","1d414ef0":"markdown","0cf7eb7c":"markdown","dfa1cda1":"markdown","340d3926":"markdown","286e3edf":"markdown","bfececf1":"markdown","28477612":"markdown","334fc2f9":"markdown","eaa8fea0":"markdown","99f6b3e3":"markdown","072a13c2":"markdown","58afe3ca":"markdown","671c1d83":"markdown","3b0fbe0c":"markdown","646fa723":"markdown","9ec9814f":"markdown","d71f6636":"markdown","7d1f157e":"markdown","b646d6b6":"markdown","365f0c55":"markdown","db539204":"markdown"},"source":{"847fb8de":"# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# automated feature engineering\nimport featuretools as ft\n\n# Filter out pandas warnings\nimport warnings \nwarnings.filterwarnings('ignore')","95491700":"# Read in the datasets and limit to the first 1000 rows (sorted by SK_ID_CURR) \n# This allows us to actually see the results in a reasonable amount of time! \napp_train = pd.read_csv('..\/input\/application_train.csv').sort_values('SK_ID_CURR').reset_index().loc[:1000, :].drop(columns = ['index'])\napp_test = pd.read_csv('..\/input\/application_test.csv').sort_values('SK_ID_CURR').reset_index().loc[:1000, :].drop(columns = ['index'])\nbureau = pd.read_csv('..\/input\/bureau.csv').sort_values(['SK_ID_CURR', 'SK_ID_BUREAU']).reset_index().loc[:1000, :].drop(columns = ['index'])\nbureau_balance = pd.read_csv('..\/input\/bureau_balance.csv').sort_values('SK_ID_BUREAU').reset_index().loc[:1000, :].drop(columns = ['index'])\ncash = pd.read_csv('..\/input\/POS_CASH_balance.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index().loc[:1000, :].drop(columns = ['index'])\ncredit = pd.read_csv('..\/input\/credit_card_balance.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index().loc[:1000, :].drop(columns = ['index'])\nprevious = pd.read_csv('..\/input\/previous_application.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index().loc[:1000, :].drop(columns = ['index'])\ninstallments = pd.read_csv('..\/input\/installments_payments.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index().loc[:1000, :].drop(columns = ['index'])","be89a412":"app_types = {}\n\n# Iterate through the columns and record the Boolean columns\nfor col in app_train:\n    # If column is a number with only two values, encode it as a Boolean\n    if (app_train[col].dtype != 'object') and (len(app_train[col].unique()) <= 2):\n        app_types[col] = ft.variable_types.Boolean\n\nprint('Number of boolean variables: ', len(app_types))","71aa2ff5":"# Record ordinal variables\napp_types['REGION_RATING_CLIENT'] = ft.variable_types.Ordinal\napp_types['REGION_RATING_CLIENT_W_CITY'] = ft.variable_types.Ordinal\n\napp_test_types = app_types.copy()\ndel app_test_types['TARGET']","c0834829":"# Record boolean variables in the previous data\nprevious_types= {'NFLAG_LAST_APPL_IN_DAY': ft.variable_types.Boolean,\n                 'NFLAG_INSURED_ON_APPROVAL': ft.variable_types.Boolean}","f2754661":"import re\n\ndef replace_day_outliers(df):\n    \"\"\"Replace 365243 with np.nan in any columns with DAYS\"\"\"\n    for col in df.columns:\n        if \"DAYS\" in col:\n            df[col] = df[col].replace({365243: np.nan})\n\n    return df\n\n# Replace all the day outliers\napp_train = replace_day_outliers(app_train)\napp_test = replace_day_outliers(app_test)\nbureau = replace_day_outliers(bureau)\nbureau_balance = replace_day_outliers(bureau_balance)\ncredit = replace_day_outliers(credit)\ncash = replace_day_outliers(cash)\nprevious = replace_day_outliers(previous)\ninstallments = replace_day_outliers(installments)","8765cfd5":"# Establish a starting date for all applications at Home Credit\nstart_date = pd.Timestamp(\"2016-01-01\")\nstart_date","963b5dc6":"# Convert to timedelta in days\nfor col in ['DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'DAYS_CREDIT_UPDATE']:\n    bureau[col] = pd.to_timedelta(bureau[col], 'D')\n    \nbureau[['DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'DAYS_CREDIT_UPDATE']].head()","cc80acf9":"# Create the date columns\nbureau['bureau_credit_application_date'] = start_date + bureau['DAYS_CREDIT']\nbureau['bureau_credit_end_date'] = start_date + bureau['DAYS_CREDIT_ENDDATE']\nbureau['bureau_credit_close_date'] = start_date + bureau['DAYS_ENDDATE_FACT']\nbureau['bureau_credit_update_date'] = start_date + bureau['DAYS_CREDIT_UPDATE']","f75a4987":"import matplotlib.pyplot as plt\nimport seaborn as sns\n# Set up default plot styles\nplt.rcParams['font.size'] = 26\nplt.style.use('fivethirtyeight')\n\n# Drop the time offset columns\nbureau = bureau.drop(columns = ['DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'DAYS_CREDIT_UPDATE'])\n\nplt.figure(figsize = (10, 8))\nsns.distplot((bureau['bureau_credit_end_date'] - bureau['bureau_credit_application_date']).dropna().dt.days);\nplt.xlabel('Length of Loan (Days)', size = 24); plt.ylabel('Density', size = 24); plt.title('Loan Length', size = 30);","8f3ce1b2":"# Convert to timedelta\nbureau_balance['MONTHS_BALANCE'] = pd.to_timedelta(bureau_balance['MONTHS_BALANCE'], 'M')\n\n# Make a date column\nbureau_balance['bureau_balance_date'] = start_date + bureau_balance['MONTHS_BALANCE']\nbureau_balance = bureau_balance.drop(columns = ['MONTHS_BALANCE'])\n\n# Select one loan and plot\nexample_credit = bureau_balance[bureau_balance['SK_ID_BUREAU'] == 5001709]\nplt.plot(example_credit['bureau_balance_date'], example_credit['STATUS'], 'ro');\nplt.title('Loan 5001709 over Time'); plt.xlabel('Date'); plt.ylabel('Status');","644f9e78":"# Convert to timedeltas in days\nfor col in ['DAYS_DECISION', 'DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION']:\n    previous[col] = pd.to_timedelta(previous[col], 'D')\n    \n# Make date columns\nprevious['previous_decision_date'] = start_date + previous['DAYS_DECISION']\nprevious['previous_drawing_date'] = start_date + previous['DAYS_FIRST_DRAWING']\nprevious['previous_first_due_date'] = start_date + previous['DAYS_FIRST_DUE']\nprevious['previous_last_duefirst_date'] = start_date + previous['DAYS_LAST_DUE_1ST_VERSION']\nprevious['previous_last_due_date'] = start_date + previous['DAYS_LAST_DUE']\nprevious['previous_termination_date'] = start_date + previous['DAYS_TERMINATION']\n\n# Drop the time offset columns\nprevious = previous.drop(columns = ['DAYS_DECISION', 'DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION'])\n\nplt.figure(figsize = (8, 6))\nexample_client = previous[previous['SK_ID_CURR'] == 100007]\nplt.plot(example_client['previous_decision_date'], example_client['AMT_CREDIT'], 'ro')\nplt.title('Client 100007 Previous Loan Amounts'); plt.xlabel('Date'); plt.ylabel('Credit Amount');","61f0e5c7":"# Convert to timedelta objects\ncredit['MONTHS_BALANCE'] = pd.to_timedelta(credit['MONTHS_BALANCE'], 'M')\ncash['MONTHS_BALANCE'] = pd.to_timedelta(cash['MONTHS_BALANCE'], 'M')\n\n# Make a date column\ncredit['credit_balance_date'] = start_date + credit['MONTHS_BALANCE']\ncredit = credit.drop(columns = ['MONTHS_BALANCE'])\n\n# Make a date column\ncash['cash_balance_date'] = start_date + cash['MONTHS_BALANCE']\ncash = cash.drop(columns = ['MONTHS_BALANCE'])\n\n# Select on loan and plot\nexample_credit = cash[cash['SK_ID_PREV'] == 1369693]\n\nplt.plot(example_credit['cash_balance_date'], example_credit['NAME_CONTRACT_STATUS'], 'ro');\nplt.title('Loan 1369693 over Time'); plt.xlabel('Date'); plt.ylabel('Contract Status');","1a529b92":"# Conver to time delta object\ninstallments['DAYS_INSTALMENT'] = pd.to_timedelta(installments['DAYS_INSTALMENT'], 'D')\ninstallments['DAYS_ENTRY_PAYMENT'] = pd.to_timedelta(installments['DAYS_ENTRY_PAYMENT'], 'D')\n\n# Create time column and drop\ninstallments['installments_due_date'] = start_date + installments['DAYS_INSTALMENT']\ninstallments = installments.drop(columns = ['DAYS_INSTALMENT'])\n\ninstallments['installments_paid_date'] = start_date + installments['DAYS_ENTRY_PAYMENT']\ninstallments = installments.drop(columns = ['DAYS_ENTRY_PAYMENT'])\n\n# Select one loan and plot\nexample_credit = installments[installments['SK_ID_PREV'] == 1369693]\nplt.plot((example_credit['installments_due_date'] - example_credit['installments_paid_date']).dt.days, example_credit['AMT_INSTALMENT'], 'ro');\nplt.title('Loan 1369693'); plt.xlabel('Days Paid Early'); plt.ylabel('Installment Amount');","fa158efd":"# Make an entityset\nes = ft.EntitySet(id = 'clients')","0d2fe33a":"# Entities with a unique index\nes = es.entity_from_dataframe(entity_id = 'app_train', dataframe = app_train, \n                              index = 'SK_ID_CURR', variable_types = app_types)\n\nes = es.entity_from_dataframe(entity_id = 'app_test', dataframe = app_test, \n                              index = 'SK_ID_CURR', variable_types = app_test_types)\n\nes = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, \n                              index = 'SK_ID_BUREAU', time_index='bureau_credit_application_date')\n\nes = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, \n                              index = 'SK_ID_PREV', time_index = 'previous_decision_date',\n                              variable_types = previous_types)\n\n# Entities that do not have a unique index\nes = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, \n                              make_index = True, index = 'bb_index',\n                              time_index = 'bureau_balance_date')\n\nes = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, \n                              make_index = True, index = 'cash_index',\n                              time_index = 'cash_balance_date')\n\nes = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,\n                              make_index = True, index = 'installments_index',\n                              time_index = 'installments_paid_date')\n\nes = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,\n                              make_index = True, index = 'credit_index',\n                              time_index = 'credit_balance_date')","92c01a18":"# Relationship between app and bureau\nr_app_bureau = ft.Relationship(es['app_train']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n\n# Test Relationship between app and bureau\nr_test_app_bureau = ft.Relationship(es['app_test']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n\n# Relationship between bureau and bureau balance\nr_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n\n# Relationship between current app and previous apps\nr_app_previous = ft.Relationship(es['app_train']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n\n# Test Relationship between current app and previous apps\nr_test_app_previous = ft.Relationship(es['app_test']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n\n# Relationships between previous apps and cash, installments, and credit\nr_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\nr_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\nr_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])\n\n# Add in the defined relationships\nes = es.add_relationships([r_app_bureau, r_test_app_bureau, r_bureau_balance, r_app_previous, r_test_app_previous,\n                           r_previous_cash, r_previous_installments, r_previous_credit])\n# Print out the EntitySet\nes","6464efa5":"time_features, time_feature_names = ft.dfs(entityset = es, target_entity = 'app_train', \n                                           trans_primitives = ['cum_sum', 'time_since_previous'], max_depth = 2,\n                                           agg_primitives = ['trend'] ,\n                                           features_only = False, verbose = True,\n                                           chunk_size = len(app_train),\n                                           ignore_entities = ['app_test'])","ed9e56f4":"time_features.iloc[:, -10:].head()","0be2aa5a":"plt.figure(figsize = (8, 6))\nplt.hist(time_features['TREND(bureau.AMT_CREDIT_SUM, bureau_credit_application_date)'].dropna(), edgecolor = 'k');\nplt.xlabel('TREND(bureau.AMT_CREDIT_SUM, bureau_credit_application_date)'); plt.ylabel('Counts'); plt.title('Distribution of Trends in Credit Sum');","02f68bd4":"plt.figure(figsize = (8, 6))\nplt.hist(time_features['TREND(previous.AMT_APPLICATION, previous_decision_date)'].dropna(), edgecolor = 'k');\nplt.xlabel('TREND(previous.AMT_APPLICATION, previous_decision_date)'); plt.ylabel('Counts'); plt.title('Distribution of Trends in Amount of Application');","6b649f43":"time_feature_names[-10:]","73edb391":"previous['NAME_CONTRACT_STATUS'].value_counts()","1a7a5fc6":"# Assign interesting values\nes['previous']['NAME_CONTRACT_STATUS'].interesting_values = ['Approved', 'Refused', 'Canceled']\n\n# Calculate the features with intereseting values\ninteresting_features, interesting_feature_names = ft.dfs(entityset=es, target_entity='app_train', max_depth = 1, \n                                                         where_primitives = ['mean', 'mode'], \n                                                         trans_primitives=[], features_only = False, verbose = True,\n                                                         chunk_size = len(app_train),\n                                                         ignore_entities = ['app_test'])","58ccc2b4":"interesting_features.iloc[:, -6:, ].head()","b4807424":"plt.figure(figsize = (10, 8))\nsns.kdeplot(interesting_features['MEAN(previous.CNT_PAYMENT WHERE NAME_CONTRACT_STATUS = Approved)'].dropna(), label = 'Approved')\nsns.kdeplot(interesting_features['MEAN(previous.CNT_PAYMENT WHERE NAME_CONTRACT_STATUS = Canceled)'].dropna(), label = 'Canceled')\nplt.xlabel('MEAN(previous.CNT_PAYMENT)'); plt.ylabel('Density'); plt.title('Average Term of Previous Credit');","08bf33f6":"# Plot of client type when contract was approved\nplt.figure(figsize = (20, 6))\nplt.subplot(1, 2, 1)\nplt.bar(list(range(3)), interesting_features['MODE(previous.NAME_CLIENT_TYPE WHERE NAME_CONTRACT_STATUS = Approved)'].value_counts())\nplt.xticks(list(range(3)), interesting_features['MODE(previous.NAME_CLIENT_TYPE WHERE NAME_CONTRACT_STATUS = Approved)'].value_counts().index);\nplt.xlabel(\"Client Type\"); plt.ylabel(\"Counts\");\nplt.title(\"Most Common Client Type where Contract was Approved\");\n\n# Plot of client type where contract was refused\nplt.subplot(1, 2, 2)\nplt.bar(list(range(4)), interesting_features['MODE(previous.NAME_CLIENT_TYPE WHERE NAME_CONTRACT_STATUS = Refused)'].value_counts())\nplt.xticks(list(range(4)), interesting_features['MODE(previous.NAME_CLIENT_TYPE WHERE NAME_CONTRACT_STATUS = Refused)'].value_counts().index);\nplt.xlabel(\"Client Type\"); plt.ylabel(\"Counts\");\nplt.title(\"Most Common Client Type where Contract was Refused\");","548c696a":"# Late Payment seed feature\nlate_payment = ft.Feature(es['installments']['installments_due_date']) < ft.Feature(es['installments']['installments_paid_date'])\n\n# Rename the feature\nlate_payment = late_payment.rename(\"late_payment\")\n\n# DFS with seed features\nseed_features, seed_feature_names = ft.dfs(entityset = es,\n                                           target_entity = 'app_train',\n                                           agg_primitives = ['percent_true', 'mean'],\n                                           trans_primitives = [], \n                                           seed_features = [late_payment],\n                                           features_only = False, verbose = True,\n                                           chunk_size = len(app_train),\n                                           ignore_entities = ['app_test'])","297404c7":"seed_features.iloc[:, -2:].head(10)","cedba1bb":"sns.kdeplot(seed_features['PERCENT_TRUE(installments.late_payment)'].dropna(), label = '')\nplt.xlabel('Late Installments'); plt.ylabel('Density'); plt.title('Late Installment Fraction by Client');","ac3184d2":"# Create a feed representing whether the loan is past due\npast_due = ft.Feature(es['bureau_balance']['STATUS']).isin(['1', '2', '3', '4', '5'])\npast_due = past_due.rename(\"past_due\")","8fcf28ad":"# DFS with specified seed feature\nseed_features, seed_feature_names = ft.dfs(entityset = es,\n                                           target_entity = 'app_train',\n                                           agg_primitives = ['percent_true', 'mean'],\n                                           trans_primitives = [], \n                                           seed_features = [past_due],\n                                           features_only = False, verbose = True,\n                                           chunk_size = len(app_train),\n                                           ignore_entities = ['app_test'])","3e3a310f":"from featuretools.variable_types import (\n    Boolean, Datetime,\n    DatetimeTimeIndex,\n    Discrete,\n    Index,\n    Numeric,\n    Variable,\n    Id\n)\n\nfrom featuretools.primitives import AggregationPrimitive, make_agg_primitive\nfrom datetime import datetime, timedelta\n\nfrom collections import Counter\n\n\ndef normalized_mode_count(x):\n    \"\"\"\n    Return the fraction of total observations that \n    are the most common observation. For example, \n    in an array of ['A', 'A', 'A', 'B', 'B'], the \n    function will return 0.6.\"\"\"\n    \n    if x.mode().shape[0] == 0:\n        return np.nan\n            \n    # Count occurence of each value\n    counts = dict(Counter(x.values))\n    # Find the mode\n    mode = x.mode().iloc[0]\n    # Divide the occurences of mode by the total occurrences\n    return counts[mode] \/ np.sum(list(counts.values()))\n    \n\nNormalizedModeCount = make_agg_primitive(function = normalized_mode_count, \n                                         input_types = [Discrete],\n                                         return_type = Numeric)\n\n# Function from https:\/\/codereview.stackexchange.com\/a\/15095\ndef longest_repetition(x):\n    \"\"\"\n    Returns the item with most consecutive occurrences in `x`. \n    If there are multiple items with the same number of conseqcutive occurrences,\n    it will return the first one. If `x` is empty, returns None. \n    \"\"\"\n    \n    x = x.dropna()\n    \n    if x.shape[0] < 1:\n        return None\n    \n    # Set the longest element\n    longest_element = current_element = None\n    longest_repeats = current_repeats = 0\n    \n    # Iterate through the iterable\n    for element in x:\n        if current_element == element:\n            current_repeats += 1\n        else:\n            current_element = element\n            current_repeats = 1\n        if current_repeats > longest_repeats:\n            longest_repeats = current_repeats\n            longest_element = current_element\n            \n    return longest_element\n\nLongestSeq = make_agg_primitive(function = longest_repetition,\n                                     input_types = [Discrete],\n                                     return_type = Discrete)    ","96ae808a":"# DFS with custom features\ncustom_features, custom_feature_names = ft.dfs(entityset = es,\n                                              target_entity = 'app_train',\n                                              agg_primitives = [NormalizedModeCount, LongestSeq],\n                                              max_depth = 2,\n                                              trans_primitives = [],\n                                              features_only = False, verbose = True,\n                                              chunk_size = len(app_train),\n                                              ignore_entities = ['app_test'])\n\ncustom_features.iloc[:, -40:].head()","8cdfebfb":"plt.figure(figsize = (8, 6))\nplt.bar(custom_features['LONGEST_REPETITION(previous.NAME_YIELD_GROUP)'].value_counts().index, custom_features['LONGEST_REPETITION(previous.NAME_YIELD_GROUP)'].value_counts(), edgecolor = 'k')\nplt.xlabel('NAME_YIELD_GROUP'); plt.ylabel('Counts'); plt.title('Longest Repetition of Previous Name Yield Group');","69f55b55":"plt.figure(figsize = (8, 6))\nsns.kdeplot(custom_features['NORMALIZED_MODE_COUNT(previous.NAME_PRODUCT_TYPE)'], label = 'NORMALIZED_MODE_COUNT(previous.NAME_PRODUCT_TYPE)')\nsns.kdeplot(custom_features['NORMALIZED_MODE_COUNT(bureau.CREDIT_ACTIVE)'], label = 'NORMALIZED_MODE_COUNT(bureau.CREDIT_ACTIVE)')","8a8df812":"# Building on the Trend Aggregation Primitive\n# Copied from https:\/\/github.com\/Featuretools\/featuretools\/blob\/master\/featuretools\/primitives\/aggregation_primitives.py\n\ndef most_recent(y, x):\n    df = pd.DataFrame({\"x\": x, \"y\": y}).dropna()\n            \n    if df.shape[0] < 1:\n        return np.nan\n\n    # Sort the values by timestamps reversed\n    df = df.sort_values('x', ascending = False).reset_index()\n\n    # Return the most recent occurence\n    return df.iloc[0]['y']\n\nMostRecent = make_agg_primitive(function = most_recent,\n                                input_types = [Discrete, Datetime],\n                                return_type = Discrete)","dd2a84c4":"# DFS with custom feature\ncustom_features, custom_feature_names = ft.dfs(entityset = es,\n                                              target_entity = 'app_train',\n                                              agg_primitives = ['last', MostRecent],\n                                               max_depth = 1,\n                                              trans_primitives = [],\n                                              features_only = False, verbose = True,\n                                              chunk_size = len(app_train),\n                                              ignore_entities = ['app_test'])","7766391e":"custom_features[['MOST_RECENT(bureau.CREDIT_TYPE, bureau_credit_end_date)', 'MOST_RECENT(bureau.CREDIT_TYPE, bureau_credit_application_date)']].head(10)","5fb090c9":"# Run and create the features\nfeature_matrix, feature_names = ft.dfs(entityset = es, target_entity = 'app_train',\n                                       agg_primitives = ['mean', 'max', 'min', 'trend', 'mode', 'count', \n                                                         'sum', 'percent_true', NormalizedModeCount, MostRecent, LongestSeq],\n                                       trans_primitives = ['diff', 'cum_sum', 'cum_mean', 'percentile'], \n                                       where_primitives = ['mean', 'sum'],\n                                       seed_features = [late_payment, past_due],\n                                       max_depth = 2, features_only = False, verbose = True,\n                                       chunk_size = len(app_train),\n                                       ignore_entities = ['app_test'])","c9bb860e":"# Run and create the features\nfeature_matrix_test, feature_names_test = ft.dfs(entityset = es, target_entity = 'app_test',\n                                                   agg_primitives = ['mean', 'max', 'min', 'trend', 'mode', 'count', \n                                                                     'sum', 'percent_true', NormalizedModeCount, MostRecent, LongestSeq],\n                                                   trans_primitives = ['diff', 'cum_sum', 'cum_mean', 'percentile'], \n                                                   where_primitives = ['mean', 'sum'],\n                                                   seed_features = [late_payment, past_due],\n                                                   max_depth = 2, features_only = False, verbose = True,\n                                                   chunk_size = len(app_test),\n                                                   ignore_entities = ['app_train'])","01e1f3b4":"import random\nrandom.sample(feature_names, 10)","b2260976":"from featuretools import selection\n\n# Remove low information features\nfeature_matrix2 = selection.remove_low_information_features(feature_matrix)\nprint('Removed %d features from training features'  % (feature_matrix.shape[1] - feature_matrix2.shape[1]))\n\nfeature_matrix_test2 = selection.remove_low_information_features(feature_matrix_test)\nprint('Removed %d features from testing features' % (feature_matrix_test.shape[1] - feature_matrix_test2.shape[1]))","e17f85f2":"train_labels = feature_matrix['TARGET']\nfeature_matrix, feature_matrix_test = feature_matrix2.align(feature_matrix_test2, join = 'inner', axis = 1)\nfeature_matrix['TARGET'] = train_labels\n\nprint('Final training shape: ', feature_matrix.shape)\nprint('Final testing shape: ', feature_matrix_test.shape)","c4e85f39":"# Save the feature matrix to a csv\nfeature_matrix.to_csv('feature_matrix.csv')\nfeature_matrix_test.to_csv('feature_matrix_test.csv')","29627711":"# Properly Representing Variable Types\n\nThere are a number of columns in the `app` dataframe that are represented as integers but are really discrete variables that can only take on a limited number of features. Some of these are Boolean flags (only 1 or 0) and two columns are ordinal (ordered discrete). To tell featuretools to treat these as Boolean variables, we need to pass in the correct datatype using a dictionary mapping {`variable_name`: `variable_type`}. ","e1cb9ac7":"The previous data also has two Boolean variables. ","3bdde733":" These four columns represent different offsets:\n\n* `DAYS_CREDIT`: Number of days before current application at Home Credit client applied for loan at other financial institution. We will call this the application date, `bureau_credit_application_date` and make it the `time_index` of the entity. \n* `DAYS_CREDIT_ENDDATE`: Number of days of credit remaining at time of client's application at Home Credit. We will call this the ending date, `bureau_credit_end_date`\n* `DAYS_ENDDATE_FACT`: For closed credits, the number of days before current application at Home Credit that credit at other financial institution ended. We will call this the closing date, `bureau_credit_close_date`. \n* `DAYS_CREDIT_UPDATE`: Number of days before current application at Home Credit that the most recent information about the previous credit arrived. We will call this the update date, `bureau_credit_update_date`. \n\nIf we were doing manual feature engineering, we might want to create new columns such as by subtracting `DAYS_CREDIT_ENDDATE` from `DAYS_CREDIT` to get the planned length of the loan in days, or subtracting `DAYS_CREDIT_ENDDATE` from `DAYS_ENDDATE_FACT` to find the number of days the client paid off the loan early. However, in this notebook we will not make any features by hand, but rather let featuretools develop useful features for us.\n\nTo make date columns from the `timedelta`, we simply add the offset to the start date. ","02adadb1":"We will now do the same operation applied to the test set. Doing the calculations separately should prevent leakage from the testing data into the training data.","d7cb7cc5":"First we can establish an arbitrary date and then convert the time offset in months into a Pandas `timedelta` object. ","96f937e5":"When we're done, we probably want to save the results to a csv. We want to be careful because the index of the dataframe is the identifying column, so we should keep the index. We also should align the training and testing dataframes to make sure they have the same columns.","6b8075f7":"It looks as if there are a number of loans that are unreasonably long. Reading through the discussions, other people had noticed this as well. At this point, we will just leave in the outliers. We also will drop the time offset columns.","69b45c85":"One of the features is `MEAN(previous.CNT_PAYMENT WHERE NAME_CONTRACT_STATUS = Approved)`. This shows the average \"term of previous credit\" on previous loans conditioned on the previous loan being approved. We can compare the distribution of this feature to the `MEAN(previous.CNT_PAYMENT WHERE NAME_CONTRACT_STATUS = Canceled)` to see how these loans differ.","f2f395c6":"#### Previous Credit and Cash\n\nThe `credit_card_balance` and `POS_CASH_balance` each have a `MONTHS_BALANCE` column with the month offset. This is the number of months before the current application at Home Credit of the previous application record. These will represent the `time_index` of the data. ","8afc5816":"### Relationships\n\nNot surprisingly, the relationships between tables has not changed since the previous implementation. ","85f6c89f":"For client 100002, the most recent type of credit was `Credit card` if we order by the application date, but `Consumer credit` if we order by the end date of the loan. Whether this is actually useful knowledge is hard to say! \n\n","8552ad8a":"Another seed feature we can use is whether or not a previous loan at another institution was past due. ","ce98ed5b":"# References Used \n[Automated Feature Engineering | Credit Default](https:\/\/www.kaggle.com\/davidrivasphd\/automated-feature-engineering-credit-default) by David Rivas, Ph.D.  \n[Tuning Automated Feature Engineering (Exploratory)](http:\/\/kaggle.com\/willkoehrsen\/tuning-automated-feature-engineering-exploratory\/notebook) by Will Koehrsen  \n\n\n","cc5f4b46":"### Plot for a sanity check\n\nTo make sure the conversion went as planned, let's make a plot showing the distribution of loan lengths.","3b7d931a":"# Advanced Auto-Feature Engineering | Credit Default  \n**David Rivas, Ph.D.**  \n  \n# Abstract  \nIn this notebook we will expand upon the [Automated Feature Engineering | Credit Default](http:\/\/kaggle.com\/davidrivasphd\/automated-feature-engineering-credit-default) applied to the Home Credit Default Risk competition. We will explore a few different methods for improving the set of features and incorporating domain knowledge into the final dataset. These methods include:\n\n* Properly representing variable types\n* Creating and using time variables\n* Setting interesting values of variables\n* Creating seed features\n* Building custom primitives\n\nReading through the discussion around this competition and working through some of the top kernels, intricate feature engineering is a must. Using the default feature primitives in the basic notebook did improve our score, but to do better we will need some more advanced methods. \n\nThis will be more as an exploration of the capabilities of featuretools than a complete implementation. I'm still working on figuring out the most useful features to build by reading through other kernels, finding features, and figuring out how to recreate and build upon those in featuretools. Any ideas would be much appreciated! \n\nThis work draws heavily on the [featuretools documentation](https:\/\/docs.featuretools.com\/) and the [featuretools GitHub repository](https:\/\/github.com\/Featuretools\/featuretools). ","1d414ef0":"#### Bureau Balance\n\nThe bureau balance dataframe has a `MONTHS_BALANCE` column that we can use as a months offset. The resulting column of dates can be used as a `time_index`.","0cf7eb7c":"# Seed Features\n\nAn additional extension to the default aggregations and transformations is to use [seed features](https:\/\/docs.featuretools.com\/automated_feature_engineering\/dfs_usage_tips.html#specifying-list-of-aggregation-functions). These are user defined features that we provide to deep feature synthesis that can then be built on top of where possible. \n\nAs an example, we can create a seed feature that determines whether or not a payment was late. This time when we make the `dfs` function call, we need to pass in the `seed_features` argument.","dfa1cda1":"# Putting it all Together\n\nFinally, we can run deep feature synthesis with the time variables, with the correct specified categorical variables, with the interesting features, with the seed features, and with the custom features. To actually run this on the entire dataset, we can take the code here, put it in a script, and then use more computational resources. ","340d3926":"# Applying Featuretools\n\nWe can now start making features using the time columns. We will create an entityset named clients much as before, but now we have time variables that we can use. ","286e3edf":"#### Installments Payments \n\nThe `installments_payments` data contains information on each payment made on the previous loans at Home Credit. It has two date offset columns:\n\n* `DAYS_INSTALMENT`: number of days before current application at Home Credit that previous installment was supposed to be paid\n* `DAYS_ENTRY_PAYMENT`: number of days before current application at Home Credit that previous installment was actually paid\n\nBy now the process should be familiar: convert to timedeltas and then make time columns. The DAYS_INSTALMENT will serve as the `time_index`. ","bfececf1":"These features could be completely useless, or they may be helpful. Only building a model and training it with the features will help us determine the answer. \n\n### MostRecent\n\nThe final custom feature will be `MOSTRECENT`. This simply returns the most recent value of a discrete variable with respect to time columns in a dataframe. When we create an entity, featuretools will [sort the entity](https:\/\/github.com\/Featuretools\/featuretools\/blob\/master\/featuretools\/entityset\/entity.py) by the `time_index`. Therefore, the built-in aggregation primitive `LAST` calculates the most recent value based on the time index. However, in cases where there are multiple different time columns, it might be useful to know the most recent value with respect to all of the times. To build the custom feature primitive, I adapted the existing `TREND` primitive ([code here](https:\/\/github.com\/Featuretools\/featuretools\/blob\/master\/featuretools\/primitives\/aggregation_primitives.py)). ","28477612":"## Time Features\n\nLet's look at some of the time features we can make from the new time variables. Because these times are relative and not absolute, we are only interested in values that show change over time, such as trend or cumulative sum. We would not want to calculate values like the year or month since we choose an arbitrary starting date. \n\nThroughout this notebook, we will pass in a `chunk_size` to the `dfs` call which specifies the number of rows (if an integer) or the fraction or rows to use in each chunk (if a float). This can help to optimize the `dfs` procedure, and the `chunk_size` can have a [significant effect on the run time](https:\/\/docs.featuretools.com\/guides\/performance.html). Here we will use a chunk size equal to the number of rows in the data so all the results will be calculated in one pass. We also want to avoid making any features with the testing data, so we pass in `ignore_entities = [app_test]`.","334fc2f9":"Let's visualize one of these new variables. We can look at the trend in credit size over time. A positive value indicates that the loan size for the client is increasing over time. ","eaa8fea0":"### Replace Outliers\n\nThere are a number of day offsets that are recorded as 365243. Reading through discussions, others replaced this number with `np.nan`. If we don't do this, Pandas will not be able to convert into a timedelta and throws an error that the number is too large. The following code has been adapted from a script on [GitHub](https:\/\/github.com\/JYLFamily\/Home_Credit_Default_Risk\/blob\/master\/20180603\/FeaturesV2\/ApplicationTestFeatures.py).","99f6b3e3":"# Conclusions \n\nIn this notebook we explored some of the advanced functionality in featuretools including:\n\n* Time Variables: allow us to track trends over time \n* Interesting Variables: condition new features on values of existing features\n* Seed Features: define new features manually that featuretools will then build on top of\n* Custom feature primitives: design any transformation or aggregation feature that can incorporate domain knowledge\n\nWe can use these methods to encode domain knowledge about a problem into our features or create features based on what others have found useful. The next step from here would be to run the script on the entire dataset, then use the features for modeling. We could use the feature importances from the model to determine the most relevant features, perform feature selection, and then go through another round of feature synthesis with a new set of of primitives, seed features, and interesting features. As with many aspects of machine learning, feature creation is largely an empirical and iterative procedure. ","072a13c2":"# Create Custom Feature Primitives\n\nIf we are not satisfied with the existing primitives in featuretools, we [can write our own](https:\/\/docs.featuretools.com\/automated_feature_engineering\/primitives.html#defining-custom-primitives). This is an extremely powerful method that lets us expand the capabilities of featuretools. \n\n### NormalizedModeCount and LongestSeq\n\nAs an example, we will make three features, building on code from the [featuretools GitHub](https:\/\/github.com\/Featuretools\/featuretools). These will be aggregation primitives, where the function takes in an array of values and returns a single value.  The first, `NormalizedModeCount`, builds upon the `Mode` function by returning the fraction of total observations in a categorical feature that the model makes up. In other words, for a client with 5 total `bureau_balance` observations where 4 of the `STATUS` were `X`, the value of the `NormalizedModeCount` would be 0.8. The idea is to record not only the most common value, but also the relative frequency of the most common value compared to all observations.  \n\nThe second custom feature will record the longest consecutive run of a discrete variable. `LongestSeq` takes in an array of discrete values and returns the element that appears the most consecutive times. Because entities in the entityset are sorted by the `time_index`, this will return the value that occurs the most number of times in a row with respect to time. \n\n\n\n","58afe3ca":"# Interesting Values\n\nAnother method we can use in featuretools is \"interesting values.\" Specifying interesting values will calculate new features conditioned on values of existing features. For example, we can create new features that are conditioned on the value of `NAME_CONTRACT_STATUS` in the `previous` dataframe. Each stat will be calculated for the specified interesting values which can be useful when we know that there are certain indicators that are of greater importance in the data.  ","671c1d83":"Based on the most important features returned by a model, we can create new interesting features. This is one area where we can apply domain knowledge to feature creation.","3b0fbe0c":"To use interesting values, we assign them to the variable and then specify the `where_primitives` in the `dfs` call. ","646fa723":"### Entities\n\nWhen creating the entities, we specify the `index`, the `time_index` (if present), and the `variable_types` (if they need to be specified). ","9ec9814f":"#### Previous Applications\n\nThe `previous` dataframe holds previous applications at Home Credit. There are a number of time offset columns in this dataset:\n\n* `DAYS_DECISION`: number of days before current application at Home Credit that decision was made about previous application. This will be the `time_index` of the data.\n* `DAYS_FIRST_DRAWING`: number of days before current application at Home Credit that first disbursement was made\n* `DAYS_FIRST_DUE`: number of days before current application at Home Credit that first due was suppoed to be\n* `DAYS_LAST_DUE_1ST_VERSION`: number of days before current application at Home Credit that first was??\n* `DAYS_LAST_DUE`: number of days before current application at Home Credit of last due date of previous application\n* `DAYS_TERMINATION`: number of days before current application at Home Credit of expected termination\n\nLet's convert all these into timedeltas in a loop and then make time columns.","d71f6636":"## Remove Features\n\n[Feature selection](https:\/\/en.wikipedia.org\/wiki\/Feature_selection) is an entire topic to itself. However, one thing we can do is use the built-in featuretools [selection function to remove](https:\/\/docs.featuretools.com\/generated\/featuretools.selection.remove_low_information_features.html) columns that only have one unique value or have all null values. ","7d1f157e":"# Time Variables\n\nTime can be a crucial factor in many datasets because behaviors change over time and therefore we want to make features to reflect this. For example, a client might be taking out larger and larger loans over time which could be an indicator that they are about to default or they could have a run of missed payments but then get back on track.\n\nThere are no explicit datetimes in the data, but there are relative time offsets. All the time offset are measured from the current application at Home Credit and are measured in months or days. For example, in `bureau`, the `DAYS_CREDIT` column represents \"How many days before current application did client apply for Credit Bureau credit\". (Credit Bureau refers to any other credit organization besides Home Credit). Although we do not know the actual application date, if we assume a starting application date that is the same for all clients, then we can convert the `MONTHS_BALANCE` into a datetime. This can then be treated as a relative time that we can use to find trends or identify the most recent value of a variable. ","b646d6b6":"There are also two ordinal variables in the `app` data: the rating of the region with and without the city. ","365f0c55":"To test whether this function works as intended, we can compare the most recent variable of `CREDIT_TYPE` ordered by two different dates. ","db539204":"### Read in Data and Create Smaller Datasets\n\nWe will limit the data to 1000 rows because automated feature engineering is computationally intensive work. Later we can refactor this code into functions and put it in a script to run on a more powerful machine. "}}