{"cell_type":{"3d0b842e":"code","253fd3a9":"code","357739c6":"code","3ee1e0b5":"code","46d8681e":"code","a0179538":"code","28ea4a6b":"code","dae5e4fa":"code","36947d31":"code","40d325d4":"code","587bce5c":"code","d4522de7":"code","60ac3009":"code","5c8833ec":"code","e643771f":"code","9268a53d":"code","a161767c":"code","4268de3c":"code","62922a25":"code","591550f9":"code","2e765b54":"code","12d0afec":"code","b255e716":"code","5e616952":"code","f294966a":"code","6b51c2a9":"code","15ed0129":"code","a5cead8f":"code","b84d3015":"code","3c3d8f82":"code","dd81192e":"code","a2821c7b":"code","a0a24927":"code","4b4a7fa2":"code","439ea634":"code","07588839":"code","a046ce1c":"code","28a7fdab":"code","7dad9aff":"code","36ffa9c2":"code","d69d2b0c":"code","27518431":"code","24ff5dd4":"code","ac7f3f64":"code","4ca96686":"code","d5031ce3":"code","46e37c4f":"code","e7dfe26c":"code","c3d74cfe":"code","fb478846":"code","3f097a30":"code","29a9260c":"code","d3780d7f":"code","29c7b896":"code","e8445b2b":"code","9fb3f349":"code","0ec5d96c":"code","4bfde0cb":"code","b66fd7c4":"code","fd484ce1":"code","79f833f5":"code","a93767ad":"code","c2daa461":"code","3a553a15":"code","5fdea7c4":"code","b47207f5":"code","4a17623d":"code","86e62248":"code","ebd69abf":"code","a22ccc1a":"code","d30c16f0":"code","914a2a17":"code","27fac81f":"code","2cd25c93":"code","195390f5":"code","140e0877":"code","31484fc4":"code","704e9b3b":"code","26958ccf":"code","b49d2257":"code","2c1f1519":"code","5d7019ff":"code","f6f02f2a":"code","8f11d9f1":"code","1b48d661":"code","20dbf43a":"code","2046951d":"code","4dfcc4b2":"code","455ba88a":"code","65f14b2e":"code","7f85e3fb":"code","ea1a98d6":"markdown","d8b4eb17":"markdown","caa57325":"markdown","ef9fd664":"markdown","9bc1a43c":"markdown","76dfe81d":"markdown","5a55938b":"markdown","d6b4971b":"markdown","136333dc":"markdown","d8f417c1":"markdown","eb875214":"markdown","045664b0":"markdown","b31fe24f":"markdown","94d4ca61":"markdown","bfc5f4c7":"markdown","297841b5":"markdown","daaf2f54":"markdown","bcb2d9da":"markdown","e1a7cc78":"markdown","a69d6373":"markdown","71a7d1bd":"markdown","d97d8690":"markdown","86147619":"markdown","59651eb2":"markdown","2b0b553e":"markdown","a14e315f":"markdown","4116d92d":"markdown","20900595":"markdown","1f27ad8b":"markdown","142da6ec":"markdown","af06c46e":"markdown","ddac6cca":"markdown","b6f19a75":"markdown","1ac849d0":"markdown","e6bec957":"markdown","af052b65":"markdown","7859a3d0":"markdown","f62223c5":"markdown","6e633f49":"markdown","5c56d0c5":"markdown","cfa03c88":"markdown","260b340d":"markdown","c9f2aae8":"markdown","7620aee5":"markdown","b737a879":"markdown","c322a6f2":"markdown","4626d923":"markdown","86316d38":"markdown","c2e6da7f":"markdown","e356cef9":"markdown","346fea39":"markdown","91717bdd":"markdown","a9399cce":"markdown","7a135445":"markdown","6e4e7cd4":"markdown","a4a337f4":"markdown","85da2679":"markdown","90e6ead1":"markdown","9726ec54":"markdown","04f74b73":"markdown","77df6f06":"markdown","5c5e3980":"markdown","31e6ed29":"markdown","e806dd3e":"markdown","887acf8d":"markdown","1e0111f8":"markdown","f1367a9e":"markdown","ce03e4b3":"markdown","63c2a671":"markdown","46e79db8":"markdown","34cbc182":"markdown","52c0e818":"markdown","bd980ffe":"markdown","5ebacd48":"markdown","219de605":"markdown","13fd60b9":"markdown","093bf0d0":"markdown","ff1002ab":"markdown","687efe92":"markdown","0bfc9ce2":"markdown","45b3c05c":"markdown","d092a26f":"markdown","75d07f5e":"markdown","4e96a233":"markdown","b6cfc34c":"markdown"},"source":{"3d0b842e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\nimport os\nprint(os.listdir(\"..\/input\"))","253fd3a9":"train = pd.read_csv(\"..\/input\/train.csv\")\n#drop cabin, Name and Ticket data that are not neccesary to train the model\ntrain.head()","357739c6":"#Check for the missing values in the columns \nfig, ax = plt.subplots(figsize=(9,5))\nsns.heatmap(train.isnull(), cbar=False, cmap=\"YlGnBu_r\")\nplt.show()","3ee1e0b5":"#I drop those columns\ntrain = train.drop(columns = ['Cabin','Name','Ticket','PassengerId'])","46d8681e":"#filling Non valid values with mean for age, \ntrain['Age'].fillna((train['Age'].mean()), inplace=True)","a0179538":"sns.barplot(x='Sex', y='Survived', data=train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Sex\", fontsize=16)\n\nplt.show()\ntrain[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","28ea4a6b":"#This information can be displayed in the next plot too:\n#sns.catplot(x='Sex', col='Survived', kind='count', data=train);","dae5e4fa":"sns.barplot(x='Pclass', y='Survived', data=train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Pclass\", fontsize=16)\n\nplt.show()\ntrain[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","36947d31":"sns.barplot(x='Sex', y='Survived', hue='Pclass', data=train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Pclass and Sex\")\nplt.show()","40d325d4":"train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","587bce5c":"train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","d4522de7":"sns.pairplot(data=train, hue=\"Survived\")","60ac3009":"# I Create a swarmplot to detect patterns, where is the highest survival rate? \nsns.swarmplot(x = 'SibSp', y = 'Parch', hue = 'Survived', data = train, split = True, alpha=0.8)\nplt.show()","5c8833ec":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nyf = train.Survived\nbase_features = ['Parch',\n                 'SibSp','Age', 'Fare','Pclass']\n\nXf = train[base_features]\n\ntrain_X, val_X, train_y, val_y = train_test_split(Xf, yf, random_state=1)\nfirst_model = RandomForestRegressor(n_estimators=21, random_state=1).fit(train_X, train_y)","e643771f":"#Explore the relationship between SipSp and Parch in the predictions for a RF Model\ninter  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=['SibSp', 'Parch'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter, feature_names=['SibSp', 'Parch'], plot_type='contour')\nplt.show()","9268a53d":"train['FamilySize'] = train['SibSp'] + train['Parch'] \ntrain[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).agg('mean')","a161767c":"train['IsAlone'] = 0\ntrain.loc[train['FamilySize'] == 0, 'IsAlone'] = 1\n\ntrain[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","4268de3c":"cols = ['Survived', 'Parch', 'SibSp', 'Embarked','IsAlone', 'FamilySize']\n\nnr_rows = 2\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        \n        i = r*nr_cols+c       \n        ax = axs[r][c]\n        sns.countplot(train[cols[i]], hue=train[\"Survived\"], ax=ax)\n        ax.set_title(cols[i], fontsize=14, fontweight='bold')\n        ax.legend(title=\"survived\", loc='upper center') \n        \nplt.tight_layout()","62922a25":"feat_name = 'Fare'\npdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","591550f9":"train[[\"Fare\", \"Survived\"]].groupby(['Survived'], as_index=False).mean().sort_values(by='Survived', ascending=False)","2e765b54":"train.groupby(['Sex','Survived'])[['Fare']].agg(['min','mean','max'])","12d0afec":"train.loc[ train['Fare'] <= 7.22, 'Fare'] = 0\ntrain.loc[(train['Fare'] > 7.22) & (train['Fare'] <= 21.96), 'Fare'] = 1\ntrain.loc[(train['Fare'] > 21.96) & (train['Fare'] <= 40.82), 'Fare'] = 2\ntrain.loc[ train['Fare'] > 40.82, 'Fare'] = 3\ntrain['Fare'] = train['Fare'].astype(int)","b255e716":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Fare', bins=20)\nplt.show()","5e616952":"sns.barplot(x='Sex', y='Survived', hue='Fare', data=train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Fare and Sex\")\nplt.show()","f294966a":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)\nplt.show()","6b51c2a9":"feat_name = 'Age'\npdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()\n#Exploring the relationship between Age and Pclass for a given model preductions\ninter  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=['Age', 'Pclass'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter, feature_names=['Age', 'Pclass'], plot_type='contour')\nplt.show()","15ed0129":"#bins=np.arange(0, 80, 10)\ng = sns.FacetGrid(train, row='Sex', col='Pclass', hue='Survived', margin_titles=True, size=3, aspect=1.1)\ng.map(sns.distplot, 'Age', kde=False, bins=4, hist_kws=dict(alpha=0.6))\ng.add_legend()  \nplt.show()","a5cead8f":"train.loc[ train['Age'] <= 16, 'Age'] = 1\ntrain.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = 2\ntrain.loc[(train['Age'] > 32) & (train['Age'] <= 64), 'Age'] = 3\ntrain.loc[ train['Age'] > 64, 'Age'] = 4\ntrain['Age'] = train['Age'].astype(int)","b84d3015":"sns.barplot(x='Pclass', y='Survived', hue='Age', data=train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Age and Sex\")\nplt.show()","3c3d8f82":"train['Age*Class'] = train.Age * train.Pclass","dd81192e":"train[[\"Age*Class\", \"Survived\"]].groupby(['Age*Class'], as_index=False).mean().sort_values(by='Survived', ascending=False)","a2821c7b":"pd.crosstab([train.Survived], [train.Sex,train['Age*Class']], margins=True).style.background_gradient(cmap='autumn_r')","a0a24927":"pd.crosstab([train.Survived], [train.Sex,train['IsAlone']], margins=True).style.background_gradient(cmap='autumn_r')","4b4a7fa2":"pd.crosstab([train.Survived], [train.Fare], margins=True).style.background_gradient(cmap='autumn_r')","439ea634":"train.head()","07588839":"y2 = train.Survived\n\nbase_features2 = ['Parch','SibSp','Age', 'Fare','Pclass','Age*Class','FamilySize','IsAlone']\n\nX2 = train[base_features2]\ntrain_X2, val_X2, train_y2, val_y2 = train_test_split(X2, y2, random_state=1)\nsecond_model = RandomForestRegressor(n_estimators=21, random_state=1).fit(train_X2, train_y2)\n\ninter2  =  pdp.pdp_interact(model=second_model, dataset=val_X2, model_features=base_features2, features=['Age', 'Pclass'])\npdp.pdp_interact_plot(pdp_interact_out=inter2, feature_names=['Age', 'Pclass'], plot_type='contour')\nplt.show()","a046ce1c":"feat_name = 'FamilySize'\npdp_dist = pdp.pdp_isolate(model=second_model, dataset=val_X2, model_features=base_features2, feature=feat_name)\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","28a7fdab":"inter2  =  pdp.pdp_interact(model=second_model, dataset=val_X2, model_features=base_features2, features=['FamilySize', 'Pclass'])\npdp.pdp_interact_plot(pdp_interact_out=inter2, feature_names=['FamilySize', 'Pclass'], plot_type='contour')\nplt.show()","7dad9aff":"feat_name = 'IsAlone'\npdp_dist = pdp.pdp_isolate(model=second_model, dataset=val_X2, model_features=base_features2, feature=feat_name)\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","36ffa9c2":"feat_name = 'Age*Class'\npdp_dist = pdp.pdp_isolate(model=second_model, dataset=val_X2, model_features=base_features2, feature=feat_name)\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","d69d2b0c":"inter2  =  pdp.pdp_interact(model=second_model, dataset=val_X2, model_features=base_features2, features=['Age*Class', 'IsAlone'])\npdp.pdp_interact_plot(pdp_interact_out=inter2, feature_names=['Age*Class', 'IsAlone'], plot_type='contour')\nplt.show()","27518431":"# convert Sex values and Embearked values into dummis to use a numerical classifier \ndummies_Sex = pd.get_dummies(train.Sex)\ndummies_Embarked = pd.get_dummies(train.Embarked)\n#join the dummies to the final dataframe\ntrain_ready = pd.concat([train, dummies_Sex,dummies_Embarked], axis=1)\ntrain_ready.head()","24ff5dd4":"#Drop the columns that are not usefull now\n#train_ready = train_ready.drop(columns = ['Sex','Embarked','male','SibSp','Parch','Q'])\n\ntrain_ready = train_ready.drop(columns = ['Sex','Embarked'])","ac7f3f64":"#train_ready = train_ready.drop(columns = ['Age*Class'])","4ca96686":"#train_ready = train_ready.drop(columns = ['FamilySize'])","d5031ce3":"#alst check before trainning\ntrain_ready.info()","46e37c4f":"train_ready.head(10)","e7dfe26c":"from scipy import stats\nfor name in train_ready:\n    print(name, \"column entropy :\", round(stats.entropy(train_ready[name].value_counts(normalize=True), base=2),2))","c3d74cfe":"#Upload the test file \ntest = pd.read_csv(\"..\/input\/test.csv\")\n\n#Drop unecessary columns\ntest = test.drop(columns = ['Cabin','Name','Ticket','PassengerId'])\n#check the test dataframe\ntest.head()","fb478846":"#Check for the missing values in the columns \nfig, ax = plt.subplots(figsize=(9,5))\nsns.heatmap(test.isnull(), cbar=False, cmap=\"YlGnBu_r\")\nplt.show()","3f097a30":"#filling Non valid values with mean for age, \ntest['Age'].fillna((test['Age'].mean()), inplace=True)\ntest['Fare'].fillna((test['Fare'].mean()), inplace=True)","29a9260c":"test.loc[ test['Fare'] <= 7.22, 'Fare'] = 0\ntest.loc[(test['Fare'] > 7.22) & (test['Fare'] <= 21.96), 'Fare'] = 1\ntest.loc[(test['Fare'] > 21.96) & (test['Fare'] <= 40.82), 'Fare'] = 2\ntest.loc[ test['Fare'] > 40.82, 'Fare'] = 3","d3780d7f":"test['FamilySize'] = test['SibSp'] + test['Parch'] + 1\ntest['IsAlone'] = 0\ntest.loc[test['FamilySize'] == 1, 'IsAlone'] = 1","29c7b896":"test.loc[ test['Age'] <= 16, 'Age'] = 1\ntest.loc[(test['Age'] > 16) & (test['Age'] <= 32), 'Age'] = 2\ntest.loc[(test['Age'] > 32) & (test['Age'] <= 64), 'Age'] = 3\ntest.loc[ test['Age'] > 64, 'Age'] = 4","e8445b2b":"test['Age*Class'] = test.Age * test.Pclass","9fb3f349":"test.info()","0ec5d96c":"#as in the train dataset, build dummis in the sex and embarked columns\ntest_dummies_Sex = pd.get_dummies(test.Sex)\ntest_dummies_Embarked = pd.get_dummies(test.Embarked)\ntest_ready = pd.concat([test, test_dummies_Sex,test_dummies_Embarked], axis=1)\ntest_ready.head()","4bfde0cb":"#drop these columns, we keep only numerical values\n#train_ready = train_ready.drop(columns = ['Sex','Embarked','Survived','SibSp','Parch'])\ntest_ready = test_ready.drop(columns = ['Sex','Embarked'])","b66fd7c4":"#test_ready = test_ready.drop(columns = ['Age*Class'])","fd484ce1":"#test_ready = test_ready.drop(columns = ['FamilySize'])","79f833f5":"#check all is ok \ntest_ready.info()","a93767ad":"test_ready.head()","c2daa461":"from scipy import stats\nfor name in test_ready:\n    print(name, \"column entropy :\",round(stats.entropy(test_ready[name].value_counts(normalize=True), base=2),2))","3a553a15":"## import ML\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline","5fdea7c4":"# Create arrays for the features and the response variable\ny = train_ready['Survived'].values\nX = train_ready.drop('Survived',axis=1).values","b47207f5":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=21, stratify=y)","4a17623d":"#Importing the auxiliar and preprocessing librarys \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.metrics import accuracy_score\n\n#Models\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding","86e62248":"clfs = []\nseed = 3\n\nclfs.append((\"LogReg\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBClassifier())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsClassifier(n_neighbors=8))]))) \n\nclfs.append((\"DecisionTreeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeClassifier())]))) \n\nclfs.append((\"RandomForestClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestClassifier())]))) \n\nclfs.append((\"GradientBoostingClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingClassifier(n_estimators=100))]))) \n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\nclfs.append((\"BaggingRidgeClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"BaggingClassifier\", BaggingClassifier())])))\n\nclfs.append((\"ExtraTreesClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreesClassifier())])))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'accuracy'\nn_folds = 7\n\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, \n                                 cv= 5, scoring=scoring,\n                                 n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20)\nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","ebd69abf":"perm_xgb = PermutationImportance(XGBClassifier().fit(X_train, y_train), random_state=1).fit(X_test,y_test)\neli5.show_weights(perm_xgb, feature_names = train_ready.drop('Survived',axis=1).columns.tolist())","a22ccc1a":"perm_knn = PermutationImportance(KNeighborsClassifier(n_neighbors=8).fit(X_train, y_train), random_state=1).fit(X_test,y_test)\neli5.show_weights(perm_knn, feature_names = train_ready.drop('Survived',axis=1).columns.tolist())","d30c16f0":"perm_gbc = PermutationImportance(GradientBoostingClassifier(n_estimators=100).fit(X_train, y_train), random_state=1).fit(X_test,y_test)\neli5.show_weights(perm_gbc, feature_names = train_ready.drop('Survived',axis=1).columns.tolist())","914a2a17":"perm_gbc = PermutationImportance(RidgeClassifier().fit(X_train, y_train), random_state=1).fit(X_test,y_test)\neli5.show_weights(perm_gbc, feature_names = train_ready.drop('Survived',axis=1).columns.tolist())","27fac81f":"#train_ready.drop('Survived',axis=1).columns","2cd25c93":"train_ready.drop('Survived',axis=1).info()","195390f5":"train_ready.shape","140e0877":"#apply Scla to train in order to standardize data \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nscaler.fit(X)\nscaled_features = scaler.transform(X)\ntrain_sc = pd.DataFrame(scaled_features) # columns=df_train_ml.columns[1::])\n\n#apply Scla to test csv (new file)  in order to standardize data \n\nX_csv_test = test_ready.values  #X_csv_test the new data that is going to be test \nscaler.fit(X_csv_test)\nscaled_features_test = scaler.transform(X_csv_test)\ntest_sc = pd.DataFrame(scaled_features_test) # , columns=df_test_ml.columns)","31484fc4":"scaled_features_test.shape","704e9b3b":"scaled_features.shape","26958ccf":"# Import KNeighborsClassifier from sklearn.neighbors\nfrom sklearn.neighbors import KNeighborsClassifier \n\n# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 19)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')","b49d2257":"# Import KNeighborsClassifier from sklearn.neighbors\nfrom sklearn.neighbors import KNeighborsClassifier \n\n# Create a k-NN classifier with 6 neighbors: knn\nknn_6 = KNeighborsClassifier(n_neighbors = 6)\n\n# Fit the classifier to the data\nknn_6.fit(scaled_features,y)\n\n# Predict the labels for the training data X\ny_pred_knn_6 = knn_6.predict(scaled_features_test)","2c1f1519":"# Import KNeighborsClassifier from sklearn.neighbors\nfrom sklearn.neighbors import KNeighborsClassifier \n\n# Create a k-NN classifier with 6 neighbors: knn\nknn_10 = KNeighborsClassifier(n_neighbors = 10)\n\n# Fit the classifier to the data\nknn_10.fit(scaled_features,y)\n\n# Predict the labels for the training data X\ny_pred_knn_10 = knn_10.predict(scaled_features_test)","5d7019ff":"#Upload the test file for KNN (scaled)\nresult_knn_6 = pd.read_csv(\"..\/input\/gender_submission.csv\")\nresult_knn_6['Survived'] = y_pred_knn_6\nresult_knn_6.to_csv('Titanic_knn_5.csv', index=False)","f6f02f2a":"#Upload the test file for KNN (scaled)\nresult_knn_10 = pd.read_csv(\"..\/input\/gender_submission.csv\")\nresult_knn_10['Survived'] = y_pred_knn_10\nresult_knn_10.to_csv('Titanic_knn_7.csv', index=False)","8f11d9f1":"logreg = LogisticRegression()\nlogreg.fit(scaled_features,y)\ny_pred_logreg = logreg.predict(scaled_features_test)\ny_pred_logreg.shape","1b48d661":"#Upload the test file for Random Forest \nresult_logreg = pd.read_csv(\"..\/input\/gender_submission.csv\")\nresult_logreg['Survived'] = y_pred_logreg\nresult_logreg.to_csv('Titanic_logreg.csv', index=False)","20dbf43a":"import xgboost as xgb\nfrom xgboost import XGBClassifier\n\nclf = xgb.XGBClassifier(n_estimators=250, random_state=4,bagging_fraction= 0.791787170136272, colsample_bytree= 0.7150126733821065,feature_fraction= 0.6929758008695552,gamma= 0.6716290491053838,learning_rate= 0.030240003246947006,max_depth= 2,min_child_samples= 5,num_leaves= 15,reg_alpha= 0.05822089056228967,reg_lambda= 0.14016232510869098,subsample= 0.9)\n\nclf.fit(scaled_features, y)\n\ny_pred_xgb= clf.predict(scaled_features_test)","2046951d":"#Upload the test file for Random Forest \nresult_xgb = pd.read_csv(\"..\/input\/gender_submission.csv\")\nresult_xgb['Survived'] = y_pred_xgb\nresult_xgb.to_csv('Titanic_xgb.csv', index=False)","4dfcc4b2":"rcf= RidgeClassifier()\nrcf.fit(scaled_features, y)\n\ny_pred_rcf= rcf.predict(scaled_features_test)","455ba88a":"#Upload the test file for  Ridge Classifier\nresult_rcf = pd.read_csv(\"..\/input\/gender_submission.csv\")\nresult_rcf['Survived'] = y_pred_rcf\nresult_rcf.to_csv('Titanic_rcf.csv', index=False)","65f14b2e":"gbc= GradientBoostingClassifier(n_estimators=100)\ngbc.fit(scaled_features, y)\ny_pred_gbc= gbc.predict(scaled_features_test)","7f85e3fb":"#Upload the test file for Bagging Ridge Classifie\nresult_gbc = pd.read_csv(\"..\/input\/gender_submission.csv\")\nresult_gbc['Survived'] = y_pred_gbc\nresult_gbc.to_csv('Titanic_gbc.csv', index=False)","ea1a98d6":"Less values give a clue of more survival mean, however a crosstab maybe would give more clear information:","d8b4eb17":"I check in a first model how can age correlate with the chance of survive, also related to the passanger Class:","caa57325":"The first features to work on are SibSp and Parch","ef9fd664":"I explore the entropy to check wheter the values can give a good learning to the algoritmh","9bc1a43c":"To explore better the relationship between these variables before featuring, I create a first model:","76dfe81d":"Supposing that the categorical values such as the Name, the Cabin, the Ticket code and the ID doesnt have any relationship to the fact that the passanger died or survived:","5a55938b":"this is how the new train dataframe looks like:","d6b4971b":"We tested 4 options, first one training without any change in the features","136333dc":"Next, we include only with those features with entropy < 2, that is droping those columns that maybe add more noise than value","d8f417c1":"Also for the Passanger Class and its family size. After defining the new groups, it's more clear for the algorithm that lower class and lower family size increase the chance of survive ","eb875214":"**5th model: Gradient Boosting Classifier**","045664b0":" I explore both PClass and Sex in the same plot:","b31fe24f":"To sum up the work, the next set of graphics shows the relationships with and without the new features","94d4ca61":"![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*kZ9X3rMW9-Ohxd2UKx-y1w.png)","bfc5f4c7":"This last crosstab show how the people having Fare group 1 (Fare > 7.22 & Fare <= 21.96) have the lower chance of survive, \n\nThe presented groupes show the tendency of the data. However is hard to know wheter these groups really optimize the larning. This work only can be done by trial and error","297841b5":"The same analysis for the IsAlone feature:","daaf2f54":"I will keep two of these models, with 6 neighbors and with 10 neighbors","bcb2d9da":"Age has continue values too:","e1a7cc78":"**Third model : XGB Classifier**","a69d6373":"I explore the entropy to check wheter the values can give a good learning to the algoritmh","71a7d1bd":"**Import Data & Exploratory Data Analysis**","d97d8690":"<h1> Welcome to my Titanic Kernel! <\/h1>","86147619":"The next option is to cerate IsAlone feature to check wheter a person traveling alolne is more likely to survived or died","59651eb2":"Then, introducing new features as Family size (to join these Parch and SibSp)","2b0b553e":"men who were alone have lower chance of survive","a14e315f":"<a id=\"test\"><\/a> <br> \n**4. Preparing the Test dataframe**","4116d92d":"**Estimation of the Survival rate using the new features defined **","20900595":"the new featre show how female in AgexClass between 2 to 6 have better chance of survive, and male from 4 to 6 AgexClass have lower chance of survive ","1f27ad8b":"<a id=\"Feature\"><\/a> <br> \n **3. Feature Engineering**","142da6ec":"This continus feature could be converted in a continues feature in order to increase prediction of the model","af06c46e":"<a id=\"results\"><\/a> <br> \n**7. Results** \n","ddac6cca":"the next try included all the features created in the first section:","b6f19a75":" <font color=\"red\">If this kernel were useful for you, please <b>UPVOTE<\/b> the kernel ;)<\/font>","1ac849d0":"<a id=\"ML\"><\/a> <br> \n**5. Testing several Supervise learning models** \n","e6bec957":"First we run this loop to detect the correct number of Nieghbors in KNN","af052b65":"The fare is distributed in several continues values, and it is not clear how can we discretize these values to improve model's performance.","7859a3d0":"This plot show us how higher class and lower age have better chance of survive, while lower class (3) and older (age >2) have lower chance of survive.\nThis seems logic, the reduction of classes can improve the learning of the model based on the (relative small) data we have  ","f62223c5":"**Now the relationship between age and survived**","6e633f49":"**Survival as function of Pclass and Sex**\n","5c56d0c5":"It seems this feature combination give the better accuracy for all the algorithms","cfa03c88":" Table of Contents:\n \n **1. [Introduction & Imports](#Introduction)** <br>\n **2. [Exploratory Data Analysis](#EDA)** <br>\n **3. [Feature Engineering](#Feature)** <br>\n **4. [Preparing the Test dataframe](#test)** <br>\n **5. [Testing several Supervise learning models](#ML)** <br>\n **6. [Trainning all data on some Classifiers](#train)** <br>\n **7. [Results](#results)** <br>\n","260b340d":"Here you find basic Data Exploration and Visualization, data handling with some features, and modelling.\n\n**This Kernel Focus on the effects of some features in the performance of the learning algorithms** \n\n\nI used most the common supervise learning classification algorithms. \nI compared them in a train\/test set and I chose some for submiting the answers","c9f2aae8":"<a id=\"Introduction\"><\/a> <br> \n **1. Introduction & Imports** \n","7620aee5":"Now, I would simulate the training using the new features ","b737a879":"**4th Model RidgeClassifier**","c322a6f2":"![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*ofXtRDzL5PtXvhIT0Qdk1A.png)","4626d923":"When dropping the colmuns in the train dataset it would be neccesary to do the same in the test dataset:","86316d38":"**Bulding the model for the test set **","c2e6da7f":"![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*xZwK315Z-wxV5EmqGI9y9Q.png)\n","e356cef9":"**The Fare Column**","346fea39":"It is clear that women have better chance than men. \nIf you create a model saying that only woman survive it would have a score of **0.76555**, so the mission is to create a model at least better\n\nNext, I explore the change of survive regardign the passanger Class:","91717bdd":"***Here I can drop the columns as AgexClass and IsAlone to check wheter the algorithms produce better performance ***","a9399cce":"same for the FamiliSize columns","7a135445":"Another point, for example, when trainning a KNN with several neighbords, the result depend on the features defined.\nWe plot the accuracy of KNN for several neighbors:","6e4e7cd4":"The first step is the detect in which columns there are non valid values","a4a337f4":"Now I explore the effects of the other featuers independently","85da2679":"Based on the exploration of the data, I propose to discretize the Fare in four states:","90e6ead1":"After submitting the diferent CSV I have obatined this results:\n- Using Random Forest: 0.73684\n- Using KNN with 6 neighboors:  **0.77990**\n- Using KNN with 10 neighboors: 0.77033\n- Using KGBClassifier:** 0.77990**\n- Using Ridge Classifier: 0.77511","9726ec54":"**Second model: Logistic Regression**","04f74b73":"This plot show us how the new fare states are relatid to Sex and rate of survival. Higher fare have better chance of survive than lower fare, and female more than male in general.","77df6f06":"the AgexClass can be dropped or not as I experiment to increase the general performance of the model in the next steps:","5c5e3980":"I train the model, then I came back and drop AgexClass Siwe column (entropy 2,14) then I train again the model droping the FamilySize column (entropy 1,82)","31e6ed29":"Following the graphics below, The age can be groupped into less classes:","e806dd3e":"Here I complete the same steps for the test set","887acf8d":"<a id=\"train\"><\/a> <br> \n**6. Trainning all data on several Classifier**","1e0111f8":"finally I explore new features, for example, a measure of 'Age x Class' would give better insight of the survival rate? ","f1367a9e":"**This plot vary depending the features in the train dataframe **","ce03e4b3":"**First Model: KNN**","63c2a671":"To start the exploration, it is possible to group passanger by Sex and Class, these groups could give insights if higher class have better chance of survive, or woman have better chance than men for example.","46e79db8":"First, I would use a train\/test division on the test csv, and I would check the performance of several algorithms:","34cbc182":"<a id=\"EDA\"><\/a> <br> \n **2. Exploratory Data Analysis**","52c0e818":"As Sex and Embarked are not numerical I do the pandas OneHotEncoder:","bd980ffe":"It seems like less age and higher class is a better combination to survive. ","5ebacd48":"![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*Rn1oQJHrDRcDs3_oM6ZxbA.png)","219de605":"it seems that the global accuracy of all the models is increasing\nNext, we select only those features with entropy < 1,5\u00a0:","13fd60b9":"These new features provide a more clear distribution that the dataframe without features:","093bf0d0":"and drop the respective columns:","ff1002ab":"Next, Explore the Parch and SibSp column:","687efe92":"To solve this problem, first, it would be likely to think that the chance of survival could depend on the Fare","0bfc9ce2":"Import some libraries for data exploration","45b3c05c":"Highest fare mean improve the chance of survival\nWe know that female have better chance than male, so we group the data in these values:","d092a26f":"![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*XbtLALV28nrdzBSG5IHm4g.png)","75d07f5e":"The features can be built to:\n- reduce the number of states of the SibSp and Parch column\n- Create smaller classes for continues columns, such as Age and Fare\n- Create new columns that could improve prediction: such as if the passanger is alone or not\n- Drop columns that doesn't improve predictions","4e96a233":"To get a better insight of the relationship of these features and the survival rate, a general pairplot will give some clues:","b6cfc34c":"For Is alone and AgexClass, the effect in the survival rate is not that clear"}}