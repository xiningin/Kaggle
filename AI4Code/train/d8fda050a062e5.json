{"cell_type":{"3cc326c8":"code","6affc48b":"code","f3574788":"code","0c224f8c":"code","68d897a8":"code","3b93ed84":"code","3a04c629":"code","535bd05f":"code","0109e812":"code","d0a3874a":"code","2517703a":"code","02683e3b":"code","2ec8eeb8":"code","0b0bfc14":"code","16f2dd61":"code","767b40e3":"code","8102ebd4":"code","6b2bf3f9":"code","99909125":"code","7ed0cab2":"code","292c3f6c":"code","88deac7d":"code","b9a660a4":"code","fdcd657c":"code","fd2e7ec0":"code","3808220d":"code","71343c33":"code","be704013":"code","d4e71c2e":"code","39fe9f5a":"code","e3833e49":"code","a134c375":"markdown","033efc79":"markdown","18023a80":"markdown","adeda5d5":"markdown","04754f63":"markdown","94fea5b7":"markdown","b9a20150":"markdown","0b7440fe":"markdown","223c124a":"markdown","accb2ebb":"markdown","e07fbeea":"markdown","27d7c40a":"markdown","830e80f4":"markdown","640bd688":"markdown","0e85bac9":"markdown","1269f859":"markdown","8c730dc7":"markdown","9b500b4c":"markdown","16680aca":"markdown"},"source":{"3cc326c8":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom joblib import Parallel, delayed\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgbm","6affc48b":"train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\nbook_train = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet')\nbook_test = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_test.parquet')\ntrade_train = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet')\ntrade_test = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/trade_test.parquet')\nsubmission = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/sample_submission.csv')","f3574788":"book_train","0c224f8c":"train","68d897a8":"book_train_df = book_train[book_train[\"stock_id\"]==0]\ntrain_df = train[train[\"stock_id\"]==0]\ntrain_df['row_id'] = train_df['stock_id'].astype(str) + '-' + train_df['time_id'].astype(str)","3b93ed84":"# ------------These codes are just remarks since eventually not used \/ \u7d50\u5c40\u4f7f\u308f\u306a\u3044\u306e\u3067\u30ea\u30de\u30fc\u30af\u6271\u3044\n\n# book_train_df['bid_impact1'] = book_train_df.bid_price1 * book_train_df.bid_size1\n# book_train_df['ask_impact1'] = book_train_df.ask_price1 * book_train_df.ask_size1\n# book_train_df['impact_bid'] = (book_train_df.bid_impact1 - book_train_df.ask_impact1) \/ book_train_df.bid_impact1\n# book_train_df['impact_ask'] = (book_train_df.ask_impact1 - book_train_df.bid_impact1) \/ book_train_df.ask_impact1\n# book_train_df.loc[book_train_df['impact_bid']<0, \"impact_bid\"] = 0\n# book_train_df.loc[book_train_df['impact_ask']<0, \"impact_ask\"] = 0","3a04c629":"# ------------These codes are just remarks since eventually not used \/ \u7d50\u5c40\u4f7f\u308f\u306a\u3044\u306e\u3067\u30ea\u30de\u30fc\u30af\u6271\u3044\n\n# time_bid_max = book_train_df[['time_id', 'impact_bid']].groupby('time_id').max()\n# time_bid_max.columns = ['max_bid_impact1']\n# time_ask_max = book_train_df[['time_id', 'impact_ask']].groupby('time_id').max()\n# time_ask_max.columns = ['max_ask_impact1']\n# time_bid_std = book_train_df[['time_id', 'impact_bid']].groupby('time_id').std()\n# time_bid_std.columns = ['std_bid_impact1']\n# time_ask_std = book_train_df[['time_id', 'impact_ask']].groupby('time_id').std()\n# time_ask_std.columns = ['std_ask_impact1']\n\n# train_df = pd.merge(train_df, time_bid_max, on = 'time_id', how ='left')\n# train_df = pd.merge(train_df, time_ask_max, on = 'time_id', how ='left')\n# train_df = pd.merge(train_df, time_bid_std, on = 'time_id', how ='left')\n# train_df = pd.merge(train_df, time_ask_std, on = 'time_id', how ='left')\n\n# train_df","535bd05f":"# This function not used after all, but just as reference... \/ \u3053\u306e\u95a2\u6570\u7d50\u5c40\u4f7f\u3044\u307e\u305b\u3093\u304c\u3054\u53c2\u8003\u307e\u3067\u3002 \n\ndef pre_process1(df):\n    # \u7279\u5fb4\u91cfimpact1\u306e\u8ffd\u52a0\n    df.bid_impact1 = df.bid_price1 * df.bid_size1\n    df.ask_impact1 = df.ask_price1 * df.ask_size1\n    df[\"impact_bid\"] = (df.bid_impact1 - df.ask_impact1) \/ (df.bid_size1 + df.ask_size1)\n    df[\"impact_ask\"] = (df.ask_impact1 - df.bid_impact1) \/ (df.bid_size1 + df.ask_size1)\n    df.loc[df['impact_bid']<0, \"impact_bid\"] = 0\n    df.loc[df['impact_ask']<0, \"impact_ask\"] = 0\n    df[\"impact1\"] = df.impact_bid - df.impact_ask\n    df.drop(['impact_bid', 'impact_ask'], axis =1, inplace =True)\n    return df\n\npre_process1(book_train_df)\nbook_train_df\n\n# impact1_mean was finally merged to train_df as follows, though eventually not used...\n#\u3000\u7d50\u5c40\u4f7f\u3044\u307e\u305b\u3093\u3067\u3057\u305f\u304c\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u96c6\u8a08\u3057\u3066\u307e\u3057\u305f\u3002\n\n# def pre_process2(df) :\n#    time_impact1_mean = df[['time_id', 'impact1']].groupby('time_id').mean()\n#    time_impact1_mean.columns = ['mean_impact1']\n#    return time_impact1_mean\n\n# time_impact1_mean = pre_process2(book_train_df)\n# train_df = pd.merge(train_df, time_impact1_mean, on = 'time_id', how ='left')\n# train_df","0109e812":"def pre_process1_stability(df):\n    df['bid1_diff'] = df.bid_price1.diff()\n    df['ask1_diff'] = df.ask_price1.diff()\n    df.loc[df['bid1_diff'].isnull(), 'bid1_diff'] = 0\n    df.loc[df['ask1_diff'].isnull(), 'ask1_diff'] = 0\n    return df\n\npre_process1_stability(book_train_df)\nbook_train_df.head()","d0a3874a":"# def get_group_features(stock_ids : list, dataType = 'train'):\n\n#    preprocess_df = Parallel(n_jobs=-1)(\n#        delayed(preprocess)(stock_id, dataType) \n#        for stock_id in stock_ids\n#    )\n    \n#    group_features_df = pd.concat(preprocess_df, ignore_index = True)\n\n#    return group_features_df","2517703a":"#  trade_df =  pd.read_parquet(f'..\/input\/optiver-realized-volatility-prediction\/trade_{dataset}.parquet\/stock_id={stock_id}\/')\n\n#  trade_df['stock_id'] = stock_id\n#  trade_df = trade_df.groupby(['stock_id', 'time_id']).agg(Price=('price', 'mean'),\n#                                                            Size=('size', 'sum'),\n#                                                            order_count=('order_count', 'sum'),\n#                                                            trade_count=('order_count', 'count')).reset_index()\n#    \n#    group_features = group_features.merge(trade_df, on=['stock_id', 'time_id'], how='left')","02683e3b":"train[\"target\"].plot.hist(bins=20)","2ec8eeb8":"np.log(train[\"target\"]).plot.hist(bins=20)","0b0bfc14":"book_train_df = book_train\ntrain_df = train","16f2dd61":"train_df[\"target\"] = np.log(train_df[\"target\"])","767b40e3":"train_df['row_id'] = train_df['stock_id'].astype(str) + '-' + train_df['time_id'].astype(str)","8102ebd4":"train_df","6b2bf3f9":"def pre_process1_impact1(df):\n    df['bid_impact1'] = df.bid_price1 * df.bid_size1\n    df['ask_impact1'] = df.ask_price1 * df.ask_size1\n    df[\"impact_bid\"] = (df.bid_impact1 - df.ask_impact1) \/ (df.bid_size1 + df.ask_size1)\n    df[\"impact_ask\"] = (df.ask_impact1 - df.bid_impact1) \/ (df.bid_size1 + df.ask_size1)\n    df.loc[df['impact_bid']<0, \"impact_bid\"] = 0\n    df.loc[df['impact_ask']<0, \"impact_ask\"] = 0\n    df[\"impact1\"] = df.impact_bid - df.impact_ask\n    df.drop(['bid_impact1', 'ask_impact1', 'impact_bid', 'impact_ask'], axis =1, inplace =True)\n    \n    return df","99909125":"def pre_process1_stability(df):\n    df['bid1_diff'] = df.bid_price1.diff()\n    df['ask1_diff'] = df.ask_price1.diff()\n    df.loc[df['bid1_diff'].isnull(), 'bid1_diff'] = 0\n    df.loc[df['ask1_diff'].isnull(), 'ask1_diff'] = 0\n    df['bid_ask_gap1'] = df.ask1_diff - df.bid1_diff\n    return df","7ed0cab2":"def pre_process1_spread(df):\n    \n    df['bid_price_spread'] = df.bid_price1 - df.bid_price2\n    df['ask_price_spread'] = df.ask_price2 - df.ask_price1\n    \n    df['bid_spread_diff'] = df.bid_price_spread.diff()\n    df['ask_spread_diff'] = df.ask_price_spread.diff()\n    \n    df.loc[df['bid_spread_diff'].isnull(), 'bid_spread_diff'] = 0\n    df.loc[df['ask_spread_diff'].isnull(), 'ask_spread_diff'] = 0\n    \n    df.drop(['bid_price_spread', 'ask_price_spread'], axis =1, inplace =True)\n    \n    return df","292c3f6c":"def preprocess(stock_id, dataset):\n    #book\n    book_df = pd.read_parquet(f'..\/input\/optiver-realized-volatility-prediction\/book_{dataset}.parquet\/stock_id={stock_id}\/')\n    book_df['stock_id'] = stock_id\n    \n    pre_process1_impact1(book_df)\n    pre_process1_stability(book_df)\n    pre_process1_spread(book_df)\n    \n    group_features = book_df.groupby(['stock_id','time_id']).agg(impact1=('impact1', 'mean'),                                                                 \n                                                                 bid1_diff=('bid1_diff', 'mean'),\n                                                                 ask1_diff=('ask1_diff', 'mean'),                                                                 \n                                                                 bid_Cnt = ('bid_price1', 'nunique'),\n                                                                 ask_Cnt = ('ask_price1', 'nunique'),\n                                                                 bid_ask_gap1=('bid_ask_gap1', 'mean'),\n                                                                 bid_spread_diff=('bid_spread_diff', 'mean'),\n                                                                 ask_spread_diff=('ask_spread_diff', 'mean')\n                                                                ).reset_index()\n        \n    #trade\n    trade_df =  pd.read_parquet(f'..\/input\/optiver-realized-volatility-prediction\/trade_{dataset}.parquet\/stock_id={stock_id}\/')\n    trade_df['stock_id'] = stock_id\n    trade_df = trade_df.groupby(['stock_id', 'time_id']).agg(Price=('price', 'mean'),\n                                                            Size=('size', 'sum'),\n                                                            order_count=('order_count', 'sum'),\n                                                            trade_count=('order_count', 'count')).reset_index()\n    \n    group_features = group_features.merge(trade_df, on=['stock_id', 'time_id'], how='left')\n    \n    return group_features","88deac7d":"def get_group_features(stock_ids : list, dataType = 'train'):\n\n    preprocess_df = Parallel(n_jobs=-1)(\n        delayed(preprocess)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    \n    group_features_df = pd.concat(preprocess_df, ignore_index = True)\n\n    return group_features_df","b9a660a4":"%%time\n\nstock_ids = list(train['stock_id'].unique())\ngroup_features_df = get_group_features(stock_ids, 'train')\n\ntrain_df = pd.merge(train_df, group_features_df, on = ['stock_id', 'time_id'], how ='left')\ntrain_df","fdcd657c":"group_features_df","fd2e7ec0":"train_df[\"row_id\"] = train_df[\"row_id\"].astype('category')\n\nX_train = train_df.drop(['target'], axis=1)\nY_train = train_df['target']","3808220d":"kf = KFold(n_splits = 3)\nmodels = []\nrmspes =[]\nlgbm_params ={\"objective\":\"regression\",\n              \"random_seed\":1234,\n              'num_leaves': 35,\n              'max_bin': 83,\n              'feature_fraction': 0.44892224224701704,\n              'bagging_fraction': 0.8310957587108416,\n              'bagging_freq': 10,\n              'min_data_in_leaf': 16,\n              'min_sum_hessian_in_leaf': 10}\ncategories = [\"row_id\", \"stock_id\"]\n\nfor train_index, val_index in kf.split(X_train):\n    XX_train = X_train.iloc[train_index]\n    XX_valid = X_train.iloc[val_index]\n    YY_train = Y_train.iloc[train_index]\n    YY_valid = Y_train.iloc[val_index]\n    \n    lgbm_train = lgbm.Dataset(XX_train, YY_train, categorical_feature = categories)\n    lgbm_eval = lgbm.Dataset(XX_valid, YY_valid, categorical_feature = categories, reference=lgbm_train)\n    \n    model_lgbm = lgbm.train(lgbm_params,\n                           lgbm_train,\n                           valid_sets = lgbm_eval,\n                           num_boost_round = 100,\n                           early_stopping_rounds = 20,\n                           verbose_eval = 10,\n                           )\n    y_pred = model_lgbm.predict(XX_valid, num_iteration = model_lgbm.best_iteration)\n    \n    tmp_rmspe = np.sqrt(np.mean(np.square((YY_valid - y_pred) \/ YY_valid)))\n    print (tmp_rmspe)\n    \n    models.append(model_lgbm)\n    rmspes.append(tmp_rmspe)","71343c33":"test_df = test\nbook_test_df = book_test\n\ntest_df","be704013":"stock_ids = list(test_df['stock_id'].unique())\ngroup_features_df = get_group_features(stock_ids, 'test')\n\ntest_df = pd.merge(test_df, group_features_df, on = ['stock_id', 'time_id'], how ='left')\ntest_df","d4e71c2e":"test_df[\"row_id\"] = test_df[\"row_id\"].astype('category')\n\nX_test = test_df\nX_test","39fe9f5a":"preds = []\n\nfor model in models:\n    pred = model.predict(X_test)\n    preds.append(pred)\n    \npreds_array = np.array(preds)\npreds_mean = np.mean(preds_array, axis =0)\n\npreds_mean = np.exp(preds_mean)\n\npreds_mean","e3833e49":"sub = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/sample_submission.csv')\nsub['target']=0.00000000\nsub['target'] = preds_mean\n\nsub.to_csv('submission.csv',index=False)\nsub","a134c375":"The result of re-submission shows 0.77856. This is also a bit big improvement. Good.\n\n\u518d\u63d0\u51fa\u3059\u308b\u3068\u3001\u30b9\u30b3\u30a2\u306f0.77856\u3078\u306e\u305d\u308c\u306a\u308a\u306b\u5927\u304d\u306a\u6539\u5584\u3002Good\u3002\n","033efc79":"**(9) 0.59093 --> 0.48964**\n\nAny good method..??? Once sit down, let me check how target's hist shows??\n\n\u4ed6\u306b\u624b\u306f\u306a\u3044\u304b\u306a\u30fc\u3002\u305d\u3046\u3044\u3048\u3070\u3001Target\u3063\u3066\u3001\u3069\u3046\u3044\u3046Hist\u306b\u306a\u3063\u3066\u3044\u308b\u3093\u3060\u308d\u3046\uff1f","18023a80":"**Beginner's gradual improvement \/ Optiver (Eng\/\u65e5\u672c\u8a9e)**\n\n(Updated with (11) insight for popular 'wap'\n\n(Updated with (10) 0.48964 --> 0.43887)\n\n(Credit to Raj Gandhi's notebook for Parallel\/delayed operation and adding trade_df. Thanks)\n\nThis is the 2nd try for me to join kaggle competition, still a beginner.\nThe public score has been improved from 2.51735 to 0.48964 after a lot of try and error, but still there are much room to improve... I'd be happy if you have a quick look at this notebook and get a slice of findings about how the score was improved with what operation.. \n\n\u4eca\u56de\u304c\uff12\u56de\u76ee\u306e\u30b3\u30f3\u30da\u53c2\u52a0\u3001\u307e\u3060\u307e\u3060\u99c6\u3051\u51fa\u3057\u3002\u3053\u306e\u30b3\u30f3\u30da\u3082\u304b\u306a\u308a\u82e6\u52b4\u3057\u3066\u30b9\u30b3\u30a2\u30922.51735\u21920.43887\u307e\u3067\u4f38\u3070\u3057\u305f\u3051\u308c\u3069\u3001\u307e\u3060\u307e\u3060\u6539\u5584\u306e\u4f59\u5730\u3042\u308a\u3002\u3002\u4f55\u3092\u3057\u305f\u3089\u3069\u308c\u3060\u3051\u6539\u5584\u3057\u305f\u304b\u306e\u53c2\u8003\u7a0b\u5ea6\u306b\u3054\u89a7\u3044\u305f\u3060\u3051\u308c\u3070\u5e78\u3044\u3067\u3059\u3002\n\n![image.png](attachment:2d8e4f22-7ff1-4bcc-a571-b1c2d1b8dd2b.png)\n","adeda5d5":"**(8) 0.77856 --> 0.59093**\n\nIt also may be good to use the count of price-change in every time_id(10 minutes). The result was, improved to **0.59093**. Quite good improvemet again.\n\n\u300c\u4e0d\u5b89\u5b9a\u3055\u300d\u3092\u793a\u3059\u306e\u306b\u30011time-id\uff0810\u5206\u9593\uff09\u306e\u9593\u306b\u4f55\u56de\u30d7\u30e9\u30a4\u30b9\u304c\u5909\u308f\u3063\u305f\u304b\u3001\u306e\u56de\u6570\u3082\u6709\u52b9\u304b\u3082\u3002bid_Cnt\u3068ask_Cnt\u3082\u7279\u5fb4\u91cf\u306b\u52a0\u3048\u3066\u307f\u308b\u3002\u7d50\u679c\u306f**0.59093**\u3078\u3068\u3001\u3053\u308c\u307e\u305f\u6bd4\u8f03\u7684\u5927\u304d\u306a\u6539\u5584\u3002","04754f63":"Try to execute LightGBM and submit again, using these new features.....then, pubic score shows **1.17477**.\nQuite improved. OK,OK :-)\n\n\u3053\u308c\u3089\u65b0\u3057\u3044\u7279\u5fb4\u91cf\u3092\u4f7f\u3063\u3066\u3001\u518d\u5ea6LightGBM\u306b\u639b\u3051\u308b\u3068\u3002\u3002\u3002\u30b9\u30b3\u30a2\u306f\u3001**1.17477**\u3002\u7d50\u69cb\u6539\u5584\u3057\u305f\u3002\u3088\u3057\u3088\u3057\u3002","94fea5b7":"There seems a room to improve. Try to take a log.\n\n\u3053\u308c\u306f\u6539\u5584\u306e\u3088\u3061\u3042\u308a\u305d\u3046\u3002\u5bfe\u6570\uff08log\uff09\u3092\u3068\u3063\u3066\u307f\u308b\u3002","b9a20150":"**(4) 1.61337 --> 1.17477**\n\nSo far, tried to use prices or sizes in every second as they are, but it may be better to have a 'stability (unchanged)' as a feature, besed on the thought Volatility means Unstability in other words. Actually, prices in upper records of book_train don't change frequently. Thus, added 'bid1_diff' and 'ask1_diff' as a new feature.\n\n\u3053\u308c\u307e\u3067\u306f\u3001\u6bce\u79d2\u306e\uff08\u53b3\u5bc6\u306b\u306f\u6bce\u79d2\u3067\u306f\u306a\u3044\u304c\uff09Price\u30fbSize\u3092\u305d\u306e\u307e\u307e\u4f7f\u3063\u3066\u3044\u305f\u304c\u3001\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u304c\u300c\u4e0d\u5b89\u5b9a\u3055\u300d\u3060\u3068\u3059\u308b\u3068\u3001\u3080\u3057\u308d\u5024\u52d5\u304d\u304c\u306a\u3044\u3053\u3068\u3092\u7279\u5fb4\u3068\u3057\u3066\u6301\u3064\u3079\u304d\u304b\u3082\u3002book_train\u306e\u6700\u521d\u306e\u307b\u3046\u306e\u30ec\u30b3\u30fc\u30c9\u307f\u3066\u3082\u4fa1\u683c\u3042\u3093\u307e\u308a\u52d5\u3044\u3066\u306a\u3044\u3082\u3093\u306d\u3002\u3053\u308c\u3089\u306f\u5024\u52d5\u304d\u30bc\u30ed\u3068\u3057\u3066\u300c\u5b89\u5b9a\u3057\u3066\u3044\u308b\u300d\u3068\u3068\u3089\u3048\u308b\u3079\u304d\u304b\u3068\u3002\u3068\u3044\u3046\u3053\u3068\u3067\u3001\u7279\u5fb4\u91cf\u306bbid1_diff\u3068ask1_diff\u3092\u52a0\u3048\u3066\u307f\u308b\u3002\n\n![image.png](attachment:8c8b6e12-ddec-4331-a94f-2863baeb4393.png)","0b7440fe":"The result was... **2.51735**. That's super bad! Quite shocked...\n\n\u7d50\u679c\u306f\u3001**2.51735**\u3002\u3002\u3002\u3063\u3066\u3001\u30d2\u30c9\u3059\u304e\u3067\u3057\u3087\uff01\u304b\u306a\u308a\u30b7\u30e7\u30c3\u30af\u3067\u3059\u3002","223c124a":"**(5) 1.17477 --> 1.15836**\n\nit could be said that Market tends to be unstable when the gap between Bid price and Ask price is large. Try to add this as a new feature. Public score was slightly improved to 1.15836. \n\nBid\u3068Ask\u306e\u5dee\u304c\u5927\u304d\u3044\u3068\u305d\u306e\u5206\u8352\u308c\u3084\u3059\u3044\u306e\u304b\u3082\u3057\u308c\u306a\u3044\u3002Bid price\u3068Ask price\u306e\u5dee\u3082\u7279\u5fb4\u91cf\u306b\u52a0\u3048\u3066\u307f\u308b\u30021.15836\u306b\u5c0f\u5e45\u6539\u5584\u3002","accb2ebb":"Take the gap between Bid and Ask (2 patterns for Bid>Ask and Bid<Ask) and calculate Max and Std for each, then execute LightGBM. After that I'll try to submit the prediction...\n\n\u3068\u3044\u3046\u3053\u3068\u3067\u3001Bid\u3068Ask\u306eImpact\uff08\u4fa1\u683c\uff58\u91cf\uff09\u306e\u5dee\u5206(Bid\uff1eAsk\u3068Bid\uff1cAsk\u306e\uff12\u30d1\u30bf\u30fc\u30f3\uff09\u306b\u3064\u3044\u3066\u305d\u308c\u305e\u308cMax\u3068Std\u3092\u3068\u308a\u3001LightGBM\u306b\u639b\u3051\u3001submit\u3057\u3066\u307f\u308b\u3068\u3002\u3002\u3002","e07fbeea":"I can see bid_price (buying price which buyer presents), ask_price (selling price which seller presents), and each size. \nLooks like what I should do is to calculate the price\/size which changes by every second and aggregate the information into time_id (10-minute unit), and then calculate\/predict the volatility. \nBased on the first idea that I could say the market is unstable if Bid and Ask are imbalance, I'll try to use the gap between Bid's impact and Ask's impact (price x size) as a feature for predict.\n\nBid\u4fa1\u683c\uff08\u8cb7\u3044\u305f\u3044\u4eba\u304c\u63d0\u793a\u3059\u308b\u8cb7\u5024\uff09\u3001Ask\u4fa1\u683c\uff08\u58f2\u308a\u305f\u3044\u4eba\u304c\u63d0\u793a\u3059\u308b\u58f2\u5024\uff09\u3001\u305d\u308c\u305e\u308c\u306e\u91cf\uff08Size\uff09\u304c\u57fa\u672c\u7684\u306b\u3042\u308b\u306e\u304b\u3002\n\u305d\u308c\u304c\u79d2\u5358\u4f4d\u3067\u5909\u52d5\u3059\u308b\u306e\u3067\u3001\u305d\u306e\u5909\u52d5\u60c5\u5831\u309210\u5206\u5358\u4f4d\u306etime_id\u3054\u3068\u306b\u96c6\u8a08\u3057\u3066\u3001\u305d\u306e10\u5206\u306b\u304a\u3051\u308b\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u7b97\u51fa\u30fb\u4e88\u6e2c\u3059\u308b\u3001\u3068\u3044\u3046\u3053\u3068\u306d\u3002\n\u307e\u305a\u306f\u3001Bid\u3068Ask\u304c\u4e0d\u91e3\u308a\u5408\u3044\u3060\u3068\u300c\u4e0d\u5b89\u5b9a\u300d\u3068\u8a00\u3048\u308b\u3093\u3060\u308d\u3046\u306a\u3002\u3002\u3002\u3068\u3044\u3046\u8003\u3048\u3067\u3001\u300cBid\u3068Ask\u306e\u30a4\u30f3\u30d1\u30af\u30c8\uff08\u4fa1\u683c\uff38\u91cf\uff09\u306e\u5dee\u300d\u3092\u7279\u5fb4\u91cf\u3068\u3057\u3066\u8003\u3048\u3066\u307f\u308b\u300210\u5206\u9593\u306b\u304a\u3051\u308b\u30a4\u30f3\u30d1\u30af\u30c8\u306e\u6700\u5927\u5024\u3068\u6a19\u6e96\u504f\u5dee\u3092\u7279\u5fb4\u91cf\u3068\u3057\u3066\u4f7f\u3063\u3066\u307f\u308b\u3002","27d7c40a":"**(3) 1.69818 --> 1.61337**\n\nAfter reconsidering I may not need to have 2 columns 'bid1-ask1' and 'ask1-bid1', hence they were consolidated into 1 coulmn named 'impact1'. At the same time, aggregation method was changed from std to mean. It contributed to a slight improvement to 1.61337.\n\n\u305d\u3082\u305d\u3082bid1-ask1\u3068ask1-bid1\u30682\u5217\u6301\u3064\u5fc5\u8981\u3082\u306a\u3044\u306a\u3002\u3002\u3068\u601d\u3044\u3001impact1\u3068\u3044\u3046\u4e00\u5217\u306b\u7d71\u5408\u3002\u3042\u308f\u305b\u3066std\u3067\u306f\u306a\u304fmean\u3067\u96c6\u8a08\u3059\u308b\u3053\u3068\u306b\u5909\u66f4\u3002\u3053\u308c\u306b\u3088\u308a1.61337\u3078\u3068\u5c0f\u5e45\u6539\u5584\u3002","830e80f4":"**(6) 1.15836 --> 0.90530**\n\nThough gradually improved so far, score is still over 1.0 and almost near to the worst in the leaderboard. Now it may be a time to try more drastic method. What to do first is to use all stock_ids, though I purpposely used stock_id=0 only since I always encounter memory-over error when use all stocks. However, I noticed I could use a convenient method 'Parallel \/ delayed' when I refered to Raj Gandhi' notebook. Try this.\n\n\u3053\u308c\u307e\u3067\u6539\u5584\u3092\u7e70\u308a\u8fd4\u3057\u3066\u304d\u305f\u3082\u306e\u306e\u3001\u305d\u308c\u3067\u3082\u307e\u30601.0\u3092\u8d85\u3048\u3066\u304a\u308aLeaderboard\u3067\u306f\u6700\u4e0b\u4f4d\u306b\u8fd1\u3044\u3002\u3053\u306e\u3042\u305f\u308a\u3067\u3001\u7279\u5fb4\u91cf\u4ee5\u5916\u306e\u6839\u672c\u7684\u306a\u624b\u3092\u6253\u3064\u3079\u304d\u304b\u3082\u3002\u307e\u305a\u3084\u308b\u3079\u304d\u306f\u3001\u5bfe\u8c61\u3092Stock_id=0\u3060\u3051\u306b\u7d5e\u3063\u3066\u3044\u305f\u3053\u3068\u3002\u7e8f\u3081\u3066\u5168\u3066\u306eStock_id\u3092\u5bfe\u8c61\u306b\u3059\u308b\u3068\u5fc5\u305a\u30e1\u30e2\u30ea\u30fc\u30aa\u30fc\u30d0\u30fc\u30a8\u30e9\u30fc\u3068\u306a\u308b\u3053\u3068\u304b\u30890\u3060\u3051\u306b\u7d5e\u3063\u3066\u3044\u305f\u304c\u3001Raj Gandhi\u3055\u3093\u306eNotebook\u3092\u898b\u3066\u3001Parallel\u51e6\u7406\u3092\u3059\u308b\u4fbf\u5229\u306a\u624b\u6cd5\u304c\u3042\u308b\u3053\u3068\u3092\u77e5\u3063\u305f\u3002\u3053\u308c\u3092\u8a66\u3057\u3066\u307f\u308b\u3002\n","640bd688":"**(2) 2.51735 --> 1.69818**\u3000\n\nHowever, once sit down and reconsider, I noticed it may be a bit weird to use Max. Even though Bad and Ask is imbalance, if the imbalace continues for long time, we could say the situation is 'stable'.\nLet's remove 'max' from features.\n\nThe result of re-submission is **1.69818**. Much improved!\n\n\u305f\u3060\u3001\u51b7\u9759\u306b\u8003\u3048\u3066\u307f\u308b\u3068\u3001Max\u3092\u4f7f\u3046\u306e\u306f\u5909\u304b\u306a\u3068\u6c17\u3065\u3044\u305f\u3002Bid\u3068Ask\u304c\u4e0d\u91e3\u308a\u5408\u3044\u3067\u3082\u3001\u305d\u306e\u72b6\u614b\u304c\u5909\u52d5\u305b\u305a\u306b\u305a\u3063\u3068\u7d9a\u3044\u3066\u308b\u3063\u3066\u3001\u3042\u308b\u610f\u5473\u3001\u4e0d\u91e3\u308a\u5408\u3044\u306e\u307e\u307e\u300c\u5b89\u5b9a\u300d\u3057\u3066\u3044\u308b\u3063\u3066\u3053\u3068\u3060\u3082\u3093\u306d\u3002\u7279\u5fb4\u91cf\u304b\u3089max\u3092\u9664\u5916\u3057\u3066\u307f\u3088\u3046\u3002\n\u7d50\u679c\u306f\u3001**1.69818**\u3002\u3053\u308c\u3060\u3051\u3067\u304b\u306a\u308a\u6539\u5584\u3002","0e85bac9":"As expected ! Let's use target after taking log :-)\u3000The result was, improved to **0.48964**. OK!!\n\n\n\u3084\u3063\u3071\u308a\u306d\u3002target\u306fLog\u3057\u305f\u3082\u306e\u3092\u4f7f\u304a\u3046\u3002\u7d50\u679c\u306f**0.48964**\u3078\u3068\u6539\u5584\u3002\uff2f\uff2b\uff01\n\n\n**(10) 0.48964 --> 0.43887**\n\nLet's last try! Try to use Bid2, Ask2 which were not used so far. Add diff of bid1-bid2 and ask2-ask1 as a feature.\nImproved from 0.48964 to 0.43887 :-)\n\nAfter all, my notebook became as follows.\n\n\u6700\u5f8c\u306b\u3082\u3046\u4e00\u62bc\u3057\u3002\u3053\u308c\u307e\u3067\u3064\u304b\u3063\u3066\u3044\u306a\u304b\u3063\u305fBid2\u3001Ask2\u3092\u4f7f\u3063\u3066\u307f\u307e\u3057\u305f\u3002Bid1-Bid2\u3001Ask2-Ask1\u306eDiff\u3092\u7279\u5fb4\u91cf\u306b\u8ffd\u52a0\u3002\n0.43887\u306b\u6539\u5584\u3057\u307e\u3057\u305f\u3002\n\n**(11) Try to use popular 'wap' instead of my 'impact'\u3000--> not improved!**\n\nTrying further improvement, I re-consider whether or not my own idea 'impact1' is really correct. Having a look at others' Notebook, I noticed 'wap' is popular, which is bid1_price * ask_size1, when calculate bid\/ask price and size. Honestly, I'm not sure why bid_price and ask_size should be multipled... In my own idea 'impact1', simply 'bid_price' and 'bid_size' are multipled. But, let's try to use 'wap' instead of 'impact1'!! ... but result of public score is 0.46991...Got worse.. My original idea 'impact1' looks better than popular 'wap', though I'm not confident...\n\n\u3055\u3089\u306a\u308b\u6539\u5584\u304c\u3067\u304d\u306a\u3044\u304b\u3068\u8003\u3048\u3001\u6211\u6d41\u3067\u4f7f\u3063\u3066\u3044\u308b\u300cimpact1\u300d\u3068\u3044\u3046\u7279\u5fb4\u91cf\u306f\u672c\u5f53\u306b\u6b63\u3057\u3044\u304b\u8003\u3048\u3066\u307f\u305f\u3002bid\/ask price\u3068size\u3092\u3069\u3046\u7d44\u307f\u5408\u308f\u305b\u308b\u304b\u3068\u3044\u3046\u554f\u984c\u3067\u3001\u4ed6\u306eNotebook\u3067\u306f\u3001bid1_price * ask_size1\u3068\u3044\u3046\u3088\u3046\u306b\u3001\u4f55\u6545\u304bprice\u3068size\u306e\u639b\u3051\u7b97\u3092\u3059\u308b\u3068\u304d\u306bbid\u3068ask\u3092\u9006\u306b\u3057\u3066\u300cwap\u300d\u3068\u3057\u3066\u4f7f\u3063\u3066\u3044\u308b\u3002\u4e00\u65b9\u3001\u6211\u6d41\u306eimpact1\u306f\u3001bid1_price * bid1_size\u3068\u3044\u3046\u3088\u3046\u306b\u3001price\u3082size\u3082\u540c\u3058bid\u306e\u3082\u306e\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002\u5b9f\u306f\u3001\u4e00\u822c\u7684\u306b\u4f55\u6545bid\u3068ask\u3092\u9006\u306b\u3057\u305f\u3082\u306e\u3092\u3064\u304b\u3063\u3066\u3044\u308b\u306e\u304b\u7406\u7531\u304c\u3088\u304f\u308f\u304b\u3089\u306a\u3044\u3002\u3002\u305f\u3060\u3001\u3082\u306e\u306f\u8a66\u3057\u306b\u3068\u3044\u3046\u3053\u3068\u3067\u3001\u6211\u6d41\u306eimpact1\u306e\u4ee3\u308f\u308a\u306bwap1\u3068\u3044\u3046\u3082\u306e\u3092\u3064\u304b\u3063\u3066\u307f\u305f\u3002\u7d50\u679c\u306f\u30010.43887-->0.46991\u3078\u3068\u60aa\u5316\u3002\u624b\u524d\u5473\u564c\u3067\u6050\u7e2e\u3067\u3059\u304c\u3001\u3084\u3063\u3071\u308a\u6211\u6d41\u306eimpact1\u304c\u826f\u3055\u305d\u3046\u3002\n\n**\u3068\u3044\u3046\u3053\u3068\u3067\u3001\u3053\u308c\u307e\u3067\u306e\u7d50\u679c\u3001Notebook\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002**","1269f859":"Public score was improved to **0.90530**. Wow, score became under1.0 ! Ok,Ok, :-)\n\n\u30b9\u30b3\u30a2\u306f**0.90530**\u3068\u3001\u521d\u3081\u30661.0\u3092\u5207\u3063\u305f\u3002\u826f\u3057\u3088\u3057\u3002","8c730dc7":"**(7) 0.90530 --> 0.77856**\n\nNext, try to use Trade information. Not only price presentation but also actual trade information may be an important factor.\n\n\u6b21\u306b\u3001\u3053\u308c\u307e\u3067\u4f7f\u3063\u3066\u3044\u306a\u304b\u3063\u305fTrade\u60c5\u5831\u3092\u4f7f\u3063\u3066\u307f\u308b\u3002Price\u306e\u63d0\u793a\u3060\u3051\u3057\u3066\u3044\u3066\u3082\u5b9f\u969b\u306bDeal\u304c\u6210\u7acb\u3057\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u306f\u5927\u304d\u306a\u8981\u7d20\u3060\u3057\u306d\u3002","9b500b4c":"At the beginning, I frequently encountered memory-over error when I try to use all stock_ids, and hence used stock_id=0 only. This problem was solved thanks to the notebook shared by Raj Gandhi, but I'll mention about that afterward.\n\n\u6700\u521d\u306e\u3046\u3061\u306f\u5168stock_id\u3092\u5bfe\u8c61\u306b\u3059\u308b\u3068\u30e1\u30e2\u30ea\u30fc\u30aa\u30fc\u30d0\u30fc\u30a8\u30e9\u30fc\u304c\u51fa\u307e\u304f\u3063\u3066\u3044\u305f\u306e\u3067\u3001\u5bfe\u8c61\u3092stock_id\u30920\u3060\u3051\u306b\u7d5e\u3063\u3066\u3044\u305f\u3002\u3053\u308c\u306f\u3001Raj Gandhi\u3055\u3093\u306eNotebook\u3092\u53c2\u8003\u306b\u3057\u3066\u89e3\u6d88\u3067\u304d\u305f\uff08\u5168stock_id\u3092\u5bfe\u8c61\u306b\u3067\u304d\u305f\uff09\u306e\u3060\u304c\u3001\u3053\u308c\u306b\u3064\u3044\u3066\u306f\u5f8c\u8ff0\u3002","16680aca":"**(1)** First of all, think about \"Volatility\". Refering to the old textbook on Option dealing I bought in the past, Volatility means 'how easily price changes in the market'. It seems there are 2 kinds of Volatility, Historical volatility(HV) and Implicit volatility(IV). HV seems easier to understand, typical situation is that a war happens somewhere in the world and the market is unstable, then peaple say 'volatility is high'. However, considering the given conditions in this competition, looks like 'volatility' means historical volatility which is calculated based on the actual price change in the past.\n\nIn short, it seems ok if I can find any features to show 'how unstable'.\n\n\u307e\u305a\u306f\u3001\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u306b\u3064\u3044\u3066\u8003\u3048\u3066\u307f\u308b\u3002\u6614\u8cb7\u3063\u305f\u30aa\u30d7\u30b7\u30e7\u30f3\u30c7\u30a3\u30fc\u30eb\u306e\u672c\u3092\u898b\u3066\u307f\u308b\u3068\u3001\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3068\u306f\u300c\u4fa1\u683c\u5909\u52d5\u306e\u5ea6\u5408\u3044\u300d\u3060\u3051\u3069\u30012\u7a2e\u985e\u3042\u308b\u307f\u305f\u3044\u3002\u30d2\u30b9\u30c8\u30ea\u30ab\u30eb\u30fb\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3(HV)\u3068\u30a4\u30f3\u30d7\u30ea\u30b7\u30c3\u30c8\u30fb\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3(IV)\u3002\u611f\u899a\u7684\u306b\u306fIV\u306e\u307b\u3046\u304c\u5206\u304b\u308a\u3084\u3059\u3044\u304b\u306a\u3001\u4f8b\u3048\u3070\u6226\u4e89\u306a\u3093\u304b\u304c\u8d77\u3053\u3063\u3066\u5e02\u5834\u5fc3\u7406\u304c\u4e0d\u5b89\u5b9a\u306b\u306a\u3063\u3066\u3044\u308b\u5834\u5408\u3001\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u304c\u9ad8\u3044\u3068\u8a00\u308f\u308c\u308b\u3002\u305f\u3060\u3001\u4eca\u56de\u306e\u306f\u904e\u53bb\u306e\u4fa1\u683c\u63a8\u79fb\u3092\u3082\u3068\u306b\u7b97\u51fa\u3055\u308c\u308bHV\u306e\u307b\u3046\u306e\u30b3\u30f3\u30da\u307f\u305f\u3044\u3002\n\n\u8981\u306f\u3001\u300c\u5982\u4f55\u306b\u4e0d\u5b89\u5b9a\u304b\u300d\u3092\u793a\u3059\u7279\u5fb4\u91cf\u3092\u898b\u3064\u3051\u51fa\u305b\u308c\u3070\u826f\u3055\u305d\u3046\u3002"}}