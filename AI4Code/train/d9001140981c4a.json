{"cell_type":{"3ba1b248":"code","7f7f9d7e":"code","9208d12c":"code","ee4010ae":"code","b48e475d":"code","377ae18e":"code","1cba366f":"code","149ab13b":"code","a3f874ff":"code","880dd856":"code","bebb5c25":"code","f46159d3":"code","bd4a5f2d":"code","91a5292b":"code","d0497749":"code","2e74b173":"code","02852176":"code","71b4b08b":"code","e8af6b11":"code","8b4a96ce":"code","c2ecf058":"code","6cf05adb":"code","24134306":"code","2c9b7e62":"code","7ae2517b":"code","e6fa5590":"code","54be2a6a":"code","00d5e315":"code","eee7582d":"code","83aa8d84":"code","1204316f":"code","e0259d83":"code","0f6bd09c":"code","493bdd82":"code","7f452ff0":"code","2b234a4d":"code","2f021de7":"code","a4b1a9c3":"code","2e96c8a1":"code","59fc06bf":"code","34b762cd":"code","7dfa656f":"code","8c517324":"code","fb6025cd":"code","8ad2c027":"code","db85fe8f":"code","ec4007dd":"code","8c902d97":"code","264403b7":"code","fbe8e1e1":"code","1a7b369c":"code","4961c90b":"code","3775c4ef":"code","f5749f47":"code","528aeab8":"code","8d7d5269":"code","bf6856d8":"code","0123497e":"code","22875159":"code","9ba89e55":"code","444077c0":"markdown","b89c4a0c":"markdown","90dfd6c9":"markdown","fccda11c":"markdown","a7639daa":"markdown","1c8f87f7":"markdown","570d7df3":"markdown","89f406de":"markdown","51d5e833":"markdown","414c6d09":"markdown","41da6038":"markdown","4f6d786c":"markdown","37fe030c":"markdown","308ad816":"markdown","17161b79":"markdown","23f50300":"markdown","d242c4f3":"markdown","69b0477b":"markdown","dcc9b9a0":"markdown","16f05379":"markdown","1fc67966":"markdown","9ee5a491":"markdown","3250504b":"markdown","26f6c45e":"markdown","2119213e":"markdown","113ec131":"markdown","ad8eddde":"markdown","6e9e1837":"markdown"},"source":{"3ba1b248":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport datetime\nfrom kaggle.competitions import nflrush\nimport tqdm\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport keras\n\nsns.set_style('darkgrid')\nmpl.rcParams['figure.figsize'] = [15,10]","7f7f9d7e":"env = nflrush.make_env()","9208d12c":"train = pd.read_csv('..\/input\/nfl-big-data-bowl-2020\/train.csv', dtype={'WindSpeed': 'object'})","ee4010ae":"train.drop('Yards', axis=1, inplace=True) ","b48e475d":"train['target'] = 1\nmerge = [train] \n\nfor test, sample in tqdm.tqdm(env.iter_test()):\n    test['target'] = 0\n    merge.append(test)\n    env.predict(pd.DataFrame(data=np.zeros((len(test.PlayId.unique()), 199)) ,columns=sample.columns))\n\ntrain = pd.concat(merge) ","377ae18e":"train.head()","1cba366f":"train['PlayId'].value_counts()","149ab13b":"cat_features = []\nfor col in train.columns:\n    if train[col].dtype =='object':\n        cat_features.append((col, len(train[col].unique())))","a3f874ff":"off_form = train['OffenseFormation'].unique()\ntrain['OffenseFormation'].value_counts()","880dd856":"train = pd.concat([train.drop(['OffenseFormation'], axis=1), pd.get_dummies(train['OffenseFormation'], prefix='Formation')], axis=1)\ndummy_col = train.columns","bebb5c25":"train['GameClock'].value_counts()","f46159d3":"def strtoseconds(txt):\n    txt = txt.split(':')\n    ans = int(txt[0])*60 + int(txt[1]) + int(txt[2])\/60\n    return ans","bd4a5f2d":"train['GameClock'] = train['GameClock'].apply(strtoseconds)","91a5292b":"sns.distplot(train['GameClock'])","d0497749":"train['PlayerHeight']","2e74b173":"train['PlayerHeight'] = train['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))","02852176":"train['TimeHandoff']","71b4b08b":"train['TimeHandoff'] = train['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\ntrain['TimeSnap'] = train['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))","e8af6b11":"train['TimeDelta'] = train.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)","8b4a96ce":"train['PlayerBirthDate'] = train['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m\/%d\/%Y\"))","c2ecf058":"seconds_in_year = 60*60*24*365.25\ntrain['PlayerAge'] = train.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()\/seconds_in_year, axis=1)","6cf05adb":"train = train.drop(['TimeHandoff', 'TimeSnap', 'PlayerBirthDate'], axis=1)","24134306":"train['WindSpeed'].value_counts()","2c9b7e62":"train['WindSpeed'] = train['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)","7ae2517b":"train['WindSpeed'].value_counts()","e6fa5590":"#let's replace the ones that has x-y by (x+y)\/2\n# and also the ones with x gusts up to y\ntrain['WindSpeed'] = train['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))\/2 if not pd.isna(x) and '-' in x else x)\ntrain['WindSpeed'] = train['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))\/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)","54be2a6a":"def str_to_float(txt):\n    try:\n        return float(txt)\n    except:\n        return -1","00d5e315":"train['WindSpeed'] = train['WindSpeed'].apply(str_to_float)","eee7582d":"train['WindDirection'].value_counts()","83aa8d84":"train.drop('WindDirection', axis=1, inplace=True)","1204316f":"train['PlayDirection'].value_counts()","e0259d83":"train['PlayDirection'] = train['PlayDirection'].apply(lambda x: x is 'right')","0f6bd09c":"train['Team'] = train['Team'].apply(lambda x: x.strip()=='home')","493bdd82":"train['GameWeather'].unique()","7f452ff0":"train['GameWeather'] = train['GameWeather'].str.lower()\nindoor = \"indoor\"\ntrain['GameWeather'] = train['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x)\ntrain['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly') if not pd.isna(x) else x)\ntrain['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('clear and sunny', 'sunny and clear') if not pd.isna(x) else x)\ntrain['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)","2b234a4d":"train['GameWeather'].unique()","2f021de7":"from collections import Counter\nweather_count = Counter()\nfor weather in train['GameWeather']:\n    if pd.isna(weather):\n        continue\n    for word in weather.split():\n        weather_count[word]+=1\n        \nweather_count.most_common()[:15]","a4b1a9c3":"def map_weather(txt):\n    ans = 1\n    if pd.isna(txt):\n        return 0\n    if 'partly' in txt:\n        ans*=0.5\n    if 'climate controlled' in txt or 'indoor' in txt:\n        return ans*3\n    if 'sunny' in txt or 'sun' in txt:\n        return ans*2\n    if 'clear' in txt:\n        return ans\n    if 'cloudy' in txt:\n        return -ans\n    if 'rain' in txt or 'rainy' in txt:\n        return -2*ans\n    if 'snow' in txt:\n        return -3*ans\n    return 0","2e96c8a1":"train['GameWeather'] = train['GameWeather'].apply(map_weather)","59fc06bf":"train['IsRusher'] = train['NflId'] == train['NflIdRusher']","34b762cd":"train.drop(['NflId', 'NflIdRusher'], axis=1, inplace=True)","7dfa656f":"train = train.sort_values(by=['PlayId', 'Team', 'IsRusher']).reset_index()","8c517324":"train.drop(['GameId', 'PlayId', 'index', 'IsRusher', 'Team'], axis=1, inplace=True)","fb6025cd":"cat_features = []\nfor col in train.columns:\n    if train[col].dtype =='object':\n        cat_features.append(col)\n        \ntrain = train.drop(cat_features, axis=1)","8ad2c027":"train.fillna(-999, inplace=True)","db85fe8f":"players_col = []\nfor col in train.columns:\n    if train[col][:22].std()!=0:\n        players_col.append(col)","ec4007dd":"X_train = np.array(train[players_col]).reshape(-1, 11*22)","8c902d97":"play_col = train.drop(players_col+['target'], axis=1).columns\nX_play_col = np.zeros(shape=(X_train.shape[0], len(play_col)))\nfor i, col in enumerate(play_col):\n    X_play_col[:, i] = train[col][::22]","264403b7":"X_train = np.concatenate([X_train, X_play_col], axis=1)\ny_train = train['target'][::22].values","fbe8e1e1":"from keras.callbacks import EarlyStopping","1a7b369c":"from keras import backend as K\n\n\n__all__ = ['RAdam']\n\n\nclass RAdam(keras.optimizers.Optimizer):\n    \"\"\"RAdam optimizer.\n    # Arguments\n        learning_rate: float >= 0. Learning rate.\n        beta_1: float, 0 < beta < 1. Generally close to 1.\n        beta_2: float, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n        decay: float >= 0. Learning rate decay over each update.\n        weight_decay: float >= 0. Weight decay for each param.\n        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n            algorithm from the paper \"On the Convergence of Adam and\n            Beyond\".\n        total_steps: int >= 0. Total number of training steps. Enable warmup by setting a positive value.\n        warmup_proportion: 0 < warmup_proportion < 1. The proportion of increasing steps.\n        min_lr: float >= 0. Minimum learning rate after warmup.\n    # References\n        - [Adam - A Method for Stochastic Optimization](https:\/\/arxiv.org\/abs\/1412.6980v8)\n        - [On the Convergence of Adam and Beyond](https:\/\/openreview.net\/forum?id=ryQu7f-RZ)\n        - [On The Variance Of The Adaptive Learning Rate And Beyond](https:\/\/arxiv.org\/pdf\/1908.03265v1.pdf)\n    \"\"\"\n\n    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., weight_decay=0., amsgrad=False,\n                 total_steps=0, warmup_proportion=0.1, min_lr=0., **kwargs):\n        learning_rate = kwargs.pop('lr', learning_rate)\n        super(RAdam, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n            self.total_steps = K.variable(total_steps, name='total_steps')\n            self.warmup_proportion = K.variable(warmup_proportion, name='warmup_proportion')\n            self.min_lr = K.variable(min_lr, name='min_lr')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n        self.initial_weight_decay = weight_decay\n        self.initial_total_steps = total_steps\n        self.amsgrad = amsgrad\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n\n        if self.initial_decay > 0:\n            lr = lr * (1. \/ (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n\n        if self.initial_total_steps > 0:\n            warmup_steps = self.total_steps * self.warmup_proportion\n            decay_steps = K.maximum(self.total_steps - warmup_steps, 1)\n            decay_rate = (self.min_lr - lr) \/ decay_steps\n            lr = K.switch(\n                t <= warmup_steps,\n                lr * (t \/ warmup_steps),\n                lr + decay_rate * K.minimum(t - warmup_steps, decay_steps),\n            )\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='m_' + str(i)) for (i, p) in enumerate(params)]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='v_' + str(i)) for (i, p) in enumerate(params)]\n\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='vhat_' + str(i)) for (i, p) in enumerate(params)]\n        else:\n            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        beta_1_t = K.pow(self.beta_1, t)\n        beta_2_t = K.pow(self.beta_2, t)\n\n        sma_inf = 2.0 \/ (1.0 - self.beta_2) - 1.0\n        sma_t = sma_inf - 2.0 * t * beta_2_t \/ (1.0 - beta_2_t)\n\n        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n\n            m_corr_t = m_t \/ (1.0 - beta_1_t)\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                v_corr_t = K.sqrt(vhat_t \/ (1.0 - beta_2_t))\n                self.updates.append(K.update(vhat, vhat_t))\n            else:\n                v_corr_t = K.sqrt(v_t \/ (1.0 - beta_2_t))\n\n            r_t = K.sqrt((sma_t - 4.0) \/ (sma_inf - 4.0) *\n                         (sma_t - 2.0) \/ (sma_inf - 2.0) *\n                         sma_inf \/ sma_t)\n\n            p_t = K.switch(sma_t >= 5, r_t * m_corr_t \/ (v_corr_t + self.epsilon), m_corr_t)\n\n            if self.initial_weight_decay > 0:\n                p_t += self.weight_decay * p\n\n            p_t = p - lr * p_t\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    @property\n    def lr(self):\n        return self.learning_rate\n\n    @lr.setter\n    def lr(self, learning_rate):\n        self.learning_rate = learning_rate\n\n    def get_config(self):\n        config = {\n            'learning_rate': float(K.get_value(self.learning_rate)),\n            'beta_1': float(K.get_value(self.beta_1)),\n            'beta_2': float(K.get_value(self.beta_2)),\n            'decay': float(K.get_value(self.decay)),\n            'weight_decay': float(K.get_value(self.weight_decay)),\n            'epsilon': self.epsilon,\n            'amsgrad': self.amsgrad,\n            'total_steps': float(K.get_value(self.total_steps)),\n            'warmup_proportion': float(K.get_value(self.warmup_proportion)),\n            'min_lr': float(K.get_value(self.min_lr)),\n        }\n        base_config = super(RAdam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","4961c90b":"def plot_with_dots(ax, np_array):\n    ax.scatter(list(range(1, len(np_array) + 1)), np_array, s=50)\n    ax.plot(list(range(1, len(np_array) + 1)), np_array)","3775c4ef":"def train_model(x_tr, y_tr, x_vl, y_vl):\n    model = keras.models.Sequential([\n        keras.layers.Dense(units=512, input_shape=[X_train.shape[1]]),\n        keras.layers.BatchNormalization(),\n        keras.layers.LeakyReLU(0.3),\n        keras.layers.Dropout(0.25),\n        \n        keras.layers.Dense(units=512),\n        keras.layers.BatchNormalization(),\n        keras.layers.LeakyReLU(0.3),\n    \n        keras.layers.Dropout(0.25),\n        \n        keras.layers.Dense(units=1, activation='sigmoid')\n    ])\n    \n    er = EarlyStopping(patience=3, min_delta=1e-4, restore_best_weights=True, monitor='val_accuracy')\n    \n    model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-5), \n                  loss='binary_crossentropy',\n                  metrics=['accuracy'] \n                 )\n    h=model.fit(x_tr, y_tr, epochs=10, callbacks=[er], validation_data=[x_vl, y_vl], verbose=0)\n    return h.history['val_accuracy'] ","f5749f47":"from sklearn.preprocessing import StandardScaler","528aeab8":"scaler = StandardScaler() \nX_train = scaler.fit_transform(X_train) ","8d7d5269":"from sklearn.model_selection import StratifiedKFold \n\nrkf = StratifiedKFold(n_splits=5)","bf6856d8":"from keras import backend as K","0123497e":"list_acc = []\nfor tr_idx, vl_idx in tqdm.tqdm_notebook(rkf.split(X_train, y_train)):\n    \n    x_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n    x_vl, y_vl = X_train[vl_idx], y_train[vl_idx]\n    \n    acc = train_model(x_tr, y_tr, x_vl, y_vl)\n    list_acc.append(acc)","22875159":"plt.figure(figsize=(10, 7))\nfor acc in list_acc:\n    plot_with_dots(plt, [0] + list(acc))","9ba89e55":"env.write_submission_file()","444077c0":"- Let's see how PlayId is distribuited","b89c4a0c":"We are going to apply the following preprocessing:\n \n- Lower case\n- N\/A Indoor, N\/A (Indoors) and Indoor => indoor Let's try to cluster those together.\n- coudy and clouidy => cloudy\n- party => partly\n- sunny and clear => clear and sunny\n- skies and mostly => \"\"","90dfd6c9":"## Time handoff and snap and Player BirthDate","fccda11c":"Since I don't have any knowledge about formations, I am just goig to one-hot encode this feature","a7639daa":"## Game Clock","1c8f87f7":"Since we already have the quarter feature, we can just divide the Game Clock by 15 minutes so we can get the normalized time left in the quarter.","570d7df3":"## Team","89f406de":"## Wind Speed and Direction","51d5e833":"We can see there are some values that are not standardized(e.g. 12mph), we are going to remove mph from all our values.","414c6d09":"## Player height","41da6038":"The wind direction won't affect our model much because we are analyzing running plays so we are just going to drop it.","4f6d786c":"Let's preprocess some of those features.","37fe030c":"We know that 1ft=12in, thus:","308ad816":"Game clock is supposed to be a numerical feature.","17161b79":"## NflId NflIdRusher","23f50300":"So current network can split train and test data. Thus I'm not recommend use current feature set.\nNext step is used LightGBM for having possibility doing feature importance plot and removing 'bad' feature subset. ","d242c4f3":"To encode our weather we are going to do the following map:\n \n- climate controlled or indoor => 3, sunny or sun => 2, clear => 1, cloudy => -1, rain => -2, snow => -3, others => 0\n- partly => multiply by 0.5\n\nI don't have any expercience with american football so I don't know if playing in a climate controlled or indoor stadium is good or not, if someone has a good idea on how to encode this it would be nice to leave it in the comments :)","69b0477b":"## PlayDirection","dcc9b9a0":"Let's drop the categorical features and run a simple random forest in our model","16f05379":"## Offense formation","1fc67966":"Let's use the time handoff to calculate the players age","9ee5a491":"As expected, we have 22 of each playid since we have 22 players.\n","3250504b":"We are now going to make one big row for each play where the rusher is the last one","26f6c45e":"## Game Weather","2119213e":"# Categorical features","113ec131":"Let's now look at the most common words we have in the weather description","ad8eddde":"# Baseline model","6e9e1837":"# Overall analysis"}}