{"cell_type":{"5975c209":"code","a5442c93":"code","1b2be1b5":"code","6fff66eb":"code","b4d09a8f":"code","b4211549":"code","ec8db4c4":"code","03e42087":"code","85857886":"code","d76ad8b8":"code","2736fdb5":"code","9f7a7206":"code","3eace710":"code","1b6689cf":"code","f5ad95e7":"code","8d86845c":"code","c7a7c85f":"code","ae6ee9a7":"code","d6a74f20":"code","7c52f9a7":"code","963e2a51":"code","cc8a4af5":"markdown","6c43c85c":"markdown","bc79b960":"markdown","3cb39e6a":"markdown","2e0291aa":"markdown","1db03810":"markdown","617f37bc":"markdown","676d6ea1":"markdown","5b84f872":"markdown","2c1a603f":"markdown","b12dacb4":"markdown","d8239549":"markdown","48bbe874":"markdown","5b30cf15":"markdown","657e0918":"markdown","f117ba9f":"markdown","de2c2180":"markdown"},"source":{"5975c209":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# sklearn-packages\nfrom sklearn.model_selection import train_test_split\n","a5442c93":"# Read in the dataset as a dataframe\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.shape, test.shape","1b2be1b5":"sns.set_style(\"whitegrid\")\nmissing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()","6fff66eb":"missing = ((len(train) - train.count())\/len(train)).to_frame(name='%missing_values')\nmissing[missing['%missing_values']>0].sort_values(by='%missing_values', ascending=False)","b4d09a8f":"drop_cols = missing[missing['%missing_values']>0.7].index\ntrain = train.drop(drop_cols, axis=1)","b4211549":"corr = train.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)","ec8db4c4":"corr[\"SalePrice\"].sort_values(key=abs, ascending=False)","03e42087":"from pandas_profiling import ProfileReport\nprofile = ProfileReport(train, title=\"Advanced Housing Report\", explorative=True)","85857886":"ProfileReport(train, title=\"Advanced Housing Report\", explorative=True)","d76ad8b8":"from IPython.display import IFrame\nSAVE_HTML_FILE = False\nif SAVE_HTML_FILE:\n    # save the html file\n    profile.to_file(\"train_report.html\")\n    # load the html file:\n    IFrame(src='train_report.html', width=1200, height=600)","2736fdb5":"X = train.drop(['SalePrice'], axis=1)\ny = train.SalePrice\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=75)\n","9f7a7206":"# first separate num vars and cat vars\n\ncats = list(dict(filter(lambda x: x[1]=='object', X_train.dtypes.iteritems())).keys())\nnums = [var for var in X_train.columns if var not in cats]","3eace710":"from sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass CustomizedImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, num_vars, cat_vars): \n        self._num_vars = num_vars\n        self._cat_vars = cat_vars\n        # initializa categorical imputer and numerical imputer\n        self.num_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n        self.cat_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n        \n    def fit(self, X, y=None):\n        # fit th numeric imputer on the numerical features\n        self.num_imputer.fit(X[self._num_vars])\n        # fit the categoric imputer on the categorical features\n        self.cat_imputer.fit(X[self._cat_vars])\n        return self\n\n    \n    def transform(self, X, y=None):\n        # stock the initial order of the columns\n        initial_cols_order = list(X.columns)\n        # transform the dataframe subset composed of numerical features\n        X_num = self.num_imputer.transform(X[self._num_vars])\n        # transform the dataframe subset composed of categorical features\n        X_cat = self.cat_imputer.transform(X[self._cat_vars])\n        X = pd.concat([pd.DataFrame(X_num, columns=self._num_vars), \n                       pd.DataFrame(X_cat, columns=self._cat_vars)], \n                      axis=1)\n        return X[initial_cols_order]\n    \n    \n","1b6689cf":"my_imputer = CustomizedImputer(nums, cats)\n\nX_train = my_imputer.fit_transform(X_train)\nX_test = my_imputer.transform(X_test)","f5ad95e7":"unique_values = pd.Series(index=cats, dtype='float')\nfor cat_var in cats:\n    unique_values[cat_var] = train[cat_var].nunique()\nunique_values","8d86845c":"from sklearn.preprocessing import OneHotEncoder\n# create the ohe\nohe_enc = OneHotEncoder(handle_unknown='ignore')\n#fit transform the encoder\nohe_frame = ohe_enc.fit_transform(X_train[cats])\n\n# retrieve the columns name\n# each category will be suffixed with its modalities\nnew_cat_columns = []\nfor cat_var, cat_var_list in zip(cats, ohe_enc.categories_):\n    new_cat_columns = new_cat_columns + [f\"{cat_var}_{x.replace(' ', '')}\" for x in cat_var_list ]\n    \n# transform the sparse matrix to a dataframe with efficient columns names\nohe_frame = pd.DataFrame(ohe_frame.toarray(), columns=new_cat_columns)\n\n# now drop the categorical variables\nX_train = X_train.drop(cats, axis=1)\n# append the ohencoded dataframe to the training dataset\nX_train = pd.concat([X_train, ohe_frame], axis=1)\n\n","c7a7c85f":"# do the same to the test subset but now we will apply the pre trained encoder\nohe_frame = ohe_enc.transform(X_test[cats])\n# transform the sparse matrix to a dataframe with the same as the training set\nohe_frame = pd.DataFrame(ohe_frame.toarray(), columns=new_cat_columns)\n# now drop the categorical variables\nX_test = X_test.drop(cats, axis=1)\n# append the ohencoded dataframe to the test dataset\nX_test = pd.concat([X_test, ohe_frame], axis=1)","ae6ee9a7":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.1)\nmy_model.fit(X_train, y_train, early_stopping_rounds=5,\n             eval_set=[(X_test, y_test)], verbose=False)","d6a74f20":"from sklearn.metrics import mean_absolute_error, r2_score\n\npredictions = my_model.predict(X_test)\nprint(\"Root Mean Absolute Error : \" + str(np.sqrt(mean_absolute_error(predictions, y_test))))\n\nprint(\"R2 Score : \" + str(r2_score(y_test, predictions)))","7c52f9a7":"result = pd.DataFrame(columns=['Id', 'pred', 'real'])\nresult['Id'] = X_test['Id'].astype(int).values\nresult['pred'] = predictions\nresult['real'] = y_test.values","963e2a51":"result.head(10)","cc8a4af5":"=> we have an excellent R2 Score > 0.85","6c43c85c":"let's evaluate the RMSE on the holdout dataset","bc79b960":"### Save","3cb39e6a":"Or we can to generate a HTML report file and save it for later usage","2e0291aa":"### Train\/test split\n\nBefore applying, the encoding we will keep aside a test subset to test the generalisation error","1db03810":"as we don't have large cardinalities, it is simplier to apply  one hot encoding","617f37bc":"### Correlation analysis","676d6ea1":"## Numerical features : \nAs we will use **Gradient Boosted Decision Trees** based algorithms we don't have to apply the scaling on the numerical features\n\nCheck out this excellent [post](https:\/\/towardsdatascience.com\/all-about-feature-scaling-bcc0ad75cb35) that specify which types of algorithms you should\/should not apply the scaling: \n\n> Algorithms that do not require normalization\/scaling are the ones that rely on rules. They would not be affected by any monotonic transformations of the variables. Scaling is a monotonic transformation. Examples of algorithms in this category are all the tree-based algorithms \u2014 CART, Random Forests, Gradient Boosted Decision Trees. These algorithms utilize rules (series of inequalities) and do not require normalization.","5b84f872":"## Read Data","2c1a603f":"Let's define our customized imputer based on `sklearn.impute.SimpleImputer`: \n* `mean` strategy with numerical vars\n* `most_frequent` strategy with categorical vars","b12dacb4":"we will use `pandas_profiling` to generate profile reports from a pandas DataFrame : which is a deeper EDA than the df.describe() function","d8239549":"We will drop all vars that with %missing_values > 0.7","48bbe874":"## EDA","5b30cf15":"### Modeling\n\nWe will use Xgboost","657e0918":"We notice that the top 3 features correlated with the target are:\n* **OverallQual** : we assume that house's price is proportional to the overall quality of the house \n* **GrLivArea** : the more spacious the house, the higher the price\n* **GarageCars** : if the garage house can care more than one car, we assume that the price is higher because it would increase the house's area which is highly correlated with the price (see the correlation matrix heatmap above)","f117ba9f":"### Missing values","de2c2180":"## Categoricals\nfor each categorical feature, we will check the number of unique values so that we can chose the best way to encode them"}}