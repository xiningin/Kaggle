{"cell_type":{"e96fd8b0":"code","47cf9a05":"code","237dc8d6":"code","319451fe":"code","26fee2b4":"code","c99485f8":"code","68929b52":"code","65e95aee":"code","afe805ea":"code","d63b5bd3":"code","a1288115":"code","289e61c8":"code","8cddb97b":"code","06ad959f":"code","0f16407d":"code","57b284ff":"code","15604174":"code","bead4949":"code","9b940b18":"code","fbe95049":"code","5c63605c":"code","7eaccc90":"code","d30353d2":"code","5d5fd78d":"code","f7413a34":"code","6bed17d5":"code","fc61678f":"code","a0500ae2":"markdown","e7d22d17":"markdown","a7f5bc81":"markdown","0f21f935":"markdown","ff8f5d5a":"markdown","9dffc3bf":"markdown","779a0557":"markdown"},"source":{"e96fd8b0":"import nltk","47cf9a05":"myfile = open(\"..\/input\/bible.txt\", encoding=\"utf8\")\ntxt = myfile.read()\ntxt[:300]","237dc8d6":"txt_tkn = txt.split('\\n\\n')\ntxt_tkn = [tkn for tkn in txt_tkn if tkn != '']\ntxt_tokens = []\nfor verse in txt_tkn:\n    if '\\n' in verse:\n        verse = verse.replace('\\n', ' ')\n    txt_tokens.append(verse)\ntxt_tokens[:20]","319451fe":"tokenizer_spa=nltk.data.load('tokenizers\/punkt\/english.pickle')\n\ntokenizer_spa.tokenize(txt)[:20]","26fee2b4":"from nltk.tokenize import WordPunctTokenizer # TreebankWordTokenizer\ntokenizer=WordPunctTokenizer() # TreebankWordTokenizer\ntxt_wd = tokenizer.tokenize(txt)\ntxt_wd[:20]","c99485f8":"from nltk.tokenize import RegexpTokenizer\ntokenizer=RegexpTokenizer(\"[\\w]+\")\ntxt_words = tokenizer.tokenize(txt)\ntxt_words[:20]","68929b52":"from nltk.corpus import stopwords\nstops=set(stopwords.words('english'))\nstops","65e95aee":"# Getting out stopwords and change to lower case\ntext_out_stopw = [word.lower() for word in txt_words if word not in stops]\ntext_out_stopw[:20]","afe805ea":"freq = nltk.FreqDist(text_out_stopw)\nfreq","d63b5bd3":"sorted_by_value = sorted(freq.items(), key=lambda kv: kv[1], reverse=True)\nfor key,val in sorted_by_value:\n    print (str(key) + ':' + str(val))\n    if val < 1000: break","a1288115":"from nltk import ngrams","289e61c8":"n = 3\nn_grams = ngrams(text_out_stopw, n)\nn_grams_list = [grams for grams in n_grams]","8cddb97b":"from collections import defaultdict\n\nn_grams_freq = defaultdict(int)\n\nfor curr in n_grams_list:\n    n_grams_freq[curr] += 1\n\nn_grams_sorted_by_value = sorted(n_grams_freq.items(), key=lambda kv: kv[1], reverse=True)\n\nfor key,val in n_grams_sorted_by_value:\n    print (str(key) + ':' + str(val))\n    if val < 100: break","06ad959f":"from nltk.stem import SnowballStemmer\nstemmer = SnowballStemmer('english')","0f16407d":"stems = []\nfor word in text_out_stopw:\n    word = stemmer.stem(word)\n    if word != \"\":\n        stems.append(word)\nstems[:20]","57b284ff":"stems_freq = nltk.FreqDist(stems)\nstems_sorted_by_value = sorted(stems_freq.items(), key=lambda kv: kv[1], reverse=True)\nfor key,val in stems_sorted_by_value:\n    print (str(key) + ':' + str(val))\n    if val < 1000: break","15604174":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nbag_of_words = vectorizer.fit_transform(txt_tokens).todense()\nprint('Bag shape =',bag_of_words.shape, 'Bag type =', type(bag_of_words))","bead4949":"list(vectorizer.vocabulary_)[:50]","9b940b18":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(stop_words=stops)\ntfidf_matrix = tfidf_vectorizer.fit_transform(txt_tokens) \nprint(list(tfidf_vectorizer.vocabulary_)[:50])","fbe95049":"from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n\nlda_model = LatentDirichletAllocation(n_components=15,               # Number of topics\n                                      max_iter=10,               # Max learning iterations\n                                      learning_method='online',   \n                                      random_state=100,          # Random state\n                                      batch_size=128,            # n docs in each learning iter\n                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n                                     )\nlda_output = lda_model.fit_transform(tfidf_matrix)\n\nprint(lda_model)  # Model attributes","5c63605c":"from sklearn.model_selection import GridSearchCV\n\nlda_test_model = LatentDirichletAllocation()\nparam_grid = {'n_components': [5, 10, 15, 20], 'max_iter': [5, 10, 15, 20],\n              'learning_method': ['batch', 'online'], 'random_state': [50, 100, 200]}\n\ngrid_search = GridSearchCV(lda_test_model, param_grid, cv=5)\n\ngrid_search.fit(tfidf_matrix)","7eaccc90":"def print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"Topic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n        \ntf_feature_names = tfidf_vectorizer.get_feature_names()\n\nn_top_words = 10\n\nprint_top_words(lda_model, tf_feature_names, n_top_words)","d30353d2":"from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nclusters = KMeans(n_clusters=6, random_state=100).fit_predict(lda_output)\n\nsvd_model = TruncatedSVD(n_components=2)\nlda_output_svd = svd_model.fit_transform(lda_output)\n\nx = lda_output_svd[:, 0]\ny = lda_output_svd[:, 1]\n\nplt.figure(figsize=(12,12))\nplt.scatter(x,y,c=clusters)\nplt.ylabel('Component 2')\nplt.xlabel('Component 1')\nplt.title('Topic segregation',)","5d5fd78d":"from textblob import TextBlob","f7413a34":"count = 1\npolarity_list = []\nsub_list = []\nnum_tkn = []\n\nfor num, token in enumerate(txt_tokens):\n        analysis = TextBlob(token)\n        num_tkn.append(num)\n        sub_list.append(analysis.subjectivity)\n        polarity_list.append(analysis.polarity)","6bed17d5":"plt.figure(figsize=(15,15))\nplt.scatter(num_tkn, polarity_list)\nplt.title(\"Sentiment on Bible's verses\")\nplt.xlabel(\"Token\")\nplt.ylabel(\"Polarity\")  ","fc61678f":"plt.figure(figsize=(15,15))\nplt.scatter(num_tkn[:500], sub_list[:500])\nplt.title(\"Sentiment on Bible's verses\")\nplt.xlabel(\"Token\")\nplt.ylabel(\"Subjectivity\")  ","a0500ae2":"## Bag of words","e7d22d17":"## Stopwords","a7f5bc81":"## Tf-idf","0f21f935":"## Stemmer","ff8f5d5a":"## Sentiment Analysis","9dffc3bf":"## N-gramas","779a0557":"## Word Frequency"}}