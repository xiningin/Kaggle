{"cell_type":{"3fb1ccc8":"code","c6c7351f":"code","2d8cd9e9":"code","b4938a82":"code","282d7a21":"code","2bd5e9a4":"code","8b4d1e08":"code","8e73159c":"code","01341087":"code","57edb271":"code","2c702d92":"code","4b805eb7":"code","e92f10a6":"code","5386407e":"code","58ce14e7":"code","1e53d603":"code","323cf880":"markdown","43bd95dc":"markdown","317cbc75":"markdown","2461d93d":"markdown","0a4ede85":"markdown","66b151c9":"markdown","9b05c69c":"markdown","66937589":"markdown","f66a31a7":"markdown","8b0abb73":"markdown","291ca5e3":"markdown","0b784f01":"markdown","f08897bc":"markdown"},"source":{"3fb1ccc8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6c7351f":"import torch as t\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d","2d8cd9e9":"class plot_error(object):\n    \n    # Constructor\n    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n        W = np.linspace(-w_range, w_range, n_samples)\n        B = np.linspace(-b_range, b_range, n_samples)\n        w, b = np.meshgrid(W, B)    \n        Z = np.zeros((30, 30))\n        count1 = 0\n        self.y = Y.numpy()\n        self.x = X.numpy()\n        for w1, b1 in zip(w, b):\n            count2 = 0\n            for w2, b2 in zip(w1, b1):\n                Z[count1, count2] = np.mean((self.y - w2 * self.x + b2) ** 2)\n                count2 += 1\n            count1 += 1\n        self.Z = Z\n        self.w = w\n        self.b = b\n        self.W = []\n        self.B = []\n        self.LOSS = []\n        self.n = 0\n        if go == True:\n            plt.figure()\n            plt.figure(figsize = (10, 7.5))\n            plt.axes(projection = '3d').plot_surface(self.w, self.b, self.Z, rstride = 1, cstride = 1,cmap = 'viridis', edgecolor = 'none')\n            plt.title('Cost Function')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.show()\n            plt.figure()\n            plt.title('Contour Plot')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.contour(self.w, self.b, self.Z)\n            plt.show()\n    \n    # Setter\n    def set_para_loss(self, W, B, loss):\n        self.n = self.n + 1\n        self.W.append(W)\n        self.B.append(B)\n        self.LOSS.append(loss)\n    \n    # Plot diagram\n    def final_plot(self): \n        ax = plt.axes(projection = '3d')\n        ax.plot_wireframe(self.w, self.b, self.Z)\n        ax.scatter(self.W, self.B, self.LOSS, c = 'orange', marker = 'o', s = 200, alpha = 1)\n        plt.figure()\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c = 'orange', marker = 'o')\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n    \n    # Plot diagram\n    def plot_ps(self):\n        plt.subplot(121)\n        plt.ylim\n        plt.plot(self.x, self.y, c = 'orange', marker = 'o', label = \"training points\")\n        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label = \"estimated line\")\n        plt.xlabel('x')\n        plt.ylabel('y')\n        plt.ylim((-30, 30))\n        plt.title('Iteration: ' + str(self.n))\n        plt.subplot(122)\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c = 'purple', marker = 'x')\n        plt.title('Iteration: ' + str(self.n))\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n        \n            ","b4938a82":"# setting random seed and adding some noise to the data\nt.manual_seed(1)\nX = t.arange(-3, 3, 0.1).view(-1, 1)\nf = 1 * X - 1\nY = f + 0.1 * t.randn(X.size())","282d7a21":"# Plotting out the data\nplt.plot(X.numpy(), Y.numpy(), 'ro')\nplt.plot(X.numpy(), f.numpy(), 'black')\nplt.show()","2bd5e9a4":"def prediction(x):\n    return x * w + b","8b4d1e08":"def criterion(ypred, y):\n    return t.mean((ypred - y)**2)","8e73159c":"get_surface = plot_error(15, 15, X, Y, 30)","01341087":"# Define the parameters w, b for y = wx + b,\nw = t.tensor(-15.0, requires_grad = True)\nb = t.tensor(-10.0, requires_grad = True)","57edb271":"lr = 0.05\nLOSS_GD = []","2c702d92":"def train_model(iter):\n    \n    for epoch in range(iter):\n        \n        # make a prediction\n        ypred = prediction(X)\n        \n        # calculate the error\n        loss = criterion(ypred, Y)\n        \n        # plotting\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.data.tolist())\n        get_surface.plot_ps()\n         \n        # storing the cost in a list\n        LOSS_GD.append(loss)\n        \n        # compute the gradient of the cost function with respect to all the parameters\n        loss.backward()\n        \n        # update the parameters (Batch Gradient Descent)\n        w.data = w.data - lr * w.grad.data\n        b.data = b.data - lr * b.grad.data\n        \n        # setting the gradients to 0 before running the backward pass again\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n\n\n        ","4b805eb7":"train_model(10)","e92f10a6":"get_surface = plot_error(15, 15, X, Y, 30, go = False)","5386407e":"# The function for training the model\n\nLOSS_SGD = []\nw = t.tensor(-15.0, requires_grad = True)\nb = t.tensor(-10.0, requires_grad = True)\nlr1 = 0.01\ndef train_model_SGD(iter):\n    \n    # Loop\n    for epoch in range(iter):\n        \n        # SGD is an approximation of out true total loss\/cost, in this line of code we calculate our true loss\/cost and store it\n        Yhat = prediction(X)\n\n        # store the loss \n        LOSS_SGD.append(criterion(Yhat, Y).tolist())\n        \n        for x, y in zip(X, Y):\n            \n            # make a pridiction\n            yhat = prediction(x)\n        \n            # calculate the loss \n            loss = criterion(yhat, y)\n\n            # Section for plotting\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n        \n            # backward pass: compute gradient of the loss with respect to all the learnable parameters\n            loss.backward()\n        \n            # update parameters slope and bias\n            w.data = w.data - lr1 * w.grad.data\n            b.data = b.data - lr1 * b.grad.data\n\n            # zero the gradients before running the backward pass\n            w.grad.data.zero_()\n            b.grad.data.zero_()\n            \n        #plot surface and data space after each epoch    \n        get_surface.plot_ps()\n    \n","58ce14e7":"# Training the model with 10 iterations\ntrain_model_SGD(10)","1e53d603":"plt.plot(LOSS_GD, label = 'Batch Gradient Descent')\nplt.plot(LOSS_SGD, label = 'Stochastic Gradient Descent')\nplt.xlabel('Epoch')\nplt.ylabel('Loss\/Cost')\nplt.legend()\nplt.show()","323cf880":"**- The Cost Function which measures the model's error (Mean Squared Error):**","43bd95dc":"*  **Setting the parameter requires_grad to \"True\" allows PyTorch to calculate the partial derivatives of the cost function with respect to the parameters.**","317cbc75":"* **Set the first learning rate to 0.1.**\n","2461d93d":"**- The prediction function:**","0a4ede85":"* **Creating the functions in order to train the linear model:**","66b151c9":"# Univariate Linear Regression with Plotting","9b05c69c":"* **Plotting a learning curve in order to see which one performed better.**","66937589":"* **Training the model with Stochastic Gradient Descent.**","f66a31a7":"* **Creating an object in order to visualize the parameter space during training.**","8b0abb73":"* **This class is just to help visualising the parameter space**","291ca5e3":"* **Creating data. Generating values between -3 and 3 that create a line with a slope of 1 and a bias of -1.**","0b784f01":"* **Creating an empty list for containing the loss after every iteration.**","f08897bc":"* **Creating a function to train our model using Batch Gradient Descent.**"}}