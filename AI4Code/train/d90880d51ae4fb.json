{"cell_type":{"7704c83d":"code","169141d7":"code","4948ebec":"code","4931a85a":"code","8a19f913":"code","dfd20b94":"code","91c7832e":"code","c24dbb3a":"code","ca205bf1":"code","e520046c":"code","2788132e":"code","f04ac5f1":"code","9f311e10":"code","b9f2c606":"code","422fc5a7":"code","ecd38d30":"code","4d60dde6":"code","2daf1ff3":"code","555b88d8":"code","f505c453":"code","ce6de935":"code","3f31dd5f":"code","12c1f93f":"code","6d588fc0":"code","2f0ed6c0":"code","25982196":"code","07b768dd":"code","4b0e3115":"code","8d31c92d":"code","95c13671":"code","36cfe45b":"code","24e9d6bf":"code","62f5e3fc":"code","cb83cc64":"code","0c61353f":"code","0a7fecdd":"markdown","33c89a61":"markdown","d05dc1fb":"markdown","57ed2fa4":"markdown","eb0b16c1":"markdown","74f0820d":"markdown","52407de3":"markdown","010b9b0c":"markdown","bf5e137e":"markdown","2e15ed31":"markdown","d31fd6d1":"markdown","210ca1d6":"markdown","73799802":"markdown","bfbfec2f":"markdown","57984cad":"markdown","a8026025":"markdown","a0f2ec27":"markdown","9815c9bc":"markdown","f716b3e5":"markdown","aa76a115":"markdown"},"source":{"7704c83d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_rows',1000)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold,ShuffleSplit,GridSearchCV,StratifiedShuffleSplit\nfrom sklearn.preprocessing import LabelEncoder\nfrom math import sqrt\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom rfpimp import *\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","169141d7":"df_train = pd.read_csv('train.csv')\ndf_test = pd.read_csv('test.csv')\n","4948ebec":"final = pd.concat([df_train,df_test])","4931a85a":"final['Avg_Dose_Insect'] = final['Estimated_Insects_Count']\/final['Number_Doses_Week']\nfinal['TotalDoses'] = final['Number_Doses_Week'] * final['Number_Weeks_Used']\n# final['Doses_Week_Quit'] = final['Number_Doses_Week'] + final['Number_Weeks_Quit']\nfinal['Doses_Week_Used_Quit'] = final['Number_Weeks_Used'] + final['Number_Weeks_Quit']\nfinal['Doses_Week_Used_Quit_Divide'] = final['Number_Weeks_Used'] \/ final['Number_Weeks_Quit']","8a19f913":"final = final.sort_values(by='ID')\nfinal.reset_index(inplace=True,drop=True)","dfd20b94":"from collections import defaultdict\ndict_groups = defaultdict()","91c7832e":"res = [[]]\ncounter = 0\nfor i, j in zip(final['Number_Doses_Week'].tolist(),final['Number_Doses_Week'].tolist()[1:]):\n    if i <= j or i-2<=j:\n        res[-1].append(final.iloc[counter]['ID'])\n        counter += 1\n    else:\n        res.append([final.iloc[counter]['ID']]) \n        counter += 1\n#There was a bug in the above code so it doesn't take ,last ids so manually append for doses and \nres[21640].append('F00155945')","c24dbb3a":"def get_proper_groups(res):\n    for i in range(len(res)):\n        if i == 0:\n            continue\n        else:\n            res[i-1].append(res[i][0])\n            res[i].remove(res[i][0])\n    dict_groups = defaultdict()\n    for i in range(len(res)):\n        dict_groups['group_{}'.format(i)] = res[i]\n    return dict_groups","ca205bf1":"final_groups = get_proper_groups(res)\n#The above function is also there to correct some bugs in the code to get the groups","e520046c":"import pickle\nwith open('groups_data_doses.pkl','rb') as f:\n    groups_doses = pickle.load(f)\nimport pickle\nwith open('groups_data.pkl','rb') as f:\n    final_groups = pickle.load(f)\n#Load the pickled files","2788132e":"df_groups = pd.DataFrame(final_groups.items(),columns=['Groups','ID'])\ndf_doses =pd.DataFrame(groups_doses.items(),columns=['Groups_Doses','ID'])","f04ac5f1":"s = df_groups.apply(lambda x:pd.Series(x['ID']),axis=1).stack().reset_index(level=1, drop=True)\ns.name = 'ID'\ndf_groups = df_groups.drop('ID',axis=1).join(s)\ns = df_doses.apply(lambda x:pd.Series(x['ID']),axis=1).stack().reset_index(level=1, drop=True)\ns.name = 'ID'\ndf_doses = df_doses.drop('ID',axis=1).join(s)\nfinal = pd.merge(final,df_groups,on='ID')\nfinal = pd.merge(final,df_doses,on='ID')\n#This where I merge the groups","9f311e10":"def get_start_cycle(x):\n    x = x.tolist()\n    return x[0]\ndef get_end_cycle(x):\n    x = x.tolist()\n    return x[-1]","b9f2c606":"temp = final.groupby(['Groups'])['ID'].agg(get_start_cycle)\ntemp = temp.reset_index()\ntemp['Start_Sequence_Insect'] = 1\nfinal = pd.merge(final,temp[['ID','Start_Sequence_Insect']],on='ID',how='left')\ntemp = final.groupby(['Groups'])['ID'].agg(get_end_cycle)\ntemp = temp.reset_index()\ntemp['End_Sequence_Insect'] = 1\nfinal = pd.merge(final,temp[['ID','End_Sequence_Insect']],on='ID',how='left')\ntemp = final.groupby(['Groups_Doses'])['ID'].agg(get_start_cycle)\ntemp = temp.reset_index()\ntemp['Start_Sequence_Doses'] = 1\nfinal = pd.merge(final,temp[['ID','Start_Sequence_Doses']],on='ID',how='left')\ntemp = final.groupby(['Groups_Doses'])['ID'].agg(get_end_cycle)\ntemp = temp.reset_index()\ntemp['End_Sequence_Insect_Doses'] = 1\nfinal = pd.merge(final,temp[['ID','End_Sequence_Insect_Doses']],on='ID',how='left')","422fc5a7":"final['Start_Sequence_Insect'].fillna(0,inplace = True)\nfinal['End_Sequence_Insect'].fillna(0,inplace = True)\nfinal['Start_Sequence_Doses'].fillna(0,inplace = True)\nfinal['End_Sequence_Insect_Doses'].fillna(0,inplace = True)","ecd38d30":"def type_change_groups(x,groups):\n    list1 = []\n    for i in tqdm(range(len(final))):\n        if i != final.shape[0]-1 and final.iloc[i+1][x] != final.iloc[i][x] and final.iloc[i+1][groups] != final.iloc[i][groups]:\n            list1.append(1)\n        else:\n            list1.append(0)\n    return list1","4d60dde6":"temp = final.groupby(['ID','Groups']).agg({'ID':['count'],\n                                   'Number_Weeks_Quit':['sum','min','max','mean','std'],\n                                   'Number_Weeks_Used':['sum','min','max','mean','std'],\n                                           'Number_Doses_Week':['sum','min','max','mean','std'],\n                                   'TotalDoses':['sum','min','max','mean','std'],\n                                    'Doses_Week_Used_Quit':['sum','min','max','mean','std'],})\ntemp.columns = ['_ID_GROUP_COUNT_'.join(x) for x in temp.columns]\nfinal = pd.merge(final,temp,on=['ID','Groups'])","2daf1ff3":"temp = final.groupby(['ID','Groups_Doses']).agg({'ID':['count'],\n                                   'Number_Weeks_Quit':['sum','min','max','mean','std'],\n                                   'Number_Weeks_Used':['sum','min','max','mean','std'],\n                                           'Number_Doses_Week':['sum','min','max','mean','std'],\n                                   'TotalDoses':['sum','min','max','mean','std'],\n                                    'Doses_Week_Used_Quit':['sum','min','max','mean','std'],})\ntemp.columns = ['_ID_Doses_COUNT_'.join(x) for x in temp.columns]\nfinal = pd.merge(final,temp,on=['ID','Groups_Doses'])","555b88d8":"temp = final.groupby(['Groups']).agg({'ID':['count'],})\ntemp.columns = ['_ONLY_GROUP_COUNT_'.join(x) for x in temp.columns]\nfinal = pd.merge(final,temp,on=['Groups'])","f505c453":"temp = final.groupby(['Groups_Doses']).agg({'ID':['count']})\ntemp.columns = ['_'.join(x) for x in temp.columns]\nfinal = pd.merge(final,temp,on=['Groups_Doses'])","ce6de935":"for i in tqdm(range(1,10)):\n    final[f'next_{i}_crop'] = final.groupby(['Groups'])['Crop_Damage'].apply(lambda x: x.shift(i).ffill())\n    final[f'previous_{i}_crop'] = final.groupby(['Groups'])['Crop_Damage'].apply(lambda x: x.shift(-i).bfill())\n    final[f'next_{i}_crop_Doses'] = final.groupby(['Groups_Doses'])['Crop_Damage'].apply(lambda x: x.shift(i).ffill())\n    final[f'previous_{i}_crop_Doses'] = final.groupby(['Groups_Doses'])['Crop_Damage'].apply(lambda x: x.shift(-i).bfill())\n    final[f'next_{i}_insects'] = final.groupby(['Groups'])['Estimated_Insects_Count'].apply(lambda x: x.shift(i).ffill())\n#     final[f'next_{i}_insects_dose'] = final.groupby(['Groups'])['Number_Doses_Week'].apply(lambda x: x.shift(i).ffill())","3f31dd5f":"for i in tqdm(range(1,10)):\n    final[f'next_{i}_Dose'] = final.groupby(['Groups_Doses'])['Number_Doses_Week'].apply(lambda x: x.shift(i).ffill())\n#     final[f'previous_{i}_Dose_insect'] = final.groupby(['Groups_Doses'])['Estimated_Insects_Count'].apply(lambda x: x.shift(i).ffill())\n    ","12c1f93f":"for i in tqdm(range(1,10)):\n    final[f'next_{i}_Soil'] = final.groupby(['Groups','Soil_Type'])['Crop_Damage'].apply(lambda x: x.shift(i).ffill())\n    final[f'next_{i}_Crop'] = final.groupby(['Groups','Crop_Type'])['Crop_Damage'].apply(lambda x: x.shift(i).ffill())\n    final[f'next_{i}_Pesticide'] = final.groupby(['Groups','Pesticide_Use_Category'])['Crop_Damage'].apply(lambda x: x.shift(i).ffill())\n    final[f'next_{i}_Season'] = final.groupby(['Groups','Season'])['Crop_Damage'].apply(lambda x: x.shift(i).ffill())\n    final[f'next_{i}_Soil_D'] = final.groupby(['Groups_Doses','Soil_Type'])['Crop_Damage'].apply(lambda x: x.shift(i).ffill())\n    final[f'next_{i}_Crop_D'] = final.groupby(['Groups_Doses','Crop_Type'])['Crop_Damage'].apply(lambda x: x.shift(i).ffill())\n    final[f'next_{i}_Pesticide_D'] = final.groupby(['Groups_Doses','Pesticide_Use_Category'])['Crop_Damage'].apply(lambda x: x.shift(i).ffill())\n    final[f'next_{i}_Season_D'] = final.groupby(['Groups_Doses','Season'])['Crop_Damage'].apply(lambda x: x.shift(i).ffill())\n    \n#     final[f'previous_{i}_Dose_insect'] = final.groupby(['Groups_Doses'])['Estimated_Insects_Count'].apply(lambda x: x.shift(i).ffill())\n    ","6d588fc0":"final['Soil_groups']=type_change_groups(x='Soil_Type',groups='Groups')\nfinal['Crop_groups']=type_change_groups(x='Crop_Type',groups='Groups')\nfinal['Season_groups']=type_change_groups(x='Season',groups='Groups')\nfinal['Pesticide_groups']=type_change_groups(x='Pesticide_Use_Category',groups='Groups')","2f0ed6c0":"final['Seq_Add_Doses'] = final['Start_Sequence_Doses'] + final['End_Sequence_Insect_Doses']\nfinal['Seq_Add_Insect'] = final['Start_Sequence_Insect'] + final['End_Sequence_Insect']","25982196":"final['Groups'] = pd.factorize(final.Groups)[0]\nfinal['Groups_Doses'] = pd.factorize(final.Groups_Doses)[0]","07b768dd":"df_train = final[final['Crop_Damage'].notnull()]\ndf_test = final[final['Crop_Damage'].isnull()]","4b0e3115":"df_test.drop(['Crop_Damage'],axis=1,inplace=True)","8d31c92d":"df_train.fillna(-9999,inplace=True)\ndf_test.fillna(-9999,inplace=True)","95c13671":"def expanding_mean(df1,df2,df_test=None,target=None,cols=None):\n    df_1 = df1.copy()\n    df_2 = df2.copy()\n    cumulative_sum = df_1.groupby(cols)[target].cumsum() - df_1[target]\n    cumulative_count = df_1.groupby(cols).cumcount()\n    df_1[cols + \"_mean_target\"] = cumulative_sum\/cumulative_count\n    vals =df_1.groupby(cols).agg({target:['mean']})\n    vals.columns = [x[0] for x in vals.columns]\n    vals.rename(columns={target:cols+'_mean_target'},inplace=True)\n    df_2 = pd.merge(df_2,vals,on=cols,how='left')\n    df_1.fillna(df_1[cols + \"_mean_target\"].mean(),inplace=True)\n    df_2.fillna(df_2[cols + \"_mean_target\"].mean(),inplace=True)\n    df_1.drop([cols],axis=1,inplace=True)\n    df_2.drop([cols],axis=1,inplace=True)\n    return df_1,df_2,vals","36cfe45b":"X_train = df_train.drop(['ID'],axis=1)\ny_train=df_train['Crop_Damage']\nX_train,_,vals = expanding_mean(X_train,X_train,target='Crop_Damage',cols='Groups')df_test_id = df_test.copy()\ndf_test_id = df_test.copy()\ndf_test.drop('ID',axis=1,inplace=True)\ndf_test_id.reset_index(inplace=True)\nX_train.drop('Crop_Damage',axis=1,inplace=True)\ndf_test = pd.merge(df_test,vals,on='Groups',how='left')","24e9d6bf":"splits = 15\nfolds =StratifiedKFold(n_splits=splits,random_state=22,shuffle=True)\n# predictions = np.zeros((len(X_valid), 1))\noof_preds = np.zeros((len(df_test), 3))\nfeature_importance_df = pd.DataFrame()\nfeature_importance_df['Feature'] = X_train.columns\nfinal_preds = []\nrandom_state = [77,89,22,1007,1997,1890,2000,2020,8989,786,787,1999992,2021,7654]\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train,y_train)):\n        print(\"Fold {}\".format(fold_))\n        \n        X_trn,y_trn = X_train[cols].iloc[trn_idx],y_train.iloc[trn_idx]\n        X_val,y_val = X_train[cols].iloc[val_idx],y_train.iloc[val_idx]\n        clf = lgb.LGBMClassifier(random_state=22,n_jobs=-1,n_estimators=1000)\n        clf.fit(X_trn, y_trn,eval_set=[(X_val,y_val)],eval_metric='multi_logloss',verbose=100, early_stopping_rounds=100)\n        final_preds.append(accuracy_score(y_pred=clf.predict(X_val),y_true=y_val))\n#         imp = importances(clf,X_val,y_val)\n#         imp.rename(columns={\n#             'Importance':f'Importance_{fold_}'\n#         },inplace=True)\n#         feature_importance_df = pd.merge(feature_importance_df,imp,on='Feature')\n#         predictions += np.abs(clf.predict(X_valid)).reshape(-1,1)\n        oof_preds += clf.predict_proba(df_test[cols])\nprint(sum(final_preds)\/splits)\nprint(final_preds)","62f5e3fc":"df_sub = pd.read_csv('sample_submission.csv')","cb83cc64":"df_sub['Crop_Damage'] = [np.argmax(x) for x in fin_preds]\ndf_sub['ID'] = df_test_id['ID']","0c61353f":"df_sub.to_csv('Final.csv',index=False)","0a7fecdd":"The below code will create groups\/clusters on the basis of IDs so later you can join it properly","33c89a61":"The first features , are trying to proabably guess a sequence of what will be the next crop_damage in a certain group","d05dc1fb":"This wasnt you average time series competition, and i kinda didnt treat it as one it was more of a sequence prediction one in certain groups, basically if you can tell the model how is the sequence like and include more pattern recognition features you could improve scores,\nthis was fun if you figure out the pattern which i did and i loved it as i added more features and it boosted my cv","57ed2fa4":"So, Since this competition was a repeat of a previous comp,I still tried my best to get the best scores ","eb0b16c1":"Below features are the most exciting ones as they tell you,whats the next crop damage in a certain group or a previous damage in a certain group its quite intutive , basically in a time series you it helps if you can sort of get an idea about that,or a next insect in a certain group this boosted my cv a lot","74f0820d":"This Basic FE gets me to 84.7xx","52407de3":"Since , creating the dictionary of those groups take time,i suggest just save it as a pickle","010b9b0c":"This is basically the function above i told you","bf5e137e":"this is doing the same thing as above , but in a very different way i.e by taking and group, and the characteristics of that group,now this didnt help that much,\nbut was the part of the FE ","2e15ed31":"The below funtions help me to get features like,when and if the soil type,crop types changes after certain group,i create a binary feature on the basis of that","d31fd6d1":"   The Above loop had some bugs so i had to create a function to remove those bugs, you can optimize it the way you want","210ca1d6":"The below function does the expanded mean encoding simply of the group variable seems to increase my cv,now this meamy n encoding is a bit irregular but is the most stable of all it is also used by catboost internally, this function like my notebook is a bit poorly coded (I m working on my coding a bit)","73799802":"Creating Some Groups","bfbfec2f":"Finally i just took my oof predictions as my final submission probably a bad idea to use a stratified kfold, and just better go with a train test split , as i saw in my submissions because my public LB is 96.xx and Private is 95.84xx, just didnt got the time to experiment with that.","57984cad":"Some stats based FEATURES","a8026025":"Filling nulls","a0f2ec27":"More FE","9815c9bc":"Creating some features on groups,i.e basically when does a group start or end etc, ie when does a cluster start or a cluster end","f716b3e5":"Converting dictionaries to dataframe,the merging with original data, now we have our two groups","aa76a115":"But after seeing LB just 3 hrs after the comp started i saw people getting 96.xx so i thought well i should see some eda and after seeing eda i still couldnt figure out anything,so i just saw the data and saw that the insect data was in a sequence of some sorts, so I did create groups based on increasing order of insects, then after that my lb increased to 88, then i tried to see more such Patterns and then i found the same patter in the Doses Column"}}