{"cell_type":{"dbd4fb0b":"code","846f440a":"code","06bc44d8":"code","fc390b28":"code","a5653e52":"code","2ef251d1":"code","3d597c7e":"code","936d48d7":"code","9901c06e":"code","8aa5aa10":"code","e74e6848":"code","02c4c8c3":"code","9d2e4244":"code","86aa5488":"code","df24c1fe":"code","020d846e":"code","1741563f":"code","c0a771c6":"code","f60d7248":"code","246a64bc":"code","f2991eb0":"code","134365c1":"code","b2b242db":"code","0abc903e":"markdown","506d89e9":"markdown","7ff22353":"markdown","c9b68b79":"markdown","16d400f6":"markdown","c5347788":"markdown","35436d8e":"markdown","e026db57":"markdown","3cafcb73":"markdown","33c03fe7":"markdown"},"source":{"dbd4fb0b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","846f440a":"import torch\nfrom torch import nn","06bc44d8":"import math\nimport matplotlib.pyplot as plt","fc390b28":"import torchvision\nimport torchvision.transforms as transforms","a5653e52":"torch.manual_seed(111)","2ef251d1":"device = \"\"\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")","3d597c7e":"print(device)","936d48d7":"transform = transforms.Compose(\n    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n)","9901c06e":"train_set = torchvision.datasets.MNIST(root='.',train=True,download=True,transform=transform)","8aa5aa10":"batch_size=32\n\ntrain_loader=torch.utils.data.DataLoader(train_set,batch_size=batch_size,shuffle=True)","e74e6848":"train_loader","02c4c8c3":"real_samples, mnist_labels = next(iter(train_loader))\nfor i in range(16):\n    ax = plt.subplot(4, 4, i + 1)\n    print(mnist_labels[i])\n    plt.imshow(real_samples[i].reshape(28, 28), cmap=\"gray_r\")\n    \n    plt.xticks([])\n    plt.yticks([])","9d2e4244":"class Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(784, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        x = x.view(x.size(0), 784)\n        output = self.model(x)\n        return output","86aa5488":"discriminator=Discriminator().to(device=device)","df24c1fe":"class Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(100, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 784),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        output = output.view(x.size(0), 1, 28, 28)\n        return output","020d846e":"generator=Generator().to(device=device)","1741563f":"lr = 0.0001\nnum_epochs = 50\nloss_function = nn.BCELoss()","c0a771c6":"optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\noptimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)","f60d7248":"for epoch in range(num_epochs):\n    for n,(real_samples,mnist_labels) in enumerate(train_loader):\n        \n        #Data for training the discriminator(real & latentspace samples)\n        real_samples=real_samples.to(device=device)\n        real_sample_labels=torch.ones((batch_size,1)).to(device=device)\n        \n        #Taking random 100 coefficients as samples\n        latent_space_samples=torch.randn((batch_size,100)).to(device=device)\n        \n        #Passing random points to the generator\n        generated_samples=generator(latent_space_samples)\n        #As they are generated samples, giving the labels as 0\n        generated_sample_labels=torch.zeros((batch_size,1)).to(device=device)\n        \n        #Creating all samples\n        all_samples=torch.cat((real_samples,generated_samples))\n        all_sample_labels=torch.cat((real_sample_labels,generated_sample_labels))\n        \n        \n        \n        #Training the discriminator\n        discriminator.zero_grad()   #in order to accumulate the gradients to zero at every step\n        output_discriminator=discriminator(all_samples)\n        loss_discriminator=loss_function(output_discriminator,all_sample_labels)\n        loss_discriminator.backward()\n        optimizer_discriminator.step()\n        \n        \n        #Data for training the generator\n        latent_space_samples=torch.randn((batch_size,100)).to(device=device)\n        \n        \n        #Training the generator\n        generator.zero_grad()\n        generated_samples=generator(latent_space_samples)\n        output_discriminator_generated=discriminator(generated_samples)\n        loss_generator=loss_function(output_discriminator_generated,real_sample_labels)\n        loss_generator.backward()\n        optimizer_generator.step()\n        \n        #To print the loss\n        if n==batch_size-1:\n            print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n            print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")\n            plt.show(epoch,loss_discriminator)\n            plt.show(epoch,loss_generator)\n        ","246a64bc":"latent_space_sample=torch.randn((batch_size,100)).to(device=device)\ngenerator_samples=generator(latent_space_sample)","f2991eb0":"type(generator_samples)","134365c1":"generated_samples = generator_samples.cpu().detach()\nfor i in range(16):\n    ax = plt.subplot(4, 4, i + 1)\n    plt.imshow(generated_samples[i].reshape(28, 28), cmap=\"gray_r\")\n    plt.xticks([])\n    plt.yticks([])","b2b242db":"len(generated_samples)","0abc903e":"**To perform the conversions to the data, we need to define the transforms**","506d89e9":"**Here we give the 100 dimensional data and the model() returns the 784 dimensional data and converting into 28x28 image**","7ff22353":"# Discriminator","c9b68b79":"# Generator","16d400f6":"**Load the train dataset**","c5347788":"# Prepare the training data","35436d8e":"1. **ToTensor() converts the data into tensor**\n2. **Normalize() subtracts the value of data by the first parameter and divide by the second parameter**","e026db57":"# Train the models","3cafcb73":"**In the forward method, we converts the image shape,by 32 \u00d7 1 \u00d7 28 \u00d7 28 into 32 \u00d7 784 where 32 is the batch size**","33c03fe7":"# Check samples generated by Generator"}}