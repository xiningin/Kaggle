{"cell_type":{"7f0a1d9f":"code","24cbd3b3":"code","2c5cdc53":"code","87ac8871":"code","90db143e":"code","9ebfdedb":"code","4a9ece29":"code","2732edaf":"code","950e942d":"code","0520be47":"code","b0adc452":"code","0907216a":"code","5dc73ec5":"code","b73c0d0a":"code","08854975":"code","31821bf2":"code","603454ff":"code","c3f7485f":"code","14112a32":"code","4b26e8be":"code","c25d97cc":"code","5f46a15c":"code","105a7320":"code","33c13e9c":"code","54f479cf":"code","f2cba51c":"code","a79d81f8":"code","9562e842":"code","b69afdfd":"code","b5da4b1f":"markdown","c4738e16":"markdown","26512549":"markdown","a02226a4":"markdown","99d5c9b4":"markdown","39fc5150":"markdown","9c63904d":"markdown","9ce04f36":"markdown","cebf31bb":"markdown","59b193c5":"markdown","90c070f8":"markdown"},"source":{"7f0a1d9f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","24cbd3b3":"# setting up default plotting parameters\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = [20.0, 7.0]\nplt.rcParams.update({'font.size': 22,})\n\nsns.set_palette('viridis')\nsns.set_style('white')\nsns.set_context('talk', font_scale=0.8)","2c5cdc53":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nprint('Train Shape: ', train.shape)\nprint('Test Shape: ', test.shape)\n\ntrain.head()","87ac8871":"# using seaborns countplot to show distribution of questions in dataset\nfig, ax = plt.subplots()\ng = sns.countplot(train.target, palette='viridis')\ng.set_xticklabels(['0', '1'])\ng.set_yticklabels([])\n\n# function to show values on bars\ndef show_values_on_bars(axs):\n    def _show_on_single_plot(ax):        \n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() \/ 2\n            _y = p.get_y() + p.get_height()\n            value = '{:.0f}'.format(p.get_height())\n            ax.text(_x, _y, value, ha=\"center\") \n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\nshow_values_on_bars(ax)\n\nsns.despine(left=True, bottom=True)\nplt.xlabel('')\nplt.ylabel('')\nplt.title('Distribution of Target', fontsize=30)\nplt.tick_params(axis='x', which='major', labelsize=15)\nplt.show()","90db143e":"# prepare for modeling\nX_train_df = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nX_test = test.drop(['id'], axis=1)\n\n# scaling data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train_df)\nX_test = scaler.transform(X_test)","9ebfdedb":"lr = LogisticRegression(solver='liblinear')\nrfc = RandomForestClassifier(n_estimators=100)\n\nlr_scores = cross_val_score(lr,\n                            X_train,\n                            y_train,\n                            cv=5,\n                            scoring='roc_auc')\nrfc_scores = cross_val_score(rfc, X_train, y_train, cv=5, scoring='roc_auc')\n\nprint('LR Scores: ', lr_scores)\nprint('RFC Scores: ', rfc_scores)","4a9ece29":"# checking which are the most important features\nfeature_importance = rfc.fit(X_train, y_train).feature_importances_\n\n# Make importances relative to max importance.\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\nsorted_idx = sorted_idx[-20:-1:1]\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_train_df.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Feature Importance', fontsize=30)\nplt.tick_params(axis='x', which='major', labelsize=15)\nsns.despine(left=True, bottom=True)\nplt.show()","2732edaf":"# check missing values\ntrain.isnull().any().any()","950e942d":"from sklearn import feature_selection\n\nsel = feature_selection.VarianceThreshold()\ntrain_variance = sel.fit_transform(train)\ntrain_variance.shape","0520be47":"# find correlations to target\ncorr_matrix = train.corr().abs()\n\nprint(corr_matrix['target'].sort_values(ascending=False).head(10))","b0adc452":"# Select upper triangle of correlation matrix\nmatrix = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nsns.heatmap(matrix)\nplt.show;","0907216a":"# Find index of feature columns with high correlation\nto_drop = [column for column in matrix.columns if any(matrix[column] > 0.50)]\nprint('Columns to drop: ' , (len(to_drop)))","5dc73ec5":"# feature extraction\nk_best = feature_selection.SelectKBest(score_func=feature_selection.f_classif, k=100)\n# fit on train set\nfit = k_best.fit(X_train, y_train)\n# transform train set\nunivariate_features = fit.transform(X_train)","b73c0d0a":"lr = LogisticRegression(solver='liblinear')\nrfc = RandomForestClassifier(n_estimators=100)\n\nlr_scores = cross_val_score(lr, univariate_features, y_train, cv=5, scoring='roc_auc')\nrfc_scores = cross_val_score(rfc, univariate_features, y_train, cv=5, scoring='roc_auc')\n\nprint('LR Scores: ', lr_scores)\nprint('RFC Scores: ', rfc_scores)","08854975":"# checking which are the most important features\nfeature_importance = rfc.fit(univariate_features, y_train).feature_importances_\n\n# Make importances relative to max importance.\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\nsorted_idx = sorted_idx[-20:-1:1]\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_train_df.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Feature Importance', fontsize=30)\nplt.tick_params(axis='x', which='major', labelsize=15)\nsns.despine(left=True, bottom=True)\nplt.show()","31821bf2":"# feature extraction\nrfe = feature_selection.RFE(lr, n_features_to_select=100)\n\n# fit on train set\nfit = rfe.fit(X_train, y_train)\n\n# transform train set\nrecursive_features = fit.transform(X_train)","603454ff":"lr = LogisticRegression(solver='liblinear')\nrfc = RandomForestClassifier(n_estimators=10)\n\nlr_scores = cross_val_score(lr, recursive_features, y_train, cv=5, scoring='roc_auc')\nrfc_scores = cross_val_score(rfc, recursive_features, y_train, cv=5, scoring='roc_auc')\n\nprint('LR Scores: ', lr_scores)\nprint('RFC Scores: ', rfc_scores)","c3f7485f":"# checking which are the most important features\nfeature_importance = rfc.fit(recursive_features, y_train).feature_importances_\n\n# Make importances relative to max importance.\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\nsorted_idx = sorted_idx[-20:-1:1]\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_train_df.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Feature Importance', fontsize=30)\nplt.tick_params(axis='x', which='major', labelsize=15)\nsns.despine(left=True, bottom=True)\nplt.show()","14112a32":"# feature extraction\nselect_model = feature_selection.SelectFromModel(lr)\n\n# fit on train set\nfit = select_model.fit(X_train, y_train)\n\n# transform train set\nmodel_features = fit.transform(X_train)","4b26e8be":"lr = LogisticRegression(solver='liblinear')\nrfc = RandomForestClassifier(n_estimators=100)\n\nlr_scores = cross_val_score(lr, model_features, y_train, cv=5, scoring='roc_auc')\nrfc_scores = cross_val_score(rfc, model_features, y_train, cv=5, scoring='roc_auc')\n\nprint('LR Scores: ', lr_scores)\nprint('RFC Scores: ', rfc_scores)","c25d97cc":"# checking which are the most important features\nfeature_importance = rfc.fit(model_features, y_train).feature_importances_\n\n# Make importances relative to max importance.\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\nsorted_idx = sorted_idx[-20:-1:1]\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_train_df.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Feature Importance', fontsize=30)\nplt.tick_params(axis='x', which='major', labelsize=15)\nsns.despine(left=True, bottom=True)\nplt.show()","5f46a15c":"from sklearn.decomposition import PCA\n# pca - keep 90% of variance\npca = PCA(0.90)\n\nprincipal_components = pca.fit_transform(X_train)\nprincipal_df = pd.DataFrame(data = principal_components)\nprincipal_df.shape","105a7320":"lr = LogisticRegression(solver='liblinear')\nrfc = RandomForestClassifier(n_estimators=100)\n\nlr_scores = cross_val_score(lr, principal_df, y_train, cv=5, scoring='roc_auc')\nrfc_scores = cross_val_score(rfc, principal_df, y_train, cv=5, scoring='roc_auc')\n\nprint('LR Scores: ', lr_scores)\nprint('RFC Scores: ', rfc_scores)","33c13e9c":"# pca keep 75% of variance\npca = PCA(0.75)\nprincipal_components = pca.fit_transform(X_train)\nprincipal_df = pd.DataFrame(data = principal_components)\nprincipal_df.shape","54f479cf":"lr = LogisticRegression(solver='liblinear')\nrfc = RandomForestClassifier(n_estimators=100)\n\nlr_scores = cross_val_score(lr, principal_df, y_train, cv=5, scoring='roc_auc')\nrfc_scores = cross_val_score(rfc, principal_df, y_train, cv=5, scoring='roc_auc')\n\nprint('LR Scores: ', lr_scores)\nprint('RFC Scores: ', rfc_scores)","f2cba51c":"# checking which are the most important features\nfeature_importance = rfc.fit(principal_df, y_train).feature_importances_\n\n# Make importances relative to max importance.\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\nsorted_idx = sorted_idx[-20:-1:1]\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_train_df.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Feature Importance', fontsize=30)\nplt.tick_params(axis='x', which='major', labelsize=15)\nsns.despine(left=True, bottom=True)\nplt.show()","a79d81f8":"# feature extraction\nrfe = feature_selection.RFE(lr, n_features_to_select=100)\n\n# fit on train set\nfit = rfe.fit(X_train, y_train)\n\n# transform train set\nrecursive_X_train = fit.transform(X_train)\nrecursive_X_test = fit.transform(X_test)\n\nlr = LogisticRegression(C=1, class_weight={1:0.6, 0:0.4}, penalty='l1', solver='liblinear')\nlr_scores = cross_val_score(lr, recursive_X_train, y_train, cv=5, scoring='roc_auc')\nlr_scores.mean()","9562e842":"predictions = lr.fit(recursive_X_train, y_train).predict_proba(recursive_X_test)","b69afdfd":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = predictions\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","b5da4b1f":"Logistic Regression Public Leaderboard Score: 0.667\n\n## 6. Feature selection using SelectFromModel\n\nLike recursive feature selection, sklearn's SelectFromModel is used with any estimator that has a coef_ or feature_importances_ attribute. It removes features with values below a set threshold.\n","c4738e16":"The competition description stated that our features are all continuous.  We can see from above there are no features with the same value in all columns, so we have no features to remove here.  We can revisit this technique later and consider removing features with low variance later.\n\n## 3.  Remove highly correlated features\n\nFeatures that are highly correlated or colinear can cause overfitting.  Here we will explore correlations among features.","26512549":"## 4.  Univariate Feature Selection\nWe can use sklearn's SelectKBest to select a number of features to keep.  This method uses statistical tests to select features having the highest correlation to the target.  Here we will keep the top 100 features.","a02226a4":"> Recursive feature selection appears to perform the best above.  We'll use our tuned hyperparameters from our [hyperparameter tuning kernel](https:\/\/www.kaggle.com\/tboyle10\/hyperparameter-tuning) and submit to the competition.","99d5c9b4":"Our cross validation scores are improved as compared to the baseline above, but we still can see variation in the scores which indicates overfitting.  \n\nLogistic Regression Public Leaderboard Score: 0.676\n\n## 5.  Recursive Feature Elimination\n\nRecursive feature selection works by eliminating the least important features. It continues recursively until the specified number of features is reached. Recursive elimination can be used with any model that assigns weights to features, either through coef_ or feature_importances_  \n  \nHere we will use Random Forest to select the 100 best features:","39fc5150":"## Baseline Models\nWe'll use logistic regression is a good baseline as it is fast to train and predict and scales well.  We'll also use random forest.  With  its attribute feature_importances_ we can get a sense of which features are most important.\n\nThe logistic regression baseline model scored 0.666 on the public leaderboard.","9c63904d":"The dataset has no missing values and therefore no features to remove at this step.\n\n## 2.  Remove features with low variance\nIn sklearn's feature selection module we find VarianceThreshold.  It removes all features whose variance doesn't meet some threshold.  By default it removes features with zero variance or features that have the same value for all samples.\n","9ce04f36":"# Feature Selection and Dimensionality Reduction\nAccording to [wikipedia](https:\/\/en.wikipedia.org\/wiki\/Feature_selection), \"feature selection is the process of selecting a subset of relevent features for use in model construction\".  In normal circumstances, domain knowledge plays an important role.  Unfortunately, here in the Don't Overfit II competition, we have a binary target and 300 continuous variables \"of mysterious origin\" which forces us to try automatic feature selection techniques.\n\nIn this kernel we will explore the following feature selection and dimensionality reduction techniques:\n\n1. Remove features with missing values\n2. Remove features with low variance\n3. Remove highly correlated features\n4. Univariate feature selection\n5. Recursive feature elimination\n6. Feature selection using SelectFromModel\n7. PCA","cebf31bb":"From the above correlation matrix we see that there are no highly correlated features in the dataset.  And even exploring correlation to target shows feature 33 with the highest correlation of only 0.37.","59b193c5":"While logistic regression is clearly the superior model, we can see that both modesl are likely overfitting from the fluctuating cross validation scores.  We can apply feature selection techniques to improve model performance.\n\n## 1.  Remove features with missing values\nThis one is pretty self explanatory.  First we check for missing values and then can remove columns exceeding a threshold we define.","90c070f8":"## 7. PCA\n\nPCA (Principle Component Analysis) is a dimensionality reduction technique that projects the data into a lower dimensional space.\nWhile there are many effective dimensionality reduction techniques, PCA is the only example we will explore here.\nPCA can be useful in many situations, but especially in cases with excessive multicollinearity or explanation of predictors is not a priority."}}