{"cell_type":{"988d2e2d":"code","369df8fd":"code","5d78e103":"code","161b0f04":"code","2e3b03d0":"code","22cb01bc":"code","4c235f5f":"code","892a2f58":"code","7c1a6cff":"code","f0ef4a0f":"code","8ac9653b":"code","1c704a74":"code","dedf972a":"code","5c35406c":"code","2ead4df3":"code","c7a2445a":"code","c18f6074":"code","877515e7":"code","e9b18cc1":"code","a613e338":"code","5f481ba7":"code","92995b3c":"code","0c525e5e":"code","f1b67e52":"code","f6ba05fd":"code","b3d5fef2":"code","9abebfb3":"code","f1cd5f04":"code","8ce2965a":"code","09b7661e":"code","422861d1":"code","1dc54369":"code","c7018cbb":"code","9e856103":"code","ed333d5e":"code","dfbc5d66":"code","a25fcb1b":"code","8326ce9d":"code","fe165a65":"code","0ea0ab0c":"code","d31ea660":"code","e081e3ca":"code","4986c51f":"code","9fcb1d5b":"markdown","c46ea811":"markdown","fbeb8023":"markdown","77f1e6be":"markdown","6825da19":"markdown","8e122f2c":"markdown","58fee7ee":"markdown","a46384bf":"markdown","182ff130":"markdown","fb0ecc10":"markdown","f90411e5":"markdown","8fec514e":"markdown","529067e5":"markdown","d1a35545":"markdown","73a0f992":"markdown","48833382":"markdown","2e924415":"markdown","8fe04d6d":"markdown","25e160ca":"markdown","c7777805":"markdown","1da47cda":"markdown","b0d9ca4d":"markdown","8cff355b":"markdown","f5b792c9":"markdown","126aacc5":"markdown","825941c7":"markdown","009367ca":"markdown","1f026921":"markdown","0be561c1":"markdown","7d6cc08e":"markdown","3e178b62":"markdown","5de13949":"markdown","eaad443f":"markdown","7d6ff7f8":"markdown"},"source":{"988d2e2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","369df8fd":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.set_option('display.max_rows',500)","5d78e103":"cc=pd.read_excel(os.path.join(dirname, filename))\ncc.head()","161b0f04":"cc.shape","2e3b03d0":"cc.info()","22cb01bc":"for i in cc.columns:\n    sns.distplot(cc[i])\n    plt.show()","4c235f5f":"cc.describe()","892a2f58":"cc.isnull().sum()","7c1a6cff":"q1=cc.quantile(0.25)\nq3=cc.quantile(0.75)\nIQR=q3-q1\ncwo=((cc.iloc[:] <(q1-1.5*IQR))|(cc.iloc[:]>(q3+1.5*IQR))).sum(axis=0)\nopdf=pd.DataFrame(cwo,index=cc.columns,columns=['No. of Outliers'])\nopdf['Percentage Outliers']=round(opdf['No. of Outliers']*100\/len(cc),2)\nopdf","f0ef4a0f":"rwo=(((cc[:]<(q1-1.5*IQR))|(cc[:]>(q3+1.5*IQR))).sum(axis=1))\nro005=(((rwo\/len(cc.columns))<0.05).sum())*100\/len(cc)\nro01=(((rwo\/len(cc.columns))<0.1).sum())*100\/len(cc)\nro015=(((rwo\/len(cc.columns))<0.15).sum())*100\/len(cc)\nro02=(((rwo\/len(cc.columns))<0.2).sum())*100\/len(cc)\nro025=(((rwo\/len(cc.columns))<0.25).sum())*100\/len(cc)\nro03=(((rwo\/len(cc.columns))<0.30).sum())*100\/len(cc)\nro035=(((rwo\/len(cc.columns))<=0.35).sum())*100\/len(cc)\nro04=(((rwo\/len(cc.columns))<=0.4).sum())*100\/len(cc)\nro045=(((rwo\/len(cc.columns))<=0.45).sum())*100\/len(cc)\nro05=(((rwo\/len(cc.columns))<=0.50).sum())*100\/len(cc)\nro055=(((rwo\/len(cc.columns))<0.55).sum())*100\/len(cc)\nro06=(((rwo\/len(cc.columns))<0.6+0).sum())*100\/len(cc)\nro=pd.DataFrame(np.round([ro005,ro01,ro015,ro02,ro025,ro03,ro035,ro04,ro045,ro05,ro055,ro06],2),\n             index=['5%','10%','15%','20%','25%','30%','35%','40%','45%','50%','55%','60%'],\n            columns=['% Data'])\nro.index.name='% Outlier'\nro","8ac9653b":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","1c704a74":"imp = IterativeImputer()\nimp.fit(cc)\ncc=pd.DataFrame(imp.transform(cc),columns=cc.columns)","dedf972a":"cc.isnull().sum()","5c35406c":"g=cc.groupby('Age (day)')\ng1=g.get_group(1)\ng3=g.get_group(3)\ng7=g.get_group(7)\ng14=g.get_group(14)\ng28=g.get_group(28)\npd.DataFrame(round(g28.iloc[:,-1].sort_values()).unique(),columns=['Comp Strength @ 28 days'])","2ead4df3":"cp = cc.corr()\nmask = np.zeros_like(cp)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(8,8))\nwith sns.axes_style(\"white\"):\n    sns.heatmap(cp,annot=True,linewidth=2,mask = mask,cmap=\"coolwarm\")\nplt.title(\"Correlation Plot\")\nplt.show()","c7a2445a":"import statsmodels.api as sm\nX=cc.iloc[:,:8]\nY=cc.iloc[:,8]","c18f6074":"ls=sm.OLS(Y,sm.add_constant(X))\nresults=ls.fit()\nresults.summary()","877515e7":"ls=sm.OLS(Y,X)\nresults=ls.fit()\nresults.summary()","e9b18cc1":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split","a613e338":"X_train,X_test,y_train,y_test = train_test_split(X, Y, random_state=150, test_size=0.3 )","5f481ba7":"lr=LinearRegression()\nlr.fit(X_train,y_train)\nprint('Score: ',lr.score(X_train,y_train))\ny_pred_lrtr=lr.predict(X_train)\ny_pred_lrte=lr.predict(X_test)\nfrom sklearn.metrics import r2_score\nprint('Train R2 score: ',r2_score(y_train,y_pred_lrtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_lrte))","92995b3c":"from sklearn.preprocessing import PolynomialFeatures\npf = PolynomialFeatures(degree = 2)\nX_polytr = pf.fit_transform(X_train)\nlr.fit(X_polytr,y_train)\ny_pred_lr2tr = lr.predict(X_polytr)\nprint(\"Training R2 - degree 2 polynomial: \",r2_score(y_train, y_pred_lr2tr ))\nX_polyte = pf.fit_transform(X_test)\ny_pred_lr2te= lr.predict(X_polyte)\nprint(\"Test R2 - degree 2 polynomial: \",r2_score(y_test,y_pred_lr2te))","0c525e5e":"pf = PolynomialFeatures(degree = 3)\nX_polytr = pf.fit_transform(X_train)\nlr.fit(X_polytr,y_train)\ny_pred_lr2tr = lr.predict(X_polytr)\nprint(\"Training R2 - degree 2 polynomial: \",r2_score(y_train, y_pred_lr2tr ))\nX_polyte = pf.fit_transform(X_test)\ny_pred_lr2te= lr.predict(X_polyte)\nprint(\"Test R2 - degree 2 polynomial: \",r2_score(y_test,y_pred_lr2te))","f1b67e52":"pf = PolynomialFeatures(degree = 4)\nX_polytr = pf.fit_transform(X_train)\nlr.fit(X_polytr,y_train)\ny_pred_lr2tr = lr.predict(X_polytr)\nprint(\"Training R2 - degree 2 polynomial: \",r2_score(y_train, y_pred_lr2tr ))\nX_polyte = pf.fit_transform(X_test)\ny_pred_lr2te= lr.predict(X_polyte)\nprint(\"Test R2 - degree 2 polynomial: \",r2_score(y_test,y_pred_lr2te))","f6ba05fd":"from sklearn.tree import DecisionTreeRegressor","b3d5fef2":"dt=DecisionTreeRegressor()\ndt.fit(X_train,y_train)\ndt.score(X_train,y_train)\ny_pred_dttr=dt.predict(X_train)\ny_pred_dtte=dt.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_dttr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_dtte))","9abebfb3":"from sklearn.model_selection import GridSearchCV\nparam_grid = {'max_depth': np.arange(3, 8),\n             'criterion' : ['mse','mae'],\n             'max_leaf_nodes': [5,10,20,100],\n             'min_samples_split': [2, 5, 10, 20]}\n\ngrid_tree = GridSearchCV(DecisionTreeRegressor(), param_grid, cv = 5, scoring= 'r2')\ngrid_tree.fit(X_train, y_train)\nprint(grid_tree.best_estimator_)\nprint(np.abs(grid_tree.best_score_))","f1cd5f04":"dtpr=DecisionTreeRegressor(criterion='mse', max_depth=7, max_features=None,\n                      max_leaf_nodes=100, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=10, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=None, splitter='best')\ndtpr.fit(X_train,y_train)\ndtpr.score(X_train,y_train)\ny_pred_dtprtr=dtpr.predict(X_train)\ny_pred_dtprte=dtpr.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_dtprtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_dtprte))","8ce2965a":"param_grid = {'max_depth': np.arange(3, 6),\n             'criterion' : ['mse','mae'],\n             'max_leaf_nodes': [100,105, 90,95],\n             'min_samples_split': [6,7,8,9,10],\n             'max_features':[2,3,4,5,6]}\n\ngrid_tree = GridSearchCV(DecisionTreeRegressor(), param_grid, cv = 5, scoring= 'r2')\ngrid_tree.fit(X_train, y_train)\nprint(grid_tree.best_estimator_)\nprint(np.abs(grid_tree.best_score_))","09b7661e":"dtpr=DecisionTreeRegressor(criterion='mae', max_depth=5, max_features=6,\n                      max_leaf_nodes=95, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=8, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=None, splitter='best')\ndtpr.fit(X_train,y_train)\ndtpr.score(X_train,y_train)\ny_pred_dtprtr=dtpr.predict(X_train)\ny_pred_dtprte=dtpr.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_dtprtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_dtprte))","422861d1":"from sklearn.ensemble import AdaBoostRegressor\nabr = AdaBoostRegressor(random_state=0, n_estimators=100)\nabr.fit(X_train, y_train)\nabr.feature_importances_  \nabr.fit(X_train,y_train)\nabr.score(X_train,y_train)\ny_pred_abrtr=abr.predict(X_train)\ny_pred_abrte=abr.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_abrtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_abrte))","1dc54369":"from sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor()\nrfr.fit(X_train,y_train)\nrfr.score(X_train,y_train)\ny_pred_rfrtr=rfr.predict(X_train)\ny_pred_rfrte=rfr.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_rfrtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_rfrte))","c7018cbb":"param_grid = {'max_depth': np.arange(3, 8),\n             'criterion' : ['mse','mae'],\n             'max_leaf_nodes': [100,105, 90,95],\n             'min_samples_split': [6,7,8,9,10],\n             'max_features':['auto','sqrt','log2']}\n\ngrid_tree = GridSearchCV(RandomForestRegressor(), param_grid, cv = 5, scoring= 'r2')\ngrid_tree.fit(X_train, y_train)\nprint(grid_tree.best_estimator_)\nprint(np.abs(grid_tree.best_score_))","9e856103":"rfr=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=7,\n                      max_features='auto', max_leaf_nodes=90,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=2, min_samples_split=7,\n                      min_weight_fraction_leaf=0.0, n_estimators=100,\n                      n_jobs=None, oob_score=False, random_state=None,\n                      verbose=0, warm_start=False)\nrfr.fit(X_train,y_train)\nrfr.score(X_train,y_train)\ny_pred_rfrtr=rfr.predict(X_train)\ny_pred_rfrte=rfr.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_rfrtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_rfrte))","ed333d5e":"from sklearn.ensemble import GradientBoostingRegressor\ngb=GradientBoostingRegressor()\ngb.fit(X_train,y_train)\ngb.score(X_train,y_train)\ny_pred_gbtr=gb.predict(X_train)\ny_pred_gbte=gb.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_gbtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_gbte))","dfbc5d66":"param_grid = {'n_estimators': [230],\n              'max_depth': range(10,31,2), \n              'min_samples_split': range(50,501,10), \n              'learning_rate':[0.2]}\nclf = GridSearchCV(GradientBoostingRegressor(random_state=1), \n                   param_grid = param_grid, scoring='r2', \n                   cv=5).fit(X_train, y_train)\nprint(clf.best_estimator_) \nprint(\"R Squared:\",clf.best_score_)","a25fcb1b":"gb=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n                          learning_rate=0.2, loss='ls', max_depth=14,\n                          max_features=None, max_leaf_nodes=None,\n                          min_impurity_decrease=0.0, min_impurity_split=None,\n                          min_samples_leaf=1, min_samples_split=150,\n                          min_weight_fraction_leaf=0.0, n_estimators=230,\n                          n_iter_no_change=None, presort='auto', random_state=1,\n                          subsample=1.0, tol=0.0001, validation_fraction=0.1,\n                          verbose=0, warm_start=False)\ngb.fit(X_train,y_train)\ngb.score(X_train,y_train)\ny_pred_gbtr=gb.predict(X_train)\ny_pred_gbte=gb.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_gbtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_gbte))","8326ce9d":"from xgboost import XGBRegressor\n\nxgb=XGBRegressor()\nxgb.fit(X_train,y_train)\nprint('Model Score: ', xgb.score(X_train,y_train))\ny_pred_xgbtr=xgb.predict(X_train)\ny_pred_xgbte=xgb.predict(X_test)\nprint('Train R2-Score: ', r2_score(y_train,y_pred_xgbtr))\nprint('Test R2-Score: ', r2_score(y_test,y_pred_xgbte))","fe165a65":"xgb=XGBRegressor(base_score=0.7, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=0.65, colsample_bytree=1, gamma=0.3,\n             importance_type='weight', learning_rate=0.2, max_delta_step=150,\n             max_depth=4, min_child_weight=0.5, missing=None, n_estimators=200,\n             n_jobs=1, nthread=None, objective='reg:squarederror', random_state=0,\n             reg_alpha=0.001, reg_lambda=1, scale_pos_weight=1, seed=None,\n             silent=None, subsample=1, verbosity=1)\nxgb.fit(X_train,y_train)\nprint('Model Score: ', xgb.score(X_train,y_train))\ny_pred_xgbtr=xgb.predict(X_train)\ny_pred_xgbte=xgb.predict(X_test)\nprint('Train R2-Score: ', r2_score(y_train,y_pred_xgbtr))\nprint('Test R2-Score: ', r2_score(y_test,y_pred_xgbte))","0ea0ab0c":"import shap","d31ea660":"explainer = shap.TreeExplainer(xgb)\nshap_values = explainer.shap_values(X_train)","e081e3ca":"for i in X_train.columns:\n    shap.dependence_plot(i,shap_values, X_train)","4986c51f":"shap.summary_plot(shap_values, X_train)","9fcb1d5b":"## Gradient Boosting","c46ea811":"More than 10% outliers of each row is not present. Hence the few outliers that will be treated using MICE (Multiple Imputation using Chained Equations) approach after these outliers are converted to NaN values.","fbeb8023":"## Decision Tree Regressor","77f1e6be":"Adaboost has reduced the variance and improved the model performance as well.","6825da19":"## RandomForest Regressor","8e122f2c":"* We can observe that there is a steady increase in the compressive strength of concrete with passing time.","58fee7ee":"We calculate the outliers in each columns.","a46384bf":"## AdaBoost Regressor","182ff130":"## EDA","fb0ecc10":"## Polynomial Regression - Degree 3","f90411e5":"Without the constant term we can observe that the R-squared value has increased drastically.","8fec514e":"Here we have achieved a model which performes well with both test and train data. This is very lightly overfit. It can be further adjusted. But this project will focus on the interpretability of the model.","529067e5":"The random forest is overfitting but has improved the model performance. So we now tune the hyper parameters to reduce the overfit.","d1a35545":"## Bivariate Analysis","73a0f992":"## Polynomial Regression - Degree 2","48833382":"This model is slightly overfit. XGBoost may or may not reduce it. Trying out XGBoost in the next step.","2e924415":"## Linear Regression - OLS","8fe04d6d":"Then we calculate the columns wise outliers. To determine in each row what is the presence of outliers.","25e160ca":"## Univariate Analysis","c7777805":"## Introduction:\n\n* The conventional process of testing the compressive strength of concrete involves casting several cubes for the respective grade (such as M5, M10, M15 etc.) and observing the strength of the concrete over a period of time ranging from 7 to 28 days. \n\n* Various combinations of the components of concrete are selected and cubes for each combination is casted and its test strength at 7, 14 and 28 days is noted dow.\n\n* This is a time consuming and rather tedious process. \n\n#### This project aims to predict the compressive strength of concrete with maximum accuracy, for various quantities of constituent components as the input.\n\n* The conrete cube exhibits behavioral differences in their compressive strengths for cubes that are cured\/not cured. Curing is the process of maintaining the moisture to ensure uninterrupted hydration of concrete.\n\n* The concrete strength increases if the concrete cubes are cured periodically. The rate of increase in strength is described here.\n\n|Time|% Of Total Strength Achieved|\n|---|---|\n|1 day|16%|\n|3 days|40%|\n|7 days|65%|\n|14 days|90%|\n|28 days|99%|\n\n* At 28 days, concrete achieves 99% of the strength. Thus usual measurements of strength are taken at 28 days.\n","1da47cda":"Here we can see that the constant term is having P value greater than 0.05 viz. the assumed level of significance, thus we remove the constant term from modelling","b0d9ca4d":"## Outlier Treatment","8cff355b":"1. From the first plot we can see that Cement content and presence of super plasticizer has a linear impact on the model. When the cement content is less than 300 compressive strength decreases. As the cement content increases beyond 300, the compressive strength increases as well. The compressive strength increases with higher content of super plasticizers.\n2. From the second plot we observe, when the blast furnace slag is greater than 50 kg\/m3, the comrpessive strength increases. This feature in combination with age is responsible for the compressive strength. \n3. From the third plot we observe, when there is no fly ash present, but the mix with highest content of superplasticizers have a positive impact on the compressive strength. However the fly ash has an increasing followed by decreasing trend with the compressive strength. When the fly ash is in the range of 75-150 kg\/m3 range, with super plasticizer content of 10 kg\/m3 leads to highest compressive strength observed. Similary in the range of fly ash > 150 kg\/m3 the least compressive strength was observed. \n4. From plot 4, we observe that water and blast furnace slag along with their interactive effect contributes to the compressive strength. Water content less than 150 kg\/m3 with lower blast furnac slag provides the highest compressive strength. For water content greater than 150 kg\/m3, higher blast furnace slag is preferred to have greater compressive strength.\n5. From plot 5, superplasticizers in the range of 10-12kg\/m3 along with higher blast furnace slag increases the compressive strength.\n6. Plot 6 suggests that, with increasing coarse aggregate, keeping the coment content lower, has positive impact on the compressive strength.\n7. For fine aggregates less than around 650 kg\/m3, the water content should be greater than 180 kg\/m3. Further, if the fine aggregate content is greater than 650 kg\/m3, water content should be lesser than 170 kg\/m3.\n8. From plot 8, with increasing age, the compressive strength surely increases, however different amount of water content surely has an effect with age. For 28 days strength, having water content greater than 300kg\/m3 is desired.","f5b792c9":"* Different types of concrete grades available and usually used are M7,M7.5,M10,M15,M20,M725,M30,M35,M40,M45,M50,M55,M60,M65,M70\n\n* It essentially means at 28 days time the compressive strength should be 7MPa for M7 and 70MPa for M70.\n\n* However we can see lots of grades of concrete. This could be due to variation in other contents of the concrete.","126aacc5":"## XGBoost Regressor","825941c7":"None of the features are highly inter correlated or correlated with the target variable.","009367ca":"It's severely overfit even now. We still have to prune it.","1f026921":"## Polynomial Regression - Degree 4","0be561c1":"Beyond this, the model does not perform well. From this it is clear that the model is non linear. Thus we proceed to other non-linear models.","7d6cc08e":"Thus we can see all outliers have been treated and removed by converting them to NaN values and imputing them using MICE.","3e178b62":"## Interpreting Black Box Models","5de13949":"The fully grown tree is overfitting. This can be controlled by pruning the tree. Using grid search we find the optimum depth and the impurity criterion and other hyper parameters.","eaad443f":"The overfit has reduced but the model performance has nt imporoved on the test data. So we  now move onto other models.","7d6ff7f8":"## SKLEARN - Linear Regression"}}