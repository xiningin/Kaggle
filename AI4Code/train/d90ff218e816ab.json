{"cell_type":{"5ed7e6d0":"code","66f80538":"code","4c08ad35":"code","4663a2a5":"code","49604cd7":"code","4270431b":"code","13639819":"code","3a725f47":"code","f8127bde":"code","32871311":"code","33b4c9d2":"code","11e6dc5b":"code","0955d541":"markdown","05cd448c":"markdown","4ef254ca":"markdown","ad4ece88":"markdown","dec6beb0":"markdown","8a290c54":"markdown","3fca1e2d":"markdown"},"source":{"5ed7e6d0":"# Making essential imports\nimport numpy as np\nimport pandas as pd \nimport seaborn as sb\nimport tensorflow as tf\nimport os\nimport matplotlib.pyplot as plt\n\n# printing directory contents\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print( 'Data path => '+ os.path.join(dirname, filename))\n\n\n# loading data\ndataFrame = pd.read_csv('\/kaggle\/input\/pulsar-dataset-htru2\/HTRU_2.csv')\ndataFrame.info()","66f80538":"maper = {\n    '140.5625' : 'Mean of integrated profile',\n    '55.68378214' : 'Standard Deviation of integrated profile',\n    '-0.234571412' : 'Excess kurtosis of the integrated profile',\n    '-0.699648398' : 'Skewness of the integrated profile',\n    '3.199832776'  : 'Mean of the DM-SNR curve',\n    '19.11042633'  : 'Standard deviation of the DM-SNR curve',\n    '7.975531794'  : 'Excess kurtosis of the DM-SNR curve',\n    '74.24222492'  :'Skewness of the DM-SNR curve',\n    '0' : 'Target'\n}\ndataFrame = dataFrame.rename(columns = maper )\ndataFrame[0:10]","4c08ad35":"from sklearn.preprocessing import StandardScaler\nfrom  sklearn.decomposition import PCA\n\nfeatures = list (dataFrame.columns[0:-1])\nfeatureVector = dataFrame[features]\ntargets = dataFrame['Target']\n\n# Scalling data\nstdSc = StandardScaler()\nfeatureVector = stdSc.fit_transform(featureVector)\n\n# pca clustering\npca = PCA(n_components = 2)\npca.fit(featureVector)\nfeatureVectorT = pca.transform(featureVector)\n\n#Converting to dataset\ndimReducedDataFrame = pd.DataFrame(featureVectorT)\ndimReducedDataFrame = dimReducedDataFrame.rename(columns = {0: 'V1', 1 : 'V2'})\ndimReducedDataFrame['Target'] = targets\n\n# Plotting\nplt.figure(figsize = (10, 7))\nsb.scatterplot(data = dimReducedDataFrame, x = 'V1', y = 'V2', hue = 'Target')\nplt.grid(True)\nplt.show()\n","4663a2a5":"from sklearn.model_selection import train_test_split\nxTrain, xTest, yTrain, yTest = train_test_split(featureVector, targets, test_size = 0.1, random_state = 42)\nm,n = xTrain.shape\n# defining model\nsimpleAnn = tf.keras.models.Sequential([\n    tf.keras.layers.Input(shape = (n, )),\n    tf.keras.layers.Dense(32, activation = 'relu'),\n    tf.keras.layers.Dense(64, activation = 'relu'),\n    tf.keras.layers.Dropout(0.07),\n    tf.keras.layers.Dense(128, activation = 'relu'),\n    tf.keras.layers.Dense(1, activation = 'sigmoid')\n])\n\nsimpleAnn.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy',\n                                                                              tf.keras.metrics.AUC()])","49604cd7":"retVal = simpleAnn.fit(xTrain, yTrain, validation_data = (xTest, yTest), batch_size = 100, epochs = 50, verbose = 1)","4270431b":"plt.plot(retVal.history['loss'], label = 'training loss')\nplt.plot(retVal.history['accuracy'], label = 'training accuracy')\nplt.plot(retVal.history['auc'], label = 'auc_train')\nplt.grid(True)\nplt.legend()\nplt.show()","13639819":"plt.plot(retVal.history['val_loss'], label = 'validation loss')\nplt.plot(retVal.history['val_accuracy'], label = 'validation accuracy')\nplt.plot(retVal.history['val_auc'], label = 'auc_validation')\nplt.grid(True)\nplt.legend()\nplt.show()","3a725f47":"# I am copy pasting the code, this is a bad practice avoid it at all cost\n# use functions, unless you want to save time\nfrom sklearn.model_selection import train_test_split\nxTrain1, xTest1, yTrain1, yTest1 = train_test_split(featureVectorT, targets, test_size = 0.1, random_state = 42)\nm,n = xTrain.shape\n# defining model\nsimpleAnnPCA = tf.keras.models.Sequential([\n    tf.keras.layers.Input(shape = (n, )),\n    tf.keras.layers.Dense(32, activation = 'relu'),\n    tf.keras.layers.Dense(64, activation = 'relu'),\n    tf.keras.layers.Dropout(0.07),\n    tf.keras.layers.Dense(128, activation = 'relu'),\n    tf.keras.layers.Dense(1, activation = 'sigmoid')\n])\n\nsimpleAnnPCA.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy',\n                                                                              tf.keras.metrics.AUC()])","f8127bde":"retVal = simpleAnnPCA.fit(xTrain, yTrain, validation_data = (xTest, yTest), batch_size = 100, epochs = 50, verbose = 1)","32871311":"inp1 = tf.keras.layers.Input(shape = ((featureVector.shape[1], )), name = 'inp1')\ninp2 = tf.keras.layers.Input(shape = ((featureVectorT.shape[1], )), name = 'inp2')\nD1 = tf.keras.layers.Dense(32, activation = 'relu')(inp1)\nD2 = tf.keras.layers.Dense(32, activation = 'relu')(inp2)\nConcLayer = tf.keras.layers.concatenate([D1, D2])\nD3 = tf.keras.layers.Dense(64, activation = 'relu')(ConcLayer)\nD4 = tf.keras.layers.Dense(64, activation = 'relu')(D3)\nD5 = tf.keras.layers.Dropout(0.07)(D4)\noutput = tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'opt')(D3)\n\nModel = tf.keras.Model(inputs = [inp1, inp2], outputs = [output])\nModel.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy', tf.keras.metrics.AUC()])\nModel.summary()\n","33b4c9d2":"import keras\nkeras.utils.plot_model(Model, \"MultiHeadModel.png\", show_shapes=True)","11e6dc5b":"# let train\nretVal = Model.fit({'inp1' : featureVector, 'inp2' : featureVectorT},  np.array(targets, dtype = 'int'), epochs = 50)","0955d541":"As You can see the multi-Head \/ ensembled network  works better than the other 2.\nHence Enmebling is the key to victory.\n\n\nThanks, if you Reached here, I hope this notebook was helpful.","05cd448c":"again results in acceptable ranges let us now do what the title of notebook says\ndesign a multi-Input TensorFlow model.","4ef254ca":"98 % validation accuracy, let's see what will happen if use pca clustered data for our testing","ad4ece88":"Well this is a good sign as you can see there exists\na  decission boundary, excluding some outliers, to what I believe\na simple tensorflow model should be able to perform well on this dataset.\nLet's see if this is true.\n\nAlso just for fun a model predicting every signal as non - pulsar will have an accuracy of 90%\nso I believe we shall try to achieve an accuracy of more than 90 %","dec6beb0":"# Visualizing Data Set\nFor Visualizing your data perform the following sets:\n\n1.) select the features & targets from your dataset\n\n2.) Do standard scalling (this step is important)\n\n3.) Do pca Clustering and convert the resultant data to dataFrame\n\n4.) use seaborn for visualizing this data\n","8a290c54":"# Introdution\n\nIn this notebook we intend to develope a multi Input tensorflow model for the given data set\n\nNote: The given data set is unbalanced ie number of positives small as compared to negatives.\nHence we will develope seperate models, other than neural networks, which deals with these imbalance\nby synthetically enhancing data using techinques like smote etc","3fca1e2d":"For our convinence lets map the columns to their respective name in the data set"}}