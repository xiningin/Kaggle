{"cell_type":{"9b86c78c":"code","26bb6b81":"code","0b183e46":"code","86a938d6":"code","24c79afc":"code","bf9b30a1":"code","b3aa0de8":"code","39766f1e":"code","f946dfa5":"code","5518a701":"code","4f42ce17":"code","ca07aacd":"code","af29dd94":"code","41d7632a":"code","0bb78574":"code","bc087aae":"code","b1c4f9e1":"markdown","47907603":"markdown","0526f313":"markdown","ea9241f9":"markdown","c920d744":"markdown","494ae2d3":"markdown","8532692a":"markdown","f56f7e8e":"markdown","17eaecf5":"markdown","04dc43d1":"markdown","f7479607":"markdown"},"source":{"9b86c78c":"import gc\nimport psutil\nimport multiprocessing as mp\nimport copy\nmp.cpu_count()\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport keras\nfrom keras import Input,Sequential, initializers, optimizers, callbacks, layers, models\nfrom keras.models import Model,Sequential\nfrom keras.layers import Dense,Conv2D,MaxPooling2D,Dropout,BatchNormalization,Lambda,Activation,Input,Flatten,Reshape,Conv2DTranspose\nimport keras.backend as K\nfrom keras.layers.merge import add\nfrom sklearn.model_selection import train_test_split\nimport os\nimport glob\nfrom time import time,asctime\nfrom random import randint as r\nimport random","26bb6b81":"imgs = glob.glob(\"..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/*.jpg\")\nimgs[0]","0b183e46":"train_y = []\ntrain_y2 = []\nfor _ in range(0,100000):\n  if _%20000 == 0:\n    print(\"{} \/ 100000\".format(_))\n  img = cv2.imread(imgs[_])\n  img = cv2.resize(img,(32,32))\n  train_y.append(img.astype(\"float32\")\/255.0)\nfor _ in range(100000,200000):\n  if _%20000 == 0:\n    print(\"{} \/ 200000\".format(_))\n  img = cv2.imread(imgs[_])\n  img = cv2.resize(img,(32,32))\n  train_y2.append(img.astype(\"float32\")\/255.0)\ntrain_y = np.array(train_y)\ntrain_y2 = np.array(train_y2)\nY_data = np.vstack((train_y,train_y2))\nprint(psutil.virtual_memory())\ndel train_y,train_y2\ngc.collect()\nprint(psutil.virtual_memory())","86a938d6":"test_Y = []\nfor _ in range(200000,202599):\n    if _%2000 == 0:\n        print(\"{} \/ 202599\".format(_))\n    img = cv2.imread(imgs[_])\n    img = cv2.resize(img,(32,32))\n    test_Y.append(img.astype(\"float32\")\/255.0)\ntest_Y = np.array(test_Y)","24c79afc":"def sample_z(layers):\n    std_norm = K.random_normal(shape=(K.shape(layers[0])[0], latent_dim), mean=0, stddev=1)\n    return layers[0] + layers[1]*std_norm","bf9b30a1":"latent_dim = 50\ntotal_epoch = 50\nbatch_size = 32\nimg_dim = (32, 32)\n\nencoder_inputs = keras.Input(shape=(*img_dim, 3))\n\nx = layers.Conv2D(32, kernel_size=(5,5), padding='SAME')(encoder_inputs)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2D(32, kernel_size=(5,5), strides=(2,2),padding='SAME')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2D(64, kernel_size=(5,5), strides=(2,2),padding='SAME')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2D(64, kernel_size=(5,5), strides=(1,1), padding='SAME')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2D(128, kernel_size=(5,5), strides=(1,1), padding='SAME')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(128, activation=\"relu\")(x)\n\nx = layers.Dense(64, activation=\"relu\")(x)\n\nmu = layers.Dense(latent_dim)(x)\nlog_sigma = layers.Dense(latent_dim)(x)\n\nz = layers.Lambda(sample_z)([mu, log_sigma])\n\nencoder = keras.Model(encoder_inputs,z)\n\nencoder.summary()\n","b3aa0de8":"latent_inputs = keras.Input(shape=(latent_dim,))\nx = layers.Dense(8 * 8 * 128, activation=\"relu\")(latent_inputs)\nx = layers.Reshape((8, 8, 128))(x)\n\nx = layers.Conv2D(128, kernel_size=(5,5), strides=(1,1), padding='SAME')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(64, kernel_size=(5,5),strides=(1,1), padding='SAME')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Conv2DTranspose(64, kernel_size=(5,5),strides=(2,2), padding='SAME')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(32, kernel_size=(5,5),strides=(2,2),padding='SAME')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(32, kernel_size=(5,5), padding='SAME')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU(0.2)(x)\ndecoder_outputs = layers.Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\ndecoder = keras.Model(latent_inputs, decoder_outputs)\ndecoder.summary()","39766f1e":"def reconstruction_loss(y_true, y_pred):\n    return K.mean(K.square(y_true - y_pred))\n\ndef kl_loss(y_true, y_pred):\n    kl_loss = K.abs(keras.losses.KLD(y_true, y_pred))\n    return kl_loss\n\ndef vae_loss(y_true, y_pred):\n    return reconstruction_loss(y_true, y_pred)+0.25*kl_loss(y_true, y_pred)","f946dfa5":"vae = keras.Model(encoder_inputs, decoder(encoder(encoder_inputs)))","5518a701":"vae.compile(optimizer = \"adam\",loss = vae_loss,metrics = [kl_loss,reconstruction_loss])","4f42ce17":"vae.fit(Y_data,Y_data,batch_size = 32,epochs = total_epoch)","ca07aacd":"pred = vae.predict(test_Y)","af29dd94":"temp = r(0,2599)\nprint(temp)\nplt.subplot(1,2,1)\nplt.imshow(test_Y[temp])\nplt.subplot(1,2,2)\nplt.imshow(pred[temp])","41d7632a":"gen =np.random.uniform(0, 1, size = (1,32,32,3))\ngen_sample = vae.predict(gen)\nplt.subplot(1,2,1)\nplt.imshow(gen[0])\nplt.subplot(1,2,2)\nplt.imshow(gen_sample[0])","0bb78574":"img_l=[]\nimg_path =(\"..\/input\/someface\/14997843405964e4941e4293.64533925.jpg\")\nimg = cv2.imread(img_path)\nimg = cv2.resize(img,(32,32))\nimg=np.array(img.astype(\"float32\")\/255.0)\nimg_l.append(img)\nimg=np.array(img_l)\nimg_sample = vae.predict(img)\nplt.subplot(1,2,1)\nplt.imshow(img[0])\nplt.subplot(1,2,2)\nplt.imshow(img_sample[0])","bc087aae":"img_l=[]\nimg_path =(\"..\/input\/someface2\/depositphotos.jpg\")\nimg = cv2.imread(img_path)\nimg = cv2.resize(img,(32,32))\nimg=np.array(img.astype(\"float32\")\/255.0)\nimg_l.append(img)\nimg=np.array(img_l)\nimg_sample = vae.predict(img)\nplt.subplot(1,2,1)\nplt.imshow(img[0])\nplt.subplot(1,2,2)\nplt.imshow(img_sample[0])","b1c4f9e1":"# Test data","47907603":"# Training the VAE","0526f313":"# Decoder part","ea9241f9":"# LOSS FUNCTIONS","c920d744":"Generating a new face by passing a random normal sample of size (32,32,3) and observing the output","494ae2d3":"# Displaying result","8532692a":"# Encoder part","f56f7e8e":"# **Variational Auto-Encoder**","17eaecf5":"# Trai data ","04dc43d1":"# Sampler function\n**Variational Auto Encoder has a sampler In VAE, the encoder outputs two vectors.one is mean and the other is standard_deviation.sample from these two are taken as a final vector that can be done using the sampler function.**","f7479607":"# Connecting the Encoder and Decoder to make the **Auto Encoder**"}}