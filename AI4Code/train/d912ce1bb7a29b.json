{"cell_type":{"4e365f10":"code","d8d934f3":"code","70b2be0f":"code","02286a62":"code","0b36a0cd":"code","580a6739":"code","5ff9ed5b":"code","04850916":"code","f59b4a70":"code","26d92dd7":"code","73c75bc7":"code","b4a59e49":"code","d17237c7":"code","262db70f":"code","4d4fb004":"code","831a702a":"code","8ea66fcc":"code","928f99fe":"code","20fbb74d":"code","8e3745b7":"code","f7746b21":"code","6d3bd001":"code","d9868736":"code","d3e2374a":"code","315de20c":"code","75138648":"code","6f7ef64e":"code","dbda7e4e":"code","23b53269":"code","6dc708b0":"code","8841a9b5":"code","c60cf27d":"code","43400ebb":"code","73450994":"code","53906466":"code","c9d2afd5":"code","b2840f1b":"code","ce2a0f5f":"code","e24c3ebf":"code","301877a4":"code","a8edcdf6":"code","2d8907b6":"code","081c0de3":"code","f2ff798e":"code","d5f9e2c3":"code","366cec86":"code","7cbe28d0":"code","0b4282c1":"code","5e273f9f":"code","9c0afc5f":"code","18174b0b":"code","b153b0fd":"code","e7d76229":"code","31964bdc":"code","842fc50b":"code","00775f1a":"code","69191425":"code","03dc1210":"code","df94945b":"code","97d658e0":"code","c133b0a5":"code","0335ae2c":"code","89005e57":"code","a762b1dc":"code","8ae869c5":"code","6e919ccf":"code","223940e7":"code","0e54395d":"code","2ca844a1":"code","206a0428":"code","69eeb6fc":"code","129a661c":"code","39729234":"code","89ff5ef3":"code","5f4ffc55":"code","7e4a7af6":"code","ee6387bf":"code","66b6117b":"code","8abc65bc":"code","663f1a4d":"code","f5aafc00":"code","66917010":"code","34d824f5":"code","ce3f9d38":"code","c8fada76":"code","bfced6dd":"code","b90901bc":"code","488419bc":"code","00d8d83d":"code","c1828701":"code","477670cf":"code","85faf4a3":"code","8d8a58c1":"code","6439d258":"code","dce2ad91":"code","6276f613":"code","52038366":"code","9077305c":"code","19365a01":"code","4e930044":"code","6916e0bf":"code","a3b4790b":"code","8155d137":"code","6a91700a":"code","486af28c":"code","e5acfd42":"code","ca07f694":"code","ea9b4875":"code","1353c363":"code","eb73ae40":"code","d7d28785":"code","771f5fdb":"code","d63690aa":"code","1b258730":"code","74037760":"code","adacdf98":"code","e6a766f1":"code","c9abcb72":"code","806ae384":"code","071e5390":"code","778aea10":"code","8ea837d1":"code","db1887bc":"markdown","f86b85a2":"markdown","90a9da74":"markdown","7c00c686":"markdown","de912b90":"markdown","9edcf490":"markdown","97935b28":"markdown","0555f4d6":"markdown","57235153":"markdown","8e5a5cdd":"markdown","518b69b5":"markdown","d4cf5700":"markdown","c9ee22e9":"markdown","ec475128":"markdown","ae8d1a40":"markdown","bb7aabdd":"markdown","f34eb7d1":"markdown","b078f0a1":"markdown","9b873abe":"markdown","4d0a4a33":"markdown","b21d926d":"markdown","3d91657f":"markdown","e7b12965":"markdown","19085d6e":"markdown","4b5a732d":"markdown","7419d632":"markdown","87fca13c":"markdown","6ee32ed3":"markdown","0147855e":"markdown","64f16219":"markdown"},"source":{"4e365f10":"# importing the libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('darkgrid')\n\nimport nltk\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import pairwise_distances\nimport string\nstring.punctuation\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns',100)","d8d934f3":"# reading the data\ndf = pd.read_csv('\/kaggle\/input\/predict-the-match-percentage\/data.csv')\n\n# looking at the data\ndf.head()","70b2be0f":"df.info()","02286a62":"# checking for duplicates\n# and we see that the ids are all unique\ndf.user_id.nunique()","0b36a0cd":"# doesn't help in model\ndf.drop('username', axis=1, inplace=True)","580a6739":"df.age.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","5ff9ed5b":"# Looking the distribution of column Age\nplt.figure(figsize=(12,5))\n\nskewness = round(df.age.skew(),2)\nkurtosis = round(df.age.kurtosis(),2)\nmean = round(np.mean(df.age),0)\nmedian = np.median(df.age)\n\nplt.subplot(1,2,1)\nsns.boxplot(y=df.age)\nplt.title('Boxplot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.subplot(1,2,2)\nsns.distplot(df.age)\nplt.title('Distribution Plot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.show()","04850916":"# creating a new column and divides the age into the bins\ndf['age_bin'] = pd.cut(df.age, bins=[17,24,30,40,50,70],labels=['17-24','25-30','31-40','41-50','50+'])","f59b4a70":"# making the dummy variables for the age_bin column \naged = pd.get_dummies(df.age_bin,prefix='age_')\ndf = pd.concat([df,aged], axis=1)\ndf.drop(['age','age_bin'],axis=1,inplace=True)","26d92dd7":"df.height.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","73c75bc7":"# Looking the distribution of column height\nplt.figure(figsize=(12,5))\n\nskewness = round(df.height.skew(),2)\nkurtosis = round(df.height.kurtosis(),2)\nmean = round(np.mean(df.height),0)\nmedian = np.median(df.height)\n\nplt.subplot(1,2,1)\nsns.boxplot(y=df.height)\nplt.title('Boxplot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.subplot(1,2,2)\nsns.distplot(df.height)\nplt.title('Distribution Plot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.show()","b4a59e49":"# creating a new column which stores height in feets\ndf['height_feet'] = round(df['height']*0.08333,1)","d17237c7":"df.height_feet.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","262db70f":"# creating a column which has height in bins\ndf['height_bin'] = pd.cut(df.height_feet,bins=[4,5,6,7],labels=['4-5feets','5-6feets','6-7feets'],right=False)","4d4fb004":"# making a dummy variable for the height_bin column\nheightd = pd.get_dummies(df.height_bin,prefix='height_')\ndf = pd.concat([df,heightd], axis=1)\ndf.drop(['height','height_feet','height_bin'],axis=1,inplace=True)","831a702a":"df.status.value_counts(normalize=True)","8ea66fcc":"# 'single' and 'available' both have same context in dating site so just combine them\ndf['status'] = df['status'].replace('available','single')","928f99fe":"# making a dummy variable for status\nstatusd = pd.get_dummies(df.status,prefix='status_')\ndf = pd.concat([df,statusd], axis=1)\ndf.drop('status',axis=1,inplace=True)","20fbb74d":"df.sex.value_counts(normalize=True)","8e3745b7":"# converting to numeric\ndf['sex'] = df['sex'].replace(('m','f'),(1,0))","f7746b21":"df.orientation.value_counts(normalize=True)","6d3bd001":"# creating a new column and apply the logic to fill the values\ndf['looking_for'] = np.NaN\n\ndf[(df.orientation=='straight') & (df.sex==1)]['looking_for']='female'\ndf[(df.orientation=='straight') & (df.sex==0)]['looking_for']='male'\n\ndf[(df.orientation=='gay') & (df.sex==1)]['looking_for']='male'\ndf[(df.orientation=='gay') & (df.sex==0)]['looking_for']='female'\n\ndf[(df.orientation=='bisexual') & (df.sex==1)]['looking_for']='both'\ndf[(df.orientation=='bisexual') & (df.sex==0)]['looking_for']='both'","d9868736":"# dropping the column\ndf.drop('orientation',axis=1,inplace=True)","d3e2374a":"# making the dummy variables for looking_for column\nlfd = pd.get_dummies(df.looking_for,prefix='looking_')\ndf = pd.concat([df,lfd], axis=1)\ndf.drop('looking_for',axis=1,inplace=True)","315de20c":"df.drinks.value_counts(normalize=True)","75138648":"# making the dummy variables for drinks column\ndrinkd = pd.get_dummies(df.drinks,prefix='drink_')\ndf = pd.concat([df,drinkd], axis=1)\ndf.drop('drinks',axis=1,inplace=True)","6f7ef64e":"df.drugs.value_counts(normalize=True)","dbda7e4e":"# making the dummy variables for drugs column\ndrugd = pd.get_dummies(df.drugs,prefix='drug_')\ndf = pd.concat([df,drugd], axis=1)\ndf.drop('drugs',axis=1,inplace=True)","23b53269":"df.job.value_counts(normalize=True)","6dc708b0":"# as the count of last four jobs has less than 1%, so we just combine them into the 'other'\ndf['job'] = df['job'].replace(('retired','rather not say','unemployed','military'),\n                             ('other','other','other','other'))","8841a9b5":"# making the dummy variables for job column\njd = pd.get_dummies(df.job,prefix='job_')\ndf = pd.concat([df,jd], axis=1)\ndf.drop('job',axis=1,inplace=True)","c60cf27d":"locn = df[['location']]\nlocn[['city','state']] = locn.location.str.split(',',expand=True)","43400ebb":"locd = pd.get_dummies(locn.city,prefix='lives_in_')\nlocn = pd.concat([locn,locd], axis=1)\n\nlocn.head()","73450994":"locn.iloc[:,3:].sum().sort_values(ascending=False).index","53906466":"locn.iloc[:,3:].sum().sort_values(ascending=False).values","c9d2afd5":"# rearranging the columns such that they appeared according to their count in descending order\nlocn = locn[['lives_in__san francisco', 'lives_in__oakland', 'lives_in__berkeley','lives_in__san mateo', 'lives_in__palo alto', \n             'lives_in__alameda','lives_in__san rafael', 'lives_in__san leandro','lives_in__redwood city', \n             'lives_in__emeryville', 'lives_in__daly city','lives_in__walnut creek', 'lives_in__hayward', 'lives_in__pacifica',\n             'lives_in__el cerrito', 'lives_in__menlo park','lives_in__mountain view', 'lives_in__richmond', \n             'lives_in__martinez','lives_in__burlingame', 'lives_in__benicia', 'lives_in__vallejo','lives_in__mill valley', \n             'lives_in__south san francisco','lives_in__pleasant hill', 'lives_in__novato','lives_in__castro valley', \n             'lives_in__lafayette','lives_in__san carlos', 'lives_in__belmont', 'lives_in__san bruno','lives_in__el sobrante', \n             'lives_in__millbrae', 'lives_in__fremont','lives_in__half moon bay', 'lives_in__albany', 'lives_in__hercules',\n             'lives_in__stanford', 'lives_in__san pablo', 'lives_in__san lorenzo','lives_in__fairfax', 'lives_in__atherton', \n             'lives_in__moraga','lives_in__sausalito', 'lives_in__san anselmo','lives_in__corte madera', 'lives_in__woodacre', \n             'lives_in__green brae','lives_in__belvedere tiburon', 'lives_in__rodeo', 'lives_in__orinda','lives_in__larkspur', \n             'lives_in__pinole', 'lives_in__canyon country','lives_in__stockton', 'lives_in__santa rosa', 'lives_in__brisbane',\n             'lives_in__brooklyn', 'lives_in__point richmond', 'lives_in__lagunitas','lives_in__cincinnati', 'lives_in__phoenix',\n             'lives_in__petaluma','lives_in__north hollywood', 'lives_in__nha trang','lives_in__foster city', \n             'lives_in__moss beach','lives_in__hacienda heights', 'lives_in__montara','lives_in__woodside']]","b2840f1b":"# creating a new column\nlocn['others'] = locn.iloc[:,13:].sum(axis=1).astype('int')","ce2a0f5f":"locn = locn[['lives_in__san francisco', 'lives_in__oakland', 'lives_in__berkeley','lives_in__san mateo', 'lives_in__palo alto', \n             'lives_in__alameda','lives_in__san rafael', 'lives_in__san leandro','lives_in__redwood city', \n             'lives_in__emeryville', 'lives_in__daly city','lives_in__walnut creek', 'lives_in__hayward','others', \n             'lives_in__pacifica','lives_in__el cerrito', 'lives_in__menlo park','lives_in__mountain view', 'lives_in__richmond', \n             'lives_in__martinez','lives_in__burlingame', 'lives_in__benicia', 'lives_in__vallejo','lives_in__mill valley', \n             'lives_in__south san francisco','lives_in__pleasant hill', 'lives_in__novato','lives_in__castro valley', \n             'lives_in__lafayette','lives_in__san carlos', 'lives_in__belmont', 'lives_in__san bruno','lives_in__el sobrante', \n             'lives_in__millbrae', 'lives_in__fremont','lives_in__half moon bay', 'lives_in__albany', 'lives_in__hercules',\n             'lives_in__stanford', 'lives_in__san pablo', 'lives_in__san lorenzo','lives_in__fairfax', 'lives_in__atherton', \n             'lives_in__moraga','lives_in__sausalito', 'lives_in__san anselmo','lives_in__corte madera', 'lives_in__woodacre', \n             'lives_in__green brae','lives_in__belvedere tiburon', 'lives_in__rodeo', 'lives_in__orinda','lives_in__larkspur', \n             'lives_in__pinole', 'lives_in__canyon country','lives_in__stockton', 'lives_in__santa rosa', 'lives_in__brisbane',\n             'lives_in__brooklyn', 'lives_in__point richmond', 'lives_in__lagunitas','lives_in__cincinnati', 'lives_in__phoenix',\n             'lives_in__petaluma','lives_in__north hollywood', 'lives_in__nha trang','lives_in__foster city', \n             'lives_in__moss beach','lives_in__hacienda heights', 'lives_in__montara','lives_in__woodside']]","e24c3ebf":"# storing only those columns whose count is greater than 20 and plus one extra column 'other'\nlocn = locn.iloc[:,:14]","301877a4":"# concatinate the main df with this location df\ndf = pd.concat([df,locn],axis=1)\n\n# dropping the column\ndf.drop('location',axis=1,inplace=True)","a8edcdf6":"df.pets.value_counts(normalize=True)","2d8907b6":"# creating a fuction that return whether user likes cats or dogs or both or none\ndef pet_like(txt):\n    if txt.find('likes dogs and likes cats')!= -1:\n        return 'dog and cat'\n    elif txt.find('likes dogs')!= -1:\n        return 'dog'\n    elif txt.find('likes cats')!= -1:\n        return 'cat'\n    else:\n        return 'none'\n\n# calling the above function\ndf['pet_like'] = df['pets'].apply(lambda x: pet_like(x))","081c0de3":"# creating a function that returns whether a person owned cats or dogs or both or none\ndef pet_owned(txt):\n    if txt.find('has dogs and has cats')!= -1:\n        return 'dog and cat'\n    elif txt.find('has dogs')!= -1:\n        return 'dog'\n    elif txt.find('has cats')!= -1:\n        return 'cat'\n    else:\n        return 'none'\n\n# calling the function\ndf['pet_owned'] = df['pets'].apply(lambda x: pet_owned(x))","f2ff798e":"# making the dummy variables for pet_like column\npetld = pd.get_dummies(df.pet_like,prefix='petLike_')\ndf = pd.concat([df,petld], axis=1)\ndf.drop('pet_like',axis=1,inplace=True)","d5f9e2c3":"# making the dummy variables for pet_owned column\npetod = pd.get_dummies(df.pet_owned,prefix='petOwn_')\ndf = pd.concat([df,petod], axis=1)\ndf.drop('pet_owned',axis=1,inplace=True)","366cec86":"# dropping the variable\ndf.drop('pets',axis=1,inplace=True)","7cbe28d0":"df.smokes.value_counts(normalize=True)","0b4282c1":"# combining the categories as they have similar context\ndf['smokes'] = df['smokes'].replace('trying to quit','sometimes')\ndf['smokes'] = df['smokes'].replace('when drinking','sometimes')","5e273f9f":"# making the dummy variables for smokes column\nsmoked = pd.get_dummies(df.smokes,prefix='smoke_')\ndf = pd.concat([df,smoked], axis=1)\ndf.drop('smokes',axis=1,inplace=True)","9c0afc5f":"df.new_languages.value_counts(normalize=True)","18174b0b":"# dropping the variable as it has no relevancy\nnld = pd.get_dummies(df.new_languages,prefix='new_lang_')\ndf = pd.concat([df,nld], axis=1)\ndf.drop('new_languages',axis=1,inplace=True)","b153b0fd":"df.body_profile.value_counts(normalize=True)","e7d76229":"# making the dummy variables for body_profile column\nbd = pd.get_dummies(df.body_profile,prefix='body_')\ndf = pd.concat([df,bd], axis=1)\ndf.drop('body_profile',axis=1,inplace=True)","31964bdc":"df.education_level.value_counts(normalize=True)","842fc50b":"# using scaler to convert them between 0-1\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf['education_level'] = scaler.fit_transform(df[['education_level']])","00775f1a":"df.dropped_out.value_counts(normalize=True)","69191425":"# converting to numeric\ndf['dropped_out'] = df['dropped_out'].replace(('no','yes'),(0,1))","03dc1210":"df.interests.value_counts(normalize=True)","df94945b":"df.other_interests.value_counts(normalize=True)","97d658e0":"# making the set of all the interest from both the columns and taking union of that so that there is no repetation\ns1 = set(df.interests.value_counts(normalize=True).index)\ns2 = set(df.other_interests.value_counts(normalize=True).index)\ns3 = s1.union(s2)","c133b0a5":"# creating a new columns by combining the two interests columns\ndf['hobbies'] = df['interests']+','+df['other_interests']","0335ae2c":"# creating new columns of hobbies as a dummy variable\nfor col in list(s3):\n    df[col] = df.hobbies.str.find(col).apply(lambda x: 0 if x==-1 else 1)","89005e57":"df.drop(['interests','other_interests','hobbies'],axis=1,inplace=True)","a762b1dc":"df.location_preference.value_counts(normalize=True)","8ae869c5":"# making the dummy variables for location_preference column\nlocd = pd.get_dummies(df.location_preference,prefix='location_pref_')\ndf = pd.concat([df,locd], axis=1)\ndf.drop('location_preference',axis=1,inplace=True)","6e919ccf":"# copying the language column in other object\nlang = df[['language']]\n\n# splitting the column\nlang[['L1','L2','L3','L4','L5']] = lang.language.str.split(',',expand=True)\n\nlang.head(10)","223940e7":"# looking for null entries\nlang.isnull().sum()","0e54395d":"# replacing the null values with 'none'\nlang['L2'] = lang['L2'].replace(np.NaN, 'None')\nlang['L3'] = lang['L3'].replace(np.NaN, 'None')\nlang['L4'] = lang['L4'].replace(np.NaN, 'None')\nlang['L5'] = lang['L5'].replace(np.NaN, 'None')","2ca844a1":"# creating a function that returns the language that user marked as fluent he\/she is in\ndef fluent(txt):\n    l1 = list(txt)\n    l2 = list(txt.str.contains('(fluently)'))\n    l3 = []\n    for i,j in enumerate(l2):\n        if j==True:\n            l3.append(l1[i])\n        else:\n            l3.append('None')\n    return l3","206a0428":"# calling the above function on all the five columns\nlang['F1'] = fluent(lang['L1'])\nlang['F2'] = fluent(lang['L2'])\nlang['F3'] = fluent(lang['L3'])\nlang['F4'] = fluent(lang['L4'])\nlang['F5'] = fluent(lang['L5'])","69eeb6fc":"lang.head()","129a661c":"lang.isnull().sum()","39729234":"# removing any extra white spaces\nlang['F1'] = lang['F1'].str.strip()\nlang['F2'] = lang['F2'].str.strip()\nlang['F3'] = lang['F3'].str.strip()\nlang['F4'] = lang['F4'].str.strip()\nlang['F5'] = lang['F5'].str.strip()","89ff5ef3":"# creating the sets\nf1 = set(lang.F1.unique())\nf2 = set(lang.F2.unique())\nf3 = set(lang.F3.unique())\nf4 = set(lang.F4.unique())\nf5 = set(lang.F5.unique())","5f4ffc55":"# getting all the unique languages from all these columns\nu1 = f1.union(f2)\nu2 = u1.union(f3)\nu3 = u2.union(f4)\nu4 = u3.union(f5)","7e4a7af6":"lang.head()","ee6387bf":"# creating the columns\nfor col in list(u4):\n    lang[col] = lang.language.str.find(col).apply(lambda x: 0 if x==-1 else 1)","66b6117b":"# creating a column which gives number of languages a user know\nlang['lang_known'] = lang['language'].apply(lambda x:len(x.split(',')))","8abc65bc":"lang.sample(10)","663f1a4d":"# keeping only relevant columns\nlang = lang.iloc[:,11:]","f5aafc00":"lang.sum(axis=0).sort_values(ascending=False)","66917010":"# dropping the language\nlang.drop('c++ (fluently)',axis=1,inplace=True)","34d824f5":"# arranging the columns in the order of their counts\nlang = lang[['lang_known','english (fluently)','spanish (fluently)','chinese (fluently)','french (fluently)',\n             'german (fluently)','italian (fluently)','farsi (fluently)','hindi (fluently)','russian (fluently)',\n             'hebrew (fluently)','tagalog (fluently)','japanese (fluently)', 'sign language (fluently)','portuguese (fluently)',\n             'swedish (fluently)','korean (fluently)','sanskrit (fluently)', 'dutch (fluently)', 'arabic (fluently)', \n             'hungarian (fluently)', 'icelandic (fluently)','gujarati (fluently)','irish (fluently)', 'vietnamese (fluently)',\n             'esperanto (fluently)', 'tamil (fluently)', 'bulgarian (fluently)', 'indonesian (fluently)','norwegian (fluently)',\n             'thai (fluently)', 'urdu (fluently)', 'ukrainian (fluently)','cebuano (fluently)', 'polish (fluently)', \n             'bengali (fluently)','ancient greek (fluently)', 'slovak (fluently)', 'None','afrikaans (fluently)', \n             'maori (fluently)', 'czech (fluently)','danish (fluently)', 'latin (fluently)', 'other (fluently)',\n             'ilongo (fluently)', 'greek (fluently)', 'lisp (fluently)','turkish (fluently)']]","ce3f9d38":"# keeping those languages whose count is greater than 5 and combine rest other into one single column\nlang['others'] = lang.iloc[:,17:].sum(axis=1)","c8fada76":"lang.head()","bfced6dd":"# rearranging of columns\nlang = lang[['lang_known','english (fluently)','spanish (fluently)','chinese (fluently)','french (fluently)',\n             'german (fluently)','italian (fluently)','farsi (fluently)','hindi (fluently)','russian (fluently)',\n             'hebrew (fluently)','tagalog (fluently)','japanese (fluently)', 'sign language (fluently)','portuguese (fluently)',\n             'swedish (fluently)','korean (fluently)','others','sanskrit (fluently)', 'dutch (fluently)', 'arabic (fluently)', \n             'hungarian (fluently)', 'icelandic (fluently)','gujarati (fluently)','irish (fluently)', 'vietnamese (fluently)',\n             'esperanto (fluently)', 'tamil (fluently)', 'bulgarian (fluently)', 'indonesian (fluently)','norwegian (fluently)',\n             'thai (fluently)', 'urdu (fluently)', 'ukrainian (fluently)','cebuano (fluently)', 'polish (fluently)', \n             'bengali (fluently)','ancient greek (fluently)', 'slovak (fluently)', 'None','afrikaans (fluently)', \n             'maori (fluently)', 'czech (fluently)','danish (fluently)', 'latin (fluently)', 'other (fluently)',\n             'ilongo (fluently)', 'greek (fluently)', 'lisp (fluently)','turkish (fluently)']]","b90901bc":"# keeping only relevant languages\nlang = lang.iloc[:,:18]\nlang.head()","488419bc":"# concate the lang_fluent and lang_known to the main dataframe\ndf = pd.concat([df,lang],axis=1)","00d8d83d":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf['lang_known'] = scaler.fit_transform(df[['lang_known']])\ndf.drop('language',axis=1,inplace=True)","c1828701":"bio = df[['bio']]\nbio.head()","477670cf":"# converting all words to lowercase\n\nbio['bio'] = bio['bio'].str.lower()","85faf4a3":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n                           \"you're\": \"you are\", \"you've\": \"you have\"}\n\n\ndef contraction(txt):\n    l1 = list(txt)\n    l2 = []\n    for i in l1:\n        l2.append(' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in i.split(\" \")]))\n    return l2\n\n\nbio['bio_clean']= contraction(bio['bio'])","8d8a58c1":"# creating a function that removes \"'s\" part from the words\ndef removing_s(txt):\n    l1 = list(txt)\n    l2 = []\n    for i in l1:\n        l2.append(re.sub(r\"'s\\b\",\"\",i))\n    return l2\n\nbio['bio_clean']= removing_s(bio['bio_clean'])","6439d258":"# function that keep only letters and remove other things\ndef only_letters(txt):\n    l1 = list(txt)\n    l2 = []\n    for i in l1:\n        l2.append(re.sub(\"[^a-zA-Z]\", \" \", i))\n    return l2\n\nbio['bio_clean']= only_letters(bio['bio_clean'])","dce2ad91":"# function that removes puctuations\n\nimport string\nstring.punctuation\n\ndef remove_punctuation(txt):\n    txt_nopunct = \"\".join([c for c in txt if c not in string.punctuation])\n    return txt_nopunct\n\nbio['bio_clean_rp'] = bio['bio_clean'].apply(lambda x: remove_punctuation(x))\nbio.head()","6276f613":"# function for tokenization\n\nimport re\nfrom nltk.tokenize import word_tokenize\n\ndef tokenize(txt):\n    token = re.split('\\W+', txt)\n    return token\n\nbio['bio_clean_tokenize'] = bio['bio_clean_rp'].apply(lambda x: tokenize(x))\n\n# another way\n# df['msg_clean_tokenize'] = df['msg_clean'].apply(word_tokenize)\n# df['msg_clean_tokenize'] = df['msg_clean'].apply(lambda x: x.split())\n\nbio.head()","52038366":"# function for removing stopwords\n\nstopwords = nltk.corpus.stopwords.words('english')\n\n# list of stopwords\n# print(stopwords[:30])\n\ndef remove_stopwords(txt):\n    txt_clean = [word for word in txt if word not in stopwords]\n    return txt_clean\n\nbio['bio_no_sw'] = bio['bio_clean_tokenize'].apply(lambda x: remove_stopwords(x))\nbio.head()","9077305c":"# function for lemmatization\nwn = nltk.WordNetLemmatizer()\n\ndef lemmatizing(txt):\n    text = [wn.lemmatize(word) for word in txt]\n    return text\n\nbio['bio_lemmatize'] = bio['bio_no_sw'].apply(lambda x: lemmatizing(x))\nbio.head()","19365a01":"#bio['length_before'] = bio['bio'].apply(lambda x:len(x.split(' ')))\n#bio['length_after'] = bio['bio_lemmatize'].apply(lambda x:len(x))","4e930044":"# joined all the clean words back to the sentences\nbio['final'] = bio['bio_lemmatize'].apply(lambda x:\" \".join(x))","6916e0bf":"# creating a function for scoring the sentiment of the sentence\n# x<0 - Negative\n# x=0 - Neutral\n# x>0 & x<=1 - Positive\n\ndef sentiment(txt):\n    return (TextBlob(txt).sentiment.polarity)\n\nbio['sentiment_score'] = bio['bio'].apply(lambda x: sentiment(x))\n\n# replacing all the negative values to 0\nbio['sentiment_score'] = bio['sentiment_score'].apply(lambda x: 0 if x<0 else x)","a3b4790b":"bio.head()","8155d137":"# function that removes words whose length less than 3\ndef lessthan3(txt):\n    l1 = list(txt.strip().split(' '))\n    l2 = []\n    for i in l1:\n        if len(i)>3:\n            l2.append(i)\n    return(\" \".join(l2))\n\nbio['final1'] = bio['final'].apply(lambda x:lessthan3(x))","6a91700a":"# graph showing top 50 words used by the users\nall_words = []\nfor line in list(bio['final1']):\n    words = line.split()\n    for word in words:\n        all_words.append(word)\n        \nplt.figure(figsize=(15,5))\nplt.title('Top 50 most common words')\nplt.xticks(fontsize=13)\nfd = nltk.FreqDist(all_words)\nfd.plot(50,cumulative=False)\nplt.show()","486af28c":"# keeping only relevant columns\nbio_final = bio[['final1','sentiment_score']]","e5acfd42":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# creating a TF-IDF matrix with 1000 words\ntfidf = TfidfVectorizer(max_features=1000)\n\nX = tfidf.fit_transform(bio_final['final1'])\nprint(X.shape)","ca07f694":"# converting it into dataframe\nbio_tfidf = pd.DataFrame(X.toarray(), columns=tfidf.get_feature_names())\nbio_tfidf.head()","ea9b4875":"# concat the TF-IDF and sentiment_score to the main dataframe\ndf = pd.concat([df,bio_tfidf,bio_final[['sentiment_score']]],axis=1)\n\ndf.drop('bio',axis=1,inplace=True)","1353c363":"df.head()","eb73ae40":"df1 = df.set_index('user_id')\ndf1.head()","d7d28785":"from sklearn.metrics.pairwise import pairwise_distances\n\n# User Similarity Matrix using 'cosine' measure\nuser_correlation = (1 - pairwise_distances(df1, metric='cosine'))*100\n\nprint(user_correlation)","771f5fdb":"user_correlation.shape","d63690aa":"# putting the diagonal elements to 0\na = np.matrix(user_correlation)\n#np.fill_diagonal(a,0.00)\n\n# converting the matrix to dataframe\nfinal_df = pd.concat([df[['user_id']],pd.DataFrame(a,columns=df.user_id)],axis=1)","1b258730":"final_df = final_df.round(2)\nfinal_df.head()","74037760":"# reading the data\nsample = pd.read_csv('\/kaggle\/input\/predict-the-match-percentage\/data.csv')\n\n# keeping only three columns\nsample = sample[['user_id','sex','orientation']]\n\n# joined sex and orientation column\nsample[\"pref\"] = sample[\"sex\"] + \" \" + sample[\"orientation\"]\n\n# converting into category\nsample[\"pref\"] = sample[\"pref\"].astype(\"category\")\n\n# getting the code for each category\nsample[\"code\"] = sample[\"pref\"].cat.codes\n\n# adding one to code so that it starts from 1\nsample[\"code\"] = sample[\"code\"] + 1\n\nsample.head()","adacdf98":"# created a new dataframe with one column user_id\ntable = pd.DataFrame({'user_id':final_df.columns})\ntable = table.iloc[1:,:]\n\n# create a new column \ntable['uid'] = range(1,2002)\n\ntable.head()","e6a766f1":"# merging the two dataframe\nsample = sample.merge(table,on='user_id')\nsample.head()","c9abcb72":"# checking the code\nsample.groupby('pref')['code'].mean()","806ae384":"# merging the dataframe with itself\nsample2 = sample[[\"uid\",\"code\"]].merge(sample[[\"uid\",\"code\"]], on=\"code\")\n\n# getting the pivot table\nsample2 = sample2.pivot_table(index='uid_x',columns='uid_y',values='code')\n\nsample2","071e5390":"# applying the logic and replace the values\nsample3 = sample2.replace([4,np.NaN,2,3,5,6],[1,1,0,0,0,0])\nsample3","778aea10":"# converting it into matrix and filling diagonal elements to 0\nb = np.matrix(sample3)\nnp.fill_diagonal(b,0.00)\n\n# converting it into dataframe\nsample4 = pd.DataFrame(b,columns=sample3.columns)\n\n# setting the index\nsample4.index = sample4.columns\n\nsample4","8ea837d1":"sample5 = pd.DataFrame(np.multiply(np.matrix(final_df.iloc[:,1:]), np.matrix(sample4)),columns=final_df.columns[1:])\nsample5.index = final_df.columns[1:]\nsample5.reset_index(inplace=True)\nsample5.rename(columns={'index':'user_id'},inplace=True)\nsample5.head()","db1887bc":"---\n#### education level","f86b85a2":"The above matrix is the final submission file and it shows the match percentage between each user with that other user and also following the instructions given in the **note** of the problem. \n**My score is 97.87761**.","90a9da74":"---\n#### dropped out","7c00c686":"---\n#### interests and other_interests","de912b90":"- There is no null values in any of the columns\n- Most of the variables are object.","9edcf490":"---\n#### username","97935b28":"---\n#### location preference","0555f4d6":"---\n#### orientation","57235153":"---\n#### status","8e5a5cdd":"Here we just looking for cities whose count is more than 20 and combine rest of the cities into the 'other' category","518b69b5":"#### user id","d4cf5700":"## Probem Statement:\n### Predict the match percentage\nIn an era where technology plays a significant role in people\u2019s lives, one cannot deny that it changes the way people interact and communicate with others. Today, technology has caused some significant changes in the dating world as well. Online dating is a new trend that is influencing many people around the world.<br>\nAs a data scientist, you are required to predict the match percentage between the users in a matrix format based on the attributes provided by the user on a dating website.\n\n#### Note:\nBased on the user\u2019s sexual orientation, you are required to perform the following:\n- If a user is heterosexual (prefers the opposite sex), then the match percentage must be 0 for this user with respect to other users of the same gender if the other users have the same behavior.\n- If a user is a homosexual (prefers the same sex), then the match percentage must be 0 for this user with respect to other users of the opposite gender if the other users have the same behavior.\n- The match percentage of a user with her\/himself must be zero.","c9ee22e9":"---\n##### location","ec475128":"For this column we have use **text mining** technique and finally use TF-IDF matrix","ae8d1a40":"---\n#### sex","bb7aabdd":"### I. Matrix 1","f34eb7d1":"---\n#### drugs","b078f0a1":"---\n#### pets","9b873abe":"### Multiplying the two matrix","4d0a4a33":"---\n#### Height","b21d926d":"---\n#### job","3d91657f":"---\n#### drinks","e7b12965":"## Approach\nWe will create two matrix and then multiply these two matrix element-wise.<br>\nThe two matrix are - \n1. `Matrix 1: user-user similarity matrix`\n    - We have a final dataframe that has all the variables which are needed for the model building\n    - Setting the user ids as an index of the dataframe\n    - Then we use cosine similarity on this dataframe which will results in a matrix of size 20001x2001. \n    - Convert all the diagonals elements to 0 and then convert it into a dataframe which has columns and rows as user ids with value fill as similarity score between each user.\n2. `Matrix 2: binary user-user matrix`\n    - The elements of this matrix are 0 or 1 based upon the condition given in the **problem's note**.\n    - At last we multiply these two matrices and get our final match percentage.","19085d6e":"### II. Matrix 2","4b5a732d":"---\n#### smokes","7419d632":"---\n#### bio","87fca13c":"---\n#### language","6ee32ed3":"---\n#### new language","0147855e":"---\n#### body profile","64f16219":"---\n#### age"}}