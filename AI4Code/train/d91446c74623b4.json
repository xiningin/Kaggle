{"cell_type":{"ca416e80":"code","14bf3ea4":"code","072c6259":"code","209c5016":"code","d37ea41e":"code","061f9a3e":"code","08a0e0f7":"code","5584e5af":"code","ec89f411":"code","a49261bf":"code","511de2cb":"code","a328ee5f":"code","e739b553":"code","40eedd74":"code","1addb7d9":"code","3d2bcd8a":"code","7e728ba2":"code","39fe2bcb":"code","fbe70355":"code","9c2e57c7":"code","39ba932d":"code","6e2a8a97":"code","ac67cb01":"code","8e9766cb":"code","86ed0802":"code","969632b1":"code","5a4917c2":"code","a988ccc1":"code","8c17b0a5":"code","488291b8":"code","651b9dc3":"code","4aae75e7":"code","9a7f25a6":"code","ef8e6848":"code","339a6d8b":"code","d8949b58":"code","44d5b196":"code","300b706a":"code","e6769ad2":"code","c1af6d0c":"code","b34de7d3":"code","18928c81":"code","a11d0014":"code","605d49ea":"code","6fa0907d":"markdown","a4119dd5":"markdown","73742ac3":"markdown","49ed5976":"markdown","e50c494a":"markdown","044751cc":"markdown","57c474f9":"markdown","50a5e47e":"markdown","a91f2817":"markdown","2868c74a":"markdown","58f4584b":"markdown","bd1c2354":"markdown","d9f540b3":"markdown","81bbd1e2":"markdown","b2450cb1":"markdown","8ed80e24":"markdown","9da88d9c":"markdown","7ca390f9":"markdown","0b1f2f31":"markdown","508700d9":"markdown","a3f1dc53":"markdown","c41ab592":"markdown","335a8e15":"markdown","69f5054f":"markdown"},"source":{"ca416e80":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","14bf3ea4":"# Importando os dados\ndf = pd.read_csv('\/kaggle\/input\/big-mart-sales\/train_v9rqX0R.csv')\ntest = pd.read_csv('\/kaggle\/input\/test-sales\/test_AbJTz2l.csv')\n\n\n# Verificando tamanho (linhas, colunas)\nprint(\"Train:\",df.shape) \nprint(\"Test:\",test.shape) ","072c6259":"# Verificando tipos e quantidades\ndf.info()","209c5016":"# Verificando o dataframe de teste\ntest.info()","d37ea41e":"# dando uma olhada nos dados\ndf.head(5)","061f9a3e":"# Vamos verificar os dados de test\ntest.head(5)","08a0e0f7":"df.shape","5584e5af":"# Quantidade de valores unicos para ID\ndf['Item_Identifier'].nunique()","ec89f411":"df['Item_Fat_Content'].nunique()","a49261bf":"df['Item_Fat_Content'].unique()","511de2cb":"#Vamos limpar a coluna Item_Fat_Content para ter apenas 2 cat\ndict_fat = {'low fat' : 'Low Fat','LF' : 'Low Fat', 'reg' :  'Regular' }\ndf['Item_Fat_Content'].replace(dict_fat,inplace=True)\ndf['Item_Fat_Content'].unique() #vamos verificar se foi feita a limpeza","a328ee5f":"df['Item_Type'].nunique()","e739b553":"df['Item_Type'].unique()","40eedd74":"df['Outlet_Identifier'].nunique()","1addb7d9":"df['Outlet_Identifier'].unique()","3d2bcd8a":"df['Outlet_Size'].unique()","7e728ba2":"df['Outlet_Location_Type'].unique()\n ","39fe2bcb":"df['Outlet_Type'].unique()\n","fbe70355":"df.info()","9c2e57c7":"df['Item_Weight'].hist()","39ba932d":"df['Item_Weight'].describe()","6e2a8a97":"print(\"Item_Weight Valores Ausentes: \",(df['Item_Weight'].isna().sum()\/len(df))*100)","ac67cb01":"df['Item_Weight'] = df['Item_Weight'].fillna(12.6)\ndf['Item_Weight'].describe() #verificando como ficou a var apos substituicao\n","8e9766cb":"df['Outlet_Size'].value_counts(normalize=True, dropna=False)","86ed0802":"df['Outlet_Size'].value_counts(normalize=True)","969632b1":"df['Outlet_Size'] = df['Outlet_Size'].fillna(pd.Series(np.random.choice(['Medium', 'Small', 'High'], \n                                                      p=[0.46, 0.39, 0.15], size=len(df))))\n","5a4917c2":"df['Outlet_Size'].value_counts(normalize=True)","a988ccc1":"df.info()","8c17b0a5":"#importando metodo do sklearn para variaveis categoricas\nfrom sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(categories='auto')\nohe_arr = ohe.fit_transform(df[['Item_Fat_Content','Item_Type','Outlet_Size','Outlet_Location_Type','Outlet_Type']]).toarray()\nohe_labels = ohe.get_feature_names(['Item_Fat_Content','Item_Type','Outlet_Size','Outlet_Location_Type','Outlet_Type'])\nohe_df = pd.DataFrame(ohe_arr, columns= ohe_labels)\nohe_df.head()","488291b8":"df.drop(columns= ['Item_Fat_Content','Item_Type','Outlet_Size','Outlet_Location_Type','Outlet_Type'], inplace=True)\ndf = pd.concat([df,ohe_df],axis=1,join='inner')\ndf.info() ","651b9dc3":"test.info()","4aae75e7":"#tratamento NA em item_weight\ntest['Item_Weight'] = test['Item_Weight'].fillna(12.6)\n#tratamento NA em outlet_size\ntest['Outlet_Size'] = test['Outlet_Size'].fillna(pd.Series(np.random.choice(['Medium', 'Small', 'High'], \n                                                      p=[0.46, 0.39, 0.15], size=len(test)))) \n#one hot encoder para base test\nohe_arrT = ohe.fit_transform(test[['Item_Fat_Content','Item_Type','Outlet_Size','Outlet_Location_Type','Outlet_Type']]).toarray()\nohe_labels = ohe.get_feature_names(['Item_Fat_Content','Item_Type','Outlet_Size','Outlet_Location_Type','Outlet_Type'])\nohe_dfT = pd.DataFrame(ohe_arrT, columns= ohe_labels)\n#adicionar colunas com var dummies na base test\ntest.drop(columns= ['Item_Fat_Content','Item_Type','Outlet_Size','Outlet_Location_Type','Outlet_Type'], inplace=True)\ntest = pd.concat([test,ohe_dfT],axis=1,join='inner')\ntest.info() \n","9a7f25a6":"# Divisao base de treino e validacao\nfrom sklearn.model_selection import train_test_split\ntrain, valid = train_test_split(df, random_state=42)\nprint(\"Train:\", train.shape)\nprint(\"Valid:\", valid.shape)","ef8e6848":"df.columns","339a6d8b":"# Criar a lista das colunas de entrada\nremoved_cols = ['Item_Identifier','Item_Outlet_Sales','Outlet_Identifier']\n\nfeats = [c for c in df.columns if c not in removed_cols]\n","d8949b58":"# Importando os modelos\nfrom sklearn import ensemble\n\nmodels = [\n    ensemble.RandomForestRegressor(random_state=42, n_jobs=-1),\n    ensemble.GradientBoostingRegressor(random_state=42),\n    ensemble.AdaBoostRegressor(random_state=42),\n    ensemble.BaggingRegressor(random_state=42, n_jobs=-1),\n    ensemble.ExtraTreesRegressor(random_state=42, n_jobs=-1)   \n]","44d5b196":"# lista que salva os scores de 5 modelos de regressao\nscores = []\nfor m in range(len(models)):\n    models[m].fit(train[feats], train['Item_Outlet_Sales'])\n    s = models[m].score(valid[feats],valid['Item_Outlet_Sales'])\n    scores.append(s)\n    ","300b706a":"#resultados dos modelos\nmodelos = [\"Random Forest Regressor\", \"Gradient Boosting Regressor\", \"AdaBoost Regressor\",\n          \"Bagging Regressor\", \"ExtraTreesRegressor\"]\nfor i in range(len(modelos)):\n    print(modelos[i],\" :\", scores[i])","e6769ad2":"# Previsoes\npredRF = models[0].predict(test[feats])\npredGB = models[1].predict(test[feats])\npredAB = models[2].predict(test[feats])\npredBag = models[3].predict(test[feats])\npredET = models[4].predict(test[feats])\n","c1af6d0c":"#Adicionando na base de teste para envio\ntest['Item_Outlet_Sales']= predRF\n# Gerando o arquivo para submeter ao kaggle\ntest[['Item_Identifier','Outlet_Identifier','Item_Outlet_Sales']].to_csv('modelRF.csv', index=False)","b34de7d3":"#Adicionando na base de teste para envio\ntest['Item_Outlet_Sales']= predGB\n# Gerando o arquivo para submeter ao kaggle\ntest[['Item_Identifier','Outlet_Identifier','Item_Outlet_Sales']].to_csv('modelGB.csv', index=False)","18928c81":"#Adicionando na base de teste para envio\ntest['Item_Outlet_Sales']= predAB\n# Gerando o arquivo para submeter ao kaggle\ntest[['Item_Identifier','Outlet_Identifier','Item_Outlet_Sales']].to_csv('modelAB.csv', index=False)","a11d0014":"#Adicionando na base de teste para envio\ntest['Item_Outlet_Sales']= predBag\n# Gerando o arquivo para submeter ao kaggle\ntest[['Item_Identifier','Outlet_Identifier','Item_Outlet_Sales']].to_csv('modelBag.csv', index=False)","605d49ea":"#Adicionando na base de teste para envio\ntest['Item_Outlet_Sales']= predET\n# Gerando o arquivo para submeter ao kaggle\ntest[['Item_Identifier','Outlet_Identifier','Item_Outlet_Sales']].to_csv('modelET.csv', index=False)","6fa0907d":"Coluna Outlet_Location_Type n\u00e3o necessita de limpeza","a4119dd5":"**SUBMISS\u00c3O DO MODELO 3**","73742ac3":"Coluna Outlet_Identifier n\u00e3o necessita de limpeza","49ed5976":"Vamos fazer o One Hot Encoder das Variaveis Categoricas","e50c494a":"Temos 2 features que possuem valores ausentes :  Item_Weight  e Outlet_Size","044751cc":"Vamos aplicar as mesmas transforma\u00e7\u00f5es feitas na base de treino tamb\u00e9m na base de teste","57c474f9":"A vari\u00e1vel tem 17% de valores ausentes. Vamos substituir os valores ausentes pelo valor da mediana","50a5e47e":"**MODELOS - PROBLEMA DE REGRESS\u00c3O**","a91f2817":"Coluna Item_Type n\u00e3o necessita de limpeza","2868c74a":"Essa var tem 28% de valores ausentes. Como estrategia, vamos substituir os valores ausentes com uma distriuicao randomica de acordo com a porcentagem das classes.","58f4584b":"A coluna Item_Fat_Content necessita de limpeza. N\u00e3o s\u00e3o 5 categorias, mas sim 2.","bd1c2354":"A coluna Item_Identifier possui muitos valores, alta cardinalidade.","d9f540b3":"**SUBMISS\u00c3O DO MODELO 1**","81bbd1e2":"**SUBMISS\u00c3O DO MODELO 4**","b2450cb1":"**SUBMISS\u00c3O DO MODELO 2**","8ed80e24":"**SUBMISS\u00c3O DO MODELO 5**","9da88d9c":"Primeiramente, vamos analisar as vari\u00e1veis object e ver se h\u00e1 necessidade de alguma limpeza ou tratamento antes de transform\u00e1-las em var. num\u00e9ricas.","7ca390f9":"**DIVIS\u00c3O DAS BASES**","0b1f2f31":"**REALIZANDO TRANSFORMA\u00c7\u00d5ES NOS DADOS**","508700d9":"Coluna Outlet_Size n\u00e3o necessita de limpeza","a3f1dc53":"Agora vamos tratar os valores ausentes","c41ab592":"Coluna Outlet_Type n\u00e3o necessita de limpeza","335a8e15":"Vamos dropar as colunas categoricas e concatenar com as colunas ja dummizadas","69f5054f":"**IMPORTA\u00c7\u00c3O DOS DADOS E VISUALIZA\u00c7\u00c3O DOS DADOS**"}}