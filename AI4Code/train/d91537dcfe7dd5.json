{"cell_type":{"8ec05f00":"code","3cf5e576":"code","6529aa07":"code","34d1ab5f":"code","0168a3ff":"code","72a64b26":"code","558cde18":"code","f5903a54":"code","d057b24b":"code","9761427d":"code","12d8fecd":"code","077810d1":"code","810fbb4d":"code","4b8c4a7a":"code","4dc5629c":"markdown","45f257e8":"markdown","72062eaa":"markdown","aa1b61e1":"markdown","afadb9e3":"markdown","64f18123":"markdown","68cd1062":"markdown","4b469abc":"markdown","db1ce0cf":"markdown","df76949e":"markdown","0da81594":"markdown","7c08cf0f":"markdown"},"source":{"8ec05f00":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","3cf5e576":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\norig_X = pd.read_csv('..\/input\/train.csv')\norig_X['Label'] = 'train'\n\norig_X_test = pd.read_csv('..\/input\/test.csv')\norig_X_test['Label'] = 'test'\n\n# Process as full dataset\norig_X.dropna(axis=0, subset=['Survived'], inplace=True) # Drop rows with uknown survival\nX_full = pd.concat([orig_X.drop('Survived', axis = 1), orig_X_test], axis = 0)\nX_full.drop('PassengerId', axis = 1, inplace=True)\n\n# Select categorical columns\nprint(\"Categorical features: \", [cname for cname in X_full.columns if X_full[cname].dtype == \"object\"])\n\n# Select numeric columns\nprint(\"Numeric features: \", [cname for cname in X_full.columns if X_full[cname].dtype in ['int64', 'float64']])\n\nX_full.head()","6529aa07":"# Determine the number of missing values in each column of training data\nmissing_val_count_by_column = (X_full.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","34d1ab5f":"X_full.dtypes","0168a3ff":"from sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Remove unuseful features\nX_full.drop('Name', axis=1, inplace=True)\nX_full.drop('Ticket', axis=1, inplace=True)\nX_full.drop('Cabin', axis=1, inplace=True)\n\n# Setup method for missing data using a median imputer for important numeric features\nnum_simple_imputer = SimpleImputer(strategy='median')\nnumeric_features = ['Age', 'Fare', 'Pclass', 'SibSp', 'Parch']\nnum_transformed = pd.DataFrame(num_simple_imputer.fit_transform(X_full[numeric_features]))\nnum_transformed.columns = numeric_features\n\n# Setup one hot enoding for catagorical features\ncat_simple_imputer = SimpleImputer(strategy='constant', fill_value='missing')\ncategorical_features = ['Embarked','Sex', 'Label']\ncat_transformed = pd.DataFrame(cat_simple_imputer.fit_transform(X_full[categorical_features]))\ncat_transformed.columns = categorical_features\nX_dummies = pd.get_dummies(cat_transformed, columns = categorical_features)\nX_full = pd.concat([num_transformed, X_dummies], axis = 1)\n\nprint(X_full.dtypes)\nprint(X_full.head())","72a64b26":"import seaborn as sns\ncorr = X_full.corr()\nsns.heatmap(corr, cmap = sns.color_palette(\"coolwarm\", 10))","558cde18":"# Split your data\nX = X_full[X_full['Label_train'] == 1].copy()\nX_test = X_full[X_full['Label_test'] == 1].copy()\n\n# Drop your labels\nX.drop('Label_train', axis=1, inplace=True)\nX.drop('Label_test', axis=1, inplace=True)\nX_test.drop('Label_test', axis=1, inplace=True)\nX_test.drop('Label_train', axis=1, inplace=True)\ny = orig_X.Survived","f5903a54":"# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.7, test_size=0.3,\n                                                                random_state=0)","d057b24b":"# Framework via: https:\/\/machinelearningmastery.com\/feature-importance-and-feature-selection-with-xgboost-in-python\/\nfrom numpy import sort\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBClassifier\n\n# Initial model\nmodel = XGBClassifier(random_state = 18)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_valid)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_valid, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\n# Fit model using each importance as a threshold\nthresholds = sort(model.feature_importances_)\nmodels = []\nfor thresh in thresholds:\n    # Select features using threshold\n    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    # Train model\n    selection_model = XGBClassifier()\n    selection_model.fit(select_X_train, y_train)\n    models.append([selection_model, selection])\n    # Eval model\n    select_X_valid = selection.transform(X_valid)\n    select_y_pred = selection_model.predict(select_X_valid)\n    predictions = [round(value) for value in select_y_pred]\n    accuracy = accuracy_score(y_valid, predictions)\n    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))\n    print(\"MAE: \", mean_absolute_error(y_valid, predictions))","9761427d":"# Finalize transformations\nfinal_model = models[3][0]\nfinal_selection = models[3][1]\nfinal_X_train = final_selection.transform(X_train)\n\nfinal_X_test = final_selection.transform(X_test)\nfinal_X_valid = final_selection.transform(X_valid)\n\nfinal_y_pred = final_model.predict(final_X_valid)\nfinal_predictions = [round(value) for value in final_y_pred]\n\n# Print evaluation metrics\naccuracy = accuracy_score(y_valid, final_predictions)\nprint(\"n=%d, Accuracy: %.2f%%\" % (final_X_train.shape[1], accuracy*100.0))\nprint(\"MAE: \", mean_absolute_error(y_valid, final_predictions))","12d8fecd":"# from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\n\n# Set up GridSearchCV in order to determine the best parameters for a gradient boosting model\ngrid_param = {  \n    'n_estimators': [12, 25, 50, 75],\n    'max_depth': [3, 4, 5],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'early_stopping_rounds': [3, 4, 5, 6]\n    }\n\ngd_sr = GridSearchCV(estimator = final_model, param_grid = grid_param, \n                     cv = 3, n_jobs = -1, verbose = 2)\n\ngd_sr.fit(X_train, y_train)  \nbest_parameters = gd_sr.best_params_\nprint(best_parameters)","077810d1":"selection = SelectFromModel(model, threshold=0.027, prefit=True)\nselect_X_train = selection.transform(X_train)\nselect_X_test = selection.transform(X_test)\nselect_X_valid = selection.transform(X_valid)\nanother_model = XGBClassifier(early_stopping_rounds=3, learning_rate=0.01, max_depth=5, n_estimators=75)\nanother_model.fit(select_X_train, y_train)\n\nselect_y_pred = another_model.predict(select_X_valid)\nselect_predictions = [round(value) for value in select_y_pred]\n\n# Print evaluation metrics\naccuracy = accuracy_score(y_valid, select_predictions)\nprint(\"n=%d, Accuracy: %.2f%%\" % (select_X_train.shape[1], accuracy*100.0))\nprint(\"MAE: \", mean_absolute_error(y_valid, select_predictions))","810fbb4d":"# Form confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_valid, final_y_pred)","4b8c4a7a":"final_preds = another_model.predict(select_X_test)\n\n# Save test predictions to file\noutput = pd.DataFrame({'PassengerId': orig_X_test.PassengerId,'Survived': final_preds})\nprint(output.head(15))\noutput.to_csv('submission.csv', index=False)","4dc5629c":"## Build Final Model Using Best Threshold","45f257e8":"## Import and Explore Data","72062eaa":"## Create Submission","aa1b61e1":"Our approach is to concatenate train and test data for processing, then separate them later for modelling.","afadb9e3":"## Addition as of 6\/6 - Using GridSearch Parameters","64f18123":"Examining the column names, we will keep all but two: name and ticket number. These are additional unique identifiers to PassengerId.","68cd1062":"## Process the Data\nHere we will impute missing values in these columns and transform relevant categorical data through one-hot encoding.","4b469abc":"## Build Initial Model w\/ Feature Importance","db1ce0cf":"## Use of GridSearchCV Search to Determine the Best Model Parameters","df76949e":"## Examine Attribute Correlations","0da81594":"It appears we are missing values from four columns. Two of them are categorical, two of them are numeric.","7c08cf0f":"# Competition Description\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n## Practice Skills\n+ Binary classification\n+ Python and R basics"}}