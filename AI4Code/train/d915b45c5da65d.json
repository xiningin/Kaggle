{"cell_type":{"36a35982":"code","36e27a56":"code","ab57d295":"code","6bebd616":"code","455d9fb9":"code","7b2e249e":"code","56e66708":"code","a908696a":"code","94c2795c":"code","650a96f5":"code","7648fac2":"code","201a4530":"code","472750d7":"code","4025183b":"code","4da712a5":"code","c244c031":"code","7cbddb2d":"code","75d3d148":"code","a6bfefe6":"code","bbd36251":"code","2701d1e4":"code","5e253df3":"code","7aa00701":"code","d7895b53":"code","30b7c8e8":"code","18c282ff":"code","7d4f3bf8":"code","ce3a912d":"code","968c7e62":"code","5bbb1e84":"code","cb85a597":"code","88d90e97":"code","2d17635e":"code","0a7b9ef3":"code","3e2bb40b":"code","c49747a8":"code","0ac25b4d":"code","4ae61512":"code","88479178":"markdown","5ccfb1a5":"markdown","76bc885d":"markdown","2265e004":"markdown","cccf4509":"markdown"},"source":{"36a35982":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py","36e27a56":"!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","ab57d295":"!export XLA_USE_BF16=1\n!pip install -q colored\n!pip install -q efficientnet_pytorch","6bebd616":"import os\nimport gc\nimport cv2\nimport time\nimport numpy as np\nimport pandas as pd\n\nfrom colored import fg, attr\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch import FloatTensor, LongTensor, DoubleTensor\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import Dataset, DataLoader, sampler\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom efficientnet_pytorch import EfficientNet\nfrom albumentations import Normalize, VerticalFlip, HorizontalFlip, Compose","455d9fb9":"W = 512\nH = 512\nFRAC = 0.25\nSPLIT = 0.8\n\nEPOCHS = 2\nLR = 1e-3, 1e-3\nBATCH_SIZE = 32\nVAL_BATCH_SIZE = 128\n\nMODEL = 'efficientnet-b3'\nTEST_IMG_PATH = '..\/input\/siim-isic-melanoma-classification\/jpeg\/test\/'\nTRAIN_IMG_PATH = '..\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'","7b2e249e":"np.random.seed(42)\ntorch.manual_seed(42)","56e66708":"print(os.listdir('..\/input\/siim-isic-melanoma-classification'))","a908696a":"test_df = pd.read_csv('..\/input\/siim-isic-melanoma-classification\/test.csv')\ntrain_df = pd.read_csv('..\/input\/siim-isic-melanoma-classification\/train.csv')","94c2795c":"test_df.head(10)","650a96f5":"train_df.head(10)","7648fac2":"def display_images(num):\n    sq_num = np.sqrt(num)\n    assert sq_num == int(sq_num)\n\n    sq_num = int(sq_num)\n    image_ids = os.listdir(TEST_IMG_PATH)\n    fig, ax = plt.subplots(nrows=sq_num, ncols=sq_num, figsize=(20, 20))\n\n    for i in range(sq_num):\n        for j in range(sq_num):\n            idx = i*sq_num + j\n            ax[i, j].axis('off')\n            img = cv2.cvtColor(cv2.imread(TEST_IMG_PATH + image_ids[idx]), cv2.COLOR_BGR2RGB)\n            ax[i, j].imshow(img); ax[i, j].set_title('Test Image {}'.format(idx), fontsize=12)\n\n    plt.show()","201a4530":"display_images(36)","472750d7":"## Create Feature \n\ntrain_df['sex_feat'] = (train_df['sex'].values == 'male')*1\ntest_df['sex_feat'] = (test_df['sex'].values == 'male')*1\n\ntrain_df['age_approx_feat'] = train_df['age_approx'].fillna(train_df['age_approx'].mean())\ntest_df['age_approx_feat'] = test_df['age_approx'].fillna(test_df['age_approx'].mean())","4025183b":"train_df['age_approx_feat'] = train_df['age_approx_feat'] \/train_df['age_approx_feat'].values.max()","4da712a5":"test_df['age_approx_feat'] = test_df['age_approx_feat'] \/test_df['age_approx_feat'].values.max()","c244c031":"train_df['anatom_site_general_challenge_feat'] = train_df['anatom_site_general_challenge'].fillna('unknown')\ntest_df['anatom_site_general_challenge_feat'] = test_df['anatom_site_general_challenge'].fillna('unknown')","7cbddb2d":"train_df['diagnosis'] = train_df['diagnosis'].fillna('na')\n","75d3d148":"from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\nfor df in [train_df,test_df]:\n    df['anatom_site_general_challenge_feat_c'] = lb_make.fit_transform(df['anatom_site_general_challenge_feat'])\n\ntrain_df['diag_aux'] = lb_make.fit_transform(train_df['diagnosis'])\n\n","a6bfefe6":"feat_columns=['sex_feat','age_approx_feat','anatom_site_general_challenge_feat_c']","bbd36251":"train_feat = train_df[feat_columns].copy()\ntest_feat = test_df[feat_columns].copy()","2701d1e4":"def ToTensor(data):\n    return [FloatTensor(point) for point in data]\n\nclass SIIMFeatDataset(Dataset):\n    def __init__(self, df, aug, targ, ids, path):\n        self.df, self.targ, self.aug = df, targ, aug\n\n        self.mu = [0.485, 0.456, 0.406]\n        self.sigma = [0.229, 0.224, 0.225]\n        self.img_ids, self.img_path = ids, path\n        self.norm = Normalize(mean=self.mu, std=self.sigma, p=1)\n        self.vflip, self.hflip = VerticalFlip(p=0.5), HorizontalFlip(p=0.5)\n        \n        if self.aug: self.transformation = self.norm\n        else: self.transformation = Compose([self.norm, self.vflip, self.hflip])\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def __getitem__(self, i):\n        feat_columns=['sex_feat','age_approx_feat','anatom_site_general_challenge_feat_c'] \n        target = [self.df.target[i]] if self.targ else 0\n        feat = self.df[feat_columns].values[i]\n        image = cv2.imread(self.img_path + self.img_ids[i])\n        image = cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), (H, W))\n        return ToTensor([self.transformation(image=image)['image'], target,feat])","5e253df3":"### Checking the dataset function\ntrain_ids = train_df.image_name.apply(lambda x: x + '.jpg')\n\ntrain_set = SIIMFeatDataset(train_df, True, True, train_ids, TRAIN_IMG_PATH)\nout = train_set[0]\nout","7aa00701":"def GlobalAveragePooling(x):\n    return x.mean(axis=-1).mean(axis=-1)\n\nclass CancerNet(nn.Module):\n    def __init__(self, features):\n        super(CancerNet, self).__init__()\n        self.avgpool = GlobalAveragePooling\n        self.dense_output = nn.Linear(features, 1)\n        self.efn = EfficientNet.from_pretrained(MODEL)\n        \n    def forward(self, x):\n        x = x.view(-1, 3, H, W)\n        x = self.efn.extract_features(x)\n        return self.dense_output(self.avgpool(x))","d7895b53":"class CancerNet2(nn.Module):\n    def __init__(self, features,num_patient_feat):\n        super(CancerNet2, self).__init__()\n        self.avgpool = GlobalAveragePooling\n        self.dense_output = nn.Linear(features, 64)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.num_patient_feat = num_patient_feat \n        self.l0=nn.Linear(self.num_patient_feat,64)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.dropout = nn.Dropout(0.3)\n        self.conc_feat =128\n        self.final_output = nn.Linear(128,1)\n        self.efn = EfficientNet.from_pretrained(MODEL)\n        \n    def forward(self, x,train_patient_feat):\n        x = x.view(-1, 3, H, W)\n        x = self.efn.extract_features(x)\n\n        x = self.avgpool(x)\n       \n        x = self.dense_output(x)\n        x=  self.dropout(x)\n        x_l0 = self.bn2(F.relu(self.l0(train_patient_feat)))\n        x_l0 = self.dropout(x_l0)\n\n        x = torch.cat((x,x_l0),1)\n        \n        \n        x = self.final_output(x)\n        return x","30b7c8e8":"## Check Net\nimport torch.nn.functional as F\ndevice = xm.xla_device()\ntrain_feat_dummy =torch.Tensor(np.random.randn(32,3)).to(device)\nimg_dummy = torch.Tensor(np.random.randn(32,3,512,512)).to(device)\nnet=CancerNet2(1536,3).to(device)\nout = net(img_dummy,train_feat_dummy)","18c282ff":"from sklearn.metrics import accuracy_score, roc_auc_score\n\ndef bce(y_true, y_pred):\n    return nn.BCEWithLogitsLoss()(y_pred, y_true)\n\n\ndef roc_auc(y_true, y_pred):\n    y_true = y_true.squeeze().cpu().detach().numpy()\n    y_pred = nn.Sigmoid()(y_pred.squeeze()).cpu().detach().numpy()\n    return roc_auc_score(y_true,y_pred)","7d4f3bf8":"def print_metric(data, batch, epoch, start, end, metric, typ):\n    t = typ, metric, \"%s\", data, \"%s\"\n    if typ == \"Train\": pre = \"BATCH %s\" + str(batch-1) + \"%s  \"\n    if typ == \"Val\": pre = \"\\nEPOCH %s\" + str(epoch+1) + \"%s  \"\n    time = np.round(end - start, 1); time = \"Time: %s{}%s s\".format(time)\n    fs = [(fg(211), attr('reset')), (fg(212), attr('reset')), (fg(213), attr('reset'))]\n    xm.master_print(pre % fs[0] + \"{} {}: {}{}{}\".format(*t) % fs[1] + \"  \" + time % fs[2])","ce3a912d":"class ImbalancedSampler(sampler.Sampler):\n\n    def __len__(self):\n        return self.num_samples\n    \n    def _get_label(self, dataset, idx):\n        return dataset.df[\"target\"][idx]\n\n    def __iter__(self):\n        return (self.indices[i] for i in self._get_probs())\n    \n    def _get_weight(self, idx, count_dict):\n        return 1.0\/count_dict[self._get_label(self.dataset, idx)]\n    \n    def _get_probs(self):\n        return torch.multinomial(self.weights, self.num_samples, replacement=True)\n\n    def __init__(self, dataset, indices=None, num_samples=None):\n        self.indices = list(range(len(dataset))) if indices is None else indices\n        self.num_samples = len(self.indices) if num_samples is None else num_samples\n\n        count = {}\n        self.dataset = dataset\n        for idx in self.indices:\n            label = self._get_label(dataset, idx)\n            if label in count: count[label] += 1\n            if label not in count: count[label] = 1\n\n        self.weights = DoubleTensor([self._get_weight(idx, count) for idx in self.indices])","968c7e62":"cut = int(FRAC*len(train_df))\ntrain_df = shuffle(train_df).reset_index(drop=True).loc[:cut]\n\nsplit = int(SPLIT*len(train_df))\ntrain_df, val_df = train_df.loc[:split], train_df.loc[split:]\ntrain_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)","5bbb1e84":"val_ids = val_df.image_name.apply(lambda x: x + '.jpg')\ntrain_ids = train_df.image_name.apply(lambda x: x + '.jpg')\n\nval_set = SIIMFeatDataset(val_df, False, True, val_ids, TRAIN_IMG_PATH)\ntrain_set = SIIMFeatDataset(train_df, True, True, train_ids, TRAIN_IMG_PATH)\n\ntrain_sampler = ImbalancedSampler(train_set)\nval_loader = DataLoader(val_set, VAL_BATCH_SIZE, shuffle=False)\ntrain_loader = DataLoader(train_set, BATCH_SIZE, sampler=train_sampler)\n\ndevice = xm.xla_device()\nnetwork = CancerNet2(features=1536,num_patient_feat=3).to(device)\noptimizer = Adam([{'params': network.efn.parameters(), 'lr': LR[0]},\n                  {'params': network.dense_output.parameters(), 'lr': LR[1]}])","cb85a597":"start = time.time()\nxm.master_print(\"STARTING TRAINING ...\\n\")\n\nfor epoch in range(EPOCHS):\n    fonts = (fg(48), attr('reset'))\n    xm.master_print((\"EPOCH %s\" + str(epoch+1) + \"%s\") % fonts)\n    \n    batch = 1\n    network.train()\n    for train_batch in train_loader:\n        train_img, train_targ ,train_feat= train_batch\n        train_targ = train_targ.view(-1, 1)\n        train_img, train_targ,train_feat = train_img.to(device), train_targ.to(device),train_feat.to(device)\n            \n        train_preds = network.forward(train_img,train_feat)\n        train_auc = roc_auc(train_targ, train_preds)\n        train_loss = bce(train_targ, train_preds)\n            \n        optimizer.zero_grad()\n        train_loss.backward()\n        xm.optimizer_step(optimizer, barrier=True)\n            \n        end = time.time()\n        batch = batch + 1\n        auc = np.round(train_auc.item(), 3)\n        if batch %1 ==0:\n            print_metric(auc, batch, 0, start, end, metric=\"roc-auc\", typ=\"Train\")\n            \n    network.eval()\n    val_loss, val_auc, val_points = 0, 0, 0\n        \n    with torch.no_grad():\n        for val_batch in tqdm(val_loader):\n            val_img, val_targ,val_feat = val_batch\n            val_targ = val_targ.view(-1, 1)\n            val_img, val_targ,val_feat = val_img.to(device), val_targ.to(device),val_feat.to(device)\n\n            val_points += len(val_targ)\n            val_preds = network.forward(val_img,val_feat)\n            val_auc += roc_auc(val_targ, val_preds).item()*len(val_preds)\n            val_loss += bce(val_targ, val_preds).item()*len(val_preds)\n        \n    end = time.time()\n    val_auc \/= val_points\n    val_loss \/= val_points\n    auc = np.round(val_auc, 3)\n    print_metric(auc, 0, epoch, start, end, metric=\"roc-auc\", typ=\"Val\")\n    \n    xm.master_print(\"\")\n\nxm.master_print(\"\\nENDING TRAINING ...\")","88d90e97":"def display_preds(num,test_df):\n    sq_num = np.sqrt(num)\n    assert sq_num == int(sq_num)\n\n    sq_num = int(sq_num)\n    image_ids = os.listdir(TEST_IMG_PATH)\n    fig, ax = plt.subplots(nrows=sq_num, ncols=sq_num, figsize=(20, 20))\n    norm = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1)\n    feat_columns=['sex_feat','age_approx_feat','anatom_site_general_challenge_feat_c'] \n    \n    for i in range(sq_num):\n        for j in range(sq_num):\n            idx = i*sq_num + j\n            ax[i, j].axis('off')\n            pred_dict = {0: '\"No-Melanoma\"', 1: '\"Melanoma\"'}\n            print(image_ids[idx])\n            img = cv2.resize(cv2.cvtColor(cv2.imread(TEST_IMG_PATH + image_ids[idx]), cv2.COLOR_BGR2RGB), (H, W))\n            pred = nn.Sigmoid()(network.forward(FloatTensor(norm(image=img)['image'].reshape(1, 3, H, W)).to(device),FloatTensor(test_df.loc[test_df.image_name==image_ids[idx].split('.')[0]][feat_columns].values).to(device)))\n            ax[i, j].imshow(img); ax[i, j].set_title('Prediction: {}'.format(pred_dict[round(pred.item())]), fontsize=12)\n\n    plt.show()","2d17635e":"display_preds(16,test_df)","0a7b9ef3":"def sigmoid(x):\n    return 1\/(1 + np.exp(-x))\n\ntest_ids = test_df.image_name.apply(lambda x: x + '.jpg')\ntest_set = SIIMFeatDataset(test_df, False, False, test_ids, TEST_IMG_PATH)\ntest_loader = tqdm(DataLoader(test_set, VAL_BATCH_SIZE, shuffle=False))\n\nnetwork.eval()\ntest_preds = []\nwith torch.no_grad():\n    for test_batch in test_loader:       \n        test_img,label, test_feat = test_batch\n        test_img = test_img.to(device)\n        test_feat = test_feat.to(device)\n        test_preds.extend(network.forward(test_img,test_feat).squeeze().detach().cpu().numpy())","3e2bb40b":"path = '..\/input\/siim-isic-melanoma-classification\/'\nsample_submission = pd.read_csv(path + 'sample_submission.csv')","c49747a8":"sample_submission.target = sigmoid(np.array(test_preds))","0ac25b4d":"sample_submission.head()","4ae61512":"sample_submission.to_csv('submission.csv', index=False)","88479178":"## Run inference on the test data\n\n* Next I will run inference on the test data and store the test predictions in a list.\n* These predictions are logits and will be converted to probabilities later using <code>sigmoid<\/code>.","5ccfb1a5":"## Visualize sample test predictions\n\n* Now since the model is trained, we will visualize predictions made on unseen test images.","76bc885d":"## As you can mainly see , the maximum portion of code is copied from this two kernels. I removed the notes from Tarun's Kernel because , he has already made a great kernel , it will be redundant if i paste here .\n\nhttps:\/\/www.kaggle.com\/tarunpaparaju\/siim-isic-melanoma-eda-pytorch-baseline\n\n\nhttps:\/\/www.kaggle.com\/tunguz\/melanoma-classification-eda-and-modeling\n","2265e004":"## This is a simple baseline implementation for multinput pytorch pipeline to combine both Patient Features and Image Features . I am just trying it out . It might be wrong as well , so please take it with a pinch of salt . Inspiration is below , However , this version focuses on creating a simple pipeline with few features and simple network . \n\n![image.png](attachment:image.png)","cccf4509":"#### Note : Something wrong with the metrics . May be someone can correct me ."}}