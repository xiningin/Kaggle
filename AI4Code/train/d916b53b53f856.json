{"cell_type":{"5a8d5210":"code","ecf7d949":"code","a01c7f00":"code","133648ce":"code","0589b71a":"code","f0a80f70":"code","561d6462":"code","a70f641d":"code","ee41b3b1":"code","7a0c0ef0":"code","c6f23e8d":"code","c77e0eab":"code","dde024c0":"code","62646d5f":"code","7b1eb0fa":"code","a862794b":"code","87a8b615":"code","b09091ef":"code","1a866605":"code","03e714d9":"code","c4f28db2":"code","29dabd45":"code","237c6668":"code","ec341097":"code","fabe2439":"code","421cff5d":"code","242c04ea":"code","7bab922a":"code","e467e829":"code","48adb0ab":"code","d559a8cc":"code","28117c5d":"code","28c59940":"code","7c7ede2c":"code","8e8c724f":"code","3014a194":"code","f199ec25":"code","fcd86d6b":"code","b36bc5c4":"code","2c1e7ad7":"code","9e91ae4e":"code","d55769aa":"code","df77014c":"code","7b699213":"code","4d6b8246":"code","92663377":"markdown","6117d10c":"markdown","15962f86":"markdown","bec037ba":"markdown","5667420f":"markdown","a48d073f":"markdown","9f282093":"markdown","dfad5cca":"markdown","3dc65098":"markdown","03272023":"markdown","87da12d1":"markdown","8444c03d":"markdown","2a199262":"markdown","3c2c3b1c":"markdown","3d82044b":"markdown","336a54e2":"markdown","28321825":"markdown","ba6d630f":"markdown","02490456":"markdown","a10265db":"markdown","c2324ddd":"markdown","7a7689b8":"markdown","4a0b06fc":"markdown","625d6d5e":"markdown","712aa870":"markdown","7d715195":"markdown","465b640b":"markdown","fdabe178":"markdown","b8a58a50":"markdown"},"source":{"5a8d5210":"# Loading of libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib as mp\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.model_selection import GridSearchCV","ecf7d949":"d = pd.read_csv(\"..\/input\/dissolved-oxygen-prediction-in-river-water\/train.csv\")\nd.head(5)","a01c7f00":"# Here we have a statistical look at the data\nd.describe()","133648ce":"d.isna().sum()","0589b71a":"#Our total rows and columns\nd.shape","f0a80f70":"#Creating our sample data set.\n\nd1 = d[['target',\"O2_1\", \"O2_2\"]]\nd1","561d6462":"d1.isna().sum()","a70f641d":"# dropping the two missing values\nd2 = d1.dropna()\nd2","ee41b3b1":"d2.isna().sum()","7a0c0ef0":"# First a scatter plot is created to view the distribution of the data.\n\nscatter_matrix(d2, figsize=(10,5))","c6f23e8d":"a1 = sns.distplot(d2)","c77e0eab":"d2.plot(kind='box')\n","dde024c0":"d2.plot(kind=\"line\", figsize=(8,4))","62646d5f":"d2[['O2_1', 'O2_2']].plot(kind=\"line\", figsize=(8, 4))","7b1eb0fa":"d2.describe()","a862794b":"#replacing the maximum values which seem to be outlairs with a mean value\nd3 = d2.replace(d2[\"O2_1\"].max(), value=d2[\"O2_1\"].mean())\nd4 = d3.replace(d2[\"O2_2\"].max(), value=d2[\"O2_2\"].mean())\n\nd4[['target','O2_1' ,'O2_2']].plot(kind=\"line\", figsize=(7,4))","87a8b615":"a = sns.distplot(d4)","b09091ef":"# Creating the target and feature sets\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nX = d2.drop('target', axis=1)\ny = d2[['target']]\n","1a866605":"# Creation of the train test split sets as well as standardisation of the data.\n\nx_train, x_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=35)\nX_train = preprocessing.StandardScaler().fit(x_train).transform(x_train)\nX_test = preprocessing.StandardScaler().fit(x_test).transform(x_test)\n\n\n## change to 1d array\ny_train = np.array(y_train)\ny_train = y_train.ravel()\n\ny_test = np.array(y_test)\ny_test = y_test.ravel()\n","03e714d9":"# Testing a simple linear regression\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import linear_model\nreg = linear_model.LinearRegression(fit_intercept=True)\nreg.fit(X_train, y_train)\n\n\nprint('R2 model score:  ', reg.score(X_test, y_test))\nprint('RMSE    :  ', np.sqrt(mean_squared_error(reg.predict(X_test), y_test)))","c4f28db2":"#define a function to test several regression models\n\ndef model_result(m_odel):\n    m = m_odel\n    m.fit(X_train, y_train)\n    print('R2 model score:  ', m.score(X_test, y_test))\n    print('RMSE    :  ', np.sqrt(mean_squared_error(m.predict(X_test), y_test)))\n    ","29dabd45":"#testing ridge regression\nfrom sklearn.linear_model import Ridge\nmodel_result(m_odel=Ridge(alpha = 1, random_state = 42))","237c6668":"#testing Lasso regression\nfrom sklearn.linear_model import Lasso\nmodel_result(m_odel=Lasso(alpha = 1, random_state = 42))","ec341097":"#testing huber regression\nfrom sklearn.linear_model import HuberRegressor\nmodel_result(m_odel=HuberRegressor())","fabe2439":"from sklearn.linear_model import ElasticNet\nmodel_result(m_odel=ElasticNet(alpha = 1, random_state = 42))","421cff5d":"##support vector regressor\n\nfrom sklearn.svm import LinearSVR\n\nsvm_reg = LinearSVR(epsilon=2.5543, random_state=42)\nsvm_reg.fit(X_train, y_train)\nsvm_reg.score(X_test, y_test)","242c04ea":"#random forest\n\nfrom sklearn.ensemble import RandomForestRegressor\nftree = RandomForestRegressor(max_depth=2,random_state=42)\nftree.fit(X_train, y_train)","7bab922a":"pre = ftree.predict(X_test)\nf = mean_squared_error(y_test, pre)\nFs = np.sqrt(f)\nprint(\"RMSE\",Fs)\nprint(\"Accuracy\", ftree.score(X_test, y_test))","e467e829":"\n#Testing polynomial regression\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\n\nX_poly = poly_features.fit_transform(X)\n\nXp_train, Xp_test, yp_train, yp_test = train_test_split(X_poly, y, test_size=0.2, random_state=35)\n\nyp_train = np.array(yp_train)\nyp_train = yp_train.ravel()\n\nyp_test = np.array(yp_test)\nyp_test = yp_test.ravel()\n","48adb0ab":"# define mode to test accuracy\n\ndef model_resultp(m_odelp):\n    mp = m_odelp\n    mp.fit(Xp_train, yp_train)\n    print('R2 model score =',mp.score(Xp_test, yp_test))\n    print('RMSE =',np.sqrt(mean_squared_error(mp.predict(Xp_test), yp_test)))\n  ","d559a8cc":"# Here we test the accuracy of the linear model with polynomial features\n\nmodel_resultp(m_odelp=linear_model.LinearRegression(fit_intercept=True))","28117c5d":"# Here we test the accuracy of the Ridge regression model with polynomial features\n\nmodel_resultp(m_odelp=Ridge(alpha = 1, random_state = 42))","28c59940":"# Here we test the accuracy of the Lasso regression model with polynomial features\n\nmodel_resultp(m_odelp=Lasso(alpha = 1, random_state = 42))","7c7ede2c":"# Here we test the accuracy of the Huber regression model with polynomial features\n\nmodel_resultp(m_odelp=HuberRegressor())","8e8c724f":"# Here we test the accuracy of the Elastic net model with polynomial features\n\nmodel_resultp(m_odelp=ElasticNet(alpha = 1, random_state = 42))","3014a194":"#Tunning for ridge regression\n\nparam_grid2 = [\n \n {'alpha' : np.logspace(-1,0.00001,1000), 'max_iter' : [1000], \n  \"fit_intercept\": [True, False], \"solver\": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}]\nmodel2 = Ridge(random_state=42 )\ngrid_search2 = GridSearchCV(model2, param_grid2, cv=5,\n scoring='neg_mean_squared_error',\nreturn_train_score=True)\ngrid_search2.fit(X_train, y_train)\n\nRRg = grid_search2.best_estimator_\nFL1 = RRg.predict(X_test)\n\nrmse3 = np.sqrt(mean_squared_error(FL1, y_test))\n\n\n\nprint(\"RSE\",rmse3)\nridge_score = RRg.score(X_test, y_test)\nprint(\"accuracy score\", ridge_score)","f199ec25":"#cross validation on polynomial features\n\nmodelsT = [Ridge, Lasso, ElasticNet, HuberRegressor]\nmodel_names = ['ridge', 'lasso', 'elasticnet', 'huber']\n\nfor x in range(len(modelsT)):\n    print(model_names[x])\n    \n    param_grid = {'alpha' : np.logspace(-1,0.009,2500),\n                  'max_iter' : [1000]}\n    lin_model  = modelsT[x]() \n    model_cv   = GridSearchCV(estimator  = lin_model, \n                        param_grid = [param_grid],\n                        cv = 5,\n                        scoring='neg_mean_squared_error', \n                        n_jobs = -1,\n                        verbose = 1)\n    model_cv.fit(Xp_train, yp_train)\n\n    best_model              = model_cv.best_estimator_\n    print(best_model)\n    bestmodelFitTime        = model_cv.cv_results_['mean_fit_time'][model_cv.best_index_]\n    bestmodelScoreTime      = model_cv.cv_results_['mean_score_time'][model_cv.best_index_]\n    best_model.fit(Xp_train, yp_train)\n    print('R2 score: ', best_model.score(Xp_test, yp_test))\n\n    \n    y_pred = best_model.predict(Xp_test)\n    rmse   = np.sqrt(mean_squared_error(y_pred, yp_test))\n    print('Test RMSE : ', rmse)\n    print(\"**********************************\")","fcd86d6b":"my_model = Lasso(alpha=0.25455422642263903, copy_X=True, fit_intercept=True, max_iter=1000,\n      normalize=False, positive=False, precompute=False, random_state=None,\n      selection='cyclic', tol=0.0001, warm_start=False)\n\nmy_model.fit(Xp_train, yp_train)\nmy_model.score(Xp_test, yp_test)","b36bc5c4":"# Predicting the target 02 concentrations in the remainded of the test set.\nmy_model.predict(Xp_test)","2c1e7ad7":"##Loading test data\nTest = pd.read_csv(\"..\/input\/dissolved-oxygen-prediction-in-river-water\/test.csv\")\nTest","9e91ae4e":"Test.describe()","d55769aa":"# We notice there is an outlier in O2_2 measurements so we replace it with the mean\nTest = Test.replace(Test[\"O2_2\"].max(), value=Test[\"O2_2\"].mean())","df77014c":"#creating o2 data set\nTest_X = Test[[\"O2_1\", \"O2_2\"]]\nTest_X","7b699213":"# Creating and fitting ploynomial features\n\npoly_features_X = PolynomialFeatures(degree=2, include_bias=False)\n\nTes_X_poly = poly_features_X.fit_transform(Test_X)","4d6b8246":"# Here we predict the target station O2 concentration from test set\n\nmy_model.predict(Tes_X_poly)","92663377":"##### Now cross validation will be used to fine the best model with **polynomial features**.","6117d10c":"### In this step we try using polynomial features see the effect on the accuracy. This involves:","15962f86":"###### There seems to be an abnormal spike in O2 levels at two of the station. Lets have a closer look","bec037ba":"###### With the outlaired removed, we can move onto the next step.","5667420f":"##### There were high amounts of null values in the data set and as a result the model had to be built on 2 feature sets - station 1 and 2. Further more there were outlier values not only present in the O2 data but within the entire data set. The final, best accuracy score which I was able to get, given these constraints was 72%. ","a48d073f":"### Data Modeling","9f282093":"##### Aftertuning, the ridge regression modelimproved slightly from to **67.38%** to **67.40%**","dfad5cca":"## Data Exploration","3dc65098":"##### Test set O2 predictions","03272023":"##### For predicting O2 levels at the target station, we will use O2 reading from station 1 and 2 which has significantly less missing data compared to over 50% in the other columns.","87da12d1":"### Testing various regression models","8444c03d":"## Predicting the target station from test data","2a199262":"###### We also note that the random forest performs even more poorly.","3c2c3b1c":"##### First we will tune the hyper parameters for the **ridge regression model with linear features**.","3d82044b":"##### From our observation so far, the Ridge regression model had the highest accuracy at **67.38%**","336a54e2":"## Testing a Random forest model","28321825":"###### The accuracy is not as high as with the linear model","ba6d630f":"##### - importing polynomial features\n##### - transforming features\n##### - creating new train test splits","02490456":"###### A few sigle widely dispursed outlaires can be observed from the scatter plots and further observed in the histogram.\n###### In order to get a better picture, a line graph wil now be created.","a10265db":"###### There are high rates of null values from stations 3 - 7 within the entire data set.","c2324ddd":"#### The best performing model turned out to be the Lasso regression with alpha = 0.254. An accuracy of **72.69%** was achieved!","7a7689b8":"###### Concentrations of approximately 46.95 and 40.9 are observed at this stations which is clearly abnormal. From further [investigations](https:\/\/www.fondriest.com\/environmental-measurements\/parameters\/water-quality\/dissolved-oxygen\/#:~:text=As%20oxygen%20in%20the%20atmosphere,100%25%20air%20saturation%20at%20equilibrium.), such high levels of O2 in river water, especially in this data set, might be an error.\n###### These vaues will be removed and replaced with the mean of the respective column.","4a0b06fc":"## Data Visualization","625d6d5e":"###### Creating our data set with O2 measures","712aa870":"##### Now that we have isolated the best performing models, we will now twick the parameters in order to get the highest accuracy","7d715195":"## Model Fine Tuning.","465b640b":"### Conclusion\n","fdabe178":"##### There is a huge improvement in model accuracy when using polynomial features. Results all range between **71** and **72%!**","b8a58a50":"## Testing a support vector model"}}