{"cell_type":{"649943f9":"code","82f6c38e":"code","1302b21b":"code","5902575e":"code","1329848b":"code","fbc803d2":"code","927ec971":"code","e519b68e":"code","fcc6fa59":"code","0e1ad9fd":"code","080633c9":"code","225987e3":"code","b3ed4080":"code","6d5dbed5":"code","e3f6bd52":"code","2e66c96a":"code","e41cf7db":"code","f3e1d340":"code","6627628f":"code","c1d6522c":"code","e98079a4":"code","c4513ceb":"code","ed3f0404":"code","a9459b5f":"code","eb56b75d":"code","b37cec56":"code","2326a14f":"code","ed05cd3c":"code","d80effc5":"code","9f99b852":"code","2a9db1f2":"code","31f3c712":"code","65a795f5":"markdown","ab5c2ca7":"markdown","bd249207":"markdown","0e63a9b0":"markdown","c82cfec1":"markdown","d9416449":"markdown","78c6af55":"markdown","5865bb7d":"markdown","e58993f6":"markdown","7c638780":"markdown","31a2cea5":"markdown","136ae8fb":"markdown","18dffd01":"markdown","00e9cb1a":"markdown","a6224e8d":"markdown","00f10b70":"markdown","9974a6e1":"markdown","251badff":"markdown"},"source":{"649943f9":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\n        \ninput_path = Path('\/kaggle\/input\/tabular-playground-series-feb-2021\/')","82f6c38e":"train = pd.read_csv(input_path \/ 'train.csv', index_col='id')\npd.set_option('display.max_columns', None)\ndisplay(train.head())","1302b21b":"train.describe()","5902575e":"test = pd.read_csv(input_path \/ 'test.csv', index_col='id')\ndisplay(test.head())","1329848b":"test.describe()","fbc803d2":"submission = pd.read_csv(input_path \/ 'sample_submission.csv')\ndisplay(submission.head())","927ec971":"target = train.pop('target')","e519b68e":"cols = train.columns\nnum_cols = train._get_numeric_data().columns\ncat_features= list(set(cols) - set(num_cols))\ncat_features.sort()","fcc6fa59":"boxplot = train.boxplot(column=num_cols.values.tolist(),\n                       figsize=(12,9))","0e1ad9fd":"boxplot = test.boxplot(column=num_cols.values.tolist(),\n                       figsize=(12,9))","080633c9":"fig, ax = plt.subplots(figsize=(20,10))\nmatrix = np.triu(train.corr())\ncor = train.corr()\nsns.heatmap(cor, annot=True, mask=matrix,cmap= 'coolwarm', linewidths=.5, ax=ax)","225987e3":"plt.figure(figsize=(20,20))\nfor i, col in enumerate(cat_features):\n    plt.subplot(5,2,i+1)\n    sns.countplot(x=col,data=train, order=('A','B','C','D','E','F','G','H','I','J','K','L','N'))\nplt.tight_layout()","b3ed4080":"plt.figure(figsize=(20,20))\nfor i, col in enumerate(cat_features):\n    plt.subplot(5,2,i+1)\n    sns.countplot(x=col,data=test, order=('A','B','C','D','E','F','G','H','I','J','K','L','N'))\nplt.tight_layout()","6d5dbed5":"for feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train[feature])\n    train[feature] = le.transform(train[feature])\n    test[feature] = le.transform(test[feature])","e3f6bd52":"train.head()","2e66c96a":"train.shape","e41cf7db":"test.shape","f3e1d340":"\nX_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.80)  # change 60 to 80 ","6627628f":"from sklearn.dummy import DummyRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import LassoLars\nfrom sklearn.linear_model import ARDRegression\nfrom sklearn.linear_model import PassiveAggressiveRegressor\nfrom sklearn.linear_model import TheilSenRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBRegressor","c1d6522c":"def plot_results(name, y, yhat, num_to_plot=10000, lims=(0,12), figsize=(6,6)):\n    plt.figure(figsize=figsize)\n    score = mean_squared_error(y, yhat, squared=False)\n    plt.scatter(y[:num_to_plot], yhat[:num_to_plot])\n    plt.plot(lims, lims)\n    plt.ylim(lims)\n    plt.xlim(lims)\n    plt.title(f'{name}: {score:0.5f}', fontsize=18)\n    plt.show()","e98079a4":"def FitAndScoreModel(df,name, model,X_tr,y_tr,X_tst,y_tst):\n    model.fit(X_tr,y_tr)\n    Y_pred = model.predict(X_tst)\n    score=mean_squared_error(y_tst, Y_pred, squared=False)\n    df = df.append({'Model':name, 'MSE': score},ignore_index = True) \n   # plot_results(name, y_test, Y_pred)\n    return df","c4513ceb":"dResults = pd.DataFrame(columns = ['Model', 'MSE'])","ed3f0404":"classifiers = [\n    DummyRegressor(strategy='median'),\n   # SVR(),\n    SGDRegressor(),\n    BayesianRidge(),\n    LassoLars(),\n    ARDRegression(),\n    LinearRegression(),\n    LGBMRegressor(),\n    RandomForestRegressor(n_estimators=50, n_jobs=-1)]\n\n \n#for item in classifiers:\n#    print(item)\n#    clf = item\n#    dResults=FitAndScoreModel(dResults,item,item,X_train,y_train,X_test,y_test) \n    ","a9459b5f":"#dResults.sort_values(by='MSE', ascending=True,inplace=True)\n#dResults.set_index('MSE',inplace=True)\n#dResults.head(dResults.shape[0])","eb56b75d":"import optuna\nimport sklearn\n\noptuna.logging.set_verbosity(optuna.logging.ERROR)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef objective(trial):    \n    list_bins = [25, 50, 75, 100, 125, 150, 175, 200, 225, 250,500,750,1000]   \n\n    param = {\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.006,0.008,0.01,0.014,0.017,0.02,0.05]),\n        'max_depth': trial.suggest_categorical('max_depth', [10,20,50,100]),\n        'num_leaves' : trial.suggest_int('num_leaves', 2, 1000),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 15),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 400),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 1, 256),\n        'cat_l2' : trial.suggest_int('cat_smooth', 1, 256),\n        'max_bin': trial.suggest_categorical('max_bin', list_bins)\n    }\n    \n\n    model = LGBMRegressor(**param,objective='regression',metric= 'rmse',boosting_type='gbdt',verbose=-1,random_state=42,n_estimators=20000,cat_feature= [x for x in range(len(cat_features))])\n    \n    \n    model.fit(X_train, y_train,eval_set=[(X_test,y_test)], early_stopping_rounds=150,verbose=False)\n    \n    preds = model.predict(X_test)\n    \n    rmse = mean_squared_error(y_test, preds,squared=False)\n    \n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=300)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n    \n","b37cec56":"params = study.best_params\nparams","2326a14f":"study.best_value","ed05cd3c":"#Visualize parameter importance.\noptuna.visualization.plot_param_importances(study)","d80effc5":"#plot_optimization_history: shows the scores from all trials as well as the best score so far at each point.\noptuna.visualization.plot_optimization_history(study)","9f99b852":"from sklearn.model_selection import KFold\n\nn_fold = 20\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\ntrain_columns = train.columns.values\n\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train, target.values)):\n    \n    strLog = \"fold {}\".format(fold_)\n    print(strLog)\n    \n    X_tr, X_val = train.iloc[trn_idx], train.iloc[val_idx]\n    y_tr, y_val = target.iloc[trn_idx], target.iloc[val_idx]\n\n    model = LGBMRegressor(**params, objective='regression',metric= 'rmse',boosting_type='gbdt',random_state=42,verbose=-1,n_estimators=20000,cat_feature= [x for x in range(len(cat_features))])\n   \n    model.fit(X_tr, y_tr, \n              eval_set=[(X_tr, y_tr), (X_val, y_val)], eval_metric='rmse',\n              verbose=-1, early_stopping_rounds=400)\n    \n    \n    oof[val_idx] = model.predict(X_val, num_iteration=model.best_iteration_)\n\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = train_columns\n    fold_importance_df[\"importance\"] = model.feature_importances_[:len(train_columns)]\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += model.predict(test, num_iteration=model.best_iteration_) \/ folds.n_splits\n\n","2a9db1f2":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:3014].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure()\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()","31f3c712":"LGBMsubmission=submission.copy()\nLGBMsubmission['target'] = predictions\nLGBMsubmission.to_csv('submission_LGBM.csv', header=True, index=False)\nLGBMsubmission.head()","65a795f5":"Again we see a symetry between train and test data ","ab5c2ca7":"We see some strong correlations that could be explore further","bd249207":"## We need to encode the categoricals.\n\nThere are different strategies to accomplish this, and different approaches will have different performance when using different algorithms. For this starter notebook, we'll use simple encoding.","0e63a9b0":"## Setup and Read Data","c82cfec1":"Hmm, data looks very symetrical (at least on numericals) in test and train, outliers and all","d9416449":"## Submission of Results","78c6af55":"## Lets Look at our Features","5865bb7d":"## LGBM Tuning","e58993f6":"## Review Categorical Data","7c638780":"## Read in the data files","31a2cea5":"Optuna is the best out of those tested. Let's tune it.","136ae8fb":"# Feb 2021 Tabular Playground:LGBM with Optuna Tuning ","18dffd01":"## Look at Correlation","00e9cb1a":"## Make a validation split","a6224e8d":"## Model Evaluation","00f10b70":"## Pull out the target","9974a6e1":"## Identify Categorical Columns","251badff":" This note book is a respomse to to the Kaggle [Tabular Playground Series - Feb 2021 competition.](http:\/\/https:\/\/www.kaggle.com\/c\/tabular-playground-series-feb-2021).  The Approach I have taken is as follows:\n \n* Setup including reading in the data \n* Examination of the Data \n* Evaluation of Models (spoiler LGBM wins)\n* Tuning of the model\n* Execution of tuned model\n* Submission of Results\n"}}