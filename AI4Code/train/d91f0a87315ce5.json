{"cell_type":{"770ce6a9":"code","82d0a4a2":"code","59b9be19":"code","bf263906":"code","84ed6fbe":"code","95b22d7b":"code","dbf06143":"code","e7203833":"code","0c310fde":"code","273574dd":"code","acff4ff5":"code","9c8d673b":"code","b85f0c7e":"code","43c9726c":"code","d311799b":"code","9d60df96":"code","276588c7":"code","7f0d6ae7":"code","201358cb":"code","19d129f1":"code","254ac307":"code","5551998f":"code","1cfd742e":"code","d414f9bd":"code","9313dc30":"code","12e3c85c":"code","f933a0b2":"code","889aff90":"markdown"},"source":{"770ce6a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport re\nimport pandas as pd\nimport json\nimport string\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom transformers.modeling_tf_utils import shape_list\nfrom transformers import BertTokenizerFast, TFBertModel,TFXLMRobertaModel,XLMRobertaTokenizerFast\nfrom sklearn.model_selection import train_test_split","82d0a4a2":"max_length = 384\ndoc_stride = 128","59b9be19":"#tokenizer = BertTokenizerFast.from_pretrained(\"..\/input\/bert-base-multilingual-cased\/bert-base-multilingual-cased\")\ntokenizer = XLMRobertaTokenizerFast.from_pretrained(\"..\/input\/jplu-tf-xlm-roberta-large\")","bf263906":"train = pd.read_csv('\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')\ntrain.head()","84ed6fbe":"test = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\nprint(test.shape)\ntest.head()","95b22d7b":"train,val = train_test_split(train,test_size=0.20,random_state=2021)\ntrain = train.reset_index(drop=True)\nval = val.reset_index(drop=True)","dbf06143":"def prepare_train_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        list(examples[\"question\"].values),\n        list(examples[\"context\"].values),\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n    \n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples.loc[sample_index,\"answer_text\"]\n        answer_start = examples.loc[sample_index,\"answer_start\"]\n        \n        # If no answers are given, set the cls_index as answer.\n        if answer_start is None:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start\/end character index of the answer in the text.\n            start_char = answer_start\n            end_char = start_char + len(answers)\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the last word (edge case).\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples\n","e7203833":"def prepare_validation_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        list(examples[\"question\"].values),\n        list(examples[\"context\"].values),\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 \n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples.loc[sample_index,\"id\"])\n\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n        # position is part of the context or not.\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","0c310fde":"train_encodings  =  prepare_train_features(train)\nval_encodings    =  prepare_validation_features(val)\ntest_encodings   =  prepare_validation_features(test)","273574dd":"train_dataset = tf.data.Dataset.from_tensor_slices((\n    {key: train_encodings[key] for key in ['input_ids', 'attention_mask']},\n    {key: train_encodings[key] for key in ['start_positions', 'end_positions']}\n))\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    {key: val_encodings[key] for key in ['input_ids', 'attention_mask']},\n    \n))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    {key: test_encodings[key] for key in ['input_ids', 'attention_mask']},\n    \n))","acff4ff5":"train_dataset = train_dataset.map(lambda x, y: (x, (y['start_positions'], y['end_positions'])))\nval_dataset = val_dataset.map(lambda x: x)\ntest_dataset = test_dataset.map(lambda x: x)","9c8d673b":"def create_model():\n    ## encoder\n    #encoder = TFBertModel.from_pretrained(\"..\/input\/bert-base-multilingual-cased\/bert-base-multilingual-cased\")\n    encoder = TFXLMRobertaModel.from_pretrained(\"..\/input\/jplu-tf-xlm-roberta-large\")\n    ## QA Model\n    input_ids = layers.Input(shape=(max_length,), dtype=tf.int32,name='input_ids')\n    #token_type_ids = layers.Input(shape=(max_length,), dtype=tf.int32,name='token_type_ids')\n    attention_mask = layers.Input(shape=(max_length,), dtype=tf.int32,name='attention_mask')\n    embedding = encoder(\n        input_ids, attention_mask=attention_mask\n    )[0]\n\n    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax,name='start_positions')(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax,name='end_positions')(end_logits)\n\n    model = keras.Model(\n        inputs=[input_ids, attention_mask],\n        outputs=[start_probs, end_probs],\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    return model","b85f0c7e":"model = create_model()\nmodel.summary()","43c9726c":"model.fit(train_dataset.shuffle(1000).batch(8), epochs=1, batch_size=8)","d311799b":"start,end = model.predict(val_dataset.batch(4))","9d60df96":"start.shape,end.shape","276588c7":"from tqdm.auto import tqdm\nimport collections\n\ndef postprocess_qa_predictions(examples, features, start,end, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = start,end\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features[\"example_id\"]):\n        \n        features_per_example[example_id_to_index[feature]].append(i)\n\n    # The dictionaries we have to fill.\n    predictions = collections.OrderedDict()\n\n    # Logging.\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features['input_ids'])} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples.context.values)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None # Only used if squad_v2 is True.\n        valid_answers = []\n        \n        context = example\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[\"offset_mapping\"][feature_index]\n\n            # Update minimum null prediction.\n            cls_index = features[\"input_ids\"][feature_index].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n            # failure.\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n        #if not squad_v2:\n        #    predictions[example[\"id\"]] = best_answer[\"text\"]\n        #else:\n        answer = best_answer[\"text\"] \n        predictions[example_index] = answer\n\n    return predictions","7f0d6ae7":"val_predictions = postprocess_qa_predictions(val, val_encodings, start,end)","201358cb":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","19d129f1":"score = []\nfor idx in range(len(val)):\n    str1 = val.answer_text.values[idx]\n    str2 = val_predictions[idx]\n    score.append(jaccard(str1,str2))","254ac307":"np.array(score).mean()","5551998f":"sub = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/sample_submission.csv')\nsub.head()","1cfd742e":"start,end = model.predict(test_dataset.batch(4))\nstart.shape,end.shape","d414f9bd":"test_predictions = postprocess_qa_predictions(test, test_encodings, start,end)","9313dc30":"test_preds = []\nfor idx in range(len(test)):\n    test_preds.append(test_predictions[idx])\n","12e3c85c":"test['PredictionString'] = test_preds\ntest[['id','PredictionString']].head()","f933a0b2":"test[['id','PredictionString']].to_csv('submission.csv',index=False)","889aff90":"# Keras based notebook for QA\n\nThe code used in this notebook is largely derived from two excellent sources:\nhttps:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb\nhttps:\/\/keras.io\/examples\/nlp\/text_extraction_with_bert\/"}}