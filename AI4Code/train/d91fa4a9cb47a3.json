{"cell_type":{"80c3e059":"code","494e92d3":"code","0a50398e":"code","6d50d198":"code","43bb6e4d":"code","ac6eaa5e":"code","3841747f":"code","75274ab3":"code","64dbdfe0":"code","f338c51b":"code","e990e768":"code","49c0bb1a":"code","1e1092d9":"code","ef6ea360":"code","3219b926":"code","d26016f3":"code","edb0efd4":"code","04e6a55a":"markdown","eb3d0bc6":"markdown","96443f55":"markdown","08382e0c":"markdown","9ac823be":"markdown","d79254aa":"markdown","47a9c45b":"markdown","dfe1bca7":"markdown","949add86":"markdown","f3c53df0":"markdown","671572c8":"markdown","92704a47":"markdown","2443edcf":"markdown","5d0b81db":"markdown","b75b2faa":"markdown","7151441c":"markdown","a3f2f0e0":"markdown","69d10305":"markdown","02471b7e":"markdown","469ffd7c":"markdown","7c39bdd6":"markdown","d724ae85":"markdown"},"source":{"80c3e059":"import nltk\nimport random\nfrom nltk.corpus import movie_reviews\n\ndocuments = [(list(movie_reviews.words(fileid)), category)\n             for category in movie_reviews.categories()\n             for fileid in movie_reviews.fileids(category)]\n\nrandom.shuffle(documents)\n# documents[10]","494e92d3":"all_words = []\nfor w in movie_reviews.words():\n    all_words.append(w.lower())\n\nall_words = nltk.FreqDist(all_words)\nword_features = list(all_words.keys())[:3000]\n# word_features[0:10]","0a50398e":"def find_features(document):\n    words = set(document)\n    features = {}\n    for w in word_features:\n        features[w] = (w in words) # will return either True or False\n\n    return features\n\nfeaturesets = [(find_features(rev), category) for (rev, category) in documents]\n# featuresets[0]","6d50d198":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\n\nfrom wordcloud import WordCloud,STOPWORDS\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output","43bb6e4d":"data = pd.read_csv('..\/input\/Sentiment.csv')\ndata.head()","ac6eaa5e":"# Keeping only the neccessary columns\ndata = data[['text','sentiment']]\ndata.head()","3841747f":"# Splitting the dataset into train and test set\ntrain, test = train_test_split(data,test_size = 0.1)\n# Removing neutral sentiments\ntrain = train[train.sentiment != \"Neutral\"]","75274ab3":"tweets = []\nstopwords_set = set(stopwords.words(\"english\"))\n\nfor index, row in train.iterrows():\n    # Filtering out the words with less than 4 characters     \n    words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3]\n    \n    # Here we are filtering out all the words that contains link|@|#|RT     \n    words_cleaned = [word for word in words_filtered\n        if 'http' not in word\n        and not word.startswith('@')\n        and not word.startswith('#')\n        and word != 'RT']\n    \n    # filerting out all the stopwords \n    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\n    \n    # finally creating tweets list of tuples containing stopwords(list) and sentimentType \n    tweets.append((words_without_stopwords, row.sentiment))","64dbdfe0":"# It will extract all words from every tweets and will put it into seperate list\ndef get_words_in_tweets(tweets):\n    all = []\n    for (words, sentiment) in tweets:\n        all.extend(words)\n    # extracted all the words only    \n    return all\n\n# Note that, we are not using this frequency of word occurance anywhere, So it will just return the unique word list. \ndef get_word_features(wordlist):\n    wordlist = nltk.FreqDist(wordlist)\n    features = wordlist.keys()\n    return features\n\nall_words_in_tweets = get_words_in_tweets(tweets)\n\nw_features = get_word_features(all_words_in_tweets)\n\n# w_features","f338c51b":"def wordcloud_draw(data, color = 'black'):\n    words = ' '.join(data)\n    cleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and not word.startswith('#')\n                                and word != 'RT'\n                            ])\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color=color,\n                      width=2500,\n                      height=2000\n                     ).generate(cleaned_word)\n    plt.figure(1,figsize=(13, 13))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n\nwordcloud_draw(w_features)","e990e768":"def extract_features(document):\n    document_words = set(document)\n    features = {}\n    for word in w_features:\n        features[f'contains({word})'] = (word in document_words)\n    return features","49c0bb1a":"training_set = nltk.classify.apply_features(extract_features,tweets)\n# training_set[0]","1e1092d9":"len(training_set), len(tweets)","ef6ea360":"len(training_set[0][0]), len(w_features)","3219b926":"classifier = nltk.NaiveBayesClassifier.train(training_set)","d26016f3":"test_pos = test[ test['sentiment'] == 'Positive']\ntest_pos = test_pos['text']\ntest_neg = test[ test['sentiment'] == 'Negative']\ntest_neg = test_neg['text']","edb0efd4":"neg_cnt = 0\npos_cnt = 0\nfor obj in test_neg: \n    res =  classifier.classify(extract_features(obj.split()))\n    if(res == 'Negative'): \n        neg_cnt = neg_cnt + 1\nfor obj in test_pos: \n    res =  classifier.classify(extract_features(obj.split()))\n    if(res == 'Positive'): \n        pos_cnt = pos_cnt + 1\n        \nprint('[Negative]: %s\/%s '  % (len(test_neg),neg_cnt))        \nprint('[Positive]: %s\/%s '  % (len(test_pos),pos_cnt))    ","04e6a55a":"# B) NLTK Twitter Sentiment Analysis\nIn below work I am using Peter Nagy's amazing Kernel:\nhttps:\/\/www.kaggle.com\/ngyptr\/python-nltk-sentiment-analysis\n****\nHere, I am just explaining the feature extraction done by him.","eb3d0bc6":"**Stop Word:** Stop Words are words which do not contain important significance to be used in Search Queries. Usually these words are filtered out from search queries because they return vast amount of unnecessary information. ( the, for, this etc. )","96443f55":"This dictionary(in training_set touple) contains keys(all words from all tweets) and True value for words that are present in that tweet, False for remaining. That is why every dictionary contains same number of keys as the length of total word list(w_features). You can cross verify also by checking the first 5 key in training_set[0] and first 5 items in w_features list, they are same.","08382e0c":"This is the main feature extraction step, here we are again itterating our **documents** and passing each document containing review words and its category to the find_features function. In this function we are checking if the a review words are present in the complete word_features list, if yes, then we are marking them as 'true' and remaining as 'false' word_features as 'false'.\n\n\nFor eg : \n\nSuppose a - z are total words we have, and are stored in **word_features**\n\n>     word_features = [a, b, c, d, e, f, ....., z]\n\n[a, c, f, i] are the words obtained from a positive review file.\n   \n>     documents = [([a, c, f, i], 'pos'), ... ]\n    \nAfter applying find_features we will get below record in our **featuresets** variable\n\n>     featuresets = [({a:True, b:False, c:True, d:False, e:False, f:True, g:False, h:False, i:True, j:False, k:False, l:False, m:False, n:False, o:False, p:False, q:False, r:False, s:False, t:False, u:False, v:False, w:False, x:False, y:False, z:False}, 'pos'), ... ]\n\nSo eventually we will get 3000 keys (with True\/False based on reviewFileWords presence) for each reviewFile. This is our final feature the needs to given to classifier for training.","9ac823be":"**Sentiment Analysis:**\nthe process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n\n  [1]: https:\/\/github.com\/nagypeterjob","d79254aa":"We can use obtained 'featuresets' to train NaiveBayesClassifier algorithm.","47a9c45b":"I decided to only do sentiment analysis on this dataset, therfore I dropped the unnecessary colunns, keeping only *sentiment* and *text*.","dfe1bca7":"In below script, we are listing out all the words in movie_reviews (pos and neg), and finding the frequency of each word (in form of ({a: 734, b: 500, c: 402, d: 357, ....}) ) and at last considering only top 3000 frequently used words in **word_features**.","949add86":"**=================================================================================**","f3c53df0":"We are basically transforming all the tweets record in specific format, where every list item has been replace with a touple of dictionary and sentimentType. That is why length of tweets and training_set is same. check below code.","671572c8":"Finally, with not-so-intelligent metrics, I tried to measure how the classifier algorithm scored.","92704a47":"In below code, we are calling extract_features for every single tweet and in extract_features function, creating a key for every uniue word (from all tweets) and giving its value True for all words that are present in that specific tweet(passed in function).","2443edcf":"Hereby I plotted the most frequently distributed words. The most words are centered around debate nights.\n\n**NOTE : *It has nothing to do with sentiment analysis. It just depicts the use of wordcloud. That's all***","5d0b81db":"Amar - Now, if you have a question about this format of training_set, then I think this is the format which is required by nltk.NaiveBayesClassifier. Now below portion is not much complex.","b75b2faa":"First of all, splitting the dataset into a training and a testing set. The test set is the 10% of the original dataset. For this particular analysis I dropped the neutral tweets, as my goal was to only differentiate positive and negative tweets.","7151441c":"# A) Feature Extraction - \nI picked this example from:\nhttps:\/\/pythonprogramming.net\/words-as-features-nltk-tutorial\/","a3f2f0e0":"Hello guys, I am just a newbie on Python & ML. While studying NLKT, I faced difficulty in understanding feature extraction, So just treid to walk through some amazing examples. Finally understood, hope it helps you as well.","69d10305":"## Creating the list of all unique words(from all tweets): \n\nAs a next step I extracted the so called features with nltk lib, first by measuring a frequent distribution and by selecting the resulting keys.","02471b7e":"In below script, execution will start from second for-in loop, here we are iterrating for all review categories which will be ('neg', 'pos'), then in second for loop we will itterate for the fileIds (i.e. 'neg\/cv000_29416.txt' for 'neg' category & 'pos\/cv000_29590.txt' for 'pos' category), after that we will create a list of touples which contains list of words from specific file(negative review or positive review) and it's category. We will store this list of touples in documents and will perform shuffle operation.","469ffd7c":"## Building and Traininig Model","7c39bdd6":"## Feature Extraction:\n\nUsing the NLTK NaiveBayes Classifier I classified the extracted tweet word features.","d724ae85":"## Epilog ##\n\nIn this project I was curious how well nltk and the NaiveBayes Machine Learning algorithm performs for Sentiment Analysis. In my experience, it works rather well for negative comments. The problems arise when the tweets are ironic, sarcastic has reference or own difficult context.\n\nConsider the following tweet:\n*\"Muhaha, how sad that the Liberals couldn't destroy Trump.  Marching forward.\"*\nAs you may already thought, the words **sad** and **destroy** highly influences the evaluation, although this tweet should be positive when observing its meaning and context. \n\nTo improve the evalutation accuracy, we need something to take the context and references into consideration. As my project 2.0, I will try to build an LSTM network, and benchmark its results compared to this nltk Machine Learning implementation. Stay tuned. "}}