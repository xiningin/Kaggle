{"cell_type":{"c8cf9171":"code","a6666b91":"code","a94effa6":"code","414920cc":"code","1df9a197":"code","1cb3faaf":"code","fc1d5566":"code","9d23114e":"code","fa4bb5e6":"code","4e806314":"code","9d50e4fc":"code","b9a48e94":"code","d2b05df3":"code","4501fc1c":"code","ce711fb6":"code","d59c557e":"code","d8ba23ab":"code","03b24912":"code","c0803082":"code","b13531ed":"code","0f010783":"code","e4c2fd7b":"code","32200220":"code","77611423":"code","d09b0b37":"code","11cd878d":"code","1557ad56":"code","06e1d450":"code","936af6c3":"code","60cabda7":"code","4d38ba23":"code","02f57f8b":"code","137f2dc8":"code","0117f213":"code","ce16bf60":"code","8e685208":"code","d87607f8":"code","d752fa0e":"code","cf75be12":"code","8d16a3c4":"code","f8b7a393":"code","b09ffd9d":"code","5aa93677":"code","02d202cd":"markdown","02c34ce8":"markdown","942bb62a":"markdown","d8a28fbf":"markdown","c5c3da56":"markdown","d7d649f6":"markdown","9af41ad9":"markdown","970a382a":"markdown","83afc9ed":"markdown","6d010412":"markdown","5565d020":"markdown","f5ac0633":"markdown","46b62d91":"markdown","dd5a9a90":"markdown","7f2971ba":"markdown","b277c2bb":"markdown","b15e4fa8":"markdown","b79fc554":"markdown","8311abc9":"markdown","5064481e":"markdown","c630a026":"markdown","8caf06f6":"markdown","21e932b6":"markdown","9473c02c":"markdown","25719c6d":"markdown","f8d981ea":"markdown","71d09173":"markdown","97603376":"markdown","fe3cdfc4":"markdown","deb01e5c":"markdown","f5bde377":"markdown"},"source":{"c8cf9171":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nimport tkinter\nfrom tkinter import *\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a6666b91":"steam_review = pd.read_csv(\"\/kaggle\/input\/steam-reviews-dataset\/steam_reviews.csv\")","a94effa6":"steam_review.head()","414920cc":"steam_review.describe()","1df9a197":"steam_review.info()","1cb3faaf":"steam_review.isnull().sum()","fc1d5566":"steam_review = steam_review.dropna()","9d23114e":"steam_review.isnull().sum()","fa4bb5e6":"steam_review.info()","4e806314":"steam_review.recommendation.value_counts()\n","9d50e4fc":"graph_sizes = [steam_review.recommendation.value_counts()[0], steam_review.recommendation.value_counts()[1]]\nlabels = [\"Recommended\", \"Not Recommended\"]\n\nexplode = (0, 0.1)\nfig1, ax1 = plt.subplots()\nax1.set_title('Games recommendation')\nax1.pie(graph_sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\n\nax1.axis('equal')\nplt.tight_layout()\nplt.show()\n","b9a48e94":"steam_review['hour_played_reviews'] = steam_review.groupby('hour_played')['hour_played'].transform('count')\nx = steam_review.hour_played\ny = steam_review['hour_played_reviews']\nfig = plt.figure(figsize = (13,8))\nax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\nax.scatter(x,y)\nax.set_title('Dependence of the number of ratings on the duration of the game')\nax.set_xlabel('Hours played')\nax.set_ylabel('Number of reviews')\n","d2b05df3":"steam_review.dataframeName = 'steam_reviews.csv'\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","4501fc1c":"plotPerColumnDistribution(steam_review, 10, 5)","ce711fb6":"plotCorrelationMatrix(steam_review, 8)","d59c557e":"plotScatterMatrix(steam_review, 9, 10)","d8ba23ab":"top_games = steam_review.title.value_counts()\nprint(\"Top 10 games are\\n\\n\",steam_review.title.value_counts()[:10])","03b24912":"steam_review['review_length'] = steam_review.apply(lambda row: len(str(row['review'])), axis=1)\n\nsteam_review['recommendation_int'] = steam_review['recommendation'] == 'Recommended'\nsteam_review['recommendation_int'] = steam_review['recommendation_int'].astype(int)\n","c0803082":"reviews_count = steam_review.groupby(['title'])['review'].count().sort_values(ascending=False)\n\nreviews_count = reviews_count.reset_index()\n\nsns.set(style=\"darkgrid\")\nplt.figure(figsize=(25,20))\nsns.barplot(y=reviews_count['title'], x=reviews_count['review'], data=reviews_count,\n            label=\"Total\", color=\"r\")\n\nreviews_count_pos = steam_review.groupby(['title', 'recommendation_int'])['review'].count().sort_values(ascending=False)\nreviews_count_pos = reviews_count_pos.reset_index()\nreviews_count_pos = reviews_count_pos[reviews_count_pos['recommendation_int'] == 1]\nsns.barplot(y=reviews_count_pos['title'], x=reviews_count_pos['review'], data=reviews_count_pos,\n            label=\"Total\", color=\"b\")\n","b13531ed":"steam_review['review_length'] = steam_review.apply(lambda row: len(str(row['review'])), axis=1)\n\nsteam_review['recommendation_int'] = steam_review['recommendation'] == 'Recommended'\nsteam_review['recommendation_int'] = steam_review['recommendation_int'].astype(int)","0f010783":"steam_review","e4c2fd7b":"steam_review.info()","32200220":"steam_review.duplicated().sum()\n\n","77611423":"steam_review['review'] = [review.strip().lower() for review in steam_review['review']]","d09b0b37":"steam_review['review'] = steam_review['review'].replace(r\"[^a-zA-Z\\d\\_\\+\\-\\'\\.\\\/\\s]+\", ' ', regex = True)\n\nsteam_review['review'] = steam_review['review'].replace([\".\/ \", \"' \", \" '\"], \" \", regex = True)","11cd878d":"steam_review.head()\n","1557ad56":"steam_review.isnull().sum()\n","06e1d450":"data_frame = steam_review","936af6c3":"data_frame, test_data = train_test_split(data_frame, test_size=0.20)","60cabda7":"train_data, dev_data = train_test_split(data_frame, test_size=0.20)","4d38ba23":"fig, axes = plt.subplots(ncols=3)\n\nplot1 = train_data.recommendation_int.value_counts().sort_index().plot(kind='bar', legend=True, rot=0, ax=axes[0])\nplot1.set_title(\"Train\")\nplot1.set_xlabel(\"Rating\")\nplot1.set_ylabel(\"number of rating\")\nplot2 = test_data.recommendation_int.value_counts().sort_index().plot(kind='bar', legend=True, rot=0, ax=axes[1])\nplot2.set_title(\"Test\")\nplot2.set_xlabel(\"Rating\")\nplot3 = dev_data.recommendation_int.value_counts().sort_index().plot(kind='bar', legend=True, rot=0,figsize=(17, 8))\nplot3.set_title(\"Dev\")\nplot3.set_xlabel(\"Rating\")","02f57f8b":"tfidf = TfidfVectorizer(max_features=1000)\n\n# transforming traing data to tfidf form arrays. we can use this to train any model\nx_train = tfidf.fit_transform(train_data['review']).toarray()\ny_train = np.asarray(train_data['recommendation_int'])","137f2dc8":"x_dev = tfidf.transform(dev_data['review']).toarray()\ny_dev = np.asarray(dev_data['recommendation_int'])","0117f213":"alpha = [0.01, .1, 0.5, 1, 2, 3]\nmultinomial_nb = {}\nmse_multinomial_nb = {}\naccuracy_multinomial_nb = {}\ny_dev_pred_multinomial_nb = {}\nfor a in alpha:\n    multinomial_nb[a] = MultinomialNB(alpha=a)\n    multinomial_nb[a].fit(x_train, y_train)\n\n    y_dev_pred_multinomial_nb[a] = (multinomial_nb[a].predict(x_dev))\n\n    # Calculate the Mean Squared Error and Accuracy\n    mse_multinomial_nb[a] = mean_squared_error(y_dev, y_dev_pred_multinomial_nb[a])\n    accuracy_multinomial_nb[a] = accuracy_score(y_dev, y_dev_pred_multinomial_nb[a])*100\n\n    # Print the Mean Squared Error and Accuracy\n    print(f'Mean Squared Error = {mse_multinomial_nb[a]} for alpha = {a}')\n    print(f'Accuracy = {accuracy_multinomial_nb[a]} for alpha = {a}')","ce16bf60":"mse = {}\naccuracy = {}\nmodel = {}\nmodel[\"MNB\"] = multinomial_nb\nmse[\"MNB\"] = mse_multinomial_nb\naccuracy[\"MNB\"] = accuracy_multinomial_nb\n","8e685208":"C = [0.01, 0.1, 1, 10, 100, 1000]\nsvm = {}\nmse_svm = {}\naccuracy_svm = {}\ny_dev_pred_svm = {}\nfor c in C:\n    svm[c] = LinearSVC(C=c, dual = False)\n    svm[c].fit(x_train, y_train)\n\n    y_dev_pred_svm[c] = (svm[c].predict(x_dev))\n\n    mse_svm[c] = mean_squared_error(y_dev, y_dev_pred_svm[c])\n    accuracy_svm[c] = accuracy_score(y_dev, y_dev_pred_svm[c])*100\n\n    print(f'Mean Squared Error = {mse_svm[c]} for C = {c}')\n    print(f'Accuracy = {accuracy_svm[c]} for C = {c}')\n","d87607f8":"model['SVM'] = svm\nmse['SVM'] = mse_svm\naccuracy['SVM'] = accuracy_svm","d752fa0e":"def svmmnb(x):\n    x_array = tfidf.transform(x).toarray()\n    y_pred_dict = {}\n\n    for a in alpha:\n        y_pred = model['MNB'][a].predict(x_array)[0]\n        if y_pred in y_pred_dict:\n            y_pred_dict[y_pred] += (accuracy['MNB'][a])\n        else:\n            y_pred_dict[y_pred] = (accuracy['MNB'][a])\n\n    for c in C:\n        y_pred = model['SVM'][c].predict(x_array)[0]\n        if y_pred in y_pred_dict:\n            y_pred_dict[y_pred] += (accuracy['SVM'][c])\n        else:\n            y_pred_dict[y_pred] = (accuracy['SVM'][c])\n\n    inverse = [(value, key) for key, value in y_pred_dict.items()]\n    return max(inverse)[1]\n","cf75be12":"y_dev_pred_ens = []\n\nfor index, row in dev_data.iterrows():\n    y_dev_pred_ens.append(svmmnb([row['review']]))\n\n\nmse_ens = mean_squared_error(y_dev, y_dev_pred_ens)\naccuracy_ens = accuracy_score(y_dev, y_dev_pred_ens)*100\n\nprint(f'Mean Squared Error = {mse_ens}')\nprint(f'Accuracy = {accuracy_ens}')\n","8d16a3c4":"estimators = [10, 50]\nrfc = {}\nmse_rfc = {}\naccuracy_rfc = {}\ny_dev_pred_rfc = {}\nfor n in estimators:\n    rfc[n] = RandomForestClassifier(max_depth=25, n_estimators=n)\n    rfc[n].fit(x_train, y_train)\n\n    y_dev_pred_rfc[n] = (rfc[n].predict(x_dev))\n\n    mse_rfc[n] = mean_squared_error(y_dev, y_dev_pred_rfc[n])\n    accuracy_rfc[n] = accuracy_score(y_dev, y_dev_pred_rfc[n])*100\n\n    print(f'Mean Squared Error = {mse_rfc[n]} for n = {n}')\n    print(f'Accuracy = {accuracy_rfc[n]} for n = {n}')\n","f8b7a393":"x_test = tfidf.transform(test_data['review']).toarray()\ny_test = np.asarray(test_data['recommendation_int'])\n","b09ffd9d":"c=1\ny_test_pred = (svm[c].predict(x_test))\n\nmse_test = mean_squared_error(y_test, y_test_pred)\naccuracy_test = accuracy_score(y_test, y_test_pred)*100\n\nprint(f'Mean Squared Error test = {mse_test}')\nprint(f'Accuracy test = {accuracy_test}')\n","5aa93677":"def rating():\n    x = textfield_label.get()\n    results.delete(\"all\")\n    value = svmmnb([x])\n    print(value)\n    if value == 0:\n        text = \"Negative\"\n    else:\n        text = \"Positive\"\n    results.create_text(200, 80, text=\"The comment is \"+text, font='Arial 20', fill='white')\n    return value\n\nwindow = Tk()\nwindow.geometry(\"500x500\")\nwindow.title(\"Game Recommender\")\nwindow.config(bg=\"black\")\nhead = Label(window, text=\"Game Review Classifier\", bg=\"black\", foreground=\"white\", font=(\"Arial\", 25)).pack()\nLabel(window, text=\"Comment\", bg=\"black\", foreground=\"white\", font=(\"Arial\", 10)).place(x=100, y=80)\nLabel(window, text=\"______________________________\", bg=\"black\", foreground=\"white\").place(x=173, y=90)\ntextfield_label = Entry(window, bg=\"black\", foreground=\"white\", border=0, insertbackground=\"white\")\ntextfield_label.focus()\ntextfield_label.place(x=175, y=80, height=20, width=150)\noutput = Label(window, bg=\"black\", foreground=\"white\", font=(\"Arial\", 25)).place(x=50, y=250)\nsearch = Button(window, text=\"Rate\", command=rating).place(x=200, y=150, width=100)\nprint(search)\nresults = tkinter.Canvas(bg='black', width='400', height='200', highlightthickness=0)\nresults.place(x=50, y=250)\nwindow.mainloop()","02d202cd":"tf-idf(t, d) = tf(t, d) * idf(t) and idf is computed as idf(d, t) = log [ (1 + n) \/ (1 + df(d, t)) ] + 1","02c34ce8":"Using the test data to test our best model","942bb62a":"For text analysis the best classifiers are Naive Bayes, Support Vector Machine and Random Forest Classifier","d8a28fbf":"Trying Random Forest Classifier","c5c3da56":"Cleaned Data","d7d649f6":"Support Vector Machines (SVM)","9af41ad9":"**GUI**","970a382a":"Correlation Matrix of the columns","83afc9ed":"Plot per Column Distribution. Column name vs there couunt","6d010412":"This gave me an accuracy of **80.2494%** which is very less compared to the other 2 models. We got good accuracy for SVM.","5565d020":"**Building Classification Model**","f5ac0633":"Trying out Naive bayes Model as this is a text analysis we will use multinomial Naive Bayes.","46b62d91":"Alpha at 0.1 gave us the best accuracy","dd5a9a90":"**Top 10 Most Reviewed Games**","7f2971ba":"When SVM and MNB were combined I got an accuracy of **85.4081%**","b277c2bb":"Hours Played Vs Number of Reviews","b15e4fa8":"**Data Cleaning**","b79fc554":"But before we train our model we have to convert reviews to TF IDF form","8311abc9":"**Data Visualization**","5064481e":"Games Recommended Vs Not Recommended besed on the review","c630a026":"Reading the Data file. The data file size is 115mb, it has **434891** entries","8caf06f6":"Converting all the reviews to lower case and removing special characters","21e932b6":"We will be diving the data into Train data, Test data and Dev data using train_test_split.","9473c02c":"Plot of each game Vs the number of positive and negative reviews","25719c6d":"Combining 2 models to test the accuracy.","f8d981ea":"**Data Preprocessing**","71d09173":"Assigning integer value for recommended and not recommended as it will be easier for working.","97603376":"Scatter Matrix of each column compared with other columns","fe3cdfc4":"Checking if there are any null values present in the Dataset. We will be cleaning them out as they can affect our predections.","deb01e5c":"The formula that is used to compute the tf-idf is","f5bde377":"**Import Statements**"}}