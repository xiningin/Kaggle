{"cell_type":{"2ae24cb7":"code","83210d58":"code","12791b5c":"code","80ac2cd6":"code","c44aca51":"code","0598e040":"code","ca19b4e7":"code","072411ce":"code","de432b6d":"code","74c31da1":"code","b52c7e70":"code","e35a9e4a":"code","e86ffc43":"code","8b1a7210":"code","dc4a6f93":"code","3cc34854":"code","d1db33ed":"code","0a036a3f":"code","051f60ab":"code","808bebda":"code","7f66a0b1":"code","17e207be":"code","96f7fc69":"code","11a96727":"code","78d157a4":"code","b756c29b":"markdown","f6e9514e":"markdown","e0d9023e":"markdown","6e0de60f":"markdown","5507aeec":"markdown","2218cca0":"markdown","f17fd01b":"markdown","c921b6d4":"markdown","84205280":"markdown","814ed12f":"markdown","2e9d1618":"markdown","f4d2cd53":"markdown","92d88c75":"markdown","b1462b3f":"markdown","a28a9834":"markdown","dae3f02a":"markdown","6bd06702":"markdown","a725bbac":"markdown","2998ed1e":"markdown","fb8aedf9":"markdown","743ba913":"markdown","44f40aec":"markdown","9d4665ff":"markdown","85911273":"markdown","139139d7":"markdown","7a0c9476":"markdown","3917571e":"markdown","b20d857a":"markdown","de381b41":"markdown","5cd4dd24":"markdown","15adc7be":"markdown","a095bf25":"markdown","d15f40b6":"markdown","bc469085":"markdown","2d07783c":"markdown"},"source":{"2ae24cb7":"import numpy as np\nnp.random.seed(1)\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport altair as alt\n%matplotlib inline\n\nfrom fbprophet import Prophet\nfrom sklearn.ensemble import IsolationForest","83210d58":"imposter = pd.read_csv('..\/input\/nab\/realTraffic\/realTraffic\/speed_6005.csv')\n\nprint(f'speed_6005.csv: {imposter.shape}')\nimposter.head()","12791b5c":"imposter.info()","80ac2cd6":"imposter.describe()","c44aca51":"imposter['timestamp'] = pd.to_datetime(imposter['timestamp'])\nimposter.info()","0598e040":"imposter['year'] = imposter['timestamp'].apply(lambda x: x.year)\nimposter['month'] = imposter['timestamp'].apply(lambda x: x.month)\nimposter['day'] = imposter['timestamp'].apply(lambda x: x.day)\nimposter['weekday'] = imposter['timestamp'].apply(lambda x: x.weekday())\nimposter['hour'] = imposter['timestamp'].apply(lambda x: x.hour)\n\nimposter = imposter[['timestamp', 'year', 'month', 'day', 'weekday', 'hour', 'value']]\nimposter.rename(columns={'timestamp': 'Datetime'}, inplace=True)\n\n# Weekday starts from Monday\nprint(f'{imposter.Datetime[0]} with weekday {imposter.weekday[0]} is {imposter.Datetime[0].strftime(\"%A\")}.\\n')\n\nimposter.sample(5)","ca19b4e7":"fig = px.line(imposter, x='Datetime', y='value', title='Overview of out time series data')\n\nfig.update_xaxes(rangeslider_visible=True,\n                rangeselector=dict(\n                buttons=[\n                    dict(count=1, label='1m', step='month', stepmode='backward'),\n                    dict(count=6, label='6m', step='month', stepmode='backward'),\n                    dict(step='all')\n                ]))\nfig","072411ce":"sns.displot(imposter.value)","de432b6d":"fig = px.histogram(imposter, x='Datetime', y='value', histfunc='avg', title='Histogram and Scatter on Date Axes')\n\nfig.update_traces(xbins_size='M1')\nfig.update_xaxes(showgrid=True, ticklabelmode='period', dtick='M1', tickformat='%b\\n%Y')\nfig.update_layout(bargap=0.1)\nfig.add_trace(go.Scatter(mode='markers', x=imposter['Datetime'], y=imposter['value'], name='daily'))\nfig","74c31da1":"alt.Chart(imposter).mark_rect().encode(alt.X('hour:O', title='hour of day'),\n                                      alt.Y('day:O', title='date'),\n                                      alt.Color('value:Q', title='speed'))","b52c7e70":"alt.Chart(imposter).mark_bar().encode(x = 'weekday:O',\n                                     y = 'value:Q',\n                                     color = alt.condition(alt.datum.weekday == 0,\n                                                              alt.value('orange'),\n                                                              alt.value('steelblue'))).properties(width=600)","e35a9e4a":"fig = px.line(imposter, x='Datetime', y='value',\n             title='SUS pattern-ANOMALY?',\n             range_x=['2015-08-31 19:00:00', '2015-09-17 17:00:00'])\n\nfig.update_layout(shapes=[dict(type = 'rect',\n                              xref = 'x',\n                              yref = 'paper',\n                              x0 = '2015-09-05',\n                              y0 = 0,\n                              x1 = '2015-09-08 11:00:00', \n                              y1 = 1,\n                              fillcolor = 'Red',\n                              opacity = 0.5,\n                              layer = 'below',\n                              line_width = 0),\n                          dict(type = 'rect',\n                              xref = 'x',\n                              yref = 'paper',\n                              x0 = '2015-09-09',\n                              y0 = 0,\n                              x1 = '2015-09-10 09:00:00', \n                              y1 = 1,\n                              fillcolor = 'Red',\n                              opacity = 0.5,\n                              layer = 'below',\n                              line_width = 0),\n                          dict(type = 'rect',\n                              xref = 'x',\n                              yref = 'paper',\n                              x0 = '2015-09-16 20:00:00',\n                              y0 = 0,\n                              x1 = '2015-09-17 12:00:00', \n                              y1 = 1,\n                              fillcolor = 'Red',\n                              opacity = 0.5,\n                              layer = 'below',\n                              line_width = 0)],\n                 annotations=[dict(x = '2015-09-08 11:00:00',\n                                  y = 0.99,\n                                  xref = 'x',\n                                  yref = 'paper',\n                                  showarrow = False,\n                                  xanchor = 'right',\n                                  text = 'SUS activity 1'),\n                              dict(x = '2015-09-10 09:00:00',\n                                  y = 0.99,\n                                  xref = 'x',\n                                  yref = 'paper',\n                                  showarrow = False,\n                                  xanchor = 'right',\n                                  text = 'SUS activity 2'),\n                              dict(x = '2015-09-17 12:00:00',\n                                  y = 0.99,\n                                  xref = 'x',\n                                  yref = 'paper',\n                                  showarrow = False,\n                                  xanchor = 'right',\n                                  text = 'SUS activity 3')])\nfig","e86ffc43":"model = IsolationForest(verbose=1)\nmodel.fit(imposter[['value']])","8b1a7210":"print(f\"type(imposter['value']) = {type(imposter['value'])}\")\nprint(f\"type(imposter[['value']]) = {type(imposter[['value']])}\")","dc4a6f93":"imposter['outliers'] = pd.Series(model.predict(imposter[['value']])).apply(lambda x: 'yes' if (x==-1) else 'no')\nimposter.query('outliers==\"yes\"')","3cc34854":"fig = px.scatter(imposter.reset_index(), x='Datetime', y='value',\n                hover_data=['weekday'], color='outliers', title='NAM-speed outliers')\nfig.update_xaxes(rangeslider_visible=True)\nfig","d1db33ed":"model = IsolationForest(contamination=0.01, verbose=1)\nmodel.fit(imposter[['value']])\nimposter['outliers'] = pd.Series(model.predict(imposter[['value']])).apply(lambda x: 'yes' if (x==-1) else 'no')\nfig = px.scatter(imposter.reset_index(), x='Datetime', y='value',\n                hover_data=['weekday'], color='outliers', title='NAM-speed outliers')\nfig.update_xaxes(rangeslider_visible=True)\nfig","0a036a3f":"imposter_prophet = imposter.copy()\nimposter_prophet = imposter_prophet.reset_index()[['Datetime', 'value']].rename({\n    'Datetime': 'ds', 'value': 'y'\n}, axis='columns')","051f60ab":"model = Prophet(changepoint_range=0.95)\nmodel.fit(imposter_prophet)","808bebda":"future = model.make_future_dataframe(periods=100, freq='H')\nforecast = model.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","7f66a0b1":"results = pd.concat([\n    imposter_prophet.set_index('ds')['y'], forecast.set_index('ds')[['yhat', 'yhat_lower', 'yhat_upper']]\n], axis=1)\n\nresults.head()","17e207be":"fig1 = model.plot(forecast)","96f7fc69":"comp = model.plot_components(forecast)","11a96727":"results['error'] = results['y'] - results['yhat']\nresults['uncertainity'] = results['yhat_upper'] - results['yhat_lower']\nresults[results['error'].abs() > 1.5*results['uncertainity']]","78d157a4":"results['anomaly'] = results.apply(lambda x: 'yes' if ( np.abs(x['error']) > 1.5*x['uncertainity'] ) else 'No', axis=1)\n\nfig = px.scatter(results.reset_index(), x='ds', y='y',\n                color='anomaly', title='NAM-speed outliers')\nfig.update_xaxes(rangeslider_visible=True,\n                rangeselector=dict(buttons=list([\n                    dict(count=1, label='1y', step='year', stepmode='backward'),\n                    dict(count=2, label='3y', step='year', stepmode='backward'),\n                    dict(count=3, label='3y', step='year', stepmode='backward'),\n                    dict(step='all')\n                ])))\nfig","b756c29b":"**Actual Insights**:\n- The speed across 17 days exhibits only **stationarity** and not seasonality\n- The value of CPC lies between 20 and 109, **most of them lies in between 70 to 90** (see below)\n- **The drop seen in the later time is huge** compared to the drop happened in the initial days of September\n- Even though we have same pattern, you can visually see the **same speed at Sep4-Sep8**. Could it be an ANOMALY ?","f6e9514e":"## Changing the Datatype of Timestamp\nSo that we can extract the data from the time stamp like `year`, `month`, `day`, `hour`, `weekdays`. This will help us to reveal a lot of information from the data. We gotta look for all possible ways to find the imposter among us","e0d9023e":"**Discussions**:<br>\nThe highest was recorded on a weekend - Saturday. But there has been low records of speed during thursday and Friday and also Monday has low speed. We can accept our 1st assumption of hike of speed during weekend to be more specific it was only the start of the weeekend and end of the weekend doesn't have much speed in sensors.\n\n| Inocents | Statements |\n| --- | ---------- |\n| **Pink** | ***I was working on increasing speed during the weekends at medbay*** |\n| ~~**Orange**~~ | ~~I was working on increasing speed during holiday months such as December and January at admin~~ |\n| ~~**Yellow**~~ | ~~I was working on increasing speed during the late night hours at storage~~ |\n| ~~**Cyan**~~ | ~~I proposed a strategy to form a seasonality across 2015 for speed at shields~~ |\n| **Red** | I worked really hard during Sep 4- Sep 10 to fix our ship at reactor |","6e0de60f":"Yes we got the anomaly points which were recorded on Sep 1 and Sep 17.","5507aeec":"## Overview of time series data\nLet's take the `Datetime` as x axis and plot the values and identify whether it has the characteristics of time series data and also check against **what they stated**.","2218cca0":"From the information we can identify that\n- We don't have any null records in the dataset. BAM !\n- timestamp column is an object data type. small bam!","f17fd01b":"## [IsolationForest](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.IsolationForest.html)\n\n- Return the anomaly score of each sample using the IsolationForest algorithm.\n\nThe IsolationForest \u2018isolates\u2019 observations by **randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature**.<br>\nSince recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.<br>\nThis path length, averaged over a forest of such random trees, is a measure of normality and our decision function.<br>\n**Random partitioning produces noticeably shorter paths for anomalies**. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.\n\n1. How Isolation Forest works<br>\n![](https:\/\/miro.medium.com\/max\/3000\/1*d-4xINDQHv0G82o2GUApJQ.png)<br>\nAbove I show examples of the procedure after four splits, respectively. In this case, I had only two features `x` and `y` and four observations to check. The first condition is the one that distinguishes a normal observation from an anomaly. If `x` is bigger than 120, then the observation is an outlier and is coloured in red. Then, normal and anomalous data points can be distinguished based on average path length: **shorter paths indicates that we have anomalies**, while **longer path shows that there are normal observations**.\n\n2. Anomaly Score<br>\n![](https:\/\/miro.medium.com\/max\/1704\/1*D78QLbcwXesymhquuofnOg.png)<br>\nThe isolation forest needs an Anomaly Score to have an idea of how anomalous a data point is. Its values lie between 0 and 1. The anomaly score is defined as:<br>\n![](https:\/\/miro.medium.com\/max\/3000\/1*GMWS-FkTTYWaRgOhKV_QCQ.png)<br>\nwhere `E(h(x))` is the average of `h(x)`, which is the path length from the root node to the external node `x`, while `c(n)` is the average of `h(x)` given n and is used to normalize `h(x)`. There are three possible situations:\n - When the score of the observation is close to 1, the path length is very small, and then the data point is easily isolated. We have an **anomaly**.\n - When the score of that observation is smaller than 0.5, the path length is large, and then we have a **normal data point**.\n - If all the observations have an anomaly score around 0.5, then the **entire sample doesn\u2019t have any anomaly**.","c921b6d4":"# Libraries","84205280":"Now let's classify the anamoly as yes if the error lies beyond 1.5 times of `uncertainity`. With `threshold = 1.5`, it depends on the application we are working on, here we are not concerned about the value that landed as uncertain but what lies beyond those uncertain values which has to be classified as anamoly.","814ed12f":"## [Prophet](https:\/\/pypi.org\/project\/fbprophet\/)\n\n**FBProphet: Automatic Forecasting Procedure**\n> Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n\n![](https:\/\/miro.medium.com\/max\/5452\/1*gkpBh6iZZBk5-fyZeOCK8A.jpeg)\nAn Additive Model above can **absorb the absence of seasonal effects by having s(t) = 0**, as the other terms of the equation have no impact to predict future values in y(t). Unlike, fixed and linear regression models like Fama \u2014 French above, . . . **Prophet is a modular and non \u2014 linear regression model** that separates and recombines a single dataset of history. Feature Engineering when features explain a future value . . .or when factors drive a forecast are removed . . . **leaving more room for the option of a variable Domain Knowledge** for a user of Prophet.","2e9d1618":"# Getting to Know the Data","f4d2cd53":"Now we can also make future predictions of our data, this is the important part where we can get the lower and upper interval range. We can get a suitable dataframe that extends into the future a specified number of days using the helper method `Prophet.make_future_dataframe`. By default it will also include the dates from the history, so we will see the model fit as well.\n\nWe can say that the predicted values may vary inbetween these intervals, no predictions can be accurate. But **we are 95% confident that our predicted values can fall in the interval**.","92d88c75":"# Tasks\n- Getting to Know the Data\n> - Changing the Datatype of Timestamp\n- Emergency Meetings\n> - Overview of time series data\n> - Histogram and Scatter on datetime\n> - Which hour and day of the month we had high CPC?\n> - Behaviour during weekend\n- Sus Pattern Via Visuals - RED SUS?\n> - Let's Brainstorm What Happened\n- Building Model to Trace Anomalies\n> - IsolationForest\n>  - (Extra Notes)\n> - Prophet","b1462b3f":"# Story\nOn an important mission to Mars, Red, Blue, Green, Pink, Orange, Yellow, Black, White, Purple, Brown, Cyan and Lime were boarded on to the spaceship. On their way to explore the Mars, the shapeship undergoes series of internal component failure in the navigation and flight parts. It was time for the crew to repair the issues . But there was one Imposter one Betrayer one Anamoly among them and to identify who that is there are set of tasks which has to be performed in the data to reveal who is the imposter in the dataset.\n\nTo understand what this this project really focuses on is that there are many realtime cases we are witnessing on tracking abonormal data which possess a serious threat to the business in the field of IT, health and various other sectors. Even though the cyber security teams are forging to figure out the anomaly behaviour in the transactions, the system built using algorithms are not efficient enough to capture all anomaly's. Huge millions of money are lost due to the cyber attacks. It not only affects the business revenue but also the reputation and trust of doing business with the firm.\n\n## Where to find the Imposter ?\nTO find the imposter in our spaceship, we use [umenta Anomaly Benchmark (NAB)](https:\/\/www.kaggle.com\/boltzmannbrain\/nab), where we consider speed_6005 which has the dataset with 2500 rows of the speed for specific sensors in the spaceship.\n\n**CSV name**: speed_6005.csv\n\nIn these dataset above, The crew will analyse the dataset with time-series visualizations and perform analysis to detect the anomaly records and thereby capture the imposter. These are crucial records which can help in identify suspicious speed recorded in the sensors.","a28a9834":"We can confirm that **the speed was high in case of weekends** and in rest of the days it as dim except wednesday. Anomaly might fall in that day.\n\nAlso if take a closer look at the chart, according to the time, **the speed was high from early morning till evening and later dipped down midnight**.\n\nConsiderint the days, there was a steep increase in the initial recording days and there was a steep downhill and was never risen again. Can we connect all the dots?\n\nBefore passing any judgements, let's **figure out the error value from the predictions** and also **calculate the uncertainity by differencing the lower and upper interval**, that leaves us with the records which lies above the intervals which is an unusual case, we can term those as anamolies.","dae3f02a":"### (Extra Notes)","6bd06702":"**Discussions**:<br>\nWe can clearly see there is not much speed during late night hours compared to morning hours - our 3rd assumption is false, and since Yellow said it's true I feel yellow is sus.\n\n| Inocents | Statements |\n| --- | ---------- |\n| **Pink** | I was working on increasing speed during the weekends at medbay |\n| ~~**Orange**~~ | ~~I was working on increasing speed during holiday months such as December and January at admin~~ |\n| ~~**Yellow**~~ | ~~I was working on increasing speed during the late night hours at storage~~ |\n| ~~**Cyan**~~ | ~~I proposed a strategy to form a seasonality across 2015 for speed at shields~~ |\n| **Red** | I worked really hard during Sep 4- Sep 10 to fix our ship at reactor |","a725bbac":"# Building Model to Trace Anomalies","2998ed1e":"## Which hour and day of the month we had high CPC?\nLet's use altair library to plot a beautiful heatmap which can help us to identify which hour and which day of the month were speed higher","fb8aedf9":"## Behaviour during weekend\nLet's check out our final assumption of whether there is a rise in CPC during weekends. Since most of them will be free to surf internet and tend to click more ads","743ba913":"# References\n\n- [Anomaly Detection With Isolation Forest](https:\/\/betterprogramming.pub\/anomaly-detection-with-isolation-forest-e41f1f55cc6)\n- [Detecting anomalies using Isolation Trees: Practical Machine Learning](https:\/\/www.youtube.com\/watch?v=smiu01pLosI)\n- [\u3010\u7570\u5e38\u6aa2\u6e2c\u3011\u5b64\u7acb\u68ee\u6797\uff08Isolation Forest\uff09\u6f14\u7b97\u6cd5\u7c21\u4ecb](https:\/\/codingnote.cc\/zh-tw\/p\/177980\/)\n- [The Facebook Prophet Prediction Model and Product Analytics](https:\/\/foxworthy-8036.medium.com\/the-facebook-prophet-prediction-model-and-product-analytics-a1db05fbe454)\n- [Fortune-Telling with Python: An Intro to Facebook Prophet](https:\/\/www.youtube.com\/watch?v=95-HMzxsghY)\n- [Quick Start | Prophet - Facebook Open Source](https:\/\/facebook.github.io\/prophet\/docs\/quick_start.html)\n- [Facebook\u6642\u9593\u5e8f\u5217\u9810\u6e2c\u6f14\u7b97\u6cd5Prophet\u7684\u7814\u7a76](https:\/\/iter01.com\/10666.html)\n\nThis notebook was created based on this amazing notebook:\n- [@benroshan](https:\/\/www.kaggle.com\/benroshan)\n - [\u26a0\ufe0f Anomaly Detection \ud83d\udea8 AMONG US](https:\/\/www.kaggle.com\/benroshan\/anomaly-detection-among-us)\n \nPlease go check it out and give applause to the author!","44f40aec":"Now let's call the library and initialize the `changepoint_range` to 95%. It is just the **confidence level** fixed for any statistical analysis. Here we only let 5% margin of error because finding an anamoly comes under rigorous scrutiny of data.","9d4665ff":"# Next: Ensemble\/ Vote?\nEnsemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model...","85911273":"**Discussions**:<br>\nWe don't have the data for entire 2015, instead we have only for 1 month(Sep-17days) and it doesn't exhibit seasonality - we can reject our 4th assumption and since cyan said it's true I feel cyan is sus.<br>\nAnd, let's straight away reject the 2nd assumption as we don't have enough data to prove it, ORANGE IS SUS MAX.\n\n| Inocents | Statements |\n| --- | ---------- |\n| **Pink** | I was working on increasing speed during the weekends at medbay |\n| ~~**Orange**~~ | ~~I was working on increasing speed during holiday months such as December and January at admin~~ |\n| **Yellow** | I was working on increasing speed during the late night hours at storage |\n| ~~**Cyan**~~ | ~~I proposed a strategy to form a seasonality across 2015 for speed at shields~~ |\n| **Red** | I worked really hard during Sep 4- Sep 10 to fix our ship at reactor |","139139d7":"**Actual Insights**:\n- We have **only one data point from Aug** and we can't consider the left bar and if you see the September bar we can see the average speed was around 81.9\n- We can see **3 points on Sep end** which looks like an outlier, but that doesn't mean they are anamoly\n- We can also notice that there are **no speed recorded in the mid Sep**, Could it be a shutdown ?and that might even invite cyber attacks in our spaceship","7a0c9476":"We can see the future predicted values and it follows the same seasonal patterns. Let's check the model components which gives us the trend pattern followed across different time.","3917571e":"According to dataset information, it has the following features :\n- **timestamp**:\n - This is the date and time when click is made by the visitor in the website\n- **value**:\n - This is the speed recorded in the specific sensor\n\n*Take a note that the speed (value) doesn't actually have any units nor the metadata doesn't have any information on that.*","b20d857a":"Alright, I'll initalize the library with `contamination = 0.01`. We can also fix the `contamination` rate as **per the domain**. Since we got only one imposter I have set the `threshold` very low.","de381b41":"**Actual Insights**:\n- We can also see the recordings started at Aug 31 6pm to Sep 17 6 pm. In these days no speed recorded during the rest of the hours.\n- We can also notice that there are several shutdown of sensors happening between the hours. Are those Anomalys ? Let's find out!","5cd4dd24":"# Sus Pattern Via Visuals - RED SUS?\nLet's see the anomaly patterns that are visible to naked eye. Here, the anomaly points which are highlighted may not be an anamoly thrown by the algorithm since it purely based on visualizations","15adc7be":"### Algorithm Workflow\n\n> Isolation forest is an **unsupervised learning algorithm** for anomaly detection that works on the principle of isolating anomalies, instead of the most common techniques of profiling normal points.\n> 1. In the first stage, a training dataset is used to build iTrees as described in previous sections.\n> 2. In the second stage, each instance in the test set is passed through the iTrees build in the previous stage, and a proper \u201c`anomaly score`\u201d is assigned to the instance. Once all the instances in the test set have been assigned an `anomaly score`, it is possible to mark as \u201canomaly\u201d any point whose score is greater than a predefined `threshold`, which depends on the **domain** the analysis is being applied to.","a095bf25":"## Histogram and Scatter on datetime\nLet's plot combined chart. If you wanna find some imposters in our data, scatter and box plot are the best.","d15f40b6":"### Data Preprocessing\n\nFirst let's rename the columns according to the prophet's standards. The input to Prophet is always a dataframe with two columns: ds and y. The ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp. The y column must be numeric, and represents the measurement we wish to forecast.","bc469085":"## Let's Brainstorm What Happened\n**Suspicious activity 1**<br>\nThere are no speed recorded during this period, the sensor got stuck, the imposter must have SABOTAGED the sensors to get in the spaceship without alerting anyone. Imposter is still among us and he possibly could have entered during this time","2d07783c":"# Emergency Meetings\n| Inocents | Statements |\n| --- | ---------- |\n| **Pink** | I was working on increasing speed during the weekends at medbay |\n| **Orange** | I was working on increasing speed during holiday months such as December and January at admin |\n| **Yellow** | I was working on increasing speed during the late night hours at storage |\n| **Cyan** | I proposed a strategy to form a seasonality across 2015 for speed at shields |\n| **Red** | I worked really hard during Sep 4- Sep 10 to fix our ship at reactor |\n\nLet's putforth our work status and check each's proposal by looking at the visualizations."}}