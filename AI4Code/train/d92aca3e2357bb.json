{"cell_type":{"e485403c":"code","efe1b954":"code","7ed63bef":"code","e4b0c58e":"code","c0c6cd5e":"code","e218c974":"code","fb2d28f0":"code","1c94d1ee":"code","c36df032":"code","6fcbd898":"code","1a268aa3":"code","223d688a":"code","4f4e78ba":"code","2bc891b8":"code","bcc15ced":"code","344dc9b9":"code","31f5deff":"code","5642aa21":"code","297fc73f":"code","f2b3f2ab":"code","def87754":"code","fb771466":"code","b674396b":"code","5059651d":"markdown","f716aa6e":"markdown","26dc6db9":"markdown","b54e6499":"markdown","54a69e09":"markdown","800e3ff3":"markdown","e8b23123":"markdown","e5a2f2af":"markdown","8f87e4ef":"markdown","4f50e8f1":"markdown","eb4176c8":"markdown","9f202d89":"markdown","2f9b47d8":"markdown","d2397131":"markdown","c7c11aaf":"markdown","ef5609b0":"markdown","2ec8cbe4":"markdown","2c95ecd4":"markdown","09156960":"markdown","94d6d601":"markdown","d6798030":"markdown","9d59a125":"markdown","9f78dd33":"markdown"},"source":{"e485403c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport plotly.graph_objects as go\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing \nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom category_encoders import HashingEncoder\nfrom category_encoders import WOEEncoder\nfrom category_encoders import BinaryEncoder\n\nfrom sklearn import metrics\nimport itertools\nimport gc\nimport matplotlib.pyplot as plt\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score","efe1b954":"!pip install pandas_profiling","7ed63bef":"!pip install category_encoders","e4b0c58e":"train = pd.read_csv(\"\/kaggle\/input\/term-deposit-prediction-data-set\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/term-deposit-prediction-data-set\/test.csv\")\n\ntrain_back = train.copy()\ntest_back = test.copy()\n\nnumerical_features = [feature for feature in train.columns if train[feature].dtypes != 'O']\nprint(\"Numerical_features: \",numerical_features)\ncategorical_features = [feature for feature in train.columns if train[feature].dtypes == 'O']\nprint(\"Categorical_features: \",categorical_features)\n\ntrain.info()","c0c6cd5e":"from pandas_profiling import ProfileReport\nprofile = ProfileReport(train)\nprofile.to_widgets()","e218c974":"# Hack - There are some zero or 01 values. So ading 1 or 2, before doing log transformation.\ntrain['duration'] = train['duration'] + 1\ntrain['previous'] = train['previous'] + 2\ntrain['pdays'] = train['pdays'] + 2\ntrain['balance'] = train['balance'] + 1\ncols = ['age','pdays','previous','campaign','duration']\n\nfor feature in cols:\n    print(\"\\nMin\/Max values of {} are {}, {}\".format(feature, train[feature].min(), train[feature].max()))    \n    if 0 in train[feature].unique():\n        pass\n    else:\n        try:\n            train[feature] = np.log(train[feature])           \n        except:\n            print(\"some error in train: \", feature)\n    print(\"After log, transformation - Min\/Max values of {} are {} - {}\".format(feature, train[feature].min(), train[feature].max()))\n\nprint(\" = \" * 60)\ntest['duration'] = test['duration'] + 1\ntest['previous'] = test['previous'] + 2\ntest['pdays'] = test['pdays'] + 2\n\nfor feature in cols:\n    print(\"\\nMin\/Max values of {} are {}, {}\".format(feature, test[feature].min(), test[feature].max()))    \n    if 0 in test[feature].unique():\n        pass\n    else:\n        try:\n            test[feature] = np.log(test[feature])\n        except:\n            print(\"some error in test: \", feature)\n    print(\"After log transformation, Min\/Max values of {} are {} - {}\".format(feature, test[feature].min(), test[feature].max()))","fb2d28f0":"x_data = ['Age', 'Pdays', 'Previous', 'Duration', 'Campaign']\n\nN = 50\n\ny0 = train['age']\ny1 = train['pdays']\ny2 = train['previous']\ny3 = train['duration']\ny4 = train['campaign']\n\ny_data = [y0, y1, y2, y3, y4]\n\ncolors = ['rgba(93, 164, 214, 0.5)', 'rgba(255, 144, 14, 0.5)', 'rgba(44, 160, 101, 0.5)', 'rgba(255, 65, 54, 0.5)', 'rgba(207, 114, 255, 0.5)']\n\nfig = go.Figure()\n\nfor xd, yd, cls in zip(x_data, y_data, colors):\n        fig.add_trace(go.Box(\n            y=yd,\n            name=xd,\n            boxpoints='outliers', notched=True,\n            jitter=0.5,\n            whiskerwidth=0.4,\n            fillcolor=cls,\n            marker_size=2,\n            line_width=1)\n        )\n\nfig.update_layout(\n    title='Box plots of numerical columns',\n    yaxis=dict(\n        autorange=True,\n        showgrid=True,\n        zeroline=True,\n        dtick=5,\n        gridcolor='rgb(255, 255, 255)',\n        gridwidth=1,\n        zerolinecolor='rgb(255, 255, 255)',\n        zerolinewidth=2,\n    ),\n    margin=dict(l=40, r=30, b=80, t=100,),\n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)',\n    showlegend=False\n)\nfig.show()","1c94d1ee":"# Discover the number of categories within each categorical feature:\nlen(train.job.unique()),  len(train.poutcome.unique()),len(train.month.unique()),len(train.contact.unique()), len(train.marital.unique()), len(train.loan.unique()), len(train.education.unique()), len(train.housing.unique()),len(train.default.unique())","c36df032":"train_back","6fcbd898":"cat_features = [feature for feature in train_back.columns if ( train_back[feature].dtypes == 'O') ]\ncat_features.append('day')\nprint(\"Removed columns - \", cat_features.pop(-2))\n\n# We create a helper function to get the scores for each encoding method:\ndef get_score(model, X, y, X_val, y_val,X_test):\n    model.fit(X, y)\n    y_pred = model.predict_proba(X_val)[:,1]\n    score = roc_auc_score(y_val, y_pred)\n    y_pred = model.predict(X_test)\n    return score,y_pred\n\ntarget_feature =  'subscribed'\n\nSEED = 123\nlogit = LogisticRegression(random_state=SEED)\nrf = RandomForestClassifier(random_state=SEED)","1a268aa3":"lb_train = train_back.copy()\nlb_test = test_back.copy()\n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor feature in cat_features:\n  lb_train[feature]= label_encoder.fit_transform(lb_train[feature]) \n  lb_test[feature]= label_encoder.fit_transform(lb_test[feature]) \n\nlb_train[target_feature] = lb_train[target_feature].map({\"yes\":1, \"no\":0})\nlb_y = lb_train[target_feature]\nlb_train.drop([target_feature],axis= 1, inplace=True)\n\n# feature scaling\nscaler = StandardScaler()\nlb_train = scaler.fit_transform(lb_train)\nlb_test = scaler.transform(lb_test)\n\n# Split dataset into train and validation subsets:\nX_train, X_val, y_train, y_val = train_test_split(lb_train, lb_y, test_size=0.2, random_state = SEED)\n\nbaseline_logit_with_standard,y_pred_logit = get_score(logit, X_train, y_train, X_val, y_val, lb_test)\nprint('Logistic Regression score without feature engineering:', baseline_logit_with_standard)\n\nbaseline_rf_with_standard,y_pred_rf = get_score(rf, X_train, y_train, X_val, y_val, lb_test)\nprint('Random Forest score without feature engineering:', baseline_rf_with_standard)\n\ndel lb_train;\ngc.collect() \ndel lb_test;\ngc.collect() ","223d688a":"lb_train = train_back.copy()\nlb_test = test_back.copy()\n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor feature in cat_features:\n  lb_train[feature]= label_encoder.fit_transform(lb_train[feature]) \n  lb_test[feature]= label_encoder.fit_transform(lb_test[feature]) \n\nlb_train[target_feature] = lb_train[target_feature].map({\"yes\":1, \"no\":0})\nlb_y = lb_train[target_feature]\nlb_train.drop([target_feature],axis= 1, inplace=True)\n\n# feature scaling\nscaler = MinMaxScaler()\nlb_train = scaler.fit_transform(lb_train)\nlb_test = scaler.transform(lb_test)\n\n# Split dataset into train and validation subsets:\nX_train, X_val, y_train, y_val = train_test_split(lb_train, lb_y, test_size=0.2, random_state = SEED)\n\nbaseline_logit_with_minmax, y_pred_logit_minmax = get_score(logit, X_train, y_train, X_val, y_val, lb_test)\nprint('Logistic Regression score without feature engineering:', baseline_logit_with_minmax)\n\nbaseline_rf_with_minmax,y_pred_rf_minmax = get_score(rf, X_train, y_train, X_val, y_val, lb_test)\nprint('Random Forest score without feature engineering:', baseline_rf_with_minmax)\n\ndel lb_train;\ngc.collect() \ndel lb_test;\ngc.collect() ","4f4e78ba":"ohe_train = train_back.copy()\nohe_test = test_back.copy()\n\none_hot_enc = OneHotEncoder(sparse=False)\n\nohe_train[target_feature] = ohe_train[target_feature].map({\"yes\":1, \"no\":0})\nohe_y = ohe_train[target_feature]\nohe_train.drop([target_feature],axis= 1, inplace=True)\n\nprint(\"Before Target Encoder - Shape of Train\/Test: \", ohe_train.shape, ohe_test.shape)\nohe_train = (one_hot_enc.fit_transform(ohe_train[cat_features]))\nohe_test = (one_hot_enc.transform(ohe_test[cat_features]))\nprint(\"After One-Hot Encoder - Shape of Train\/Test: \", ohe_train.shape, ohe_test.shape)\n\n# feature scaling\nscaler = StandardScaler()\nohe_train = scaler.fit_transform(ohe_train)\nohe_test = scaler.transform(ohe_test)\n\n# Split dataset into train and validation subsets:\nohe_X_train, ohe_X_val, ohe_y_train, ohe_y_val = train_test_split(ohe_train, ohe_y, test_size=0.2, random_state = SEED)\n\nohe_logit_score, y_pred_logit_ohe = get_score(logit, ohe_X_train, ohe_y_train, ohe_X_val, ohe_y_val, ohe_test)\nprint('Logistic Regression score without feature engineering:', ohe_logit_score)\n\nohe_rf_score, y_pred_rf_ohe = get_score(rf, ohe_X_train, ohe_y_train, ohe_X_val, ohe_y_val, ohe_test)\nprint('Random Forest score without feature engineering:', ohe_rf_score)\n\ndel ohe_train;\ngc.collect() \ndel ohe_test;\ngc.collect() ","2bc891b8":"hash_train = train_back.copy()\nhash_test = test_back.copy()\n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor feature in cat_features:\n  hash_train[feature]= label_encoder.fit_transform(hash_train[feature]) \n  hash_test[feature]= label_encoder.fit_transform(hash_test[feature]) \n\nhash_train[target_feature] = hash_train[target_feature].map({\"yes\":1, \"no\":0})\nhash_y = hash_train[target_feature]\nhash_train.drop([target_feature],axis= 1, inplace=True)\n\nfrom category_encoders import TargetEncoder\ncolumns = ['job', 'marital', 'education', 'default', \\\n       'housing', 'loan', 'contact', 'day', 'month', 'poutcome']\n\ntarg_enc = TargetEncoder(cols = columns, smoothing=8, min_samples_leaf=5).fit(hash_train, hash_y)\n\nprint(\"Before Target Encoder - Shape of Train\/Test: \", hash_train.shape, hash_test.shape)\nhash_train_te = targ_enc.transform(hash_train.reset_index(drop=True))\nhash_test_te = targ_enc.transform(hash_test.reset_index(drop=True))\nprint(\"After Target Encoder - Shape of Train\/Test: \", hash_train_te.shape, hash_test_te.shape)\n\n# Split dataset into train and validation subsets:\nX_train, X_val, y_train, y_val = train_test_split(hash_train_te, hash_y, test_size=0.2, random_state = SEED)\n\nte_logit_score, y_pred_logit_te = get_score(logit, X_train, y_train, X_val, y_val, hash_test)\nprint('Logistic Regression score with target encoding:', te_logit_score)\n\nte_rf_score, y_pred_rf_te = get_score(rf, X_train, y_train, X_val, y_val, hash_test)\nprint('Random Forest score with target encoding:', te_rf_score)\n\ndel hash_train;\ngc.collect() \ndel hash_test;\ngc.collect() ","bcc15ced":"he_train = train_back.copy()\nhe_test = test_back.copy()\n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor feature in cat_features:\n  he_train[feature]= label_encoder.fit_transform(he_train[feature]) \n  he_test[feature]= label_encoder.fit_transform(he_test[feature]) \n\nhe_train[target_feature] = he_train[target_feature].map({\"yes\":1, \"no\":0})\nhe_y = he_train[target_feature]\nhe_train.drop([target_feature],axis= 1, inplace=True)\n\ncolumns = ['job', 'marital', 'education', 'default', \\\n       'housing', 'loan', 'contact', 'day', 'month', 'poutcome']\n\ntarg_enc = HashingEncoder(cols = columns,  n_components=1000).fit(he_train, he_y)\nprint(\"Before Hashing Encoder - Shape of Train\/Test: \", he_train.shape, he_test.shape)\nhe_train = targ_enc.transform(he_train.reset_index(drop=True))\nhe_test = targ_enc.transform(he_test.reset_index(drop=True))\nprint(\"After Hashing Encoder - Shape of Train\/Test: \", he_train.shape, he_test.shape)\n\n# Split dataset into train and validation subsets:\nX_train_te, X_val_te, y_train_te, y_val_te = train_test_split(he_train, he_y, test_size=0.2, random_state = SEED)\n\nhe_logit_score, y_pred_logit_he = get_score(logit, X_train_te, y_train_te, X_val_te, y_val_te, he_test)\nprint('Logistic Regression score with Hashing encoding:', he_logit_score)\n\nhe_rf_score, y_pred_rf_he = get_score(rf, X_train_te, y_train_te, X_val_te, y_val_te, he_test)\nprint('Random Forest score with Hashing encoding:', he_rf_score)\n\ndel he_train;\ngc.collect() \ndel he_test;\ngc.collect() ","344dc9b9":"woe_train = train_back.copy()\nwoe_test = test_back.copy()\n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor feature in cat_features:\n  woe_train[feature]= label_encoder.fit_transform(woe_train[feature]) \n  woe_test[feature]= label_encoder.fit_transform(woe_test[feature]) \n\nwoe_train[target_feature] = woe_train[target_feature].map({\"yes\":1, \"no\":0})\nwoe_y = woe_train[target_feature]\nwoe_train.drop([target_feature],axis= 1, inplace=True)\n\ncolumns = ['job', 'marital', 'education', 'default', \\\n       'housing', 'loan', 'contact', 'day', 'month', 'poutcome']\nwoe_enc = WOEEncoder(cols=columns, random_state=17).fit(woe_train, woe_y)\n\nprint(\"Before WOE Encoder - Shape of Train\/Test: \", woe_train.shape, woe_test.shape)\nwoe_train_wo = woe_enc.transform(woe_train.reset_index(drop=True))\nwoe_test_wo = woe_enc.transform(woe_test.reset_index(drop=True))\nprint(\"After WOE Encoder - Shape of Train\/Test: \", woe_train_wo.shape, woe_test_wo.shape)\n\n# Split dataset into train and validation subsets:\nX_train_woe, X_val_woe, y_train_woe, y_val_woe = train_test_split(woe_train_wo, woe_y, test_size=0.2, random_state = SEED)\n\nwoe_logit_score, y_pred_logit_woe = get_score(logit, X_train_woe, y_train_woe, X_val_woe, y_val_woe, woe_test)\nprint('Logistic Regression score with Weight Of Evidence encoding:', woe_logit_score)\n\nwoe_rf_score, y_pred_rf_woe = get_score(rf, X_train_woe, y_train_woe, X_val_woe, y_val_woe, woe_test)\nprint('Random Forest score with Weight Of Evidence encoding:', woe_rf_score)\n\ndel woe_train;\ngc.collect() \ndel woe_test;\ngc.collect() ","31f5deff":"be_train = train_back.copy()\nbe_test = test_back.copy()\n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor feature in cat_features:\n  be_train[feature]= label_encoder.fit_transform(be_train[feature]) \n  be_test[feature]= label_encoder.fit_transform(be_test[feature]) \n\nbe_train[target_feature] = be_train[target_feature].map({\"yes\":1, \"no\":0})\nbe_y = be_train[target_feature]\nbe_train.drop([target_feature],axis= 1, inplace=True)\n\ncolumns = ['job', 'marital', 'education', 'default', \\\n       'housing', 'loan', 'contact', 'day', 'month', 'poutcome']\nbe_enc = BinaryEncoder(cols=columns).fit(be_train, be_y)\n\nprint(\"Before Binary Encoder - Shape of Train\/Test: \", be_train.shape, be_test.shape)\nbe_train = be_enc.transform(be_train.reset_index(drop=True))\nbe_test = be_enc.transform(be_test.reset_index(drop=True))\nprint(\"After Binary Encoder - Shape of Train\/Test: \",be_train.shape, be_test.shape)\n\n# Split dataset into train and validation subsets:\nX_train_be, X_val_be, y_train_be, y_val_be = train_test_split(be_train, be_y, test_size=0.2, random_state = SEED)\n\nbe_logit_score, y_pred_logit_be = get_score(logit, X_train_be, y_train_be, X_val_be, y_val_be, be_test)\nprint('Logistic Regression score with Binary encoding:', be_logit_score)\n\nbe_rf_score, y_pred_rf_be = get_score(rf, X_train_be, y_train_be, X_val_be, y_val_be, be_test)\nprint('Random Forest score with Binary encoding:', be_rf_score)\n\ndel be_train;\ngc.collect() \ndel be_test;\ngc.collect() ","5642aa21":"from prettytable import PrettyTable\n\nmyTable = PrettyTable([\"SNo.\", \"Encoder\", \"Logistic\", \"Random Forest\", \"No. of Cols added\"]) \n  \n# Add rows \nmyTable.add_row([\"1\", \"One Hot\", round(ohe_logit_score,4), round(ohe_rf_score,4), 58 ]) \nmyTable.add_row([\"2\", \"Hashing\", round(he_logit_score,4), round(he_rf_score,4), 0]) \nmyTable.add_row([\"3\", \"Target\", round(te_logit_score,4), round(te_rf_score,4), 0 ]) \nmyTable.add_row([\"4\", \"Weight of Evaluation\", round(woe_logit_score,4), round(woe_rf_score,4), 0]) \nmyTable.add_row([\"5\", \"Binary\", round(be_logit_score,4), round(be_rf_score,4) , 24 ]) \nmyTable.add_row([\"6\", \"Label Encoder + Standard Scaler\", round(baseline_logit_with_standard,4), round(baseline_rf_with_standard,4), 0 ]) \nmyTable.add_row([\"7\", \"Label Encoder + MinMax Scaler\", round(baseline_logit_with_minmax,4), round(baseline_rf_with_minmax, 4), 0 ]) \n\nprint(myTable)","297fc73f":"models = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier(), AdaBoostClassifier(), GradientBoostingClassifier(), KNeighborsClassifier(), SVC(), XGBClassifier()]\nmodel_names = ['LogisticRegression', 'DecisionTreeClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'GradientBoostingClassifier', 'KNeighborsClassifier', 'SVC', 'XGBClassifier']\naccuracy_train = []\naccuracy_val = []\nfor model in models:\n    mod = model\n    mod.fit(X_train, y_train)\n    y_pred_train = mod.predict(X_train)\n    y_pred_val = mod.predict(X_val)\n    accuracy_train.append(accuracy_score(y_train, y_pred_train))\n    accuracy_val.append(accuracy_score(y_val, y_pred_val))\ndata = {'Modelling Algorithm' : model_names, 'Train Accuracy' : accuracy_train, 'Validation Accuracy' : accuracy_val}\ndata = pd.DataFrame(data)\ndata['Difference'] = ((np.abs(data['Train Accuracy'] - data['Validation Accuracy'])) * 100)\/(data['Train Accuracy'])\ndata.sort_values(by = 'Difference')","f2b3f2ab":"xgb = XGBClassifier()\n\nparameters = {   'eta': [0.1], 'colsample_bytree':[0.7],\n               'min_child_weight': [5], 'max_depth' :[7], 'max_features':[5],'subsample': [0.7],\n               'reg_alpha':[1], 'n_estimators': [100] ,'seed':[11] }\n\nxgb_clf = GridSearchCV(xgb, parameters, cv = 5, n_jobs = -1, verbose=1)","def87754":"xgb_train = train_back.copy()\nxgb_test = test_back.copy()\n\nxgb_train['duration'] = xgb_train['duration'] + 1\nxgb_train['previous'] = xgb_train['previous'] + 2\nxgb_train['pdays'] = xgb_train['pdays'] + 2\n\ncols = ['age','pdays','previous','campaign','duration']\n\nfor feature in cols:\n    print(\"\\nMin\/Max values of {} are {}, {}\".format(feature, xgb_train[feature].min(), xgb_train[feature].max()))    \n    if 0 in xgb_train[feature].unique():\n        pass\n    else:\n        try:\n            xgb_train[feature] = np.log(xgb_train[feature])           \n        except:\n            print(\"some error in train: \", feature)\n    print(\"After log, transformation - Min\/Max values of {} are {} - {}\".format(feature, xgb_train[feature].min(), xgb_train[feature].max()))\n\nprint(\" = \" * 60)\n\nxgb_test['duration'] = xgb_test['duration'] + 1\nxgb_test['previous'] = xgb_test['previous'] + 2\nxgb_test['pdays'] = xgb_test['pdays'] + 2\n\nfor feature in cols:\n    print(\"\\nMin\/Max values of {} are {}, {}\".format(feature, xgb_test[feature].min(), xgb_test[feature].max()))    \n    if 0 in xgb_test[feature].unique():\n        pass\n    else:\n        try:\n            xgb_test[feature] = np.log(xgb_test[feature])\n        except:\n            print(\"some error in test: \", feature)\n    print(\"After log transformation, Min\/Max values of {} are {} - {}\".format(feature, xgb_test[feature].min(), xgb_test[feature].max()))\n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor feature in cat_features:\n  xgb_train[feature]= label_encoder.fit_transform(xgb_train[feature]) \n  xgb_test[feature]= label_encoder.fit_transform(xgb_test[feature]) \n\nxgb_train[target_feature] = xgb_train[target_feature].map({\"yes\":1, \"no\":0})\nxgb_y = xgb_train[target_feature]\nxgb_train.drop([target_feature],axis= 1, inplace=True)\n\nxgb_train.drop(['ID'],axis= 1, inplace=True)\nxgb_test.drop(['ID'],axis= 1, inplace=True)\n\n# feature scaling\nscaler = StandardScaler()\nxgb_train = scaler.fit_transform(xgb_train)\nxgb_test = scaler.transform(xgb_test)\n\n# Split dataset into train and validation subsets:\nX_train, X_val, y_train, y_val = train_test_split(xgb_train, xgb_y, test_size=0.25, random_state = SEED)\n\nxgb_clf.fit(X_train, y_train)\npredictions = xgb_clf.predict_proba(X_val)[:,1]\nscore = roc_auc_score(y_val,predictions)\nprint(\"XGB score: \",score)\n\ny_pred = xgb_clf.predict(X_val)\n\ndel xgb_train;\ngc.collect() \ndel xgb_test;\ngc.collect() ","fb771466":"y_pred_test = pd.DataFrame(y_pred, columns = ['Prediction'])\nprint(y_pred_test.head())\n\ny_pred_test.to_csv('Prediction.csv')","b674396b":"#Generate Confusion Matrix\n\nconf_matrix = confusion_matrix(y_pred,y_val)\nprint(conf_matrix)\n\nprint(\" = \"*60)\nprint(classification_report(y_pred,y_val))","5059651d":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of contents<\/h3>\n\n* [Notebook](#note)\n    * [1. Data](#n1)\n    * [2. EDA](#n2)\n    * [3. Log Transformation of numerical columns](#n3)\n    * [4. Box plots of numerical columns](#n4)\n    * [5. Unique Values of Various Categorical Columns](#n5)\n    * [6. Label Encoder with StandardScaler](#n6)\n    * [7. Label Encoder with MinMaxScaler](#n7)\n    * [8. One-Hot Encoder](#n8)\n    * [9. Target Encoder](#n9)\n\t* [10. Hashing Encoder](#n10)\n\t* [11. Weight of Evidence](#n11)\n\t* [12. Binary Encoder](#n12)\n\t* [13. Comparison Table - Encoders](#n13)\n\t* [14. Models Comparison](#n14)\n\t* [15. Modelling with Best Classifier](#n15)\n\t* [16. Confusion Matrix and Classification report](#n16)","f716aa6e":"<a id=\"n3\"><\/a>\n<font color=\"darkblue\" size=+3>Log transformation of Numerical Columns<\/font>","26dc6db9":"<a id=\"n11\"><\/a>\n<font color=\"darkblue\" size=+3>Weight Of Evidence (WOE)<\/font>","b54e6499":"# Encoders\n* Since there are a bunch of categorical columns, I decided to try the efficacy of various encoders.\n* Label encoder was the simplest and most effective\n* Since dates are not given and I presume it didn't matter\n* I have used the months, days columns as categorical variables\n* To compare the different encoders, I have used basic models - Logistic Regression and Random Forest models to conclude\n* Label Encoder - I have used with Standard scaler and MinMax scaler. But there was not a remarkable difference.","54a69e09":"<a id=\"n2\"><\/a>\n<font color=\"darkblue\" size=+3>EDA through Profiling Report<\/font>","800e3ff3":"<a id=\"n8\"><\/a>\n<font color=\"darkblue\" size=+3>One-Hot Encoder<\/font>","e8b23123":"<a id=\"n1\"><\/a>\n<font color=\"darkblue\" size=+3>Reading Data and types of data<\/font>","e5a2f2af":"<a id=\"n6\"><\/a>\n<font color=\"darkblue\" size=+3>Label Encoder with StandardScaler<\/font>","8f87e4ef":"<a id=\"n9\"><\/a>\n<font color=\"darkblue\" size=+3>Target Encoder<\/font>","4f50e8f1":"<a id=\"n7\"><\/a>\n<font color=\"darkblue\" size=+3>Label Encoder with MinMaxScaler<\/font>","eb4176c8":"<a id=\"n14\"><\/a>\n<font color=\"darkblue\" size=+3>Models Comparison<\/font>","9f202d89":"<font color=\"darkblue\" size=+3>Imports and Installations<\/font>","2f9b47d8":"<font size=\"+3\" color=\"darkblue\" ><b> <center><u>Term Deposit Subscription Prediction <\/u><\/center><\/b><\/font>","d2397131":"# Observations:\n* Target Encoder, Weight of Evalaution, Binary Encoder performed well in Random forest with 0.94 accuracy score.\n* However the above encoders were not performing well with  logistic regression.\n* Label encoder was performing well with both logistic and randomforest models\n* Extra Columns were added only to One hot and Binary encoders.","c7c11aaf":"<a id=\"n16\"><\/a>\n<font color=\"darkblue\" size=+3>Confusion Matrix and Classification report<\/font>","ef5609b0":"<a id=\"n5\"><\/a>\n<font color=\"darkblue\" size=+3>Unique Values of Various categorical Columns<\/font>","2ec8cbe4":"<a id=\"n4\"><\/a>\n<font color=\"darkblue\" size=+3>Box plots of Numerical Columns<\/font>","2c95ecd4":"* XGBClassifier is high on train and validation accuracy. So, I have picked this up.\n* roc_auc_score is at .93.\n","09156960":"<a id=\"n12\"><\/a>\n<font color=\"darkblue\" size=+3>Binary Encoder<\/font>","94d6d601":"<a id=\"n10\"><\/a>\n<font color=\"darkblue\" size=+3>Hashing Encoder<\/font>","d6798030":"* Label Encoder has been most effective encoder for categorical columns.\n* Below code has been borrrowed from Mohapatra's notebook.","9d59a125":"<a id=\"n15\"><\/a>\n<font color=\"darkblue\" size=+3>Modelling with Best Classifier<\/font>","9f78dd33":"<a id=\"n13\"><\/a>\n<font color=\"darkblue\" size=+3>Comparison Table - Encoders<\/font>"}}