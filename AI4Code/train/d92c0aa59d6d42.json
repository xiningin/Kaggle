{"cell_type":{"72dbe9c9":"code","7967b6bf":"code","c8750f5b":"code","b2997be9":"code","f06f3ef0":"code","0a6d1c65":"code","5fbbe85a":"code","cc8e3f72":"code","a556d93f":"code","50b9ba1b":"code","0677491b":"code","c69796c9":"code","6820f26f":"code","0a76e002":"code","6c136b19":"code","9acf774a":"code","c26eb695":"code","206c4a05":"code","1b61f337":"code","c372e230":"markdown","235eeca3":"markdown","86f8c61a":"markdown","b1de7a21":"markdown","cb0e5997":"markdown","0d29654e":"markdown","5b6500de":"markdown","e8f029b9":"markdown"},"source":{"72dbe9c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n# print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7967b6bf":"data=pd.read_csv('..\/input\/heart.csv')\ndata.head()","c8750f5b":"msk=np.random.rand(len(data))<0.8\ntrain_df=data[msk]\ntest_df=data[~msk]","b2997be9":"len(train_df)","f06f3ef0":"len(test_df)","0a6d1c65":"from sklearn import ensemble","5fbbe85a":"train_df_x=train_df.drop(columns=['target'])\ntrain_df_y=train_df.target\n\ntest_df_x=test_df.drop(columns=['target'])\ntest_df_y=test_df.target","cc8e3f72":"regr1=ensemble.RandomForestClassifier()","a556d93f":"regr1.fit(train_df_x,train_df_y)","50b9ba1b":"from sklearn.metrics import accuracy_score","0677491b":"train_ac=accuracy_score(regr1.predict(train_df_x),train_df_y)\ntrain_ac","c69796c9":"test_ac=accuracy_score(regr1.predict(test_df_x),test_df_y)\ntest_ac","6820f26f":"pd.DataFrame({'Features':train_df_x.columns,'Importance':regr1.feature_importances_}).plot.bar(x='Features')","0a76e002":"gbt=ensemble.GradientBoostingClassifier()","6c136b19":"gbt.fit(train_df_x,train_df_y)","9acf774a":"train_ac=accuracy_score(gbt.predict(train_df_x),train_df_y)\ntrain_ac","c26eb695":"test_ac=accuracy_score(gbt.predict(test_df_x),test_df_y)\ntest_ac","206c4a05":"pd.DataFrame({'Features':train_df_x.columns,'Importance':gbt.feature_importances_}).plot.bar(x='Features',color='orange')","1b61f337":"pd.DataFrame({'Features':train_df_x.columns,\n              'Importance_RF':regr1.feature_importances_,\n              'Importance_GBT':gbt.feature_importances_}).plot.bar(x='Features')","c372e230":"Let's look at the feature importance","235eeca3":"Creating a training and testing dataset","86f8c61a":"Let's analyse what factors are responsible for cause of heart disease","b1de7a21":"Reading the dataset","cb0e5997":"As you can see, both the models give different degree of feature importances.","0d29654e":"Let's look at Feature Importance","5b6500de":"This shows GBT had overfitting problem.\n\nIt is learning well on train model, but can't apply the same on test model.","e8f029b9":"Let's see what happens when we use Gradient Boosted Classifier"}}