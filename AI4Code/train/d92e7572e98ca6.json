{"cell_type":{"b9973796":"code","452fa3d9":"code","fe8464f7":"code","b34e5368":"code","db9d814c":"code","1a24eef7":"code","1d3bc52d":"code","4b8e1af7":"code","044b89f8":"code","7fcc51a9":"code","b8ce4202":"code","058dafbe":"code","a15aff1b":"code","17f86484":"code","00fe5350":"code","0de2f531":"code","c95d19d6":"markdown","c0efd832":"markdown","ddbfbc9f":"markdown","6e2f5111":"markdown","a6b11f86":"markdown","740aa086":"markdown","e11b0be8":"markdown","23426d27":"markdown","2dc4026f":"markdown","80225093":"markdown"},"source":{"b9973796":"max_events = None","452fa3d9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D # needed for 3D scatter plots\n%matplotlib inline \nimport seaborn as sns\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nPATH='..\/input\/'\n\nimport os\nprint(os.listdir(PATH))","fe8464f7":"train = pd.read_csv('{}\/train.csv'.format(PATH), nrows=max_events)\ntest  = pd.read_csv('{}\/test.csv'.format(PATH), nrows=max_events)\n\ny = train['Cover_Type']\ntrain.drop('Cover_Type', axis=1, inplace=True)\n\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","b34e5368":"print('Train shape: {}'.format(train.shape))\nprint('Test  shape: {}'.format(test.shape))","db9d814c":"train.info(verbose=False)","1a24eef7":"def convert_OHE2LE(df):\n    tmp_df = df.copy(deep=True)\n    for s_ in ['Soil_Type', 'Wilderness_Area']:\n        cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n        #deal with those OHE, where there is a sum over columns == 0\n        if 0 in sum_ohe:\n            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n                  .format(s_))\n            # dummy colmn name to be added\n            col_dummy = s_+'_dummy'\n            # add the column to the dataframe\n            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n            # add the name to the list of columns to be label-encoded\n            cols_s_.append(col_dummy)\n            # proof-check, that now the category is complete\n            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n            if 0 in sum_ohe:\n                 print(\"The category completion did not work\")\n        tmp_df[s_ + '_LE'] = tmp_df[cols_s_].idxmax(axis=1).str.replace(s_,'').astype(np.uint16)\n        tmp_df.drop(cols_s_, axis=1, inplace=True)\n    return tmp_df\n\n\n\ndef train_test_apply_func(train_, test_, func_):\n    xx = pd.concat([train_, test_])\n    xx_func = func_(xx)\n    train_ = xx_func.iloc[:train_.shape[0], :]\n    test_  = xx_func.iloc[train_.shape[0]:, :]\n\n    del xx, xx_func\n    return train_, test_","1d3bc52d":"train_x, test_x = train_test_apply_func(train, test, convert_OHE2LE)","4b8e1af7":"train_x.head()","044b89f8":"def preprocess(df_):\n    df_['fe_E_Min_02HDtH'] = (df_['Elevation']- df_['Horizontal_Distance_To_Hydrology']*0.2).astype(np.float32)\n    df_['fe_Distance_To_Hydrology'] = np.sqrt(df_['Horizontal_Distance_To_Hydrology']**2 + \n                                              df_['Vertical_Distance_To_Hydrology']**2).astype(np.float32)\n    \n    feats_sub = [('Elevation_Min_VDtH', 'Elevation', 'Vertical_Distance_To_Hydrology'),\n                 ('HD_Hydrology_Min_Roadways', 'Horizontal_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways'),\n                 ('HD_Hydrology_Min_Fire', 'Horizontal_Distance_To_Hydrology', 'Horizontal_Distance_To_Fire_Points')]\n    feats_add = [('Elevation_Add_VDtH', 'Elevation', 'Vertical_Distance_To_Hydrology')]\n    \n    for f_new, f1, f2 in feats_sub:\n        df_['fe_' + f_new] = (df_[f1] - df_[f2]).astype(np.float32)\n    for f_new, f1, f2 in feats_add:\n        df_['fe_' + f_new] = (df_[f1] + df_[f2]).astype(np.float32)\n        \n    # The feature is advertised in https:\/\/douglas-fraser.com\/forest_cover_management.pdf\n    df_['fe_Shade9_Mul_VDtH'] = (df_['Hillshade_9am'] * df_['Vertical_Distance_To_Hydrology']).astype(np.float32)\n    \n    # this mapping comes from https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/covtype\/covtype.info\n    climatic_zone = {}\n    geologic_zone = {}\n    for i in range(1,41):\n        if i <= 6:\n            climatic_zone[i] = 2\n            geologic_zone[i] = 7\n        elif i <= 8:\n            climatic_zone[i] = 3\n            geologic_zone[i] = 5\n        elif i == 9:\n            climatic_zone[i] = 4\n            geologic_zone[i] = 2\n        elif i <= 13:\n            climatic_zone[i] = 4\n            geologic_zone[i] = 7\n        elif i <= 15:\n            climatic_zone[i] = 5\n            geologic_zone[i] = 1\n        elif i <= 17:\n            climatic_zone[i] = 6\n            geologic_zone[i] = 1\n        elif i == 18:\n            climatic_zone[i] = 6\n            geologic_zone[i] = 7\n        elif i <= 21:\n            climatic_zone[i] = 7\n            geologic_zone[i] = 1\n        elif i <= 23:\n            climatic_zone[i] = 7\n            geologic_zone[i] = 2\n        elif i <= 34:\n            climatic_zone[i] = 7\n            geologic_zone[i] = 7\n        else:\n            climatic_zone[i] = 8\n            geologic_zone[i] = 7\n            \n    df_['Climatic_zone_LE'] = df_['Soil_Type_LE'].map(climatic_zone).astype(np.uint8)\n    df_['Geologic_zone_LE'] = df_['Soil_Type_LE'].map(geologic_zone).astype(np.uint8)\n    return df_","7fcc51a9":"train_x = preprocess(train_x)\ntest_x = preprocess(test_x)","b8ce4202":"from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom  sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb","058dafbe":"y = y-1","a15aff1b":"X_train, X_test, y_train, y_test = train_test_split(train_x, y, test_size=0.15, random_state=315, stratify=y)","17f86484":"def learning_rate_decay_power_0995(current_iter):\n    base_learning_rate = 0.15\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return lr if lr > 1e-2 else 1e-2\n\nclfs = {'rf': (RandomForestClassifier(n_estimators=200, max_depth=1, random_state=314, n_jobs=4),\n               {'max_depth': [20,25,30,35,40,45,50]}, \n               {}),\n        'xt': (ExtraTreesClassifier(n_estimators=200, max_depth=1, max_features='auto',random_state=314, n_jobs=4),\n               {'max_depth': [20,25,30,35,40,45,50]},\n               {}),\n        'lgbm': (lgb.LGBMClassifier(max_depth=-1, min_child_samples=400, \n                                 random_state=314, silent=True, metric='None', \n                                 n_jobs=4, n_estimators=5000, learning_rate=0.1), \n                 {'colsample_bytree': [0.75], 'min_child_weight': [0.1,1,10], 'num_leaves': [18, 20,22], 'subsample': [0.75]}, \n                 {'eval_set': [(X_test, y_test)], \n                  'eval_metric': 'multi_error', 'verbose':500, 'early_stopping_rounds':100, \n                  'callbacks':[lgb.reset_parameter(learning_rate=learning_rate_decay_power_0995)]}\n                )\n       }","00fe5350":"gss = {}\nfor name, (clf, clf_pars, fit_pars) in clfs.items():\n    print('--------------- {} -----------'.format(name))\n    gs = GridSearchCV(clf, param_grid=clf_pars,\n                            scoring='accuracy',\n                            cv=5,\n                            n_jobs=1,\n                            refit=True,\n                            verbose=True)\n    gs = gs.fit(X_train, y_train, **fit_pars)\n    print('{}:  train = {:.4f}, test = {:.4f}+-{:.4f} with best params {}'.format(name, \n                                                                                  gs.cv_results_['mean_train_score'][gs.best_index_],\n                                                                                  gs.cv_results_['mean_test_score'][gs.best_index_],\n                                                                                  gs.cv_results_['std_test_score'][gs.best_index_],\n                                                                                  gs.best_params_\n                                                                                 ))\n    print(\"Valid+-Std     Train  :   Parameters\")\n    for i in np.argsort(gs.cv_results_['mean_test_score'])[-5:]:\n        print('{1:.3f}+-{3:.3f}     {2:.3f}   :  {0}'.format(gs.cv_results_['params'][i], \n                                        gs.cv_results_['mean_test_score'][i], \n                                        gs.cv_results_['mean_train_score'][i],\n                                        gs.cv_results_['std_test_score'][i]))\n    gss[name] = gs","0de2f531":"# gss = {}\n# for name, (clf, clf_pars, fit_pars) in clfs.items():\n#     if name == 'lgbm':\n#         continue\n#     print('--------------- {} -----------'.format(name))\n#     gs = GridSearchCV(clf, param_grid=clf_pars,\n#                             scoring='accuracy',\n#                             cv=5,\n#                             n_jobs=1,\n#                             refit=True,\n#                             verbose=True)\n#     gs = gs.fit(X_train, y_train, **fit_pars)\n#     print('{}:  train = {:.4f}, test = {:.4f}+-{:.4f} with best params {}'.format(name, \n#                                                                                   gs.cv_results_['mean_train_score'][gs.best_index_],\n#                                                                                   gs.cv_results_['mean_test_score'][gs.best_index_],\n#                                                                                   gs.cv_results_['std_test_score'][gs.best_index_],\n#                                                                                   gs.best_params_\n#                                                                                  ))\n#     print(\"Valid+-Std     Train  :   Parameters\")\n#     for i in np.argsort(gs.cv_results_['mean_test_score'])[-5:]:\n#         print('{1:.3f}+-{3:.3f}     {2:.3f}   :  {0}'.format(gs.cv_results_['params'][i], \n#                                         gs.cv_results_['mean_test_score'][i], \n#                                         gs.cv_results_['mean_train_score'][i],\n#                                         gs.cv_results_['std_test_score'][i]))\n#     gss[name] = gs","c95d19d6":"## OHE into LE","c0efd832":"# Let's do some feature engineering","ddbfbc9f":"# Optimise various classifiers","6e2f5111":"One little caveat: looking through the OHE, `Soil_Type 7, 15`, are present in the test, but not in the training data","a6b11f86":"The head of the training dataset","740aa086":"Parameters to be used in optimisation for various models","e11b0be8":"Read in data","23426d27":"We subtract 1 to have the labels starting with 0, which is required for LightGBM","2dc4026f":"Helper function to transfer One-Hot Encoding (OHE) into a Label Encoding (LE). It was taken from https:\/\/www.kaggle.com\/mlisovyi\/lighgbm-hyperoptimisation-with-f1-macro\n\nThe reason to convert OHE into LE is that we plan to use a tree-based model and such models are dealing well with simple interger-label encoding. Note, that this way we introduce an ordering between categories, which is not there in reality, but in practice in most use cases GBMs handle it well anyway.","80225093":"# Hyper parameters\nThe goal here is to demonstrate how to optimise hyper-parameters of various models\n\nThe kernel is a short version of https:\/\/www.kaggle.com\/mlisovyi\/featureengineering-basic-model"}}