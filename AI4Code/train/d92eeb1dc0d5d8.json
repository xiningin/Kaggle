{"cell_type":{"26460516":"code","b4534d1f":"code","bdf3ff49":"code","564f6d20":"code","e63a77a5":"code","765ad71b":"code","7882769f":"code","b5794150":"code","c7e04839":"code","fe5dfd82":"code","e0dffff6":"code","072e0599":"code","6857a4c2":"code","cd199387":"code","af8e6763":"code","39b9ac1d":"code","d41d5e2b":"markdown","02f13c58":"markdown","a2320933":"markdown","bd73af12":"markdown","e702b2c5":"markdown","c7a6dfc3":"markdown","bab0bf82":"markdown","615302cb":"markdown","f6e22cb0":"markdown","0d4d19f5":"markdown","0dfebed9":"markdown","9cb91a77":"markdown","03f55bfd":"markdown","c8066b61":"markdown","9096c47d":"markdown","45b59ca9":"markdown","f5401e12":"markdown","25b02111":"markdown","3edadb6e":"markdown","62627164":"markdown","40f306dc":"markdown","4caac6d4":"markdown","193a6344":"markdown","d653fc9d":"markdown","60ba5e5d":"markdown","a7f2998e":"markdown","07b4a374":"markdown","3eaa2bbb":"markdown","f4f792b5":"markdown","d62a2891":"markdown","8b9a17bd":"markdown","362587e5":"markdown","5a5fb9ff":"markdown"},"source":{"26460516":"import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import rcParams\n\nrcParams[\"figure.figsize\"] = [12, 9]\nrcParams[\"xtick.labelsize\"] = 15\nrcParams[\"ytick.labelsize\"] = 15\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples=1000, n_features=10)","b4534d1f":"import lightgbm as lgbm  # standard alias\n\nclf = lgbm.LGBMClassifier(objective=\"binary\")  # or 'mutliclass'\nreg = lgbm.LGBMRegressor()  # default - 'regression'","bdf3ff49":"reg.fit(X, y)","564f6d20":"tps_march = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/train.csv\")\ntps_march.head()","e63a77a5":"tps_march.shape","765ad71b":"tps_march.dtypes.value_counts()","7882769f":"X, y = tps_march.drop(\"target\", axis=1), tps_march[[\"target\"]].values.flatten()\n\n# Encode categoricals\nX_enc = pd.get_dummies(X)\n\nclf = lgbm.LGBMClassifier(objective=\"binary\", n_estimators=1000, random_state=1121218)\nclf.fit(X_enc, y)","b5794150":"clf = lgbm.LGBMClassifier(\n    objective=\"binary\", n_estimators=3000, learning_rate=0.1, random_state=1121218\n)\nclf.fit(X_enc, y)","c7e04839":"import xgboost as xgb\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\n\nX, y = tps_march.drop(\"target\", axis=1), tps_march[[\"target\"]].values.flatten()\n\n# Encode categoricals\nX_enc = pd.get_dummies(X)\nX_train, X_eval, y_train, y_eval = train_test_split(X_enc, y, test_size=0.2, stratify=y)","fe5dfd82":"%%time\n\nxgb_clf = xgb.XGBClassifier(\n    objective=\"binary:logistic\",\n    random_state=1121218,\n    n_estimators=10000,\n    tree_method=\"hist\",  # enable histogram binning in XGB\n)\n\nxgb_clf.fit(\n    X_train,\n    y_train,\n    eval_set=[(X_eval, y_eval)],\n    eval_metric=\"logloss\",\n    early_stopping_rounds=150,\n    verbose=False,  # Disable logs\n)\n\npreds = xgb_clf.predict_proba(X_eval)\nprint(f\"XGBoost logloss on the evaluation set: {log_loss(y_eval, preds):.5f}\")","e0dffff6":"%%time\n\nlgbm_clf = lgbm.LGBMClassifier(\n    objective=\"binary\",\n    random_state=1121218,\n    n_estimators=10000,\n    boosting=\"gbdt\",  # default histogram binning of LGBM\n    #     device='gpu'  # uncomment to use GPU training\n)\n\nlgbm_clf.fit(\n    X_train,\n    y_train,\n    eval_set=[(X_eval, y_eval)],\n    eval_metric=\"binary_logloss\",\n    early_stopping_rounds=150,\n    verbose=False,  # Disable logs\n)\n\npreds = lgbm_clf.predict_proba(X_eval)\nprint(f\"LightGBM logloss on the evaluation set: {log_loss(y_eval, preds):.5f}\")","072e0599":"X, y = tps_march.drop(\"target\", axis=1), tps_march[[\"target\"]].values.flatten()\n\n# Extract categoricals and their indices\ncat_features = X.select_dtypes(exclude=np.number).columns.to_list()\ncat_idx = [X.columns.get_loc(col) for col in cat_features]\n\n# Convert cat_features to pd.Categorical dtype\nfor col in cat_features:\n    X[col] = pd.Categorical(X[col])\n\n# Unencoded train\/test sets\nX_train, X_eval, y_train, y_eval = train_test_split(\n    X, y, test_size=0.2, random_state=4, stratify=y\n)","6857a4c2":"%%time\n\n# Model initialization is the same\neval_set = [(X_eval, y_eval)]\n\n_ = lgbm_clf.fit(\n    X_train,\n    y_train,\n    categorical_feature=cat_idx,  # Specify the categoricals\n    eval_set=eval_set,\n    early_stopping_rounds=150,\n    eval_metric=\"logloss\",\n    verbose=False,\n)\n\npreds = lgbm_clf.predict_proba(X_eval)\nloss = log_loss(y_eval, preds)\nprint(f\"LGBM logloss with default cateogircal feature handling: {loss:.5f}\")","cd199387":"pd.DataFrame(\n    {\"score\": [0.35482, 0.35256, 0.34823], \"runtime\": [\"1min 22s\", \"17.2 s\", \"15 s\"]},\n    index=[\n        \"XGB (one-hot)\",\n        \"LGBM (one-hot)\",\n        \"LGBM (cat-handling)\",\n    ],\n).T","af8e6763":"import time\n\nfrom sklearn.model_selection import StratifiedKFold\n\nN_SPLITS = 7\nstrat_kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=1121218)\n\nscores = np.empty(N_SPLITS)\nfor idx, (train_idx, test_idx) in enumerate(strat_kf.split(X, y)):\n    print(\"=\" * 12 + f\"Training fold {idx}\" + 12 * \"=\")\n    start = time.time()\n\n    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_val = y[train_idx], y[test_idx]\n    eval_set = [(X_val, y_val)]\n\n    lgbm_clf = lgbm.LGBMClassifier(n_estimators=10000)\n    lgbm_clf.fit(\n        X_train,\n        y_train,\n        eval_set=eval_set,\n        categorical_feature=cat_idx,\n        early_stopping_rounds=200,\n        eval_metric=\"binary_logloss\",\n        verbose=False,\n    )\n\n    preds = lgbm_clf.predict_proba(X_val)\n    loss = log_loss(y_val, preds)\n    scores[idx] = loss\n    runtime = time.time() - start\n    print(f\"Fold {idx} finished with score: {loss:.5f} in {runtime:.2f} seconds.\\n\")","39b9ac1d":"print(f\"Average log loss: {np.mean(scores):.4f}\")","d41d5e2b":"Early stopping is only enabled when you pass a set of evaluation sets to `eval_set` parameter of the `fit` method. These evaluation sets are used to keep track of the quality of the predictions from one boosting round to the next:","02f13c58":"# Model initialization and objectives","a2320933":"# Cross-validation with LightGBM","bd73af12":"# Eval sets and metrics","e702b2c5":"Like XGBoost, LGBM has two APIs \u2014 core learning API and Sklearn-compatible one. You know I am a big fan of Sklearn, so this tutorial will focus on that version.\n\n> Sklearn-compatible API of XGBoost and LGBM allows you to integrate their models in the Sklearn ecosystem so that you can use them inside pipelines in combination with other transformers.\n\nSklearn API exposes `LGBMRegressor` and `LGBMClassifier`, with the familiar `fit\/predict\/predict_proba` pattern:","c7a6dfc3":"# Controlling the number of decision trees","bab0bf82":"> You can achieve up to 8x speed up if you use `pandas.Categorical` data type when using LGBM.","615302cb":"Adding more trees leads to more accuracy but increases the risk of overfitting. To combat this, you can create many trees (+2000) and choose a smaller `learning_rate` (more on this later).","f6e22cb0":"```python\nfrom sklearn.model_selection import tXGBClassifiertest_split\n\nX_train, X_eval, y_train, y_eval = train_test_split(X_enc, y, test_size=0.1)\n\nclf = lgbm.LGBMClassifier(objective=\"binary\", n_estimators=10000)\neval_set = [(X_eval, y_eval)]\n\nclf.fit(\n    X_train,\n    y_train,\n    eval_set=eval_set,\n    early_stopping_rounds=100,\n    eval_metric=\"binary_logloss\",\n)\n\n----------------------------------------------------------\n\nTraining until validation scores don't improve for 100 rounds\n[1]\tvalid_0's binary_logloss: 0.542547\n[2]\tvalid_0's binary_logloss: 0.515902\n[3]\tvalid_0's binary_logloss: 0.494678\n[4]\tvalid_0's binary_logloss: 0.477235\n[5]\tvalid_0's binary_logloss: 0.462826\n...\n[737]\tvalid_0's binary_logloss: 0.350179\n[738]\tvalid_0's binary_logloss: 0.350162\nEarly stopping, best iteration is:\n[638]\tvalid_0's binary_logloss: 0.350073\n```\n","0d4d19f5":"To specify the categorical features, pass a list of their indices to `categorical_feature` parameter in the `fit` method:","0dfebed9":"# How to Beat the Heck Out of XGBoost with LightGBM: Comprehensive Tutorial\n## Not anymore, XGBoost, not anymore\n![](https:\/\/miro.medium.com\/max\/2000\/1*cpq9uZdaapAKu2GmckskuA.jpeg)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Photo by \n        <a href='https:\/\/unsplash.com\/@grstocks?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>GR Stocks<\/a>\n        on \n        <a href='https:\/\/unsplash.com\/s\/photos\/win?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Unsplash.<\/a> All images are by the author unless specified otherwise.\n    <\/strong>\n<\/figcaption>","9cb91a77":"# Early stopping","03f55bfd":"The number of decision trees inside the ensemble significantly affects the results. You can control it using the `n_estimators` parameter in both the classifier and regressor. Below, we will fit an LGBM binary classifier on the Kaggle TPS March dataset with 1000 decision trees:","c8066b61":"# XGBoost vs. LightGBM","9096c47d":"In each round of `n_estimators`, a single decision tree is fit to (`X_train`, `y_train`) and predictions are made on the passed evaluation set (`X_eval`, `y_eval`). The quality of predictions is measured with a passed metric in `eval_metric`.\n\nThe training stops at the 738th iteration because the validation score has not improved since the 638th one \u2014 early stopping of 100 rounds. Now, we have the luxury of creating as many trees as we want and `early_stopping_rounds` can discard the unnecessary ones.","45b59ca9":"Each tree in the ensemble builds on the predictions of the last tree \u2014 i.e., each boosting round is an improvement of the last.\n\nIf the predictions don\u2019t improve after a sequence of rounds, it is sensible to stop the training of the ensemble even if we are not at a hard stop for `n_estimators`.\n\nTo achieve this, LGBM provides `early_stopping_rounds` parameter inside the `fit` function. For example, setting it to 100 means we stop the training if the predictions have not improved for the last 100 rounds.\n\nBefore looking at a code example, we should learn a couple of concepts connected to early stopping.","f5401e12":"# Establish a baseline","25b02111":"Histogram binning in LGBM comes with built-in support for handling missing values and categorical features. TPS March dataset contains 19 categoricals, and we have been using one-hot encoding up to this point.\n\nThis time, we will let LGBM deal with categoricals and compare the results with XGBoost once again:","3edadb6e":"# You might also be interested...\n- [Tired of Clich\u00e9 Datasets? Here are 18 Awesome Alternatives From All Domains](https:\/\/towardsdatascience.com\/tired-of-clich%C3%A9-datasets-here-are-18-awesome-alternatives-from-all-domains-196913161ec9)\n- [Love 3Blue1Brown Animations? Learn How to Create Your Own in Python in 10 Minutes](https:\/\/towardsdatascience.com\/love-3blue1brown-animations-learn-how-to-create-your-own-in-python-in-10-minutes-8e0430cf3a6d)\n- [7 Cool Python Packages Kagglers Are Using Without Telling You](https:\/\/towardsdatascience.com\/7-cool-python-packages-kagglers-are-using-without-telling-you-e83298781cf4)\n- [25 Pandas Functions You Didn\u2019t Know Existed | P(Guarantee) = 0.8](https:\/\/towardsdatascience.com\/25-pandas-functions-you-didnt-know-existed-p-guarantee-0-8-1a05dcaad5d0)","62627164":"# Conclusion","40f306dc":"Like in XGBoost, fitting a single decision tree to the data is called a **boosting round**.","4caac6d4":"# Categorical and missing values support","193a6344":"The most common way of doing CV with LGBM is to use Sklearn CV splitters.\n\nI am not talking about utility functions like `cross_validate` or `cross_val_score` but splitters like `KFold` or `StratifiedKFold` with their `split` method. Doing CV in this way gives you more control over the whole process.\n\n> I have talked many times about the importance of cross-validation. You can read [this post](https:\/\/towardsdatascience.com\/6-sklearn-mistakes-that-silently-tell-you-are-a-rookie-84fa55f2b9dd?source=your_stories_page-------------------------------------) for more details.\n\nAlso, it enables you to use early stopping during cross-validation in a hassle-free manner. Here is what this looks like for the TPS March data:","d653fc9d":"Let\u2019s establish a baseline score with what we know so far. We will do the same for XGBoost so that we can compare the results:","60ba5e5d":"I am confused.\n\nSo many people are drawn to XGBoost like a moth to a flame. Yes, it has seen some glorious days in prestigious competitions, and it\u2019s still the most widely-used ML library.\n\nBut, it has been 4 years since XGBoost lost its top spot in terms of performance. In 2017, Microsoft open-sourced LightGBM (Light Gradient Boosting Machine) that gives equally high accuracy with 2\u201310 times less training speed.\n\nThis is a game-changing advantage considering the ubiquity of massive, million-row datasets. There are other distinctions that tip the scales towards LightGBM and give it an edge over XGBoost.\n\nBy the end of this post, you will learn about these advantages, including:\n- how to develop LightGBM models for classification and regression tasks\n- structural differences between XGBoost and LGBM\n- how to use early stopping and evaluation sets\n- enabling powerful categorical feature support for up 8x times speed increase\n- implementing successful cross-validation with LGBM\n- hyperparameter tuning with Optuna (Part II)","a7f2998e":"`objective` specifies the type of learning task. Besides the common ones like `binary`, `multiclass` and `regression` tasks, there are others like `poisson`, `tweedie` regressions. See [this section](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html#core-parameters) of the documentation for the full list of objectives.","07b4a374":"# Setup","3eaa2bbb":"# Introduction","f4f792b5":"LGBM achieved a smaller loss in ~4 times less runtime. Let\u2019s see a final LGBM trick before we move on to cross-validation.","d62a2891":"The table shows the final scores and runtimes of both models. As you can see, the version with default categorical handling beat others both in accuracy and speed. Cheers!","8b9a17bd":"When LGBM got released, it came with ground-breaking changes to the way it grows decision trees.\n\n> Both XGBoost and LightGBM are ensebmle algorithms. They use a special type of decision trees, also called weak learners, to capture complex, non-linear patterns.\n\nIn XGBoost (and many other libraries), decision trees were built one level at a time:\n\n![](https:\/\/lightgbm.readthedocs.io\/en\/latest\/_images\/level-wise.png)\n\nThis type of structure tends to result in unnecessary nodes and leaves because the trees continued to build until the `max_depth` reached. This led to higher model complexity and training cost runtime.\n\nIn contrast, LightGBM takes a leaf-wise approach:\n\n![](https:\/\/lightgbm.readthedocs.io\/en\/latest\/_images\/leaf-wise.png)\n\nThe structure continues to grow with the most promising branches and leaves (nodes with the most delta loss), holding the number of the decision leaves constant. (If this doesn\u2019t make sense to you, don\u2019t sweat. This won\u2019t prevent you from effectively using LGBM).\n\nThis is one of the main reasons LGBM crushed XGBoost in terms of speed when it first came out.\n\n![image.png](attachment:31471688-061b-493c-84d5-aca91e5228f6.png)\n\nAbove is a benchmark comparison of XGBoost with traditional decision trees and LGBM with leaf-wise structure (first and last columns) on datasets with ~500k-13M samples. It shows that LGBM is orders of magnitude faster than XGB.\n\nLGBM also uses h[istogram binning](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Features.html#optimization-in-speed-and-memory-usage) of continuous features, which provides even more speed-up than traditional gradient boosting. Binning numeric values significantly decrease the number of split points to consider in decision trees, and they remove the need to use sorting algorithms, which are always computation-heavy.\n\nInspired by LGBM, XGBoost also introduced histogram-binning, which gave massive speed-up but still not enough to match LGBM\u2019s:\n\n![image.png](attachment:472c7f66-048b-4fd3-84e4-ad65419bdfa3.png)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Histogram-binning comparison - second and third columns.\n    <\/strong>\n<\/figcaption>\n\nWe will continue exploring the differences in the coming sections.","362587e5":"In this post, we learned pure modeling techniques with LightGBM. Next up, we will explore how to squeeze every bit of performance out of LGBM models using Optuna.\n\nSpecifically, Part II of this article will include a detailed overview of the most important LGBM hyperparameters and introduce a well-tested hyperparameter tuning workflow. Stay tuned!","5a5fb9ff":"First, create a CV splitter \u2014 we are choosing `StratifiedKFold` because it is a classification problem. Then, loop through each train\/test sets using `split`. In each fold, initialize and train a new LGBM model and optionally report the score and runtime. That's it! That's how most people do CV, including on Kaggle."}}