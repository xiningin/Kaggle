{"cell_type":{"8b5a0e48":"code","690ca31d":"code","421bdd34":"code","1b87aeb4":"code","380a0d6d":"code","9ce73e8d":"code","f5c58c40":"code","2e1bcc7d":"code","7793f9fb":"code","296ae50e":"code","4fa8221b":"code","3a5592d4":"code","6feadc7d":"code","4af925b2":"code","7bcfc257":"code","56f6315e":"code","4ff6bf03":"code","d768f151":"code","cadf7811":"code","3bbd1855":"code","9092b4f0":"code","567f23a2":"code","2b7b8a7c":"code","88c08cf7":"code","33af6bc2":"code","7315ad20":"code","019e1031":"code","7cc7dbef":"code","b3cc28cc":"code","077c983f":"code","1a20fc92":"code","23ab8e4a":"code","0be90f1e":"code","804eddb7":"code","b58a43c8":"code","e04883be":"code","77d8b0ba":"code","a1967380":"code","f5be7530":"markdown","a50e6367":"markdown","e4660b8f":"markdown","ab7fca81":"markdown","31691dba":"markdown","30d5320f":"markdown","fb980617":"markdown","9b1e7459":"markdown"},"source":{"8b5a0e48":"#importing lib\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom datetime import date, timedelta\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold","690ca31d":"#reading data\ntrain= pd.read_csv('\/kaggle\/input\/womenintheloop-data-science-hackathon\/train.csv')\ntest= pd.read_csv('\/kaggle\/input\/womenintheloop-data-science-hackathon\/test_QkPvNLx.csv')\nsample= pd.read_csv('\/kaggle\/input\/womenintheloop-data-science-hackathon\/sample_submission_pn2DrMq.csv')\n\nprint(train.shape)\nprint(test.shape)\nprint(sample.shape)","421bdd34":"print(train.columns)\nprint(\"------------\")\nprint(test.columns)","1b87aeb4":"train.head()","380a0d6d":"train.dtypes","9ce73e8d":"numerical_feats = train.dtypes[train.dtypes != \"object\"].index\nprint(\"Number of Numerical features: \", len(numerical_feats))\n\ncategorical_feats = train.dtypes[train.dtypes == \"object\"].index\nprint(\"Number of Categorical features: \", len(categorical_feats))","f5c58c40":"print(train[numerical_feats].columns)\nprint(\"*\"*100)\nprint(train[categorical_feats].columns)","2e1bcc7d":"for col in numerical_feats:\n    print('{:15}'.format(col), \n          'Skewness: {:05.2f}'.format(train[col].skew()) , \n          '   ' ,\n          'Kurtosis: {:06.2f}'.format(train[col].kurt())  \n         )","7793f9fb":"#data prep: appending both train and test to preprocess together\ndf= train.append(test)\ndf.shape","296ae50e":"#null check and impute if any\ndf.isnull().sum()","4fa8221b":"#df['Competition_Metric'].fillna(df['Competition_Metric'].median(), inplace = True)","3a5592d4":"# feature engineering\ndf['CM_binned'] = pd.cut(df['Competition_Metric'],bins=5, labels=['very_low','low','medium','high','very_high'])\ndf.CM_binned","6feadc7d":"df.CM_binned.value_counts()","4af925b2":"# Converting Day_No to corresponding date and then date to day, month and year\n# Adapted from https:\/\/www.kaggle.com\/yacotaco\/learnx-sales-forecasting\n\ndef day_to_date(dataset):\n    start = date(2016,12,31)\n    dataset['Date'] = dataset['Day_No'].apply(lambda x: start + timedelta(x)) \n\ndef day_month_year(dataset): \n    dataset['Day'] = dataset['Date'].apply(lambda x: x.day)\n    dataset['Month'] = dataset['Date'].apply(lambda x: x.month)\n    dataset['Year'] = dataset['Date'].apply(lambda x: x.year)","7bcfc257":"day_to_date(df)\nday_month_year(df)\ndf.head()","56f6315e":"train.ID.nunique()","4ff6bf03":"df.Course_Domain.value_counts()","d768f151":"df.Course_Type.value_counts()","cadf7811":"df.dtypes","3bbd1855":"# Categorical col encoding\ncat_cols= ['Course_Domain','Course_Type','CM_binned']\ndf= pd.get_dummies(df, columns= cat_cols, drop_first=True)","9092b4f0":"df.head()","567f23a2":"plt.subplots(figsize=(16,16))\nsns.heatmap(df.corr(), annot=True, square= True)","2b7b8a7c":"df.columns","88c08cf7":"features= ['Competition_Metric', 'Course_ID', 'Long_Promotion',\n       'Public_Holiday','Short_Promotion',\n       'Day', 'Month', 'Year', 'Course_Domain_Development',\n       'Course_Domain_Finance & Accounting',\n       'Course_Domain_Software Marketing', 'Course_Type_Degree',\n       'Course_Type_Program', 'CM_binned_low', 'CM_binned_medium',\n       'CM_binned_high', 'CM_binned_very_high']","33af6bc2":"#splitting train and test from df\ntrain= df[df['Sales'].isnull()!= True]\ntest= df[df['Sales'].isnull()== True].drop(['Sales'], axis=1)\nprint(train.shape)\nprint(test.shape)","7315ad20":"print('len of features= ',len(features))","019e1031":"from sklearn.ensemble import RandomForestRegressor\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor","7cc7dbef":"#custom metric for hackathon\ndef rmsle1000(y_true, y_pred):\n    return np.sqrt(np.mean(np.power(np.log1p(y_true + 1) - np.log1p(y_pred + 1), 2))) *1000","b3cc28cc":"#xgb\nkf = StratifiedKFold(n_splits=5,shuffle=True,random_state=123)\n\nX= train[features]\ny= train.Sales\ncv_score =[]\ni=1\nfor train_index,test_index in kf.split(X, y):\n    print('Fold no. = ', i)\n    \n    x_train, x_test = X.loc[train_index], X.loc[test_index]\n    y_train, y_test = y.loc[train_index], y.loc[test_index]\n    \n    #model\n    xgb = XGBRegressor(n_estimators= 500)\n    xgb.fit(x_train, y_train)\n    y_pred= xgb.predict(x_test)\n    score = rmsle1000(y_test, y_pred)\n    print('RMSLE score:',score)\n    cv_score.append(score)    \n    \n    i+=1","077c983f":"#xgb mean rmsle\nnp.mean(cv_score)","1a20fc92":"# xgb feature importance\nfeat_importances = pd.Series(xgb.feature_importances_, index=features)\nfeat_importances.plot(kind='barh')","23ab8e4a":"#lgbm\ncv_score =[]\ni=1\nfor train_index,test_index in kf.split(X, y):\n    print('Fold no. = ', i)\n    \n    x_train, x_test = X.loc[train_index], X.loc[test_index]\n    y_train, y_test = y.loc[train_index], y.loc[test_index]\n    \n    #model\n    lgbm = LGBMRegressor(n_estimators= 500 )\n    lgbm.fit(x_train, y_train)\n    y_pred= lgbm.predict(x_test)\n    score = rmsle1000(y_test, y_pred)\n    print('RMSLE score:',score)\n    cv_score.append(score)    \n    \n    i+=1","0be90f1e":"#lgbm mean rmsle\nnp.mean(cv_score)","804eddb7":"# lgbm feature importances\nfeat_importances = pd.Series(lgbm.feature_importances_, index=features)\nfeat_importances.plot(kind='barh')","b58a43c8":"# for submission\nxgb = XGBRegressor(n_estimators= 500)\nxgb.fit(train[features], train.Sales)\nxgb_preds = xgb.predict(test[features])","e04883be":"# for submission\nlgbm = LGBMRegressor(n_estimators= 500)\nlgbm.fit(train[features], train.Sales)\nlgbm_preds = lgbm.predict(test[features])","77d8b0ba":"submission = pd.DataFrame()\nsubmission['ID'] = test['ID']\nsubmission['Sales'] = xgb_preds\nsubmission.to_csv('xgb.csv', index=False)\nsubmission.head()","a1967380":"submission2 = pd.DataFrame()\nsubmission2['ID'] = test['ID']\nsubmission2['Sales'] = lgbm_preds\nsubmission2.to_csv('lgbm.csv', index=False)\nsubmission2.head()","f5be7530":"Two categorical variables- Course_Domain and Course_Type","a50e6367":"#### Data Preprocessing Steps ","e4660b8f":"* ID is unique for each entry, Can drop it safely.","ab7fca81":"* Competition metric have some null values.\n* Not going to impute coz tree based models (xgb) can handle null values on its own. ","31691dba":"#### Modelling","30d5320f":"* Problem Name: LearnX Sales Forcasting\n* Problem type: Regression\n* Evaluation Metric: (RMSLE *1000)\n\n\n* Models Used: XGB and LGBM Regressor ( without hypertuning )\n* Validation Technique Used: StratifiedKFold","fb980617":"#### Data Exploration","9b1e7459":"#### A Basic and Beginner friendly Kernel demonstrating use of XGB and LGBM Regressor for solving a Regression Problem. It includes data exploration, preprocessing steps, basic feature engineering and modelling steps with Stratified K fold validation technique."}}