{"cell_type":{"689b27b3":"code","b61a485e":"code","3c085b65":"code","01c1b297":"code","9cf3b193":"code","d4c9e512":"code","8e806454":"code","d55d90d2":"code","4e8d8098":"code","da44f308":"code","d6185a03":"markdown","c99f0e84":"markdown","ba4dc93e":"markdown","a7f562d9":"markdown","1d9dd72b":"markdown","d01783b7":"markdown"},"source":{"689b27b3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier","b61a485e":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","3c085b65":"## creates a dictionary with (# of tweet, # position of word in alphabetically ordered list over whole set) # occurence of word in tweet\ncount_vectorizer = feature_extraction.text.CountVectorizer()\nTfidf = feature_extraction.text.TfidfVectorizer()\n\n## let's get counts for the first 5 tweets in the data\nexample_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])","01c1b297":"## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)(sparse - mostly zeros, dense matrix - non zeroes)\nprint(example_train_vectors.todense().shape, \"<-- in the first 5 tweets there are 54 unique words (tokens)\")\nprint(example_train_vectors[0].todense(),\"<-- representation of the 13 words in the first tweet in the list of 54 tokens\")","9cf3b193":"#train_vectors = Tfidf.fit_transform(train_df[\"text\"])\n#test_vectors = Tfidf.transform(test_df[\"text\"])\n\ntrain_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])\n\n## note that we're NOT using .fit_transform() for the test. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.","d4c9e512":"## Because the vectors are big, we risk running into a high variance scenario. We introduce ridge regression to reduce this. \n# Ridge regression penalizes big weights CostRR = Cost + (alpha * weights^2) and can therefore reduce the chance of overfitting the training data\nclf = linear_model.RidgeClassifier(alpha = 5.5)\n#clf = SGDClassifier(loss=\"log\", max_iter=500, alpha = 0.001)","8e806454":"scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\nprint(sum(scores\/3))\n","d55d90d2":"clf.fit(train_vectors, train_df[\"target\"])","4e8d8098":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission[\"target\"] = clf.predict(test_vectors)\nsample_submission.head()","da44f308":"sample_submission.to_csv(\"submission.csv\", index=False)","d6185a03":"Classifiers\n- [X] SGD (Count Vectorizer) - 0.79243\n- [O] SGD (Tfidf) - Poor cv performance\n- [X] Ridge classifier (Count Vectorizer) - 0.77096\n- [X] Ridge classifier (Tfidf) -","c99f0e84":"- [ ] Choose Alpha for ridge regression\n- [ ] Other clf possible?","ba4dc93e":"##Select alpha for Ridge\ni = 1\nwhile i <= 10:\n    clf = linear_model.RidgeClassifier(alpha = i)\n    scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n    print(sum(scores)\/3, 'i = ', i)\n    i = i+0.5","a7f562d9":"##Select alpha for SGD\ni = 0.0001\nwhile i <= 1:\n    clf = SGDClassifier(loss=\"log\", max_iter=500, alpha = 0.001)\n    scoresSGD = model_selection.cross_val_score(clfSGD, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n    print(\"SGD\")\n    print(sum(scoresSGD)\/3, 'i = ', i)\n    i = i*2","1d9dd72b":"- [ ] atm only text is used. Find use for keyword and location.\n\nprocessing:\n- [X]count vectorizer\n- [ ]TFIDF (term frequency-inverse document frequency)\n- [ ]LSA (Latent semantic analysis)\n- [ ]LSTM (Long short-term memory)\n- [ ]RNN (Recurrent neural network)","d01783b7":"**Version 1 is a copy and paste of the NLP tutorial that can be found [here](https:\/\/www.kaggle.com\/philculliton\/nlp-getting-started-tutorial)**\nThis serves as my base for further improvement."}}