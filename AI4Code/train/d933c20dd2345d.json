{"cell_type":{"d0d19345":"code","f28bc286":"code","6206fde4":"code","6d055dc7":"code","5a25b651":"code","520749bd":"code","002a73cc":"code","92f62b7b":"code","79a90862":"code","b7de928d":"code","8ae96a30":"code","b989540b":"code","2d792430":"code","db2d658c":"code","118d087a":"code","f050418e":"code","69d0572f":"code","18219ae5":"code","5ef4ef79":"code","38b62b64":"code","270d3ed3":"markdown","6c56bd21":"markdown","0d99c956":"markdown","cef62695":"markdown","4ceaec54":"markdown","17761028":"markdown","a24012ce":"markdown","8d1efcc3":"markdown","f87f452a":"markdown"},"source":{"d0d19345":"# !pip install PySastrawi &> \/dev\/null","f28bc286":"import numpy as np\nimport pandas as pd \nimport os\nimport re\n# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nimport string\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport nltk\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nfrom sklearn.model_selection import train_test_split","6206fde4":"data = pd.read_csv('..\/input\/indonesian-abusive-and-hate-speech-twitter-text\/data.csv', encoding='latin-1')\nalay_dict = pd.read_csv('..\/input\/indonesian-abusive-and-hate-speech-twitter-text\/new_kamusalay.csv', names = ['original', 'replacement'], encoding='latin-1')\nabusive_dict = pd.read_csv('..\/input\/indonesian-abusive-and-hate-speech-twitter-text\/abusive.csv', encoding='latin-1')\nstopword_dict = pd.read_csv('..\/input\/indonesian-stoplist\/stopwordbahasa.csv', names = ['stopword'], encoding='latin-1')","6d055dc7":"data","5a25b651":"# factory = StemmerFactory()\n# stemmer = factory.create_stemmer()\n\ndef lowercase(text):\n    return text.lower()\n\ndef remove_unnecessary_char(text):\n    text = re.sub('\\\\+n', ' ', text)\n    text = re.sub('\\n',\" \",text) # Remove every '\\n'\n    \n    text = re.sub('rt',' ',text) # Remove every retweet symbol\n    text = re.sub('RT',' ',text) # Remove every retweet symbol\n    text = re.sub('user',' ',text) # Remove every username\n    text = re.sub('USER', ' ', text)\n    text = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+)|(http?:\/\/[^\\s]+))',' ',text) # Remove every URL\n    text = re.sub(':', ' ', text)\n    text = re.sub(';', ' ', text)\n    text = re.sub('\\\\+n', ' ', text)\n    text = re.sub('\\n',\" \",text) # Remove every '\\n'\n    text = re.sub('\\\\+', ' ', text)\n    text = re.sub('  +', ' ', text) # Remove extra spaces\n    return text\n    \ndef remove_nonaplhanumeric(text):\n    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n    return text\n\nalay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\ndef normalize_alay(text):\n    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n\ndef remove_stopword(text):\n    text = ' '.join(['' if word in stopword_dict.stopword.values else word for word in text.split(' ')])\n    text = re.sub('  +', ' ', text) # Remove extra spaces\n    text = text.strip()\n    return text\n\n# def stemming(text):\n#     return stemmer.stem(text)\n\ndef remove_emoticon_byte(text):\n    text = text.replace(\"\\\\\", \" \")\n    text = re.sub('x..', ' ', text)\n    text = re.sub(' n ', ' ', text)\n    text = re.sub('\\\\+', ' ', text)\n    text = re.sub('  +', ' ', text)\n    return text\n\ndef remove_early_space(text):\n    if text[0] == ' ':\n        return text[1:]\n    else:\n        return text\n\n# print(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"Halooo,,,,, duniaa!!\"))\n# print(\"lowercase: \", lowercase(\"Halooo, duniaa!\"))\n# print(\"stemming: \", stemming(\"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan\"))\n# print(\"remove_unnecessary_char: \", remove_unnecessary_char(\"Hehe\\n\\n RT USER USER apa kabs www.google.com\\n  hehe\"))\n# print(\"normalize_alay: \", normalize_alay(\"aamiin adek abis\"))\n# print(\"remove_stopword: \", remove_stopword(\"ada hehe adalah huhu yang hehe\"))\n\n","520749bd":"def preprocess(text):\n#     text = lowercase(text) # 1\n#     text = remove_nonaplhanumeric(text) # 2\n    text = remove_unnecessary_char(text) # 2\n    text = normalize_alay(text) # 3\n    text = remove_unnecessary_char(text)\n#     text = stemming(text) # 4\n#     text = remove_stopword(text) # 5\n    text = remove_emoticon_byte(text)\n    text = remove_early_space(text)\n    return text\n\ndef classify(hs):\n    retval = \"\"\n    if int(hs) == 1:\n        retval = 'positive'\n    else:\n        retval = 'negative'\n    return retval","002a73cc":"data['text'] = data['Tweet'].apply(preprocess)\ndata['hs_class'] = data['HS'].apply(classify)\ndata[['text', 'hs_class']].sample(10)","92f62b7b":"train = data[['hs_class', 'text']]\nbase_train = train\ntrain.to_csv('train_preprocessed.csv', index = False)\ntrain.sample(5)","79a90862":"temp = train.groupby('hs_class').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","b7de928d":"# balancing datasize\ntrain = data[['hs_class', 'text']]\ntrain_pos = train[train['hs_class']=='positive']\ntrain_neg = train[train['hs_class']=='negative']\ntrain_neg, removed = train_test_split(train_neg, test_size=(1.9\/7), shuffle=True)\ntrain = pd.concat([train_pos, train_neg])\ntemp = train.groupby('hs_class').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","8ae96a30":"plt.figure(figsize=(12,6))\nsns.countplot(x='hs_class',data=train)","b989540b":"fig = go.Figure(go.Funnelarea(\n    text =temp['hs_class'],\n    values = temp.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n    ))\nfig.show()","2d792430":"train['temp_list'] = train['text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","db2d658c":"positive_text = train[train['hs_class']=='positive']\nnegative_text = train[train['hs_class']=='negative']","118d087a":"top = Counter([item for sublist in positive_text['temp_list'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","f050418e":"top = Counter([item for sublist in negative_text['temp_list'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(20))\ntemp_negative = temp_negative.iloc[1:,:]\ntemp_negative.columns = ['Common_words','count']\ntemp_negative.style.background_gradient(cmap='Reds')","69d0572f":"def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(14.0,8.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'u', \"im\"}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=400, \n                    height=200,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout() ","18219ae5":"pos_mask = np.array(Image.open('..\/input\/masksforwordclouds\/twitter_mask.png'))\nplot_wordcloud(positive_text.text,color='white',max_font_size=100,title_size=30,title=\"WordCloud of Neutral Tweets\", mask=pos_mask)","5ef4ef79":"pos_mask = np.array(Image.open('..\/input\/masksforwordclouds\/twitter_mask.png'))\nplot_wordcloud(negative_text.text,color='white',max_font_size=100,title_size=30,title=\"WordCloud of Neutral Tweets\", mask=pos_mask)","38b62b64":"exp_data = train[['hs_class', 'text']].dropna()\nexp_data.to_csv('data_preprocessed.csv', index = False)\n\ndata_train_pos = exp_data[exp_data['hs_class']=='positive']\ndata_train_neg = exp_data[exp_data['hs_class']=='negative']\n\ntrain_test_pos, validate_pos = train_test_split(data_train_pos, test_size=.2)\ntrain_pos, test_pos = train_test_split(train_test_pos, test_size=.125)\n\ntrain_test_neg, validate_neg = train_test_split(data_train_neg, test_size=.2)\ntrain_neg, test_neg = train_test_split(train_test_neg, test_size=.125)\n\ntrain_exp = pd.concat([train_pos, train_neg])\nvalidate_exp = pd.concat([validate_pos, validate_neg])\ntest_exp = pd.concat([test_pos, test_neg])\n\ntrain_exp.to_csv('train_split.csv', index=False)\nvalidate_exp.to_csv('validate_split.csv', index=False)\ntest_exp.to_csv('test_split.csv', index=False)\n\n","270d3ed3":"# 3.0 Data Visualization and WordCloud for Better Data Understanding and Exploration","6c56bd21":"# 1.2 Importing Data","0d99c956":"# 1.0 Installing Required Library and Matching Required Library Version","cef62695":"# Indonesia Hatespeech Recognition\n* 2301859650 - Cornelius Tantius\n* 2301860154 - Jonathan Kristanto\n* 2301865741 - Edgard Jonathan Putra Pranoto","4ceaec54":"### Data Balancing","17761028":"# 5.0 Data Export\nExport to 70% Train, 20% Validate and 10% Test with some proper classed splitting","a24012ce":"# 4.0 Initial Modelling Sample\nModel moved to https:\/\/www.kaggle.com\/corneliustantius\/indo-hatespeech-classifier-model","8d1efcc3":"# 1.1 Importing Necessities","f87f452a":"# 2.0 Data Cleaning"}}