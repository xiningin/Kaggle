{"cell_type":{"d47dfb06":"code","8a4b69f2":"code","306c4ded":"code","72fa7292":"code","76210334":"code","b70f3e44":"code","3a3f54d9":"code","827dbcf7":"code","d4d1f317":"code","e37591f1":"code","dea545b4":"code","7e7515bb":"code","50e71f89":"code","182bc964":"code","4fb32430":"code","0ded8fda":"code","72b6edd9":"code","9b5064f4":"code","34b0875f":"code","12f78952":"code","a884ff76":"code","7b4af1e2":"code","84178c87":"markdown","6f68b8ec":"markdown","a310cd4f":"markdown","fd94f9e1":"markdown","0a15065c":"markdown","ccdfd23b":"markdown","5e079088":"markdown","57534e5d":"markdown","f7dc0b43":"markdown"},"source":{"d47dfb06":"import sys\nsys.path.append(\"..\/input\/timmmaster\/\")\n\n\n\nimport os\nimport math\nimport cv2\nimport timm\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch.optim as optim\nimport albumentations\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm\nfrom typing import List, Optional\nfrom dataclasses import dataclass\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils import data as torch_data\nfrom catalyst import dl\nfrom catalyst.data import ToTensor\nfrom catalyst.contrib.nn import BatchScheduler\nfrom catalyst.utils.torch import set_optimizer_momentum\nfrom sklearn.model_selection import StratifiedKFold, train_test_split","8a4b69f2":"def set_seed(seed: int):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n@dataclass\nclass CFG:\n    image_size: int = 256\n    vflip_p: float = 0.01\n    fold: int = 0\n    lr: float = 5e-4\n    reg_dropout: float = 0.1\n    batch_size: int = 64\n    reg_epochs: int = 5\n    seed: int = 2809\n        \ncfg = CFG()\nset_seed(cfg.seed)","306c4ded":"df = pd.read_csv(\"..\/input\/same-old-creating-folds\/train_10folds.csv\")\n\ndf_train = df[df.kfold != cfg.fold].reset_index(drop=True)\ndf_valid = df[df.kfold == cfg.fold].reset_index(drop=True)\n\ncharacteristics = [\n    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n]\n\ntrain_img_paths = [f\"..\/input\/petfinder-pawpularity-score\/train\/{x}.jpg\" for x in df_train[\"Id\"].values]\nvalid_img_paths = [f\"..\/input\/petfinder-pawpularity-score\/train\/{x}.jpg\" for x in df_valid[\"Id\"].values]","72fa7292":"def get_center(x):\n    hist, bins = np.histogram(x, bins=128)\n    a = hist.argmax()\n    return (bins[a + 1] + bins[a]) \/ 2","76210334":"fig = ff.create_distplot([df_train.Pawpularity.values, df_valid.Pawpularity.values], ['Train', 'Valid'])\nfig.update_layout(\n    title=\"Pawpularity Distribution\",\n    xaxis_title=\"Pawpularity\",\n    yaxis_title=\"Density\")\nCX = get_center(df.Pawpularity.values)\nLNX = CX\nRNX = 100 - CX\nfig.add_vline(get_center(df.Pawpularity.values), line_width=3, line_color=\"red\")\nfig.show()","b70f3e44":"def norm(x, center=CX, lscale=LNX, rscale=RNX):\n    x = x.copy().astype(np.float64)\n    x -= center\n    x[x <= 0] \/= lscale\n    x[x > 0] \/= rscale\n    return x\n\ndef inorm(x, center=CX, lscale=LNX, rscale=RNX):\n    x = x.copy().astype(np.float64)\n    x[x <= 0] *= lscale\n    x[x > 0] *= rscale\n    return x + center","3a3f54d9":"fig = ff.create_distplot([norm(df_train.Pawpularity.values), norm(df_valid.Pawpularity.values)], ['Train', 'Valid'], bin_size=0.01)\nfig.update_layout(\n    title=\"Pawpularity Distribution\",\n    xaxis_title=\"Pawpularity\",\n    yaxis_title=\"Density\")\nfig.show()","827dbcf7":"_c = [\n    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur', 'Pawpularity'\n]\nfig = fig = px.imshow(df[_c].corr())\nfig.update_layout(\n    title=\"Characteristics - Pawpularity Correlation\")\nfig.show()","d4d1f317":"widths = np.array([10 for _ in characteristics])\n\ndata = {\n    \"Positive\": df[characteristics].values.sum(0),\n    \"Negative\": len(df) -  df[characteristics].values.sum(0)\n}\n\nfig = go.Figure()\nfor key in data:\n    fig.add_trace(go.Bar(\n        name=key,\n        y=data[key],\n        x=np.cumsum(widths)-widths,\n        width=widths,\n        offset=0,\n        customdata=np.transpose([characteristics, data[key]]),\n        texttemplate=\"%{customdata[0]}<br>%{customdata[1]}\",\n        textposition=\"inside\",\n        textangle=0,\n        textfont_color=\"white\",\n        hovertemplate=\"<br>\".join([\n            \"characteristic: %{customdata[0]}\",\n            \"Total Num.: %{customdata[1]}\",\n        ])\n    ))\n\nfig.update_xaxes(\n    tickvals=np.cumsum(widths)-widths\/2,\n    ticktext= [\"%s\" % (l) for l in characteristics]\n)\n\nfig.update_xaxes(range=[0,sum(widths)])\nfig.update_yaxes(range=[0,len(df)])\n\nfig.update_layout(\n    title_text=\"Characteristic Pos.\/Neg. Disttribution\",\n    barmode=\"stack\",\n    uniformtext=dict(mode=\"hide\", minsize=10),\n)","e37591f1":"def visualize_img_characteristic(characteristic):\n    neg = [cv2.cvtColor(\n        cv2.imread(f\"..\/input\/petfinder-pawpularity-score\/train\/{idx}.jpg\"), \n        cv2.COLOR_BGR2RGB) for idx in df.loc[df[characteristic] == 0, 'Id'].sample(1)]\n    pos = [cv2.cvtColor(\n        cv2.imread(f\"..\/input\/petfinder-pawpularity-score\/train\/{idx}.jpg\"), \n        cv2.COLOR_BGR2RGB) for idx in df.loc[df[characteristic] == 1, 'Id'].sample(1)]\n\n    fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n\n    for p, n in zip(pos, neg):\n        axs[0].imshow(p)\n        axs[0].set_title('1')\n        axs[0].axis('off')\n    \n    \n        axs[1].imshow(n)\n        axs[1].set_title('0')\n        axs[1].axis('off')\n    fig.suptitle(characteristic)\n    plt.show()","dea545b4":"for c in characteristics:\n    visualize_img_characteristic(c)","7e7515bb":"class DataRetriever(torch_data.Dataset):\n    def __init__(self, image_paths: List[str], targets: np.ndarray, augmentations: albumentations.Compose):\n        self.image_paths = image_paths\n        self.targets = norm(targets)\n        self.augmentations = augmentations\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        image = cv2.imread(self.image_paths[index])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n            \n        image = torch.tensor(np.transpose(image, (2, 0, 1)).astype(np.float32), dtype=torch.float)\n        \n        y = torch.tensor(self.targets[index], dtype=torch.float)\n        return {\"features\": image, \"targets\": y}","50e71f89":"class RegModel(nn.Module):\n    def __init__(self, base_model: nn.Module, config: CFG):\n        super().__init__()\n        self.model = base_model\n        self.model.classifier = nn.Sequential(\n                    nn.Dropout(config.reg_dropout),\n                    nn.Linear(in_features=self.model.classifier.in_features, out_features=1, bias=False)\n        )\n    \n    def forward(self, x):\n        out = self.model(x)\n        return out[:, 0]","182bc964":"class CosineHardRestartWarmupBatchShedulerWrapper(BatchScheduler):\n    def __init__(self, optimizer: optim.Optimizer, \n                       num_warmup_steps: int, \n                       num_training_steps: int, \n                       num_cycles: int = 1,\n                       last_itter: int = -1,\n                       gamma: float = 0.9,\n                       verbose: bool = False):\n        self.__num_warmup_steps = num_warmup_steps\n        self.__num_training_steps = num_training_steps\n        self.__num_cycles = num_cycles\n        self.__gamma = gamma\n        self.__steps_per_epoch = (num_training_steps - num_warmup_steps) \/\/ (num_cycles)\n        self.total_groups = len(optimizer.param_groups)\n        super().__init__(optimizer, last_itter, verbose)\n        \n    def get_momentum(self):\n        return [] * self.total_groups\n    \n    \n    def get_lr(self):\n        return [lr * self._form_function(self._step_count) * self.__gamma ** (self._step_count \/\/ self.__steps_per_epoch) for lr in self.base_lrs]\n    \n    def _form_function(self, count):\n        if count < self.__num_warmup_steps:\n            return float(count) \/ float(max(1, self.__num_warmup_steps))\n        progress = float(count - self.__num_warmup_steps) \/ float(max(1, self.__num_training_steps - self.__num_warmup_steps))\n\n        if progress >= 1.0:\n            return 0\n        return max(0, 0.5 * (1.0 + math.cos(math.pi * ((float(self.__num_cycles) * progress) % 1.0))))","4fb32430":"train_aug = albumentations.Compose(\n    [\n        albumentations.Resize(cfg.image_size, cfg.image_size, p=1),\n        albumentations.augmentations.transforms.HorizontalFlip(p=cfg.vflip_p),\n        albumentations.Normalize(),\n    ],\n    p=1.0,\n)\n\nvalid_aug = albumentations.Compose(\n    [\n        albumentations.Resize(cfg.image_size, cfg.image_size, p=1),\n        albumentations.Normalize(),\n    ],\n    p=1.0,\n)","0ded8fda":"tr_dataset = DataRetriever(train_img_paths, df_train.loc[:, 'Pawpularity'].values, train_aug)\nvl_dataset = DataRetriever(valid_img_paths, df_valid.loc[:, 'Pawpularity'].values, valid_aug)\n\ntr_loader = DataLoader(tr_dataset, batch_size=cfg.batch_size, num_workers=8)\nvl_loader = DataLoader(vl_dataset, batch_size=cfg.batch_size, num_workers=8)\nloaders = {\"train\": tr_loader, \"valid\": vl_loader}","72b6edd9":"base_model = timm.create_model(\"tf_efficientnet_b0_ns\", pretrained=False, in_chans=3)\nbase_model.load_state_dict(torch.load('..\/input\/timms-effb0\/tf_efficientnet_b0_ns-c0e6a31c.pth'))\n\nreg_model = RegModel(base_model, cfg)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(reg_model.parameters(), lr=cfg.lr)\nscheduler = CosineHardRestartWarmupBatchShedulerWrapper(\n                                    optimizer, \n                                    num_warmup_steps=len(tr_loader), \n                                    num_training_steps=len(tr_loader) * cfg.reg_epochs,\n                                    num_cycles=cfg.reg_epochs - 1\n)\n\nmodel_parameters = filter(lambda p: p.requires_grad, reg_model.parameters())\nprint(\"Total N params\",sum([np.prod(p.size()) for p in model_parameters]))\n\nrunner = dl.SupervisedRunner()\n# model training\nrunner.train(\n    model=reg_model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    num_epochs=cfg.reg_epochs,\n    logdir=\".\/logs_reg\",\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=True,\n    verbose=True\n)","9b5064f4":"valid_metrics = pd.read_csv('.\/logs_reg\/logs\/valid.csv')\ntrain_metrics = pd.read_csv('.\/logs_reg\/logs\/train.csv')\n\nfig = px.line(\n    pd.DataFrame(\n        {\n        'Epoch': train_metrics.step,\n        'Valid RMSE': np.sqrt(valid_metrics.loss.values),\n        'Train RMSE': np.sqrt(train_metrics.loss.values)\n        }\n    ),\n    x='Epoch',\n    y=['Valid RMSE', 'Train RMSE'],\n    title='Train\/Valid RMSE'\n)\n\nfig.show()","34b0875f":"reg_model.load_state_dict(torch.load('logs_reg\/checkpoints\/best_full.pth')['model_state_dict'])","12f78952":"reg_model.eval()\nvalid_pred = []\nvalid_target = []\n\nwith torch.no_grad():\n    for x in tqdm(vl_loader):\n        valid_pred.extend(reg_model(x['features'].to('cuda')).cpu().numpy().tolist())\n        valid_target.extend(x['targets'].numpy().tolist())\n        \nfig = ff.create_distplot([inorm(np.array(valid_pred)), inorm(np.array(valid_target))], ['Predicted', 'Target'])\nfig.update_layout(\n    title=\"Pawpularity Distribution\",\n    xaxis_title=\"Pawpularity\",\n    yaxis_title=\"Density\")\nfig.show()","a884ff76":"reg_model.eval()\npred = []\ntarget = []\n\nwith torch.no_grad():\n    for x in tqdm(tr_loader):\n        pred.extend(reg_model(x['features'].to('cuda')).cpu().numpy().tolist())\n        target.extend(x['targets'].numpy().tolist())\n        \nfig = ff.create_distplot([inorm(np.array(pred)), inorm(np.array(target))], ['Predicted', 'Target'])\nfig.update_layout(\n    title=\"Pawpularity Distribution\",\n    xaxis_title=\"Pawpularity\",\n    yaxis_title=\"Density\")\nfig.show()","7b4af1e2":"df_test = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")\ntest_img_paths = [f\"..\/input\/petfinder-pawpularity-score\/test\/{x}.jpg\" for x in df_test[\"Id\"].values]\n\ntest_dataset = DataRetriever(\n    image_paths=test_img_paths,\n    targets=np.ones(len(test_img_paths)),\n    augmentations=valid_aug,\n)\n\nreg_model.eval()\nfinal_test_predictions = []\n\nwith torch.no_grad():\n    for x in tqdm(DataLoader(test_dataset, batch_size=cfg.batch_size, num_workers=8)):\n        final_test_predictions.extend(reg_model(x['features'].to('cuda')).cpu().numpy().tolist())\n\nfinal_test_predictions = inorm(np.array(final_test_predictions))\nfinal_test_predictions[final_test_predictions < 0] = 0\nfinal_test_predictions[final_test_predictions > 100] = 100\ndf_test[\"Pawpularity\"] = final_test_predictions\ndf_test = df_test[[\"Id\", \"Pawpularity\"]]\ndf_test.to_csv(\"submission.csv\", index=False)","84178c87":"<a id=\"1.3\"><\/a>\n### Image for each charecteristic","6f68b8ec":"<a id=\"4\"><\/a>\n## References\n* [same old creating folds](https:\/\/www.kaggle.com\/abhishek\/same-old-creating-folds)","a310cd4f":"<a id=\"3\"><\/a>\n## 3. Inference","fd94f9e1":"<img align=\"left\" src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25383\/logos\/header.png?t=2021-08-31-18-49-29\" data-canonical-src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25383\/logos\/header.png?t=2021-08-31-18-49-29\" width=\"1350\" \/>\n","0a15065c":"1. [EDA](#1)\n    * [Pawpularity](#1.1)\n    * [Additional Characteristics](#1.2)\n    * [Image for each charecteristic](#1.3)\n2. [Train](#2)\n3. [Inference](#3)\n4. [References](#4)","ccdfd23b":"<a id=\"1.2\"><\/a>\n### Additional Characteristics\n* Focus - Pet stands out against uncluttered background, not too close \/ far.\n* Eyes - Both eyes are facing front or near-front, with at least 1 eye \/ pupil decently clear.\n* Face - Decently clear face, facing front or near-front.\n* Near - Single pet taking up significant portion of photo (roughly over 50% of photo width or height).\n* Action - Pet in the middle of an action (e.g., jumping).\n* Accessory - Accompanying physical or digital accessory \/ prop (i.e. toy, digital sticker), excluding collar and leash.\n* Group - More than 1 pet in the photo.\n* Collage - Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos).\n* Human - Human in the photo.\n* Occlusion - Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are considered occlusion.\n* Info - Custom-added text or labels (i.e. pet name, description).\n* Blur - Noticeably out of focus or noisy, especially for the pet\u2019s eyes and face. For Blur entries, \u201cEyes\u201d column is always set to 0.\n","5e079088":"<a id=\"2\"><\/a>\n## 2. Train","57534e5d":"<a id=\"1.1\"><\/a>\n### Pawpularity\nThe Pawpularity Score is derived from each pet profile's page view statistics at the listing pages, using an algorithm that normalizes the traffic data across different pages, platforms (web & mobile) and various metrics.","f7dc0b43":"<a id=\"1\"><\/a>\n## 1. EDA"}}