{"cell_type":{"2dfdb391":"code","0a52bf89":"code","65e2774b":"code","7f182414":"code","5c053f4a":"code","370dc018":"code","8ab41738":"code","543768da":"code","4621184d":"code","2f81515c":"code","d0379763":"code","f1699d95":"code","47659180":"code","ad7d949d":"code","9327806d":"code","0f350fd9":"code","786598f4":"code","de9c7876":"code","17bc684b":"code","e63396b3":"code","4e700610":"code","c2d0f449":"code","3d9d25d3":"code","8f1259c7":"code","9e39cafd":"markdown","40a499f2":"markdown","f2b4faea":"markdown","ae44b805":"markdown","afcbf0dc":"markdown"},"source":{"2dfdb391":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn","0a52bf89":"df=pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf.head(3)","65e2774b":"#Checking and updating Null Values\ndf.isnull().sum()","7f182414":"#We can convert Age to Int\ndf.age=df.age.astype(int)\nprint(df.dtypes)\nprint('shape:',df.shape)\ndf.head(2)","5c053f4a":"sns.distplot(df.platelets)","370dc018":"#we can see that the death rate is high for people over 80\nplt.scatter(df.age,df.DEATH_EVENT)","8ab41738":"print('Death Rate for less than 50 y\/o:',int(df[df['age']<=50].DEATH_EVENT.sum()*100\/df[df['age']<=50].shape[0]),'%')\nprint('Death Rate for more than 80 y\/o:',int(df[df['age']>=80].DEATH_EVENT.sum()*100\/df[df['age']>=80].shape[0]),'%')\nprint('Death Rate for more than 50 and less than 80 y\/o:',int(df[(df['age']>50)&(df['age']<80)].DEATH_EVENT.sum()*100\/df[(df['age']>50)&(df['age']<80)].shape[0]),'%')","543768da":"# We can see that there is a significant difference between the death rates for people below and above the 80 y\/o mark\n#so we split them into two categories Old and Not Old\nbins = [0,80,100]\ngroup_names = ['MiddleAged', 'Old']\ndf['Age_bin']=pd.cut(df['age'],bins,labels=group_names)\ndf.drop('age',axis=1,inplace=True)\nage_variables = pd.get_dummies(df['Age_bin'])\nage_variables.drop('Old',axis=1,inplace=True)\ndf=pd.concat([df,age_variables],axis=1)\ndf.tail(10)","4621184d":"df.drop('Age_bin',axis=1,inplace=True)","2f81515c":"#model training\nfrom sklearn.model_selection import train_test_split\nX=df.drop('DEATH_EVENT',axis=1)\ny=df['DEATH_EVENT']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.28,random_state=2)","d0379763":"#Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix","f1699d95":"#Initially we do not perform any hyperparameter tuning, we use the default parameters itself\nfrom sklearn.ensemble import RandomForestClassifier\nrf_random=RandomForestClassifier()\nrf_random.fit(X_train,y_train)\nprint('Accuracy score:',accuracy_score(y_test, rf_random.predict(X_test)))\nprint(classification_report(y_test,rf_random.predict(X_test)))\nprint(confusion_matrix(y_test,rf_random.predict(X_test)))","47659180":"print(rf_random.feature_importances_)\nfeat_importances=pd.Series(rf_random.feature_importances_,index=X.columns)\nfeat_importances.nlargest(11).plot(kind='barh')\nplt.show()\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import f_classif\nprint('Chi Square:')\nprint(chi2(X,y))\nprint('F:')\nprint(f_classif(X,y))","ad7d949d":"#We can see that Diabetes, Sex, Smoking have almost no correlation with Death, so we drop them.\nX_train.drop(['diabetes','sex','smoking'],axis=1,inplace=True)\nX_test.drop(['diabetes','sex','smoking'],axis=1,inplace=True)\nprint(X_train.shape)\nprint(X_test.shape)","9327806d":"rf_random=RandomForestClassifier()\nrf_random.fit(X_train,y_train)\nprint('Accuracy score:',accuracy_score(y_test, rf_random.predict(X_test)))\nprint(classification_report(y_test,rf_random.predict(X_test)))\nprint(confusion_matrix(y_test,rf_random.predict(X_test)))","0f350fd9":"o=[]\nfor i in range (1,100):\n    o.append(i)\nfrom sklearn.model_selection import GridSearchCV\nparameters=[{'n_estimators':o,'criterion':['gini','entropy']}]\ngrid_search=GridSearchCV(estimator=rf_random,param_grid=parameters,scoring='accuracy',cv=10)\ngrid_search=grid_search.fit(X_train,y_train)\ngrid_search.best_params_","786598f4":"rf_random=RandomForestClassifier(n_estimators=49,criterion='gini')\nrf_random.fit(X_train,y_train)\nprint('Accuracy score:',accuracy_score(y_test, rf_random.predict(X_test)))\nprint(classification_report(y_test,rf_random.predict(X_test)))\nprint(confusion_matrix(y_test,rf_random.predict(X_test)))","de9c7876":"class_weight=({0:1,1:1.8})\nrf_random=RandomForestClassifier(n_estimators=59,criterion='entropy',class_weight=class_weight)\nrf_random.fit(X_train,y_train)\nprint('Accuracy score:',accuracy_score(y_test, rf_random.predict(X_test)))\nprint(classification_report(y_test,rf_random.predict(X_test)))\nprint(confusion_matrix(y_test,rf_random.predict(X_test)))","17bc684b":"from sklearn.linear_model import LogisticRegression\nlogmodel=LogisticRegression()\nlogmodel.fit(X_train,y_train)\nprint('Accuracy score:',accuracy_score(y_test, logmodel.predict(X_test)))\nprint(classification_report(y_test,logmodel.predict(X_test)))\nprint(confusion_matrix(y_test,logmodel.predict(X_test)))","e63396b3":"#we try feature scaling for improving the model performance\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train_scaled=sc.fit_transform(X_train)\nX_test_scaled=sc.transform(X_test)\nX_train_scaled","4e700610":"logmodel=LogisticRegression()\nlogmodel.fit(X_train_scaled,y_train)\nprint('Accuracy score:',accuracy_score(y_test, logmodel.predict(X_test_scaled)))\nprint(classification_report(y_test,logmodel.predict(X_test_scaled)))\nprint(confusion_matrix(y_test,logmodel.predict(X_test_scaled)))","c2d0f449":"#the performance doesn't improve, we now try the MinMax scaler\nfrom sklearn import preprocessing\nminmaxscaler=preprocessing.MinMaxScaler(feature_range=(0,1))\nX_train_scaled=minmaxscaler.fit_transform(X_train)\nX_test_scaled=minmaxscaler.transform(X_test)","3d9d25d3":"logmodel=LogisticRegression()\nlogmodel.fit(X_train_scaled,y_train)\nprint('Accuracy score:',accuracy_score(y_test, logmodel.predict(X_test_scaled)))\nprint(classification_report(y_test,logmodel.predict(X_test_scaled)))\nprint(confusion_matrix(y_test,logmodel.predict(X_test_scaled)))","8f1259c7":"#by adding class weight to balance the dataset\nclass_weight=({0:1,1:1.8})\nlogmodel=LogisticRegression(class_weight=class_weight)\nlogmodel.fit(X_train_scaled,y_train)\nprint('Accuracy score:',accuracy_score(y_test, logmodel.predict(X_test_scaled)))\nprint(classification_report(y_test,logmodel.predict(X_test_scaled)))\nprint(confusion_matrix(y_test,logmodel.predict(X_test_scaled)))","9e39cafd":"# Logistic Regression","40a499f2":"#### Random Forest Classifier with GridSearch CV","f2b4faea":"## Data Analysis","ae44b805":"#### Random Forest Algorithm with GridSearchCv and Class weights is the best model","afcbf0dc":"# Random Forest Classifier"}}