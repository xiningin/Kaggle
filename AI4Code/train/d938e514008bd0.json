{"cell_type":{"978db105":"code","7b945a9f":"code","1a7db14e":"code","65a22bf4":"code","6ddc9976":"code","0f84d370":"code","1ba7f0a0":"code","c5501539":"code","3d8892d1":"code","16772953":"code","10ecf35b":"code","c1529edd":"code","2518ed87":"code","1a56acf7":"code","819d2e8a":"code","a2dd1d52":"code","f976f52d":"code","e3a9d9af":"code","9819156f":"code","d4bd651d":"code","41e1091b":"code","05a682dd":"code","5e401fb7":"markdown","6a4d132d":"markdown","a84c8a59":"markdown","6d24b761":"markdown","8647fb0e":"markdown","acf4c16c":"markdown","690311a6":"markdown","73c8fc26":"markdown","73de7deb":"markdown","af457a0e":"markdown","0e1da679":"markdown","988b5d2c":"markdown","e9170426":"markdown","a360e33e":"markdown","68bf6d3e":"markdown","09bd1739":"markdown"},"source":{"978db105":"%%writefile requirements.txt\nplotly\nsnoop\n\ntensorflow\ntensorboard_plugin_profile\ntensorflow_addons\ntensorflow_hub","7b945a9f":"!pip install -qUr requirements.txt","1a7db14e":"import os\nimport subprocess\nimport json\nimport pickle\nimport datetime\nimport enum\n\nimport numpy as np\n# import numba\n# from numba import njit, prange\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras as tk\nimport tensorflow.keras.backend as K\n# from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\nimport tensorflow_hub as hub\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom PIL import Image\n\nfrom tqdm import tqdm\n\nimport snoop\nsnoop.install()\n\n%load_ext tensorboard\n%load_ext snoop\n\n\npp(tf.__version__)\npp(tfa.__version__)\npp(hub.__version__)","65a22bf4":"# Use these if run into Conv2D error \n# gpus = tf.config.list_physical_devices(\"GPU\")\n# for gpu in gpus:\n#     tf.config.experimental.set_memory_growth(gpu, True)\n# gpus","6ddc9976":"ds_name = \"cifar-10\"\n\n# Behaviors\ndo_make_dataset = True  #@param {type:\"boolean\"}\ndo_use_mixed_precision = True  #@param {type:\"boolean\"}\ndo_augmentation = True  #@param {type:\"boolean\"}\ndo_find_lr = True  #@param {type:\"boolean\"}\ndo_train = True  #@param {type:\"boolean\"}\ndo_load_model = False  #@param {type:\"boolean\"}\n\non_colab = False  #@param {type:\"boolean\"}\non_kaggle = True  #@param {type:\"boolean\"}\nassert (on_colab and on_kaggle) == False\n# << Behaviors\n\n# Directory\nhome_dir = os.path.expanduser(\"~\")\nnow_str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\nif on_colab:\n    work_dp = r\"\/content\/drive\/My Drive\/AITrainingRecipe\/super_convergence\"\nelif on_kaggle:\n    work_dp = r\"\/kaggle\/working\/AITrainingRecipe\/super_convergence\"\nelse:\n    work_dp = None\n    \nif not work_dp is None:\n    os.makedirs(work_dp, exist_ok=True)\n    os.chdir(work_dp)\n# !pwd && ls -lh && du -h\n\ndataset_dp = os.path.join(home_dir, \"datasets\", ds_name)\nos.makedirs(dataset_dp, exist_ok=True)\n\ntfhub_cache_dir = os.path.join(home_dir, \"tfhub_modules\")\nos.environ[\"TFHUB_CACHE_DIR\"] = tfhub_cache_dir\nos.makedirs(tfhub_cache_dir, exist_ok=True)\n\nlr_find_result_dir = os.path.join(\"lr_find_result\")\nos.makedirs(lr_find_result_dir, exist_ok=True)\n# << Directory\n\n# Dataset info\nimage_shape = [32, 32, 3]\nn_classes = 10\n# << Dataset info\n\n# Training\nn_epochs = 18\n# << Training","0f84d370":"# Input prefetch.\n!lscpu -e \n\n# Number of CPU threads (nproc --all)\nworkers = int(subprocess.check_output(\"nproc --all\", shell=True))\n\nprefetch_cfg = dict(\n    max_queue_size=10,\n    workers=workers,  \n)\npp(prefetch_cfg)\n# << Input prefetch.\n\n# Reduce 'Kernel Launch' time.\nos.environ[\"TF_GPU_THREAD_MODE\"] = \"gpu_private\"\n\n# Mixed precision (use of both 16-bit and 32-bit).\n# If using \"Mixed precision\", remember to cast output of last layer to \"float32\" for numeric stability.\n# For GPU: \"mixed_float16\"; for TPU: \"mixed_bfloat16\"\nif do_use_mixed_precision:\n    policy = mixed_precision.Policy(\"mixed_float16\")\n    mixed_precision.set_policy(policy)\n\n    pp(policy.compute_dtype)\n    pp(policy.variable_dtype)\n    pp(policy.loss_scale)\n!nvidia-smi -L\n# << Mixed precision.\n\n\ndef reset_env():\n    \"\"\"Call this before creating new model.\"\"\"\n    tf.keras.backend.clear_session()\n    tf.config.optimizer.set_jit(True)  # Enable XLA","1ba7f0a0":"%%time\n# https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html\n\ndef unpickle(file):\n    with open(file, 'rb') as fo:\n        d = pickle.load(fo, encoding='bytes')\n    return d\n\n\ndef data_to_img(data):\n    data = np.array_split(data, 3, axis=0)\n    data = [np.array_split(channel, 32, axis=0) for channel in data]\n    data = np.stack(data, axis=-1)\n    return data\n\n\ndef save_images(images, dir, labels, filenames, target_size=None, do_override=False):\n    # Resize image\n    image_size = images.shape[1:3]\n    if target_size is not None and list(target_size) != image_size:\n        target_height, target_width = target_size\n        images = tf.image.resize_with_pad(images, target_height, target_width).numpy()\n        print(\"Resized images from {} to {}.\".format(image_size, target_size))\n\n    for image, label, filename in tqdm(zip(images, labels, filenames)):\n        image_fp = os.path.join(dir, label, filename)\n\n        if not os.path.isfile(image_fp) or do_override:\n            pil_im = Image.fromarray(image.astype(np.uint8))\n            pil_im.save(image_fp)\n\n\ndef make_dataset(pickle_path, dir):\n    raw = unpickle(pickle_path)\n    x = np.array([data_to_img(img_data) for img_data in raw[b\"data\"]])\n    y = np.array(raw[b\"labels\"])\n    pp(x.shape, y.shape)\n\n    labels = np.array(meta[b\"label_names\"], dtype=str)[y]\n    filenames = np.array(raw[b\"filenames\"], dtype=str)\n\n    save_images(x, dir, labels, filenames, target_size=None, do_override=False)\n\n\ntrain_dp = os.path.join(dataset_dp, \"cifar-10_32x32\/train\/\")\ntest_dp = os.path.join(dataset_dp, \"cifar-10_32x32\/test\/\")\n\nif do_make_dataset:\n    # Download and extract raw data\n    raw_data_fp = \"cifar-10-python.tar.gz\"\n    !cd $dataset_dp && wget -nc https:\/\/www.cs.toronto.edu\/~kriz\/cifar-10-python.tar.gz -O $raw_data_fp\n    !cd $dataset_dp && tar --skip-old-files -xf $raw_data_fp\n\n    # Find top-level directory(-ies) of an archive.\n    raw_data_dp = subprocess.check_output(\n        \"cd {} && tar -tf {} | sed -e 's@\/.*@@' | uniq\".format(dataset_dp, raw_data_fp), \n        shell=True,\n    )\n    raw_data_dp = raw_data_dp.decode().replace(\"\\n\", \"\")\n    # Raw data dir path\n    raw_data_dp = os.path.join(dataset_dp, raw_data_dp)\n\n    # Make dataset\n    meta = unpickle(os.path.join(raw_data_dp, \"batches.meta\"))\n\n    train_pickle_paths = [\n        os.path.join(raw_data_dp, \"data_batch_1\"),\n        os.path.join(raw_data_dp, \"data_batch_2\"),\n        os.path.join(raw_data_dp, \"data_batch_3\"),\n        os.path.join(raw_data_dp, \"data_batch_4\"),\n        os.path.join(raw_data_dp, \"data_batch_5\"),\n    ]\n    test_pickle_paths = [os.path.join(raw_data_dp, \"test_batch\")]\n\n    os.makedirs(train_dp, exist_ok=True)\n    os.makedirs(test_dp, exist_ok=True)\n\n    for cls in meta[b\"label_names\"]:\n        cls = cls.decode(\"utf-8\")\n        os.makedirs(os.path.join(train_dp, cls), exist_ok=True)\n        os.makedirs(os.path.join(test_dp, cls), exist_ok=True)\n    \n    for train_pickle_path in train_pickle_paths:\n        pp(train_pickle_path, train_dp)\n        make_dataset(train_pickle_path, train_dp)\n    \n    for test_pickle_path in test_pickle_paths:\n        pp(test_pickle_path, test_dp)\n        make_dataset(test_pickle_path, test_dp)\n\n!cd $dataset_dp && pwd && ls -lh && du -h","c5501539":"# tensorflow_hub\n# def cache_tfhub_model(tfhub_cache_dir, hub_model_link, net_name):\n#     \"\"\"\n#     Manually cache tfhub model, \n#     use this in case automatically caching fail, especially when on Colab.\n#     \"\"\"\n#     tfhub_module_dir = os.path.join(tfhub_cache_dir, net_name)\n\n#     if not os.path.isfile(os.path.join(tfhub_module_dir, \"saved_model.pb\")):\n#         os.makedirs(tfhub_module_dir, exist_ok=True)\n#         subprocess.run(\n#             f\"curl -L {hub_model_link}?tf-hub-format=compressed | tar -zxvC {tfhub_module_dir}\",\n#             shell=True,\n#             check=True,\n#         )\n\n#     return tfhub_module_dir\n\n\n# hub_model_link = \"https:\/\/tfhub.dev\/google\/bit\/m-r50x1\/1\"\n# net_name = \"bit_m-r50x1\"\n# input_shape = [128, 128, 3]\n# batch_size = 256\n\n# hub_model_link = \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b0\/feature-vector\/1\"\n# net_name = \"EfficientNetB0\"\n# input_shape = [224, 224, 3]\n# batch_size = 64  # 256 if do_use_mixed_precision\n\n# handle = hub_model_link\n# handle = cache_tfhub_model(tfhub_cache_dir, hub_model_link, net_name)\n# << tensorflow_hub\n\n\n# # tf.keras.applications\nbase_model = tk.applications.EfficientNetB0\npreprocessing_function = tk.applications.efficientnet.preprocess_input\ninput_shape = [224, 224, 3]\nbatch_size = 256  # 256 if do_use_mixed_precision\n\nnet_name = base_model.__name__\n# # << tf.keras.applications","3d8892d1":"image_dataset_cfg = dict(\n    labels=\"inferred\",\n    label_mode=\"categorical\",\n    color_mode=\"rgb\", \n    batch_size=batch_size, \n    image_size=image_shape[:2],\n)\n\ntrain_data = tk.preprocessing.image_dataset_from_directory(train_dp, shuffle=True, **image_dataset_cfg)\ntest_data = tk.preprocessing.image_dataset_from_directory(train_dp, shuffle=False, **image_dataset_cfg)\n\nassert train_data.class_names == test_data.class_names\ntrain_data.class_names","16772953":"%%time\ndef get_inputs(input_shape, name=\"inputs\"):\n    inputs = tk.layers.Input(original_input_shape, name=name)\n    \n#     if do_augmentation:\n#         pass\n    \n    height, width = input_shape[:2]\n    resize_rescale = tk.Sequential([\n        preprocessing.Resizing(height, width, interpolation=\"bilinear\", name=\"resizing\"),\n    ])\n    inputs = resize_rescale(inputs)\n    return inputs\n\n\ndef get_model(input_shape, n_classes):\n    # tf.keras.applications\n    base = base_model(\n        include_top=False,\n        weights=\"imagenet\",\n        pooling=\"avg\",\n    )\n    base.trainable = True\n    \n    # Freeze BatchNormalization layers, DON'T DO IT.\n    # for layer in base.layers:\n    #     if isinstance(layer, tk.layers.BatchNormalization):\n    #         layer.trainable = False\n    # << tf.keras.applications\n    \n    # tensorflow_hub\n    # base = hub.KerasLayer(handle, name=net_name, trainable=True)\n    \n    # ATTENTION:\n    # When Fine-tuning, it's important to initialise the weights of newly added layers to all zeros.\n    inputs = get_inputs(input_shape, name=\"inputs\")\n    x = base(inputs)\n    \n    x = tk.layers.Dense(n_classes, kernel_initializer=\"zeros\", name=\"logits\")(x)\n    outputs = tk.layers.Activation(\"softmax\", name=\"pred\", dtype=tf.float32)(x)  # Use \"float32\" for numeric stability.\n\n    model = tk.models.Model(inputs, outputs, name=f\"{ds_name}_{net_name}\")\n    print(\"Layers' computations dtype \", x.dtype)\n    print(\"Outputs' dtype \", outputs.dtype)\n    return model\n\n\nreset_env()\nmodel = get_model(input_shape, n_classes)\n\nmodel_plot_fp = f\"{ds_name}_{net_name}.png\"\ntk.utils.plot_model(model, show_shapes=True, to_file=model_plot_fp)\nmodel.summary()","10ecf35b":"class DecayType(enum.IntEnum):\n    \"\"\"Data class, each decay type is assigned a number.\"\"\"\n    LINEAR = 0\n    COSINE = 1\n    EXPONENTIAL = 2\n    POLYNOMIAL = 3\n\n\nclass DecayScheduler():\n    \"\"\"Given initial and endvalue, \n    this class generates the value depending on decay type and decay steps (by calling).\n    \"\"\"\n\n    def __init__(self, start_val, end_val, decay_steps, decay_type, extra=1.0):\n        self.start_val = start_val\n        self.end_val = end_val\n        self.decay_steps = decay_steps\n        self.decay_type = decay_type\n        self.extra = extra\n    \n    def __call__(self, step):\n        if self.decay_type == DecayType.LINEAR:\n            pct = step \/ self.decay_steps\n            return self.start_val + pct * (self.end_val - self.start_val)\n        elif self.decay_type == DecayType.COSINE:\n            cos_out = np.cos(np.pi * step \/ self.decay_steps) + 1\n            return self.end_val + (self.start_val - self.end_val) \/ 2 * cos_out\n        elif self.decay_type == DecayType.EXPONENTIAL:\n            ratio = self.end_val \/ self.start_val\n            return self.start_val * ratio **  (step \/ self.decay_steps)\n        elif self.decay_type == DecayType.POLYNOMIAL:\n            return self.end_val + (self.start_val - self.end_val) * (1 - step \/ self.decay_steps) ** self.extra","c1529edd":"def get_decay_steps(cycle_len, anneal_pct):\n    phase_len = int(cycle_len * (1 - anneal_pct) \/ 2)\n    anneal_len = cycle_len - phase_len * 2\n    return phase_len, phase_len, anneal_len\n\ndef onecyle_learning_rate(\n    init_lr,\n    end_lr,\n    train_steps,\n    decay_type=DecayType.LINEAR,\n    anneal_pct=0.075,\n):\n    \"\"\"OneCyle learning rates\n    Args:\n        anneal_pct (float): \n            Percentage to leave for the annealing at the end.\n            The annealing phase goes from the minimum lr to 1\/100th of it linearly.\n    \"\"\"\n    phase_decay_steps = get_decay_steps(train_steps, anneal_pct)\n\n    lr_schedulers = [\n        DecayScheduler(init_lr, end_lr, phase_decay_steps[0], decay_type),\n        DecayScheduler(end_lr, init_lr, phase_decay_steps[1], decay_type),\n        DecayScheduler(init_lr, init_lr \/ 100., phase_decay_steps[2], decay_type),\n    ]\n\n    learning_rates = []\n    for lr_scheduler, decay_steps in zip(lr_schedulers, phase_decay_steps):\n        learning_rates.append(lr_scheduler(np.arange(decay_steps)))\n\n    learning_rates = np.concatenate(learning_rates, 0)\n    return learning_rates\n\n\ndef onecyle_momentum(\n    init_mom,\n    end_mom,\n    train_steps,\n    decay_type=DecayType.LINEAR,\n    anneal_pct=0.075,\n):\n    \"\"\"OneCyle learning rates\n    Args:\n        anneal_pct (float): \n            Percentage to leave for the annealing at the end.\n            The annealing phase use constant maximum momentum.\n    \"\"\"\n    phase_decay_steps = get_decay_steps(train_steps, anneal_pct)\n\n    mom_schedulers = [\n        DecayScheduler(init_mom, end_mom, phase_decay_steps[0], decay_type),\n        DecayScheduler(end_mom, init_mom, phase_decay_steps[1], decay_type),\n    ]\n\n    moms = []\n    for mom_scheduler, decay_steps in zip(mom_schedulers, phase_decay_steps):\n        moms.append(mom_scheduler(np.arange(decay_steps)))\n    moms.append(np.array([init_mom] * phase_decay_steps[2]))\n\n    moms = np.concatenate(moms, 0)\n    return moms","2518ed87":"class OneCycleScheduler(tk.callbacks.Callback):\n    \"\"\"Callback that update lr, momentum at begining of mini batch based on OneCycle policy.\"\"\"\n\n    def __init__(\n        self,\n        init_lr,\n        end_lr,\n        train_steps,\n        decay_type=DecayType.LINEAR,\n        anneal_pct=0.075,\n        init_mom=None,\n        end_mom=None,\n    ):\n        super().__init__()\n        self.train_steps = train_steps\n\n        common_kwargs = dict(\n            train_steps=train_steps,\n            decay_type=decay_type,\n            anneal_pct=anneal_pct,\n        )\n\n        self.learning_rates = onecyle_learning_rate(\n            init_lr=init_lr,\n            end_lr=end_lr,\n            **common_kwargs\n        )\n\n        if not (init_mom is None or end_mom is None):\n            self.moms = onecyle_momentum(\n                init_mom=init_mom,\n                end_mom=end_mom,\n                **common_kwargs\n            )\n        else:\n            self.moms = None\n\n    def on_train_begin(self, logs=None):\n        self.train_step = 0\n\n    def on_train_batch_begin(self, batch, logs=None):\n        # Set lr, momentum for current mini batch.\n        if self.train_step < self.train_steps:\n            K.set_value(self.model.optimizer.learning_rate, self.learning_rates[self.train_step])\n\n            if not self.moms is None:\n                mom = self.moms[self.train_step]\n                \n                # Dict that map momentum attributes to respected optimizer classes\n                mom_attrs = {\n                    \"beta_1\" : tf.keras.optimizers.Adam,\n                    \"beta_1\" : tfa.optimizers.AdamW,\n                    \"momentum\" : tf.keras.optimizers.SGD,\n                    \"momentum\" : tf.keras.optimizers.RMSprop,\n                }\n                \n                if (\n                    do_use_mixed_precision \n                    and isinstance(self.model.optimizer, mixed_precision.LossScaleOptimizer)\n                ):\n                    real_optimizer = self.model.optimizer._optimizer\n                else:\n                    real_optimizer = self.model.optimizer  \n                    \n                for mom_attr, optimizer_class in mom_attrs.items():\n                    if isinstance(real_optimizer, optimizer_class):\n                        K.set_value(getattr(real_optimizer, mom_attr), mom)\n                        break\n\n        self.train_step += 1\n        \n\n    def plot(self):\n        a_train_steps = np.arange(self.train_steps)\n        traces = [\n            go.Scatter(x=a_train_steps, y=self.learning_rates, name=\"learning_rate\"),\n        ]\n        if not self.moms is None:\n            traces.append(go.Scatter(x=a_train_steps, y=self.moms, name=\"momentum\"))\n\n        fig = make_subplots(len(traces), 1)\n        for i, trace in enumerate(traces):\n            fig.add_trace(trace, row=i+1, col=1)\n        fig.show()","1a56acf7":"class LRFinder(tk.callbacks.Callback):\n    \"\"\"Learning Rate Finder Callback\"\"\"\n    \n    def __init__(\n        self, \n        init_lr,\n        end_lr,\n        decay_type=DecayType.EXPONENTIAL,\n        beta=0.98\n    ):\n        super().__init__()\n        self.init_lr = init_lr\n        self.end_lr = end_lr\n        self.decay_type = decay_type\n        self.beta = beta\n\n    def on_train_begin(self, logs=None):\n        # pp(self.params)\n        self.train_steps = self.params[\"epochs\"] * self.params[\"steps\"]\n\n        self.scheduler = DecayScheduler(\n            start_val=self.init_lr, \n            end_val=self.end_lr, \n            decay_steps=self.train_steps,\n            decay_type=self.decay_type, \n        )\n        self.learning_rates = self.scheduler(np.arange(self.train_steps))\n\n        self.history = {}\n        self.train_step = 0\n        self.avg_loss = 0.\n        self.best_loss = 0.\n        self.best_learning_rate = 0.\n\n    def on_train_batch_begin(self, batch, logs=None):\n        K.set_value(self.model.optimizer.learning_rate, self.learning_rates[self.train_step])\n\n    def on_train_batch_end(self, batch, logs=None):\n        # Compute the smoothed loss\n        self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * logs[\"loss\"]\n        smoothed_loss = self.avg_loss \/ (1 - self.beta ** (self.train_step + 1))\n\n        # Stop if the loss is exploding\n        if self.train_step > 1 and smoothed_loss > 4 * self.best_loss:\n            self.model.stop_training = True\n            print(\"Stop training because loss is exploding.\")\n\n        # Record the best loss, learning_rate\n        if self.train_step == 1 or smoothed_loss < self.best_loss:\n            self.best_loss = smoothed_loss\n            self.best_learning_rate = self.learning_rates[self.train_step]\n\n        if not self.model.stop_training:\n            # History\n            for key, val in logs.items():\n                self.history.setdefault(key, []).append(val)\n            self.history.setdefault(\"smoothed_loss\", []).append(smoothed_loss)\n\n            self.train_step += 1\n\n    def plot_learning_rate(self):\n        fig = px.line(\n            x=np.arange(self.train_step),\n            y=self.learning_rates[:self.train_step],\n            labels=dict(x=\"train_step\", y=\"learning_rate\"),\n        )\n        fig.show()\n\n    def _plot_metric(self, metric, metric_name):\n        fig = px.line(\n            x=self.learning_rates[:self.train_step],\n            y=metric, \n            log_x=True,\n            labels=dict(x=\"learning_rate(log)\", y=metric_name),\n        )\n        fig.show()\n\n    def plot_accuracy(self):\n        self._plot_metric(self.history[\"accuracy\"], \"accuracy\")\n\n    def plot_loss(self):\n        self._plot_metric(self.history[\"loss\"], \"loss\")\n\n    def plot_smoothed_loss(self):\n        self._plot_metric(self.history[\"smoothed_loss\"], \"smoothed_loss\")\n\n\ndef plot_metric(learning_rates, metrics, metric_name, optimizer_cfgs):\n    \"\"\"\n    Plot the same metric from multiple training run (using different optimizer configuration),\n    against learning rates.\n    \"\"\"\n    fig = go.Figure()\n\n    for metric, optimizer_cfg in zip(metrics, optimizer_cfgs):\n        if len(metric) < len(learning_rates):\n            metric = np.append(\n                metric, [np.nan] * (len(learning_rates) - len(metric))\n            )\n        fig.add_trace(\n            go.Scatter(x=learning_rates, y=metric, name=f\"{optimizer_cfg}\")\n        )\n\n    fig.update_xaxes(type=\"log\")\n    fig.update_layout(xaxis_title=\"learning_rate(log)\", yaxis_title=metric_name)\n    fig.show()\n    return fig","819d2e8a":"loss = \"categorical_crossentropy\"\n\noptimizer_class = tfa.optimizers.AdamW\noptimizer_default_cfg = dict(beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n\nweight_decays = [1e-2, 1e-3, 1e-4]\noptimizer_cfgs = [\n    dict(weight_decay=wd) for wd in weight_decays\n]\n\n# optimizer_class = tk.optimizers.RMSprop\n# optimizer_class = tk.optimizers.Adam\n# optimizer_cfgs = [{}]","a2dd1d52":"%%time\nif do_find_lr:\n    init_lr = 1e-4\n    end_lr = 1\n\n    learning_rates = None\n    accuracies = []\n    losses = []\n    smoothed_losses = []\n    best_losses = []\n    best_learning_rates = []\n\n    for optimizer_cfg in optimizer_cfgs:\n        pp(optimizer_cfg)\n        \n        reset_env()\n        model = get_model(input_shape, n_classes)\n\n        cb_lrfinder = LRFinder(init_lr, end_lr, decay_type=DecayType.EXPONENTIAL, beta=0.98)\n\n        optimizer = optimizer_class(**{**optimizer_default_cfg, **optimizer_cfg})\n        model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n        \n        model.fit(\n            train_data,\n            epochs=1, \n            batch_size=batch_size, \n            callbacks=[cb_lrfinder], \n            verbose=1,\n            **prefetch_cfg\n        )\n\n        # cb_lrfinder.plot_learning_rate()\n        # cb_lrfinder.plot_accuracy()\n        # cb_lrfinder.plot_loss()\n        # cb_lrfinder.plot_smoothed_loss()\n        # pp(cb_lrfinder.best_loss, cb_lrfinder.best_learning_rate)\n\n        if learning_rates is None:\n            learning_rates = cb_lrfinder.learning_rates\n\n        accuracies.append(cb_lrfinder.history[\"accuracy\"])\n        losses.append(cb_lrfinder.history[\"loss\"])\n        smoothed_losses.append(cb_lrfinder.history[\"smoothed_loss\"])\n        best_losses.append(cb_lrfinder.best_loss)\n        best_learning_rates.append(cb_lrfinder.best_learning_rate)","f976f52d":"lr_find_result_f = os.path.join(lr_find_result_dir, f\"{net_name}.pickle\")\n\nif do_find_lr:\n    lr_find_result = {\n        \"accuracies\" : accuracies,\n        \"losses\" : losses,\n        \"smoothed_losses\" : smoothed_losses,\n        \"learning_rates\" : learning_rates,\n        \"best_losses\" : best_losses,\n        \"best_learning_rates\": best_learning_rates,\n    }\n    with open(lr_find_result_f, \"wb\") as pickle_fo:\n        pickle.dump(lr_find_result, pickle_fo)\nelse:\n    if os.path.isfile(lr_find_result_f):\n        with open(lr_find_result_f, \"rb\") as pickle_fo:\n            lr_find_result = pickle.load(pickle_fo)\n    else:\n        lr_find_result = {}        \n\nfor key, val in lr_find_result.items():\n    if key in [\"accuracies\", \"losses\", \"smoothed_losses\"]:\n        fig = plot_metric(\n            lr_find_result[\"learning_rates\"],\n            metrics=val,\n            metric_name=key,\n            optimizer_cfgs=optimizer_cfgs,\n        )\n\npp(optimizer_cfgs, lr_find_result.get(\"best_losses\"), lr_find_result.get(\"best_learning_rates\"))","e3a9d9af":"do_use_onecyle = True\npp(do_use_onecyle)\n\noptimizer_cfg = dict(\n    weight_decay=0.001,  # <<SET THIS, bit_m-r50x1: 0.0001, EfficientNetB0: 0.001 \n)\n\nif do_use_onecyle:\n    n_batches_per_epoch = len(train_data)\n    n_train_steps = n_epochs * n_batches_per_epoch\n    pp(n_train_steps)\n\n    # Learning rate\n    end_lr = 0.05  # <<SET THIS, bit_m-r50x1: 0.0015; EfficientNetB0, mixed: 0.1, 0.05; EfficientNetB0: 0.0066, 0.004\n    init_lr = end_lr \/ 10.\n\n    # Momemtum\n    init_mom = 0.95\n    end_mom = 0.85\n\n    cb_onecycle = OneCycleScheduler(\n        init_lr, \n        end_lr,\n        n_train_steps,\n        decay_type=DecayType.LINEAR,\n        anneal_pct=0.075,\n        init_mom=init_mom,\n        end_mom=end_mom,\n    )\n    cb_onecycle.plot()\nelse:\n    optimizer_cfg[\"learning_rate\"] = 0.0015  # <<SET THIS, bit_m-r50x1: 0.0015\n    cb_onecycle = None\n    \npp(optimizer_cfg)","9819156f":"monitor = \"val_accuracy\"\nmode = \"max\"\n# goal = 0.97\nmin_delta = 1e-3\n\nvalidation_freq = 3\n\ncb_early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor=monitor,\n    mode=mode,\n    min_delta=min_delta,\n    patience=9,\n    restore_best_weights=True,\n)\n\n# Model checkpoint dir\nmodel_root_dir = os.path.join(\"models\/\", net_name)\nos.makedirs(model_root_dir, exist_ok=True)\n\ncheckpoint_fp = os.path.join(model_root_dir, now_str)\nmodel_fps = os.listdir(model_root_dir)\nlastest_model_fp = (\n    os.path.join(model_root_dir, sorted(model_fps)[-1]) \n    if len(model_fps) > 0 \n    else None\n)\n\ncb_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    checkpoint_fp,\n    monitor=monitor,\n    mode=mode,\n    save_weights_only=False,\n    save_best_only=True,\n    save_format=\"tf\",\n    include_optimizer=True,\n)\n\n# Tensorboard logs dir\ntb_logs_root_dir = os.path.join(\"tb_logs\/\", net_name)\nif do_train:\n    tb_logs_dp = os.path.join(tb_logs_root_dir, now_str)\nelse:\n    tb_logs_dps = os.listdir(tb_logs_root_dir)\n    tb_logs_dp = (\n        os.path.join(tb_logs_root_dir, sorted(tb_logs_dps)[-1]) \n        if len(tb_logs_dps) > 0 \n        else None\n    )\n\ncb_tensorboard = tf.keras.callbacks.TensorBoard(\n    log_dir=tb_logs_dp,\n    histogram_freq=1,\n    update_freq='epoch',\n    profile_batch=\"2,22\",\n)\n\n\n# class Goal(tf.keras.callbacks.Callback):\n#     def __init__(self, monitor, mode, goal):\n#         super().__init__()\n#         self.monitor = monitor\n#         self.mode = mode\n#         self.goal = goal\n\n#     def on_epoch_end(self, epoch, logs={}):\n#         if self.mode == \"min\":\n#             goal_achieved = logs[self.monitor] <= self.goal\n#         elif self.mode == \"max\":\n#             goal_achieved = logs[self.monitor] >= self.goal\n\n#         if goal_achieved:\n#             print(\"Goal {}: {} achieved. Stop training.\".format(self.monitor, self.goal))\n#             self.model.stop_training = True\n\n\n# cb_goal = Goal(monitor, mode, goal)\n\n# cb_reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n#     monitor=monitor,\n#     mode=mode,\n#     min_delta=min_delta,\n#     patience=5,\n#     factor=0.2,\n#     min_lr=1e-6,\n# )\n\ncallbacks = [\n    cb_checkpoint,\n    cb_early_stopping,\n    cb_tensorboard,\n    # cb_goal,\n    # cb_reduce_lr,\n]\nif not cb_onecycle is None:\n    callbacks.append(cb_onecycle)","d4bd651d":"%%time\nreset_env()\ntf.get_logger().setLevel('ERROR')\n\n# For test run\n# n_epochs = 3\n\nif do_train:\n\n    model_loaded = False\n    if do_load_model:\n        try:\n            model = tk.models.load_model(lastest_model_fp, compile=True)\n            model_loaded = True\n            print(\"Model loaded: {}\".format(lastest_model_fp))\n        except Exception as ex:\n            print(\"ERROR load_model: {}\".format(ex))\n    \n    if not do_load_model or not model_loaded:\n        model = get_model(input_shape, n_classes)\n        optimizer = optimizer_class(**{**optimizer_default_cfg, **optimizer_cfg})\n        model.compile(\n            loss=loss,\n            optimizer=optimizer,\n            metrics=[\"accuracy\"],\n        )\n        print(\"Created new model.\")\n\n    history = model.fit(\n        train_data,\n        epochs=n_epochs,\n        validation_data=test_data,\n        validation_freq=validation_freq,\n        callbacks=callbacks,\n        verbose=1,\n        **prefetch_cfg,\n    )\nelse:\n    model = tk.models.load_model(lastest_model_fp, compile=True)\n","41e1091b":"model.evaluate(test_data, verbose=1, **prefetch_cfg)","05a682dd":"%tensorboard --logdir $tb_logs_dp","5e401fb7":"# Utils","6a4d132d":"## Results\n| model                 | params     | input_shape   | epochs | batch_size | optimizer | learning_rate              | momentum              | weight_decay | val_loss            | val_accuracy       | checkpoint                         | tensorboard_log                     |\n| --------------------- | ---------- | ------------- | ------ | ---------- | --------- | -------------------------- | --------------------- | ------------ | ------------------- | ------------------ | ---------------------------------- | ----------------------------------- |\n| bit_m-r50x1 fine-tune | 23,520,842 | [128, 128, 3] | 18     | 256        | AdamW     | OneCycle [0.00015, 0.0015] | OneCycle [0.85, 0.95] | 0.0001       | 0.12395142763853073 | 0.964400053024292  | models\/bit_m-r50x1\/20201208-051510 | tb_logs\/bit_m-r50x1\/20201208-051510 |\n| bit_m-r50x1 fine-tune | 23,520,842 | [128, 128, 3] | 18     | 256        | AdamW     | 0.0015                     | 0.9                   | 0.0001       | 0.304882675409317   | 0.9014000296592712 | models\/bit_m-r50x1\/20201209-024614 | tb_logs\/bit_m-r50x1\/20201209-024614 |\n\n- For Adam\/AdamW, momentum is beta_1, other params are kept as default (beta_2=0.999, epsilon=1e-07, amsgrad=False).","a84c8a59":"# README\nUse Cifar10 dataset. Train a model within 18 epochs, to at least 0.94 validation accuracy.","6d24b761":"# Make Dataset","8647fb0e":"# Train","acf4c16c":"# Config","690311a6":"## To-do\n- Make preprocessing data run on GPU (Colab, Kaggle has very weak CPU).\n    - https:\/\/www.tensorflow.org\/tfx\/tutorials\/transform\/simple\n    - https:\/\/www.tensorflow.org\/guide\/keras\/preprocessing_layers\n- Add Confusion Matrix.\n- Try https:\/\/github.com\/szagoruyko\/wide-residual-networks","73c8fc26":"# LICENSE\nMIT License\n\nCopyright (c) 2020 Tran (Le Thai) Son\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.","73de7deb":"# LR Finder","af457a0e":"# Model & Dataset\n**ATTENTION**:  \nGPU Tensor Cores (XLA) requires: Units, filters of layers; batch_size to be a multiple of 8.","0e1da679":"# Evaluation","988b5d2c":"# HP Scheduler","e9170426":"# Hyper Params\nLOOK at the graphs above to find optimal hyper parameters (weight_decay, learning_rate).","a360e33e":"## References\n- [Dataset](https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html)\n- [LR Finder](https:\/\/sgugger.github.io\/how-do-you-find-a-good-learning-rate.html)\n- [Super Convegence with AdamW](https:\/\/www.fast.ai\/2018\/07\/02\/adam-weight-decay\/)\n- About Transfer Learning & Fine Tuning\n    - https:\/\/keras.io\/guides\/transfer_learning\/\n    - https:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/\n    - https:\/\/blog.tensorflow.org\/2020\/05\/bigtransfer-bit-state-of-art-transfer-learning-computer-vision.html","68bf6d3e":"# Callbacks","09bd1739":"# Training Optimization"}}