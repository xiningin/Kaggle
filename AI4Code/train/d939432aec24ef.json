{"cell_type":{"7937a318":"code","764759b3":"code","ec50e164":"code","ef5ade68":"code","cac7bdf7":"code","6d15b76c":"code","34061512":"code","579887fc":"code","af8e2ad7":"code","0cfbe6bc":"code","1727d0ac":"code","19db241f":"code","5aa25c6c":"code","b5afb11d":"code","88bcf642":"code","67d49a8c":"code","3de6a5f4":"code","936e4ebe":"code","c78847e1":"code","ac1f339a":"code","a1b3a925":"code","8fe62051":"code","97eb39ef":"code","7645fa00":"code","1e8c8e68":"code","c1e8b898":"code","c80d5125":"code","9e294a29":"code","c0657e33":"code","c5f97d3e":"code","64d2a581":"code","29e61e0a":"code","77ef1574":"code","923a70df":"code","79b6fa6b":"code","e8537268":"code","4a01ab74":"code","3d48c8a7":"markdown","f3a54fdb":"markdown","7d1724f3":"markdown","363da7cf":"markdown","929bbc41":"markdown","28962f9f":"markdown","393b776a":"markdown","46f4878d":"markdown","537d4f0e":"markdown","9a18488a":"markdown","8777d62b":"markdown","0323f1cb":"markdown","ae552b9b":"markdown","d935244d":"markdown","08466d34":"markdown","1ac00de7":"markdown","a8807aa6":"markdown","9c84d613":"markdown","77d447fd":"markdown","91c0f836":"markdown","d22e9ff0":"markdown","6374e4c7":"markdown","15e3ae63":"markdown","f073639a":"markdown","7009296d":"markdown","441d74ae":"markdown","ccef8117":"markdown","2b940f1d":"markdown","a9ad3ace":"markdown","1448b421":"markdown","af6a4786":"markdown","a386ac92":"markdown","9ec2008e":"markdown","65305054":"markdown","0279b995":"markdown","c422855a":"markdown","572e5e1a":"markdown","da4e19d1":"markdown","4d89382c":"markdown","0a8fc205":"markdown","a1750645":"markdown","53f34d81":"markdown","e20ff326":"markdown","a9b63899":"markdown","38a31c2b":"markdown","0368123c":"markdown"},"source":{"7937a318":"# Principal ML and visualizations libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\nimport plotly.graph_objects as go\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nsns.set()\n\n%matplotlib inline","764759b3":"dataset = pd.read_csv('..\/input\/reviews\/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\ndataset.head()","ec50e164":"Labels = pd.DataFrame(dataset['Liked'].value_counts()).reset_index()\nLabels.columns = ['Liked','Total']\nLabels['Liked'] = Labels['Liked'].map({1: 'Positive', 0: 'Negative'})\n\nfig = px.pie(Labels, values = 'Total', names = 'Liked', title='Percentage of reviews', hole=.4, color = 'Liked',\n             width=800, height=400)\nfig.show()","ef5ade68":"positive = dataset[dataset[\"Liked\"] == 1][[\"Review\", \"Liked\"]]","cac7bdf7":"plt.subplots(figsize=(16,13))\nwordcloud = WordCloud(\n                          background_color='black',max_words = 10000,\n                          width=1500, stopwords=STOPWORDS,\n                          height=1080\n                         ).generate(\" \".join(positive.Review))\nplt.title(\"Positive Reviews\", fontsize=20)\nplt.imshow(wordcloud.recolor( colormap= 'viridis'))\nplt.axis('off')\nplt.show()","6d15b76c":"negative = dataset[dataset[\"Liked\"] == 0][[\"Review\", \"Liked\"]]\n\nplt.subplots(figsize=(16,13))\nwordcloud = WordCloud(\n                          background_color='black',max_words = 1000, \n                          width=1500, stopwords=STOPWORDS,\n                          height=1080\n                         ).generate(\" \".join(negative.Review))\nplt.title(\"Negative Reviews\", fontsize=20)\nplt.imshow(wordcloud.recolor( colormap= 'Pastel2'))\nplt.axis('off')\nplt.show()","34061512":"# NLP libraries\n\nimport re # Regular expressions\nimport nltk # Natural language tool kit\nfrom nltk.corpus import stopwords # This will help us get rid of useless words.\n\n# Extra needed packages\n\nnltk.download('punkt') \nnltk.download('stopwords')\nnltk.download('wordnet')","579887fc":"first_review = dataset['Review'][0]\nfirst_review","af8e2ad7":"first_review = first_review.lower()\nfirst_review = re.sub(\"[^a-zA-Z]\", \" \", first_review)\nfirst_review","0cfbe6bc":"# Tokenization\n\nfirst_review_list = nltk.word_tokenize(first_review)\nfirst_review_list","1727d0ac":"stopwords = nltk.corpus.stopwords.words('english')\n\nfirst_review_list_cleaned = [word for word in first_review_list if word.lower() not in stopwords]\nprint(first_review_list_cleaned)","19db241f":"from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmer.stem('lover'), stemmer.stem('cellphone')","5aa25c6c":"for w in first_review_list_cleaned:\n        print(stemmer.stem(w))","b5afb11d":"from nltk.stem import WordNetLemmatizer \n\nlemma = WordNetLemmatizer()\nlemma.lemmatize('lover'), lemma.lemmatize('cellphone')","88bcf642":"for w in first_review_list_cleaned:\n        print(lemma.lemmatize(w))","67d49a8c":"corpus=[]\n\nfor review in dataset['Review']:\n    review = review.lower()\n    review = re.sub(\"[^a-zA-Z]\", \" \", review)\n    review = nltk.word_tokenize(review)\n    review = [word for word in review if word.lower() not in stopwords]\n    lemma = WordNetLemmatizer()\n    review = [lemma.lemmatize(word) for word in review]\n    review = \" \".join(review) # Building again the dataframe\n    corpus.append(review)\n    \ndataset[\"Review\"] = corpus\ndataset.head(5)","3de6a5f4":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer()\nbag_of_words = cv.fit_transform(corpus).toarray()\nbag_of_words","936e4ebe":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier","c78847e1":"y = dataset.iloc[:,1].values\n\nX_train, X_test, y_train, y_test = train_test_split(bag_of_words, y, test_size = 0.20, random_state = 0)","ac1f339a":"def clf_model(model):\n    clf = model\n    clf.fit(X_train, y_train)\n    accuracy = accuracy_score(y_test, clf.predict(X_test).round())\n    recall = recall_score(y_test, clf.predict(X_test).round())\n    precision = precision_score(y_test, clf.predict(X_test).round())\n    return clf, accuracy, recall, precision","a1b3a925":"model_performance = pd.DataFrame(columns = [\"Model\", \"Accuracy\", \"Recall\", \"Precision\"])\n\nmodels_to_evaluate = [DecisionTreeClassifier(), LogisticRegression(), RandomForestClassifier(n_estimators=1000),\n                      KNeighborsClassifier(n_neighbors = 7, metric = \"minkowski\", p = 2),\n                      SVC(kernel = 'rbf'), GaussianNB(), XGBClassifier(n_estimators=300, learning_rate=0.01)]\n\nfor model in models_to_evaluate:\n    clf, accuracy, recall, precision = clf_model(model)\n    model_performance = model_performance.append({\"Model\": model, \"Accuracy\": accuracy,\n                                                  \"Recall\": recall, \"Precision\": precision}, ignore_index=True)\n\nmodel_performance","8fe62051":"my_reviews = {'Review': [\"I highly recommend the restaurant.\",\"I will never go back!!\", \n                        \"Disgusting food and poor service.\",\"Lovely evening and delicious dessert.\"]}\nmy_reviewsDF = pd.DataFrame.from_dict(my_reviews)\nmy_reviewsDF","97eb39ef":"corpus=[]\n\nfor review in my_reviewsDF['Review']:\n    review = review.lower()\n    review = re.sub(\"[^a-zA-Z]\", \" \", review)\n    review = nltk.word_tokenize(review)\n    review = [word for word in review if word.lower() not in stopwords]\n    lemma = WordNetLemmatizer()\n    review = [lemma.lemmatize(word) for word in review]\n    review = \" \".join(review) # Building again the dataframe\n    corpus.append(review)\n    \nmy_reviewsDF[\"Review\"] = corpus\n\nbag_of_words = cv.transform(corpus).toarray() # Using the same CV as before!!","7645fa00":"LR = LogisticRegression()\nLR.fit(X_train, y_train)\n\nLabel = LR.predict(bag_of_words)\n\nLabel = pd.DataFrame({'Label':Label})\njoined = my_reviewsDF.join(Label)\njoined","1e8c8e68":"def unigram(corpus, n=None):\n    cv = CountVectorizer().fit(corpus)\n    bag_of_words = cv.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef bigram(corpus, n=None):\n    cv = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = cv.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n\ndef trigram(corpus, n=None):\n    cv = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = cv.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","c1e8b898":"positive = dataset[dataset[\"Liked\"] == 1][[\"Review\", \"Liked\"]]","c80d5125":"pos_uni = unigram(positive['Review'], 20)\ntemp = pd.DataFrame(pos_uni, columns = ['words' ,'count'])\nfig = px.bar(temp, x = 'words', y = 'count', color = 'words', title='Top 20 unigrams in positive reviews')        \nfig.show()","9e294a29":"pos_bi = bigram(positive['Review'], 20)\ntemp = pd.DataFrame(pos_bi, columns = ['words' ,'count'])\nfig = px.bar(temp, x = 'words', y = 'count', color = 'words', title='Top 20 bigrams in positive reviews')        \nfig.show()","c0657e33":"pos_tri = trigram(positive['Review'], 20)\ntemp = pd.DataFrame(pos_tri, columns = ['words' ,'count'])\nfig = px.bar(temp, x = 'words', y = 'count', color = 'words', title='Top 20 trigrams in positive reviews')        \nfig.show()","c5f97d3e":"negative = dataset[dataset[\"Liked\"] == 0][[\"Review\", \"Liked\"]]\nneg_uni = unigram(negative['Review'], 20)\ntemp = pd.DataFrame(neg_uni, columns = ['words' ,'count'])","64d2a581":"fig = go.Figure(data =[go.Bar(x = temp['words'].tolist(), y= temp['count'].tolist())])\nfig.update_traces(marker_color='rgb(0,0,139)', marker_line_color='rgb(8,48,107)',\n                  marker_line_width=1.5, opacity=0.6)\nfig.update_layout(title_text='Top 20 unigrams in negative reviews')\nfig.show()","29e61e0a":"neg_bi = bigram(negative['Review'], 20)\ntemp = pd.DataFrame(neg_bi, columns = ['words' ,'count'])\n\nfig = go.Figure(data =[go.Bar(x = temp['words'].tolist(), y= temp['count'].tolist())])\nfig.update_traces(marker_color='rgb(0,0,139)', marker_line_color='rgb(8,48,107)',\n                  marker_line_width=1.5, opacity=0.6)\nfig.update_layout(title_text='Top 20 bigrams in negative reviews')\nfig.show()","77ef1574":"neg_tri = trigram(negative['Review'], 20)\ntemp = pd.DataFrame(neg_tri, columns = ['words' ,'count'])\n\nfig = go.Figure(data =[go.Bar(x = temp['words'].tolist(), y= temp['count'].tolist())])\nfig.update_traces(marker_color='rgb(0,0,139)', marker_line_color='rgb(8,48,107)',\n                  marker_line_width=1.5, opacity=0.6)\nfig.update_layout(title_text='Top 20 trigrams in negative reviews')\nfig.show()","923a70df":"!pip install NRCLex","79b6fa6b":"from nrclex import NRCLex","e8537268":"text_object = NRCLex(' '.join(dataset['Review']))\n\nEmotionDF = pd.DataFrame.from_dict(text_object.affect_frequencies, orient='index').sort_values(by=0, ascending=False).reset_index()\nEmotionDF.columns = ['Emotion', 'Frequency']","4a01ab74":"fig = px.pie(EmotionDF, values = 'Frequency', names='Emotion',\n             title='Emotion Frequency',\n             hover_data=['Emotion'], labels={'Emotion':'Emotion'})\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()","3d48c8a7":"## What we'll be doing ","f3a54fdb":"Stopwords are the words in any language which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence such as the, is, at, which, and on.\n\nOn removing stopwords, dataset size decreases, and the time to train the model also decreases without a huge impact on the accuracy of the model. Stopword removal can potentially help in improving performance.","7d1724f3":"## Tokenization","363da7cf":"![image.png](attachment:image.png)","929bbc41":"![image.png](attachment:image.png)","28962f9f":"Tokenization is the task of chopping the sentence up into pieces, called tokens. It\u2019s a fundamental step in both traditional NLP methods like Count Vectorizer and Advanced Deep Learning-based architectures.\n\nTokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types \u2013 word, character, and subword (n-gram characters) tokenization.","393b776a":"The frequencies of the various possible n-grams are characteristic of languages. Studying them will help us know with combinations of words or expressions are more commonly used for positive and negative reviews.","46f4878d":"Now it's the time to predict the label for reviews! I'll try many different classifiers.","537d4f0e":"## Negative reviews","9a18488a":"**Stemming** is the process of reducing inflection in words to their root forms. This algorithm may result in words that are not actual words in the language. Stems are created by removing the suffixes or prefixes used with a word.","8777d62b":"## Data cleaning","0323f1cb":"The datasets comes in a .tsv files, this means is a tab-separated values (TSV) file. As in text we usually have commas all over, different columns can be better separated with tabs. Read csv will work with this files, but we have to clarify the column delimiter. Quoting number refers to how the quotes inside the reviews are delimited (\" \"), this will ensure the algorithm ignores them.","ae552b9b":"I'll lowercase the sentence before starting the process, and get rid of any commas and any other non-character symbols. In this case I'll also get rid of numbers, keeping only words from a to z.","d935244d":"## Unigrams, bigrams and trigrams","08466d34":"## Emotions on reviews","1ac00de7":"First we'll be doing the process for only one review, and then generalize it to the whole set.","a8807aa6":"## Checking out the classification model with custom reviews","9c84d613":"## Review labeling predictions!!","77d447fd":"Languages we speak and write are made up of several words often derived from one another. When a language contains words that are derived from another word as their use in the speech changes is called Inflected Language.\n\nThe degree of inflection may be higher or lower in a language.","91c0f836":"**Sentiment analysis:**\n\nSentiment Analysis (also known as opinion mining or emotion AI) is a sub-field of NLP that tries to identify and extract opinions within a given text across blogs, reviews, social media, forums, news etc. Sentiment Analysis can help craft all this exponentially growing unstructured text into structured data using NLP and open source tools. This is what we are going to do here.","d22e9ff0":"**Lemmatization** unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. In general, lemmatization offers better precision than stemming. The con is that is slower than stemming","6374e4c7":"We tried different classifiers to see which one performs better. In this case we would keep logistic regression or Supported Vector Classifier.","15e3ae63":"**We have 1000 reviews with their labels**","f073639a":"Let's see the most used words in positive reviews!","7009296d":"![image.png](attachment:image.png)","441d74ae":"## Sentiment analysis with NRCLex","ccef8117":"Natural Language Processing (NLP) is a type of computational linguistics that uses machine learning algorithms to understand how us people communicate. This includes voice assistance, chat bots, sentiment analysis and many more.\n\nWith NLP, computers can understand human speech in text and written form without the need of someone to structure their conversation in any way.\n","2b940f1d":"As we are working with text, the data cleaning and preprocessing step are different as other algorithms like classification or regression supervised ones.\n\nWe'll be doing the processing steps for one review and then replicate it for all the reviews in the set.","a9ad3ace":"**Wordcloud**","1448b421":"## Bag of words","af6a4786":"The ML moodel needs the text to be transformed before working with it, so now we will make numbers for it to understand.\n\n**Bag of words** is a representation of text that describes the occurrence of words within a text. We need to have a vocabulary of known words and a way to measure the presence of these known words. Any information about the order or structure of words in the text is discarded. \n\nThe model is only concerned with whether known words occur in the text. We will do this with CountVectorizer.","a386ac92":"## Vectorizing","9ec2008e":"## Building the steps together","65305054":"Now that we have our sparsed matrix in which each row represents a review and each column a different word, we will have a 1 in a cell if that word is inside that review.","0279b995":"In this kernel, the main steps for any NLP problem will be described, as well as their lines of code. This will help further analysis and will be a good starting point to build more complex models.\n\nIn the dataset of this exercise we have 1000 reviews from a restaurant, labeled as **positive reviews (1) or negative ones (0).**\n\nWe will be building a model that may predict, given the words used in the review, a label for each of the reviews in the test set. ","c422855a":"## Text Stemming \/ Lemmatization","572e5e1a":"Now let's see what happens in the negative reviews","da4e19d1":"## Natural Language Processing","4d89382c":"![image.png](attachment:image.png)","0a8fc205":"## Stopwords","a1750645":"![image.png](attachment:image.png)","53f34d81":"## Visualizations on the dataset","e20ff326":"From https:\/\/pypi.org\/project\/NRCLex\/: \n\nNRCLex will measure emotional affect from a body of text. Affect dictionary contains approximately 27,000 words, and is based on the National Research Council Canada (NRC) affect lexicon.\n\nEmotional affects measured include the following: fear, anger, anticipation, trust, surprise, positive, negative, sadness, disgust, joy.","a9b63899":"**Importing main ML and visualization libraries and modules**","38a31c2b":"## Positive reviews","0368123c":"## Lowering case and cleaning non-characters."}}