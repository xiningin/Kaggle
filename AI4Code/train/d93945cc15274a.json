{"cell_type":{"e974a013":"code","0a67f1f7":"code","4562a4cc":"code","f3dfd04d":"code","f20ba281":"code","fcf2116b":"code","0204affa":"code","6b9a430e":"code","e47e32ba":"code","72195948":"code","4cad2a51":"code","1ad72733":"code","375dec38":"code","b7fbf5d1":"code","d12fef86":"code","88ffc425":"code","9d567895":"code","a8f20b36":"code","901febf4":"code","b2ea4f3f":"code","b6e30596":"code","1bb450f7":"code","1da798de":"code","95ff5dc9":"code","20a1eff4":"code","c234172d":"code","fbd84b90":"code","88f43399":"code","6a056c6b":"code","ced5d359":"code","27b80ab8":"code","40db2671":"code","30824bf7":"markdown","f0315f0a":"markdown","76fe7947":"markdown","a71c6487":"markdown","0eab84de":"markdown","e54e062f":"markdown","fc2296d7":"markdown","a078e84c":"markdown","89f65732":"markdown","28f92c23":"markdown","9b3c0922":"markdown","f45d8a3e":"markdown","ff326028":"markdown","c65d9d49":"markdown","90227511":"markdown","cbc4b369":"markdown","3fd21d8f":"markdown","c113de6a":"markdown"},"source":{"e974a013":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0a67f1f7":"# Numpy and Pandas for statistics and DataSet\nimport numpy as np\nimport pandas as pd\n\n# Seaborn and matplotlib for Graphs and Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.express as px\nimport matplotlib\n\n# Encoder for categorical Columns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Libraries for Machine Learning Algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Warning Library to avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')","4562a4cc":"df = pd.read_csv(\"\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")\ndf.head()","f3dfd04d":"df.shape","f20ba281":"df.columns","fcf2116b":"df.isnull().sum()","0204affa":"df.info()","6b9a430e":"for col in df.columns:\n    if df[col].dtypes == 'object':\n        print(\"{} have {} unique values: \".format(col, df[col].nunique()))\nprint(\"*\" * 35)\nfor col in df.columns:\n    if df[col].nunique() <= 16:\n        print(\"{}: {}\".format(col, df[col].unique()))","e47e32ba":"# Fill Missing value in Numerical columns with mean functions\ndf['MinTemp']=df['MinTemp'].fillna(df['MinTemp'].mean())\ndf['MaxTemp']=df['MaxTemp'].fillna(df['MaxTemp'].mean())\ndf['Rainfall']=df['Rainfall'].fillna(df['Rainfall'].mean())\ndf['Evaporation']=df['Evaporation'].fillna(df['Evaporation'].mean())\ndf['Sunshine']=df['Sunshine'].fillna(df['Sunshine'].mean())\ndf['WindGustSpeed']=df['WindGustSpeed'].fillna(df['WindGustSpeed'].mean())\ndf['WindSpeed9am']=df['WindSpeed9am'].fillna(df['WindSpeed9am'].mean())\ndf['WindSpeed3pm']=df['WindSpeed3pm'].fillna(df['WindSpeed3pm'].mean())\ndf['Humidity9am']=df['Humidity9am'].fillna(df['Humidity9am'].mean())\ndf['Humidity3pm']=df['Humidity3pm'].fillna(df['Humidity3pm'].mean())\ndf['Pressure9am']=df['Pressure9am'].fillna(df['Pressure9am'].mean())\ndf['Pressure3pm']=df['Pressure3pm'].fillna(df['Pressure3pm'].mean())\ndf['Cloud9am']=df['Cloud9am'].fillna(df['Cloud9am'].mean())\ndf['Cloud3pm']=df['Cloud3pm'].fillna(df['Cloud3pm'].mean())\ndf['Temp9am']=df['Temp9am'].fillna(df['Temp9am'].mean())\ndf['Temp3pm']=df['Temp3pm'].fillna(df['Temp3pm'].mean())\nprint(\"Process Completed\")","72195948":"df.isnull().sum()","4cad2a51":"df['RainToday']=df['RainToday'].fillna(df['RainToday'].mode()[0])\ndf['RainTomorrow']=df['RainTomorrow'].fillna(df['RainTomorrow'].mode()[0])\n\ndf['WindDir9am'] = df['WindDir9am'].fillna(df['WindDir9am'].mode()[0])\ndf['WindGustDir'] = df['WindGustDir'].fillna(df['WindGustDir'].mode()[0])\ndf['WindDir3pm'] = df['WindDir3pm'].fillna(df['WindDir3pm'].mode()[0])\nprint(\"Process Completed\")","1ad72733":"df.isnull().sum()","375dec38":"sns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (10, 6)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'","b7fbf5d1":"px.histogram(df, x='Location', \n             title='Location vs. Rainy Days', \n             color='RainToday')","d12fef86":"sns.kdeplot(data=df, x=\"Temp9am\", hue=\"RainToday\", multiple=\"stack\")","88ffc425":"f, axs = plt.subplots(1, 2, figsize=(8, 4), gridspec_kw=dict(width_ratios=[4, 3]))\nsns.scatterplot(data=df, x=\"MinTemp\", y=\"MaxTemp\", hue=\"RainToday\", ax=axs[0])\nsns.histplot(data=df, x=\"RainToday\", hue=\"RainToday\", shrink=.8, alpha=.9, legend=False, ax=axs[1])\nf.tight_layout()","9d567895":"data = df.apply(lambda x: x.factorize()[0]).corr(method='pearson')\nplt.figure(figsize=(15,11))\nsns.heatmap(data, linecolor='white',linewidths=1, cmap=\"YlGnBu\", annot=True)\nplt.title('Rain in Australia Feature Correlation', size=30)\nfigure = plt.gcf()\nfigure.set_size_inches(20, 20)\nplt.show()","a8f20b36":"sns.set(style='whitegrid')\nplt.figure(figsize=(14, 7))\nlabels=['No', 'Yes']\nplt.pie(df['RainToday'].value_counts(),labels=labels,explode=[0.1,0.1],\n        autopct='%1.2f%%',colors=['#E37383','#FFC0CB'], startangle=90)\nplt.title('Rain')\nplt.axis('equal')\nplt.show()","901febf4":"plt.figure(figsize=(14,8))\nplt.title('Correlation Analysis',color='Red',fontsize=20,pad=40)\n\ncorr = df.corr()\nmask = np.triu(np.ones_like(corr,dtype = bool))\nsns.heatmap(df.corr(),mask=mask,annot=True,linewidths=.5);\nplt.xticks(rotation=60)\nplt.yticks(rotation = 60)\nplt.show()","b2ea4f3f":"sns.set_palette(\"plasma\")\nsns.jointplot(data=df, x=\"Temp9am\", y=\"Temp3pm\", kind=\"hist\")\nplt.title('Tempeture at 9am and Tempeture 3pm Relationship', fontsize=15, fontweight='bold',y=1.3,loc=\"right\")\nplt.show()","b6e30596":"sns.set_context(\"notebook\")\nsns.set_palette(\"pastel\")\ng = sns.JointGrid(data=df, x=\"MinTemp\", y=\"MaxTemp\")\ng.plot(sns.regplot, sns.boxplot)\nplt.title('Minimun Tempreture and Maximum Tempeture Relationship', fontsize=15, fontweight='bold',y=1.3,loc=\"right\")\nplt.show()","1bb450f7":"X = df.drop(['RainTomorrow', 'Date'], axis=1) # Features\ny = df['RainTomorrow'] # Label\n\nX.shape, y.shape","1da798de":"X_enc = X.copy()\ny_enc = y.copy()\nfor col in X.columns:\n    if X[col].dtypes == 'object':\n        lb = LabelEncoder()\n        X_enc[col] = lb.fit_transform(X[col].values)\n        y_enc = lb.fit_transform(df['RainTomorrow'])\n        \n\nX_enc.head()","95ff5dc9":"X_train, X_test, y_train, y_test = train_test_split(X_enc, y_enc, test_size=0.3, random_state=1)","20a1eff4":"import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\n\ndef dt_classifer(X, y):\n    list = []\n    # Split data in train_test_split\n    print(\"First we split data with train_test_split and kfolds: \")\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1)\n    # Find best parameters for decision tree\n    print(\"Lets findout best parameters for decision tree: \")\n    dt_classifier = DecisionTreeClassifier()\n    param_dist = {\"max_depth\": [3, 10],\n              \"random_state\": [3, 11],\n              \"max_features\": range(1, 11),\n            \"splitter\": [\"best\", \"random\"], \n              \"min_samples_split\": range(2, 11),\n              \"criterion\": [\"gini\", \"entropy\"]}\n    random_search = RandomizedSearchCV(dt_classifier, param_distributions=param_dist, n_iter=10, cv=5)\n    random_search.fit(X_train, y_train)\n    print(\"best parameters: \", random_search.best_params_)\n    print(\"best parameters score: \", random_search.best_score_ * 100)\n    # Save the dict in list\n    for v in random_search.best_params_.values():\n        list.append(v)\n    \n    # Give parameters to Decision tree with train_test_split\n    print(\"Now we use this parameters in our model: \")\n    dt_classifier = DecisionTreeClassifier(splitter= list[0], random_state=list[1], min_samples_split=list[2], max_features=list[3], max_depth=list[4], criterion=list[5])\n    dt_classifier.fit(X_train, y_train)\n    y_pred_dt = dt_classifier.predict(X_test)\n    dt_test_accuracy = accuracy_score(y_test, y_pred_dt)\n    dt_train_acc = cross_val_score(dt_classifier, X_train, y_train)\n    print(\"Model Accuracy with train_test_split: \", dt_test_accuracy * 100)\n    \n    # Give same parameters to kfold splitting\n    kfolds = KFold(n_splits=10, random_state=7, shuffle=True)\n    model = DecisionTreeClassifier(splitter= list[0], random_state=list[1], min_samples_split=list[2], max_features=list[3], max_depth=list[4], criterion=list[5])\n    score = cross_val_score(model, X, y, cv=kfolds)\n    print(\"Accuracy of model with Kfolds: \",(score.mean()*100))\n    \n\ndt_classifer(X_enc, y_enc)","c234172d":"xgb_classifier = XGBClassifier(random_state=42, verbosity = 0)\nxgb_classifier.fit(X_train, y_train)\ny_pred_xgb = xgb_classifier.predict(X_test)\nxgb_accuracy = accuracy_score(y_test, y_pred_xgb)\nxgb_train_acc = cross_val_score(xgb_classifier, X_train, y_train)","fbd84b90":"random_forest = RandomForestClassifier(max_depth=5)\nrandom_forest.fit(X_train, y_train)\nrandom_pred = random_forest.predict(X_test)\nrandom_test_score = accuracy_score(y_test, random_pred)\nacc_random = cross_val_score(random_forest, X_train, y_train, cv=5)","88f43399":"random_forest.feature_importances_\nfeature_importance = pd.DataFrame({'importance': random_forest.feature_importances_}, index= X_enc.columns).sort_values('importance')\nfeature_importance","6a056c6b":"feature_importance.plot.barh()","ced5d359":"feature_importance[feature_importance.importance > 0.04]","27b80ab8":"rf_columns = df.loc[:, ('WindGustSpeed', 'Pressure3pm', 'Humidity9am', 'Cloud3pm', 'Sunshine', 'Rainfall', 'RainToday', 'Humidity3pm', 'Pressure9am')]\nlb = LabelEncoder()\nx_enc = lb.fit_transform(rf_columns['RainToday'].values)\nx_enc\nrf_columns['RainToday1'] = x_enc\nX = rf_columns.drop('RainToday', axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.3, random_state=1)\nrandom_forest = RandomForestClassifier(max_depth=5)\nrandom_forest.fit(X_train, y_train)\nrandom_pred = random_forest.predict(X_test)\nrandom_test_scores = accuracy_score(y_test, random_pred)\nacc_randomm = cross_val_score(random_forest, X_train, y_train, cv=5)","40db2671":"results = pd.DataFrame({\n    'Model': ['Random Forest', 'XGBoost'],\n    'Train Score': [acc_randomm.mean(), xgb_train_acc.mean()],\n    'Test Score': [random_test_scores, xgb_accuracy]          \n              })\nresult_df = results.sort_values(by='Train Score', ascending=False)\nresult_df = result_df.set_index('Model')\nresult_df","30824bf7":"# Train Model","f0315f0a":"**Decision Tree Classifier**","76fe7947":"**Thank you so much to visit, If you like my work please upvote and Comment too,**\n***I WILL UPDATE THIS NOTEBOOK REGULARY***","a71c6487":"# Categorical to Numerical\n**Change Categorical columns into Numerical values with the help of Label Encoder**","0eab84de":"# Import Helpful And Important Libraries","e54e062f":"**XGBoost Classifier**","fc2296d7":"* **Data set have 1,45,460 Rows and 23 Columns**\n* **In Columns 16 have Numerical Values and 7 have Categorical Values**\n* **Only Date and Location haven't Missing Values**\n","a078e84c":"**Hey Friends, Thanks for Coming on my page.\nToday we are going to analysis RainFall in Australia.\nHope you enjoy and feel free to comment my mistake.**","89f65732":"**Columns Unique Values**","28f92c23":"# Fill Missing Values in Numerical columns\n**Select Mean method to fill numerical missing values, First find mean value of each columns and fill that value in empty Columns**","9b3c0922":"* **In Scatter Plot, for rain the Mintemp is between -0 to 25 and for MaxTemp is between -10 to 30**\n* **In Count Plot, No have more counts then Yess**","f45d8a3e":"# Now we fill Categorical columns\n**Will Use Mode Method**","ff326028":"# **Explore Data**","c65d9d49":"# Exploratory Data Analysis","90227511":"# **Load Data**","cbc4b369":"**Here we see more rain in \"Portland\" with a count of 1094 times and less rain in \"Canberra\" with a count of 2807**","3fd21d8f":"**Random Forest**","c113de6a":"**Lets Find best Feature for random forest**"}}