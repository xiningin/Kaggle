{"cell_type":{"f0d7fb41":"code","99151c22":"code","d6706f99":"code","dde108e2":"code","364160e8":"code","81f0f13a":"code","ec8ad02b":"code","826da0d4":"code","618751ff":"code","0e5b5ce8":"code","87f15f0e":"code","3fc35066":"code","6da26db0":"code","e667d864":"code","0d0f68cf":"markdown","f8b50994":"markdown","8aeeaa05":"markdown","e8a756d5":"markdown","2e60e471":"markdown","dff23a46":"markdown"},"source":{"f0d7fb41":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport spacy \nnlp = spacy.load('en', parse=True, tag=True, entity=True)\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport sqlite3\nimport os\nimport re\ntokenizer = ToktokTokenizer()\nstopword_list = nltk.corpus.stopwords.words('english')\nstopword_list.remove('no')\nstopword_list.remove('not')\nimport unicodedata\n\n# Any results you write to the current directory are saved as output.","99151c22":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nsql_conn = sqlite3.connect('..\/input\/database.sqlite')\ncorpus = pd.read_sql(\"SELECT author, body, ups, downs, controversiality,removal_reason,score,score_hidden FROM May2015 where subreddit = 'psychology' and LENGTH(body) > 30 AND LENGTH(body) < 250 LIMIT 10000\", sql_conn)\n\ncorpus.to_csv('psychology.csv', index=False)\ncorpus.head()","d6706f99":"## Step 1: Text Preprocessing\ndef remove_special_char(text):\n    #replace special characters with ''\n    text = re.sub('[^\\w\\s]', '', text)\n    #remove chinese character\n    text = re.sub(r'[^\\x00-\\x7f]',r'', text)\n    #remove numbers\n    text = re.sub('\\d+', '', text)\n    text = re.sub('_', '', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip()\n    #text = text.lower()\n    #remove accented characters\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n    \ndef remove_stopwords(text):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    text = [token for token in tokens if token.lower() not in stopword_list]\n    return \" \".join(text)\n\ndef stem_text(text):\n    stemmer = nltk.porter.PorterStemmer()\n    text = [stemmer.stem(word) for word in text.split()]\n    return \" \".join(text)\n\ndef lemmatize_text(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text\n","dde108e2":"def normalize_corpus(corpus):\n    \n    normalized_corpus = []\n    # normalize each document in the corpus\n    for doc in corpus:\n        # remove special character and normalize docs\n        doc = remove_special_char(doc)\n        # remove stopwards \n        doc = remove_stopwords(doc)\n        # lemmatize docs    \n        doc = lemmatize_text(doc)\n        normalized_corpus.append(doc)\n        \n    return normalized_corpus","364160e8":"normalized_corpus = normalize_corpus(corpus['body'])\nnormalized_corpus[0]","81f0f13a":"from textblob import TextBlob\n\n# compute sentiment scores (polarity) and labels\nsentiment_scores_tb = [round(TextBlob(article).sentiment.polarity, 3) for article in normalized_corpus]\nsentiment_category_tb = ['positive' if score > 0 \n                             else 'negative' if score < 0 \n                                 else 'neutral' \n                                     for score in sentiment_scores_tb]\n\n\n# sentiment statistics per news category\ndf_revised = pd.DataFrame([normalized_corpus, sentiment_scores_tb, sentiment_category_tb]).T\ndf_revised.columns = ['body', 'sentiment_score', 'sentiment_category']\ndf_revised['sentiment_score'] = df_revised.sentiment_score.astype('float')\n\ndf_revised.head()","ec8ad02b":"sentiment_spread = (df_revised.groupby(by=['sentiment_category'])\n                           .size()\n                           .reset_index().rename(columns={0 : 'Frequency'}))\nprint(sentiment_spread)","826da0d4":"from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\n\ninit_notebook_mode(connected=True) #do not miss this line\n\ndata_bar = [go.Bar(\n            x=sentiment_spread['sentiment_category'],\n            y=sentiment_spread['Frequency']\n    )]\nlayout = go.Layout(\n    autosize=False,\n    width=500,\n    height=500\n)\n\nfig = go.Figure(data=data_bar, layout = layout)\npy.offline.iplot(fig, filename='basic-bar')","618751ff":"from spacy import displacy\n\ndf_revised['entity_type'] = None\ndf_revised['named_entity'] = None\n\ndef ner_tagging(corpus):\n    for doc in corpus['body']:\n        temp_entity_name=''\n        temp_named_entity =''\n        sentence_nlp = nlp(doc)\n        for word in sentence_nlp:\n            if word.ent_type_:\n                temp_entity_name = ' '.join([temp_entity_name,word.ent_type_]).strip()\n                temp_named_entity = ' '.join([temp_named_entity,word.text]).strip()\n        corpus.loc[corpus['body']== doc,['entity_type']]=temp_entity_name\n        corpus.loc[corpus['body']== doc,['named_entity']]=temp_named_entity\n    return corpus\n\n# print named entities in article\n#print([(word, word.ent_type_) for word in sentence_nlp if word.ent_type_])\n\n# visualize named entities\n#displacy.render(sentence_nlp, style='ent', jupyter=True)\ndf_with_NER = ner_tagging(df_revised)\ndf_with_NER.head()","0e5b5ce8":"named_entity_positive = []\nnamed_entity_negative = []\nnamed_entity_neutral = []\n\nfor index, row in df_with_NER.iterrows():\n    temp = row['named_entity'].split()\n    if row['sentiment_category']=='positive':\n        named_entity_positive.extend(temp)\n    if row['sentiment_category']=='negative':\n        named_entity_negative.extend(temp)\n    if row['sentiment_category']=='neutral':\n        named_entity_neutral.extend(temp)\n\ntemp_pos = pd.DataFrame([named_entity_positive, ['positive']*len(named_entity_positive)]).T\ntemp_neg = pd.DataFrame([named_entity_negative, ['negative']*len(named_entity_negative)]).T\ntemp_neu = pd.DataFrame([named_entity_neutral, ['neutral']*len(named_entity_neutral)]).T\n\nnamed_entity_df = pd.concat([temp_pos,temp_neg,temp_neu])\nnamed_entity_df.columns = ['entity', 'sentiment']","87f15f0e":"named_entity_df.head()","3fc35066":"entity_type_positive = []\nentity_type_negative = []\nentity_type_neutral = []\n\nfor index, row in df_with_NER.iterrows():\n    temp = row['entity_type'].split()\n    if row['sentiment_category']=='positive':\n        entity_type_positive.extend(temp)\n    if row['sentiment_category']=='negative':\n        entity_type_negative.extend(temp)\n    if row['sentiment_category']=='neutral':\n        entity_type_neutral.extend(temp)\n\ntemp_pos = pd.DataFrame([entity_type_positive, ['positive']*len(entity_type_positive)]).T\ntemp_neg = pd.DataFrame([entity_type_negative, ['negative']*len(entity_type_negative)]).T\ntemp_neu = pd.DataFrame([entity_type_neutral, ['neutral']*len(entity_type_neutral)]).T\n\nentity_type_df = pd.concat([temp_pos,temp_neg,temp_neu])\nentity_type_df.columns = ['type', 'sentiment']\nentity_type_df.head()","6da26db0":"#from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n% matplotlib inline\n\ndef display_wordcloud(data):\n    d = {}\n    for a, x in data.values:\n        d[a] = x\n    wordcloud=WordCloud(background_color=\"white\").generate_from_frequencies(frequencies=d)\n    return wordcloud\n\nfig = plt.figure(figsize=(30,20))\ni=0\nfor item in named_entity_df.sentiment.unique():\n    ax = fig.add_subplot(1,3,i+1)\n    temp = named_entity_df[named_entity_df['sentiment']==item]['entity']\n    temp_freq=temp.value_counts()\n    data_freq = pd.DataFrame({'entity':temp_freq.index, 'Frequency':temp_freq.values})\n    wordcloud = display_wordcloud(data_freq)\n    ax.imshow(wordcloud)\n    ax.axis('off')\n    i=i+1\nplt.show() \n\n\n","e667d864":"type_freq= (entity_type_df.groupby(by=['type']).size().reset_index().sort_values(0, ascending=False).rename(columns={0 : 'Frequency'}))\nx_pos = type_freq['Frequency']\ny_pos = type_freq['type']\n\nfig_bar, ax = plt.subplots()\nax.barh(y_pos,x_pos, color='green')\nplt.ylabel(\"Entity Type\")\nplt.xlabel(\"Frequency\")\nplt.title(\"Frequency of Entity Types\")\nplt.show()\n\n\n","0d0f68cf":"**Let's visualize the spread of sentiment score and category**","f8b50994":"As we can see the dataset is quite balanced across all 3 classes of sentiments","8aeeaa05":"As we can see the wordclouds show that the most frequently referenced named entities are pretty much the same across all 3 sentiment categories i.e China, US, America stand out in all 3. We also have high references to some date indicators like One, Year, Two etc. Let us now plot the frequency of various entity types in the corpus.","e8a756d5":"**Visualize the distribution of entity names and entity types along with sentiments attached to them**","2e60e471":"**Named Entity Recognition**","dff23a46":"**Let us analyze the sentiments in these comments **"}}