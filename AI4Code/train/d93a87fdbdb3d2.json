{"cell_type":{"4d618859":"code","7bc09bac":"code","6e5b32d1":"code","e02f4197":"code","d865b66e":"code","aafbeebc":"code","fbf31d54":"code","b38cb87c":"code","95c970d7":"code","3b5b895a":"code","8b3f838e":"code","cee818e8":"code","d184e961":"code","5ee2c1c2":"code","5fc4f911":"code","e1e4ac43":"code","65f42b4b":"code","44c761cc":"code","3caa438b":"code","745270e1":"code","06035b56":"code","f220f38e":"code","afa2a977":"code","cd6ea9d9":"code","bbaa4c08":"code","882bf19f":"code","fd147fd5":"markdown","16f5624c":"markdown","47a2e535":"markdown","689a3c9c":"markdown","197e9718":"markdown","b72baea2":"markdown","cef0b320":"markdown","6514fea5":"markdown","4b03ea89":"markdown","50898058":"markdown","2ad31836":"markdown","a81241c3":"markdown"},"source":{"4d618859":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport re\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7bc09bac":"train_df = pd.read_csv('..\/input\/fake-news\/train.csv')\ntest_df = pd.read_csv('..\/input\/fake-news\/test.csv')\n\nprint('Number of train: {}'.format(train_df.shape))\nprint('Number of test: {}'.format(test_df.shape))","6e5b32d1":"train_df.head(5)","e02f4197":"test_df.head(5)","d865b66e":"train_df = train_df.drop(['id'], axis=1)\ntest_df = test_df.drop(['id'], axis=1)","aafbeebc":"label = train_df['label'].values\ntrain_df = train_df.drop(['label'], axis=1)","fbf31d54":"df = pd.concat([train_df, test_df], axis=0)\nprint('Number of total data: {}'.format(df.shape))\ndf.head(5)","b38cb87c":"df.isnull().sum()","95c970d7":"df.text = df.text.fillna('no data')\ndf['author']= df['author'].fillna('unknown') # different expression of dataframe\ndf['title'] = df['title'].fillna(df['text'])\ndf.isnull().sum()","3b5b895a":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom wordcloud import WordCloud\nnltk.download('stopwords')","8b3f838e":"stemmer = LancasterStemmer()\nstop_words = stopwords.words('english')\nlemmatizer = WordNetLemmatizer()","cee818e8":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","d184e961":"df['text'] = df['text'].apply(lambda x: remove_URL(x))\ndf['text'] = df['text'].apply(lambda x: remove_html(x))\ndf['text'] = df['text'].apply(lambda x: remove_emoji(x))","5ee2c1c2":"def ntlk_process(text):\n    text = text.lower()\n    \n    text = re.sub(r'[!]+', '!', text)\n    text = re.sub(r'[?]+', '?', text)\n    text = re.sub(r'[.]+', '.', text)\n    text = re.sub(r\"'\", \"\", text)\n    text = re.sub(r\"'s\", \"\", text)\n    text = re.sub('\\s+', ' ', text).strip()  # Remove and double spaces\n    text = re.sub(r'&amp;?', r'and', text)  # replace & -> and\n    text = re.sub(r'[:\"$%&\\*+,-\/:;<=>@\\\\^_`{|}~]+', '', text)\n    \n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            tokens.append(lemmatizer.lemmatize(token))\n    return \" \".join(tokens)","5fc4f911":"df['text'] = df['text'].apply(lambda x: ntlk_process(x))","e1e4ac43":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nwordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=800, height=600)\nwordcloud = wordcloud.generate(' '.join(df['text']))\n\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","65f42b4b":"train_X = df['text'][:train_df.shape[0]] + df['author'][:train_df.shape[0]]\ntest_X = df['text'][train_df.shape[0]:] + df['author'][train_df.shape[0]:]\ntrain_y = label\nprint('Number of Train X: {}'.format(train_X.shape))\nprint('Shape of Label: {}'.format(train_y.shape))\nprint('Number of Test X: {}'.format(test_X.shape))","44c761cc":"train_X.head()","3caa438b":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size = 0.4, random_state=42)\nprint('Shape of train: {}'.format(X_train.shape))\nprint('Shape of valid: {}'.format(X_val.shape))","745270e1":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D, BatchNormalization, Embedding\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau","06035b56":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nvoc_size = len(tokenizer.word_index) + 1\nprint('vocabulary size: {}'.format(voc_size))","f220f38e":"X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen = 256)\nX_val = pad_sequences(tokenizer.texts_to_sequences(X_val), maxlen=256)\ntest_X = pad_sequences(tokenizer.texts_to_sequences(test_X), maxlen=256)\n\nprint('Shape of train: {}'.format(X_train.shape))\nprint('Shape of label of train: {}'.format(y_train.shape))\nprint('Shape of validation: {}'.format(X_val.shape))\nprint('Shape of label of val: {}'.format(y_val.shape))","afa2a977":"inputs = Input(shape=(256,), dtype='int32')\n\nmodel = Sequential(\n    [Embedding(voc_size, 500),\n    SpatialDropout1D(0.2),\n    LSTM(256),\n    Dropout(0.3),\n    Dense(128, activation = 'relu'),\n    Dropout(0.3),\n    Dense(1, activation = 'sigmoid'),]\n)\n\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","cd6ea9d9":"model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs=10, batch_size=64)","bbaa4c08":"y_pred = model.predict_classes(test_X)\ntype(y_pred)","882bf19f":"submission = pd.read_csv('..\/input\/fake-news\/submit.csv')\nsubmission['label'] = y_pred.astype(int)\nsubmission.to_csv('submission.csv',index=False)","fd147fd5":"#### Word Cloud","16f5624c":"### Delete useless words using <code>nltk<\/code>","47a2e535":"#### Combine train and test data","689a3c9c":"#### Tokenize","197e9718":"#### Erase unnecessary data, <code>id<\/code>","b72baea2":"#### Split data and label","cef0b320":"#### Modeling","6514fea5":"> **Stemming**\n\nStemming means elimination of affix.\nInstead of saving all shape of words, a stemmed word can reduce the size of index and increase the accuracy.\n\n<code>PorterStemmer<\/code><br>\nIt eliminates the suffix.<br>\n\n<code>LancasterStemmer<\/code><br>\nIt is similar to <code>ProterStemmer<\/code>, but the performance is better.\n* presumably -> presum\n* cement -> cem\n* owed -> ow\n* ear -> ear\n\n<code>SnowballStemmer<\/code><br>\nIt supports for the 13 languages except for English.\n* Autobahnen -> autobahn<\/n>\n\n<code>WordNetLemmatizer<\/code>\n* dogs -> dog\n* churches -> church\n* abaci -> abacus","4b03ea89":"#### Find missing data","50898058":"## Load Dataset and Preprocessing","2ad31836":"## Build model","a81241c3":"#### Split train and test dataset"}}