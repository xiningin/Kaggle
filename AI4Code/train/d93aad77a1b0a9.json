{"cell_type":{"ca1d5d46":"code","334b9ae2":"code","4f623607":"code","9ca460e5":"code","9e1f7632":"code","85ed37c0":"code","dff2b59b":"code","088bce41":"code","1a6f86fe":"code","d3123b7c":"code","8b84a7ab":"code","4b0d9a74":"code","6744848b":"code","1419af05":"code","67b73283":"code","6bcdec3f":"code","b52fa2e0":"code","c8ae64af":"code","3db4ca06":"code","02d577cf":"code","29cba77e":"code","02283e7a":"code","eeffb81b":"code","cb9cc5ec":"code","cf006117":"code","6405bf3d":"code","7792c332":"code","ab1c2115":"code","d54d4283":"code","1a0fbefb":"code","ae60ff91":"code","eb8618f4":"code","b083c554":"code","902e2766":"code","f737c4f4":"code","18ec4a52":"code","0c4f741f":"code","fb862ed8":"code","da8f6931":"code","375e78e7":"code","cb8cbfc9":"markdown","f080dcdc":"markdown","80a2159c":"markdown","06cea1ba":"markdown","898dc6d5":"markdown","a61bcf81":"markdown","5a0ad106":"markdown","22b54fa4":"markdown","dfc2eeae":"markdown","2b417604":"markdown","744c96fa":"markdown","f6dc3808":"markdown","c6cfb6d2":"markdown","7b4cc2f0":"markdown","3b07e1b5":"markdown","efe0fd32":"markdown","90ca253a":"markdown","c748e1ed":"markdown","cf0edaa6":"markdown","8ab85104":"markdown","42907933":"markdown","fa4623a6":"markdown","4fbfcaf4":"markdown","ff0a7405":"markdown","e642ab46":"markdown"},"source":{"ca1d5d46":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","334b9ae2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport random\n\nfrom transformers import pipeline\n\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nfrom wordcloud import WordCloud\nfrom plotly.offline import iplot\nimport os\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom wordcloud import WordCloud\nfrom collections import Counter\n\nfrom spacy.lang.hi import Hindi\nfrom spacy.lang.ta import Tamil\nfrom spacy.lang.hi import STOP_WORDS as hindi_stopwords\nfrom spacy.lang.ta import STOP_WORDS as tamil_stopwords\n\n\nseed=111\nnp.random.seed(seed)","4f623607":"# Path to the data diectory\ndata_dir = Path(\"..\/input\/chaii-hindi-and-tamil-question-answering\/\")\n\n# Read the training and test csv files\ntrain_df = pd.read_csv(data_dir \/ \"train.csv\", encoding=\"utf8\")\ntest_df = pd.read_csv(data_dir \/ \"test.csv\", encoding=\"utf8\")\n\n# How many training and test samples have been provided?\nprint(\"Number of training samples: \", len(train_df))\nprint(\"Number of test samples: \", len(test_df))","9ca460e5":"train_df.head(2)","9e1f7632":"test_df.head(2)","85ed37c0":"plt.figure(figsize=(10, 5))\nsns.countplot(data=train_df, x=\"language\",palette=['lightgrey','#eeb977'])\nplt.title(\"The Distribution of languages in the Training Data\")\nplt.show()","dff2b59b":"# Get the actual count values\ntrain_df[\"language\"].value_counts()","088bce41":"train_df[\"question\"] = train_df[\"question\"].str.replace(\"?\", \"\", regex=False).str.strip()\ntrain_df.head(2)","1a6f86fe":"# Get the text for both the languages\ntamil_text = \" \".join(train_df[train_df[\"language\"]==\"tamil\"][\"question\"])\nhindi_text = \" \".join(train_df[train_df[\"language\"]==\"hindi\"][\"question\"])","d3123b7c":"# Download and extract the fonts\n!wget -q http:\/\/www.lipikaar.com\/sites\/www.lipikaar.com\/themes\/million\/images\/support\/fonts\/Devanagari.zip\n!wget -q http:\/\/www.lipikaar.com\/sites\/www.lipikaar.com\/themes\/million\/images\/support\/fonts\/Tamil.zip\n\n!unzip -qq Devanagari.zip\n!unzip -qq Tamil.zip","8b84a7ab":"# Get the tokens and frequencies for Hindi language\n\nhindi_nlp = Hindi()\nhindi_doc = hindi_nlp(hindi_text)\nhindi_tokens = set([token.text for token in hindi_doc])\nhindi_tokens_counter = Counter(hindi_tokens)\n\n\n# Get the tokens and frequencies for Tamil language\ntamil_nlp = Tamil()\ntamil_doc = hindi_nlp(tamil_text)\ntamil_tokens = set([token.text for token in tamil_doc])\ntamil_tokens_counter = Counter(tamil_tokens)","4b0d9a74":"def plot_wordcloud(\n    font_path,\n    frequencies,\n    stopwords,\n    width=500,\n    height=500,\n    background_color=\"white\",\n    collocations=True,\n    min_font_size=6,\n):\n    \"\"\"Generates wordcloud from word frequencies.\"\"\"\n    \n    wordcloud = WordCloud(font_path=font_path,\n                      width=width,\n                      colormap= 'YlOrBr',\n                      height=height,\n                      background_color=background_color,\n                      stopwords=stopwords,\n                      collocations=collocations,\n                      min_font_size=min_font_size).generate_from_frequencies(frequencies)\n\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()","6744848b":"# Plot the wordcloud for hindi langauge\nplot_wordcloud(font_path=\"Devanagari\/Lohit-Devanagari.ttf\",\n               frequencies=hindi_tokens_counter,\n               stopwords=hindi_stopwords\n              )","1419af05":"# Plot the wordcloud for tamil language\nplot_wordcloud(font_path=\"Tamil\/Lohit-Tamil.ttf\",\n               frequencies=tamil_tokens_counter,\n               stopwords=tamil_stopwords\n              )","67b73283":"import pandas as pd\nfrom collections import Counter\nfrom transformers import AutoTokenizer\nimport plotly.express as px\n\ndf = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv\")","6bcdec3f":"tokenizer = AutoTokenizer.from_pretrained(\"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-base-squad2\")\n\ncontext_tokens = [tokenizer(x)[\"input_ids\"] for x in df[\"context\"]]\nquestion_tokens = [tokenizer(x)[\"input_ids\"] for x in df[\"question\"]]\nanswer_tokens = [tokenizer(x)[\"input_ids\"] for x in df[\"answer_text\"]]\n\ndf[\"num_tokens_context\"] = [len(x) for x in context_tokens]\ndf[\"num_chars_context\"] = [len(x) for x in df[\"context\"]]\ndf[\"num_tokens_question\"] = [len(x) for x in question_tokens]\ndf[\"num_chars_question\"] = [len(x) for x in df[\"question\"]]\ndf[\"num_tokens_answer\"] = [len(x) for x in answer_tokens]\ndf[\"num_chars_answer\"] = [len(x) for x in df[\"answer_text\"]]","b52fa2e0":"context_tokens_flat = sum(context_tokens, [])\nquestion_tokens_flat = sum(question_tokens, [])\nanswer_tokens_flat =  sum(answer_tokens, [])\n\nunk = tokenizer.unk_token\n\nunk in context_tokens_flat, unk in question_tokens_flat, unk in answer_tokens_flat","c8ae64af":"contexts = df[\"context\"]\nanswers = df[\"answer_text\"]\n\nall_chars_ctx = \"\".join(contexts)\nall_chars_ans = \"\".join(answers)\n\nunq_chars_ctx = sorted(list(set(all_chars_ctx)))\nunq_chars_ans = sorted(list(set(all_chars_ans)))","3db4ca06":"\"\".join(unq_chars_ctx)","02d577cf":"\"\".join(unq_chars_ans)","29cba77e":"answers[answers.str.contains(r\"\\.\")][1:5]","02283e7a":"most_common = Counter(all_chars_ctx).most_common(50)\n\"\".join([x[0] for x in most_common])\nx=[x[0] for x in most_common]\ny=[x[1] for x in most_common]\npx.bar(x=[x[0] for x in most_common], y=[x[1] for x in most_common], color_discrete_sequence =['#eeb977']*len(x),labels={\"x\": \"character\", \"y\": \"count\"})","eeffb81b":"most_common_ans = Counter(all_chars_ans).most_common(50)\npx.bar(x=[x[0] for x in most_common_ans], y=[x[1] for x in most_common_ans], color_discrete_sequence =['#eeb977']*len(x), labels={\"x\": \"character\", \"y\": \"count\"})","cb9cc5ec":"px.histogram(df, x=\"num_tokens_context\", color=\"language\",color_discrete_sequence=['#eeb977','darkgrey'])","cf006117":"only_hindi = df[df[\"language\"]==\"hindi\"]\nonly_tamil = df[df[\"language\"]==\"tamil\"]\nnum_hindi = len(only_hindi)\nnum_tamil = len(only_tamil)\n\nlengths = list(range(0, df[\"num_tokens_context\"].max(), 25))\nhindi_counts = []\ntamil_counts = []\nfor l in lengths:\n    hindi_counts.append((only_hindi[\"num_tokens_context\"]<=l).sum()\/num_hindi)\n    tamil_counts.append((only_tamil[\"num_tokens_context\"]<=l).sum()\/num_tamil)\n\ncounts_df = pd.DataFrame(data={\"count\": hindi_counts+tamil_counts, \"length\": lengths*2, \"language\": [\"hindi\"]*len(hindi_counts)+[\"tamil\"]*len(tamil_counts)})\n    \npx.line(counts_df, x=\"length\", y=\"count\", color=\"language\", color_discrete_sequence=['#eeb977','darkgrey'],labels={\"count\": \"fraction below length\"})","6405bf3d":"px.histogram(df, x=\"num_tokens_answer\", color=\"language\", color_discrete_sequence=['#eeb977','darkgrey'])","7792c332":"import torch\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom torch.optim import Adam,AdamW\nfrom torch.utils.data import SequentialSampler\nfrom torch.utils.data import Dataset,DataLoader\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import XLMRobertaTokenizer,XLMRobertaModel,AutoTokenizer,XLMRobertaModel,XLMRobertaConfig,AutoModel,AutoConfig\nfrom transformers import logging\nfrom transformers import (\n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup\n)\n\nlogging.set_verbosity_error()","ab1c2115":"config = {\n    'base_model':\"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-base-squad2\",\n    'batch_size':16,\n    \"epochs\":2,\n    'folds':5,\n    'device':torch.device('cuda')\n}","d54d4283":"class ChaiiDataset(Dataset):\n    \n    def __init__(self,df,max_len=356,doc_stride=128):\n        \n        self.df = df\n        self.max_len = max_len \n        self.doc_stride = doc_stride\n        self.labelled = 'answer_text' in df\n        self.tokenizer = AutoTokenizer.from_pretrained(config['base_model'],add_special_tokens=True)        \n        self.tokenized_samples = self.tokenizer(\n                                self.df['context'].values.tolist(),\n                                self.df['question'].values.tolist(),\n                                truncation=\"only_first\",\n                                max_length=self.max_len,\n                                stride=self.doc_stride,\n                                return_overflowing_tokens=True,\n                                return_offsets_mapping=True,\n                                padding=\"max_length\")\n        \n    \n        \n    def __getitem__(self,idx):\n        \n        data = {}\n        ids,mask,offset = self.tokenized_samples['input_ids'][idx],\\\n                        self.tokenized_samples['attention_mask'][idx],\\\n                        self.tokenized_samples['offset_mapping'][idx]\n        \n        data['index'] = idx\n        data['ids'] = torch.tensor(ids)\n        data['mask'] = torch.tensor(mask)\n        data['offset'] = offset\n        if self.labelled:\n            \n            answer_text,start,end = self.get_targets(idx)\n            data['answer_text'] = answer_text\n            data['start'] = torch.tensor(start)\n            data['end'] = torch.tensor(end)\n            \n        \n        return data\n        \n    \n    def get_targets(self,idx):\n        \n        df_index = self.tokenized_samples['overflow_to_sample_mapping'][idx]\n        start_char = (self.df.iloc[df_index]['answer_start'])\n        end_char = start_char + len(self.df.iloc[df_index]['answer_text'])\n        offset = self.tokenized_samples['offset_mapping'][idx]\n        sequence_ids = self.tokenized_samples.sequence_ids(idx)\n        end_offset = len(self.tokenized_samples['input_ids'][idx])-1\n        start_offset = 1\n        while sequence_ids[end_offset] != 0:\n            end_offset -= 1\n            \n            \n        start_idx = 0;end_idx=0\n        ## answer not in context\n        if (start_char > offset[end_offset][0] or end_char < offset[start_offset][0]):\n            #print(\"In first loop\")\n            start_idx = 0;end_idx=0\n            answer_text=\"\"\n            \n        ## answer partially in context\n        elif ((start_char <= offset[end_offset][0]) and (end_char >  offset[end_offset][0])):\n            #print(\"in second loop\")\n            start_idx = 0;end_idx=0\n            answer_text = \"\"\n        \n        ## answer fully inside context\n        else:\n            #print(\"In third loop\")\n            i=0\n            while (start_idx < len(offset) and offset[i][0]<=start_char and offset[i][1]<start_char):\n                start_idx+=1\n                i+=1\n            end_idx = i\n            while (end_idx < len(offset) and offset[i][1]<end_char):\n                end_idx+=1\n                i+=1\n            answer_text = self.df.iloc[df_index]['answer_text'].strip()\n            \n        \n        return answer_text,start_idx, end_idx \n    \n    \n    def post_process(self,batch,pred_start,pred_end):\n        batch_pred,indices = [],[]\n        for idx,start,end in zip(batch['index'],pred_start,pred_end):\n            a,b = self.tokenized_samples['offset_mapping'][idx][start][0],self.tokenized_samples['offset_mapping'][idx][end][1]\n            df_index = self.tokenized_samples['overflow_to_sample_mapping'][idx]\n\n            if a>b:\n                batch_pred.append(\"\")\n                indices.append(df_index)\n            else: \n                pred_string = self.df.iloc[df_index]['context'][a:b].strip()   \n                batch_pred.append(pred_string.strip())\n                indices.append(df_index)\n\n        return batch_pred,indices\n\n    \n    \n    \n    def __len__(self):\n        return len(self.tokenized_samples['overflow_to_sample_mapping'])","1a0fbefb":"class ChaiiModel(nn.Module):\n    \n    def __init__(self):\n        super(ChaiiModel,self).__init__()\n        \n        self.model_config = AutoConfig.from_pretrained(config['base_model'])\n        self.model_config.return_dict=True\n        self.model_config.output_hidden_states=True\n        self.model = AutoModel.from_pretrained(config['base_model'],config=self.model_config)\n        self.dropout = nn.Dropout(0.1)\n        self.fc = nn.Linear(self.model_config.hidden_size,2)\n        self.__init_weights(self.fc)\n        \n    def __init_weights(self,module):\n        if isinstance(module,nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        \n    def forward(self,input_ids,attention_mask):\n        \n        output = self.model(input_ids,attention_mask)\n        hidden_states = output['hidden_states'][-1]\n        #x = torch.stack([hidden_states[-1],hidden_states[-2],hidden_states[-3],hidden_states[-4]])\n        #x = torch.mean(x,0)\n        x = self.dropout(hidden_states)\n        x = self.fc(x)\n        start_logits,end_logits = x.split(1,dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n                \n        return start_logits, end_logits","ae60ff91":"def safe_div(x,y):\n    if y == 0:\n        return 1\n    return x \/ y\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return safe_div(float(len(c)) , (len(a) + len(b) - len(c)))\n\ndef get_jaccard_score(y_true,y_pred):\n    assert len(y_true)==len(y_pred)\n    score=0.0\n    for i in range(len(y_true)):\n        score += jaccard(y_true[i], y_pred[i])\n        \n    return score\n\ndef chaii_loss(start_logits, end_logits, start_positions, end_positions):\n    ce_loss = nn.CrossEntropyLoss()\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)    \n    total_loss = (start_loss + end_loss)\/2\n    return total_loss","eb8618f4":"def train_model(model,dataloaders,criterion,optimizer,scheduler=None,epochs=3,filename='saved.pth'):\n    \n    \n    model.cuda()\n    \n    for epoch in range(epochs):\n        for phase in ['train','valid']:\n            \n            if phase=='train':\n                model.train()\n                \n            else:\n                model.eval()\n                \n            epoch_loss = 0.0\n            epoch_jaccard = 0.0\n            bar = tqdm(dataloaders[phase],total=len(dataloaders[phase]))\n            for i,data in enumerate(bar):\n                \n                input_ids = data['ids'].cuda()\n                masks = data['mask'].cuda()\n                start,end = data['start'].cuda(),data['end'].cuda()\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == 'train'):\n\n                    start_logits, end_logits = model(input_ids, masks)\n                    loss = criterion(start_logits,end_logits,start,end)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        if scheduler!=None:\n                            scheduler.step()\n\n                    epoch_loss += loss.item() * len(input_ids)\n                    \n                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n                    \n                    pred_start,pred_end = start_logits.argmax(axis=1),end_logits.argmax(axis=1)\n                    prediction_strings,_ = dataloaders[phase].dataset.post_process(data,pred_start, pred_end)\n                    \n                    epoch_jaccard += get_jaccard_score(data['answer_text'],prediction_strings)\n                    \n                \n                    bar.set_postfix(Batch=i, Phase=phase, loss= epoch_loss\/(config['batch_size']*(i+1)),\n                        jaccard_score = epoch_jaccard\/(config['batch_size']*(i+1)))\n                \n            epoch_loss = epoch_loss \/ len(dataloaders[phase].dataset)\n            epoch_jaccard = epoch_jaccard \/ len(dataloaders[phase].dataset)\n            \n            print('Epoch {}\/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n                epoch + 1, epochs, phase, epoch_loss, epoch_jaccard))\n    \n    torch.save(model.state_dict(), filename)","b083c554":"def train_and_eval(train,valid,fold):\n    \n    train = ChaiiDataset(train)\n    train_loader = DataLoader(train,batch_size=config['batch_size'],shuffle=True)\n    \n    valid = ChaiiDataset(valid)\n    valid_loader = DataLoader(valid,batch_size=config['batch_size'],shuffle=True)\n    \n    model = ChaiiModel()\n    criterion = chaii_loss\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.01,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters,lr=4e-5)\n    steps = (len(train)*config['epochs'])\/\/config['batch_size']\n    scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=int(0*steps),num_training_steps=steps)\n    dataloaders = {'train':train_loader,'valid':valid_loader}\n    filename = f\"{fold}_chaii_model.pth\"\n    train_model(model,dataloaders,criterion,optimizer,scheduler,config['epochs'],filename)\n    ","902e2766":"def run_k_fold(folds=5):\n    \n    \n    \n    train = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')\n    external_mlqa = pd.read_csv('..\/input\/mlqa-hindi-processed\/mlqa_hindi.csv').sample(354)\n    external_xquad = pd.read_csv('..\/input\/mlqa-hindi-processed\/xquad.csv').sample(400)\n    external_train = pd.concat([external_mlqa, external_xquad])\n    external_train['id'] = list(np.arange(1, len(external_train)+1))\n    df = pd.concat([train, external_train]).reset_index(drop=True)\n    print(f\"Number of samples in train data is {df.shape[0]}\")\n\n    \n    df[\"kfold\"] = -1\n\n    kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n    for f, (t_, v_) in enumerate(kf.split(X=df, y=df.language.values)):\n        df.loc[v_, 'kfold'] = f\n        \n    for fold in range(folds):\n        \n        \n            print(f\"Training fold {fold}\")\n\n            train = df[df['kfold']!=fold]\n            valid = df[df['kfold']==fold]\n\n            train_and_eval(train,valid,fold)\n\n            print(\"----------------------\")","f737c4f4":"run_k_fold(config['folds'])","18ec4a52":"test_df = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv\")","0c4f741f":"def post_process(prediction_strings,indices):\n    \n    df = pd.DataFrame()\n    df['index'] = indices\n    df['answer'] = prediction_strings\n    \n    def best_answer(x):\n        x = [k for k in x if len(k)>0]\n        if len(x)>0:\n            return min(x,key=len)\n        return \"\"\n    \n    answer = df.groupby(['index'])['answer'].apply(lambda x : best_answer(x))\n    \n    return answer\n\n    \n    \n    \n\ndef inference_fn(test,fold):\n    \n    prediction_strings,indices = [],[]\n    test = ChaiiDataset(test)\n    test_loader = DataLoader(test,batch_size=config['batch_size'],shuffle=False)\n\n    model = ChaiiModel()\n    filename = f\"..\/input\/chaiibaseline\/{fold}_chaii_model.pth\"\n    \n    model.load_state_dict(torch.load(filename))\n    model.to(config['device'])\n    model.eval()\n    \n    for i,data in enumerate(test_loader):\n        \n        input_ids = data['ids'].cuda()\n        masks = data['mask'].cuda()\n        \n        start_logits,end_logits = model(input_ids,masks)\n        \n        start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n        end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n                    \n        pred_start,pred_end = start_logits.argmax(axis=1),end_logits.argmax(axis=1)\n        preds,ind = test_loader.dataset.post_process(data,pred_start, pred_end)\n        prediction_strings.extend(preds);indices.extend(ind)\n        \n    \n    return  post_process(prediction_strings,indices)\n        \n\n\ndef get_best_prediction(df):\n    \n    all_answers=[]\n    for i,row in df.iterrows():\n    \n        candidates = [k.strip() for k in row.values.tolist() if len(k)<100 and len(k)>0]\n        if len(candidates)>0:\n            counter = Counter(candidates)\n            answer = counter.most_common(1)[0][0]\n        else:\n            answer = \"\"\n\n        all_answers.append(answer)\n    \n    return all_answers\n","fb862ed8":"df = pd.DataFrame()\nfor fold in tqdm(range(config['folds'])):\n    \n    preds = inference_fn(test_df,fold)\n    df[f'fold_{fold}'] = preds\n    ","da8f6931":"answers =  get_best_prediction(df)   \nsubmission = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/sample_submission.csv\")\nsubmission['PredictionString'] = answers\nsubmission.to_csv('submission.csv',index=False)","375e78e7":"submission.head(2)","cb8cbfc9":"## Checking for unknown tokens","f080dcdc":"### Competition Description: \n- With nearly 1.4 billion people, India is the second-most populated country in the world. Yet Indian languages, like Hindi and Tamil, are underrepresented on the web. Popular Natural Language Understanding (NLU) models perform worse with Indian languages compared to English, the effects of which lead to subpar experiences in downstream web applications for Indian users.\n\n\n- Predicting answers to questions is a common NLU task, but not for Hindi and Tamil. Current progress on multilingual modeling requires a concentrated effort to generate high-quality datasets and modelling improvements. Additionally, for languages that are typically underrepresented in public datasets, it can be difficult to build trustworthy evaluations. \n\n### About the languages spoken in India:\n- Languages spoken in India belong to several language families, the major ones being the Indo-Aryan languages spoken by 78.05% of Indians and the Dravidian languages spoken by 19.64% of Indians. \n- Languages spoken by the remaining 2.31% of the population belong to the Austroasiatic, Sino\u2013Tibetan, Tai\u2013Kadai and a few other minor language families and isolates.\n- 283 India has the world's fourth highest number of languages (447), after Nigeria (524), Indonesia (710) and Papua New Guinea (840).\n- The Eighth Schedule of the Indian Constitution lists 22 languages, which have been referred to as scheduled languages and given recognition, status and official encouragement. \n- In addition, the Government of India has awarded the distinction of classical language to Kannada, Malayalam, Odia, Sanskrit, Tamil and Telugu. Classical language status is given to languages which have a rich heritage and independent nature.\n\n### The Task\n- We are given questions in Tamil\/Hindi about some Wikipedia articles, and we have to generate the answers for those questions using our model.\n\n### Dataset\n- We have been provided with a new question-answering dataset with question-answer pairs, and it goes by the name chaii-1. The task is straightforward.\n\n### Evaluation Metric\n- The predictions would be evaluated using word-level Jaccard score. \n- A sample code has also been provided for the same.","80a2159c":"### Languages of India : Region Wise\n\n![Languages of India](https:\/\/4.bp.blogspot.com\/-LVOObgDzDjw\/WbSFz80TLZI\/AAAAAAABBvM\/t-u8-pf2h7cDM5sf_zI2XhtuE_25R4QUACLcBGAs\/s1600\/Languages-Map.jpg)\n\n### Languages of India : % Speakers Wise\n![The % of people speaking these languages](https:\/\/qph.fs.quoracdn.net\/main-qimg-cbcf7165e9ef13620a378555d781a83f)","06cea1ba":"### What fraction of the contexts are below a certain length?","898dc6d5":"### Observations:\n- The number of instances for Hindi language is almost double the number of instance of Tamil language in the training dataset. Let's also get the actual count to see the difference","a61bcf81":"# Remove punctuation\nAll the questions presented here are represented with a question mark. We will simply remove it and along with it, we will alos strip any whitespace around the text","5a0ad106":"## Tokenizing","22b54fa4":"### Observations : \n\n- There can be English words as well in the given questions answer_start column isn't in the test dataset, but it gives important information about the training dataset, the starting character for the context\n- The language column is present in both train and test. \n- One of the things that we can try is to build two separate models, one for Hindi and one for Tamil, and then make the predictions accordingly using the values in this column","dfc2eeae":"### Number of tokens in context\n\n- Long contexts are probably harder because the model can not look at everything at once. \n- Even Big Bird can't do 14k tokens. \n- Maybe there could be an approach to identify the chunk of 1,000 or 500 tokens from where the answer is likely, and then that smaller chunk goes into the QA model.","2b417604":"# Baseline Model","744c96fa":"# Imports","f6dc3808":"### Observation:\n\n- For the Answer characters some numbers showing up! ","c6cfb6d2":"# Distribution of the languages in the training dataset","7b4cc2f0":"## Looking at character level","3b07e1b5":"### Observations: \n\nAbout 190 contexts and 120 answers are duplicates\n\nLooking at what types of characters are in the context.\nIt turns out there are many languages in addition to Hindi and Tamil. I see:\n\n- English\n- Latin\n- Greek\n- Japanese\n- Chinese\n- Arabic\n- Nepali\n\n#### A multi-lingual model will be very important","efe0fd32":"# chaii - Hindi and Tamil Question Answering","90ca253a":"For generating the wordlcoud, we need the right font\n\n- [Font for Hindi](http:\/\/www.lipikaar.com\/support\/download-unicode-fonts-for-hindi-marathi-sanskrit-nepali)\n- [Font for Tamil](http:\/\/www.lipikaar.com\/support\/download-unicode-fonts-for-tamil)","c748e1ed":"### Observation : \n- Answer characters are less varied\n- Noticed one of my predictions having a parenthesis around it which made the jaccard score 0. (1990 compared to 1990. \n- It might be a good idea to clean un-balanced punctuation or non-letters (commas, periods) at the beginning\/end of answers. \n- Might be wrong, but it looks like the only languages in the answer are English, Hindi, and Tamil.","cf0edaa6":"# Basic Data Exploration","8ab85104":"# WordCloud\nWe will generate two wordclouds, one for each language.","42907933":"Taken insights and help from \n\n- https:\/\/www.kaggle.com\/shahules\/chaii-xlm-base-custom-qa-train-infer\n- https:\/\/www.kaggle.com\/nbroad\/chaii-qa-character-token-languages-eda\n\n\nSome of my other works\n- [How did Covid-19 impact Digital Learning - EDA](https:\/\/www.kaggle.com\/udbhavpangotra\/how-did-covid-19-impact-digital-learning-eda)\n- [EDA + Optuna: An attempt at a clean notebook](https:\/\/www.kaggle.com\/udbhavpangotra\/eda-optuna-an-attempt-at-a-clean-notebook)\n- [Heart Attacks! Extensive EDA and visualizations :)](https:\/\/www.kaggle.com\/udbhavpangotra\/heart-attacks-extensive-eda-and-visualizations)\n- [CommonLit Readibility Prize Extensive EDA + Model](https:\/\/www.kaggle.com\/udbhavpangotra\/commonlit-readibility-prize-extensive-eda-model)\n- [TPS-Sept-Extensive EDA + Baseline + SHAP](https:\/\/www.kaggle.com\/udbhavpangotra\/extensive-eda-baseline-shap\/data)","fa4623a6":"- There are only ~1100 training samples, meaning we are in a low data regime, suggesting that transfer-learning and fine-tuning are the best shots if we are going to use DNNs for this task. \n- This doesn't mean we shouldn't build your models!\n- Let's take a look at the training data and the test data","4fbfcaf4":"## Context characters\n\nLooking mostly Hindi and Tamil in the top 50 characters","ff0a7405":"# Exploratory Data Analysis \n- Looking at number of tokens, characters, unknown tokens, what languages are in text, and more\n- There might be some useful post-processing approaches that could be explored based on this work","e642ab46":"### Observation:\n- Answers with periods are often for numbers or dates.\n\n\n***\u0b95\u0bbf.\u0bae\u0bc1 means BC and \u0b95\u0bbf.\u0baa\u0bbf means AD***\n\n\n***\u0908.\u092a\u0942. means BC and \u0908 means AD***\n\n- Would guess that the answers that end in ... or start with . are annotator mistakes. \n- There are some inconsistencies with AD and BC ending in periods (263 and 288) though it might have to do with the source text. Might be worth probing."}}