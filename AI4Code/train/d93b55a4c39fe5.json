{"cell_type":{"b687b874":"code","0a5bcfd7":"code","110bd579":"code","05cdca38":"code","f98026e4":"code","1713d817":"code","d67aaa1c":"code","b0261e50":"code","b9437200":"code","fdba9b9b":"code","17342a54":"code","e8e3b42a":"code","0b329183":"code","18dc50eb":"code","689e0064":"code","8c3976c5":"code","635fe4a3":"code","89b0e9db":"markdown","25afa6f7":"markdown","3d456a65":"markdown","f577e37b":"markdown","385704da":"markdown","b1963e87":"markdown","7d97e2ee":"markdown","10dd35e3":"markdown","15ff3ecb":"markdown","e658c620":"markdown","2399fb21":"markdown","0a96a016":"markdown","70e75ac0":"markdown","98fa5d03":"markdown","bc9b645a":"markdown","8fddfeeb":"markdown","21422818":"markdown","99f2861e":"markdown","69643f29":"markdown","fde0de82":"markdown","0a2d7e73":"markdown","f7861bec":"markdown","f158a9f4":"markdown","040f8a96":"markdown","e9e95513":"markdown","b9c7fe12":"markdown","fab077cf":"markdown"},"source":{"b687b874":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n# All imports done here \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom sklearn.mixture import GaussianMixture\nimport math\nfrom IPython.display import display, HTML\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0a5bcfd7":"# Read the TSV (Tab separated values) into a pandas data frame\ndf = pd.read_csv('..\/input\/en.openfoodfacts.org.products.tsv', low_memory=False, sep='\\t')\n# Dataframe info\ndf.info()","110bd579":"# Starter code for exploration - https:\/\/pandas.pydata.org\/pandas-docs\/stable\/getting_started\/10min.html \u2764\ufe0f \n# Rows in a dataframe\nprint('Entries: '+str(len(df)))\n# All the columns\nprint(df.columns)\n# All the column types\nprint(df.dtypes)\n# Some example data?\ndf.head()","05cdca38":"#Filtering, sorting and slicing \n#1.Select\n#2.Sort\n#3.Slice\nprint(df[['product_name','energy_100g']].sort_values(by=['energy_100g'],ascending=False).head(20))\n#Slice rows\nprint(df[0:5])","f98026e4":"#Basic statistics\ndf.describe()","1713d817":"#Set the size of the figure\nplt.figure(figsize=(5,30))\n#Prepare the data\n#Count and sort by the number of missing values.\ncounts_series = df.isnull().mean(axis=0).apply(lambda x: x*100).sort_values(ascending=False)\n#Set the title and other labels if need be\nplt.title(\"Proportion of missing values in each column\")\nplt.ylabel(\"column\")\nplt.xlabel(\"percentage\")\n#Plot the chart\nplt.barh(counts_series.index,counts_series.values)\n#Always show after plotting \u26a1\nplt.show()","d67aaa1c":"# Change this to change the threshold for filtering - 0 to 100\ncutoff = 30\n\ndef split_data_by_nan(dataframe,threshold):\n    # percentage of null rows\n    s = dataframe.isnull().mean(axis=0).apply(lambda x: x*100)\n    # keep column if less nulls than threshold\n    cols_of_interest = s[s<= threshold].index\n    return cols_of_interest\n\n# Filter columns by threshold\ndf_fs = df[split_data_by_nan(df,cutoff)]\n\nprint(\"Original number of features: \" + str(df.shape[1]))\nprint(\"Number of features with less than \"+ str(cutoff)+\"% nans: \" + str(df_fs.shape[1]))\nprint(df_fs.columns)","b0261e50":"sns.set(context=\"paper\", font_scale = 1.2)\n# compute the correlation matrix for all the numeric columns\ncorrmat = df_fs.corr()\n# size of the plot\nf, ax = plt.subplots(figsize=(12, 12))\n# set the plot heading\nf.text(0.45, 0.93, \"Pearson's correlation coefficients\", ha='center', fontsize = 18)\n# plot matrix as a heatmap\nsns.heatmap(corrmat, square=True, linewidths=0.01, cmap=\"coolwarm\")\nplt.tight_layout()","b9437200":"# Features for modelling\nfiltered_columns = ['saturated-fat_100g','fat_100g','carbohydrates_100g','sugars_100g','proteins_100g','salt_100g','energy_100g']\n\n# Cleaning if any values are empty in a row\ndf_fs[\"isempty\"] = np.where(df_fs[filtered_columns].isnull().sum(axis=1) >= 1, 1, 0)\n# Estimation of cleaning proportion\npercentage = (df_fs.isempty.value_counts()[1] \/ df_fs.shape[0]) * 100\nprint(\"Percentage of incomplete tables: \" + str(percentage))\nprint(df_fs.isempty.value_counts())\n# Cleaning\ndf_cleaned = df_fs[df_fs.isempty==0]\n# Check if cleaning is successful\ndf_cleaned.isnull().sum()","fdba9b9b":"df_cleaned[\"reconstructed_energy\"] = df_cleaned[\"fat_100g\"] * 39 + df_cleaned[\"carbohydrates_100g\"] * 17 + df_cleaned[\"proteins_100g\"] * 17\ndf_cleaned.describe()\ndf_train = df_cleaned","17342a54":"'''\nplt.figure(figsize = (10,10))\nplt.scatter(df_cleaned[\"energy_100g\"], df_cleaned[\"reconstructed_energy\"])\nplt.plot([0,5000], [0, 5000], color = 'black', linewidth = 2)\nplt.xlabel(\"given energy\")\nplt.ylabel(\"reconstructed energy\")\nplt.show()\n'''","e8e3b42a":"'''\nprint(len(df_cleaned[df_cleaned[\"fat_100g\"] + df_cleaned[\"carbohydrates_100g\"] + df_cleaned[\"proteins_100g\"] > 100]))\ndf_train = df_cleaned[df_cleaned[\"fat_100g\"] + df_cleaned[\"carbohydrates_100g\"] + df_cleaned[\"proteins_100g\"] <= 100] \n'''","0b329183":"%%time\n# change the number of clusters\nn_clusters = 10\n\n#replacing energy with reconstructed energy\nfeatures = ['saturated-fat_100g','fat_100g','carbohydrates_100g','sugars_100g','proteins_100g','salt_100g','reconstructed_energy']\nX_train = df_train[features].values\n\n#create the model\nmax_iter = 100\nmodel = GaussianMixture(n_components=n_clusters, covariance_type=\"full\", n_init = 5, max_iter = max_iter)\n#fit model on data\nmodel.fit(X_train)\n\n#predict cluster number for the date\nresults = df_train\nresults[\"cluster\"] = model.predict(X_train)","18dc50eb":"%%time\ndef make_word_cloud(data, cluster, subplotax, title):\n    # Get words from the product name \n    words = data[data.cluster==cluster][\"product_name\"].apply(lambda l: l.lower().split() if type(l) == str else '')\n    # Add to a pandas series\n    cluster_words=words.apply(pd.Series).stack().reset_index(drop=True)\n    # Split and join\n    text = \" \".join(w for w in cluster_words)\n\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud(max_font_size=30, max_words=30, background_color=\"white\", colormap=\"YlGnBu\").generate(text)\n\n    # Display the generated image:    \n    subplotax.imshow(wordcloud, interpolation='bilinear')\n    subplotax.axis(\"off\")\n    subplotax.set_title(title,fontweight=\"bold\", size=20)\n    return subplotax\n\nrows = math.ceil(n_clusters\/2)\nfig, ax = plt.subplots(rows,2, figsize=(20,50))\nfor m in range(rows):\n    for n in range(2):\n        cluster = m*2+ n\n        title = \"Cluster \" + str(cluster) \n        make_word_cloud(results, cluster, ax[m,n], title)","689e0064":"# Cluster ? has just one element\n'''\nresults[results['cluster'] == 6]\n'''","8c3976c5":"for cluster in range(0,n_clusters):\n    display(\"Cluster \"+str(cluster))\n    display(results[results['cluster'] == cluster][features].describe())","635fe4a3":"# Change this to include the clusters you want\n# Eg. 1 portion carbohydrates, 1 seasoning, 1 dairy product, 2 portions of proteins\nmeal_clusters = [1,2,3,9,9]\n\nindices = []\nresults['rownum'] = results.index\nfor cluster in meal_clusters:\n    indices+=results[results['cluster'] == cluster].sample(1)['rownum'].tolist()\n\nmeal = pd.DataFrame()\nfor index in indices:\n    meal = meal.append(results[results['rownum'] == index])\nmeal","89b0e9db":"# Modelling\n\n1. Train - Teach the model to identify patterns in the data.\n2. Evaluate - Evaluate different algorithms and different preprocessing against a common measure.\n3. Iterate - Change how the algorithm learns the patterns (hyperparamater tuning) and do a different kind of preprocessing (feature selection \/ engineering \/ preprocessing).\n4. Predict - When you are satisfied with the model, productionize the model and run predictions on test data.\n\n## Clustering\nClustering is a machine learning technique to group items so that items in one group are statistically similar to each other in the group than items in other groups. \nA few common [clustering](https:\/\/towardsdatascience.com\/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68) techniques are:\n1. K Means: Every point is classified based on the group center it is closest too. After every round, group centers are recalculated. The process stops, when points do no get reassigned any more.\n![K-Means](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*KrcZK0xYgTa4qFrVr0fO2w.gif)\n2. Agglomerative clustering: When starting, every item is treated as a cluster. It is then iteratively merged with an item\/group which is closest too. The result is a heirarchical tree structure.\n![Agglomerative Clustering](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*ET8kCcPpr893vNZFs8j4xg.gif)\n\n\n### Guassian Mixture Models\nK-Means takes cluster centers as a way to determine the cluster an item belongs to. But that might be an over-simplication since the averages\/cluster centers can be the same for more than one cluster. Or the center might not be representative of the cluster. \n\n![Guassian Mixture Models wins](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*Xvl-pXxsLAZ7gbTUuvgMtA.png)\n\nGuassian mixture models take one step ahead to use standard deviation along with the mean. So, every cluster is described by a center and a standard deviation of the distribution of items in the cluster. \nInitially, every cluster assumes a random distribution. For every item, the probability of belonging to a cluster is calculated (based on how close the item is to the center). Cluster distributions are recalculated to maximize the probablity of items in the cluster. The process stops, when distributions do not change.\n\n\n### Scikit-learn\n[Scikit-learn](https:\/\/scikit-learn.org\/stable\/tutorial\/basic\/tutorial.html) is an open source machine learning library. It includes implementation for most machine learning algorithms:\n1. Classification such as Decision trees, ensemble methods, Naive Bayes, Support Vector Machines, Logistic Regression\n2. Regression such as linear regression, nearest neighbours regression\n3. Clustering including K-Means, Guassian Mixture, Agglomerative, DBSCAN\n4. Dimensionality Reduction such as Principal Component Analysis\n5. Preprocessing including but not limited to stratified splits, count vectorizer, scaling\n6. Evaluation metrics","25afa6f7":"# Business Problem\n[Open food Facts ](https:\/\/world.openfoodfacts.org\/) is a crowd sourced food database containing including ingredients, allergens and nutrition information and information about products around the world.\n\n5000+ contributors have added  over 600000 products from more than 150 countries.\n\nOur business problem for today is to identify natural similarities between these products. This information can then be used for creating automated diet plans based on nutrition goals. Diet plans can be customized to prevent certain allergens and meet dietery requirements eg. low fat, low carb, high protein diet. It can also be used to break the monotony of using the same ingredients to match the required calorie intake. \n\n### So, this evening give your dinner a much required makeover!!! \ud83d\ude09\ud83d\ude09\ud83d\ude09\u2764\ufe0f \u2764\ufe0f ","3d456a65":"#### \ud83d\udd2cCo-relation matrix","f577e37b":"#### \ud83d\udc69\u200d\ud83d\udd2c Feature Selection","385704da":"#### \ud83e\udd13 Cleaning - if sum of nutrients > 100g","b1963e87":"#### \ud83d\udd2cCluster statistics\n","7d97e2ee":"#### \ud83d\udca1 NOTE:\n- Cluster ?? look like they are rich in carbohydrates. \ud83c\udf5c\ud83c\udf5c\ud83c\udf5d\ud83c\udf5d\ud83c\udf5e \n- Cluster ? has seasoning \n- Cluster ? looks like dairy \ud83e\udd5b\ud83e\udd5b\n- Cluster ? looks like dessert, ice cream and chocolate \ud83c\udf66\ud83c\udf6b\ud83c\udf6a\ud83c\udf69\n- Cluster ? is all about cheese \ud83e\uddc0 \ud83e\uddc0 \ud83e\uddc0 \n- Cluster ?? has a lot of protein products \ud83e\udd5c\ud83d\udc04\ud83e\udd5a","10dd35e3":"#### \ud83d\udca1 So many missing values!!!\nAs we suspected, more than 50% of the columns have more than 50% of missing values. Some even have close to 100% missing values. At this point, we should ask our ourselves two questions:\n1. Are these columns adding any value to our goal?\n2. Can we ignore them and use only a few columns we understand?\n3. What is the threshold we want to filter on?","15ff3ecb":"#### \ud83d\udca1 Some conclusions!!!\n- Nutrition scores are more positively related to saturated fats than proteins and fats.\n- Nutrition scores are inversely proportional to salt, sodium and proteins.\n- Sugars and proteins are inversely related.\n- Salt and sodium are very strongly positively related. Do we need both?\n\nAny more?","e658c620":"#### \u2705 Further feature selection and cleaning\nFor modelling purposes, we have removed soduim, additives, palm oil related columns and nutrition scores from consideration sure to:\n- Sodium and salt are strongly positively related and logically the same column, hence only keeping one.\n- Additives and palm oil related columns might not be very useful for modelling.\n- Nutrition scores are a combination of nutrient values.\n","2399fb21":"## \ud83d\udc69\u200d\ud83d\udd2c Make your own meal\n\n","0a96a016":"#### \ud83e\udd13 Is reconstructed energy same as energy?","70e75ac0":"#### \u2705 Import libraries","98fa5d03":"### How to use this notebook?\n\n##### 1. Always run cells as they are after \u2705\n##### 2. Experiment with cells after \ud83d\udd2c\n##### 3. Customize as you need for cells after \ud83d\udc69\u200d\ud83d\udd2c\n##### 4. Shhhh...experts only \ud83e\udd13\n##### 5. **BREAK THINGS**\n##### 6. **HAVE FUN**\n\n","bc9b645a":"# \u23ed\ufe0f  Next Steps\n1. Evaluation - Training and validation sets\n2. Iterative hyperparamater tuning and feature engineering\/feature selection\n3. K-Means and other kinds of clustering\n4. Productionizing your model","8fddfeeb":"## Visualisations\nVisualizations form a very important part of the EDA process. They are useful in quickly determining and summarizing the general trends and outliers in the data. Tread carefully \u26a1\u26a1\u26a1!! Correctly representing the data, scaling your visualizations and labelling them properly to communicate your story is a great skill to acquire!\n\nSome of the charts you can use:\n- Line\n- Bar, stacked bar charts\n- Histograms\n- Scatter plot\n- Pie\n- Sunburst chart \ud83d\udd25\ud83d\udd25\n- Tree Map \n- Box Plot\n\n### Matplotlib\n[Matplotlib](https:\/\/matplotlib.org\/) is a Python plotting library for 2D figures. For simple plotting the pyplot module provides a MATLAB-like interface, particularly when combined with IPython. \n\n### Plotly\n[Plotly](https:\/\/github.com\/plotly\/plotly.py) is another well known and useful open source Python graphing library. The main advantage of using plotly is how easy it is to make interactive graphs. What's more, the same visualizations can be migrated to R or javascript with minimal effort.\n\n### Seaborn\n[Seaborn](https:\/\/seaborn.pydata.org\/) is based off matplotlib and is great for statistical visualizations. It allows processing large datasets natively to create visualizations. More information: https:\/\/seaborn.pydata.org\/introduction.html#introduction","21422818":"#### \u2705 Feature engineering\nThe open food facts app requires users to add energy per 100g. But based on the geographical location, product packaging may have energy in kJ or kCal.\nTo resolve this discrepancy, we reconstruct energy in kJ from the nutrient values.","99f2861e":"# Preproprocessing\nThese are the steps taken before actual modelling takes place. Some of the steps in this process in no particular order are:\n1. Data Cleaning\n2. Feature Selection\n3. Feature Engineering\n\n### Feature Selection \nFeature Selection is one of the core concepts in advanced analytics. It entails selecting a few features with high predictive value or importance. This process can be manual or automated. There is no exact science around this and it really comes down to the creativity of the data scientist and domain knowledge about the dataset.\nA few techniques used for this are:\n1. Statistical approaches\n2. Correlation matrix esp. for predictive use cases\n3. Feature importance eg. using tree based methods\n\n### Feature Engineering\nFeature engineering is a technique to generate more meaningful features using the domain knowledge about the dataset. The idea is to create more meaningful features from the existing to better represent the data. Again, there are no strict rules around this and it is more art than science.\nSome data types such as image, video, audio etc. have too many features in raw form to have any real predictive power. In such cases, hand crafted rules or automated feature engineering (eg. neural networks) might be handy.\n\nAll in all, different preprocessing techniques can lead to the following benefits:\n1. Improve Accuracy\n2. Reduce training time\n3. The impact of noise\n4. Simpler models with better explainability\n","69643f29":"#### \ud83d\udd2cExplore","fde0de82":"#### \u2705Import Data","0a2d7e73":"#### \ud83d\udd2cLet's plot\nAs per our earlier exploration, a lot of columns looked empty. To confirm this, let's plot the percentage of missing values in each column.","f7861bec":"#### \ud83d\udc69\u200d\ud83d\udd2c Run modelling and prediction","f158a9f4":"#### \ud83d\udca1 Some conclusions\n- The dataset has ~350k food items with 163 features (columns). \n- Most of these features are nutrient values (Fat, carbohydrates, proteins) etc. along with a few text fields including product names, geographical details, allergens, ingredients etc.\n- A lot of columns look empty at first glance.\n\nAny more conclusions?","040f8a96":"#### \ud83d\udd2cBasic Stats","e9e95513":"# Evaluation\n- [Word Cloud](https:\/\/github.com\/amueller\/word_cloud)\n- Cluster statistics\n","b9c7fe12":"#### \ud83d\udd2cMore Exploration","fab077cf":"# Exploratory Data Analysis (EDA) \ud83d\udc69\u200d\ud83d\udd2c \ud83d\udc69\ud83c\udffe\u200d\ud83d\udd2c \ud83d\udc69\ud83c\udffd\u200d\ud83d\udd2c \ud83d\udc69\ud83c\udffb\u200d\ud83d\udd2c\n\nEDA is one of the most crucial steps of this advanced analytics. It entails learning about the intricacies of the data. You can go really crazy and mildly overboard \ud83e\udd2a in this step. Anything from visualizations to basic descriptive statistics, summaries, correlations, transformations! \n\nTry to answer these questions:\n- What does each column represent?\n- Is data structured or unstructured?\n- What are the column types - integers, geography, strings you name it!\n- Are some columns related to each other?\n- Does data have missing values?\n- Does data need to be cleaned or some values are incorrect?\n\n\n### Pandas\nPandas is a Python package providing fast, flexible, and expressive data structures designed to make working with \u201crelational\u201d or \u201clabeled\u201d data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. It has inbuilt functionalities to handle missing data, type conversions, slicing, merging datasets quickly. \n\nDataframe and Series are the basic data structures in pandas. \nGet started with Pandas: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/getting_started\/10min.html"}}