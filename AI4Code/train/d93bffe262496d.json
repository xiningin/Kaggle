{"cell_type":{"91da1ca6":"code","b94daaeb":"code","7ba1517b":"code","b8b19f64":"code","ded0343c":"code","85c1216d":"code","5bae984e":"code","316d17ad":"code","dc3dfda7":"code","e6bcca53":"code","ded93767":"code","b307f9dd":"code","117d8ec3":"code","22942903":"code","5b666e28":"code","b8cd08dc":"code","4713da47":"code","eb00d37b":"code","8cb242da":"code","86078247":"code","de69de30":"code","73c827cc":"code","218c1ffd":"code","40147f3d":"code","352c13a7":"code","98e81b2a":"code","f329f670":"code","da08d611":"markdown","84e1e5fe":"markdown","c40915c1":"markdown","c1ff7174":"markdown","3124e73e":"markdown","cc18e30f":"markdown","9ad48e88":"markdown","0c140e42":"markdown","eb8b7033":"markdown","f5fce36b":"markdown","f683d84b":"markdown","2ec30a0c":"markdown","3437f22d":"markdown","ba3e72aa":"markdown","0725481b":"markdown","08747dcc":"markdown","e8be7f39":"markdown","33a12de0":"markdown","cdc8beaa":"markdown","edf378b4":"markdown","f7b6b72c":"markdown","e994a5da":"markdown","99777006":"markdown","687ceb4f":"markdown","3d4424fe":"markdown","da720097":"markdown","d53b6110":"markdown","067e5217":"markdown","d201ac5f":"markdown","cc1b0e06":"markdown","a62449f7":"markdown","5e1a765e":"markdown"},"source":{"91da1ca6":"import pandas as pd\n\nwork_data = pd.read_csv('\/kaggle\/input\/79k-gaia-dr2-stars-crossmatched-with-hipparcos\/hipparcos-gaia-data.csv')\nlen(work_data)","b94daaeb":"work_data = work_data[work_data['parallax'] >= 2.0].reset_index(drop=True)\nlen(work_data)","7ba1517b":"import types\nimport numpy as np\nimport warnings\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler \n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nnp.random.seed(2019050001)\n\ndef get_cv_model_transform(data_frame, label_extractor, var_extractor, trainer_factory, response_column='response', \n                           id_column='source_id', n_runs=2, n_splits=2, max_n_training=None, scale=False,\n                           trim_fraction=None, classification=False):\n    '''\n    Creates a transform function that results from training an arbitrary regression model with k-fold\n    cross-validation. Multiple runs are averaged out. The transform function takes a frame and adds a \n    response column to it. Results are \"out of bag\" (not overfitted).\n    '''\n    default_model_list = []\n    sum_series = pd.Series([0] * len(data_frame)).astype(float)\n    for r in range(n_runs):\n        shuffled_frame = data_frame.sample(frac=1)\n        shuffled_frame.reset_index(inplace=True, drop=True)\n        response_frame = pd.DataFrame(columns=[id_column, 'response'])\n        kf = KFold(n_splits=n_splits)\n        first_fold = True\n        for train_idx, test_idx in kf.split(shuffled_frame):\n            train_frame = shuffled_frame.iloc[train_idx]\n            if trim_fraction is not None:\n                helper_labels = label_extractor(train_frame) if isinstance(label_extractor, types.FunctionType) else train_frame[label_extractor] \n                train_label_ordering = np.argsort(helper_labels)\n                orig_train_len = len(train_label_ordering)\n                head_tail_len_to_trim = int(round(orig_train_len * trim_fraction * 0.5))\n                assert head_tail_len_to_trim > 0\n                trimmed_ordering = train_label_ordering[head_tail_len_to_trim:-head_tail_len_to_trim]\n                train_frame = train_frame.iloc[trimmed_ordering]\n            if max_n_training is not None:\n                train_frame = train_frame.sample(max_n_training)\n            train_labels = label_extractor(train_frame) if isinstance(label_extractor, types.FunctionType) else train_frame[label_extractor]\n            test_frame = shuffled_frame.iloc[test_idx]\n            train_vars = var_extractor(train_frame)\n            test_vars = var_extractor(test_frame)\n            scaler = None\n            if scale:\n                scaler = StandardScaler()  \n                scaler.fit(train_vars)\n                train_vars = scaler.transform(train_vars)  \n                test_vars = scaler.transform(test_vars) \n            trainer = trainer_factory()\n            fold_model = trainer.fit(train_vars, train_labels)\n            test_responses = fold_model.predict_proba(test_vars)[:,1] if classification else fold_model.predict(test_vars)\n            test_id = test_frame[id_column]\n            assert len(test_id) == len(test_responses)\n            fold_frame = pd.DataFrame({id_column: test_id, 'response': test_responses})\n            response_frame = pd.concat([response_frame, fold_frame], sort=False)\n            if first_fold:\n                first_fold = False\n                default_model_list.append((scaler, fold_model,))\n        response_frame.sort_values(id_column, inplace=True)\n        response_frame.reset_index(inplace=True, drop=True)\n        assert len(response_frame) == len(data_frame), 'len(response_frame)=%d' % len(response_frame)\n        sum_series += response_frame['response']\n    cv_response = sum_series \/ n_runs\n    assert len(cv_response) == len(data_frame)\n    assert len(default_model_list) == n_runs\n    response_map = dict()\n    sorted_id = np.sort(data_frame[id_column].values) \n    for i in range(len(cv_response)):\n        response_map[str(sorted_id[i])] = cv_response[i]\n    response_id_set = set(response_map)\n    \n    def _transform(_frame):\n        _in_trained_set = _frame[id_column].astype(str).isin(response_id_set)\n        _trained_frame = _frame[_in_trained_set].copy()\n        _trained_frame.reset_index(inplace=True, drop=True)\n        if len(_trained_frame) > 0:\n            _trained_id = _trained_frame[id_column]\n            _tn = len(_trained_id)\n            _response = pd.Series([None] * _tn).astype(float)\n            for i in range(_tn):\n                _response[i] = response_map[str(_trained_id[i])]\n            _trained_frame[response_column] = _response\n        _remain_frame = _frame[~_in_trained_set].copy()\n        _remain_frame.reset_index(inplace=True, drop=True)\n        if len(_remain_frame) > 0:\n            _unscaled_vars = var_extractor(_remain_frame)\n            _response_sum = pd.Series([0] * len(_remain_frame)).astype(float)\n            for _model_tuple in default_model_list:\n                _scaler = _model_tuple[0]\n                _model = _model_tuple[1]\n                _vars = _unscaled_vars if _scaler is None else _scaler.transform(_unscaled_vars)\n                _response = _model.predict_proba(_vars)[:,1] if classification else _model.predict(_vars)\n                _response_sum += _response\n            _remain_frame[response_column] = _response_sum \/ len(default_model_list)\n        _frames_list = [_trained_frame, _remain_frame]\n        _result = pd.concat(_frames_list, sort=False)\n        _result.reset_index(inplace=True, drop=True)\n        return _result\n    \n    return _transform\n\nimport scipy.stats as stats\n\ndef print_evaluation(data_frame, label_column, response_column):\n    '''\n    Compares a label with a model response and prints RMSE and correlation statistics.\n    '''\n    response = response_column(data_frame) if isinstance(response_column, types.FunctionType) else data_frame[response_column]\n    label = label_column(data_frame) if isinstance(label_column, types.FunctionType) else data_frame[label_column]\n    residual = label - response\n    rmse = np.sqrt(sum(residual ** 2) \/ len(data_frame))\n    correl = stats.pearsonr(response, label)[0]\n    print('RMSE: %.5f | Correlation: %.4f' % (rmse, correl,), flush=True)","b8b19f64":"def extract_vars(data_frame):\n    return np.transpose([\n        1.0 \/ data_frame['parallax'],\n        data_frame['phot_g_mean_mag'],\n        data_frame['phot_bp_mean_mag'],\n        data_frame['phot_rp_mean_mag'],\n    ])","ded0343c":"from sklearn.linear_model import LinearRegression\n\nLABEL_COLUMN = 'hpmag'\n\ndef extract_label(data_frame):\n    return data_frame[LABEL_COLUMN]\n\n\ndef get_base_trainer():\n    return LinearRegression()\n\n\nbase_transform = get_cv_model_transform(work_data, extract_label, extract_vars, get_base_trainer, \n        n_runs=3, n_splits=5, max_n_training=None, response_column='base_response' , scale=False)\nwork_data = base_transform(work_data)\nprint_evaluation(work_data, LABEL_COLUMN, 'base_response')","85c1216d":"from sklearn.neural_network import MLPRegressor\n\ndef base_residual_transform(data_frame):\n    new_frame = data_frame.copy()\n    new_frame['base_residual'] = new_frame[LABEL_COLUMN] - new_frame['base_response']\n    return new_frame\n\n\ndef extract_res_vars(data_frame):\n    g_mag = data_frame['phot_g_mean_mag']\n    bp_mag = data_frame['phot_bp_mean_mag']\n    rp_mag = data_frame['phot_rp_mean_mag']\n    btmag = data_frame['btmag']\n    hpmag = data_frame['hpmag']\n    vmag = data_frame['vmag']\n    vtmag = data_frame['vtmag']\n    return np.transpose([\n        g_mag - bp_mag, \n        rp_mag - g_mag,\n        btmag - vmag,\n        vmag - vtmag,\n        hpmag - vmag,\n        hpmag - vtmag,\n        btmag - vtmag,\n    ])\n\n\ndef extract_res_label(data_frame):\n    return data_frame['base_residual']\n\n\ndef get_res_trainer():\n    return MLPRegressor(hidden_layer_sizes=(20), max_iter=400, alpha=0.1, random_state=np.random.randint(1,10000))\n\n\nwork_data = base_residual_transform(work_data)\nres_transform = get_cv_model_transform(work_data, extract_res_label, extract_res_vars, get_res_trainer, \n        n_runs=3, n_splits=3, max_n_training=None, response_column='modeled_residual' , scale=True)\nwork_data = res_transform(work_data)\nprint_evaluation(work_data, 'base_residual', 'modeled_residual')","5bae984e":"def mag_change_transform(data_frame):\n    new_frame = data_frame.copy()\n    new_frame['prelim_mag_change_estimate'] = new_frame['modeled_residual'] - new_frame['base_residual']\n    return new_frame\n\n\nwork_data = mag_change_transform(work_data)","316d17ad":"import plotly.offline as py\nimport plotly.graph_objs as go\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npy.init_notebook_mode(connected=False)\n\ndef line_plot(bins_x, bins_y, lower_y=None, upper_y=None, x_range=None, x_label='', y_range=None, y_label='', title=''):\n    trace1 = go.Scatter(\n        x=bins_x,\n        y=bins_y,\n        mode='lines',\n        line=dict(width=4),\n        marker=dict(\n            size=3,\n        ),\n        showlegend=False\n    )\n    scatter_data = [trace1]\n    if lower_y is not None and upper_y is not None:\n        scatter_data.append(go.Scatter(\n            x=bins_x,\n            y=upper_y,\n            marker=dict(color=\"#444\"),\n            line=dict(width=0),\n            mode='lines',\n            showlegend=False\n        ))\n        scatter_data.append(go.Scatter(\n            x=bins_x,\n            y=lower_y,\n            marker=dict(color=\"#444\"),\n            line=dict(width=0),\n            mode='lines',\n            fillcolor='rgba(68, 68, 68, 0.1)',\n            fill='tonexty',\n            showlegend=False\n        ))\n        \n    \n    layout = go.Layout(\n        title=title,\n        xaxis=dict(\n            title=x_label,\n            range=x_range,\n        ),\n        yaxis=dict(\n            title=y_label,\n            range=y_range,\n        ),\n    )\n    fig = go.Figure(data=scatter_data, layout=layout)\n    py.iplot(fig)\n\n    \ndef plot_binned(data_frame, x_column, y_column, step=0.1, x_range=None, x_label='', y_range=None, y_label='', title=''):\n    x = data_frame[x_column].astype(float)\n    factor = 1.0 \/ step\n    bin_x = np.round(x * factor) \/ factor\n    work_frame = data_frame.assign(__bin_x = bin_x)\n    groups = work_frame.groupby('__bin_x')\n    bins_x = []\n    bins_y = []\n    lower_y = []\n    upper_y = []\n    for group in groups:\n        key = group[0]\n        group_frame = group[1]\n        len_gf = len(group_frame)\n        if len_gf >= 3:\n            values = group_frame[y_column].values\n            mean_value = np.mean(values)\n            std_value = np.std(values) \/ np.sqrt(len_gf)\n            upper_bound = mean_value + std_value * 1.96\n            lower_bound = mean_value - std_value * 1.96\n            bins_x.append(key)\n            bins_y.append(mean_value)\n            lower_y.append(lower_bound)\n            upper_y.append(upper_bound)\n    line_plot(bins_x, bins_y, upper_y=upper_y, lower_y=lower_y, x_range=x_range, \n              x_label=x_label, y_range=y_range, y_label=y_label, title=title)\n    \n    \nplot_binned(work_data, 'phot_g_mean_mag', 'prelim_mag_change_estimate', \n            x_range=None, x_label='Gaia G magnitude', step=0.05,\n            y_range=[-0.2,0.2], y_label='Preliminary magnitude change estimate',\n            title='Magnitude (binned) vs. magnitude change estimate')","dc3dfda7":"work_data['distance'] = 1000.0 \/ work_data['parallax']\nplot_binned(work_data, 'distance', 'prelim_mag_change_estimate', \n            x_range=None, x_label='Parallax distance (Parsecs)', step=5,\n            y_range=[-0.05,0.05], y_label='Preliminary magnitude change estimate',\n            title='Distance (binned) vs. magnitude change estimate')","e6bcca53":"from sklearn.ensemble import RandomForestRegressor\n\ndef extract_posbias_vars(data_frame):\n    ra = np.deg2rad(data_frame['ra'].values)\n    dec = np.deg2rad(data_frame['dec'].values)\n    return np.transpose([ra, dec])    \n\n\ndef get_posbias_trainer():\n    return RandomForestRegressor(n_estimators=60, max_depth=18, min_samples_split=30, random_state=np.random.randint(1,10000))\n\n\ntransform_posbias = get_cv_model_transform(work_data, 'prelim_mag_change_estimate', extract_posbias_vars, get_posbias_trainer, \n        n_runs=3, n_splits=2, max_n_training=None, response_column='modeled_posbias', scale=False,\n        trim_fraction=0.003)\nwork_data = transform_posbias(work_data)\nprint_evaluation(work_data, 'prelim_mag_change_estimate', 'modeled_posbias')","ded93767":"def scatter_plot(bins_x, bins_y, colors, x_range=None, x_label='', y_range=None, y_label='', title='', colorbar_title=''):\n    trace1 = go.Scatter(\n        x=bins_x,\n        y=bins_y,\n        mode='markers',\n        marker=dict(\n            size=5,\n            color=colors,\n            colorbar=dict(\n                title=colorbar_title\n            ),\n        ),\n        showlegend=False\n    )\n    scatter_data = [trace1]\n    layout = go.Layout(\n        title=title,\n        xaxis=dict(\n            title=x_label,\n            range=x_range,\n        ),\n        yaxis=dict(\n            title=y_label,\n            range=y_range,\n        ),\n    )\n    fig = go.Figure(data=scatter_data, layout=layout)\n    py.iplot(fig)\n\n    \nsample_data = work_data.sample(n=10000)\nscatter_plot(sample_data['ra'], sample_data['dec'], sample_data['modeled_posbias'],\n             x_label='Right ascension (degrees)', y_label='Declination (degrees)', colorbar_title='Modeled residual',\n             title='Responses of model of blend residual<br>by position in the sky')","b307f9dd":"pos_set_high = work_data.sort_values('modeled_posbias', ascending=False).head(1000)\npos_set_low = work_data.sort_values('modeled_posbias', ascending=True).head(1000)\n\nplot_binned(pos_set_high, 'phot_g_mean_mag', 'prelim_mag_change_estimate', \n            x_range=None, x_label='Gaia G magnitude', step=0.1,\n            y_range=[-0.2,0.2], y_label='Preliminary magnitude change estimate',\n            title='Magnitude (binned) vs. magnitude change estimate<br>' +\n            'for \"high\" outliers in position-based model')","117d8ec3":"plot_binned(pos_set_low, 'phot_g_mean_mag', 'prelim_mag_change_estimate', \n            x_range=None, x_label='Gaia G magnitude', step=0.1,\n            y_range=[-0.2,0.2], y_label='Preliminary magnitude change estimate',\n            title='Magnitude (binned) vs. magnitude change estimate<br>' +\n            'for \"low\" outliers in position-based model')","22942903":"def extract_mbias_vars(data_frame):\n    return data_frame[['phot_g_mean_mag', 'phot_bp_mean_mag', 'phot_rp_mean_mag', 'distance']]\n\n\ndef get_mbias_trainer():\n    return MLPRegressor(hidden_layer_sizes=(15), max_iter=500, alpha=0.1, random_state=np.random.randint(1,10000))\n\n\nmbias_transform = get_cv_model_transform(work_data, 'prelim_mag_change_estimate', extract_mbias_vars, get_mbias_trainer, \n        n_runs=5, n_splits=3, max_n_training=None, response_column='mbias_response', scale=True, \n        trim_fraction=0.005)\nwork_data = mbias_transform(work_data)\nprint_evaluation(work_data, 'prelim_mag_change_estimate', 'mbias_response')","5b666e28":"work_data['mag_change_estimate'] = work_data['prelim_mag_change_estimate'] - work_data['mbias_response']\nplot_binned(work_data, 'phot_g_mean_mag', 'mag_change_estimate', \n            x_range=None, x_label='Gaia G magnitude', step=0.05,\n            y_range=[-0.2,0.2], y_label='Corrected magnitude change estimate',\n            title='Magnitude (binned) vs. magnitude change estimate')","b8cd08dc":"plot_binned(work_data, 'distance', 'mag_change_estimate', \n            x_range=None, x_label='Parallax distance (Parsecs)', step=5,\n            y_range=[-0.05,0.05], y_label='Corrected magnitude change estimate',\n            title='Distance (binned) vs. magnitude change estimate')","4713da47":"work_data['mag_change_estimate'].astype(float).describe()","eb00d37b":"def plot_distribution(x, xlabel, xrange=None, yrange=None, xbins=None, lineX=None, title=None):\n    hist_trace = go.Histogram(\n        x=x,\n        xbins=xbins,\n    )\n    data = [hist_trace]\n    shapes = None\n    if lineX is not None:\n        shapes = [\n            { \n                'type': 'line', 'yref': 'paper', 'x0': lineX, 'x1': lineX, 'y0': 0, 'y1': 1.0, \n                'line': { 'color': 'orange', 'width': 3, 'dash': 'dashdot',},\n            }\n        ]\n    layout = go.Layout(barmode='overlay', \n        title=title,\n        xaxis=dict(\n            title=xlabel,\n            range=xrange,\n        ),\n        yaxis=dict(\n            title='Frequency',\n            range=yrange,\n        ),\n        shapes=shapes,\n    )\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig)\n    \n    \nplot_distribution(work_data['mag_change_estimate'], 'Magnitude change', xrange=[-0.5,0.8], yrange=[0,200], title='Distribution of approx. magnitude change in ~25 years')","8cb242da":"anom_threshold = np.std(work_data['mag_change_estimate']) * 3\nextreme_dimmers = work_data[work_data['mag_change_estimate'] >= anom_threshold]\ncomparison_sample = work_data.sample(n=len(extreme_dimmers))\n\ned_pm = np.sqrt(extreme_dimmers['pmra'] ** 2 + extreme_dimmers['pmdec'] ** 2)\ncs_pm = np.sqrt(comparison_sample['pmra'] ** 2 + comparison_sample['pmdec'] ** 2)\n\ned_pm.describe()","86078247":"cs_pm.describe()","de69de30":"ed_ef = extreme_dimmers['phot_g_mean_flux_error'] \/ extreme_dimmers['phot_g_mean_flux']\ncs_ef = comparison_sample['phot_g_mean_flux_error'] \/ comparison_sample['phot_g_mean_flux']\n\ned_ef.describe()","73c827cc":"cs_ef.describe()","218c1ffd":"from sklearn.ensemble import RandomForestRegressor\n\ndef extract_error_vars(data_frame):\n    flux_error = data_frame['phot_g_mean_flux_error']\n    flux = data_frame['phot_g_mean_flux']\n    gaia_error_feature = np.log((flux + flux_error) \/ flux) ** 2\n    \n    return np.transpose([\n        gaia_error_feature,\n        data_frame['parallax_error'],\n        flux,\n    ])\n\n\ndef get_residual_trainer():\n    return RandomForestRegressor(n_estimators=100, max_depth=8, min_samples_split=10, random_state=np.random.randint(1,10000))\n\n\ntransform_em_bias = get_cv_model_transform(work_data, 'mag_change_estimate', extract_error_vars, get_residual_trainer, \n        n_runs=4, n_splits=2, response_column='expected_em_mc', scale=False,\n        trim_fraction=None)\nwork_data = transform_em_bias(work_data)\nprint_evaluation(work_data, 'mag_change_estimate', 'expected_em_mc')","40147f3d":"def get_squared_mag_change(data_frame):\n    return (data_frame['mag_change_estimate'] - data_frame['expected_em_mc']) ** 2\n\ntransform_expected_mc_sq = get_cv_model_transform(work_data, get_squared_mag_change, extract_error_vars, get_residual_trainer, \n        n_runs=4, n_splits=2, response_column='expected_mc_sq', scale=False,\n        trim_fraction=None)\nwork_data = transform_expected_mc_sq(work_data)\nprint_evaluation(work_data, get_squared_mag_change, 'expected_mc_sq')","352c13a7":"work_data = work_data.assign(mag_change_std_error = np.sqrt(work_data['expected_mc_sq'].astype(float)))\nwork_data = work_data.assign(updated_mag_change_estimate = work_data['mag_change_estimate'] - work_data['expected_em_mc'])\nwork_data = work_data.assign(mag_change_anomaly = work_data['updated_mag_change_estimate'] \/ work_data['mag_change_std_error'])","98e81b2a":"np.std(work_data['mag_change_anomaly'])","f329f670":"saved_data = pd.DataFrame({\n    'source_id': work_data['source_id'],\n    'mag_change_est_linear': -work_data['base_residual'],\n    'mag_change_est': work_data['updated_mag_change_estimate'],\n    'mag_change_std_error': work_data['mag_change_std_error'],\n    'mag_change_anomaly': work_data['mag_change_anomaly'],\n})\n\nsaved_data.round(4).to_csv('hipparcos-gaia-mag-change-estimate.csv', index=False)","da08d611":"## Non-linear model of residuals\n\nDataset magnitudes are in different bands, and much of the error could be due to spectrophotometric peculiarities of different stars (in addition to crossmatching error, systematics and so forth.)\n\nIn order to deal with spectrophotometric error, we will train a Neural Network that models the linear residual as a function of color features (i.e. differences between magnitudes of different bands in each dataset.) Note that no cross-database differences should be used as variables of this model, for obvious reasons.","84e1e5fe":"How does a random sample of the same size compare?","c40915c1":"This is magnitude vs. magnitude change for \"low\" outliers:","c1ff7174":"We will add some columns to the dataset:\n* *mag_change_std_error*: The square root of the modeled square error (similar to an RMSE or standard deviation.)\n* *updated_mag_change_estimate*: The new magnitude change due to the bias modeled in the variable space of the error model.\n* *mag_change_anomaly*: The new magnitude change estimate divided by the modeled standard error.","3124e73e":"This bias model improves our overall accuracy.\n\nNow we calculate a new model residual, and then we model its square. Because we're using a Random Forest, responses of the error model can be thought of as regional mean squares of deviations from the model, in the variable space.","cc18e30f":"## Distribution and outliers\nLet's look at summary statistics of the *mag_change_estimate* metric (approx. magnitude change in ~25 years).","9ad48e88":"There are some indications of bias in certain places, but this mostly looks like noise. Just to make sure there aren't complex interactions\/systematics between distance or magnitude and position in the sky, we will get two sets of a thousand outliers at the two ends of the modeled position bias, and analyze how they behave.\n\nThe following plot shows magnitude vs. estimated magnitude change for \"high\" outliers.","0c140e42":"The distribution is quite skewed right, as we can see in the following histogram, where we've truncated the y axis.","eb8b7033":"In both outlier datasets, magnitude bias is consistent with what we see in the whole dataset. We will not correct for position biases. While model accuracy would improve a little, the trade-off is that we would lose some position-based information regarding regional dimming or brightening.","f5fce36b":"Distance (below) does not have nearly as much bias, and it has much less variance, even though there's plenty of uncertainty in parallax observations. This can be explained by observing that any any given distance in the dataset, there are stars of different luminosity.","f683d84b":"We also get a slight correction of distance-dependent biases:","2ec30a0c":"## The dataset\nWe will use a Kaggle dataset titled _[79K Gaia DR2 Stars Crossmatched With Hipparcos](https:\/\/www.kaggle.com\/solorzano\/79k-gaia-dr2-stars-crossmatched-with-hipparcos)_. The number of rows in the dataset is:","3437f22d":"The standard deviation of the *mag_change_anomaly* metric should theoretically be 1.0, provided there are no extreme outliers.","ba3e72aa":"## Output file\nWe now dump model results to a file, with the following columns:\n* *source_id*: The Gaia DR2 ID of the star.\n* *mag_change_est_linear*: The magnitude change estimated with the simple linear model.\n* *mag_change_est*: The modeled magnitude change from Hipparcos to Gaia.\n* *mag_change_std_error*: The square root of the modeled square error.\n* *mag_change_anomaly*: The magnitude change estimate divided by the modeled standard error.","0725481b":"In this case, there's an evident issue. And it shouldn't be surprising. The higher the Gaia flux error, the less certainty we should have about the magnitude change estimate. If we were calculating a linear regression slope, we would want to estimate the standard slope error. The more noisy the data, the higher the standard error. We need to do something similar in this case, i.e. model the error.","08747dcc":"## Base model\nWe will start with a simple linear model. The variables will be the 3 magnitude bands made available by Gaia DR2, plus parallax distance in kilo-parsecs.","e8be7f39":"## Magnitude bias analysis\nWe've looked at possible model biases in distance, galactic latitude, celestial declination and magnitude. With magnitude, there does appear to be a bias that should be corrected. Below is a binned line plot of Gaia G magnitude vs. the estimated magnitude change from Hipparcos to Gaia.","33a12de0":"This is a good improvement over the linear model (~34%).","cdc8beaa":"## Magnitude bias correction\n\nWe will attempt to correct the magnitude-dependent bias with a Neural Network. We will use Gaia magnitudes as variables of the correction model. Hipparcos magnitudes should not be used as variables, as they would leak information, given that our original label is a Hipparcos magnitude. This is true regardless of the fact that we're only modeling a residual of the original model.\n\nWe include parallax distance as a variable of the model as well. A magnitude correction could introduce distance-based distortions.","edf378b4":"Position-dependent biases are depicted below in a color-coded scatter chart.","f7b6b72c":"How does it compare to a random sample?","e994a5da":"## Exclusion of distant stars\nWe're only interested in stars that are within 500 parsecs of the sun. Limiting distance prior to modeling also improves model accuracy. The resulting number of rows is now:","99777006":"## Preliminary magnitude change estimate\nThe difference between the residual of the base linear model and the modeled residual is an approximation of the magnitude change from Gaia to Hipparcos. We want the negative of this (Hipparcos to Gaia).","687ceb4f":"## Modeling helper functions\nWe use a function named *get_cv_model_transform* to do model training using an arbitrary Machine Learning algorithm. The function averages multiple runs of k-fold cross-validation, and produces a transform function that can be applied to any dataframe with the required variables. Model results are \"out of bag\" (i.e. not overfitted to the training data.)","3d4424fe":"Proper motion of the random sample is slightly higher, so proper motion doesn't explain the dim outliers.\n\nWe can also look at summary statistics of the Gaia flux error of the extreme dimmers.","da720097":"## Acknowledgements\n\nThis work has made use of data from the European Space Agency (ESA) mission Gaia (https:\/\/www.cosmos.esa.int\/gaia), processed by the Gaia Data Processing and Analysis Consortium (DPAC, https:\/\/www.cosmos.esa.int\/web\/gaia\/dpac\/consortium). Funding for the DPAC has been provided by national institutions, in particular the institutions participating in the Gaia Multilateral Agreement.\n\nThe Hipparcos and Tycho Catalogues are the primary products of the European Space Agency's astrometric mission, Hipparcos. The satellite, which operated for four years, returned high quality scientific data from November 1989 to March 1993.","d53b6110":"## Position bias analysis\nWe have observed that stellar datasets, particularly those produced using observations from the ground, contain line-of-sight distortions. We will model the preliminary magnitude change estimate as a function of Right Ascension and Declination coordinates, using a Random Forest, to determine what we're dealing with.","067e5217":"Let's get a set of 3-sigma dimmers and check the summary statistics of their proper motions.","d201ac5f":"After correction, this is what the magnitude vs. magnitude change plot looks like:","cc1b0e06":"## Squared magnitude change (i.e. squared error) modeling\nWe will model the square of the magnitude change estimate as a function of the Gaia magnitude error, the parallax error, and the Gaia flux, using a Random Forest.\n\nIt's not enough to model the the squared residual so far, as that residual could have biases in the variable space we're using to train the error model. So we first train for bias using the same variable space and training algorithm that we will use to model squared residuals.","a62449f7":"The training label will be <code>hpmag<\/code>, which is the median Hipparcos magnitude. This is the magnitude that seems to produce the most accurate model. Model evaluation follows:","5e1a765e":"## Overview\nThe Hipparcos mission obtained magnitude data for 118K stars beginning in 1989. The Gaia mission was launched in 2013 to gather data on about a billion objects. We model one of the Hipparcos magnitudes as a function of those from Gaia DR2 and derive an approximation of magnitude change in ~25 years.\n\nWe also model the standard error of the magnitude change. A *mag_change_anomaly* metric is provided along with other results in the Output section of this notebook."}}