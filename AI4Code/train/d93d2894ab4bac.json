{"cell_type":{"01551b15":"code","abecf66e":"code","ec387abf":"code","70c0c4e5":"code","87b78f65":"code","daf83733":"code","c7979559":"code","4f272a89":"code","5ae98507":"code","e5db4456":"code","cc4ab1ce":"code","0b98bf11":"code","3e5fc2e2":"code","3551a379":"code","2d0b191b":"code","aacd87d2":"code","7c90cbf6":"code","95a69007":"code","8ca37653":"code","cf478034":"code","11f93094":"code","f474b99e":"code","2cfe80fd":"code","3f1b57fa":"code","0a74cc03":"code","fc6bff0c":"code","fad6b192":"code","e0f943c4":"code","755702ed":"code","54871ec0":"code","1bb21213":"code","b2d766a5":"code","87a35129":"code","efe087b2":"code","4ec6a073":"code","407bb61a":"code","911d7163":"code","53f7b0c2":"code","0e65fc4a":"code","c47fd2e2":"code","1b9b3b09":"code","a43f4924":"code","e1176f16":"code","d57448a8":"code","18ea9e34":"code","f9f81758":"code","448c6f04":"code","eb9ea043":"code","5aa08a88":"code","a7b2fea9":"code","32769052":"code","e1f8e19d":"code","c3f7eda4":"code","ad495d96":"code","b2cddcd2":"code","e103269c":"code","c0e65d0f":"code","e0993bcd":"code","3a94ffe4":"code","f7251c78":"code","700ea267":"code","23919236":"code","7d19e817":"markdown","0350ddd7":"markdown","37758603":"markdown","121e6b2c":"markdown","79ece73e":"markdown","40758cf0":"markdown","5d3a74fe":"markdown","1819b2fd":"markdown","14cf43c0":"markdown","0bc96b1a":"markdown","81f55aea":"markdown","1a20435a":"markdown","76a42cbc":"markdown","ca22576a":"markdown","90ee3256":"markdown","01b69bd6":"markdown","3782fab3":"markdown","d29721dd":"markdown","250d2749":"markdown","bd54f963":"markdown","0892bb37":"markdown","c6a19d54":"markdown","7b6eb8be":"markdown","0374c628":"markdown","6335427e":"markdown","290bdd82":"markdown","6611aba7":"markdown","d79b0228":"markdown","c25ffa48":"markdown","0400de39":"markdown","fb0e852a":"markdown","8e6b2530":"markdown","06b997ac":"markdown","bce3432b":"markdown","95561a54":"markdown","73c850dc":"markdown","f6f1a98d":"markdown","3e4ee9ae":"markdown","3a61c672":"markdown","49b1ecfc":"markdown","3e336047":"markdown","8fbc39ac":"markdown","79ad01ab":"markdown","60d6925d":"markdown","c201204f":"markdown","4cdc6d47":"markdown","f69aa36f":"markdown","f816947b":"markdown","03231c00":"markdown","d4c1b575":"markdown","5f93f6e9":"markdown","79695c73":"markdown","d237c829":"markdown","9f0cee16":"markdown"},"source":{"01551b15":"!pip install split-folders","abecf66e":"# Third party\nfrom keras.applications import VGG16\nfrom keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D\nfrom keras.models import Sequential, Model\nfrom keras.applications import VGG16, VGG19, Xception, MobileNetV2,InceptionResNetV2,DenseNet201\nfrom keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nimport numpy as np\nimport splitfolders\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau,LearningRateScheduler\nfrom keras.constraints import maxnorm\nfrom keras.layers import Dropout\nfrom keras.layers import GaussianNoise\n\n\n# Local application\nimport miner_a_de_datos_redes_neuronales_utilidad as utils","ec387abf":"device_type = \"GPU\"\n\nutils.check_physical_devices(device_type)","70c0c4e5":"seed = 27912\nratio = (0.8, 0.2)\ngroup_prefix = None","87b78f65":"input_folder = \"..\/input\/recognition2020\/train\"\noutput_folder = \"..\/new_input\/recognition2020\"","daf83733":"splitfolders.ratio(input_folder,\n                   output=output_folder,\n                   seed=seed,\n                   ratio=(0.8, 0.2),\n                   group_prefix=None)","c7979559":"rescale = 1 \/ 255","4f272a89":"train_generator = ImageDataGenerator(rescale=rescale,\n                                     width_shift_range=0.1,\n                                     height_shift_range=0.1,\n                                     zoom_range=0.1,\n                                     shear_range=0.1,\n                                     #brightness_range=[0.2,1.0]\n                                    )","5ae98507":"validation_generator = ImageDataGenerator(rescale=rescale)","e5db4456":"test_generator = ImageDataGenerator(rescale=rescale)","cc4ab1ce":"batch_size = 64\ntarget_size = (128, 128)","0b98bf11":"directory = \"..\/new_input\/recognition2020\/train\"\n\ntrain_iterator = train_generator.flow_from_directory(seed=seed,\n                                                     directory=directory,\n                                                     batch_size=batch_size,\n                                                     target_size=target_size)","3e5fc2e2":"directory = \"..\/new_input\/recognition2020\/val\"\n\nvalidation_iterator = validation_generator.flow_from_directory(seed=seed,\n                                                          directory=directory,\n                                                          batch_size=batch_size,\n                                                          target_size=target_size)","3551a379":"directory = \"..\/input\/recognition2020\/test\"\nshuffle = False\n\ntest_iterator = test_generator.flow_from_directory(seed=seed,\n                                                   shuffle=shuffle,\n                                                   directory=directory,\n                                                   batch_size=batch_size,\n                                                   target_size=target_size)","2d0b191b":"data_format = \"channels_last\"\ninput_shape = (*target_size, 3)","aacd87d2":"activation = \"relu\"\n\nnoise_layer = GaussianNoise(0.2,input_shape=input_shape)","7c90cbf6":"convolutional_layer1 = Conv2D(64, 3, activation=activation)\nconvolutional_layer2 = Conv2D(128, 3, activation=activation)\nconvolutional_layer3 = Conv2D(256, 3, activation=activation)\nconvolutional_layer4 = Conv2D(512, 3, activation=activation)\n","95a69007":"max_pooling_layer1 = MaxPooling2D(2)\nmax_pooling_layer2 = MaxPooling2D(2)\nmax_pooling_layer3 = MaxPooling2D(2)\nmax_pooling_layer4 = MaxPooling2D(2)","8ca37653":"hidden_layer = Dense(512, activation=activation)\nhidden_layer2 = Dense(256, activation=activation)","cf478034":"flatten_layer = Flatten(data_format=data_format)","11f93094":"dropout_layer1 = Dropout(0.2)\ndropout_layer2 = Dropout(0.3)\n","f474b99e":"activation = \"softmax\"\n\noutput_layer = Dense(10, activation=activation)","2cfe80fd":"convolutional_neural_network_model = utils.group_layers(noise_layer,\n                                                        convolutional_layer1,\n                                                        max_pooling_layer1,\n                                                        dropout_layer1,\n                                                        convolutional_layer2,\n                                                        max_pooling_layer2,\n                                                        dropout_layer2,\n                                                        convolutional_layer3,\n                                                        max_pooling_layer3,\n                                                        convolutional_layer4,\n                                                        max_pooling_layer4,\n                                                        flatten_layer,\n                                                        hidden_layer,\n                                                        hidden_layer2,\n                                                        output_layer)","3f1b57fa":"lr = 1e-3","0a74cc03":"line_length = 79","fc6bff0c":"convolutional_neural_network_model.summary(line_length)","fad6b192":"model = convolutional_neural_network_model\n\nutils.compile_model(model, lr)","e0f943c4":"weights = \"imagenet\"\ninclude_top = False\n\nxception_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n\n","755702ed":"model = xception_model\n\nutils.freeze_layers(model)","54871ec0":"last_layer = xception_model.output","1bb21213":"flatten_layer = Flatten(data_format=data_format)(last_layer)","b2d766a5":"activation = \"relu\"\n\nnoise_layer = GaussianNoise(0.1)(flatten_layer)\n\nhidden_layer_1 = Dense(512, activation=activation)(noise_layer)\n\ndropout_layer_1 = Dropout(rate=0.15)(hidden_layer_1)\n\nhidden_layer_2 = Dense(256, activation=activation)(dropout_layer_1)\n\ndropout_layer_2 = Dropout(rate=0.15)(hidden_layer_2)\n\nhidden_layer_3 = Dense(128, activation=activation)(dropout_layer_2)\n\ndropout_layer_3 = Dropout(rate=0.15)(hidden_layer_3)","87a35129":"activation = \"softmax\"\n\noutput_layer = Dense(10, activation=activation)(dropout_layer_3)","efe087b2":"input_layer = xception_model.input","4ec6a073":"transfer_learning_model = Model(inputs=input_layer, outputs=output_layer)","407bb61a":"transfer_learning_model.summary(line_length)","911d7163":"model = transfer_learning_model\n\nutils.compile_model(model, lr)","53f7b0c2":"noise_layer = GaussianNoise(0.2, input_shape=input_shape)","0e65fc4a":"dropout_layer1 = Dropout(0.2)\ndropout_layer2 = Dropout(0.2)","c47fd2e2":"flatten_layer = Flatten(data_format=data_format, input_shape=input_shape)","1b9b3b09":"activation = \"relu\"\nlayer_1 = Dense(512, activation=activation)\nlayer_2 = Dense(512, activation=activation)\nlayer_3 = Dense(256, activation=activation)\nlayer_4 = Dense(256, activation=activation)\nlayer_5 = Dense(128, activation=activation)\nlayer_6 = Dense(128, activation=activation)","a43f4924":"activation = \"softmax\"\n\noutput_layer = Dense(10, activation=activation)","e1176f16":"perceptron_neural_network_model = utils.group_layers(noise_layer,\n                                                     flatten_layer,\n                                                        layer_1,\n                                                        layer_2,\n                                                        dropout_layer1,\n                                                        layer_3,\n                                                        layer_4,\n                                                        dropout_layer2,\n                                                        layer_5,\n                                                        layer_6,\n                                                        output_layer)","d57448a8":"line_length = 79","18ea9e34":"perceptron_neural_network_model.summary(line_length)","f9f81758":"lr = 1e-3","448c6f04":"model = perceptron_neural_network_model\n\nutils.compile_model(model, lr)\n\n","eb9ea043":"epochs = 20\ngenerator = train_iterator\nvalidation_data = validation_iterator","5aa08a88":"def scheduler(epoch, lr):\n    if epoch < 10:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)","a7b2fea9":"model = transfer_learning_model\n\ntransfer_learning_history = model.fit_generator(epochs=epochs,\n                  generator=generator,\n                  callbacks=[EarlyStopping(monitor = 'val_loss', patience = 3, restore_best_weights = True),\n                  ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001, verbose=2)],\n                  validation_data=validation_data)","32769052":"epochs = 15\nmodel2 = convolutional_neural_network_model\n\nconvolutional_neural_network_history = model2.fit_generator(epochs=epochs,\n        generator=generator,\n        callbacks=[EarlyStopping(monitor = 'val_loss', patience = 3, restore_best_weights = True),\n                  ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001, verbose=2),\n                  #LearningRateScheduler(lr_decay, verbose=1)\n                  ],\n        validation_data=validation_data)","e1f8e19d":"model3 = perceptron_neural_network_model\n\nperceptron_neural_network_model_history = model3.fit_generator(epochs=20,\n        generator=generator,\n        callbacks=[EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3, verbose = 1, restore_best_weights = True),\n                  ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001, verbose=2),\n                  #LearningRateScheduler(lr_decay, verbose=1)\n                  ], \n        validation_data=validation_data)","c3f7eda4":"history = perceptron_neural_network_model_history\n\nutils.plot_history(history)","ad495d96":"history2 = convolutional_neural_network_history\n\nutils.plot_history(history2)","b2cddcd2":"history3 = transfer_learning_history\n\nutils.plot_history(history3)","e103269c":"generator = test_iterator","c0e65d0f":"model = convolutional_neural_network_model\n\nconvolutional_neural_network_predictions = utils.predict(model, generator)","e0993bcd":"model2 = perceptron_neural_network_model\n\nperceptron_neural_network_predictions = utils.predict(model2, generator)","3a94ffe4":"model3 = transfer_learning_model\n\ntransfer_learning_predictions = utils.predict(model3, generator)","f7251c78":"filepath = \"convolutional_neural_network_model.h5\"\n\nconvolutional_neural_network_model.save(filepath)","700ea267":"filepath = \"transfer_learning_model.h5\"\n\ntransfer_learning_model.save(filepath)","23919236":"filepath = \"perceptron_neural_network_model.h5\"\n\nperceptron_neural_network_model.save(filepath)","7d19e817":"Definimos las capas completamente conectadas:","0350ddd7":"Procedemos con la capa de aplanamiento:","37758603":"Como se puede observar, esta red \u00fanicamente tiene que aprender 16943242 par\u00e1metros (en lugar de los 37804722 par\u00e1metros totales). ","121e6b2c":"Un aspecto muy interesante es visualizar la evoluci\u00f3n del valor de la funci\u00f3n de p\u00e9rdida y de la tasa de acierto con respecto al n\u00famero de *epochs*: ","79ece73e":"# 2. Carga de datos","40758cf0":"Por \u00faltimo, definimos la capa de salida, indicando el n\u00famero de posibles salidas y juntamos todas las capas en el modelo final.","5d3a74fe":"Al tratarse de un modelado tan sencillo, los detalles de cada una de las capas cobran mayor importancia. Es por esto que debemos tener en cuenta la aparici\u00f3n de un posible sobreajuste, debido a que todas las capas son completamente conectadas. Para paliarlo, vamos a a\u00f1adir capas tanto de ruido como de dropout. Esta \u00faltima actuar\u00e1 como primera capa, por lo que debemos introducirle el valor input_shape.","1819b2fd":"Ahora solo nos queda crear el modelo:","14cf43c0":"Finalmente solo nos queda especificar la capa de entrada y salida:","0bc96b1a":"El proceso de construcci\u00f3n de la red neuronal en transfer learning es un poco distinto al resto de redes. En este caso debemos ir creando cada capa una a una y transferirla a la anterior para crear la red. \nEn primer lugar, obtenemos las caracter\u00edsticas del modelo xception y lo aplanamos para que pueda utilizarse en nuestra red. ","81f55aea":"# 4. Modelado y evaluaci\u00f3n","1a20435a":"Inicialmente importamos las librer\u00edas que nos ser\u00e1n necesarias a lo largo de la pr\u00e1ctica.","76a42cbc":"Entrenamos los modelos de cada una de las redes neuronales, indicando para cada uno de ellos el n\u00famero de epochs a ejecutar y los callbacks que utilizar\u00e1n.\nComo se puede comprobar, finalmente no hemos utilizado LearningRateScheduler porque entorpec\u00eda el entrenamiento de los modelos e imped\u00eda obtener mejores resultados. En casi todas nuestras redes, los valores crecen de forma continua sin problemas, por lo que no resulta necesario modificar el ratio de aprendizaje obligatoriamente despu\u00e9s de ning\u00fan n\u00famero de epochs.\nLos otros dos Callbacks, sin embargo, si son utilizados. Utilizamos valores est\u00e1ndar que hemos consultado en varias p\u00e1ginas para sus par\u00e1metros y los aplicamos en todos los modelos. A pesar de la sencillez de este proceso, ha beneficiado enormemente el rendimiento de todos los entrenamientos, tal y como veremos a continuaci\u00f3n.","ca22576a":"Al generador de im\u00e1genes de train le podemos pasar par\u00e1metros que modifiquen las im\u00e1genes de alguna manera, lo que nos permite generar varianzas en las im\u00e1genes y enriquecer la base de datos. Hay muchos modificadores disponibles, entre los que destacaremos los siguientes:\n\n- *Horizontal flip*: invierte la imagen respecto al eje horizontal.\n- *Vertical flip*: invierte la imagen respecto al eje vertical.\n- *Width Shift range*: indica el grado de desplazamiento de la imagen horizontalmente.\n- *Height Shift range*: indica el grado de desplazamiento de la imagen verticalmente.\n- *Zoom range*: aumenta o reduce una porci\u00f3n de la imagen.\n- *Brightness range*: aumenta o reduce el brillo de la imagen.\n- *Shear range*: indica la intensidad del corte sobre la imagen.\n- *Rotation range*: indica el grado de inclinaci\u00f3n de la imagen.\n\nPara este caso, par\u00e1metros como horizontal flip y vertical flip no tienen sentido, porque si el objetivo es reconocer n\u00fameros no debemos modificar su apariencia b\u00e1sica.\nPor otro lado, el grado de brillo no influye porque trabajamos con im\u00e1genes muy simples en blanco y negro, en las que un cambio de brillo no modifica la apariencia del n\u00famero.\nAl igual que con los p\u00e1rametros de inversi\u00f3n, el grado de rotaci\u00f3n puede resultar contraproducente, ya que si se inclinan demasiado los n\u00fameros dejan de estar bien escritos y por lo tanto no deber\u00eda reconocerlos.","90ee3256":"Creamos la capa de aplanamiento y dos de dropout que utilizaremos para controlar el sobreajuste en el modelo.","01b69bd6":"Antes de comenzar con la pr\u00e1ctica es necesario comprobar que se est\u00e9 utilizando una *GPU*. De esta manera se incrementar\u00e1 la capacidad computacional reduciendo as\u00ed el tiempo de aprendizaje e inferencia de las redes neuronales:","3782fab3":"Definimos a continuaci\u00f3n los generadores de validaci\u00f3n y test, a los que no incluiremos ning\u00fan par\u00e1metro.","d29721dd":"En primer lugar, definimos el formato de la informaci\u00f3n como channels-last y la forma del input utilizando la variable target_size antes creada.","250d2749":"## Red Neuronal #1","bd54f963":"Introducimos el ratio de aprendizaje a trav\u00e9s de la variable lr, tomando el valor est\u00e1ndar 0.001.","0892bb37":"Comenzamos con la primera capa, en la que debemos especificar el tama\u00f1o de la imagen de entrada. En este caso, como la primera capa es la de ruido gaussiano, debemos incluirle el atributo input_shape.","c6a19d54":"Creamos despu\u00e9s las capas de drop out, que servir\u00e1n para \"obviar\" el output de algunas de las capas y hacer el proceso de entrenamiento m\u00e1s ruidoso y, por lo tanto, m\u00e1s s\u00f3lido y resistente al sobreajuste.","7b6eb8be":"#\u00a05. Predicci\u00f3n","0374c628":"Definimos las carpetas donde se van a almacenar la base de imagenes que recibimos y la que generamos como salida.\nY con splitfolders separamos las imagenes en 2 subconjuntos, uno con un 20% de las imagenes y otro con un 80%, ya que le hemos pasado como par\u00e1metro el ratio entre otros valores.","6335427e":"Creamos las capas convolucionales indicando su n\u00famero de filtros, el tama\u00f1o del kernel y la activaci\u00f3n por Relu.","290bdd82":"A\u00f1adimos las capas de pooling (una para cada capa convolucional) con tama\u00f1o de ventana 2:","6611aba7":"Esta segunda red neuronal la vamos a configurar utilizando transfer-learning. Por ello, el proceso va a resultar completamente distinto al de la red anterior\n\nVamos a utilizar imagenet, que posee una lista de modelos preentrenados que podemos utilizar. Hemos probado varios de ellos (VGG16, VGG19, DenseNet201 y MobileNetV2 y finalmente nos hemos decantado por Xception por obtener resultados ligeramente mejores y funcionar con mayor rapidez). Una vez que tenemos un modelo elegido, solo tenemos que crearlo utilizando Xception.","d79b0228":"Como podemos observar en los diagramas anteriores, las tres redes neuronales dan unos resultados parecidos siguiendo una trayectoria similar. Sin embargo, hay varios apuntes que podemos realizar en vista a la informaci\u00f3n obtenida:\n- La red m\u00e1s eficiente de las tres es la convolucional, ya que no solo obtiene una tasa de precisi\u00f3n y p\u00e9rdida mejores, sino que las obtiene en un menor n\u00famero de epochs y de forma m\u00e1s consistente.\n- En general las ejecuciones tienen un cierto aspecto aleatorio, ya que los valores no son exactamente iguales entre una ejecuci\u00f3n y otra. Dicho esto, la red convolucional es la m\u00e1s coherente de las tres y la red de transfer learning la m\u00e1s variable.\n- En todas las redes vemos que el conjunto de validaci\u00f3n suele obtener mejores resultados que el de entrenamiento. Se nos ocurren dos posibles explicaciones: una es que el conjunto de entrenamiento, al incluir los modificadores de im\u00e1genes produce casos de identificaci\u00f3n m\u00e1s \"dif\u00edciles\" que los de validaci\u00f3n, que no est\u00e1n modificados. La otra es que puede que haya un exceso de ruido o de dropout en la construcci\u00f3n de los modelos, haciendo que resulte m\u00e1s dif\u00edcil para el conjunto de entrenamiento el obtener buenos resultados.\n- El uso de callbacks favorece enormemente el proceso de entrenamiento, ya que en el registro de las ejecuciones podemos ver como modifican el learning rate cuando el aprendizaje se estanca o lo detienen por completo cuando empieza a disminuir. Esto supone una mejora tremenda en la rapidez y la eficiencia del aprendizaje.\n- El n\u00famero de epochs utilizado es mayor en las redes de perceptr\u00f3n y transfer learning porque hemos observado que requieren de un tiempo mayor para llegar a su mejor registro. La capa convolucional, sin embargo, necesita menos epochs para aprender y, en muchos casos, ni siquiera los realiza todos debido al callback EarlyStop.","c25ffa48":"## Red Neuronal #3","0400de39":"En este caso, la red neuronal  tiene que aprender 25676810 par\u00e1metros. ","fb0e852a":"En este apartado vamos a construir los 3 modelos basados en redes neuronales que m\u00e1s tarde vamos a entrenar y evaluar. ","8e6b2530":"## Red Neuronal #2","06b997ac":"Y finalizamos con la capa de salida:","bce3432b":"Una vez tenemos las redes neuronales entrenadas, simplemente nos queda obtener las predicciones sobre el conjunto de datos de prueba:","95561a54":"Y definimos el resto de capas utilizando la funci\u00f3n dense, para hacerlas completamente conectadas. Tambi\u00e9n utilizaremos la funci\u00f3n de activaci\u00f3n ReLu.","73c850dc":"Esta red tiene un total de 11122570 par\u00e1metros, todos ellos entrenables.","f6f1a98d":"Y la capa de salida:","3e4ee9ae":"# 3. Modelos basados en redes neuronales","3a61c672":"Una vez realizado este proceso, vamos a a\u00f1adir una serie de capas completamente conectadas que mejoren el entrenamiento. Para evitar un posible sobreajuste, a\u00f1adiremos una capa de ruido gaussiano inicial y tres capas de dropout despu\u00e9s de cada capa completamente conectada.","49b1ecfc":"Vamos a utilizar el conjunto MNIST, muy conocido y utilizado como benchmark en gr\u00e1n cantidad de algoritmos de clasificaci\u00f3n.\nNuesto objetivo es identificar y clasificar im\u00e1genes de n\u00fameros escritos a mano del 0 al 9.\n\nSe va a dividir el conjunto de datos MNIST en 3 conjuntos: Entrenamiento, Validaci\u00f3n y Prueba(test) a los que asignaremos 3 iteradores que trabajaran sobre ellos.","3e336047":"# Pr\u00e1ctica 3: Redes neuronales (*Deep Learning*)\n\n####\u00a0Miner\u00eda de Datos: Curso acad\u00e9mico 2020-2021\n\n### Profesorado:\n\n* Juan Carlos Alfaro Jim\u00e9nez\n* Jos\u00e9 Antonio G\u00e1mez Mart\u00edn\n\n### Alumnos:\n\n* Rub\u00e9n Mart\u00ednez Sotoca\n* Pablo Moreira Garc\u00eda\n\nEn esta pr\u00e1ctica trabajaremos con redes neuronales a trav\u00e9s de la *API* (*Advanced Programming Interface*) de `Keras`. Esta se trata de una de las librer\u00edas m\u00e1s populares actualmente debido a su facilidad de uso y versatilidad.","8fbc39ac":"Por \u00faltimo, y para experimentar un poco m\u00e1s con los distintos tipos de redes neuronales, vamos a crear una red neuronal de tipo perceptr\u00f3n multicapa. La principal diferencia ser\u00e1, por lo tanto, que no necesitaremos a\u00f1adir capas convolucionales ya que todas ser\u00e1n completamente conectadas.\nEsto hace el proceso m\u00e1s sencillo, ya que se trata de una sucesi\u00f3n secuencial de capas del mismo tipo. Sin embargo, esto tambi\u00e9n produce una serie de inconvenientes que estudiaremos a continuaci\u00f3n.","79ad01ab":"Antes de comenzar a modelar nuestras redes neuronales, vamos a crear algunas funciones callback que no resultar\u00e1n muy \u00fatiles en el proceso.\nVamos a estudiar tres:\n- EarlyStop: monitoriza un valor escogido y detiene el entrenamiento cuando dicho valor deja de mejorar.\n- ReduceLROnPlateau: reduce el ratio de aprendizaje cuando el valor monitorizado deja de mejorar.\n- LearningRateScheduler: actualiza el valor del ratio de aprendizaje para cada epoch a trav\u00e9s de una funci\u00f3n especificada.\n\nPara los dos primeros callbacks, que necesitan un par\u00e1metro para funcionar, hemos escogido val_loss porque, adem\u00e1s de ser el valor por defecto que suele utilizarse para estas funciones, muestra muy bien la mejor\u00eda del aprendizaje.","60d6925d":"Vamos a crear ahora un generador de im\u00e1genes para el conjunto train y el conjunto test, los cuales utilizaremos m\u00e1s adelante.","c201204f":"Con el objetivo de garantizar la reproducibilidad de los experimentos, vamos a serializar y almacenar los modelos. De esta manera, cualquiera puede recrear los mismos modelos y resultados:","4cdc6d47":"Debemos fijar las capas del modelo pre-entrenado xception para que no se modificquen. Para ello, utilizamos la funci\u00f3n freeze_layers.","f69aa36f":"## Un peque\u00f1o par\u00e9ntesis: Garantizado la reproducibilidad de los experimentos","f816947b":"Vamos a configurar nuestra primera red con la siguiente secuencia de capas:\n\n* Capa de ruido.\n* Capa convolucional (con 32 filtros y un tama\u00f1o de filtro de 3) y *ReLU* \n* Capa de *pooling* (con un tama\u00f1o de ventana de 2).\n* Capa convolucional (con 64 filtros y un tama\u00f1o de filtro de 3) y *ReLU* \n* Capa de *pooling* (con un tama\u00f1o de ventana de 2).\n* Capa convolucional (con 128 filtros y un tama\u00f1o de filtro de 3) y *ReLU* \n* Capa de *pooling* (con un tama\u00f1o de ventana de 2).\n* Capa convolucional (con 256 filtros y un tama\u00f1o de filtro de 3) y *ReLU* \n* Capa de *pooling* (con un tama\u00f1o de ventana de 2).\n* Capa de aplanamiento\n* Capa completamente conectada (con 512 neuronas).\n* Capa completamente conectada (con 256 neuronas).\n* Capa de salida.","03231c00":"#\u00a01. Preliminares","d4c1b575":"Fijamos una semilla, un ratio para la divisi\u00f3n de conjuntos y dejamos el prefjio de los conjuntos vac\u00edo.","5f93f6e9":"Una vez tenemos preparados los iteradores correspondientes a los conjuntos de datos de entrenamiento, validaci\u00f3n y prueba, podemos pasar a la fase de modelado.","79695c73":"Ahora que tenemos los generadores, debemos definir el tama\u00f1o de la muestra que utilizaremos y el del objetivo. Vamos a utilizar un tama\u00f1o de batch de 64 porque, al ser menor que el n\u00famero de instancias del conjunto de datos, ofrece un rendimiento en memoria superior.\n\nUna vez hemos indicado estos tama\u00f1os, podemos crear los iteradores para los conjuntos train, validation y test.","d237c829":"La primera red neuronal la vamos a configurar como red neuronal convolucional. Por ello, y aunque gran parte del objetivo de esta pr\u00e1ctica consiste en experimentar con distintas metodolog\u00edas, vamos a mantenernos fieles a la arquitectura recomendada en este enunciado y que define la estructura de una red neuronal como:\n\n1. `M` secuencias de una capa convolucional + *ReLU* + *pooling* (opcional).\n2. `K` secuencias de una capa completamente conectada + *ReLU*.\n3. Capa completamente conectada.\n\nCon `M > 0` y `K >= 0 & K < 3`.","9f0cee16":"La funci\u00f3n scheduler que se muestra a continuaci\u00f3n, es necesaria para aplicar el callback LearningRateScheduler. Al tratarse de una funci\u00f3n b\u00e1sica, se limita a comprobar el n\u00famero de epoch en el que se encuentra y, si es posterior al noveno, modifica el valor del ratio de aprendizaje con un exponente negativo."}}