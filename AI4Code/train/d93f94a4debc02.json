{"cell_type":{"cc1ffc0e":"code","612c602d":"code","677a94c5":"code","1de73cac":"code","b73fb5f0":"code","ed837ca9":"code","42eab337":"code","41f962e3":"code","e56849ae":"code","f7b0d007":"code","73f5350a":"code","6abb1299":"code","6781f721":"code","e289f733":"code","acf6b6fe":"code","d2f5e93f":"code","66b2f461":"code","9871a5dd":"code","a83c8249":"code","16ee8e9e":"code","3b838503":"code","362a07e0":"code","b5a564ec":"code","2d09ed59":"code","e06ea0aa":"code","5b4c7a4a":"code","5fc57ac8":"code","0e8ce41d":"code","0a627ab6":"code","8dff4988":"code","efdbe656":"code","98daa1d1":"code","a5862de2":"code","df9ac370":"code","ea258d29":"code","524359f7":"code","19e95a00":"code","1a58e29a":"code","0a716f25":"code","9a69872e":"code","c24ccf76":"code","f07e8e4a":"code","f8eeed74":"code","77ea4c0c":"code","f0bac9b8":"code","f4b0d0e6":"code","ec026b17":"code","84259703":"code","3b1e2bbc":"code","44cba489":"code","eccc5191":"code","5ff48cc1":"code","8e191400":"code","c143ecf3":"code","5dade7ee":"code","fcc869c9":"code","0307954f":"code","50fdbf26":"code","d01dae0c":"code","05673976":"code","115a683b":"code","1cfbbc6c":"code","5ececa80":"code","31b7acc0":"code","859e2b70":"code","c8303a53":"code","916500e4":"code","3e4aca03":"code","3d4765b3":"code","1931eca8":"code","30154ed4":"code","7605ac94":"code","8b66d843":"code","d574171e":"code","033045e1":"code","ff5c2dd6":"code","ec8aa3e5":"code","9452b3ba":"code","c8c09774":"markdown","0b142683":"markdown","c6d7576a":"markdown","b2473a8e":"markdown","c33b4e08":"markdown","ef61eb80":"markdown","574dc90b":"markdown","ecf377ac":"markdown","bff8eeb1":"markdown","9a96b2f9":"markdown","63d2eeb1":"markdown","194033b3":"markdown","7ed75b9b":"markdown","2c647f46":"markdown","78d9995d":"markdown","5c629adf":"markdown","25475025":"markdown","172977d4":"markdown","303ba949":"markdown","18cc716f":"markdown","903e1ec6":"markdown"},"source":{"cc1ffc0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.set()\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","612c602d":"bigmart_train = pd.read_csv('..\/input\/bigmart-sales-data\/Train.csv')\nbigmart_test = pd.read_csv('..\/input\/bigmart-sales-data\/Test.csv')\nbigmart_train.head()","677a94c5":"#dataset shape\nbigmart_train.shape","1de73cac":"#dataset dtypes\nbigmart_train.dtypes","b73fb5f0":"#dataset properities\nbigmart_train.info()","ed837ca9":"#Missing values Discovery\nbigmart_train.isnull().sum()","42eab337":"#So, Missing values are present in one numerical column- Item_weight and one Categorical column- Outlet_Size\n#Lets plot out the distribution of the data in numerical column\n\nbigmart_train['Item_Weight'].hist()\nplt.show()","41f962e3":"bigmart_train['Item_Weight'].fillna(bigmart_train['Item_Weight'].mean(), inplace=True)","e56849ae":"sns.countplot(x='Outlet_Size', data=bigmart_train)\nplt.show()","f7b0d007":"bigmart_train['Outlet_Size'].fillna('Medium', inplace=True)","73f5350a":"#rechecking missing values in Dataset\nbigmart_train.isnull().sum()","6abb1299":"#lets examine Outlet_Establishment_Year\nbigmart_train['Outlet_Establishment_Year'].value_counts()","6781f721":"#Lets understand the distribution of Establishment Year\nbigmart_train['Outlet_Establishment_Year'].hist()\nplt.show()","e289f733":"sns.barplot(data=bigmart_train, x='Outlet_Establishment_Year', y='Item_Outlet_Sales')\nplt.show()","acf6b6fe":"#Binning values\n\n# Specify the boundaries of the bins\nbins = [-np.inf, 1990, 2000, 2010]\n# Bin labels\nlabels = ['1980-1990', '1990-2000', '2000-2010']\n\nbigmart_train['Outlet_Establishment_Year']=pd.cut(bigmart_train['Outlet_Establishment_Year'], \n                                                  bins=bins, labels=labels)\nbigmart_train['Outlet_Establishment_Year'].value_counts().astype('object')","d2f5e93f":"obj_cols= bigmart_train.columns[bigmart_train.dtypes == 'object']","66b2f461":"for obj_cols in bigmart_train[obj_cols]:\n   print(bigmart_train[obj_cols].value_counts())","9871a5dd":"#examine Fat_Content column\nbigmart_train['Item_Fat_Content'].value_counts()","a83c8249":"bigmart_train['Item_Fat_Content']= bigmart_train['Item_Fat_Content'].replace(\n    {'LF':'Low Fat','low fat':'Low Fat', 'reg':'Regular'})\n\nprint(bigmart_train['Item_Fat_Content'].value_counts())","16ee8e9e":"bigmart_train['Item_Identifier'].value_counts()","3b838503":"bigmart_train['Item_Type_Extracted']=bigmart_train['Item_Identifier'].apply(lambda x:x[0:2])\nbigmart_train['Item_Type_Extracted'].value_counts()","362a07e0":"# Create a dictionary that maps strings\nmapping = {'FD':'Food', 'NC':'Non Consumable', 'DR':'Drinks'}\nbigmart_train['Item_Type_Combined']=bigmart_train['Item_Type_Extracted'].map(mapping)","b5a564ec":"bigmart_train['Item_Type_Combined'].value_counts().astype('object')","2d09ed59":"#Drop columns\nbigmart_train= bigmart_train.drop(['Item_Identifier', 'Item_Type','Item_Type_Extracted',\n                                   'Outlet_Identifier'], axis='columns')","e06ea0aa":"bigmart_train.head()","5b4c7a4a":"bigmart_train[\"Outlet_Establishment_Year\"]= bigmart_train[\"Outlet_Establishment_Year\"].astype('object')\nbigmart_train.dtypes","5fc57ac8":"bigmart_train.describe()","0e8ce41d":"bigmart_train.describe(exclude='number')","0a627ab6":"bigmart_train.corr()","8dff4988":"bigmart_train.var()","efdbe656":"bigmart_train.hist()\nplt.show()","98daa1d1":"sns.barplot(data=bigmart_train, x='Item_Type_Combined',y='Item_Outlet_Sales', hue=\"Outlet_Location_Type\")\nplt.show()","a5862de2":"sns.barplot(data=bigmart_train, x='Outlet_Location_Type',y='Item_Outlet_Sales', hue=\"Outlet_Type\")\nplt.show()","df9ac370":"sns.barplot(data=bigmart_train, x='Outlet_Size',y='Item_Outlet_Sales')\nplt.show()","ea258d29":"sns.barplot(data=bigmart_train, x='Outlet_Establishment_Year', y='Item_Outlet_Sales', hue=\"Outlet_Location_Type\")\nplt.legend(loc='best')\nplt.show()","524359f7":"sns.barplot(data=bigmart_train, x= \"Item_Type_Combined\", y=\"Item_Outlet_Sales\", hue=\"Item_Fat_Content\")\nplt.legend(loc='best')\nplt.show()","19e95a00":"sns.scatterplot(data=bigmart_train, x=\"Item_Visibility\", y=\"Item_Outlet_Sales\")\nplt.show()","1a58e29a":"bigmart_train.boxplot()\nplt.show()","0a716f25":"#Encode categorical columns to Numerical values to make ML algorithm understands it better\n#here we use OneHotEncoding\n\ncategorical_cols= bigmart_train.columns[bigmart_train.dtypes == 'object'].tolist()\ncategorical_cols","9a69872e":"encoded_df= pd.get_dummies(bigmart_train[categorical_cols], drop_first=True)\nencoded_df.head()","c24ccf76":"#concatenate encoded dataframe with train dataframe\ndf_encoded= pd.concat([bigmart_train, encoded_df], axis=1)\ndf_encoded.head()","f07e8e4a":"#Drop columns from dataset\ndf_final= df_encoded.drop(categorical_cols, axis=1)\ndf_final.head()","f8eeed74":"print(df_final.shape)","77ea4c0c":"from sklearn.preprocessing import StandardScaler\n\nscaler= StandardScaler()\n\nscaler.fit(df_final[['Item_MRP']])\nscaler.fit(df_final[['Item_Weight']])\nscaler.fit(df_final[['Item_Visibility']])\n\ndf_final['Item_MRP_scaled']=scaler.transform(df_final[['Item_MRP']])\ndf_final['Item_Weight_scaled']=scaler.transform(df_final[['Item_Weight']])\ndf_final['Item_Visibility_scaled']=scaler.transform(df_final[['Item_Visibility']])\n\ndf_final[['Item_MRP_scaled', 'Item_MRP', 'Item_Weight_scaled', 'Item_Weight', \n          'Item_Visibility_scaled', 'Item_Visibility']].head()","f0bac9b8":"#drop the original columns and keep the scaled column in dataframe\ndf=df_final.drop(['Item_MRP', 'Item_Weight', 'Item_Visibility'], axis=1)\ndf.head()","f4b0d0e6":"#seperate the training and test set\n\ny= df['Item_Outlet_Sales']\nX= df.drop('Item_Outlet_Sales', axis=1)","ec026b17":"#import necessary scikit learn libraries\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, accuracy_score\n\n#Instantiate Linear Regression model\nlr= LinearRegression(normalize=True)","84259703":"# Compute 5-fold cross-validation scores: cv_scores\ncv_scores= cross_val_score(lr, X, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint(cv_scores)\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))","3b1e2bbc":"#Lets divide the data-set into training and test-set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=30)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","44cba489":"#Fit the linear regression model to training data\nlr.fit(X_train, y_train)","eccc5191":"predicted_Outlet_sales = lr.predict(X_test)\nmean_squared_error(y_test, predicted_Outlet_sales)","5ff48cc1":"# Compute and print the RMSE\nrmse = np.sqrt(mean_squared_error(y_test,predicted_Outlet_sales))\nprint(\"RMSE: %f\" % (rmse))","8e191400":"#training set accuracy\nlr.score(X_train, y_train)","c143ecf3":"#test set accuracy\nlr.score(X_test, y_test)","5dade7ee":"#review test dataframe\nbigmart_test.head()","fcc869c9":"bigmart_test.shape","0307954f":"#Missing Value Discovery in Test Data\nbigmart_test.isnull().sum()","50fdbf26":"#impute the missing values same like Train data\nbigmart_test['Item_Weight'].fillna(bigmart_test['Item_Weight'].mean(), inplace=True)","d01dae0c":"bigmart_test['Outlet_Size'].fillna('Medium', inplace=True)","05673976":"bigmart_test.isnull().sum()","115a683b":"#Binning values\n\n# Specify the boundaries of the bins\nbins = [-np.inf, 1990, 2000, 2010]\n# Bin labels\nlabels = ['1980-1990', '1990-2000', '2000-2010']\n\nbigmart_test['Outlet_Establishment_Year']=pd.cut(bigmart_test['Outlet_Establishment_Year'], \n                                                  bins=bins, labels=labels)\nbigmart_test['Outlet_Establishment_Year'].value_counts().astype('object')","1cfbbc6c":"obj_cols= bigmart_test.columns[bigmart_test.dtypes == 'object']","5ececa80":"bigmart_test['Item_Fat_Content']= bigmart_test['Item_Fat_Content'].replace(\n    {'LF':'Low Fat','low fat':'Low Fat', 'reg':'Regular'})\n\nprint(bigmart_test['Item_Fat_Content'].value_counts())","31b7acc0":"bigmart_test['Item_Type_Extracted']=bigmart_test['Item_Identifier'].apply(lambda x:x[0:2])\nbigmart_test['Item_Type_Extracted'].value_counts()","859e2b70":"# Create a dictionary that maps strings\nmapping = {'FD':'Food', 'NC':'Non Consumable', 'DR':'Drinks'}\nbigmart_test['Item_Type_Combined']=bigmart_test['Item_Type_Extracted'].map(mapping)","c8303a53":"bigmart_test['Item_Type_Combined'].value_counts().astype('object')","916500e4":"#Drop columns\nbigmart_test= bigmart_test.drop(['Item_Identifier', 'Item_Type','Item_Type_Extracted',\n                                 'Outlet_Identifier'], axis='columns')","3e4aca03":"bigmart_test.head()","3d4765b3":"bigmart_test[\"Outlet_Establishment_Year\"]= bigmart_test[\"Outlet_Establishment_Year\"].astype('object')","1931eca8":"categorical_cols_test= bigmart_test.columns[bigmart_test.dtypes == 'object'].tolist()\ncategorical_cols_test","30154ed4":"test_df= pd.get_dummies(bigmart_test[categorical_cols_test], drop_first=True)\ntest_df.head()","7605ac94":"df_test= pd.concat([bigmart_test, test_df], axis=1)\ndf_test.head()","8b66d843":"#Drop columns from dataset\ntest_final= df_test.drop(categorical_cols, axis=1)\ntest_final.head()","d574171e":"#Apply trained scaler to the test set\ntest_final['Item_MRP_scaled']=scaler.transform(test_final[['Item_MRP']])\ntest_final['Item_Weight_scaled']=scaler.transform(test_final[['Item_Weight']])\ntest_final['Item_Visibility_scaled']=scaler.transform(test_final[['Item_Visibility']])\n\ntest_final[['Item_MRP_scaled', 'Item_MRP', 'Item_Weight_scaled', 'Item_Weight', \n          'Item_Visibility_scaled', 'Item_Visibility']].head()","033045e1":"#drop the original columns and keep the scaled column in datafram\ntest_X=test_final.drop(['Item_MRP', 'Item_Weight', 'Item_Visibility'], axis=1)\ntest_X.head()","ff5c2dd6":"#predicting Outlet Sales for Test Set\npredicted_Outlet_Sales= lr.predict(test_X)\npredicted_Outlet_Sales","ec8aa3e5":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\nlr= LinearRegression()\nridge= Ridge(alpha=0.05,solver='cholesky')\nlasso= Lasso(alpha=0.01)\ndt= DecisionTreeRegressor()\nrf= RandomForestRegressor(n_estimators=100)\nbr= BaggingRegressor(max_samples=70)\nabr= AdaBoostRegressor()\ngbr= GradientBoostingRegressor()","9452b3ba":"regressors = [('Linear Regression', lr), ('Ridge', ridge), ('Lasso', lasso), ('Decision Tree Regressor', dt),\n             ('RandomForest Classifier', rf), ('Bagging Regressor', br), ('Ada Boost', abr), ('Gradient Boost', gbr)]\n\n# Iterate over the pre-defined list of regressors\nfor reg_name, reg in regressors:   \n    # Fit clf to the training set\n    reg.fit(X_train, y_train)    \n    y_pred = reg.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n\n    print('{:s} : {:.3f}'.format(reg_name, mse))\n    print('{:s} : {:.3f}'.format(reg_name, rmse))","c8c09774":"Though we see outliers are present in Item_Visibility present we assume outliers have less impact on Model performance and build a model","0b142683":"Item_Weight are almost evenly distributed. so filling this column with mean value is good choice","c6d7576a":"We have created scalers based on a column, and then applied the scaler to the same data that it was trained on. So, we built our models on historic data (train set) and apply our model to new unseen data (test set). In these case we will need to ensure that the same scaling is being applied to both the training and test data.\n\nsame applies to encoding categorical columns","b2473a8e":"We will try to fit the train set with other Regressor models and perform Model Validation","c33b4e08":"Low Fat, LF, low fat are same so as reg, Regular\nlets replace it with unique for Low Fat and Regular","ef61eb80":"### Visual EDA","574dc90b":"##### Model Validation:\nMean Squared error= (absolute-predicted)*1\/2","ecf377ac":"Training accuracy and Testing accuracy is almost same. so, there is no over or underfitting\nWe should improve our model performance by reducing RMSE","bff8eeb1":"Item_Visibility ranges from 0.0-0.20 yields majority of sales in Outlets","9a96b2f9":"Sales are high in Tier 3 cities\nGrocery Store contributes less Sales compared to other outlets","63d2eeb1":"If you watch closely, First two characters of Item Identifier signifies Item Type.\nLets interpret further into this","194033b3":"### Conclusion: Gradient Boosting predicts Bigmart Sales better than Other models","7ed75b9b":"#### Feature Engineering with Categorical features","2c647f46":"Lets plot countplot to understand the statistical summaries of categorical columns","78d9995d":"### Scaling and Transforming new data","5c629adf":"Numerical Columns are in Different Scale. making sure that all of your data is on the same scale is advisable for most analyses","25475025":"Medium is highest no.of Outlet-Size. So, lets fill it with this constant","172977d4":"Since one of the column is affected by outliers it is hard to compare two scaled columns if it is normalization.\nInstead Standardization- where instead of having a strict upper and lower bound, We center the data around its mean, and calculate the number of standard deviations away from mean each data point is","303ba949":"### Stastical EDA","18cc716f":"Since there atleast 10% missing values are present. Listwise deletion is not good choice. By dropping the columns we must lose useful information of Dataset","903e1ec6":"As we can see there are missing values present in Columns- Item_Weight, Outlet_Size."}}