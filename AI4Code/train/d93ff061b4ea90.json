{"cell_type":{"f7c3a6db":"code","1892f491":"code","57fcce69":"code","5fbd38cd":"code","5f4f6e49":"code","a915bf7b":"code","0bf22d39":"code","a42dd6a6":"code","7f179b36":"code","5cd32505":"code","c46e9c06":"code","590ec851":"code","f225ab9f":"code","9e1a3260":"code","4d9b73f5":"code","9b77232f":"code","b8541e3c":"code","b3a57c4c":"code","16ddf814":"code","57c740bc":"code","25609300":"code","f7bdf1df":"code","551992b3":"code","36b0bf83":"code","8134ef0e":"code","2c4cec8e":"code","1c8b93ff":"code","fac59563":"code","db8b99d2":"code","5b58d752":"code","ba25da4b":"code","34ab41f3":"code","9806e37d":"code","01b8b768":"code","c9ecd131":"code","bf0eaa7f":"markdown","8e6e68fb":"markdown"},"source":{"f7c3a6db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nimport json\nfrom multiprocessing import Pool\nimport random\nimport pickle\nimport re\nfrom functools import reduce\n# Any results you write to the current directory are saved as output.\nimport networkx as nx\n\nfrom gensim.models import Word2Vec\nimport gc","1892f491":"filenames_list = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for each_filename in filenames:\n        filenames_list.append(os.path.join(dirname, each_filename))","57fcce69":"len(filenames_list)","5fbd38cd":"# for filename in random.sample(filenames_list, 2):\n#     if filename.split(\".\")[-1] == \"json\":\n#         ifp = open(os.path.join(dirname, filename))\n#         research_paper = json.load(ifp)\n#         title = research_paper[\"metadata\"][\"title\"]\n#         print(title, \"\\n\\n\")\n#         abstract_text = \" \".join([each[\"text\"] for each in research_paper[\"abstract\"]])\n#         print(abstract_text, \"\\n\\n\")\n#         body_text = \" \".join([each[\"text\"] for each in research_paper[\"body_text\"]])\n#         print(body_text)","5f4f6e49":"research_paper_title_list = []\nfor filename in filenames_list:\n    if filename.split(\".\")[-1] == \"json\":\n        ifp = open(os.path.join(dirname, filename))\n        research_paper = json.load(ifp)\n        research_paper_title_list.append(research_paper[\"metadata\"][\"title\"])\n        for each_ref in research_paper[\"bib_entries\"]:\n            research_paper_title_list.append(research_paper[\"bib_entries\"][each_ref][\"title\"])\n\n        \n        \n        \n        \n        ","a915bf7b":"len(research_paper_title_list)","0bf22d39":"paper_id_dict= dict(zip(research_paper_title_list, list(map(lambda x: str(x), range(len(research_paper_title_list))))))","a42dd6a6":"id_paper_dict = dict(zip(paper_id_dict.values(), paper_id_dict.keys()))","7f179b36":"paper_undirected_degree_dict= dict(zip(research_paper_title_list, [0]*len(research_paper_title_list)))","5cd32505":"adj_mat = {}\nfor filename in filenames_list:\n    if filename.split(\".\")[-1] == \"json\":\n        ifp = open(os.path.join(dirname, filename))\n        research_paper = json.load(ifp)\n        adj_mat[paper_id_dict[research_paper[\"metadata\"][\"title\"]]] = [paper_id_dict[research_paper[\"bib_entries\"][each_key][\"title\"]] for each_key in research_paper[\"bib_entries\"]]\n        paper_undirected_degree_dict[research_paper[\"metadata\"][\"title\"]] += len(adj_mat[paper_id_dict[research_paper[\"metadata\"][\"title\"]]])\n        for each_key in research_paper[\"bib_entries\"]:\n            paper_undirected_degree_dict[research_paper[\"bib_entries\"][each_key][\"title\"]] += 1\n        ","c46e9c06":"pruned_adj_mat = {}\nfor each_key in adj_mat:\n    freq_ref = [paper_undirected_degree_dict[id_paper_dict[each_id]] for each_id in adj_mat[each_key]]\n    ref_freq_dict = dict(zip(adj_mat[each_key], freq_ref))\n    pruned_adj_mat[each_key] = sorted(adj_mat[each_key], key=lambda x: ref_freq_dict[x], reverse=True)[:25]\n            ","590ec851":"for each_key in random.sample(adj_mat.keys(), 5):\n    print(each_key, adj_mat[each_key])","f225ab9f":"degrees = [len(adj_mat[each_key]) for each_key in adj_mat]","9e1a3260":"np.mean(degrees)","4d9b73f5":"np.median(degrees)","9b77232f":"np.max(degrees)","b8541e3c":"np.min(degrees)","b3a57c4c":"nodes_in_pruned_graph = list(reduce(lambda x, y: x + y, pruned_adj_mat.values())) + list(pruned_adj_mat.keys())","16ddf814":"len(nodes_in_pruned_graph)","57c740bc":"len(set(nodes_in_pruned_graph))","25609300":"citation_graph = nx.from_dict_of_lists(pruned_adj_mat)","f7bdf1df":"adj_mat = None\ngc.collect()","551992b3":"pruned_degrees = list(dict(citation_graph.degree).values())\nprint(len(pruned_degrees), np.mean(pruned_degrees), np.median(pruned_degrees), np.min(pruned_degrees), np.max(pruned_degrees))","36b0bf83":"def random_walk(arg):\n    root_node, walk_length = arg\n    walk = [root_node]\n\n    for i in range(1, walk_length):\n        cur = walk[i-1]\n#         try:\n        neighbours = list(citation_graph.neighbors(cur))\n        if len(neighbours) > 0:\n            walk.append(random.choice(neighbours))\n        else:\n            walk = walk[:-1]\n            break\n#         if type(walk[-1]) == str:\n#             print(walk[-1])\n#             walk = walk[:-1]\n#             break\n#         except:\n#             break\n    return walk","8134ef0e":"def deepwalk_random_walks(num_walks, walk_length):\n    nodes = list(citation_graph.nodes())\n    walks = []\n    for i in range(num_walks):\n        print(\"walk no. \", i)\n        random.shuffle(nodes)\n        with Pool(processes=32) as pool:\n            walks = walks + pool.map(random_walk, zip(nodes,[walk_length]*len(nodes)))\n    return walks","2c4cec8e":"random_walks = deepwalk_random_walks(20, 10)","1c8b93ff":"len(random_walks)","fac59563":"random_walks[2]","db8b99d2":"model = Word2Vec(random_walks, size=32, window=4, alpha=0.005, min_count=0, sg=1, workers=16, iter=5, negative=5)","5b58d752":"def most_similar_papers(title, topn=20):\n    return [(id_paper_dict[each[0]], each[1]) for each in model.wv.most_similar(paper_id_dict[title], topn=topn)]","ba25da4b":"most_similar_papers('Discovery and Characterization of Novel Bat Coronavirus Lineages from Kazakhstan. Viruses')","34ab41f3":"most_similar_papers('Ebola virus enters host cells by macropinocytosis and clathrin-mediated endocytosis')","9806e37d":"most_similar_papers('Enhanced growth of a murine coronavirus in transformed mouse cells')","01b8b768":"model.save(\"\/kaggle\/working\/node2vec_citation_graph_covid19.wv\")","c9ecd131":"paper_id_ = list(zip(paper_id_dict.keys(), paper_id_dict.values()))\npaper_id_df = pd.DataFrame(paper_id_, columns=[\"paper_title\", \"paper_id\"])\npaper_id_df.to_csv(\"\/kaggle\/working\/paper_id_map.csv\")\nofp = open(\"\/kaggle\/working\/id_paper_map.pickle\", \"wb\")\npickle.dump(id_paper_dict, ofp)\nofp = open(\"\/kaggle\/working\/paper_id_map.pickle\", \"wb\")\npickle.dump(paper_id_dict, ofp)","bf0eaa7f":"## Some examples","8e6e68fb":"* In this kernel, I create a citation graph based on the research papers in the json file. \n* Most papers have bibref section which captures the citations in the paper.\n* Most of the papers in the citation are not present in the cord-19 dataset and details are also in the kernel.\n* I create a citation graph and then apply DeepWalk algorithm to create embeddings of the papers such that related papers are closer.\n* Due to memory constraint of the kernel, I have pruned the graph such that each paper can have at most 25 directed edges going to its references (fan-out). They are selected based on the freqency. \n* However, a paper can have many edges (fan-in).\n\nAlso, reference to my other kernel on creating vocabulary of short-hand notation in the research papers https:\/\/www.kaggle.com\/midnitekoder\/coronavirus-jargon-vocabulary\n"}}