{"cell_type":{"a88dd2b1":"code","5586dcea":"code","5ee181fe":"code","c8624702":"code","45b25b37":"code","1ed53166":"code","a935e277":"code","61bb68c2":"code","e4dbf253":"code","b8d0841e":"code","4758fe26":"code","468c5f2d":"code","b0fc49ee":"code","3ceaf7e7":"code","5f40f7fd":"code","27d6e613":"code","b3e7e98b":"code","5baacb37":"code","634d81ce":"code","7435223f":"code","f39749f4":"code","8a2fc70c":"code","7b7cea4b":"markdown","f6e7027c":"markdown","64104469":"markdown","8f84d674":"markdown","31025469":"markdown","0bb1ba87":"markdown","449e437c":"markdown","3739dcc4":"markdown"},"source":{"a88dd2b1":"\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\n\nimport tensorflow as tf\nimport os\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\n","5586dcea":"os.listdir('..\/input\/tabular-playground-series-sep-2021\/')\n","5ee181fe":"train_data = pd.read_csv(\n    '..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsubmission_data = pd.read_csv(\n    '..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\n","c8624702":"y = train_data['claim']\ntmp = train_data.copy()\ntmp['n_missing'] = train_data.isnull().sum(axis=1)\nX = tmp.drop(['claim', 'id'], axis=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=0)","45b25b37":"imputer = SimpleImputer(strategy='mean')\nimp_X_train = pd.DataFrame(imputer.fit_transform(X_train))\nimp_X_val = pd.DataFrame(imputer.transform(X_val))\nimp_X_train.columns = X_train.columns\nimp_X_val.columns = X_val.columns","1ed53166":"test_tmp = test_data.copy().drop('id', axis=1)\ntest_tmp['n_missing'] = test_data.isnull().sum(axis=1)\n\nimp_X_test = pd.DataFrame(imputer.fit_transform(test_tmp))\nimp_X_test.columns = test_tmp.columns","a935e277":"imp_X_train","61bb68c2":"y_train","e4dbf253":"imp_X_test","b8d0841e":"print(imp_X_train.shape)\nprint(y_train.shape)\nprint(imp_X_test.shape)\n","4758fe26":"X_train_expanded = tf.expand_dims(imp_X_train, axis=-1)\nX_val_expanded = tf.expand_dims(X_val, axis=-1)","468c5f2d":"model = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Input(shape=(imp_X_train.shape[1], 1,)))\n\nmodel.add(tf.keras.layers.Conv1D(128, kernel_size=1,\n          activation='relu', padding='same'))\nmodel.add(tf.keras.layers.Conv1D(\n    128, kernel_size=1, activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=1, strides=1))\nmodel.add(tf.keras.layers.Dropout(0.25))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Conv1D(256, kernel_size=1,\n          activation='relu', padding='same'))\nmodel.add(tf.keras.layers.Conv1D(\n    128, kernel_size=1, activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=1, strides=1))\nmodel.add(tf.keras.layers.Dropout(0.25))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Conv1D(512, kernel_size=1,\n          activation='relu', padding='same'))\nmodel.add(tf.keras.layers.Conv1D(\n    128, kernel_size=1, activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=1, strides=1))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.add(tf.keras.layers.Dense(512, activation='relu', activity_regularizer=tf.keras.regularizers.l2(\n    0.00001), bias_regularizer=tf.keras.regularizers.l2(0.0001)))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(512, activation='relu', activity_regularizer=tf.keras.regularizers.l2(\n    0.00001), bias_regularizer=tf.keras.regularizers.l2(0.0001)))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(512, activation='relu', activity_regularizer=tf.keras.regularizers.l2(\n    0.00001), bias_regularizer=tf.keras.regularizers.l2(0.0001)))\nmodel.add(tf.keras.layers.Dropout(0.5))\n\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()","b0fc49ee":"tf.keras.utils.plot_model(model, show_shapes=True)\n","3ceaf7e7":"auc = tf.keras.metrics.AUC(name='aucroc')\noptimizer = tf.keras.optimizers.RMSprop(learning_rate=0.00001, rho=0.9, epsilon=1e-08, decay=0.0)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', auc])\n","5f40f7fd":"earlystopping = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", min_delta=0, patience=5, verbose=1, restore_best_weights=True\n)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.3, patience=3, verbose=1, min_delta=1e-4\n)\n\ncallbacks = [earlystopping, reduce_lr]\n\nhistory = model.fit(x=X_train_expanded, y=y_train, batch_size=512, shuffle=True,\n                    epochs=30, validation_data=(X_val_expanded, y_val), callbacks=callbacks)\n","27d6e613":"def vis_training(history_list, start=1):\n    loss = np.concatenate([h.history[\"loss\"] for h in history_list])\n    val_loss = np.concatenate([h.history[\"val_loss\"] for h in history_list])\n    acc = np.concatenate([h.history[\"accuracy\"] for h in history_list])\n    val_acc = np.concatenate([h.history[\"val_accuracy\"] for h in history_list])\n\n    epoch_range = range(1, len(loss) + 1)\n\n    plt.figure(figsize=[12, 6])\n    plt.subplot(1, 2, 1)\n    plt.plot(epoch_range[start - 1:], loss[start - 1:], label=\"Training Loss\")\n    plt.plot(epoch_range[start - 1:],\n             val_loss[start - 1:], label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epoch_range[start - 1:], acc[start - 1:],\n             label=\"Training Accuracy\")\n    plt.plot(\n        epoch_range[start - 1:], val_acc[start -\n                                         1:], label=\"Validation Accuracy\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.legend()\n\n    plt.show()\n","b3e7e98b":"vis_training([history])\n","5baacb37":"X_test_extended = tf.expand_dims(imp_X_test, axis=-1);","634d81ce":"sub = pd.DataFrame()\nsub['id'] = test_data['id']\nsub['claim'] = model.predict(X_test_extended)\nsub = sub.set_index('id')","7435223f":"sub.head()","f39749f4":"sub.shape\n","8a2fc70c":"sub.to_csv('submission.csv')\n","7b7cea4b":"Loading the dataset","f6e7027c":"Let's build our model and train it! I am using a high epoch but should stop if things go down.","64104469":"Creating the submission thingy.","8f84d674":"### Preps & Introduction\n\nSo I've checked the public notebooks before I start and saw everbody is using **XGB**, **LGBM**, **CatBoost** and other shenanigans, decided to try CNN so here we go. Let's import the libraries.","31025469":"### Creating CNN model.\n\nThere's likely a better approach. \ud83e\udd37\u200d\u2642\ufe0f","0bb1ba87":"Examine our model.","449e437c":"### Pre-processing\n\nStandard stuff.","3739dcc4":"I like plots. \ud83e\uddd1\u200d\ud83d\udcbb"}}