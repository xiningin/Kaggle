{"cell_type":{"185f84db":"code","b737ebf8":"code","3d6f32bc":"code","fae76c92":"code","f7d0b70e":"code","50289745":"code","92e3b79a":"code","2c313a10":"code","7df3c207":"code","88d2d862":"code","0f68e041":"code","9952a103":"code","c4bd5547":"code","29274578":"code","4262053c":"code","f956a85c":"code","832f924e":"code","ffcb1c6a":"code","f33352ae":"code","77062902":"code","15039f65":"code","dd6f4b40":"code","37e19661":"code","3b98092b":"code","57ddbe3f":"code","258e0fb9":"code","e73d7069":"code","5d35b40f":"code","b82a8409":"code","3280a75a":"code","00b6f6ec":"code","63bfc86b":"code","79beac12":"code","2ad10e64":"code","ea14dbd1":"code","8a166466":"code","1886ed77":"code","a730d07e":"code","5669782b":"code","c87d429d":"code","d70b145a":"code","b2197879":"code","33a4577a":"code","2301cc94":"code","d2761960":"code","9163d23e":"code","0d07a81e":"code","c0ef4d9f":"code","5a3fce26":"code","93e6e63d":"code","a6f1a5aa":"code","226e4af9":"code","595fe194":"code","0d8c1291":"code","a4d7cf3a":"code","c811a14a":"markdown","4bbecee6":"markdown","e1424c8c":"markdown","6ff89e4c":"markdown","c30947c9":"markdown","927244fa":"markdown","1dd2c54e":"markdown","7a4b3ee0":"markdown","13b1d35a":"markdown","23529fb6":"markdown","36a4c710":"markdown","9191c06e":"markdown","f0e19bd7":"markdown","d6bb4217":"markdown","500d46ab":"markdown","968fcaa8":"markdown","47f30a25":"markdown","97774eff":"markdown","bf0e911b":"markdown","456d61e0":"markdown","1c3b7b0c":"markdown","19c05907":"markdown","a079f981":"markdown","f45d3f05":"markdown"},"source":{"185f84db":"# Basic\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Tools\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.manifold import TSNE\nfrom imblearn.over_sampling import SMOTE\nfrom scipy import stats\nfrom collections import Counter\n\n# Model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, cross_val_score, cross_val_predict\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.under_sampling import NearMiss\n\n# Algorithms\nfrom sklearn import ensemble, tree, svm, naive_bayes, neighbors, linear_model, gaussian_process, neural_network\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\n# Evaluation\nfrom sklearn.metrics import f1_score, accuracy_score, roc_curve, roc_auc_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix, recall_score, precision_score, precision_recall_curve\n\n# System\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","b737ebf8":"print(os.listdir(\"..\/input\"))","3d6f32bc":"df = pd.read_csv(\"..\/input\/creditcard.csv\")","fae76c92":"df.shape","f7d0b70e":"df.columns","50289745":"df.head()","92e3b79a":"sns.countplot('Class', data=df)\nprint('Frauds: ', round(df['Class'].value_counts()[1] \/ len(df) * 100, 2), '%')","2c313a10":"df.describe()","7df3c207":"df.info()","88d2d862":"df.isnull().sum()","0f68e041":"# Scale whole dataset\nrs = RobustScaler()\n\ndf['Time'] = rs.fit_transform(df['Time'].values.reshape(-1,1))\ndf['Amount'] = rs.fit_transform(df['Amount'].values.reshape(-1,1))","9952a103":"# Check\ndf.head()","c4bd5547":"# TODO: Normlize dataset with BoxCox transformation.","29274578":"#TODO: Random undersample with NearMiss algorithm. ","4262053c":"# Count Frauds\ndf['Class'].value_counts()[1]","f956a85c":"# Create balanced sub-dataset\nfrauds = df.loc[df['Class'] == 1]\nnonfrauds = df.loc[df['Class'] == 0][:492]\n\nundersample = pd.concat([frauds, nonfrauds])","832f924e":"# Check\nundersample.shape","ffcb1c6a":"X = df.drop(['Class'], axis=1)\ny = df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","f33352ae":"#Apply SMOTE to train set\nsm = SMOTE(random_state=22)\nX_oversample, y_oversample = sm.fit_sample(X_train, y_train)\n\n#Check majority vs. minority class distribution in train set after resampling\n\nprint('Fraudulent share, train set (after SMOTE): {0:.2%}'.format(sum(y_resampled==1)\/len(y_resampled)))","77062902":"# Convert array into dataframe\nX_oversample = pd.DataFrame(X_oversample)\ny_oversample = pd.DataFrame(y_oversample)","15039f65":"X = undersample.drop(['Class'], axis=1)\ny = undersample['Class']\n\ntsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)","dd6f4b40":"# Define plot\nf, (ax1) = plt.subplots(1, 1, figsize=(16,8))\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n# t-SNE scatter plot\nax1.scatter(tsne[:,0], tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax1.scatter(tsne[:,0], tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\nax1.grid(True)","37e19661":"X = undersample.drop('Class', axis=1)\ny = undersample['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","3b98092b":"# Convert values into an array\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","57ddbe3f":"MLA = [\n    ensemble.AdaBoostClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    gaussian_process.GaussianProcessClassifier(),\n    linear_model.LogisticRegressionCV(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.Perceptron(),\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    neighbors.KNeighborsClassifier(),\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    xgb.XGBClassifier()\n    ]","258e0fb9":"col = []\nalgorithms = pd.DataFrame(columns = col)\nidx = 0\n\n#Train and score algorithms\nfor a in MLA:\n    \n    a.fit(X_oversample, y_oversample)\n    pred = a.predict(original_Xtest)\n    acc = accuracy_score(original_ytest, pred) #Other way: a.score(X_test, y_test)\n    f1 = f1_score(original_ytest, pred)\n    cv = cross_val_score(a, original_Xtest, original_ytest).mean()\n    \n    Alg = a.__class__.__name__\n    \n    algorithms.loc[idx, 'Algorithm'] = Alg\n    algorithms.loc[idx, 'Accuracy'] = round(acc * 100, 2)\n    algorithms.loc[idx, 'F1 Score'] = round(f1 * 100, 2)\n    algorithms.loc[idx, 'CV Score'] = round(cv * 100, 2)\n\n    idx+=1","e73d7069":"#Compare invidual models\nalgorithms.sort_values(by = ['CV Score'], ascending = False, inplace = True)    \nalgorithms.head()","5d35b40f":"#Plot them\ng = sns.barplot(\"CV Score\", \"Algorithm\", data = algorithms)\ng.set_xlabel(\"CV score\")\ng = g.set_title(\"Algorithm Scores\")","b82a8409":"# Train\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_train)\n#cv_pred = cross_val_predict(xgb, X_train, y_train, cv=kfold)\n\n# Score\nprint('XGBoost classifier ROC AUC score: ', roc_auc_score(y_train, y_pred))","3280a75a":"# We will undersample during cross validating\nundersample_X = df.drop('Class', axis=1)\nundersample_y = df['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(undersample_X, undersample_y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values \n\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\n# Implementing NearMiss Technique \n# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\nX_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n# Cross Validating the right way\n\nfor train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), xgb) # SMOTE happens during Cross Validation not before..\n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n    \n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))","00b6f6ec":"X = df.drop('Class', axis=1)\ny = df['Class']\n\noriginal_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)","63bfc86b":"kfold = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)","79beac12":"# Use pre-trained model on the full sample\n#xgb = XGBClassifier()\n#xgb.fit(X_train, y_train)\n#cv_pred = cross_val_predict(xgb, X_train, y_train, cv=kfold)\ncv_pred = cross_val_predict(xgb, original_Xtrain, original_ytrain, cv=kfold)\n\n# Score\nprint('XGBoost classifier ROC AUC score: ', roc_auc_score(original_ytrain, cv_pred))","2ad10e64":"clf_xgb = XGBClassifier(objective = 'binary:logistic')","ea14dbd1":"param_dist = {'n_estimators': stats.randint(150, 1000),\n              'learning_rate': stats.uniform(0.01, 0.6),\n              'subsample': stats.uniform(0.3, 0.9),\n              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n              'colsample_bytree': stats.uniform(0.5, 0.9),\n              'min_child_weight': [1, 2, 3, 4]\n             }","8a166466":"#numFolds = 5\n#kfold_5 = cross_validation.KFold(n = len(X), shuffle = True, n_folds = numFolds)\n\nkfold = StratifiedKFold(n_splits=5)\n\nclf = RandomizedSearchCV(clf_xgb, \n                         param_distributions = param_dist,\n                         cv = kfold,  \n                         n_iter = 5,\n                         scoring = 'roc_auc', \n                         error_score = 0, \n                         verbose = 3, \n                         n_jobs = -1)","1886ed77":"# Train\nclf = XGBClassifier()\nclf.fit(X_train, y_train)\ncv_pred = cross_val_predict(clf, original_Xtest, original_ytest, cv=kfold)\n\n# Score\nprint('XGBoost classifier ROC AUC score: ', roc_auc_score(original_ytrain, cv_pred))","a730d07e":"clf = clf.fit(X_train, y_train)\npred = clf.predict(original_Xtest)\nacc = accuracy_score(original_ytest, pred) #Other way: vc.score(X_test, y_test)\nf1 = f1_score(original_ytest, pred)\ncv = cross_val_score(clf, original_Xtest, original_ytest).mean()","5669782b":"# Convert values into an array\n'''original_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values'''","c87d429d":"# List to append the score and then find the average\n'''accuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []'''","d70b145a":"# Make SMOTE happen during Cross Validation\n'''for train, test in kfold.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), clf)\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = clf.best_estimator_\n    prediction = clf.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))'''","b2197879":"import keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\n\nn_inputs = X_train.shape[1]\n\nundersample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])","33a4577a":"undersample_model.summary()","2301cc94":"undersample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","d2761960":"undersample_model.fit(X_train, y_train, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=2)","9163d23e":"undersample_predictions = undersample_model.predict(original_Xtest, batch_size=200, verbose=0)","0d07a81e":"undersample_fraud_predictions = undersample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)","c0ef4d9f":"import itertools\n\n# Create a confusion matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=14)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","5a3fce26":"undersample_cm = confusion_matrix(original_ytest, undersample_fraud_predictions)\nlabels = ['No Fraud', 'Fraud']\n\nplot_confusion_matrix(undersample_cm, labels, title=\"UnderSample \\n Confusion Matrix\", cmap=plt.cm.Reds)","93e6e63d":"fpr, tpr, thresold = roc_curve(y_train, cv_pred)","a6f1a5aa":"def logistic_roc_curve(fpr, tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('ROC AUC Curve', fontsize=16)\n    plt.plot(fpr, tpr, 'b-', linewidth=2)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n    \n    \nlogistic_roc_curve(fpr, tpr)\nplt.show()","226e4af9":"print('ROC AUC score: ', roc_auc_score(y_train, cv_pred))","595fe194":"confusion_matrix(y_train, cv_pred)","0d8c1291":"precision, recall, threshold = precision_recall_curve(y_train, cv_pred)\n\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","a4d7cf3a":"print(\"Precision:\", precision_score(y_train, cv_pred))\nprint(\"Recall:\",recall_score(y_train, cv_pred))","c811a14a":"## t-SNE","4bbecee6":"## Under-sample","e1424c8c":"## Baseline","6ff89e4c":"## Baseline","c30947c9":"# Explore","927244fa":"# Model","1dd2c54e":"# Introdution","7a4b3ee0":"# Evaluate","13b1d35a":"Note this over-sampling technique with SMOTE is very time expensive and can take hours to process.","23529fb6":"## Precision-Recall Curve","36a4c710":"This is anomaly type dataset. What you will essentially see here is using an under-sample to train a model and then some over-sampling technique (particualry SMOTE) to apply the model for whole dataset. The most common way how to evalute such model is a confusion matrix and precision-recall curve.","9191c06e":"## Full Sample","f0e19bd7":"All the the columns with the exception for Time and Amount have probably underwent PCA dimensionality reduction with feature scaling.","d6bb4217":"## Over-sampling","500d46ab":"# Load Libraries","968fcaa8":"# Load Data","47f30a25":"### Hyper-parameter tunning","97774eff":"# Data Preprocessing","bf0e911b":"## Confusion Matrix","456d61e0":"## Scaling","1c3b7b0c":"## ROC AUC","19c05907":"## Over-sample","a079f981":"## NN with under-sample","f45d3f05":"## Under-Sampling"}}