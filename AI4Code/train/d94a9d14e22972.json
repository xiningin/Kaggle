{"cell_type":{"73ac3c2a":"code","6d9c6d73":"code","9749eb11":"code","735545e3":"code","b2b3eadc":"code","fb24b277":"code","629559e7":"code","337541ab":"code","d72933c7":"code","98a52454":"code","c04679fe":"code","5ae13321":"code","a799d98b":"code","58da4578":"code","6d0323fd":"code","451f2f0c":"code","1564324b":"code","f0b248cd":"code","a5db62cd":"code","e89c7a49":"code","7273075b":"code","ad9577e8":"code","7aba2d2b":"code","ee954b9a":"code","9fe4eb5a":"code","1a4a7cf1":"code","64e13928":"code","9943cff2":"code","ea3d0a1b":"code","d1579b9b":"code","94853ddc":"code","a87798d0":"code","9382ae9b":"code","ffed4544":"code","8a33d2b1":"code","67edb659":"code","dd4ae8a7":"code","e8c6e967":"code","21d9feaa":"code","c5a65a1a":"code","7768719e":"code","3afd3dad":"code","11390a01":"code","d0daf1d0":"code","fef33298":"code","360bd6f8":"code","6de0086b":"code","02811445":"code","609cd8da":"code","c5e2d35c":"code","063c209b":"code","3fdef719":"code","89a3e88d":"code","183249db":"code","73e647ec":"code","67048b11":"code","0596a56a":"code","b73196fc":"code","d9f9a6a5":"code","1f242e93":"code","29ea43c0":"code","9d3d19a3":"code","9319da34":"code","6bd139fc":"code","cda51aae":"code","1bef3f81":"code","67c10023":"code","e030acf8":"code","b01f7013":"code","3dc2035c":"code","637d881d":"code","277f7e53":"code","87e06bf2":"code","384a0b6c":"code","a305dcfb":"code","9fb6d709":"code","b2f522f0":"code","b27d2edb":"code","c366a856":"code","3bc1c574":"code","050f2b06":"code","3accfa71":"code","7ef02949":"code","cfe8eb61":"code","6b797fc9":"code","cb208f60":"code","c86c0230":"code","8746c977":"code","2df7b0ea":"code","8e8102b1":"code","d6c7f0f5":"code","5d2803d8":"code","1f0f22ab":"code","89d17c92":"code","eb47c121":"code","f9ca3d4c":"code","11d2f29d":"code","5ae8e0e0":"code","cc003101":"code","82bd1b8f":"code","fdd3d1a4":"code","0a9b21c6":"code","fa9684cf":"code","6d8564ae":"code","ba3da848":"code","69bb3e70":"code","751487ec":"code","c358e227":"code","eda8fff2":"code","0706c6e2":"code","dbd89bc6":"code","ceba406b":"code","2573864c":"code","bd1257cf":"code","d36e2202":"code","13d81878":"code","eef70231":"code","710b1825":"code","9660ea80":"code","26c55bfb":"code","170e9859":"code","406f53e2":"code","95a168b2":"code","2d1f648c":"code","795b24e4":"code","ca26f2b9":"code","ddc00712":"code","2950acf4":"code","e95fbfd4":"code","bbcfbd74":"code","af1c51f0":"code","cc94b9c1":"code","7364038b":"code","80643c20":"code","b199182c":"code","c0588037":"code","767abd7e":"code","ea9cdecb":"code","1a5a24b4":"markdown","5ccd6b59":"markdown","dec7da76":"markdown","8b53e345":"markdown","6ba11dfd":"markdown","59b72038":"markdown","e6fa62bc":"markdown","4277c569":"markdown","3b3583b3":"markdown","771d51b4":"markdown","d0baac56":"markdown","b2e0ea53":"markdown","38f1efae":"markdown","5a8dff3b":"markdown","8d3e407a":"markdown","178afced":"markdown","ba119f9e":"markdown","63c45027":"markdown","37de1d52":"markdown","21933863":"markdown","e3193727":"markdown","dc9510a0":"markdown","6ae46163":"markdown","39bfa7bc":"markdown","f2aa3c2a":"markdown","2f9915c3":"markdown","b2e03a84":"markdown"},"source":{"73ac3c2a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6d9c6d73":"\"\"\"\nIr\u0131s data seti y\u00fckledik:\n\"\"\"\niris = pd.read_csv('..\/input\/iris-flower-dataset\/IRIS.csv')","9749eb11":"\"\"\"\nsepal_length, sepal_width, petal_length, petal_width are features (predictor variables)\n\nspecies (setosa, versicolor, virginica) are target variables\n\n\"\"\"\n\niris.head()","735545e3":"\"\"\"\nbunu belirtmek i\u00e7in target variable = y, predictor variables = X olarak ay\u0131rmam\u0131z gerekiyor.\n\n\"\"\"\n\nX = iris.drop('species', axis=1).values  \ny = iris['species'].values\n\n\"\"\"\n.values diyerek modelin istedi\u011fi formata (array) soktuk. \n\n\"\"\"","b2b3eadc":"\"\"\"\nBasic Idea of KNN: k-closest data point'e bakarak unlabeled data point'in label'\u0131n\u0131 tahmin etme\n\n\"\"\"","fb24b277":"#model importing\n\nfrom sklearn.neighbors import KNeighborsClassifier #classifier \u0131 import ettik.\n\n#calling classifier with a variable\n\nknn = KNeighborsClassifier(n_neighbors=6) #classifier'\u0131 knn variable'\u0131na atad\u0131k. ","629559e7":"#train-test-split\n\n#modele yerle\u015ftirilen datan\u0131n bir k\u0131sm\u0131 ile train yap\u0131l\u0131rken di\u011fer bir k\u0131sm\u0131 ile de bu train\n#test edilir. yani, \u00f6rne\u011fin, datan\u0131n %70'i ile optimum weight'ler belirlenir, kalan %30luk k\u0131s\u0131m ise bu modelin\n#do\u011frulu\u011fu test edilir.\n\nfrom sklearn.model_selection import train_test_split\n\n\"\"\"\nX_train: train data\nX_test: test data\ny_train: training labels\ny_test: test labels\n\nX:feature data, \ny:target, \ntest_size:train-test oran\u0131, \nrandom_state:ayn\u0131 sonucu verir denedi\u011fimiz randomstate Id'si.\n\"\"\"\nX_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=21) # X ve y'yi kullanarak train-test yapt\u0131k.\n","337541ab":"#.fit() \n\nknn.fit(X_train, y_train) #train data ile fit(yani train) yapt\u0131k. bu \u015fu demek, datan\u0131n %70lik k\u0131sm\u0131n\u0131n feature ve target(elimizde bilgi mevcut)\n#valuelar\u0131n\u0131 kullanarak modeli olu\u015fturduk. ","d72933c7":"#.predict()\n\ny_pred = knn.predict(X_test) #X_test (yani kalan %30luk feature datas\u0131) n\u0131 kullanarak bu feature dataya kar\u015f\u0131l\u0131k gelen prediction'lar\u0131 olu\u015fturduk\n#buna da y_pred dedik. elimizde \u015fu an y_pred ve y_test(ger\u00e7ek de\u011ferler) mevcut. yani art\u0131k modelin performans\u0131n\u0131 \u00f6l\u00e7ebiliriz.b","98a52454":"#.score()\n\nknn.score(X_test, y_test) #score() ile modelin performans\u0131n\u0131 \u00f6l\u00e7eriz. X_test'i verip, ondan elde edilen tahmin ile, y_test(ger\u00e7ek de\u011fer)i k\u0131yaslar.\n","c04679fe":"\"\"\"\nOverfitting - Underfitting: finding optimum k-value\n\nburada, manual olarak belirlenmesi gereken n_neighbors(k) parametresi mevcut. (hyperparameter)\nk de\u011feri k\u00fc\u00e7\u00fck olursa overfitting yapm\u0131\u015f olabiliriz.\nb\u00fcy\u00fcd\u00fck\u00e7e ise underfiting yapm\u0131\u015f olabiliriz. \noptimum k de\u011ferini bulmam\u0131z gerekir!\n\n\"\"\"\n\n# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 20)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n\n#grafi\u011fe g\u00f6re, testing acuracy 2-3-4-5 ayn\u0131 sonucu veriyor, sonra d\u00fc\u015f\u00fc\u015f var. n=5 ideal se\u00e7im diyebiliriz.","5ae13321":"\"\"\"\n\u0130smi regression olabilir, ama classification'da kullan\u0131l\u0131r.\n\nMekanizmay\u0131 \u00f6zetlemek gerekirse, \nLog Reg bize bir p,probability verir. p>0.5 ise target=1, p<0.5 ise target=0 diye sonu\u00e7lan\u0131r.\nlog reg bize linear decision boundary sa\u011flar. \n\n\"\"\"","a799d98b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","58da4578":"logreg = LogisticRegression()","6d0323fd":"X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.4, random_state=42)","451f2f0c":"logreg.fit(X_train, y_train)","1564324b":"y_pred = logreg.predict(X_test)","f0b248cd":"\"\"\"\nyukar\u0131da bahsetti\u011fimiz p-value default 0.5, ancak bu de\u011feri de\u011fi\u015ftirirsek ne olur:\nmesela, p=0.0 olursa, her\u015feyi 1 tahmin eder, p=1.0 olursa da her\u015feyi 0 tahmin edecektir.\nbu p-value'ya threshold deriz, bu threshold'un de\u011fi\u015fiminin etkisini inceledi\u011fimiz curve'e ise\nROC (Receiver Operating Characteristic) Curve deriz.\nAncak, ROC curve binary classification i\u00e7in ge\u00e7erlidir. bu y\u00fczden heart disease dataseti kullanaca\u011f\u0131z:\n\n\"\"\"","a5db62cd":"df = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","e89c7a49":"df.head()","7273075b":"X = df.drop('target', axis=1).values  \ny = df['target'].values","ad9577e8":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","7aba2d2b":"logreg = LogisticRegression()","ee954b9a":"X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.4, random_state=42)","9fe4eb5a":"logreg.fit(X_train, y_train)","1a4a7cf1":"y_pred = logreg.predict(X_test)","64e13928":"from sklearn.metrics import roc_curve","9943cff2":"y_pred_prob = logreg.predict_proba(X_test)[:,1] #predict_proba 2 column d\u00f6ner, 1.si index oldu\u011fu i\u00e7in 2.column\u0131 se\u00e7tik.","ea3d0a1b":"fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob) #fpr:false pos.rate, tpr: true positive rate","d1579b9b":"#grafi\u011fi \u00e7izelim:\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label= 'Log Reg')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('LogReg ROC Curve')\nplt.show()","94853ddc":"\"\"\" Another metric for classification models.\n\nROC Curve'un alt\u0131ndaki alan (Area Under the ROC Curve - AUC) ne kadar fazla ise model o kadar iyi.\n\n\"\"\"","a87798d0":"from sklearn.metrics import roc_auc_score","9382ae9b":"roc_auc_score(y_test, y_pred_prob)","ffed4544":"from sklearn.model_selection import cross_val_score","8a33d2b1":"cv_scores = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc') #scoring ile metodu belirledik.","67edb659":"print(cv_scores)","dd4ae8a7":"\"\"\"\nAccuracy scores her zaman bize do\u011fru bilgi vermez. \u00e7\u00fcnk\u00fc;\n\u00f6rne\u011fin, spam - not spam \u00f6rne\u011finde, %99 real email, %1 spam iken hepsine real derse %99 do\u011fru sonucu vermi\u015f olur,\naccuracy %99 olur. ama, bu bize modelin do\u011frulu\u011fu ile ilgili bilgiden ziyade, datan\u0131n i\u00e7eri\u011finden kaynakl\u0131 bilgi vermi\u015f olur.\nyani, data imbalanced't\u0131r. \n\nConfusion Matrix:\n    Classification problemlerinde kullan\u0131l\u0131r. CM nedir:\n    e\u011fer email spam'se ve spam olarak tahmin edildiyse True Positive\n    e\u011fer email spam'se ve real olarak tahmin edildiyse False Negative\n    e\u011fer email real'se ve real olarak tahmin edildiyse True Negative\n    e\u011fer email real'se ve spam olarak tahmin edildiyse False Positive'dir.\n    \n    burada spam arad\u0131\u011f\u0131m\u0131z i\u00e7in spam'e positive demi\u015f olduk. bu bize ba\u011fl\u0131.\n    \n    accuracy = (tp+tn) \/ (tp+tn+fp+fn)\n    precision = tp \/ (tp+fp)\n    recall (sensitivity) = tp \/ (tp+fn)\n    f1-score = 2 * precision * recall \/ (precision + recall)\n    \n    high preision: not many real emails predicted as spam\n    high recall: predicted most spam emails correctly.\n    \n\n\"\"\"","e8c6e967":"\"\"\"\nyukar\u0131da a\u00e7\u0131klad\u0131k zaten CM'i.\n\"\"\"\n#model import:\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n","21d9feaa":"#yukar\u0131daki KNN modelini kullanarak Confusion Matrix'i \u00f6rneklendirelim:\nX = iris.drop('species', axis=1).values  \ny = iris['species'].values","c5a65a1a":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=8) ","7768719e":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=21)","3afd3dad":"knn.fit(X_train, y_train)","11390a01":"y_pred = knn.predict(X_test)","d0daf1d0":"print(confusion_matrix(y_test, y_pred)) #-->>> ilk arguman true labellardan, ikinci arg\u00fcman da predictionlardan olu\u015ftu.","fef33298":"print(classification_report(y_test, y_pred))","360bd6f8":"boston = pd.read_csv('..\/input\/bostoncsv\/Boston.csv')","6de0086b":"boston.drop('Unnamed: 0', axis=1, inplace=True)\nboston.head()\n\n#target value medv'dir, di\u011ferleri feature(predictor variable)","02811445":"# X ve y yi belirleyelim:\n\nX = boston.drop('medv', axis=1).values \ny = boston['medv'].values","609cd8da":"#linear regression, y=a+bx. more dimensional olunca y = a+bx1+cx2... \n#ama\u00e7, bu line \u00f6yle bir line olsun ki, residuals'\u0131(error unction)(loss or cost function) \n#(data point'lerin line'dan uzakl\u0131klar\u0131(n\u0131n kareleri)) toplam\u0131 minimum olsun.\n#y target, x feature, a ve b de bizim \u00f6\u011frenmek istedi\u011fimiz parametreler.\n#elde etti\u011fimiz, residuals\u0131n karelerini minimize eden functiona da OLS (ORD\u0130NARY Least Squares) denir.","c5e2d35c":"#sadece 1 column'\u0131 alarak ba\u015flayal\u0131m; (one dimensional linear regression)\nX_rooms = X[:,5]","063c209b":"#shape'lere bakal\u0131m:\n\nX_rooms.shape","3fdef719":"y.shape","89a3e88d":"#shape'leri istenen formata getirelim:\n\ny = y.reshape(-1,1)","183249db":"#istenen shape:\n\ny.shape","73e647ec":"X_rooms = X_rooms.reshape(-1,1)","67048b11":"X_rooms.shape\n\n#tamamd\u0131r!..","0596a56a":"#bir grafikle iki variable'\u0131 g\u00f6relim:\n\nplt.scatter(X_rooms, y)\nplt.ylabel('value of house')\nplt.xlabel('nr of rooms')","b73196fc":"#art\u0131k regression'a geldik:\n\nfrom sklearn import linear_model #linear model'\u0131 import ettik.\n","d9f9a6a5":"reg = linear_model.LinearRegression() #modeli reg variable'\u0131na atad\u0131k.","1f242e93":"reg.fit(X_rooms, y) #X_rooms ve y'ye g\u00f6re fit ettik.","29ea43c0":"#fitting line'\u0131 grafik olarak g\u00f6stermek i\u00e7in,\nprediction_space = np.linspace(min(X_rooms), max(X_rooms)).reshape(-1,1)\nplt.scatter(X_rooms, y)\nplt.plot(prediction_space, reg.predict(prediction_space), color='blue')","9d3d19a3":"#\u015e\u0130MD\u0130, t\u00fcm datay\u0131 kullanarak more dimensional linear regression yapal\u0131m:\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=21)","9319da34":"reg_all = linear_model.LinearRegression()\n\nreg_all.fit(X_train, y_train)\n\ny_pred = reg_all.predict(X_test)\n\nreg_all.score(X_test, y_test)","6bd139fc":"\"\"\"\nModel performans\u0131 datan\u0131n nas\u0131l split edildi\u011fiyle alakal\u0131 olabilir.\nBu y\u00fczden, buldu\u011fumuz model performans score aldat\u0131c\u0131 olabilir.\nBu sorunu \u00e7\u00f6zmek i\u00e7in cross validation metodu kullan\u0131l\u0131r.\n\n\u0130\u015fleyi\u015fi \u015fu \u015fekildedir:\ntest_size=0.2 ise, CV ile, farkl\u0131 0.2'lik gruplara ay\u0131rarak modeli test eder.\n\u00f6nce ilk 0.2'yi test olarak al\u0131r, sonra 2. 0.2lik k\u0131sm\u0131...\nbu \u015fekilde 5 farkl\u0131 train-test grubuna g\u00f6re performans belirlenir.\nbu, 5-fold CV olarak addedilir. \n\nk-fold CV 'deki k de\u011ferinin optimumunu bulmam\u0131z gerekir. (k b\u00fcy\u00fck olursa i\u015flem s\u00fcresi artar, \nk k\u00fc\u00e7\u00fck olursa da ge\u00e7erlili\u011fini \u00f6l\u00e7emeyiz)\n\n\"\"\"","cda51aae":"#import CV:\nfrom sklearn.model_selection import cross_val_score ","1bef3f81":"#\u00fcstteki dataya g\u00f6re 5-fold cv:\ncv_results = cross_val_score(reg_all, X, y, cv=5) ","67c10023":"#sonu\u00e7lar:\ncv_results","e030acf8":"# 5 farkl\u0131 sonucun ortalamas\u0131:\nnp.mean(cv_results)","b01f7013":"\"\"\"\nLinear Regression ile loss function'\u0131 minimize etmektir fitting line'\u0131n amac\u0131.\ny=a1x1+a2x2+... 'daki a1,a2,ai lerin se\u00e7imi yap\u0131l\u0131r buna g\u00f6re. \nburadaki coefficient'lar\u0131n fazla b\u00fcy\u00fck se\u00e7imi overfitting getirir.\nmulti dimensional linear regression'\u0131 d\u00fc\u015f\u00fcn\u00fcrsek bu durum sa\u011fl\u0131kl\u0131 sonu\u00e7 almay\u0131 engeller.\nbunu a\u015fmak i\u00e7in large coefficientlar\u0131 penalize eden bir yap\u0131 ile loss function kontrol edilebilir.\n\u0130\u015fte buna regularization deniyor.\n\n\"\"\"","3dc2035c":"\"\"\"\nRegularized Regression Types:\n\n1. Ridge Regression \n\n(Loss function = OLS loss function + alfa* sum of squared values of each coefficient) \n\n(bu \u015fekilde, Loss function coefficientlerin karelerinin toplamlar\u0131n\u0131n alfa ile \u00e7arp\u0131m\u0131 kadar artm\u0131\u015ft\u0131r. \nama\u00e7 ikinci k\u0131sm\u0131n da min olmas\u0131n\u0131 sa\u011flamakt\u0131r.)\n\n(alpha, hyperparameter'd\u0131r.)\n\n2. Lasso Regression\n\n(Loss function = OLS loss function + alfa* sum of absolute values of each coefficient) !!!Tek fark bu!!!\n\n!! Lasso regression, important feature se\u00e7imi yap\u0131lmakta kullan\u0131l\u0131r. \u00c7\u00fcnk\u00fc, k\u00fc\u00e7\u00fck coefficientleri 0'a k\u00fc\u00e7\u00fcltme e\u011filimindedir.\n0'a yuvarlanmam\u0131\u015f feature'lar important olarak tan\u0131mlanabilir.\n\n\"\"\"","637d881d":"#importing\nfrom sklearn.linear_model import Ridge","277f7e53":"ridge = Ridge(alpha=0.1, normalize=True) #modeli olu\u015fturduk. normalize=True t\u00fcm variable'lar ayn\u0131 scale'da olmas\u0131 i\u00e7in.","87e06bf2":"ridge.fit(X_train, y_train) #train ettik","384a0b6c":"ridge_pred= ridge.predict(X_test)","a305dcfb":"ridge.score(X_test, y_test)","9fb6d709":"from sklearn.linear_model import Lasso","b2f522f0":"lasso = Lasso(alpha=0.1, normalize=True)","b27d2edb":"lasso.fit(X_train, y_train)","c366a856":"lasso_pred = lasso.predict(X_test)","3bc1c574":"lasso.score(X_test, y_test)","050f2b06":"#yukar\u0131da bahsetti\u011fimiz, \u00f6nemsiz feature'lar\u0131 0 yap\u0131p \u00f6nemli olanlar\u0131 belirleme i\u015flemi:\nlasso_coef = lasso.fit(X, y).coef_","3accfa71":"lasso_coef","7ef02949":"\"\"\"\nModeldeki hyperparameterlar\u0131 belirterek grid search yaparak hyperparameterlar\u0131n optimum de\u011ferini bulmam\u0131z\u0131 sa\u011flar.\n\"\"\"","cfe8eb61":"iris = pd.read_csv('..\/input\/iris-flower-dataset\/IRIS.csv')","6b797fc9":"X = iris.drop('species', axis=1).values  \ny = iris['species'].values","cb208f60":"from sklearn.model_selection import GridSearchCV","c86c0230":"param_grid = {'n_neighbors': np.arange(1,50)} #dict i\u00e7inde '' i\u00e7inde grid edilecek parametreyi yaz\u0131p, kar\u015f\u0131l\u0131\u011f\u0131nda da hangi de\u011ferlerin \n#denenece\u011fini yazd\u0131k.","8746c977":"knn = KNeighborsClassifier()","2df7b0ea":"knn_cv = GridSearchCV(knn, param_grid, cv=5) #this return the grid search object. fit etmeliyiz","8e8102b1":"knn_cv.fit(X,y)","d6c7f0f5":"knn_cv.best_params_","5d2803d8":"knn_cv.best_score_","1f0f22ab":"\"\"\"\nGridSearchCV'den fark\u0131, \u00e7oklu hyperparameter belirlemede calculation yo\u011funlu\u011fu oldu\u011fu zaman kullan\u0131lmas\u0131d\u0131r.\nT\u00fcm de\u011ferler denenmez, bir prob.dist kullan\u0131larak hesaplama yap\u0131l\u0131r.\n\nBu uygulamay\u0131 yapmak i\u00e7in RandomForest kullanal\u0131m:\n\"\"\"","89d17c92":"from scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","eb47c121":"# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [3, None],\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}","f9ca3d4c":"# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()","11d2f29d":"# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)","5ae8e0e0":"# Fit it to the data\ntree_cv.fit(X, y)","cc003101":"# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))","82bd1b8f":"\"\"\"\ncategorical data type i\u00e7in, \n\n\n\n\"\"\"","fdd3d1a4":"df = pd.read_csv('..\/input\/iris-flower-dataset\/IRIS.csv')","0a9b21c6":"df.head()","fa9684cf":"df_species = pd.get_dummies(df)","6d8564ae":"df_species.head()","ba3da848":"\"\"\"\n3'\u00fcnden 2si de\u011filse di\u011feridir. bu y\u00fczden kulland\u0131\u011f\u0131m\u0131z bir parametre var.\n\"\"\"","69bb3e70":"df_species2 = pd.get_dummies(df, drop_first=True)","751487ec":"df_species2.head()","c358e227":"\"\"\"\nbazen, missing value olarak g\u00f6r\u00fclmez, 0, '', vs vs olur.\nbu durumda, hem nan value^lar\u0131 smartly doldurmak i\u00e7in Imputer object kullanabiliriz.\n\n\"\"\"","eda8fff2":"df = pd.read_csv('..\/input\/diabetescsv\/diabetes.csv')","0706c6e2":"df.head() #mesela burada, insulin = 0, bu not possible","dbd89bc6":"df.Insulin.replace(0, np.nan, inplace=True) #T\u00dcM 0 lar\u0131 nan value yapt\u0131k.\ndf.SkinThickness.replace(0, np.nan, inplace=True)\ndf.BMI.replace(0, np.nan, inplace=True)","ceba406b":"df.head()","2573864c":"df.info()","bd1257cf":"# df = df.dropna() # missing value olan t\u00fcm row'lar\u0131 drop ettik.","d36e2202":"# df.shape #ancak, datan\u0131n yar\u0131s\u0131n\u0131 kaybettik. unacceptible","13d81878":"from sklearn.linear_model import LogisticRegression\n\n#nan value'lar\u0131n yerine ba\u015fka \u015feyler koyabiliriz stratejiye ba\u011fl\u0131 olarak:\nfrom sklearn.preprocessing import Imputer","eef70231":"imp = Imputer(missing_values='NaN', strategy='mean', axis=0) #missing_values = 'Nan' olarak g\u00f6sterimi\u015f demek. axis=0 ile de sadece o column'a bakt\u0131k.","710b1825":"X = df.drop('Outcome', axis=1).values  \ny = df['Outcome'].values","9660ea80":"imp.fit(X)","26c55bfb":"X = imp.transform(X)","170e9859":"\"\"\"\nImputer'\u0131n yapt\u0131\u011f\u0131 i\u015fi yapar, \u00fcst\u00fcne bir de model \u00e7al\u0131\u015ft\u0131r\u0131r.\n\"\"\"","406f53e2":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n","95a168b2":"imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\nlogreg = LogisticRegression()","2d1f648c":"steps = [('imputation', imp), \n         ('logistic_regression', logreg)] #imputer modeli ve uygulanacak reg modelini steps e yazd\u0131k","795b24e4":"pipeline = Pipeline(steps)","ca26f2b9":"X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=21)","ddc00712":"pipeline.fit(X_train, y_train)","2950acf4":"y_pred = pipeline.predict(X_test)","e95fbfd4":"pipeline.score(X_test, y_test)","bbcfbd74":"\"\"\"\nFarkl\u0131 feature'lar i\u00e7in farkl\u0131 range'de de\u011ferlerin olmas\u0131, ML modellerinin \u00e7al\u0131\u015fmas\u0131n\u0131 etkileyecektir.\nE\u011fer scaling(normalizing) yapmazsak baz\u0131 feature'lar modelde daha a\u011f\u0131rl\u0131kl\u0131 de\u011ferlendirilecek, \nsonu\u00e7lar yan\u0131lt\u0131c\u0131 olacakt\u0131r.\n\n\"\"\"","af1c51f0":"iris = pd.read_csv('..\/input\/iris-flower-dataset\/IRIS.csv')","cc94b9c1":"iris.head()","7364038b":"X =iris.drop('species', axis=1).values  \ny = iris['species'].values","80643c20":"from sklearn.preprocessing import scale","b199182c":"X_scaled = scale(X)","c0588037":"np.mean(X), np.std(X)","767abd7e":"np.mean(X_scaled), np.std(X_scaled)","ea9cdecb":"\"\"\" burada da, pipeline ile hem imputation, hem scaling, hem model \u00e7al\u0131\u015fmas\u0131,\n    ard\u0131ndan da gridsearchcv ile hyperparameter optimization bir arada.\n    \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\n\n# Setup the pipeline steps: steps\nsteps = [('imputation', Imputer(missing_values='NaN', strategy='mean', axis=0)),\n         ('scaler', StandardScaler()),\n         ('elasticnet', ElasticNet())]\n\n# Create the pipeline: pipeline \npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Create the GridSearchCV object: gm_cv\ngm_cv = GridSearchCV(pipeline, parameters)\n\n# Fit to the training set\ngm_cv.fit(X_train, y_train)\n\n# Compute and print the metrics\nprint(\"Tuned ElasticNet Alpha: {}\".format(gm_cv.best_params_))\n\n\"\"\"","1a5a24b4":"<a id=\"9\"><\/a> <br>\n**REGRESSIONS**","5ccd6b59":"<a id=\"22\"><\/a> <br>\n**Pipeline**","dec7da76":"**Supervised Learning**: uses labeled data\n\n**Unsupervised Learning**: uses unlabeled data. uncovering hidden patterns and structures from unlabeled data.\n\n","8b53e345":"<a id=\"7\"><\/a> <br>\n**AUC using Cross Validation**","6ba11dfd":"<a id=\"17\"><\/a> <br>\n**Randomized Search CV**","59b72038":"<a id=\"14\"><\/a> <br>\n**Lasso Regression**","e6fa62bc":"<a id=\"6\"><\/a> <br>\n**AUC (Area Under the ROC Curve)**","4277c569":"<a id=\"21\"><\/a> <br>\n**Imputer**","3b3583b3":"<a id=\"2\"><\/a> <br>\n**CLASSIFICATIONS**","771d51b4":"<a id=\"3\"><\/a> <br>\n**K-Nearest Neighbors (KNN)**","d0baac56":"**Not:**\n\nE\u011fer **target** variable evet-hay\u0131r, spam-notspam gibi **kategori** bi\u00e7iminde ise **CLASSIFICATION**,\n\nE\u011fer **target** variable **continuously varying variable** ise, \u00f6rne\u011fin maa\u015f, ev fiyat\u0131 gibi, **REGRESSION** uygulan\u0131r.","b2e0ea53":"<a id=\"19\"><\/a> <br>\n**Dummy Variables**","38f1efae":"<a id=\"23\"><\/a> <br>\n**Normalizing (Scaling, or Centering)**","5a8dff3b":"<a id=\"13\"><\/a> <br>\n**Ridge Regression**","8d3e407a":"<a id=\"20\"><\/a> <br>\n**Missing Values**","178afced":"Not:\n\nE\u011fer **target** variable **continuously varying variable** ise, \u00f6rne\u011fin maa\u015f, ev fiyat\u0131 gibi, **REGRESSION** uygulan\u0131r.","ba119f9e":"<a id=\"12\"><\/a> <br>\n**Regularized Regression**","63c45027":"<a id=\"16\"><\/a> <br>\n**Grid Search CV**","37de1d52":"<a id=\"18\"><\/a> <br>\n**Preprocessing Data**","21933863":"<a id=\"1\"><\/a> <br>\n**SUPERVISED LEARNING**\n\nSupervised learning'de, predictor variables (features)(independent variable) ve target variable (dependent variable) (response variable) vard\u0131r. \n\nAma\u00e7, target variable'\u0131 predictor variable'lar\u0131 kullanarak tahmin edecek modeli olu\u015fturmakt\u0131r.","e3193727":"<a id=\"8\"><\/a> <br>\n**Confusion Matrix**","dc9510a0":"****CONTENT****\n\n[SUPERVISED LEARNING](#1)\n*  [CLASSIFICATIONS](#2) \n    * [K-Nearest Neighbors (KNN)](#3)\n    * [Logistic Regression](#4)\n    * [ROC Curve with Log Reg](#5)\n    * [AUC (Area Under the ROC Curve)](#6)\n    * [AUC using Cross Validation](#7)\n    * [Confusion Matrix](#8)\n*  [REGRESSIONS](#9)  \n    * [L\u0131near Regression](#10)\n    * [Cross Validation](#11)\n    * [Regularized Regression](#12)\n        * [Ridge Regression](#13)\n        * [Lasso Regression](#14)  \n* [Hyperparameter Tuning](#15)\n    * [Grid Search CV](#16)\n    * [Randomized Search CV](#17)\n* [Preprocessing Data](#18)\n    * [Dummy Variables](#19)\n    * [Missing Values](#20)\n        * [Imputer](#21)\n        * [Pipeline](#22)  \n    * [Normalizing (Scaling, or Centering)](#23)\n    \n        \n        \n    ","6ae46163":"<a id=\"10\"><\/a> <br>\n**L\u0131near Regression**","39bfa7bc":"<a id=\"11\"><\/a> <br>\n**Cross Validation**","f2aa3c2a":"<a id=\"15\"><\/a> <br>\n**Hyperparameter Tuning**","2f9915c3":"<a id=\"4\"><\/a> <br>\n**Logistic Regression**","b2e03a84":"<a id=\"5\"><\/a> <br>\n**ROC Curve with Log Reg**"}}