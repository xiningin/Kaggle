{"cell_type":{"ef2ee3c9":"code","26571fee":"code","6840dae9":"code","4e3e3ca9":"code","f0379609":"code","dac221b5":"code","28e33853":"code","ec067263":"code","4853a99d":"code","4d0f588b":"code","6c613a26":"code","7bbf619e":"code","0c0fc101":"code","f7202f43":"code","65c6c9f4":"code","215241ad":"code","39018c49":"code","49afc133":"code","33d9e0ce":"code","369fc74d":"code","98a20135":"code","96bf03c6":"code","a4589905":"code","d27fa66b":"code","b79a8504":"code","30933491":"code","fa30660e":"code","d0668611":"code","d2133ed0":"code","384002c0":"code","a4c4226d":"code","d3937f2d":"code","40a11545":"code","cad9f202":"code","20aea4a0":"code","d128fd96":"code","253119df":"markdown","352afeb9":"markdown","e6fc3509":"markdown"},"source":{"ef2ee3c9":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n\n#sklearn \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\n\n# nlp preprocessing lib\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nimport string \npunctation = string.punctuation","26571fee":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","6840dae9":"train_df.head()","4e3e3ca9":"train_df = train_df.drop(['id', 'keyword', 'location'], axis = 1)","f0379609":"train_df.shape","dac221b5":"train_df.columns","28e33853":"train_df.info()","ec067263":"train_df.describe()","4853a99d":"train_df[train_df[\"target\"] == 1][\"text\"].values[0]","4d0f588b":"train_df[train_df[\"target\"] == 1][\"text\"].values[1]","6c613a26":"print(\"Number of duplicates in data : {}\".format(len(train_df[train_df.duplicated()])))","7bbf619e":"print(\"Duplicated rows before remove them : \")\ntrain_df[train_df.duplicated(keep=False)].sort_values(by=\"text\").head(8)","0c0fc101":"#remove duplicated rows\ntrain_df.drop_duplicates(inplace=True)","f7202f43":"print(\"Number of duplicates in data : {}\".format(len(train_df[train_df.duplicated()])))","65c6c9f4":"train_df['target'].value_counts()","215241ad":"# count plot \"Histogram\" of Frequencies of Subjects for true news\nplt.figure(figsize=(10,6))\nplt.title(\"Frequencies of tweets for Disaster\")\nsns.countplot(x = 'target', data = train_df)\nplt.xlabel('Disaster Type')","39018c49":"Real_Disaster_df = train_df[train_df['target'] == 1]\nReal_Disaster_df.head()","49afc133":"Not_Real_Disaster_df = train_df[train_df['target'] == 0]\nNot_Real_Disaster_df.head()","33d9e0ce":"Real_Disaster_text = ' '.join(Real_Disaster_df.text.tolist())","369fc74d":"wordcloud_true = WordCloud().generate(Real_Disaster_text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud_true)\nplt.axis('off')\nplt.title(\"Word Cloud of Real Disaster news\")\nplt.tight_layout(pad=0)\nplt.show()","98a20135":"Not_Real_Disaster_text = ' '.join(Not_Real_Disaster_df.text.tolist())","96bf03c6":"wordcloud_true = WordCloud().generate(Not_Real_Disaster_text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud_true)\nplt.axis('off')\nplt.title(\"Word Cloud of Not RealDisaster twittes\")\nplt.tight_layout(pad=0)\nplt.show()\n","a4589905":"# take text and preprocess 'remove stopwords [a, the, and, thus, ... etc] and punctations[,%$ ..etc] and len of text less than 3' \ndef clean_text(text):\n    \"\"\"\n        text: a string \n        return: cleaned string\n    \"\"\"\n    result = []\n    for token in simple_preprocess(text):\n        if token not in STOPWORDS and token not in punctation and  len(token) >= 3 :\n            token = token.lower() \n            result.append(token)    \n    return \" \".join(result)","d27fa66b":"train_df['text'] = train_df['text'].map(clean_text)\ntrain_df.head()","b79a8504":"from sklearn.utils import shuffle\ntrain_df_shuffled = shuffle(train_df)\ntrain_df_shuffled.head()","30933491":"X = train_df_shuffled['text']\ny = train_df_shuffled['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42, stratify = y)","fa30660e":"X_test","d0668611":"from sklearn.model_selection import cross_val_score\nnb_classifier = Pipeline([('vect', CountVectorizer()),\n               ('tfidf', TfidfTransformer()),\n               ('clf', MultinomialNB()),])\n\nnb_classifier.fit(X_train, y_train)\n\ny_pred = nb_classifier.predict(X_test)\nprint('accuracy {}'.format(accuracy_score(y_pred, y_test)))","d2133ed0":"sgd = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', SGDClassifier(loss='epsilon_insensitive', penalty='l2',alpha=1e-3, random_state=42, max_iter=1000, tol=None)),])\n\n\nsgd.fit(X_train, y_train)\ny_pred = sgd.predict(X_test)\nprint('accuracy {}'.format(accuracy_score(y_pred, y_test)))","384002c0":"test_df = test_df.drop(['id', 'keyword', 'location'], axis = 1)","a4c4226d":"test_df['text'] = test_df['text'].map(clean_text)\ntest_df.head()","d3937f2d":"y_pred = nb_classifier.predict(test_df['text'])","40a11545":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","cad9f202":"sample_submission[\"target\"] = y_pred","20aea4a0":"sample_submission.head()","d128fd96":"sample_submission.to_csv(\"submission.csv\", index=False)","253119df":"### Text Preprocessing","352afeb9":"Now, in the viewer, you can submit the above file to the competition! Good luck!","e6fc3509":"### EDA\n\nLet's Explore our data.."}}