{"cell_type":{"303eae2f":"code","26cd1ff7":"code","9032a6fd":"code","b4acfdc6":"code","446b78f4":"code","8ec87de1":"code","7a56d7be":"code","1d51b9f0":"code","5c4f9aba":"code","0827657e":"code","8f3cf2bc":"code","70c6cebb":"code","94dab489":"code","43701d0b":"code","3172e596":"code","5b64cc7c":"code","cf3ae9ee":"code","e4e101b9":"code","75db055e":"code","035ead0c":"code","0e5c4267":"code","6e5071ee":"code","8e991322":"code","68f29af5":"code","a3527fd6":"code","cab7e6a9":"code","4448acc0":"code","c0688c08":"code","a463309e":"code","d5def076":"code","6e4a54f0":"code","ef8f8170":"code","ba157de8":"code","1e3cb389":"code","f22a00ad":"code","f2aea311":"code","0f589fa7":"code","78cbe7e2":"code","b910baa0":"code","68bb3a41":"code","a581e156":"code","9d055e8c":"code","4a259bfc":"code","1ba4ceaf":"code","0340f5aa":"code","4af432b0":"code","04daec1f":"code","415fdbc1":"code","ebc84084":"code","fce1a08f":"code","5f64a64e":"code","45385b79":"code","768d21cd":"code","5ca1620e":"code","61e32656":"code","adbaf885":"code","e89e588d":"code","b92cda14":"code","44ea9318":"code","20363346":"code","513eee93":"code","360770ee":"code","a1a0ab92":"code","946c8b78":"code","b5c4f4b4":"code","c1d7d931":"code","125e81e0":"code","d0fd2e80":"code","ec14fca4":"code","69d64447":"code","f0f27046":"code","eb8e4a1b":"code","82f17060":"code","c5fcf67c":"code","5f7eb5b7":"code","10e4c346":"code","124fbc59":"code","a8c1af31":"code","3593556b":"code","157c1c80":"code","3a366b07":"code","19f0dbdf":"code","977ba729":"code","15ae7e2c":"code","9f62a06d":"code","2b9f0354":"code","f96595b6":"code","98b61e2d":"code","7271e72a":"code","f5bf6746":"code","08286cd0":"code","630a2635":"markdown","db47c15b":"markdown","eec87ccd":"markdown","f6356f98":"markdown","d56580cd":"markdown","11fd2fa4":"markdown","a7b106a1":"markdown","0b803090":"markdown","404fb5a8":"markdown","001e62bb":"markdown","af2740b1":"markdown","804463c6":"markdown","7ed8e433":"markdown","b44a5372":"markdown","8a9b9825":"markdown","ebc7ad92":"markdown","f6480c51":"markdown","2b2049fb":"markdown","abff4b2a":"markdown","ef04056a":"markdown","c8144874":"markdown","6e16a34d":"markdown","31099f73":"markdown","b850ac82":"markdown","0d390b70":"markdown","0e948b01":"markdown","edc55caa":"markdown","bb619ff2":"markdown","623d7f9f":"markdown","ccaf064d":"markdown","31a46f97":"markdown","d34ecb2a":"markdown","4ebbed80":"markdown","d537c34d":"markdown","20f1fff8":"markdown","e3e3bca5":"markdown","a49c321b":"markdown","bb9f46ad":"markdown","51e76998":"markdown","b9ad2056":"markdown","7bc79402":"markdown","4b872769":"markdown","0279ebcd":"markdown","6332fbe0":"markdown","b5b353b2":"markdown","74445319":"markdown","53aca42c":"markdown","44513830":"markdown","771e0c15":"markdown","b8a3a656":"markdown","bcd13832":"markdown","8912e5fa":"markdown","17ba9593":"markdown","e35730c1":"markdown","4284de09":"markdown","c9070959":"markdown","3f4630d0":"markdown","ed3e3d27":"markdown","f5c561cd":"markdown","53a80b2e":"markdown","7a36221e":"markdown","4f59b675":"markdown","0a5e6eff":"markdown","ab08ce52":"markdown","5df41359":"markdown","0d17091a":"markdown","e68e6aa4":"markdown","68e53aab":"markdown","f8fe2452":"markdown","0f85939c":"markdown"},"source":{"303eae2f":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","26cd1ff7":"import matplotlib.pyplot as plt\nplt.style.use('seaborn')\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","9032a6fd":"train_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","b4acfdc6":"house_desc = open('\/kaggle\/input\/house-prices-advanced-regression-techniques\/data_description.txt','r')\nfor i in house_desc:\n    print(i)","446b78f4":"train_data.head()","8ec87de1":"test_data.head()","7a56d7be":"train_data.drop(['Id'], axis = 1, inplace = True)\ntest_data.drop(['Id'], axis = 1, inplace = True)","1d51b9f0":"train_data.describe()","5c4f9aba":"print(train_data.shape)\nprint(test_data.shape)","0827657e":"plt.figure(figsize = (16,9))\ncorr_train_data = train_data.corr()\nsns.heatmap(corr_train_data)\nplt.title('Correleation Between Numerical Columns')\nprint(\"Top 20 Numeric Columns which are highly correlated to SalePrice are:\")\nprint(corr_train_data.nlargest(21, 'SalePrice')['SalePrice'])\nplt.show()","8f3cf2bc":"cat_cols = [x for x in train_data.columns if train_data[x].dtype == 'object']\nnum_cols = [x for x in train_data.columns if train_data[x].dtype != 'object']","70c6cebb":"print('Number of Categorical Columns:',len(cat_cols))\nprint('Number of Numerical Columns:',len(num_cols))","94dab489":"num_cols.remove('SalePrice')","43701d0b":"plt.figure(figsize = (16,10))\nfor i in range(36):\n    plt.subplot(6,6,i+1)\n    plt.scatter(train_data[num_cols[i]], train_data['SalePrice'])\n    plt.title(num_cols[i])\nplt.tight_layout()","3172e596":"sns.jointplot(train_data['GrLivArea'], train_data['SalePrice'], kind = 'reg')\nsns.jointplot(train_data['GarageArea'], train_data['SalePrice'], kind = 'reg')","5b64cc7c":"train_data = train_data.drop(train_data[(train_data['GrLivArea']>4000) & (train_data['SalePrice']<250000)].index).reset_index(drop=True)\ntrain_data = train_data.drop(train_data[(train_data['GarageArea']>1150) & (train_data['SalePrice']<200000)].index).reset_index(drop=True)\nsns.jointplot(train_data['GrLivArea'], train_data['SalePrice'], kind = 'reg')\nsns.jointplot(train_data['GarageArea'], train_data['SalePrice'], kind = 'reg')","cf3ae9ee":"sns.jointplot(train_data['TotalBsmtSF'], train_data['SalePrice'], kind = 'reg')\nsns.jointplot(train_data['1stFlrSF'], train_data['SalePrice'], kind = 'reg')","e4e101b9":"train_data = train_data.drop(train_data[(train_data['TotalBsmtSF']>6000) & (train_data['SalePrice']<200000)].index).reset_index(drop=True)\ntrain_data = train_data.drop(train_data[(train_data['1stFlrSF']>4000) & (train_data['SalePrice']<200000)].index).reset_index(drop=True)\nsns.jointplot(train_data['TotalBsmtSF'], train_data['SalePrice'], kind = 'reg')\nsns.jointplot(train_data['1stFlrSF'], train_data['SalePrice'], kind = 'reg')","75db055e":"sns.jointplot(train_data['YearBuilt'], train_data['SalePrice'], kind = 'reg')\nsns.jointplot(train_data['YearRemodAdd'], train_data['SalePrice'], kind = 'reg')\nsns.jointplot(train_data['GarageYrBlt'], train_data['SalePrice'], kind = 'reg')","035ead0c":"sns.jointplot(train_data['MasVnrArea'], train_data['SalePrice'], kind = 'reg')\nsns.jointplot(train_data['BsmtFinSF1'], train_data['SalePrice'], kind = 'reg')","0e5c4267":"sns.jointplot(train_data['LotFrontage'], train_data['SalePrice'], kind = 'reg')\nsns.jointplot(train_data['WoodDeckSF'], train_data['SalePrice'], kind = 'reg')\nsns.jointplot(train_data['2ndFlrSF'], train_data['SalePrice'], kind = 'reg')","6e5071ee":"sns.jointplot(train_data['OpenPorchSF'], train_data['SalePrice'], kind = 'reg')\nsns.jointplot(train_data['LotArea'], train_data['SalePrice'], kind = 'reg')","8e991322":"train_data = train_data.drop(train_data[(train_data['LotFrontage']>300) & (train_data['SalePrice']<400000)].index).reset_index(drop=True)\ntrain_data = train_data.drop(train_data[(train_data['OpenPorchSF']>500) & (train_data['SalePrice']<100000)].index).reset_index(drop=True)\nsns.jointplot(train_data['LotFrontage'], train_data['SalePrice'], kind = 'reg')\nsns.jointplot(train_data['OpenPorchSF'], train_data['SalePrice'], kind = 'reg')","68f29af5":"plt.figure(figsize = (9,6))\nsns.distplot(train_data['SalePrice'])\nplt.title('Dsitribution of SalesPrice')\nplt.show()","a3527fd6":"train_data['SalePrice'] = np.log(train_data['SalePrice'])","cab7e6a9":"plt.figure(figsize = (9,6))\nsns.distplot(train_data['SalePrice'])\nplt.title('Dsitribution of SalesPrice')\nplt.show()","4448acc0":"target = train_data['SalePrice']\ntrain_data.drop(['SalePrice'], axis = 1, inplace = True)","c0688c08":"full_data = pd.concat([train_data, test_data], axis = 0)\nprint('Shape of full_data will be:',full_data.shape)","a463309e":"miss_val = full_data.isnull().sum()\nmiss_full_data = miss_val[miss_val > 0].sort_values(ascending = False)\nprint('Columns with missing values:',len(miss_full_data))\nprint(miss_full_data)","d5def076":"plt.figure(figsize = (16,9))\nsns.barplot(y = miss_full_data.index, x = miss_full_data)\nplt.title('Number of missing values in columns of Full Dataset')\nplt.show()","6e4a54f0":"miss_cat = full_data[cat_cols].isnull().sum()\nmiss_full_cat = miss_cat[miss_cat > 0].sort_values(ascending = False)\nprint('Categorical Columns with missing values:',len(miss_full_cat))\nprint(miss_full_cat)","ef8f8170":"plt.figure(figsize = (16,7))\nsns.barplot(y = miss_full_cat.index, x = miss_full_cat)\nplt.title('Number of missing values in Categorical columns of Full Dataset')\nplt.show()","ba157de8":"miss_num = full_data[num_cols].isnull().sum()\nmiss_full_num = miss_num[miss_num > 0].sort_values(ascending = False)\nprint('Numerical Columns with missing values:',len(miss_full_num))\nprint(miss_full_num)","1e3cb389":"plt.figure(figsize = (16,6))\nsns.barplot(y = miss_full_num.index, x = miss_full_num)\nplt.title('Number of missing values in Numerical columns of Full Dataset')\nplt.show()","f22a00ad":"none_cols = ['PoolQC', 'MiscFeature', 'Alley', 'Fence','FireplaceQu', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n            'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'MasVnrType']","f2aea311":"print('Number of Categorical columns in which are going to be filled with \"None\":',len(none_cols))\nfor i in none_cols:\n    print()\n    print(full_data[i].value_counts())","0f589fa7":"for i in none_cols:\n    full_data[i] = full_data[i].fillna('None')","78cbe7e2":"full_data[none_cols].isnull().sum()","b910baa0":"cat_min_none = list(set(miss_full_cat.index) - set(none_cols))","68bb3a41":"print('Categorical columns remaining are:',len(cat_min_none))\nfor i in cat_min_none:\n    print()\n    print(full_data[i].value_counts())","a581e156":"for i in cat_min_none:\n    if i == 'Utilities':\n        full_data.drop([i], axis = 1, inplace = True)\n    else:\n        full_data[i] = full_data[i].fillna(full_data[i].mode()[0])","9d055e8c":"cat_min_none.remove('Utilities')\ncat_cols.remove('Utilities')","4a259bfc":"full_data[cat_min_none].isnull().sum()","1ba4ceaf":"miss_full_num = list(miss_full_num.index)\nprint('Number of Missing Numerical Columns:',len(miss_full_num))\nprint(miss_full_num)","0340f5aa":"full_data[\"LotFrontage\"] = full_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\nfor i in miss_full_num:\n    if i != 'LotFrontage':\n        full_data[i] = full_data[i].fillna(0)","4af432b0":"full_data[miss_full_num].isnull().sum()","04daec1f":"full_data.info()","415fdbc1":"# full_data['Year_BuiltAndRemod'] = full_data['YearBuilt'] + full_data['YearRemodAdd']\n# full_data['Total_SF'] = full_data['TotalBsmtSF'] + full_data['1stFlrSF'] + full_data['2ndFlrSF']\n# full_data['Total_sqr_footage'] = full_data['BsmtFinSF1'] + full_data['BsmtFinSF2'] + full_data['1stFlrSF'] + full_data['2ndFlrSF']\n# full_data['Total_Bath'] = full_data['FullBath'] + (0.5 * full_data['HalfBath']) + full_data['BsmtFullBath'] + (0.5 * full_data['BsmtHalfBath'])\n# full_data['Total_porch_SF'] = full_data['OpenPorchSF'] + full_data['3SsnPorch'] + full_data['EnclosedPorch'] + full_data['ScreenPorch'] + full_data['WoodDeckSF']","ebc84084":"print('Number of Numerical Columns:',len(num_cols))\nprint('Number of Categorical Columns:',len(cat_cols))","fce1a08f":"full_data[num_cols].head()","5f64a64e":"# num_cat = ['MSSubClass', 'OverallQual', 'OverallCond', 'MoSold', 'YrSold']\n# for i in num_cat:\n#     full_data[i] = full_data[i].apply(str)\n#skewness.drop(['MSSubClass', 'OverallQual', 'OverallCond', 'MoSold', 'YrSold'], axis = 0, inplace = True)","45385b79":"# num_only = list(set(num_cols)-set(num_cat))","768d21cd":"from sklearn.preprocessing import MinMaxScaler\nmm_scaler = MinMaxScaler(feature_range = (-1,1))\nfull_data[num_cols] = mm_scaler.fit_transform(full_data[num_cols])","5ca1620e":"full_data[num_cols].head()","61e32656":"# from scipy.stats import skew\n# skewed_feats = full_data[num_cols].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n# print(\"\\nSkew in numerical features: \\n\")\n# skewness = pd.DataFrame({'Skew' :skewed_feats})\n# print(skewness.shape)\n# skewness","adbaf885":"# from scipy.special import boxcox1p\n# skewness = skewness[abs(skewness) > 0.75]\n# print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\n# skewed_features = skewness.index\n# lam = 0.15\n# for feat in skewed_features:\n#     full_data[feat] = boxcox1p(full_data[feat], lam)","e89e588d":"# from sklearn.preprocessing import OrdinalEncoder\n# oe = OrdinalEncoder()\n# full_data[num_cat] = oe.fit_transform(full_data[num_cat])","b92cda14":"full_data[cat_cols].head()","44ea9318":"from sklearn.feature_extraction import FeatureHasher\nfh = FeatureHasher(n_features = 3, input_type = 'string')\ndf_cat_cols = pd.DataFrame()\n\nfor i in cat_cols:\n    df_cat_cols = pd.concat([pd.DataFrame(fh.fit_transform(full_data[i]).toarray()).add_prefix(i+'_'),df_cat_cols], axis = 1)\n\ndf_cat_cols.head()","20363346":"full_data = pd.concat([full_data[num_cols].reset_index(drop = True), df_cat_cols], axis = 1)","513eee93":"train = full_data[:train_data.shape[0]]\ntest = full_data[train_data.shape[0]:]","360770ee":"print('Shape of Training set:',train.shape)\nprint('Shape of Test set:',test.shape)","a1a0ab92":"train.head()","946c8b78":"test = test.reset_index(drop = True)\ntest.head()","b5c4f4b4":"for i in train.columns:\n    if train[i].dtype == 'object':\n        print(i)","c1d7d931":"from sklearn.feature_selection import SelectKBest, mutual_info_regression\nfrom functools import partial\n\nnum_cat_index = list(np.argwhere(train.columns.isin(num_cat)).ravel())\nscore_func = partial(mutual_info_regression, discrete_features = num_cat_index)\nskb = SelectKBest(score_func)\nskb_features = skb.fit_transform(train[num_cols], target)\nskb_cols = list(skb.get_support(indices = True))","125e81e0":"train = pd.concat([train,pd.DataFrame(skb_features, columns = range(0,10)).add_prefix('SKB_')], axis = 1)\ntest = pd.concat([test,pd.DataFrame(test.iloc[:,skb_cols].values, columns = range(0,10)).add_prefix('SKB_')], axis = 1)\ntrain.drop(num_cols, axis = 1, inplace = True)\ntest.drop(num_cols, axis = 1, inplace = True)\n\nprint('Revised Shape of train set',train.shape)\nprint('Revised Shape of test set',test.shape)","d0fd2e80":"train.head()","ec14fca4":"test.head()","69d64447":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size = 0.2)","f0f27046":"from sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import KFold, cross_val_score, RandomizedSearchCV\nfrom sklearn.metrics import make_scorer, mean_squared_error","eb8e4a1b":"def rmsle_func(actual, pred):\n    return np.sqrt(((np.log(pred + 1) - np.log(actual + 1))**2).mean())\n\nrmsle = make_scorer(rmsle_func, greater_is_better = False)","82f17060":"def generate_clf(clf, params, x, y):\n    rs = RandomizedSearchCV(clf, params)\n    rs_obj = rs.fit(x, y)\n    best_rs = rs_obj.best_estimator_\n    print('Best parameters:',rs_obj.best_params_)\n    pred = rs.predict(X_valid)\n    print('Training score:',rs.score(x, y))\n    print('Validation score:',rs.score(X_valid, y_valid))\n    print('Validation RMSLE error:',rmsle_func(pred, y_valid))\n    kf = KFold(n_splits = 5, shuffle = True)\n    return np.sqrt(-1 * cross_val_score(best_rs, x, y, cv = kf, scoring = rmsle)) , rs_obj.best_params_","c5fcf67c":"from sklearn.linear_model import Lasso\n\n# las_clf = Pipeline([('scaler', RobustScaler()),\n#                     ('clf', Lasso())])\n# las_clf = Lasso()\n\n# las_params = {'alpha':[1e-4, 1e-3, 1e-2, 0.1, 0.05]}\n# las_score , las_best_params = generate_clf(las_clf, las_params, train, target)\n# print('Lasso RMSLE on Training set:',las_score.mean())","5f7eb5b7":"las_clf_final = Lasso(alpha = 0.01)\n# las_clf_final = Lasso(**las_best_params)","10e4c346":"from sklearn.linear_model import ElasticNet\n\n# elas_clf = Pipeline([('scaler', RobustScaler()),\n#                     ('clf', ElasticNet())])\n# elas_clf = ElasticNet()\n\n# elas_params = {'alpha':[1e-4, 1e-3, 1e-2, 0.1, 0.05],\n#               'l1_ratio':[0.2, 0.4, 0.5, 0.6, 0.8]}\n# elas_score , elas_best_params = generate_clf(elas_clf, elas_params, train, target)\n# print('ElasticNet RMSLE on Complete Train set:',elas_score.mean())","124fbc59":"elas_clf_final = ElasticNet(alpha = 0.01)\n# elas_clf_final = ElasticNet(**elas_best_params)","a8c1af31":"from sklearn.ensemble import GradientBoostingRegressor\n\n# gbm_clf = GradientBoostingRegressor()\n\n# gbm_params = {'n_estimators':[2000, 2500, 3000, 3500, 4000],\n#              'min_samples_split':[5, 10, 20, 30],\n#              'max_depth':[3, 5, 7, 9],\n#              'max_features':[100, 150, 200],\n#              'learning_rate':[0.02, 0.04, 0.05, 0.1],\n#              'loss': ['huber']}\n\n# gbm_score , gbm_best_params = generate_clf(gbm_clf, gbm_params, train, target)\n# print('GBM RMSLE on Complete Train set:',gbm_score.mean())","3593556b":"gbm_clf_final = GradientBoostingRegressor(loss = 'huber', n_estimators = 3000)\n# gbm_clf_final = GradientBoostingRegressor(**gbm_best_params)","157c1c80":"from xgboost import XGBRegressor\n\n# xgb_clf = XGBRegressor()\n\n# xgb_params = {'n_etimators':[2000, 2500, 3000, 3500, 4000],\n#              'gamma':[0.02, 0.04, 0.05, 1],\n#              'max_depth':[3, 5, 7, 9],\n#              'alpha':[0.02, 0.04, 0.05, 1],\n#              'eta':[0.02, 0.04, 0.05, 0.1]}\n\n# xgb_score , xgb_best_params = generate_clf(xgb_clf, xgb_params, train, target)\n# print('XGB RMSLE on Complete Train set:',xgb_score.mean())","3a366b07":"xgb_clf_final = XGBRegressor(n_estimators = 3000)\n# xgb_clf_final = XGBRegressor(**xgb_best_params)","19f0dbdf":"from lightgbm import LGBMRegressor\n\n# lgbm_clf = LGBMRegressor()\n\n# lgbm_params = {'application':['regression'],\n#               'num_iterations':[500, 700, 1000, 1500],\n#               'max_depth':[3, 5, 7, 9],\n#               'min_data_in_leaf':[4, 5, 6, 7, 8],\n#               'feature_fraction':[0.2, 0.3, 0.4],\n#               'bagging_fraction':[0.6, 0.7, 0.8, 0.9],\n#                'num_leaves':[5, 7, 10],\n#               'learning_rate':[0.01, 0.1, 0.02, 0.05, 0.03]}\n\n# lgbm_score ,lgbm_best_params = generate_clf(lgbm_clf, lgbm_params, X_train, y_train)\n# print('LGBM RMSLE on Complete Train set:',lgbm_score.mean())","977ba729":"lgbm_clf_final = LGBMRegressor(application = 'regression', num_iterations = 1000, max_depth = 7, num_leaves = 70)\n# lgbm_clf_final = LGBMRegressor(**lgbm_best_params)","15ae7e2c":"from sklearn.ensemble import RandomForestRegressor\n\n# rf_clf = RandomForestRegressor()\n\n# rf_params = {'n_estimators':[300, 500, 1000, 1500, 2000],\n#             'max_features':['auto', 'sqrt'],\n#             'max_depth':[3, 5, 7, 9],\n#             'min_samples_leaf':[50, 55, 60]}\n\n# rf_score , rf_best_params = generate_clf(rf_clf, rf_params, train, target)\n# print('RF RMSLE on train set:',rf_score.mean())","9f62a06d":"rf_clf_final = RandomForestRegressor(n_estimators = 1500, min_samples_leaf = 50, max_depth = 5)\n# rf_clf_final = RandomForestRegressor(**rf_best_params)","2b9f0354":"from sklearn.ensemble import StackingRegressor\n\nmy_estimators = [#('lgbm', lgbm_clf_final),\n                ('xgb', xgb_clf_final)]\n\nstreg_clf = StackingRegressor(estimators = my_estimators, final_estimator = las_clf_final)\nstreg_clf.fit(train, target)\nstreg_clf_pred = np.exp(streg_clf.predict(test))\nprint(streg_clf.score(X_train, y_train))\nprint(streg_clf.score(X_valid, y_valid))\nprint(streg_clf.score(train, target))","f96595b6":"# from sklearn.feature_selection import RFE\n\n# rfe = RFE(estimator = rf_clf_final, n_features_to_select = 30)\n# rfe.fit(train, target)\n# rfe_final_cols = train.columns[rfe.support_]\n# streg_clf_rfe = StackingRegressor(estimators = my_estimators, final_estimator = las_clf_final)\n# streg_clf_rfe.fit(train[rfe_final_cols], target)\n# streg_clf_pred_rfe = np.exp(streg_clf_rfe.predict(test[rfe_final_cols]))\n# print(streg_clf_rfe.score(X_train[rfe_final_cols], y_train))\n# print(streg_clf_rfe.score(X_valid[rfe_final_cols], y_valid))\n# print(streg_clf_rfe.score(train[rfe_final_cols], target))","98b61e2d":"# from boruta import BorutaPy\n\n# bor = BorutaPy(rf_clf_final, n_estimators = 'auto')\n# bor.fit(train.values, target.values.ravel())\n# bor_final_cols = train.columns[bor.support_]\n# streg_clf_bor = StackingRegressor(estimators = my_estimators, final_estimator = las_clf_final)\n# streg_clf_bor.fit(train[bor_final_cols], target)\n# streg_clf_pred_bor = np.exp(streg_clf_bor.predict(test[bor_final_cols]))\n# print(streg_clf_bor.score(X_train[bor_final_cols], y_train))\n# print(streg_clf_bor.score(X_valid[bor_final_cols], y_valid))\n# print(streg_clf_bor.score(train[bor_final_cols], target))","7271e72a":"submission_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission_data['SalePrice'] = streg_clf_pred\nsubmission_data.to_csv('output.csv', index = False)","f5bf6746":"submission_data.info()","08286cd0":"submission_data.head()","630a2635":"I will use 'boxcox1p' to transform all the numerical columns. I am using it because it internally checks the type of transformation function which will be good for a particular column to make it normal.","db47c15b":"## Imputation Work","eec87ccd":"Now, I am splitting the training data into training and validation set.","f6356f98":"I am taking out pure num columns.","d56580cd":"After the following cell, I will have columns seperated as Numerical and Categorical Columns. ","11fd2fa4":"Before that I will take away the target variable 'SalePrice'","a7b106a1":"Well, both train and test sets both contains so much columns that it is very difficult to read them at once. So, I will seperate them into Numerical and Categorical columns.","0b803090":"After dealing with Numerical data lets see Categorical data now.","404fb5a8":"This seems quite interesting, maximum of numerical columns does not seems to be normally distributed. Lets try to make them normal with the help of box cox transformation.","001e62bb":"Apart from 'LotFrontage', I am filling all other columns with 0. 'LotFrontage' depends on the street area so I will take help from the 'Neighbourhood' column to fill it up.","af2740b1":"num_cols contain 'SalePrice' as well but that is our target so I will remove it for now for this list.","804463c6":"In the following cell, I am building a function which will do all the hardwork of getting the best parameters for our models. For this I am using RandomizedSearchCV becuase it is faster than GridSearchCV.","7ed8e433":"I will make use of FeaturHasher. I can use oneHot encoding too but that will increase the shape of the data and make features sparse.","b44a5372":"Now I have combined both the dataset, lets check for null or missing values in the dataset and try to infer from these null values and then I will plan for imputation strategy for each column which has null values.","8a9b9825":"Heatmap helps a lot in knowing the correlation between columns of a dataset.","ebc7ad92":"So, I have the complete data without any missing value. Now I will do a little bit of feature engineering in the data.\nAs discussed above after the correlation map, let us take some features out and merge some of them.","f6480c51":"So from above cell I have converted the categorical column into 3 columns and that 3 columns will be used to signify any value uniquely.\nNext, I will concat this dataframe into with dataframe of num_cols.","2b2049fb":"So, upto this point all the categorical columns are free of null values.","abff4b2a":"## Submission","ef04056a":"It seems to be unhealthy but I have a solution for this and a very easy one. I can take its log and I think it will be good to go(but why log? because it seems to be right skewed).","c8144874":"## Importing Libraries and Input Files","6e16a34d":"## Modelling","31099f73":"For the imputation of missing values in a dataframe, the knowledge of dataset is must. eg- 'PoolQC' contains the highest number of missing values that is because most of the houses do not have pools in their houses, so I will replace their null values with 'None' or null values in garage and basement columns also make sense as they are not present in every house.","b850ac82":"I will take out some columns which are numerical in nature but are actually categorical. These includes 'MSSubClass', 'OverallQual', 'OverallCond', 'MoSold', 'YrSold'.","0d390b70":"Now I will check that how many Categorical columns have missing values.","0e948b01":"Now, as I seperated out categorical and numerical columns(very annoying to type those big names so I will call them as cat and num respectively), lets see the plot of num columns with the target column that is 'SalePrice'.","edc55caa":"Lets Scale the numerical columns with MinMaxScaler.","bb619ff2":"cleaning 'Utilities' from other lists as well because I will use these lists in future.","623d7f9f":"So our dataset is cleaned now as all the missing values are now filled with appropriate entries","ccaf064d":"Lets verify this by checking all the columns of a complete dataset.","31a46f97":"I have defined all my final models upto this point.\nIn the next cell, I will be making a StackingRegressor to stack these models.","d34ecb2a":"There are many columns which have outliers so I need to take care of them. Lets check the columns which have highest correlation with 'SalePrice'.","4ebbed80":"### Models\n* Lasso\n* Elastic\n* GBMRegressor  (time consuming)\n* LGBMRegressor\n* XGBRegressor\n* RFRegressor","d537c34d":"Lets start with the cat columns as they have the most number of missing columns. In this list, most of the columns are self explanatory and in the description of those columns as well it is written that they are null because they are not present in the house so I will fill null values of those columns as 'None'.","20f1fff8":"Now lets check what these remaining columns have got.","e3e3bca5":"All columns other than 'Utilities', I will fill with most_frequent values. I don't think that 'Utilities' will make much impact on 'SalePrice' because of the reason that there is only 1 'NoSeWa' value in that column that too in train set and rest are 'AllPub', apart from these there are only 2 null values so if I fill those with most_frequent then the whole column except one value will be 'AllPub'. So, I think it will be better dropping it from the dataset.","a49c321b":"These above columns do not seem to have any outliers. Lets try some other columns as well.","bb9f46ad":"Looking at the shapes of both the datasets","51e76998":"Looks like 1 outlier in LotFrontage and all others are looking good.","b9ad2056":"Filling these above columns with 'None' according to the description.","7bc79402":"These are the number of missing values in the dataset and their plot in the following cell.","4b872769":"I have removed all the null values from num_cols.","0279ebcd":"I dont think there is need to clean those columns as well.","6332fbe0":"So, I have got my Training and Testing sets back. Let me take a look on them now.","b5b353b2":"Defining function for error calculations","74445319":"So till now, I think  I have removed all the outliers from the Dataset. You can try for more if you think there are columns left.\nNote: I have clean outliers from top 20 numerical columns from which the outliers could be remove.","53aca42c":"OpenPorch also seems to have 1 outlier. Lets clean 1 outlier from both LotFrontage and OpenPorchSF.","44513830":"So now let me check the plot of our target whether it is good or need to be medicated.","771e0c15":"So, I have removed 5 outliers from both the above columns. Lets try out the same for other columns which are in the list of top 20 corrrelated columns.","b8a3a656":"Numerical but categorical columns are: MSSubClass, OverallQual, OverallCond, MoSold, YrSold","bcd13832":"Now lets take care of Numerical columns.","8912e5fa":"## More Libraries","17ba9593":"Same check for Numerical columns.","e35730c1":"No Output, that means all columns are in good condition to move into the model.","4284de09":"For the columns in num_cat, I will use OrdinalEncoder. You can use LabelEncoder as well but it is designed to encode one column at a time. OrdinalEncoder can encode mutiple columns at a time.","c9070959":"Lets start by looking at numerical data in our dataset.","3f4630d0":"A quick view on training data","ed3e3d27":"After getting the best parameters for the model, I will pass these paramters to the final model.","f5c561cd":"I have created the Hard-working function, now I will define my models and call the above function with each of my models to get the best parameters.","53a80b2e":"## Encoding and Transformation","7a36221e":"Now, as the preprocessing has been done, I will take back the training and testing data for modelling.","4f59b675":"\nIt is time to check for the null values in the dataset. For this I will combine both training and test set and then I will manipulate null or missing values in the complete dataset.","0a5e6eff":"2 outliers from above columns. Lets look at more columns.","ab08ce52":"The stuff I have done in last 2 cells, I will do the same for all the other models as well in now following cells.","5df41359":"So, I have treated 15 out of 23 categorical columns which has null values(with just 'None' lol). Now for the remaining ones I will read their desciption and try to infer what could be the possible missing value for that column (this can be done with the help of domain knowledge as well).","0d17091a":"## Exploration Analysis","e68e6aa4":"Lets have a look on the data.","68e53aab":"1 outlier from each above columns.","f8fe2452":"But before that, I will drop 'Id' column from both training and test set.","0f85939c":"It looks pretty goood now."}}