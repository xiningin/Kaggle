{"cell_type":{"fabf5904":"code","43f27895":"code","b1631d17":"code","09af0c34":"code","27934e89":"code","f7b2d14e":"code","9cfabe9f":"markdown","c19e6669":"markdown","00ac36ed":"markdown","5956b8ec":"markdown"},"source":{"fabf5904":"import json\nimport math\nimport os\nfrom tqdm import tqdm\n\nimport cv2\nfrom PIL import Image\n\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport matplotlib.pyplot as plt\n\nfrom keras import layers\nfrom keras.applications import DenseNet121\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model\nfrom keras.optimizers import Adam\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\n\nprint(os.listdir('..\/input'))\n\nim_size = 320","43f27895":"test_df = pd.read_csv('..\/input\/aptos2019-blindness-detection\/test.csv')\n\nprint(test_df.shape)","b1631d17":"# Crop function: https:\/\/www.kaggle.com\/ratthachat\/aptos-updated-preprocessing-ben-s-cropping\ndef crop_image1(img,tol=7):\n    # img is image data\n    # tol  is tolerance\n        \n    mask = img>tol\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef crop_image_from_gray(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n            img = np.stack([img1,img2,img3],axis=-1)\n        return img\n\ndef preprocess_image(image_path, desired_size=224):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = crop_image_from_gray(img)\n    img = cv2.resize(img, (desired_size,desired_size))\n    img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), desired_size\/30) ,-4 ,128)\n    \n    return img\n\nN = test_df.shape[0]\nx_test = np.empty((N, im_size, im_size, 3), dtype=np.uint8)\n\ntry:\n    for i, image_id in enumerate(test_df['id_code']):\n        x_test[i, :, :, :] = preprocess_image(\n            f'..\/input\/aptos2019-blindness-detection\/test_images\/{image_id}.png',\n            desired_size=im_size\n        )\n    print('Test dataset correctly processed')\nexcept:\n    print('Test dataset NOT processed')","09af0c34":"densenet = DenseNet121(\n    weights='..\/input\/densenet-keras\/DenseNet-BC-121-32-no-top.h5',\n    include_top=False,\n    input_shape=(im_size,im_size,3)\n    )\n\n\ndef build_model():\n    model = Sequential()\n    model.add(densenet)\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(5, activation='sigmoid'))\n    return model\n\nmodel = build_model()\n\nmodel.load_weights('..\/input\/aptos2019traineddenset\/model.h5')\nmodel.summary()","27934e89":"y_test = model.predict(x_test) > 0.5\ny_test = y_test.astype(int).sum(axis=1) - 1\n\ntest_df['diagnosis'] = y_test\ntest_df.to_csv('submission.csv',index=False)\nprint(test_df.head())","f7b2d14e":"dist = (test_df.diagnosis.value_counts()\/len(test_df))*100\nprint('Prediction distribution:')\nprint(dist)\ntest_df.diagnosis.hist()\nplt.show()","9cfabe9f":"# DenseNet Inference (Old & New Data)\n\n---\n\nThis is an inference Kernel. To see how I got the model, please go to the [Training Kernel here](https:\/\/www.kaggle.com\/raimonds1993\/aptos19-densenet-trained-with-old-and-new-data). \n\nIf this helped you, <span style=\"color:red\"> please upvote <\/span>.\n\nAs a recap, I trained a Denset on both old (2015) and new (2019) competition dataset. Considering the size of the overall dataset, I decided to split it in buckets and train the model iteratively.\n\nUPDATE:\nAs you see the LB score is decreasing version by version. It was discussed in many topics and kernels that we should not trust this public test set. You can notice from the training kernel that Holdout Kappa score is actually increasing.\n\nSo, here some questions I ask myself:\n\n- **Should we trust our validation score?**\n- **Is 10% Holdout enough?**\n- **Should we implement KFold CV?**\n\nI'd love to hear what you think :)\n","c19e6669":"# Processing Test Images\n\n---\n\nNotice that here we can only see the public dataset. Once we submit our results, the kernel will rerun on the overall test set (public + private) that is around 20 GB. That needs to be considered otherwise our kernel may not work.","00ac36ed":"# Submission\n\n---\n","5956b8ec":"# Model - DenseNet 121\n\n--- \n\nHere we need to replicate the architecture we used during the training phase. The difference is that we are going to load the weights we have previously found."}}