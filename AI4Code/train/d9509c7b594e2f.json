{"cell_type":{"fc6fed58":"code","f101c3dc":"code","875653a7":"code","1858e0ab":"code","ba526698":"code","f0d07d4f":"code","c1c6abad":"code","09518ed0":"code","f941eab0":"code","59a311a1":"code","668044c7":"code","e314810a":"code","63f201e4":"code","99f84b88":"code","22560033":"code","25f91369":"code","97fd51c7":"code","81ecdedd":"code","b7183e70":"code","50b682d4":"code","d3de39ae":"code","2630ee70":"code","7948b451":"code","b873ab38":"code","41dde87a":"code","c1bc35d7":"code","45420c4f":"code","1e14a5bb":"code","a31fe865":"code","003ca07d":"code","d5fb5976":"code","3fbb5399":"markdown","44a66438":"markdown","36afbafd":"markdown","08f1e7d3":"markdown","1fad30c7":"markdown","8ac8825a":"markdown","319e5ea9":"markdown","f63eccf6":"markdown","e1d8db05":"markdown","f60e0d1e":"markdown","a698255a":"markdown","33998517":"markdown","bfeef4f7":"markdown","1bf196f7":"markdown","771dc01a":"markdown","7418f85e":"markdown","37e9b041":"markdown","a8a16ca1":"markdown","d7397d05":"markdown"},"source":{"fc6fed58":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Conv1D, Flatten, Dropout, Activation, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow.keras.backend as K\n\nfrom shutil import copyfile\n#copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"..\/input\/bert-tokenization\/bert_tokenization.py\", dst = \"..\/working\/bert_tokenization.py\")\n\nimport bert_tokenization\n#from transformers import BertTokenizer","f101c3dc":"df_train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\ndf_train.isnull().sum()","875653a7":"df_train.dropna(how='any',axis=0,inplace=True)\ndf_train.head()","1858e0ab":"df_test.isnull().sum()","ba526698":"df_test.head()","f0d07d4f":"colors = sns.color_palette()\nplt.subplot(211)\nsentiment_num_1 = df_train['sentiment'].value_counts()\nsentiment_num_1.plot(kind='bar',figsize=(10,10),color=colors[0],rot=0)\nplt.title('Sentiment Distribution for Train Data')\n\nplt.subplot(212)\nsentiment_num_2 = df_test['sentiment'].value_counts()\nsentiment_num_2.plot(kind='bar',figsize=(10,10),color=colors[1],rot=0)\nplt.title('Sentiment Distribution for Test Data')\n\nplt.tight_layout(pad =3)\nplt.show()","c1c6abad":"df_train['word_cnt_full_texts'] = df_train['text'].apply(lambda x: len(x.split()))\ndf_train['word_cnt_sel_texts'] = df_train['selected_text'].apply(lambda x: len(x.split()))\nfig,axes = plt.subplots(nrows=3,ncols=1,figsize=(8,20))\nfor i,s in enumerate(['positive','negative','neutral']):\n    sns.distplot(df_train[df_train.sentiment==s]['word_cnt_full_texts'],\n                 bins=20, color='skyblue', label='full texts', ax=axes[i])\n    sns.distplot(df_train[df_train.sentiment==s]['word_cnt_sel_texts'],\n                 bins=20, color='red', label='sel texts', ax=axes[i])\n    axes[i].legend(fontsize=14)\n    axes[i].set_title('%s: full texts length vs selected texts length'%(s),fontsize=15,fontweight='bold')","09518ed0":"bert_layer = hub.KerasLayer('..\/input\/berthub', trainable=True)\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)","f941eab0":"print(df_train['text'][1]) \nprint(tokenizer.tokenize(df_train['text'][1]))\nprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df_train['text'][1])))","59a311a1":"print(df_train['selected_text'][1])\nprint(tokenizer.tokenize(df_train['selected_text'][1]))\nprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df_train['selected_text'][1])))","668044c7":"print(tokenizer.convert_tokens_to_ids(['[CLS]']))\nprint(tokenizer.convert_tokens_to_ids(['[SEP]']))","e314810a":"train_texts = df_train['text']\ntrain_sel_texts = df_train['selected_text']\ntrain_sentiment = df_train['sentiment']\ntexts = list(train_texts)\nsentiments = list(train_sentiment)\nsel_texts = list(train_sel_texts)\nlen(sel_texts),len(texts),len(sentiments)","63f201e4":"sentiments[1],texts[1],sel_texts[1]","99f84b88":"start_idx = texts[1].find(sel_texts[1])\nend_idx = start_idx + len(sel_texts[1])-1                \nsentiment = sentiments[1]\nfull_text_1 = tokenizer.tokenize(texts[1][:start_idx])\nfull_text_2 = tokenizer.tokenize(texts[1][start_idx:end_idx+1])\nfull_text_3 = tokenizer.tokenize(texts[1][end_idx+1:])\nsentiment,full_text_1,full_text_2,full_text_3","22560033":"max_len=150","25f91369":"input_tokens = ['[CLS]'] + [sentiment] + ['[SEP]'] + full_text_1+ full_text_2 + full_text_3 +['[SEP]']\npad_len = max_len - len(input_tokens)\nvalid_input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\ninput_ids = valid_input_ids + [0]*pad_len\nattention_masks = [1]*len(valid_input_ids) + [0]*pad_len\ntype_ids = [0]*3 + [1]*(len(valid_input_ids)-3)+[0]*pad_len\nprint(input_tokens)\nprint(input_ids)\nprint(attention_masks)\nprint(type_ids)","97fd51c7":"start_tokens = [0]*(len(full_text_1)+3)+[1]+[0]*(max_len-len(full_text_1)-4)\nend_tokens = [0]*(len(full_text_1)+len(full_text_2)+2)+[1]+[0]*(max_len-len(full_text_1)-len(full_text_2)-3)\nprint(start_tokens)\nprint(end_tokens)","81ecdedd":"len(input_ids),len(attention_masks),len(type_ids),len(start_tokens),len(end_tokens)","b7183e70":"def bert_encode_train(sentiments, texts, sel_texts, tokenizer, max_len =512):\n    all_input_ids = []\n    all_masks = []\n    all_type_ids = []\n    all_start_tokens = []\n    all_end_tokens = []\n    \n    \n    for i in range(len(texts)):\n        \n        start_idx = texts[i].find(sel_texts[i])\n        end_idx = start_idx + len(sel_texts[i])-1                \n        sentiment = sentiments[i]\n        full_text_1 = tokenizer.tokenize(texts[i][:start_idx])\n        full_text_2 = tokenizer.tokenize(texts[i][start_idx:end_idx+1])\n        full_text_3 = tokenizer.tokenize(texts[i][end_idx+1:])\n        \n        input_tokens = ['[CLS]'] + [sentiment] + ['[SEP]'] + full_text_1+ full_text_2 + full_text_3 +['[SEP]']\n        pad_len = max_len - len(input_tokens)\n        valid_input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n        input_ids = valid_input_ids + [0]*pad_len\n        attention_masks = [1]*len(valid_input_ids) + [0]*pad_len\n        type_ids = [0]*3 + [1]*(len(valid_input_ids)-3) + [0]*pad_len\n        #type_ids = [0]*len(input_ids)\n        \n        start_tokens = [0]*(len(full_text_1)+3)+[1]+[0]*(max_len-len(full_text_1)-4)\n        end_tokens = [0]*(len(full_text_1)+len(full_text_2)+2)+[1]+[0]*(max_len-len(full_text_1)-len(full_text_2)-3)\n        \n        all_input_ids.append(input_ids)\n        all_masks.append(attention_masks)\n        all_type_ids.append(type_ids)\n        all_start_tokens.append(start_tokens) \n        all_end_tokens.append(end_tokens)\n        \n    return np.array(all_input_ids), np.array(all_masks), np.array(all_type_ids),np.array(all_start_tokens),np.array(all_end_tokens)","50b682d4":"max_len = 0\nfor i in range(df_train.shape[0]+1):\n    try:\n        tokens = tokenizer.tokenize(df_train['text'][i])\n        input_ids = tokenizer.convert_tokens_to_ids(['[CLS]']+list(df_train.loc[i,'sentiment'])+\n                                       ['[SEP]']+tokens+['[SEP]'])\n        max_len = max(max_len, len(input_ids))\n    except:\n        pass\n\nprint('Max length for training data: ', max_len)","d3de39ae":"max_len = 0\nfor i in range(df_train.shape[0]+1):\n    try:\n        tokens = tokenizer.tokenize(df_test['text'][i])\n        input_ids = tokenizer.convert_tokens_to_ids(['[CLS]']+list(df_test.loc[i,'sentiment'])+\n                                       ['[SEP]']+tokens+['[SEP]'])\n        max_len = max(max_len, len(input_ids))\n    except:\n        pass\n\nprint('Max length for test data: ', max_len)","2630ee70":"train_texts = df_train[df_train['sentiment']!='neutral']['text']\ntrain_sel_texts = df_train[df_train['sentiment']!='neutral']['selected_text']\ntrain_sentiment = df_train[df_train['sentiment']!='neutral']['sentiment']\n#train_texts = df_train['text']\n#train_sel_texts = df_train['selected_text']\n#train_sentiment = df_train['sentiment']\nfull_texts = list(train_texts)\nsentiments = list(train_sentiment)\nsel_texts = list(train_sel_texts)\n\ntrain_input = bert_encode_train(sentiments,full_texts,sel_texts,tokenizer, max_len =150)[:3]\ntrain_labels = bert_encode_train(sentiments,full_texts,sel_texts,tokenizer, max_len =150)[3:]","7948b451":"def bert_encode_test(sentiments, texts, tokenizer, max_len =512):\n    all_input_ids = []\n    all_masks = []\n    all_type_ids = []\n\n    for i in range(len(texts)):\n        text = tokenizer.tokenize(texts[i])               \n        sentiment = sentiments[i]\n        input_tokens = ['[CLS]'] + [sentiment] + ['[SEP]'] + text +['[SEP]']\n        pad_len = max_len - len(input_tokens)\n        valid_input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n        input_ids = valid_input_ids + [0]*pad_len\n        attention_masks = [1]*len(valid_input_ids) + [0]*pad_len\n        type_ids = [0]*3 + [1]*(len(valid_input_ids)-3) + [0]*pad_len\n        #type_ids = [0]*len(input_ids)\n        \n        all_input_ids.append(input_ids)\n        all_masks.append(attention_masks)\n        all_type_ids.append(type_ids)\n      \n    return np.array(all_input_ids), np.array(all_masks), np.array(all_type_ids)","b873ab38":"test_texts = df_test[df_test['sentiment']!='neutral']['text']\ntest_sentiment = df_test[df_test['sentiment']!='neutral']['sentiment']\n#test_texts = df_test['text']\n#test_sentiment = df_test['sentiment']\nfull_texts_test = list(test_texts)\nsentiments_test = list(test_sentiment)\n\ntest_input = bert_encode_test(sentiments_test, full_texts_test, tokenizer, max_len =150)","41dde87a":"# K.clear_session()","c1bc35d7":"def build_bert(bert_layer, max_len =512):\n    adam = Adam(lr=3e-5)\n    main_input = Input(shape =(max_len,), dtype =tf.int32)\n    input_word_ids = Input(shape = (max_len,),dtype =tf.int32)\n    input_mask = Input(shape = (max_len,),dtype =tf.int32)\n    input_type_ids = Input(shape = (max_len,),dtype =tf.int32)\n    \n    clf_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n    #pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n    #clf_output = sequence_output[1]\n    \n    out1 = Dropout(0.1)(clf_output[1])\n    out1 = Conv1D(filters=1, kernel_size=1) (out1)\n    out1 = Flatten()(out1)\n    out1 = Activation('softmax')(out1)\n    \n    out2 = Dropout(0.1)(clf_output[1])\n    out2 = Conv1D(filters=1, kernel_size=1) (out2)\n    out2 = Flatten()(out2)\n    out2 = Activation('softmax')(out2)\n    \n    model = Model(inputs = [input_word_ids, input_mask, input_type_ids], outputs =[out1,out2])\n    model.compile(optimizer=Adam(lr=3e-5) ,loss = 'categorical_crossentropy')\n    print(model.summary())\n    return model\n\n\nmodel = build_bert(bert_layer,max_len=150)\nfilepath='best_weight.hdf5'\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nmodel.fit(train_input, train_labels, epochs =3, batch_size = 16, callbacks=[checkpoint], validation_split=0.2)\n#for layer in model.layers:\n    #print(layer.output_shape)","45420c4f":"def jaccard_similarity(str1,str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    if not a and not b:\n        return 0.5 \n    c = a.intersection(b)\n    return float(len(c)\/(len(a)+len(b)-len(c)))","1e14a5bb":"jaccard_similarity(' Sooo SAD I will miss you here in San Diego!!!','Sooo SAD')","a31fe865":"best_model = load_model('.\/best_weight.hdf5',custom_objects={'KerasLayer':bert_layer})\npred_start,pred_end = model.predict(test_input)\nresults = []\nfor k in range(test_input[0].shape[0]):\n    a = np.argmax(pred_start[k])\n    b = np.argmax(pred_end[k])\n    \n    if a>b:\n        sel_text = full_texts_test[k]\n    else:\n        sel_text = ' '.join(tokenizer.convert_ids_to_tokens(test_input[0][k,a:b+1]))\n        \n    results.append(sel_text)\n\n#google fulltokenizer will generate meaingless punction ##   \nresults = [x.replace(' ##','') for x in results]\n    \nfor k in range(df_test.shape[0]):\n    if df_test.loc[k, 'sentiment'] == 'neutral':\n        df_test.loc[k, 'selected_text'] = df_test.loc[k, 'text']\n\ndf_test.loc[df_test['sentiment']!='neutral','selected_text'] = results\n#df_test['selected_text'] = results\noutput = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')\noutput['selected_text'] = df_test['selected_text']","003ca07d":"output.to_csv('submission.csv',index=False,header=True)","d5fb5976":"output['text'] = df_test['text']\noutput['sentiment'] = df_test['sentiment']\noutput.head(10)","3fbb5399":"# Bert","44a66438":"## Build Bert Model with CNN head","36afbafd":"Simple visualization for data distribution (The sentiment distributions for train and test data are almost same)","08f1e7d3":"First I take a look at how the tokenizer works","1fad30c7":"Step 1. Decompose full texts into three parts: texts before selected texts, selected texts and texts after selected texts","8ac8825a":"Step 2. tokenize three input arrays: input_ids,attention_masks,type_ids","319e5ea9":"It looks like for neutral tweets, slected texts and full texts are almost the same. While for positive and negative tweets, selected texts are only a small part of full texts.","f63eccf6":"## Wrap the step-by-step encoding methods","e1d8db05":"Since the word count of full texts and selected texts for neutral text is almost the same, I only use positive and negative texts as training and test data. ","f60e0d1e":"Import libraries that we need","a698255a":"Check max length after bert encoding","33998517":"# Data Overview","bfeef4f7":"Then I segment encoding part step by step","1bf196f7":"Check if all the arrays have the same length","771dc01a":"When it comes to the submission, selected texts of neutral texts are assigned with full texts.","7418f85e":"## Encode train and test data","37e9b041":"I use the second sentence as an example","a8a16ca1":"Step 3. tokenize two output arrays: start and end","d7397d05":"# Predict and Submission"}}