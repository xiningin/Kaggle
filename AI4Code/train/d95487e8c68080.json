{"cell_type":{"3e0e41d9":"code","911e64b3":"code","8ab08f8a":"code","2a47a85c":"code","27336dad":"code","5e07512f":"code","6b9688bf":"code","f460d8e9":"code","e3e73e8a":"code","a5fa9002":"code","6c91251e":"code","b10c4bc1":"code","184fd1c3":"code","b8aa7bcf":"code","9393631f":"code","776b34ef":"code","a5ff6120":"code","91050ef9":"code","74597f34":"code","81a7aa8c":"code","b72a7bbc":"code","63867f2d":"code","b87a720e":"code","4257d359":"markdown","ef6eb55e":"markdown","bb196852":"markdown","fea522a9":"markdown","50b73383":"markdown","37d89073":"markdown","8a537e61":"markdown"},"source":{"3e0e41d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","911e64b3":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 14})\n\nimport re\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndistricts_info = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv\")\nproducts_info = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv\")","8ab08f8a":"districts_info.head()","2a47a85c":"products_info.head()","27336dad":"districts_info = districts_info[districts_info.state.notna()].reset_index(drop=True)","5e07512f":"temp_sectors = products_info['Sector(s)'].str.get_dummies(sep=\"; \")\ntemp_sectors.columns = [f\"sector_{re.sub(' ', '', c)}\" for c in temp_sectors.columns]\nproducts_info = products_info.join(temp_sectors)\nproducts_info.drop(\"Sector(s)\", axis=1, inplace=True)\n\ndel temp_sectors","6b9688bf":"products_info['primary_function_main'] = products_info['Primary Essential Function'].apply(lambda x: x.split(' - ')[0] if x == x else x)\nproducts_info['primary_function_sub'] = products_info['Primary Essential Function'].apply(lambda x: x.split(' - ')[1] if x == x else x)\n\n# Synchronize similar values\nproducts_info['primary_function_sub'] = products_info['primary_function_sub'].replace({'Sites, Resources & References' : 'Sites, Resources & Reference'})\nproducts_info.drop(\"Primary Essential Function\", axis=1, inplace=True)","f460d8e9":"PATH = '..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data' \n\ntemp = []\n\nfor district in districts_info.district_id.unique():\n    df = pd.read_csv(f'{PATH}\/{district}.csv', index_col=None, header=0)\n    df[\"district_id\"] = district\n    temp.append(df)\n    \n    \nengagement = pd.concat(temp)\nengagement = engagement.reset_index(drop=True)","e3e73e8a":"fig, ax = plt.subplots(1, 1, figsize=(8,4))\n\nsns.histplot(engagement.groupby('district_id').time.nunique(), bins=30)\nax.set_title('Unique Days of Engagement Data per District')\nplt.show()","a5fa9002":"# Delete previously created engagement dataframe and create a new one\ndel engagement\n\ntemp = []\n\nfor district in districts_info.district_id.unique():\n    df = pd.read_csv(f'{PATH}\/{district}.csv', index_col=None, header=0)\n    df[\"district_id\"] = district\n    if df.time.nunique() == 366:\n        temp.append(df)\n\nengagement = pd.concat(temp)\nengagement = engagement.reset_index(drop=True)\n\n# Only consider districts with full 2020 engagement data\ndistricts_info = districts_info[districts_info.district_id.isin(engagement.district_id.unique())].reset_index(drop=True)\nproducts_info = products_info[products_info['LP ID'].isin(engagement.lp_id.unique())].reset_index(drop=True)","6c91251e":"for district in districts_info.district_id.unique()[:10]:\n    df = pd.read_csv(f'{PATH}\/{district}.csv', index_col=None, header=0)\n    print(f'District {district} uses {df.lp_id.nunique()} unique products.')\n    \nprint(f'\\nConcatenated engagement data contains {engagement.lp_id.nunique()} unique products.')","b10c4bc1":"print(len(engagement))\nengagement = engagement[engagement.lp_id.isin(products_info['LP ID'].unique())]\nprint(len(engagement))","184fd1c3":"engagement.time = engagement.time.astype('datetime64[ns]')","b8aa7bcf":"\nus_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\ndistricts_info['state_abbrev'] = districts_info['state'].replace(us_state_abbrev)\ndistricts_info_by_state = districts_info['state_abbrev'].value_counts().to_frame().reset_index(drop=False)\ndistricts_info_by_state.columns = ['state_abbrev', 'num_districts']\n\nfig = go.Figure()\nlayout = dict(\n    title_text = \"Number of Available School Districts per State\",\n    geo_scope='usa',\n)\n\nfig.add_trace(\n    go.Choropleth(\n        locations=districts_info_by_state.state_abbrev,\n        zmax=1,\n        z = districts_info_by_state.num_districts,\n        locationmode = 'USA-states', # set of locations match entries in `locations`\n        marker_line_color='white',\n        geo='geo',\n        colorscale=px.colors.sequential.Teal, \n    )\n)\n            \nfig.update_layout(layout)   \nfig.show()","9393631f":"districts_info.pp_total_raw.unique()\ntemp = districts_info.groupby('locale').pp_total_raw.value_counts().to_frame()\ntemp.columns = ['amount']\n\ntemp = temp.reset_index(drop=False)\n\ntemp = temp.pivot(index='locale', columns='pp_total_raw')['amount']\ntemp = temp[['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ]]\n\n\nfig, ax = plt.subplots(1, 2, figsize=(24,4))\n\nsns.countplot(data=districts_info, x='locale', ax=ax[0], palette='GnBu')\n\nsns.heatmap(temp, annot=True,  cmap='GnBu', ax=ax[1])\nax[1].set_title('Heatmap of Districts According To locale and pp_total_raw')\nplt.show()","776b34ef":"fig, ax = plt.subplots(2, 2, figsize=(16,8))\n\nsns.countplot(data=districts_info, x='pct_black\/hispanic', order=['[0, 0.2[', '[0.2, 0.4[', '[0.4, 0.6[', '[0.6, 0.8[','[0.8, 1[', ], palette='GnBu', ax=ax[0,0])\nax[0,0].set_ylim([0,135])\nsns.countplot(data=districts_info, x='pct_free\/reduced', order=['[0, 0.2[', '[0.2, 0.4[', '[0.4, 0.6[', '[0.6, 0.8[','[0.8, 1[', ], palette='GnBu', ax=ax[0,1])\nax[0,1].set_ylim([0,135])\n\nsns.countplot(data=districts_info, x='county_connections_ratio', palette='GnBu', ax=ax[1,0])\nax[1,0].set_ylim([0,135])\nsns.countplot(data=districts_info, x='pp_total_raw', order=['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ], palette='GnBu', ax=ax[1,1])\nax[1,1].set_ylim([0,135])\nax[1,1].set_xticklabels(ax[1,1].get_xticklabels(), rotation=90)\n\nplt.tight_layout()\nplt.show()","a5ff6120":"def replace_ranges_pct(range_str):\n    if range_str == '[0, 0.2[':\n        return 0.1\n    elif range_str == '[0.2, 0.4[':\n        return 0.3\n    elif range_str == '[0.4, 0.6[':\n        return 0.5\n    elif range_str == '[0.6, 0.8[':\n        return 0.7\n    elif range_str == '[0.8, 1[':\n        return 0.9\n    else:\n        return np.nan\n    \ndef replace_ranges_raw(range_str):\n    if range_str == '[4000, 6000[':\n        return 5000\n    elif range_str == '[6000, 8000[':\n        return 7000\n    elif range_str == '[8000, 10000[':\n        return 9000\n    elif range_str == '[10000, 12000[':\n        return 11000\n    elif range_str ==  '[12000, 14000[':\n        return 13000\n    elif range_str ==  '[14000, 16000[':\n        return 15000\n    elif range_str == '[16000, 18000[':\n        return 17000\n    elif range_str ==  '[18000, 20000[':\n        return 19000\n    elif range_str ==  '[20000, 22000[':\n        return 21000\n    elif range_str ==  '[22000, 24000[':\n        return 21000\n    else: \n        return np.nan\n    \ndistricts_info['pct_black_hispanic_num'] = districts_info['pct_black\/hispanic'].apply(lambda x: replace_ranges_pct(x))\ndistricts_info['pct_free_reduced_num'] = districts_info['pct_free\/reduced'].apply(lambda x: replace_ranges_pct(x))\ndistricts_info['pp_total_raw_num'] = districts_info['pp_total_raw'].apply(lambda x: replace_ranges_raw(x))\n\ndef plot_state_mean_for_var(col):\n    temp = districts_info.groupby('state_abbrev')[col].mean().to_frame().reset_index(drop=False)\n\n    fig = go.Figure()\n    layout = dict(\n        title_text = f\"Mean {col} per State\",\n        geo_scope='usa',\n    )\n\n    fig.add_trace(\n        go.Choropleth(\n            locations=temp.state_abbrev,\n            zmax=1,\n            z = temp[col],\n            locationmode = 'USA-states', # set of locations match entries in `locations`\n            marker_line_color='white',\n            geo='geo',\n            colorscale=px.colors.sequential.Teal, \n        )\n    )\n\n    fig.update_layout(layout)   \n    fig.show()\n\nplot_state_mean_for_var('pct_black_hispanic_num')\nplot_state_mean_for_var('pct_free_reduced_num')\nplot_state_mean_for_var('pp_total_raw_num')","91050ef9":"fig, ax = plt.subplots(1, 2, figsize=(16,4))\nsns.countplot(data=products_info, x='primary_function_main', palette ='GnBu', ax=ax[0])\nax[0].set_title('Main Categories in Primary Functions')\n\nsns.countplot(data=products_info[products_info.primary_function_main == 'LC'], x='primary_function_sub', palette ='GnBu', ax=ax[1])\nax[1].set_title('Sub-Categories in Primary Function LC')\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\nplt.show()","74597f34":"virtual_classroom_lp_id = products_info[products_info.primary_function_sub == 'Virtual Classroom']['LP ID'].unique()\n\n# Remove weekends from the dataframe\nengagement['weekday'] = pd.DatetimeIndex(engagement['time']).weekday\nengagement_without_weekends = engagement[engagement.weekday < 5]\n\n# Figure 1\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(24, 6))\nfor virtual_classroom_product in virtual_classroom_lp_id:\n    temp = engagement_without_weekends[engagement_without_weekends.lp_id == virtual_classroom_product].groupby('time').pct_access.mean().to_frame().reset_index(drop=False)\n    sns.lineplot(x=temp.time, y=temp.pct_access, label=products_info[products_info['LP ID'] == virtual_classroom_product]['Product Name'].values[0])\nplt.legend()\nplt.show()\n\n# Figure 2\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(24, 6))\nfor virtual_classroom_product in virtual_classroom_lp_id:\n    temp = engagement_without_weekends[engagement_without_weekends.lp_id == virtual_classroom_product].groupby('time').engagement_index.mean().to_frame().reset_index(drop=False)\n    sns.lineplot(x=temp.time, y=temp.engagement_index, label=products_info[products_info['LP ID'] == virtual_classroom_product]['Product Name'].values[0])\nplt.legend()\nplt.show()","81a7aa8c":"products_info['lp_id'] = products_info['LP ID'].copy()\n\nf, ax = plt.subplots(nrows=3, ncols=3, figsize=(18, 8))\n\ni = 0\nj = 0\nfor subfunction in products_info[products_info.primary_function_main == 'LC'].primary_function_sub.unique():\n    lp_ids = products_info[products_info.primary_function_sub == subfunction]['LP ID'].unique()\n\n    temp = engagement_without_weekends[engagement_without_weekends.lp_id.isin(lp_ids)]\n    temp = temp.groupby('lp_id').pct_access.mean().sort_values(ascending=False).to_frame().reset_index(drop=False)\n    temp = temp.merge(products_info[['lp_id', 'Product Name']], on='lp_id').head()\n    \n    sns.barplot(data=temp, x='pct_access', y='Product Name', palette='GnBu', ax=ax[i, j])\n    \n    ax[i, j].set_title(f'Top 5 in \\n{subfunction}', fontsize=12)\n    ax[i, j].set_xlim([0, 20])\n    j = j + 1\n    if j == 3:\n        i = i + 1\n        j = 0\n        \nf.delaxes(ax[2, 1])\nf.delaxes(ax[2, 2])\n\nplt.tight_layout()\nplt.show()\n","b72a7bbc":"engagement['quarter'] = pd.DatetimeIndex(engagement['time']).quarter.astype(str)\n\ntemp = engagement.merge(products_info[['lp_id', 'Product Name', 'primary_function_main', 'primary_function_sub']], on='lp_id')\ntemp = temp[temp.primary_function_sub.notna()]\ntemp = temp.groupby(['quarter', 'primary_function_sub'])['pct_access', 'engagement_index'].mean().reset_index(drop=False)\n\ntemp = temp.pivot(index='primary_function_sub', columns='quarter')[['pct_access', 'engagement_index']].fillna(0)\n\ntemp.columns = [\"_\".join(a) for a in temp.columns.to_flat_index()]\n\ntemp['pct_access_delta'] = temp['pct_access_4'] - temp['pct_access_1']\ntemp['engagement_index_delta'] = temp['engagement_index_4'] - temp['engagement_index_1']\ntemp=temp.reset_index(drop=False)\n#temp = temp.merge(products_info[['lp_id', 'Product Name', 'primary_function_sub']], on='lp_id')\n\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n\ndf = temp.sort_values(by='pct_access_delta', ascending=False)#.head(5)\n\nsns.barplot(data=df, x='pct_access_delta', y='primary_function_sub', palette='GnBu', ax=ax[0])\n\ndf = temp.sort_values(by='engagement_index_delta', ascending=False)#.head(5)\n\nsns.barplot(data=df, x='engagement_index_delta', y='primary_function_sub', palette='GnBu', ax=ax[1])\nplt.tight_layout()\nplt.show()","63867f2d":"temp = engagement.fillna(0).groupby(['quarter', 'lp_id'])['pct_access', 'engagement_index'].mean().reset_index(drop=False)\n\ntemp = temp.pivot(index='lp_id', columns='quarter')[['pct_access', 'engagement_index']].fillna(0)\n\ntemp.columns = [\"_\".join(a) for a in temp.columns.to_flat_index()]\n\ntemp['pct_access_delta'] = temp['pct_access_4'] - temp['pct_access_1']\ntemp['engagement_index_delta'] = temp['engagement_index_4'] - temp['engagement_index_1']\ntemp = temp.merge(products_info[['lp_id', 'Product Name', 'primary_function_sub']], on='lp_id')\n\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n\ndf = temp.sort_values(by='pct_access_delta', ascending=False).head(10)\n\nsns.barplot(data=df, x='pct_access_delta', y='Product Name', palette='GnBu', ax=ax[0])\n\ndf = temp.sort_values(by='engagement_index_delta', ascending=False).head(10)\n\nsns.barplot(data=df, x='engagement_index_delta', y='Product Name', palette='GnBu', ax=ax[1])\nplt.tight_layout()\nplt.show()\n","b87a720e":"districts_info.to_csv(\"submission.csv\", index=False)","4257d359":"Consider distrcits with engagement data for everyday in 2020.","ef6eb55e":"Splitting up the Primary Essential Function","bb196852":"remove engagement data for unknown products","fea522a9":"Dropping Districts with NaN States\n\n","50b73383":"Initial Exploratory Data Analysis (EDA)\n\nFirst of all, I am interested how diverse the available school districts are. As you can see in below plot, the available data does not cover all the states in the U.S. (19\/50). The states with the most available school districts are CT (29) and UT (24) while there are also states with only one school district (FL, TN, NY, AZ).","37d89073":"convert the time column to the type datetime64[ns] ","8a537e61":"For the majority of school districts (133) there are 366 unique days available. However, for 43 school districts there are less than 366 unique days of data available. "}}