{"cell_type":{"d0f38c3c":"code","24d09f7e":"code","58166612":"code","7974fd10":"code","7b5f2a11":"code","a60c2fe9":"code","634bde8c":"code","5f3eabb0":"code","ffd9c575":"markdown"},"source":{"d0f38c3c":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.callbacks as C","24d09f7e":"ROOT_COMPETITION = '\/kaggle\/input\/predict-volcanic-eruptions-ingv-oe\/'","58166612":"train_original = pd.read_csv(ROOT_COMPETITION+'train.csv')\ntrain = train_original.loc[0:2000] # training on a part only in order to avoid OOM \ndev = train_original.loc[3000:3500]\n\n\n","7974fd10":"class DataLoader():\n    def __init__(self):\n        self.TRAIN_TEST_SPLIT=0.1\n    \n    def get_sample_all(sample_id, where):\n        sample_path = ROOT_COMPETITION+where+'\/'+str(sample_id)+'.csv'\n        df = pd.read_csv(sample_path).fillna(0)\n        np_data = df.values\n        np_data = np_data[:60000,:]\n\n        return np_data\n    \nclass EngineNN():\n    def __init__(self):\n        self.INPUT_SHAPE=10\n        self.OUTPUT_SHAPE=1\n        self.SPLITS=5\n        self.STRIDES=1000\n        self.SEED=42\n        self.EPOCHS = 100\n        self.BATCH_SIZE=128\n        self.DROPOUT = 0.2\n        self.HIDDEN_SIZE=100\n        self.LOSS = 'mean_absolute_error'\n        self.METRICS = ['mean_absolute_error']\n        self.OUTPUT_ACTIVATION = \"sigmoid\"\n        self.HIDDEN_ACTIVATION = \"relu\"\n        \n    def lstm_layer_BD_3D(self, hidden_size=self.HIDDEN_SIZE):\n        return L.Bidirectional(\n                                L.LSTM(hidden_size,\n                                dropout=self.DROPOUT,\n                                return_sequences=True,\n                                kernel_initializer='orthogonal'))\n    \n    def lstm_layer_BD_2D(self):\n        return L.Bidirectional(\n                                L.LSTM(self.HIDDEN_SIZE,\n                                dropout=self.DROPOUT,\n                                return_sequences=False,\n                                kernel_initializer='orthogonal'))\n    \n    def make_model_lstm_pooling(self, inshape=L11):\n        z = L.Input(shape=(60000, 10))\n        x = L.MaxPool1D(pool_size=self.STRIDES, strides=self.STRIDES)(z)\n        #x = L.AveragePooling1D(pool_size=self.STRIDES, strides=self.STRIDES)(z)\n        \n        x = self.lstm_layer_BD_3D()(x)\n        x = self.lstm_layer_BD_3D()(x)\n        \n        x = self.lstm_layer_BD_2D()(x)\n        x = L.Dense(self.HIDDEN_SIZE, activation='relu')(x)\n        x = L.Dense(self.OUTPUT_SHAPE, activation='sigmoid')(x)\n        \n        model = tf.keras.Model(z, x)\n        model.compile(optimizer='adam', loss=self.LOSS, metrics=self.METRICS)\n        return model\n    ","7b5f2a11":"dd = DataLoader()\n\nnp_train = train['segment_id'].apply(lambda x: dd.get_sample_all(x, 'train')).values\nnp_train = np.stack(np_train, axis=0)\nnp_train = np.nan_to_num(np_train)\n\nX_max = np.amax(np_train)\nX_min = np.amin(np_train)\n\nX_train = (np_train - X_min) \/ float(X_max - X_min)\n\nnp_dev = dev['segment_id'].apply(lambda x: dd.get_sample_all(x, 'train')).values\nnp_dev = np.stack(np_dev, axis=0)\nnp_dev = np.nan_to_num(np_dev)\n\nX_dev = (np_dev - X_min) \/ float(X_max - X_min)\n\nY_min = 6250\nY_max = 49046087\n\nY_train = train['time_to_eruption']\nY_train =  (Y_train - Y_min) \/ (Y_max - Y_min)\n\nY_dev = dev['time_to_eruption']\nY_dev =  (Y_dev - Y_min) \/ (Y_max - Y_min)\n","a60c2fe9":"model = eng.make_model_lstm_pooling()  \nf = 'best_model_pool.h5'   ","634bde8c":"checkpoint = C.ModelCheckpoint(\n                filepath=f,\n                save_best_only=True,  \n                monitor='val_loss',\n                mode='min')\n\ncallback_lr = C.ReduceLROnPlateau()\n\nhistory = model.fit(\n            X_train, Y_train,\n            validation_data=(X_dev, Y_dev),\n            epochs=eng.EPOCHS,\n            batch_size=eng.BATCH_SIZE,\n            callbacks=[checkpoint, callback_lr]\n                   )","5f3eabb0":"mae_val_loss = history.history['val_mean_absolute_error']\nmae_min = min(mae_val_loss)\nprint(\"MAE Min\", mae_min)","ffd9c575":"In this competition we have a huge amount of data for one sample: more than 60000 x 10 elements. \n\nI tried to reduce this number of features by using Pool1D."}}