{"cell_type":{"c111878c":"code","e5959325":"code","c477fef8":"code","468f2cec":"code","6af8150f":"code","e07d5b4e":"code","2dfbd670":"code","433c6d7e":"code","32cde984":"code","e3b981c1":"code","a60ee692":"code","3bb7f17e":"code","94ab9ace":"code","e6f7cf0a":"code","d5b3fb72":"code","6e7fd37c":"code","1ccb4bff":"code","8809a2e2":"code","48da3e5b":"code","578d4ceb":"code","5c42ef1d":"code","9aed9dcc":"code","1c700e78":"code","c93ad6a3":"code","29075cbd":"code","e1a8d294":"code","abfb92ce":"code","b0a8374c":"code","a2faf7ca":"code","0c3ecbf0":"code","27f50341":"code","29824a89":"code","98ad99ec":"code","104cc9b6":"code","c7fec933":"code","a4bbb0d6":"code","12f1669f":"code","be807fb5":"code","d45fd52c":"code","1283aadd":"code","c1de7c4c":"code","335cbea5":"code","61a908bb":"markdown","18556f96":"markdown","d159ff7e":"markdown","28fbf5e3":"markdown","00103774":"markdown","21c859f3":"markdown","02768fd6":"markdown","3fa2d9cd":"markdown","c11503dd":"markdown","7f34abfc":"markdown","9ef3a92d":"markdown","65699508":"markdown","82a8ec91":"markdown","f095e405":"markdown","f20665fe":"markdown","af08f35c":"markdown","a103bd2a":"markdown","4103f659":"markdown","88dbdaf9":"markdown","6a4e43f7":"markdown"},"source":{"c111878c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\n\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\nimport warnings\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\nfrom scipy.stats import norm, skew #for some statistics\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nseed = 5\nnp.random.seed(seed)\n\n%matplotlib inline","e5959325":"import os\nprint(os.listdir(\"..\/input\"))\n\ninput_train_file=\"..\/input\/train.csv\"\ninput_test_file=\"..\/input\/test.csv\"\ntrain_input = pd.read_csv(input_train_file)\ntest_input = pd.read_csv(input_test_file)\n\nsample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")","c477fef8":"print (\"Train input data info: \")\nprint (train_input.info())\nprint (train_input.describe())","468f2cec":"print (\"Has missing values? \", train_input.isnull().values.any())\nprint (\"\\nNumber of missing values :\", train_input.isnull().sum().sum())  \nprint (\"\\nColumn with missing value: \",train_input.columns[train_input.isnull().any()])","6af8150f":"print (\"Missing value by column name: \") \nprint (train_input.isnull().sum())\n#print (input_train.isnull().sum().sum())    ","e07d5b4e":"train_input.head(5)","2dfbd670":"print (\"Test input data info: \")\nprint (test_input.info())\nprint (test_input.describe())","433c6d7e":"print (\"Has missing values? \", test_input.isnull().values.any())\nprint (\"\\nNumber of missing values :\", test_input.isnull().sum().sum())  \nprint (\"\\nColumn with missing value: \",test_input.columns[test_input.isnull().any()])","32cde984":"test_input.head(5)","e3b981c1":"#train_input.hist()","a60ee692":"train_input['SalePrice'].describe()","3bb7f17e":"#histogram\nsns.distplot(train_input['SalePrice']);","94ab9ace":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train_input['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_input['SalePrice'].kurt())","e6f7cf0a":"#heatmap\ncorrmat = train_input.corr()\nf, ax = plt.subplots(figsize=(14, 10))\nsns.heatmap(corrmat, vmax=.8, square=True);","d5b3fb72":"#scatter plot GarageCars\/saleprice\nvar = 'GarageCars'\ndata = pd.concat([train_input['SalePrice'], train_input[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","6e7fd37c":"#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([train_input['SalePrice'], train_input[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","1ccb4bff":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([train_input['SalePrice'], train_input[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","8809a2e2":"var = 'YearBuilt'\ndata = pd.concat([train_input['SalePrice'], train_input[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","48da3e5b":"#drop unused column id\nhouse_train_data = train_input.drop('Id', axis=1)\nhouse_test_data = test_input.drop('Id', axis=1)\n\n#We will find all the columns which have more than 40 % NaN data and drop then\nthreshold=0.4 * len(house_train_data)\ndf=pd.DataFrame(len(house_train_data) - house_train_data.count(),columns=['count'])\ndf.index[df['count'] > threshold]","578d4ceb":"#drop columns not use \nhouse_train_data = house_train_data.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\nhouse_test_data = house_test_data.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)","5c42ef1d":"house_train_data.select_dtypes(include=np.number).columns #will give all numeric columns ,we will remove the SalePrice column \nfor col in ('MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n       'MoSold', 'YrSold'):\n    \n    house_train_data[col] = house_train_data[col].fillna(0)\n    house_test_data[col] = house_test_data[col].fillna('0')","9aed9dcc":"house_train_data.select_dtypes(exclude=np.number).columns\nfor col in ('MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n       'PavedDrive', 'SaleType', 'SaleCondition'):\n    \n    house_train_data[col] = house_train_data[col].fillna('None')\n    house_test_data[col] = house_test_data[col].fillna('None')","1c700e78":"# Verify that there are no null values in the data set\nhouse_train_data[house_train_data.isnull().any(axis=1)]","c93ad6a3":"house_test_data[house_test_data.isnull().any(axis=1)]","29075cbd":"#columns before hot encoding\nhouse_train_data.columns","e1a8d294":"# Combining the two datasets and then doing One Hot Encoding on the combined dataset.\ntrain=house_train_data\ntest=house_test_data\n\n#Assigning a flag to training and testing dataset for segregation after OHE .\ntrain['train']=1 \ntest['train']=0\n\n#Combining training and testing dataset\ncombined=pd.concat([train,test])","abfb92ce":"#Applying One Hot Encoding to categorical data\nohe_data_frame=pd.get_dummies(combined, \n                           columns=['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n       'PavedDrive', 'SaleType', 'SaleCondition'],\n      )\n","b0a8374c":"#columns after hot encoding with additional columns created\nhouse_train_data.columns","a2faf7ca":"#Splitting the combined dataset after doing OHE .\ntrain_df=ohe_data_frame[ohe_data_frame['train']==1]\ntest_df=ohe_data_frame[ohe_data_frame['train']==0]\n\ntrain_df.drop(['train'],axis=1,inplace=True)             #Drop the Flag(train) coloumn from training dataset\ntest_df.drop(['train','SalePrice'],axis=1,inplace=True)     #Drop the Flag(train),Label(SalePrice) coloumn from test dataset","0c3ecbf0":"# re-assign values back\nhouse_train_data=train_df\nhouse_test_data=test_df","27f50341":"house_train_data.head()","29824a89":"# features data\nX_train = house_train_data.drop('SalePrice', axis=1)\n\n# labels\nY_train = house_train_data['SalePrice']\nY_train = np.log(Y_train+1)\n\n# test data\nX_test = house_test_data","98ad99ec":"# find outliner then remove it\nfrom sklearn.linear_model import Ridge, ElasticNet\nrr = Ridge(alpha=10)\nrr.fit(X_train, Y_train)\nnp.sqrt(-cross_val_score(rr, X_train, Y_train, cv=5, scoring=\"neg_mean_squared_error\")).mean()","104cc9b6":"y_pred = rr.predict(X_train)\nresid = Y_train - y_pred\nmean_resid = resid.mean()\nstd_resid = resid.std()\nz = (resid - mean_resid) \/ std_resid\nz = np.array(z)\noutliers1 = np.where(abs(z) > abs(z).std() * 3)[0]\noutliers1","c7fec933":"#delete outliners\nX_train = X_train.drop([30, 88, 142, 277, 328, 410, 462, 495, 523, 533, 581, 588, 628, 632, 681, 688, 710, 714, 728, 774, 812, 874, 898, 916, 968, 970, 1181, 1182, 1298, 1324, 1383, 1423, 1432, 1453])\nY_train = Y_train.drop([30, 88, 142, 277, 328, 410, 462, 495, 523, 533, 581, 588, 628, 632, 681, 688, 710, 714, 728, 774, 812, 874, 898, 916, 968, 970, 1181, 1182, 1298, 1324, 1383, 1423, 1432, 1453])\nprint (\"dropped outliners\")","a4bbb0d6":"#GardientBoosting\nparams = {'n_estimators': 400, 'max_depth': 5, 'min_samples_split': 2,'learning_rate': 0.09, 'loss': 'ls'}\ngbr_model = GradientBoostingRegressor(**params)\ngbr_model.fit(X_train, Y_train)","12f1669f":"gbr_model.score(X_train, Y_train)","be807fb5":"from sklearn.model_selection import cross_val_score\n#cross validation\nnp.sqrt(-cross_val_score(gbr_model, X_train, Y_train, cv=5, scoring=\"neg_mean_squared_error\")).mean()","d45fd52c":"#Predicting the SalePrice for the test data\ny_grad_predict = gbr_model.predict(X_test)\ny_grad_predict=np.exp(y_grad_predict)-1\n#print(y_grad_predict)","1283aadd":"#Submission \n#my_submission = pd.DataFrame({'Id': test_input.Id, 'SalePrice': y_grad_predict})\n#print(my_submission)\nsample_submission[\"SalePrice\"] = y_grad_predict\nsample_submission.to_csv(\"house_price_submission2.csv\", index=False)","c1de7c4c":"#pre = pd.read_csv(\"house_price_submission.csv\")\n#pre.head()","335cbea5":"print (\"the end\")","61a908bb":"**Prepare data**","18556f96":"The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. \n\nHyper-parameters are parameters that are not directly learnt within estimators. I\nExhaustive search over specified parameter values for an estimator.\nImportant members are fit, predict.","d159ff7e":"1. ### Predict House Prices with ML GradientBoostingRegressor\nObjective: \nbuild a regression model that can predict the sales price of the house that match market value. \nThe key here is to make the develop a linear regression models to help us predict housing prices based on most relevant features and greatest impact to the price.","28fbf5e3":"Does number of bedroom affect sales price?","00103774":"For this model implementation, let's use Gradient Boosting for regression and find the best parameter for GBR using GridSearchCV as the optimal approach.","21c859f3":"reference: https:\/\/scikit-learn.org\/stable\/","02768fd6":"**Explorer input data**","3fa2d9cd":"**Prepare data for model training**\nthis is the most important part","c11503dd":"**Model Building**","7f34abfc":"**Correlation matrix**","9ef3a92d":"Training data","65699508":"Does year built has impact on sales price?","82a8ec91":"#### Gradient Boosting for regression.\n\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.","f095e405":"how about living area?","f20665fe":"Input Test Data","af08f35c":"hows does number of car garage affect sales price?","a103bd2a":"Find all the numeric columns and replace the NaN values with 0 , and for categorical columns ,replace NaN values with 'None'.","4103f659":"**Relationship with features**","88dbdaf9":"#### Analysing label 'SalePrice'   and relationships with features","6a4e43f7":"Data Cleaning is now complete We can now use our data to build our models"}}