{"cell_type":{"fbe89e49":"code","0c641620":"code","ec60e437":"code","913abbe7":"code","6fa61b88":"code","ab18bdd5":"code","a16246e4":"code","523ca95b":"code","eede6d18":"code","8065bfd3":"code","f0130372":"code","bbe839fe":"code","995165a9":"code","682843c6":"code","f3f24ceb":"code","37ff8fce":"code","1190c216":"code","69d20d2f":"code","6e46d464":"code","434d3739":"code","e9c52549":"code","b48047b0":"code","cb8f2c42":"code","c396478e":"code","09d370ee":"code","44fcc736":"code","e0261393":"code","ca40e817":"code","3ed7058a":"code","94c3eceb":"code","2561f4ef":"code","638b2a75":"code","217962bb":"code","247a973a":"code","6072f3c9":"code","230c83de":"code","be51c849":"code","ba5e62b0":"code","57fec1d6":"code","c79c7047":"code","b8d8044f":"code","861b24a6":"code","fc66cc1a":"code","f9e785b1":"code","ccdb4048":"code","d6787104":"code","bf90f2e0":"code","bc61d862":"code","797384e0":"code","99d7e9bb":"code","5df31052":"code","3e1a56aa":"code","008cd9bd":"code","208c70f5":"code","fb794312":"code","14692af5":"code","5d766f88":"code","0d0b4460":"markdown","90646b36":"markdown","d438fab3":"markdown","61d51c75":"markdown","a1498fad":"markdown","c0726f6e":"markdown","90fbfb8b":"markdown","fae16921":"markdown","aa800839":"markdown","0861e180":"markdown","31f05b87":"markdown","110e2166":"markdown","01389a37":"markdown","41756570":"markdown","4e5b22cd":"markdown","e86880ec":"markdown","d1bddfdc":"markdown","2c1ed4bf":"markdown","3e6c9e53":"markdown","2499afc3":"markdown","2e002b6e":"markdown","cbc4d6f4":"markdown","a99e6f70":"markdown","ebc21ee4":"markdown","fac912de":"markdown","0741c78a":"markdown"},"source":{"fbe89e49":"# Install the imutils library for image preprocessing\n!pip install imutils","0c641620":"# Imorting necessary libraries\nfrom imutils import paths\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\nfrom tensorflow.keras import Model, Sequential\nfrom tensorflow.keras import layers \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow.keras.metrics as metrics\n\n# Setting some variables for plotting via matplotlib\nplt.rcParams[\"figure.figsize\"] = (10, 10)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nplt.rc('lines', linewidth=3)\nplt.rc('font', size=15)","ec60e437":"import random\n\nrandom.seed(10)\nnp.random.seed(10)\ntf.random.set_seed(10)","913abbe7":"# Find and load normal image directories into a list\ndirectory_test = '..\/input\/chest-xray-pneumonia\/\/chest_xray\/chest_xray\/test\/NORMAL\/'\ndirectory_train = '..\/input\/chest-xray-pneumonia\/\/chest_xray\/chest_xray\/train\/NORMAL\/'\ndirectory_val = '..\/input\/chest-xray-pneumonia\/\/chest_xray\/chest_xray\/val\/NORMAL\/'\nimage_paths_norm = sorted(list(paths.list_images(directory_test))) + \\\n                   sorted(list(paths.list_images(directory_train))) + \\\n                   sorted(list(paths.list_images(directory_val)))\n\n# Declare a normal label list\nlabels_norm = ['Normal'] * len(image_paths_norm)","6fa61b88":"# Find and load pneumonia image directories into a list\ndirectory_test_pn = '..\/input\/chest-xray-pneumonia\/\/chest_xray\/chest_xray\/test\/PNEUMONIA\/'\ndirectory_train_pn = '..\/input\/chest-xray-pneumonia\/\/chest_xray\/chest_xray\/train\/PNEUMONIA\/'\ndirectory_val_pn = '..\/input\/chest-xray-pneumonia\/\/chest_xray\/chest_xray\/val\/PNEUMONIA\/'\nimage_paths_pn = sorted(list(paths.list_images(directory_test_pn))) + \\\n                 sorted(list(paths.list_images(directory_train_pn))) + \\\n                 sorted(list(paths.list_images(directory_val_pn)))\n\n# Declare a pneumonia label list for binary\nlabels_pn = ['Pneumonia'] * len(image_paths_pn)\n\n# Merge the image and label lists together and turn them into NumPy arrays\nimage_paths = np.array(image_paths_norm + image_paths_pn)\nlabels = np.concatenate((labels_norm, labels_pn))","ab18bdd5":"# Plot a bar with numbers of normal and pneumonia diagnoses\nplt.bar([1, 2], height=[len(labels_norm), len(labels_pn)], \n        tick_label=['({0:d} Normal Samples)'.format(len(labels_norm)), \n                    '({0:d} Pneumonia Samples)'.format(len(labels_pn))], \n        color=['m', 'g'])\n\nplt.title('Number Of Samples Per Available Label ({0:d} Total Samples)'.\n          format(len(labels_norm) + len(labels_pn)))\n\nplt.show()","a16246e4":"# Build a 2 x 2 figure\nrows, columns = 2, 2\nfig, axes = plt.subplots(rows, columns)\n\n# Choose random indices for image selection\nnorm_indices = np.random.choice(len(image_paths_norm), rows * columns)\npn_indices = np.random.choice(len(image_paths_pn), rows * columns)\n\n# Make a list with images to plot\nimgs_to_plot_norm = [image_paths_norm[i] for i in norm_indices]\nimgs_to_plot_pn = [image_paths_pn[i] for i in pn_indices]\n\n# Variable used for indending through the image list\ncurrent_index = 0\n\n# Set figure title\nfig.suptitle('4 Random Normal Samples')\n\n# Plot normal images\nfor i in range(rows):\n    for j in range(columns):\n        img = load_img(imgs_to_plot_norm[current_index])        \n        axes[i, j].imshow(img)\n        axes[i, j].set_xticks([])\n        axes[i, j].set_yticks([])\n        current_index +=1\n        \nplt.show()","523ca95b":"fig, axes = plt.subplots(rows, columns)\n\ncurrent_index = 0\n\n# Set figure title\nfig.suptitle('4 Random Pneumonia Samples')\n\n# Plot pneumonia images\nfor i in range(rows):\n    for j in range(columns):\n        img = load_img(imgs_to_plot_pn[current_index])        \n        axes[i, j].imshow(img)\n        axes[i, j].set_xticks([])\n        axes[i, j].set_yticks([])\n        current_index +=1\n        \nplt.show()","eede6d18":"from sklearn.model_selection import train_test_split\n\n# Divide the image path & label arrays into train & test sets. This is done without loading images to save memory\nX_train_dir, X_test_dir, y_train, y_test = \\\n                                           train_test_split(image_paths, labels, test_size=0.3)\n\n# Divide the test set into validation (for use during training) and test (for post-training evaulation) sets\nX_val_dir, y_val = X_test_dir[:len(X_test_dir) \/\/ 2], \\\n                   y_test[:len(y_test) \/\/ 2]\n\nX_test_dir, y_test = X_test_dir[len(X_test_dir) \/\/ 2:], \\\n                     y_test[len(y_test) \/\/ 2:]","8065bfd3":"# Create DataFrames for the image generator\ntrain_df = pd.DataFrame(np.transpose([X_train_dir, y_train]), \n                        columns=['filename', 'class'])\n\nval_df = pd.DataFrame(np.transpose([X_val_dir, y_val]), \n                      columns=['filename', 'class'])\n\ntest_df = pd.DataFrame(np.transpose([X_test_dir, y_test]), \n                       columns=['filename', 'class'])","f0130372":"# Set the batch size for the generator and training\nBATCH_SIZE = 64\n\n# Declare an image generator for image augmentation\ndatagen = ImageDataGenerator(rescale = 1.\/255,\n                             zoom_range=0.1, \n                             height_shift_range=0.05, \n                             width_shift_range=0.05,\n                             rotation_range=5)\n\n\n# Declare an image generator for validation & testing without generation\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\n# Declare generators for training, validation, and testing from DataFrames\ntrain_gen = datagen.flow_from_dataframe(train_df,\n                                        target_size=(512, 512),\n                                        color_mode='grayscale',\n                                        batch_size=BATCH_SIZE,\n                                        class_mode='binary',\n                                        shuffle=True)\n\nval_gen = test_datagen.flow_from_dataframe(val_df,\n                                        target_size=(512, 512),\n                                        color_mode='grayscale',\n                                        batch_size=BATCH_SIZE,\n                                        class_mode='binary',\n                                        shuffle=False)\n\ntest_gen = test_datagen.flow_from_dataframe(test_df,\n                                        target_size=(512, 512),\n                                        color_mode='grayscale',\n                                        batch_size=BATCH_SIZE,\n                                        class_mode='binary',\n                                        shuffle=False)","bbe839fe":"# Declare TensorFlow Datasets for more efficient training\ntrain_data = tf.data.Dataset.from_generator(lambda: train_gen,\n                                            output_types=(tf.float32, tf.int32),\n                                            output_shapes=([None, 512, 512, 1], [None, ]))\n\nval_data = tf.data.Dataset.from_generator(lambda: val_gen,\n                                          output_types=(tf.float32, tf.int32),\n                                          output_shapes=([None, 512, 512, 1], [None, ]))\n\ntest_data = tf.data.Dataset.from_generator(lambda: test_gen,\n                                           output_types=(tf.float32, tf.int32),\n                                           output_shapes=([None, 512, 512, 1], [None, ]))","995165a9":"images_to_augment = []\n\nfor image_path in image_paths[:4]:\n    image = load_img(image_path, target_size=(512, 512))\n    image = img_to_array(image)\n    images_to_augment.append(image)\n    \nimages_to_augment = np.array(images_to_augment)\n\nimages_augmented = next(datagen.flow(x=images_to_augment,\n                                batch_size=10,\n                                shuffle=False))","682843c6":"from tensorflow.keras.preprocessing.image import array_to_img\n\nfig, axes = plt.subplots(2, 2)\n\nfor i in range(2):\n    axes[i, 0].imshow(array_to_img(images_to_augment[i]), \n                      interpolation='nearest')\n    \n    axes[i, 1].imshow(array_to_img(images_augmented[i]), \n                      interpolation='nearest')\n    \n    axes[i, 0].set_xticks([])\n    axes[i, 1].set_xticks([])\n    \n    axes[i, 0].set_yticks([])\n    axes[i, 1].set_yticks([])\n    \ncolumns = ['Base Image', 'Augmented Image']\nfor ax, column in zip(axes[0], columns):\n    ax.set_title(column) \n    \nplt.show()","f3f24ceb":"def feed_data(dataset):\n    \"\"\"Feed data to a model with prefetching\n    \n    Arguments:\n        dataset (tf.Dataset): A dataset that to be fed to the model\n        \n    Returns:\n        dataset (tf.Dataset): A prefetched dataset\n    \"\"\"\n    \n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  \n    \n    return dataset","37ff8fce":"# Define the CNN Keras model\ndef create_model():\n    \"\"\"\n    Create a model\n    \n    Returns:\n        model (tf.keras.Model): An instance of Model\n    \"\"\"\n    \n    # Model input\n    input_layer = layers.Input(shape=(512, 512, 1), name='input')    \n\n    # First block\n    x = layers.Conv2D(filters=64, kernel_size=3, \n                      activation='relu', padding='same', \n                      name='conv2d_1')(input_layer)\n    x = layers.MaxPool2D(pool_size=2, name='maxpool2d_1')(x)\n    x = layers.Dropout(0.1, name='dropout_1')(x)\n    \n    # Second block\n    x = layers.Conv2D(filters=96, kernel_size=3, \n                      activation='relu', padding='same', \n                      name='conv2d_2')(x)\n    x = layers.MaxPool2D(pool_size=2, name='maxpool2d_2')(x)\n    x = layers.Dropout(0.1, name='dropout_2')(x)\n\n    # Third block\n    x = layers.Conv2D(filters=128, kernel_size=3, \n                      activation='relu', padding='same', \n                      name='conv2d_3')(x)\n    x = layers.MaxPool2D(pool_size=2, name='maxpool2d_3')(x)\n    x = layers.Dropout(0.1, name='dropout_3')(x)\n    \n    # Fourth block\n    x = layers.Conv2D(filters=160, kernel_size=3, \n                      activation='relu', padding='same', \n                      name='conv2d_4')(x)\n    x = layers.MaxPool2D(pool_size=2, name='maxpool2d_4')(x)\n    x = layers.Dropout(0.1, name='dropout_4')(x)\n\n    # Fifth block\n    x = layers.Conv2D(filters=192, kernel_size=3, \n                      activation='relu', padding='same', \n                      name='conv2d_5')(x)\n    x = layers.MaxPool2D(pool_size=2, name='maxpool2d_5')(x)\n    x = layers.Dropout(0.1, name='dropout_5')(x)\n    \n    # Sixth block\n    x = layers.Conv2D(filters=224, kernel_size=3, \n                      activation='relu', padding='same', \n                      name='conv2d_6')(x)\n    x = layers.MaxPool2D(pool_size=2, name='maxpool2d_6')(x)\n    x = layers.Dropout(0.1, name='dropout_6')(x)\n    \n    # Seventh block\n    x = layers.Conv2D(filters=256, kernel_size=3, \n                      activation='relu', padding='same', \n                      name='conv2d_7')(x)\n    x = layers.MaxPool2D(pool_size=2, name='maxpool2d_7')(x)\n    x = layers.Dropout(0.1, name='dropout_7')(x)\n\n    # Pooling and output\n    x = layers.GlobalAveragePooling2D(name='global_average_pooling2d')(x)    \n    output = layers.Dense(units=1, \n                          activation='sigmoid', \n                          name='output')(x)\n\n    # Model creation and compilation\n    \n    model = Model (input_layer, output)    \n    model.compile(optimizer='adam', \n                  loss='binary_crossentropy', \n                  metrics=['accuracy'])\n\n    return model","1190c216":"# Create a Model object\nmodel = create_model()","69d20d2f":"# See the layer and parameter summary\nmodel.summary()","6e46d464":"from tensorflow.keras.utils import plot_model\n\n# See graphical representation of the model\nplot_model(model, show_shapes=True)","434d3739":"# Set up exponential learning rate decay\ndef lr_decay(epoch):\n    \"\"\"\n    Create a learning rate reduction scheduler\n    \n    Arguments:\n        epoch (int): The index of the current epoch        \n        \n    Returns:\n        lr (float): Learning rate as of epoch\n    \"\"\"\n    \n    initial_lr = 0.001    \n    lr = initial_lr * np.exp(-0.1 * epoch)\n    return lr","e9c52549":"# Import classes for metric saving, model saving, and LR reduction\nfrom tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n\nlr_scheduler = LearningRateScheduler(lr_decay, 1)\ncsv_logger = CSVLogger(filename='7-layer_double_adam_512_aug_bn_dropout01_explr.csv')\nmodel_checkpoint = ModelCheckpoint(filepath='7-layer_double_adam_512_aug_bn_dropout01_explr_{epoch:04d}.hdf5')","b48047b0":"# Calculate the number of steps for training and validation\ntrain_steps = train_gen.samples \/\/ BATCH_SIZE\nval_steps = val_gen.samples \/\/ BATCH_SIZE","cb8f2c42":"'''\nhistory = model.fit(feed_data(train_data),                    \n                    epochs=30,                    \n                    steps_per_epoch=train_steps,                    \n                    validation_data=(feed_data(val_data)),\n                    validation_steps=val_steps,                    \n                    shuffle=False,\n                    callbacks=[lr_scheduler, csv_logger, model_checkpoint])'''","c396478e":"# Load the weights of a pretrained model\nmodel.load_weights('\/kaggle\/input\/model-final\/7-layer_adam_512_aug_dropout01_explr_0023.hdf5')","09d370ee":"import pandas as pd\none_layer = pd.read_csv(\"..\/input\/model-final\/1-layer_adam_512.csv\")\nthree_layer = pd.read_csv(\"..\/input\/model-final\/3-layer_adam_512.csv\")\nfive_layer = pd.read_csv(\"..\/input\/model-final\/5-layer_adam_512.csv\")\nfive_layer_aug = pd.read_csv(\"..\/input\/model-final\/5-layer_adam_512_aug.csv\")\nseven_layer = pd.read_csv(\"..\/input\/model-final\/7-layer_adam_512.csv\")\n\n\n# Augmented\n# I have two separate CSV files since I first trained the model for 20 epochs and decided to add another 10 later.\nseven_layer_aug20 = pd.read_csv(\"..\/input\/model-final\/7-layer_adam_512_aug.csv\")\nseven_layer_aug30 = pd.read_csv(\"..\/input\/model-final\/7-layer_adam_512_aug (1).csv\")\n\n# Joining the two CSV files\nseven_layer_aug = seven_layer_aug20.append(seven_layer_aug30, ignore_index=True)\n\n# Dropout & LR Reduction\nseven_layer_aug_drop_20 = pd.read_csv(\"..\/input\/model-final\/7-layer_adam_512_aug_dropout02.csv\")\nseven_layer_aug_drop_10 = pd.read_csv(\"..\/input\/model-final\/7-layer_adam_512_aug_dropout01.csv\")\nseven_layer_aug_drop_10_explr = pd.read_csv(\"..\/input\/model-final\/7-layer_adam_512_aug_dropout01_explr.csv\")","44fcc736":"def plot_loss(results, title, ylim=None, figsize=(15, 15)):\n    \"\"\"\n    Plot the loss metrics from a DataFrame file\n    \n    Arguments:\n        results (pd.DataFrame): DataFrame containing loss metrics\n        title (string): Title for the plot\n        ylim (float): Limit for the plot's y axis, default=None\n        figsize (tuple of ints): Figure size, default=(15, 15)\n    \"\"\"\n    \n    plt.figure(figsize=figsize)\n\n    for name, result in results:\n        val = plt.plot(range(len((result['epoch']))), result['val_loss'],\n                       '--', label=name.title()+', Validation', lw=3.0)\n        plt.plot(range(len((result['epoch']))), result['loss'], color=val[0].get_color(),\n                 label=name.title()+', Training', lw=3.0)\n\n    plt.title(title)\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.ylim(ylim)\n    plt.grid(lw=2, ls='--')","e0261393":"def plot_accuracy(results, title, x_range=20, figsize=(15, 15)):\n    \"\"\"\n    Plot the accuracy metrics from a DataFrame file\n    \n    Arguments:\n        results (pd.DataFrame): DataFrame containing accuracy metrics\n        title (string): Title for the plot\n        ylim (float): Limit for the plot's y axis, default=None\n        figsize (tuple of ints): Figure size, default=(15, 15)\n    \"\"\"    \n    \n    plt.figure(figsize=figsize)\n\n    for name, result in results:\n        val = plt.plot(range(len((result['epoch']))), result['val_accuracy'],\n                       '--', label=name.title()+', Validation', lw=3.0)\n        plt.plot(range(len((result['epoch']))), result['accuracy'], color=val[0].get_color(),\n                 label=name.title()+', Training', lw=3.0)\n\n    plt.title(title)\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(lw=2, ls='--')","ca40e817":"plot_loss([('3 Layers',  three_layer),\n           ('5 Layers', five_layer),\n           ('7 Layers', seven_layer)],\n          'Loss, 3, 5, & 7 Conv Layers')","3ed7058a":"plot_accuracy([('3 Layers',  three_layer),\n               ('5 Layers', five_layer),\n               ('7 Layers', seven_layer)],\n              'Accuracy, 3, 5, & 7 Conv Layers')","94c3eceb":"plot_loss([('Without Augmentation',  seven_layer),\n           ('With Augmentation',  seven_layer_aug)],\n          'Loss, 7 Conv Layers W\/ & W\/out Image Augmentation')","2561f4ef":"plot_accuracy([('Without Augmentation',  seven_layer),\n               ('With Augmentation',  seven_layer_aug)],\n              'Accuracy, 7 Conv Layers W\/ & W\/out Image Augmentation')","638b2a75":"plot_loss([('0.2 Dropout',  seven_layer_aug_drop_20),\n           ('0.1 Dropout',  seven_layer_aug_drop_10)],\n          'Loss, Regularized Model W\/ & W\/out Dropout')","217962bb":"plot_accuracy([('0.2 Dropout',  seven_layer_aug_drop_20),\n               ('0.1 Dropout',  seven_layer_aug_drop_10)],\n              'Accuracy, Regularized Model W\/ & W\/out Dropout')","247a973a":"plot_loss([('Without Decay',  seven_layer_aug_drop_10),\n           ('With Decay',  seven_layer_aug_drop_10_explr)],\n          'Loss, Regularized Model W\/ & W\/out LR Decay')","6072f3c9":"plot_accuracy([('Without Decay',  seven_layer_aug_drop_10),\n               ('With Decay',  seven_layer_aug_drop_10_explr)],\n              'Accuracy, Regularized Model W\/ & W\/out LR Decay')","230c83de":"plot_loss([('Base',  seven_layer),\n           ('Regularized',  seven_layer_aug_drop_10_explr)],\n          'Loss, 7-Layer Base & Regularized')","be51c849":"plot_accuracy([('Base',  seven_layer),\n               ('Regularized',  seven_layer_aug_drop_10_explr)],\n              'Accuracy, 7-Layer Base & Regularized')","ba5e62b0":"# Evaluate the model on the test set\ntest_steps = test_gen.samples \/\/ BATCH_SIZE\nmodel.evaluate(test_data, steps=test_steps)\n\n# Declare an image generator on test data for augmented images\ntest_aug_gen = datagen.flow_from_dataframe(test_df,\n                                           target_size=(512, 512),\n                                           color_mode='grayscale',\n                                           batch_size=BATCH_SIZE,\n                                           class_mode='binary',\n                                           shuffle=False)\n\ntest_aug_data = tf.data.Dataset.from_generator(lambda: test_aug_gen,\n                                           output_types=(tf.float32, tf.int32),\n                                           output_shapes=([None, 512, 512, 1], [None, ]))\n\n# Evaluate the model on augmented test data\ntest_aug_steps = test_aug_gen.samples \/\/ BATCH_SIZE\nmodel.evaluate(test_aug_data, steps=test_aug_steps)","57fec1d6":"# Calculate precision and recall based on  test data\n\nprecision = tf.keras.metrics.Precision()\nrecall = tf.keras.metrics.Recall()\n\npredictions = model.predict(test_data, steps=test_steps).flatten()\ny_true = test_gen.classes[:len(predictions)]\n\nprecision.update_state(y_true, predictions)\nrecall.update_state(y_true, predictions)","c79c7047":"print('Precision with base test data:', precision.result().numpy())\nprint('Recall with base test data:', recall.result().numpy())","b8d8044f":"# Calculate precision and recall based on augmented test data\n\nprecision.reset_states()\nrecall.reset_states()\n\npredictions_aug = model.predict(test_aug_data, steps=test_steps).flatten()\ny_true = test_gen.classes[:len(predictions_aug)]\n\nprecision.update_state(y_true, predictions_aug)\nrecall.update_state(y_true, predictions_aug)","861b24a6":"print('Precision with augmented test data:', precision.result().numpy())\nprint('Recall with augmented test data:', recall.result().numpy())","fc66cc1a":"from sklearn.metrics import confusion_matrix\n\nconfusion_mat = confusion_matrix(y_true, predictions > 0.5)","f9e785b1":"fig, ax = plt.subplots()\n\nax.matshow(confusion_mat, cmap=plt.cm.Oranges)\n\nax.set_xlabel('Prediction')\nax.set_ylabel('True Label')\n\ntick_labels = ['Normal', 'Pneumonia']\n\nax.set_xticks(range(len(tick_labels)))\nax.set_yticks(range(len(tick_labels)))\nax.set_xticklabels(tick_labels)\nax.set_yticklabels(tick_labels)\n\nfor i in range(len(tick_labels)):\n    for j in range(len(tick_labels)):\n        ax.text(j, i, confusion_mat[i, j],\n               ha='center', va='center')\n        \nplt.show()","ccdb4048":"from tensorflow.keras.preprocessing.image import array_to_img\n\ndef plot_image(image, prediction, label):\n    \"\"\"\n    Plot images along with predictions and true labels\n    \n    Arguments:\n        image (4-D array): The image to visualize\n        prediction (2-D array): Model's prediction on the image\n        label (1-D array): True label of the image\n    \"\"\"\n    \n    label_names = ['Normal', 'Pneumonia']    \n        \n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n\n    plt.imshow(array_to_img(image * 255), interpolation='nearest', cmap='gray')\n    \n    if prediction <= 0.5:\n        predicted_label = 0\n    else:\n        predicted_label = 1\n\n    if predicted_label == label:\n        color = 'blue'\n    else:\n        color = 'red' \n    \n    plt.xlabel(\"{} {:2.0f}% \\n ({})\".format(label_names[predicted_label], 100 * prediction[0], label_names[label]), color=color)","d6787104":"images, labels = next(iter(test_data))\ny_pred = model.predict(images)\nnum_rows = 4\nnum_cols = 4\nnum_images = num_rows * num_cols\nplt.figure(figsize=(2*2*num_cols, 2.2*2*num_rows))\n\nfor i in range(num_images):\n    plt.subplot(num_rows, num_cols, i+1)\n    plot_image(images[i], y_pred[i], labels[i])\nplt.show()","bf90f2e0":"def calculate_metrics(predictions, labels, threshold):\n    \"\"\"\n    Calculate precision, recall, and binary accuracy given a dataset\n    \n    Arguments:\n        predictions (1-D array): Model predictions\n        labels (1-D array): True labels\n        threshold (float): The metrics' decision threshold\n        \n    Returns:\n        Mean of metrics on all batches of data\n    \"\"\"\n    \n    precision = tf.keras.metrics.Precision(thresholds=threshold)\n    recall = tf.keras.metrics.Recall(thresholds=threshold)\n    binary_accuracy = tf.keras.metrics.BinaryAccuracy(threshold=threshold)        \n    \n    precision.update_state(y_true, predictions) \n    recall.update_state(y_true, predictions)     \n    binary_accuracy.update_state(y_true, predictions) \n        \n    return precision.result().numpy(), recall.result().numpy(), binary_accuracy.result().numpy()","bc61d862":"thresholds = np.linspace(0, 1, 100)\n\nprecisions, recalls, binary_accuracies = [], [], []","797384e0":"for threshold in thresholds:\n    precision, recall, binary_accuracy = calculate_metrics(predictions, y_true, threshold)\n    \n    precisions.append(precision)\n    recalls.append(recall)\n    binary_accuracies.append(binary_accuracy)","99d7e9bb":"def plot_metrics(precisions, recalls, accuracies, thresholds):\n    \"\"\"\n    Plot the dependency of metrics on threshold values\n    \n    Arguments:\n        precisions (1-D array): Precisions calculated on thresholds\n        recalls (1-D array): Recalls calculated on thresholds\n        accuraries (1-D array): Accuracies calculated on thresholds\n        thresholds (1-D array): Thresholds upon which metrics were evaluated\n    \n    \"\"\"\n    \n    plt.plot(thresholds, precisions, label='Precison')\n    plt.plot(thresholds, recalls, '--', label='Recall')\n    plt.plot(thresholds, accuracies,'-.', label='Accuracy')\n    \n    plt.xticks(np.linspace(0, 1, 10))\n    plt.xlabel('Threshold')\n    plt.ylabel('Score')\n    plt.ylim([0.7, 1])\n    plt.grid()\n    plt.legend()\n    plt.title('The Dependence Of Metric Scores On The Decision Threshold')\n    plt.show()","5df31052":"plot_metrics(precisions, recalls, binary_accuracies, thresholds)","3e1a56aa":"pip install https:\/\/github.com\/raghakot\/keras-vis\/archive\/master.zip","008cd9bd":"from vis.visualization import visualize_cam, visualize_saliency, visualize_activation, overlay\nfrom vis.utils import utils\n\nimages, labels = next(iter(test_data))\n\npredictions = model.predict(images)","208c70f5":"def visualize_cams(images, predictions, labels): \n    '''\n    Plot Grad-CAMs on a batch of images\n    \n    Arguments:\n        images (4-D array): Images to compute Grad-CAM on\n        predictions (2-D array): Predicted labels\n        labels (1-D array): True labels\n    '''\n    \n    rows, cols = 3, 3\n\n    fig, axes = plt.subplots(rows, cols)\n    fig.set_size_inches(2*2*cols, 2.2*2*rows)    \n    fig.suptitle('Grad-CAMs On 9 Images')\n    current_index = 0\n    label_names = ['Normal', 'Pneumonia']\n    \n    for i in range(rows) :\n        for j in range(cols):\n            image = images[current_index]\n            image_rgb = tf.image.grayscale_to_rgb(image)\n            visualization = visualize_cam(model, -1, filter_indices=0, seed_input=image, penultimate_layer_idx=-4)            \n            \n            axes[i, j].imshow(tf.image.grayscale_to_rgb(image))\n            axes[i, j].imshow(visualization, interpolation='nearest', alpha=0.6)\n            axes[i, j].set_xticks([])\n            axes[i, j].set_yticks([])\n            \n            if predictions[current_index] <= 0.5:\n                predicted_label = 0\n            else:\n                predicted_label = 1                 \n            \n            if predicted_label == labels[current_index]:\n                color = 'blue'\n            else:\n                color = 'red'\n            \n            label = labels[current_index]\n            axes[i, j].set_xlabel(\"{} {:2.0f}% \\n ({})\".format(label_names[predicted_label], 100 * predictions[current_index][0], label_names[label]), color=color)\n            current_index += 1\n            \n    plt.show()","fb794312":"# Visualize Grad-CAMs\nvisualize_cams(images, predictions, labels)","14692af5":"def visualize_sal(images, predictions, labels):\n    \"\"\"\n    Plot saliency map on a batch of images\n    \n    Arguments:\n        images (array-like): Images to compute saliency upon\n        predictions (array-like): Model's predictions on images\n        labels (array-like): True labels of the images\n    \"\"\"\n    \n    rows, cols = 3, 3\n\n    fig, axes = plt.subplots(rows, cols)\n    fig.set_size_inches(2*2*cols, 2.2*2*rows)\n    fig.suptitle('Saliency Maps On 9 Images')\n    current_index = 0\n    label_names = ['Normal', 'Pneumonia']    \n    \n    for i in range(rows) :\n        for j in range(cols):\n            image = images[current_index]\n            image_rgb = tf.image.grayscale_to_rgb(image)\n            visualization = visualize_saliency(model, -1, filter_indices=None, seed_input=image)                  \n            \n            #axes[i, j].imshow(tf.image.grayscale_to_rgb(image))\n            axes[i, j].imshow(visualization * 255, interpolation='nearest', alpha=1)\n            axes[i, j].set_xticks([])\n            axes[i, j].set_yticks([])\n            \n            if predictions[current_index] <= 0.5:\n                predicted_label = 0\n            else:\n                predicted_label = 1                 \n            \n            if predicted_label == labels[current_index]:\n                color = 'blue'\n            else:\n                color = 'red'\n            \n            label = labels[current_index]\n            axes[i, j].set_xlabel(\"{} {:2.0f}% \\n ({})\".format(label_names[predicted_label], 100 * predictions[current_index][0], label_names[label]), color=color)\n            current_index += 1","5d766f88":"# Visualize saliency maps\nvisualize_sal(images, predictions, labels)","0d0b4460":"Another method that I've used to optimize model performance was learning rate decay. I've used exponential decay since it seemed the most promising - however, other decay schedules may show better performance.\n\nThe *lr_decay* method is to be given to Keras' *LearningRateScheduler* object and then provided to the model as a callback. ","90646b36":"Generally, pneumonia manifests itself in alveoli consolidations mainly caused by bacteria and fluid. The differences between healthy and pneumonia X-Rays are pretty clearly visible, though in some images, the differences may not be as definite. ","d438fab3":"Finally, we've reached model creation. The model architecture that you see below is the final architecture that I chose after some testing. I've saved the results of my test runs in csv files, and we'll be plotting them later so that you see how my model selection went.\n\nNow, let's define the model and then plot its summary along with the model architecture.","61d51c75":"And below is how I used the *prefetch* method of the *tf.Dataset* class that I talked about.","a1498fad":"Below, I create three callbacks for training - the aforementioned *LearningRateScheduler*, a *CSVLogger* object for model result saving, and a *ModelCheckpoint* object for model weight saving after each epoch. I did the last two to save results and models since I did my analysis in multiple notebook runs.\n\nHere, I am declaring the *CSVLogger* and *ModelCheckpoint* for demonstration - we won't be using them since I've already done the training & testing.","c0726f6e":"Now, let's plot a couple of images from each diagnosis to see how they look like.","90fbfb8b":"Next, we need to define the number of training and validation steps. I've calculated them by diving sample number by batch size and then rounding the result down. Since the result is rounded down, not all images will be fed into the model. I did this consciously since I've also tested models without data augmentation - without data augmentation, I might have been providing duplicate images to the model, which I wanted to avoid.","fae16921":"I will be feeding the data into the model via an *ImageDataGenerator* object. However, because we have image paths rather than image arrays, we can't use the *flow* method. Instead, we have to use the *flow_from_dataframe* method, which requires DataFrames containing image paths under the \"filename\" column and labels under the \"class\" column.\n\nBelow, we create DataFrames for each of our sets.","aa800839":"As our next step, we will divive our image paths and labels into training, validation, and test sets. The purpose of these sets is as follows:\n1. **Training data** will be used to update the weights of the network to achieve high performance.\n2. **Validation data** will be used to tweak model hyperparameters to achieve low loss, high accuracy, and no overfitting.\n3. **Test data** will be used for final performance validation and to make sure that the model doesn't overfit to the parameters we choose based on validation performance.\n\nNote that I am not using the splits proposed by the original publisher of the data - this is because I have been unable to achieve high performance with it. Maybe it's possible, but I decided not to waste time and split the data anew myself.\n\nFor splitting, I am using scikit-learn's train_test_split method. Since this method only divides the dataset into training and test sets, I've then further divided the test set into validation and test sets - the former and latter halves of the initial test set respectively. The data distribution is 70% training and 15% validation & test data.","0861e180":"Based on saliency maps and Grad-CAMs, the model seems to pay attention to the right areas on the images (i.e. the lungs). However, the 1st and 9th Grad-CAMs don't look too good - the upper left corner is excessively highlighted in them. This may be because the model needs additional tweaking, or maybe Grad-CAM failed to localize regions of interest with those particular images. On saliency maps, these images look fine (although the saliency maps are very vague as well).\n\nFor real-world deployment, models like this would require more in-depth analysis and tweaking. Furthemore, cooperation between ML engineers and doctors would be necessary to identify weak points & faulty predictions and then try to fix the model. \n\nWith all that said, this experiment has been pretty interesting, and hopefully, you've learned a thing or two from it (particularly regarding the *ImageDataGenerator* class and overfitting reduction - I certainly did).","31f05b87":"Above, I've used 0.5 as the decision threshold. This threshold is also used during training for accuracy computations.\n\nYou can adjust the decision threshold in the [0, 1] range to tweak the confidence with which the model will make predictions. Needless to say, this will change how well the model performs. For a better look, let's see how the metrics change with a few thresholds values.","110e2166":"And a confusion matrix for a better look at true and false prediction counts.","01389a37":"In this notebook, I am going to try and create a high-performance model for pneumonia diagnosis based on X-Rays.\n\nLet's get started by installing and importing the necessary libraries.","41756570":"It's interesting to see how the diagnoses are distributed in the dataset. Below is a bar plot where we can see that pneumonia samples are predominant in the dataset - about 73%. Thanks to this, the model should be able to easily learn to identify pneumonia and thus should not need any class weight adjustment.","4e5b22cd":"As we can see, precision is low with low thresholds and high with high thresholds. The dependence for recall is inverse - it's the highest when the threshold is 0 and the lowest when it's 1. As for accuracy, it's somewhat dependent on precision and recall, so its highest scores are somewhere in the middle.","e86880ec":"For a little added input pipeline efficiency, I've also created *tf.Dataset* objects. I wanted to make use of the *prefetch* method of the *tf.Dataset* class to accelerate training a little. \n\nImage reading from the disk is a huge bottleneck for the model, and although my approach isn't the best one, it accelerates each epoch by about 20-30 seconds.","d1bddfdc":"Below, I commence training will validation data and callbacks. I've commented this part out because training takes a long time and because I have the results anyway.","2c1ed4bf":"And finally, let's have a look at saliency maps and Grad-CAMs.","3e6c9e53":"Now, we load the weights of the model that I think did the best among my tested architectures. It's probably not the very best possible model, but I didn't dive too deep into model tweaking. However, I did reduce overfitting.","2499afc3":"To give you an idea of how the model performs, let's also plot a few images along with their predictions and true labels.","2e002b6e":"Let's see how the model does on test data. I've used the test data that I've declared earlier, as well as checked test data with augmentation just out of interest. I also computed precision and recall for both augmented and base test data.\n\nTest results with and without data augmentation are pretty close, so the model did a good job of learning the distribution of the data.","cbc4d6f4":"In the following couple of cells, we will be reading image jpeg files from the disk. The images are separated into directories based on their diagnosis (NORMAL and PNEUMONIA) and purpose (test, train, val). Thanks to the separation via diagnosis, I can easily create corresponding labels for each category.\n\nNote that the PNEUMONIA X-Rays in this dataset are also divided into bacterial and viral pneumonia (which can be seen from their filenames). However, in this particular notebook, I will only be reviewing binary classification - NORMAL vs PNEUMONIA.\n\nI will not be loading the images into memory right now - instead, this will be done by ImageDataGenerator objects during training. This is because I wanted to feed 512 x 512 x 1 images, which was too big for loading the entire dataset into memory. So right now, I am only declaring lists containing image paths, not image arrays.\n\nThe labels are strings as well since this is what's required by the *flow_from_dataframe* method of the *ImageDataGenerator* class that I will be using later.","a99e6f70":"Now, let's see how our training generator works in terms of data augmentation. Let's load a couple of images from the disk, pass them to the *flow* method of the generator (this method acccepts image arrays), and plot the base and augmented images in RGB.\n\nAs you'll see, the augmented images look pretty decently and are realistic enough for training.","ebc21ee4":"Now, let's have a look at performance plots of all the models I've tested. These plots show how I determined that the architecture that was defined above was the best among the ones I tested.","fac912de":"Next, we declare ImageDataGenerator objects for batch input along with data augmentation to reduce overfitting. Data augmentation is only done on the training data (via the *datagen* object) - as for the validation and test data, only scaling to [0, 1] was performed (via the *test_datagen* object).\n\nFor data augmentation, I picked parameters so that the output images would be realistic and similar to what the model would be seeing in real-world use.\n\nAfter declaring generator objects, I declare *DataFrameIterator* objects via the *flow_from_dataframe* method. I passed arguments for: \n1. Target image size (512 x 512). \n2. Color mode (grayscale).\n3. Batch size (64).\n4. Class mode (binary).\n5. Shuffling (*True* for training data and *False* for validation & test data).\n\nWhen called, the *DataFrameIterator* will read images from the provided filenames, resize them, convert string labels to binary (\"Pneumonia\" to 1 and \"Normal\" to 0), and return 64 image-label pairs.","0741c78a":"In the code cell below, I'm fixing random seeds for reproducibility, although note that the apparent non-determinism of TensorFlow doesn't allow full reproducibility."}}