{"cell_type":{"2bc05d54":"code","50ca885d":"code","5fc90055":"code","46f32975":"code","ba5d952d":"code","83b7cc01":"code","54390f80":"code","6801636e":"code","64ddd437":"code","1c590e68":"code","7f93727f":"code","1937e39c":"code","24fa29b1":"code","d5c2ac5e":"code","37bce737":"code","b96c315a":"code","6ce89622":"code","25ec5ce3":"code","1f202ea3":"code","65cde554":"code","35d223b9":"markdown","9518c310":"markdown","5b256291":"markdown","9e8dd755":"markdown"},"source":{"2bc05d54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nDATA_PATH = \"..\/input\/ashrae-energy-prediction\/\"\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nimport matplotlib.pyplot as plt\nimport datetime\nfrom sklearn import metrics\nimport seaborn as sns\nimport sklearn.ensemble as ske\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50ca885d":"#Loading data\nWeather_Train = pd.read_csv(DATA_PATH + 'weather_train.csv')\nTrain= pd.read_csv(DATA_PATH +'train.csv')\nBuilding= pd.read_csv(DATA_PATH +'building_metadata.csv')","5fc90055":"#Function That reduces the used memory\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    return df","46f32975":"#Reducing the needed data's memory usage\nTrain = reduce_mem_usage(Train)\nWeather_Train = reduce_mem_usage(Weather_Train)\nBuilding = reduce_mem_usage(Building)","ba5d952d":"#Merging tables\nresults = Building.merge(Train,left_on='building_id',right_on='building_id',how='left')\ndata = results.merge(Weather_Train,left_on=['site_id','timestamp'],right_on=['site_id','timestamp'],how='left')\n\n#printing first 5 rows in the dataset\ndata.head()","83b7cc01":"# Add and Drop Features\ndata = data.drop(columns=['year_built', 'floor_count', 'wind_direction', 'dew_temperature'])\n\n#Function that is used to clean memory from deleted data\ngc.collect()","54390f80":"#fixing timestamp and taking only the day and the month and then dropping timestamp column\ndata[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\ndata[\"day\"]= data[\"timestamp\"].dt.day\ndata[\"month\"]= data[\"timestamp\"].dt.month\ndata= data.drop(\"timestamp\", axis = 1) ","6801636e":"#Change data type to float 32 for filling NA value before transforming them into int for smooth modeling processing\ndata['wind_speed'] = data['wind_speed'].astype('float32')\ndata['air_temperature'] = data['air_temperature'].astype('float32')\ndata['precip_depth_1_hr'] = data['precip_depth_1_hr'].astype('float32')\ndata['cloud_coverage'] = data['cloud_coverage'].astype('float32')","64ddd437":"#Filling Null Values\ndata['precip_depth_1_hr'].fillna(data['precip_depth_1_hr'].mean(), inplace = True)\ndata['cloud_coverage'].fillna(data['cloud_coverage'].mean(), inplace = True)\ndata['wind_speed'].fillna(data['wind_speed'].mean(), inplace=True)\ndata['air_temperature'].fillna(data['air_temperature'].mean(), inplace=True)\n\n# Printing the sum of nulls inside the columns\ndata.isnull().sum()","1c590e68":"# Here column 'primaty_use' was treated by get_dummies function and get_dummies is used for data manipulation\ndata_linearR = pd.get_dummies(data,columns = ['primary_use'])","7f93727f":"#printing the columns of \"data_linearR\"\ndata_linearR.columns","1937e39c":"#Using the important features\nXD =data_linearR[['building_id', 'meter', 'air_temperature', 'wind_speed', 'precip_depth_1_hr', 'cloud_coverage',\n       'square_feet', 'primary_use_Education', 'primary_use_Entertainment\/public assembly',\n       'primary_use_Food sales and service', 'primary_use_Healthcare',\n       'primary_use_Lodging\/residential',\n       'primary_use_Manufacturing\/industrial', 'primary_use_Office',\n       'primary_use_Other', 'primary_use_Parking',\n       'primary_use_Public services', 'primary_use_Religious worship',\n       'primary_use_Retail', 'primary_use_Services',\n       'primary_use_Technology\/science', 'primary_use_Utility',\n       'primary_use_Warehouse\/storage', 'month', 'day']]\n\n# Create target variable\nYD = data_linearR['meter_reading']\n\n# Train, test, split\nXD_train,XD_test, YD_train, YD_test = train_test_split(XD,YD, test_size = .20, random_state= 0)","24fa29b1":"#applying the DecisionTreeRegressor at depth = 2 & 5 and fitting the model\nregr_depth2 = DecisionTreeRegressor(max_depth=2)\nregr_depth5 = DecisionTreeRegressor(max_depth=5)\nregr_depth2.fit(XD_train, YD_train)\nregr_depth5.fit(XD_train, YD_train)","d5c2ac5e":"#generating the predicted values when the depth = 2 & 5\ny_1 = regr_depth2.predict(XD_test)\ny_2 = regr_depth5.predict(XD_test)","37bce737":"#creating a DataFrame that contains the actual and the predicted values while the depth = 2\ndf=pd.DataFrame({'Actual':YD_test, 'Predicted':y_1})\n#printing the head of the data (first 5 columns)\ndf.head()","b96c315a":"#Caluclating Mean Absolute Error, Mean Squared Error, Root Mean Squared Error, R^2 & The Accuracy when the depth =2\nprint('Mean Absolute Error:', metrics.mean_absolute_error(YD_test, y_1))\nprint('Mean Squared Error:', metrics.mean_squared_error(YD_test, y_1))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(YD_test, y_1)))\nprint('R^2 =',metrics.explained_variance_score(YD_test,y_1))\nprint('Accuracy for depth2  %d', regr_depth2.score(XD_train, YD_train))\n#For depth 2 desicion tree modeling, R2 was obtained at 0.147","6ce89622":"#creating a DataFrame that contains the actual and the predicted values while the depth = 5\ndf=pd.DataFrame({'Actual':YD_test, 'Predicted':y_2})\ndf.head()","25ec5ce3":"#Caluclating Mean Absolute Error, Mean Squared Error, Root Mean Squared Error, R^2 & The Accuracy when the depth =5\nprint('Mean Absolute Error:', metrics.mean_absolute_error(YD_test, y_2))\nprint('Mean Squared Error:', metrics.mean_squared_error(YD_test, y_2))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(YD_test, y_2)))\nprint('R^2 =',metrics.explained_variance_score(YD_test,y_2))\nprint('Accuracy for depth5 %d', regr_depth5.score(XD_train, YD_train))\n#For depth 5 desicion tree modeling, R2 was obtained at 0.723","1f202ea3":"#Plot that compares the results of depth=2 & depth =5\nplt.plot(XD_test, y_1, color=\"blue\",label=\"max_depth=2\", linewidth=2)\nplt.plot(XD_test, y_2, color=\"green\", label=\"max_depth=5\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.show()","65cde554":"## Predicting test set results\nyd_pred = regr_depth5.predict(XD_test)\nyd_pred","35d223b9":"* **Features Engineering**","9518c310":"* **DecisionTree model**","5b256291":"* **Memory usage reduction**","9e8dd755":"* **Import data**"}}