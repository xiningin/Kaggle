{"cell_type":{"b3f45b9e":"code","d4eae1fe":"code","c7be2182":"code","2df49728":"code","9befe1e0":"code","48a3af35":"code","cec0028b":"code","56a711df":"code","d7ab3e87":"code","ca70bd46":"code","c650c8d9":"code","2c67a32b":"code","e54fbd97":"code","1ffe225e":"code","7af747a6":"code","0defdd1e":"code","c512771b":"code","9a607428":"code","0167cdf9":"code","bd869039":"code","a315f1cc":"code","001fda7f":"code","d79682e9":"markdown","715c2fd2":"markdown","7b006170":"markdown","5b8f2c02":"markdown","e3cdd99b":"markdown","b40a668d":"markdown","3f728057":"markdown"},"source":{"b3f45b9e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4eae1fe":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport matplotlib\n\n# Preprocessing\nfrom sklearn import model_selection\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\n# Model\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","c7be2182":"# Read in the training and testing data\ntrain = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\", index_col=\"id\")\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\", index_col=\"id\")","2df49728":"# Show the shape of the dataset\nprint(\"Traing data count:\", train.shape)\nprint(\"Testing data count:\", test.shape)","9befe1e0":"train.head()","48a3af35":"train.info()","cec0028b":"test.info()","56a711df":"# Null data count\nnull_col = train.isnull().sum()\n\nplt.figure(figsize=(14, 6))\nplt.bar(train.columns, null_col)\nplt.tight_layout()","d7ab3e87":"# Null data\nnull_data = train.isnull()\n\nimport seaborn as sns\nsns.heatmap(null_data.values.astype(int), cmap=\"gray\")\nplt.xticks(np.arange(len(train.columns)), train.columns)\nprint(\"Heatmap for the figure\")","ca70bd46":"# Drop the duplicate data if there are two same rows\ntrain.drop_duplicates(inplace=True)","c650c8d9":"# Remove the rows with missing value\n# train.dropna(axis=0, inplace=True)\n\n# or \n\n# You can impute the missing value using sklearn.impute\n# See the section for dealing object columns","2c67a32b":"# In this step, we can separate the training and testing data\nX, y = train.drop(columns=[\"target\"]), train[\"target\"]","e54fbd97":"# Select the columns num_type_cols numerical types\nnum_type_cols = X.select_dtypes(include=\"number\").columns\nobj_type_cols = X.select_dtypes(include=\"object\").columns\n\n# Use boxplot to observe the outliers\n# This step can be done in two ways:\n#  either using .describe() or draw the box plot\n\n# sns.boxplot(data=train[num_cols])\nplt.boxplot(X[num_type_cols])\nplt.title(\"Box Plot for numerical features\")","1ffe225e":"# Using the normal distribution to remove the outliers\n# (Uncomment the below line for the usage)\n\n# num_columns_mean = X[num_type_cols].mean()\n# num_columns_std = X[num_type_cols].std()\n\n# clear_data = X[(np.abs(X[num_type_cols] - num_columns_mean) <= (3 * num_columns_std)) &\n#                (np.abs(X[num_type_cols] - num_columns_mean) >= (3 * num_columns_std))]","7af747a6":"# Found IQR\nQ1 = X[num_type_cols].quantile(0.25)\nQ3 = X[num_type_cols].quantile(0.75)\nIQR = Q3 - Q1\nfloor, ceil = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n\n# Clip the data\nX[num_type_cols] = X[num_type_cols].clip(lower=floor, upper=ceil,\n                                         axis=1)","0defdd1e":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy=\"mean\")\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"scaler\", OrdinalEncoder())\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numerical_transformer, num_type_cols),\n        (\"cat\", categorical_transformer, obj_type_cols)\n    ],\n  )","c512771b":"# Split the data into train test set\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, \n                                                  random_state=0)","9a607428":"# Set up the model\nmodel = XGBRegressor(n_estimators=1000, learning_rate=0.1)","0167cdf9":"# Build up the pipeline\npipeline = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", model)\n])","bd869039":"# Preprocessing of training data, fit model \npipeline.fit(X_train, y_train)","a315f1cc":"import math\n\n# Test it on the validation\nprediction = pipeline.predict(X_val)\n\n# Evaluate the model\nscore = mean_squared_error(y_val, prediction)\nrmse = math.sqrt(score)\nprint('RMSE:', rmse)","001fda7f":"test_predicion = pipeline.predict(test)\noutput = pd.DataFrame({\n    \"Id\": test.index,\n    \"target\": test_predicion\n})\noutput.to_csv(\"output.csv\", index=False)","d79682e9":"### Deal with outliers\n\n1. Using box plot to visualize the outliers\n2. Remove or Clip the outliers\n    * Using [Normal distribution](https:\/\/www.simplypsychology.org\/normal-distribution.html)\n    * Using [Inter quartile range](https:\/\/towardsdatascience.com\/practical-implementation-of-outlier-detection-in-python-90680453b3ce)","715c2fd2":"### 1. Read in data\n1. `pd.read_csv`","7b006170":"### 2. Check the null data\n\n1. Using heatmap or bar chart to visuallize the null data\n2. Fill in the null value","5b8f2c02":"### Deal with the columns with type `object`\n\nThis will be one of the elements in the pipeline we're going to make.\nThere will be two ways to convert the `object` data\n* OrdianlEncoder\n* OneHotEncoder","e3cdd99b":"## Data processing\nThe following steps should be done before we trained\n1. Read in data\n2. Deal with the null values\n3. Deal with the outliers\n4. Deal with the columns with type `object`\n5. Split the data into training and validation \n6. Other preprocessing","b40a668d":"## Build the model\nIn this section, we're going to build the model","3f728057":"### Separate the data into training and validation"}}