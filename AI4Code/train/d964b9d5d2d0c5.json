{"cell_type":{"319f1362":"code","993e6be2":"code","c3fb36c4":"code","d7ecf76d":"code","f2a34fd8":"code","c55b6a51":"code","9f02f1cc":"code","cd46bb8d":"code","976b9379":"code","b94290fd":"code","9d7525bd":"code","bd100b67":"code","c0829a78":"code","5e906c5d":"code","694ffc41":"code","b13d13c3":"code","eb473dcc":"markdown","73219658":"markdown","c153c7a1":"markdown","7cbfc2d1":"markdown","0f5fd831":"markdown","2fef55bc":"markdown","b500adac":"markdown","37f22557":"markdown","355b92bd":"markdown","81380266":"markdown","50f880dd":"markdown","0c24f84e":"markdown"},"source":{"319f1362":"from pprint import pprint\n!pip install emoji\n\n# Setup\nimport os\nimport re\nimport time\n\nimport emoji\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as ss\nimport sklearn\nfrom keras_preprocessing.sequence import pad_sequences\nfrom keras_preprocessing.text import Tokenizer\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.utils.validation import check_is_fitted\n\n%matplotlib inline\nsns.set_style('white')\nsns.set_context('notebook')\n\nrandom_state = 143","993e6be2":"def load_emotion_data(dataset):\n    \"\"\"\n    Loads the required dataset from the emotions input\n    :param dataset:\n    :return:\n    \"\"\"\n    base_path = '..\/input\/emotion-classification-nlp\/'\n    dataset_name = 'emotion-labels-%s.csv' % dataset\n\n    data = pd.read_csv(os.path.join(base_path, dataset_name))\n    return data['text'].values, data['label'].values\n\ntrain_X, train_y = load_emotion_data('train')\nval_X, val_y = load_emotion_data('val')\ntest_X, test_y = load_emotion_data('test')\n\n\n\nprint(train_X)","c3fb36c4":"# Labels\nsns.displot(train_y, height=5, aspect=2)\nplt.title('Ratio of different emotion labels in text')\nplt.show()\n","d7ecf76d":"vectorizer = CountVectorizer()\nvectorizer.fit(train_X)\n\nvocabulary = pd.DataFrame(vectorizer.vocabulary_.items(), columns=['word', 'count'])\nvocabulary['length'] = vocabulary.apply(lambda x: len(x[0]), axis=1)\n\n\nprint(vocabulary.head())\n\nplt.figure(figsize=(10, 10))\nsns.scatterplot(data=vocabulary, x='length', y='count')\nplt.show()","f2a34fd8":"def test_classifiers(classifiers, X, y, output=True):\n    best_classifier = None\n    best_score = 0\n    for _clf in classifiers:\n        try:\n            clf = sklearn.clone(_clf)\n            start = time.time()\n            scores = cross_val_score(clf, X, y)\n\n            if output:\n                print('Using classifier:', clf)\n                print('\\ttime:', time.time() - start)\n                print('\\tscores:', scores)\n                print('\\tbest:', np.max(scores))\n                print('\\taverage:', np.average(scores))\n\n            max_score = np.average(scores)\n            if max_score > best_score:\n                best_score = max_score\n                best_classifier = clf\n        except Exception as e:\n            print('** FAILED ** classifier', _clf)\n            print(e)\n\n    print('Fitting best classifier')\n    best_classifier.fit(X, y)\n    y_pred = best_classifier.predict(X)\n\n    conf_matrix = confusion_matrix(y, y_pred, normalize='true')\n\n    plt.figure(figsize=(10, 5))\n    sns.heatmap(conf_matrix, annot=True, fmt='.4f')\n    plt.show()\n\n    return best_classifier, best_score\n\n\nclassifiers = [\n    KNeighborsClassifier(),\n    LinearSVC(max_iter=10000, random_state=random_state),\n    RandomForestClassifier(random_state=random_state, n_jobs=8),\n    # MLPClassifier(random_state=random_state), # Takes a long time\n]\nprep_pipeline = Pipeline([\n    ('vectorizer', CountVectorizer())\n])\n\ntrain_X_transformed = prep_pipeline.fit_transform(train_X)\nprint(train_X[0])\nprint(train_X_transformed[0])\n\nbest_clf, best_score = test_classifiers(classifiers, train_X_transformed, train_y)\nprint('Best classifier is:', best_clf, best_score)","c55b6a51":"# Test using Keras tokenizer\ntokenizer = Tokenizer(oov_token='<OOV>')\ntokenizer.fit_on_texts(train_X)\n\ntrain_X_transformed = tokenizer.texts_to_sequences(train_X)\ntrain_X_padded = pad_sequences(train_X_transformed)\nprint(train_X_padded)\n\nprint(train_X[0])\nprint(train_X_transformed[0])\n\nbest_clf, best_score = test_classifiers(classifiers, train_X_padded, train_y)\nprint('Best classifier is:', best_clf, best_score)","9f02f1cc":"class RemoveHandles(TransformerMixin, BaseEstimator):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        values = pd.Series(X)\n        return values.replace(r'@\\w+', '', regex=True).values\n\nremove_handles = RemoveHandles()\nprint(remove_handles.fit_transform(train_X[:10]))","cd46bb8d":"class RemoveCodedValues(TransformerMixin, BaseEstimator):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        values = pd.Series(X)\n        return values.replace(r'&\\w+;', '', regex=True).values\n\nremove_coded = RemoveCodedValues()\n\npat = re.compile(r'&\\w+;')\nfor f in remove_coded.fit_transform(train_X):\n    found = pat.findall(f)\n    if len(found) > 0:\n        print(pat.findall(f))","976b9379":"class TokenizeEmoji(TransformerMixin, BaseEstimator):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return np.array([emoji.demojize(text) for text in X])\n\ntokenize_emoji = TokenizeEmoji()\ntest = tokenize_emoji.fit_transform(train_X[:10])\n\nvectorizer.fit(test)\nprint(vectorizer.vocabulary_)\n","b94290fd":"class Emojifier(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Converts characters like :) :( :\/ to a unique value\n    \"\"\"\n    def __init__(self, emoji_pattern=r'[:;Xx][)(\\\/D]|[)(\\\/D][:;]'):\n        self.emoji_pattern = emoji_pattern\n\n    def fit(self, X, y=None):\n        emoji_list = set()\n        pattern = re.compile(self.emoji_pattern)\n\n        for line in X:\n            emoji_list.update(pattern.findall(line))\n\n\n        self.found_emojis_ = {}\n        for i, emoji in enumerate(emoji_list):\n            self.found_emojis_[emoji] = '<EMOJI_%d>' % i\n\n        return self\n\n    def transform(self, X):\n        # Validate\n        check_is_fitted(self, ['found_emojis_'])\n\n        # Transform\n        data = pd.Series(X)\n        for emoji, name in self.found_emojis_.items():\n            data = data.str.replace(emoji, name, regex=False)\n\n        return data.values\n\nemojifier = Emojifier()\nemojifier.fit(train_X)\nprint(emojifier.found_emojis_)\n\nemojified_X = emojifier.transform(train_X)","9d7525bd":"for i, val in enumerate(train_X[:100]):\n    m = re.findall(r'[:;Xx][)(\\\/D]|[)(\\\/D][:;x]', val)\n    if len(m) > 0:\n        print(set(m))\n        print(i, val, train_y[i])\n        print(i, emojified_X[i])\n","bd100b67":"pipeline = Pipeline([\n    ('handles', RemoveHandles()),\n    ('coded', RemoveCodedValues()),\n    ('emoji', TokenizeEmoji()),\n    ('emojify', Emojifier()),\n    ('counts', CountVectorizer())\n])\n\nprepared_train_X = pipeline.fit_transform(train_X)\n\nbest_clf, score = test_classifiers(classifiers, prepared_train_X, train_y)","c0829a78":"# Try best classifier in the validation dataset\nval_X_transformed = pipeline.transform(val_X)\nval_y_predicted = best_clf.predict(val_X_transformed)\n\naccuracy = accuracy_score(val_y, val_y_predicted)\nprint('Accuracy on validation:', accuracy)\n\nconf_matrix = confusion_matrix(val_y, val_y_predicted, normalize='true')\nconf_matrix = pd.DataFrame(conf_matrix, columns=best_clf.classes_, index=best_clf.classes_)\n# sums = np.sum(conf_matrix, axis=1, keepdims=True)\n\n\nplt.figure(figsize=(10, 5))\nsns.heatmap(conf_matrix, annot=True, fmt='.5f')\nplt.show()","5e906c5d":"# Plot the previous heatmap, but with emphasis in the errors\n\nnp.fill_diagonal(conf_matrix.values, 0)\n\nplt.figure(figsize=(10, 5))\nsns.heatmap(conf_matrix, annot=True, fmt='.4f')\nplt.show()\n\nprint(classification_report(val_y, val_y_predicted))\nprint(best_clf.classes_)","694ffc41":"# Check some examples of errors\n\nerrors = val_y != val_y_predicted\nerrors_to_show = 10\n\nfor text, label, predicted in zip(val_X[errors], val_y[errors], val_y_predicted[errors]):\n    print('%s -> %s\\n\\t%s' % (label, predicted, text))\n\n    errors_to_show -= 1\n    if errors_to_show == 0:\n        break","b13d13c3":"pipeline = Pipeline([\n    ('handles', RemoveHandles()),\n    ('coded', RemoveCodedValues()),\n    ('emoji', TokenizeEmoji()),\n    ('emojify', Emojifier()),\n    ('counts', CountVectorizer()),\n    ('clf', RandomForestClassifier(random_state=random_state))\n])\n\ngrid_params = [{\n    # 'handles': ['passthrough', RemoveHandles()], # Eliminate bias per author or mentioned people\n    'coded': ['passthrough', RemoveCodedValues()],\n    'emoji': ['passthrough', TokenizeEmoji()],\n    'emojify': ['passthrough', Emojifier()],\n    # 'clf__n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 5)],\n    # 'clf__max_features': ['auto', 'sqrt'],\n    'clf__max_depth': [int(x) for x in np.linspace(10, 100, num=4)] + [None,],\n    'clf__bootstrap': [True, False]\n}]\n\ngrid_search = GridSearchCV(pipeline, grid_params, scoring='accuracy',\n                           refit=True, cv=5, n_jobs=16,\n                           verbose=2)\ngrid_search.fit(train_X, train_y)\n\nprint(grid_search.best_score_, grid_search.best_estimator_)","eb473dcc":"## Data Exploration\n\nColumns in the text\n\n| Column name | Description |\n|-|-|\n| text | Piece of text |\n| label | Assigned emotion to the text |","73219658":"## Hyper-parameter tweaking\n\nTry to get the most of the classifiers by tweaking hyperparameters.","c153c7a1":"### Remove URLs or other input","7cbfc2d1":"# Emotion Classification NLP","0f5fd831":"## Baseline\nDo a simple ML approach to see how much accuracy we can get.","2fef55bc":"### Remove coded value","b500adac":"## Test full pipeline with current changes","37f22557":"## Error analysis\n\nLook at the errors that the classifier is doing to get where the problem could be\narising.","355b92bd":"## Frame the problem and look at the big picture\nFrom the information in several texts to determine the **emotion** that each of these convey.\n\n* How will your solution be used?\nIt will be used to categorise other tweets and other trending information regarding a specific\ntopic to determine the emotion behind.\n\n* What are the current solutions\/workarounds (if any)?\nCurrent solutions use natural language processing by creating, or using readily available, embeddings\nof the words. These embeddings are vectors that represent several *dimensions* of the word and allow them\nto be compared with each other. Words with similar embeddings tend to have a similar meaning or used\nin similar contexts. (Similar work)\n\n* How should you frame this problem (supervised\/unsupervised, online\/offline, etc.)\nData is classified, so a supervised algorithm might be better suited. Embeddings can be tested using\nwords within the data or using other databases in english.\n\n* How should performance be measured?\nPerformance is measured in accuracy, though ideally it should be able to output the probability of it\nbeing from several classes (e.g. happy and angry).\n\n## Data loading","81380266":"### Convert emojis to tokens","50f880dd":"## Text tokenizer comparison\n\nThe CountTokenizer creates vector per phrase with n columns, where n is the amount of different words\nfound in the text corpus. On the other hand, Keras Tokenizer converts each sentence to the numbers that\nrepresent the word in the dictionary, maintaining the order of the words.","0c24f84e":"## Preprocessing\n\nWhile it didn't give better results, it would be interesting to check with other classifiers\nor go directly with an RNN. Before that I'll be trying several preprocessing techniques\nto see if filtering the text might help. Among these are:\n- Removing handles (@name) as I don't want the algorithm to associate a person with sentiment.\n- Remove coded values (e.g. &amp;).\n- Convert emojis to tokens (as those are good representation of sentiment yet are filtered by the\ntokenizers).\n- Review if there is other extraneous input like URL's or other\n\n### Remove handles"}}