{"cell_type":{"7fb768a5":"code","da7e9966":"code","3c2b9436":"code","c2a5f96f":"code","b0d3ea1c":"code","5fd6e88b":"code","240cc67e":"code","ad31da89":"code","d352b780":"code","db74e22c":"code","c403b6a7":"code","96863ea1":"code","4279aa21":"code","e37e6921":"code","8a5f28c7":"code","53b07da9":"code","c8b4da09":"code","0f4fa210":"code","ae07310e":"code","d44248b9":"code","951d7819":"markdown","279ec81a":"markdown","a644865d":"markdown","f13a6028":"markdown","04bb5a8b":"markdown","c2a3e8e9":"markdown","203d80ab":"markdown","c86fd208":"markdown","86c64551":"markdown"},"source":{"7fb768a5":"%pylab inline\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# from matplotlib.pyplot import imread\n# import imageio\nfrom matplotlib import image\n\n\nfrom sklearn.metrics import accuracy_score\n\n# import tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, Convolution2D, Flatten, MaxPooling2D, Reshape, InputLayer\nfrom keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.preprocessing.image import load_img","da7e9966":"path = '\/kaggle\/input\/challenges-in-representation-learning-facial-expression-recognition-challenge\/'\nos.listdir(path)","3c2b9436":"data = pd.read_csv(path+'icml_face_data.csv')","c2a5f96f":"data.head()","b0d3ea1c":"def prepare_data(data):\n    \"\"\" Prepare data for modeling \n        input: data frame with labels und pixel data\n        output: image and label array \"\"\"\n    \n    image_array = np.zeros(shape=(len(data), 48, 48))\n    image_label = np.array(list(map(int, data['emotion'])))\n    \n    for i, row in enumerate(data.index):\n        image = np.fromstring(data.loc[row, ' pixels'], dtype=int, sep=' ')\n        image = np.reshape(image, (48, 48))\n        image_array[i] = image\n        \n    return image_array, image_label","5fd6e88b":"train_image_array, train_image_label = prepare_data(data[data[' Usage']=='Training'])\nval_image_array, val_image_label = prepare_data(data[data[' Usage']=='PrivateTest'])\ntest_image_array, test_image_label = prepare_data(data[data[' Usage']=='PublicTest'])","240cc67e":"train_images = train_image_array.reshape((train_image_array.shape[0], 48, 48, 1))\nX_train = train_images.astype('float32')\/255\nval_images = val_image_array.reshape((val_image_array.shape[0], 48, 48, 1))\nX_val = val_images.astype('float32')\/255\ntest_images = test_image_array.reshape((test_image_array.shape[0], 48, 48, 1))\nX_test = test_images.astype('float32')\/255","ad31da89":"y_train = keras.utils.to_categorical(train_image_label)\ny_val = keras.utils.to_categorical(val_image_label)\ny_test = keras.utils.to_categorical(test_image_label)\n#keras.utils.to_categorical(train['label'].values)","d352b780":"pd.DataFrame(y_train)","db74e22c":"# With data augmentation to prevent overfitting\nfrom keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","c403b6a7":"# define vars\ninput_reshape = (48, 48, 1)\n\npool_size = (2, 2)\n\nhidden_num_units = 265\noutput_num_units = 7\n\nepochs = 10\nbatch_size = 128","96863ea1":"%time\n\n\n\nmodel = Sequential([\n\nConvolution2D(75,(2,2), activation='relu',input_shape=input_reshape),\nMaxPooling2D((2,2)),\n\nConvolution2D(50,(2,2), activation='relu'),\nMaxPooling2D((2,2)),\n\nConvolution2D(25,(2,2), activation='relu'),\n\nFlatten(),\n\nDense(hidden_num_units, 'relu'),\n\nDense(output_num_units,'softmax'),\n ])\n","4279aa21":"model.summary()","e37e6921":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\ntrained_model_conv = model.fit(X_train, y_train, epochs =epochs, batch_size=batch_size, validation_data=(X_val, y_val))","8a5f28c7":"plt.plot(trained_model_conv.history['loss'],label='Train Loss')\nplt.plot(trained_model_conv.history['val_loss'],label='Val Loss')\nplt.xlabel('Ephocs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","53b07da9":"model = Sequential()\n#Block-1\nmodel.add(Convolution2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = input_reshape))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Convolution2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Dropout(0.2))\n\n#Block-2\nmodel.add(Convolution2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = input_reshape))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Convolution2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Dropout(0.2))\n\n#Block-3\nmodel.add(Convolution2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = input_reshape))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Convolution2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Dropout(0.2))\n\n#Block-4\nmodel.add(Convolution2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = input_reshape))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Convolution2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Dropout(0.2))\n\n#Block-5\nmodel.add(Flatten())\nmodel.add(Dense(units = 256 , activation = 'relu'))\nmodel.add(Dropout(0.3))\n\n#Block-6\nmodel.add(Dense(units = 7 , activation = 'softmax'))\nmodel.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","c8b4da09":"from keras.optimizers import RMSprop,SGD,Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","0f4fa210":"checkpoint = ModelCheckpoint('EmotionDetectionModel.h5',\n                             monitor='val_loss',\n                             mode='min',\n                             save_best_only=True,\n                             verbose=1)\nearlystop = EarlyStopping(monitor='val_loss',\n                          min_delta=0,\n                          patience=3,\n                          verbose=1,\n                          restore_best_weights=True\n                          )\nreduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                              factor=0.2,\n                              patience=3,\n                              verbose=1,\n                              min_delta=0.0001)\ncallbacks = [earlystop,checkpoint,reduce_lr]","ae07310e":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\ntrained_model_conv = model.fit(X_train, y_train, epochs =20, batch_size=128,callbacks=callbacks, validation_data=(X_val, y_val))","d44248b9":"plt.plot(trained_model_conv.history['loss'],label='Train Loss')\nplt.plot(trained_model_conv.history['val_loss'],label='Val Loss')\nplt.xlabel('Ephocs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","951d7819":"I have added 2 convolutional layers each followed by an activation and then Dropout technique. 20 ephocs ","279ec81a":" Define training, validation and test data:","a644865d":"Before compiling i will create 3 things using keras.callbacks class:\n\n**1-Checkpoint( Function \u2014 ModelCheckpoint() )**\n\nIt will monitor the validation loss and will try to minimize the loss using the mode=\u2019min\u2019 property. When the checkpoint is reached it will save the best trained weights. Verbose=1 is just for visualization when the code created checkpoint.Here i am using it\u2019s following parameters:\n\n**file-path:** Path to save the model file.Here i am saving the model file with the name EmotionDetectionModel.h5\n**monitor:** Quantity to monitor.Here i am monitoring the validation loss.\n\n**mode:** One of {auto, min, max}. If save_best_only=True, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity.\n\n**save_best_only:** If save_best_only=True, the latest best model according to the quantity monitored will not be overwritten.\n**verbose:** int. 0: quiet, 1: update messages.\n\n**2-Early Stopping ( Function \u2014 EarlyStopping() )**\n\nThis will stop the execution early by checking the following properties.\n\n**monitor:** Quantity to monitor.Here i am monitoring the validation loss.\n\n**min_delta:** Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than **min_delta,** will count as no improvement.Here i have given it 0.\n\n**patience:** Number of epochs with no improvement after which training will be stopped. Here i have given it 3.\n\n**restore_best_weights:** Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.Here i have given it True.\n\n**verbose:** int. 0: quiet, 1: update messages.\n\n**3-Reduce Learning Rate ( Function \u2014 ReduceLROnPlateau() )**\n\nModels often benefit from reducing the learning rate by a factor of 2\u201310 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a \u2018patience\u2019 number of epochs, the learning rate is reduced. I have used the following properties for this.\n\n**monitor**: To monitor a particular loss. Here i am monitoring the validation loss.\n\n**factor**: Factor by which the learning rate will be reduced. new_lr = lr * factor. Here i am using 0.2 as factor.\n\n**patience**: Number of epochs with no improvement after which learning rate will be reduced.Here i am using 3.\n\n**min_delta**: Threshold for measuring the new optimum, to only focus on significant changes.\n\n**verbose**: int. 0: quiet, 1: update messages.","f13a6028":"Now we will be using Image Augmentation techniques om our dataset.","04bb5a8b":"# Now Using CNN Model:","c2a3e8e9":"Encoding of the target value:","203d80ab":"# Data Augmentation","c86fd208":"Reshape and scale the images:","86c64551":"The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories:"}}