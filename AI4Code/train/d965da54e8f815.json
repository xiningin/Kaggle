{"cell_type":{"981b7b73":"code","73e37509":"code","53912b29":"code","5d9ab43b":"code","55c1ed17":"code","219e85d4":"code","251bba83":"code","60785817":"code","a927d267":"code","c6e807d8":"code","680c7043":"code","28ff9a53":"code","09b7e546":"code","cb68138b":"code","eaa0d410":"code","1604dd96":"code","ed6a3458":"code","703a05a8":"code","8726271a":"code","d9a75b1a":"code","60b9ed86":"code","d039a6f7":"code","8081dee9":"code","5b196c07":"code","fc7986cf":"code","7deafcc1":"code","f24fbd4f":"code","e04b6b64":"code","4842fe67":"code","32e05a44":"code","9cbaecef":"code","f559e685":"code","02adf340":"code","73004d10":"code","d764d491":"code","61a96d2e":"code","80da078b":"code","e88170bb":"code","bb281b63":"code","238ea1b3":"code","20ee50d8":"code","50596eda":"code","79292977":"code","2987d47c":"code","6b5d80b7":"code","ab59a953":"code","955d1d21":"code","0dd88c67":"code","828cde4e":"code","3c23e10f":"code","5b5f9ed9":"code","f19a1586":"code","c5027abd":"code","9578eeed":"code","3dbf6ed1":"code","7fa7a7c6":"code","6f838da2":"code","4aea07c0":"code","1c8d3f40":"code","9874c540":"code","68230576":"code","6976334c":"code","661576e9":"code","8f350e43":"code","9ff450e6":"code","d83ab3db":"code","f9c51ad3":"code","d30cf586":"code","217ff06b":"code","327a9cb4":"code","99bf0ab7":"code","f8b6edf6":"code","e9e86b0d":"code","c2d11c36":"code","ee55a835":"code","d855ed1f":"code","c2dac3e5":"code","4a3ef26d":"code","e3388d8d":"code","d5a96372":"code","5a853a02":"code","112a62cb":"markdown","26942862":"markdown","2d1bf1b0":"markdown","f3100904":"markdown","e1317e93":"markdown","5d6932ec":"markdown","04bce5b3":"markdown","d49fed8b":"markdown","2ed297db":"markdown","6beb41bb":"markdown","3d914d83":"markdown","927aea95":"markdown","b4e23dbc":"markdown","dfcd8721":"markdown","a6a81b81":"markdown","bbdfa722":"markdown","dccebc85":"markdown","434ea550":"markdown","bd94d15a":"markdown","39e2b84a":"markdown","72330581":"markdown","59ac83c3":"markdown","ac2c592e":"markdown","41e3c57e":"markdown","d2a8208e":"markdown","2d013b20":"markdown","9442ba5e":"markdown"},"source":{"981b7b73":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n","73e37509":"train = pd.read_csv(\"..\/input\/predicting-tweet-sentiments\/train.csv\")\ntrain.head()","53912b29":"train.shape","5d9ab43b":"train.columns","55c1ed17":"train.isnull().sum()","219e85d4":"train.dtypes","251bba83":"train.describe()","60785817":"train.columns","a927d267":"train.head()","c6e807d8":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\n# DOWNLOAD VADER_LEXICON FOR CALCULATING SENTIMENT INTENSITY\nnltk.download('vader_lexicon')\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\nnltk.download('wordnet')","680c7043":"def tweet_to_words(raw_tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_tweet) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words ))","28ff9a53":"#FUNCTION TO FIND POLARITY SCORE\ndef polar_score(text):\n    score = sid.polarity_scores(text)\n    x = score['compound']\n    return x\n","09b7e546":"#CREATING A COLUMN NAMED CLEAN_TEXT WHICH STORES CLEANED TEXT \ntrain['clean_text']=train['original_text'].apply(lambda x: tweet_to_words(x))\n\n#CREATING A COLUMN NAMED COMPOUND SCORE WITH SENTIMENT INTENSITY AS ITS VALUE \ntrain['compound_score'] = train['original_text'].apply(lambda x : polar_score(x))\n\n#CREATING A COLUMN NAMED LENGTH WITH LENGTH OF REVIEWS AS ITS VALUE\ntrain['length'] = train['clean_text'].apply(lambda x: len(x) - x.count(\" \"))","cb68138b":"train.head()","eaa0d410":"train.shape","1604dd96":"# \"LANG\" varibale has lots of irrelevant values\n\ntrain = train[train['lang'] == \"en\"]","ed6a3458":"train.shape","703a05a8":"train.compound_score.plot(kind=\"hist\", color=\"lime\")","8726271a":"train.length.plot(kind=\"hist\", color=\"teal\")","d9a75b1a":"train['retweet_count'].value_counts().plot(kind=\"bar\")","60b9ed86":"train.sentiment_class.value_counts().plot(kind='bar', color=['purple', 'orange', 'lime'])","d039a6f7":"test = pd.read_csv('..\/input\/predicting-tweet-sentiments\/test.csv')\n\ntest.shape","8081dee9":"test.head(5)","5b196c07":"#CREATING A COLUMN NAMED CLEAN_TEXT WHICH STORES CLEANED TEXT \ntest['clean_text']=test['original_text'].apply(lambda x: tweet_to_words(x))\n\n#CREATING A COLUMN NAMED COMPOUND SCORE WITH SENTIMENT INTENSITY AS ITS VALUE \ntest['compound_score'] = test['original_text'].apply(lambda x : polar_score(x))\n\n#CREATING A COLUMN NAMED LENGTH WITH LENGTH OF REVIEWS AS ITS VALUE\ntest['length'] = test['clean_text'].apply(lambda x: len(x) - x.count(\" \"))","fc7986cf":"test.head()","7deafcc1":"# train = 2994\n# test  = 1387","f24fbd4f":"common_df = pd.DataFrame(pd.concat([train[\"clean_text\"], test[\"clean_text\"]])).reset_index(drop=True)","e04b6b64":"common_df = pd.concat([train[\"clean_text\"], test[\"clean_text\"]]).reset_index(drop=True)","4842fe67":"common_df.head()","32e05a44":"type(common_df)","9cbaecef":"len(common_df)","f559e685":"common_df[0]","02adf340":"documents = []\n\nfrom nltk.stem import WordNetLemmatizer\n\nstemmer = WordNetLemmatizer()\n\nfor sen in range(0, len(common_df)):\n    # Remove all the special characters\n    document = re.sub(r'\\W', ' ', str(common_df[sen]))\n    \n    # remove all single characters\n    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n    \n    # Remove single characters from the start\n    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n    \n    # Substituting multiple spaces with single space\n    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n    \n    # Removing prefixed 'b'\n    document = re.sub(r'^b\\s+', '', document)\n    \n    # Converting to Lowercase\n    document = document.lower()\n    \n    # Lemmatization\n    document = document.split()\n\n    document = [stemmer.lemmatize(word) for word in document]\n    document = ' '.join(document)\n    \n    documents.append(document)","73004d10":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features=500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\nX = vectorizer.fit_transform(documents).toarray()","d764d491":"X","61a96d2e":"from sklearn.feature_extraction.text import TfidfTransformer\ntfidfconverter = TfidfTransformer()\nX = tfidfconverter.fit_transform(X).toarray()","80da078b":"X","e88170bb":"x_df = pd.DataFrame(X)","bb281b63":"for col in x_df.columns:\n    new_col = \"Column_\" + str(col)\n    #print(new_col)\n    x_df.rename(columns={col:new_col}, inplace=True)","238ea1b3":"x_df.shape","20ee50d8":"train_x_df = x_df[:2994]\ntest_x_df = x_df[2994:]","50596eda":"test_x_df.reset_index(drop=True, inplace=True)","79292977":"train_new = pd.concat([train, train_x_df], axis=1)\ntest_new = pd.concat([test, test_x_df], axis=1, ignore_index=True)","2987d47c":"train_new.columns","6b5d80b7":"train_new.reset_index(drop=True, inplace=True)\ntest_new.reset_index(drop=True, inplace=True)","ab59a953":"x = train_new.drop(['id', 'original_text', 'lang', 'original_author', 'sentiment_class', 'clean_text'], axis=1)\ny = train_new['sentiment_class']","955d1d21":"x.shape","0dd88c67":"x.fillna(x.median(), inplace=True)","828cde4e":"for col in x.columns:\n    x[col] = x[col].apply(lambda x: x.median() if x == np.inf or x == -np.inf else x)","3c23e10f":"x[\"length\"].dtype","5b5f9ed9":"x.columns[1:]","f19a1586":"for col in x.columns[1:]:\n    x[col] = x[col].apply(lambda x: float(\"{:.2f}\".format(x)) )","c5027abd":"for col in x.columns:\n    x[col] = x[col].astype(np.float64)","9578eeed":"x.reset_index(drop=True, inplace=True)","3dbf6ed1":"for col in x.columns:\n    x[col] = x[col].apply(lambda x: x.median() if x == \"\" else x)","7fa7a7c6":"x","6f838da2":"cols_with_missing = [col for col in x.columns \n                                 if x[col].isnull().any()]\n","4aea07c0":"y","1c8d3f40":"y.fillna(0.0, inplace=True)","9874c540":"y.isnull().any()","68230576":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100)","6976334c":"print(\"X_train: \", X_train.shape) \nprint(\"X_test: \", X_test.shape)\nprint(\"y_train: \", y_train.shape) \nprint(\"y_test: \", y_test.shape)","661576e9":"X_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\ny_test.reset_index(drop=True, inplace=True)","8f350e43":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n# feature extraction\nmodel = LogisticRegression(solver='lbfgs')\nrfe = RFE(model, 3)\nfit = rfe.fit(x, y)\n\nprint(\"Num Features: %d\" % fit.n_features_)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)\n\n\ndf_feat = pd.DataFrame(fit.ranking_, x.columns)\ndf_feat.rename(columns = {0:\"Feature_Ranking\"}, inplace=True)\n","9ff450e6":"df_feat[:15].sort_values(by=\"Feature_Ranking\").plot(kind='bar', figsize=(18,7))","d83ab3db":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, accuracy_score, roc_auc_score","f9c51ad3":"from sklearn.tree import DecisionTreeClassifier\n\n#making the instance\nmodel= DecisionTreeClassifier(random_state=1234)\n\n#Hyper Parameters Set\nparam_grid = {'max_features': ['auto', 'sqrt', 'log2'],\n          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n          'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11],\n          'random_state':[123]}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_dt = clf.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_dt.predict(X_test)\n\nprint(\"*******************ACCURACY***************************************************************\")\n#Check Prediction Score\nprint(\"Accuracy of Decision Trees: \",accuracy_score(y_test, predictions))\n\nprint(\"*******************CLASSIFICATION - REPORT***************************************************************\")\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n\n\n","d30cf586":"# Fit RF on full data\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n#making the instance\nmodel= RandomForestClassifier(random_state=1234)\n\n#Hyper Parameters Set\nparam_grid = {'criterion':['gini','entropy'],\n          'n_estimators':[10,15,20,25,30],\n          'min_samples_leaf':[1,2,3],\n          'min_samples_split':[3,4,5,6,7], \n          'random_state':[123],\n          'n_jobs':[-1]}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_rf = clf.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_rf.predict(X_test)\n\n#Check Prediction Score\nprint(\"Accuracy of Random Forest: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n\n","217ff06b":"# Fit RF on full data\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n#making the instance\nmodel= RandomForestClassifier(random_state=1234)\n\n#Hyper Parameters Set\nparam_grid = {'criterion':['gini','entropy'],\n          'n_estimators':[10,15,20,25,30],\n          'min_samples_leaf':[1,2,3],\n          'min_samples_split':[3,4,5,6,7], \n          'random_state':[123],\n          'n_jobs':[-1]}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_rf = clf.fit(x, y)\n","327a9cb4":"test_new = pd.concat([test, test_x_df], axis=1)","99bf0ab7":"test_new.shape","f8b6edf6":"test_new.dtypes","e9e86b0d":"import re","c2d11c36":"def check_integer(x):\n    try:\n        num = int(x) \n        return x\n    except: \n        return 0","ee55a835":"test_new[\"retweet_count\"] = test_new[\"retweet_count\"].apply(lambda x: check_integer(x))","d855ed1f":"test_for_prediction = test_new.drop(['id', 'original_text', 'lang', 'original_author', 'clean_text'], axis=1)","c2dac3e5":"for col in test_for_prediction.columns:\n    test_for_prediction[col] = test_for_prediction[col].astype(np.float64)","4a3ef26d":"test_for_prediction.fillna(test_for_prediction.median(), inplace=True)\n\nfor col in test_for_prediction.columns:\n    test_for_prediction[col] = test_for_prediction[col].apply(lambda x: x.median() if x == np.inf or x == -np.inf else x)\n\nfor col in test_for_prediction.columns[1:]:\n    test_for_prediction[col] = test_for_prediction[col].apply(lambda x: float(\"{:.2f}\".format(x)))\n\ntest_for_prediction.reset_index(drop=True, inplace=True)\n\n","e3388d8d":"test_for_prediction.head()","d5a96372":"#Predict\n\nprediction_from_dt  = best_clf_dt.predict(test_for_prediction)\ndf_prediction_from_dt = pd.DataFrame({'id':test_new['id'], 'sentiment_class': prediction_from_dt})\ndf_prediction_from_dt.to_csv(\"Final_output_prediction_from_dt.csv\")\ndf_prediction_from_dt.head()\n","5a853a02":"prediction_from_rf  = best_clf_rf.predict(test_for_prediction)\ndf_prediction_from_rf = pd.DataFrame({'id':test_new['id'], 'sentiment_class': prediction_from_rf})\ndf_prediction_from_rf.to_csv(\"Final_output_prediction_from_rf.csv\")\ndf_prediction_from_rf.head()","112a62cb":"<table>\t\t\t\t\t\t\n<tr>\t<th>\tColumn Name\t<\/th> <th>\tDescription\t<\/th>\t<\/tr>\n<tr>\t<td>\tid\t<\/td> <td>\tID of tweet\t<\/td>\t<\/tr>\n<tr>\t<td>\toriginal_text\t<\/td> <td>\tText of tweet\t<\/td>\t<\/tr>\n<tr>\t<td>\tlang\t<\/td> <td>\tLanguage of tweet\t<\/td>\t<\/tr>\n<tr>\t<td>\tretweet_count\t<\/td> <td>\tNumber of times retweeted\t<\/td>\t<\/tr>\n<tr>\t<td>\toriginal_author\t<\/td> <td>\tTwitter handle of Author\t<\/td>\t<\/tr>\n<tr>\t<td>\tsentiment_class\t<\/td> <td>\tSentiment of Tweet (Target)\t<\/td>\t<\/tr>\n<\/table>\t\t\t\t\t\t","26942862":"Once the dataset has been imported, the next step is to preprocess the text. Text may contain numbers, special characters, and unwanted spaces. Depending upon the problem we face, we may or may not need to remove these special characters and numbers from text. However, for the sake of explanation, we will remove all the special characters, numbers, and unwanted spaces from our text","2d1bf1b0":"## 6.2 Split Data","f3100904":"# Phase1: Model Building On Training Data","e1317e93":"# Step5: Creating a common vocabulary for TRAIN and TEST data","5d6932ec":"# Step1: Read Data","04bce5b3":"The following script uses the bag of words model to convert text documents into corresponding numerical features","d49fed8b":"## 3.1 Data Visualization","2ed297db":"You work in an event management company. On Mother's Day, your company has organized an event where they want to cast positive Mother's Day related tweets in a presentation. Data engineers have already collected the data related to Mother's Day that must be categorized into positive, negative, and neutral tweets.\n\nYou are appointed as a Machine Learning Engineer for this project. Your task is to build a model that helps the company classify these sentiments of the tweets into positive, negative, and neutral.\n\n<b> Target Variable: sentiment_class <b>","6beb41bb":"# Problem Statement","3d914d83":"# Step6: Model Building","927aea95":"# Step4: Import Test Data","b4e23dbc":"To convert values obtained using the bag of words model into TFIDF values, execute the following script","dfcd8721":"## 6.1 Re-setting Index Before Splitting","a6a81b81":"# Step2: Exploratory Data Analysis","bbdfa722":"# Step3: Data Cleaning","dccebc85":"## 6.2 Importing and Model Fitting","434ea550":"# Step7: Creating Train and Test Set In Ratio 80:20","bd94d15a":"## 2.2 Data Type Analysis ","39e2b84a":"## 6.1 Identification Of Best Features","72330581":"# Final Step: Generating prediction on Test data","59ac83c3":"## 2.1 Missing Data Analysis ","ac2c592e":"# Step6: Separating X and Y","41e3c57e":"### 6.2.2 Random Forest","d2a8208e":"### 6.2.1 Decision Trees","2d013b20":"## 6.3 Fill Data for NaNs or Infinity","9442ba5e":"## 2.3 Univariate Analysis\n\nAt this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable type is categorical or continuous. Let\u2019s look at these methods and statistical measures for categorical and continuous variables individually:\n\n<b> Continuous Variables:- <\/b> In case of continuous variables, we need to understand the central tendency and spread of the variable. These are measured using various statistical metrics such as Histogram and Bar plots: "}}