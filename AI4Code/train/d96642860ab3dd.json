{"cell_type":{"d3576870":"code","a4ec4a3b":"code","2700279a":"code","b5818480":"code","b7dda2c4":"code","f622e8e8":"code","dcbca85f":"code","15928e47":"code","ed10d9dc":"code","21b49d40":"code","a693d596":"code","3adbdcb7":"code","e55d493c":"code","fb8b8ef0":"code","a264afbd":"code","d0b3fa18":"code","4b96fb3c":"code","89f6a527":"code","34da9e2b":"code","34e19632":"code","356d60f6":"code","7853ff33":"code","8d53a548":"code","638bbf87":"code","87afaa44":"code","85abbd9a":"code","25cafd42":"code","5ad45726":"code","1a26b551":"code","eabe457c":"code","2ee0b588":"code","5ce3a849":"code","01391062":"code","e32594f7":"code","5c517f16":"code","c22823f6":"code","1faf0017":"code","d1366c3f":"code","52b5ab99":"code","573fe88c":"code","081beb47":"code","e4bd684d":"code","aa3a98c3":"code","182f17de":"code","c22905b0":"code","96158142":"code","91b2250f":"code","1bc9d966":"code","7ad19549":"code","a9c78c05":"code","6e8b72e7":"code","d06d2d25":"code","165ddafe":"code","04c98e1b":"code","0249e7ba":"markdown","ab8d680d":"markdown","d5283737":"markdown","a9617330":"markdown","44ee7a66":"markdown","10148c3f":"markdown","71faf57d":"markdown","2ad7641b":"markdown","7b335632":"markdown","961fc457":"markdown","4caa6e02":"markdown","2766c8d4":"markdown","bbfd1f9d":"markdown","d00530c0":"markdown","0d7993ca":"markdown","3dabeb68":"markdown","da7d03aa":"markdown","6421c979":"markdown","7fb7431d":"markdown","849bb700":"markdown","4c7f3d0d":"markdown","6e2429b8":"markdown","c4fe491c":"markdown","68bae819":"markdown","38d27ad7":"markdown","d8308ef6":"markdown","86782fe4":"markdown","166f4342":"markdown","103fb5ca":"markdown","4056b799":"markdown","6e11f238":"markdown","c1b9e848":"markdown"},"source":{"d3576870":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4ec4a3b":"import matplotlib.pyplot as plt\nimport seaborn as sns\n## Display all the columns of the dataframe\npd.pandas.set_option('display.max_columns',None)","2700279a":"\"\"\"Machine learning models.\"\"\"\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n\"\"\"Classification (evaluation) metrices.\"\"\"\nfrom sklearn.metrics import accuracy_score, r2_score, f1_score,  classification_report \nfrom sklearn.model_selection import cross_val_score, GridSearchCV,RandomizedSearchCV, cross_val_predict","b5818480":"data_train=pd.read_csv(\"..\/input\/titanic\/train.csv\")\nprint(data_train.shape)\ndata_train.head(2)","b7dda2c4":"display(data_train.isnull().sum())","f622e8e8":"sns.countplot(\"Survived\", data=data_train)\nplt.ylabel(\"Total no of people\",size=10)\nplt.title(\"Survived vs Dead\")\nplt.show()","dcbca85f":"plt.figure(figsize=(15,5))\n\nplt.subplot(121)\nsns.countplot(\"Sex\", data=data_train)\nplt.title(\"male vs female \")\nplt.ylabel(\"Total no of people\",size=10)\n\nplt.subplot(122)\nsns.countplot(\"Sex\", data=data_train, hue=\"Survived\")\nplt.title(\"male vs female survived and death\")\nplt.ylabel(\"no of people\")\nplt.show()","15928e47":"plt.figure(figsize=(15,5))\n\nplt.subplot(121)\nsns.countplot(\"Pclass\", data=data_train)\nplt.ylabel(\"Total no of people\",size=10)\nplt.title(\"Pclass Data\")\nplt.xticks(rotation=90)\n\nplt.subplot(122)\nsns.countplot(\"Pclass\", data=data_train, hue=\"Survived\")\nplt.title(\"Pclass survived vs Dead\")\nplt.ylabel(\"no of people\")\nplt.show()","ed10d9dc":"fig, ax = plt.subplots(1,2, figsize=(15,5) )\n\nsns.countplot(\"Embarked\", data=data_train, ax=ax[0])\nax[0].set_title('Embarked ', fontsize=16)\nax[0].set_ylabel('No. of People count ', fontsize=10)\n\nsns.countplot(\"Embarked\", data=data_train, hue=\"Survived\")\nplt.title(\"Embarked survived vs Dead\")\n\nplt.show()","21b49d40":"data_train['Cabin']=data_train['Cabin'].fillna('Missing').apply(lambda x: x[0] if x!='Missing' else 'Missing')#.value_counts()","a693d596":"sns.countplot(\"Cabin\", data=data_train, hue=\"Survived\")\nplt.show()","3adbdcb7":"data_train[\"familySize\"] = data_train[\"SibSp\"]+data_train[\"Parch\"]+1# Adding 1 for single person\n#then delete [ SibSp, Parch ] \ndata_train.drop(['SibSp', 'Parch'], inplace=True, axis=1)\n\n#create 4 buckets namely single, small, medium, and large for rest of them.\ndata_train[\"familySize\"] = data_train[\"familySize\"].apply(lambda x: 'single' if x==1 else \n                                                    ('small' if (x==2 or x==3) else\n                                                    ('medium' if (x==4 or x==5) else 'large' )))\n","e55d493c":"sns.countplot(\"familySize\", data=data_train, hue=\"Survived\")\nplt.show()","fb8b8ef0":"#lets take only calling name \n#Mr ,Miss , Mrs ,Master ----------etc\ndata_train['Name']=data_train['Name'].apply(lambda x: (x.split('.')[0]).split(',')[1] )\n\n# Professionals like Dr, Rev, Col, Major, Capt will be put into 'Officer'\n# Dona, Jonkheer, Countess, Sir, Lady, Don will be put into aristocrats\n# Mlle and Ms with Miss and Mme by Mrs as these are French titles.\ndata_train[\"Name\"] = data_train[\"Name\"].replace(to_replace=[\"Dr\", \"Rev\", \"Col\", \"Major\", \"Capt\"], value = \"Officer\", regex=True )\ndata_train[\"Name\"].replace(to_replace=[\"Dona\", \"Jonkheer\", \"the Countess\", \"Sir\", \"Lady\", \"Don\"], value = \"Aristocrat\", inplace = True,regex=True)\ndata_train[\"Name\"].replace({\"Mlle\":\"Miss\", \"Ms\":\"Miss\", \"Mme\":\"Mrs\"}, inplace = True,regex=True)\ndata_train['Name'].value_counts()","a264afbd":"f, ax = plt.subplots(1,2, figsize=(15,5))\n\ndata_train['Name'].value_counts().plot.pie( autopct='%1.2f%%', ax=ax[0], shadow=True, startangle=180)\nax[0].set_title('Pie Chart Plot of Name', fontsize=16)\nax[0].legend(loc='lower left')\n\n\nsns.countplot('Name', data=data_train, hue=\"Survived\" )\n\nplt.show()","d0b3fa18":"#Assign 'N' if there is only digits in Ticket. Otherwise just get the 1st character from Ticket.\ndata_train['Ticket']=np.where(data_train['Ticket'].str.isdigit(), \"N\", data_train['Ticket'].str[0] )","4b96fb3c":"sns.countplot(\"Ticket\", data=data_train, hue=\"Survived\")\nplt.show()","89f6a527":"plt.figure(figsize=(15,5))\n\nplt.subplot(121)\nbar=sns.distplot(data_train.Fare)\nbar.legend(['Fare skewness :: {:.2f}'.format(data_train.Fare.skew())])\n\nplt.subplot(122)\nbar=sns.distplot(data_train.Age)\nbar.legend(['Age skewness :: {:.2f}'.format(data_train.Age.skew())])\nplt.xticks(range(0,int(np.max(data_train.Age))+10,10))\n\nplt.show()","34da9e2b":"plt.figure(figsize=(15,5))\n\nplt.subplot(121)\nsns.boxplot('Fare',data=data_train)\nplt.title('outlier')\n\nplt.subplot(122)\nsns.boxplot(np.log(data_train['Fare']))\nplt.title('with log distribution')\n\nplt.show()","34e19632":"train=pd.read_csv('..\/input\/titanic\/train.csv')\ntest=pd.read_csv('..\/input\/titanic\/test.csv')\nsubmis=pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\n\n#display shape of datasets\nprint(train.shape)\nprint(test.shape)\nprint(submis.shape)","356d60f6":"# store Survived values from train to survived\n# then drop it and display first 2 rows\nsurvived=train['Survived']\ntrain.drop('Survived', axis=1, inplace=True)\ndisplay(train.head(2))","7853ff33":"##concatinate test and train for data pre-processing\n# display 5 rows \ndataset=pd.concat([train,test],axis=0)\ndataset=dataset.reset_index(drop=True)\n#shape\ndisplay(dataset.shape)\n#check null\ndisplay(dataset.isnull().sum())\ndataset.head()","8d53a548":"## Here we will check the percentage of nan values present in each feature\n## 1 -step make the list of features which has missing values\nfeatures_with_na=[features for features in dataset.columns if dataset[features].isna().sum()>=1]\n\n## 2- step print the feature name and the percentage of missing values\nfor feature in features_with_na:\n    print(feature, np.round(dataset[feature].isnull().mean(), 4),  ' % missing values')\n    \nprint(f\"The total feature which have missing values are :: {len(features_with_na)}\")","638bbf87":"Age_median_value=dataset['Age'].median() \ndataset['Age'].fillna(Age_median_value,inplace=True)","87afaa44":"# As fare also contain zero (0) values,  which is not suitable values for fare , that must be NaN values\ndataset['Fare'].replace(to_replace=0, value=np.nan, inplace = True,regex=True)","85abbd9a":"# ## replace Fare by using median as it have outlier\nFare_median_value=int(dataset['Fare'].median())    \ndataset['Fare'].fillna(Fare_median_value,inplace=True)","25cafd42":"#lets fill nan value with mode of Embarked column\nmode_value=dataset['Embarked'].mode()[0]    \ndataset['Embarked'].fillna(mode_value,inplace=True)","5ad45726":"# #let take first char for cabin feature and escape for NaN\ndataset['Cabin']=dataset['Cabin'].apply(lambda x: x[0] if type(x)==str else x )#.value_counts()","1a26b551":"# lets fill nan with ffil and then bfill\ndataset['Cabin'].fillna(method='ffill', inplace=True)\ndataset['Cabin'].fillna(method='bfill', inplace=True)","eabe457c":"# Again check for NaN\ndataset[features_with_na].isnull().sum()","2ee0b588":"dataset[\"familySize_num\"] = dataset[\"SibSp\"]+dataset[\"Parch\"]+1# Adding 1 for single person\n#then delete [ SibSp, Parch ] \ndataset.drop(['SibSp', 'Parch'], inplace=True, axis=1)\n\n#create 4 buckets namely single, small, medium, and large for rest of them.\ndataset[\"familySize\"] = dataset[\"familySize_num\"].apply(lambda x: 'single' if x==1 else \n                                                    ('small' if (x==2 or x==3) else\n                                                    ('medium' if (x==4 or x==5) else 'large' )))\n","5ce3a849":"#lets take only calling name \n#Mr ,Miss , Mrs ,Master ----------etc\ndataset['Name']=dataset['Name'].apply(lambda x: (x.split('.')[0]).split(',')[1] )\n\n# Professionals like Dr, Rev, Col, Major, Capt will be put into 'Officer'\n# Dona, Jonkheer, Countess, Sir, Lady, Don will be put into aristocrats\n# Mlle and Ms with Miss and Mme by Mrs as these are French titles.\ndataset[\"Name\"] = dataset[\"Name\"].replace(to_replace=[\"Dr\", \"Rev\", \"Col\", \"Major\", \"Capt\"], value = \"Officer\", regex=True )\ndataset[\"Name\"].replace(to_replace=[\"Dona\", \"Jonkheer\", \"the Countess\", \"Sir\", \"Lady\", \"Don\"], value = \"Aristocrat\", inplace = True,regex=True)\ndataset[\"Name\"].replace({\"Mlle\":\"Miss\", \"Ms\":\"Miss\", \"Mme\":\"Mrs\"}, inplace = True,regex=True)","01391062":"plt.figure(figsize=(15,5))\n\nplt.subplot(121)\nbar=sns.distplot(dataset.Fare)\nbar.legend(['Fare skewness :: {:.2f}'.format(dataset.Fare.skew())])\n\nplt.subplot(122)\nbar=sns.distplot(dataset.Age)\nbar.legend(['Age skewness :: {:.2f}'.format(dataset.Age.skew())])\nplt.xticks(range(0,int(np.max(dataset.Age))+10,10))\n\nplt.show()","e32594f7":"dataset.Fare=np.log(dataset.Fare)\n\nplt.figure(figsize=(10,5))\nbar=sns.distplot(dataset.Fare)\nbar.legend(['Fare skewness :: {:.2f}'.format(dataset.Fare.skew())])\n\nplt.show()","5c517f16":"#lets drop PassengerId and Ticket\ndataset.drop(['PassengerId','Ticket'],axis=1,inplace=True)","c22823f6":"dataset.dtypes","1faf0017":"dataset['Pclass']=dataset['Pclass'].astype('category') ","d1366c3f":"dataset.head()","52b5ab99":"# use one hot encoding instead of label encoding because algorithm might give weights to higher values if label encoding is \n#used to encode numeric variables.\ndataset1=pd.get_dummies(dataset)\ndisplay(dataset1.shape)\ndataset1.head()","573fe88c":"scal_fet=['Age','Fare','familySize_num']","081beb47":"from sklearn.preprocessing import MinMaxScaler\n\nscale=MinMaxScaler(feature_range=(0,1))\n\ndataset1[scal_fet]=scale.fit_transform(dataset1[scal_fet])","e4bd684d":"dataset1.head()","aa3a98c3":"#Let's split the train and test set \ndf_train=dataset1.iloc[:891,:]\ndf_test=dataset1.iloc[891:,:]\n\nX_train=df_train\nX_test=df_test\n\ny_train=survived\n\nprint(f\"Train Data Dimension: {X_train.shape}\")\nprint(f\"Train output: {y_train.shape}\")\nprint(f\"Test Data Dimension: {X_test.shape}\")\n","182f17de":"\"\"\"The  different  model we will try are::\"\"\"\n# 1. Logistic Regression\nlr = LogisticRegression()\n# 2. DecisionTreeClassifier\ndtree=DecisionTreeClassifier()\n# 3. Random Forest Classifier\nrand_forest = RandomForestClassifier()\n# 4. KNN\nknn=KNeighborsClassifier()\n# 5. Support Vector Machines [ SV Classifier ] \nsvc=SVC()\n# 6. Gaussian Naive Bayes\ngau_nb = GaussianNB()\n# 7. Multinomial Naive Bayes\nmult_nb=MultinomialNB()\n# 8. Gradient Boosting Classifier\ngbc = GradientBoostingClassifier()\n# 9. Adaboost Classifier\nabc = AdaBoostClassifier()\n# 10. ExtraTrees Classifier\netc = ExtraTreesClassifier()\n# 11. Extreme Gradient Boosting\nxgbc = XGBClassifier()\n\n#list of model defined above \nmodels = [lr, dtree, rand_forest, knn, svc, gau_nb, mult_nb, gbc,abc, etc, xgbc]\nmodelNames=['LogisticRegression','DecisionTreeClassifier',\"RandomForestClassifier\",'KNeighborsClassifier',\n          'SupportVectorClassifier','GaussianNB','MultinomialNB','GradientBoostingClassifier','AdaBoostClassifier',\n          'ExtraTreesClassifier','XGBClassifier']","c22905b0":"\"\"\"Create a function that returns train accuracy of different models.\"\"\"\ndef calculateTrainAccuracy(model):    \n    model.fit(X_train,y_train)\n    trainAccuracy = model.score(X_train,y_train)\n    trainAccuracy = round(trainAccuracy*100, 2)\n    return trainAccuracy\n\n# Calculate train accuracy of all the models and store them in a dataframe\ntrainModelScores = list(map(calculateTrainAccuracy, models))\ntrainAccuracy_df = pd.DataFrame(trainModelScores, columns = [\"trainAccuracy\"], index=modelNames)\ntrainAccuracy_df=trainAccuracy_df.sort_values(by=\"trainAccuracy\", ascending=False)\ntrainAccuracy_df","96158142":"\"\"\"Create a function that returns mean cross validation score for different models.\"\"\"\ndef calculateXValScore(model):    \n    xValScore = cross_val_score(model,\n                                X_train, y_train,\n                                cv = 10, n_jobs =4,\n                                scoring=\"accuracy\"\n                               ).mean()\n    xValScore = round(xValScore*100, 2)\n    return xValScore\n\n# Calculate cross validation scores of all the models and store them in a dataframe\nCVScores = list(map(calculateXValScore, models))\nxValScores_df = pd.DataFrame(CVScores, columns = [\"Xcross_val_score\"], index=modelNames)\nxValScores_df_sort = xValScores_df.sort_values(by=\"Xcross_val_score\", ascending=False)\n\nprint(\"---------------Model Evaluation cross_val_score ---------------\")\nxValScores_df_sort","91b2250f":"\"\"\"Define all the models hyperparameters one by one first::\"\"\"\n\n# 1. Logistic Regression\nlrParams = {\"penalty\":[\"l1\", \"l2\"],\n            \"C\": np.logspace(0, 4, 10),\n            \"max_iter\":[100,500,1000,5000]}\n\n# 2. DecisionTreeClassifier\ndtreeParams = {'criterion':['gini','entropy'],\n               \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n               \"min_samples_split\": [ 2,  4,  6,  8, 10, 12, 14], \n               \"min_samples_leaf\":[ 1,  3,  5,  7,  9, 11],#np.arange(1,12,2)\n               \"random_state\":[40,43,None]}\n\n# 3. Random Forest Classifier\nrand_forestParams = {\"criterion\":[\"gini\",\"entropy\"],\n                     \"n_estimators\":[80,100, 300, 500, 800,1100,1500],\n                     \"min_samples_leaf\":[ 2,3,4,6,8],\n                     \"min_samples_split\":[2,3,5,7,9], #np.arange(1,12,2)\n                     \"max_features\":[\"sqrt\", \"auto\", \"log2\"],\n                     \"random_state\":[40,43,44,None]}\n\n# 4. KNN\nknnParams = {\"n_neighbors\":[3,5,7,11],#np.arange(3,9)\n             \"leaf_size\":[2,25,30,35,40],\n             \"weights\":[\"uniform\", \"distance\"],\n             \"algorithm\":[\"auto\", \"ball_tree\",\"kd_tree\",\"brute\"]}\n\n# 5. Support Vector Machines [ SV Classifier ] \nsvcParams = {\"C\": [0.1,1,5,10,100],\n             \"kernel\": [\"linear\",\"rbf\"],\n             \"gamma\": [5 ,1 ,0.1,0.5, 0.001,'scale','auto'],\n             'random_state':[40,43,45,None] \n            }\n\n# 6. Gaussian Naive Bayes\ngau_nbParams = {\"priors\":[0.5,0.8,1,None],\n               \"var_smoothing\":[1e-09,1e-06,0.01]\n                }\n\n# 7. Multinomial Naive Bayes\nmult_nbParams={\"alpha\":[0.1,1,10],\n               \"fit_prior\":[True,False],\n               'class_prior': [0.5,0.8,1,None]\n                }\n\n# 8. Gradient Boosting Classifier\ngbcParams = {'n_estimators':[100,200,300,500,800],\n             \"learning_rate\": [5,1, 0.1, 0.01,0.02,0.05,0.001],\n             \"max_depth\": [4, 6, 8],\n             \"max_features\": [1,0.8,0.5, 0.3, 0.1], \n             \"min_samples_split\": [ 2, 3, 4,5],\n             \"random_state\":[40,43,45,None]}\n\n# 9. Adaboost Classifier\nabcParams = {\"n_estimators\":[50,70,100,200,300,500],\n             \"learning_rate\":[ 0.02,0.01,0.2,0.1,1,5,10],\n             \"random_state\":[40,43,45,None]}\n\n# 10. ExtraTrees Classifier\netcParams = {\"n_estimators\":[10,20,30,50,100],\n             \"criterion\":[\"gini\",\"entropy\"],\n             \"min_samples_split\":[2, 3, 4],\n             \"min_samples_leaf\":[1, 2,3 ],\n             \"max_depth\":[2,3,5,10,None],\n             \"max_features\":[3, 10,\"auto\"],\n             \"random_state\":[40,43,45,None]}\n\n# 11. Extreme Gradient Boosting\nxgbcParams = {\"n_estimators\": [100, 250, 400, 550,700, 850, 1000,1200],\n              \"learning_rate\": [0.01,0.1,0.5,0.300000012],\n              'booster':[ 'gbtree', 'gblinear'],\n              \"max_depth\": [ 3,4, 6, 10, 15],## np.arange(3,15,2)\n              \"min_child_weight\": [1, 2, 3],\n              \"subsample\": [0.6,0.7,0.8,0.9],\n              \"colsample_bytree\": (0.5,0.7, 0.9,1),\n              'base_score':[0.25,0.5,0.75,1],\n              \"random_state\":[40,43,45,None]}\n\n# list of Paramaters defined\nparametersLists = [lrParams,dtreeParams,rand_forestParams,knnParams, svcParams,gau_nbParams,mult_nbParams,\n                   gbcParams, abcParams, etcParams,xgbcParams]","1bc9d966":"\"\"\"Create a function to tune hyperparameters of the selected models.\"\"\"\ndef GSCVtuneHyperParam(model, params):\n    # Construct grid search object with 10 fold cross validation.\n    gridSearch = GridSearchCV(estimator=model, param_grid=params, \n                              verbose=0, cv=10, n_jobs =4, \n                              return_train_score=False,)\n    # Fit using grid search.\n    gridSearch.fit(X_train,y_train)\n    bestParams, bestScore = gridSearch.best_params_, round(gridSearch.best_score_*100, 2)\n    return bestScore, bestParams\n\ndef RSCVtuneHyperparam(model, params):\n    randSearch = RandomizedSearchCV(estimator=model,\n                                    param_distributions=params, \n                                    verbose=0, cv=10,\n                                    n_jobs =4, n_iter=30,\n                                    return_train_score=False,)\n    randSearch.fit(X_train,y_train)\n    bestParams, bestScore = randSearch.best_params_, round(randSearch.best_score_*100, 2)\n    model=randSearch.best_estimator_#redefine the model with best parameters\n    return bestScore, bestParams","7ad19549":"# ## #Perform RandomizedSearchCV \nbestScore=[]\nbestParam=[]\nfor model,param in zip(models,parametersLists):\n    Score,Param=RSCVtuneHyperparam(model,param)\n    bestScore.append(Score)\n    bestParam.append(Param)\n    \n\"\"\" create a dataframe to store best score and best params.\"\"\"\nrsCV_df=pd.DataFrame()\n# rsCV_df['modelNames']=modelNames\nrsCV_df['bestScore']=bestScore\nrsCV_df['bestParam']=bestParam\nrsCV_df['Xcross_val_score']=xValScores_df.iloc[:,0].values\nrsCV_df.index=modelNames\nrsCV_df = rsCV_df.sort_values(by=\"bestScore\", ascending=False)\n# rsCV_df=rsCV_df.reset_index(drop=True)\nprint(\" --------------------- RSCVtuneHyperparam --------------------- \")\nrsCV_df","a9c78c05":"rsCV_df.to_csv('rsCV_df.csv',index=False)","6e8b72e7":"\"\"\"Define the models with optimized hyperparameters.\"\"\"\n\noptParam=rsCV_df.sort_index().loc[:,'bestParam']\n\n# 1. Logistic Regression\nlr = LogisticRegression( **optParam[\"LogisticRegression\"] )\n# 2. DecisionTreeClassifier\ndtree=DecisionTreeClassifier(**optParam[\"DecisionTreeClassifier\"])\n# 3. Random Forest Classifier\nrand_forest = RandomForestClassifier( **optParam[\"RandomForestClassifier\"] )\n# 4. KNN\nknn=KNeighborsClassifier(**optParam[\"KNeighborsClassifier\"] )\n# 5. Support Vector Machines [ SV Classifier ] \nsvc=SVC(**optParam[\"SupportVectorClassifier\"])\n# 6. Gaussian Naive Bayes\ngau_nb = GaussianNB(**optParam[\"GaussianNB\"] )\n# 7. Multinomial Naive Bayes\nmult_nb=MultinomialNB( **optParam[\"MultinomialNB\"] )\n# 8. Gradient Boosting Classifier\ngbc = GradientBoostingClassifier( **optParam[\"GradientBoostingClassifier\"] )\n# 9. Adaboost Classifier\nabc = AdaBoostClassifier( **optParam[\"AdaBoostClassifier\"] )\n# 10. ExtraTrees Classifier\netc = ExtraTreesClassifier(**optParam[\"ExtraTreesClassifier\"] )\n# 11. Extreme Gradient Boosting\nxgbc = XGBClassifier( **optParam[\"XGBClassifier\"] )\n\n#list of model defined above  again\nmodels = [lr, dtree, rand_forest, knn, svc, gau_nb, mult_nb, gbc,abc, etc, xgbc]\nmodelNames=['LogisticRegression','DecisionTreeClassifier',\"RandomForestClassifier\",'KNeighborsClassifier',\n          'SupportVectorClassifier','GaussianNB','MultinomialNB','GradientBoostingClassifier','AdaBoostClassifier',\n          'ExtraTreesClassifier','XGBClassifier']","d06d2d25":"\"\"\"Instantiate the models with optimized hyperparameters.\"\"\"\nScore=[]\nfor model in models:\n    model.fit(X_train,y_train)\n    cv_score=cross_val_score(model, X_train, y_train, cv = 10, scoring=\"accuracy\").mean()\n    cv_score = round(cv_score*100, 2)\n    Score.append(cv_score)\n\n    \n\"\"\" create a dataframe to store score after optimized parameter is fit.\"\"\"\noptParamTrainScore_df=pd.DataFrame()\noptParamTrainScore_df['CV_Score_now']=Score\noptParamTrainScore_df['bestScore']=bestScore\noptParamTrainScore_df['Xcross_val_score_before_hypParmet']=xValScores_df.iloc[:,0].values\noptParamTrainScore_df.index=modelNames\noptParamTrainScore_df = optParamTrainScore_df.sort_values(by=\"CV_Score_now\", ascending=False)\nprint(\" --------------------- RSCVtuneHyperparam --------------------- \")\noptParamTrainScore_df","165ddafe":"\"\"\"Make prediction using all the trained models with best parameter.\"\"\"\n\nmodelPrediction = pd.DataFrame({\"LogisticRegression\":lr.predict(X_test),\n                                \"DecisionTreeClassifier\":dtree.predict(X_test),\n                                \"RandomForestClassifier\":rand_forest.predict(X_test),\n                                \"KNeighborsClassifier\":knn.predict(X_test), \n                                \"SVC\":svc.predict(X_test),\n                                \"GaussianNB\":gau_nb.predict(X_test),\n                                \"MultinomialNB\":mult_nb.predict(X_test), \n                                \"GradientBoostingClassifier\":gbc.predict(X_test), \n                                \"AdaBoostClassifier\":abc.predict(X_test),\n                                \"ExtraTreesClassifier\":etc.predict(X_test), \n                                \"XGBClassifier\":xgbc.predict(X_test)\n                                })\nmodelPrediction.head()","04c98e1b":"for modelName in modelPrediction.columns:\n    submission = pd.DataFrame({\"PassengerId\": test[\"PassengerId\"],\"Survived\": modelPrediction[modelName] })\n    submission.to_csv(modelName+'.csv',index=False)","0249e7ba":"### At first find the relationship between two variables i.e Bivariate Analysis, predictor [ feature ] and target variables [ outcome ].","ab8d680d":"### 2.6 Encode","d5283737":"### 2.5 Correcting Data Types","a9617330":"### 2.7 Feter Scaling are mostly used where the distance formula is used in algorithm","44ee7a66":"### 1.8 Ticket column","10148c3f":"### 1.6 familySize = [ SibSp, Parch ]\nSince these two [ SibSp, Parch ] variables together indicate the size of a family, we would create a new variable 'familySize' from these two variables.","71faf57d":"### 1.1 Survived Feature","2ad7641b":"This missing value must be removed or fill with some other suitable values","7b335632":"### End_ For_ Now","961fc457":"Thus, [ RandomForestClassifier, GradientBoostingClassifier, XGBClassifier ] have the highest accuracy after tunning hyperparameters with RandomizedSearchCV. and the others comes on---","4caa6e02":"### 1.2 Sex Feature","2766c8d4":"# 1. EDA Data Visualization","bbfd1f9d":"### 2.2 Feature Engineering familySize = SibSp + Parch","d00530c0":"### 2.1 Handaling Missing Value ","0d7993ca":"#### 2.1.2 Fare column Missing values","3dabeb68":"data_train['Ticket'] contain Numerical N values more so it must be maintain","da7d03aa":"### 2.3 Feature Engineering Name feature","6421c979":"#### 2.1.3 Embarked column Missing values","7fb7431d":"### Error RuntimeWarning: divide by zero encountered in log result = getattr(ufunc, method)(*inputs, **kwargs) This error is because fare feature contain 0 values, but it must not have zero(0) values. which means zero is nan value","849bb700":"## Modeling ML","4c7f3d0d":"## 1.9 Fare and Age Features","6e2429b8":"#### 2.1.1 Age column Missing values","c4fe491c":"### 2.4 Feature Engineering Fare and Age feature","68bae819":"### make sample_submission file","38d27ad7":"# 2. Pre-Process Feature Engineering","d8308ef6":"### 1.3 Pclass Feature","86782fe4":"### 1.7 Handaling Name column feature","166f4342":"#### 2.1.4 Cabin column Missing values","103fb5ca":"### Hyperparameters Tuning\nPerform GridSearchCV or RandomizedSearchCV to all the classifiers with optimizing their hyperparameters and thus improving their accuracy.then answer default model parameters the best bet or not?","4056b799":"Kaggle DataSets :: https:\/\/www.kaggle.com\/c\/titanic\/data","6e11f238":"### 1.4 Embarked Feature","c1b9e848":"### 1.5 Cabin Feature"}}