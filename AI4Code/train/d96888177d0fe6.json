{"cell_type":{"3fe78611":"code","90d97ff7":"code","c4c94969":"code","ca49bfd1":"code","bfdcc45a":"code","8232d9e9":"code","8c284321":"code","f9fb766b":"code","877d2d10":"code","8c6f0faa":"code","f1c3a947":"code","fc7995f5":"code","bfe844d8":"code","5034e48c":"code","e4e7cc52":"code","e4ee835d":"code","675ef3b2":"code","f2b7b67b":"code","9fa77342":"code","3172d152":"code","bd976eea":"code","2021d24d":"code","4402603a":"code","b5ce2a5a":"code","a99c2957":"code","82be0b79":"code","af6b6076":"code","09df06ba":"code","3b47bdd5":"code","a3089a65":"code","bbbbedd0":"code","5239674c":"code","785982e8":"code","986d1ee8":"code","44512163":"code","9d74e00e":"code","a98025a7":"code","47bf15ef":"code","1b630167":"code","c3a7b4a1":"code","1e2fe161":"code","a0b041ed":"code","62ba930c":"code","db57774e":"code","75e5112d":"code","3615e2bc":"markdown","37eef269":"markdown","0382ac81":"markdown","6c232c20":"markdown","9542d2ba":"markdown","7732dd81":"markdown","ed791840":"markdown"},"source":{"3fe78611":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport matplotlib\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","90d97ff7":"df = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')\ndf","c4c94969":"df.info()","ca49bfd1":"df.describe()","bfdcc45a":"#finding null values in dataset\ndf.isnull().sum()","8232d9e9":"#filling the missing value of categorivccal data\ndf['Gender'] = df[\"Gender\"].fillna(df['Gender'].mode()[0])\ndf['Married'] = df[\"Married\"].fillna(df['Married'].mode()[0])\ndf['Self_Employed'] = df[\"Self_Employed\"].fillna(df['Self_Employed'].mode()[0])\ndf['Dependents'] = df[\"Dependents\"].fillna(df['Dependents'].mode()[0])","8c284321":"df['Credit_History'].unique()","f9fb766b":"sns.countplot(\"Credit_History\", data=df)","877d2d10":"# We are replacing credit history NaN values with mode because\n\n# credit history has only 2 unique values and replacing it with mode is better idea","8c6f0faa":"df['Credit_History'] = df[\"Credit_History\"].fillna(df['Credit_History'].mode()[0])","f1c3a947":"splot = sns.countplot(x ='Loan_Amount_Term', data = df)\nfor p in splot.patches:\n    splot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')","fc7995f5":"# We are replcing NaN values of Loan_Amount_Term with mode because\n# 360 has the highest number of occurence","bfe844d8":"df['Loan_Amount_Term'] = df[\"Loan_Amount_Term\"].fillna(df['Loan_Amount_Term'].mode()[0])","5034e48c":"splot = sns.countplot(x ='LoanAmount', data = df)\nfor p in splot.patches:\n    splot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')","e4e7cc52":"# Here we cannot replace the null values with mode.\n# Here mean or median will be a better option. \n# We have to check for outliers before replcing NaN values with mean or median.","e4ee835d":"# Checking for outliers in \"LoanAmount\"\n\nQ1 = df['LoanAmount'].quantile(0.25)\nQ3 = df['LoanAmount'].quantile(0.75)\nIQR = Q3 - Q1","675ef3b2":"low_lim = Q1 - 1.5 * IQR\nup_lim = Q3 + 1.5 * IQR\nprint('low_limit is', low_lim)\nprint('up_limit is', up_lim)","f2b7b67b":"outlier = []\nfor x in df['LoanAmount']:\n    if ((x > up_lim) or (x < low_lim)):\n         outlier.append(x)\nprint('Outlier in the dataset is', outlier)","9fa77342":"len(outlier)","3172d152":"# 6.5% of data is in outlier so we will not remove the outliers from the dataset.\n# We will replace the NaN values with median because median is not affected by outliers.","bd976eea":"df['LoanAmount'] = df[\"LoanAmount\"].fillna(df['LoanAmount'].median())","2021d24d":"df['Total_Income'] = df['ApplicantIncome'] + df['CoapplicantIncome']\ndf","4402603a":"plt.figure(figsize=(8,6))\nsns.countplot(df['Loan_Status']);\n\nprint('The percentage of Y class : %.2f' % (df['Loan_Status'].value_counts()[0] \/ len(df)))\nprint('The percentage of N class : %.2f' % (df['Loan_Status'].value_counts()[1] \/ len(df)))\n","b5ce2a5a":"df['Loan_Status'].replace('N',0,inplace=True)\ndf['Loan_Status'].replace('Y',1,inplace=True)","a99c2957":"grid = sns.FacetGrid(df,col='Loan_Status', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'Credit_History');\n# Those applicants wo have credit history are more likely to get loan.","82be0b79":"grid = sns.FacetGrid(df,col='Gender', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'Loan_Status');\n\n# Chances of getting loan to female applicant is higher.","af6b6076":"sns.countplot(x='Married', hue='Loan_Status', data=df);\n# Married applicants have higher chances getting loan","09df06ba":"grid = sns.FacetGrid(df,col='Dependents', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'Loan_Status');","3b47bdd5":"grid = sns.FacetGrid(df,col='Loan_Status', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'Dependents');","a3089a65":"grid = sns.FacetGrid(df,col='Self_Employed', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'Loan_Status');\n# If the applicant is having a job he or she has the higher chances of getting loan.","bbbbedd0":"grid = sns.FacetGrid(df,col='Property_Area', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'Loan_Status');\n#Semiurban applicants have higher chances.","5239674c":"sns.countplot(x='Education', hue='Loan_Status', data=df);\n# Graduate applicants have higher chances of getting loan.","785982e8":"plt.figure(figsize=(8,6))\nsns.boxplot(df['Loan_Status'], df['Total_Income']);\n# Total income does not effect the loan status.\n","986d1ee8":"plt.figure(figsize=(15,15))\nsns.countplot(x=\"Loan_Amount_Term\", hue=\"Loan_Status\", data=df)\n#no correlation","44512163":"plt.figure(figsize=(8,10))\nsns.boxplot(x=\"Loan_Status\",y=\"LoanAmount\", data=df)","9d74e00e":"# drop unnecessary columns\ncols = ['ApplicantIncome', 'CoapplicantIncome', \"LoanAmount\", \"Loan_Amount_Term\", \"Total_Income\", 'Loan_ID', 'Dependents']\ndf = df.drop(columns=cols, axis=1)\ndf.head()","a98025a7":"from sklearn.preprocessing import LabelEncoder\ncols = ['Gender',\"Married\",\"Education\",'Self_Employed',\"Property_Area\"]\nle = LabelEncoder()\nfor col in cols:\n    df[col] = le.fit_transform(df[col])","47bf15ef":"df.head()","1b630167":"# specify input and output attributes\nX = df.drop(columns=['Loan_Status'], axis=1)\ny = df['Loan_Status']","c3a7b4a1":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","1e2fe161":"# classifier function\nfrom sklearn.model_selection import cross_val_score\ndef classify(model, x, y):\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n    model.fit(x_train, y_train)\n    print(\"Accuracy is\", model.score(x_test, y_test)*100)\n    # cross validation - it is used for better validation of model\n    # eg: cv-5, train-4, test-1\n    score = cross_val_score(model, x, y, cv=5)\n    print(\"Cross validation is\",np.mean(score)*100)","a0b041ed":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nclassify(model, X, y)","62ba930c":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nclassify(model, X, y)","db57774e":"from sklearn.metrics import confusion_matrix\ny_pred = model.predict(x_test)\ncm = confusion_matrix(y_test, y_pred)\ncm","75e5112d":"sns.heatmap(cm, annot=True)","3615e2bc":"# Checking for Data Imbalance ","37eef269":"# Exploring the individual features","0382ac81":"# Label Encoding to categorical data","6c232c20":"# Preprocessing of the dataset","9542d2ba":"# Importing the required libraries","7732dd81":"# Loading the Dataset","ed791840":"# Dataset Info"}}