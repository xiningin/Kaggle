{"cell_type":{"d05d5d1a":"code","00328356":"code","2cce2331":"code","9bb2dd79":"code","99711c3c":"code","c98acb13":"code","f6326675":"code","442ec4dc":"code","bff790ab":"code","6748fe8b":"code","6338a8e7":"code","e5b854a8":"code","362ef854":"code","2f528455":"code","b7dcba44":"code","54fad2e3":"code","598243b2":"code","5125ef3c":"code","164e5b93":"code","5f383107":"code","a46a0176":"code","74b1116f":"code","5fbc783f":"code","eb89a8a5":"code","ce2644d6":"code","a9125643":"code","fbfbcb01":"code","ff412d33":"code","ef558007":"code","d3ad724b":"code","ae314af2":"code","81db6172":"code","aa9c304d":"code","46edb1b6":"code","2758d2ed":"code","370d80dd":"code","f1364b34":"code","eae9517a":"code","d17554db":"code","f0b57f39":"code","d4e9e4bf":"markdown"},"source":{"d05d5d1a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","00328356":"# Import Linear Regression machine learning library\nfrom sklearn.linear_model import LinearRegression\n\n# importing ploting libraries\nimport matplotlib.pyplot as plt   \nimport matplotlib.style\nplt.style.use('classic')\n\n#importing seaborn for statistical plots\nimport seaborn as sns","2cce2331":"# reading the CSV file into pandas dataframe\nmpg_df = pd.read_csv('\/kaggle\/input\/autompg-dataset\/auto-mpg.csv')  ","9bb2dd79":"mpg_df.columns","99711c3c":"# drop the car name column as it is useless for the model\nmpg_df = mpg_df.drop('car name', axis=1)\n\n# Convert categorical variable into dummy\/indicator variables. As many columns will be created as distinct values\n#One hot coding. The column names will be America, Europe and Asia... with one hot coding\nmpg_df = pd.get_dummies(mpg_df, columns=['origin'])","c98acb13":"#Lets analysze the distribution of the dependent (mpg) column\nmpg_df.describe().transpose()\n\n# Note:  Horsepower column is missing the describe output. That indicates column 'Horsepower' is an object dtype.","f6326675":"#Check if the horsepower column contains anything other than digits \n# run the \"isdigit() check on 'hp' column of the mpg_df dataframe. Result will be True or False for every row\n# capture the result in temp dataframe and dow a frequency count using value_counts()\n\n\ntemp = pd.DataFrame(mpg_df.horsepower.str.isdigit())\n\n# There are six records with non digit values in 'hp' column\n\ntemp[temp['horsepower'] == False]   # from temp take only those rows where hp has false\n","442ec4dc":"# On inspecting records number 32, 126 etc, we find \"?\" in the columns. Replace them with \"nan\"\n#Replace them with nan and remove the records from the data frame that have \"nan\"\nmpg_df = mpg_df.replace('?', np.nan)\n\n#Let us see if we can get those records with nan\n\nmpg_df[mpg_df.isnull().any(axis=1)]","bff790ab":"# There are various ways to handle missing values. Drop the rows, replace missing values with median values etc. \n#instead of dropping the rows, lets replace the missing values with median value. \nmpg_df.median()\n\n# replace the missing values in 'hp' with median value of 'hp' :Note, we do not need to specify the column names\n# every column's missing value is replaced with that column's median respectively  (axis =0 means columnwise)\n#mpg_df = mpg_df.fillna(mpg_df.median())\n\nmpg_df = mpg_df.apply(lambda x: x.fillna(x.median()),axis=0)\n\nmpg_df['hp'] = mpg_df['horsepower'].astype('float64')  # converting the hp column from object \/ string type to float\n\nmpg_df.dtypes","6748fe8b":"# Let us do a correlation analysis among the different dimensions and also each dimension with the dependent dimension\n# This is done using scatter matrix function which creates a dashboard reflecting useful information about the dimensions\n# The result can be stored as a .png file and opened in say, paint to get a larger view \n\nmpg_df_attr = mpg_df.iloc[:, 0:10]\n\n\nsns.pairplot(mpg_df_attr, diag_kind='kde')   # to plot density curve instead of histogram\n","6338a8e7":"mpg_df.columns","e5b854a8":"\n#Observation\n#The data distribution across various dimensions except 'Acc' do not look normal\n#Close observation between 'mpg' and other attributes indicate the relationship is not really linear\n#relation between 'mpg' and 'horsepower' show hetroscedacity... which will impact model accuracy\n#How about 'mpg' vs 'yr' surprising to see a positive relation\n\n# Copy all the predictor variables into X dataframe. Since 'mpg' is dependent variable drop it\nX = mpg_df.drop('mpg', axis=1)\nX = X.drop({'origin_1', 'origin_2' ,'origin_3'}, axis=1)\n\n# Copy the 'mpg' column alone into the y dataframe. This is the dependent variable\ny = mpg_df[['mpg']]\n","362ef854":"#Let us break the X and y dataframes into training set and test set. For this we will use\n#Sklearn package's data splitting function which is based on random function\n\nfrom sklearn.model_selection import train_test_split\n\n# Split X and y into training and test set in 75:25 ratio\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30 , random_state=1)\n\n# invoke the LinearRegression function and find the bestfit model on training data\n\nregression_model = LinearRegression()\nregression_model.fit(X_train, y_train)\n\n","2f528455":"# Let us explore the coefficients for each of the independent attributes\n\nfor idx, col_name in enumerate(X_train.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, regression_model.coef_[0][idx]))","b7dcba44":"# Let us check the intercept for the model\n\nintercept = regression_model.intercept_[0]\n\nprint(\"The intercept for our model is {}\".format(intercept))","54fad2e3":"\nregression_model.score(X_train, y_train)","598243b2":"# Model score - R2 or coeff of determinant\n# R^2=1\u2013RSS \/ TSS =  RegErr \/ TSS\n\nregression_model.score(X_test, y_test)\n#Model Score for Test data will be 82%","5125ef3c":"# ---------------------------------- Using Statsmodel library to get R type outputs -----------------------------","164e5b93":"#Reason behind using Stats Model\n# R^2 is not a reliable metric as it always increases with addition of more attributes even if the attributes have no \n# influence on the predicted variable. Instead we use adjusted R^2 which removes the statistical chance that improves R^2\n# Scikit does not provide a facility for adjusted R^2... so we use \n# statsmodel, a library that gives results similar to\n# what you obtain in R language\n# This library expects the X and Y to be given in one single dataframe\n\ndata_train = pd.concat([X_train, y_train], axis=1)\ndata_train.head()\n","5f383107":"data_train.columns\n#let's rename the column 'model year' to 'model_year'\n\ndata_train.rename(columns={'model year' : 'model_year'}, inplace=True)","a46a0176":"import statsmodels.formula.api as smf\nlm1 = smf.ols(formula= 'mpg ~ cylinders+displacement+horsepower+weight+acceleration+model_year', data = data_train).fit()\nlm1.params","74b1116f":"print(lm1.summary())  #Inferential statistics","5fbc783f":"# Let us check the sum of squared errors by predicting value of y for test cases and \n# subtracting from the actual y for the test cases\n\nmse = np.mean((regression_model.predict(X_test)-y_test)**2)","eb89a8a5":"# underroot of mean_sq_error is standard deviation i.e. avg variance between predicted and actual\n\nimport math\n\nmath.sqrt(mse)\n# so there is avg of 3.0 (roundoff) mpg difference from real mpg on an avg","ce2644d6":"# Model score - R2 or coeff of determinant\n# R^2=1\u2013RSS \/ TSS\n\nregression_model.score(X_test, y_test)","a9125643":"# predict mileage (mpg) for a set of attributes not in the training or test set\ny_pred = regression_model.predict(X_test)","fbfbcb01":"# Since this is regression, plot the predicted y value vs actual y values for the test data\n# A good model's prediction will be close to actual leading to high R and R2 values\n#plt.rcParams['figure.dpi'] = 500\n\n\n\nplt.scatter(y_test['mpg'], y_pred)\n\n","ff412d33":"# ------------------------------------------------- ITERATION 2  ---------------------------------------------------","ef558007":"# How do we improve the model? the R^2 is .844, how do we improve it\n# The indpendent attributes have different units and scales of measurement \n# It is always a good practice to scale all the dimensions using z scores or someother methode to address the problem of different scales \n","d3ad724b":"from scipy import stats\nfrom scipy.stats import zscore\n\n\n\n#X_test_scaled = X_test.apply(zscore)\n#y_train_scaled = y_train.apply(zscore)\n#y_test_scaled = y_test.apply(zscore)","ae314af2":"#converting the variables into integer to apply zscore\nX_train = X_train.astype(int)\nX_test = X_test.astype(int)\ny_train = y_train.astype(int)\ny_test = y_test.astype(int)\n","81db6172":"X_train_scaled = X_train.apply(zscore)\nX_test_scaled = X_test.apply(zscore)\ny_train_scaled = y_train.apply(zscore)\ny_test_scaled = y_test.apply(zscore)","aa9c304d":"# invoke the LinearRegression function and find the bestfit model on training data\n\nregression_model = LinearRegression()\nregression_model.fit(X_train_scaled, y_train_scaled)","46edb1b6":"# Let us explore the coefficients for each of the independent attributes\n\nfor idx, col_name in enumerate(X_train.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, regression_model.coef_[0][idx]))","2758d2ed":"intercept = regression_model.intercept_[0]\n\nprint(\"The intercept for our model is {}\".format(intercept))","370d80dd":"# Model score - R2 or coeff of determinant\n# R^2=1\u2013RSS \/ TSS\n\nregression_model.score(X_test_scaled, y_test_scaled)","f1364b34":"# Let us check the sum of squared errors by predicting value of y for training cases and \n# subtracting from the actual y for the training cases\n\nmse = np.mean((regression_model.predict(X_test_scaled)-y_test_scaled)**2)","eae9517a":"# underroot of mean_sq_error is standard deviation i.e. avg variance between predicted and actual\n\nimport math\n\nmath.sqrt(mse)","d17554db":"# predict mileage (mpg) for a set of attributes not in the training or test set\ny_pred = regression_model.predict(X_test_scaled)","f0b57f39":"# Since this is regression, plot the predicted y value vs actual y values for the test data\n# A good model's prediction will be close to actual leading to high R and R2 values\nplt.scatter(y_test_scaled['mpg'], y_pred)","d4e9e4bf":"# Conclusion\n\n###  * Evaluation metric for regression algorithm will be (MSE) Mean square error and (MAE) Mean absolute error\n### * Prevent overfitting by understanding the different between Train and Test data.\n### * R^2 is not a reliable metric as it always increases with addition of more attributes even if the attributes have no influence on the predicted variable. Instead we use adjusted R^2 which removes the statistical chance that improves R^2"}}