{"cell_type":{"ee143158":"code","014a66bb":"code","a917d4b1":"code","cb56d94d":"code","fb9e3643":"code","0f991ecb":"code","2cf32c5a":"code","d4b0d37b":"code","8a62c550":"code","f4115a1b":"code","7f5a22d2":"code","1208f8eb":"code","74c23067":"code","bcb6e5a1":"code","8b12a5df":"code","be2f3c59":"code","02f96188":"code","0668b328":"code","23d912c4":"code","4cbec254":"code","21135dc4":"code","5e1d8e98":"code","0f924ae3":"code","86c6b109":"markdown","3bd2fdd3":"markdown","555ad4b5":"markdown","66cd43ec":"markdown","f33b9386":"markdown","b2205da1":"markdown","5f7eff23":"markdown","8788802a":"markdown","bd6d98b9":"markdown","163a3941":"markdown","d68f5ce6":"markdown","ff5f7877":"markdown","63be8b7b":"markdown","fc06d5dd":"markdown","02786813":"markdown","2cc38617":"markdown","fd7f8800":"markdown","a7c1a39e":"markdown","130d7668":"markdown","86a41cfd":"markdown","456159d7":"markdown","943032b9":"markdown","90d82d2d":"markdown","5b346729":"markdown","1fdfc304":"markdown","5cdabc57":"markdown","9c87d365":"markdown","9e6b3b2d":"markdown","1c4472fc":"markdown","a3168844":"markdown","5b716fe4":"markdown"},"source":{"ee143158":"from IPython.display import HTML\nHTML('<center><iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/tlThdr3O5Qo?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","014a66bb":"!pip install pymap3d==2.1.0\n!pip install -U l5kit","a917d4b1":"import l5kit, os\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom l5kit.data import PERCEPTION_LABELS\nfrom prettytable import PrettyTable\nfrom l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\nfrom IPython.display import display, clear_output\nimport PIL\nfrom IPython.display import display, clear_output\nfrom IPython.display import HTML\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom PIL import Image\nfrom datetime import time,date\nimport nltk\nimport spacy\nimport re","cb56d94d":"os.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"\ndm = LocalDataManager()\nsample_path = '..\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/sample.zarr'\nsample_dataset = ChunkedDataset(sample_path)\nsample_dataset.open()\nprint(sample_dataset)","fb9e3643":"sample_agents = sample_dataset.agents\nsample_agents = pd.DataFrame(sample_agents)\nsample_agents.columns = [\"data\"]; features = ['centroid', 'extent', 'yaw', 'velocity', 'track_id', 'label_probabilities']\n\nfor i, feature in enumerate(features):\n    sample_agents[feature] = sample_agents['data'].apply(lambda x: x[i])\nsample_agents.drop(columns=[\"data\"],inplace=True)\nsample_agents.head()","0f991ecb":"del sample_agents","2cf32c5a":"test_path = '..\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/test.zarr'\ntest_dataset = ChunkedDataset(test_path)\ntest_dataset.open()\nprint(test_dataset)","d4b0d37b":"train_path = '..\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/train.zarr'\ntrain_dataset = ChunkedDataset(train_path)\ntrain_dataset.open()\nprint(train_dataset)","8a62c550":"valid_path = '..\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/validate.zarr'\nvalid_dataset = ChunkedDataset(valid_path)\nvalid_dataset.open()\nprint(valid_dataset)","f4115a1b":"cfg = {}\nraster_params = {'raster_size':[512,512],\n                 'pixel_size':[0.5,0.5],\n                 'ego_center':[0.25,0.5],\n                 'map_type':'py_semantic',\n                 'satellite_map_key': 'aerial_map\/aerial_map.png',\n                 'semantic_map_key': 'semantic_map\/semantic_map.pb',\n                 'dataset_meta_key': 'meta.json',\n                 'filter_agents_threshold': 0.5}\nmodel_params ={'model_architecture': 'effnetB5',\n               'history_num_frames': 0,\n               'history_step_size': 1,\n               'history_delta_time': 0.1,\n               'future_num_frames': 50,\n               'future_step_size': 1,\n               'future_delta_time': 0.1}\ncfg['raster_params'] = raster_params\ncfg['model_params'] = model_params\nrast = build_rasterizer(cfg,dm)\ndataset = EgoDataset(cfg, sample_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","7f5a22d2":"cfg = {}\nraster_params = {'raster_size':[512,512],\n                 'pixel_size':[0.5,0.5],\n                 'ego_center':[0.25,0.5],\n                 'map_type':'py_semantic',\n                 'satellite_map_key': 'aerial_map\/aerial_map.png',\n                 'semantic_map_key': 'semantic_map\/semantic_map.pb',\n                 'dataset_meta_key': 'meta.json',\n                 'filter_agents_threshold': 0.5}\nmodel_params ={'model_architecture': 'effnetB5',\n               'history_num_frames': 0,\n               'history_step_size': 1,\n               'history_delta_time': 0.1,\n               'future_num_frames': 50,\n               'future_step_size': 1,\n               'future_delta_time': 0.1}\ncfg['raster_params'] = raster_params\ncfg['model_params'] = model_params\nrast = build_rasterizer(cfg,dm)\ndataset = EgoDataset(cfg, train_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","1208f8eb":"cfg = {}\nraster_params = {'raster_size':[512,512],\n                 'pixel_size':[0.5,0.5],\n                 'ego_center':[0.25,0.5],\n                 'map_type':'py_semantic',\n                 'satellite_map_key': 'aerial_map\/aerial_map.png',\n                 'semantic_map_key': 'semantic_map\/semantic_map.pb',\n                 'dataset_meta_key': 'meta.json',\n                 'filter_agents_threshold': 0.5}\nmodel_params ={'model_architecture': 'effnetB5',\n               'history_num_frames': 0,\n               'history_step_size': 1,\n               'history_delta_time': 0.1,\n               'future_num_frames': 50,\n               'future_step_size': 1,\n               'future_delta_time': 0.1}\ncfg['raster_params'] = raster_params\ncfg['model_params'] = model_params\nrast = build_rasterizer(cfg,dm)\ndataset = EgoDataset(cfg, valid_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","74c23067":"cfg = {}\nraster_params = {'raster_size':[512,512],\n                 'pixel_size':[0.5,0.5],\n                 'ego_center':[0.25,0.5],\n                 'map_type':'py_semantic',\n                 'satellite_map_key': 'aerial_map\/aerial_map.png',\n                 'semantic_map_key': 'semantic_map\/semantic_map.pb',\n                 'dataset_meta_key': 'meta.json',\n                 'filter_agents_threshold': 0.5}\nmodel_params ={'model_architecture': 'effnetB5',\n               'history_num_frames': 0,\n               'history_step_size': 1,\n               'history_delta_time': 0.1,\n               'future_num_frames': 50,\n               'future_step_size': 1,\n               'future_delta_time': 0.1}\ncfg['raster_params'] = raster_params\ncfg['model_params'] = model_params\nrast = build_rasterizer(cfg,dm)\ndataset = EgoDataset(cfg, test_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","bcb6e5a1":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, sample_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","8b12a5df":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, train_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","be2f3c59":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, test_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","02f96188":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, valid_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","0668b328":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, sample_dataset, rast)\nscene_idx = 34\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","23d912c4":"import matplotlib.animation as animation\ncfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, sample_dataset, rast)\nscene_idx = 34\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\nfig = plt.figure()\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    images.append(PIL.Image.fromarray(im[::-1]))\nim = plt.imshow(images[0], animated=True)\nplt.axis('off')\ndef animate(i):\n    im.set_data(images[i])\nani = animation.FuncAnimation(fig, animate, interval=100, blit=False,\n                                repeat_delay=1000)\nHTML(ani.to_html5_video())","4cbec254":"import imageio\nimport IPython.display\nimageio.mimsave(\"\/tmp\/gif.gif\", images, duration=0.0001)\nIPython.display.Image(filename=\"\/tmp\/gif.gif\", format='png')","21135dc4":"image = cv2.imread('..\/input\/lyft-motion-prediction-autonomous-vehicles\/aerial_map\/aerial_map.png')\nimage = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(32,32))\nplt.imshow(image)","5e1d8e98":"from l5kit.data.map_api import MapAPI\nfrom l5kit.rasterization.rasterizer_builder import _load_metadata\n\nsemantic_map_filepath = dm.require(cfg[\"raster_params\"][\"semantic_map_key\"])\ndataset_meta = _load_metadata(cfg[\"raster_params\"][\"dataset_meta_key\"], dm)\nworld_to_ecef = np.array(dataset_meta[\"world_to_ecef\"], dtype=np.float64)\n\nmap_api = MapAPI(semantic_map_filepath, world_to_ecef)\nMAP_LAYERS = [\"junction\", \"node\", \"segment\", \"lane\"]\n\n\ndef element_of_type(elem, layer_name):\n    return elem.element.HasField(layer_name)\n\n\ndef get_elements_from_layer(map_api, layer_name):\n    return [elem for elem in map_api.elements if element_of_type(elem, layer_name)]\n\n\nclass MapRenderer:\n    \n    def __init__(self, map_api):\n        self._color_map = dict(drivable_area='#a6cee3',\n                               road_segment='#1f78b4',\n                               road_block='#b2df8a',\n                               lane='#474747')\n        self._map_api = map_api\n    \n    def render_layer(self, layer_name):\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_axes([0, 0, 1, 1])\n        \n    def render_lanes(self):\n        all_lanes = get_elements_from_layer(self._map_api, \"lane\")\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_axes([0, 0, 1, 1])\n        for lane in all_lanes:\n            self.render_lane(ax, lane)\n        return fig, ax\n        \n    def render_lane(self, ax, lane):\n        coords = self._map_api.get_lane_coords(MapAPI.id_as_str(lane.id))\n        self.render_boundary(ax, coords[\"xyz_left\"])\n        self.render_boundary(ax, coords[\"xyz_right\"])\n        \n    def render_boundary(self, ax, boundary):\n        xs = boundary[:, 0]\n        ys = boundary[:, 1] \n        ax.plot(xs, ys, color=self._color_map[\"lane\"], label=\"lane\")\n        \n        \nrenderer = MapRenderer(map_api)\nfig, ax = renderer.render_lanes()","0f924ae3":"print(\"scenes\", sample_dataset.scenes)\nprint(\"scenes[0]\", sample_dataset.scenes[0])\nprint(\"scenes\", test_dataset.scenes)\nprint(\"scenes[0]\", test_dataset.scenes[0])\nprint(\"scenes\", train_dataset.scenes)\nprint(\"scenes[0]\", train_dataset.scenes[0])\nprint(\"scenes\", valid_dataset.scenes)\nprint(\"scenes[0]\", valid_dataset.scenes[0])","86c6b109":"<center><h2> Let's explore the data<\/h2><\/center>\n\n# 5. Necessary Imports\n\n## L5Kit is a library which lets you:\n\n* Load driving scenes from zarr files\n* Read semantic maps\n* Read aerial maps\n* Create birds-eye-view (BEV) images which represent a scene around an AV or another vehicle\n* Sample data\n* Train neural networks\n* Visualize results","3bd2fdd3":"# 2. What is the difference between this and previous competition?","555ad4b5":"From the above satellite view we can see that the pink line is the vehicle's movement. Well let's animate and try to make into video.","66cd43ec":"# A. Sample Zarr","f33b9386":"# Satellite view","b2205da1":"<h3>As we can see there's a lot of information in this image itself. Let's discuss about this image now.<h3>\n\n* We see around ten roads in the picture. 4 are main and remaining are inside some area like flat or mall or something like that.But we can see vehicles are on the 4 roads.So we can continue our discussion on these 4 roads alone.\n* The green line represents the autonomous vehicle in the road and its movement we want to predict in this competition.\n* The blue line represents the real vehicle in the road.\n* We can also see the vehicles moving.","5f7eff23":"The general view of the street us borrowed from [this notebook](https:\/\/www.kaggle.com\/t3nyks\/lyft-working-with-map-api)","8788802a":"[In the previous competition, we were tasked with detecting three dimensional objects that we see normally on the road to teach AV's how to recognize these. ](https:\/\/www.kaggle.com\/c\/3d-object-detection-for-autonomous-vehicles\/)\n<center><img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/Lyft-Kaggle\/Kaggle-01.png\" width=\"100%\"><\/center>","bd6d98b9":"<center><h2>A self driving car in action <\/h2><\/center>\nBefore we dive into the technical details of this kernel, let us watch an interesting video of a self-driving car in action.","163a3941":"As we have seen the basic exploration above for 4 different ZARR's we will now explore them one by one in detail now","d68f5ce6":"# B. Test Zarr","ff5f7877":"The above gif file gives us better view than the above video","63be8b7b":"From the above video we can see the movement of vehicles and movement of our Autonomous Vehicle(Pink path). And also we can see that our AV takes a straight path.","fc06d5dd":"So welcome kagglers. In this Lyft competition, we are asked to predict the motion of self driving vehicles. And this knowledge can be used to predict  how cars, cyclists,and pedestrians move in the AV's environment.We have to predict the location of objects agents in the next 50 frames.","02786813":"We see that the satellite view gives us better info than the previous view.\nWe will now see the trajectory path of someother scene.","2cc38617":"Now, let's look at the scenes.","fd7f8800":"# 1. What are we asked to predict in this competition?","a7c1a39e":"# D. Validation Zarr","130d7668":"# SEMANTIC VIEW","86a41cfd":"1. [1st place solution](https:\/\/www.kaggle.com\/c\/3d-object-detection-for-autonomous-vehicles\/discussion\/122820)\n2. [2nd place solution](https:\/\/www.kaggle.com\/c\/3d-object-detection-for-autonomous-vehicles\/discussion\/123004)\n3. [3rd place solution](https:\/\/www.kaggle.com\/c\/3d-object-detection-for-autonomous-vehicles\/discussion\/117269)","456159d7":"# 3. Previous Competition Winners Solutions","943032b9":"<center><h1> Thanks for Reading My Kernel <\/h1><\/center>\n<center><h2> Please upvote if you find it useful <\/h2><\/center>","90d82d2d":"# Credits :\n[Lyft: Understanding the data + baseline model by Trigram(@nxrprime)](https:\/\/www.kaggle.com\/nxrprime\/lyft-understanding-the-data-baseline-model)","5b346729":"## Evaluation Metric: Negative log-likelihood \n\nWe calculate the negative log-likelihood of the ground truth data given the multi-modal predictions. You can get more information [here](https:\/\/www.kaggle.com\/c\/lyft-motion-prediction-autonomous-vehicles\/overview\/evaluation).\n![](https:\/\/camo.githubusercontent.com\/b3634eea5be5501318957e21086781666018efa1\/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532304c2532302533442532302d2532302535436c6f6725323070253238785f253742312532432532302535436c646f747325324325323054253744253243253230795f253742312532432532302535436c646f74732532432532305425374425374363253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742782537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742792537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253239)","1fdfc304":"<center><img src=\"https:\/\/i.imgur.com\/8dUga6i.jpg\" width=\"1000px\"><\/center> ","5cdabc57":"The files are stored in the [.zarr file](https:\/\/zarr.readthedocs.io\/en\/stable\/) format with Python, which we can easily load using the Level 5 Kit (l5kit for the pip package). Within our training ZARRs, we have the agents, the masks for agents, frames and scenes (which you might recollect from last year) and traffic light faces.","9c87d365":"# Let's Explore Zarr","9e6b3b2d":"# Aerial Map","1c4472fc":"# C. Train Zarr","a3168844":"<center><h1> Lyft: EDA <\/h1><\/center>\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/Lyft-Kaggle\/Motion%20Prediction\/BP9I1484%20(1).jpg\">","5b716fe4":"# 4. How files are stored?"}}