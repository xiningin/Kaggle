{"cell_type":{"9bef0e6e":"code","eb701390":"code","21fcc4ce":"code","27305645":"code","a8198095":"code","dd8ae4e0":"code","695e213f":"code","8af3b1f7":"code","bbdde0ea":"code","623be205":"code","7d2582a8":"markdown","84982eb1":"markdown","3f080867":"markdown","62a38af5":"markdown"},"source":{"9bef0e6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","eb701390":"\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nimport numpy as np\nimport json\nimport glob\nimport sys\n\nsys.path.insert(0, \"..\/\")\n\nroot_path = '\/kaggle\/input\/'\n\ncorona_features = {\"doc_id\": [None], \"source\": [None], \"title\": [None],\n                  \"abstract\": [None], \"text_body\": [None]}\ncorona_df = pd.DataFrame.from_dict(corona_features)\n\njson_filenames = glob.glob(f'{root_path}\/**\/*.json', recursive=True)","21fcc4ce":"def return_corona_df(json_filenames, df):\n\n    for file_name in json_filenames:\n\n        row = {\"doc_id\": None, \"source\": None, \"title\": None,\n              \"abstract\": None, \"text_body\": None}\n\n        with open(file_name) as json_data:\n            data = json.load(json_data)\n\n            doc_id = data['paper_id']\n            row['doc_id'] = doc_id\n            row['title'] = data['metadata']['title']\n\n            # Now need all of abstract. Put it all in \n            # a list then use str.join() to split it\n            # into paragraphs. \n\n            abstract_list = [abst['text'] for abst in data['abstract']]\n            abstract = \"\\n \".join(abstract_list)\n\n            row['abstract'] = abstract\n\n            # And lastly the body of the text. \n            body_list = [bt['text'] for bt in data['body_text']]\n            body = \"\\n \".join(body_list)\n            \n            row['text_body'] = body\n            \n            # Now just add to the dataframe. \n            \n#             if source == 'b':\n#                 row['source'] = \"BIORXIV\"\n#             elif source == \"c\":\n#                 row['source'] = \"COMMON_USE_SUB\"\n#             elif source == \"n\":\n#                 row['source'] = \"NON_COMMON_USE\"\n#             elif source == \"p\":\n#                 row['source'] = \"PMC_CUSTOM_LICENSE\"\n            \n            df = df.append(row, ignore_index=True)\n    \n    return df\n    \ncorona_df = return_corona_df(json_filenames, corona_df)","27305645":"corona_df.shape","a8198095":"corona_df.columns","dd8ae4e0":"risk_ind = []\ncount_risk = []\n\nfor i in corona_df['abstract']:\n    if (str(i).lower().find('risk') != -1 and str(i).lower().find('covid') != -1):\n    \n        risk_ind.append(i)\n        count_risk.append(i.lower().count('risk'))\ncorona_df_risk_covid = corona_df[corona_df['abstract'].isin(risk_ind)] \ncorona_df_risk_covid['count_risk'] = count_risk","695e213f":"corona_df_risk_covid.drop_duplicates(['title'])","8af3b1f7":"!pip install bert-extractive-summarizer\n!pip install spacy\n!pip install transformers==2.2.0","bbdde0ea":"from summarizer import Summarizer\n\nmodel = Summarizer()\nsummary = []\nfor i in corona_df_risk_covid['text_body']:\n    result = model(i, min_length=60)\n    full = ''.join(result)\n    summary.append(full)","623be205":"summary[0]","7d2582a8":"# **Bert Extractive Summarizer\nThis tool utilizes the HuggingFace Pytorch transformers library to run extractive summarizations.\nThis works by first embedding the sentences, then running a clustering algorithm, finding the sentences\nthat are closest to the cluster's centroids. This library also uses coreference techniques,\nutilizing the https:\/\/github.com\/huggingface\/neuralcoref library to resolve words in summaries that need\nmore context. ","84982eb1":"This notebook means to extract articles that addessing risk factor contributing to Covid-19 and ranked them based on their relevant to the subject.Also highlights the sentences that contain the valuable information.\nTo achieve this goal, this first draft of the process was designed to show how the data mining pipeline of the process will look like in this case. the ingredients are as following:\n1. A high recall method that extracts articles. (We started by  mentions of 'risk' and 'covid')\n2. A ranking algorithm. (We ranked the articles  based on the frequency of 'risk' in their abstract).\n3. An extractive summarization algorithm that extract the relevant part of the selected articles (We use BERT summarization   algorithm here).\n4. Ideally expert feedback on relevancy of the extracted content (I am no expertt but I plan to read random sample of both population :) ) \n5. Another extractive summary between all articles that were selected.\n6. Abstractive summary of that selection.\n7. Visulazation of what was achieved. \n","3f080867":"# Extractive summary of the articles that are risk related","62a38af5":"# ***Extracting papers with risk and covid words in their abstracts\n"}}