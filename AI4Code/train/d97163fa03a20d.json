{"cell_type":{"ca26f63c":"code","c3b547f9":"code","7ee2e6af":"code","beab9239":"code","3995112e":"code","fecd7aa6":"code","ae556fcf":"code","16bf6337":"code","4f492bef":"code","bc3508e1":"code","4d33136d":"code","34eb70d3":"code","becb66a5":"code","8c4fd285":"code","7f0f2b0f":"code","99de3b39":"code","df785847":"code","7f464b00":"code","ae6db529":"code","462b8dfc":"code","093de223":"code","0fb958ab":"code","2219b596":"code","a6c90c87":"code","f2661c92":"code","631d2564":"code","921a935c":"code","5e55e4f2":"code","566f78f8":"code","9e226ac4":"code","828f2090":"code","f7c7c43d":"code","2031cf21":"code","a0b3907d":"code","569fd278":"code","35f6d950":"code","900a5cb0":"code","c8981f1b":"code","ab2f14b1":"code","0a0d172e":"code","70a716fa":"code","603751f9":"code","e0b0fbb9":"code","6d5c0231":"code","1d41ed19":"code","f0379c3d":"markdown","d10f0c40":"markdown","edb7a8b0":"markdown","f0228877":"markdown","9936b9f8":"markdown","191d0949":"markdown","6d4b1814":"markdown","ab7b028c":"markdown","1829a499":"markdown","198042e8":"markdown","c2d665cb":"markdown","2fede2c3":"markdown","d0e75501":"markdown","e4edac6d":"markdown","07591f13":"markdown","373780be":"markdown","14150c66":"markdown","1398afee":"markdown","42837e68":"markdown","e89db752":"markdown","971de113":"markdown"},"source":{"ca26f63c":"# import shutil\n# shutil.rmtree(\"\/kaggle\/working\/structured_data_regressor*\") \n\n# import os, glob\n# for filename in glob.glob(\"\/kaggle\/working\/structured_data_regressor\/trial*\"):\n#     os.remove(filename) ","c3b547f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ee2e6af":"import sys\nsys.path.append('..\/input\/autokeras')\n","beab9239":"import re\nimport string\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.datasets import load_files\nimport autokeras as ak\n","3995112e":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport spacy","fecd7aa6":"train = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")","ae556fcf":"train.head()","16bf6337":" train.sort_values(by=['target']).head()","4f492bef":"import itertools\nimport collections\nimport matplotlib.pyplot as plt\n","bc3508e1":"train.sort_values(by=['target']).head().iloc[0]['excerpt']\n","4d33136d":"train.sort_values(by=['target']).head().iloc[1]['excerpt']\n","34eb70d3":"train.sort_values(by=['target']).head().iloc[0]['excerpt']\nd =  list(train.sort_values(by=['target']).head(20).iloc[0:20]['excerpt'])","becb66a5":"words_in_lowest = [lowest_text.lower().split() for lowest_text in d]","8c4fd285":"# stop_words = set(stopwords.words('english'))","7f0f2b0f":"# cleaned_words_in_lowest = [[word for word in words if not word in stop_words]\n#               for words in words_in_lowest]","99de3b39":"# List of all words \nall_words= list(itertools.chain(*words_in_lowest))\n\ncounts_no_words = collections.Counter(all_words)\n\nlowest_count = counts_no_words.most_common(50)\ncounts_no_words.most_common(50)","df785847":"clean_lowest = pd.DataFrame(lowest_count,\n                             columns=['words', 'count'])\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot horizontal bar graph\nclean_lowest.sort_values(by='count').plot.barh(x='words',\n                      y='count',\n                      ax=ax,\n                      color=\"grey\")\n\nax.set_title(\"Common Words Found in Lowest Target Excerpt (With Stop Words)\")\n\nplt.show()","7f464b00":" train.sort_values(by=['target']).head().iloc[2]['excerpt']\n","ae6db529":" train.sort_values(by=['target']).head().iloc[4]['excerpt']\n","462b8dfc":"train.sort_values(by=['target'],ascending=False).head().iloc[0]['excerpt']","093de223":"h =  list(train.sort_values(by=['target'],ascending=False).head(20).iloc[0:20]['excerpt'])","0fb958ab":"words_in_highest = [higest_text.lower().split() for higest_text in h]","2219b596":"all_words= list(itertools.chain(*words_in_highest))\n\ncounts_no_words = collections.Counter(all_words)\n\nhighest_count = counts_no_words.most_common(50)\ncounts_no_words.most_common(50)","a6c90c87":"clean_highest = pd.DataFrame(highest_count,\n                             columns=['words', 'count'])\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot horizontal bar graph\nclean_highest.sort_values(by='count').plot.barh(x='words',\n                      y='count',\n                      ax=ax,\n                      color=\"pink\")\n\nax.set_title(\"Common Words Found in Highest Target Excerpt (With Stop Words)\")\n\nplt.show()","f2661c92":"import seaborn as sns\n\nsns.heatmap(train.isnull(), cbar=False,cmap=\"Blues\")","631d2564":"keywords = ['while', 'which', 'as', 'been','its','through']\n\n\ntrain_count_keyword = train['excerpt'].str.count('|'.join(keywords))\ntest_count_keyword = test['excerpt'].str.count('|'.join(keywords))\n\n","921a935c":"text = train['excerpt'][0]","5e55e4f2":"import spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nspacy.displacy.render(doc, style='dep')","566f78f8":"print(nlp.pipe_names)\n\nfor ent in doc.ents:\n    print(ent.text, ent.label_)","9e226ac4":"spacy.displacy.render(doc, style='ent', jupyter=True)","828f2090":"with nlp.disable_pipes():\n    train_array = np.array([nlp(text).vector for text in train.excerpt])\n    test_array = np.array([nlp(text).vector for text in test.excerpt])","f7c7c43d":"y_train = np.array(train.target)","2031cf21":"x_train = np.column_stack((train_array,train_count_keyword))\nx_test = np.column_stack((test_array,test_count_keyword))","a0b3907d":"x_train.shape","569fd278":"sdr = ak.StructuredDataRegressor(\n    loss=\"mean_squared_error\",\n    project_name=\"structured_data_regressor\",\n    max_trials=20,\n    objective=\"val_loss\",\n    overwrite=False,\n    seed=5)","35f6d950":"sdr.fit(\n    x_train, y_train, epochs=100, validation_split=0.2,verbose=2)","900a5cb0":"predicted_test = sdr.predict(x_test)","c8981f1b":"predicted_test","ab2f14b1":"submission['target'] = predicted_test","0a0d172e":"submission.to_csv('submission.csv', index=False)","70a716fa":"# # Initialize the text regressor.\n# regressor = ak.TextRegressor(overwrite=True, max_trials=8)  \n# # Feed the text regressor with training data.\n# split = round(len(x_train)*0.85)\n# x_val = x_train[split:]\n# y_val = y_train[split:]\n# x_train = x_train[:split]\n# y_train = y_train[:split]\n# regressor.fit(\n#     x_train,\n#     y_train,\n#     epochs=20,\n#     # Use your own validation set.\n#     validation_data=(x_val, y_val),\n# )\n","603751f9":"# input_node = ak.TextInput()\n# output_node = ak.TextBlock(block_type=\"sequence\")(input_node)\n# output_node = ak.RegressionHead()(output_node)\n# reg = ak.AutoModel(\n#     inputs=input_node, outputs=output_node, overwrite=True, max_trials=9\n# )\n# reg.fit(x_train, y_train, epochs=100)","e0b0fbb9":"# input_node = ak.TextInput()\n# output_node = ak.TextToIntSequence()(input_node)\n# #do not have to feed the max_features as we use TextToIntSequence\n# output_node = ak.Embedding()(output_node)\n# # Use separable Conv layers\n# output_node = ak.ConvBlock(separable=True)(output_node)\n# output_node = ak.RegressionHead()(output_node)\n# reg = ak.AutoModel(\n#     inputs=input_node, outputs=output_node, overwrite=True, max_trials=8\n# )\n# reg.fit(x_train, y_train, epochs=100)","6d5c0231":"# auto_predict = reg.predict(x_test)\n# submission['target'] = auto_predict","1d41ed19":"\n# submission.to_csv('submission.csv', index=False)","f0379c3d":"### Parse Text for features","d10f0c40":"### Let's examine the sample with the lowest target value ","edb7a8b0":"### Visualize and see whether the pretrained entity helps in context\n","f0228877":"### Test how the Spacy works by visulization\n","9936b9f8":"### Customize search space - Automodel \nConfigure blocks for standard ConvNets aka Vanilla ConvNets(standard backpropagation).\n(The following will take a bit more time to run)","191d0949":"Awesome spacy tutorial - https:\/\/course.spacy.io\/en\/","6d4b1814":"### Count keywords feature ","ab7b028c":"### Let's read a couple more ones. ","1829a499":"### TextRegressor\nOne can directly use the Text regressor. Basically user only needs to define the input data.","198042e8":"### Import neccessary library","c2d665cb":"### Examine the missing value\n","2fede2c3":"An assumption made here is that the one with lowest target score seems to have a lot of sub sentence (child sentence) or possessive such as \"its\" or \";\" or perfect tense. ","d0e75501":"### Autokeras - as the name suggest is a system based on Keras. \nThe configuration is similar to keras - one can essentially configure nodes and layers.\nThis notebook is a try out to see how it performs as a benchmark.","e4edac6d":"Only the url_legal and license got missing values. However, the missing percentage is huge.","07591f13":"### Let's see the one with the highest target score","373780be":"### StructuredDataRegressor\nHere we initiate the baseline StructuredDataRegressor. For faster processing we set the max_trials to 20.\n(Note - in this notebook experiment the baseline structured data regressor performs better than directly feeding text input for TextBlock in the following part of the notebook).","14150c66":"### Examine the whole text of the lowest target value","1398afee":"### TextBlock\nOne can directly use the TextBlock and specifying the block_type (can be sequence, ngram, transformer or just none then it'd be tunned automatically). Further more, we can also feed the pretrained word embedding (if we have it) into the TextBlock","42837e68":"### Combine the additional feature (keywords) with the spacy features","e89db752":"Comment out the stopwords and decide not to clean out the text - assuming there is actual difference between these stopwords and pre-clean context of the high and low target excerpt group","971de113":"### Let's do an experiment with the top 20 lowest target text "}}