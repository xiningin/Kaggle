{"cell_type":{"1e771c91":"code","07de4da5":"code","26777523":"code","ab99ffd9":"code","dc177fe5":"code","92d337f7":"code","4394659c":"code","4106e1e6":"code","99d5c87a":"code","90bd2fcf":"code","69525941":"code","76f413bd":"code","38c126de":"code","bf1214fd":"code","37d8685a":"code","26946891":"code","67d50780":"code","d9245380":"code","8e5d5aa7":"code","923eef43":"code","d9c124a6":"code","5f9a0fe3":"code","308c56f3":"code","89fa94b8":"code","00723686":"code","6b7568bd":"code","e5f3e635":"code","dce17b48":"code","98490368":"code","fc249ca3":"code","ee45204b":"code","46f32740":"code","355528cd":"code","76a2b53e":"code","edb25e7b":"code","6c841b27":"code","fc55b21c":"code","443ffcb6":"code","6e9f2c0c":"code","9ef5a8bc":"markdown","c86d2831":"markdown","7f542b18":"markdown","e697c7c3":"markdown","c0a1ce36":"markdown","5769edd3":"markdown","c6623762":"markdown","f7dfad49":"markdown","c54446ef":"markdown","20309850":"markdown","7bc2ebae":"markdown","f870e4ac":"markdown","2afbb2d0":"markdown","50ff2ccb":"markdown"},"source":{"1e771c91":"!pip install efficientnet","07de4da5":"import tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\nfrom tensorflow.image import *\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nimport tensorflow_addons as tfa \n\nimport matplotlib.pyplot as plt\nimport efficientnet.tfkeras as efn\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport math\n\nimport wandb","26777523":"#Some Pre-processing\nbase_path='..\/input\/siim-isic-melanoma-classification\/'\ntrain = pd.read_csv(os.path.join(base_path, 'train.csv'))\ntest = pd.read_csv(os.path.join(base_path, 'test.csv'))\nsample = pd.read_csv(os.path.join(base_path, 'sample_submission.csv'))\ntrain_img_path = base_path+'jpeg\/train\/'\ntest_img_path = base_path+'jpeg\/test\/'\n\nEPOCHS=30\nBATCH_SIZE=10\ninput_shape=(512, 512, 3)\nlr=1e-3","ab99ffd9":"train.head()","dc177fe5":"#Basic Helper Functions\n\ndef get_model(shape, weights):\n    \n   \n    input = Input(shape=shape)\n\n    base_model = efn.EfficientNetB3(weights=weights,include_top=False, input_shape=shape)\n    base_model.trainable = True\n    \n    output = base_model(input)\n    output = GlobalMaxPooling2D()(output)\n    output = Dense(256)(output)\n    output = LeakyReLU(alpha = 0.25)(output)\n    output = Dropout(0.25)(output)\n\n    output = Dense(16,activation=\"relu\")(output)\n    output = Dropout(0.15)(output)\n\n    output = Dense(1,activation=\"sigmoid\")(output)\n    \n    model = Model(input,output)\n    \n    return model\n    \ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n\n        pt_1 = K.clip(pt_1, 1e-3, .999)\n        pt_0 = K.clip(pt_0, 1e-3, .999)\n\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n\n    return focal_loss_fixed\n\ndef get_cosine_schedule_with_warmup(lr,num_warmup_steps, num_training_steps, num_cycles=0.75):\n  \n    def lrfn(epoch):\n        if epoch < num_warmup_steps:\n            return (float(epoch) \/ float(max(1, num_warmup_steps))) * lr\n\n        progress = float(epoch - num_warmup_steps ) \/ float(max(1, num_training_steps - num_warmup_steps))\n\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr\n\n    return tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nlr_schedule= get_cosine_schedule_with_warmup(lr=0.00004,num_warmup_steps=4, num_training_steps=EPOCHS)\n\ndef lrfn2(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr","92d337f7":"train.columns = [ 'img_name', 'id', 'sex', 'age', 'location', 'diagnosis',\n    'benign_malignant', 'target'\n]\ntest.columns = ['img_name', 'id', 'sex', 'age', 'location']","4394659c":"print(train['location'].value_counts())\nprint(\"Number of NA values in location\", train['location'].isna().sum())","4106e1e6":"for df in [train, test]:\n    df['location'].fillna('unknown', inplace=True) #Replacing","99d5c87a":"# Filling age and sex with appropriate values.\n\ntrain['sex'].fillna(train['sex'].mode()[0], inplace=True)\ntrain['age'].fillna(train['age'].median(), inplace=True)\nprint(\n    f'Train missing value count: {train.isnull().sum().sum()}\\nTest missing value count: {train.isnull().sum().sum()}'\n)","90bd2fcf":"train['location'].value_counts()","69525941":"location='head\/neck'    #Containing images with locations as 'Head\/Neck'\ntrain_neck_head = train.loc[train['location']==location]\ntrain_neck_head.head()","76f413bd":"'''\nAppending '.jpg' in front of each image name so that it can be used by our Train Generator\n'''\n\ndef append_ext(fn):\n    return fn+\".jpg\"\n\ntrain_neck_head[\"img_name\"]=train_neck_head[\"img_name\"].apply(append_ext)\ntrain_neck_head[\"target\"] = train_neck_head['target'].astype(\"str\")\ntrain_neck_head.head()","38c126de":"aug = ImageDataGenerator(width_shift_range=0.5,\n    height_shift_range=0.5, shear_range=0.5, zoom_range=0.5,\n    channel_shift_range=0.5, rescale=1\/255, validation_split=0.25)\n\nimage_gen = aug.flow_from_dataframe(dataframe=train_neck_head, directory=train_img_path, x_col=\"img_name\",\ny_col=\"target\", subset=\"training\",\nbatch_size=BATCH_SIZE,\nseed=42,\nshuffle=True,\nclass_mode=\"binary\",\ntarget_size=(300,300))\n\nimage_gen.class_indices","bf1214fd":"model_head_neck = get_model(input_shape, 'imagenet')  #Can also use noisy-student weights\nmodel_head_neck.summary()","37d8685a":"#Compile the Model\n'''\nUsing Focal loss due to imbalance\n'''\n\nmodel_head_neck.compile(\n        optimizer='adam',\n        loss = tfa.losses.SigmoidFocalCrossEntropy(reduction=tf.keras.losses.Reduction.AUTO),\n        metrics=['accuracy',tf.keras.metrics.AUC()]\n    )","26946891":"filepath = 'EffNetB3-Head-Neck.h5'\nmc = tf.keras.callbacks.ModelCheckpoint(filepath=filepath , monitor='loss', save_weights_only=False, save_model=True, save_best_only=True)\n#lr_callback2 = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n\ncallbacks = [mc, lr_schedule]","67d50780":"history = model_head_neck.fit(image_gen,\n          epochs=2,    #Increase the Number of Epochs\n          batch_size=BATCH_SIZE,          \n          verbose=1,\n          steps_per_epoch=math.ceil(len(train_neck_head)\/\/BATCH_SIZE),\n          callbacks=callbacks)","d9245380":"location='lower extremity'\ntrain_lower_ex = train.loc[train['location']==location]\nNUM_SAMPLES = len(train_lower_ex)\ntrain_lower_ex.head()","8e5d5aa7":"def append_ext(fn):\n    return fn+\".jpg\"\n\ntrain_lower_ex[\"img_name\"]=train_lower_ex[\"img_name\"].apply(append_ext)\ntrain_lower_ex[\"target\"] = train_lower_ex['target'].astype(\"str\")\ntrain_lower_ex.head()","923eef43":"aug = ImageDataGenerator(width_shift_range=0.5,\n    height_shift_range=0.5, shear_range=0.5, zoom_range=0.5,\n    channel_shift_range=0.5, rescale=1\/255, validation_split=0.25)\n\nimage_gen = aug.flow_from_dataframe(dataframe=train_lower_ex, directory=train_img_path, x_col=\"img_name\",\ny_col=\"target\", subset=\"training\",\nbatch_size=BATCH_SIZE,\nseed=42,\nshuffle=True,\nclass_mode=\"binary\",\ntarget_size=(300,300))\n\nimage_gen.class_indices","d9c124a6":"model_lower_ex = get_model(input_shape, 'imagenet')  #Can also use noisy-student weights\nmodel_lower_ex.summary()","5f9a0fe3":"#Compile the Model\nmodel_lower_ex.compile(\n        optimizer='adam',\n        loss = tfa.losses.SigmoidFocalCrossEntropy(reduction=tf.keras.losses.Reduction.AUTO),\n        metrics=['accuracy',tf.keras.metrics.AUC()]\n    )","308c56f3":"filepath = 'EffNetB3-Lower-extremity.h5'\nmc = tf.keras.callbacks.ModelCheckpoint(filepath=filepath , monitor='loss', save_weights_only=False, save_model=True, save_best_only=True)\nlr_callback2 = tf.keras.callbacks.LearningRateScheduler(lrfn2, verbose = True)\n\ncallbacks = [mc, lr_schedule]","89fa94b8":"history = model_lower_ex.fit(image_gen,\n          epochs=2,\n          batch_size=BATCH_SIZE,          \n          verbose=1,\n          steps_per_epoch=math.ceil(NUM_SAMPLES\/\/BATCH_SIZE),\n          callbacks=callbacks)","00723686":"from PIL import Image\n#from tensorflow.keras.preprocessing.image import img_to_array\n\ndef prepare_img(path, name, target_size):\n    try:\n        img = Image.open(path+name)\n    except:\n        return \"File not found\"\n        \n    img_arr = img_to_array(img)\n    img_arr = resize(img_arr, target_size)\n    img_arr = img_arr\/255\n    img_arr = np.expand_dims(img_arr, axis=0)\n    \n    return img_arr","6b7568bd":"location='head\/neck'\ntest_head_neck = test.loc[test['location']==location]\nprint(len(test_head_neck))\ntest_head_neck.head()","e5f3e635":"#Same Procedure\n\ndef append_ext(fn):\n    return fn+\".jpg\"\n\ntest_head_neck[\"img_name\"]=test_head_neck[\"img_name\"].apply(append_ext)\ntest_head_neck.head()","dce17b48":"model_head_neck = load_model('EffNetB3-Head-Neck.h5')","98490368":"preds_head=[]\nimg_name = []\nimport random\nfor img in list(test_head_neck['img_name']):\n    img_name.append(img.split('.')[0])\n    img = prepare_img(test_img_path, img, (512, 512))\n    preds_head.append(model_head_neck.predict(img)[0][0])\n    #preds_head.append(random.choice([0,1]))\n    \nlen(preds_head)","fc249ca3":"df_preds_head = pd.DataFrame(list(zip(img_name, preds_head)), columns=['image_name', 'target'])\ndf_preds_head.head()","ee45204b":"df_preds_head.to_csv(sub_base_path+'preds-head-neck.csv', index=False)","46f32740":"# lower extremity\nlocation='lower extremity'\ntest_lower = test.loc[test['location']==location]\ntest_lower.head()","355528cd":"def append_ext(fn):\n    return fn+\".jpg\"\n\ntest_lower[\"img_name\"]=test_lower[\"img_name\"].apply(append_ext)\ntest_lower.head()","76a2b53e":"model_lower = load_model('EffNetB3-Lower-extremity.h5')","edb25e7b":"preds_lower=[]\nimg_name = []\nfor img in list(test_lower['img_name']):\n    img_name.append(img.split('.')[0])\n    img = prepare_img(test_img_path, img, (512, 512))\n    preds_lower.append(model_lower.predict(img)[0][0])\n    #preds_head.append(random.choice([0,1]))\n    \nlen(preds_lower)","6c841b27":"df_preds_lower = pd.DataFrame(list(zip(img_name, preds_lower)), columns=['image_name', 'target'])\ndf_preds_lower.head()","fc55b21c":"df_preds_lower.to_csv(sub_base_path+'preds_lower.csv', index=False)","443ffcb6":"df_preds_combined = df_preds_upper.append([df_preds_unknown, df_preds_torso, df_preds_palms,\n                           df_preds_oral, df_preds_lower, df_preds_head], ignore_index=True)\nprint(len(df_preds_combined))\ndf_preds_combined.head()","6e9f2c0c":"del sample['target']\nsample = sample.merge(df_preds_combined, on='image_name')\nsample.to_csv('Submissions\/individual1.csv', index=False)\nsample.head()","9ef5a8bc":"I will be implementing this approach for 2 body parts. Following this, you can easily extend it to all the other parts","c86d2831":"The idea behind this approach is that instead of jumping to directly classifying images into Melignant\/ Benign, it might fruitful to check that melanoma images for different parts of the body are different in appearance. For instance, the metadata gives us different locations such as **Torso, Oral\/Genital, Lower extremity, Upper extremity, Palms\/Soles** for each image . Given this, we can try to make a separate classifier for images belonging to different parts of the body.","7f542b18":"Continuing in a similar fashion, we can get a Dataframe (and hence a CSV file) containing predictions of test images belonging to a particular location","e697c7c3":"# Imputing Missing Data","c0a1ce36":"# Predictions","5769edd3":"## 2. Lower Extremity","c6623762":"# Selecting images by Body Parts\n\n## 1. Head\/Neck","f7dfad49":"## 2. Lower Extremity","c54446ef":"We have imputed the missing locations with a new label **'unknown'**","20309850":"## 1. Head\/Neck Test Images","7bc2ebae":"The metadata contains some missing values. So it is necessary to impute them.","f870e4ac":"# Combining all the Prediction","2afbb2d0":"If you continue to repeat this process for all the parts, we will get 7 different models for 7 different body locations. After these models have been trained to satisfaction, we can test these on the test set in a similar manner i.e. 'model_head_neck' for test images having location 'head\/neck'","50ff2ccb":"Thanks for reading this kernel. I would love to read your thoughts on this approach. \n\n### This is my first competition and also my first notebook on Kaggle. Please upvote if you found it useful. \n\nTHANKS :)\n"}}