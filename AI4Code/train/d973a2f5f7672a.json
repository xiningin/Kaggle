{"cell_type":{"e6074588":"code","ec9a46b9":"code","03d5d19b":"code","7479a6f6":"code","cde2a147":"code","db0d09f2":"code","8f4bd3c8":"code","4f35151e":"code","d9496059":"code","9283b659":"code","495dfc73":"code","29c6340b":"code","f11dc5a3":"code","52fbb54b":"code","0f4dea7a":"code","aa632e00":"code","bccf51ca":"code","7a411b04":"code","3d666684":"code","16173dc6":"code","cddfce21":"code","b6febcdf":"code","8c0c05ea":"code","ad005f82":"code","f4c19687":"code","4a0b6b9e":"code","47d86230":"code","1ee460ec":"code","7110eb30":"code","3b9099be":"code","d808527b":"code","fd7afdd9":"code","cfe4f944":"code","2a5dc5e1":"code","96176caa":"code","e6f008c3":"code","bb28d209":"code","bd03c321":"code","e1d18a4d":"code","5c09057e":"code","b8ad943e":"code","3668d016":"code","8bc8fc57":"code","10a9c0f2":"code","f01ef120":"code","2434d9e6":"code","fc50f580":"code","666d711a":"code","bd3d609b":"code","f0c8dbe8":"code","6f9d6632":"code","ec03f125":"code","5d999859":"code","c6ab5710":"code","32691b73":"code","f7c917ca":"code","e8ba3dad":"code","b9c4cf56":"markdown","09b06ef9":"markdown","9800389a":"markdown","54c3a1ef":"markdown","49a1d1f6":"markdown","0f023bfc":"markdown","8180c554":"markdown","02609fd4":"markdown","05322029":"markdown","7f763cd7":"markdown"},"source":{"e6074588":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec9a46b9":"housing = pd.read_csv('\/kaggle\/input\/california-housing-prices\/housing.csv')","03d5d19b":"housing","7479a6f6":"housing['ocean_proximity'].value_counts()","cde2a147":"import matplotlib.pyplot as plt\nhousing.hist(bins=50,figsize=(20,15))","db0d09f2":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size = 0.2, random_state = 42)","8f4bd3c8":"housing['income_cat'] = pd.cut(housing['median_income'], bins = [0,1.5,3,4.5,6, np.inf],labels = [1,2,3,4,5])","4f35151e":"housing","d9496059":"# split.split() returns indicies for training and test set\nfrom sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","9283b659":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","495dfc73":"housing = strat_train_set.copy()","29c6340b":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)","f11dc5a3":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\ns=housing[\"population\"]\/100, label=\"population\", figsize=(10,7),\nc=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n)\nplt.legend()","52fbb54b":"# how much each attribute correlates with the median house value\ncorr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","0f4dea7a":"# Another way to check for correlation between attributes is to use the pandas scatter_matrix() function\nfrom pandas.plotting import scatter_matrix\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n\"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))","aa632e00":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"] = housing[\"population\"]\/housing[\"households\"]","bccf51ca":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","7a411b04":"# let\u2019s revert to a clean training set\nhousing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","3d666684":"# Dealing with missing values:\n#housing.dropna(subset=[\"total_bedrooms\"]) # option 1\n#housing.drop(\"total_bedrooms\", axis=1) # option 2\nmedian = housing[\"total_bedrooms\"].median() # option 3\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)","16173dc6":"housing.isna().sum()","cddfce21":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")\nhousing_num = housing.drop(\"ocean_proximity\", axis=1) #Since the median can only be computed on numerical attributes\nimputer.fit(housing_num) # This only computes the median of each attribute and stores the result in its statistics_ instance variable\nX = imputer.transform(housing_num) # Result of this is a numpy array\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns,\nindex = housing_num.index)","b6febcdf":"# Categorical variables:\nhousing_cat = housing[[\"ocean_proximity\"]]\nfrom sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:10]","8c0c05ea":"# To get the list of categories:\nordinal_encoder.categories_","ad005f82":"# Employing one-Hot Encoding:\nfrom sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot # output is a SciPy sparse matrix, instead of a NumPy array.","f4c19687":"housing_cat_1hot.toarray()","4a0b6b9e":"from sklearn.base import BaseEstimator, TransformerMixin\nrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room \n    def fit(self, X,y=None):\n        return self # nothing else to do \n    def transform(self, X,y=None):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, household_ix] \n        population_per_household = X[:, population_ix] \/ X[:, household_ix] \n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix] \n            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","47d86230":"# a small pipeline for the numerical attributes\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nnum_pipeline = Pipeline([\n('imputer', SimpleImputer(strategy=\"median\")),\n('attribs_adder', CombinedAttributesAdder()),\n('std_scaler', StandardScaler()),\n])\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","1ee460ec":"# Using column transformer:\n\nfrom sklearn.compose import ColumnTransformer\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\nfull_pipeline = ColumnTransformer([\n(\"num\", num_pipeline, num_attribs),\n(\"cat\", OneHotEncoder(), cat_attribs),\n])\nhousing_prepared = full_pipeline.fit_transform(housing)","7110eb30":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","3b9099be":"some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))","d808527b":"print(\"Labels:\", list(some_labels))","fd7afdd9":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","cfe4f944":"from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)\n\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","2a5dc5e1":"tree_reg.predict(housing_prepared)","96176caa":"housing_labels","e6f008c3":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\nscoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","bb28d209":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\ndisplay_scores(tree_rmse_scores)","bd03c321":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\nscoring=\"neg_mean_squared_error\", cv=10)\n\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","e1d18a4d":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)\nforest_predictions = forest_reg.predict(housing_prepared)\nforest_rmse = mean_squared_error(housing_labels, forest_predictions,squared=False)\n\n#display_scores(forest_rmse_scores)","5c09057e":"lin_scores_forest = cross_val_score(forest_reg, housing_prepared, housing_labels,\nscoring=\"neg_mean_squared_error\", cv=10)\n\nforest_rmse_scores = np.sqrt(-lin_scores_forest)\ndisplay_scores(forest_rmse_scores)","b8ad943e":"from sklearn.model_selection import GridSearchCV\nparam_grid = [\n{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\nscoring='neg_mean_squared_error',\nreturn_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)\ngrid_search.best_params_","3668d016":"grid_search.best_estimator_","8bc8fc57":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","10a9c0f2":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","f01ef120":"# Let\u2019s display these importance scores next to their corresponding attribute names:\nextra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","2434d9e6":"final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse) # => evaluates to 47,730.2","fc50f580":"from scipy import stats\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\nloc=squared_errors.mean(),\nscale=stats.sem(squared_errors)))","666d711a":"from sklearn.svm import SVR\nmodel_svm = SVR(kernel = 'linear', C=1.0, epsilon=0.2)\nmodel_svm.fit(housing_prepared, housing_labels)","bd3d609b":"svm_predictions = model_svm.predict(housing_prepared)","f0c8dbe8":"mean_squared_error(housing_labels, svm_predictions, squared = False)","6f9d6632":"# Doing a grid search\nparam_grid_svm = [\n{'kernel': ['linear'], 'C': [0.5,1]},\n{'kernel': ['rbf'], 'C': [1,2]}\n]\nmodel_svm = SVR()\ngrid_search_svm = GridSearchCV(model_svm, param_grid_svm, cv=5,\nscoring='neg_mean_squared_error',\nreturn_train_score=True)\ngrid_search_svm.fit(housing_prepared, housing_labels)\ngrid_search_svm.best_params_","ec03f125":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n    ]\n\nsvm_reg = SVR()\ngrid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\ngrid_search.fit(housing_prepared, housing_labels)","5d999859":"from sklearn.model_selection import RandomizedSearchCV\nrand_reg = RandomForestRegressor()\n\nparam_grid = [\n{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n]\n\nrand_search = RandomizedSearchCV(rand_reg, param_grid, cv=10, n_iter = 50, verbose = 2, n_jobs = -1, random_state=42,\nscoring='neg_mean_squared_error',\nreturn_train_score=True)\nrand_search.fit(housing_prepared, housing_labels)\nrand_search.best_params_","c6ab5710":"# a small pipeline for the numerical attributes\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nnum_pipeline = Pipeline([\n('imputer', SimpleImputer(strategy=\"median\")),\n('attribs_adder', CombinedAttributesAdder()),\n('std_scaler', StandardScaler()),\n])\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","32691b73":"# Using column transformer:\n\nfrom sklearn.compose import ColumnTransformer\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\nfull_pipeline = ColumnTransformer([\n(\"num\", num_pipeline, num_attribs),\n(\"cat\", OneHotEncoder(), cat_attribs),\n])\nhousing_prepared = full_pipeline.fit_transform(housing)","f7c917ca":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","e8ba3dad":"# Let\u2019s display these importance scores next to their corresponding attribute names:\nextra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","b9c4cf56":"# Kfold","09b06ef9":"**To see the RMSE:**","9800389a":"# 3. Adding a transformer","54c3a1ef":"**Let\u2019s compute the same scores for the Linear Regression model just to be sure:**","49a1d1f6":"\n**RandomForestRegressor:**","0f023bfc":"# Exercises","8180c554":"**Let\u2019s train a DecisionTreeRegressor**","02609fd4":"# Grid search","05322029":"# **Training and Evaluating on the Training Set**","7f763cd7":"# Randomized search"}}