{"cell_type":{"53a948e2":"code","fff0ba67":"code","24ef943b":"code","980cee15":"code","9af34fdc":"code","63310c74":"code","8b5ef8c5":"code","68658803":"code","b2e861f5":"code","f8e77136":"code","03f7ba60":"code","4d8642dd":"code","04352825":"code","7a14702f":"code","494450bb":"code","342dc6c5":"code","94dcb9b4":"code","1a23abe4":"code","8b70a47a":"code","0ad4fbf0":"code","6aeefd9c":"code","58256e80":"code","1c1429ae":"code","a5e644e7":"code","3f9b8ce2":"code","983596e7":"code","7c2bbe7a":"code","2376cf58":"code","f670ac64":"code","927feb9e":"code","0ecc32ab":"code","79547c77":"code","3eca61d8":"code","e0c4defb":"code","94f19f53":"code","db5a9b98":"code","1109f7a2":"code","a7e9a498":"code","162c4cc6":"code","7c7f1631":"markdown","0a989b08":"markdown","78f12761":"markdown","28870750":"markdown","57b30f70":"markdown","eaf5ccbe":"markdown","fbead7b4":"markdown","f5671375":"markdown","fc3fd2ae":"markdown","d9b1f786":"markdown","fe9efd2e":"markdown"},"source":{"53a948e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fff0ba67":"import re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()","24ef943b":"data_eng1 = pd.read_excel(\"..\/input\/Unstructured Data English.xlsx\")\ndata_eng = pd.read_excel(\"..\/input\/Unstructured Data English.xlsx\")\n","980cee15":"data_eng.drop([\"Unnamed: 2\",\"Unnamed: 3\"],axis = 1,inplace=True)","9af34fdc":"data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].str.lower()","63310c74":"data_eng['Katakana text Translated'].head()","8b5ef8c5":"\n\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r'\\W',' ',str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"http\\S+\", \"\", str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r'http\\s+|www.\\s+',r'', str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+',' ',str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r'\\^[a-zA-Z]\\s+',' ',str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r'\\s+',' ',str(x)))\n\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\\u2019\", \"\\'\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"isn\\'t\", \"is not\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"won\\'t\", \"will not\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"can\\'t\", \"can not\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"don\\'t\", \"do not\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"dont\", \"do not\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"ain\\'t\", \"are not\", str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"n\\'t\", \" not\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\'re\", \" are\", str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\'s\", \" is\", str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\\u2019d\", \" would\", str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\d\", \" would\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\'ll\", \" will\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\'t\", \" not\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\'ve\", \" have\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\'m\", \" am\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\n\", \"\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\r\", \"\", str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"[0-9]\", \"digit\", str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\'\", \"\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\\"\", \"\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r'[?|!|\\'|\"|#|&|;|@|:]',r'', str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r'[.|,|)|(|\\|\/]',r' ', str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].apply(lambda x: remove_stopwords(x))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"amp\", \"\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"[0-9]([a][m]|[p][m])\", \"\", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"-\", \" \", str(x)))\ndata_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r'<.*?>', \"\", str(x)))","68658803":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","b2e861f5":"data_eng['Katakana text Translated'].head(1)","f8e77136":"corpus = []\nfor x in range(0,len(data_eng['Katakana text Translated'])):\n    #rem_num = re.sub('[^A-Za-z]',\" \",data_eng['Katakana text Translated'][x])\n    #rem_punct = \"\".join([word.lower() for word in rem_num if word not in string.punctuation])\n    tokens = re.split('\\W+',data_eng['Katakana text Translated'][x])\n    noStop = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n    noStop = \" \".join(tokens)\n    corpus.append(noStop)","03f7ba60":"data_eng['Katakana text Translated'][15]\n","4d8642dd":"data_eng1['Katakana text Translated'][15]","04352825":"corpus","7a14702f":"from sklearn.feature_extraction.text import CountVectorizer","494450bb":"\ndef get_top_n_words(corpus, n=20,k=1):     \n    vec = CountVectorizer(ngram_range=(k,k),stop_words = 'english').fit(corpus)     \n    bag_of_words = vec.transform(corpus)     \n    sum_words = bag_of_words.sum(axis=0)      \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]     \n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) \n    return words_freq[:n]\nget_top_n_words(corpus)","342dc6c5":"def get_top_n_words(corpus, n=12,k=7):     \n    vec = CountVectorizer(ngram_range=(k,k),stop_words = 'english').fit(corpus)     \n    bag_of_words = vec.transform(corpus)     \n    sum_words = bag_of_words.sum(axis=0)      \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]     \n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) \n    return words_freq[:n]\nget_top_n_words(corpus)","94dcb9b4":"data_eng2 = pd.read_excel(\"..\/input\/Unstructured Data English.xlsx\")","1a23abe4":"data_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].str.lower()\n","8b70a47a":"\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r'<.*?>', \"\", str(x)))","0ad4fbf0":"data_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"http\\S+\", \"\", str(x)))","6aeefd9c":"data_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub('\\\\n',' ',str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r'\\W',' ',str(x)))\n#data_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"http\\S+\", \"\", str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r'http\\s+|www.\\s+',r'', str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+',' ',str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r'\\^[a-zA-Z]\\s+',' ',str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r'\\s+',' ',str(x)))\n\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"\\\u2019\", \"\\'\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"isn\\'t\", \"is not\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"won\\'t\", \"will not\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"can\\'t\", \"can not\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"don\\'t\", \"do not\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"dont\", \"do not\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"ain\\'t\", \"are not\", str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"n\\'t\", \" not\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"\\'re\", \" are\", str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\'s\", \" is\", str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\\u2019d\", \" would\", str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"\\d\", \" would\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"\\'ll\", \" will\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"\\'t\", \" not\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"\\'ve\", \" have\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"\\'m\", \" am\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"\\n\", \"\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"\\r\", \"\", str(x)))\n#data_eng['Katakana text Translated'] = data_eng['Katakana text Translated'].map(lambda x: re.sub(r\"[0-9]\", \"digit\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"\\'\", \"\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"\\\"\", \"\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r'[?|!|\\'|\"|#|&|;|@|:]',r'', str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r'[.|,|)|(|\\|\/]',r' ', str(x)))\n\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"amp\", \"\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"[0-9]([a][m]|[p][m])\", \"\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"-\", \" \", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"[0-9]\", \"\", str(x)))\ndata_eng2['Katakana text Translated'] = data_eng2['Katakana text Translated'].map(lambda x: re.sub(r\"[_]\", \"\", str(x)))\n","58256e80":"corpus1 = []\nfor x in range(0,len(data_eng2['Katakana text Translated'])):\n    #rem_num = re.sub('[^A-Za-z]',\" \",data_eng2['Katakana text Translated'][x])\n    rem_punct = \"\".join([word.lower() for word in data_eng2['Katakana text Translated'][x] if word not in string.punctuation])\n    tokens = re.split('\\W+',data_eng2['Katakana text Translated'][x])\n    noStop = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n    noStop = \" \".join(noStop)\n    corpus1.append(noStop)","1c1429ae":"tokens","a5e644e7":"vec = CountVectorizer()\nX = vec.fit_transform(corpus1)\ndf = pd.DataFrame(X.toarray(),columns = vec.get_feature_names())\n","3f9b8ce2":"df.drop([\"aaa\",\"aaaand\",\"ab\",\"aarp\",\"abbyrossss\",\"abbyyflores\",\"abcchicago\"],axis = 1,inplace = True)\ndf.head()","983596e7":"df1 = df.T\ndf1.head(100)","7c2bbe7a":"data_eng2['Katakana text Translated'][19]","2376cf58":"text = []\ntext.append(\" \".join([y.lower() for y in corpus1]))\ntext1 = str(text[0]) ","f670ac64":"import matplotlib.pyplot as plt","927feb9e":"from wordcloud import WordCloud\ncloud = WordCloud().generate(text1)\nplt.figure(figsize=(10, 10), facecolor=None)\nplt.imshow(cloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","0ecc32ab":"def get_top_n_words(corpus, n=20,k=1):     \n    vec = CountVectorizer(ngram_range=(k,k),stop_words = 'english').fit(corpus)     \n    bag_of_words = vec.transform(corpus)     \n    sum_words = bag_of_words.sum(axis=0)      \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]     \n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) \n    return words_freq[:n]\nget_top_n_words(corpus1)","79547c77":"data_jap = pd.read_excel(\"..\/input\/Unstructured Data Japanese.xls\")","3eca61d8":"data_jap.head()","e0c4defb":"data_jap['Kraft Super bowl data'] = data_jap['Kraft Super bowl data'].map(lambda x: re.sub(r'[+|?|!|\\'|\"|#|&|;|@|:]',r' ', str(x)))\ndata_jap['Kraft Super bowl data'] = data_jap['Kraft Super bowl data'].map(lambda x: re.sub(r'[.|,|)|(|\\|\/]',r' ', str(x)))","94f19f53":"corpus11 = []\nfor x in range(0,len(data_jap['Kraft Super bowl data'])):\n    rem_num = re.sub('[A-Za-z0-9]',\" \",data_jap['Kraft Super bowl data'][x])\n    corpus11.append(rem_num)","db5a9b98":"corpus11","1109f7a2":"pip3 install mecab-python3\n","a7e9a498":"def tokenize(sentence_str):\n    mt = MeCab.Tagger(\"-Owakati\")\n    wordlist = mt.parse(sentence_str)\n    token_list = wordlist.rstrip(\" \\n\").split(\" \")\n    return token_list","162c4cc6":"token_list_lst = []\nfor x in range(0,len(data_jap['Kraft Super bowl data'])):\n    token_list_lst.append(tokenize(data_jap['Kraft Super bowl data'][x]))","7c7f1631":"Droping unnecessary columns from dataframe","0a989b08":"function to find frequently used phrases","78f12761":"Loadind excel files to dataframe","28870750":"code to generate document term frequency","57b30f70":"importing CountVectorizer","eaf5ccbe":"Tokenization and lemmatization(converting words into orignal form)","fbead7b4":"converting text to lower","f5671375":"function to generate top 10 words ","fc3fd2ae":"Performing regular expressions to clean our data","d9b1f786":"Importing necessary modules","fe9efd2e":"Code to show primary words using WordCloud"}}