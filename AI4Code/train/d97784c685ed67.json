{"cell_type":{"2f830831":"code","98cc9088":"code","7f70a6a6":"code","f9cd52bf":"code","c749c2aa":"code","56bcef60":"code","19287010":"code","2b2a4748":"code","c1af4270":"code","0df52281":"markdown","0130bc44":"markdown","c302c5f5":"markdown","ede4b2af":"markdown"},"source":{"2f830831":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport csv\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","98cc9088":"#Training\n\ndf = pd.read_csv('\/kaggle\/input\/random-linear-regression\/train.csv')\ndf = df.dropna()\ndata = df.values\n\nX = data[:,0]\nY = data[:,1]\n\ntemp = np.dot(X,X)\nB = np.dot(X,Y)\nBeta = B\/temp #Model\n\nprint(Beta)","7f70a6a6":"#Testing\n\ndf_test=pd.read_csv('\/kaggle\/input\/random-linear-regression\/test.csv')\ndf_test = df_test.dropna()\ndata_test = df_test.values\n\nX_test = data_test[:,0]\nY_test = data_test[:,1]\n\nY_cap = X_test * Beta #Predection","f9cd52bf":"#Comparison\n\nrmse = (((Y_cap - Y_test)**2).mean())**0.5\nprint(rmse)\n\nplt.figure(num=None, figsize=(20, 15), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(X_test, Y_test,'go',X_test, Y_cap,'r')","c749c2aa":"import random as rm","56bcef60":"alpha = 0.0001\nbeta = rm.random()\nm = len(Y_cap)\n'''\nfor i in range(50):\n    Y_cap = beta * X\n    temp = np.dot((Y_cap - Y),X)\n    beta = beta - alpha*temp\/m\n    print(\"Beta\",beta)\n    rmse = (((Y_cap - Y)**2).mean())**0.5\n    print(\"RMSE =\",rmse)\n'''\n\nrmse1 = 1000\nrmse2 = 1000\nrmse3 = 1000\n\nwhile True:\n    Y_cap = beta * X\n    temp = np.dot((Y_cap - Y),X)\n    beta = beta - alpha*temp\/m\n    #print(\"Beta\",beta)\n    rmse = (((Y_cap - Y)**2).mean())**0.5\n    #print(\"RMSE =\",rmse)\n    rmse1 = rmse2\n    rmse2 = rmse3\n    rmse3 = rmse\n    temp_mean = (rmse1 +rmse2 +rmse3 )\/3\n    if(abs(temp_mean - rmse) < 0.001):\n        break\nprint(\"Beta\",beta)\nprint(\"RMSE =\",rmse)","19287010":"temp_rmse = 0 \nwhile True:\n    Y_cap = beta * X\n    temp = np.dot((Y_cap - Y),X)\n    beta = beta - alpha*temp\/m\n    #print(\"Beta\",beta)\n    rmse = (((Y_cap - Y)**2).mean())**0.5\n    #print(\"RMSE =\",rmse)\n    if(temp_mean > rmse):\n        break\n    temp_mean = rmse\nprint(\"Beta\",beta)\nprint(\"RMSE =\",rmse)","2b2a4748":"#Testing\n\nY_cap = X_test * beta #Predection","c1af4270":"#Comparison\n\nrmse = (((Y_cap - Y_test)**2).mean())**0.5\nprint(rmse)\n\nplt.figure(num=None, figsize=(20, 15), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(X_test, Y_test,'go',X_test, Y_cap,'r')","0df52281":"# Linear models and Least Squares using iteration\nFinding $\\hat{\\beta}$ using $\\hat{\\beta} = (X^TX)^{\u22121}X^Ty $ equation may take much longer time due to finding od inverse of matrix ($O(n^3)$). If data is larger than 10000 inputs so we need alternate method. <br\/>\n\nThe below method we can use for larger inputs. It takes $O(kn^2)$ time. <br\/>\n\nrepeat until convergence:<br\/>\n{<br\/>\n&nbsp;&nbsp;&nbsp; $ \\beta_j = \\beta_j \u2212 \\alpha \\frac{1}{m} \\sum_{i=1}^p(\\hat{y}(x(i))\u2212y(i))\u22c5x_j(i) $  &nbsp;&nbsp; for j = 0...n <br\/>\n}<br\/>\nUse temporary $\\beta$ in between loop and after 1 iteration is over then simultaneously update all values of $\\beta$ vector. ","0130bc44":"$ \\beta_j = \\beta_j \u2212 \\alpha \\frac{1}{m} \\sum_{i=1}^p(\\hat{y}(x(i))\u2212y(i))\u22c5x_j(i) $  &nbsp;&nbsp; for j = 0...n","c302c5f5":"# Linear models and Least Squares using equation\nGiven a vector of inputs $X^T = (X_1,X_2,...,X_p),$ we predict the output $Y$ via the model <br\/>\n\n$\\hat{Y} = \\hat{\\beta_0} + \\sum_{j=1}^pX_j\\hat{\\beta_j}$<br\/>\n\nThe term $\\hat{\\beta_0}$ is the intercept, also known as the bias in machine learning. Often it is convenient to include the constant variable 1 in $X$, include $\\hat{\\beta_0}$ in the vector of coefficients $\\hat{\\beta}$, and then write the linear model in vector form as an inner product <br\/>\n\n$\\hat{Y} = X^T\\hat{\\beta}$ <br\/>\n\nwhere $X^T$ denotes vector or matrix transpose ($X$ being a column vector). Here we are modeling a single output, so $\\hat{Y}$ is a scalar; in general $\\hat{Y}$ can be a K\u2013vector, in which case $\\beta$ would be a $p \\times K$ matrix of coefficients. In the (p + 1)-dimensional input\u2013output space, $(X, \\hat{Y})$ represents a hyperplane. If the constant is included in $X$, then the hyperplane includes the origin and is a subspace; if not, it is an affine set cutting the $Y$-axis at the point $(0,\\hat{\\beta_0})$. From now on we assume that the intercept is included in $\\hat{\\beta}$.<br\/>\n\nViewed as a function over the p-dimensional input space, $f(X) = X^T\\beta$ is linear, and the gradient $f'(X) = \\beta$ is a vector in input space that points\nin the steepest uphill direction. <br\/>\n\nHow do we fit the linear model to a set of training data? There are many different methods, but by far the most popular is the method of **least squares**. In this approach, we pick the coefficients \u03b2 to minimize the residual sum of squares <br\/>\n\n$RSS(\\beta) = \\sum_{i=1}^N(y_i \u2212 x^T_i\\beta)^2$ <br\/>\n\n$RSS(\\beta)$ is a quadratic function of the parameters, and hence its minimum always exists, but may not be unique. The solution is easiest to characterize\nin matrix notation. We can write <br\/>\n\n$RSS(\\beta) = (y \u2212 X\\beta)^T(y \u2212 X\\beta)$ <br\/>\n\nwhere $X$ is an $N \\times p$ matrix with each row an input vector, and y is an N-vector of the outputs in the training set. Differentiating w.r.t. $\\beta$ we get the normal equations <br\/>\n\n$X^T(y \u2212 X\\beta) = 0$ <br\/>\n\nIf $X^TX$ is nonsingular, then the unique solution is given by <br\/>\n\n$\\hat{\\beta} = (X^TX)^{\u22121}X^Ty $ <br\/>\n\nand the fitted value at the ith input $x_i$ is $\\hat{y_i} = \\hat{y}(x_i) = x^T_i\\hat{\\beta}$. At an arbitrary input $x_0$ the prediction is $\\hat{y}(x_0) = x^T_0\\hat{\\beta}$ The entire fitted surface is characterized by the p parameters $\\hat{\\beta}$. Intuitively, it seems that we do not need a very large data set to fit such a model.","ede4b2af":"$\\hat{\\beta} = (X^TX)^{\u22121}X^Ty $ <br\/>"}}