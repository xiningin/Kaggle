{"cell_type":{"8450775a":"code","8d81703e":"code","394f51eb":"code","3a65bcec":"code","d59af699":"code","2f940c86":"code","61359f4f":"code","d9d0b3b0":"code","3eb0aad0":"code","98fab2bb":"code","eee79c81":"code","eec9b879":"code","5c174d6a":"code","cd7140fe":"code","602d8869":"code","d8c9e56c":"code","6c5f1901":"code","8530b91e":"code","f8f31558":"code","5b2e5ec4":"code","28e820d1":"code","e393e57d":"code","03db0b76":"code","1366e8ff":"code","e0bb0ed9":"code","b9a2633f":"code","ec6e482b":"code","e9aed47e":"code","8b17988d":"code","7ecf83e0":"code","78dee303":"code","f1f657cf":"code","e05e6911":"code","083afe20":"code","a7af9789":"code","d334d90a":"code","4df3412a":"code","5fc5f53c":"code","bdf60ea8":"code","1c71410e":"code","ebe33b61":"code","2436e8da":"code","7f065955":"code","de290e4a":"code","9e89c95f":"code","ba625e2e":"code","76ccc489":"code","3bf6980e":"code","7c28c702":"code","9a76816b":"code","13214d42":"code","765752df":"code","2f8e25cc":"code","ce58958c":"markdown","6f1d2835":"markdown","be0a31ae":"markdown","b1e71d8c":"markdown","4a108d50":"markdown","9b15e001":"markdown","8ef5c491":"markdown","1a77de5a":"markdown","8f5d057a":"markdown","97491363":"markdown","46e2138c":"markdown","a2290b76":"markdown","73e00760":"markdown","f9511a58":"markdown","d699101f":"markdown","f61b144e":"markdown","6f9bbfec":"markdown","133b6d29":"markdown","5b556919":"markdown","3da5e94e":"markdown","96c17a21":"markdown","9504363f":"markdown","00982bed":"markdown","3f0b5be7":"markdown","c0ceb991":"markdown","8763c016":"markdown","77433bc4":"markdown","ca2299a1":"markdown","61801635":"markdown","28a1b784":"markdown","56aeee26":"markdown","3bc97dc9":"markdown","b8c9f756":"markdown"},"source":{"8450775a":"# Work with Data - the main Python libraries\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Preprocessing\nimport os\nimport random as rn\nfrom sklearn.preprocessing import StandardScaler, Binarizer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, KFold, ShuffleSplit, GridSearchCV\n\n# Modeling\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\n# For Deep learing (LSTM)\nimport tensorflow \nfrom tensorflow import keras\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D\nfrom tensorflow.keras.optimizers import SGD \nimport itertools\n\n# Metrics\nfrom sklearn.metrics import accuracy_score\n\nimport warnings\nwarnings.simplefilter('ignore')","8d81703e":"# Set random seed in all models\nSEED = 42\ntensorflow.random.set_seed(SEED)\nos.environ['PYTHONHASHSEED']=str(SEED)\nnp.random.seed(SEED)\nrn.seed(SEED)","394f51eb":"keras.__version__","3a65bcec":"# Choose the numbers of stations with the best series of observations, located sequentially along the river\nstations_id = [14, 15, 16]\ntarget_station = 14   # Observatory station located downstream (numbering from the mouth of the river)","d59af699":"# Choose frequency of data: 'D' (a day), '1W' (a week), '2W' (2 weeks), 'M' (a month), ...\nfreq_data = 'M'","2f940c86":"# Choose the target indicator\ntarget_data_name = 'Suspended'  # From the list ['NH4', 'BSK5', 'Suspended', 'O2', 'NO3', 'NO2', 'SO4', 'PO4', 'CL']","61359f4f":"# Choose the target shift in future\n# For what period the forecast with periodicity freq_data is required\ntarget_shift = test_size = 4","d9d0b3b0":"# Choose the size of validation dataset (with periodicity freq_data)\nvalid_size = 12","3eb0aad0":"# Download data from the dataset \ndata = pd.read_csv('..\/input\/wq-southern-bug-river-01052021\/PB_All_2000_2021.csv', sep=';', header=0)\ndata = data[data['id'].isin(stations_id)]\ndata","98fab2bb":"# Download data about monitoring stations from the dataset\ndata_about = pd.read_csv('..\/input\/wq-southern-bug-river-01052021\/PB_stations.csv', sep=';', header=0, encoding='cp1251')\ndata_about = data_about[data_about['id'].isin(stations_id)]\ndata_about.sort_values(by=['length'], ascending=False)","eee79c81":"# Amount data observations of stations\ndata['id'].value_counts().sort_values().plot(kind='barh')","eec9b879":"# Determination the year of observations\ndata['ds'] = pd.to_datetime(data['date'])\ndata['year'] = data['ds'].dt.year\ndata.info()","5c174d6a":"# Set input indicators\n#feature_target_all = ['NH4', 'BSK5', 'NO3', 'NO2', 'SO4', 'PO4', 'CL']\nfeature_target_all = ['BSK5', 'NO3']\nfeature_data_all = feature_target_all + [target_data_name]\nfeature_data_all","cd7140fe":"# Data sampling only for good stations\ndf_indicator = data[['id', 'ds'] + feature_data_all]\ndf_indicator = df_indicator[df_indicator['id'].isin(stations_id)].dropna().reset_index(drop=True)\ndf_indicator","602d8869":"cols = []\nfor station in stations_id:\n    for feature in feature_data_all:\n        cols.append(str(station) + \"_\" + feature)\ncols","d8c9e56c":"# Transformation of the DataFrame\ndf = pd.pivot_table(df_indicator, index=[\"ds\"], columns=[\"id\"], values=feature_data_all).dropna()\ndf.columns = cols\ndf","6c5f1901":"# Get target observation station name\ntarget_feature = str(target_station) + \"_\" + target_data_name\ntarget_feature","8530b91e":"# Resampling of data to a given frequency freq_data\ndf = df.resample(freq_data).mean()\ndf","f8f31558":"# Data imputing\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\ndf = pd.DataFrame(imp.fit_transform(df), columns = df.columns, index = df.index)\n\n# Display data\ndf","5b2e5ec4":"# Creation the target \ndf['target_feature_diff'] = df[target_feature].diff()\ndf['target_feature_diff_sign'] = (df['target_feature_diff'] > 0).astype('int')\ndf = df[1:]\ndf['target'] = df['target_feature_diff_sign'].shift(target_shift)\ndf = df[target_shift:]\ndf['target'] = df['target'].astype('int')\ndf","28e820d1":"# Delete leak features\ndf = df.drop(columns=[target_feature, 'target_feature_diff', 'target_feature_diff_sign'])","e393e57d":"# Data visualization\ndf.plot(figsize=(18,10))","03db0b76":"# EDA with Pandas Profiling\n#pp.ProfileReport(df)","1366e8ff":"# Get target data\ntarget_data = df.pop('target')\ntarget_data","e0bb0ed9":"# Dividing data into test, validation and training\ntrain = df[:-(valid_size+test_size)]\ntarget = target_data[:-(valid_size+test_size)]\nvalid = df[-(valid_size+test_size):-test_size]\nvalid_target = target_data[-(valid_size+test_size):-test_size]\ntest = df[-test_size:]\ntest_target = target_data[-test_size:]\nprint(train.shape, valid.shape, test.shape)","b9a2633f":"# Display the statistics for training data\ntrain.describe()","ec6e482b":"# Display the statistics for test data\nvalid.describe()","e9aed47e":"# Standartization data\nscaler = StandardScaler().fit(train)\ntrain = pd.DataFrame(scaler.transform(train), columns = train.columns, index = train.index)\n\n# Display training data\ntrain","8b17988d":"# Standartization data\nvalid = pd.DataFrame(scaler.transform(valid), columns = valid.columns, index = valid.index)","7ecf83e0":"# Cross-validation of training data with shuffle\ncv_train = ShuffleSplit(n_splits=3, test_size=0.5, random_state=SEED)","78dee303":"def calc_forecast_and_acc_for_model(df, target, model, type_data, model_name, result):\n    # Calculates the forecast and accuracy of the forecast for a given model and dataframe df of type_data with target\n    # Returns the result is stored in dataframe result for the model_name and the forecast\n    \n    # Calculates the forecast\n    y_pred = model.predict(df).astype('int')\n    if max(y_pred) > 1:\n        transformer = Binarizer(0.5)\n        y_pred = transformer.fit_transform(y_pred)\n\n    # Calculates accuracy of the forecast \n    acc = round(accuracy_score(target, y_pred), 2)\n    print(f'Accuracy of {model_name} model {type_data} is {acc}')\n\n    # Save to the result dataframe\n    if type_data == 'training':\n        col = 'train_score'\n    elif type_data == 'validation':\n        col = 'valid_score'\n    else: col = 'test_score'\n    result.loc[result['model'] == model_name, col] = acc\n    \n    return result, y_pred","f1f657cf":"# Creation the dataframe with the resulting score of all models\nresult = pd.DataFrame({'model' : ['LSTM', 'Random Forest Classifier', 'XGBoost Classifier'], \n                       'train_score': 0, 'valid_score': 0, 'test_score': 0})\nresult","e05e6911":"# Reshape input data to be 3D [samples, timesteps, features]\ntrain_X = train.values.reshape((train.shape[0], 1, train.shape[1]))\nvalid_X = valid.values.reshape((valid.shape[0], 1, valid.shape[1]))\ntest_X = test.values.reshape((test.shape[0], 1, test.shape[1]))\ntrain_X","083afe20":"print(train_X.shape, valid_X.shape, test.shape)","a7af9789":"%%time\n# LSTM\n# Building model          \nlstm_model = Sequential()\nlstm_model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\nlstm_model.add(Dropout(0.2))\nlstm_model.add(Dense(1))\n\n# lr_schedule = tensorflow.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate=1e-2,\n#     decay_steps=10,\n#     decay_rate=0.9)\n# opt = tensorflow.keras.optimizers.SGD(learning_rate=lr_schedule)\nopt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)\nlstm_model.compile(loss='binary_crossentropy', optimizer=opt)\n\n# Set callbacks\nlr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.75, patience=3, verbose=1)\nes = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=1, mode=\"min\", restore_best_weights=True)\n# sv = keras.callbacks.ModelCheckpoint(\n#     checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n#     save_weights_only=False, mode='auto', save_freq='epoch',\n#     options=None\n# )\n\n\n# Fit LSTM\nhistory = lstm_model.fit(train_X, target, epochs=200, batch_size=30, \n                         validation_data=(valid_X, valid_target), \n                         verbose=1, \n                         shuffle=False, \n                         callbacks=[lr, es])","d334d90a":"# Summarize history for loss\nplt.figure(figsize=(10,8))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\n\nplt.legend(['train', 'valid'], loc='upper right')\nplt.show()","4df3412a":"# Find history index less than 1.0\nstart_less_1 = next(x[0] for x in enumerate(history.history['loss']) if x[1] < 1.0)\nstart_less_1","5fc5f53c":"# Summarize history for loss from (start_less_1)th epoch\nplt.figure(figsize=(16,10))\nplt.plot(history.history['loss'][start_less_1:])\nplt.plot(history.history['val_loss'][start_less_1:])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel(f'epoch (0 ==> {start_less_1}th epoch)')\nplt.legend(['train', 'valid'], loc='upper right')\nplt.show()","bdf60ea8":"# Calculates the forecast and accuracy of the forecast for training, validation and test datasets\nresult, y_lstm_train = calc_forecast_and_acc_for_model(train_X, target, lstm_model, 'training', \"LSTM\", result)\nresult, y_lstm_val = calc_forecast_and_acc_for_model(valid_X, valid_target, lstm_model, 'validation', \"LSTM\", result)\nresult, y_lstm_test = calc_forecast_and_acc_for_model(test_X, test_target, lstm_model, 'test', \"LSTM\", result)","1c71410e":"%%time\n# Random Forest Classifier\nrf = RandomForestClassifier()\nparam_grid = {'n_estimators': [10, 20, 30], 'min_samples_split': [12, 15, 18], 'min_samples_leaf': [10, 15, 20], \n              'max_features': ['auto'], 'max_depth': [5, 6, 7, 8], 'criterion': ['gini'], 'bootstrap': [False], \n              'random_state': [SEED]}\n\n# Training model\nrf_CV = GridSearchCV(rf, param_grid=param_grid, cv=cv_train, verbose=False)\nrf_CV.fit(train, target)\nprint(rf_CV.best_params_)","ebe33b61":"# Calculates the forecast and accuracy of the forecast for training, validation and test datasets\nresult, y_rf_train = calc_forecast_and_acc_for_model(train, target, rf_CV, 'training', 'Random Forest Classifier', result)\nresult, y_rf_val = calc_forecast_and_acc_for_model(valid, valid_target, rf_CV, 'validation', 'Random Forest Classifier', result)\nresult, y_rf_test = calc_forecast_and_acc_for_model(test, test_target, rf_CV, 'test', 'Random Forest Classifier', result)","2436e8da":"%%time\n# XGBoost Classifier\nxgbr = xgb.XGBClassifier() \nparameters = {'n_estimators': [30, 50], \n              'learning_rate': [0.05, 0.1],\n              'max_depth': [5, 6, 7],\n              'objective': ['binary:hinge'],\n              'eval_metric': ['logloss'],\n              'random_state': [SEED]}\n\n# Training model\nxgb_CV = GridSearchCV(estimator=xgbr, param_grid=parameters, cv=cv_train, \n                      verbose=0, n_jobs=-1)\nxgb_CV.fit(train, target)\nprint(\"Best score: %0.3f\" % xgb_CV.best_score_)\nprint(\"Best parameters set:\", xgb_CV.best_params_)","7f065955":"# Calculates the forecast and accuracy of the forecast for training, validation and test datasets\nresult, y_xgb_train = calc_forecast_and_acc_for_model(train, target, xgb_CV, 'training', 'XGBoost Classifier', result)\nresult, y_xgb_val = calc_forecast_and_acc_for_model(valid, valid_target, xgb_CV, 'validation', 'XGBoost Classifier', result)\nresult, y_xgb_test = calc_forecast_and_acc_for_model(test, test_target, xgb_CV, 'test', 'XGBoost Classifier', result)","de290e4a":"# Feature importance diagram\nxgbr = xgb.XGBClassifier(**xgb_CV.best_params_)\nxgbr.fit(train, target)\nfig =  plt.figure(figsize = (10,8))\naxes = fig.add_subplot(111)\nxgb.plot_importance(xgbr,ax = axes,height = 0.5)\nplt.show();\nplt.close()","9e89c95f":"def draw_plot(df, y_list, name_data):\n    # Building plot for predictions for the name_data data \n    # y_list consist of: target and predictions of LSTM , RF and XGB models\n    \n    x = np.arange(len(df))\n    plt.figure(figsize=(16,10))\n    if y_list[0] is not None:\n        plt.scatter(x, y_list[0], label = f\"Target {name_data} data\", color = 'g')\n    plt.plot(x, y_list[2], label = \"Random Forest prediction\", color = 'y')\n    plt.plot(x, y_list[3], label = \"XGBoost Classifier prediction\", color = 'brown')\n    plt.plot(x, y_list[1], label = \"LSTM prediction\", color = 'b')\n    plt.title(f'Prediction for the {name_data} data')\n    plt.legend(loc='best')\n    plt.grid(True)","ba625e2e":"# Building plot for prediction for the training data \ny_train = [target, y_lstm_train, y_rf_train, y_xgb_train]\ndraw_plot(train, y_train, 'training')","76ccc489":"# Building plot for prediction for the valid data \ny_val = [valid_target, y_lstm_val, y_rf_val, y_xgb_val]\ndraw_plot(valid, y_val, 'validation')","3bf6980e":"# Building plot for prediction for the test data \ny_test = [test_target, y_lstm_test, y_rf_test, y_xgb_test]\ndraw_plot(test, y_test, 'test')","7c28c702":"# Display results of modeling\nresult.sort_values(by=['valid_score', 'train_score', 'test_score'], ascending=False)","9a76816b":"# Select models\n#result_best = result[(result['train_score'] - result['valid_score']).abs() < 0.15]\nresult_best = result\nresult_best.sort_values(by=['test_score', 'valid_score', 'train_score'], ascending=False)","13214d42":"# Select the best model\nresult_best.nlargest(1, 'test_score')","765752df":"# Find a name of the best model (with maximal valid score)\nbest_model_name = result_best.loc[result_best['test_score'].idxmax(result_best['test_score'].max()), 'model']","2f8e25cc":"print(f'The best model is \"{best_model_name}\"')","ce58958c":"### 3.4. Cross-validation of training data<a class=\"anchor\" id=\"3.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","6f1d2835":"I hope you find this notebook useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\n[Go to Top](#0)","be0a31ae":"### 3.1. Statistics & FE<a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","b1e71d8c":"**TASK:** You can select other size of validation dataset: target_shift, 2target_shift, 3target_shift, ... or 10, 20, ....","4a108d50":"## 5. Result visualization<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","9b15e001":"<a class=\"anchor\" id=\"0\"><\/a>\n# Dataset [River Water Quality EDA and Forecasting](https:\/\/www.kaggle.com\/vbmokin\/wq-southern-bug-river-01052021)","8ef5c491":"**ADDITIONAL TASK:** \n1. Set number of splitting = 5, 7, 10 and to compare of results.\n2. Try use another method for cross-validation of training data (without shuffle):\n\n        KFold(n_splits=5, shuffle=False, random_state=SEED)","1a77de5a":"### Data processing:\n* resampling (from a days in different months to a month)\n* imputing of NAN\n* splitting to training, validation and test datasets\n* standartization\n* shifting of target on N months in future\n* building models: LSTM, Random Forest, XGBoost\n* forecasting on N months in future\n* drawing plot for training, validation and test datasets (data and predictions).","8f5d057a":"**TASK:** You can select other frequency of data.","97491363":"Dataset contains data on river water quality for 8 indicators for 22 monitoring stations.\n\nData for 2000-2021 for the Southern Bug (or Pivdennyi Booh) river.","46e2138c":"**TASK:** You can select other target indicator.","a2290b76":"### Task: to predict on N of the future moments of time (days, weeks, months...) will increase or decrease (target = 1 or 0) values of given time series (an indicator of the quality of water in the observatory station located downstream) according to data of other time series: the same and other indicators of the quality of water in this observation station and observation stations located upstream.","73e00760":"### 4.2. Random Forest Classifier<a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","f9511a58":"### 3.2. Training data splitting<a class=\"anchor\" id=\"3.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","d699101f":"## 3. EDA & FE & Preprocessing data<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","f61b144e":"## 2. Download data<a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","6f9bbfec":"### 4.3. XGBoost Classifier<a class=\"anchor\" id=\"4.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","133b6d29":"### 3.3. Data standartization<a class=\"anchor\" id=\"3.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","5b556919":"## 6. Select the best model <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","3da5e94e":"### Notebook [WQ SB river : EDA and Forecasting](https:\/\/www.kaggle.com\/vbmokin\/wq-sb-river-eda-and-forecasting) shows that the best time series of observations in this data set are at the following three state monitoring stations located consecutive along the river (numbering from the mouth of the river): 16, 15, 14 in Vinnytsia region.","96c17a21":"**TASK:** You can select other random seed: 1, 2, 3, ...","9504363f":"## Acknowledgements\n* [Data Science for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques)\n* [EDA for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/eda-for-tabular-data-advanced-techniques)\n* [Datasets for river water quality prediction](https:\/\/www.kaggle.com\/vbmokin\/datasets-for-river-water-quality-prediction)\n* [WQ SB river : EDA and Forecasting](https:\/\/www.kaggle.com\/vbmokin\/wq-sb-river-eda-and-forecasting)\n* [Time-series data analysis using LSTM (Tutorial)](https:\/\/www.kaggle.com\/amirrezaeian\/time-series-data-analysis-using-lstm-tutorial)\n* [AI-ML-DS Training. L1T : Titanic - Decision Tree](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l1t-titanic-decision-tree)\n* [Heart Disease - Automatic AdvEDA & FE & 20 models](https:\/\/www.kaggle.com\/vbmokin\/heart-disease-automatic-adveda-fe-20-models)\n* [\u26a1\ufe0f Hybrid CNN_ENC_DEC + Sample Weights \u26a1\ufe0f](https:\/\/www.kaggle.com\/jmcslk\/hybrid-cnn-enc-dec-sample-weights)\n* [The system \"MONITORING AND ENVIRONMENTAL ASSESSMENT OF WATER RESOURCES OF UKRAINE\", State Agency of Water Resources of Ukraine](http:\/\/monitoring.davr.gov.ua\/EcoWaterMon\/GDKMap\/Index)","00982bed":"## 1. Import libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","3f0b5be7":"# Tree Classifier Models and LSTM models for the Multiple Time Series Forecasting Task","c0ceb991":"## 4. Modeling<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","8763c016":"**TASK:** You can select other target shift: 1, 2, ...","77433bc4":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download data](#2)\n1. [EDA & FE & Preprocessing data](#3)\n    - [Statistics & FE](#3.1)\n    - [Training data splitting](#3.2)\n    - [Data standartization](#3.3)\n    - [Cross-validation of training data](#3.4)\n1. [Modeling](#4)\n    - [LSTM](#4.1)\n    - [Random Forest Classifier](#4.2)\n    - [XGBoost Classifier](#4.3)    \n1. [Results visualization](#5)\n1. [Select the best model](#6)","ca2299a1":"### Map of the stations:\nhttp:\/\/monitoring.davr.gov.ua\/EcoWaterMon\/GDKMap\/Index\n\n![image.png](attachment:7d210839-9bcd-46a8-a58d-ce90f29fb294.png)\n\nThe water quality state monitoring stations of the Southern Bug (or Pivdennyi Booh) river.","61801635":"**TASK:** You can select other observation station numbers.","28a1b784":"**ADDITIONAL TASKS:** \n1. Try to change the parameters (see examples above).\n2. Try deleting anomalous data. \n3. Add to dataframe result also calculated array: y_train, y_val.","56aeee26":"**TASK:** You can select other sets of indicators.","3bc97dc9":"### 4.1. LSTM<a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","b8c9f756":"Thanks to https:\/\/www.kaggle.com\/amirrezaeian\/time-series-data-analysis-using-lstm-tutorial"}}