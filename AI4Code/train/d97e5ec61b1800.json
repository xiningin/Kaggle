{"cell_type":{"b8cd72b0":"code","68e92652":"code","f56ec483":"code","375d0096":"code","a148315e":"code","8103e53a":"code","5997a062":"code","308972a6":"code","eb3dac9a":"code","71b7c56c":"code","161be512":"code","55ef9441":"code","a2385f80":"code","bf933eaf":"code","a7a22c00":"code","e47a2bae":"code","921c4a55":"code","00b83738":"code","20bdc983":"code","401508fc":"code","8c3de9c2":"code","6c4692db":"code","d137378f":"code","4593272c":"code","7b945ebb":"code","7fb73a14":"code","25a85f1f":"code","86774181":"code","0948e1d5":"code","c930175d":"code","4953f40f":"code","feacf42c":"code","ec49e184":"code","6fffa943":"code","83be60de":"code","be43664b":"code","377edf57":"code","0e3b19f9":"code","4e993a5e":"code","b9ce2913":"code","b05cd3cc":"code","dc78a4d3":"code","cc6dea34":"code","5a97ec05":"markdown","2369282d":"markdown","75f16740":"markdown","b52578e5":"markdown","ba254e86":"markdown","4a51e2ab":"markdown","deef62b8":"markdown","807897ba":"markdown","c9b5923f":"markdown","90ded08e":"markdown","a88ea94b":"markdown","33048785":"markdown","a587d01c":"markdown","eed0f390":"markdown","31cb055c":"markdown","c0683d63":"markdown","a7ae4cc9":"markdown","3c3fbed4":"markdown","94330943":"markdown","6a96c59b":"markdown","0a3932b2":"markdown","bdbcdd46":"markdown","4cf5fdd4":"markdown","62bb62e1":"markdown","3bed3fdb":"markdown","63963acf":"markdown","258f2acb":"markdown","058baff1":"markdown","25811c23":"markdown","f34962b4":"markdown","ef4ba3e1":"markdown"},"source":{"b8cd72b0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","68e92652":"import numpy as np\nimport pylab as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.model_selection import cross_val_score, GridSearchCV","f56ec483":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv')\nsample_sub = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')","375d0096":"train['target'].value_counts()","a148315e":"train.head().style.background_gradient(cmap = \"Blues\")","8103e53a":"train.head().style.background_gradient(cmap = \"Spectral\")","5997a062":"from sklearn import preprocessing \n\nle = preprocessing.LabelEncoder()\n\ntrain['target'] = le.fit_transform(train['target'])","308972a6":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LassoCV\n\nX = train.drop(['target', 'id'], axis = 1)\ny = train['target']\n\n","eb3dac9a":"plt.figure(figsize=(10, 12))\n\nlasso = LassoCV().fit(X, y)\nimportance = np.abs(lasso.coef_)\nfeature_names = np.array(X.columns)\nplt.barh(feature_names, importance)\nplt.title(\"Feature Importances via Coefficients [ Lasso CV ]\")\nplt.figure(figsize=(12, 22))\nplt.show()","71b7c56c":"from sklearn.feature_selection import SelectFromModel\n\nfrom time import time\n\nthreshold = np.sort(importance)[-3]\n\ntic = time()\nsfm = SelectFromModel(lasso, threshold = threshold).fit(X, y)\ntoc = time()\nprint(\"Features Selcted by SelectFromModel : \"f\"{feature_names[sfm.get_support()]}\")\nprint(f\"Done in {toc - tic : .3f}s\")","161be512":"feature_names_lassocv = feature_names","55ef9441":"# from sklearn.feature_selection import SequentialFeatureSelector\n\n# tic_fwd = time()\n# sfs_forward = SequentialFeatureSelector(lasso, n_features_to_select = 4, direction = 'forward').fit(X, y)\n# toc_fwd = time()\n\n# tic_bwd = time()\n# sfs_backward = SequentialFeatureSelector(lasso, n_features_to_select = 4, direction = 'backward').fit(X, y)\n# toc_bwd = time()\n\n# print(\"Features Selected by Forward Sequential Selection : \"f\"{feature_names[sfs_forward.get_support()]}\")\n# print(f\"Done in {toc_fwd - tic_fwd:.3f}s\")\n\n# print(\"Features Selected by Backward Sequential Selection: \"f\"{feature_names[sfs_backward.get_support()]}\")\n# print(f\"Done in {toc_bwd - tic_bwd: .3f}s\")\n","a2385f80":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nclf = ExtraTreesClassifier(n_estimators = 50)\nclf = clf.fit(X, y)\n\nmodel = SelectFromModel(clf, prefit = True)\nfeature_names_extratreesclf = feature_names[model.get_support()]\n\nprint(\"Features Selcted by Extra Tree Classifier and SelectFromModel : \"f\"{feature_names[model.get_support()]}\")\n","bf933eaf":"feature_names_extratreesclf.shape","a7a22c00":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 42)\n\nfeature_names = np.array(X.columns)\n\nforest = RandomForestClassifier(random_state = 0)\nforest.fit(X_train, y_train)","e47a2bae":"import time\n\nstart_time = time.time()\nimportances = forest.feature_importances_\nstd = np.std([\n    tree.feature_importances_ for tree in forest.estimators_], axis = 0)\n\nelapsed_time = time.time() - start_time\n\nprint(f\"Elapsed time to compute the importances:\" f\"{elapsed_time:.3f} seconds\" )\n","921c4a55":"import pandas as pd\n\n\nforest_importances = pd.Series(importances, index = feature_names)\n\nfig, ax = plt.subplots(figsize = (10, 12))\nforest_importances.plot.barh(std, ax)\nax.set_title(\"Feature importances using MDI (Mean Decrease in Impurity)\")\nax.set_ylabel(\" Mean Decrease in Impurity\")\nfig.tight_layout()\n","00b83738":"feature_importance_mdi = forest_importances","20bdc983":"type(feature_importance_mdi)","401508fc":"forest_importances.loc[forest_importances > np.mean(forest_importances)].index","8c3de9c2":"from sklearn.inspection import permutation_importance\n\nstart_time = time.time()\nresult = permutation_importance(\n                               forest, X_test, y_test, n_repeats = 10, random_state = 42)\nelapsed_time = time.time() - start_time\n\nprint(f\"Elapsed time to compute the importances :\" f\"{elapsed_time:.3f} seconds\")\n\nforest_importances = pd.Series(result.importances_mean, index = feature_names)","6c4692db":"fig, ax = plt.subplots(figsize = (10, 12))\nforest_importances.plot.barh(result.importances_std, ax)\nax.set_title(\"Feature Importances using permutation on Full Model\")\nax.set_xlabel(\"Mean Accuracy decrease\")\nfig.tight_layout()\nplt.show()","d137378f":"# import matplotlib.pyplot as plt\n# from sklearn.svm import SVC\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.feature_selection import RFECV\n\n# svc = SVC(kernel = \"linear\")\n\n# min_features_to_select = 4 # Min number of features to consider\n# rfecv = RFECV(estimator = svc, step = 1, cv = StratifiedKFold(2), scoring = 'accuracy',\n#              min_features_to_select = min_features_to_select)\n\n# rfecv.fit(X_train, y_train)\n\n# print(\"Optimal Number of Features : %d\" % rfecv.n_features_)\n\n# plt.figure()\n# plt.xlabel(\"Number of features selected\")\n# plt.ylabel(\"Cross Validation Score (nb of Correct Classifications)\")\n# plt.plot(range(min_features_to_select, \n#               len(rfecv.grid_scores_) + min_features_to_select),\n#         rfecv.grid_scores_)\n# plt.show()","4593272c":"from sklearn.feature_selection import chi2\n\nX1 = X.abs()\nchi_scores = chi2(X1, y)\n","7b945ebb":"chi_scores","7fb73a14":"p_values = pd.Series(chi_scores[1], index = X.columns)\np_values.sort_values(ascending = False, inplace = True)","25a85f1f":"p_values.plot.bar(figsize = (10, 12))","86774181":"from functools import wraps\nimport datetime as dt\n\ndef log_step(func):\n    \n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        \n        tic = dt.datetime.now()\n        result = func(*args, **kwargs)\n        time_taken = str(dt.datetime.now() - tic)\n        print(f\"just ran step {func.__name__} shape = {result.shape} took {time_taken}s\")\n        return result\n    return wrapper","0948e1d5":"@log_step\ndef start_pipeline(dataf):\n    return dataf.copy() ","c930175d":"from sklearn import preprocessing\n\n@log_step\ndef label_encode(data):\n    le = preprocessing.LabelEncoder()\n\n    for c in data.columns:\n\n        if (data[c].dtype == 'object'):\n            data[c] = le.fit_transform(data[c])\n\n    return data\n        ","4953f40f":"@log_step\ndef corelation_target(data, target):\n    \n    \"\"\"\n    Find Co-relation of different features with the \"Target\" column in Descending Order\n    \"\"\"\n    plt.figure(figsize = (8, 12))\n\n    heatmap = sns.heatmap(data.corr()[[target]].drop(index = target, axis = 0).sort_values(by = target, ascending = False),\n                         vmin = -1,\n                         vmax = 1, \n                         annot = True, \n                         cmap = 'BrBG')\n\n    heatmap.set_title(f\"Features Correlating with {target} column\", \n                      fontdict = {'fontsize':18}, pad = 16)\n    \n    return data","feacf42c":"@log_step\ndef corelation_horizontal_target(data, target):\n    \n    \"\"\"\n    Horizontal Bar Plot of the Co-relation of individual features with the Target Column \n    \"\"\"\n    plt.figure(figsize=(10, 12))\n\n    corr = data.corr()[[target]].drop(index = target, axis = 0) # Removes the 1st row i.e. Corelation of target with itself\n    plt.barh(corr.index, corr.reset_index(drop = True).to_numpy().ravel())\n    plt.title(\"Corelation with target\")\n    plt.figure(figsize=(12, 22))\n    plt.show()\n    \n    return data","ec49e184":"@log_step\ndef zero_percent(data):\n    \n    \"\"\"\n    Horizontal Bar Plot of Percentage of Data containing '0' in each feature\n    \"\"\"\n    \n    raw_light_palette = [\n        (0, 122, 255), # Blue\n        (255, 149, 0), # Orange\n        (52, 199, 89), # Green\n        (255, 59, 48), # Red\n        (175, 82, 222),# Purple\n        (255, 45, 85), # Pink\n        (88, 86, 214), # Indigo\n        (90, 200, 250),# Teal\n        (255, 204, 0)  # Yellow\n    ]\n\n    light_palette = np.array(raw_light_palette) \/ 255\n\n    zero_data = ((data.iloc[:, :50] == 0 ).sum() \/ len(data) * 100)[::-1]\n    fig, ax = plt.subplots(1, 1, figsize = (10, 19))\n\n    ax.barh(zero_data.index, 100, color = '#dadada', height = 0.6)\n    barh = ax.barh(zero_data.index, zero_data, color = light_palette[1], height = 0.6)\n    ax.bar_label(barh, fmt = '%.01f %%', color = 'black')\n\n    # Line noting the data area boundaries\n    ax.spines[['left', 'bottom']].set_visible(False)\n\n    # xticks : Set the current label of x-axis\n    ax.set_xticks([])\n\n    ax.set_title('# of Zeros (by feature)', loc = 'center', fontweight = 'bold', fontsize = 15)\n    plt.show()\n    \n    return data","6fffa943":"@log_step\ndef bargraph_average_by_class_by_feature(data, target):\n    \n    \"\"\"\n    Bar Graph Plot of Mean of Each value (Class) in a Feature \n    \"\"\"\n    \n    raw_dark_palette = [\n    (10, 132, 255), # Blue\n    (255, 159, 10), # Orange\n    (48, 209, 88),  # Green\n    (255, 69, 58),  # Red\n    (191, 90, 242), # Purple\n    (94, 92, 230),  # Indigo\n    (255, 55, 95),  # Pink\n    (100, 210, 255),# Teal\n    (255, 214, 10)  # Yellow\n    ]\n\n    dark_palette = np.array(raw_dark_palette)\/255\n\n    fig, axes = plt.subplots(13, 4, figsize = (10, 16))\n\n    target_order = sorted(data[target].unique())\n    mean = data.groupby(target).mean().sort_index()\n    std = data.groupby(target).std().sort_index()\n\n    for idx, ax in zip(range(50), axes.flatten()):\n        #main code\n        ax.bar(mean[f'feature_{idx}'].index, mean[f'feature_{idx}'],\n              color = dark_palette[:4], width = 0.6)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        ax.margins(0.1)\n        ax.spines['left'].set_visible(False)\n        ax.set_title(f'Feature_{idx}', loc = 'right', weight = 'bold', fontsize = 11)\n\n    axes.flatten()[-1].axis('off')\n    axes.flatten()[-2].axis('off')\n\n    fig.supxlabel('AVERAGE by class (by feature)', ha = 'center', fontweight = 'bold')\n\n    fig.tight_layout()\n    plt.show()\n\n    return data","83be60de":"train_df = (train\n           .pipe(start_pipeline)\n           .pipe(label_encode)\n           .pipe(corelation_target, target = 'target')\n           .pipe(corelation_horizontal_target, target = 'target')\n           .pipe(zero_percent)\n           .pipe(bargraph_average_by_class_by_feature, target = 'target'))","be43664b":"     \n\nfrom cycler import cycler\n\n\nraw_light_palette = [\n    (0, 122, 255), # Blue\n    (255, 149, 0), # Orange\n    (52, 199, 89), # Green\n    (255, 59, 48), # Red\n    (175, 82, 222),# Purple\n    (255, 45, 85), # Pink\n    (88, 86, 214), # Indigo\n    (90, 200, 250),# Teal\n    (255, 204, 0)  # Yellow\n]\n\nraw_dark_palette = [\n    (10, 132, 255), # Blue\n    (255, 159, 10), # Orange\n    (48, 209, 88),  # Green\n    (255, 69, 58),  # Red\n    (191, 90, 242), # Purple\n    (94, 92, 230),  # Indigo\n    (255, 55, 95),  # Pink\n    (100, 210, 255),# Teal\n    (255, 214, 10)  # Yellow\n]\n\nraw_gray_light_palette = [\n    (142, 142, 147),# Gray\n    (174, 174, 178),# Gray (2)\n    (199, 199, 204),# Gray (3)\n    (209, 209, 214),# Gray (4)\n    (229, 229, 234),# Gray (5)\n    (242, 242, 247),# Gray (6)\n]\n\nraw_gray_dark_palette = [\n    (142, 142, 147),# Gray\n    (99, 99, 102),  # Gray (2)\n    (72, 72, 74),   # Gray (3)\n    (58, 58, 60),   # Gray (4)\n    (44, 44, 46),   # Gray (5)\n    (28, 28, 39),   # Gray (6)\n]\n\n\nlight_palette = np.array(raw_light_palette)\/255\ndark_palette = np.array(raw_dark_palette)\/255\ngray_light_palette = np.array(raw_gray_light_palette)\/255\ngray_dark_palette = np.array(raw_gray_dark_palette)\/255\n\nmpl.rcParams['axes.prop_cycle'] = cycler('color',dark_palette)\nmpl.rcParams['figure.facecolor']  = gray_dark_palette[-2]\nmpl.rcParams['figure.edgecolor']  = gray_dark_palette[-2]\nmpl.rcParams['axes.facecolor'] =  gray_dark_palette[-2]\n\nwhite_color = gray_light_palette[-2]\nmpl.rcParams['text.color'] = white_color\nmpl.rcParams['axes.labelcolor'] = white_color\nmpl.rcParams['axes.edgecolor'] = white_color\nmpl.rcParams['xtick.color'] = white_color\nmpl.rcParams['ytick.color'] = white_color\n\nmpl.rcParams['figure.dpi'] = 200\n\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False\n\n","377edf57":"import warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category = FutureWarning)\nwarnings.filterwarnings('ignore', category = RuntimeWarning)\nwarnings.filterwarnings('ignore', category = UserWarning)\nwarnings.filterwarnings('ignore', category = sklearn.exceptions.UndefinedMetricWarning)\n\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport os\n#fmin : Minimize function using simplex downhill algorithm\nfrom scipy.optimize import fmin as scip_fmin\n\n# visualization \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style = \"whitegrid\")\n\n# Machine Learning\n\n# Utils\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, cross_validate\nfrom sklearn.model_selection import cross_val_score, train_test_split, KFold\nfrom sklearn import preprocessing\nimport category_encoders as ce\n\n# Feature Selection\nfrom sklearn.feature_selection import chi2, f_classif, f_regression\nfrom sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, VarianceThreshold\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\n\n","0e3b19f9":"def seed_everything(seed = RANDOM_SEED):\n    \n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)","4e993a5e":"train_df.describe().T","b9ce2913":"not)features = ['id', 'target']\nfeatures = []\n\nfor feat in train_df.columns:\n    \n    if feat not in not_features:\n        features.append(feat)\n        \nprint(features)","b05cd3cc":"scaler = preprocessing.StandardScaler()\nscaler.fit(train_df[features])\ntrain_df[features] = scaler.transform(train_df[features])\ntest_df[features] = scaler.transform(test_df[features])","dc78a4d3":"NUM_SPLITS = 5\n\ntrain_df[\"kfold\"] = -1\ntrain_df = train_df.sample(frac = 1).reset_index(drop = True)\ny = train_df.target.values\nkf = StratifiedKFold(n_splits = NUM_SPLITS)\n\nfor f, (t_, v_) in enumerate(kf.split(X = train_df, y = y)):\n    train_df.loc[v_, 'kfold'] = f\n    \ntrain_df.head()","cc6dea34":"# FROM abhishek thakur's book \n\nclass UnivariateFeatureSelection:\n    \n    def __init__(self, n_features, problem_type, scoring, return_cols = True):\n        \n        \"\"\"\n        Custom Univariate Feature Selction wrapper on different Univariate Feature selection \n        models from Scikit-Learn. \n        : param n_features: SelectPercentile if Float else SelectKBest\n        : param problem_type : classification or regression\n        : param scoring : scoring function, string\n        \"\"\"\n        \n        self.n_features = n_features\n        \n        if problem_type = \"classification\":\n            \n            valid_scoring = {\n                \"f_classif\": f_classif, \n                \"chi2\" : chi2,\n                \"mutual_info_classif\": mutual_info_classif\n            }\n            \n        else : \n            valid_scoring = {\n                \"f_regression\" : f_regression,\n                \"mutual_info_regression\" : mutual_info_regression\n            }\n            \n        if scoring not in valid_scoring:\n            raise Exception(\"Invalid scoring function\")\n            \n        if isinstance(n_features, int):\n            \n            self.selection = SelectKBest(\n                                            valid_scoring[scoring])","5a97ec05":"## Feature Permutation \n\nPermutation Feature importance overcomes limitations of impurity-based-feature importance: they do not have bias toward high-cardinality features can be computed on a left-out test set","2369282d":"# Importance of Features","75f16740":"## LABEL ENCODING","b52578e5":"# Model-based and Sequential Feature Selection","ba254e86":"**This also seems to run for quite a lot of time. Not a good option**","4a51e2ab":"## Feature Selection with respect to the Mean Decrease in Impurity","deef62b8":"This information sometimes gets lost in the Heatmap","807897ba":"# OBSERVATION : \n\n**'feature_13' 'feature_29' 'feature_36' are useful features as per LassoCV** ","c9b5923f":"### Feature Importance based on Mean Decrease in Impurity and Feature Permutation","90ded08e":"**BTW this has been running Endlessly. \nNot a good option !**","a88ea94b":"# KFOLD SPLITS\n\nBefore moving to feature engineering, it's better to perform cross validation splits. \n\nIn that way, we will not risk any data leakage and would be more certain of the validation set being aptly representative of the real world unknown data. ","33048785":"## Exploring a bit of a new thing : \n\n### Coloring a dataframe based on Frequencies","a587d01c":" **Soft Voting Ensemble Starter :** https:\/\/www.kaggle.com\/manabendrarout\/soft-voting-ensemble-starter-tps-may21","eed0f390":"### Looks like there has been a reduction. 49 features -> 23 features","31cb055c":"## Tree Based Feature Selection\n\nUsed to compute **Impurity-Based Feature Importances** , which in turn can be used to discard irrelevant features","c0683d63":"* 'feature_13' 'feature_29' 'feature_36' are useful as per LassoCV\n* Below features are selected by ExtraTreeClassifier : \n\n'feature_3' 'feature_7' 'feature_8' 'feature_9' 'feature_14' 'feature_15'\n 'feature_17' 'feature_18' 'feature_19' 'feature_21' 'feature_23'\n 'feature_24' 'feature_28' 'feature_31' 'feature_34' 'feature_35'\n 'feature_38' 'feature_40' 'feature_41' 'feature_48' 'feature_49' \n \n* Mean Decrease in Impurity : \n\n'feature_3', 'feature_7', 'feature_8', 'feature_9', 'feature_12',\n'feature_14', 'feature_15', 'feature_17', 'feature_18', 'feature_19',\n'feature_21', 'feature_24', 'feature_28', 'feature_31', 'feature_34',\n'feature_35', 'feature_38', 'feature_40', 'feature_41', 'feature_48',\n'feature_49'","a7ae4cc9":"## Recursive Feature Elimination","3c3fbed4":"**OBSERVATION : Feature Importance is different from what we found with LassoCV**","94330943":"# PIPELINES","6a96c59b":"# FEATURE SELECTION\n\nWe need to select only the important features for better performance of the model. \nAn unnecessary in best case scenario will not add to any productive calculation of the algorithm or in worst case scenario 'confuse' the model. `\n\nTo DO THE SAME LET'S CREATE A WRAPPER CLASS THAT HAS ALL BUILD IN STATISTICAL TESTS REQUIRED TO PERFORM FEATURE SELECTION AND TAKE SOME BASIC INPUTS FROM USER and spits out the required features \n","0a3932b2":"**here first array represents chi square values and second array represnts p-values**","bdbcdd46":"# Selecting Features with Sequential Feature Selection\n\nGreedy procedure where , at each iteration, we choose the best new feature to add to our selected features based a Cross-Validation Score. \nThe procedure is repeated until we reachthe desired number of selected Features. \n\nWe can also go back in reverse direction **(backward SFS)** i.e. start with all features and greedily chose features to remove one by one. ","4cf5fdd4":"## Chi Square Test","62bb62e1":"### Selecting Features based on Importance","3bed3fdb":"We cannot validate with the test dataset, as there is no 'target' column . \n\nHave to split the train dataset.\n","63963acf":"### Selecting features based on Feature Importance from Co-efficients\n\n### LASSOCV","258f2acb":"# OBSERVATION : \n\n**Feature 6 has the highest p-value, hence it is Independent of the values in the 'target' column.\nIt cannot be considered for Model Training**","058baff1":"# Label Encoding the 'target' column","25811c23":"# CORRELATIONS of different features with the \"target\" column in Descending Order","f34962b4":"An Interesting Package that I came across. \n**sklearn.inspection is a base version of ExplainableAI concepts**","ef4ba3e1":"# Feature Scaling\n\nTo bring all features into a similar scale let's use simple scaler to scale all the features"}}