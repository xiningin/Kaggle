{"cell_type":{"34232313":"code","fa5d9648":"code","6f5e9f62":"code","2d044d9d":"code","bc283d1e":"code","c6269b00":"code","1bd3b46b":"code","a224468c":"code","f5ce1de4":"code","b301cd03":"code","074f7a5f":"code","9409b056":"code","3f39e3fc":"code","183d291b":"markdown","c41e3f75":"markdown","5315dfaa":"markdown","683dca10":"markdown","6577faf0":"markdown","c718e56a":"markdown","27f08ef6":"markdown","70bb1d0c":"markdown","ab6e370c":"markdown","882c1339":"markdown","d528e56c":"markdown"},"source":{"34232313":"import pandas as pd\nimport numpy as np","fa5d9648":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\n\ncolumn_names = [ 'age', 'workclass', 'fnlwgt', 'education', 'education.num', \n                'marital.status', 'occupation', 'relationship', 'race', \n                'sex', 'capital.gain', 'capital.loss', 'hour.per.week', \n                'native.country', 'income' ]\n\ncolumns_to_encoding = [ 'workclass', 'marital.status', 'occupation',\n                        'relationship', 'race', 'sex' ]\n\ncolumns_to_normalize = [ 'age', 'education.num', 'hour.per.week', \n                         'capital.gain', 'capital.loss' ]\n\nle = LabelEncoder()\nscaler = StandardScaler()\npl = PolynomialFeatures(2, include_bias=False)\n\ndef feature_engineering(filename, train=True):\n    df = pd.read_csv(filename, index_col=False, names=column_names)\n    df.drop(['fnlwgt', 'education', 'native.country'], axis=1, inplace=True)\n    df = pd.get_dummies(df, columns=columns_to_encoding)\n    df[\"income\"] = le.fit_transform(df['income'])\n    if train:\n        X_temp = pl.fit_transform(df[columns_to_normalize])\n        X_temp = scaler.fit_transform(X_temp)\n        df.drop(columns_to_normalize, axis=1, inplace=True)\n        X_train = np.hstack((df.values, X_temp))\n        y_train = df['income']\n        columns_names = pl.get_feature_names(df.columns)\n        return np.hstack((df.columns.values, columns_names)), X_train, y_train\n    else:\n        X_temp = pl.transform(df[columns_to_normalize])\n        X_temp = scaler.transform(X_temp)\n        df.drop(columns_to_normalize, axis=1, inplace=True)\n        X_test = np.hstack((df.values, X_temp))\n        y_test = df['income']\n        columns_names = pl.get_feature_names(df.columns)\n        return np.hstack((df.columns.values, columns_names)), X_test, y_test","6f5e9f62":"columns_names, X_train, y_train = feature_engineering('..\/input\/adult.data', train=True)\ncolumns_names, X_test, y_test = feature_engineering('..\/input\/adult.test', train=False)","2d044d9d":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nparam_distribution = {\n    'max_depth': np.arange(1, 15),\n}\n\nscoring = {    \n    'Accuracy': make_scorer(accuracy_score),\n    'F1_Score': make_scorer(fbeta_score, beta=1),    \n}\n\nresult = []","bc283d1e":"result = []\nfor i in range(1, 20):\n    # train\n    pca = PCA(i)\n    X_t = pca.fit_transform(X_train)\n    search_cv = RandomizedSearchCV(DecisionTreeClassifier(), param_distribution,\n                                   scoring=scoring, n_jobs=-1, \n                                   cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=10), \n                                   refit='F1_Score') \n    search_cv.fit(X_t, y_train.values)\n    model = search_cv.best_estimator_\n\n    # test\n    X_t = pca.transform(X_test)\n    y_pred = model.predict(X_t)\n    \n    # model evaluation\n    f1 = fbeta_score(y_test.values, y_pred, beta=1)\n    acc = accuracy_score(y_test.values, y_pred)\n    print(f\"{i} {acc} {f1}\")\n    \n    result.append((i, acc, f1, pca, model))","c6269b00":"best_f1 = 0\nbest_model = None\nfor n, acc, f1, pca, model in result:\n    if best_f1 < f1:\n        best_f1 = f1\n        best_model=(n, acc, f1, pca, model)\nbest_model","1bd3b46b":"from sklearn import metrics\n\npca, model = best_model[-2], best_model[-1]\nprobs = model.predict_proba(pca.transform(X_test))\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","a224468c":"def plot_feature_importances(clf, X_train, y_train=None, \n                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n#     https:\/\/www.kaggle.com\/grfiv4\/plotting-feature-importances\n    __name__ = \"plot_feature_importances\"\n    \n    import pandas as pd\n    import numpy  as np\n    import matplotlib.pyplot as plt\n    \n    X_train = pd.DataFrame(data=X_train, columns=[f\"PC{i}\" for i in range(1, X_train.shape[1] + 1)])\n    \n    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n    feat_imp['feature'] = X_train.columns\n    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n    feat_imp = feat_imp.iloc[:top_n]\n    \n    feat_imp.sort_values(by='importance', inplace=True)\n    feat_imp = feat_imp.set_index('feature', drop=True)\n    feat_imp.plot.barh(title=title, figsize=figsize)\n    plt.xlabel('Feature Importance Score')\n    plt.show()\n    \n    if print_table:\n        from IPython.display import display\n        print(\"Top {} features in descending order of importance\".format(top_n))\n        display(feat_imp.sort_values(by='importance', ascending=False))\n        \n    return feat_imp\n\npca, clf = best_model[-2], best_model[-1]\nfeature_importance = plot_feature_importances(clf, pca.transform(X_train), top_n=X_train.shape[1], title=clf.__class__.__name__)","f5ce1de4":"# https:\/\/stackoverflow.com\/questions\/22348668\/pca-decomposition-with-python-features-relevances\npca, clf = best_model[-2], best_model[-1]\nindex_components = [int(x[2:]) for x in feature_importance.index.values]\ndef features_used_to_generate_pca_components(index_components, pca, clf, columns_names):    \n    for i in index_components:\n        index_features = np.abs(pca.components_[i - 1]).argsort()[:4]\n        features = columns_names[index_features]\n        print(f'PC{i}')\n        print(f'Features:')\n        for f in features:\n            print(\"\\t\" + f)\n        print()\n        \nfeatures_used_to_generate_pca_components(index_components, pca, clf, columns_names)","b301cd03":"from sklearn.metrics import confusion_matrix\n\npca, clf = best_model[-2], best_model[-1]\n\ny_pred = clf.predict(pca.transform(X_test))\n\ncm = confusion_matrix(y_test, y_pred)\ncm","074f7a5f":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    \nplot_confusion_matrix(cm, [0, 1], True)","9409b056":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","3f39e3fc":"from sklearn.externals import joblib\n\njoblib.dump(best_model, 'lgr.joblib')","183d291b":"# Find Best number of components to PCA","c41e3f75":"## Classification Report","5315dfaa":"# Interface function to feature engineering data","683dca10":"## Confusion Matrix","6577faf0":"# Import Base Packages","c718e56a":"# Get Best Model","27f08ef6":"# Save Best Model","70bb1d0c":"## Get Features Used to Generate PCA Components","ab6e370c":"# Load Data","882c1339":"## Plot feature importances","d528e56c":"# Analyse Model Result"}}