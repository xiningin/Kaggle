{"cell_type":{"1358cf99":"code","07b818fa":"code","c2daaa81":"code","3546d7e6":"code","bcf700bf":"code","a75eefa1":"code","aaaf82d3":"code","dbe3ca3a":"code","fd43fc7f":"code","44a55a76":"code","4817d1bf":"code","e711eb4e":"code","78424771":"code","c3f6d387":"code","43a11942":"code","97432f84":"code","71141dbb":"code","619abd59":"code","6975501d":"code","9a1443e6":"code","a672616e":"code","22c9e460":"code","26f3f80d":"code","a6f392ad":"code","3e0921da":"code","de8fb5f4":"code","e9f675f5":"code","d3f08427":"code","7c66ed31":"code","912615e1":"code","52ac2a10":"code","4fc474aa":"code","27e53ca0":"code","125a850c":"code","69eb1189":"code","48cd0339":"code","9afda13d":"code","e913973c":"code","2c5a9394":"code","463a55fe":"code","49e3f3ed":"code","7db63cf6":"code","f30848c1":"code","86760194":"code","2e51e592":"code","e894e540":"code","9e98b989":"code","4003cf21":"code","860433c5":"code","cdf80520":"code","fe029bd0":"code","9d747e3e":"code","35973f3d":"code","34315088":"code","56ef0357":"code","51262308":"code","a6733d3c":"code","d1e10e9b":"code","bc5a52c4":"code","d6baeb01":"code","ff48e636":"code","579c9ee2":"code","24060533":"code","7a50aca8":"code","8a654b8c":"code","ecd10bcb":"code","e26032fd":"code","833c5d44":"code","e8adc4c2":"code","c0c95062":"code","e99a684f":"code","418ef234":"code","5b5176dd":"code","cdd296ed":"code","fc2d9fd3":"code","a4dab8a9":"code","b13a7d5d":"code","e24db539":"code","836e9d92":"code","dd91bdb2":"code","4b5f3dfc":"code","60306a96":"markdown","f885aec8":"markdown","ce3bc04b":"markdown","eb01ce77":"markdown","6af4c983":"markdown","4f4f226d":"markdown","268bf231":"markdown","4a1a9a06":"markdown","07baf080":"markdown","852ed28f":"markdown","d7dd27ca":"markdown","b670ad49":"markdown","e456016d":"markdown","fd5c9e07":"markdown","7f43c0be":"markdown","8182dded":"markdown","99f8a763":"markdown","96ae1433":"markdown","cb1e2a82":"markdown","75f66314":"markdown","e0905f07":"markdown","b83952e7":"markdown","bd85f5a5":"markdown","919003fc":"markdown","462137d6":"markdown","6525a4c2":"markdown","93fa75b9":"markdown","aae7ca7a":"markdown","a5a4d29e":"markdown","9eacd0ea":"markdown","bb6ae0a4":"markdown","b7efe315":"markdown","6c3593d6":"markdown","437e52f4":"markdown","5ce1eab5":"markdown","11b4e682":"markdown","6833b053":"markdown","a1bca055":"markdown","060fe647":"markdown","ae0d0c69":"markdown","9d1089e8":"markdown","e6a71e06":"markdown","d652caca":"markdown","6363a9c0":"markdown","4f72aef7":"markdown","1c5322be":"markdown","a8ff58c1":"markdown","d8805696":"markdown","fc5bf64b":"markdown","00be26d2":"markdown","e2361b9e":"markdown","c72bc248":"markdown","3776e46e":"markdown","c68632b9":"markdown","30e0fa23":"markdown","fa27f899":"markdown","0a455513":"markdown","63e74068":"markdown","6224ad93":"markdown"},"source":{"1358cf99":"# Essentials\nimport numpy as np\nimport pandas as pd\nimport random\nimport time\nimport gc\nimport os\nfrom datetime import datetime\n\n# Plots\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\nfigure(num=None, figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')\nimport seaborn as sns\nfrom matplotlib.pylab import rcParams\n##set up the parameters\nrcParams['figure.figsize'] = 80,60\n\n# Models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom mlxtend.classifier import StackingCVClassifier\nfrom mlxtend.regressor import StackingCVRegressor\nfrom lightgbm.plotting import plot_importance\nimport lightgbm\nimport xgboost as xgb\nimport catboost\nfrom xgboost import plot_tree\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n\n# Misc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom dateutil import tz\nfrom geopy import distance\nimport shap\n\npd.set_option('display.max_columns', None)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\nprint(os.listdir(\"..\/input\/\"))","07b818fa":"def show_time(diff):\n   m, s = divmod(diff, 60)\n   h, m = divmod(m, 60)\n   s,m,h = int(round(s, 0)), int(round(m, 0)), int(round(h, 0))\n   print(\"Execution Time: \" + \"{0:02d}:{1:02d}:{2:02d}\".format(h, m, s))","c2daaa81":"# Takes in a classifier, calculates the training + prediction times and accuracy score, returns a model\ndef Train(clf, X, y, X_predict, y_predict, type='classification'):\n    # Train\n    start = time.time()\n    model = clf.fit(X,y)\n    end = time.time()\n    print('Training time: ')\n    show_time(end - start)\n    training_times.append(end - start)\n\n    # Predict\n    start = time.time()\n    if(type=='classification'):\n        scores.append(accuracy_score(y_predict, model.predict(X_predict)))\n    else:\n        scores.append(rmse(y_test, model.predict(X_test)))\n    end = time.time()\n    prediction_times.append(end - start)\n    print('\\nPrediction time: ')\n    show_time(end - start)\n    return model\n\n# Takes in a classifier, calculates the training + prediction times and accuracy score, returns a model\ndef GridSearch(clf, params, X, y, X_predict, y_predict, type='classification'):\n    # Train\n    start = time.time()\n    if(type=='classification'):\n        model = GridSearchCV(clf, params, scoring='accuracy', n_jobs=-1, cv=5).fit(X,y).best_estimator_\n    else:\n        model = GridSearchCV(clf, params, scoring='r2', n_jobs=-1, cv=5).fit(X,y).best_estimator_\n    end = time.time()\n    print('Training time: ')\n    show_time(end - start)\n    training_times.append(end - start)\n\n    # Predict\n    start = time.time()\n    if(type=='classification'):\n        scores.append(accuracy_score(y_predict, model.predict(X_predict)))\n    else:\n        scores.append(rmse(y_test, model.predict(X_test)))\n    end = time.time()\n    prediction_times.append(end - start)\n    print('Prediction time: ')\n    show_time(end - start)\n    return model","3546d7e6":"# Takes in model scores and plots them on a bar graph\ndef plot_metric(model_scores, score='Accuracy'):\n    # Set figure size\n    rcParams['figure.figsize'] = 7,5\n    plt.bar(model_scores['Model'], height=model_scores[score])\n    xlocs, xlabs = plt.xticks()\n    xlocs=[i for i in range(0,6)]\n    xlabs=[i for i in range(0,6)]\n    if(score != 'Prediction Times'):\n        for i, v in enumerate(model_scores[score]):\n            plt.text(xlocs[i] - 0.25, v + 0.01, str(v))\n    plt.xlabel('Model')\n    plt.ylabel(score)\n    plt.xticks(rotation=45)\n    plt.show()","bcf700bf":"# Takes in training data and a model, and plots a bar graph of the model's feature importances\ndef feature_importances(df, model, model_name, max_num_features=10):\n    feature_importances = pd.DataFrame(columns = ['feature', 'importance'])\n    feature_importances['feature'] = df.columns\n    feature_importances['importance'] = model.feature_importances_\n    feature_importances.sort_values(by='importance', ascending=False, inplace=True)\n    feature_importances = feature_importances[:max_num_features]\n    # print(feature_importances)\n    plt.figure(figsize=(12, 6));\n    sns.barplot(x=\"importance\", y=\"feature\", data=feature_importances);\n    plt.title(model_name+' features importance:');\n\n# Takes in training data and a model, and plots a bar graph of SHAP values\ndef shap_values(df, model, model_name):\n    shap_values = shap.TreeExplainer(model).shap_values(df)\n    shap_values[:5]\n    shap.summary_plot(shap_values, df.iloc[:1000,:])","a75eefa1":"# Read in dataset\nfetch_from = '..\/input\/fashionmnist\/fashion-mnist_train.csv'\ntrain = pd.read_csv(fetch_from)\n\nfetch_from = '..\/input\/fashionmnist\/fashion-mnist_test.csv'\ntest = pd.read_csv(fetch_from)","aaaf82d3":"# Perform train-test split\nX_train, y_train, X_test, y_test = train.iloc[:,1:], train['label'], test.iloc[:,1:], test['label']\nX_train.head()","dbe3ca3a":"X_train.shape, X_test.shape\n# Each image is 28*28(=784) pixels, hence the 784 features","fd43fc7f":"# Sample some images in the dataset\ndef plot_digits(instances, images_per_row=10, **options):\n    size = 28\n    images_per_row = min(len(instances), images_per_row)\n    images = [instance.reshape(size,size) for instance in instances]\n    n_rows = (len(instances) - 1) \/\/ images_per_row + 1\n    row_images = []\n    n_empty = n_rows * images_per_row - len(instances)\n    images.append(np.zeros((size, size * n_empty)))\n    for row in range(n_rows):\n        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n        row_images.append(np.concatenate(rimages, axis=1))\n    image = np.concatenate(row_images, axis=0)\n    plt.imshow(image, cmap = mpl.cm.binary, **options)\n    plt.axis(\"off\")\nplt.figure(figsize=(9,9))\nexample_images = X_train[:100]\nplot_digits(example_images.values, images_per_row=10)\nplt.show()","44a55a76":"prediction_times = []\ntraining_times = []\nscores = []\n# training_times\n# prediction_times","4817d1bf":"xgboost = Train(XGBClassifier(n_estimators=50, max_depth=5), X_train, y_train, X_test, y_test)","e711eb4e":"lgb = Train(LGBMClassifier(n_estimators=50, max_depth=5), X_train, y_train, X_test, y_test)","78424771":"cat = Train(CatBoostClassifier(n_estimators=50, verbose=False, max_depth=6), X_train, y_train, X_test, y_test)","c3f6d387":"# XGBoost with GridSearch\nparam_grid=[{'learning_rate':[0.05,0.1,0.15],\n            'max_depth':[5,10,20,50],\n            'min_child_weight' : [1,3,6],\n            'n_estimators':[200],\n            'colsample_bytree':[0.7,0.85]}]\nxgboost_gs = GridSearch(XGBClassifier(random_state=42), param_grid, X_train, y_train, X_test, y_test)","43a11942":"# LightGBM with GridSearch\nparam_grid=[{'learning_rate':[0.05,0.1,0.15],\n            'max_depth':[5,10,20,50],\n            'min_child_weight' : [1,3,6],\n            'num_leaves': [100,500,1200],\n            'n_estimators':[200],\n            'colsample_bytree':[0.7,0.85]}]\nlgb_gs = GridSearch(LGBMClassifier(random_state=42), param_grid, X_train, y_train, X_test, y_test)","97432f84":"# CatBoost with GridSearch\nstart = time.time()\nparam_grid=[{'learning_rate':[0.05,0.1,0.15],\n            'depth': [5,10,20,50],\n            'n_estimators':[200],\n            'iterations': [500]\n            'l2_leaf_reg': [1,4,9],\n            'rsm':[0.5,0.8]}]\ncat_gs = GridSearch(CatBoostClassifier(random_state=42, silent = True,\n                        bootstrap_type = 'Bernoulli'), param_grid, X_train, y_train, X_test, y_test)","71141dbb":"# free up memory be deleting dataframes no longer needed\ndel [[y_train, X_test, y_test, train, test]]","619abd59":"models = [('XGBoost', xgboost),\n         ('LightGBM', lgb),\n         ('CatBoost', cat),\n         ('XGBoost GridSearch', xgboost_gs),\n         ('LightGBM GridSearch', lgb_gs),\n         ('CatBoost GridSearch', cat_gs)]","6975501d":"model_scores = pd.DataFrame({ 'Model': [name for name, _ in models], 'Accuracy': scores })\nmodel_scores.sort_values(by='Accuracy',ascending=False,inplace=True)\nplot_metric(model_scores, score='Accuracy')","9a1443e6":"training_times = [round(time,2) for time in training_times]\nmodel_train_times = pd.DataFrame({ 'Model': [name for name, _ in models], 'Training Times': training_times })\nplot_metric(model_train_times, score='Training Times')","a672616e":"prediction_times = [round(time,2) for time in prediction_times]\nmodel_train_times = pd.DataFrame({ 'Model': [name for name, _ in models], 'Prediction Times': prediction_times })\nplot_metric(model_train_times, score='Prediction Times')","22c9e460":"# XGBoost\nfeature_importances(X_train, xgboost, 'XGBoost')","26f3f80d":"# CatBoost\nfeature_importances(X_train, cat, 'CatBoost')","a6f392ad":"# LightGBM\nfeature_importances(X_train, lgb, 'LightGBM')","3e0921da":"# XGBoost\nshap_values(X_train.iloc[:500,:], xgboost, 'XGBoost')","de8fb5f4":"# LightGBM\nshap_values(X_train.iloc[:500,:], lgb, 'LightGBM')","e9f675f5":"# Set figure size for decision tree plots\nrcParams['figure.figsize'] = 80,50","d3f08427":"# LightGBM\nlightgbm.plot_tree(lgb);","7c66ed31":"# XGBoost\nxgb.plot_tree(xgboost);","912615e1":"# Clear memory before moving onto the next round\nimport gc\ngc.collect()","52ac2a10":"del [[X_train]]","4fc474aa":"# Get data from New York City Taxi Fare Prediction\nn = 60000\ntrain = pd.read_csv('..\/input\/nyctaxi\/train.csv', nrows=n)\ntest = pd.read_csv('..\/input\/nyctaxi\/test.csv')\ntrain.head(5)","27e53ca0":"train.shape","125a850c":"test.shape","69eb1189":"# Feature Engineering\n# this cell was adapted from https:\/\/www.kaggle.com\/mahtieu\/nyc-taxi-fare-prediction-data-expl-xgboost\ndef feature_engineering(df):\n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n    #Drop rows with null values\n    df = df.dropna(how = 'any', axis = 'rows')\n    #Free rides, negative fares and passenger count filtering\n    df = df[df.eval('(fare_amount > 0) & (passenger_count <= 6)')]\n    # Coordinates filtering - Pickup and dropoff locations should be within the limits of NYC\n    df = df[(df.pickup_longitude >= -77) &\n                  (df.pickup_longitude <= -70) &\n                  (df.dropoff_longitude >= -77) &\n                  (df.dropoff_longitude <= 70) &\n                  (df.pickup_latitude >= 35) &\n                  (df.pickup_latitude <= 45) &\n                  (df.dropoff_latitude >= 35) &\n                  (df.dropoff_latitude <= 45)]\n\n    df.pickup_datetime = df.pickup_datetime.dt.tz_localize('UTC')\n    df.pickup_datetime = df.pickup_datetime.dt.tz_convert(tz.gettz('America\/New_York'))\n\n    # Fares may change every year\n    df['year'] = df.pickup_datetime.dt.year\n\n    # Different fares during weekdays and weekends\n    df['dayofweek'] = df.pickup_datetime.dt.dayofweek\n\n    # Different fares during public holidays\n    df['dayofyear'] = df.pickup_datetime.dt.dayofyear\n\n    # Different fares in peak periods and off-peak periods\n    df['hourofday'] = df.pickup_datetime.dt.hour\n\n    df = df.drop('pickup_datetime', axis=1)\n\n    # Computes the distance (in miles) between the pickup and the dropoff locations\n    df['distance'] = df.apply(\n        lambda x: distance.distance((x.pickup_latitude, x.pickup_longitude), (x.dropoff_latitude, x.dropoff_longitude)).miles,\n        axis = 1)\n\n    df = df[df.eval('(distance > 0) & (distance < 150)')]\n    fare_distance_ratio = (df.fare_amount\/df.distance)\n    fare_distance_ratio.describe()\n\n    (fare_distance_ratio[fare_distance_ratio < 45]).hist()\n\n    # Drop incoherent fares\n    df = df[fare_distance_ratio < 45]\n    del fare_distance_ratio\n\n    # Coordinates of the 3 airpots of NYC\n    airports = {'jfk': [40.6441666, -73.7822222],\n                'laguardia': [40.7747222, -73.8719444],\n                'newark': [40.6897222, -74.175]}\n\n    # Computes the distance between the pickup location and the airport\n    pickup = df.apply(lambda x: distance.distance((x.pickup_latitude, x.pickup_longitude), (airports.get('jfk'))).miles, axis=1)\n    # Computes the distance between the dropoff location and the airport\n    dropoff = df.apply(lambda x: distance.distance((x.dropoff_latitude, x.dropoff_longitude), (airports.get('jfk'))).miles, axis=1)\n    # Selects the shortest distance\n    df['to_jfk'] = pd.concat((pickup, dropoff), axis=1).min(axis=1)\n\n    pickup = df.apply(lambda x: distance.distance((x.pickup_latitude, x.pickup_longitude), (airports.get('laguardia'))).miles, axis=1)\n    dropoff = df.apply(lambda x: distance.distance((x.dropoff_latitude, x.dropoff_longitude), (airports.get('laguardia'))).miles, axis=1)\n    df['to_laguardia'] = pd.concat((pickup, dropoff), axis=1).min(axis=1)\n\n    pickup = df.apply(lambda x: distance.distance((x.pickup_latitude, x.pickup_longitude), (airports.get('newark'))).miles, axis=1)\n    dropoff = df.apply(lambda x: distance.distance((x.dropoff_latitude, x.dropoff_longitude), (airports.get('newark'))).miles, axis=1)\n    df['to_newark'] = pd.concat((pickup, dropoff), axis=1).min(axis=1)\n    del pickup, dropoff\n    return df\n\ndef remove_sparse(df):\n    features = [x for x in df.columns]\n    for feature in features:\n        if len(np.unique(df[feature]))<2:\n            df.drop(feature, axis=1, inplace=True)\n    return df","48cd0339":"train = remove_sparse(train)\ntest = remove_sparse(test)\ntrain = feature_engineering(train)\ntest = feature_engineering(test)\ny_train = train.fare_amount\nX_train = train.drop('fare_amount', axis=1)\ny_test = test.fare_amount\nX_test = test.drop('fare_amount', axis=1)","9afda13d":"X_train.head(5)","e913973c":"X_test.head(5)","2c5a9394":"prediction_times = []\ntraining_times = []\nscores = []\n# training_times\n# prediction_times","463a55fe":"def rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","49e3f3ed":"# XGBoost\nxgboost = Train(XGBRegressor(n_estimators=50,\n                        max_depth = 9,\n                        boosting_type = 'gbdt',\n                        learning_rate = 0.05,\n                        subsample = 0.85,\n                        colsample_bytree = 0.85,\n                        reg_alpha = 1e-4,\n                        silent = True,\n                        n_jobs = -1), X_train, y_train, X_test, y_test, type='reg')","7db63cf6":"# LightGBM\nlgb = Train(LGBMRegressor(n_estimators=50,\n                    max_depth = 9,\n                    boosting_type = 'gbdt',\n                    learning_rate = 0.05,\n                    subsample = 0.85,\n                    colsample_bytree = 0.85,\n                    reg_alpha = 1e-4,\n                    silent = True,\n                    n_jobs = -1), X_train, y_train, X_test, y_test, type='reg')","f30848c1":"# Catboost\ncat = Train(CatBoostRegressor(n_estimators=50,\n                        max_depth = 9,\n                        loss_function = 'RMSE',\n                        eval_metric = 'RMSE',\n                        learning_rate = 0.05,\n                        boosting_type = 'Plain',\n                        bootstrap_type = 'Bernoulli',\n                        subsample = 0.85,\n                        silent = True), X_train, y_train, X_test, y_test, type='reg')","86760194":"# XGBoost with GridSearch\nparam_grid = [{'n_estimators': [400],\n               'num_leaves': [100,500,1200],\n               'min_child_weight' : [1,3,6],\n               'max_depth':[5,10,20,50],\n               'colsample_bytree': [0.8, 0.9],\n               'learning_rate':[0.05,0.1,0.15],\n               'boosting_type': ['gbdt'],\n               'reg_alpha': [1e-4]\n               }]\nxgboost_gs = GridSearch(XGBRegressor(), param_grid, X_train[:4000], y_train[:4000], X_test, y_test, type='reg')","2e51e592":"# LightGBM with GridSearch\nparam_grid = [{'n_estimators': [400],\n               'num_leaves': [100,500,1200],\n               'min_child_weight' : [1,3,6],\n               'max_depth':[5,10,20,50],\n               'colsample_bytree': [0.8, 0.9],\n               'learning_rate':[0.05,0.1,0.15],\n               'boosting_type': ['gbdt'],\n               'reg_alpha': [1e-4]\n               }]\nlgb_gs = GridSearch(LGBMRegressor(), param_grid, X_train[:4000], y_train[:4000], X_test, y_test, type='reg')","e894e540":"# CatBoost with GridSearch\nparam_grid = [{'learning_rate':[0.05,0.1,0.15],\n                'depth': [5,10,20,50],\n                'n_estimators':[200],\n                'iterations': [500]\n                'l2_leaf_reg': [1,4,9],\n                'subsample': [0.8, 0.9]\n               }]\ncat_gs = GridSearch(CatBoostRegressor(loss_function = 'RMSE',\n                        eval_metric = 'RMSE',\n                        boosting_type = 'Plain',\n                        bootstrap_type = 'Bernoulli',\n                        silent = True), param_grid, X_train[:4000], y_train[:4000], X_test, y_test, type='reg')","9e98b989":"models = [('XGBoost', xgboost),\n         ('LightGBM', lgb),\n         ('CatBoost', cat),\n         ('XGBoost GridSearch', xgboost_gs),\n         ('LightGBM GridSearch', lgb_gs),\n         ('CatBoost GridSearch', cat_gs)]","4003cf21":"scores = [round(score) for score in scores]\nmodel_scores = pd.DataFrame({ 'Model': [name for name, _ in models], 'R2': scores })\nmodel_scores.sort_values(by='R2',ascending=False,inplace=True)\nplot_metric(model_scores, score='R2')","860433c5":"training_times = [round(time,2) for time in training_times]\nmodel_train_times = pd.DataFrame({ 'Model': [name for name, _ in models], 'Training Times': training_times })\nplot_metric(model_train_times, score='Training Times')","cdf80520":"model_train_times = pd.DataFrame({ 'Model': [name for name, _ in models], 'Prediction Times': prediction_times })\nplot_metric(model_train_times, score='Prediction Times')","fe029bd0":"# XGBoost\nfeature_importances(X_train, xgboost, 'XGBoost')","9d747e3e":"# CatBoost\nfeature_importances(X_train, cat, 'CatBoost')","35973f3d":"# LightGBM\nfeature_importances(X_train, lgb, 'LightGBM')","34315088":"# XGBoost\nshap_values(X_train.iloc[:500,:], xgboost, 'XGBoost')","56ef0357":"# LightGBM\nshap_values(X_train.iloc[:500,:], lgb, 'LightGBM')","51262308":"# Set figure size for decision tree plots\nrcParams['figure.figsize'] = 80,50","a6733d3c":"# LightGBM\nlightgbm.plot_tree(lgb);","d1e10e9b":"# XGBoost\nxgb.plot_tree(xgboost);","bc5a52c4":"# Clear memory before moving onto the next round\nimport gc\ngc.collect()","d6baeb01":"# Get data from New York City Taxi Fare Prediction\n# allocate 1000 rows for test set, the rest for training set\nn=2000000\ntrain = pd.read_csv('..\/input\/nyctaxi\/train_20mil.csv', nrows=n)\ntest = pd.read_csv('..\/input\/nyctaxi\/train_20mil.csv', skiprows=n)\ntrain.head(5)","ff48e636":"train.shape, test.shape","579c9ee2":"prediction_times = []\ntraining_times = []\nscores = []","24060533":"train = remove_sparse(train)\ntest = remove_sparse(test)\ntrain = feature_engineering(train)\ntest = feature_engineering(test)\ny_train = train.fare_amount\nX_train = train.drop('fare_amount', axis=1)\ny_test = test.fare_amount\nX_test = test.drop('fare_amount', axis=1)","7a50aca8":"X_train.head()","8a654b8c":"X_test.head()","ecd10bcb":"# XGBoost\nxgboost = Train(XGBRegressor(n_estimators=3,\n                        max_depth = 9,\n                        boosting_type = 'gbdt',\n                        learning_rate = 0.05,\n                        subsample = 0.85,\n                        colsample_bytree = 0.8,\n                        reg_alpha = 1e-4,\n                        silent = True,\n                        n_jobs = -1), X_train, y_train, X_test, y_test, type='reg')","e26032fd":"# LightGBM\nlgb = Train(LGBMRegressor(n_estimators=3,\n                    max_depth = 9,\n                    boosting_type = 'gbdt',\n                    learning_rate = 0.05,\n                    subsample = 0.85,\n                    colsample_bytree = 0.8,\n                    reg_alpha = 1e-4,\n                    silent = True,\n                    n_jobs = -1), X_train, y_train, X_test, y_test, type='reg')","833c5d44":"# Catboost\ncat = Train(CatBoostRegressor(n_estimators=3,\n                        max_depth = 9,\n                        loss_function = 'RMSE',\n                        eval_metric = 'RMSE',\n                        boosting_type = 'Plain',\n                        bootstrap_type = 'Bernoulli',\n                        learning_rate = 0.05,\n                        subsample = 0.85,\n                        silent = True), X_train, y_train, X_test, y_test, type='reg')","e8adc4c2":"models = [('XGBoost', xgboost),\n         ('LightGBM', lgb),\n         ('CatBoost', cat)]","c0c95062":"scores = [round(score) for score in scores]\nmodel_scores = pd.DataFrame({ 'Model': [name for name, _ in models], 'R2': scores })\nmodel_scores.sort_values(by='R2',ascending=False,inplace=True)\nplot_metric(model_scores, score='R2')","e99a684f":"training_times = [round(time,2) for time in training_times]\nmodel_train_times = pd.DataFrame({ 'Model': [name for name, _ in models], 'Training Times': training_times })\nplot_metric(model_train_times, score='Training Times')","418ef234":"model_train_times = pd.DataFrame({ 'Model': [name for name, _ in models], 'Prediction Times': prediction_times })\nplot_metric(model_train_times, score='Prediction Times')","5b5176dd":"# XGBoost\nfeature_importances(X_train, xgboost, 'XGBoost')","cdd296ed":"# CatBoost\nfeature_importances(X_train, cat, 'CatBoost')","fc2d9fd3":"# LightGBM\nfeature_importances(X_train, lgb, 'LightGBM')","a4dab8a9":"# XGBoost\nshap_values(X_train.iloc[:500,:], xgboost, 'XGBoost')","b13a7d5d":"# LightGBM\nshap_values(X_train.iloc[:500,:], lgb, 'LightGBM')","e24db539":"# Set figure size for decision tree plots\nrcParams['figure.figsize'] = 80,50","836e9d92":"# LightGBM\nlightgbm.plot_tree(lgb);","dd91bdb2":"# XGBoost\nxgb.plot_tree(xgboost);","4b5f3dfc":"# Clear memory before moving onto the next round\nimport gc\ngc.collect()","60306a96":"#### Feature Importances","f885aec8":"### LightGBM","ce3bc04b":"### R2 Scores","eb01ce77":"### LightGBM","6af4c983":"### Interpretability","4f4f226d":"## D. The Results","268bf231":"### XGBoost","4a1a9a06":"**CatBoost**\n\nCatBoost ships with no plotting function for its trees. If you really need to visualize CatBoost results, a work-around is proposed here: https:\/\/blog.csdn.net\/l_xzmy\/article\/details\/81532281","07baf080":"### Feature Importances","852ed28f":"## C. Fine-tuned models","d7dd27ca":"## B. Baseline models","b670ad49":"## A. Explore the Fashion MNIST dataset (60000 rows, 784 features)","e456016d":"## B. Baseline models","fd5c9e07":"## NYC Taxi dataset (2 million rows, 6 features)","7f43c0be":"#### Visualize Trees","8182dded":"## A. Baseline models","99f8a763":"# Round 2: Regression on a medium Dataset: Predict NYC Taxi fares","96ae1433":"### Interpretability","cb1e2a82":"**CatBoost**\nCatBoost doesn't work out of the box with shap_values() and results in the kernel crashing.","75f66314":"**CatBoost**\n\nCatBoost ships with no plotting function for its trees. If you really need to visualize CatBoost results, a work-around is proposed here: https:\/\/blog.csdn.net\/l_xzmy\/article\/details\/81532281","e0905f07":"# The Goal\n\n## What're we doing?\nWe're going to let XGBoost, LightGBM and Catboost battle it out in 3 rounds:\n\n- **Classification:** Classify images in the Fashion MNIST (60,000 rows, 784 features)\n\n- **Regression:** Predict NYC Taxi fares (60,000 rows, 7 features)\n\n- **Massive Dataset:** Predict NYC Taxi fares (2 million rows, 7 features)\n\n\n## How're we doing it?\nIn each round here are the steps we'll follow:\n1. Train baseline models of XGBoost, Catboost, LightGBM (trained using the same paramaters for each model)\n2. Train fine-tuned models of XGBoost, Catboost, LightGBM using GridSearchCV\n3. Measure performance on the following metrics:\n    - training and prediction times\n    - prediction score\n    - interpretability (feature importance, shap values, visualize trees)\n\n## What does this all mean?\nA detailed analysis of the results can be found on my blog at: https:\/\/lavanya.ai\/2019\/06\/27\/battle-of-the-boosting-algorithms\/\n\nI hope you find this analysis useful, I would love to hear your feedback on improving it! I encourage you to fork this kernel, and play with the code.\n\nIf you like this kernel, please give it an upvote. Thank you!","b83952e7":"#### Visualize Trees","bd85f5a5":"### Training and Prediction Times","919003fc":"### XGBoost","462137d6":"## A. Explore the NYC Taxi dataset (60,000 rows, 6 features)","6525a4c2":"### CatBoost","93fa75b9":"### LightGBM","aae7ca7a":"### CatBoost","a5a4d29e":"#### SHAP Values","9eacd0ea":"### XGBoost","bb6ae0a4":"#### SHAP Values","b7efe315":"### CatBoost","6c3593d6":"### CatBoost","437e52f4":"### R2 Scores","5ce1eab5":"## B. The Results","11b4e682":"## C. Fine-tuned models","6833b053":"**CatBoost**\n\nCatBoost ships with no plotting function for its trees. If you really need to visualize CatBoost results, a work-around is proposed here: https:\/\/blog.csdn.net\/l_xzmy\/article\/details\/81532281","a1bca055":"We're using the same dataset as above, but this time instead of training on 60,000 rows, we're going to train the models on 2 million rows and see if they're up to the challenge!","060fe647":"### Training and Prediction Times","ae0d0c69":"### CatBoost","9d1089e8":"#### Visualize Trees","e6a71e06":"### Accuracy Scores","d652caca":"#### SHAP Values","6363a9c0":"# Round 3: Regression on a massive Dataset: Predict NYC Taxi fares (2 million rows, 7 features)","4f72aef7":"# The Analysis\n\nI hope you find this analysis useful! I encourage you to fork this kernel, and play with the code.\n\nA detailed analysis of the results obtained in this kernel can be found on my blog at: https:\/\/lavanya.ai\/2019\/06\/27\/battle-of-the-boosting-algorithms\/\n\nIf you like this kernel, please give it an upvote. Thank you!","1c5322be":"A model's prediction score only paints a partial picture of its predictions. We also want to know *why* the model is making its predictions.\n\nHere we plot the model's feature importances, SHAP values and draw an actual decision tree to get a firmer understanding of the model's predictions.","a8ff58c1":"# Setup","d8805696":"# Round 1: Classification: Classify images in the Fashion MNIST","fc5bf64b":"#### Feature Importances","00be26d2":"### Training and Prediction Times","e2361b9e":"### LightGBM","c72bc248":"**CatBoost**\nCatBoost doesn't work out of the box with shap_values() and results in the kernel crashing.","3776e46e":"### XGBoost","c68632b9":"A model's prediction score only paints a partial picture of its predictions. We also want to know *why* the model is making its predictions.\n\nHere we plot the model's feature importances, SHAP values and draw an actual decision tree to get a firmer understanding of the model's predictions.","30e0fa23":"## D. The Results","fa27f899":"### LightGBM","0a455513":"Reference table for understanding which class names the class indexes refer to in the graphs below:\n\n| Class | Name |\n| --- | --- |\n| 0 | T-shirt\/top |\n| 1 | Trouser |\n| 2 | Pullover |\n| 3 | Dress |\n| 4 | Coat |\n| 5 | Sandal |\n| 6 | Shirt |\n| 7 | Sneaker |\n| 8 | Bag |\n| 9 | Ankle boot |","63e74068":"### XGBoost","6224ad93":"**CatBoost**\nCatBoost doesn't work out of the box with shap_values() and results in the kernel crashing."}}