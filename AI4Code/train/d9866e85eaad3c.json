{"cell_type":{"9ed348bb":"code","7e0d098c":"code","c61a9fdb":"code","162ced3d":"code","d896b6c6":"code","c5b5770d":"code","6b882184":"code","d6cdf041":"code","a1dd1b58":"code","a30f6463":"code","890b4e9a":"code","48b56a48":"code","ed892753":"code","93aed8d9":"code","d563c21b":"code","f553fe05":"code","541c6cbe":"code","c4a51a77":"code","22077861":"code","1e099665":"code","80ae336b":"code","63c8c7d3":"code","887055ed":"code","cb4833db":"code","0b507a96":"code","b636c550":"code","d27aed5c":"code","693c3dcd":"code","6aab74ee":"code","59daa2ab":"code","baf7148a":"code","5c559c1d":"code","587e92ea":"code","4345e188":"code","db6bd092":"code","bfe877d8":"code","9a73ea44":"code","d0a83058":"code","0968fb5f":"markdown","0a944efa":"markdown","9e8353b3":"markdown","4a512e04":"markdown","1cb7ed6d":"markdown","b284261f":"markdown","be96f512":"markdown","2f946e59":"markdown","b922c6b6":"markdown","5e26b2ab":"markdown","1606c15c":"markdown","c05e4cca":"markdown","7097f6a4":"markdown","720b3100":"markdown","6efbe375":"markdown","6d02cede":"markdown","d2ba1846":"markdown","0eda5c88":"markdown"},"source":{"9ed348bb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom scipy.stats import norm\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\npd.set_option(\"display.max_columns\", 100)\npd.set_option(\"display.max_rows\",100)","7e0d098c":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","c61a9fdb":"print(\"Shape of the training data:\",train.shape)\nprint(\"Shape of the testing data:\",test.shape)","162ced3d":"train.head()","d896b6c6":"train.info()","c5b5770d":"##lets check the statistics of the saleprice\ntrain['SalePrice'].describe()","6b882184":"## lets explore our target coloumn\n## check the distribution and also the statistics.\n### Distribution of a variable can be seen in 3 ways: kde, histogram, Rug plot. Distplot is combination of all.\nplt.figure(figsize=(10,5))\nplt.title(\"Distribution Plot of SalePrice\")\nsns.distplot(train['SalePrice'],kde=True,hist=True,rug=True,fit=norm)\nplt.show()","d6cdf041":"## lets look at the skewwness and kurtosis.\nprint(\" Skewness of Saleprice : \", train['SalePrice'].skew())\nprint(\" Kurtosis of Saleprice : \", train['SalePrice'].kurt())\n\n### lets treat the skewness of the target coloumn.\ntrain['SalePrice'] = np.log(train['SalePrice'])\nprint(\" Skewness of Saleprice after log transformation : \", train['SalePrice'].skew())","a1dd1b58":"## now lets again check how the distribution of target variable look like.\nplt.figure(figsize=(10,5))\nplt.title(\"Distribution Plot of SalePrice after Log Transformation\")\nsns.distplot(train['SalePrice'],kde=True,hist=True,rug=True,fit=norm)\nplt.show()","a30f6463":"## lets check distributions of few more variables.\n##lets check MSSubclass which Identifies the type of dwelling involved in the sale\nplt.figure(figsize=(10,5))\nplt.title(\"MSSubClass Distribution\")\nsns.countplot(x=train['MSSubClass'],order =train['MSSubClass'].value_counts().index )\n## seems like most of the houses are newer and not older than 1946 and 1 story. 20 (1-STORY 1946 & NEWER ALL STYLES) ","890b4e9a":"## check LotArea\nplt.figure(figsize=(10,5))\nplt.title(\"LotArea Distribution\")\nsns.distplot(train['LotArea'],rug=True, kde= False)\n## Right skewwed distribution\/ +ve skew distribution. Also have some unusual lot sizes which are more than 100k.","48b56a48":"## lets check neighborhood.\nplt.figure(figsize=(10,5))\nplt.title(\"Popular Neighborhood\")\nplt.xticks(rotation=90)\nsns.countplot(x=train['Neighborhood'],order =train['Neighborhood'].value_counts().index )\n## Names (North Ames),CollgCr\tCollege Creek,OldTown\tOld Town,Edwards\tEdwards are popular areas\/neighborhoods in Ames.a","ed892753":"##lets check yearbuilt\nplt.figure(figsize=(10,5))\nplt.title(\"Which year did the most number of huses were built\")\nplt.xticks(rotation=90)\nsns.distplot(train['YearBuilt'],bins=15,kde=False)\n## Most number of houses were built after 2000.","93aed8d9":"## lets check TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n## most of the houses have 6 rooms.\nsns.distplot(train['TotRmsAbvGrd'])","d563c21b":"varlist =['LotArea','YearBuilt','TotalBsmtSF','GrLivArea','GarageArea','PoolArea']\nfor var in varlist:\n    figure = plt.figure\n    ax = plt.gca()\n    sns.scatterplot(data=train,x=train[var],y=train['SalePrice'])\n    ax.set_title(\"{} vs {}\".format(var, 'SalePrice'))\n    plt.show()","f553fe05":"### we can see that there are two outliers in SP vs GrLiveArea where area > 4000 and SP < 30000 lets go ahead and remove them.\ntrain = train.drop(train[(train['GrLivArea'] > 4000) & (train['SalePrice'] < 300000)].index)","541c6cbe":"##Lets check relationship of Saleprice with few categorical columns.\ncatvar=['OverallQual','OverallCond','TotRmsAbvGrd']\nfor var in catvar:\n    figure = plt.figure\n    ax = plt.gca()\n    sns.boxplot(x=train[var],y=train['SalePrice'],data=train)\n    ax.set_title(\"{} vs {}\".format(var, 'SalePrice'))\n    plt.show()","c4a51a77":"## we can always check the correlation of saleprice with all the variables.\n## Correlation Plot\ncdf = train.corr()\nplt.figure(figsize=(25,25))\nsns.heatmap(cdf)\nplt.title(\"Correlation with Sale Price\")","22077861":"## lets check top features\n## Top correlated features\nprint(\"Top 12 highly correlated features with SalesPrice\")\ntop12=cdf.sort_values(by='SalePrice',ascending=False).head(1).transpose().sort_values(by='SalePrice',ascending=False).head(12)\ntop12","1e099665":"### lets get the pairplots for top features.\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars',\n       'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']\nsns.pairplot(train[cols])","80ae336b":"## As given in the documentation we can replace few missing values NA to None.\n## PoolQC,MiscFeature,Alley,Fence,FireplaceQu,GarageCond,GarageType,GarageFinish,GarageQual,BsmtExposure,BsmtFinType2 with None\n## LotFrontage (mean), GarageYrBlt( if there is no garage we can impute 0 here), MasVnrArea(mean), Electrical","63c8c7d3":"### Lets combine the dataset for missing value treatment and categorical encoding.\nntrain=train.shape[0]\nntest=test.shape[0]\ny_salesprice = train.SalePrice.values\ncom_data = pd.concat((train, test)).reset_index(drop=True)\ncom_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"Combined Data size is : {}\".format(com_data.shape))","887055ed":"## check missing columns in combined dataset.\ncountcom=com_data.isna().sum().sort_values(ascending= False)\npercentcom = (com_data.isna().sum()\/com_data.isnull().count()).sort_values(ascending= False)\nmissingcomdf= pd.concat([countcom, percentcom], axis=1, keys=['Total', 'Percent'])\nmissingcomdf[missingcomdf['Total']> 0]","cb4833db":"missingcomdf[missingcomdf['Total']> 0].index","0b507a96":"### Imputing missing column values\n### NA in below cols means there is no availability of such a facility. Hence we can impute None for NA.\nmissingcols_Cat1_none = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageCond','GarageQual', 'GarageType',\n                   'GarageFinish','BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'BsmtCond', 'BsmtQual','MasVnrType']\n## These are the columns which can be imputed with Mode.\nmissingcols_Cat2_mode = ['MSZoning','Utilities','Functional','Exterior1st','Exterior2nd','SaleType','Electrical','KitchenQual']\n## As there is no basement below cols will be obviously 0.\nmissingcols_bsmt_0 = ['BsmtFullBath','BsmtHalfBath','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF']\n## Since there is no garage available below columns will be 0.\nmissingcols_gar_0=['GarageArea', 'GarageCars','GarageYrBlt']\n## Since there is no Veneer type MasVnrArea becomes 0.\n## we can fill in LotFrontage with Median\/Mean value.\n\n##Imputing columns which can be None\nfor col in missingcols_Cat1_none:\n    com_data[col] = com_data[col].fillna('None')\n    \nfor col in missingcols_Cat2_mode:\n    com_data[col] = com_data[col].fillna(com_data[col].mode()[0])\n\nfor col in [missingcols_bsmt_0,missingcols_gar_0]:\n    com_data[col] = com_data[col].fillna(0)\n    \ncom_data['MasVnrArea'] = com_data['MasVnrArea'].fillna(0)\ncom_data['LotFrontage'] = com_data['LotFrontage'].fillna(com_data['LotFrontage'].mode()[0])\n","b636c550":"## lets check if there are any missing values in the combined dataset.\ncom_data.isnull().sum()","d27aed5c":"### Lets now deal with the categorical columns.\n## there are few columns which should be categorical like MSSubClass\n\nprint(\"Datatype before conversion:\",com_data['MSSubClass'].dtype)\ncom_data['MSSubClass'] = com_data['MSSubClass'].astype(str)\nprint(\"Datatype after conversion:\",com_data['MSSubClass'].dtype)","693c3dcd":"### Encoding all categorical columns into numerical.\n\n## we can do label encoding for cols whose unique values are <=3.\nle = ['Street','Alley','LandSlope','CentralAir','PavedDrive']\nlabelen = preprocessing.LabelEncoder()\nfor col in le:\n    com_data[col] = labelen.fit_transform(com_data[col])\n    print(com_data[col].unique())","6aab74ee":"### converting few columns like quality where order matters.\nExterQual_val = {'Ex': 5,'Gd': 4,'TA': 3,'Fa': 2,'Po': 1}\nExterCond_val = {'Ex': 5,'Gd': 4,'TA': 3,'Fa': 2,'Po': 1}\nBsmtQual_val = {'Ex': 5,'Gd': 4,'TA': 3,'Fa': 2,'Po': 1,'None': 0}\nBsmtCond_val = {'Ex': 5,'Gd': 4,'TA': 3,'Fa': 2,'Po': 1,'None': 0}\nBsmtExposure_val = {'Gd': 4,'Av': 3,'Mn': 2,'No': 1,'None': 0}\nBsmtFinType1_val = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1,'None': 0}\nBsmtFinType2_val = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1,'None': 0}\nHeatingQC_val = {'Ex': 5,'Gd': 4,'TA': 3,'Fa': 2,'Po': 1}\nKitchenQual_val = {'Ex': 5,'Gd': 4,'TA': 3,'Fa': 2,'Po': 1}\nFireplaceQu_val = {'Ex': 5,'Gd': 4,'TA': 3,'Fa': 2,'Po': 1,'None': 0}\nGarageQual_val = {'Ex': 5,'Gd': 4,'TA': 3,'Fa': 2,'Po': 1,'None': 0}\nGarageCond_val = {'Ex': 5,'Gd': 4,'TA': 3,'Fa': 2,'Po': 1,'None': 0}\nPoolQC_val = {'Ex': 4,'Gd': 3,'TA': 2,'Fa': 1,'None': 0}\n\n### map the values\ncom_data['ExterQual']= com_data['ExterQual'].map(ExterQual_val)\ncom_data['ExterCond']= com_data['ExterCond'].map(ExterCond_val)\ncom_data['BsmtQual']= com_data['BsmtQual'].map(BsmtQual_val)\ncom_data['BsmtCond']= com_data['BsmtCond'].map(BsmtCond_val)\ncom_data['BsmtExposure']= com_data['BsmtExposure'].map(BsmtExposure_val)\ncom_data['BsmtFinType1']= com_data['BsmtFinType1'].map(BsmtFinType1_val)\ncom_data['BsmtFinType2']= com_data['BsmtFinType2'].map(BsmtFinType2_val)\ncom_data['HeatingQC']= com_data['HeatingQC'].map(HeatingQC_val)\ncom_data['KitchenQual']= com_data['KitchenQual'].map(KitchenQual_val)\ncom_data['FireplaceQu']= com_data['FireplaceQu'].map(FireplaceQu_val)\ncom_data['GarageQual']= com_data['GarageQual'].map(GarageQual_val)\ncom_data['GarageCond']= com_data['GarageCond'].map(GarageCond_val)\ncom_data['PoolQC']= com_data['PoolQC'].map(PoolQC_val)","59daa2ab":"### One Hot Encoding on all the categorical coloumns.\nprint(\"Shape before one hot encoding:\",com_data.shape)\ncom_data = pd.get_dummies(com_data)\nprint(\"Shape after one hot encoding:\",com_data.shape)","baf7148a":"##Check the final dataset\ncom_data.head()","5c559c1d":"## Divide combined dataset into original train and test datasets.\ntrain_new = com_data[:ntrain]\ntest_new = com_data[ntrain:]","587e92ea":"## Check the dimension\nprint(train_new.shape)\nprint(test_new.shape)","4345e188":"### split the data into train and test \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\n\ntrain_X, test_X, train_y, test_y = train_test_split(train_new, y_salesprice, test_size=0.25)","db6bd092":"##### hyper parameter tuning for XGBOOST\n#XGBoost hyper-parameter tuning\ndef xgbtuning(X_train, y_train):\n    params = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 7],\n        'min_child_weight': [3, 5, 7, 9],\n        'subsample': [0.6, 0.7, 0.8, 0.9],\n        'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n        'n_estimators' : [1000],\n        'objective': ['reg:linear']\n    }\n\n    xgb_model = XGBRegressor()\n\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = params,                        \n                           cv = 5,\n                           n_jobs = -1,\n                           verbose = 1)\n\n    gsearch.fit(X_train,y_train)\n\n    return gsearch.best_params_","bfe877d8":"## Check for best parameters.\n##xgbtuning(train_X, train_y)\n\n## provides output like below.\n#[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n#[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.1min\n#[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 10.0min\n#[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 21.1min\n#[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 37.5min\n#[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 61.7min\n#[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 93.2min\n#[Parallel(n_jobs=-1)]: Done 1920 out of 1920 | elapsed: 102.2min finished\n#[15:10:59] WARNING: C:\/Users\/Administrator\/workspace\/xgboost-win64_release_1.0.0\/src\/objective\/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n#{'colsample_bytree': 0.7,\n# 'learning_rate': 0.1,\n# 'max_depth': 3,\n# 'min_child_weight': 3,\n# 'n_estimators': 1000,\n# 'objective': 'reg:linear',\n# 'subsample': 0.8}\n","9a73ea44":"xgb_model = XGBRegressor(\n        objective = 'reg:linear',\n        colsample_bytree = 0.7,\n        learning_rate = 0.1,\n        max_depth = 3,\n        min_child_weight = 3,\n        n_estimators = 1000,\n        subsample = 0.8)\n        #gamma=0.2)\n\n%time xgb_model.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(test_X, test_y)], verbose=False)\n\ny_pred_xgb = xgb_model.predict(test_X)\n\nprint(\"root Mean squared Error : \" + str(sqrt(mean_squared_error(test_y,y_pred_xgb))))","d0a83058":"ypred_xgb_grid = xgb_model.predict(test_new)\nprint(ypred_xgb_grid)\nsubmission =pd.DataFrame()\nsubmission['Id'] = test_new['Id']\nsubmission['SalePrice'] = np.exp(ypred_xgb_grid)\nsubmission.to_csv(\".\/Submissionxgbgrid.csv\",index=False)","0968fb5f":"Fit the Model on Best Parameters","0a944efa":"<font size=4> Missing Value Treatment <\/font>","9e8353b3":"### We can see below observations.\n#### Saleprice increases as LotArea increases. But there are some outliers after 100k sqft.\n#### Saleprice is increasing by year.\n#### For Houses with basement, saleprice increases with increase in basement size.one outlier after 6000sqft\n#### As GrLivArea increases, saleprice also increases again with two outliers.\n#### General trend says as garage area increases sp increases.\n#### There is no solid pattern for PoolArea.","4a512e04":"<font size=3> Importing all the Libraries <\/font>","1cb7ed6d":"Min price 34k, Max of 755k and Average price of the house being 180k.","b284261f":"<font size= 3> Check the shape\/dimension of the data<\/font>","be96f512":"<font size=3> Check the data <\/font>","2f946e59":"<font size =3> Handling Categorical Variables <\/font>","b922c6b6":"skewness > 0.5 is considered to moderately skewwed.\nkurtosis > 3 is considered to be higly skewwed.","5e26b2ab":"### Distribution is not normal. it is right skewwed or +ve skew distribution. we can always apply log transformations to make near normal.","1606c15c":"<font size=5>EDA and XGBoost Model Building <\/font>\n\nTanushree Wandkar- Feb 2021\n\n\n\n<font size=3> Objective: To predict the prices of the houses by using The Ames Housing dataset.\n\nData: 79 explanatory variables describing every aspect of residential homes in Ames,Iowa. <\/font>\n\n\n\n\nBelow are the contents of this notebook:\n\n1.Exploratory data analysis\n\n2.Missing value treatment\n\n3.Categorical value treatment\n\n4.Feature selection and extraction\n\n5.Model Building\n\n6.Testing and Validation\n\n\nI have learnt alot and used lot of information from below notebooks and I would like to thank them.\n1. https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python by Pedro Marcelino \n2. https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard by Serigne\n\n\nLets get started. Hope this helps.","c05e4cca":"Prediction on Test data","7097f6a4":"<font size=3> Overview of the data <\/font>","720b3100":"<font size=3> Read the data <\/font>","6efbe375":"#### As overallquality of the house increases there is a increase in saleprice.\n#### Also saleprice increases till 11 rooms after which it decreases.","6d02cede":"<font size =5> EDA <\/font>","d2ba1846":"### Now lets check how SalePrice is varies with other independent Numerical variables.\n### I think 'LotArea','YearBuilt','TotalBsmtSF','GrLivArea','GarageArea','PoolArea' might affect price of the house.lets see","0eda5c88":"<font size=3> Model Building <\/font>"}}