{"cell_type":{"65511206":"code","4b74a12b":"code","d712f76b":"code","d1b9c630":"code","a40cc904":"code","267c705b":"code","5652a522":"code","dac5ca2f":"code","cd065f07":"code","be8114b3":"code","961f9d82":"code","53b332bb":"code","606718e5":"code","7f8aaa9d":"code","fe63c506":"code","e8bca489":"code","0f23bb6c":"code","935f9b0c":"code","1cc7a575":"code","d6e1f6f6":"code","86b5b888":"code","b234b226":"code","b0b357de":"code","6b1f5cf6":"code","811021c3":"code","bad13f7c":"code","d1813353":"code","62151d92":"code","867c7342":"code","e0626394":"code","4e6086bf":"code","67adc499":"code","6025c9d4":"code","dbcb2eed":"code","34a1c38d":"markdown","7af36fb7":"markdown","fb7c700a":"markdown","880f7468":"markdown","20ab00d7":"markdown","eb4a34c5":"markdown","a08c69b4":"markdown","c152118a":"markdown","8d5175cf":"markdown","86c5998d":"markdown","ea95a7c7":"markdown","fd0f28b5":"markdown","5053b279":"markdown","9a39ee39":"markdown","0824aa4d":"markdown","03a62747":"markdown","5915ac3d":"markdown","dc876aea":"markdown","f0d560a0":"markdown","b9c3ad40":"markdown"},"source":{"65511206":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","4b74a12b":"train = pd.read_csv(\"..\/input\/jobathon-may-2021-credit-card-lead-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/jobathon-may-2021-credit-card-lead-prediction\/test.csv\")\ntrain","d712f76b":"train.info()","d1b9c630":"train.describe(include='all')","a40cc904":"train.isna().sum()","267c705b":"## for this case, i will delete the null value\n\ntrain = train[~train['Credit_Product'].isna()]\ntrain.isna().sum()","5652a522":"## Drop Feature which are not used\n#train = train.reset_index().set_index('ID')\ntrain = pd.DataFrame(train.reset_index(drop=True))\ntrain.head()","dac5ca2f":"# Visualizing Categorical features\n# Note we have filled the missng 'Credit_Product' with 'Missing' for sake of visualization\ncat_features = ['Gender','Region_Code','Occupation','Channel_Code','Credit_Product','Is_Active']\n\nplt.figure(figsize=(16, 14))\nsns.set(font_scale= 1.2)\nsns.set_style('ticks')\n\nfor i, feature in enumerate(cat_features):\n    plt.subplot(3, 2, i+1)\n    sns.countplot(data=train, x=feature, hue='Is_Lead', palette='rainbow')\n    if feature == 'Region_Code':\n        plt.xticks(rotation=90)\n    \nsns.despine()","cd065f07":"# It was found that the age could be dividen into age groups\nplt.figure(figsize=(16, 7))\ntemp = train.copy()\n\nsns.countplot(data=temp, x='Age', hue='Is_Lead', palette='autumn')\n\nplt.show()","be8114b3":"#We shall now plot the numberical variables to look at the distribution\nnumerical = ['Age','Vintage','Avg_Account_Balance']\nsns.pairplot(data=train,x_vars=numerical, hue = 'Is_Lead', palette='Set2')","961f9d82":"# We shall log trasform the variables and plot again\ntemp = train.copy()\ntemp[numerical] = np.log(train[numerical])\nsns.pairplot(data=temp,x_vars=numerical, hue = 'Is_Lead', palette='Set2')","53b332bb":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nX_orig = train.copy()\nX = np.zeros((len(train['Gender']),1))\nfor i, name in enumerate(cat_features):\n    x = label_encoder.fit_transform(train[name]).reshape(-1,1)\n    X = np.hstack((X,x))\n\nX = pd.DataFrame(X).drop([0],axis=1)\nX.columns = cat_features\nfor i, name in enumerate(numerical):\n    if name == 'Avg_Account_Balance':\n        X = pd.concat([X,np.log(train[name])],axis=1)\n    else:\n        X = pd.concat([X,train[name]],axis=1)\ndata = pd.concat([X,train['Is_Lead']],axis=1)\nY = data.iloc[:,-1:]","606718e5":"sns.countplot(x = \"Is_Lead\",data = data)","7f8aaa9d":"indices=range(len(X))\nX_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(X,Y,indices, test_size = 0.25, stratify=Y,\n                                                    random_state=123)","fe63c506":"X_test_raw = X_test.copy()\nscaler = StandardScaler()\nscaled_numfeats_train = pd.DataFrame(scaler.fit_transform(X_train[numerical]), \n                                     columns=numerical, index= X_train.index)\nfor col in numerical:\n    X_train[col] = scaled_numfeats_train[col]\n    \nscaled_numfeats_test = pd.DataFrame(scaler.transform(X_test[numerical]),\n                                    columns=numerical, index= X_test.index)\n\nfor col in numerical:\n    X_test[col] = scaled_numfeats_test[col]","e8bca489":"from imblearn.over_sampling import SMOTENC\nsmote_nc = SMOTENC(categorical_features=list(range(0,(len(X_train.columns)-len(numerical)))), random_state=0)\nX_train, y_train = smote_nc.fit_resample(X_train, y_train)","0f23bb6c":"y_train.value_counts()","935f9b0c":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV","1cc7a575":"key = ['LogisticRegression']\nvalue = [LogisticRegression(),KNeighborsClassifier(algorithm = 'kd_tree', n_jobs = 1, n_neighbors = 1, weights = 'uniform'),\n         SVC(C=.5, gamma = 0.1,kernel = 'rbf'),\n         DecisionTreeClassifier(),RandomForestClassifier(n_estimators = 1000),GradientBoostingClassifier(),AdaBoostClassifier(),xgb.XGBClassifier()]\nmodels = dict(zip(key,value))\nprint(models)","d6e1f6f6":"predicted =[]\nfor name,algo in models.items():\n    model=algo\n    model.fit(X_train,y_train)\n    predict = model.predict(X_test)\n    acc = accuracy_score(y_test, predict)\n    predicted.append(acc)\n    print(name,acc)","86b5b888":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout","b234b226":"model = xgb.XGBClassifier(num_class  =2, objective='multi:softprob')\nmodel.fit(X_train,y_train)\npredict = model.predict(X_test)\nacc = accuracy_score(y_test, predict)\nprint('XGB Accuracy: ',acc)","b0b357de":"tmp = predict = model.predict_proba(X_test)\ntmp[:,1]","6b1f5cf6":"test.isna().sum()","811021c3":"## for this case, i will delete the null value\n\ntest = test[~test['Credit_Product'].isna()]\ntest.isna().sum()","bad13f7c":"## Drop Feature which are not used\ntest = test.drop(['ID'],axis=1)\ntest = pd.DataFrame(test.reset_index(drop=True))\ntest.head()","d1813353":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nX_unseen = np.zeros((len(test['Gender']),1))\nfor i, name in enumerate(cat_features):\n    x_unseen = label_encoder.fit_transform(test[name]).reshape(-1,1)\n    X_unseen = np.hstack((X_unseen,x_unseen))\n\nX_unseen = pd.DataFrame(X_unseen).drop([0],axis=1)\nX_unseen.columns = cat_features\nfor i, name in enumerate(numerical):\n    if name == 'Avg_Account_Balance':\n        X_unseen = pd.concat([X_unseen,np.log(test[name])],axis=1)\n    else:\n        X_unseen = pd.concat([X_unseen,test[name]],axis=1)\nX_unseen.head()","62151d92":"scaled_numfeats_unseen = pd.DataFrame(scaler.fit_transform(X_unseen[numerical]), \n                                      columns=numerical, index= X_unseen.index)\nfor col in numerical:\n    X_unseen[col] = scaled_numfeats_unseen[col]\nX_unseen.head()","867c7342":"d1 = pd.DataFrame(y_test.values,columns = ['target'],index= y_test.index).reset_index()\nd2 = X_orig.loc[indices_test].reset_index()\ncols = X_test.columns\ncols = [x+\"_T\" for x in cols]\nX_test.columns=cols\nd3 = X_test.reset_index()\n\nprint(d2.shape)\ntemp = d2.merge(d1, left_index=True, right_index=True)\ntemp = d3.merge(temp, left_index=True, right_index=True)\ntemp.tail()","e0626394":"predicted = model.predict(X_test).reshape(-1,1)\npredicted_proba = model.predict_proba(X_test)[:,1]\n#X_test.reset_index(inplace = True)\n#target = pd.concat([X_orig.loc[indices_test],pd.DataFrame(y_test,columns = ['target'])],axis=1)\n#scores = pd.concat([pd.DataFrame(predicted_proba,columns = ['prediction_prob']),pd.DataFrame(predicted,columns = ['prediction'])],axis=1)\n\nfinal = pd.concat([temp,pd.DataFrame(predicted_proba,columns = ['prediction_prob']),pd.DataFrame(predicted,columns = ['prediction'])],axis=1)\n#final.dropna(thresh=2,inplace=True)\n#final = final[final.ID.notnull()]\nfinal.to_csv(\"otuput_data_test.csv\",index=False)\n#target.to_csv(\"target.csv\",index=False)\n#scores.to_csv(\"scores.csv\",index=False)","4e6086bf":"final[[\"Gender\",\"Gender_T\"]].tail()","67adc499":"final.columns","6025c9d4":"len(predicted_proba)\nacc = accuracy_score(final.Is_Lead, final.prediction)\nprint('XGB Accuracy: ',acc)\nprint(predicted_proba[0:10])","dbcb2eed":"X_test.tail()","34a1c38d":"## Check Null Value and Handling","7af36fb7":"### Check and Handling the Missing Value","fb7c700a":"## Dataset Information","880f7468":"## Prediction Using XGB","20ab00d7":"## Visuzalization","eb4a34c5":"### Balancing data","a08c69b4":"### Standarize Data","c152118a":"## Standarize Before Predict the Unseen Data","8d5175cf":"We should apply log transformation in \"Avg_Account_Balance\"","86c5998d":"We must balancing the data firstly","ea95a7c7":"## Results","fd0f28b5":"## Deep Neural Network","5053b279":"### Train - Test Split","9a39ee39":"## Data Preprocessing","0824aa4d":"### Check Distribution of Data","03a62747":"## Machine Learning Model","5915ac3d":"Firstly, we will convert categorical to numerical data","dc876aea":"## Prediction 'Is_Lead' of Unseen Data","f0d560a0":"## Preprocessing Data Before Predict the Unseen Data","b9c3ad40":"## Load Dataset"}}