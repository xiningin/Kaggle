{"cell_type":{"bd99611c":"code","a08213df":"code","f4e32e2e":"code","1fdaf1fb":"code","8ede3943":"code","68438d74":"code","d56c57f3":"code","63bc21ca":"code","9802308e":"code","ae34800a":"code","4c671377":"code","e8f4733e":"code","5086c666":"code","2e1ac38d":"code","19e2624e":"code","21fc36d8":"code","6f8391c9":"code","a0a6693b":"code","849de495":"code","5611bcfb":"code","22ec90ab":"code","9789e0c5":"code","f8c12248":"code","0276546d":"code","113cdf5a":"code","496fd657":"code","70c548fa":"code","459b8b34":"code","faa87c91":"code","8b8e5a26":"code","d009c622":"code","65175eec":"code","d1da0dcd":"code","338d0b53":"code","8047c75e":"code","2b92879e":"code","ee5c4041":"code","177b876e":"code","f5827988":"code","5d9d4316":"code","f7b2aab0":"code","eaf5e1c3":"code","60bd74bd":"code","5f2f08b8":"code","88d69f62":"code","ccc72378":"code","7fb0978f":"code","c9e698d7":"code","00734f4c":"code","475fa081":"code","33d49b72":"code","14a39bcd":"code","28d7d42e":"code","04720c6e":"code","fde5a95a":"markdown","d397fc0f":"markdown","fb7bba46":"markdown","85f9f142":"markdown","a274ec0a":"markdown","ff867f15":"markdown","c318a1b2":"markdown","9b7556fa":"markdown","ef17a559":"markdown","028ddd29":"markdown","80732345":"markdown","069530cb":"markdown","b54d76b6":"markdown","9a0cd27d":"markdown","b0d242cc":"markdown","66cb4c5a":"markdown","0e4e8f5a":"markdown","75448426":"markdown","aa316d0a":"markdown","1b444473":"markdown","3c6ec32e":"markdown","5814a2f5":"markdown","5a1e3d8a":"markdown","d145af44":"markdown","633fbbf6":"markdown","23edde74":"markdown","8c0ebd8f":"markdown"},"source":{"bd99611c":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a08213df":"!unzip ..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\n!unzip ..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip\n!unzip ..\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv.zip\n!unzip ..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip","f4e32e2e":"import os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport matplotlib.pyplot as plt\nimport torch as nn\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn import Dropout , Linear ","1fdaf1fb":"### Config File\nTRAIN = '.\/train.csv'\nTEST = '.\/test.csv'\nTEST_LABEL = '.\/test_labels.csv'\nSAMPLE = '.\/sample_submission.csv'\nEPOCHS = 2","8ede3943":"train_csv = pd.read_csv(TRAIN)\ntest_csv = pd.read_csv(TEST)\ntest_label = pd.read_csv(TEST_LABEL)\nsample_sub = pd.read_csv(SAMPLE)\ntrain_csv.describe()","68438d74":"train_csv.head()","d56c57f3":"train_csv['toxic'].unique()","63bc21ca":"train_csv['severe_toxic'].unique()","9802308e":"train_csv['comment_text'][3]","ae34800a":"train_csv['comment_text'][5]","4c671377":"import re\ndef text_preprocessing(text):\n    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n    text = re.sub(r'[0-9]+' , '' ,text)\n    text = re.sub(r'\\s([@][\\w_-]+)', '', text).strip()\n    text = re.sub(r'&amp;', '&', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    text = text.replace(\"#\" , \" \")\n    encoded_string = text.encode(\"ascii\", \"ignore\")\n    decode_string = encoded_string.decode()\n    return decode_string","e8f4733e":"print('Original sentence :'  , train_csv['comment_text'][3])\nprint('Processed Sentence :' , text_preprocessing(train_csv['comment_text'][3]))","5086c666":"test_csv.head()","2e1ac38d":"print('Original : ' ,test_csv['comment_text'][2])\nprint('Processed :' , text_preprocessing(test_csv['comment_text'][2]))","19e2624e":"test_label.head()","21fc36d8":"sample_sub.head()","6f8391c9":"X = []\nfor items in train_csv['comment_text']:\n    X.append(text_preprocessing(items))","a0a6693b":"commonWord = ' '.join(X)","849de495":"from wordcloud import WordCloud,STOPWORDS\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(commonWord)","5611bcfb":"plt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","22ec90ab":"fig , axes = plt.subplots(2,3,figsize = (10,10), constrained_layout = True)\nsns.countplot(ax=axes[0,0],x='toxic',data=train_csv )\nsns.countplot(ax=axes[0,1],x='severe_toxic',data=train_csv)\nsns.countplot(ax=axes[0,2],x='obscene',data=train_csv)\nsns.countplot(ax = axes[1,0],x='threat',data=train_csv)\nsns.countplot(ax=axes[1,1],x='insult',data=train_csv)\nsns.countplot(ax=axes[1,2],x='identity_hate',data=train_csv)\nplt.suptitle('No Of Classes Of Each Category')\nplt.show()","9789e0c5":"X = train_csv.comment_text.values\nY = train_csv.toxic.values\nX_train, X_val, y_train, y_val =\\\n    train_test_split(X, Y, test_size=0.1, random_state=2021)","f8c12248":"import torch\n\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","0276546d":"#!pip install transformers","113cdf5a":"#pip install transformers==4.5.1","496fd657":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=True)\ndef preprocessing_for_bert(data):\n    input_ids = []\n    attention_masks = []\n\n    # For every sentence...\n    for sent in data:\n        encoded_sent = tokenizer.encode_plus(\n            text=text_preprocessing(sent),  # Preprocess sentence\n            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n            max_length=MAX_LEN,                  # Max length to truncate\/pad\n            pad_to_max_length=True,         # Pad sentence to max length\n            #return_tensors='pt',           # Return PyTorch tensor\n            truncation = True,\n            return_attention_mask=True      # Return attention mask\n            )\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n\n    # Convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    return input_ids, attention_masks","70c548fa":"MAX_LEN = 300\n\ntoken_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\nprint('Original: ', X[0])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train)\nval_inputs, val_masks = preprocessing_for_bert(X_val)","459b8b34":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 32\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","faa87c91":"%time\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel\n\n# Create the BertClassfier class\nclass BertClassifier(nn.Module):\n    def __init__(self, freeze_bert=False):\n        super(BertClassifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        D_in, H, D_out = 768, 50, 2\n\n        # Instantiate BERT model\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        # self.LSTM = nn.LSTM(D_in,D_in,bidirectional=True)\n        # self.clf = nn.Linear(D_in*2,2)\n\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            # nn.LSTM(D_in,D_in)\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        # Feed input to BERT\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)\n        # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:, 0, :]\n        # Feed input to classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits","8b8e5a26":"from transformers import AdamW, get_linear_schedule_with_warmup\n\ndef initialize_model(epochs=4):\n    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n    \"\"\"\n    # Instantiate Bert Classifier\n    bert_classifier = BertClassifier(freeze_bert=False)\n\n    # Tell PyTorch to run the model on GPU\n    bert_classifier.to(device)\n\n    # Create the optimizer\n    optimizer = AdamW(bert_classifier.parameters(),\n                      lr=5e-5,    # Default learning rate\n                      eps=1e-8    # Default epsilon value\n                      )\n\n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","d009c622":"import random\nimport time\n\n# Specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            # Load batch to GPU\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20 batches\n            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training results\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss \/ batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss \/ len(train_dataloader)\n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n    \n    print(\"Training complete!\")\n    #import torch\n    #torch.save(model,'.\/')\n    #print('Model saved')\n\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's performance\n    on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","65175eec":"set_seed(42)    # Set seed for reproducibility\nbert_classifier, optimizer, scheduler = initialize_model(epochs=2)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)","d1da0dcd":"mkdir out","338d0b53":"#cd out","8047c75e":"#cd ..","2b92879e":"ls","ee5c4041":"#print(bert_classifier.state_dict().keys())","177b876e":"#checkpoint = {'model': BertClassifier(),\n#             'state_dict': bert_classifier.state_dict(),\n#             'optimizer': optimizer.state_dict()}\n#torch.save(checkpoint, 'out\/checkpoint.pth')","f5827988":"#def load_checkpoint(filepath):\n#    cp = torch.load(filepath)\n#    model = cp['model']\n#    model.load_state_dict(cp['state_dict'])\n#        param.requires_grad(False)\n#    model.eval()\n#    return model","5d9d4316":"#model = load_checkpoint('out\/checkpoint.pth')\n#print(model)","f7b2aab0":"import gzip, pickle, pickletools\nwith gzip.open('bert_uncsd.pkl', \"wb\") as f:\n    pickled = pickle.dumps(bert_classifier)\n    optimized_pickle = pickletools.optimize(pickled)\n    f.write(optimized_pickle)","eaf5e1c3":"mdl = BertClassifier()\nwith gzip.open('bert_uncsd.pkl', 'rb') as f:\n    p = pickle.Unpickler(f)\n    mdl = p.load()","60bd74bd":"torch.save(bert_classifier,'bertt.pth')","5f2f08b8":"import torch.nn.functional as F","88d69f62":"def bert_predict(model, test_dataloader):\n    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n    on the test set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    all_logits = []\n\n    # For each batch in our test set...\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n\n    return probs","ccc72378":"from sklearn.metrics import accuracy_score, roc_curve, auc\n\ndef evaluate_roc(probs, y_true):\n    \"\"\"\n    - Print AUC and accuracy on the test set\n    - Plot ROC\n    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n    \"\"\"\n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n    print(f'AUC: {roc_auc:.4f}')\n       \n    # Get accuracy over the test set\n    y_pred = np.where(preds >= 0.5, 1, 0)\n    accuracy = accuracy_score(y_true, y_pred)\n    print(f'Accuracy: {accuracy*100:.2f}%')\n    \n    # Plot ROC AUC\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","7fb0978f":"probs = bert_predict(bert_classifier, val_dataloader)\nevaluate_roc(probs, y_val)","c9e698d7":"test_csv.comment_text[:10]","00734f4c":"test_inputs, test_masks = preprocessing_for_bert(test_csv.comment_text[:1000])\n\n# Create the DataLoader for our test set\ntest_dataset = TensorDataset(test_inputs, test_masks)\ntest_sampler = SequentialSampler(test_dataset[:1000])\ntest_dataloader = DataLoader(test_dataset[:1000], sampler=test_sampler, batch_size=32)","475fa081":"probs = bert_predict(bert_classifier, test_dataloader)\n\n# Get predictions from the probabilities\nthreshold = 0.9\npreds = np.where(probs[:, 1] > threshold, 1, 0)\n\n# Number of tweets predicted toxic\nprint(\"Number of tweets predicted toxic: \", preds.sum())","33d49b72":"for i in range(len(probs)):\n    #print(probs[i][0])\n    #break\n    if probs[i][1]>0.5:\n        print(test_csv.comment_text[i])","14a39bcd":"test_csv.drop('comment_text',axis = 1,inplace=True)","28d7d42e":"test_csv['severe_toxic'] = 0\ntest_csv['obscene'] = 0\ntest_csv['threat'] = 0\ntest_csv['insult'] = 0\ntest_csv['identify_hate'] = 0","04720c6e":"test_csv.to_csv('submission.csv')","fde5a95a":"<hr style=\"border:4px solid pink\"> <\/hr>","d397fc0f":"    TEXT PREPROCESSING FOR BERT","fb7bba46":"<hr style=\"border:4px solid pink\"> <\/hr>","85f9f142":"<hr style=\"border:4px solid pink\"> <\/hr>","a274ec0a":"<hr style=\"border:4px solid pink\"> <\/hr>\n","ff867f15":"    Unziping the data\n","c318a1b2":"test_csv['toxic'] = preds","9b7556fa":"<hr style=\"border:4px solid pink\"> <\/hr>","ef17a559":"    SETTING UP THE GPU IF POSSIBLE","028ddd29":"<hr style=\"border:4px solid pink\"> <\/hr>","80732345":"    ANALYZING THE MOST COMMON WORD IN OUR TRAIN DATASET USING WORDCLOUD","069530cb":"    CONFIG FILE","b54d76b6":"<hr style=\"border:4px solid pink\"> <\/hr>\n","9a0cd27d":"    BERT Fine-Tuning Tutorial with PyTorch","b0d242cc":"    TRANSFORMERS \n    State-of-the-art Natural Language Processing for PyTorch and TensorFlow 2.0 \n\n**\ud83e\udd17 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation, etc in 100+ languages. Its aim is to make cutting-edge NLP easier to use for everyone.**\n\n**\ud83e\udd17 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets then share them with the community on our model hub. At the same time, each python module defining an architecture can be used as a standalone and modified to enable quick research experiments.**\n\n**\ud83e\udd17 Transformers is backed by the two most popular deep learning libraries, PyTorch and TensorFlow, with a seamless integration between them, allowing you to train your models with one then load it for inference with the other.**\n\n\n[LINK TO GITHUB REPO OF THE TRANSFORMERS](http:\/\/github.com\/huggingface\/transformers)","66cb4c5a":"<hr style=\"border:4px solid pink\"> <\/hr>\n","0e4e8f5a":"<hr style=\"border:4px solid pink\"> <\/hr>\n","75448426":"    SPLITTING YOUR TRAIN DATA INTO TRAIN AND VALIDATION SET FOR TRAINING","aa316d0a":"<hr style=\"border:4px solid pink\"> <\/hr>\n","1b444473":"<hr style=\"border:4px solid pink\"> <\/hr>","3c6ec32e":"    We are required to:\n1. Add special tokens to the start and end of each sentence.\n2. Pad & truncate all sentences to a single constant length.\n3. Explicitly differentiate real tokens from padding tokens with the \u201cattention mask\u201d.","5814a2f5":"    PREDICTING NUMBER OF TOXIC TWEETS","5a1e3d8a":"    Installing the Hugging Face Library","d145af44":"    The BERT model has 201 different named parameters.\n\n    ==== Embedding Layer ====\n\n    bert.embeddings.word_embeddings.weight                  (30522, 768)\n    bert.embeddings.position_embeddings.weight                (512, 768)\n    bert.embeddings.token_type_embeddings.weight                (2, 768)\n    bert.embeddings.LayerNorm.weight                              (768,)\n    bert.embeddings.LayerNorm.bias                                (768,)\n\n    ==== First Transformer ====\n\n    bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n    bert.encoder.layer.0.attention.self.query.bias                (768,)\n    bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n    bert.encoder.layer.0.attention.self.key.bias                  (768,)\n    bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n    bert.encoder.layer.0.attention.self.value.bias                (768,)\n    bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n    bert.encoder.layer.0.attention.output.dense.bias              (768,)\n    bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n    bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n    bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n    bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n    bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n    bert.encoder.layer.0.output.dense.bias                        (768,)\n    bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n    bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n\n    ==== Output Layer ====\n\n    bert.pooler.dense.weight                                  (768, 768)\n    bert.pooler.dense.bias                                        (768,)\n    classifier.weight                                           (2, 768)\n    classifier.bias                                                 (2,)","633fbbf6":"<hr style=\"border:4px solid pink\"> <\/hr>\n","23edde74":"    IMPORTING THE LIBRARIES\n","8c0ebd8f":"    AUC SCORE - 0.9853"}}