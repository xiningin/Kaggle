{"cell_type":{"d0ad37c3":"code","b1a36701":"code","a37dbc20":"code","1d9fdeb3":"code","2949808d":"code","1260c5d6":"code","109d7fc0":"code","5669225c":"code","bc5f8b07":"code","54fb1b77":"code","6a859d63":"code","c60e0fb3":"code","00aeb7c7":"code","3914f257":"code","ec4065e0":"code","ad1bf11a":"code","788b23b6":"code","c9e9a5d5":"code","a50a36e2":"code","0d45c931":"code","d2bbdbea":"code","e6cc60c3":"code","e693ad01":"markdown","180f03f8":"markdown","3e724a41":"markdown","1fe537ba":"markdown","e49a4d0d":"markdown","0aaa1ba2":"markdown","80e8ba06":"markdown","ebab415e":"markdown","190b782f":"markdown","c49b8b3e":"markdown","a6ad3a17":"markdown","660f1454":"markdown","541139b5":"markdown","c8bc9b6f":"markdown","af8533f2":"markdown","ec748518":"markdown","1a6b5649":"markdown","805fbc21":"markdown","60fe339b":"markdown","ab76a17b":"markdown","26cb134b":"markdown","f2374afd":"markdown","3b2080a2":"markdown","af528983":"markdown","dcb0d113":"markdown","beee48b0":"markdown","7f1d1ab2":"markdown","6f05c49b":"markdown","b4e8e2d1":"markdown","4e8de6bf":"markdown","e7dbc470":"markdown","8e3415bf":"markdown","4542711e":"markdown","f03c45db":"markdown","92039b7a":"markdown","f9f23923":"markdown","627a5b9a":"markdown","236baf9f":"markdown","9c1c85bb":"markdown","4210faca":"markdown"},"source":{"d0ad37c3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nfrom sklearn.model_selection import KFold,cross_val_score, RepeatedStratifiedKFold,StratifiedKFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler,PowerTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.dummy import DummyClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler,PowerTransformer,LabelEncoder\nfrom sklearn.metrics import accuracy_score,classification_report, recall_score,confusion_matrix, roc_auc_score, precision_score, f1_score, roc_curve, auc, plot_confusion_matrix,plot_roc_curve\n\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport optuna\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import make_column_transformer\n\nfrom sklearn.model_selection import KFold, cross_val_predict, train_test_split,GridSearchCV,cross_val_score\nfrom sklearn.metrics import accuracy_score,classification_report\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n\nimport plotly \nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b1a36701":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nfrom sklearn.model_selection import cross_val_score,cross_val_predict, train_test_split\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler,PowerTransformer,LabelEncoder\n\nfrom sklearn.metrics import accuracy_score,classification_report, recall_score,confusion_matrix, roc_auc_score, precision_score, f1_score, roc_curve, auc, plot_confusion_matrix,plot_roc_curve\n\n\nimport optuna\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\n#from lightgbm import LGBMClassifier, plot_importance\nfrom catboost import CatBoostClassifier\n\n\n\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n\nimport plotly \nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nimport shap \n\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a37dbc20":"pd.set_option('max_columns',100)\npd.set_option('max_rows',900)\n\npd.set_option('max_colwidth',200)\n\ndf = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf.head()","1d9fdeb3":"df.info()","2949808d":"df.duplicated().sum()","1260c5d6":"def missing (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values\n\nmissing(df)","109d7fc0":"numerical= df.drop(['HeartDisease'], axis=1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns\n\nprint(f'Numerical Columns:  {df[numerical].columns}')\nprint('\\n')\nprint(f'Categorical Columns: {df[categorical].columns}')","5669225c":"df[categorical].nunique()","bc5f8b07":"df.HeartDisease.value_counts()","54fb1b77":"df.HeartDisease.value_counts(normalize=True)","6a859d63":"df.HeartDisease.iplot(kind=\"hist\")","c60e0fb3":"accuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\n\n\ndf = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf1 = df.copy()\n\n#for target feature\nle = LabelEncoder()\ndf1['HeartDisease']=le.fit_transform(df1['HeartDisease'])\n\n#we convert real int into float and keep int-like categorical observations as they are\ndf1['Age']= df1['Age'].astype(float)\ndf1['RestingBP']=df1['RestingBP'].astype(float)\ndf1['Cholesterol']=df1['Cholesterol'].astype(float)\ndf1['MaxHR']=df1['MaxHR'].astype(float)\ndf1['FastingBS'] = df['FastingBS'].astype(float)\n\n\nX=df1.drop('HeartDisease', axis=1)\ny=df1['HeartDisease']\n\n#indeces of categorical observations\ncategorical_features_indices = np.where(X.dtypes != np.float)[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#since our dataset is not imbalanced, we do not have to use scale_pos_weight parameter to counter balance our results\n#catboost_5 = CatBoostClassifier(verbose=False,random_state=0,scale_pos_weight=5)\ncatboost_5 = CatBoostClassifier(verbose=False,random_state=0)\n\ncatboost_5.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_test, y_test),early_stopping_rounds=100)\ny_pred = catboost_5.predict(X_test)\n\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nrecall.append(round(recall_score(y_test, y_pred),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\nprecision.append(round(precision_score(y_test, y_pred),4))\n\nmodel_names = ['Catboost_adjusted_weight_5']\nresult_df1 = pd.DataFrame({'Accuracy':accuracy,'Recall':recall, 'Roc_Auc':roc_auc, 'Precision':precision}, index=model_names)\nresult_df1","00aeb7c7":"def objective(trial):\n    df = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\n    df1 = df.copy()\n    \n    le = LabelEncoder()\n    df1['HeartDisease']=le.fit_transform(df1['HeartDisease'])\n    \n    df1['Age']= df1['Age'].astype(float)\n    df1['RestingBP']=df1['RestingBP'].astype(float)\n    df1['Cholesterol']=df1['Cholesterol'].astype(float)\n    df1['MaxHR']=df1['MaxHR'].astype(float)\n    df1['FastingBS'] = df['FastingBS'].astype(float)\n\n    X= df1.drop('HeartDisease', axis=1)\n    y= df1['HeartDisease']\n    \n    categorical_features_indices = np.where(X.dtypes != np.float)[0]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    param = {\n        \"objective\": \"Logloss\",\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n        ),\n        \"used_ram_limit\": \"3gb\",\n    }\n\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    cat_cls = CatBoostClassifier(verbose=False,random_state=0,scale_pos_weight=1.2, **param)\n\n    cat_cls.fit(X_train, y_train, eval_set=[(X_test, y_test)], cat_features=categorical_features_indices,verbose=0, early_stopping_rounds=100)\n\n    preds = cat_cls.predict(X_test)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_test, pred_labels)\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100, timeout=600)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","3914f257":"accuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\n\n\ndf = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf1 = df.copy()\n\n#for target feature\nle = LabelEncoder()\ndf1['HeartDisease']=le.fit_transform(df1['HeartDisease'])\n\n#we convert real int into float and keep int-like categorical observations as they are\ndf1['Age']= df1['Age'].astype(float)\ndf1['RestingBP']=df1['RestingBP'].astype(float)\ndf1['Cholesterol']=df1['Cholesterol'].astype(float)\ndf1['MaxHR']=df1['MaxHR'].astype(float)\ndf1['FastingBS'] = df['FastingBS'].astype(float)\n\nX=df1.drop('HeartDisease', axis=1)\ny=df1['HeartDisease']\n\n#indeces of categorical observations\ncategorical_features_indices = np.where(X.dtypes != np.float)[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#since our dataset is not imbalanced, we do not have to use scale_pos_weight parameter to counter balance our results\n#catboost_5 = CatBoostClassifier(verbose=False,random_state=0,scale_pos_weight=5)\ncatboost_5 = CatBoostClassifier(verbose=False,random_state=0,\n                                colsample_bylevel=0.08879392186283433,\n                                depth=6,\n                                boosting_type=\"Plain\",\n                                bootstrap_type=\"Bayesian\",\n                                bagging_temperature=1.9053399201345542)\n\ncatboost_5.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_test, y_test), early_stopping_rounds=100)\ny_pred = catboost_5.predict(X_test)\n\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nrecall.append(round(recall_score(y_test, y_pred),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\nprecision.append(round(precision_score(y_test, y_pred),4))\n\nmodel_names = ['Catboost_adjusted_weight_5_optuna']\nresult_df2 = pd.DataFrame({'Accuracy':accuracy,'Recall':recall, 'Roc_Auc':roc_auc, 'Precision':precision}, index=model_names)\nresult_df2","ec4065e0":"accuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\n\n\ndf = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf1 = df.copy()\nle = LabelEncoder()\ndf1['HeartDisease']=le.fit_transform(df1['HeartDisease'])\n\n\nX= df1.drop('HeartDisease', axis=1)\ny= df1['HeartDisease']\n\n#if you want a variable to be perecived as categorical the you need to covert it to object type\nfor col in X.columns:\n    col_type = X[col].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        X[col] = X[col].astype('category')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#lgbmc_5=lgb.LGBMClassifier(random_state=0,scale_pos_weight=5)\nlgbmc_5=lgb.LGBMClassifier(random_state=0)\n\n#y_train,categorical_feature = 'auto' takes all categoricals automatically\nlgbmc_5.fit(X_train, y_train,categorical_feature = 'auto',eval_set=(X_test, y_test),feature_name='auto', verbose=0, early_stopping_rounds=100)\n\ny_pred = lgbmc_5.predict(X_test)\n\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nrecall.append(round(recall_score(y_test, y_pred),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\nprecision.append(round(precision_score(y_test, y_pred),4))\n\nmodel_names = ['LightGBM_adjusted_weight_5']\nresult_df3 = pd.DataFrame({'Accuracy':accuracy,'Recall':recall, 'Roc_Auc':roc_auc, 'Precision':precision}, index=model_names)\nresult_df3","ad1bf11a":"def objective(trial):\n    df = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\n    df1 = df.copy()\n    le = LabelEncoder()\n    df1['HeartDisease']=le.fit_transform(df1['HeartDisease'])\n   \n    df1['Age'] = df['Age'].astype(float)\n    df1['MaxHR'] = df['MaxHR'].astype(float)\n    df1['Cholesterol'] = df['Cholesterol'].astype(float)\n    df1['RestingBP'] = df['RestingBP'].astype(float)\n    df1['FastingBS'] = df['FastingBS'].astype(float)\n    \n    X= df1.drop('HeartDisease', axis=1)\n    y= df1['HeartDisease']\n    \n    for col in X.columns:\n        col_type = X[col].dtype\n        if col_type == 'object' or col_type.name == 'category':\n            X[col] = X[col].astype('category')    \n    \n    param = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"dart\",\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2,2000),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n    \n    lgbmc_adj=lgb.LGBMClassifier(random_state=0,scale_pos_weight=5,**param)\n    lgbmc_adj.fit(X_train, y_train,categorical_feature = 'auto',eval_set=(X_test, y_test),feature_name='auto', verbose=0, early_stopping_rounds=100)\n\n    preds = lgbmc_adj.predict(X_test)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_test, pred_labels)\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","788b23b6":"accuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\n\n\ndf = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf1 = df.copy()\nle = LabelEncoder()\ndf1['HeartDisease']=le.fit_transform(df1['HeartDisease'])\n\ndf1['Age'] = df['Age'].astype(float)\ndf1['MaxHR'] = df['MaxHR'].astype(float)\ndf1['Cholesterol'] = df['Cholesterol'].astype(float)\ndf1['RestingBP'] = df['RestingBP'].astype(float)\ndf1['FastingBS'] = df['FastingBS'].astype(float)\n\nX= df1.drop('HeartDisease', axis=1)\ny= df1['HeartDisease']\n\n#if you want a variable to be perecived as categorical then you need to covert it to object type\nfor col in X.columns:\n    col_type = X[col].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        X[col] = X[col].astype('category')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nlgbmc_5=lgb.LGBMClassifier(random_state=0,scale_pos_weight=5,\n                            num_leaves=1839,\n                            max_depth=7,\n                            lambda_l1=2.3073560633061619e-07,\n                            lambda_l2=0.015207212535708567,\n                            feature_fraction=0.7830686118906401,\n                            bagging_fraction=0.7628439841977933,\n                            bagging_freq=7,\n                            min_child_samples=7)\n\n#y_train,categorical_feature = 'auto' takes all categoricals automatically\nlgbmc_5.fit(X_train, y_train,categorical_feature = 'auto',eval_set=(X_test, y_test),feature_name='auto', verbose=0, early_stopping_rounds=100)\n\ny_pred = lgbmc_5.predict(X_test)\n\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nrecall.append(round(recall_score(y_test, y_pred),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\nprecision.append(round(precision_score(y_test, y_pred),4))\n\nmodel_names = ['LightGBM_adjusted_weight_5_optuna']\nresult_df4 = pd.DataFrame({'Accuracy':accuracy,'Recall':recall, 'Roc_Auc':roc_auc, 'Precision':precision}, index=model_names)\nresult_df4","c9e9a5d5":"from  xgboost import XGBClassifier\naccuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\n\n\ndf = pd.read_csv(\"\/kaggle\/input\/heart-failure-prediction\/heart.csv\")\ndf1 = df.copy()\nle = LabelEncoder()\ndf1['HeartDisease']=le.fit_transform(df1['HeartDisease'])\n\n#Since XGBoost does not handle categorical values itself, we use get_dummies to convert categorical variables into numeric variables.\ndf1= pd.get_dummies(df1)\nX= df1.drop('HeartDisease', axis=1)\ny= df1['HeartDisease']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nxgbc_5 = XGBClassifier(random_state=0)\n\nxgbc_5.fit(X_train, y_train)\ny_pred = xgbc_5.predict(X_test)\n\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nrecall.append(round(recall_score(y_test, y_pred),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\nprecision.append(round(precision_score(y_test, y_pred),4))\n\nmodel_names = ['XGBoost_adjusted_weight_5']\nresult_df5 = pd.DataFrame({'Accuracy':accuracy,'Recall':recall, 'Roc_Auc':roc_auc, 'Precision':precision}, index=model_names)\nresult_df5","a50a36e2":"import numpy as np\nimport optuna\n\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\ndef objective(trial):\n    \n    df = pd.read_csv(\"\/kaggle\/input\/heart-failure-prediction\/heart.csv\")\n    df1 = df.copy()\n    le = LabelEncoder()\n    df1['HeartDisease']=le.fit_transform(df1['HeartDisease'])\n\n    df1= pd.get_dummies(df1)\n    X= df1.drop('HeartDisease', axis=1)\n    y= df1['HeartDisease']\n    \n    #(data, target) = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25)\n    dtrain = xgb.DMatrix(train_x, label=train_y)\n    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n\n    param = {\n        \"verbosity\": 0,\n        \"objective\": \"binary:logistic\",\n        # use exact for small dataset.\n        \"tree_method\": \"exact\",\n        # defines booster, gblinear for linear functions.\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n        # L2 regularization weight.\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        # L1 regularization weight.\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n        # sampling ratio for training data.\n        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n        # sampling according to each tree.\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n    }\n\n    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n        # maximum depth of the tree, signifies complexity of the tree.\n        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n        # minimum child weight, larger the term more conservative the tree.\n        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n        # defines how selective algorithm is.\n        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n\n    if param[\"booster\"] == \"dart\":\n        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n\n    bst = xgb.train(param, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100, timeout=600)\n\n    print(\"Number of finished trials: \", len(study.trials))\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","0d45c931":"from  xgboost import XGBClassifier\naccuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\n\n\ndf = pd.read_csv(\"\/kaggle\/input\/heart-failure-prediction\/heart.csv\")\ndf1 = df.copy()\nle = LabelEncoder()\ndf1['HeartDisease']=le.fit_transform(df1['HeartDisease'])\n\ndf1= pd.get_dummies(df1)\nX= df1.drop('HeartDisease', axis=1)\ny= df1['HeartDisease']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nxgbc_5 = XGBClassifier(random_state=0,\n    booster=\"gblinear\",\n    lambda_=3.495744286068875e-07,\n    alpha=7.63164509093154e-06,\n    subsample=0.4223870512856449,\n    colsample_bytree=0.35105083241256907)\n\nxgbc_5.fit(X_train, y_train)\ny_pred = xgbc_5.predict(X_test)\n\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nrecall.append(round(recall_score(y_test, y_pred),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\nprecision.append(round(precision_score(y_test, y_pred),4))\n\nmodel_names = ['XGBoost_adjusted_weight_5_optuna']\nresult_df6 = pd.DataFrame({'Accuracy':accuracy,'Recall':recall, 'Roc_Auc':roc_auc, 'Precision':precision}, index=model_names)\nresult_df6","d2bbdbea":"result_final= pd.concat([result_df1,result_df2,result_df3,result_df4,result_df5,result_df6],axis=0)\nresult_final","e6cc60c3":"result_final.sort_values(by=['Accuracy'], ascending=True,inplace=True)\nfig = px.bar(result_final, x='Accuracy', y=result_final.index,title='Model Comparison',height=600,labels={'index':'MODELS'})\nfig.show()","e693ad01":"## XGBoost \n\n**With Default Parameters - All codes in one cell**","180f03f8":"# Model Comparision","3e724a41":"![image.png](attachment:5b60950a-51ef-4e2b-b012-0293cb117ce9.png)","1fe537ba":"## CatBoost \n\n**With Default Parameters - All codes in one cell**","e49a4d0d":"# CatBoost","0aaa1ba2":"- **OPTUNA** parameters give us a higher **Accuracy** score (0.92).","80e8ba06":"- After hyperparameter tunning with **OPTUNA** we have **Accuracy** score of 0.92.","ebab415e":"Unlike most Machine Learning models available today, **CatBoost** requires minimal data preparation. It handles:\n\n- Missing values for **Numeric** variables\n- Non encoded **Categorical** variables.\n\n\n**Note**: missing values have to be filled beforehand for **Categorical** variables. Common approaches replace NAs with a new category \u2018missing\u2019 or with the most frequent category.","190b782f":"## OPTUNA - Hyperparameter Tunning","c49b8b3e":"**It was developed by Yandex.**","a6ad3a17":"## OPTUNA - Hyperparameter Tunning","660f1454":"![](https:\/\/repository-images.githubusercontent.com\/64991887\/dc855780-e34b-11ea-9ab8-e08ca33288b0)","541139b5":"## Enjoy \ud83e\udd18","c8bc9b6f":"- With **OPTUNA** parameters, we manged to increase our **Accuracy** score from 0.89 to 0.90.","af8533f2":"## OPTUNA - Hyperparameter Tunning","ec748518":"## LightGBM \n\n**With Default Parameters - All codes in one cell**","1a6b5649":"In this notebook we will implement new beginner friendly end to end ML model by using CATBOOST, LightGMB, and XGBoost with Optuna HyperParameter Tuning.","805fbc21":"- Based on the data and data dictionary, we have a **classification** problem.\n- We wil make classification on the target variable **HeartDisease**.\n- And we will build a model to get best calssification possible on the target variable.\n- or that we will look at the balance of the target variable.\n- As we will see later, our target variable has **balanced** or **balanced like** data.\n- For that reason we will use **Accuracy** score.","60fe339b":"- We have developed model to classifiy heart disease cases.\n\n- First, we made the detailed exploratory analysis.\n\n- We have decided which metric to use (**Accuracy).\n\n- We looked in detail **Catboost**, **LightGBM**, and **XGBoost** models.\n\n- We made hyperparameter tuning of for each model with **OPTUNA** to see the improvement.","ab76a17b":"**It was developed by Microsoft.**","26cb134b":"- With default parameters, our **Accuracy** score is 0.82.","f2374afd":"- With our default parameters, we have 0.88 **Accuracy** score.","3b2080a2":"![](https:\/\/avatars.mds.yandex.net\/get-bunker\/56833\/dba868860690e7fe8b68223bb3b749ed8a36fbce\/orig)","af528983":"- No missing values, no duplication.","dcb0d113":"# CONCLUSION","beee48b0":"- After applying **OPTUNA** parameters to our **XGBoost** model, we get higher score than the one with default parameters.","7f1d1ab2":"# LightGBM","6f05c49b":"## CatBoost\n**With OPTUNA Parameters**","b4e8e2d1":"![](https:\/\/miro.medium.com\/max\/1400\/1*1kjLMDQMufaQoS-nNJfg1Q.png)","4e8de6bf":"- Almost 55% of the patients (508 patient) had a heart disease.\n- Almost 45% of patients (410 patients) didn't have a heart disease.","e7dbc470":"# Exploratory Data Analysis","8e3415bf":"# DATA","4542711e":"- With **OPTUNA** parameters, we get **Accuracy** score of 0.88 and have trade-off between **Recall** and **Precision** scores.","f03c45db":"- We have balanced-like target variable. Therefore, we will use accurcy as our evaluation metric.","92039b7a":"# Hi all. \ud83d\ude4b","f9f23923":"- **OPTUNA** parameters give higher **Accuracy** score.","627a5b9a":"# XGBoost","236baf9f":"- No zero variance and no extremely high variance.","9c1c85bb":"## LightGBM\n**With OPTUNA Parameters**","4210faca":"- With our default parameters, **CatBoost** gives us 0.8913 score of **Accuracy**."}}