{"cell_type":{"85d6565b":"code","3d5a2f54":"code","598f1e6d":"code","240b48d4":"code","ee8ef975":"code","8fea0240":"code","1b822f83":"code","79dd8632":"code","476e53ee":"code","69b6e03a":"code","32206bd2":"code","06d9f5b0":"code","099198fd":"code","6d96c1b8":"code","e1eb6379":"code","cb9f6f1d":"code","f07d8475":"code","9c82a0b6":"code","a661034c":"code","6ca23b61":"code","b4dbd77b":"code","4d9bce7b":"code","d40458fe":"code","1526209e":"code","69864aac":"code","471b1c9b":"code","218fdf9a":"code","abc71a1a":"code","0f86e9fa":"code","9b9661f6":"code","0aca1a10":"code","175208b3":"code","ca908eef":"code","19a2c7bb":"code","bf0e9684":"code","cf8aa8c3":"code","334d781f":"code","0b667d74":"code","1e3baaf2":"code","eff2ea67":"code","b1111f57":"code","79f2288a":"code","298f9eaf":"code","da62fc6d":"code","1f9656bd":"code","bd30f662":"code","a03df73e":"code","c4f23b42":"code","76e0fd8b":"code","fc52d7d6":"code","f6b684e8":"code","57e4b656":"code","b750f956":"code","13b567d2":"code","c64f7cd1":"code","97cba50c":"code","f2d701b5":"code","acf06517":"code","0ca81eaf":"code","d19a3e9f":"code","c6669c5e":"code","c309fceb":"code","7f135fa5":"code","652a1fec":"code","5a2e1c5d":"code","7f5488be":"code","48440a7b":"code","09d63177":"code","9fda4108":"code","158b2a04":"code","965b0fe7":"code","4d8d02f2":"code","a9241d3a":"code","ccb4fae0":"code","3c83a613":"code","55528b24":"code","3cec5e21":"code","d937d73f":"code","ebff1b2c":"code","2e1e0dd3":"code","868bc521":"code","25815f6e":"code","6ad89c28":"code","9d8a2d37":"code","cea778c5":"code","ec2d805e":"code","e7cb6270":"code","e2e4c560":"markdown","db1c6554":"markdown","3d318417":"markdown","0ac04670":"markdown","cf79c70c":"markdown","1b40361a":"markdown","83745549":"markdown","7604726a":"markdown","6a290267":"markdown","fc28a14e":"markdown","bd5d4a74":"markdown","3625a65a":"markdown","f6b239ce":"markdown","fdb0a070":"markdown","73f3a8c9":"markdown","4de3bd0a":"markdown","99ce3d66":"markdown","5c73f0f3":"markdown","afce73cb":"markdown","5d8bd674":"markdown","523ff481":"markdown","67c511fc":"markdown","b15a2421":"markdown","d27e3d2e":"markdown","2613691c":"markdown","5c153337":"markdown","b0dec2d2":"markdown","72754a4d":"markdown","f1b10961":"markdown","bb4be08a":"markdown","3316ed60":"markdown","594dbac4":"markdown","955996f1":"markdown","2320a586":"markdown","108ff106":"markdown","c00e0316":"markdown","d4280835":"markdown","98baaa1b":"markdown","a893d459":"markdown","bbd924d2":"markdown","67ce15f9":"markdown","4c075658":"markdown","9f52852b":"markdown","7531937e":"markdown","3d0d7e98":"markdown","b22f8578":"markdown","2abb44cf":"markdown","1d41015b":"markdown","f438f6b0":"markdown","87bb66bc":"markdown","391aba3f":"markdown","45b4c9a3":"markdown","948c8a02":"markdown"},"source":{"85d6565b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.multioutput import MultiOutputRegressor, RegressorChain","3d5a2f54":"import warnings \nwarnings.simplefilter(action = \"ignore\")","598f1e6d":"# Reading Train & Test datasets\ndf_train = pd.read_csv('..\/input\/bigquery-geotab-intersection-congestion\/train.csv')\ndf_test = pd.read_csv('..\/input\/bigquery-geotab-intersection-congestion\/test.csv')","240b48d4":"# display shape of the 2 datasets \n# Train dataset has 15 more columns than Test dataset and Test dataset has twice the number of observations\nprint (\"shape of train dataset :\", df_train.shape)\nprint (\"shape of test dataset :\", df_test.shape)","ee8ef975":"# display the first 5 observations of Train dataset \ndf_train.head()","8fea0240":"# display the first 5 observations of Test dataset \ndf_test.head()","1b822f83":"# train dataset info - 6 columns with text valuse and 22 with numeric values\ndf_train.info()","79dd8632":"# train dataset info - 6 columns with text valuse and 7 with numeric values\ndf_test.info()","476e53ee":"# display the ommon columns between the 2 datasets \nprint (\"Common columns between Train & Test datasets :\", np.intersect1d(df_train.columns, df_test.columns).tolist())","69b6e03a":"# display different columns between the 2 datasets - only in train dataset \nprint (\"Columns in Train dataset only :\", df_train.columns.symmetric_difference(df_test.columns).values )","32206bd2":"# Same Cities are used in Train & Test datasets\nprint (\"Cities in Train dataset:\", df_train['City'].unique().tolist())\nprint (\"Cities in Test dataset:\", df_test['City'].unique().tolist())","06d9f5b0":"# Number of observations per \"City\" in Train & Test dataset - similar distribution of data per city \ntrain_city = df_train.groupby('City').size().reset_index().rename(columns={0:'train'})\ntest_city = df_test.groupby('City').size().reset_index().rename(columns={0:'test'})\n\ndata = train_city.merge(test_city, on='City').sort_values('test')\ndisplay (data)\n\nsns.barplot(x='City',y='value',hue='variable',data=data.melt(id_vars='City', value_vars=['train','test']))","099198fd":"# Number of Intersections per City in Train & Test dataset - similar distribution of data per city \ntrain_intersection = df_train[['City', 'IntersectionId']].drop_duplicates().groupby('City').size().reset_index().rename(columns={0:'train'})\ntest_intersection = df_test[['City', 'IntersectionId']].drop_duplicates().groupby('City').size().reset_index().rename(columns={0:'test'})\n\ndata = train_intersection.merge(test_intersection, on='City').sort_values('train')\ndisplay(data)\n\nsns.barplot(x='City',y='value',hue='variable',data=data.melt(id_vars='City', value_vars=['train','test']))","6d96c1b8":"# Number of observation per Month in Train & Test dataset - Small about of data for Jan & May and missing data for Feb-Apr\ntrain_months = df_train.groupby('Month').size().reset_index().rename(columns={0:'train'})\ntest_months = df_test.groupby('Month').size().reset_index().rename(columns={0:'test'})\n\ndata = train_months.merge(test_months, on='Month')\ndisplay(data)\n\nsns.barplot(x='Month',y='value',hue='variable',data=data.melt(id_vars='Month', value_vars=['train','test']))","e1eb6379":"# Number of observation per Hour in Train & Test dataset - Similar distrbution of data in Train & Test datasets\ntrain_hours = df_train.groupby('Hour').size().reset_index().rename(columns={0:'train'})\ntest_hours = df_test.groupby('Hour').size().reset_index().rename(columns={0:'test'})\n\ndata = train_hours.merge(test_hours, on='Hour')\n#display(data)\n\nsns.barplot(x='Hour',y='value',hue='variable',data=data.melt(id_vars='Hour', value_vars=['train','test']))","cb9f6f1d":"# Descriptive statistics of the \"Total time stopped\"\ndf_train[['TotalTimeStopped_p20','TotalTimeStopped_p40','TotalTimeStopped_p50',\n          'TotalTimeStopped_p60','TotalTimeStopped_p80']].describe().T","f07d8475":"# Descriptive statistics of the \"Time from first stop\" \ndf_train[['TimeFromFirstStop_p20','TimeFromFirstStop_p40', 'TimeFromFirstStop_p50','TimeFromFirstStop_p60',\n          'TimeFromFirstStop_p80']].describe().T","9c82a0b6":"# Descriptive statistics of the \"\u2022Distance to first stop\" \ndf_train[['DistanceToFirstStop_p20','DistanceToFirstStop_p40', 'DistanceToFirstStop_p50','DistanceToFirstStop_p60',\n 'DistanceToFirstStop_p80']].describe().T","a661034c":"# Averages per City based on 50 & 80 percentile \ndf_train.groupby('City').agg({'TotalTimeStopped_p50':'mean','TimeFromFirstStop_p50':'mean','DistanceToFirstStop_p50':'mean','TotalTimeStopped_p80':'mean','TimeFromFirstStop_p80':'mean','DistanceToFirstStop_p80':'mean'})","6ca23b61":"data = df_train.groupby(['City','IntersectionId','Latitude','Longitude']).agg({'TotalTimeStopped_p50':'mean'}).reset_index()\n\nfig,axes=plt.subplots(nrows=2, ncols=2, figsize=(15,10))\nfor i,city in enumerate(data['City'].unique().tolist()):   \n    sns.scatterplot(x='Latitude',y='Longitude',data=data[data['City']==city],hue='TotalTimeStopped_p50',ax=axes[i%2,i\/\/2],legend=False)\n    axes[i%2,i\/\/2].set_title(city)\n    axes[i%2,i\/\/2].set_xlabel('')\n    axes[i%2,i\/\/2].set_ylabel('')","b4dbd77b":"# Hourly Traffic per City on Weekdays using 80 percentile \ndata = df_train[df_train['Weekend']==0].groupby(['City','Hour']).agg({'TotalTimeStopped_p80':'mean'}).reset_index()\n\nfig,axes = plt.subplots(nrows=1, ncols=data['City'].nunique(), figsize=(20,4), sharey=True)\nfor i,city in enumerate(data['City'].unique()):\n    sns.barplot(data=data[data['City']==city] ,x='Hour', y='TotalTimeStopped_p80',ax=axes[i], color='C0')\n    axes[i].set_ylabel('')\n    axes[i].set_title(city)\n    axes[i].set_xlabel('')\n    axes[i].get_xaxis().set_ticks([])\n    axes[i].spines['top'].set_visible(False)\n    axes[i].spines['right'].set_visible(False)\nplt.subplots_adjust(top=0.8)\nfig.suptitle('Hourly Traffic on Weekdays')","4d9bce7b":"# Hourly Traffic per City on Weekends using 80 percentile \ndata = df_train[df_train['Weekend']==1].groupby(['City','Hour']).agg({'TotalTimeStopped_p80':'mean'}).reset_index()\n\nfig,axes = plt.subplots(nrows=1, ncols=data['City'].nunique(), figsize=(20,4), sharey=True)\nfor i,city in enumerate(data['City'].unique()):\n    sns.barplot(data=data[data['City']==city] ,x='Hour', y='TotalTimeStopped_p80',ax=axes[i], color='C0')\n    axes[i].set_ylabel('')\n    axes[i].set_title(city)\n    axes[i].set_xlabel('')\n    axes[i].get_xaxis().set_ticks([])\n    axes[i].spines['top'].set_visible(False)\n    axes[i].spines['right'].set_visible(False)\nplt.subplots_adjust(top=0.8)\nfig.suptitle('Hourly Traffic on Weekends')","d40458fe":"# Monthly Traffic per Hour using 80 percentile \ndata = df_train.groupby(['Month','Hour']).agg({'TotalTimeStopped_p80':'mean'}).reset_index()\n\nfig,axes = plt.subplots(nrows=data['Month'].nunique()\/\/3, ncols=data['Month'].nunique()\/\/3, figsize=(15,8), sharey=True,sharex=True)\nfor i,month in enumerate(sorted(data['Month'].unique())):\n    sns.barplot(data=data[data['Month']==month] ,x='Hour', y='TotalTimeStopped_p80',ax=axes[i%3,i\/\/3], color='C0')\n    axes[i%3,i\/\/3].set_title(f'month = {month}')\n    axes[i%3,i\/\/3].set_ylabel('')\n    axes[i%3,i\/\/3].set_xlabel('')\n    axes[i%3,i\/\/3].get_xaxis().set_ticks([])\n    axes[i%3,i\/\/3].spines['top'].set_visible(False)\n    axes[i%3,i\/\/3].spines['right'].set_visible(False)","1526209e":"# Monthly averages based on 50 & 80 percentile \ndf_train.groupby('Month').agg({'Month':'count','TotalTimeStopped_p50':'mean','TimeFromFirstStop_p50':'mean','DistanceToFirstStop_p50':'mean','TotalTimeStopped_p80':'mean','TimeFromFirstStop_p80':'mean','DistanceToFirstStop_p80':'mean'}).rename(columns = {'Month':'Count'})","69864aac":"# Train correlation - \"Total Time\", \"Time from First Stop\" & \"Distance from First Stop\" are all postively correlated \ncorr = df_train.iloc[:,12:-1].corr()\nmask = np.triu(np.ones_like(corr,dtype=bool))\ncmap = sns.diverging_palette(250,15,s=75,l=40, n=9, center='light', as_cmap=True)\nfig = plt.figure(figsize=(12,12))\nsns.heatmap(corr, mask=mask, cmap=cmap, annot=True, fmt='.2f')","471b1c9b":"# check for Null Values in Train dataset - EntryStreetName & ExitStreetName are the only 2 columns with missing values \ndf_train.isna().sum()","218fdf9a":"# Ratio of missing data in Train dataset\ndf_train[['EntryStreetName','ExitStreetName']].isna().sum() \/ df_train.shape[0]","abc71a1a":"# check for Null Values in Test dataset - EntryStreetName & ExitStreetName are the only 2 columns with missing values \ndf_test.isna().sum()","0f86e9fa":"# Ratio of missing data in Test dataset\ndf_test[['EntryStreetName','ExitStreetName']].isna().sum() \/ df_test.shape[0]","9b9661f6":"# check some of data data to see if we can fill missing data\ndf_train[df_train['EntryStreetName'].isna()].tail()","0aca1a10":"# \"East\" entry of Intersection# 1696 in Philadelphia shows missing EntryStreetName \n# this means we cannot simply fill the data by using Intersection# & City\ndf_train[(df_train['IntersectionId']==1696) & (df_train['City']=='Philadelphia')].groupby(['EntryStreetName','ExitStreetName','EntryHeading','ExitHeading','Path'], dropna=False).size()","175208b3":"# Intersection can have mutiple entries and exits - Intersection# 0 in Boston has 4 Entries (NE,E,W,S) & 5 Exits (NE,NW,E,W,S)\ndf_train[(df_train['IntersectionId']==0) & (df_train['City']=='Boston')][['EntryStreetName','ExitStreetName','EntryHeading','ExitHeading','Path']].value_counts()","ca908eef":"# type of streets can be identified from the Street Name\ndata = df_train[['City','EntryStreetName','IntersectionId']].drop_duplicates()\nprint(\"Number of Avenues :\", data['EntryStreetName'].str.contains('Avenue').sum())\nprint(\"Number of Streets :\", data['EntryStreetName'].str.contains('Street').sum())\nprint(\"Number of Boulevards :\", data['EntryStreetName'].str.contains('Boulevard').sum())\nprint(\"Number of Roads:\", data['EntryStreetName'].str.contains('Road').sum())\nprint(\"Number of Highways :\", data['EntryStreetName'].str.contains('Highway').sum())\nprint(\"Number of Drives :\", data['EntryStreetName'].str.contains('Drive').sum())\nprint(\"Number of Parkways :\", data['EntryStreetName'].str.contains('Parkway').sum())","19a2c7bb":"# using directions to identify possible EntryHeading & ExitHeading values\nprint (df_train['EntryHeading'].unique())\nprint (df_train['ExitHeading'].unique())","bf0e9684":"# Total time stopped = the amount of time spent at 0 speed\ncols = ['TotalTimeStopped_p20','TotalTimeStopped_p40','TotalTimeStopped_p50', \n        'TotalTimeStopped_p60', 'TotalTimeStopped_p80']\nsns.boxplot(data=df_train[cols],orient='h')","cf8aa8c3":"# Time from first stop = time from the first stop until the vehicle passes through the intersection\ncols = ['TimeFromFirstStop_p20', 'TimeFromFirstStop_p40','TimeFromFirstStop_p50', \n        'TimeFromFirstStop_p60', 'TimeFromFirstStop_p80']\nsns.boxplot(data=df_train[cols],orient='h')","334d781f":"# Distance to first stop\ncols = ['DistanceToFirstStop_p20', 'DistanceToFirstStop_p40', 'DistanceToFirstStop_p50',\n       'DistanceToFirstStop_p60', 'DistanceToFirstStop_p80']\nsns.boxplot(data=df_train[cols],orient='h')","0b667d74":"# We start by creating a columns to identify the type of the Street\n#df_train['EntryStreetType'] = np.NaN\n#df_train['ExitStreetType'] = np.NaN\n#df_test['EntryStreetType'] = np.NaN\n#df_test['ExitStreetType'] = np.NaN\n\nstr_code = ['Avenue','Street','Boulevard','Road','Highway','Drive','Parkway','Square','Way','Ave','St','Pkwy','Lane','Circle','Place','Other']\nstr_name = ['Avenue','Street','Boulevard','Road','Highway','Drive','Parkway','Square','Way','Avenue','Street','Parkway','Lane','Circle','Place','Other']\n\nfor st in range(len(str_code)):\n    df_train.loc[~(df_train['EntryStreetName'].isna()) & (df_train['EntryStreetName'].str.contains(str_code[st])), 'EntryStreetType'] = str_name[st]\n    df_train.loc[~(df_train['ExitStreetName'].isna()) & (df_train['ExitStreetName'].str.contains(str_code[st])), 'ExitStreetType'] = str_name[st]\n    df_test.loc[~(df_test['EntryStreetName'].isna()) & (df_test['EntryStreetName'].str.contains(str_code[st])), 'EntryStreetType'] = str_name[st]\n    df_test.loc[~(df_test['ExitStreetName'].isna()) & (df_test['ExitStreetName'].str.contains(str_code[st])), 'ExitStreetType'] = str_name[st]\n    \ndf_train['EntryStreetType'].fillna('Other',inplace=True)\ndf_train['ExitStreetType'].fillna('Other',inplace=True)\n\ndf_test['EntryStreetType'].fillna('Other',inplace=True)\ndf_test['ExitStreetType'].fillna('Other',inplace=True)","1e3baaf2":"print (df_train[(df_train['EntryStreetType']=='Other') & ~(df_train['EntryStreetName'].isna())]['EntryStreetName'].unique())","eff2ea67":"# Number of intersection per \"EntryStreetType\" - mostly Streets and Avenues\ndf_train[['City','EntryStreetType','IntersectionId']].drop_duplicates().groupby('EntryStreetType',dropna=False).size().sort_values()","b1111f57":"# Averages per Street Type based on 50 & 80 percentile \ndf_train.groupby('EntryStreetType').agg({'RowId':'count','TotalTimeStopped_p50':'mean','TimeFromFirstStop_p50':'mean','DistanceToFirstStop_p50':'mean','TotalTimeStopped_p80':'mean','TimeFromFirstStop_p80':'mean','DistanceToFirstStop_p80':'mean'}).rename(columns = {'RowId':'Count'}).reset_index().sort_values('TotalTimeStopped_p50', ascending=False)","79f2288a":"# We can create columns to identify the number of directions for each intersection\nentry_data = df_train[['City','IntersectionId','EntryHeading']].drop_duplicates().groupby(['City','IntersectionId']).agg({'EntryHeading':'count'}).reset_index().rename(columns={'EntryHeading':'EntryCount'})\nexit_data = df_train[['City','IntersectionId','ExitHeading']].drop_duplicates().groupby(['City','IntersectionId']).agg({'ExitHeading':'count'}).reset_index().rename(columns={'ExitHeading':'ExitCount'})\n\n# Then we add Number of Entries & Exits for each intersection\ndf_train = df_train.merge(entry_data, on=['City','IntersectionId'], how='left')\ndf_train = df_train.merge(exit_data, on=['City','IntersectionId'], how='left')\ndf_train.head()","298f9eaf":"# Replicate for Test dataset\nentry_data = df_test[['City','IntersectionId','EntryHeading']].drop_duplicates().groupby(['City','IntersectionId']).agg({'EntryHeading':'count'}).reset_index().rename(columns={'EntryHeading':'EntryCount'})\nexit_data = df_test[['City','IntersectionId','ExitHeading']].drop_duplicates().groupby(['City','IntersectionId']).agg({'ExitHeading':'count'}).reset_index().rename(columns={'ExitHeading':'ExitCount'})\n\ndf_test = df_test.merge(entry_data, on=['City','IntersectionId'], how='left')\ndf_test = df_test.merge(exit_data, on=['City','IntersectionId'], how='left')\ndf_test.head()","da62fc6d":"# Averages per 'Entry Count' based on 50 & 80 percentile - intersecions with more entries are busier\ndf_train.groupby('EntryCount').agg({'RowId':'count','TotalTimeStopped_p50':'mean','TimeFromFirstStop_p50':'mean','DistanceToFirstStop_p50':'mean','TotalTimeStopped_p80':'mean','TimeFromFirstStop_p80':'mean','DistanceToFirstStop_p80':'mean'}).rename(columns = {'RowId':'Count'}).reset_index().sort_values('TotalTimeStopped_p50', ascending=False)","1f9656bd":"# EntryHeading & ExitHeading while keeping the sequence in order\nheading_map = {'N':1,'NE':2,'E':3,'SE':4,'S':5, 'SW':6, 'W':7, 'NW': 8}\n\ndf_train['EntryHeading'] = df_train['EntryHeading'].map(heading_map)\ndf_train['ExitHeading'] = df_train['ExitHeading'].map(heading_map)\ndf_test['EntryHeading'] = df_test['EntryHeading'].map(heading_map)\ndf_test['ExitHeading'] = df_test['ExitHeading'].map(heading_map)\n\ndf_train.head()","bd30f662":"# Turn Type - difference between Exit & Entry\ndf_train['TurnType'] = df_train['ExitHeading'] - df_train['EntryHeading']\ndf_test['TurnType'] = df_test['ExitHeading'] - df_test['EntryHeading']\ndf_train.head()","a03df73e":"# Averages per 'EntryExitDiff' based on 50 & 80 percentile \ndf_train.groupby('TurnType').agg({'RowId':'count','TotalTimeStopped_p50':'mean','TimeFromFirstStop_p50':'mean','DistanceToFirstStop_p50':'mean','TotalTimeStopped_p80':'mean','TimeFromFirstStop_p80':'mean','DistanceToFirstStop_p80':'mean'}).rename(columns = {'RowId':'Count'}).reset_index().sort_values('TotalTimeStopped_p50', ascending=False)","c4f23b42":"from sklearn.neighbors import DistanceMetric\n\ndef calc_distance(row):#(lat1, lon1, lat2, lon2):\n    R = 6373.0\n    lat1 = row['CCLatitude']\n    lon1 = row['CCLongitude']\n    lat2 = row['Latitude']\n    lon2 = row['Longitude']\n    dist = DistanceMetric.get_metric('haversine')\n    X = [[np.radians(lat1), np.radians(lon1)], [np.radians(lat2), np.radians(lon2)]]\n    distance = np.abs(np.array(R * dist.pairwise(X)).item(1))\n    return distance","76e0fd8b":"# https:\/\/www.latlong.net\/country\/united-states-236.html\ncities = ['Atlanta', 'Boston', 'Chicago', 'Philadelphia']\ncc_lat = [33.753746, 42.361145,41.881832,39.952583]\ncc_lon = [-84.386330,-71.057083,-87.623177,-75.165222]\n\nfor c in range(len(cities)):\n    df_train.loc[df_train['City']==cities[c], 'CCLatitude'] = cc_lat[c]\n    df_train.loc[df_train['City']==cities[c], 'CCLongitude'] = cc_lon[c]\n    df_test.loc[df_test['City']==cities[c], 'CCLatitude'] = cc_lat[c]\n    df_test.loc[df_test['City']==cities[c], 'CCLongitude'] = cc_lon[c]\n    \n    \ndf_train['CCDist'] = df_train.apply(calc_distance, axis=1)\ndf_test['CCDist'] = df_test.apply(calc_distance, axis=1)\ndf_train.head()","fc52d7d6":"# Use Month for indetify weather impact on traffic\n# https:\/\/www.climatestotravel.com\/\ntemp = [['Chicago',-4.6,-2.4,3.2,9.4,15.1,20.5,23.3,22.4,18.1,11.4,4.6,-2.3],\n['Boston',-1.5,0,3,9,14.5,19.5,23,22,18,12,7,1.5],\n['Atlanta',6.6,8.7,12.6,16.8,21.4,25.3,26.8,26.3,23.2,17.6,12.5,7.7],\n['Philadelphia',0.5,2.1,6.4,12.2,17.7,23,25.6,24.8,20.6,14.2,8.6,3]]\n\nrain = [['Chicago',45,45,65,85,95,90,95,125,80,80,80,55],\n['Boston',85,85,110,95,90,95,85,85,85,100,100,95],\n['Atlanta',105,120,120,85,95,100,135,100,115,85,105,100],\n['Philadelphia',75,65,95,90,95,85,110,90,95,80,75,90]]\n\ncolumns = ['City'] + np.linspace(1,12,12,dtype=int).tolist()\n\ndf_temp = pd.DataFrame(temp, columns = columns).set_index('City').unstack().reset_index()\ndf_temp.columns = ['Month','City','Temperature']\n\ndf_rain = pd.DataFrame(rain, columns = columns).set_index('City').unstack().reset_index()\ndf_rain.columns = ['Month','City','Rainfall']\n\ndf_train = df_train.merge(df_temp, on=['Month','City'])\ndf_train = df_train.merge(df_rain, on=['Month','City'])\n\ndf_test = df_test.merge(df_temp, on=['Month','City'])\ndf_test = df_test.merge(df_rain, on=['Month','City'])\n\ndf_train.head()","f6b684e8":"df_train.columns","57e4b656":"# features to be used in modeling \nfeatures = ['Hour', 'Weekend','EntryStreetType', 'ExitStreetType', 'EntryCount', \n            'ExitCount','TurnType', 'CCDist','Temperature','Rainfall','City']\n\n_df_train = df_train[features]\n_df_test = df_test[features]","b750f956":"# encoding city name\ncity_encoder = LabelEncoder().fit(cities)\n_df_train.loc[:,'City'] = city_encoder.transform(_df_train['City'])\n_df_test.loc[:,'City'] = city_encoder.transform(_df_test['City'])\n\n# encoding street type\nStreetType = np.unique(_df_train['EntryStreetType'].unique().tolist() + _df_test['ExitStreetType'].unique().tolist()) \nstreet_encoder = LabelEncoder().fit(StreetType)\n_df_train.loc[:,'EntryStreetType'] = street_encoder.transform(_df_train['EntryStreetType'])\n_df_train.loc[:,'ExitStreetType'] = street_encoder.transform(_df_train['ExitStreetType'])\n_df_test.loc[:,'EntryStreetType'] = street_encoder.transform(_df_test['EntryStreetType'])\n_df_test.loc[:,'ExitStreetType'] = street_encoder.transform(_df_test['ExitStreetType'])\n\n_df_train.head()","13b567d2":"scaler = StandardScaler().fit(_df_train)\ndf_train_scaled = scaler.transform(_df_train)\ndf_test_scaled = scaler.transform(_df_test)","c64f7cd1":"# We create PCA and plot variance explained\npca = PCA()\npca.fit_transform(df_train_scaled)\nfeatures = range(pca.n_components_)\nplt.bar(features, pca.explained_variance_)\nplt.ylabel('variance explained')\nplt.xlabel('PCA feature')","97cba50c":"# We also plot the explained variance ratio.\nplt.plot(pca.explained_variance_ratio_)\nplt.xlabel('number of components')\nplt.ylabel('cumulative variance ratio')","f2d701b5":"# We also plot the cumultive explained variance ratio.\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative variance')\nplt.ylim([0,1.1])","acf06517":"print (\"Number of components with explained variance ratio >= 0.01 :\", (pca.explained_variance_ratio_>=0.05).sum())\nprint (f\"Total explained variance retained : {pca.explained_variance_ratio_[:np.sum(pca.explained_variance_ratio_>=.01)].sum():2.4f}\")","0ca81eaf":"# create a pca dataframe based on 90% explained variance retained \npca = PCA(n_components=.90).fit(df_train_scaled)\npca_train = pca.transform(df_train_scaled)\npca_test = pca.transform(df_test_scaled)\ncol_lst = []\nfor i in range(0,pca_train.shape[1]):\n    col_lst.append(f'PC{i}')\n    \ndf_pca_train = pd.DataFrame(pca_train,columns=col_lst)\ndf_pca_test = pd.DataFrame(pca_test,columns=col_lst)\ndf_pca_train.head()","d19a3e9f":"# Trying both the scaled & PCA data to see if we can maintain good accuracy level with less features\nX = df_train_scaled \nX_pca = df_pca_train\ny = df_train[['TotalTimeStopped_p20','TotalTimeStopped_p50','TotalTimeStopped_p80',\n     'DistanceToFirstStop_p20','DistanceToFirstStop_p50','DistanceToFirstStop_p80']]\n\n# split the data into train and test \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.3, random_state=42)","c6669c5e":"# Create function to run different models and return rmse\ndef modeling(X_train, X_test, y_train, y_test):\n    models = []\n    models.append(('LR', LinearRegression()))\n    models.append(('KNN', KNeighborsRegressor()))\n    models.append(('DT', DecisionTreeRegressor(random_state = 1)))\n    models.append(('RF', RandomForestRegressor(random_state = 1)))\n    models.append(('GB', MultiOutputRegressor(GradientBoostingRegressor(random_state = 1))))\n#    models.append(('RG-SVR', RegressorChain(SVR())))\n\n    #Evaluate each model in turn\n    names = []\n    rmses = []\n    \n    for name, model in models:\n#    cv = RepeatedKFold(n_splits=3, n_repeats=3, random_state=1)\n#    cv_scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n        model.fit (X_train, y_train)\n        y_pred = model.predict(X_test)\n        mse = mean_squared_error (y_test, y_pred)\n        rmse = np.sqrt(mse)\n        print (f'{name} : mse {mse} - rmse {rmse}')\n        names.append(name)\n        rmses.append(rmse)\n    return names, rmses","c309fceb":"# run modeling function on scaled data\nprint (\"Scaled Data Modeling :\")\nnames, rmses = modeling (X_train, X_test, y_train, y_test)","7f135fa5":"# run modeling function on pca data\nprint (\"PCA Data Modeling :\")\npca_names, pca_rmses = modeling (X_train_pca, X_test_pca, y_train_pca, y_test_pca)","652a1fec":"# plot and compare the results of scaled data and pca data\nresults = pd.DataFrame([range(1,6),names,rmses, pca_rmses]).T\nresults.columns = ['idx','names', 'scaled data', 'pca data']\nprint (results.melt(id_vars=['names','idx']))\nax = sns.barplot(x='idx',y='value',hue='variable', data=results.melt(id_vars=['names','idx']))\nax.set_xticklabels(names)\nax.set_xlabel(\"model\")\nax.set_ylabel(\"rmse\")","5a2e1c5d":"# i tried different parameters but best results were obtained when I kept the default parameters\n# that's why I commented the paramaters to help the application run faster \nrf_params = {}#\"n_estimators\" :[50, 100], \n             #\"min_samples_split\": [10,20],\n             #\"max_depth\": [10,20]}","7f5488be":"rf_model = RandomForestRegressor(random_state = 1)","48440a7b":"rf_cv = GridSearchCV(rf_model, \n                    rf_params,\n                    cv = 3).fit(X_train, y_train)","09d63177":"rf_cv.best_params_","9fda4108":"rf_tuned = rf_cv.best_estimator_\ny_pred = rf_tuned.predict(X_test)\nrf_mse = mean_squared_error(y_test, y_pred)\nrf_rmse = np.sqrt(rf_mse)\nprint ('mse :', rf_mse)\nprint ('rmse :', rf_rmse) ","158b2a04":"feature_imp = pd.Series(rf_tuned.feature_importances_,\n                        index=_df_train.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Significance Score Of Variables')\nplt.ylabel('Variables')\nplt.title(\"Variable Severity Levels\")\nplt.show()","965b0fe7":"gb_params = {}#\"n_estimators\" :[50, 100, 200], \n             #\"min_samples_split\": [5,10,15]}#,\n            #\"max_depth\": [5,10,20]}","4d8d02f2":"gb_model = MultiOutputRegressor(GradientBoostingRegressor(random_state = 1))","a9241d3a":"gb_cv = GridSearchCV(gb_model, \n                    gb_params,\n                    cv = 3).fit(X_train, y_train)","ccb4fae0":"gb_tuned = gb_cv.best_estimator_\ny_pred = gb_tuned.predict(X_test)\ngb_mse = mean_squared_error(y_test, y_pred)\ngb_rmse = np.sqrt(gb_mse)\nprint ('mse :', gb_mse)\nprint ('rmse :', gb_rmse)","3c83a613":"feature_imp = pd.Series(gb_tuned.estimators_[0].feature_importances_,\n                        index=_df_train.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Significance Score Of Variables')\nplt.ylabel('Variables')\nplt.title(\"Variable Severity Levels\")\nplt.show()","55528b24":"print ('RF model : mse :', rf_mse, '- rmse :', rf_rmse) \nprint ('GB model : mse :', gb_mse, '- rmse :', gb_rmse) ","3cec5e21":"# predict test dataset\nrf_pred = rf_tuned.predict(df_test_scaled)","d937d73f":"# create TargetId & Target as required by the competition \nrf_submission = pd.DataFrame (rf_pred, columns=range(0,6)).reset_index()\nrf_submission = rf_submission.melt(id_vars='index', value_vars=range(0,6), value_name='Target')\nrf_submission['TargetId'] = rf_submission['index'].astype(str) + '_' + rf_submission['variable'].astype(str)\nrf_submission.sort_values(['index','variable'], inplace=True)","ebff1b2c":"rf_submission.shape","2e1e0dd3":"sample = pd.read_csv(\"..\/input\/bigquery-geotab-intersection-congestion\/sample_submission.csv\")\nsample.shape","868bc521":"# The number of rows required in the submission file is slightly less - re-adjust the size of the submission file\nrf_results = rf_submission.merge(sample[['TargetId']], on='TargetId', how='inner')\nrf_results.shape","25815f6e":"rf_results[['TargetId','Target']].to_csv('rf_submission.csv', index=False)","6ad89c28":"# predict test dataset\ngb_pred = gb_tuned.predict(df_test_scaled)","9d8a2d37":"gb_submission = pd.DataFrame (gb_pred, columns=range(0,6)).reset_index()\ngb_submission = gb_submission.melt(id_vars='index', value_vars=range(0,6), value_name='Target')\ngb_submission['TargetId'] = gb_submission['index'].astype(str) + '_' + gb_submission['variable'].astype(str)\ngb_submission.sort_values(['index','variable'], inplace=True)","cea778c5":"gb_submission.shape","ec2d805e":"# The number of rows required in the submission file is slightly less - re-adjust the size of the submission file\ngb_results = gb_submission.merge(sample[['TargetId']], on='TargetId', how='inner')\ngb_results.shape","e7cb6270":"gb_results[['TargetId','Target']].to_csv('gb_submission.csv', index=False)","e2e4c560":"Next we will tune the GB paramters","db1c6554":"Atlanta is more congested than the other cities with average waiting time of 9.7 seconds and distance to intersecion of 30 'meters' followed by Boston and Chicago","3d318417":"Number of Entries and Exits of the intersection can affect the traffic and waiting time. This data will be extracted from \"EntryHeading\" & \"ExitHeading\"","0ac04670":"### 3.3 Turn Type","cf79c70c":"Both datasets have similar distrubtion of data for the 4 cities and for the same months\n\nNow we will focus our analysis on the Train dataset","1b40361a":"### 7.1 Random Forests Tuning","83745549":"Here we notice couple of things : \n- there are different type of Streets (Avenues \/ Street \/ Highways ..etc)\n- each street have different number of Entries and Exits - this is identified by direction ","7604726a":"## 1. Data Analysis","6a290267":"### 3.2 Number of Entries & Exits","fc28a14e":"Driving is a means of travelling that is preferred by many people. We all want to drive efficiently and reach our destination with as little time stuck in a traffic jam as possible. The ability to predict traffic at intersections enables us to plan our course ahead of time and avoid busy streets and intersections. In this project, we attempt to train machine learning models to predict the time it takes to cross an intersection and how congested it is at an intersection.\n\n\n### Objective:\n\nThe objective is to predict congestion, based on an aggregate measure of stopping distance and waiting times, at intersections in 4 major US cities: Atlanta, Boston, Chicago & Philadelphia.\n\n### Data columns:\n\n#### 1. Independent Variables (Features)\n- IntersectionId: Represents a unique intersectionID for some intersection of roads within a city.\n- Latitude: The latitude of the intersection.\n- Longitude: The longitude of the intersection.\n- EntryStreetName: The street name from which the vehicle entered towards the intersection.\n- ExitStreetName: The street name to which the vehicle goes from the intersection.\n- EntryHeading: Direction to which the car was heading while entering the intersection.\n- ExitHeading: Direction to which the car went after it went through the intersection.\n- Hour: The hour of the day.\n- Weekend: It's weekend or not.\n- Month: Which Month it is.\n- Path: It is a concatination in the format: EntryStreetName_EntryHeading ExitStreetName_ExitHeading.\n- City: Name of the city\n\n#### 2. Dependent Variables (Targets)\n- TotalTimeStopped_p20: Total time for which 20% of the vehicles had to stop at an intersection.\n- TotalTimeStopped_p40: Total time for which 40% of the vehicles had to stop at an intersection.\n- TotalTimeStopped_p50: Total time for which 50% of the vehicles had to stop at an intersection.\n- TotalTimeStopped_p60: Total time for which 60% of the vehicles had to stop at an intersection.\n- TotalTimeStopped_p80: Total time for which 80% of the vehicles had to stop at an intersection.\n- TimeFromFirstStop_p20: Time taken for 20% of the vehicles to stop again after crossing a intersection.\n- TimeFromFirstStop_p40: Time taken for 40% of the vehicles to stop again after crossing a intersection.\n- TimeFromFirstStop_p50: Time taken for 50% of the vehicles to stop again after crossing a intersection.\n- TimeFromFirstStop_p60: Time taken for 60% of the vehicles to stop again after crossing a intersection.\n- TimeFromFirstStop_p80: Time taken for 80% of the vehicles to stop again after crossing a intersection.\n- DistanceToFirstStop_p20: How far before the intersection the 20% of the vehicles stopped for the first time.\n- DistanceToFirstStop_p40: How far before the intersection the 40% of the vehicles stopped for the first time.\n- DistanceToFirstStop_p50: How far before the intersection the 50% of the vehicles stopped for the first time.\n- DistanceToFirstStop_p60: How far before the intersection the 60% of the vehicles stopped for the first time.\n- DistanceToFirstStop_p80: How far before the intersection the 80% of the vehicles stopped for the first time.\n\n#### 3. Target Output (based on Competition's Rules)\n\nTotal time stopped at an intersection, 20th, 50th, 80th percentiles and Distance between the intersection and the first place the vehicle stopped and started waiting, 20th, 50th, 80th percentiles\n\n- TotalTimeStopped_p20\n- TotalTimeStopped_p50\n- TotalTimeStopped_p80\n- DistanceToFirstStop_p20\n- DistanceToFirstStop_p50\n- DistanceToFirstStop_p80","bd5d4a74":"## 3. Feature Engineering","3625a65a":"### 3.3 Distance from City Centre","f6b239ce":"### 7.2 Gradient Boosting Tuning","fdb0a070":"On average, the distance from the first stop until the vehicle passes through the intersection for 80the perctile of the cars is 60 'meters'","73f3a8c9":"### 2.1 Missing Observation Analysis","4de3bd0a":"## 10. Reporting","99ce3d66":"### 1.4 Data Correlation","5c73f0f3":"## 9. Final Model Installation","afce73cb":"On average, time from the first stop until the vehicle passes through the intersection for 80the perctile of the cars is 27 seconds","5d8bd674":"The data is presented in pecetiles up to 80% - Therefore, outliers for extreme traffic congestions beyond 80% already been removed from data - therefore, we will not drop the outliers but we will normalize the data before modeling","523ff481":"### 1.1 Descriptive Statistics","67c511fc":"We can improve the performance of the models by standardization. These are methods such as\" Normalize\",\" MinMax\",\" Robust\" and \"Scale\" that can be used for standardization","b15a2421":"Monthly weather including temperature & rain can affect congestion. This can also help generalize the model for the missing months ","d27e3d2e":"### 1.3 Monthly Traffic Analysis","2613691c":"Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.","5c153337":"## 11. Final Remarks","b0dec2d2":"Both RF & GB results were submitted in Kaggle competition. \n- RF submission got a score of 97.790061\n- GB submission got a score of 82.170315","72754a4d":"Since RF performed the best in our initial testing, we will start by trying to tune the RF paramters","f1b10961":"### 9.2 Gradient Boosting ","bb4be08a":"City Centers are busier than country sides. Distance to City Center is culculated using Latitude and Longitude","3316ed60":"RF shows better rmse score than GB","594dbac4":"Categorical variables in the data set should be converted into numerical values. For this reason, these transformation processes are performed with Label Encoding","955996f1":"## 8 Comparison of Final Models","2320a586":"## 6. Base Models","108ff106":"## 2. Data Preprocessing","c00e0316":"### 1.2 Traffic Congestion by City","d4280835":"### 9.1 Random Forest","98baaa1b":"## 4. Data Scaling","a893d459":"**Train dataset includes :**\n- Total time stopped = the amount of time spent at 0 speed\n- Time from first stop = time from the first stop until the vehicle passes through the intersection\n- Distance to first stop = the distance from the center of the intersection to the first stop, to give an idea of queue length\n\nThe data is presented in percentile ","bbd924d2":"The aim of this study was to create regression models to predict traffic congestion in 4 major cities in th US. \nThe work done is as follows:\n\n1) Train & Test Data Set read.\n\n2) With Exploratory Data Analysis; The data set's structural data were checked. The types of variables in the dataset were examined. Size information of the dataset was accessed. There are missing values in the data set  but that doesn't affect the modeling. Descriptive statistics of the data set were examined.\n\n3) Data Preprocessing section; The outliers were determined  and X variables were standardized with the rubost method..\n\n4) During Model Building; Linear Regression, KNN, Decision Tree, Random Forest & Grediant Boosting machine learning models were calculated. Later Random Forest  and Grediant Boosting hyperparameter optimizations optimized to increase accuracy level.\n\n5) Result; The model created as a result of Random Forest hyperparameter optimization became the model with the lowest RMSE value. (63.8)","67ce15f9":"# BigQuery-Geotab Intersection Congestion\n## Can you predict wait times at major city intersections?","4c075658":"### 2.2 Outlier Observation Analysis","9f52852b":"### 3.1 Street Type","7531937e":"\n\n## 7. Model Tuning","3d0d7e98":"Type of turn (left turns \/ right_turn \/ same entry-same exit directions ..etc) can affect congestion and waiting time. This extracted from EntryHeading & ExitHeading\n \nBut to implement that, first we need to map directions while keeping the right sequence: N = 1, NE = 2, E = 3 ...etc ","b22f8578":"## 5. PCA","2abb44cf":"On average, the first 60% of cars didn't stop at the intersecton while the cars at the 80th percentile stopped for 16 seconds","1d41015b":"Street Type can affect traffic as smaller roads tend to be busier while wider roads tend to be faster. Street Type can be extracted from \"Street Name\". ","f438f6b0":"Although we didn't fill or drop the missing data for StreetName, we identified 2 ascpects of the data that can help in our modeling. This will further explored in the Feature Engineering Section. \n\nFurthermore, StreetName will not be used in our model so having the missing data will not affect our analysis","87bb66bc":"Best rmse result was obtained when we ran RF on scaled data. \n\nIn general, modeling scaled data performed better that PCA data since we use didn't try to reduce dimesionality of the data. Yet the gap in the different models vary as we see in the LR the gap in rmse is very minimum while it's quiet high in the DT model","391aba3f":"- Minimum data provided for Jan & May and no data provided for Feb to April\n- For Jun to Dec, similar trend with minimum traffic from midnight to early morning and then it peaks up from 7am to 9am in the morning and again from 3pm to 5pm which is typically the time to and from work","45b4c9a3":"### 3.4 Weather Information","948c8a02":"Since this is a supervised regression problem, we will use regression models (LR\/KNN\/DT\/RF) and comapare the results.\n\nWe will use Root Mean Squared Error (rmse) as a criteria to evaluate the performance of each model."}}