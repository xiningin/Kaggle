{"cell_type":{"65f98c6c":"code","66711dca":"code","06338749":"code","f7942ae2":"code","e90e8d38":"code","f1cfea2e":"code","f165d754":"code","ac552f50":"code","3613728f":"code","3d577de6":"code","9f351fe0":"code","384c90e4":"code","a0205fe1":"code","6507b894":"code","b7067960":"code","d57582a4":"code","8bd097bf":"code","74c9352d":"code","8cc6f2de":"code","530c7610":"code","1526da11":"code","3485fc76":"code","dcc42df2":"code","e9abfa14":"code","9c0b9463":"code","d8382453":"code","98b74917":"code","0e3f2021":"code","f4527dd8":"code","f7d0a6c2":"code","284660a4":"code","fe408bc1":"code","39e45112":"code","3abfbbd0":"code","54a3636f":"code","268c9564":"code","d775e4ef":"code","52a30470":"code","c8328169":"code","f6d1b0dc":"markdown","d0c9dea2":"markdown","2d495176":"markdown","c143e096":"markdown","79ec47e7":"markdown","37d54057":"markdown","9cdb459a":"markdown","93f0775e":"markdown","6b48154f":"markdown","bc72ddfb":"markdown","18e68e25":"markdown","7eacd61e":"markdown","77186cf7":"markdown","36e65a95":"markdown","e1b04357":"markdown","9f090cd9":"markdown","98bfda99":"markdown","1a758b9c":"markdown","093340d8":"markdown","6267640d":"markdown","9231fbcf":"markdown","968f9362":"markdown","a0d682ed":"markdown","6f42d47b":"markdown","8c5db225":"markdown","c796d07a":"markdown","f9e89fae":"markdown","55733562":"markdown","d7139a04":"markdown","8914d462":"markdown"},"source":{"65f98c6c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization purposes\nimport seaborn as sns # for statistical data visualization\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nimport warnings\n\nwarnings.filterwarnings('ignore')","66711dca":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","06338749":"# let's chekc the general info about the dataset.\ndf.info()","f7942ae2":"df.isnull().sum()","e90e8d38":"df.describe()","f1cfea2e":"plt.figure(figsize=(14,10))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","f165d754":"sns.pairplot(df,hue='Outcome')","ac552f50":"columns = df.columns\nfor col in columns:\n     print(col, 'contains', len(df[col].unique()),'lables.')","3613728f":"x = df.drop(['Outcome'],axis=1)\ny = df['Outcome']","3d577de6":"x.head()","9f351fe0":"y.head()","384c90e4":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=0)\nx_train.shape,x_test.shape","a0205fe1":"col = x_train.columns","6507b894":"col","b7067960":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)","d57582a4":"x_train = pd.DataFrame(x_train, columns=[col])\nx_test = pd.DataFrame(x_test,columns=[col])","8bd097bf":"x_train.head()","74c9352d":"x_test.head()","8cc6f2de":"len(x_train)","530c7610":"len(x_test)","1526da11":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\n\ngnb.fit(x_train,y_train)","3485fc76":"y_pred = gnb.predict(x_test)","dcc42df2":"y_pred","e9abfa14":"from sklearn.metrics import accuracy_score","9c0b9463":"print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","d8382453":"y_pred_train = gnb.predict(x_train)\ny_pred_train","98b74917":"print(\"Training-set accuracy score: {0:0.4f}\".format(accuracy_score(y_train,y_pred_train)))","0e3f2021":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(gnb.score(x_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(gnb.score(x_test, y_test)))","f4527dd8":"y_train.value_counts()","f7d0a6c2":"y_test.value_counts()","284660a4":"# check null accuracy score\n\nnull_accuracy = (157\/(157+74))\n\nprint('Null accuracy score: {0:0.4f}'. format(null_accuracy))","fe408bc1":"from sklearn.metrics import confusion_matrix","39e45112":"cm = confusion_matrix(y_test,y_pred)","3abfbbd0":"print('Confusion matrix\\n\\n', cm)","54a3636f":"print('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])","268c9564":"plt.figure(figsize=(10,8))\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()","d775e4ef":"test_pred = gnb.predict(x_test)","52a30470":"from sklearn import metrics","c8328169":"print(metrics.classification_report(y_test,test_pred))","f6d1b0dc":"# Confusion matrix \n\nA confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n\n\nFour types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n\n- `True Positives (TP)` \u2013 True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n\n- `True Negatives (TN)` \u2013 True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n\n- `False Positives (FP)` \u2013 False Positives occur when we predict an observation belongs to a certain class but the observation actually does not belong to that class. This type of error is called `Type I error.`\n\n- `False Negatives (FN)` \u2013 False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called `Type II error`.\n\nThese four outcomes are summarized in a confusion matrix given below.\n","d0c9dea2":"we will be using `231` record to test our model.","2d495176":"# Evaluating Model","c143e096":"## Data\nWe will use the Pima indian diabetes dataset. The data is available at Kaggle and can be downloaded from <a href='https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database'>here<\/a>. The datasets nine columns: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age and Outcome. The first eight are features and the last one ( Outcome) is the label. Outcome has two types of labels 0 (Non-Diabetic) and 1 (Diabetic).","79ec47e7":"Let's check the corelation between the attributes.","37d54057":"## Check accuracy score\nAccuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right. Formally, accuracy has the following definition: Accuracy = Number of correct predictions Total number of predictions.","9cdb459a":"`Note` : here we have all the numerica data it won't make sense, but if it was nominal data then we have to check for cardinality.","93f0775e":"## Predict the results","6b48154f":"# Naive Bayes classification algorithm \n\n## What is a classifier?\nA classifier is a machine learning model that is used to `discriminate different objects based on certain features.`\n\n\nNaive Bayes methods are a set of supervised learning algorithms based on applying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable. \n\n## What is Naive Bayes algorithm?\n\nOne of the supervised machine learning algorithm \u201cNaive Bayes\u201d mainly used for classification. It is a classification technique based on Bayes\u2019 Theorem with an assumption of independence among predictors. `In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.`  For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as \u2018Naive\u2019.  \n`Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.`\n\n### Principle of Naive Bayes Classifier:\n\nA Naive Bayes classifier is a probabilistic machine learning model that\u2019s used for classification task. It works on Bayes theorem of probability to predict the class of unknown data sets. The crux of the classifier is based on the Bayes theorem.\n\n### Bayes Theorem:\n\n<img src=\"https:\/\/miro.medium.com\/max\/638\/1*tjcmj9cDQ-rHXAtxCu5bRQ.png\">\n\n\nUsing Bayes theorem, we can find the probability of A happening, given that B has occurred. Here, B is the evidence and A is the hypothesis. The assumption made here is that the predictors\/features are independent. That is presence of one particular feature does not affect the other. Hence it is called naive.\n\n\nTo understand this we need to know first Conditional probability. please refer to ther <a href=\"http:\/\/www.cs.uni.edu\/~campbell\/stat\/prob4.html\">Conditional probability and the product rule<\/a>\n\n","bc72ddfb":"these dataset we will be using to rain our module. `537`","18e68e25":"## Declare feature vector and target variable","7eacd61e":"Model accuracy is `76%`.\n\nHere, y_test are the true class labels and y_pred are the predicted class labels in the test-set.","77186cf7":"The training-set accuracy score is `0.7672` while the test-set accuracy to be `0.7619`. These two values are quite comparable. So, there is no sign of overfitting.","36e65a95":"## Results and conclusion\n\n- I applied Naive Bayes classification algorithm to predict whether or not the patients in the dataset have diabetes or not. To evaluate the model we used accuracy and classification report generated using sklearn.\n\n1. The model yields a very good performance as indicated by the model accuracy which was found to be `0.7672`.\n\n\n2. The training-set accuracy score is `0.7672` while the test-set accuracy to be `0.7619`. These two values are quite comparable. So, there is no sign of overfitting.\n\n3. I have compared the model accuracy score which is `Model accuracy score: 0.7229` with null `accuracy score which is 0.6797`. So, we can conclude that our Gaussian Na\u00efve Bayes classifier model is doing a very good job in predicting the class labels.","e1b04357":"## Compare the train-set and test-set accuracy\n\nNow, I will compare the train-set and test-set accuracy to check for `overfitting` and `underfitting`.","9f090cd9":"### Compare model accuracy with null accuracy\nSo, the model accuracy is `0.7229`. But, we cannot say that our model is very good based on the above accuracy. We must compare it with the null accuracy. Null accuracy is the accuracy that could be achieved by always predicting the most frequent class.\n\nSo, we should first check the class distribution in the test set.","98bfda99":"We can see that the occurences of most frequent class is 157. So, we can calculate null accuracy by dividing 157 by total number of occurences.","1a758b9c":"## Let's generate a classification report to measure the quality if prediction from Naive Bayes model.","093340d8":"## Exploratory data analysis","6267640d":"### visualize confusion matrix with seaborn heatmap","9231fbcf":"The confusion matrix shows `124 + 43 = 167 correct predictions` and `33 + 31 = 64 incorrect predictions`.\n\nIn this case, we have\n\n- True Positives (Actual Positive:1 and Predict Positive:1) - 124\n- True Negatives (Actual Negative:0 and Predict Negative:0) - 43\n- False Positives (Actual Negative:0 but Predict Positive:1) - 33 (Type I error)\n- False Negatives (Actual Positive:1 but Predict Negative:0) - 31 (Type II error)","968f9362":"## Split data into separate training and test set\n\n<img src='https:\/\/github.com\/nijatullahmansoor\/IBM-Data-Science-Professional-Certificate\/raw\/master\/Machine%20learning%20With%20python\/MSC\/kaggle.PNG'>\n\nWe will be using the second Test\/Split method here.","a0d682ed":"we can see that we have all numeric data. and we don't have any null values.","6f42d47b":"Let's import the required library.","8c5db225":"#### We now have X_train dataset ready to be fed into the Gaussian Naive Bayes classifier.","c796d07a":"These metrics are calculated using True Positive\/TP ( person has diabetes and predicted diabetes) , True Negative\/TN ( person did not have diabetes and predicted non- diabetic), False Positive\/FP ( person did not have diabetes but predicted diabetes) and False Negative\/FN ( person had diabetes but predicted non-diabetic).","f9e89fae":"`Precision` - What percent of the prediction were correct ? Accuracy of positive prediction.\n\n`Precision = TP\/(TP+FP)`\n\n`Recall `\u2014 What percent of the positive cases did we catch? Fraction of positives that were correctly identified.\n\n`Recall = TP\/ (TP+FN)` \n\n`F1 score` \u2014 What percent of positive prediction were correct?\n\n`F1 score` = 2*(Recall*Precision)\/(Recall+Precision)\n\n","55733562":"## Number of labels: cardinality\nThe number of labels within each attribute is known as `cardinality`. A high number of labels within a variable is known as `high cardinality.` High cardinality may pose some serious problems in the machine learning model.","d7139a04":"## Feature Scaling \n\n`Feature scaling is a method used to normalize the range of independent variables or features of data.` In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.","8914d462":"We can see that our model accuracy score is 0.7229 but null accuracy score is 0.6797. So, we can conclude that our Gaussian Naive Bayes Classification model is doing good job in predicting the class labels. \nBut, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making.\n\nWe have another tool called `Confusion matrix` that comes to our rescue."}}