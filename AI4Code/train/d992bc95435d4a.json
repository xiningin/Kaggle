{"cell_type":{"59be4d7d":"code","ecc886f1":"code","d91728ad":"code","b201c8fb":"code","9b6414af":"code","fba8ffc0":"code","99f42ff3":"code","17e77672":"code","797c7d9c":"code","ff0ffae0":"code","30ea03d3":"code","d1a746d1":"code","9323cc2e":"code","6bb1e4b9":"code","cb8483db":"code","2a47e6be":"code","90be015d":"code","a438ef86":"code","815caa3a":"code","621068df":"markdown","668bebd8":"markdown"},"source":{"59be4d7d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ecc886f1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","d91728ad":"train_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')","b201c8fb":"sub0 = pd.read_csv('..\/input\/tpsjan22-eda-baseline-train-submission\/submission.csv')\nsub1 = pd.read_csv('..\/input\/tpsjan22-06-lightgbm-quickstart\/submission_lightgbm_quickstart.csv')\nsub2 = pd.read_csv('..\/input\/tpsjan22-03-linear-model\/submission_linear_model_rounded.csv')","9b6414af":"pred = np.array([np.array(sub0['num_sold'].values), np.array(sub1['num_sold'].values), np.array(sub2['num_sold'].values)])\npred.T","fba8ffc0":"np.log(pred.T)","99f42ff3":"plt.figure(figsize=(16,3))\nplt.hist(train_df['num_sold'], bins=np.linspace(0, 3000, 201),\n         density=True, label='Training')\nplt.hist(sub0['num_sold'], bins=np.linspace(0, 3000, 201),\n         density=True, rwidth=0.5, label='sub0 predictions')\nplt.hist(sub1['num_sold'], bins=np.linspace(0, 3000, 201),\n         density=True, rwidth=0.5, label='sub1 predictions')\nplt.hist(sub2['num_sold'], bins=np.linspace(0, 3000, 201),\n         density=True, rwidth=0.5, label='sub2 predictions')\nplt.xlabel('num_sold')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()","17e77672":"test_df['num_sold0'] = sub0.num_sold\ntest_df.head()","797c7d9c":"country='Norway'\nstore='KaggleMart'\nproduct='Kaggle Hat'","ff0ffae0":"plt.figure(figsize=(20, 6))\ntrain_subset = train_df[(train_df.country == country) & (train_df.store == store) & (train_df['product'] == product)]\nplt.scatter(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5, color='red', s=3)\nplt.legend()\nplt.title('Predictions and true num_sold for five years')\nplt.show()","30ea03d3":"plt.figure(figsize=(20, 6))\ntrain_subset = train_df[(train_df.country == country) & (train_df.store == store) & (train_df['product'] == product)]\nplt.scatter(np.arange(len(train_subset)),np.log(train_subset.num_sold), label='true', alpha=0.5, color='red', s=3)\nplt.legend()\nplt.title('Log of True num_sold for five years')\nplt.show()","d1a746d1":"test_df['num_sold0'] = sub0.num_sold\ntest_df['num_sold1'] = sub1.num_sold\ntest_df['num_sold2'] = sub2.num_sold","9323cc2e":"plt.figure(figsize=(20, 6))\ntrain_subset = train_df[(train_df.country == country) & (train_df.store == store) & (train_df['product'] == product)]\nsub_subset = test_df[(test_df.country == country) & (test_df.store == store) & (test_df['product'] == product)]\n#plt.scatter(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5, color='red', s=3)\nplt.scatter(np.arange(len(sub_subset)), sub_subset.num_sold0, label='sub0', alpha=0.5, color='orange', s=3)\nplt.scatter(np.arange(len(sub_subset)), sub_subset.num_sold1, label='sub1', alpha=0.5, color='green', s=3)\nplt.scatter(np.arange(len(sub_subset)), sub_subset.num_sold2, label='sub2', alpha=0.5, color='blue', s=3)\nplt.legend()\nplt.title('Predicted num_sold for five years')\nplt.show()","6bb1e4b9":"plt.figure(figsize=(20, 6))\ntrain_subset = train_df[(train_df.country == country) & (train_df.store == store) & (train_df['product'] == product)]\nsub_subset = test_df[(test_df.country == country) & (test_df.store == store) & (test_df['product'] == product)]\n#plt.scatter(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5, color='red', s=3)\nplt.scatter(np.arange(len(sub_subset)), np.log(sub_subset.num_sold0), label='sub0', alpha=0.5, color='orange', s=3)\nplt.scatter(np.arange(len(sub_subset)), np.log(sub_subset.num_sold1), label='sub1', alpha=0.5, color='green', s=3)\nplt.scatter(np.arange(len(sub_subset)), np.log(sub_subset.num_sold2), label='sub2', alpha=0.5, color='blue', s=3)\nplt.legend()\nplt.title('Log of predicted num_sold for five years')\nplt.show()","cb8483db":"mean = np.mean(pred, axis=0)\nmed = np.median(pred, axis=0)\nmaxi = np.max(pred, axis=0)\n\nlog_mean = np.mean(np.log(pred), axis=0)","2a47e6be":"sub['num_sold'] = np.round(med)\nsub.to_csv('submission_median.csv', index=False)\nsub.head(5)","90be015d":"sub['num_sold'] = np.round(mean)\nsub.to_csv('submission_mean.csv', index=False)\nsub.head(5)","a438ef86":"sub['num_sold'] = np.round(maxi)\nsub.to_csv('submission_maximum.csv', index=False)\nsub.head(5)","815caa3a":"sub['num_sold'] = np.round(log_mean)\nsub.to_csv('submission_log_mean.csv', index=False)\nsub.head(5)","621068df":"#### Did it work?\n- Simply finding the average of all predictions and rounding result.\n- Finding the median and rounding, so the result will theoretically satisfy both requirements of metrics.\n- Finding the maximum and rounding, so SMAPE will be decreased because of denominator.\n- Finding the mean of logarithm, then taking the exponent and rounding.\n\n#### What did you not understand about this process?\nWell, everything provides in the competition data page. I've no problem while working on it. If you guys don't understand the thing that I'll do in this notebook then please comment on this notebook.\n\n#### What else do you think you can try as part of this approach?\nFinding median and max of logarithm unnecessary to check because logarithm is strictly increasing function. ","668bebd8":"#### What are you trying to do in this notebook?\nThe goal is to combine several models' predictions to achieve the highest score with no overfitting. \nFor this hyperparameters should be tuned wihout information.\nIt is better to see what techniques can be applied to achieve maximize the score function.\n#### Why are you trying it?\n- To minimize the difference between the actual and predicted values and maximize their average.\n- The goal is that predicted value should be close to actual and at the same time as large as possible. \n- Ensembling the logarithm of values may be better because the logarithm decreases the range of possible values of prediction and therefore makes values of different models close to each other.\n- Ensembling them and taking the exponent should arrive to better point than blending values at high distance from each other, etc."}}