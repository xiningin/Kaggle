{"cell_type":{"9f0e2a7c":"code","5af5cea0":"code","bfcadac6":"code","b135ba25":"code","d09f2538":"code","570bd110":"code","8c8c28c5":"code","3eb21d50":"code","f973363d":"code","aa712296":"code","853be40e":"code","96951782":"code","9f95ed67":"code","8b7e2c89":"code","3a8d1a54":"code","3228fed8":"code","1a63abaa":"code","dadbc021":"code","186e718d":"code","93a22e07":"code","ce40fbc3":"code","409dc8a8":"code","8a69201d":"code","4a8a57e8":"code","60a1f0b1":"code","c4285a0f":"code","b8341d59":"code","aab61d56":"code","cef71417":"code","122de663":"code","aa6f0ab3":"code","37d67fcf":"code","18ecb37a":"code","da6de578":"code","88d15ff3":"code","239bd97c":"code","9346f666":"code","0ebdda97":"code","b60b384f":"code","2d8dc519":"code","e7d93171":"code","722c76a1":"code","84aead4e":"code","04a30798":"code","9ba0f122":"code","4a1f2d9e":"code","6bee3972":"code","b511183d":"code","39cb792a":"code","4c448b46":"code","3a9f5916":"markdown","5abc7a26":"markdown","e1f135e6":"markdown","99aeb1c5":"markdown","f8351a74":"markdown","cf194ada":"markdown","710b6ec6":"markdown","194995b3":"markdown","03d645da":"markdown","8146ae26":"markdown","1163a0f0":"markdown","fd798e57":"markdown","df044a63":"markdown","9ff82bc4":"markdown","8308db1f":"markdown","edc8b02e":"markdown","35363a97":"markdown","06d6b8f3":"markdown","dae284d8":"markdown","aa69d2ec":"markdown","29e7f74f":"markdown","e07d15af":"markdown","3cb50d80":"markdown","1aeecea7":"markdown","393c8e8f":"markdown","3a66711f":"markdown","653807c5":"markdown","fbd27d31":"markdown","550dc7ca":"markdown","bf08a8a4":"markdown","b311ffee":"markdown","c1c848d0":"markdown","0e1716ae":"markdown","df9de02d":"markdown","d228b8c7":"markdown","fd038a05":"markdown","18b0de7f":"markdown","a8eb3097":"markdown","e4caf301":"markdown","19fd336d":"markdown","e75149f1":"markdown","7660c928":"markdown"},"source":{"9f0e2a7c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")  # plt.style.available => if you write this you can see the other styles\n\n\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5af5cea0":"data = pd.read_csv(\"\/kaggle\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv\")","bfcadac6":"len(data)","b135ba25":"test = data.iloc[140:210]\ntest[\"target\"].value_counts()\ndata.drop(index=range(140,211), axis=0, inplace=True)\nlen_train = len(data)","d09f2538":"len_train","570bd110":"data.columns","8c8c28c5":"data.head()","3eb21d50":"data.describe()","f973363d":"data.info()","aa712296":"def bar_plot(variable):\n    \"\"\"\n    input: variable ex: sex\n    output: barplot & value count\n    \"\"\"\n    # get features\n    var = data[variable]\n    # count number of categorical variable (value\/sample)\n    varValue = var.value_counts()\n    # visualize\n    plt.figure(figsize=(9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequanc\")\n    plt.title(variable)\n    plt.show()\n    \n    print(\"{}\\n{}\".format(variable, varValue))","853be40e":"categorical = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\", \"target\"]\nfor c in categorical:\n    bar_plot(c)","96951782":"def plot_hist(variable):\n    plt.figure(figsize=(9,3))\n    plt.hist(data[variable], bins=40)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequancy\")\n    plt.title(\"{} distribution with histogram\".format(variable))\n    plt.show()","9f95ed67":"numericVar = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\nfor n in numericVar:\n    plot_hist(n)","8b7e2c89":"# sex vs target\ndata[[\"sex\",\"target\"]].groupby([\"sex\"], as_index=False).mean().sort_values(by=\"target\", ascending=False)","3a8d1a54":"# cp vs target\ndata[[\"cp\",\"target\"]].groupby([\"cp\"], as_index=False).mean().sort_values(by=\"target\", ascending=False)","3228fed8":"# fbs vs target\ndata[[\"fbs\",\"target\"]].groupby([\"fbs\"], as_index=False).mean().sort_values(by=\"target\", ascending=False)","1a63abaa":"# restecg vs target\ndata[[\"restecg\",\"target\"]].groupby([\"restecg\"], as_index=False).mean().sort_values(by=\"target\", ascending=False)","dadbc021":"# exang vs target\ndata[[\"exang\",\"target\"]].groupby([\"exang\"], as_index=False).mean().sort_values(by=\"target\", ascending=False)","186e718d":"# slope vs target\ndata[[\"slope\",\"target\"]].groupby([\"slope\"], as_index=False).mean().sort_values(by=\"target\", ascending=False)","93a22e07":"# ca vs target\ndata[[\"ca\",\"target\"]].groupby([\"ca\"], as_index=False).mean().sort_values(by=\"target\", ascending=False)","ce40fbc3":"# thal vs target\ndata[[\"thal\",\"target\"]].groupby([\"thal\"], as_index=False).mean().sort_values(by=\"target\", ascending=False)","409dc8a8":"data.head()","8a69201d":"def outlier_detection(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st Quantile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # Detect outlier and their indices\n        outlier_indices_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        \n        # store indices\n        outlier_indices.extend(outlier_indices_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outlier = list(i for i, v in outlier_indices.items() if v >= 2)\n    \n    return multiple_outlier","4a8a57e8":"data.loc[outlier_detection(data, [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"])]","60a1f0b1":"# drop outliers\ndata = data.drop(outlier_detection(data, [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]), axis=0).reset_index(drop=True)","c4285a0f":"data = pd.concat([data,test], axis=0).reset_index(drop=True)","b8341d59":"data.head()","aab61d56":"data.columns[data.isnull().any()]","cef71417":"data.isnull().sum()","122de663":"# age sex cp trestbps chol fbs restecg  thalach\texang oldpeak slope\tca thal\ttarget\nlist1 = [\"age\",\"sex\",\"trestbps\",\"chol\",\"thalach\",\"oldpeak\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\", \"target\"]\nplt.figure(figsize=(15,10))\nsns.heatmap(data[list1].corr(), annot=True, fmt=\".2f\")\nplt.show()","aa6f0ab3":"g = sns.FacetGrid(data, col=\"target\")\ng.map(sns.distplot, \"thalach\", bins=40)\nplt.show()","37d67fcf":"g = sns.factorplot(x=\"thalach\", y=\"target\", data=data, kind=\"bar\", size=6, orient=\"h\")\ng.set_ylabels(\"Heart Attack Probability\")\nplt.show()","18ecb37a":"g = sns.FacetGrid(data, col=\"target\")\ng.map(sns.distplot, \"oldpeak\", bins=40)\nplt.show()","da6de578":"g = sns.factorplot(x=\"oldpeak\", y=\"target\", data=data, kind=\"bar\", size=6, orient=\"h\")\ng.set_ylabels(\"Heart Attack Probability\")\nplt.show()","88d15ff3":"g = sns.factorplot(x=\"cp\", y=\"target\", data=data, kind=\"bar\", size=6)\ng.set_ylabels(\"Heart Attack Probability\")\nplt.show()","239bd97c":"g = sns.factorplot(x=\"exang\", y=\"target\", data=data, kind=\"bar\", size=6)\ng.set_ylabels(\"Heart Attack Probability\")\nplt.show()","9346f666":"g = sns.factorplot(x=\"ca\", y=\"target\", data=data, kind=\"bar\", size=6)\ng.set_ylabels(\"Heart Attack Probability\")\nplt.show()","0ebdda97":"g = sns.factorplot(x=\"slope\", y=\"target\", data=data, kind=\"bar\", size=6)\ng.set_ylabels(\"Heart Attack Probability\")\nplt.show()","b60b384f":"g = sns.factorplot(x=\"thal\", y=\"target\", data=data, kind=\"bar\", size=6)\ng.set_ylabels(\"Heart Attack Probability\")\nplt.show()","2d8dc519":"#cp -- exang -- target\ng = sns.FacetGrid(data, col=\"target\", row=\"exang\")\ng.map(plt.hist, \"cp\")\nplt.show()","e7d93171":"g = sns.FacetGrid(data, col=\"cp\", size=3)\ng.map(sns.pointplot, \"exang\", \"target\", \"sex\")\ng.add_legend()\nplt.show()","722c76a1":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","84aead4e":"len_train = len_train-1 # I dropped 1 feature that outlier one. This is wht \u0131 do this\ntrain = data[:len_train]\ntest = data[len_train:]\ntest_x = test.drop([\"target\"], axis=1)\ntest_y = test[\"target\"]\nprint(\"train len : \",len(train))\nprint(\"test len : \", len(test))","04a30798":"X_train = train.drop([\"target\"] , axis=1)\nY_train = train[\"target\"]\nx_train, x_test, y_train, y_test = train_test_split(X_train,Y_train, test_size=0.2, random_state=42)\nprint(\"x_train len:\", len(x_train))\nprint(\"x_test len:\", len(x_test))\nprint(\"y_train len:\", len(y_train))\nprint(\"y_test len:\", len(y_test))\nprint(\"test len:\", len(test))","9ba0f122":"lr = LogisticRegression()\nlr.fit(x_train,y_train)\ny_head = lr.predict(x_test)\nacc_log_train = round(lr.score(x_train, y_train)*100,2)\nacc_log_test = round(lr.score(x_test,y_test)*100,2)\nprint(\"Logistic Regression Train Accuracy: %\", acc_log_train)\nprint(\"Logistic Regression Test Accuracy: %\", acc_log_test)","4a1f2d9e":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state=random_state),\n             SVC(random_state=random_state),\n             RandomForestClassifier(random_state=random_state),\n             LogisticRegression(random_state=random_state),\n             KNeighborsClassifier()]\n\ndt_param_grid = {\"min_samples_split\":range(10,500,20),\n                \"max_depth\":range(1,20,2)}\n\nsvc_param_grid = {\"kernel\":[\"rbf\"],\n                 \"gamma\":[0.001,0.01,0.1,1],\n                 \"C\":[1,10,50,100,200,300,1000]}\n\nrf_param_grid = {\"max_features\":[1.3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlr_param_grid = {\"C\":np.logspace(-3,3,7),\n                \"penalty\":[\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\":np.linspace(1,19,10, dtype=int).tolist(),\n                 \"weights\":[\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\n\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   lr_param_grid,\n                   knn_param_grid]","6bee3972":"model_names=[\"DecisionTree :\", \"SVC : \", \"RandomForest : \", \"LogisticRegression : \", \"KNN : \"]\ncv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv=StratifiedKFold(n_splits=10), scoring=\"accuracy\", n_jobs=-1, verbose=1)\n    clf.fit(x_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(model_names[i], cv_result[i])","b511183d":"print(cv_result)\ncv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[\"DecisionTreeClassifier\", \"SVC\", \"RandomForestClassifier\", \"LogisticRegression\", \"KNeigborsClassifier\"]})\n\ng = sns.barplot(x=\"Cross Validation Means\", y = \"ML Models\", data=cv_results)\ng.set_xlabel(\"Means Accuracy\")\ng.set_title(\"Cross Validation Scores\")\nplt.show()","39cb792a":"votingC = VotingClassifier(estimators=[(\"rf\", best_estimators[2]),\n                                      (\"lr\", best_estimators[3])],\n                          voting=\"soft\", n_jobs=-1)\nvotingC = votingC.fit(x_train, y_train)\nprint(\"Accuracy :\", accuracy_score(votingC.predict(x_test), y_test))","4c448b46":"print(\"Accuracy :\",accuracy_score(votingC.predict(test_x), test_y))","3a9f5916":"<a id=\"3\"><\/a><br>\n## Univariate Variable Analysis\n* Catagorical Variable\n* Numerical Variable","5abc7a26":"<a id=\"12\"><\/a><br>\n## thalach -- target","e1f135e6":"<a id=\"13\"><\/a><br>\n## cp -- target","99aeb1c5":"<a id=\"14\"><\/a><br>\n## exang -- target","f8351a74":"If ca value equals 0 or 4, they have more heart attack risk","cf194ada":"<a id=\"22\"><\/a><br>\n## Train - Test Split","710b6ec6":"<a id=\"20\"><\/a><br>\n## Feature Engineering","194995b3":"<a id=\"21\"><\/a><br>\n## Modelling","03d645da":"<a id=\"9\"><\/a><br>\n## Find Missing Value","8146ae26":"* There is no missing vallue so I'm passing this step","1163a0f0":"<a id=\"11\"><\/a><br>\n### Corelation between all features","fd798e57":"If thal value equal 2, thay have more heart attack risk","df044a63":"<a id=\"1\"><\/a><br>\n# Load and Check Data","9ff82bc4":"* black=male\n* blue=female","8308db1f":"<a id=\"18\"><\/a><br>\n## cp -- exang -- target","edc8b02e":"If cp value is equals 1,2,3 , they have more harth attack risk","35363a97":"<a id=\"2\"><\/a><br>\n## Variable Describtion\n\n1. age\n1. sex\n1. cp: chest pain type (4 values)\n1. trestbps : resting blood pressure\n1. chol : serum cholestoral in mg\/dl\n1. fbs : fasting blood sugar > 120 mg\/dl\n1. restecg : resting electrocardiographic results (values 0,1,2)\n1. thalach : maximum heart rate achieved\n1. exang : exercise induced angina\n1. oldpeak : ST depression induced by exercise relative to rest\n1. slope : the slope of the peak exercise ST segment\n1. ca : number of major vessels (0-3) colored by flourosopy\n1. thal :  0 = normal; 1 = fixed defect; 2 = reversable defect\n1. target : 0= less chance of heart attack 1= more chance of heart attack","06d6b8f3":"<a id=\"24\"><\/a><br>\n## Hyper Parameter -- Grid Search -- Cross Validation\nWe will compare 5 ml classifier and evaluate mean accuracy of each of them by stratified cross validation.\n\n* DecisionTree\n* SVM\n* RandomForest\n* KNN\n* LogisticRegression","dae284d8":"# Introduction\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to\nthis date.The \"target\" field refers to the presence of heart disease in the patient. It is integer valued 0 = no\/less chance of heart attack and 1 = more chance of heart attack\n\n<font color = \"lightblue\">\n    Content :\n\n1. [Load and Check Data](#1)\n1. [Variable Describtion](#2)\n      * [Univariate Variable Analysis](#3)\n          * [Chatagorical Variable:](#4)\n          * [Numerical Variable:](#5)\n1. [Basic Data Analysis](#6)\n1. [Outlier Detection](#7)\n1. [Missing Value](#8)\n    * [Find Missing Value](#9)\n1. [Vissualization](#10)\n    * [Corelation between all features](#11)\n    * [thalach -- target](#12)\n    * [cp -- target](#13)\n    * [exang -- target](#14)\n    * [ca -- target](#15)\n    * [slope -- target](#16)\n    * [thal -- target](#17)\n    * [cp -- exang -- target](#18)\n    * [cp -- exang -- target -- sex](#19)\n1. [Feature Engineering](#20)\n1. [Modelling](#21)\n    * [Train - Test Split](#22)\n    * [Simple Logistic Regression](#23)\n    * [Hyper Parameter -- Grid Search -- Cross Validation](#24)\n    * [Ensemble Modelling](#25)","aa69d2ec":"<a id=\"15\"><\/a><br>\n## ca -- target","29e7f74f":"<a id=\"23\"><\/a><br>\n## Simple Logistic Regression","e07d15af":"<a id=\"8\"><\/a><br>\n## Missing Value\n* Find Missing Value\n* Fill Missing Value","3cb50d80":"<a id=\"12\"><\/a><br>\n## oldpeak -- target","1aeecea7":"<a id=\"4\"><\/a><br>\n## Catagorical Variable","393c8e8f":"<a id=\"7\"><\/a><br>\n## Outlier Detection","3a66711f":"If thalach is higher than 140, they have more heart attack risk","653807c5":"And last , I use first test data for prediction","fbd27d31":"If exangina value equal 0, they have more heart attack risk","550dc7ca":"<a id=\"17\"><\/a><br>\n## thal -- target","bf08a8a4":"kategorical feature lar\u0131 ay\u0131r ve 0 ve 1 den olu\u015ftur","b311ffee":"If slope value equal 2, they have more heart attack risk","c1c848d0":"<a id=\"19\"><\/a><br>\n## cp -- exang -- target -- sex","0e1716ae":"<a id=\"16\"><\/a><br>\n## slope -- target","df9de02d":"<a id=\"6\"><\/a><br>\n## Basic Data Analysis\n* sex vs target\n* cp vs target\n* fbs vs target\n* restecg vs target\n* exang vs target\n* slope vs target\n* ca vs target\n* thal vs target","d228b8c7":"If oldpeak is lower than 0.50, they have more heart attack risk","fd038a05":"thalach, oldpeak, cp(chest pain), exang(exercies induced angina), ca, slope and thal are corelated with tharget","18b0de7f":"<a id=\"10\"><\/a><br>\n## Vissualization","a8eb3097":"* Data has 303 feature.\n* I split 70 feature before I analysis the data. I will use this test data.","e4caf301":"* int64(4): age, trestbps, chol, thalach\n* float64(1): oldpeak\n* category(9): sex, cp, fbs, restecg, exang, slope, ca, thal, target","19fd336d":"data = data.astype({\"sex\":\"int64\",\n             \"cp\":\"category\",\n            \"fbs\":\"category\",\n            \"restecg\":\"category\",\n            \"exang\":\"category\",\n            \"slope\":\"category\",\n            \"ca\":\"category\",\n            \"thal\":\"category\",\n            \"target\":\"int64\"})\ndata.info()","e75149f1":"<a id=\"25\"><\/a><br>\n## Ensemble Modelling","7660c928":"<a id=\"5\"><\/a><br>\n## Numerical Variable"}}