{"cell_type":{"e6d82c10":"code","9f4da610":"code","eb93e485":"code","8005ca54":"code","59568352":"code","4513bd41":"code","bf300bb8":"code","07300fac":"code","fc57f64b":"code","59abec62":"code","4ade9249":"code","c1299a23":"code","64c48119":"code","5957e9ba":"code","49b489c4":"code","b6d15f9e":"code","83dc4b67":"code","1e09d2b7":"markdown","d247cc88":"markdown","28189be5":"markdown","1ec05cb9":"markdown","2d1a9c0e":"markdown","4f08cf47":"markdown","bd6c5aaa":"markdown","dde5a6fa":"markdown","a56b462f":"markdown","82a57208":"markdown","bbdc769a":"markdown","9f6b7267":"markdown","accbd888":"markdown","c6200fcc":"markdown","0aaf80f4":"markdown","0a503a30":"markdown"},"source":{"e6d82c10":"import warnings\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.set_option('display.max.columns', None)\npd.set_option('display.max.rows', None)\nplt.rcParams['figure.figsize'] = [20, 6]\nsns.set_theme(style='whitegrid')\nwarnings.simplefilter(action='ignore')\n\ncredit_card = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\ncredit_card.head()\n","9f4da610":"# Dropping the redundant columns\ncredit_card.drop(columns=credit_card.columns[[0, 21, 22]], inplace=True)\n\nprint('Missing values:')\ndisplay(credit_card.isna().sum())\n\nprint('\\nData types of the variables:')\ndisplay(credit_card.dtypes)\n\ncredit_card.head()\n","eb93e485":"print('Overall summary statistics of the numerical features:')\ndisplay(credit_card.describe())\n\nprint('\\nSummary statistics of the numerical features by churning status:')\ncredit_card.groupby('Attrition_Flag').describe()\n","8005ca54":"print('Overall summary statistics of the categorical features:')\ndisplay(credit_card.describe(exclude='number'))\n\nprint('\\nSummary statistics of the categorical features by churning status:')\ndisplay(credit_card.groupby('Attrition_Flag').describe(exclude='number'))\n","59568352":"# Recoding the Card_Category variable\ncredit_card['Card_Category'] = np.where(\n    credit_card.Card_Category == 'Blue', 'Blue', 'Others')\nassert len(credit_card.Card_Category.unique()) == 2\n","4513bd41":"from scipy.stats import chi2_contingency\n# Visualising the categorical variables\nfig, axes = plt.subplots(2, 3)\ncat_columns = [col for col in credit_card.select_dtypes(\n    'object').columns if col != 'Attrition_Flag']\n\nfor col, ax in zip(cat_columns, axes.ravel()):\n    col_chi2_p = chi2_contingency(pd.crosstab(\n        credit_card.Attrition_Flag, credit_card[col]))[1]\n    sns.countplot(y=col, hue='Attrition_Flag', data=credit_card, ax=ax)\n    sns.despine()\n    ax.set(xlabel='', ylabel='')\n    ax.set_title(\n        f\"{col.replace('_', ' ')}, chi-squared test p value: {round(col_chi2_p, 6)}\")\n    handles, labels = ax.get_legend_handles_labels()\n    ax.legend('')\nfig.suptitle(\n    'Bar plots of the frequencies of each category in the categorical features')\nfig.legend(handles, labels)\nplt.delaxes(axes[1, 2])\nplt.tight_layout()\nplt.show()\n","bf300bb8":"from scipy.stats import kruskal\nnum_columns = list(set(credit_card.drop(columns='Attrition_Flag').columns).difference(\n    cat_columns))\n\n# Visualising numerical features\nfig, axes = plt.subplots(3, 5)\nfor column, ax in zip(num_columns, axes.ravel()):\n    kruskal_p = kruskal(credit_card.query(\"`Attrition_Flag` == 'Existing Customer'\")[\n                        column], credit_card.query(\"`Attrition_Flag` == 'Attrited Customer'\")[column],\n                        nan_policy='omit')[1]\n    sns.violinplot(x='Attrition_Flag', y=column,\n                   split=True, data=credit_card, ax=ax)\n    ax.set(xlabel='', ylabel='',\n           title=f\"{column.replace('_', ' ')}, kruskal p value:{round(kruskal_p, 6)}\")\n    sns.despine()\n\nfig.suptitle(\n    'Distribution of numerical features by churning status of customers')\nfig.delaxes(axes[2, 4])\nplt.tight_layout()\nplt.show()\n","07300fac":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Recoding the target variable\ncredit_card['Has_Churned'] = np.where(\n    credit_card.Attrition_Flag == 'Attrited Customer', 1, 0)\nassert credit_card['Has_Churned'].dtype == 'int'\n\n# Splitting the data set into train and test sets\nX = credit_card.drop(columns=['Attrition_Flag', 'Has_Churned'])\ny = credit_card['Has_Churned']\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, test_size=0.33, random_state=1)\n\n# Preprocessing steps of the data\ndf_preprocess = ColumnTransformer([('standardise', StandardScaler(), num_columns),\n                                   ('ohe', OneHotEncoder(), cat_columns)], remainder='passthrough')\n","fc57f64b":"from sklearn.metrics import f1_score, log_loss\nfrom sklearn.model_selection import cross_val_score\nfrom catboost import cv, Pool\n\n# Building model evaluation function\n\n\ndef evaluate_model(model, cat_columns=None, X_test=X_test, y_test=y_test):\n    y_pred = model.predict(X_test)\n    y_pred_prob = model.predict_proba(X_test)\n    if type(model) == type(CatBoostClassifier()):\n        pool = Pool(data=X_train, label=y_train,\n                    cat_features=cat_columns)\n        catboost_cv = cv(pool, params=model.get_params(),\n                         nfold=5, logging_level='Silent', early_stopping_rounds=int(model.tree_count_ * 0.1))\n        cv_f1_score = np.max(catboost_cv['test-F1-mean'])\n        cv_log_loss = np.min(catboost_cv['test-Logloss-mean'])\n    else:\n        cv_f1_score = np.mean(cross_val_score(\n            model, X_train, y_train, cv=5, scoring='f1', verbose=1))\n        cv_log_loss = - np.mean(cross_val_score(\n            model, X_train, y_train, cv=5, scoring='neg_log_loss', verbose=1))\n    test_f1_score = f1_score(y_test, y_pred)\n    test_log_loss = log_loss(y_test, y_pred_prob)\n    return {'5-fold CV F1 score': cv_f1_score,\n            '5-fold CV log loss': cv_log_loss,\n            'Test F1 score': test_f1_score,\n            'Test log loss': test_log_loss}\n","59abec62":"from catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\n\n# Building the logistic regression model\nlogreg = Pipeline([('preprocess', df_preprocess),\n                   ('logreg_clf', LogisticRegressionCV(random_state=1, class_weight='balanced'))])\n\nlogreg.fit(X_train, y_train)\nprint('Performance on metrics for the logistic regression model:')\ndisplay(evaluate_model(logreg))\n\n# Building the random forest model\nprint('\\nPerformance on metrics for the random forest model:')\nrfc = Pipeline([('preprocess', df_preprocess),\n                ('rf_clf', RandomForestClassifier(random_state=1, oob_score=True, class_weight='balanced'))])\nrfc.fit(X_train, y_train)\ndisplay(evaluate_model(rfc))\n\n# Building the catboost model\ncat_column_indices = [idx for idx, col in enumerate(\n    X.columns) if col in cat_columns]\nclass_weight = len(y[y == 0]) \/ len(y[y == 1])\ncatboost = CatBoostClassifier(cat_features=cat_column_indices,\n                              verbose=0,\n                              random_state=1,\n                              eval_metric='F1',\n                              objective='Logloss',\n                              scale_pos_weight=class_weight)\ncatboost.fit(X_train,\n             y_train,\n             eval_set=(X_test, y_test),\n             use_best_model=True,\n             early_stopping_rounds=100)  # which is 10% of the default n_estimators of 1000 in CatBoost\n\nprint('\\nPerformance on metrics for the catboost model:')\nevaluate_model(catboost, cat_column_indices)\n","4ade9249":"catboost_feature_select = catboost.select_features(X_train, y_train, eval_set=(X_test, y_test), features_for_select=list(\n    X_train.columns), num_features_to_select=10, logging_level='Silent', train_final_model=False, plot=True)\n","c1299a23":"# Extracting the five features to be removed\nremove_features = {idx: feature for idx, feature in enumerate(\n    X_train.columns) if idx in catboost_feature_select['eliminated_features'][:5]}\n\n# Dropping the removed features\nX_train.drop(columns=remove_features.values(), inplace=True)\nX_test.drop(columns=remove_features.values(), inplace=True)\nassert X_train.shape[1] == 14 and X_test.shape[1] == 14\n","64c48119":"# Specifying indices of cat features\ncat_simple_indices = [idx for idx, feature in enumerate(\n    X_train.columns) if X_train[feature].dtype == 'object']\n\n# Fitting the parsimonious model\ncatboost_simple = CatBoostClassifier(cat_features=cat_simple_indices,\n                                     verbose=0,\n                                     random_state=1,\n                                     eval_metric='F1',\n                                     objective='Logloss',\n                                     scale_pos_weight=class_weight)\ncatboost_simple.fit(X_train,\n                    y_train,\n                    eval_set=(X_test, y_test),\n                    use_best_model=True,\n                    early_stopping_rounds=100)\n\nprint(\"The performance on the metrics of the parsimonious catboost model is:\")\ndisplay(evaluate_model(catboost_simple, cat_simple_indices))\n\nprint(\n    f\"\\nThe eliminated features in the parsimonious model are: {list(remove_features.values())}.\")\n","5957e9ba":"from hyperopt import hp, tpe, fmin, Trials\n\ncatboost_space = {\n    'n_estimators': hp.quniform('n_estimators', 100, 2000, 50),\n    'max_depth': hp.quniform('max_depth', 2, 10, 1),\n    'learning_rate': hp.quniform('learning_rate', 0.01, 0.3, 0.01),\n    'colsample_bylevel': hp.quniform('colsample_bylevel', 0.5, 0.9, 0.01),\n    'subsample': hp.quniform('subsample', 0.5, 0.9, 0.01),\n    'reg_lambda': hp.quniform('reg_lambda', 1, 200, 1),\n    'random_strength': hp.quniform('random_strength', 0, 10, 0.1)\n}\n\n\ndef catboost_objective(params):\n    catboost_params = {\n        'n_estimators': int(params['n_estimators']),\n        'max_depth': int(params['max_depth']),\n        'learning_rate': params['learning_rate'],\n        'colsample_bylevel': params['colsample_bylevel'],\n        'subsample': params['subsample'],\n        'reg_lambda': params['reg_lambda'],\n        'random_strength': params['random_strength']\n    }\n    pool = Pool(data=X_train,\n                label=y_train,\n                cat_features=cat_simple_indices)\n    catboost = CatBoostClassifier(cat_features=cat_simple_indices,\n                                  verbose=0,\n                                  random_state=1,\n                                  eval_metric='F1',\n                                  objective='Logloss',\n                                  scale_pos_weight=class_weight)\n    catboost_cv = cv(pool,\n                     params=catboost.set_params(\n                         **catboost_params).get_params(),\n                     nfold=5,\n                     logging_level='Silent',\n                     early_stopping_rounds=int(params['n_estimators']) * 0.1)  # 10% of the total number of trees in a model\n    loss = np.min(catboost_cv['test-Logloss-mean'])\n    return loss\n\n\ntrials = Trials()\ncatboost_best_params = fmin(\n    catboost_objective, catboost_space, algo=tpe.suggest, max_evals=500, trials=trials, rstate=np.random.seed(1))\nprint(catboost_best_params)\n","49b489c4":"catboost_best_stopping_rounds = int(catboost_best_params['n_estimators'] * 0.1)\n\ncatboost_tuned = CatBoostClassifier(cat_features=cat_simple_indices,\n                                    verbose=0,\n                                    random_state=1,\n                                    eval_metric='F1',\n                                    objective='Logloss',\n                                    scale_pos_weight=class_weight, **catboost_best_params)\ncatboost_tuned.fit(X_train,\n                   y_train,\n                   eval_set=(X_test, y_test),\n                   use_best_model=True,\n                   early_stopping_rounds=catboost_best_stopping_rounds)\nprint('The performance of the tuned CatBoost model is: ')\ndisplay(evaluate_model(catboost_tuned, cat_simple_indices))\n\n# Saving the tuned model\ncatboost_pool = Pool(\n    X_train, y_train, cat_features=cat_simple_indices)\ncatboost_tuned.save_model('credit_card_catboost_best', pool=catboost_pool)\n","b6d15f9e":"from sklearn.metrics import classification_report\ny_pred = catboost_tuned.predict(X_test)\ncatboost_clf_report = pd.DataFrame(classification_report(\n    y_test, y_pred, output_dict=True)).transpose()\n\ndisplay(catboost_clf_report)\n","83dc4b67":"catboost_feature = pd.Series(catboost_tuned.feature_importances_,\n                             index=catboost_tuned.feature_names_).sort_values(ascending=False)\nsns.barplot(x=catboost_feature, y=catboost_feature.index, color='tab:blue')\nfor idx, importance in enumerate(catboost_feature):\n    plt.annotate(round(importance, 2), (importance+0.2, idx+0.2))\nplt.gca().set(xlabel='Feature importance values', ylabel='',\n              title='Importance of each feature in the CatBoost model')\nplt.tight_layout()\nplt.show()\n","1e09d2b7":"But how should the bank reach out to customers? In other words, how should the bank design its retention strategies according to which features of the customers? Visualising the feature importance in the CatBoost model can help us answer this question. For the CatBoost model, feature importance measures the magnitude of change to the predictions if a feature were permuted (i.e. shuffled of its values). The higher the feature importance value, the more important a certain feature is to the model.\n\nConsistent with the results from the exploratory data analysis before, the total transaction amounts and counts of customers in the last 12 months are the most important features in predicting whether a customer will churn. In fact, `Total_Trans_Ct` and `Total_Trans_Amt` together contribute to almost 60 out of 100 in the feature importance value.\n\n The third most important feature, although being much less in terms of magnitude than the first two most important features, will be the change in transaction *amount* from Q4 to Q1, and the change in transaction *count* from Q4 to Q1 is the fifth most important feature. By contrast, the amount of inactive months in the last year is surprisingly not a very important feature in predicting churning, and some demographic information like marital status and gender contributed the least to the model's prediction.","d247cc88":"As for the categorical features, there are some interesting patterns exhibited as well:\n* The proportion of churned customers, as shown by the `Attrition_Flag` column in the above table, was only about 16%. This may present the problem of class imbalance for later machine learning tasks which I should bear in mind\n* Over 90% of the customers have Blue cards from the company, so it may be a good idea to recode customers into either having blue card or not since other types of cards are extremely rare to have sufficient observations for training the model\n* The bank overall has slightly more female customers than male ones, and most (46%) customers are married \n* About 35% of the customers earn less than 40 thousand dollars per year, and 3 out of 10 hold graduate degrees","28189be5":"Let's also visualise the features by the churning status of customers. For the categorical features, these two groups of customers do not appear to differ much substantively based on the chi-squared test with $\\alpha$ at 0.05 level except for `Gender` and `Income Category`. Whether and how strongly they may be related to customer churning or not can be tested by looking at the importance of these features in the trained classification model later.","1ec05cb9":"# Predicting credit card customer churning and devising strategies for customer retention\n\n## Backgroud\nI will analyse a dataset consisting of information on demographics and credit card status of 10000 customers in a bank ([source](https:\/\/www.kaggle.com\/sakshigoyal7\/credit-card-customers)) to build a model for predicting churning and identifying crucial features so that the bank can devise more effective retention strategies. This analysis is thus solving a supervised, classification problem.\n\nThis analysis demonstrates my skills in data exploration, manipulation and visualisation, machine learning model training as well as telling stories with data to solve business problems.\n\nI will use `pandas` and `numpy` for data exploration and manipulation, `matplotlib` and `seaborn` for data viz, `scikit-learn` and `catboost` for machine learning model building, evaluation and data preprocessing.","2d1a9c0e":"After running the models, the CatBoost model performs significantly better than the rest on both F1 score and log loss in 5-fold cross validation and test set. Even without feature selection and hyperparameter tuning, the CatBoost model can already achieve an F1 score of 0.9078 and a log loss of 0.0908 in the test data. I will now move on with the CatBoost model to see if I could further improve its performance. ","4f08cf47":"With some hyperparameter tuning, the CatBoost model further improves its performance in both achieving lower log loss and a higher F1 score in the test set. Specifically, the log loss decreased by 0.0008 while the F1 score increased by 0.0061 in the test set for CatBoost model with tuned hyperparameters. We are now ready to look deeper into the model's performance and identify features that are closely related to customer churning.","bd6c5aaa":"As for the numerical features, according to the results of the Kruskal-Wallis test which tests whether difference exists between the medians of churning and non-churning customers, using $\\alpha$ at 0.05 level as the threshold, we can see that significant differences do exist for all but `Customer Age` and `Months on book`. The fact that the median of the `Months on book` feature is not statistically significant between churning and non-churning customers suggests that how much time a customer has used the bank's service may not be closely related to the probability of churning.\n\nIn terms of the more substantive differences, here are some observed from the violin plots on these numerical features:\n1. The distribution of the average utilisation rate of credit limits for churning customers peaks at 0%, whereas for existing customers they usually use 20% of their credits per month\n2. Both the medians of transaction amounts (`Total Trans Amt`) and counts (`Total Trans Ct`) per month by existing customers are higher than churning ones\n3. Lastly, churned customers on average contacted the bank more frequently and had more inactive months in last year than existing customers","dde5a6fa":"## Looking deeper into the CatBoost model\nWith the primary business goal of finding potential churning customers so that the bank can reach out in advance for retention, it is necessary to look deeper into the **recall** of the model because this metric is the most relevant in this context. By looking at the classification report below, the model performs really well on identifying churning customers, since the **recall** score is around 0.96, only about 0.04 less than the maximum possible value of 1. \n\nEven though the precision of the model is only about 0.86, in this context having a model which generates more false positives (i.e. predicting existing customers will churn) will likely be more desirable than a model which performs worse in correctly predicting churning customers to be churning soon, because failing to identify churning customers will then mean the bank losing sources of revenue.   ","a56b462f":"# Exploratory data analysis\nFirst of all, let's glimpse over the data types of the features and whether any missing values are presented in the dataset. No missing values are observed, and all features are defined in reasonable types. The `Attrition_Flag` will be the target variable which indicates whether a customer has churned on the bank or not. \n\nMoreover, the last two columns of the dataset seems to be about previous attempts of building a naive bayes model to predict churning. Let's also drop these two columns since we are more interested in how the existing features can help predict customer churning. The `CLIENTNUM` column which records the metadata of customers should also be dropped since this will not be a useful feature for the classification task.","82a57208":"# Predicting customer churning with machine learning model\nNow that I have explored the data and performed some data transformations when needed, it is time to build the models to predict customer churning. Specifically, I will try one linear classifier (Logistic Regression) and two ensemble methods (Random Forest and CatBoost). \n\nThe reason that I picked CatBoost as the gradient boosting method used in this analysis is that 5 out of the 19 features are categorical, and CatBoost can offer both one-hot encoding for binary features and **ordered target encoding** for high cardinal features which, essentially, first permutes the dataset and then target encode each sample using *only* objects before this given sample ([source](https:\/\/towardsdatascience.com\/categorical-features-parameters-in-catboost-4ebd1326bee5)). Ordered target encoding can avoid the excessive sparsity introduced by one hot encoding for high cardinality features which will hamper the performance of tree-based models.\n\nFirst of all, one-third of the dataset will be split as the test set for model validation, with both the train and test sets being stratified on the target variable (`Attrition_Flag`) due to the rare case problem mentioned before. Let's also recode the target variable into 1 or 0, with 1 meaning a customer has churned and 0 otherwise.","bbdc769a":"The next step will be looking at the summary statistics of the columns. Starting with the numerical features, there does not seem to be any abnormal values due to data entry or other human errors and all fall within reasonable ranges. Here are some preliminary insights:\n\n* The range of the ages of customers is quite wide, as the youngest customers are only 26, whereas the oldest are 73. The average of the customer is 46 years old\n* At least 75% of the customers have one or more dependents, with 25% of the customers having 3 or more\n* The customers in general having 3 years of relationship with the bank, with the shortest being only a year and the longest being almost 5 years\n* Each customer generally purchases 4 products from the bank, and 75% of them have at least 3 products in hand\n* Within the last year, each customers was inactive on average for 2 months and made 2 contacts with the bank. Moreover, these two features (coincidentally) share the same ranges and quartiles\n* The distribution of the credit limits of the customers appears to be right-skewed since the median amount is much lower than the mean, but there is not any imminent reasons to remove outliers whose values are unlikely to be invalid entries\n* The magnitude of change in the amount and counts of transaction from Q4 to Q1 does not seems to be very large overall, as 75% of the changes were below 1% in both types of changes\n* On the other hand, total transaction amounts and transaction counts of the customers exhibit signs of right-skewness in their distributions, namely, there are some particularly huge transactors with their credit cards\n* The same right skewness seems to exist for the credit utilisation rate as well, with some customers spending virtually all of their credit card limits even though on average customers only use one-quarter of their credit limits","9f6b7267":"### Selecting features for CatBoost\nThe next step will be to eliminate insignificant features to make the model more parsimonious, but how many should I remove? This can be decided by looking at the log loss value of each model with a given number of features removed vis-a-vis that of the model with all features used for training. The catboost module provides the convenient `select_features` method to do so. Let's see how the log loss value of the model would change if I removed up to **9** features in the dataset.\n","accbd888":"Based on the above feature importance, I suggest the bank to design customer retention strategies with the following points in mind:\n1. The bank should reach out to customers for retention if their transaction amounts and\/or counts were relatively below the average level in the last 12 months, since these two features are the most likely to be associated with churning later. Some strategies for retaining these customers could be offering a credit card package with lower interest rates, or asking if they would be interested in upgrading their membership level if they have been a loyal customers (say for 2 years).\n2. Relatedly, monitoring the magnitude of changes in the transaction amount and\/or count from Q4 to Q1 may also provide signs that a customer is using less frequently the credit card services from the bank and thus be likely to churn. In fact, since Q4 coincides with the Christmas holiday in late December and that should be when consumption levels should rise, if we observe that a customer's usage of the bank's credit cards is not increasing significantly, then this may be a sign that the customer is not interested in using the bank's credit card service anymore in the future.","c6200fcc":"Looking from the above plots, it seems like I can remove up to 5 features without making the model having a higher log loss than when it is trained with all available features. Thus, I will create the parsimonious model with the remaining 14 features as the input. \n\nConsistent with the line plot monitoring the change in log loss in relation to removing features above, the simplified CatBoost model performs very similarly to the one with all features used during training in all of the evaluation metrics. Specifically, the F1 score only dropped by 0.0005, whereas the log loss only increased by 0.0002.\n","0aaf80f4":"### Hyperparameter tuning\nThe next step I can take to improve the performance of the CatBoost model is hyperparameter tuning. I will use the `hyperopt` which uses bayesian optimisation to search for the optimal hyperparameter values of machine learning models.","0a503a30":"### Building baseline models\nI will first test a few classification models with their default parameters to obtain their baseline performance and decide how to proceed next. Note that the class weights are adjusted in the model parameters (`class_weight` in logistic regression and random forest\/ `scale_pos_weight` in CatBoost) as the proportion of the number of the majority class (i.e. existing customers) to that of the minority class (i.e. those who churned).\n\nAs for the metrics used for judging model performance, apart from **log loss** which will be the loss function to be minimised for training the classification models, the **F1 Score** which is the harmonic mean of *precision* (the proportion of customers predicted to be churning by the model are actually churning) and *recall* (the proportion of churning customers in reality that the model also correctly predicts as churning) will also be used, since by definition the cost of wrongly predicting a churning customer as not churning (i.e. false negative) will be larger than that of predicting a non-churning customer as churning (i.e. false positive) in this context. Moreover, in a class imbalance case, the F1 score is more sensitive to false positives than the ROC AUC score, thereby reflecting the actual performance of the model in predicting positive cases more realistically ([source](https:\/\/www.kaggle.com\/lct14558\/imbalanced-data-why-you-should-not-use-roc-curve))."}}