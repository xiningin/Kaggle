{"cell_type":{"a5e1d9c5":"code","b05584c2":"code","9381cb61":"code","95b57c7b":"code","6b0d2e55":"code","7c2bca30":"code","a9dd9f1b":"code","5fffd703":"code","41acd559":"code","0ca4b2a7":"code","b791548f":"code","8bc07c4a":"code","6a359788":"code","92c2f80d":"code","92b4c197":"code","2c22222b":"code","6b0d7b55":"code","bcf867fb":"code","a126cacd":"code","b5f56594":"code","1e22268d":"code","cdfaffc3":"code","0651a40d":"code","022b8852":"code","e73de26a":"code","426507ff":"code","889f9394":"code","93d3cb31":"code","a10366c9":"code","59cb059e":"code","66440f60":"code","6521feff":"code","d6bc2c50":"code","feeeab2a":"code","48b89424":"code","8009ee34":"code","9812a6a1":"code","d7879dca":"code","18d296fc":"code","4178dae7":"code","a33bf1c1":"code","dd7dadfd":"code","298a86c5":"code","1bed9fe1":"code","e5ed0cd8":"code","ff68236f":"code","1ff0fb53":"code","8c91cd96":"code","0c039342":"code","6c993748":"code","84197180":"code","0b00647e":"code","41f7e589":"code","cb12ab5a":"markdown","d272aa56":"markdown","f02ed9e8":"markdown","1cda69e9":"markdown","50f0f1b4":"markdown","ec8ed371":"markdown","48346bcc":"markdown","1e2ab944":"markdown","4ae3f994":"markdown","2cc0a935":"markdown","e882fe24":"markdown","c48a9074":"markdown","1c90ba51":"markdown","3debc5cd":"markdown","e41e02a5":"markdown","df57aa0c":"markdown","4f3ddd8e":"markdown"},"source":{"a5e1d9c5":"#13:56\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv')","b05584c2":"# Setting up graphics and color palette\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 9, 7\n\nsns.set_context('notebook')\nsns.set_style('whitegrid')\npal = sns.color_palette('Set2')\nsns.set_palette(pal)\n\nimport warnings  \nwarnings.filterwarnings('ignore')","9381cb61":"print(train_data.info())\nprint(test_data.info())","95b57c7b":"X_train = train_data.drop(columns=['PassengerId', 'Survived', 'Name'])\ny_train = train_data['Survived']\nX_test = test_data.drop(columns=['PassengerId', 'Name'])\nplot_data = X_train.join(y_train)\nnum_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\ncat_cols = ['Sex', 'Embarked']","6b0d2e55":"X_train[num_cols].describe()","7c2bca30":"sns.histplot(data=plot_data, x='Age', hue='Survived' , kde=True)","a9dd9f1b":"sns.countplot(data=plot_data, x='Embarked', hue='Survived')","5fffd703":"sns.countplot(data=plot_data, x='Sex', hue='Survived')","41acd559":"sns.kdeplot(data=plot_data, x='Parch', hue='Survived', multiple='stack', bw_method=.3)","0ca4b2a7":"sns.kdeplot(data=plot_data, x='SibSp', hue='Survived', multiple='stack', bw_method=.3)","b791548f":"sns.stripplot(data=plot_data, x='Pclass', y='Fare', hue='Survived', dodge=True)","8bc07c4a":"cabins = list(X_train['Cabin'].unique())\ncabins = [str(x) for x in cabins]\ncabins.remove('nan')\ncabins_cat = pd.Series([s[0] for s in cabins])\ncabins_num = pd.Series([int(s[1:]) for s in cabins])\nprint(cabins_cat.unique())\nsns.histplot(data = cabins_num)","6a359788":"tickets = list(train_data['Ticket'].unique())\ntickets.remove(np.nan)\ntickets_formatted = [('xxx', x) if not x.lower().islower() else (x.split(' ')[0].lower().replace('.', ''), x.split(' ')[-1]) for x in tickets]\ntickets_cat, tickets_num = list(zip(*tickets_formatted))\ntickets_temp = np.array([np.array(x.split('\/')) for x in tickets_cat])\ntickets_cat_list = []\nfor x in tickets_temp:\n    for y in x:\n        tickets_cat_list.append(y)\ntickets_cat_list = pd.Series(tickets_cat_list)\ntickets_cat = pd.Series(tickets_cat)\ntickets_num = pd.Series(tickets_num)\nprint(tickets_cat_list.unique())","92c2f80d":"print(pd.Series(train_data['Ticket'].unique()).count())\ntickets_num = pd.Series([int(x) for x in tickets_num if x != ''])\nsns.histplot(data=tickets_num)\ntickets_num","92b4c197":"def split_cabin(df):\n    data = df.copy()\n    data['Cabin'] = data['Cabin'].fillna(value='Z0')\n    data['Cabin_cat'] = list(map(lambda s: s[0], data['Cabin']))\n    data['Cabin_num'] = list(map(lambda s: int(s[1:]), data['Cabin']))\n    return data.drop(columns=['Cabin'])\n\nX_train = split_cabin(X_train)\nX_test = split_cabin(X_test)\nX_train","2c22222b":"def split_ticket(df):\n    data = df.copy()\n    data['Ticket'] = data['Ticket'].fillna(value='xxx 0')\n    tickets_formatted = [('xxx', x) if not x.lower().islower() else (x.split(' ')[0].lower().replace('.', ''), x.split(' ')[-1]) for x in data['Ticket']]\n    tickets_cat, tickets_num = list(zip(*tickets_formatted))\n    tickets_cat = pd.Series(tickets_cat)\n    tickets_num = pd.Series(tickets_num).replace('', '0').astype(int)\n    data['Ticket_cat'] = pd.Series(tickets_cat)\n    data['Ticket_num'] = pd.Series(tickets_num)\n    return data.drop(columns=['Ticket'])\n\nX_train = split_ticket(X_train)\nX_test = split_ticket(X_test)\nX_train","6b0d7b55":"from sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder()","bcf867fb":"def enc_cat(df, enc, train=True):\n    data = df.copy()\n    data['Embarked'].fillna(value='Z', inplace=True)\n    if train:\n        data[['Sex', 'Embarked', 'Cabin_cat', 'Ticket_cat']] = enc.fit_transform(data[['Sex', 'Embarked', 'Cabin_cat', 'Ticket_cat']])\n    else:\n        data[['Sex', 'Embarked', 'Cabin_cat', 'Ticket_cat']] = enc.transform(data[['Sex', 'Embarked', 'Cabin_cat', 'Ticket_cat']])\n    data.loc[data['Embarked']==3, ['Embarked']] = np.nan\n    return data\n    \nX_train = enc_cat(X_train, encoder)\nX_test = enc_cat(X_test, encoder, train=False)\nX_train","a126cacd":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncolumns = X_train.columns\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns=columns)\nX_test = pd.DataFrame(scaler.transform(X_test), columns=columns)\nX_train.info()","b5f56594":"X_train.describe()","1e22268d":"from sklearn.neighbors import KNeighborsRegressor\nknn_reg = KNeighborsRegressor(n_neighbors=50, weights='distance')\n\nfor col in ['Fare', 'Embarked', 'Age']:\n    train_na = X_train[X_train[col].isna()].drop(columns=[col]).dropna()\n    knn_reg.fit(X_train.dropna().drop(columns = [col]), X_train.dropna()[col])\n    X_train.loc[train_na.index, col] = knn_reg.predict(train_na)\n    test_na = X_test[X_test[col].isna()].drop(columns=[col]).dropna()\n    X_test.loc[test_na.index, col] = knn_reg.predict(test_na)\n    print('Filled', col, 'missing values.')","cdfaffc3":"X_train.fillna(value=0, inplace=True)\nX_test.fillna(value=0, inplace=True)\nX_test.info()","0651a40d":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score","022b8852":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n\nqda_model = QDA(reg_param=.35)\nqda_model.fit(X_train, y_train)","e73de26a":"qda_pred = qda_model.predict_proba(X_test)[:, -2]\nthresh = .06\npseudo_labeled_X = np.argwhere(np.logical_or(qda_pred>1-thresh, qda_pred<thresh)).ravel()\npseudo_labeled_X = X_test.loc[pseudo_labeled_X, :]\npseudo_labeled_y = pd.Series(qda_model.predict(pseudo_labeled_X))","426507ff":"X_train = pd.concat([X_train, pseudo_labeled_X], ignore_index=True)\ny_train = pd.concat([y_train, pseudo_labeled_y], ignore_index=True)","889f9394":"from sklearn.model_selection import train_test_split\n\nprint(\"# of samples: \" + str(y_train.shape[0]))\n\n# Splitting data into train (85%) CV (15%)\nX, y = (X_train, y_train)\nX_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size = .1, stratify = y_train, random_state = 69)\ny_train = np.array(y_train).astype(np.float32).reshape((-1,1))\ny_dev = np.array(y_dev).astype(np.float32).reshape((-1,1))\n\nprint(\"X_train shape: \" + str(X_train.shape) + \"\\t y_train shape:\" + str(y_train.shape))\nprint(\"X_dev shape:  \" + str(X_dev.shape) + \"\\t y_dev shape: \" + str(y_dev.shape))\n\nprint(sum(y_train==1))\nprint(sum(y_dev==1))","93d3cb31":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Grid-Search Tuning Hyper-Params\nrf_params= [{\n    'min_samples_split': [70, 90, 100, 120]\n}]\n\nrf_model = GridSearchCV(\n    RandomForestClassifier(), rf_params, scoring='accuracy', verbose=3\n)\nrf_model.fit(X, np.ravel(y))","a10366c9":"print('Best score achieved:', rf_model.best_score_)\nprint('With params:\\n', rf_model.best_params_)","59cb059e":"# rf_model = RandomForestClassifier(n_estimators=100, criterion='entropy', min_samples_leaf=20)\n# rf_model.fit(X_train, np.ravel(y_train))\n\n# y_pred = rf_model.predict_proba(X_train)\n# print('Score on the training set:')\n# print(classification_report(y_train, np.around(y_pred[:, 1])))\n# print('roc_auc score: ', end='')\n# print(roc_auc_score(y_train, y_pred[:, 1]))\n# print('f1 score:', f1_score(y_train,np.around(y_pred[:, 1])), end='\\n\\n')\n\n# y_pred = rf_model.predict_proba(X_dev)\n# print('Score on the dev set:')\n# print(classification_report(y_dev, np.around(y_pred[:, 1])))\n# print('roc_auc score: ', end='')\n# print(roc_auc_score(y_dev, y_pred[:, 1]))\n# print('f1 score:', f1_score(y_dev,np.around(y_pred[:, 1])), end='\\n\\n')","66440f60":"import xgboost as xgb\n\nxgb_params= [{\n    'max_depth': [2, 3],\n    'min_child_weight': [60, 70, 80],\n    'lambda': [2.2, 2.5, 2.8]\n}]\n\nxgb_model = GridSearchCV(\n    xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric='rmse', use_label_encoder = False, random_state=42),\n    xgb_params, scoring='accuracy', verbose=3\n)\nxgb_model.fit(X, np.ravel(y))","6521feff":"print('Best score achieved:', xgb_model.best_score_)\nprint('With params:\\n', xgb_model.best_params_)","d6bc2c50":"# y_pred = xgb_model.predict_proba(X_train)\n# print('Score on the training set:')\n# print(classification_report(y_train, np.around(y_pred[:, 1])))\n# print('roc_auc score: ', end='')\n# print(roc_auc_score(y_train, y_pred[:, 1]))\n# print('f1 score:', f1_score(y_train,np.around(y_pred[:, 1])), end='\\n\\n')\n\n# y_pred = xgb_model.predict_proba(X_dev)\n# print('Score on the dev set:')\n# print(classification_report(y_dev, np.around(y_pred[:, 1])))\n# print('roc_auc score: ', end='')\n# print(roc_auc_score(y_dev, y_pred[:, 1]))\n# print('f1 score:', f1_score(y_dev,np.around(y_pred[:, 1])), end='\\n\\n')","feeeab2a":"def dfify(hist):\n\tdf = pd.DataFrame(hist.history)\n\tdf['epoch'] = df.index\n\tval_cols = [x for x in df.columns if x.startswith('val')]\n\tdf_val = df[val_cols+['epoch']]\n\tdf.drop(columns=val_cols, inplace=True)\n\tdf_val.rename(columns={col: col.split('val_')[-1] for col in df_val.columns}, inplace=True)\n\tdf['phase'] = 'train'\n\tdf_val['phase'] = 'val'\n\treturn pd.concat([df, df_val], ignore_index=True)\n\ndef visu_history(hist):\n    rcParams['figure.figsize'] = 20, 7\n    hist_df = dfify(hist)\n    fig, axes = plt.subplots(1, 2)\n    grid = sns.lineplot(data = hist_df, x='epoch', y='loss', hue='phase', ax=axes[0])\n    grid.set(yscale='log')\n    sns.lineplot(data = hist_df, x='epoch', y='accuracy', hue='phase', ax=axes[1])\n    plt.show()","48b89424":"import tensorflow as tf\nimport tensorflow_addons as tfa\nfrom sklearn.metrics import classification_report\n\ntf.keras.backend.clear_session()\n\nregu = lambda y : tf.keras.regularizers.L2(l2=y)\n\ndef make_model(optimizer, loss_fn, metrics, output_bias='zeros', dropout=0, l2regu=0):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Flatten(input_shape=(X_train.shape[-1],)),\n        tf.keras.layers.Dense(100, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal, kernel_regularizer=regu(l2regu)),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(50, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal, kernel_regularizer=regu(l2regu)),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(30, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal, kernel_regularizer=regu(l2regu)),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(10, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal, kernel_regularizer=regu(l2regu)),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(3, activation='tanh', kernel_initializer=tf.keras.initializers.GlorotNormal, kernel_regularizer=regu(l2regu)),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.initializers.GlorotNormal, bias_initializer=output_bias)\n    ])\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=loss_fn,\n        metrics=metrics\n    )\n    \n    return model\n\n\nloss_fn = tf.losses.BinaryCrossentropy()\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=1)\n\nann_model = make_model(optimizer, loss_fn, ['accuracy'])\nann_model.summary()","8009ee34":"history = ann_model.fit(X_train[100:120], y_train[100:120], epochs=300, batch_size=1024, verbose=2)","9812a6a1":"visu_history(history)","d7879dca":"ann_model = make_model(tf.keras.optimizers.Adam(learning_rate=.004), loss_fn, ['accuracy'])\nhistory = ann_model.fit(X_train, y_train, epochs=1000, batch_size=2048, verbose=2, validation_data=(X_dev, y_dev))","18d296fc":"visu_history(history)","4178dae7":"callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=200, mode='max', restore_best_weights=True, verbose=2)\nann_model = make_model(tfa.optimizers.AdamW(learning_rate=.004, weight_decay=7e-5), loss_fn, ['accuracy'], dropout=0.05, l2regu=2e-5)\nhistory = ann_model.fit(\n    X_train, y_train, epochs=500, batch_size=2048, \n    callbacks=[callback], validation_data=(X_dev, y_dev), verbose=2\n)","a33bf1c1":"visu_history(history)","dd7dadfd":"iterations = 50\n# Parameters:\nl_rate_range = 10**np.random.uniform(-4.5, -1.5, iterations)\nw_decay_range = 10**np.random.uniform(-2.5, -5.5, iterations)\ndropout_range = np.random.uniform(.01, .1, iterations)\nlambd_regu_range = 10**np.random.uniform(-3, -7, iterations)\n\n#combos = [(0.00018754084977224016, 2.4085576475004506e-05, 0.04475658897898782, 0.0002886421264356717)]\ncombos = list(zip(l_rate_range, w_decay_range, dropout_range, lambd_regu_range))\nbest_accuracy = 0\n\ncombo_scores = pd.DataFrame(columns=['l_rate', 'w_decay', 'droupout', 'lambd_regu', 'score'])\n\n# Same initial weights for consistency:\nann_model = make_model(tfa.optimizers.AdamW(learning_rate=2e-4, weight_decay=2e-5), loss_fn, ['accuracy'], dropout=.045, l2regu=2e-5)\nann_model.save_weights('initial_weights')","298a86c5":"i=0\nfor l_rate, w_decay, dropout, lambd_regu in combos:\n    i=i+1\n    print('********* iteration', i, '\/', iterations,'*********')\n    print('L_rate:', l_rate, '\\tw_decay:', w_decay, '\\tdropout:', dropout, '\\tlambd_regu:', lambd_regu)\n    callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=500, mode='max', restore_best_weights=True, verbose=2)\n    ann_model = make_model(tfa.optimizers.AdamW(learning_rate=l_rate, weight_decay=w_decay), loss_fn, ['accuracy'], dropout=dropout, l2regu=lambd_regu)\n    ann_model.load_weights('initial_weights')\n    history = ann_model.fit(\n        X_train, y_train, epochs=1500, batch_size=2048, \n        callbacks=[callback], validation_data=(X_dev, y_dev), verbose=0\n    )\n    \n    val_acc = ann_model.evaluate(X_dev, y_dev, batch_size=2048)[1]\n    combo_scores.loc[i-1] = (*combos[i-1], val_acc)\n    \n    print('score:', val_acc)\n    if val_acc > best_accuracy:\n        best_accuracy = val_acc\n        best_history = history\n        ann_model.save_weights('best_weights')\n        print('BEST SCORE YET!!!')\n        \nann_model.load_weights('best_weights')","1bed9fe1":"ann_model.load_weights('best_weights')","e5ed0cd8":"combo_scores.loc[combo_scores['score'].argmax()]","ff68236f":"visu_history(best_history)","1ff0fb53":"rcParams['figure.figsize'] = 20, 13\nmin_score = .838\nmax_score = .84\nfig, axes = plt.subplots(2,3)\ng = sns.scatterplot(data=combo_scores, x='l_rate', y='w_decay', hue='score', hue_norm=(min_score, max_score), ax=axes[0,0])\ng.set(xscale='log', yscale='log')\ng = sns.scatterplot(data=combo_scores, x='l_rate', y='droupout', hue='score', hue_norm=(min_score, max_score), ax=axes[0,1])\ng.set(xscale='log')\ng = sns.scatterplot(data=combo_scores, x='l_rate', y='lambd_regu', hue='score', hue_norm=(min_score, max_score), ax=axes[0,2])\ng.set(xscale='log', yscale='log')\ng = sns.scatterplot(data=combo_scores, x='droupout', y='w_decay', hue='score', hue_norm=(min_score, max_score), ax=axes[1,0])\ng.set(yscale='log')\ng = sns.scatterplot(data=combo_scores, x='lambd_regu', y='w_decay', hue='score', hue_norm=(min_score, max_score), ax=axes[1,1])\ng.set(xscale='log', yscale='log')\ng = sns.scatterplot(data=combo_scores, x='droupout', y='lambd_regu', hue='score', hue_norm=(min_score, max_score), ax=axes[1,2])\ng.set(yscale='log')\n\nfor ax in np.ravel(axes):\n    ax.get_legend().remove()\n\nplt.show()","8c91cd96":"y_pred = ann_model.predict(X_train)\nprint('Score on the training set:')\nprint(classification_report(y_train, np.around(y_pred)))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_train, y_pred))\nprint('f1 score:', f1_score(y_train,np.around(y_pred)), end='\\n\\n')\n\ny_pred = ann_model.predict(X_dev)\nprint('Score on the dev set:')\nprint(classification_report(y_dev, np.around(y_pred)))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_dev, y_pred))\nprint('f1 score:', f1_score(y_dev,np.around(y_pred)), end='\\n\\n')","0c039342":"class ensemble:\n    def __init__(self, prev_layer, esbl_model):\n        self.esbl_model = esbl_model\n        self.prev_layer = prev_layer\n        self.prev_layer_pred_train = np.array([])\n        self.prev_layer_pred = np.array([])\n        \n    def fit(self, X_loc, y_loc):\n        self.prev_layer_pred_train = np.zeros(shape=(len(X_loc), len(self.prev_layer)))\n        for i in range(len(self.prev_layer)):\n            self.prev_layer_pred_train[:, i] = self.prev_layer[i].predict_proba(X_loc)[:, -1]\n        \n        self.esbl_model.fit(self.prev_layer_pred_train, np.ravel(y_loc))\n        \n    def predict_prev(self, X_loc):\n        self.prev_layer_pred = np.zeros(shape=(len(X_loc), len(self.prev_layer)))\n        for i in range(len(self.prev_layer)):\n            self.prev_layer_pred[:, i] = self.prev_layer[i].predict_proba(X_loc)[:, -1]\n        return pd.DataFrame(self.prev_layer_pred)\n    \n    def predict_proba(self, X_loc):\n        self.predict_prev(X_loc)\n        return self.esbl_model.predict_proba(self.prev_layer_pred)\n    \n    def predict(self, X_loc):\n        self.predict_prev(X_loc)\n        return self.esbl_model.predict(self.prev_layer_pred)","6c993748":"xgb_esbl = xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric='rmse', use_label_encoder = False, random_state=42)\nrf_esbl = RandomForestClassifier(min_samples_split=100)\nesbl_model = ensemble([rf_model, xgb_model, ann_model], xgb_esbl)\nesbl_model.fit(X_train, y_train)","84197180":"y_pred = esbl_model.predict(X_train)\nprint('Score on the training set:')\nprint(classification_report(y_train, np.around(y_pred)))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_train, y_pred))\nprint('f1 score:', f1_score(y_train,np.around(y_pred)), end='\\n\\n')\n\ny_pred = esbl_model.predict(X_dev)\nprint('Score on the dev set:')\nprint(classification_report(y_dev, np.around(y_pred)))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_dev, y_pred))\nprint('f1 score:', f1_score(y_dev,np.around(y_pred)), end='\\n\\n')","0b00647e":"first_layer_pred = esbl_model.predict_prev(X_test)","41f7e589":"# Choose the model:\nmodel = ann_model\ndecision = np.around(model.predict(X_test).ravel()).astype(int)\n# decision = np.around(first_layer_pred.mean(axis=1)).astype(int)\n\nsubmission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': decision})\n\nfrom IPython.display import HTML\nimport base64\ndef create_download_link(df, title = \"Download CSV file\", filename = \"submission.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(submission)","cb12ab5a":"## Ticket and Cabin","d272aa56":"# Ensemble Model (Layer 2)","f02ed9e8":"Regularization:","1cda69e9":"## Encoding\n","50f0f1b4":"# EDA:","ec8ed371":"### ANN:","48346bcc":"## Hyper parameter Tuning","1e2ab944":"## Cabin and Ticket","4ae3f994":"Overfitting a random 20 rows:","2cc0a935":"# Pseudo Labeling with QDA","e882fe24":"## 1st Layer:","c48a9074":"# Machine Learning:","1c90ba51":"### XGBoost ","3debc5cd":"### Random Forest ","e41e02a5":"Overfitting:","df57aa0c":"# Preproc:","4f3ddd8e":"# Submission:"}}