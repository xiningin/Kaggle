{"cell_type":{"1e3e9b2c":"code","e47d39e5":"code","0e4f9774":"code","165214f6":"code","d3d68f8c":"code","2862cfa8":"code","47191435":"code","5f5a5f7f":"code","d98f62e6":"markdown","c5f6232c":"markdown","7b06e44b":"markdown","bb147158":"markdown","67327c89":"markdown","ea51cf5a":"markdown","6dd41eb9":"markdown","b1963faa":"markdown","6c05ad83":"markdown","c90f0d5c":"markdown","f24838f1":"markdown","d481fa50":"markdown","7fd42ec2":"markdown","a2e7a6c0":"markdown","813f921f":"markdown"},"source":{"1e3e9b2c":"# importing important libraries for code\n\nimport csv\nimport os\nimport random\nimport sys\n\nimport numpy as np\nimport pandas as pd\nfrom os import listdir\nfrom os.path import isfile, join\nimport unidecode\nimport re\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset)\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.autograd import Variable\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm, trange\nfrom scipy.stats.stats import pearsonr\n\nfrom transformers import BertTokenizer, BertConfig\nfrom transformers import AdamW, BertModel","e47d39e5":"# Checking for GPU availability\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\nn_gpu = torch.cuda.device_count()\nprint(\"device: {} n_gpu: {}\".format(device, n_gpu))","0e4f9774":"# Use this class for linear regression model\n\nclass linearRegression(nn.Module):\n    def __init__(self):\n        super(linearRegression, self).__init__()\n        self.linear = nn.Linear(768, 1)  # input and output is 1 dimension\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out","165214f6":"# Initializing the fine-tuned model\n\nmodel_class= BertModel\nmodel_dir= '..\/input\/models\/'\n\n# Load a trained model and config that you have fine-tuned\ntokenizer = BertTokenizer.from_pretrained('..\/input\/tokenizer\/bert-base-uncased-vocab.txt')\nmodel = model_class.from_pretrained(model_dir)\nregression = torch.load(join(model_dir,\"regression_model.pth\"), map_location=torch.device(device))\nmodel.to(device)\nregression.to(device)\n\n# Initializing the parameters\nmax_seq_length= 128\nbatch_size=1","d3d68f8c":"# You can update this query list for your desire output\n\nquery_list = ['vaccine vaccination dose antitoxin serum immunization inoculation for covid 19 or coronavirus related research work', \n              'therapeutics treatment therapy drug antidotes cures remedies medication prophylactic restorative panacea for covid 19 or coronavirus']","2862cfa8":"#Initialize evaluation mode\nmodel.eval()\nregression.eval()\n\n#Loading the test set given by this challange\ntest_path= '..\/input\/CORD-19-research-challenge\/metadata.csv'\ndf_test= pd.read_csv(test_path)\n\n#Empty List for saving the final computated score\nfinal_score= list()\nquery_number=1\n\n# Loop that calculate score corresponding to each query given in above list\nfor query in query_list:\n    for i in range(len(df_test)):\n        \n        # Aggregating the [title + abstract + journal]. You can modify it according to your input\n        document= str(df_test.iloc[i].title) + str(df_test.iloc[i].abstract) + str(df_test.iloc[i].journal)\n    \n        iteration = int(len(document)\/max_seq_length)\n    \n        if iteration==0:\n            final_score.append(0)\n            continue\n    \n        result= list()\n        sentence= list()\n        \n        # Loop for create sentence inputs\n        for i in range(0,iteration):\n            sent= document[(i*max_seq_length):(i+1)*max_seq_length]\n            sentence.append(sent)\n        \n        df_temp= pd.DataFrame(sentence, columns=['d_sent'])\n        df_temp['q']= query\n    \n        # Create text sequence\n        sentences_1 = df_temp.q\n        sentences_2 = df_temp.d_sent\n    \n        # We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n        special_sentences_tempe_1 = [\"[CLS] \" + sentence for sentence in sentences_1]\n        special_sentences_tempe_2 = [\" [SEP] \" + sentence for sentence in sentences_2]\n        special_sentences = [i + j for i, j in zip(special_sentences_tempe_1, special_sentences_tempe_2)]\n        \n        tokenized_texts = [tokenizer.tokenize(sentence) for sentence in special_sentences]\n        \n        # Max sentence input \n        MAX_LEN = max_seq_length\n        \n        # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n        input_sentences = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n        \n        # Pad our input tokens\n        input_sentences = pad_sequences(input_sentences, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n        \n        # Create attention Masks\n        attention_masks = []\n        \n        # Create a mask of 1s for each token followed by 0s for padding\n        \n        for seq in input_sentences:\n            seq_mask = [float(i>0) for i in seq]\n            attention_masks.append(seq_mask)\n        \n        # Convert all of our data into torch tensors, the required datatype for our model\n        test_inputs = torch.tensor(input_sentences)\n        test_masks = torch.tensor(attention_masks)\n        \n        # Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n        batch_size = batch_size\n        \n        # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n        # with an iterator the entire dataset does not need to be loaded into memory\n        test_data = TensorDataset(test_inputs, test_masks)\n        test_sampler = RandomSampler(test_data)\n        test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n        \n        # Loop for calculating the similarity score corresponding to each sentence\n        for step, batch in enumerate(test_dataloader):\n            input_ids, input_mask= batch\n            input_ids=input_ids.to(device)\n            input_mask=input_mask.to(device)\n            \n            outputs = model(input_ids, attention_mask=input_mask)\n            last_hidden_states = outputs[1]\n            pred_score= regression(last_hidden_states)\n            pred_score= np.squeeze(pred_score, axis=1)\n            pred_score = pred_score.detach().cpu().numpy()\n            result.extend(pred_score)\n            \n        result.sort(reverse=True)\n        \n        # This is to calculate final score. Here, we are using max sentence score only. You can change it according to your requirement.\n        \n        final_score.append(max(result)\/5)\n    \n    result_query= 'q'+str(query_number)+'_score'\n    query_number+=1\n    df_test[result_query]= final_score\n\n# Saving the final results as output.csv file\ndf_test.to_csv('..\/input\/output\/output.csv')","47191435":"path= '..\/input\/output\/output.csv'\ndf= pd.read_csv(path)\n\nlabel= list()\n\nfor i in range(len(df)):\n    other_prob= 1 - df.iloc[i].q1_score - df.iloc[i].q2_score\n    \n    if other_prob<0:\n        other.append(0)\n    else:\n        other.append(other_prob)\n    \n    max_val= max(df.iloc[i].q1_score, df.iloc[i].q2_score, other_prob)\n    \n    if max_val==df.iloc[i].vaccine:\n        l='vaccine'\n    elif max_val==df.iloc[i].therapeutics:\n        l='therapeutics'\n    elif max_val==other_prob:\n        l='other'\n    \n    label.append(l)\n\ndf['label']= label","5f5a5f7f":"df = pd.read_csv(\"..\/input\/output\/final_data.csv\")\ndf.head(40) # prints the first 40 rows of the table.","d98f62e6":"# Project Description\n\nThe objective of this project is to use the state-of-the-art Clinical Semantic Textual Similarity (STS) technique to create a method to search an answer for the given query through the dataset of research papers provided as a part of [Kaggle's competition CORD-19-research-challenge](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge). Currently, we are considering a limited amount of given text (Titles + Abstracts + Journals) for each paper to show the effectiveness of the proposed approach. Gradually, this project could be extended into a scientific search system where you can extract machine-readable scientific data for a given query for further analysis.","c5f6232c":"## Scalability of Proposed Model\n\nIn this work, a limited amount of data (i.e., [title + abstract + journal]) is used to show the effectiveness of this given methodology. However, this model is easily scalable on the full-text of the given research article since it is using a sentence level inference for calculating the similarity score between the query and given article. You can see our interactive implementation of the proposed model and scale it as per your requirements.  ","7b06e44b":"# Proposed Methodology\n\nNeural Networks have become more popular in the domain of information retrieval. As we know, BERT-based scoring systems outperform previous approaches to the document retrieval as shown in [Yang et al. 2019](https:\/\/www.aclweb.org\/anthology\/D19-3004.pdf). Here, we are using Clinical Semantic Textual Similarity (STS)-based novel approach for document scoring. The primary aim of this method is to find documents that are most relevant to the given query. Here, we have used the Clinical STS dataset from [n2c2 Challange 2019](https:\/\/n2c2.dbmi.hms.harvard.edu\/) to fine-tune the BERT model. As illustrated in below figure (a), we have used BERT model for Next Sentence Prediction, and feed pair of sentences together ([CLS]Sentence 1[SEP]Sentence 2) to the BERT at training time, and used CLS token embedding for predicting similarity score between two sentences. The core idea behind using this model is that we want to generate embedding which contains the relevance between sentence 1 and sentence 2. Here, we have used Linear Regression (LR) as a regression model. The dataset consists of two sentences and their clinical semantic similarity score ranging from 0 to 5.0. The following is an example of a data item from the dataset. As can be seen from the example, the two sentences are to be scored on their semantic similarity.\n\nSentence 1: Neuro:  Proximal and distal strength in the upper and lower extremities is grossly intact.\t\nSentence 2: The distal circulation, sensation, and motor function is intact.\t\nSimilarity score: 2.5\/5.0\n\nBERT with linear regression on the top of the output CLS token is trained with the ClinicalSTS dataset. As a model input shown in figure (b), the query Q and the document text A are concatenated as a text sequence [[CLS] Q [SEP] A [SEP]], where document A = [title + abstract + journal], and passed through the trained BERT + linear regression to obtain a similarity score between Q and A. ","bb147158":"For the fine-tuning of the BERT, we used the Clinical STS dataset from [n2c2 Challange 2019](https:\/\/n2c2.dbmi.hms.harvard.edu\/). This dataset is not publicly available. Hence, we haven't provided a code here for fine-tuning of the BERT, however, you can visit our [GitHub link](https:\/\/github.com\/md-labs\/covid19-kaggle\/tree\/Mihir_3009\/scripts) for this script for fine-tuning and tried out some other scientific or clinical STS datasets. Here, the fine-tuned model of the BERT on the Clinical STS dataset is uploaded for further use. You can use the below code to use this fine-tuned model.","67327c89":"To calculate the label, we are calculating maximum score for each query and give label accoding to it. Below you can see the results of the above method.","ea51cf5a":"![(a) Fine-tuning, (b) Fine-tuned model](attachment:bertmodel.jpeg)","6dd41eb9":"### On March 19, 2020, the White House Office of Science and Technology Policy (WH-OSTP) issued a statement announcing the release of an extensive machine-readable collection of scientific articles about COVID-19, SARS-CoV-2, and the coronavirus group, jointly by several institutions including National Library of Medicine and Allen Institute for AI, and WH-OSTP: \n> the institutions in issuing a call to action to the Nation\u2019s artificial intelligence experts to develop new text and data mining techniques that can help the science community answer high-priority scientific questions related to COVID-19\n### The dataset called COVID-19 Open Research Dataset (CORD-19) presently has nearly 59,000 articles (extracted from various archives), with more than 35,000 of which have full text. The institutions further compiled a series of questions to be answered. For example, some questions related to COVID-19 vaccines and therapeutics are:\n* Effectiveness of drugs being developed and tried to treat COVID-19 patients.\n* Exploration of use of best animal models and their predictive value for a human vaccine.\n* Efforts targeted at a universal coronavirus vaccine.\n* Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers","b1963faa":"# Results\n\nHere, I have provided results for two queries:\n\n* Q1: Vaccine vaccination dose antitoxin serum immunization inoculation for COVID-19 or coronavirus related research work\n* Q2: Therapeutics treatment therapy drug antidotes cures remedies medication prophylactic restorative panacea for COVID-19 or coronavirus\n\nUsing these two queries, I have predicted the labels corresponding to each paper. For that, you can refer below code:","6c05ad83":"The main aim of this work is to provide a system that can give you documents that are relevant to the given query. Here, you can provide your custom query to get the desired output. This model uses several queries that are likely to be useful in searching for a vaccine and therapeutics related articles. The COVID-19 dataset challenge questions are leveraged for this purpose. Examples are:\n\n* Q1: Vaccine vaccination dose antitoxin serum immunization inoculation for COVID-19 or coronavirus related research work\n* Q2: Therapeutics treatment therapy drug antidotes cures remedies medication prophylactic restorative panacea for COVID-19 or coronavirus\n\nWe provided the model output in the result section.","c90f0d5c":"![corona.jpeg](attachment:corona.jpeg)","f24838f1":"# Implementation\n\nWe present a notebook that makes it possible for anyone to reproduce our system or modify it according to their requirements. We made our fine-tuned BERT models available and code to use it. Our notebook is set up to allow similarity score to be calculated for given test set collection (i.e., set of research articles in this competition). Here, we fine-tuned the BERT base model with a learning rate $2 x 10^{-4}$ for 10 epochs.\n\nFor the implementation of the above approach, we have used [Pytorch](https:\/\/pytorch.org\/) implementation of the BERT model by [huggingface](https:\/\/huggingface.co\/transformers\/index.html). All Code, Models, Data and Results mentioned in this Kernel are well documented and available in this [GitHub Link](https:\/\/github.com\/md-labs\/covid19-kaggle\/tree\/Mihir_3009\/scripts).","d481fa50":"In this simple approach, long text poses a problem since base-BERT can handle only the 512 length of sentence at a time. In this case, we have research papers with full body text. Based on Yang et al. 2019, we presented sentence level inference as a solution to this problem. Hence, we generated the score for each sentence in the given document and aggregate that score to produce the document score. To get the score for the whole document, we simply devise function inspired by [Kotzias et al. 2015](https:\/\/dl.acm.org\/doi\/10.1145\/2783258.2783380) that assumes that the score of a document is obtained by averaging the score of its top n sentences. Assume candidate document d and corresponding set of sentences $\\mathcal{S}$ from d. Hence, document score ($S_d$) is given by below averaging formula:\n\n$S_d = \\frac{1}{|\\mathcal{S}|} \\sum_{i=0}^{n} \\text{score}(s_i)$,\n\nwhere $\\text{score}(s_i)$ denoted the function which gererates similarity score corresponding to the given input. In this formula, we are only considering the top n sentences from given text with high scores because they are more responsible for identifying the text. At the end, we normalized the obtained score between [0,1].","7fd42ec2":"# Future Plans and Remarks\n\nThis work describes the novel methodology based on STS for a straightforward application of BERT to compute similarity score and classify in particular tasks via sentence-level inference and aggregate scoring. In the future, we are planning to extend this approach for full-text. Moreover, we are planning to use more scientific and Clinical STS datasets for fine-tuning BERT to improve the results on this dataset.","a2e7a6c0":"# COVID-19: BERT-based STS Method to Effectively Identify Articles related to Therapeutics and Vaccines\n\n* #### Team: MD-Lab, ASU\n* #### Author: Mihir Parmar\n* #### Team Members: Rishab Banerjee, Hong Guan, Jitesh Pabla, Ashwin Karthik Ambalavanan, Murthy Devarakonda\n* #### Email ID: loccapollo@gmail.com, hguan6@asu.edu, jpabla1@asu.edu, aambalav@asu.edu, Murthy.Devarakonda@asu.edu\n* #### Kaggle ID: loccapollo, hongguan, jiteshpabla, aambalav, murthydevarakonda\n* #### This is a Team Submission","813f921f":"Below is the code for computing similarity score between the query and given scientific article. You can modify this code according to your requirements."}}