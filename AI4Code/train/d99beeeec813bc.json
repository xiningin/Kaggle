{"cell_type":{"428c3685":"code","071e6f28":"code","760774ab":"code","1602c0d0":"code","3cab1cc5":"code","3df0733e":"code","992af75a":"code","1113a64c":"code","efc38fe1":"code","0fbc4633":"code","eaab8ad6":"code","c375bbaa":"markdown","1f13a48f":"markdown","9aa507a9":"markdown","4be0d7a4":"markdown","9848459e":"markdown"},"source":{"428c3685":"import pylab as pl\n\nf = lambda p: p**4 * (1-p)**2\nx = pl.linspace(0,1,100)\npl.plot(x, f(x))\npl.plot(0.667, f(0.667), 'r*')\npl.xlabel('p')\npl.ylabel('Likelihood');","071e6f28":"import pylab as pl\n\nd = 0.97\nf = lambda p: p * (1-p*d) * p*d**3 * (1-p*d**5)\nx = pl.linspace(0,1,100)\npl.plot(x, f(x))\npl.plot(0.545, f(0.545), 'r*')\npl.xlabel('p')\npl.ylabel('Likelihood');","760774ab":"%%writefile ML_agent.py\n\nimport numpy as np\nfrom collections import defaultdict\nfrom random import choices\nfrom scipy.special import softmax\nfrom scipy.optimize import minimize\n\ndef sigmoid(x, x0=10):\n    return 1 \/ (1 + np.exp(x0-x))\n\ndef maximize(f, bound=[0,1], res=0.1, tol=1e-4):\n    x = np.linspace(bound[0], bound[1], int((bound[1]-bound[0])\/res)+1)\n    a = x[f(x).argmax()]\n    if res < tol:\n        return a\n    else:\n        return maximize(f, [max(bound[0], a-res), min(bound[1], a+res)], res\/10, tol)\n\n\nnum_activated, num_activated_byme, LL, total_rewards, my_last_action, p_estimate = (None, )* 6\n\ndef agent(observation, configuration):\n    global num_activated, num_activated_byme, LL, total_rewards, my_last_action, p_estimate\n    \n    N = configuration['banditCount']\n    d = configuration['decayRate']\n    \n    # initialization\n    if num_activated is None:\n        num_activated = np.zeros(N)\n        num_activated_byme = np.zeros(N)\n        LL = defaultdict(lambda: '1')\n        total_rewards = 0\n        my_last_action = -1\n        p_estimate = np.zeros(N) + 0.5\n\n    \n    # update\n    if observation['lastActions']:\n        num_activated[observation['lastActions'][0]] += 1\n        num_activated[observation['lastActions'][1]] += 1\n        num_activated_byme[my_last_action] += 1\n        last_reward = observation['reward'] - total_rewards\n        total_rewards = observation['reward']\n        LL[my_last_action] += f' * (p* {d ** (num_activated[my_last_action]-1)})' if last_reward else f' * (1 - p* {d ** (num_activated[my_last_action]-1)})'\n    \n    # decision\n    if min(num_activated_byme) < 2:\n        my_last_action = int(num_activated_byme.argmin())\n        return my_last_action\n    \n    next_prob_estimate = np.zeros(N)\n    for b in observation['lastActions']:\n        f = lambda p: eval(LL[b])\n        best_p = maximize(f, bound=[0,1])\n        r = sigmoid(num_activated_byme[b], 3)\n        p_estimate[b] = best_p * r + 0.5 * (1-r)\n    for b in range(N):\n        next_prob_estimate[b] = p_estimate[b] * d ** num_activated[b]\n    \n    my_last_action = int(np.argmax(next_prob_estimate))\n    return my_last_action","1602c0d0":"%%writefile random_agent.py\n\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","3cab1cc5":"%%writefile ordinal_agent.py\n\nlast = -1\ndef ordinal_agent(observation, configuration):\n    global last\n    \n    last += 1\n    return last % 100","3df0733e":"!pip install kaggle-environments --upgrade","992af75a":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\n\nenv.run([\"ML_agent.py\", \"random_agent.py\"])\nenv.render(mode=\"ipython\", width=800, height=800)","1113a64c":"import seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n\ndef run(agent1, agent2):\n    env = make(\"mab\", debug=True)\n    env.run([agent1, agent2])\n    return env.state[0]['reward'] - env.state[1]['reward']\n\n\ndef evaluate(agents, episodes=5):\n    result = pd.DataFrame(columns=agents, index=agents)\n    for i, agent1 in enumerate(agents):\n        for j, agent2 in enumerate(agents[:i+1]):\n            result.loc[agent1, agent2] = [run(agent1, agent2) for _ in range(episodes)]\n            result.loc[agent2, agent1] = [-score for score in result.loc[agent1, agent2]]\n    return result\n\nresult = evaluate(['random_agent.py', 'ML_agent.py', 'ordinal_agent.py'], 2)\nsns.heatmap(result.applymap(np.nanmax), cbar_kws={'label':'Maximum Lead'});","efc38fe1":"sns.heatmap(result.applymap(np.nanmin), cbar_kws={'label':'Minimum Lead'});","0fbc4633":"sns.heatmap(result.applymap(np.nanmean), cbar_kws={'label':'Average Lead'});","eaab8ad6":"result","c375bbaa":"# Maximum Likelihood Estimation\n\nMLE finds the parameter that makes the data most likely. It does not imply the best parameter given the data, for that we need the Bayesian approach. But first, it is good to practice with MLE.\n\nEvery single badit pull can be imagined as a Bernouli experiment which can be succesfull with probability p(t) and unsuccessfull by probability 1-p(t). \"t\" here denotes the time since the bandit's assigned probability decays over time by pulling it. This can be simplified as:\n\n\\\\[p(t) = p(n), \\\\]\n\nwhere n is the number of times the bandit has been pulled so far, or:\n\n\\\\[p(n) = p_0 d^{n} ,\\\\]\n\nwhere d is the decay rate (0.97).\n\nIn our case, the number of times the bandits have been pulled are known but the initial success probability of the bandits are unknown and we should estimate them. By having a good estimate of the \\\\(p_0\\\\)s we can then calculate the success probability for each bandit in the next round and chose the most probable one in the next action.\n\nIn normal Bernouli experiment where there is no decay, you would repeat the experiemt and calculate the p that has the maximum likelihood. For example, if you repeat it 6 times with 4 times success and 2 times unsuccess, you can estimate the likelihood as \n\n\\\\[p^4.(1-p)^2\\\\]\n\nand the identify the \\\\(p\\\\) that maximizes the likelihood. (in this case \\\\( p \\approx 0.66 \\\\) )","1f13a48f":"## A way to evaluate Agents before submission","9aa507a9":"## Agent\n\nThe following Agent works by estimating the initial probability by maximum likelihood and then calculates the success probability of next pulling for bandits. Of course, the estimates are noisy in the beginning but gets better by more experiments.","4be0d7a4":"## The Agent against the random Agent","9848459e":"In our case we have two problem, first the decay, and second unknown rewards by opponent pulling. In this case after 6 times pulling the bandit the reward might be something like this:\n\n\\\\[ 1 0 X 1 X 0 \\\\]\n\nwhere X denotes unknown rewards. The probability of succes and fail has also changed over time:\n\n|experiment | 1 | 0 | X | 1 | X | 0 |\n|:----------|:---:|:---:|:---:|:---:|:---:|:---:|\n|**success prob** | \\\\(p\\\\) | \\\\(pd\\\\) | \\\\(pd^2\\\\) | \\\\(pd^3\\\\) | \\\\(pd^4\\\\) | \\\\(pd^5\\\\) |\n|**fail prob** |\\\\((1-p)\\\\)| \\\\((1-pd)\\\\) | \\\\((1-pd^2)\\\\) | \\\\((1-pd^3)\\\\) | \\\\((1-pd^4)\\\\) | \\\\((1-pd^5)\\\\) |\n| |<img width=60\/>|<img width=60\/>|<img width=60\/>|<img width=60\/>|<img width=60\/>|<img width=60\/>|\n\nThe lilkelihood equation that we can infer from this experiment is:\n\n\\\\[p.(1-pd).pd^3.(1-pd^5)\\\\]\n\nwhich is maximum when \\\\(p \\approx 0.545\\\\)"}}