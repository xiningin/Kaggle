{"cell_type":{"2684938b":"code","b0fdd3a6":"code","198d7494":"code","b23255db":"code","3bd06bea":"code","2636dce9":"code","1c939ecb":"code","fd93d5a4":"code","2509f003":"code","3eb074d0":"code","4700efee":"code","1456d6a4":"code","2c22d170":"code","1bab1d16":"code","24490304":"code","74910c99":"code","f26a47b6":"code","ed5c1bac":"code","6e96db2c":"code","f76d7c79":"code","c7141d89":"code","68a55593":"code","9f301d69":"code","70d43810":"code","533d8f46":"markdown","a8d3592f":"markdown","366c7186":"markdown","2fe3471c":"markdown","e4bb45d1":"markdown","5212b6d3":"markdown","8ca383c1":"markdown","9ad12cfb":"markdown","5ef603f9":"markdown","73aee364":"markdown","5d4c458d":"markdown"},"source":{"2684938b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\nfrom sklearn.model_selection import KFold\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n%matplotlib inline","b0fdd3a6":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","198d7494":"train.head()","b23255db":"all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']))","3bd06bea":"matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nprices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train[\"SalePrice\"])})\nprices.hist()","2636dce9":"#log transform the target:\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])","1c939ecb":"all_data = pd.get_dummies(all_data)","fd93d5a4":"#filling NA's with the mean of the column:\nall_data = all_data.fillna(all_data.mean())","2509f003":"#creating matrices for sklearn:\nX_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\ny = train.SalePrice","3eb074d0":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, Lasso, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, np.array(X_train), np.array(y), scoring=\"neg_mean_squared_error\", cv = kfolds))\n    return(rmse)","4700efee":"ridge_model = Ridge(alpha=0.1)\nridge_model.fit(X_train, y)\nprint(rmse_cv(ridge_model))\n\n\npredictions = np.expm1(ridge_model.predict(X_test))","1456d6a4":"turn_in = pd.DataFrame(test['Id'])\nturn_in['SalePrice'] = predictions\n\nturn_in.to_csv('FirstSubmission.csv',index=False)\n# We got a RMSE of 0.13029","2c22d170":"# Problem 3\n\nridgecv_model = RidgeCV()\nlassocv_model = LassoCV()\n\nridgecv_model.fit(X_train, y)\nlassocv_model.fit(X_train, y)\n\nprint(rmse_cv(ridgecv_model))\nprint(rmse_cv(lassocv_model))","1bab1d16":"# Best score for Ridge: .1104\n# Best score for Lasso: .1736","24490304":"# Problem 4\n\nalphas = 10**np.linspace(10,-6,100)*.5\nnon_zero_coefs = []\n\nfor a in alphas:\n    lasso_model = Lasso(alpha=a,max_iter=100000)\n    lasso_model.fit(X_train, y)\n    \n    num_non_zero = 0\n    for coef in lasso_model.coef_:\n        if (abs(coef) !=0):\n            num_non_zero += 1\n    non_zero_coefs.append(num_non_zero)\n    \n    \nsns.lineplot(list(range(0,100)),non_zero_coefs)\n# Shown below, we are plotting the number of non-zero coefficients as our alpha size decreases. This graph validates our intuition\n# that as our regularization weight increases, more coefficients will zero out.","74910c99":"# Problem 5\n# Add the outputs of models as features and train a ridge regression on all features plus model outputs\nstacking_data_train = X_train.copy(deep=True)\nstacking_data_train['ridge_predictions'] = pd.Series(ridgecv_model.predict(X_train))\nstacking_data_train['lasso_predictions'] = pd.Series(lassocv_model.predict(X_train))\n\nstacking_data_test = X_test.copy(deep=True)\nstacking_data_test['ridge_predictions'] = pd.Series(ridgecv_model.predict(X_test))\nstacking_data_test['lasso_predictions'] = pd.Series(lassocv_model.predict(X_test))\n\n\nstacked_ridge_model = RidgeCV()\nstacked_ridge_model.fit(stacking_data_train, y)\n\npredictions = np.expm1(stacked_ridge_model.predict(stacking_data_test))\nturn_in = pd.DataFrame(test['Id'])\nturn_in['SalePrice'] = predictions\n\nturn_in.to_csv('StackedRidgeSubmission.csv',index=False)\n","f26a47b6":"# The above model returned a RMSE of 0.12241 which is an improvement over the previous one!","ed5c1bac":"# Problem 6\nfrom xgboost import XGBRegressor\n\nmy_model = XGBRegressor(silent=True)\nmy_model.fit(X_train, y, verbose=False)\nprint(rmse_cv(my_model))\n\npredictions = np.expm1(my_model.predict(X_test))\nturn_in = pd.DataFrame(test['Id'])\nturn_in['SalePrice'] = predictions\n\nturn_in.to_csv('XGBoost_no_tuning.csv',index=False)","6e96db2c":"xgb_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=.4603, gamma=.0468, learning_rate=0.05, max_delta_step=0,\n       max_depth=3, min_child_weight=1.7817, missing=None, n_estimators=2200,\n       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0.4640, reg_lambda=0.8571, scale_pos_weight=1, seed=42,\n       silent=True, subsample=0.5213)\nprint(rmse_cv(xgb_model))\n\nxgb_model.fit(X_train, y)\npredictions = np.expm1(xgb_model.predict(X_test))\nturn_in = pd.DataFrame(test['Id'])\nturn_in['SalePrice'] = predictions\n\nturn_in.to_csv('XGBoost_with_tuning.csv',index=False)","f76d7c79":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import ElasticNetCV\n\nalphas_ridge = list(np.linspace(14.5,15.6,11))\nalphas_lasso = [5e-05,.0001,.0002,.0003,.0004,.0005,.0006,.0007,.0008]\nalphas_e = [.0001,.0002,.0003,.0004,.0005,.0006,.0007]\nl1ratio_e = [.8,.85,.9,.95,.99,1]\n\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_ridge, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(alphas=alphas_lasso, max_iter=1e7, cv=kfolds, random_state=42))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=alphas_e, cv=kfolds, l1_ratio=l1ratio_e))\n\nmodels = {'Ridge': ridge,\n          'Lasso': lasso,\n          'ElasticNet': elasticnet}\n\npredictions = {}\nscores = {}\n\nfor name, model in models.items():\n    model.fit(X_train, y)\n    predictions[name] = model.predict(X_train)\n    \n#     score = rmse_cv(model)\n#     scores[name] = (score.mean(), score.std())\n    \nprint(scores)    ","c7141d89":"from sklearn.metrics import mean_squared_error\n\nblended_predictions = (predictions['ElasticNet'] + predictions['Lasso'] + predictions['Ridge'])\/3\nprint(np.sqrt(mean_squared_error(y, blended_predictions)))\n","68a55593":"test_predictions = {}\n\nfor name, model in models.items():\n    model.fit(X_train, y)\n    test_predictions[name] = np.expm1(model.predict(X_test))\n\nfinal_predictions = (test_predictions['ElasticNet'] + test_predictions['Lasso'] + test_predictions['Ridge'])\/3\nturn_in = pd.DataFrame(test['Id'])\nturn_in['SalePrice'] = final_predictions\n\nturn_in.to_csv('BlendingMLRModels.csv',index=False)","9f301d69":"# Ok so now we have tried blending and stacking. Let's see if we can put it all together and climb the leaderboards.\n\nfrom mlxtend.regressor import StackingCVRegressor\n\nstacking_model = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, xgb_model), meta_regressor=ridge, use_features_in_secondary=True)\nstacking_model.fit(X_train, y)","70d43810":"# Stacking has worked, now we should get our blended results\nblended_stacked_results = (np.expm1(xgb_model.predict(X_test)) + np.expm1(stacking_model.predict(np.array(X_test))) + np.expm1(lasso.predict(X_test)) + np.expm1(ridge.predict(X_test)) + np.expm1(elasticnet.predict(X_test)))\/5\nprint(blended_stacked_results)\n\nturn_in = pd.DataFrame(test['Id'])\nturn_in['SalePrice'] = blended_stacked_results\nturn_in.to_csv('StackedBlended.csv',index=False)","533d8f46":"###Data preprocessing: \nWe're not going to do anything fancy here: \n \n- First I'll transform the skewed numeric features by taking log(feature + 1) - this will make the features more normal    \n- Create Dummy variables for the categorical features    \n- Replace the numeric missing values (NaN's) with the mean of their respective columns","a8d3592f":"## Problem 7\nFor this part we are going to try and get the best score we can. First we are going to try and blend 3 different MLR models and see what we get. After that, we will attempt to stack and then blend many models to improve accuracy and prevent overfitting.","366c7186":"Our final and best MSE from a stacked and blended model was 0.1898 which puts us in the top 18% of competitors. I believe that in order to achieve a better MSE, we would need to do a bit more work in preprocessing our data. We kind of overlooked this part but could tell its importance by looking at other popular notebooks. Other things we learned is that ensembling is an effective strategy that should be used in regression.","2fe3471c":"## Problem 3: Comparing Ridge and Lasso\nIn this problem we run a CV ridge and lasso model and compare the best scores we can get from each model. From the results we see that Ridge performs better than Lasso generally for this data set.","e4bb45d1":"## Problem 6 \nFor the first part of this problem we just run a default XGBoost and get a score. It's not too great so we find some optimal hyperparametes per other notebooks and train another XGBoost.","5212b6d3":"## Lab 3: \n\n* Jacob Stokes - jts3867\n* Andrew Annestrand - ata758 \n* Musa Rafik - mar6827","8ca383c1":"## Problem 2: Simple Ridge Regression\n\nHere we run a ridge regression with alpha set to 0.1. ","9ad12cfb":"* XGBoost MSE no tuning: 0.13904\n* XGBoost MSE with tuning: 0.13136","5ef603f9":"## Problem 5 \nFor this problem, we train both a lassocv and ridgecv and use their predictions on test data as features for another ridgecv model. This is the essence of \"stacking.\" We stack the results of the models we train so that we can get the best from each and reduce our bias.","73aee364":"## Preprocessing","5d4c458d":"## Problem 4\nFor this problem we are training many lasso models and graphing the effect of the magnitude of alphas on coefficients. In our graph, the number of non-zero coefficients is shown to increase as the value of alpha decreases."}}