{"cell_type":{"0902cb0e":"code","bcb7d87f":"code","172095f8":"code","906c938f":"code","1068d0c4":"code","8192c661":"code","1d17b3a7":"code","b238bf62":"code","4bb1148e":"code","3d7ec50b":"code","9d92ff76":"code","e6f6cf76":"code","b0ffd146":"code","93c716f9":"code","e4b83a5f":"code","28d380ce":"code","7f304969":"code","5beda8db":"code","d9e4e78f":"code","1cd458f7":"code","d74840bd":"code","b8593e28":"code","7ae50501":"code","f084b18a":"code","e1e95b5b":"markdown"},"source":{"0902cb0e":"import gc\nfrom functools import partial\nfrom pathlib import Path\n\nfrom fastai.text import *\nfrom fastai.callbacks import *\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# pd.set_option('display.max_colwidth', 200)\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.min_rows', 100)\n# pd.set_option('display.max_rows', 100)\n\nhome = Path(\".\")\ninput_dir = Path(\"\/kaggle\/input\/google-quest-challenge\/\")\n\n!mkdir -p ~\/.fastai\/models\/\n!cp -R \/kaggle\/input\/fastai-wt103-models\/* ~\/.fastai\/models\/","bcb7d87f":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# seed_everything(42)","172095f8":"raw_train = pd.read_csv(input_dir\/\"train.csv\")\nraw_test = pd.read_csv(input_dir\/\"test.csv\")\nsubm = pd.read_csv(input_dir\/\"sample_submission.csv\")\n\nq_labels = subm.columns[subm.columns.str.startswith(\"question_\")].to_list()\nassert len(q_labels) == 21\n\na_labels = subm.columns[subm.columns.str.startswith(\"answer_\")].to_list()\nassert len(a_labels) == 9","906c938f":"train_df = raw_train.iloc[np.random.permutation(len(raw_train))]\ncut = int(0.2 * len(train_df)) + 1\ntrain_df, valid_df = train_df[cut:], train_df[:cut]\ntrain_lm_df = train_df.append(raw_test, ignore_index=True, sort=False)","1068d0c4":"data_lm = TextLMDataBunch.from_df(home, train_lm_df, valid_df,\n                                  text_cols=[\"question_title\", \"question_body\", \"answer\"],\n                                  mark_fields=True,\n                                  bs=128)\n\nq_data_clas = TextClasDataBunch.from_df(home, train_df, valid_df, raw_test,\n                                      vocab=data_lm.train_ds.vocab,\n                                      text_cols=[\"question_title\", \"question_body\"],\n                                      label_cols=q_labels,\n                                      mark_fields=True,\n                                      bs=64)\n\na_data_clas = TextClasDataBunch.from_df(home, train_df, valid_df, raw_test,\n                                      vocab=data_lm.train_ds.vocab,\n                                      text_cols=[\"question_title\", \"question_body\", \"answer\"],\n                                      label_cols=a_labels,\n                                      bs=64)\n\ndata_lm.save('.\/data_lm_export.pkl')\nq_data_clas.save('.\/q_data_clas.pkl')\na_data_clas.save('.\/a_data_clas.pkl')","8192c661":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5,\n                               metrics=[accuracy, Perplexity()],\n                               callback_fns=[partial(SaveModelCallback, monitor=\"perplexity\", mode=\"min\", name=\"best_model\"),\n                                             partial(EarlyStoppingCallback, monitor=\"perplexity\", mode=\"min\", patience=10)])\nlearn = learn.to_fp16()\nlr = 5e-02\nmoms = (0.8, 0.7)\nwd=0.1","1d17b3a7":"learn.fit_one_cycle(5, slice(lr), moms=moms, wd=wd)","b238bf62":"learn.unfreeze()\nlearn.fit_one_cycle(10, slice(lr\/2), moms=moms, wd=wd)","4bb1148e":"learn.save_encoder('ft_enc')\nlearn.save('lm_model')","3d7ec50b":"del learn\ndel data_lm\ngc.collect()","9d92ff76":"# q_data_clas.show_batch()","e6f6cf76":"# a_data_clas.show_batch()","b0ffd146":"from scipy.stats import spearmanr\n\nclass AvgSpearman(Callback):\n    def on_epoch_begin(self, **kwargs):\n        self.preds = None\n        self.target = None\n    \n    def on_batch_end(self, last_output, last_target, **kwargs):\n        if self.preds is None or self.target is None:\n            self.preds = last_output\n            self.target = last_target\n        else:\n            self.preds = np.append(self.preds, last_output, axis=0)\n            self.target = np.append(self.target, last_target, axis=0)\n    \n    def on_epoch_end(self, last_metrics, **kwargs):\n        spearsum = 0\n        for col in range(self.preds.shape[1]):\n            spearsum += spearmanr(self.preds[:,col], self.target[:,col]).correlation\n        res = spearsum \/ (self.preds.shape[1] + 1)\n        return add_metrics(last_metrics, res)","93c716f9":"q_learn = text_classifier_learner(q_data_clas, AWD_LSTM,\n                                metrics=[AvgSpearman()],\n                                callback_fns=[partial(EarlyStoppingCallback, monitor='avg_spearman', mode=\"max\", min_delta=0.01, patience=7),\n                                              partial(SaveModelCallback, monitor=\"avg_spearman\", mode=\"max\", name=\"best_model\"),]).to_fp16()\nq_learn.load_encoder(\"ft_enc\");","e4b83a5f":"lr = 5e-02\nmoms = (0.8, 0.7)\nwd=0.1","28d380ce":"def fit(learn, name):\n    learn.fit_one_cycle(4, lr, moms=moms, wd=wd)\n    learn.freeze_to(-2)\n    learn.fit_one_cycle(2, slice(lr\/2\/(2.6**4),lr), moms=moms, wd=wd)\n    learn.freeze_to(-3)\n    learn.fit_one_cycle(2, slice(lr\/4\/(2.6**4),lr\/2), moms=moms, wd=wd)\n    learn.unfreeze()\n    learn.save(f'{name}-stage3-clas')\n    learn.fit_one_cycle(20, slice(lr\/20\/(2.6**4),lr), moms=moms, wd=wd)","7f304969":"fit(q_learn, \"q\")","5beda8db":"q_test_preds, _ = q_learn.get_preds(DatasetType.Test, ordered=True)","d9e4e78f":"del q_learn\ndel q_data_clas\ngc.collect()","1cd458f7":"a_learn = text_classifier_learner(a_data_clas, AWD_LSTM,\n                                  metrics=[AvgSpearman()],\n                                  callback_fns=[partial(EarlyStoppingCallback, monitor='avg_spearman', mode=\"max\", min_delta=0.01, patience=5),\n                                                partial(SaveModelCallback, monitor=\"avg_spearman\", mode=\"max\", name=\"best_model\"),]).to_fp16()\na_learn.load_encoder(\"ft_enc\");","d74840bd":"fit(a_learn, \"a\")","b8593e28":"a_test_preds, _ = a_learn.get_preds(DatasetType.Test, ordered=True)","7ae50501":"sample_submission = pd.DataFrame(columns=[\"qa_id\"]+q_labels+a_labels)\nsample_submission.loc[:, \"qa_id\"] = raw_test[\"qa_id\"]\nsample_submission.loc[:, q_labels] = q_test_preds\nsample_submission.loc[:, a_labels] = a_test_preds\n# sample_submission.loc[:, 1:] = np.clip(sample_submission.loc[:, 1:], 0.00001, 0.999999)\n\nsample_submission.to_csv(\"submission.csv\", index=False)","f084b18a":"sample_submission.tail()","e1e95b5b":"This notebook is to train fastai classifier with transfer learning.\n\nSources\nhttps:\/\/www.kaggle.com\/melissarajaram\/roberta-fastai-huggingface-transformers"}}