{"cell_type":{"6498eb08":"code","f6368f45":"code","854c6ca5":"code","fb3400e7":"code","658d3cd0":"code","fb4f6c42":"code","fe95b25a":"code","f0edda32":"code","9bda20a7":"code","0b7eebb7":"code","a7313d6e":"code","dd866c3a":"code","ae30691c":"code","f87c1f7c":"code","0f9e9857":"code","12066536":"code","09295280":"code","fd571bf0":"code","9c7fb386":"code","0c06a67f":"code","327ff8ee":"code","232362ff":"code","d03344e0":"code","ddae3adc":"code","748e3bb1":"code","350acbc5":"code","3803610d":"code","3504238f":"code","896cef0b":"code","17220a97":"markdown","7e762566":"markdown","3b00616f":"markdown","dd651fd8":"markdown","2107cc7c":"markdown","775e07ae":"markdown","ca42dd64":"markdown","597859d3":"markdown","982a39c1":"markdown","95762c57":"markdown","488cada3":"markdown","8c7fdb1b":"markdown","c7077d1f":"markdown"},"source":{"6498eb08":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno\nimport time\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pickle","f6368f45":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndata = pd.read_excel('\/kaggle\/input\/covid19\/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx')\npd.options.display.max_columns = None","854c6ca5":"data.shape","fb3400e7":"cat_features = data.select_dtypes('object').columns\ncat_features = list(cat_features)\ncat_features","658d3cd0":"print(data['AGE_PERCENTIL'].unique())\nprint('\\n',data['WINDOW'].unique())\n","fb4f6c42":"data.fillna(0.0, inplace=True)","fe95b25a":"data.head()","f0edda32":"data.drop('PATIENT_VISIT_IDENTIFIER', axis=1, inplace=True)","9bda20a7":"data_one_hot = pd.get_dummies(data,columns=cat_features)","0b7eebb7":"data_one_hot.head()","a7313d6e":"X = data_one_hot.drop('ICU', axis=1)\nY = data_one_hot['ICU']","dd866c3a":"X = (X - np.min(X)) \/ np.std(X)","ae30691c":"X.head(1)","f87c1f7c":"def fit_algo(algo, x, y, cv):\n    #Fit the model\n    model = algo.fit(x, y)\n    \n    #Check its score\n    acc = round(model.score(x, y) *100, 2)\n    y_pred = model_selection.cross_val_predict(algo, x, y, cv=cv, n_jobs = -1)\n    \n    acc_cv = round(metrics.accuracy_score(Y,y_pred)*100, 2)\n    \n    return y_pred, acc, acc_cv, model","0f9e9857":"from sklearn.linear_model import LogisticRegression\nstart_time = time.time()\npred_now, acc_lr, acc_cv_lr, lr = fit_algo(LogisticRegression(C=0.1)\n                                        , X, Y, 10)\n\nlr_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_lr)\nprint(\"Accuracy of CV: %s\" % acc_cv_lr)\nprint(\"Execution time: %s\" % lr_time)","12066536":"def feature_plot(imp):\n    global X\n    fimp = pd.DataFrame({'Feature': X.columns, 'Importance' : np.round(imp)})\n    fimp =fimp.sort_values(by='Importance', ascending=False)\n    plt.figure(figsize=(10,10))\n    plt.plot(fimp['Feature'], fimp['Importance'])\n    plt.xticks(rotation=90);","09295280":"feature_plot(lr.coef_[0])","fd571bf0":"fimp_lr = pd.DataFrame({'Feature': X.columns, 'Importance' : np.round(lr.coef_[0])})\nfimp_lr =fimp_lr.sort_values(by='Importance', ascending=False)\nfimp_lr","9c7fb386":"from sklearn.tree import DecisionTreeClassifier\nstart_time = time.time()\npred_now, acc_dt, acc_cv_dt, dt = fit_algo(DecisionTreeClassifier(random_state = 1)\n                                        , X, Y, 10)\n\ndt_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_dt)\nprint(\"Accuracy of CV: %s\" % acc_cv_dt)\nprint(\"Execution time: %s\" % dt_time)","0c06a67f":"from sklearn.ensemble import RandomForestClassifier\nstart_time = time.time()\npred_now, acc_rf, acc_cv_rf, rf = fit_algo(RandomForestClassifier(n_estimators = 100)\n                                        , X, Y, 10)\n\nrf_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_rf)\nprint(\"Accuracy of CV: %s\" % acc_cv_rf)\nprint(\"Execution time: %s\" % rf_time)\n","327ff8ee":"from sklearn.ensemble import RandomForestClassifier\nstart_time = time.time()\npred_now, acc_rf2, acc_cv_rf2, rf2 = fit_algo(RandomForestClassifier(n_estimators = 100, criterion='entropy')\n                                        , X, Y, 10)\n\nrf2_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_rf2)\nprint(\"Accuracy of CV: %s\" % acc_cv_rf2)\nprint(\"Execution time: %s\" % rf2_time)","232362ff":"from sklearn.neural_network import MLPClassifier\n\nstart_time = time.time()\npred_now, acc_nn, acc_cv_nn, nn = fit_algo(MLPClassifier(hidden_layer_sizes = (50,10), activation='relu', solver='adam')\n                                        , X, Y, 5)\n\nnn_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_nn)\nprint(\"Accuracy of CV: %s\" % acc_cv_nn)\nprint(\"Execution time: %s\" % nn_time)","d03344e0":"from sklearn.naive_bayes import GaussianNB\nstart_time = time.time()\n\npred_now, acc_gnb, acc_cv_gnb, gnb= fit_algo(GaussianNB()\n                                        ,X,Y,5)\n\ngnb_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_gnb)\nprint(\"Accuracy of CV: %s\" % acc_cv_gnb)\nprint(\"Execution time: %s\" % gnb_time)","ddae3adc":"from sklearn.ensemble import GradientBoostingClassifier\nstart_time = time.time()\n\npred_now, acc_gbt, acc_cv_gbt, gbt= fit_algo(GradientBoostingClassifier()\n                                        , X, Y, 10)\n\ngbt_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_gbt)\nprint(\"Accuracy of CV: %s\" % acc_cv_gbt)\nprint(\"Execution time: %s\" % gbt_time)","748e3bb1":"from sklearn.svm import LinearSVC\nstart_time = time.time()\n\npred_now, acc_svc, acc_cv_svc, svc= fit_algo(LinearSVC()\n                                        ,X,Y,10)\n\nsvc_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_svc)\nprint(\"Accuracy of CV: %s\" % acc_cv_svc)\nprint(\"Execution time: %s\" % svc_time)","350acbc5":"algo_name = ['Log. Reg.', 'Decision Tree', 'RandomForest Gini', 'RandomForest IG', 'Neural Network', 'Gaussian NB', 'GBC', 'SVM']\nacc_df = pd.DataFrame({'Algorithm' : algo_name, 'Accuracy %' : [acc_cv_lr, acc_cv_dt, acc_cv_rf, acc_cv_rf2, acc_cv_nn, acc_cv_gnb, acc_cv_gbt, acc_cv_svc] })\nacc_df = acc_df.sort_values(by='Accuracy %', ascending = False)\nacc_df = acc_df.reset_index(drop=True)\nacc_df","3803610d":"fimp_rf = pd.DataFrame({'Feature' : X.columns, 'Importance' : (gbt.feature_importances_*100).astype(float)})\nfimp_rf = fimp_rf.sort_values(by='Importance', ascending=False)\nfimp_rf","3504238f":"feature_plot(gbt.feature_importances_*100)","896cef0b":"filename = 'gbt_covid_icu.sav'\npickle.dump(rf2, open(filename, 'wb'))","17220a97":"# Random Forest (IG)","7e762566":"# Gaussian Naive Bayes","3b00616f":"**Note: I am just a beginner, doing a really simple EDA, would appreciate your input so I can improve!**","dd651fd8":"# Machine Learning Models","2107cc7c":"# Gradient Boosting","775e07ae":"#### Here I will not split the data to a test set, I will just keep the training to do cross validation.\n#### Since future data will be coming from the same distribution, we will not need a test-set, rather use all the available data.","ca42dd64":"# Logistic Regression","597859d3":"# Neural Networks","982a39c1":"# Random Forest","95762c57":"# Decision Trees","488cada3":"# SVM","8c7fdb1b":"# Split Data into X and Y","c7077d1f":"# Cross-validation Accuracy Comparison"}}