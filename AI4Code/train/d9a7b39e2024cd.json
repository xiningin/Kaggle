{"cell_type":{"faa22d54":"code","c95091a8":"code","6a209811":"code","96f3119f":"code","daad6c00":"code","d7ce9d5a":"code","3e8aabc0":"code","da315c7a":"code","1e970004":"code","a7223482":"code","8ebf09cb":"code","9515a6d6":"code","2aca176c":"code","f8e43881":"code","495ccf5f":"code","d6f48d8d":"code","9a1cf596":"code","aeb934c3":"code","d528215c":"code","8b5173d4":"code","ca6b126f":"markdown","f8c78920":"markdown","1198ed2b":"markdown","61b5a2a7":"markdown","f0a733f3":"markdown","9b78c920":"markdown","5927d808":"markdown","2bee3534":"markdown","cc21320c":"markdown","130e7d3f":"markdown","1c85a7f7":"markdown","9c39481e":"markdown"},"source":{"faa22d54":"import numpy as np\nimport pandas as pd\nimport time\nimport os\nimport matplotlib.pyplot as plt","c95091a8":"class Config:\n    input_path = \"..\/input\/tabular-playground-series-jan-2022\"\n    train_path = os.path.join(input_path, \"train.csv\")\n    test_path = os.path.join(input_path, \"test.csv\")\n    n_folds = 5\n    submission_path = os.path.join(input_path, \"sample_submission.csv\")\nconfig = Config()","6a209811":"train = pd.read_csv(config.train_path)\ntrain.head()","96f3119f":"test = pd.read_csv(config.test_path)\ntest.head()","daad6c00":"submission = pd.read_csv(config.submission_path)\nsubmission.head()","d7ce9d5a":"def visualize(df, column):\n    df[column].value_counts().plot(kind=\"bar\")\n    plt.title(\"Distribution of %s\"%(column))\n    plt.show()\n    df.groupby(column)[\"num_sold\"].sum().plot(kind=\"bar\")\n    plt.title(\"Total Sale Data in different %s\"%(column))\n    plt.show()\n    df.groupby(column)[\"num_sold\"].mean().plot(kind=\"bar\")\n    plt.title(\"Average Sale Data in different %s\"%(column))\n    plt.show()","3e8aabc0":"for column in [\"country\", \"product\", \"store\"]:\n    visualize(train, column)","da315c7a":"def day_of_year(date):\n    daysInMonth = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n    year = 0\n    month = 0\n    day = 0\n    i = 0\n    value = 0\n    for c in date:\n        value = ord(c) - 48\n        if value >= 0:\n            if i == 0:\n                year = year * 10 + value\n            elif i == 1:\n                month = month * 10 + value\n            else:\n                day = day * 10 + value\n        else:\n            i += 1\n    num_days = day + daysInMonth[month - 1]\n    is_leap = year % 400 == 0 if year % 100 == 0 else year % 4 == 0\n    if is_leap and month > 2:\n        num_days += 1\n    return num_days\n\ndef add_datetime_features(df):\n    new_df = df.copy()\n    years = []\n    months = []\n    days = []\n    weekdays = []\n    weekends = []\n    seasons = []\n    day_of_years = []\n    for item in df[\"date\"]:\n        dt = time.strptime(item, '%Y-%m-%d')\n        is_weekend = dt.tm_wday >= 5\n        season = (dt.tm_mon - 3) \/\/ 3 % 4\n        years.append(dt.tm_year)\n        months.append(dt.tm_mon)\n        days.append(dt.tm_mday)\n        weekdays.append(dt.tm_wday)\n        weekends.append(is_weekend)\n        seasons.append(season)\n        day_of_years.append(day_of_year(item))\n    new_df[\"year\"] = years\n    new_df[\"month\"] = months\n    new_df[\"day\"] = days\n    new_df[\"weekday\"] = weekdays\n    new_df[\"weekend\"] = weekends\n    new_df[\"season\"] = seasons\n    new_df[\"day_of_year\"] = day_of_years\n    new_df[\"end_of_year\"] = new_df[\"day_of_year\"] >= 350\n    new_df[\"end_of_year\"] = new_df[\"end_of_year\"].astype(int)\n    new_df.pop(\"date\")\n    return new_df","1e970004":"train_df = add_datetime_features(train)\ntrain_df.head()","a7223482":"test_df = add_datetime_features(test)\ntest_df.head()","8ebf09cb":"train_df.pop(\"row_id\")\ntest_df.pop(\"row_id\");","9515a6d6":"for column in [\"year\", \"month\", \"day\", \"weekday\", \"season\", \"weekend\", \"end_of_year\"]:\n    visualize(train_df, column)","2aca176c":"data = pd.concat([train_df, test_df])\ncategorical_columns = ['country', 'store', 'product', 'year', \"month\", 'weekday', 'season']\nfor column in categorical_columns:\n    item = pd.get_dummies(data[column])\n    item.columns = [\"_\".join([column, str(item)]) for item in item.columns]\n    data = pd.concat([data, item], axis=1)\n    data.pop(column)\ntrain_df = data[0:len(train_df)]\ntest_df = data[len(train_df):]\ntest_df.pop(\"num_sold\");","f8e43881":"corr = train_df.corr()\ncorr","495ccf5f":"corr[\"num_sold\"].sort_values(key=lambda item: abs(item), ascending=False)[:30]","d6f48d8d":"import seaborn as sns\ncorrelated_features = list(corr[corr[\"num_sold\"].abs() > 0.08].index)\nplt.figure(figsize=(15, 15))\nsns.heatmap(train_df[correlated_features].corr(), annot=True)","9a1cf596":"def smape(y_true, y_pred):\n    return 2.0 * np.mean(np.abs(y_pred - y_true) \/ (np.abs(y_pred) + np.abs(y_true))) * 100\n\ndef inference(models, X):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(X)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","aeb934c3":"from catboost import CatBoostRegressor\nfrom sklearn.model_selection import TimeSeriesSplit, KFold\nkfold =TimeSeriesSplit()\n#kfold =KFold(config.n_folds, shuffle=True, random_state=42)\ncats = []\nscores = []\nbest_score = 100\nbest_fold = 0\nworst_score = 0\nworst_fold = 0 \nfor fold, (train_indices, valid_indices) in enumerate(kfold.split(train_df)):\n    print(\"Fold %d:\"%(fold))\n    X_train = train_df.iloc[train_indices]\n    y_train = X_train.pop(\"num_sold\")\n    X_val = train_df.iloc[valid_indices]\n    y_val = X_val.pop(\"num_sold\")\n    params = {\n        'n_estimators': 10000, \n        #'od_wait': 1000, \n        'learning_rate': 0.03, \n        'depth': 7, \n        #'l2_leaf_reg': 5,\n        'verbose' : 1000,\n        \"eval_metric\": \"SMAPE\",\n        \"objective\": \"RMSE\"\n    }\n    cat = CatBoostRegressor(**params)\n    cat.fit(X_train, y_train, eval_set=(X_val, y_val))\n    cats.append(cat)\n    y_pred = cat.predict(X_val)\n    score = smape(y_val, y_pred)\n    scores.append(score)\n    if score < best_score:\n        best_score = score\n        best_fold = fold\n    if score > worst_score:\n        worst_score = score\n        worst_fold = fold\nprint(\"Average SMAPE: %.2f\"%(np.mean(scores)))","d528215c":"models = []\nfor fold in range(len(cats)):\n    models.append(cats[fold])","8b5173d4":"y_pred = inference(models, test_df)\nsubmission[\"num_sold\"] = y_pred\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","ca6b126f":"## Modeling","f8c78920":"## EDA & Preprocessing","1198ed2b":"### Feature Engineering for datetime","61b5a2a7":"As we can see that the most sinificant feature are product, store, country, whether it is weekend, weekday, seasons.","f0a733f3":"## Import datasets","9b78c920":"### More EDA\nAs we can see that Sale Data is increasing with year, but it is greater in end of month, end of week and Spring and Winter. It has strong cyclicity.","5927d808":"### Feature Correlation","2bee3534":"## Submission","cc21320c":"### Drop Id columns\n","130e7d3f":"# TPS-01-22 with Catboost\n\n## Overview\nIn this notebook I will build a Catboost Model for [Tabular Playground Series - Jan 2022 Competition](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022). Before Modeling, I will also perform some Exploratory data analysis and feature engineering to find insights.","1c85a7f7":"### Handle Categorical Features","9c39481e":"For different countries, Norway has the highest Sale Data; For different products, Kaggle Hat has the highest Sale Data; For different stores, KaggleRama has the highest Sale Data."}}