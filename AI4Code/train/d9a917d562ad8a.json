{"cell_type":{"5f0a44da":"code","4ddeaaed":"code","f796bb2f":"code","c6d3bc76":"code","75eae7c2":"code","40320a68":"code","2e43b19b":"code","4ff44865":"code","4f4d0167":"code","7b040d49":"code","0312c921":"code","e4d2e25f":"code","c3d32997":"code","dca04832":"code","6f7e8534":"code","233c732a":"code","b0181369":"code","b20c6da5":"code","afa4e6d1":"code","e16e86e5":"code","9af0eb92":"code","3321f929":"code","e1546158":"code","b42aae59":"code","0f6a8d48":"code","d89c4430":"code","3a9181b2":"code","2e7612a2":"code","465c090e":"code","b1b9a199":"code","8f791e0f":"code","b24c3450":"code","6937d843":"code","d21127d7":"code","f6eaf922":"markdown","ebafe85a":"markdown"},"source":{"5f0a44da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4ddeaaed":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom matplotlib.ticker import FormatStrFormatter\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nfrom sklearn.tree import DecisionTreeClassifier ,export_graphviz \nimport graphviz\nfrom IPython.display import Image  # To plot decision tree.\nfrom sklearn.externals.six import StringIO","f796bb2f":"df = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndf.head()","c6d3bc76":"df.info()","75eae7c2":"display(np.round(df.describe()))","40320a68":"df.groupby('quality').count()","2e43b19b":"null_columns=df.columns[df.isnull().any()]\nprint(null_columns)","4ff44865":"# Finding the correlation bewteen the Features.\nplt.figure(figsize=(10,5))\nheatmap = sns.heatmap(df.corr(), annot=True, fmt=\".1f\", cmap=\"Reds\")\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=12)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=12)\nplt.show()","4f4d0167":"#relationship between the some of features\ncols_sns = ['residual sugar', 'chlorides', 'density', 'pH', 'alcohol', 'quality']\nsns.set(style=\"ticks\")\nsns.pairplot(df[cols_sns],hue=\"quality\")","7b040d49":"#BoxPlot for different features.\nfeatures = [\"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\", \"chlorides\",\n            \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\", \"pH\", \"sulphates\", \"alcohol\"]\n\nfig = plt.figure(figsize=(16,8))\nfor i in range(len(features)):\n    ax1 = fig.add_subplot(3, 4, i+1)\n    ax1.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n    ax1.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n    sns.boxplot(x='quality', y=features[i], data=df,palette=\"Set3\")\n    i = i + 1\nplt.subplots_adjust(hspace = 0.5)\nplt.subplots_adjust(wspace = 0.5)\nplt.show()","0312c921":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x='quality', y='sulphates',data=df)","e4d2e25f":"from sklearn.preprocessing import LabelEncoder\nbins = (2, 5.5, 8)\ngroup_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)\n\nlabel_quality = LabelEncoder()\ndf['quality'] = label_quality.fit_transform(df['quality'])\ndf['quality'].value_counts()","c3d32997":"X = df.drop('quality', axis=1)\ny = df['quality']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.2)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","dca04832":"print(\"X_train {0} , X_test {1} \" .format(X_train.shape , X_test.shape))","6f7e8534":"#------------------------######### LogisticRegression #######-----------------\nlrg_classifier = LogisticRegression(solver='newton-cg',tol= 0.0001,C= 0.5,)\nlrg_classifier.fit(X_train, y_train.ravel())\n\ncv_lr = cross_val_score(estimator = lrg_classifier, X = X_train, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", cv_lr.mean())\n\nlt_y_pred_train = lrg_classifier.predict(X_train)\naccuracy_lr_train = accuracy_score(y_train, lt_y_pred_train)\nprint(\"Training set accuracy for Logistic Regression: \", accuracy_lr_train)\n\ny_pred_lr_test = lrg_classifier.predict(X_test)\naccuracy_lr_test = accuracy_score(y_test, y_pred_lr_test)\nprint(\"Test set accuracy for Logistic Regression: \", accuracy_lr_test)","233c732a":"lrg_classifier.fit(X_train, y_train)\nlr_y_pred_test = lrg_classifier.predict(X_test)\n\nprint(classification_report(y_test, lr_y_pred_test))","b0181369":"confusion_matrix(y_test, lr_y_pred_test)\n\ntp_lr = confusion_matrix(y_test, lr_y_pred_test)[0,0]\nfp_lr = confusion_matrix(y_test, lr_y_pred_test)[0,1]\ntn_lr = confusion_matrix(y_test, lr_y_pred_test)[1,1]\nfn_lr = confusion_matrix(y_test, lr_y_pred_test)[1,0]","b20c6da5":"#------------------------#########  KNeighbors #######-----------------\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_classifier = KNeighborsClassifier(leaf_size = 1, metric = 'minkowski', n_neighbors = 28, weights = 'distance')\nknn_classifier.fit(X_train, y_train.ravel())","afa4e6d1":"# Predicting Cross Validation Score\nknn_cv = cross_val_score(estimator = knn_classifier, X = X_train, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", knn_cv.mean())\n\nknn_y_pred_train = knn_classifier.predict(X_train)\nknn_accuracy_train = accuracy_score(y_train, knn_y_pred_train)\nprint(\"Training set accuracy for KNN: \", knn_accuracy_train)\n\nknn_y_pred_test = knn_classifier.predict(X_test)\nknn_accuracy_test = accuracy_score(y_test, knn_y_pred_test)\nprint(\"Test set accuracy for KNN: \", knn_accuracy_test)","e16e86e5":"confusion_matrix(y_test, knn_y_pred_test)\n\ntp_knn = confusion_matrix(y_test, knn_y_pred_test)[0,0]\nfp_knn = confusion_matrix(y_test, knn_y_pred_test)[0,1]\ntn_knn = confusion_matrix(y_test, knn_y_pred_test)[1,1]\nfn_knn = confusion_matrix(y_test, knn_y_pred_test)[1,0]","9af0eb92":"#------------------------#########  SVC #######-----------------\nfrom sklearn.svm import SVC\nsvm_linear_classifier = SVC(kernel = 'linear')\nsvm_linear_classifier.fit(X_train, y_train.ravel())","3321f929":"svm_linear_cv = cross_val_score(estimator = svm_linear_classifier, X = X_train, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", svm_linear_cv.mean())\n\nsvm_linear_train_y_pred = svm_linear_classifier.predict(X_train)\n\nsvm_linear_accuracy_train = accuracy_score(y_train, svm_linear_train_y_pred)\nprint(\"Training set accuracy for SVC: \", svm_linear_accuracy_train)\n\nsvm_linear_y_pred_test = svm_linear_classifier.predict(X_test)\nsvm_linear_accuracy_test = accuracy_score(y_test, svm_linear_y_pred_test)\nprint(\"Test set accuracy for SVC: \", svm_linear_accuracy_test)","e1546158":"confusion_matrix(y_test, svm_linear_y_pred_test)\n\ntp_svm_linear = confusion_matrix(y_test, svm_linear_y_pred_test)[0,0]\nfp_svm_linear = confusion_matrix(y_test, svm_linear_y_pred_test)[0,1]\ntn_svm_linear = confusion_matrix(y_test, svm_linear_y_pred_test)[1,1]\nfn_svm_linear = confusion_matrix(y_test, svm_linear_y_pred_test)[1,0]","b42aae59":"#------------------------#########  DecisionTree Classifier #######-----------------\n\ndt_classifier = DecisionTreeClassifier(criterion = 'gini', max_features=6, max_leaf_nodes=400, random_state = 33, max_depth=4)\ndt_classifier.fit(X_train, y_train.ravel())","0f6a8d48":"dt_cv = cross_val_score(estimator = dt_classifier, X = X_train, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", dt_cv.mean())\n\ndt_y_pred_train = dt_classifier.predict(X_train)\ndt_accuracy_train = accuracy_score(y_train, dt_y_pred_train)\nprint(\"Training set accuracy for DecisionTree: \", dt_accuracy_train)\n\ndt_y_pred_test = dt_classifier.predict(X_test)\ndt_accuracy_test = accuracy_score(y_test, dt_y_pred_test)\nprint(\"Test set accuracy for DecisionTree: \", dt_accuracy_test)","d89c4430":"confusion_matrix(y_test, dt_y_pred_test)\n\ntp_dt = confusion_matrix(y_test, dt_y_pred_test)[0,0]\nfp_dt = confusion_matrix(y_test, dt_y_pred_test)[0,1]\ntn_dt = confusion_matrix(y_test, dt_y_pred_test)[1,1]\nfn_dt = confusion_matrix(y_test, dt_y_pred_test)[1,0]","3a9181b2":"data = export_graphviz(dt_classifier,out_file=None,feature_names=list(X.columns.values),class_names=None,   \n                         filled=True, rounded=True,  \n                         special_characters=True,proportion=True)\n\ngraph = graphviz.Source(data)\ngraph","2e7612a2":"#------------------------#########  RandomForest Classifier #######-----------------\nfrom sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(criterion = 'entropy', max_features = 4, n_estimators = 800, random_state=33)\nrf_classifier.fit(X_train, y_train.ravel())","465c090e":"# Predicting Cross Validation Score\nrf_cv = cross_val_score(estimator = rf_classifier, X = X_train, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", rf_cv.mean())\n\nrf_y_pred_train = rf_classifier.predict(X_train)\nrf_accuracy_train = accuracy_score(y_train, rf_y_pred_train)\nprint(\"Training set: \", rf_accuracy_train)\n\nrf_y_pred_test = rf_classifier.predict(X_test)\nrf_accuracy_test = accuracy_score(y_test, rf_y_pred_test)\nprint(\"Test set: \", rf_accuracy_test)","b1b9a199":"#..--------Important Features of Random Forest\nfeature_importances = pd.DataFrame(rf_classifier.feature_importances_,\n                                   index = X.columns,columns=['importance']).sort_values('importance', ascending=False)\nprint(feature_importances)","8f791e0f":"importance = pd.DataFrame({'Importance': rf_classifier.feature_importances_}, index=X.columns)\nimportance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', color='r')\nplt.xlabel('Variable importance')\nplt.legend(loc='lower right')\nplt.legend()\nplt.show()","b24c3450":"confusion_matrix(y_test, rf_y_pred_test)\n\ntp_rf = confusion_matrix(y_test, rf_y_pred_test)[0,0]\nfp_rf = confusion_matrix(y_test, rf_y_pred_test)[0,1]\ntn_rf = confusion_matrix(y_test, rf_y_pred_test)[1,1]\nfn_rf = confusion_matrix(y_test, rf_y_pred_test)[1,0]","6937d843":"#------------\n###-----------Comparsion between the different models of Accuracy and Cross Validation.\n\nmodels = [('Logistic Regression', tp_lr, fp_lr, tn_lr, fn_lr, accuracy_lr_train, accuracy_lr_test, cv_lr.mean()),\n          ('K-Nearest Neighbors (KNN)', tp_knn, fp_knn, tn_knn, fn_knn, knn_accuracy_train, knn_accuracy_test, knn_cv.mean()),\n          ('SVM', tp_svm_linear, fp_svm_linear, tn_svm_linear, fn_svm_linear, svm_linear_accuracy_train, svm_linear_accuracy_test, svm_linear_cv.mean()),\n          ('Decision Tree Classification', tp_dt, fp_dt, tn_dt, fn_dt, dt_accuracy_train, dt_accuracy_test, dt_cv.mean()),\n          ('Random Forest Tree Classification', tp_rf, fp_rf, tn_rf, fn_rf, rf_accuracy_train, rf_accuracy_test, rf_cv.mean())\n         ]\n\npredict = pd.DataFrame(data = models, columns=['Model', 'True Positive', 'False Positive', 'True Negative','False Negative', 'Accuracy(training)', 'Accuracy(test)',\n                                               'Cross-Validation'])\npredict","d21127d7":"f, axes = plt.subplots(2,1, figsize=(14,10))\npredict.sort_values(by=['Accuracy(training)'], ascending=False, inplace=True)\nsns.barplot(x='Accuracy(training)', y='Model', data = predict, palette='Blues_d', ax = axes[0])\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxes[0].set_xlabel('Accuracy (Training)', size=16)\naxes[0].set_ylabel('Model')\naxes[0].set_xlim(0,1.0)\naxes[0].set_xticks(np.arange(0, 1.1, 0.1))\npredict.sort_values(by=['Accuracy(test)'], ascending=False, inplace=True)\nsns.barplot(x='Accuracy(test)', y='Model', data = predict, palette='Reds_d', ax = axes[1])\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxes[1].set_xlabel('Accuracy (Test)', size=16)\naxes[1].set_ylabel('Model')\naxes[1].set_xlim(0,1.0)\naxes[1].set_xticks(np.arange(0, 1.1, 0.1))\nplt.show()","f6eaf922":"After comparing all the models, we can observe that RF classifier has produced better results..\n\nPlease leave in comments in case of any questions, concerns, and feedback! Thank you.","ebafe85a":"In this notebook,I have done some Exploratory Data Analysis(EDA) on the data and also, I used different classifier models to predict the quality of the wine.\n1.\tLogistic Regression\n2.\tKNeighborsClassifier\n3.\tSVC\n4.\tDecisionTree Classifier\n5.\tRandomForest Classifier\nAnd also, I used cross validation evaluation technique to optimize the model performance.\n\n"}}