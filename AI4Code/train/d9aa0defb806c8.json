{"cell_type":{"ac42ddf6":"code","4bbea716":"code","386d06a3":"code","87df7488":"code","05876f8e":"code","158fb209":"code","57bf11c4":"code","465b2e31":"code","37a2c28a":"code","71eefd67":"code","7320670b":"code","659c3986":"code","bfb6b785":"code","dad35807":"code","85536ffb":"code","79cd99f3":"code","37e350d6":"code","476bfb9f":"code","f49f1830":"code","99aa266d":"code","d9c770f9":"code","2fb78a7d":"code","0d149e1d":"code","a3121247":"code","583d7494":"code","4715f8a1":"code","bd64aed2":"code","a9d2a2b1":"code","f3f65d75":"code","57f4c0c5":"code","102fb90b":"code","b919fe1d":"code","6c128770":"code","562d472b":"code","f0e8b910":"code","02579693":"code","fbc4368a":"code","6185da66":"code","a0ae59a1":"code","105db4c6":"code","dff6a8a4":"markdown","34a983d8":"markdown","9925bd40":"markdown","aec82944":"markdown","37392650":"markdown","16ca9d43":"markdown","7476a2ab":"markdown","45879db9":"markdown","246daf0b":"markdown","af4fba82":"markdown","d31c6b78":"markdown","acb05d4d":"markdown","6d0a6810":"markdown"},"source":{"ac42ddf6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn as skl\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4bbea716":"df = pd.read_csv(\"\/kaggle\/input\/parkinsons-data-set\/parkinsons.data\")\ndf.head()","386d06a3":"df.info()","87df7488":"df.shape","05876f8e":"df.isna().sum()","158fb209":"print(df.describe().T.shape)\ndf.describe().T","57bf11c4":"df.drop('name',axis=1,inplace=True)","465b2e31":"# sns.pairplot(df)","37a2c28a":"\ndef plotDistribution(df,rows,columns,plot):\n\n    fig, axs = plt.subplots(rows, columns,figsize=(40, 20))\n\n    ctr = 0\n    for i in range(rows):\n        for j in range(columns):\n            try:\n              #print(ctr)\n              plot(df.iloc[:,ctr],ax=axs[i, j])\n            except Exception as ex:\n               print('Exception: ', ex)\n               print(\"Column index not found:\",ctr)\n            ctr = ctr +1\n\n\nplotDistribution(df,4,6,sns.distplot)\n\nplotDistribution(df,4,6,sns.boxplot)\n","71eefd67":"corr = df.corr()\ncorr","7320670b":"\nfig, axs = plt.subplots(figsize=(20,20))         # Sample figsize in inches\nsns.heatmap(corr, annot=True, linewidths=.8, ax=axs)\n\n    ","659c3986":"df.columns","bfb6b785":"df_dropped = df.drop(['MDVP:Jitter(Abs)',\n 'MDVP:RAP',\n 'MDVP:PPQ',\n 'Jitter:DDP',\n'MDVP:Shimmer(dB)',\n 'Shimmer:APQ3',\n 'Shimmer:APQ5',\n\n 'MDVP:APQ',\n 'Shimmer:DDA',\n'spread1','NHR'],axis=1)","dad35807":"df_dropped.columns","85536ffb":"fig, axs = plt.subplots(figsize=(10,10))         # Sample figsize in inches\nsns.heatmap(df_dropped.corr(), annot=True, linewidths=.8, ax=axs)","79cd99f3":"df_dropped['status'].value_counts()","37e350d6":"df_dropped_min = df_dropped[df_dropped[\"status\"] == 0] \ndf_dropped_maj = df_dropped[df_dropped[\"status\"] == 1]\ndf_dropped_min_upsampled = skl.utils.resample(df_dropped_min,n_samples=147,random_state=1);\n\ndf_dropped_upsampled = pd.concat([df_dropped_maj,df_dropped_min_upsampled])\n\ndf_dropped_upsampled[\"status\"].value_counts()","476bfb9f":"# Splitting the data into independent and dependent variables\n\ny = df_dropped[\"status\"]\nX = df_dropped.drop([\"status\"],axis=1)\n","f49f1830":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\n\nprint(y_test.shape)","99aa266d":"y_train.value_counts()","d9c770f9":"def getOulierCount(df):\n    \n    for columnName in df.columns:\n        if(columnName == 'status'):\n         continue\n        try:\n            featureValues = df[columnName]\n            q1 = featureValues.quantile(0.25)\n            q3 = featureValues.quantile(0.75)\n            iqr = q3-q1\n            upperlimit = q3+(1.5*iqr)\n            lowerlimit = q1-(1.5*iqr)\n            print(\"ColumnName:\",columnName)\n            \n            outlierCount = featureValues.loc[(featureValues < lowerlimit) | (featureValues > upperlimit)].count()\n\n            outlierPercentage = (outlierCount\/featureValues.count())*100\n            print(\"Outlier Percentage:\",outlierPercentage);\n            \n           \n        except Exception as ex:\n           \n            print(ex)","2fb78a7d":"getOulierCount(X_train)","0d149e1d":"def ReplaceOutliersWithMedian(df):\n    \n    for columnName in df.columns:\n        if(columnName == 'status'):\n         continue\n        try:\n            featureValues = df[columnName]\n            q1 = featureValues.quantile(0.25)\n            q3 = featureValues.quantile(0.75)\n            iqr = q3-q1\n            upperlimit = q3+(1.5*iqr)\n            lowerlimit = q1-(1.5*iqr)\n            #print(\"ColumnName:\",columnName)\n            median = featureValues.median()\n            featureValues.loc[(featureValues < lowerlimit) | (featureValues > upperlimit)] = median\n            #featureValues.fillna(median,inplace=True)\n            \n            outlierCount = featureValues.loc[(featureValues < lowerlimit) | (featureValues > upperlimit)].count()\n            outlierPercentage = (outlierCount\/featureValues.count())*100\n            #print(\"Outlier Percentage:\",outlierPercentage);\n            \n           \n        except Exception as ex:\n           \n            print(ex)\n    return df","a3121247":"\n\nplotDistribution(X_train,2,6,sns.boxplot)","583d7494":"from sklearn.linear_model import LogisticRegression as logisticRegressor\nimport sklearn.metrics as skmetrics\nimport sklearn.metrics as skmetrics\nfrom  sklearn.neighbors import KNeighborsClassifier as knnClassifier\nfrom  sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier","4715f8a1":"def fit_and_predict(algorithm,hyperparameter, X_train,y_train,X_test,y_test,scale=True):\n\n     if(scale):\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_test  = scaler.fit_transform(X_test)\n  \n     if (hyperparameter != None):\n      model = GridSearchCV(algorithm,hyperparameter).fit(X_train,y_train)\n     else:\n      model = algorithm.fit(X_train,y_train)\n     y_pred = model.predict(X_test)\n     test_score = skmetrics.accuracy_score(y_test,y_pred)\n        \n     \n     accuracy = test_score*100\n     #print(\"Accuracy score on test data:\",accuracy)\n     return model,accuracy","bd64aed2":"\ndef switchData(dataType,algorithm,hyperparameter,X_train,y_train,X_test,y_test):\n\n\n\n    if(dataType == 'Scaled_Treated'):\n       return fit_and_predict(algorithm,hyperparameter,ReplaceOutliersWithMedian(X_train),y_train,X_test,y_test,scale=True)\n        \n    elif (dataType == 'Scaled_NotTreated'):\n       return fit_and_predict(algorithm,hyperparameter,X_train,y_train,X_test,y_test,scale=True)\n        \n    elif (dataType == 'NotScaled_Treated'):\n       return fit_and_predict(algorithm,hyperparameter,ReplaceOutliersWithMedian(X_train),y_train,X_test,y_test,scale=False)\n        \n    elif (dataType == 'NotScaled_NotTreated'):\n       return fit_and_predict(algorithm,hyperparameter,X_train,y_train,X_test,y_test,scale=False)\n     \n\n    ","a9d2a2b1":"dataTypes = pd.Series(data=['Scaled_Treated','Scaled_NotTreated','NotScaled_Treated','NotScaled_NotTreated'])\n\naccuracy_df = pd.DataFrame(columns=['dataType','Logistic','KNN','NB','SVM','Stacking','Bagging','AdaBoost','GradBoost','RandomForest'])\n                           \n                           \n                           \n                           \n                           \n                           \naccuracy_df['dataType'] = dataTypes\n\n\ndef makeSeries(algorithm,hyperparameter,X_train,y_train,X_test,y_test):\n        acc = []\n        for dtype in dataTypes:\n            model,accuracy = switchData(dtype,algorithm,hyperparameter,X_train,y_train,X_test,y_test)\n            acc.append(accuracy)\n        return pd.Series(data=acc)\n            \n \n  \n      ","f3f65d75":"algorithm = logisticRegressor(max_iter=10000,random_state=1);\n\nhyperparameter = {'solver' : ['newton-cg', 'lbfgs','liblinear', 'sag', 'saga']}\n\n#get_accuracy(algorithm,X_train,y_train)\naccuracy_df['Logistic'] = makeSeries(algorithm,hyperparameter, X_train,y_train,X_test,y_test)\n\n\n","57f4c0c5":"algorithm = knnClassifier();\n\nhyperparameter = {'algorithm' : ['ball_tree', 'kd_tree', 'brute'],\n                   'n_neighbors': np.arange(5 , 20 , 2),\n                   'metric': ['euclidean','manhattan']}\n\n#get_accuracy(algorithm,X_train,y_train)\n\n#knnModel ,y_pred_knn = fit_and_predict(algorithm,hyperparameter,X_train,y_train,X_test,y_test,scale=False)\n#knnModel_scaled ,y_pred_knn_scaled = fit_and_predict(algorithm,hyperparameter,X_train,y_train,X_test,y_test,scale=True)\n\naccuracy_df['KNN'] = makeSeries(algorithm,hyperparameter, X_train,y_train,X_test,y_test)\n","102fb90b":"algorithm =GaussianNB() \n# NBModel,y_pred_NB = fit_and_predict(GaussianNB(),None,X_train,y_train,X_test,y_test,scale=False)\n# NBModel_scaled,y_pred_NB_scaled = fit_and_predict(GaussianNB(),None,X_train,y_train,X_test,y_test,scale=True)\n\naccuracy_df['NB'] = makeSeries(algorithm,None, X_train,y_train,X_test,y_test)\n","b919fe1d":"algorithm = SVC();\n\nhyperparameter = {'C': [0.01,0.1,1,10,100],\n                  'gamma':[0.01,0.1,1,10,100]\n                 }\n                   \n\n#get_accuracy(algorithm,X_train,y_train)\n\n#SVCModel ,y_pred_svc = fit_and_predict(algorithm,hyperparameter,X_train,y_train,X_test,y_test,scale=False)\n#SVCModel_scaled ,y_pred_svc_scaled= fit_and_predict(algorithm,hyperparameter,X_train,y_train,X_test,y_test,scale=True)\n\naccuracy_df['SVM'] = makeSeries(algorithm,hyperparameter, X_train,y_train,X_test,y_test)\n\n\n","6c128770":"\n\nestimators = [\n    ('lr', logisticRegressor(max_iter=10000, random_state=1, solver='liblinear')),\n    ('knn',knnClassifier(metric='euclidean', n_neighbors=9)),\n    ('nb',SVC(C=100, gamma=0.01))\n ]\n\nfinal_estimator = logisticRegressor()\n\nalgorithm = StackingClassifier(estimators,final_estimator);\n\n\n\n#StackingModel ,y_pred_svc = fit_and_predict(algorithm,hyperparameter,X_train,y_train,X_test,y_test,scale=True)\n\naccuracy_df['Stacking'] = makeSeries(algorithm,None, X_train,y_train,X_test,y_test)\n#print(algorithm.fit(X_train,y_train).score(X_test,y_test))\n\n","562d472b":"algorithm = BaggingClassifier(random_state=1);\n\n#get_accuracy(algorithm,X_train,y_train)\n\n#baggingModel ,y_pred_bag = fit_and_predict(algorithm,None,X_train,y_train,X_test,y_test,scale=True)\n#SVCModel_scaled ,y_pred_svc_scaled= fit_and_predict(algorithm,hyperparameter,X_train,y_train,X_test,y_test,scale=True)\naccuracy_df['Bagging'] = makeSeries(algorithm,None, X_train,y_train,X_test,y_test)\n","f0e8b910":"algorithm = RandomForestClassifier(random_state=1);\n\n#get_accuracy(algorithm,X_train,y_train)\n\n#rForest ,y_pred_bag = fit_and_predict(algorithm,None,X_train,y_train,X_test,y_test,scale=True)\n#SVCModel_scaled ,y_pred_svc_scaled= fit_and_predict(algorithm,hyperparameter,X_train,y_train,X_test,y_test,scale=True)\naccuracy_df['RandomForest'] = makeSeries(algorithm,None, X_train,y_train,X_test,y_test)","02579693":"algorithm = AdaBoostClassifier(random_state=1);\n\n#get_accuracy(algorithm,X_train,y_train)\n\n#adaBoost ,y_pred_bag = fit_and_predict(algorithm,None,X_train,y_train,X_test,y_test,scale=True)\n#SVCModel_scaled ,y_pred_svc_scaled= fit_and_predict(algorithm,hyperparameter,X_train,y_train,X_test,y_test,scale=True)\naccuracy_df['AdaBoost'] = makeSeries(algorithm,None, X_train,y_train,X_test,y_test)","fbc4368a":"algorithm = GradientBoostingClassifier(random_state=1);\n\n#get_accuracy(algorithm,X_train,y_train)\n\n#gradBoost ,y_pred_bag = fit_and_predict(algorithm,None,X_train,y_train,X_test,y_test,scale=True)\n#SVCModel_scaled ,y_pred_svc_scaled= fit_and_predict(algorithm,hyperparameter,X_train,y_train,X_test,y_test,scale=True)\naccuracy_df['GradBoost'] = makeSeries(algorithm,None, X_train,y_train,X_test,y_test)","6185da66":"accuracy_df","a0ae59a1":"accuracy_df.max()","105db4c6":"#sns.pointplot(x=accuracy_df[\"dataType\"],y=accuracy_df[\"Logistic\"])\n\nfig, axs = plt.subplots(1, 1,figsize=(15, 8))\n\nfor column in accuracy_df.columns.drop(\"dataType\"):\n    axs.plot(accuracy_df[\"dataType\"],accuracy_df[column],label = column )\n\n\nplt.title(\"Accuracy Comparison\", fontsize = 20)\nplt.xlabel(\"Date Treatment Parameter\", fontsize = 15)\nplt.ylabel(\"Accuracy Percentage\", fontsize = 15)\n\naxs.legend(loc='upper right',bbox_to_anchor=(1.15,1))\nplt.show()\n\n","dff6a8a4":"Stacking Classifier\n","34a983d8":"Naive Bayes","9925bd40":"Outlier Treatment using IQR method.. Identifying the outlier counts first","aec82944":"Logistic Regression\n","37392650":"**","16ca9d43":"Replacing outliers with Median.","7476a2ab":"KNN Classifier\n","45879db9":"Bagging Classifier","246daf0b":"There are lot of correlated data.\nThe range of data in every attribute is totally different and arriving at a common scale would be a challenge.\nHence standardization should be applied\nMost of the attributes seem skewed to the right","af4fba82":"We see that most of the features are heavily skewed to the right showing the presence of lot of outliers.\nHence Outlier treatment should be done to achieve consistency in the data.","d31c6b78":"Name column is not needed. Hence dropping name column","acb05d4d":"Dropping Columns with string positive correlation\n\n'MDVP:Jitter(%)'\n\n 'MDVP:Jitter(Abs)'\n 'MDVP:RAP'\n 'MDVP:PPQ'\n 'Jitter:DDP'   - Positive correlation - Frequency measure\n\n\n'MDVP:Shimmer'\n 'MDVP:Shimmer(dB)'\n 'Shimmer:APQ3'\n 'Shimmer:APQ5'\n\n 'MDVP:APQ'\n 'Shimmer:DDA' - Positive Correlation - Amplitude measure\n \n PPE and Spread 1 are correlated , dropping spread 1\n NHR and Jitter % are correlated , dropping NHR as HNR will contribute to the noise factor","6d0a6810":"SVM"}}