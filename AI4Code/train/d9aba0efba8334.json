{"cell_type":{"6ba6197d":"code","88645350":"code","4cf76a59":"code","6b2ea264":"code","dfd032f9":"code","75b9ec02":"code","f9864a79":"code","c66c4df0":"code","dd1016bf":"code","7dd92fa4":"code","e279ce90":"code","e3dcfc60":"code","311a3b0c":"code","8f2b032d":"code","ed42d9c8":"code","be3044cf":"code","0824c27a":"code","dc7849af":"code","83fc5dd8":"code","b857e289":"code","cb97e7b8":"code","21693a68":"code","576f7d16":"code","1f4c473b":"code","aff59eef":"code","1e65a398":"code","19806550":"code","9daf34f3":"code","f97cee8e":"code","361ad489":"markdown","84a52350":"markdown","d8984d9b":"markdown","f8772faf":"markdown","e20f893b":"markdown","7be6c9e4":"markdown","57d72b0d":"markdown","b5bc3a50":"markdown","4c737e23":"markdown","5db77b64":"markdown","cad455f1":"markdown","445a00dc":"markdown","95b714b5":"markdown","fc9bd6d2":"markdown","e93596c5":"markdown","aea80ef1":"markdown","300673fa":"markdown","b23e75ec":"markdown","f2234f31":"markdown","57b7c643":"markdown","09c8a837":"markdown"},"source":{"6ba6197d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set()","88645350":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4cf76a59":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata.head(3)","6b2ea264":"data.shape","dfd032f9":"survived = sns.color_palette()[2]   # Green\ndied = sns.color_palette()[3]       # Red","75b9ec02":"data_vis = data.copy()\ndata_vis['Survived'] = data_vis['Survived'].replace({0:'Died', 1:'Survived'})","f9864a79":"sns.countplot(data=data_vis, x='Pclass', hue='Survived', palette={'Died':died, 'Survived':survived})\nplt.xticks([0,1,2],['Upper class','Middle class','Lower class'])\nplt.xlabel('Passenger Class')\nplt.title('Survivors\/Deaths in each passenger class')\nplt.show()","c66c4df0":"#sns.histplot(data=data_vis, x='Age', hue='Survived', multiple='layer', element='step', palette={'Died':died, 'Survived':survived}, hue_order=['Survived','Died'])\n\nplt.hist(data_vis[data_vis['Survived']=='Died']['Age'], histtype='stepfilled', color=died, alpha=0.6, label='Died', bins=np.linspace(0,80,20))\nplt.hist(data_vis[data_vis['Survived']=='Survived']['Age'], histtype='stepfilled', color=survived, alpha=0.6, label='Survived', bins=np.linspace(0,80,20))\nplt.title('Survivors\/Deaths\\' distribution')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.legend(title='Survived')\nplt.show()","dd1016bf":"sns.countplot(data=data_vis, x='Sex', hue='Survived', palette={'Died':died, 'Survived':survived})\nplt.title('Survivors\/Deaths\\' gender')\nplt.xlabel('Gender')\nplt.show()","7dd92fa4":"sns.countplot(data=data_vis, x='Embarked', hue='Survived', palette={'Died':died, 'Survived':survived})\nplt.xticks([0,1,2],['Southampton', 'Cherbourg',  'Queenstown'])\nplt.title('Survivors\/Deaths classified by embarked town')\nplt.xlabel('Embarked town')\nplt.show()","e279ce90":"data.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\ndata.head(3)","e3dcfc60":"missing = pd.DataFrame(data.isnull().sum()\/len(data), columns=['NaN'])\n\nmissing.style.background_gradient(cmap=sns.light_palette(\"red\", as_cmap=True))","311a3b0c":"data = data.drop(['Cabin'], axis=1)","8f2b032d":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean').fit(data[['Age']])\n\ndata_imputed = data.copy()\ndata_imputed[['Age']] = imputer.transform(data_imputed[['Age']])\n","ed42d9c8":"data_imputed.fillna(method='ffill', inplace=True)","be3044cf":"data_imputed.isnull().sum()","0824c27a":"data_imputed.head(3)","dc7849af":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\n\nX, y = data_imputed.drop('Survived',axis=1), data_imputed[['Survived']]\n\n\nohe = OneHotEncoder().fit(X.loc[:,['Sex','Embarked']])\n\nct = ColumnTransformer([\n    ('One Hot Encode', ohe, [1,6])\n], remainder='passthrough')\n\n\ndata_imputed_encoded = ct.fit_transform(X)","83fc5dd8":"X_train, X_test, y_train, y_test = train_test_split(data_imputed_encoded, y, test_size=0.2, stratify=y)","b857e289":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X_train)\n\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","cb97e7b8":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, classification_report, plot_roc_curve, roc_auc_score\n\nmodels = dict()\n\nmodels['Logistic Regression'] = LogisticRegression()\nmodels['Decision Tree'] = DecisionTreeClassifier()\nmodels['Random Forest'] = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           n_jobs=-1,) \nmodels['Gradient Boosting'] = GradientBoostingClassifier()\nmodels['Support Vector'] = SVC()\nmodels['Naive Bayes'] = GaussianNB()\n\n\ndef fit_models(models, X_train_scaled, y_train):\n    for x in models:\n        models[x].fit(X_train_scaled, y_train)\n        print(x+': fitted')\n\ndef get_report(models, X_test_scaled, y_test):\n    reports = dict()\n    for x in models:\n        model = models[x]\n        y_pred = model.predict(X_test_scaled)\n        reports[x] = [confusion_matrix(y_test, y_pred),classification_report(y_test, y_pred), roc_auc_score(y_test, y_pred)]\n        print(x+': reported')\n    return reports\n","21693a68":"fit_models(models, X_train_scaled, y_train)","576f7d16":"reports = get_report(models, X_test_scaled, y_test)","1f4c473b":"for each in reports:\n    cm, cr, roc = reports[each]\n    print('------------'+each+'------------')\n    print(roc)\n    print(cm)\n    print(cr)","aff59eef":"def PREPROCESS(data):\n    \n    # 1.1) Feature Selection: drop identity\n    data.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n    \n    # 1.2) Missing Value\n        # Drop too many missing\n    data.drop(['Cabin'], axis=1, inplace=True)\n        # Impute numerical feaure\n    data_imputed = data.copy()\n    data_imputed[['Age']] = imputer.transform(data_imputed[['Age']])\n        # Forward fill categorical features\n    data_imputed.fillna(method='ffill', inplace=True)\n    \n    # 1.3) Encode categorical features\n    data_imputed_encoded = ct.fit_transform(data_imputed)\n    \n    # 1.4) Scale the data\n    X_test_scaled = scaler.transform(data_imputed_encoded)\n    \n    return X_test_scaled\n    ","1e65a398":"TEST = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nindex = TEST['PassengerId']\nTEST.head(3)","19806550":"TEST_PREPROCESS = PREPROCESS(TEST)","9daf34f3":"test_pred = models['Support Vector'].predict(TEST_PREPROCESS)\n\nout = pd.DataFrame({'Survived':test_pred}, index=index)","f97cee8e":"out.to_csv('OUTPUT_2.csv')","361ad489":"There are a very obvious trend here. Male is much more likely to die than female. <br>\n","84a52350":"'Age' will be imputed with its mean.","d8984d9b":"## Feature description \n|Feature|Description|Value|\n|---|---|---|\n|survival|Survival |0 = No, 1 = Yes\n|pclass|Ticket class|1 = 1st, 2 = 2nd, 3 = 3rd\n|sex||\n|Age|Age in year|\n|sibsp|\t# of siblings \/ spouses aboard the Titanic\t|\n|parch|\t# of parents \/ children aboard the Titanic\t|\n|ticket|\tTicket number\t|\n|fare\tPassenger fare\t|\n|cabin\t|Cabin number\t|\n|embarked\t|Port of Embarkation|\tC = Cherbourg, Q = Queenstown, S = Southampton","f8772faf":"## 0.2) Looking into 'Age'","e20f893b":"# 1) Preprocessing","7be6c9e4":"We see that 'Cabin' is 77.1% missing. Thus, we manage to drop this feature.","57d72b0d":"## 0.1) Looking into 'passenger class'","b5bc3a50":"# 0) Let's explore our data","4c737e23":"From histogram above, we see that most passengers are 20-50 years old. <br>\nAlso, we notice that child passenger(0-20 years old) has more survived than died.  ","5db77b64":"## 1.3) Encoding categorical features**","cad455f1":"'Embarked' will be forwward filled.","445a00dc":"We can obviously see that passengers in 'Upper class' are more likely to survive than other classes, While the 'Lower class' passengers are died the most.","95b714b5":"### My guess by looking to the data, <br>\n'Upper class', 'Children', 'Female' passenger are more likely to survie.","fc9bd6d2":"## 1.2) Missing value","e93596c5":"## 0.4) Looking into 'embarked town'","aea80ef1":"## 1.1) Feature Selection","300673fa":"### Drop identity: 'PassengerID', 'Name', 'Ticket'","b23e75ec":"## 0.3) Looking into 'gender'","f2234f31":"## Declare the color for each target","57b7c643":"## 1.4) Scale the data","09c8a837":"# 2) Building the learning models"}}