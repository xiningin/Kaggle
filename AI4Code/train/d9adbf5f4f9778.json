{"cell_type":{"7ebdd7b8":"code","7ca09655":"code","6bf2fcc4":"code","158ade2e":"code","0beeb91a":"code","45a35746":"code","e05d02d0":"code","694fa2b0":"code","de380380":"code","c15bbd24":"code","97d6dad6":"code","0358b97e":"code","d55a3f0f":"code","57fb6619":"code","ffe0d936":"code","e541892f":"code","9394de59":"code","1a637916":"code","90996567":"code","cd708e29":"code","ee36d9b0":"code","f4279f07":"code","f381916a":"code","17961cbd":"code","77f29792":"code","2c12ff34":"code","49b8f34a":"code","a723f47c":"code","775cc0c1":"code","778a6a66":"code","34f06150":"code","859b9e3e":"code","fe3c209f":"code","7b6eacdb":"code","750e19ef":"code","0bd69934":"markdown","fbe6f098":"markdown","c8802fed":"markdown","d6d62acd":"markdown","9072e7a6":"markdown","048cb23b":"markdown","f3f277d5":"markdown","f9c62bf0":"markdown","7060539f":"markdown","50cff2f3":"markdown","4d6addc0":"markdown","917d0c8b":"markdown","9c893e85":"markdown","8883fa3d":"markdown","0efa8f9a":"markdown","b44aa44c":"markdown","cf5a83f3":"markdown","5da431f1":"markdown","b6f2d5fb":"markdown","31cddb83":"markdown","59b42a5a":"markdown","cc74b444":"markdown","68866114":"markdown","fa9faea4":"markdown","ece1f90b":"markdown","9a6b1813":"markdown","1c6ac967":"markdown","05c223e0":"markdown","b99b3214":"markdown"},"source":{"7ebdd7b8":"# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.core.display import HTML # permet d'afficher du code html dans jupyter","7ca09655":"from sklearn.model_selection import learning_curve\ndef plot_learning_curve(est, X_train, y_train) :\n    train_sizes, train_scores, test_scores = learning_curve(estimator=est, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10),\n                                                        cv=5,\n                                                        n_jobs=-1)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.figure(figsize=(8,10))\n    plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\n    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n    plt.plot(train_sizes, test_mean,color='green', linestyle='--',marker='s', markersize=5,label='validation accuracy')\n    plt.fill_between(train_sizes,test_mean + test_std,test_mean - test_std,alpha=0.15, color='green')\n    plt.grid(b='on')\n    plt.xlabel('Number of training samples')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.6, 1.0])\n    plt.show()","6bf2fcc4":"def plot_roc_curve(est,X_test,y_test) :\n    probas = est.predict_proba(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,probas[:, 1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.figure(figsize=(8,8))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')        # plus mauvaise courbe\n    plt.plot([0,0,1],[0,1,1],'g:')     # meilleure courbe\n    plt.xlim([-0.05,1.2])\n    plt.ylim([-0.05,1.2])\n    plt.ylabel('Taux de vrais positifs')\n    plt.xlabel('Taux de faux positifs')\n    plt.show","158ade2e":"df = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')","0beeb91a":"df.head()","45a35746":"labels = [\"T-shirt\/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\n          \"Sneaker\",\"Bag\",\"Ankle boot\"]","e05d02d0":"print(labels[df.label[0]])","694fa2b0":"#on s\u00e9pare X et y\nX = df.drop('label', axis = 1)\ny = df.label","de380380":"#conversion de X (format df) en tableau\nX_array = np.array(X)","c15bbd24":"X_array[5]","97d6dad6":"image = X_array[5].reshape(28,28)","0358b97e":"plt.imshow(image)\nprint(\"Type of cloth : \", labels[y[5]])","d55a3f0f":"plt.figure(figsize=(20,10))\nfor i in range (0, 50):\n    plt.subplot(5, 10, i+1)\n    plt.axis(\"off\")\n    image = X_array[i].reshape(28, 28)\n    plt.imshow(image, cmap=\"gray_r\")\n    plt.title(labels[y[i]])","57fb6619":"df.shape","ffe0d936":"from sklearn.utils import resample","e541892f":"data_sample = resample(df, n_samples = 5000)","9394de59":"data_sample.shape","1a637916":"X_sp = data_sample.drop('label', axis = 1)\ny_sp = data_sample.label","90996567":"plt.figure(figsize=(10,5))\nplt.subplot(1, 2, 1)\nplt.hist(y)\nplt.title(\"Distribution dataset original\")\nplt.subplot(1, 2, 2)\nplt.hist(y_sp)\nplt.title(\"Distribution \u00e9chantillon\")\nplt.show()","cd708e29":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_sp, y_sp, train_size = 0.8)","ee36d9b0":"from sklearn import neighbors\nknn = neighbors.KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)","f4279f07":"from sklearn import svm\nsvc = svm.SVC()\nsvc.fit(X_train, y_train)\nsvc.score(X_test, y_test)","f381916a":"from sklearn.model_selection import GridSearchCV","17961cbd":"# Set the parameters by cross-validation\n\"\"\"\nparameters = {'C': [1, 10, 100, 1000]}\n\n\nclf = GridSearchCV(svm.SVC(), parameters)\nclf.fit(X_train, y_train)\n\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(clf.best_params_)\nprint()\nprint(\"Best score:\")\nprint()\nprint(clf.best_score_)\n\"\"\"","77f29792":"from tensorflow import keras\n\nimg_rows, img_cols = 28, 28\nnum_classes = 10\n\ndef prep_data(raw):\n    raw = raw.to_numpy()\n    y = raw[:, 0]\n    out_y = keras.utils.to_categorical(y, num_classes)\n    \n    x = raw[:,1:]\n    num_images = raw.shape[0]\n    out_x = x.reshape(num_images, img_rows, img_cols, 1)\n    out_x = out_x \/ 255\n    return out_x, out_y","2c12ff34":"X, y = prep_data(df)","49b8f34a":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8)","a723f47c":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D","775cc0c1":"#define\nmodel = Sequential()\n\nmodel.add(Conv2D(12, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=(img_rows, img_cols, 1)))\n\nmodel.add(Conv2D(20, kernel_size=(3, 3), activation='relu'))\nmodel.add(Conv2D(40, kernel_size=(3, 3), activation='relu'))\nmodel.add(Conv2D(60, kernel_size=(3, 3), activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n\n#compile\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n#fit\nmodel.fit(X_train, y_train,\n          validation_data = (X_test, y_test),\n          batch_size = 32,\n          epochs = 10)","778a6a66":"# Test\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy : %.2f%%\" % (scores[1]*100))","34f06150":"model.save(\"ZalandoCNNModel.h5\")\nprint(\"The model has been saved!\")","859b9e3e":"preds = model.predict_classes(X_test)","fe3c209f":"#our y_test was categorical, so let's get it back to normal\ny_test_transfo = y_test.argmax(1)","7b6eacdb":"wrong_preds = []\nfor i in range(0, len(preds), 1):\n    if preds[i].any() != y_test_transfo[i].any():\n        wrong_preds.append(i)","750e19ef":"plt.figure(figsize=(25,30))\ni=1\nfor j in wrong_preds :\n    if (i<51) :\n        plt.subplot(10,5,i)\n        plt.axis('off')\n        plt.imshow(X_test[j][:,:,0], cmap=\"gray_r\")\n        pred_classe = preds[j].argmax(axis=-1)\n        plt.title('Prediction : %s \/ Label : %s' % (labels[preds[j]], labels[y_test_transfo[j]]))\n        i+=1","0bd69934":"Affichage des 50 premiers \u00e9l\u00e9ments","fbe6f098":"Comme sugg\u00e9r\u00e9 ci-avant, nous allons maintenant appliquer des m\u00e9thodes de Deep Learning au dataset Zalando, afin de tenter d'am\u00e9liorer le score obtenu avec le SVC. \n\nJe vais r\u00e9-utilser le r\u00e9seau neuronal convolutif de mon notebook **Basic Classifiers VS CNN**, lui-m\u00eame tir\u00e9 du tutoriel Kaggle sur le Deep Learning.","c8802fed":"Comparaison entre des techniques de Machine Learning basiques (classifiers K-nn et SVC) et des techniques de Deep Learning.","d6d62acd":"### Pour aller plus loin...\n\nIl serait int\u00e9ressant d'appliquer sur ce dataset des m\u00e9thodes de Deep Learning, afin de comparer les performances des deux techniques. Un travail que j'ai r\u00e9alis\u00e9 sur le dataset MNIST des chiffres : https:\/\/www.kaggle.com\/pauldubois\/basic-classifiers-vs-cnn. \n\nSans aucun doute, le Deep Learning offrira une pr\u00e9cision bien sup\u00e9rieure aux mod\u00e8les basiques de Machine Learning.","9072e7a6":"**Afficher les 50 premiers \u00e9l\u00e9ments du dataset avec leur label**  \n","048cb23b":"## Zalando Fashion MNIST","f3f277d5":"### Building the model","f9c62bf0":"Pour une phase d'entrainement plus rapide, on va prendre seulement un \u00e9chantillon de notre dataset.","7060539f":"### Entrainement de 2 mod\u00e8les : K-nn & SVC","50cff2f3":"## Deep Learning","4d6addc0":"La premi\u00e8re image du dataset est un pull :","917d0c8b":"## Machine Learning","9c893e85":"# Exercice : Zalando Fashion MNIST","8883fa3d":"Fonction pour tracer la courbe ROC :","0efa8f9a":"## Librairies et fonctions utiles","b44aa44c":"### Optimisation des hyperparam\u00e8tres","cf5a83f3":"### Erreurs de pr\u00e9diction\n\nIl peut \u00eatre int\u00e9ressant de voir quelles images n'ont pas \u00e9t\u00e9 bien class\u00e9es par notre mod\u00e8le.","5da431f1":"Afin de maximiser la pr\u00e9cision de nos mod\u00e8les, on peut optimiser leurs hyperparam\u00e8tres.\n\nComme sur le dataset du diab\u00e8te, on va utiliser la validation crois\u00e9e avec GridSearchCV.","b6f2d5fb":"### Pr\u00e9-traitement des donn\u00e9es pour le Deep","31cddb83":"On gagne ainsi 7 points d'accuracy par rapport au SVC.","59b42a5a":"On teste en affichant le 5e \u00e9l\u00e9ment","cc74b444":"<span style=\"color:red\"> Je retrouve le m\u00eame probl\u00e8me que sur le dataset du diab\u00e8te, avec un GridSearch me donnant un \"meilleur score\" optimis\u00e9 plus bas que celui du mod\u00e8le de base.","68866114":"Fonction pour tracer les courbes d'apprentissage sur l'ensemble d'apprentissage et l'ensemble de validation :","fa9faea4":"<img src=\"https:\/\/github.com\/zalandoresearch\/fashion-mnist\/blob\/master\/doc\/img\/fashion-mnist-sprite.png?raw=true\">","ece1f90b":"Le dataset a \u00e9t\u00e9 constitu\u00e9 par Zalando :  \nhttps:\/\/github.com\/zalandoresearch\/fashion-mnist  \n  \nOn a un ensemble d'apprentissage de 60 000 images 28x28 pixels en niveaux de gris, et 10 classes de v\u00eatements : jupes, pantalons, baskets, ...","9a6b1813":"La distribution de notre \u00e9chantillon est \u00e9quilibr\u00e9e.","1c6ac967":"En regardant les erreurs faites par le mod\u00e8le, on remarque qu'un humain pourrait \u00e9galement se tromper. Certains labels sont en effet trompeurs. \n\nPour juger l'accuracy de notre mod\u00e8le, il serait ainsi int\u00e9ressant d'utiliser la baseline humaine en donnant le m\u00eame travail de classification \u00e0 un humain. L'accuracy de l'humain nous permettrait ainsi de mieux juger l'efficacit\u00e9 de notre mod\u00e8le.","05c223e0":"Les 3 \u00e9tapes pour construire un CNN :\n\n- definition: on d\u00e9finit la structure du mod\u00e8le, le type et le nombre de couches et leurs param\u00e8tres\n- compilation: on choisit les indicateurs qui seront utilis\u00e9s pour l'entrainement\n- fitting: on entraine le mod\u00e8le et on d\u00e9finit les param\u00e8tres d'entrainement (batch_size et epochs)","b99b3214":"### Train\/test split"}}