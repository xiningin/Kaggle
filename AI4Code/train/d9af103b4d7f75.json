{"cell_type":{"68b21d2b":"code","9c1b99a4":"code","c8844fac":"code","4ec04784":"code","c055bd54":"code","fcac47b9":"code","75e81da8":"code","82d95276":"code","74d6ce18":"code","3bdfd812":"code","ad863f3e":"code","b595c023":"code","7be22ee5":"code","92b0db3a":"code","1ceb9557":"code","a628b258":"code","732091ec":"code","a7d201bf":"code","0ad1c4a0":"code","46e28f1c":"code","bda981ce":"code","e503473e":"code","8ded4886":"code","30383b1e":"code","b1041794":"code","cda17dbc":"code","e3ef041d":"markdown","0484149e":"markdown","39398628":"markdown","4f20c117":"markdown","98c37ba6":"markdown","29ebd742":"markdown","94b468b5":"markdown","42e9156d":"markdown","1bde4f3e":"markdown","c227a455":"markdown","4d787986":"markdown","97f8adda":"markdown","e0446443":"markdown","5f34d62d":"markdown","ba12f6ba":"markdown","000eea2a":"markdown","aa5584e1":"markdown","5d81948b":"markdown","eff51d9f":"markdown","a9523eb4":"markdown","60c5c91f":"markdown","7f791bd8":"markdown","025eae4c":"markdown","59172fa1":"markdown","3f2e0a39":"markdown","88073e03":"markdown","20e6b927":"markdown","6efb3cb4":"markdown","60cdb4e8":"markdown","6cd08436":"markdown","f9b7887c":"markdown","5b741305":"markdown","521e563f":"markdown"},"source":{"68b21d2b":"# Import libraries \nimport pandas                as pd\nimport numpy                 as np \nimport sklearn               as sk\nimport seaborn               as sb\nimport matplotlib.pyplot     as plt\n\nimport os\nimport sklearn.preprocessing \n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors       import KNeighborsClassifier\nfrom sklearn.ensemble        import AdaBoostClassifier\nfrom sklearn.ensemble        import RandomForestClassifier\nfrom sklearn.ensemble        import GradientBoostingClassifier\nfrom sklearn.naive_bayes     import GaussianNB\nfrom sklearn                 import svm","9c1b99a4":"# Import datasets and fill na values with \"?\"\ntrainData = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\", na_values=\"?\")\ntestData  = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\", na_values=\"?\")","c8844fac":"print(\"[Rows x Columns] -> \" + str(np.transpose(trainData.shape)))","4ec04784":"trainData.head()","c055bd54":"trainData.describe()","fcac47b9":"trainData.isna().sum()","75e81da8":"# Train data\ntrainData[\"workclass\"]       = trainData[\"workclass\"].fillna(trainData[\"workclass\"].mode())\ntrainData[\"occupation\"]      = trainData[\"occupation\"].fillna(trainData[\"occupation\"].mode())\ntrainData[\"native.country\"]  = trainData[\"native.country\"].fillna(trainData[\"native.country\"].mode())\n# Test data\ntestData[\"workclass\"]       = testData[\"workclass\"].fillna(trainData[\"workclass\"].mode())\ntestData[\"occupation\"]      = testData[\"occupation\"].fillna(trainData[\"occupation\"].mode())\ntestData[\"native.country\"]  = testData[\"native.country\"].fillna(trainData[\"native.country\"].mode())","82d95276":"encoder = sk.preprocessing.LabelEncoder()\n\nlabelColumnsTrain = [\"occupation\", \"education\", \"relationship\", \"sex\", \"race\", \"native.country\", \"workclass\", \"marital.status\", \"income\"]\nlabelColumnsTest = [\"occupation\", \"education\", \"relationship\", \"sex\", \"race\", \"native.country\", \"workclass\", \"marital.status\"]\n\nfor label in labelColumnsTrain:\n    trainData[label] = encoder.fit_transform(trainData[label].astype(str))\n\nfor label in labelColumnsTest:\n    testData[label] = encoder.fit_transform(testData[label].astype(str))\n","74d6ce18":"plt.scatter(range(32560), trainData[\"capital.gain\"])","3bdfd812":"plt.scatter(range(32560), trainData[\"capital.loss\"])","ad863f3e":"plt.scatter(range(32560), trainData[\"hours.per.week\"])","b595c023":"plt.scatter(range(32560), trainData[\"age\"])","7be22ee5":"scaler = sklearn.preprocessing.StandardScaler()\n\nscalableColumns = [\"age\", \"education.num\", \"capital.loss\", \"capital.gain\", \"hours.per.week\"]\n\ntrainData[scalableColumns] = scaler.fit_transform((trainData[scalableColumns]))\ntestData[scalableColumns]  = scaler.fit_transform((testData[scalableColumns]))","92b0db3a":"plt.scatter(range(32560), trainData[\"capital.gain\"])\nplt.scatter(range(32560), trainData[\"capital.loss\"])\nplt.scatter(range(32560), trainData[\"hours.per.week\"])\nplt.scatter(range(32560), trainData[\"age\"])","1ceb9557":"corr_mat = trainData.corr()\nsb.set()\nplt.figure(figsize=(26,18))\nsb.heatmap(corr_mat, annot=True, cmap='Blues')","a628b258":"trainData = trainData.drop(columns=[\"Id\", \"fnlwgt\"])\ntestData  = testData.drop(columns=[\"Id\", \"fnlwgt\"])","732091ec":"knnInput = trainData.drop(columns=[\"income\", \"native.country\", \"education\"])\n\nknnInput[\"education.num\"] = knnInput[\"education.num\"]*4\nknnInput[\"sex\"] = knnInput[\"sex\"]*9\nknnInput[\"age\"] = knnInput[\"age\"]*6\nknnInput[\"race\"] = knnInput[\"race\"]*5\nknnInput[\"hours.per.week\"] = knnInput[\"hours.per.week\"]*12\nknnInput[\"capital.gain\"] = knnInput[\"capital.gain\"]*8\nknnInput[\"capital.loss\"] = knnInput[\"capital.loss\"]*20\nknnInput[\"marital.status\"] = knnInput[\"marital.status\"]*10\n\nincomeInput = trainData[\"income\"]\n\nKNN = KNeighborsClassifier(n_neighbors=20)\nKNN.fit(knnInput, incomeInput)\nscoreKNN = cross_val_score(KNN, knnInput, incomeInput, cv=15)\nprint(\"KNN SCORE = \" + str(scoreKNN.mean())) # 0.8501535178490945","a7d201bf":"nbInput = trainData.drop(columns=[\"income\", \"native.country\", \"education\"])\n\n\nnb = GaussianNB()\nnb.fit(nbInput, incomeInput)\n\nscoreNB = cross_val_score(nb, nbInput, incomeInput, cv=5)\nprint(\"NB SCORE = \" + str(scoreNB.mean())) # 0.8005221130221128","0ad1c4a0":"svmInput = trainData.drop(columns=[\"income\", \"native.country\", \"education\"])\n\n\nsvmC = svm.SVC()\nsvmC.fit(svmInput, incomeInput)\n\nscoreSVM = cross_val_score(svmC, svmInput, incomeInput, cv=5)\nprint(\"SVM SCORE = \" + str(scoreSVM.mean())) # 0.8459152334152333\n","46e28f1c":"adaInput = trainData.drop(columns=[\"income\", \"native.country\", \"education\"])\nscoreADAB = 0\nfor i in [1400, 1450, 1500, 1550, 1600]:\n    adaB = AdaBoostClassifier(n_estimators=i, random_state=200, learning_rate=1)\n    adaB.fit(adaInput, incomeInput)\n    a = cross_val_score(adaB, adaInput, incomeInput, cv=5)\n    if scoreADAB < a.mean():\n        scoreADAB = a.mean()\n        betterEstimators_ADAB = i\n\nprint(\"ADAB SCORE = \" + str(scoreADAB.mean()) + \" with \" + str(betterEstimators_ADAB) + \" estimators\") # 0.8722972972972972","bda981ce":"rfInput = trainData.drop(columns=[\"income\", \"native.country\", \"education\"])\n\n\nrandomF = RandomForestClassifier(n_estimators=1500, max_depth=20, random_state=0)\nrandomF.fit(rfInput, incomeInput)\n\nscoreRF = cross_val_score(randomF, rfInput, incomeInput, cv=5)\nprint(\"RANDOM FOREST SCORE = \" + str(scoreRF.mean())) # 0.8642813267813267","e503473e":"gbInput = trainData.drop(columns=[\"income\", \"native.country\", \"education\"])\n\nGB = GradientBoostingClassifier(n_estimators=100, max_depth=20, random_state=0)\nGB.fit(gbInput, incomeInput)\n\nscoreGB = cross_val_score(GB, gbInput, incomeInput, cv=5)\nprint(\"GRADIENT BOOSTING SCORE = \" + str(scoreGB.mean())) # 0.8636363636363636","8ded4886":"print(\"KNN score: \" + str(scoreKNN.mean()))\nprint(\"Naive Bayes score: \" + str(scoreNB.mean()))\nprint(\"Support vector machine score: \" + str(scoreSVM.mean()))\nprint(\"Adaboost score: \" + str(scoreADAB.mean()))\nprint(\"Random Forest score: \" + str(scoreRF.mean()))\nprint(\"Gradient Boosting score: \" + str(scoreGB.mean()))","30383b1e":"FinalClassifier = adaB","b1041794":"FinalClassifierTest = testData.drop(columns=[\"native.country\", \"education\"])\npredict = FinalClassifier.predict(FinalClassifierTest)","cda17dbc":"# Import datasets and fill na values with \"?\"\ntrainData = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\", na_values=\"?\")\ntestData  = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\", na_values=\"?\")\n\nencoder = sk.preprocessing.LabelEncoder()\nencoder.fit(trainData[\"income\"])\n\nframe = {\"Id\": testData.Id, \"income\": encoder.inverse_transform(predict)}\nsolution = pd.DataFrame(frame)\nsolution.to_csv(\"submission.csv\", index = False)\nsolution.head()","e3ef041d":"Ploting some resultant data to see if it worked","0484149e":"Building the CSV:","39398628":"The gradient boosting is a classifier based on boosting regression trees","4f20c117":"First, we can train the KNN, with weights in each feature, to maximize accuracy","98c37ba6":"Here we can remove the columns that aren't going to be used, the Id and fnlwgt","29ebd742":"Training the adaboost classifier, and selecting the best number of estimators. The Adaboost is fits a classifier in a dataset, here, the Random Forest is used, and weights each classifier by each score","94b468b5":"# 2.1 Missing Data\nHere, we need to treat in some way the mising data in the dataset, so first, let's see what is missing here","42e9156d":"# 4.0 Comparing Classifiers","1bde4f3e":"# 3.6 Gradient Boosting","c227a455":"Now, normalizing the data with the StandardScaler function:","4d787986":"# 2. Data Preparation\nIn this step, we'll study our dataset, in order to use it in a good way\n\n\nFirst, let's get the format of our dataset","97f8adda":"To see how each feature can be more closely related with the income, we can plot all the correlations","e0446443":"# 2.5 Feature Selection","5f34d62d":"Now, we can replace the missing data with it respective column mode","ba12f6ba":"Predicting the solution with our classifier","000eea2a":"To use the not numeric atributes, we need to encode it as numeric values. The columns that needs to be encoded are: occupation, education, relationship, sex, race, native.country, workclass, marital.status.","aa5584e1":"# 1. Imports","5d81948b":"# 3.2 Naive Bayes","eff51d9f":"# 5.0 Exporting Solution","a9523eb4":"# 3. Training Classifiers","60c5c91f":"The naive bayes is a simple classifier based on the bayes theorem","7f791bd8":"# 2.3 Normalize Data\n\nFirst, we need to see what data need to be normalized:","025eae4c":"# 3.5 Random Forest","59172fa1":"Seeing this, we can select as the most accurate classifier:","3f2e0a39":"# 3.4 Adaboost Classifier","88073e03":"# 2.4 Correlation\n","20e6b927":"To classify our data, we can train some types and choose the one with the highest score","6efb3cb4":"# 3.3 SVM","60cdb4e8":"For easy viewing, we can print all the scores of all the classifiers:","6cd08436":"Training the support vector machine, a method that founds hyperplans in the data to divide it.","f9b7887c":"# 2.2 Encode data","5b741305":"# 3.1 KNN","521e563f":"Training the random forest:"}}