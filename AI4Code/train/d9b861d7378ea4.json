{"cell_type":{"e0cb0fdc":"code","c6e34b62":"code","34f42acc":"code","ac87064c":"code","ac90a7c7":"code","09ba44dd":"code","39bef7e8":"code","868ee91d":"code","85c13020":"code","fa17439b":"code","8857f182":"code","64d61bbf":"code","aa691a23":"code","ae5d1fae":"code","e1d0a6f0":"code","600cf0e8":"code","8f7ea271":"code","01b083cc":"code","121605fb":"code","acbd07cb":"code","85e68d0e":"code","0dd9765d":"code","46734929":"code","93dac216":"code","b4acfc78":"code","918940d8":"code","8ef730aa":"code","f2fed1c8":"code","3de96c40":"code","95b12b6e":"code","84c827ee":"code","a3bbbd33":"code","ee5388d2":"code","d451b29f":"code","ce1b2f81":"code","76bf077f":"code","b70f20fc":"code","319641e3":"code","671ef4dc":"code","d1def09d":"code","a16f3344":"code","bfa16502":"code","f8d7355f":"code","86152c60":"code","7a186fc0":"code","c55f863d":"code","2bdba8c6":"code","1571588a":"code","c51757de":"code","2579e064":"code","d043a685":"code","2a11154b":"code","8f31a115":"code","9e43dd20":"code","a4f0624a":"code","fb28c00f":"code","f50dce06":"code","ae2e4732":"code","f81b0b33":"code","e18a549a":"code","3295e9e3":"code","ab366c6d":"code","2d3089b4":"code","07b86faa":"code","78ab4591":"code","3c1cfa55":"code","bb71f77b":"code","9e10b0bb":"code","72bce432":"code","258d481c":"code","cba33447":"code","d3ec3b4b":"code","85956928":"code","3fe3a626":"code","7b800cd4":"code","23e71638":"code","2ba6721d":"code","14cab841":"code","f5278d96":"code","67587c0d":"code","9c5a5474":"code","cbb872d4":"code","d8136ec5":"code","93466632":"code","13f875f1":"code","a2b11f07":"code","fd9e7d50":"code","906864eb":"code","0ebcc7cf":"code","2b9c29a2":"code","37c73f35":"code","b2ba45c2":"code","ffb6f279":"code","95e185a0":"code","7e11c7e0":"code","3981cec5":"code","c2882b36":"code","d7b8a625":"code","010b2bfd":"code","de2e6f0d":"code","c858e07a":"code","425df235":"code","d3ab5bd3":"code","49a16cd1":"code","cdf4a9c5":"code","c56bbe70":"code","8ac7d7cd":"code","63f99a87":"code","7dc2e2dd":"code","eae8d27e":"code","4a2dcd5a":"code","ae8d4de0":"code","9b013c49":"code","72f4c41a":"markdown","c58d93e4":"markdown","49436ae5":"markdown","d5f9882f":"markdown","57cc01a7":"markdown","c49e12b3":"markdown","867e67ff":"markdown","f2f422b9":"markdown","8663b3da":"markdown","e03f47fa":"markdown","dde5722b":"markdown","2ea3c940":"markdown","387831b3":"markdown","00e25c1b":"markdown","d2a74a5c":"markdown","9be13959":"markdown","c63c8473":"markdown","1638373a":"markdown","0984080c":"markdown","e8051725":"markdown","31103830":"markdown","3522c6d5":"markdown","c162c5a7":"markdown","182a55b1":"markdown","a43ac07a":"markdown","b33435bc":"markdown","a27552ad":"markdown","ffbd4259":"markdown","4eb7d607":"markdown","92d3fa18":"markdown","dacb3045":"markdown","6c2403b2":"markdown","e333bcfb":"markdown","26454356":"markdown","04e8f9e0":"markdown","292b4815":"markdown","e44c8de9":"markdown","89e534ea":"markdown","cf45f087":"markdown","466e6300":"markdown","3ff25527":"markdown","fc333df2":"markdown","3da788d0":"markdown","48247135":"markdown","9b142ebb":"markdown","38d6bcef":"markdown","3a5f6ef8":"markdown","770a7d98":"markdown","85e9e867":"markdown","c2c853b4":"markdown","e8195ea7":"markdown","56949271":"markdown","db88e95a":"markdown","40c007b8":"markdown","c694d4e3":"markdown","ba4f6d0b":"markdown","c91be8c6":"markdown"},"source":{"e0cb0fdc":"#import Lib's\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np # linear algebra\nimport os # accessing directory structure\n\n\n%matplotlib inline\npd.options.display.float_format = '{:.2f}'.format\n\nimport warnings\nwarnings.filterwarnings('ignore')","c6e34b62":"#Load CSV\ndf_tele_churn = pd.read_csv(\"..\/input\/telecom_churn_data.csv\", encoding = \"ISO-8859-1\", low_memory=False )\ndf_tele_churn.head()","34f42acc":"print (df_tele_churn.shape)","ac87064c":"#Find columns with recharge info\n#since total recharge amount is to be used for High Value Customer Filter\ntot_rech_cols = [col for col in df_tele_churn.columns if 'total_rech_' in col]\nprint(tot_rech_cols)","ac90a7c7":"#Derive Total Data Recharge Amounts\n\ndf_tele_churn[\"total_rech_data_amt_6\"] = df_tele_churn[\"total_rech_data_6\"]  * df_tele_churn['av_rech_amt_data_6']\ndf_tele_churn[\"total_rech_data_amt_7\"] = df_tele_churn[\"total_rech_data_7\"]  * df_tele_churn['av_rech_amt_data_7']\ndf_tele_churn[\"total_rech_data_amt_8\"] = df_tele_churn[\"total_rech_data_8\"]  * df_tele_churn['av_rech_amt_data_8']\ndf_tele_churn[\"total_rech_data_amt_9\"] = df_tele_churn[\"total_rech_data_9\"]  * df_tele_churn['av_rech_amt_data_9']","09ba44dd":"# Drop total_rech_data_* and av_rech_amt_data_*\ndrop_col = [\"total_rech_data_6\", \"total_rech_data_7\", \"total_rech_data_8\", \"total_rech_data_9\", \n                'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8', 'av_rech_amt_data_9']\ndf_tele_churn.drop(drop_col, axis=1, inplace=True)","39bef7e8":"#Average of Recharge Amount in 6th and 7th Month\nav_rech_amt_6n7 = (df_tele_churn[\"total_rech_amt_6\"].fillna(0) + df_tele_churn[\"total_rech_data_amt_6\"].fillna(0) + \ndf_tele_churn[\"total_rech_amt_7\"].fillna(0) + df_tele_churn[\"total_rech_data_amt_7\"].fillna(0))\/2.0\n\n#70th Percentile of Avg Recharge Amount\npc70_6n7 = np.percentile(av_rech_amt_6n7, 70.0)\nprint('70 percentile of 6th and 7th months avg recharge amount: ', pc70_6n7)\n\ndf_high_val_cust = df_tele_churn[av_rech_amt_6n7 >= pc70_6n7]\nprint('Dataframe Shape after Filtering HIgh Value Customers: ', df_high_val_cust.shape)","868ee91d":"#Remove Data which has only 1 unique Value\n\n#List of columns with only 1 unqiue value\ncol_list = df_high_val_cust.loc[:,df_high_val_cust.apply(pd.Series.nunique) == 1]\ncol_list.head(5)\n","85c13020":"#Remove Columns with only 1 unique value\ndf_high_val_cust = df_high_val_cust.loc[:,df_high_val_cust.apply(pd.Series.nunique) != 1]\ndf_high_val_cust.shape\n","fa17439b":"#Rename Columns with Meaning full Names\n# aug_vbc_3g jul_vbc_3g jun_vbc_3g sep_vbc_3g\n\ndf_high_val_cust.rename(columns={'jun_vbc_3g': 'vbc_3g_6', 'jul_vbc_3g': 'vbc_3g_7', 'aug_vbc_3g': 'vbc_3g_8', 'sep_vbc_3g': 'vbc_3g_9'}, inplace=True)","8857f182":"#Check for Columns that can be changed to integers, floats or other types\nobject_col_data = df_high_val_cust.select_dtypes(include=['object'])\nprint(object_col_data.iloc[0])","64d61bbf":"# convert to datetime\nfor col in object_col_data.columns:\n    df_high_val_cust[col] = pd.to_datetime(df_high_val_cust[col])\n\ndf_high_val_cust.shape","aa691a23":"#Drop Columns with > 30% of missing values and not for 9th Month\ncolumns = df_high_val_cust.columns\npercent_missing = df_high_val_cust.isnull().sum() * 100 \/ len(df_high_val_cust)\nmissing_value_df = pd.DataFrame({'column_name': columns,\n                                 'percent_missing': percent_missing})\n\ndrop_col = missing_value_df.loc[(missing_value_df[\"column_name\"].str.contains('_9')==False) & (missing_value_df[\"percent_missing\"] > 30.0)][\"column_name\"]\ndrop_col","ae5d1fae":"df_high_val_cust.drop(drop_col, axis=1, inplace=True)\ndf_high_val_cust.shape","e1d0a6f0":"#Drop Rows with all Null Values\ndf_high_val_cust = df_high_val_cust.dropna(how='all',axis=0) \ndf_high_val_cust.shape","600cf0e8":"#Find Columns with Unique Value but Insignificant Frequency\nfor col_name in df_high_val_cust.columns:\n    if (len(df_high_val_cust[col_name].unique()) <= 8):\n        print(df_high_val_cust[col_name].value_counts())\n        print(f\"\\n{35 * '-'}\")","8f7ea271":"#Find Highly correlated data and drop Highly Correlated Columns\ncor = df_high_val_cust.corr()\ncor.loc[:,:] = np.tril(cor, k=-1)\ncor = cor.stack()\ncor[(cor > 0.60) | (cor < -0.60)].sort_values()","01b083cc":"drop_col_list = ['loc_og_t2m_mou_6','std_og_t2t_mou_6','std_og_t2t_mou_7','std_og_t2t_mou_8','std_og_t2t_mou_9','std_og_t2m_mou_6',\n                'std_og_t2m_mou_7','std_og_t2m_mou_8','std_og_t2m_mou_9','total_og_mou_6','total_og_mou_7','total_og_mou_8',\n                'loc_ic_t2t_mou_6','loc_ic_t2t_mou_7','loc_ic_t2t_mou_8','loc_ic_t2t_mou_9','loc_ic_t2m_mou_6','loc_ic_t2m_mou_7','loc_ic_t2m_mou_8','loc_ic_t2m_mou_9',\n                'std_ic_t2m_mou_6','std_ic_t2m_mou_7','std_ic_t2m_mou_8','std_ic_t2m_mou_9','total_ic_mou_6','total_ic_mou_7','total_ic_mou_8',\n                'total_rech_amt_6','total_rech_amt_7','total_rech_amt_8','total_rech_amt_9','arpu_2g_9','count_rech_2g_9','count_rech_3g_9','vol_3g_mb_6','vol_3g_mb_7','vol_3g_mb_8',\n                'loc_og_t2t_mou_6','loc_og_t2t_mou_7','loc_og_t2t_mou_8','loc_og_t2t_mou_9','loc_og_t2f_mou_6','loc_og_t2f_mou_7','loc_og_t2f_mou_8','loc_og_t2f_mou_9',\n                'loc_og_t2m_mou_6','loc_og_t2m_mou_7','loc_og_t2m_mou_8','loc_og_t2m_mou_9','loc_ic_t2f_mou_6','loc_ic_t2f_mou_7','loc_ic_t2f_mou_8','loc_ic_t2f_mou_9',\n                'date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8']\n                 \ndf_high_val_cust.drop(drop_col_list, axis=1, inplace=True)\ndf_high_val_cust.shape","121605fb":"df_high_val_cust[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']].head()","acbd07cb":"#churned customers (churn=1, else 0) \ndf_high_val_cust['churn'] = np.where(df_high_val_cust[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']].sum(axis=1) == 0, \n                                   1,\n                                   0)","85e68d0e":"df_high_val_cust['churn'].head()","0dd9765d":"#Remove All 9th Month related columns\ndrop_cols = [col for col in df_high_val_cust.columns if '_9' in col]\nprint(drop_cols)\n\ndf_high_val_cust.drop(drop_cols, axis=1, inplace=True)\n\ndf_high_val_cust.shape","46734929":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","93dac216":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = \"Telecom Churn\"\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","b4acfc78":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","918940d8":"plotPerColumnDistribution(df_high_val_cust, 10, 5)","8ef730aa":"plotCorrelationMatrix(df_high_val_cust, 53)","f2fed1c8":"plotScatterMatrix(df_high_val_cust, 20, 10)","3de96c40":"col_list = df_high_val_cust.filter(regex='_6|_7').columns.str[:-2]\ncol_list.unique()\n\nprint (df_high_val_cust.shape)\n\nfor idx, col in enumerate(col_list.unique()):\n    print(col)\n    avg_col_name = \"avg_\"+col+\"_av67\"\n    col_6 = col+\"_6\"\n    col_7 = col+\"_7\"\n    df_high_val_cust[avg_col_name] = (df_high_val_cust[col_6]  + df_high_val_cust[col_7])\/ 2\n\n","95b12b6e":"print (df_high_val_cust.shape)\n\ncol_list = df_high_val_cust.filter(regex='_6|_7').columns\n\ndf_high_val_cust.drop(col_list, axis=1, inplace=True)\ndf_high_val_cust.shape","84c827ee":"df_high_val_cust.head()","a3bbbd33":"#Conevrt AON in Months\ndf_high_val_cust['aon_mon'] = df_high_val_cust['aon']\/30\ndf_high_val_cust.drop('aon', axis=1, inplace=True)\ndf_high_val_cust['aon_mon'].head()","ee5388d2":"#Churn Distribution\nax = (df_high_val_cust['churn'].value_counts()*100.0 \/len(df_high_val_cust)).plot.pie(autopct='%.1f%%', labels = ['No', 'Yes'],figsize =(5,5), fontsize = 12 )                                                                           \n\nax.set_ylabel('Churn',fontsize = 12)\nax.set_title('Churn Distribution', fontsize = 12)","d451b29f":"plotPerColumnDistribution(df_high_val_cust, 10, 5)","ce1b2f81":"plotCorrelationMatrix(df_high_val_cust, 53)","76bf077f":"plotScatterMatrix(df_high_val_cust, 20, 10)","b70f20fc":"ax = sns.distplot(df_high_val_cust['aon_mon'], hist=True, kde=False, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax.set_ylabel('No of Customers')\nax.set_xlabel('Tenure (months)')\nax.set_title('Customers by their tenure')","319641e3":"tn_range = [0, 6, 12, 24, 60, 61]\ntn_label = [ '0-6 Months', '6-12 Months', '1-2 Yrs', '2-5 Yrs', '5 Yrs and above']\ndf_high_val_cust['tenure_range'] = pd.cut(df_high_val_cust['aon_mon'], tn_range, labels=tn_label)\ndf_high_val_cust['tenure_range'].head()","671ef4dc":"sns.set()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\ntemp = pd.Series(data = 'tenure_range')\nfig, ax = plt.subplots()\nwidth = len(df_high_val_cust['tenure_range'].unique()) + 6 + 4*len(temp.unique())\nfig.set_size_inches(width , 7)\n\ntotal = float(len(df_high_val_cust.index))\nax = sns.countplot(x=\"tenure_range\", data=df_high_val_cust, palette=\"Set2\", hue = \"churn\");\nfor p in ax.patches:\n                ax.annotate('{:1.1f}%'.format((p.get_height()*100)\/float(len(df_high_val_cust))), (p.get_x()+0.05, p.get_height()+20))\nplt.xticks(rotation=90)\nplt.show()","d1def09d":"#Get Correlation of \"Churn\" with other variables:\nplt.figure(figsize=(20,10))\ndf_high_val_cust.corr()['churn'].sort_values(ascending = False).plot(kind='bar')","a16f3344":"df_high_val_cust[['total_rech_num_8', 'arpu_8']].plot.scatter(x = 'total_rech_num_8',\n                                                              y='arpu_8')","bfa16502":"df_high_val_cust[['aon_mon', 'avg_arpu_av67']].plot.scatter(x = 'aon_mon',\n                                                              y='avg_arpu_av67')","f8d7355f":"sns.boxplot(x = df_high_val_cust.churn, y = df_high_val_cust.aon_mon)\n","86152c60":"ax = sns.kdeplot(df_high_val_cust.avg_max_rech_amt_av67[(df_high_val_cust[\"churn\"] == 0)],\n                color=\"Red\", shade = True)\nax = sns.kdeplot(df_high_val_cust.avg_max_rech_amt_av67[(df_high_val_cust[\"churn\"] == 1)],\n                ax =ax, color=\"Blue\", shade= True)\nax.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nax.set_ylabel('Density')\nax.set_xlabel('Volume based cost')\nax.set_title('Distribution of Volume based cost by churn')","7a186fc0":"ax = sns.kdeplot(df_high_val_cust.max_rech_amt_8[(df_high_val_cust[\"churn\"] == 0)],\n                color=\"Red\", shade = True)\nax = sns.kdeplot(df_high_val_cust.max_rech_amt_8[(df_high_val_cust[\"churn\"] == 1)],\n                ax =ax, color=\"Blue\", shade= True)\nax.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nax.set_ylabel('Density')\nax.set_xlabel('Volume based cost')\nax.set_title('Distribution of Max Recharge Amount by churn')","c55f863d":"df_high_val_cust.head()","2bdba8c6":"#Lets Create New DF for Model Building\n\ndf = df_high_val_cust[:].copy()\n\n#Dropping tenure_range since we have AON MONTH already and columns are highly coorelated\ndf.drop('tenure_range', axis=1, inplace=True)\ndf.drop('mobile_number', axis=1, inplace=True)\n\n#Since All The Values are realted to Price\/ Cost\/ Amount, Filling NaN with 0\n\ndf.fillna(0, inplace=True)\n\ndf.head()","1571588a":"X = df.drop(['churn'], axis=1)\ny = df['churn']\n\ndf.drop('churn', axis=1, inplace=True)\n","c51757de":"from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = preprocessing.StandardScaler().fit(X)\nX = scaler.transform(X)","2579e064":"# Split in train & Test\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=1)","d043a685":"print(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape : \", X_test.shape)\n\ny_train_imb = (y_train != 0).sum()\/(y_train == 0).sum()\ny_test_imb = (y_test != 0).sum()\/(y_test == 0).sum()\nprint(\"Imbalance in Train Data : \", y_train_imb)\nprint(\"Imbalance in Test Data : \", y_test_imb)","2a11154b":"# Balancing DataSet\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(kind = \"regular\")\nX_tr,y_tr = sm.fit_sample(X_train,y_train)","8f31a115":"print(\"X_tr Shape\", X_tr.shape)\nprint(\"y_tr Shape\", y_tr.shape)\n\nimb = (y_tr != 0).sum()\/(y_tr == 0).sum()\nprint(\"Imbalance in Train Data : \",imb)","9e43dd20":"from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n\nlr = LogisticRegression()\n\nlr.svm = SVC(kernel='linear') \nlr.svm.fit(X_train,y_train)\npreds = lr.svm.predict(X_test)\nmetrics.accuracy_score(y_test, preds)","a4f0624a":"# Feature reduction using RFE\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nfrom sklearn.feature_selection import RFE\n\n# running RFE with 15 variables as output\nrfe = RFE(lr, 15)   \nrfe = rfe.fit(X_tr, y_tr)","fb28c00f":"rfe_features = list(df.columns[rfe.support_])\nprint(\"Features identified by RFE \", rfe_features)","f50dce06":"X_rfe = pd.DataFrame(data=X_tr).iloc[:, rfe.support_]\ny_rfe = y_tr","ae2e4732":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=1)\nlr.fit(X_rfe, y_rfe)","f81b0b33":"X_test_rfe = pd.DataFrame(data=X_test).iloc[:, rfe.support_]\n\ny_pred = lr.predict(X_test_rfe)\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","e18a549a":"print('Accuracy of Logistic Regression Model on test set is ',lr.score(X_test_rfe, y_test))","3295e9e3":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","ab366c6d":"# To get the weights of all the variables\nweights = pd.Series(lr.coef_[0],\n                 index=rfe_features)\nweights.sort_values(ascending = False)","2d3089b4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=1)\n\n#Applying Smote\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(kind = \"regular\")\nX_tr,y_tr = sm.fit_sample(X_train,y_train)\nprint(X_tr.shape)\nprint(y_tr.shape)\n","07b86faa":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(random_state=100)\n\n#Doing the PCA on the train data\npca.fit(X_tr)","78ab4591":"X_tr_pca = pca.fit_transform(X_tr)\nprint(X_tr_pca.shape)\n\nX_test_pca = pca.transform(X_test)\nprint(X_test_pca.shape)","3c1cfa55":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlr_pca = LogisticRegression(C=1e9)\nlr_pca.fit(X_tr_pca, y_tr)\n\n# Predicted probabilities\ny_pred = lr_pca.predict(X_test_pca)\n\n# Converting y_pred to a dataframe which is an array\ny_pred_df = pd.DataFrame(y_pred)","bb71f77b":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Printing confusion matrix\nprint(confusion_matrix(y_test,y_pred))","9e10b0bb":"print(\"Logistic Regression accuracy with PCA: \",accuracy_score(y_test,y_pred))","72bce432":"col = list(df.columns)\ndf_pca = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'PC3':pca.components_[2],'Feature':col})\ndf_pca.head(10)","258d481c":"#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","cba33447":"np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)","d3ec3b4b":"pca_32 = PCA(n_components=32)\n\ndf_tr_pca_32 = pca_32.fit_transform(X_tr)\nprint(df_tr_pca_32.shape)\n\ndf_test_pca_32 = pca_32.transform(X_test)\nprint(df_test_pca_32.shape)","85956928":"# Let's run the model using the selected variables\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlr_pca1 = LogisticRegression(C=1e9)\nlr_pca1.fit(df_tr_pca_32, y_tr)\n\n# Predicted probabilities\ny_pred32 = lr_pca1.predict(df_test_pca_32)\n\n# Converting y_pred to a dataframe which is an array\ndf_y_pred = pd.DataFrame(y_pred32)","3fe3a626":"print(confusion_matrix(y_test,y_pred32))","7b800cd4":"print(\"Logistic Regression accuracy with PCA: \",accuracy_score(y_test,y_pred32))","23e71638":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=1)\n\n#Applying Smote\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(kind = \"regular\")\nX_tr,y_tr = sm.fit_sample(X_train,y_train)\nprint(X_tr.shape)\nprint(y_tr.shape)","2ba6721d":"from sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n \nlsvc = LinearSVC(C=0.001, penalty=\"l1\", dual=False).fit(X_tr, y_tr)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_lasso = model.transform(X_tr)\npos = model.get_support(indices=True)\n ### Feature reduction using RFE\nprint(X_lasso.shape)\nprint(pos)","14cab841":"#feature vector for decision tree\nlasso_features = list(df.columns[pos])\nprint(\"Features for LASSO model buidling: \", lasso_features)","f5278d96":"# Importing decision tree classifier from sklearn library\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\n# Fitting the decision tree with default hyperparameters, apart from\n# max_depth which is 5 so that we can plot and read the tree.\ndt1 = DecisionTreeClassifier(max_depth=5)\ndt1.fit(X_lasso, y_tr)","67587c0d":"# The evaluation metrics of our default model\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Making predictions\nX_test = pd.DataFrame(data=X_test).iloc[:, pos]\ny_pred1 = dt1.predict(X_test)\n\n# Printing classification report\nprint(classification_report(y_test, y_pred1))","9c5a5474":"# Printing confusion matrix and accuracy\nprint(confusion_matrix(y_test,y_pred1))\nprint('Accuracy of Decision Tree :',accuracy_score(y_test,y_pred1))","cbb872d4":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(1, 40)}\n\n# instantiate the model\ndt = DecisionTreeClassifier(criterion = \"gini\", \n                               random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dt, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\ntree.fit(X_lasso, y_tr)","d8136ec5":"# scores of GridSearch CV\nscore = tree.cv_results_\npd.DataFrame(score).head()","93466632":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(score[\"param_max_depth\"], \n         score[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(score[\"param_max_depth\"], \n         score[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","13f875f1":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(5, 200, 20)}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(criterion = \"gini\", \n                               random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\ntree.fit(X_lasso, y_tr)","a2b11f07":"# scores of GridSearch CV\nscore = tree.cv_results_\npd.DataFrame(score).head()","fd9e7d50":"# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(score[\"param_min_samples_leaf\"], \n         score[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(score[\"param_min_samples_leaf\"], \n         score[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","906864eb":"# GridSearchCV to find optimal min_samples_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_split': range(5, 200, 20)}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(criterion = \"gini\", \n                               random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\ntree.fit(X_lasso, y_tr)","0ebcc7cf":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","2b9c29a2":"# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","37c73f35":"# Create the parameter grid \nparam_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(25, 175, 50),\n    'min_samples_split': range(50, 150, 50),\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nn_folds = 5\n\n# Instantiate the grid search model\ndtree = DecisionTreeClassifier()\ngrid_search = GridSearchCV(estimator = dtree, param_grid = param_grid, \n                          cv = n_folds, verbose = 1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_lasso, y_tr)","b2ba45c2":"# cv results\ncv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results\n# printing the optimal accuracy score and hyperparameters\nprint(\"Best Accuracy\", grid_search.best_score_)\n","ffb6f279":"print(grid_search.best_estimator_)","95e185a0":"# model with optimal hyperparameters\nclf_gini = DecisionTreeClassifier(criterion = \"gini\", \n                                  random_state = 100,\n                                  max_depth=5, \n                                  min_samples_leaf=25,\n                                  min_samples_split=50)\nclf_gini.fit(X_lasso, y_tr)","7e11c7e0":"# accuracy score\nprint ('Accuracy Score for Decision Tree Final Model :',clf_gini.score(X_test,y_test))","3981cec5":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_lasso, y_tr)\n\n# Make predictions\nprediction_test = model_rf.predict(X_test)\nprint ('Randon Forest Accuracy with Default Hyperparameter',metrics.accuracy_score(y_test, prediction_test))","c2882b36":"print(classification_report(y_test,prediction_test))","d7b8a625":"# Printing confusion matrix\nprint(confusion_matrix(y_test, prediction_test))","010b2bfd":"# GridSearchCV to find optimal n_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(2, 20, 5)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_lasso, y_tr)","de2e6f0d":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","c858e07a":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","425df235":"##Tuning n_estimators\n## GridSearchCV to find optimal n_estimators\n#from sklearn.model_selection import KFold\n#from sklearn.model_selection import GridSearchCV\n#\n#\n## specify number of folds for k-fold CV\n#n_folds = 5\n#\n## parameters to build the model on\n#parameters = {'n_estimators': range(100, 1500, 400)}\n#\n## instantiate the model (note we are specifying a max_depth)\n#rf = RandomForestClassifier(max_depth=4)\n#\n#\n## fit tree on training data\n#rf = GridSearchCV(rf, parameters, \n#                    cv=n_folds, \n#                   scoring=\"accuracy\")\n#rf.fit(X_lasso, y_tr)","d3ab5bd3":"## scores of GridSearch CV\n#scores = rf.cv_results_\n#\n## plotting accuracies with n_estimators\n#plt.figure()\n#plt.plot(scores[\"param_n_estimators\"], \n#         scores[\"mean_train_score\"], \n#         label=\"training accuracy\")\n#plt.plot(scores[\"param_n_estimators\"], \n#         scores[\"mean_test_score\"], \n#         label=\"test accuracy\")\n#plt.xlabel(\"n_estimators\")\n#plt.ylabel(\"Accuracy\")\n#plt.legend()\n#plt.show()","49a16cd1":"# GridSearchCV to find optimal min_samples_leaf\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(50, 400, 10)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_lasso, y_tr)","cdf4a9c5":"# scores of GridSearch CV\nscores = rf.cv_results_\n\n# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","c56bbe70":"# GridSearchCV to find optimal min_samples_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_split': range(100, 500, 25)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_lasso, y_tr)","8ac7d7cd":"# scores of GridSearch CV\nscores = rf.cv_results_\n\n# plotting accuracies with min_samples_split\nplt.figure()\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","63f99a87":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [4,8,10],\n    'min_samples_leaf': range(100, 300, 100),\n    'min_samples_split': range(200, 500, 100),\n    'n_estimators': [500,700], \n    'max_features': [10,20,25]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1,verbose = 1)","7dc2e2dd":"#Commenting as it takes long time\n# Fit the grid search to the data\n#grid_search.fit(X_lasso, y_tr)\n# printing the optimal accuracy score and hyperparameters\n#print('Accuracy is',grid_search.best_score_,'using',grid_search.best_params_)","eae8d27e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n\nmodel_rf = RandomForestClassifier(bootstrap=True,\n                                  max_depth=10,\n                                  min_samples_leaf=100, \n                                  min_samples_split=200,\n                                  n_estimators=1000 ,\n                                  oob_score = True, n_jobs = -1,\n                                  random_state =50,\n                                  max_features = 15,\n                                  max_leaf_nodes = 30)\nmodel_rf.fit(X_train, y_train)\n\n# Make predictions\nprediction_test = model_rf.predict(X_test)","4a2dcd5a":"# evaluation metrics\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,prediction_test))\nprint(confusion_matrix(y_test,prediction_test))","ae8d4de0":"# accuracy score\nprint ('Accuracy Score for Random Forest Final Model :',metrics.accuracy_score(y_test, prediction_test))","9b013c49":"X = df\n# Scaling all the variables to a range of 0 to 1\n#from sklearn.preprocessing import MinMaxScaler\nfeatures = X.columns.values\nX = pd.DataFrame(scaler.transform(X))\nX.columns = features\n\nimportances = model_rf.feature_importances_\nweights = pd.Series(importances,\n                 index=X.columns.values)\nweights.sort_values()[-10:].plot(kind = 'barh')","72f4c41a":"### ** Summary : Telecom Churn**\n* Very Less Amount of High Value customers are churning which is a good service indicator\n* Large no of Customers are new to Telecom Company and fall under < 5 Yr Tenure\n* Std Outgoing Calls and Revenue Per Customer are strong indicators of Churn\n* People with less than 4 Yrs of Tenure are more likely to Churn\n* Behaviour of Volume Based Cost is not a strong indicator of Churn\n* Max Recharge Amount could be a good Churn Indicator\n* Random Forest is the best method to Predict Churn followed by SVM, other models too do a fair job\n* Behaviour is 8 Month can be the base of Churn Analysis\n* Local Incoming and Outgoing Calls for 8th Month and Average Revenue in 8th Month are strong indicators of Churn Behaviour","c58d93e4":"Lets look at the relation between Tenure And Revenue","49436ae5":"Finally, let's take a look at out predictor variable (Churn) and understand its interaction with other important variables as was found out in the correlation plot.","d5f9882f":"Feature reduction using LASSO\n","57cc01a7":"#### Hyperparameter Tuning","c49e12b3":"Tuning min_samples_leaf","867e67ff":"PCA Summary :\n*  Model has 81% Accuracy\n* 32 Features can obtain optimal accuracy with 90% Variance\n* Main Features are arpu_8,onnet_mou_8,offnet_mou_8,roam_ic_mou_8,roam_og_mou_8","f2f422b9":"Applying LR with 32 columns with 90% variance\n","8663b3da":"All of above columns looks significant so will not drop those","e03f47fa":"#### Summary - Decision Tress\n* Getting around 87% accuracy \n* Confusion matix shows lot of false positives still exist.\n* 31 Features were selected for Model Building","dde5722b":"#### Drop Highly Correlated Columns","2ea3c940":"#### RFE","387831b3":"#### Check Columns that can be changed to integers, floats or other types","00e25c1b":"#### Churn Vs Volume based cost","d2a74a5c":"In our data, 91% of the customers do not churn. Clearly the data is skewed as we would expect a large majority of the customers to not churn. This is important to keep in mind for our modelling as skeweness could lead to a lot of false negatives. We will see in the modelling section on how to avoid skewness in the data.","9be13959":"### Model Building","c63c8473":"Scatter and density plots:","1638373a":"### Reduce the No of Columns by Creating New Meaningful Features","0984080c":"Applying logistic regression\n","e8051725":"Tuning min_samples_split","31103830":"Distribution graphs (histogram\/bar graph) of sampled columns:","3522c6d5":"#### Support Vecor Machine (SVM)","c162c5a7":"### Filter High Value Customer","182a55b1":"Tuning min_samples_leaf","a43ac07a":"Avg STD Outgoing Calls for Month 6 & 7, Outgoing calls in Roaming seems to be positively correlated with Churn while Avg Revenue, No Of Recharge for 8th Month seems negatively correlated.","b33435bc":"Tuning max_depth","a27552ad":"### Tag Churners","ffbd4259":"### Random Forest","4eb7d607":"Scatter and density plots:\n","92d3fa18":"Correlation matrix:","dacb3045":"#### Drop Columns with > 30% of missing values","6c2403b2":"#### Covert Column Names to meaningfull names","e333bcfb":"min_samples_leaf = 25 looks to be optimal","26454356":"#### Hyperparameter Tuning\nTuning max_depth","04e8f9e0":"### Logistic Regression","292b4815":"#### Churn Vs Max Recharge Amount","e44c8de9":"Correlation matrix:\n","89e534ea":"Create Columns with Average of 6th & 7th Month Since it's a \"Good\" Phase and Keep the 8th month untouched as it's \"Action\" Phase, for now to see if it can give any additional insight","cf45f087":"#### Correlation of \"Churn\" with other variables:\n","466e6300":"Grid Search to Find Optimal Hyperparameters","3ff25527":"Distribution graphs (histogram\/bar graph) of sampled columns:","fc333df2":"#### Decision Tress","3da788d0":"According to above plot, max_depth =10 is optimal","48247135":"#### Churn vs Tenure ","9b142ebb":"min_samples_leaf=50 looks optimal","38d6bcef":"Applying  PCA","3a5f6ef8":"LR Summary\n* Model Accuracy is 79%\n* Confusion matix clearly shows that the model has drawback in predicting churn as high false positives.","770a7d98":"* People Who Recharge with less Amount are more likely to Churn\n* There is no visible difference in Volume Based Cost & Churn","85e9e867":"Tuning min_samples_split","c2c853b4":"Observations:\n\n* From random forest algorithm, Local Incoming for Month 8, Average Revenue Per Customer for Month 8 and Max Recharge Amount for Month 8 are the most important predictor variables to predict churn.\n* The results from random forest are very similar to that of the logistic regression and in line to what we had expected from our EDA","e8195ea7":"# Telecom Churn - ML Group Case Study\n-  Predict churn only on high-value customers\n-  Predict usage-based definition to define churn\n-  This project is based on the Indian and Southeast Asian market\n-  Churn Phases\n    -  In \u2018good\u2019 phase the customer is happy with the service and behaves as usual\n    -  In \u2018action\u2019 phase The customer experience starts to sore in this phase\n    -  In \u2018churn\u2019 phase the customer is said to have churned\n","56949271":"Decision Tree with Default Hyperparameter","db88e95a":"As we can see form the below plot, the customers who do not churn, they tend to stay for a longer tenure with the telecom company.","40c007b8":"Random Forest with Default Hyperparameters","c694d4e3":"Lets look at the relation between total recharge in 8th Month Vs Average Revenue in 8th Month","ba4f6d0b":"### Data Cleaning & EDA","c91be8c6":"#### PCA"}}