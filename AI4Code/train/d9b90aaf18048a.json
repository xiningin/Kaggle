{"cell_type":{"8d5d5ec2":"code","ac1eca5e":"code","cdcca157":"code","c20e20b9":"code","a4fd94d9":"code","6b63df92":"code","d4e96800":"code","501de786":"code","9ee91d2b":"code","9e70aeb9":"code","e669e72c":"code","5688e542":"code","d8e974b9":"code","088d3bb5":"code","78724cb1":"code","80e104ec":"code","da18f053":"code","dcf2abe8":"code","e3c8db9a":"code","bd53b2ab":"code","5c3cd872":"code","c2cf7c7d":"code","7bcb66cf":"code","3f1b1f0e":"code","18b25766":"code","8a26bcf3":"code","011fd2a6":"code","77c057b4":"code","3ccde193":"code","8fd641b7":"code","65f46fea":"code","db6c8384":"code","d40aa60a":"code","92384ab7":"code","bebe558b":"code","bd6d5ac7":"code","77fbe0d8":"markdown","8f08e20f":"markdown","d3d34f21":"markdown","c2ada395":"markdown","1168f470":"markdown","7054f002":"markdown","596d3ce3":"markdown","9bbf4a97":"markdown","271ec659":"markdown","4071aff6":"markdown","5cb46dee":"markdown","4b262bb9":"markdown","7a3257ca":"markdown","2a75f766":"markdown","002d350f":"markdown","25a6b1b5":"markdown","e789ec01":"markdown","f028006d":"markdown","acf33814":"markdown","b9859f75":"markdown","53a8d7c3":"markdown","3a773a5f":"markdown","c772d479":"markdown","0d80f802":"markdown","662719cf":"markdown","a4b828b0":"markdown","db81a6eb":"markdown","74cb5346":"markdown","d02e6a6d":"markdown","e44af2bf":"markdown","6d415b68":"markdown","9f5bd5f3":"markdown","0db073bb":"markdown","5669a69e":"markdown","89b1412d":"markdown","06e3283a":"markdown","81174fbb":"markdown","965e5234":"markdown","3c3dfed3":"markdown","56068196":"markdown","c51205a2":"markdown","83510592":"markdown","78f907c2":"markdown"},"source":{"8d5d5ec2":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Put this when it's called\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier","ac1eca5e":"# Create table for missing data\ndef draw_missing_data_table(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending = False)\n    missing_data = pd.concat([total, percent], axis = 1, keys = ['Total', 'Percent'])\n    return missing_data","cdcca157":"# Plot learning curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","c20e20b9":"# Import dataset\ndf = pd.read_csv('..\/input\/bike-buyers\/bike_buyers.csv')\ndf_raw = df.copy() # save original data, just in case","a4fd94d9":"# Overview\ndf.head()","6b63df92":"# Analyze missing data\ndraw_missing_data_table(df)","d4e96800":"# Delete observation without gender\ndf.drop(df[pd.isnull(df['Gender'])].index, inplace = True)\n\n# Delete observation without cars\ndf.drop(df[pd.isnull(df['Cars'])].index, inplace = True)\n\n# Delete observation without children\ndf.drop(df[pd.isnull(df['Children'])].index, inplace = True)\n\n# Delete observation without marital status\ndf.drop(df[pd.isnull(df['Marital Status'])].index, inplace = True)\n\n# Delete observation without home owner\ndf.drop(df[pd.isnull(df['Home Owner'])].index, inplace = True)","501de786":"# Means age based on marital status\nage_means = df.groupby('Marital Status')['Age'].mean()\n\n# Means income per region\nincome_means = df.groupby('Region')['Income'].mean()","9ee91d2b":"# Transform age_means into a dictionary\nmap_age_means = age_means.to_dict()\n\n# Transform income_means income into a dictionary\nmap_income_means = income_means.to_dict()","9e70aeb9":"# Impute age based on marital status\nidx_nan_age = df.loc[pd.isnull(df['Age'])].index\ndf.loc[idx_nan_age, 'Age'] = df['Marital Status'].loc[idx_nan_age].map(map_age_means)\n\n# Impute income based on region\nidx_nan_income = df.loc[pd.isnull(df['Income'])].index\ndf.loc[idx_nan_income, 'Income'] = df['Region'].loc[idx_nan_income].map(map_income_means)","e669e72c":"# Change purchased bike value into int\ndf['Purchased Bike'] = df['Purchased Bike'].map({'Yes' : 1.0, 'No' : 0.0})","5688e542":"# Transform into categorical \ndf['Marital Status'] = pd.Categorical(df['Marital Status'])\n\n# Plot \nsns.barplot(x = 'Marital Status', y = 'Purchased Bike', data = df)","d8e974b9":"# Plot\nsns.barplot(x = 'Gender', y = 'Purchased Bike', data = df)","088d3bb5":"# Plot\nplt.figure(figsize = (7.5,5))\nsns.boxplot(df['Purchased Bike'], df['Income'])","78724cb1":"# Plot\nsns.barplot(x = 'Children', y = 'Purchased Bike', data = df)","80e104ec":"# Plot\nsns.barplot(x = 'Education', y = 'Purchased Bike', data = df)\nplt.xticks(rotation = 90)","da18f053":"# Plot Education vs Income\nsns.barplot(x = 'Education', y = 'Income', data = df)\nplt.xticks(rotation = 90)","dcf2abe8":"# Plot\nsns.barplot(x = 'Occupation', y = 'Purchased Bike', data = df)\nplt.xticks(rotation = 90)","e3c8db9a":"# Plot\nsns.barplot(x = 'Home Owner', y = 'Purchased Bike', data = df)","bd53b2ab":"# Plot\nsns.barplot(x = 'Cars', y = 'Purchased Bike', data = df)\nplt.xticks(rotation = 90)","5c3cd872":"# Plot\nsns.barplot(x = 'Cars', y = 'Income', data = df)\nplt.xticks(rotation = 90)","c2cf7c7d":"# Transform commute distance into near and far\ndistance = {'0-1 Miles' : 'Near',\n            '1-2 Miles' : 'Near',\n            '2-5 Miles' : 'Near',\n            '5-10 Miles' : 'Far',\n            '10+ Miles' : 'Far'}\n\ndf['Commute Distance'] = df['Commute Distance'].map(distance)\n\n# Plot \nsns.barplot(x = 'Commute Distance', y = 'Purchased Bike', data = df)","7bcb66cf":"# Plot\nsns.barplot(x = 'Region', y = 'Purchased Bike', data = df)","3f1b1f0e":"# Transform age into 3 category\ndf['Age'] = pd.cut(df['Age'], bins = [0,50,100], labels = ['Adult', 'Elder'])\n\n# Plot\nsns.barplot(x = 'Age', y = 'Purchased Bike', data = df)","18b25766":"# Drop unnecessary columns\ndf.drop('ID', axis = 1, inplace = True)\ndf.drop('Gender', axis = 1, inplace = True)\ndf.drop('Home Owner', axis = 1, inplace = True)","8a26bcf3":"df.dtypes","011fd2a6":"# Change object types into categorical\ndf['Education'] = pd.Categorical(df['Education'])\ndf['Occupation'] = pd.Categorical(df['Occupation'])\ndf['Commute Distance'] = pd.Categorical(df['Commute Distance'])\ndf['Region'] = pd.Categorical(df['Region'])","77c057b4":"# Tansform categorical feature into dummy variables\ndf = pd.get_dummies(df, drop_first= True)","3ccde193":"# Checking for null value\ndf.isnull().sum()","8fd641b7":"# Get training and test set\nfrom sklearn.model_selection import train_test_split\n\nX = df.loc[:, df.columns != 'Purchased Bike']\ny = df['Purchased Bike']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","65f46fea":"# Apply box-cox transformation\nfrom scipy.stats import boxcox\n\nX_train_transformed = X_train.copy()\nX_train_transformed['Income'] = boxcox(X_train_transformed['Income'] + 1)[0]\n\nX_test_transformed = X_test.copy()\nX_test_transformed['Income'] = boxcox(X_test_transformed['Income'] + 1)[0]\n","db6c8384":"# Rescale data\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train_transformed_scaled = scaler.fit_transform(X_train_transformed)\nX_test_transformed_scaled = scaler.transform(X_test_transformed)","d40aa60a":"# Get polynomial feature\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree = 2).fit(X_train_transformed)\nX_train_poly = poly.transform(X_train_transformed_scaled)\nX_test_poly = poly.transform(X_test_transformed_scaled)","92384ab7":"# Select feature using chi-squared test\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n## Get score using original model\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\nscores = cross_val_score(classifier, X_train, y_train, cv = 10)\nprint('CV accurary(original) : %.3f +\/- %.3f' % (np.mean(scores), np.std(scores)))\nhighest_score = np.mean(scores)\n\n## Get score using models with feature selection\nfor i in range(1, X_train_poly.shape[1] + 1, 1):\n    # Select i features\n    select = SelectKBest(score_func = chi2, k = i)\n    select.fit(X_train_poly, y_train)\n    X_train_poly_selected = select.transform(X_train_poly)\n\n    # Model with i features selected\n    classifier.fit(X_train_poly_selected, y_train)\n    scores = cross_val_score(classifier, X_train_poly_selected, y_train, cv = 10)\n    print('CV accurary (Number of features = %i) : %.3f +\/- %.3f' % (i, np.mean(scores), np.std(scores)))\n\n    # Save result if best score\n    if np.mean(scores) > highest_score:\n        highest_score = np.mean(scores)\n        std = np.std(scores)\n        k_features_highest_score = i\n    elif np.mean(scores) == highest_score:\n        if np.std(scores) < std:\n            highest_score = np.mean(scores)\n            std = np.std(scores)\n            k_features_highest_score = i \n\n# Print the number of fuatures\nprint('Number of feature when the highest score : %i' % k_features_highest_score)","bebe558b":"# Select feature\nselect = SelectKBest(score_func= chi2, k = k_features_highest_score)\nselect.fit(X_train_poly, y_train)\nX_train_poly_selected = select.transform(X_train_poly)\n\n# Fit model\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train_poly_selected, y_train)\n\n# Model ferpormance\nscores = cross_val_score(classifier, X_train_poly_selected, y_train, cv = 10)\nprint('CV accuracy : %.3f +\/- %.3f' % (np.mean(scores), np.std(scores)))","bd6d5ac7":"# Plotting learning curve\ntitle = 'Learning Curves (Random Forest)'\ncv = 10\nplot_learning_curve(classifier, title, X_train_poly_selected, y_train, ylim = (0.5, 1.01), cv = cv, n_jobs = 1)","77fbe0d8":"#### Age","8f08e20f":"#### Cars","d3d34f21":"#### Income","c2ada395":"# 1. Imports libraries, functions, and dataset","1168f470":"Box-Cox transformations aim to normalize variables. These transformations are an alternative to the typical transformations, such as square root transformations, log transformations, and inverse transformations. The main advantage of Box-Cox transformations is that they optimally normalize the chosen variable. Thus, they avoid the need to randomly try different transformations and automatize the data transformation process.","7054f002":"Learning curves in a nutshell:\n\n- Learning curves allow us to diagnose if the is overfitting or underfitting.\n- When the model overfits, it means that it performs well on the training set, but not not on the validation set. Accordingly, the model is not able to generalize to unseen data. If the model is overfitting, the learning curve will present a gap between the training and validation scores. Two common solutions for overfitting are reducing the complexity of the model and\/or collect more data.\n- On the other hand, underfitting means that the model is not able to perform well in either training or validations sets. In those cases, the learning curves will converge to a low score value. When the model underfits, gathering more data is not helpful because the model is already not being able to learn the training data. Therefore, the best approaches for these cases are to improve the model (e.g., tuning the hyperparameters) or to improve the quality of the data (e.g., collecting a different set of features).\n\nDiscussion of our results:\n\nThe model is underfit. Our final score is about 0.714. We should, improve our model or collecting a different set of features.","596d3ce3":"We think that the percentage of males buying a bike will be higher than that of females, as we know males prefer something related to sports.","9bbf4a97":"We hypothesize that customers with marital status single will have a higher tendency to buy a bike instead of the married customer. Because a bike generally can only be ridden by one person. Married customers tend to buy car or motorcycle instead.","271ec659":"As we can see, about 55% of the customer with marital status single buy the bike. Meanwhile, about 45% of married customers buy the bike. Although the difference is not too far, this still can give us an insight though. So we'll keep this column as an input.","4071aff6":"It looks good. Every columns do not have too many missing value. So we can move to the next step to remove any rows which have missing value, or replace them with certain value. In this case, we can directly remove any rows which have missing value on <b>Gender<\/b>, <b>Cars<\/b>, <b>Children<\/b>, <b>Marital Status<\/b>, and <b>Home Owner<\/b> columns, because we can't replace them to any values unless we have the customer contact, then we ask them about. For <b>Age<\/b> column, we can fill by the mean based on their <b>Marital Status<\/b>, and for <b>Income<\/b> column, we can fill by the mean based on the <b>Region<\/b> they live. For <b>Purchased Bike<\/b> column, the target variable, we gonna transform the value into binary variable.","5cb46dee":"The result shows younger customers will have higher chances to buy a bike instead of the older one.","4b262bb9":"Seems like customers who have no child will be our main target as we see on the chart that customers with no child have tendency to buy a bike.","7a3257ca":"The result shows that customers with higher education will buy the bike. The reason is the higher the education of the customer, the higher income they will earn. This is proved on the Education vs Income chart below.","2a75f766":"# 4. Feature Engineering","002d350f":"#### Occupation","25a6b1b5":"# 5. Fit model for the best feature combination","e789ec01":"#### Children","f028006d":"#### Region","acf33814":"# Exploratory data analysis with python for 'purchased bike' data\nUsing <b>data visualization<\/b> and <b>feature selection<\/b> to make a simple random forest modeling.\n\n[Rifky]() - August 2021\n***","b9859f75":"#### Home Owner","53a8d7c3":"We will transform this column into 2 categories, which are 'Adult' and 'Elder'. People who are older than 50 years old, will be categorized into 'Elder' category, and others will be categorized into 'Adult' category.","3a773a5f":"As we see the result, customers who have 'Near' commute distance have higher chances to buy the bike. So, this customer will become our target for marketing purposes.","c772d479":"Exploratory data analysis is often mentioned as one of the most important steps in the data analysis process. Through this way, we can make a hypothesis by looking relationship between the variables and target variable. As a result, we'll end up with a comprehensive view of the variables that should belong to the prediction model.","0d80f802":"In this commute distance column, we will transform the data into 2 values, which are 'Near' and 'Far'. The distance above 5 miles will be catagorized into 'Far', and the others will be catagorized 'Near'","662719cf":"Our hypothesis is customers with higher income will not buy the bike because they may buy motorcycle or car as they have a high salary or income.","a4b828b0":"Based from the data, we got information that profesional workers tend to buy a bike that others.","db81a6eb":"Based on data, we know the customers who have no car, have higher chances to buy the bike. As we know the reason is that customers who have a car, have a higher income. This is showed on the chart below.","74cb5346":"# 6. Learning Curve","d02e6a6d":"Ups, seems like in this data there is no correlation between gender and target variable (Purchased bike). So, maybe we can drop the column later on.","e44af2bf":"The next step is to perform feature selection. Feature selection is about chosing the relevant information. It is good to add and generate features, but at some point we need to exclude irrelevant features. Otherwise, we will be penalizing the predictive power of our model. You can find a concise introduction to the feature selection subject in Guyon & Elisseeff (2003).\n\nIn this work, we will use a univariate statistics approach. This approach selects features based on univariate statistical tests between each feature and the target variable. The intuition is that features that are independent from the target variable, are irrelevant for classification.\n\nWe will use the chi-squared test for feature selection. This means that we have to choose the number of features that we want in the model. For example, if we want to have three features in our model, the method will select the three features with highest  \u03c72  score.\n\nSince we don't know the ideal number of features, we will test the method with all the possible number of features and choose the number of features with better performance.","6d415b68":"#### Gender","9f5bd5f3":"Based on the chart above, customers who live in Pacific Region have a higher chances to buy the bike.","0db073bb":"#### Commute Distance","5669a69e":"One standard way to enrich our set of features is to generate polynomials. Polynomial expansion creates interactions between features, as well as creates powers (e.g. square of a feature). This way we introduce a nonlinear dimension to our data set, which can improve the predictive power of our model.\n\nWe should scale our features when we have polynomial or interaction terms in our model. These terms tend to produce multicollinearity, which can make our estimates very sensitive to minor changes in the model. Scaling features to a range allow us to reduce multicollinearity and its problems.\n\nTo scale the features, we will transform the data so that it lies between a given minimum and maximum value. We will follow the common practice and say that our minimum value is zero, and our maximum value is one.","89b1412d":"The most important thing after importing the dataset is checking whether our data contain any missing value. There are several strategies to deal with missing data. Some of the most common are:\n\n- Use only valid data, deleting the cases where data is missing.\n- Impute data using values from similar cases or using the mean value.\n- Impute data using model-based methods, in which models are defined to predict the missing values.\n\nIn this case, let's assume the column containing more than 25% missing value, will be removed directly.","06e3283a":"##### Marital Status","81174fbb":"# 3. Exploratory data analysis","965e5234":"#### Education","3c3dfed3":"There is no such different result from customers who have a house and who don't, to buy a bike. So, we may drop this column as an input later on.","56068196":"The plot suggests that those who do not buy the bike have a higher income than the one who buys the bike. So, this proves our hypothesis and we'll use this column as an input.","c51205a2":"I started learning python last year when the pandemic just came. And this is my first exercise to do data analysis and data modeling things. The plan is I would create like this continuously as my practice. So, please give me advice, critics, or anything else that can help me to keep improved. Thank you...\n\nThe whole of analysis is inspired by [Pedro Marcelino]() on his [Exploratory data analysis and feature extraction with python](https:\/\/www.kaggle.com\/pmarcelino\/data-analysis-and-feature-extraction-with-python\/notebook#3.-Unicorn-modelhttps:\/\/www.kaggle.com\/pmarcelino\/data-analysis-and-feature-extraction-with-python\/notebook#3.-Unicorn-model) script.\n\nSo, just let's begin.\n\nThis dataset used has details of 1000 users from different backgrounds and whether or not they buy a bike. The dataset can be downloaded on this [link](https:\/\/www.kaggle.com\/heeraldedhia\/bike-buyerssis-and-feature-extraction-with-python\/notebook#3.-Unicorn-model).\n\nThe first step is importing some libraries and general functions.","83510592":"# 2. Handling the missing data","78f907c2":"After importing the dataset, we can look to get a clear and concise picture of what is there and what we can do with it.\nHere is column definitions and quick thoughts:\n- <b>ID<\/b>. Unique identification of the customer. It shouldn't be necessary for a machine learning model.\n- <b>Marital Status<\/b>. The marital status of customer (single or married). Ready to used.\n- <b>Gender<\/b> Gender of the customer. Ready to used.\n- <b>Income<\/b> Customer's income. Ready to used.\n- <b>Children<\/b> Number of children the customer has. Ready to used.\n- <b>Education<\/b> The last customer's education. Ready to used\n- <b>Occupation<\/b> Customer's occupation. Ready to used\n- <b>Home Owner<\/b> Information whether the customer has a house or not. Ready to used.\n- <b>Cars<\/b> Number of cars the customer has. Ready to used.\n- <b>Commute Distance<\/b> The distance between the customer's live and their office. It needs to be parsed.\n- <b>Region<\/b> Region where the customer live. Ready to used.\n- <b>Age<\/b> Age of the customer. Ready to used\n- <b>Purchased Bike<\/b> (Target variable) The information whether the customer buys the bike or not. It seems we should transform it into binary variable. So, the later 'Purchased Bike' column will contain '0' for 'No' and '1' for 'Yes.\n\nWe already have a set of features that we can easily use in our machine learning model."}}