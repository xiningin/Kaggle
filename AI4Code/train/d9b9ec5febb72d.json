{"cell_type":{"d1eda7a4":"code","24e5e97b":"code","4f4ae842":"code","693d0b2b":"code","21199b13":"code","1f3d6e5a":"code","30eb4771":"code","b0707f7e":"code","cbb439b7":"code","8b0ba9c2":"code","c471c380":"code","807dfc7a":"code","7a7cf096":"code","79f696d8":"code","45482476":"code","a13838bd":"code","d647f921":"code","09b910c3":"code","1137bb54":"code","228b4de0":"code","6084abea":"code","65630c1d":"code","95783fd6":"code","24f30eec":"code","2e048bb8":"code","beb29523":"code","d225dfae":"code","9d0474f4":"code","6ae1d215":"code","1c6ccb09":"code","b27d9fe2":"code","2d6340e0":"code","6ef1df79":"code","21d0170f":"code","2bd5e0c7":"code","7639e86d":"markdown","36e30d13":"markdown","01205489":"markdown","850d8d87":"markdown","09ce3b82":"markdown","c4913a7a":"markdown","f3ee6dda":"markdown","00107864":"markdown","d7d36dd3":"markdown","7352f752":"markdown","3753f749":"markdown","1fe1f379":"markdown","1004c822":"markdown","8342c061":"markdown","6c81c0ba":"markdown","c94c5175":"markdown","ab6930fc":"markdown","2ba68f63":"markdown","2630cb54":"markdown","c8374642":"markdown"},"source":{"d1eda7a4":"!pip install tez -q\n!pip install iterative-stratification -q","24e5e97b":"import gc\nimport os\nimport tez\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import AdamW, AutoTokenizer, AutoModel\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\nimport optuna\nimport time\n\n# \u0414\u043b\u044f \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport nltk\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport re","4f4ae842":"df = pd.read_csv('..\/input\/toxic-comments\/train.csv')\ndf.columns","693d0b2b":"def washing_machine(comments): # \u0427\u0438\u0441\u0442\u043a\u0430 \u0442\u0435\u043a\u0441\u0442\u0430 \u043e\u0442 \u043c\u0443\u0441\u043e\u0440\u0430\n    corpus=[]\n    for i in tqdm(range(len(comments))):\n        comment = re.sub('[^a-zA-Z]', ' ', comments[i])\n        comment = comment.lower()\n        comment = comment.split()\n        stemmer = SnowballStemmer('english')\n        lemmatizer = WordNetLemmatizer()\n        all_stopwords = stopwords.words('english')\n        comment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\n        comment = [lemmatizer.lemmatize(word) for word in comment]\n        comment = ' '.join(comment)\n        corpus.append(comment)\n\n    return corpus\n\ndf['cleaned_comment_text'] = washing_machine(df['comment_text'].values)","21199b13":"def create_folds(data, num_splits):\n    data.loc[:,'kfold'] = -1\n    X = data['cleaned_comment_text']\n    y = data[['toxic', 'severe_toxic', 'obscene', 'threat',\n           'insult', 'identity_hate']]\n    mskf = MultilabelStratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n    \n    for fold, (trn_, val_) in enumerate(mskf.split(X,y)):\n        data.loc[val_,'kfold'] = fold\n        \n    return data","1f3d6e5a":"df_5 = create_folds(df.copy(), 5)\ndf_5.to_csv('5folds.csv', index=False)\n\ndf_10 = create_folds(df.copy(), 10)\ndf_10.to_csv('10folds.csv', index=False)\n\ndf_5.head()","30eb4771":"df_5['kfold'].value_counts()","b0707f7e":"class Config:\n    model_name = 'roberta-base'\n    batch_size = 96\n    lr = 1e-4\n    weight_decay = 0.01\n    scheduler = 'CosineAnnealingLR'\n    early_stopping_epochs = 1\n    epochs = 15 # 20\n    max_length = 128\n    #max_length = 196\n    num_folds = 2","cbb439b7":"class ToxicDataset:\n    def __init__(self, data, tokenizer,  max_len=196):\n        self.comments = data['comment_text'].values\n        self.tokenizer = tokenizer\n        self.targets = data[[\n            'toxic', 'severe_toxic', 'obscene',\n            'threat','insult', 'identity_hate']].values\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.comments)\n    \n    def __getitem__(self, idx):\n        \n        tokenized = self.tokenizer.encode_plus(\n            self.comments[idx],\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length'\n        )\n        \n        input_ids = tokenized['input_ids']\n        attention_mask = tokenized['attention_mask']\n        \n        toxic, severe_toxic, obscene, threat, insult, identity_hate = self.targets[idx]\n\n        return {\n            'input_ids' : torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask' : torch.tensor(attention_mask, dtype=torch.long),\n            'toxic' : torch.tensor(toxic, dtype=torch.float),\n            'severe_toxic' : torch.tensor(severe_toxic, dtype=torch.float),\n            'obscene' : torch.tensor(obscene, dtype=torch.float),\n            'threat' : torch.tensor(threat, dtype=torch.float),\n            'insult' : torch.tensor(insult, dtype=torch.float),\n            'identity_hate' : torch.tensor(identity_hate, dtype=torch.float)\n        }","8b0ba9c2":"class ToxicModel(nn.Module):\n    def __init__(self, args, model_name):\n        super(ToxicModel, self).__init__()\n        self.args = args\n        self.model = AutoModel.from_pretrained(self.args.model_name)\n        self.dropout = nn.Dropout(p=0.2)\n        self.toxic = nn.Linear(768, 1)\n        self.stoxic = nn.Linear(768, 1)\n        self.obs = nn.Linear(768, 1)\n        self.threat = nn.Linear(768, 1)\n        self.insult = nn.Linear(768, 1)\n        self.id_hate = nn.Linear(768, 1)\n    \n        \n    def forward(self, input_ids, attention_mask):\n        \n        out = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=False\n        )\n        \n        out = self.dropout(out[1])\n        \n        toxic = self.toxic(out)\n        stoxic = self.stoxic(out)\n        obs = self.obs(out)\n        threat = self.threat(out)\n        insult = self.insult(out)\n        id_hate = self.id_hate(out)\n\n        return [toxic, stoxic, obs, threat, insult, id_hate]\n        ","c471c380":"def loss_fn(outputs, targets):\n    o1, o2, o3, o4, o5, o6 = outputs\n    t1, t2, t3, t4, t5, t6 = targets\n    l1 = nn.BCEWithLogitsLoss()(o1, t1.view(-1,1))\n    l2 = nn.BCEWithLogitsLoss()(o2, t2.view(-1,1))\n    l3 = nn.BCEWithLogitsLoss()(o3, t3.view(-1,1))\n    l4 = nn.BCEWithLogitsLoss()(o4, t4.view(-1,1))\n    l5 = nn.BCEWithLogitsLoss()(o5, t5.view(-1,1))\n    l6 = nn.BCEWithLogitsLoss()(o6, t6.view(-1,1))\n    \n    total_loss = (l1+l2+l3+l4+l5+l6)\/6\n    \n    return total_loss","807dfc7a":"def metrics(outputs, targets):\n    auc_scores=[]\n    for o, t in zip(outputs, targets):\n        o = o.cpu().detach().numpy()\n        t = t.cpu().detach().numpy()\n        auc = roc_auc_score(o, t)\n        auc_scores(auc)\n\n    return np.mean(auc_scores)","7a7cf096":"def train_epoch(args, dataloader, model, optimizer, scheduler, epoch):\n    model.train()\n    epoch_loss = 0.0\n    running_loss = 0.0\n    dataset_size=0\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        optimizer.zero_grad()\n        \n        input_ids = data['input_ids'].cuda()\n        attention_mask = data['attention_mask'].cuda()\n        toxic = data['toxic'].cuda()\n        severe_toxic = data['severe_toxic'].cuda()\n        obscene = data['obscene'].cuda()\n        threat = data['threat'].cuda()\n        insult = data['insult'].cuda()\n        identity_hate = data['identity_hate'].cuda()\n        \n        batch_size = args.batch_size\n        \n        targets = (toxic, severe_toxic, obscene, threat, insult, identity_hate)\n        outputs = model(input_ids, attention_mask)\n        \n        \n        loss = loss_fn(outputs, targets)\n\n        \n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        epoch_loss = running_loss \/ dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    return epoch_loss","79f696d8":"def validation(args, dataloader, model):\n    model.eval()\n    epoch_loss = 0.0\n    running_loss = 0.0\n    dataset_size=0\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    with torch.no_grad():\n        for step, data in bar:\n            batch_size = args.batch_size\n\n            input_ids = data['input_ids'].cuda()\n            attention_mask = data['attention_mask'].cuda()\n            toxic = data['toxic'].cuda()\n            severe_toxic = data['severe_toxic'].cuda()\n            obscene = data['obscene'].cuda()\n            threat = data['threat'].cuda()\n            insult = data['insult'].cuda()\n            identity_hate = data['identity_hate'].cuda()\n\n            targets = (toxic, severe_toxic, obscene, threat, insult, identity_hate)\n\n            outputs = model(input_ids, attention_mask)\n\n            loss = loss_fn(outputs, targets)\n\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n\n            epoch_loss = running_loss \/ dataset_size\n\n            bar.set_postfix(Valid_Loss=epoch_loss,\n                            Stage='Validation') \n    return epoch_loss","45482476":"def get_optimizer(args, params):\n    opt = AdamW(params, lr=args.lr, weight_decay=args.weight_decay)\n    return opt","a13838bd":"def get_scheduler(args, optimizer):\n    if args.scheduler == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=500, \n                                                   eta_min=1e-6)\n    else:\n        schduler = None\n    return scheduler","d647f921":"def run(data, fold, args=None, save_model=False):\n    print('-'*50)\n    print(f'Fold : {fold}')\n    print('-'*50)\n    \n    if args is None:\n        args = Config()\n        \n    start = time.time()\n    model = ToxicModel(args, args.model_name)\n    model = model.cuda()\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    \n    optimizer = get_optimizer(args, model.parameters())\n    scheduler = get_scheduler(args, optimizer)\n    \n    train = data[data['kfold']!=fold]\n    valid = data[data['kfold']==fold]\n    \n    train_dataset = ToxicDataset(train, tokenizer, args.max_length)\n    valid_dataset = ToxicDataset(valid, tokenizer, args.max_length)\n    \n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size)\n    valid_loader = DataLoader(valid_dataset, batch_size=2*args.batch_size)\n    \n    best_val_loss = np.inf\n    patience_counter = 0\n\n    for epoch in range(args.epochs):\n        \n        train_loss = train_epoch(args, train_loader, model, optimizer, scheduler, epoch)\n        valid_loss = validation(args, valid_loader, model)\n        \n        if valid_loss <= best_val_loss:\n            print(f\"Validation Loss Improved ({best_val_loss} ---> {valid_loss})\")\n            best_val_loss = valid_loss\n            \n            if save_model:\n                PATH = f\"model_fold_{fold}.bin\"\n                torch.save(model.state_dict(), PATH)\n                print(f\"----------Model Saved----------\")\n        \n        else:\n            patience_counter += 1\n            print(f'Early stopping counter {patience_counter} of {args.early_stopping_epochs}')\n            if patience_counter == args.early_stopping_epochs:\n                print('*************** Early Stopping ***************')\n                break\n    \n    end = time.time()\n    time_elapsed = end-start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_val_loss))\n    \n    del model, train_loader, valid_loader\n    gc.collect()\n    return best_val_loss","09b910c3":"df = pd.read_csv('5folds.csv')\ndf.head()","1137bb54":"df = df.dropna()","228b4de0":"df.isnull().sum()","6084abea":"# df_trial = df[:100]","65630c1d":"# def objective(trial):\n#     args = Config()\n#     args.epochs=1\n#     args.lr = trial.suggest_uniform('lr',1e-6, 1e-3)\n#     all_losses = []\n#     for fold in range(5):\n#         temp_loss = run(df_trial, fold, args=args)\n#         all_losses.append(temp_loss)\n    \n#     return np.mean(all_losses)","95783fd6":"# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=10)\n\n# print('Best Trial:')\n# trial_ = study.best_trial\n# print(trial_.values)","24f30eec":"# trial_.params['lr']","2e048bb8":"args=Config()\nargs.lr = 0.0005149849355804644\n\n# \u041f\u0440\u043e\u0431\u0435\u0433\u0430\u0435\u043c\u0441\u044f \u043f\u043e \u0432\u0441\u0435\u043c \u0444\u043e\u043b\u0434\u0430\u043c\n# run(df, fold=0, save_model=True, args=args)\n\nfor fold in tqdm(range(5)):\n    run(df, fold=fold, save_model=True, args=args)\n","beb29523":"# com1 = washing_machine(df['less_toxic'].values)\n# com2 = washing_machine(df['more_toxic'].values)","d225dfae":"def get_predictions(args, dataloader, model):\n    model.eval()\n    all_outputs=[]\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    with torch.no_grad():\n        for step, data in bar:\n            batch_size = args.batch_size\n\n            input_ids = data['input_ids'].cuda()\n            attention_mask = data['attention_mask'].cuda()\n            outputs = model(input_ids, attention_mask)\n            outputs = outputs.cpu().detach().numpy()\n            outputs = [sum(output) for output in outputs]\n            all_outputs.append(outputs)\n\n            bar.set_postfix(Stage='Inference') \n    return np.hstack(all_outputs)","9d0474f4":"def inference(data):\n    args=Config()\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    base_path='.\/'\n    \n    dataset = ToxicDataset(data, tokenizer)\n    dataloader = DataLoader(dataset, batch_size=16*args.batch_size)\n    \n    final_preds = []\n    \n    num_folds = args.num_folds\n    \n    for fold in range(num_folds):\n        model = ToxicModel(args)\n        model = model.cuda()\n        path = base_path + f'model_fold_{fold}.bin'\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {fold+1}\")\n        preds = get_predictions(args, dataloader, model)\n        final_preds.append(np.vstack(preds))\n        del model\n        gc.collect()\n    return np.hstack(sum(final_preds)\/num_folds)","6ae1d215":"# pred1 = inference(com1)\n# pred2 = inference(com2)","1c6ccb09":"df = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')","b27d9fe2":"comments = washing_machine(df['text'].values)","2d6340e0":"class Config:\n    model_name = 'roberta-base'\n    batch_size = 96\n    lr = 1e-4\n    weight_decay = 0.01\n    scheduler = 'CosineAnnealingLR'\n    early_stopping_epochs = 1\n    epochs = 15 # \u0431\u044b\u043b\u043e 20\n    max_length = 196 # = 128\n    num_folds = 2\n    \nclass ToxicModel(nn.Module):\n    def __init__(self, args):\n        super(ToxicModel, self).__init__()\n        self.args = args\n        self.model = AutoModel.from_pretrained(self.args.model_name)\n        self.dropout = nn.Dropout(p=0.2)\n        self.toxic = nn.Linear(768, 1)\n        self.stoxic = nn.Linear(768, 1)\n        self.obs = nn.Linear(768, 1)\n        self.threat = nn.Linear(768, 1)\n        self.insult = nn.Linear(768, 1)\n        self.id_hate = nn.Linear(768, 1)\n    \n        \n    def forward(self, input_ids, attention_mask):\n        \n        out = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=False\n        )\n        \n        out = self.dropout(out[1])\n        \n        toxic = self.toxic(out)\n        stoxic = self.stoxic(out)\n        obs = self.obs(out)\n        threat = self.threat(out)\n        insult = self.insult(out)\n        id_hate = self.id_hate(out)\n\n        return torch.cat([toxic, stoxic, obs, threat, insult, id_hate], dim=-1)\n            \n    \nclass ToxicDataset:\n    def __init__(self, comments, tokenizer, max_len=196):\n        self.comments = comments\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.comments)\n    \n    def __getitem__(self, idx):\n        \n        tokenized = self.tokenizer.encode_plus(\n            self.comments[idx],\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length'\n        )\n        \n        input_ids = tokenized['input_ids']\n        attention_mask = tokenized['attention_mask']\n        \n\n        return {\n            'input_ids' : torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask' : torch.tensor(attention_mask, dtype=torch.long)\n        }","6ef1df79":"pred = inference(comments)","21d0170f":"df['score'] = pred","2bd5e0c7":"df[['comment_id', 'score']].to_csv('submission.csv', index=False)","7639e86d":"# Optuna","36e30d13":"# Aleron's review \ud83d\udc48\n\n\n### \u0411\u0443\u0434\u044c <span style=\"color:red\">\u0412\u043d\u0438\u043c\u0430\u0442\u0435\u043b\u044c\u043d\u0435\u0435<\/span> \u0415\u0441\u0442\u044c \u043d\u0435\u0434\u043e\u0447\u0435\u0442\u044b:)\n\n* \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043f\u0440\u043e\u0448\u043b\u044b\u0445 \u043b\u0435\u0442.\n* \u041d\u0435\u0441\u043e\u0432\u0441\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u043e \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u0430 \u043b\u043e\u0441 \u0444\u0443\u043d\u043a\u0446\u0438\u044f. \n* \u041d\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442\u0441\u044f \u0442\u043e\u043a\u0435\u043d\u0430\u0439\u0437\u0435\u0440\n* \u041d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0444\u0430\u0437\u044b\u0439 \u043d\u0430\u0434\u043e \u0432\u044b\u043d\u0435\u0441\u0442\u0438 \u0432 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u043d\u043e\u0443\u0442\u0431\u0443\u043a (\u0447\u0438\u0441\u0442\u043a \u0438 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445)\n\n\nModel used : RoBERTa-base (RoBERTa-large is way too big and takes a whole lot of time training O.o)\n\n# \u0418\u043d\u0444\u0435\u0440\u0435\u043d\u0441 \u043c\u043e\u0434\u0435\u043b\u0438 \u0442\u0443\u0442: [ \u0418\u043d\u0444\u0435\u0440\u0435\u043d\u0441 | Final_blending_all_models | \ud83d\ude0e](https:\/\/www.kaggle.com\/aleron751\/final-blending-all-models\/edit) \n\n**\u0411\u044b\u043b\u043e \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043e \u043d\u0430 \u043e\u0434\u043d\u043e\u043c \u0444\u043e\u043b\u0434\u0435 0.798 \u043d\u0430 LB (made public)**","01205489":"# Config","850d8d87":"# Using Toxic Comment Classification\n\nHere I have used the multi-label Toxic Comment Classification Dataset (by Jigsaw).\n\nI have used Roberta-base to predict 6 different outputs each belonging to one label (toxic, severe_toxic, etc...)\n\nI have not passed the output logits through any sigmoid function. Just used the outputs for these 6 labels and taken a linear average of them.\n\nThat's what I have used as the score for comparison.\n\n#### I have used:\n\n* BCELogitsLoss as the loss funtion : It already applies a sigmoid function on the output before calculating the loss\n* Early stooping with a patience of 1 : You can modify according to your need\n* Used CosineAnnealingLR scheduler : It changes the LR per step following a cosine funtion.\n","09ce3b82":"# Optimizer","c4913a7a":"# Dataset","f3ee6dda":"# \u0427\u0438\u0441\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 | \u0415\u0433\u043e\u0440 \ud83d\udd25\ud83d\udd25","00107864":"# Training and validation Loop","d7d36dd3":"# Validation Epoch","7352f752":"# \u0413\u043e\u0442\u043e\u0432\u0438\u043c submission | Inference","3753f749":"# Loss Function","1fe1f379":"# Read the data","1004c822":"It has 6 output heads giving outputs to the 6 different labels to determine.\n\nThis 6 different heads are attached on top the roberta-base (for now) will implement roberta-large too.","8342c061":"I have used 5 folds that I have cleaned using this notebook: [Multi-Label Stratified K-fold | Toxic Comments](https:\/\/www.kaggle.com\/kishalmandal\/multi-label-stratified-k-fold-toxic-comments)\n\nThe data is the same data used in the toxicity classification challenge by Jigsaw","6c81c0ba":"## The final layer of the model looks like this:\n\n ![NN](https:\/\/user-images.githubusercontent.com\/74188336\/141213710-3a1b7473-8436-4683-841e-64d87789f47e.png)","c94c5175":"# Training epoch","ab6930fc":"# Scheduler","2ba68f63":"# The Model","2630cb54":"# \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u043c \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 | \u0410\u043d\u0434\u0440\u0435\u0439 \ud83d\udd25","c8374642":"# Run training"}}