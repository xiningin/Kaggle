{"cell_type":{"6973edbf":"code","6f1d177e":"code","b836af93":"code","0931ff46":"code","70004ac4":"code","e3a04c6d":"code","8cbf72be":"code","bcbf6ec5":"code","5ca7d5a6":"code","300e3037":"code","cfb69836":"code","464a34fa":"code","b4f6e9ba":"code","016c514d":"code","a69a78b4":"code","e2471592":"code","ec674de8":"code","9a75cd80":"code","e551c544":"code","81270cf5":"code","b3597593":"code","91ad8d34":"markdown","55967e09":"markdown","72885c43":"markdown","b6eeebcd":"markdown","65dc0731":"markdown","b627a1f3":"markdown","8627f320":"markdown","dd992de2":"markdown","154dab18":"markdown","8265f0b0":"markdown","bb5d78a3":"markdown","84e70566":"markdown","47ce7dc4":"markdown","b84b6ead":"markdown","f9c8857d":"markdown","e93516c4":"markdown","5e7561e4":"markdown","0e9b2121":"markdown","d8bdb1d2":"markdown","73693003":"markdown"},"source":{"6973edbf":"import pandas as pd\nimport re\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport nltk.corpus\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob","6f1d177e":"data=pd.read_csv(\"..\/input\/tweets-for-georgefloydfuneral\/George Floyd Twitter Data.csv\")\ndata.head()","b836af93":"data_copy=data.copy()","0931ff46":"def cleaned_data(text):\n    clean=re.sub(\"http\\S+\",\"\",text)\n    clean=clean.lower()\n    clean=re.sub(\"\\d\",\"\",clean)\n    clean=re.sub(\"[^a-z]\",\" \",clean)\n    clean=re.sub(\"\\s{2,}\",\" \",clean)\n    clean=clean.lstrip()\n    clean=re.sub(\"don t\",\"do not\",clean)\n    clean=re.sub(\"amp\",\"\",clean)\n    clean=re.sub(\"i m\",\"i am\",clean)\n    clean=re.sub(\"didn t\",\"did not\",clean)\n    clean=re.sub(\"couldn t\",\"could not\",clean)\n    clean=re.sub(\"son s\",\"son\",clean)\n    #clean=re.sub(\"n\",\"and\",clean)\n    clean=re.sub(\"pu sy\",\"pussy\",clean)\n    clean=re.sub(\"fu ker\",\"fucker\",clean)\n    clean=re.sub(\"can t\",\"can not\",clean)\n    clean=re.sub(\"sh t\",\"shit\",clean)\n    #clean=re.sub(\"inj\",\"injustice\",clean)\n    clean=re.sub(\"you re\",\"you are\",clean)\n    clean=re.sub(\"doesn t\",\"does not\",clean)\n    clean=re.sub(\"wasn t\",\"was not\",clean)\n    #clean=re.sub(\"presi\",\"president\",clean)\n    clean=re.sub(\"let s\",\"let us\",clean)\n    clean=re.sub(\"people s\",\"people\",clean)\n    clean=re.sub(\"birble\",\"bible\",clean)\n    clean=re.sub(\"servi\",\"service\",clean)\n    clean=re.sub(\"it s\",\"it is\",clean)\n    clean=re.sub(\"we re\",\"we are\",clean)\n    clean=re.sub(\"ameri\",\"america\",clean)\n    clean=re.sub(\"b c\",\"because\",clean)\n    #clean=re.sub(\"cha\",\"change\",clean)\n    clean=re.sub(\"sacri\",\"sacrifice\",clean)\n    clean=re.sub(\"cant\",\"can not\",clean)\n    clean=re.sub(\"today s\",\"today\",clean)\n    clean=re.sub(\"floyd s\",\"floyd\",clean)\n    clean=clean=re.sub(\"nationalguard\",\"national guard\",clean)\n    clean=re.sub(\"minori\",\"minority\",clean)\n    clean=re.sub(\"georgeflyod s\",\"georgefloyd\",clean)\n    #clean=re.sub(\"p\",\"protest\",clean)\n    #clean=re.sub(\"nd\",\"second\",clean)\n    clean=re.sub(\"shouldn t\",\"should not\",clean)\n    #clean=re.sub(\"racis\",\"racism\",clean)\n    clean=re.sub(\"how d\",\"how did\",clean)\n    #clean=re.sub(\"mu\",\"music\",clean)\n    clean=re.sub(\"wanna\",\"want to\",clean)\n    clean=re.sub(\"addre\",\"address\",clean)\n    clean=re.sub(\"isince\",\"since\",clean)\n    clean=re.sub(\"didid\",\"did\",clean)\n    clean=re.sub(\"ya ll\",\"you all\",clean)\n    clean=re.sub(\"ican notbreathe\",\"i can not breathe\",clean)\n    clean=re.sub(\"caign\",\"campaign\",clean)\n    clean=re.sub(\"caigning\",\"campaigning\",clean)\n    clean=re.sub(\"didemocrats\",\"democrats\",clean)\n    clean=re.sub(\"i amean\",\"i mean\",clean)\n    clean=re.sub(\"americacans\",\"americans\",clean)\n    clean=re.sub(\"concealin\",\"concealing\",clean)\n    clean=re.sub(\"notell\",\"tell\",clean)\n    clean=re.sub(\"what s\",\"what is\",clean)\n    clean=re.sub(\"americaca\",\"america\",clean)\n    clean=re.sub(\"\\s{2,}\",\" \",clean)\n    return clean\ndata[\"cleaned_tweets\"]=data[\"Tweets\"].apply(cleaned_data)","70004ac4":"data.head(10)","e3a04c6d":"data.drop(columns=[\"Unnamed: 0\"],inplace=True)","8cbf72be":"data.head()","bcbf6ec5":"stop=stopwords.words('english')\ndata[\"cleaned_tweets\"]=data[\"cleaned_tweets\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","5ca7d5a6":"data.head()","300e3037":"data[\"clean_cleaned\"]=data[\"cleaned_tweets\"].apply(lambda x: nltk.word_tokenize(x))","cfb69836":"data.head()","464a34fa":"def word_lemmatizer(text):\n    lem_text = [WordNetLemmatizer().lemmatize(i,pos='v') for i in text]\n    return lem_text\ndata[\"clean_cleaned_tweets\"]=data[\"clean_cleaned\"].apply(lambda x: word_lemmatizer(x))\ndata[\"clean_cleaned_tweets\"]=data[\"clean_cleaned_tweets\"].apply(lambda x: ' '.join(x))","b4f6e9ba":"plt.figure(figsize=(12,6))\nfreq=pd.Series(' '.join(data[\"cleaned_tweets\"]).split()).value_counts()[:20]\nfreq.plot(kind=\"bar\")\nplt.title(\"Most frequent words in the cleaned tweets\",size=20)","016c514d":"cloud=WordCloud(colormap=\"OrRd_r\",width=700,height=350).generate(str(data[\"clean_cleaned_tweets\"]))\nfig=plt.figure(figsize=(12,18))\nplt.axis(\"off\")\nplt.imshow(cloud)","a69a78b4":"def analyze_sentiment(tweet):\n    analysis = TextBlob(tweet)\n    if analysis.sentiment.polarity > 0:     #### For positive sentiment\n        return 1\n    elif analysis.sentiment.polarity == 0:  ### Neutral\n        return 0\n    else:\n        return -1                           #### Negative sentiment\ndata['SA'] = data[\"clean_cleaned_tweets\"].apply(lambda x: analyze_sentiment(x))","e2471592":"data.head()","ec674de8":"print(len(data[data[\"SA\"]==0]))  ## Neutral\nprint(len(data[data[\"SA\"]==-1])) ## Negative\nprint(len(data[data[\"SA\"]==1]))  ## Positive","9a75cd80":"print(len(data[data[\"SA\"]==0])\/len(data[\"SA\"])*100)\nprint(len(data[data[\"SA\"]==-1])\/len(data[\"SA\"])*100)\nprint(len(data[data[\"SA\"]==1])\/len(data[\"SA\"])*100)","e551c544":"cloud=WordCloud().generate(str(data[data[\"SA\"]==-1][\"clean_cleaned_tweets\"]))\nfig=plt.figure(figsize=(12,14))\nplt.axis(\"off\")\nplt.imshow(cloud)","81270cf5":"cloud=WordCloud().generate(str(data[data[\"SA\"]==1][\"clean_cleaned_tweets\"]))\nfig=plt.figure(figsize=(12,14))\nplt.axis(\"off\")\nplt.imshow(cloud)","b3597593":"descending_order=data[\"SA\"].value_counts().sort_values(ascending=False).index\nsns.catplot(x=\"SA\",data=data,kind=\"count\",height=5,aspect=2,order=descending_order)","91ad8d34":"Finally we will show graphically how many sentiments are negative, how many positive and how many neutral.","55967e09":"Now we will use \"textblob\" for doing our sentiment analysis. ","72885c43":"But as can be seen above much cleaning has been done. So we are ready to go ahead. Also we will remove the column Unnamed:0, as it is unnecessary.","b6eeebcd":"Now we will tokenize our text, it is also an important part of the text preprocessing.","65dc0731":"![floyd-funeral-768x432.jpg](attachment:floyd-funeral-768x432.jpg)","b627a1f3":"To verify whether Unnamed column has been removed or not, we will use head method again.","8627f320":"First of all we will import all the necessary libraries for our analysis.","dd992de2":"Now we will do some text normalization using lemmatization. Lemmatization is done to bring all the words into its base form, like bringing \"walking\", \"walks\", \"walked\" to \"walk\".","154dab18":"So in the above we can see that we have created a column called \"SA\", which are the \"Sentiments\" for the text data. Now we will see how many of them are positive, how many of them are negative and how many of them are neutral.","8265f0b0":"Now we will move to our next text processing step, which is removing stop words. Stop words are the words which are pretty common in sentences like \"is\", \"the\", \"are\". These words need to be removed as they don't add much value to our analysis.","bb5d78a3":"Next we will see the percentage value for these.","84e70566":"As we all know what is going on in America, protest after the death of George Floyd, and I also fully stand for equality and I am also against what happened. So I thought of extracting twitter data for the hashtag #GeorgeFloydFuneral and do some analysis, and my intention is to do sentiment analysis for the tweets for this hashtag.","47ce7dc4":"So to check whether cleaning has been done or not, we will check the first 10 rows.","b84b6ead":"So using the above step, all the stop words has been removed, and to confirm we will again use head method.","f9c8857d":"Now we will clean the data a bit, as we all know twitter data is not use to be very clean. So cleaning is very important before moving ahead.","e93516c4":"Now we will generate wordcloud for the negative sentiments.","5e7561e4":"First we will show 20 most frequent words.","0e9b2121":"Now we will do some vizualization.","d8bdb1d2":"Now we will do for the positive sentiments.","73693003":"# Sentiment Analysis for the #GeorgeFloydFuneral"}}