{"cell_type":{"985166f9":"code","0d4c49f6":"code","b773e75d":"code","8824f62b":"code","27858cba":"code","d91570b0":"code","c4e3ba55":"code","6f6c4f03":"code","73b53b04":"code","9daef2be":"code","78d3d4ef":"code","f239b933":"code","6e04c017":"code","b6e09c54":"code","6c992456":"code","e1445341":"code","cc8257ae":"code","30e23a1b":"code","97f83781":"code","b9ea2acc":"code","8e5f766b":"code","cf3816bd":"code","f5ce4179":"markdown","14bb6c90":"markdown","454130f7":"markdown","98cd3857":"markdown","1288b7e1":"markdown","ee30cc9b":"markdown","3b8e5df6":"markdown","f9159f96":"markdown","885b2155":"markdown","12a5e3c4":"markdown","2032411f":"markdown","2fb2817b":"markdown"},"source":{"985166f9":"! pip install ..\/input\/mmdetectionv260\/addict-2.4.0-py3-none-any.whl","0d4c49f6":"! pip install ..\/input\/mmdetectionv260\/mmcv_full-latesttorch1.6.0cu102-cp37-cp37m-manylinux1_x86_64.whl","b773e75d":"! pip install ..\/input\/mmdetectionv260\/mmpycocotools-12.0.3-cp37-cp37m-linux_x86_64.whl","8824f62b":"! pip install ..\/input\/mmdetectionv260\/mmdet-2.6.0-py3-none-any.whl","27858cba":"import copy\nimport json\nimport os.path as osp\nfrom glob import glob\nfrom tqdm import tqdm\n\n# Check Pytorch installation\nimport torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\n\nfrom sklearn.model_selection import train_test_split\n\n# Check MMDetection installation\nimport mmdet\nprint(mmdet.__version__)\n\n# Check mmcv installation\nimport mmcv\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nfrom mmcv import Config\nprint(get_compiling_cuda_version())\nprint(get_compiler_version())\n\nfrom mmdet.datasets import build_dataset, CocoDataset\nfrom mmdet.models import build_detector\nfrom mmdet.datasets.builder import DATASETS\nfrom mmdet.datasets.custom import CustomDataset\nfrom mmdet.apis import train_detector, set_random_seed, init_detector, inference_detector\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nimport nflimpact","d91570b0":"video_labels = pd.read_csv('..\/input\/nfl-impact-detection\/train_labels.csv')\nvideo_labels_with_impact = video_labels[video_labels['impact'] > 0]\nfor row in tqdm(video_labels_with_impact[['video','frame','label']].values):\n    frames = np.array([-4, -3, -2,-1, 1,2, 3, 4])+row[1]\n    video_labels.loc[(video_labels['video'] == row[0]) \n                                 & (video_labels['frame'].isin(frames))\n                                 & (video_labels['label'] == row[2]), 'impact'] = 1\nvideo_labels['image_name'] = video_labels['video'].str.replace('.mp4', '') + '_' + video_labels['frame'].astype(str) + '.jpg'\nvideo_labels = video_labels[video_labels.groupby('image_name')['impact'].transform(\"sum\") > 0].reset_index(drop=True)\nvideo_labels.fillna({'impact': 2}, inplace=True)","c4e3ba55":"train_labels = video_labels.copy()\ntrain_labels.head()","6f6c4f03":"unique_image_lists = list(train_labels['image_name'].unique())","73b53b04":"train_images, valid_images = train_test_split(unique_image_lists, test_size = 0.05, random_state=42)\ntrain_isin_filter = train_labels['image_name'].isin(train_images)\nvalid_isin_filter = train_labels['image_name'].isin(valid_images)\n\ntrain_df = train_labels[train_isin_filter]\nvalid_df = train_labels[valid_isin_filter]\ntrain_df = train_df.reset_index(drop=True)\nvalid_df = valid_df.reset_index(drop=True)\nprint(f'train labels: {len(train_df)}, valid labels {len(valid_df)}')","9daef2be":"def gen_classes(CLASSES):\n    classes = list()      \n    \n    for i, CLASS in enumerate(CLASSES):\n        single_class = {} \n        single_class['id'] = i + 1\n        single_class['name'] = CLASS\n        classes.append(single_class)\n    return classes","78d3d4ef":"def gen_objs(df, debug = False):\n    \n    if debug:\n        \n        df = df[:int(len(df)*0.05)]\n        \n    \n    img_lists = list(df['image_name'].unique())\n    imgs = list()\n    objs = list()   \n    \n    ## gen images information\n    for i in tqdm(range(len(img_lists))):\n        '''\n        \n        I just notice that all images were preprocessed image size 720x1280\n        If you want to check real image size, use this code below\n        \n        img = cv2.imread(os.path.join(data_path, img_lists[i]))\n        \n        single_img_obj = {}\n        single_img_obj['file_name'] = img_lists[i]\n        single_img_obj['height'] = img.shape[0]\n        single_img_obj['width'] = img.shape[1]\n        single_img_obj['id'] = i + 1\n        '''\n        \n        single_img_obj = {}\n        single_img_obj['file_name'] = img_lists[i]\n        single_img_obj['height'] = 720 \n        single_img_obj['width'] = 1280\n        single_img_obj['id'] = i + 1        \n        \n        imgs.append(single_img_obj)\n        \n  \n    ## gen objs information    \n    for j in tqdm(range(len(df))):\n        single_obj = {}\n        single_obj['id'] = j + 1\n        single_obj['image_id'] = img_lists.index(df['image_name'][j]) + 1\n        single_obj['category_id'] = int(df['impact'][j] ) ## You need to customize if you want to add some 'impact' class\n        single_obj['area'] = float(df['width'][j]*df['height'][j])\n        single_obj['bbox'] = [int(df['left'][j]), int(df['top'][j]), int(df['width'][j]), int(df['height'][j])]\n        single_obj['iscrowd'] = 0        \n        \n        objs.append(single_obj)\n    \n    print(f'images: {len(imgs)}, objs: {len(objs)}')\n    \n    return imgs, objs","f239b933":"def gen_coco(outpath, classes, objs, imgs, train=True):\n    if train:\n        data_dict = {}\n        data_dict['images'] = []\n        data_dict['annotations'] = []\n        data_dict['categories'] = []\n        data_dict['images'].extend(imgs)\n        data_dict['annotations'].extend(objs)\n        data_dict['categories'].extend(classes)\n        \n    else:\n        data_dict = {}\n        data_dict['images'] = []\n        data_dict['categories'] = []\n        \n        data_dict['images'].extend(imgs)\n        data_dict['categories'].extend(classes)   \n\n    with open(outpath, 'w') as f_out:\n        json.dump(data_dict, f_out)","6e04c017":"CLASSES = ['impact', 'helmet']\nclasses = gen_classes(CLASSES)\nclasses","b6e09c54":"train_imgs, train_objs = gen_objs(train_df)\nvalid_imgs, valid_objs = gen_objs(valid_df)","6c992456":"gen_coco('train.json', classes, train_objs, train_imgs, train=True)\ngen_coco('valid.json', classes, valid_objs, valid_imgs, train=True)","e1445341":"@DATASETS.register_module()\nclass ImpactDataset(CocoDataset):\n    CLASSES = set(CLASSES)","cc8257ae":"config_file = '..\/input\/nflbaselinecascadercnn\/cascade_rcnn_r50_fpn.py'\ncfg = Config.fromfile(config_file)","30e23a1b":"cfg.total_epochs = 1\ncfg.work_dir = '.\/'\ncfg.seed = 0\nset_random_seed(0, deterministic=False)\ncfg.gpu_ids = range(1)\ncfg.data_root = '..\/input\/nfl-frame\/'\n\ncfg.data.train.ann_file='.\/train.json'\ncfg.data.train.img_prefix= cfg.data_root\n\ncfg.data.val.ann_file='.\/valid.json',\ncfg.data.val.img_prefix= cfg.data_root\n\ncfg.data.test.ann_file='.\/valid.json', ## I just set validation data for test because it's not our main purpose\ncfg.data.test.img_prefix= cfg.data_root","97f83781":"print(f'Config:\\n{cfg.pretty_text}')","b9ea2acc":"# Build dataset\ndatasets = [build_dataset(cfg.data.train)]\n\n# Build the detector\nmodel = build_detector(\n    cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n# Add an attribute for visualization convenience\nmodel.CLASSES = datasets[0].CLASSES\n\n# Create work_dir\nmmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\ntrain_detector(model, datasets, cfg, distributed=False, validate=True)","8e5f766b":"fig = plt.figure()\n\n# Specify the path to model config and checkpoint file\ncheckpoint_file = '..\/input\/nflbaselinecascadercnn\/epoch_9.pth' # 10 epochs\n\n# build the model from a config file and a checkpoint file\nmodel = init_detector(config_file, checkpoint_file, device='cuda:0')\n\n# test a video and show the results\nvideo = mmcv.VideoReader('..\/input\/nfl-impact-detection\/test\/57906_000718_Endzone.mp4')\n\nims = []\n\nfor frame in video:\n    result = inference_detector(model, frame)\n    single_img = model.show_result(frame, result, wait_time=1)\n    im = plt.imshow(single_img, animated=True)\n    ims.append([im])\n    \nani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n                                repeat_delay=1000)","cf3816bd":"display(HTML(ani.to_html5_video()))","f5ce4179":"## Generate COCO Format Json\n\nTo use MMdetection tool box, we need to set-up COCO Format Json for our dataset.","14bb6c90":"## Train model","454130f7":"# Test video\n\nLet's test video and visualize","98cd3857":"You can check config","1288b7e1":"## Model build\nIn this notebook, I used cascade_rcnn_r50_fpn for baseline. I trained 9epochs and I got a `` public LB score.\nconfig_file is from [nfl_baseline_cascade_rcnn dataset](https:\/\/www.kaggle.com\/jinssaa\/nflbaselinecascadercnn) which I made for this Notebook.\n\nI saved 9epochs pretrained model with configs","ee30cc9b":"# Set Dataset\n\nThis class is from [MMdetection COCO.py](https:\/\/github.com\/open-mmlab\/mmdetection\/blob\/master\/mmdet\/datasets\/coco.py)\n\n\nI just set `CLASSES` only `impact`, `helmet`, So you can customize if you want.\n","3b8e5df6":"# Display video\n\nSorry for the low resolution. please let's me know if there a way to improve video rendering resolution !","f9159f96":"# Set up environment","885b2155":"# Data preparation\n\nI refered [2Class Object Detection Training](https:\/\/www.kaggle.com\/its7171\/2class-object-detection-training) by [@tito](https:\/\/www.kaggle.com\/its7171)\n\nI set +-4 frames from impact as a `impact class`\n\nthere are two classes\n\n* impact : 1 --> impact\n* impact : 2 --> helmet","12a5e3c4":"# Introduction\n\n**Main Topic**\n\nThis notebook is for **Detecting Impact using [MMdetection](https:\/\/github.com\/open-mmlab\/mmdetection) without internet** using **train_labels** and **[nfl-frame](https:\/\/www.kaggle.com\/jinssaa\/nfl-frame)**\n\nI refered idea from [2Class Object Detection Training](https:\/\/www.kaggle.com\/its7171\/2class-object-detection-training) by [@tito](https:\/\/www.kaggle.com\/its7171)\n\nI also used two classes(Impact, Helmet).\n\n**References**\n\n**Chris Deotte, Grand master** : [How to Install Without Internet](https:\/\/www.kaggle.com\/c\/severstal-steel-defect-detection\/discussion\/113195)\n\n**tito, Grand master** : [2Class Object Detection Training](https:\/\/www.kaggle.com\/its7171\/2class-object-detection-training)\n\n[**MMdetection Official Documents**](https:\/\/mmdetection.readthedocs.io\/)\n","2032411f":"# Install MMdetection from scratch\n\n**Version info.**\n\n- MMdetection 2.6.0\n- mmcv-full 1.2.0, torch 1.6, cu102\n\nBecause this Competetion is [Notebook Competetion](https:\/\/www.kaggle.com\/docs\/competitions#notebooks-only-FAQ), we need to inference .mp4 video without interent. \n\nSo I made `*.whl` files to install MMdetection, mmcv-full without internet. you can use this files from [mmdetection-v2.6.0 dataset](https:\/\/www.kaggle.com\/jinssaa\/mmdetectionv260).\n\n- note I think we don't need to `train` without internet so It's better to set local env if you want. this step for `inference` using weight.","2fb2817b":"## Split train validation dataset"}}