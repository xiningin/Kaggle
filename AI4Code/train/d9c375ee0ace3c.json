{"cell_type":{"9900e075":"code","ac3b1f79":"code","eb6a2e07":"code","81d8ef7e":"code","2ad47203":"code","8f6122c0":"code","6d4c022a":"code","341aa1f0":"code","0b8e3c71":"code","aaa67f1e":"code","ad69d7a4":"code","f57eaf73":"code","2d053092":"code","0336ef7d":"code","557332c8":"markdown","939e16a1":"markdown","e59b927e":"markdown","a7cf7055":"markdown","6addd8e8":"markdown","4641480e":"markdown","f48d5d37":"markdown","abf531a6":"markdown"},"source":{"9900e075":"import warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nimport sys\nsys.path.insert(0, \"..\/input\/transformers\/transformers-master\/\")\n\nfrom collections import Counter\nimport os, re, json, torch\n\nfrom functools import partial\n\nimport pandas as pd\nimport numpy as np\n\nimport Levenshtein\n\nfrom transformers import BertTokenizer,BertTokenizerFast, BertForSequenceClassification, BertConfig, BertForTokenClassification,AutoModelForTokenClassification","ac3b1f79":"# reading csv files and train & test file paths\ntrain_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nsample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","eb6a2e07":"def read_append_return(filename, train_files_path=train_files_path, output='text'):\n    \"\"\"\n    Function to read json file and then return the text data from them and append to the dataframe\n    \"\"\"\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","81d8ef7e":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","2ad47203":"def jaccard_similarity(str1: str, str2: str) -> float:\n    a = set(str1.split()) \n    b = set(str2.split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","8f6122c0":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = re.sub('[^A-Za-z0-9\\.\\:\\,\\!\\?\\;\\&]+', ' ', str(text)).strip()\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = re.sub('\\s', ' ', text)\n    return text.strip()","6d4c022a":"%%time\n#train_df['text'] = train_df['Id'].apply(read_append_return)\nsample_sub['text'] = sample_sub['Id'].apply(read_append_return, train_files_path=test_files_path)","341aa1f0":"sub = {}\nnew_existing_labels = []\ng2 = ['survey',\"study\",\"database\",\"data\",\"dataset\",\"initiative\",'research']\ng = ['survey',\"study\",\"database\",\"dataset\"]\nbad_w = ['consortium','organization','bureau','development','center','table','department','university','bank','class','user'\n         'appendix','supplementary','supplement','major','association','journal','commission','associates','board','agency',\n        'administration','federation','ministry','form','score','management','accounts','account','feasibility']\nbad1 = ['USGS','GWAS','ECLS','aDAS','NCDC','NDBC','UDS','GTD','ISC','DGP','EDC','FDA','TSE','DEA','CDA','IDB','NGDC','JODC','EDM','FADN','LRD','DBDM','DMC','WSC']\n\nfor index, row in sample_sub.iterrows():#sample_sub train_df.iloc[2019:2020]\n    sub[row['Id']] = ''\n    sample_text = text_cleaning(row['text'])\n    sample_text1 = sample_text.replace(\",\",\"\").lower()\n    \n    b = []\n    c = []    \n    for i,cl in enumerate(re.finditer(r'[a-z]*[A-Z]{2,5}[a-z]*[A-Z]{1,5}', sample_text)):\n        if cl[0].upper() in bad1:\n            continue\n        x1 = \"[a-z ]+\".join(list(cl[0].upper()))+'[a-z]+(?: [Dd]ata| [Ss]urvey| [Ss]ample| [Ss]tudy)*'\n\n        s = False\n        ans = re.search(x1, sample_text[cl.start()-100:cl.start()])\n        if ans:\n            s = clean_text(ans[0])\n        if s:\n            it_bad = False\n            for w2 in s.split():\n                if w2 in bad_w: \n                    it_bad = True\n            if not it_bad:\n                if s.split()[-1] in g2:\n                    b.append(clean_text(s)) #.rstrip(\"s\")\n                    c.append(clean_text(cl[0]))\n\n    \n\n    for s in c:\n        if len(sub[row['Id']]) and not s in sub[row['Id']]:\n            sub[row['Id']] = sub[row['Id']] + '|'+ s\n        elif not len(sub[row['Id']]):\n            sub[row['Id']] = s\n    \n    for s in b:\n        if len(sub[row['Id']]) and not s in sub[row['Id']]:\n            it_bad = False\n            for p in sub[row['Id']].split(\"|\"):\n                if p in s:\n                    it_bad = True\n                    break\n\n            if not it_bad:\n                sub[row['Id']] = sub[row['Id']] + '|'+ s\n                new_existing_labels.append(s)\n        elif not len(sub[row['Id']]):\n            sub[row['Id']] = s\n            new_existing_labels.append(s)\n\n\n    a = re.findall(r'(?<=[^\\.] )[A-Z][a-z]{3,20} (?:(?:[A-Z][a-z]{2,20}|of|up|to|and|the|in|on|COVID-19|s|for)[- \\.,]){0,10}(?:[A-Z][a-z]{3,20})(?: data| survey| sample| study)*', sample_text)\n    a = [s for s in a if len(re.findall(r'[A-Z]',s))>2]\n    cnt = Counter(a).most_common()\n    \n    cnt = {k[0]:k[1] for k in cnt if k[1] > 0}\n    \n    for s in cnt.keys():\n        s = clean_text(s)\n        if \"\".join([w[0] for w in s.split()]).upper() in bad1:\n            continue\n            \n        for w in s.split():\n            if w in g:\n                it_bad = False\n                for w2 in s.split():\n                    if w2 in bad_w: \n                        it_bad = True\n                if not it_bad:\n                    if not len(sub[row['Id']]):\n                        sub[row['Id']] = s\n                        new_existing_labels.append(s)\n                    elif len(sub[row['Id']]) and not s in sub[row['Id']]:\n                        for p in sub[row['Id']].split(\"|\"):\n                            if p in s or Levenshtein.distance(p,s)\/len(s)<0.2:\n                                it_bad = True\n                                break\n                        if not it_bad:\n                            sub[row['Id']] = sub[row['Id']] + '|'+ s\n                            new_existing_labels.append(s)\n\n                    break\n","0b8e3c71":"\ncnt = Counter(new_existing_labels).most_common()\ncnt = {k[0]:k[1] for k in cnt if k[1] > 1}\nnew_existing_labels = list(cnt.keys())\n\nnew_existing_labels = sorted(new_existing_labels, key=len)[::-1]\n\nfor index, row in sample_sub.iterrows():\n    if not row['Id'] in sub:\n        sub[row['Id']] = ''    \n    sample_text = text_cleaning(row['text'])\n    for s in new_existing_labels:#\n        if ' ' in s and s in sample_text.lower():\n            if len(sub[row['Id']]) and not s in sub[row['Id']]:\n                it_bad = False\n                for p in sub[row['Id']].split(\"|\"):\n                    if p in s or Levenshtein.distance(p,s)\/len(s)<0.2:\n                        it_bad = True\n                if not it_bad:\n                    sub[row['Id']] = sub[row['Id']] + '|'+ s\n            elif not len(sub[row['Id']]):\n                sub[row['Id']] = s\n","aaa67f1e":"class VTBDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, size_s, labels = None):\n        self.encodings = encodings\n        self.labels = labels\n        self.size_s = size_s\n    def __getitem__(self, idx):\n        self.item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels:\n            self.item['labels'] = torch.tensor(self.labels[idx])\n        return self.item\n\n    def __len__(self):\n        return self.size_s","ad69d7a4":"def encode_tags(labels, encodings):\n    encoded_labels = []\n    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n        # create an empty array of -100\n        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n        arr_offset = np.array(doc_offset)\n        # set labels whose first offset position is 0 and the second is not 0\n        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels[:sum((arr_offset[:,0] == 0) & (arr_offset[:,1] != 0))]\n\n        encoded_labels.append(doc_enc_labels.tolist())\n    return encoded_labels","f57eaf73":"BERT_MODEL_PATH = '..\/input\/bert-base-cased'\ntokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL_PATH,do_lower_case=False)\nbatch_size_test = 100\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSigmoid = torch.nn.Sigmoid()","2d053092":"\nmodel = BertForTokenClassification.from_pretrained(BERT_MODEL_PATH,num_labels=3)\nmodel = model.to(device)\n\nckpt = torch.load(\"..\/input\/07-model\/0.94model.bin\",map_location=device)\nmodel.load_state_dict(ckpt)   \nmodel.eval()\ngood_word = ['resource','report','research','survey','agriculture','service',\"study\",\"database\",\"program\",\"data\",\"dataset\",\"assessment\",'monitoring','surveys','initiative','system','student',\n    'observation','census','directory','reports','statistics','codes','student','students','baccalaureate','sample','project','initiatives']\nfor index, row in sample_sub.iterrows():#\n    ans = []\n    sample_text = text_cleaning(row['text'])\n    sample_text = sample_text.split(\". \")\n    \n    sent = ''\n    new_s = []\n    for i,sentens in enumerate(sample_text):\n        if not sent:\n            \n            if len(sentens)>200 and len(sentens)<400:\n                new_s.append(sentens)\n            elif len(sentens)>=400:\n                new_s.extend(re.findall(r'(?: |^).{0,150}[A-Z][a-z]{2,20} (?:(?:[A-Z][a-z]{2,20}|of|up|to|and|the|in|on|COVID-19|s|for|[0-9]{4}})[- \\.,]){0,10}(?:[A-Z][a-z]{2,20})(?: data| survey| sample| study| [0-9]{2,4})*.{0,150}(?:[\\. ]|$)', sentens))\n                new_s.extend(re.findall(r'(?: |^).{0,200}(?: [Dd]ata| [Rr]egistry|[Gg]enome [Ss]equence| [Mm]odel| [Ss]tudy| [Ss]urvey).{0,200}(?:[\\. ]|$)', sentens))\n                new_s.extend(re.findall(r'(?: |^).{0,200}[A-Z]{4,10}.{0,200}(?:[\\. ]|$)', sentens))                \n            else:\n                sent = sentens\n        else:\n            if len(sent + sentens) >= 400:\n                new_s.append(sent)\n                sent = ''\n                if len(sentens)>200 and len(sentens)<400:\n                    new_s.append(sentens)\n                elif len(sentens)>=400:\n                    new_s.extend(re.findall(r'(?: |^).{0,150}[A-Z][a-z]{2,20} (?:(?:[A-Z][a-z]{2,20}|of|up|to|and|the|in|on|COVID-19|s|for|[0-9]{4}})[- \\.,]){0,10}(?:[A-Z][a-z]{2,20})(?: data| survey| sample| study| [0-9]{2,4})*.{0,150}(?:[\\. ]|$)', sentens))\n                    new_s.extend(re.findall(r'(?: |^).{0,200}(?: [Dd]ata| [Rr]egistry|[Gg]enome [Ss]equence| [Mm]odel| [Ss]tudy| [Ss]urvey).{0,200}(?:[\\. ]|$)', sentens))\n                    new_s.extend(re.findall(r'(?: |^).{0,200}[A-Z]{4,10}.{0,200}(?:[\\. ]|$)', sentens))    \n                else:\n                    sent = sentens\n            else:\n                sent = sent +'. ' + sentens\n    if sent:\n        new_s.append(sent)\n    \n    new_s_2 = []\n    for s in new_s:\n        \n        a = re.findall(r'(?:(?:[A-Z][a-z]{2,20}|of|in|COVID-19|s|for|and) ){3,6}', s)\n        \n        a.extend(re.findall(r'(?: [Dd]ata| [Rr]egistry|[Gg]enome [Ss]equence| [Mm]odel| [Ss]tudy| [Ss]urvey)', s))\n        a.extend(re.findall(r'[A-Z]{4,10}', s))\n        if a:\n            new_s_2.append(s)\n    if new_s_2:\n        t_x = [s.split() for s in new_s_2]\n        valid_y = [[1]*len(x) for x in t_x]\n        \n        val_encodings = tokenizer(t_x, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True,max_length = 256)    \n        val_labels = encode_tags(valid_y, val_encodings)    \n        val_encodings.pop(\"offset_mapping\")\n        \n        valid_dataset = VTBDataset(val_encodings,len(t_x),val_labels)\n        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size_test, shuffle=False)\n        \n        valid = pd.DataFrame()\n        valid['val_labels'] = val_encodings['input_ids']\n        val_labels = len(val_encodings['input_ids'])\n        len_val = len(val_encodings['input_ids'][0])\n\n        valid_preds1 = np.zeros((val_labels,len_val), dtype = np.float32)\n        valid_preds2 = np.zeros((val_labels,len_val), dtype = np.float32)\n\n        avg_accuracy = 0.\n        with torch.no_grad():\n            for i,(batch)  in enumerate(valid_loader):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels']\n                outputs = model(input_ids, attention_mask=attention_mask, labels=None)\n\n                logits1 = outputs[0][:,:,1].detach()\n                logits1[labels<0] = -10\n                logits2 = outputs[0][:,:,2].detach()\n                logits2[labels<0] = -10                   \n                valid_preds2[i*batch_size_test:(i+1)*batch_size_test,:]=logits2.cpu().numpy()  \n                valid_preds1[i*batch_size_test:(i+1)*batch_size_test,:]=logits1.cpu().numpy()                             \n\n    \n    ans = []\n    for index, row_1 in valid.iterrows():\n    \n        preds1, = np.where(valid_preds1[index]>1)\n        preds2, = np.where(valid_preds2[index]>0)\n        \n        preds2.sort()\n        max_a = 0\n        g_all = []\n        for min_a in preds1:\n            if max_a > min_a:\n                continue\n            g = ''\n            max_a = 0\n            for min_b in preds2:\n                if min_b>min_a:\n                    max_a = min_b\n                    break\n            if max_a == 0 and valid_preds1[index][min_a]>2:\n                max_a = min_a  \n            if max_a-min_a > 10:\n                continue\n            if max_a>=min_a and min_a>0:\n                k = 0\n                  \n                b = np.array(row_1[\"val_labels\"])\n\n                s = tokenizer.convert_ids_to_tokens(b[min_a:])\n                for j,w in enumerate(s):\n                    \n                    if j<=max_a - min_a or \"##\" in w  :\n                        g += w + ' '\n                    else:    \n                        break\n            g = g.replace(\" ##\",\"\").strip()\n            \n            if g and sum(map(str.isupper,g))\/len(g.split())>0.5:\n                g = clean_text(g)\n                it_bad = False\n                for w2 in g.split():\n                    if w2 in bad_w: \n                        it_bad = True\n                if not it_bad:\n                    for w in g.split():\n                        if w in good_word or len(g.split())==1:\n                            g_all.append(g)\n                            break            \n    \n        ans.extend(g_all)\n\n    if not row['Id'] in sub:\n        sub[row['Id']] = ''\n    for s in ans:\n        if len(sub[row['Id']]) and not s in sub[row['Id']]:\n            it_bad = False\n            for p in sub[row['Id']].split(\"|\"):\n                if p in s or Levenshtein.distance(p,s)\/len(s)<0.2:\n                    it_bad = True\n            if not it_bad:              \n                sub[row['Id']] = sub[row['Id']] + '|'+ s\n        elif not len(sub[row['Id']]):\n            sub[row['Id']] = s    \n","0336ef7d":"submission = pd.DataFrame.from_dict(sub, orient='index', columns=['PredictionString']).reset_index()\nsubmission.columns = [\"Id\",\"PredictionString\"]\nsubmission.to_csv('submission.csv', index=False)","557332c8":"# BertForTokenClassification","939e16a1":"# Answer","e59b927e":"# Moduls","a7cf7055":"# Coleridge Initiative - Show US the Data","6addd8e8":"I tried two approaches (no dataset label string matching)\n\n* Regular expression - 0.5\/0.56\n* BERT NER - 0.37\/0.52\n\nEnsemble, searching among known datasets and other approaches with BERT did not improve the results\n\n## Regular expressions\n* Looking for uppercase letters\n* Looking for nearby words with these letters\n* Just looking for words beginning with uppercase letters\n* I memorize all found and search among other documents\n## BERT NER\n* I divide the document into sentences of 200-400 characters\n* Select potential candidates\n* I take 90% of sentences with tags and 10% without tags\n* Use 3 classes - no class, first word and last word of dataset name\n* Predict and select good candidates","4641480e":"# Read data","f48d5d37":"# re.search","abf531a6":"# \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435"}}