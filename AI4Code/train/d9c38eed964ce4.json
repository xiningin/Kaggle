{"cell_type":{"93cea9b7":"code","9cfb967d":"code","63191eb4":"code","ae8c59f8":"code","86a234c7":"code","00b34f56":"code","ebcb12c0":"code","53bc3d6a":"markdown","9d793a0f":"markdown","1ce82581":"markdown","4f9b198d":"markdown","76220009":"markdown","5866182d":"markdown","52e326a5":"markdown"},"source":{"93cea9b7":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nfrom matplotlib.colors import ListedColormap","9cfb967d":"class AdalineSGD(object):\n    \"\"\"ADAptive LInear NEuron classifier.\n    Parameters\n    ------------\n    eta : float\n        Learning rate (between 0.0 and 1.0)\n    n_iter : int\n        Passes over the training dataset.\n    shuffle : bool (default: True)\n        Shuffles training data every epoch if True to prevent cycles.\n    random_state : int\n        Random number generator seed for random weight initialization.\n    Attributes\n    -----------\n    w_ : 1d-array\n        Weights after fitting.\n    cost_ : list\n        Sum-of-squares cost function value averaged over all training samples in each epoch.\n    \"\"\"\n    \n    def __init__(self, eta=0.01, n_iter=10,shuffle=True, random_state=None):\n        self.eta = eta;\n        self.n_iter = n_iter;\n        self.w_initialized = False;\n        self.shuffle = shuffle;\n        self.random_state = random_state;\n    \n    def fit(self, X, Y):\n        \"\"\" Fit training data.\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and n_features is the number of features.\n        Y : array-like, shape = [n_samples]\n            Target values.\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # initialize weights\n        self._initialize_weights(X.shape[1]);\n        self.cost_ = [];\n\n        # run through the epochs\n        for _ in range(self.n_iter):\n            # shuffle the dataset\n            if self.shuffle:\n                X,Y = self._shuffle(X,Y);\n            cost = [];\n            # run through each training example\n            for xi, target in zip(X,Y):\n                # append cost after each update\n                cost.append(self._update_weights(xi,target));\n            # average the cost after each epoch\n            avg_cost = sum(cost) \/ len(Y);\n            # record the average cost after each epoch\n            self.cost_.append(avg_cost);\n        return self;\n    \n    def partial_fit(self,X,Y):\n        \"\"\"Fit training data without reinitializing the weights for online data\"\"\"\n        # initialize weights if not initialized\n        if not self.w_initialized:\n            self._initialezed_weights(X.shape[1]);\n        if Y.ravel().shape[0] > 1:\n            for xi, target in zip(X,Y):\n                self._update_weights(xi,target);\n        else:\n            self._update_weights(X,Y);\n        return self;\n    \n    def _shuffle(self,X,Y):\n        \"\"\"Shuffle training data\"\"\"\n        r = self.rgen.permutation(len(Y));\n        return X[r], Y[r];\n    \n    def _initialize_weights(self,m):\n        \"\"\"Initialize weights to small random numbers\"\"\"\n        self.rgen = np.random.RandomState(self.random_state);\n        # size 1 + n_features, here is for bias,b \n        self.w_ = self.rgen.normal(loc = 0.0, scale = 0.01, size = 1 + m);\n        self.w_initialized = True;\n        \n    def _update_weights(self,xi,target):\n        \"\"\"Apply Adaline learning rule to update the weights\"\"\"\n        # get output for a training example i.e. activation(Z), where Z = w^T * X + b\n        output = self.activation(self.net_input(xi));\n        error = (target - output);\n        # updating weigths i.e. w = w + alpha * dJ\/dw = w + aplha * (Yi - Y^i) * Xi, here alpha ie the learning rate\n        self.w_[1:] += self.eta * xi.dot(error);  \n        # updating bias i.e b = b + alpha * (Yi - Y^i)\n        self.w_[0] += self.eta * error;\n        # calculating cost i.e. J = 1\/2 * (Yi - Y^i)^2\n        cost = 1\/2 * error**2;\n        return cost;\n    \n    def net_input(self, X):\n        \"\"\"Calculate net input\"\"\"\n        # calcu;ating Z = w^T * xi + b\n        return np.dot(X, self.w_[1:]) + self.w_[0];\n    \n    def activation(self, X):\n        \"\"\"Compute linear activation\"\"\"\n        return X;\n    \n    def predict(self, X):\n        \"\"\"Return class label after unit step\"\"\"\n        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1);\n        ","63191eb4":"df = pd.read_csv('..\/input\/Iris.csv');\ndf.tail()","ae8c59f8":"# select setosa and versicolor samples\n# an overview of first 100 samples\ndf.iloc[0:100,5].value_counts();\n# get the class labels for first 100 samples\nY = df.iloc[0:100,5].values;\n# set class labels as 1 and -1\nY = np.where(Y == 'Iris-setosa',-1,1);\n# select sepal and petal length freatures\nX = df.iloc[0:100,[1,3]].values\n\n# plot the data for setosa class\nplt.scatter(X[:50,0],X[:50,1],color='red',marker='o',label='setosa')\n# plot the data for versicolor class\nplt.scatter(X[50:100,0],X[50:100,1],color='blue',marker='x',label='vesicolor')\nplt.xlabel('sepal length[cm]')\nplt.ylabel('petal length[cm]')\nplt.legend(loc='upper left')\nplt.show()","86a234c7":"# making a copy of the dataset\nX_std = np.copy(X)\n# standardizing sepal length\nX_std[:,0] = (X[:,0] - X[:,0].mean()) \/ X[:,0].std()\n# standardizing petal length\nX_std[:,1] = (X[:,1] - X[:,1].mean()) \/ X[:,1].std()","00b34f56":"def plot_decision_regions(X,Y,model,resolution = 0.02):\n    # setup colormap and markers\n    markers = ('s','x','o','^','v');\n    colors = ('red','blue','lightgreen','gray','cyan');\n    cmap = ListedColormap(colors[:len(np.unique(Y))]);\n    # plot the decision surface\n    # determin minimim ans maximum values for the two features\n    x1_min,x1_max =  X[:,0].min() - 1, X[:,0].max() + 1;\n    x2_min,x2_max =  X[:,1].min() - 1, X[:,1].max() + 1; \n    # create a pair of grid arrays\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),np.arange(x2_min, x2_max, resolution))\n    # predict on our model by flattening the arrays and making it into a matrix\n    Z = model.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    \n    # plot class samples\n    for idx, cl in enumerate(np.unique(Y)):\n        plt.scatter(x=X[Y == cl, 0],y=X[Y == cl, 1],alpha=0.8,c=colors[idx],marker=markers[idx],label=cl,edgecolor='black')\n","ebcb12c0":"ada = AdalineSGD(n_iter=15, eta=0.01, random_state=1)\nada.fit(X_std, Y)\nplot_decision_regions(X_std, Y, model=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.xlabel('sepal length [standardized]')\nplt.ylabel('petal length [standardized]')\nplt.legend(loc='upper left')\nplt.show()\nplt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o')\nplt.xlabel('Epochs')\nplt.ylabel('Average Cost')\nplt.show()","53bc3d6a":"Creating the model(classifier) via Object Oriented approach:","9d793a0f":"This notebook implements the Adaline(Adaptive Linear Neuron) model trained with Stochastic Gradient Descent(SGD) as explained in the book \"**Python Machine Learning**\" by **Sebastian Raschka and Vahid Mirjalili**.\n\n**Prerequisites:**\n\n* Python \n* pandas\n* numpy\n\n**Dataset: **IRIS\n\n**Note:** Descriptive comments explain the code in a better way\n\nImport necessary packages","1ce82581":"Now we will visualize the first 100 samples(50 for iris-versicolor class i.e. 1 and another 50 for iris-setosa class i.e. -1) and will use only two feature i.e. sepal lenght and petal length:","4f9b198d":"Since standardizing the dataset helps to converge the cost function faster with a small learning rate, lets do that to our dataset:\n","76220009":"Load the Iris dataset in a panda dataframe:\n\n","5866182d":"Here we can see that our AdalineSGD model converges very well and the two classes have been succesfully separated by the linear boundary. SGD mostly comes in handy when we have a very huge dataset and thus want our cost function to converge faster and due to frequent weight updates the errors surface of the SGD becomes much more noisy than GD which in a way prevents the cost function to converge in a local minimum. To achieve much better results we may also use a varying\/adaptive learning rate along with SGD. \n\nA compromised approach between SGD and GD is mini-batch GD which is faster than GD and we may use vectorized calculations for the mini-batch which further improves computational efficiency.","52e326a5":"Plot the decision boundary on our data after training our model:"}}