{"cell_type":{"cdffcf87":"code","e9392fa7":"code","59bd1fa2":"code","7c89f690":"code","245554f0":"code","828a46e4":"code","b5471dc6":"code","2645c513":"code","e5d2850c":"code","f8780dd9":"code","d05deefc":"code","86691435":"code","bf087978":"code","33bb36a5":"code","c858f080":"code","5d64e8b6":"code","c27efea4":"code","7b339d13":"code","14a9d247":"code","12b619b2":"code","eddba0de":"code","8609f6ec":"code","009b3094":"code","97852faa":"code","462644aa":"code","7b64c8fb":"code","694b50e3":"code","82729ed6":"markdown","18c4984f":"markdown","c63abb88":"markdown","d41f41c4":"markdown","739eebe0":"markdown","e219d221":"markdown","9c5947ce":"markdown","47237ebe":"markdown"},"source":{"cdffcf87":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e9392fa7":"from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom xgboost.sklearn import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgb\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","59bd1fa2":"def MAPE(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","7c89f690":"# data\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntesting = test.copy()\n\ntarget = \"SalePrice\"\n\ndf = pd.concat([train, test], axis = 0)\ndf.info()\ndf.head(5)","245554f0":"for column in df.columns:\n    try:\n        df[column] = pd.to_numeric(df[column])\n    except ValueError:\n        pass","828a46e4":"df.drop(columns = ['Id'], inplace = True)","b5471dc6":"plt.subplots(figsize = (10, 5))\nsns.distplot(df[target].dropna(), kde = True, label = target, color = 'green', bins = 100)\nplt.legend(prop = {'size': 12})\nplt.show()","2645c513":"plt.figure(figsize = (12, 10))\n(df.loc[:, df.isnull().any()].isna().sum() \/ df.shape[0]).sort_values().plot(kind = 'barh', label = '% of missing values')\nplt.axvline(x = 0.2, color = 'r', linestyle = '--', label = 'Reference line')\nplt.legend()\nplt.title('Percentage of missing values:')\nplt.xlabel('% of missing data')\nplt.show()","e5d2850c":"# Manually dropping features which has more then 20% missing vlaues.\ntry:\n    df.drop(columns = ['MiscFeature', 'Fence', 'PoolQC', 'FireplaceQu', 'Alley'], inplace = True)\nexcept KeyError:\n    print('Features already dropped')\n    pass\n\ncolumns_missing_less_200 = df[df.select_dtypes(exclude = object).columns].loc[:, df.isnull().sum() < 200].columns\nfor col in columns_missing_less_200:\n    df[col] = df[col].fillna(df[col].median())\n    \ndf[df.select_dtypes(exclude = object).columns].loc[:, df.isnull().any()].isna().sum()","f8780dd9":"data_corr = df.corr().abs().unstack().sort_values(kind = \"quicksort\", ascending = False).reset_index()\ndata_corr.loc[data_corr['level_0'] == 'LotFrontage']","d05deefc":"# Filling missing values by most correlated feature's median\ndf['LotFrontage'] = df['LotFrontage'].fillna(df.groupby(['LotArea'])['LotFrontage'].transform('median'))\ndf['LotFrontage'] = df['LotFrontage'].fillna(df.groupby(['1stFlrSF'])['LotFrontage'].transform('median'))\ndf['LotFrontage'] = df['LotFrontage'].fillna(df.groupby(['MSSubClass'])['LotFrontage'].transform('median'))\n\ndf[df.select_dtypes(exclude = object).columns].loc[:, df.isnull().any()].isna().sum()","86691435":"corrMatrix = df.corr()\nmask = np.zeros_like(corrMatrix)\nmask[np.triu_indices_from(mask)] = True\n\nplt.subplots(figsize=(28, 12))\nsns.heatmap(corrMatrix, mask = mask, vmax = .3, square = False, annot = True)\nplt.show()","bf087978":"plt.subplots(figsize = (20, 5))\ncorrMatrix[target].drop([target]).sort_values().plot(kind = 'bar')\nplt.axhline(y = 0.05, color = 'red', linestyle = '--')\nplt.axhline(y = -0.05, color = 'red', linestyle = '--')\nplt.show()\n\n#Correlation with output variable\ncor_target = abs(corrMatrix[target])\n\n#Selecting correlated features\ncorrelated_features = cor_target[cor_target >= 0.05]\nprint('Amount of total features: ', len(df.select_dtypes(include = object).columns))\nprint('Amount of correlated features: ', len(correlated_features))","33bb36a5":"for i in range(0, df[correlated_features.index].shape[1], 6):\n    sns.pairplot(data = df[correlated_features.index],\n                x_vars =  df[correlated_features.index].columns[i:i+6],\n                y_vars = ['SalePrice'], kind='reg')","c858f080":"df.loc[(df['BsmtFinSF1'] > 4000) & (df[target].notnull()), 'BsmtFinSF1'] = None\ndf.loc[(df['TotalBsmtSF'] > 4000) & (df[target].notnull()), 'TotalBsmtSF'] = None\ndf.loc[(df['GrLivArea'] > 4000) & (df[target] < 400000) & (df[target].notnull()), 'GrLivArea'] = None\ndf.loc[(df['1stFlrSF'] > 4000) & (df[target].notnull()), '1stFlrSF'] = None\ndf.loc[(df['LotFrontage'] > 200) & (df[target].notnull()), 'LotFrontage'] = None\ndf.loc[(df['LotArea'] > 60000) & (df[target].notnull()), 'LotArea'] = None\ndf.loc[(df['GarageArea'] > 1200) & (df[target] < 400000) & (df[target].notnull()), 'GarageArea'] = None\n","5d64e8b6":"for i in range(0, df[correlated_features.index].shape[1], 6):\n    sns.pairplot(data = df[correlated_features.index],\n                x_vars =  df[correlated_features.index].columns[i:i+6],\n                y_vars = ['SalePrice'], kind='reg')","c27efea4":"# Check for any missing values in categorical features\ndf[df.select_dtypes(include = object).columns].loc[:, df.isnull().any()].isna().sum()","7b339d13":"for col in df.select_dtypes(include = object).columns:\n    df[col] = df[col].fillna(df[col].mode()[0])\n    \ndf[df.select_dtypes(include = object).columns].loc[:, df.isnull().any()].isna().sum()","14a9d247":"import scipy.stats as ss\n\ndef cramers_v(confusion_matrix):\n    \"calculate Cramers V statistic for categorial-categorial association\"\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum()\n    phi2 = chi2 \/ n\n    r, k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))\/(n-1))\n    rcorr = r - ((r-1)**2)\/(n-1)\n    kcorr = k - ((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr \/ min((kcorr-1), (rcorr-1)))\n\ncramers_values = []\nobj_columns = df.select_dtypes(include = object).columns\n\nfor column in obj_columns:\n    confusion_matrix = pd.crosstab(df[column], df[target])\n    cramers_values.append(cramers_v(confusion_matrix.values))\n\ncramers_values_df = pd.DataFrame()\ncramers_values_df['feature'], cramers_values_df['value'] = obj_columns, cramers_values\n\nplt.subplots(figsize = (12, 10))\nplt.barh(cramers_values_df[cramers_values_df['value'] > 0].sort_values(['value'])['feature']\n        ,cramers_values_df[cramers_values_df['value'] > 0].sort_values(['value'])['value'])\nplt.axvline(x = 0.15, color = 'red', linestyle = '--', label = 'Reference line')\nplt.title(\"Categorical feature correlation with the target\")\nplt.show()\n\ncramers_values_df = cramers_values_df[cramers_values_df['value'] > 0.15]['feature'].to_list() # Select correlated features\nlen(cramers_values_df)","12b619b2":"# Final dataset\ndf = pd.concat([df[cramers_values_df], df[correlated_features.index]], axis = 1)\ndf","eddba0de":"# Preparing features for analysis\ndummy_features = df.select_dtypes(include = object).columns\n    \ndf = pd.concat([df, pd.get_dummies(df[dummy_features], drop_first = True)], axis = 1, sort = False)\ndf.drop(columns = df[dummy_features], inplace = True)\n\ndf.tail()","8609f6ec":"test = df[df[target].isnull()].drop([target], axis = 1).copy()\ntrain = df[df[target].notnull()].copy()\n\ntrain.head(5)","009b3094":"# Drop NaN just to be safe\ntrain.dropna(inplace = True)\ntest.dropna(inplace = True)","97852faa":"# Separating target column from other features\ny = train[target]\nx = train.drop(columns = target)\n\n# Train and Test dataset split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42, shuffle = True)\nx_train.shape, x_test.shape","462644aa":"CBR = CatBoostRegressor()\nCBR_params = {\n              'n_estimators':[n for n in range(600, 1400, 200)]\n              ,'max_depth': [n for n in range(2, 10, 2)]\n              ,'random_state' : [42]\n              ,\"learning_rate\": [0.05, 0.03]\n                }\n\nCBR_model = GridSearchCV(CBR, param_grid = CBR_params, cv = 5, n_jobs = -1)\nCBR_model.fit(x_train, y_train, silent = True)\n\nCBR_predictions = CBR_model.predict(x_test)\n\nprint(\"R2 score: \" + '%.3f' % r2_score(y_test, CBR_predictions))\nprint(\"RMSE score: \", f\"{np.sqrt(mean_squared_error(y_test, CBR_predictions)):,.0f}\")\nprint(\"MAE score: \", f\"{mean_absolute_error(y_test, CBR_predictions):,.0f}\")\n\nprint(\"Best Hyper Parameters: \",CBR_model.best_params_)\nprint(\"Best Score: \" + '%.3f' % CBR_model.best_score_)","7b64c8fb":"# Plot feature importance\nfeature_importance = pd.Series(CBR_model.best_estimator_.feature_importances_, index = x_train.columns)\n\nplt.figure(figsize = (10,8))\nfeature_importance.nlargest(15).sort_values(ascending = True).plot(kind = 'barh')\nplt.show()","694b50e3":"submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\npredictions = CBR_model.predict(test[x_train.columns])\n\nsubmission[\"SalePrice\"] = predictions\nsubmission.to_csv(\"submission.csv\", index = False)\nprint(\"Done\")","82729ed6":"# Exporting predictions","18c4984f":"# Selecting categorical features","c63abb88":"## Removing some outliers","d41f41c4":"# Data input","739eebe0":"# Selecting numeric features","e219d221":"# Selecting model","9c5947ce":"# Preparing data for modeling","47237ebe":"# Handling missing data"}}