{"cell_type":{"04cfacc6":"code","ab01379f":"code","9d16e96a":"code","a1004ab0":"code","80e80631":"code","283ff720":"code","6588964b":"code","6aaf052c":"code","01d7e5b2":"code","474661d1":"code","5b2ba006":"code","989236c5":"code","d283adc8":"code","60c0bcb6":"code","28925a11":"code","d3312e4f":"code","e1ffdbb7":"code","4b98f087":"code","e3bba6ad":"code","25408968":"code","0533d5fd":"code","bb91f78c":"code","2206d7fe":"code","d6cc026e":"code","5290b826":"code","b937946d":"code","888fc56a":"code","ca615894":"code","384eea03":"code","a555a920":"code","72a2ed53":"code","ca6cd363":"code","a0cf6305":"code","d5315611":"code","e4e09203":"code","a286d9d5":"code","c33e39b0":"code","cb2bd4d2":"code","7b232a57":"code","948ee14a":"code","d868b669":"code","bea08444":"code","3ddf043b":"code","bd28ebef":"code","cdbf6265":"code","a1bdfdc9":"code","60d8ebc9":"code","2f7dde36":"code","1966220d":"code","3085c3c6":"code","bc21d94a":"code","1cd32ca0":"code","784bd9fe":"code","f0702f18":"code","f5748b5e":"code","4fd4f416":"code","300d0c08":"code","3a956bb9":"code","4cf4b7d3":"code","7b88fbf2":"code","816dba75":"code","ff804d41":"code","704d323f":"code","29d8ca8e":"code","12933590":"code","4c7af222":"code","162ea1c8":"code","e33c075c":"code","a6a71053":"code","b4196d60":"code","d67e0a59":"code","678c7c72":"code","7f29d3bb":"code","dc60256d":"code","d5f70dda":"code","b559c5f4":"code","05ca50d9":"code","c32f5f1c":"code","bb6db92f":"code","e5a40e22":"code","7ae175fa":"code","1ccec80a":"code","acca0d8d":"code","220ca6af":"code","81657da7":"code","7eb63c87":"code","15a7235f":"code","ea742a23":"code","3a4caac0":"code","b1eb96c2":"code","084ffb0a":"code","e2b478e0":"code","b74ee79d":"code","2d713acf":"code","d02b2ad9":"code","88ff9af5":"code","f8e06d4d":"code","18a41b19":"code","a3867d17":"code","35bbeacc":"code","fd1b5662":"code","93ffacd0":"code","e4e5b695":"code","e7597265":"code","5705d652":"code","6c92f619":"markdown","07b056aa":"markdown","0ad65c8f":"markdown","cb3cea27":"markdown","d52148bf":"markdown","3f552752":"markdown","23a2687a":"markdown","b15010aa":"markdown","20832905":"markdown","31cf6408":"markdown","2419b4b6":"markdown","45e2c174":"markdown","2a38e09d":"markdown","f4301ec8":"markdown","7c12967c":"markdown","d95823f9":"markdown","362a4ddf":"markdown","3b879fe5":"markdown","fb802867":"markdown","375e9907":"markdown","39958860":"markdown","faae27cc":"markdown","ac138c45":"markdown","2c090ef3":"markdown","1acae1e3":"markdown","a30e3764":"markdown","1b3f4170":"markdown","fd335fb2":"markdown","d1454cd2":"markdown","0f46dbd7":"markdown","746f1ba1":"markdown","42de1b06":"markdown","6352dab8":"markdown","fa5e433e":"markdown","623d4294":"markdown","21fcaa29":"markdown","ed1b07a4":"markdown","05042f55":"markdown","2dcf9f3a":"markdown","78cac5d8":"markdown","8ae8550d":"markdown","6d017bae":"markdown","5c50f081":"markdown","025f7e54":"markdown","b626f90c":"markdown","cd876a42":"markdown"},"source":{"04cfacc6":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\nimport seaborn as sns\n\nimport re\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ab01379f":"pd.options.display.max_columns = None \n\ndf_initial = pd.read_csv('..\/input\/milan-airbnb-dataset-2020\/listings.csv')\n\n# checking shape\nprint(\"The dataset has {} rows and {} columns.\".format(*df_initial.shape))\n\n# ... and duplicates\nprint(\"It contains {} duplicates.\".format(df_initial.duplicated().sum()))","9d16e96a":"df_initial.head(2)","a1004ab0":"# check the columns we currently have\ndf_initial.columns","80e80631":"pd.options.display.max_rows = None\ndf_initial.isna().sum()","283ff720":"# define the columns we want to keep\ncolumns_to_keep = ['id', 'description','host_has_profile_pic',\n                   'neighbourhood_cleansed', 'latitude','longitude', 'property_type', 'room_type', 'accommodates',\n        'beds', 'amenities', 'price',\n       'minimum_nights', 'has_availability',\n       'number_of_reviews',\n        'review_scores_rating', 'instant_bookable'\n       ]\n    \n\ndf_raw = df_initial[columns_to_keep].set_index('id')\nprint(\"The dataset has {} rows and {} columns - after dropping irrelevant columns.\".format(*df_raw.shape))\n\n","6588964b":"df_raw.room_type.value_counts(normalize=True)","6aaf052c":"df_raw.property_type.value_counts(normalize=True)","01d7e5b2":"df_raw['price'].head()","474661d1":"# checking Nan's in \"price\" column\ndf_raw.price.isna().sum()","5b2ba006":"# clean up the columns (by method chaining)\ndf_raw.price = df_raw.price.str.replace('$', '').str.replace(',', '').astype(float)","989236c5":"df_raw['price'].describe()","d283adc8":"red_square = dict(markerfacecolor='r', markeredgecolor='r', marker='.')\ndf_raw['price'].plot(kind='box', xlim=(0, 800), vert=False, flierprops=red_square, figsize=(16,2));","60c0bcb6":"df_raw.drop(df_raw[ (df_raw.price > 300) | (df_raw.price == 0) ].index, axis=0, inplace=True)","28925a11":"df_raw['price'].describe()","d3312e4f":"print(\"The dataset has {} rows and {} columns - after being price-wise preprocessed.\".format(*df_raw.shape))","e1ffdbb7":"pd.options.display.max_rows = None\ndf_raw.isna().sum()","4b98f087":"# drop columns with too many Nan's\n#df_raw.drop(columns=['neighbourhood', 'bathrooms','neighbourhood_overview' ], inplace=True)","e3bba6ad":"# drop rows with NaN's in bathrooms, host pic, discription, besds, and bedrooms\ndf_raw.dropna(subset=[  'host_has_profile_pic' ], inplace=True)","25408968":"#df_raw.host_has_profile_pic.unique()\n# replace host_has_profile_pic Nan's with no\n#df_raw.host_has_profile_pic.fillna(value='f', inplace=True)\n#df_raw.host_has_profile_pic.unique()","0533d5fd":"pd.options.display.max_rows = None\ndf_raw.isna().sum()","bb91f78c":"print(\"The dataset has {} rows and {} columns - after having dealt with missing values.\".format(*df_raw.shape))","2206d7fe":"df_raw.info()","d6cc026e":"# filter out sub_df to work with\nsub_df = df_raw[['accommodates', 'minimum_nights', 'beds',  'price']]","5290b826":"# split datasets\ntrain_data = sub_df[sub_df['beds'].notnull()]\ntest_data  = sub_df[sub_df['beds'].isnull()]\n\n# define X\nX_train = train_data.drop('beds', axis=1)\nX_test  = test_data.drop('beds', axis=1)\n\n# define y\ny_train = train_data['beds']","b937946d":"print(\"Shape of Training Data:\", train_data.shape)\nprint(\"Shape of Test Data:    \",test_data.shape)\nprint(\"\\nShape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\nprint(\"\\nShape of y_train:\", y_train.shape)","888fc56a":"# import Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\n# instantiate\nlinreg = LinearRegression()\n\n# fit model to training data\nlinreg.fit(X_train, y_train)","ca615894":"# making predictions\ny_test = linreg.predict(X_test)","384eea03":"y_test = pd.DataFrame(y_test)\ny_test.columns = ['beds']\n#y_test = round(y_test.columns)\nprint(y_test.shape)\ny_test.head()","a555a920":"print(X_test.shape)\nX_test.head()","72a2ed53":"# make the index of X_test to an own dataframe\nprelim_index = pd.DataFrame(X_test.index)\nprelim_index.columns = ['prelim']\n\n# ... and concat this dataframe with y_test\ny_test = pd.concat([y_test, prelim_index], axis=1)\ny_test.set_index(['prelim'], inplace=True)\ny_test.head()","ca6cd363":"new_test_data = pd.concat([X_test, y_test], axis=1)","a0cf6305":"print(new_test_data.shape)\nnew_test_data.head()","d5315611":"new_test_data['beds'].isna().sum()","e4e09203":"# combine train and test data back to a new sub df\nsub_df_new = pd.concat([new_test_data, train_data], axis=0)\n\nprint(sub_df_new.shape)\nsub_df_new.head()","a286d9d5":"sub_df_new['beds'].isna().sum()","c33e39b0":"# prepare the multiple columns before concatening\ndf_raw.drop(['accommodates', 'minimum_nights', 'beds',  'price'], \n            axis=1, inplace=True)","cb2bd4d2":"# concate back to complete dataframe\nsub_df_new=round(sub_df_new)\ndf = pd.concat([sub_df_new, df_raw], axis=1)\n\nprint(df.shape)\ndf.head(2)","7b232a57":"df['beds'].isna().sum()","948ee14a":"df['beds'].describe()","d868b669":"df.drop(df[ (df['beds'] == 0.) | (df['beds'] > 8.) ].index, axis=0, inplace=True)","bea08444":"print(\"The dataset has {} rows and {} columns - after being engineered.\".format(*df.shape))","3ddf043b":"from geopy.distance import great_circle","bd28ebef":"def distance_to_mid(lat, lon):\n    milan_centre = ( 45.4641, 9.1919)\n    accommodation = (lat, lon)\n    return great_circle(milan_centre, accommodation).km","cdbf6265":"df['distance'] = df.apply(lambda x: distance_to_mid(x.latitude, x.longitude), axis=1)","a1bdfdc9":"df.head(2)","60d8ebc9":"#list(df.description[:10])","2f7dde36":"df.description.isna().sum()","1966220d":"# extract numbers \ndf['size'] = df['description'].str.extract('(\\d{2,3}\\s?[smSM][q2Q])', expand=True)\ndf['size'] = df['size'].str.replace(\"\\D\", \"\")\n\n# change datatype of size into float\ndf['size'] = df['size'].astype(float)\n\nprint('NaNs in size_column absolute:     ', df['size'].isna().sum())\nprint('NaNs in size_column in percentage:', round(df['size'].isna().sum()\/len(df),3), '%')","3085c3c6":"df[['description', 'size']].head(10)","bc21d94a":"# drop description column\ndf.drop(['description'], axis=1, inplace=True)","1cd32ca0":"df.info()","784bd9fe":"# filter out sub_df to work with\nsub_df = df[['accommodates', 'beds',  'price', 'minimum_nights','distance', 'size']]","f0702f18":"# split datasets\ntrain_data = sub_df[sub_df['size'].notnull()]\ntest_data  = sub_df[sub_df['size'].isnull()]\n\n# define X\nX_train = train_data.drop('size', axis=1)\nX_test  = test_data.drop('size', axis=1)\n\n# define y\ny_train = train_data['size']","f5748b5e":"print(\"Shape of Training Data:\", train_data.shape)\nprint(\"Shape of Test Data:    \",test_data.shape)\nprint(\"\\nShape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\nprint(\"\\nShape of y_train:\", y_train.shape)","4fd4f416":"# import Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\n# instantiate\nlinreg = LinearRegression()\n\n# fit model to training data\nlinreg.fit(X_train, y_train)","300d0c08":"# making predictions\ny_test = linreg.predict(X_test)","3a956bb9":"y_test = pd.DataFrame(y_test)\ny_test.columns = ['size']\nprint(y_test.shape)\ny_test.head()","4cf4b7d3":"print(X_test.shape)\nX_test.head()","7b88fbf2":"# make the index of X_test to an own dataframe\nprelim_index = pd.DataFrame(X_test.index)\nprelim_index.columns = ['prelim']\n\n# ... and concat this dataframe with y_test\ny_test = pd.concat([y_test, prelim_index], axis=1)\ny_test.set_index(['prelim'], inplace=True)\ny_test.head()","816dba75":"new_test_data = pd.concat([X_test, y_test], axis=1)","ff804d41":"new_test_data['size'].isna().sum()","704d323f":"# combine train and test data back to a new sub df\nsub_df_new = pd.concat([new_test_data, train_data], axis=0)\n\nprint(sub_df_new.shape)\nsub_df_new.head()","29d8ca8e":"sub_df_new['size'].isna().sum()","12933590":"# prepare the multiple columns before concatening\ndf.drop(['accommodates', 'beds',  'price', 'minimum_nights','distance', 'size'], \n            axis=1, inplace=True)\n","4c7af222":"# concate back to complete dataframe\nsub_df_new=round(sub_df_new)\ndf_new = pd.concat([sub_df_new, df], axis=1)\n\nprint(df_new.shape)\ndf_new.head(2)","162ea1c8":"df_new['size'].isna().sum()","e33c075c":"df_new['size'].describe()","a6a71053":"df_new.drop(df_new[ (df_new['size'] == 0.) | (df_new['size'] > 300.) ].index, axis=0, inplace=True)","b4196d60":"print(\"The dataset has {} rows and {} columns - after being engineered.\".format(*df_new.shape))","d67e0a59":"from collections import Counter","678c7c72":"results = Counter()\ndf_new['amenities'].str.strip('{}')\\\n               .str.replace('\"', '')\\\n               .str.lstrip('\\\"')\\\n               .str.rstrip('\\\"')\\\n               .str.split(',')\\\n               .apply(results.update)\n\nresults.most_common(30)","7f29d3bb":"# create a new dataframe\nsub_df = pd.DataFrame(results.most_common(30), columns=['amenity', 'count'])","dc60256d":"# plot the Top 20\nsub_df.sort_values(by=['count'], ascending=True).plot(kind='barh', x='amenity', y='count',  \n                                                      figsize=(10,7), legend=False, color='darkgrey',\n                                                      title='Amenities')\nplt.xlabel('Count');","d5f70dda":"df_new['Heating'] = df_new['amenities'].str.contains('Heating')\ndf_new['TV'] = df_new['amenities'].str.contains('TV')\ndf_new['Wifi'] = df_new['amenities'].str.contains('Wifi')\ndf_new['Kitchen'] = df_new['amenities'].str.contains('Kitchen')\ndf_new['Essentials'] = df_new['amenities'].str.contains('Essentials')\ndf_new['Hair dryer'] = df_new['amenities'].str.contains('Hair dryer')","b559c5f4":"df_new.drop(['amenities'], axis=1, inplace=True)","05ca50d9":"df_new.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, figsize=(10,7), \n        c=\"price\", cmap=\"gist_heat_r\", colorbar=True, sharex=False);","c32f5f1c":"df_new['neighbourhood_cleansed'].value_counts().sort_values().plot(kind='barh',figsize=(10,30), color='darkgrey')\nplt.title('Number of Accommodations per District');","bb6db92f":"# group_by neighbourhood groups, take the median price and store new values in sub_df \ndf_new_grouped = pd.DataFrame(df_new.groupby(['neighbourhood_cleansed'])['price'].agg(np.median))\ndf_new_grouped.reset_index(inplace=True)\n\n# plot this \ndf_new_grouped.sort_values(by=['price'], ascending=True)\\\n          .plot(kind='barh', x='neighbourhood_cleansed', y='price', \n                figsize=(10,30), legend=False, color='salmon')\n\nplt.xlabel('\\nMedian Price', fontsize=12)\nplt.ylabel('District\\n', fontsize=12)\nplt.title('\\nMedian Prices by Neighbourhood\\n', fontsize=14, fontweight='bold');","e5a40e22":"red_square = dict(markerfacecolor='salmon', markeredgecolor='salmon', marker='.')\n\ndf_new.boxplot(column='price', by='neighbourhood_cleansed', \n           flierprops=red_square, vert=False, figsize=(10,30))\n\nplt.xlabel('\\nMedian Price', fontsize=12)\nplt.ylabel('District\\n', fontsize=12)\nplt.title('\\nBoxplot: Prices by Neighbourhood\\n', fontsize=14, fontweight='bold')\n\n# get rid of automatic boxplot title\nplt.suptitle('');","7ae175fa":"df_new.plot.scatter(x=\"distance\", y=\"price\", figsize=(9,6), c='dimgrey')\nplt.title('\\nRelation between Distance & Median Price\\n', fontsize=14, fontweight='bold');","1ccec80a":"sns.jointplot(x=df_new[\"distance\"], y=df_new[\"price\"], kind='hex')\nplt.title('\\nRelation between Distance & Median Price\\n', fontsize=14, fontweight='bold');","acca0d8d":"plt.figure(figsize=(10,30))\nsns.heatmap(df_new.groupby(['neighbourhood_cleansed', 'beds']).price.median().unstack(), \n            cmap='Reds', annot=True, fmt=\".0f\")\n\nplt.xlabel('\\nBeds', fontsize=12)\nplt.ylabel('District\\n', fontsize=12)\nplt.title('\\nHeatmap: Median Prices by Neighbourhood and Number of Beds\\n\\n', fontsize=14, fontweight='bold');","220ca6af":"df_new.columns","81657da7":"df_new.info()","7eb63c87":"df_new.drop([ 'neighbourhood_cleansed','beds','host_has_profile_pic',\n     'property_type', \n      'has_availability', 'review_scores_rating',\n      'instant_bookable',  'number_of_reviews', 'TV', 'Wifi', 'Kitchen', 'accommodates',\n      'Essentials', 'Hair dryer'], axis=1, inplace=True)","15a7235f":"for col in [ 'room_type', 'Heating', 'price']:\n    df_new[col] = df_new[col].astype('category')","ea742a23":"# define our target\ntarget = df_new[[\"price\"]].astype('float64')\n\n# define our features \nfeatures = df_new.drop([\"price\"], axis=1)\ntarget.info()","3a4caac0":"num_feats = features.select_dtypes(include=['float64', 'int64', 'bool']).copy()\n\n# one-hot encoding of categorical features\ncat_feats = features.select_dtypes(include=['category']).copy()\ncat_feats = pd.get_dummies(cat_feats)","b1eb96c2":"features_recoded = pd.concat([num_feats, cat_feats], axis=1)","084ffb0a":"print(features_recoded.shape)\n","e2b478e0":"features_recoded.head(20)","b74ee79d":"features_recoded.info()","2d713acf":"# import train_test_split function\nfrom sklearn.model_selection import train_test_split\n# import metrics\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# split our data\nX_train, X_test, y_train, y_test = train_test_split(features_recoded, target, test_size=0.2)","d02b2ad9":"# scale data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test  = sc.transform(X_test)\nprint(y_train.columns)\ny_train.info()\n","88ff9af5":"# create a baseline\nbooster = xgb.XGBRegressor()","f8e06d4d":"from sklearn.model_selection import GridSearchCV\n\n# create Grid\nparam_grid = {'n_estimators': [100, 150, 200],\n              'learning_rate': [0.01, 0.05, 0.1], \n              'max_depth': [3, 4, 5, 6, 7],\n              'colsample_bytree': [0.6, 0.7, 1],\n              'gamma': [0.0, 0.1, 0.2]}\n\n# instantiate the tuned random forest\nbooster_grid_search = GridSearchCV(booster, param_grid, cv=3, n_jobs=-1)\n\n# train the tuned random forest\nbooster_grid_search.fit(X_train, y_train)\n\n# print best estimator parameters found during the grid search\nprint(booster_grid_search.best_params_)","18a41b19":"# instantiate xgboost with best parameters\nbooster = xgb.XGBRegressor(colsample_bytree=0.7, gamma=0.1, learning_rate=0.05, \n                           max_depth=7, n_estimators=100, random_state=4)\n\n# train\nbooster.fit(X_train, y_train)\n\n# predict\ny_pred_train = booster.predict(X_train)\ny_pred_test = booster.predict(X_test)","a3867d17":"RMSE = np.sqrt(mean_squared_error(y_test, y_pred_test))\nprint(f\"RMSE: {round(RMSE, 4)}\")","35bbeacc":"r2 = r2_score(y_test, y_pred_test)\nr2\nprint(f\"r2: {round(r2, 4)}\")","fd1b5662":"xg_train = xgb.DMatrix(data=X_train, label=y_train)","93ffacd0":"params = {'colsample_bytree':0.6, 'gamma':0.2, 'learning_rate':0.05, 'max_depth':6}\n\ncv_results = xgb.cv(dtrain=xg_train, params=params, nfold=3,\n                    num_boost_round=200, early_stopping_rounds=10, \n                    metrics=\"rmse\", as_pandas=True)","e4e5b695":"cv_results.head()","e7597265":"cv_results.tail()","5705d652":"# plot the important features\nfeat_importances = pd.Series(booster.feature_importances_, index=features_recoded.columns)\nfeat_importances.nlargest(15).sort_values().plot(kind='barh', color='darkgrey', figsize=(10,5))\nplt.xlabel('Relative Feature Importance with XGBoost');","6c92f619":"***Recoding Categorical Features***","07b056aa":"#### 2.4. Feature Engineering 1: Distance to Centroid (Duomo di Milano) of Milan\n\n","0ad65c8f":"**> Price Differences on a Map**","cb3cea27":"Now let's convert all string columns into categorical ones:","d52148bf":"The `description` column seems to be rich in content. Let's extract \n- all double-digit or three-digit numbers \n- that are followed by one of the first characters is \"s\" or \"m\" (covering \"sqm\", \"square meters\", \"m2\" etc.) with to make sure that second character would \"q\" or \"2\" (to eliminate posibility of word \"minute') \n- may or may not be connected by white space. \n\nSingle- or more than three-digit numbers for accommodation sizes are quite unlikely.\n\nI know, it's a bold move - but let's give it a try...","3f552752":"One of the most important pieces of information for predicting the rate is the size. Since there is no `size` column so we need to see if any other column is enough to provide infor mation about the size. (Besides, size in Italy can expressed in square meters, square feet, m2, mq, etc.)\n\nLet's check, if the column `description` reveals any information about size instead: ","23a2687a":"Some of the important hyperparameters to tune an XGBoost are:\n- `n_estimators` $\\;\\;\\;\\;\\;$ = Number of trees one wants to build.\n- `learning_rate` $\\;\\;\\;\\;$= Rate at which our model learns patterns in data. After every round, it shrinks the feature weights to reach the best optimum.\n- `max_depth` $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$= Determines how deeply each tree is allowed to grow during any boosting round.\n- `colsample_bytree` = Percentage of features used per tree. \n- `gamma` $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$= Specifies the minimum loss reduction required to make a split.","b15010aa":"We shouldn't miss investigating the `price` - it might need some cleaning to be of use to us:","20832905":"In order to build more robust models, it is common to conduct a k-fold cross validation where all the entries in the original training dataset are used for both training and validation. XGBoost supports k-fold cross validation via the cv method. All we have to do is specify the `nfolds` parameter, which is the number of cross validation rounds you want to build. \n\nAlso, it supports many other parameters:\n- `num_boost_round` $\\;\\;\\;\\;\\;\\;\\;\\;$ = Specifies the number of trees to build (analogous to n_estimators).\n- `metrics` $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ = Specifies the evaluation metrics to be checked during CV.\n- `as_pandas` $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$= Returns the results in a decent pandas DataFrame.\n- `early_stopping_rounds` = Finishes model training early if the hold-out metric does not improve for a given number of rounds. \n\nWe will have to first convert the dataset into an optimized data structure called DMatrix so that XGBoost's cross validation method is supported.","31cf6408":"A bit messy, as expected! \n\n75% of the apartments charge up to 112\u20ac - but the maximum value is 11164\u20ac. Let's decide on a limit of 300\u20ac, after which the outliers seem to \"fringe\", and drop all records that charge more than that. Oddly enough, we have prices set to zero. So let's drop those records, too:","2419b4b6":"## For further updates of this Kernel check into this GitHub Link: https:\/\/github.com\/ankitpatel1661\/Milan-Airbnb-Predicting-Prices\n\n\nAll resources used in this notebook are listed below.\n\nData\n- http:\/\/insideairbnb.com\/get-the-data.html\n\nImputing missing values with Linear Regression\n- https:\/\/towardsdatascience.com\/the-tale-of-missing-values-in-python-c96beb0e8a9d\n\nXGBoost\n- https:\/\/www.datacamp.com\/community\/tutorials\/xgboost-in-python\n- https:\/\/www.kaggle.com\/marcelo06\/cross-validation-with-xgboost-python\n\nGeocoding\n- https:\/\/pypi.org\/project\/geopy\/\n\nVisualizations\n- https:\/\/python-graph-gallery.com\n- https:\/\/www.kaggle.com\/kostyabahshetsyan\/boston-airbnb-visualization\n\nInspiration for Analysis\n- https:\/\/towardsdatascience.com\/digging-into-airbnb-data-reviews-sentiments-superhosts-and-prices-prediction-part1-6c80ccb26c6a\n- https:\/\/www.kaggle.com\/ibjohnsson\/predicting-listing-prices\n- https:\/\/www.kaggle.com\/mathvv\/prediction-on-house-prices-xgboost-tutorial\n- https:\/\/towardsdatascience.com\/improving-airbnb-yield-prediction-with-text-mining-9472c0181731\n- https:\/\/github.com\/joaeechew\/airbnb_nlp\/blob\/master\/Capstone%20Project.ipynb\n\n\n>  # <font color='orange'> Please UPVOTE if you found these helpful :) <\/font>","45e2c174":"#### 4.4. Cross Validation","2a38e09d":"Let's remove the dollar signs in column and convert the string values into numerical ones:","f4301ec8":"### 6. Appendix ","7c12967c":"### 2. Preprocessing the Data\n#### 2.1. Deciding which columns to work with\nLet's imagine we are in the shoes of someone who'd like to offer their home. Fixed features of our property include its rooms, size, and location. We also can decide on how we want to be listed: with a picture or not, how many minimum nights we want a guest to stay, whether we are instantly bookable, is there heater available, etc. But we can neither be a \"super host\", nor do we have any reviews yet to show - although they can be very important for setting a price. So, let's focus only on features we can influence:","d95823f9":"***Predicting missing values with regression***","362a4ddf":"**> Price Differences by Neighbourhood**","3b879fe5":"Machine learning algorithms generally need all data - including categorical data - in numeric form. To satisfy these algorithms, categorical features are converted into separate binary features called dummy variables. Therefore, we have to find a way to represent these variables as numbers before handing them off to the model. One typical way of doing this in one-hot encoding, which creates a new column for each unique category in a categorical variable. Each observation receives a 1 in the column for its corresponding category (= \"HOT\") and a 0 in all other new columns. To conduct one-hot encoding, we use the pandas get_dummies function.","fb802867":"Comparing the results, we noticed that out of 18098 only 2785 descriptions contains information about size which are not much usefull for us in this case. So we will try something new ! We genrally know that when we travel usally start to find Airbnb near main tourist attraction.Duomo di milan is in zone 1 and it's one of the main attraction. Let see how distance from centroid of milan plays role in price prediction. ","375e9907":"One of the challenges in building models is mixing features that have different scales. Look at our dataset and compare bathrooms with size or maximum_nights. When we mix units with ranges that have different orders of magnitude, our models may not be able to find the proper coefficients. To account for this problem, we standardize or normalize the features.","39958860":"Airbnb has successfully disrupted the traditional hospitality industry as more and more travelers decide to use Airbnb as their primary accommodation provider. Since its inception in 2008, Airbnb has seen an enormous growth, with the number of rentals listed on its website growing exponentially each year.\n\nIn Italy, no city is more popular than Milan for tech industry as well as  for tourism industry. That implies that Milan is one of the hottest markets for Airbnb in Europe, with over 18,783 listings as of August 2020. With a size of 181.8 km\u00b2, this means there are roughly 104 homes being rented out per km\u00b2 in Milan on Airbnb!\n\nConsidering the possibility that I might have to relocate for a new data science job, but want to keep my current flate (which is quite cheap!), I might wonder if it could be worth it to offer my jewel on Airbnb. Could this perhaps be a profitable option? However, it is difficult for potential hosts to know what the true value of their home is, and how in-demand their home might be. And since location and furniture are obviously fixed for the most part, is there anything else a host can influence - such as description, communication patterns, and\/or additional services to boost their earnings?\n\nThe following question will drive this project:<br>\n> **Can we determine a fairly spot-on daily price for a new accommodation that fits into its specific market environment and competitors in Milan?** <br>\n\nThe question focuses on the accommodation features and decisions a new host can make with regards to initial presentation, i.e. posting a picture of him- or herself on the website, determining a minimum length of stay, offering instant bookings etc. A machine learning algorithm will be applied to try to get an answer. \n\n### The dataset\n\nIn the first notebook, I will perform an analysis of the detailed Milan listings data, sourced from the Inside Airbnb website, in order to understand the rental landscape and try to recommend a price for a newbie entering the market. The dataset is named `listings.csv.gz` and was scraped on September 2nd 2020.","faae27cc":"Location is always an important factor in lodging services. To make it more descriptive, I decided to calculate each accommodation's distance to the so-called centroid (Duomo di Milano) of Milan instead of just relying on the neighbourhoods or areas. \n\nFor our convenience, let's write a quick function that does this, apply it to each accommodation, and store the values in a new column:","ac138c45":"#### 2.3. Dealing with Missing Values","2c090ef3":"***Extracting size from text***","1acae1e3":"### 4. Modeling the Data","a30e3764":"### 3. Exploratory Data Analysis (EDA)","1b3f4170":"### 1. Obtaining and Viewing the Data \n","fd335fb2":"#### 2.5. Feature Engineering 2: Lodging Size","d1454cd2":"### 5. Interpreting the Data","0f46dbd7":"#### 4.1. Preparing Target and Features","746f1ba1":"Let's add columns with amenities that are somewhat unique and not offered by all hosts: \n- a Heating\n- a TV\n- Wifi\n- Kitchen\n- Essentials and\n- Hair dryer.\n\nAfter doing this, let's drop the original column:","42de1b06":"#### 4.3. Training an XGBoost Regressor","6352dab8":"By the way, how many different **room types** do we have?","fa5e433e":"I'm interested in what amenities hosts offer their guests, and in order to enrich our prediction, whether we can determine what some of the more special and\/or rare amenities might be that make a property more desirable.","623d4294":"No, so we don't need to drop any rows.","21fcaa29":"#### 2.6. Feature Engineering 3: Lodging Amenities","ed1b07a4":"By the way, how many different **property type** do we have?","05042f55":"**> Price Differences by Number of beds**","2dcf9f3a":"#### 2.2. Cleaning Columns","78cac5d8":"Let's first check if there are any null values in the `price` column:","8ae8550d":"* As we see, the **most important features are size, minimum nights, and locatoin**, which account for approximately 45% of the daily price. Other top features are the number of people the apartment distance.","6d017bae":"**> Price Differences by Accommodation Distance to Center of Berlin**","5c50f081":"# Table of Contents\n\n\n### 1. Obtaining and Viewing the Data\n### 2. Preprocessing the Data\n#### 2.1. Deciding which columns to work with\n#### 2.2. Cleaning Price Columns\n#### 2.3. Dealing with Missing Values\n#### 2.4. Feature Engineering 1: Distance to Centroid of Milan\n#### 2.5. Feature Engineering 2: Lodging Size\n#### 2.6. Feature Engineering 3: Lodging Amenities\n\n### 3. Exploratory Data Analysis (EDA)\n### 4. Modeling the Data\n#### 4.1. Preparing Target and Features\n#### 4.2. Splitting and Scaling the Data\n#### 4.3. Training an XGBoost Regressor\n#### 4.4. Cross Validation\n\n### 5. Interpreting the Data\n\n\n### 6. Appendix","025f7e54":"As calculated further up, 102 of our records still don't have a beds information. That means we have a problem! Dropping these records isn't an option as we would loose too much valuable information. Simply replacing it with the mean or median makes no sense. That leaves a third option: predict the missing value with a Machine Learning Algorithm. To not make it too complicated, we'll only use numerical features. Next, we have to split our data into \n- a) a training set where we have beds information and \n- b) a test set where we don't.","b626f90c":"#### 4.2. Splitting and Scaling the Data","cd876a42":"## Predicting the price for an Airbnb Host in Milan"}}