{"cell_type":{"249e3032":"code","91a4d7d9":"code","b2be20ba":"code","0adc3cd0":"code","0eba29b9":"code","d34c5293":"code","a8056799":"code","078c3440":"code","ef883a93":"code","326aece9":"code","60a099f8":"code","dd899faa":"code","3443a2d4":"code","55605979":"code","609a291f":"code","068882d5":"code","6a4d2a15":"code","51a1ea64":"code","a6d6ed28":"code","8bbbd779":"code","d581d4ed":"code","fa2eef9e":"code","2315e1ea":"code","2ed01513":"code","6c773049":"code","c7fe0263":"code","7bcc95e2":"code","b18cce15":"code","4ef8d667":"code","09ae597b":"code","2aec6cbb":"code","90ced894":"code","8c99bf5a":"code","d49098ca":"code","f87f144c":"markdown"},"source":{"249e3032":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport category_encoders as ce\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\nfrom tqdm.notebook import tqdm_notebook","91a4d7d9":"df = pd.read_csv(\"..\/input\/chocolate-bar-ratings\/flavors_of_cacao.csv\",header=None,names= [\"Company\",\"Specific Bean Originor\",\"REF\",\"Review Date\", \"Cocoa Percent\",\"Company Location\",\"rating\",\"Bean Type\",\"Broad Bean Origin\"])","b2be20ba":"df.head()","0adc3cd0":"df.drop(df.index[0],axis = 0,inplace=True)","0eba29b9":"df.isnull().sum()","d34c5293":"df.dropna(inplace = True)","a8056799":"def type_cast(df,column,type):\n    return df[column].astype(type)","078c3440":"def mean_encoding(df,x,y):\n    mean_encoder=ce.TargetEncoder()\n    df = mean_encoder.fit_transform(df[x],df[y])\n    return df","ef883a93":"def clean_rating(df):\n    df[\"rating\"] = type_cast(df,\"rating\",\"float\")","326aece9":"def clean_Specific_Bean_Originor(df):\n    df_Bean_Originor = mean_encoding(df,\"Specific Bean Originor\",\"rating\")\n    df['Specific Bean Originor'] = df_Bean_Originor['Specific Bean Originor'].round(3)","60a099f8":"def clean_REF(df):\n    df[\"REF\"] = type_cast(df,\"REF\",\"int\")","dd899faa":"def clean_Cocoa_Percent(df):\n    df['Cocoa Percent'] =df['Cocoa Percent'].map(lambda x: x.replace(\"%\", \"\"))\n    df['Cocoa Percent'] = type_cast(df,\"Cocoa Percent\",\"float\")","3443a2d4":"def clean_Company_Location(df):\n    df_Company_Location = mean_encoding(df,\"Company Location\",\"rating\")\n    df['Company Location'] = df_Company_Location['Company Location'].round(3)","55605979":"def clean_Bean_Type(df):\n    empty_value = df[\"Bean Type\"].values[0]\n    def replace_nan(value):\n        if value == empty_value:\n            return \"Unknown\"\n        else:\n            return value\n    for col in df.columns:\n        if df[col].dtype == 'O':\n            df[col] = df[col].apply(lambda value: replace_nan(value))\n    df_Bean_Type = mean_encoding(df,\"Bean Type\",\"rating\")\n    df[\"Bean Type\"] = df_Bean_Type[\"Bean Type\"].round(3)","609a291f":"def clean_Broad_Bean_Origin(df):\n    df_Broad_Bean_Origin = mean_encoding(df,\"Broad Bean Origin\",\"rating\")\n    df[\"Broad Bean Origin\"] = df_Broad_Bean_Origin[\"Broad Bean Origin\"].round(3)","068882d5":"columns = [\"Company\",\"Review Date\"]\ndef columns_to_drop(df,columns):\n    df_final = df.drop(columns,axis = 1)\n    return df_final","6a4d2a15":"def preprocess_data(df):\n    clean_rating(df)\n    clean_Specific_Bean_Originor(df)\n    clean_REF(df)\n    clean_Cocoa_Percent(df)\n    clean_Company_Location(df)\n    clean_Bean_Type(df)\n    clean_Broad_Bean_Origin(df)\n    df_final = columns_to_drop(df,columns)\n    return df_final","51a1ea64":"df_final = preprocess_data(df)","a6d6ed28":"df_final.dtypes","8bbbd779":"scale_X=MinMaxScaler()\nscale_Y=MinMaxScaler()\nX = df_final.drop([\"rating\"],axis =1)\nY = df_final['rating']\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state = 27)\nX_train = scale_X.fit_transform(X_train)\nX_test = scale_X.fit_transform(X_test)\nY_train = scale_Y.fit_transform(np.array(Y_train).reshape(-1, 1)).squeeze()\nY_test = scale_Y.fit_transform(np.array(Y_test).reshape(-1, 1)).squeeze()\n","d581d4ed":"#class ofr sigmoid neuron:\nclass sigmoidNeuron:\n  def __init__(self):\n    self.w=None\n    self.b=None \n  def pereceptron(self,x):\n    return np.dot(self.w,x.T)+self.b\n  def sigmoid(self,x):\n    return 1.0\/(1.0+np.exp(-x))\n  def grad_w(self,x,y):\n    y_pred=self.sigmoid(self.pereceptron(x))\n    return (y_pred-y)*y_pred*(1-y_pred)*x\n  def grad_b(self,x,y):\n    y_pred=self.sigmoid(self.pereceptron(x))\n    return (y_pred-y)*y_pred*(1-y_pred)\n  def fit(self,X,Y,epochs=1,learning_rate=1,ini=False,display_loss=False):\n\n    if ini:\n        self.w=np.random.randn(1,X.shape[1])\n        self.b=0\n    if display_loss:\n      loss={}\n\n\n    for i in range(epochs):\n      dw=0\n      db=0\n      for x,y in zip(X,Y):\n        dw+=self.grad_w(x,y)\n        db+=self.grad_b(x,y)\n      self.w-=learning_rate*dw\n      self.b-=learning_rate*db\n      if display_loss:\n        y_pred=self.sigmoid(self.pereceptron(X))\n        loss[i]=mean_squared_error(Y,y_pred.squeeze())\n    if display_loss:\n      plt.plot(loss.values())\n      plt.xlabel('epochs')\n      plt.ylabel('loss')\n      plt.show()\n      \n\n  def predict(self,X):\n    Y_pred=[]\n    for x in X:\n      y_pred=self.sigmoid(self.pereceptron(x))\n      Y_pred.append(y_pred)\n\n    return Y_pred\n      #number of updates is eual to numbrr of epochs\n","fa2eef9e":"sn=sigmoidNeuron()\nsn.fit(X_train,Y_train,1300,0.001,ini = True,display_loss=True)","2315e1ea":"sn.w","2ed01513":"sn.b","6c773049":"Y_pred = np.array(sn.predict(X_test)).squeeze()","c7fe0263":"mean_squared_error(Y_test,Y_pred)","7bcc95e2":"r2_score(Y_test,Y_pred)","b18cce15":"class FFSNNetwork:\n  def __init__(self,n_inputs,n_outputs=1,hidden_sizes=[2]):\n    self.nx=n_inputs\n    self.ny=n_outputs\n    self.nh=len(hidden_sizes)\n    #self.sizes reffer to size of each layer (ie) \n    self.sizes=[self.nx]+hidden_sizes+[self.ny]\n    self.W={}\n    self.B={}\n    for i in range(self.nh+1):\n      self.W[i+1]=np.random.randn(self.sizes[i],self.sizes[i+1])\n      self.B[i+1]=np.zeros((1,self.sizes[i+1]))\n  def sigmoid(self,x):\n    return 1.0\/(1.0+np.exp(-x))\n  def forward_pass(self,x):\n    self.A={}\n    self.H={}\n    self.H[0]=x.reshape(1,-1)\n    for i in range(self.nh+1):\n      self.A[i+1]=np.matmul(self.H[i],self.W[i+1]+self.B[i+1])\n      self.H[i+1]=self.sigmoid(self.A[i+1])\n    return self.H[self.nh+1]\n  def grad_sigmoid(self, x):\n    return x*(1-x) \n    \n  def grad(self, x, y):\n    self.forward_pass(x)\n    self.dW = {}\n    self.dB = {}\n    self.dH = {}\n    self.dA = {}\n    L = self.nh + 1\n    self.dA[L] = (self.H[L] - y)\n    for k in range(L, 0, -1):\n      self.dW[k] = np.matmul(self.H[k-1].T, self.dA[k])\n      self.dB[k] = self.dA[k]\n      self.dH[k-1] = np.matmul(self.dA[k], self.W[k].T)\n      self.dA[k-1] = np.multiply(self.dH[k-1], self.grad_sigmoid(self.H[k-1]))\n\n  def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, display_loss=False):\n    \n    # initialise w, b\n    if initialise:\n      for i in range(self.nh+1):\n        self.W[i+1] = np.random.randn(self.sizes[i], self.sizes[i+1])\n        self.B[i+1] = np.zeros((1, self.sizes[i+1]))\n      \n    if display_loss:\n      loss = {}\n    \n    for j in range(epochs):\n\n      dW = {}\n      dB = {}\n      for i in range(self.nh+1):\n        dW[i+1] = np.zeros((self.sizes[i], self.sizes[i+1]))\n        dB[i+1] = np.zeros((1, self.sizes[i+1]))\n      for x, y in zip(X, Y):\n        self.grad(x, y)\n        for i in range(self.nh+1):\n          dW[i+1] += self.dW[i+1]\n          dB[i+1] += self.dB[i+1]\n        \n      m = X.shape[1]\n      for i in range(self.nh+1):\n        self.W[i+1] -= learning_rate * dW[i+1] \/ m\n        self.B[i+1] -= learning_rate * dB[i+1] \/ m\n      \n      if display_loss:\n        Y_pred = self.predict(X)\n        loss[j] = mean_squared_error(Y_pred, Y)\n        \n    if display_loss:\n      plt.plot(np.array(list(loss.values())).astype(float))\n      plt.xlabel('Epochs')\n      plt.ylabel('Mean Squared Error')\n      plt.show()\n      \n  def predict(self, X):\n    Y_pred = []\n    for x in X:\n      y_pred = self.forward_pass(x)\n      Y_pred.append(y_pred)\n    return np.array(Y_pred).squeeze()\n ","4ef8d667":"ffsnn = FFSNNetwork(6, hidden_sizes=[10])\nffsnn.fit(X_train, Y_train, epochs=6000, learning_rate=.001, display_loss=True)","09ae597b":"ffsnn.W","2aec6cbb":"Y_pred_train=ffsnn.predict(X_train)\nY_pred_val=ffsnn.predict(X_test)\nr2_train=r2_score(Y_train,Y_pred_train)\nr2_val=r2_score(Y_test,Y_pred_val)","90ced894":"mse_train=mean_squared_error(Y_train,Y_pred_train)\nmse_val=mean_squared_error(Y_test,Y_pred_val)","8c99bf5a":"rmse_train=np.sqrt(mean_squared_error(Y_train,Y_pred_train))\nrmse_val=np.sqrt(mean_squared_error(Y_test,Y_pred_val))","d49098ca":"print(\"R2 Score - train\",r2_train)\nprint(\"R2 Score - val\",r2_val)\nprint(\"MSE Score - train\",mse_train)\nprint(\"MSE Score - val\",mse_val)\nprint(\"RMSE Score - train\",rmse_train)\nprint(\"RMSE Score - val\",rmse_val)","f87f144c":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https:\/\/deepnote.com?utm_source=created-in-deepnote-cell&projectId=4f9af976-8588-4335-a50b-1415b08a9dfe' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image\/svg+xml;base64,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' > <\/img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote<\/span><\/a>"}}