{"cell_type":{"19f5d382":"code","c60442bf":"code","375eb512":"code","74948c4c":"code","b0a0f703":"code","249115a9":"code","dd67d0a8":"code","bc58af2e":"code","03525f90":"code","7a9fa31d":"code","13c7a850":"code","bc9c21c6":"code","109251d0":"code","dbb0c912":"code","100a530a":"code","c6aea6aa":"code","09a8daa7":"code","980c84ef":"code","ad8cb5a5":"code","a213f2f9":"code","cecc491e":"code","b3860696":"code","cd697b03":"markdown","86c36982":"markdown","1693aaa7":"markdown"},"source":{"19f5d382":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c60442bf":"## Uploading the data\n\ntitanic_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\ntitatnic_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","375eb512":"##take a peek at the top few rows of the training set:\n\ntitanic_train.head()","74948c4c":"## to see if there's any data missing\n\ntitanic_train.info()","b0a0f703":"## .describe() function to further explore the data\n\ntitanic_train.describe()","249115a9":"## Import BaseEstimator and TransformerMixin from sklearn.base\n\nfrom sklearn.base import BaseEstimator, TransformerMixin","dd67d0a8":"\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","bc58af2e":"## For the numerical data; DataFrameSelector to select all the numerical columns \n## SimpleImputer to fill the missing values with the median\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nnum_pipeline = Pipeline([\n        (\"select_numeric\", DataFrameSelector([\"Age\", \"SibSp\", \"Parch\", \"Fare\"])),\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n    ])","03525f90":"## fit the training data in this pipeline\n\nnum_pipeline.fit_transform(titanic_train)","7a9fa31d":"## We will also need an imputer for the categorical columns \n## the regular SimpleImputer does not work on those\n\nclass MostFrequentImputer (BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n                                        index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.most_frequent_)","13c7a850":"## Import OneHotEncoder from sklearn.preprocessing \n## This creates a binary column for each category and returns a sparse matrix or dense array\n\nfrom sklearn.preprocessing import OneHotEncoder","bc9c21c6":"## Pipeline for catagorical attributes\n\ncat_pipeline = Pipeline([\n        (\"select_cat\", DataFrameSelector([\"Pclass\", \"Sex\", \"Embarked\"])),\n        (\"imputer\", MostFrequentImputer()),\n        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n    ])","109251d0":"## fit the training data into this pipeline\n\ncat_pipeline.fit_transform(titanic_train)","dbb0c912":"## let's join the numerical and categorical pipelines with FeatureUnion\n\nfrom sklearn.pipeline import FeatureUnion\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", num_pipeline),\n        (\"cat_pipeline\", cat_pipeline),\n    ])","100a530a":"## Fit the training data in this pipeline\n\nX_train = preprocess_pipeline.fit_transform(titanic_train)\n\n## Save the labels in y_train\n\ny_train = titanic_train[\"Survived\"]","c6aea6aa":"## Train a RandomForest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(n_estimators=100, min_samples_leaf = 10, random_state=42)\nforest_clf.fit(X_train, y_train)","09a8daa7":"## Evaluate the model using cross_val_score\n\nfrom sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\nforest_scores.mean()","980c84ef":"titanic_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntitanic_test.head()","ad8cb5a5":"X_test_prepared = preprocess_pipeline.transform(titanic_test)","a213f2f9":"final_predictions = forest_clf.predict(X_test_prepared)","cecc491e":"final_predictions","b3860696":"output = pd.DataFrame({'PassengerId': titanic_test.PassengerId, 'Survived': final_predictions})\noutput.to_csv('my_submission.csv', index = False) ","cd697b03":"## Predictions","86c36982":"## Let's create a pre-processing pipeline","1693aaa7":"## Preparing test data for predictions"}}