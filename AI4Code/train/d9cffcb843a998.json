{"cell_type":{"4e89080c":"code","bdbe253b":"code","4332f437":"code","6f714e97":"code","6da5ad58":"code","375c6335":"code","a5deab18":"code","1cef7148":"code","b2193174":"code","86e69e42":"code","1a65292d":"code","3120a9f8":"code","d59b87ca":"code","9b881ff8":"code","f180c5ce":"code","23ace47d":"code","74c69a16":"code","c13565cf":"code","5ebd5b81":"code","d5b2e7fc":"code","ca71c238":"code","bde02bc0":"code","9b3ea5db":"code","910b0601":"code","021bce40":"code","fd3e70b4":"code","1584600c":"code","1a547402":"code","ef7f2b37":"code","23bc18ef":"code","abe2145f":"code","b3cf622a":"code","350ab282":"code","62de85e8":"code","50685096":"code","b26b5bc3":"code","ca409487":"code","04203a9c":"code","bfa472cd":"code","0fafa6a3":"code","7be061cb":"code","700420c6":"code","78f2d5af":"code","eb8b9235":"code","e1d200ef":"code","62fcc50e":"code","6564c662":"code","095a8508":"code","958a5a9b":"code","7952b078":"code","2396a075":"code","02e94181":"code","17db74a7":"code","13bf4477":"code","8e4ddcb1":"code","c4198989":"code","6391d58f":"code","e773ae06":"code","e0a0175d":"code","e62e0c3b":"code","58009736":"code","4708631a":"code","8e22fd59":"code","f890f009":"code","648882fb":"code","83e3896c":"code","2b67669f":"code","b43c8d5f":"code","69277895":"code","9b0e2d9f":"code","e652ad14":"code","63b24205":"code","9086be28":"code","670221b4":"code","6ca51c21":"code","949307df":"code","7572a157":"code","490cf497":"code","804eab20":"code","e224e47a":"code","89406cd5":"code","8a59f9c5":"markdown","405ca4ae":"markdown","2286b38e":"markdown","9ab9831d":"markdown","2478796a":"markdown","85163339":"markdown","f1a6b46e":"markdown","e7ec6641":"markdown","43e545e8":"markdown","7266b03a":"markdown","7f610e52":"markdown","e6d6f460":"markdown","d2a015f4":"markdown","e0da2137":"markdown","035b96b3":"markdown","c2bfe6a6":"markdown","72997329":"markdown","a822df2e":"markdown","f6123175":"markdown","2dbefabf":"markdown","2800d69c":"markdown","3452a8d6":"markdown","44d507a7":"markdown","ab206c3b":"markdown","a91c192a":"markdown","0b1abbfd":"markdown","d0d69dfc":"markdown","f8ed4ce8":"markdown","28f7719e":"markdown","065c0424":"markdown","0e36a534":"markdown","74fbd3cb":"markdown","fc49bc4b":"markdown","505dba88":"markdown","c38a78e9":"markdown","5cd729f6":"markdown","dc6e82d7":"markdown"},"source":{"4e89080c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #Gives us Graphics\nimport seaborn as sns #Libreria para gr\u00e1ficar\nfrom sklearn.model_selection import cross_val_score #libreria para obtener puntuaci\u00f3n de algoritmos\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bdbe253b":"#firts i import the data\ntitanicTrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntitanicTest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","4332f437":"titanicTrain.shape","6f714e97":"titanicTest.shape","6da5ad58":"titanicTrain.head()","375c6335":"titanicTest.head()","a5deab18":"titanicTrain.info()","1cef7148":"titanicTest.info()","b2193174":"titanicTrain.Survived.describe()","86e69e42":"titanicTrain['Survived'].value_counts()","1a65292d":"noSobrevivientes = titanicTrain.Survived[titanicTrain.Survived ==0].count()\nsobrevivientes = titanicTrain.Survived[titanicTrain.Survived ==1].count()\nplt.bar(['No sobrevivientes', 'Sobrevivientes'], [noSobrevivientes, sobrevivientes])\nplt.title('Survived')","3120a9f8":"plt.pie(titanicTrain.Survived.value_counts(),labels=['No sobrevivientes', 'Sobrevivientes'], autopct=\"%0.1f %%\")\nplt.title('Survived')","d59b87ca":"titanicTrain.Sex.head() #ver etiquetas","9b881ff8":"titanicTrain.Survived[(titanicTrain.Survived == 1)&(titanicTrain.Sex.str.contains('female'))].count()","f180c5ce":"plt.pie([233,109], labels=['female','male'],autopct=\"%0.1f %%\" )\nplt.title('Hombres y mujeres sobrevivientes')","23ace47d":"plt.bar(['female','male'], [233, 109], width=0.98)\nplt.title('Sobrevivientes')","74c69a16":"titanicTrain['Age'].describe()","c13565cf":"plt.hist(titanicTrain['Age'])","5ebd5b81":"menores = titanicTrain[titanicTrain.Age < 20] #164 menores de 20 a\u00f1os\nmenores","d5b2e7fc":"menores.Survived[menores.Survived == 1].count() #de 164 personas menores de 20, 79 sobrevivieron","ca71c238":"menores.Survived[menores.Survived == 0].count() #de 164 personas menores de 20 a\u00f1os, 85 no sobrevivieron.","bde02bc0":"mayores = titanicTrain[titanicTrain.Age >= 60] #26 personas tienen una edad igual o mayor a 60 a\u00f1os\nmayores","9b3ea5db":"mayores.Survived[mayores.Survived == 1].count() #7 de las 26 personas mayores sobrevivieron","910b0601":"mayores.Survived[mayores.Survived == 0].count() #19 de las 26 personas mayores no sobreviven","021bce40":"media = titanicTrain[(titanicTrain.Age >= 20) & (titanicTrain.Age <= 30)] #245 personas tienen una edad entre 20 y 30 a\u00f1os\nmedia","fd3e70b4":"media.Survived[media.Survived == 1].count() #87 de las 245 personas de edad media sobreviven","1584600c":"media.Survived[media.Survived == 0].count() #158 de las 245 personas de edad media No sobreviven","1a547402":"titanicTrain['Pclass'].value_counts()","ef7f2b37":"plt.bar(['clase 3', 'clase 1', 'clase 2'],titanicTrain['Pclass'].value_counts())\nplt.title('Distribuci\u00f3n de clases en el barco')","23bc18ef":"#Observaci\u00f3n de primera clase\nclassSobre = titanicTrain.Survived[(titanicTrain.Survived == 1)&(titanicTrain.Pclass == 1)].count()\nclassNoSobre = titanicTrain.Survived[(titanicTrain.Survived == 0)&(titanicTrain.Pclass == 1)].count()\nplt.bar(['No sobrevivientes', 'Sobrevivientes'], [classNoSobre,classSobre])\nplt.title('Distribuci\u00f3n de muerte en primera clase')","abe2145f":"plt.pie([classSobre, classNoSobre], labels=['Sobrevivientes', 'No sobrevivientes'],autopct=\"%0.1f %%\" )\nplt.title('Distribuaci\u00f3n de muerte en primera clase')","b3cf622a":"#Observaci\u00f3n de segunda clase\nclassSobre = titanicTrain.Survived[(titanicTrain.Survived == 1)&(titanicTrain.Pclass == 2)].count()\nclassNoSobre = titanicTrain.Survived[(titanicTrain.Survived == 0)&(titanicTrain.Pclass == 2)].count()\nplt.bar(['No sobrevivientes', 'Sobrevivientes'], [classNoSobre,classSobre])\nplt.title('Distribuaci\u00f3n de muerte en segunda clase')","350ab282":"plt.pie([classSobre, classNoSobre], labels=['Sobrevivientes', 'No sobrevivientes'],autopct=\"%0.1f %%\" )\nplt.title('Distribuaci\u00f3n de muerte en segunda clase')","62de85e8":"#Observaci\u00f3n de tercera clase\nclassSobre = titanicTrain.Survived[(titanicTrain.Survived == 1)&(titanicTrain.Pclass == 3)].count()\nclassNoSobre = titanicTrain.Survived[(titanicTrain.Survived == 0)&(titanicTrain.Pclass == 3)].count()\nplt.bar(['No sobrevivientes', 'Sobrevivientes'], [ classNoSobre, classSobre])\nplt.title('Distribuaci\u00f3n de muerte en tercera clase')","50685096":"plt.pie([classSobre, classNoSobre], labels=['Sobrevivientes', 'No sobrevivientes'],autopct=\"%0.1f %%\" )\nplt.title('Distribuaci\u00f3n de muerte en tercera clase')","b26b5bc3":"sns.pairplot(titanicTrain)","ca409487":"titanicTrain[['Cabin','Ticket','Embarked']]","04203a9c":"#Observemos la columna Cabin\ntitanicTrain['Cabin'].value_counts()","bfa472cd":"titanicTrain['Cabin'].isnull().sum()","0fafa6a3":"titanicTrain.drop('Cabin', axis = 1, inplace=True)\ntitanicTest.drop('Cabin', axis = 1, inplace=True)","7be061cb":"#Observemos la columna Ticket\ntitanicTrain['Ticket'].value_counts()","700420c6":"#Observemos la columna Cabin\ntitanicTrain['Ticket'].isnull().sum()","78f2d5af":"#veamos la columna Embarked\ntitanicTrain['Embarked'].isnull().sum()","eb8b9235":"titanicTrain['Embarked'].value_counts()","e1d200ef":"#Embarked S\nsobrevivientes = titanicTrain.Embarked[(titanicTrain.Survived == 1) & (titanicTrain.Embarked.str.contains('S'))].count()\nnoSobrevivientes = titanicTrain.Embarked[(titanicTrain.Survived == 0) & (titanicTrain.Embarked.str.contains('S'))].count()\nplt.pie([sobrevivientes, noSobrevivientes],labels=['Sobrevivientes', 'No Sobrevivientes'],autopct=\"%0.1f %%\")\nplt.title('Mortalidad de la embarcaci\u00f3n S')","62fcc50e":"#Embarked C\nsobrevivientes = titanicTrain.Embarked[(titanicTrain.Survived == 1) & (titanicTrain.Embarked.str.contains('C'))].count()\nnoSobrevivientes = titanicTrain.Embarked[(titanicTrain.Survived == 0) & (titanicTrain.Embarked.str.contains('C'))].count()\nplt.pie([sobrevivientes, noSobrevivientes],labels=['Sobrevivientes', 'No Sobrevivientes'],autopct=\"%0.1f %%\")\nplt.title('Mortalidad de la embarcaci\u00f3n C')","6564c662":"#Embarked Q\nsobrevivientes = titanicTrain.Embarked[(titanicTrain.Survived == 1) & (titanicTrain.Embarked.str.contains('Q'))].count()\nnoSobrevivientes = titanicTrain.Embarked[(titanicTrain.Survived == 0) & (titanicTrain.Embarked.str.contains('Q'))].count()\nplt.pie([sobrevivientes, noSobrevivientes],labels=['Sobrevivientes', 'No Sobrevivientes'],autopct=\"%0.1f %%\")\nplt.title('Mortalidad de la embarcaci\u00f3n Q')","095a8508":"titanicTrain.drop(['PassengerId','Name', \"Ticket\"], axis = 1, inplace = True)\ntitanicTest.drop(['PassengerId','Name', \"Ticket\"], axis = 1, inplace = True)","958a5a9b":"print(titanicTrain)\nprint(titanicTest)","7952b078":"#conjunto de test\ntitanicTrain['Sex'].replace(['male','female'],[0,1], inplace = True)\ntitanicTrain['Embarked'].replace(['S','C','Q'],[1,2,3], inplace = True)\n\n#conjunto de test\ntitanicTest['Sex'].replace(['male','female'],[0,1], inplace = True)\ntitanicTest['Embarked'].replace(['S','C','Q'],[1,2,3], inplace = True)","2396a075":"titanicTrain","02e94181":"titanicTest","17db74a7":"corr = titanicTrain.corr()\nsns.heatmap(corr, cmap='RdBu', annot=True, fmt=\".2f\")","13bf4477":"corr[['Survived']].sort_values(by = 'Survived',ascending = False)","8e4ddcb1":"titanicTrain = titanicTrain.loc[:,['Sex','Pclass','Age','Embarked','Survived']]\ntitanicTest = titanicTest.loc[:,['Sex','Pclass','Age','Embarked']]\ntitanicTrain, titanicTest","c4198989":"print(titanicTrain['Sex'].isnull().sum()) #Cantidad de valores vac\u00edos en entrenamiento\nprint(titanicTest['Sex'].isnull().sum())#Cantidad de valores vac\u00edos en test","6391d58f":"print(titanicTrain['Pclass'].isnull().sum()) #Cantidad de valores vac\u00edos en entrenamiento\nprint(titanicTest['Pclass'].isnull().sum())#Cantidad de valores vac\u00edos en test","e773ae06":"print(titanicTrain['Age'].isnull().sum()) #Cantidad de valores vac\u00edos en entrenamiento \nprint(titanicTest['Age'].isnull().sum()) #Cantidad de valores vac\u00edos en test","e0a0175d":"print(titanicTrain['Embarked'].isnull().sum()) #Cantidad de valores vac\u00edos en entrenamiento \nprint(titanicTest['Embarked'].isnull().sum()) #Cantidad de valores vac\u00edos en test","e62e0c3b":"print(titanicTrain['Survived'].isnull().sum()) #Cantidad de valores vac\u00edos en entrenamiento","58009736":"promedio = titanicTrain['Age'].mean()\ntitanicTrain['Age'].fillna(promedio, inplace = True)\nprint(titanicTrain['Age'].isnull().sum())\n\n#Ahora con el conjunto de prueba\npromedio = titanicTest['Age'].mean()\ntitanicTest['Age'].fillna(promedio, inplace = True)\nprint(titanicTest['Age'].isnull().sum())","4708631a":"titanicTrain['Embarked'].value_counts()","8e22fd59":"titanicTrain['Embarked'].fillna(1, inplace = True)\nprint(titanicTrain['Embarked'].isnull().sum())","f890f009":"print(titanicTrain.info())\nprint(titanicTest.info())","648882fb":"sns.boxplot(titanicTrain['Age'])","83e3896c":"titanicTrain","2b67669f":"titanicTest","b43c8d5f":"y_train = titanicTrain.loc[:, 'Survived']\nx_train = titanicTrain.drop('Survived', axis = 1)\nx_test = titanicTest","69277895":"#Regresi\u00f3n logistica\nfrom sklearn.linear_model import LogisticRegression #Importamos la librer\u00eda del modelo\n\nclasificador = LogisticRegression(random_state = 0)\nclasificador.fit(x_train, y_train) #Entrenamos el modelo con los datos de entrenamiento \n\ny_pred = clasificador.predict(x_test) #Hacemos predicciones sobre el conjunto de test\n\ncv = cross_val_score(estimator = clasificador, X = x_train, y = y_train, cv = 10) # metodo para obtener la precisi\u00f3n (Validaci\u00f3n cruzada)\ncvRegression = cv.mean()\nstdRegression = cv.std()\nprint(\"promedio Validaci\u00f3n cruzada Regresi\u00f3n log\u00edstica: \", cvRegression)\nprint('Varianza de Regresi\u00f3n log\u00edstica: ',stdRegression)","9b0e2d9f":"#KNN o k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier #Importamos la librer\u00eda del modelo\n\nclasificador = KNeighborsClassifier()\nclasificador.fit(x_train, y_train) #Entrenamos el modelo con los datos de entrenamiento \n\ny_pred = clasificador.predict(x_test) #Hacemos predicciones sobre el conjunto de test\n\ncv = cross_val_score(estimator = clasificador, X = x_train, y = y_train, cv = 10) #metodo para obtener la precisi\u00f3n\ncvKnn = cv.mean()\nstdKnn = cv.std()\nprint(\"promedio Validaci\u00f3n cruzada KNN: \", cvKnn)\nprint('Varianza de KNN: ',stdKnn)","e652ad14":"#SVM o Support Vector Machine\nfrom sklearn.svm import SVC #Importamos la librer\u00eda del modelo\n#Para implementar el modelo de SVM es obligatorio tener los datos escalados\nfrom sklearn.preprocessing import StandardScaler #Libreria para escalar los datos\n\nscaler = StandardScaler()\nx_entrenamiento = scaler.fit_transform(x_train) #Escalar los datos de entrenamiento\nx_prueba = scaler.fit_transform(x_test)\n\nclasificador = SVC(random_state = 0) \nclasificador.fit(x_entrenamiento, y_train) #Entrenamos el modelo con los datos de entrenamiento \n\ny_pred = clasificador.predict(x_prueba) #Hacemos predicciones sobre el conjunto de test\n\ncv = cross_val_score(estimator = clasificador, X = x_train, y = y_train, cv = 10) #metodo para obtener la precisi\u00f3n\ncvSVM = cv.mean()\nstdSVM = cv.std()\nprint(\"promedio Validaci\u00f3n cruzada SVM: \", cvSVM)\nprint('Varianza de SVM: ',stdSVM)","63b24205":"#Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB #Importamos la librer\u00eda del modelo\n\nclasificador = GaussianNB()\nclasificador.fit(x_train, y_train) #Entrenamos el modelo con los datos de entrenamiento \n\ny_pred = clasificador.predict(x_test) #Hacemos predicciones sobre el conjunto de test\n\ncv = cross_val_score(estimator = clasificador, X = x_train, y = y_train, cv = 10) #metodo para obtener la precisi\u00f3n\ncvNB = cv.mean()\nstdNB = cv.std()\nprint(\"promedio Validaci\u00f3n cruzada NB: \", cvNB)\nprint('Varianza de NB: ',stdNB)","9086be28":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier #Importamos la librer\u00eda del modelo\n\nclasificadorTree = DecisionTreeClassifier(random_state = 0)\nclasificadorTree.fit(x_train, y_train) #Entrenamos el modelo con los datos de entrenamiento \n\ny_pred = clasificadorTree.predict(x_test) #Hacemos predicciones sobre el conjunto de test\n\ncv = cross_val_score(estimator = clasificadorTree, X = x_train, y = y_train, cv = 10) #metodo para obtener la precisi\u00f3n\ncvTree = cv.mean()\nstdTree = cv.std()\nprint(\"promedio Validaci\u00f3n cruzada Decision Tree: \", cvTree)\nprint('Varianza de Decision Tree: ',stdTree)","670221b4":"#Random Forest \nfrom sklearn.ensemble import RandomForestClassifier #Importamos la librer\u00eda del modelo\n\nclasificadorForest = RandomForestClassifier(random_state = 0)\nclasificadorForest.fit(x_train, y_train) #Entrenamos el modelo con los datos de entrenamiento \n\ny_pred = clasificadorForest.predict(x_test) #Hacemos predicciones sobre el conjunto de test\n\ncv = cross_val_score(estimator = clasificadorForest, X = x_train, y = y_train, cv = 10) #metodo para obtener la precisi\u00f3n\ncvForest = cv.mean()\nstdForest = cv.std()\nprint(\"promedio Validaci\u00f3n cruzada Random Forest: \", cvForest)\nprint('Varianza de Random Forest: ',stdForest)","6ca51c21":"#score = [scoreRegression, scoreKnn, scoreSVM, scoreNB, scoreTree, scoreForest]\nmeanValidaCruz = [cvRegression, cvKnn, cvSVM, cvNB, cvTree, cvForest]\nstdValidaCruz = [stdRegression, stdKnn, stdSVM, stdNB, stdTree, stdForest]\nlabels = ['Regresion lineal', 'Knn', 'SVM', 'Naive Bayes', '\u00c1rbol de decision', 'Random Forest']\n\nscoreModels = pd.DataFrame({'Models': labels,\n                            'Vali. Cruz.': meanValidaCruz,\n                            'Varianza': stdValidaCruz})\nscoreModels.sort_values(by = 'Vali. Cruz.', ascending = False)","949307df":"width = 0.30\nind =np.arange(6)\nplt.bar(ind ,meanValidaCruz, width, label='VC')\nplt.bar(ind + width,stdValidaCruz, width, label='Varianza')\n#plt.bar(ind + width*2,score, width, color='r', label='Score')\nplt.xticks(ind + width\/2,('Regression', 'Knn', 'SVM', 'NB', 'Tree','Forest'))\nplt.legend()\nplt.title(\"Puntuaciones\")","7572a157":"#GirdSearchCV con el modelo de \u00e1rbol de decisi\u00f3n\n\nfrom sklearn.model_selection import GridSearchCV #Importamos la librer\u00eda\n\nminSplit = np.arange(20,26,1)\nmaxDepth = np.arange(4.0,4.6,0.1)\n\nparametros = [{'criterion': ['gini'],'max_depth': [3.5,4.1], 'min_samples_split': minSplit},\n              {'criterion': ['entropy'], 'max_depth':  [3.5,4.1], 'min_samples_split':  minSplit}\n             ] #Definimos los parametros para que la librer\u00eda nos indique cu\u00e1l es el mejor\n\ngridSearch = GridSearchCV(estimator = clasificadorTree, \n                          param_grid = parametros,\n                          scoring = 'accuracy',\n                          cv = 10) #En el constructor pasamos el modelo que queremos mejorar, los parametros, medida de calificacion y nuemero de pruebas.\n\ngridSearch = gridSearch.fit(x_train, y_train) #Entrenamos el GridSearchCV\nprint(gridSearch.best_score_) #vemos como la puntuaci\u00f3n cv pas\u00f3 de 0.7924 a  0.8215\nprint(gridSearch.best_params_) #Muestras los parametros m\u00e1s optimos\n\n\n#Aplicacion de parametros\nclasificadorTree = DecisionTreeClassifier(criterion= 'gini', \n                                          max_depth= 4.1,min_samples_split = 21,\n                                          random_state = 0)\n\nclasificadorTree.fit(x_train, y_train) #Entrenamos el modelo con los datos de entrenamiento \n\ny_pred = clasificadorTree.predict(x_test) #Hacemos predicciones sobre el conjunto de test\n\nscoreTreeGS = clasificadorTree.score(x_train, y_train) #Obtenemos la precisi\u00f3n del algoritmo\nprint('Precisi\u00f3n de algoritmo de Decision Tree: ',scoreTreeGS)\n\ncv = cross_val_score(estimator = clasificadorTree, X = x_train, y = y_train, cv = 10) #Metodo para obtener la precisi\u00f3n\ncvTreeGS = cv.mean()\nstdTreeGS = cv.std()\nprint(\"promedio Validaci\u00f3n cruzada Decision Tree: \", cvTreeGS)\nprint('Varianza de Decision Tree: ',stdTreeGS)\n\n","490cf497":"#Cuadro comparativo de resultados\ndefectpVsGSTree = pd.DataFrame({'Parametros. \u00c1rbol de Decisi\u00f3n':['Por Defecto', 'GridSearchCV'],\n                                'Mean Vali. Cruz.':[cvTree,cvTreeGS ],\n                                'Varianza': [stdTree, stdTreeGS]})\n\ndefectpVsGSTree","804eab20":"#GirdSearchCV con el modelo de bosques aleatorios\n\nnEstimators = np.arange(80,86)\nmaxDepth = np.arange(4.0,4.3,0.1)\n\n#best'max_features': ['auto'],\n#best'criterion' :['entropy']\nparametros = { \n    'n_estimators': nEstimators,\n    'max_depth' :  maxDepth,\n}#Definimos los parametros que para que la librer\u00eda nos indique cu\u00e1l es el mejor\n\ngridSearch = GridSearchCV(estimator = clasificadorForest, \n                          param_grid = parametros,\n                          scoring = 'accuracy',\n                          cv = 10)#En el constructor pasamos el modelo que queremos mejorar, los parametros, medida de calificacion y nuemero de pruebas.\n\ngridSearch = gridSearch.fit(x_train, y_train) #Entrenamos el GridSearch\nprint(gridSearch.best_score_) #vemos como la puntuaci\u00f3n de cv pas\u00f3 de 0.8025 a 0.8237 \nprint(gridSearch.best_params_) #Vemos los parametros m\u00e1s optimos\n\n\n#Aplicaci\u00f3n de parametros \nclasificadorForest = RandomForestClassifier(max_features ='auto',\n                                            criterion = 'entropy',\n                                            n_estimators = 84,\n                                            max_depth =  4,\n                                            random_state = 0)\nclasificadorForest.fit(x_train, y_train) #Entrenamos el modelo con los datos de entrenamiento \n\ny_pred = clasificadorForest.predict(x_test) #Hacemos predicciones sobre el conjunto de test\n\nscoreForestGS = clasificadorForest.score(x_train, y_train) #Obtenemos la precisi\u00f3n del algoritmo\nprint('Precisi\u00f3n de algoritmo de Random Forest: ',scoreForestGS)\n\ncv = cross_val_score(estimator = clasificadorForest, X = x_train, y = y_train, cv = 10) #Otro metodo para obtener la precisi\u00f3n\ncvForestGS = cv.mean()\nstdForestGS = cv.std()\nprint(\"promedio Validaci\u00f3n cruzada Random Forest: \", cvForestGS)\nprint('Varianza de Random Forest: ',stdForestGS)","e224e47a":"#Cuadro comparativo de resultados\ndefectpVsGSForest = pd.DataFrame({'Bosques Aleatorios':['Por Defecto', 'GridSearchCV'],\n                                'Mean Vali. Cruz.':[cvForest,cvForestGS],\n                                'Varianza': [stdForest, stdForestGS]})\n\ndefectpVsGSForest","89406cd5":"#Cuadro comparativo de modelos\nmodelos = pd.DataFrame({'Modelo': ['DecisionTree', 'RandomForest'],\n                      'Precision': [cvTreeGS, cvForestGS],\n                       'Varianza': [stdTreeGS, stdForestGS]\n                      })\nmodelos.sort_values(by='Precision',  ascending = False)\n","8a59f9c5":"Por \u00faltimo vamos a observar como se comporta la columna \"Pclass\" que es una de las columnas que hemos supuesto como importante ante la mortandad del accidente.","405ca4ae":"Esta columna tiene sus registros completos, pero no creo que por el ticket de una persona se pueda decidir si alguien sobrevive o no sobrevive. Sin embargo, se podr\u00e1 ver la relaci\u00f3n que tiene esta columna con nuestro Target (Objetivo de predecir supervivencia o no) en un grafico m\u00e1s adelante. ","2286b38e":"Con este grafico podemos observar que la clase 3 es un poco m\u00e1s que el doble que la clase 1. Mientras que la clase 2 es un poco menor en tama\u00f1o que la clase 1.\n\nAhora observemos la mortandad en cada clase.","9ab9831d":"y ac\u00e1 podemos ver los porcentajes ordenados de manera descendente:","2478796a":"De esta manera hemos rellenado los valores vac\u00edos de la columna \"Age\" con el promedio de edad. Ahora nos falta la columna \"Embarked\" el cual tiene 2 registros faltantes de 891. En este caso vamos a rellenar estos dos valores vac\u00edos con el numero que m\u00e1s se repite.","85163339":"# 2. Analizar los datos","f1a6b46e":"Y para finalizar el analisis de la columna \"Age\", observemos como resulta estos datos con las personas de edad media.","e7ec6641":"Con estas gr\u00e1ficas podemos darnos una idea de la mortaldiad correspondiente a cada embarcaci\u00f3n. A simple vista podemos ver que la mortalidad en cada una de ellas se da de manera com\u00fan, por lo tanto no hay indicio de que pertenecer a cierta embarcaci\u00f3n tuviera conseciencia en sobrevivir o no al hundimiento del barco. Pero no podemos descartarla a\u00fan. \n\nEn cuanto a las columnas de \"PassengerId\", \"Name\" y \"Ticket\" las descartaremos ya que estas columnas de tipo Object dificilmente se pueden pasar a tipo numerico, y adem\u00e1s no podr\u00edan aportar nada a nuestros modelos.","43e545e8":"Como podemos observar en las gr\u00e1ficas y como se pod\u00eda tener una idea, la primera clase tuvo una gran cantidad de sobrevivientes (63%) en comparaci\u00f3n con las dem\u00e1s clases. Mientras que en la 3ra clase solo sobrevivi\u00f3 un 24.2% de la tripulaci\u00f3n, teniendo en cuenta que la cantidad de personas pertenecientes a la 3ra clase era m\u00e1s del doble que los de la primera clase.\n\nUna vez habiendo analizado las columnas que cre\u00edmos importantes, vamos a observar las columnas restantes. ","7266b03a":"# 4. Hacer predicciones\n\nEn esta etapa entrenaremos nuestros modelos con la data previamente procesada y haremos nuestras predicciones. Utilizaremos modelos de clasificaci\u00f3n tales como :\n\n* Logistic Regression\n* KNN o k-Nearest Neighbors\n* SVM o Support Vector Machine\n* Naive Bayes classifier\n* Decision Tree\n* Random Forrest\n\nCompararemos y mejoraremos los resultados.","7f610e52":"Una vez hemos visto el comportamiento de cada una de las columnas de nuestros datos; vamos a observar la correlaci\u00f3n de ellas con nuestro target \"Survived\". Pero como esta correlaci\u00f3n solo se puede dar con variables de tipo numerico, debemos transformar las variables \"Sex\" y \"Embarked\" que son categoricas a variables numericas.","e6d6f460":"Ahora que sabemos la precisi\u00f3n de cada modelo debemos mejorar las puntuaciones obtenidas. Como los modelos Random Forest y \u00c1rbol de decisi\u00f3n presentaron los mejores resultados; nos enfocaremos en optimizar dichos modelos. Para esto haremos uso de la librer\u00eda GridSearchCV de Sklearn que nos permite hallar los mejores parametros para implementar nuestros algoritmos. \n\n# 5. Mejorar los resultados","d2a015f4":"Ahora obervemos esto mismo con las personas mayores de 60 a\u00f1os ","e0da2137":"El resultado obtenido nos indica que tan solo sobrevivieron 342 personas de las 891 que iban a bordo.\n\nVeamoslo gr\u00e1ficamente:\n\n(Para eso usaremos la librer\u00eda de matplotlib asignada como plt)","035b96b3":"De 891 registros, la columna Cabin contiene 687 valores nulos, lo que significa que debemos eliminarla por su gran cantidad de datos vacios. ","c2bfe6a6":"Ya sabemos cual fue el porcentaje de sobrevivientes. Ahora veamos cual es la cantidad de mujeres que sobrevivieron","72997329":"Como vemos, la columna \"Embarked\" tiene dos registros faltantes que no representan ning\u00fan problema. Se pueden eliminar estas dos filas o llenarlas con el promedio de la embarcaci\u00f3n. Tambien vemos que existen 3 embarcaciones: S,C,Q y con base a esto nos podemos preguntar \u00bfLa embarcaci\u00f3n represent\u00f3 un factor para la supervivencia de los tripulantes ? \u00bfComo se presenta la mortandad en cada una de las embarcaciones? Vamos a responder estas preguntas.","a822df2e":"# My first classification notebook\nEl objetivo principal de este libro es aplicar mis conocimientos basicos de machine learning en un problema practico presentado en las competencias de Kaggle, esto con el fin de recibir retroalimentaci\u00f3n de la comunidad, aprender de sus aportes y orientar a quienes recien empiezan.\n\nEn este cuaderno se llevar\u00e1 a cabo un proceso de machine learning aplicado a los datos \"Titanic: Machine Learning From Desaster\" con el fin de predecir los sobrevivientes del hundimiento del titanic en 1912. La descripci\u00f3n general de la competencia se encuentra aqu\u00ed: https:\/\/www.kaggle.com\/c\/titanic\n\nPara este proceso de predicci\u00f3n seguir\u00e9 la siguiente estructura:\n1. Definir el problema\n2. Analizar los datos\n3. Preparar los datos\n4. Hacer predicciones\n5. Mejorar los resultados\n\nEsta estructura es bien definida en el siguiente sitio: https:\/\/machinelearningmastery.com\/process-for-working-through-machine-learning-problems\/ \n\nComenzamos.","f6123175":"Con este grafico podemos observar las variables numericas que nos hace falta por explorar. Observamos que la columna \"sibps\" que representa el numero de hermanos o conyugues a bordo; tiene en su mayoria numero bajos, lo que quiere decir que exist\u00edan pocas personas con este tipo de parentesco. \n\nDe igual manera se observa con la columna \"Parch\" que representa el numero de padres e hijos a bordo; tiene en su mayoria numeros bajos. \n\ny por \u00faltimo la columna \"Fare\" que representa la tarifa de pasajero. Esta columna tiene numeros muy bajos, lo que tiene sentido porque la gran mayor\u00eda de la tripulaci\u00f3n eran de la 3ra clase. Quiz\u00e1s esta columna tenga importacia ya que dependiendo la tarifa se puede suponer la clase o estrato de la persona. Pero es algo que veremos en un grafico m\u00e1s adelante.\n\nFalta por observar las caolumnas etiquetadas como Cabin, Embarked y Ticket. Observemos:","2dbefabf":"# 1. Definir el problema\n\nA partir de un conjunto muestral de entrenamiento que enumera a los pasajeros que sobrevivieron al desastre del Titanic y a los que no, se debe crear un modelo que pueda determinar con base a un conjunto de test si dichos pasajeros sobreviven o no.\n\nLa informaci\u00f3n detallada del problema se encuentra en aqu\u00ed: https:\/\/www.kaggle.com\/c\/titanic","2800d69c":"Antes de iniciar la exploraci\u00f3n y procesamiento de datos es necesario importar las librerias que usaremos a lo largo de nuestro proyecto.","3452a8d6":"Con estos resultados hemos finalizado el proceso de predicci\u00f3n de la supervivencia de los tripulantes del titanic. \nEn este punto hemos hecho un an\u00e1lisis, pre-procesamiento de datos, selecci\u00f3n, entrenamiento y optimizaci\u00f3n de modelos para la soluci\u00f3n del problema planteado. \n\nHemos terminado con dos modelos que presentan los mejores resultados en comparaci\u00f3n con los dem\u00e1s escogidos. Estos son: El modelo de RandomForest con una precisi\u00f3n del 82,26% y con una varianza del 3,5%. El otro modelo es el de DecisionTree, que presenta una precisi\u00f3n de 82,21% con una varianza del 3,8%\n\nCon estas conclusiones pueden surgir preguntas como: \n1. \u00bfPueden mejorar estos resultados ?\n2. \u00bfComo podemos mejorar estos resultados ? \n3. \u00bfHemos hecho una correcta selecci\u00f3n de caracteristicas ? \n4. \u00bfHemos hecho un correcto pre-procesamiento de datos ? \n5. \u00bfPodr\u00edamos implementar modelos que se ajusten mejor a este problema ? \n\nSi tiene alguna respuesta a estas preguntas podr\u00eda dejarlas en un comentario, o si tiene dudas o sugerencias podr\u00eda hacermelo saber para crear retroalimentaci\u00f3n y aprender entre nosotros. \n","44d507a7":"Una vez sabiendo cual es el porcentaje de mujeres y hombres sobrevivientes, queremos saber como se comporta la edad ante la columna \"Survived\". \nPara ello primero veamos qu\u00e9 edades tenemos en el conjunto de datos de entrenamiento.","ab206c3b":"Una vez hecho nuestro an\u00e1lisis y seleccionado nuestras caracter\u00edsticas, estamos listos para seguir con nuestra siguiente etapa de preprocesamiento de datos.\n\n# 3. Preparar los datos\n\nEn esta etapa se suele hacer el siguiente preprocesamiento: \n1. Transformar variables categ\u00f3ricas a num\u00e9ricas.\n2. Tratar los datos faltantes.\n3. Tratar los datos at\u00edpicos.\n4. Considerar el escalado de los datos.\n\n\n1. Para nuestro primer punto no debemos hacer ninguna transformaci\u00f3n ya que en el proceso de an\u00e1lisis fuimos tranformando las columnas necesarias para nuestra observaci\u00f3n y ahora todo nuestro conjunto de datos es de tipo num\u00e9rico.\n2. **Tratar los datos faltantes**. En este punto si debemos analizar cada columna para ver la cantidad de datos que faltan en cada una de ellas y como tratarlos.","a91c192a":"Ahora si podemos usar nuestro cuadro de correlaci\u00f3n gracias a la libreria seaborn que hemos implementado al inicio de nuestro c\u00f3digo.","0b1abbfd":"Con esta descripci\u00f3n de la columna de edades podemos sacar conclusiones como que la edad minima es 0 a\u00f1os, la m\u00e1xima es 80 y la edad media es de 29.","d0d69dfc":"Como podemos observar, los coeficientes de correlaci\u00f3n con respecto a nuestro target son demasiado bajos, pero debemos trabajar con ellos. La puntuaci\u00f3n m\u00e1s alta es la de \"Sex\" que corresponde a un 54% de correlaci\u00f3n, por tal motivo necesitaremos esa columna para entrenar nuestros algoritmos.\n\nLa columna \"Fare\" es la puntuaci\u00f3n m\u00e1s alta por debajo de la columna \"Sex\" con un 25% de correlaci\u00f3n. Pero si nos situamos en la intersecci\u00f3n entre la columna \"Fare\" y la columna \"Pclass\" podemos ver que entre estas dos variables existe una relaci\u00f3n negativa de un 55%,lo que significa que en esa medida las dos columnas representan lo mismo y una de las dos es innecesaria. Por tal motivo vamos a eliminar la columna \"Fare\" y nos quedaremos con la columna \"Plcass\" ya que tiene una mayor relaci\u00f3n con nuestra columna \"Survived\". *(Las relaciones son independiente del signo, por tal motivo puede existir un porcentaje de correlaci\u00f3n con ambos signos y representar el mismo nivel de relaci\u00f3n)*\n\nSeguido de esto usaremos la columna \"Embarked\" que tiene un 10% de relaci\u00f3n.  \n\nPor \u00faltimo eliminaremos las columnas \"Parch\" y \"SipSp\" ya que haciendo un an\u00e1lisis mental (sin soporte estadistico) considero que tener lazos familiares no aporta en gran medida a la supervivencia de una persona. Sin embargo, NO eliminaremos la columna de \"Age\" a pesar de tener una correlaci\u00f3n tan baja, ya que se debi\u00f3 tener mayor prioridad en salvar a ni\u00f1os que a personas adultas.\n\nDe tal manera nuestros conjuntos de datos  quedan de la siguiente manera: ","f8ed4ce8":"Con este grafico podemos ver que el 68.1% de los sobrevivientes eran mujeres y el 31,9% restante eran hombres.\n\nLo mismo con el diagrama de barras:","28f7719e":"Esto nos arroja un resultado de 233, lo que significa que de los 342 sobrevivientes 233 son mujeres y el restante son hombres.\n\nAhora veamoslo gr\u00e1ficamente:","065c0424":"Teniendo en cuenta que las columnas \"Age\" y \"Embarked\" tiene valores vac\u00edos vamos a tratarlos.\n\nLa columna \"Age\" tiene 177 valores faltantes de 891. Existen distintas formas de rellenar estos valore faltantes, pero en esta ocaci\u00f3n los llenaremos con el valor promedio de edades.","0e36a534":"Veamos el comportamiento de la variable a predecir \"Survived\"","74fbd3cb":"Con el gr\u00e1fico podemos interpretar que en el barco viajaba una mayor cantidad de ni\u00f1os que de adultos mayores, que la mayor cantidad de personas abordo estaban entre los 20 y 30 a\u00f1os, Entre otras cosas...\n\nObservemos la cantidad de personas menores de 20 a\u00f1os que sobreviven y las que no:","fc49bc4b":"Esta gr\u00e1fica nos muestra algunos puntos de \u00e1nalisis, el cuartil inferior que es un poco mayor a 20, la media que es 30 y el cuartil superior que est\u00e1 al rededor de 35. los bigotes (lineas verticales de los lados) exponen que todo numero que est\u00e9 dentro de estos limites se consideran normales, mientras que los que est\u00e1n por fuera se consideran datos at\u00edpicos. En este caso se consideran datos at\u00edpicos las edades inferiores a 3 y superiores a 53 aproximadamente. Dependiendo del caso se deber\u00edan eliminar estos datos para que no afecten el rendimiento de nuestros agoritmos, pero en esta ocaci\u00f3n no es necesario, ya que s\u00ed es posible que una persona de 70 u 80 a\u00f1os estuviera en el bote, o un beb\u00e9 de apenas 1 a\u00f1o de edad. Caso contrario ser\u00eda de una persona de 150 o 200 a\u00f1os, o alguien con -3 a\u00f1os, en ese momento si se deber\u00edan eliminar dichos registros ya que es il\u00f3gico que un caso as\u00ed se presente.  \n\nUna vez considerado los datos Outliers podemos seguir con nuestro \u00faltimo paso de escalado de datos.\n\n4. **Considerar el escalado de los datos**. El escalado de los datos se debe realizar cuando existen columnas con diferencias de valores muy altos, como por ejemplo el valor de un apartamento (200000000) y el numero de habitaciones de este (4). El escalado se hace precisamente para reducir esta diferencia y poder dar los mejores resultados con nuestros algoritmos. Como nuestro problema es de clasificaci\u00f3n y no presentamos diferencias en nuestros valores de columnas, entonces no es necesario hacer el escalado de los datos. \n\nCon esto \u00faltimo hemos terminado nuestro proceso de pre-procesamiento y ya tenemos listos nuestros datos para poder entrenar nuestros modelos de la mejor manera. ","505dba88":"Hacemos una inspecci\u00f3n rapida del contenido de cada conjunto de datos.","c38a78e9":"Ahora ya no tenemos valores vac\u00edos en nuestro conjunto de datos, hemos terminado el segundo punto y podemos seguir con nuestro preprocesamiento de datos.\n\n3.  **Tratar los datos at\u00edpicos.**\nPara este punto solo debemos centrarnos en la columna \"Age\" ya que las dem\u00e1s columnas son categ\u00f3ricas y no tienen mayor variedad de n\u00fameros, de esta manera no existe riesgo de valores Outliers.","5cd729f6":"Obtenemos nuestro conjunto de entrenamiento que ser\u00e1 almacenado en la variable \"titanicTrain\", al mimsmo tiempo que obtenemos nuestro conjunto de prueba que ser\u00e1 almacenado en la variable \"titanicTest\"","dc6e82d7":"# Observaciones\nObservando ambos conjuntos de datos podemos apreciar que el conjunto de entrenamiento cuenta con 11 columnas, mientras el de test cuenta con solo 10. Esto es porque el conjunto de test no contiene la columna \"Survived\" que es la que indica si el tripulante sobrevive o no, y es con base a este conjunto de datos que vamos a evaluar nuestros modelos.\n\nTambi\u00e9n podemos hacer una suposici\u00f3n l\u00f3gica de la importancia de las columnas (Basandonos en la gravedad del suceso). Como por ejemplo que el sexo y la edad son variables de gran importancia ya que daban prioridad a mujeres y ni\u00f1os. \n\nTambi\u00e9n podemos suponer que la variable de clase \"Pclass\" debe ser importante, ya que en el barco iban personas de status social bastante alto. \n\nEl nombre no debe ser de importancia para nuestros modelos.\n\nY la columna \"Cabin\" debe ser eliminada ya que cuenta con demasiados valroes Null.\n\n**Estas suposiciones ser\u00e1n verificadas m\u00e1s adelante**"}}