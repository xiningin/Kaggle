{"cell_type":{"e0d848bf":"code","74f1d2b6":"code","ee2c3956":"code","ead4cd2b":"code","6a9a3fe6":"code","0865b647":"code","e1478027":"code","57c97620":"code","bf22dc7f":"code","c2267a74":"code","197c615f":"code","1c27875c":"code","7b239710":"code","8a5bd7b8":"code","9a9ece3e":"markdown","9acd387f":"markdown","09c4f25d":"markdown","f29dbfc8":"markdown","d340ba91":"markdown","57f4a9bb":"markdown","a7b8309c":"markdown","197eaf14":"markdown","2468e2cb":"markdown","eb42cb6e":"markdown","9e95936c":"markdown","2553a804":"markdown","d1d02b1c":"markdown","fbd59c9f":"markdown","11f7ce54":"markdown","5626aa7a":"markdown"},"source":{"e0d848bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","74f1d2b6":"data = pd.read_csv(\"..\/input\/voice.csv\")","ee2c3956":"data.label = [1 if each == \"male\" else 0 for each in data.label]","ead4cd2b":"data.head(3)","6a9a3fe6":"data.tail(3)","0865b647":"x_data = data.drop([\"label\"],axis=1)\ny = data.label.values","e1478027":"x = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","57c97620":"f,ax = plt.subplots(figsize = (20,20))\nplt.title(\"Heatmap of Human Voice\", fontsize=20)\nsns.set(font_scale=1.1)\nsns.heatmap(x_data.corr(), linewidth = 5, linecolor = \"white\", annot = True, ax = ax)\nplt.yticks(rotation='horizontal')\nplt.show()","bf22dc7f":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)","c2267a74":"x_train=x_train.T\ny_train=y_train.T\nx_test=x_test.T\ny_test=y_test.T","197c615f":"def initialize_weights_and_bias(dimension):                                     # Initial values of Weights and Bias\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\ndef sigmoid(z):                                                                 # Sigmoid Functions\n    y_head = 1\/(1+ np.exp(-z))\n    return y_head\n\ndef forward_backward_propagation(w,b,x_train,y_train):                         # Forward Backward Propagation\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]                                      # x_train.shape[1]  is for scaling\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                   # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    return cost,gradients\n\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):          # Update parameters\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))       \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.subplots(figsize = (20,20))\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n\ndef predict(w,b,x_test):                                                       # Value prediction\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    return Y_prediction\n\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):  # Logistic Regression Function\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    # Print test Errors\n    print(\"Functions that we write Test Accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n  ","1c27875c":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 500)    \n","7b239710":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"Sklearn Test Accuracy {} %\".format(lr.score(x_test.T,y_test.T)))","8a5bd7b8":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='lbfgs') # Fix this error\nlr.fit(x_train.T,y_train.T)\nprint(\"Sklearn Test Accuracy {} %\".format(lr.score(x_test.T,y_test.T)))","9a9ece3e":"* **Sklearn Function**","9acd387f":"* **Data Information**","09c4f25d":"* **Normalize the Data**","f29dbfc8":"> **3. Train and Test Data**","d340ba91":"* **Functions that we write**","57f4a9bb":"* **Adjustment the Data**","a7b8309c":"There is a direct correlation between certain values","197eaf14":"> **4. Functions**","2468e2cb":"> **5. Conclusion**","eb42cb6e":"> **2. Visualize the Data**","9e95936c":"> **1. Data Adjustment**","2553a804":"* **Reading the Data**","d1d02b1c":"* meanfreq: mean frequency (in kHz)\n* sd: standard deviation of frequency\n* median: median frequency (in kHz)\n* Q25: first quantile (in kHz)\n* Q75: third quantile (in kHz)\n* IQR: interquantile range (in kHz)\n* skew: skewness (see note in specprop description)\n* kurt: kurtosis (see note in specprop description)\n* sp.ent: spectral entropy\n* sfm: spectral flatness\n* mode: mode frequency\n* centroid: frequency centroid (see specprop)\n* peakf: peak frequency (frequency with highest energy)\n* meanfun: average of fundamental frequency measured across acoustic signal\n* minfun: minimum fundamental frequency measured across acoustic signal\n* maxfun: maximum fundamental frequency measured across acoustic signal\n* meandom: average of dominant frequency measured across acoustic signal\n* mindom: minimum of dominant frequency measured across acoustic signal\n* maxdom: maximum of dominant frequency measured across acoustic signal\n* dfrange: range of dominant frequency measured across acoustic signal\n* modindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\n* label: male or female\n\n*Retrieved from https:\/\/www.kaggle.com\/primaryobjects\/voicegender*\n\n","fbd59c9f":"*To fix this error*","11f7ce54":"They became a very successful Logistic Regression System. Sklearn library was a bit more successful than our code.However, we can close this difference by increasing the number of iterations.\n\nI published my first karnel. Sorry for my mistakes.\n\nIf you enjoy the content. Please don't forget to upvote.","5626aa7a":"**INTRODUCTION**\n1. Data adjustment\n    * Data Information\n    * Reading the Data\n    * Adjustment the Data\n    * Normalize the Data\n2. Visualize the Data\n3. Train and Test Data\n4. Functions\n    * Functions that we write\n    * Sklearn Function\n5. Conclusion\n"}}