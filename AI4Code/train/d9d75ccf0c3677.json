{"cell_type":{"2010d7c1":"code","0599b8b3":"code","eff11406":"code","a00d91af":"code","cae635f7":"code","8ba55556":"code","5fd25ebb":"code","6b99aa97":"code","94006360":"code","368e0ce2":"code","094e9c15":"code","6f9b05d8":"code","a668d029":"code","30b1666a":"code","d3112a70":"code","0e3fdfd5":"code","6662ce26":"code","8210c831":"code","43178d69":"code","66dcc891":"code","0bf43e23":"code","c8fe71fc":"code","b30a23b6":"code","83cc2197":"code","244a581c":"code","3456abb5":"code","0a30d7bb":"code","bdc704f8":"code","b15d517a":"code","e0416c2c":"code","a565c344":"code","5134f9a9":"code","b3d3ce3f":"code","80260c0b":"code","30d25c8e":"code","1a645d52":"code","6be06a8b":"code","67ba5594":"code","e8c46c89":"code","4ae02aa9":"code","cec18ec1":"code","0b7d09e9":"code","d0c6fbc9":"code","12ea9e3a":"code","3565cbdc":"code","2bd56175":"code","fa33717a":"code","bc67e5de":"code","0c90a586":"code","280268f8":"code","0e25bc06":"code","23f12797":"code","3ec804a2":"code","65279ae6":"code","bc720ebe":"code","7ce2cf27":"code","52ed7187":"code","258a5021":"code","ec403375":"code","99174684":"code","1bac57c2":"code","68a3d6cc":"code","05db8add":"code","5713398f":"code","39257fe5":"code","5d672261":"code","31676fcd":"code","7954311f":"code","d5de3f4d":"code","00acd6b4":"code","79ee7f02":"code","17315ea9":"code","1745ae9a":"code","415ba7de":"code","6770b9c8":"code","6aab1a37":"code","e60d2311":"code","973edd8a":"code","8f348dcd":"code","e5768ca2":"code","c1bbb445":"code","21f61c1b":"code","72dc02c3":"code","c4beecaf":"code","207840ab":"markdown","bc619d20":"markdown","47ac1b47":"markdown","d69d672b":"markdown","7fd9ef1f":"markdown","64ed85f4":"markdown","b2dd815c":"markdown","89436c6c":"markdown","acfc9a8a":"markdown","4a8bf9f3":"markdown","6c6341c0":"markdown","b4794c39":"markdown","c7ab8ad9":"markdown","03f33bb1":"markdown","472b15e0":"markdown","a73214c0":"markdown","e1713cc8":"markdown","a83c5c6e":"markdown","158f3cc2":"markdown","0e544a46":"markdown","758d5d9c":"markdown","b3cad670":"markdown","ed202415":"markdown","80522700":"markdown","349892ae":"markdown","1af5e653":"markdown","9f03337d":"markdown","de038eda":"markdown","fe355760":"markdown","a9bf7073":"markdown","f31f1291":"markdown","e8c97842":"markdown","a55f7aee":"markdown","b6b3abd1":"markdown","d201fa9a":"markdown","62c62b08":"markdown","d906ca73":"markdown","c06ffd36":"markdown","0ba3fa74":"markdown","597ccb78":"markdown","279f4a12":"markdown","a5900fd3":"markdown","be2b46e2":"markdown","b28b5b26":"markdown","b38ffd19":"markdown","783576c5":"markdown","019e7a49":"markdown","a6562d33":"markdown","ab89c374":"markdown","f513357c":"markdown","2adad3a2":"markdown","400528f6":"markdown","c060ab51":"markdown","69d5fbf3":"markdown","409034ec":"markdown","e382781c":"markdown","d43d6719":"markdown","3daab64b":"markdown","a58fed95":"markdown","fd3d6407":"markdown","1cbea4d5":"markdown","870bf464":"markdown","dd7ba51d":"markdown","6f4ba15e":"markdown","41d76a5f":"markdown","53e59f6c":"markdown","92774e19":"markdown","957e41e0":"markdown","8e0cc13f":"markdown","c74a81ad":"markdown"},"source":{"2010d7c1":"import numpy as np \nimport pandas as pd\n\n# Loading the multiple choices dataset, we will not look to the free form data on this study\nmc = pd.read_csv('..\/input\/kaggle-survey-2018\/multipleChoiceResponses.csv', low_memory=False)\n\n# Separating questions from answers\n# This Series stores all questions\nmcQ = mc.iloc[0,:]\n# This DataFrame stores all answers\nmcA = mc.iloc[1:,:]","0599b8b3":"# removing everyone that took less than 4 minutes or more than 600 minutes to answer the survey\nless3 = mcA[round(mcA.iloc[:,0].astype(int) \/ 60) <= 4].index\nmcA = mcA.drop(less3, axis=0)\nmore300 = mcA[round(mcA.iloc[:,0].astype(int) \/ 60) >= 600].index\nmcA = mcA.drop(more300, axis=0)\n\n# removing gender trolls, because we noticed from other kernels thata there are some ouliers here\ngender_trolls = mcA[(mcA.Q1 == 'Prefer to self-describe') | (mcA.Q1 == 'Prefer not to say')].index\nmcA = mcA.drop(list(gender_trolls), axis=0)\n\n# removing student trolls, because a student won't make more than 250k a year.\nstudent_trolls = mcA[((mcA.Q6 == 'Student') & (mcA.Q9 > '500,000+')) | \\\n                     ((mcA.Q6 == 'Student') & (mcA.Q9 > '400-500,000')) | \\\n                     ((mcA.Q6 == 'Student') & (mcA.Q9 > '300-400,000')) | \\\n                     ((mcA.Q6 == 'Student') & (mcA.Q9 > '250-300,000'))].index\nmcA = mcA.drop(list(student_trolls), axis=0)\n\n# dropping all NaN and I do not wish to disclose my approximate yearly compensation, because we are only interested in respondents that revealed their earnings\nmcA = mcA[~mcA.Q9.isnull()].copy()\nnot_disclosed = mcA[mcA.Q9 == 'I do not wish to disclose my approximate yearly compensation'].index\nmcA = mcA.drop(list(not_disclosed), axis=0)","eff11406":"# Creating a table with personal data\npersonal_data = mcA.iloc[:,:13].copy()\n\n# renaming columns\ncols = ['survey_duration', 'gender', 'gender_text', 'age', 'country', 'education_level', 'undergrad_major', 'role', 'role_text',\n        'employer_industry', 'employer_industry_text', 'years_experience', 'yearly_compensation']\npersonal_data.columns = cols\n\n# Drop text and survey_duration columns \npersonal_data.drop(['survey_duration', 'gender_text', 'role_text', 'employer_industry_text'], axis=1, inplace=True)\n\npersonal_data.head(3)","a00d91af":"from pandas.api.types import CategoricalDtype\n\n# transforming compensation into category type and ordening the values\ncateg = ['0-10,000', '10-20,000', '20-30,000', '30-40,000', '40-50,000',\n         '50-60,000', '60-70,000', '70-80,000', '80-90,000', '90-100,000',\n         '100-125,000', '125-150,000', '150-200,000', '200-250,000', '250-300,000',\n         '300-400,000', '400-500,000', '500,000+']\ncat_type = CategoricalDtype(categories=categ, ordered=True)\npersonal_data.yearly_compensation = personal_data.yearly_compensation.astype(cat_type)\n# Doing this we are transforming the category \"I do not wish to disclose my approximate yearly compensation\" into NaN\n\n# transforming age into category type and sorting the values\ncateg = ['18-21', '22-24', '25-29', '30-34', '35-39', '40-44', \n         '45-49', '50-54', '55-59', '60-69', '70-79', '80+']\ncat_type = CategoricalDtype(categories=categ, ordered=True)\npersonal_data.age = personal_data.age.astype(cat_type)\n\n# transforming years of experience into category type and sorting the values\ncateg = ['0-1', '1-2', '2-3', '3-4', '4-5', '5-10',\n         '10-15', '15-20', '20-25', '25-30', '30+']\ncat_type = CategoricalDtype(categories=categ, ordered=True)\npersonal_data.years_experience = personal_data.years_experience.astype(cat_type)\n\n# transforming education level into category type and sorting the values\ncateg = ['No formal education past high school', 'Some college\/university study without earning a bachelor\u2019s degree',\n         'Professional degree', 'Bachelor\u2019s degree', 'Master\u2019s degree', 'Doctoral degree', 'I prefer not to answer']\ncat_type = CategoricalDtype(categories=categ, ordered=True)\npersonal_data.education_level = personal_data.education_level.astype(cat_type)","cae635f7":"personal_data.yearly_compensation.value_counts(dropna=False, sort=False)","8ba55556":"compensation = personal_data.yearly_compensation.str.replace(',', '').str.replace('500000\\+', '500-500000').str.split('-')\npersonal_data['yearly_compensation_numerical'] = compensation.apply(lambda x: (int(x[0]) * 1000 + int(x[1]))\/ 2) \/ 1000 # it is calculated in thousand dollars\nprint('Dataset Shape: ', personal_data.shape)\npersonal_data.head(3)","5fd25ebb":"# Finding the compensation that separates the Top 20% most welll paid from the Bottom 80%\ntop20flag = personal_data.yearly_compensation_numerical.quantile(0.8)\ntop20flag","6b99aa97":"# Creating a flag to identify who belongs to the Top 20%\npersonal_data['top20'] = personal_data.yearly_compensation_numerical > top20flag\n\n# creating data for future mapping of values\ntop20 = personal_data.groupby('yearly_compensation', as_index=False)['top20'].min()","94006360":"# Some helper functions to make our plots cleaner with Plotly\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected=True)\n\n\ndef gen_xaxis(title):\n    \"\"\"\n    Creates the X Axis layout and title\n    \"\"\"\n    xaxis = dict(\n            title=title,\n            titlefont=dict(\n                color='#AAAAAA'\n            ),\n            showgrid=False,\n            color='#AAAAAA',\n            )\n    return xaxis\n\n\ndef gen_yaxis(title):\n    \"\"\"\n    Creates the Y Axis layout and title\n    \"\"\"\n    yaxis=dict(\n            title=title,\n            titlefont=dict(\n                color='#AAAAAA'\n            ),\n            showgrid=False,\n            color='#AAAAAA',\n            )\n    return yaxis\n\n\ndef gen_layout(charttitle, xtitle, ytitle, lmarg, h, annotations=None):  \n    \"\"\"\n    Creates whole layout, with both axis, annotations, size and margin\n    \"\"\"\n    return go.Layout(title=charttitle, \n                     height=h, \n                     width=800,\n                     showlegend=False,\n                     xaxis=gen_xaxis(xtitle), \n                     yaxis=gen_yaxis(ytitle),\n                     annotations = annotations,\n                     margin=dict(l=lmarg),\n                    )\n\n\ndef gen_bars(data, color, orient):\n    \"\"\"\n    Generates the bars for plotting, with their color and orient\n    \"\"\"\n    bars = []\n    for label, label_df in data.groupby(color):\n        if orient == 'h':\n            label_df = label_df.sort_values(by='x', ascending=True)\n        if label == 'a':\n            label = 'lightgray'\n        bars.append(go.Bar(x=label_df.x,\n                           y=label_df.y,\n                           name=label,\n                           marker={'color': label},\n                           orientation = orient\n                          )\n                   )\n    return bars\n\n\ndef gen_annotations(annot):\n    \"\"\"\n    Generates annotations to insert in the chart\n    \"\"\"\n    if annot is None:\n        return []\n    \n    annotations = []\n    # Adding labels\n    for d in annot:\n        annotations.append(dict(xref='paper', x=d['x'], y=d['y'],\n                           xanchor='left', yanchor='bottom',\n                           text= d['text'],\n                           font=dict(size=13,\n                           color=d['color']),\n                           showarrow=False))\n    return annotations\n\n\ndef generate_barplot(text, annot_dict, orient='v', lmarg=120, h=400):\n    \"\"\"\n    Generate the barplot with all data, using previous helper functions\n    \"\"\"\n    layout = gen_layout(text[0], text[1], text[2], lmarg, h, gen_annotations(annot_dict))\n    fig = go.Figure(data=gen_bars(barplot, 'color', orient=orient), layout=layout)\n    return iplot(fig)","368e0ce2":"# Counting the quantity of respondents per compensation\nbarplot = personal_data.yearly_compensation.value_counts(sort=False).to_frame().reset_index()\nbarplot.columns = ['yearly_compensation', 'qty']\n\n# mapping back to get top 20% label\nbarplot = barplot.merge(top20, on='yearly_compensation')\nbarplot.columns = ['x', 'y', 'top20']\n\n# apply color for top 20% and bottom 80%\nbarplot['color'] = barplot.top20.apply(lambda x: 'mediumaquamarine' if x else 'lightgray') \n\n# Create title and annotations\ntitle_text = ['<b>How Much Does Kagglers Get Paid?<\/b>', 'Yearly Compensation (USD)', 'Quantity of Respondents']\nannotations = [{'x': 0.06, 'y': 2200, 'text': '80% of respondents earn up to USD 90k','color': 'gray'},\n              {'x': 0.51, 'y': 1100, 'text': '20% of respondents earn more than USD 90k','color': 'mediumaquamarine'}]\n\n# call function for plotting\ngenerate_barplot(title_text, annotations)","094e9c15":"# creating masks to identify students and not students\nis_student_mask = (personal_data['role'] == 'Student') | (personal_data['employer_industry'] == 'I am a student')\nnot_student_mask = (personal_data['role'] != 'Student') & (personal_data['employer_industry'] != 'I am a student')\n\n# Counting the quantity of respondents per compensation (where is student)\nbarplot = personal_data[is_student_mask].yearly_compensation.value_counts(sort=False).to_frame().reset_index()\nbarplot.columns = ['yearly_compensation', 'qty']\n\n# mapping back to get top 20%\nbarplot.columns = ['x', 'y',]\nbarplot['highlight'] = barplot.x != '0-10,000'\n\n# applying color\nbarplot['color'] = barplot.highlight.apply(lambda x: 'lightgray' if x else 'crimson')\n\n# title and annotations\ntitle_text = ['<b>Do Students Get Paid at All?<\/b><br><i>only students<\/i>', 'Yearly Compensation (USD)', 'Quantity of Respondents']\nannotations = [{'x': 0.06, 'y': 1650, 'text': '75% of students earn up to USD 10k','color': 'crimson'}]\n\n# ploting\ngenerate_barplot(title_text, annotations)","6f9b05d8":"# Finding the compensation that separates the Top 20% most welll paid from the Bottom 80% (without students)\ntop20flag_no_students = personal_data[not_student_mask].yearly_compensation_numerical.quantile(0.8)\ntop20flag_no_students","a668d029":"# Creating a flag for Top 20% when there are no students in the dataset\npersonal_data['top20_no_students'] = personal_data.yearly_compensation_numerical > top20flag_no_students\n\n# creating data for future mapping of values\ntop20 = personal_data[not_student_mask].groupby('yearly_compensation', as_index=False)['top20_no_students'].min()\n\n# Counting the quantity of respondents per compensation (where is not student)\nbarplot = personal_data[not_student_mask].yearly_compensation.value_counts(sort=False).to_frame().reset_index()\nbarplot.columns = ['yearly_compensation', 'qty']\n\n# mapping back to get top 20%\nbarplot = barplot.merge(top20, on='yearly_compensation')\nbarplot.columns = ['x', 'y', 'top20']\nbarplot['color'] = barplot.top20.apply(lambda x: 'mediumaquamarine' if x else 'lightgray')\n\ntitle_text = ['<b>How Much Does Kagglers Get Paid?<\/b><br><i>without students<\/i>', 'Yearly Compensation (USD)', 'Quantity of Respondents']\nannotations = [{'x': 0.06, 'y': 1600, 'text': '80% of earn up to USD 100k','color': 'gray'},\n              {'x': 0.56, 'y': 800, 'text': '20% of earn more than USD 100k','color': 'mediumaquamarine'}]\n\ngenerate_barplot(title_text, annotations)","30b1666a":"# Creating a helper function to generate lineplot\ndef gen_lines(data, colorby):\n    \"\"\"\n    Generate the lineplot with data\n    \"\"\"\n    if colorby == 'top20': \n        colors = {False: 'lightgray',\n                  True: 'mediumaquamarine'}\n    else:\n        colors = {False: 'lightgray',\n                  True: 'deepskyblue'}\n\n    traces = []\n    for label, label_df in data.groupby(colorby):\n        traces.append(go.Scatter(\n                    x=label_df.x,\n                    y=label_df.y,\n                    mode='lines+markers+text',\n                    line={'color': colors[label], 'width':2},\n                    connectgaps=True,\n                    text=label_df.y.round(),\n                    hoverinfo='none',\n                    textposition='top center',\n                    textfont=dict(size=12, color=colors[label]),\n                    marker={'color': colors[label], 'size':8},\n                   )\n                   )\n    return traces","d3112a70":"# Grouping data to get compensation per gender of Top20% and Bottom 80%\nbarplot = personal_data[not_student_mask].groupby(['gender', 'top20_no_students'], as_index=False)['yearly_compensation_numerical'].mean()\nbarplot = barplot[(barplot['gender'] == 'Female') | (barplot['gender'] == 'Male')]\nbarplot.columns = ['x', 'gender', 'y']\n\n# Creates annotations\nannot_dict = [{'x': 0.05, 'y': 180, 'text': 'The top 20% men are almost 12% better paid than the top 20% woman','color': 'deepskyblue'},\n              {'x': 0.05, 'y': 60, 'text': 'At the bottom 80% there is almost no difference in payment','color': 'gray'}]\n\n# Creates layout\nlayout = gen_layout('<b>What is the gender difference in compensation at the top 20%?<\/b><br><i>without students<\/i>', \n                    'Gender', \n                    'Average Yearly Compensation (USD)',\n                    120, \n                    400,\n                    gen_annotations(annot_dict)\n                    )\n# Make plot\nfig = go.Figure(data=gen_lines(barplot, 'gender'), \n                layout=layout)\niplot(fig, filename='color-bar')","0e3fdfd5":"# Calculates compensation per education level\nbarplot = personal_data[not_student_mask].groupby(['education_level'], as_index=False)['yearly_compensation_numerical'].mean()\nbarplot['no_college'] = (barplot.education_level == 'No formal education past high school') | \\\n                        (barplot.education_level == 'Doctoral degree')\n\n# creates a line break for better visualisation\nbarplot.education_level = barplot.education_level.str.replace('study without', 'study <br> without')\n\nbarplot.columns = ['y', 'x', 'no_college']\nbarplot = barplot.sort_values(by='x', ascending=True)\nbarplot['color'] = barplot.no_college.apply(lambda x: 'coral' if x else 'a')\n\n# Add title and annotations\ntitle_text = ['<b>Impact of Formal Education on Compenstaion<\/b><br><i>without students<\/i>', 'Average Yearly Compensation (USD)', 'Level of Education']\nannotations = []\n\ngenerate_barplot(title_text, annotations, orient='h', lmarg=300)","6662ce26":"# Calculates compensation per industry\nbarplot = personal_data[not_student_mask].groupby(['employer_industry'], as_index=False)['yearly_compensation_numerical'].mean()\n\n# Flags the top 5 industries to add color\nbarplot['best_industries'] = (barplot.employer_industry == 'Medical\/Pharmaceutical') | \\\n                             (barplot.employer_industry == 'Insurance\/Risk Assessment') | \\\n                             (barplot.employer_industry == 'Military\/Security\/Defense') | \\\n                             (barplot.employer_industry == 'Hospitality\/Entertainment\/Sports') | \\\n                             (barplot.employer_industry == 'Accounting\/Finance')\n\nbarplot.columns = ['y', 'x', 'best_industries']\nbarplot = barplot.sort_values(by='x', ascending=True)\nbarplot['color'] = barplot.best_industries.apply(lambda x: 'darkgoldenrod' if x else 'a')\n\ntitle_text = ['<b>Average Compensation per Industry | Top 5 in Color<\/b><br><i>without students<\/i>', 'Average Yearly Compensation (USD)', 'Industry']\nannotations = []\n\ngenerate_barplot(title_text, annotations, orient='h', lmarg=300, h=600)","8210c831":"# Calculates compensation per role\nbarplot = personal_data[not_student_mask].groupby(['role'], as_index=False)['yearly_compensation_numerical'].mean()\n\n# Flags the top 5 roles to add color\nbarplot['role_highlight'] = (barplot.role == 'Data Scientist') | \\\n                        (barplot.role == 'Product\/Project Manager') | \\\n                        (barplot.role == 'Consultant') | \\\n                        (barplot.role == 'Data Journalist') | \\\n                        (barplot.role == 'Manager') | \\\n                        (barplot.role == 'Principal Investigator') | \\\n                        (barplot.role == 'Chief Officer')\n\nbarplot.columns = ['y', 'x', 'role_highlight']\nbarplot = barplot.sort_values(by='x', ascending=True)\nbarplot['color'] = barplot.role_highlight.apply(lambda x: 'mediumvioletred' if x else 'lightgray')\n\ntitle_text = ['<b>Average Compensation per Role | Top 7 in Color<\/b><br><i>without students<\/i>', 'Average Yearly Compensation (USD)', 'Job Title']\nannotations = [{'x': 0.6, 'y': 11.5, 'text': 'The first step into the ladder<br>of better compensation is<br>becoming a Data Scientist','color': 'mediumvioletred'}]\n\ngenerate_barplot(title_text, annotations, orient='h', lmarg=300, h=600)","43178d69":"# Replacing long country names\npersonal_data.country = personal_data.country.str.replace('United Kingdom of Great Britain and Northern Ireland', 'United Kingdom')\npersonal_data.country = personal_data.country.str.replace('United States of America', 'United States')\npersonal_data.country = personal_data.country.str.replace('I do not wish to disclose my location', 'Not Disclosed')\npersonal_data.country = personal_data.country.str.replace('Iran, Islamic Republic of...', 'Iran')\npersonal_data.country = personal_data.country.str.replace('Hong Kong \\(S.A.R.\\)', 'Hong Kong')\npersonal_data.country = personal_data.country.str.replace('Viet Nam', 'Vietnam')\npersonal_data.country = personal_data.country.str.replace('Republic of Korea', 'South Korea')\n\n# Calculates compensation per country\nbarplot = personal_data[not_student_mask].groupby(['country'], as_index=False)['yearly_compensation_numerical'].mean()\n\n# Flags the top 10 countries to add color\nbarplot['country_highlight'] = (barplot.country == 'United States') | \\\n                               (barplot.country == 'Switzerland') | \\\n                               (barplot.country == 'Australia') | \\\n                               (barplot.country == 'Israel') | \\\n                               (barplot.country == 'Denmark') | \\\n                               (barplot.country == 'Canada') | \\\n                               (barplot.country == 'Hong Kong') | \\\n                               (barplot.country == 'Norway') | \\\n                               (barplot.country == 'Ireland') | \\\n                               (barplot.country == 'United Kingdom')\n\nbarplot.columns = ['y', 'x', 'country_highlight']\nbarplot = barplot.sort_values(by='x', ascending=True)\nbarplot['color'] = barplot.country_highlight.apply(lambda x: 'mediumseagreen' if x else 'lightgray')\n\ntitle_text = ['<b>Average Compensation per Country - Top 10 in Color<\/b><br><i>without students<\/i>', 'Average Yearly Compensation (USD)', 'Country']\nannotations = []\n\ngenerate_barplot(title_text, annotations, orient='h', lmarg=300, h=1200)","66dcc891":"# Loading the cost of living\ncost_living = pd.read_csv('..\/input\/cost-of-living-per-country\/cost_of_living.csv')\ncost_living.columns = ['ranking', 'country', 'price_index']\ncost_living.head()","0bf43e23":"# joining both tables\npersonal_data = personal_data.merge(cost_living, on='country') # doing an inner join to avoid nans on normalized compensation\n\n# calculating the normalized compensation\npersonal_data['normalized_compensation'] = personal_data.yearly_compensation_numerical \/ personal_data.price_index * 10\npersonal_data['normalized_compensation'] = personal_data['normalized_compensation'].round() * 10","c8fe71fc":"# recreating masks\nis_student_mask = (personal_data['role'] == 'Student') | (personal_data['employer_industry'] == 'I am a student')\nnot_student_mask = (personal_data['role'] != 'Student') & (personal_data['employer_industry'] != 'I am a student')","b30a23b6":"# Calculates compensation per country\nbarplot = personal_data[not_student_mask].groupby(['country'], as_index=False)['normalized_compensation'].mean()\n\n# Flags the top 10 countries to add color\nbarplot['country_highlight'] = (barplot.country == 'United States') | \\\n                               (barplot.country == 'Australia') | \\\n                               (barplot.country == 'Israel') | \\\n                               (barplot.country == 'Switzerland') | \\\n                               (barplot.country == 'Canada') | \\\n                               (barplot.country == 'Tunisia') | \\\n                               (barplot.country == 'Germany') | \\\n                               (barplot.country == 'Denmark') | \\\n                               (barplot.country == 'Colombia') | \\\n                               (barplot.country == 'South Korea')\n\nbarplot.columns = ['y', 'x', 'country_highlight']\nbarplot = barplot.sort_values(by='x', ascending=True)\nbarplot['color'] = barplot.country_highlight.apply(lambda x: 'mediumseagreen' if x else 'lightgray')\n\ntitle_text = ['<b>Normalized Average Compensation per Country - Top 10 in Color<\/b><br><i>without students<\/i>', \n              'Normalized Average Yearly Compensation (USD)', 'Country']\nannotations = []\n\ngenerate_barplot(title_text, annotations, orient='h', lmarg=300, h=1200)","83cc2197":"# Defining the threshold for top 20% most paid\ntop20_tresh = personal_data.normalized_compensation.quantile(0.8)\npersonal_data['top20'] = personal_data.normalized_compensation > top20_tresh\n\n# creating data for future mapping of values\ntop20 = personal_data.groupby('normalized_compensation', as_index=False)['top20'].min()\n\n# Calculates respondents per compensation\nbarplot = personal_data.normalized_compensation.value_counts(sort=False).to_frame().reset_index()\nbarplot.columns = ['normalized_compensation', 'qty']\n\n# mapping back to get top 20% and 50%\nbarplot = barplot.merge(top20, on='normalized_compensation')\nbarplot.columns = ['x', 'y', 'top20']\nbarplot['color'] = barplot.top20.apply(lambda x: 'mediumaquamarine' if x else 'lightgray')\n\ntitle_text = ['<b>How Much Does Kagglers Get Paid?<br><\/b><i>normalized by cost of living<\/i>', 'Normalized Yearly Compensation', 'Quantity of Respondents']\nannotations = [{'x': 0.1, 'y': 1000, 'text': '20% Most well paid','color': 'mediumaquamarine'}]\n\ngenerate_barplot(title_text, annotations)","244a581c":"# First we store all answers in a dict\nanswers = {'Q1': mcA.iloc[:,1],\n           'Q2': mcA.iloc[:,3],\n           'Q3': mcA.iloc[:,4],\n           'Q4': mcA.iloc[:,5],\n           'Q5': mcA.iloc[:,6],\n           'Q6': mcA.iloc[:,7],\n           'Q7': mcA.iloc[:,9],\n           'Q8': mcA.iloc[:,11],\n           'Q9': mcA.iloc[:,12],\n           'Q10': mcA.iloc[:,13],\n           'Q11': mcA.iloc[:,14:21],\n           'Q12': mcA.iloc[:,22],\n           'Q13': mcA.iloc[:,29:44],\n           'Q14': mcA.iloc[:,45:56],\n           'Q15': mcA.iloc[:,57:64],\n           'Q16': mcA.iloc[:,65:83],\n           'Q17': mcA.iloc[:,84],\n           'Q18': mcA.iloc[:,86],\n           'Q19': mcA.iloc[:,88:107],\n           'Q20': mcA.iloc[:,108],\n           'Q21': mcA.iloc[:,110:123],\n           'Q22': mcA.iloc[:,124],\n           'Q23': mcA.iloc[:,126],\n           'Q24': mcA.iloc[:,127],\n           'Q25': mcA.iloc[:,128],\n           'Q26': mcA.iloc[:,129],\n           'Q27': mcA.iloc[:,130:150],\n           'Q28': mcA.iloc[:,151:194],\n           'Q29': mcA.iloc[:,195:223],\n           'Q30': mcA.iloc[:,224:249],\n           'Q31': mcA.iloc[:,250:262],\n           'Q32': mcA.iloc[:,263],\n           'Q33': mcA.iloc[:,265:276],\n           'Q34': mcA.iloc[:, 277:283],\n           'Q35': mcA.iloc[:, 284:290],\n           'Q36': mcA.iloc[:,291:304],\n           'Q37': mcA.iloc[:,305],\n           'Q38': mcA.iloc[:,307:329],\n           'Q39': mcA.iloc[:,330:332],\n           'Q40': mcA.iloc[:,332],\n           'Q41': mcA.iloc[:,333:336],\n           'Q42': mcA.iloc[:,336:341],\n           'Q43': mcA.iloc[:,342],\n           'Q44': mcA.iloc[:,343:348],\n           'Q45': mcA.iloc[:,349:355],\n           'Q46': mcA.iloc[:,355],\n           'Q47': mcA.iloc[:,356:371],\n           'Q48': mcA.iloc[:,372],\n           'Q49': mcA.iloc[:,373:385],\n           'Q50': mcA.iloc[:,386:394]}","3456abb5":"# Then we store all questions in another dict\nquestions = {\n'Q1': 'What is your gender?',\n'Q2': 'What is your age (# years)?',\n'Q3': 'In which country do you currently reside?',\n'Q4': 'What is the highest level of formal education that you have attained or plan to attain within the next 2 years?',\n'Q5': 'Which best describes your undergraduate major?',\n'Q6': 'Select the title most similar to your current role (or most recent title if retired)',\n'Q7': 'In what industry is your current employer\/contract (or your most recent employer if retired)?',\n'Q8': 'How many years of experience do you have in your current role?',\n'Q9': 'What is your current yearly compensation (approximate $USD)?',\n'Q10': 'Does your current employer incorporate machine learning methods into their business?',\n'Q11': 'Select any activities that make up an important part of your role at work',\n'Q12': 'What is the primary tool that you use at work or school to analyze data?',\n'Q13': 'Which of the following integrated development environments (IDEs) have you used at work or school in the last 5 years?',\n'Q14': 'Which of the following hosted notebooks have you used at work or school in the last 5 years?',\n'Q15': 'Which of the following cloud computing services have you used at work or school in the last 5 years?',\n'Q16': 'What programming languages do you use on a regular basis?',\n'Q17': 'What specific programming language do you use most often?',\n'Q18': 'What programming language would you recommend an aspiring data scientist to learn first?',\n'Q19': 'What machine learning frameworks have you used in the past 5 years?',\n'Q20': 'Of the choices that you selected in the previous question, which ML library have you used the most?',\n'Q21': 'What data visualization libraries or tools have you used in the past 5 years?',\n'Q22': 'Of the choices that you selected in the previous question, which specific data visualization library or tool have you used the most?',\n'Q23': 'Approximately what percent of your time at work or school is spent actively coding?',\n'Q24': 'How long have you been writing code to analyze data?',\n'Q25': 'For how many years have you used machine learning methods (at work or in school)?',\n'Q26': 'Do you consider yourself to be a data scientist?',\n'Q27': 'Which of the following cloud computing products have you used at work or school in the last 5 years?',\n'Q28': 'Which of the following machine learning products have you used at work or school in the last 5 years?',\n'Q29': 'Which of the following relational database products have you used at work or school in the last 5 years?',\n'Q30': 'Which of the following big data and analytics products have you used at work or school in the last 5 years?',\n'Q31': 'Which types of data do you currently interact with most often at work or school?',\n'Q32': 'What is the type of data that you currently interact with most often at work or school? ',\n'Q33': 'Where do you find public datasets?',\n'Q34': 'During a typical data science project at work or school, approximately what proportion of your time is devoted to the following?',\n'Q35': 'What percentage of your current machine learning\/data science training falls under each category?',\n'Q36': 'On which online platforms have you begun or completed data science courses?',\n'Q37': 'On which online platform have you spent the most amount of time?',\n'Q38': 'Who\/what are your favorite media sources that report on data science topics?',\n'Q39': 'How do you perceive the quality of online learning platforms and in-person bootcamps as compared to the quality of the education provided by traditional brick and mortar institutions?',\n'Q40': 'Which better demonstrates expertise in data science: academic achievements or independent projects? ',\n'Q41': 'How do you perceive the importance of the following topics?',\n'Q42': 'What metrics do you or your organization use to determine whether or not your models were successful?',\n'Q43': 'Approximately what percent of your data projects involved exploring unfair bias in the dataset and\/or algorithm?',\n'Q44': 'What do you find most difficult about ensuring that your algorithms are fair and unbiased? ',\n'Q45': 'In what circumstances would you explore model insights and interpret your models predictions?',\n'Q46': 'Approximately what percent of your data projects involve exploring model insights?',\n'Q47': 'What methods do you prefer for explaining and\/or interpreting decisions that are made by ML models?',\n'Q48': 'Do you consider ML models to be \"black boxes\" with outputs that are difficult or impossible to explain?',\n'Q49': 'What tools and methods do you use to make your work easy to reproduce?',\n'Q50': 'What barriers prevent you from making your work even easier to reuse and reproduce?',\n'top7_job_title': 'Select the title most similar to your current role (or most recent title if retired)',\n'job_title_student': 'Select the title most similar to your current role (or most recent title if retired)',\n'top10_country': 'In which country do you currently reside?',\n'age': 'What is your age (# years)?',\n'gender-Male': 'What is your gender?',\n'top2_education_level': 'What is the highest level of formal education that you have attained or plan to attain within the next 2 years?',\n'top5_industries': 'In what industry is your current employer\/contract (or your most recent employer if retired)?',\n'industry_student': 'In what industry is your current employer\/contract (or your most recent employer if retired)?',\n'years_experience': 'How many years of experience do you have in your current role?'}","0a30d7bb":"def normalize_labels(full_label):\n    \"\"\"\n    treat labels for new column names\n    \"\"\"\n    try:\n        label = full_label.split('<>')[1] # split and get second item\n    except IndexError:\n        label = full_label.split('<>')[0] # split and get first item\n\n    return label\n\ndef treat_data(data, idx, tresh):\n    \"\"\"\n    Clean and get dumies for columns\n    \"\"\" \n    # get dummies with a distinct separator\n    result = pd.get_dummies(data, prefix_sep='<>', drop_first=False)\n    # gets and normalize dummies names\n    cols = [normalize_labels(str(x)) for x in result.columns]\n    \n    # build columns labels with questions\n    try:\n        Qtext = mcQ['Q{}'.format(idx)]\n    except KeyError:\n        try:\n            Qtext = mcQ['Q{}_Part_1'.format(idx)]\n        except KeyError:\n            Qtext = mcQ['Q{}_MULTIPLE_CHOICE'.format(idx)]\n            \n    # Build new columns names\n    prefix = 'Q{}-'.format(idx)\n    result.columns = [prefix + x for x in cols]\n    \n    # dropping columns that had less than 10% of answers to avoid overfitting\n    percent_answer = result.sum() \/ result.shape[0]\n    for row in percent_answer.iteritems():\n        if row[1] < tresh:\n            result = result.drop(row[0], axis=1)\n        \n    return result","bdc704f8":"# selecting the questions\nselected_questions = [1, 2, 3, 4, 6, 7, 8, 10, 11, 15, 16, 17, 18, 19, 21, 23, 24, 25, 26, 29, 31, 36, 38, 40, 42, 47, 48, 49]\ntreated_data = {}\n\n# Formatting all answers from the selected questions, dropping answers with less than 5%\nfor sq in selected_questions:\n    treated_data['Q{}'.format(sq)] = treat_data(answers['Q{}'.format(sq)], sq, 0.05)   \n# Done! Now we are able to rebuild a much cleaner dataset!\n\n# Define target variable\ncompensation = mcA.Q9.str.replace(',', '').str.replace('500000\\+', '500-500000').str.split('-')\nmcA['yearly_compensation_numerical'] = compensation.apply(lambda x: (int(x[0]) * 1000 + int(x[1]))\/ 2) \/ 1000 # it is calculated in thousand dollars\nclean_dataset = (mcA.yearly_compensation_numerical > 100).reset_index().astype(int)\nclean_dataset.columns = ['index', 'top20']\n\n# Join with treated questions\nfor key, value in treated_data.items():\n    value = value.reset_index(drop=True)\n    clean_dataset = clean_dataset.join(value, how='left')\n\nclean_dataset = clean_dataset.drop('index', axis=1)\n\n# saving back to csv so others may use it\nclean_dataset.to_csv('clean_dataset.csv')\n\nclean_dataset.head()","b15d517a":"shape = clean_dataset.shape\nprint('Our cleaned dataset has {} records and {} features'.format(shape[0], shape[1]))","e0416c2c":"# Create correlation matrix\ncorrel = clean_dataset.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = correl.where(np.triu(np.ones(correl.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.5\nto_drop = [column for column in upper.columns if any(upper[column] > 0.5)]\n\n# Drop features \nclean_dataset_dropped = clean_dataset.drop(to_drop, axis=1)\n\nshape = clean_dataset_dropped.shape\nprint('After dropping highly correlated features, our has {} records and {} features'.format(shape[0], shape[1]))\nprint('Dropped features: ', to_drop)","a565c344":"# Finding NANs\ndf = clean_dataset_dropped.isnull().sum().to_frame()\nprint('We found {} NaNs on the dataset after treatment'.format(df[df[0] > 0].shape[0]))","5134f9a9":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(clean_dataset_dropped, test_size=0.2, random_state=42)\nprint('Train Shape:', train.shape)\nprint('Test Shape:', test.shape)","b3d3ce3f":"# Separating X,y train and test sets\nytrain = train['top20'].copy()\nXtrain = train.drop(['top20'], axis=1).copy() # removing both target variables from features\n\nytest = test['top20'].copy()\nXtest = test.drop(['top20'], axis=1).copy() # removing both target variables from features","80260c0b":"# Helper function to help evaluating the model\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\ndef display_scores(predictor, X, y):\n    \"\"\"\n    Calculates metrics and display it\n    \"\"\"\n    print('\\n### -- ### -- ' + str(type(predictor)).split('.')[-1][:-2] + ' -- ### -- ###')\n    # Getting the predicted values\n    ypred = predictor.predict(X)\n    ypred_score = predictor.predict_proba(X)\n    \n    # calculating metrics\n    accuracy = accuracy_score(y, ypred)\n    roc = roc_auc_score(y, pd.DataFrame(ypred_score)[1])\n    confusion = confusion_matrix(y, ypred)\n    \n    print('Confusion Matrix: ', confusion)\n    print('Accuracy: ', accuracy)\n    print('AUC: ', roc)\n    \n    type1_error = confusion[0][1] \/ confusion[0].sum() # False Positive - model predicted in top 20%, while it wasn't\n    type2_error = confusion[1][0] \/ confusion[1].sum() # False Negative - model predicted out of top 20%, while it was\n    \n    print('Type 1 error: ', type1_error)\n    print('Type 2 error: ', type2_error)","30d25c8e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nrforest = RandomForestClassifier(n_estimators=100, random_state=42)\nlreg = LogisticRegression(solver='liblinear', random_state=42)\n\n# Fit the models\nrforest.fit(Xtrain, ytrain)\nlreg.fit(Xtrain, ytrain)\n\n# Check some metrics\ndisplay_scores(rforest, Xtrain, ytrain)\ndisplay_scores(lreg, Xtrain, ytrain)","1a645d52":"from sklearn.model_selection import cross_val_score\n\ndef do_cv(predictor, X, y, cv):\n    \"\"\"\n    Executes cross validation and display scores\n    \"\"\"\n    print('### -- ### -- ' + str(type(predictor)).split('.')[-1][:-2] + ' -- ### -- ###')\n    cv_score = cross_val_score(predictor, X, y, scoring='roc_auc', cv=5)\n    print ('Mean AUC score after a 5-fold cross validation: ', cv_score.mean())\n    print ('AUC score of each fold: ', cv_score)\n    \ndo_cv(rforest, Xtrain, ytrain, 5)\nprint('\\n ----------------------------- \\n')\ndo_cv(lreg, Xtrain, ytrain, 5)","6be06a8b":"from collections import Counter\nfrom imblearn.under_sampling import RandomUnderSampler\n\nprint('Quantity of samples on each class BEFORE undersampling: ', sorted(Counter(ytrain).items()))\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(Xtrain, ytrain)\nprint('Quantity of samples on each class AFTER undersampling: ', sorted(Counter(y_resampled).items()))","67ba5594":"# refit the model\nrforest.fit(X_resampled, y_resampled)\nlreg.fit(X_resampled, y_resampled)\n\n# do Cross Validation\ndo_cv(rforest, Xtrain, ytrain, 5)\ndisplay_scores(rforest, Xtrain, ytrain)\nprint('\\n ----------------------------- \\n')\ndo_cv(lreg, Xtrain, ytrain, 5)\ndisplay_scores(lreg, Xtrain, ytrain)","e8c46c89":"display_scores(lreg, Xtest, ytest)","4ae02aa9":"# calculating scores\nscores = pd.DataFrame(lreg.predict_proba(Xtest)).iloc[:,1]\nscores = pd.DataFrame([scores.values, ytest.values]).transpose()\nscores.columns = ['score', 'top20']\n\n# Add histogram data\nx0 = scores[scores['top20'] == 0]['score']\nx1 = scores[scores['top20'] == 1]['score']\n\nbottom80 = go.Histogram(\n    x=x0,\n    opacity=0.5,\n    marker={'color': 'lightgray'},\n    name='Bottom 80%'\n\n)\ntop20 = go.Histogram(\n    x=x1,\n    opacity=0.5,\n    marker={'color': 'mediumaquamarine'},\n    name='Top 20%'   \n)\n\nannot_dict = [{'x': 0.2, 'y': 180, 'text': 'The 80% less paid tend<br>to have lower scores','color': 'gray'},\n              {'x': 0.75, 'y': 95, 'text': 'Top 20% tend to have<br>higher scores','color': 'mediumaquamarine'}]\n\nlayout = gen_layout('<b>Distribution of Scores From the Top 20% and Bottom 80%<\/b><br><i>test data<\/i>', \n                    'Probability Score',\n                    'Quantity of Respondents',\n                    annotations=gen_annotations(annot_dict),\n                    lmarg=150, h=400\n                    )\nlayout['barmode'] = 'overlay'\n\ndata = [bottom80, top20]\nlayout = go.Layout(layout)\nfig = go.Figure(data=data, layout=layout)\n\niplot(fig)","cec18ec1":"from sklearn.metrics import roc_curve\n\nyscore = pd.DataFrame(lreg.predict_proba(Xtest)).iloc[:,1]\nfpr, tpr, _ = roc_curve(ytest, yscore)\n\ntrace1 = go.Scatter(x=fpr, y=tpr, \n                    mode='lines', \n                    line=dict(color='mediumaquamarine', width=3),\n                    name='ROC curve'\n                   )\n\ntrace2 = go.Scatter(x=[0, 1], y=[0, 1], \n                    mode='lines', \n                    line=dict(color='lightgray', width=1, dash='dash'),\n                    showlegend=False)\n\nlayout = gen_layout('<b>Receiver Operating Characteristic Curve<\/b><br><i>test data<\/i>', \n                    'False Positive Rate',\n                    'True Positive Rate',\n                    lmarg=50, h=600\n                    )\n\n\nfig = go.Figure(data=[trace1, trace2], layout=layout)\n\niplot(fig)","0b7d09e9":"def calc_proba(model):\n    # calculating scores for the test data\n    scores = pd.DataFrame(model.predict_proba(Xtest)).iloc[:,1]\n    scores = pd.DataFrame([scores.values, ytest.values]).transpose()\n    scores.columns = ['score', 'top20']\n\n    # create 10 evenly spaced bins\n    scores['bin'] = pd.cut(scores.score, [-0.01, 0.05, 0.1, 0.2, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.975, 1])\n\n    # count number of individuals in Top20% and Bottom80% per bin\n    prob = scores.groupby(['bin', 'top20'], as_index=False)['score'].count()\n    prob = pd.pivot_table(prob, values='score', index=['bin'], columns=['top20'])\n\n    # calculates the probability\n    prob['probability'] = prob[1.0] \/ (prob[0.0] + prob[1.0])\n    return prob['probability']\n\n# Calculates the probabilities of belonging to Top20% per range of score based on test data\ncalc_proba(lreg).to_frame()","d0c6fbc9":"print('Our model\\'s intercept is:', lreg.intercept_[0])","12ea9e3a":"# treating the questions just to display better names\nfeatures = pd.DataFrame([Xtrain.columns, lreg.coef_[0]]).transpose()\nfeatures.columns = ['feature', 'coefficient']\nfeatures['abs_coefficient'] = features['coefficient'].abs()\nfeatures['question_number'] = features.feature.str.split('-').str[0]\nfeatures['answer'] = features.feature.str[3:]\nfeatures['answer'] = features.answer.apply(lambda x: x[1:] if x[0] == '-' else x)\n\nfeatures['question'] = features['question_number'].map(questions)\n\n\nanswers_dict = {'age': 'continuous feature',\n                'top10_country': 'live at one of the top 10 countries',\n                'top7_job_title': 'has one of the top 7 job titles',\n               }\n\nfeatures['question'] = features['question_number'].map(questions)\nfeatures = features[['question_number', 'question', 'answer', 'coefficient', 'abs_coefficient']]","3565cbdc":"# Helper functions for building clean plots\ndef gen_yaxis(title):\n    \"\"\"\n    Create y axis\n    \"\"\"\n    yaxis=dict(\n            title=title,\n            titlefont=dict(\n                color='#AAAAAA'\n            ),\n            showgrid=False,\n            color='#AAAAAA',\n            tickfont=dict(\n            size=12,\n            color='#444444'\n        ),\n            )\n    return yaxis\n\n\ndef gen_layout(charttitle, xtitle, ytitle, annotations=None, lmarg=120, h=400):  \n    \"\"\"\n    Create layout\n    \"\"\"\n    return go.Layout(title=charttitle, \n                     height=h, \n                     width=800,\n                     showlegend=False,\n                     xaxis=gen_xaxis(xtitle), \n                     yaxis=gen_yaxis(ytitle),\n                     annotations = annotations,\n                     margin=dict(l=lmarg),\n                    )\n\ndef split_string(string, lenght):\n    \"\"\"\n    Split a string adding a line break at each \"lenght\" words\n    \"\"\"\n    result = ''\n    idx = 1\n    for word in string.split(' '):\n        if idx % lenght == 0:\n            result = result + '<br>' + ''.join(word)\n        else:    \n            result = result + ' ' + ''.join(word)\n        idx += 1\n    return result\n\ndef gen_bars_result(data, color, orient):\n    \"\"\"\n    Create bars\n    \"\"\"\n    bars = []\n    for label, label_df in data.groupby(color):\n        if orient == 'h':\n            label_df = label_df.sort_values(by='x', ascending=True)\n        if label == 'a':\n            label = 'lightgray'\n        bars.append(go.Bar(x=label_df.x,\n                           y=label_df.y,\n                           name=label,\n                           marker={'color': label},\n                           orientation = orient,\n                           text=label_df.x.astype(float).round(3),\n                           hoverinfo='none',\n                           textposition='auto',\n                           textfont=dict(size=12, color= '#444444')\n                          )\n                   )\n    return bars\n\ndef plot_result (qnumber):\n    \"\"\"\n    Plot coefficients for a given question number\n    \"\"\"\n    data = features[features.question_number == qnumber]\n    title = qnumber + '. ' + data.question.values[0]\n    title = split_string(title, 8)\n    barplot = data[['answer', 'coefficient']].copy()\n    barplot.answer = barplot.answer.apply(lambda x: split_string(x, 5))\n    barplot.columns = ['y', 'x']\n    bartplot = barplot.sort_values(by='x', ascending=False)\n    barplot['model_highlight'] = barplot.x > 0\n    barplot['color'] = barplot.model_highlight.apply(lambda x: 'cornflowerblue' if x else 'a')\n\n    layout = gen_layout('<b>{}<\/b>'.format(title), \n                        'Model Coefficient', \n                        '',\n                        lmarg=300,\n                        h= 600)\n\n    fig = go.Figure(data=gen_bars_result(barplot, 'color', orient='h'), \n                    layout=layout)\n    iplot(fig, filename='color-bar')","2bd56175":"plot_result('Q1')","fa33717a":"plot_result('Q2')","bc67e5de":"plot_result('Q3')","0c90a586":"plot_result('Q4')","280268f8":"plot_result('Q6')","0e25bc06":"plot_result('Q7')","23f12797":"plot_result('Q8')","3ec804a2":"plot_result('Q10')","65279ae6":"plot_result('Q11')","bc720ebe":"plot_result('Q15')","7ce2cf27":"plot_result('Q16')","52ed7187":"plot_result('Q17')","258a5021":"plot_result('Q18')","ec403375":"plot_result('Q19')","99174684":"plot_result('Q21')","1bac57c2":"plot_result('Q23')","68a3d6cc":"plot_result('Q24')","05db8add":"plot_result('Q26')","5713398f":"plot_result('Q29')","39257fe5":"plot_result('Q31')","5d672261":"plot_result('Q36')","31676fcd":"plot_result('Q38')","7954311f":"plot_result('Q40')","d5de3f4d":"plot_result('Q42')","00acd6b4":"plot_result('Q47')","79ee7f02":"plot_result('Q48')","17315ea9":"plot_result('Q49')","1745ae9a":"### Training the model again with fewer questions\n# Selecting just the questions we are putting in production\nselected_questions = [1, 2, 3, 4, 6, 7, 8, 10, 11, 15, 16, 23, 31, 42]\ntreated_data = {}\n\n# Let's select answers that had more than 5% of answers\nfor sq in selected_questions:\n    treated_data['Q{}'.format(sq)] = treat_data(answers['Q{}'.format(sq)], sq, 0.05)\n    \n# Done! Now we are able to rebuild a much cleaner dataset!\n\n# Define target variable\ncompensation = mcA.Q9.str.replace(',', '').str.replace('500000\\+', '500-500000').str.split('-')\nmcA['yearly_compensation_numerical'] = compensation.apply(lambda x: (int(x[0]) * 1000 + int(x[1]))\/ 2) \/ 1000 # it is calculated in thousand dollars\nclean_dataset = (mcA.yearly_compensation_numerical > 100).reset_index().astype(int)\nclean_dataset.columns = ['index', 'top20']\n\n# Join wit treated questions\nfor key, value in treated_data.items():\n    value = value.reset_index(drop=True)\n    clean_dataset = clean_dataset.join(value, how='left')\n\nclean_dataset = clean_dataset.drop('index', axis=1)\n# saving back to csv so others may use it\nclean_dataset.to_csv('production_clean_dataset.csv')","415ba7de":"# Create correlation matrix\ncorrel = clean_dataset.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = correl.where(np.triu(np.ones(correl.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.5\nto_drop = [column for column in upper.columns if any(upper[column] > 0.5)]\n\n# Drop features \nclean_dataset_dropped = clean_dataset.drop(to_drop, axis=1)","6770b9c8":"# splitting train and test data\ntrain, test = train_test_split(clean_dataset_dropped, test_size=0.2, random_state=42)\nytrain = train['top20'].copy()\nXtrain = train.drop(['top20'], axis=1).copy() # removing both target variables from features\n\nytest = test['top20'].copy()\nXtest = test.drop(['top20'], axis=1).copy() # removing both target variables from features\n\n# undersampling\nX_resampled, y_resampled = rus.fit_resample(Xtrain, ytrain)","6aab1a37":"# fitting the model\nlreg = LogisticRegression(solver='liblinear', random_state=42)\nlreg.fit(X_resampled, y_resampled)\n\n# validating on test data\ndisplay_scores(lreg, Xtest, ytest)","e60d2311":"# Calculates the probabilities of belonging to Top20% per range of score based on test data\ncalc_proba(lreg).to_frame()","973edd8a":"input_json = {\n    \"Q1\": \"q1_other\",\n    \"Q2\": \"q2_25_29\",\n    \"Q3\": \"q3_united_\",\n    \"Q4\": \"q4_other\",\n    \"Q6\": \"q6_student\",\n    \"Q7\": \"q7_other2\",\n    \"Q8\": \"q8_2_3\",\n    \"Q10\": \"q10_we_rec\",\n    \"q11_analyz\": \"on\",\n    \"q11_run_a_\": \"on\",\n    \"q11_build_\": \"on\",\n    \"q15_amazon\": \"on\",\n    \"other\": \"on\",\n    \"q16_python\": \"on\",\n    \"q16_sql\": \"on\",\n    \"Q23\": \"q23_25_to_\",\n    \"q31_catego\": \"on\",\n    \"q31_geospa\": \"on\",\n    \"q31_numeri\": \"on\",\n    \"q31_tabula\": \"on\",\n    \"q31_text_d\": \"on\",\n    \"q31_time_s\": \"on\",\n    \"q42_revenu\": \"on\"\n}","8f348dcd":"import re\n# treating the questions to match the input json\nfeatures = pd.DataFrame([Xtrain.columns, lreg.coef_[0]]).transpose()\nfeatures.columns = ['feature', 'coefficient']\nfeatures['answer'] = features.feature\nfeatures['answer'] = features['answer'].apply(lambda x: re.sub(r\"[^a-zA-Z0-9]+\", ' ', x))\nfeatures['answer'] = features['answer'].str.replace(' ', '_')\nfeatures['answer'] = features['answer'].str.lower()\nfeatures['answer'] = features['answer'].str.replace('_build_and_or_', '_')\nfeatures['answer'] = features['answer'].str.replace('_metrics_that_consider_', '_')\nfeatures['answer'] = features['answer'].str[:10]\n\nfeatures['question_number'] = features['answer'].str.split('_').str[0]\nfeatures = features[['question_number', 'answer', 'coefficient']]\nfeatures.head(3)","e5768ca2":"# treating the input json to keep it in the same format as the coeficcients\ndef treat_input(input_json):\n    treated = dict()\n    for key, value in input_json.items():\n        if key[0] == 'Q':\n            treated[value] = 1\n        else:\n            treated[key] = 1\n    return treated\n\ntreated_input_json = treat_input(input_json)\nprint('First 8 elements of the treated input:', dict(list(treated_input_json.items())[0:8]))","c1bbb445":"features['positive'] = features['answer'].map(treated_input_json)\nfeatures.fillna(0, inplace=True)\nfeatures['points'] = features.positive * features.coefficient\nfeatures.head(5)","21f61c1b":"from math import exp\n# Creating a function to normalize the scores between 0 and 1000\n\ndef normalize(points):\n    \"\"\"\n    Normalize to get values between 0 and 1000\n    \"\"\"\n    return int(1 \/ (1 + exp(-points)) * 1000)\n\n# suming all points + intercept then normalizing between 0 and 1\nscore = features['points'].sum() + lreg.intercept_[0]\nprint('Calculated score is:', normalize(score))","72dc02c3":"import requests\nimport json\n\ninput_json = {\n    \"Q1\": \"q1_other\",\n    \"Q2\": \"q2_25_29\",\n    \"Q3\": \"q3_united_\",\n    \"Q4\": \"q4_other\",\n    \"Q6\": \"q6_student\",\n    \"Q7\": \"q7_other2\",\n    \"Q8\": \"q8_2_3\",\n    \"Q10\": \"q10_we_rec\",\n    \"q11_analyz\": \"on\",\n    \"q11_run_a_\": \"on\",\n    \"q11_build_\": \"on\",\n    \"q15_amazon\": \"on\",\n    \"other\": \"on\",\n    \"q16_python\": \"on\",\n    \"q16_sql\": \"on\",\n    \"Q23\": \"q23_25_to_\",\n    \"q31_catego\": \"on\",\n    \"q31_geospa\": \"on\",\n    \"q31_numeri\": \"on\",\n    \"q31_tabula\": \"on\",\n    \"q31_text_d\": \"on\",\n    \"q31_time_s\": \"on\",\n    \"q42_revenu\": \"on\"\n}\n\ntreated_input_json = treat_input(input_json)\nheader = {'Content-Type': 'application\/x-www-form-urlencoded'}\n\nurl = 'https:\/\/tk9k0fkvyj.execute-api.us-east-2.amazonaws.com\/default\/top20-predictor'\n\nrequests.post(url, params=treated_input_json, headers=header).json()","c4beecaf":"# Making a get to our API. It triggers a lambda function that counts the number of objects inside our bucket.\nurl = 'https:\/\/wucg3iz2r4.execute-api.us-east-2.amazonaws.com\/default\/count-kaggle-top20-objects'\nrequests.get(url).json()","207840ab":"### How input data will look like\nWe will deploy a web page to collect some answers and send it to our model. We expect it to send us the content by QueryString parameters, after decoding that on AWS API Gateway we have a simple json that looks like this:","bc619d20":"When it comes to gender, being female decreases your chances of earning more. We have already seen that in the EDA, and it'as confirmed by the model we built.\n\n---","47ac1b47":"## Dealing With Missing Data\nAs result of dataset cleaning we are left with no questions with NaNs.","d69d672b":"To get the final score we sum all points, them sum it with the intercept and them normalize to get values between 0 and 1000.","7fd9ef1f":"### Should You Aim at the C-level?\nIt's obvious that a C-level compensation is much higher than an analyst's. But how much? Almost 3x. Also, managerial levels have better compensation. Makes sense, no?","64ed85f4":"First we will train the model again with fewer questions. We don't want ask to much questions and consequently discourage people to answer the survey.","b2dd815c":"## Spliting Into Train and Test Data","89436c6c":"## Implementing the parametrized model as a AWS Lambda Function.\nTo get this model into production, we are creating a Lambda function that receives the input json through an API Gateway Integration, calculates the score and then returns it to the API. You can access the API by doing a simple POST with the input json at the body.\n\n[**All code for Lambda are available at this git.**](https:\/\/github.com\/andresionek91\/kaggle-top20-predictor)\n\nInstructions on how to deploy a model to AWS lambda were found on this [Towards Data Science post, by Ben Weber](https:\/\/towardsdatascience.com\/data-science-for-startups-model-services-2facf2dde81d).\n\n### Below we test the API using requests\nNotice that the usage is limited to 1 request\/second at this moment.\n","acfc9a8a":"We see on the above chart that the two classes are very well defined, and distinct from each other. This is confirmed when we plot the ROC curve.\n\n> In a Receiver Operating Characteristic (ROC) curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points. Each point on the ROC curve represents a sensitivity\/specificity pair corresponding to a particular decision threshold. A test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner (100% sensitivity, 100% specificity). Therefore the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test (Zweig & Campbell, 1993).","4a8bf9f3":"Top 20ies won't spend to much time coding. They are probably involved with strategic decisions, rather than coding.\n\n---","6c6341c0":"--- \n# What If We Remove Students From Our Data?\nWe have seen that students aren't usually remunerated: 76% of them earn up to USD 10k. Because they have very low compensation (and aren't actually working), they are probably biasing the data towards lower compensation. That is why we are removing them from the rest of EDA.","b4794c39":"# Have any question or idea?\nPlease feel free to ask anything and criticize the work. Hope that you find it useful to define your career path.","c7ab8ad9":"### Are there any gender difference between the top 20% most well paid?\nUnfortunatelly we still have differences in payment due to gender. This gets very noticeable when we compare the top 20% most well paid men and women.","03f33bb1":"We see that compensation is much smoother when we divide it by the cost of living. By livinng in most countries around the world, you should get almost the same compensation on average (between USD 30k and 40k per year). A few countries pay above the average (United States pays better than any other country), and other few countries pay below the average. \n\n## Top 20% by considering each country cost of living\nLet's define the top 20% again, now based on the normalized compensation to see it in a chart. ","472b15e0":"Learning and using Caffe, Fastai, SparkMLlib, Lightgbm and Xgboost and will add value to your resume.\n\n---","a73214c0":"Working for a company that has well stablished ML models in production for more than 2 years is the dream for those who want to increase their earnings. Notice that there is a natural order to the levels of ML adoption, those who work for companies that are more experienced in ML tend to have higher compensation.  If your company does not use ML methods at all, then you are in the wrong place. Consider finding a new job.\n\n---","e1713cc8":"This means that everyone starts with 0.913. Then you may add or subtract points from your it, depending of the answers you give to each question.\n\n* **Positive Coefficients:** If the coefficient is positive, means that a positive answer increases the chances of belonging to the Top 20%. \n* **Negative Coefficients:**  If the coefficient is negative, then a positive answer decreases the probability of belonging to the Top 20%.\n\n**Takeaway:** Have an attitude towards positiveness. Don't do negative stuff. =D","a83c5c6e":"## Advanced Data Cleaning\nThe data is really, really messy... We are going to clean it and transform all questions in dummies.  For the purpose of modelling, we are just selecting some questions that we believe might explain higher salaries. All other questions were left here, but were not treated, so you can use this kernel for other purposes.","158f3cc2":"Primary usage of a language other than Python and R will get you closer to Top 20%. But if you have to chose one between them, then choose Python. Knowing R is a plus, but using it most often than other languages will decrease your compensation. Python wins this one probably because it's more versatile than R when it comes to putting models in production.\n\n---","0e544a46":"##  Finding Correlation Between Features\nLet's identify all features that have correlation higher than ```abs(\u00b10.5)``` and drop them from the dataset. ","758d5d9c":"To deploy this model we had basically two options:\n1. Export the model as as serialized object, load into AWS lambda and start making predictions.\n2. [Parametrize](https:\/\/en.wikipedia.org\/wiki\/Parametrization) the model and load all coefficients and intercept manually to Lambda.\n\nI'm choosing the second option because I want the model to be transparent to anyone. It should be clear what the coefficients are and how the score is calculated. A serialized model wouldn't let us see that in detail.\n\n#### Parametrization: What the hell is\u00a0that?\nA Logistic Regression model can be written like this:\n\n```log-odds = b0 + b1x1 + b2x2 + ... + bnxn``` \n\n**Where:**\n* ```b0``` is the intercept\n* ```b1, b2, bn``` are the coefficients\n* ```x1, x2, xn``` are the variables\n\nWe trained our model and got all coefficient's values. So it's just a matter of writing that equation manually. Then to get scores between 0 and 1 we do:\n\n```scores = 1 \/ (1 + exp(-(b0 + b1x1 + b2x2 + ... + bnxn)))```\n\n### Treating all model's coefficients to have a single naming convention","b3cad670":"Let's see how scores for both Top20% and Bottom 80% are distributed in test data?","ed202415":"---\n# Creating an Online Model to Deploy as a Service at AWS Lambda\n\nNext we will create an API that can calculate your score (i.e. your chance of earning more than U$ 100k per year). To do that we are using AWS Lambda and API Gateway. Check out the development below:\n\n![Lambda](https:\/\/i.imgur.com\/pkYNCkm.png)","80522700":"Then we treat the input json to keep it at the same naming convention.","349892ae":"If you reside in the United States, your chance of earning more is increased. By living in China or India, you are probably earning less.\n\n---","1af5e653":"## Probability of Being in the Top 20% per Score\nNow let's calculate the probability of belonging to the Top 20% given a certain score. To do that we will create score ranges. We calculate the probability based on how the model performed on test data. Below we show the probability for each bin.","9f03337d":"## Validating the Model on the Test Data\nNow we finally test the model on the test dataset! We see that the Accuracy is around 81% and AUC is now 89%. It is a quite good prediction model.","de038eda":"Some of the richiest Kagglers are not involved with an organization that builds ML models. But if you are, set the metrics on revenue and business goals.\n\n---","fe355760":"I personally like and use Plotly, so I'm losing some points here. One reconforting thought is that Plotly is built on top of D3.\n\n---","a9bf7073":"Use cloud computing services! Get used to AWS, or other leading cloud providers. Using IBM Cloud might reduce your chances of earning more, but can't see any reasons for that right now.\n\n---","f31f1291":"# Predicting if a Kaggler Earns More than USD 100k (Top 20% - without students)\nAfter doing this initial EDA, we see that there are a ton of different features to explore one by one (there were 50 question on the survey). So we propose creating a model to show wich features makes a Kaggler be in the top 20%. After that we may look at them more carefully and draw some conclusions.","e8c97842":"Again, get experienced before asking for a raise.\n\n---","a55f7aee":"When the subject is belonging to the Top 20%, then doing certified couses at developers.google.com or Online University Courses are probably the most profitable investments. Online courses are probably negatively associated with compensation because most people who are starting in the field are learning at those platforms, we might see a shift here over the next years.\n\n---","b6b3abd1":"Now we can map the input json to the coefficients to calculate the points.","d201fa9a":"We noticed that questions 1 through 9 are all about personal information of data scientists. So we are first focusing on them.","62c62b08":"Listen to the money's voice. People who earn more recommend learning Python as the first language.\n\n---","d906ca73":"A Random Forest model with 100% accuracy and AUC is definetly overfitted. Probably due to imbalanced classes. LogReg has better results, but still might be overfitted.\n\nAfter a 5-fold cross validation, we see that the real accuracy is around 88% for Random Forest and 90% for Logistic Regression.","c06ffd36":"Using AWS RDS will greatly increase your skills. Using Microsoft Access or not using any relational database decreasees your probability of earning more. The takeaway is: learn and use relational databases, and if you can choose, go with RDS.\n\n---","0ba3fa74":"---\n# EDA - Studying the problem: How personal data affects compensation?\n\nNow that we have done a basic data cleaning, we are able to do some in depth EDA and ultimately build a model to predict the earnings of data scientists and find the most important features that affect compensation. Because we had 50 multiple choice questions, many of them with multiple answers, we will do an EDA only to analyse how personal data correlates to compensation. Other features will be used later on this study, on the modeling step.\n\n**Note about splitting the data into train and test sets:** \n1. Ideally we would split the data into train and test sets before doing the Exploratory Data Analysis. This is good practice to avoid cognitive bias and overfitting the data. I'm not doing it here because I have already studied many of this dataset's kernels before start working on the data, so I'm probably already biased. \n2. We also have static data, I mean we are not getting any new answers from this survey. If we do the EDA and impute NaNs with the whole dataset, instead of just the training set, we shouldn't have any problem, because we are working with the full universe of data available. \n\n### Finding the Top 20% most well paid","597ccb78":"Be realistic, you won't be in the Top 20% with just 1 or 2 years of experience.\n\n---","279f4a12":"### Which industry should you target?\nIf you concentrate your efforts on some industry specific problems you'll eventually get hired by them. Bellow we show the top 5 industries, and their average yearly compensation, compared to all others sectors. Choose wisely!","a5900fd3":"Doing the crossvalidation again, we see that the mean AUC Score stays in 88% for Random Forest and 90% for the Logistic Regression model, but we had differences in the confusion matrices and reduction of type 2 error for LogReg. We are selecting Logistic Regression to continue this study because we had better score and can draw more conclusions from the model coeficients. ","be2b46e2":"The top 20% don't think that independent projects are equally important as academic achievements.\n\n---","b28b5b26":"### Which countries pay more?\nDoes living on certain contries impact the average compensation you get? Below we show how much, on average, you should expect to earn in each country.","b38ffd19":"We have already dropped all participants that did not disclose their compensation. Let's see how many answers we have at each compensation, including NaNs (we expect to see none).","783576c5":"We see that countries that have a higher cost of living are showing up at the top, paying more. Let's try to divide the average compensation by the cost of living to normalize this feature? We found a ranking of cost of living per country at [Expatistan.com](https:\/\/www.expatistan.com\/cost-of-living\/country\/ranking) on Nov. 15th 2018. This source also provide a price index for each country, that is calculated as described below: \n\n> To calculate each country's Price Index value, we start by assigning a value of 100 to a central reference country (that happens to be the Czech Republic). Once the reference point has been established, the Price Index value of every other country in the database is calculated by comparing their cost of living to the cost of living in the Czech Republic.\nTherefore, if a country has a Price Index of 134, that means that living there is 34% more expensive than living in the Czech Republic. Source: [Expatistan.com](https:\/\/www.expatistan.com\/cost-of-living\/country\/ranking)","019e7a49":"Defining random seeds, usage of virtual machines and containers and making sure the code is well documented are all good practices that will probably increase your earnings.\n\n---","a6562d33":"## Getting the Number of Requests Made to the API\nOur Lambda function is also saving each input and the resulting score as a json object inside a S3 Bucket. We wrote another lambda function, and an API, to count the number of objects in a bucket.","ab89c374":"## Creating a Flask App\nNext we created a Flask App to run our html and collect answers from a form, send it to lambda and show the result to the user. It was deployed using AWS Beanstalk.\n\n[More detailed instructions on app deployment are available here](https:\/\/medium.com\/p\/2ece8e66d98c\/)\n\n[You can access the working app here.](http:\/\/www.data-scientist-value.com\/)\n\n[Or see the git with code to create the Flask App here.](https:\/\/github.com\/andresionek91\/data-scientist-value)","f513357c":"When it comes to activities, try to build prototypes or Machine Learning services. Exploring the application of ML to new areas and using it to improve your products or workflows is the way to get closer to earning more than USD 100k per year.\u00a0On the other side, if an important part of your role is to do Business Intelligence to analyze and understand data to influence product or business decisions, then you should expect to earn less. Same thing if you run the data infrastructure.\n\n---","2adad3a2":"### Mapping input to coefficients and calculating the score","400528f6":"Using R, PHP, Java or Bash on a regular basis contribute to earning more. \n\nJava is one of the languages that most contributes to the probability of belonging to the Top 20% probably because some Big Data tools, such as Hadoop, are written in Java. Bash, the Unix shell and command language, is also valuable. You see that some of them will increase your salary, while others may decrease it. Choose wisely what to learn next!\n\n---","c060ab51":"It' water or wine. If you earn tons of dollars have to be pretty sure of what you are. Probably those who do not consider themselves to be Data Scientists are in managerial or C-level positions. Being in doubt is probably associated with junior professionals, who are starting their careers.\n\n---","69d5fbf3":"If you want to earn more, a good idea is to do a Doctoral Degree. But don't be to strict on this rule, remember from EDA that *\"no college at all\"* also pays well?\n\n---","409034ec":" Being a student might be a source of frustration and lower salaries. Get out and get a job! Start as a data analyst, then focus on becoming a Data Scientist. But you see that software engineers earn more. \n \n Why not be a [Type B data scientist](https:\/\/medium.com\/@jamesdensmore\/there-are-two-types-of-data-scientists-and-two-types-of-problems-to-solve-a149a0148e64) and deploy models into production? To achieve that you have to develop your software engineering skills as well.\n\n---","e382781c":"# 124 Ways to Increase Your Earnings\nOur model had a total of 124 features. From their coefficients we may draw some ideas that might help you find your pile of money. Let's first look where the intercept is:","d43d6719":"### By following those ideas, maybe one day we might find ourselves laying down on a pile of money. But unlike Mr. Babineaux, it will be our money. And it will be legal.\n![](https:\/\/media.giphy.com\/media\/3oKIPm3BynUpUysTHW\/giphy.gif)\n","3daab64b":"Working with Genetic and Video Data will add more value to your resume. If aren't that exotic, then it's good to know that Geospatial and Time Series Data will also boost your carrer! Everyone works with Numerical Data, so learn the basics, then go to more advanced data types if you want to have good news on your pay check.\n\n---","a58fed95":"## Fitting the Model\nWe want to draw some conclusions on the data, so let's try a Random Forest and a Logistic Regression.","fd3d6407":"Get informed on Reddit,  FiveThirtyEight.com or O'Reilly Data Newsletter.  Read machine learning scientific papers on ArXiv.\n\n---","1cbea4d5":"It seems like the Top20iers are so involved with managerial decisions that they consider ML models as black boxes, and delegate the task of explaining outputs to experts.\n\n---","870bf464":"## Undersampling \nThe classes are imbalanced and it's impacting our model. We defined the ```top20``` target variable in a way that we have about 80% of our sample at class 0, and 20% at class 1. Let's fix that.","dd7ba51d":"If you want to get rich, run from Academics\/Education. In the complete study EDA you'll see that Academics\/Education has the lowest average compensation compared to other industries, and this is confirmed by the model's coefficient. I feel bad that one of the most important areas for the future of data science is the one that has the lowest salaries. If you want to earn more, then working at the computers\/technology, accounting\/finance or other industries might increase your likelihood of belonging to the Top 20% most well paid.\n\n---","6f4ba15e":"# What Makes a Kaggler Valuable? \nEver wondered what you should do to add some weight to your Data Science resume? Many of us already have a good notion of what is important to build a strong data science career. Of what is relevant to increase our compensation. But I personally, have never seen a systematic, data based, approach to this problem. That was the motivation of building a model to explain what makes a data scientist valuable to the market. Some results are pretty obvious, but many others might really help you boost your earnings.\n\n* [See the article published on **Towards Data Science** that was written from this Kernel](https:\/\/towardsdatascience.com\/what-makes-a-data-scientist-valuable-b723e6e814aa) \n\n* [We also deployed a model at AWS Lambda to predict the probability of earning more than U$100k per year. Check the code.](https:\/\/github.com\/andresionek91\/kaggle-top20-predictor)\n\n* [Finally we deployed a Flask App so you can predict your own probability.](http:\/\/www.data-scientist-value.com\/)\n\n[![website](https:\/\/i.imgur.com\/8ahkF2J.png)](http:\/\/www.data-scientist-value.com\/)\n\n## Learning how to increase your own compensation\nWe only could do this study because Kaggle has released the data from its [second annual Machine Learning and Data Science Survey](https:\/\/www.kaggle.com\/kaggle\/kaggle-survey-2018). The survey was live for one week in October 2018 and got a total of 23,859 responses. The results include raw numbers about who is working with data, what\u2019s happening with machine learning in different industries, and the best ways for new data scientists to break into the field.\n\nWith access to that data, we wanted to understand what affects a Kaggler\u2019s compensation (we are calling Kaggler anyone that answered the survey). Our idea was to give you precise insights of what is more valuable to the market, so you can stop spending time on things that won\u2019t have a good ROI (return on investment) and speed up towards higher compensation. Following those insights, extracted from data, I hope one day you might find yourself laying down on a pile of money like Mr. Babineaux down here.\n\n![](https:\/\/media.giphy.com\/media\/w1z2ilkWZagRG\/giphy.gif)\n\n## Considerations\n1. We are assuming that respondents were honest and sincere in their answers.\n2. This may not represent the whole universe of data professionals (it only has answers from Kaggle users), but it's a good proxy.\n\n# Basic Cleaning\nSurvey answers are messy... Most survey softwares dont deliver the data on tidy format, and it is exactly the case of this survey. So we are going to have some really hard work to clean it. First lets just look at some personal data and clean it while we do our EDA.","41d76a5f":"Now we want to create a numerical feature that describes compensation. I'm doing that by summing the lower and upper bound and then dividing by 2.  The highest range (500,000+) is summed with itself.","53e59f6c":"Do you know how to do perturbation importance or sensitivity analysis? If you don't, then it is time to learn. *I hope this article is giving you some hints on examination of model coeficients.*\n\n---","92774e19":"## Reducing Answers Dimensions\nSome answers have multiple choices, to avoid overfitting we will drop all answers that had less than 5% of respondents. I'm doing this because we could have some answers with just a few respondents biasing the model towards the Top20%. One example below:\n\n1. On average we expect each answer to have around 80% of the respondents from the Bottom 80% and the rest from the Top 20%. \n2. Let's say we get an answer that has only 10 respondents, and 4 of them are in the Top 20%. This makes this answer very strong towards the top 20%.\n3. Our model will probably consider this feature very important (because the odds of belonging to the Top 20% is much greater to who ansered that question, odds are 40% in fact).\n4. But... A question with only 10 respondents is not representative of the whole population, so our model will be overfitted.\n5. Dropping answers that have less than 5% of respondents will avoid this kind of overfitting, because we know that each answer represents at least 5% of our population. \n6. It's better to have a slightly underfitted than a overfitted model.\n\n*Why 5%? Because it's round and because I want it to be 5%. Could be 10%, 15%, 20%... Try it out and see how the results change.*","957e41e0":"### Should you get formal education?\nTo earn more in this field you have either to go all the way up the formal education and get a Doctoral Degree, or you just don't get any formal education at all. It obviously doesn't mean that you should quit college, but that you are problable better off studying by yourself than attending a post-gratuation program.","8e0cc13f":"Be patient. Give time to time. Your chances of belonging to the Top 20% most well paid will increase as you get older. Makes sense, no?\n\n*Probably there are more people in the 22-24 years range that are just starting their careers in Data Science, that is why we seee an inversion in age.*\n\n---","c74a81ad":"## The selected questions are:\n* Q1. What is your gender?\n* Q2. What is your age (# years)?\n* Q3. In which country do you currently reside?\n* Q4. What is the highest level of formal education that you have attained or plan to attain within the next 2 years?\n* Q6. Select the title most similar to your current role (or most recent title if retired)\n* Q7. In what industry is your current employer\/contract (or your most recent employer if retired)?\n* Q8. How many years of experience do you have in your current role?\n* Q10. Does your current employer incorporate machine learning methods into their business?\n* Q11. Select any activities that make up an important part of your role at work\n* Q15. Which of the following cloud computing services have you used at work or school in the last 5 years?\n* Q16. What programming languages do you use on a regular basis?\n* Q17. What specific programming language do you use most often?\n* Q18. What programming language would you recommend an aspiring data scientist to learn first?\n* Q19. What machine learning frameworks have you used in the past 5 years?\n* Q21 What data visualization libraries or tools have you used in the past 5 years?\n* Q23. Approximately what percent of your time at work or school is spent actively coding?\n* Q24. How long have you been writing code to analyze data?\n* Q26. Do you consider yourself to be a data scientist?\n* Q29. Which of the following relational database products have you used at work or school in the last 5 years?\n* Q30. Which of the following big data and analytics products have you used at work or school in the last 5 years?\n* Q31. Which types of data do you currently interact with most often at work or school?\n* Q36. On which online platforms have you begun or completed data science courses?\n* Q38. Who\/what are your favorite media sources that report on data science topics?\n* Q40. Which better demonstrates expertise in data science: academic achievements or independent projects? \n* Q42. What metrics do you or your organization use to determine whether or not your models were successful?\n* Q47. What methods do you prefer for explaining and\/or interpreting decisions that are made by ML models?\n* Q48. Do you consider ML models to be \"black boxes\" with outputs that are difficult or impossible to explain?\n* Q49. What tools and methods do you use to make your work easy to reproduce?"}}