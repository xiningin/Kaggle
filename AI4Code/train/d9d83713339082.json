{"cell_type":{"943dc23d":"code","0daf50ac":"code","a0298a50":"code","6e0a04aa":"code","fd195ee8":"code","701098f5":"code","e616df20":"code","b4c8fe0d":"code","5af0c5b2":"code","bb33044a":"code","f46cc704":"code","a1361b26":"code","5844c0fc":"markdown","45f1befe":"markdown","cd102b57":"markdown","fb983a6e":"markdown","cd96c4d2":"markdown","169bc8cf":"markdown","a4dae9dd":"markdown","7cb758be":"markdown","0dcc9fba":"markdown","32b25b04":"markdown","9e6a01a7":"markdown","249c564b":"markdown","2331f57b":"markdown","987bdcb1":"markdown","e86d171f":"markdown","07e679ce":"markdown","a75f8888":"markdown","4dc952e0":"markdown","f9c3f31a":"markdown","9b135e76":"markdown","b38dfa9d":"markdown","c92a46ea":"markdown"},"source":{"943dc23d":"from keras.preprocessing.image import ImageDataGenerator\n# All images will be rescaled by 1.\/255\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_dir = '..\/input\/cats-and-dogs-small\/cats_and_dogs_small\/train'\ntrain_generator = train_datagen.flow_from_directory(\n        # This is the target directory\n        train_dir,\n        # All images will be resized to 150x150\n        target_size=(150, 150),\n        batch_size=20,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode='binary')\n\nvalidation_dir = '..\/input\/cats-and-dogs-small\/cats_and_dogs_small\/validation'\nvalidation_generator = test_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(150, 150),\n        batch_size=20,\n        class_mode='binary')","0daf50ac":"from keras.applications import VGG16\n\nconv_base = VGG16(weights='imagenet',\n                  include_top=False,\n                  input_shape=(150, 150, 3))","a0298a50":"conv_base.summary()","6e0a04aa":"from keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(conv_base)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","fd195ee8":"model.summary()","701098f5":"print('This is the number of trainable weights '\n      'before freezing the conv base:', len(model.trainable_weights))","e616df20":"conv_base.trainable = False","b4c8fe0d":"print('This is the number of trainable weights '\n      'after freezing the conv base:', len(model.trainable_weights))","5af0c5b2":"from keras import optimizers\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizers.RMSprop(lr=1e-4),\n              metrics=['acc'])","bb33044a":"history = model.fit(\n      train_generator,\n      steps_per_epoch=100,\n      epochs=5,\n      validation_data=validation_generator,\n      validation_steps=50)","f46cc704":"import matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","a1361b26":"model.save('cats_and_dogs_small_VGG16.h5')","5844c0fc":"# Using a pre-trained convnet \n* This notebook is based on Chollet, F. (2018). Deep learning with Python. Available at https:\/\/www.manning.com\/books\/deep-learning-with-python. Chollet is the creator of Keras https:\/\/keras.io\/. \n\n## Learning objectives\n1. What is transfer learning?\n1. How to reuse a pre-trained convnet and build a new model?\n1. How to implement the reuse of a convnet with Keras?","45f1befe":"### Level of generality\n|<img src=\"https:\/\/drek4537l1klr.cloudfront.net\/chollet\/Figures\/05fig02.jpg\">|\n|:--:| \n|[A spatial hierarchy of visual modules](https:\/\/drek4537l1klr.cloudfront.net\/chollet\/Figures\/05fig02.jpg)|\n\n* The level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on the depth of the layer in the model. \n    * Layers that come earlier in the model extract local, highly generic feature maps \n        * such as visual edges, colors, and textures\n    * Layers higher-up extract more abstract concepts\n        * such as \"cat ear\" or \"dog eye\". \n\n* If the new dataset differs a lot from the dataset that the original model was trained on, it may be better off using only the first few layers of the model to do feature extraction, rather than using the entire convolutional base.","cd102b57":"### Trainable layers\n*  only the weights from the two `Dense` layers that we added will be trained. \n* That's a total of four weight tensors: two per layer (the main weight matrix and the bias vector).\n","fb983a6e":"### Architecture of the VGG16 convolutional base\n* The architecture of the VGG16 convolutional base is very similar to simple convnets \n* The final feature map has shape `(4, 4, 512)`\n    *  the feature on top for a densely-connected classifier.","cd96c4d2":"### Three arguments to the constructor of VGG16\n* `weights`: to specify which weight checkpoint to initialize the model from\n* `include_top`: refers to including or not the densely-connected classifier on top of the network. By default, this densely-connected classifier would correspond to the 1000 classes from ImageNet. Since we intend to use our own densely-connected classifier (with only two classes, cat and dog), we don't need to include it.\n* `input_shape`: the shape of the image tensors that we will feed to the network. This argument is purely optional: if we don't pass it, then the network will be able to process inputs of any size.\n","169bc8cf":"### Extending the model\n* Extende the model  (`conv_base`) by adding `Dense` layers on top \n    * This technique is so expensive that it is better to run on a GPU.\n    * Running time is too long on CPU. ","a4dae9dd":"### Number of parameters\n* The convolutional base of VGG16 has 14,714,688 parameters,  very large. \n* The dense layer has a 2,097,408 parameters.","7cb758be":"### Reuse the convolutional base or densely-connected classifier? \n* In general, it should be avoided reusing densely-connected classifier. \n* The reason is simply that the representations learned by the convolutional base are likely to be more generic and therefore more reusable\n    * the feature maps of a convnet are presence maps of generic concepts over a picture, which is likely to be useful regardless of the computer vision problem. \n    * On the other end, the representations learned by the classifier will necessarily be very specific to the set of classes that the model was trained on -- they will only contain information about the presence probability of this or that class in the entire picture.  \n    * For problems where object location matters, densely-connected features would be largely useless.","0dcc9fba":"## Part 2 Feature extraction\n### Feature extraction\n* One way  to leverage a pre-trained network is  *feature extraction*  \n* Feature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. \n* These features are then run through a new classifier","32b25b04":"###  Freeze the convolutional base\n|<img src=\"https:\/\/drek4537l1klr.cloudfront.net\/chollet\/Figures\/05fig14_alt.jpg\" width=400>|\n|:--:|\n|[Freeze the convolutional base](https:\/\/drek4537l1klr.cloudfront.net\/chollet\/Figures\/05fig14_alt.jpg)|\n\n\n* Before we compile and train the model, it is important to freeze the convolutional base. \n    * \"Freezing\" a layer or set of layers means preventing their weights from getting updated during training. \n    * Without freezing, representations that were  previously learned by the convolutional base would get modified during training. \n    * Since the `Dense` layers on top are randomly initialized, very large weight updates would be propagated through the network, effectively destroying the representations previously learned.\n* In Keras, freezing a network is done by setting its `trainable` attribute to `False`:","9e6a01a7":"## Part 1 Transfer Learning\n### What is transfer Learning?\n |<img src=\"https:\/\/datascience.aero\/wp-content\/uploads\/2020\/03\/transferlearning-119.jpg\" width =800 class=\"center\">|\n |:--:|\n |[transfer learning](https:\/\/datascience.aero\/wp-content\/uploads\/2020\/03\/transferlearning-119.jpg)|\n \n* Transfer learning is a machine learning method. \n    * a model has been developed for solving one problem.\n    * then it  is reused as the starting model for solving another different but related problem.\n* For example, a neural network is pretrained on ImageNet and then it is reused as a starting model on X-ray images for different disease classifications  \n> Transfer learning and domain adaptation refer to the situation where what has been learned in one setting ... is exploited to improve generalization in another setting.\n[deep learning](https:\/\/www.deeplearningbook.org\/)\n \n* In deep learning, pre-trained models are often reused as the starting point in the area of computer vision and natural language processing\n\n","249c564b":"## Part 3 Implementation of reusing a pre-trained convnet base\n### Procedure\n|<img src=\"https:\/\/drek4537l1klr.cloudfront.net\/chollet\/Figures\/05fig14_alt.jpg\" width=400>|\n|:--:|\n|[Swapping classifiers while keeping the same convolutional base](https:\/\/drek4537l1klr.cloudfront.net\/chollet\/Figures\/05fig14_alt.jpg)|\n\n1. First use the convolutional base of a pre-trained network on ImageNet  to extract interesting features from  cat and dog images\n1. Then training a cat vs. dog classifier on top of these features.","2331f57b":"#### ImageNet  dataset\n|<img src=\"https:\/\/devopedia.org\/images\/article\/172\/7316.1561043304.png\" width=500>|\n|:--:|\n|[ImageNet](https:\/\/devopedia.org\/images\/article\/172\/7316.1561043304.png)|\n\n* [ImageNet dataset](http:\/\/www.image-net.org\/) (1.4 million labeled images and 1000 different classes). \n    * ImageNet contains many animal classes, including different species of cats and dogs\n    * A pre-trained model on ImageNet is expected to perform well on the cat vs. dog classification problem.\n","987bdcb1":"## Summary   \n* Transfer learning is a machine learning method. \n    * a model has been developed for solving one problem.\n    * then it  is reused as the starting model for solving another different but related problem.\n* Reusing an pre-trained convnet belongs to transfer learning\n    * First reuse the convolutional base of a pre-trained network. This base is used to extract feature maps\n    * Then train  a new classifier (dense layers) on top of these features. \n     ","e86d171f":"### Instantiate a VGG16 model \n* The VGG16 model is pre-packaged with Keras.\n* Import it from the `keras.applications` module. \n* Instantiate a VGG16 model","07e679ce":"### Convolutional base\n|<img src=\"https:\/\/www.researchgate.net\/publication\/337619475\/figure\/fig2\/AS:830399039156224@1574993967836\/The-general-structure-of-convolutional-neural-network-Convolutional-neural-network.png\" width=600>|\n|:--:|\n|[Convnet](https:\/\/www.researchgate.net\/publication\/337619475\/figure\/fig2\/AS:830399039156224@1574993967836\/The-general-structure-of-convolutional-neural-network-Convolutional-neural-network.png)|\n\n\n* Convnets used for image classification comprise two parts\n    * a series of pooling and convolution layers for feature maps\n    * densely-connected layers for classification and output. \n* The first part is called the \"**convolutional base**\" of the model. \n* In the case of convnets, \"**feature extraction**\" will simply consist of taking the convolutional base of a previously-trained network, running the new data through it, and training a new classifier on top of the output.","a75f8888":"### Compile the model\n* If you ever modify weight trainability after compilation, you should then re-compile the model, or these changes would be ignored.\n ","4dc952e0":"### Number of trainable weights before and after freezing","f9c3f31a":"\n### Pre-trained networks\n|<img src=\"https:\/\/dev-to-uploads.s3.amazonaws.com\/i\/2oyycxxs02jmcghplwc4.png\" width=500 class =\"center\">|\n|:--:|\n|[Reuse a pre-trained networks](https:\/\/dev-to-uploads.s3.amazonaws.com\/i\/2oyycxxs02jmcghplwc4.png)|\n\n* Leveraging a pre-trained network is a common and highly effective approach to deep learning on small image datasets. \n    * A pre-trained network is a saved network previously trained on a large dataset, typically on a large-scale image classification task. \n    * If this original dataset is large enough and general enough, then the spatial feature hierarchy learned by the pre-trained network can effectively act as a generic model of our visual world\n    * The features from the original dataset are useful for many different computer vision problems, even though these new problems might involve completely different classes from those of the original task.  \n* Such portability of learned features across different problems is a key advantage of deep learning compared to many older shallow learning approaches, and it makes deep learning very effective for small-data problems.","9b135e76":"### Plot results","b38dfa9d":"### Data preprocessing\n* Data are JPEG images.\n* The steps for getting data into the network.\n    * Read the picture files.\n    * Decode the JPEG content to RBG grids of pixels.\n    * Convert these into floating point tensors.\n    * Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values).\n* Keras has utilities to implement these steps automatically.\n    * Keras has a module with image  processing helper tools, located at `keras.preprocessing.image`. \n    * In particular,   the class `ImageDataGenerator`   allows to  quickly set up Python generators that can automatically turn image files on disk into batches of pre-processed tensors.  ","c92a46ea":"### Summary of the model  "}}