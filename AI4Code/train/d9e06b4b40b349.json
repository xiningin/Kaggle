{"cell_type":{"a1f645bd":"code","6606cfbd":"code","00304a04":"code","e94ca07c":"code","573b2605":"code","bb2a32b4":"code","b4478be3":"code","dd1b40c6":"code","782492d5":"code","26e4a5b7":"code","8704a815":"code","55f9cf6d":"code","065c4b1b":"code","aee6222d":"code","37337a27":"code","e7f94c38":"code","108d8505":"code","4ef6899c":"code","c524e986":"code","fd22699a":"markdown","384ed96b":"markdown","89400c77":"markdown","68cb81ea":"markdown"},"source":{"a1f645bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n","6606cfbd":"data = np.load('..\/input\/Sign-language-digits-dataset\/X.npy')\nlabels = np.load('..\/input\/Sign-language-digits-dataset\/Y.npy')\ndata.shape, labels.shape # data have 2062 observations, each consists in a 64x64 px\n                         # in labels, we have same numbers of row in 10 col \n\n","00304a04":"def plot_lang_digits(data, label):\n    print('Dataset examples:')\n    number_of_classes = label.shape[1]\n    col_idx = [i for i in range(number_of_classes)]\n    plt.figure(figsize=(18, 6))\n\n    for i in col_idx:\n        ax = plt.subplot(2, 5, i+1) # row col index\n        ax.set_title(\"idx: \" + str(i))\n        plt.imshow(data[np.argwhere(label[:,i]==1)[0][0],:]) # , cmap='gray'\n        #plt.gray()\n        plt.axis('off')\n        \nplot_lang_digits(data, labels)","e94ca07c":"'index is not correct'\n# we create dictionary and match the corresponding\n# label_idx_map = { 0:9, 1:0, 2:7, 3:6, 4:1, 5:8, 6:4, 7:3, 8:2, 9:5}\nlabel_idx_map={0:9,1:0, 2:7, 3:6, 4:1, 5:8, 6:4, 7:3, 8:2, 9:5}\n\nY = np.zeros(data.shape[0]) # create array of '0' with data size as given data (2062, 64, 64)\n#  fill array with correct labels\nY[:204] = 9; Y[204:409] = 0; Y[409:615] = 7; Y[615:822] = 6; Y[822:1028] = 1; Y[1028:1236] = 8; Y[1236:1443] = 4; \nY[1443:1649] = 3; Y[1649:1855] = 2; Y[1855:] = 5\n","573b2605":"'Analysing only 0s & 1s'\nX = np.concatenate((data[204:409], data[822:1027]), axis=0) # Images of all Zeros and Ones \n# creating labels for 0 and 1 sign images.\nz=np.zeros(205)\no=np.ones(205)\nY=np.concatenate((z,o), axis=0).reshape(X.shape[0],1)\n# The shape of X is (410, 64, 64), where:\n# 410 images (zero and one signs), 64 means that image size is 64x64 (64x64 pixels)\n# The shape of the Y is (410,1), where:\n# 410 means that we have 410 labels (0 and 1)  , X.shape[0] = 410\n\nprint(\" X shape: \",  X.shape, \"\\n Y shape: \",  Y.shape)","bb2a32b4":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\nprint(' X_train.shape :',X_train.shape,'\\n X_test.shape  :',X_test.shape)\n# 3 Dimensional input array X\n","b4478be3":"# Y (labels) is already in 2D\n# X (image array) to be Flattened from 3D array to 2D\nX_train_flatten = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\nX_test_flatten = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\nprint(' X_train_flatten :',X_train_flatten.shape ,'\\n X_test_flatten  :',X_test_flatten.shape)\n# We have 348 and 62 image arrays and each image has 4096 pixels\n","dd1b40c6":"# Transpose each of the four image arrays\nx_train = X_train_flatten.T\nx_test = X_test_flatten.T\ny_train = Y_train.T\ny_test = Y_test.T\nprint(\" x_train:\",x_train.shape, \n      \"\\n x_test: \",x_test.shape,\n      \"\\n y_train:\",y_train.shape,\n      \"\\n y_test: \",y_test.shape)\n","782492d5":"'Initilizing parameters'\n#np.full((3, 1), 7, dtype=int) 3 is height\n\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01) # initialize weight 0.01\n    b = 0.0 # initialize bias 0\n    return w, b\n\n\nw,b = initialize_weights_and_bias(4096)","26e4a5b7":"'Forward Propagation'\n\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head\n\n# y_head = sigmoid(0)\n# y_head\n\n# find z = w.T*x+b\n# calculation of z is: z = np.dot(w.T,x_train)+b\n# y_head = sigmoid(z) # probabilistic 0-1\n# loss(error) = loss(y,y_head)\n# cost = sum(loss)\n\ndef forward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z) \n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    return cost ","8704a815":"'Optimization Algorithm with Gradient Descent'\n#w,b = initialize_weights_and_bias(1)\ncost = forward_propagation(w,b,x_train,y_train)\ncost # without any iteration or 0 iteration","55f9cf6d":"# In backward propagation we will use y_head that found in forward progation\n# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n   \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","065c4b1b":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate, number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n\nparameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.009, number_of_iterarion = 200)","aee6222d":" # prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","37337a27":"#Applying all functions\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate , num_iterations):\n    # initialize\n    dimension = x_train.shape[0]  # that is 4096\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)","e7f94c38":"'Logistic Regression with Sklearn'\nfrom sklearn import linear_model\n\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\n\nprint(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","108d8505":"# intialize parameters and layer sizes\ndef initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1,\n                  \"bias1\": np.zeros((3,1)),\n                  \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,\n                  \"bias2\": np.zeros((y_train.shape[0],1))}\n    return parameters\n\ndef forward_propagation_NN(x_train, parameters):\n\n    Z1 = np.dot(parameters[\"weight1\"],x_train) +parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache\n\n# Compute cost\ndef compute_cost_NN(A2, Y, parameters):\n    logprobs = np.multiply(np.log(A2),Y)\n    cost = -np.sum(logprobs)\/Y.shape[1]\n    return cost\n\n# Backward Propagation\ndef backward_propagation_NN(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads\n\n# update parameters\ndef update_parameters_NN(parameters, grads, learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters\n\n# prediction\ndef predict_NN(parameters,x_test):\n    # x_test is a input for forward propagation\n    A2, cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","4ef6899c":"# 2 - Layer neural network\ndef two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n    cost_list = []\n    index_list = []\n    #initialize parameters and layer sizes\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n\n    for i in range(0, num_iterations):\n         # forward propagation\n        A2, cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost = compute_cost_NN(A2, y_train, parameters)\n         # backward propagation\n        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n         # update parameters\n        parameters = update_parameters_NN(parameters, grads)\n        \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\n","c524e986":"parameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=2500)","fd22699a":"<br><br><br>\n<h3>With 2-layers Neural Network<h3>\n","384ed96b":"*Up to this point we have created basic neural network and implemented functions for: \n* Size of layers and initializing parameters weights and bias \n* Forward propagation \n* Loss function and Cost function \n* Backward propagation \n* Update Parameters \n* Prediction with learnt parameters weight and bias \n* Create Model and applied\n","89400c77":"<br><br><br>\n**Now, checking straight with Sklearn - logistic Regression **","68cb81ea":"*Waoo amazing accuracy - direct from Sklearn*"}}