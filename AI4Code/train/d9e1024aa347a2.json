{"cell_type":{"d264c78b":"code","534a448e":"code","cb68df37":"code","58e6f795":"code","666bc506":"code","f35bcd67":"code","9ce07656":"code","f95dd977":"code","d5787ffc":"code","25b3e371":"code","d9d5a5f1":"code","9f073f31":"code","e71ba3cd":"code","81dbb8e2":"code","722a3da1":"code","8cf910d0":"code","1021ce90":"code","54b17344":"code","7c1309d2":"code","115601f9":"code","8c301115":"code","38689640":"code","8550250a":"markdown","571680f6":"markdown","00f0a661":"markdown","480d08fe":"markdown","229d97d3":"markdown","75ad750e":"markdown","093f5bc3":"markdown","6e298462":"markdown"},"source":{"d264c78b":"TPU=True\nTEST =False\nDEBUG = False\nNUM_FOLDS=4\n#FOLDS = [3]\nFOLDS = [1, 2, 3]\nRETRAIN=False\nLIMIT_NUM=0","534a448e":"GCS_DS_PATH_1= \"kds-d36965c71f1aba91ef7a2904c04ed5d71407e6cbcc88de690f93bdda\"\nGCS_DS_DIR_1=\"dataset1\"\nGCS_DS_PATH_2= \"kds-ab101f0bc8bef40c66229723dae46eb3cebe1c43d19df839b985153b\"\nGCS_DS_DIR_2=\"dataset2\"\nGCS_DS_PATH_3= \"kds-2f531983451093758fb7f53b2c108f3c640609fb9276735c7725e75b\"\nGCS_DS_DIR_3=\"dataset3\"\nGCS_DS_PATH_4= \"kds-e2ecb8c94994392b4f693b80830db7d7418a02e1ca08946d3f060fd7\"\nGCS_DS_DIR_4=\"dataset4\"\nGCS_DS_PATH_5= \"kds-339d2b166669775eda09e16316f0e8c48c8435ba7db037bb56bdc9ed\"\nGCS_DS_DIR_5=\"dataset5\"","cb68df37":"import warnings\nwarnings.simplefilter('ignore')\nimport sys\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n#from tensorflow.keras.mixed_precision import experimental as mixed_precision\nfrom tqdm.notebook import tqdm\n\nimport re\nimport os\nimport io\nimport time\nimport pickle\nimport math\nimport random\nimport sys\n\nimport itertools\nimport tensorflow_hub as hub\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import KFold\nfrom pathlib import Path\nfrom typing import Optional, Tuple\n\n#from kaggle_datasets import KaggleDatasets\n\nprint(f'tensorflow version: {tf.__version__}')\nprint(f'tensorflow keras version: {tf.keras.__version__}')\nprint(f'python version: P{sys.version}')","58e6f795":"N_CHANNELS = 3\nN_SAMPLES = 1580470\nN_CLASSES = 81313\nEPOCHS = 20\nSEED = 1213\nBATCH_SIZE=24\n#BATCH_SIZE=64\nIMAGE_SIZE=420\n","666bc506":"def set_seed(seed=42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nset_seed(SEED)","f35bcd67":"def auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n        TPU=True\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    return strategy","9ce07656":"strategy = auto_select_accelerator()\nREPLICAS = strategy.num_replicas_in_sync\nAUTO = tf.data.experimental.AUTOTUNE","f95dd977":"gcs_paths = []\ngcs_paths.append('gs:\/\/'+GCS_DS_PATH_1)\ngcs_paths.append('gs:\/\/'+GCS_DS_PATH_2)\ngcs_paths.append('gs:\/\/'+GCS_DS_PATH_3)\ngcs_paths.append('gs:\/\/'+GCS_DS_PATH_4)\ngcs_paths.append('gs:\/\/'+GCS_DS_PATH_5)\n    \nall_files = []\nfor path in gcs_paths:\n    all_files.extend(np.sort(np.array(tf.io.gfile.glob(path + \"\/*.tfrec\"))))\n\nprint(\"train files: \", len(all_files))","d5787ffc":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.image.resize(image, size=(IMAGE_SIZE, IMAGE_SIZE))\n    image = tf.cast(image, tf.float32) \/ 255.0\n    return image\n\n\ndef read_labeled_tfrecord(example):\n    tfrec_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64),\n    }\n\n    example = tf.io.parse_single_example(example, tfrec_format)\n    posting_id = example[\"image_name\"]\n    image = decode_image(example[\"image\"])\n    label_group = tf.cast(example[\"target\"], tf.int32)\n    matches = 1\n    return posting_id, image, label_group, matches\n\n\ndef arcface_format(posting_id, image, label_group, matches):\n    return posting_id, {'inp1': image, 'inp2': label_group}, label_group, matches\n\n# This function loads TF Records and parse them into tensors\ndef load_dataset(filenames, batch_size=64, cache=False, repeat=False, shuffle=False):        \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n    if cache:\n        dataset = dataset.cache()\n\n    if shuffle:\n        dataset = dataset.shuffle(2048)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        dataset = dataset.with_options(opt)\n    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls = AUTO)\n    dataset = dataset.map(arcface_format, num_parallel_calls=AUTO)\n    if repeat:\n        dataset = dataset.repeat()\n    dataset = dataset.batch(batch_size * REPLICAS)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\ndef count_data_items(filenames):\n    # The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n","25b3e371":"NUM_TRAINING_IMAGES = count_data_items(all_files)\nNUM_TRAINING_IMAGES","d9d5a5f1":"class GeM(tf.keras.layers.Layer):\n    def __init__(self, pool_size, init_norm=3.0, normalize=False, **kwargs):\n        self.pool_size = pool_size\n        self.init_norm = init_norm\n        self.normalize = normalize\n\n        super(GeM, self).__init__(**kwargs)\n\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'pool_size': self.pool_size,\n            'init_norm': self.init_norm,\n            'normalize': self.normalize,\n        })\n        return config\n\n    def build(self, input_shape):\n        feature_size = input_shape[-1]\n        self.p = self.add_weight(name='norms', shape=(feature_size,),\n                                 initializer=tf.keras.initializers.constant(self.init_norm),\n                                 trainable=True)\n        super(GeM, self).build(input_shape)\n\n    def call(self, inputs):\n        x = inputs\n        x = tf.math.maximum(x, 1e-6)\n        x = tf.pow(x, self.p)\n\n        x = tf.nn.avg_pool(x, self.pool_size, self.pool_size, 'VALID')\n        x = tf.pow(x, 1.0 \/ self.p)\n\n        if self.normalize:\n            x = tf.nn.l2_normalize(x, 1)\n        return x\n\n    def compute_output_shape(self, input_shape):\n        return tuple([None, input_shape[-1]])","9f073f31":"class ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https:\/\/arxiv.org\/pdf\/1801.07698.pdf\n        https:\/\/github.com\/lyakaap\/Landmark2019-1st-and-3rd-Place-Solution\/\n            blob\/master\/src\/modeling\/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output\n","e71ba3cd":"FOLDER_EFF_ORG='gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-l-21k-ft1k\/classification'\n#FOLDER_EFF_ORG='..\/input\/efficientnetv2-tfhub-weight-files\/tfhub_models\/efficientnetv2-m-21k-ft1k\/classification'\nFOLDER_EFF_ORG","81dbb8e2":"def build_model(size=384, count=0):\n    inp = tf.keras.layers.Input(shape=(size, size, 3), name=\"inp1\")\n    label = tf.keras.layers.Input(shape=(), name=\"inp2\")\n    x = hub.KerasLayer(FOLDER_EFF_ORG, trainable=True)(inp)\n    #x = GeM(8)(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(1000, name=\"dense_before_arcface\", kernel_initializer=\"he_normal\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = ArcMarginProduct(\n        n_classes=N_CLASSES,\n        s=30,\n        m=0.5,\n        name=\"head\/arc_margin\",\n        dtype=\"float32\"\n    )([x, label])\n    output = tf.keras.layers.Softmax(dtype=\"float32\")(x)\n    model = tf.keras.Model(inputs=[inp, label], outputs=[output])\n    lr_decayed_fn = tf.keras.experimental.CosineDecay(1e-3, count)\n    opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate=1e-4)\n    model.compile(\n        optimizer=opt,\n        loss=[tf.keras.losses.SparseCategoricalCrossentropy()],\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n    )\n    return model","722a3da1":"sample_model=build_model()","8cf910d0":"# Plot model summary\nsample_model.summary()","1021ce90":"# Plot slightly more detailed model summary\ntf.keras.utils.plot_model(sample_model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=False)","54b17344":"def get_lr_callback(plot=False):\n    lr_start   = 1e-4\n    lr_max     = 0.00003 * BATCH_SIZE  \n#    lr_max     = 0.000012 * BATCH_SIZE  \n    lr_min     = 1e-5\n    lr_ramp_ep = 4\n    lr_sus_ep  = 0\n    lr_decay   = 0.9\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n        \n    if plot:\n        epochs = list(range(EPOCHS))\n        learning_rates = [lrfn(x) for x in epochs]\n        plt.scatter(epochs,learning_rates)\n        plt.show()\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback\n\nget_lr_callback(plot=True)","7c1309d2":"kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\nfiles_train_all = np.array(all_files)\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(files_train_all)):\n    pd.Series(data=trn_idx,index=None).to_csv(f\"train_fold{fold}.csv\",index=None)\n    pd.Series(data=val_idx,index=None).to_csv(f\"val_fold{fold}.csv\",index=None)\n","115601f9":"for fold, (trn_idx, val_idx) in enumerate(kf.split(files_train_all)):\n    if fold not in FOLDS:\n        continue\n    files_train = files_train_all[trn_idx]\n    files_valid = files_train_all[val_idx]\n\n    print(\"=\" * 120)\n    print(f\"Fold {fold}\")\n    print(\"=\" * 120)\n\n    train_image_count = count_data_items(files_train)\n    valid_image_count = count_data_items(files_valid)\n\n    tf.keras.backend.clear_session()\n    strategy = auto_select_accelerator()\n\n    with strategy.scope():\n        model = build_model(\n            size=IMAGE_SIZE,\n            count=train_image_count \/\/ BATCH_SIZE \/\/ REPLICAS \/\/ 4\n        )\n    if RETRAIN:\n        model.load_weights(RELOAD_NAME)\n \n    model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n        str(f\"M480_fold{fold}.h5\"), monitor=\"val_loss\", verbose=1, save_best_only=False,\n        save_weights_only=True, mode=\"min\", save_freq=\"epoch\"\n    )\n\n    train_dataset = load_dataset(files_train, batch_size=BATCH_SIZE, shuffle=True, repeat=True)\n    train_dataset = train_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n\n    valid_dataset = load_dataset(files_valid, batch_size=BATCH_SIZE * 2, shuffle=False, repeat=False)\n    valid_dataset = valid_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n\n    STEPS_PER_EPOCH = train_image_count \/\/ BATCH_SIZE \/\/ REPLICAS \/\/ 4\n    history = model.fit(\n        train_dataset,\n#        initial_epoch=6,\n        epochs=EPOCHS,\n        callbacks=[model_ckpt, get_lr_callback()],\n        steps_per_epoch=STEPS_PER_EPOCH,\n#        validation_data=valid_dataset,\n        verbose=1\n    )","8c301115":"#model.save(\"drive\/MyDrive\/efficientnetv2model\/my_model\")\n#model2=tf.keras.models.load_model(\"save_model\")","38689640":"#plot_history_metric(history, 'loss', np.argmin)","8550250a":"# Data Loading","571680f6":"# Config","00f0a661":"# Dataset","480d08fe":"# Training","229d97d3":"# Model","75ad750e":"# Training","093f5bc3":"## Other training utilities","6e298462":"# Config"}}