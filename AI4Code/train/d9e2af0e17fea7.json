{"cell_type":{"5a75f7d2":"code","c3257f07":"code","eefa4e99":"code","9882930f":"code","a7b80ff8":"code","e980fa8b":"code","2459819f":"markdown","cac85ad5":"markdown","4733c1f9":"markdown","7d659934":"markdown","402e2e77":"markdown","43e25c3c":"markdown"},"source":{"5a75f7d2":"import os\nimport multiprocessing\nimport pandas as pd\nimport time\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_squared_error\n\nPATH = '..\/input\/tabular-feature-engineering-dataset'\n\nTOKEN = \"_train.csv\"\nVERBOSE = 0\nTHREADS = multiprocessing.cpu_count()\nCYCLES = 5\nFAIL_ON_NAN = False\nSAMPLE = 1.0\n            \n# Human readable time elapsed string.\ndef hms_string(sec_elapsed):\n    h = int(sec_elapsed \/ (60 * 60))\n    m = int((sec_elapsed % (60 * 60)) \/ 60)\n    s = sec_elapsed % 60.\n    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n            \n# Build a deep neural network for the experiments.\ndef neural_network_regression(x_train):\n    model = Sequential()\n    model.add(Dense(400, input_dim=x_train.shape[1], activation='relu'))\n    model.add(Dense(200, activation='relu'))\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(50, activation='relu'))\n    model.add(Dense(25, activation='relu'))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n\n# Grid-search for a SVM with good C and Gamma.\ndef svr_grid():\n    param_grid = {\n        'C': [1e-2, 1, 1e2],\n        'gamma': [1e-1, 1, 1e1]\n\n    }\n    clf = GridSearchCV(SVR(kernel='rbf'), cv=5, verbose=VERBOSE,\n                       n_jobs=THREADS, param_grid=param_grid)\n    return clf\n\n# Perform an experiment for a single model type.\ndef run_model(name, model, results, x_train, y_train, x_validate, y_validate):\n    model_name = model.__class__.__name__\n\n    # Normalize, if called for\n    if 'GridSearchCV' in model_name:\n        x_train = MinMaxScaler().fit_transform(x_train)\n        x_validate = MinMaxScaler().fit_transform(x_validate)\n    \n    # Run cycles\n    cycle_list = []\n    for cycle_num in range(1, CYCLES + 1):\n        start_time = time.time()\n        if 'KerasRegressor' in model_name:\n\n            monitor = EarlyStopping(\n                monitor='val_loss', min_delta=1e-3, patience=20, verbose=VERBOSE, mode='auto')\n            model.fit(x_train, y_train,\n                      validation_data=(x_validate, y_validate),\n                      callbacks=[monitor], verbose=VERBOSE, epochs=100000)\n        else:\n            model.fit(x_train, y_train)\n            \n        score = validate_model(model, x_validate, y_validate)\n        elapsed_time = hms_string(time.time() - start_time)\n        line = [name, model_name, score, np.std(y_validate), \n                np.mean(y_validate), elapsed_time]\n        cycle_list.append(line)\n        print(f\"Cycle {cycle_num}:{line}\")\n\n    best_cycle = min(cycle_list, key=lambda k: k[2])\n    print(\"{}(Best)\".format(best_cycle))\n    results.append(best_cycle)\n\n    #writer.writerow(best_cycle)\n    \ndef validate_model(model, x_validate, y_validate):\n    model_name = model.__class__.__name__\n    if 'KerasRegressor' in model_name:\n            pred = model.predict(x_validate, verbose=VERBOSE)\n    else:\n        pred = model.predict(x_validate)\n\n    # Get the validatoin score\n    if np.isnan(pred).any():\n        if FAIL_ON_NAN:\n            raise Exception(\"Unstable model. Can't validate.\")\n        score = 1e5 # a bad score\n    else:\n        score = np.sqrt(mean_squared_error(pred, y_validate))\n        score \/= np.std(y_validate)\n\n    return score\n\ndef eval_data(name, results):\n    path_train = os.path.join(PATH,f\"{name}_train.csv\")\n    path_validate = os.path.join(PATH,f\"{name}_validate.csv\")\n    df_train=pd.read_csv(path_train)\n    df_validate=pd.read_csv(path_validate)\n    if SAMPLE<1.0:\n        df_train = df_train.sample(frac=SAMPLE)\n        df_validate = df_validate.sample(frac=SAMPLE)\n    print(f\"Training size: {len(df_train)}\")\n    print(f\"Validate size: {len(df_validate)}\")\n    x_cols = list(df_train.columns)\n    x_cols.remove('y1')\n    y_cols = ['y1']\n    x_train = df_train[x_cols].values\n    y_train = df_train[y_cols].values.ravel()\n    x_validate = df_validate[x_cols].values\n    y_validate = df_validate[y_cols].values.ravel()\n    \n    models = [\n        svr_grid(),\n        RandomForestRegressor(n_estimators=100),\n        GradientBoostingRegressor(\n            n_estimators=100, learning_rate=0.1, max_depth=10, random_state=0, verbose=VERBOSE),\n        KerasRegressor(build_fn=neural_network_regression, x_train=x_train)\n    ]\n    \n    for model in models:\n        run_model(name, model, results, x_train, y_train, x_validate, y_validate)\nresults = []\n\n# Find all of the tests\ntests = set()\nfor dirname, _, filenames in os.walk(PATH):\n    for filename in filenames:\n        if filename.endswith(TOKEN):\n            tests.add(filename[:-len(TOKEN)])\n            \n# run the tests\nstart_time = time.time()\nfor test in tests:\n    eval_data(test, results)\nprint(f\"Total elapsed time: {hms_string(time.time() - start_time)}\")\n            \n# format results\ndf = pd.DataFrame(results)\ndf.columns = [\"equation\",\"model\", \"score\", \"std\", \"mean\", \"time\"]\ndf.to_csv(\"\/kaggle\/working\/results.csv\",index=False)","c3257f07":"df","eefa4e99":"df[df.model=='GridSearchCV'].plot.bar(x=\"equation\",y=\"score\",title=\"Support Vector Machine\")","9882930f":"df[df.model=='RandomForestRegressor'].plot.bar(x=\"equation\",y=\"score\",title=\"Random Forest\")","a7b80ff8":"df[df.model=='GradientBoostingRegressor'].plot.bar(x=\"equation\",y=\"score\",title=\"Gradient Boosted Machine\")","e980fa8b":"df[df.model=='KerasRegressor'].plot.bar(x=\"equation\",y=\"score\",title=\"Neural Network\")","2459819f":"## Neural Network Results","cac85ad5":"## An Empirical Analysis of Feature Engineering for Predictive Modeling\nThis example notebook performs the analysis for the following paper. The code has been updated and converted to work with Kaggle.\n\nHeaton, J. (2016, April). [An Empirical Analysis of Feature Engineering for Predictive Modeling](https:\/\/arxiv.org\/abs\/1701.07852). In *SoutheastCon 2016* (pp. 1-6). IEEE.\n\n## Paper Abstract\n\nMachine learning models, such as neural networks, decision trees, random forests, and gradient boosting machines, accept a feature vector, and provide a prediction.  These models learn in a supervised fashion where we provide feature vectors with the expected output.  It is common practice to engineer new features from the provided feature set.  Such engineered features will either augment or replace portions of the existing feature vector.  These engineered features are essentially calculated fields based on the values of the other features.  \n\nEngineering such features is primarily a manual, time-consuming task.  Additionally, each type of model will respond differently to different kinds of engineered features.  This paper reports empirical research to demonstrate what kinds of engineered features are best suited to various machine learning model types.  We provide this recommendation by generating several datasets that we designed to benefit from a particular type of engineered feature.  The experiment demonstrates to what degree the machine learning model can synthesize the needed feature on its own.  If a model can synthesize a planned feature, it is not necessary to provide that feature.  The research demonstrated that the studied models do indeed perform differently with various types of engineered features. ","4733c1f9":"# Gradient Boosted Machine Results","7d659934":"## Random Forest Results","402e2e77":"## Data Collected","43e25c3c":"# Support Vector Machine Results"}}