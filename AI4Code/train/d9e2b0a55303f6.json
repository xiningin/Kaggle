{"cell_type":{"6a3513df":"code","44b6adba":"code","42ba8c91":"code","f4893a40":"code","dcc92ace":"code","3226c766":"code","42bb13a8":"code","b3a19d43":"code","b3c20077":"code","1ec0ceeb":"code","7fc0aa65":"code","0bd69583":"code","7a773606":"markdown","29b8d8dd":"markdown","9a9227ee":"markdown","7123598d":"markdown","b3714229":"markdown","fb238c8e":"markdown","87e1c1de":"markdown","0bafa3a9":"markdown"},"source":{"6a3513df":"# The original code comes from https:\/\/github.com\/ardamavi\/Sign-Language-Digits-Dataset\n\nimport os\nfrom os import listdir\nimport glob\nimport numpy as np\nimport cv2\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\nfrom matplotlib import pyplot as plt","44b6adba":"dataset_path = \"..\/input\/sign_lang_dataset\/Dataset\"\n\nsub_dir = glob.glob(os.path.join(dataset_path, '*'))\nsub_dir","42ba8c91":"\ndef display_img(img_path):\n    img = cv2.imread(img_path)\n    #TODO: colour correct the image\n    color_corrected = None\n\n    plt.imshow(color_corrected)\n    plt.title(img_path)\n    plt.show()","f4893a40":"img_dir = sub_dir[0]\n\nimg_files = listdir(img_dir)\nimg_files[:3]\n","dcc92ace":"display_img(img_path=os.path.join(img_dir, img_files[0]))","3226c766":"img_dir = sub_dir[4]\n\nimg_files = listdir(img_dir)\nimg_files[:3]\n\n#TODO: Can you display the names of the 5th to 10th image files?\n","42bb13a8":"display_img(img_path=os.path.join(img_dir, img_files[0]))","b3a19d43":"\ndef get_gsimg(image_path):\n    img = cv2.imread(image_path)\n    #TODO: resize the image to 64 x 64 and extract the greyscale values of the given image\n    resize_img = None\n    gs_img = None\n    return gs_img\n","b3c20077":"gs_img = get_gsimg(os.path.join(img_dir, img_files[0]))\n# the shape of the image array should be (64, 64)\ngs_img.shape\n","1ec0ceeb":"plt.imshow(gs_img, cmap='gray')\nplt.show()","7fc0aa65":"def extract_array(dataset_path):\n    label_dirs = glob.glob(os.path.join(dataset_path, '*'))\n    num_classes = len(label_dirs)\n    X = []\n    Y = []\n    for label_path in label_dirs:\n        label = int(str.split(label_path, '\/')[-1])\n        imgs = glob.glob(os.path.join(label_path, '*.JPG'))\n        for img in imgs:\n            gs_img = get_gsimg(img)\n            X.append(gs_img)\n            Y.append(label)\n    #TODO: normalize the values in X \n    X = np.array(X).astype('float32')\n    #TODO: make the label a one-hot vector for example if the value is 3, then [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n    Y = None\n    return X, Y\n","0bd69583":"X, Y = extract_array(dataset_path)\nprint(X.shape)\n# X shape should be (2062, 64, 64)\nprint(Y.shape)\n# Y shape should be (2062, 10)","7a773606":"# Import libraries\n\nTypical machine learning pipeline\n\nData Processing -> Model -> Training -> Validation -> Prediction\n\nWe first have to view and understand the data, extract and transform it into a form that can be used for machine learning\n\nWe will use \n- opencv to load the images as an array\n- use numpy to manipulate into a large tensor that we can train and validate\n\nThe output will be 2 large matrix X and Y","29b8d8dd":"# Dataset\n\nFirst open the folder sign_lang_dataset and explore the input images. It gives you a sense of what the images look like and how it is organized\n\nThe sign language images are for the digits 0 to 9 and the images are organized in directory. The directories are labeled with the digit of the images.\n\nLet's first explore the dataset located into sign_lang_dataset\/Dataset","9a9227ee":"## Save the arrays to files X.npy and Y.npy\n\nThe following section will not run because we cannot write to the file system.\nIf you want to run this, you will have to download this and run it as a notebook in your laptop","7123598d":"array_path = os.path.join(dataset_path, '..', 'Arrays')\ntry:\n    os.mkdir(array_path)\nexcept FileExistsError:\n    print(array_path + ' already exist.')\n    \nnp.save(os.path.join(array_path, 'X.npy'), X)\nnp.save(os.path.join(array_path, 'Y.npy'), Y)","b3714229":"Look at python list operations \n\n[Python list operations](https:\/\/www.tutorialspoint.com\/python\/python_lists_data_structure.htm)","fb238c8e":"## View sample images\n\nOpenCV by default loads images using BGR causing a strange tinge when displayed in notebook.\n\nYou need to adjust the colour mapping.\n\nLook at cv2.cvtColor. We want to convert from BGR to RGB\n\n[OpenCV colorspaces](https:\/\/docs.opencv.org\/3.2.0\/df\/d9d\/tutorial_py_colorspaces.html)\n","87e1c1de":"## Extract the images array and also constract the label array\n\nThe images is to be extracted as multi-dimensional array\n\nFor example if there are 2 images, we should end up having an 3 dimensional array that looks like this\n\n\\[\n\\[\\[64\\], \\[64\\]\\],\n\\[\\[64\\], \\[64\\]\\]\n\\]\n\nTo normalize X, remember GRAYSCALE images contain pixels and each pixel value is between 0 and 255. How would you normalize it.\n\nTo convert to one-hot vector, there's a convenient function already imported to do that. Look in the imports.\n\n","0bafa3a9":"## Extract images as array\n\nIn order to train the model with the images, we need to extract the pixel values as an array\nTo make things simpler and manageable, we will resize the image to 64 x 64 and only use grayscale values \nso that we only have 1 channel to deal with\n\nUse the same function for converting colour spaces as we did earlier for converting from BGR to RBG. This time we want to resize and then convert from BGR to GRAY.\n\nFor resize please look at \n[cv2 resize](https:\/\/docs.opencv.org\/3.4.3\/da\/d6e\/tutorial_py_geometric_transformations.html)\nUse the function that takes the image and a tuple (W, H)"}}