{"cell_type":{"5345b8a5":"code","de289b2a":"code","d3c130dc":"code","2748d1e6":"code","87b35d7c":"code","31735266":"code","f0ca8104":"code","8309ff91":"code","f749028d":"code","c427de6a":"code","05a1c308":"code","265284a8":"code","053c7bcf":"code","48278ad4":"code","6fc50d30":"code","64af1320":"code","71f1fe8d":"code","4158561b":"code","ba0dd018":"code","3d62f7c9":"code","858dd460":"code","adc4685d":"code","f0e71a43":"code","5b604f91":"code","7642afc9":"code","6be8402d":"code","458c58f8":"code","7c401085":"code","45644743":"code","ec1dd110":"code","65d44d38":"code","a83b1092":"code","ef395d74":"code","5a38b9f8":"code","274ccecd":"code","98c6a34d":"code","2061670b":"code","0efe2c26":"code","ea68b488":"code","3c3b859f":"markdown","a8eb2b71":"markdown","f52fd645":"markdown","3f30cd87":"markdown","86822eab":"markdown","a31592b3":"markdown","c069bb52":"markdown","de11453c":"markdown","082c290e":"markdown","7565fc22":"markdown","b3c94694":"markdown","e1e61ced":"markdown","8dbb023f":"markdown","9cff2b84":"markdown","3dd2d479":"markdown","4e546223":"markdown","760dcd8e":"markdown","0e54a8b9":"markdown","0c70f97b":"markdown","909db6f4":"markdown","7eed7699":"markdown","f9390a4a":"markdown","c8025ec1":"markdown","ae3510ae":"markdown","3d62c85d":"markdown","5d3c6a1f":"markdown","8fdea0de":"markdown","c4aab976":"markdown","1b446182":"markdown","a1666041":"markdown","7368a27d":"markdown","873c20d7":"markdown","6d52e13d":"markdown","7aa49ccd":"markdown","be44b177":"markdown","0f8458d6":"markdown","f6891985":"markdown","cbfe601c":"markdown","77998626":"markdown","193ad4ff":"markdown","a076eb33":"markdown","e566cf74":"markdown","d2999a0e":"markdown","ed3a8cee":"markdown","d56ab26c":"markdown","2b44595b":"markdown","85205d86":"markdown","5f006432":"markdown"},"source":{"5345b8a5":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set(style = \"whitegrid\")\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nfrom sklearn import svm\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestClassifier\n\n%matplotlib inline","de289b2a":"data = pd.read_csv(\"..\/input\/FIFA 2018 Statistics.csv\")\ndata.shape","d3c130dc":"data.head(2)","2748d1e6":"gc = np.array([], dtype = 'int')\nfor i in range(0, 128, 2):\n    gc = np.append(gc, data.loc[i+1, \"Goal Scored\"])\n    gc = np.append(gc, data.loc[i, \"Goal Scored\"])\n    \ndata.insert(4, \"Goal Conceded\", pd.Series(gc))","87b35d7c":"data.head(2)","31735266":"conditions = [(data[\"Goal Scored\"] > data[\"Goal Conceded\"]), (data[\"Goal Scored\"] == data[\"Goal Conceded\"]), (data[\"Goal Scored\"] < data[\"Goal Conceded\"])]\nresult = np.array([0, 1, 2], dtype = 'int')\ndata.insert(5, \"Result\", pd.Series(np.select(conditions, result, default = -1)))","f0ca8104":"data.head(2)","8309ff91":"data.insert(15, \"Total Set Pieces\", pd.Series(data[\"Corners\"] + data[\"Free Kicks\"], dtype = 'int'))\n","f749028d":"data.head(2)","c427de6a":"sns.lmplot(x = 'Ball Possession %', y = 'Attempts', data = data)","05a1c308":"data[\"Ball Possession %\"].corr(data[\"Attempts\"], method = 'pearson')","265284a8":"sns.lmplot(x = 'Passes', y = 'Distance Covered (Kms)', data = data)","053c7bcf":"data[\"Passes\"].corr(data[\"Distance Covered (Kms)\"], method = 'pearson')","48278ad4":"sns.set_context(\"paper\")\nsns.swarmplot( x = 'Result', y = 'Total Set Pieces', data = data)","6fc50d30":"data.isna().sum()","64af1320":"data[['Own goals', 'Own goal Time']].head()","71f1fe8d":"data[['Own goals', 'Own goal Time']] = data[['Own goals', 'Own goal Time']].fillna(0)\ndata.isna().sum()","4158561b":"data[\"1st Goal\"].head()","ba0dd018":"data[\"1st Goal\"] = data[\"1st Goal\"].fillna(90)\ndata.isna().sum()","3d62f7c9":"data.dtypes.unique()","858dd460":"cat = data.columns.values[data.dtypes == object]\ncat","adc4685d":"data.drop([\"Date\"], axis = 1, inplace = True)\n\ntemp = np.array(['Man of the Match', 'Round', 'PSO'])\nfor i in range(0,len(temp)):\n    x = temp[i]\n    data[x] = data[x].astype('category').cat.codes\n    \ndata.head(2)","f0e71a43":"# I am currently working on the above mentioned approach.\n# This is a temporary solution.\n\ndata[\"Team\"] = data[\"Team\"].astype('category').cat.codes\ndata[\"Opponent\"] = data[\"Opponent\"].astype('category').cat.codes","5b604f91":"data.head()","7642afc9":"features = data.drop([\"Man of the Match\"], axis = 1)\ntarget = data[\"Man of the Match\"]","6be8402d":"modelxgb = XGBClassifier()\nmodelxgb.fit(features, target)\n\nprint(modelxgb.feature_importances_)","458c58f8":"from xgboost import plot_importance\nplot_importance(modelxgb)","7c401085":"f_xgb = pd.DataFrame(data = {'feature' : features.columns, 'value' : modelxgb.feature_importances_})\nf_xgb = f_xgb.sort_values(['value'], ascending = False)\ntop10xgb = f_xgb.head(10)\nplt.figure(figsize=(15,8))\nsns.barplot(x = top10xgb[\"feature\"], y = top10xgb[\"value\"])","45644743":"modellgbm = LGBMClassifier()\nmodellgbm.fit(features, target)\n\nprint(modellgbm.feature_importances_)","ec1dd110":"f_lgbm = pd.DataFrame(data = {'feature' : features.columns, 'value' : modellgbm.feature_importances_})\nf_lgbm = f_lgbm.sort_values(['value'], ascending = False)\ntop10lgbm = f_lgbm.head(10)\nplt.figure(figsize=(15,8))\nsns.barplot(x = top10lgbm[\"feature\"], y = top10lgbm[\"value\"])","65d44d38":"modeletc = ExtraTreesClassifier()\nmodeletc.fit(features, target)\n\nprint(modeletc.feature_importances_)","a83b1092":"f_etc = pd.DataFrame(data = {'feature' : features.columns, 'value' : modeletc.feature_importances_})\nf_etc = f_etc.sort_values(['value'], ascending = False)\ntop10etc = f_etc.head(10)\nplt.figure(figsize=(15,8))\nsns.barplot(x = top10etc[\"feature\"], y = top10etc[\"value\"])","ef395d74":"ft = pd.merge(f_xgb, f_lgbm, how = 'inner', on = [\"feature\"])\nft = pd.merge(ft, f_etc, how = 'inner', on = [\"feature\"])","5a38b9f8":"ft.head(5)","274ccecd":"features = ft[\"feature\"].head(5).values\nX = data[features]\nY = target.values","98c6a34d":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state = 7)","2061670b":"modelsvc = svm.SVC(kernel = 'rbf', gamma='auto')\nmodelsvc.fit(X_train, y_train)\ny_svc = modelsvc.predict(X_test)\naccuracy_score(y_test, y_svc)","0efe2c26":"modelreg = linear_model.LogisticRegression()\nmodelreg.fit(X_train, y_train)\ny_reg = modelreg.predict(X_test)\naccuracy_score(y_test, y_reg.round(), normalize = False)","ea68b488":"modelrf = RandomForestClassifier(max_depth=2, random_state=0)\nmodelrf.fit(X_train, y_train)\ny_rf = modelrf.predict(X_test)\naccuracy_score(y_test, y_rf)","3c3b859f":"Creating a feature of who won the match using goals scored and goals conceded. <br> \nAttributes: Won, Lost, Draw (0, 1, 2) to keep no. of categories low ","a8eb2b71":"Plot of Ball Possession vs Attempts. ","f52fd645":"> # Dealing with Categorical Variables","3f30cd87":"This again is an corroborating conclusion. <br>\nIt was believed that this was the world cup of set pieces. <br> \nWe can observe that more number of teams tend to win given equal number of set pieces. (0 and 1 on x-axis) <br>","86822eab":"**The 2 new features that were created in the begining are prominent for both, XGBoost and LGBM** <br>\n**Infact, feature *\"Result\"*(as expected) is the most prominent and feature *\"Total Set Pieces\"* is also dominating**","a31592b3":"# **Goal: Classify weather or not a team player wins MotM Award.**","c069bb52":"**Index:** <br>\n<br>\n**1. Exploratory Data Analysis <br>**\n        a. Feature Engineering <br>\n        b. Graphs <br>\n\n**2. Data Organising** <br>\n        a. Dealing with Null Values <br>\n        b. Dealing with Categorical Variables <br>\n    \n**3. Feature Importance Calculation** <br>\n        a. XGBoost <br>\n        b. LGBM <br>\n        c. Extra Trees Classifier <br>\n    \n** 4. Training and Classification** <br>\n        a. Support Vector Machine (RBF Kernel) <br>\n        b. Linear Regression <br>\n        c. Random Forests Classifier <br>","de11453c":"Plot of Distance Covered vs Passes. ","082c290e":"Out of these features: <br>\n1. Date can be dropped. <br>\n2. Team and Opponent contain names of countries. <br>\n3. Man of the Match and PSO have values in Yes or No. <br>\n4. Round has 6 different values. <br>\n\nI'll deal with Team and Opponent separately.","7565fc22":"We obtain 84% accuracy using Random Forests Classifier. ","b3c94694":"**I'll calculate feature importance using gradient boosting method.** <br>\n1. XGBoost Classifier\n2. Gradient Boosting Machince\n3. Extra Trees Classifier <br>\nAfter calculating feature importance using various algorithms, I'll select top 5 features consistent in each algortihm and finally train the actual model.","e1e61ced":"![https:\/\/imgur.com\/a\/MJAaE7W](http:\/\/)","8dbb023f":"# Data Organising","9cff2b84":"***Therefore, for this case, Random Forests Classifier prove to be the best classifier algorithm.***","3dd2d479":"> # Feature Engineering","4e546223":"# Exploratory Data Analysis","760dcd8e":"We obtain 34% accuracy using Logistic Regression Model. ","0e54a8b9":"*Therefore, it can be observed that different algorithms give different Feature Importance Scores.*","0c70f97b":"**Again, feature *Result* is the most dominant feature.**","909db6f4":"> Merging all the 3 feature importance dataframes, we get a comprehensive account of important features","7eed7699":"For Team and Opponent, I'll first get categorical codes for all the countries and then directly assign that codes to Team and Opponent.","f9390a4a":"XGBoost Classifier","c8025ec1":"*As I have used 2 boosting algorithms, I'll further use Extra Trees Classifier provided by sci-kit learn to obtain feature importance.*","ae3510ae":"Therefore, top 5 features are:\n1. Result\n2. Off-Target\n3. 1st Goal\n4. Corners\n5. Team\n\nNow, I'll use these features to train data and predict the target.","3d62c85d":"Using intersection, I'll select all the features that are prevalent in all the three models.","5d3c6a1f":"> # Graphs","8fdea0de":"These are the matches in which one of the teams were not able to score a single score. <br>\nHere, I replace NaN with full-time minutes as replacing it with 0 will give a different inference. <br>","c4aab976":"It can be observed that there is a linear relation between attempts made and % ball possession. <br>\nMoreover, number of attempts tend to rise with increase in % ball possession. <br>\nCorrelation between % Ball Possession and Attempts: 0.54 (good correlation) <br>","1b446182":"As it can be seen, there were 116\/2 = 58 matches in which either of the team did not score an own goal. <br>\nTherefore, I replace NaN with 0 in both the columns, as it seems to be the most logical idea.","a1666041":"# This is my first attempt to work on a data science and machine learning problem so please share your views and comments regarding this kernel so that I can improve my work. <br>\n# A big thank you for taking out time and efforts to read through the entire kernel.","7368a27d":"There are features with Object, Integer and Float datatypes. <br>\nWe need to deal with features having Object datatype. <br>","873c20d7":"# Feature Importance Calculation","6d52e13d":"> # Random Forests Classifier","7aa49ccd":"Comparison between number of set pieces for wins, losses and draws.","be44b177":"Creating a feature of total set pieces (free kicks + corners) <br>\n(penalties ignored as information is not provided) ","0f8458d6":"> # Logistic Regression","f6891985":"# Now our data is clean and ready to train.","cbfe601c":"> # Support Vector Machine (Radial Basis Function)","77998626":"Plot of F Scores (Feature Importance Scores) vs Features","193ad4ff":"Therefore, Date is dropped. Man of the Match, Round and PSO is converted to categorical variables.","a076eb33":"> # Dealing with Null Values","e566cf74":"Creating a feature of goals conceded by a team in a match using goals scored by the opposition","d2999a0e":"Now there are no Null or NaN values in the dataset.","ed3a8cee":"**I am experimenting with Simple SVM (RBF Kernel), Linear Regression and Random Forests.**","d56ab26c":"# Training and Classification","2b44595b":"We obtain 66% accuracy using SVM.","85205d86":"> # Now, based on these 3 algorithms, I will select top 5 features for training and prediction","5f006432":"This is an interesting result. <br>\nI thought there should be a good (inverse) correlation between passes and distance covered but it is very less.  <br>\nCorrelation between Passes and Distance Covered: 0.18 <br>"}}