{"cell_type":{"7cc650ee":"code","a098b074":"code","2efddaa0":"code","80dc54ec":"code","bc66d3b6":"code","7fb95f3d":"code","c06a80e4":"code","24e5032b":"code","322aa177":"code","e17d745b":"markdown","bb4981d1":"markdown","a3e4fcb3":"markdown","7f73a931":"markdown","70d510ce":"markdown","a27a2f3f":"markdown","210478f0":"markdown","9708a4bf":"markdown","8ea4f6f8":"markdown","a059055c":"markdown","24b82e11":"markdown","78c07c08":"markdown","2d0dad2b":"markdown"},"source":{"7cc650ee":"!pip install transformers[ja]","a098b074":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nto\nimport gensim.downloader\nimport transformers\nfrom wordcloud import WordCloud\n\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv',dtype={'id': np.int16, 'target': np.int8})","2efddaa0":"tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')\n##== whwn you want to treat Japanese character,\n# !pip install transformers[ja]\n#tokenizer = transformers.AutoTokenizer.from_pretrained('cl-tohoku\/bert-base-japanese-whole-word-masking')\ntokenizer(train.text[0])","80dc54ec":"class RNN:\n    def __init__(self, Wx, Wh, b):\n        self.params = [Wx, Wh, b]\n        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n        self.cache = None\n    \n    def forward(self, x, h_prev):\n        Wx, Wh, b = self.params\n        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n        h_next = np.tanh(t)\n        \n        self.cache = (x, h_prev, h_next)\n        return h_next\n    \n    def backward(self, dh_next):\n        Wx, Wh, d = self.params\n        x, h_prev, h_next = self.cache\n        \n        dt = dh_next * (1 - h_next ** 2)\n        db = np.sum(dt, axis=0)\n        dWh = np.dot(h_prev.T, dt)\n        dh_prev = np.dot(x.T, dt)\n        dx = np.dot(dt, Wx.T)\n        \n        self.grads[0][...] = dWx  ## three dots\n        self.grads[1][...] = dWh\n        self.grads[2][...] = db\n        \n        return dx, dh_prev","bc66d3b6":"class TimeRNN:\n    def __init__(self, Wx, Wh, b, stateful=False):\n        self.params = [Wx, Wh, b]\n        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n        self.layers = None\n\n        self.h, self.dh = None, None\n        self.stateful = stateful\n\n    def forward(self, xs):\n        Wx, Wh, b = self.params\n        N, T, D = xs.shape\n        D, H = Wx.shape\n\n        self.layers = []\n        hs = np.empty((N, T, H), dtype='f')\n\n        if not self.stateful or self.h is None:\n            self.h = np.zeros((N, H), dtype='f')\n\n        for t in range(T):\n            layer = RNN(*self.params)  ## <= RNN\u3092T\u500b\u751f\u6210\u3057\u3066\u3044\u308b\u3002\n            self.h = layer.forward(xs[:, t, :], self.h)\n            hs[:, t, :] = self.h\n            self.layers.append(layer)\n\n        return hs\n    \n    def backward(self, dhs):\n        Wx, Wh, b = self.params\n        N, T, H = dhs.shape\n        D, H = Wx.shape\n\n        dxs = np.empty((N, T, D), dtype='f')\n        dh = 0\n        grads = [0, 0, 0]\n        for t in reversed(range(T)):\n            layer = self.layers[t]\n            dx, dh = layer.backward(dhs[:, t, :] + dh)\n            dxs[:, t, :] = dx\n\n            for i, grad in enumerate(layer.grads):\n                grads[i] += grad\n\n        for i, grad in enumerate(grads):\n            self.grads[i][...] = grad\n        self.dh = dh\n\n        return dxs\n\n    def set_state(self, h):\n        self.h = h\n\n    def reset_state(self):\n        self.h = None","7fb95f3d":"class SimpleRnnlm:\n    def __init__(self, vocab_size, wordvec_size, hidden_size):\n        V, D, H = vocab_size, wordvec_size, hidden_size\n        rn = np.random.randn\n\n        # \u91cd\u307f\u306e\u521d\u671f\u5316\n        embed_W = (rn(V, D) \/ 100).astype('f')\n        rnn_Wx = (rn(D, H) \/ np.sqrt(D)).astype('f')\n        rnn_Wh = (rn(H, H) \/ np.sqrt(H)).astype('f')\n        rnn_b = np.zeros(H).astype('f')\n        affine_W = (rn(H, V) \/ np.sqrt(H)).astype('f')\n        affine_b = np.zeros(V).astype('f')\n\n        # \u30ec\u30a4\u30e4\u306e\u751f\u6210\n        self.layers = [\n            TimeEmbedding(embed_W),\n            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n            TimeAffine(affine_W, affine_b)\n        ]\n        self.loss_layer = TimeSoftmaxWithLoss()\n        self.rnn_layer = self.layers[1]\n\n        # \u3059\u3079\u3066\u306e\u91cd\u307f\u3068\u52fe\u914d\u3092\u30ea\u30b9\u30c8\u306b\u307e\u3068\u3081\u308b\n        self.params, self.grads = [], []\n        for layer in self.layers:\n            self.params += layer.params\n            self.grads += layer.grads\n\n    def forward(self, xs, ts):\n        for layer in self.layers:\n            xs = layer.forward(xs)\n        loss = self.loss_layer.forward(xs, ts)\n        return loss\n\n    def backward(self, dout=1):\n        dout = self.loss_layer.backward(dout)\n        for layer in reversed(self.layers):\n            dout = layer.backward(dout)\n        return dout\n\n    def reset_state(self):\n        self.rnn_layer.reset_state()\n        \nclass TimeEmbedding:\n    def __init__(self, W):\n        self.params = [W]\n        self.grads = [np.zeros_like(W)]\n        self.layers = None\n        self.W = W\n\n    def forward(self, xs):\n        N, T = xs.shape\n        V, D = self.W.shape\n\n        out = np.empty((N, T, D), dtype='f')\n        self.layers = []\n\n        for t in range(T):\n            layer = Embedding(self.W)\n            out[:, t, :] = layer.forward(xs[:, t])\n            self.layers.append(layer)\n\n        return out\n\n    def backward(self, dout):\n        N, T, D = dout.shape\n\n        grad = 0\n        for t in range(T):\n            layer = self.layers[t]\n            layer.backward(dout[:, t, :])\n            grad += layer.grads[0]\n\n        self.grads[0][...] = grad\n        return None\n\nclass TimeAffine:\n    def __init__(self, W, b):\n        self.params = [W, b]\n        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n        self.x = None\n\n    def forward(self, x):\n        N, T, D = x.shape\n        W, b = self.params\n\n        rx = x.reshape(N*T, -1)\n        out = np.dot(rx, W) + b\n        self.x = x\n        return out.reshape(N, T, -1)\n\n    def backward(self, dout):\n        x = self.x\n        N, T, D = x.shape\n        W, b = self.params\n\n        dout = dout.reshape(N*T, -1)\n        rx = x.reshape(N*T, -1)\n\n        db = np.sum(dout, axis=0)\n        dW = np.dot(rx.T, dout)\n        dx = np.dot(dout, W.T)\n        dx = dx.reshape(*x.shape)\n\n        self.grads[0][...] = dW\n        self.grads[1][...] = db\n\n        return dx\n    \nclass TimeSoftmaxWithLoss:\n    def __init__(self):\n        self.params, self.grads = [], []\n        self.cache = None\n        self.ignore_label = -1\n\n    def forward(self, xs, ts):\n        N, T, V = xs.shape\n\n        if ts.ndim == 3:  # \u6559\u5e2b\u30e9\u30d9\u30eb\u304cone-hot\u30d9\u30af\u30c8\u30eb\u306e\u5834\u5408\n            ts = ts.argmax(axis=2)\n\n        mask = (ts != self.ignore_label)\n\n        # \u30d0\u30c3\u30c1\u5206\u3068\u6642\u7cfb\u5217\u5206\u3092\u307e\u3068\u3081\u308b\uff08reshape\uff09\n        xs = xs.reshape(N * T, V)\n        ts = ts.reshape(N * T)\n        mask = mask.reshape(N * T)\n\n        ys = softmax(xs)\n        ls = np.log(ys[np.arange(N * T), ts])\n        ls *= mask  # ignore_label\u306b\u8a72\u5f53\u3059\u308b\u30c7\u30fc\u30bf\u306f\u640d\u5931\u30920\u306b\u3059\u308b\n        loss = -np.sum(ls)\n        loss \/= mask.sum()\n\n        self.cache = (ts, ys, mask, (N, T, V))\n        return loss\n\n    def backward(self, dout=1):\n        ts, ys, mask, (N, T, V) = self.cache\n\n        dx = ys\n        dx[np.arange(N * T), ts] -= 1\n        dx *= dout\n        dx \/= mask.sum()\n        dx *= mask[:, np.newaxis]  # ignore_label\u306b\u8a72\u5f53\u3059\u308b\u30c7\u30fc\u30bf\u306f\u52fe\u914d\u30920\u306b\u3059\u308b\n\n        dx = dx.reshape((N, T, V))\n\n        return dx","c06a80e4":"class LSTM:\n    def __init__(self, Wx, Wh, b):\n\n        self.params = [Wx, Wh, b]\n        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n        self.cache = None\n\n    def forward(self, x, h_prev, c_prev):\n        Wx, Wh, b = self.params\n        N, H = h_prev.shape\n\n        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n\n        f = A[:, :H]\n        g = A[:, H:2*H]\n        i = A[:, 2*H:3*H]\n        o = A[:, 3*H:]\n\n        f = sigmoid(f)\n        g = np.tanh(g)\n        i = sigmoid(i)\n        o = sigmoid(o)\n\n        c_next = f * c_prev + g * i\n        h_next = o * np.tanh(c_next)\n\n        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n        return h_next, c_next\n\n    def backward(self, dh_next, dc_next):\n        Wx, Wh, b = self.params\n        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n\n        tanh_c_next = np.tanh(c_next)\n\n        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n\n        dc_prev = ds * f\n\n        di = ds * g\n        df = ds * c_prev\n        do = dh_next * tanh_c_next\n        dg = ds * i\n\n        di *= i * (1 - i)\n        df *= f * (1 - f)\n        do *= o * (1 - o)\n        dg *= (1 - g ** 2)\n\n        dA = np.hstack((df, dg, di, do))\n\n        dWh = np.dot(h_prev.T, dA)\n        dWx = np.dot(x.T, dA)\n        db = dA.sum(axis=0)\n\n        self.grads[0][...] = dWx\n        self.grads[1][...] = dWh\n        self.grads[2][...] = db\n\n        dx = np.dot(dA, Wx.T)\n        dh_prev = np.dot(dA, Wh.T)\n\n        return dx, dh_prev, dc_prev","24e5032b":"words = [ w for t in train[\"text\"].str.split().tolist() for w in t]","322aa177":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(words[:200]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","e17d745b":"#### 0.1\u3000\u6700\u65b0\u7248\u306e\u65e5\u672c\u8a9e\u5bfe\u5fdctransformer \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n\u73fe\u5728\u306etransformers\u306fmecab\u3067\u306f\u306a\u304ffugashi\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\uff08fugashi\u306fMeCab\u306e\u30e9\u30c3\u30d1\u30fc\uff09<br>\n\u4ee5\u4e0b\u3092\u5b9f\u884c\u3059\u308b\u3060\u3051\u3067\u3001fugashi\u3082\u8f9e\u66f8\u3082\u5168\u90e8\u5165\u308a\u307e\u3059\u3002<br>\n\u6614\u306ffugashi\u3092\u5225\u9014install\u3057\u3066\u3044\u305f\u3002<br>\n\nhttps:\/\/qiita.com\/m__k\/items\/863013dbe847dc613844","bb4981d1":"## **6. Visualization**\n\n#### **6.1 word cloud**","a3e4fcb3":"#### 3.\uff12 Time RNN one layer\n\nRNN\u3092\u6a2a\u306bT\u500b\u4e26\u3079\u305f\u3082\u306e\u3002<br>\nRNN\u3067\u4e00\u3064\u306e\u30bb\u30f3\u30c6\u30f3\u30b9\u9577\u3092\u53d6\u308a\u6271\u3044\u3001\u6a2a\u306bT\u500b\u4e26\u3079\u308b\u4e8b\u3067\u6642\u9593\u65b9\u5411\u306e\u89e3\u50cf\u5ea6\u3092\u4f5c\u308a\u51fa\u3057\u3066\u3044\u308b\u3002<br>\n\n#### \u5f0f\u306e\u8aac\u660e\n<br>\n\n##### **forward\u306b\u3064\u3044\u3066**\n<br>\n<img src=\"https:\/\/camo.qiitausercontent.com\/3d6925879a4d36d8cd7562a9adf75d76f43f281e\/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f65363761373333612d623134662d373232362d646262382d3061393135396663396434312e706e67\" width=\"600\">\n<br><br>\n\n##### **backword\u306b\u3064\u3044\u3066**\n<br>\n<img src=\"https:\/\/camo.qiitausercontent.com\/78da7c43ce5218b6eb574f29e3ffb574f977b92e\/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f39383436336136652d313534392d356264322d373530312d6635353066346538303932302e706e67\" width=\"400\">\n<br><br>\n","7f73a931":"## Table of Contents:<br>\n1. [refernce](https:\/\/www.kaggle.com\/tfukuda675\/lnp-beginner#1.-reference)<br>\n2. [Unique process of NLP](https:\/\/www.kaggle.com\/tfukuda675\/lnp-beginner#2.-Unique-process-of-NLP)<br>\n 2.1 Tokenization\n 2.2 Padding\n3. [RNN](https:\/\/www.kaggle.com\/tfukuda675\/lnp-beginner#3.-RNN)<br>\n 3.1 simple code<br>\n4. LSTM<br>\n 4.1 simple code<br>\n5. Transformer<br>\n 5.1 base line<br>","70d510ce":"## **4. LSTM**\n\n1\u3064\u306eLSTM\u306e\u4e2d\u306b\u3001forget agte\u3068\u3001input gate\u3068state\u3092\u8a18\u61b6\u3059\u308bcell\u3068\u3001output\u30b2\u30fc\u30c8\u304c<br>\n\u5165\u529b\u5074\u304b\u3089\u4e26\u5217\u306b\u3001\u305d\u308c\u305e\u308c\u306eoutput\u304c\u76f4\u5217\u306b\u3064\u306a\u304c\u3063\u3066\u3044\u308b\u3002<br>\n\u3053\u306e\u69cb\u9020\u304c\u308f\u304b\u308c\u3070\u3001\u305d\u308c\u305e\u308c\u306e\u30d1\u30fc\u30c4\u306f\u7c21\u5358\u3002<br>\n\n#### 4.1.1 Confirm \"gradient vanishing problem\"\n\n<br>\n\n#### 4.1.2 Confirm \"gradient exploding problem\"\n\n<br>\n\n#### 4.1.3 diff RNN and LSTM\n\n* RNN<br>\n<img src=\"https:\/\/camo.qiitausercontent.com\/b643944d722e601f9e3d4b7856cd096895f5ce1f\/687474703a2f2f636f6c61682e6769746875622e696f2f706f7374732f323031352d30382d556e6465727374616e64696e672d4c53544d732f696d672f4c53544d332d53696d706c65524e4e2e706e67\" width=500>\n<br><br>\n* LSTM<br>\n<img src=\"https:\/\/camo.qiitausercontent.com\/a12fe62032a6633b05b8ef7c2512d1e54c9c4afd\/687474703a2f2f636f6c61682e6769746875622e696f2f706f7374732f323031352d30382d556e6465727374616e64696e672d4c53544d732f696d672f4c53544d332d636861696e2e706e67\" width=500><br><br>\n\n<br>\n\n#### ** 4.1.4 LSTM Core Idea\nThe key to LSTMs is the cell state, the horizontal line running through the top of the diagram.<br>\n<img src=\"http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-C-line.png\" width=500><br><br>\n\n#### **\u30b3\u30fc\u30c9\u306e\u8aac\u660e**\n\nDx4H\u3067\u30014\u304c\u3064\u3044\u3066\u3044\u308b\u7406\u7531\u306f\u3001f\u3068g\u3068i\u3068o\u306eW\u3092\u6a2a\u306b\u3064\u306a\u3052\u3066\u4e00\u6c17\u306b\u6271\u3063\u3066\u3044\u308b\u70ba\u3002\n","a27a2f3f":"#### \uff14.2 LSTM one layer\n\nUnderstand LSTM structure via python code.<br>\n\n#### \u5f0f\u306e\u8aac\u660e\n##### **forwar\u306b\u3064\u3044\u3066** <br>\n<img src=\"https:\/\/camo.qiitausercontent.com\/b9387ed37d1f247f8f679184f8dd2efd1e7a709a\/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f35306164343264652d643433642d333162392d333761652d3637666462313032393939612e706e67\" width=600>\n<br><br>\n\n##### **backward\u306b\u3064\u3044\u3066** <br>\n<img src=\"https:\/\/camo.qiitausercontent.com\/bd2fad9749a2dd322a1dd9f8dcfbcf3ee209107a\/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f62366631386438372d613436662d643762302d303435382d3733656163373434663465662e706e67\" width=600>\n<br><br>\n","210478f0":"## **5. Transformer**\n","9708a4bf":"## **1. reference**\n\n#### This kernel includes codes and ideas from kernels below.\n* [NLP with Disaster Tweets - EDA, Cleaning and BERT](https:\/\/www.kaggle.com\/tfukuda675\/nlp-with-disaster-tweets-eda-cleaning-and-bert\/edit\/run\/78390965) by [gunesevitan](https:\/\/www.kaggle.com\/gunesevitan)\n* [n-Depth Guide \ud83d\udcd9 to Google's BERT](https:\/\/www.kaggle.com\/ratan123\/in-depth-guide-to-google-s-bert) by [ratan rohith]\n(https:\/\/www.kaggle.com\/ratan123)\n* [Introduction to Japanese spaCy\/GINZA [\u65e5\u672c\u8a9e\/Eng]](https:\/\/www.kaggle.com\/marutama\/introduction-to-japanese-spacy-ginza-eng) by [NIWASHI](https:\/\/www.kaggle.com\/marutama)\n* <a href=\"https:\/\/jp.mathworks.com\/content\/dam\/mathworks\/mathworks-dot-com\/company\/events\/webinar-cta\/2514077_JP_2018-09-07_Deep_Learning_LSTM_1.pdf#page=9\">Structure of RNN<\/a>\n* [\u6df1\u5c64\u5b66\u7fd2\uff0f\u30bc\u30ed\u304b\u3089\u4f5c\u308bDeep Learning\uff12\u3000\u7b2c\uff15\u7ae0\u30e1\u30e2](https:\/\/qiita.com\/jun40vn\/items\/35f6f0d26f9e58f01e4e)\n* [\u6df1\u5c64\u5b66\u7fd2\uff0f\u30bc\u30ed\u304b\u3089\u4f5c\u308bDeep Learning\uff12\u3000\u7b2c\uff16\u7ae0\u30e1\u30e2](https:\/\/qiita.com\/jun40vn\/items\/e690dfe80faa6512049f)\n* [\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u5fc5\u9808\u77e5\u8b58 Transformer \u3092\u5fb9\u5e95\u89e3\u8aac\uff01](https:\/\/deepsquare.jp\/2020\/07\/transformer\/)\n* [LSTM\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6982\u8981](https:\/\/qiita.com\/KojiOhki\/items\/89cd7b69a8a6239d67ca)\n* [colah\u6c0f\u306e\u89e3\u8aac Understanding LSTM](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/)\n* [\u753b\u50cf\u7528Transformer\u3092\u5229\u7528\u3057\u3066\u885b\u661f\u753b\u50cf\u306e\u5206\u985e\u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b](https:\/\/sorabatake.jp\/20454\/) @\u7a7a\u7551\n* [PyTorch\u3067\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3067\u3088\u304f\u4f7f\u7528\u3055\u308c\u308bTransformer\u3092\u5c11\u306a\u3044\u30b3\u30fc\u30c9\u3067\u4f7f\u7528\u3057\u3066\u307f\u308b](https:\/\/www.yurui-deep-learning.com\/2021\/01\/07\/pytorch-transformer\/)\n* [LSTM\u306b\u3088\u308b\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u4e88\u6e2c\u3068\u5206\u985e](https:\/\/jp.mathworks.com\/content\/dam\/mathworks\/mathworks-dot-com\/company\/events\/webinar-cta\/2514077_JP_2018-09-07_Deep_Learning_LSTM_1.pdf) @mathworks\n* [RNN\u306e\u554f\u984c\u70b9](https:\/\/www.anarchive-beta.com\/entry\/2021\/01\/07\/180000)\n* [\u4f5c\u3063\u3066\u7406\u89e3\u3059\u308b Transformer \/ Attention](https:\/\/qiita.com\/halhorn\/items\/c91497522be27bde17ce)\n* [AI\u754c\u3092\u5e2d\u5dfb\u3059\u308b\u300cTransformer\u300d\u3092\u3086\u3063\u304f\u308a\u89e3\u8aac](https:\/\/zenn.dev\/attentionplease\/articles\/1a01887b783494)\n* [\u8ad6\u6587\u89e3\u8aac Attention Is All You Need (Transformer)](https:\/\/deeplearning.hatenablog.com\/entry\/transformer)\n* [\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u5de8\u7363\u300cTransformer\u300d\u306eSelf-Attention Layer\u7d39\u4ecb](https:\/\/medium.com\/lsc-psd\/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E3%81%AE%E5%B7%A8%E7%8D%A3-transformer-%E3%81%AEself-attention-layer%E7%B4%B9%E4%BB%8B-a04dc999efc5)\n* [Transformers Explained Visually (Part 3): Multi-head Attention, deep dive](https:\/\/towardsdatascience.com\/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)\n* [\u306f\u3058\u3081\u3066\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406](https:\/\/www.ogis-ri.co.jp\/otc\/hiroba\/technical\/similar-document-search\/)\n\n<br><br>\n#### \u5404Layer\u306e\u8aac\u660e\n* [positional encoding](https:\/\/qiita.com\/halhorn\/items\/c91497522be27bde17ce#positional-encoding)\n* [Feed Forward Net](https:\/\/www.hellocybernetics.tech\/entry\/2016\/05\/22\/014656)\n  \u96a0\u308c\u5c641\u5c64\u306e\u5168\u7d50\u5408NN\n* [Batch Normalize](https:\/\/qiita.com\/t-tkd3a\/items\/14950dbf55f7a3095600#%E5%85%A8%E7%B5%90%E5%90%88nn-%E3%81%AE-batch-normalization)\n* [Layer Normalization](https:\/\/data-analytics.fun\/2020\/07\/16\/understanding-layer-normalization\/)\n* [gMLP](https:\/\/deepsquare.jp\/2021\/05\/gmlp\/)\n\n<br><br>\n#### \u3088\u304f\u5fd8\u308c\u308b\u5358\u8a9e\u307e\u3068\u3081\n* [\u30ce\u30eb\u30e0](https:\/\/manabitimes.jp\/math\/1269)\n* [\u30a2\u30de\u30c0\u30fc\u30eb\u7a4d](https:\/\/python.atelierkobato.com\/hadamard\/)<br>\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/0866ce505a5888cd4db8e6ba2b710fe8cd5a6578\" width=\"500\">\n* [\u30a2\u30d5\u30a3\u30f3\u5909\u63db](https:\/\/qiita.com\/koshian2\/items\/c133e2e10c261b8646bf)<br>\n\u5168\u7d50\u5408\u5c64\u306b\u3088\u308b\u5909\u63db\u306e\u4e8b\u3002\u5e7e\u4f55\u5b66\u306e\u5206\u91ce\u3067\u306f\u30a2\u30d5\u30a3\u30f3\u5909\u63db\u306b\u76f8\u5f53\u3059\u308b\u306e\u3067\u3001\u3053\u3053\u3067\u306f\u30a2\u30d5\u30a3\u30f3\u5909\u63db\u3068\u8aad\u3093\u3067\u3044\u308b\u3002<br>\n* tanh\u3068sigmoid\u306b\u3064\u3044\u3066<br>\ntanh\u306f\u3001\u4f55\u304b\u3057\u3089\u306e\u60c5\u5831\u306e\u51fa\u529b\u306b\u3064\u3044\u3066\u3044\u308b\u3002\u60c5\u5831\u306b\u5bfe\u3059\u308b\u5f37\u5f31\u3068\u89e3\u91c8\u3067\u304d\u308b\u3002<br>\nsigmoid\u306f\u3001\u30c7\u30fc\u30bf\u3092\u3069\u308c\u3060\u3051\u901a\u3059\u304b\u3068\u3044\u3046\u5272\u5408\u3002<br>\n* [Batch Normalization](https:\/\/yaakublog.com\/batch-normalization)<br>\n* [\u8a55\u4fa1\u95a2\u6570\u307e\u3068\u3081](https:\/\/amateur-engineer-blog.com\/machine-learning-metrics\/)<br>\n","8ea4f6f8":"## **3. RNN** \n\n#### 3.1 RNN one layer\n\nUnderstand rnn structure via python code.<br>\n\n#### \u5f0f\u306e\u8aac\u660e\n##### **forwar\u306b\u3064\u3044\u3066**<br>\n<img src=\"https:\/\/camo.qiitausercontent.com\/d0046c189cf724199009651c48433d0097df591a\/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f34303634653037662d623962622d303432392d616339322d6338366233313837616164622e706e67\" width=600>\n<br><br>\n\n##### **backward\u306b\u3064\u3044\u3066**<br>\n\n<img src=\"https:\/\/camo.qiitausercontent.com\/8942bf9b069aece2d1c19a676a089414643a02c8\/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f31396133383030642d336236342d393133652d396165342d3731616664653534356138652e706e67\" width=600>\n\n<br><br>\n#### **\u30b3\u30fc\u30c9\u306e\u8aac\u660e**\nthree dots \u306b\u3064\u3044\u3066\u306f[\u3053\u3061\u3089](https:\/\/note.nkmk.me\/python-numpy-ellipsis\/)<br>","a059055c":"#### 2.2 Padding\n\u5b66\u7fd2\u6642\u306b\u306f\u5165\u529b\u306f\u56fa\u5b9a\u9577\u304c\u671f\u5f85\u3055\u308c\u3066\u3044\u307e\u3059\u3002<br>\n\u30c8\u30fc\u30af\u30f3\u5316\u3057\u305f\u3082\u306e\u306e\u9577\u3055\u3092\u63c3\u3048\u308b\u305f\u3081\u3001padding\u3067\"0\"\u306a\u3069\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002<br>\n<br>\n\npadding\u524d\u306e\u30c8\u30fc\u30af\u30f3\u5316\u6e08\u307f\u30c7\u30fc\u30bf\n```\nraw_inputs = [\n    [711, 632, 71],\n    [73, 8, 3215, 55, 927],\n    [83, 91, 1, 645, 1253, 927],\n]\n```\npadding\u5f8c\n```\n## Result\n[[ 711  632   71    0    0    0]\n [  73    8 3215   55  927    0]\n [  83   91    1  645 1253  927]]\n```\n<br>","24b82e11":"\n## **0. \u306f\u3058\u3081\u306b**\n\n\u65e5\u672c\u8a9e\u306e\u51e6\u7406\u3082\u307e\u3058\u3048\u306a\u304c\u3089\u3001\u30b3\u30f3\u30da\u30c7\u30fc\u30bf\u3092\u4f7f\u3044\u3064\u3064\u81ea\u5206\u7528\u306bNLP\u3092\u5c11\u3057\u305a\u3064\u307e\u3068\u3081\u3066\u3044\u304d\u307e\u3059\u3002<br>\n\u30bc\u30ed\u304b\u3089\u4f5c\u308bDeep Learning\u2015\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u7de8\u3067\u7406\u89e3\u3057\u305f\u3053\u3068\u3082\u307e\u3068\u3081\u3066\u3044\u304d\u307e\u3059\u3002<br>\nRNN\u304b\u3089\u307e\u3068\u3081\u3066\u3044\u304d\u307e\u3059\u304c\u3001\u5c11\u3057\u305a\u3064\u66f8\u304d\u8db3\u3057\u3066\u3044\u304d\u307e\u3059\u3002<br>","78c07c08":"#### **3.3 RNNLM**\n\nTimeRNN\u3000layer\u306b\u52a0\u3048\u3001TimeAffine\u3000layer\u3001 TimeEmbedding layer\u3001TimeSoftmaxWithLoss\u3000layer\u3092\u7d44\u307f\u5408\u308f\u305b\u3066<br>\nRNNLM\u3092\u4f5c\u308b\u3002\n\n\n##### **Structure of RNNLM**\n\n<img src =\"https:\/\/camo.qiitausercontent.com\/ac8303dfae114898ac9d1f752e0b9378ba975784\/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f61346461333038372d623736642d353636322d386539322d3364306131333062643737342e706e67\" width=\"500\">\n<br><br>\n\n##### **Time Embedding**\n\n<img src=\"https:\/\/camo.qiitausercontent.com\/32654fed87f53f2b82b773d50c36e94a97f9bec0\/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f31363033633765302d653532392d346237632d653936312d3039396561653661663433632e706e67\" width=\"500\">\n<br>\nTime Embedding\u30ec\u30a4\u30e4\u306f\u3001xs\u304b\u30891\u5217\u3065\u3064\u30c7\u30fc\u30bf\u3092\u5207\u308a\u51fa\u3057Embedding\u30ec\u30a4\u30e4\u306b\u5165\u529b\u3057\u3066\u3001<br>\n\u305d\u306e\u51fa\u529b\u3092out(N, T, D)\u306b\u6e9c\u3081\u8fbc\u3080\u3001\u3068\u3044\u3046\u52d5\u4f5c\u3092for\u30eb\u30fc\u30d7\u3067T\u56de\u7e70\u308a\u8fd4\u3059\u3068\u3044\u3046\u3082\u306e\u3067\u3059\u3002\n<br><br>\n\n##### **Time Affine Layer**\n\n<img src=\"https:\/\/camo.qiitausercontent.com\/40771ddfdce111906aa55c50a000af060e146803\/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f37393361623630622d336263332d653063332d663166342d3064316436653038333730352e706e67\" width=\"500\">\n<br>\nTime Affine\u30ec\u30a4\u30e4\u306f\u3001Affine\u30ec\u30a4\u30e4\u306e\u5165\u51fa\u529b\u306b\u3001\u6642\u9593\u8ef8\u65b9\u5411\u306eT\u306b\u5bfe\u5fdc\u51fa\u6765\u308b\u3088\u3046\u306b\u305d\u308c\u305e\u308creshape\u3092\u4ed8\u3051\u52a0\u3048\u305f\u3082\u306e\u3067\u3059\u3002<br>\n<br><br>\n\n##### **Time SoftMax WithLoss**\n<img src=\"https:\/\/camo.qiitausercontent.com\/e2d2604218ab475a245c7bae0a160e7f97416108\/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f63616563366363642d373362332d663733612d653230352d3737363838326639346230652e706e67\" width=\"400\">\n<br>\nTime Softmax with Loss \u30ec\u30a4\u30e4\u30fc \u306f\u3001xt,ttxt,tt\u306eSotmax with Loss \u3092T\u500b\u5408\u7b97\u3057\u3066T\u3067\u5272\u308b\u30ec\u30a4\u30e4\u30fc\u3067\u3059\u3002<br>\n\n#### **\u30b3\u30fc\u30c9\u306e\u8aac\u660e**\nthree dots \u306b\u3064\u3044\u3066\u306f[\u3053\u3061\u3089](https:\/\/note.nkmk.me\/python-numpy-ellipsis\/)<br>","2d0dad2b":"## **2. Unique process of NLP**\n\nRNN, LSTM, transformer cannot treat texts directory.<br>\nAt first, we need to run tokernize. This process transform words to ids.<br>\nNext, run padding process. this process add special id to align the length to the longest sentence.<br>\nThose process use language model, BERT etc,.<br>\n<br>\n\u6587\u5b57\u5217\u3092\u305d\u306e\u307e\u307e\u51e6\u7406\u3067\u304d\u306a\u3044\u70ba\u3001\u307e\u305a\u30c8\u30fc\u30af\u30f3\u5316\u3092\u884c\u3044\u307e\u3059\u3002<br>\n\u305d\u306e\u5f8c\u3001\u5404\u30bb\u30f3\u30c6\u30f3\u30b9\u306e\u9577\u3055\u3092\u63c3\u3048\u308b\u70ba\u3001padding\u3092\u884c\u3044\u307e\u3059\u3002<br>\n\u3053\u308c\u3089\u306e\u30d7\u30ed\u30bb\u30b9\u3067\u306f\u3001BERT\u3067\u77e5\u3089\u308c\u308b\u3088\u3046\u306a\u30e2\u30c7\u30eb\u3092\u4f7f\u3044\u307e\u3059\u3002<br>\n\u5f53\u7136\u30e2\u30c7\u30eb\u306e\u5f71\u97ff\u3082\u5927\u304d\u3044\u3067\u3059\u3002<br>\n\u30e2\u30c7\u30eb\u306b\u3064\u3044\u3066\u306f\u5225\u9014\u307e\u3068\u3081\u307e\u3059\u3002<br>\n\n#### 2.1 Tokenization\n\u6587\u5b57\u5217\u3092\u305d\u306e\u307e\u307e\u5b66\u7fd2\u3059\u308b\u4e8b\u306f\u3067\u304d\u307e\u305b\u3093\u3002<br>\n\u5b66\u7fd2\u3067\u304d\u308b\u3088\u3046\u306b\u3001\u6570\u5024\u306b\u7f6e\u304d\u63db\u3048\u308b\u51e6\u7406\u304c\u5fc5\u8981\u3067\u3059\u3002\u305d\u308c\u3092\u30c8\u30fc\u30af\u30f3\u5316(Tokenize)\u3068\u8a00\u3044\u307e\u3059\u3002<br>\n\u4ee5\u4e0b\u306e\u753b\u50cf\u306e\u4e0a2\u3064\u304c\u30c8\u30fc\u30af\u30f3\u5316\u306b\u76f8\u5f53\u3057\u307e\u3059\u3002<br>\n<img src=\"https:\/\/camo.qiitausercontent.com\/51b2ff47f2bb952f38a63b02d056ebfc7dea097d\/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36323435302f35323261393063362d613732352d333731362d346161382d6637626563396430396262622e706e67\" width=600>\n<br>\n\n#### 2.2 Tokenization result of 1st text\n\nShow BERT tokenizer result with transformer bert model.<br>\nThe 'input_ids' result is tokenization text.<br>\n<br>\n\u65e5\u672c\u8a9e\u3092\u6271\u3046\u5834\u5408\u306f\u3001\u65e5\u672c\u8a9e\u306etokenizer\u3092\u5229\u7528\u3059\u308b\u3002<br>"}}