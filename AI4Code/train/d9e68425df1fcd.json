{"cell_type":{"6ed4bebe":"code","f4bf447a":"code","5dab6040":"code","cc57cf25":"code","4e575816":"code","fa79f559":"code","02f89035":"code","c7cb8917":"code","3284a68e":"code","97e91121":"code","6e92166e":"code","6ef98a20":"code","cc4540fb":"markdown","ca7ceb96":"markdown","81b399fa":"markdown","1b4c0e3d":"markdown","48dcbd5e":"markdown","bd2e4883":"markdown","f4b6ef50":"markdown","ac5d2ed2":"markdown","2a7d94b5":"markdown","19fba05a":"markdown","c3918b45":"markdown","4ec0c2a0":"markdown","950465d0":"markdown","5a3ae5e0":"markdown"},"source":{"6ed4bebe":"from sklearn.feature_extraction.text import CountVectorizer","f4bf447a":"vect = CountVectorizer()\nvect","5dab6040":"corpus = ['Hi my name is kasana.','I love coding.','Kasana loves reading scripts.']\nX= vect.fit_transform(corpus)\nX # note the dimensions of X(3X9) means 3 rows and 9 columns. ","cc57cf25":"vect.get_feature_names()","4e575816":"X.toarray()","fa79f559":"vect.transform(['hi,whats your name?.']).toarray()","02f89035":"import nltk\nporter = nltk.PorterStemmer()\n[porter.stem(t) for t in vect.get_feature_names()]","c7cb8917":"list(set([porter.stem(t) for t in vect.get_feature_names()]))","3284a68e":"WNlemma = nltk.WordNetLemmatizer()\n[WNlemma.lemmatize(t) for t in list(set([porter.stem(t) for t in vect.get_feature_names()]))]","97e91121":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nprint(lemmatizer.lemmatize(\"cats\"))\nprint(lemmatizer.lemmatize(\"cacti\"))\nprint(lemmatizer.lemmatize(\"geese\"))\nprint(lemmatizer.lemmatize(\"rocks\"))\nprint(lemmatizer.lemmatize(\"python\"))\nprint(lemmatizer.lemmatize(\"better\", pos=\"a\"))\nprint(lemmatizer.lemmatize(\"best\", pos=\"a\"))\nprint(lemmatizer.lemmatize(\"run\"))\nprint(lemmatizer.lemmatize(\"run\",'v'))","6e92166e":"from nltk import word_tokenize, pos_tag\nsentence = \"Kaggle is very good learning platform,Do you agree?\"","6ef98a20":"sen_token = word_tokenize(sentence)\npos_tag(sen_token)","cc4540fb":"### See this is the frequency matrix in the given documents","ca7ceb96":"See the loves has now become love.","81b399fa":"**Part of speech tagging** :Apart from the grammar relations, every word in a sentence is also associated with a part of speech (pos) tag (nouns, verbs, adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence. H ere is a list of all possible pos-tags defined by Pennsylvania university. Following code using NLTK performs pos tagging annotation on input text. (it provides several implementations, the default one is perceptron tagger)","1b4c0e3d":"### Normalization and stemming\nSince the words like love and loves has same meaning so, why not we treat them same?\n","48dcbd5e":"Each term found by the analyzer during the fit is assigned a unique integer index corresponding to a column in the resulting matrix. This interpretation of the columns can be retrieved as follows:","bd2e4883":"# Explaining the NLP terms   <br>\n<div class=\"section\" id=\"the-bag-of-words-representation\">\n<h3>1 The Bag of Words representation<a class=\"headerlink\" href=\"#the-bag-of-words-representation\" title=\"Permalink to this headline\">\u00b6<\/a><\/h3>\n<p>Text Analysis is a major application field for machine learning\nalgorithms. However the raw data, a sequence of symbols cannot be fed\ndirectly to the algorithms themselves as most of them expect numerical\nfeature vectors with a fixed size rather than the raw text documents\nwith variable length.<\/p>\n<p>In order to address this, scikit-learn provides utilities for the most\ncommon ways to extract numerical features from text content, namely:<\/p>\n<ul class=\"simple\">\n<li><strong>tokenizing<\/strong> strings and giving an integer id for each possible token,\nfor instance by using white-spaces and punctuation as token separators.<\/li>\n<li><strong>counting<\/strong> the occurrences of tokens in each document.<\/li>\n<li><strong>normalizing<\/strong> and weighting with diminishing importance tokens that\noccur in the majority of samples \/ documents.<\/li>\n<\/ul>\n<p>In this scheme, features and samples are defined as follows:<\/p>\n<ul class=\"simple\">\n<li>each <strong>individual token occurrence frequency<\/strong> (normalized or not)\nis treated as a <strong>feature<\/strong>.<\/li>\n<li>the vector of all the token frequencies for a given <strong>document<\/strong> is\nconsidered a multivariate <strong>sample<\/strong>.<\/li>\n<\/ul>\n<p>A corpus of documents can thus be represented by a matrix with one row\nper document and one column per token (e.g. word) occurring in the corpus.<\/p>\n<p>We call <strong>vectorization<\/strong> the general process of turning a collection\nof text documents into numerical feature vectors. This specific strategy\n(tokenization, counting and normalization) is called the <strong>Bag of Words<\/strong>\nor \u201cBag of n-grams\u201d representation. Documents are described by word\noccurrences while completely ignoring the relative position information\nof the words in the document.<\/p>\n<\/div>\n<p><a class=\"reference internal\" href=\"generated\/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\" title=\"sklearn.feature_extraction.text.CountVectorizer\"><code class=\"xref py py-class docutils literal\"><span class=\"pre\">CountVectorizer<\/span><\/code><\/a> implements both tokenization and occurrence\ncounting in a single class:<\/p>","f4b6ef50":"Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform method:","ac5d2ed2":"<br>Note the dimensions of X (3X9) means 3 rows and 9 columns. <br>\nas there are three documents and 9 unique words<br>\nSee","2a7d94b5":"## Please upvote!\ni will be keep on updating !","19fba05a":"Now we have total 8 unique features","c3918b45":"<br><p>Let\u2019s use it to tokenize and count the word occurrences of a minimalistic\ncorpus of text documents:<\/p>","4ec0c2a0":"* ![](https:\/\/cdn-images-1.medium.com\/max\/800\/0*V635bzjWK2n1jBsd.png)","950465d0":"# Lemmatization\nA very similar operation to stemming is called lemmatizing. The major difference between these is, as you saw earlier, stemming can often create non-existent words, whereas lemmas are actual words.\n\nSo, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma.\n\nSome times you will wind up with a very similar word, but sometimes, you will wind up with a completely different word. Let's see some examples.","5a3ae5e0":"As you can see there the words are tagged by various parts of speech\n"}}