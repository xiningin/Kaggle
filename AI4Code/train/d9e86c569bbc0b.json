{"cell_type":{"c08c2dac":"code","dce05fe8":"markdown"},"source":{"c08c2dac":"\"\"\"\nScript:  JBR_NLP.py\nPurpose:  Extends and customizes the analyze_book1.py code example from Think Python, 2nd Edition, by Allen Downey (http:\/\/thinkpython2.com)\nhttp:\/\/thinkpython2.com\/code\/analyze_book1.py\n\"\"\"\n\nfrom __future__ import print_function, division\n\nimport random\nimport string\nimport re\nimport os\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\n\n\ndef process_file(filename, wordHist, skip_header):\n    \"\"\"Makes a wordHistogram that contains the words from a file.\n\n    filename: string\n    skip_header: boolean, whether to skip the Gutenberg header\n   \n    returns: map from each word to the number of times it appears.\n    \"\"\"\n    try:\n        fp = open(filename)\n    except:\n        print(\"\\nDOUBLE CHECK OPENING: \", filename)\n\n    if skip_header:\n        skip_header(fp)\n\n    try:    \n        for line in fp:\n            process_line(line, wordHist)\n    except:\n        print(\"\\nDOUBLE CHECK PROCESSING LINES IN\", filename)\n\n    return wordHist\n\n\ndef skip_header(fp):\n    \"\"\"Reads from fp until it finds the line that ends the header\n\n    fp: open file object\n    \"\"\"\n    for line in fp:\n        if line.startswith('*END*THE SMALL PRINT!'):\n            break\n\ndef process_line(line, wordHist):\n    \"\"\"Adds the words in the line to the wordHistogram.\n\n    Modifies wordHist.\n\n    line: string\n    wordHist: wordHistogram (map from word to frequency)\n    \"\"\"\n    strippables = string.punctuation + string.whitespace + string.digits\n\n    for word in line.split():\n        # remove punctuation and convert to lowercase\n        word = word.strip(strippables)\n        word = re.sub(\"\/\", \" \", word)\n        word = word.lower()\n\n        # remove URLs\n        if word.startswith('http') or word.startswith('https') or word.startswith('www.'):\n            continue\n        elif word.endswith('.pdf') or word.endswith('online') or word.endswith('.org'):\n            continue\n        elif '.....' in word:\n            continue\n        elif '-' in word:\n            continue\n        else:\n            # update the wordHistogram with root word\n            wordHist[word] = wordHist.get(word, 0) + 1\n \ndef most_common(wordHist):\n    \"\"\"Makes a list of word-freq pairs in descending order of frequency.\n\n    wordHist: map from word to frequency\n\n    returns: list of (frequency, word) pairs\n    \"\"\"\n    c = []\n    for key, value in wordHist.items():\n        c.append((value, key))\n\n    c.sort()\n    c.reverse()\n    return c\n\ndef print_most_common(wordHist, num=10):\n    \"\"\"Prints the most commons words in a wordHistgram and their frequencies.\n    \n    wordHist: wordHistogram (map from word to frequency)\n    num: number of words to print\n    \"\"\"\n    c = most_common(wordHist)\n    print(\"\\n\")\n    print(num, ' MOST COMMON WORDS:')\n    for freq, word in c[:num]:\n        print(word, '\\t', freq)\n\ndef subtractDict(d1, d2):\n    \"\"\"Returns a dictionary with all keys that appear in d1 but not d2\n\n    d1, d2: dictionaries\n    \"\"\"\n    res = {}\n    for key, value in d1.items():\n        if key not in d2:\n            res[key] = value\n    return res\n\ndef total_words(wordHist):\n    \"\"\"Returns the total of the frequencies in a wordHistogram.\"\"\"\n    return sum(wordHist.values())\n\n\ndef different_words(wordHist):\n    \"\"\"Returns the number of different words in a wordHistogram.\"\"\"\n    return len(wordHist)\n\n\ndef main():\n    lemmatizer = WordNetLemmatizer()\n    grammarHist = {}\n    grammarWords = process_file('..\/input\/jobbulletindata\/JBR_Resources\/grammarWords.txt', grammarHist, skip_header=False)\n    JobBulletins = [os.path.join(root, file) for root, folder, JobBulletin in os.walk('..\/input\/jobbulletindata\/JobBulletins') for file in JobBulletin]\n\n    # get stats for all Job Bulletins\n    complexity = 10\n    wordHist = {}\n    for JobBulletin in JobBulletins:\n        filename = re.sub('[\\']','',JobBulletin)\n        wordHist = process_file(filename, wordHist, skip_header=False)\n    commonWords = most_common(wordHist)\n    meaningWords = subtractDict(wordHist, grammarWords)\n    longWords = [key for key,value in meaningWords.items() if len(key) > complexity]\n    longWords.sort()\n    fo = open(\"longwords.txt\", 'w')\n    print(longWords,file=fo)\n    fo.close()\n\n    fs = open(\"totWords.txt\", 'w')\n    print(total_words(wordHist),file=fs)\n    print(different_words(wordHist),file=fs)\n    fs.close()\n\n    print(\"JOB BULLETINS\", len(JobBulletins))\n    print('\\nTOTAL WORDS:', total_words(wordHist))\n    print('\\nTOTAL DIFFERENT WORDS:', different_words(wordHist))\n    print_most_common(wordHist, 50)\n    print('\\nTOTAL COMPLEX WORDS:', len(longWords))\n    #print('\\nCOMPLEX WORDS:', longWords)\n    \n    # get stats for each Job Bulletin\n    print(\"\\nComputing statistics for each Job Bulletin\")\n    wordHist = {}\n    fileStatsRow = {}\n    fileStats = []\n    totWords = 0\n    difWords = 0\n    totLongWords = 0\n    for JobBulletin in JobBulletins:\n\n        wordHist = {}\n        filename = re.sub('[\\']','',JobBulletin)                            # strip the single quote so the file will open\n        wordHist = process_file(filename, wordHist, skip_header=False)\n        commonWords = most_common(wordHist)\n        longWords = subtractDict(wordHist, grammarWords)\n        longWords = [key for key,value in longWords.items() if len(key) > complexity]\n        longWords.sort()\n\n        totWords += total_words(wordHist)\n        difWords += different_words(wordHist)\n        totLongWords +=  len(longWords)\n\n        fileStatsRow['FILE_NAME'] = filename\n        fileStatsRow['TOT_WORDS']= total_words(wordHist)\n        fileStatsRow['TOT_DIF_WORDS'] = different_words(wordHist)\n        fileStatsRow['TOT_LONG_WORDS'] = len(longWords)\n        fileStatsRow['LONG_WORDS'] = longWords\n        fileStats.append(fileStatsRow)\n        fileStatsRow = {}\n\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    \n    df = pd.DataFrame(fileStats)\n    df.index.name = 'IDX'\n    df.to_csv(\"fileStats.csv\")\n\n    print(\"\\nStatistics for each file have been saved in fileStats.csv\")\n\nif __name__ == '__main__':\n    main()","dce05fe8":"# Word Counts for Los Angeles Job Bulletins\n\nTo begin text analysis, this program reads all the Job Bulletins and reports raw statistics.  The Job Bulletins contain 1,060,919 words. However, only 8,200 different words are used. This indicates that these Job Bulletins consistently use a select group of words. \n\nThe most used words are grammar words, such as \"the\" (used 52,261 times). Grammar words are also called \"stopwords\".  Looking past grammar words, the Job Bulletins focus on candidates because \"candidates\" was used 6,647  times and \"applicants\" was used 5,150 times. Many jobs require an \"examination\" (used 5,691 times) and experience (used 4,115 times) plus qualifications (used 3,810  times).  \n\n1434 complex words were found meaning these words had more than 10 characters.  A quick review, shows that more analysis is needed.  Some jobs are scientific jobs and the Job Bulletins contain some scientific terms such as thermoplastics, sedimentation, electrolysis, photogrammetry and others.\n\nNotes:\n- in the list of Top 50 Common Words, the first row shows that there are 182,861 spaces\n- POLICE COMMANDER 2251 092917.txt cannot be read"}}