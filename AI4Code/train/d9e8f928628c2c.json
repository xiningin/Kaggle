{"cell_type":{"56598d8f":"code","dde88b09":"code","168357e1":"code","f9184722":"code","eeaff4a9":"code","4937b0ae":"code","c3e1463f":"code","682818c4":"code","2cd5870e":"code","5ed169f8":"code","48ceb3fd":"code","54ed3a07":"code","a1b09d2a":"code","4beeab18":"code","d3c1bfae":"code","3e411e37":"code","339eda51":"code","79825bdb":"code","9943005c":"code","8053326f":"markdown","d1936a54":"markdown","739d14d5":"markdown","39aa690c":"markdown","56a20afa":"markdown","0b5e22fc":"markdown","153b1c80":"markdown","c9d251ee":"markdown","8a45f87d":"markdown","ae591f77":"markdown","419b6f0d":"markdown","e494df4e":"markdown","0b07c804":"markdown","853ac566":"markdown","83238370":"markdown","50295afe":"markdown","ac7546b7":"markdown","d6d54122":"markdown","2c940e07":"markdown","41743807":"markdown","33f4cdc0":"markdown"},"source":{"56598d8f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nprint('Getting traing dataset...')\ntrain_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\nprint('Traing data set obtained \\n')\n\nprint('Getting test dataset...')\ntest_data  = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\nprint('Test data set obtained \\n')","dde88b09":"dictionary = {\n    0 : 'T-shirt\/Top', \n    1 : 'Trouser',\n    2 : 'Pullover',\n    3 : 'Dress',\n    4 : 'Coat',\n    5 : 'Sandal',\n    6 : 'Shirt',\n    7 : 'Sneaker',\n    8 : 'Bag',\n    9 : 'Ankle boot'\n}\n[dictionary[i] for i in range(0,10)]","168357e1":"train_data.head()","f9184722":"# The first function return the matrix out from the dataframe\ndef return_matrix(train, n=0):\n    import numpy as np # linear algebra\n    import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n    import matplotlib.pyplot as plt\n    \n    if ((n>=0) & (n<=train_data.shape[0])) :\n        img_try = train.iloc[n].values.reshape(28,28)\n    else :\n        print('Insert a n between 0 and '.train_data.shape[0])\n        pass \n    return img_try\n\n# The second function eats dataframe plus an integer number between 0 and 21999\ndef print_matrix(train, test, n=0):\n    import numpy as np # linear algebra\n    import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n    import matplotlib.pyplot as plt\n    \n    if ((n>=0) & (n<=train_data.shape[0])) :\n        mat = return_matrix(train, n)\n    \n        fig = plt.figure(figsize=(5, 5))\n        plt.imshow(mat)\n        plt.title(train_label[n], fontsize=30)\n    else : \n        print('Insert a n between 0 and '.train_data.shape[0])\n        pass\n    \n    return print('Done')\n        \n# The third function eats the training dataframe and spits  features and labels\ndef prepare_train_data(df):\n    import pandas as pd \n    import numpy as np\n    from keras import utils as ku\n    \n    print('Transforming the data...')\n    train_img = df.drop('label', axis=1)\n    train_img = train_img\/255 #normalize [0,255] to [0.0 , 1.0]\n    features = train_img.values.reshape(-1,28,28,1)\n    \n    labels = ku.to_categorical(df['label'] ,num_classes=10)\n    print('Transformation done \\n')\n    \n    return features, labels\n\n# The fourth function eats the test dataframe and spits the features\ndef prepare_test_data(df):\n    import pandas as pd \n    import numpy as np\n    \n    print('Transforming the data...')\n    train_img = df\n    train_img = train_img\/255 #normalize [0,255] to [0.0 , 1.0]\n    features = train_img.values.reshape(-1,28,28,1)\n    \n    print('Transformation done \\n')\n    \n    return features","eeaff4a9":"features, labels = prepare_train_data(train_data)","4937b0ae":"plt.figure(figsize=(15,10))\nfor i in range(60):\n    plt.subplot(6,10,i+1)\n    plt.imshow(features[i].reshape((28,28)),cmap='binary')\n    plt.title(train_data['label'].map(dictionary).iloc[i], fontsize=13) # use the dictionary to get the label names\n    plt.axis(\"off\")\nplt.show()","c3e1463f":"def DT_RF_classifier(data, kind='DT', test_size=0.3, max_depth=None):\n    import numpy as np # linear algebra\n    import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report,confusion_matrix\n    \n    # 1. we need to transform the data\n    train_img = train_data.drop('label', axis=1)\n    train_label = train_data['label'].values\n    \n    features = []\n    labels = []\n\n    print('Transforming the data...')\n    for i in np.arange(0, train_label.size):\n        features.append(train_img.iloc[i].values)\n        labels.append(train_label[i])\n    print('Transformation done \\n')\n    \n    \n    # 2. we need to split the dataset into training and test subsets\n    print('Splitting the dataset... ')\n    X_train, X_test, y_train, y_test = train_test_split(np.array(features), np.array(labels), test_size=test_size)\n    print('Training records:',y_train.size)\n    print('Test records:',y_test.size)\n    print('Splitting done \\n')\n    \n    #3. Defining the Classifier: DecisionTree or RandomForest\n    print('Initializing classifier...')\n    if (kind == 'DT'): # Decision Tree\n        from sklearn.tree import DecisionTreeClassifier\n        \n        print('Classifier type: Decision Tree ')\n        print('Fitting classifier...')\n        clf = DecisionTreeClassifier(max_depth=max_depth)\n        clf.fit(X_train,y_train)\n        \n        predictions = clf.predict(X_test)\n        print('Fit done. \\n')\n    \n    else : # Random Forest\n        from sklearn.ensemble import RandomForestClassifier\n        \n        print('Classifier type: Random Forest ')\n        print('Fitting classifier...')\n        clf = RandomForestClassifier(max_depth=max_depth, n_estimators=100)\n        clf.fit(X_train,y_train)\n        \n        predictions = clf.predict(X_test)\n        print('Fit done. \\n')\n    \n    # 4. Evaluation of the Model\n    print('Evaluating the model...')\n    \n    print(classification_report(y_test,predictions))\n    print('The score is: ', clf.score(X_test, y_test))\n    print('\\n')\n    cm = confusion_matrix(y_test,predictions)\n    print(cm)\n    #df_cm = pd.DataFrame(cm, index = '0 1 2 3 4 5 6 7 8 9'.split(), columns = '0 1 2 3 4 5 6 7 8 9'.split())\n    df_cm = pd.DataFrame(cm, index = [dictionary[i] for i in range(0,10)], columns = [dictionary[i] for i in range(0,10)])\n    \n    \n    plt.figure(figsize = (7,7))\n    sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n    plt.xlabel(\"Predicted Class\", fontsize=18)\n    plt.ylabel(\"True Class\", fontsize=18)\n    \n        \n        \n    plt.show()\n    print('\\n ')\n    print('Process ended. ')\n    \n    return clf","682818c4":"DT_RF_classifier(train_data)","2cd5870e":"DT_RF_classifier(train_data, kind='RF')\n","5ed169f8":"import sys\nimport keras\nprint('Keras version:',keras.__version__)\n\nfrom keras import backend as K\n\nfrom keras.preprocessing.image import array_to_img\n\nprint('\\n Done. \\n')","48ceb3fd":"features, labels = prepare_train_data(train_data)\n\n# Train test split\nfrom sklearn.model_selection import train_test_split\n\nprint('Splitting the dataset... ')\nX_train, X_test, Y_train, Y_test = train_test_split(np.array(features), np.array(labels), test_size=0.3)\nprint('Splitting done \\n')\n\nprint('Shapes of X_train, X_test, Y_train, Y_test:')\nprint(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\nprint(X_train.shape[0])\nplt.figure(figsize=(15,8))\nfor i in range(60):\n    plt.subplot(6,10,i+1)\n    plt.imshow(X_train[i].reshape((28,28)),cmap='binary')\n    plt.axis(\"off\")\nplt.show()","54ed3a07":"# Creating batches for the CNN\nfrom keras.preprocessing.image import ImageDataGenerator\n\nclassnames = '0 1 2 3 4 5 6 7 8 9'.split()\nbatch_size = 128\n\nprint(\"Getting Data...\")\ndatagen = ImageDataGenerator(rescale=1.\/255, # normalize pixel values\n                             validation_split=0.3) # hold back 30% of the images for validation\n        # Generate batches of tensor image data with real-time data augmentation.\n        # The data will be looped over (in batches).\n\n\nprint(\"Preparing training dataset...\")\ntrain_generator = datagen.flow(X_train, Y_train, batch_size=batch_size)\nprint(\"Training dataset done. \\n\")\n\nprint(\"Preparing validation dataset...\")\nvalidation_generator = datagen.flow(X_test, Y_test, batch_size=batch_size)\nprint(\"Test dataset done. \\n\")\n\nprint('Done. \\n')","a1b09d2a":"# Define a CNN classifier network\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Flatten, Dense\nfrom keras import optimizers\n\n# Define the model as a sequence of layers\nmodel = Sequential() # Sequential() : Linear stack of layers.\n\n## Add some convolutional layers to extract features = Feature map\nmodel.add(Conv2D(16, (3, 3), padding = 'same', activation = 'relu', input_shape = (28, 28, 1)))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(32, (3, 3), padding = 'same', activation = 'relu', input_shape = (28, 28, 1)))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\n\n# Now we'll flatten the feature maps \nmodel.add(Flatten())\n\n# and generate an output layer with a predicted probability for each class\nmodel.add(Dense(len(labels[0]), activation='softmax'))\n    # The softmax activation function is used for multi-class classifiers\n\n# We'll use the ADAM optimizer\nopt = optimizers.Adam(lr=0.001)\n\n# With the layers defined, we can now compile the model for categorical (multi-class) classification\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nprint(model.summary())","4beeab18":"# Train the model over 15 epochs\nnum_epochs = 15\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = (X_train.shape[0] \/ batch_size) ,\n    validation_data = validation_generator, \n    epochs = num_epochs)","d3c1bfae":"import matplotlib.pyplot as plt\n\nepoch_nums = range(1,num_epochs+1)\ntraining_loss = history.history[\"loss\"]\nvalidation_loss = history.history[\"val_loss\"]\ntrain_acc = history.history[\"accuracy\"]\nval_acc = history.history['val_accuracy']\n\nplt.figure(figsize=(13,5))\n\nplt.subplot(1,2,1)\nplt.plot(epoch_nums, training_loss)\nplt.plot(epoch_nums, validation_loss)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['training', 'validation'], loc='upper right')\n\nplt.subplot(1,2,2)\nplt.plot(epoch_nums, train_acc)\nplt.plot(epoch_nums, val_acc)\nplt.xlabel('epoch')\nplt.ylabel('Accuracy')\nplt.legend(['training', 'validation'], loc='lower right')\n\nplt.show()","3e411e37":"import numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n\nclasses ='0 1 2 3 4 5 6 7 8 9'.split()\nprint(\"Generating predictions from validation data...\")\n# Get the image and label arrays for the first batch of validation data\nx_test = validation_generator[0][0]\ny_test = validation_generator[0][1]\n\n# Use the moedl to predict the class\nclass_probabilities = model.predict(x_test)\n\n# The model returns a probability value for each class\n# The one with the highest probability is the predicted class\npredictions = np.argmax(class_probabilities, axis=1)\n\n# The actual labels are hot encoded (e.g. [0 1 0], so get the one with the value 1\ntrue_labels = np.argmax(y_test, axis=1)\n\n\n# Plot the confusion matrix\ncm = confusion_matrix(true_labels, predictions)\ntick_marks = np.arange(len(classes))\n\n\ndf_cm = pd.DataFrame(cm, index = [dictionary[i] for i in range(0,10)], columns = [dictionary[i] for i in range(0,10)])\nplt.figure(figsize = (7,7))\nsns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\nplt.xlabel(\"Predicted Class\")\nplt.ylabel(\"True Class\")\nplt.show()","339eda51":"test_features = prepare_test_data(test_data.drop('label', axis=1))\n\nprint('Starting predictions...')\nfinal = model.predict(test_features)\nfinal = np.argmax(final,axis = 1)\nfinal_df = pd.Series(final, name=\"Label\")\nprint('Predictions done. \\n')","79825bdb":"def show_pred_imag(test_data, final, n=0):\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import numpy as np\n    \n    if ((n>=0) & (n<=test_data.shape[0])):\n        mat = return_matrix(test_data, n)\n        \n        plt.imshow(mat)\n        plt.title(final[n], fontsize=30)\n        plt.show()\n    else :\n        print('Insert a number between 0 and '.test_data.shape[0])\n        pass\n    \n    return print('Done. \\n')","9943005c":"plt.figure(figsize=(14,8))\nfor i in range(60):\n    plt.subplot(6,10,i+1)\n    mat = return_matrix(test_data.drop('label', axis=1), i)\n    plt.imshow(mat, cmap='binary')\n    plt.title(dictionary[final[i]], fontsize=14)\n    plt.axis(\"off\")\n\nplt.subplots_adjust(hspace=1.5)\nplt.subplots_adjust(wspace=0.3)\nplt.show()","8053326f":"Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n\nThe training data set, (train.csv), has 785 columns. **The first column, called \"label\", is the digit that was drawn by the user**. The rest of the columns contain the pixel-values of the associated image.\n\nEach pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).","d1936a54":"It easy to see that we are **not** experiencing *overfitting*!","739d14d5":"We recall that the labels are integer numbers between 0 and 9, with the followinf identification:\n- 0 T-shirt\/top\n- 1 Trouser\n- 2 Pullover\n- 3 Dress\n- 4 Coat\n- 5 Sandal\n- 6 Shirt\n- 7 Sneaker\n- 8 Bag\n- 9 Ankle boot","39aa690c":"We thus need to splits label and feature:","56a20afa":"We should remark a remarkable improvement in the accuracy using the Random Forest, as expected. ","0b5e22fc":"We now set the features and Labels:","153b1c80":"### Confusion Matrix\n\nWe now plot the confusion matrix as a seaborn heatmap","c9d251ee":"### RECOVERING IMAGES\n\nNow we try to print a single image. In order to do so, we need to convert the pandas dataframe into a matrix, i.e. an ordered numpy array.\n\nWe will thus define two functions; \n\n1. return_matrix: given the dataset and an integer *n* it returns the *n*-th image as matrix\n2. print_matrix: given the dataset, the label set and an integer *n* it prints  the *n*-th image as matrix\n3. prepare_train_data: given the train_dataset it spits the images features and labels\n4. prepare_test_data: given the test_dataset it spits the image features (no labels here!)","8a45f87d":"### Prepare data and Train\/test splitting them\n\nWe need to call our prepare_train_data function in order to prepare the data. We also train\/test split them. \n\nWe also plot some of the images contained into the train set.","ae591f77":"### Train the model\n\nWe train the model over 15 epoch just to keep it fast. ","419b6f0d":"As an example, we plot the first sixty images of the dataset;\n\nnotice that we will use the dictionary to print the name of the labels instead of the relative number;","e494df4e":"## Preprocessing, augmenting and Batching the data\n\n#### Prepare the Data\nBefore we can train the model, we need to prepare the data.\n\nWe'll divide the feature values by 255 to normalize them as floating point values between 0 and 1, and we'll split the data so that we can use 70% of it to train the model, and hold back 30% to validate it.\n\n#### Batching\nWe also creating the batches for training over epoch the CNN","0b07c804":"# Fashion MNIST\n\n## Context\n\nFashion-MNIST is a dataset of Zalando's article images\u2014consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n\n## Content\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n\n- To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix.\n- For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n\n### Labels\n\nEach training and test example is assigned to one of the following labels:\n\n0 T-shirt\/top\n1 Trouser\n2 Pullover\n3 Dress\n4 Coat\n5 Sandal\n6 Shirt\n7 Sneaker\n8 Bag\n9 Ankle boot\n\n### Parent Notebook\n\nWe will now apply methodologies similar to my previous notebook on standard MNIST as [1] and [2] to use classical ML algorithm as well as Convolutional Neural Networks to classify the images.\n\n\n--------------\n[1] https:\/\/www.kaggle.com\/androbomb\/mnist-ale-decisiontree\n\n[2] https:\/\/www.kaggle.com\/androbomb\/mnist-ale-cnn","853ac566":"## Creating the CNN Model\nWe now need to create our Convolutional Neural Network model. In order to do so, we need to choose the convolution and poolying layers.\n\nWe will thus:\n\n1. Define the model as a sequential layers\n2. Introduce the Convolutions\n3. they are importan to create a feature map\n4. Introduce the Poolings\n\nMaxPooling is used to reduce dimensionality. In MaxPooling, the output value is just the maximum of the input values in each patch (for ex. The maximum pixel in a span of 3 pixels).\nFlatten the data in order to have a np.array to feed the NN.\n\nIn particoular, we will apply\n\n1. 16 3x3 Convolution matrix with activation='relu', i.e. a Rectified Linear Unit (ReLU) activation.\n2. A 2x2 MaxPooling\n3. 32 3x3 Convolution matrix with activation='relu', i.e. a Rectified Linear Unit (ReLU) activation.\n4. A 2x2 MaxPooling\n5. 64 3x3 Convolution matrix with activation='relu', i.e. a Rectified Linear Unit (ReLU) activation.\n6. A 2x2 MaxPooling\n7. 64 3x3 Convolution matrix with activation='relu', i.e. a Rectified Linear Unit (ReLU) activation.\n8. A 2x2 MaxPooling\n\nAs a optimaizer, we decided to use the **ADAM (ADAptive Moment estimation)** optimization algorithm, that is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing.5. ","83238370":"# Using Convolutional Neural Networks ","50295afe":"# Classical Machine Learning Algorithms","ac7546b7":"### Random Forest Example:","d6d54122":"### Decision Tree example:","2c940e07":"## Generate predictions with the CNN","41743807":"### View the Loss History\n\nWe tracked average training and validation loss and accuracy history for each epoch.\n\nWe can plot these to verify that loss reduced as the model was trained while accuracy increased, and to detect over-fitting (which is indicated by a continued drop in training loss after validation loss has levelled out or started to increase).","33f4cdc0":"## Decision Tree and Random Forest Classifier\n\nThe function **DT_RF_classifier** will have 4 entries, but only one obligatory: Dataset (obligatory), kind of the Classifier (either DecisionTree 'DT' or RandomForest 'RT'), the test_size, and the max_depth for the Random Forest, and it outputs the trained classifiers.\n\nIt is split in 4 block:\n1. The first block transforms the dataset, extracting features and labels;\n2. The second block gives the train\/test split, printing the size of the two sets\n3. The third block defines the classifier, either a Decision Tree or a Random Forest\n4. The fourth block test the models, printing the accuracy score and the confusion matrix as a seaborn heatmap\n"}}