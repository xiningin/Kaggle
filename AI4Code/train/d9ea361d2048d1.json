{"cell_type":{"18e504d0":"code","cc6dd0ba":"code","a6089b79":"code","737b8dec":"code","055d2eb7":"code","8acceed6":"code","8ccf8915":"code","1ed5012a":"code","d83cb926":"code","e31f827b":"code","e7269212":"code","0bd0652f":"code","7bf038e1":"code","34956f75":"code","2e9927ad":"markdown","5d3e045a":"markdown","6a68377a":"markdown"},"source":{"18e504d0":"import numpy as np, pandas as pd, os, gc\nimport matplotlib.pyplot as plt, time\nfrom PIL import Image \nimport warnings\nimport random\nwarnings.filterwarnings(\"ignore\")\n\npath = '..\/input\/severstal-steel-defect-detection\/'\ntrain = pd.read_csv(path + 'train.csv')\n\n# RESTRUCTURE TRAIN DATAFRAME\ntrain['ImageId'] = train['ImageId_ClassId'].map(lambda x: x.split('.')[0]+'.jpg')\ntrain2 = pd.DataFrame({'ImageId':train['ImageId'][::4]})\ntrain2['e1'] = train['EncodedPixels'][::4].values\ntrain2['e2'] = train['EncodedPixels'][1::4].values\ntrain2['e3'] = train['EncodedPixels'][2::4].values\ntrain2['e4'] = train['EncodedPixels'][3::4].values\ntrain2.reset_index(inplace=True,drop=True)\ntrain2.fillna('',inplace=True); \ntrain2['count'] = np.sum(train2.iloc[:,1:]!='',axis=1).values\n\nindexes = list(range(len(train2)))\nrandom.shuffle(indexes)\ntrain_ratio = 0.95\npartio = int(len(train2) * train_ratio)\ntrain_indexes = indexes[:partio]\nval_indexes = indexes[partio:]\ntrain_df = train2.iloc[train_indexes, :]\nval_df = train2.iloc[val_indexes, :]","cc6dd0ba":"from albumentations import (\n    Compose, HorizontalFlip, ShiftScaleRotate, PadIfNeeded, RandomCrop,\n    RGBShift, RandomBrightness, RandomContrast, VerticalFlip, \n)\ncrop_size = [256, 416]\ntrain_augmentator = Compose([\n        HorizontalFlip(p=0.5),\n        VerticalFlip(p=0.5),\n        ShiftScaleRotate(shift_limit=0.03, scale_limit=0,\n                         rotate_limit=(-3, 3), border_mode=0, p=0.75),\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size),\n        RandomBrightness(limit=(-0.25, 0.25), p=0.75),\n        RandomContrast(limit=(-0.15, 0.4), p=0.75),\n        RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.75)\n    ], p=1)","a6089b79":"import mxnet as mx\nfrom mxnet.gluon import data, HybridBlock, nn\nimport pandas as pd\nimport cv2\nimport os\nimport numpy as np\nfrom mxnet.gluon.data.vision import transforms\nfrom mxnet.gluon.model_zoo import vision\nfrom mxnet.lr_scheduler import CosineScheduler\nfrom mxnet.gluon import loss, Trainer\nfrom mxnet import autograd\nimport random\nfrom PIL import Image, ImageOps, ImageFilter\nfrom mxnet import nd as F, lr_scheduler as lrs\nfrom mxnet.gluon.contrib.estimator import Estimator\nimport gluoncv.model_zoo  as gm\n\ndef scale_func(image_shape):\n    return random.uniform(0.5, 1.2)\n\n\nclass SteelDataset(data.Dataset):\n    def __init__(self, df, img_dir, debug=False):\n        \n        self.train_df = df\n        self.root_dir = img_dir\n        self.transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=(0.485, 0.456, 0.406),\n                   std=(0.229, 0.224, 0.225)\n                )\n            ]\n        )\n        \n        self.debug = debug\n        \n    def __getitem__(self, i):\n        if self.debug:\n            curr_df = self.train_df.head(20)\n        masks = np.zeros((256, 1600), np.uint8)\n        img_names = []\n        item = self.train_df.iloc[i, :]\n        img_name = item['ImageId']\n        for j in range(4):\n            curr_item = item[\"e{}\".format(j+1)]\n            if len(curr_item) > 0:\n                rle_pixels = curr_item\n                label = rle_pixels.split(\" \")\n                positions = list(map(int, label[0::2]))\n                length = list(map(int, label[1::2]))\n                mask = np.zeros(256 * 1600, dtype=np.uint8)\n                for pos, le in zip(positions, length):\n                    mask[pos - 1:(pos + le - 1)] = j+1\n                count = np.sum(np.where(mask==(j+1), 1, 0))\n                if count < 8:\n                    mask = np.where(mask==(j+1), -1, 0)\n                    \n                masks[ :, :] += mask.reshape(256, 1600, order='F')\n                \n        oimg = cv2.imread(os.path.join(self.root_dir, img_name))[:, :, ::-1]\n        oimg, masks = self.rescale_sample(oimg, masks)\n        aug_out = train_augmentator(image=oimg, mask=masks)\n        oimg = aug_out['image']\n        masks = aug_out['mask']\n        img = F.array(oimg)\n        img = self.transform(img)\n        \n        if self.debug:\n            return img, F.array(masks[::4, ::4]), oimg, masks, curr_df\n        else:\n            return img, F.array(masks)\n        \n    def __len__(self):\n        return len(self.train_df)\n\n\n    def rescale_sample(self, image, mask):\n\n        scale = scale_func(image.shape)\n        image = cv2.resize(image, (0, 0), fx=scale, fy=scale)\n        new_size = (image.shape[1], image.shape[0])\n\n        mask = cv2.resize(mask, new_size, interpolation=cv2.INTER_NEAREST)\n\n        return image, mask\n","737b8dec":"# for test\nimport matplotlib.pyplot as plt\ncsv_file = 'train.csv'\nimg_dir = '..\/input\/severstal-steel-defect-detection\/train_images\/'\nsteel_dataset = SteelDataset(train2, img_dir, debug=True)\nprint(len(steel_dataset))\n_, mm, im, mask, curr_df = steel_dataset[11]\nplt.figure(figsize=(20, 20))\nplt.subplot(2, 1, 1)\nplt.imshow(im)\nplt.subplot(2, 1, 2)\nplt.imshow(mask[::4, ::4])\nmm.flatten().shape\n","055d2eb7":"from gluoncv.model_zoo.resnetv1b import resnet50_v1s, resnet101_v1s, resnet152_v1s\nimport mxnet as mx\n\nclass ResNetBackbone(mx.gluon.HybridBlock):\n    def __init__(self, backbone='resnet101', pretrained_base=True,dilated=True, **kwargs):\n        super(ResNetBackbone, self).__init__()\n\n        with self.name_scope():\n            if backbone == 'resnet50':\n                pretrained = resnet50_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n            elif backbone == 'resnet101':\n                pretrained = resnet101_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n            elif backbone == 'resnet152':\n                pretrained = resnet152_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n            else:\n                raise RuntimeError(f'unknown backbone: {backbone}')\n\n            self.conv1 = pretrained.conv1\n            self.bn1 = pretrained.bn1\n            self.relu = pretrained.relu\n            self.maxpool = pretrained.maxpool\n            self.layer1 = pretrained.layer1\n            self.layer2 = pretrained.layer2\n            self.layer3 = pretrained.layer3\n            self.layer4 = pretrained.layer4\n\n    def hybrid_forward(self, F, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        c1 = self.layer1(x)\n        c2 = self.layer2(c1)\n        c3 = self.layer3(c2)\n        c4 = self.layer4(c3)\n\n        return c1, c2, c3, c4","8acceed6":"import mxnet as mx\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.nn import HybridBlock\n\nclass ResNetFPN(mx.gluon.HybridBlock):\n    def __init__(self, backbone= 'resnet101', backbone_lr_mult=0.1, **kwargs):\n        super(ResNetFPN, self).__init__()\n\n        self.backbone_name = backbone\n        self.backbone_lr_mult = backbone_lr_mult\n        self._kwargs = kwargs\n\n        with self.name_scope():\n            self.backbone = ResNetBackbone(backbone=self.backbone_name, pretrained_base=False, dilated=False, **kwargs)\n\n            self.head = _FPNHead(output_channels=256, **kwargs)\n\n    def load_pretrained_weights(self):\n        pretrained = ResNetBackbone(backbone=self.backbone_name, pretrained_base=True, dilated=False, **self._kwargs)\n        backbone_params = self.backbone.collect_params()\n        pretrained_weights = pretrained.collect_params()\n        for k, v in pretrained_weights.items():\n            param_name = backbone_params.prefix + k[len(pretrained_weights.prefix):]\n            backbone_params[param_name].set_data(v.data())\n\n        self.backbone.collect_params().setattr('lr_mult', self.backbone_lr_mult)\n\n    def hybrid_forward(self,F, x):\n        c1, c2, c3, c4 = self.backbone(x)\n        p1, p2, p3, p4 = self.head(c1, c2, c3, c4)\n\n        return p1, p2, p3, p4\n\nclass ResNetUnet(mx.gluon.HybridBlock):\n    def __init__(self, backbone= 'resnet101', backbone_lr_mult=0.1, **kwargs):\n        super(ResNetUnet, self).__init__()\n\n        self.backbone_name = backbone\n        self.backbone_lr_mult = backbone_lr_mult\n        self._kwargs = kwargs\n\n        with self.name_scope():\n            self.backbone = ResNetBackbone(backbone=self.backbone_name, pretrained_base=False, dilated=False, **kwargs)\n\n            self.head = _UnetHead(**kwargs)\n\n    def load_pretrained_weights(self):\n        pretrained = ResNetBackbone(backbone=self.backbone_name, pretrained_base=True, dilated=False, **self._kwargs)\n        backbone_params = self.backbone.collect_params()\n        pretrained_weights = pretrained.collect_params()\n        for k, v in pretrained_weights.items():\n            param_name = backbone_params.prefix + k[len(pretrained_weights.prefix):]\n            backbone_params[param_name].set_data(v.data())\n\n        self.backbone.collect_params().setattr('lr_mult', self.backbone_lr_mult)\n\n    def hybrid_forward(self,F, x):\n        c1, c2, c3, c4 = self.backbone(x)\n        out = self.head(c1, c2, c3, c4)\n\n        return out\n\nclass _DecoderBlock(HybridBlock):\n    def __init__(self, output_channels, norm_layer=nn.BatchNorm):\n        super(_DecoderBlock, self).__init__()\n\n        with self.name_scope():\n            self.block = nn.HybridSequential()\n            self.block.add(ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer))\n            self.block.add(ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer))\n\n    def hybrid_forward(self, F, x, y=None):\n        if y is not None:\n            x = F.contrib.BilinearResize2D(x, scale_height=2, scale_width=2)\n            x = F.concat(x, y, dim=1)\n        out = self.block(x)\n        return out\n\n\nclass _UnetHead(HybridBlock):\n    def __init__(self, num_classes, output_channels=[256, 128, 64, 32], scale=4, norm_layer=nn.BatchNorm):\n        super(_UnetHead, self).__init__()\n        \n        self.scale = scale\n        with self.name_scope():\n            self.block4 = _DecoderBlock(output_channels[0], norm_layer=norm_layer)\n            self.block3 = _DecoderBlock(output_channels[1], norm_layer=norm_layer)\n            self.block2 = _DecoderBlock(output_channels[2], norm_layer=norm_layer)\n            self.block1 = _DecoderBlock(output_channels[3], norm_layer=norm_layer)\n            self.postprocess_block = nn.Conv2D(num_classes, kernel_size=1)\n\n    def hybrid_forward(self, F, c1, c2, c3, c4):\n\n        p4 = self.block4(c4)\n        p3 = self.block3(p4, c3)\n        p2 = self.block2(p3, c2)\n        p1 = self.block1(p2, c1)\n        if self.scale > 1:\n            p1 = F.contrib.BilinearResize2D(p1, scale_height=self.scale, scale_width=self.scale)\n        out = self.postprocess_block(p1)\n\n        return out\n\n\nclass _FPNHead(HybridBlock):\n    def __init__(self, output_channels=256, norm_layer=nn.BatchNorm):\n        super(_FPNHead, self).__init__()\n        self._hdsize = {}\n\n        with self.name_scope():\n            self.block4 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n            self.block3 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n            self.block2 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n            self.block1 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n\n    def hybrid_forward(self, F, c1, c2, c3, c4):\n        p4 = self.block4(c4)\n        p3 = self._resize_as(F, 'id_1', p4, c3) + self.block3(c3)\n        p2 = self._resize_as(F, 'id_2', p3, c2) + self.block2(c2)\n        p1 = self._resize_as(F, 'id_3', p2, c1) + self.block1(c1)\n\n        return p1, p2, p3, p4\n\n    def _resize_as(self, F, name, x, y):\n        h_key = name + '_h'\n        w_key = name + '_w'\n\n        if hasattr(y, 'shape'):\n            _, _, h, w = y.shape\n            _, _, h2, w2 = x.shape\n\n            if h == h2 and w == w2:\n                h = 0\n                w = 0\n\n            self._hdsize[h_key] = h\n            self._hdsize[w_key] = w\n        else:\n            h, w = self._hdsize[h_key], self._hdsize[w_key]\n\n        if h == 0 and w == 0:\n            return x\n        else:\n            return F.contrib.BilinearResize2D(x, height=h, width=w)\n\n\nclass SemanticFPNHead(HybridBlock):\n    def __init__(self, num_classes, output_channels=128, norm_layer=nn.BatchNorm):\n        super(SemanticFPNHead, self).__init__()\n        self._hdsize = {}\n\n        with self.name_scope():\n            self.block4_1 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n            self.block4_2 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n            self.block4_3 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n\n            self.block3_1 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n            self.block3_2 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n\n            self.block2 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n            self.block1 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n\n            self.postprocess_block = nn.Conv2D(num_classes, kernel_size=1)\n\n    def hybrid_forward(self, F, c1, c2, c3, c4):\n        out4 = self._resize_as(F, 'id_1', self.block4_1(c4), c3)\n        out4 = self._resize_as(F, 'id_2', self.block4_2(out4), c2)\n        out4 = self._resize_as(F, 'id_3', self.block4_3(out4), c1)\n\n        out3 = self._resize_as(F, 'id_4', self.block3_1(c3), c2)\n        out3 = self._resize_as(F, 'id_5', self.block3_2(out3), c1)\n\n        out2 = self._resize_as(F, 'id_6', self.block2(c2), c1)\n\n        out1 = self.block1(c1)\n\n        out = out1 + out2 + out3 + out4\n\n        out = self.postprocess_block(out)\n        out = F.contrib.BilinearResize2D(out,scale_height=4,scale_width=4)\n        return out\n\n    def _resize_as(self, F,name, x, y):\n        h_key = name + '_h'\n        w_key = name + '_w'\n\n        if hasattr(y, 'shape'):\n            _, _, h, w = y.shape\n            _, _, h2, w2 = x.shape\n\n            if h == h2 and w == w2:\n                h = 0\n                w = 0\n\n            self._hdsize[h_key]=h\n            self._hdsize[w_key]=w\n        else:\n            h, w = self._hdsize[h_key], self._hdsize[w_key]\n\n        if h == 0 and w == 0:\n            return x\n        else:\n            return F.contrib.BilinearResize2D(x,height=h,width=w)\n\n\nclass ConvBlock(HybridBlock):\n    def __init__(self, output_channels, kernel_size, padding=0, activation='relu', norm_layer=nn.BatchNorm):\n        super().__init__()\n        self.body = nn.HybridSequential()\n        self.body.add(\n            nn.Conv2D(output_channels, kernel_size=kernel_size, padding=padding, activation=activation),\n            norm_layer(in_channels=output_channels)\n        )\n\n    def hybrid_forward(self, F, x):\n        return self.body(x)\n","8ccf8915":"class SteelFPN(HybridBlock):\n    \n    def __init__(self, n_classes=5, ctx=mx.cpu()):\n        super().__init__()\n        with self.name_scope():\n            self.feature_extractor = ResNetFPN()\n            self.segment_head = SemanticFPNHead(num_classes=n_classes)\n    def hybrid_forward(self, F, x):\n        fpn_feature = self.feature_extractor(x)\n        segment_out = self.segment_head(*fpn_feature)\n        return segment_out\n","1ed5012a":"# unet = ResNetUnet(output_channels=[256, 128, 64, 32], num_classes=5)\n# unet.collect_params().initialize()\n# unet.load_pretrained_weights()\n# a = mx.nd.normal(shape=(1, 3, 512, 512))\n# out = unet(a)\n# print(out.shape)","d83cb926":"import numpy as np\nimport mxnet as mx\nfrom mxnet import gluon\nfrom mxnet import nd\nfrom mxnet.gluon.loss import Loss, _apply_weighting, _reshape_like\n\nclass NormalizedFocalLossSoftmax(Loss):\n    def __init__(self, sparse_label=True, batch_axis=0, ignore_label=-1,\n                 size_average=True, detach_delimeter=True, gamma=2, eps=1e-10, **kwargs):\n        super(NormalizedFocalLossSoftmax, self).__init__(None, batch_axis, **kwargs)\n        self._sparse_label = sparse_label\n        self._ignore_label = ignore_label\n        self._size_average = size_average\n        self._detach_delimeter = detach_delimeter\n        self._eps = eps\n        self._gamma = gamma\n        self._k_sum = 0\n\n    def hybrid_forward(self, F, pred, label):\n        label = F.expand_dims(label, axis=1)\n        softmaxout = F.softmax(pred, axis=1)\n\n        t = label != self._ignore_label\n        pt = F.pick(softmaxout, label, axis=1, keepdims=True)\n        pt = F.where(t, pt, F.ones_like(pt))\n        beta = (1 - pt) ** self._gamma\n\n        t_sum = F.cast(F.sum(t, axis=(-2, -1), keepdims=True), 'float32')\n        beta_sum = F.sum(beta, axis=(-2, -1), keepdims=True)\n        mult = t_sum \/ (beta_sum + self._eps)\n        if self._detach_delimeter:\n            mult = mult.detach()\n        beta = F.broadcast_mul(beta, mult)\n        self._k_sum = 0.9 * self._k_sum + 0.1 * mult.asnumpy().mean()\n\n        loss = -beta * F.log(F.minimum(pt + self._eps, 1))\n\n        if self._size_average:\n            bsum = F.sum(t_sum, axis=self._batch_axis, exclude=True)\n            loss = F.sum(loss, axis=self._batch_axis, exclude=True) \/ (bsum + self._eps)\n        else:\n            loss = F.sum(loss, axis=self._batch_axis, exclude=True)\n\n        return loss\n\n    def log_states(self, sw, name, global_step):\n        sw.add_scalar(tag=name + '_k', value=self._k_sum, global_step=global_step)\n\n\nclass NormalizedFocalLossSigmoid(gluon.loss.Loss):\n    def __init__(self, axis=-1, alpha=0.25, gamma=2,\n                 from_logits=False, batch_axis=0,\n                 weight=None, size_average=True, detach_delimeter=True,\n                 eps=1e-12, scale=1.0,\n                 ignore_label=-1, **kwargs):\n        super(NormalizedFocalLossSigmoid, self).__init__(weight, batch_axis, **kwargs)\n        self._axis = axis\n        self._alpha = alpha\n        self._gamma = gamma\n        self._ignore_label = ignore_label\n\n        self._scale = scale\n        self._from_logits = from_logits\n        self._eps = eps\n        self._size_average = size_average\n        self._detach_delimeter = detach_delimeter\n        self._k_sum = 0\n\n    def hybrid_forward(self, F, pred, label, sample_weight=None):\n        one_hot = label > 0\n        t = F.ones_like(one_hot)\n\n        if not self._from_logits:\n            pred = F.sigmoid(pred)\n\n        alpha = F.where(one_hot, self._alpha * t, (1 - self._alpha) * t)\n        pt = F.where(one_hot, pred, 1 - pred)\n        pt = F.where(label != self._ignore_label, pt, F.ones_like(pt))\n\n        beta = (1 - pt) ** self._gamma\n\n        t_sum = F.sum(t, axis=(-2, -1), keepdims=True)\n        beta_sum = F.sum(beta, axis=(-2, -1), keepdims=True)\n        mult = t_sum \/ (beta_sum + self._eps)\n        if self._detach_delimeter:\n            mult = mult.detach()\n        beta = F.broadcast_mul(beta, mult)\n\n        ignore_area = F.sum(label == -1, axis=0, exclude=True).asnumpy()\n        sample_mult = F.mean(mult, axis=0, exclude=True).asnumpy()\n        if np.any(ignore_area == 0):\n            self._k_sum = 0.9 * self._k_sum + 0.1 * sample_mult[ignore_area == 0].mean()\n\n        loss = -alpha * beta * F.log(F.minimum(pt + self._eps, 1))\n        sample_weight = label != self._ignore_label\n\n        loss = _apply_weighting(F, loss, self._weight, sample_weight)\n        if self._size_average:\n            bsum = F.sum(sample_weight, axis=self._batch_axis, exclude=True)\n            loss = F.sum(loss, axis=self._batch_axis, exclude=True) \/ (bsum + self._eps)\n        else:\n            loss = F.sum(loss, axis=self._batch_axis, exclude=True)\n\n        return self._scale * loss\n\n    def log_states(self, sw, name, global_step):\n        sw.add_scalar(tag=name + '_k', value=self._k_sum, global_step=global_step)\n\n\nclass FocalLoss(gluon.loss.Loss):\n    def __init__(self, axis=-1, alpha=0.25, gamma=2,\n                 from_logits=False, batch_axis=0,\n                 weight=None, num_class=None,\n                 eps=1e-9, size_average=True, scale=1.0, **kwargs):\n        super(FocalLoss, self).__init__(weight, batch_axis, **kwargs)\n        self._axis = axis\n        self._alpha = alpha\n        self._gamma = gamma\n\n        self._scale = scale\n        self._num_class = num_class\n        self._from_logits = from_logits\n        self._eps = eps\n        self._size_average = size_average\n\n    def hybrid_forward(self, F, pred, label, sample_weight=None):\n        if not self._from_logits:\n            pred = F.sigmoid(pred)\n\n        one_hot = label > 0\n        pt = F.where(one_hot, pred, 1 - pred)\n\n        t = label != -1\n        alpha = F.where(one_hot, self._alpha * t, (1 - self._alpha) * t)\n        beta = (1 - pt) ** self._gamma\n\n        loss = -alpha * beta * F.log(F.minimum(pt + self._eps, 1))\n        sample_weight = label != -1\n\n        loss = _apply_weighting(F, loss, self._weight, sample_weight)\n        if self._size_average:\n            tsum = F.sum(label == 1, axis=self._batch_axis, exclude=True)\n            loss = F.sum(loss, axis=self._batch_axis, exclude=True) \/ (tsum + self._eps)\n        else:\n            loss = F.sum(loss, axis=self._batch_axis, exclude=True)\n\n        return self._scale * loss\n\n\nclass SoftmaxCrossEntropyLoss(Loss):\n    def __init__(self, sparse_label=True, batch_axis=0, ignore_label=-1,\n                 size_average=True, grad_scale=1.0, **kwargs):\n        super(SoftmaxCrossEntropyLoss, self).__init__(None, batch_axis, **kwargs)\n        self._sparse_label = sparse_label\n        self._ignore_label = ignore_label\n        self._size_average = size_average\n        self._grad_scale = grad_scale\n\n    def hybrid_forward(self, F, pred, label):\n        softmaxout = F.SoftmaxOutput(\n            pred, label.astype(pred.dtype), ignore_label=self._ignore_label,\n            multi_output=self._sparse_label,\n            use_ignore=True, normalization='valid' if self._size_average else 'null',\n            grad_scale=self._grad_scale,\n        )\n        loss = -F.pick(F.log(softmaxout), label, axis=1, keepdims=True)\n        loss = F.where(label.expand_dims(axis=1) == self._ignore_label,\n                       F.zeros_like(loss), loss)\n        return F.mean(loss, axis=self._batch_axis, exclude=True)\n\n\nclass SigmoidBinaryCrossEntropyLoss(Loss):\n    def __init__(self, from_sigmoid=False, weight=None, batch_axis=0, ignore_label=-1, **kwargs):\n        super(SigmoidBinaryCrossEntropyLoss, self).__init__(\n            weight, batch_axis, **kwargs)\n        self._from_sigmoid = from_sigmoid\n        self._ignore_label = ignore_label\n\n    def hybrid_forward(self, F, pred, label):\n        label = _reshape_like(F, label, pred)\n        sample_weight = label != self._ignore_label\n        label = F.where(sample_weight, label, F.zeros_like(label))\n\n        if not self._from_sigmoid:\n            loss = F.relu(pred) - pred * label + \\\n                F.Activation(-F.abs(pred), act_type='softrelu')\n        else:\n            eps = 1e-12\n            loss = -(F.log(pred + eps) * label\n                     + F.log(1. - pred + eps) * (1. - label))\n\n        loss = _apply_weighting(F, loss, self._weight, sample_weight)\n        return F.mean(loss, axis=self._batch_axis, exclude=True)","e31f827b":"def compute_iou(label, pred):\n    union = np.logical_or(label, pred)\n    intersection = np.logical_and(label, pred)\n    iou = intersection \/ (union + 1e-5)\n    return np.mean(iou)\n\ndef iou_metric(labels, preds):\n    \n#     labels = F.array(labels)\n#     preds = F.array(preds)\n    labels = labels.asnumpy()\n    preds = F.argmax(F.softmax(preds, axis=1), axis=1).asnumpy()\n    ious = []\n    for i in range(5):\n        curr_pred = np.where(preds==i, 1, 0)\n        curr_labels = np.where(labels==i, 1, 0)\n        curr_iou = compute_iou(curr_labels, curr_pred)\n        ious.append(curr_iou)\n    mean_iou = np.mean(ious)\n    ious.append(mean_iou)\n#     print(\"IOU_INFO:: bg:{}, 1:{}, 2:{}, 3:{}, 4:{}, mean_iou:{}\".format(*ious))\n    cls = ['bg', '1', '2', '3', '4', 'mean_iou']\n    return {k:v for k, v in zip(cls, ious)}\n","e7269212":"def training(epoch, data, net, loss, trainer, ctx):\n    train_loss = 0.0\n    train_iou = 0.0\n    bg_iou, iou1, iou2, iou3, iou4 = [0.0] * 5\n    hybridize = False\n    tbar = tqdm(data)\n    for i, batch_data in enumerate(tbar):\n        image, mask = batch_data\n        image = image.as_in_context(ctx)\n        mask = mask.as_in_context(ctx)\n        with autograd.record():\n            outputs = net(image)\n            losses = loss(outputs, mask)\n            ious = iou_metric(mask, outputs)\n        losses.backward()\n        global_step = epoch * len(data) + i\n        trainer.step(len(batch_data))\n\n        batch_loss = sum(loss.asnumpy().mean() for loss in losses) \/ len(losses)\n        train_loss += batch_loss\n        train_iou += ious['mean_iou']\n        bg_iou += ious['bg']\n        iou1 += ious['1']\n        iou2 += ious['2']\n        iou3 += ious['3']\n        iou4 += ious['4']\n        if i % 20:\n            tbar.set_description(f'Epoch {epoch}, training loss {train_loss\/(i+1):.6f}, training_ious:{train_iou\/(i+1):.6f}, bg_ious:{bg_iou\/(i+1):.6f},class1_ious:{iou1\/(i+1):.6f},class2_ious:{iou2\/(i+1):.6f}, class3_ious:{iou3\/(i+1):.6f}, class4_ious:{iou4\/(i+1):.6f}')\n","0bd0652f":"def evaluation(data, net, ctx):\n    val_iou = 0.0\n    bg_iou, iou1, iou2, iou3, iou4 = [0.0] * 5\n    hybridize = False\n    tbar = tqdm(data)\n    for i, batch_data in enumerate(tbar):\n        image, mask = batch_data\n        image = image.as_in_context(ctx)\n        mask = mask.as_in_context(ctx)\n        outputs = net(image)\n        ious = iou_metric(mask, outputs)\n        val_iou += ious['mean_iou']\n        bg_iou += ious['bg']\n        iou1 += ious['1']\n        iou2 += ious['2']\n        iou3 += ious['3']\n        iou4 += ious['4']\n        if i % 20:\n            tbar.set_description(f'val_ious:{val_iou\/(i+1):.6f}, bg_ious:{bg_iou\/(i+1):.6f},class1_ious:{iou1\/(i+1):.6f},class2_ious:{iou2\/(i+1):.6f}, class3_ious:{iou3\/(i+1):.6f}, class4_ious:{iou4\/(i+1):.6f}')\n    return val_iou * 1.0 \/(i+1)","7bf038e1":"import os\nfrom tqdm import tqdm\ndef train_from_manual(train_df, val_df, img_dir, batch_size, epoches, lr=0.001, ctx=mx.cpu()):\n    # TODO: finish trainer .etc, add ctx\n    steel_dataset = SteelDataset(train_df, img_dir)\n    steel_data = data.DataLoader(steel_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n    \n    val_steel_dataset = SteelDataset(val_df, img_dir)\n    val_steel_data = data.DataLoader(val_steel_dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n    \n    normal_focal_loss = NormalizedFocalLossSoftmax(ignore_label=-1, gamma=1)\n#     normal_focal_loss = SoftmaxCrossEntropyLoss()\n#     unet = SteelUnet(n_classes=5, ctx=ctx)\n#     unet.initialize(mx.init.Xavier(rnd_type='gaussian', magnitude=2), ctx=ctx)\n#     unet.feature_extractor.load_pretrained_weights()\n    \n    unet = ResNetUnet(output_channels=[256, 128, 64, 32], num_classes=5)\n    unet.initialize(mx.init.Xavier(rnd_type='gaussian', magnitude=2), ctx=ctx)\n    unet.load_pretrained_weights()\n    for k, v in unet.collect_params('.*beta|.*gamma|.*bias').items():\n        v.wd_mult = 0.0\n    lr_sche = lrs.FactorScheduler(step=5, base_lr=lr, factor=0.7,  warmup_steps=2, warmup_begin_lr=0.00002)\n    trainer = Trainer(unet.collect_params(), 'adam', \n                        {'learning_rate': lr,\n                         'wd':1e-5,\n#                          'lr_scheduler': lr_sche\n                        })\n    for epoch in range(epoches):\n        max_iou = -1\n        if epoch in [10, 15, 20, 25, 30]:\n            lr = lr * 0.7\n            trainer.set_learning_rate(lr=lr)\n        training(epoch, steel_data, unet, normal_focal_loss, trainer, ctx)\n        if epoch % 2 == 0:\n            val_iou = evaluation(val_steel_data, unet, ctx)\n            unet.save_parameters('unet_{}_{}.params'.format(epoch, max_iou))","34956f75":"batch_size = 12\ncsv_file = '..\/input\/severstal-steel-defect-detection\/train.csv'\nimg_dir = '..\/input\/severstal-steel-defect-detection\/train_images\/'\n\nepoches = 15\ntrain_from_manual(train_df, val_df, img_dir, batch_size, epoches, ctx=mx.gpu())","2e9927ad":"### Data augument with albumentations","5d3e045a":"### loss function\n- focal loss\n- normalize focal loss\n- dice loss\n- bce loss","6a68377a":"### FPN-based segmentation"}}