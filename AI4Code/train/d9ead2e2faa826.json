{"cell_type":{"ca1b4116":"code","bad2f786":"code","2f10912c":"code","bf95d208":"code","e51fb051":"code","99790f2a":"code","1c5e6183":"code","b73d9fad":"code","a0c7bd85":"code","07bc4aa4":"code","4692d6fa":"code","52346e0a":"code","dcdda9e5":"code","6cbc7b7b":"code","d6cc16b1":"code","e9978a6d":"code","e898723c":"markdown","a7af023f":"markdown","0d608efe":"markdown","2a383864":"markdown","f439a805":"markdown","cc0807cc":"markdown","013054dc":"markdown","ba6c0e80":"markdown","c035fbac":"markdown"},"source":{"ca1b4116":"!pip install sentence_transformers","bad2f786":"from sentence_transformers import util, SentenceTransformer\n\nimport pandas as pd\nimport os\n\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nimport numpy as np\nimport re\nimport random\nimport string","2f10912c":"VALID_DATA_PATH = \"..\/input\/jigsaw-toxic-severity-rating\/\"\n\nvalidation_df=pd.read_csv(os.path.join(VALID_DATA_PATH,'validation_data.csv'))\nvalidation_df","bf95d208":"SENTENCE_BERT_PATH=\"\/kaggle\/input\/sentence transformers\"\nmodel = SentenceTransformer('stsb-bert-base')","e51fb051":"def text_cleaning(text):\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n\n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n\n    emoji_pattern = re.compile(\"[\"\n                              u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                              u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                              u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                              u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                              u\"\\U00002702-\\U000027B0\"\n                              u\"\\U000024C2-\\U0001F251\"\n                              \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n\n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\nvalidation_df['less_toxic']=validation_df['less_toxic'].apply(text_cleaning)\nvalidation_df['more_toxic']=validation_df['more_toxic'].apply(text_cleaning)","99790f2a":"import time\n\nstart_time = time.time()\n\n#less_toxic, more_toxic\u3000 embedding\nless_embedding = model.encode(validation_df.less_toxic, convert_to_tensor=True)\nmore_embedding = model.encode(validation_df.more_toxic, convert_to_tensor=True)\n\npass_time=round(time.time() - start_time)\nprint(f'time:{pass_time}s')\nprint(less_embedding.shape)\nprint(more_embedding.shape)","1c5e6183":"#Calculating sentence-transformers\nscores_list=[]\n\nfor idx in range(len(less_embedding)):  \n    scores = util.pytorch_cos_sim(less_embedding[idx], more_embedding[idx])\n    scores=scores.squeeze().numpy()\n    scores_list.append(scores)","b73d9fad":"#for visible, convert DataFrame\nsentence_df = pd.DataFrame(scores_list, columns = ['sentence_transformers_score'])\nsentence_df.head()","a0c7bd85":"#\u3000sentence_transformers_score Distribution\nimport seaborn as sns\nsns.histplot(sentence_df['sentence_transformers_score'])","07bc4aa4":"high_score_index=[]\n\nfor index,score in enumerate(scores_list):\n    if score >= 0.7:\n        high_score_index.append(index)\n\nprint(len(high_score_index))","4692d6fa":"#show high_score_index\nprint(high_score_index[:10])","52346e0a":"print('more_toxic:','\\n',validation_df.loc[780,'more_toxic'],'\\n')\nprint('less_toxic:','\\n',validation_df.loc[780,'less_toxic']) ","dcdda9e5":"print('more_toxic:','\\n',validation_df.loc[781,'more_toxic'],'\\n') \nprint('less_toxic:','\\n',validation_df.loc[781,'less_toxic'],'\\n') \nprint('more_toxic:','\\n',validation_df.loc[782,'more_toxic'],'\\n') \nprint('less_toxic:','\\n',validation_df.loc[782,'less_toxic']) ","6cbc7b7b":"print('more_toxic:','\\n',validation_df.loc[1160,'more_toxic'],'\\n') \nprint('less_toxic:','\\n',validation_df.loc[1160,'less_toxic']) ","d6cc16b1":"print('more_toxic:','\\n',validation_df.loc[1290,'more_toxic'],'\\n') \nprint('less_toxic:','\\n',validation_df.loc[1290,'less_toxic']) ","e9978a6d":"print('more_toxic:','\\n',validation_df.loc[1402,'more_toxic'],'\\n')\nprint('less_toxic:','\\n',validation_df.loc[1402,'less_toxic']) ","e898723c":"This is beginner's room of sentence-transformers\n\n### There is a possibility of random selection when comparing the toxicity of toxic texts that are close in meaning.\n\n### In this notebook, It try to get closer to a reliable validation data by removing more & less text that are close in meaning from the validation data.\n\n### This notebook is one of the approaches to creating reliable validation data, which is described in the discussion [here.](http:\/\/www.kaggle.com\/c\/jigsaw-toxic-severity-rating\/discussion\/303429)","a7af023f":"It's able to extract text with similar meanings.","0d608efe":"# sentence-transformers","2a383864":"## Let's look at sample of scores above 0.7(high score sample)","f439a805":"781,782 is same text as 780","cc0807cc":"### Let's look at some other samples.\nHowever, these texts do not have a similar meaning.\n\nSince the max length of sentence-transformers is 128, and the validation data is more than 128,\nThis may be because the validation data does not fit into the maximum length of sentence-transformer.\n\nhttps:\/\/huggingface.co\/sentence-transformers\/stsb-bert-base","013054dc":"### Thank you for watching so far!\n\n### I will be happy if I can contribute to kaggler.","ba6c0e80":"# clean text\n\nhttps:\/\/www.kaggle.com\/vitaleey\/tfidf-ridge","c035fbac":"# import sentence-transformers"}}