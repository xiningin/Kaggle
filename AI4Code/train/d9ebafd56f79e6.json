{"cell_type":{"ea07bd41":"code","3524e57f":"code","4b679907":"code","26cf8a6c":"code","e284110e":"code","56daf0e2":"code","3db16355":"code","40484353":"code","bc907fed":"code","9042fca8":"code","4e445a15":"code","dd118756":"code","fb9db1ca":"code","8b9d4d3b":"code","a4a863eb":"code","339c82b4":"code","7e3c4e8a":"code","c59fd5b1":"code","01595b2a":"code","c43470b0":"code","1989660d":"code","01c377e5":"code","013e973f":"code","f0c28984":"code","263944bd":"code","989c82bb":"code","f15d4674":"markdown","de56e366":"markdown","94982fc7":"markdown","5edf6bfa":"markdown","3e7b055c":"markdown","7f83095b":"markdown","eb2510f3":"markdown","08b7f317":"markdown","0aceb0e4":"markdown","aa2f2455":"markdown","c6dd5921":"markdown","fe34fb96":"markdown","d48da66b":"markdown","398aec5f":"markdown","9c475135":"markdown","5f4fcf1b":"markdown","8c1212c9":"markdown","8b95faa2":"markdown","56b7c098":"markdown","b9e4458d":"markdown","28d37425":"markdown","12d02adc":"markdown","80d3964b":"markdown","50b296d1":"markdown","cc1051d2":"markdown","95dc6447":"markdown","cbe0fa82":"markdown","21a60611":"markdown","7ab56cf6":"markdown","4ac96460":"markdown","e29fc4f4":"markdown","ca8af4a5":"markdown","e660ecb7":"markdown","db8b6a67":"markdown","7337e2e5":"markdown","8a9190a8":"markdown","2fe60be7":"markdown","3b13e017":"markdown","3007ced8":"markdown","5b0e7fcd":"markdown","e44da69d":"markdown"},"source":{"ea07bd41":"import pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns\nimport glob\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\npd.set_option('max_columns', 1000)\n\nfrom sklearn.neighbors import BallTree\n\n#from BDBUtils.Utilities import CreateNFLData\nfrom IPython.core.display import HTML\nimport time\nimport math\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nfrom datetime import datetime\nimport pytz\nfrom IPython.display import HTML\nimport scipy.stats as stats\nimport matplotlib as mpl\nfrom matplotlib import animation, rc\nfrom matplotlib.patches import Rectangle, Arrow\nimport tensorflow as tf\n#from BDBUtils.Utilities import CreateNFLData\n\nimport os\nfrom pylab import rcParams\nimport warnings  \nwarnings.filterwarnings('ignore')\n\n\n\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    \n    return df\n\n\ndef get_dx_dy(radian_angle, dist):\n    dx = dist * math.cos(radian_angle)\n    dy = dist * math.sin(radian_angle)\n    return dx, dy\n\n\n\ndef create_football_field(linenumbers=True,\n                          endzones=True,\n                          highlight_line=False,\n                          highlight_line_number=50,\n                          highlighted_name='Line of Scrimmage',\n                          fifty_is_los=False,\n                          figsize=(12*2, 6.33*2)):\n    \"\"\"\n    Function that plots the football field for viewing plays.\n    Allows for showing or hiding endzones.\n    \"\"\"\n    rect = patches.Rectangle((0, 0), 120, 53.3, linewidth=0.1,\n                             edgecolor='r', facecolor='slategrey', zorder=0)\n\n    fig, ax = plt.subplots(1, figsize=figsize)\n    ax.add_patch(rect)\n\n    plt.plot([10, 10, 10, 20, 20, 30, 30, 40, 40, 50, 50, 60, 60, 70, 70, 80,\n              80, 90, 90, 100, 100, 110, 110, 120, 0, 0, 120, 120],\n             [0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3,\n              53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 53.3, 0, 0, 53.3],\n             color='white')\n    if fifty_is_los:\n        plt.plot([60, 60], [0, 53.3], color='gold')\n        plt.text(62, 50, '<- Player Yardline at Snap', color='gold')\n    # Endzones\n    if endzones:\n        ez1 = patches.Rectangle((0, 0), 10, 53.3,\n                                linewidth=0.3,\n                                edgecolor='k',\n                                facecolor='royalblue',\n                                alpha=0.4,\n                                zorder=1)\n        ez2 = patches.Rectangle((110, 0), 120, 53.3,\n                                linewidth=0.3,\n                                edgecolor='k',\n                                facecolor='royalblue',\n                                alpha=0.4,\n                                zorder=1)\n        ax.add_patch(ez1)\n        ax.add_patch(ez2)\n    plt.xlim(0, 230)\n    plt.ylim(0, 93.3)\n    plt.axis('off')\n    if linenumbers:\n        for x in range(20, 110, 10):\n            numb = x\n            if x > 50:\n                numb = 120 - x\n            plt.text(x, 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white')\n            plt.text(x - 0.95, 53.3 - 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white', rotation=180)\n    if endzones:\n        hash_range = range(11, 110)\n    else:\n        hash_range = range(1, 120)\n\n    for x in hash_range:\n        ax.plot([x, x], [0.4, 0.7], color='white')\n        ax.plot([x, x], [53.0, 52.5], color='white')\n        ax.plot([x, x], [22.91, 23.57], color='white')\n        ax.plot([x, x], [29.73, 30.39], color='white')\n\n    if highlight_line:\n        hl = highlight_line_number + 10\n        plt.plot([hl, hl], [0, 53.3], color='yellow')\n        plt.text(hl + 2, 50, '<- {}'.format(highlighted_name),\n                 color='yellow')\n    return fig, ax\n\n\nclass CreateNFLData:\n\n    def __init__(self):\n        pass\n\n    def LoadData(self, Normal=True):\n        if Normal == True:\n            print(\"Loading Original Data\")\n            globbed_files = glob.glob(\"..\/input\/nfl-big-data-bowl-2021\/week*.csv\") #creates a list of all csv files\n            data = []\n            for csv in tqdm(globbed_files):\n                frame = pd.read_csv(csv, index_col=0)\n                data.append(frame)\n\n            WeekData = pd.concat(data).reset_index()\n            WeekData\n        \n        else:\n            print(\"Loading Modified Data\")\n            globbed_files = glob.glob(\"Revised Data\/*.csv\") #creates a list of all csv files\n            data = []\n            for csv in tqdm(globbed_files):\n                frame = pd.read_csv(csv, index_col=0)\n                data.append(frame)\n\n            WeekData = pd.concat(data).reset_index()\n            WeekData\n        return WeekData\n\n\n\n    def Standardize(self,W):\n        print(\"Standardizing Data..\")\n        W['Dir_rad'] = np.mod(90 - W.dir, 360) * math.pi\/180.0\n        W['ToLeft'] = W.playDirection == \"left\"\n        W['TeamOnOffense'] = \"home\"\n        W.loc[W.possessionTeam != W.PlayerTeam, 'TeamOnOffense'] = \"away\"\n        W['IsOnOffense'] = W.PlayerTeam == W.TeamOnOffense # Is player on offense?\n        W['YardLine_std'] = 100 - W.yardlineNumber\n        W.loc[W.yardlineSide.fillna('') == W.possessionTeam,  \n                'YardLine_std'\n                ] = W.loc[W.yardlineSide.fillna('') == W.possessionTeam,  \n                'yardlineNumber']\n        W['X_std'] = W.x\n        W.loc[W.ToLeft, 'X_std'] = 120 - W.loc[W.ToLeft, 'x'] \n        W['Y_std'] = W.y\n        W.loc[W.ToLeft, 'Y_std'] = 160\/3 - W.loc[W.ToLeft, 'y'] \n        #W['Orientation_std'] = -90 + W.Orientation\n        #W.loc[W.ToLeft, 'Orientation_std'] = np.mod(180 + W.loc[W.ToLeft, 'Orientation_std'], 360)\n        W['Dir_std'] = W.Dir_rad\n        W.loc[W.ToLeft, 'Dir_std'] = np.mod(np.pi + W.loc[W.ToLeft, 'Dir_rad'], 2*np.pi)\n        W['dx'] = round(W['s']*np.cos(W['Dir_std']),2)\n        W['dy'] = round(W['s']*np.sin(W['Dir_std']),2)\n        W['X_std'] = round(W['X_std'],2)\n        W['Y_std'] = round(W['Y_std'],2)\n        #W['Orientation_rad'] = np.mod(W.o, 360) * math.pi\/180.0\n        W['Orientation_rad'] = np.mod(-W.o + 90, 360) * math.pi\/180.0\n        W['Orientation_std'] = W.Orientation_rad\n        W.loc[W.ToLeft, 'Orientation_std'] = np.mod(np.pi + W.loc[W.ToLeft, 'Orientation_rad'], 2*np.pi)\n        W['MPH'] = W['s'] \/ 0.488889\n        return W\n\n\n    def mergeGameData2(self,df):\n        print(\"Merging Data..\")\n        games = pd.read_csv('..\/input\/nfl-big-data-bowl-2021\/games.csv', index_col=0)\n        plays = pd.read_csv('..\/input\/nfl-big-data-bowl-2021\/plays.csv', index_col=0, usecols= ['gameId', 'playId','yardlineNumber','possessionTeam','yardlineSide'])\n        df = pd.merge(df, games, how='left', left_on=['gameId'], right_on=['gameId'])\n        df['PlayerTeam'] = np.where(df['team'] == 'home', df['homeTeamAbbr'], df['visitorTeamAbbr'] )\n        df = df.drop(['homeTeamAbbr','visitorTeamAbbr'], axis=1)\n        df = pd.merge(df, plays, how='left', left_on=['gameId', 'playId'], right_on=['gameId', 'playId'])\n        df['OnOffense'] = np.where(df['PlayerTeam'] == df['possessionTeam'], True, False)\n        TargetedWR = pd.read_csv('..\/input\/nfl-big-data-bowl-2021-bonus\/targetedReceiver.csv', index_col=0)\n        Coverages = pd.read_csv('..\/input\/nfl-big-data-bowl-2021-bonus\/coverages_week1.csv', index_col=0)\n        df = pd.merge(df, Coverages, how='left', left_on=['gameId','playId'], right_on=['gameId','playId'])\n        df = pd.merge(df, TargetedWR, how='left', left_on=['gameId','playId'], right_on=['gameId','playId'])\n        Offense = df.query('OnOffense == 1').reset_index()\n        Defense = df.query('OnOffense == 0').reset_index()\n        Football = df.query('displayName == \"Football\"').reset_index()\n        return df\n    \n    def PositionRank(self,df):\n        print(\"Creating Position Ranks..\")\n        group = df.groupby(['week','gameId','possessionTeam','targetNflId','playId'])['offensePlayResult','epa_x','yac_epa','air_epa','air_yards','complete_pass','touchdown'].max().reset_index()\n        group = group.groupby(['week','gameId','possessionTeam','targetNflId'])['offensePlayResult','epa_x','yac_epa','air_epa','air_yards','complete_pass','touchdown'].agg(['sum','count']).reset_index()\n        group.columns = [' '.join(col).strip() for col in group.columns.values]\n        group.columns = group.columns.str.replace('sum' , '')\n        #wrs= df.filter(['gameId','targetNflId',\"receiver_player_id\",\"complete_pass\",\"down\",\"qb_dropback\",\"air_yards\",\"yards_after_catch\",'yards_gained','shotgun','no_huddle','pass_touchdown','yardline_100','ydstogo','pass_length','pass_location','epa','air_epa','yac_epa'])\n        #wrs['yards_after_catch'] = wrs['yards_after_catch'].fillna(0)\n\n        #wrs['Targets'] = (wrs['yardline_100'] > 0).astype(int)\n\n        Cols = ['week', 'gameId','possessionTeam','targetNflId','offensePlayResult count','complete_pass ','touchdown ', 'offensePlayResult ', 'air_yards ']\n        group = group[Cols]\n        group.columns = ['week', 'gameId','Team','targetNflId','Targets','Rec','TD', 'Yards ', 'Air_yards']\n\n        group = group.drop_duplicates(subset=['week','gameId','targetNflId'])\n        Numdf = ['Targets','Rec','TD', 'Yards ', 'Air_yards']\n        for col in tqdm(Numdf):\n            group[col + '_Rolling'] = group.groupby(['targetNflId']).apply(lambda x: x[col].shift(1).expanding(1).mean()).reset_index(0,drop=True)\n\n        group['Targets_Rolling'] = group['Targets_Rolling'].fillna(group.groupby(['targetNflId'])['Targets_Rolling'].transform('median'))\n        group['Rec_Rolling'] = group['Rec_Rolling'].fillna(group.groupby(['targetNflId'])['Rec_Rolling'].transform('median'))\n        group['TD_Rolling'] = group['TD_Rolling'].fillna(group.groupby(['targetNflId'])['TD_Rolling'].transform('median'))\n        group['Yards _Rolling'] = group['Yards _Rolling'].fillna(group.groupby(['targetNflId'])['Yards _Rolling'].transform('median'))\n        group['Air_yards_Rolling'] = group['Air_yards_Rolling'].fillna(group.groupby(['targetNflId'])['Air_yards_Rolling'].transform('median'))\n        players = pd.read_csv('players.csv', index_col=0, usecols=['nflId','position','displayName'])\n        group = pd.merge(group, players, how='left',left_on=['targetNflId'],right_on=['nflId'])\n        group['Pos_Score'] = group['Targets_Rolling']*.35\t + group['Yards _Rolling']*.30 + group['Air_yards_Rolling']*.20 + group['TD_Rolling']*.15\n        group['Pos_Score'] = group['Pos_Score'].fillna(0)\n        group['Pos_Rank'] = group['position'] + round(group.groupby(['Team','gameId','position'])['Pos_Score'].rank(ascending=False),0).astype(int).astype(str)\n\n        Cols = ['gameId','targetNflId','Pos_Rank']\n        group1 = group[Cols]\n\n\n        Cols = [1,3,5,7,9,13]\n\n        for c in Cols:\n            trim1 = group[(group['Pos_Rank'] == \"WR1\") & (group['week'] == c) ].filter(['displayName','Team','Pos_Score','Pos_Rank'],axis=1).sort_values(by=['Pos_Score'], ascending=False).reset_index().head(5)\n            trim2 = group[(group['Pos_Rank'] == \"WR2\") & (group['week'] == c) ].filter(['displayName','Team','Pos_Score','Pos_Rank'],axis=1).sort_values(by=['Pos_Score'], ascending=False).reset_index().head(5)\n            print('Week ',str(c))\n            display(HTML(trim1.head(5).to_html()))\n            display(HTML(trim2.head(5).to_html()))\n   #     group1.to_csv('Pos_Rank.csv')\n        df1 = pd.merge(df, group1, how='left',left_on=['gameId','nflId'],right_on=['gameId','targetNflId'])\n        return df1\n    \n    def FrameData(self,df):\n        NotNone = df.query('event != \"None\"')\n        NotNone = NotNone.groupby(['gameId','playId','event'])['frameId'].max().reset_index()\n        NotNone = NotNone.set_index(['gameId','playId','event'], drop= True).unstack('event').reset_index()\n        NotNone.columns = [' '.join(col).strip() for col in NotNone.columns.values]\n        NotNone.columns = NotNone.columns.str.replace('frameId' , '')\n        NotNone.columns = NotNone.columns.str.replace(' ' , '')\n        NotNone = NotNone.filter(['gameId','playId','ball_snap', 'pass_arrived', 'pass_forward','penalty_flag', 'play_action','touchdown','pass_outcome_caught'], axis=1)\n        for col in tqdm(NotNone.columns):\n            NotNone['Contains_' + str(col)] = np.where(NotNone[col] > 0, True, False)\n\n\n        WeekData1 = pd.merge(df, NotNone, how=\"left\", left_on=['gameId','playId'], right_on=['gameId','playId'] )\n        WeekData1['After_snap'] = np.where(WeekData1['frameId'] > WeekData1['ball_snap'],1,0)\n        WeekData1['After_Throw'] = np.where(WeekData1['frameId'] > WeekData1['pass_forward'],1,0)\n        WeekData1['After_PassArrived'] = np.where(WeekData1['frameId'] > WeekData1['pass_arrived'],1,0)\n        WeekData1['After_PlayAction'] = np.where(WeekData1['frameId'] > WeekData1['play_action'],1,0)\n   #     WeekData1['After_run_pass_option'] = np.where(WeekData1['frameId'] > WeekData1['run_pass_option'],1,0)\n        WeekData1['After_Catch'] = np.where(WeekData1['frameId'] > WeekData1['pass_outcome_caught'],1,0)\n        return WeekData1\n    \n    def import_data(self,file):\n        \"\"\"create a dataframe and optimize its memory usage\"\"\"\n        df = pd.read_csv(file, low_memory=False)\n        df = reduce_mem_usage(df)\n        return df\n    \n\n    ","3524e57f":"import gc\n\nCreate = CreateNFLData()\n\nstart = time.process_time()\n\nWeeks = range(1,18)\n\n#globbed_files = glob.glob(\"..\/input\/revised-data\/*.csv\") #creates a list of all csv files\ndata = []\nfor n in tqdm(Weeks):\n    filename = '..\/input\/revised-data\/week' + str(n) + '.csv'\n    frame = Create.import_data(filename)\n    frame = frame.drop(['gameDate', 'gameTimeEastern','playDirection','yardlineSide','ToLeft', 'Opp_Dist_COpp.1'], axis=1)\n    data.append(frame)\n    gc.collect()\n\nWeekData = pd.concat(data).reset_index()\ngc.collect()\ndel data\nFinaldf = Create.FrameData(WeekData)\ndel WeekData\n#Finaldf = Finaldf.drop(['ball_snap', 'field_goal_blocked', 'field_goal_fake', 'first_contact', 'fumble', 'fumble_defense_recovered', 'fumble_offense_recovered', 'handoff', 'huddle_break_offense', 'huddle_start_offense', 'lateral', 'line_set', 'man_in_motion', 'out_of_bounds', 'pass_arrived', 'pass_forward', 'pass_lateral', 'pass_outcome_caught', 'pass_outcome_incomplete', 'pass_outcome_interception', 'pass_outcome_touchdown', 'pass_shovel', 'pass_tipped', 'penalty_accepted', 'penalty_flag', 'play_action', 'punt_fake', 'qb_sack', 'qb_spike', 'qb_strip_sack', 'run', 'shift','tackle',  'touchback', 'touchdown'], axis=1)\nFinaldf = Finaldf.drop(['ball_snap','pass_arrived',\t'pass_forward',\t'penalty_flag',\t'play_action','Contains_gameId','IsOnOffense',\t'Contains_playId',\t'Contains_ball_snap',\t'Contains_pass_arrived',\t'Contains_pass_forward'], axis=1)\nFinaldf['Code'] = Finaldf['gameId'].astype(str) + \"-\" + Finaldf['playId'].astype(str)\n\n\nplays = pd.read_csv('..\/input\/nfl-big-data-bowl-2021\/plays.csv', usecols=['gameId', 'playId','down', 'yardsToGo','penaltyCodes', 'penaltyJerseyNumbers', 'passResult', 'offensePlayResult', 'playResult', 'epa', 'isDefensivePI'])\n\n\nplays['Code'] = plays['gameId'].astype(str) + \"-\" + plays['playId'].astype(str)\nplays = plays.set_index('Code')\nplays = plays.loc[~plays.index.duplicated(keep='first')]\n\nCols = ['down', 'yardsToGo','penaltyCodes', 'penaltyJerseyNumbers', 'passResult', 'offensePlayResult', 'playResult', 'epa', 'isDefensivePI']\n\nfor col in tqdm(Cols):\n    Finaldf[col] = Finaldf.Code.map(plays[col])\n\n\n#Finaldf = pd.merge(Finaldf,plays, how='left', left_on=['gameId', 'playId'], right_on=['gameId', 'playId'])\n\ndel plays\nprint((time.process_time() - start) \/ 60)\nprint(Finaldf.shape)\ngc.collect()","4b679907":"Trim = Finaldf.filter(['gameId','playId','frameId','nflId','displayName','jerseyNumber','position','X_std',\t'Y_std','dx','dy','Orientation_std','MPH','a','Opp_Dist_COpp','closestOpp_Id','position_COpp','Pos_Rank_COpp','closestTeam_Id',\t'Team_Dist_CTm','position_CTm','FootDist'],axis=1)\nTrim.head(10)","26cf8a6c":"#Find the 2 most outside recievers by taking the max & min of the Y axis for each play\nOutsideWR = Finaldf.query('event == \"ball_snap\" & OnOffense == True ').groupby(['gameId','playId'])['Y_std'].agg(['min','max']).reset_index()\n\nShadow = Finaldf.query('position == \"CB\" & event == \"ball_snap\"').groupby(['gameId','displayName','Pos_Rank_COpp'])['X_std'].count().reset_index()\nShadow = Shadow.pivot_table(index=['gameId','displayName'], columns='Pos_Rank_COpp', values='X_std',aggfunc=np.sum, fill_value=0, margins=True).reset_index()\n\ncols = Shadow.drop(['gameId','displayName','All'], axis=1).columns\nShadow[cols] = Shadow[cols].div(Shadow[cols].sum(axis=1), axis=0)\nShadow = Shadow.query('gameId != \"All\"')\n\nShadow['WR1Shadow?'] = np.where((Shadow['WR1'] > .60) & (Shadow['All'] > 20), 1,0)\nShadow['WR2Shadow?'] = np.where((Shadow['WR2'] > .60) & (Shadow['All'] > 20), 1,0)\nShadow['WR3Shadow?'] = np.where((Shadow['WR3'] > .60) & (Shadow['All'] > 20), 1,0)\nShadow['TE1Shadow?'] = np.where((Shadow['TE1'] > .60) & (Shadow['All'] > 20), 1,0)\nShadow = Shadow.groupby(['gameId','displayName'])['WR1','WR1Shadow?','WR2','WR2Shadow?','WR3','WR3Shadow?','TE1','TE1Shadow?','All'].max().reset_index()\nShadow['gameId'] = Shadow['gameId'].astype(float)\n\ndel Trim\ngc.collect()\n\nShadow.query('displayName == \"Jalen Ramsey\"')","e284110e":"CBalign = Finaldf.query('position == \"CB\" & event == \"ball_snap\"').groupby(['gameId','playId','Y_std_COpp','displayName'])['Y_std','FootDist'].mean().reset_index()\nCBalign1 = pd.merge(CBalign, OutsideWR, how='left', left_on=['gameId','playId'], right_on=['gameId','playId'])\nCBalign1['Outside'] = np.where((CBalign1['Y_std_COpp'] == CBalign1['min']) | (CBalign1['Y_std_COpp'] == CBalign1['max']),1,0)\nCBalign1['Slot'] = np.where(CBalign1['Outside'] == 0,1,0)\n\ndel OutsideWR\ngc.collect()\n\nPlayer = CBalign1.groupby(['displayName','gameId'])['Outside','Slot'].sum().reset_index()\nPlayer['gameId'] = Player['gameId'].astype(float)\nPlayer = pd.merge(Player, Shadow, how='left', left_on=['displayName',\t'gameId'], right_on=['displayName',\t'gameId'])\nPlayer = Player.groupby(['displayName'])['Outside','Slot','WR1','WR1Shadow?','WR2','WR2Shadow?','WR3','WR3Shadow?','TE1','TE1Shadow?','All'].mean().reset_index()\n\ndel Shadow\ngc.collect()\n\nPlayer.query('displayName == \"Jalen Ramsey\"')","56daf0e2":"CBalign = Finaldf.query('position == \"CB\" & event == \"ball_snap\"').groupby(['displayName'])['Y_std','FootDist','Opp_Dist_COpp'].agg(['mean','std']).reset_index()\nCBalign.columns = [' '.join(col).strip() for col in CBalign.columns.values]\nCBalign.columns = CBalign.columns.str.replace('mean' , '')\nCBalign.columns = CBalign.columns.str.replace(' ' , '')\nCBalign = pd.merge(CBalign, Player, how='left', left_on=['displayName'], right_on=['displayName'])\nCBalign['Total'] = CBalign['Outside'] + CBalign['Slot']\nCBalign['OutsidePct'] = CBalign['Outside'] \/ CBalign['Total']\nCBalign['SlotPct'] = CBalign['Slot'] \/ CBalign['Total']\ndel Player\ngc.collect()\nCBalign.query('All > 20 ').filter(['displayName','All','OutsidePct','WR1','WR1Shadow?','WR2','WR2Shadow?','WR3','WR3Shadow?'],axis=1).sort_values(by=['WR1Shadow?'], ascending=False).head(10)","3db16355":"trimmed = Finaldf.query('After_snap == 1 & After_PassArrived == 0 & OnOffense == False')\n\nBase = trimmed.groupby(['gameId','playId','nflId','displayName','position','PlayerTeam','closestOpp_Id','Pos_Rank_COpp','possessionTeam','passResult','offensePlayResult','isDefensivePI','epa','Targeted'])['Opp_Dist_COpp'].agg(['first','last','var','mean','std','median','count']).reset_index()\nBase['Total'] = Base.groupby(['gameId','playId','displayName'])['count'].transform('sum')\nBase['FramePct'] = Base['count'] \/ Base['Total']\nBase['Coverage'] = np.where(Base['FramePct'] > .65,\"Man-to-Man\",'Zone')\nBase['mean'] = Base['mean'].astype(np.float32)\n\nRoutes = trimmed.groupby(['gameId','playId','displayName','position','closestOpp_Id','route_COpp'])['Opp_Dist_COpp','epa'].agg(['mean']).reset_index()\nRoutes[('Opp_Dist_COpp', 'mean')] = Routes[('Opp_Dist_COpp', 'mean')].astype(np.float32)\nRoutes = Routes.set_index(['gameId','playId','displayName','position','closestOpp_Id','route_COpp'], drop= True).unstack('route_COpp').reset_index()\nRoutes.columns = [' '.join(col).strip() for col in Routes.columns.values]\nRoutes.columns = Routes.columns.str.replace('mean' , '')\nRoutes.columns = Routes.columns.str.replace(' ' , '')\n\nEvents = trimmed.groupby(['gameId','playId','displayName','position','closestOpp_Id','event'])['Opp_Dist_COpp','epa'].agg(['mean']).reset_index()\nEvents[('Opp_Dist_COpp', 'mean')] = Events[('Opp_Dist_COpp', 'mean')].astype(np.float32)\nEvents = Events.set_index(['gameId','playId','displayName','position','closestOpp_Id','event'], drop= True).unstack('event').reset_index()\nEvents.columns = [' '.join(col).strip() for col in Events.columns.values]\nEvents.columns = Events.columns.str.replace('mean' , '')\nEvents.columns = Events.columns.str.replace(' ' , '')\n\nComb = pd.merge(Base, Events, how='left', left_on=['gameId','playId','displayName','position','closestOpp_Id'], right_on=['gameId','playId','displayName','position','closestOpp_Id'])\n\nComb = pd.merge(Comb, Routes, how='left', left_on=['gameId','playId','displayName','position','closestOpp_Id'], right_on=['gameId','playId','displayName','position','closestOpp_Id'])\n\ndel Routes\ndel Events\ndel trimmed\ngc.collect()\n\nComb","40484353":"Comb['Targeted'] = np.where(Comb['Targeted'] == True, 1, 0 )\nComb['Complete'] = np.where(Comb['passResult'] == \"C\", 1, 0 )\nComb['GroundCovered'] = Comb['Opp_Dist_COpppass_forward'] - Comb['Opp_Dist_COpppass_arrived']","bc907fed":"Man = Comb.query('Coverage == \"Man-to-Man\" & position == \"CB\" & Targeted == 1 & isDefensivePI == False')\n#Man = Man.groupby(['Defender','Defender_pos'])['mean','GO','HITCH','ANGLE','CORNER','CROSS','IN','OUT','POST','SCREEN','SLANT','WHEEL'].agg(['mean','count']).reset_index()\n#Man = Man.groupby(['Defender','Defender_pos'])['mean','Targeted','Complete','offensePlayResult'].agg(['mean','count']).reset_index()\n#Man.columns = Man.columns.map('_'.join)\n#Man = Man.sort_values(by=['mean_count'], ascending=False).head(100).sort_values(by=['Complete_mean'], ascending=True)\n#Man\nMan = Man.groupby(['nflId','displayName','position']).agg({'mean':[('Avg_Separation', 'mean'),('Num_of_Routes', 'count')],\n                                                            'Complete':[('Completions_Allowed', 'sum')],\n                                                            'offensePlayResult':[('Yards_Allowed', 'sum'),('AvgYards_Allowed', 'mean')],\n                                                            'GroundCovered':[('GroundCovered_Avg', 'mean')],\n                                                            'epa':[('EPA', 'mean')],\n                                                            'Opp_Dist_COppGO':[('Go_Separation', 'mean'),('Go_Routes', 'count')],\n                                                            'Opp_Dist_COppHITCH':[('HITCH_Separation', 'mean'),('HITCH_Routes', 'count')],\n                                                            'Opp_Dist_COppCROSS':[('CROSS_Separation', 'mean'),('CROSS_Routes', 'count')],\n                                                            'Opp_Dist_COppOUT':[('OUT_Separation', 'mean'),('OUT_Routes', 'count')],\n                                                            'Opp_Dist_COppPOST':[('POST_Separation', 'mean'),('POST_Routes', 'count')],\n                                                            'Opp_Dist_COppIN':[('IN_Separation', 'mean'),('IN_Routes', 'count')],\n                                                            'Opp_Dist_COppFLAT':[('FLAT_Separation', 'mean'),('FLAT_Routes', 'count')],\n                                                            'Opp_Dist_COppCORNER':[('CORNER_Separation', 'mean'),('CORNER_Routes', 'count')],\n                                                            'Opp_Dist_COppSLANT':[('SLANT_Separation', 'mean'),('SLANT_Routes', 'count')],\n                                                            'epaGO':[('EPA_GO', 'mean')],\n                                                            'epaHITCH':[('EPA_HITCH', 'mean')],\n                                                            'epaCROSS':[('EPA_CROSS', 'mean')],\n                                                            'epaOUT':[('EPA_OUT', 'mean')],\n                                                            'epaPOST':[('EPA_POST', 'mean')],\n                                                            'epaIN':[('EPA_IN', 'mean')],\n                                                            'epaFLAT':[('EPA_FLAT', 'mean')],\n                                                            'epaCORNER':[('EPA_CORNER', 'mean')],\n                                                            'epaSLANT':[('EPA_SLANT', 'mean')],}).reset_index(drop=False)\nMan.columns = Man.columns.map('_'.join)\nMan['Comp_pct'] = Man['Complete_Completions_Allowed'] \/ Man['mean_Num_of_Routes']\nMan = Man.sort_values(by=['mean_Num_of_Routes'], ascending=False).head(100).sort_values(by=['epa_EPA'], ascending=True)\n\nFinal = pd.merge(Man, CBalign, how='left', left_on=['displayName_'], right_on=['displayName'])\ndel Man\ndel Comb\ngc.collect()\nFinal.head(10)","9042fca8":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nNumdf = Final.select_dtypes(include=numerics)\n#Numdf = Numdf.drop(['GameID'], axis=1)\nCols = ['Outside', 'Slot']\nNumdf = Numdf[Cols]\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nx = SimpleImputer().fit_transform(Numdf)\nx = StandardScaler().fit_transform(x)\n\nimport scipy.cluster.hierarchy as sch\n\ndendrogram = sch.dendrogram(sch.linkage(x, method = 'ward'))\nplt.title('Dendrogam', fontsize = 20)\nplt.xlabel('Defenders')\nplt.ylabel('Ecuclidean Distance')\nplt.show()","4e445a15":"# for basic mathematics operation \nimport numpy as np\nimport pandas as pd\nfrom pandas import plotting\n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n# for interactive visualizations\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected = True)\nimport plotly.figure_factory as ff\n\nfrom sklearn.cluster import KMeans\n#x = CBalign.drop(['Defender'], axis=1).values\n\nfrom sklearn.cluster import KMeans\nx = Final[['Outside', 'Slot','WR1Shadow?']].values\nkm = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 1000, n_init = 10, random_state = 0)\nkm.fit(x)\nlabels = km.labels_\ncentroids = km.cluster_centers_\n\nFinal['labels'] =  labels\ntrace1 = go.Scatter3d(surfacecolor='darkgrey',\n    x= Final['Outside'],\n    y= Final['Slot'],\n    z= Final['WR1Shadow?'],\n    mode='markers',text=Final['displayName'],\n     marker=dict(\n        color = Final['labels'], \n        colorscale='Jet',\n        size= 5,\n        line=dict(\n            color= Final['labels'],\n            width= 10\n        ),\n        opacity=0.8\n     )\n)\ndf = [trace1]\n\nlayout = go.Layout(\n    title = 'Corner Types',\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0  \n    ),\n    scene = dict(\n            xaxis = dict(title  = 'Outside'),\n            yaxis = dict(title  = 'Slot'),\n            zaxis = dict(title  = 'WR1 Shadow %')\n        )\n)\n\nfig = go.Figure(data = df, layout = layout)\npy.iplot(fig)","dd118756":"Cols = ['nflId_', 'displayName_','labels','mean_Avg_Separation', 'mean_Num_of_Routes', 'epa_EPA', 'Comp_pct','offensePlayResult_AvgYards_Allowed', 'GroundCovered_GroundCovered_Avg','epaGO_EPA_GO', 'Opp_Dist_COppGO_Go_Separation', 'Opp_Dist_COppGO_Go_Routes', 'epaHITCH_EPA_HITCH','Opp_Dist_COppHITCH_HITCH_Separation', 'Opp_Dist_COppHITCH_HITCH_Routes', 'epaCROSS_EPA_CROSS',  'Opp_Dist_COppCROSS_CROSS_Separation', 'Opp_Dist_COppCROSS_CROSS_Routes', 'epaOUT_EPA_OUT', 'Opp_Dist_COppOUT_OUT_Separation', 'Opp_Dist_COppOUT_OUT_Routes', 'epaPOST_EPA_POST', 'Opp_Dist_COppPOST_POST_Separation', 'Opp_Dist_COppPOST_POST_Routes', 'epaIN_EPA_IN', 'Opp_Dist_COppIN_IN_Separation', 'Opp_Dist_COppIN_IN_Routes', 'epaFLAT_EPA_FLAT', 'Opp_Dist_COppFLAT_FLAT_Separation', 'Opp_Dist_COppFLAT_FLAT_Routes', 'epaCORNER_EPA_CORNER',  'Opp_Dist_COppCORNER_CORNER_Separation', 'Opp_Dist_COppCORNER_CORNER_Routes','epaSLANT_EPA_SLANT', 'Opp_Dist_COppSLANT_SLANT_Separation', 'Opp_Dist_COppSLANT_SLANT_Routes','Y_std',\t'Y_stdstd',\t'FootDist',\t'FootDiststd',\t'Opp_Dist_COpp',\t'Opp_Dist_COppstd',\t'Outside',\t'Slot',\t'WR1',\t'WR1Shadow?',\t'WR2',\t'WR2Shadow?',\t'WR3',\t'WR3Shadow?',\t'TE1',\t'TE1Shadow?',\t'All',\t'Total',\t'OutsidePct',\t'SlotPct']\nFinal2 = Final[Cols]\n\ndel Final\ngc.collect()\n\nFinal2.columns = ['nflId', 'Name','Cluster','Avg_Separation', 'Num_of_Routes', 'Avg_EPA', 'Comp_pct', 'AvgYards_Allowed', 'GroundCovered_Avg','EPA_GO', 'Go_Separation', 'Go_Routes', 'EPA_HITCH','HITCH_Separation', 'HITCH_Routes', 'EPA_CROSS',  'CROSS_Separation', 'CROSS_Routes', 'EPA_OUT', 'OUT_Separation', 'OUT_Routes', 'EPA_POST', 'POST_Separation', 'POST_Routes', 'EPA_IN', 'IN_Separation', 'IN_Routes', 'EPA_FLAT', 'FLAT_Separation', 'FLAT_Routes', 'EPA_CORNER',  'CORNER_Separation', 'CORNER_Routes','EPA_SLANT', 'SLANT_Separation', 'SLANT_Routes','Avg_Yaxis',\t'Yaxis_std','AvgFootball_DistAtSnap',\t'Football_DistAtSnap_std',\t'AvgOpp_Dist_COpp_AtSnap',\t'Opp_Dist_COppstd_AtSnap',\t'Outside',\t'Slot',\t'WR1',\t'WR1Shadow?',\t'WR2',\t'WR2Shadow?',\t'WR3',\t'WR3Shadow?',\t'TE1',\t'TE1Shadow?',\t'All',\t'Total',\t'OutsidePct',\t'SlotPct']\nFinal2.query('Cluster == 0').head(3)","fb9db1ca":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nNumdf = Final2.select_dtypes(include=numerics)\nCols = ['Avg_Separation', 'Avg_EPA', 'Comp_pct', 'AvgYards_Allowed', 'GroundCovered_Avg', 'EPA_GO', 'Go_Separation', 'EPA_HITCH', 'HITCH_Separation', 'EPA_CROSS', 'CROSS_Separation', 'EPA_OUT', 'OUT_Separation','EPA_POST', 'POST_Separation', 'EPA_IN', 'IN_Separation', 'EPA_FLAT', 'FLAT_Separation', 'EPA_CORNER', 'CORNER_Separation','EPA_SLANT', 'SLANT_Separation']\ndf1 = Final2.filter(['nflId','Name','Cluster'],axis=1)\n\nfor col in Cols:\n    df1[col + '_rank'] = Final2.groupby(['Cluster'])[col].rank(ascending=True)\n\ndf1['Score'] = df1.iloc[:,-len(Cols):].mean(axis = 1, skipna = True)\nRankings = df1.filter(['nflId','Score'],axis=1).sort_values(by='Score',ascending=True)\nRankings = Rankings.set_index('nflId')\nRankings = Rankings.loc[~Rankings.index.duplicated(keep='first')]\nFinal2[\"Rank\"] = Final2.nflId.map(Rankings['Score'])\n\n\ndisplay(HTML(Final2.head(5).to_html()))\n\ndisplay(HTML(df1.head(5).to_html()))\n#df1.query('Name == \"Josh Norman\"')","8b9d4d3b":"display(HTML(df1.query('Cluster == 0').filter(['Name','Score'],axis=1).sort_values(by='Score',ascending=True).head(10).to_html()))","a4a863eb":"display(HTML(df1.query('Cluster == 1').filter(['Name','Score'],axis=1).sort_values(by='Score',ascending=True).head(10).to_html()))","339c82b4":"display(HTML(df1.query('Cluster == 2').filter(['Name','Score'],axis=1).sort_values(by='Score',ascending=True).head(10).to_html()))","7e3c4e8a":"import plotly.graph_objects as go\nimport plotly.express as px\nfrom itertools import cycle\n\nTop10 = df1.query('Cluster == 0').sort_values(by='Score',ascending=True).head(10)\n\npalette = cycle(px.colors.qualitative.Bold)\npalette = cycle(px.colors.sequential.dense)\n\nfig = go.Figure(data=[\n    go.Bar(name='GO', x=Top10.Name, y=Top10.Go_Separation_rank,marker_color=next(palette)),\n    go.Bar(name='Hitch', x=Top10.Name, y=Top10.HITCH_Separation_rank,marker_color=next(palette)),\n    go.Bar(name='CROSS', x=Top10.Name, y=Top10.CROSS_Separation_rank,marker_color=next(palette)),\n    go.Bar(name='OUT', x=Top10.Name, y=Top10.OUT_Separation_rank,marker_color=next(palette)),\n    go.Bar(name='POST', x=Top10.Name, y=Top10.POST_Separation_rank,marker_color=next(palette)),\n    go.Bar(name='IN', x=Top10.Name, y=Top10.IN_Separation_rank,marker_color=next(palette)),\n    go.Bar(name='CORNER', x=Top10.Name, y=Top10.FLAT_Separation_rank,marker_color=next(palette)),\n    go.Bar(name='FLAT', x=Top10.Name, y=Top10.CORNER_Separation_rank,marker_color=next(palette)),\n    go.Bar(name='SLANT', x=Top10.Name, y=Top10.SLANT_Separation_rank,marker_color=next(palette)),\n\n])\n# Change the bar mode\nfig.update_layout(barmode='group', template='plotly_dark', title=\"Avg. Separation Rank By Route\")\nfig.update_layout(\n    autosize=False,\n    width=1200,\n    height=500,\n)\nfig.show()","c59fd5b1":"import plotly.graph_objects as go\nimport plotly.express as px\nfrom itertools import cycle\n\npalette = cycle(px.colors.qualitative.Bold)\npalette = cycle(px.colors.diverging.curl)\n#palette = cycle(px.colors.cyclical.Edge)\n#palette = cycle(px.colors.sequential.tempo)\n\nTop10 = df1.query('Cluster == 0').sort_values(by='Score',ascending=True).head(10)\n\nfig = go.Figure(data=[\n    go.Bar(name='GO', x=Top10.Name, y=Top10.EPA_GO_rank,marker_color=next(palette)),\n    go.Bar(name='Hitch', x=Top10.Name, y=Top10.EPA_HITCH_rank,marker_color=next(palette)),\n    go.Bar(name='CROSS', x=Top10.Name, y=Top10.EPA_CROSS_rank,marker_color=next(palette)),\n    go.Bar(name='OUT', x=Top10.Name, y=Top10.EPA_OUT_rank,marker_color=next(palette)),\n    go.Bar(name='POST', x=Top10.Name, y=Top10.EPA_POST_rank,marker_color=next(palette)),\n    go.Bar(name='IN', x=Top10.Name, y=Top10.EPA_IN_rank,marker_color=next(palette)),\n    go.Bar(name='CORNER', x=Top10.Name, y=Top10.EPA_CORNER_rank,marker_color=next(palette)),\n    go.Bar(name='FLAT', x=Top10.Name, y=Top10.EPA_FLAT_rank,marker_color=next(palette)),\n    go.Bar(name='SLANT', x=Top10.Name, y=Top10.EPA_SLANT_rank,marker_color=next(palette)),\n\n])\n# Change the bar mode\nfig.update_layout(barmode='group', template='plotly_dark', title=\"Avg. EPA Rank By Route\")\nfig.update_layout(\n    autosize=False,\n    width=1200,\n    height=500,\n)\nfig.show()","01595b2a":"Speed = Finaldf.query('displayName != \"Football\" & gameId != 2018102101 & position == \"CB\"').groupby(['nflId','week','gameId','playId'])['s','a','dis'].max().reset_index()\nSpeed['MPH'] = Speed['s'] \/ 0.488889\nSpeed1 = Speed.groupby(['nflId','week','gameId'])['MPH'].apply(lambda x: x.nlargest(2).mean()).reset_index()\nSpeed1['a'] = Speed.groupby(['nflId','week','gameId'])['a'].apply(lambda x: x.nlargest(2).mean()).reset_index()['a']\nSpeed1['dis'] = Speed.groupby(['nflId','week','gameId'])['dis'].apply(lambda x: x.nlargest(2).mean()).reset_index()['dis']\nSpeed1 = Speed1.groupby(['nflId'])['MPH','a','dis'].mean().reset_index()\nSpeed1.sort_values(by=['MPH'], ascending=False).head(10)","c43470b0":"Speed1 = Speed1.set_index('nflId')\nSpeed1 = Speed1.loc[~Speed1.index.duplicated(keep='first')]\nFinal2[\"Speed\"] = Final2.nflId.map(Speed1['MPH'])\nFinal2[\"Acceleration\"] = Final2.nflId.map(Speed1['a'])\nFinal2[\"yds\/frame\"] = Final2.nflId.map(Speed1['dis'])\nFinal2.head(5)","1989660d":"fig = px.scatter(Final2, x=\"Speed\", y=\"Acceleration\", \n                 color=\"Name\",\n                 width=800, height=700,template='plotly_dark')\nfig.show()","01c377e5":"import requests\nfrom pandas.io.json import json_normalize\nfrom bs4 import BeautifulSoup\nimport re\n\ndef findTables(url):\n    res = requests.get(url)\n    comm = re.compile(\"<!--|-->\")\n    soup = BeautifulSoup(comm.sub(\"\", res.text), 'lxml')\n    divs = soup.findAll('div', id = \"content\")\n    divs = divs[0].findAll(\"div\", id=re.compile(\"^all\"))\n    ids = []\n    for div in divs:\n        searchme = str(div.findAll(\"table\"))\n        x = searchme[searchme.find(\"id=\") + 3: searchme.find(\">\")]\n        x = x.replace(\"\\\"\", \"\")\n        if len(x) > 0:\n            ids.append(x)\n    return(ids)\n\ndef pullTable(url, tableID, header = True):\n    res = requests.get(url)\n    ## Work around comments\n    comm = re.compile(\"<!--|-->\")\n    soup = BeautifulSoup(comm.sub(\"\", res.text), 'lxml')\n    tables = soup.findAll('table', id = tableID)\n    data_rows = tables[0].findAll('tr')\n    game_data = [[td.getText() for td in data_rows[i].findAll(['th','td'])]\n        for i in range(len(data_rows))\n        ]\n    data = pd.DataFrame(game_data)\n    if header == True:\n        data_header = tables[0].findAll('thead')\n        data_header = data_header[0].findAll(\"tr\")\n        data_header = data_header[0].findAll(\"th\")\n        header = []\n        for i in range(len(data.columns)):\n            header.append(data_header[i].getText())\n        data.columns = header\n        data = data.loc[data[header[0]] != header[0]]\n    data = data.reset_index(drop = True)\n    return(data)\n\n\ndef PullData(df, start, end):\n    Db_Names = df['Name'].to_list()\n    Pos = ['CB','DB','FS','SS','S']\n    combinedata = []\n    Years = range(start, end+1)\n    for yr in Years:\n        print(yr, end=' ')\n        URL = ('https:\/\/www.pro-football-reference.com\/draft\/' + str(yr) + '-combine.htm')\n        id = 'combine'\n        data1 = pullTable(URL, id, header=True)\n        data1['Year'] = yr\n        data1 = data1[(data1['Player'].isin(Db_Names)) & (data1['Pos'].isin(Pos)) ]\n        combinedata.append(data1)\n    final = pd.concat(combinedata)\n    final.rename(columns = {\"Player\": \"Name\"},inplace = True) \n    final = final.set_index('Name')\n    final = final.loc[~final.index.duplicated(keep='first')]\n    df[\"Ht\"] = df.Name.map(final['Ht']).apply(lambda x: np.nan if pd.isnull(x) else 12*int(x.split('-')[0])+int(x.split('-')[1]))\n    df['Wt'] = df.Name.map(final['Wt'].astype(float))\n    df['40yd'] = df.Name.map(final['40yd'].astype(float))\n    df['Vertical'] = df.Name.map(final['Vertical'])\n    df['Bench'] = df.Name.map(final['Bench'])\n    df['Broad Jump'] = df.Name.map(final['Broad Jump'])\n    df['3Cone'] = df.Name.map(final['3Cone'])\n    df['Shuttle'] = df.Name.map(final['Shuttle'])\n    df['YearsInLeague'] = 2018 - df.Name.map(final['Year'].astype(int))\n #   df[\"Ht\"] = np.where(df[\"Ht\"].notnull() == True,df[\"Ht\"].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1])),df[\"Ht\"])\n    return df\n\n\ndef change_names(x):\n    return x.replace('0','Outside Corners').replace('1','Slot Corners').replace('2',\"Hybrid Corners\")","013e973f":"data = PullData(Final2,2000, 2019)\ndata['Cluster_Rank'] = data.groupby(['Cluster'])['Rank'].rank(ascending=True)\n\n\n\ndata = data.replace(r'^\\s*$', np.nan, regex=True)\n\ndata['40yd'] = data['40yd'].astype(float)\ndata['Vertical'] = data['Vertical'].astype(float)\ndata['Bench'] = data['Bench'].astype(float)\ndata['Broad Jump'] = data['Broad Jump'].astype(float)\ndata['3Cone'] = data['3Cone'].astype(float)\ndata['Shuttle'] = data['Shuttle'].astype(float)\n\n\ndata['Tier'] = np.where(data['Cluster_Rank'] <= 10,\"Top Ten\",\"Middle\")\ndata['Worst_ClustRank'] = data.groupby(['Cluster'])['Cluster_Rank'].transform('max')\ndata['Tier'] = np.where(data['Cluster_Rank'] \/ data['Worst_ClustRank'] > .70,\"Worst\",data['Tier'])\n\ndata['Ht'] = data['Ht'].fillna(data.groupby(['Cluster'])['Ht'].transform('median'))\ndata['Wt'] = data['Wt'].fillna(data.groupby(['Cluster'])['Wt'].transform('median'))\ndata['40yd'] = data['40yd'].fillna(data.groupby(['Cluster'])['40yd'].transform('median'))\ndata['Vertical'] = data['Vertical'].fillna(data.groupby(['Cluster'])['Vertical'].transform('median'))\ndata['Bench'] = data['Bench'].fillna(data.groupby(['Cluster'])['Bench'].transform('median'))\ndata['Broad Jump'] = data['Broad Jump'].fillna(data.groupby(['Cluster'])['Broad Jump'].transform('median'))\ndata['3Cone'] = data['3Cone'].fillna(data.groupby(['Cluster'])['3Cone'].transform('median'))\ndata['Shuttle'] = data['Shuttle'].fillna(data.groupby(['Cluster'])['Shuttle'].transform('median'))\ndata['YearsInLeague'] = data['YearsInLeague'].fillna(data.groupby(['Cluster'])['YearsInLeague'].transform('median'))\ndata['Cluster'] = data['Cluster'].astype(str).map(lambda x: change_names(x))\ndata","f0c28984":"!pip install joypy","263944bd":"def CreatePlots(df, feature):\n    import joypy\n    %matplotlib inline\n\n    df[\"Top Ten Tier\"] = df.apply(lambda row: row[feature] if row[\"Tier\"] == \"Top Ten\" else np.nan, axis = 1)\n    df[\"Middle Tier\"] = df.apply(lambda row: row[feature] if row[\"Tier\"] == \"Middle\" else np.nan, axis = 1)\n    df[\"Worst Tier\"] = df.apply(lambda row: row[feature] if row[\"Tier\"] == \"Worst\" else np.nan, axis = 1)\n\n\n    plt.figure(dpi = 4000)\n    # plot the data using joypy\n    fig, axes = joypy.joyplot(data, \n                            column=['Top Ten Tier', 'Middle Tier', 'Worst Tier'], # colums to be plotted.\n                            by = \"Cluster\", # separate the data by this value. Creates a separate distribution for each one.\n                            ylim = 'own', \n                            figsize = (10,7), \n                            legend = True, \n                            color = [ '#66ffcc','#4d94ff', '#ff9966'],\n                            background='white',\n                            alpha = 0.6, linecolor=\"k\",linewidth=3,ylabels=True,xlabels=True,\n                            )\n\n    plt.title(str(feature) + ' distribution of Tiers by the Cluster', fontsize = 35, color = 'Black', alpha = 1)\n    plt.rc(\"font\", size = 20)\n    plt.xlabel(str(feature) + ' of Player by Cluster',  fontsize = 16, color = 'Black', alpha = 1)\n    plt.ylabel('Cluster',  fontsize = 16, color = 'Black', alpha = 1);\n    plt.show()","989c82bb":"Cols = ['Avg_Separation', 'Avg_EPA', 'Comp_pct', 'AvgYards_Allowed', 'GroundCovered_Avg', 'Speed', 'Acceleration', 'yds\/frame', 'Ht', 'Wt', '40yd', 'Vertical', 'Bench', 'Broad Jump', '3Cone', 'Shuttle', 'YearsInLeague']\n\nfor col in Cols:\n    CreatePlots(data, col)","f15d4674":"------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","de56e366":"# Clustering Analysis:\n\nWe will cluster the cornerbacks based on their alignment to appropriately evaluate them. Outside cornerbacks are generally the best cornerbacks for their team and have a different playing style than a slot corner. Slot corners are lined up predominately opposite the inside recevier, and are usually quicker than most outside corners\n\nSo we will create 3 groups: Outside corners, Slot corners, and Hybrids (play slot and outside)\n","94982fc7":"# Combine Metrics:\n\nWe are going to scrape https:\/\/www.pro-football-reference.com\/ and get combine data for the following players. This will help us see if any of the metrics like 40 yard dash, 3 cone drill, shuttle, etc. are significant in player evaluation.","5edf6bfa":"## Other Work:\n\n1. [Shadow Cornerback + Coverage Analysis](https:\/\/www.kaggle.com\/jdruzzi\/shadow-cornerback-coverage-analysis)\n\n2. [Defender Bite Velocity on Play-Action](https:\/\/www.kaggle.com\/jdruzzi\/defender-bite-velocity-on-play-action)\n\n3. [Pass Coverage Classification](https:\/\/www.kaggle.com\/jdruzzi\/pass-coverage-classification-80-recall)\n\n4. [Quantifying Press Coverage](https:\/\/www.kaggle.com\/jdruzzi\/quantifying-press-coverage-ability)\n\n5. [Defender Tendencies: One-Cut Routes + Double Moves](https:\/\/www.kaggle.com\/jdruzzi\/defender-tendencies-one-cut-routes-double-moves)\n\n## Data:\n\n[Revised BDB Data](https:\/\/www.kaggle.com\/jdruzzi\/revised-bdb-data)","3e7b055c":"![](https:\/\/jaguarswire.usatoday.com\/wp-content\/uploads\/sites\/46\/2019\/08\/usatsi_11396149.jpg?resize=900,446)","7f83095b":"![](https:\/\/nfl-static.s3.amazonaws.com\/attachments\/ckdhpvebs087j0iodyvyrb7b6-nfl-flag-play-routetree1-1.44.106.986.571.max.png)","eb2510f3":"Here you can see AJ Green blowing right by cornerback Antwon Blake, he simply was not able to make up the ground necessary to stop Green from making the catch","08b7f317":"# Speed Metrics:\n\n- MPH\n- Acceleration\n- Yards\/frame\n\nWe will take the top 2 best values for each speed metric, of each player in each game, then average them out to get a good gauge on how fast\/quick they are. Grabbing the top 2 speeds is to ensure there are no outliers in the data, I did find erroneous tracking speeds in gameid 2018102101","0aceb0e4":"![](https:\/\/www.pennlive.com\/resizer\/kwEG97BhW1r6R3M8K_fQakZh-Jo=\/1280x0\/smart\/cloudfront-us-east-1.images.arcpublishing.com\/advancelocal\/6VQZXVVQQRDFFK6MA32VUSX4OQ.jpg)","aa2f2455":"# Final Thoughts:\n\nIf we focus solely on Top Tier outside corners, based on the plots above, it appears that the elite group may have a slight speed and size advantage. Which makes complete sense, size gives them the ability to contest catches or be more physical at the line of scrimmage and speed allows them to run stride for stride next to any receiver. \n\nA decent amount of the elite cover corners consistently hit near 19 mph on average. What I found interesting was the 40 yard dash distributions didn't really align with the MPH distribution, are track speed and game speed really two different things?\n\nAlso, the 3 cone drill and shuttle are combine tests to measure quickness and change of direction, but in each cluster group you could see that there was hardly any difference between tiers.\n\n\n# Who will find this submission useful:\n\n- Fantasy sites \/ private companies that want to use tracking data as a means to gather shadow coverage statistics. I know PFF tracks shadow stats, but their method of doing so could be having to physically watch each individual play which is time consuming, costly and prone to human error. I think the whole process could be automated or semi-automated.\n\n- Inspiring Fantasy sites \/ private companies to look at new ways of feature engineering new variables that their end users will find useful\n\n- NFL Analytics staffers may gain some insight or inspiration from this submission\n\n- A curious NFL fan","c6dd5921":"# Marlon Humphrey\n\n### - Average EPA Cluster Rank: 1st \ud83e\udd47\n\n### - Average Completion Percent Cluster Rank: 1st \ud83e\udd47\n\n### - Average Separation Cluster Rank: 2nd \ud83e\udd48\n\n### - Average Yards Allowed Cluster Rank: 3rd \ud83e\udd49","fe34fb96":"You can see that Humphrey rarely let's his guy get a step on him, on nearly every route.","d48da66b":"#### Credits that inspired the code necessary for this analysis:\n 1. @CPMP's Big Data Bowl 2020 initial wrangling & Voronoi areas notebook - for standardizing tracking data\n 2. @Shahsquatch's Field Control Model & @robikscube's notebook inspired the play visualizations plot\n 3. @AndikaRachman's Identifying Coverage Scheme code helped expedite my revised data, showing distance between players, that code can be found in my other notebook\n 4. @tombliss's additional data provided - coverage scheme and targeted receviers\n \n The data used has already been preprocessed and revised in my [other notebook](https:\/\/www.kaggle.com\/jdruzzi\/revised-bdb-data) to save time and RAM. I recommend running the other notebook locally due to kaggles RAM constraints","398aec5f":"# Ranking System:\n\nInstead of taking these metrics at face value, let's rank how well the defender did for each variable compared to others in their cluster. Then we average the rankings across the board to get a single score value to determine who was really the best at their respective cluster positions.\n\nThe lower the score, the better!","9c475135":"#                                              Shadow Cornerback + Coverage Analysis \ud83d\udd12\ud83c\udfc8\ud83c\udfc6","5f4fcf1b":"## Hybrid Corners","8c1212c9":"Here are the corners that generally shadow the WR1","8b95faa2":"# Separation:\n\nWe will take a look into how the corner's euclidean distance (Separation) from the closest offensive player fares over all of the events(snap of the ball, play action, etc.) and for each route in the route tree given in the dataset(Slant, Hitch, etc.), the distance metric will be called \"Opp_Dist_COpp\"\n\nWell in order to give a fair evaluation, we would need to determine if the defensive playcall is zone or man. Zone coverage can skew separation metrics, due to the defender doing their job and staying a certain zone on the field, so we will have to determine man coverage. I did this by taking into account every frame the defender stayed with the closest offensive player. My assumptions were if they stayed with the recevier for more 60% of the frames, then it can be classified as true man-to-man coverage snap. Obviously, there are flaws to this logic, but for time and energy sake this should suffice. I may delve into predicting whether it's man \/ zone in another submission, and build off of this [academic paper](https:\/\/arxiv.org\/pdf\/1906.11373.pdf)\n\n# EPA:\n\nIn addition to separation metrics, I will also track Expected Points Allowed (EPA) for each situation. I will not going into the specifics of EPA, but please refer to [this](https:\/\/www.stat.cmu.edu\/~ryurko\/files\/greatlakes_2017.pdf) if you're interested. EPA can be used as an evaluation metric, \"estimates a play\u2019s value based on the change in situation, providing a point value\". For offensive players a high EPA is good, for defensive players a low EPA is good.","56b7c098":"![](https:\/\/images.actionnetwork.com\/blog\/2018\/11\/brad.gif)","b9e4458d":"The dendrogram confirms our theory","28d37425":"Since outside corners are generally the best of the bunch, I'm crowning **Marlon Humphrey** as the best corner in 2018\n\n# The Fairest Shadow Corner of them all in 2018: \n","12d02adc":"Here's a decent image I was able to find that showed some of the routes that a recevier runs.","80d3964b":"## Outside Corners","50b296d1":"![](https:\/\/wp.usatodaysports.com\/wp-content\/uploads\/sites\/76\/2015\/12\/antwon-blake.gif)","cc1051d2":"In this revised dataset, I manipulated the data by standardizing all of the player tracking data to better help our analysis. I've also included some metrics like the euclidean distance between opposing teams closest player, as well as, players from the same team. Here are some key variables\n\n###### COpp = Closest Opponent\n###### CTm = Closest Teammate","95dc6447":"# Objective:\n\n### 1. Identify Shadow Corners\n\n### 2. Evaluate them\n\n### 3. Identify certain characteristics about them","cbe0fa82":"Humphrey's weakness, if any, looks to be an out route","21a60611":"# Speed Vs. Acceleration","7ab56cf6":"# Separation Vs. Routes:\n\nLet's evaluate top cornerbacks and their separation\/EPA ranks across the route tree, some defenders may struggle to maintain close separation depending on the route.","4ac96460":"# Tier Density Plots:\n\nNow we will for loop through meaningful features, groupby cluster, and compare the ranking tiers via density plots","e29fc4f4":"# Trim Data:\n\nWe want to trim this dataset down to:\n\n- Man-to-Man coverage\n- Defender was targeted (Target data was used from @tombliss's notebook, and matched with closest offensive player ID)\n- Penalty = False\n\n# Ground Covered:\n\nI've also added a \"Ground Covered\" variable that takes the defenders separation with closest offensive player when the ball is leaving the Quarterback's hands, and takes the difference from the defenders separation when the pass arrives\n\nA large ground covered value can show how quickly the defender is able to hone in on the receiver if he's been beat off the ball. But, then again a small value can make a case for them sticking closely with the receiver for the entirety of the play.","ca8af4a5":"The term **\"Shadow Corner\"** has been thrown around fantasy football leagues for some time now, and is a widely used indicator by savvy fantasy players when determining who to start\/sit in their lineups each week.\n\nSo what actually is a **\"Shadow Corner\"**? \n\nIt is generally the team's best cornerback on defense, being tasked with covering the opposing team's \"best\" wide receiver, for the majority of the game. This strategy has proven to be effective if your shadow corner has the ability to completely shut down the opposing team's best offensive weapon all game.","e660ecb7":"## Slot Corners","db8b6a67":"Some players have no combine data for some events, or none at all. So we will fill those in with the cluster median for each feature","7337e2e5":"You can see above, James Bradberry in the black jersey, attempted to shadow Mike Evans of the Buccaneers for a majority of the game in 2018 and was quite successful. He only allowed 1 catch for 16 yards.","8a9190a8":"------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","2fe60be7":"# Top Corners By Cluster","3b13e017":"#### How to read this:\n\nJalen Ramsey plays an average of 27 Ouside snaps per game, 7 Slot snaps, Guards the WR1 for the opposing team on 48% of routes, and had a true \"shadow\" coverage game with the WR1 nearly 31% of the time, and so on for WR2, WR3 and TE1.","3007ced8":"Here you can visualize in 3D the cornerback clusters, with a Z dimension of WR1 shadow %","5b0e7fcd":"------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","e44da69d":"# Position Rankings:\n\nPosition Rankings were done offline locally and added in the other notebook, WR1 = Teams best receiver, WR2 = 2nd best, ect.\n\nData was pulled from [2018 PBP data](https:\/\/github.com\/guga31bb\/nflfastR-data\/blob\/master\/data\/play_by_play_2018.csv.gz?)\n\nDeterming the best wide receiver for each team,on any given week can be quite subjective for some teams. Depending on injuries, and weekly inconsistencies from some players, it is diifcult to determine who is the \"1st best\", \"2nd best\" at each position.\n\nI derived a simple formula based on a rolling mean lag of targets, yards, air yards, and touchdowns. Weights were determined from my personal domain knowlege.\n\nPosition Score = ['Targets_Rolling']*.35 + ['Yards _Rolling']*.30 + ['Air_yards_Rolling']*.20 + ['TD_Rolling']*.15\n\nand then ranked from largest score to least, grouped by week,team,positon \n\nPosition Rank = Position + (Position Score Rank)\n\n\n# Alignment:\n\nWe also want to determine the position align of the corner at the beginning of the snap. Instead of taking the min\/max Y coordinates of the corner to determine alignment, we will take the min\/max of the wide receivers for each play and match that with the closest defender. Corners aligned with the 2 most outside receivers will be \"Outside\" and \"Slot\" if aligned with slot receivers.\n\n# Determining a Shadow coverage snap:\n\nMy assumptions were that if a corner was lined up opposite of the receiver at the start of each play, then that play can be classified as a shadow coverage snap for the corner, regardless of whether it was Man or Zone coverage. In order to be classified as a true shadow coverage game, then the corner must have shadowed the receiver for more than 60% of the coverage snaps and must be more than 20 total snaps to qualify. "}}