{"cell_type":{"9f300dfd":"code","f2c4cab4":"code","3a1b053f":"code","9b493746":"code","22fcdb7a":"code","6fbd7710":"code","3530f4fa":"code","2eec33c9":"code","d0b968c2":"code","bfa18467":"code","c074f661":"code","2e52d4fd":"code","00cb8f02":"code","14fa42cb":"code","ce5b22b6":"code","b0e07041":"code","15db583f":"code","a12e9764":"code","df274f54":"code","b5c1bd66":"code","f2bf414d":"code","d286d417":"markdown","3744625c":"markdown","d760de5a":"markdown","c3dff3e8":"markdown","de10910f":"markdown","ce53ba57":"markdown","7d87472d":"markdown","b5d3fd88":"markdown","73a85f45":"markdown","c0c24159":"markdown","a3e0869a":"markdown"},"source":{"9f300dfd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2c4cab4":"data = pd.read_csv('\/kaggle\/input\/forbes-highest-paid-athletes-19902019\/Forbes Richest Atheletes (Forbes Richest Athletes 1990-2019).csv')\ndata","3a1b053f":"inflation_data = {'cpi_per_year': [53.2, 56.45, 58.18, 59.87, 61.51, 63.16, 64.76, 66.92, 68.05, \n                                  69.15, 71.01, 73.41, 74.55, 76.32, 77.76, 80.29, 83.03, 85.14, 88.62, \n                                  88.7, 91.11, 92.47, 95.21, 96.87, 98.33, 99.07, 99.79, 101.86, 104.01, 106], \n                 'year': [1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003,\n                          2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017,\n                         2018, 2019]}\ninflation_df = pd.DataFrame(inflation_data, columns=['year', 'cpi_per_year'])\ninflation_df","9b493746":"npv = []\nfor year, dollars in zip(data['Year'], data['earnings ($ million)']): \n    for yr, cpi in zip(inflation_data['year'], inflation_data['cpi_per_year']):\n        if year == yr: \n            dollars = dollars*((inflation_df.at[29, 'cpi_per_year'])\/cpi)\n            npv.append(round(dollars, 2))\ndata['npv'] = npv\ndata\n    ","22fcdb7a":"data.isnull().sum()","6fbd7710":"# Going to drop columns with missing values \ncols_with_missing = [col for col in data.columns\n                     if data[col].isnull().any()]\ndata = data.drop(cols_with_missing, axis=1)\n\n# We are also going to drop these two columns because they are not relevant to what we are trying to predict \n# The 'Nationality' column has other complications that come with splitting the data so it must be dropped \ndata = data.drop(['Name', 'Nationality'], axis=1)\ndata\n","3530f4fa":"#Here we are putting the sports all in lowercase because they were previously in both upper and lower \n#which made for a larger number of unique values, by putting them all in lowercase there are less unique values\ndata['Sport'] = data['Sport'].astype(str).str.lower()\ndata['Sport'].unique()","2eec33c9":"# this is our prediction target\ny = data['npv']\n\n#these are our 'features', or columns used to determine the earnings of each athlete, used to make predictions\nforbes_features = ['Current Rank', 'Sport', 'Year']\nX = data[forbes_features]\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","d0b968c2":"# Get list of categorical variables\ns = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)\n#Since we dropped the other categorical variable columns which included 'Name' and 'Nationality',\n#we should expect the 'Sport' column to be the only column containing categorical variables ","bfa18467":"print('Unique values in \"Sport\" training data: ', X_train['Sport'].unique())\nprint('\\nUnique values in \"Sport\" valid data: ', X_valid['Sport'].unique())","c074f661":"#We only have one column for this iteration, but if you wanted to incorporate 'Nationality' or another\n#categorical variable column, then this iteration would be useful to check if you will later find errors \n#when using label encoding\n\nfor col in object_cols: \n    if set(X_valid[col]).issubset(set(X_train[col])):\n        good_label_cols = col\n    \nprint(\"Categorical variables that can be used in label encoding: \", good_label_cols)\n#print(\"Categorical variables to be dropped: \", bad_label_cols)","2e52d4fd":"# Try dropping categorical variables \n\n#Define function to compare approaches \ndef compare(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100,random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\n\ndrop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n\nprint('MAE from dropping categorical variable columns:')\nprint(compare(drop_X_train, drop_X_valid, y_train, y_valid))\n    ","00cb8f02":"label_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()\n\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n    label_X_valid[col] = label_encoder.transform(X_valid[col])\nprint('MAE from label encoding:')\nprint(compare(label_X_train, label_X_valid, y_train, y_valid))\n","14fa42cb":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\nprint(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(compare(OH_X_train, OH_X_valid, y_train, y_valid))","ce5b22b6":"y_valid = y_valid.tolist()","b0e07041":"\n\n# predict the data based on the original prediction target and dropped categorical features\n\ndrop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n\n\n    # Defining our model: using RandomForestRegressor \nmodel = RandomForestRegressor(n_estimators=100,random_state=0)\n    # Fitting the model with the training data \nmodel.fit(drop_X_train, y_train)\n    # Making predictions based on validation data \npredictions = model.predict(drop_X_valid)\ndata = {\"Prediction\": predictions}\ndrop_df = pd.DataFrame(data, columns=['Prediction'])\ndrop_df['Outcome'] = y_valid\ndrop_df['Percent Error'] = round((abs((drop_df['Outcome']) - (drop_df['Prediction']))\/(drop_df['Outcome']))*100, 2)\ndrop_df['Correct?'] = drop_df['Percent Error'] < 5\ndrop_df = drop_df[['Outcome', 'Prediction', 'Percent Error', 'Correct?']]\ndrop_df\n\n\n\n\n","15db583f":"\ndef percentCorrect(df): \n    true_count = 0\n    false_count = 0\n    total_count = 0\n    for answer in df['Correct?']: \n        if answer == True: \n            true_count += 1\n            total_count += 1\n        else: \n            false_count += 1\n            total_count += 1\n    print('Percent true = ', (round((true_count\/total_count)*100, 2)), '%')\n    print('Percent false = ', (round((false_count\/total_count)*100, 2)), '%')\npercentCorrect(drop_df)\n            ","a12e9764":"    # Defining our model: using RandomForestRegressor \nmodel = RandomForestRegressor(n_estimators=100,random_state=0)\n    # Fitting the model with the training data \nmodel.fit(label_X_train, y_train)\n    # Making predictions based on validation data \npredictions = model.predict(label_X_valid)\ndata = {\"Prediction\": predictions}\nlabel_df = pd.DataFrame(data, columns=['Prediction'])\nlabel_df['Outcome'] = y_valid\nlabel_df['Percent Error'] = round((abs((label_df['Outcome']) - (label_df['Prediction']))\/(label_df['Outcome']))*100, 2)\nlabel_df['Correct?'] = label_df['Percent Error'] < 5\nlabel_df = label_df[['Outcome', 'Prediction', 'Percent Error', 'Correct?']]\nlabel_df\n\n# sports_df = pd.DataFrame(X_train['Sport'].unique(), columns=['list_sports'])\n# sports_df['list_encoded_sports'] = label_encoder.fit_transform(sports_df.list_sports)\n# sports_df\n\n","df274f54":"percentCorrect(label_df)","b5c1bd66":"    # Defining our model: using RandomForestRegressor \nmodel = RandomForestRegressor(n_estimators=100,random_state=0)\n    # Fitting the model with the training data \nmodel.fit(OH_X_train, y_train)\n    # Making predictions based on validation data \npredictions = model.predict(OH_X_valid)\ndata = {\"Prediction\": predictions}\nnew_df = pd.DataFrame(data, columns=['Prediction'])\nnew_df['Outcome'] = y_valid\nnew_df['Percent Error'] = round((abs((new_df['Outcome']) - (new_df['Prediction']))\/(new_df['Outcome']))*100, 2)\nnew_df['Correct?'] = new_df['Percent Error'] < 5\nnew_df = new_df[['Outcome', 'Prediction', 'Percent Error', 'Correct?']]\nnew_df\n","f2bf414d":"percentCorrect(new_df)","d286d417":"We can see that there are 24 values out of 2328 total values in the dataset that are null. This is not so many to where it would have a large effect, but as you can see from the dataset printed out above are are some non-null character values that also might have a strange effect on our predictions. The simplest way to deal with this is by dropping the column. ","3744625c":"Method 3) One Hot Encoding categorical variables:","d760de5a":"From sklearn we are importing classes in order to split our data and predict outcomes. \nWe will approach this data and performing predictions on it in three different ways: \n1) Dropping categorical variables columns \n2) Label encoding categorical variables \n3) One Hot Encoding categorical variables \nAnd we will test which one is the best approach by finding mean absolute error ","c3dff3e8":"Method 1) Dropping categorical variables:","de10910f":"As seen above, Method 1 of dropping categorical variable columns is the most accurate because its mean absolute error is the least. Below we are building the model using the data with the dropped categorical variable columns.","ce53ba57":"Here we are seeing how many values in each column of the data are null or do not have something in them, this is so we can get a sense of how much we are missing and if it might skew the predictions","7d87472d":"Below I am calculating the time value of money with inflation data. Using the Consumer Price Index (CPI) from 1991 to 2019 and so on. With this I use the formula \n    **Final value = Initial value * CPI final\/CPI initial**\nI was able to get the CPI from each year and plug it into the formula above. So based on the year, the for loop below outputs a list of the Net Present Value for each salary, and now we have the value of each salary for the year 2019 which is the last year this data was collected. We can now make predictions on the net present value of the earnings.","b5d3fd88":"Predictions using One Hot Encoding method:","73a85f45":"The way we have split the data, it follows that the valid data is a subset of the training data. If the valid data was not a subset of the training data it would throw an error because label encoder would not have made labels for any data that it had not seen before, so you would still have unencoded categorical variables in your valid data. This is the reason why the 'Nationality' column needed to be dropped.","c0c24159":"# Machine Learning Practice with Categorical Variables on Forbes Highest Paid Athletes Data from 1990 - 2019","a3e0869a":"Method 2) Label Encoding categorical variables:"}}