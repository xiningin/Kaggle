{"cell_type":{"d1608d10":"code","cdae8785":"code","02c0555a":"code","7da7dc6f":"code","edaf9fb7":"code","7fb169a8":"code","89d314f9":"code","07a37800":"code","bb84e6ac":"code","cae831e9":"code","5e187d99":"code","87d82fe4":"code","7e98091c":"code","18078438":"code","2fc386b6":"code","a2cd1ec0":"code","032224dd":"code","4ccb0494":"code","e16df56b":"code","5d7fcaa8":"code","a9e6c65c":"code","2d560ed8":"code","68f44f47":"code","3224a711":"code","1e81f88d":"code","4a39971b":"code","49a8e7eb":"code","0150ddec":"code","a7c38f68":"code","3e083a1e":"code","a2386b0d":"code","04777baa":"code","a66f8e8a":"code","45275bbc":"code","622b9cf8":"code","b5b8dca6":"code","6bd49dd5":"code","6228636a":"code","0d0d3751":"code","42999e32":"code","9821d62f":"code","d020d06e":"code","5cd209e3":"code","e751b782":"code","80015be0":"code","6a1eb502":"code","88a134cc":"code","405c6dd8":"code","8df93fb9":"code","95a690b0":"code","3d2af30d":"code","89bc03f4":"code","ec401469":"code","ed0ff80b":"code","8621fcaa":"code","642f6212":"code","6720240d":"code","67b8fc07":"code","cf49e567":"code","9b4ad2b8":"code","8b044346":"code","e92e77cf":"code","161ec969":"code","39d774a8":"code","1384bbdd":"markdown","f0621e81":"markdown","69cb10c2":"markdown","a285ee5a":"markdown","8098ca0e":"markdown","4dd28628":"markdown","4b32eb73":"markdown","9991862c":"markdown","eae6d68f":"markdown","3251a9ac":"markdown","5f63bcc3":"markdown","5e5c4ad2":"markdown","9b329ed7":"markdown","027d12a7":"markdown","b5a09723":"markdown","2171dee1":"markdown","53e58efa":"markdown","d18f01b0":"markdown","192020a9":"markdown","8dc52cbd":"markdown","5548abaa":"markdown","2b90ce80":"markdown","c255c834":"markdown","66140390":"markdown","61aca4b0":"markdown","0a056417":"markdown","5031bc2c":"markdown","b7decd60":"markdown","9dd3bbc9":"markdown","2709745a":"markdown","fdbeaacc":"markdown","b15c00fc":"markdown","394d52bb":"markdown","3e393b53":"markdown","55eebd3f":"markdown","43586c62":"markdown","3dd3cf49":"markdown","d20c2b57":"markdown","54ab13ad":"markdown","7879b770":"markdown","ac2c683a":"markdown","6787a18b":"markdown","757089ec":"markdown","48761147":"markdown","2028c9e5":"markdown","8ecba6f4":"markdown","fcf4f026":"markdown","a5398dd3":"markdown","d21f17c5":"markdown","171e0295":"markdown","9fbca3e2":"markdown","b56d1784":"markdown","df9fb60f":"markdown","4b5c632a":"markdown","5c0fedba":"markdown","ac99ae84":"markdown","b8e72d4b":"markdown","fdc99ca9":"markdown","78e8c873":"markdown","bbaf84d4":"markdown","71faa637":"markdown","e466206e":"markdown","bef584e1":"markdown","3dc5d0c8":"markdown","28c80cad":"markdown","529ac3d2":"markdown","27032335":"markdown","ded4a952":"markdown","d716b908":"markdown","49332d8f":"markdown","cd792fba":"markdown","2937ae24":"markdown","9afe3dfb":"markdown","6aa292b9":"markdown","dcf0bf76":"markdown","ed168e5a":"markdown","321f6ce5":"markdown","7589a963":"markdown","8e75943b":"markdown","f9b86d29":"markdown","9e48a35f":"markdown","28507bea":"markdown","3369f67b":"markdown","928718d6":"markdown","8abb4a16":"markdown","e7925d0b":"markdown","a1220a88":"markdown","c8029c87":"markdown","1503cfcb":"markdown"},"source":{"d1608d10":"# Data clearnning and EDA\nimport pandas as pd\nimport numpy as np\nimport math\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns","cdae8785":"data = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","02c0555a":"data.head()","7da7dc6f":"data.info()","edaf9fb7":"categorical_features = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking', 'DEATH_EVENT']\nnumerical_features = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']\nhealth_features = ['anaemia', 'diabetes', 'high_blood_pressure', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium']","7fb169a8":"data[numerical_features].describe()","89d314f9":"sns.histplot(x=data.age)\nplt.title('Age Distribution')\nplt.xlabel('Age')\nplt.show()","07a37800":"sns.displot(data=data, x=\"age\", hue=\"DEATH_EVENT\", multiple=\"stack\", kind='kde')\nplt.title('Distribution of Age and Death event')\nplt.xlabel('Age')\nplt.show() ","bb84e6ac":"fig, ax = plt.subplots(figsize=(10,5))\nsns.boxplot(x=data.ejection_fraction, ax=ax)\nplt.title('Box plot of ejection fraction')\nplt.xlabel('Ejection fraction')\nplt.show()","cae831e9":"sns.displot(data=data, x=\"ejection_fraction\", hue=\"DEATH_EVENT\", multiple=\"stack\", kind='kde')\nplt.title('Distribution of ejection fraction and Death event')\nplt.xlabel('ejection_fraction')\nplt.show()","5e187d99":"fig, ax = plt.subplots(figsize=(20,5))\nsns.boxplot(x=data.creatinine_phosphokinase, ax=ax)\nplt.title('Box plot of creatinine phosphokinase')\nplt.xlabel('Creatinine phosphokinase')\nplt.show()","87d82fe4":"fig, ax = plt.subplots(figsize=(20,10))\nsns.histplot(x=data.creatinine_phosphokinase, ax=ax)\nplt.title('Distribution of creatinine phosphokinase')\nplt.xlabel('creatinine phosphokinase')\n\nplt.show()","7e98091c":"sns.displot(data=data, x=\"creatinine_phosphokinase\", hue=\"DEATH_EVENT\", multiple=\"stack\", kind='kde')\nplt.title('Distribution of creatinine phosphokinase and Death event')\nplt.xlabel('creatinine_phosphokinase')\nplt.show()","18078438":"fig, ax = plt.subplots(figsize=(20,5))\nsns.boxplot(x=data.platelets, ax=ax)\nplt.title('Boxplot of platelets')\nplt.xlabel('Plateletes')\nplt.show()","2fc386b6":"sns.displot(data=data, x=\"platelets\", hue=\"DEATH_EVENT\", multiple=\"stack\", kind='kde')\nplt.title('Distribution of platelets and Death event')\nplt.xlabel('platelets')\nplt.show()","a2cd1ec0":"fig, ax = plt.subplots(figsize=(20,5))\nsns.boxplot(x=data.serum_creatinine, ax=ax)\nplt.title('Box plot of serum cretinine')\nplt.xlabel('Serum creatinine')\nplt.show()","032224dd":"sns.displot(data=data, x=\"serum_creatinine\", hue=\"DEATH_EVENT\", multiple=\"stack\", kind='kde')\nplt.title('Distribution of serum_creatinine and Death event')\nplt.xlabel('serum_creatinine')\nplt.show()","4ccb0494":"fig, ax = plt.subplots(figsize=(20,5))\nsns.boxplot(x=data.serum_sodium, ax=ax)\nplt.title('Box plot of serum sodium')\nplt.xlabel('Seruim sodium')\nplt.show()","e16df56b":"sns.displot(data=data, x=\"serum_sodium\", hue=\"DEATH_EVENT\", multiple=\"stack\", kind='kde')\nplt.title('Distribution of serum_sodium and Death event')\nplt.xlabel('serum_sodium')\nplt.show()","5d7fcaa8":"fig, ax = plt.subplots(figsize=(20,5))\nsns.boxplot(x=data.time, ax=ax)\nplt.title('Box plot of time')\nplt.show()","a9e6c65c":"data['anaemia'].value_counts().plot(kind='bar')\nplt.xlabel(\"anaemia\", labelpad=14)\nplt.ylabel(\"Count of people\", labelpad=14)\nplt.title('The number of people has and do not have anaemia')","2d560ed8":"pd.crosstab(data.anaemia  ,data.DEATH_EVENT).plot(kind='bar')\nplt.title('Mortality rate correlating to anaemia')\nplt.xlabel('Anaemia')\nplt.ylabel('Death')\nplt.show()","68f44f47":"data['diabetes'].value_counts().plot(kind='bar')\nplt.xlabel(\"diabetes\", labelpad=14)\nplt.ylabel(\"Count of people\", labelpad=14)\nplt.title('The number of people has and do not have diabetes')","3224a711":"pd.crosstab(data.diabetes  ,data.DEATH_EVENT).plot(kind='bar')\nplt.title('Mortality rate correlating to diabetes')\nplt.xlabel('Diabetes')\nplt.ylabel('Death')\nplt.show()","1e81f88d":"data['high_blood_pressure'].value_counts().plot(kind='bar')\nplt.xlabel(\"high_blood_pressure\", labelpad=14)\nplt.ylabel(\"Count of people\", labelpad=14)\nplt.title('The number of people with or without high_blood_pressure')","4a39971b":"pd.crosstab(data.high_blood_pressure  ,data.DEATH_EVENT).plot(kind='bar')\nplt.title('Mortality rate correlating to high_blood_pressure')\nplt.ylabel('high_blood_pressure')\nplt.show()","49a8e7eb":"data['sex'].value_counts().plot(kind='bar')\nplt.xlabel(\"gender\", labelpad=14)\nplt.ylabel(\"Count of people\", labelpad=14)\nplt.title('Number of male and female')","0150ddec":"data['smoking'].value_counts().plot(kind='bar')\nplt.xlabel(\"Smoke\", labelpad=14)\nplt.ylabel(\"Count of people\", labelpad=14)\nplt.title('Number of patient smoke')","a7c38f68":"pd.crosstab(data.smoking  ,data.DEATH_EVENT).plot(kind='bar')\nplt.title('Mortality rate correlating to smoking')\nplt.ylabel('Count of people')\nplt.show()","3e083a1e":"data['DEATH_EVENT'].value_counts().plot(kind='bar')\nplt.xlabel(\"DEATH_EVENT\", labelpad=14)\nplt.ylabel(\"Count of people\", labelpad=14)\nplt.title('Number of death patients')","a2386b0d":"data['DEATH_EVENT'].value_counts()\n","04777baa":"plt.figure(figsize=(16,10))\nsns.heatmap(data.corr(method='pearson'), annot=True)","a66f8e8a":"from sklearn import linear_model\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf","45275bbc":"# rsquared: get r squared\n# fvalue: get f value\n# f_pvalue: get p value\n# params: get coefficient\n# tvalues: get t-statistic\n\ntest_results = []\ncoef_results = []\nt_results = []\nf_pass_results = []\nt_pass_results = []\nf_crit = 3.026\nt_crit = 1.968\n\nfor feature in health_features:\n    results = smf.ols(feature + ' ~ age + sex', data=data).fit()\n    \n    test = [round(results.rsquared, 3), round(results.fvalue, 3), round(results.f_pvalue, 3)]\n    coef = [round(results.params[1], 4), round(results.params[2], 4)]\n    t = [round(results.tvalues[1], 3), round(results.tvalues[2], 3)]\n    f_pass = [\"Pass\" if round(results.fvalue, 3) <= f_crit else \"Fail\"]\n    t_pass = [\"Pass\" if (t[0] <= t_crit) and (t[0] >= t_crit * -1) else \"Fail\",\n              \"Pass\" if (t[1] <= t_crit) and (t[1] >= t_crit * -1) else \"Fail\"]\n    \n    test_results.append(test)\n    coef_results.append(coef)\n    t_results.append(t)\n    f_pass_results.append(f_pass)\n    t_pass_results.append(t_pass)\n    \n    print('\\033[1m ANOVA of age and sex with', feature,'\\033[0m')\n    print(results.summary())\n    print('\\n')","622b9cf8":"fig, ax = plt.subplots(figsize = (20,5)) \nax.set_axis_off() \ntable = ax.table( \n    cellText = test_results,  \n    rowLabels = health_features,  \n    colLabels = ['r squared', 'F', 'p'],\n    colWidths = [0.1] * 3,\n    loc = 'center') \ntable.set_fontsize(14)\ntable.scale(2, 2)\n   \nax.set_title('r squared, F, and p value of both age and sex in regards to each health indicator (indicators are dependent)', fontweight =\"bold\") \n   \nplt.show() ","b5b8dca6":"fig, ax = plt.subplots(figsize = (20,5)) \nax.set_axis_off() \ntable = ax.table( \n    cellText = coef_results,  \n    rowLabels = health_features,  \n    colLabels = ['age', 'sex'],\n    colWidths = [0.1] * 2,\n    loc = 'center') \ntable.set_fontsize(14)\ntable.scale(2, 2)\n   \nax.set_title('Cooeficients of age and sex in regards to each health indicator (indicators are dependent)', fontweight =\"bold\") \n   \nplt.show() ","6bd49dd5":"fig, ax = plt.subplots(figsize = (20,5)) \nax.set_axis_off() \ntable = ax.table( \n    cellText = t_results,  \n    rowLabels = health_features,  \n    colLabels = ['age', 'sex'],\n    colWidths = [0.1] * 2,\n    loc = 'center') \ntable.set_fontsize(14)\ntable.scale(2, 2)\n   \nax.set_title('t-statistics of age and sex in regards to each health indicator (indicators are dependent)', fontweight =\"bold\") \n   \nplt.show() ","6228636a":"fig, ax = plt.subplots(figsize = (20,5)) \nax.set_axis_off() \ntable = ax.table( \n    cellText = f_pass_results,  \n    rowLabels = health_features,  \n    colLabels = ['F test result'],\n    colWidths = [0.1],\n    loc = 'center') \ntable.set_fontsize(14)\ntable.scale(2, 2)\n   \nax.set_title('Null hypothesis result of age and sex in regards to each health indicator (F critical = 3.026)', fontweight =\"bold\") \n   \nplt.show() ","0d0d3751":"fig, ax = plt.subplots(figsize = (20,5)) \nax.set_axis_off() \ntable = ax.table( \n    cellText = t_pass_results,  \n    rowLabels = health_features,  \n    colLabels = ['age', 'sex'],\n    colWidths = [0.1] * 2,\n    loc = 'center') \ntable.set_fontsize(14)\ntable.scale(2, 2)\n   \nax.set_title('t test result of age and sex in regards to each health indicator (t critical = 1.968)', fontweight =\"bold\") \n   \nplt.show() ","42999e32":"# rsquared: get r squared\n# fvalue: get f value\n# f_pvalue: get p value\n# params: get coefficient\n# tvalues: get t-statistic\n\ntest_results = []\ncoef_and_t_results = []\nt_pass_results = []\nt_crit = 1.968\nindex = 1\n\nresults = smf.ols('DEATH_EVENT ~ age + anaemia + creatinine_phosphokinase + diabetes + ejection_fraction + high_blood_pressure' +\n                  ' + platelets + serum_creatinine + serum_sodium + sex + smoking + time', data=data).fit()\n\ntest_values = [round(results.rsquared, 3), round(results.fvalue, 3), round(results.f_pvalue, 3)]\ntest_results.append(test_values)\nwhile index < len(results.params):\n    coef_and_t = [round(results.params[index], 4), round(results.tvalues[index], 3)]\n    t_pass = [\"Pass\" if (round(results.tvalues[index], 3) <= t_crit) and (round(results.tvalues[index], 3) >= t_crit * - 1) else \"Fail\"]\n    \n    coef_and_t_results.append(coef_and_t)\n    t_pass_results.append(t_pass)\n    index += 1\n\nprint(results.summary())\nprint('\\n')","9821d62f":"fig, ax = plt.subplots(figsize = (20,3)) \nax.set_axis_off() \ntable = ax.table( \n    cellText = test_results,    \n    colLabels = ['r squared', 'F', 'p'],\n    colWidths = [0.1] * 3,\n    loc = 'center') \ntable.set_fontsize(14)\ntable.scale(2, 2)\n   \nax.set_title('r squared, F, and p value of all health indicators in regards to DEATH_EVENT (indicators are independent)', fontweight =\"bold\") \n   \nplt.show() ","d020d06e":"fig, ax = plt.subplots(figsize = (20,7)) \nax.set_axis_off() \ntable = ax.table( \n    cellText = coef_and_t_results,  \n    rowLabels = data.columns[:12],  \n    colLabels = ['coefficient', 't'],\n    colWidths = [0.1] * 2,\n    loc = 'center') \ntable.set_fontsize(14)\ntable.scale(2, 2)\n   \nax.set_title('Cooeficient and t_statistic of each health indicator in regards to DEATH_EVENT (indicators are independent)', fontweight =\"bold\") \n   \nplt.show() ","5cd209e3":"fig, ax = plt.subplots(figsize = (20,7)) \nax.set_axis_off() \ntable = ax.table( \n    cellText = t_pass_results,  \n    rowLabels = data.columns[:12],  \n    colLabels = ['t test result'],\n    colWidths = [0.1],\n    loc = 'center') \ntable.set_fontsize(14)\ntable.scale(2, 2)\n   \nax.set_title('t test result of each health indicator in regards to DEATH_EVENT (t critical = 1.968)', fontweight =\"bold\") \n   \nplt.show()","e751b782":"# Import ML library\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, precision_score\nfrom sklearn.metrics import plot_confusion_matrix, plot_roc_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, train_test_split, RandomizedSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb \n","80015be0":"# Split the data into train and test set \ny = data['DEATH_EVENT']\nX = data.drop(['DEATH_EVENT'], axis=1)\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=42)","6a1eb502":"# Over sampling train data to avoid imbalance data set\nfrom imblearn.over_sampling import SMOTE\n\ndf = pd.concat([X_train, y_train], axis=1)\n\nsm = SMOTE(sampling_strategy='minority', random_state=7)\n\n# Fit the model to generate the data.\noversampled_trainX, oversampled_trainY = sm.fit_resample(df.drop('DEATH_EVENT', axis=1), df['DEATH_EVENT'])\noversampled_train = pd.concat([pd.DataFrame(oversampled_trainY), pd.DataFrame(oversampled_trainX)], axis=1)\noversampled_train['DEATH_EVENT'].value_counts()","88a134cc":"X_train = oversampled_trainX.copy()\ny_train = oversampled_trainY.copy()","405c6dd8":"def evaluate_model_performance(clf, X_train, y_train):\n    '''evaluate a classification model's performance\n    INPUT:\n    clf - Model object\n    X_train - Training data matrix\n    y_train - Expected model output vector\n    OUTPUT:\n    clf_accuracy: Model accuracy\n    clf_f1_score: Model F1-score\n    clf_recall_score: model recall score\n    clf_precision_score: model precision score\n    '''\n    y_pred_rf = clf.predict(X_train)\n    clf_accuracy = accuracy_score(y_train, y_pred_rf)\n    clf_f1_score = f1_score(y_train, y_pred_rf)\n    clf_recall_score = recall_score(y_train, y_pred_rf, average='binary')\n    clf_precision_score = precision_score(y_train, y_pred_rf, average='binary')\n\n    confusion_matrix = plot_confusion_matrix(clf, X_train, y_train, \n                                             cmap=plt.cm.plasma,\n                                             normalize='true')\n    plt.grid(False)\n    plt.title(\"Confusion matrix\")\n    \n    roc_display = plot_roc_curve(clf, X_train, y_train)\n    plt.title(\"ROC Curve and AUC score\")\n\n    return clf_accuracy, clf_f1_score, clf_recall_score, clf_precision_score","8df93fb9":"model_score = []","95a690b0":"%%time\n\nrf =  RandomForestClassifier(n_jobs=4)\n\nrf.fit(X_train, y_train)\n\nacc, f1, recall, precision = evaluate_model_performance(rf, X_val, y_val)\nprint(acc)","3d2af30d":"# Feature Selection\n\nfeat_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\nprint(feat_importances.sort_values(ascending=True))\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","89bc03f4":"selected_features = ['time', 'age', 'ejection_fraction', 'serum_creatinine', \"serum_sodium\"]","ec401469":"%%time\n\nnb_clf =  GaussianNB()\n\nnb_clf.fit(X_train, y_train)  \n\nprint('Best Score: ', nb_clf.score(X_val, y_val))\n\nacc, f1, recall, precision = evaluate_model_performance(nb_clf, X_val, y_val)\nmodel_score.append(['Gaussian Naive Bayes', acc, f1, recall, precision])","ed0ff80b":"%%time\n\nmodel =  LogisticRegression()\n\nparameters = {\n    'penalty': ['l2'],\n    'solver': ['lbfgs', 'liblinear'],\n    'C': [ 0.01, 0.1, 10, 100],\n    'max_iter': [5000, 10000, 20000]\n}\n\nlog_reg = GridSearchCV(model, parameters, refit=True, verbose=1, cv = 5, n_jobs = 4)\nlog_reg.fit(X_train, y_train)\n\nprint('Best Score: ', log_reg.best_score_*100, '\\nBest Parameters: ', log_reg.best_params_)\n\nacc, f1, recall, precision = evaluate_model_performance(log_reg, X_val, y_val)\nmodel_score.append(['Logistic regression', acc, f1, recall, precision])","8621fcaa":"\n%%time\n\nmodel = AdaBoostClassifier()\n\nparameters = {\n    'n_estimators': [200, 300, 500, 600, 800],\n    'learning_rate':[0.001, 0.1, 0.2, 0.5]\n}\n\nada_clf = GridSearchCV(model, parameters, refit=True, verbose=1, cv = 5, n_jobs = 4)\nada_clf.fit(X_train, y_train)\n\nprint('Best Score: ', ada_clf.best_score_*100, '\\nBest Parameters: ', ada_clf.best_params_)\n\nacc, f1, recall, precision = evaluate_model_performance(ada_clf, X_val, y_val)\nmodel_score.append(['Adaboost classifier', acc, f1, recall, precision])","642f6212":"%%time\nmodel =  RandomForestClassifier()\n\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\nparameters = {'n_estimators': [100, 200, 300],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': max_depth,\n               'min_samples_split': [2, 5],\n               'min_samples_leaf': [1, 2] }\n\nrf_clf= GridSearchCV(model, parameters, refit=True, verbose=1, cv = 5, n_jobs = 4)\nrf_clf.fit(X_train, y_train)\n\nprint('Best Score: ', rf_clf.best_score_*100, '\\nBest Parameters: ', rf_clf.best_params_)\n\nacc, f1, recall, precision = evaluate_model_performance(rf_clf, X_val, y_val)\nmodel_score.append(['Random Forest', acc, f1, recall, precision])","6720240d":"%%time\n\nmodel = lgb.LGBMClassifier()\n# Create parameters to search\ngridParams = {\n    'learning_rate': [0.01, 0.1, 0.001],\n    'max_depth': [5, 10, 15, None],\n    'min_data_in_leaf': [30, 50, 100],\n    'boosting_type': ['gbdt', 'dart']\n    }\n\n# To view the default model params:\nmodel.get_params().keys()\n\n# Create the grid\nlgb_clf = GridSearchCV(model, gridParams,\n                    verbose=1,\n                    cv = 5,\n                    n_jobs = 4)\n# Run the grid\nlgb_clf.fit(X_train, y_train)\n\n# Print the best parameters found\nprint(lgb_clf.best_params_)\nprint(lgb_clf.best_score_)\n\nacc, f1, recall, precision = evaluate_model_performance(lgb_clf, X_val, y_val)\nmodel_score.append(['LightGBM', acc, f1, recall, precision])","67b8fc07":"# Display the score of model with test data set\nscores = pd.DataFrame(model_score, columns =['Model', 'Accuracy Score', 'F1 Score', 'Recall score', 'Precision score'])\nscores","cf49e567":"fig, axes = plt.subplots(2, 2, figsize=(20,10))\nfig.suptitle('Model performance comparision')\n\nsns.barplot(data=scores, x='Model', y='Accuracy Score',  ax=axes[0][0])\naxes[0][0].set_title('Accuraccy Score')\n\nsns.barplot(data=scores, x='Model', y='F1 Score',  ax=axes[0][1])\naxes[0][1].set_title('F1 Score')\n\nsns.barplot(data=scores, x='Model', y='Recall score',  ax=axes[1][0])\naxes[1][0].set_title('Recall Score')\n\nsns.barplot(data=scores, x='Model', y='Precision score',  ax=axes[1][1])\naxes[1][1].set_title('Precision Score')","9b4ad2b8":"X_train_selected = X_train[selected_features]\nX_val_selected = X_val[selected_features]\nX_train_selected","8b044346":"%%time\n\nmodel = lgb.LGBMClassifier()\n# Create parameters to search\ngridParams = {\n    'learning_rate': [0.01, 0.1, 0.001],\n    'max_depth': [5, 10, 15, None],\n    'min_data_in_leaf': [30, 50, 100],\n    'boosting_type': ['gbdt', 'dart']\n    }\n\n# To view the default model params:\nmodel.get_params().keys()\n\n# Create the grid\nlgb = GridSearchCV(model, gridParams,\n                    verbose=1,\n                    cv = 5,\n                    n_jobs = 4)\n# Run the grid\nlgb.fit(X_train_selected, y_train)\n\n# Print the best parameters found\nprint(lgb.best_params_)\nprint(lgb.best_score_)\n\nacc, f1, recall, precision = evaluate_model_performance(lgb, X_val_selected, y_val)\n\nprint(\"Accuraccy:\", acc*100)\nprint(\"F1 score:\", f1)\nprint(\"Recall:\", recall)\nprint(\"precision:\", precision)\nmodel_score.append(['LBGM_classifier with feature selection', acc, f1, recall, precision])\n","e92e77cf":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel =  KNeighborsClassifier()\n\nparameters = { \n    'n_neighbors':[1, 3, 5, 10, 15],\n    'weights':['uniform', 'distance'],\n    'metric':['euclidean', 'manhattan', 'minkowski'] \n}\n\nknn_clf= GridSearchCV(model, parameters, refit=True, verbose=1, cv = 10, n_jobs = 4)\nknn_clf.fit(X_train_selected, y_train)\n\nprint('Best Score: ', knn_clf.best_score_*100, '\\nBest Parameters: ', knn_clf.best_params_)\nacc, f1, recall, precision = evaluate_model_performance(knn_clf, X_val_selected, y_val)\n\nprint(\"Accuraccy:\", acc*100)\nprint(\"F1 score:\", f1)\nprint(\"Recall:\", recall)\nprint(\"precision:\", precision)\nmodel_score.append(['KNN with feature selection', acc, f1, recall, precision])","161ec969":"from sklearn.neural_network import MLPClassifier\n\nmodel =  MLPClassifier()\n\nparameters = { \n    'activation': ['relu', 'logistic', 'tanh'],\n    'hidden_layer_sizes':[20, 50, 100],\n    'solver': ['adam', 'lbfgs'],\n    'batch_size' :[10, 20, 30],\n    'learning_rate_init': [0.0001, 0.001, 0.1],\n    'max_iter': [1000, 2000]\n}\n\nmlp_clf= GridSearchCV(model, parameters, refit=True, verbose=3, cv = 5, n_jobs = 4)\nmlp_clf.fit(X_train_selected, y_train)\n\nprint('Best Score: ', knn_clf.best_score_*100, '\\nBest Parameters: ', knn_clf.best_params_)\nacc, f1, recall, precision = evaluate_model_performance(mlp_clf, X_val_selected, y_val)\n\nprint(\"Accuraccy:\", acc*100)\nprint(\"F1 score:\", f1)\nprint(\"Recall:\", recall)\nprint(\"precision:\", precision)\nmodel_score.append(['MLP with feature selection', acc, f1, recall, precision])","39d774a8":"scores = pd.DataFrame(model_score, columns =['Model', 'Accuracy Score', 'F1 Score', 'Recall score', 'Precision score'])\nscores","1384bbdd":"From the graph above we can see that `anaemia`, `high_blood_pressure`, `diabetes` have the least impact on the target. Whle `serum_creatine`, `time`, `ejection_fraction` and `age` are the most important feature which similar to the ANOVA analysis above.\n","f0621e81":"The box plot of platelets shows the mean value of platelets in blood of all patients is 260 kiloplatelets\/mL. The Q1 and Q3 value are 220 kiloplatelets\/mL and 300 kiloplatelets\/mL, respectively. There are many outliers in the box plot. \n","69cb10c2":"From the table and the bar chart, we can saw that the `Gaussian Naive Bayes`, `Logistic Regression` and `LightGBM` is model with high accuraccy and F1 scores. It is strange since the simplest model produce the best result. However, looking on the data set with originally have only 300 records which is considered small, hence, the complex models couldn't have enough data to optimize result bad performance. The `lightGBM` is an exception, the accuracy of the model is slightly lower than `naive bayes` but have higher AUC score","a285ee5a":"The Exploratory data analysis step above gave us a profound understanding about the data set, thus, the next step is to perform predictive analysis. The aim of this stage is to built a classification model which can made accurate prediction about the death event of given patient. In the next sections, several steps need to performed which are feature engineering, hyperparatunning, data modelling and model evaluation. ","8098ca0e":"# HEART FAILURE PREDICTION\n","4dd28628":"Gaussian Naive Bayes model is a basic classification model based on Naivebayes theorem. Basically, the model will compute the probability of the hypothesis based on the prior knowlegde about the hypothesis. The model we applied in this project is the variant of the naive bayes model which it assumpt that the data is gausian distribution. ","4b32eb73":"1. Gaussian Naive Bayes","9991862c":"##### Smoking","eae6d68f":"The distribution of serum creatinine and death events is shown in the above line chart. The survival density area lying between two curves increases when the level of creatinine in the blood is between 1 mg\/dL and 1.8 mg\/dL. However, the survival density decreases dramatically when the level of creatinine is either below 1 mg\/dL or above 2 mg\/dL. \n","3251a9ac":"From the result, we can see that data set with feature selection have improve the accuraccy of the LightGBM model to 80% and the F1 score to 0.739 which is the highest compare to other model. Also, the AUC score is also seen a little improvement. Hence, we can see the faeature selection can improve the model performance and also generalize model. As a result, it is resonable to conclude that the LightGBM model is the most suitable for this problems, however, Gaussian Naive bayes and Logistic Regression should be considered because of its performance and simplicity. ","5f63bcc3":"The distribution of ejection fraction and death event is shown as above. The area between two curves represents the survival density while the area between the lower curve and the horizontal axis is the death density. The survival density reaches its peak of above 0.04 at around 40% of the ejection fraction, however, drops significantly if the ejection fraction is either below 20% or above 70%. ","5e5c4ad2":"The dataset contains the medical records of 299 patients of heart failure, which consisted of 194 men and 105 women. The patient's ages vary from 40 to 95 years old. The dataset keeps track of clinical, body, and lifestyle information of the patients, which are called features in this research. There are 13 features in the dataset, 12 of which are considered to be potential reasons contributing to mortality of patients with Cardiovascular Heart Disease (CHD): age, anaemia, Creatinine-Phosphokinase (CPK), diabetes, Ejection Fraction (EF), High Blood Pressure (HBP), platelets, serum creatinine, serum sodium, sex, smoking. Some of the features are of binary data type: anaemia, diabetes, HBP, sex, smoking and death event, which then are taken as category features. The other are continuous (analogous) values, which are taken as numeric features. \n","9b329ed7":"3. AdaboostClassifier","027d12a7":"##### Creatinine phosphokinase","b5a09723":"There are 13 rows in the above table representing 13 clinical features in the patients\u2019 profile: 12 complementary features and one target feature (death event). There is no null value for all features and the type of data is either of type float or integer, which means the data is cleaned and ready to be analysed.","2171dee1":"The mortality rate correlating to anaemia is shown in the following bar chart. The first two columns show the mortality rate of the patients who did not have anaemia and the other two columns show the mortality rate for those who had anaemia. For those who did not have anaemia, there were 120 survived patients and 50 dead patients. For those who had, there were 80 survived patients and 40 dead patients. \n","53e58efa":"The target of this data set show imbalance which can cause bias to the model which is affect our final prediction, therefore, we have to balancing the target of the data set by over sampling method using imblearn package. We applied SMOTE technique which first selects a minority class instance a at random and finds its k nearest minority class neighbors. Then it will created sample based on the K nearest neighbour class.\n","d18f01b0":"Although the simplicity of the model, the model show great result with 80% accuraccy and AUC is 0.85. It is noted that, in the confusion matrix, the false positive is higher than true positive which is because the unbalance data set at the beginning.","192020a9":"##### Results analysis  \nWith the F value being 17.036 while the F critical value is 1.786, we reject the null hypothesis. This means that all health indicators together affect the mortality rate\n\nThe t test result of each indicator against DEATH_EVENT is shown as below:","8dc52cbd":"#### 3. Do health indices affect the mortality rate of patients?\n\n","5548abaa":"The box plot of ejection fraction is as shown in above table. The mean value is 38%, the Q1 and Q3 are 30% and 45% respectively. The minimum value is 10% and the maximum value is 65%.  \n","2b90ce80":"#### Categorical features","c255c834":"The results show how age and sex individually affects other indicators. If a factor passes the t test against an indicator, it means that the factor has no effect on it and vice versa.\n\nFrom the results, we can confirm that:\n<ol>\n<li>Age individually affects:<\/li>\n    <ul>\n        <li>Serum creatinine<\/li>\n    <\/ul>\n<li>Sex individually affects:<\/li>\n    <ul>\n        <li>Diabetes<\/li>\n        <li>Ejection fraction<\/li>\n        <li>Platelets<\/li>\n    <\/ul>\n<\/ol>","66140390":"##### High blood pressure","61aca4b0":"The number of dead patients who did smoke was 30, compared to over 60 dead patients who did not. The survived patients who did smoke were 60, while the survived patients who did not smoke were nearly 140 patients. \n","0a056417":"### Data modelling","5031bc2c":"##### Death event","b7decd60":"##### Anaemia","9dd3bbc9":"Adaboost classifier is a high performance stacked model, it use a combination of weak learner, combine it by weighted majority and made prediction.","2709745a":"Heart is among the most important organs, early-predicting heart failure is a vital matter to any patients with cardiovascular diseases (CVDs). In this background, electronic health records  (EHRs, also called medical records) is a useful source of information on which several screening studies have been working. However, healthcare data has vast data sources, with multiple attributes, which causes difficulty in manually handling the data. Many models varying from standard statistical techniques for sufficient datasets to machine learning models for large-scale datasets have been developed and applied in identifying risks at early stages of heart failure. \n\nThis project is only concerned with the standard statistical techniques as it is sufficient for such a dataset of 299 patients to deal with 3 main problems: (1) Analyze the change of health indicator with regards to gender and age of the patients, (2) Analyze which factors contribute to the mortality rate of a patient and (3) From the given data, Can we build a model which can effectively predict the patient possibility of death. In the following part, we will have a quick review about the related work regarding this data set and discussion about the result. \n","fdbeaacc":"4. Random forest","b15c00fc":"## I. Exploratory Data Analysis","394d52bb":"From this, we accept the null hypothesis of age and sex when the dependent variables are\n<ul>\n<li>Anaemia<\/li>\n<li>Creatinine phosphokinase<\/li>\n<li>Platelets<\/li>    \n<li>Serum sodium<\/li>\n<\/ul>\nThis means both factors together do not affect these indicators while the remaining indicators are affected","3e393b53":"For this task, we peform ANOVA analysis on DEATH_EVENT, with all other variables being factors\n\nCrtical values: Given data size = 299, degree of freedom = 12, significance level = 0.05, we have:\n<ul>\n<li>F critical = 1.786<\/li>\n<li>t critical = 1.968<\/li>\n<\/ul>","55eebd3f":"Random forest model is an algorithm by randomize the each batch of data set to the decision tree and perform voting to make final prediction.","43586c62":"### Model Evaluation","3dd3cf49":"For the patient who did not have diabetes, the survived patients were nearly 120 patients and the dead patients were around 55 patients. For one who had, the dead patients\u2019 number was over 80 and the dead patients were below 50 patients. \n","d20c2b57":"In this section we use `GridSearchCV` to perform hyperparameter trainning and cross validation in order to achieve model with the optimize initialization. In this section we will train 5 different models from the simple model to high performance stacked model and compare the result to select the best model for predict the survival rate of a patient.","54ab13ad":"Before train the model and perform hyperparameter tunning, we train an baseline model `RandomForest` to observe the feature important of each feature.","7879b770":"##### Age","ac2c683a":"The statistical quantitative characteristics of the numerical feature of the dataset is reported in the table above. The total number, mean value, standard deviation, minimum value and maximum value, and the quartiles of the numeric features are taken in full sample (all patients). The calculated quantitative description of each  feature then can be used to build their respective box plot.\n","6787a18b":"The model also show great result with high accuraccy and AUC score. Also, the false positive is better than naive bayes model.","757089ec":"As stated above, in this section, we have to modify the data set before use it to train model. The steps are:\n* split the data\n* oversampling data set using SMOTE technique  \n\nIt is noticeable to mention that we have try remove outlier in the data set but not increase the accuracy of the model.","48761147":"5. LightGBM","2028c9e5":"Average value of follow-up time is 130 days. The patients had the follow-up time of 4-285 days. \n","8ecba6f4":"2. Logistic regression","fcf4f026":"The age and death event distribution is shown in the following line chart. The area between two curves indicates the number of survived patients with respect to their age, while the area between lower curve and the horizontal axis shows the number of dead patients. In detail, the area of survived patients increases from the age of 45 to 75 before decreasing after the age of 75. The peak of survival density of above 0.03 is between 60 and 70 years old. The patients of 80 years old or above had the slimmest density of survival with below 0.01 survival density compared to 0.005 death density. \n","a5398dd3":"#### Data Modelling with feature selection","d21f17c5":"### Feature selection","171e0295":"From the ANOVA results, the most crucial variables are selected. These include:\n<ul>\n<li>Coeficients: shows the slope of each factor<\/li>\n<li>F and p-value: determines whether the null hypothesis is rejected or not in general<\/li>\n<li>t-statistics: determines whether the null hypothesis is rejected or not for each factor<\/li>    \n<li>r-squared: determines how much data fits the regression model<\/li>\n<\/ul>","9fbca3e2":"There are many outlier in Creatinine phosphokinase feature. We can consider eliminate those during feature engineering process.","b56d1784":"We choose the `Naive Bayes` and `LightGBM` to train with data with selected feature which are  `serum_creatine`, `serum_sodium`, `time`, `ejection_fraction` and `age`","df9fb60f":"The model have 90% accuraccy on the trainning set and good AUC score on validation set.","4b5c632a":"### 1. Overview on data set","5c0fedba":"##### Ejection fraction","ac99ae84":"For those who had hypertension, the dead patients were 40 patients, compared to 65 survived patients. While the number of dead patients were 55 patients compared to nearly 140 survived patients in the group of non-hypertension patients. \n","b8e72d4b":"There were around 200 dead patients and nearly 100 survived patients. We can see that there exist an imbalance between two value in the target of the data set (death event).\n","fdc99ca9":"The distribution of creatinine phosphokinase is shown in the above bar chart. The level of CPK enzyme in the blood is mostly distributed from 0 to below 3000 mcg\/L. \n","78e8c873":"The minimum value of serum sodium is 125 mEq\/L. The Q1 and Q3 values are 134 mEq\/L and mEq\/L respectively. The mean is 137 mEq\/L. The maximum value is 150 mEq\/L. There are many outliers to the left. \n","bbaf84d4":"The distribution of creatinine phosphokinase and death events. The survival density is enlarged when the level of CPK enzyme in the blood is from 0 to 1000 mcg\/L and decreases if the creatinine phosphokinase level is higher than 2000 mcg\/L. \n","71faa637":"##### Diabetes","e466206e":"##### Plateletes","bef584e1":"## Introduction","3dc5d0c8":"In this project, the use the accuracy score, F1 score and ROC curve to evaluate the performance of model. While accuracy show the overall accuracy of the model, F1 score give us insight from the precision and recall score to understand if there exist any problem regarding the imbalance data set. In addition, the ROC curve is used for measuring the trade off between true positive rate and fasle positive rate","28c80cad":"##### Seruim sodium","529ac3d2":"In conclusion, through this project our team have applied several data analysis and machine learning techiniques to gain useful insight from the data set to answer the research questions. In the exploratory data analysis part, each feature is inspected to have a basic understand about each featue, then we perform analysis of variance (ANOVA) in order to evaluate the impact of patient's age and sex of health indicators and the affect of each health inidcator to the morality rate. We have founf that time, age, ejection fraction and serum creatinine are major factor contribute to the patient's chance of survival. Finally, we have build and analyze different models to predict the survival chance of the patient. The result show that LightGBM and Gausian Naive Bayes shows the best performance whith 80% accuraccy and 0.74 F1 score. In the future, more investigation need to perform to gather more patients's data and increase the features to improve the accuraccy of the model.","27032335":"From this, we notice that only 4 indicators fail the t test against DEATH_EVENT, which are:\n<ul>\n<li>Age<\/li>\n<li>Ejection fraction<\/li>\n<li>Serum creatinine<\/li>    \n<li>Time<\/li>\n<\/ul>\nThis means that only these factors affect the mortality rate","ded4a952":"##### Time","d716b908":"### Evaluation Framework","49332d8f":"## Conclusion","cd792fba":"After trainning model and hyperparameter tunning phase, model with their best initialization is use to predict the test data set.","2937ae24":"The number of the patients did not have anaemia was over 160 patients. The number of patients had anaemia was 40 patients less than the one had not, which was over 120 patients. \n","9afe3dfb":"To analyze the effects of age and sex on other health indicators, we performed one-way ANOVA analysis of each indicator with age and sex as factors. The ANOVA tests yield crucial information which helps determine whether the null hypothesis is accepted or decline for each indicator\n\nThe ANOVA tests focus on performing Ordinary Least Square regression (OLS regression) between the factors and each dependent variable. The regression method estimates the parameters (slope and intercept) by minimizing the sum of square of differences between the available data and the predicted values. The method is chosen due to the simple model of the data (2 factors and 1 dependent variable for each model)\n\nCrtical values:\nGiven data size = 299, degree of freedom = 2, significant level = 0.05, we have\n<ul>\n<li>F critical = 3.026<\/li>\n<li>t critical = 1.968<\/li>\n<\/ul>","6aa292b9":"The model show great result. the AUC score is higher which prove that the model have a good measure of separability.","dcf0bf76":"##### Serum creatinine","ed168e5a":"The age distribution is shown in the above bar chart. All of the patients are between 40 and 95 years old with most of them being from 40 to 72 years old. The patients\u2019 number of 56 to 62 years old got the highest count, with more than 50 people. From the age of 75 onwards, the patients\u2019 number decreased in proportional to their age range. Three last columns of age ranges got the least number of patients (under 10 patients). \n","321f6ce5":"The logistic regression is a linear approach for classification problems with a major difference is it uses sigmoid function. ","7589a963":"The performance is quite similar to the random forest model, except it AUC score is slightly higher","8e75943b":"##### ANOVA results","f9b86d29":"The t test result of age and sex in regards to each health indicator is shown as below:","9e48a35f":"#### 2. The patients\u2019 age and sex affect the their health\u2019s indicators \n\n","28507bea":"##### Results analysis","3369f67b":"The lightGBM is an gradient boosting model uses tree based learning algorithms. It is widely known as the more efficient version of XGBoosting model and can have great performance when dealing with large data set.","928718d6":"### Feature engineering","8abb4a16":"The null hypothesis result for each indicator is shown as below:","e7925d0b":"In this section, some basic investigation about the data is performed to gain deeper insight about the data set. Then, more analysis is conducted to answer our research question.","a1220a88":"##### Gender","c8029c87":"## Predictive analysis","1503cfcb":"The distribution of platelets and death events is in the above line chart. The area of survival, which lies between two curves, increases when the platelets in blood is from 200 kiloplatelets\/mL to 350 kiloplatelets\/mL, however, decreases when the platelets drops out of the range. \n"}}