{"cell_type":{"5b93e368":"code","81e8bd1d":"code","8c342e74":"code","17878e63":"code","c071cba5":"code","8599c3d5":"code","fe6e1555":"code","185a7d04":"code","ea2aa486":"code","b38c5f3d":"code","52ed5764":"code","0cdccafc":"code","5ba34e6d":"code","bba1fdcd":"code","b8092258":"code","af75ee94":"code","ec524d56":"code","10d2aefc":"code","21db6ff6":"code","1b02b90c":"code","50a30225":"code","6b5cf5bd":"code","10c071d4":"code","2b2c6f2b":"code","bdfb6c1a":"code","2af598f6":"code","5faaf500":"code","ba3793f0":"code","fcde303a":"code","144a5cd1":"code","568d14b5":"code","0058d3e0":"code","14b76d78":"code","38e59dba":"code","8ad867fa":"code","74252ca6":"code","6ca4ac83":"code","f48cc559":"code","c04d543b":"code","52660f04":"code","176d8c9d":"code","546d0b26":"code","e31ad3d8":"code","601866f5":"code","7bdc59ac":"code","c5e189b4":"code","23481440":"code","bea6ac97":"code","f024ec3e":"code","ef4c533f":"code","b04a9859":"code","0788d8fe":"code","909da383":"code","bfe18d85":"code","8cf45142":"code","9685fc78":"code","171d8cf9":"code","270eb99d":"code","946cd779":"code","b58e9e14":"code","133e1a97":"code","d6b287fa":"code","b9227f27":"code","28bf0b65":"code","e4b226e7":"code","5963a39b":"code","9fb68532":"code","e8bc9745":"code","1a49a0ef":"code","7b7d6207":"code","c86653e8":"code","4e2899e0":"code","2e9ad40f":"code","34cc1309":"code","bb7faa0a":"code","7bd796c7":"code","c021e7da":"code","a01d2bc9":"code","3700c0e4":"code","e41dd1a7":"code","401bed9e":"code","1a57373e":"code","6ec8d60d":"code","bd3ded0d":"code","fc66ec27":"code","a8f4e861":"code","cdceb072":"code","0a17e1bc":"code","a33eccb9":"code","acbfce90":"code","dd56439d":"code","ef0b4955":"code","634c4144":"code","8d2e5a83":"code","76e6c1f8":"code","bcf9fd0e":"code","bd3b9b71":"code","62dc26d6":"code","33f9ce62":"code","c9c28a46":"code","d8bdea11":"code","e2a516b1":"code","b22c9146":"code","a5380643":"markdown","0066e064":"markdown","5e1dbe3f":"markdown","373905da":"markdown","f1cbcfc1":"markdown","0fc718d2":"markdown","0792067e":"markdown","ddcaf51b":"markdown","bcebf20e":"markdown","27e4c371":"markdown","f7f7c6f3":"markdown","97255270":"markdown","1a54264d":"markdown","9309eb45":"markdown","67a431e3":"markdown","c9f0d887":"markdown","6ac249e3":"markdown","bd817eca":"markdown","f45de29b":"markdown","2a65efc2":"markdown","a321fe93":"markdown","a9f2b448":"markdown","ad3d4d68":"markdown","99ed0d86":"markdown","72311433":"markdown","be07bf66":"markdown","92f325fb":"markdown","8acf46d0":"markdown","6c4d9d4a":"markdown","45aa3adf":"markdown","3d508b97":"markdown","83348da0":"markdown","532f0a89":"markdown","aa51f26d":"markdown","05a61222":"markdown","757fd3c3":"markdown","e8c5ff70":"markdown","dc15d892":"markdown","e9366fd8":"markdown","1a237e58":"markdown","5c1502dd":"markdown","49680a20":"markdown","4b6ce772":"markdown","1737b73c":"markdown","e1d18860":"markdown","1683e9b1":"markdown","239791af":"markdown","1407c65b":"markdown","fc661e02":"markdown","d236e12b":"markdown","1ce70b37":"markdown","10988b1e":"markdown","bf85e4e9":"markdown","75c15504":"markdown","4cfd8284":"markdown","94d731e0":"markdown","b65a0f65":"markdown","a44a92ce":"markdown","7f176adf":"markdown","519248ac":"markdown","2caa111b":"markdown","ffd8f0bb":"markdown","2b1fa293":"markdown","3d7cdf08":"markdown","f1e0e497":"markdown","b396c87f":"markdown","cbf86630":"markdown","fadc8564":"markdown","d3dbcfc1":"markdown","14a18dfc":"markdown","81cd9197":"markdown","8659fa75":"markdown","354d86db":"markdown","49d03d76":"markdown"},"source":{"5b93e368":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport holidays\n\nfrom IPython.core.display  import HTML\nfrom IPython.display       import Image\nfrom datetime              import date\nfrom tabulate              import tabulate\nfrom scipy.stats           import chi2_contingency\n\nfrom boruta                import BorutaPy\nfrom sklearn.ensemble      import RandomForestRegressor\n\nfrom sklearn.metrics       import mean_absolute_error, mean_squared_error\nfrom sklearn.linear_model  import LinearRegression, Lasso\nfrom sklearn.ensemble      import RandomForestRegressor\nimport xgboost as xgb\n\nimport random\nimport warnings\nwarnings.filterwarnings( 'ignore' )","81e8bd1d":"def mean_percentage_error( y, yhat ):\n    return np.mean( ( y - yhat ) \/ y )\n\ndef cross_validation( X_training, kfold, model_name, model, verbose=False ):\n    \n    mae_list  = []\n    mape_list = []\n    rmse_list = []\n    \n    for k in reversed( range( 1, kfold+1 ) ):\n        if verbose:\n            print( '\\nKFold Number: {}'.format( k ) )\n        # start and end date for validation\n        validation_start_date_1  = X_training[X_training['electric_company']==1]['datetime'].max() - datetime.timedelta( days=k*12*30 )\n        validation_start_date_2  = X_training[X_training['electric_company']==2]['datetime'].max() - datetime.timedelta( days=k*12*30 )\n        validation_start_date_3  = X_training[X_training['electric_company']==3]['datetime'].max() - datetime.timedelta( days=k*12*30 )\n        validation_start_date_4  = X_training[X_training['electric_company']==4]['datetime'].max() - datetime.timedelta( days=k*12*30 )\n        validation_start_date_5  = X_training[X_training['electric_company']==5]['datetime'].max() - datetime.timedelta( days=k*12*30 )\n        validation_start_date_6  = X_training[X_training['electric_company']==6]['datetime'].max() - datetime.timedelta( days=k*12*30 )\n        validation_start_date_7  = X_training[X_training['electric_company']==7]['datetime'].max() - datetime.timedelta( days=k*12*30 )\n        validation_start_date_8  = X_training[X_training['electric_company']==8]['datetime'].max() - datetime.timedelta( days=k*12*30 )\n        validation_start_date_9  = X_training[X_training['electric_company']==9]['datetime'].max() - datetime.timedelta( days=k*12*30 )\n        validation_start_date_10 = X_training[X_training['electric_company']==10]['datetime'].max() - datetime.timedelta( days=k*12*30 )\n        validation_start_date_11 = X_training[X_training['electric_company']==11]['datetime'].max() - datetime.timedelta( days=k*12*30 )\n        validation_start_date_12 = X_training[X_training['electric_company']==12]['datetime'].max() - datetime.timedelta( days=k*12*30 )\n\n        validation_end_date_1    = X_training[X_training['electric_company']==1]['datetime'].max() - datetime.timedelta( days=(k-1)*12*30 )\n        validation_end_date_2    = X_training[X_training['electric_company']==2]['datetime'].max() - datetime.timedelta( days=(k-1)*12*30 )\n        validation_end_date_3    = X_training[X_training['electric_company']==3]['datetime'].max() - datetime.timedelta( days=(k-1)*12*30 )\n        validation_end_date_4    = X_training[X_training['electric_company']==4]['datetime'].max() - datetime.timedelta( days=(k-1)*12*30 )\n        validation_end_date_5    = X_training[X_training['electric_company']==5]['datetime'].max() - datetime.timedelta( days=(k-1)*12*30 )\n        validation_end_date_6    = X_training[X_training['electric_company']==6]['datetime'].max() - datetime.timedelta( days=(k-1)*12*30 )\n        validation_end_date_7    = X_training[X_training['electric_company']==7]['datetime'].max() - datetime.timedelta( days=(k-1)*12*30 )\n        validation_end_date_8    = X_training[X_training['electric_company']==8]['datetime'].max() - datetime.timedelta( days=(k-1)*12*30 )\n        validation_end_date_9    = X_training[X_training['electric_company']==9]['datetime'].max() - datetime.timedelta( days=(k-1)*12*30 )\n        validation_end_date_10   = X_training[X_training['electric_company']==10]['datetime'].max() - datetime.timedelta( days=(k-1)*12*30 )\n        validation_end_date_11   = X_training[X_training['electric_company']==11]['datetime'].max() - datetime.timedelta( days=(k-1)*12*30 )\n        validation_end_date_12   = X_training[X_training['electric_company']==12]['datetime'].max() - datetime.timedelta( days=(k-1)*12*30 )\n            \n        # filtering dataset\n        training = X_training[ ( (X_training['electric_company']==1) & (X_training['datetime'] < validation_start_date_1) )\n                 | ( (X_training['electric_company']==2) & (X_training['datetime'] < validation_start_date_2) )\n                 | ( (X_training['electric_company']==3) & (X_training['datetime'] < validation_start_date_3) )\n                 | ( (X_training['electric_company']==4) & (X_training['datetime'] < validation_start_date_4) )\n                 | ( (X_training['electric_company']==5) & (X_training['datetime'] < validation_start_date_5) )\n                 | ( (X_training['electric_company']==6) & (X_training['datetime'] < validation_start_date_6) )\n                 | ( (X_training['electric_company']==7) & (X_training['datetime'] < validation_start_date_7) )\n                 | ( (X_training['electric_company']==8) & (X_training['datetime'] < validation_start_date_8) )\n                 | ( (X_training['electric_company']==9) & (X_training['datetime'] < validation_start_date_9) )\n                 | ( (X_training['electric_company']==10)  & (X_training['datetime'] < validation_start_date_10) ) \n                 | ( (X_training['electric_company']==11) & (X_training['datetime'] < validation_start_date_11) )\n                 | ( (X_training['electric_company']==12) & (X_training['datetime'] < validation_start_date_12) ) ]\n        \n        validation = X_training[ ( (X_training['electric_company']==1) & (X_training['datetime'] >= validation_start_date_1) & (X_training['datetime'] <= validation_end_date_1) ) \n                 | ( (X_training['electric_company']==2) & (X_training['datetime'] >= validation_start_date_2) & (X_training['datetime'] <= validation_end_date_2) )\n                 | ( (X_training['electric_company']==3) & (X_training['datetime'] >= validation_start_date_3) & (X_training['datetime'] <= validation_end_date_3) )\n                 | ( (X_training['electric_company']==4) & (X_training['datetime'] >= validation_start_date_4) & (X_training['datetime'] <= validation_end_date_4) )\n                 | ( (X_training['electric_company']==5) & (X_training['datetime'] >= validation_start_date_5) & (X_training['datetime'] <= validation_end_date_5) )\n                 | ( (X_training['electric_company']==6) & (X_training['datetime'] >= validation_start_date_6) & (X_training['datetime'] <= validation_end_date_6) )\n                 | ( (X_training['electric_company']==7) & (X_training['datetime'] >= validation_start_date_7) & (X_training['datetime'] <= validation_end_date_7) )\n                 | ( (X_training['electric_company']==8) & (X_training['datetime'] >= validation_start_date_8) & (X_training['datetime'] <= validation_end_date_8) )\n                 | ( (X_training['electric_company']==9) & (X_training['datetime'] >= validation_start_date_9) & (X_training['datetime'] <= validation_end_date_9) )\n                 | ( (X_training['electric_company']==10) & (X_training['datetime'] >= validation_start_date_10) & (X_training['datetime'] <= validation_end_date_10) ) \n                 | ( (X_training['electric_company']==11) & (X_training['datetime'] >= validation_start_date_11) & (X_training['datetime'] <= validation_end_date_11) )\n                 | ( (X_training['electric_company']==12) & (X_training['datetime'] >= validation_start_date_12) & (X_training['datetime'] <= validation_end_date_12) ) ]\n        \n        # training and validation dataset\n        # training\n        xtraining = training.drop( ['datetime', 'mw_energy_consumption' ], axis=1 )\n        ytraining = training['mw_energy_consumption']\n\n        #validation\n        xvalidation = validation.drop( ['datetime', 'mw_energy_consumption'], axis=1 )\n        yvalidation = validation['mw_energy_consumption']\n\n        # model\n        m = model.fit( xtraining, ytraining )\n\n        # prediction\n        yhat = m.predict( xvalidation )\n\n        # performance\n        m_result = ml_error( model_name, np.expm1( yvalidation ), np.expm1( yhat ) )\n\n        # store performance of each kfold interation\n        mae_list.append( m_result['MAE'] )\n        mape_list.append( m_result['MAPE'] )\n        rmse_list.append( m_result['RMSE'] )\n\n    return pd.DataFrame( { 'Model Name': model_name,\n                            'MAE CV': np.round( np.mean( mae_list ), 2 ).astype( str ) + ' +\/- ' + np.round( np.std( mae_list ), 2 ).astype( str ),\n                            'MAPE CV': np.round( np.mean( mape_list ), 2 ).astype( str ) + ' +\/- ' + np.round( np.std( mape_list ), 2 ).astype( str ),\n                            'RMSE CV': np.round( np.mean( rmse_list ), 2 ).astype( str ) + ' +\/- ' + np.round( np.std( rmse_list ), 2 ).astype( str ) }, index=[0] )\n\ndef mean_absolute_percentage_error( y, yhat ):\n    return np.mean( np.abs( (y - yhat ) \/ y ) )\n\ndef ml_error( model_name, y, yhat ):\n    mae = mean_absolute_error( y, yhat )\n    mape = mean_absolute_percentage_error( y, yhat )\n    rmse = np.sqrt( mean_squared_error( y, yhat ) )\n    \n    return pd.DataFrame( { 'Model Name': model_name,\n                           'MAE': mae,\n                           'MAPE': mape,\n                           'RMSE': rmse }, index=[0] )\n\ndef cramer_v( x, y ):\n    cm = pd.crosstab( x, y ).values\n    n = cm.sum()\n    r, k = cm.shape\n    \n    chi2 = chi2_contingency( cm )[0]\n    chi2corr = max( 0, chi2 - (k-1)*(r-1)\/(n-1) )\n    \n    kcorr = k - (k-1)**2\/(n-1)\n    rcorr = r - (r-1)**2\/(n-1)\n\n    return np.sqrt( (chi2corr\/n) \/ ( min( kcorr-1, rcorr-1 ) ) )\n\ndef jupyter_settings():\n    %matplotlib inline\n    %pylab inline\n    \n    plt.style.use( 'bmh' )\n    plt.rcParams['figure.figsize'] = [25, 12]\n    plt.rcParams['font.size'] = 24\n    \n    display( HTML( '<style>.container { width:100% !important; }<\/style>') )\n    pd.options.display.max_columns = None\n    pd.options.display.max_rows = None\n    pd.set_option( 'display.expand_frame_repr', False )\n    \n    sns.set()","8c342e74":"jupyter_settings()","17878e63":"aep = pd.read_csv( '..\/input\/hourly-energy-consumption\/AEP_hourly.csv', low_memory=False )\ncomed = pd.read_csv( '..\/input\/hourly-energy-consumption\/COMED_hourly.csv', low_memory=False )\ndayton = pd.read_csv( '..\/input\/hourly-energy-consumption\/DAYTON_hourly.csv', low_memory=False )\ndeok = pd.read_csv( '..\/input\/hourly-energy-consumption\/DEOK_hourly.csv', low_memory=False )\ndom = pd.read_csv( '..\/input\/hourly-energy-consumption\/DOM_hourly.csv', low_memory=False )\nduq = pd.read_csv( '..\/input\/hourly-energy-consumption\/DUQ_hourly.csv', low_memory=False )\nekpc = pd.read_csv( '..\/input\/hourly-energy-consumption\/EKPC_hourly.csv', low_memory=False )\nfe = pd.read_csv( '..\/input\/hourly-energy-consumption\/FE_hourly.csv', low_memory=False )\nni = pd.read_csv( '..\/input\/hourly-energy-consumption\/NI_hourly.csv', low_memory=False )\npjm = pd.read_csv( '..\/input\/hourly-energy-consumption\/PJM_Load_hourly.csv', low_memory=False )\npjme = pd.read_csv( '..\/input\/hourly-energy-consumption\/PJME_hourly.csv', low_memory=False )\npjmw = pd.read_csv( '..\/input\/hourly-energy-consumption\/PJMW_hourly.csv', low_memory=False )","c071cba5":"# I will combine all the datasets into one. For this I will create a column in each of the datasets that will identify your respective electric company.\n\naep['electric_company'] = 'AEP'\ncomed['electric_company'] = 'COMED'\ndayton['electric_company'] = 'DAYTON'\ndeok['electric_company'] = 'DEOK'\ndom['electric_company'] = 'DOM'\nduq['electric_company'] = 'DUQ'\nekpc['electric_company'] = 'EKPC'\nfe['electric_company'] = 'FE'\nni['electric_company'] = 'NI'\npjm['electric_company'] = 'PJM'\npjme['electric_company'] = 'PJME'\npjmw['electric_company'] = 'PJMW'","8599c3d5":"# the column in each dataset that shows the Megawatt Energy Consumption, I will rename it with the same name in all.\n\naep = aep.rename( columns={'AEP_MW': 'mw_energy_consumption'} )\ncomed = comed.rename( columns={'COMED_MW': 'mw_energy_consumption'} )\ndayton = dayton.rename( columns={'DAYTON_MW': 'mw_energy_consumption'} )\ndeok = deok.rename( columns={'DEOK_MW': 'mw_energy_consumption'} )\ndom = dom.rename( columns={'DOM_MW': 'mw_energy_consumption'} )\nduq = duq.rename( columns={'DUQ_MW': 'mw_energy_consumption'} )\nekpc = ekpc.rename( columns={'EKPC_MW': 'mw_energy_consumption'} )\nfe = fe.rename( columns={'FE_MW': 'mw_energy_consumption'} )\nni = ni.rename( columns={'NI_MW': 'mw_energy_consumption'} )\npjm = pjm.rename( columns={'PJM_Load_MW': 'mw_energy_consumption'} )\npjme = pjme.rename( columns={'PJME_MW': 'mw_energy_consumption'} )\npjmw = pjmw.rename( columns={'PJMW_MW': 'mw_energy_consumption'} )","fe6e1555":"# Concatenate all the datasets\ndf_raw = pd.concat( [aep, comed, dayton, deok, dom, duq, ekpc, fe, ni, pjm, pjme, pjmw] )\ndf_raw.sample(5) # check","185a7d04":"df_raw.shape","ea2aa486":"# At the beginning of each step I will make a copy of the dataset to make it easier to reload the data in case there is a problem with the project.\ndf1 = df_raw.copy()","b38c5f3d":"# Although I already renamed some columns when I was preparing the datasets to concatenate them, I will check if there are any more columns that are not in the format I want them to be.\ndf1.columns","52ed5764":"# Only the column \"Datetime\" don't have the format that I want.\ncols_old = ['Datetime', 'mw_energy_consumption', 'electric_company']\n\n# If the dataset had too many columns to change I would use the \"snakecase\" technique through the \"inflection\" function. Since this dataset has only one column to change the name, I will do this process manually.\ncols_new = ['datetime', 'mw_energy_consumption', 'electric_company']\n\n# rename\ndf1.columns = cols_new","0cdccafc":"df1.columns","5ba34e6d":"print( 'Number of Rows: {}'.format( df1.shape[0] ) )\nprint( 'Number of Cols: {}'.format( df1.shape[1] ) )\n# Evaluate the possibilite do use this project in your computer","bba1fdcd":"df1.dtypes\n# Observe datetime. It has a different data type.","b8092258":"# function \"datetime\" transforma the columns \"datetime\" from object (string) to datetime.\ndf1['datetime'] = pd.to_datetime( df1['datetime'] )","af75ee94":"df1.dtypes","ec524d56":"df1.isna().sum()","10d2aefc":"# We don't have NA value to fillout... :)","21db6ff6":"# We already did the change types.","1b02b90c":"# divide into numerical and categorical columns, as they will be treated with different techniques.\nnum_attributes = df1.select_dtypes( include=['float64'] )\ncat_attributes = df1.select_dtypes( exclude=['float64', 'datetime64[ns]'] )","50a30225":"num_attributes.sample()","6b5cf5bd":"cat_attributes.sample()","10c071d4":"# Central Tendency - mean, median\nct1 = pd.DataFrame( num_attributes.apply( np.mean ) ).T\nct2 = pd.DataFrame( num_attributes.apply( np.median ) ).T\n\n# Dispersion - std, min, max, range, skew, kurtosis\nd1 = pd.DataFrame( num_attributes.apply( np.std ) ).T\nd2 = pd.DataFrame( num_attributes.apply( min ) ).T\nd3 = pd.DataFrame( num_attributes.apply( max ) ).T\nd4 = pd.DataFrame( num_attributes.apply( lambda x: x.max() - x.min() ) ).T\nd5 = pd.DataFrame( num_attributes.apply( lambda x: x.skew() ) ).T\nd6 = pd.DataFrame( num_attributes.apply( lambda x: x.kurtosis() ) ).T\n\n# concatenate\nm = pd.concat( [d2, d3, d4, ct1, ct2, d1, d5, d6] ).T.reset_index()\nm.columns = ( ['attributes', 'min', 'max', 'range', 'mean', 'median', 'std', 'skew', 'kurtosis'])","2b2c6f2b":"m","bdfb6c1a":"sns.distplot( df1['mw_energy_consumption'] )","2af598f6":"cat_attributes.apply( lambda x: x.unique().shape[0] )","5faaf500":"sns.boxplot( x= 'electric_company', y='mw_energy_consumption' , data=df1 )","ba3793f0":"df2 = df1.copy()","fcde303a":"Image( '..\/input\/image\/Hyphoteses_Map.png' )\n# As we can see in the Hypothesis Map below, there are many factors that influence the consumption of electricity and that we could use to try to predict them.\n# The point is that we only have \"datetime\" information in the dataset. Because of this we will only use a few elements and their attributes.","144a5cd1":"## date\n# I want to have a view of the evolution of energy consumption by looking only at the dates as information, excluding an hour when they were recorded.\n\ndf2['date'] = df2['datetime'].dt.date\n\n## year\n# I will not directly validate any hypothesis with the date \"year\", but I will use this information to better visualize some variables in the validation process within the EDA.\n\ndf2['year'] = df2['datetime'].dt.year\n\n## month\n# I will not directly validate any hypothesis with the date \"month\", but I will use this information to better visualize some variables in the validation process within the EDA.\n\ndf2['month'] = df2['datetime'].dt.month\n\n## hour_of_day:\n\n# Initially I thought about converting the hours in the \"datetime\" column to float, because it would be better to visualize the energy consumption by the evolution of the hours, but observing the part of the hours in the \"datetime\" column we can see that all of them have only the same hour and not the minute and second. For this reason I will only create the \"hour_of_day\" column using the \"hour\" function.\n\ndf2['hour_of_day'] = df2['datetime'].dt.hour\n\n# season:\n\n# - Winter -> December (12), January (01) and February (02);\n# - Spring -> March (03), April (04) and May (05);\n# - Summer -> Jun (06), July (07) and August (08);\n# - Autumn -> September (09), October (10) and November (11).\n\ndf2['season'] = df2['datetime'].apply( lambda x: 'Winter' if x.month == 12 or x.month == 1 or x.month == 2 else 'Spring' if  x.month == 3 or x.month == 4 or x.month == 5 else 'Summer' if  x.month == 6 or x.month == 7 or x.month == 8 else 'Autumn' if  x.month == 9 or x.month == 10 or x.month == 11 else '')\n\n## holiday:\n\n# I will use the \"holidays\" library to identify whether the date represents a holiday. An idea for a next cycle in the project is to differentiate, according to some rule, the different dates of the holidays.\n# The documentation of this librarie is in this link: https:\/\/pypi.org\/project\/holidays\/\n# Another rule that I will use is to consider the eve of the dates as a \"holiday\".\n\ndf2['holidays'] = df2['datetime'].apply( lambda x: 'Holiday' if x in holidays.US() else 'Holiday' if x + datetime.timedelta(days=1) in holidays.US() else 'Normal day' )\n\n## day_of_week:\n\n# 0 - Monday\n# 1 - Tuesday\n# 2 - Wednesday\n# 3 - Thursday\n# 4 - Friday\n# 5 - Saturday\n# 6 - Sunday\n\ndf2['day_of_week'] = df2['datetime'].dt.weekday","568d14b5":"# Recheck data types after feature engineering\ndf2.dtypes","0058d3e0":"# The column \"date\" is a datetime type columns, but it is like a object type column. I will change the type.\ndf2['date'] = pd.to_datetime( df2['date'] )","14b76d78":"df2.dtypes","38e59dba":"num_attributes = df2.select_dtypes( include=['int64', 'float64'] )\ncat_attributes = df2.select_dtypes( exclude=['int64', 'float64', 'datetime64[ns]'] )","8ad867fa":"df2.sample(10).T","74252ca6":"df3 = df2.copy()","6ca4ac83":"df3.head()","f48cc559":"# There isn't any line filtering to do in this project","c04d543b":"# There isn't any column selection to do in this project","52660f04":"df4 = df3.copy()","176d8c9d":"sns.distplot( df4['mw_energy_consumption'], kde=False )","546d0b26":"num_attributes.hist( bins=25 )","e31ad3d8":"cat_attributes.head()","601866f5":"df4['electric_company'].drop_duplicates()","7bdc59ac":"df4['season'].drop_duplicates()","c5e189b4":"df4['holidays'].drop_duplicates()","23481440":"# electric_company\n\nplt.subplot( 3, 2, 1)\nsns.countplot( df4['electric_company'] )\n\nplt.subplot( 3, 2, 2)\nsns.kdeplot( df4[df4['electric_company'] == 'AEP']['mw_energy_consumption'], label='AEP', shade=True )\nsns.kdeplot( df4[df4['electric_company'] == 'COMED']['mw_energy_consumption'], label='COMED', shade=True )\nsns.kdeplot( df4[df4['electric_company'] == 'DAYTON']['mw_energy_consumption'], label='DAYTON', shade=True )\nsns.kdeplot( df4[df4['electric_company'] == 'DEOK']['mw_energy_consumption'], label='DEOK', shade=True )\nsns.kdeplot( df4[df4['electric_company'] == 'DOM']['mw_energy_consumption'], label='DOM', shade=True )\nsns.kdeplot( df4[df4['electric_company'] == 'DUQ']['mw_energy_consumption'], label='DUQ', shade=True )\nsns.kdeplot( df4[df4['electric_company'] == 'EKPC']['mw_energy_consumption'], label='EKPC', shade=True )\nsns.kdeplot( df4[df4['electric_company'] == 'FE']['mw_energy_consumption'], label='FE', shade=True )\nsns.kdeplot( df4[df4['electric_company'] == 'NI']['mw_energy_consumption'], label='NI', shade=True )\nsns.kdeplot( df4[df4['electric_company'] == 'PJM']['mw_energy_consumption'], label='PJM', shade=True )\nsns.kdeplot( df4[df4['electric_company'] == 'PJME']['mw_energy_consumption'], label='PJME', shade=True )\nsns.kdeplot( df4[df4['electric_company'] == 'PJMW']['mw_energy_consumption'], label='PJMW', shade=True )\n\n# season\n\nplt.subplot( 3, 2, 3)\nsns.countplot( df4['season'] )\n\nplt.subplot( 3, 2, 4)\nsns.kdeplot( df4[df4['season'] == 'Winter']['mw_energy_consumption'], label='Winter', shade=True )\nsns.kdeplot( df4[df4['season'] == 'Autumn']['mw_energy_consumption'], label='Autumn', shade=True )\nsns.kdeplot( df4[df4['season'] == 'Summer']['mw_energy_consumption'], label='Summer', shade=True )\nsns.kdeplot( df4[df4['season'] == 'Spring']['mw_energy_consumption'], label='Spring', shade=True )\n\n# holidays\n\nplt.subplot( 3, 2, 5)\nsns.countplot( df4['holidays'] )\n\nplt.subplot( 3, 2, 6)\nsns.kdeplot( df4[df4['holidays'] == 'Holiday']['mw_energy_consumption'], label='Holiday', shade=True )\nsns.kdeplot( df4[df4['holidays'] == 'Normal day']['mw_energy_consumption'], label='Normal day', shade=True )","bea6ac97":"plt.subplot(1,3,1)\naux1 = df4[['hour_of_day', 'mw_energy_consumption']].groupby( 'hour_of_day' ).sum().reset_index()\nsns.barplot( x='hour_of_day', y='mw_energy_consumption', data=aux1)\n\nplt.subplot(1,3,2)\nsns.regplot( x='hour_of_day', y='mw_energy_consumption', data=aux1 )\n\nplt.subplot(1,3,3)\nsns.heatmap( aux1.corr( method='pearson' ), annot=True );","f024ec3e":"aux1 = df4[['season', 'mw_energy_consumption']].groupby( 'season' ).sum().reset_index()\nplt.subplot( 2, 1, 1 )\nsns.barplot( x='season', y='mw_energy_consumption', data=aux1, order=['Winter', 'Spring', 'Summer', 'Autumn'] )\n\naux2 = df4[['year', 'season', 'mw_energy_consumption']].groupby( ['year', 'season'] ).sum().reset_index()\nplt.subplot( 2, 1, 2 )\nsns.barplot( x='year', y='mw_energy_consumption', hue='season', data=aux2, orient=['Winter', 'Spring', 'Summer', 'Autumn'] );","ef4c533f":"aux1 = df4[['holidays', 'mw_energy_consumption']].groupby( 'holidays' ).sum().reset_index()\nplt.subplot( 2, 1, 1 )\nsns.barplot( x='holidays', y='mw_energy_consumption', data=aux1 )\n\naux2 = df4[['year', 'holidays', 'mw_energy_consumption']].groupby( ['year', 'holidays'] ).sum().reset_index()\nplt.subplot( 2, 1, 2 )\nsns.barplot( x='year', y='mw_energy_consumption', hue='holidays', data=aux2 );","b04a9859":"plt.subplot( 2, 2, 1)\naux1 = df4[['day_of_week', 'mw_energy_consumption']].groupby( 'day_of_week' ).sum().reset_index()\nsns.barplot( x='day_of_week', y='mw_energy_consumption', data=aux1)\n\naux2 = df4[['month', 'day_of_week', 'mw_energy_consumption']].groupby( ['month', 'day_of_week'] ).sum().reset_index()\nplt.subplot( 2, 2, 2 )\nsns.barplot( x='month', y='mw_energy_consumption', hue='day_of_week', data=aux2 );\n\nplt.subplot(2, 2, 3)\nsns.regplot( x='day_of_week', y='mw_energy_consumption', data=aux1 )\n\nplt.subplot(2, 2, 4)\nsns.heatmap( aux1.corr( method='pearson' ), annot=True );\n\n# To better understand what these numbers represent in relation to the days of the week follow the legend below:\n# Integer Value             Day of the week\n#      0                        Monday\n#      1                       Tuesday\n#      2                      Wednesday\n#      3                      Thursday\n#      4                       Friday\n#      5                      Saturday\n#      6                       Sunday","0788d8fe":"tab = [['Hypotheses', 'Conclusion', 'Relevance'],\n       ['H1', 'False', 'High'],\n       ['H2', 'True', 'Medium'],\n       ['H3', 'False', 'Medium'],\n       ['H4', 'False', 'High']\n      ]\nprint( tabulate( tab, headers='firstrow' ) )","909da383":"correlation = num_attributes.corr( method='pearson' )\nsns.heatmap( correlation, annot=True )\n\n# As we can see, the numerical variables (year and month) that we created just to help visualize the evolution of energy consumption, have very little relevance.","bfe18d85":"# When we are working with categorical variables we use another statistical technique to visualize the correlations between them. This technique is called \"Cram\u00e9r's V\" and a function was created with its formula and it is in the section \"0.1 Helper Function\".\n# In practice, below I will create by hand the correlation table between categorical variables.\n\n# Only categorical data\nc1 = cramer_v( cat_attributes['electric_company'], cat_attributes['electric_company'] )\n\n# Calculate cramer V\nc2 = cramer_v( cat_attributes['electric_company'], cat_attributes['season'] )\nc3 = cramer_v( cat_attributes['electric_company'], cat_attributes['holidays'] )\n\nc4 = cramer_v( cat_attributes['season'], cat_attributes['electric_company'] )\nc5 = cramer_v( cat_attributes['season'], cat_attributes['season'] )\nc6 = cramer_v( cat_attributes['season'], cat_attributes['holidays'] )\n\nc7 = cramer_v( cat_attributes['holidays'], cat_attributes['electric_company'] )\nc8 = cramer_v( cat_attributes['holidays'], cat_attributes['season'] )\nc9 = cramer_v( cat_attributes['holidays'], cat_attributes['holidays'] )\n\n# Final dataset\nd = pd.DataFrame( {'electric_company': [c1, c2, c3],\n               'season': [c4, c5, c6],\n               'holidays': [c7, c8, c9]} )\n\nd = d.set_index( d.columns )\n\nsns.heatmap( d, annot=True )\n","8cf45142":"df5 = df4.copy()","9685fc78":"# We don't have any data distribution like a Gaussian","171d8cf9":"# We don't have any numerical variables that need scaling.\n# The numerical variables in the dataset will need a nature transformation (cyclical).","270eb99d":"# The only categorical variable which I will use the encoding will be the \"holidays\". I won't use the \"season\" in this moment because it's a cyclical variable and I will transform it after soon.\n\n# As the values in the variable \"holidays\" do not have an important relationship between them and our final dataset will not have many columns I will use the \"One Hot Encoding\" technique.\n\ndf5 = pd.get_dummies( df5, prefix=['holidays'], columns=['holidays'] )","946cd779":"df5['mw_energy_consumption'] = np.log1p( df5['mw_energy_consumption'] )","b58e9e14":"sns.distplot( df5['mw_energy_consumption'] )","133e1a97":"# My goal here is to take the numerical data and transform its natures to cyclical.\n# The cyclical variable in this dataset are \"hour_of_day\", \"season\" and \"day_of_week\"\n\n# I am not going to go into mathematical details about the method used to turn numeric or categorical variables into cyclical ones, but I am going to give you an idea of what will be done.\n# Cyclic behavior looks like a circle. The code will place the values within each variable in this circle and for this the concepts of sine and cosine will be used. \n# Note that at the end of the formula I divide by the amount of cyclic values present in each variable.","d6b287fa":"# hour_of_day\ndf5['hour_of_day_sin'] = df5['hour_of_day'].apply( lambda x: np.sin( x * ( 2. * np.pi\/24 ) ) )\ndf5['hour_of_day_cos'] = df5['hour_of_day'].apply( lambda x: np.cos( x * ( 2. * np.pi\/24 ) ) )\n\n\n# season\n\n# As the variable \"season\" is categorical I will first turn it into a numeric variable making its values \u200b\u200bhave an ordinal idea, since the seasons follow an order.\nassortment_dict = {'Winter': 1, 'Spring': 2, 'Summer': 3, 'Autumn': 4}\ndf5['season'] = df5['season'].map( assortment_dict )\n\ndf5['season_sin'] = df5['season'].apply( lambda x: np.sin( x * ( 2. * np.pi\/4 ) ) )\ndf5['season_cos'] = df5['season'].apply( lambda x: np.cos( x * ( 2. * np.pi\/4 ) ) )\n\n\n# day_of_week\ndf5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi\/7 ) ) )\ndf5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi\/7 ) ) )","b9227f27":"df5.sample(5)","28bf0b65":"df6 = df5.copy()","e4b226e7":"df6.shape","5963a39b":"# First I will drop the columns that I will not use. Basically they are the columns that were only used to generate new columns in the Feature Engineering process.\ncols_drop = [ 'year', 'month', 'hour_of_day', 'season', 'day_of_week' ]\ndf6 = df6.drop( cols_drop, axis=1 )\n\n# # As this is a time-series project, the goal is to forecast energy consumption for the next 6 months. \n# Because of this we have to separate the test and train dataset taking into account the variable \"datetime\", that is, the most recent 6 months will be in the test dataset and the rest in the train dataset.\n\n# First, I will create a table showing, for the electric company, what is the minimum and maximum date and see if it will be possible to determine the same 6-month cut in the variable \"date\".\nmin_max = []\nfor i in df6['electric_company'].unique():\n    min_max.append( [ i, df6[df6['electric_company']==i]['date'].min(), df6[df6['electric_company']==i]['date'].max() ] )\n\n# Creating de dataframe to better visualize.\nmm = pd.DataFrame( columns=[ 'electric_company', 'min_date', 'max_date' ], data=min_max )\nmm","9fb68532":"# Since we do not have the same minimum and maximum dates for all electrical companies, I will stipulate the minimum and maximum cutoffs for test and training dates for each one.\n# Rules:\n# - min_date <= TRAIN DATA < cut_date (max_date - 12 months)\n# - cut_date (max_date - 12 months) <= TEST DATA <= max_date\n\n# Determine the cutoff date at each electrical company.\ncut_date =[]\nfor i in range( 0, 12 ):\n    cut_date.append( [ df6['electric_company'].unique()[i], df6[['electric_company', 'date']].groupby( 'electric_company' ).max().reset_index()['date'][i] - datetime.timedelta( days=12*30 ) ] )\n\n# Creating de dataframe to better visualize.\ncd = pd.DataFrame( columns=[ 'electric_company', 'cut_date' ], data=cut_date )\ncd","e8bc9745":"# training dataset\nX_train = df6[ ( ( (df6['electric_company']=='AEP') \n                 | (df6['electric_company']=='COMED') \n                 | (df6['electric_company']=='DAYTON') \n                 | (df6['electric_company']=='DEOK') \n                 | (df6['electric_company']=='DOM') \n                 | (df6['electric_company']=='DUQ') \n                 | (df6['electric_company']=='EKPC') \n                 | (df6['electric_company']=='FE') \n                 | (df6['electric_company']=='PJME') \n                 | (df6['electric_company']=='PJMW') ) & (df6['date'] < '2017-08-08') ) \n                 | (df6['electric_company']=='NI') & (df6['date'] < '2010-01-06') \n                 | (df6['electric_company']=='PJM') & (df6['date'] < '2001-01-06') ]\n\ny_train = X_train['mw_energy_consumption']\n\n# test dataset\nX_test = df6[ ( ( (df6['electric_company']=='AEP') \n                 | (df6['electric_company']=='COMED') \n                 | (df6['electric_company']=='DAYTON') \n                 | (df6['electric_company']=='DEOK') \n                 | (df6['electric_company']=='DOM') \n                 | (df6['electric_company']=='DUQ') \n                 | (df6['electric_company']=='EKPC') \n                 | (df6['electric_company']=='FE') \n                 | (df6['electric_company']=='PJME') \n                 | (df6['electric_company']=='PJMW') ) & (df6['date'] >= '2017-08-08') ) \n                 | (df6['electric_company']=='NI') & (df6['date'] >= '2010-01-06') \n                 | (df6['electric_company']=='PJM') & (df6['date'] >= '2001-01-06') ]\n\ny_test = X_test['mw_energy_consumption']\n\n# Determine the train min and max date & test min and max date.\ntrain_test =[]\nfor i in df6['electric_company'].unique():\n    train_test.append( [ i, X_train[X_train['electric_company']==i]['date'].min(), X_train[X_train['electric_company']==i]['date'].max(), \n                        X_test[X_test['electric_company']==i]['date'].min(), X_test[X_test['electric_company']==i]['date'].max() ] )\n\n# Creating de dataframe to better visualize.\ntt = pd.DataFrame( columns=[ 'electric_company', 'train_min_date', 'train_max_date', 'test_min_date', 'test_max_date' ], data=train_test )\ntt","1a49a0ef":"# For the algorithm understand the variable \"electric company\" we have to transform from string to number.\nassortment_dict = { 'AEP': 1, 'COMED': 2, 'DAYTON': 3, 'DEOK': 4, 'DOM': 5, 'DUQ': 6, 'EKPC': 7, 'FE': 8, 'NI': 9, 'PJM': 10, 'PJME': 11, 'PJMW': 12 }\nX_train['electric_company'] = X_train['electric_company'].map( assortment_dict )\nX_test['electric_company'] = X_test['electric_company'].map( assortment_dict );","7b7d6207":"# I will use the Boruta algorithm to determine the most relevant variables for the model and then I will compare it with what I decided at the end of the EDA.\n\n# training and test dataset for Boruta\nX_train_n = X_train.drop( ['date', 'datetime', 'mw_energy_consumption'], axis=1 ).values\ny_train_n = y_train.values.ravel()\n\n# define RandomForestRegression\nrf = RandomForestRegressor( n_jobs=-1 )\n\n# define Boruta\nboruta = BorutaPy( rf, n_estimators='auto', verbose=2, random_state=42 ).fit( X_train_n, y_train_n )","c86653e8":"cols_selected = boruta.support_.tolist()\n\n# best feature\nX_train_fs = X_train.drop( ['date', 'datetime', 'mw_energy_consumption'], axis=1 )\ncols_selected_boruta = X_train_fs.iloc[:, cols_selected].columns.tolist()\n\n# not selected boruta\ncols_not_selected_boruta = list( np.setdiff1d( X_train_fs.columns, cols_selected_boruta ) )","4e2899e0":"cols_selected_boruta","2e9ad40f":"cols_not_selected_boruta","34cc1309":"# Now I will check the features selected by Boruta and compare with the eatures that I selected in EDA.\n\ntab = [['Hypotheses', 'Conclusion', 'Relevance'],\n       ['H1', 'False', 'High'], # Confirmed.\n       ['H2', 'True', 'Medium'], # Boruta determined as High.\n       ['H3', 'False', 'Medium'], # Boruta determined as Low.\n       ['H4', 'False', 'High'] # Confirmed.\n      ]\nprint( tabulate( tab, headers='firstrow' ) )","bb7faa0a":"# Now I will include the \"season\" and \"day_of_week\" cosin column.\n\ncols_selected_boruta = [\n     'electric_company',\n     'hour_of_day_sin',\n     'hour_of_day_cos',\n     'season_sin',\n     'season_cos',\n     'day_of_week_sin',\n     'day_of_week_cos']\n\n# columns to add\nfeat_to_add = ['datetime', 'mw_energy_consumption']\n\n# final features\ncols_selected_boruta_full = cols_selected_boruta.copy()\ncols_selected_boruta_full.extend( feat_to_add )","7bd796c7":"cols_selected_boruta","c021e7da":"cols_selected_boruta_full","a01d2bc9":"x_train = X_train[ cols_selected_boruta ]\nx_test = X_test[ cols_selected_boruta ]\n\n# Time Series Data Preparartion\nX_training = X_train[ cols_selected_boruta_full ]\n\n# Before continuing into sessions that we will implement the Machine Learning models, I would like to explain about the error meanings.\n# MAE:\n# It is usually what I use to report to the business team because is easier to explain.\n# It takes the absolute value of the difference between the real and the predicted number and divides it by the number of predictions, that is, for each value that the model predicts it varies the MAE value on average (both for more and for less).\n# Due to the way the MAE is calculated, it is not sensitive to outliers. If I'm on a project where having an outlier-sensitive error is super important I will use to evaluate my model another error.\n# MAPE:\n# It is also used to facilitate the understanding of the business team about the error of your model. \n# The MAPE simply represents the percentage of the MAE, that is, how much the error that the model has means in percentage of the real value.\n# RMSE:\n# This is the error most used by data scientists to measure the performance of the model and its value serves as a parameter in the process of trying to decrease the model error within the project.\n# This high use of RMSE is due to the fact that it is sensitive to outliers and this helps data scientists to be more rigorous with model errors.\n# In cases where detecting the presence of outliers is important within the project I use RMSE instead of MAE.","3700c0e4":"# The \"Average Model\" is a very basic algorithm to have a base to compare its error with the errors of the other algorithms.\n\naux1 = x_test.copy()\naux1['mw_energy_consumption'] = y_test.copy()\n\n# prediction\naux2 = aux1[['electric_company', 'mw_energy_consumption']].groupby( 'electric_company' ).mean().reset_index().rename( columns={'mw_energy_consumption': 'predictions'} )\naux1 = pd.merge( aux1, aux2, how='left', on='electric_company' )\nyhat_baseline = aux1['predictions']\n\n# performance\nbaseline_result = ml_error( 'Average Model', np.expm1( y_test ), np.expm1( yhat_baseline ) )\nbaseline_result","e41dd1a7":"# In the beginning I use the Linear Model to see how linear or non-linear our dataset is.\n\n# model\nlr = LinearRegression().fit( x_train, y_train )\n\n# prediction\nyhat_lr = lr.predict( x_test )\n\n# performance\nlr_result = ml_error( 'Linear Regression', np.expm1( y_test ), np.expm1( yhat_lr ) )\nlr_result\n\n# Seeing the value of RMSE we notice that it is more than 3 times greater than the Average Model, that is, our dataset is non-linear.","401bed9e":"# In order to have a more real number of the Linear Model error I will use \"Cross Validation\" and see if the error continues to remain very high.\n\nlr_result_cv = cross_validation( X_training, 5, 'Linear Regression', lr, verbose=False )\nlr_result_cv\n\n# As we can see the error fell a little, but it remains very high.","1a57373e":"# model\nlrr = Lasso( alpha=0.00001 ).fit( x_train, y_train )\n\n# prediction\nyhat_lrr = lrr.predict( x_test )\n\n# performance\nlrr_result = ml_error( 'Linear Regression - Lasso', np.expm1( y_test ), np.expm1( yhat_lrr ) )\nlrr_result","6ec8d60d":"lrr_result_cv = cross_validation( X_training, 5, 'Linear Regression - Lasso', lrr, verbose=False )\nlrr_result_cv","bd3ded0d":"# Now I'm going to start using a model that fits more with our dataset and one of the best and most used algorithms.\n\n# model\nrf = RandomForestRegressor( n_estimators=100, n_jobs=-1, random_state=42 ).fit( x_train, y_train )\n\n# prediction\nyhat_rf = rf.predict( x_test )\n\n# performance\nrf_result = ml_error( 'Random Forest Regressor', np.expm1( y_test ), np.expm1( yhat_rf ) )\nrf_result\n\n# Seeing the RMSE we can see that the error has plummeted. Almost ten times smaller than the \"Linear Model\" and half the \"Average Model\"\n# Seeing also the MAPE we can see that the error represents on average only 10% of the real value.","fc66ec27":"rf_result_cv = cross_validation( X_training, 5, 'Random Forest Regressor', rf, verbose=True )\nrf_result_cv\n\n# When we use \"Cross Validation\" we see that the error drops even more.","a8f4e861":"# model\nmodel_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n                             n_estimators=100, \n                             eta=0.01,\n                             max_depth=10,\n                             subsample=0.7,\n                             colsample_bytree=0.9 ).fit( x_train, y_train )\n\n# prediction\nyhat_xgb = model_xgb.predict( x_test )\n\n# performance\nxgb_result = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb ) )\nxgb_result\n\n# With this model the error went up a lot.","cdceb072":"xgb_result_cv = cross_validation( X_training, 5, 'XGBoost Regressor', model_xgb, verbose=True )\nxgb_result_cv","0a17e1bc":"# I will set up a table that shows all the algorithms used and their errors, but without taking in consideration Croos Validation.\n\nmodelling_result = pd.concat( [baseline_result, lr_result, lrr_result, rf_result, xgb_result] )\nmodelling_result.sort_values( 'RMSE' )","a33eccb9":"modelling_result_cv = pd.concat( [lr_result_cv, lrr_result_cv, rf_result_cv, xgb_result_cv] )\nmodelling_result_cv\n\n# The \"Random Forest Regressor\" is the best model of all tested here.\n# In the first cycle I will take the model and continue the project steps. In the next cycle, if necessary, I test other models and also improve data handling.","acbfce90":"# I will use the Random Search method to choose the better Hyperparameter in Random Forest Regressor\n\nparam = {\n         'bootstrap': [True, False],\n         'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n         'max_features': ['auto', 'sqrt'],\n         'min_samples_leaf': [1, 2, 4],\n         'min_samples_split': [2, 5, 10],\n         'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n        }\n\nMAX_EVAL = 5","dd56439d":"final_result = pd.DataFrame()\n\nimport random\n\nfor i in range( MAX_EVAL):\n    #chose values for parameters randomly\n    hp = { k: random.sample( v, 1 )[0] for k, v in param.items() }\n    print( hp )\n    \n    # model\n    rf = RandomForestRegressor( bootstrap = hp['bootstrap'],\n                                max_depth = hp['max_depth'],\n                                max_features = hp['max_features'],\n                                min_samples_leaf = hp['min_samples_leaf'],\n                                min_samples_split = hp['min_samples_split'],\n                                n_estimators = hp['n_estimators'] )\n\n    # performance\n    result = cross_validation( X_training, 5, 'Random Forest Regressor', rf, verbose=False)\n    final_result = pd.concat( [final_result, result] )\n\nfinal_result\n\n# We can see that the first one is the better model because the RMSE is the lower one.","ef0b4955":"param_tuned = {\n         'bootstrap': True,\n         'max_depth': 60,\n         'max_features': 'sqrt',\n         'min_samples_leaf': 4,\n         'min_samples_split': 10,\n         'n_estimators': 800\n        }","634c4144":"# model\nrf_tuned = RandomForestRegressor( bootstrap = param_tuned['bootstrap'],\n                                  max_depth = param_tuned['max_depth'],\n                                  max_features = param_tuned['max_features'],\n                                  min_samples_leaf = param_tuned['min_samples_leaf'],\n                                  min_samples_split = param_tuned['min_samples_split'],\n                                  n_estimators = param_tuned['n_estimators'] ).fit( x_train, y_train )\n\n# prediction\nyhat_rf_tuned = rf_tuned.predict( x_test )\n\n# performance\nrf_result_tuned = ml_error( 'Random Forest Regressor', np.expm1( y_test ), np.expm1( yhat_rf_tuned ) )\nrf_result_tuned","8d2e5a83":"# As the MPE value is negative, we found that the model in general is overestimating the forecast, that is, it is forecasting more values above the real.\n# In any case, the value of the MPE is very small and does not have much influence on our model.\n\nmpe = mean_percentage_error( np.expm1( y_test ), np.expm1( yhat_rf_tuned ) )\nmpe","76e6c1f8":"df9 = X_test[ cols_selected_boruta_full ]\n\n# rescale\ndf9['mw_energy_consumption'] = np.expm1( df9['mw_energy_consumption'] )\ndf9['predictions'] = np.expm1( yhat_rf_tuned )","bcf9fd0e":"# Now we are going to take the mathematical numbers, represented by the predictions and their errors, and translate it into a business result.\n# In the case of this project, let's say what the error tells us regarding the amount of energy consumed in the next year in each electric company.\n\n# sum of predictions\ndf91 = df9[['electric_company', 'predictions']].groupby( 'electric_company' ).sum().reset_index()\n\n# MAE e MAPE\ndf9_aux1 = df9[['electric_company', 'mw_energy_consumption', 'predictions']].groupby( 'electric_company' ).apply( lambda x: mean_absolute_error( x['mw_energy_consumption'], x['predictions'] ) ).reset_index().rename( columns={0:'MAE'} )\ndf9_aux2 = df9[['electric_company', 'mw_energy_consumption', 'predictions']].groupby( 'electric_company' ).apply( lambda x: mean_absolute_percentage_error( x['mw_energy_consumption'], x['predictions'] ) ).reset_index().rename( columns={0:'MAPE'} )\n\n# Merge\ndf9_aux3 = pd.merge( df9_aux1, df9_aux2, how='inner', on='electric_company' )\ndf92 = pd.merge( df91, df9_aux3, how='inner', on='electric_company' )\n\n# Scenarios\ndf92['worst_scenario'] = df92['predictions'] - df92['MAE']\ndf92['best_scenario'] = df92['predictions'] + df92['MAPE']\n\n# order columns\ndf92 = df92[['electric_company', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE']]","bd3b9b71":"df9_aux1.head(12)","62dc26d6":"# Here we can see, for each electric company, the forecasts and their respective errors.\ndf92.head(12)","33f9ce62":"df92.sort_values( 'MAPE', ascending=False ).head(12)","c9c28a46":"# Observing the distribution of the MAPE error in the graph, we can see that the model has very small errors in all cases. \n# It does not even reach the value of 0.15.\nsns.scatterplot( x='electric_company', y='MAPE', data=df92 )","d8bdea11":"df93 = df92[['predictions', 'worst_scenario', 'best_scenario']].apply( lambda x: np.sum( x ), axis=0 ).reset_index().rename( columns={'index': 'Scenario', 0:'Values'} )\ndf93['Values'] = df93['Values'].map( '{:,.2f} MW'.format )\ndf93","e2a516b1":"df9['error'] = df9['mw_energy_consumption'] - df9['predictions']\ndf9['error_rate'] = df9['predictions'] \/ df9['mw_energy_consumption']\n\n# Remember that, as we have the forecast within three different years, I have separated them into three different datasets for better visualization.\ndf9_1 = df9[(df9['electric_company']==1) | (df9['electric_company']==2) | (df9['electric_company']==3) | (df9['electric_company']==4) | (df9['electric_company']==5) | (df9['electric_company']==6)\n           | (df9['electric_company']==7) | (df9['electric_company']==8) | (df9['electric_company']==11) | (df9['electric_company']==12)]\ndf9_2 = df9[df9['electric_company']==9]\ndf9_3 = df9[df9['electric_company']==10]","b22c9146":"# Compare the real response with the one predicted and observe if the model is performing well. \nplt.subplot( 4, 2, 1 )\nsns.lineplot( x='datetime', y='mw_energy_consumption', data=df9_1, label='ENERGY CONSUMPTION' )\nsns.lineplot( x='datetime', y='predictions', data=df9_1, label='PREDICTIONS' )\n\nplt.subplot( 4, 2, 2 )\nsns.lineplot( x='datetime', y='mw_energy_consumption', data=df9_2, label='ENERGY CONSUMPTION' )\nsns.lineplot( x='datetime', y='predictions', data=df9_2, label='PREDICTIONS' )\n\nplt.subplot( 4, 2, 3 )\nsns.lineplot( x='datetime', y='mw_energy_consumption', data=df9_3, label='ENERGY CONSUMPTION' )\nsns.lineplot( x='datetime', y='predictions', data=df9_3, label='PREDICTIONS' )\n\n# Insert a line passing the value 1 (one) to see the predictions in relation to a perfect prediction that would be the value 1 (one) itself.\nplt.subplot( 4, 2, 4 )\nsns.lineplot( x='datetime', y='error_rate', data=df9_1 )\nplt.axhline( 1, linestyle='--' )\n\nplt.subplot( 4, 2, 5 )\nsns.lineplot( x='datetime', y='error_rate', data=df9_2 )\nplt.axhline( 1, linestyle='--' )\n\nplt.subplot( 4, 2, 6 )\nsns.lineplot( x='datetime', y='error_rate', data=df9_3 )\nplt.axhline( 1, linestyle='--' )\n\n# To see the distribution of errors.\nplt.subplot( 4, 2, 7 )\nsns.distplot( df9['error'] )\n\nplt.subplot( 4, 2, 8 )\nsns.scatterplot( df9['predictions'], df9['error'] )","a5380643":"## 2.3 Feature Engineering","0066e064":"### 7.3.1 Linear Regression - Lasso - Cross Validation","5e1dbe3f":"### H2. Seasons with higher or lower temperatures use more energy\n\n**TRUE** When we see the sum of all energy consumptions, we see that winter and summer, which are the seasons with the most acute temperatures, register the highest energy consumption. At the same time when we spread out over the years we observe the same behavior with rare exceptions.","373905da":"### 4.3.2 Categorical Attributes","f1cbcfc1":"**1.** Routines that start when there is less sunlight use more energy\n\n**2.** Tribes spend less energy\n\n**3.** Nations spend more energy\n\n**4.** Communities spend less energy\n\n**5.** Holiday periods spend more energy","0fc718d2":"### 4.1.3 Categorical Variable","0792067e":"### 6.2.1. Best Feature from Boruta","ddcaf51b":"## 2.1 Creation of Hypotheses","bcebf20e":"**1.** Seasons with higher temperatures use more energy\n\n**2.** Locations present in geographic accidents spend more energy\n\n**3.** Climate with higher temperatures use more energy","27e4c371":"## 6.1 Split dataframe into training and test dataset","f7f7c6f3":"## 5.1 Normalization","97255270":"## 9.3. Machine Learning Performance","1a54264d":"### 4.3.1 Numerical Attributes","9309eb45":"## 5.3 Transformation","67a431e3":"## 9.1 Business Performance","c9f0d887":"## 2.2 Final List of Hypotheses","6ac249e3":"## 1.6 Change Types","bd817eca":"# 4.0 STEP 04 - EXPLORATORY DATA ANALYSIS","f45de29b":"## 7.1 Average Model","2a65efc2":"# 1.0 STEP 01 - DESCRIPTION OF DATA","a321fe93":"## 1.5 Fillout NA","a9f2b448":"### 2.1.1 Demographic Hypotheses","ad3d4d68":"# 5.0 STEP 05 - DATA PREPARATION","99ed0d86":"## 8.1 Random Search","72311433":"## 0.2 Loading Data","be07bf66":"## 6.2 Boruta as Feature Selector","92f325fb":"**1.** Family activity influences energy expenditure in the early morning and late afternoon only\n\n**2.** Seasons with higher or lower temperatures use more energy\n\n**3.** Holiday periods spend more energy\n\n**4.** Weekends periods spend more energy","8acf46d0":"# 6.0 STEP 06 - FEATURE SELECTION","6c4d9d4a":"### 4.1.2 Numerical Variable","45aa3adf":"### 5.3.1 Encoding","3d508b97":"## 9.2 Total Performance","83348da0":"# 3.0 STEP 03 - VARIABLE FILTERING","532f0a89":"## 1.7 Descriptive Statistical","aa51f26d":"## 3.1 Line Filtering","05a61222":"### 7.6.2 Real Performance - Cross Validation","757fd3c3":"## 3.2 Column Selection","e8c5ff70":"## 1.1 Rename Columns","dc15d892":"### 7.5.1 XGBoost Regressor - Cross Validation","e9366fd8":"## 1.3 Data Types","1a237e58":"## 4.2 Bivariate Analysis","5c1502dd":"### 4.2.1 Summary of Hypotheses","49680a20":"## 1.4 Check NA","4b6ce772":"**1.** Older people spend less energy\n\n**2.** Larger families spend more energy\n\n**3.** Female people spend more energy\n\n**4.** Family activity influences energy expenditure in the early morning and late afternoon.","1737b73c":"## 0.1 Helper Function","e1d18860":"# 8.0. STEP 08 - HYPERPARAMETER FINE TUNING","1683e9b1":"## 7.6 Compare Model's Performance","239791af":"# 9.0. STEP 09 - TRANSLATION AND INTERPRETATION OF THE ERROR","1407c65b":"### 1.7.1 Numerical Attibutes","fc661e02":"### 2.1.2 Geographic Hypotheses","d236e12b":"### 1.7.2 Categorical Attibutes","1ce70b37":"### 4.1.1 Response Variable","10988b1e":"### H4. Weekends periods spend more energy\n\n**FALSE** Weekends periods spend less energy.","bf85e4e9":"### H1. Family activity influences energy expenditure in the early morning and late afternoon only\n\n**FALSE** Energy consumption increases early in the morning, but it keeps increasing until it has another strong rise in the late afternoon and then falls.","75c15504":"## 8.2 Final Model","4cfd8284":"## 4.1 Univariate Analysis","94d731e0":"# 7.0. STEP 07 - MACHINE LEARNING MODELLING","b65a0f65":"## 7.5 XGBoost Regressor","a44a92ce":"# 10.0 STEP 10 - DEPLOY MODEL TO PRODUCTION","7f176adf":"## 1.2 Data Dimensions","519248ac":"## 5.2 Rescaling","2caa111b":"# 2.0 STEP 02 - FEATURE ENGINEERING","ffd8f0bb":"## 7.3. Linear Regression Regularized Model","2b1fa293":"### 7.4.1 Random Forest Regressor - Cross Validation","3d7cdf08":"### 7.2.1 Linear Regression Model - Cross Validation","f1e0e497":"### 5.3.2 Response Variable Transformation","b396c87f":"## 4.3 Multivariate Analysis","cbf86630":"**In this project I won't put the model in production. If you want to see in more details how I do it, check my other project where I did the model deploy.** <br>\n**Rossmann Project:** https:\/\/github.com\/panambY\/Rossmann_Store_Sale\n\nIn this part, some Python code is built to assemble the entire structure that will trigger the classes and functions to make the model trigger when the store number is entered. In this first moment, the model will be stored in the cloud inside the Heroku platform and accessed through the Jupyter Notebook as a test to verify its correct functioning.\n\nThe goal here is to make the prediction model accessible to anyone. To achieve this, an API is created.\n\nThe architecture of the model in production:\n\n- **Handler API** -> is the part that receives the requests and plays for the other parts so that the data is processed and then brings everything together, returning the final answer.\n- **Data Preparation** -> all the treatments and modifications we made to the data will be kept inside. When the Handler receives the raw data it will throw it here within this list of treatment codes so that they are prepared so that they can be ready to be used within the Machine Learning model.\n- **Model Training** -> this is our trained model that has been saved and will be placed inside this folder in our production architecture. The Handler will take the data processed within Data Preparation and play it inside the model so that it provides the prediction.\n\nAt the end of the construction of all this architecture and being put into production, the way it will be visualized can be through an App, Dashboard or a website.","fadc8564":"### 5.3.1 Nature Transformation","d3dbcfc1":"### 2.1.3 Sociocultural Hypotheses","14a18dfc":"### H3. Holiday periods spend more energy\n\n**FALSE** Holiday periods spend less energy","81cd9197":"### 7.6.1 Single Performance","8659fa75":"## 7.2 Linear Regression Model","354d86db":"# 0.0 IMPORTS","49d03d76":"## 7.4 Random Forest Regressor"}}