{"cell_type":{"d13ca61d":"code","c4e5167b":"code","84912db2":"code","a078dd9f":"code","a7f69405":"code","34e2a924":"code","d3457254":"code","10d87ff2":"code","e4c2db85":"code","467754e6":"code","9648bb11":"code","253b1074":"code","ec67d5c4":"code","bdbc29a4":"code","2ae4135c":"code","e8cda8f8":"code","29317c25":"code","b38e612b":"code","5f1d369c":"code","28b15c49":"code","cae8c7d9":"code","15eccbcb":"code","79924617":"code","9b95bb08":"code","c924a83b":"code","6bba3774":"code","507e0d87":"code","d5fcba69":"code","d71d31b8":"code","6bde5743":"code","ea1c3fc2":"code","3ce03957":"code","e5fe0246":"code","bfc6ccd8":"code","a7a6e32c":"code","ba21803e":"code","48f12c3e":"code","f9257c97":"code","3dd91325":"code","7b773a83":"code","8a2ac7ff":"code","ecf7f43a":"code","b587958e":"code","32d9b6e8":"code","589254ff":"code","4c6af179":"code","1c813f33":"code","20fb8bb1":"code","7f6ca106":"code","19959662":"code","bbca32f4":"code","f963b45e":"code","6d88ca66":"code","ded8bb15":"code","1a6bbb07":"code","a7d60067":"code","14bacbf2":"code","d83bbf74":"code","51c65f74":"code","d80d6c78":"code","7ab1c69a":"code","9021ae71":"markdown","cac7d976":"markdown","80ceebb9":"markdown","49c422bb":"markdown","85db3f91":"markdown","b9dbdbd7":"markdown","318c79bf":"markdown","e39a859f":"markdown","eb659c9a":"markdown","59ea19fb":"markdown"},"source":{"d13ca61d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nemployee_df = pd.read_csv('\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nemployee_df.head()","c4e5167b":"employee_df['PercentSalaryHike'].describe()","84912db2":"employee_df['PercentSalaryHike'].unique()","a078dd9f":"#import for interactive plotting\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots","a7f69405":"employee_df.info()","34e2a924":"sns.set()\n# Let's see if we have any missing data, luckily we don't!\nsns.heatmap(employee_df.isnull(), yticklabels=False, cbar=False, cmap='Blues')","d3457254":"for column in employee_df.select_dtypes(include=['object']):\n    print(column)\n    print(employee_df[column].unique())\n    print('-----------------------------------')","10d87ff2":"employee_df['Attrition'] = employee_df['Attrition'].apply(lambda x:1 if x == \"Yes\" else 0 )\nemployee_df['OverTime'] = employee_df['OverTime'].apply(lambda x:1 if x ==\"Yes\" else 0 )\n#drop the column, use inplce True for delete the information from the memeory\nemployee_df.drop(['EmployeeCount','EmployeeNumber','Over18','StandardHours'], axis=1, inplace=True)","e4c2db85":"employee_df.head()","467754e6":"# Visulazing the distibution of the data for every feature\nemployee_df.hist(bins=30, figsize=(20,20), color='b', alpha=0.6)","9648bb11":"attrition = employee_df[employee_df['Attrition'] == 1]\nno_attrition = employee_df[employee_df['Attrition']==0]","253b1074":"fig = make_subplots(rows=1, cols=2,\n                    specs=[[{\"type\":\"xy\"},{\"type\":\"domain\"}]],\n                    subplot_titles= (\"Count of Attrition\", \"Distribution of Attrition\"))\n\nfig.add_trace(go.Bar(x = employee_df['Attrition'].value_counts(),\n                     y = ['Employee who stayed', 'Employee who left'],\n                     orientation = 'h',\n                     opacity=0.8),\n                     row=1,col=1)\n\nfig.add_trace(go.Pie(values=employee_df['Attrition'].value_counts(),\n                    opacity=0.8),\n                    row=1, col=2)\nfig.update_layout(height=400, showlegend=False)\n\nfig.show()","ec67d5c4":"def categorical_colum_investigaton(col_name):\n    \"\"\"First Plot: Pie chart for categorical column to see percentage of each value\n       Secons Plot: Count plot for categorical column to see the number of count for each of the type\n       Third Plot is Number of Count for separeted for Attribition\"\"\"\n\n    f,ax = plt.subplots(1,3, figsize=(18,6))\n    employee_df[col_name].value_counts().plot.pie(autopct='%1.1f%%',ax=ax[0],shadow=True, cmap='Set3')\n    employee_df[col_name].value_counts().plot.bar(cmap='Set3',ax=ax[1])\n    ax[1].set_title(f'Number of Employee by {col_name}')\n    ax[1].set_ylabel('Count')\n    ax[1].set_xlabel(f'{col_name}')\n    sns.countplot(col_name, hue='Attrition',data=employee_df, ax=ax[2], palette='Set3')\n    ax[2].set_title(f'Attrition by {col_name}')\n    ax[2].set_xlabel(f'{col_name}')\n    ax[2].set_ylabel('Count')\n","bdbc29a4":"categorical_colum_investigaton('Gender')","2ae4135c":"categorical_colum_investigaton('BusinessTravel')","e8cda8f8":"categorical_colum_investigaton('EducationField')","29317c25":"categorical_colum_investigaton('MaritalStatus')\ncategorical_colum_investigaton('Department')\ncategorical_colum_investigaton('JobRole')","b38e612b":"correlations = employee_df.corr()\nfig, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(correlations, annot=True)\n# Job level is strongly correlated with total working hours\n# Monthly income is strongly correlated with Job level\n# Monthly income is strongly correlated with total working hours\n# Age is stongly correlated with monthly income\n","5f1d369c":"def categorical_numerical_comperation(numerical_col, caterical_col1, caterical_col2):\n    \n\n    f,ax = plt.subplots(1,2, figsize=(18,6))\n    \n    g1= sns.swarmplot( caterical_col1, numerical_col,hue='Attrition', data=employee_df, dodge=True, ax=ax[0], palette='Set2')\n    ax[0].set_title(f'{numerical_col} vs {caterical_col1} separeted by Attrition')\n    g1.set_xticklabels(g1.get_xticklabels(), rotation=45) \n\n    \n    g2 = sns.swarmplot( caterical_col2, numerical_col,hue='Attrition', data=employee_df, dodge=True, ax=ax[1], palette='Set2')\n    ax[1].set_title(f'{numerical_col} vs {caterical_col1} separeted by Attrition')\n    g2.set_xticklabels(g2.get_xticklabels(), rotation=45) ","28b15c49":"categorical_numerical_comperation('Age','Gender','MaritalStatus')","cae8c7d9":"categorical_numerical_comperation('Age','JobRole','EducationField')","15eccbcb":"categorical_numerical_comperation('MonthlyIncome','Gender','MaritalStatus')","79924617":"categorical_colum_investigaton('Total_Satisfaction')","9b95bb08":"categorical_colum_investigaton('EnvironmentSatisfaction')","c924a83b":"categorical_colum_investigaton('JobInvolvement')","6bba3774":"categorical_colum_investigaton('NumCompaniesWorked')","507e0d87":"categorical_colum_investigaton('YearsWithCurrManager')","d5fcba69":"categorical_colum_investigaton('OverTime')","d71d31b8":"categorical_colum_investigaton('StockOptionLevel')","6bde5743":"categorical_colum_investigaton('Education')","ea1c3fc2":"categorical_colum_investigaton('TrainingTimesLastYear')","3ce03957":"def numerical_colum_investigaton(col_name):\n    f,ax = plt.subplots(1,2, figsize=(18,6))\n    sns.kdeplot(attrition[col_name], label='Employee who left',ax=ax[0], shade=True, color='palegreen')\n    sns.kdeplot(no_attrition[col_name], label='Employee who stayes', ax=ax[0], shade=True, color='salmon')\n    \n    sns.boxplot(y=col_name, x='Attrition',data=employee_df, palette='Set2', ax=ax[1])\n    ","e5fe0246":"numerical_colum_investigaton(\"Age\")","bfc6ccd8":"numerical_colum_investigaton('DistanceFromHome')","a7a6e32c":"numerical_colum_investigaton('MonthlyIncome')","ba21803e":"numerical_colum_investigaton('HourlyRate')","48f12c3e":"numerical_colum_investigaton('JobInvolvement')","f9257c97":"numerical_colum_investigaton('PercentSalaryHike')","3dd91325":"numerical_colum_investigaton('Age')","7b773a83":"numerical_colum_investigaton('DailyRate')","8a2ac7ff":"numerical_colum_investigaton('TotalWorkingYears')","ecf7f43a":"numerical_colum_investigaton('YearsAtCompany')","b587958e":"employee_df['Total_Satisfaction'] = (employee_df['EnvironmentSatisfaction'] + \n                                     employee_df['JobInvolvement'] + \n                                     employee_df['JobSatisfaction'] + \n                                     employee_df['RelationshipSatisfaction'] +\n                                     employee_df['WorkLifeBalance']) \/5 \n\n# drop used column and \nemployee_df.drop(['EnvironmentSatisfaction','JobInvolvement','JobSatisfaction','RelationshipSatisfaction','WorkLifeBalance'], axis=1, inplace=True)","32d9b6e8":"employee_df['Total_Satisfaction_bool'] = employee_df['Total_Satisfaction'].apply(lambda x:1 if x>=2.2 else 0 ) \nemployee_df.drop('Total_Satisfaction', axis=1, inplace=True)\n\n#Age Column:Employee more likey the drop the job younger than 35\nemployee_df['Age_bool'] = employee_df['Age'].apply(lambda x:1 if x<35 else 0)\nemployee_df.drop('Age', axis=1, inplace=True)\n\n#Daily Rate:Employee more likey the drop the job if dailtRate less than 750\nemployee_df['DailyRate_bool'] = employee_df['DailyRate'].apply(lambda x:1 if x<750 else 0)\nemployee_df.drop('DailyRate', axis=1, inplace=True)\n\n#Departman: Employee more likey the drop the job if the employee working at Sales Departmaen\nemployee_df['Department_bool'] = employee_df['Department'].apply(lambda x:1 if x=='Sales' else 0)\nemployee_df.drop('Department', axis=1, inplace=True)\n\n# DistanceFromHome: Employee more likey the drop the job if the employeeworking far more than 10\nemployee_df['DistanceFromHome_bool'] = employee_df['DistanceFromHome'].apply(lambda x:1 if x>10 else 0)\nemployee_df.drop('DistanceFromHome', axis=1, inplace=True)\n\n# HourlyRate: Employee  more likey the drop the job if the employee working hourly rate less than 65\nemployee_df['HourlyRate_bool'] = employee_df['HourlyRate'].apply(lambda x:1 if x<65 else 0)\nemployee_df.drop('HourlyRate', axis=1, inplace=True)\n\n#JobRole: Employee more likey the drop the job if the employee working as Sales Executive \nemployee_df['JobRole_bool'] = employee_df['JobRole'].apply(lambda x:1 if x=='Sales Executive' else 0)\nemployee_df.drop('JobRole', axis=1, inplace=True)\n\n#MontlyIncome:Employee more likey the drop the job if the employee working as Sales Executive \nemployee_df['MonthlyIncome_bool'] = employee_df['MonthlyIncome'].apply(lambda x:1 if x<3500 else 0)\nemployee_df.drop('MonthlyIncome', axis=1, inplace=True)\n\n#NumCompaniesWorked: Employee more likey the drop the job if the employee working as Sales Executive \nemployee_df['NumCompaniesWorked_bool'] = employee_df['NumCompaniesWorked'].apply(lambda x:1 if x>4 else 0)\nemployee_df.drop('NumCompaniesWorked', axis=1, inplace=True)\n\n#TotalWorkingYears: Employee  more likey the drop the job if the employee working as Sales Executive \nemployee_df['TotalWorkingYears_bool'] = employee_df['TotalWorkingYears'].apply(lambda x:1 if x<8 else 0)\nemployee_df.drop('TotalWorkingYears', axis=1, inplace=True)\n\n#YearsAtCompany: Employee more likey the drop the job if the employee working as Sales Executive \nemployee_df['YearsAtCompany_bool'] = employee_df['YearsAtCompany'].apply(lambda x:1 if x<3 else 0)\nemployee_df.drop('YearsAtCompany', axis=1, inplace=True)\n\n#JobRole Column  more likey the drop the job if the employee working as Sales Executive \nemployee_df['YearsInCurrentRole_bool'] = employee_df['YearsInCurrentRole'].apply(lambda x:1 if x<3 else 0)\nemployee_df.drop('YearsInCurrentRole', axis=1, inplace=True)\n\n#JobRole Column  more likey the drop the job if the employee working as Sales Executive \nemployee_df['YearsSinceLastPromotion_bool'] = employee_df['YearsSinceLastPromotion'].apply(lambda x:1 if x<1 else 0)\nemployee_df.drop('YearsSinceLastPromotion', axis=1, inplace=True)\n\n#JobRole Column  more likey the drop the job if the employee working as Sales Executive \nemployee_df['YearsWithCurrManager_bool'] = employee_df['YearsWithCurrManager'].apply(lambda x:1 if x<1 else 0)\nemployee_df.drop('YearsWithCurrManager', axis=1, inplace=True)\n\nemployee_df.drop(['MonthlyRate','PercentSalaryHike'], axis=1, inplace=True)\n\n\n","589254ff":"convert_category = ['BusinessTravel','Education','EducationField','MaritalStatus','StockOptionLevel','OverTime','Gender','TrainingTimesLastYear']\nfor col in convert_category:\n        employee_df[col] = employee_df[col].astype('category')\n","4c6af179":"employee_df.info()\n","1c813f33":"#separate the categorical and numerical column\nX_categorical = employee_df.select_dtypes(include=['category'])\nX_numerical = employee_df.select_dtypes(include=['int64'])\n\n#create teh target column\ny = employee_df['Attrition']\nX_numerical.drop('Attrition', axis=1, inplace=True)\n","20fb8bb1":"#handle the categorical variable\nfrom sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder()\n\nX_categorical = onehotencoder.fit_transform(X_categorical).toarray()\nX_categorical = pd.DataFrame(X_categorical)\nX_categorical","7f6ca106":"#concat the categorical and numerical values\n\nX_all = pd.concat([X_categorical, X_numerical], axis=1)\nX_all.head()","19959662":"#Scaler the data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X_all)\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.25)\n\nprint(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")","bbca32f4":"#import libraires\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nmodel = LogisticRegression()\n\nmodel.fit(X_train, y_train)\nmodel\ny_pred = model.predict(X_test)\nprint(f\"Accuracy of Logistic Regression: %{100* accuracy_score(y_pred, y_test)}\")","f963b45e":"cm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True)","6d88ca66":"print(classification_report(y_test, y_pred))","ded8bb15":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n#predict probabilities\ny_probability = model.predict_proba(X_test)\n#keep probabilities positive outcomes only\ny_probability = y_probability[:,1]\n## calculate scores\nlogistic_roc_score = roc_auc_score(y_test, y_probability)\nprint(\"Logistic ROC AUC:%.3f\" %(logistic_roc_score) )\n\n# generate a no skill prediction (majority class)\nnoskill_prob = [0 for _ in range(len(y_test))]\nnoskill_roc_auc = roc_auc_score(y_test,noskill_prob )\nprint(\"No Skill ROC AUC:%.3f\" %(noskill_roc_auc))\n\n#calculate the roc curve\nlo_fpr, lo_tpr, _ =roc_curve(y_test, y_probability)\nno_fpr, no_tpr, _ =roc_curve(y_test, noskill_prob)\n\nplt.plot(lo_fpr, lo_tpr, linestyle='dashed', label ='Logistic')\nplt.plot(no_fpr, no_tpr, linestyle='dotted', label= 'No Skill')\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","1a6bbb07":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_Ran = RandomForestClassifier()\n\nmodel_Ran.fit(X_train, y_train)\n\ny_pred_Ran = model_Ran.predict(X_test)\n\nprint(f\"Accuracy of Randon Forest Model: {accuracy_score(y_test, y_pred_Ran)}\")\n","a7d60067":"cm_Ran = confusion_matrix(y_test, y_pred_Ran)\nsns.heatmap(cm_Ran, annot=True)\n","14bacbf2":"print(classification_report(y_test, y_pred_Ran))","d83bbf74":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\n# generate a no skill prediction (majority class)\nnoskill_prob = [0 for _ in range(len(y_test))]\nnoskill_roc_auc = roc_auc_score(y_test,noskill_prob )\nprint(\"No Skill ROC AUC:%.3f\" %(noskill_roc_auc))\n\n#predict probabilities\ny_probability = model.predict_proba(X_test)\n#keep probabilities positive outcomes only\ny_probability = y_probability[:,1]\n## calculate scores\nlogistic_roc_score = roc_auc_score(y_test, y_probability)\nprint(\"Logistic ROC AUC:%.3f\" %(logistic_roc_score) )\n\n\n\n#Probabilities for random forest\ny_probability_Ran = model_Ran.predict_proba(X_test)\n#keep probabilities positive outcomes only\ny_probability_Ran = y_probability_Ran[:,1]\n## calculate scores\nRandomF_roc_score = roc_auc_score(y_test, y_probability_Ran)\nprint(\"Random Forest ROC AUC:%.3f\" %(RandomF_roc_score) )\n\n\n#calculate the roc curve\nlo_fpr, lo_tpr, _ =roc_curve(y_test, y_probability)\nno_fpr, no_tpr, _ =roc_curve(y_test, noskill_prob)\nra_fpr, ra_tpr, _ =roc_curve(y_test, y_probability_Ran)\n\n\nplt.plot(lo_fpr, lo_tpr, linestyle='dashed', color= 'r', label ='Logistic Regression Model')\nplt.plot(ra_fpr, ra_tpr, linestyle='dashed', color= 'b', label= 'Random Forest Model')\nplt.plot(no_fpr, no_tpr, linestyle='dotted', label= 'No Skill')\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","51c65f74":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n\n\n\nkfold = KFold(n_splits=10, random_state=22,shuffle=True)\nxyz= []\naccuracy = []\nstd = []\ny_pred_list = []\n\n\nclassifiers = ['Linear Svm',\n              'Radical Svm',\n              'Logistic Regression',\n              'KNN',\n              'Decision Tree',\n              'Naive Bayes',\n              'Random Forest',\n              'XGBoost']\n\nmodels = [svm.SVC(kernel='linear'),\n         svm.SVC(kernel='rbf'),\n         LogisticRegression(solver='liblinear'),\n         KNeighborsClassifier(),\n          DecisionTreeClassifier(),\n          GaussianNB(),\n          RandomForestClassifier(n_estimators=100),\n          XGBClassifier()\n        ]\n\nfor i in models:\n    model = i \n    cv_result =cross_val_score(model, \n                               X_train, \n                               y_train, \n                               cv=kfold,\n                              scoring='accuracy')\n    y_pred = cross_val_predict(model, X, y, cv=10)\n    y_pred_list.append(y_pred)\n    cv_result =cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\n    \n    \n    \n    \nnew_model_data_frame =pd.DataFrame({'Cv Mean': xyz,\n                                   'Std': std},\n                                   index=classifiers)\n\nnew_model_data_frame","d80d6c78":"plt.subplots(figsize=(12,16))\nplt.xticks(rotation=45)\nsns.boxplot(new_model_data_frame.index, accuracy)","7ab1c69a":"from sklearn.model_selection import GridSearchCV\nC=[0.05, 0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\ngamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nkernel=['rbf','linear']\nhyper = {'kernel':kernel, 'C':C, 'gamma':gamma}\ngd =GridSearchCV(estimator=svm.SVC(), param_grid=hyper, verbose=True)\n\n\ngd.fit(X_train,y_train)\n\n\nprint(gd.best_score_)\nprint(gd.best_estimator_)","9021ae71":"## A LOGISTIC REGRESSION CLASSIFIER","cac7d976":"* Age: Create a new column using qcut for categorixing the Age\n* Daily Rate: Morelikely the employee left compant if the dailt rate less than 700. So Create new column if daily  arte less than 700 is 1 else 0\n* DistanceFromHome:If the DistanceFromHome more than 10 then employee more likely quit the job.\n* Create new column called Total_Satisfaction using  EnvironmentSatisfaction, JobInvolvement,JobSatisfaction, RelationshipSatisfaction,WorkLifeBalance\n* \n                                     \n","80ceebb9":"## TRYING MULTIPLE MODEL WITH CROSS VALIDATION","49c422bb":"## Feature Enginiring\n\n","85db3f91":"## Load the libraries and the data","b9dbdbd7":"## CREATE TESTING AND TRAINING DATASET & PERFORM DATA CLEANING","318c79bf":"### Calculation Roc Curve and driving","e39a859f":"# Human Resorce Data to Predict Employee Attrition\n\nIn this project we are going to develop a model that could predict which employees are more likely to quit. \nThis fictional data created by IBM data scientist. We are going to explore the data and then create a model to predict how likely the employee quit the job.","eb659c9a":"* 1470 employee worked at the company over time\n* We have categorical and numerical data: we need to investigate the this separately\n* Attrition columns indicate \"Yes\" if the employee quit the job and \"No\" employee stayed at the company. We should convert this 1 and 0 for using in Machine learning model. \n* Overtime also has Yes and NO. So we need to convert this column to 1 and 0.\n\n* Over18 column has only Y[Yes] meaning every employee over18 so we don't need this information.\n* Standard Hour has only unique value 80. So we don't need this column.\n* Same including the 'EmployeeCount','EmployeeNumber' column.\n","59ea19fb":"## TRAIN AND EVALUATE A RANDOM FOREST CLASSIFIER"}}