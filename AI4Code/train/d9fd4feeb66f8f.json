{"cell_type":{"2f1ebad5":"code","64ce1d5f":"code","c360842f":"code","452b7280":"code","a9299ae5":"code","3b230c64":"code","95d350bf":"code","9cca0d33":"code","fd030feb":"code","c7b6dd12":"code","406146b3":"code","4c5081ff":"markdown","a0683af1":"markdown","53b1ca12":"markdown","61b42b19":"markdown","85ae0d58":"markdown","ffc8d4a3":"markdown","5120ae6c":"markdown","384df470":"markdown","5a920bd0":"markdown","172e16fd":"markdown"},"source":{"2f1ebad5":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","64ce1d5f":"import numpy as np, random, scipy.stats as ss\n\ndef majority_vote_fast(votes):\n    mode, count = ss.mstats.mode(votes)\n    return mode\n\ndef distance(p1, p2):\n    return np.sqrt(np.sum(np.power(p2 - p1, 2)))\n\ndef find_nearest_neighbors(p, points, k=5):\n    distances = np.zeros(points.shape[0])\n    for i in range(len(distances)):\n        distances[i] = distance(p, points[i])\n    ind = np.argsort(distances)\n    return ind[:k]\n\ndef knn_predict(p, points, outcomes, k=5):\n    ind = find_nearest_neighbors(p, points, k)\n    return majority_vote_fast(outcomes[ind])[0]","c360842f":"import pandas as pd\n# importing dataset\ndata = pd.read_csv(\"\/kaggle\/input\/wine-quality-red-and-white\/wine.csv\")\ndata.head()","452b7280":"data[\"is_red\"] = (data[\"color\"] == \"red\").astype(int)\nnumeric_data = data.drop(\"color\", axis=1)\n\nnumeric_data.groupby('is_red').count()","a9299ae5":"import sklearn.preprocessing\nscaled_data = sklearn.preprocessing.scale(numeric_data)\nnumeric_data = pd.DataFrame(scaled_data, columns = numeric_data.columns)\n\nimport sklearn.decomposition\npca = sklearn.decomposition.PCA(n_components=2)\nprincipal_components = pca.fit_transform(numeric_data)\n\nprincipal_components.shape","3b230c64":"import matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.backends.backend_pdf import PdfPages\nobservation_colormap = ListedColormap(['red', 'blue'])\nx = principal_components[:,0]\ny = principal_components[:,1]\nplt.title(\"Principal Components of Wine\")\nplt.scatter(x, y, alpha = 0.2,\n    c = data['high_quality'], cmap = observation_colormap, edgecolors = 'none')\nplt.xlim(-8, 8); plt.ylim(-8, 8)\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.show();","95d350bf":"import numpy as np \nnp.random.seed(1) # do not change\n\nx = np.random.randint(0, 2, 1000)\ny = np.random.randint(0 ,2, 1000)\n\ndef accuracy(predictions, outcomes):\n    return 100*np.mean(predictions == outcomes)\n\nprint(accuracy(x,y))","9cca0d33":"accuracy(0,data[\"high_quality\"])","fd030feb":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(numeric_data, data['high_quality'])\nlibrary_predictions = knn.predict(numeric_data)\nprint(accuracy(library_predictions, data[\"high_quality\"]))","c7b6dd12":"n_rows = data.shape[0]\nrandom.seed(123)\nselection=random.sample(range(n_rows), 10)\nselection","406146b3":"predictors = np.array(numeric_data)\ntraining_indices = [i for i in range(len(predictors)) if i not in selection]\noutcomes = np.array(data[\"high_quality\"])","4c5081ff":"Unlike the `scikit-learn` function, our homemade kNN classifier does not take any shortcuts in calculating which neighbors are closest to each observation, so it is likely too slow to carry out on the whole dataset. Now, we will select a subset of our data to use in our homemade kNN classifier.","a0683af1":"We will analyze a dataset consisting of an assortment of wines classified as \"high quality\" and \"low quality\" and will use k-Nearest Neighbors classification to determine whether or not other information about the wine helps us correctly guess whether a new wine will be of high quality.","53b1ca12":"Next, we will inspect the dataset and perform some mild data cleaning.","61b42b19":"We want to ensure that each variable contributes equally to the kNN classifier, so we will need to scale the data by subtracting the mean of each variable (column) and dividing each variable (column) by its standard deviation. Then, we will use principal components to take a linear snapshot of the data from several different angles, with each snapshot ordered by how well it aligns with variation in the data. In this exercise, we will scale the numeric data and extract the first two principal components.","85ae0d58":"The dataset remains stored as data. Because most wines in the dataset are classified as low quality, one very simple classification rule is to predict that all wines are of low quality. In this exercise, we determine the accuracy of this simple rule.","ffc8d4a3":"Now, we will use the kNN classifier from `scikit-learn` to predict the quality of wines in our dataset.","5120ae6c":"We will create a function that calculates the accuracy between predictions and outcomes.","384df470":"We are now ready to use our homemade kNN classifier and compare the accuracy of our results to the baseline.","5a920bd0":"We will plot the first two principal components of the covariates in the dataset. The high and low quality wines will be colored using red and blue, respectively.","172e16fd":"Our first step is to import the dataset."}}