{"cell_type":{"312601ff":"code","983660f2":"code","49c872a9":"code","d8a09fa4":"code","4ab620e6":"code","7300ed23":"code","0549cfce":"code","62524fd6":"code","99da4977":"code","bd14ac95":"code","fab90728":"code","7689a479":"code","b0013dc1":"code","af1fb59d":"code","94721e1b":"code","1a9ad445":"code","b11ce33c":"code","5a8155fa":"code","81ac25cf":"code","81f13e83":"code","76c34f0f":"code","de16c891":"code","9344621d":"markdown","78868518":"markdown","df9b2879":"markdown","b99e3f05":"markdown","1ed5685a":"markdown","83ff6014":"markdown"},"source":{"312601ff":"import numpy as np\nimport pandas as pd\ntrain_data=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest_data=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nss=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","983660f2":"from sklearn.model_selection import KFold\ntrain_data['fold']=-1\nkf=KFold(n_splits=5,shuffle=True,random_state=42)\nfor fold,(ti,vi) in enumerate(kf.split(train_data)):\n    train_data.loc[vi,'fold']=fold","49c872a9":"train_data.fold.value_counts()","d8a09fa4":"train_data.describe()","4ab620e6":"train_data.isnull().any()","7300ed23":"pd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","0549cfce":"sns.histplot(data=train_data,x='id',y='claim')","62524fd6":"sns.countplot(data=train_data,x='claim')","99da4977":"# normalize data\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\ntraindf=train_data.copy()\ntestdf=test_data.copy()\nfrom sklearn.preprocessing import StandardScaler\nuseful_cols=[col for col in testdf.columns if (col!='id' and col !='fold')] \n\nfor col in useful_cols:\n    mean_=traindf[col].mean()\n    std_=traindf[col].std()\n    traindf[col]=(traindf[col]-mean_)\/std_\n    testdf[col]=(testdf[col]-mean_)\/std_ \n#     min_=traindf[col].min()\n#     max_ =traindf[col].max()\n#     traindf[col]=(traindf[col]-min_)\/(max_-min_)\n#     testdf[col]=(testdf[col]-min_)\/(max_-min_)\n    \n#     traindf[col]=traindf[col].fillna(traindf[col].mean())\n#     testdf[col]=testdf[col].fillna(traindf[col].mean())\n","bd14ac95":"testdf.isnull().sum().sum()","fab90728":"from sklearn import metrics\n\n# lets first define a function that'll help us know how good\/bad our model is doing\ndef get_scores(y_preds,y):\n    return {\n        'Accuracy':metrics.accuracy_score(y_preds,y),\n        'Precision':metrics.precision_score(y_preds,y),\n        'Recall':metrics.recall_score(y_preds,y),\n        'F1':metrics.f1_score(y_preds,y),\n        'ROC_AUC': metrics.roc_auc_score(y_preds,y)\n    }","7689a479":"from xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import train_test_split","b0013dc1":"X=traindf[useful_cols]\ny=traindf['claim']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)","af1fb59d":"def train_model(model):\n    model_=model\n    model_.fit(X_train,y_train)\n    y_preds=model_.predict(X_val)\n    return model_,get_scores(y_preds,y_val)","94721e1b":"model_list=[\n            DecisionTreeClassifier(random_state=42), \n            RandomForestClassifier(random_state=42),\n            XGBClassifier(random_state=42,tree_method='gpu_hist'), \n            LGBMClassifier(random_state=42), \n            LogisticRegression(random_state=42),\n            svm.SVC(random_state=42),\n            CatBoostClassifier(random_state=42,verbose=100),\n            AdaBoostClassifier(random_state=42)\n           ]\nmodel_names=['Decision Tree', 'Random Forest', 'XG Boost', 'Light GBM', 'Logistic Regression','SVM','CatBoost','AdaBoost']\n","1a9ad445":"# # Now lets train all the models and see how are they doing\n# model_store=[]\n# scores = pd.DataFrame(columns=['Name','Accuracy','Precision',\n#                                 'Recall',\n#                                 'F1',\n#                                 'ROC_AUC'])\n# for i in range(len(model_list)):\n#     model,score=train_model(model_list[i])\n#     scores.loc[i]=[model_names[i]]+list(score.values())\n#     model_store.append(model)\n#     print(model_list[i], ' done')","b11ce33c":"# figure, axis = plt.subplots(2, 3)\n# figure.set_figheight(15)\n# figure.set_figwidth(20)\n\n# for i in range(2):\n#     for j in range(3):\n#         axis[i,j].set_xlim([.3,.9])\n        \n# axis[0, 0].barh(scores['Name'],scores['Accuracy'],height=.5)\n# axis[0, 0].set_title(\"Accuracy Score\")\n  \n# axis[0, 1].barh(scores['Name'],scores['Precision'],height=.5)\n# axis[0, 1].set_title(\"Precision\")\n\n# axis[1, 0].barh(scores['Name'],scores['Recall'],height=.5)\n# axis[1, 0].set_title(\"Recall\")\n\n# axis[1, 2].barh(scores['Name'],scores['F1'],height=.5)\n# axis[1, 2].set_title(\"F1\")\n\n# axis[0, 2].barh(scores['Name'],scores['ROC_AUC'],height=.5)\n# axis[0, 2].set_title('ROC_AUC')\n\n# axis[1, 1].set_visible(False)\n\n# plt.show()","5a8155fa":"import optuna\nimport sklearn\ndef objective(trial):\n    score=0\n    n_estimators = trial.suggest_int('n_estimators', 10, 1000)\n    max_depth = trial.suggest_int('max_depth', 1, 27)\n    reg_lambda = trial.suggest_loguniform('reg_lambda', 0.1, 5)\n    alpha = trial.suggest_loguniform('alpha', .1, 5)\n    min_child_weight= trial.suggest_loguniform('min_child_weight', 1, 50)\n    clf = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth,\n                        reg_lambda=reg_lambda,alpha=alpha,min_child_weight=min_child_weight, \n                        tree_method='gpu_hist',random_state=42)\n    clf.fit(X_train[useful_cols],y_train)\n    preds=clf.predict(X_val[useful_cols])\n    return metrics.roc_auc_score(preds,y_val)","81ac25cf":"# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=10)","81f13e83":"# best_params=study.best_trial.params\nbest_params={'n_estimators': 433, 'max_depth': 27, \n             'reg_lambda': 0.5955576227964456, 'alpha': 3.8018858996918654, \n             'min_child_weight': 5.2345922504984905}","76c34f0f":"## Finally predict for each KFold and take mean to get final submission\nimport sklearn\npredictions=[]\nfor i in range(5):\n    train=traindf.loc[traindf.fold!=i]\n    val=traindf.loc[traindf.fold==i]\n    Xtrain=train[useful_cols]\n    ytrain=train['claim']\n    Xval=val[useful_cols]\n    yval=val['claim']\n    clf = XGBClassifier(**best_params, tree_method='gpu_hist', random_state=i)\n    clf.fit(Xtrain,ytrain)\n    preds=clf.predict_proba(testdf[useful_cols])[:,1]\n    predictions.append(preds)\n    print('fold ' +str(i), get_scores(clf.predict(Xval),yval))","de16c891":"preds=np.mean(predictions,axis=0)\nss['claim']=preds\nss.to_csv('submission,csv',index=False)","9344621d":"# Hyperparameter Tuning using optuna ","78868518":"# EDA","df9b2879":"## Its good that the two classes are pretty balanced","b99e3f05":"## Here I created a list of models and trained all of them to see how are they performing","1ed5685a":"# Training","83ff6014":"## Hey Everyone, In this notebook, we'll see how we can use optuna for hyperparameter tuning and how to use KFolds to make a prediction !!\n## Let's go !!"}}