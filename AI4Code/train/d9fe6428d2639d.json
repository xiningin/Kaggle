{"cell_type":{"70703dc1":"code","9202e0fb":"code","67368d9e":"code","4dd4cbcd":"code","a596c4c8":"code","b0b82fdb":"code","e8b5ec9c":"code","085e5bf8":"code","46602814":"code","a5d3715d":"code","a491ecf2":"code","60187c45":"code","bd9e1ee9":"code","ffb6dddc":"code","8e4a65ca":"code","3a27b923":"code","dbce17ef":"code","8f3ee957":"code","8dd1395f":"code","fbd6cc08":"code","97dd57d6":"code","de642fcd":"code","803f8721":"code","72c9fc93":"code","924db3c4":"code","30b4e402":"code","6f4b51cc":"code","6952a483":"code","c4529e85":"code","e071bb91":"code","942887e2":"code","da7be47c":"code","8b3b14fc":"code","124d2a4c":"markdown","c5ccd549":"markdown","1d6fd29a":"markdown","034ecf63":"markdown"},"source":{"70703dc1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9202e0fb":"#reading files\ntrain=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","67368d9e":"train.head()","4dd4cbcd":"test.head()","a596c4c8":"#combining parch , sib SP to one family size column\ntrain['family_size'] = train['SibSp'] + train['Parch'] +1 #+1 for passenger itself\ntest['family_size'] = test['SibSp'] + test['Parch'] +1\n","b0b82fdb":"train.head()","e8b5ec9c":"sns.barplot(x='Sex',y='Survived',data=train)","085e5bf8":"sns.barplot(x='Embarked',y='Survived',data=train)","46602814":"sns.barplot(x='Pclass',y='Survived',data=train)","a5d3715d":"#null values\ntrain.isna().sum()","a491ecf2":"test.isna().sum()","60187c45":"sns.heatmap(train.isna())","bd9e1ee9":"#replacing Embarked column missing values with most frequent ones\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values= np.nan, strategy='most_frequent')\nimputer2 = SimpleImputer(missing_values= np.nan, strategy='median')\ntrain[\"Embarked\"]=imputer.fit_transform(train[[\"Embarked\"]]).ravel()\ntest[\"Embarked\"]=imputer.fit_transform(test[[\"Embarked\"]]).ravel()\ntest[\"Fare\"]=imputer2.fit_transform(test[[\"Fare\"]]).ravel()","ffb6dddc":"train.corr()","8e4a65ca":"plt.figure(figsize=(6,6))\nsns.boxplot(x='Pclass' , y='Age', data = train)","3a27b923":"def age1(cols):\n    Age=cols[0]\n    Pclass=cols[1]\n    if Pclass==1:\n        return 37\n    elif Pclass==2:\n        return 29\n    elif Pclass==3:\n        return 24\n    else:\n        return Age\ntrain[\"Age\"]=train[[\"Age\",\"Pclass\"]].apply(age1,axis=1)\ntest[\"Age\"]=test[[\"Age\",\"Pclass\"]].apply(age1,axis=1)","dbce17ef":"sns.heatmap(train.isna())","8f3ee957":"train.isna().sum()","8dd1395f":"test.isna().sum()","fbd6cc08":"#extracting name titles\ntrain['name_title'] = train['Name'].str.extract(' ([A-Za-z]+)\\.',expand=False)\ntest['name_title'] = test['Name'].str.extract(' ([A-Za-z]+)\\.',expand=False)","97dd57d6":"train[\"name_title\"].value_counts()","de642fcd":"test[\"name_title\"].value_counts()","803f8721":"train['name_title'] = train['name_title'].replace('Mlle' , 'Miss')\ntrain['name_title'] = train['name_title'].replace('Mlle' , 'Miss')\ntrain['name_title'] = train['name_title'].replace('Ms' , 'Mrs')\ntrain['name_title'] = train['name_title'].replace(['Dr','Rev','Major','Col','Capt','Jonkheer','Mme','Don'] , 'others')\ntrain['name_title'] = train['name_title'].replace(['Master','Lady','Sir','Countess'] , 'royals')\n\ntest['name_title'] = test['name_title'].replace('Mlle' , 'Miss')\ntest['name_title'] = test['name_title'].replace('Mlle' , 'Miss')\ntest['name_title'] = test['name_title'].replace('Ms' , 'Mrs')\ntest['name_title'] = test['name_title'].replace(['Dr','Rev','Major','Col','Capt','Jonkheer','Mme','Dona'] , 'others')\ntest['name_title'] = test['name_title'].replace(['Master','Lady','Sir','Countess'] , 'royals')\n","72c9fc93":"test[\"name_title\"].value_counts()","924db3c4":"#turning categorical features to numerical ones\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\n\ntrain.iloc[:,4] = label.fit_transform(train.iloc[:,4].values)\ntrain.iloc[:,11] = label.fit_transform(train.iloc[:,11].values)\n\ntest.iloc[:,3] = label.fit_transform(test.iloc[:,3].values)\ntest.iloc[:,10] = label.fit_transform(test.iloc[:,10].values)\n\n","30b4e402":"title_map = {'Mr':1,'Miss':2,'Mrs':3,'royals':4,'others':5}\ntrain['name_title'] = train['name_title'].map(title_map)\ntest['name_title'] = test['name_title'].map(title_map)","6f4b51cc":"#dropping unwanted columns\n\ntrain.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin'],axis=1,inplace=True)\npid = test['PassengerId']\ntest.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin'],axis=1,inplace=True)","6952a483":"test.head()","c4529e85":"x=train.iloc[:,1:8]\ny=train.iloc[:,0]\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.20, random_state = 0)\n\n","e071bb91":"train","942887e2":"#predicting model results\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\ny_pred=lr.predict(x_test)\naccuracy_lr=round(accuracy_score(y_pred,y_test)*100, 2)\nprint('accuracy_lr')\nprint(accuracy_lr)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 5,metric = 'minkowski', p=2)\nknn.fit(x_train,y_train)\ny_pred=knn.predict(x_test)\naccuracy_knn=round(accuracy_score(y_pred,y_test)*100, 2)\nprint('accuracy_knn')\nprint(accuracy_knn)\n    \nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(criterion = 'entropy',random_state = 0)\ntree.fit(x_train,y_train)\ny_pred=tree.predict(x_test)\naccuracy_tree=round(accuracy_score(y_pred,y_test)*100, 2)\nprint('accuracy_tree')\nprint(accuracy_tree)\n    \nfrom sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(n_estimators = 10,criterion = 'entropy',random_state=0)\nforest.fit(x_train,y_train)\ny_pred=forest.predict(x_test)\naccuracy_forest=round(accuracy_score(y_pred,y_test)*100, 2)\nprint('accuracy_forest')\nprint(accuracy_forest)","da7be47c":"prediction = forest.predict(test)\nfinal = pd.DataFrame({'PassengerId':pid , 'Survived':prediction})\n","8b3b14fc":"final.to_csv('submission.csv',index=False)","124d2a4c":"now we have prepared our data ,we'll start training and predicting data","c5ccd549":"> **IMPORTING LIB**","1d6fd29a":"so, for Pclass-1 median age range is around 37 ,for Pclass-2 median age range is around 29 and for Pclass-2 median age range is around 24","034ecf63":"we can see that age is highly correlated with pclass.we are going to replace missing values of age wrt Pclass"}}