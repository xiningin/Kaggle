{"cell_type":{"efe79f09":"code","02321729":"code","0c8d3ecc":"code","c622d533":"code","dd0f3ef7":"code","951a1c12":"code","e3f14838":"code","0d980b4b":"code","396ed844":"code","4df798e2":"code","a044d0a4":"code","2607d89a":"code","4eb45fb8":"code","3b217c54":"code","41cb41a7":"code","6ec8085f":"markdown","5dd42d3d":"markdown","9adad685":"markdown","0693984e":"markdown","4b69f467":"markdown","6d0fda46":"markdown","3a441b94":"markdown"},"source":{"efe79f09":"import itertools\nimport os\n\nimport matplotlib.pylab as plt\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nprint(\"TF version:\", tf.__version__)\nprint(\"Hub version:\", hub.__version__)\nprint(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")","02321729":"model_name = \"mobilenet_v3_small_100_224\" \nmodel_handle_map = {\n  \"efficientnetv2-s\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet1k_s\/feature_vector\/2\",\n  \"efficientnetv2-m\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet1k_m\/feature_vector\/2\",\n  \"efficientnetv2-l\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet1k_l\/feature_vector\/2\",\n  \"efficientnetv2-s-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_s\/feature_vector\/2\",\n  \"efficientnetv2-m-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_m\/feature_vector\/2\",\n  \"efficientnetv2-l-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_l\/feature_vector\/2\",\n  \"efficientnetv2-xl-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_xl\/feature_vector\/2\",\n  \"efficientnetv2-b0-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_b0\/feature_vector\/2\",\n  \"efficientnetv2-b1-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_b1\/feature_vector\/2\",\n  \"efficientnetv2-b2-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_b2\/feature_vector\/2\",\n  \"efficientnetv2-b3-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_b3\/feature_vector\/2\",\n  \"efficientnetv2-s-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_s\/feature_vector\/2\",\n  \"efficientnetv2-m-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_m\/feature_vector\/2\",\n  \"efficientnetv2-l-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_l\/feature_vector\/2\",\n  \"efficientnetv2-xl-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_xl\/feature_vector\/2\",\n  \"efficientnetv2-b0-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_b0\/feature_vector\/2\",\n  \"efficientnetv2-b1-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_b1\/feature_vector\/2\",\n  \"efficientnetv2-b2-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_b2\/feature_vector\/2\",\n  \"efficientnetv2-b3-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_b3\/feature_vector\/2\",\n  \"efficientnetv2-b0\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet1k_b0\/feature_vector\/2\",\n  \"efficientnetv2-b1\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet1k_b1\/feature_vector\/2\",\n  \"efficientnetv2-b2\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet1k_b2\/feature_vector\/2\",\n  \"efficientnetv2-b3\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet1k_b3\/feature_vector\/2\",\n  \"efficientnet_b0\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b0\/feature-vector\/1\",\n  \"efficientnet_b1\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b1\/feature-vector\/1\",\n  \"efficientnet_b2\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b2\/feature-vector\/1\",\n  \"efficientnet_b3\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b3\/feature-vector\/1\",\n  \"efficientnet_b4\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b4\/feature-vector\/1\",\n  \"efficientnet_b5\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b5\/feature-vector\/1\",\n  \"efficientnet_b6\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b6\/feature-vector\/1\",\n  \"efficientnet_b7\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b7\/feature-vector\/1\",\n  \"bit_s-r50x1\": \"https:\/\/hub.tensorflow.google.cn\/google\/bit\/s-r50x1\/1\",\n  \"inception_v3\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/inception_v3\/feature-vector\/4\",\n  \"inception_resnet_v2\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/inception_resnet_v2\/feature-vector\/4\",\n  \"resnet_v1_50\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/resnet_v1_50\/feature-vector\/4\",\n  \"resnet_v1_101\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/resnet_v1_101\/feature-vector\/4\",\n  \"resnet_v1_152\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/resnet_v1_152\/feature-vector\/4\",\n  \"resnet_v2_50\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/resnet_v2_50\/feature-vector\/4\",\n  \"resnet_v2_101\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/resnet_v2_101\/feature-vector\/4\",\n  \"resnet_v2_152\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/resnet_v2_152\/feature-vector\/4\",\n  \"nasnet_large\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/nasnet_large\/feature_vector\/4\",\n  \"nasnet_mobile\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/nasnet_mobile\/feature_vector\/4\",\n  \"pnasnet_large\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/pnasnet_large\/feature_vector\/4\",\n  \"mobilenet_v2_100_224\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/mobilenet_v2_100_224\/feature_vector\/4\",\n  \"mobilenet_v2_130_224\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/mobilenet_v2_130_224\/feature_vector\/4\",\n  \"mobilenet_v2_140_224\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/mobilenet_v2_140_224\/feature_vector\/4\",\n  \"mobilenet_v3_small_100_224\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/mobilenet_v3_small_100_224\/feature_vector\/5\",\n  \"mobilenet_v3_small_075_224\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/mobilenet_v3_small_075_224\/feature_vector\/5\",\n  \"mobilenet_v3_large_100_224\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/mobilenet_v3_large_100_224\/feature_vector\/5\",\n  \"mobilenet_v3_large_075_224\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/mobilenet_v3_large_075_224\/feature_vector\/5\",\n}\n\nmodel_image_size_map = {\n  \"efficientnetv2-s\": 384,\n  \"efficientnetv2-m\": 480,\n  \"efficientnetv2-l\": 480,\n  \"efficientnetv2-b0\": 224,\n  \"efficientnetv2-b1\": 240,\n  \"efficientnetv2-b2\": 260,\n  \"efficientnetv2-b3\": 300,\n  \"efficientnetv2-s-21k\": 384,\n  \"efficientnetv2-m-21k\": 480,\n  \"efficientnetv2-l-21k\": 480,\n  \"efficientnetv2-xl-21k\": 512,\n  \"efficientnetv2-b0-21k\": 224,\n  \"efficientnetv2-b1-21k\": 240,\n  \"efficientnetv2-b2-21k\": 260,\n  \"efficientnetv2-b3-21k\": 300,\n  \"efficientnetv2-s-21k-ft1k\": 384,\n  \"efficientnetv2-m-21k-ft1k\": 480,\n  \"efficientnetv2-l-21k-ft1k\": 480,\n  \"efficientnetv2-xl-21k-ft1k\": 512,\n  \"efficientnetv2-b0-21k-ft1k\": 224,\n  \"efficientnetv2-b1-21k-ft1k\": 240,\n  \"efficientnetv2-b2-21k-ft1k\": 260,\n  \"efficientnetv2-b3-21k-ft1k\": 300, \n  \"efficientnet_b0\": 224,\n  \"efficientnet_b1\": 240,\n  \"efficientnet_b2\": 260,\n  \"efficientnet_b3\": 300,\n  \"efficientnet_b4\": 380,\n  \"efficientnet_b5\": 456,\n  \"efficientnet_b6\": 528,\n  \"efficientnet_b7\": 600,\n  \"inception_v3\": 299,\n  \"inception_resnet_v2\": 299,\n  \"nasnet_large\": 331,\n  \"pnasnet_large\": 331,\n}\n\nmodel_handle = model_handle_map.get(model_name)\npixels = model_image_size_map.get(model_name, 224)\n\nprint(f\"Selected model: {model_name} : {model_handle}\")\n\nIMAGE_SIZE = (pixels, pixels)\nprint(f\"Input size {IMAGE_SIZE}\")\n\nBATCH_SIZE =  16","0c8d3ecc":"data_dir = tf.keras.utils.get_file(\n    'flower_photos',\n    'https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/flower_photos.tgz',\n    untar=True)","c622d533":"def build_dataset(subset):\n    return tf.keras.preprocessing.image_dataset_from_directory(\n      data_dir,\n      validation_split=.20,\n      subset=subset,\n      label_mode=\"categorical\",\n      # Seed needs to provided when using validation_split and shuffle = True.\n      # A fixed seed is used so that the validation set is stable across runs.\n      seed=123,\n      image_size=IMAGE_SIZE,\n      batch_size=1)\n\ntrain_ds = build_dataset(\"training\")\nclass_names = tuple(train_ds.class_names)\ntrain_size = train_ds.cardinality().numpy()\ntrain_ds = train_ds.unbatch().batch(BATCH_SIZE)\ntrain_ds = train_ds.repeat()\n\nnormalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(\n\n    1. \/ 255)\npreprocessing_model = tf.keras.Sequential([normalization_layer])\ndo_data_augmentation = False\nif do_data_augmentation:\n    preprocessing_model.add(\n      tf.keras.layers.experimental.preprocessing.RandomRotation(40))\n    preprocessing_model.add(\n      tf.keras.layers.experimental.preprocessing.RandomTranslation(0, 0.2))\n    preprocessing_model.add(\n      tf.keras.layers.experimental.preprocessing.RandomTranslation(0.2, 0))\n  # Like the old tf.keras.preprocessing.image.ImageDataGenerator(),\n  # image sizes are fixed when reading, and then a random zoom is applied.\n  # If all training inputs are larger than image_size, one could also use\n  # RandomCrop with a batch size of 1 and rebatch later.\n    preprocessing_model.add(\n      tf.keras.layers.experimental.preprocessing.RandomZoom(0.2, 0.2))\n    preprocessing_model.add(\n      tf.keras.layers.experimental.preprocessing.RandomFlip(mode=\"horizontal\"))\ntrain_ds = train_ds.map(lambda images, labels:\n                        (preprocessing_model(images), labels))\n\nval_ds = build_dataset(\"validation\")\nvalid_size = val_ds.cardinality().numpy()\nval_ds = val_ds.unbatch().batch(BATCH_SIZE)\nval_ds = val_ds.map(lambda images, labels:\n                    (normalization_layer(images), labels))","dd0f3ef7":"class_names","951a1c12":"do_fine_tuning = False","e3f14838":"print(\"Building model with\", model_handle)\nmodel = tf.keras.Sequential([\n    # Explicitly define the input shape so the model can be properly\n    # loaded by the TFLiteConverter\n    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n    hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n    tf.keras.layers.Dropout(rate=0.2),\n    tf.keras.layers.Dense(len(class_names),\n                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n])\nmodel.build((None,)+IMAGE_SIZE+(3,))\nmodel.summary()","0d980b4b":"model.compile(\n  optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9), \n  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n  metrics=['accuracy'])","396ed844":"steps_per_epoch = train_size \/\/ BATCH_SIZE\nvalidation_steps = valid_size \/\/ BATCH_SIZE\nhist = model.fit(\n    train_ds,\n    epochs=5, steps_per_epoch=steps_per_epoch,\n    validation_data=val_ds,\n    validation_steps=validation_steps).history","4df798e2":"plt.figure()\nplt.ylabel(\"Loss (training and validation)\")\nplt.xlabel(\"Training Steps\")\nplt.ylim([0,2])\nplt.plot(hist[\"loss\"])\nplt.plot(hist[\"val_loss\"])\n\nplt.figure()\nplt.ylabel(\"Accuracy (training and validation)\")\nplt.xlabel(\"Training Steps\")\nplt.ylim([0,1])\nplt.plot(hist[\"accuracy\"])\nplt.plot(hist[\"val_accuracy\"])","a044d0a4":"x, y = next(iter(val_ds))\nimage = x[0, :, :, :]\ntrue_index = np.argmax(y[0])\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n\n# Expand the validation image to (1, 224, 224, 3) before predicting the label\nprediction_scores = model.predict(np.expand_dims(image, axis=0))\npredicted_index = np.argmax(prediction_scores)\nprint(\"True label: \" + class_names[true_index])\nprint(\"Predicted label: \" + class_names[predicted_index])","2607d89a":"saved_model_path = f\"flowers_model_{model_name}\"\ntf.saved_model.save(model, saved_model_path)","4eb45fb8":"optimize_lite_model = False \nnum_calibration_examples = 60 \nrepresentative_dataset = None\nif optimize_lite_model and num_calibration_examples:\n  # Use a bounded number of training examples without labels for calibration.\n  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n  representative_dataset = lambda: itertools.islice(\n      ([image[None, ...]] for batch, _ in train_ds for image in batch),\n      num_calibration_examples)\n\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\nif optimize_lite_model:\n  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n  if representative_dataset:  # This is optional, see above.\n    converter.representative_dataset = representative_dataset\nlite_model_content = converter.convert()\n\nwith open(f\"lite_flowers_model_{model_name}.tflite\", \"wb\") as f:\n  f.write(lite_model_content)\nprint(\"Wrote %sTFLite model of %d bytes.\" %\n      (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))","3b217c54":"interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n# This little helper wraps the TF Lite interpreter as a numpy-to-numpy function.\ndef lite_model(images):\n  interpreter.allocate_tensors()\n  interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n  interpreter.invoke()\n  return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])","41cb41a7":"num_eval_examples = 50 \neval_dataset = ((image, label)  # TFLite expects batch size 1.\n                for batch in train_ds\n                for (image, label) in zip(*batch))\ncount = 0\ncount_lite_tf_agree = 0\ncount_lite_correct = 0\nfor image, label in eval_dataset:\n  probs_lite = lite_model(image[None, ...])[0]\n  probs_tf = model(image[None, ...]).numpy()[0]\n  y_lite = np.argmax(probs_lite)\n  y_tf = np.argmax(probs_tf)\n  y_true = np.argmax(label)\n  count +=1\n  if y_lite == y_tf: count_lite_tf_agree += 1\n  if y_lite == y_true: count_lite_correct += 1\n  if count >= num_eval_examples: break\nprint(\"TF Lite model agrees with original model on %d of %d examples (%g%%).\" %\n      (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree \/ count))\nprint(\"TF Lite model is accurate on %d of %d examples (%g%%).\" %\n      (count_lite_correct, count, 100.0 * count_lite_correct \/ count))","6ec8085f":"## \u8bad\u7ec3\u6a21\u578b","5dd42d3d":"## \u5728\u7535\u8111\u4e0a\u8dd1lite\u7248\u6a21\u578b\uff0c\u770b\u770b\u6548\u679c","9adad685":"## \u8bad\u7ec3\u597d\u7684\u6a21\u578b\u8f6c\u5316\u4e3aLite\u7248","0693984e":"## \u5b9a\u4e49\u6a21\u578b","4b69f467":"## \u9009\u62e9\u6a21\u578b","6d0fda46":"# \u6784\u5efa\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6","3a441b94":"## \u4e0b\u8f7d\u9c9c\u82b1\u6570\u636e\u96c6"}}