{"cell_type":{"380d45b7":"code","1215cba2":"code","3a687fd3":"code","e7a518d8":"code","e91c9b52":"code","b023129d":"code","68383a73":"code","91db2ebc":"code","7520a352":"code","e4408fc6":"code","0d4f8238":"code","effa7e7b":"code","63757179":"code","2f4c69b0":"code","55bfe9f8":"code","654b15d0":"code","38d07082":"code","245c72a0":"code","d286f059":"code","a24c9262":"markdown","c7a27238":"markdown","72feb953":"markdown","bfa24cd8":"markdown","7e35a172":"markdown","451eea46":"markdown","12314bac":"markdown","d1273690":"markdown","f013bd2f":"markdown","9c825f12":"markdown","a40d7047":"markdown","ec541557":"markdown","7e2baef4":"markdown"},"source":{"380d45b7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport random\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve,roc_auc_score,accuracy_score\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","1215cba2":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\ny = train['target']","3a687fd3":"cols = ['f'+str(i) for i in range(100)]","e7a518d8":"# apply standar scaler to the data\nscaler = StandardScaler()\ntrain[cols] = scaler.fit_transform(train[cols])\ntest[cols] = scaler.transform(test[cols])","e91c9b52":"def fit_linear_regression(train, test):\n    \"\"\"\n    Fits logistic regression to the train data, prints the accuracy and auc\n    and returns the fitted model and test predictions\n    \"\"\"\n    preds_test = np.zeros(test.shape[0])\n    preds_train = np.zeros(train.shape[0])\n\n    model = LogisticRegression(solver='liblinear')\n    model.fit(train[cols],y)\n\n    preds_test  += model.predict_proba(test[cols])[:,1]\n    preds_train += model.predict_proba(train[cols])[:,1]\n\n    auc = roc_auc_score(y, preds_train)\n    acc = accuracy_score(y, model.predict(train[cols]))\n\n    print(f\"accuracy: {round(acc*100,3)} , auc: {round(auc*100,3)}\")\n    \n    return model, preds_test","b023129d":"def plot_histograms(train, var = 'LRLC', b = np.arange(-4, 4, 0.1)):\n    \"\"\"\n    Plots two histograms - distributions of \"var\" for training samples with target zero \/ one.\n    \"\"\"\n    y0 = train[train['target'] == 0]\n    y1 = train[train['target'] == 1]\n\n    plt.hist(y0[var], bins = b, alpha = 0.3);\n    plt.title('target = 0');\n    plt.xlabel(var);\n    plt.axvline(0, color = 'k');\n\n    plt.show()\n\n    plt.hist(y1[var], bins = b, alpha = 0.3);\n    plt.title('target = 1');\n    plt.xlabel(var);\n    plt.axvline(0, color = 'k');","68383a73":"model, preds_test = fit_linear_regression(train, test)\nsub['target']=preds_test\nsub.to_csv('submission.csv', index=False)","91db2ebc":"c0 = model.intercept_[0]\nci = model.coef_[0]\n\ntrain['LRLC'] = c0\ntest['LRLC'] = c0\nfor i in range(100):\n    train['LRLC'] += ci[i] * train['f' + str(i)]\n    test['LRLC']  += ci[i] * test['f' + str(i)]\n    \nplot_histograms(train, 'LRLC', np.arange(-3, 3, 0.1))","7520a352":"THR = 0.3\nmislabelled_guys = ((train['LRLC'] > THR) & (train['target'] == 0)) | ((train['LRLC'] < - THR) & (train['target'] == 1))\ntrain.loc[mislabelled_guys, 'target'] = 1 - train.loc[mislabelled_guys, 'target']\nnp.save('targets1.npy', train['target'].values)","e4408fc6":"model, preds_test = fit_linear_regression(train, test)\nsub['target']=preds_test\nsub.to_csv('submission2.csv', index=False)","0d4f8238":"c0 = model.intercept_[0]\nci = model.coef_[0]\n\ntrain['LRLC2'] = c0\ntest['LRLC2'] = c0\nfor i in range(100):\n    train['LRLC2'] += ci[i] * train['f' + str(i)]\n    test['LRLC2']  += ci[i] * test['f' + str(i)]","effa7e7b":"plot_histograms(train, 'LRLC2')","63757179":"THR = 1.2\nmislabelled_guys = ((train['LRLC2'] > THR) & (train['target'] == 0)) | ((train['LRLC2'] < - THR) & (train['target'] == 1))\ntrain.loc[mislabelled_guys, 'target'] = 1 - train.loc[mislabelled_guys, 'target']\nnp.save('targets2.npy', train['target'].values)","2f4c69b0":"model, preds_test = fit_linear_regression(train, test)\nsub['target']=preds_test\nsub.to_csv('submission3.csv', index=False)","55bfe9f8":"c0 = model.intercept_[0]\nci = model.coef_[0]\n\ntrain['LRLC3'] = c0\ntest['LRLC3'] = c0\nfor i in range(100):\n    train['LRLC3'] += ci[i] * train['f' + str(i)]\n    test['LRLC3']  += ci[i] * test['f' + str(i)]\n    \nplot_histograms(train, 'LRLC3')","654b15d0":"THR = 1.2\nmislabelled_guys = ((train['LRLC3'] > THR) & (train['target'] == 0)) | ((train['LRLC3'] < - THR) & (train['target'] == 1))\ntrain.loc[mislabelled_guys, 'target'] = 1 - train.loc[mislabelled_guys, 'target']\nnp.save('targets3.npy', train['target'].values)","38d07082":"model, preds_test = fit_linear_regression(train, test)\nsub['target']=preds_test\nsub.to_csv('submission4.csv', index=False)","245c72a0":"c0 = model.intercept_[0]\nci = model.coef_[0]\n\ntrain['LRLC4'] = c0\ntest['LRLC4'] = c0\nfor i in range(100):\n    train['LRLC4'] += ci[i] * train['f' + str(i)]\n    test['LRLC4']  += ci[i] * test['f' + str(i)]\n    \nplot_histograms(train, 'LRLC4')","d286f059":"THR = 1.2\nmislabelled_guys = ((train['LRLC4'] > THR) & (train['target'] == 0)) | ((train['LRLC4'] < - THR) & (train['target'] == 1))\ntrain.loc[mislabelled_guys, 'target'] = 1 - train.loc[mislabelled_guys, 'target']\n\nnp.save('targets4.npy', train['target'].values)","a24c9262":"Fit the model, save the resulting predictions","c7a27238":"# Import libraries","72feb953":"# Second round","bfa24cd8":"# Helper functions","7e35a172":"# Import data and scale the features","451eea46":"# Third round","12314bac":"Add the logistic regression variable to the training set, plot its distribution on target == 0 and target == 1 subsets","d1273690":"Correct the samples we believe are mislabeled. Save the result.","f013bd2f":"# Summary","9c825f12":"From now on, we just repeat the same procedure three more times, always saving the results.","a40d7047":"We used logistic regression to try finding and correcting the mislabeled samples in the training set. We performed four iterations, always relabelling the most discrepant samples.\n\nThe underlying model does not seem to be a simple logistic regression and thus there is a risk that some of the corrections are wrong, especially in the latter iterations.\n\nWe output the target values we corrected in each iteration as a \\*.npy file, together with predictions of the logistic regression. The score raises from 0.74529 to 0.74637 between the first and fourth iteration.\n\nBuilt upon the [notebook](https:\/\/www.kaggle.com\/hamzaghanmi\/make-it-simple).","ec541557":"# First round","7e2baef4":"# Fourth round"}}