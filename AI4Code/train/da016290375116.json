{"cell_type":{"f4dceee0":"code","be462467":"code","7de531e3":"code","3ba4109c":"code","ac575b28":"code","b1b6637e":"code","92b0a43e":"code","17049593":"code","4afcf73d":"code","febb8bad":"code","47077489":"code","15520012":"code","696df2f8":"code","74e1db51":"code","4eb33f3e":"code","c506f8a9":"code","309544a4":"code","b209c6ed":"code","40d230c6":"code","a3a3935e":"code","eed32fe4":"code","c123bf0b":"code","a3a92187":"code","422962ff":"code","b5a7ffbf":"code","ee02e2f6":"code","6be9fe06":"code","15bd1ab3":"code","fa0b0988":"code","68e33363":"code","0b16bc58":"code","c3184c90":"code","99372edb":"code","cec5dd2f":"code","60dee508":"code","ac576d76":"code","b4ee958c":"code","8672ebe7":"code","0f7b9b26":"code","d1633aab":"code","113e2cec":"code","aa672c1e":"code","15d972a1":"code","1d779eb0":"code","e4aa079d":"markdown","3913bcf9":"markdown","9eaae12e":"markdown","713956b9":"markdown","a62d5cfc":"markdown","bf3a5238":"markdown","ae8473b9":"markdown","06d9f40a":"markdown","95733558":"markdown","5c633264":"markdown","22748b39":"markdown","bdb29788":"markdown","d7271a0c":"markdown","842335fb":"markdown","aa0772dc":"markdown","dcdda9a6":"markdown","76c8363c":"markdown","9a11b04b":"markdown","087e36f4":"markdown","4e6fd168":"markdown","7d8019fe":"markdown","1c92e786":"markdown","8f494a87":"markdown","4a97f533":"markdown","c3aaf7e5":"markdown","70bbb195":"markdown","4f9c0807":"markdown","a9f5e236":"markdown","140df131":"markdown","e74a3da5":"markdown","d3bd2f71":"markdown","9e0f48a3":"markdown","cda99e8e":"markdown","29a4bc27":"markdown"},"source":{"f4dceee0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","be462467":"import pandas as pd\nimport numpy as np \n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","7de531e3":"gender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain.info() # Rough estimating the missing values for test","3ba4109c":"test.info() # Rough estimating the missing values for train\norignal_test = test.copy()","ac575b28":"train.head() # Survival is given and we need to predict it for the test set","b1b6637e":"test.head() ","92b0a43e":"total = train.isnull().sum().sort_values(ascending=False)\nprint(total)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nprint(percent)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(3)","17049593":"train.describe()","4afcf73d":"train.describe(include = ['O'])","febb8bad":"Pclass_analyze = train[['Survived','Pclass']].groupby(['Pclass'],as_index = False).mean() # Pclass have no missing values (Total count 891 in train and 418 in test)\nprint(Pclass_analyze) \nsns.barplot(x = train['Pclass'],y = train['Survived'])\n# Ratio gives us the probability for the survival (for example for Pclass 1 the survived probability in 0.629 and the death probability is 0.371)","47077489":"Sex_analyze = train[['Sex','Survived']].groupby(('Sex'),as_index = False).mean() # Sex have no missing values (Total count 891 in train and 418 in test)\nprint(Sex_analyze)\nsns.barplot(x=train['Sex'],y=train['Survived'])\n# Shows that more women survive as compare to men ","15520012":"data = [train,test] # create a new array\n\nfor d in data:\n    d['Family'] = d['SibSp']+d['Parch']  # combining sibling and spouse as a new parameter Family  \n    Family_analyze = train[['Family','Survived']].groupby(['Family'],as_index=False).mean()\nprint(Family_analyze)\nsns.barplot(x=train['Family'],y=train['Survived'])","696df2f8":"for d in data:\n    d['IsAlone'] = 0\n    d.loc[d['Family'] == 0, 'IsAlone'] = 1\nprint (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())\nsns.barplot(x=train['IsAlone'],y=train['Survived'])\n# Where  1 = alone and 0 = with family","74e1db51":"train['Embarked'].isnull().sum() # missing check again ","4eb33f3e":"train['Embarked'].mode()","c506f8a9":"train['Embarked'] = train['Embarked'].fillna('S') # filling the 2 missing values with mode i.e S=Southampton\nprint(train[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean())\nsns.barplot(x=train['Embarked'],y=train['Survived'])","309544a4":"sns.boxplot( y=d['Fare'] );\nplt.show()","b209c6ed":"test.isnull().sum() # as there is one missing value in fare in the test data set so we need to fill it\ntest['Fare'] = test['Fare'].fillna(test['Fare'].median()) # as we can see much outliers in the fare so fill th emissing value with median\ntest['Fare']","40d230c6":"train['CategoricalFare'] = pd.qcut(train['Fare'], 3)# We quartiled the fare data into 3 section as the lowest medium and higher\nprint (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())\nsns.barplot(x=train['CategoricalFare'],y=train['Survived'])","a3a3935e":"#first lets check the number of people survived acording to there age \nage_plot = sns.FacetGrid(train, col = 'Survived')\nage_plot.map(plt.hist, 'Age')\n\n#clearly visible that most of the people 15-30 died while children less then 10 and old people of 80 survived. \n\n","eed32fe4":"#As Age have missing values in both test and train data we will simply fill these values with their mean \nprint('Age : Train Null value: ',train['Age'].isnull().sum())\nprint('Age : Test Null value: ',test['Age'].isnull().sum())\nMean_train = train['Age'].mean()\nMean_test = test['Age'].mean()\ntrain['Age'].fillna(Mean_train,inplace=True)\ntrain['Age'] = train['Age'].astype(int)\ntest['Age'].fillna(Mean_test,inplace=True)\ntest['Age'] = test['Age'].astype(int)\ntrain.info()\ntest.info()\n","c123bf0b":"# Now we create age-slots to represent the people Age\ntrain['AgeSlots'] = pd.cut(train['Age'],5)\ntest['AgeSlots'] = pd.cut(test['Age'],5)\nprint(train[['AgeSlots','Survived']].groupby(['AgeSlots'],as_index = False).mean())","a3a92187":"#We can extract relevant information from the title's \n# We want to see what relation does these title have with survivabilty\n# The RegEx pattern (\\w+\\.) matches the first word which ends with a dot character within Name feature. The expand=False flag returns a DataFrame.\nData = [train,test]\nfor d in Data:\n    d['Title']=d.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\npd.crosstab(train['Title'], train['Sex'])","422962ff":"#We can further divide many of these title  in a Uncommon category \nfor d in data:\n    d['Title'] = d['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Uncommon')\n    d['Title'] = d['Title'].replace('Mlle', 'Miss')\n    d['Title'] = d['Title'].replace('Ms', 'Miss')\n    d['Title'] = d['Title'].replace('Mme', 'Mrs')\nprint(train[['Title','Survived']].groupby(['Title'],as_index=False).mean())\nsns.barplot(x=train['Title'],y=train['Survived'])\ntest.head()","b5a7ffbf":"#Lets check the data \nprint(train.info())\nprint(test.info())","ee02e2f6":"for d in data:\n    \n    # SEX\n    d['Sex'] = d['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Title (Name)\n    apply_map = { 'Mr': 1, 'Miss': 2, 'Mrs' : 3, 'Master': 4, 'Other': 5 } \n    d['Title'] = d['Title'].map(apply_map)\n    d['Title'] = d['Title'].fillna(0)\n   \n    # Embarked\n    d['Embarked'] = d['Embarked'].map({ 'C': 1, 'S': 2, 'Q': 3 }).astype(int)\n    \n    # FareSlots (Fare)\n    d.loc[d['Fare'] <= 8.662,'Fare'] = 0                        \n    d.loc[(d['Fare'] > 8.662) & (d['Fare']<= 26.0) ,'Fare'] = 1   \n    d.loc[(d['Fare'] > 26.0),'Fare'] = 2                         \n    \n    # AgeSLots (Age)\n    d.loc[d['Age'] <= 16.0 ,'Age'] = 0\n    d.loc[(d['Age'] > 16.0)  &  (d['Age'] <= 32.0) , 'Age' ] = 1\n    d.loc[(d['Age'] > 32.0)  &  (d['Age'] <= 48.0) , 'Age' ] = 2\n    d.loc[(d['Age'] > 48.0)  &  (d['Age'] <= 64.0) , 'Age' ] = 3\n    d.loc[(d['Age'] > 64.0),'Age'] = 4\n    ","6be9fe06":"#Now we will change the data type of Title and Fare\nfor d in data:\n    #Fare\n    d['Fare'] = d['Fare'].astype(int)\n    \n    #Title\n    d['Title'] = d['Title'].astype(int)\n","15bd1ab3":"print(train.info())\nprint(test.info())","fa0b0988":"train.drop(['PassengerId','Cabin','Ticket','CategoricalFare','SibSp','AgeSlots','Name','Parch','Family'],axis=1, inplace=True)\ntest.drop(['PassengerId','Cabin','Ticket','SibSp','Name','Parch','Family','AgeSlots'],axis=1, inplace=True)\nprint(train.head())\nprint(test.head())\n","68e33363":"import seaborn as sns \nplt.figure(figsize=(14,12))\nsns.heatmap(train.corr(),cmap='twilight',linecolor='white', annot=True)\nplt.title('Pearson Correlation',y = 2, size=20)\n# This correlation diagram shows that Survive have maximum correlation with the Title.\n","0b16bc58":"age_plot = sns.FacetGrid(train, col = 'Sex',row='Pclass')\nage_plot.map(plt.hist, 'Age')\nage_plot.add_legend()","c3184c90":"grid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2.2, aspect=1.8)\ngrid.map(plt.hist, 'Age', bins=20)\ngrid.add_legend();","99372edb":"X_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nX_test = test\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape)","cec5dd2f":"LR = LogisticRegression()\nLR.fit(X_train,Y_train)\nLR_predict = LR.predict(X_test)\nLR_score = (LR.score(X_train,Y_train))*100\nLR_score","60dee508":"knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train,Y_train)\nknn_predict = knn.predict(X_test)\nknn_score = (knn.score(X_train,Y_train))*100\nknn_score","ac576d76":"SVC = SVC()\nSVC.fit(X_train,Y_train)\nSVC_predict = SVC.predict(X_test)\nSVC_score = (SVC.score(X_train,Y_train))*100\nSVC_score","b4ee958c":"GNB = GaussianNB()\nGNB.fit(X_train, Y_train)\nGNB_pred = GNB.predict(X_test)\nGNB_score = (GNB.score(X_train,Y_train))*100\nGNB_score","8672ebe7":"LSVC = LinearSVC()\nLSVC.fit(X_train, Y_train)\nLSVC_pred = LSVC.predict(X_test)\nLSVC_score = (LSVC.score(X_train,Y_train))*100\nLSVC_score","0f7b9b26":"DTC = DecisionTreeClassifier()\nDTC.fit(X_train, Y_train)\nDTC_predict = DTC.predict(X_test)\nDTC_score = (DTC.score(X_train,Y_train))*100\nDTC_score","d1633aab":"RFC = RandomForestClassifier(n_estimators = 1000)\nRFC.fit(X_train, Y_train)\nRFC_predict = RFC.predict(X_test)\nRFC_score = (RFC.score(X_train,Y_train))*100\nRFC_score","113e2cec":"perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nscore_perceptron = (perceptron.score(X_train,Y_train))*100\nscore_perceptron","aa672c1e":"sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nsgd_pred = sgd.predict(X_test)\nscore_sgd = (sgd.score(X_train,Y_train))*100\nscore_sgd","15d972a1":"#Dataframe for score is created and ranked \nfinal_model = pd.DataFrame({ 'Model' :['Support Vector Machines', 'KNN', \n    'Logistic Regression','Random Forest', 'Naive Bayes', 'Perceptron', \n     'Stochastic Gradient Decent', 'Linear SVC','Decision Tree'],\n    'Score' : [SVC_score,knn_score,LR_score, RFC_score, GNB_score, \n            score_perceptron, score_sgd, LSVC_score, DTC_score]})\n\nfinal_model.sort_values(by = 'Score', ascending=False)","1d779eb0":"submission = pd.DataFrame({\"PassengerId\": orignal_test['PassengerId'], \"Survived\": RFC_predict})\nsubmission.to_csv(\"Submission1.csv\", index=False)\nprint(submission)","e4aa079d":"# Model Evaluation ","3913bcf9":"# Perceptron","9eaae12e":"### Missing values\n\nmissing values in training set and there respective percentage","713956b9":"## 6. Fare","a62d5cfc":"##  Feature Selection","bf3a5238":"## 3. SibSp and Parch\n\nWith the number of siblings\/spouse and the number of children\/parents we can create new feature called Family Size.","ae8473b9":"# Submission","06d9f40a":"# Decision Tree","95733558":"# SVM","5c633264":"# Gaussian Naive Bayes","22748b39":"# Linear SVC ","bdb29788":"Observation\n* In Pclass 2 and 3 most of the people that survived are infants\n* Most of the people stayed in Pclass 3\n* Most of the people in Pclass  survived \n* In only Pclass 1 the people of old age (slot 4) survived ","d7271a0c":"# Stochastic Gradient Descent","842335fb":"## 8. Name","aa0772dc":"## 5. Cabin\n\nAs every cabin has unique id and 77 percent values are missing so this sort coulmn in useless for survival prediction","dcdda9a6":"# The current prediction accuracy is almost 86%. I haven't included Cabin feature due to a large number of null values. if it is included the prediction accuracy might increase.\n","76c8363c":"# Data Cleaning \n","9a11b04b":"# Analyzing through Visualization","087e36f4":"## 2.Sex ","4e6fd168":"Probability of being alone and with family survival rate","7d8019fe":"## General Information \n\npclass   = Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n\nsex      = Sex\t\nAge      = Age (years)\t\nsibsp    = number of siblings \/ spouses aboard the Titanic\t\nparch    = number of parents \/ children aboard the Titanic\t\nticket   = Ticket number\t\nfare     = Passenger fare\t\ncabin    = Cabin number\t\nembarked = Port of Embarkation\t(C = Cherbourg, Q = Queenstown, S = Southampton) \n \nsurvival = Survival\t(0 = No, 1 = Yes)","1c92e786":"Now we are able to train our model and predict whether a person survived or not according to the parameter(Age,Title, Sex, etc) that we have shortlisted through features selection. Our problem is a classification and regression problem and it fall in the category of Supervise Learning. So through these two categories we can narrowdown our choice of model to a few. These include:\n* Logistic Regression\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forrest\n* Perceptron\n* Artificial neural network\n* RVM or Relevance Vector Machine","8f494a87":"## 1. Pclass ","4a97f533":"* Most of the people in Pclass 3 are Male and belong slot 1 of Age \n* In Pclass 1 & 2 people of different gender are more evenly distributed","c3aaf7e5":"## 7. Age","70bbb195":"* Train samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n* Survived is a categorical feature with 0 or 1 values.\n* Around 38% samples survived representative of the actual survival rate at 32%.\n* Most passengers (> 75%) did not travel with parents or children.\n* Nearly 30% of the passengers had siblings and\/or spouse aboard.\n* Fares varied significantly with few passengers (<1%) paying as high as  512.\n* Few elderly passengers (<1 percent) within age range 65-80.\n* Names are unique across the dataset (count=unique=891)\n* Sex variable as two possible values with 65% male (top=male, freq=577\/count=891).\n* Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n* Embarked takes three possible values. S port used by most passengers (top=S)\n* Ticket feature has high ratio (22%) of duplicate values (unique=681).","4f9c0807":"# EDA (Exploratory Data Analysis)","a9f5e236":"\n# Introduction    \u261c(\uff9f\u30ee\uff9f\u261c)\n\nProblem statement : predicting survival prediction based on the given dimension","140df131":"## 4. Embarked","e74a3da5":"## KNN","d3bd2f71":"# Random Forest","9e0f48a3":"# Modeling & Prediction  ","cda99e8e":"# Logistic Regression\n","29a4bc27":"# Understanding the data"}}