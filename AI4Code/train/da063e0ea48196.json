{"cell_type":{"d55ea1bf":"code","f6a9c520":"code","2fd96311":"code","9163d014":"code","90e8c7c1":"code","10a42a44":"code","4e09c3cf":"code","7a7d5ca1":"code","3d849a64":"code","d25fc634":"code","afc3a87e":"code","7c5d6c17":"code","90225774":"code","373e800d":"markdown","18ba82e8":"markdown","22aeafb4":"markdown","25eb4c95":"markdown","bef1a240":"markdown","708cc389":"markdown","dfd94649":"markdown","8de8988c":"markdown","529ca347":"markdown","ffda409c":"markdown","7314c19c":"markdown","70195bb3":"markdown"},"source":{"d55ea1bf":"%%HTML\n<style type=\"text\/css\">\n\ndiv.h1 {\n    font-size: 32px; \n    margin-bottom:2px;\n    background-color: steelblue; \n    color: white; \n    text-align: center;\n}\ndiv.h2 {\n    background-color: steelblue; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 24px; \n    max-width: 1500px; \n    margin-top: 50px;\n    margin-bottom:4px;\n    \n}\ndiv.h3 {\n    color: steelblue; \n    font-size: 20px; \n    margin-top: 4px; \n    margin-bottom:8px;\n}\ndiv.h4 {\n    font-size: 15px; \n    margin-top: 20px; \n    margin-bottom: 8px;\n}\n\n<\/style>","f6a9c520":"# Familiar imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n # For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nprint(\"loaded libraries\")","2fd96311":"# Load the training data\ndf = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\")\n# Preview the data\ndf.head()","9163d014":"\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]","90e8c7c1":"final_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    params ={'random_state': 1,   \n             'booster': 'gbtree',\n             'n_estimators': 10000,\n             'learning_rate': 0.03628302216953097,\n             'reg_lambda': 0.0008746338866473539,\n             'reg_alpha': 23.13181079976304,\n             'subsample': 0.7875490025178415,\n             'colsample_bytree': 0.11807135201147481,\n             'max_depth': 3    } \n    model = XGBRegressor(n_jobs=4,**params)\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n#create a newdataframe  with id and the predications from validation set\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"train_pred_1.csv\", index=False)\n#create a new test csv with the final test prediction\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_1\"]\nsample_submission.to_csv(\"test_pred_1.csv\", index=False)\n\n","10a42a44":"\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    params = {'learning_rate': 0.07853392035787837,\n              'reg_lambda': 1.7549293092194938e-05,\n              'reg_alpha': 14.68267919457715,\n              'subsample': 0.8031450486786944,\n              'colsample_bytree': 0.170759104940733,\n              'max_depth': 3}\n    model = XGBRegressor(random_state=fold,n_jobs=4,n_estimators=5000,**params)\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_2\"]\nfinal_valid_predictions.to_csv(\"train_pred_2.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_2\"]\nsample_submission.to_csv(\"test_pred_2.csv\", index=False)","4e09c3cf":"from catboost import CatBoostRegressor","7a7d5ca1":"final_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    params = {'learning_rate': 0.02,\n              'grow_policy':'Depthwise',\n              'iterations':4000,\n              'use_best_model':True,\n              'eval_metric':'RMSE',\n              'od_type':'iter',\n              'od_wait':40,\n              'max_depth': 3 }\n       \n    model = CatBoostRegressor(\n        random_state=fold,\n        **params\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=400, eval_set=[(xvalid, yvalid)], verbose=1000)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_3\"]\nfinal_valid_predictions.to_csv(\"train_pred_3.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_3\"]\nsample_submission.to_csv(\"test_pred_3.csv\", index=False)","3d849a64":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\ndf1 = pd.read_csv(\"train_pred_1.csv\")\ndf2 = pd.read_csv(\"train_pred_2.csv\")\ndf3 = pd.read_csv(\"train_pred_3.csv\")\n\ndf_test1 = pd.read_csv(\"test_pred_1.csv\")\ndf_test2 = pd.read_csv(\"test_pred_2.csv\")\ndf_test3 = pd.read_csv(\"test_pred_3.csv\")\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\ndf = df.merge(df3, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")\ndf.head()\n","d25fc634":"from sklearn.linear_model import LinearRegression","afc3a87e":"useful_features = [\"pred_1\", \"pred_2\",\"pred_3\"]\ndf_test = df_test[useful_features]\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    model = LinearRegression()\n    \n    model.fit(xtrain, ytrain)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n","7c5d6c17":"model.coef_","90225774":"sample_submission.target = np.mean(np.column_stack(final_predictions), axis=1)\nsample_submission.to_csv(\"Blended.csv\", index=False)\n\n","373e800d":"<a id='mb'><\/a>\n<div class=\"h3\">Model Building<\/div>\n","18ba82e8":"Blending 3 different models","22aeafb4":"\n<div class=\"h1\">30 Days of ML- Competition<\/div>\n\n","25eb4c95":"<a id='rd'><\/a>\n<div class=\"h3\">Read and Understand Data<\/div>\n\n","bef1a240":"<a id='il'><\/a>\n<div class=\"h3\">Import Libraries<\/div>","708cc389":"<div class=\"h1\">Table of Contents<\/div>\n\n  - <a href='#il'>Import Libraries<\/a>\n  - <a href='#rd'>Read and Understand Data<\/a>\n  - <a href='#mb'>Model Building <\/a>\n  - <a href='#bl'>Blending<\/a> ","dfd94649":"**2nd model XGboost different parameters**","8de8988c":"**Blending**\n\nBlending combine the decisions from multiple models to improve the overall performance. This approach allows for better predictive performance compared to a single model.It help us to improve performance and increase accuracy.  \n\nBlending ensemble are a type of stacking where the meta-model is fit using predictions on a holdout validation dataset instead of out-of-fold predictions.\n\nFor example, a linear regression model when predicting a numerical value or a logistic regression model when predicting a class label would calculate a weighted sum of the predictions made by base models and would be considered a blending of predictions.\n\n Understanding the blending process\n - The train set is split into two parts, viz-training and validation sets.\n - Model(s) are fit on the training set.\n - The predictions are made on the validation set and the test set.\n - The validation set and its predictions are used as features to build a new model.\n - This model is used to make final predictions on the test and meta-features.\n \n**Resources**\n\nhttps:\/\/machinelearningmastery.com\/blending-ensemble-machine-learning-with-python\/\n\nhttps:\/\/www.youtube.com\/watch?v=ISZYWtvoCAc ( Abhishek Thakur Video)\n\nhttps:\/\/www.youtube.com\/watch?v=TuIgtitqJho ( Abhishek Thakur Video)\n\nhttps:\/\/mlwave.com\/kaggle-ensembling-guide\/?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3BPZ4T3JLHTu%2BOWNI0d5kFbg%3D%3D\n\n\n\n","529ca347":"**1st Model XGBoost**","ffda409c":"**3rd Model CatBoost**","7314c19c":"This notebook is created as part  of 30 day ML challenge. This book is divided in 3 part EDA & kfold(https:\/\/www.kaggle.com\/yogidsba\/eda-kfold-30-day-ml) , Modelbuilding using XGBoost ,Understanding blending\n\n**About the data**\nThe dataset is used for this competition is synthetic, but based on a real dataset. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.\n\n\n**Evaluation**\n\nSubmissions are scored on the root mean squared error.\n\n\n**Special mention**\nI learned about Blending from Abhishek Thakur YoutubeChannel. For that code you can refer his notebook or watch youtube channel.(https:\/\/www.youtube.com\/watch?v=ISZYWtvoCAc)Thank you @Abhisek","70195bb3":"<a id='bl'><\/a>\n<div class=\"h3\">Blending Models<\/div>\n"}}